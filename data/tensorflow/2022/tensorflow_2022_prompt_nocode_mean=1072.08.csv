725,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.image.extract_patches fails to JIT-compile inside keras model)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Have you reproduced the bug with TF nightly? Yes  Source binary  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04, Mac OS X 12.6  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shkarupa-alex,tf.image.extract_patches fails to JIT-compile inside keras model,"Click to expand!    Issue Type Bug  Have you reproduced the bug with TF nightly? Yes  Source binary  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04, Mac OS X 12.6  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` ",2022-12-31T09:44:36Z,awaiting review type:bug comp:keras comp:xla TF 2.11,closed,0,13,https://github.com/tensorflow/tensorflow/issues/59058,"Hi alex ! I was able to replicate this bug with tf.function style and workaround is to use autoclustering.   ! Could you look at this issue. Attached gist in 2.9, 2.10, 2.11 and nightly for reference. Thank you!","The function tf.image.extract_patches is not supported on cloud TPU. Check out this link for more information https://cloud.google.com/tpu/docs/tensorflowops  In you autoclustering code, the code fails if we try to set jit_compile=True in model.compile function. Can you elaborate more about the autoclustering code? ","Hi  , XLA have some limitations.At present all the Ops are not compilable with `jit_compile=True`.There are also some known errors as documented here. The error might be probably due to either the Op is not compatible with Jit_compile or the shapes might be dynamic which is not supported by XLA. If you choose auto_clustering then it will automatically chooses from CPU to GPU based on implementation which is not possible with explicit setting in model.compile(jit_compile=True) which forces to use only `jit_compile` and hence throwing the error. Please refer to autoclustering tutorial here. However `autoclustering` too have limitations at present like below as per source. > Note: In TF2, only the code inside `tf.function` will be clustered. > Note: Autoclustering support on CPU and on multiGPU environments is experimental. It seems the shape of the Op `tf.image.extract_patches` is not inferrable at the time of compilation i.e. the output shape of the Op output can't be inferred without actual computation which might happen only during `model.fit()` in this case.Could you cross check and confirm?", Thanks for the explanation. Now it makes sense as to why the error shows up when jit_compile is set to true. However i am not sure how to identify whether the shape of the Op tf.image.extract_patches is inferrable at the time of the compilation or not.  ,"Hi  , If the shape is not inferrable without some computation that may happens during `model.fit()` or say during training step then it is not available for compilation. In our case we can check it by `model.summary()` before `model.compile()`.If we don't get the proper shape(of course batch size might be neglected here since we pass it during training only) may be we can't use `jit_compile` there. Please refer the demo gist for our case.Hope it helps for you and comeback if still needs clarity. Thankyou!","> Hi  , >  > If the shape is not inferrable without some computation that may happens during `model.fit()` or say during training step then it is not available for compilation. In our case we can check it by `model.summary()` before `model.compile()`.If we don't get the proper shape(of course batch size might be neglected here since we pass it during training only) may be we can't use `jit_compile` there. Please refer the demo gist for our case.Hope it helps for you and comeback if still needs clarity. >  > Thankyou! Even if we set constant shape for input layer in this gist (and all shapes in model summary become defined), it still fails with jit_compile=True","As far as i understand, this issue along with https://github.com/tensorflow/tensorflow/issues/59061 come from gradient implementation here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.pyL1021","alex , Even if the input shapes are known but if the function output shapes depends upon computation that happening inside the function then jit_compile=True may not work. For example let take the sample code of yours below:  The shape of the variable `outputs` might depends upon the parameters `([1, 16, 16, 1], [1, 8, 8, 1], [1] * 4)` passed in to the Op `tf.image.extract_patches `. Here the shape only inferrable after calling with some input data which do some computation.This computation won't happen during Jit compilation. Hence you may get error. If you can build the model such a way that the output shapes independent of actual computation then jit_compile should work.Please review once again and comeback if anything missing or needs clarification here. Thankyou!", i figured out that issue comes from gradient computation. Here is a fix https://github.com/tensorflow/tensorflow/pull/59185,"alex , Thanks for the PR.Will also keep track on this and let you know once merged.Thanks!","I have validated my comment above.I tried to pass the Inputs with clear input shape `(16, 16, 8)` and model.summary() prints the output shape as `(None, 2, 2, 1)` which means all the shapes are inferrable at the compilation time itself.Please refer attached revised gist.  Hence this problem is not related to shape inference as suspected earlier by me.May be this is valid bug which needs fix.Hoping the above commit by alex fixes this issue.Thanks!",Are you satisfied with the resolution of your issue? Yes No,The failed code tested with tfnightly after PR merge and it is running fine without error. Please find the gist here. alex Thanks for your contribution.
1033,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Training horizontally stacked layers does not happen in parallel)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Bug  Source source  Tensorflow Version 2.7.0  Custom Code Yes  Current Behaviour? When using Keras to build a multiple layer model, by stacking layers horizontally instead of vertically (deep network), training happens sequentially, and adding more layers increases each epoch duration linearly, while GPU utilization and memory usage remains the same as a single layer regardless of the number of total layers.  Standalone code to reproduce the issue In the example below I would expect that each subnetwork called though the `cnn_example()` function, would run in parallel before the `GlobalAveragePooling1D` operation is called. However this does not happen, regardless the number of times the layer is repeated. Is this expected behavior or am I missing something? )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,carlosg-m,Training horizontally stacked layers does not happen in parallel,"  Issue Type Bug  Source source  Tensorflow Version 2.7.0  Custom Code Yes  Current Behaviour? When using Keras to build a multiple layer model, by stacking layers horizontally instead of vertically (deep network), training happens sequentially, and adding more layers increases each epoch duration linearly, while GPU utilization and memory usage remains the same as a single layer regardless of the number of total layers.  Standalone code to reproduce the issue In the example below I would expect that each subnetwork called though the `cnn_example()` function, would run in parallel before the `GlobalAveragePooling1D` operation is called. However this does not happen, regardless the number of times the layer is repeated. Is this expected behavior or am I missing something? ",2022-12-29T11:45:23Z,stat:awaiting response type:bug stale comp:keras TF 2.7,closed,0,6,https://github.com/tensorflow/tensorflow/issues/59049,m  I tried to reproduce the issue on Colab using TF v2.11 but faced a different error. Could you please find the gist here for reference and provide complete code or colab gist to replicate the issue reported here. Thank you!,", sorry about that, I was hoping that would be enough. Anyway, I followed your template and created a Colab notebook, with a complete reproducible example:   https://colab.research.google.com/drive/11jtCBrwWqDYbdsuMu04ptRfFXi0sr1EV?usp=sharing The notebook was running on a GPU with small batch sizes of `32` to keep resource usage low, so when the number of stacked layers increases, there are free resources available to allocate, assuming a quasiparallel execution. However once again that was not the case:   1 model: ~5 seconds epoch   2 models: ~10 seconds epoch   3 models: ~15 seconds epoch",m  This issue seems to be a keras related issue. Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
656,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The second-order gradient of tf.math.log and tf.math.log1p is 0 when the input is NaN.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  When `1x),) (tensor(nan, grad_fn=),) ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,maybeLee,The second-order gradient of tf.math.log and tf.math.log1p is 0 when the input is NaN.,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  When `1x),) (tensor(nan, grad_fn=),) ``` ",2022-12-29T06:05:44Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/59046,", I was able to reproduce the issue on tensorflow v2.9, v2.11 and nightly. Kindly find the gist of it here.","This is a bit of an oddball. Your input function is _actually_ independent of `y`, since analytically  So technically, the first derivative w.r.t. y should be zero.  Except that we don't do analytical math, so we end up with a `NaN * 0 = NaN`. For the second derivative, we have special handling inside the reciprocal gradient (reciprocal being the gradient of log) that avoids generating NaNs when passing zero gradients through the singularity at 0.  This is where we are returning 0, and where something like `sin` does not.  I _can_ make that consistent to return NaN.  But really, your gradients should technically be zero.",Are you satisfied with the resolution of your issue? Yes No
660,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(unable to build Tf lite .whl file for raspberry pi 0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf lite  Custom Code No  OS Platform and Distribution Ubuntu 20.04.5 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Athuliva,unable to build Tf lite .whl file for raspberry pi 0,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf lite  Custom Code No  OS Platform and Distribution Ubuntu 20.04.5 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-28T18:25:05Z,stat:awaiting response type:build/install stale comp:lite comp:micro TF 2.11,closed,2,10,https://github.com/tensorflow/tensorflow/issues/59037,Hi  ! Could you check the document specific  to linux runtime like raspberry pi. Thank you!,"the os architecture of the raspberry pi 0 that I tried is `armv6l`, I am trying to build the .whl file from my laptop (ubuntu)",Ok  ! Thanks for the update. Could you check this CMake documentation for Armv6.  Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you., I followed CMake documentation for Armv6..  Running the cmake generated some files i am not sure what to do next. Is it possible to generate .whl file from those generated files?,  ! Thanks for the update on armv6 wheels.    You can check instructions in Tensorlfow/Tflitemicro for raspberry pi installations as an alternate .  Thank you! ,Building the whl for armv6 should be fixed with 63c028fe7ba4d82c0ad9156533acaddaad377049 and was broken for some time due to an outdated toolchain.  you can sync to HEAD and run the following command: ,"Hi,   Apologize for the delayed response and could please refer above comment and please use below command for Build TensorFlow Lite Python Wheel Package and check whether your issue is resolving or not ? Also please refer our official documentation Ref1, Ref2 which may help you to resolve your issue `make C tensorflow/lite/tools/pip_package dockerbuild TENSORFLOW_TARGET=rpi0` If issue still persists please let us know or Could you please confirm if this issue is resolved for you ? Please feel free to close the issue if it is resolved ? Thank you!",This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1635,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Earlystopping 'auto' mode is misleading and errorprone)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Trying to use the EarlyStopping callback, the documentation states for the 'auto' mode: > mode: One of `{""auto"", ""min"", ""max""}`. In `min` mode,         training will stop when the quantity         monitored has stopped decreasing; in `""max""`         mode it will stop when the quantity         monitored has stopped increasing; in `""auto""`         mode, the direction is automatically inferred         from the name of the monitored quantity. In fact the tensorflow code is just:  This means that for any metric that is basically not the accuracy the 'auto' mode will default to 'min', while a vast majority of performance metrics are designed to be increasing functions of performance. This documentation is extremely misleading, and given this behavior I think we can safely say that almost nothing is inferred from the monitored metric. In order to protect other users from this trap, I would advise to remove the 'auto' mode and its associated argument altogether.  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,qmarcou,Earlystopping 'auto' mode is misleading and errorprone,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Trying to use the EarlyStopping callback, the documentation states for the 'auto' mode: > mode: One of `{""auto"", ""min"", ""max""}`. In `min` mode,         training will stop when the quantity         monitored has stopped decreasing; in `""max""`         mode it will stop when the quantity         monitored has stopped increasing; in `""auto""`         mode, the direction is automatically inferred         from the name of the monitored quantity. In fact the tensorflow code is just:  This means that for any metric that is basically not the accuracy the 'auto' mode will default to 'min', while a vast majority of performance metrics are designed to be increasing functions of performance. This documentation is extremely misleading, and given this behavior I think we can safely say that almost nothing is inferred from the monitored metric. In order to protect other users from this trap, I would advise to remove the 'auto' mode and its associated argument altogether.  Standalone code to reproduce the issue   Relevant log output _No response_",2022-12-28T18:20:55Z,stat:awaiting response type:bug stale comp:keras TF 2.9,closed,1,4,https://github.com/tensorflow/tensorflow/issues/59036,Hi  ! Thanks for sharing your observation . Could you create a PR in  kerasteam/keras repo for review. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 .  Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
836,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([help wanted] tf.strings.operations raises TypeError: Value passed to parameter 'input' has DataType string not in list of allowed)ï¼Œ å†…å®¹æ˜¯ (I'm just trying to create a superficial layer to preprocess data inside models. Here is an my Layer:  But it keeps giving me: **`TypeError: Value passed to parameter 'input' has DataType string not in list of allowed values: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, float16, uint32, uint64, complex64, complex128, bool, variant`** *I know I can do this thing out of the model, but my main goal is to somehow do this inside the model; this way, I'll be able to maximize the GPU utilization.*)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,maifeeulasad,[help wanted] tf.strings.operations raises TypeError: Value passed to parameter 'input' has DataType string not in list of allowed,"I'm just trying to create a superficial layer to preprocess data inside models. Here is an my Layer:  But it keeps giving me: **`TypeError: Value passed to parameter 'input' has DataType string not in list of allowed values: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, float16, uint32, uint64, complex64, complex128, bool, variant`** *I know I can do this thing out of the model, but my main goal is to somehow do this inside the model; this way, I'll be able to maximize the GPU utilization.*",2022-12-28T17:48:06Z,stat:awaiting response stat:awaiting tensorflower type:feature stale comp:model TF 2.11,closed,0,17,https://github.com/tensorflow/tensorflow/issues/59035,  Could you please fill the issue template and provide complete code to replicate the issue reported here. Thank you!, this is not an issue; this is more like a 'coding help needed' thread. Any help is highly appreciated for version ` > tf2.6`,"Hi  , Iam not sure your exact requirement.Providing more context may be helpful for us to understand your requirement. AFAIK The error might be due to that `tf.keras.Input()` won't accept `string` dtypes. Could you please refer this alternative Tutorial related to text and let us know if it helps. Thankyou !",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Sometimes we have models trained with special kinds of data, let say for example:   cased   uncased And it's well known that cased models don't go well with an uncased dataset and vice versa. Now we can certainly preprocess. But I want to achieve this inside `tf.keras.Model`, as I may use different models parallelly. Changing the case is one thing, but I was looking forward to something more generic. Then we will be able to unlock the next level. This is why I'm looking for some approach where I can iterate `tf.string` and modify it.","Hi  , The requested feature may not be possible inside the Model.However your intention is to use GPU you can use the context of tf.device('GPU') like below where the code within this context runs under GPU. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Surely it's do able, as much as I can think. Here is my most updated code:  Let me explain what it does:   `tf.strings.split(inputs, sep=self.seperator)` simply splits the Tensor   Then we can map over it, like `tf.map_fn(mapp,splits)` (**This is where I needed help**)   `tf.strings.reduce_join(mapped, axis=2, separator=self.seperator)`, we join back the RaggedTensor into one for further Now I can lower case, or upper case, or some predefined function. But I was trying to make it pure generic, so that we could do anything. Like:   stemming   lemmatization Just think about the possibilities. Currently reading: https://github.com/tensorflow/tensorflow/blob/b2755d85dbf99a6d3cf6ffd92e358377e3c46652/tensorflow/python/ops/string_ops.py There is a layer called `Rescaling`, which is used quite a lot nowadays in CV problems. No one wants to preprocess and feed it to model; maybe the preprocessing function will have slight variation, and it will/may impact the outcome. So why don't we add it to the model? I'm also trying to move forward with this vision. *Not sure if it is the best approach or not.* Pinging "," , It seems some of the strings Ops not working inside models.For example `tf.strings.split()` not working with model whereas `tf.strings.lower()` working fine which might be due to difference in implementation for reasons unknown to me.Please refer to attached gist. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you., nice observation. ğŸ’¯  And I don't think that this issue should be marked as stalled and closed later on. This is a great feature to work on. ğŸ¤“ ," ,The issue escalated to concern developer as a potential feature request.Thanks!"," okay. ğŸº I'm available to participate in any brain storming session. And if you think I can implement, or contribute with team. Just let me know. Regards.",I like to bring you to my project wrist watch neural interface..,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1857,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFlite 2.11.0 C API no longer compiles due to missing dependency)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Android  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am using the approach described here to integrate C Api for tensorflow lite: https://www.tensorflow.org/lite/android/developmenttools_for_building_with_c_and_c According to the description I should do the following:  > Using this API is the recommended approach for developers using the NDK. Download the TensorFlow Lite AAR hosted at MavenCentral file, rename to tensorflowlite*.zip, and unzip it. You must include the four header files in the headers/tensorflow/lite/ and headers/tensorflow/lite/c/ folders and the relevant libtensorflowlite_jni.so dynamic library in the jni/ folder in your NDK project. >  > The c_api.h header file contains basic documentation about using the TFLite C API. This approach works for any Tflite version prior to 2.11. When I try to upgrade to a v2.11 I get the following error:   This happens due to a recent changes that we introduced in v2.11 and replaced the content of the `tensorflow/lite/c/c_api.h` header file to the following:   The problem is that `tensorflow/lite/core/c/c_api.h` is not included in the aar file in Maven repo   Standalone code to reproduce the issue Follow the guidelines described in the TFLite C API and use the 2.11.0 version aar from the Maven repo  Error during build:    Relevant log output )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,alexanderar,TFlite 2.11.0 C API no longer compiles due to missing dependency,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Android  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am using the approach described here to integrate C Api for tensorflow lite: https://www.tensorflow.org/lite/android/developmenttools_for_building_with_c_and_c According to the description I should do the following:  > Using this API is the recommended approach for developers using the NDK. Download the TensorFlow Lite AAR hosted at MavenCentral file, rename to tensorflowlite*.zip, and unzip it. You must include the four header files in the headers/tensorflow/lite/ and headers/tensorflow/lite/c/ folders and the relevant libtensorflowlite_jni.so dynamic library in the jni/ folder in your NDK project. >  > The c_api.h header file contains basic documentation about using the TFLite C API. This approach works for any Tflite version prior to 2.11. When I try to upgrade to a v2.11 I get the following error:   This happens due to a recent changes that we introduced in v2.11 and replaced the content of the `tensorflow/lite/c/c_api.h` header file to the following:   The problem is that `tensorflow/lite/core/c/c_api.h` is not included in the aar file in Maven repo   Standalone code to reproduce the issue Follow the guidelines described in the TFLite C API and use the 2.11.0 version aar from the Maven repo  Error during build:    Relevant log output ",2022-12-27T13:29:50Z,stat:awaiting response type:bug comp:lite TF 2.11,closed,0,7,https://github.com/tensorflow/tensorflow/issues/59026,Hi  ! Thanks for reporting this bug.  ! Could you look at this issue. Attached gist from 2.11 wheel for reference. Thank you!,"I can confirm that this bug report is valid, i.e. it does describe a real bug. Apologies for the breakage. The right solution here is to include the headers from the tensorflow/lite/core/c directory in the AAR file. We should also add a test in the release process to prevent similar regressions in future.",This should now be fixed on the master branch.  We will also try to cherry pick the fix to the 2.12 release branch.,"Hi   As this comment suggests, the issue is fixed by commit https://github.com/tensorflow/tensorflow/commit/b558a557fef2f88bca8bde5e628b437daef1b2eb with following lines https://github.com/tensorflow/tensorflow/blob/d577af9cac504776a2d0ddbb0a445ba311aa1fea/tensorflow/lite/java/BUILDL147L150 Could you please confirm and close the issue if it is resolved? Thanks.",Was the fix finally cherry picked in 2.12?,"Hi tgl  The fix has been cherry picked in 2.12 with the PR CC(r2.12 cherrypick: 30bf24f83ee ""Fix bug where AAR file wasn't including headers from lite/core/c/""). Thanks.",Are you satisfied with the resolution of your issue? Yes No
1639,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support for keepdims and padding in tf.boolean_mask or tf.ragged.boolean_mask)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Given two example tensors `input` and `mask`:  I need to mask out `input` according to `mask` where values are 0. However, since the number of masked out elements for each example in the batch `input` might be different, to keep the output a valid tensor, the output should be:  i.e. in the output, the masked input keeps only elements where `mask` is 1, and, with zeropadding at the end to ensure that the output is a valid tensor. I've searched around and tried using: 1. `tf.gather`, however, still can't figure out how to proceed. 2. `tf.boolean_mask`, however, it doesn't support masking but just drops the first (zeroth) dimension, as shown below:  3. `tf.ragged.boolean_mask`, this is by far the closest one to what I want, it keeps the dimension, however, still doesn't support masking, so the result is a ragged tensor... Similar issues are mentioned in GitHub: https://github.com/tensorflow/tensorflow/issues/18238 In short:   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jamie0725,Support for keepdims and padding in tf.boolean_mask or tf.ragged.boolean_mask,"Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Given two example tensors `input` and `mask`:  I need to mask out `input` according to `mask` where values are 0. However, since the number of masked out elements for each example in the batch `input` might be different, to keep the output a valid tensor, the output should be:  i.e. in the output, the masked input keeps only elements where `mask` is 1, and, with zeropadding at the end to ensure that the output is a valid tensor. I've searched around and tried using: 1. `tf.gather`, however, still can't figure out how to proceed. 2. `tf.boolean_mask`, however, it doesn't support masking but just drops the first (zeroth) dimension, as shown below:  3. `tf.ragged.boolean_mask`, this is by far the closest one to what I want, it keeps the dimension, however, still doesn't support masking, so the result is a ragged tensor... Similar issues are mentioned in GitHub: https://github.com/tensorflow/tensorflow/issues/18238 In short:   Standalone code to reproduce the issue   Relevant log output _No response_",2022-12-27T09:55:14Z,type:feature comp:ops,closed,1,0,https://github.com/tensorflow/tensorflow/issues/59024
1908,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NVIDIA XLA] large GPU memory overhead observed due to seems un-necessary copy operators introduced by XLA )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf r2.11  Custom Code No  OS Platform and Distribution Linux ipp11316 5.4.0125generic CC(typo in decaying the learning rate example)Ubuntu SMP Wed Aug 10 13:42:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux  Mobile device _No response_  Python version 3.8  Bazel version 5.30  GCC/Compiler version gcc  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to debug an OOM issue for LLM model, seems that one possible cause of memory consumption is from unnecessary copy operators in XLA. From the HLO graph, we observed that the input buffer is aliased to output buffer, so it is donated to XLA.  When entry computation uses the input buffer, it first creates a copy of the input buffer, and use the copied buffer inside computation, but the original input buffer is never used further.  Attaching the HLO graph for it.  Here, you can see that parameter(4) of ENTRY computation is aliased. {4}: (4, {}, mayalias), but inside ENTRY computation:   i.e, input buffer is first copied, and later operators are using the copy.339, while Arg_4.5 is never used again.  Why copy.339 is introduced?  Buffer donation is enabled for this case, and should the copy not exist? We observed a lot of similar cases for other input parameters, due to this reason, the copy could cause additional several GB of memory waste, we really want to optimize for this case.  Standalone code to reproduce the issue I upload all the necessary HLO dumps required for debug here:  https://drive.google.com/drive/folders/1wiOtUZVAX)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shawnwang18,[NVIDIA XLA] large GPU memory overhead observed due to seems un-necessary copy operators introduced by XLA ,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf r2.11  Custom Code No  OS Platform and Distribution Linux ipp11316 5.4.0125generic CC(typo in decaying the learning rate example)Ubuntu SMP Wed Aug 10 13:42:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux  Mobile device _No response_  Python version 3.8  Bazel version 5.30  GCC/Compiler version gcc  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to debug an OOM issue for LLM model, seems that one possible cause of memory consumption is from unnecessary copy operators in XLA. From the HLO graph, we observed that the input buffer is aliased to output buffer, so it is donated to XLA.  When entry computation uses the input buffer, it first creates a copy of the input buffer, and use the copied buffer inside computation, but the original input buffer is never used further.  Attaching the HLO graph for it.  Here, you can see that parameter(4) of ENTRY computation is aliased. {4}: (4, {}, mayalias), but inside ENTRY computation:   i.e, input buffer is first copied, and later operators are using the copy.339, while Arg_4.5 is never used again.  Why copy.339 is introduced?  Buffer donation is enabled for this case, and should the copy not exist? We observed a lot of similar cases for other input parameters, due to this reason, the copy could cause additional several GB of memory waste, we really want to optimize for this case.  Standalone code to reproduce the issue I upload all the necessary HLO dumps required for debug here:  https://drive.google.com/drive/folders/1wiOtUZVAX",2022-12-27T06:42:18Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:xla TF 2.11,closed,0,6,https://github.com/tensorflow/tensorflow/issues/59022,Tracked as b/270342146,Regarding Arg_4.5: it is used inside a while loop. This requires a copy: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/copy_insertion.hL35 I suspect the other copies are needed for similar reasons.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1210,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(error: 'tf.Conv2D' op is neither a custom op nor a flex op)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): v2.10  2. Code Code for conversion  Code for the model    3. Failure after conversion conversion fails  5. (optional) Any other info / logs error.log  According to the error message, I suspect that it can not recognize the input shape. But as you can see on the above code, input is specified for the functional API for `decoder` model.  (FYI, The inference code is called with `predict_on_batch` method. I found out other model with `predict_on_batch` is converted successfully, but that model doesn't contain `conv2d` block inside. Can using `predict_on_batch` together with `conv2d` be a problem?) **I'm sure `conv2d` is on the allowlist for TFLite operators. Any suggestions for this problem? Thank you.**)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kkimmk,error: 'tf.Conv2D' op is neither a custom op nor a flex op," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): v2.10  2. Code Code for conversion  Code for the model    3. Failure after conversion conversion fails  5. (optional) Any other info / logs error.log  According to the error message, I suspect that it can not recognize the input shape. But as you can see on the above code, input is specified for the functional API for `decoder` model.  (FYI, The inference code is called with `predict_on_batch` method. I found out other model with `predict_on_batch` is converted successfully, but that model doesn't contain `conv2d` block inside. Can using `predict_on_batch` together with `conv2d` be a problem?) **I'm sure `conv2d` is on the allowlist for TFLite operators. Any suggestions for this problem? Thank you.**",2022-12-26T09:33:04Z,stat:awaiting response type:support comp:lite TFLiteConverter TF 2.10,closed,0,9,https://github.com/tensorflow/tensorflow/issues/59017,Hi   I made a simple reproducer. ,Hi  ! Please go with select ops syntax as the model involves operators which are not supported in built in ops list yet. Attached resolved gist for reference. Thank you!,"Thank you for your suggestion! I have followup questions 1. The op being issued is `tf.Conv2D` which is the most used layer in any model and actually it's in the list. Is my reproducer not using it? If then, how can I check it? 2. Doc indicates that adding select ops might NOT be suitable when deploying the model on Google Play services. I'm planning to deploy it on Android, so I guess I can not use this gist for my case.","Hi   I resolved this issue. If I insert `latent = tf.reshape(latent, (1, n_h, n_w, 4))` after `latent = keras.layers.Input((n_h, n_w, 4))`, it works well. I guess it might be caused by reshaping _None_ sized tensor.", ! Thanks for confirmation.  I think  dynamic batch size was fixed recently in nightly (2.12dev) version . Could you check in 2.11 or nightly version and confirm the same. Thank you!,"Thank you for the update   I've run the initial code with _tfnightly 2.12.0.dev20221226_ and it finished successfully without an error! However when I compare the two results(tf2.10 with `tf.reshape(latent, (1, n_h, n_w, 4))` vs tfnightly without reshap), the output graph is different(tfnightly's graph is much more complicated) and output tensor for same input differs. ", ! Thank for the update. Can you create a separate issue for that and mark this one as resolved. Thank you,Are you satisfied with the resolution of your issue? Yes No,Are you satisfied with the resolution of your issue? Yes No
1889,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with CUDA library directory not being detected properly by Tensorflow Ubuntu 22.04)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  20221225 20:02:49.726066: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20221225 20:02:50.208855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory 20221225 20:02:50.208895: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory 20221225 20:02:50.208901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 20221225 20:02:50.833468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Srking501,Issue with CUDA library directory not being detected properly by Tensorflow Ubuntu 22.04,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  20221225 20:02:49.726066: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20221225 20:02:50.208855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory 20221225 20:02:50.208895: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory 20221225 20:02:50.208901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 20221225 20:02:50.833468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS",2022-12-25T20:23:45Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 2.11,closed,0,11,https://github.com/tensorflow/tensorflow/issues/59013,"I had a similar problem the last few days following the instructions at https://www.tensorflow.org/install/pip, for Ubuntu Linux on Windows 11 WSL2. After looking through several similar situations it seemed that the XLA_FLAGS environment variable needed to be updated with the location of the cuda installation. It seemed to me that something went wrong with the miniconda tensorflow installation, I couldn't even find the cuda files. So I installed it without miniconda: `sudo apt install nvidiacudatoolkit` but it still didn't work until I updated the ~/.profile file with `export XLA_FLAGS=xla_gpu_cuda_data_dir=/usr/lib/cuda/`, which is where the installation ended up, and restarted the session. Then everything worked fine. I'm not sure if it was just me and the process of fiddling trying to fix it but I made a few other changes to my .profile file that may have contributed, so here it is in full. This Ubuntu Linux instance was only setup to run a local session of Google Colab with tensorflow right now. ","Hi  , I have tried to replicate the issue you mentioned in a VM with Ubuntu 20.04, 2 x NVIDIA A100 40GB GPUs, x86/64 Architecture and with Venv of TF2.11.Initially i got the same error as mentioned by you.Then i tried `whereis cuda` command and it returns location `cuda: /local/usr/cuda` which is contrast. Then i proceeded with commands as per below attached photographs and the Error is gone. Even though my VM has Ubuntu 20.04 but the Error is same as you mentioned which was  mentioned by you.For me the error was gone and starts working fine when i installed nvidiacudatoolkit in venv.      Could you please refer above details and comeback with your views ?","Tensorflow requires the use of the CUDA library toolkit to be able to utilise the Nvidia's GPUs (`nvidiacudatoolkit`)  also requires the use of Nvidia CUDNN library (`nvidiacudnn`) to run the model under the Tensorflow beginner tutorial `Last updated 20221223 UTC`, at least in Ubuntu 22.04 desktop (`nvidiacudnn`). So that seems to be right.  However, I have noticed that the Ubuntu Official Repository of `nvidiacudatoolkit` installs the CUDA library in `/usr/lib/cuda` whereas if you use Nvidia's Official Repository of `cuda` for Ubuntu and install it through `sudo aptget install cuda` (and to include all GDS packages `sudo aptget install nvidiagds`) the CUDA library will end up in `/usr/local/cuda`.","Hi  , Thanks for confirmation.It seems the behaviour in VM has been different.I will also double check VM behaviour meanwhile with fresh installation.  I assume the behaviour mentioned by you is in Local Linux desktop right? So in your case symlink using `sudo ln s /usr/lib/cuda/ /usr/local/cuda` alone worked for your case ?Whether XLA compilation also works with it or it needs additional `export XLA_FLAGS=xla_gpu_cuda_data_dir=/usr/lib/cuda/` setting in `~/.profile `.Could you please test and confirm the same so that it can taken care in documentation or if possible in build config also. Thank you!","Glad for all the help and replies! I would test the following and get back to you. But first, may I ask how do I do XLA compilation?","Hi  , XLA compilation can be done by the setting `jit_compile=True` or `jit_compile='autoclustering'`. For keras models you can pass this argument during `model.compile()` and for custom training loops you can pass it to `tf.function()`.Please note that not all Ops are compatible to XLA. We have also option of autoclustering which automatically does jit_compilation if the Op is available for jit_compile but this is also having limitations and also kind of unpredictable w.r.t behaviour and performance. Please refer to attached resource here and subsequent pages for details about XLA and how to implement it in the code. You may test with minimal code only that might be available  from the tutorials. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Sorry for the delay, I have still have not tested to what you stated. I was really busy at the time; I will hopefully reply within 2 Days  if further delays happen, I will notify this issue about it.","Hello , I've tried to make `XLA_FLAGS=xla_gpu_cuda_data_dir=/usr/lib/cuda/` in `~/.profile` but always breaks for me, I still do not why it is breaking (maybe try it in your side by spinning a Ubuntu Desktop Image). But at least, the `sudo ln s /usr/lib/cuda/ /usr/local/cuda` seemed to be working fine with no issues. I deeply apologise for the delay, I had really busy 2 weeks. Kindest Regards and Thank you for taking the issue.","Hello  , I have tested on VM with Ubuntu 22.04 and the installation done at usr/local/cuda12.0 as attached below.  Then i tested the code referred by you and it's executing fine and GPU also detected fine.  I learnt that some Local Ubuntu OS coming with pre installed Nvidia drivers.May be the difference may be due to the way CUDA drivers being installed.On the VM I tested it installs the driver/cuda toolkit as expected.We can get the location of CUDA using `whereis cuda` and if it could not found at `usr/local/cuda` then sym link as confirmed by you will make it works. Thanks for sharing your observations and discussions happened in the thread.Shall we close the issue if the purpose resolved. Thank you!",Are you satisfied with the resolution of your issue? Yes No
1878,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Crash when running tensorflow.python.ops.gen_collective_ops.collective_gather)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output ```shell 20221225 14:28:42.276547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 20221225 14:28:43.989383: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set 20221225 14:28:43.994221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1 20221225 14:28:44.069864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero 20221225 14:28:44.070300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:  pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5 coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s 20221225 14:28:44.070361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 20221225 14:28:44.086951: I tensorflow/stream_executor/platform/default/dso_load)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cse19922021,Crash when running tensorflow.python.ops.gen_collective_ops.collective_gather,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output ```shell 20221225 14:28:42.276547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 20221225 14:28:43.989383: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set 20221225 14:28:43.994221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1 20221225 14:28:44.069864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero 20221225 14:28:44.070300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:  pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5 coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s 20221225 14:28:44.070361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0 20221225 14:28:44.086951: I tensorflow/stream_executor/platform/default/dso_load",2022-12-25T19:33:32Z,stat:awaiting response type:bug comp:ops TF 2.4,closed,0,3,https://github.com/tensorflow/tensorflow/issues/59008,"Hi  ! Thanks for sharing your observation w.r.to 2.4. I could not replicate this issue in 2.11.  Attached gist in 2.10, 2.11 and nightly for reference. Thank you!","This seems invalid. Affected version is old, out of support.",Are you satisfied with the resolution of your issue? Yes No
718,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Crash when running tensorflow.python.ops.ragged.ragged_string_ops.ngrams)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,cse19922021,Crash when running tensorflow.python.ops.ragged.ragged_string_ops.ngrams,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-12-25T19:23:39Z,stat:awaiting response type:bug comp:ops TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/59007,"  I was able to execute the given code without a session crash or Segmentation fault (core dumped) on Colab using TF v2.11. Could you please try with the latest version which is TF v2.11 and also v2.4 is pretty old, There is a high possibility that this was fixed with later TF versions. Please find the gist here for reference and let us know if it helps. Thank you!",">  I was able to execute the given code without a session crash or Segmentation fault (core dumped) on Colab using TF v2.11. Could you please try with the latest version which is TF v2.11 and also v2.4 is pretty old, There is a high possibility that this was fixed with later TF versions. Please find the gist here for reference and let us know if it helps. Thank you! This issue is for 2.4. only.",This seems invalid. Log message does not show claimed crash. Affected version is old.,Are you satisfied with the resolution of your issue? Yes No
719,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Crash when running tensorflow.python.ops.gen_image_ops.extract_glimpse_v2)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cse19922021,Crash when running tensorflow.python.ops.gen_image_ops.extract_glimpse_v2,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-12-25T17:32:48Z,stat:awaiting response type:bug comp:ops TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/59003,", I was able to execute the code without crashing in the latest version of the tensorflow and it was executed with the error **Error:Expected a nonnegative size, got 3 [Op:ExtractGlimpseV2]** which was expected. Kindly find the gist of it here and also v2.4 is pretty old, There is a high possibility that this was fixed with later TF versions. Thank you!",">  Thanks for the reply. I'm running my fuzzer on tensorflow v.2.4.0 to find security vulnerabilities. I just double checked and got the following output:  I think if you run on 2.4, you will get the crash. Anyways, thanks.","Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work. Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md",Are you satisfied with the resolution of your issue? Yes No
1994,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NotImplementedError: Cannot convert a symbolic tf.Tensor (strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  MacOS (m1):  Conda:  TensorFlow library (version=2.10): MacOS  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1) get_concrete_function()   Error and Logs Traceback (most recent call last):   File ""/Users/thao/Private/TFLite/covert.py"", line 20, in      conc_func = gelu_tanh_activation.get_concrete_function()   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/def_function.py"", line 1239, in get_concrete_function     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/def_function.py"", line 1219, in _get_concrete_function_garbage_collected     self._initialize(args, kwargs, add_initializers_to=initializers)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/def_function.py"", line 785, in _initialize     self._stateful_fn._get_concrete_function_internal_garbage_collected(   pylint: disable=protectedaccess   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/function.py"", line 2523, in _get_concrete_function_internal_garbage_collected     graph_function, _ = self._maybe_define_function(args, kwargs)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/function.py"", line 2760, in _maybe_define_function     graph_function = self._create_graph_function(args, kwargs)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/function.py"", line 2)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,UnivexDont,"NotImplementedError: Cannot convert a symbolic tf.Tensor (strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported."," 1. System information  MacOS (m1):  Conda:  TensorFlow library (version=2.10): MacOS  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1) get_concrete_function()   Error and Logs Traceback (most recent call last):   File ""/Users/thao/Private/TFLite/covert.py"", line 20, in      conc_func = gelu_tanh_activation.get_concrete_function()   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/def_function.py"", line 1239, in get_concrete_function     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/def_function.py"", line 1219, in _get_concrete_function_garbage_collected     self._initialize(args, kwargs, add_initializers_to=initializers)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/def_function.py"", line 785, in _initialize     self._stateful_fn._get_concrete_function_internal_garbage_collected(   pylint: disable=protectedaccess   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/function.py"", line 2523, in _get_concrete_function_internal_garbage_collected     graph_function, _ = self._maybe_define_function(args, kwargs)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/function.py"", line 2760, in _maybe_define_function     graph_function = self._create_graph_function(args, kwargs)   File ""/Users/thao/miniforge3/lib/python3.10/sitepackages/tensorflow/python/eager/function.py"", line 2",2022-12-24T09:50:23Z,stat:awaiting response stale comp:lite TFLiteConverter TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58993,  I was able to reproduce the issue on Colab using  TF v2.11. Please find the gist here for reference. Thank you!,>  I was able to reproduce the issue on Colab using TF v2.11. Please find the gist here for reference. Thank you! , Mixing numpy ops and tf ops is not suggested. The workaround is to return `xFeatures` a list instead of converting into numpy array. Code  Let us know if that helps. Thank you.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
611,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Missing zip file)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version not available, see ""Current Behaviour""  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device   Python version   Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 12  GPU model and memory NVidia GForce  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,RolWinde,Missing zip file,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version not available, see ""Current Behaviour""  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device   Python version   Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 12  GPU model and memory NVidia GForce  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-12-23T21:51:29Z,type:build/install subtype:windows TF 2.11,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58991,Hi  ! Could you try now. I can download through above zip link. Attached gist for reference. Thank you!,"Thank you Manas With that attached link I was able to download. best regards RolWinde Von: Manas Mohanty ***@***.***>  Gesendet: Sonntag, 25. Dezember 2022 04:03 An: tensorflow/tensorflow ***@***.***> Cc: RolWinde ***@***.***>; Mention ***@***.***> Betreff: Re: [tensorflow/tensorflow] Missing zip file (Issue CC(Missing zip file)) Hi    ! Could you try now. I can download through above zip link. Attached gist   for reference. Thank you! â€” Reply to this email directly, view it on GitHub  , or unsubscribe  . You are receiving this because you were mentioned.   Message ID: ***@***.*** ***@***.***> >",Ok  ! Thanks for the confirmation. Marking this issue as resolved then.  Thank you!,Are you satisfied with the resolution of your issue? Yes No,Hi  ! Thanks for bringing out the breakage of windows GPU wheel.  You can put your comments in CC(Window only link) as reopening this issue be duplicating the effort. Thank you!
1860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NNAPI initialization from cache has hardly any time benefit)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.10.1  Custom Code Yes  OS Platform and Distribution Android 12  Mobile device Pixel 3a  Python version N/A  Bazel version cmake 3.16.3  GCC/Compiler version NDK r19c  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to use compilation caching with the NNAPI delegate to reduce startup time. To test this, I have written a small program that loads a tflite model from disk, applies the NNAPI delegate and runs invoke once.  The NNAPI delegate is provided with the necessary settings (`model_token` and `cache_dir`), and indeed on first invocation I as expected receive:  After this first execution, the relevant file was created and subsequent executions did indeed no longer show the cache miss error. The problem is that there are no significant time savings. I ran the test binary produced with the steps below on my Pixel3a and crudely measured the first execution takes 2.62 seconds while the second one takes 2.49 seconds (see logs). For reference, the same execution takes only 0.29 seconds when not applying the NNAPI delegate (again see logs). I have tried this with a few different models, but results were always similar. Note that this same setup works fine with the GPU delegate, where the first execution takes a couple seconds and consecutive executions are drastically faster. Is this expected behavior with the NNAPI delegate, possibly a bug, or is there something wrong with how I am trying to use compilation caching?  Standalone code to reproduce the issue ```c++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,derpda,NNAPI initialization from cache has hardly any time benefit,"Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.10.1  Custom Code Yes  OS Platform and Distribution Android 12  Mobile device Pixel 3a  Python version N/A  Bazel version cmake 3.16.3  GCC/Compiler version NDK r19c  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to use compilation caching with the NNAPI delegate to reduce startup time. To test this, I have written a small program that loads a tflite model from disk, applies the NNAPI delegate and runs invoke once.  The NNAPI delegate is provided with the necessary settings (`model_token` and `cache_dir`), and indeed on first invocation I as expected receive:  After this first execution, the relevant file was created and subsequent executions did indeed no longer show the cache miss error. The problem is that there are no significant time savings. I ran the test binary produced with the steps below on my Pixel3a and crudely measured the first execution takes 2.62 seconds while the second one takes 2.49 seconds (see logs). For reference, the same execution takes only 0.29 seconds when not applying the NNAPI delegate (again see logs). I have tried this with a few different models, but results were always similar. Note that this same setup works fine with the GPU delegate, where the first execution takes a couple seconds and consecutive executions are drastically faster. Is this expected behavior with the NNAPI delegate, possibly a bug, or is there something wrong with how I am trying to use compilation caching?  Standalone code to reproduce the issue ```c++",2022-12-23T16:50:57Z,comp:lite type:performance TFLiteNNAPIDelegate TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58990," Thanks for reporting the issue. The NNAPI compilation caching behavior depends on the capability of the underlying driver. Based on the provided device info (Pixel 3a Android 12), it is likely that the underlying qtigpu driver does not implement compilation caching support.","Thanks for responding to this! I'll try it on other devices and see if I get any benefit. Let me just note that I am using qtidefault on the Pixel 3a (as in, I don't specify the accelerator name), which I think ends up using the qtidsp driver. To check this, I tried specifying qtigpu and there are no log messages regarding cache usage. Startup is very fast (suspiciously fast...), leading me to believe that the delegate isn't correctly applied. The CPUlike performance supports that as well. Specifying qtidsp yields the slow startup but fast inference I laid out in this issue. For the GPU delegate (not through NnApi), compilation provides significant benefit and works fine.","Thanks for the additional info!  Sorry for the confusion. I originally thought you were using qtigpu because you mentioned GPU delegate as well, but it looks like it is actually using qtidsp. (qtigpu is for floating point models and qtidsp is for quantized models) qtidsp should have some level of compilation caching support with sufficient driver version, but pixel 3a may not be a supported device variant. Could you inspect the android logcat to see if there is any useful information? On Android 12, the following logcat message (when compiling the second time) may indicate compilation caching is not supported:  Starting from Android 13, you can expect the following logcat when compilation caching is supported: ","In my actual app I do indeed see   on the second startup. On the first I get a cachemiss as expected. I failed to reproduce that particular message in the code I posted for this issue and couldn't figure out why, so I left it at that since the startup time issue was reliably reproduced. I just borrowed a Pixel 6 Pro from a coworker and checked the logs. I indeed see  during the second startup and it is much faster than the first startup (where I get the obvious cache miss), so this confirms what you said. I think we can close this issue, thank you!"
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version 2.10.1  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1..33  GPU model and memory NVIDIA QUADRO P620 / 4096 MB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Anil-io,Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0,Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version 2.10.1  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1..33  GPU model and memory NVIDIA QUADRO P620 / 4096 MB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-22T13:07:58Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58986,"io, Tensorflow v2.10 is compatible with cuDNN 8.1.0 and CUDA 11.2. Could you please try with the mentioned compatible version from here and release notes for the reference.  Also please check at this path ""Control Panel >System and Security>System>Advanced System settings"" , and the link as well. Thank you!", Thank you for your response. I had to reinstall CUDA toolkit v11.8 and cuDNN v.8.6 and that has solved the issue. the mentioned compatible versions in tensorflow website didn't work for me. ,"io, I tried to follow the tested build configurations and install the tensorflow v2.10 as mentioned here and it was installed without any issues. The tested build configurations which are mentioned are tested on various platforms with the compatible CUDA, compiler and cuDNN and it will be successfully installed.  !Screenshot (105) Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
695,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Windows C API package is missing files)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version libtensorflowcpuwindowsx86_642.11.0.zip  Custom Code No  OS Platform and Distribution MS Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AlexSnail124,Windows C API package is missing files,Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version libtensorflowcpuwindowsx86_642.11.0.zip  Custom Code No  OS Platform and Distribution MS Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-12-22T12:12:53Z,stat:awaiting response type:build/install stale subtype:windows TF 2.11,closed,2,6,https://github.com/tensorflow/tensorflow/issues/58985,Hi  ! Thanks for work on  windows c wheel.  Could your raise a PR for review.  ! Could you look at this issue. Thank you!,"Hi   I think you're suggesting I submit a change for review to fix this issue. I had originally thought I was just submitting a ticket for someone else to fix (maybe you). I'm happy to give it a go at fixing it though. I am a professional software engineer but am new to github and open source contribution. Can you point me to the right area that controls how the build packages are put together? The more specific the better. And I'll work out the rest. Thanks Alex On Fri, 23 Dec 2022, 02:29 Manas Mohanty, ***@***.***> wrote: > Hi   ! > > Thanks for work on windows c wheel. > Could your raise a PR for the review. > > Thank you! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Hi, Do you think this could be due to the changes made here https://github.com/tensorflow/tensorflow/commit/843cede83294063f88bf17097c2a446db70d9617, changes mainly reflecting on the header file `tf_buffer.h`. I can also see buffer file which is created at the location tensorflow/c/tf_buffer.h. Let me know if you have any further question s on the dependency to work with. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
663,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(pip subprocess to install build dependencies did not run successfully)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version Tf 2.3  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.9.7  Bazel version 3.1.0  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4/8  GPU model and memory NVIDIA RTX 3060 Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lakpa-tamang9,pip subprocess to install build dependencies did not run successfully,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version Tf 2.3  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.9.7  Bazel version 3.1.0  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4/8  GPU model and memory NVIDIA RTX 3060 Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-22T05:34:31Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.3,closed,18,4,https://github.com/tensorflow/tensorflow/issues/58982,"tamang9,  Its unlikely for TF 2.3 version to receive any bug fixes except when we have security patches. There is a high possibility that this was fixed with later TF versions. Perhaps you can use the latest tf versions for your case. Also please take a look at this official doc link for the reference. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"TF 2.3 was released before Python 3.9. Dependencies it contains are also not able to work with Python 3.9 On the other hand, you are compiling from source and if you look at the error message closely this is what is happening to `scipy` too, pip tries to compile it from source. You could succeed if you ensure you have the needed environment to do that, but this is no longer a TF issue. Hence, closing.",Are you satisfied with the resolution of your issue? Yes No
932,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Installing sktime all_extras got compatible issues with tensorflow)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version M1 macbook metal tf2.5  Custom Code Yes  OS Platform and Distribution Mac OS 13.1  Mobile device M1 macbook 2020  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory apple metal  Current Behaviour?   Standalone code to reproduce the issue ```shell I am trying to install sktime with all soft dependencies including tensorflow, when I do developer install ""pip install e .""[all_extras,dev]"" I got this error: ""ERROR: Ignored the following versions that require a different python version: 0.52.0 RequiresPython >=3.6,=3.6,)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,xxl4tomxu98,Installing sktime all_extras got compatible issues with tensorflow,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version M1 macbook metal tf2.5  Custom Code Yes  OS Platform and Distribution Mac OS 13.1  Mobile device M1 macbook 2020  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory apple metal  Current Behaviour?   Standalone code to reproduce the issue ```shell I am trying to install sktime with all soft dependencies including tensorflow, when I do developer install ""pip install e .""[all_extras,dev]"" I got this error: ""ERROR: Ignored the following versions that require a different python version: 0.52.0 RequiresPython >=3.6,=3.6,",2022-12-21T16:14:06Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.5,closed,0,12,https://github.com/tensorflow/tensorflow/issues/58975,", Could you please try to install the Python version which is **>=3.6 and <3.9** and let us know if you are facing the same issue. Also we recommend to install the latest stable version where there is a high possibility that this was fixed. Thank you!","The following is what I got when I created python 3.8 version using ""conda create n sktimedev python=3.8"", and then ""conda activate sktimedev"", and then ""pip install e .[all_extras,dev]"" and the error is as following: extra options: 'faltivec I/System/Library/Frameworks/vecLib.framework/Headers'             clang: numpy/core/src/multiarray/array_assign_scalar.c             clang: numpy/core/src/multiarray/alloc.c             clang: numpy/core/src/multiarray/buffer.c             clang: numpy/core/src/multiarray/conversion_utils.c             clang: numpy/core/src/multiarray/datetime_strings.c             clang: numpy/core/src/multiarray/descriptor.c             clang: numpy/core/src/multiarray/common.c             clang: build/src.macosx11.0arm643.8/numpy/core/src/multiarray/einsum.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/multiarray/hashdescr.c             clang: build/src.macosx11.0arm643.8/numpy/core/src/multiarray/lowlevel_strided_loops.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/multiarray/multiarraymodule.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/multiarray/nditer_constr.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/multiarray/refcount.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/multiarray/scalarapi.c             clang: numpy/core/src/multiarray/temp_elide.c             clang: numpy/core/src/multiarray/vdot.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: build/src.macosx11.0arm643.8/numpy/core/src/umath/loops.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/umath/ufunc_object.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/umath/ufunc_type_resolution.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/common/array_assign.c             clang: build/src.macosx11.0arm643.8/numpy/core/src/npymath/ieee754.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: numpy/core/src/common/ucsnarrow.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: build/src.macosx11.0arm643.8/numpy/core/src/common/npy_cpu_features.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: /private/var/folders/50/zx6p2vtx3h9bvdft7b3h85sm0000gn/T/pipinstall8r_vptpw/numpy_99bc95ff8ac74139aa6de758ed12d4b3/numpy/_build_utils/src/apple_sgemv_fix.c             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             clang: error: the clang compiler does not support 'faltivec', please use maltivec and include altivec.h explicitly             Running from numpy source directory.             /private/var/folders/50/zx6p2vtx3h9bvdft7b3h85sm0000gn/T/pipbuildenvn_5x0ru4/overlay/lib/python3.8/sitepackages/setuptools/_distutils/dist.py:265: UserWarning: Unknown distribution option: 'define_macros'               warnings.warn(msg)             error: Command ""clang Wnounusedresult Wsigncompare Wunreachablecode DNDEBUG fwrapv O2 Wall fPIC O2 isystem /Users/tomxu/miniforge3/envs/sktimedev/include arch arm64 fPIC O2 isystem /Users/tomxu/miniforge3/envs/sktimedev/include arch arm64 DNPY_INTERNAL_BUILD=1 DHAVE_NPY_CONFIG_H=1 D_FILE_OFFSET_BITS=64 D_LARGEFILE_SOURCE=1 D_LARGEFILE64_SOURCE=1 DNO_ATLAS_INFO=3 DHAVE_CBLAS Ibuild/src.macosx11.0arm643.8/numpy/core/src/umath Ibuild/src.macosx11.0arm643.8/numpy/core/src/npymath Ibuild/src.macosx11.0arm643.8/numpy/core/src/common Inumpy/core/include Ibuild/src.macosx11.0arm643.8/numpy/core/include/numpy Inumpy/core/src/common Inumpy/core/src Inumpy/core Inumpy/core/src/npymath Inumpy/core/src/multiarray Inumpy/core/src/umath Inumpy/core/src/npysort I/Users/tomxu/miniforge3/envs/sktimedev/include/python3.8 Ibuild/src.macosx11.0arm643.8/numpy/core/src/common Ibuild/src.macosx11.0arm643.8/numpy/core/src/npymath c numpy/core/src/multiarray/alloc.c o build/temp.macosx11.0arm64cpython38/numpy/core/src/multiarray/alloc.o MMD MF build/temp.macosx11.0arm64cpython38/numpy/core/src/multiarray/alloc.o.d faltivec I/System/Library/Frameworks/vecLib.framework/Headers"" failed with exit status 1             [end of output]         note: This error originates from a subprocess, and is likely not a problem with pip.         ERROR: Failed building wheel for numpy       Failed to build numpy       ERROR: Could not build wheels for numpy, which is required to install pyproject.tomlbased projects       [end of output]   note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocessexitedwitherror Ã— pip subprocess to install build dependencies did not run successfully. â”‚ exit code: 1 â•°â”€> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip.","Question, since all of my setup is python3.9, creating python3.8 seems to be awkward. is there a way to do this with python3.9 with all_extras?","Hi  , For latest Versions of Tensorflow, let's say if you use TF2.11 then Python versions 3.73.10 are supported as per source.Kindly upgrade to TF 2.10v or 2.11v to keep your Python environment same.Also please refer to Developer comment regarding older versions related."," , TF2.11 available in Mac RAM M1 format?, can you please provide a link for instruction to update from TF2.5 to TF2.11 in ARM? Furthermore, command: ""pip install e .[all_extras,dev]"" should install the most recent version pop TF2.11, is it true? why do I have to separately update TF version, the above command should install soft dependencies for me, correct?"," , I also tried to do  ""pip install e .[dev]"" first with success and then separately install most recent TF2.11 and other related dependencies, it allows me to import TF2.11, but when it is used to fit and predict, it gives this error: Metal device set to: Apple M1 systemMemory: 16.00 GB maxCacheSize: 5.33 GB Model: ""model"" _________________________________________________________________  Layer (type)                Output Shape              Param     =================================================================  input_1 (InputLayer)        [(None, 100, 6)]          0           conv1d (Conv1D)             (None, 94, 6)             258         average_pooling1d (AverageP  (None, 31, 6)            0           ooling1D)                                                         conv1d_1 (Conv1D)           (None, 25, 12)            516         average_pooling1d_1 (Averag  (None, 8, 12)            0           ePooling1D)                                                       flatten (Flatten)           (None, 96)                0           dense (Dense)               (None, 4)                 388        ================================================================= Total params: 1,162 Trainable params: 1,162 Nontrainable params: 0 _________________________________________________________________ Epoch 1/200 /Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.   warnings.warn( 20221229 12:04:05.465251: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 20221229 12:04:05.465271: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) 20221229 12:04:05.622791: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz 20221229 12:04:05.851814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled. 20221229 12:04:05.993145: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x13f038990 20221229 12:04:05.993172: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x13f038990 20221229 12:04:06.003877: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x13f038990 20221229 12:04:06.005433: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x13f038990 20221229 12:04:06.009941: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x13f038990 20221229 12:04:06.009960: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x13f038990 Output exceeds the size limit. Open the full output data in a text editor  NotFoundError                             Traceback (most recent call last) Cell In[4], line 2       1 network = CNNClassifier(n_epochs=200, verbose=True) > 2 network.fit(X_train, y_train)       3 network.score(X_test, y_test) File ~/Documents/sktime/sktime/classification/base.py:191, in BaseClassifier.fit(self, X, y)     186         raise AttributeError(     187             ""self.n_jobs must be set if capability:multithreading is True""     188         )     190  pass coerced and checked data to inner _fit > 191 self._fit(X, y)     192 self.fit_time_ = int(round(time.time() * 1000))  start     194  this should happen last File ~/Documents/sktime/sktime/classification/deep_learning/cnn.py:181, in CNNClassifier._fit(self, X, y)     179 if self.verbose:     180     self.model_.summary() > 181 self.history = self.model_.fit(     182     X,     183     y_onehot,     184     batch_size=self.batch_size,     185     epochs=self.n_epochs,     186     verbose=self.verbose,     187     callbacks=self._callbacks,     188 )     189 return self File ~/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/utils/traceback_utils.py:70, in filter_traceback..error_handler(*args, **kwargs)      67     filtered_tb = _process_traceback_frames(e.__traceback__)      68      To get the full stack trace, call:      69      `tf.debugging.disable_traceback_filtering()` > 70     raise e.with_traceback(filtered_tb) from None      71 finally:      72     del filtered_tb File ~/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      50 try:      51   ctx.ensure_initialized() > 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,      53                                       inputs, attrs, num_outputs)      54 except core._NotOkStatusException as e:      55   if name is not None: NotFoundError: Graph execution error: Detected at node 'StatefulPartitionedCall_4' defined at (most recent call last):     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/runpy.py"", line 197, in _run_module_as_main       return _run_code(code, main_globals, None,     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/runpy.py"", line 87, in _run_code       exec(code, run_globals)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel_launcher.py"", line 17, in        app.launch_new_instance()     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/traitlets/config/application.py"", line 1041, in launch_instance       app.start()     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel/kernelapp.py"", line 711, in start       self.io_loop.start()     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/tornado/platform/asyncio.py"", line 215, in start       self.asyncio_loop.run_forever()     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/asyncio/base_events.py"", line 601, in run_forever       self._run_once()     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/asyncio/base_events.py"", line 1905, in _run_once       handle._run()     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/asyncio/events.py"", line 80, in _run       self._context.run(self._callback, *self._args)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel/kernelbase.py"", line 510, in dispatch_queue       await self.process_one()     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel/kernelbase.py"", line 499, in process_one       await dispatch(*args)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel/kernelbase.py"", line 406, in dispatch_shellawait result     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel/kernelbase.py"", line 729, in execute_request       reply_content = await reply_content     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel/ipkernel.py"", line 411, in do_execute       res = shell.run_cell(     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/ipykernel/zmqshell.py"", line 530, in run_cell       return super().run_cell(*args, **kwargs)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 2940, in run_cell       result = self._run_cell(     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 2995, in _run_cell       return runner(coro)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner       coro.send(None)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 3194, in run_cell_async       has_raised = await self.run_ast_nodes(code_ast.body, cell_name,     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 3373, in run_ast_nodes       if await self.run_code(code, result, async_=asy):     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 3433, in run_code       exec(code_obj, self.user_global_ns, self.user_ns)     File ""/var/folders/50/zx6p2vtx3h9bvdft7b3h85sm0000gn/T/ipykernel_51235/1980619450.py"", line 2, in        network.fit(X_train, y_train)     File ""/Users/tomxu/Documents/sktime/sktime/classification/base.py"", line 191, in fit       self._fit(X, y)File ""/Users/tomxu/Documents/sktime/sktime/classification/deep_learning/cnn.py"", line 181, in _fit       self.history = self.model_.fit(     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/utils/traceback_utils.py"", line 65, in error_handler       return fn(*args, **kwargs)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/engine/training.py"", line 1650, in fit       tmp_logs = self.train_function(iterator)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/engine/training.py"", line 1249, in train_function       return step_function(self, iterator)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/engine/training.py"", line 1233, in step_function       outputs = model.distribute_strategy.run(run_step, args=(data,))     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/engine/training.py"", line 1222, in run_step       outputs = model.train_step(data)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/engine/training.py"", line 1027, in train_step       self.optimizer.minimize(loss, self.trainable_variables, tape=tape)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/optimizers/optimizer_experimental/optimizer.py"", line 527, in minimize       self.apply_gradients(grads_and_vars)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1140, in apply_gradients       return super().apply_gradients(grads_and_vars, name=name)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/optimizers/optimizer_experimental/optimizer.py"", line 634, in apply_gradients       iteration = self._internal_apply_gradients(grads_and_vars)     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1166, in _internal_apply_gradients ...     File ""/Users/tomxu/miniforge3/envs/sktimedev/lib/python3.9/sitepackages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1211, in apply_grad_to_update_var       return self._update_step_xla(grad, var, id(self._var_key(var))) Node: 'StatefulPartitionedCall_4' could not find registered platform with id: 0x13f038990 	 [[{{node StatefulPartitionedCall_4}}]] [Op:__inference_train_function_1051]","Hi  , > warnings.warn( > 20221229 12:04:05.465251: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. > 20221229 12:04:05.465271: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) > physical PluggableDevice (device: 0, name: METAL, pci bus id: ) It seems you are trying to use GPU on MacOS. Currently Tensorflow GPU not supported on MacOS.Please refer to attached documentation for  pip and source installations and tested configurations.Can you please try on CPU versions and let us know if any problem arises there. Thank you!",M1 Macbook has 8 core GPU that can be used for ML purposes. I have successfully used M1 GPU for tensorflow using Apple Metal package. I am surprized you don't seem to have knowledge of it. ,"Hi , For `tensorflow` currently no official GPU support for MacOS. If you are using Metal Plugin which uses `tensorflowmacos` you need to follow instructions by Apple here since `tensorflow` and `tensorflowmacos` both are different and `tensorflowmacos` maintained by Apple.Please refer to developer comments on similar issue here.For tensorflow you need to follow instructions mentioned in earlier comment. Can you please confirm which instructions you have followed ? If you are using metal plugin instructions can you try below commands and let us know if it works.  Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
684,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Almost wiped my PC trying to make Tensorboard work on windows Jupyter notebook)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,elijah123815,Almost wiped my PC trying to make Tensorboard work on windows Jupyter notebook,Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-21T07:17:35Z,stat:awaiting response stale type:others comp:core TF 2.11,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58969,"Hi  , `shutil.rmtree(logdir, ignore_errors=True)` removes all the directories and sub directories in `logdir` path/directory/File Hence we should be careful in selecting the `logdir` path. Though I am not sure how this is related to tensorflow.Whether you have find any such instructions in Tensorflow tutorial ? If so please let us know.  Iam not sure about how to recover those deleted files.You may try your luck referring to below links link1 & link2. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
605,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Not able to )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,varun-sappa,Not able to ,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-20T16:19:20Z,stat:awaiting response type:build/install stale subtype:windows TF 2.11,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58960,Hi sappa ! Could you install transformer using below command.  `pip install tensorflow==2.11 transformers ` Attached gist for reference.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
707,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow.experimental.numpy.lcm giving inconsistent results)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue Using Tensorflow  Using numpy  ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fnhirwa,tensorflow.experimental.numpy.lcm giving inconsistent results,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue Using Tensorflow  Using numpy  ```  Relevant log output _No response_,2022-12-20T11:33:00Z,stat:awaiting response type:bug comp:ops TF 2.11,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58955,  I was able to reproduce the issue on Colab using TF v2.11. Please find the gist here for reference. Thank you!,"Hi nshuti , Thanks for reporting. We have observed the behaviour of this API recently in CC(The experimental lcm function gives wrong result) and reported the same to Developer Team for action. Thankyou!","Hi nshuti , The issue has been fixed in tfnightly.Please refer to the attached gist. Could you please crosscheck and confirm from your side? Please feel free to close if resolved. Thanks!",I am running the gist from my end but getting a runtime errorğŸ§ ,"nshuti , The error is related to numpy version mismatch in Colab environment which appears some times. Could you please try upgrading numpy version to 1.22.4 using pip and then restart runtime in colab and check whether numpy version upgraded to 1.22.4. Once done you can test the code.  Please find the gist where I followed the above steps and able to resolve the reported runtime error and also the main reported issue addressed with latest nightly_version(tf_nightly2.13.0.dev20230220).  Please check and close the issue if resolved. Thanks again for reporting the issue .",Hi   Thanks for fixing the issue I tested the code from my end and it works as expectedğŸ™‚,Are you satisfied with the resolution of your issue? Yes No
741,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow device is mapped to multiple devices when using tf.estimator models and setting visible device using TF v1 api with TF2.11)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version python 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.3  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,LuFinch,TensorFlow device is mapped to multiple devices when using tf.estimator models and setting visible device using TF v1 api with TF2.11,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version python 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.3  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-20T08:01:58Z,stat:awaiting response type:bug comp:apis TF 2.11,closed,1,9,https://github.com/tensorflow/tensorflow/issues/58952,"Ok  ! Thanks for sharing your observation w.r.to tf.estimator and Horovod multiGPU card. Just quick observation here. If you are trying to use compat.v1 mode, you need to disable v2 behavior  too as below.    Could you look at this issue. Attached gist for reference. Thank you!",I have tried `tensorflow.compat.v1.disable_v2_behavior()` but it does not work and throw the same error.,"Hi  , I Just to remind you about the Warning for the tf.estimator API as below. > Warning: Estimators are not recommended for new code. Estimators run v1.Sessionstyle code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details. I tried to replicate the issue on a Multi GPU VM. I referred the source to install horovod and used this command  `HOROVOD_GPU_OPERATIONS=NCCL pip install horovod` to install horovod with GPU support but this failed. Could you please confirm the commands you used for installing horovod and also to replicate the problem so that i can try to replicate it and for doing needful. Thankyou!","Hi ï¼Œ thanks for your time. I know that tf.estimator API is not recommanded. However, a part of models we used were implemented by v1 style, and we just want to use them without changing any code or adding any TF2 code. I use the same command to install horovod and it works. Could you please try again? If you can't install it, you can also replace  with   and directly run it with  `python xxx.py` without horovod to reproduce it in single process case. Thanks you !","Hi  , I tried with TF 2.10V and found no error as below.  However with tfnightly i replicated the error you mentioned as below.  Could you please refer to CC(GPU remapping using visible_device_list is broken) and CC(Running two Models in two GPUs for prediction in c++) related to similar issue and confirm if got any insights there ? Thankyou!","Hi . I have checked these two issues before. But the senario is different here. As we mentioned aboved, if we set   and run it with `python xxx.py`,  it is actually a single process job using physical GPU 1 instead of a multiprocesses or multithreads job. I think it is a normal case that some customers want to run TF job with specific GPU.  Can you help to find out the error reason? Thanks!","Hi  , Please refer to the Documentation Source on Distribution Training w.r.t  Estimator API.  As estimator API has limited support and also Training/Evaluation are under experimental and hence can't predict the behaviour. Thank you!",Are you satisfied with the resolution of your issue? Yes No,"I met the same issue, is there any suggestions?"
1850,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.custom_gradient with multiple input and output)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**:    **TensorFlow version (use command below)**: 2.9.2    **Python version**: 3.9    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**:  Describe the problem I have a function with 4 inputs (x1, x2, x3, x4) and 2 outputs (y1, y2) using Tensorflow. I would like to specify the gradients, since I perform some nonautodiff operations inside the function. I need to specify the derivatives of the outputs with respect to the inputs. We can see these derivatives as a Jacobian of size (2,4). Regarding this, I have 8 derivatives: dy1_dx1, dy1_dx2, dy1_dx3, dy1_dx4, dy2_dx1, dy2_dx2, dy2_dx3 and dy2_dx4. However, the grad function used in this tf.custom.gradient needs to have the same length as the inputs, this is 4. So, I do not know how Tensorflow handles with the introduction of the 8 derivatives using just 4 elements. I tried to include them as lists, but it gives the error. Here is a general code to reproduce the error:  The  error says: ""custom_gradient function expected to return 4 gradients, but returned 8 instead"". I expect someway to specify the correspondent 8 derivatives. Thank you in advance!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jesgosi,tf.custom_gradient with multiple input and output," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**:    **TensorFlow version (use command below)**: 2.9.2    **Python version**: 3.9    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**:  Describe the problem I have a function with 4 inputs (x1, x2, x3, x4) and 2 outputs (y1, y2) using Tensorflow. I would like to specify the gradients, since I perform some nonautodiff operations inside the function. I need to specify the derivatives of the outputs with respect to the inputs. We can see these derivatives as a Jacobian of size (2,4). Regarding this, I have 8 derivatives: dy1_dx1, dy1_dx2, dy1_dx3, dy1_dx4, dy2_dx1, dy2_dx2, dy2_dx3 and dy2_dx4. However, the grad function used in this tf.custom.gradient needs to have the same length as the inputs, this is 4. So, I do not know how Tensorflow handles with the introduction of the 8 derivatives using just 4 elements. I tried to include them as lists, but it gives the error. Here is a general code to reproduce the error:  The  error says: ""custom_gradient function expected to return 4 gradients, but returned 8 instead"". I expect someway to specify the correspondent 8 derivatives. Thank you in advance!",2022-12-19T10:35:13Z,stat:awaiting response type:support stale comp:ops TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58941,", I was facing a different issue while executing the mentioned code. Kindly find the gist of it here and provide the dependencies to debug the issue.","Also When we are using the function that takes multiple variables as input, the grad function must also return the same number of variables. Please take a look at this official doc link of **tf.custom_gradient** for the reference. Thank you!","> Also When we are using the function that takes multiple variables as input, the grad function must also return the same number of variables. Please take a look at this official doc link of **tf.custom_gradient** for the reference. Thank you! Than you for your response. I updated my question and the gist in order to better reproduce the error. I have already read the reference and I know I must return the same number of variables, but mathematically, I should return 8 gradients instead of 4, so I do not know how tensorflow makes it possible.",", This type of questions are better asked on TensorFlow Forum since it is not a bug or feature request. There is also a larger community that reads and responds to the questions there. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
693,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bad download link for Windows GPU only binaries zip file)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version x86_642.11.0  Custom Code No  OS Platform and Distribution Windows x86 GPU only  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,krsummersBA,Bad download link for Windows GPU only binaries zip file,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version x86_642.11.0  Custom Code No  OS Platform and Distribution Windows x86 GPU only  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-12-18T04:14:37Z,type:docs-bug stat:awaiting response type:build/install ready to pull subtype:windows TF 2.11,closed,0,8,https://github.com/tensorflow/tensorflow/issues/58933,"Hi  ! I can confirm your observations on GPU wheels from pip installation document . ( Can downloads CPU wheels as expected and seeing  404 error on GPU wheels) Could you point the location of this ""storage.googleapis.com/tensorflow/libtensorflow/libtensorflowgpuwindowsx86_642.11.0.zip "" file too.   ! Could you look at this issue. Attached gist for reference. Thank you!","All the Windows GPU links mentioned in Documentation are empty.This might be due to recent development with Windows or due to some broken links during recent updation in Documentation.   , Thanks for reporting this and we will have a look into it and Meanwhile if you want you can refer here for instructions on how to install Tensorflow with GPU support for Windows. Thankyou!","Can you provide an alternate / temporary link for the  GPU .zip file?   I don't want to build/install this from scratch, but just need the valid zip file so I can reference some of the included DLL files.","> Can you provide an alternate / temporary link for the GPU .zip file? I don't want to build/install this from scratch, but just need the valid zip file so I can reference some of the included DLL files. Note sure about how to get the latest version, but the previous can be downloaded by changing the url: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflowgpuwindowsx86_642.10.0.zip","> All the Windows GPU links mentioned in Documentation are empty.This might be due to recent development with Windows or due to some broken links during recent updation in Documentation. >  >  , Thanks for reporting this and we will have a look into it and Meanwhile if you want you can refer here for instructions on how to install Tensorflow with GPU support for Windows. >  > Thankyou! hey SuryanarayanaY this issue is still open ?","Hi   , TensorFlow 2.10 was the last TensorFlow release that supported GPU on nativeWindows.We are not maintaining the GPU wheel for tensorflow 2.11. A PR has been moved for updating those broken/non existing GPU wheels with TF 2.10v. Thank you!"," , The mentioned PR has been merged.Could you please check and confirm? Thanks!",Are you satisfied with the resolution of your issue? Yes No
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reproducible dataset order after shuffle from checkpoint)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution macOS Monterey 12.6, Debian GNU/Linux bookworm/sid  Mobile device _No response_  Python version 3.8.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4 (Debian)  GPU model and memory NVIDIA GeForce RTX 2080 Ti (Debian)  Current Behaviour? I would like to get reproducible results for datasets in which the data is shuffled. In PyTorch I could save rng and load rng to get the same shuffling order, but in Tensorflow I don't know how to do that. I would like to get results like this from a shuffled dataset. Additionally, I would also want to know how to do it on `Keras` datasets which are used in `model.fit()`   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,WolodjaZ,Reproducible dataset order after shuffle from checkpoint,"Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution macOS Monterey 12.6, Debian GNU/Linux bookworm/sid  Mobile device _No response_  Python version 3.8.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4 (Debian)  GPU model and memory NVIDIA GeForce RTX 2080 Ti (Debian)  Current Behaviour? I would like to get reproducible results for datasets in which the data is shuffled. In PyTorch I could save rng and load rng to get the same shuffling order, but in Tensorflow I don't know how to do that. I would like to get results like this from a shuffled dataset. Additionally, I would also want to know how to do it on `Keras` datasets which are used in `model.fit()`   Standalone code to reproduce the issue   Relevant log output _No response_",2022-12-16T15:06:20Z,stat:awaiting response stat:awaiting tensorflower type:feature stale comp:data TF 2.11,closed,0,13,https://github.com/tensorflow/tensorflow/issues/58925,"Hi  , In Tensorflow we have Two Seeds one at Global level and another at Op level.When included both it shall create reproducible results.Please refer to the source for examples on how it works. In your example you can set global seed using `tf.random.set_seed` and at Op level say for `shuffle()` you have `seed` argument where you can set some seed value here. Iam attaching sample gist explaining how it works in Tensorflow. Please refer and come back if more details needed. Thank you!","Yes, thank you for your reply. However, I had something else in mind. I would like to be able to take the state from the Dataloader, save it and from the checkpoint resume with the shuffle done in the same order. I was able to do this by passing a seed that is changed every epoch for this init_seed + epoch function. However, I don't think this is the best way to do it. Does the shuffle function use this seed, or does it set a global seed every time? I want to do a tutorial/blog on how to do repeatable results in several frameworks. The code for this example is here https://github.com/WolodjaZ/reproducmltutorial/blob/main/mnist_tensorflow.py, if you have any advice I'd be happy to hear. Sincerely, Vladimir Zaigrajew pon., 26 gru 2022 o 09:34 SuryanarayanaY ***@***.***> napisaÅ‚(a): > Hi   , > > In Tensorflow we have Two Seeds one at Global level and another at Op > level.When included both it shall create reproducible results.Please refer > to the source >  for > examples on how it works. In your example you can set global seed using > tf.random.set_seed and at Op level say for shuffle() you have seed > argument where you can set some seed value here. > > Iam attaching sample gist >  > explaining how it works in Tensorflow. > > Please refer and come back if more details needed. > > Thank you! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Hi  , shuffle function uses its local seed and it has no control on global seed set by `tf.random.set_seed`. Both global seed and local seed together decides the Random Number generation.The local seed changes the value with every epoch but it will be in conjunction with global seed.Please refer to shuffle API here. I just attached a gist here to test how global seed and random seed works.I run the same code in 3 different cells and all producing same numbers in order.Then I tried with restart runtime and also disconnect and delete runtime and every time the results are in same with order also.This is a simple demo to explain how random seeding works in tensorflow.Please confirm whether it is useful for you. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi SuryanarayanaY, Your solution works well for accessing data from a specific epoch. By giving the seed value to shuffle based on the epoch, I can easily access the data from that epoch. However, I think that this solution may not be perfect for handling data augmentation, where frameworks like PyTorch can set all seeds for a given worker to handle this. In your framework, I am not sure how to tackle this problem of reproducible data from datasets from checkpoint. Additionally, I have been trying to make reproducible training possible from a checkpoint, both on CPU and GPU. Can you advise me on how to achieve this? I have been using the code from this GitHub repository: https://github.com/WolodjaZ/reproducmltutorial/blob/main/index.ipynb. I want to be able to obtain the same loss and accuracy in the initial first training loop and from the training loop starting from the second epoch checkpoint. Specifically, after the first training loop, I want to be able to continue training for a few more epochs and then come back to the second epoch and continue training, with the aim of obtaining the same loss and accuracy as in the initial training loop.  Thank you for your assistance. Sincerely, Vladimir Zaigrajew Å›r., 4 sty 2023 o 12:48 SuryanarayanaY ***@***.***> napisaÅ‚(a): > Hi   , > shuffle function uses its local seed and it has no control on global seed > set by tf.random.set_seed. Both global seed and local seed together > decides the Random Number generation.The local seed changes the value with > every epoch but it will be in conjunction with global seed.Please refer to > shuffle API here > . > I just attached a gist here >  > to test how global seed and random seed works.I run the same code in 3 > different cells and all producing same numbers in order.Then I tried with > restart runtime and also disconnect and delete runtime and every time the > results are in same with order also.This is a simple demo to explain how > random seeding works in tensorflow.Please confirm whether it is useful for > you. > Thank you! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","The issue with the original repro is that it doesn't set the `reshuffle_each_iteration` argument to shuffle. By default `shuffle` will reshuffle, so you get a different order each iteration. You can save dataset iterators into checkpoints, so that they can be restored to the exact state.","I apologise for the late reply. Then I want to discuss a few things. First of all, I want to know more about how checkpoints store an iterator where I will be comparing it with my current approach (screenshot added in the appendix). So how the iterator is stored, because if it is stored as an object (pickle for example) then it is not optimal for storage space (mentioned: Note however that the iterator checkpoints can be large). Compared to my solution, I only need to store/know the seed value to recreate the batches. If the checkpoint only stores seeds or something related optimised for disk space, then I would try to implement it. Secondly, I understand how shuffle works and how the reshuffle_each_iteration parameter affects, however, I want to shuffle my train dataset every epoch and also want to know how shuffle shuffles this dataset every time, so using this parameter doesn't help me (looking at my example https://github.com/WolodjaZ/reproducmltutorial/blob/main/mnist_tensorflow.py ). wt., 17 sty 2023 o 23:23 Andrew Audibert ***@***.***> napisaÅ‚(a): > The issue with the original repro is that it doesn't set the > reshuffle_each_iteration argument to shuffle > . > By default shuffle will reshuffle, so you get a different order each > iteration. > > You can save dataset iterators into checkpoints > , > so that they can be restored to the exact state. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >"," Hello, I have been looking into the same subject. What are your current insights on this ? I would love to know. I will shortly experiment with saving the dataset iterator with imagenet training and would give my stats here with and without iterator.","Hi  , Apologies for the delay. Did you checked this comment ? As mentioned the shuffle method has an argument `reshuffle_each_iteration` which is True by default. You need to set it to False explicitly that can probably fixes the issue. Also I have gone through your repo and you have not set any thing to `reshuffle_each_iteration` in `shuffle()`.So by default its doing shuffling and hence changing the dataset order. Setting this argument to `False` might resolve your error. Please check and confirm if still have problems? Thank you!",", okay, I agree that it might work with `reshuffle_each_iteration`. However, what if I want to reshuffle after each iteration (epoch) and still be able to reproduce the iterator state (shuffle order) from the checkpoint? Is this scenario achievable? , after quite some time, I stopped searching for a solution and settled with changing the seed every epoch and explicitly shuffling the dataset with it. Thanks to that approach, you can save the seed and reproduce the state of the iterator from the checkpoint. The code I used","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The shuffle function uses its own internal seed, which is independent of the global seed set by tf.random.set_seed. The combination of both the global and local seeds determines the final random number generation. The local seed changes with each epoch, but it still interacts with the global seed. Ref The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NVIDIA TF-TRT] Adding TF_TRT_OP_FAKELIST env var to allow TF-TRT segmentation experimentation)ï¼Œ å†…å®¹æ˜¯ (This PR introduces the env var `TF_TRT_OP_FAKELIST=OpName1,OpName2,...`. The motivation of this change is to allow to experiment with the impact on graph segmentation of adding converter X, Y or Z. This API is not designed for users, but rather TFTRT team to evaluate the importance / impact of the different fearture requests.  It works by intercepting the call to `OpConverterRegistry::Impl::LookUp` and return the `FakeOp` converter in case the OpName matches any of the names passed in `TF_TRT_OP_FAKELIST`.  This is expected to have zero impact on any of the current workflows. Replaces PR: CC([TFTRT] Adding `TF_TRT_OP_FAKELIST` env var to allow TFTRT segmentation experimentation))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DEKHTIARJonathan,[NVIDIA TF-TRT] Adding TF_TRT_OP_FAKELIST env var to allow TF-TRT segmentation experimentation,"This PR introduces the env var `TF_TRT_OP_FAKELIST=OpName1,OpName2,...`. The motivation of this change is to allow to experiment with the impact on graph segmentation of adding converter X, Y or Z. This API is not designed for users, but rather TFTRT team to evaluate the importance / impact of the different fearture requests.  It works by intercepting the call to `OpConverterRegistry::Impl::LookUp` and return the `FakeOp` converter in case the OpName matches any of the names passed in `TF_TRT_OP_FAKELIST`.  This is expected to have zero impact on any of the current workflows. Replaces PR: CC([TFTRT] Adding `TF_TRT_OP_FAKELIST` env var to allow TFTRT segmentation experimentation)",2022-12-14T23:29:13Z,awaiting review ready to pull size:M comp:gpu:tensorrt,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58898,Hi  Can you please review this PR ? Thank you!," Hi, can you find an NVIDIA reviewer for this? I can serve as reviewer on the Google side. Also, it looks like this doesn't modify runtime runtimes and won't affect built models  can you confirm?"," can you review please ? >  Also, it looks like this doesn't modify runtime runtimes and won't affect built models  can you confirm? Correct it only affects model segmentation "
968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NVIDIA TF-TRT] Adding TF_TRT_OP_FAKELIST env var to allow TF-TRT segmentation experimentation)ï¼Œ å†…å®¹æ˜¯ (This PR introduces the env var `TF_TRT_OP_FAKELIST=OpName1,OpName2,...`. The motivation of this change is to allow to experiment with the impact on graph segmentation of adding converter X, Y or Z. This API is not designed for users, but rather TFTRT team to evaluate the importance / impact of the different fearture requests.  It works by intercepting the call to `OpConverterRegistry::Impl::LookUp` and return the `FakeOp` converter in case the OpName matches any of the names passed in `TF_TRT_OP_FAKELIST`.  This is expected to have zero impact on any of the current workflows. Replaces PR: CC([TFTRT] Adding `TF_TRT_OP_FAKELIST` env var to allow TFTRT segmentation experimentation))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DEKHTIARJonathan,[NVIDIA TF-TRT] Adding TF_TRT_OP_FAKELIST env var to allow TF-TRT segmentation experimentation,"This PR introduces the env var `TF_TRT_OP_FAKELIST=OpName1,OpName2,...`. The motivation of this change is to allow to experiment with the impact on graph segmentation of adding converter X, Y or Z. This API is not designed for users, but rather TFTRT team to evaluate the importance / impact of the different fearture requests.  It works by intercepting the call to `OpConverterRegistry::Impl::LookUp` and return the `FakeOp` converter in case the OpName matches any of the names passed in `TF_TRT_OP_FAKELIST`.  This is expected to have zero impact on any of the current workflows. Replaces PR: CC([TFTRT] Adding `TF_TRT_OP_FAKELIST` env var to allow TFTRT segmentation experimentation)",2022-12-14T23:16:31Z,size:M comp:gpu:tensorrt,closed,0,1,https://github.com/tensorflow/tensorflow/issues/58896,Closing PR to test Trusted Partner Program
1774,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Variables across different runners in TFLite and SavedModel)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No    **TensorFlow installed from (source or binary)**: Binary    **TensorFlow version (use command below)**: v2.11.0rc217gd5b57ca93e5    **Python version**: 3.9    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**: Not used    **GPU model and memory**: Not used    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You can obtain the TensorFlow version with:   Describe the problem I am unsure if this is a bug or an expected behavior. I am trying to share `tf.Variable`s between signature runners in TFLite but it does not seem to work. Executing eagerly or with a SavedModel, things work as expected, but the TFLite export somehow flattens/prunes the graph and does not share the variables globally between the signature runners.  Source code / logs I prepared a minimal example here: https://colab.research.google.com/drive/1DTau1na7YfzCWtRk8RC5KHVzZaNp7Tsx?usp=sharing With TFLite my target is more C++ but the problem appears in Python too, and it was easier to share and test with Python.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,paulfd,Variables across different runners in TFLite and SavedModel," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No    **TensorFlow installed from (source or binary)**: Binary    **TensorFlow version (use command below)**: v2.11.0rc217gd5b57ca93e5    **Python version**: 3.9    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**: Not used    **GPU model and memory**: Not used    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You can obtain the TensorFlow version with:   Describe the problem I am unsure if this is a bug or an expected behavior. I am trying to share `tf.Variable`s between signature runners in TFLite but it does not seem to work. Executing eagerly or with a SavedModel, things work as expected, but the TFLite export somehow flattens/prunes the graph and does not share the variables globally between the signature runners.  Source code / logs I prepared a minimal example here: https://colab.research.google.com/drive/1DTau1na7YfzCWtRk8RC5KHVzZaNp7Tsx?usp=sharing With TFLite my target is more C++ but the problem appears in Python too, and it was easier to share and test with Python.",2022-12-14T14:38:38Z,stat:awaiting response type:bug type:support stale comp:lite TF 2.11,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58886," I was able to replicate the issue, please find the gist here. Thank you!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
628,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot make use of GPU)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version v2.11.0rc217gd5b57ca93e5  Custom Code Yes  OS Platform and Distribution Linux Manjaro 6.0.11  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 12  GPU model and memory RTX 4090  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AntouanK,Cannot make use of GPU,Click to expand!    Issue Type Support  Source source  Tensorflow Version v2.11.0rc217gd5b57ca93e5  Custom Code Yes  OS Platform and Distribution Linux Manjaro 6.0.11  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 12  GPU model and memory RTX 4090  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-13T11:50:34Z,type:support comp:lite TFLiteGpuDelegate TF 2.11,closed,0,14,https://github.com/tensorflow/tensorflow/issues/58875,"Hi  ! In TFlite side, GPU delegate is not supported for Personal computers yet and will leverage only CPU/XNN pack delegates when needed. Speaking from Mediapipe point of view , Could you check and modify your steps with mediapipe's GPU instruction  for running in linux systems.  Thank you!",thanks for the answer. not sure how to do all that. so far I only installed cuda/tensorflow related packages from pacman ( arch ) or pip/conda ( python ). I haven't built anything from source. Could you maybe give me a hint on how to try that?,Hi  ! Attached sample bazel notebook  for reference.  (Note: Please set the path for Bazel binary  in environment variables before using this notebook Please give us an update after you modify your commands as per above comment. Thank you!,  I opened the notebook you mentioned and I connected it to my local runtime. On the 3rd step I get an error !image EDIT : I had to add `nocheckcertificate` to the command.,"  got an error on this step  !image I got those files, not sure how to tell bazel about it !image **EDIT** adding `copt I/usr/include/opencv4` solved that issue","So, I managed to build it successfully. !image Not sure what the next step is. Running that executable doesn't do anything. !image", ! Thanks for the update on the build the executable. Could you run the executable along the .pbtxt and selfisegmentation.py file then  Thank you!,">  ! Thanks for the update on the build the executable. Could you run the executable along the .pbtxt and selfisegmentation.py file then >  >  >  > Thank you! that pbtxt file doesn't exist in my directory !image If I change the command to   I get a window with my face and a blue background. FPS is 60 so it works great with GPU. Logs :  :+1:  Now the problem is that I don't know how to use that for my purpose. All I want is to have a fake background for my webcam when I use it for video calls. Can I combine this with the python scrips I found on github? Also, 2 more questions. 1.  How do I change resolution? My webcam is 1920x1080 but the window I get has a way smaller resolution. 2. The area around my face is very pixelated. Is there a way to fine tune this to be smoother or something? Thank you so much for all the help. !image", ! You can resize the images using opencv and also do the same w.r.to Medipipe framework and calculators.  Please check in the closed issue in Mediapipe repo/ TFforum  for further assistance. Marking this issue to resolved now from above comment as original issue on using GPU is resolved. Thank you !,Are you satisfied with the resolution of your issue? Yes No,"  > You can resize the images using opencv and also do the same w.r.to Medipipe framework and calculators. I don't understand that at all. All I have is an executable and a `.pbtxt` file that I don't know what language it's in. I don't see any way to resize anything or how to configure it or pass the input/output, or use it with python. I understand that the ""can it use the GPU"" issue is closed, but I'm totally clueless on how to use that binary that I've built in conjunction with python or js. Any hint on how to do that? Thank you.","Hi  ! Attached below code snippet for image resizing. Ref You can use superresolution api from Tensorflow to clear the pixelation issue. Ref 1, 2. Thank you!"," thank you for the answer, but I think I'm not clear on my question. I don't understand how I can combine a python script, with an executable binary that accepts some sort of protobuf(?) graph file. How am I supposed to use those two together? That binary I built with bazel, how can it be used from any other python script on my machine? Can you please point on how to do that? I've googled a lot but cannot find anything close to an answer. The tensorflow website is huge, I don't even know where to start in order to understand how to combine the two.", ! Just FYI: GPU delegates for AMD and Windows machines has been enabled through CMake. You can build the GPU enabled TFlite run time by using below flag. `DTFLITE_ENABLE_GPU=[ON]` You can also build the delegates using sample bazel notebook and use load delegate option to load the same gpu delegates. We can reopen if still is not resolved from your side. Thank you!
1861,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(compile tensorflow-lite-select-tf-ops no error show,but exit)ï¼Œ å†…å®¹æ˜¯ ( 1. System information tensorflow 2.6.0 bazel 3.7.2 Androidndk r21e Androidsdk 30 jdk14.0.1  2. Code compile tensorflowlite success bazel build c opt fat_apk_cpu=arm64v8a \   host_crosstool_top=//tools/cpp:toolchain \   //tensorflow/lite/java:tensorflowlite compile tensorflowliteselecttfops failed bazel build c opt fat_apk_cpu=arm64v8a host_crosstool_top=//tools/cpp:toolchain //tensorflow/lite/java:tensorflowliteselecttfops  3. logs Analyzing: target //tensorflow/lite/java:tensorflowliteselecttfops (223 packages loaded, 11969 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/f7b1fa6f5ebec5780e626aa48d582f2519a01632.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found INFO: Analyzed target //tensorflow/lite/java:tensorflowliteselecttfops (243 packages loaded, 25811 targets configured). INFO: Found 1 target... [0 / 1] [Prepa] BazelWorkspaceStatusAction stablestatus.txt [24 / 50] [Prepa] Expanding template external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/desugar/Desugar [49 / 64] [Prepa] Desugaring tensorflow/lite/java/libtensorflowlite_java.jar for Android [136 / 349] [Scann] Compiling flatbuffers/src/idl_gen_python.cpp [148 / 349] [Scann] Compiling flatbuffers/grpc/src/compiler/go_generator.cc [176 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_string_field.cc [211 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/java/java_message_field_lite.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,zf853109035,"compile tensorflow-lite-select-tf-ops no error show,but exit"," 1. System information tensorflow 2.6.0 bazel 3.7.2 Androidndk r21e Androidsdk 30 jdk14.0.1  2. Code compile tensorflowlite success bazel build c opt fat_apk_cpu=arm64v8a \   host_crosstool_top=//tools/cpp:toolchain \   //tensorflow/lite/java:tensorflowlite compile tensorflowliteselecttfops failed bazel build c opt fat_apk_cpu=arm64v8a host_crosstool_top=//tools/cpp:toolchain //tensorflow/lite/java:tensorflowliteselecttfops  3. logs Analyzing: target //tensorflow/lite/java:tensorflowliteselecttfops (223 packages loaded, 11969 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/f7b1fa6f5ebec5780e626aa48d582f2519a01632.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found INFO: Analyzed target //tensorflow/lite/java:tensorflowliteselecttfops (243 packages loaded, 25811 targets configured). INFO: Found 1 target... [0 / 1] [Prepa] BazelWorkspaceStatusAction stablestatus.txt [24 / 50] [Prepa] Expanding template external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/desugar/Desugar [49 / 64] [Prepa] Desugaring tensorflow/lite/java/libtensorflowlite_java.jar for Android [136 / 349] [Scann] Compiling flatbuffers/src/idl_gen_python.cpp [148 / 349] [Scann] Compiling flatbuffers/grpc/src/compiler/go_generator.cc [176 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_string_field.cc [211 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/java/java_message_field_lite.",2022-12-13T10:22:17Z,stat:awaiting response type:build/install comp:lite 2.6.0,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58873, Android tool needs to be added in the configure.py file. Could you try to upgrade to the latest TF  version and check this gist . Please let us know if it helps?  Thank you!, tksï¼ŒI've solved it. It's not enough memory that causes the background program to hang., Could you please close this issue if it is resolved? Thank you!,Are you satisfied with the resolution of your issue? Yes No
1041,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NVIDIA XLA] add a new mode that hlo_parser can parse computations without module header.)ï¼Œ å†…å®¹æ˜¯ (This PR add a new mode in hlo_parser that can parse a HloModule from sequence of computation text without module header, the computation annotated with `ENTRY` or the last computation if no `ENTRY` annotation will be used as the entry computation of the module.  Also, will force the module's input/output layout as entry computation's parameter and ROOT instruction layout.  With this updates, XLA tool `replay_computation` can just run a computation by just specifying the computation dump out.  It is useful/convient when we just want `replay_computation` to focus on the perf of a specific computation, or fused kernel (use fused computation as replay_computation input), so just need to copy the target computation HLO texts and run with `replay_computation`  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shawnwang18,[NVIDIA XLA] add a new mode that hlo_parser can parse computations without module header.,"This PR add a new mode in hlo_parser that can parse a HloModule from sequence of computation text without module header, the computation annotated with `ENTRY` or the last computation if no `ENTRY` annotation will be used as the entry computation of the module.  Also, will force the module's input/output layout as entry computation's parameter and ROOT instruction layout.  With this updates, XLA tool `replay_computation` can just run a computation by just specifying the computation dump out.  It is useful/convient when we just want `replay_computation` to focus on the perf of a specific computation, or fused kernel (use fused computation as replay_computation input), so just need to copy the target computation HLO texts and run with `replay_computation`  ",2022-12-13T09:02:55Z,awaiting review ready to pull comp:xla size:M,closed,0,9,https://github.com/tensorflow/tensorflow/issues/58870,"What is the motivation here, to avoid writing ""HloModule XXX"" line at the top?","Note that you do not have to specify input/output layout, just writing ""HloModule m"" at the top will do.",Would you like to write up a motivation for this change at https://github.com/openxla/xla/discussions ?," The motivation for this PR is that I want to run the computation exactly the same as when it is running in a larger inclusion HloModule. i.e, want to run the same input/output layout.   The frequent debug scenario I had is like below:   1. When running an XLA model, from profiler, I saw that a fusion operator may have perf problem, and you dump the XLA graph during running the model to get the fusion computation graph.  2. Then I want to focus on the target problematic fusion operator, and want to reproduce it with the minimum run, i.e, running just the fused computation with exactly same input/output layout.  3. I copied the fusion computation to a separate file, and use `replay_computation` to reproduce the run.  Yes, I can just add `HloModule m` at the top to run the computation, but this will not mandatory set the input/output layout I want to run, XLA may change the layout.  So the motivation for this PR is mainly for running the computation with mandatory layout as described by computation.  Does this make sense?  "," I did not find a create new discussion option in https://github.com/openxla/xla/discussions, is there some permission issue? ", interesting!  are there special permissions which are required?,Hi  Can you please check 's comments and keep us posted ? Thank you!,"    I added a unittest for the mode, could you take a look at it and proceed the request?  Thanks.", 
1852,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorfllow issue:  local installation not working.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version v2.11.0rc217gd5b57ca93e5 2.11.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory Nvidia Geforce RTX 3060 mobile 6GB  Current Behaviour?  Now as it is supposedly working, whenever I try to fit a model some weird error logs, which are included in Standalone code to reproduce the issue and Relevant log output. So my goal here is to continue using my GPU to run TensorFlow, shell import tensorflow as tf from tensorflow.keras import Sequential from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense from tensorflow.keras.optimizers import Adam  Check physical devices for GPU. physical_devices = tf.config.list_physical_devices('GPU') print(""Num GPUs:"", len(physical_devices))  Set random seed. tf.random.set_seed(24)  Build the model. model_1 = Sequential([     Conv2D(10, 3, activation='relu', input_shape = (224, 224, 3)),     Conv2D(10, 3, activation='relu'),     MaxPool2D(),     Conv2D(10, 3, activation='relu'),     Conv2D(10, 3, activation='relu'),     MaxPool2D(),     Flatten(),     Dense(len(class_names), activation='softmax') ])  Compile the model. model_1.compile(loss=[""categorical_crossentropy""],                 optimizer=Adam(),                 metrics=[""accuracy""])  Fit the model. history_1 = model_1.fit(train_data,                         epochs=5,                         steps_per_epoch=len(train_data),        )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,santiagorg2401,Tensorfllow issue:  local installation not working.,"Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version v2.11.0rc217gd5b57ca93e5 2.11.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory Nvidia Geforce RTX 3060 mobile 6GB  Current Behaviour?  Now as it is supposedly working, whenever I try to fit a model some weird error logs, which are included in Standalone code to reproduce the issue and Relevant log output. So my goal here is to continue using my GPU to run TensorFlow, shell import tensorflow as tf from tensorflow.keras import Sequential from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense from tensorflow.keras.optimizers import Adam  Check physical devices for GPU. physical_devices = tf.config.list_physical_devices('GPU') print(""Num GPUs:"", len(physical_devices))  Set random seed. tf.random.set_seed(24)  Build the model. model_1 = Sequential([     Conv2D(10, 3, activation='relu', input_shape = (224, 224, 3)),     Conv2D(10, 3, activation='relu'),     MaxPool2D(),     Conv2D(10, 3, activation='relu'),     Conv2D(10, 3, activation='relu'),     MaxPool2D(),     Flatten(),     Dense(len(class_names), activation='softmax') ])  Compile the model. model_1.compile(loss=[""categorical_crossentropy""],                 optimizer=Adam(),                 metrics=[""accuracy""])  Fit the model. history_1 = model_1.fit(train_data,                         epochs=5,                         steps_per_epoch=len(train_data),        ",2022-12-12T14:24:14Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.11,closed,1,15,https://github.com/tensorflow/tensorflow/issues/58859,"_Why would tensorflow not be working here?_ TensorFlow may not be working due to an issue with the version of CUDA and cuDNN installed, the LD_LIBRARY_PATH environment variable, missing libnvinfer.so.7 and libnvinfer_plugin.so.7 files, or conflicts with other versions of TensorFlow installed. _What is tensorflowhub and how might it be causing this issue?_ TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. It can cause the TensorFlow error given in the stacktrace if it is not compatible with the version of TensorFlow installed, or if there is a conflict with other versions of TensorFlow installed. _How could this be checked?_ You can check the version of CUDA and cuDNN installed by running the commands: `nvcc version` and `cat /usr/local/cuda/include/cudnn.h`. You can uninstall and reinstall TensorFlow using the command: `pip uninstall tensorflow`. You can set the LD_LIBRARY_PATH environment variable using the command: `export LD_LIBRARY_PATH=/usr/local/cuda/lib64`. You can check if libnvinfer.so.7 and libnvinfer_plugin.so.7 are installed and properly linked by running the command: `ldconfig p | grep libnvinfer.so.7`. You can check if other versions of TensorFlow are installed by running the command: `pip list`. Let me know if that helps!  Used Clerkie (ai code debugging tool)  https://clerkie.co/, to help me brainstorm on approaches here (still reviewed/investigated feedback before sharing). Hope it helped! ","Hi, As I mentioned, I used the following official tutorial to install TensorFlow using a Conda environment to avoid installing CUDA in my system root. Following, the cudnn and cudatoolkit versions are: !image Which are compatible with TensorFlow 2.11 and follow the tutorial's versions. !image Running pip list shows that the only TensorFlow version installed in the conda environment is the 2.11. Finally, the LD_LIBRARY_PATH environment variable is already set as the tutorial mentions: !image !image","So I followed the same tutorial for TensorFlow V2.9.0 and it worked with no issues, I'd think it is safe to assume that the tutorial may not be working with TensorFlow 2.11.0",Ah gotcha  makes sense! ,  I tried to reproduce the issue on linux and successfully installed tensorflowgpu as per official documentation  I tried to replicate the given code but facing a different error please refer to  snapshot Could you please provide a complete code with all dependencies to reproduce the issue reported here. Thank you!,"Of course,  it is a Google Colab notebook, it is attached to this reply. 03_introduction_to_computer_vision_with_tensorflow_MultiClassClassification.zip",  I was able to reproduce the issue on Linux using TF v2.11. Please find the below screenshot for reference: !Screenshot from 20221221 141629 !Screenshot from 20221221 020948 Please find the gist here for reference. Thank you!,"Hi  , The problem might be happened due to compatibility issues between Tensorflow and tensorflow_hub. I tried to replicate the problem on a Ubuntu VM but there I could find no Error. The VM I used is having 18.04 Ubuntu OS, TF 2.10v and i just used `pip install upgrade tensorflow_hub` to install tensorflow_hub. Then I tried executing the code provided by you and found executing without any error.Please refer to attached snapshot of same.  Can you confirm whether you have tried TF 2.10V and whether problem exists there or not. Also please confirm the tensorflow_hub version you installed and command you used for installing tensorflow_hub. Thankyou!","Hi, I tried with TF V2.10 and it works fine. On Mon, 26 Dec 2022 at 08:44, SuryanarayanaY ***@***.***> wrote: > Hi   , > The problem might be happened due to compatibility issues between > Tensorflow and tensorflow_hub. > > I tried to replicate the problem on a Ubuntu VM but there I could find no > Error. The VM I used is having 18.04 Ubuntu OS, TF 2.10v and i just used pip > install upgrade tensorflow_hub to install tensorflow_hub. Then I tried > executing the code provided by you and found executing without any > error.Please refer to attached snapshot of same. > > [image: Screenshot 20221226 at 5 39 13 PM] >  > > Can you confirm whether you have tried TF 2.10V and whether problem exists > there or not. Also please confirm the tensorflow_hub version you installed > and command you used for installing tensorflow_hub. > > Thankyou! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >   * Santiago Restrepo GarcÃ­a.* *Mechatronics engineer.*","Hi  , As Tf 2.10 works fine for you it shall work for TF 2.11V also with required configurations. Could you please confirm whether we can close the issue now ?","Hi, What would be those required configurations?","Hi  , The problem might be due to CUDA path.Could you please check the CUDA path using `whereis cuda` command. It seems there is change of CUDA path with installations.Please refer to  ubuntu forum comment. If you find the CUDA path as usr/lib/cuda then symlink using `sudo ln s /usr/lib/cuda/ /usr/local/cuda` should work.Please try symlink using the mentioned command and let us know if it works. I tried the code on a GCP VM with TF 2.11 and it works fine.  ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
846,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA tools build broken (//tensorflow/compiler/xla/tools:replay_computation_gpu ) on  tensorflow master branch)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version build from commit: a3f3bb0eeda308acb970478034449d4d272cabae  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.5 LTS (Focal Fossa)  Mobile device _No response_  Python version 3.8  Bazel version 5.3.0  GCC/Compiler version gcc (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell bazel build  verbose_failures  //tensorflow/compiler/xla/tools:replay_computation_gpu    Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shawnwang18,XLA tools build broken (//tensorflow/compiler/xla/tools:replay_computation_gpu ) on  tensorflow master branch,Click to expand!    Issue Type Bug  Source source  Tensorflow Version build from commit: a3f3bb0eeda308acb970478034449d4d272cabae  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.5 LTS (Focal Fossa)  Mobile device _No response_  Python version 3.8  Bazel version 5.3.0  GCC/Compiler version gcc (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell bazel build  verbose_failures  //tensorflow/compiler/xla/tools:replay_computation_gpu    Relevant log output  ,2022-12-12T03:24:33Z,stat:awaiting tensorflower type:bug comp:xla TF 2.11,closed,0,21,https://github.com/tensorflow/tensorflow/issues/58851,Seems that recent commits to `tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc` has broken the build?  bazel build  verbose_failures  //tensorflow/compiler/xla/tools:replay_computation_gpu ,"FYI, Nov17's commit 82fc78bb3c200c2c8b57fd09b74d89942dbc9b3c works. ","Likely fallout from 944586a977dd12db51b388d9fa029aae454db927. Adding the needed `cc_impl` proto dependencies should do the trick, but I'm not exactly sure what the desired layout for these BUILD rules is.", Could you take look at this issue?  seems that the broken is related to your commits to `tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc`,  I tried to reproduce the issue on Colab but got a different output. Please find the gist here for reference. Could you please look into this. Thank you!,">  I tried to reproduce the issue on Colab but got a different output. Please find the gist here for reference. Could you please look into this. Note, the colab checkout TF 2.11. The issue reported here is about the master branch, not the last release."," Yes, I can also successfully build r2.11, the failure is on `master` branch.  ",>  Could you take look at this issue? seems that the broken is related to your commits to `tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc` Apologies for the slow reply. I can't see how my commit might have caused this error.  's theory looks a lot more likely to me.,  ! Could you look at this issue. Thank you!,`//tensorflow/compiler/xla/tools:run_hlo_module` is also broken. Can the build of one of those 2 tools (or both) be included in the CI? It isn't great to have our basic tools broken for over a week now.,"> Can the build of one of those 2 tools (or both) be included in the CI? It really should be. Maybe the issue is that there are no tests for them, and OSS build only builds those files which are required for tests?",Filed b/263465207 to track internally.,"Any update? I tried locally, now it build, but without the CUDA backend included: ",This is weird =/ Does run_hlo_module work?,Here is some instruction to reproduce the issue:  Current output:  UPDATE: Similar errors with: `bazel build verbose_failures c opt //tensorflow/compiler/xla/tools:run_hlo_module`,"Sorry, I was out for a few weeks. Does 333d4220e7a5963733b21686d803da1648ee3186 fix the issues you're seeing?","Now run_hlo_module compile, but replay_computation still doesn't compile, but have a much shorter, new error: ",Now run_hlo_module is broken again on this commit:  Error:  The instruction to reproduce: ,I think 6f76ee24b2ec75730212471a734ecc09a13fd030 fixed this. I'll work on a follow up change to add coverage for these binaries so a similar situation won't happen in the future. Sorry about this! Please let me know if it's back to a working state for you. ,I tried it and it works now. Thanks for the help.,Are you satisfied with the resolution of your issue? Yes No
1964,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow compilation fails on Raspberry 32bit compute module with message ""error: static assertion failed: Source and destination types should have equal sizes."")ï¼Œ å†…å®¹æ˜¯ (I am trying to compile Tensorflow on Rpi 32bit 4GB board and encountered compilation failure after 14 hours. I have been following https://qengineering.eu/installtensorflow2.1.0onraspberrypi4.html and compiled Bezel before this. Following is the git HEAD of Tensorflow code I sync'ed: commit fcdf659347024dc5a3130e866ba3dde10bac72b0 (HEAD, tag: v2.5.0rc3) Merge: a7b36464407 efe04c1b561 Author: Mihai Maruseac  Date:   Tue May 4 16:35:43 2021 0700     Merge pull request CC(Update version numbers for TensorFlow 2.5.0rc3) from tensorflowjenkins/versionnumbers2.5.0rc313515     Update version numbers for TensorFlow 2.5.0rc3 This is the command I used: $ sudo bazel host_jvm_args=Xmx1624m build \              config=noaws \              config=nogcp \              config=nohdfs \              config=nonccl \              config=monolithic \              config=v2 \              local_cpu_resources=1 \              copt=mfpu=neonvfpv4 \              copt=ftreevectorize \              copt=funsafemathoptimizations \              copt=ftreeloopvectorize \              copt=fomitframepointer \              copt=DRASPBERRY_PI \              host_copt=DRASPBERRY_PI \              linkopt=Wl,latomic \              host_linkopt=Wl,latomic \              //tensorflow/tools/pip_package:build_pip_package Following is the snippet from ""lscpu"" command output of my Rpi: Architecture:                    armv7l Byte Order:                      Little Endian CPU(s):                          4 Online CPU(s) list:             03 Thread(s) per core:              1 Core(s) per socket:              4 Socket(s):        )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,schindam,"Tensorflow compilation fails on Raspberry 32bit compute module with message ""error: static assertion failed: Source and destination types should have equal sizes.""","I am trying to compile Tensorflow on Rpi 32bit 4GB board and encountered compilation failure after 14 hours. I have been following https://qengineering.eu/installtensorflow2.1.0onraspberrypi4.html and compiled Bezel before this. Following is the git HEAD of Tensorflow code I sync'ed: commit fcdf659347024dc5a3130e866ba3dde10bac72b0 (HEAD, tag: v2.5.0rc3) Merge: a7b36464407 efe04c1b561 Author: Mihai Maruseac  Date:   Tue May 4 16:35:43 2021 0700     Merge pull request CC(Update version numbers for TensorFlow 2.5.0rc3) from tensorflowjenkins/versionnumbers2.5.0rc313515     Update version numbers for TensorFlow 2.5.0rc3 This is the command I used: $ sudo bazel host_jvm_args=Xmx1624m build \              config=noaws \              config=nogcp \              config=nohdfs \              config=nonccl \              config=monolithic \              config=v2 \              local_cpu_resources=1 \              copt=mfpu=neonvfpv4 \              copt=ftreevectorize \              copt=funsafemathoptimizations \              copt=ftreeloopvectorize \              copt=fomitframepointer \              copt=DRASPBERRY_PI \              host_copt=DRASPBERRY_PI \              linkopt=Wl,latomic \              host_linkopt=Wl,latomic \              //tensorflow/tools/pip_package:build_pip_package Following is the snippet from ""lscpu"" command output of my Rpi: Architecture:                    armv7l Byte Order:                      Little Endian CPU(s):                          4 Online CPU(s) list:             03 Thread(s) per core:              1 Core(s) per socket:              4 Socket(s):        ",2022-12-10T20:31:15Z,stat:awaiting response type:build/install comp:micro TF 2.5,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58845,Hi  ! Sorry for the late response. Please refer documentations from TFliteruntime for linux devices and Cmake for arm device to raspberry usage. It saves space and speed up inference time by using Lite run time (in the edge device). Thank you!,"Thank you. As the reference links are changed, a different direction is now pursued. We can close this ticket.",Are you satisfied with the resolution of your issue? Yes No
1891,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The output_details and actual output of my converted TF lite model are of different shape )ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Win10**  TensorFlow installation (pip package or built from source): **pip**  TensorFlow library (version, if pip package or github SHA, if built from source): **2.10.1**  2. Code Colab notebook that includes all from model creation to testing the converted model  3. Problem Setting I am trying to build a BERTbased model that does tokenlevel prediction. So the output of my model should be (batch_size, input_length, num_classes). In my case, for a single prediction, that is (1, 128, 6). In my understanding, higher level APIs like the BertNLClassifier API do not work for this use case as they can do just one prediction for the whole input. That is why I have to build it myself. I used TFBert from the transformers library and put a custom classification head on top. Checking the output:  > KerasTensor(type_spec=TensorSpec(shape=(None, 128, 6), dtype=tf.float32, name=None), name='dense/BiasAdd:0', description=""created by layer 'dense'"") So output is exactly what I want, great! After checking the output shape I converted that model to TF lite format like this:   4. Failure after conversion After converting the model I ran it in my mobile Android app and was surprised when I got this error message: > E/tf_predict: Message: Cannot copy from a TensorFlowLite tensor (StatefulPartitionedCall:0) with shape [1, 1, 8] to a Java object with shape [1, 128, 8]. So apparently my output shape changed after conversion. Strange, I thought. So after that did not work I read about the python tf lite interpreter API , which I th)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DerEchteFeuerpfeil,The output_details and actual output of my converted TF lite model are of different shape ," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Win10**  TensorFlow installation (pip package or built from source): **pip**  TensorFlow library (version, if pip package or github SHA, if built from source): **2.10.1**  2. Code Colab notebook that includes all from model creation to testing the converted model  3. Problem Setting I am trying to build a BERTbased model that does tokenlevel prediction. So the output of my model should be (batch_size, input_length, num_classes). In my case, for a single prediction, that is (1, 128, 6). In my understanding, higher level APIs like the BertNLClassifier API do not work for this use case as they can do just one prediction for the whole input. That is why I have to build it myself. I used TFBert from the transformers library and put a custom classification head on top. Checking the output:  > KerasTensor(type_spec=TensorSpec(shape=(None, 128, 6), dtype=tf.float32, name=None), name='dense/BiasAdd:0', description=""created by layer 'dense'"") So output is exactly what I want, great! After checking the output shape I converted that model to TF lite format like this:   4. Failure after conversion After converting the model I ran it in my mobile Android app and was surprised when I got this error message: > E/tf_predict: Message: Cannot copy from a TensorFlowLite tensor (StatefulPartitionedCall:0) with shape [1, 1, 8] to a Java object with shape [1, 128, 8]. So apparently my output shape changed after conversion. Strange, I thought. So after that did not work I read about the python tf lite interpreter API , which I th",2022-12-10T18:25:08Z,stat:awaiting response type:bug comp:lite TFLiteConverter Fixed in Nightly TF 2.10,closed,1,5,https://github.com/tensorflow/tensorflow/issues/58844,Hi  ! This is a known bug in TFLite conversion process.  Usual work around for these  issues are to use model binding/meta data writer/ signature to avoid distortion of input/output shape during conversion. Did you check in 2.11 /nightly version too ?. A toy Bert model from your side would be handy to replicate this issue. Thank you!,"Hey , all the necessary code is provided in the colab notebook that I linked in the very top of my issue. It includes the model, the model saving, the conversion and the testing. TF 2.11 still fails, but I just tested it with tfnightly `2.12.0dev20221213` and it actually works there! (See screenshots) !image !image Marking this as closed then?","HI  ! Thanks for sharing your update with respect to nightly version. Yeah! I agree it is a valid bug since 2.9/2.10 version. But, Can we consider this as fixed in nightly version (it was recently addressed in 2.12 dev) ? Thank you!",Consider this fixed with the nightly version.,Are you satisfied with the resolution of your issue? Yes No
482,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Documentation: Fix broken Classify CIFAR-10 with XLA build)ï¼Œ å†…å®¹æ˜¯ ( Upgrade TF to 2.11 (Colab uses 2.9.2)  Use the new Keras Optimizer API (TF 2.10+)    This fixes the broken build on https://www.tensorflow.org/xla/tutorials/autoclustering_xla    fyi Not recommending users to use keras.optimizers.legacy)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,8bitmp3,Documentation: Fix broken Classify CIFAR-10 with XLA build, Upgrade TF to 2.11 (Colab uses 2.9.2)  Use the new Keras Optimizer API (TF 2.10+)    This fixes the broken build on https://www.tensorflow.org/xla/tutorials/autoclustering_xla    fyi Not recommending users to use keras.optimizers.legacy,2022-12-10T00:38:54Z,awaiting review ready to pull comp:xla size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/58842,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB
677,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Process finished with exit code 134 when applying random transformations for data augmentation on a dataset on apple m1 Mac)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.2  Custom Code Yes  OS Platform and Distribution macOS13.0.1arm64arm64bit  Mobile device No  Python version 3.10.8  Bazel version No  GCC/Compiler version No  CUDA/cuDNN version No  GPU model and memory No  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kp3393,Process finished with exit code 134 when applying random transformations for data augmentation on a dataset on apple m1 Mac,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.2  Custom Code Yes  OS Platform and Distribution macOS13.0.1arm64arm64bit  Mobile device No  Python version 3.10.8  Bazel version No  GCC/Compiler version No  CUDA/cuDNN version No  GPU model and memory No  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-09T21:07:48Z,stat:awaiting response type:bug stale comp:keras subtype:macOS TF 2.9,closed,0,13,https://github.com/tensorflow/tensorflow/issues/58840,Hi  ! Sorry for the late response. I could replicate this issue with 2.9 and 2.10 macos wheel and getting an accuracy of 0.7446 with 2.11 macos wheel. I just changed the second step as below. `python m pip install tensorflowmacos==2.11` Attached screenshot for reference. Could you update from your side. Thank you!,"  No worries! I appreciate you helping me out. Currently, people are facing issues installing `tensorflowmacos==2.11` on M1 macs. Here is the link to the problem description https://developer.apple.com/forums/thread/721619.  I'm also facing the same problem with installing `tensorflowmacos==2.11` on the M1 mac. Perhaps, you can guide me in installing 2.11 on the M1 so that I can solve this issue with the data augmentation.",Ok  ! Here were my commands for tensorflowmacos.11 installation.  Reference.  Thank you!,"Dear , Thank you for your prompt reply.  I follow the same procedure; instead of the bash script, I use Miniconda3 macOS Apple M1 64bit pkg installer. From my understanding, it should not make a difference.  Can you also please share your OS Platform and Distribution along with the list of packages? Thank you!","Ok  ! Here is my os details .  and list of packages when i enter ""pip list""  Please check with TF 2.11 and metal plugin instructions as mentioned in above comment and let us know whether the issue still persists. Thank you!","Hello , I see where the problem is. You are still at macOSÂ Monterey (your platform is **macOS12.6.1arm64arm64bit**).  Please, refer to my problem description. I have mentioned my platform to be **macOS13.0.1arm64arm64bit**. If you update to macOS Ventura, there is a high possibility that you will face the same issues.",Ok  ! Thanks for the clarification. I will do an OS update to MacOs 13.0.1 tonight and let you know.  ! Could you look at this issue. Thank you!,"Hello  , Any updates? I will be grateful for some help to solve this issue. Thank you for your time!","Commenting on the thread to prevent it from closing as stale. Please, will be grateful for any updates. ",Apologies for the delayed response.  It looks like the memory issue on your system side.  Could you please try with less number of epochs or try the same example mentioned in the document here https://www.tensorflow.org/tutorials/images/data_augmentationoverviewwith 5 epochs and let us know if you still face the error. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1797,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`mypy` compatibility: missing `py.typed`)ï¼Œ å†…å®¹æ˜¯ ( Issue Type Feature Request  Source binary  Tensorflow Version tf 2.8, tf 2.10  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04, macOS Ventura arm  Mobile device _No response_  Python version 3.7, 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? `mypy` is one of the great tools that many people use to run type checking via type annotations. Unfortunatelly, `tensorflow` does not play very nicely with it. As specified in the examples, many `tensorflow`based objects are seen as `Any` by `mypy`. This can lead to various unexpected (and confusing) behaviour in the code that depends on `tensorflow`.  Extra change required IIUC this is an invalid type annotation according to `mypy`: https://github.com/tensorflow/tensorflow/blob/bff338f421322c2ccd1d7e2c5e24ff6c1b0c133c/tensorflow/python/ops/ragged/dynamic_ragged_shape.pyL206 Changing to                 static_inner_shape: Any = None): should solve the problem (of course a more specific type annotation would be better).  Proposed solution I suspect that this is because a single file is missing to make `tensorflow` type annotations recognised by `mypy`: `py.typed`. Adding that file resolves the issue, and stops `mypy` from treating tensorflow entities as `Any`. Please kindly let me know if I missed anything.  Standalone code to reproduce the issue  Basic example  Without `py.typed`:   With `py.typed`:    More complicated example (comments explain what is seen without `py.typed`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Corwinpro,`mypy` compatibility: missing `py.typed`," Issue Type Feature Request  Source binary  Tensorflow Version tf 2.8, tf 2.10  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04, macOS Ventura arm  Mobile device _No response_  Python version 3.7, 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? `mypy` is one of the great tools that many people use to run type checking via type annotations. Unfortunatelly, `tensorflow` does not play very nicely with it. As specified in the examples, many `tensorflow`based objects are seen as `Any` by `mypy`. This can lead to various unexpected (and confusing) behaviour in the code that depends on `tensorflow`.  Extra change required IIUC this is an invalid type annotation according to `mypy`: https://github.com/tensorflow/tensorflow/blob/bff338f421322c2ccd1d7e2c5e24ff6c1b0c133c/tensorflow/python/ops/ragged/dynamic_ragged_shape.pyL206 Changing to                 static_inner_shape: Any = None): should solve the problem (of course a more specific type annotation would be better).  Proposed solution I suspect that this is because a single file is missing to make `tensorflow` type annotations recognised by `mypy`: `py.typed`. Adding that file resolves the issue, and stops `mypy` from treating tensorflow entities as `Any`. Please kindly let me know if I missed anything.  Standalone code to reproduce the issue  Basic example  Without `py.typed`:   With `py.typed`:    More complicated example (comments explain what is seen without `py.typed`) ",2022-12-09T20:31:52Z,stat:awaiting response type:feature stale comp:ops TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58837,  I was able to reproduce the issue on Colab using TF v2.11. Please find the gist here and attachment for reference. Thank you!,Could we please have an update for this?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
690,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exporting ğŸ¤— TF BERT model with multiple dynamic axes to MLIR fails with invalid ops generated)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11  Custom Code Yes  OS Platform and Distribution Fedora 37  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,mfuntowicz,Exporting ğŸ¤— TF BERT model with multiple dynamic axes to MLIR fails with invalid ops generated,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11  Custom Code Yes  OS Platform and Distribution Fedora 37  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-09T18:16:22Z,stat:awaiting response type:bug stale comp:model TF 2.11,closed,1,10,https://github.com/tensorflow/tensorflow/issues/58835,"Hmm, any chance this might be a transformers version mismatch error? ",  I was able to reproduce the issue on Colab using TF v2.11. Please find the gist here for reference. Thank you!," I was able to replicate the issue on colab using TF v2.11 , please find the gist here. Could you check the compatible transformer if it is mismatched . Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No," I confirm I'm using the same versions as yours, is there any way to overcome this issue?","Hi  , I tried to replicate this issue and I am getting different error below as per attached gist. `CompilerToolError: Error invoking IREE compiler tool ireeimporttf ` I am not sure whether this is related to tensorflow or not. Tensorflow supports XLA but not sure about this iree compiler. Could you please explain or submit a code snippet to reproduce this issue without external dependencies that are not mentioned in Tensorflow. Thanks!",This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1876,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Select TensorFlow opï¼ˆFlexTensorArrayV3ï¼‰is not supported by this interpreter)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (Win10):  TensorFlow version (TensorFlow_v1): **Provide the text output from tflite_convert** I have already followed the instructions: https://www.tensorflow.org/lite/guide/ops_select When I'm trying to convert, I used:  It's all right when converting. Then, I add the ""org.tensorflow:tensorflowliteselecttfops"" dependency in Android Studio by using the following code: `implementation 'org.tensorflow:tensorflowlite:2.11.0` `implementation 'org.tensorflow:tensorflowliteselecttfops:2.11.0` However,there is something wrong. This is the model that I used https://github.com/kwea123/fish_detection/tree/master/fish_inception_v2_graph2/frozen_inference_graph.pb Here is my error in Android Studio: E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select E/tflite: Node number 3 (FlexTensorArrayV3) failed to prepare. E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select E/tflite: Node number 3 (FlexTensorArrayV3) failed to prepare. E/TaskJniUtils: Error getting native address of native library: ta)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Clara2333333,Select TensorFlow opï¼ˆFlexTensorArrayV3ï¼‰is not supported by this interpreter,"**System information**  OS Platform and Distribution (Win10):  TensorFlow version (TensorFlow_v1): **Provide the text output from tflite_convert** I have already followed the instructions: https://www.tensorflow.org/lite/guide/ops_select When I'm trying to convert, I used:  It's all right when converting. Then, I add the ""org.tensorflow:tensorflowliteselecttfops"" dependency in Android Studio by using the following code: `implementation 'org.tensorflow:tensorflowlite:2.11.0` `implementation 'org.tensorflow:tensorflowliteselecttfops:2.11.0` However,there is something wrong. This is the model that I used https://github.com/kwea123/fish_detection/tree/master/fish_inception_v2_graph2/frozen_inference_graph.pb Here is my error in Android Studio: E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select E/tflite: Node number 3 (FlexTensorArrayV3) failed to prepare. E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select E/tflite: Node number 3 (FlexTensorArrayV3) failed to prepare. E/TaskJniUtils: Error getting native address of native library: ta",2022-12-09T13:28:17Z,type:bug comp:lite TF 2.11,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58833,Hi  ! I think it is a compatibility issue coming tf.compat.v1.lite.TFLiteConverter.from_frozen_graph with 2.x select ops syntax. Can you convert the above frozen graph  to 2.x saved model format  using tf.io.gfile.Gfile and test the conversion again  with select ops syntax  to see whether the error exists. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,> Hi  ! I think it is a compatibility issue coming tf.compat.v1.lite.TFLiteConverter.from_frozen_graph with 2.x select ops syntax. >  > Can you convert the above frozen graph to 2.x saved model format using tf.io.gfile.Gfile and test the conversion again with select ops syntax to see whether the error exists. >  > Thank you! I'll try this method~ Thanks for your reply!!!, ! We are checking to see whether you still need help in this issue .  Could you confirm whether the issue is resolved or not . Thank you!,>  ! >  > We are checking to see whether you still need help in this issue . Could you confirm whether the issue is resolved or not . >  > Thank you! The issue is resolved by using other ways to transfer. Thanks!, ! Thanks for the confirmation. Marking this as resolved from above comment. Feel free to open if you still need further assistance. Thank you!,Are you satisfied with the resolution of your issue? Yes No
601,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.compat.v1.profiler.profile don't report flops of ops in tf.cond)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Bug  Source source  Tensorflow Version tf 2.5.1   Custom Code Yes  Current Behaviour? A bug happened! I use a tf.cond op in model:  there are a subtract op and a multiply op in tf.cond. then i  try to get flops of ""model"":   I print the flops detail log and found the ops in tf.cond are ignored, only an add op's flops is reported  ```)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,zzq96,tf.compat.v1.profiler.profile don't report flops of ops in tf.cond,"  Issue Type Bug  Source source  Tensorflow Version tf 2.5.1   Custom Code Yes  Current Behaviour? A bug happened! I use a tf.cond op in model:  there are a subtract op and a multiply op in tf.cond. then i  try to get flops of ""model"":   I print the flops detail log and found the ops in tf.cond are ignored, only an add op's flops is reported  ```",2022-12-09T02:50:03Z,stat:awaiting response type:bug stale comp:ops TF 2.5,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58829,> Click to expand!,> > Click to expand! hiï¼Œi have updated," I was able to run the code without any error using TF v2.5 and faced a warning using TF v2.11 on colab , please find the attached gists. Could you please check and confirm the same? Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1887,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Significant slowdown in Keras model.fit() on simple problem when using validation data)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Ubuntu 18.04  Mobile device _No response_  Python version 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2 / 8.1.0  GPU model and memory NVIDIA Quadro GV100 (32 GB)  Current Behaviour? I observe a significant slowdown when using Keras model.fit() to train a simple 3layer fullyconnected network when the validation_data parameter is set.  A typical run of the provided code on my machine results in the following output:  The validation data set is 1/8 the size of the training dataset.  I don't expect validation to be free, but it should not result in a 10x slowdown. Some notes / things I've tried:   Setting validation_batch_size makes no difference.   Using a single NumPy array to hold both the training and validation datasets    and using validation_split instead of validation_data also makes no    difference.   I found the possibly related issue CC(Keras model training is slow without ""tf.compat.v1.disable_eager_execution()""), which suggests that it might be a    problem related to eager execution.  The poster of that issue is able to fix    things by calling tf.compat.v1.disable_eager_execution(), but if I try this    in my own code with TF 2.11.0, I run into errors (not really surprising).   One of the respondents on that same issue suggested swapping out the NumPy    arrays for tf.data.Dataset objects.  I found this made no difference, even    using `.prefetch(tf.data.AUTOTUNE)` as recommended.   Deactivating )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,aaustin141,Significant slowdown in Keras model.fit() on simple problem when using validation data,"Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Ubuntu 18.04  Mobile device _No response_  Python version 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2 / 8.1.0  GPU model and memory NVIDIA Quadro GV100 (32 GB)  Current Behaviour? I observe a significant slowdown when using Keras model.fit() to train a simple 3layer fullyconnected network when the validation_data parameter is set.  A typical run of the provided code on my machine results in the following output:  The validation data set is 1/8 the size of the training dataset.  I don't expect validation to be free, but it should not result in a 10x slowdown. Some notes / things I've tried:   Setting validation_batch_size makes no difference.   Using a single NumPy array to hold both the training and validation datasets    and using validation_split instead of validation_data also makes no    difference.   I found the possibly related issue CC(Keras model training is slow without ""tf.compat.v1.disable_eager_execution()""), which suggests that it might be a    problem related to eager execution.  The poster of that issue is able to fix    things by calling tf.compat.v1.disable_eager_execution(), but if I try this    in my own code with TF 2.11.0, I run into errors (not really surprising).   One of the respondents on that same issue suggested swapping out the NumPy    arrays for tf.data.Dataset objects.  I found this made no difference, even    using `.prefetch(tf.data.AUTOTUNE)` as recommended.   Deactivating ",2022-12-08T22:13:59Z,comp:keras type:performance TF 2.11,closed,1,3,https://github.com/tensorflow/tensorflow/issues/58828,Hi  ! I was able to replicate this issue in Colab( approx 3.5 times slow in 2.11 version). 1/10 inference time compared to  inference time without dataset might be valid issue. But  3.5 times in validation dataset looks reasonable compared to training without validation dataset. Please post this issue on kerasteam/keras repo To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 .  Thank you!,"Thank you for getting back to me, .  It's good to see that someone else can replicate this.  I have crossposted this issue to the Keras repository as you instructed (see kerasteam/tfkeras CC(Cannot make tensorflow to  use a local gcc installation?)). That said, I must take issue with this conclusion: > But 3.5 times in validation dataset looks reasonable compared to training > without validation dataset. The validation data set is 1/8 the size of the training set.  Perhaps I'm missing something, but it doesn't seem reasonable to me to incur even ""only"" a 3.5x performance penalty to process just 12.5% more data. (Actually, it's even worse:  as I mentioned on the Keras ticket, I see the same slowdown even if I reduce the validation dataset size to a single example.  So it's more like a 3.5x penalty to process 0.05% more data...)",Ok  ! Thanks for the clarification.  Moving this issue to closed status as it will be tracked in Keras  CC(Feature Request: Training on device). Thank you!
622,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can't use adpt function to normarlization the data)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11  GPU model and memory 3060  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jermmy19998,Can't use adpt function to normarlization the data,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11  GPU model and memory 3060  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-08T14:29:54Z,type:bug comp:keras TF 2.8,closed,1,1,https://github.com/tensorflow/tensorflow/issues/58821,Are you satisfied with the resolution of your issue? Yes No
615,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Installation process on dead pip)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version 11.3.0  CUDA/cuDNN version _No response_  GPU model and memory None  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pedrodicati,Installation process on dead pip,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version 11.3.0  CUDA/cuDNN version _No response_  GPU model and memory None  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-08T12:15:37Z,type:build/install subtype: ubuntu/linux TF 2.11,closed,0,2,https://github.com/tensorflow/tensorflow/issues/58820,"I solved the problem, I believe it is a memory problem. I did the following steps from the following link: https://www.anaconda.com/tensorflowinanaconda/  Then I got the following error when importing tensorflow:  Then I did the following: ",Are you satisfied with the resolution of your issue? Yes No
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update label_image.py)ï¼Œ å†…å®¹æ˜¯ (if we're using TensorFlow 2.0, we should do the following changes too: tf.read_file() becomes> tf.io.read_file() tf.gfile.GFile(label_file) becomes> tf.io.gfile.GFile(label_file))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yo08,Update label_image.py,"if we're using TensorFlow 2.0, we should do the following changes too: tf.read_file() becomes> tf.io.read_file() tf.gfile.GFile(label_file) becomes> tf.io.gfile.GFile(label_file)",2022-12-07T18:43:31Z,size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/58815,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deploy yolo5 model into TensorFlow Lite Object Detection Android)ï¼Œ å†…å®¹æ˜¯ (Hello Tensorflow team! I checked your real time object detection app which works good with the initial models (MobileNet V1, EfficientNet Lite0, EfficientNet Lite1, EfficientNet Lite2). https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android But I receive errors when I try to add other model for example yolo5s model trained on COCO dataset which converted from **pt** format to **tflite** with export.py script https://github.com/ultralytics/yolov5/blob/master/export.py Below you can find converted models for yolo5s (fp16 and int8 options), I already checked they are working fine with detect.py. https://github.com/HripsimeS/Projects/blob/main/yolov5sfp16.tflite https://github.com/HripsimeS/Projects/blob/main/yolov5sint8.tflite Is it possible to deploy/integrate yolo5 model into your real time object detection app? If yes, can you please check if you can deploy into app one of those two models (fp16 or int8) I shared with you above. In case if it works for you, can you share your experience what did you exactly modify in initial TensorFlow Lite Object Detection app scripts. Thank you in advance!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,HripsimeS,Deploy yolo5 model into TensorFlow Lite Object Detection Android,"Hello Tensorflow team! I checked your real time object detection app which works good with the initial models (MobileNet V1, EfficientNet Lite0, EfficientNet Lite1, EfficientNet Lite2). https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android But I receive errors when I try to add other model for example yolo5s model trained on COCO dataset which converted from **pt** format to **tflite** with export.py script https://github.com/ultralytics/yolov5/blob/master/export.py Below you can find converted models for yolo5s (fp16 and int8 options), I already checked they are working fine with detect.py. https://github.com/HripsimeS/Projects/blob/main/yolov5sfp16.tflite https://github.com/HripsimeS/Projects/blob/main/yolov5sint8.tflite Is it possible to deploy/integrate yolo5 model into your real time object detection app? If yes, can you please check if you can deploy into app one of those two models (fp16 or int8) I shared with you above. In case if it works for you, can you share your experience what did you exactly modify in initial TensorFlow Lite Object Detection app scripts. Thank you in advance!",2022-12-07T13:56:27Z,stat:awaiting response type:support stale comp:lite comp:lite-examples TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58812,"The app use models with postprocessing part (which includes nonmaximum suppression, NMS). You may need to change output processing part and implement NMS."," thank you very much for your reply. Can you let me know where I need to change output processing part and implement NMS. If there are some examples how to do it, can you please share with me. Look forward to hearing from you!",Hi  ! Sorry for the late response.  Please have a look at the relevant thread for implementation which gives instructions for both CPU and GPU implementation of Yolo v5 models through Metadata writer. (Unifies models results with respect to different android architecture) You can discuss any blockers/results on TFForum for further assistance. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1844,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite example label_image run err)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  arm64 buildroot  TensorFlow installation (pip package or built from source):  TensorFlow lite  TensorFlow library (version, if pip package or github SHA, if built from source):  2.10  2. describe Hi, I am trying to use tensorflow lite on an arm64 device, I compiled using the instructions on this page to compileï¼šhttps://www.tensorflow.org/lite/guide/build_cmake_arm?hl=zhcn `export ARMCC_PREFIX=~/tool_chain/gccarm10.32021.07x86_64aarch64nonelinuxgnu/bin/aarch64nonelinuxgnu && export ARMCC_FLAGS=""funsafemathoptimizations""  && cmake DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc  DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++  DCMAKE_C_FLAGS=""${ARMCC_FLAGS}""  DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   DCMAKE_SYSTEM_NAME=Linux   DCMAKE_SYSTEM_PROCESSOR=aarch64 DTFLITE_ENABLE_XNNPACK=ON DTFLITE_ENABLE_GPU=ON ../tensorflow/lite/` The final compilation libtensorflowlite.a, label_image , etc. And then I do this: scp label_image into my device and I try to run it: ./label_image tflite_model testdata/mobilenet_v1_1.0_224.tflite  labels testdata/labels.txt image testdata/grace_hopper.bmp It came up with this log: INFO: Loaded model testdata/mobilenet_v1_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,zack-huangzihan,TensorFlow Lite example label_image run err," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  arm64 buildroot  TensorFlow installation (pip package or built from source):  TensorFlow lite  TensorFlow library (version, if pip package or github SHA, if built from source):  2.10  2. describe Hi, I am trying to use tensorflow lite on an arm64 device, I compiled using the instructions on this page to compileï¼šhttps://www.tensorflow.org/lite/guide/build_cmake_arm?hl=zhcn `export ARMCC_PREFIX=~/tool_chain/gccarm10.32021.07x86_64aarch64nonelinuxgnu/bin/aarch64nonelinuxgnu && export ARMCC_FLAGS=""funsafemathoptimizations""  && cmake DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc  DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++  DCMAKE_C_FLAGS=""${ARMCC_FLAGS}""  DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   DCMAKE_SYSTEM_NAME=Linux   DCMAKE_SYSTEM_PROCESSOR=aarch64 DTFLITE_ENABLE_XNNPACK=ON DTFLITE_ENABLE_GPU=ON ../tensorflow/lite/` The final compilation libtensorflowlite.a, label_image , etc. And then I do this: scp label_image into my device and I try to run it: ./label_image tflite_model testdata/mobilenet_v1_1.0_224.tflite  labels testdata/labels.txt image testdata/grace_hopper.bmp It came up with this log: INFO: Loaded model testdata/mobilenet_v1_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get XNNPACK profile information. ERROR: failed to get",2022-12-07T09:42:23Z,comp:lite TFLiteConverter TF 2.10,closed,0,2,https://github.com/tensorflow/tensorflow/issues/58810,"if you `./label_image help`, you can see there are `g` and `use_gpu=true` you can use to enable GPU delegate.","ok,thank"
1959,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MirroredStrategy error INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds when training a model, most related to batch size strategy across all GPUs)ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: ngc    **TensorFlow version (use command below)**: 2.10    **Python version**: 3.8.10    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**: 11.8    **GPU model and memory**: A100 80G (8 Gpus)    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,xiaogangzhu,"MirroredStrategy error INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds when training a model, most related to batch size strategy across all GPUs","Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: ngc    **TensorFlow version (use command below)**: 2.10    **Python version**: 3.8.10    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**: 11.8    **GPU model and memory**: A100 80G (8 Gpus)    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You",2022-12-07T03:44:27Z,stat:awaiting response type:support stale comp:dist-strat TF 2.11,closed,0,26,https://github.com/tensorflow/tensorflow/issues/58809,"Hi  , You have used `strategy = tf.distribute.MirroredStrategy()` which is ideally suitable for Single Machine with multiple replicas. Are the mentioned 8 no GPUs are on same machine ? If yes you can use this API.  If you are using GPUs on multiple machines in that case you should use  `tf.distribute.MultiWorkerMirroredStrategy()` Please refer to the attached links 1 & 2 and demo for multi worker training for reference. Please review and come back. Thankyou!","Hi, 8 GPUs are on the same machine. It goes well when I just use strategy = tf.distribute.MirroredStrategy() until the training goes to the last step whose batch size is smaller. For example, batch size is 4096, and my data has 10000 samples, for each step the input shape would be (4096,40,4) (4096,40,4)  (4096,40,4) (1808,40,4), and the error shows up at the third step.","Hi  , Could you please cross check this. Yu have taken `BATCH_SIZE_PER_REPLICA = 4096` and `strategy.num_replicas_in_sync =8 ` since you mentioned 8 GPUs in your system then Global `BATCH_SIZE = 32768` which becomes more than your data size=10000. Could you please make `BATCH_SIZE_PER_REPLICA = 4096/8 (=512)` then test the code and let us know if still problem persists. Thankyou!","Actually my data size is much larger than 10000, it is 3000000+, so batch size 4096*8=32768 is not that problem I think. Can you guys reproduce the error? Thanks!","Hi  , Thanks for confirmation.If you could provide minimum inputs required we definitely try to replicate the issue. Please provide all the minimal resources required for replicating the issue. Thankyou!",Also could you please confirm the following things: 1.For single GPU as you confirmed  the code works fine.Please confirm the exact code that you used. 2.Please confirm the output of `print('Number of devices: {}'.format(strategy.num_replicas_in_sync))` in Multi GPU code.,"Hi, I am running my code on a 8GPU machine, the environment is from https://docs.nvidia.com/deeplearning/frameworks/tensorflowreleasenotes/rel2210.htmlrel2210 So I am running the code on the cloud which gives me access to 8 GPUS with this docker images.  Yes, I confirm that for a single GPU the code goes well. print('Number of devices: {}'.format(strategy.num_replicas_in_sync)) this command just prints 8.","Hi  , Could you please provide `val.csv` if not full at least with some dummy data with feature details that you are using.I will try to replicate the issue and try to debug it for resolving the error . Thankyou!",test.zip,"Hi  , Sorry for the delayed response. While checking your environment details and noticed that you are using CUDA 11.8 with TF 2.10V and it may raise compatibility issues.Please refer to the tested configurations. Either you use CUDA 11.2 for TF2.10/TF2.11 versions or you can use tfnightly with CUDA 11.8. Please try any of the above mentioned configurations and let us know if still having any problem? Thankyou!","Hi, So I am using the official ngc docker image https://docs.nvidia.com/deeplearning/frameworks/tensorflowreleasenotes/rel2210.htmlrel2210,  do you guys tried that and make the conclusion? So to fix this problem, I should reinstall cuda 11.2 instead of cuda 11.8 of the docker image, is that correct?",Do you guys know how to downgrade cuda from 11.8 to 11.2 in the ngc docker image?,Hi I tired TF2.11 and cuda11.2 and the issue still present.,Any update?,"Hi  , Apologies for the delay. I have tried to replicate the issue in a 8GPU VM with TF2.10. Iam getting same error as below.It needs investigation and will update you once got update.  To add more I tested the code on same VM with 2GPUs with and without `validation_data` in `model.fit()` and both cases training is successful.The problem seems to be with sharding.Needs to be analysed more. Could you please test the code on 2GPUs using `strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])` and confirm whether it works fine for you also? Iam attaching the code as gist here).Please check and confirm whether the same being tested by you also.","Yeah, I will try. Thanks!","Hi, 2 GPUs work fine for me. Is there any update about this issue?","Hi, As I doubted the error might be due to batch sizing. I have added drop_reinder=True in batch() and the code works fine on 8 GPU also.Please refer to my changed code.  Please refer to attached output log here.  Hope this solve your issue.Please let us know if it works and comeback if still having problems.","Hi,  drop_reinder=True will not cause error. However, it will lose some data which is not what we want to see. Is there any other solutions? I think it is a bug of tensorflow and if you guys can fix it?","The problem is if I preprocess my data into a npy file (data has been already windowed) and fit in model it will not cause error whatever batch size I use. However, when I am using tf.data.dataset window method and my predefined data generator the error occurs. I prefer the second version because I don't need to save a large npy file. So I was wondering why this error will happened in my code. Thanks!"," , without drop_reminder=True the code will not work as we have to tell the TF what to do with reminder data which is lower than the batch size.Best way is to pass `shuffle=True `to `model.fit` which will shuffle the data in each epoch thereby you are not loosing any data from being trained. Also you can use repeat method on the dataset to repeat the data without loosing reminders. Or you can reduce batch size which may not be reccommended."," , Since I am using tf.data.dataset, the shuffle method of model.fit is not working just as what model.fit documentation said ( This argument is ignored when x is a generator or an object of tf.data.Dataset. ).  The point is, why I got the error when running my code? As we know tensorflow will handle the data even if the sample size can not be divided by the batch size.(e.g. I got 500 samples, and my batch size is 64, the training should go well). I want to know the reason why my code will fail and if this is a bug of tensorflow and should be fixed? Thanks!",">  , Since I am using tf.data.dataset, the shuffle method of model.fit is not working just as what model.fit documentation said ( This argument is ignored when x is a generator or an object of tf.data.Dataset. ).   , In tf.data.Dataset module `shuffle`  is a method which shuffles the data in datapipe line before passing it for training and in `model.fit()` shuffle is an argument and both are not same and works in different way.The argument `shuffle` is ignored if x is a generator or an object of `tf.data.Dataset` in `model.fit()` only and it has nothing to do with the `shuffle()` on `tf.data.Dataset` as it will be applied already before going for training in model.fit() irrespective of shuffle argument value. >The point is, why I got the error when running my code? As we know tensorflow will handle the data even if the sample size can not be divided by the batch size.(e.g. I got 500 samples, and my batch size is 64, the training should go well). I want to know the reason why my code will fail and if this is a bug of tensorflow and should be fixed? Thanks! For this I can give you some inputs though not a direct Answer to do some analysis from your end.There is a note regarding the argument `drop_reminder` as below.  Also from tf.distribute module the input value should have same rank and shape(except in axis direction).Since you are applying `flat_map()` the shape has only one value and it might be causing problem here.If possible can you construct pipeline without flat_map() and with `drop_reimnder=False` and let us know if it works.  Document Reference Alternatively to avoid loss of data in training you can use repeat() method like below and you can reduce `epochs` value also. `val_dataset = val_dataset.cache().repeat(2).batch(4096*9,drop_remainder=True).prefetch(buffer_size=tf.data.AUTOTUNE) ` Please comeback with tested results. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1835,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow does not build on macOS)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution macOS 12.6.1 21G217 x86_64  Mobile device _No response_  Python version 3.10, 3.9, 3.8  Bazel version 5.3.2  GCC/Compiler version Xcode 14.1 14B47b  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to update the MacPorts distribution of `tensorflow`. It fails to build with the latest version 2.11.0. The build uses Xcode. Related: * https://github.com/macports/macportsports/pull/15397  CC(impossibility to build from source  Mac M1)  Standalone code to reproduce the issue  The MacPorts build recipe has worked up until the last version or so: https://github.com/macports/macportsports/blob/master/python/pytensorflow/Portfile shell :debug:build system W /opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/tensorflow2.11.0: PATH=/opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/bazelwrap:/opt/local/libexec/bazel/bin:/opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/bin:/opt/local/bin:/opt/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin BAZEL_SH=/bin/bash CC_OPT_FLAGS=march=x8664 /opt/local/libexec/bazel/bin/bazel max_idle_secs=10 output_user_root=/opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/bazel_build  fetch //tensorflow/tools/pip_package:build_pip_package â€¦ :info:build ld: malformed trie, childNodeOffset==0 file 'bazelout/darwinopt/bin/_solib_darwin_x86_64/libtensorflow_Spyth)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,essandess,tensorflow does not build on macOS,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution macOS 12.6.1 21G217 x86_64  Mobile device _No response_  Python version 3.10, 3.9, 3.8  Bazel version 5.3.2  GCC/Compiler version Xcode 14.1 14B47b  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to update the MacPorts distribution of `tensorflow`. It fails to build with the latest version 2.11.0. The build uses Xcode. Related: * https://github.com/macports/macportsports/pull/15397  CC(impossibility to build from source  Mac M1)  Standalone code to reproduce the issue  The MacPorts build recipe has worked up until the last version or so: https://github.com/macports/macportsports/blob/master/python/pytensorflow/Portfile shell :debug:build system W /opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/tensorflow2.11.0: PATH=/opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/bazelwrap:/opt/local/libexec/bazel/bin:/opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/bin:/opt/local/bin:/opt/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin BAZEL_SH=/bin/bash CC_OPT_FLAGS=march=x8664 /opt/local/libexec/bazel/bin/bazel max_idle_secs=10 output_user_root=/opt/local/var/macports/build/_opt_local_ports_python_pytensorflow/py310tensorflow/work/bazel_build  fetch //tensorflow/tools/pip_package:build_pip_package â€¦ :info:build ld: malformed trie, childNodeOffset==0 file 'bazelout/darwinopt/bin/_solib_darwin_x86_64/libtensorflow_Spyth",2022-12-06T22:52:48Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.11,closed,0,15,https://github.com/tensorflow/tensorflow/issues/58805, Could you make sure to follow the instructions mentioned here and check the tested build configurations . Thank you!,">  Could you make sure to follow the instructions mentioned here and check the tested build configurations . > Thank you! Thatâ€™s exactly what I have done. The failed build command provided above is exactly that specified in your link. It worked with previous versions of `tensorflow`. It no longer works. There are multiple independent reports online and issues reporting this problem, e.g. CC(impossibility to build from source  Mac M1). Iâ€™m simply trying to update the MacPorts Portfile that builds from source, and this fails at the end in with a linker failure, noted above and the other issues reported online."," FYR, as I commented before in https://github.com/tensorflow/tensorflow/issues/58368issuecomment1321729076, it's likely a linked (ld) or linker+compiler problem. I replaced ld in Xcode 14.x with the one from Xcode 13.x, then I can successfully build it. ",">  FYR, as I commented before in  CC(TF 2.11.0/2.12 fails to build in MacOS 13   XCode 14.1  issue with ld linker) (comment), it's likely a linked (ld) or linker+compiler problem. I replaced ld in Xcode 14.x with the one from Xcode 13.x, then I can successfully build it. Thanks. Generally there's only the latest available Xcode available on macOS, but there is `ld64`'s `ld` linker. In fact, this `ld64` binary is the one found in my own build attempt via `$PATH`:  Clearly, `bazel` is hardcoding a different linker from Xcode somehow. How does one coerce `bazel` to use a specific linker?","I tried setting `bazel build linkopt=""fuseld=${prefix}/bin/ld""` but this also fails with the same error.","I don't think bazel uses a different linker. Yes, on my mac, I have `/opt/local/bin/ld`  installed via macport, too. But, that's symbolic links and shell script wrappers to Xcode ld.  And   That why I replaced the `ld` in /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ as I commented in  https://github.com/tensorflow/tensorflow/issues/58368issuecomment1321729076. You can find older ld binaries in Xcode 13.x or Command Line Tools 13.x from Apple Developer site.", is exactly right. I went ahead and documented the steps I followed here Building Tensorflow and Tensorflow Text on a M1 Mac,">  is exactly right. I went ahead and documented the steps I followed here Building Tensorflow and Tensorflow Text on a M1 Mac That's awesome and everything for hacking one's own personal TF binary. But if one is trying to accomplish what I'm trying to accomplish: a general build formula that can be used to provide TF binaries across a suite of platforms in the MacPorts package manager, then not so much. Much better if this upstream would test the releases against uptodate build configurations and avoid this whole reporting and hacking cycle.","Hi, Your issue seems similar to the issue here https://github.com/tensorflow/tensorflow/issues/58368, could you please close this issue and track the progress in that thread, since it is already involving lots of discussion. You can follow the instructions here for the M1 mac installations of Tensorflow prebuilt library here https://developer.apple.com/metal/tensorflowplugin/","> You can follow the instructions here for the M1 mac installations of Tensorflow prebuilt library here https://developer.apple.com/metal/tensorflowplugin/ Thatâ€™s prebuilt for specific python versions, and doesnâ€™t cover all of them. This issue is about TF failing to compile from source on macOS. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. Could you please Set DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer (path to Xcode) and try running the build again and also try with XCode 14.3 version as mentioned in https://github.com/tensorflow/tensorflow/issues/60179issuecomment1492839793 The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1000,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bug when using TFLite converter - error message suggests to send it to you)ï¼Œ å†…å®¹æ˜¯ (I was trying to convert a standard model (properly defined, works just fine when trying to predict stuff) when I encountered this error:  I am using a book ,,TinyML: Machine Learning with TensorFlow on Arduino, and UltraLow Power MicroControllers "", which seems to be written by someone from TF development team. I follow all the instructions from chapter 4. If you are interested in my colab code, here it is: [https://colab.research.google.com/drive/1eyqMB1POAZ9qw3lPCZkiyU7zfQ0Y1OxFscrollTo=u26RmYUMC3Rw] Any idea, any help? UPDATE 06.12.2022 After rerunning the entire code the day after I posted the issue (which is today), it worked just fine. Still, I would like to know what might have caused the problem. Should I close the issue?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FranciszekNowak,Bug when using TFLite converter - error message suggests to send it to you,"I was trying to convert a standard model (properly defined, works just fine when trying to predict stuff) when I encountered this error:  I am using a book ,,TinyML: Machine Learning with TensorFlow on Arduino, and UltraLow Power MicroControllers "", which seems to be written by someone from TF development team. I follow all the instructions from chapter 4. If you are interested in my colab code, here it is: [https://colab.research.google.com/drive/1eyqMB1POAZ9qw3lPCZkiyU7zfQ0Y1OxFscrollTo=u26RmYUMC3Rw] Any idea, any help? UPDATE 06.12.2022 After rerunning the entire code the day after I posted the issue (which is today), it worked just fine. Still, I would like to know what might have caused the problem. Should I close the issue?",2022-12-06T08:14:53Z,stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.11,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58797," This warning will not hurt performance. If you face this issue in the future you may add this line just before importing tensorflow: tf.autograph.set_verbosity(3, True)?  It will show the additional information along with the warning.  After rerunning your code if the issue doesn't persist now, then please move this issue to closed status. Thank you!",Thank you very much!,Are you satisfied with the resolution of your issue? Yes No
681,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.math.cumsum() propagates Inf or NaN when axis is ragged)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.7, cuDNN 8.2.4  GPU model and memory RTX 3090 24GiB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,MKimiSH,tf.math.cumsum() propagates Inf or NaN when axis is ragged,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.7, cuDNN 8.2.4  GPU model and memory RTX 3090 24GiB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-12-05T23:37:00Z,stat:awaiting response type:bug stale comp:ops TF 2.11,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58790,"My guess is that when doing `cumsum` on a ragged dimension, the summation is actually somehow carried out on the entirety of flat values, and then some subtractions based on `row_splits` turn the long `cumsum` list to the result at each dimension. To explain with an example:   In this way, we get `inf  inf => nan` when an `inf` appears.  ","Hi again,  Any updates? I would also be interested in knowing a suggested workaround that doesn't require calling `rt.to_tensor()`. "," I tried to replicate the issue on colab using TF v2.11, and didn't face the error reported.  Please find the gist here and let us know if I have missed something to replicate the issue. Thank you!",">  I tried to replicate the issue on colab using TF v2.11, and didn't face the error reported. Please find the gist here and let us know if I have missed something to replicate the issue. Thank you! Thanks for looking into it. It looks like you got the same output as I did, actually confirming the bug.  I think I was not using the term ""expected output"" correctly, the ""expected output"" written in the original description is the actual buggy output. I updated the issue, renaming ""expected output"" to ""actual output"" and inserted a new ""expected output"" paragraph with reference to the nonragged tensor operation output. ","> My guess is that when doing `cumsum` on a ragged dimension, the summation is actually somehow carried out on the entirety of flat values, and then some subtractions based on `row_splits` turn the long `cumsum` list to the result at each dimension. To explain with an example: >  >  >  > In this way, we get `inf  inf => nan` when an `inf` appears. After some more checks I don't think it's as simple as this. If it really just sums up `float_values`, there will also be overflow problems. But it seems fine (actually not, see update):   Update: uh wait. Something is not fine with `float32`. "," I was able to replicate the issue, please find the attached gist here. Thank you!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
635,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MNMG training with jit_compile=True is not working)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.11  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.0  Bazel version 5.1.1  GCC/Compiler version _No response_  CUDA/cuDNN version cuda 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,YuchengT,MNMG training with jit_compile=True is not working,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.11  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.0  Bazel version 5.1.1  GCC/Compiler version _No response_  CUDA/cuDNN version cuda 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-05T20:05:56Z,stat:awaiting response type:bug stale comp:model comp:xla TF 2.11,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58787,Hi  ! Thanks for PR 10845 and sharing your observation w.r.to XLA(tf.function).  ! Could you look at this issue. Thank you!,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
643,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Layer Reshape not working for same size arrays)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Windows 10 Pro  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.8  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dpm16,Tensorflow Layer Reshape not working for same size arrays,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Windows 10 Pro  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.8  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-05T19:12:04Z,stat:awaiting response type:bug stale comp:keras TF 2.11,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58786,  I was able to execute the given code without error on Colab using TF v2.11. Please find the gist here for reference Thank you !,  Please find the above implementation here gist. Did you try this same on the Jupyter Notebook?,The error only happens on windows and works on linux. It does work on the Jupyter notebook but not an any Windows machine running it.,  I was able to reproduce the issue on the windows. Please refer to the snapshot !Screenshot (8) This issue seems to be a keras related issue. Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1829,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Convert saved model (trained in TF1) from using Float32 into BFloat16 (or Float16))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Python version 3.8  CUDA/cuDNN version 11.4  GPU model and memory NVIDIA GeForce RTX 3070 Laptop GPU ; 8GB GPU RAM + 64 GB RAM  Current Behaviour?   Standalone code to reproduce the issue Python  Code to transform the model from Float32 > BFloat16 from argparse import ArgumentParser  from tensorflow.core.protobuf import config_pb2  from tensorflow.core.protobuf import rewriter_config_pb2  from tensorflow.python.grappler import tf_optimizer  from tensorflow.python.tools import saved_model_utils  import tensorflow as tf  import time    parser = ArgumentParser()  parser.add_argument(""input_dir"", help=""Input directory containing saved_model.pb."", type=str)  parser.add_argument(""output_dir"", help=""Directory to store output graph."", type=str)  parser.add_argument(""output_graph"", help=""Output graph name. (e.g., foo.pb,""                      ""foo.pbtxt, etc)"", type=str)  args = parser.parse_args()    graph_options = tf.compat.v1.GraphOptions(rewrite_options=rewriter_config_pb2.RewriterConfig(      auto_mixed_precision_onednn_bfloat16=rewriter_config_pb2.RewriterConfig.ON))  optimizer_config = tf.compat.v1.ConfigProto(graph_options=graph_options)  metagraph_def = saved_model_utils.get_meta_graph_def(args.input_dir, ""myTag"")  output_graph = tf_optimizer.OptimizeGraph(optimizer_config, metagraph_def)  tf.io.write_graph(output_graph, args.output_dir, args.output_graph,                    as_text=False)  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bashirmindee,Convert saved model (trained in TF1) from using Float32 into BFloat16 (or Float16),"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Python version 3.8  CUDA/cuDNN version 11.4  GPU model and memory NVIDIA GeForce RTX 3070 Laptop GPU ; 8GB GPU RAM + 64 GB RAM  Current Behaviour?   Standalone code to reproduce the issue Python  Code to transform the model from Float32 > BFloat16 from argparse import ArgumentParser  from tensorflow.core.protobuf import config_pb2  from tensorflow.core.protobuf import rewriter_config_pb2  from tensorflow.python.grappler import tf_optimizer  from tensorflow.python.tools import saved_model_utils  import tensorflow as tf  import time    parser = ArgumentParser()  parser.add_argument(""input_dir"", help=""Input directory containing saved_model.pb."", type=str)  parser.add_argument(""output_dir"", help=""Directory to store output graph."", type=str)  parser.add_argument(""output_graph"", help=""Output graph name. (e.g., foo.pb,""                      ""foo.pbtxt, etc)"", type=str)  args = parser.parse_args()    graph_options = tf.compat.v1.GraphOptions(rewrite_options=rewriter_config_pb2.RewriterConfig(      auto_mixed_precision_onednn_bfloat16=rewriter_config_pb2.RewriterConfig.ON))  optimizer_config = tf.compat.v1.ConfigProto(graph_options=graph_options)  metagraph_def = saved_model_utils.get_meta_graph_def(args.input_dir, ""myTag"")  output_graph = tf_optimizer.OptimizeGraph(optimizer_config, metagraph_def)  tf.io.write_graph(output_graph, args.output_dir, args.output_graph,                    as_text=False)  ",2022-12-05T13:19:28Z,stat:awaiting response type:bug stale comp:apis TF 2.11,closed,0,12,https://github.com/tensorflow/tensorflow/issues/58781,"  There is another way you can try to enable AMP during inference on CPUs that bf16 is supported, just add below line, it will run AMP for fp32 model automatically:  tf.config.optimizer.set_experimental_options({""auto_mixed_precision_mkl"": True})  Reproduce: 1. I changed the model input from **X** to **x** in the model saving script:  2. Inference script: infernce.py  3. Run inference.py with with oneDNN verbose log, you should be able to see bf16 kernels: ", Could you refer to the comment above and this SO thread. Please let us know if it helps? Thank you!,"Thank you for your answer, I tried your answer with two custom models I am working on on the new Sapphire Rapid CPUs, and I found that the performance is worse than using ONEDNN default behavior: * 166ms (mixed precision)  * vs 256ms (default ONEDNN);  can I run the model on GPU using BFloat16 in some easy way ? ",What kind of model topologies are you trying?,Linknet,"Hi , for the new Sapphire Rapid CPUs, could you try **pip install inteltensorflow==2.11.dev202242** also, could you share some GitHub link for Linknet you are using so that we can reproduce?", Did you refer this comment above and update on the same? Please check this in the latest intelTF version and let us know if that helps? Thank you!,hello I haven't had the time to do the test yet... I see that on the releases page  inteltensorflow 2.11 was released...  Maybe I should test using the final version instead ?, Thank you for your response! Please test it in the final version and let us know? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
707,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tfds plant_leaves downloads failing)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution macOS Monterey version 12.6  Mobile device n/a  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour? I am getting timeouts when trying to download the `plant_leaves` dataset on December 4th, 2022.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jamesbraza,tfds plant_leaves downloads failing,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution macOS Monterey version 12.6  Mobile device n/a  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour? I am getting timeouts when trying to download the `plant_leaves` dataset on December 4th, 2022.  Standalone code to reproduce the issue   Relevant log output  ",2022-12-04T23:23:03Z,type:build/install subtype:macOS TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58777,I just realized that perhaps I should have opened this issue in tensorflow/datasets. Let me know if you would like me to close and reopen there.,"For what it's worth, looks like `plantae_k` dataset is also not downloadable right now.  Very similar stack trace:  !image Additionally Mendeley has newer versions:  `plant_leaves`: version 5  `plantae_k`: version 2","Hi,   Apologize for the delay and I was able to replicate the issue on Google Colab and on `MacOS` also but I'm also getting the same error message   It seems like some images url or url iteself for images are not found because we are getting `HTTP code: 404`. so for your reference I have added gistfile and also `MacOS` screenshot below  I have also tried with `tfds.core.DatasetBuilder() `approach to download as mentioned below in the code snippet but still getting the same error but I'm able to see it's creating some images folders with tfrecord files It seems like this issue with `plant_leaves` dataset because I have tried with `mnist` dataset and it's working fine, for your reference here is gistfile so could you please post this issue in this repo for faster resolution. Thank you!  ", thank you for confirming! I have made a duplicate issue here: https://github.com/tensorflow/datasets/issues/4744.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi,   Thank you for posting this issue in this repo and this issue will be taken care by that team so I'm closing the issue since it is being tracked in `tensorflow/datasets` repo and if you need any further help please post your comments or reopen this issue. Thank you!",Are you satisfied with the resolution of your issue? Yes No
644,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(//tensorflow/compiler/xla/tools:interactive_graphviz_build_only_test fails to build)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.3.0  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,elfringham,//tensorflow/compiler/xla/tools:interactive_graphviz_build_only_test fails to build,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.3.0  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-12-02T10:50:39Z,type:build/install subtype: ubuntu/linux subtype:bazel awaiting PR merge,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58766,Introduced by https://github.com/tensorflow/tensorflow/commit/944586a977dd12db51b388d9fa029aae454db927,Hi  ! Thank for the fix . This issue will be closed once PR CC([Linaro:XLA] fix fail to build graphviz_interactive) is merged. Thank you!,Are you satisfied with the resolution of your issue? Yes No
1087,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(To use the argument `sampling_rate`, you should install tensorflow_io. You can install it via `pip install tensorflow-io`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu  Mobile device 22.04  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell import tensorflow as tf from tensorflow.keras.utils import audio_dataset_from_directory dataset = audio_dataset_from_directory(     ""dataset"",     labels=""inferred"",     label_mode=""categorical"",     class_names=None,     batch_size=1,     sampling_rate=16000,     output_sequence_length=100,     ragged=False,     shuffle=True,     seed=None,     validation_split=None,     subset=None,     follow_links=False, )   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,SajjadAemmi,"To use the argument `sampling_rate`, you should install tensorflow_io. You can install it via `pip install tensorflow-io`","Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code Yes  OS Platform and Distribution Linux Ubuntu  Mobile device 22.04  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell import tensorflow as tf from tensorflow.keras.utils import audio_dataset_from_directory dataset = audio_dataset_from_directory(     ""dataset"",     labels=""inferred"",     label_mode=""categorical"",     class_names=None,     batch_size=1,     sampling_rate=16000,     output_sequence_length=100,     ragged=False,     shuffle=True,     seed=None,     validation_split=None,     subset=None,     follow_links=False, )   Relevant log output  ",2022-12-02T07:37:33Z,stat:awaiting response type:bug stale comp:apis TF 2.11,closed,0,12,https://github.com/tensorflow/tensorflow/issues/58762,"Hmm, you can try installing the TensorflowIO library via pip install tensorflowio (as noted in the error). Alternatively, I think? you can also try setting the 'sampling_rate' argument to 0, as this will force Keras to use the default sampling rate. Used Clerkie (ai code debugging tool)  https://clerkie.co/, to help me brainstorm on approaches here (still reviewed/investigated feedback before sharing). Hope it helped! ","  i installed TensorflowIO, but same error"," I was able to replicate this issue on colab, please find the gist here. Thank you!","Hi  , The Error coming from this piece of code. It is expecting `tensorflow_io` imported as `tfio`. Could you try adding the import statement like below and confirm whether it works for you ? `import tensorflow_io as tfio `",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No,"Hi   i import tensorflow_io, but i have same error !image","Hi  , It seems the version compatibility between tensorflow and tensorflowio causing the problem. Can you try installing tensorflowio alongwith its compatible tensorflow using the below code. `!pip install tensorflowio[tensorflow]` For me the reported Import error gone with above installation code.Please refer to attached gist.","Thanks  , it works for me ğŸ’ªğŸ»"," , Glad that it works for you. Please feel free to close the issue. Thanks!",This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
659,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Legalization TFL.Call_Once to TOSA Insert Pattern Failure)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version N/A  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Jerry-Ge,Legalization TFL.Call_Once to TOSA Insert Pattern Failure,Click to expand!    Issue Type Others  Source source  Tensorflow Version N/A  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output ,2022-12-02T05:36:53Z,type:others,closed,0,0,https://github.com/tensorflow/tensorflow/issues/58761
1137,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to convert model with multiple input?)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., window 10):  TensorFlow installation (pip package):  TensorFlow library (tensorflow 2.9.1):  2. Code Provide code to help us reproduce your issues using one of the following options: I'm use code as below, what should I change to make convertion success?    3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. error info: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MPolaris,How to convert model with multiple input?," 1. System information  OS Platform and Distribution (e.g., window 10):  TensorFlow installation (pip package):  TensorFlow library (tensorflow 2.9.1):  2. Code Provide code to help us reproduce your issues using one of the following options: I'm use code as below, what should I change to make convertion success?    3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. error info: ",2022-12-02T05:09:17Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58760,"Hi  ! Int8 quantization does not seem to be feasible here. You can skip the below code snippet on inference input and output type as a work around. (Tested with concatenation of outputs and other  flags, checked in 2.11 and nightly too.)  Attached gist for reference. Thank you!",Hi ! Thanks for your response. I think this model is very simple and only owns two input nodes. It's very confusing for  is not support quantization for multiple input model?," ! Thanks for the clarification . (Quantization issue with multiple input model).  Actually, Model did not pass  for single quantization even with single input for above model (earlier trials) My guess was merging the outputs of 2 output of two conv2d layers.  Anyway, You can find the quantization issues through quantization_debugger and replace/disable the quantization of respective layers.  ! Could you look at this issue. Attached gist for reference. Thank you!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
391,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add tensorflow io support in Docker container)ï¼Œ å†…å®¹æ˜¯ (Add tensorflowio partial Dockerfile and updated the spec.yml file, to provide object storage support for tensorflow images out of the box as seen in this issue)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,dilverse,Add tensorflow io support in Docker container,"Add tensorflowio partial Dockerfile and updated the spec.yml file, to provide object storage support for tensorflow images out of the box as seen in this issue",2022-11-30T11:45:06Z,size:L,closed,1,16,https://github.com/tensorflow/tensorflow/issues/58742,"While I appreciate the work you've done here, I prefer not to accept this change because:  These are ""officially supported"" TensorFlow docker containers, and it's not our goal to include community / thirdparty support as well  The complicated ""partials"" system is a nightmare to maintain, and we're discussing deprecation of these containers in favor of a different Docker recommendation (but no concrete plans yet)."," tensorflowio is officially part of tensorflow repo, how can this be considered 3rd party and with the new `io` repo tensorflow docker images have broken support for object storage files systems that used to work before, this PR is trying to address that.","`tensorflowio` is a SIG IO project, not a Google project","   well, `gcs` support is also part of `tensorflowio` and it got a special separate wheel which is being added as a dependency to `tensorflow`, in this scenario, the `tensorflowiogcsfilesystem` is also a 3rd party isn't it? it sits on the `tensorflowio` repo, per https://github.com/tensorflow/tensorflow/commit/7add88f2982331be6ba65ddc66f1c88053028fff it seems the split was for modularity and not to remove functionality from TF Support for other filesystems was part of TensorFlow all the way up to `2.4`, by removing them from upstream the message we are getting is, don't use upstream for other cloud providers storage other than Google unless you really know what you are doing, which in my opinion is a very steep learning curve for anyone trying to use TF and for serious users they are not going to trust a 3rd party docker image all the time, users simply won't keep up with upstream TF since they have to built specialized docker images. With this PR we are simply trying to return a functionality that was already present and all over the internet you can see the struggle of people trying to solve this problem over and over, the way I see the reason why there's only one tensorflow docker image for tensorflow, tensorboard and tensorflowserve means, you prefer a one stop solution, so if the fix won't be coming to core, it should at least be in the main container image.",The difference is that there is an equivalent of GCS filesystem built internally and both internal and external GCS support is tested nightly. We need to strike a balance between what batteries are included within TF and what we ask the OSS team to support. It is unreasonable for a team of a handful of engineers to support all possible filesystems that exist out there.,"if it comes to having testing, we can also build that, at least for S3 and I'm sure the Azure guys would also be down to adding some testing for their part, that would make a lot of sense if we were do readd it to core tensorflow, do you want us to add tests for this part via a github action?   We can definitively commit to supporting S3 support, we are no strangers to OSS :P ","That would work, I think. but note that I'm no longer in TF, only doing drive by help (and should mostly focus on security stuff only). So I cannot do the decision here. Leaving to  and the team"," Any suggestions as to when this PR will be merged, we have a need to leverage this change ASAP. ","Sorry, I'm still against adding this to the official tensorflow docker containers. It would be expanding the implied support range for something that's already not very maintained to a wider scope, and ultimately wouldn't be a good precedent for us. toplay  FYI","Hi vk, Having software that can be easily changed/updated is key to developer productivity. Additional support can make the code rigid and future upgrades/changes more difficult (due to incompatibilities, etc). A developer who would like to make a change in the software would also have to read and understand a broader scope. Taking into account the maintainability downsides, the proposed PR/change don't seem the best option to me.","Hmm I am somewhat conflicted with this one.  I tend to lean towards what  and toplay have said when it comes to how this ends up expanding implied support.  On the other hand, before joining the TF team i have encountered this scenario in a previous position, so i know it is annoying thing to encounter. (For me it was minio and tensorboard)   My initial question from reading this would be how many other such packages that look to extend the base docker image exist?   Is this truly a one off that we should consider exempting or is it one of many in which we should consider how to better support extending the base images? ",Hi  Can you please resolve conflicts? Thank you!,We are looking to take the docker support in a different direction.  We are are going to differentiate the docker containers that are used for building and development TensorFlow from those that are built for downstream usage of TensorFlow.  The ones used for downstream usage that are officially maintained are looking to move to the SigBuild repo   ,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB,Hi  Can you please resolve conflicts? Thank you!,These containers have been moved to https://github.com/tensorflow/build/tree/master/tensorflow_runtime_dockerfiles.
716,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf2.11 tf.profiler.experimental  doesn't generate trace.json.gz and other files compared with tf2.10)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Linux Ubuntu 20.04  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,LIONEFAN,tf2.11 tf.profiler.experimental  doesn't generate trace.json.gz and other files compared with tf2.10,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Linux Ubuntu 20.04  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-30T03:15:08Z,stat:awaiting response type:bug stale comp:apis TF 2.11,closed,0,15,https://github.com/tensorflow/tensorflow/issues/58738,", I tried to execute the mentioned code and was able to fetch the similar output in both tensorflow versions 2.10 and 2.11. Kindly find the gist of it here and the images for the reference. !image  Thank you!","> , I tried to execute the mentioned code and was able to fetch the similar output in both tensorflow versions 2.10 and 2.11. Kindly find the gist of it here and the images for the reference. !image Thank you! Hi , I also test it in gist here, but my results are different with you.  !image I think you maybe run them all in tf2.11.  I suggest you manually run the tf2.10 pip install and code firstly. Then run the tf2.11 pip install and **restart the runtime in order to use newly installed versions**. And then run the code in tf2.11. Thanks.","Hi , can you reproduce this problem?",", Sorry for the delay. I tried on tensorflow v2.10 to reproduce the issue but I was not able to fetch the files which are mentioned. Kindly find the gist and the image for the reference. !image",", I have run the gist you provided. But it told me an error about the RAM is ran out. Therefore, you can't eventually get the folder of **""plugins""** compared with your first running, because the program **stopped early.**  I have turned down the size of training epochs from 10000 to 2000 `for _ in range(10000):` > `for _ in range(2000):` And  I got the expected results. **So please you try it again to use following codes, thanks!** BTW, I added an output to the last line to make sure we know that the program is fully executed. ",", I was able to reproduce the issue on tensorflow v2.10, v2.11 and nightly. Kindly find the gist of it here.","Hi, we have same issue with 2.11 release.  Is there anything equalivant for trace.json. Is the data moved to `ipxxx.xplane.pb` file?","Hi,  This behavior was updated as part of commit https://github.com/tensorflow/tensorflow/commit/52992fc29f00fc743e07e16067f6418af6c489ff to only save the xplane.pb file as the other files are derived from this. The results are now computed when viewing the analysis results on Tensorboard.   You can view the trace and other tool results on Tensorboard with the tensorboardpluginprofile[1].  Would you like to access the trace.json.gz directly? We would like to know more about your use case.  1 : Profiling on colab",We just try to figure out the changes for these trace data. Thanks for the reply,"  Thanks for your reply. Now I can view the trace by using Tensorboard on tf 2.11. But is there a way to directly generate the trace.json.gz on tf 2.11? Because in some case, I only want to focus on the timeline. And then I can load it in chrome://tracing directly."," , as per the commit mentioned here  https://github.com/tensorflow/tensorflow/commit/52992fc29f00fc743e07e16067f6418af6c489ff, I don't think there is any workaround to generate trace.json.gz file other than the solution mentioned above.  Even the Test cases has been moved out in the above change. !image",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Thanks, I don't have any questions.",Are you satisfied with the resolution of your issue? Yes No,"> Hi, >  > This behavior was updated as part of commit 52992fc to only save the xplane.pb file as the other files are derived from this. The results are now computed when viewing the analysis results on Tensorboard. >  > You can view the trace and other tool results on Tensorboard with the tensorboardpluginprofile[1]. >  > Would you like to access the trace.json.gz directly? We would like to know more about your use case. >  > 1 : Profiling on colab Yes. Accessing trace.json.gz helps. we also developed some scripts to parse and analysis among multiple json files, so it will be helpful to have the trace.json.gz file. Even we only have xplane.pb file, is that possible to generate trace.json.gz file by running some python scripts? thanks a lot Louie"
656,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF 2.11 fails to compile on Linux with GPU support - Model Loads - Inference Fails)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11  Custom Code No  OS Platform and Distribution Debian 11  Mobile device _No response_  Python version 3.9  Bazel version 5.1.1  GCC/Compiler version 9.3.0  CUDA/cuDNN version 11.2,8.1  GPU model and memory RTX 3060/12G  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,sniafas,TF 2.11 fails to compile on Linux with GPU support - Model Loads - Inference Fails,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11  Custom Code No  OS Platform and Distribution Debian 11  Mobile device _No response_  Python version 3.9  Bazel version 5.1.1  GCC/Compiler version 9.3.0  CUDA/cuDNN version 11.2,8.1  GPU model and memory RTX 3060/12G  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-11-29T15:32:08Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.11,closed,0,9,https://github.com/tensorflow/tensorflow/issues/58731,`tensorflow` and `tensorflowgpu` are the same thing since TF 2.0,"Hi   when compiling from source with CUDA support the built pip wheel should have the following format  and when not, this format  For some reason the configuration above does not produce the first format","This is not true, has not been true since TF 2.0. If you look on PyPI, `tensorflow` and `tensorflowgpu` packages are identical, except the name","Frankly speaking, since my architecture is kind of old (1st gen icore), I have been compiling from source quite a while.  I suppose that there has to be some kind of special argument in bazel compiler, complementary to CUDA. If I have an update for the latest TF version, I ll keep you posted","Yes, the naming of the wheel is not too relevant (it's controlled by some flag). The build config you posted should compile the GPU kernels though"," , As you said your architecture is kind of old there might be chances that compatibility issues w.r.t CUDA or Bazel. If you have updated the Architecture and tested with latest TF please let us know if the problem persists even with the New architecture. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to write to input tensors in custom op?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,alanbacellar,How to write to input tensors in custom op?,Click to expand!    Issue Type Others  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ,2022-11-28T22:37:01Z,stat:awaiting response type:support stale type:others comp:ops TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58721, Could you please refer to this link and let us know if it helps? Thank you!,"Hi , thank you for the reply. I already read it all in the tensorflow custom op  webpage but there is nothing there about writing to inputs in an custom op, only to outputs. I know that the tensorflow way of updating an input variable is to write to an output, and then call tf.assign to update the value. But in the problem I have this is too costly in both memory and compute time, so I would like to alter an input inplace within the custom op, without needing to write to an output tensor and then calling tf.assign. I tried const_cast the Eigen tensor to an non const but that didn't work as it gives undefined behaviour. Is there an official way to do it? ","Hi, If you are looking for assigning value to the Tensor variable, you can do it using `tf.Variable.assign()`.  Let us know if you are looking for something else. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add tensorflow io support)ï¼Œ å†…å®¹æ˜¯ (Add tensorflowio package to the pippackages that gets picked in tensorflow docker images, to provide object storage support for tensorflow images out of the box as seen in this issue)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,dilverse,Add tensorflow io support,"Add tensorflowio package to the pippackages that gets picked in tensorflow docker images, to provide object storage support for tensorflow images out of the box as seen in this issue",2022-11-28T20:17:34Z,size:XS,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58718,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",I think the plan is for users to manually install the TF IO wheel. We don't want `pip install tensorflow` to install all and every package that exists in TF ecosystem.," The idea was for the docker images to pick the tensorflowio support, since applications like tensorboard with object storage or other file system support is purely dependent on that package. This adds complexity to the deployment as described in the issue  CC(Add support for 3rd part object storage (S3) in Docker image), what would be your recommended approach?","Why is there an exception for `tensorflowiogcsfilesystem` if that's also part of `tensorflowio` ? S3 support was standard up to TF 2.4.0 until the code got split into tensorflowio and broke support for it but support for GCS made it as an exception for some reason. We tracked this problem all the way from kubeflow which uses a lot of TensorFlow components, and they all make the assumption that s3 is present, but it was removed so using a newer tensorflow breaks the components (tensorboard, tf serving, for example) The idea of this PR is to make sure the standard TensorFlow image includes this dependency, if this used to be part of the core code and it got split to alleviate the load of the main repository, I don't see a problem in adding the dependency into the final package/container.",">  The idea was for the docker images to pick the tensorflowio support, since applications like tensorboard with object storage or other file system support is purely dependent on that package. This adds complexity to the deployment as described in the issue  CC(Add support for 3rd part object storage (S3) in Docker image), what would be your recommended approach? Then it should be added to the dockerfile installs, not to the `setup.py` package > Why is there an exception for `tensorflowiogcsfilesystem` if that's also part of `tensorflowio` ? S3 support was standard up to TF 2.4.0 until the code got split into tensorflowio and broke support for it but support for GCS made it as an exception for some reason. >  > We tracked this problem all the way from kubeflow which uses a lot of TensorFlow components, and they all make the assumption that s3 is present, but it was removed so using a newer tensorflow breaks the components (tensorboard, tf serving, for example) >  > The idea of this PR is to make sure the standard TensorFlow image includes this dependency, if this used to be part of the core code and it got split to alleviate the load of the main repository, I don't see a problem in adding the dependency into the final package/container. The GCS filesystem is needed for most of the tutorials on TF.org, has a larger level of maintenance and replicates the support for internal TF. The S3 filesystem does not match that. Users should do `pip install tensorflowio` to bring in the additional filesystems, as needed."
760,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(On threading library using the GPU implementing multithreaded RL algorithm)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version (v2.11.0rc217gd5b57ca93e5 2.11.0) But tried many different 2.11 versions  Custom Code Yes  OS Platform and Distribution Ubuntu 22.04.1 LTS  Mobile device Ubuntu 22.04.1 LTS  Python version 3.10 and 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cuda 11.0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,alebldn,On threading library using the GPU implementing multithreaded RL algorithm,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version (v2.11.0rc217gd5b57ca93e5 2.11.0) But tried many different 2.11 versions  Custom Code Yes  OS Platform and Distribution Ubuntu 22.04.1 LTS  Mobile device Ubuntu 22.04.1 LTS  Python version 3.10 and 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cuda 11.0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-28T18:52:42Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.11,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58715,"Hi  , You can safely ignore the warning.The reasons for this are mentioned below: `WARNING:tensorflow:5 out of the last 5 calls to  triggered tf.function retracing. ` The reasons for this are mentioned below:  Please refer the resources attached here 1 & 2","Kind sir, thanks for your reply. When using .function on the gpu I already do ignore the warnings and it works fine, it does train decently. But, unfortunately, when commenting .function the errors that show up don't allow me to execute the code further. (in the repo I posted two examples)","Hi  , Could you please confirm whether you have followed all the steps mentioned here for enabling GPU support ?  Thankyou!","Sorry, I thought I mentioned it but I didn't: I just used the other method available in the page and downloaded the docker image  tensorflow/tensorflow:latestgpujupyter from dockerhub as shown here. ","Hi  , Thanks for confirmation.Could you please confirm the output of the below code.Just to ensure GPU installation is successful or not.  Also the attached code of you is very lengthy and debugging might be difficult and may take a long time.If possible could you please submit a minimal code snippet that can replicate your problem quickly to exactly address the issue. Thankyou!","The output of the provided command is:  20221205 11:52:16.442813: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20221205 11:52:18.095820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero 20221205 11:52:18.096095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero 20221205 11:52:18.098533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero 20221205 11:52:18.098818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero 20221205 11:52:18.099018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero 20221205 11:52:18.099184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]  The output of nvidiasmi during training is:    So we know it does see both the GPUS I have on my server and they are indeed used to train the AI.  Unfortunately, I am not able to provide a smaller snippet to reproduce the problem right now. ","Thanks  , I can see GPU is enabled and working fine.So we have to check the source of error. Also the errors might be due to the reason that few ops are not working on GPU as they are not yet implemented on GPU.Could you also find any such ops in the code so that we can test those separately.I will also do it parallel. Thankyou!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
664,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for 3rd part object storage (S3) in Docker image)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.6+  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dilverse,Add support for 3rd part object storage (S3) in Docker image,Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.6+  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-28T07:08:17Z,stat:awaiting response type:build/install type:support stale 2.6.0,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58710, Could you please check this container and let us know if it helps? Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1908,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(error occurs when convert a tflite model with uint8 inference_input_type during quantization-aware-training)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10  TensorFlow installation (pip package or built from source): tf 2.8  TensorFlow library (version, if pip package or github SHA, if built from source): tf2.8  2. Code I want to convert a model by quantization aware training. the fp32 model is simple and it can only be convert to a tflite model with fp32 input/output. how can i convert a fp32 model with uint8 intput and output, since it can be done by posttraining quantization by assign the converter.inference_input_type = tf.uint8 and converter.inference_output_type  = tf.uint8.   if i can not assign inference_input_type or inference_output_type   during quantizationawaretraining, how can i convert a tflite model with specified input output dtype?  Thank you ! **minimum code:**  **ERRORS:**  Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fjzhangcr,error occurs when convert a tflite model with uint8 inference_input_type during quantization-aware-training," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10  TensorFlow installation (pip package or built from source): tf 2.8  TensorFlow library (version, if pip package or github SHA, if built from source): tf2.8  2. Code I want to convert a model by quantization aware training. the fp32 model is simple and it can only be convert to a tflite model with fp32 input/output. how can i convert a fp32 model with uint8 intput and output, since it can be done by posttraining quantization by assign the converter.inference_input_type = tf.uint8 and converter.inference_output_type  = tf.uint8.   if i can not assign inference_input_type or inference_output_type   during quantizationawaretraining, how can i convert a tflite model with specified input output dtype?  Thank you ! **minimum code:**  **ERRORS:**  Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN",2022-11-28T06:40:43Z,stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58709,Hi  ! I could not replicate the same behaviour in 2.9.2 version (default in colab). Attached gist for reference. Thank you!,"the version of the tf on my pc is 2.8, it shows the converter.convert() goes wrong~~~ but the tf of 2.8 or 2.9 on colabe shows everything goes well... is it common or wired tf may be install incorrectly on my pc? i was using pip install tensorflow==2.8 on anaconda to setup my virtual env....this env seems ok when i used it. !sunlogin_20221129102607",Hi  ! I was able to get a TFlite model from above script in my local windows machine after following instructions latest instructions for windows Could you create a new environment as per above instruction and let us know whether it works. Thank you!,thank you~~ maybe something went wrong when i installed it~~,Are you satisfied with the resolution of your issue? Yes No
682,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problems with TensorFlow for C, Windows CPU Only, version 2.11.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,LeoDecking,"Problems with TensorFlow for C, Windows CPU Only, version 2.11.0",Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-27T22:05:19Z,stat:awaiting response type:build/install stale subtype:windows TF 2.11 subtype:cpu-intel,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58707,  I was able to reproduce the issue and please find the gist here for reference. Thank you! ," , Do you think the above issues could be related to one of the changes made by you here https://github.com/tensorflow/tensorflow/commit/843cede83294063f88bf17097c2a446db70d9617.","Thanks for notifying me . I'm not familiar with Windows build, but with taking a quick look I see somehow the issue is that the Windows CPU only 2.11 archive doesn't include the `tf_buffer.h` introduced in my change. In comparison, the Linux archives have the header. I'll relay to folks who are familiar with TF releases to investigate the issue.",same problem with 2.12.0, We will try to reproduce this locally. And let you know the result soon. Thanks. ,"It appears this problem still exists in the 2.15.0 zip. It is also missing the tsl include hierarchy `tensorflow\2.15.0\include\tensorflow\c\tf_status.h(20,10): fatal error C1083: Cannot open include file: 'tsl/c/tsl_status.h': No such file or directory`","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bug: TypeError: can't multiply sequence by non-int of type 'NoneType')ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version v2.4.049g85c8b2a817f 2.4.1  Custom Code No  OS Platform and Distribution Kaggle Notebook  Mobile device No  Python version 3.7.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory TPU v38  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,guerchen,Bug: TypeError: can't multiply sequence by non-int of type 'NoneType',Click to expand!    Issue Type Bug  Source source  Tensorflow Version v2.4.049g85c8b2a817f 2.4.1  Custom Code No  OS Platform and Distribution Kaggle Notebook  Mobile device No  Python version 3.7.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory TPU v38  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-25T20:42:01Z,type:bug,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58690,Hey! The code you checked doesn't correspond to what's happening when you run the code because it is a more recent version. You're running version 2.4.1 so the corresponding code can be found here. As you can see there's no check on the `rank` in this code. There may be something wrong with your `train` input that causes `np.ndim` to return `None`.,"Ah! Thanks for the response! After some investigation, I found that my problem actually is loading the local data into a GCS bucket so that I can feed it to Kaggle TPU. Now it makes sense as to why my dimensions are None.",Are you satisfied with the resolution of your issue? Yes No
696,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_unique_id')ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Debian64 ( Unix )  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AngraPatrick,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_unique_id',Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Debian64 ( Unix )  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-25T11:33:46Z,stat:awaiting response type:support comp:ops TF 2.10,closed,1,4,https://github.com/tensorflow/tensorflow/issues/58686,"resolved with: self.Actor.optimizer.apply_gradients ( zip ( [ grads2 ] , [ std ]  ) ) you can close this thread",", Glad the issue is resolved for you, please feel free to move this to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'float_val'
1877,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to set the half_pixel_centers of ResizeBilinear to False in the .tflite?)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (Linux Ubuntu 22.04): Hi, i convert the model from pytorch into tfliteï¼š    pytorch = 1.10 (.pth) > onnx = 1.12 (.onnx) > tensorflow = 2.11  (.tflite) I find the half_pixel_centers  of the ResizeBilinear  operator is True, but i need this option to be False.   How can i set the half_pixel_centers to False during model convert? !image I try to set the half_pixel_centers  to False, when parsing the onnx model, and converting it into keras model:  def keras_builder(onnx_model, new_input_nodes:list=None, new_output_nodes:list=None):     model_graph = onnx_model.graph     '''         init onnx model's buildin tensors     '''     onnx_weights = dict()     for initializer in model_graph.initializer:         onnx_weights[initializer.name] = numpy_helper.to_array(initializer)     '''         build input nodes     '''     tf_tensor, input_shape = {}, []     for inp in model_graph.input:         input_shape = [x.dim_value for x in inp.type.tensor_type.shape.dim]         if input_shape == []:             continue         batch_size = 1 if input_shape[0]  0:             _inputs = tf_tensor[node_inputs[0]] if node_inputs[0] in tf_tensor else onnx_weights[node_inputs[0]]         for index in range(len(node_outputs)):             tf_tensor[node_outputs[index]] = tf_operator(tf_tensor, onnx_weights, node_inputs, op_attr, index=index)(_inputs)             if(op_name == ""Resize""):                 print(tf_operator(tf_tensor, onnx_weights, node_inputs, op_attr, index=index)(_inputs))         '''             reorganize input and output nodes         '''    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,zhanggd001,How to set the half_pixel_centers of ResizeBilinear to False in the .tflite?," 1. System information  OS Platform and Distribution (Linux Ubuntu 22.04): Hi, i convert the model from pytorch into tfliteï¼š    pytorch = 1.10 (.pth) > onnx = 1.12 (.onnx) > tensorflow = 2.11  (.tflite) I find the half_pixel_centers  of the ResizeBilinear  operator is True, but i need this option to be False.   How can i set the half_pixel_centers to False during model convert? !image I try to set the half_pixel_centers  to False, when parsing the onnx model, and converting it into keras model:  def keras_builder(onnx_model, new_input_nodes:list=None, new_output_nodes:list=None):     model_graph = onnx_model.graph     '''         init onnx model's buildin tensors     '''     onnx_weights = dict()     for initializer in model_graph.initializer:         onnx_weights[initializer.name] = numpy_helper.to_array(initializer)     '''         build input nodes     '''     tf_tensor, input_shape = {}, []     for inp in model_graph.input:         input_shape = [x.dim_value for x in inp.type.tensor_type.shape.dim]         if input_shape == []:             continue         batch_size = 1 if input_shape[0]  0:             _inputs = tf_tensor[node_inputs[0]] if node_inputs[0] in tf_tensor else onnx_weights[node_inputs[0]]         for index in range(len(node_outputs)):             tf_tensor[node_outputs[index]] = tf_operator(tf_tensor, onnx_weights, node_inputs, op_attr, index=index)(_inputs)             if(op_name == ""Resize""):                 print(tf_operator(tf_tensor, onnx_weights, node_inputs, op_attr, index=index)(_inputs))         '''             reorganize input and output nodes         '''    ",2022-11-25T08:41:50Z,type:support comp:lite TFLiteConverter,closed,0,2,https://github.com/tensorflow/tensorflow/issues/58684,Are you satisfied with the resolution of your issue? Yes No,Hi  ! You can use tfl.quantization_debugger to disable the layers. But not sure on setting half_pixel_centers to False through that. Feel free to see tensorflow model optimization documentation too.  Thank you!
680,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite Model Maker has version conflict during install)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Windows 11 22H2  Mobile device _No response_  Python version 3.17.13 and 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jericholongabela,TensorFlow Lite Model Maker has version conflict during install,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Windows 11 22H2  Mobile device _No response_  Python version 3.17.13 and 3.10.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-25T03:16:54Z,stat:awaiting response type:build/install stale comp:lite subtype:windows TF 2.10,closed,1,6,https://github.com/tensorflow/tensorflow/issues/58680,   Could you please refer to TensorFlow Lite Model Maker and let us know if it is helps. Thank you !,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"i run into the exact same problem.   `pip install tflitemodelmakernightly` as well as `pip install tflitemodelmaker` leading to the exact same error.   For me happening on windows 11 with python 3.10.9 installed.       exact error          EDIT:  Checking out github runs into an other problem, but i guess it is more a current master problem:    Logs        EDIT:  And running  `pip install U tflitemodelmaker` on Ubuntu Subsystem for WIndows ran into some version conflicts but finally resolved them for me. Seems to be an issue with some windows versions or something..  ",  I was able to install tflitemodelmaker on windows without any issue. Please find the below screenshots for reference: !Screenshot 20221220 8 21 17 AM !Screenshot 20221220 8 25 31 AM Thank you!,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1834,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory Safe Computations with XLA)ï¼Œ å†…å®¹æ˜¯ (Hello, In this pull request I would like to introduce the code of the paper that has been accepted at the NeurIPS 2022. This is the joint work of Yuze An (an), Tilman Roeder (), Mark van der Wilk () and myself (). We added optimization passes to the XLA optimization pipeline that automatically adapt the computational graph to make the computation more memory tolerable on a single device. That is, the code that fails on a single device with the out of memory (OOM) issue would be automatically modified by the compiler to an analogue that fits into memory and runs successfully. In general, many computational graphs can be transformed invariantly to the computation into another representation promoting speed and (or) memory advantages. Our implementation does not offer updates in the memory allocation manager and focuses solely on adjusting the computational graph. We call the set of introduced optimizations  eXLA. The eXLA consists of the following passes: * Match and replace (peephole) optimization (`euclidean_distance_rewriter.[hcc]`) optimization passes which prepare the computational graph for tensor splitting optimization pass. In some cases, graphs can be polluted with reshape, broadcast, transpose and reduce operations which can be safely removed from the graph and increase the chance of splitting longer paths.  Our extension allows a user to control the splitting optimization via two options: `xla_tensor_size_threshold` and `xla_tensor_split_size`. The `xla_tensor_size_threshold` option controls when the splitting optimization should be run to search for the splitting path, i.e. if )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,awav,Memory Safe Computations with XLA,"Hello, In this pull request I would like to introduce the code of the paper that has been accepted at the NeurIPS 2022. This is the joint work of Yuze An (an), Tilman Roeder (), Mark van der Wilk () and myself (). We added optimization passes to the XLA optimization pipeline that automatically adapt the computational graph to make the computation more memory tolerable on a single device. That is, the code that fails on a single device with the out of memory (OOM) issue would be automatically modified by the compiler to an analogue that fits into memory and runs successfully. In general, many computational graphs can be transformed invariantly to the computation into another representation promoting speed and (or) memory advantages. Our implementation does not offer updates in the memory allocation manager and focuses solely on adjusting the computational graph. We call the set of introduced optimizations  eXLA. The eXLA consists of the following passes: * Match and replace (peephole) optimization (`euclidean_distance_rewriter.[hcc]`) optimization passes which prepare the computational graph for tensor splitting optimization pass. In some cases, graphs can be polluted with reshape, broadcast, transpose and reduce operations which can be safely removed from the graph and increase the chance of splitting longer paths.  Our extension allows a user to control the splitting optimization via two options: `xla_tensor_size_threshold` and `xla_tensor_split_size`. The `xla_tensor_size_threshold` option controls when the splitting optimization should be run to search for the splitting path, i.e. if ",2022-11-24T22:52:51Z,stale comp:xla size:XL,closed,34,21,https://github.com/tensorflow/tensorflow/issues/58679,"Thanks a lot for the PR, looks very impressive! Please give us some time to figure this out!", Can you please resolve conflicts? Thank you!,Hi  Can you please resolve conflicts? Thank you!,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hi, ! I will get back to the PR next week. I was going to split it in 3 parts to make it more reviewfriendly: 1. Matrix chain optimisations 2. Distance matrix optimisations 3. Splitting optimisations Wdyt? Or should I keep this PR as is (but synced with master)?", do you know anything about https://github.com/openxla/xla? Will PRs to tensorflow automatically merged into the open XLA?,"Yes, for now.",Hi  Can you please resolve conflicts? Thank you!,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,Hi  Any update on this PR? Please. Thank you!,Hi  Any update on this PR? Please. Thank you!,", apologies for not being more active here. I'll try to find a workaround to catch up with current updates. Although, this would be a bit challenging considering how many new things XLA has introduced. Any help would be appreciated.",Hi  Can you please assist on above comments from . Thank you!,Hi  Any update on this PR? Please. Thank you!," Hi!, There will be another attempt to merge current state of the branch with the main branch next week. Also flash attention (https://arxiv.org/abs/2205.14135) is kind of doing the similar thing as this PR. This PR implements more naive version, however, it might be improved.",Hi  Any update on this PR? Please. Thank you!,Hi  Any update on this PR? Please. Thank you!,Hi  Any update on this PR? Please. Thank you!,Hi  Can you please resolve conflicts? Thank you!,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
1881,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Higher Memory Usage with model.predict in Recent TF Versions (TF 2.10, 2.11 etc))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary   we use `pip install` to reproduce the issue although we use `poetry` in production.   Tensorflow Version 2.10 and 2.11 and maybe others after 2.8.2 all have this issue  Custom Code No (can reproduce the issue with simple code below)  OS Platform and Distribution    Mobile device _No response_  Python version 3.8.15  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? We use TF to serve multiple models in production and recently we tried updating from TF 2.8.2 to TF 2.10. We noticed on many of our services/workers there was a memory increase/leak with some services even crashing. We had to revert back to 2.8.2. I also see the issue in TF 2.11. With this deploy in particular the only thing we upgraded was Tensorflow. No application code changes. We monitor the memory with DataDog and all our host graphs showed major spikes. These are mostly `tf.keras` models. I thought originally it was a `ddtrace` issue because from TF 2.9 and up `model.predict` started printing progress bars and I thought all those were getting collected in  DataDog traces. But I set `verbose=0` on all model.predict calls and turned off ddtrace an the issue was still there. Some screenshots from two different services    Standalone code to reproduce the issue Run this code on a machine with 8 cores and 16GB of ram for example. Here is also a colab with the same code as below although not sure you can replicate the memory leak issue in colab. I am replicating the issue o)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DrChrisLevy,"Higher Memory Usage with model.predict in Recent TF Versions (TF 2.10, 2.11 etc)",Click to expand!    Issue Type Bug  Source binary   we use `pip install` to reproduce the issue although we use `poetry` in production.   Tensorflow Version 2.10 and 2.11 and maybe others after 2.8.2 all have this issue  Custom Code No (can reproduce the issue with simple code below)  OS Platform and Distribution    Mobile device _No response_  Python version 3.8.15  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? We use TF to serve multiple models in production and recently we tried updating from TF 2.8.2 to TF 2.10. We noticed on many of our services/workers there was a memory increase/leak with some services even crashing. We had to revert back to 2.8.2. I also see the issue in TF 2.11. With this deploy in particular the only thing we upgraded was Tensorflow. No application code changes. We monitor the memory with DataDog and all our host graphs showed major spikes. These are mostly `tf.keras` models. I thought originally it was a `ddtrace` issue because from TF 2.9 and up `model.predict` started printing progress bars and I thought all those were getting collected in  DataDog traces. But I set `verbose=0` on all model.predict calls and turned off ddtrace an the issue was still there. Some screenshots from two different services    Standalone code to reproduce the issue Run this code on a machine with 8 cores and 16GB of ram for example. Here is also a colab with the same code as below although not sure you can replicate the memory leak issue in colab. I am replicating the issue o,2022-11-24T21:14:17Z,comp:runtime comp:keras type:performance TF 2.11,open,17,55,https://github.com/tensorflow/tensorflow/issues/58676,", I was facing a different error while trying to execute the code. Kindly find the gist of it here and provide the dependencies which helps to debug the issue. Thank you!", yes of course you need to download an image first. I updated the code and added a `wget` for you to download the image.  I updated the description/code slightly. Also added a colab but Im not sure you can simply reproduce the mem leak in colab. I added a minimal Dockerfile to reproduce the issue in a simple ENV., I added even more details and log outputs in the simple reproducible Docker ENV for both 2.8.2 and 2.11. Let me know if you need anything else from my end., It looks TF2.9.1 doesn't seem to have the memory issue.,", I was able to reproduce the issue on tensorflow v2.9, v2.11 and nightly. Kindly find the gist of it here."," I am an engineer from Intel and was able to reproduce this issue on TF2.10+. This commit which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak. As a work around, could you please try setting the env var to false at runtime.  e.g. TF_RUN_EAGER_OP_AS_FUNCTION=false python test_script.py",">  I am an engineer from Intel and was able to reproduce this issue on TF2.10+. This commit which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak. As a work around, could you please try setting the env var to false at runtime. e.g. TF_RUN_EAGER_OP_AS_FUNCTION=false python test_script.py Cc'ing , the author of the commit.", from the graphs I see a higher peak memory usage which then goes back down  is there ever a case where the memory usage does not go back down?,">  from the graphs I see a higher peak memory usage which then goes back down  is there ever a case where the memory usage does not go back down? Those graphs showing the memory going back down were either services crashing or we deployed a revert to go back to TF 2.8.2. That was in our prod system just to give some more ""proof"" of the issue. From now on, ignore those graphs. In every case I have seen the memory does **not** go back down. Look at the reproducible example I provided. The memory will grow forever. ",">  I am an engineer from Intel and was able to reproduce this issue on TF2.10+. This commit which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak. As a work around, could you please try setting the env var to false at runtime. e.g. TF_RUN_EAGER_OP_AS_FUNCTION=false python test_script.py I tried changing this ENV var in the simple reproducible example I provided.  `vim .env.docker`  and add one line to this file > `TF_RUN_EAGER_OP_AS_FUNCTION=false` Start the docker: `docker run envfile .env.docker it tf /bin/bash`   This means that in the suspected commit, `os.getenv(""TF_RUN_EAGER_OP_AS_FUNCTION"", ""1"") == ""1""` will return `False`. Then copy/paste the code I provided for producing the memory leak. Let it run for an hour or so.  **I still see the memory leak occurring in TF 2.11** **No memory leak in TF 2.10 if**  `TF_RUN_EAGER_OP_AS_FUNCTION=false` See Comment below"," Can you please share the TFversion you tried? I used TF2.10 and did not see the issue with the env var set to false. However, I see the issue in TF2.11 again and on further investigation found that the env var was removed from TF2.11+ with this commit Can you please test TF2.10? cc:  ",">  Can you please share the TFversion you tried? I used TF2.10 and did not see the issue with the env var set to false. However, I see the issue in TF2.11 again and on further investigation found that the env var was removed from TF2.11+ with this commit Can you please test TF2.10? >  > cc:  Good Catch. Yes I can confirm that. I updated my prev comment. Memory leak still in TF 2.11 and not in TF 2.10 **if** we set `TF_RUN_EAGER_OP_AS_FUNCTION=false`",So what would you advise on next steps? I guess one option is to run TF 2.10 with `TF_RUN_EAGER_OP_AS_FUNCTION=false` in our prod env. Another option would be to wait until this is sorted out in TF 2.11 or next version? ,"  I have reproduced the issue and looking into how best to address it.  TF 2.9 does not have this issue.  It's only in TF 2.10 and believe it got introduced in this commit c1286fe88e1.  Its same thing identified by nervana. One workaround in TF 2.10+ is, convert the input array to a TF Dataset for example    This will stop the memory leak.  It has so far only been observed when you pass arrays directly into `model.predict` or to other model APIs (not verified). The reason setting the flag `TF_RUN_EAGER_OP_AS_FUNCTION=false` no longer works in TF 2.11 is possibly due to this commit 0bbe98a93de, that disables the ability to turn off `EagerOpsFunction`.", . Thanks for looking into this. We call model.predict all over our code base in lots of different places with numpy array input and various types of models. Some of the inputs expect multiple types of input etc. I don't think I will be converting everything to  `tf.data.Dataset.from_tensor_slices`. It's a bit of a  hack solution. Do you have a rough estimate of how long it would take for this problem to get solved? Are we talking weeks/months?,", ,  , any updates on this?",", ,   We are seeing this issue for TF 2.11 and we also use poetry in production. Is there a TAT on this issue as this looks like a major issue with the newer TF versions.",Update:  and  from TF apis are currently assigned to take a look. Alan/Jeremy can you provide an update?,Can you provide an update on the issue? It affects our production system as well. ,Its still being looked into.  Does the workaround suggested here  https://github.com/tensorflow/tensorflow/issues/58676issuecomment1341147862 work for you until we have an resolution?,"> Its still being looked into. Does the workaround suggested here   CC(Higher Memory Usage with model.predict in Recent TF Versions (TF 2.10, 2.11 etc)) (comment) work for you until we have an resolution? Nope. The types of dataset we use are really complex and use sophisticated data on timeseries. I know how cool TF Dataset is but moving everything to TF Datset is not doable atm. ",> Its still being looked into.  Does the workaround suggested here  https://github.com/tensorflow/tensorflow/issues/58676issuecomment1341147862 work for you until we have an resolution? It would be a pretty big refactor to do this. Also correct me if I'm wrong but aren't TF datasets more for setting up efficient pipelines to utilize CPU and GPU when training models etc. Here we are just talking about inference and serving models. ,Thanks for the feedback.  We will followup and try to find the root of the memory leak to this issue.,"Hey  , just checking for another update. Is it still being worked on? Thank you.","Hi , I'm taking a look at this and other memory leaks. We're actively looking into this to find the root cause.",Hello! Is there any update on fixing this memory leak issue? This leak prevents us from upgrading of tensorflow since version 2.8.0  quite problematic.. Thank you very much!,Is the problem fixed in release 2.13?,What is the latest on this issue? Is there a workaround in TF 2.11? We are also seeing increased memory usage for models on TF 2.11 compared to TF 2.4,   Any update on this ?,"This is work in progress, hope to have an update soon."
1765,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Feature request (TensorFlow Lite): Implement mechanism to load Flex ops in C for Android)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code**: no    **OS Platform and Distribution**: macOS Monterey 12.6    **TensorFlow installed from**: source    **TensorFlow version**: 2.10.0    **Python version**: 3.10.8    **Bazel version**: 5.1.1    **GCC/Compiler version**: Apple clang 14.0.0 (clang1400.0.29.202)    **CUDA/cuDNN version**: Not using CUDA    **GPU model and memory**: Not using GPU acceleration    **Exact command to reproduce**: Feature request, no   Describe the problem This is a feature request. In TensorFlow Lite 2.7 and higher, if a model contains Flex ops you must go through Java to load the Flex library (see NativeInterpreterWrapper.java and FlexDelegate.java). We develop a crossplatform app where most of the ML inference logic runs in a separate C library. We test that library separately from the rest of the app using GTest, without going through the Android runtime. For our purposes, it would be much more convenient if we could load the Flex library directly in C. As a tentative implementation, we could move the logic in `NativeInterpreterWrapper.java` to C, so that we could automatically load Flex ops when creating models from the C API with `TfLiteModelCreate`. The existing Java classes could be kept as shallow wrappers to the underlying C implementation. Java and C entry points could be toggled at build time through a flag. Is this something that you would consider implementing, and, if not, would you accept PRs that go in that direction?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,andreasavio,Feature request (TensorFlow Lite): Implement mechanism to load Flex ops in C for Android," System information    **Have I written custom code**: no    **OS Platform and Distribution**: macOS Monterey 12.6    **TensorFlow installed from**: source    **TensorFlow version**: 2.10.0    **Python version**: 3.10.8    **Bazel version**: 5.1.1    **GCC/Compiler version**: Apple clang 14.0.0 (clang1400.0.29.202)    **CUDA/cuDNN version**: Not using CUDA    **GPU model and memory**: Not using GPU acceleration    **Exact command to reproduce**: Feature request, no   Describe the problem This is a feature request. In TensorFlow Lite 2.7 and higher, if a model contains Flex ops you must go through Java to load the Flex library (see NativeInterpreterWrapper.java and FlexDelegate.java). We develop a crossplatform app where most of the ML inference logic runs in a separate C library. We test that library separately from the rest of the app using GTest, without going through the Android runtime. For our purposes, it would be much more convenient if we could load the Flex library directly in C. As a tentative implementation, we could move the logic in `NativeInterpreterWrapper.java` to C, so that we could automatically load Flex ops when creating models from the C API with `TfLiteModelCreate`. The existing Java classes could be kept as shallow wrappers to the underlying C implementation. Java and C entry points could be toggled at build time through a flag. Is this something that you would consider implementing, and, if not, would you accept PRs that go in that direction?",2022-11-24T11:24:54Z,stat:awaiting response type:feature stale comp:lite TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58669,Hi  ! Really sorry for the late response.  You can build the flex delegate in C using Bazel and load it with interpreter  options if necessary. ( but it takes the select ops runtime during inference if already built into bazel bin folder). Ref Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Regularizer loss out of scope in train step when loss includes conditional statements)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution CentOS Linux 7 (Core)  Mobile device _No response_  Python version 3.7.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jacob-hjortlund,Regularizer loss out of scope in train step when loss includes conditional statements,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0  Custom Code Yes  OS Platform and Distribution CentOS Linux 7 (Core)  Mobile device _No response_  Python version 3.7.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-23T22:32:17Z,stat:awaiting response type:bug stale TF 2.11,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58666,"hjortlund, I do not have access to the link you have provided. Could you please provide the required permissions to view the files or provide the colab gist. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1306,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFlite IOS will not build when imported as part of a package. 100+ Undefined Symbols reported at link phase)ï¼Œ å†…å®¹æ˜¯ ( Issue Type Build/Install  System information Mac OS 12.6.1 XCode 14.1 TFLite 2.9.1  Describe the problem I am trying to add TFLite to a swift package that we have. Essentially our code runs its own tflite models, so it has a dependency on TFLite Swift, which depends on TFLiteC. Since this is a swift package, it is not an option to use pods. Using information from this thread, I was able to add the TFLite dependencies, and successfully build my swift package as a standalone package. However, once I add my package into an actual project, the linking phase completely breaks, with over 100 undefined symbols.  This also happens when using the examples at the end of the above thread. For clarity, adding the package here, under Frameworks, Libraries, and Embeded content causes the linking failure.    Source code / logs Here is a link to the test project that reproduces this error. (50mb so I cannot directly upload it here) https://drive.google.com/file/d/1xVqSxF7z94SqFSrf1QWBkwHesIug0wt/view?usp=sharing)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,arctop-sk,TFlite IOS will not build when imported as part of a package. 100+ Undefined Symbols reported at link phase," Issue Type Build/Install  System information Mac OS 12.6.1 XCode 14.1 TFLite 2.9.1  Describe the problem I am trying to add TFLite to a swift package that we have. Essentially our code runs its own tflite models, so it has a dependency on TFLite Swift, which depends on TFLiteC. Since this is a swift package, it is not an option to use pods. Using information from this thread, I was able to add the TFLite dependencies, and successfully build my swift package as a standalone package. However, once I add my package into an actual project, the linking phase completely breaks, with over 100 undefined symbols.  This also happens when using the examples at the end of the above thread. For clarity, adding the package here, under Frameworks, Libraries, and Embeded content causes the linking failure.    Source code / logs Here is a link to the test project that reproduces this error. (50mb so I cannot directly upload it here) https://drive.google.com/file/d/1xVqSxF7z94SqFSrf1QWBkwHesIug0wt/view?usp=sharing",2022-11-23T18:57:16Z,comp:lite subtype:macOS TF 2.9,closed,0,2,https://github.com/tensorflow/tensorflow/issues/58663,"sk you appear to not have included the standard C++ library in your project. You can try to add `libc++.tdb` to your ""Frameworks, Libraries, and Embedded Content"" section: !index Also, IIRC, this is only an issue if you build TFLite as static framework. Consider building them as dynamic frameworks if possible. Your project appears to be in Swift. Swift cannot directly talk to C++, so you may need to deal with the complexity of writing some wrappers in ObjectiveC around the TFLite API. I would either make an ObjectiveC project instead of Swift, or consider getting TFLite into your project using Cocoa Pods: https://www.tensorflow.org/lite/guide/ios","  Thanks. In fact adding libc++ does solve the linking issue. This was a test, where ultimately i will be using TFLite swift as the API, it just depends on getting this part correctly compiling. As stated, i need to have this work as a swift package until the team releases a working swift package as mentioned in the referenced thread. For now, this should work."
1901,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inconsistency in connection with setting both clipnorm and global_clipnorm attibute for an optimizer)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? **Basic problem statement:** It is possible to set both the `clipnorm` and the `global_clipnorm` attributes for an optimizer. A model configured with this optimizer can be saved, but loading fails with a ValueError. **Details:** It is not possible to instantiate a Keras optimizer with both `clipnorm` and `global_clipnorm` kwargs set, since that produces a value error, e.g.  However, it is possible to first instantiate the optimizer with just one of the two settings, and then, likely by accident, set the other via the optimizer's corresponding attribute:  This does not raise any exceptions. An optimizer with both `clipnorm` and `global_clipnorm` set can be set as the optimizer for a Keras model, and the model trains with this optimizer without any problems. The Keras model can also be **saved** along with this optimizer without any warning or error raised. However, when attempting to **load** a Keras model with such an optimizer, the same ValueError as mentioned above is raised as the saved optimizer is instantiated. One possible solution would be to have the already existing `.setter` method check whether `self._global_clipnorm` is already set and delete the latter setting if it is. Same for `.setter` and checking `self._clipnorm`. However, I b)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,gpetho,Inconsistency in connection with setting both clipnorm and global_clipnorm attibute for an optimizer,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? **Basic problem statement:** It is possible to set both the `clipnorm` and the `global_clipnorm` attributes for an optimizer. A model configured with this optimizer can be saved, but loading fails with a ValueError. **Details:** It is not possible to instantiate a Keras optimizer with both `clipnorm` and `global_clipnorm` kwargs set, since that produces a value error, e.g.  However, it is possible to first instantiate the optimizer with just one of the two settings, and then, likely by accident, set the other via the optimizer's corresponding attribute:  This does not raise any exceptions. An optimizer with both `clipnorm` and `global_clipnorm` set can be set as the optimizer for a Keras model, and the model trains with this optimizer without any problems. The Keras model can also be **saved** along with this optimizer without any warning or error raised. However, when attempting to **load** a Keras model with such an optimizer, the same ValueError as mentioned above is raised as the saved optimizer is instantiated. One possible solution would be to have the already existing `.setter` method check whether `self._global_clipnorm` is already set and delete the latter setting if it is. Same for `.setter` and checking `self._clipnorm`. However, I b",2022-11-23T12:08:49Z,stat:awaiting response type:bug comp:keras TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58655,", Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!","Sorry about this, thank you, moved to https://github.com/kerasteam/tfkeras/issues/347",Are you satisfied with the resolution of your issue? Yes No
1668,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.io.gfile.rename cause UnicodeError on non-English Windows)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflow2.9.1  Custom Code No  OS Platform and Distribution Windows 10 Family Chinese  Mobile device _No response_  Python version 3.10.6 (conda)  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? The problem is first caused by the rename issue at https://github.com/tensorflow/tensorflow/issues/45980 Even when this rename error happen, it should produce   However, when the language of Windows is set to be nonEnglish language, like Chinese, it cause `UnicodeError` like  I guess the reason is that, the `""Access is denied""` is actually the error message formatted by Win32 `FormatError()` (https://learn.microsoft.com/enus/windows/win32/api/winbase/nfwinbaseformatmessage). When `dwLanguageId` is set to be something like `MAKELANGID(NEUTRAL, SUBLANG_DEFAULT)`, it will produce a Chinese encoding error message `""æ‹’ç»è®¿é—®""` in `cp936` (`gbk`) encoding when the system code page is `936` (Simplified Chinese with `gbk` encoding). When you want to decode it as python string using `utf8` encoding, it cause this problem.  Note `""æ‹’ç»è®¿é—®""` in `gbk` is `""\xbe\xdc\xbe\xf8\xb7\xc3\xce\xca""`.  Standalone code to reproduce the issue Just follow the official tutorial at https://www.tensorflow.org/tutorials/images/data_augmentation   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,E-train-Liu,tf.io.gfile.rename cause UnicodeError on non-English Windows,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflow2.9.1  Custom Code No  OS Platform and Distribution Windows 10 Family Chinese  Mobile device _No response_  Python version 3.10.6 (conda)  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? The problem is first caused by the rename issue at https://github.com/tensorflow/tensorflow/issues/45980 Even when this rename error happen, it should produce   However, when the language of Windows is set to be nonEnglish language, like Chinese, it cause `UnicodeError` like  I guess the reason is that, the `""Access is denied""` is actually the error message formatted by Win32 `FormatError()` (https://learn.microsoft.com/enus/windows/win32/api/winbase/nfwinbaseformatmessage). When `dwLanguageId` is set to be something like `MAKELANGID(NEUTRAL, SUBLANG_DEFAULT)`, it will produce a Chinese encoding error message `""æ‹’ç»è®¿é—®""` in `cp936` (`gbk`) encoding when the system code page is `936` (Simplified Chinese with `gbk` encoding). When you want to decode it as python string using `utf8` encoding, it cause this problem.  Note `""æ‹’ç»è®¿é—®""` in `gbk` is `""\xbe\xdc\xbe\xf8\xb7\xc3\xce\xca""`.  Standalone code to reproduce the issue Just follow the official tutorial at https://www.tensorflow.org/tutorials/images/data_augmentation   Relevant log output  ",2022-11-21T23:17:42Z,stat:awaiting response type:bug stale comp:data TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58641,"Update: Do not need to worry about the rename error. It is caused by a very old `tensorflowdatasets` version. After updating the `1.2.0` version from conda to `4.7.0` version from pip, the rename error disappears. Now we only need to solve the encoding error. Many other file errors can reproduce it. Like, try to rename a nonexisting file.","trainLiu, Could you please check if tfdsnightly works for you and also please try this code **pip install tensorflowdatasets==4.7.0** for the latest tfds.  Could you please raise this issue on the TensorFlow Datasets repository as well? https://github.com/tensorflow/datasets/issues Thank you!","I am sorry for misleading. **I am reporting for the encoding error instead of the rename error.** That is, when there is some filesystem error on windows (including coping a nonexisting file or being denied to rename a file), instead of reporting the error normally, it will raise an `UnicodeError` (which may be caused by the encoding mismatch). For example  On `enUS` Windows system (default code page: `1252`), it report the error as expected  On `zhCN` Windows system (default code page `936`), should work as above. Or if follow the Windows locale, it should work like   However, it now produce  I also guessed the cause of this issue, and wrote it in the ""Current Behaviour"" section of the first issue. By the way, as my first P.S. said, I switched to `tensorflowdataset` `4.7.0`, and the rename error is fixed. But it is not the main content of this issue. Again, sorry for misunderstanding.","Hi trainLiu , Apologies for delayed response.From error log I see that error originated from the code `tfds.load()`  It seems the error is generated from trying to decode the images. Could you please add the argument  `tfds.load(decoders={  'image': tfds.decode.SkipDecoding() }) ` and let us know if it works for your case.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
811,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bazel build failure)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.4.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04.6 LTS  Mobile device _No response_  Python version 3.6.9  Bazel version 3.1.0  GCC/Compiler version 7.5.0  CUDA/cuDNN version _No response_  GPU model and memory Jetson Nano, 32GB of memory  Current Behaviour? Compiling Tensorflow fails at the end of the process without being able to compile the pooling_ops_gpu file. I am trying to compile Tensorflow in order to enable the tflite_with_xnnpack flag  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,zoythum,Bazel build failure,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.4.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04.6 LTS  Mobile device _No response_  Python version 3.6.9  Bazel version 3.1.0  GCC/Compiler version 7.5.0  CUDA/cuDNN version _No response_  GPU model and memory Jetson Nano, 32GB of memory  Current Behaviour? Compiling Tensorflow fails at the end of the process without being able to compile the pooling_ops_gpu file. I am trying to compile Tensorflow in order to enable the tflite_with_xnnpack flag  Standalone code to reproduce the issue   Relevant log output  ",2022-11-21T09:40:36Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58630,", Could you please try installing TensorFlow v2.4 with Complier GCC 7.3.1, Bazel 3.1.0, CUDA 8.0 and Cudnn 11.0 and check if you are facing the same error. For more information please take a look at the tested build configurations. Thank you! Version  11.0",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( Cannot build with CUDA support on Windows.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version TF 2.11  Custom Code No  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version 5.3.2  GCC/Compiler version MSVC 14.34.31933  CUDA/cuDNN version 11.8/8.6  GPU model and memory RTX3090 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Zhaopudark, Cannot build with CUDA support on Windows.,Click to expand!    Issue Type Support  Source source  Tensorflow Version TF 2.11  Custom Code No  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version 5.3.2  GCC/Compiler version MSVC 14.34.31933  CUDA/cuDNN version 11.8/8.6  GPU model and memory RTX3090 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-21T09:31:58Z,type:build/install comp:gpu subtype:windows,closed,0,8,https://github.com/tensorflow/tensorflow/issues/58629,"Hi  ! It is correct from 2.11 onwards, we have to rely on TensorflowDirect ML  plugin / Windows Subsystem linux for GPU usage in windows machines. Attached relevant thread  on Tensorflow direct ML  plugin for reference. Thank you!","  Thanks a lot.  I have been compiling and using the latest version of TensorFlow GPU on Windows for several years, which can always have all features supported on Linux, such as AVX2, XLA .etc., and can work well with other components, such as PowerShell, MySQL, Python, Conda .etc. It can even benefit from automatic GPU overclocking on Windows. It may a little bit disappointing that since TF 2.11 onwards, the original Windows Platform will not be supported by TensorFlow GPU.  Anyway, thanks a lot for contributing to TF and making it better and better. Nowadays, I have totally changed to WSL2, compiling and using TF2.11 GPU successfully. In my practice, there may be approximately a 45% performance loss if compared to the one on the original Window. But the cost is still acceptable. WSL2 may always be a lessthancorrect and desirable choice.  Most users choose WSL2 may because some of the codes or components they need are not supported on Windows but are supported on Linux, and, they do not necessarily have two separate platforms. But from my point of view,  for most users, the codes and components they want are actually supported on Windows, it just requires them to invest some time to learn extra Windows operations and maintenance, such as the running mechanisms, the syntax of PowerShell or CMD, and the command line mode of other tools on Windows, and then, they can find a good and easy way to realize the desired codes or components on Windows well. That is, the reason for most users not choosing the Window platform may be to save time, but if the Windows community is also active, users don't need to spend much time. And, if we always use new versions, the activity of the community is not so important. Because people who are willing to try new features are always in the minority, there is a limited amount of experience to refer to. Moreover, from my practice, I found the numbers of unexpected compiling bugs of a new TF version on both Linux and Windows are usually similar, which means the maintenance difficulty is probably similar between Linux and Windows.  Therefore, the sudden nolongersupporting for original Windows is perplexing. Long term, if a user of a machine learning framework uses the Window platform, he/she will be seen as a unicorn. This is unfair because there is always someone who does not have that much money to support two separate platforms, or even three, and as a researcher, the Windows platform is naturally the most economical choice in order to cope with clerical work simultaneously. These people are even willing to spend more time learning Windows operation and maintenance for better usage of the machine learning frameworks but are often discriminated against for ""why don't use Mac and Linux servers?"". Users' choices should not be discriminated against, but unfortunately, this nolongersupporting for original Windows will make Windows users feel more discriminated, even if it is not the official intent. Even though maintaining one more platform will cost more manpower, I still hope the original Windows support will come back.  Thanks again and best wishes.","Hi  ! We are not dropping windows support . You can still use TensorflowCPU in windows machines. It is just that GPU support will be maintained by third party colloborators (Intel, AWS, ARM, linaro etc. ) considering the effort to maintain the integrity and  diversity of Tensorflow framework . Ref You can use TensorflowDirect ML plugin in same windows machine as suggested in previous thread.  (Note: A Windows user myself). Thank you!",", thank you. I have also tried TensorflowDirect ML plugin. Even though it is convenient for its simple configuration, it is still in the development phase with less than complete functionality, such as does not support  XLA, mixed precision .etc.  So, now the situation may be that it is not available in a short term to use the latest version of TensorflowGPU or its equivalent replacement on Windows native platform as same as the original and without performance loss. Hope this 'painful' period will end soon.  Thank you again, and thanks to all TensorFlow contributors/maintainers for your efforts.", ! Thanks for sharing your concern.  ! Could you look at this issue. Thank you!,", Your concerns regarding dropping the TF_GPU support on windows native is well noted and will be escalated to the concern team.But at present it is official from the documentation that: `TensorFlow 2.10 was the last TensorFlow release that supported GPU on nativeWindows. Starting with TensorFlow 2.11, you will need to install TensorFlow in WSL2` and there is little we can do here for now at least.  I am not sure whether Windows native support can be extended in Future versions or not but also can't deny it. It's all based on requirements from community and Tensorflow Team is keenly watching the observations,concerns,inputs from community and may act accordingly. For time being as there is little we can do now shall we close this issue ?  Thankyou!","  Thanks for your detailed information, I will always look forward to a better and widely supported TensorFlow.  And, this issue can be closed. Thank you.",Are you satisfied with the resolution of your issue? Yes No
663,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.distribute.experimental.rpc.Server memory leak on server)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory Nvidia GeForce RTX 2070 6233MB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,YHL04,tf.distribute.experimental.rpc.Server memory leak on server,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory Nvidia GeForce RTX 2070 6233MB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-21T06:35:21Z,stat:awaiting response type:bug stale comp:apis type:performance TF 2.11,closed,0,13,https://github.com/tensorflow/tensorflow/issues/58628,  I tried to reproduce the issue on Colab by using TF v2.11 but facing a different error. Could you please find the gist here for reference and confirm the same. Thank you!,"If you comment out the two lines it should run without error:      server.close()      print(""After shutdown: "", process.memory_info().rss/1024/1024) but the graph still shows memory increasing linearly",  I was able to reproduce the issue on Colab using TF v2.11. Please find the gist here for reference. Thank you!,  Just wondering if it will be a quick fix or what the plan is since my project is on a deadline... Thank you.," , I tested the code both on CPU and GPU.Even with CPU also i observe continuous increase in memory which is conflicting as you confirmed with CPU: `(Memory stays constant after awhile with CPU)` .Please refer to attached gist where i observed the same behaviour with CPU also.Please go through and confirm whether the CPU behaviour here is also same of GPU ? Thankyou!",Uncommented:  TF 2.9 CPU: !image TF 2.11 CPU: !image TF 2.11.0 CPU on my desktop: Commented out:  !Figure_1 Not sure why I wasn't able to reproduce the same result...,"Hi  , Thanks for coming back again to confirm the behaviour on CPU.As you can also confirm the behaviour on CPU also the Memory is in increasing trend,can we confirm that it is not abnormal ? Could you share your opinion. Thankyou!","Hi ,  I'm not sure why the memory increase would be intended, especially when it ends up freezing the desktop, each Client call is independent and there is no reason to store anything after the Client call. If the behavior is intended, could you please explain why?","Hi  , Iam not confirming yet it as intended behaviour. My intention is to confirm with you the whether the behaviour of Memory Increase is there both on CPU and GPU?  could you please look at the issue. Thankyou!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. I tried to execute the mentioned code on tfnightly and observed the code was provided the output as expected. Kindly find the gist of it here. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
675,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(from keras.models import load_model raises no module named tensorflow.compat error)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.11  Custom Code No  OS Platform and Distribution Windoes 11  Mobile device _No response_  Python version 3.7.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sryu1,from keras.models import load_model raises no module named tensorflow.compat error,Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.11  Custom Code No  OS Platform and Distribution Windoes 11  Mobile device _No response_  Python version 3.7.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-18T12:10:40Z,type:support comp:keras subtype:windows TF 2.11,closed,0,11,https://github.com/tensorflow/tensorflow/issues/58610,"Hi  , Instead of `keras.models import load_model`, Please use `from tensorflow.keras.models import load_model` .",I've already tried it previously and it didn't work  ,"Also, I noticed the tf 2.10 tag, for tensorflow 2.10,  `import keras.models import load_model` works but not for 2.11","Hi  , Sorry i didn't noticed your OS earllier. Please refer the note from Documentation. `Caution: TensorFlow 2.10 was the last TensorFlow release that supported GPU on nativeWindows. Starting with TensorFlow 2.11, you will need to install TensorFlow in WSL2, or install tensorflowcpu and, optionally, try the TensorFlowDirectMLPlugin` Please refer the attached link for more details.","Ah, ok. Well, if I'm using tensorflowcpu, it should work, right? It seems like it doesn't though, it gives back the same error...",", Could you please check whether the instructions mentioned in documentation followed correctly?Please refer link here.Please ignore step5 which is for GPU setup.Please cross check again and confirm us if still problem persists. Thankyou!","Yes, using a conda environment worked... Is there any way to use windows native and tensorflowcpu though? Or is that the only option for tf 2.11?","Wait, nevermind, using tensorflowcpu works with windows native now, I tried uninstalling everything related to tensorflow (tensorflowcpu, tensorflowintel, keras, etc.) with pip then installed tensorflowcpu. :)","So, in conclusion: `from keras.models import load_model` does not work with native windows and tensorflow 2.11 It works with tensorflowcpu OR tensorflow 2.11 with a conda environment or other sorts of similar.","That's it, thank you so much! I'll open this issue up again if I have any question regarding this...",Are you satisfied with the resolution of your issue? Yes No
1863,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite benchmark tool - example to use input_layer_value_files)ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: No    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: Source    **TensorFlow version (use command below)**:     **Python version**:    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You can obtain the TensorFlow version wit)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AbinayaKumar25,TFLite benchmark tool - example to use input_layer_value_files,"Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: No    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: Source    **TensorFlow version (use command below)**:     **Python version**:    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You can obtain the TensorFlow version wit",2022-11-16T18:11:12Z,stat:awaiting response type:support stale comp:lite,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58603,  Could you please refer to the Benchmark tools and let us know if it helps. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1481,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix the endianness issue in v1 frozen graphs in python:lite_test on BE machines)ï¼Œ å†…å®¹æ˜¯ (This PR is to add bigendian support to TensorFlow v1 frozen graphs. Subtests `FromFrozenGraphObjectDetection.testTFLiteGraphDef` and `FromFrozenGraphObjectDetection.testModifyIOToUint8` in `//tensorflow/lite/python:lite_test` fail on s390x (bigendian (BE) arch) because the tensor content in `graph_def` which is parsed from the file containing frozen GraphDef is in littleendian (LE) format. To solve this issue, this PR refactored the `byte_swap_tensor` related code in `tensorflow/python/saved_model/utils_impl.py` and added helper function `swap_tensor_content_in_graph_node()` to deal with the LE/BE conversion in frozen graphs. This function is invoked in frozen graph read/write process so that v1 frozen graphs would be stored with LE format in files and converted to BE format right after being loaded into memory on BE machines. This code change also extended the covered code paths for byteswapping in read/write process of TensorFlow v2 saved models. This PR could fix the above mentioned subtests in `//tensorflow/lite/python:lite_test` on s390x (BE machines) and will not cause any regressions on LE/BE platforms. Fixes issue https://github.com/tensorflow/tensorflow/issues/57215 . Signedoffby: KunLu )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kun-lu20,Fix the endianness issue in v1 frozen graphs in python:lite_test on BE machines,"This PR is to add bigendian support to TensorFlow v1 frozen graphs. Subtests `FromFrozenGraphObjectDetection.testTFLiteGraphDef` and `FromFrozenGraphObjectDetection.testModifyIOToUint8` in `//tensorflow/lite/python:lite_test` fail on s390x (bigendian (BE) arch) because the tensor content in `graph_def` which is parsed from the file containing frozen GraphDef is in littleendian (LE) format. To solve this issue, this PR refactored the `byte_swap_tensor` related code in `tensorflow/python/saved_model/utils_impl.py` and added helper function `swap_tensor_content_in_graph_node()` to deal with the LE/BE conversion in frozen graphs. This function is invoked in frozen graph read/write process so that v1 frozen graphs would be stored with LE format in files and converted to BE format right after being loaded into memory on BE machines. This code change also extended the covered code paths for byteswapping in read/write process of TensorFlow v2 saved models. This PR could fix the above mentioned subtests in `//tensorflow/lite/python:lite_test` on s390x (BE machines) and will not cause any regressions on LE/BE platforms. Fixes issue https://github.com/tensorflow/tensorflow/issues/57215 . Signedoffby: KunLu ",2022-11-16T14:16:37Z,awaiting review comp:lite ready to pull size:M,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58601,"Hi  , Could you please review this PR when you have some time? Thank you very much!","Hi  , Hope all is well.  Could you please have a look at this PR? Thank you!", Can you PTAL at this? This is really not my domain. Thanks1,"Hi  , Could you please review this PR when you have some time? Thank you very much!","Hi  , Thanks for your valuable comments! I've improved the code change and updated the PR description accordingly. Could you please take a look? ","Hi  , Happy New Year! Could you please review this PR again? Thank you very much!","Hi  , I've solved the conflicts from the master branch. Could you please take a look at this PR?  Thank you very much!","Hi  , Could you please review this PR again when you have some time? Thank you very much!",Thanks  !,lu20 Thanks for your contribution!
637,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DNN library is not found.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mechmech1995,DNN library is not found.,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-15T17:10:18Z,stat:awaiting response type:bug stale comp:model TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58592,Hey check this out does it solve the problem? https://stackoverflow.com/a/71039779,As for now we will have to opt for a lower version.,"Hi  ! Would it possible to share the reproducible code too. There might be multiple scenarios as mentioned in above thread.. If It is TF 2.10 version in your use case  , I assume that you have installed Cuda 11.2 and Cudnn 8.1 through below command and set path for cuda files.  As you said it is fixed in nightly version, Feel free to test in 2.11.0rc2 version too. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
740,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF Lite C library fails to build)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Linux Debian 11 (bullseye)  Mobile device _No response_  Python version 3.9.2  Bazel version _No response_  GCC/Compiler version 10.2.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell git clone depth 1 https://github.com/tensorflow/tensorflow mkdir tflite_build cd tflite_build cmake ../tensorflow/tensorflow/lite/c   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,stakach,TF Lite C library fails to build,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Linux Debian 11 (bullseye)  Mobile device _No response_  Python version 3.9.2  Bazel version _No response_  GCC/Compiler version 10.2.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell git clone depth 1 https://github.com/tensorflow/tensorflow mkdir tflite_build cd tflite_build cmake ../tensorflow/tensorflow/lite/c   Relevant log output  ,2022-11-14T09:34:48Z,stat:awaiting response type:build/install comp:lite TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58550,The build works if I do the following: `cp ./tensorflow/tensorflow/lite/core/c/c_api.cc ./tensorflow/tensorflow/lite/c/c_api.cc` all up this generates the desired output  outputs  as expected,Hi  ! Thanks  for reporting this bug. I could replicate this issue in master branch.   ! Could you look at this bug. Thank you!,Let me handle this issue.," , https://github.com/tensorflow/tensorflow/commit/17e3646683f0149091384d250b27d6f52556ee78 fix should resolve your build issue. Thanks  for the quick fix.",working for me,"Thanks for the confirmation, could you please close the issue as well. ",Are you satisfied with the resolution of your issue? Yes No
858,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Trying to train transformer like the NMT Keras Transformer, for passage summarization task. The model's masked accuracy goes upto ~30% but the transformer's output is very poor. Could you help me understand what I might be doing wrong?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,iwinterknight,"Trying to train transformer like the NMT Keras Transformer, for passage summarization task. The model's masked accuracy goes upto ~30% but the transformer's output is very poor. Could you help me understand what I might be doing wrong?",Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-13T19:38:32Z,stat:awaiting response type:support stale type:performance TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58549,"Hi , Iam unable to access the provided colab file. I request you to please convert it into Github gist and share the same with us. Thankyou!","Hi, posting the link here. https://github.com/thewinterknight14/frost_queen/blob/master/transformer_passage_summarization.ipynb It is the same code as the tutorial with `cnn_dailymail` data. Thank you so much for taking a look!","Hi  , As per the gist you attached i understand that you need to improve the performance of your model right ?. For such issues we have dedicated TF Forum where you can post the issues and ask for suggestions from larger community.Iam confident you may get some help there. Thankyou!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
716,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ModuleNotFoundError: No module named '_pywrap_tensorflow'   Failed to load the native TensorFlow runtime.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cool-man-vk,ModuleNotFoundError: No module named '_pywrap_tensorflow'   Failed to load the native TensorFlow runtime.,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-13T07:28:31Z,type:build/install TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58547,Hi manvk ! Python 3.11 has not been added as supported version yet and in the road map for future releases. Ref. Could you share the reproducible command  and OS details too to replicate this issue. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,My python version is 3.11.0 and I migrated to tensorflow 1.0.0 as I was facing issues in the latest version. And this is the details about the tensorflow which I'm using. !image,Hi manvk ! Would it be possible to share the commands? Can i go with below command.   Issue is coming from   > _pywrap_tensorflow = swig_import_helper() Thank you!,"Yes . `pip install tensorflow==1.0.0` > This is the command which I've tried. And installed the tf succesfully. > But when I use in python , > it arises this error.","Hey manvk ! 1.x version are not supported any more.  Please go through tested configs from 2.x documentation (2.10 , Python 3.73.10). For 1.0.0 , Max supported python version is Python 3.6 . Feel free to open if you are facing issue in 2.x versions.  Thank you!",Are you satisfied with the resolution of your issue? Yes No
808,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(interactive_graphviz reports error ""Unable to render graph as URL: FAILED_PRECONDITION: Can't render as URL; no URL renderer was registered"")ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.10  Custom Code No  OS Platform and Distribution Mac OS with M1   Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  The generated html file is actually empty. shell interactive_graphviz hlotext ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shawnwang18,"interactive_graphviz reports error ""Unable to render graph as URL: FAILED_PRECONDITION: Can't render as URL; no URL renderer was registered""",Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.10  Custom Code No  OS Platform and Distribution Mac OS with M1   Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  The generated html file is actually empty. shell interactive_graphviz hlotext ```  Relevant log output _No response_,2022-11-11T14:58:34Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.10,closed,0,8,https://github.com/tensorflow/tensorflow/issues/58538,Isn't `hlo_text`? https://github.com/tensorflow/tensorflow/blob/3e3f1ba6dbefcb7643a22409e242b0f0e32e71c5/tensorflow/compiler/xla/tools/interactive_graphviz.ccL106,Hi  ! Could you pass the path binary_proto in hlo_txt as suggested in above comment   and let us know the results. `interactive_graphviz hlo_text= ` Thank you!," Is it `hlo_txt `, `hlotext=` or `hlo_text=`?"," ! My mistake, It is `hlo_txt= ` . Thanks for correcting me again. But I could not find proper documentation on above interactive_graphviz command.  ! Attached relevant documentation on graphviz usage  for reference. 1, 2 (for graphviz usage visualization of Tensorflow operations in graph mode). Could you confirm whether that above documentation and comments will address this issue. Thank you!",> But I could not find proper documentation on above interactive_graphviz command. Yes no doc currently. It is only in the usage and in the source code.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
660,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to build Tensorflow 2.12.0 Open CL delegate for aarch64 platform)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.12.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Khadas Vim 3  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 10.3.1  CUDA/cuDNN version _No response_  GPU model and memory Mali G52  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,csn1800,Unable to build Tensorflow 2.12.0 Open CL delegate for aarch64 platform,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.12.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Khadas Vim 3  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 10.3.1  CUDA/cuDNN version _No response_  GPU model and memory Mali G52  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-09T17:40:50Z,type:build/install comp:lite subtype: ubuntu/linux TFLiteGpuDelegate,closed,0,9,https://github.com/tensorflow/tensorflow/issues/58497,"The issue was was happening due to Tensorflow build referring to local CL includes which are at a different version. It got resolved with a workaround of renaming the /usr/include CL directory temporarily. There was no issue in compiling with the old compiler version (GCC 8.3.0) which comes with Tensorflow as well.  I do not think the build should refer to the host /usr/includes in prior to what is present in the build directories (_opencl_headers_ is present in the build). Hence, keeping this issue open!",Hi  ! Sorry for the late response.  Yeah! I could replicate this issue in master branch. Attached gist for reference. Currently GPU delegates are enabled only for Android in Bazel Builds. Could you refer the  CMake arm  documentation for use case and let us know. Thank you!," I could build tensorflow lite static library following instructions from **Build for AArch64 (ARM64)** under CMake arm link you shared, still, had to use following flags additionally in cmake:  flatc issues I observed seem similar to Exec format error when cross compiling(v2.0.5 and v2.0.6). I had seen flatc building with target's compiler (it must be executable for host I assume) which causes errors in certain environment. Still, with the above two flags, I could get libtensorflowlite.a generated. Further, changed GIT_TAG for flatbuffers to v2.0.8 and it worked fine for FLATBUFFERS_BUILD_FLATC=ON"," ! If your device support Open CL support, you can enable GPU delegate by adding one more  flag. `DTFLITE_ENABLE_GPU=ON ` Reference.  Thank you!"," I had tried that too and it seems to work fine (could find GPU related symbols in the lib, though I have not  tried with any app). I could generate shared libraries with BUILD_SHARED_LIBS=ON and CL_DELEGATE_NO_GL seems to be enabled by default. So, all looked fine. Only minor issue is that the .so names  whether it is CPU, GPU (CL/GL)  comes out to be the same which can be confusing. Thank you.",! Thanks for quick update. you can check whether the build contains GPU delegate or not using grep command. `nm C libtensorflowlite.a | grep gpu ` Reference  Marking this as resolved from above comment . Feel free to reopen if it still persists. Thank you!,Are you satisfied with the resolution of your issue? Yes No,"> Currently GPU delegates are enabled only for Android in Bazel Builds.  , are you implying that gpu delegate will not work with gradle build? It will only work with bazel builds?  Is this arch specific or does it also apply to arm (qualcomm architecture)", ! You can use CMake documentation for Arm specific builds.  Thank you!
1288,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add big-endian support to TFLite FlatBuffers)ï¼Œ å†…å®¹æ˜¯ (This PR aims to achieve a holistic/generic solution for TFLite FlatBuffers endianness issue on s390x(bigendian machines). It follows the below guidelines on BE machines: 1. Provide the endianness conversion feature for constant buffers in a TFLite model, both in C++ and Python code. 2. Convert the buffers from LE(littleendian) to BE(bigendian) format when loading a TFLite model from a file. 3. Convert the buffers from BE to LE format when writing a serialized string of TFLite model to a file. 4. Keep the buffers in BE format when the model/buffer is in memory. This PR won't cause regression on LE machines. After applying this patch, LE format serialized TFLite model files (.tflite or .bin) could be used across platforms with different endianness format. The following test case failures could be fixed by this code change on BE machines:  This PR will also fix issue CC(TF Lite issue when loading a saved TF Lite model on platforms with different endianness) and PR CC(Fix endianness issue in unquantized quant_model on s390x). Signedoffby: KunLu )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kun-lu20,Add big-endian support to TFLite FlatBuffers,"This PR aims to achieve a holistic/generic solution for TFLite FlatBuffers endianness issue on s390x(bigendian machines). It follows the below guidelines on BE machines: 1. Provide the endianness conversion feature for constant buffers in a TFLite model, both in C++ and Python code. 2. Convert the buffers from LE(littleendian) to BE(bigendian) format when loading a TFLite model from a file. 3. Convert the buffers from BE to LE format when writing a serialized string of TFLite model to a file. 4. Keep the buffers in BE format when the model/buffer is in memory. This PR won't cause regression on LE machines. After applying this patch, LE format serialized TFLite model files (.tflite or .bin) could be used across platforms with different endianness format. The following test case failures could be fixed by this code change on BE machines:  This PR will also fix issue CC(TF Lite issue when loading a saved TF Lite model on platforms with different endianness) and PR CC(Fix endianness issue in unquantized quant_model on s390x). Signedoffby: KunLu ",2022-11-09T14:31:13Z,awaiting review comp:lite ready to pull size:L,closed,0,17,https://github.com/tensorflow/tensorflow/issues/58494,"Hi , Hope all is well. Since this PR will fix the TFLite FlatBuffers endianness issue discussed in https://github.com/tensorflow/tensorflow/issues/45009, could you please review it when you have some time? Thank you very much!","Hi  , Could you please review this PR when you have some time? Thank you very much!","Hi  , Thanks for your feedback! I've replied to your comments, please take a look when you have some time.","Hi  , Could you please review this PR again? Thank you very much!","Hi  , I've solved the conflicts from master branch. Could you please do the review again when you have some time?  Thank you!","Hi  , Thanks again for your valuable comments! I've updated the PR as per your suggestions, please review it again when you have some time.",Hi  Can you please review this PR ? Thank you!,"Hi  , Happy New Year! I've solved the conflicts from the master branch. Could you please review this PR again? Thank you very much!","Hi  , I've solved the conflicts from the master branch. Could you please take a look at this PR? Thank you very much!","Hi  , Thanks for your feedback! I've run the formatter and updated all the related code.  Please take a look when you have some time. Thanks again!","Hi  , Hope all is well. Could you please review this PR again? Thank you very much!","Hi  , Could you please have a look at the code changes when you have some time? Thank you!","Sure, will do. Thanks  !",Thanks  ,"Hi  , I've fixed the conflicts from the master branch. Please take a look when you have some time.  BTW, I found that the latest result from `feedback/copybara` showed the internal checks failed on `20230216`. Are there any code/BUILD files I need to change?  Thanks!","Hi  , I saw all the checks have passed. Wondering if it's ok for this PR to be merged? Thank you very much!","Hi  , Thanks for pointing out the above issues. I've addressed them and please take a look when you have a chance. Thanks again!"
617,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(File system scheme 's3' not implemented)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Windows11  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  I find this method is the cause of the problem    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,greper,File system scheme 's3' not implemented,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Windows11  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  I find this method is the cause of the problem    ,2022-11-09T08:47:15Z,stat:awaiting response type:bug comp:ops TF 2.10,closed,1,16,https://github.com/tensorflow/tensorflow/issues/58488,"`tf.io.gfile.exists(""s3://minst/log/xxxxxx"")` https://github.com/tensorflow/tensorflow/blob/359c3cdfc5fabac82b3c70b3b6de2b0a8c16874f/tensorflow/python/lib/io/file_io.pyL270L272 `os.system(""tensorboard logdir=s3://mnist/log port=6123"")` What TB version are you using? Check https://github.com/tensorflow/tensorboard/pull/5491 as it is not too old.",tensorboard is the latest,"Looks like it should be a problem with tensorboard, it shouldn't use `tf.io.gfile.exists`, I'll go to the tensorboar to report this problem. thanks . ",I am asking about the version cause it seems similar to https://github.com/tensorflow/tensorboard/issues/5480,My tensorboard version is 2.10.1 I see that the tensorboard source code is actually using `tf.io.gfile.exists` https://github.com/tensorflow/tensorboard/blob/53e6d9ba3c2b1fb749725acb02e154d5387d9217/tensorboard/backend/event_processing/io_wrapper.pyL195,Do you have also an updated tensorflowio version installed in the venv?,"tensorflow_io==0.27.0 , The problem is still there","My s3 backend is minio, I don't know if it matters",I remember we had a thread on the forum: https://discuss.tensorflow.org/t/writingtensorboardlogstos3bucketusingkerascallbacks/9960,"Same issue over here.  It seems to me that S3 tensorboard support has never worked in recent history, courtesy of the issue  describes.",If it is a TB issue I think you could post your case at https://github.com/tensorflow/tensorboard/issues/4255,"If 'tf.io.gfile.exists' does not support s3, then what interface can determine whether s3 files and directories exist","> If 'tf.io.gfile.exists' does not support s3, then what interface can determine whether s3 files and directories exist S3 is implemented in the IO repo so it is better that you ask there or on the forum","I have submitted an issue to io , thank you for your reply", Thank you for the update! Could you move this issue to closed status as it will be addressed in the IO repo. Thank you!,Are you satisfied with the resolution of your issue? Yes No
937,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Visualizing class activation maps with tflite weights)ï¼Œ å†…å®¹æ˜¯ (I am able to visualize GradCAM using keras weights(H5). I am not able to find to visualize last CNN layer using tflite weights.  https://pyimagesearch.com/2020/03/09/gradcamvisualizeclassactivationmapswithkerastensorflowanddeeplearning/  https://medium.com/analyticsvidhya/visualizingactivationheatmapsusingtensorflow5bdba018f759 Found a related link on TF discussion forum mentioning that this is not possible as of 2021.  https://discuss.tensorflow.org/t/applyinggradcamtoamodelthatwasconvertedtotensorflowlite/3810/7 Can you please provide a sample code to execute this OR guide me with a suitable documentation.  To model.summary() equivalent for tflite  Code to visualize last CNN feature map)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ra9hur,Visualizing class activation maps with tflite weights,I am able to visualize GradCAM using keras weights(H5). I am not able to find to visualize last CNN layer using tflite weights.  https://pyimagesearch.com/2020/03/09/gradcamvisualizeclassactivationmapswithkerastensorflowanddeeplearning/  https://medium.com/analyticsvidhya/visualizingactivationheatmapsusingtensorflow5bdba018f759 Found a related link on TF discussion forum mentioning that this is not possible as of 2021.  https://discuss.tensorflow.org/t/applyinggradcamtoamodelthatwasconvertedtotensorflowlite/3810/7 Can you please provide a sample code to execute this OR guide me with a suitable documentation.  To model.summary() equivalent for tflite  Code to visualize last CNN feature map,2022-11-07T07:23:14Z,type:support comp:lite TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58468,I think that you could try to post directly in that forum thread.,"Hi  ! You can see the model details of lite model using netron web app. To print all tensor details, you need to invoke the interpreter with respective TFlite model and get the tensor details by index of respective tensor. On GradCam Visualization perspective, You can use .function and tape gradient according to  On device training documentation as mentioned in this thread.  Please post on the same TF Forum  thread if you are looking for more insights on the implementation. Thank you!","  Thanks for the suggestion. Haven't got a chance to try it out, will revert for any help. Thanks again !!", ! Could we consider this as resolved then. Thank you!,"Sure, thanks again !!", ! Moving this to resolved status then. Thank you!,Are you satisfied with the resolution of your issue? Yes No
642,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.queue.FIFOQueue not savable by tf.saved_model.save in tf2.10)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution windows 10  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 8.2.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,timedcy,tf.queue.FIFOQueue not savable by tf.saved_model.save in tf2.10,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution windows 10  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 8.2.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-07T05:47:39Z,stat:awaiting response type:support stale comp:keras TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58466,Have you checked https://github.com/kerasteam/keras/issues/16015 ?,Hi ! Please post this issue on kerasteam/Keras repo for further assistance. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 .  Thank you!!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Serialization does not seem to be supported for tf.queue.FIFOqueue. For more details and in depth explanation, please look at the following comment. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
773,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.constant(1)` maxes VRAM)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit 11.3.1  GPU model and memory GTX 1070, 8GB  Current Behaviour? 7.5GB VRAM used until kernel is restarted. Removing `import torch` fixes this. `torch` installed with `conda`, `tensorflowgpu` with `pip`.    conda list    Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,OverLordGoldDragon,`tf.constant(1)` maxes VRAM,"  Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit 11.3.1  GPU model and memory GTX 1070, 8GB  Current Behaviour? 7.5GB VRAM used until kernel is restarted. Removing `import torch` fixes this. `torch` installed with `conda`, `tensorflowgpu` with `pip`.    conda list    Standalone code to reproduce the issue   Relevant log output _No response_",2022-11-06T22:54:51Z,stat:awaiting response type:bug comp:gpu comp:core TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58463,"After  below suffices to reproduce the issue  New list:  conda list   I see TF wants CUDA 11.2, but torch wants 11.3, either way this shouldn't happen.  Not reproduced with `tensorflowcpu`.",Hi  ! I think you should not mix Pytorch and Tensorflow in one environment as those two have different architecture on memory management. Could you create a fresh environment and follow the instructions from this document (11.2 and 8.1 is tested config for 2.10 version) and please specify the cuda config while installing cuda through Conda channel. `conda install c condaforge cudatoolkit=11.2 cudnn=8.1.0  ` Thank you!,Libraries supporting both as backends need this to avoid making a separate testing environment.,Are you satisfied with the resolution of your issue? Yes No, ! Thanks for the feedback .
1855,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF 2.10 fails to build gpu support - inference crashes)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Debian 11  Mobile device _No response_  Python version 3.8  Bazel version 5.3.0  GCC/Compiler version 9.3.0  CUDA/cuDNN version 11.2,8.1  GPU model and memory RTX 3060/12G  Current Behaviour?  shell import os import pathlib import matplotlib import matplotlib.pyplot as plt import io import scipy.misc import numpy as np from six import BytesIO from PIL import Image, ImageDraw, ImageFont from six.moves.urllib.request import urlopen from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) config.gpu_options.per_process_gpu_memory_fraction = 0.75 config.gpu_options.allow_growth = True  session = InteractiveSession(config=config) InteractiveSession(config=config) import tensorflow as tf import tensorflow_hub as hub tf.get_logger().setLevel('ERROR') def load_image_into_numpy_array(path):   """"""Load an image from file into a numpy array.   Puts image into numpy array to feed into tensorflow graph.   Note that by convention we put it into a numpy array with shape   (height, width, channels), where channels=3 for RGB.   Args:     path: the file path to the image   Returns:     uint8 numpy array with shape (img_height, img_width, 3)   """"""   image = None   if(path.startswith('http')):     response = urlopen(path)     image_data = response.read()     image_data = BytesIO(image_data)     image = Image.open(image_data)   else:     image_data = tf.io.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sniafas,TF 2.10 fails to build gpu support - inference crashes,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Debian 11  Mobile device _No response_  Python version 3.8  Bazel version 5.3.0  GCC/Compiler version 9.3.0  CUDA/cuDNN version 11.2,8.1  GPU model and memory RTX 3060/12G  Current Behaviour?  shell import os import pathlib import matplotlib import matplotlib.pyplot as plt import io import scipy.misc import numpy as np from six import BytesIO from PIL import Image, ImageDraw, ImageFont from six.moves.urllib.request import urlopen from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) config.gpu_options.per_process_gpu_memory_fraction = 0.75 config.gpu_options.allow_growth = True  session = InteractiveSession(config=config) InteractiveSession(config=config) import tensorflow as tf import tensorflow_hub as hub tf.get_logger().setLevel('ERROR') def load_image_into_numpy_array(path):   """"""Load an image from file into a numpy array.   Puts image into numpy array to feed into tensorflow graph.   Note that by convention we put it into a numpy array with shape   (height, width, channels), where channels=3 for RGB.   Args:     path: the file path to the image   Returns:     uint8 numpy array with shape (img_height, img_width, 3)   """"""   image = None   if(path.startswith('http')):     response = urlopen(path)     image_data = response.read()     image_data = BytesIO(image_data)     image = Image.open(image_data)   else:     image_data = tf.io.",2022-11-06T22:24:51Z,type:build/install comp:gpu TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58461,", I see that you are using cuDNN version 8100 and that could be the reason for your issue. Please have a look at here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.ccL5309). Could you please try to upgrade cuDNN version > 8205 in your system and let us know if you are facing a similar issue.  Thank you!","  I have already tried with higher versions, it shouldn't be a problem since 8.1 is the version that is suggested in the guide After all, the assertion here checks handles the discrepancies. I'll give it a try, the one that concerns me more, is the compute capability map, which goes until 7.0 ","Hi  , As i observed you are using bazel 5.3.0,which is not in tested versions of TF.Please refer the attached compatibilities here.There might be compatibility issues with the newer versions.I request you to please use tested Bazel versions with its compatible CUDA,cuDNN and let us know if problem still persists. There are couple of similar issues 58428 and 56456 ofcourse those are not bazel builds.Please check whether if they are useful. Thankyou!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi , of course because it asks for 5.3.0 ",This hopefully should have been fixed https://github.com/tensorflow/docs/commit/67e8be9afda4baffadc821a0c99ca423e3e5e415 Thanks   Feel free to reopen if any updates are needed.,Are you satisfied with the resolution of your issue? Yes No
703,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 1.19.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Theresaliu,ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`.,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 1.19.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-06T09:30:46Z,type:bug comp:keras,closed,0,1,https://github.com/tensorflow/tensorflow/issues/58460,Are you satisfied with the resolution of your issue? Yes No
695,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting Troubles when installing tf.2.10_gpu with anaconda virtual env.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 8.4.1.50  GPU model and memory NVIDIA GeForce RTX 2080 Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MuellerJY,Getting Troubles when installing tf.2.10_gpu with anaconda virtual env.,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 8.4.1.50  GPU model and memory NVIDIA GeForce RTX 2080 Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-05T10:44:58Z,stat:awaiting response type:build/install stale subtype:windows TF 2.10,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58459,Test this (from the official doc) ,> Test this (from the official doc) >  >  Thank you for your reply! It that means that the version of cudatoolkit and cudnn mismatch with tensorflow? ,What do you mean?,"Hi  , Please confirm whether you have also installed NVIDIA GPU driver ? Please refer the documentation attached here. Also for tested configurations of CUDA,cuDNN please refer documentation here for Build/install. Thankyou !","> What do you mean? Sorry to interrupt you again. I've met a new bug :( When using: `conda install c condaforge cudatoolkit=11.2 cudnn=8.1.0` `python3 m pip install tensorflow` GPU is now workable during model training, but here remains: ` Relying on driver to perform ptx compilation.` ` Modify $PATH to customize ptxas location.` Presenting that the model is trained by CPU on first epoch and gradually turns to GPU.  Unfortunately, no ptxas.exe can be found in my activated env. under anaconda fold. I found that it is recommended by using: `conda install c nvidia cudanvcc ` But it still not works. 1. Should specific version of cudanvcc be assigned when using `conda install c nvidia cudanvcc `? How to determine the version of cudanvcc to be installed  2. Is this problem due to the fact that I used a conda for cudnn and cudatoolkit installation? Thank you so much.","> Hi  , >  > Please confirm whether you have also installed NVIDIA GPU driver ? Please refer the documentation attached here. Also for tested configurations of CUDA,cuDNN please refer documentation here for Build/install. >  > Thankyou ! Thank you. Please take a look at my new problem at your convenient.",Can you check with `conda install c condaforge cudatoolkitdev`?,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
939,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Download of fresh release of clang fails on TF 2.10.0-2.11.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0rc2  Custom Code No  OS Platform and Distribution MacOS 13.0  Mobile device _No response_  Python version 3.10.6  Bazel version 5.3.0  GCC/Compiler version XCode 14.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  Compilation fails immediately:  The issue has been present since 2.5.0 at least, and it is present today in TF 2.10.0 and TF 2.11.0. shell When running `./configure`, select `y` when asked: Do you wish to download a fresh release of clang? (Experimental) [y/N]: y Clang will be downloaded and used to compile tensorflow.   Relevant log output  ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,feranick,Download of fresh release of clang fails on TF 2.10.0-2.11.0,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0rc2  Custom Code No  OS Platform and Distribution MacOS 13.0  Mobile device _No response_  Python version 3.10.6  Bazel version 5.3.0  GCC/Compiler version XCode 14.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  Compilation fails immediately:  The issue has been present since 2.5.0 at least, and it is present today in TF 2.10.0 and TF 2.11.0. shell When running `./configure`, select `y` when asked: Do you wish to download a fresh release of clang? (Experimental) [y/N]: y Clang will be downloaded and used to compile tensorflow.   Relevant log output  ``` ",2022-11-04T22:09:44Z,stat:awaiting tensorflower type:build/install subtype:macOS subtype:bazel TF 2.10,closed,0,16,https://github.com/tensorflow/tensorflow/issues/58453,"This is a similar issue, but it's now closed due to lack of activity. Since I can't reopen it, I submit it with updated info as this issue.  https://github.com/tensorflow/tensorflow/issues/46976","Yes, It seems it was removed with e70d095050e1d4858b2fcf0a5b55c31a2d631387","Hi , Please find the attached documentation link specifying the compatibilities of MacOS CPU with Bazel and Clang from xcode. Could you please cross check the installation with these configurations and let us know if problem still persists.","Sorry, I don't see any attachment to the original email and in GitHub. "," , Sorry for missing the attachment link. Please find the Documentation link here. Thankyou!","Reading from the documentation, XCode 10.4 is ancient (before 2020) and not sure why it is listed. I don't know what 10.14 refers to, it certainly makes little sense in relation to the actual versions of XCode available today.  Anyway, Compilation works for TF 2.10.0 Bazel 5.1.1 and Clang from XCode 13.x (the one supported in MacOS 12.x). Compilation fails with XCode 14.1, the one supporting MacOS Ventura. This means that compilation when using MacOS Ventura is not possible. The same is true for TF 2.11.0rcx and Bazel 5.3.0. ","The point of this issue, however, is not what works and what not based on a specific configuration. Rather that there is no way to download a viable compiler, as the `./configure` tool clearly intends to do.  If I run compilation, I get this error (as stated above): `ERROR: Config value 'download_clang' is not defined in any .rc file` In essence, regardless of what works or not, even defining what `download_clang` should do is missing. ", Can you make a quick test reintroducing these lines https://github.com/tensorflow/tensorflow/commit/e70d095050e1d4858b2fcf0a5b55c31a2d631387diff544556920c45b42cbfe40159b082ce8af6bd929e492d076769226265f215832fL69L77 ?,I am running a compilation test using TF 2.11.0rc2 codebase with the readded lines. Will keep you posted.,"Compilation fails just the same (i.e. with the same error). It seem that despite the added lines, bazel still uses the installed version of clang. Log attached. fail.txt",I see that `ERROR: Config value 'download_clang' is not defined in any .rc file` disappeared now but still not using the new one.,"Yes, that is correct.",Your env varriable is correctly set to pass the condition https://github.com/tensorflow/tensorflow/blob/1d34295f45965c7090039773dea44c25ac00ec00/third_party/clang_toolchain/cc_configure_clang.bzlL9L19 But I suppose that as we are not testing this in the CI the download part is quite unmaintained.  Last commit and clang updated hash > Sept. 2019 https://github.com/tensorflow/tensorflow/blob/master/third_party/clang_toolchain/download_clang.bzl,/ ,"Since the option of downloading a new version of clang (experimental) is no longer present, this can be closed.",Are you satisfied with the resolution of your issue? Yes No
1867,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to read .csv file through mapping function of tf.data.dataset?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Ubuntu 18.04 & Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 1051, in train_function  *         return step_function(self, iterator)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 1040, in step_function  **         outputs = model.distribute_strategy.run(run_step, args=(data,))     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 1030, in run_step  **         outputs = model.train_step(data)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 894, in train_step         return self.compute_metrics(x, y, y_pred, sample_weight)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 987, in compute_metrics         self.compiled_metrics.update_state(y, y_pred, sample_weight)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\compile_utils.py"", line 480, in update_state         self.build(y_pred, y_true)     File ""C:\Users\dasmehdix\A)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dasmehdix,How to read .csv file through mapping function of tf.data.dataset?,"Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Ubuntu 18.04 & Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 1051, in train_function  *         return step_function(self, iterator)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 1040, in step_function  **         outputs = model.distribute_strategy.run(run_step, args=(data,))     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 1030, in run_step  **         outputs = model.train_step(data)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 894, in train_step         return self.compute_metrics(x, y, y_pred, sample_weight)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\training.py"", line 987, in compute_metrics         self.compiled_metrics.update_state(y, y_pred, sample_weight)     File ""C:\Users\dasmehdix\AppData\Local\Programs\Python\Python39\lib\sitepackages\keras\engine\compile_utils.py"", line 480, in update_state         self.build(y_pred, y_true)     File ""C:\Users\dasmehdix\A",2022-11-04T12:32:33Z,stat:awaiting response type:support comp:data TF 2.10,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58448,Have you tried to map a function with `tf.io.decode_csv()` as in the example/doc at: https://www.tensorflow.org/tutorials/load_data/csvtfdataexperimentalcsvdataset,"> Have you tried to map a function with `tf.io.decode_csv()` as in the example/doc at: >  > https://www.tensorflow.org/tutorials/load_data/csvtfdataexperimentalcsvdataset This method is not appropriate for my case. Also, I tried to read with Pandas. Same problem on Pandas too.",I think you could share a colab  +  a very small sample of your input data to reproduce your case.,", Code shared is full of indentation errors, Kindly share a colab gist with issue reported or indented code with all dependencies such that we can replicate the issue reported. Thank you!",I have created a sample dataset and code block on Kaggle instances for this issue. You can check dataset and code here.   ,", I tried to download the dataset and code from the mentioned Kaggle link and was facing a different issue while execution. Kindly find the gist of it here. Thank you! ",> I tried to download the dataset and code from the mentioned Kaggle link and was facing a different issue while execution. Cause you missed to copy the first cell from the user's Kaggle Colab: `!pip install natsort`," The problem solved by adding `presence.set_shape([1,1,1,44])` to the mapping function. After usage of tf.py_function or tf.numpy_function you have to set the shape  to work correctly.",Are you satisfied with the resolution of your issue? Yes No,"Yes, you could evaluate also to use `from_generator` if you are manually preparing you data: https://github.com/tensorflow/tensorflow/issues/57485issuecomment1251185536"
666,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.raw_ops.FakeQuantWithMinMaxArgs` loses extra precision with certain inputs)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution Colab  Mobile device _No response_  Python version 3.7.15  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2.152, 8.1.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,creakseek,`tf.raw_ops.FakeQuantWithMinMaxArgs` loses extra precision with certain inputs,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution Colab  Mobile device _No response_  Python version 3.7.15  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2.152, 8.1.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-11-04T11:45:48Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58447,", I was able to reproduce the issue on tensorflow v2.9, v2.10 and nightly. Kindly find the gist of it here.","After some deep digging, you're getting different answers because of the use of compilergenerated fusedmultiplyadds (FMA).  It's a compiler optimization that actually makes the GPU kernel _slightly_ more accurate.  The GPU kernel is built with FMA enabled by default (generally, not specific to TF):  `fmad=true for nvcc` docs  `ffpcontract=fast` for clang docs The CPU kernel is not built with FMA by default, though you can enable it by specifically enabling fma via compile flag `mfma`. If you explicitly enable FMA in both cases you get the same values (the GPU values you report  mostly  except potentially for some tailend values, depending on how the compiler decides to fuse some operations).  If you explicitly _disable_ FMA in both cases, you get the same values again (but this time the CPU values you report). Welcome to the wonderful world of floatingpoint arithmetic and compiler optimizations.",Are you satisfied with the resolution of your issue? Yes No
617,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unit test nn_ops:pooling_ops_3d_test_cpu broken with oneDNN enabled)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.3.0  GCC/Compiler version 9.3.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,elfringham,Unit test nn_ops:pooling_ops_3d_test_cpu broken with oneDNN enabled,Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.3.0  GCC/Compiler version 9.3.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-04T10:54:08Z,type:bug type:build/install subtype: ubuntu/linux awaiting PR merge,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58446,  ,Introduced by https://github.com/tensorflow/tensorflow/commit/41affcdc1919df3e29684a946416192bfbc36eee,"hi, there is an internal ticket that is currently being worked on to fix this bug. ", PR for a fix has been submitted. https://github.com/tensorflow/tensorflow/pull/58525,PR merged,Are you satisfied with the resolution of your issue? Yes No
639,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFBertMainLayer cannot be loaded from .h5)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,prsatyal,TFBertMainLayer cannot be loaded from .h5,Click to expand!    Issue Type Others  Source source  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-04T04:53:38Z,stat:awaiting response type:others comp:keras TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58444,"Hi  ! Can you add the custom object "" TFDistilBertModel "" while loading the model and see if it works. `loaded_model = tf.keras.models.load_model('bert_model.h5',custom_objects={'TFDistilBertModel':TFDistilBertModel}) ` Reference. Please post this issue on kerasteam/keras repo for further assistance. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thank you!","Thank you for your help! The model loaded perfectly but is not performing as well as it did when i didnt load the model. It also gave me this error and im getting mixed answers all over the internet. How do i resolve this error and what does it mean? WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.", ! It might be getting resolved in 2.10 and nightly version then. Ref. Could you post on Keras Repo and close this thread here. Thank you!
620,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TIMEOUT while running the Bazel Tests on windows)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10  Bazel version 5.1  GCC/Compiler version MSVC  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,mraunak,TIMEOUT while running the Bazel Tests on windows,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.11  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10  Bazel version 5.1  GCC/Compiler version MSVC  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-03T23:41:44Z,stat:awaiting response type:bug type:build/install stale subtype:windows,closed,0,11,https://github.com/tensorflow/tensorflow/issues/58442,Do you have access to the specific log?  E.g.   `C:/67cictjz/execroot/org_tensorflow/bazelout/x64_windowsopt/testlogs/tensorflow/examples/custom_ops_doc/multiplex_3/multiplex_3_test/test.log`,  Looks like tests in `//tensorflow/examples/custom_ops_doc/...` and `//tensorflow/examples/...` are timing out on Windows. Can you take a look? Thanks!,"The bug report links to this CI run: https://tensorflowci.intel.com/job/tfwintestall/21/artifact/test_run.log The logs include strange conflicting output about multiplex tests and a few others. The logs look like the runs are passing, but the summary says they're timing out:  But it also says:  So  I'm kindof lost.    +1 to  's comment, is it possible to get one of these logs?  > C:/67cictjz/execroot/org_tensorflow/bazelout/x64_windowsopt/testlogs/tensorflow/examples/custom_ops_doc/multiplex_3/multiplex_3_test/test.log","Hey  and , I have attached the logs, but I didn't find much information in them. I rerun it but got the same output log. Below is the log when there is a TIMEOUT error error_log.docx.","Thanks . Yeah, I guess we were all hoping there would be more details.  adjusted the test size, to see if it somehow just needs more time, but I'l not holding my breath on that one given the strange logs: `Ran 6 tests in 0.373s ...  TIMEOUT in 3 out of 3 in 300.9s`",">  adjusted the test size, to see if it somehow just needs more time, but I'l not holding my breath on that one given the strange logs: `Ran 6 tests in 0.373s ... TIMEOUT in 3 out of 3 in 300.9s` My change was for consistency for one of the tests. (One of my general philosophies for debugging is that it is better to fix something that is wrong than to assume it's irrelevant.) It did not necessarily increase a timeout. I would be surprised if it helped. > TIMEOUT in 3 out of 3 in 300.9s I am not sure what this 5 minute timeout is. My understanding is that these tests have a 1 minute timeout. My guess is that the timeout is in some second layer that is above the tests. (But I could just be confused about something.) One thing that seems to be in common with the failing tests but different than the majority of tests is that they use custom ops in the typical OSS way where each test loads a dynamically linked library that contains the op. I wonder if loading these libraries could be related to the problem?","Hi All, the issue of TIMEOUT is resolved now, by changing the jobs 'jobs' from the Bazel command from 64 to a lower value, I have set it as 16. ","Hi  , Glad that you have found a workaround.I think this might be also related to the memory resources and/or CPU cores. Higher value to the `jobs` argument means higher concurrent jobs. The number of concurrent jobs that Bazel will run is determined not only by the `jobs` setting, but also by Bazel's scheduler, which tries to avoid running concurrent jobs that will use up more resources (RAM or CPU) than are available, based on some (very crude) estimates of the resource consumption of each job. The behavior of the scheduler can be controlled by the `local_ram_resources` option. May be the flag `local_ram_resources` also might have some effect here. WDYT ? Shall we consider this as resolved. Please confirm. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to build Tensorflow lite OpenCL delegate for Open CL 1.2)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Khadas Vim 3  Python version 3.8  Bazel version 4.2.1  GCC/Compiler version 8.3.0  CUDA/cuDNN version NA  GPU model and memory Vivante NPU (VIPNanoQ)  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,csn1800,Unable to build Tensorflow lite OpenCL delegate for Open CL 1.2,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Khadas Vim 3  Python version 3.8  Bazel version 4.2.1  GCC/Compiler version 8.3.0  CUDA/cuDNN version NA  GPU model and memory Vivante NPU (VIPNanoQ)  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-03T19:03:35Z,stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux TFLiteGpuDelegate TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/58435,Source https://github.com/tensorflow/tensorflow/issues/58432issuecomment1302422753,Hi  ! Is this issue possible  duplicate to CC(Unable to build Tensorflow 2.12.0 Open CL delegate for aarch64 platform).   Thank you!," Contexts are different, but it can be due to similar underlying issues that refers the host CL, instead of the one in build folder.",Sure  ! Thanks for the update.  ! Could you look at this issue. Thank you!,"I am able to reproduce on branch r2.8, and I get a different error on branch r2.13: ","Hi , OpenCL support is experimental, can you try with cmake instead to see if this resolves your issue? https://www.tensorflow.org/lite/guide/build_cmakeopencl_gpu_delegate","While it's technically possible to run this on the PC with the right components, this is not a supported use case and the plumbing is on your end.  You can probably make things work with Mesa (I never had success with that) or strip out all the GLES related code.",Are you satisfied with the resolution of your issue? Yes No
673,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Worker nodes exits early in MultiWorkerMirroredStrategy, causing error task reported on chief node.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.8/8.6  GPU model and memory 320G  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,YuchengT,"Worker nodes exits early in MultiWorkerMirroredStrategy, causing error task reported on chief node.",Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.8/8.6  GPU model and memory 320G  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-03T17:12:56Z,stat:awaiting response type:bug stale comp:dist-strat TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58434,", TensorFlow v2.10 is compatible with CUDA 11.2 and cuDNN 8.1. Could you please try with the compatible build configurations check if you are facing the same error. Thank you!  ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1478,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OperatorNotAllowedInGraphError: using random seed in tf.function)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.1.74 / 8.0.5  GPU model and memory NVidia Tesla V100SXM232GB  Current Behaviour? `tf.random.set_seed()` in a tf.function doesn't seem to do anything.  Standalone code to reproduce the issue For reproducible tests it might be useful to set seeds. When using a distributed strategy, it would be nice to have a different seed for each replica, hence set the seed **inside** the tf.function. I've done tests (without a distributed strategy for sake of simplicity): This works:  This works too:  But this fails:  ... with this error:   Note that setting a seed in a tf.function doesn't seem to work:   prints different values for the two `f()` invocations Using an externally provided seed for `tf.random.normal()` also results in a `OperatorNotAllowedInGraphError`:  Consider this last attempt:  Again, this seems silly but the only purpose of using seeds **inside** a tf.function is to provide each replica with a different seed.  ```)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,thierryherrmann,OperatorNotAllowedInGraphError: using random seed in tf.function,"Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.1.74 / 8.0.5  GPU model and memory NVidia Tesla V100SXM232GB  Current Behaviour? `tf.random.set_seed()` in a tf.function doesn't seem to do anything.  Standalone code to reproduce the issue For reproducible tests it might be useful to set seeds. When using a distributed strategy, it would be nice to have a different seed for each replica, hence set the seed **inside** the tf.function. I've done tests (without a distributed strategy for sake of simplicity): This works:  This works too:  But this fails:  ... with this error:   Note that setting a seed in a tf.function doesn't seem to work:   prints different values for the two `f()` invocations Using an externally provided seed for `tf.random.normal()` also results in a `OperatorNotAllowedInGraphError`:  Consider this last attempt:  Again, this seems silly but the only purpose of using seeds **inside** a tf.function is to provide each replica with a different seed.  ```",2022-11-03T15:44:58Z,type:others comp:core TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58433, is working for me. See https://colab.research.google.com/gist/bhack/815bd1283b4de03f21702c627919047e/untitled181.ipynb  Is failing cause autograph is not going to covert correctly this condition https://github.com/tensorflow/tensorflow/blob/bf22440f8485b5af3823cdd1b32b52cbce15b487/tensorflow/python/framework/random_seed.pyL89L91 ,Basically It is like ,"Hi  ! I think I found the issue here . Actually, you are passing a non integer seed in the tf.random.set_seed which is causing the issue. If only integer value are passed in above reproducible code snippets, I can get the expected values. Attached resolved gist for reference. Ref Thank you! ","thanks a lot  for your response. Unfortunately for your first code snippet, you mention it works for you. Indeed it works but if you read carefully my initial comment, if you invoke `f()` several times (without redefining the function), it will give you different random numbers, which is not what we expect, given the `tf.random.set_seed(1)`. That's why in my snippet, I called `f()` twice after the function definition (this was not a typo). I realize I should have RTFM: tf.random.set_seed()'s doc contains much more info than I thought. For a repeatable result calling `f()` several times, there is tf.random.stateless_normal() and other `stateless_xxx()` functions. E.g. this gives the same result for the two invocations of `f()`:  I'm not sure I understand your explanation about `if seeds == (0, 0): `. This is part of the `get_seed()` function and not the `set_seed()` function. But thanks for having taken the time to answer and for the snippets.","Thanks a lot  for your response. Unfortunately I'm afraid the seed I pass is really an integer.   If you add this line:  it'll print `tf.int32` I saw that you replaced `self.seed + repl_id` with just `self.seed` to make the code work.   But that addition `+ repl_id` was intentional: if you only use `self.seed` both replicas will generate the same random numbers. By adding `repl_id` you have a different seed for each replica, but if you rerun the code, those two seeds will remain the same (self.seed + 0 for replica0 and self.seed + 1 for replica1), satisfying the reproducibility requirement. Unfortunately this ends with a `OperatorNotAllowedInGraphError` But thanks for having taken the time to answer, I appreciate it.","Thanks again ,  for your contribution. I searched more and the `stateless_xxx()` functions are the solution. This code satisfies my requirements:  Output:  So with a given seed, the results are reproducible: each replica generates different numbers but a second execution will have the replicas generate the same numbers as with the previous execution. Changing the seed changes completely the sequences as expected. The issue can be closed with the RTFM label if that exists!! Thank you both.","Ok  ! Thanks for the update. Actually, What I meant seed should be a python integer not an integer Tensorflow Tensor. Marking this as resolved from above comment. Thank you!",Are you satisfied with the resolution of your issue? Yes No,"  > What I meant seed should be a python integer not an integer Tensorflow Tensor. Actually passing the seed to the tf.function as a tf.constant rather than a python int was intentional. autograph reconstructs a new graph on each invocation with a different python int value. While with a tf.constant, it reuses the same graph. My simplified example doesn't show the input_signatures I use with the tf.function to restrict inputs and ensure only one graph is generated. This wouldn't be serious in my simple example, but in our real (big) model, graph regeneration would take seconds and consume memory. Fortunately `tf.random.stateless_xxx()` solves everything. Thanks for your feedback.","> Unfortunately for your first code snippet, you mention it works for you. Indeed it works but if you read carefully my initial comment Yes sorry is that it seems, without the output, that it was empty. But reading again the ""text"" it is clear. So the pointer to the stateless version is correct. P.s. the autograph problem mentioned for the non stateless impl at https://github.com/tensorflow/tensorflow/issues/58433issuecomment1302627410 is still valid."
750,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Confusing Problems(Not a ndarray) occur when training wGAN with tensorflow.keras API : train_on_batch)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.4.0  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Cuda compilation tools, release 11.1, V11.1.74;  GPU model and memory NVIDIA GeForce RTX 2080 Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MuellerJY,Confusing Problems(Not a ndarray) occur when training wGAN with tensorflow.keras API : train_on_batch,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.4.0  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Cuda compilation tools, release 11.1, V11.1.74;  GPU model and memory NVIDIA GeForce RTX 2080 Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-11-03T11:23:59Z,type:support comp:keras TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58429,"Hi  ! 2.4 version is outdated now. I would suggest you test in 2.9/2.10 version.  If the error persists, you can use make_ndarray to convert the tensors to nd array prior training. Please post this issue on kerasteam/keras repo for further assistance. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thank you!","> Hi  ! 2.4 version is outdated now. I would suggest you test in 2.9/2.10 version. If the error persists, you can use make_ndarray to convert the tensors to nd array prior training. >  > Please post this issue on kerasteam/keras repo for further assistance. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thank you! Thank you for your recommendation. I am about to try on TensorFlow 2.9/2.10 version.",Ok  ! Thanks for the update. Closing this issue here as it will be tracked in Keras  repo. Thank you!,Are you satisfied with the resolution of your issue? Yes No
1621,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA Version: 11.6  GPU model and memory NVIDIA GeForce RTX 2060  Current Behaviour? I want to know why tensorflow always spamms that message in the official docker container (`tensorflow/tensorflow:2.10.0gpujupyter`)? `I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero` This is probably not a problem at runtime, but it is still very annoying that the output is spammed every time. I only found CC(I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero) which is closed due to inactivity. I tried https://github.com/tensorflow/tensorflow/issues/56456issuecomment1156008981, which I think means:  but that does not help.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,alkatar21,"I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero","Click to expand!    Issue Type Others  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA Version: 11.6  GPU model and memory NVIDIA GeForce RTX 2060  Current Behaviour? I want to know why tensorflow always spamms that message in the official docker container (`tensorflow/tensorflow:2.10.0gpujupyter`)? `I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero` This is probably not a problem at runtime, but it is still very annoying that the output is spammed every time. I only found CC(I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero) which is closed due to inactivity. I tried https://github.com/tensorflow/tensorflow/issues/56456issuecomment1156008981, which I think means:  but that does not help.  Standalone code to reproduce the issue   Relevant log output  ",2022-11-03T10:50:27Z,type:others comp:gpu comp:xla awaiting PR merge,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58428,This logic now was moved here: https://github.com/tensorflow/tensorflow/blob/4432e35cb20a6cc42a14ef539cf7fdc4fefcda29/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.ccL951L1004 As you can see there is a mentioned internal ticket http://b/18228951 that is not accessible to the external users. But you could understand a bit the logic of this code with: https://stackoverflow.com/questions/44232898/memoryerrorintensorflowandsuccessfulnumanodereadfromsysfshadnegativ,Check https://github.com/tensorflow/tensorflow/pull/58430,"I'm not sure I understand why this is not set correctly on the PC the container is running on, but I understand where the problem is and the note may help a bit.","From https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 > What:		/sys/bus/pci/devices/.../numa_node Date:		Oct 2014 Contact:	Prarit Bhargava  Description: 		This file contains the NUMA node to which the PCI device is 		attached, or 1 if the node is unknown.  The initial value 		comes from an ACPI _PXM method or a similar firmware 		source.  If that is missing or incorrect, this file can be 		written to override the node.  In that case, please report 		a firmware bug to the system vendor.  Writing to this file 		taints the kernel with TAINT_FIRMWARE_WORKAROUND, which 		reduces the supportability of your system.",This is the note I meant. I just don't understand why it happens in my case that the file is not correctly filled with 0. Everything should be up to date.,As you car read there if you manually access to that sysfs on the host and it is `< 0` it could be a firmware issue. You can use the suggested workaround and report it to your PC/laptop/server vendor as it is probably a firmware bug.
1038,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deleting legacy Java client from TensorFlow main repository)ï¼Œ å†…å®¹æ˜¯ (TensorFlow main repository still contains the old Java client based on TF1.x that has been replaced a few years ago by the new version maintained by SIGJVM. This is very misleading for users who wants to discover the capabilities of running TensorFlow models on Java (just this week a new example of such question appeared on the forum). This issue is to start the process of deleting this client for good in TF main repo. We could start by replacing this README for simply saying that this client is deprecated and provide links to the new repo. Then we can proceed to the folder deletion, making sure it won't break any code, CI jobs or external scripts (like the documentation one). If need be, we at SIGJVM can take care of pushing a series of pull requests to achieve this goal.  CC\  , )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,karllessard,Deleting legacy Java client from TensorFlow main repository,"TensorFlow main repository still contains the old Java client based on TF1.x that has been replaced a few years ago by the new version maintained by SIGJVM. This is very misleading for users who wants to discover the capabilities of running TensorFlow models on Java (just this week a new example of such question appeared on the forum). This issue is to start the process of deleting this client for good in TF main repo. We could start by replacing this README for simply saying that this client is deprecated and provide links to the new repo. Then we can proceed to the folder deletion, making sure it won't break any code, CI jobs or external scripts (like the documentation one). If need be, we at SIGJVM can take care of pushing a series of pull requests to achieve this goal.  CC\  , ",2022-11-03T01:16:18Z,stat:awaiting tensorflower type:bug comp:apis,open,0,16,https://github.com/tensorflow/tensorflow/issues/58424,/ ,P.s. I suggested to open an initial Issue (this) and PR sin the last SIGJVM meeting.  The first one is at: https://github.com/tensorflow/tensorflow/pull/58427,https://github.com/tensorflow/tensorflow/pull/58427 was merged. Do we need something else?,"Getting this link to point at the new API would be useful. It's where the ""Java"" link goes to from here.", Could you please let us know if the above comments helped? Thank you!,"Sorry if I did not got back on this before. Like  said, links to the old Java API should now point to the new documentation.  Also, more PR related to this story must be pushed (and ultimately all of the old Java client in the main repo should be deleted). I was busy on something else and I'll come back to this work later, please keep the issue open  , thank you!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.","The old Java API should still be removed from this repository. We've tagged the first stable release of TFJava over at https://github.com/tensorflow/java, users should be redirected there."," Thank you! , As mentioned could you please raise the issue in the https://github.com/tensorflow/java repository for the TFJava related. Thank you!","It should be removed from **THIS** repository, not the tensorflow/java one. It doesn't make sense to raise an issue to a repository that just happens to share a name with a word from this issue"," As we said, we're happy to create PRs to remove the legacy Java client from this repository. However it's still receiving some commits and we've had no contact from the team inside Google who are doing those commits so we don't know if there are people inside Google still using it.","I think it's safe to remove, the recent commits seem to be only from automation and refactoring","Unfortunately, `tensorflow/lite/java` still depends on `tensorflow/java` so we need to wait until Lite is fully separated and then migrated to the SIG repo.",Ok. Is there some issue which tracks that process or is it internal to Google?,The repo is at https://github.com/googleaiedge/litert but it still has a warning/note that development comes from TF. I guess an issue could be opened there or we can wait until said note is removed,I opened an issue on litert  https://github.com/googleaiedge/LiteRT/issues/429.
682,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Delete the old Java client from TensorFlow main repository)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.10  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,karllessard,Delete the old Java client from TensorFlow main repository,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.10  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-11-03T01:14:02Z,type:build/install TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58423,"CC\  , ","This template is not right for this kind of demand, closing this and starting from a blank form...",Are you satisfied with the resolution of your issue? Yes No
341,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adding approach to fix the issue with tf.tile call)ï¼Œ å†…å®¹æ˜¯ (this PR is trying to solve the issue listed here : https://github.com/tensorflow/tensorflow/issues/58002)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,saloni346,Adding approach to fix the issue with tf.tile call,this PR is trying to solve the issue listed here : https://github.com/tensorflow/tensorflow/issues/58002,2022-11-02T23:17:32Z,stale size:XS comp:core,closed,0,9,https://github.com/tensorflow/tensorflow/issues/58422,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hi , this is not worthy of a complete PR. I have just added some bunch of comments describing the approach. I am new to this source code, so need your handholding for a while. Some questions: 1. Can you share some example code where we have changed the Shape from one dimension to another? 2. Which functions can help me prepend the entries to the multiples shape data structure? 3. Where do I have to write the tests for it? 4. Why the above cla/google and AMD ROCm is failing? Please, help me out whenever free. ", Can you please sign CLA. Thank you!,"> 1. Can you share some example code where we have changed the Shape from one dimension to another? > 2. Which functions can help me prepend the entries to the multiples shape data structure? For the `multiples` data, we don't really expect it to be very big (the max size is governed by the max number of dimensions we support, which is usually around 6 or 7... looks like 7 for this op by scanning below in the tile_ops.cc file for gradients).  This gets passed around as an `ArraySlice` that is created here.  If there is a size mismatch, we can probably just copy this to an `std::vector`, and prepend ones to that. For the input tensor, you will need to use the Eigen tensor `.reshape( )` function.  That accepts an `std::array` (or `Eigen::DSizes` or similar) of new sizes.  Since we need to have a fixed known number of dimensions, you'll need to break it up into cases of output sizes 0  7.  There's an example in `tile_ops_impl.h` that uses `.reshape()`. > 3. Where do I have to write the tests for it? Most tests go through python testing.  Looks like we don't have great testing for `tf.tile` currently. The python tests are generally here for array ops, unless there are a sufficient number of tests for the op to warrant breaking it out into its own file. > 4. Why the above cla/google and AMD ROCm is failing? You should be able to click on the ""Details"" to find out, but it looks like an unrelated configuration error > Package 'pythonpip' has no installation candidate ROCm probably updated their docker images and we haven't caught up.  It's not a blocking failure, so I wouldn't worry about it atm.","Hi, sorry for the delayed response as I was engaged with my academic work.  1. can you tell me the steps or point me to the right documentation from where I can know how to compile my changes in my local system? 2. I have tried to update my signature on the CLA form but it still showing failed. please let me know where might be the issue?",> * can you tell me the steps or point me to the right documentation from where I can know how to compile my changes in my local system? https://www.tensorflow.org/install/source > * I have tried to update my signature on the CLA form but it still showing failed. please let me know where might be the issue? https://github.com/tensorflow/tensorflow/pull/58422/checks?check_run_id=9261642567, Can you please check 's comments and keep us posted ? Thank you!, Any update on this PR? Please. Thank you!,"Let's close this for now.  It looks like there has been no progress, and the current change isn't yet meaningful. Please reopen when you have something working."
1395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Converting model to frozen pb causes original model to go into an ""Invalid State"")ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.4  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I currently am trying to convert a Tensorflow 2 Keras model into a Tensorflow 1 frozen pb. My code is able to accomplish this and freezes the model correctly. I do this by creating my model, then saving it as an h5, then load that models h5 as a separate model and freeze it. However, if I try to load and freeze the model and then continue on to use the original model (the untouched one), it's put into an ""Invalid State"". I've tried looking to see if there is an issue with the keras backend session being confused or if the two models have the same reference but they are all separate.  It's as if the original model and the loaded model are the same one. I'm not sure if this is a bug or more likely user error.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,matthewfernst,"Converting model to frozen pb causes original model to go into an ""Invalid State""","Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.4  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I currently am trying to convert a Tensorflow 2 Keras model into a Tensorflow 1 frozen pb. My code is able to accomplish this and freezes the model correctly. I do this by creating my model, then saving it as an h5, then load that models h5 as a separate model and freeze it. However, if I try to load and freeze the model and then continue on to use the original model (the untouched one), it's put into an ""Invalid State"". I've tried looking to see if there is an issue with the keras backend session being confused or if the two models have the same reference but they are all separate.  It's as if the original model and the loaded model are the same one. I'm not sure if this is a bug or more likely user error.  Standalone code to reproduce the issue   Relevant log output  ",2022-11-02T01:23:46Z,stat:awaiting response type:support comp:keras TF 2.4,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58411,", I was facing a different error while executing the mentioned code. Kindly find the gist of it here and share the required dependencies and also please try to update to latest stable v2.10 & let us know if the issue still persists. Thank you!", Sorry about that! I created a gist here detailing the issue. This is with TF 2.9.2 and it's still having the same issue. Maybe I'm doing something wrong? Thank you for your help!,", Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!", Yes I can move it there. I can close this issue. Keras issue can be found here for anyone else following this thread.,Are you satisfied with the resolution of your issue? Yes No
627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to catch XlaRuntimeError in python)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf2.10  Custom Code No  OS Platform and Distribution Ubuntu  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vinayburugu,How to catch XlaRuntimeError in python,Click to expand!    Issue Type Support  Source source  Tensorflow Version tf2.10  Custom Code No  OS Platform and Distribution Ubuntu  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-01T18:13:47Z,stat:awaiting response type:support stale comp:xla TF 2.10,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58407,I don't think it is exposed currently. What is your use case? E.g. Jax it is handling it internally: https://github.com/google/jax/blob/main/jax/_src/stages.pyL191L204,Hi  ! Does documentation 1  and 2 address your requirement to catch xla run time error. Thank you!,"  Since XLA is common to both Tensorflow and Pytorch, accessing XlaRuntimeError and able to throw / catch will enable me to rethrow relevant message that the exception root cause is within XLA and not the framework.  I don't think 1 and 2 would solve my use case.",Ok  ! Thanks for the update.   ! Could you look at this issue. Thank you!,">  Since XLA is common to both Tensorflow and Pytorch, accessing XlaRuntimeError and able to throw / catch will enable me to rethrow relevant message that the exception root cause is within XLA and not the framework. I think it was exposed in Jax with https://github.com/google/jax/pull/10676. Is this really a TF related request? If not XLA has a new standalone repository at https://github.com/openxla/xla."," Though the exception is defined in xla, it seems to be up to every frontend framework to expose the exceptions as you pointed out. The request here is specific to TensorFlow's frontend.   You should file an issue with XLA to make this contract between compiler and frontend simpler. If XLA standardizes its exception definitions under one easy to import module, frontends can expose those with something like ","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. Have you got the chance to have a look at the jax.errors.JaxRuntimeError which is alias of XlaRuntimeError https://jax.readthedocs.io/en/latest/errors.htmljax.errors.JaxRuntimeError The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
623,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite ""Unrecognized GetAddress selector"")ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10.0, master  Custom Code No  OS Platform and Distribution _No response_  Mobile device iOS  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell  ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Kepnu4,"TFLite ""Unrecognized GetAddress selector""","Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10.0, master  Custom Code No  OS Platform and Distribution _No response_  Mobile device iOS  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell  ```  Relevant log output _No response_",2022-11-01T12:26:59Z,stat:awaiting response type:bug stale comp:lite TFLiteGpuDelegate TF 2.10,closed,0,21,https://github.com/tensorflow/tensorflow/issues/58402,"Hi  ! Could you confirm that you have followed CoreML delegate documentation  and used a static batch size instead of   dynamic batch size for IOS. Ref  A Quantized lite model is always recommended for GPU delegate usage.  Would it be possible to share a reproducible code, device details along a sample lite model for replicating this issue. Thank you!","Hi   I use metal delegate with static input size (i.e batch size, width, height, channels). Here is the model  model.tflite.zip", What static input size have you used? https://github.com/tensorflow/tensorflow/issues/37012issuecomment652682677,>  What static input size have you used? >  >  CC(Not able to perform tflite inferences for batch sizes beyond 1 (COCO SSD MobileNet v1)) (comment) It's `[3x256x256x5]`. I have static batch size  because channel is a sample in my case and it's always 3. If i convert with static size `[1x256x256x5]` and pass channel by channel then it works fine,"Isn't expected that the layout is `[B, C, H, W]`?",Check the layouts at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.mdsupportedops,> Check the layouts at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.mdsupportedops I don't use CoreML delegate. I use metal delegate,Looking at the Metal Convolution tests the layout is the same: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/metal/kernels/conv_test.mm,> Looking at the Metal Convolution tests the layout is the same: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/metal/kernels/conv_test.mm Where do you see it? As see `BHWC` layout:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/metal/kernels/conv_test.mmL262,"> Where do you see it? As see BHWC layout: No, I meant that looking at metal test is the same ""as your""  `[3x256x256x5]`  `[1x256x256x5]`. So `[1x256x256x5]` is working correctly and `[3x256x256x5]` is failing right?", ! Thanks for sharing your results. Could you check against [5x256x256x3] and let us know the results. Thank you!,">  ! >  > Thanks for sharing your results. Could you check against [5x256x256x3] and let us know the results. >  > Thank you! i can't shuffle channels because i will need to change network architecture  > > Where do you see it? As see BHWC layout: >  > No, I meant that looking at metal test is the same ""as your"" `[3x256x256x5]` `[1x256x256x5]`. >  > So `[1x256x256x5]` is working correctly and `[3x256x256x5]` is failing right? yes, right. i can see that it's failing at his line with node name `""winograd_4x4_to_36 15""` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/task/tensor_desc.ccL981",Your model should work after this commit https://github.com/tensorflow/tensorflow/commit/89b2c189500f9537ddd958d247cead8fc555c575,Thanks . It doesn't show error but gives wrong result with batch size > 1. I tried to add batch to test and it seems that `Winograd4x4To36Test` gives zeros for all except first batch. Algthough i'm not sure if it is correct behaviour or not https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/tasks/winograd_test_util.ccL165,"We will fix this. If you want to test your model before our patch, just change line 70 to this if (X / 4 >= args.tiles_x  Y / 4 >= args.tiles_y) return; And line 126 to this int dst_x = Y / 4 * args.tiles_x + X / 4; In file  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/tasks/winograd.cc",Should be fixed after this commit https://github.com/tensorflow/tensorflow/commit/78cbae184cb2cd9541fb21074c3420b930c682ad,> Should be fixed after this commit 78cbae1 It works correctly with this commit. Thanks  ,Hi  ! Could you confirm that the issue is fixed with commits https://github.com/tensorflow/tensorflow/commit/89b2c189500f9537ddd958d247cead8fc555c575 and https://github.com/tensorflow/tensorflow/commit/78cbae184cb2cd9541fb21074c3420b930c682ad . Feel free to close the issue if it is fixed. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
640,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fail to build TensorFlowLiteC_framework on multiple computers. )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Mac  Mobile device iOS  Python version 3.10.1  Bazel version 5.1.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,erikbylow-vol,Fail to build TensorFlowLiteC_framework on multiple computers. ,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Mac  Mobile device iOS  Python version 3.10.1  Bazel version 5.1.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-11-01T10:42:54Z,stat:awaiting response type:build/install stale comp:lite subtype:macOS TF 2.10,closed,0,12,https://github.com/tensorflow/tensorflow/issues/58398,"vol ! Thanks for reporting this issue. I could replicate this issue in 2.9, 2.10 and nightly in my Mac M1(Silicon chip).  ! Could you look at this issue. Thank you!","That's weird, I just try building the framework with the same command and it builds successfully. Mine is also M1. What's your bazel version?",Mine is 5.1.1,vol Do you have the same problem with TF 2.11.0rc2?,"Hi, sorry I just tried switching branch to v2.10.0, it also failed with the same error, but it succeeds on master. We already drop support for 32 bits since Apple drops support for it since iOS 11. To build for 64bits devices, you can run the following command: ",How will this work with simulators? My job computer is an Intel Mac.,> vol Do you have the same problem with TF 2.11.0rc2? It works on my Intel Mac on v2.11.0rc2,"Hi if you want to build for simulators, please replace `ios_multi_cpus=arm64` with `ios_multi_cpus=x86_64,sim_arm64`","Hi vol !  Could you test above suggestion and let us know. ` bazel build ios_multi_cpus=x86_64,sim_arm64   c opt config=ios cxxopt=std=c++17 //tensorflow/lite/ios:TensorFlowLiteC_framework ` Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
737,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite won't build using ccmake [vs cmake])ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version commit 50a22ec283d14a9142bbd77cac77750714500768  Custom Code No  OS Platform and Distribution Ubuntu 22.04.1 LTS  Mobile device _No response_  Python version 3.10.6 [ubuntu default]  Bazel version _No response_  GCC/Compiler version gcc (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,chunky,TFLite won't build using ccmake [vs cmake],Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version commit 50a22ec283d14a9142bbd77cac77750714500768  Custom Code No  OS Platform and Distribution Ubuntu 22.04.1 LTS  Mobile device _No response_  Python version 3.10.6 [ubuntu default]  Bazel version _No response_  GCC/Compiler version gcc (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-31T21:18:19Z,type:build/install comp:lite subtype: ubuntu/linux comp:lite-examples TF 2.10,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58392,"Hi  ! Thanks for reporting this issue. I could replicate this issue in Colab and local machine too. It is throwing ""Cmakelist.txt not found"" error in my case.  ! Could you check this issue. Attached gist for reference. Thank you!", Your gist has many errors/typo like:  and you have not replicated the user output as you have:  cause you are working on the wrong directory.  I can use `ccmake` without problems with a reproducible env: ```bash docker run rm it ubuntu /bin/bash aptget update aptget install y git cmakecursesgui g++ clangformat git clone depth 1 singlebranch branch r2.10 https://github.com/tensorflow/tensorflow.git tensorflow_src mkdir tflite_build cd tflite_build/ ccmake ../tensorflow_src/tensorflow/lite/  Please remember to configure with `c` twice and the generate with `g` make,Ok  ! Thanks for correcting me again. I just followed ccmake documentation (it was suggesting to run ccmake.. in same directory). Thank you!,"> (it was suggesting to run ccmake.. in same directory) Yes but it was not the problem, the problem is that you was in a folder where the parent you have referred `..` has no  `CMakeLists.txt` that it is exactly the output message it gives your colab. So probably you want to do something like `cmake ../..` in your gist instead.   But it is generally better to build in an external directory as suggested by our official documentation https://www.tensorflow.org/lite/guide/build_cmake so you are not going to mix build and sources.",Hi  ! Can you check again with below syntax.  Ref. Thank you!,"  Doing a build precisely like that: The first time I hit ""c"", it gets through without an error. But the second time I hit ""c"", I get:  CMakeError.log CMakeOutput.log", This is why I have asked you to try to reproduce the same with a controllable environment like docker so we are for sure exactly on the same page. If you don't want to use a clean/under control environment please check that your custom env respect the tested build configuration: https://www.tensorflow.org/install/sourcetested_build_configurations `identification is GNU 11.3.0` is not in the table.,"I confirm I can build using the reproducible build you suggest. I guess this could be closed. Really this whole thing is me trying to diagnose why a big scary C++cmake codebase [that I'm trying to build TFLite into, that I can't share] won't build on windows [but works fine on Linux]. As I work through my problems, I'm finding weirdnesses like this.",Thanks !  for the confirmation.  Marking this issue as resolved as you have confirmed that you could build with tested config in above comment. Thank you!,Are you satisfied with the resolution of your issue? Yes No
683,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Video classification notebook is showing error in output when replicated using TF 2.9)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version TF 2.9  Custom Code No  OS Platform and Distribution Google Colab, Linux  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,RenuPatelGoogle,Video classification notebook is showing error in output when replicated using TF 2.9,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version TF 2.9  Custom Code No  OS Platform and Distribution Google Colab, Linux  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-10-31T09:08:30Z,stat:awaiting response type:bug comp:keras TF 2.9,closed,0,2,https://github.com/tensorflow/tensorflow/issues/58378,", I tried to execute the mentioned code/notebook on tensorflow stable v2.10 & TFnightly version and it was executed without any issue/error. Kindly find the gist of it here and let us know if the issue still persists. Thank you!",Are you satisfied with the resolution of your issue? Yes No
1391,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to apply XNNPACK after quantization aware training)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution: Ubuntu 18.04  TensorFlow installation (pip package): 2.7.0  TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.0  2. Code The model has two inputs, where the range of first one is [0, 1.0] and then second one is [1.0, 1.0].   3. Failure after conversion We use quantizationaware training to train our models, and the deploy as `tf.int8` TFLite models.  Actually, the model is converted successful, and fully functional on Qualcomm's CPU and Hexagon. But it run into errors when using XNNPACK delegate. We test the model with tflite benchmark with xnnpack enabled, and found the following error messages:  However, XNNPACK is not always failed. It can run normally in some case: 1. If we deploy the model directly without training, it works. 2. Basically we training the model for 40k steps. if we train it shortly, e.g. only 25k, it works. 3. The backbone of the model is HRNet, If we used another architecture, e.g. Hourglass, it works. We have been haunted by this strange thing for a month, please kindly let us know what's wrong. Thank you for your time.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Janus-Shiau,Failed to apply XNNPACK after quantization aware training," 1. System information  OS Platform and Distribution: Ubuntu 18.04  TensorFlow installation (pip package): 2.7.0  TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.0  2. Code The model has two inputs, where the range of first one is [0, 1.0] and then second one is [1.0, 1.0].   3. Failure after conversion We use quantizationaware training to train our models, and the deploy as `tf.int8` TFLite models.  Actually, the model is converted successful, and fully functional on Qualcomm's CPU and Hexagon. But it run into errors when using XNNPACK delegate. We test the model with tflite benchmark with xnnpack enabled, and found the following error messages:  However, XNNPACK is not always failed. It can run normally in some case: 1. If we deploy the model directly without training, it works. 2. Basically we training the model for 40k steps. if we train it shortly, e.g. only 25k, it works. 3. The backbone of the model is HRNet, If we used another architecture, e.g. Hourglass, it works. We have been haunted by this strange thing for a month, please kindly let us know what's wrong. Thank you for your time.",2022-10-31T03:37:09Z,stat:awaiting response type:support stale comp:lite ModelOptimizationToolkit comp:lite-xnnpack TF 2.7,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58376,Hi Shiau ! Can you check with below code snippet and let us know if it works.  I would suggest you to save  .pb with tf.saved_model format and reload it using 2.x lite converter api for better results. (2.10 or nightly version please) Thank you!,"I test the `tf.saved_model` on tensorflow 2.7 and 2.10 with 2.x lite converter API.  But the error is still not solved. Our converting code is provided as follows:  We still have no idea here, thank you for your time and any comment.",Ok Shiau ! Would it be possible to share a sample saved model to replicate the issue.  Another issue might be usage of dynamic batch size (static batch size or batch size = 1) or invalid data format ( NHWC to NCHW is efficient ) in model itself. Ref %2C%20datatype%20(FP32)%20and%20microkernel%20type%20(if%20applicable)%20are%20shown.) Thank you!,"I can provide a tester model for you, which contains similar the above error:  The model download link: https://drive.google.com/file/d/19XB3XwbvfhmsvFJ_SKuDQi_QgN5L2t/view?usp=share_link",Is there any update? Thanks!,This error still happens now. We are not able to find a solution. Is there any update? Thanks!,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1831,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(High Batch Size Exceeds Memory)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.8  GPU model and memory NVIDIA GTX 1080Ti  Current Behaviour?   Standalone code to reproduce the issue  shell Epoch 1/5 20221030 14:08:24.668673: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 375.00MiB (rounded to 393216000)requested by op model/BERT_encoder/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_encoder/StatefulPartitionedCall/transformer/layer_0/self_attention/einsum_1/Einsum If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.  Current allocation summary follows. Current allocation summary follows. 20221030 14:08:24.668778: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc 20221030 14:08:24.668825: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): 	Total Chunks: 90, Chunks in use: 89. 22.5KiB allocated for chunks. 22.2KiB in use in bin. 2.7KiB clientrequested in use in bin. 20221030 14:08:24.668864: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): 	Total Chunks: 3, Chunks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB clientrequested in use in bin. 20221030 14:08:24.668900: I tensorflow/core/common_runtime/bfc_allocator.cc:1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MarkKocherovsky,High Batch Size Exceeds Memory,"Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.8  GPU model and memory NVIDIA GTX 1080Ti  Current Behaviour?   Standalone code to reproduce the issue  shell Epoch 1/5 20221030 14:08:24.668673: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 375.00MiB (rounded to 393216000)requested by op model/BERT_encoder/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_encoder/StatefulPartitionedCall/transformer/layer_0/self_attention/einsum_1/Einsum If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.  Current allocation summary follows. Current allocation summary follows. 20221030 14:08:24.668778: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc 20221030 14:08:24.668825: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): 	Total Chunks: 90, Chunks in use: 89. 22.5KiB allocated for chunks. 22.2KiB in use in bin. 2.7KiB clientrequested in use in bin. 20221030 14:08:24.668864: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): 	Total Chunks: 3, Chunks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB clientrequested in use in bin. 20221030 14:08:24.668900: I tensorflow/core/common_runtime/bfc_allocator.cc:1",2022-10-30T18:11:19Z,stat:awaiting response stale comp:keras type:performance TF 2.10,closed,0,9,https://github.com/tensorflow/tensorflow/issues/58373,",  Could you please use the below code to list out GPU devices first  `physical_devices =tf.config.list_physical_devices('GPU')` and then try to use below code to allow the Memory growth using `tf.config.experimental.set_memory_growth(physical_devices[0], True)` Please take a look at the official document for the reference.  Thankyou",I have the following code close to the top of the program:  Which returns the following log on startup ,"Hi , At present TF2.10 actively supports CUDA versions upto 11.2 .Please refer tested configurations here. I request you to please try tested configurations as per tensorflow official documenation .",So should I manually downgrade to 11.2?,"Hi  , You can do that.Please follow the procedure to install tensorflow with GPU as per the documentation attached here. Please follow it and let us know if any problem persists in installing Tensorflow with GPU. Also i believe that batch_size of 1500 may be very high and also i see your network is also deeper and bigger it may needs High RAM resources.Could you please also check with lower Batch sizes if working fine or not.If works with lower batch sizes and not working with Higher batch sizes probably its Out Of Memory issue with RAM.Then in that case you should opt for Higher Resources.","I will try downgrading my CUDA, but I will say that it does work with smaller batch sizes, so I'll look into Higher Resources. Is that a specific option I can enable?","Yes  , Please opt for Higher Resources for training on high batch sizes.You can opt for distributed training on multi GPUs as mentioned here tensorflow.org for better performance.Please try this and reach out us incase of any issues. Thankyou!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
681,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF 2.11.0/2.12 fails to build in MacOS 13  - XCode 14.1 - issue with ld linker)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0rc  Custom Code No  OS Platform and Distribution MacOS 13.0  Mobile device _No response_  Python version 3.10.6  Bazel version 5.3.0  GCC/Compiler version clang  XCode 14.1rc2  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,feranick,TF 2.11.0/2.12 fails to build in MacOS 13  - XCode 14.1 - issue with ld linker,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0rc  Custom Code No  OS Platform and Distribution MacOS 13.0  Mobile device _No response_  Python version 3.10.6  Bazel version 5.3.0  GCC/Compiler version clang  XCode 14.1rc2  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-29T17:21:06Z,stat:awaiting tensorflower type:build/install subtype:macOS subtype:bazel TF 2.10,closed,5,36,https://github.com/tensorflow/tensorflow/issues/58368, ! Could you look at this issue. Thank you!,Bug persists with TF 2.11rc2 and stable release of XCode 14.1.,"FYR, mostly, this is a bug of Xcode 14.x (or more specifically ld? There were some new changes in ld. It failed to build gcc with some early Xcode 14 versions). I ran into this problem since early Xcode 14 betas. I could build TensorFlow with Xcode 13.x on macOS Monterey (12.x) without problems. When I tried to build it with Xcode 14 on macOS 12.x, it failed. ","Technically, bazel allows for downloading a ""fresh version"" of clang upon configuring TF previous to compilation. But that option is broken.  See related issues:  https://github.com/tensorflow/tensorflow/issues/46976 https://github.com/tensorflow/tensorflow/issues/58453","> Technically, bazel allows for downloading a ""fresh version"" of clang upon configuring TF previous to compilation. But that option is broken.  >  > See related issues:  > https://github.com/tensorflow/tensorflow/issues/46976 > https://github.com/tensorflow/tensorflow/issues/58453 Yes I don't know what was the rationale of  this commit https://github.com/tensorflow/tensorflow/issues/58453issuecomment1304365920 I think that this configuration is currently not tested by the CI jobs so that commit was not checked by the CI.","Another approach may be to use one of the version of clang from MacPorts. I have several to choose from, but it is not clear how to enable such a version to compile TF in `.bazelrc`. That could at least be a way to test versions that do work. Suggestions are welcome.",", , and   It's not a compiler issue, it's a linker (ld) issue. As I said it seems Apple folks changed ld recently. I managed to build recent master branch with Xcode 14.1 by replacing the ld with the one from Xcode 13.x. My Xcode 14.1 is installed at /Application, I backed up the `ld` in   and replaced it with the one from 13. The ld from Xcode 13.4.1 works for me  ld from 14.x, e.g, 14.1 doesn't work  Unfortunately, it seems publicly available `ld64` source code is 609, `https://github.com/appleossdistributions/ld64/tree/ld64609`. I cannot find ld64 7xx and 8xx. So I don't know what exactly the issue is.","Hi guys, Xcode 14.2 RC is out. Maybe it will help you?","> Hi guys, Xcode 14.2 RC is out. Maybe it will help you? Thanks. Just tried (with both TF 2.11.0 and TF2.10) and it is still broken. Log attached (it's similar to the previous).  log.txt","According to `xcrun  ld v`, the ld comes with Xcode 14.2 RC is still **ld64820.1**. The **BUILD** time changed, though. ",Was this an issue with TF 2.10? (Trying to see if this is a regression or missing support),"Yes. The issue is not specific to any version of TF. It started to  appear in 2.11 simply because of the new XCode that came out at about  the same time. You can compile either 2.10 or 2.11 with older versions  of XCode (ld in particular). On 12/13/22 10:14 PM, Mihai Maruseac wrote: > > Was this an issue with TF 2.10? (Trying to see if this is a regression  > or missing support) > > â€” > Reply to this email directly, view it on GitHub  > ,  > or unsubscribe  > . > You are receiving this because you were mentioned.Message ID:  > ***@***.***> >","In this case, probably this will need to be handled when the team migrates the MacOS build infrastructure to one that contains the new version of XCode. Unfortunately, the release process can only run on a single architecture at a time, it will be too expensive to test all combinations. So we could not uncover this before as we're using old `ld` at this point. PRs to fix this are welcome, if you want to spend some time debugging and fixing this faster than it would take the OSS team to do the migration. CCing  too in case there is more available info",Does it help if you use the supported versions as documented in https://www.tensorflow.org/install/sourcemacos?, this looks a bit like the other ld build failures we have seen during the build infra migration.  Could you take a quick look. ,Not sure if this issue is related but we have seen `ld: malformed trie` errors when trying to build with Xcode command line tools. The issue goes away if built with the full Xcode. (see related comment),> Not sure if this issue is related but we have seen `ld: malformed trie` errors when trying to build with Xcode command line tools. The issue goes away if built with the full Xcode. (see related comment) Can you please point to a howto guide on using the full XCode for compilation (rather than using the command tools? I'd be happy to try if this works.,"> Not sure if this issue is related but we have seen `ld: malformed trie` errors when trying to build with Xcode command line tools. The issue goes away if built with the full Xcode. (see related comment) Thank you for prompt feedback ! It looks related. The only difference when building with XCode is that the deployment target (or platform version) is actually applied to the final binary, as expected from `â€”macos_minimum_os=11.0` when built with config `macos_arm64`. That can be confirmed with `dyld_info`. Before, it would have been `12.0` and after it is `11.0`. In my understanding, that disables linker's new â€œchained fixups formatâ€ and the same could be achieved with `linkopt=Wl,no_fixup_chains`. With 12.0 very similar `malformed trie` errors also happen at the runtime when loading (successfully compiled) dynamic libraries from the Python.  (E.g. https://github.com/tensorflow/text/issues/823issuecomment1251596272). What's more cryptic  with 11.0 it would compile&link but would not export the symbols properly (`nm` would list them, but `dyld_info exports` would not) and then just crash the Python process with `SIGBUS`, even with the `lldb` attached, with what looks like some form of OOM on parsing export entries recursively. It seems that XCode 14.x just ships a problematic linker, indeed, as downgrading to commandline tools 13.x (`@()PROGRAM:ld  PROJECT:ld64764`) resolves the issue, even if the deployment target/platform version is still 12.0 ğŸ¤·","> > Not sure if this issue is related but we have seen `ld: malformed trie` errors when trying to build with Xcode command line tools. The issue goes away if built with the full Xcode. (see related comment) >  > It seems that XCode 14.x just ships a problematic linker, indeed, as downgrading to commandline tools 13.x (`@()PROGRAM:ld PROJECT:ld64764`) resolves the issue, even if the deployment target/platform version is still 12.0 ğŸ¤· Ah, thats unfortunate. Thank you for the insight! > Can you please point to a howto guide on using the full XCode for compilation (rather than using the command tools? I'd be happy to try if this works. Set `DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer` (path to Xcode) and try running the build again.","> Set `DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer` (path to Xcode) and try running the build again. Thank you. Sorry for the silly question, but where should I set that flag? Bash profile? bazelrc?","Either way works, it is an environment variable for specifying the active developer directory for Xcode.  If using a .bazelrc file, include `build action_env  DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer` in it.","> > > Not sure if this issue is related but we have seen `ld: malformed trie` errors when trying to build with Xcode command line tools. The issue goes away if built with the full Xcode. (see related comment) > >  > >  > > It seems that XCode 14.x just ships a problematic linker, indeed, as downgrading to commandline tools 13.x (`@()PROGRAM:ld PROJECT:ld64764`) resolves the issue, even if the deployment target/platform version is still 12.0 shrug >  > Ah, thats unfortunate. Thank you for the insight! >  > > Can you please point to a howto guide on using the full XCode for compilation (rather than using the command tools? I'd be happy to try if this works. >  > Set `DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer` (path to Xcode) and try running the build again.   I don't think that will work. My `xcodeselect p` was `/Applications/Xcode.app/Contents/Developer` (and `xcrun f ld` reports right ld) and I ran into such problem. If I replace `/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ld` with ld from Xcode 13.x, then it goes. If setting `DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer`, then something is wrong (e.g., ld other than the one in `/Applications/Xcode.app` is used). I'll try it anyway.", FYR. Setting `DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer` doesn't work for me on macOS 13.1 + Xcode 14.2 (either M1 or x86_64).,The issue still applies to TF 2.12rc0. Same error. ,"I am checking XCode 14.3 beta and it appears that ld is still in version , so there should be no difference in behavior compared to previous XCode 14.x. ","> , , and  >  > It's not a compiler issue, it's a linker (ld) issue. As I said it seems Apple folks changed ld recently. I managed to build recent master branch with Xcode 14.1 by replacing the ld with the one from Xcode 13.x. My Xcode 14.1 is installed at /Application, I backed up the `ld` in >  >  >  > and replaced it with the one from 13. >  > The ld from Xcode 13.4.1 works for me >  >  >  > ld from 14.x, e.g, 14.1 doesn't work >  >  >  > Unfortunately, it seems publicly available `ld64` source code is 609, `https://github.com/appleossdistributions/ld64/tree/ld64609`. I cannot find ld64 7xx and 8xx. So I don't know what exactly the issue is. Hi    thank you for your insights! Could you please let me know how did you do  ""replace the ld file in Xcode 14.x by an old version from Xcode 13.x""? Did you replace other files other than ld, or just the ld file? Asking because I tried to ONLY replace the ld file, but I got the following info by running ld v  It looks like the ld version is correct, but LLVM and TAPI versions are still incorrect. Many thanks!","As this is now affecting 2 releases (2.11 and the upcoming 2.12), the least that could be done is to put  a warning on the release notes. ","> Hi   thank you for your insights! Could you please let me know how did you do  ""replace the ld file in Xcode 14.x by an old version from Xcode 13.x""? Did you replace other files other than ld, or just the ld file? >  replacing the ld is enough.  > Asking because I tried to ONLY replace the ld file, but I got the following info by running ld v >  >  >  > It looks like the ld version is correct, but LLVM and TAPI versions are still incorrect. >  That should be fine.",Would anyone have a workable version of ld to share? I can't install XCode 13.x once 14.x is in place with MacOS 13.x is installed.. ,> Would anyone have a workable version of ld to share? I can't install XCode 13.x once 14.x is in place with MacOS 13.x is installed.. Hi   this is fixed in https://developer.apple.com/servicesaccount/download?path=/Developer_Tools/Xcode_14.3_beta_2/Xcode_14.3_beta_2.xip. Can you please check and verify?
631,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to enable GSPMD?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.7  Custom Code No  OS Platform and Distribution Linux   Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,xiaodathereal,How to enable GSPMD?,Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.7  Custom Code No  OS Platform and Distribution Linux   Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-10-29T08:47:09Z,stat:awaiting response type:feature comp:gpu comp:xla TF 2.7,closed,0,11,https://github.com/tensorflow/tensorflow/issues/58363,Hi  ! Not sure on GSPMD ( General and Scalable Parallelization for ML Computation Graphs). But you can mentions the GPU's  which you want to use with tf.device / distribution strategy (Data Sharding policy is there) Could you post on TF forum for further assitance. Thank you!,"By GSPMD, we mean this paper: https://arxiv.org/abs/2105.04663. We use the 'xla_sharding.mesh_split' API, but it seems that GSPMD does not enabled at all.",Ok  ! Thanks for the update. Did you check in 2.9/2.10 version too ?  ! Could you look at this issue. Thank you!,"Hi , We are not sure of how GSPMD works and i gone through the link you provided and unable to found any useful insights to address your issue.Please check the content of generated files for any insights. Could you please confirm details like what strategy you used for distribution computing and how you enabled GPUs for tensorflow etc. We also recommend to refer an old issue here incase if useful to you. We recommend to use this [tensorflow.org/xla]/(https://www.tensorflow.org/xla) document and tutorials there to enable XLA for tensorflow models for current TF versions(2.10) and also use tensorflow distribution strategies as mentioned above and here.If you find any difficulties in above please reachout to us. Thank you.",Thanks for the answer . But we still want to reproduce the use cases mentioned in the GSPMD paper.   could you please check this question?,"Hello, At the moment JAX has the cleanest GSPMD API integration as pjit https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html Here are some examples of usage in two model codebases: TF: https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm JAX: https://github.com/google/paxml/tree/main/paxml/tasks/lm (which uses layers from https://github.com/google/praxis) Note that we currently have not started to support OSS users for the two codebases above. So I'd encourage you to explore the pjit API directly, e.g., here is a test: https://github.com/google/jax/blob/main/tests/pjit_test.py","Thanks for the answer   I see the current situations of directly utilization of GSPMD on TF. But, is there a plan (or time schedule) to open the APIs for users to directly use GSPMD? For me, it is confusing that GSPMD is supported on Jax but not on TF. Because GSPMD is implemented in XLA, which seems irrelevant of different frontends. I cannot see the technical barriers for TF utilizing GSPMD.","It is already directly usable on TPUs. The codebase I mentioned earlier is an example of using it.  However, I don't work on TensorFlow and am not aware of the progress for GPU APIs. On the other hand, JAX has the same API for all backends.","OK, thanks for the answer  "," , Could you please close the issue, if your issue is resolved. Also, refer to the code directory of XLA compiler SPMD here, could be of some help for you. Thanks!","Issue closed, thanks"
636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RaggedTensorToVariant abort)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,creakseek,RaggedTensorToVariant abort,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-28T04:47:20Z,stat:awaiting response type:bug comp:ops TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58352,and there's a related CVE,", In tfnightly no crash observed and displaying only **Invalid Argument error** as the arguments you provided are not valid. Hence the session crash problem was taken care in tfnightly version. Kindly find the gist of it here. Thank you!",It was already protected in the upcoming release `pip install tensorflowcpu==2.11.0rc1` so I think we could close it.,Are you satisfied with the resolution of your issue? Yes No
1432,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(PyPI tensorflow-macos wheels 2.9.[012] and 2.10.0 don't work on Apple silicon + Rosetta)ï¼Œ å†…å®¹æ˜¯ (I'm supposed to be able to install and run PyPI's tensorflowmacos on Apple silicon with Rosetta, right? At least I was able to with 2.8.0. All versions in PyPI since then don't work because they are compiled with AVX extension which Rosetta doesn't support. See details in ""Click to expand!"" below for repro and verification steps. I tested 2.9.[012] and 2.9.10. I suspect newer packages were compiled incorrectly with AVX. In an x86 Rosettatranslated Python virtualenv, I expect to be able to 1. `pip install tensorflowmacos==AFFECTED_VERSIONS` 2. successfully run `python c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__)""` without getting SIGKILL (Illegal instruction) error Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.0, 2.9.1, 2.9.2, 2.10.0  Custom Code No  OS Platform and Distribution Apple silicon, M1 Macbook Pro, macOS 12.6  Mobile device _No response_  Python version 3.8.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,davidxia,PyPI tensorflow-macos wheels 2.9.[012] and 2.10.0 don't work on Apple silicon + Rosetta,"I'm supposed to be able to install and run PyPI's tensorflowmacos on Apple silicon with Rosetta, right? At least I was able to with 2.8.0. All versions in PyPI since then don't work because they are compiled with AVX extension which Rosetta doesn't support. See details in ""Click to expand!"" below for repro and verification steps. I tested 2.9.[012] and 2.9.10. I suspect newer packages were compiled incorrectly with AVX. In an x86 Rosettatranslated Python virtualenv, I expect to be able to 1. `pip install tensorflowmacos==AFFECTED_VERSIONS` 2. successfully run `python c ""import tensorflow as tf; print('TensorFlow version:', tf.__version__)""` without getting SIGKILL (Illegal instruction) error Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.0, 2.9.1, 2.9.2, 2.10.0  Custom Code No  OS Platform and Distribution Apple silicon, M1 Macbook Pro, macOS 12.6  Mobile device _No response_  Python version 3.8.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-10-27T13:13:03Z,stat:awaiting response type:bug type:build/install subtype:macOS TF 2.9,closed,0,24,https://github.com/tensorflow/tensorflow/issues/58339,"I don't know if the condaforge tensorflow packages are maintained by Apple or Google, but 2.10.0 and 2.9.1 on Rosetta + py38 work for me.", Could you please confirm if your issue has been resolved? Thank you!," I just downloaded the  `tensorflow_macos2.9.2cp38cp38macosx_11_0_x86_64.whl` file from PyPI here, unzipped it, and ran `objdump d /tmp/tensorflow_macos2.9.2/tensorflow/python/util/_tf_stack.so  head     6165: c5 f8 11 40 10               	vmovups	%xmm0, 16(%rax)     63f7: c5 fc 11 40 20               	vmovups	%ymm0, 32(%rax)     63fc: c5 fc 11 40 40               	vmovups	%ymm0, 64(%rax)     6401: c5 fc 11 40 60               	vmovups	%ymm0, 96(%rax)     6406: c5 fc 11 80 80 00 00 00      	vmovups	%ymm0, 128(%rax)     640e: c5 fc 11 80 c0 00 00 00      	vmovups	%ymm0, 192(%rax)     6416: c5 fc 11 80 e0 00 00 00      	vmovups	%ymm0, 224(%rax)     641e: c5 fc 11 80 38 01 00 00      	vmovups	%ymm0, 312(%rax)     6426: c5 fc 11 80 20 01 00 00      	vmovups	%ymm0, 288(%rax)     642e: c5 fc 11 80 00 01 00 00      	vmovups	%ymm0, 256(%rax) ``` I think all the `tensorflow_macos(2.9.0, 2.9.1, 2.9.2, 2.10.0)cp(38, 39, 310)...86_64.whl`s on PyPI that have AVX instructions will need to be replaced, right?","Hi, just checking for any updates on this. Thanks!","Hi , Could you please share your thoughts on this issue?",FYI  ,"To clarify, I'm just seeking clarity on whether I can expect tensorflowmacos to support Apple silicon + Rosetta. Totally fine if the decision by Google and/or Apple is no after a certain version. I just want an explicit and transparent notification of this. Thanks.",Hi David! Thanks for reaching out about this. We are currently discussing this internally and will have a response for you soon. . Thanks! ,"Thanks tf  and toplay for pointing out this issue. Apologize for delay in response. > To clarify, I'm just seeking clarity on whether I can expect tensorflowmacos to support Apple silicon + Rosetta. Totally fine if the decision by Google and/or Apple is no after a certain version. I just want an explicit and transparent notification of this. Thanks.  . This is a good question. Let me provide some context (you might already be familiar or would have guessed). As you mentioned Rosetta doesn't support AVX, so TF built with AVX enabled will have issue which you pointed out. Till TF 2.8 we were building the packages : CC_OPT_FLAGS=""march=core2 Wnosigncompare""  and then we switched to because we were not building the TF base optimally for x86 builds.   CC_OPT_FLAGS=""march=native Wnosigncompare"" In order to be consistent with Tensorflow base builds we switched to using the `native` flag which is akin to `c opt` recommended by Google as well. We have couple of options.  1. Use the native ARM64 builds. Can you please provide reason why you need to use Rosetta and can't use the native builds ?  2. We can add back the `core2` flag which will not be the most optimal build, which I would like to avoid if we can. If there is a compelling reason we can go with option (2). Please let me know your thoughts. ","Thanks for the update. 1. We would love to use native macosx arm64 builds. But my team depends not only on `tensorflow` (usually only a transitive dependency) but also `tfx` and a lot of other tensorflow related packages like `tfxbsl`, `tensorflowdatavalidation`, etc. In order to use native macosx arm64 builds, all these other tensorflow packages will also need to provide macosx arm64 builds. So we are stuck using macosx x86 + Rosetta. 2. I thought the whole point of Google and/or Apple creating an entirely new `tensorflowmacos` PyPI package is to provide macosx x86 builds that Rosetta can run? Otherwise I can just use `tensorflow` macosx x86 builds which are here on PyPI, right? Happy to provide more details.","> Thanks for the update. >  > 1. We would love to use native macosx arm64 builds. But my team depends not only on `tensorflow` (usually only a transitive dependency) but also `tfx` and a lot of other tensorflow related packages like `tfxbsl`, `tensorflowdatavalidation`, etc. In order to use native macosx arm64 builds, all these other tensorflow packages will also need to provide macosx arm64 builds. So we are stuck using macosx x86 + Rosetta. > 2. I thought the whole point of Google and/or Apple creating an entirely new `tensorflowmacos` PyPI package is to provide macosx x86 builds that Rosetta can run? Otherwise I can just use `tensorflow` macosx x86 builds which are here on PyPI, right? >  > Happy to provide more details. Thanks . This makes sense. We will look into enabling Rosetta support in upcoming release. I will update here.","Thanks! Is there a way to fix the regression and support Rosetta for already released 2.9.0, 2.9.1, 2.9.2, and 2.10.0?","> Thanks! Is there a way to fix the regression and support Rosetta for already released 2.9.0, 2.9.1, 2.9.2, and 2.10.0? This will be hard. We keep our versions in sync with TF core. And updating those packages in pypi is not possible (atleast not  desirable) without just bumping the version numbers. On top of that we will have to do extensive testing as well. So i would say going forward we can fix it. "," I think you can replace the miscompiled wheels with correctly compiled ones by using a `build tag` in the wheel's file name. This is documented in PEP427 > The wheel filename is `{distribution}{version}({build tag})?{python tag}{abi tag}{platform tag}.whl` and explained here. > it seems everybody agrees you can't overwrite existing pypi uploads, or reupload a fixed version after you delete a broken version. However, it seems actually possible and officially supported: ""build numbers"" are a feature that nobody has ever used or remembers they exist, but that seems to work, at least for me. > The trick is to rename the files in the following pattern: > `mypackage0.31.0py2.py3noneany.whl` > `mypackage0.31.01py2.py3noneany.whl` > `mypackage0.31.02py2.py3noneany.whl` > The ""1"" or ""2"" are build numbers. If pip finds all these files for the same release, it will pick the one with the highest build number. It also seems to work if it finds only a single file with a nonzero build number, so you can use that after you deleted the original. > A typical example for when I think you should use this is after one of the wheels was badly built and you need to replace it with a correctlybuilt wheel from the same sources Is this an acceptable way for your team? ğŸ¤ ","Hi  ! I was able to use Tensorflowmacos wheel after i installed either tensorflowmetal or tensorflowdeps through conda . It worked in few test cases and glitched in some case. Attached few screenshots of  Test cases for reference. 1.  Illegal instruction without tensorflowdeps (in base environment with clang 6). 2. After install conda and install tensorflowdeps (clang 14 is clang version here) 3.  Test case executed with git_58587.py But the more effective solution, we found was through Metal plugin from Apple and they have introduced  GPU support and distribution computing recently.  Clang support is not stable yet (for Testing with Clang 14) , So I think we should stick to metal plugin for a while . Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,ok  ! Thanks for the clarification on misplaced wheels on PyPi. Reopening this issue as requested. Thank you!," thanks for reopening and for the tip. Just to be clear, your suggestion to use `tensorflowmetal` is for Apple silicon native (i.e. arm64) without Rosetta translation, right? And it requires conda and pip packages? Unfortunately, my company currently only uses PyPI instead of conda. So we cannot `conda install c apple tensorflowdeps` according to the Apple doc here. Moreover, even if we did use a hybrid conda + PyPI environment (which isn't recommended), we'd need to first install pip packages since our software depends on `tfx` which pulls in `tensorflow` as a transitive dependency. This is further against the conda doc's recommendations (""Use pip only after conda""). Then we'd need to run `conda install forcereinstall tensorflow=2.9.1` to replace the bad PyPI `tensorflowmacos` with the osxarm64 ones from condaforge. And these condaforge packages are uploaded by usernames like `ngam` and `uwe.korn` whose affiliations with official maintainer teams from Google and Apple are unclear. full conda + pip virtualenv repro steps P.S. Your links for 2 and 3 above  > 2. After install conda and install tensorflowdeps > 3. Test case executed with git_58587.py seem to be the same screenshot. Is 3 supposed to be different?",">  I think you can replace the miscompiled wheels with correctly compiled ones by using a `build tag` in the wheel's file name. This is documented in PEP427 >  > > The wheel filename is `{distribution}{version}({build tag})?{python tag}{abi tag}{platform tag}.whl` >  > and explained here. >  > > it seems everybody agrees you can't overwrite existing pypi uploads, or reupload a fixed version after you delete a broken version. However, it seems actually possible and officially supported: ""build numbers"" are a feature that nobody has ever used or remembers they exist, but that seems to work, at least for me. >  > > The trick is to rename the files in the following pattern: >  > > `mypackage0.31.0py2.py3noneany.whl` > > `mypackage0.31.01py2.py3noneany.whl` > > `mypackage0.31.02py2.py3noneany.whl` >  > > The ""1"" or ""2"" are build numbers. If pip finds all these files for the same release, it will pick the one with the highest build number. It also seems to work if it finds only a single file with a nonzero build number, so you can use that after you deleted the original. >   , thanks for the link. That seems like a viable option for fixing the builds. The main concern about the earlier TF builds is testing and releasing them. We would have to do testing against all 3 python version for x86 and amr64 architectures. Also would have to make sure the Metal plugin builds from the time are working. This will be a lot of effort and currently not in a position to signup. We have released the TF v2.11 builds which were built with Rosetta compatible flags. Can you verify if it works for you? > > A typical example for when I think you should use this is after one of the wheels was badly built and you need to replace it with a correctlybuilt wheel from the same sources >  > Is this an acceptable way for your team? ğŸ¤",> This will be a lot of effort and currently not in a position to signup. No worries. Totally understand. > We have released the TF v2.11 builds which were built with Rosetta compatible flags. Can you verify if it works for you? Thanks! I just checked. It works! ğŸ™ ," , Thanks for verifying. If your issue is resolved, Could you please close this issue. For latest release files of `tensorflowmacos` on Pypi you can refer to this page https://pypi.org/project/tensorflowmacos/files",Are you satisfied with the resolution of your issue? Yes No
1865,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to build 2.10 from source for GPU due to `absl::bit_cast`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version TF 2.10  Custom Code No  OS Platform and Distribution Ubuntu 16.04.7 LTS  Mobile device _No response_  Python version 3.9  Bazel version 5.2.0  GCC/Compiler version 11.2  CUDA/cuDNN version 11.3  GPU model and memory GeForce GTX 1050 Ti  Current Behaviour? When compiling version 2.10 from source for GPU, I get the attached error after issuing my `bazel build` command. It looks that the issue is the `absl::bit_cast` command and the compiler being unable to instantiate the templated functions. The problem maybe with my build setup but I am not able to further debug what exactly is wrong. With exactly the same setup I was able to build from source version the GPU variant of version 2.9.  Standalone code to reproduce the issue  shell external/com_google_absl/absl/base/casts.h(164): error: type name is not allowed external/com_google_absl/absl/base/casts.h(164): error: identifier ""__builtin_bit_cast"" is undefined           detected during instantiation of ""Dest absl::lts_20220623::bit_cast>(const Source &) [with Dest=uint16_t, Source=int16_t, =0]"" external/com_google_absl/absl/base/internal/endian.h(143): here external/com_google_absl/absl/base/internal/endian.h(143): error: no instance of function template ""absl::lts_20220623::bit_cast"" matches the argument list             argument types are: () external/com_google_absl/absl/base/casts.h(164): error: identifier ""__builtin_bit_cast"" is undefined           detected during instantiation of ""Dest absl::lts_20220623::bit_cast>(const Source &) [with Dest=uint32_t, Sou)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,babarmehta,Unable to build 2.10 from source for GPU due to `absl::bit_cast`,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version TF 2.10  Custom Code No  OS Platform and Distribution Ubuntu 16.04.7 LTS  Mobile device _No response_  Python version 3.9  Bazel version 5.2.0  GCC/Compiler version 11.2  CUDA/cuDNN version 11.3  GPU model and memory GeForce GTX 1050 Ti  Current Behaviour? When compiling version 2.10 from source for GPU, I get the attached error after issuing my `bazel build` command. It looks that the issue is the `absl::bit_cast` command and the compiler being unable to instantiate the templated functions. The problem maybe with my build setup but I am not able to further debug what exactly is wrong. With exactly the same setup I was able to build from source version the GPU variant of version 2.9.  Standalone code to reproduce the issue  shell external/com_google_absl/absl/base/casts.h(164): error: type name is not allowed external/com_google_absl/absl/base/casts.h(164): error: identifier ""__builtin_bit_cast"" is undefined           detected during instantiation of ""Dest absl::lts_20220623::bit_cast>(const Source &) [with Dest=uint16_t, Source=int16_t, =0]"" external/com_google_absl/absl/base/internal/endian.h(143): here external/com_google_absl/absl/base/internal/endian.h(143): error: no instance of function template ""absl::lts_20220623::bit_cast"" matches the argument list             argument types are: () external/com_google_absl/absl/base/casts.h(164): error: identifier ""__builtin_bit_cast"" is undefined           detected during instantiation of ""Dest absl::lts_20220623::bit_cast>(const Source &) [with Dest=uint32_t, Sou",2022-10-26T07:47:58Z,type:build/install subtype: ubuntu/linux TF 2.10,closed,0,11,https://github.com/tensorflow/tensorflow/issues/58316,Have you already tried to compile with c++17 flags? https://github.com/tensorflow/tensorflow/blob/cdf6e5f905056c19d591d6c0ce414a100f04f750/.bazelrcL354L364 Is `DNO_CONSTEXPR_FOR_YOU` still valid?,"Yes, I tried removing `DNO_CONSTEXPR_FOR_YOU` and explicitly enabled C++17, but it is always erroring out at same place. Anywhere `GpuLaunchKernel` is called the build errors out.  I don't know much about the code base but do the nonGPU builds not use `abseil`? Because I can build the CPU variant (with and without `mkl`) just fine, only the GPU variant fails. Is this a problem with `nvcc` maybe?","I've just compiled master, few days, ago inside the Docker image we use for producing the nightly wheels `gcr.io/tensorflowsigs/build:latestpython3.10` and it compiled without any problem. Just to understand if it is something related to your environment can you try to reproduce the build with that image?","I can compile inside a container with that image, and I believe it is definitely my environment. But what I am trying to understand is what in my environment is causing this, because I can build `2.9` GPU variant just fine. :(","Maybe this newer version of `abseil` is just not compatible with my environment, although I would like to understand why that is. Do you think I could just patch the source code to use the same `abseil` that `2.9` does and still get a working installation?",Mhh... have you tried to align your env with the versions specified in the Tested configurations? See https://www.tensorflow.org/install/sourcetested_build_configurations,"That's interesting...with GCC `9.3` the compilation is success. Not sure what the problem is, because `11.2` is a newer version after all! But I don't know how I can create a minimum repro example out of this and submit to GCC devs because it is very hard to understand where the bug is.",We had other issues with GCC 11 https://github.com/tensorflow/tensorflow/issues/50303 As the build is only tested with what you see in the table we could have still some compatibility issue (or with our dependencies versions) with other GCC releases.,"I see, that is good to know. Thank you for all your assist, I think we can close this issue now unless you want to investigate the root cause. ğŸ˜€",Are you satisfied with the resolution of your issue? Yes No,"> think we can close this issue now unless you want to investigate the root cause Thanks, when we will upgrade the official build with new GCC versions probably we will need to check the dependencies versions and eventually their own incompatibility."
881,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow on C++ Builder 11.2)ï¼Œ å†…å®¹æ˜¯ (Is there any support to use C++ builder 11.2 with tensorflow , when i try to include the lib file it complains no symbols are included. the command i use to add the library is pragma comment(lib,""lib\\tensorflow.lib"") , or i add to the project it gets greyed out right away , the only include i have is include ""include\tensorflow\c\c_api.h"" & just a simple call as a test >  {      TF_Status *status = NULL;     status = TF_NewStatus(); } Running 11.2 Version 28.0.46141.0937 ( Latest version) , project is X64 error when doing full build is (Path before lib removed.) > [ilink64 Error] Fatal: Archive file 'LIB\TENSORFLOW.LIB' lists no symbols in its dictionary.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,delphianaus,Tensorflow on C++ Builder 11.2,"Is there any support to use C++ builder 11.2 with tensorflow , when i try to include the lib file it complains no symbols are included. the command i use to add the library is pragma comment(lib,""lib\\tensorflow.lib"") , or i add to the project it gets greyed out right away , the only include i have is include ""include\tensorflow\c\c_api.h"" & just a simple call as a test >  {      TF_Status *status = NULL;     status = TF_NewStatus(); } Running 11.2 Version 28.0.46141.0937 ( Latest version) , project is X64 error when doing full build is (Path before lib removed.) > [ilink64 Error] Fatal: Archive file 'LIB\TENSORFLOW.LIB' lists no symbols in its dictionary.",2022-10-25T13:43:15Z,stat:awaiting response type:build/install stale comp:runtime subtype:windows,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58298,  Could you please fill the issue template. Thank you!,"Is the windows build broken ? i want to build cuda debug and see if i can import the dll into C++ builder. Bazel  5.3.1 VC 2022 Branch 2.10 from git hub, using latest copy of anaconda3 ( Python 3.10)",Hi  ! Sorry for the late response.  Just wanted to let you know  earlier that from 2.11 onwards Windows users has to use Cuda Compute capability either through WSL2/TensorflowDirect ML plugin.   Ref. Thank you!,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. TensorFlow with GPU access is supported for WSL2 on Windows 10 19044 or higher. This corresponds to Windows 10 version 21H2, the November 2021 update. You can get the latest update from here: Download Windows 10. For instructions, see Install WSL2 and NVIDIAâ€™s setup docs for CUDA in WSL. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1090,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fail to cross-compile tflite for android on windows and ubuntu)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version latest  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am following this tutorial to crosscompile tflite for android on windows, I execute following command in powershell but got error:  the content in CMakeOutput.log is :  ```  Standalone code to reproduce the issue execute  `D:\AI\repos\tensorflow\tflite_build_android> cmake DCMAKE_TOOLCHAIN_FILE=C:/Users/user/AppData/Local/Android/Sdk/ndk/21.3.6528147/build/cmake/android.toolchain.cmake DANDROID_PLATFORM=30 DANDROID_ABI=armeabiv7a DTFLITE_ENABLE_GPU=ON ../tensorflow/lite ` on Windows)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,wwdok,Fail to cross-compile tflite for android on windows and ubuntu,"Click to expand!    Issue Type Support  Source source  Tensorflow Version latest  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am following this tutorial to crosscompile tflite for android on windows, I execute following command in powershell but got error:  the content in CMakeOutput.log is :  ```  Standalone code to reproduce the issue execute  `D:\AI\repos\tensorflow\tflite_build_android> cmake DCMAKE_TOOLCHAIN_FILE=C:/Users/user/AppData/Local/Android/Sdk/ndk/21.3.6528147/build/cmake/android.toolchain.cmake DANDROID_PLATFORM=30 DANDROID_ABI=armeabiv7a DTFLITE_ENABLE_GPU=ON ../tensorflow/lite ` on Windows",2022-10-25T13:30:20Z,stat:awaiting response type:build/install stale comp:lite subtype:windows,closed,0,12,https://github.com/tensorflow/tensorflow/issues/58297,"Hi  ! Above error is pointing towards misconfigured path of visual studio 2019. You can set the environment path for visual studio 2019 or select visual studio as Generator to fix this issue. Ref . To crosscompile with Arm tool chain, Please follow   ARM v7 documentation . Thank you!","Hi   As you suggest, I add `G ""Visual Studio 16""` in cmake command line, but still has the same error: !image Then I also try to cross compile tflite android library on Ubuntu with this tutorial, but also got error: !image I have a question here, what is difference between  and  to build the tflite android library  ? BTW, do you have prebuilt header and library of tflite android to share ? I want to directly use other's built files ~","I try execute   on ubuntu instead of windows. Got error againï¼š !image I try to use NDK of  higher version 24 instead of 21, but there are still new errorsï¼ŒI want to give up compile tflite android libraryï¼Œ so does anyone can share their built library and header ï¼Ÿ",I try to use compile it by bazel on ubuntuï¼Œbut still got errorï¼š !image I have installed bazel by deb fileï¼Œi can use bazel in other terminal except the terminal opened under tensorflow directoryï¼š !image,Sorry!  for the inconvience. I am working on this issue and will post my updates when I have a working gist (By EOD ). Thank you for the patience.," No hurry, you can choose the most convenient compilation way, no matter it's windows or ubuntu, no matter it's cmake or bazel. Thank you for your help !"," ! Actually, CMake version is 3.16 for the lite build. But I could replicate this issue with CMake 3.23 version later. Thank you!","I can build tensorflow lite on ubuntu with bazel, but I need to delete `.bazelversion` . And for the build with cmake, I still can did not get it work.","Hi , We recommend bazel for Android builds, can you follow these instructions: https://www.tensorflow.org/lite/android/lite_build and see if that works for you? Also are you installing bazel or bazelisk? Bazelisk is recommended as per the Bazel Documentation: https://bazel.build/install",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1274,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Supporting Gather_OP on GPU delegate)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: Ubuntu 20.04.5 LTS  TensorFlow installed from: source  TensorFlow version: v2.10.0rc2 (https://github.com/tensorflow/tensorflow/tree/v2.10.0rc2) Hello, For performance reason I was looking to support `gather_op` on GPU delegate. After some investigation I've noticed that the shader for such op seems to be already written at  `/tensorflow/lite/delegates/gpu/common/tasks/gather.cc` Even tests seems to be already implemented, see: `/tensorflow/lite/delegates/gpu/common/tasks/gather_test_util.cc` Is there a particular reason why the support for this op on GPU is not included already? Is it bugged? I tried to include such op by modifying `model_builder.cc` by adding the operation parser such as:  after this the gpu_delegate (open_cl) run the gather operations inside my model without complaining and I can even see a sharp increasing in performance in terms of inference time. Only problem is that numerical results are wrong (while they are correct if I run gather on CPU). Any help, advice?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Daveorr,Supporting Gather_OP on GPU delegate,"**System information**  OS Platform and Distribution: Ubuntu 20.04.5 LTS  TensorFlow installed from: source  TensorFlow version: v2.10.0rc2 (https://github.com/tensorflow/tensorflow/tree/v2.10.0rc2) Hello, For performance reason I was looking to support `gather_op` on GPU delegate. After some investigation I've noticed that the shader for such op seems to be already written at  `/tensorflow/lite/delegates/gpu/common/tasks/gather.cc` Even tests seems to be already implemented, see: `/tensorflow/lite/delegates/gpu/common/tasks/gather_test_util.cc` Is there a particular reason why the support for this op on GPU is not included already? Is it bugged? I tried to include such op by modifying `model_builder.cc` by adding the operation parser such as:  after this the gpu_delegate (open_cl) run the gather operations inside my model without complaining and I can even see a sharp increasing in performance in terms of inference time. Only problem is that numerical results are wrong (while they are correct if I run gather on CPU). Any help, advice?",2022-10-25T11:52:48Z,stat:awaiting tensorflower type:support comp:lite TFLiteGpuDelegate TF 2.10,closed,3,6,https://github.com/tensorflow/tensorflow/issues/58295,"""Is there a particular reason why the support for this op on GPU is not included already? Is it bugged?"" The reason is very limited support of parameters, it will work correctly only in specific case. Looks like your case is not supported or smt else is wrong.","Thanks for the insights, My case is pretty simple, I have  `index` tensor of shapes `(nxm)`  `params` tensor of shapes `(1xk)` (basically we sample always from a flat tensor) expected behaviour would be something like: ''' INPUTS  params = [a,b,c,d,e,f] index = [[2,3,1], [1,2,3], [1,4,3]] OUTPUT out = [[c,d,b],[b,c,d],[b,e,d]] ''' But I noticed that `gather_OP` for cl GPU delegate is tested only for the case in which `index` and `params` are both flat tensor, do you know by chance if this case is supposed to be supported already so I can better understand how to fix this behavior? Do you also know by chance what is the expected OOB behavior of this op when running on GPU?","> ""Is there a particular reason why the support for this op on GPU is not included already? Is it bugged?"" The reason is very limited support of parameters, it will work correctly only in specific case. Looks like your case is not supported or smt else is wrong. Is it supported that one of the two inputs of the gather operator is a constant tensor?  Thanksï¼",Has the problem of incorrect accuracy of the gather operator been solved?,"We have just merged a parser for the Gather op for OpenCl. Unfortunately, using a >=1 dimensional index is a narrow use case, and a pain to implement. To me the reward/effort ratio is not there. As such, your example listed above is not supported. If you would like to add support, please submit a PR. Thanks.",Are you satisfied with the resolution of your issue? Yes No
754,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.data.experimental.CsvDataset` cause IOT instruction crash)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10, tfnightly  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output ```shell F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,creakseek,`tf.data.experimental.CsvDataset` cause IOT instruction crash,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10, tfnightly  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output ```shell F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 ",2022-10-25T02:56:12Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:data TF 2.10,closed,0,13,https://github.com/tensorflow/tensorflow/issues/58290,Can you share your `/tmp/my_file0.csv` ?,"hi   if you run the code example in the report directly, it would automatically generate `/tmp/my_file0.csv` for you! (which is also from doc) ","Ok, probably It could be better ""protected"" instead of crashing but what is your use case about batching multiple time in the loop? I suppose you want to use just a final batching or not? "," yes you are right, indeed in this code example the batching multiple times is not necessary. However, I think it is still better to ""protect"" such an operation instead of crashing?"," Yes, but you are generating an overflow with your original loop Let see if we could substitute `CHECK_LE` with something else: https://github.com/tensorflow/tensorflow/blob/38af575aa32c5633395a538114fd85e5e7fe049b/tensorflow/core/framework/tensor_shape.ccL418L419", Are these considered Security Issues? E.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa2022069.md,">  Are these considered Security Issues? E.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa2022069.md Ok, It seems no more after https://github.com/tensorflow/tensorflow/commit/620ab3e167b0a64b6732dac16fd0edaf8284cb8e",Are you satisfied with the resolution of your issue? Yes No,I've tested the gist again with the last nightly and the code path of this example it seems to be different from the one fixed with the PR https://github.com/tensorflow/tensorflow/pull/58328: https://colab.research.google.com/gist/bhack/9dc16546d360291eb943bbeb36a01c57/untitled192.ipynb But I don't have enough cloud resources to recompile TF again a prepare a new PR / ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
680,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Migrate/Extend Stale management to Github Action)ï¼Œ å†…å®¹æ˜¯ (https://docs.github.com/en/actions/managingissuesandpullrequests/closinginactiveissues With this PR I propose to move the stale management to use the regular Github Action:   to standardize and lower the barrier to the external contribution  to extend the stale management coverage All days thresholds  were temp set to `1` so that we could discuss the numbers here. Complete options list: https://github.com/marketplace/actions/closestaleissues)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,bhack,Migrate/Extend Stale management to Github Action,https://docs.github.com/en/actions/managingissuesandpullrequests/closinginactiveissues With this PR I propose to move the stale management to use the regular Github Action:   to standardize and lower the barrier to the external contribution  to extend the stale management coverage All days thresholds  were temp set to `1` so that we could discuss the numbers here. Complete options list: https://github.com/marketplace/actions/closestaleissues,2022-10-24T19:16:13Z,awaiting review ready to pull size:M,closed,0,14,https://github.com/tensorflow/tensorflow/issues/58285,//devsupport,Please check also this sorted list so that we could better finetune the labels that we want to handle in this PR: https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+sort%3Aupdatedasc+,Hi  Can you please review this PR ? Thank you!,/,"Thanks for the CC, let me check on some things internally on this", and   what are your thoughts on what the stale / closed values should be here.  The original 7 stale and 7 to close seem a bit short. ,I'd say 1 month should be enough,Here we have multiple values that we have not set (1). The PR is editable on your side so we could differentiate the number of days x labels. We need to set something to replace `1` E.g. `contribution welcome` could have a different threshold from `awaiting response` etc..,The times LGTM.  Do we want to add any sort of freeze label to exemptissue/prlabels to disable bot if we want? ,> The times LGTM.  Do we want to add any sort of freeze label to exemptissue/prlabels to disable bot if we want?  Do you have any specific use case in mind?,"For example the Python3.11 issue (https://github.com/tensorflow/tensorflow/issues/58032). We know is a roadmap of a large amount of work, so we would add a label to it to not automatically close / nag people.",Ok you can add this label to the repo and use  `exemptprlabels`: https://github.com/marketplace/actions/closestaleissuesexemptprlabels,Thank you all for sharing your inputs. Our team will take a look into github actions as a potential alternative workflow. We will also update the stale duration to 14 days for stale and another 14 days to close (total of 4 weeks). I will update this thread once the change is in effect/additional updates.  ,Just to summarize the scope of this PR:  Enable the community contribution to the infrastructure on a well know DSL and transparent orchestration (GitHub Action)  Specialize stale management for different labels specially for the contribution specific labels.  As we we have accumulated over the years contribution labels it is really hard to cherry pick a contribution for the community as we don't retriage regularly these labels and I suppose we don't know if we have a valid reviewer for the queue (I guessing this just by the public turnaround pings we have on PRs).  Also the code is full of TODO referring internal only url tickets so we don't have many inline element to contribute. I don't know what  think about the current status of our label inclusivity but it would be nice to collect an opinion: https://chaoss.community/metricissuelabelinclusivity/
777,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Disable rocBLAS atomics when asked)ï¼Œ å†…å®¹æ˜¯ (For some reason, rocBLAS matrix ops (on AMD GPUs) are nondeterministic by default (as opposed to cuBLAS ones). Among other things, this causes covariance matrices to lose the positive semidefinite property. This PR makes the rocBLAS backend optin to Tensorflow's determinism flag, restoring deterministic behavior. I used the following code to test on gfx90a (requires jax, transformers, torch):  It should print an array with all elements equal to `True`  if the execution is deterministic. Execute with `TF_DETERMINISTIC_OPS=1` environment variable set. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,jglaser,Disable rocBLAS atomics when asked,"For some reason, rocBLAS matrix ops (on AMD GPUs) are nondeterministic by default (as opposed to cuBLAS ones). Among other things, this causes covariance matrices to lose the positive semidefinite property. This PR makes the rocBLAS backend optin to Tensorflow's determinism flag, restoring deterministic behavior. I used the following code to test on gfx90a (requires jax, transformers, torch):  It should print an array with all elements equal to `True`  if the execution is deterministic. Execute with `TF_DETERMINISTIC_OPS=1` environment variable set. ",2022-10-24T19:02:30Z,awaiting review ready to pull comp:xla size:S,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58284,FYI  the AMD ROCm CI is broken from before this PR. We have a PR up to fix that: https://github.com/tensorflow/tensorflow/pull/58137,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,  FYI
525,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to allocate memory for input tensors - iOS)ï¼Œ å†…å®¹æ˜¯ (Installed TensorflowLite on an example Xcode project I am using for tests with these version:  When I try to allocate the interpreter:  I get this error: `Error: Failed to allocate memory for input tensors. ` The model I am trying to use is the one attached here. hifill_inpaint.tflite.zip)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DanielZanchi,Failed to allocate memory for input tensors - iOS,Installed TensorflowLite on an example Xcode project I am using for tests with these version:  When I try to allocate the interpreter:  I get this error: `Error: Failed to allocate memory for input tensors. ` The model I am trying to use is the one attached here. hifill_inpaint.tflite.zip,2022-10-24T14:19:19Z,stat:awaiting response type:bug stale comp:lite comp:runtime TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58280,"Hi  ! Sorry for the late response. Have you checked in nightly version too. Could you let us know your system details for replicating this issue. (Xcode version, OS details, lite c wheel ). For Darwin Arm64(Mac M1) , You can go with this wheel for Tensorflowlite_c installation.  For other Mac Os, you have to build it from source using bazel using below command.  `bazel build config=monolithic c opt tensorflow/lite/c:libtensorflowlite_c.dylib ` Ref  1, 2. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
948,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tflite_runtime.interpreter.Interpreter causes segfault when using delegate)ï¼Œ å†…å®¹æ˜¯ (I am trying to run label_image.py using tfliteruntime. It works fine without any delegate. However, when I use a delegate, `tflite_runtime.interpreter.Interpreter()` causes a segfault (`tflite_runtime.interpreter.load_delegate()` doesn't cause any problem). I have tried it for the tflite gpu delegate and armnn delegate (for all combinations of backends) with multiple models from the tflite examples and it fails for all of them. The output......   I am using an Odroid XU4 board (armv7l) that has a Mali T628 GPU and 2GB RAM running Ubuntu Mate 22.04. I am running the latest version of tfliteruntime, however it fails for all the previous versions as well. Python version is 3.7.14 .)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vn218,tflite_runtime.interpreter.Interpreter causes segfault when using delegate,"I am trying to run label_image.py using tfliteruntime. It works fine without any delegate. However, when I use a delegate, `tflite_runtime.interpreter.Interpreter()` causes a segfault (`tflite_runtime.interpreter.load_delegate()` doesn't cause any problem). I have tried it for the tflite gpu delegate and armnn delegate (for all combinations of backends) with multiple models from the tflite examples and it fails for all of them. The output......   I am using an Odroid XU4 board (armv7l) that has a Mali T628 GPU and 2GB RAM running Ubuntu Mate 22.04. I am running the latest version of tfliteruntime, however it fails for all the previous versions as well. Python version is 3.7.14 .",2022-10-23T15:44:17Z,stat:awaiting response type:support stale comp:lite comp:micro TFLiteGpuDelegate comp:lite-examples,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58267,Hi  ! Could you share the build command used for arm too.  CMake procedure is recommended to build gpu delegate and benchmark the models for ARM devices. As Bazel method is not yet enabled for build gpu delegates for ARM devices. Ref Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1875,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(pip install tflite-runtime fails on Fedora 36, possibly due to Python 3.10)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Linux Fedora 36  Mobile device _No response_  Python version 3.10.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  NB that `python version` on this Fedora Fedora 36 (Workstation) says `Python 3.10.7`... ... and a friend of mine pointed out that this could well be because on https://pypi.org/project/tfliteruntime/ we can see Python 3.7, 3.8, 3.9  but not 3.10, yet. (Whereas e.g. https://pypi.org/project/numpy/ already has 3.10 and 3.11, and I CAN install that.) Perhaps you would like to consider relaxing those version constraints, if that's possible? Or publish tflite for more recent Python versions already as well? Please let me know if this is easy enough to contribute a PR for an test  I'm happy to help! (Full disclosure: I'm a Googler  at work, playing with this on the weekend at home on a personal computer.) In the mean time, I'll try to work around this with https://www.tensorflow.org/lite/guide/build_cmake_pip, or perhaps just use full Tensorflow instead of Lite and learn ;) how to adapt projectkeywordspotter... Background: I'm a new and noob user and wanted to figure out how to run e.g. https://github.com/googlecoral/projectkeywordspotter/ on my workstation instead of on my Coral Dev board. https://www.tensorflow.org/lite/guide/python says that I should do `python3 m pip install tfliteruntime` but this fails on Fedora 36 (Workstation), as descri)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vorburger,"pip install tflite-runtime fails on Fedora 36, possibly due to Python 3.10","Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Linux Fedora 36  Mobile device _No response_  Python version 3.10.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  NB that `python version` on this Fedora Fedora 36 (Workstation) says `Python 3.10.7`... ... and a friend of mine pointed out that this could well be because on https://pypi.org/project/tfliteruntime/ we can see Python 3.7, 3.8, 3.9  but not 3.10, yet. (Whereas e.g. https://pypi.org/project/numpy/ already has 3.10 and 3.11, and I CAN install that.) Perhaps you would like to consider relaxing those version constraints, if that's possible? Or publish tflite for more recent Python versions already as well? Please let me know if this is easy enough to contribute a PR for an test  I'm happy to help! (Full disclosure: I'm a Googler  at work, playing with this on the weekend at home on a personal computer.) In the mean time, I'll try to work around this with https://www.tensorflow.org/lite/guide/build_cmake_pip, or perhaps just use full Tensorflow instead of Lite and learn ;) how to adapt projectkeywordspotter... Background: I'm a new and noob user and wanted to figure out how to run e.g. https://github.com/googlecoral/projectkeywordspotter/ on my workstation instead of on my Coral Dev board. https://www.tensorflow.org/lite/guide/python says that I should do `python3 m pip install tfliteruntime` but this fails on Fedora 36 (Workstation), as descri",2022-10-22T16:55:42Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,1,20,https://github.com/tensorflow/tensorflow/issues/58264,"BTW https://pypi.org/project/tensorflow/ has _Python :: 3.10_  perhaps it would be sensible to publish both `tensorflow` and `tfliteruntime` for the same Python versions on PyPI? I'm happy to try to propose a PR doing that, if you want to point me to where this is specified. PS: It would be neat if `pip install` would be more helpful here... see https://github.com/pypa/pip/issues/7565, https://github.com/pypa/pip/issues/8831, https://github.com/pypa/pip/issues/4228, https://github.com/pypa/pip/issues/6526. > In the mean time, I'll try to work around this with https://www.tensorflow.org/lite/guide/build_cmake_pip FTR: That seems to require more RAM than I have, and at least on a 20 GB laptop kicks in the OOM killer ğŸ˜¿ ",This is essentially a duplicate of:  https://github.com/tensorflow/tensorflow/issues/56137issuecomment1156723441  mentioned he could build & upload 3.10 wheels for `tfliteruntime`:  https://github.com/tensorflow/tensorflow/issues/56137issuecomment1156729053 But unfortunately it has been several months and the latest `tfliteruntimenightly` still only has Python 3.73.9 wheels.,"TFLite team and TF team have different release processes, although the code resides in the same repository.", Would you happen to know where the build automation for `tfliteruntime` is if community members wanted to like to try to help update it to build 3.10 wheels?,I'd triage this to  or someone else from TFLite team. I left TF in June and only do OSS driveby work now,Left a comment on https://github.com/tensorflow/tensorflow/issues/56137issuecomment1289275446 We need a community contribution to fix the issue.,"You can close this issue and track the progress in the ongoing thread in the issue here https://github.com/tensorflow/tensorflow/issues/56137, , unless if you like to convert this to a feature request as mentioned below. > BTW https://pypi.org/project/tensorflow/ has _Python :: 3.10_  perhaps it would be sensible to publish both `tensorflow` and `tfliteruntime` for the same Python versions on PyPI? I'm happy to try to propose a PR doing that, if you want to point me to where this is specified.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Bump. This issue still needs resolved., is there any update on this issue? Or is there another way to avoid this blocking users on with python 3.10 (arm64 Ubuntu 22.04)?,"  See   https://github.com/tensorflow/tensorflow/issues/56137issuecomment1342705829 We're basically blocked on either DeadSnakes Ubuntu 18.04 3.10 `armhf` builds being fixed, which some of us in the community tried to help with but we're unsuccessful, OR bumping the Docker image builds for tfliteruntime up to Ubuntu `20.04`. As a community member, any help with these would be greatly appreciated as this is a big blocker for many of us.",If you are stuck in my boat here is a solution that I created: https://github.com/tensorflow/tensorflow/issues/56137issuecomment1367563496 might work for some of you.,Some notes on a summary of the blockers here  https://github.com/tensorflow/tensorflow/issues/56137issuecomment1397351028,Should be fixed by   CC(tfliteruntime wheels for Python 3.10),Hi   The tfliteruntime wheels for 3.10 have now been merged with the PR CC(tfliteruntime wheels for Python 3.10)  https://github.com/tensorflow/tensorflow/blob/012530303cf66c15a7dc431d62e78e28a66858cc/tensorflow/lite/tools/pip_package/MakefileL16L18 https://github.com/tensorflow/tensorflow/blob/012530303cf66c15a7dc431d62e78e28a66858cc/tensorflow/lite/tools/pip_package/setup_with_binary.pyL53 Thanks., I see that a new stable release of `tfliteruntime` was made `2.12.0`  https://pypi.org/project/tfliteruntime/2.12.0/files But this one does not include the 3.10 wheels (`cp310`) that have been in `tfliteruntimenightly` for a long time  https://pypi.org/project/tfliteruntimenightly/2.14.0.dev20230510/files Did something go wrong with the stable release? CC  ,For the changes needed to support Python 3.10 wasn't included in r2.12 branch. The Python 3.10  package will be available from r2.13 releases.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
686,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(No graph trace in tensorboard when using @tf.function(jit_compile=True))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yz-chen18,No graph trace in tensorboard when using @tf.function(jit_compile=True),Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-10-22T15:23:24Z,comp:tensorboard stat:awaiting response type:bug stale TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58263,I am also tracking the same at https://github.com/tensorflow/profiler/issues/503 But the ownership of this issue is still not clear. /,"chen18, As mentioned above, Could you please track the issue https://github.com/tensorflow/profiler/issues/503. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1607,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failing to fit an lstm model.fit(x=g,) with generator `g`, custom loss function and `run_eagerly` is False)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.10.0rc36g359c3cdfc5f 2.10.0  Custom Code Yes  OS Platform and Distribution Linux xxxxxx 5.10.019amd64   SMP Debian 5.10.1491 (20221017) x86_64 GNU/Linux  Mobile device _No response_  Python version python  3.9.13   h9a8a25e_0_cpython    condaforge  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit  11.2.2  hbe64b41_10  condaforge,  cudnn  8.1.0.77  h90431f1_0  condaforge  GPU model and memory Quadro RTX 4000, 8GB  Current Behaviour? I was trying to rerun some earlier work, that used to work on TF 2.6.0 I think (not sure).  In a nutshell, extracted for a repro gist thereafter:   if x is a `` and `run_eagerly` is `False`, it seems that invalid ground truth observations `y_true` is passed to the custom loss function, which reports:  Model fitting works however if `run_eagerly` is True, or if x is passed a dataset created with `tf.data.Dataset.from_generator`. Note that I run the jupyter python kernel launched via `optirun` to access the GPU, in case this has relevance. The bug anyway occurs also if forcing the execution on the CPU.  Standalone code to reproduce the issue A selfcontained notebook (python file obtained with jupytext) is available at this gist  Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jmp75,"Failing to fit an lstm model.fit(x=g,) with generator `g`, custom loss function and `run_eagerly` is False","Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.10.0rc36g359c3cdfc5f 2.10.0  Custom Code Yes  OS Platform and Distribution Linux xxxxxx 5.10.019amd64   SMP Debian 5.10.1491 (20221017) x86_64 GNU/Linux  Mobile device _No response_  Python version python  3.9.13   h9a8a25e_0_cpython    condaforge  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit  11.2.2  hbe64b41_10  condaforge,  cudnn  8.1.0.77  h90431f1_0  condaforge  GPU model and memory Quadro RTX 4000, 8GB  Current Behaviour? I was trying to rerun some earlier work, that used to work on TF 2.6.0 I think (not sure).  In a nutshell, extracted for a repro gist thereafter:   if x is a `` and `run_eagerly` is `False`, it seems that invalid ground truth observations `y_true` is passed to the custom loss function, which reports:  Model fitting works however if `run_eagerly` is True, or if x is passed a dataset created with `tf.data.Dataset.from_generator`. Note that I run the jupyter python kernel launched via `optirun` to access the GPU, in case this has relevance. The bug anyway occurs also if forcing the execution on the CPU.  Standalone code to reproduce the issue A selfcontained notebook (python file obtained with jupytext) is available at this gist  Relevant log output  ",2022-10-22T06:37:58Z,stat:awaiting response type:bug comp:autograph TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58258,", To work in Graph mode, Please remove or comment out the raise **ValueError()** and code works without any issue.  Under the  Graph mode TF automatically creates placeholders for the None shapes and it is an intended behaviour.  Please refer to attached gist here.  Thank you!","Thank you for this, I'll study this and communicate to colleagues. ",Are you satisfied with the resolution of your issue? Yes No
1344,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Lite conversion output vs. Tensorflow output)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04.4 LTS (GNU/Linux 5.4.0128generic x86_64)  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1  2. Code Provide code to help us reproduce your issues using one of the following options:  Option B: Paste your code here or provide a link to a custom endtoend colab    input_msg.csv  3. Failure after conversion genMat.txt If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results vs. the Tensorflow model. I have attached the pkl file as a text file which gets loaded and then the input csv gets loaded (input_msg.csv). The output of the TF invocation if correct and printed out ([0. 1. 0. ... 1. 0. 1.]). The TFLite invocation output is printed and is incorrect ([128  127  127 ...  48  48  127]). Not sure if this is a quantization or scaling issue? The inputs are just 0 or 1, so the representative data set is just random 0 and 1's.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gsirocco,Tensorflow Lite conversion output vs. Tensorflow output," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04.4 LTS (GNU/Linux 5.4.0128generic x86_64)  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1  2. Code Provide code to help us reproduce your issues using one of the following options:  Option B: Paste your code here or provide a link to a custom endtoend colab    input_msg.csv  3. Failure after conversion genMat.txt If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results vs. the Tensorflow model. I have attached the pkl file as a text file which gets loaded and then the input csv gets loaded (input_msg.csv). The output of the TF invocation if correct and printed out ([0. 1. 0. ... 1. 0. 1.]). The TFLite invocation output is printed and is incorrect ([128  127  127 ...  48  48  127]). Not sure if this is a quantization or scaling issue? The inputs are just 0 or 1, so the representative data set is just random 0 and 1's.",2022-10-21T01:20:53Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.9,closed,0,76,https://github.com/tensorflow/tensorflow/issues/58206,"Hi  ! The genMat model, you shared in the template  is in  .txt format. I cannot load the model after changing the extension from .txt to .pkl . Would it be possible to share the sample model file again. Thank you! ","How it doesnâ€™t accept pkl files. Maybe as a bin? On Thursday, October 20, 2022, Manas Mohanty ***@***.***> wrote: > Hi   ! > The genMat model, you shared in the template is in .txt format. I cannot > load the model after changing the extension from .txt to .pkl . Would it be > possible to share the sample model file again. > > Thank you! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","genMat.zip ok, I've zipped the genMat file. thanks!","Hi, just checking if you were able to access the genMat.pkl file successfully. Thanks!","Hi  ! Yeah ! I was able to replicate this issue in 2.8, 2.9 and nightly version with the model file link you shared. Trying  to find the  scaling issue using tfl.quantization_debugger . Thank you!", ! Could you normalize your model using metadata writer and let us know the results. Thank you! ,"I'm not sure what you mean by ""normalize"" the model? Thanks","Hi, are you just asking for us to add meta data like comments about what the model does and how it works, inputs and outputs? Thanks!"," ! Exactly, I was asking to add metadata like mean (127.5) , std (127.5) and label.txt (was not sure on this )with original lite model and save it again.  But it might be a scaling issue too which can be found using quantization_debugger.  ! Could you look at this issue. Thank you!","Hi, just some feedback regarding parameters like mean and std. This model is different and simple without any training process. I assume you would need the mean, std, and labels to debug in a way for a more typical trained model?","The inputs and outputs of the model are strictly 0 and 1 values only (binary). Hope this helps clarify. Thanks! We look forward to any help why the TFLite model is not working like the TF model, if it's some scaling or quantization issue. Please let me know if you require any more input from me in order to fix this.","Hi  and , just checking if there's any update on this issue? I'm trying to get this working for our product that utilizes an edge TPU, so before proper operation on the TPU is achieved, the TFLite model functionality would need to be correct. Is there a way for me to fix or debug this issue? Thanks, I appreciate any help and input on this issue as we are stuck right now.","Hi   , I'm just checking if you are able to replicate the issue and see a possible workaround or fix? I am ultimately trying to run on edge TPU, but if the TFLite model is not correct then the edge TPU will not be correct. Please let me know if it will be feasible to get this working, or if other alternatives need to be considered. Thanks!!","Hi   , I'm sorry to keep asking but I'm just looking for some feedback if there is an estimate of when and if this could be fixed please? We are at the stage in our project where a functionally working model is needed for overall operation. Please let me know so we can plan one way or another. Thank you!!","Hi   , just checking for an update. Please let me know of a status!!!","Hi   , checking for any progress on this issue? Thanks!",  you've changed output type to int8 so it has int8 outputs [128 127 127 ... 48 48 127] instead of float outputs [0. 1. 0. ... 1. 0. 1.]. Could you share what wrong with this?,"Even though the TF model has float, the input and output of the model is just 0s and 1s. The output of the TF invocation is correct and printed out ([0. 1. 0. ... 1. 0. 1.]). The TFLite invocation output is printed and is incorrect and depends on the representative dataset, which is just a series of random 0's and 1's. Depending on the representative dataset random sequence of 0's and 1's the output will change. The output should just be the same sequence of 0's and 1's as the output of the TF invocation. Thanks!! Please let me know if this makes sense or not.",I see. One more to confirm. Is this only happening with quantization? Does float model have the similar issue?,"I have not tried float, since in the end we need int because we want to convert the tflite model to run on the edge tpu which requires int. Thanks!","I've tested and float model works well. FYI, this is model dumps with `tf.lite.experimental.Analyzer.analyze(model_content=tflite_quant_model)` float model  int8 model  I think it's related with quantization spec from representative_dataset. Since the input range is known, it might be better to provide it manually.  any idea on this?","Thanks, I just confirmed that float works as well. How would the input range be provided manually? The range is just 0 to 1 discreet (only 0 and 1 not in between). Thanks!!","Hi, checking if there's anything that can be tried? I'm not sure how to provide the input range manually. Thanks!","Hi, just checking if there is a workaround or a fix or some other input that can be tried to convert the TF model to int or uint? Thanks!","Hi, I was wondering if there's another avenue that I should pursue to get assistance with this issue? It is a TFLite converter issue correct? Thanks","Hi, I'm checking if there's a way to fix this issue or to go through some other source? Or some other information needs to be provided? Thanks","Hi, is the best approach to debug this to go through the tflite converter process or to go through the invoke and see what the issue is?","Hi   , just checking if there's any update on what can be done for this issue. Thanks!!","Hi, we really want to productize this utilizing a TPU but if we can't find a way to do this then we'll have to try to find alternatives. Please let us know what can be done to get this working. Thanks and happy holidays!!","Hi, we are really stuck with this issue. We can only utilize the Edge TPU for this application, regular Tensorflow or TFLite won't provide the performance that is needed for this application, so unless we can convert to using int's in TFLite models properly then the TPU can't be utilized. Thank you to anyone who can help"
466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([oneDNN] Quantized batch-matmul and fusions)ï¼Œ å†…å®¹æ˜¯ (This PR enables support for quantized batched matrices in the BatchMatMul op. A few binary ops can also be fused to it. Transformers based models frequently uses it and the fusions (e.g., BatchMatMul + Mul + Add) in their selfattention.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,mdfaijul,[oneDNN] Quantized batch-matmul and fusions,"This PR enables support for quantized batched matrices in the BatchMatMul op. A few binary ops can also be fused to it. Transformers based models frequently uses it and the fusions (e.g., BatchMatMul + Mul + Add) in their selfattention.",2022-10-20T21:55:27Z,awaiting review ready to pull comp:mkl size:L,closed,0,25,https://github.com/tensorflow/tensorflow/issues/58204,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please rebase your branch and resolve conflicts? Thank you!,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,  This PR has been out of sync for long time. I am working on this PR again. I have temporarily changed the status to draft and will make it ready for review shortly.,  I have addressed review comments. Thank you very much for the suggestions.,This PR also caused Windows test failures on Broadwell. (Python 3.9) ,  I have disabled the quantized tests on Windows. Let me know if it is okay., I have restricted the test to run on CPUs that has support for VNNI/AMX INT8. Please let me know if the PR still has more issues. Thanks!,This is causing many unit test failures on AARCH64. See https://github.com/tensorflow/tensorflow/actions/runs/7791926554/job/21248977277,> This is causing many unit test failures on AARCH64. See https://github.com/tensorflow/tensorflow/actions/runs/7791926554/job/21248977277  Thanks for reporting the issue. I am not familiar with AARCH64 code. Could anybody (familiar with AARCH64) help fixing the issue.,,+,Note: we had to roll this back due to the failures on ARM., Could you please help fixing the failure?,"Hi ,  as  identified the failure comes in `Setup()` method in `MklMatMulPrimtive` class when you are trying to set `b_mem` in `context_` by reading weight description from `prim_desc` that still wasn't identified. We do it this way on AArch64 so that we can set weights as `any` as we need to reorder them in order to select the most optimal kernel from Arm Compute Library (more information on this approach are available in this PR: https://github.com/tensorflow/tensorflow/pull/61171). To fix the failure you would need to set `b_mem` later when `prime_desc` is actually created (between lines 1052 and 1059 in your PR).  One way how to do it is with this patch that  tested and those failing tests pass now on AArch64.  Hope this helps. Best, Milos.", Thanks! I will take a look.
1034,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite How to build NNAPI delegate for Android?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Android  Mobile device N/A  Python version N/A  Bazel version 5.1.1  GCC/Compiler version Android NDK 21  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  I did `nm D libtensorflowlite.so | grep Nnapi` to see if perhaps the symbols for NNAPI delegated got baked into libtensorflowlite.so. But that does not appear to be the case. Am I building NNAPI delegate incorrectly? shell bazel build c opt config android_arm64 config=monolithic copt DCL_DELEGATE_NO_GL strip=never //tensorflow/lite:libtensorflowlite.so //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so //tensorflow/lite/delegates/nnapi:nnapi_delegate shell N/A ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DwayneDuane,TensorFlow Lite How to build NNAPI delegate for Android?,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Android  Mobile device N/A  Python version N/A  Bazel version 5.1.1  GCC/Compiler version Android NDK 21  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  I did `nm D libtensorflowlite.so | grep Nnapi` to see if perhaps the symbols for NNAPI delegated got baked into libtensorflowlite.so. But that does not appear to be the case. Am I building NNAPI delegate incorrectly? shell bazel build c opt config android_arm64 config=monolithic copt DCL_DELEGATE_NO_GL strip=never //tensorflow/lite:libtensorflowlite.so //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so //tensorflow/lite/delegates/nnapi:nnapi_delegate shell N/A ``` ,2022-10-20T17:12:18Z,stat:awaiting response type:build/install stale subtype:bazel TFLiteNNAPIDelegate TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58192,Hi ! Sorry for the late response.  Could you skip the other delegate and only use one flag while building Tensorflow lite run time with below command. Please make sure you have set path for NDK and SDK tool chain and selected android build while running configure.py process( after cloning and checking out master branch). `bazel build c opt config android_arm64 config=monolithic  define tflite_with_nnapi=true strip=never //tensorflow/lite/c:libtensorflowlite_c.so` Thank you!," What you suggested works and allows me to utilise the NNAPI delegate. However, in my use case, I also need to be able to use the GPU delegate. Right now, I am building `libtensorflowlite_gpu_delegate.so` separately, but is there a build flag that allows the GPU delegate to be included within `libtensorflowlite_c.so`? I tried `define tflite_with_gpu=true` and `define tflite_with_opencl=true`, but these flags don't seem to work. "," ! I am not sure on baking all the delegates in to tfliteruntime simultaneously. So, My thought was to build them individually and  create a policy to switch to respective delegate when it does not find any supported ops during inference. But that has not been recommended earlier to avoid any runtime issues. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1841,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Mobile SSD model generates only 1 output)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  Used Colab  Used Jupyter under Ubuntu 22.04 to emulate same exercise (_i.e._ build ._tflite_ and ._txt_ files) Running original Tensorflow Lite tutorial on **flowers** image classification with only one change in parameter for _model_spec_ as noted below.  2. Code Total newbie here, sorry!  Performed Android Studio development for the classification tutorial on flowers.  Deployed to Pixel 2 XL  The app runs fine  Ran the flowers tutorial in Colab to generate the ._tflite_ and ._txt_ files.  Deployed to another Android device (?) where the error message is: _Mobile SSD models are expected to have exactly 4 outputs, found 1_ Don't have access to the source code for the another Android app. However, this app reads the COCO ._tflite_ model without errors. So the issue lies in my understanding of how I should specify the parameters to generate the .tflite model file in the expected format.  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference Image Classifier: Sample runs fine when built under Dolphin 2021.3.1 and ported to Pixel 2 XL.  Option B: Paste your code here or provide a link to a custom endtoend colab How to change the following statement to meet the requirement: _Mobile SSD models are expected to have exactly 4 outputs, found 1_  Don't know how what parameter to set to obtain a ._tflite_ model file that will meet the requirement. Did a cursory search for links to the documentation for **image_classifier.create** method but couldn't find anything owing to my inexperience.  3. Failure a)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,baqwas,Mobile SSD model generates only 1 output," 1. System information  Used Colab  Used Jupyter under Ubuntu 22.04 to emulate same exercise (_i.e._ build ._tflite_ and ._txt_ files) Running original Tensorflow Lite tutorial on **flowers** image classification with only one change in parameter for _model_spec_ as noted below.  2. Code Total newbie here, sorry!  Performed Android Studio development for the classification tutorial on flowers.  Deployed to Pixel 2 XL  The app runs fine  Ran the flowers tutorial in Colab to generate the ._tflite_ and ._txt_ files.  Deployed to another Android device (?) where the error message is: _Mobile SSD models are expected to have exactly 4 outputs, found 1_ Don't have access to the source code for the another Android app. However, this app reads the COCO ._tflite_ model without errors. So the issue lies in my understanding of how I should specify the parameters to generate the .tflite model file in the expected format.  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference Image Classifier: Sample runs fine when built under Dolphin 2021.3.1 and ported to Pixel 2 XL.  Option B: Paste your code here or provide a link to a custom endtoend colab How to change the following statement to meet the requirement: _Mobile SSD models are expected to have exactly 4 outputs, found 1_  Don't know how what parameter to set to obtain a ._tflite_ model file that will meet the requirement. Did a cursory search for links to the documentation for **image_classifier.create** method but couldn't find anything owing to my inexperience.  3. Failure a",2022-10-20T14:53:59Z,stat:awaiting response type:bug comp:lite comp:lite-examples TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58189,Hi  ! Sorry for the late response. Could you use metadata writer to normalize the ssd model then port to your android application. Reference. Thank you!,  Thank you very much. I will try your suggestion. Kind regards.,Are you satisfied with the resolution of your issue? Yes No
523,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Removed merge conflict marker)ï¼Œ å†…å®¹æ˜¯ (Hi! I was trying to compile r2.9 from source and the compiler found this merge conflict marker still in `execute.cc` which was causing compilation to fail. As far as I can see the other merge conflict markers had been removed but this one was still there. Any questions or comments please let me know! Jamie)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JamieLine,Removed merge conflict marker,Hi! I was trying to compile r2.9 from source and the compiler found this merge conflict marker still in `execute.cc` which was causing compilation to fail. As far as I can see the other merge conflict markers had been removed but this one was still there. Any questions or comments please let me know! Jamie,2022-10-20T13:39:54Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/58187,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
778,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Trying to save a model compiled with tfa.MultiOptimizer | gives error saying `TypeError: ('Not JSON Serializable:', ...`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.6.4  Custom Code Yes  OS Platform and Distribution Kaggle kernel  Mobile device _No response_  Python version 3.7.12  Bazel version _No response_  GCC/Compiler version GCC 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  It simply can't save the model when there are multiple optimizers.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,maifeeulasad,"Trying to save a model compiled with tfa.MultiOptimizer | gives error saying `TypeError: ('Not JSON Serializable:', ...`",Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.6.4  Custom Code Yes  OS Platform and Distribution Kaggle kernel  Mobile device _No response_  Python version 3.7.12  Bazel version _No response_  GCC/Compiler version GCC 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  It simply can't save the model when there are multiple optimizers.  Standalone code to reproduce the issue   Relevant log output  ,2022-10-20T10:30:13Z,stat:awaiting response type:bug comp:ops 2.6.0,closed,0,2,https://github.com/tensorflow/tensorflow/issues/58184,", We can see the similar issue was tracking here. Could you please close this issue, since it is already being tracked there? Thank you!",Are you satisfied with the resolution of your issue? Yes No
471,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix duplicated dylibs in Mac pip wheel)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Significant increase in the size of macOS wheel) On Mac, copying the runfiles with `L` duplicates the symlinked dylibs. On Linux, the duplicate copies are removed during the wheel build, so this PR does the same for Mac. Before  After   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,trevor-m,Fix duplicated dylibs in Mac pip wheel,"Fixes CC(Significant increase in the size of macOS wheel) On Mac, copying the runfiles with `L` duplicates the symlinked dylibs. On Linux, the duplicate copies are removed during the wheel build, so this PR does the same for Mac. Before  After   ",2022-10-19T22:29:57Z,ready to pull size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/58174
1148,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(IOU3d and IOU3d pairwise)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version master  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? We are going to introduce c++ custom ops for pairwise IOU3D in Kerascv with: https://github.com/kerasteam/kerascv/pull/890 Analyzing also what we have in TF 3d we are still relying on numpy helper routines Same thing for Pytorch 3d that has c++/CUDA cusotm op I want to try to understand what kind of routines we could contribute here to support these new ops and to analyze if there is any limit, down to the stack, in the case we need to implement an HLO kernel to cover the XLA jit compilation.  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bhack,IOU3d and IOU3d pairwise,"Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version master  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? We are going to introduce c++ custom ops for pairwise IOU3D in Kerascv with: https://github.com/kerasteam/kerascv/pull/890 Analyzing also what we have in TF 3d we are still relying on numpy helper routines Same thing for Pytorch 3d that has c++/CUDA cusotm op I want to try to understand what kind of routines we could contribute here to support these new ops and to analyze if there is any limit, down to the stack, in the case we need to implement an HLO kernel to cover the XLA jit compilation.  Standalone code to reproduce the issue   Relevant log output _No response_",2022-10-19T14:27:12Z,stat:awaiting response type:feature comp:core,closed,0,2,https://github.com/tensorflow/tensorflow/issues/58166,//apiowners ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The related PR's which were raised have been merged. Please take a look at the referenced PR's. https://github.com/kerasteam/kerascv/pull/890 https://github.com/kerasteam/kerascv/pull/1292 The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space."
1724,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Linking error when using tflite::StatefulNnApiDelegate )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Android Studio for building  Mobile device Android ARM64  Bazel version 5.1.1  Current Behaviour? Hi! I am trying to add a delegate to the interpreter from the C++ native side, but am unable to figure out how to make it work. Any help would be greatly appreciated! I can successfully build the model, interpreter and tensors needed for inference, but attempting to initialize `tflite::StatefulNnApiDelegate` leads to a linking error when trying to build the apk  What confuses me is that all other tflite functions and objects I needed have been found, I can even declare `tflite::StatefulNnApiDelegate::Options` without issue. 1. Am I doing something wrong in the code, from the examples I could find, and looking at the jni code for the java side I believe the code provided should be correct. Could I be missing something completely, like querying for the Nnapi or setting something in the options before creating the delegate? 2. Could it be an issue happening while building `libtensorflowlite.so`? I followed the recipe to build it with bazel for ARM chips Build TensorFlow Lite for ARM boards with the command:  and also tried   to no avail... I also included the `CMakeLists.txt` to make sure I am not doing something wrong there. Thank you in advance!  Standalone code to reproduce the issue Failing code.  CMakeLists.txt )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,NicoZobernig,Linking error when using tflite::StatefulNnApiDelegate ,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Android Studio for building  Mobile device Android ARM64  Bazel version 5.1.1  Current Behaviour? Hi! I am trying to add a delegate to the interpreter from the C++ native side, but am unable to figure out how to make it work. Any help would be greatly appreciated! I can successfully build the model, interpreter and tensors needed for inference, but attempting to initialize `tflite::StatefulNnApiDelegate` leads to a linking error when trying to build the apk  What confuses me is that all other tflite functions and objects I needed have been found, I can even declare `tflite::StatefulNnApiDelegate::Options` without issue. 1. Am I doing something wrong in the code, from the examples I could find, and looking at the jni code for the java side I believe the code provided should be correct. Could I be missing something completely, like querying for the Nnapi or setting something in the options before creating the delegate? 2. Could it be an issue happening while building `libtensorflowlite.so`? I followed the recipe to build it with bazel for ARM chips Build TensorFlow Lite for ARM boards with the command:  and also tried   to no avail... I also included the `CMakeLists.txt` to make sure I am not doing something wrong there. Thank you in advance!  Standalone code to reproduce the issue Failing code.  CMakeLists.txt ",2022-10-19T14:13:56Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/58165,Hi  ! Sorry for the late respone. I see you are using CMake 3.4.1  version .  Can you check with CMake 3.14 or higher and post the LD error too. Thank you!,"Hi  ! Thank you for looking into this. I am actually using CMake 3.18.1, setting it in the `build.gradle` file. The minimum version is just a remains from androidstudio's template as far as I remember... So the LD error is the same. I also tried linking `neuralnetworks` in `target_link_libraries` to see if that would change anything:  It didn't... I can make it work is by using the C API instead after getting `libtensorflowlite_jni.so` from the packaged `.aar` file provided here. So my assumption is that the building procedure of the C++ API library is at fault. I cannot say whether the build files have an issue or if I did something wrong there.", !  Thanks for sharing your observations.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
613,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow TF2.10 C++ API Issue)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version TF2.10.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version 5.3.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Alok-Ranjan23,Tensorflow TF2.10 C++ API Issue,Click to expand!    Issue Type Support  Source source  Tensorflow Version TF2.10.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version 5.3.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-19T13:18:29Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux subtype:bazel TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58163,"Hi Ranjan23 ! Just quick observation before going deep dive.  You are running the program with C++14, Tensorflow build are based on C++17 support now( Recently upgraded). Can you check the output against C++17 compilers and let us know. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Crash due to illegal CUDA memory access in BiasAddGrad)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9 and 2.12.0dev20221018  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.5  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shijy16,Crash due to illegal CUDA memory access in BiasAddGrad,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9 and 2.12.0dev20221018  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.5  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-19T11:41:04Z,stat:awaiting response type:bug stale comp:ops TF 2.11,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58162,  I was able to reproduce the issue on Colab using TF v2.11.Please find the gist here for reference. Thank you!,"tf.raw_ops are basically designed to be consumed by downstream library users, not end users. Usually, these APIs are less strict in terms of validation etc., which is fine since mainly library writers are supposed to use these symbols. If available, please prefer high level API for general use case scenarios. Refer this RFC for more details. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1869,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(LeakyRelu in Tensorflow lite with the Hexagon Delegate not supported)ï¼Œ å†…å®¹æ˜¯ (When using a tflite model (8bits quantized via TensorFlow lite conversion framework) that includes the activation function ""LeakyRelu"", the Hexagon delegate from tensorflow framework cannot perform the DNN inference on the whole graph, but rather it falls back to the CPU/XNNPack delegate. This is due to the fact that 'LeakyRelu' operation is not supported by the Hexagon Delegate (confirmed in TensorFlow doc: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md). When using Relu activation function (and Relu6 as well), we can see below that the TF Hexagon Delegate can process the whole DNN graph, unfortunately, the qualitative results I get are much worse, hence the need of having 'Leaky Relu' implemented in the Hexagon Delegate. We can easily reproduce this behavior by using TensorFlow Benchmark tool (see below) Could we consider to implement Leaky Relu in tensorflow lite DSP delegate ? **System information**  OS Platform and Distribution): Android 10, NDK R21e  TensorFlow installed from (source or binary): from source using the Release tag '2.9.1'  TensorFlow version (or github SHA if from source):  2.9.1 **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  Here is a colab link with the code used to:  generate the model that include leaky relu op.  quantize and save the model in int8 ops. Colab code Link: https://colab.research.google.com/drive/1xmktQACGQ6GnMmrwco3bNXcmJIDjt5PK?usp=sharing **Link to the quantized model:** my_quant_model_leaky_relu.zip The tool below from Tensorflow allow to run any model)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,eelcoder,LeakyRelu in Tensorflow lite with the Hexagon Delegate not supported,"When using a tflite model (8bits quantized via TensorFlow lite conversion framework) that includes the activation function ""LeakyRelu"", the Hexagon delegate from tensorflow framework cannot perform the DNN inference on the whole graph, but rather it falls back to the CPU/XNNPack delegate. This is due to the fact that 'LeakyRelu' operation is not supported by the Hexagon Delegate (confirmed in TensorFlow doc: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md). When using Relu activation function (and Relu6 as well), we can see below that the TF Hexagon Delegate can process the whole DNN graph, unfortunately, the qualitative results I get are much worse, hence the need of having 'Leaky Relu' implemented in the Hexagon Delegate. We can easily reproduce this behavior by using TensorFlow Benchmark tool (see below) Could we consider to implement Leaky Relu in tensorflow lite DSP delegate ? **System information**  OS Platform and Distribution): Android 10, NDK R21e  TensorFlow installed from (source or binary): from source using the Release tag '2.9.1'  TensorFlow version (or github SHA if from source):  2.9.1 **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  Here is a colab link with the code used to:  generate the model that include leaky relu op.  quantize and save the model in int8 ops. Colab code Link: https://colab.research.google.com/drive/1xmktQACGQ6GnMmrwco3bNXcmJIDjt5PK?usp=sharing **Link to the quantized model:** my_quant_model_leaky_relu.zip The tool below from Tensorflow allow to run any model",2022-10-18T21:17:41Z,stat:awaiting response type:feature stale comp:lite TFLiteHexagonDelegate,closed,0,8,https://github.com/tensorflow/tensorflow/issues/58156, ! Thanks for bringing it up again.    ! Could you look at this feature request. Thank you!,"Hi, Thanks for taking into consideration my request. Any update on your side ?","Hi  , LeakyRelu is not one of the supported ops from Hexagon nnlib (see ops.txt for the list of ops supported natively). One workaround is to decompose the leaky relu into ops that Hexagon nnlib supports, e.g.  Feel free to raise a PR for this, thanks!","Hello  , Thanks for the tip. Initially, I tried to implement it using tensorflow ops in python, but the inference time was very slow (4x slower): !image But here I guess, I need to modify directly the hexagon delegate, which should hopefully be faster: I'd like to try it, but since I'm not very familiar with the hexagon delegate code, could you give me an example on where to add the LeakyRelu support in HexagonDelegate code ? My guess is that I'd need to modify `hexagon/builders/op_builder.cc` and `hexagon/builders/activation_builder.cc`, but not familiar enough with the code to do it properly. Would be really grateful, if you could guide me in this process :) (Otherwise, let me know where to create a PR) Thanks!","Hello, Any feedback on this request ? thanks.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
858,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TF-TRT] Adding `TF_TRT_OP_FAKELIST` env var to allow TF-TRT segmentation experimentation)ï¼Œ å†…å®¹æ˜¯ (This PR introduces the env var `TF_TRT_OP_FAKELIST=OpName1,OpName2,...`. The motivation of this change is to allow to experiment with the impact on graph segmentation of adding converter X, Y or Z. This API is not designed for users, but rather TFTRT team to evaluate the importance / impact of the different fearture requests.  It works by intercepting the call to `OpConverterRegistry::Impl::LookUp` and return the `FakeOp` converter in case the OpName matches any of the names passed in `TF_TRT_OP_FAKELIST`.  This is expected to have zero impact on any of the current workflows.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DEKHTIARJonathan,[TF-TRT] Adding `TF_TRT_OP_FAKELIST` env var to allow TF-TRT segmentation experimentation,"This PR introduces the env var `TF_TRT_OP_FAKELIST=OpName1,OpName2,...`. The motivation of this change is to allow to experiment with the impact on graph segmentation of adding converter X, Y or Z. This API is not designed for users, but rather TFTRT team to evaluate the importance / impact of the different fearture requests.  It works by intercepting the call to `OpConverterRegistry::Impl::LookUp` and return the `FakeOp` converter in case the OpName matches any of the names passed in `TF_TRT_OP_FAKELIST`.  This is expected to have zero impact on any of the current workflows.",2022-10-18T15:17:21Z,size:M comp:gpu:tensorrt,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58144," as mentioned during the meeting, ready for review", Can you please check 's comments and keep us posted ? Thank you!, no more comments. Can you reapprove,Closing in order to transition to Trusted Partner Program. Will reopen.
835,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA compiler error: left_branch_shape.rank() == right_branch_shape.rank())ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit=11.2 cudnn=8.1.0  GPU model and memory GTX 1080 TI  Current Behaviour? XLA compiler error for this model when `jit_compile=True` It's very hard to determine the cause of the error, as the model works fine when running normally.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JustASquid,XLA compiler error: left_branch_shape.rank() == right_branch_shape.rank(),"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit=11.2 cudnn=8.1.0  GPU model and memory GTX 1080 TI  Current Behaviour? XLA compiler error for this model when `jit_compile=True` It's very hard to determine the cause of the error, as the model works fine when running normally.  Standalone code to reproduce the issue   Relevant log output  ",2022-10-18T00:52:08Z,stat:awaiting response type:bug stale comp:xla TF 2.10,closed,0,8,https://github.com/tensorflow/tensorflow/issues/58135,Hi  ! Proper way to use XLA with tf.function is enable >  jit_compile=True  with tf.function. Attached resolved gist for reference. Thank you!," I want to XLA compile the entire model, not just the custom layer. setting `jit_compile=True` on the `.function` while still using `jit_compile=True` on the model compilation results in the same error.", ! You can use XLAAutoclustering if you want to use XLA on entire model. Attached gist for reference. Thank you!,"Thank you   Is there any reason/documentation why `tf.config.optimizer.set_jit(True)` works here, but `jit_compile=True` in `model.compile()` does not?"," ! Attached Relevant documentation of XLA usage are attached here. with tf.function, with Autoclustering. Can we mark this as resolved now. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
683,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AttributeError: module 'tensorflow._api.v2.train' has no attribute 'summary_iterator')ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution window 11 22h  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,khengyun,AttributeError: module 'tensorflow._api.v2.train' has no attribute 'summary_iterator',Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution window 11 22h  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-18T00:18:25Z,type:support comp:apis TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58134,  Try to use 'tf.compat.v1.train.summary_iterator' instead of 'tf.train.summary_iterator'. I was able to run the given code snippet without any error on colab using TF v2.9. Could you please refer to the this documents ( tf.compat & tf.compat.v1.train.summary_iterator ) and find the gist here for reference. Thank you!,Thank you!,Are you satisfied with the resolution of your issue? Yes No,>  Try to use 'tf.compat.v1.train.summary_iterator' instead of 'tf.train.summary_iterator'. I was able to run the given code snippet without any error on colab using TF v2.9. Could you please refer to the this documents ( tf.compat & tf.compat.v1.train.summary_iterator ) and find the gist here for reference. >  > Thank you! Thank you buddy !!
647,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF Lite CMake Failes to Download neon2sse on Windows)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nathanmartz,TF Lite CMake Failes to Download neon2sse on Windows,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-17T18:15:25Z,stat:awaiting response type:build/install comp:lite TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/58126,Hi  !  I just checked with Master branch with CMake GUI (V 3.24.2) on my windows machine. Build was successful with some dev warnings. Could you check against 2.10 or Master branch with Default native  compiler and let us know. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"I'm no longer able to reproduce, so this appears to be a transient issue on google storage apis. Thanks for taking a look.",Are you satisfied with the resolution of your issue? Yes No,"I actually got this error again today, though it's surprisingly hard to repro.",Ok  ! Thanks for the update .  ! Could you look at this issue. Thank you!,"Hi , we believe this is a transitory issue in your environment as we do not encounter it. If you believe it is resolved now please feel free to close the issue.",Seems that way. Closing. ,Are you satisfied with the resolution of your issue? Yes No
1877,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to compile TF 2.10.0 from source on Ubuntu 20.04: symbol lookup error)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version   GPU model and memory 128 GB  Current Behaviour?   Standalone code to reproduce the issue  shell Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=102 INFO: Reading rc options for 'build' from /home/ubuntu/tensorflow2.10.0/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/ubuntu/tensorflow2.10.0/.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rangsimanketkaew,Failed to compile TF 2.10.0 from source on Ubuntu 20.04: symbol lookup error,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version   GPU model and memory 128 GB  Current Behaviour?   Standalone code to reproduce the issue  shell Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=102 INFO: Reading rc options for 'build' from /home/ubuntu/tensorflow2.10.0/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/ubuntu/tensorflow2.10.0/.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/",2022-10-17T15:20:50Z,type:build/install subtype: ubuntu/linux Fixed in Nightly TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58124,Hi  ! Sorry for the late response. Build seems to be passing with Bazel 5.3.0 and master branch. Attached gist for reference. Could you let us know from your side after testing with Bazel 5.3.0 and Master branch. Thank you!,I'm able to compile TF v2.11 (the latest version on the master branch of TF) with bazel 5.3.0. But what is actually a problem with v2.10?, ! You can use git diff (between 2.10 and nightly) to look for the commit which fixed this issue.  Can we consider this as resolved in nightly version then. Thank you!,"Sure, thanks again for your help. Have a nice weekend!",Are you satisfied with the resolution of your issue? Yes No
452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ROCm] Absorb fix for the LLVM AMDGPU backend addition of fp16 support of neâ€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦arbyint. The following LLVM commit adds fp16 support for the nearbyint intrinsic on the AMDGPU backend: https://github.com/llvm/llvmproject/commit/6370bc2435a8406898eee7338ae7d795a252ad04)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rsanthanam-amd,[ROCm] Absorb fix for the LLVM AMDGPU backend addition of fp16 support of neâ€¦,â€¦arbyint. The following LLVM commit adds fp16 support for the nearbyint intrinsic on the AMDGPU backend: https://github.com/llvm/llvmproject/commit/6370bc2435a8406898eee7338ae7d795a252ad04,2022-10-17T13:57:33Z,awaiting review ready to pull comp:gpu size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/58121,/ 
678,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(while processing Pruning by using Densely connected Layer...)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf.2.9.1  Custom Code Yes  OS Platform and Distribution window, jupyter Lab  Mobile device _No response_  Python version .3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Lee-junseok1025,while processing Pruning by using Densely connected Layer...,"Click to expand!    Issue Type Support  Source source  Tensorflow Version tf.2.9.1  Custom Code Yes  OS Platform and Distribution window, jupyter Lab  Mobile device _No response_  Python version .3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-10-17T04:09:52Z,stat:awaiting response type:support stale TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58114,"junseok1025  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here.  Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
671,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How can I make the dataset accept sequences or matrices? PLZ :))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution MACOS  Mobile device _No response_  Python version Python 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MinaAku,How can I make the dataset accept sequences or matrices? PLZ :),Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution MACOS  Mobile device _No response_  Python version Python 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-10-16T12:02:35Z,stat:awaiting response stat:awaiting tensorflower type:support stale comp:data TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/58109,  Could you please refer to this document(tf.data: Build TensorFlow input pipelines) and let us know if it helps. Thank you!,">  > Could you please refer to this document(tf.data: Build TensorFlow input pipelines) and let us know if it helps. > Thank you! Thanks for your reply! But I haven't solved the mass after refer document. (It's my fault T.T) My case is that, I got some image matrix data and save it in a dataframe, how do I pass it into the dataset as a feature? Using ""from_tensor_slices()"" method will report ""ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).""  Thanks everyone for your help!",  Could you please provide a code snippet to replicate the issue reported here. Thank you!,">  > Could you please provide a code snippet to replicate the issue reported here. > Thank you! Thanks for your reply and patience! I've tried many things these days, but nothing works. You are my only hope now. :) First, I am using a processed multimodal dataset with three features and a float label. The three features are text\audio\video, where text is represented by a 104long integer sequence, and audio\video is represented by a float matrix. Second, I have tried using the .astype method to adjust the types of the three feature columns, but failed. I'll show you this dataset using the .head() method. The following is the process before the error is reported: >In   [1]: train_df.dtypes >Out[1]: text      object video     object audio     object label    float32 dtype: object >In   [2]:train_df.head() >Out[2]:                                  text                               video                               audio                                                       label 0  [2, 3, 4, 5, 6, 0, 0, 0, 0, 0, ...  [[1.991939, 1.5613043, 0.2240...  [[1.2337455, 1.2865584, 1.421...   2.40 1  [7, 8, 9, 10, 11, 6, 12, 13, 14...  [[2.3410914, 2.3050823, 1.06210...  [[0.35389948, 0.30429268, 0.11...  0.80 2  [27, 28, 29, 10, 9, 13, 30, 31,...  [[0.8537923, 0.2358623, 0.155...  [[1.169397, 0.746589, 0.290762...  1.00 3  [4, 32, 33, 13, 14, 0, 0, 0, 0,...  [[1.8149365, 0.40606537, 1.4...  [[1.8315508, 0.11712463, 0.72...  1.75 4  [15, 25, 10, 34, 28, 0, 0, 0, 0...  [[1.0904845, 1.146731, 0.00874...  [[0.2464511, 0.37285712, 0.144...   0.00 >In   [3]:train_label = train_df['label']               train_feature = train_df[['text','audio','video']] >Out[3]: >In   [4]: import tensorflow as tf train_dataset = tf.data.Dataset.from_tensor_slices((train_feature, train_label)) Traceback (most recent call last):   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/data/util/structure.py"", line 102, in normalize_element     spec = type_spec_from_value(t, use_fallback=False)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/data/util/structure.py"", line 485, in type_spec_from_value     raise TypeError(""Could not build a `TypeSpec` for {} with type {}"".format( TypeError: Could not build a `TypeSpec` for                                     text                               audio                               video 0     [2, 3, 4, 5, 6, 0, 0, 0, 0, 0, ...  [[1.2337455, 1.2865584, 1.421...  [[1.991939, 1.5613043, 0.2240... 1     [7, 8, 9, 10, 11, 6, 12, 13, 14...  [[0.35389948, 0.30429268, 0.11...  [[2.3410914, 2.3050823, 1.06210... 2     [27, 28, 29, 10, 9, 13, 30, 31,...  [[1.169397, 0.746589, 0.290762...  [[0.8537923, 0.2358623, 0.155... 3     [4, 32, 33, 13, 14, 0, 0, 0, 0,...  [[1.8315508, 0.11712463, 0.72...  [[1.8149365, 0.40606537, 1.4... 4     [15, 25, 10, 34, 28, 0, 0, 0, 0...  [[0.2464511, 0.37285712, 0.144...  [[1.0904845, 1.146731, 0.00874...                                   ...                                 ...                                 ... 1278  [306, 1363, 28, 1945, 4, 681, 3...  [[0.002663397, 1.2971419, 2.5...  [[0.09433063, 0.3597318, 1.3... 1279  [1625, 141, 63, 106, 270, 28, 1...  [[1.2584158, 2.3381844, 1.256...  [[1.5783025, 0.40491882, 0.64... 1280  [2171, 32, 42, 11, 6, 0, 0, 0, ...  [[1.668467, 1.8740685, 1.1285...  [[0.7878416, 1.9826996, 0.0788... 1281  [1769, 141, 462, 1610, 2172, 37...  [[0.10788689, 1.3585961, 2.241...  [[1.7500647, 1.4413257, 1.58184... 1282  [649, 141, 2008, 480, 306, 74, ...  [[0.17980516, 2.145652, 1.055...  [[0.40465018, 0.8532134, 1.254... [1283 rows x 3 columns] with type DataFrame During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 3378, in run_code     exec(code_obj, self.user_global_ns, self.user_ns)   File """", line 3, in      train_dataset = tf.data.Dataset.from_tensor_slices((train_feature, train_label))   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/data/ops/dataset_ops.py"", line 809, in from_tensor_slices     return TensorSliceDataset(tensors, name=name)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/data/ops/dataset_ops.py"", line 4551, in __init__     element = structure.normalize_element(element)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/data/util/structure.py"", line 107, in normalize_element     ops.convert_to_tensor(t, name=""component_%d"" % i))   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/profiler/trace.py"", line 183, in wrapped     return func(*args, **kwargs)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/framework/ops.py"", line 1640, in convert_to_tensor     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/framework/constant_op.py"", line 343, in _constant_tensor_conversion_function     return constant(v, dtype=dtype, name=name)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/framework/constant_op.py"", line 267, in constant     return _constant_impl(value, dtype, shape, name, verify_shape=False,   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/framework/constant_op.py"", line 279, in _constant_impl     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/framework/constant_op.py"", line 304, in _constant_eager_impl     t = convert_to_eager_tensor(value, ctx, dtype)   File ""/Users/jawer/opt/miniconda3/envs/tensorflow29/lib/python3.9/sitepackages/tensorflow/python/framework/constant_op.py"", line 102, in convert_to_eager_tensor     return ops.EagerTensor(value, ctx.device_name, dtype) ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray). Thank you for your help!",Hi . This doc explains how to use dataframes with tf.data: https://www.tensorflow.org/tutorials/load_data/pandas_dataframea_dataframe_as_a_dictionary. The idea is to cast your dataframe as a `dict` before using `from_tensor_slices`., Could you please refer to the comment above and also refer to this link to know more about casting dataframe .  Please let us know if it helps? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1473,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Changes to metrics passed to on_batch_end(...))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.2+  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04.1  Mobile device Linux Ubuntu 22.04.1  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory _No response_  Current Behaviour? I recently updated code from TensorFlow 1 and was getting strange results for my callbacks. I eventually found issue CC(Metrics passed to on_batch_end(...) in Keras are inconsistent) and decided to test that. I discovered that in tf 2.2+ on_batch_end is now provided a moving average of all losses so far in the current epoch, while in previous versions only the loss for the current batch is provided. This matters when using the learning rate finder callback described in Cyclical Learning Rates for Training Neural Networks by Leslie N. Smith.  When using the moving average losses, the suggested range of learning rate values is shifted to the right, resulting in learning rates that are much higher than the optimal range, and leading to poorer performance than when the loss for only the current batch is used correctly.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,leland-hepworth,Changes to metrics passed to on_batch_end(...),"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.2+  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04.1  Mobile device Linux Ubuntu 22.04.1  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory _No response_  Current Behaviour? I recently updated code from TensorFlow 1 and was getting strange results for my callbacks. I eventually found issue CC(Metrics passed to on_batch_end(...) in Keras are inconsistent) and decided to test that. I discovered that in tf 2.2+ on_batch_end is now provided a moving average of all losses so far in the current epoch, while in previous versions only the loss for the current batch is provided. This matters when using the learning rate finder callback described in Cyclical Learning Rates for Training Neural Networks by Leslie N. Smith.  When using the moving average losses, the suggested range of learning rate values is shifted to the right, resulting in learning rates that are much higher than the optimal range, and leading to poorer performance than when the loss for only the current batch is used correctly.  Standalone code to reproduce the issue   Relevant log output  ",2022-10-14T22:16:12Z,stat:awaiting response type:bug comp:keras type:performance TF 2.2,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58098,"Also the ""Current Behaviour"" section in the issue template is being formatted as code.  I had to edit it to remove the code formatting.","In the meantime, I've added the following code to my learning rate finder callback to get the previous functionality: ",Hi hepworth  I was able to reproduce the issue on colab using TF v2.10. Please find the gist here for reference. This issue seems to be Keras issue. Please post this issue on kerasteam/keras repo. All issues and PRs related to keras will be addressed in that repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999. Thank you!,", thanks for your reply! I created keras issue kerasteam/tfkeras CC(TensorBoard,No scalar summary tags were found), so this can be closed.",Are you satisfied with the resolution of your issue? Yes No
271,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fixing the incorrect link in input_layer.py)ï¼Œ å†…å®¹æ˜¯ (Added the correct link for `RaggedTensor`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tilakrayal,Fixing the incorrect link in input_layer.py,Added the correct link for `RaggedTensor`,2022-10-14T12:44:43Z,comp:keras size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/58095,"Hi  It looks like your PR relates to the Keras component. Please submit it to the github.com/kerasteam/keras repository instead. Thankyou. , "
990,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([AMD-ZENDNN] Code changes to support TF-Plugin for AMD CPU Inference)ï¼Œ å†…å®¹æ˜¯ (This pull request contains necessary/base changes for supporting TFPlugin for AMD CPU Inference. With this code change we introduce,      Functionality to enable AMD CPU optimizations by setting environment variable 'TF_ENABLE_ZENDNN_OPTS=1'      Zen layout pass to rewrite supported ops with ZenOps All these code changes are under AMD_ZENDNN flag which is enabled for linux builds only. Authors:     Aakar Dwivedi ( aakar.dwivedi.com )     Aditya Chatterjee ( aditya.chatterjee.com )     Arun Coimbatore Ramachandran ( aruncoimbatore.ramachandran.com )     AvinashChandra Pandey ( avinashchandra.pandey.com )     Chandra Kumar Ramasamy ( chandrakumar.ramasamy.com )     Savan Anadani ( savan.anadani.com ) Signedoffby: Aakar Dwivedi )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,aadwived,[AMD-ZENDNN] Code changes to support TF-Plugin for AMD CPU Inference,"This pull request contains necessary/base changes for supporting TFPlugin for AMD CPU Inference. With this code change we introduce,      Functionality to enable AMD CPU optimizations by setting environment variable 'TF_ENABLE_ZENDNN_OPTS=1'      Zen layout pass to rewrite supported ops with ZenOps All these code changes are under AMD_ZENDNN flag which is enabled for linux builds only. Authors:     Aakar Dwivedi ( aakar.dwivedi.com )     Aditya Chatterjee ( aditya.chatterjee.com )     Arun Coimbatore Ramachandran ( aruncoimbatore.ramachandran.com )     AvinashChandra Pandey ( avinashchandra.pandey.com )     Chandra Kumar Ramasamy ( chandrakumar.ramasamy.com )     Savan Anadani ( savan.anadani.com ) Signedoffby: Aakar Dwivedi ",2022-10-14T10:23:20Z,size:XL comp:core,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58094,/,Hi  Can you please check 's comments and resolve conflicts?. Thank you!,"Hello  , As suggested by  we are splitting this PR into several smaller PR. We will be addressing the review comments in respective PRs. We plan to abandon this PR. Thank you!"
667,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.keras.Model()` behaves incorrectly if `inputs` is a dictionary view)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.5 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pikulmar,`tf.keras.Model()` behaves incorrectly if `inputs` is a dictionary view,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.5 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-13T13:40:08Z,stat:awaiting response type:bug comp:keras TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/58087,", Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!", Thanks for pointing me to the right location. The newly opened issue is https://github.com/kerasteam/tfkeras/issues/392.,Are you satisfied with the resolution of your issue? Yes No
671,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom layer with tf.extract_image_patches extremely slow)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution Google Colab  Mobile device _No response_  Python version 3.7.14  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Leonamrsm,Custom layer with tf.extract_image_patches extremely slow,Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution Google Colab  Mobile device _No response_  Python version 3.7.14  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-10-13T12:48:01Z,stat:awaiting tensorflower comp:ops type:performance TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58086,", I tried to execute the code on tensorflow v2.9.2 and haven't found any perfomance issue during the execution. Kindly find the gist of it here and let us know if anything is missing here. Thank you!","  Backprop through extract_image_patches is extremly slow, specifically the transpose call. It appears to be this specific call: tensorflow/python/ops/array_grad.pyL747, as raised in issue  CC(tf.extract_image_patches gradient transpose extremely slow). !image I updated the gist  with a call from the new layer and the MaxPooling2D layer from keras, using the training data from the cifar10 dataset with dimensions 5000x32x32x3. When using the new layer it took 42s to execute and when using MaxPooling2D 2s. Thank you!",", I was able to reproduce the issue on tensorflow v2.9, v2.10 and nightly. Kindly find the gist of it here.","> I tried to take a look at what's inside gen_nn_ops.max_pool, but I can't find it in the repository. All these `gen_**_ops` get automatically generated from the op definitions, e.g. nn_ops.cc:870 for ""MaxPool"".  The generated python code won't help you much  it just creates a wrapper around a call to the C++ kernel.  The actual kernel that gets called depends on the device you're running it on.  For CPU,  The `MaxPoolingOp` kernel is registered here.  The definition of `MaxPoolingOp` is here. These ops also have special gradients defined.  For ""MaxPool"", the gradient definition is here, which just calls the wrapped gradient op ""MaxPoolGrad"", which has the kernel `MaxPoolingGradOp`.   > Is it possible to create a custom pooling operation in C++ and use it in my Python code? Yes, it would be similar to the above.  See the custom op guide.  The gradient can also be a custom op, just as it is for `MaxPool`.",  Thanks for the answer. I believe this solution should work for me. I will follow this guide to create a custom operation.,"Marking as closed then.  For the slow transpose issue, if it's a blocker, please reopen CC(tf.extract_image_patches gradient transpose extremely slow)."
1467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite Interpreter.invoke()- gather index out of boundsNode number 935 (GATHER) failed to invoke.)ï¼Œ å†…å®¹æ˜¯ (Getting this error with the following sample code for prediction:  Model weights: https://drive.google.com/file/d/1SXbMtxQKiUIavh6U7lwzEZS0lYYNpJy/view?usp=sharing  `import cv2 if __name__ == '__main__':     converted_model = ""kvp_converted_model_fp16.tflite""     image_path = ""kvp_test/test_1.png""     seg_image_path=""kvp_test/seg_test_1.png""     good_image_path = ""datasets/experiment/good/g.png""     img1 = cv2.imread(image_path)     img2=cv2.imread(seg_image_path)     img = img[None, :, :, :]     resized=img     resized_img = cv2.resize(img1, (608,608)).astype('float32')     resized_seg_img = cv2.resize(img2, (608,608)).astype('float32')     resized=img.astype('float32')     resized_img = resized_img / 255.     resized_seg_img = resized_seg_img / 255.     input_image = np.expand_dims(resized_img, axis=0)     seg_image= np.expand_dims(resized_seg_img, axis=0)     print(test_img)     image_data = np.concatenate([input_image,seg_image], axis = 1).astype(np.float32)     y_inp = np.expand_dims(np.array([img1.shape[0], img1.shape[1]]), axis = 0).astype(np.float32)     prediction,pred = run_tflite_model(converted_model,image_data,y_inp)     print(pred)` **Error**: !gather)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,divyajaincs,TFLite Interpreter.invoke()- gather index out of boundsNode number 935 (GATHER) failed to invoke.,"Getting this error with the following sample code for prediction:  Model weights: https://drive.google.com/file/d/1SXbMtxQKiUIavh6U7lwzEZS0lYYNpJy/view?usp=sharing  `import cv2 if __name__ == '__main__':     converted_model = ""kvp_converted_model_fp16.tflite""     image_path = ""kvp_test/test_1.png""     seg_image_path=""kvp_test/seg_test_1.png""     good_image_path = ""datasets/experiment/good/g.png""     img1 = cv2.imread(image_path)     img2=cv2.imread(seg_image_path)     img = img[None, :, :, :]     resized=img     resized_img = cv2.resize(img1, (608,608)).astype('float32')     resized_seg_img = cv2.resize(img2, (608,608)).astype('float32')     resized=img.astype('float32')     resized_img = resized_img / 255.     resized_seg_img = resized_seg_img / 255.     input_image = np.expand_dims(resized_img, axis=0)     seg_image= np.expand_dims(resized_seg_img, axis=0)     print(test_img)     image_data = np.concatenate([input_image,seg_image], axis = 1).astype(np.float32)     y_inp = np.expand_dims(np.array([img1.shape[0], img1.shape[1]]), axis = 0).astype(np.float32)     prediction,pred = run_tflite_model(converted_model,image_data,y_inp)     print(pred)` **Error**: !gather",2022-10-13T11:03:19Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.10,closed,0,11,https://github.com/tensorflow/tensorflow/issues/58085,  I was able to reproduce the  issue on Colab using TF v2.10. Please find the gist here for reference. Thank you!,  i'm using tensorflow version 1.15.0 and tfnightly2.11.0.dev20221016, ! Sorry for the late response.  I changed the order of resizingset_tensorinvoke to resolve the issue. Could you check the required changes in this gist  from 2.10 version for reference. Thank you!," when i'm testing with your updated code, colab session is crashing with cpu. any idea why this is happening?"," ! Your input size in lite model  is (608,608) so I resized the tensor input accordingly, But while using a low resolution image, it crashed the session. Thank you!"," I'm testing with original image size of (760,1000), with gpu it's working fine but with cpu it's crashing.",ok  ! Thanks for the update.  It is a bit strange though as I ran the above the code in a cpuruntime only but it did not surface in my case.  ! Could you look at this issue. Thank you!,"Hi , it seems like your model link is no longer working, can you respond with your updated model link? Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
555,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([oneDNN] Fusing _FusedConv2D/3D with {LeakyRelu, Mish})ï¼Œ å†…å®¹æ˜¯ (!164603324b9dc982e450c46ca9bfc8bd9bf740a67 This PR fuses __FusedConv2D/3D_ with elementwise ops fused in the first iteration of the remapper optimizer. Currently, adding support for _LeakyRelu_ and _Mish_ activations. The fusion results in ~8% performance improvement on average for the models having this pattern.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,othakkar,"[oneDNN] Fusing _FusedConv2D/3D with {LeakyRelu, Mish}","!164603324b9dc982e450c46ca9bfc8bd9bf740a67 This PR fuses __FusedConv2D/3D_ with elementwise ops fused in the first iteration of the remapper optimizer. Currently, adding support for _LeakyRelu_ and _Mish_ activations. The fusion results in ~8% performance improvement on average for the models having this pattern.",2022-10-12T21:07:10Z,awaiting review ready to pull comp:grappler size:L,closed,0,4,https://github.com/tensorflow/tensorflow/issues/58080,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,"CC   , can you take a look at this fusion.",Hi  Can you please review this PR ? Thank you!
1935,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(When training the TensorFlow Lite model after loading it in C++, the logit/probabllites size is different from the number of y labels.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When I ran the python model, as a result of predict, the size of logit and probablites was the same as the size of y label.   But, in C++,  The size of logits is 0, and the size of proba is 6. Am I using C++ Code wrong?  Standalone code to reproduce the issue   ```c++ tflite::SignatureRunner* trainSignatureRunner = m_interpreter>GetSignatureRunner( ""train"" ); trainSignatureRunner>ResizeInputTensor( ""x"", { batchSize, 18 } ); trainSignatureRunner>ResizeInputTensor( ""y"", { batchSize, 5 } ); trainSignatureRunner>AllocateTensors() TfLiteTensor* x_input_tensor = trainSignatureRunner>input_tensor( ""x"" ); // fill x_input_tensor TfLiteTensor* y_input_tensor = trainSignatureRunner>input_tensor( ""y"" ); // fill y_input_tensor trainSignatureRunner>Invoke(); tflite::SignatureRunner* predictSignatureRunner = m_interpreter>GetSignatureRunner(""predict""); predictSignatureRunner>ResizeInputTensor( ""x"", { batchSize, 18 } ); TfLiteTensor* predict_x_input_tensor = predictSignatureRunner>input_tensor( ""x"" ); // fill predict_x_input_tensor predictSignatureRunner>Invoke(); const TfLiteTensor* logit_output = predictSignatureRunner>output_tensor( ""logits"" ); std::vector logitsResult; FLOAT* tempLogits = logit_output>data.f; while ( 0 output_tensor( ""output"" ); std::vector proba)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ysbaekFox,"When training the TensorFlow Lite model after loading it in C++, the logit/probabllites size is different from the number of y labels.","Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When I ran the python model, as a result of predict, the size of logit and probablites was the same as the size of y label.   But, in C++,  The size of logits is 0, and the size of proba is 6. Am I using C++ Code wrong?  Standalone code to reproduce the issue   ```c++ tflite::SignatureRunner* trainSignatureRunner = m_interpreter>GetSignatureRunner( ""train"" ); trainSignatureRunner>ResizeInputTensor( ""x"", { batchSize, 18 } ); trainSignatureRunner>ResizeInputTensor( ""y"", { batchSize, 5 } ); trainSignatureRunner>AllocateTensors() TfLiteTensor* x_input_tensor = trainSignatureRunner>input_tensor( ""x"" ); // fill x_input_tensor TfLiteTensor* y_input_tensor = trainSignatureRunner>input_tensor( ""y"" ); // fill y_input_tensor trainSignatureRunner>Invoke(); tflite::SignatureRunner* predictSignatureRunner = m_interpreter>GetSignatureRunner(""predict""); predictSignatureRunner>ResizeInputTensor( ""x"", { batchSize, 18 } ); TfLiteTensor* predict_x_input_tensor = predictSignatureRunner>input_tensor( ""x"" ); // fill predict_x_input_tensor predictSignatureRunner>Invoke(); const TfLiteTensor* logit_output = predictSignatureRunner>output_tensor( ""logits"" ); std::vector logitsResult; FLOAT* tempLogits = logit_output>data.f; while ( 0 output_tensor( ""output"" ); std::vector proba",2022-10-12T05:15:10Z,stat:awaiting response type:bug stale comp:lite TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58070,"In python code, If two types are returned as shown below, a problem seems to occur. I removed the code that returned ""logits"", left only ""output"", and it worked normally. ","Hi , can you either show your full code or a compilable toy example which shows this issue? I'm trying to answer what batchSize is and what FLOAT actually is, my guess is it's a typedef but assuming these things usually lead to trouble.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1029,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deprecate ImplicitBatchModeCompatible dynamic shape strategy)ï¼Œ å†…å®¹æ˜¯ (This strategy is intended for ease of use for people familiar with the implicit batch mode profile, in that it does not require the user to specifically call build() before trying to run an inference with TFTRT converted graph. Since this is actually a dynamic shape mode, and input shapes are required in dynamic shape mode for TensorRT profile generation; this mode makes some educated guesses for minimum and maximum shapes for inputs the TensorRT engine. This has proven to be buggy for models that include transpose and reshape operations, among others. Due to the above, and since dynamic shape mode requires users to call build() with the correct input shapes to generate TensorRT profiles correctly, this mode is being deprecated. CC:    Signedoffby: Meenakshi Venkataraman )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,meena-at-work,Deprecate ImplicitBatchModeCompatible dynamic shape strategy,"This strategy is intended for ease of use for people familiar with the implicit batch mode profile, in that it does not require the user to specifically call build() before trying to run an inference with TFTRT converted graph. Since this is actually a dynamic shape mode, and input shapes are required in dynamic shape mode for TensorRT profile generation; this mode makes some educated guesses for minimum and maximum shapes for inputs the TensorRT engine. This has proven to be buggy for models that include transpose and reshape operations, among others. Due to the above, and since dynamic shape mode requires users to call build() with the correct input shapes to generate TensorRT profiles correctly, this mode is being deprecated. CC:    Signedoffby: Meenakshi Venkataraman ",2022-10-11T21:40:45Z,awaiting review ready to pull size:XS comp:gpu:tensorrt,closed,0,0,https://github.com/tensorflow/tensorflow/issues/58065
864,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NVIDIA TF] Use NHWC for TF32 (Inference only))ï¼Œ å†…å®¹æ˜¯ (This PR enables the NHWC data layout for the inference op graphs when the underlying GPUs are Ampere+ and input dtype is TF32. Using NHWC is recommended in such cases to better utilize the 3rd generation Tensor Core. For now, the cuDNN can provide improved performance only for the fwd and bwd_data convolutions and will improve the bwd_filter convolution in the future release. Therefore, we enable this enhancement only for inference in this PR. We have conducted some perf evaluation across a range of convnets (image classification and object detection tasks) and observed a ~20% perf improvement in most cases. cc.   for viz. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kaixih,[NVIDIA TF] Use NHWC for TF32 (Inference only),"This PR enables the NHWC data layout for the inference op graphs when the underlying GPUs are Ampere+ and input dtype is TF32. Using NHWC is recommended in such cases to better utilize the 3rd generation Tensor Core. For now, the cuDNN can provide improved performance only for the fwd and bwd_data convolutions and will improve the bwd_filter convolution in the future release. Therefore, we enable this enhancement only for inference in this PR. We have conducted some perf evaluation across a range of convnets (image classification and object detection tasks) and observed a ~20% perf improvement in most cases. cc.   for viz. ",2022-10-11T16:40:54Z,ready to pull comp:grappler size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/58054
1820,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support Python 3.11)ï¼Œ å†…å®¹æ˜¯ (Currently Python 3.11 isn't fully supported in TensorFlow yet, including testing in CI and publishing wheels to PyPI. Python 3.11 is currently in Release Candidate state and will be released in about two weeks. Major packages like NumPy, Pandas, SciPy and Matplotlib have already uploaded their Python 3.11 wheels to PyPI.  Python 3.11 new features include: >  PEP 657  Include FineGrained Error Locations in Tracebacks >  PEP 654  Exception Groups and except* >  PEP 673  Self Type >  PEP 646 Variadic Generics >  PEP 680 tomllib: Support for Parsing TOML in the Standard Library >  PEP 675 Arbitrary Literal String Type >  PEP 655 Marking individual TypedDict items as required or potentiallymissing >  bpo46752 Introduce task groups to asyncio >  The Faster Cpython Project is already yielding some exciting results: this version of CPython 3.11 is ~ 19% faster on the geometric mean of the PyPerformance benchmarks, compared to 3.10.0. TensorFlow is an import package in the deep learning stack, so support at release will help speed up Python 3.11 adoption. It would be great to have full Python 3.11 support in TensorFlow by the time Python 3.11.0 gets released, (expected Monday October 6th, 2022). This includes testing in CI and publishing wheels to PyPI. This issue can be used as a tracking issue. This includes:  [ ] TensorFlow builds fully on Python 3.11  [x] All TensorFlow tests pass on Python 3.11  [ ] All CI is run and green on Python 3.11  [ ] Wheels are uploaded to PyPI for tfnightly    [x] Linux    [x] macOS    [ ] Windows  [ ] Wheels are uploaded to PyPI for at least one TensorFlow release)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,EwoutH,Support Python 3.11,"Currently Python 3.11 isn't fully supported in TensorFlow yet, including testing in CI and publishing wheels to PyPI. Python 3.11 is currently in Release Candidate state and will be released in about two weeks. Major packages like NumPy, Pandas, SciPy and Matplotlib have already uploaded their Python 3.11 wheels to PyPI.  Python 3.11 new features include: >  PEP 657  Include FineGrained Error Locations in Tracebacks >  PEP 654  Exception Groups and except* >  PEP 673  Self Type >  PEP 646 Variadic Generics >  PEP 680 tomllib: Support for Parsing TOML in the Standard Library >  PEP 675 Arbitrary Literal String Type >  PEP 655 Marking individual TypedDict items as required or potentiallymissing >  bpo46752 Introduce task groups to asyncio >  The Faster Cpython Project is already yielding some exciting results: this version of CPython 3.11 is ~ 19% faster on the geometric mean of the PyPerformance benchmarks, compared to 3.10.0. TensorFlow is an import package in the deep learning stack, so support at release will help speed up Python 3.11 adoption. It would be great to have full Python 3.11 support in TensorFlow by the time Python 3.11.0 gets released, (expected Monday October 6th, 2022). This includes testing in CI and publishing wheels to PyPI. This issue can be used as a tracking issue. This includes:  [ ] TensorFlow builds fully on Python 3.11  [x] All TensorFlow tests pass on Python 3.11  [ ] All CI is run and green on Python 3.11  [ ] Wheels are uploaded to PyPI for tfnightly    [x] Linux    [x] macOS    [ ] Windows  [ ] Wheels are uploaded to PyPI for at least one TensorFlow release",2022-10-09T23:53:59Z,stat:awaiting tensorflower type:support type:performance,closed,67,43,https://github.com/tensorflow/tensorflow/issues/58032,I guess from the past approach it might only get supported with the next release tf2.11. Also depending on support in depdencies. Python3.8 support Â· Issue CC(Python3.8 support) Â· tensorflow/tensorflow Support Python 3.9 Â· Issue CC(Support Python 3.9) Â· tensorflow/tensorflow Building from source failed on Python 3.10 Â· Issue CC(Building from source failed on Python 3.10) Â· tensorflow/tensorflow Maybe worthwhile to coordinate TF releases and upstream dependencies early on before next October. PEP 693 â€“ Python 3.12 Release Schedule | peps.python.org Would certainly help to swiftly adopt new Python versions.,"See also CC(Python3.11 support please) (not closing as duplicate, since this one has more relevant content, thank you both for the comments here)",Python 3.11 is released today.,"Yeah! Python 3.11 support please. It should be the default. Python 3.11 is between 1060% faster than Python 3.10. On average, we measured a 1.25x speedup on the standard benchmark suite. See Faster CPython for details. https://docs.python.org/3.11/whatsnew/3.11.html https://www.phoronix.com/news/Python3.11Released","We still need for all of our dependencies to have a release on Py3.11. And, unfortunately, we already cut the branch for the next TF release, so we won't be able to have a TF release with py3.11 support until TF 2.12 (though nightly should get support faster, before branch cut)",Benchmark https://www.phoronix.com/review/python311performance,"since https://github.com/tensorflow/tensorflow/pull/58345, Could `tfnightly` support 3.11 ASAP first?",The infra team needs to create jobs for the nightly jobs. CC toplay   ,Please?,"Today is November 27, 2022, and Tensorflow is still not supporting Python 3.11.",How work proceses for now?,"Today is December 1, 2022, and Tensorflow is still not supporting Python 3.11.",Thank you for reaching out! TensorFlow next release will be in Q1 2023.,"Today is December 3, 2022, and Tensorflow is still not supporting Python 3.11.","Today is December 4, 2022, and Tensorflow is still not supporting Python 3.11.","Folks, please stop spamming. `tfnightly` support will arrive first and then TF 2.12 will have support for Python 3.11. There are several dependencies that need to be updated too, I'm sure the team is working on it. When I was in the team, TF support for Python versions _has always_ arrived with the next release of TF, _after_ the Python release. There are just too many dependencies that need to be upgraded and the code needs a lot of work since we need to support multiple Python versions with very few changes (if any) to the code itself.",And something is already landing on master https://github.com/tensorflow/tensorflow/commit/36d86747b159026be77a5eda1e0aff1292b88493,Update: https://github.com/tensorflow/tensorflow/commit/3de3b132f6d8fdc6d982df9542eff840a6d0e89f adds more support. Next is the build jobs themselves.,Linux and MacOS wheels now exist in `tfnightly`: https://pypi.org/project/tfnightly/files,thanks  ,Are you satisfied with the resolution of your issue? Yes No,"toplay Could you reopen this issue? While great progress is being made, this issue is far from fully resolved (see the tracker above).","> toplay Could you reopen this issue? While great progress is being made, this issue is far from fully resolved (see the tracker above).  I think that python 3.11 compatible will be released with tensorflow 1.12. So install nightly version and be patient",">  I think that python 3.11 compatible will be released with tensorflow 1.12. So install nightly version and be patient This is my first comment since I opened this discussion on October 10th, I think I'm quite patient. I just would like to keep track of the progress, so everyone looking for Python 3.11 support can exactly see in the checklist above where we stand, what's already available (macOS and Ubuntu nightly wheels) and what's not (Windows wheel and a stable release).","Am I good to go in installing Tensor Flow on my Python 3.11.1 or are there any other ""Gotchas"" I have to worry about.    I'm on an M1 Mac Laptop,  Ventura 13.1"," , for one thing, no hardware accelerated training since Apple's last ""sort of working"" TF release (`tensorflowmacos`) is stuck on 2.9 and that wonâ€™t run with Python 3.11. So you'll have to wait for a `tensorflowcpu` release working on Python 3.11 which might be faster thanks to the faster CPython effort on 3.11 but will purely run on the CPU cores of your M1 Mac.",Python 3.11 is out in anaconda https://github.com/ContinuumIO/anacondaissues/issues/13082,"Hello, any idea of a potential release date ?","> Hello, any idea of a potential release date ? I think when cudnn new release for cuda 12.0","I have python 3.11. A week ago I couldnâ€™t install it using condo. Itâ€™s got a bit of David do you support 3.11?Sent from my iPadOn Jan 27, 2023, at 2:44 AM, Johnny ***@***.***> wrote:ï»¿ Hello, any idea of a potential release date ? I think when cudnn new release for cuda 12.0 â€”Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>"
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.raw_ops.TruncateDiv documentation is not consistent)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,KareemMAX,tf.raw_ops.TruncateDiv documentation is not consistent,Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-07T23:49:42Z,type:docs-bug comp:ops TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/58019,The proto of the ops is correctly accepting float/double: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/compat/ops_history_v2/TruncateDiv.pbtxt But at the same time we don't have registered generated kernel for float/double: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mlir_generated/gpu_op_div.ccL42:L45 ,See also: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_div.cc,Hi    Thanks for the pointers. I am trying to come with a PR to register them in kernels  and will test it with a test case prior to it. Thank you!,"I think it's more likely that it's a typo in the `math_ops.cc` file  that it should really only be defined for integer types.  The documentation page itself says: > Returns x / y elementwise for integer types. For example, all gradients for the `TruncateDiv` op are explicitly disabled.  I'm not sure it would work in XLA either.",Ok  ! Thanks for the tip.  I will change the info in website then. Thank you!
874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to build libtensorflowlite_c.so on M1 mac)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution darwin_arm64  Mobile device _No response_  Python version 3.9.12  Bazel version 5.3.0  GCC/Compiler version Apple clang version 14.0.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?    I get the following error:  Expected behavior: the `libtensorflowlite_c.so` object is built. shell From the root TensorFlow directory: bazel build config=monolithic cpu=darwin_arm64 c opt //tensorflow/lite/c:libtensorflowlite_c.so verbose_failures jobs=4   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jasonw247,Failed to build libtensorflowlite_c.so on M1 mac,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution darwin_arm64  Mobile device _No response_  Python version 3.9.12  Bazel version 5.3.0  GCC/Compiler version Apple clang version 14.0.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?    I get the following error:  Expected behavior: the `libtensorflowlite_c.so` object is built. shell From the root TensorFlow directory: bazel build config=monolithic cpu=darwin_arm64 c opt //tensorflow/lite/c:libtensorflowlite_c.so verbose_failures jobs=4   Relevant log output  ,2022-10-06T13:56:19Z,stat:awaiting response type:build/install comp:lite subtype:macOS TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/58004,Hi  ! .so extension for linux machines and  .dylib extension for macos Ref  Could you just go without .so extension and let us know . `bazel build config=monolithic cpu=darwin_arm64 c opt //tensorflow/lite/c:libtensorflowlite_c verbose_failures jobs=4 ` Thank you!,  running `bazel build config=monolithic cpu=darwin_arm64 c opt //tensorflow/lite/c:libtensorflowlite_c verbose_failures jobs=4`  results in:  running: `bazel build config=monolithic cpu=darwin_arm64 c opt //tensorflow/lite/c:libtensorflowlite_c.dylib verbose_failures jobs=4` Results in: ,"For reference, I was able to run both: `bazel build config=monolithic c opt //tensorflow/lite/c:libtensorflowlite_c.so verbose_failures jobs=4` and  `bazel build config=monolithic c opt //tensorflow/lite/c:libtensorflowlite_c.dylib verbose_failures jobs=4` on an x86 mac without failure.",Ok  ! I think you were trying to build libtensorflowlite_c for Mac M1 (Darwin Arm64). It succeeded with below command in my Mac M1 with Bazel 5.0.0 and TF 2.9 branch.  Could you test in  2.10 version with Same command . `bazel build config=monolithic c opt tensorflow/lite/c:libtensorflowlite_c.dylib verbose_failures jobs=4`  Thank you!, I can confirm this now works with current `master` and `r2.10` branches. ,Are you satisfied with the resolution of your issue? Yes No
673,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Making tensorflow.tile similar to numpy.tile and torch.tile)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,RickSanchezStoic,Making tensorflow.tile similar to numpy.tile and torch.tile,Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-06T07:46:16Z,stat:contribution welcome stat:awaiting response type:feature stale comp:ops,closed,0,10,https://github.com/tensorflow/tensorflow/issues/58002,", Could you please take a look at doc link for tf.tile which supports **the operation creates a new tensor by replicating input multiples times** and confirm if you are looking for the similar feature. Thank you!","Yes, it has the condition `Length must be the same as the number of dimensions in input` which is not the case in `numpy` or `torch`. You can confirm this by running the standalone code provided.",any updates  ? btw can you explain the `stat:awaiting tensorflower` label :),"`awaiting tensorflower` means they're waiting for a response from someone on the official TensorFlow team at Google to weigh in here (e.g. me). Sure, I think this would be valuable.  Copy the `numpy.tile` behavior, and promote the two to be the same size by prepending 1s (either to `multiples` or the shape of the input). This is not a high priority for us, so if someone wants to try to tackle this, it would be a welcome contribution.",Is this issue still open for contribution? I look forward to contributing to this issue as my first contribution to the Outreachy internship. ,> Is this issue still open for contribution? I look forward to contributing to this issue as my first contribution to the Outreachy internship. Sounds great to me.  If you create a PR you can ping my username and I will review.,Hey I think i Can work this out how do i contribute in this project??,Hi  ! We are checking to see whether you still need help in this issue . I am able to get a numpy.tile like results using tensorflow.experimental.numpy  api Attached gist for reference.  Please let us know the results from your side. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1846,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RNN/LSTM]concatenation error during invoke())ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code I converted pytorch model to tensorflow using onnx. The output of converted tensorflow model is exactly as expected. The following is the pytorch model code:         def __init__(self, device: Union[str, torch.device]=None, verbose=True, weights_fpath: Union[Path, str]=None):         super().__init__()         self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)         self.linear = nn.Linear(model_hidden_size, model_embedding_size)         self.relu = nn.ReLU()         if device is None:             device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")         elif isinstance(device, str):             device = torch.device(device)         self.device = device          Load the pretrained model'speaker weights         if weights_fpath is None:             weights_fpath = Path(__file__).resolve().parent.joinpath(""pretrained.pt"")         else:             weights_fpath = Path(weights_fpath)         if not weights_fpath.exists():             raise Exception(""Couldn't find the voice encoder pretrained model at %s."" %                             weights_fpath)         start = timer()         checkpoint = torch.load(weights_fpath, map_location=""cpu"")         self.load_state_dict(checkpoint[""model_state""], strict=False)         self.to(device)         if verbose:             print(""Loaded the voice en)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Raj-Gohil,[RNN/LSTM]concatenation error during invoke()," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code I converted pytorch model to tensorflow using onnx. The output of converted tensorflow model is exactly as expected. The following is the pytorch model code:         def __init__(self, device: Union[str, torch.device]=None, verbose=True, weights_fpath: Union[Path, str]=None):         super().__init__()         self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)         self.linear = nn.Linear(model_hidden_size, model_embedding_size)         self.relu = nn.ReLU()         if device is None:             device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")         elif isinstance(device, str):             device = torch.device(device)         self.device = device          Load the pretrained model'speaker weights         if weights_fpath is None:             weights_fpath = Path(__file__).resolve().parent.joinpath(""pretrained.pt"")         else:             weights_fpath = Path(weights_fpath)         if not weights_fpath.exists():             raise Exception(""Couldn't find the voice encoder pretrained model at %s."" %                             weights_fpath)         start = timer()         checkpoint = torch.load(weights_fpath, map_location=""cpu"")         self.load_state_dict(checkpoint[""model_state""], strict=False)         self.to(device)         if verbose:             print(""Loaded the voice en",2022-10-05T17:28:58Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57988,"Hi Gohil!  Could you confirm that input shape for both original model and lite model is [1,160,40] . If not same , you can resize the input tensor in lite model to shape of original model.  Other way is to follow fusion codelab instructions which works well with RNN/LSTM model. Would it possible to share a toy model/ Colab gist too. Thank you!","Hello , The input shape is [1,160,40] If you want i can share the .pb file.",Ok Gohil ! Did you check the fusion code lab instructions too ? Please share the model file . Thank you!,test_new1.zip   This is the folder which has the .pb and .tflite model. Please check it out and I did not get what exactly is to be done according to the fushion code lab instructions. If you can share a code snippet of what needs to be done in python... it would be great.,Do you have updates on this  ,Gohil  I was able to reproduce concatentation.. Proposed idea on Fusion code lab was like below. ( But it was throwing dimension mismatch)   !  Could you look at this issue.  Attached gist for reference. Thank you!,Hello  .... do we have any update on this?,"Hi Gohil  As this comment suggests, the issue can be while authoring the model. Can you please check the input shape compatibility with the channels while converting to tensorflow? Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
657,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Supporting int32 value type in TextFileInitializer.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Linux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,supercharleszhu,Supporting int32 value type in TextFileInitializer.,Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Linux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-05T03:34:07Z,stat:awaiting response type:feature stale comp:ops TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57976,  I was able to execute the given code without any error on Colab using TF v2.10. Please find the gist here for reference. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1229,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Grappler error when Softmax is used in a custom layer)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell `Error in PredictCost() for the op: op: ""Softmax"" attr { key: ""T"" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""NVIDIA GeForce GTX 1080 Ti"" frequency: 1670 num_cores: 28 environment { key: ""architecture"" value: ""6.1"" } environment { key: ""cuda"" value: ""11020"" } environment { key: ""cudnn"" value: ""8100"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 2883584 shared_memory_size_per_multiprocessor: 98304 memory_size: 10099490816 bandwidth: 484440000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }` ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JustASquid,Grappler error when Softmax is used in a custom layer,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell `Error in PredictCost() for the op: op: ""Softmax"" attr { key: ""T"" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""NVIDIA GeForce GTX 1080 Ti"" frequency: 1670 num_cores: 28 environment { key: ""architecture"" value: ""6.1"" } environment { key: ""cuda"" value: ""11020"" } environment { key: ""cudnn"" value: ""8100"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 2883584 shared_memory_size_per_multiprocessor: 98304 memory_size: 10099490816 bandwidth: 484440000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }` ``` ",2022-10-05T01:35:08Z,stat:awaiting response type:bug stale comp:apis TF 2.10,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57975,", Could you please try to run the below command before executing the code. I tried and was able to execute the code without any issue/error. Kindly find the gist of it here. **!apt install allowchangeheldpackages libcudnn8=8.1.0.771+cuda11.2** Thank you!",Hey  the platform is Windows 10 so I won't be able to run that command.,", Could you please create a virtual environment and test your code again. The attached gist executed without any error, then there is no issue with the code and the Tensorflow. Thank you!","  I have run this in a fresh virtual environment following the recommended GPU installation on Windows Native: https://www.tensorflow.org/install/pipwindowsnative_1 And the issue still comes up. It seems like this is a Windows issue, so it makes sense that the colab gist works correctly.",", We can see you are trying to install tensorflow v2.10 with Cuda 11.6. Could you try to install with tested build configurations as mentioned here and try to execute the code. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I am having the same problem. Running Tensorflow 2.15 with cudnn 8600. Full log: > W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: ""Softmax"" attr { key: ""T"" value { type: DT_HALF } } inputs { dtype: DT_HALF shape { unknown_rank: true } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""NVIDIA A40"" frequency: 1740 num_cores: 84 environment { key: ""architecture"" value: ""8.6"" } environment { key: ""cuda"" value: ""11080"" } environment { key: ""cudnn"" value: ""8600"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 6291456 shared_memory_size_per_multiprocessor: 102400 memory_size: 45716668416 bandwidth: 696096000 } outputs { dtype: DT_HALF shape { unknown_rank: true } }"
395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TFTRT]: Add converter for QuantizeAndDequantizeV4)ï¼Œ å†…å®¹æ˜¯ (Per TF docs, this op behaves exactly the same as QuantizeAndDequantizeV2 for inference, so the converter for QDQv4 leverages existing infrastructure. CC:     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,meena-at-work,[TFTRT]: Add converter for QuantizeAndDequantizeV4,"Per TF docs, this op behaves exactly the same as QuantizeAndDequantizeV2 for inference, so the converter for QDQv4 leverages existing infrastructure. CC:     ",2022-10-04T15:56:46Z,stale comp:gpu size:S comp:gpu:tensorrt,closed,1,4,https://github.com/tensorflow/tensorflow/issues/57973,   any chance you can review this?,Hi atwork Can you please resolve conflicts? Thank you!,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
630,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(//tensorflow/python/tools:aot_compiled_test fails to build on aarch64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.3.0  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,elfringham,//tensorflow/python/tools:aot_compiled_test fails to build on aarch64,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.3.0  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-04T10:55:07Z,type:build/install subtype: ubuntu/linux subtype:bazel awaiting PR merge,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57968,  ,Introduced by https://github.com/tensorflow/tensorflow/commit/09d49d9f6c9c13679ab8181d1b15480ea8a88da1,Hi  ! Thanks for the PR CC(Fix fail to build of aot_compiled_test on aarch64) . This issue will get closed once PR is merged.  Thank you!,Are you satisfied with the resolution of your issue? Yes No
966,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with saving a network)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu  Mobile device _No response_  Python version 3â€¤8â€¤10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When I try to save EfficietNetB0 from tf.keras.applications.efficientnet locally, I receive a TypeError. I got the same error with B1 too, so I assume there's an issue with the whole Efnet Family (and maybe some other models too) I had no issues with saving ResNet models from tf.keras.applications.  UPD: There's no such problem in tf 2.9.1   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MayStepanyan,Issue with saving a network,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu  Mobile device _No response_  Python version 3â€¤8â€¤10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When I try to save EfficietNetB0 from tf.keras.applications.efficientnet locally, I receive a TypeError. I got the same error with B1 too, so I assume there's an issue with the whole Efnet Family (and maybe some other models too) I had no issues with saving ResNet models from tf.keras.applications.  UPD: There's no such problem in tf 2.9.1   Standalone code to reproduce the issue   Relevant log output  ",2022-10-04T10:48:28Z,stat:awaiting response type:bug stale comp:apis comp:keras TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57967," I was able to replicate the issue, please find the gist here. Thank you!","Hi , You can save the model in H5 format without any issue. Please take a look at below code.  **Output** "," of course I can, but It doesn't solve the problem of not being able to save in tf format","Hi , Thanks for highlighting the issue.I have replicated the issue in both TF2.10 and tfnightly versions.Please refer the gist. Strange thing is that for this application`tf.keras.applications.efficientnet.EfficientNetB0()` , `model.save('file_path')` is throwing out error where as for Other general model its working fine without error. One more thing is that the model can be reloaded using `tf.saved_model.load('tmp/efnet') `without any error, where as `tf.keras.models.load_model('tmp/efnet')` throwing error .All the details attached in the gist above. It seems the issue is related to Keras API.Please post this issue on kerasteam/keras repo with necessary details so that it can be tracked from there To know more please see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
652,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.nn.conv2d_transpose abort with large `output_shape`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 18.04.4 LTS (x86_64)  Mobile device _No response_  Python version 3.7.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version N/A  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DNXie,tf.nn.conv2d_transpose abort with large `output_shape`,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 18.04.4 LTS (x86_64)  Mobile device _No response_  Python version 3.7.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version N/A  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-03T23:46:23Z,type:bug comp:ops awaiting PR merge TF 2.10,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57958,Added a PR CC( Fix crash in tf.nn.conv2d_transpose when output shape is invalid) for the fix.,Are you satisfied with the resolution of your issue? Yes No
985,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Performance Enhancements for Sparse Embedding Lookups)ï¼Œ å†…å®¹æ˜¯ (Introduces performance options for sparse embedding lookups that can appreciably speed up the training of recommendation systems. Sparse lookups alternatively accept inputs described by RaggedTensors which are more memory efficient. Performance is further increased by the optional use of a simplified and typically faster embedding lookup. In the sparse embedding micro benchmarks in tensorflow/python/eager/benchmarks_test.py, the number of examples per second on a DGX A100 system increases from approx. 1,300 with SparseTensor and without simplified lookup to approx. 11,200 with RaggedTensor inputs and simplified lookup (+760%). The combination of SparseTensor inputs and simplified lookup yields approx. 3,000 examples per second (+130%).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,philipphack,Performance Enhancements for Sparse Embedding Lookups,"Introduces performance options for sparse embedding lookups that can appreciably speed up the training of recommendation systems. Sparse lookups alternatively accept inputs described by RaggedTensors which are more memory efficient. Performance is further increased by the optional use of a simplified and typically faster embedding lookup. In the sparse embedding micro benchmarks in tensorflow/python/eager/benchmarks_test.py, the number of examples per second on a DGX A100 system increases from approx. 1,300 with SparseTensor and without simplified lookup to approx. 11,200 with RaggedTensor inputs and simplified lookup (+760%). The combination of SparseTensor inputs and simplified lookup yields approx. 3,000 examples per second (+130%).",2022-10-03T21:48:51Z,size:L comp:core,closed,0,28,https://github.com/tensorflow/tensorflow/issues/57956,"Thanks for the PR! This very much looks like RaggedTensors to me  https://www.tensorflow.org/api_docs/python/tf/RaggedTensor I'm not sure what the embedding_lookup support for RaggedTensor looks like but if thats available, would that be a viable alternative? ","I don't think ""compressed"" is the right term here.  A ""compressed"" sparse tensor signals to me something like compressed sparse row/column storage.  What we're talking about here is a very special kind of sparse tensor that seems specific to embeddings: (as Rohan mentioned) one which acts more like a ragged tensor. My suggestion is to either try to use RaggedTensor directly, or create a completely new type for this rather than modify `SparseTensor`.","`tf.nn.embedding_lookup` supports the `ids` argument to be a `RaggedTensor`, where all nonzero entries are leftaligned.  See https://www.tensorflow.org/guide/ragged_tensorexample_use_case for an example of using `RaggedTensor` with `embedding_lookup`. `RaggedTensor` also support `GradientTape.gradient` and `custom_gradient`. I would assume the gradient performance of a `RaggedTensor` could be better than a corresponding dense `Tensor` since the number of entries in a `RaggedTensor` is often less than that in a dense `Tensor`.",Thanks for the comments. The idea is to specifically accelerate embedding lookups based on SparseTensors. Existing lookups with RaggedTensors call `embedding_lookup` instead of `embedding_lookup_sparse`/`safe_embedding_lookup_sparse` and appeared to be substantially slower. The â€œcompressedâ€ keyword could be renamed. Introducing an entirely new datatype might be a substantial effort.,"> The â€œcompressedâ€ keyword could be renamed. Introducing an entirely new datatype might be a substantial effort. The issue is it's not _really_ a generic sparse tensor anymore  with ""compressed"" on, it's a severely restricted kind of sparse tensor  one with a very specific structure that really only comes up with embeddings.  You're really trying to squeeze in a different type into the existing `SparseTensor` class.  This should be an embeddingspecific type.","philipphack, thanks for your quick reply. I agree with cantonios on that this is not a generic sparse tensor use case.  Did you look into how slower it is with RaggedTensor or where is the performance bottleneck? I am asking because `embedding_lookup_sparse` also seems to call `embedding_lookup`: https://github.com/tensorflow/tensorflow/blob/359c3cdfc5fabac82b3c70b3b6de2b0a8c16874f/tensorflow/python/ops/embedding_ops.pyL521L522 So I assume the slow down is caused by another piece of code. One option here is to improve the performance of `embedding_lookup` with `RaggedTensor`. `RaggedTensor` also provides a `to_sparse` method if the downstream code wants to use `SparseTensor`."," in the current implementation, `embedding_lookup_sparse` does call `embedding_lookup`. However, depending on the value of the `allow_dense_grads` flag, this call (and the preceding unique call) is now avoided.","Slightly off topic since there were discussions over general sparse tensor usage. The sparse tensor input to `embedding_lookup_sparse` only represents sparsity caused by variable hotness (gather different number of rows for each sample), not the sparsity related to embedding. An alternative, and looks much more generic, way is to represent the sp_ids as a sparse matrix that multiply embedding table as matrix. Value can be omitted if combiner is not weighed, and is sp_weight if combiner is weighted. Pytorch's embedding bag follows this fashion, although in more explicit CSR format. I understand that would be a very big incompatible API change, just some thoughts.",Hi  Can you please review this PR ? Thank you!, I think we are deciding above not to accept this PR., the implementation is now based on RaggedTensor.,"philipphack, thanks for your work. I will start to read the code later today or tomorrow. ", Can you please check 's comments and keep us posted ? Thank you!, Any update on this PR? Please. Thank you!,Hi  Can you please review this PR ? Thank you!,PyLint seems to fail due to some format issues. Could you fix it? Thanks!,"Saw the following error in macos presubmit:  Trying to figure out why. Meanwhile, could you sync this PR to the head? Thanks!",Looks like a few test cases are broken: https://github.com/tensorflow/tensorflow/actions/runs/4029630014/jobs/6927701104,"Thanks, . FYI  is out this week, but he will respond once he gets back!",NNTest and TPUEmbeddingForServingTest were still calling `embedding_lookup_ragged`. The other test failures in the log don't seem to be related to this change.,"Philipp, could you fix the failed builds? I took a quick look. It seems we just need to run some bazel commands to update APIs. Let me know if you have any questions.",Saw error from `tensorflow/python/eager:gradient_input_output_exclusions_test`  Also error from `tensorflow/tools/api/tests:api_compatibility_test`  And from `tensorflow/tools/docs:tf_doctest`  I think you need to add `...` in the beginning of the continued line.,I updated goldens and the gradient exclusions. I still see a problem with the doctest which doesn't seem to be caused by this change.,Looks like the pywrap_gradient_exclusions.  It is unclear to me what is the cause. Maybe sync to master and rerun the script?,"A couple of new errors popped up. One of them is `AttributeError: 'SparseTensor' object has no attribute 'set_shape'`. Since the SparseTensor in the master branch does have `set_shape`, I guess it is related to rebase. Could you take a look at them? Thanks!",Finally the tests passed! Could you add some release notes to release note about ragged tensor support and sparse tensor improvement?,"by   > I think for the public API python API changes, the new argument should go at the end, otherwise it will break for users using positional arguments.  Can we move the allow_fast_lookup to the last argument? ",Continued in CC(Performance Enhancements for Sparse Embedding Lookups).
682,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error loading fashion_mnist dataset)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Microsoft Windows 10 Home ersion 10.0.19043 Build 19043  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA11.2.2 cuDNN8.1.0.77  GPU model and memory GeForce GTX 1070  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Urkchar,Error loading fashion_mnist dataset,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.10  Custom Code Yes  OS Platform and Distribution Microsoft Windows 10 Home ersion 10.0.19043 Build 19043  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA11.2.2 cuDNN8.1.0.77  GPU model and memory GeForce GTX 1070  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-10-03T07:35:22Z,stat:awaiting response type:bug comp:apis TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57945," I was able to execute the provided code in colab, could you please have a look at the gist here and confirm the same? Thank you!",I can confirm that gist you provided works in colab. What next? ,It seems like my installation of openssl was faulty or something. Installing it again seemed to fix it. , Thank you for the update! Could you please move this issue to closed status if it is resolved? Thank you!,Are you satisfied with the resolution of your issue? Yes No
795,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting ""OSError: [WinError 6] The handle is invalid"" after epoch ends in Tensorflow)ï¼Œ å†…å®¹æ˜¯ (I'm running a Unet model in Tensorflow with GPU enhancement. After the 2nd or 3rd epoch, I get several error messages stating: ""OSError: [WinError 6] The handle is invalid."" I did not have this issue come up when I ran it with CPU only. I'm using an Anaconda environment created with condaforge. These are the package/software versions I'm using: Python: 3.10.6, Tensorflow: 2.10.0, cudatoolkit 11.2, cudnn 8.1.0   Here is a sampling of the error message:  Does anyone know why this is happening, or what I can do to fix it?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Robert173,"Getting ""OSError: [WinError 6] The handle is invalid"" after epoch ends in Tensorflow","I'm running a Unet model in Tensorflow with GPU enhancement. After the 2nd or 3rd epoch, I get several error messages stating: ""OSError: [WinError 6] The handle is invalid."" I did not have this issue come up when I ran it with CPU only. I'm using an Anaconda environment created with condaforge. These are the package/software versions I'm using: Python: 3.10.6, Tensorflow: 2.10.0, cudatoolkit 11.2, cudnn 8.1.0   Here is a sampling of the error message:  Does anyone know why this is happening, or what I can do to fix it?",2022-10-02T02:46:33Z,stat:awaiting response type:support stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57937,  Can you please fill the issue template. Also please provide a code snippet to reproduce the issue reported here.  Thank you !,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
732,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AssertionError: Tried to export a function which references 'untracked' resource Tensor:tf.train.ExponentialMovingAverage)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,danilyef,AssertionError: Tried to export a function which references 'untracked' resource Tensor:tf.train.ExponentialMovingAverage,Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-10-01T10:54:36Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57930,  This issue seems to be a keras issue. Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
652,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Don't show correct formula )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jntdst,Don't show correct formula ,Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-09-30T11:04:14Z,type:docs-bug stat:awaiting response type:bug stale comp:keras,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57922,", Could you please feel free to submit a PR for the requested change to be made. The formula mentioned on the document looks good. Please provide the changes to be made for the formula. !image", I don't see any issue with the formula. Are you mentioning that it should be formatted? Thanks!!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,I fixed that in https://github.com/tensorflow/ranking/pull/335,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
608,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unit test float8_test fails to build on AARCH64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.13  Bazel version 5.3.0  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,elfringham,Unit test float8_test fails to build on AARCH64,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.13  Bazel version 5.3.0  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-30T10:03:59Z,type:build/install subtype: ubuntu/linux,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57918,  ,Introduced by https://github.com/tensorflow/tensorflow/commit/596560e345e55f8e19a9ad539ad5549c09c8c84f,"Thanks for pointing this out.  Strange, that path is only used if the following is false:  Which don't we have in the arm build?",// Does the compiler support C++11 math? // Let's be conservative and enable the default C++11 implementation only if we are sure it exists ifndef EIGEN_HAS_CXX11_MATH   if (EIGEN_ARCH_i386_OR_x86_64 && (EIGEN_OS_GNULINUX  EIGEN_OS_MAC))     define EIGEN_HAS_CXX11_MATH 1   else     define EIGEN_HAS_CXX11_MATH 0   endif endif,So yes the above will be false on AARCH64,Are you satisfied with the resolution of your issue? Yes No,"Thank .  That's wrong in Eigen, so I'll fix that upstream.  In the meantime, I put in a workaround that should fix it."
1086,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MoveNet Multipose lightning model cannot be converted to tflite int8 format)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 with WSL2  TensorFlow.js installed from (npm or script link): pip  TensorFlow.js version (use command below): 2.10.0  Browser version: N/A  Tensorflow.js Converter Version: Not sure how to get this **Describe the current behavior** I am using a python script to convert the MoveNet **multipose** lightning model to a tflite int8 format. I am getting this error message:  I have tried many solutions (including the one linked in the error), but I am still unable to compile the MoveNet model. **Describe the expected behavior** The MoveNet model should compile to a tflite int8 format. **Standalone code to reproduce the issue** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,benank,MoveNet Multipose lightning model cannot be converted to tflite int8 format,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 with WSL2  TensorFlow.js installed from (npm or script link): pip  TensorFlow.js version (use command below): 2.10.0  Browser version: N/A  Tensorflow.js Converter Version: Not sure how to get this **Describe the current behavior** I am using a python script to convert the MoveNet **multipose** lightning model to a tflite int8 format. I am getting this error message:  I have tried many solutions (including the one linked in the error), but I am still unable to compile the MoveNet model. **Describe the expected behavior** The MoveNet model should compile to a tflite int8 format. **Standalone code to reproduce the issue** ",2022-09-29T21:47:44Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.10,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57915,"Hi ! Sorry for the late response. Above issue can be resolved by adding select ops syntax during TFlite conversion. But I was not able to get a Int8 Quantized model with a lot of trials and would like to know from your end. You can check int8 model of Single pose model though here.  1, 2  Thank you!","Hi , thanks for your response! I tried adding all the select ops and still got some errors. It is now saying that `StridedSlice` is a custom op, so I added this line:  However, it still does not convert. I am now getting this error:  Do you have any ideas on how I can fix this? I have included my current code for reference below. Thanks!    Current code  ",Hi  ! Could you look at this issue. Thank you!,"Hi , any updates on this issue? Thanks!","Hi, In the above comment I did not see the inclusion of Select tf ops. You can try the below snippet in your code and let us know the outcome. Thanks.  Note that all ops are right now are not available in `INT8`.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1244,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Missing ops on using movenet/singlepose/lightning model )ï¼Œ å†…å®¹æ˜¯ (**System information**  Linux (armv7l) with Mali G31 GPU  TensorFlow installed from (source or binary): Source  TensorFlow version (or github SHA if from source): 2.6.0 Following is the output on trying to run the movenet singlpose lightning model (FP16  taken from TFHub) on the device:  On trying to run FP32 model available from TF Hub, following is observed:  **Standalone code to reproduce the issue**      Used `TfLiteGpuDelegateCreate `to create TF lite OpenGL delegate with default options and the model is built with `FlatBufferModel::BuildFromFile`.  Able to run the model though there are missing ops. The model runs slower with single digit FPS.  1. Could you please confirm if the aforementioned ops are not supported by TF lite? 2. Is it due to the use of OpenGL and/or Mali GPU, or it is unavailable for all backend/platforms? 3. In case if custom ops are to be implemented, are there any references (code segment/doc) you can suggest which implements ops using GPU with OpenGL backend?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,csn1800,Missing ops on using movenet/singlepose/lightning model ,"**System information**  Linux (armv7l) with Mali G31 GPU  TensorFlow installed from (source or binary): Source  TensorFlow version (or github SHA if from source): 2.6.0 Following is the output on trying to run the movenet singlpose lightning model (FP16  taken from TFHub) on the device:  On trying to run FP32 model available from TF Hub, following is observed:  **Standalone code to reproduce the issue**      Used `TfLiteGpuDelegateCreate `to create TF lite OpenGL delegate with default options and the model is built with `FlatBufferModel::BuildFromFile`.  Able to run the model though there are missing ops. The model runs slower with single digit FPS.  1. Could you please confirm if the aforementioned ops are not supported by TF lite? 2. Is it due to the use of OpenGL and/or Mali GPU, or it is unavailable for all backend/platforms? 3. In case if custom ops are to be implemented, are there any references (code segment/doc) you can suggest which implements ops using GPU with OpenGL backend?",2022-09-29T14:17:39Z,stat:awaiting response stale comp:lite type:others TFLiteGpuDelegate 2.6.0,closed,0,17,https://github.com/tensorflow/tensorflow/issues/57907,"Hi  ! Sorry for the late response. In case , It does not find any supported ops for gpu delegate then it can fall back to other delegate like nnapi ,xnnpack then to cpu delegate. Please refer below thread to set the above fall back policy. Reference  fallback policy , benchmarking Have you checked the performance in 2.9/2.10 version yet? Thank you!"," Thanks for the response. I understand that there is a fallback to CPU and hence the instructions are running on CPU instead of GPU. With the experiments I have tried with the same model on CPU and GPU, it is seen that the CPU only version has 25% less frame rate than that of GPU version. With more instructions running on GPU it should be able to get a latency improvement. So, could you please provide answers to specific questions: 1. Could you please confirm if the aforementioned ops are not supported by TF lite? 2. Is it due to the **use of OpenGL and/or Mali GPU**, or it is unavailable for all backend/platforms? 3. In case if custom ops are to be implemented, are there any references (code segment/doc) you can suggest which implements **custom ops using GPU with OpenGL backend**?"," ! Cast, GatherNd and Unpack are in Select ops allow list . GPU acceleration won't work even if you follow the select ops syntax. For Custom OP impementation, Please refer this document here. Alternatively, You can check the performance on XNNPack / NNAPi delegates through Benchmarking.  Thank you!","Thanks   for the clarifications. > Cast, GatherNd and Unpack are in Select ops allow list . GPU acceleration does not work even if you follow the select ops syntax So, you mean to say these ops are not supported on all GPUs (and backends)?  Could you please suggest a way to make these unsupported ops supported on GPUs (with OpenGL backend), if it is there? Will creating custom ops under custom_parsers.cc, inheriting the NodeShader and overriding the GenerateCode be helpful? > Alternatively, You can check the performance on XNNPack / NNAPi delegates through Benchmarking. I assume NNAPI works only on Android platform, is that correct? Or is there a way to use NPU on Linux ARM platform? The benchmark tool (linux_arm) works for CPU (xnnpack, threads etc.) but not for GPU on Linux ARM platform. Still, is there a way to get benchmarking performed on Linux ARM Platform with Mali GPU?", ! Could you look at this query. Thank you!, Could you please have a look at the above query?,"Yes, NNAPI is specific to android platform. For details on delegates registering can be found in below code files. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.h https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/metal_delegate.h"," Thank you for the response. Could you please also address the other question I had, quoting it below: > Could you please suggest a way to make these unsupported ops supported on GPUs (with OpenGL backend), if it is there? Will creating custom ops under custom_parsers.cc, inheriting the NodeShader and overriding the GenerateCode be helpful?", /  could you please look into the above query?,"Hi   As this comment suggests, Movenet didn't support GPU and it's nontrivial to do it. Can you try BlazePose as suggested here and let us know if that works for your case? Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1236,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Lite does not support tf.math.rsqrt() operation conversion.)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit  TensorFlow installed from (source or binary): Python 3.8.2, pip: 22.1, pip package  TensorFlow version (or github SHA if from source): TF version:'2.9.0' **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.  code:  Also, please include a link to a GraphDef or the model if possible. **Any other info / logs** tried to create  a single rsqrt operation containing model saved its .h5 with no problem and then convert it to its int8 .tflite but got the above error. netron of hdf5 save is as follows !image Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Saar-Ken-Ji,Tensorflow Lite does not support tf.math.rsqrt() operation conversion.,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit  TensorFlow installed from (source or binary): Python 3.8.2, pip: 22.1, pip package  TensorFlow version (or github SHA if from source): TF version:'2.9.0' **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.  code:  Also, please include a link to a GraphDef or the model if possible. **Any other info / logs** tried to create  a single rsqrt operation containing model saved its .h5 with no problem and then convert it to its int8 .tflite but got the above error. netron of hdf5 save is as follows !image Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-09-29T09:34:42Z,type:bug comp:lite TFLiteConverter TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57906,"Hi KenJi ! In Colab , Session was crashing in Colab with 2.9 .  rsqrt in select ops list. So could you try with below syntax in lite conversion. ","Hi , Thank you for the quick response and your help on this. Does this mean that the tflite default int8 builtins don't support `rsqrt` and we need to use `SELECT_TF_OPS` for converting a model which has `rsqrt` ? Please confirm.",Hi    thank you for the timely response . i have done as you have advised. but the error remains as follows. ,"KenJi ! It seems rsqrt has also been added in built in ops along select ops allow list.  Could you skip the representative dataset and let us know after trying  with inference input and output as float 32 type .  To debug quantization issues (Delist quantization for problematic layers) , You can use quantization_debugger . Thank you!","hello thank you for your suggestion, i have done as you have advised.  I was able to get the default quantization result, with input and output as float32 as you had suggested. but what i require is the int8 quantization of the same. is it not possible to achieve that? note: using the quantization_debugger also produced the same error as mentioned before as the debugger also require representative data set even in case of default quantization. so is it an issue of not being able to process that in the module?",KenJi ! Could you share your quantization debugger results error here.  Thank you!,"  the bellow given is the error when i try default quantization with debuggers debug data set  parameter assigned with the representative data set from the code above 20220930 14:10:35.736988: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220930 14:10:36.286672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2807 MB memory:  > device: 0, name: NVIDIA GeForce MX230, pci bus id: 0000:01:00.0, compute capability: 6.1 WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. C:\Users\DELL\AppData\Roaming\Python\Python38\sitepackages\tensorflow\lite\python\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn(""Statistics for quantized inputs were expected, but not "" 20220930 14:10:36.730492: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. 20220930 14:10:36.730963: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. 20220930 14:10:36.733999: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: C:\Users\DELL\AppData\Local\Temp\tmp_b1f8opg 20220930 14:10:36.735469: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve } 20220930 14:10:36.735881: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:\Users\DELL\AppData\Local\Temp\tmp_b1f8opg 20220930 14:10:36.739485: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 20220930 14:10:36.740029: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle. 20220930 14:10:36.755296: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\Users\DELL\AppData\Local\Temp\tmp_b1f8opg 20220930 14:10:36.770024: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 36022 microseconds. 20220930 14:10:36.784119: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. 20220930 14:10:36.802819: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1989] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32 error: illegal scale: INF ",Ok KenJi ! Thanks for the update.  ! Could you look at this issue. Thank you!,"`RSQRT` is supported as builtin operator, could you please check the issue at https://github.com/tensorflow/tensorflow/issues/58048. Also, refer the comment here for the code file reference to the registered `RSQRT` builtin op.",  thank you for the response. as per the discussion. I have gone through the documentation of the rsqrt() and observed that the issue was that the operator couldn't handle negative values  as input so adding a tf.math.abs() before rsqrt solved the issue. thank you very much for your guidance.,Are you satisfied with the resolution of your issue? Yes No
1863,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(representative_dataset error for TFlite converter quantization)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  Colab environment  Quantization of yolov5s weights  Successfully generated tflite files with,       No quantization       Floating Point 16       Dynamic range (no representative_dataset option)  Fails when representative_dataset option is provided    2. Code  https://github.com/tensorflow/tensorflow/issues/30861 tf.executing_eagerly() train = [] path = '/currdrive/road_hero/android_figurine/validate/' for i in range(1, 11):     filename = os.path.join(path, 'IMG_0501.jpg')     im = cv2.imread(filename)     im = im.astype(np.float32, copy=False)     input_image = im  mc.BGR_MEANS     train.append(im) train = tf.convert_to_tensor(np.array(train, dtype='float32')) my_ds = tf.data.Dataset.from_tensor_slices((train)).batch(1) def representative_data_gen():     for input_value in my_ds.take(10):         yield [input_value] file_path = './yolov5s_saved_model' tf_lite_converter = tf.lite.TFLiteConverter.from_saved_model(file_path, signature_keys=['serving_default']) tf_lite_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] tf_lite_converter._experimental_lower_tensor_list_ops = False tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT] tf_lite_converter.representative_dataset = representative_data_gen tflite_model = tf_lite_converter.convert() TF_LITE_MODEL_FILE_NAME = ""dynamic2.tflite"" with tf.io.gfile.GFile(TF_LITE_MODEL_FILE_NAME, 'wb') as f:   f.write(tflite_model)  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab 2) Attaching the colab file for your reference. yolov5_quantize.zip 3) A)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ra9hur,representative_dataset error for TFlite converter quantization," 1. System information  Colab environment  Quantization of yolov5s weights  Successfully generated tflite files with,       No quantization       Floating Point 16       Dynamic range (no representative_dataset option)  Fails when representative_dataset option is provided    2. Code  https://github.com/tensorflow/tensorflow/issues/30861 tf.executing_eagerly() train = [] path = '/currdrive/road_hero/android_figurine/validate/' for i in range(1, 11):     filename = os.path.join(path, 'IMG_0501.jpg')     im = cv2.imread(filename)     im = im.astype(np.float32, copy=False)     input_image = im  mc.BGR_MEANS     train.append(im) train = tf.convert_to_tensor(np.array(train, dtype='float32')) my_ds = tf.data.Dataset.from_tensor_slices((train)).batch(1) def representative_data_gen():     for input_value in my_ds.take(10):         yield [input_value] file_path = './yolov5s_saved_model' tf_lite_converter = tf.lite.TFLiteConverter.from_saved_model(file_path, signature_keys=['serving_default']) tf_lite_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] tf_lite_converter._experimental_lower_tensor_list_ops = False tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT] tf_lite_converter.representative_dataset = representative_data_gen tflite_model = tf_lite_converter.convert() TF_LITE_MODEL_FILE_NAME = ""dynamic2.tflite"" with tf.io.gfile.GFile(TF_LITE_MODEL_FILE_NAME, 'wb') as f:   f.write(tflite_model)  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab 2) Attaching the colab file for your reference. yolov5_quantize.zip 3) A",2022-09-29T06:59:10Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TF 2.9,closed,0,15,https://github.com/tensorflow/tensorflow/issues/57904,Hi  ! Could you also open the access to above colab notebook .  Thank you!,"Hi  , have given you the access, please check.","Hi  ! Sorry for the late response.  A Representative data set is normally used for integer quantization for deploying in integer only accelerated hardware like edge TPU.  For Dynamic range quantisation , Procedure is to go with default converter without using any optimization flag . I suggest you to follow the syntax with Posttraining integer quantization to avoid any run time issues. Thank you!","Hi  , thanks for the analysis. I just chose ""TPU"" as runtime option in Colab, still get similar error. Need your help to verify, this is really not hardware related.  error log Importing a function (__inference_pruned_6862) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.  RuntimeError                              Traceback (most recent call last) [](https://localhost:8080/) in       41 tf_lite_converter.representative_dataset = representative_data_gen      42  > 43 tflite_model = tf_lite_converter.convert()      44       45 TF_LITE_MODEL_FILE_NAME = ""dynamic2.tflite"" 11 frames /usr/local/lib/python3.7/distpackages/tensorflow/lite/python/optimize/calibrator.py in _feed_tensors(self, dataset_gen, resize_input)     127                                      signature_key)     128           else: > 129             self._calibrator.Prepare([list(s.shape) for s in input_array])     130         else:     131           if signature_key is not None: RuntimeError: tensorflow/lite/kernels/concatenation.cc:158 t>dims>data[d] != t0>dims>data[d] (189 != 40)Node number 123 (CONCATENATION) failed to prepare.","Hi  ! Colab TPU and Edge TPU have different configuration. So It might be hardware specific. Different behaviour has been seen for same code has been between two different architectures.  (CPU and GPU, Linux and Windows etc.) Can we mark this query as resolved then. Thank you!","Hi  , I am stuck at the same step on two different problems. Does that mean that we cannot execute below steps in colab notebook at all in spite of using TPU runtime ? 1. Cannot we generate dynamic range quantization weights with representative_dataset ? 2. This is next step to point 1 above. Can we not generate int8 quantization weights ? even this would involve representative_dataset. If you have seen issues closed with the reason that weights cannot be generated with representative_dataset using colab notebook, please close this issue. Also request you to enclose those issue ids as reference. That would serve as an info , if someone had sighted alternate ideas in the discussion trail.",! Could you look at this issue. Thank you!,",  Any update on this issue ?",  any update please ?,", ,    It has been more than 3 weeks. Can someone please respond ?","Hi   The representative dataset seems to be incorrectly passed to the generator. I have tested with 10 images with shape [640, 640, 3] and was able to succesfully convert it into the TFLite format.  Please find the gist here. Thanks."," , thanks for reinitiating. It has been a while since I have worked on this code. Will check and confirm.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
675,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(why my 'workers=4,use_multiprocessing=True' do not work while training?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,renguote,"why my 'workers=4,use_multiprocessing=True' do not work while training?",Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-09-29T03:15:46Z,stat:awaiting response type:bug comp:keras TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57903,!image,", Tensorflow version 2.4 is not actively supported. Hence, kindly update to the latest stable version 2.10 and let us know if you are facing the same issue. Thank you!","Still the same issue  when I updated to tensorflow==2.10.0.  Looking forward to your reply, Thank you!",", Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",Thank you!,", Could you please close this issue, since it is already being tracked there?  Thank you!",Are you satisfied with the resolution of your issue? Yes No
813,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.image.resize crash with abort when `antialias` is set to True)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0dev20220921  Custom Code No  OS Platform and Distribution Ubuntu 18.04.4 LTS (x86_64)  Mobile device _No response_  Python version 3.7.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version N/A  GPU model and memory _No response_  Current Behaviour? `tf.image.resize` crash with abort when `antialias` is set to True. It doesn't not abort when `antialias=False` Also reproduced in this gist  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DNXie,tf.image.resize crash with abort when `antialias` is set to True,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0dev20220921  Custom Code No  OS Platform and Distribution Ubuntu 18.04.4 LTS (x86_64)  Mobile device _No response_  Python version 3.7.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version N/A  GPU model and memory _No response_  Current Behaviour? `tf.image.resize` crash with abort when `antialias` is set to True. It doesn't not abort when `antialias=False` Also reproduced in this gist  Standalone code to reproduce the issue   Relevant log output  ,2022-09-28T20:12:29Z,stat:awaiting tensorflower type:bug comp:ops awaiting PR merge TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57897,Hi  ! Could you look at this issue. Attached gist in 2.9 and 2.10 and nightly for reference. Thank you!,Created a PR CC(Fix crash in tf.image.resize when antialias is True and size is large) for the fix.,Are you satisfied with the resolution of your issue? Yes No
1613,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`Nan` errors multiple-gpu training with `MirroredStrategy` - RTX A5000 PCIe4.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA11.0 & cuDNN8.1.0  GPU model and memory RTX A5000 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output shell CUDA_VISIBLE_DEVICES=""0"" python nan_error_tf.py [... logs ...] iteration=0 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 2.3019003868103027 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False} iteration=1 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 2.2981200218200684 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False} iteration=2 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 2.0697689056396484 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False} iteration=3 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 1.8743340969085693 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False}  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,JGuillaumin,`Nan` errors multiple-gpu training with `MirroredStrategy` - RTX A5000 PCIe4.0,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA11.0 & cuDNN8.1.0  GPU model and memory RTX A5000 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output shell CUDA_VISIBLE_DEVICES=""0"" python nan_error_tf.py [... logs ...] iteration=0 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 2.3019003868103027 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False} iteration=1 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 2.2981200218200684 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False} iteration=2 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 2.0697689056396484 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False} iteration=3 	 nan in per_replica_images from dataset :  {0: False} 	 averaged_loss = 1.8743340969085693 	 nan in per_replica_images              :  {0: False} 	 nan in per_replica_per_sample_losses   :  {0: False}  ",2022-09-28T17:53:42Z,type:bug comp:dist-strat TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57893,"Hi , I got `Nan` in iteration=0, looks like memory allocation exceeded. Could you confirm the issue by seeing below log. ",I think it's due to your GPU size. You should reduce the `batch_size_per_replica` to 32 or 16.  Here is a full log with 2 GPUs :  ,"After testing many stuff (update kernel, remove some GPUs (keep only 2, 4 or 8), compiling from source, playing with many obscure Nvidia env variables, ...) I realized that I had the same issue with pytorch code.  After digging more into NCCL issues, I found this one and this documentation page from NCCL.  > IO virtualization (also known as, VTd or IOMMU) can interfere with GPU Direct by redirecting all PCI pointtopoint traffic to the CPU root complex, causing a significant performance reduction or even a hang. I had to disable ACS from the BIOS. Now everything is working well.  Thank you for your help.  I wish you a great week",Are you satisfied with the resolution of your issue? Yes No
660,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MultiWorkerMirroredStrategy overhead time)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,surajitkundu-dazn,MultiWorkerMirroredStrategy overhead time,Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-09-28T14:19:49Z,stat:awaiting response stale comp:dist-strat type:performance TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57890,Hi  ! Could you look at this issue. Thank you!,"Hi dazn, Thanks for this issue. Could you please share complete code to reproduce the issue. Thank you!",Model Training Code,"Hi dazn, I am unable to access the link. Thank you!","> Hi dazn, I am unable to access the link. Thank you! Can you please try now","Hi dazn, Still I am unable to access the given link.  ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to build TensorFlow from source with Clang?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version 5.1.1  GCC/Compiler version clang 14  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ganler,How to build TensorFlow from source with Clang?,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version 5.1.1  GCC/Compiler version clang 14  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-27T19:01:34Z,stat:awaiting response stat:awaiting tensorflower type:feature type:build/install stale subtype: ubuntu/linux TF 2.10,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57872,This seems to be related to:  https://github.com/tensorflow/tensorflow/issues/55563  https://github.com/bazelbuild/bazel/issues/15359 I tried all of the mentioned fixes but none of them work...,Hi  ! Could you look at this issue. Thank you!,add `spawn_strategy=sandboxed` solved this problem for me,Hi  ! Command seemed  to be working after excluding clang support in configure.py in master branch and Bazel 5.3.0. Including clang support in configure.py threw below error in colab. `ERROR: Config value 'download_clang' is not defined in any .rc file ` Did you check 's above comment too on testing with. ` spawn_strategy=sandboxed ` flag as workaround. Attached gist for reference. But  Adding clang support is still a valid  feature request (currently experimental in 2.11 version) to me. Thank you!,"am i to take: >But Adding clang support is still a valid feature request (currently experimental in 2.11 version) to me.   as saying that clang builds are not supported, even though ./configure asks if you want to use clang? i just spent 2 hours trying to figure out why bazel repeatedly says ""this rule is missing dependency declarations for the following files included by"" over, and over, and over, for different files. I was apt installing everything it was complaining about, which requires clean expunge (and sometimes rm rf ~/.cache/bazel/ Can someone confirm or disconfirm if clang is supported?",Hi  ! Thanks for the update. Clang is supported and might still be experimental . Could you share the stacktrace or gist to replicate the above behavior. Thank you!,"> Hi  ! Thanks for the update. Clang is supported and might still be experimental . Could you share the stacktrace or gist to replicate the above behavior. >  > Thank you! Sure, is the stacktrace everything after the last command executed? or is there a file or command line switch?","Hi  , Could you please try adding the below flag to the bazel command. https://github.com/tensorflow/tensorflow/blob/9ae7af468a392c787acb7b7864e545d48569f79d/.bazelrcL332L333 Also AFAIK clang will be used for building CUDA kernels. You can find the supported configurations at .bazelrc file from source code. Please have a look at the supported flags and try the build again and let us know if the problem still persists. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
971,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.experimental.numpy.isposinf` has wrong output when input is integer.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When integer tensor with +/ inf values, is passed to `tf.experimental.numpy.isposinf`, it has different results on cpu/cuda, and is different from numpy outputs.  In the example, the input is [inf,0,inf] for `int32` data type. On cpu, `tf.experimental.numpy.isposinf` wrongly output `True` for `inf`.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,creakseek,`tf.experimental.numpy.isposinf` has wrong output when input is integer.,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When integer tensor with +/ inf values, is passed to `tf.experimental.numpy.isposinf`, it has different results on cpu/cuda, and is different from numpy outputs.  In the example, the input is [inf,0,inf] for `int32` data type. On cpu, `tf.experimental.numpy.isposinf` wrongly output `True` for `inf`.  Standalone code to reproduce the issue   Relevant log output  ",2022-09-27T13:47:04Z,stat:awaiting response type:bug stale comp:ops TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57863,  I was able to reproduce the issue on Colab using TF v2.10. Please find the gist here for reference. Thank you!,"This is due to the number `2147483648` is out of `int32` max range,  when you change the number within the int32 max range for example `2147483646` it provides the desired output. I'm not sure how this is handled in `numpy` implementation.  ``` cpu: tf.Tensor([False False False], shape=(3,), dtype=bool) cuda: tf.Tensor([False False False], shape=(3,), dtype=bool) numpy: [False False False]","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
687,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ERROR: Node number 7 (FlexRaggedTensorToTensor) failed to prepare.)ï¼Œ å†…å®¹æ˜¯ (  Error Must call allocateTensors(). ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select Use instruction https://www.tensorflow.org/lite/guide/ops_select)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,DemoCodeProfile,ERROR: Node number 7 (FlexRaggedTensorToTensor) failed to prepare.,"  Error Must call allocateTensors(). ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflowliteselecttfops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select Use instruction https://www.tensorflow.org/lite/guide/ops_select",2022-09-27T10:56:43Z,stat:awaiting response type:support stale comp:lite TFLiteConverter,closed,0,10,https://github.com/tensorflow/tensorflow/issues/57860,Hi  ! Could you confirm that you have used select ops syntax and declared the select ops dependency in build.gradle  file. Please share a sample code snippet for the model to replicate this issue. Thank you!,"This is iOS I use pods     pod 'TensorFlowLiteSwift'     pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1nightly' And add flag to build settings "," ! Could you let us know your Tensorflow version . If version is less than 2.9 , the force load syntax should be  `force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps ` Thank you!",Version 2.10.0 For this version correct flag, ! Would it be possible to share a minimal code or toy model to replicate this issue. Thank you!,https://github.com/DemoCodeProfile/models_tf/blob/main/model_old.tflite Thanks you!,Hi   The CocoaPods specs for TFLite has been updated with commit https://github.com/tensorflow/tensorflow/commit/d9c5c16bb7b24e2caea426f2bac167ae9df14225. Could you please check in latest nightly version and let us know if you are still facing the issue? Thanks.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1002,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TFTRT]: Deprecate ImplicitBatchModeCompatible dynamic shape strategy)ï¼Œ å†…å®¹æ˜¯ (This strategy is intended for ease of use for people familiar with the implicit batch mode profile, in that it does not require the user to specifically call build() before trying to run an inference with TFTRT converted graph. Since this is actually a dynamic shape mode, and input shapes are required in dynamic shape mode for TensorRT profile generation; this mode makes some educated guesses for minimum and maximum shapes for inputs the TensorRT engine. This has proven to be buggy for models that include transpose and reshape operations, among others. Due to the above, and since dynamic shape mode requires users to call build() with the correct input shapes to generate TensorRT profiles correctly, this mode is being deprecated. cc:    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,meena-at-work,[TFTRT]: Deprecate ImplicitBatchModeCompatible dynamic shape strategy,"This strategy is intended for ease of use for people familiar with the implicit batch mode profile, in that it does not require the user to specifically call build() before trying to run an inference with TFTRT converted graph. Since this is actually a dynamic shape mode, and input shapes are required in dynamic shape mode for TensorRT profile generation; this mode makes some educated guesses for minimum and maximum shapes for inputs the TensorRT engine. This has proven to be buggy for models that include transpose and reshape operations, among others. Due to the above, and since dynamic shape mode requires users to call build() with the correct input shapes to generate TensorRT profiles correctly, this mode is being deprecated. cc:    ",2022-09-27T00:02:35Z,awaiting review ready to pull size:XS comp:gpu:tensorrt,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57851
1895,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom Op failure due to Status symbol resolution in docker image tensorflow/tensorflow:2.10.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution host: ubuntu 20.04, docker image: tensorflow/tensorflow:2.10.0gpu   Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version g++ (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   1) run an interactive terminal and share the files as a volume with image 2.9.1, compile the op, and run it through python: docker run v$HOME/src/op_failure:/op rm it tensorflow/tensorflow:2.9.1gpu cd /op TF_CFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') ) TF_LFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') ) g++ std=c++14 shared kernel_example..so fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} O2 python c 'import tensorflow as tf; example_module = tf.load_op_library(""./example.so""); print(example_module.example([[1, 2], [3, 4]]).numpy())' yields a successful output: [[2 4]  [6 8]] 3) Do the same exact set of commands with image 2.10.0: docker run v$HOME/src/op_failure:/op rm it tensorflow/tensorflow:2.10.0gpu cd /op TF_CFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') ) TF_LFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') ) g++ std=c++14 shared kernel_example..so fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} O2 python c 'import tensorflow as tf; example_module )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,matthew-pickett,Custom Op failure due to Status symbol resolution in docker image tensorflow/tensorflow:2.10.0,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution host: ubuntu 20.04, docker image: tensorflow/tensorflow:2.10.0gpu   Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version g++ (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   1) run an interactive terminal and share the files as a volume with image 2.9.1, compile the op, and run it through python: docker run v$HOME/src/op_failure:/op rm it tensorflow/tensorflow:2.9.1gpu cd /op TF_CFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') ) TF_LFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') ) g++ std=c++14 shared kernel_example..so fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} O2 python c 'import tensorflow as tf; example_module = tf.load_op_library(""./example.so""); print(example_module.example([[1, 2], [3, 4]]).numpy())' yields a successful output: [[2 4]  [6 8]] 3) Do the same exact set of commands with image 2.10.0: docker run v$HOME/src/op_failure:/op rm it tensorflow/tensorflow:2.10.0gpu cd /op TF_CFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') ) TF_LFLAGS=( $(python c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') ) g++ std=c++14 shared kernel_example..so fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} O2 python c 'import tensorflow as tf; example_module ",2022-09-26T21:34:20Z,stat:awaiting response type:bug stale comp:ops comp:core TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57847,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1870,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to implement state with dynamic shape? How about a VariableArray?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I try to build a module that has dynamic shaped state and I struggle to do it in Tensorflow with minimal changes to the models. I have a custom training pipeline and build the model mostly from scratch. My usecase is a Transformer encoderdecoder model used for machine translation. In search it is enough to apply the encoder once, and the decoder for every new input, except for the decoder selfattention module. This module needs to work on the current input and all previous. Since these models are basically nested modules it would be annoying to pass the state as an argument to __call__ functions. I would need to modify many lines of code and, if I want to use the same search code for models without state, have an unused argument. A better solution would be to have the state as a member variable which can be cleared/reshaped from the outside. As far as I know tf.Variables are the only object capable of being a member variable? But since they have fixed shape after construction I cannot use them. Since TensorFlow is huge, I dont know if there are already features or tricks that can do a dynamically shaped state. Does anyone know anything about this? If it is not possible, my feature request is a VariableArray that basically wraps a Tenso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,flixxox,How to implement state with dynamic shape? How about a VariableArray?,"Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I try to build a module that has dynamic shaped state and I struggle to do it in Tensorflow with minimal changes to the models. I have a custom training pipeline and build the model mostly from scratch. My usecase is a Transformer encoderdecoder model used for machine translation. In search it is enough to apply the encoder once, and the decoder for every new input, except for the decoder selfattention module. This module needs to work on the current input and all previous. Since these models are basically nested modules it would be annoying to pass the state as an argument to __call__ functions. I would need to modify many lines of code and, if I want to use the same search code for models without state, have an unused argument. A better solution would be to have the state as a member variable which can be cleared/reshaped from the outside. As far as I know tf.Variables are the only object capable of being a member variable? But since they have fixed shape after construction I cannot use them. Since TensorFlow is huge, I dont know if there are already features or tricks that can do a dynamically shaped state. Does anyone know anything about this? If it is not possible, my feature request is a VariableArray that basically wraps a Tenso",2022-09-26T11:09:50Z,stat:awaiting response type:feature stale comp:autograph TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57843,", The problem might be because preprocessing was returning an array instead of a tuple that is required in the graph. Could you please take a look at this comment from the developer and the issue with the similar error. Thank you!",Sorry I cannot see where the linked issues relate to my issue. Where do I return an array instead of a tuple?  I did not append the full error message. Sorry for that. I updated the issue with the full traceback. Does that change something?,", I was able to reproduce the issue on tensorflow v2.8, v2.10 and nightly. Kindly find the gist of it here.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build smaller AAR files locally (tensorflow-lite.aar, tensorflow-lite-select-tf-ops.aar) fail.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version 5.3.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,farmaker47,"Build smaller AAR files locally (tensorflow-lite.aar, tensorflow-lite-select-tf-ops.aar) fail.",Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version 5.3.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-26T05:51:29Z,stat:awaiting response type:bug comp:lite TF 2.8,closed,0,20,https://github.com/tensorflow/tensorflow/issues/57840,Build of tensorflowlite.aar is successful but fails when it tries to build tensorflowliteselecttfops.aar,"Hi  ! Just making Sure things while I am replicating this issue.  For getting a Select ops aar file , TFLite model should involve select ops syntax. Ref Could you use two TF lite model in Bash script (one with Select ops syntax and one with default optimization) .  I also suspect you are using Python 3.6 with TF 2.8 . Could you confirm whether you are using Python 3.7/3.10. Please check in Bazelbin folder once you completed running the script.   Thank you!","Hi   Yes, the model I am using has select tf ops specifically FlexDecodewav. If you see at the logcat of the first cell Python 3.7 is used. Do you want any other detail? Thanks","Hi  ! It is not running the bash script properly and  looks like environment issue from below line. `from tensorflow.python._pywrap_tensorflow_internal import * ` Quick suggestions: 1. clean the previous build ` bazel clean expunge` 2. For 2.8 , You have to checkout with r2.8 branch and build with Bazel 4.2.1 (tested config) .   Could you confirm the machine type too (windows/linux/macos) Thank you!","Hi   The environment is Linux (Colab). I am connecting the colab with a Google Cloud VM instance since this procedure needs much time and high CPU, RAM (Instance is 24 CPUs and 140GB of Ram). I always build with new VM instance so no need of bazel clean up. I will try r2.8 branch and 4.2.1 Bazel and get back to you later. Thanks","Hi   Yes, the code snippet  did the trick! Build completed successfully! By checking out branch r2.8, this forced Baselisk to download and install Bazel version 4.2.1.   Then everything finished OK! I have to verify that the 2 .aar files that were generated are OK inside the android project. Thanks for the help. You can close the issue.",ok  ! Thanks for the confirmation. Glad you were able to resolve this issue. Thank you!,Are you satisfied with the resolution of your issue? Yes No,"Hi   Unfortunately the 2 generated files do not work inside android. I am getting ""E/om.vicom.teeja: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__) "" Any hints?", ! Could you confirm that you  installed the aar files to your local maven repo and declared the dependency in build.gradle file too.  Reference. Thank you!,Hi   There are various ways to add these .aar files inside an android project. I choosed [this].(https://developer.android.com/studio/projects/androidlibrarypsdaddaarjardependency) I have done this again in the past. What I am noticing during this build with branch 2.8 and Bazel 4.2.1 is that it processes .cpp and . .so files as it was doing with master branch and Bazel 5.3.1. Do you think this has something to do with the issue? Thanks, Also I see that with the 2.8 branch it uses cxxopt='std=c++14' to build the targets inside the build_aar.sh file.  At master branch it has ' cxxopt='std=c++17'',"Ok  ! Thanks for the update. Could you give a try once with the 2.9/2.10 version , Bazel 5.0.0 and procedure mentioned in above comment. Yes , the C++ version has been recently updated after 2.10. Thank you!",Keeping this thread open until it is properly resolved.  ! Could you look at this issue. Thank you!,For other people looking at this issue and building with branch 2.8 and Bazel 4.2.1. It seems that if you navigate inside the tensorflow_src/tensorflow foder and build with  everything compiles OK and the .aar file works OK inside android. BUT you have to add also tensorflowliteapi.aar file as a dependency from here https://repo1.maven.org/maven2/org/tensorflow/tensorflowliteapi/2.8.0/.  IMPORTANT you still have to add   if you have a tflite file with TensorFlow ops. The above in this comment does not solve the primary issue which is that I do not get working .aar files with , It seems that with branch r2.8 and configuration  instead of  I have a working tensorflowlite.aar file Now I am waiting for the tensorflowliteselecttfops.aar to verify all together.,Yes  ! You have to skip some flags while doing a selective build . Please update after your test. Thank you!,"Yes  I confirm that I have both .aar files that work fine inside my project! To recap for future developers. I have created a procedure to build tensorflowliteselecttfops.aar with Colab as I think this is easier than building locally with or without Docker. This has the drawback that you need high power CPU colab and you have to switch to Colab Pro or connect your runtime to a Google Cloud Platform VM instance with at least 24 CPUs and 100GBs of RAM. I had to switch to branch r2.8 and install Bazel 4.2.1 to have a successful build. Also for my phone Xiaomi Note 6 Pro I had to build for all artifacts x86,x86_64,arm64v8a,armeabiv7a and not only arm64v8a,armeabiv7a (which still is a mystery for me). The final command was  Now the final .apk is 45MB and used to be 200MB. Also with this procedure you do not have to add the tensorflowliteapi.aar file inside the project. This Colab notebook will be added to my Github account after polishing it.  Thank you for your patience."," , If the issue is resolved, could you please close the issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No
766,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Conv2D layer fails to run with XLA on CUDA)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0.dev20220921, 2.8.2  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue https://colab.research.google.com/drive/1o9NX4ZzhyuhBSI5MFXREgaBOxrCOpuM?usp=sharing Reproduced on 2.8.2, 2.11.0.dev20220921  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Co1lin,Conv2D layer fails to run with XLA on CUDA,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0.dev20220921, 2.8.2  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue https://colab.research.google.com/drive/1o9NX4ZzhyuhBSI5MFXREgaBOxrCOpuM?usp=sharing Reproduced on 2.8.2, 2.11.0.dev20220921  ",2022-09-26T03:32:38Z,stat:awaiting response type:bug stale comp:xla TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57838,", I tried to execute the mentioned code **with tf.device('GPU:0')** and it was executed without any issue/error. Kindly find the gist of it here. Thank you!","That's weird. The issue only happens when using GPU comes after using CPU, and with this Conv2D operator (using other operator will not lead to error). But I think it's worth looking into it, because we have the case that you have to switch between CPU and GPU for the same model, like in reinforcement learning pipeline. Thanks!",", I tried to execute the code with alternative approaches and it was failing when the code consists of both GPU and CPU. In other cases it was executed without any issue. Kindly find the gist of it here."," Thanks! I have found this workaround, but just as I say, in some cases like reinforcement learning, it's common to have both CPU and GPU inference in one process/code. Since having both CPU and GPU is ok for other operators (such as you substitute the Conv2D layer with other operators/layers), I think there's some issue with Conv2D in this case. It would be great to fix this issue to make TF more stable and robust. Thank you!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
568,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite gives wrong result after reshape with bool tensor)ï¼Œ å†…å®¹æ˜¯ ( 1. System information tf 2.10.0  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks https://colab.research.google.com/drive/1yySdWF9yfLYIGlnVCqAEWpnLX2HiZD0w?usp=sharing  Option B: Paste your code here or provide a link to a custom endtoend colab  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Co1lin,TFLite gives wrong result after reshape with bool tensor, 1. System information tf 2.10.0  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks https://colab.research.google.com/drive/1yySdWF9yfLYIGlnVCqAEWpnLX2HiZD0w?usp=sharing  Option B: Paste your code here or provide a link to a custom endtoend colab  ,2022-09-26T03:13:12Z,type:bug comp:lite TFLiteConverter Fixed in Nightly TF 2.10,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57837,Hi  ! It seems to be fixed in nightly version. Attached gist in nightly for reference. Thank you!,You are right. It seems fine in the newest nightly version. Thanks!,Are you satisfied with the resolution of your issue? Yes No
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inconsistencies between 2d and 3d ops)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04.6  Mobile device _No response_  Python version 3.7.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jiannanWang,Inconsistencies between 2d and 3d ops,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.10  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04.6  Mobile device _No response_  Python version 3.7.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-09-25T17:48:51Z,stat:awaiting response type:bug stale comp:ops TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57835,", I was facing a different issue while executing the given code. Kindly find the gist of it here.","Hi  , thank you for your quick response! I forgot to mention my environment was on CPU. I changed to install tensorflowcpu instead of tensorflow and then I can see it reproduces my issue. Can you please take a look again? My updated gist.",", I was able to reproduce the issue on tensorflow v2.8, v2.10 and nightly. Kindly find the gist of it here. ","From Tensorflow 2.3 2.4 onwards the changes made in `Conv2D` to add outer batch dimension support to `tf.nn.convXd` and `keras.layers.ConvXd`. One of the related PR is here https://github.com/tensorflow/tensorflow/commit/e06bd939a1e001dc519be7a74e6c0d5f78ccd3d8 which might be the reason for the changes you are observing, which I see as intended behavior.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1841,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CMakeFile.txt build failure for mlir-hlo)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0 (sha 551852a9ea9bf4e99856ce75c63516ad6d372239)  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04.3   Mobile device _No response_  Python version 3.8  Bazel version not using bazel  GCC/Compiler version 10.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  Results in a linking error (see below). The fix is this patch:  shell FAILED: tools/mlirhlo/python_packages/mlir_hlo/mlir/_mlir_libs/libMLIRHLOCAPI.so.16git  : && /usr/bin/g++10 fPIC fPIC fnosemanticinterposition fvisibilityinlineshidden Werror=datetime Wall Wextra Wnounusedparameter Wwritestrings Wcastqual Wnomissingfieldinitializers pedantic Wnolonglong Wimplicitfallthrough Wnomaybeuninitialized Wnoclassmemaccess Wnoredundantmove Wnopessimizingmove Wnonoexcepttype Wdeletenonvirtualdtor Wsuggestoverride Wnocomment Wmisleadingindentation fdiagnosticscolor ffunctionsections fdatasections fPIC fnosemanticinterposition fvisibilityinlineshidden Werror=datetime Wall Wextra Wnounusedparameter Wwritestrings Wcastqual Wnomissingfieldinitializers pedantic Wnolonglong Wimplicitfallthrough Wnomaybeuninitialized Wnoclassmemaccess Wnoredundantmove Wnopessimizingmove Wnonoexcepttype Wdeletenonvirtualdtor Wsuggestoverride Wnocomment Wmisleadingindentation fdiagnosticscolor ffunctionsections fdatasections O3 DNDEBUG  Wl,z,defs Wl,z,nodelete Wl,z,defs Wl,z,nodelete   Wl,rpathlink,/compiler/boyana.norris/developer/llvm/build/./lib  Wl,gcsections  Wl,z,defs shared Wl,soname,libMLIRHLOCAPI.so.16git o tools/mlirhlo/python_packages/mlir_hlo/mlir/)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,brnorris03,CMakeFile.txt build failure for mlir-hlo,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.11.0 (sha 551852a9ea9bf4e99856ce75c63516ad6d372239)  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04.3   Mobile device _No response_  Python version 3.8  Bazel version not using bazel  GCC/Compiler version 10.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  Results in a linking error (see below). The fix is this patch:  shell FAILED: tools/mlirhlo/python_packages/mlir_hlo/mlir/_mlir_libs/libMLIRHLOCAPI.so.16git  : && /usr/bin/g++10 fPIC fPIC fnosemanticinterposition fvisibilityinlineshidden Werror=datetime Wall Wextra Wnounusedparameter Wwritestrings Wcastqual Wnomissingfieldinitializers pedantic Wnolonglong Wimplicitfallthrough Wnomaybeuninitialized Wnoclassmemaccess Wnoredundantmove Wnopessimizingmove Wnonoexcepttype Wdeletenonvirtualdtor Wsuggestoverride Wnocomment Wmisleadingindentation fdiagnosticscolor ffunctionsections fdatasections fPIC fnosemanticinterposition fvisibilityinlineshidden Werror=datetime Wall Wextra Wnounusedparameter Wwritestrings Wcastqual Wnomissingfieldinitializers pedantic Wnolonglong Wimplicitfallthrough Wnomaybeuninitialized Wnoclassmemaccess Wnoredundantmove Wnopessimizingmove Wnonoexcepttype Wdeletenonvirtualdtor Wsuggestoverride Wnocomment Wmisleadingindentation fdiagnosticscolor ffunctionsections fdatasections O3 DNDEBUG  Wl,z,defs Wl,z,nodelete Wl,z,defs Wl,z,nodelete   Wl,rpathlink,/compiler/boyana.norris/developer/llvm/build/./lib  Wl,gcsections  Wl,z,defs shared Wl,soname,libMLIRHLOCAPI.so.16git o tools/mlirhlo/python_packages/mlir_hlo/mlir/",2022-09-24T21:49:08Z,stat:awaiting response type:bug type:support stale comp:xla TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57834,"There were some changes made in the XLA MLIR HLO code, including the changes to move the codebase to the new directory here https://github.com/tensorflow/tensorflow/commit/b0195f7011ce7e8f132c0da0f63ed4af6cbb21de. Could you please build against the master branch and let us know if you still face an issue. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1245,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failing to build libtensorflow_cc.so for v2.10.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.3.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2.152  GPU model and memory NVIDIA RTX 3090 TI (24GB)  Current Behaviour? I am trying to build the TensorFlow C++ library `libtensorflow_cc.so` from source. The same build commands that succeeded for versions below 2.10.0 now fail with the latest 2.10.0 tag of the TensorFlow repository. I'm building inside a Docker container of the official `develgpu` image, which is defined in develgpu.Dockerfile. For the build, I am calling the following. The build output is attached as the log in this issue.  The error message reads:   Standalone code to reproduce the issue First build the official GPU devel image with develgpu.Dockerfile.  Then try to build the `libtensorflow_cc.so` with the following Dockerfile.    Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lreiher,Failing to build libtensorflow_cc.so for v2.10.0,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.3.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2.152  GPU model and memory NVIDIA RTX 3090 TI (24GB)  Current Behaviour? I am trying to build the TensorFlow C++ library `libtensorflow_cc.so` from source. The same build commands that succeeded for versions below 2.10.0 now fail with the latest 2.10.0 tag of the TensorFlow repository. I'm building inside a Docker container of the official `develgpu` image, which is defined in develgpu.Dockerfile. For the build, I am calling the following. The build output is attached as the log in this issue.  The error message reads:   Standalone code to reproduce the issue First build the official GPU devel image with develgpu.Dockerfile.  Then try to build the `libtensorflow_cc.so` with the following Dockerfile.    Relevant log output  ",2022-09-23T16:37:17Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 2.10,closed,3,11,https://github.com/tensorflow/tensorflow/issues/57826,Working again with v2.11.0rc1.,Same issue on v2.11.0rc1. r2.9 works fine for me. ,"Same error, so I roll back to 2.9.2.",  ! I could replicate this issue in master branch too . Thank you!,I ran into this issue as well and with `git bisect` learned that the issue was introduced in 71b5cdf6718bc3c4a96f4be9760b2918252df64f. Reverting this particular commit allowed me to build the library (but I don't have enough context to understand what the consequences of it are). Here is a patch doing that against TF 2.10.0.,da2606aaf152055dd1996f374dd541aa9bc4a479,"Hi , thank you for the fix, this indeed helped me to build the library again. I did however notice that `libtensorflow_cc.so` seems to now be linking to `libtensorflow_framework.so`, which is also built in the process. That was not the case with v2.9.2 and below. Actually I thought that `config=monolithic` should result in only `libtensorflow_cc.so`, but it looks like `libtensorflow_framework.so` is a dependency since https://github.com/tensorflow/tensorflow/commit/843c02fe06983ac0f4382a93fff9ffd07eb93d27? I didn't find where `config=monolithic` is coming into play at the moment. My main problem at the moment is that I cannot successfully compile and run a test C++ program. Previously this worked:  With v2.10.0+ it is giving me the following error instead.  Now if I add `libtensorflow_framework.so` to the list of libs to link against, compilation works.  This time however, I am getting the following runtime error:  Any clues?",Hi  ! We are  checking to see if you are still looking for assistance in this issue. Build seems to be passing with nightly branch (2.12dev). Could you let us know from your side. Thank you!,"Hi ! First of all, building `2.10.0`, `2.10.1`, `2.11.0` is working when cherrypicking `tensorflow/BUILD` from b1bd1d6beeac169ce669f81dcbf3c48899ca1ed0. There's still another issue for which I have created CC(Monolithic build of libtensorflow_cc.so produces libtensorflow_framework.so, failing to run application). I can also confirm that the build is succeeding with `nightly`. When compiling my example application with the `nightly` build, I'm getting application build time errors though. "," ! Can we consider this as resolved for build process and track the runtime  issues there in CC(Monolithic build of libtensorflow_cc.so produces libtensorflow_framework.so, failing to run application) . Thank you!",Are you satisfied with the resolution of your issue? Yes No
1231,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SSD mobilenet v2 trained using OD API can't run on Raspberry pi)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution: Raspbian Bullseye  TensorFlow installation :  pip package, 2.8.0 for conversion training tensorflowgpu 1.15.0 tfliteruntime 2.10.0  2. Code   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model works using tensoflow 2.8.0 on my laptop using intel i5  Model doesn't work on the raspberry pi using tfliteruntime !image  when running  output  (ps : output was loo long)  when using converter.allow_custom_ops = True the conversion works on tensorflow but not on tfliteruntime  the question is: **why a model provided by google not work on tfliteruntime ?** I have tested many ssd mobilenet v2 tflite files on the raspberry pi and they work, and they are really good. But when trying to train them on custom dataset and follow the guide provided by Google, they just don't work. I can't use tensorflow on the raspberry pi !!!  appreciate any help ! Thanks)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,UcefMountacer,SSD mobilenet v2 trained using OD API can't run on Raspberry pi," 1. System information  OS Platform and Distribution: Raspbian Bullseye  TensorFlow installation :  pip package, 2.8.0 for conversion training tensorflowgpu 1.15.0 tfliteruntime 2.10.0  2. Code   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model works using tensoflow 2.8.0 on my laptop using intel i5  Model doesn't work on the raspberry pi using tfliteruntime !image  when running  output  (ps : output was loo long)  when using converter.allow_custom_ops = True the conversion works on tensorflow but not on tfliteruntime  the question is: **why a model provided by google not work on tfliteruntime ?** I have tested many ssd mobilenet v2 tflite files on the raspberry pi and they work, and they are really good. But when trying to train them on custom dataset and follow the guide provided by Google, they just don't work. I can't use tensorflow on the raspberry pi !!!  appreciate any help ! Thanks",2022-09-23T14:12:41Z,comp:lite comp:micro TF 2.10,closed,0,14,https://github.com/tensorflow/tensorflow/issues/57824,Hi  ! Select Ops feature is still not there in tfliterun time . You have to sha clone ( upto last 3 stable branches) and use  2.9/2.10 branch to use select ops functionality. Can you also  check the raspberry pi wheels from Pinto  and let us know the results .,"Hello   Thanks for your answer. The issue is indeed in tfliteruntime, because when I installed a tensorflow wheel on the rpi and it works. the loading time is an issue though. > You have to shadow clone ( upto last 3 stable branches) and use 2.9/2.10 branch to use select ops functionality. sorry, I didn't understand this part. Also, I will check the wheel from pintoo, thanks a lot for your help.",Ok  ! I was suggesting to pull only the latest 23 commits(instead of cloning all branches) through the below command. It will save some space that way in your raspberry pi. `git clone depth 3  https://github.com/tensorflow/tensorflow ` Please let us know after testing with flex delegate wheels from Pinto. Thank you!,"> I was suggesting to pull only the latest 23 commits(instead of cloning all branches) through the below command. It will save some space that way in your raspberry pi. Okey, I get it, thanks. But I installed using a wheel file, because I had to find the right installation for my python version. But thanks for the point. I'll test it and let you guys know on this thread if it worked or not. Just an additionnal question :  the model is mobilenet v2 ssd trained using tensorflow v1, the input size is 300*300, although in the official docs it says it is a model with input 320*320. The config file confirm there is an input sizer of 300*300. Can you please explain this to me, I am confused !! And also, could this resizer be the issue ? Thanks"," ! Input size is 300x300 for SSD Mobilenet v2 , Could you point me towards the SSD mobilenet v2 documentation . Thank you!","  it's the ssd_mobilenet_v2_coco_2018_03_29 http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz Btw, I'm also having issues with ssd lite 300*300, trained as well using tensorflow 1 Does that mean I should switch to tensofflow 2 ?"," ! If you have trained the model in TF 1.x version, You can use compat.v1 model in 2.x use the features from 1.x through below command. ( but results will be much better if you can write the code  again in 2.x ).  Can we consider this as resolved then (Did the flex delegate wheels for your Pi?) Thank you!","Great to know  . I'll test those. I haven't tried yet the wheel files from Pintoo, can you guys leave this issue open? I'll do it ASAP and update you ! Thanks.",Sure  ! Please share your results once you have tested .  Thank you!,"Hello   I have a question. I don't have the Pinto wheel test result yet, but I have a serious question. How come the model downloaded from tf2 model zoo can be converted to int8, while using the same code on this model but trained on custom dataset fails. I had to allow custom ops, which makes it unusable with tfliteruntime. I am really interested in knowing this. Thanks sharing code that worked with original ssd mbnet2 (tf2) ", ! Could you share the error stack trace for above issue with representative dataset. Thank you!,"  sorry I forgot to follow up, but that issue was solved by setting the representative dataset generator output to float32 Regarding tfliteruntime wheel, I found that using export_tflite_graph_tf2.py function generates a saved model that can be converted to tlite and used without issues with tfliteruntime (installed using pip). I'll close the issue as I think my original problem was solved. Thanks",Are you satisfied with the resolution of your issue? Yes No," Can you give the steps on how you resolve the issue of flex delegate with tfliteruntime(rpi), as I am also having the same issue for my custom ssd mobilenet model, it will be a great help for me."
719,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RoBERTa example from tfhub produces error ""During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string"")ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7.14   Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ShiqiaoZhou,"RoBERTa example from tfhub produces error ""During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string""",Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7.14   Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-23T10:23:05Z,type:bug comp:keras TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57820,"  I was unable to reproduce this issue on colab using latest TF version, please find the gist here and also refer this comment, it may help! This issue seems to be Keras issue.  For any further questions please post this issue on kerasteam/keras repo. as Keras development is fully moving to github.com/kerasteam/keras. All issues and PRs related to keras will be addressed in that repo. To know more see this TF forum discussion ;  https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",Thanks.  Which version of TF are you use to reproduce? are you using 2.10?,"Thank you for your help. After I updated my tensorflow, it works",Are you satisfied with the resolution of your issue? Yes No, Thank you for the update! Glad it is working fine for you.
1109,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to get mobilenetV2 normalization values?)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.2  2. Code Provide code to help us reproduce your issues using one of the following options: Same problem as CC(Fails to run the model in an Android app): Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images. So I added metadata:     3. Failure after conversion When running, it still complains about not having normalization values. Tried both [127.5],[127.5] and [255],[0] as values, but neither work. How can i find out what the models expects as normalization values?   I loaded the saved model in the netron.app and the input just says `type: float32[1,640,640,3]`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Regenhardt,How to get mobilenetV2 normalization values?," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.2  2. Code Provide code to help us reproduce your issues using one of the following options: Same problem as CC(Fails to run the model in an Android app): Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images. So I added metadata:     3. Failure after conversion When running, it still complains about not having normalization values. Tried both [127.5],[127.5] and [255],[0] as values, but neither work. How can i find out what the models expects as normalization values?   I loaded the saved model in the netron.app and the input just says `type: float32[1,640,640,3]`",2022-09-23T08:49:37Z,stat:awaiting response type:support stale comp:lite TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57817,"Hi  ! Could you check with input_norm_mean = 0 ,input_norm_std = 255 and ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu8/model.tflite  Reference. Thank you!","It works! Tried with my model, and it also works. My guess is I copied or wrote the wrong file somewhere between exporting the model and adding it to the android project. So now, again, I tried with both [127.5],[127.5] and [0],[255]  both work the same, as far as I can tell. Any idea why? Shouldn't one of them crash or something?  How do I know what values my specific model needs?"," ! I think for  mean and std fro mobilenet models are 127.5 and 127.5 respectively. Ref  You can also get those values from the model's metadata ""NormalizationOptions""  through MetadataExtractor v2 api. Ref Can we consider this issue as resolved now. Thank you!","The MetadataExtractor v2 api ref seems to extract metadata only from a finished tflite model  which means, I have to have added metadata to it beforehand.   Which I cannot do if I don't know which normalization options to add.   Or can this api extract metadata from a tflite model before I add said metadata? In that case I don't understand why we have to manually add the metadata before using the model.", ! Thanks for the update.  ! Could you look at this issue. Thank you!,"Hi   The normalization which the TFLite model expects depends upon the model from which it is being converted. The Mobilenetv2 from keras application applies the tf normalization which normalizes to the output range [1,1]. So we may have provide [127.5],[127.5] values in metadata.  https://github.com/kerasteam/kerasapplications/blob/06fbeb0f16e1304f239b2296578d1c50b15a983a/keras_applications/mobilenet.pyL84 https://github.com/kerasteam/kerasapplications/blob/06fbeb0f16e1304f239b2296578d1c50b15a983a/keras_applications/imagenet_utils.pyL42 Where as the input images for the tensorflow hub MobileNetV2 model are expected to have color values in the range [0,1] (provide [0],[255] values in metadata).  Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
728,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(No registered 'MatrixDeterminant' OpKernel for XLA_GPU_JIT devices)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version v1.12.181213g53844b49ea4 2.11.0dev20220912  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jxy,No registered 'MatrixDeterminant' OpKernel for XLA_GPU_JIT devices,Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version v1.12.181213g53844b49ea4 2.11.0dev20220912  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-09-22T20:14:48Z,stat:awaiting response type:feature stale comp:xla,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57807,"As a workaround, `tf.linalg.logdet` uses cholesky, which works in XLA. Edit: same as cholesky, `tf.linalg.logdet` only works for hermitian positive definite matrix, so it's `0.5*tf.linalg.logdet(tf.linalg.matmul(m,m,adjoint_b=True))` for general `m`.",Hi  ! You can use XLA autoclustering as work around for now. Hi  ! Could you look at this feature request. Thank you!,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1859,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Asset file vanishes after loading & saving the model twice)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.4  Custom Code No  OS Platform and Distribution Linux  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When I created the tf.Module with an asset file inside the TextFileInitializer, and save & load the module twice, the asset file will vanish after the 2nd save, and the loading will fail, complaining the absolute path of that file cannot be found.   Error: see full stack trace below   When changing `initializer` to be `self.initializer` in the object constructor, it will work as normal  I did a deep dive into the model saving logic in TF 2.4, and found the difference here: When saving the model for the first time, the model saver will save the object dependencies into the SavedObjectGraph: object_graph_def in the meta_graph_def:  As we can see above, `_initializer` is not the attribute of the toplevel object and is the child of `VocabularyTable`, which is a resource object. The dependency will look like object > table > initializer > Asset. During the first time loading * The Loader will try to recreate all the objects in the SavedObjectGraph,  * The resource object will be recreated as _RestoredResource, which inherits the base.Trackable (not AutoTrackable).  * When calling `_setattr_` in the _add_object_graph_edges(), it will not update _checkpoint_dependencies (unlike normal object inheriting AutoTrackable, which override the _setattr_ function to update the)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,supercharleszhu,Asset file vanishes after loading & saving the model twice,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.4  Custom Code No  OS Platform and Distribution Linux  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When I created the tf.Module with an asset file inside the TextFileInitializer, and save & load the module twice, the asset file will vanish after the 2nd save, and the loading will fail, complaining the absolute path of that file cannot be found.   Error: see full stack trace below   When changing `initializer` to be `self.initializer` in the object constructor, it will work as normal  I did a deep dive into the model saving logic in TF 2.4, and found the difference here: When saving the model for the first time, the model saver will save the object dependencies into the SavedObjectGraph: object_graph_def in the meta_graph_def:  As we can see above, `_initializer` is not the attribute of the toplevel object and is the child of `VocabularyTable`, which is a resource object. The dependency will look like object > table > initializer > Asset. During the first time loading * The Loader will try to recreate all the objects in the SavedObjectGraph,  * The resource object will be recreated as _RestoredResource, which inherits the base.Trackable (not AutoTrackable).  * When calling `_setattr_` in the _add_object_graph_edges(), it will not update _checkpoint_dependencies (unlike normal object inheriting AutoTrackable, which override the _setattr_ function to update the",2022-09-22T20:10:22Z,type:bug TF 2.4,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57806,Seems that it is fixed in this commit  https://github.com/tensorflow/tensorflow/commit/59e882af21831eea9e24d74c9af4242cc1514c6e Verified that it works in TF 2.9. Closing the issue.,Are you satisfied with the resolution of your issue? Yes No
1511,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(can i use graph_transforms where the input model is saved model)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,qiu-pinggaizi,can i use graph_transforms where the input model is saved model," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-09-22T02:53:02Z,stat:awaiting response type:support stale comp:model ModelOptimizationToolkit,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57796,Hi pinggaizi ! Are you looking for this api  tfmot.quantization.keras.graph_transformations..  Would it be possible to eloborate a bit with a example code. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite: Not detecting `DataType` for UINT8 `TensorImage`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution _No response_  Mobile device Android 12, target SDK 30  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,covercash2,TensorFlow Lite: Not detecting `DataType` for UINT8 `TensorImage`,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution _No response_  Mobile device Android 12, target SDK 30  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-09-21T19:31:12Z,stat:awaiting response type:support stale comp:lite TF 2.10,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57781,Hi  ! This exception occurs when datatype is not either uint8 or float 32 . Can you cross verify the the datatype after this line .  `val imageDataType = interpreter.getInputTensor(imageTensorIndex).dataType() ` Would it be possible to share a toy model to replicate this issue too Thank you!,"the debugger confirms that it is a UINT8 DataType.   inspecting the code it looks like it's failing where the type is being checked by its Java class:  unfortunately, i'll have to check whether we can share a model. we are using a modified version of EfficientNetB0. here's some metadata from the model in Netron: ","if i change the code to   i get a new error:  which means, according to the `TensorImpl.dataTypeOf`, the buffer is being determined as an `IntBuffer` instead of a `ByteBuffer` for some reason.",Ok  ! Thanks for the update.  ! Could you look at this issue. Thank you!,would appreciate any guidance on how to solve if there is a workaround ğŸ™ ,"Hi   Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
613,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GPU detection src location)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version 2  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory Tesla V100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shrebhan,GPU detection src location,Click to expand!    Issue Type Others  Source source  Tensorflow Version 2  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory Tesla V100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-20T22:49:43Z,stat:awaiting response type:feature stale comp:gpu,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57768,"hello  ! can you help me out finding the source code for where it checks if GPU or required libraries are resent on the system? Or point me someone who can ? or if there is documentation for it anywhere, that would be great, thank you","Hi  ! Sorry for the late response.  While building from source , we normally run `python configure.py`to provide cuda location and other dependencies for custom build.  Link to configure.py is here . GPU virtualization might be possible through docker. Command to check GPU capability is here.  Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
839,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) ERROR: No matching distribution found for tensorflow)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version Latest as of 9/20/22 (trying to pip install)  Custom Code No  OS Platform and Distribution MacOS Monterey version 12.6  Mobile device _No response_  Python version 3.8.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Unable to pip install tensorflow following the documentation here: https://www.tensorflow.org/install/pip.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dzenilee,ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) ERROR: No matching distribution found for tensorflow,Click to expand!    Issue Type Bug  Source source  Tensorflow Version Latest as of 9/20/22 (trying to pip install)  Custom Code No  OS Platform and Distribution MacOS Monterey version 12.6  Mobile device _No response_  Python version 3.8.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Unable to pip install tensorflow following the documentation here: https://www.tensorflow.org/install/pip.  ,2022-09-20T21:45:22Z,stat:awaiting response type:build/install stale subtype:macOS,closed,1,4,https://github.com/tensorflow/tensorflow/issues/57767, Could you make sure that you have followed the instructions as mentioned in the documentation ? Please refer this thread and try with the latest TF version. Please let us know if it helps? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1239,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(A flag to enable the previous RNG behavior for `tf.keras.initializers`)ï¼Œ å†…å®¹æ˜¯ ( Issue Type Feature Request  Source binary  Tensorflow Version TF 2.10  Custom Code No  Current Behaviour? According to release note, RNG behavior change for `tf.keras.initializers` since `TF 2.10`. In particular, `tf.random.set_seed(seed)` is not enough to get the same model weights  we need to set explicit seeds in the initializers. In general, this is NOT an issue. However, for **testing purpose**, this makes things much more difficult. For example, let's say we have `MyCustomModel`, implemented with layers without specifying any initializer (therefore, no explicit seed with initializer). However, if we want to have a CI test that verifies the model produces the same result, it is impossible with TF 2.10 due to the new change. With TF < 2.10, we can set `tf.random.set_seed(seed=0)`, and the model weights will be the same, so we can check if the output matches the expected values. It would be great if there is a way to have previous behavior for **testing purpose**.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ydshieh,A flag to enable the previous RNG behavior for `tf.keras.initializers`," Issue Type Feature Request  Source binary  Tensorflow Version TF 2.10  Custom Code No  Current Behaviour? According to release note, RNG behavior change for `tf.keras.initializers` since `TF 2.10`. In particular, `tf.random.set_seed(seed)` is not enough to get the same model weights  we need to set explicit seeds in the initializers. In general, this is NOT an issue. However, for **testing purpose**, this makes things much more difficult. For example, let's say we have `MyCustomModel`, implemented with layers without specifying any initializer (therefore, no explicit seed with initializer). However, if we want to have a CI test that verifies the model produces the same result, it is impossible with TF 2.10 due to the new change. With TF < 2.10, we can set `tf.random.set_seed(seed=0)`, and the model weights will be the same, so we can check if the output matches the expected values. It would be great if there is a way to have previous behavior for **testing purpose**.",2022-09-20T13:11:21Z,stat:awaiting tensorflower type:feature comp:keras,closed,0,12,https://github.com/tensorflow/tensorflow/issues/57763,"Not sure who to tag, but  had a commit 23d4b96c that is related to this change.",Agree. We need a flag or a function to enable/disable the stateless/stateful rng_type,"Thanks for reporting this issue. As a short cut for now (to have a determined initializer for your test), you can use `tf.keras.utils.set_random_seed()` to set the seed for python/numpy/tf/keras. ","> Thanks for reporting this issue. As a short cut for now (to have a determined initializer for your test), you can use `tf.keras.utils.set_random_seed()` to set the seed for python/numpy/tf/keras. Hi. I think you misunderstand it. Even set random seed using `tf.keras.utils.set_random_seed()`, the generated weights is different between 2.10 and 2.9/2.8 (Stateless vs Stateful rng)",I see. Is it because there are some golden number that is baked in the test case? (I think TF doesn't have a contract for the exact RNG number created  between version. Adding  for this.),"Yes, we fix some inputs, and use a seed to create models > and test the model outputs will match the predefined expected values. > TF doesn't have a contract for the exact RNG number created between version. I understand  same situation with PyTorch. So far (up to TF 2.9) our tests are fine as we still get the same model weights. But with TF 2.10, some tests are failing. We probably need to reconsider our testing approach, but still wonder if TensorFlow could provide some mechanism to make things much easier ğŸ™ ","The behavior of the stateless RNG initializers encumbers reproducibility.  As `tf.random.set_seed` no longer affects the initializers, and the only way to have a reproducible initializer is explicit passing a seed at the call site during initialization, there is no easy way to change training runs from using different initial weights to reproducible weights. If reverting the initializer behavior is not the way forward, we would like to see a new api that change the default seed of the initializers globally, without touching existing model code.","> The behavior of the stateless RNG initializers encumbers reproducibility. As `tf.random.set_seed` no longer affects the initializers, and the only way to have a reproducible initializer is explicit passing a seed at the call site during initialization, there is no easy way to change training runs from using different initial weights to reproducible weights. >  > If reverting the initializer behavior is not the way forward, we would like to see a new api that change the default seed of the initializers globally, without touching existing model code. The global seed setting for keras initializer is `tf.keras.utils.set_random_seed()`, which should make all the initializer to have a determined seed.","> The global seed setting for keras initializer is tf.keras.utils.set_random_seed(), which should make all the initializer to have a determined seed. Indeed.  Thanks. The doc string https://github.com/kerasteam/keras/blob/b80dd12da9c0bc3f569eca3455e77762cf2ee8ef/keras/utils/tf_utils.pyL44L53 needs to be updated to reflect that it's also setting https://github.com/kerasteam/keras/blob/b80dd12da9c0bc3f569eca3455e77762cf2ee8ef/keras/utils/tf_utils.pyL66",Thank a lot! `tf.keras.utils.set_random_seed()` makes TF fans' life much easier :),Ack. Closing this issue now.," I think providing a flag for the switch between stateless and stateful is still necessary. I found the new RNG behavior (same seed, same hardware with deterministic enabled) produced a different result.  Check this link (https://www.tensorflow.org/guide/random_numbers). It states, ""The RNG algorithm used by stateless RNGs is devicedependent, meaning the same op running on a different device may produce different outputs."" The same hardware + fixed seed should always generate the same numbers on a different machine. It will be very useful when conducting experiments on the GPU cluster (each time, the cluster may allocate a different machine)."
705,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Refactoring the GEMM interface)ï¼Œ å†…å®¹æ˜¯ (This PR replaces current GEMMrelated methods (dozens of them, all with 10+ arguments) in the stream executor interface with a few methods, and packs their arguments into structures.  This substantially shrinks the code and makes the interface easily extensible (if a new optional argument, like the recently added compute precision setting, needs to be passed through to stream executor, it only needs to be added to the structure definition, instead of modifying 50 function signatures.))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ekuznetsov139,Refactoring the GEMM interface,"This PR replaces current GEMMrelated methods (dozens of them, all with 10+ arguments) in the stream executor interface with a few methods, and packs their arguments into structures.  This substantially shrinks the code and makes the interface easily extensible (if a new optional argument, like the recently added compute precision setting, needs to be passed through to stream executor, it only needs to be added to the structure definition, instead of modifying 50 function signatures.)",2022-09-20T06:59:18Z,stale comp:xla size:XL,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57762,"    due to merge conflict, landing this all at once might be very hard. It would be much easier to split it up.", I wanted to follow up on  's request above. Please let us know., Any update on this PR? Please. Thank you!,It is not easy to split it up. I will attempt to cut it into two pieces., Any update on this PR? Please. Thank you!, Can you please check 's comments and keep us posted ? Thank you!,"Hi  I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen! thank you for your contribution. "
761,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow failed to build with Download from archive/13c6828bedeb815ee7748f82ca36073dbd55a9db.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version master branch commit a098414  Custom Code No  OS Platform and Distribution windows server 2019  Mobile device No response  Python version No response  Bazel version _No response_  GCC/Compiler version No response  CUDA/cuDNN version No response  GPU model and memory No response  Current Behaviour?   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fangzhouxia,Tensorflow failed to build with Download from archive/13c6828bedeb815ee7748f82ca36073dbd55a9db.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version master branch commit a098414  Custom Code No  OS Platform and Distribution windows server 2019  Mobile device No response  Python version No response  Bazel version _No response_  GCC/Compiler version No response  CUDA/cuDNN version No response  GPU model and memory No response  Current Behaviour?   Relevant log output  ,2022-09-20T06:30:02Z,type:build/install subtype:windows subtype:bazel,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57761,Hi  ! Could you let us know your Bazel version too . Standard procedure is  1. clean the previous build using below command `bazel clean expunge.` 2. Check out a stable branch 2.10.0 (instead of master) `git checkout r2.10 `2. rebuild again with flag ` define=no_tensorflow_py_deps=true` `bazel output_user_root F:\bazelTemp build jobs 8 config=opt  define=no_tensorflow_py_deps=true subcommands //tensorflow/tools/pip_package:build_pip_package 2>&1` Note: Bazel 5.1.1 is tested config for 2.10.0 branch . Thank you!,"The bazel version is 5.2.0.  I tried the method you provided, and it was successful.",Ok  !  Thanks for the confirmation. Can we mark this as resolved then. Thank you!,Thanks  this problem has been solved perfectly.,Ok  ! Marking this resolved then. Thank you!,Are you satisfied with the resolution of your issue? Yes No
970,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite's max operator has wrong broadcasting behavior)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.4 LTS  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0.dev20220914  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks https://colab.research.google.com/drive/1SWl6cWwk2zRkiNYifUXtzojSz_uUCxg?usp=sharing  Option B: Paste your code here or provide a link to a custom endtoend colab  3. Failure after conversion After converting to TFLite, the model produces tensor with wrong shape. This is inconsistent with normal TF runtime, and numpy behavior.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Co1lin,TFLite's max operator has wrong broadcasting behavior," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.4 LTS  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0.dev20220914  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks https://colab.research.google.com/drive/1SWl6cWwk2zRkiNYifUXtzojSz_uUCxg?usp=sharing  Option B: Paste your code here or provide a link to a custom endtoend colab  3. Failure after conversion After converting to TFLite, the model produces tensor with wrong shape. This is inconsistent with normal TF runtime, and numpy behavior.",2022-09-20T02:06:13Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57759,Hi  ! It is replicating in nightly but It was throwing converter error in 2.9 and 2.10. As the documentation on tf.maximum suggests you should use same shapes for both input and output. Thank you!,"Hi, thank you for the reply! Since the documentation of maximum also notes that ""Note that maximum supports broadcast semantics for x and y"" which means numpy style broadcasting is supported, it's better if TFLite can produce the same results as numpy and TF.","Ok  ! Thanks for the clarification. It is a valid bug then.  ! Could you look at this issue. Attached gist in 2.9, 2.10 and nightly for reference. Thank you!","   It seems this bug only occurs when one of the arguments in max is `tf.zeros`. When setting it to `tf.ones`, the inferred shape will be correct.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1884,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Some conv2d operations remain float32 after post training full integer quantization)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monteley 12.5.1, Android 12  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.3  2. Code  Option B: Paste your code here or provide a link to a custom endtoend colab Some of the conversion codes are as follows.     3. Failure after conversion  The conversion is successful, but some conv2d operations remain float32 after post training full integer quantization. Some conv2d operations are sandwiched between dequantize and quantize operation, and these conv2d operations has float32 weights and bias. All conv2d should have int8 weights and int32 biases.    I tried running this model on Pixel 6 NNAPI delegate with TFLite model benchmark tool. Some operations fallbacks to the XNNPACK delegate. I suspect that the fallback to XNNPACK delegate is due to the float32 conv2d layers and it causes a slight degradation in inference speed.  How can I quantize all operations to int8 weights and int32 biases? Also, will quantizing all operations to int8 improve inference speed? Quantized tflite model is here.   model_full_integer_quant.tflite.zip  4. (optional) RNN conversion support   None.    5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.   The command to run model benchmark tool on Pixel 6 is as follows. I found specifying `nnapi_accelerator_n)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ksekine,Some conv2d operations remain float32 after post training full integer quantization," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monteley 12.5.1, Android 12  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.3  2. Code  Option B: Paste your code here or provide a link to a custom endtoend colab Some of the conversion codes are as follows.     3. Failure after conversion  The conversion is successful, but some conv2d operations remain float32 after post training full integer quantization. Some conv2d operations are sandwiched between dequantize and quantize operation, and these conv2d operations has float32 weights and bias. All conv2d should have int8 weights and int32 biases.    I tried running this model on Pixel 6 NNAPI delegate with TFLite model benchmark tool. Some operations fallbacks to the XNNPACK delegate. I suspect that the fallback to XNNPACK delegate is due to the float32 conv2d layers and it causes a slight degradation in inference speed.  How can I quantize all operations to int8 weights and int32 biases? Also, will quantizing all operations to int8 improve inference speed? Quantized tflite model is here.   model_full_integer_quant.tflite.zip  4. (optional) RNN conversion support   None.    5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.   The command to run model benchmark tool on Pixel 6 is as follows. I found specifying `nnapi_accelerator_n",2022-09-20T02:05:57Z,stat:awaiting response type:support comp:lite TFLiteConverter TF 2.5 ModelOptimizationToolkit,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57758,After updating TensorFlow I got the following conversion log. TensorFlow version is `tensorflow==2.10.0`. This issue isn't solved.        ,Hi  ! Could you look at this issue. Thank you!,"Hi , some operations cannot currently be quantized, that is why you see Dequantize (8bit > 32bit) nodes and Quantize (32bit > 8bit) nodes), so that we can continue the conversion despite those operations not being supported. Currently we do not support different precision weights & biases, we do support mixed precision weights & activations though (we call this 16x8 quant mode), please review this for more information: https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8 > How can I quantize all operations to int8 weights and int32 biases? The current framework is unable to for you, there is probably a way if you go very deep into the code, if you happen to solve this, please feel free to submit a PR where we will review it. > Also, will quantizing all operations to int8 improve inference speed? Almost assuredly yes, please review this (somewhat older) benchmark data to get a better idea: https://www.tensorflow.org/lite/performance/model_optimizationquantization . This is with Pixel 2 devices using a single big core CPU. New hardware is more optimized for int8 computations, I suspect their inference improvements would be even better. Does this answer your questions?","Hi , thank you for replying. I understood the situation and what I need to do. I'll go deep into the code. Please feel free to close this issue. I'm grateful for your support. ğŸ‘","Hi , please feel free to close as unplanned if you have no more open items.",Are you satisfied with the resolution of your issue? Yes No
905,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Wrong shape broadcasting during tensor equal check)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tfnightly  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? when comparing two shapeincompatible tensors (with shape `(3,)` and `(5,)`) using `==` on cpu, tf fails to check shape and silently produce an output. if I switch to CUDA then it has the correct behavior: throws error because the shapes cannot broadcast.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,creakseek,Wrong shape broadcasting during tensor equal check,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tfnightly  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? when comparing two shapeincompatible tensors (with shape `(3,)` and `(5,)`) using `==` on cpu, tf fails to check shape and silently produce an output. if I switch to CUDA then it has the correct behavior: throws error because the shapes cannot broadcast.  Standalone code to reproduce the issue   Relevant log output  ",2022-09-19T16:48:39Z,stat:awaiting response type:bug stale comp:ops TF 2.10,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57751,Similar issue reproduced in `tf.experimental.numpy.isclose`:  Outputs: ,  I was able to reproduce the issue on Colab using TF v2.10. Please find the gist here for reference. Thank you!,"The CPU behavior is actually intentional to match numpy.  Using `==` between tensors returns False if there is a shape mismatch doc.  If you use `tf.math.equal`, it will assert on incompatible dimensions. Looks like the GPU behavior is the buggy one.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
952,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loads ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Chaztikov,"2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loads ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads",Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-18T04:54:34Z,stat:awaiting response type:support stale comp:core TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57739,", I was able to reproduce the issue on tensorflow v2.8 and nightly. Kindly find the gist of it here.","> /usr/bin/python3 > Is this the directory you're referring to? > If so, in my case I would do > ln s /usr/local/cuda/bin /usr/bin/python3 > Is that correct? Thanks Lets say you have your python code (.py file) in specific directory, you go to that directory and the run the command below. Go to the directory, where you launch your python code and create the link: `ln s /full/path/to/your/cuda/installation/bin .` Hope this helps!!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
641,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Does op ADD support int8/int16 dtype? )ï¼Œ å†…å®¹æ˜¯ (Hi, I just got a issue about the op ADDV2. My model has the following part that I need to cast the float32 input to int8/int16/int32, then cast it back. When I cast the input to int8/int16, the final addition op changed from ADD to ADDV2 as I checked the graph. Hence, how can I avoid op ADDV2 and use op ADD for dtype int8/int16 (if possible), thanks!   Environment TensorFlow version: 2.9.1 python version:  3.8.13)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,HensonMa,Does op ADD support int8/int16 dtype? ,"Hi, I just got a issue about the op ADDV2. My model has the following part that I need to cast the float32 input to int8/int16/int32, then cast it back. When I cast the input to int8/int16, the final addition op changed from ADD to ADDV2 as I checked the graph. Hence, how can I avoid op ADDV2 and use op ADD for dtype int8/int16 (if possible), thanks!   Environment TensorFlow version: 2.9.1 python version:  3.8.13",2022-09-17T13:31:06Z,type:support comp:lite TFLiteConverter,closed,0,1,https://github.com/tensorflow/tensorflow/issues/57737,"The model has Addv2 which accepts type int64. Tensorflow lite doesn't support Addv2 with int64 input/output. Since you used SELECT_TF_OPS that means all nonsupported ops will be exported as TF ops and run in TF. To run the generated tflite file, you need to use Interpreter which supports SELECT. Please follow the guide on creating an interpreter https://www.tensorflow.org/lite/guide/ops_select Another way is to update your model to use int32 instead of int64, convert without SELECT_TF_OPS and use a regular interpreter."
654,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Clang-9 incompatibility with mkl_conv_ops.cc)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version HEAD  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,stellaraccident,Clang-9 incompatibility with mkl_conv_ops.cc,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version HEAD  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-17T00:56:01Z,type:build/install,closed,0,1,https://github.com/tensorflow/tensorflow/issues/57733,Are you satisfied with the resolution of your issue? Yes No
1030,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(log, pow, div unsupported in 16x8 precision)ï¼Œ å†…å®¹æ˜¯ (It seems these ops are supported in 8x8 mode, but not in 16x8. The latter is very important for our organization, as it saves a lot of rounding error over 8x8. Is full 16x8 support on the current roadmap? If not, could we help to contribute this feature?  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04  TensorFlow installation (pip package or built from source): pip tfnightly  TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0dev20220916   2. Code   3. Failure after conversion Conversion fails, with the following error messages: > Quantization to 16x8bit not yet supported for op: 'LOG'. > Quantization to 16x8bit not yet supported for op: 'POW'. > Quantization to 16x8bit not yet supported for op: 'DIV'.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,blaine-fs,"log, pow, div unsupported in 16x8 precision","It seems these ops are supported in 8x8 mode, but not in 16x8. The latter is very important for our organization, as it saves a lot of rounding error over 8x8. Is full 16x8 support on the current roadmap? If not, could we help to contribute this feature?  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04  TensorFlow installation (pip package or built from source): pip tfnightly  TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0dev20220916   2. Code   3. Failure after conversion Conversion fails, with the following error messages: > Quantization to 16x8bit not yet supported for op: 'LOG'. > Quantization to 16x8bit not yet supported for op: 'POW'. > Quantization to 16x8bit not yet supported for op: 'DIV'.",2022-09-16T18:52:35Z,stat:awaiting response type:feature stale comp:lite TFLiteConverter TF 2.11,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57725,"Hi fs ! If 16x8 quantization is not supported for some operators in the model, then the model still can be quantized, but unsupported operators kept in float. The following option should be added to the target_spec to allow this. ` converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,tf.lite.OpsSet.TFLITE_BUILTINS]` Attached gist for reference.  Reference Thank you!","Hi .  Thanks for the tip. Keeping those ops in float is better than nothing. At least they don't have weights, so we should be guaranteed that all model parameters are quantized to int8. In our use case, we are developing a compiler backend for TFLite which targets custom hardware.  Like most inference accelerators, it lacks hardware support for floating point, so we would really rather have TFLite code that only consists of fixedpoint ops. We could work around this by doing the float > fixed conversion in our backend, but it seems like TF should be able to do this on its own, and can do it for 8x8 already. So we would like to know if there are any plans to expand 16x8 support, or if it would be open to community contributions.","Actually, looking at the MLIR dialect spec, it seems like some of these ops (for example `tfl.log`) do not support fixed point at all. When I try to compile these in 8x8 mode, I seem to silently get floating points ops instead. Do we always need to allow floating point if we want to use these operators? Or put another way, is full integer quantization supported for only a subset of TFLite?",Ok fs ! Thanks for the update.   ! Could you look at this issue. Thank you!,I was able to reproduce the issue in TF 2.11 and TF Nightly. Please find the gist here. Thank you.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
425,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Meaning of the ""FlexErf"" node? )ï¼Œ å†…å®¹æ˜¯ (I'm using the vit_keras.vit.Vit_b32 and I am trying to understand what is the meaning of the ""FlexErf"" node.  Can someone explain? I think that Is probably linked with the ERF (error function) computation...)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dgrnd4,"Meaning of the ""FlexErf"" node? ","I'm using the vit_keras.vit.Vit_b32 and I am trying to understand what is the meaning of the ""FlexErf"" node.  Can someone explain? I think that Is probably linked with the ERF (error function) computation...",2022-09-16T10:26:50Z,stat:awaiting response comp:lite type:others TFLiteConverter,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57721, ! It is gauss error function (Erf) and included in select ops list.  So It might be renaming the node as flexErf. ( flex op Erf) Thank you!,"Many thanks, closing the issue",Are you satisfied with the resolution of your issue? Yes No
1397,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to get registration from op code CUSTOM)ï¼Œ å†…å®¹æ˜¯ (Hi developers, hi all! I'm making some experiments with LSTM networks using Keras, TensorFlow, and I'm working on a Colab environment, using TF 2.11.0dev20220915 installed via pip.  On the model authoring side, I'm able to successfully build the model, apply compression techniques and get the equivalent Lite model, and the .. Specifically, starting from the base model I apply the pruning technique first, and then fully integer quantization. During the conversion process to Lite model, I do not get any errors. However, when it's time to flash the model onto the device (in this case,  an ESP32), I always get the following error **Failed to get registration from op code CUSTOM AllocateTensors() failed**.  Is this error generated because there is something in the model not yet supported? is there a way to discover which is, or are, the OPs that are not supported? I really would like to understand the reason for this error and how could I solve it. Here attached you can find a zip folder with two .tflite files (one it's related to the model not quantized yet and the other to the quantized one), the third one is the .. material_pruningfullyintquant.zip)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ChicchiPhD,Failed to get registration from op code CUSTOM,"Hi developers, hi all! I'm making some experiments with LSTM networks using Keras, TensorFlow, and I'm working on a Colab environment, using TF 2.11.0dev20220915 installed via pip.  On the model authoring side, I'm able to successfully build the model, apply compression techniques and get the equivalent Lite model, and the .. Specifically, starting from the base model I apply the pruning technique first, and then fully integer quantization. During the conversion process to Lite model, I do not get any errors. However, when it's time to flash the model onto the device (in this case,  an ESP32), I always get the following error **Failed to get registration from op code CUSTOM AllocateTensors() failed**.  Is this error generated because there is something in the model not yet supported? is there a way to discover which is, or are, the OPs that are not supported? I really would like to understand the reason for this error and how could I solve it. Here attached you can find a zip folder with two .tflite files (one it's related to the model not quantized yet and the other to the quantized one), the third one is the .. material_pruningfullyintquant.zip",2022-09-15T18:20:53Z,stat:awaiting response type:support comp:lite comp:micro TF 2.10,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57710,Hi  ! You can use netron app to see any operator which are not either supported in select ops or built in list. I found a SO thread with similar stack trace which suggest on porting lite to micro ops (perhaps you are using some flex ops) You can also check Fusion code lab documentation on converting LSTM/RNN model which sometimes don't require select ops syntax. Could you also  attach the C++ inference code used in ESP 32 inference. Thank you!,"Hi ,  thank you so much for your reply, and for pointing all the links out. I've actually realized that one of the sources of the problem was related to the fact that I had not a fixed input size in the model. I was able to repeat all tests, without the need to use tf.lite.OpsSet.SELECT_TF_OPS on model authoring side, during conversion. Everything works fine, so, I'm able to apply the pruning technique first, fully integer quantization then, and obtain .tflite .. But when I flash the model onto the device, I get a new error: **VAR_HANDLE requires resource variables. Please create ResourceVariables and pass it to the interpreter. Node VAR_HANDLE (number 1f) failed to prepare with status 1 ** Here attached you can find a zip folder containing the two tflite models (one of the pruned model but not quantized yet, the other one of the pruned and quantized model), and the two . (one is related to the pruned model but not quantized yet, the other one of the pruned and quantized model). In both cases, I get the same error. I've found this ticket https://github.com/tensorflow/tensorflow/issues/56932 related to the same problem. However, for me, the proposed solution did not work. Not only the proposed solution did not work, but the size of both . (around 7 MB), so that I was not even able to run them on the device. 1_pruning_fullyquant.zip",Ok  ! Thanks for the update. Could you convert the model to TFlite again with below flag and let us know the inference result. `converter.experimental_enable_resource_variables = True`  Ref Thank you!,"Hi , if I try to convert the model to TFLite with that flag, I still get the same error  **VAR_HANDLE requires resource variables. Please create ResourceVariables and pass it to the interpreter. Node VAR_HANDLE (number 1f) failed to prepare with status 1 ** However, I've done several attempts and I've noticed the following behaviors: 1. whether I use the flag or not, if I apply pruning and then TFLite conversion, or pruning, followed by fully integer quantization and then I apply TFLite conversion, onto the device I always get the VAR_HANDLE error, in both cases; 2. whether I use the flag or not, if I do not apply pruning technique but only quantization, or none of the two techniques, and then I apply TFLite conversion, onto the device I do not get any errors. So, it seems something related to the fact that I apply the pruning technique. Thank for your support!",Ok  ! Thanks for the update.  Can we consider this as resolved then as it seems that you were able to resolve this issue by skipping prunning technique/other methods. Thank you!,"Hy , I would say that skipping pruning is particularly a workaround, more than a solution, but at the moment is fine for me. Investigating why the error happens when pruning is applied would be interesting, but we can consider this solved.",Are you satisfied with the resolution of your issue? Yes No, ! Please share your pruning code snippets to check for any issue. Thank you. ,"I have the same problem, I have a Keras model converted to `tflite` and then to ``, but the conversation to `tlite` model is simple without any addition flags, and I'm getting the same error but without a solution  I have tried both `tflite::AllOpsResolver` and `tflite::MicroMutableOpResolver`. Any solution for this problem, I really need a solution to complete my thesis project.", is that an LSTM model? Can you share the output from netron app?,"I have the same problem. I have added the code I am working on and the SVG of the Netron app. There re two models that I need to run on my ESP32C3.  (https://drive.google.com/file/d/15as7adLykxOzsaAxV6l9UTe_EHS_WqXw/view?usp=sharing) !state_model tflite !temp_model tflite **error:**  ESPROM:esp32c3api120210207 Build:Feb  7 2021 rst:0x1 (POWERON),boot:0xc (SPI_FAST_FLASH_BOOT) SPIWP:0xee mode:DIO, clock div:1 load:0x3fcd5820,len:0x458 load:0x403cc710,len:0x814 load:0x403ce710,len:0x2880 entry 0x403cc710 **Failed to get registration from op code CUSTOM AllocateTensors() failed** models:  1. def create_state_model(input_dim):     model = Sequential()     model.add(Reshape((input_dim, 1), input_shape=(input_dim,)))     model.add(LSTM(64, return_sequences=False))     model.add(Dropout(0.3))     model.add(Dense(32, activation='relu'))     model.add(Dropout(0.3))     model.add(Dense(1, activation='sigmoid'))     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])     return model 2. def create_temp_model(input_dim, output_dim):     model = Sequential()     model.add(Reshape((input_dim, 1), input_shape=(input_dim,)))     model.add(LSTM(64, return_sequences=False))     model.add(Dropout(0.3))     model.add(Dense(32, activation='relu'))     model.add(Dropout(0.3))     model.add(Dense(output_dim, activation='softmax'))     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])     return model This is the code that I have used for conversion:  converter = tf.lite.TFLiteConverter.from_keras_model(state_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,   enable TensorFlow Lite ops.     tf.lite.OpsSet.SELECT_TF_OPS   enable TensorFlow ops. ] state_model_tflite = converter.convert() with open('state_model.tflite', 'wb') as f:     f.write(state_model_tflite) converter = tf.lite.TFLiteConverter.from_keras_model(temp_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,   enable TensorFlow Lite ops.     tf.lite.OpsSet.SELECT_TF_OPS   enable TensorFlow ops. ] temp_model_tflite = converter.convert() with open('temp_model.tflite', 'wb') as f:     f.write(temp_model_tflite)"
1320,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFlite conversion Doesn't support dense layers with inputs of rank greater than 2)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 64 bit  TensorFlow installation (pip package or built from source): Python 3.8.2, pip: 22.1, pip package  TensorFlow library (version, if pip package or github SHA, if built from source): TF version:'2.9.0'  2. Code Provide code to help us reproduce your issues using one of the following options: ``` import tensorflow as tf import numpy as np import pathlib def tflite_convert(model,data):     def representative_data_gen():             for input_value in data:                 input_value = input_value[np.newaxis, ...]                 yield [input_value]  shape should be (1, 2D ).the model has a single dense layer with 3 nodes. its kernal are of shape [12,3]. the model produces an output of shape (none,1,5,3).   in the .tflite converted file the output is of shape [1,1,1,3] (not [1,1,5,3]) and input is of shape [1,1,5,12]  this is the output i have obtained with the above code: !image its netrons with .h5 on the left and .tflite on the right !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Saar-Ken-Ji,TFlite conversion Doesn't support dense layers with inputs of rank greater than 2," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 64 bit  TensorFlow installation (pip package or built from source): Python 3.8.2, pip: 22.1, pip package  TensorFlow library (version, if pip package or github SHA, if built from source): TF version:'2.9.0'  2. Code Provide code to help us reproduce your issues using one of the following options: ``` import tensorflow as tf import numpy as np import pathlib def tflite_convert(model,data):     def representative_data_gen():             for input_value in data:                 input_value = input_value[np.newaxis, ...]                 yield [input_value]  shape should be (1, 2D ).the model has a single dense layer with 3 nodes. its kernal are of shape [12,3]. the model produces an output of shape (none,1,5,3).   in the .tflite converted file the output is of shape [1,1,1,3] (not [1,1,5,3]) and input is of shape [1,1,5,12]  this is the output i have obtained with the above code: !image its netrons with .h5 on the left and .tflite on the right !image",2022-09-14T10:26:46Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57691,"Hi  ! Could you look at this issue. Attached gist in 2.8, 2.9 and nightly for reference.","KenJi The issue seems to be resolved in TF Nightly 2.12.0dev20230117. The output shape of TF lite converted model is obtained as `[1,1,5,3]` . Please find the gist here and let us know if it helps. Thank you.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1077,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Datasets created by `from_generator` do not properly release objects)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When we create a dataset by using `tf.data.Dataset.from_generator`, it seems that Python objects captured in the generator are not properly released. This causes crucial memory leak when conducting experiments that creates datasets with large memory footprint for multiple times (e.g., cross validation).  Standalone code to reproduce the issue We expect `__del__ called!` to be printed for three times, but it is never printed.   Output   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,iwiwi,Datasets created by `from_generator` do not properly release objects,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When we create a dataset by using `tf.data.Dataset.from_generator`, it seems that Python objects captured in the generator are not properly released. This causes crucial memory leak when conducting experiments that creates datasets with large memory footprint for multiple times (e.g., cross validation).  Standalone code to reproduce the issue We expect `__del__ called!` to be printed for three times, but it is never printed.   Output   Relevant log output _No response_",2022-09-14T06:02:56Z,stat:awaiting response type:bug stale comp:data TF 2.10,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57690,"  I was able to replicate the issue on colab, please find the gist here. Thank you!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
476,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TF-TRT]: Propagate batch_dims in Gather converter)ï¼Œ å†…å®¹æ˜¯ (For gather operations where indices is a vector, this parameter specifies how many leading dimensions in indices tensor are to be used elementwise. This wasn't propagated correctly, leading to bugs in transformer models. Fix this. cc:,    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,meena-at-work,[TF-TRT]: Propagate batch_dims in Gather converter,"For gather operations where indices is a vector, this parameter specifies how many leading dimensions in indices tensor are to be used elementwise. This wasn't propagated correctly, leading to bugs in transformer models. Fix this. cc:,    ",2022-09-13T23:53:08Z,ready to pull size:S comp:gpu:tensorrt,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57686,"Thanks atwork for this PR! The converter looks good, but we would still need to test the converter with the `batch_dims` attribute. And since we are changing this converter, it would be nice to have a follow up PR that refactors `ConvertGather` to `OpConverterBase`.",Hi atwork Can you please resolve conflicts? Thank you!,> Hi atwork Can you please resolve conflicts? Thank you!    resolved.
646,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error while running Bazel Test on windows platform)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution windows 10   Mobile device _No response_  Python version 3.10  Bazel version 5.3.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,mraunak,Error while running Bazel Test on windows platform,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution windows 10   Mobile device _No response_  Python version 3.10  Bazel version 5.3.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-13T19:53:50Z,stat:awaiting response type:build/install subtype:bazel TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57685,Documentation of issue.docx,"Thanks for the detailed analysis! I'm currently looking into reproducing this on a windows device, to verify whether the issue is with `env` (there's a chance that just the size checker is faulty), and submit a fix. I'm OOO next week but will continue the investigation after.",Please see proposed fix https://github.com/tensorflow/tensorflow/pull/57944   ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.","Hi , we can close this issue, it has been fixed",Are you satisfied with the resolution of your issue? Yes No
950,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA makes copies of tensors instead of referencing them)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  and includes copy operation, while reshape can be done without any copies shell https://colab.research.google.com/drive/1Y3SvJi8zf0dpTuvSIYczyQBkC4HrL shell [5] %timeit foo_xla(rnd) 34.5 ms Â± 4.25 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) [6] %timeit foo_tf(rnd) 371 Âµs Â± 60 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each) ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mstebelev,XLA makes copies of tensors instead of referencing them,"Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  and includes copy operation, while reshape can be done without any copies shell https://colab.research.google.com/drive/1Y3SvJi8zf0dpTuvSIYczyQBkC4HrL shell [5] %timeit foo_xla(rnd) 34.5 ms Â± 4.25 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) [6] %timeit foo_tf(rnd) 371 Âµs Â± 60 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each) ``` ",2022-09-13T12:43:02Z,stat:awaiting response stale comp:xla type:performance TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57681," I tried to replicate the issue on colab using latest TF v2.10, please find the gist here and confirm the same? Thank you!","  Yes, I see that in 2.10 something is better, but the problem with copy in generated XLA code is the same and you can see it in new version https://colab.research.google.com/drive/1ESGaYLMQttOqqc5GbJZxeC0NDOfipGF7 where we have a lot of reshapes of the same tensor in one operation","In general XLA does not know whether it can alias input and output. I guess this could be fixed at the bridge level, but what we have seen so far, most large aliasing updates are done with resource variables, where aliasing does work.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
836,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TFTRT] Refactor C++ API)ï¼Œ å†…å®¹æ˜¯ (Refactors TFTRT's C++ API to better match the existing Python interface, with the intention of eventually unifying C++ and Python into one implementation.  Create an `experimental` directory with the edited files  Add a `TrtGraphConverter` class with `Convert`, `Build`, and `Summary` methods  Require a frozen `GraphDef` for conversion, as opposed to a Saved Model, as freezing in C++ has some issues  Adds experimental `TrtGraphConverterV2` in Python, enabled with the experimental feature `using_cpp_api`, that wraps the C++ API Depends on CC([TFTRT] Refactoring Final Step  Preparing for transition to C++ Conversion API))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nvkevihu,[TFTRT] Refactor C++ API,"Refactors TFTRT's C++ API to better match the existing Python interface, with the intention of eventually unifying C++ and Python into one implementation.  Create an `experimental` directory with the edited files  Add a `TrtGraphConverter` class with `Convert`, `Build`, and `Summary` methods  Require a frozen `GraphDef` for conversion, as opposed to a Saved Model, as freezing in C++ has some issues  Adds experimental `TrtGraphConverterV2` in Python, enabled with the experimental feature `using_cpp_api`, that wraps the C++ API Depends on CC([TFTRT] Refactoring Final Step  Preparing for transition to C++ Conversion API)",2022-09-12T22:34:07Z,stale size:L comp:gpu:tensorrt,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57673," This PR is in draft, any update on this? Please. Thank you!",Hi  Can you please resolve conflicts? Thank you!,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.
654,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation Fault on importing Tensorflow)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.4.1  Custom Code Yes  OS Platform and Distribution Debian GNU/Linux 10  Mobile device Debian GNU/Linux 10  Python version 3.9.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tarun-menta,Segmentation Fault on importing Tensorflow,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.4.1  Custom Code Yes  OS Platform and Distribution Debian GNU/Linux 10  Mobile device Debian GNU/Linux 10  Python version 3.9.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.0  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-09-12T11:39:17Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.4,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57669,Hi  ! Sorry for the late response. I could not replicate this issue in colab with TF 2.8 . Could you let us know after including Tensorflow dependency in yml file instead of importing default tensorflow installation in Colab. Thank you!,TF 2.4 is no longer supported. Please switch to TF 2.10,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"The bug was on TF 2.4, which I now see is no longer supported, so I'm not sure if this is still pertinent",Hi  ! Could you confirm with 2.8/ 2.10.0 version and let us know. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1408,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Model Trains on CPU but not on GPU - TensorDot/MatMul error)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA=11.2, cuDNN=8.1  GPU model and memory RTX 8000 48GB   Current Behaviour?  I have tried multiple versions of tensorflow gpu (2.5 through 2.10) and get the same error. I would expect the network to train the same on GPU as on CPU. shell The network that I have created is a bit cumbersome for posting on this platform. I am happy to send files if needed. shell Node: 'model/final_output/Tensordot/MatMul' 2 root error(s) found.   (0) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150 	 [[{{node model/final_output/Tensordot/MatMul}}]] 	 [[Nadam/Nadam/group_deps/_111]]   (1) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150 	 [[{{node model/final_output/Tensordot/MatMul}}]] 0 successful operations. 0 derived errors ignored. [Op:__inference_train_function_29232] ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,willi3by,Model Trains on CPU but not on GPU - TensorDot/MatMul error,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA=11.2, cuDNN=8.1  GPU model and memory RTX 8000 48GB   Current Behaviour?  I have tried multiple versions of tensorflow gpu (2.5 through 2.10) and get the same error. I would expect the network to train the same on GPU as on CPU. shell The network that I have created is a bit cumbersome for posting on this platform. I am happy to send files if needed. shell Node: 'model/final_output/Tensordot/MatMul' 2 root error(s) found.   (0) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150 	 [[{{node model/final_output/Tensordot/MatMul}}]] 	 [[Nadam/Nadam/group_deps/_111]]   (1) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150 	 [[{{node model/final_output/Tensordot/MatMul}}]] 0 successful operations. 0 derived errors ignored. [Op:__inference_train_function_29232] ``` ",2022-09-12T04:54:09Z,stat:awaiting tensorflower type:bug type:build/install TF 2.9,closed,0,13,https://github.com/tensorflow/tensorflow/issues/57666,"Hi, Just wanted to add some details to this to see if I can get a fix. I think through a few more hours of trial and error and searching online, it seems this is a known issue with CUDA 11.2, specifically cuBLAS 11.2. It only happens with large matrices which seems to be consistent with previous posts I have seen. Based on these posts, it looks like the error does not occur with cuBLAS 11.1 or 11.4. I am trying to configure my environment to use these versions of cuBLAS but I am not sure how to go about it. I have searched for a bit now but every time I try one of the suggestions, it seems to break everything and I have to do a clean reinstall of cuda and tensorflow. Is there a straightforward way to change the version of cuBLAS to 11.1 or 11.4? Thanks!",", To expedite the troubleshooting process, could you please provide a complete code you are using. Thank you!","I'm having this bug as well in tf2.10.0, cudatoolkit 11.2. Also failing on a giant matrix. ","Thanks for the response   and for the second . I confirmed this issue by training the network with a batch size of 1 with no issues, as the matrix was smaller than 2^20 with only one sample. I will work on getting the code up soon.  Thanks", There is a minimal code example here: https://github.com/tensorflow/tensorflow/issues/54463 for recreating the bug. I tried it on my setup and got the exception.,", Could you please take a look at this https://github.com/tensorflow/tensorflow/pull/56691 which fixed the `Unable to register cuBLAS` factory error. Please try this and let us know if the issue still persists. Thank you!",  Thanks for the link. I am not sure what to try per se. I read through and it looks like this change was committed to the master branch of tensorflow on August 10. I only recently install tensorflow using pip. Does pip pull the most recent tensorflow master or do I have to compile from source?  Thanks," , You need to install tensorflow nightly version for that PR to be available. Since this PR could not make it at the time of 2.10.0 brach cut.  It might be included if there is any patch release for 2.10.0 or it will be available in the next release, i.e 2.11.0. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you., I apologize for the delay. I updated TF and am still getting the same error. My current version of TF is 2.11.0dev20220928,"Hi , Have you tried downgrading from CUDA 11.2 to CUDA 11.1?  If you are using conda environment, following installation steps in https://www.tensorflow.org/install/pip, `conda install c condaforge cudatoolkit=11.1 cudatoolkitdev=11.1 cudnn=8.1.0 y` sets up my environment perfectly. I have been doing this to circumvent the issue with CUDA 11.2 and you might find this helpful as well. **Test With CUDA 11.1 (No Error)**  **Test With CUDA 11.2 (Throws _INTERNAL:  Blas xGEMV launch failed_ Error)**  As mentioned in https://github.com/tensorflow/tensorflow/issues/54463issuecomment1061343477, this issue is only resolved in CUDA 11.4.", That fixed it!! Thanks very much for your suggestion.,Are you satisfied with the resolution of your issue? Yes No
659,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Out Of Memory During Model Save)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0dev20220901  Custom Code No  OS Platform and Distribution Windows WSL  Mobile device _No response_  Python version Python 3.9.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory NVIDIA GeForce RTX 3090/21612 MB memory  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Moocow9m,Out Of Memory During Model Save,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.11.0dev20220901  Custom Code No  OS Platform and Distribution Windows WSL  Mobile device _No response_  Python version Python 3.9.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory NVIDIA GeForce RTX 3090/21612 MB memory  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-10T02:12:32Z,stat:awaiting response type:bug stale comp:apis wsl2 TF 2.10,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57659,"Hi , Thanks for raising this issue. Could you share the complete code snippet to reproduce the issue. Thank you!","> Hi , Thanks for raising this issue. Could you share the complete code snippet to reproduce the issue. Thank you! I have edited first post to include my imports, the only part missing now is images being loaded since I cannot include my full env and csv files to obtain the images. However, since the images are resized, any images will do. It should also be noted that this only happens on CUDA and model does not need to be trained before saving to cause this issue, further making the images I am using unnecessary to be included.","Hi , I tried to replicate your issue without `model.fit()` as you told the behaviour happens even without Training the model.I tried to check if this can be replicated on colab with GPU runtime model with labels=1000 without `model.fit()` and it throws no error.Please refer to attached gist here. This indicates there might be some problem with TF build on windows. The TF installation procedure for Windows has been attached here.Could you please go through the steps once again to check whether anything is missing being followed. Just FYI colab has CUDA11.2 installed. Also as per Documentation Tested configurations CUDA 11.2 is tested with TF 2.11 v and with tfnightly(2.12) CUDA 11.8 also supported as per announcement on TFForum. As you are using cuda 11.7 either you use TFnightly or use CUDA 11.2 for TF 2.11 version which might resolve your issue. I request you to please check the TF installation steps and also follow the tested build configurations attached above and please let us know if problem still persists. Thankyou!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
924,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Is it possible to use both cameras (FRONT AND BACK) with CameraX for object detection TFLITE *NO HARDWARE LIMITATION*)ï¼Œ å†…å®¹æ˜¯ (As the title says i tried to implement an app that with the front detects the driver face and with the back detects cars. I modified the demo object detection for tensorflow lite inrtroducing a new camera fragment and adding the permissions to the new camerabinding. Only 1 camera appears to works, getting in the LOGS that the maximum of Cameras allowed to be open are 1.  Ive seen apps with camera2  API working both cameras at the same time, so i have the next question: 1. is it possible to use camera2 API with tensorflow lite? 2. is it possible to use 2 cameras simultaniously  with CameraX api? Thanks in advance.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,JuanJoseMoralesCalvo,Is it possible to use both cameras (FRONT AND BACK) with CameraX for object detection TFLITE *NO HARDWARE LIMITATION*,"As the title says i tried to implement an app that with the front detects the driver face and with the back detects cars. I modified the demo object detection for tensorflow lite inrtroducing a new camera fragment and adding the permissions to the new camerabinding. Only 1 camera appears to works, getting in the LOGS that the maximum of Cameras allowed to be open are 1.  Ive seen apps with camera2  API working both cameras at the same time, so i have the next question: 1. is it possible to use camera2 API with tensorflow lite? 2. is it possible to use 2 cameras simultaniously  with CameraX api? Thanks in advance.",2022-09-08T10:43:24Z,stat:awaiting response type:support comp:lite TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57643,"Hi  ! Yes , Both Camera2 and CameraX api are compatible with TFlite. I am also pretty positive on using both front and rear camera using TFlite . Attaching relevant threads 1, 2 for reference. Regarding Hardware limitiation, You can devise fallback policy in code itself to use any of delegates incase it finds any supported ops during inference. Thank you!","Thanks for the reply, right now i wasnt able to use both cameras with CameraX due to ""bindToLifecycle()"" doesnt let you to use 2 cameraSelector at the same time. I will appretiate if you know any post regarding the use of both cameras with TFlite or any post related with Camera2 and tflite.  Thanks for the fast reply :D .","Hi  ! Camera 2.x does not support accessing both cameras same time as you mentioned earlier . You can opt for Camera 2 api  for accessing dual camera input .Regarding hardware configuration , I think Dual ISP configuration required to access both front and real camera same time.  Reference on Dual ISP . Please post on TF forum for further assistance. Thank you!","Ok thanks for the help, i will try from this.", ! Can we consider this as resolved then. Thank you!,Ok  ! Thanks for the nod. Marking this as resolved then. Thank you!,Are you satisfied with the resolution of your issue? Yes No,"Just Completed the task, Camera2 API is the real deal. Thanks for the help", !  You are welcome.
681,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(When Building Sequential model tensorflow gives multiple opKernel registrations)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution Windows 11 Pro  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cuda_11.7.r11.7  GPU model and memory RTX a4500 20GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,boroicamarius,When Building Sequential model tensorflow gives multiple opKernel registrations,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution Windows 11 Pro  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cuda_11.7.r11.7  GPU model and memory RTX a4500 20GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-07T14:47:56Z,stat:awaiting response type:bug comp:gpu TF 2.10,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57635,Are you satisfied with the resolution of your issue? Yes No,I have installed currently both CUDA toolkit and cuDNN and this is the first time I'm running tensorflow with a new gpu,", I was able to execute the mentioned code without any issues/errors. Kindly find the gist of it here.  Could you please create a virtual environment and test your code again. Thank you!",Are you satisfied with the resolution of your issue? Yes No
1007,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build error: Exec failed due to IOException: xcrun failed with code 1.)ï¼Œ å†…å®¹æ˜¯ ( Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution macOS 12.5.1 (Apple M1 Pro)  Mobile device _No response_  Python version 3.9.13  Bazel version 5.1.1  GCC/Compiler version Apple Clang 13.1.6  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? I'm unable to install TF 2.10.0 on macOS 12.5.1. I see the following errors during build:   Standalone code to reproduce the issue I'm trying to update the Spack package manager with the latest TF release. Just like all prior releases, we've unfortunately been unable to compile TF on Apple M1/M2. To reproduce the issue, you can use the branch where I'm trying to add TF 2.10:   Relevant log output * build log * build env)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,adamjstewart,Build error: Exec failed due to IOException: xcrun failed with code 1.," Issue Type Build/Install  Source source  Tensorflow Version 2.10.0  Custom Code Yes  OS Platform and Distribution macOS 12.5.1 (Apple M1 Pro)  Mobile device _No response_  Python version 3.9.13  Bazel version 5.1.1  GCC/Compiler version Apple Clang 13.1.6  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? I'm unable to install TF 2.10.0 on macOS 12.5.1. I see the following errors during build:   Standalone code to reproduce the issue I'm trying to update the Spack package manager with the latest TF release. Just like all prior releases, we've unfortunately been unable to compile TF on Apple M1/M2. To reproduce the issue, you can use the branch where I'm trying to add TF 2.10:   Relevant log output * build log * build env",2022-09-07T01:13:31Z,type:build/install type:support subtype:macOS,closed,0,14,https://github.com/tensorflow/tensorflow/issues/57631,"Hi , Just to make sure that you followed instructions mentioned on official Tensorflow site. For users of Apple M1 computers, to get native performance, you'll want to follow the instructions found here. Conda has shown to have the smoothest install. Thank you!"," as I mentioned above, I'm not trying to install TF using pip/conda, I'm trying to repackage it for the Spack package manager. A package manager that downloads and installs a different package manager isn't a very good package manager. We would like to build from source so we can build binaries optimized to specific CPU/GPU hardware on supercomputers, we don't want to rely on unoptimized binaries. But the fromsource build doesn't seem to be working on Apple M1 for some reason. The same build recipe works fine on x86_64 though.","Hi  , I am sure there are build problems earlier due to X_code versions on Mac. I can say this might not be problem now as I gone through Two such issues with configurations X_code 14.3,bazel 5.3.0 on Macos13.3 able to build from source successfully. Please refer the tickets CC(Unable to build tensorflow from source on Apple M1) , CC(Build failed `not all outputs were created or valid` on `darwin/amd64`) where users builds succeeded and probably you can also give it a try and let us know. Thanks!","Hi   I tried building with GNU coreutils but that didn't help. I still see the same error message:  I'm on macOS 13.3.1, bazel 5.3.0, TF 2.12.0, and Xcode 14.3.","Hi  , I want to understand more context here. Are you trying to build Tensorflow from source and then using it for the package mentioned. Please submit the command you are using for build and the environment details also. The official instructions along with tested configurations are listed here. I request you to cross check whether all the instructions are followed and I am expecting the command that is standalone to Tensorflow with bazel , in that way I want to ensure whether the problem is with TF or other environment. Thanks!","I'm using the Spack package manager, specifically this build recipe. I'll try to reproduce this without Spack to make sure it isn't something weird in our environment or patches. Currently busy with the NeurIPS deadline, but will get back to this as soon as I get a chance.","Hi  , Could you please confirm the current status. I am attaching a reference for successful build on MacM1 with Tf2.12. Thanks!",Still working on NeurIPS (datasets and benchmark track is due in a few weeks) so let me get back to you after that.," , Could you please confirm whether you got opportunity to test the build. Please confirm. Thanks!","Thanks for the reminder. Following the steps from https://www.tensorflow.org/install/source, I ran:  I see the following errors fairly early on: "," , I tried the command and found build fails. Attached below the logs for reference. Needs to check. ",Something must have changed because with the help of https://github.com/tensorflow/tensorflow/issues/60179issuecomment1491238631 I'm finally able to build from source in 2.13.0! Thanks for your help!!,Are you satisfied with the resolution of your issue? Yes No,The specific error that prompted this issue:  still occurs when building inside Spack. Removing the following flag and allowing bazel to autodetect the correct version solved my issue: `macos_sdk_version=12.5.1`.
1164,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unified Memory feature seems not working correctly)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04.1 LTS  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7/8.5.0  GPU model and memory _No response_  Current Behaviour? I am trying to use Unified Memory feature guided by the link below. https://stackoverflow.com/questions/58025069/howtoenablecudaunifiedmemoryintensorflowv2 However, it results in either an incorrect result or an error. I would really appreciate it if someone could check if it is a bug or my miss use. Amount of device memory is 40 GB and amount of host memory is 503 GiB. An result of 40GiB.py is below: 1 + 1 becomes 0.  An result of 40GB.py is below: CUDA_ERROR_ILLEGAL_ADDRESS   Standalone code to reproduce the issue 40GiB.py  40GB.py   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,satoshi-iwata,Unified Memory feature seems not working correctly,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04.1 LTS  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7/8.5.0  GPU model and memory _No response_  Current Behaviour? I am trying to use Unified Memory feature guided by the link below. https://stackoverflow.com/questions/58025069/howtoenablecudaunifiedmemoryintensorflowv2 However, it results in either an incorrect result or an error. I would really appreciate it if someone could check if it is a bug or my miss use. Amount of device memory is 40 GB and amount of host memory is 503 GiB. An result of 40GiB.py is below: 1 + 1 becomes 0.  An result of 40GB.py is below: CUDA_ERROR_ILLEGAL_ADDRESS   Standalone code to reproduce the issue 40GiB.py  40GB.py   Relevant log output _No response_",2022-09-06T20:10:31Z,stat:awaiting response type:bug stale comp:gpu TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57627,"Hi iwata ! Actually , Tensorflow has 2GB  Tensor size constraint . If I reduce the size to 2048 GB works.  Ref gist. If you are handling a huge dataset (like 1 TBs) , You can opt for multi GPU/TPU distribution strategy and use batches of dataset which shall reduce the tensor size and avoid OOM issues. Ref doc. Thank you!","Hi  ! Thanks for your reply. Could you possibly point a document or a number of line of source code noting the maximum size of a Tensor? I tried to find it, but I could not. I am confused that I could successfully run a program with tensors of 4GiB, which exceeds the 2GiB constraint. Does the word ""constraint"" mean that it results in an unexpected behavior, instead of causing an error? Another program I tried with 4 GiB tensors.  A result given by the program above. ","iwata ! I took reference on 2GB tensor size constraint from this thread. In my case , it is failing in both GPU and CPU .   ! Could you look at this issue. Thank you!",I could able to replicate the issue with CUDA 11.4 and Tensorflow 2.9. ,"Hi iwata, Could you take a look at similar issue here. Thank you!","  Thank you for the link. If I hit the constraint, I believe I should get a raised error, though I did not. So, my problem may be another problem. Anyway, thanks for your help!","Hi  ! Thanks for your comment. Actually, I have found it but it seemed a feature specific to XLA and I believe it is not for me. Moreover, even though I tried to use it, it raised another error and seemed not working properly. In addition, stats gained using  NVIDIA Nsight Systems show `cuMemAlloc` instead of `cuMemAllocManaged`. So, I believe the function does not work. Program I tried: xla.py  A result  If I run the program above with the array size of 40 MiB, instead of 40 GiB, it finishes without an error and the result is the one expected. However, it uses `cuMemAlloc` instead of `cuMemAllocManaged` too.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
717,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ImportError: cannot import name 'test' from partially initialized module 'tensorflow._api.v2.__internal__')ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution Debian GNU/Linux 11 (bullseye)  Mobile device _No response_  Python version 3.9.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fregire,ImportError: cannot import name 'test' from partially initialized module 'tensorflow._api.v2.__internal__',Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.2  Custom Code Yes  OS Platform and Distribution Debian GNU/Linux 11 (bullseye)  Mobile device _No response_  Python version 3.9.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-06T10:04:33Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57625,Problem was in Dockerfile: ` find /usr/local depth \         \( \             \( type d a \( name test o name tests o name idle_test \) \) \             o \( type f a \( name '*.pyc' o name '*.pyo' o name '*.exe' \) \) \         \) exec rm rf '{}' + ` This command removed folder 'test' which was used in tensorflow,Are you satisfied with the resolution of your issue? Yes No
1189,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GPU memory usage depends on data size)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.62.10  Custom Code Yes  OS Platform and Distribution Linux Fedora 36  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory RTX 2070, 8GiB  Current Behaviour? Training a small model on small batches runs out of GPU memory if the total amount of data is large. The same code works fine on TF 2.5, but fails on 2.6, 2.7, 2.8, and 2.9.  Standalone code to reproduce the issue Here is a handy script: https://gist.github.com/Dapid/3f58697edf91c6610ed5e7b681db440f In short, a tiny model is trained on a fixed array of data. If the total amount of data is small, it runs; otherwise, it crashes. Running `python benchmark.py` works, but `python benchmark.py big` doesn't. Using the `tf.data` API doesn't make a difference.  Relevant log output  Defining `TF_GPU_ALLOCATOR=cuda_malloc_async`:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Dapid,GPU memory usage depends on data size,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.62.10  Custom Code Yes  OS Platform and Distribution Linux Fedora 36  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7  GPU model and memory RTX 2070, 8GiB  Current Behaviour? Training a small model on small batches runs out of GPU memory if the total amount of data is large. The same code works fine on TF 2.5, but fails on 2.6, 2.7, 2.8, and 2.9.  Standalone code to reproduce the issue Here is a handy script: https://gist.github.com/Dapid/3f58697edf91c6610ed5e7b681db440f In short, a tiny model is trained on a fixed array of data. If the total amount of data is small, it runs; otherwise, it crashes. Running `python benchmark.py` works, but `python benchmark.py big` doesn't. Using the `tf.data` API doesn't make a difference.  Relevant log output  Defining `TF_GPU_ALLOCATOR=cuda_malloc_async`:  ",2022-09-06T09:00:12Z,stat:awaiting tensorflower type:bug comp:gpu TF 2.9,open,0,9,https://github.com/tensorflow/tensorflow/issues/57623," I tried to replicate the issue, could you please find the gist here and confirm the same? Thank you!","At least in the instance of Colab I am getting it is not possible to reproduce because the GPU has more memory than RAM. `nvidiasmi` reports `15109MiB`, while `free h` reports `10G` free. If you have an instance with more RAM, you can run `main(True, False)` or `main(True, True)`, and skip the argparse, which I only added because it is convenient for the CLI. By the way, I tested 2.10, and the problem persists.","Hi , When I was trying to replicating with Tf 2.9 and CUDA 11.4. I got the below error.Please take a look and provide more information. Thank you! "," is your environment working? It looks like Tensorflow is not finding CUDA, based on:  and  Note that you are training on 1000 data points, which fits perfectly in memory and works for me.","Hi , The instance has CUDA installed.  ","But look at the error:  and   On my machine:  It seems to be chocking on running on the GPU itself. It looks like it could not run anything at all. My problem is that GPU vRAM usage depends on the size of the data, which shouldn't matter because we are only sending minibatches. And it is not just for training, inference too:  `main(False)` quickly processes the 15 MiB of data, but `main(False)` runs out of GPU memory because it tries to push the 15 GiB all at once. It should work because I have 30 GiB of RAM. So, `model.predict` and `model.fit` seem to be behaving like their cousins `model.predict_on_batch` and `model.fit_on_batch`, at least when it comes to memory usage. And as I said earlier, this works with TF 2.5.",", you probably need to adjust how NVCC and libdevice are getting loaded. I hit the same problem with of a job running inside of a container that I built on top of a Miniconda3 base. An extract:  More widely applicable, you'd need to link the installation location of `libdevice.10.bc` into `${APP_PREFIX}/lib/nvvm/libdevice` and set `XLA_FLAGS=xla_gpu_cuda_data_dir=${APP_PREFIX}/lib/`, where `$APP_PREFIX` is something like `/opt/conda/devs/yourEnv` for conda or just `/lib` or `/usr/lib` for using a normal Debian base. See also the Ubuntu 20.04 instructions on the PIP installation instructions page for TF.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",The problem persists on 2.17.
830,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Building error in docker)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.11  Bazel version _No response_  GCC/Compiler version clang 11.0.0 clang version 11.0.0 (https://github.com/llvm/llvmproject.git 60a25202a7dd1e00067fcfce512086ebf3788537) Target: x86_64unknownlinuxgnu Thread model: posix InstalledDir: /tmp/llvm110install_O_D_A/bin  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,faysalhossain2007,Building error in docker,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.11  Bazel version _No response_  GCC/Compiler version clang 11.0.0 clang version 11.0.0 (https://github.com/llvm/llvmproject.git 60a25202a7dd1e00067fcfce512086ebf3788537) Target: x86_64unknownlinuxgnu Thread model: posix InstalledDir: /tmp/llvm110install_O_D_A/bin  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-09-06T08:24:50Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57622,", Could you please provide the source file you are trying to build the tensorflow. It helps to analyse the issue. Thank you!",I downloaded it from https://github.com/tensorflow/tensorflow ,"Hi , Could you try with Tensorflow master branch and  clang 13.0.0. Tensorflow master branch supports `bfloat16`. Please take a look at this link","I tried with the Tensorflow master branch. Still the same problem. Now, I am trying with clang 13.0.0.","Hi , Did you tried with clang 13.0.0.?",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1195,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF2 Keras OOM Training ImageNet with MobileNet V2)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.2  Custom Code No  OS Platform and Distribution centos rhel fedora  Mobile device _No response_  Python version 3.8.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory 4x Tesla V100 16G   Current Behaviour? As part of the effort of validating the official pretrained Keras models, I tried to measure the accuracy and try training MobileNet V2 with ImageNet2012. The measure accuracy is lower than expected, which is documented separately. The training also seems problematic with OOM errors even when the batch size is reduced to 32.  The experiment was performed on a 4GPU node with TF2/Keras. The ImageNet 2012 dataset is prepared using `tfds`. The multiGPU training used `tf.distribute.MirroredStrategy()`. Please see the code and log for more details.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lcmeng,TF2 Keras OOM Training ImageNet with MobileNet V2,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.2  Custom Code No  OS Platform and Distribution centos rhel fedora  Mobile device _No response_  Python version 3.8.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory 4x Tesla V100 16G   Current Behaviour? As part of the effort of validating the official pretrained Keras models, I tried to measure the accuracy and try training MobileNet V2 with ImageNet2012. The measure accuracy is lower than expected, which is documented separately. The training also seems problematic with OOM errors even when the batch size is reduced to 32.  The experiment was performed on a 4GPU node with TF2/Keras. The ImageNet 2012 dataset is prepared using `tfds`. The multiGPU training used `tf.distribute.MirroredStrategy()`. Please see the code and log for more details.  Standalone code to reproduce the issue   Relevant log output  ",2022-09-06T06:44:42Z,stat:awaiting response type:bug stale comp:keras TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57620,  This issue seems to be Keras issue.  Please post this issue on kerasteam/keras repo. as Keras development is fully moving to github.com/kerasteam/keras. All issues and PRs related to keras will be addressed in that repo. To know more see this TF forum discussion ;  https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!," Thank you for your response. It turned out to be a `tfds` issue most likely. I commented out the line `ds_train_parallel = ds_train_parallel.cache().prefetch(buffer_size=AUTOTUNE)`, which then prevented the OOM from happening. It could be that `cache()` conflicts with `shuffle_files=True` in `tfds.load(...)`. I also noticed some other issues with the dataset code, such as the order of augmentation and `cache()` and `batch()`.  I wonder if there's some `tfds` expert who can advise the best practice or pointers to realworld examples. Thanks.", You are just running out of memory. Can you please use distributed mirror strategy as show here to make sure all the GPUs are used correctly. Thanks!!, Thanks for your reply. The mirror strategy has been used in the original post.   I think the issue is what I commented 24 days ago above. But please correct me if I was wrong. Thanks., Can you please try batch size of 16 and let me know if you still run into OOM error?  Please take a look at all the recommendations provided here and let me know if it helps. Thanks!!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1851,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tflite_model_maker not exporting to quantised int8)ï¼Œ å†…å®¹æ˜¯ ( 1. System information _This error happens both on my PC:_  OS Platform and Distribution: Windows 10(amd64) v10.0  TensorFlow installation (pip package or built from source): From pip, TF version v2.9.018gd8ce9f9c301 2.9.1 on Python 3.9  TensorFlow library:  _AND on Google Colab_  2. Code _What I am trying to do:_ Train a model and export a quantised int8 version for later use with the TPU USB accelerator. I export a nonquantized version for comparison. _What I get:_ The model which is supposed to be quantized always exports identically to the unquantized version, with the same message when running ""model.export()"" that ""Statistics for quantized inputs were expected, but not specified; continuing anyway."" Link to the exported model: https://drive.google.com/file/d/1Gt4XT1J2Ujt3dXyvKgwESs2fciTDqFA/view?usp=sharing   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model does not produce a quantized version. The output files are of identical file size and the log below is the same for both.  5. Any other info / logs >>> model._export_tflite(tflite_filepath=tflite_filename, quantization_config=quantization_config) INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets 20220906 14:35:11.483922: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 20220906 14:35:11.484556: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting ne)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Scottyy-M,tflite_model_maker not exporting to quantised int8," 1. System information _This error happens both on my PC:_  OS Platform and Distribution: Windows 10(amd64) v10.0  TensorFlow installation (pip package or built from source): From pip, TF version v2.9.018gd8ce9f9c301 2.9.1 on Python 3.9  TensorFlow library:  _AND on Google Colab_  2. Code _What I am trying to do:_ Train a model and export a quantised int8 version for later use with the TPU USB accelerator. I export a nonquantized version for comparison. _What I get:_ The model which is supposed to be quantized always exports identically to the unquantized version, with the same message when running ""model.export()"" that ""Statistics for quantized inputs were expected, but not specified; continuing anyway."" Link to the exported model: https://drive.google.com/file/d/1Gt4XT1J2Ujt3dXyvKgwESs2fciTDqFA/view?usp=sharing   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model does not produce a quantized version. The output files are of identical file size and the log below is the same for both.  5. Any other info / logs >>> model._export_tflite(tflite_filepath=tflite_filename, quantization_config=quantization_config) INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets 20220906 14:35:11.483922: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 20220906 14:35:11.484556: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting ne",2022-09-06T06:24:48Z,type:bug comp:lite TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57619,Hi M ! Could you update the Above QuantizationConfig like below and let us know.  Reference. %3A)Thank you!,"Hi   I tried that however the results were the same. I've copied the output below: >>> config = QuantizationConfig.for_int8( ...     test_data,  split your current test_data and use yield to return some slices of  data instead of using complete test_data ...     quantization_steps=500, ...     inference_input_type=tf.uint8, ...     inference_output_type=tf.uint8, ...     supported_ops=tf.lite.OpsSet.TFLITE_BUILTINS_INT8) >>> model.export(export_dir=export_dir, tflite_filename=quant_tflite_filename, quantization_config=config) INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmptjr3ot07\assets INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmptjr3ot07\assets 20220906 17:33:48.799026: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 20220906 17:33:48.801333: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session C:\Users\sm251\AppData\Roaming\Python\Python39\sitepackages\tensorflow\lite\python\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn(""Statistics for quantized inputs were expected, but not "" 20220906 17:33:49.908961: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. 20220906 17:33:49.909206: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3 INFO:tensorflow:Label file is inside the TFLite model with metadata. INFO:tensorflow:Label file is inside the TFLite model with metadata. INFO:tensorflow:Saving labels in C:\Users\sm251\AppData\Local\Temp\tmpd12wqpnj\labels.txt INFO:tensorflow:Saving labels in C:\Users\sm251\AppData\Local\Temp\tmpd12wqpnj\labels.txt INFO:tensorflow:TensorFlow Lite model exported successfully: .\Batch2 Germ, Not, Error  First, Balanced TFLite_1 quant.tflite INFO:tensorflow:TensorFlow Lite model exported successfully: .\Batch2 Germ, Not, Error  First, Balanced TFLite_1 quant.tflite","Ok M !  Thanks for the update. I see this line  > ""tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format."" in your stacktrace.  Could you check with below syntax and give us an update.  Thank you!","Hi   Thank you I have run into issues with this too. Firstly, the line: `supported_ops=tf.lite.OpsSet.TFLITE_BUILTINS_INT8 , tf.lite.OpsSet.SELECT_TF_OPS` has an issue with ""tf.lite.OpsSet.SELECT_TF_OP"": >SyntaxError: positional argument follows keyword argument I tried removing that line, and then I got the following error: >TypeError: for_int8() got an unexpected keyword argument 'experimental_new_quantizer' Not sure what to do here. Also, I am not sure how to split my current test data and use yield to return some slices.  I checked and the default data type of the images is unit8. To change the type I could import the data differently and use something like below? `img = img.astype(np.int8)` I also tried specifying the config as the other types: `config=QuantizationConfig.for_float16()` and  `config=QuantizationConfig.for_dynamic()` Both these methods DONT complain that the >""Statistics for quantized inputs were expected, but not specified; continuing anyway"" and they DONT have the line >""fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3"" However, they do both have the lines: >W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. >W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. Also, the output file size of the dynamic model was a bit smaller, so I am confident that it worked, and that the int8() still isnt. Thanks, Scott",Ok M ! Thanks for the update.  !  Could you look at this issue.,I just checked the exported files from the 4 different methods of specifying the QuantizationConfig and I missed the size difference of the for_float16() method. The sizes of the exported models are as follows: QuantizationConfig not specified: 2.65MB QuantizationConfig.for_int8() = 2.65MB QuantizationConfig.for_float16() = 4.35MB QuantizationConfig.for_dynamic() = 2.45MB Does this mean that it is by default set as the int8 and it is working? Is there a way to inspect the exported file and tell what data types the calculations are using?,Yes M !  Default quantization is integer quantization. You can get the input and output data types through below command.  ,Thanks  ! That line in the output threw me off thinking it was not working. Thanks for the code to check the output model too!,Are you satisfied with the resolution of your issue? Yes No
1246,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Does Distributed TensorFlow can be run in independent filesystem)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf1.14  Custom Code Yes  OS Platform and Distribution Debain 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I used tensorflow version 1.14 to run distributed training and failed using the **local file system.** I saw the exchange post in 2018 on the Internet, saying that the distributed training must use the shared file system at that time, but the local file system will be supported later !image Is there a version that supports running distributed training tasks on the local file system?  Standalone code to reproduce the issue I use tf1.14 version of the distributed training framework estimator training data, with 2 ps and 3 workers, which is all configure the local file system, and found that the operation failed.   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ilixiaocui,Does Distributed TensorFlow can be run in independent filesystem,"Click to expand!    Issue Type Support  Source source  Tensorflow Version tf1.14  Custom Code Yes  OS Platform and Distribution Debain 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I used tensorflow version 1.14 to run distributed training and failed using the **local file system.** I saw the exchange post in 2018 on the Internet, saying that the distributed training must use the shared file system at that time, but the local file system will be supported later !image Is there a version that supports running distributed training tasks on the local file system?  Standalone code to reproduce the issue I use tf1.14 version of the distributed training framework estimator training data, with 2 ps and 3 workers, which is all configure the local file system, and found that the operation failed.   Relevant log output _No response_",2022-09-06T02:09:20Z,stat:awaiting response type:support stale comp:dist-strat TF 1.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57618,"Hi  ! 1.x version are not supported anymore. For 2.x versions, According to this thread , if you are going for a cluster synchronous training ( masterslave node), the cluster would fail if one of the workers fails and no failurerecovery mechanism exists.  But if you just want to use distributed computing of distribution strategy , you can use a machine with multiple GPU's in your local file system with strategy.scope(). Please post on TF Forum for further assistance and close CC([Question] Does Distributed TensorFlow can be run in  independent filesystem) as duplicate to this issue. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1321,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OnDevice prediction produces same result for all inputs in a batch)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution Mac M1, Windows x64  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? The model works well in python.  But when the model is exported as a SavedModel, and `predict` concrete function is called from C API,  all the inputs in the same batch produce exact same result.  Tested in Mac M1 and Windows x64, same behaviour.  Standalone code to reproduce the issue  If calling `predict` method from python, you can clearly see the outputs of each input in the same batch are different.  But if trying to call `predict` method of SavedModel from C API. The outputs are exactly the same for multiple inputs in the same batch.  Relevant log output The `predict` method prints out the input and output via `tf.print`  And the following logs are from stdout when calling from C, produced by the `tf.print` above. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,wangjia184,OnDevice prediction produces same result for all inputs in a batch,"  Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution Mac M1, Windows x64  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? The model works well in python.  But when the model is exported as a SavedModel, and `predict` concrete function is called from C API,  all the inputs in the same batch produce exact same result.  Tested in Mac M1 and Windows x64, same behaviour.  Standalone code to reproduce the issue  If calling `predict` method from python, you can clearly see the outputs of each input in the same batch are different.  But if trying to call `predict` method of SavedModel from C API. The outputs are exactly the same for multiple inputs in the same batch.  Relevant log output The `predict` method prints out the input and output via `tf.print`  And the following logs are from stdout when calling from C, produced by the `tf.print` above. ",2022-09-05T02:30:13Z,type:bug TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57612,Are you satisfied with the resolution of your issue? Yes No,Are you satisfied with the resolution of your issue? Yes No," Did you figure out what was causing this? I am having a similar issue, but only when my batch size is very large.",   caused by dying ReLU
507,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Running sample image classification binary application on iMX gives unexpected results)ï¼Œ å†…å®¹æ˜¯ (Hello I am trying to running the default label_image in the i.MX board and I am encountered with the below result. The below output continues endlessly. Can someone tell me why this is happening?  Current Result     Expected Result   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Aratiganesh123,Running sample image classification binary application on iMX gives unexpected results,Hello I am trying to running the default label_image in the i.MX board and I am encountered with the below result. The below output continues endlessly. Can someone tell me why this is happening?  Current Result     Expected Result   ,2022-09-02T06:17:19Z,stat:awaiting response type:support stale comp:lite comp:micro TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57606,Hi  ! Can you check the instructions from this document with 2.8 version. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
648,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Question] Does Distributed TensorFlow can be run in  independent filesystem)ï¼Œ å†…å®¹æ˜¯ (I used tensorflow version 1.14 to run distributed training and failed using the **local file system.** I saw the exchange post in 2018 on the Internet, saying that the distributed training must use the shared file system at that time, but the local file system will be supported later !image Is there a version that supports running distributed training tasks on the local file system?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ilixiaocui,[Question] Does Distributed TensorFlow can be run in  independent filesystem,"I used tensorflow version 1.14 to run distributed training and failed using the **local file system.** I saw the exchange post in 2018 on the Internet, saying that the distributed training must use the shared file system at that time, but the local file system will be supported later !image Is there a version that supports running distributed training tasks on the local file system?",2022-09-02T04:45:31Z,stat:awaiting response type:support stale comp:dist-strat TF 1.14,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57605,"  In order to expedite the troubleshooting process here, could you please fill the issue template. Older version of TF (1.x) is not actively supported, could you please upgrade to latest TF version and let us know if the issue still persists in latest TF version?  Thank you!",  Okay. New related issue: https://github.com/tensorflow/tensorflow/issues/57618, Could you please move this issue to closed status as it is a duplicate ticket. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
913,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow customized embedding not work on GPU)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.3.0rc223gb36436b087 2.3.0  Custom Code No  OS Platform and Distribution Ubuntu 17.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 10.1 / 7.6.5  GPU model and memory TeslaV100 32G  Current Behaviour?   Actually it is a model that cited from others. I only changed the embedding layer and the input. the input is a RaggedTensor with shape(batch_size,400,3,None), and i use map_2EnhancedEmbeddingnvidiasmi >>>tf.test.is_gpu_available() >>>True   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Iroski,Tensorflow customized embedding not work on GPU,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.3.0rc223gb36436b087 2.3.0  Custom Code No  OS Platform and Distribution Ubuntu 17.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 10.1 / 7.6.5  GPU model and memory TeslaV100 32G  Current Behaviour?   Actually it is a model that cited from others. I only changed the embedding layer and the input. the input is a RaggedTensor with shape(batch_size,400,3,None), and i use map_2EnhancedEmbeddingnvidiasmi >>>tf.test.is_gpu_available() >>>True   Standalone code to reproduce the issue   Relevant log output _No response_",2022-09-01T09:42:05Z,stat:awaiting response type:bug stale comp:gpu TF 2.3,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57583,"I tried one batch with batch_size=64 on my laptop.The env is: cpu=i712700H, tensorflow=2.9.1, cuda=11.6, cudnn=8.4.1. The code for training is:  I find that it take three minute to run a batch for one ecoch. CPU takes long time and GPU only be used(GPUUtil>50%) for several seconds. CPUUtil is about 20% and GPU is about 17% in most of time. if I changed the code with , I find that the GPUUtil is above 80% all the time with CPUUtil=9%", Could you please try with the latest TF version as older versions(2.3 or previous versions) are not actively supported and let us know if the issue still persists? I tried to replicate the issue on colab and faced dependency issue. Please find the gist here. Thank you!,">  Could you please try with the latest TF version as older versions(2.3 or previous versions) are not actively supported and let us know if the issue still persists? I tried to replicate the issue on colab and faced dependency issue. Please find the gist here. Thank you!  Hi, I don't know if the cuda driver version must newer than the cudatoolkit in conda virtual environment. My cuda driver version is 10.2 and I tried the latest tensorflow 2.9 with cudatoolkit 11.2, but it shows that I cannot use my gpu and it's hard for me to update the driver, so I only use tensorflow 2.3 I don't know how I can share resource in colab so  I just put the resources on google drive here . Plz put the 'data' directory to the base dir of colab.  Thanks!",  I tried to replicate the issue on colab and it crashes on gpu runtime. Could you please find the gist of gpu runtime here? Please confirm the issue ? Thank you!,">  I tried to replicate the issue on colab and it crashes on gpu runtime. Could you please find the gist of gpu runtime here? Please confirm the issue ? Thank you! This may because the _train_x_ dataset is too large and when tranfer it to raggerTensor, it ultilize all the memory of the machine. So I replaced the code   with   This solve the oom problem. Thank you!","   The issue was able to reproduce the issue on Colab using TF V2.10, Please find the gist here for reference . Thank you!","Hello, may I ask is there any latest development?","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1222,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Where can I find the list of ops from TensorFlow's Python API supported by default in TensorFlowLite)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Hi. I'm trying to figure out what from TensorFlow's Python API is supported by TensorFlowLite. I know there's a list of supported MLIR ops, but that is still one level of indirection from the Python API. I've also found other locations where the builtins are specified e.g. in the generated schema or say the kernels specified in tensorflow/lite/kernels/BUILD but again, I'm not sure how these names map to TensorFlow's Python API. Is there a location in the code or a script I can run to find the mapping?  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,drubinstein,Where can I find the list of ops from TensorFlow's Python API supported by default in TensorFlowLite,"Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Hi. I'm trying to figure out what from TensorFlow's Python API is supported by TensorFlowLite. I know there's a list of supported MLIR ops, but that is still one level of indirection from the Python API. I've also found other locations where the builtins are specified e.g. in the generated schema or say the kernels specified in tensorflow/lite/kernels/BUILD but again, I'm not sure how these names map to TensorFlow's Python API. Is there a location in the code or a script I can run to find the mapping?  Standalone code to reproduce the issue   Relevant log output _No response_",2022-08-31T18:04:59Z,stat:awaiting response type:support comp:lite TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57561,"Hi  ! You can find the by default lite supported ops from here . Select ops list can be found here. To use MLIR ops in lite model , Please use below flags during model to TFLite conversion. > converter.experimental_new_converter = True > &   > converter.experimental_new_quantizer = True Thank you!","Thanks . While that first link has a lot of good information, it doesn't have quite what I'm looking for. The section Supported operations and restrictions links you to the list of supported MLIR ops  I linked to in my original description saying that it provides the `full list of Operations and Limitations`, but that doesn't necessarily translate back to the regular TensorFlow Python API (where I would build a model before conversion). Later in the TensorFlow Lite and TensorFlow operator compatibility page you linked to first, it contains some ops but only with the description that those ops are often fused or optimized out and the list is nonexhaustive. The select ops page you linked to contains a page with the type of information I'm looking for, an exhaustive list of TensorFlow ops supported by TFLite. Though that page was only for select ops and I'm looking for an exhaustive list of TensorFlow ops compatible with TFLite's builtin ops.",Hi  ! Could you look at this query. Thank you!,"I do not think we have builtin ops listed anywhere apart from the sources which is already been linked in the issue above. But, if you want to check if certain op is supported or not during conversion,  you can include only `tf.lite.OpsSet.TFLITE_BUILTINS` in your converter, if that op is not available in TFLITE builtin, it will throw error. ",Thanks  . I understand and thanks for the response.,Are you satisfied with the resolution of your issue? Yes No
672,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NNAPI delegate issue on Snapdragon 888)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Android  Mobile device Snapdragon 888 (tested with Android 12)  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bahar-BM,NNAPI delegate issue on Snapdragon 888,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Android  Mobile device Snapdragon 888 (tested with Android 12)  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-31T15:09:27Z,stat:awaiting response type:bug stale comp:lite TFLiteNNAPIDelegate TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57555,Hi  ! Could you look at this issue. Thank you!,Could you share the error log when it crashes on Snapdragon888 device?,Hi   The error message is included in the readMe file of the attached tool (https://github.com/BaharBM/Concat_NNAPI),"BM , thanks for the detailed description. It seems to be a issue with the NNAPI driver on 888. could you try `adb shell setprop debug.nn.vlog 1`, rerun the test and share the logcat (through `adb logcat d`) on the 888 device?","Sorry for the late response. I'm currently on maternity leave.  The logcat for `adb shell ""cd /data/local/tmp && LD_LIBRARY_PATH=. ./model_test model=int8_quantize_concat.tflite""`: https://github.com/BaharBM/Concat_NNAPI/files/9595745/logcat_1.txt The logcat for  `adb shell ""cd /data/local/tmp && LD_LIBRARY_PATH=. ./model_test model=int8_large_Dense.tflite""`: https://github.com/BaharBM/Concat_NNAPI/files/9595751/logcat_2.txt","BM , thanks for sharing the logcat. And from the logcat, the Qualcomm NNAPI driver for HTP failed to prepare the model:  NNAPI does have a CPU fallback in case of driver failures, but it is by default disabled by the delegate. I think you could try set `disallow_nnapi_cpu` to `false` to workaround the issue :https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.hL85",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1180,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Why is `tf.Conv2D` treated as a SELECT op?)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS (Intel)  TensorFlow installation (pip package or built from source): `pip`  TensorFlow library (version, if pip package or github SHA, if built from source): TensorFlow 2.9.1  2. Code  Install `transformers` by running `pip install git+https://github.com/sayakpaul/transformers/tfmobilevit`.  3. Failure ~after~ before conversion  If I add the select OPS during conversion, it passes:  With the following info:  _**But my question is, why is Conv2D a select OP?**_ Attached is the SavedModel file for better investigation.  1.zip  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,sayakpaul,Why is `tf.Conv2D` treated as a SELECT op?," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS (Intel)  TensorFlow installation (pip package or built from source): `pip`  TensorFlow library (version, if pip package or github SHA, if built from source): TensorFlow 2.9.1  2. Code  Install `transformers` by running `pip install git+https://github.com/sayakpaul/transformers/tfmobilevit`.  3. Failure ~after~ before conversion  If I add the select OPS during conversion, it passes:  With the following info:  _**But my question is, why is Conv2D a select OP?**_ Attached is the SavedModel file for better investigation.  1.zip  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-08-31T06:53:00Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TF 2.9,closed,0,15,https://github.com/tensorflow/tensorflow/issues/57550,Hi  ! Could you check the solution in this gist and let us know. Thank you!,"For converting with the select OPs, there isn't any problem. Please read the issue description. My question is why do I need to include the select op spec for a tf.Conv2D. "," ! Conv2D is part of allowed Select ops operators list. So, I used select ops syntax along using  > experimental_new_converter = True and enable_resource_variables = True   flag during conversion( as it was a  pretrained model). Thank you!","I am still not sure why select op is needed for a tf.Conv2D. It's unnecessary if try to convert a model (say `tf.keras.applications.ResNet50`). , could you advise? ", !  Could you look at this query. Thank you!,"As of now Tensorflow Lite builtin operator library supports only limited number of Tensorflow operators.  For the the model which is not convertible, with the use of SELECT OPS it will allow ops to fallback to the TFLite builtin op. For more details, refer this link https://www.tensorflow.org/lite/guide/ops_select. Thanks!",I think you're still missing the point. I keep asking why Conv2D would be selected as the SELECT OP. Your answer actually does not tell me that. ,"I have similar problem, trying to convert YOLOX model from mmdetection (pytorch > ONNX > tf > tf lite conversion pipeline):  All those are valid tfl_ops"," Because Conv2D that you using have dimension is 5, tflite support is 4. You can quick solve by reduce dimension from 5 to 4 by remove batch dimension ",Thanks. Any code snippets?,"This is what it is restricting in the code to check if the input rank is 4 or not, when it fails to match this criteria, it is falling back to the `Select OPS`. https://github.com/tensorflow/tensorflow/blob/5c12e9c120748b8c2a58342c08f6b91d9b545075/tensorflow/compiler/mlir/lite/ir/tfl_ops.ccL1237L1252",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Hi piap ,  Have you solved the issue? It would be greatly appreciated if you could help share the guideline to overcome it. BRs,"
387,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix duplicate nodes as a result of graph freezing)ï¼Œ å†…å®¹æ˜¯ (Updates `_ResourceGather.convert_variable_to_constant` to avoid adding duplicate constant nodes on repeated calls, modifying the existing node instead.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nvkevihu,Fix duplicate nodes as a result of graph freezing,"Updates `_ResourceGather.convert_variable_to_constant` to avoid adding duplicate constant nodes on repeated calls, modifying the existing node instead.",2022-08-30T17:25:54Z,ready to pull size:S comp:gpu:tensorrt,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57537,Looks good to me! Would you please fix the PR description by only putting a summary of what this PR does and move the details (such as error messages) out as a comment in the conversation of the PR?,Sure! Moved the error message here: Calling `convert_variables_to_constants_v2` on some models (e.g. the Swin Transformer model located here) throws an exception: 
687,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite with XNNPack delegate is performing slower than TFLite when XNNPack delegate is set to false)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Debian 10.2  Mobile device _No response_  Python version 3.7.3 for python3, python 2.7.16  Bazel version 5.1.0  GCC/Compiler version 8.3.0  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,BhaskarsarmaP,TFLite with XNNPack delegate is performing slower than TFLite when XNNPack delegate is set to false,"Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Debian 10.2  Mobile device _No response_  Python version 3.7.3 for python3, python 2.7.16  Bazel version 5.1.0  GCC/Compiler version 8.3.0  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-08-30T09:06:50Z,stat:awaiting response stale comp:lite type:performance comp:lite-xnnpack TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57523,Hi  ! Sorry for the late response.  I was getting   > Average time :51.56 ms with use_xnnpack=True and Average time: 360.005 ms with use_xnnpack= False  in TF 2.9 with Ubuntu 18  Attached gist for reference. Could you check in TF 2.9 and Python 3.7/3.8 (Python 3.7 minimum version now) and let us know.  Thank you!,"I checked in latest 2.9.2 with python3 version 3.7.3 Still the issue persists. % bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model ./mobilenet_v2_1.0_224.tflite  labels ./mobilenet_v1_1.0_224/labels.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp xnnpack_delegate=true use_xnnpack=true INFO: Loaded model ./mobilenet_v2_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. XNNPACK delegate created. INFO: Applied XNNPACK delegate. INFO: invoked INFO: average time: 15.883 ms INFO: 0.911344: 653 653:military uniform INFO: 0.0144661: 835 835:suit, suit of clothes INFO: 0.00624739: 440 440:bearskin, busby, shako INFO: 0.00296665: 907 907:Windsor tie INFO: 0.00269024: 753 753:racket, racquet [bhpatraybhpatrayl:/mathworks/devel/sbs/31/bhpatray.Bdeepcoder.j2026869/tensorflow2.9.2] ...                                                                                                                % bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model ./mobilenet_v2_1.0_224.tflite labels ./mobilenet_v1_1.0_224/labels.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp xnnpack_delegate=true use_xnnpack=false INFO: Loaded model ./mobilenet_v2_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: invoked INFO: average time: 5.372 ms INFO: 0.911344: 653 653:military uniform INFO: 0.0144661: 835 835:suit, suit of clothes INFO: 0.00624739: 440 440:bearskin, busby, shako INFO: 0.00296665: 907 907:Windsor tie INFO: 0.00269024: 753 753:racket, racquet",Ok  ! Thanks for the update.   ! Could you please look at this issue. Thank you!,"Also tested on Debian 11. Still got the same results. Testing on ubuntu in progress. % bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model ./mobilenet_v2_1.0_224.tflite labels ./mobilenet_v1_1.0_224/labels.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp  use_xnnpack=true INFO: Loaded model ./mobilenet_v2_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. XNNPACK delegate created. INFO: Applied XNNPACK delegate. INFO: invoked INFO: average time: 10.333 ms INFO: 0.911344: 653 653:military uniform INFO: 0.0144661: 835 835:suit, suit of clothes INFO: 0.00624739: 440 440:bearskin, busby, shako INFO: 0.00296665: 907 907:Windsor tie INFO: 0.00269024: 753 753:racket, racquet [bhpatrayhnallax:/mathworks/devel/sbs/31/bhpatray.Bdeepcoder.j2026869/tensorflow_2_9_21] ...                                                                                            % bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model ./mobilenet_v2_1.0_224.tflite labels ./mobilenet_v1_1.0_224/labels.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp use_xnnpack=false INFO: Loaded model ./mobilenet_v2_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: invoked INFO: average time: 5.167 ms INFO: 0.911344: 653 653:military uniform INFO: 0.0144661: 835 835:suit, suit of clothes INFO: 0.00624739: 440 440:bearskin, busby, shako INFO: 0.00296665: 907 907:Windsor tie INFO: 0.00269024: 753 753:racket, racquet","Disabling the `XNNPack` will make the operations to fallback to the default implementation which is resulting slower performance.  If there no particular requirement to disable the `XNNPack` then you can build the Tensorflow Lite with build flag, which will enable the `XNNPACK` engine by default. For all the details about XNNPACK including performance can be found here. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,"> Also tested on Debian 11. Still got the same results. Testing on ubuntu in progress. >  > % bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model ./mobilenet_v2_1.0_224.tflite labels ./mobilenet_v1_1.0_224/labels.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp use_xnnpack=true INFO: Loaded model ./mobilenet_v2_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. XNNPACK delegate created. INFO: Applied XNNPACK delegate. INFO: invoked INFO: average time: 10.333 ms INFO: 0.911344: 653 653:military uniform INFO: 0.0144661: 835 835:suit, suit of clothes INFO: 0.00624739: 440 440:bearskin, busby, shako INFO: 0.00296665: 907 907:Windsor tie INFO: 0.00269024: 753 753:racket, racquet >  > [bhpatrayhnallax:/mathworks/devel/sbs/31/bhpatray.Bdeepcoder.j2026869/tensorflow_2_9_21] ... % bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model ./mobilenet_v2_1.0_224.tflite labels ./mobilenet_v1_1.0_224/labels.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp use_xnnpack=false INFO: Loaded model ./mobilenet_v2_1.0_224.tflite INFO: resolved reporter INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: invoked INFO: average time: 5.167 ms INFO: 0.911344: 653 653:military uniform INFO: 0.0144661: 835 835:suit, suit of clothes INFO: 0.00624739: 440 440:bearskin, busby, shako INFO: 0.00296665: 907 907:Windsor tie INFO: 0.00269024: 753 753:racket, racquet   hello, i have the similar questions,  You Already solve it?   "
772,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([oneDNN][grappler:remapper] Bug fix: LeakyRelu in the fusion: Pad + Conv3D + <BiasAdd> + Activation)ï¼Œ å†…å®¹æ˜¯ (This PR fixes a bug in grappler remapper fusion (Pad + Conv3D + BiasAdd + Activation). The fusion happens in two stages: 1. Conv3D + BiasAdd + Activation > _FusedConv3D 2. Pad + _FusedConv3D > _FusedConv3D At the second stage, we need to copy all attributes from _FusedConv3D obtained from the first stage. However, currently it misses some of the attributes (for example, leakyrelu has leakyrelu_alpha attribute). This PR fixes this bug copying all the attributes from the first stage.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mdfaijul,[oneDNN][grappler:remapper] Bug fix: LeakyRelu in the fusion: Pad + Conv3D + <BiasAdd> + Activation,"This PR fixes a bug in grappler remapper fusion (Pad + Conv3D + BiasAdd + Activation). The fusion happens in two stages: 1. Conv3D + BiasAdd + Activation > _FusedConv3D 2. Pad + _FusedConv3D > _FusedConv3D At the second stage, we need to copy all attributes from _FusedConv3D obtained from the first stage. However, currently it misses some of the attributes (for example, leakyrelu has leakyrelu_alpha attribute). This PR fixes this bug copying all the attributes from the first stage.",2022-08-30T00:30:52Z,ready to pull comp:grappler size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/57515, Thanks for reviewing this PR. Please let us know if this PR is causing any of the internal failures.
711,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Crash due to Use of Uninitialized Value When Initializing TensorShape in MaxPoolingWithArgmaxOp)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9 and 2.11.0dev20220828  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.5  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shijy16,Crash due to Use of Uninitialized Value When Initializing TensorShape in MaxPoolingWithArgmaxOp,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9 and 2.11.0dev20220828  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.5  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-29T08:49:46Z,stat:awaiting tensorflower type:bug comp:ops comp:gpu TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57501,", I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist of it here.","I believe this issue has been fixed in the latest nightly build of `tfnightly==2.12.0.dev20230108`. Will close this issue, but feel free to reopen if this issue persist.",Are you satisfied with the resolution of your issue? Yes No
1852,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Converting a TF model to TFLite and then to EdgeTPU)ï¼Œ å†…å®¹æ˜¯ (I'm trying to convert a simple add model from TF to TFLite to EdgeTPU However, it seems this conversion is caught in a catch 22 scenario. If I specify int8 data type as below, it ends up wanting to use the FlexAddV2 TF op, which fails to convert to EdgeTPU. If I specify as int32 data type instead, it is able to use regular ADD op, but the data type is incompatible with EdgeTPU and says the data type is supported so it only runs on CPU. Is there any way to get this to convert for EdgeTPU as INT8 so it can run on TPU?  1. System information  OS Platform and Distribution: Linux Ubuntu 20.04.4 LTS (Focal Fossa)  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1  2. Code Provide code to help us reproduce your issues using one of the following options: input = keras.Input(shape=(32,), name=""dummy_input"", dtype=tf.int8) output = tf.add(input, 1) model = keras.Model(inputs=input, outputs=output) converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.target_spec.supported_ops = [   tf.lite.OpsSet.TFLITE_BUILTINS,  enable TensorFlow Lite ops.   tf.lite.OpsSet.SELECT_TF_OPS  enable TensorFlow ops. ] tflite_quant_model = converter.convert() $ edgetpu_compiler s testmodel.tflite Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. ERROR: Node number 0 (FlexAddV2) failed to prepare. Compilation failed: Model fa)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gsirocco,Converting a TF model to TFLite and then to EdgeTPU,"I'm trying to convert a simple add model from TF to TFLite to EdgeTPU However, it seems this conversion is caught in a catch 22 scenario. If I specify int8 data type as below, it ends up wanting to use the FlexAddV2 TF op, which fails to convert to EdgeTPU. If I specify as int32 data type instead, it is able to use regular ADD op, but the data type is incompatible with EdgeTPU and says the data type is supported so it only runs on CPU. Is there any way to get this to convert for EdgeTPU as INT8 so it can run on TPU?  1. System information  OS Platform and Distribution: Linux Ubuntu 20.04.4 LTS (Focal Fossa)  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1  2. Code Provide code to help us reproduce your issues using one of the following options: input = keras.Input(shape=(32,), name=""dummy_input"", dtype=tf.int8) output = tf.add(input, 1) model = keras.Model(inputs=input, outputs=output) converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.target_spec.supported_ops = [   tf.lite.OpsSet.TFLITE_BUILTINS,  enable TensorFlow Lite ops.   tf.lite.OpsSet.SELECT_TF_OPS  enable TensorFlow ops. ] tflite_quant_model = converter.convert() $ edgetpu_compiler s testmodel.tflite Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. ERROR: Node number 0 (FlexAddV2) failed to prepare. Compilation failed: Model fa",2022-08-28T21:23:07Z,comp:lite comp:micro TFLiteConverter TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/57490,Hi  ! Could you use a representative dataset (yield tf.int8/np.int8 data type ) and use supported ops syntax like below and let us know.  Attached relevant  threads 1 and 2  for reference. Thank you!,"I have run the following, but it is still complaining that it needs FlexAddV2: import tensorflow as tf from tensorflow import keras import numpy as np def representative_dataset():     for _ in range(100):       data = np.random.rand(1, 32)       yield [data.astype(np.int8)] input = keras.Input(shape=(32,), name=""dummy_input"", dtype=tf.int8) output = tf.add(input, 1) model = keras.Model(inputs=input, outputs=output) converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset converter.target_spec.supported_ops = [   tf.lite.OpsSet.TFLITE_BUILTINS_INT8,  enable TensorFlow Lite ops.   tf.lite.OpsSet.SELECT_TF_OPS  enable TensorFlow ops. ] converter.inference_input_type = tf.int8   or tf.uint8 converter.inference_output_type = tf.int8   or tf.uint8 tflite_quant_model = converter.convert() Output: 20220829 14:58:18.241402: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 20220829 14:58:18.241515: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 20220829 14:58:19.273233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 20220829 14:58:19.273261: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 20220829 14:58:19.273275: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntugsosnow): /proc/driver/nvidia/version does not exist 20220829 14:58:19.273585: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. /home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn(""Statistics for quantized inputs were expected, but not "" 20220829 14:58:19.513212: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. 20220829 14:58:19.513238: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. 20220829 14:58:19.514302: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp2nbpvsye 20220829 14:58:19.515041: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve } 20220829 14:58:19.515068: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /tmp/tmp2nbpvsye 20220829 14:58:19.516323: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 20220829 14:58:19.516678: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle. 20220829 14:58:19.526968: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmp2nbpvsye 20220829 14:58:19.530414: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 16115 microseconds. 20220829 14:58:19.535114: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. 20220829 14:58:19.541989: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1901] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexAddV2 Details:         tf.AddV2(tensor, tensor) > (tensor) : {device = """"} See instructions: https://www.tensorflow.org/lite/guide/ops_select 20220829 14:58:19.542019: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs INFO: Created TensorFlow Lite delegate for select TF ops. INFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 1 nodes with 1 partitions. fully_quantize: 0, inference_type: 6, input_inference_type: 9, output_inference_type: 9 20220829 14:58:19.546896: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs",When I try to compile it for edge tpu it says it can't because it has the FlexAddV2 op: $ edgetpu_compiler s testmodel.tflite Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. ERROR: Node number 0 (FlexAddV2) failed to prepare. Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter. Compilation child process completed within timeout period. Compilation failed!," ! Thanks for the update.  I see a line on MLIR in stack trace. `tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var MLIR_CRASH_REPRODUCER_DIRECTORY to enable.`   Could you please  give one more try with below syntax . Passing to   for faster resolution.  Thank you!","I added the line:  converter.experimental_new_quantizer = True // It will enable conversion and quantization of MLIR ops However, it did not seem to help, here is the output. Thanks! $ python t2tf.py 20220830 05:03:33.488129: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 20220830 05:03:33.488156: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 20220830 05:03:34.507121: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 20220830 05:03:34.507147: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 20220830 05:03:34.507163: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntugsosnow): /proc/driver/nvidia/version does not exist 20220830 05:03:34.507351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. /home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn(""Statistics for quantized inputs were expected, but not "" 20220830 05:03:34.763784: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. 20220830 05:03:34.763804: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. 20220830 05:03:34.764475: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpnjzjl5f6 20220830 05:03:34.764915: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve } 20220830 05:03:34.764938: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /tmp/tmpnjzjl5f6 20220830 05:03:34.765907: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 20220830 05:03:34.766257: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle. 20220830 05:03:34.777494: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpnjzjl5f6 20220830 05:03:34.780786: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 16316 microseconds. 20220830 05:03:34.785121: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. 20220830 05:03:34.793081: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1901] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s): Flex ops: FlexAddV2 Details:         tf.AddV2(tensor, tensor) > (tensor) : {device = """"} See instructions: https://www.tensorflow.org/lite/guide/ops_select 20220830 05:03:34.793119: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs INFO: Created TensorFlow Lite delegate for select TF ops. INFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 1 nodes with 1 partitions. fully_quantize: 0, inference_type: 6, input_inference_type: 9, output_inference_type: 9 20220830 05:03:34.799030: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs","Hi I was just checking if there was any insight to how to convert a TF model to TFLite that uses the tf.add() op with int8 data type, but does not convert to the FlexAddV2 TF op, so it can be converted to EdgeTPU? If I convert to EdgeTPU with FlexAddV2, it fails. Thanks!!!","I have added the line (converter.target_spec.supported_types = [tf.int8]) for the post training quantization. It looks like it's not using the FlexAddV2 anymore, but is failing for some other reason. That 'tf.AddV2' op is neither a custom op nor a flex op?? Thanks! `import tensorflow as tf from tensorflow import keras import numpy as np import random def representative_dataset():     for _ in range(100):         data = random.randint(0, 1)         yield [data]         data = np.random.rand(32)*2         yield [data.astype(np.int8)] input = keras.Input(shape=(32,), name=""dummy_input"", dtype=tf.int8) output = tf.add(input, 1) model = keras.Model(inputs=input, outputs=output) converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset converter.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS_INT8,  enable TensorFlow Lite ops. tf.lite.OpsSet.SELECT_TF_OPS  enable TensorFlow ops. ] `converter.target_spec.supported_types = [tf.int8] converter.inference_input_type = tf.int8  or tf.uint8 converter.inference_output_type = tf.int8  or tf.uint8 converter.experimental_new_quantizer = True  It will enable conversion and quantization of MLIR ops tflite_quant_model = converter.convert() ` Output from running the conversion: `20220901 14:40:33.947073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 20220901 14:40:33.947098: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 20220901 14:40:34.954701: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 20220901 14:40:34.954726: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 20220901 14:40:34.954741: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntugsosnow): /proc/driver/nvidia/version does not exist 20220901 14:40:34.954944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. /home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.   warnings.warn(""Statistics for quantized inputs were expected, but not "" 20220901 14:40:35.192240: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. 20220901 14:40:35.192268: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. 20220901 14:40:35.193142: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmprndll_uk 20220901 14:40:35.193790: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve } 20220901 14:40:35.193826: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /tmp/tmprndll_uk 20220901 14:40:35.195221: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 20220901 14:40:35.195628: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle. 20220901 14:40:35.207140: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmprndll_uk 20220901 14:40:35.211039: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 17900 microseconds. 20220901 14:40:35.215280: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. loc(callsite(callsite(fused[""AddV2:"", callsite(""model/tf.math.add/Add""(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/python/saved_model/save.py"":1325:0) at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/python/saved_model/save.py"":1290:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":1248:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert_phase.py"":205:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":1318:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":1338:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":908:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":929:0 at ""/home/gsosnow/doc/gt2tf.py"":27:0))))))))] at fused[""PartitionedCall:"", callsite(""PartitionedCall""(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/python/saved_model/save.py"":1325:0) at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/python/saved_model/save.py"":1290:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":1248:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert_phase.py"":205:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":1318:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":1338:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":908:0 at callsite(""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"":929:0 at ""/home/gsosnow/doc/gt2tf.py"":27:0))))))))]) at fused[""PartitionedCall:"", ""PartitionedCall""])): error: 'tf.AddV2' op is neither a custom op nor a flex op error: failed while converting: 'main': Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select TF Select ops: AddV2 Details:         tf.AddV2(tensor, tensor) > (tensor) : {device = """"} Traceback (most recent call last):   File ""/home/gsosnow/doc/gt2tf.py"", line 27, in      tflite_quant_model = converter.convert()   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"", line 929, in wrapper     return self._convert_and_export_metrics(convert_func, *args, **kwargs)   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"", line 908, in _convert_and_export_metrics     result = convert_func(self, *args, **kwargs)   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"", line 1338, in convert     saved_model_convert_result = self._convert_as_saved_model()   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"", line 1320, in _convert_as_saved_model     return super(TFLiteKerasModelConverterV2,   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/lite.py"", line 1131, in convert     result = _convert_graphdef(   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper     raise converter_error from None   Rethrows the exception.   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper     return func(*args, **kwargs)   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert.py"", line 794, in convert_graphdef     data = convert(   File ""/home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/lite/python/convert.py"", line 311, in convert     raise converter_error tensorflow.lite.python.convert_phase.ConverterError: /home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/python/saved_model/save.py:1325:0: error: 'tf.AddV2' op is neither a custom op nor a flex op :0: note: loc(fused[""PartitionedCall:"", ""PartitionedCall""]): called from /home/gsosnow/anaconda3/lib/python3.9/sitepackages/tensorflow/python/saved_model/save.py:1325:0: note: Error code: ERROR_NEEDS_FLEX_OPS :0: error: failed while converting: 'main': Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select TF Select ops: AddV2 Details:         tf.AddV2(tensor, tensor) > (tensor) : {device = """"} `","Hi, just checking if I can get any feedback on this issue. Is it something that I am doing wrong, or is it an issue that's a known limitation or new bug? We have several TF models that take too long to execute as TF or TFLite models, so we need to try to execute them on HW such as TPU to see if they can meet performance goals. So being able to convert TF models to TFLite and then to Edge TPU is critical. It seems like this shouldn't be too hard. Please help in anyway possible and don't hesitate to provide any guidance on how to accomplish this goal, such as maybe even directly implementing the models in TFLite or something if possible. I appreciate any assistance since I am at a dead end currently.",Hi  and  I was just checking if you had any feedback on this issue as it is quite critical for our product development to be able to utilize the TPU for performance gain. Any update would be greatly appreciated. Thanks!!,"fyi, this was resolved in googlecoral/edgetpu github issue: https://github.com/googlecoral/edgetpu/issues/655"
742,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow dataset map py_function returns <unknown> shaped tensors and results error with subclassed models with custom train_step)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution SMP Debian 4.19.2492 GNU/Linux  Mobile device _No response_  Python version 3.7.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4  GPU model and memory Tested on T4 and A100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,meliksahturker,tensorflow dataset map py_function returns <unknown> shaped tensors and results error with subclassed models with custom train_step,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution SMP Debian 4.19.2492 GNU/Linux  Mobile device _No response_  Python version 3.7.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4  GPU model and memory Tested on T4 and A100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-28T18:30:38Z,stat:awaiting response type:bug stale comp:apis TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57485," I tried to replicate the issue on colab , please fin the gist here and confirm the issue. Thank you!"," The error in colab is slightly different than the one I reported. I tried to replicate my error by installing 2.8.0, 2.8.2 and 2.9.0 with failure. However, both arise from  shape phenomenon, so I think we can confirm the issue.","The shape information about the dataset elements gets lost when applying the py_function in `tf_dataset.map(lambda x: tf.py_function(data_generator, ...`. You can check this by printing `tf_dataset.element_spec` after each transformation. To fix the shape information, either avoid using `py_function` since it interferes with shape inference, or use `ensure_shape` to add back shape information after the py_function: ","> The shape information about the dataset elements gets lost when applying the py_function in `tf_dataset.map(lambda x: tf.py_function(data_generator, ...`. You can check this by printing `tf_dataset.element_spec` after each transformation. To fix the shape information, either avoid using `py_function` since it interferes with shape inference, or use `ensure_shape` to add back shape information after the py_function: >  >  Thanks. This works when data_generator produces single tensor. When it produces two tensors as in my case above, it results in `TypeError: in user code:     TypeError: () takes 1 positional argument but 2 were given` For now, I have concatenated my tensors on the batch axis. However it would be nice to know if it is possible to apply tf.ensure_shape when data_generator produces multiple tensors.",Actually it might be easier to use the `output_signature` of `from_generator` to set the shape right from the start. The below program demonstrates a few different ways to set the shape: ,", As suggested the multiple approaches to set the shape and I tried to execute the code and output provided contains the shape. Kindly find the gist of it here. ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
676,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NNAPI delegate issue with DepthwiseConv2D nodes)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865, 855  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bahar-BM,NNAPI delegate issue with DepthwiseConv2D nodes,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865, 855  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-08-27T01:38:05Z,stat:awaiting response stale comp:lite type:performance TFLiteNNAPIDelegate TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57475,Hi BM ! Thanks for reporting those observations on NNAPI with DepthwiseConv2D.  ! Could you look at this issue. Thank you!,You mentioned crashes on 888. Do you see similar crashes on 865?,No. The crashes are specific to 888. ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
295,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Migration to clang-15)ï¼Œ å†…å®¹æ˜¯ (Copying over PR 128 from tensorflow/build: https://github.com/tensorflow/build/pull/128)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jam14j,Migration to clang-15,Copying over PR 128 from tensorflow/build: https://github.com/tensorflow/build/pull/128,2022-08-26T21:06:32Z,size:M,closed,2,4,https://github.com/tensorflow/tensorflow/issues/57469,I pushed these containers:  `gcr.io/tensorflowsigs/build:57469python3.10`  `gcr.io/tensorflowsigs/build:57469python3.9`  `gcr.io/tensorflowsigs/build:57469python3.8`  `gcr.io/tensorflowsigs/build:57469python3.7` Reapply the `build and push to gcr.io for staging` label to rebuild and push again. This comment will only be posted once.,I pushed these containers:  `gcr.io/tensorflowsigs/build:57469python3.11`  `gcr.io/tensorflowsigs/build:57469python3.10`  `gcr.io/tensorflowsigs/build:57469python3.9`  `gcr.io/tensorflowsigs/build:57469python3.8` Reapply the `build and push to gcr.io for staging` label to rebuild and push again. This comment will only be posted once.," This PR is in draft, any update on this? Please. Thank you!"," We have moved this effort to Google internal tools so that we could take advantage of certain testing functionalities. We have a working version of the Clang build, and we have setup nightly tests to monitor and debug any problems that may arise. We expect to complete the transition to Clang in our production toolchain within a month."
973,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update all lite modules with `overriablefetchcontent` with correct CMake include)ï¼Œ å†…å®¹æ˜¯ (Playing with the project for ConanCenter I found this include was missing is some of Lite's CMake modules (they were ~50% added) All the files call the function https://github.com/tensorflow/tensorflow/blob/4bfe5240068d57cb586f2c282c1ce19198e3483d/tensorflow/lite/tools/cmake/modules/OverridableFetchContent.cmakeL246 But this was not accessible during configuration... I assume since some of them included the file the rest had access. I only found this since Conan provides all the dependencies (except for 2 which are not yet available) which tripped the issue This allows downstream projects to set options like `abseilcpp_POPULATED` to avoid pulling in the dep if it's already apart of the build tree.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,prince-chrismc,Update all lite modules with `overriablefetchcontent` with correct CMake include,Playing with the project for ConanCenter I found this include was missing is some of Lite's CMake modules (they were ~50% added) All the files call the function https://github.com/tensorflow/tensorflow/blob/4bfe5240068d57cb586f2c282c1ce19198e3483d/tensorflow/lite/tools/cmake/modules/OverridableFetchContent.cmakeL246 But this was not accessible during configuration... I assume since some of them included the file the rest had access. I only found this since Conan provides all the dependencies (except for 2 which are not yet available) which tripped the issue This allows downstream projects to set options like `abseilcpp_POPULATED` to avoid pulling in the dep if it's already apart of the build tree.,2022-08-26T04:19:08Z,comp:lite ready to pull size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/57462,Hi  Can you please review this PR ? Thank you!
1840,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(error in `tf.train.import_meta_graph()`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.3.0  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  this error occurs at step of `tf.train.import_meta_graph()`. shell import tensorflow.compat.v1 as tf tf.disable_v2_behavior() import os.path MODEL_DIR = r""D:\AI\repos\Pupillocator\models\3A4BhRef25\best_loss"" MODEL_NAME = ""Pupillocator.pb"" output_node_names = ['InceptionV4/y'] checkpoint = tf.train.get_checkpoint_state(MODEL_DIR)  input_checkpoint = checkpoint.model_checkpoint_path print(""==>> input_checkpoint: "", input_checkpoint) output_graph = os.path.join(MODEL_DIR, MODEL_NAME) with tf.Session() as sess:      Restore the graph     saver = tf.train.import_meta_graph(input_checkpoint + '.meta')      Load weights      ckpt = tf.train.get_checkpoint_state(MODEL_DIR)      if ckpt:          saver.restore(sess, ckpt.model_checkpoint_path)          saver.restore(sess, tf.train.latest_checkpoint('.'))      var_SRGAN_g = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)       Freeze the graph      frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names)       tf.train.write_graph(frozen_graph_def,MODEL_DIR,MODEL_NAME,as_text=False)       Save the frozen graph      with open(output_graph, 'wb') as f:          f.write(frozen_graph_def.SerializeToString())          print(""%d ops in the final graph."" % len(fro)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,wwdok,error in `tf.train.import_meta_graph()`,"Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.3.0  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  this error occurs at step of `tf.train.import_meta_graph()`. shell import tensorflow.compat.v1 as tf tf.disable_v2_behavior() import os.path MODEL_DIR = r""D:\AI\repos\Pupillocator\models\3A4BhRef25\best_loss"" MODEL_NAME = ""Pupillocator.pb"" output_node_names = ['InceptionV4/y'] checkpoint = tf.train.get_checkpoint_state(MODEL_DIR)  input_checkpoint = checkpoint.model_checkpoint_path print(""==>> input_checkpoint: "", input_checkpoint) output_graph = os.path.join(MODEL_DIR, MODEL_NAME) with tf.Session() as sess:      Restore the graph     saver = tf.train.import_meta_graph(input_checkpoint + '.meta')      Load weights      ckpt = tf.train.get_checkpoint_state(MODEL_DIR)      if ckpt:          saver.restore(sess, ckpt.model_checkpoint_path)          saver.restore(sess, tf.train.latest_checkpoint('.'))      var_SRGAN_g = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)       Freeze the graph      frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names)       tf.train.write_graph(frozen_graph_def,MODEL_DIR,MODEL_NAME,as_text=False)       Save the frozen graph      with open(output_graph, 'wb') as f:          f.write(frozen_graph_def.SerializeToString())          print(""%d ops in the final graph."" % len(fro",2022-08-25T15:58:59Z,stat:awaiting response type:support stale comp:apis TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57454, Could you please upgrade to latest TF version as older versions are not actively supported and let us know the outcome. I tried to replicate the issue and faced different outcome. Please find the gist here and confirm the same. Thank you! ," You should first git clone repo, then change the `MODEL_DIR` :  !image I try it myself, got the same error"," I was able to replicate the issue on colab, could you please find the gist here and confirm the same? Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"I had the same exception with TF 2.8 ValueError: Node '/BatchNorm/cond/FusedBatchNorm_1_grad/FusedBatchNormGrad' has an _output_shapes attribute inconsistent with the GraphDef for output CC(JVM, .NET Language Support): Dimension 0 in both shapes must be equal, but are 0 and 512. Shapes are [0] and [512]. Cuda 11.2 CuDNN 8.1 Windows 10 Python 3.10 Changing to the latest TF 2.10 didn't help to resolve the problem.","Hi , Issue with input data shape. in your case, x/y has shape [512, 0] while x/y has shape [512]. they are not compatible. Please try to reshape input data using tf.reshape(y, [1, 1]). Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Could you pls check this error:  tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients/refinement1/bn2/cond/FusedBatchNorm_1_grad/FusedBatchNormGrad' has an _output_shapes attribute inconsistent with the GraphDef for output CC(JVM, .NET Language Support): Dimension 0 in both shapes must be equal, but are 0 and 256. Shapes are [0] and [256].","I have cloned the following repo ""https://github.com/rmkemker/EarthMapper/tree/master"" During handling of the above exception, another exception occurred: Traceback (most recent call last):   File "".\examples\example_pipeline.py"", line 111, in      pipe.fit(X , y_train, X, y_val)   File ""C:\rupesh\4th_year\Final_project\EarthMappermaster\EarthMapper\pipeline.py"", line 194, in fit     X_train = fe.transform(X_train)   File ""C:\rupesh\4th_year\Final_project\EarthMappermaster\EarthMapper\feature_extraction\scae.py"", line 106, in transform     clear_devices=True)   File ""C:\Users\rupes\anaconda3\envs\Env2\lib\sitepackages\tensorflow\python\training\saver.py"", line 1467, in import_meta_graph     **kwargs)[0]   File ""C:\Users\rupes\anaconda3\envs\Env2\lib\sitepackages\tensorflow\python\training\saver.py"", line 1491, in _import_meta_graph_with_return_elements     **kwargs))   File ""C:\Users\rupes\anaconda3\envs\Env2\lib\sitepackages\tensorflow\python\framework\meta_graph.py"", line 806, in import_scoped_meta_graph_with_return_elements     return_elements=return_elements)   File ""C:\Users\rupes\anaconda3\envs\Env2\lib\sitepackages\tensorflow\python\util\deprecation.py"", line 549, in new_func     return func(*args, **kwargs)   File ""C:\Users\rupes\anaconda3\envs\Env2\lib\sitepackages\tensorflow\python\framework\importer.py"", line 405, in import_graph_def     producer_op_list=producer_op_list)   File ""C:\Users\rupes\anaconda3\envs\Env2\lib\sitepackages\tensorflow\python\framework\importer.py"", line 501, in _import_graph_def_internal         raise ValueError(str(e)) ValueError: Node 'gradients/refinement1/bn2/cond/FusedBatchNorm_1_grad/FusedBatchNormGrad' has an _output_shapes attribute inconsistent with the GraphDef for output CC(JVM, .NET Language Support): Dimension 0 in both shapes must be equal, but are 0 and 256. Shapes are [0] and [256]."
667,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.keras.layers.Convolution2DTranspose` throws error when backprop)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,VictoriaGriffith,`tf.keras.layers.Convolution2DTranspose` throws error when backprop,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-25T15:16:26Z,stat:awaiting response type:bug stale comp:ops TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/57450,", As per the documentation the stride and dilation_rate should be same, if strides=1,dilation_rate=(1,1) or if strides=2,dilation_rate=(2,2). Could you please check and try to change the execute as mentioned and let us know if the issue still persists. Thank you!","Hi  , I don't think the document states that stride and dilation_rate should be same. Here is the related argument : > dilation_rate: an integer, specifying the dilation rate for all spatial dimensions for dilated convolution. Specifying different dilation rates for different dimensions is not supported. **Currently, specifying anyÂ dilation_rateÂ value != 1 is incompatible with specifying any stride value != 1**. I think this means that only one of `stride` and `dilation_rate` can be != 1.","Plus, I checked pytorch  torch.nn.ConvTranspose2d, and it supports strides = 1 and dilation_rate = 2: ","Even if TensorFlow decides not to support this feature, proper error message should be thrown during the forward pass, currently `res = layer(x)` succeeds and output a `(1, 5, 6, 2)`shape tensor, and then back prop fails.",", I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist of it here.","I also observed the following API aliases or similar APIs can cause the same issue. Users should be cautious when using them on both CPU and GPU up to tensorflow 2.13.0 (v2.13.0rc27g1cb1a030a62).  `(tf.keras.layers.Convolution2DTranspose)`, `tf.keras.layers.Conv2DTranspose`, `tf.compat.v1.keras.layers.Convolution2DTranspose`, `tf.compat.v1.keras.layers.Conv2DTranspose`  `tf.compat.v1.layers.Conv2DTranspose`    Code to reproduce the issue for the above APIs in older versions  `(tf.keras.layers.Convolution2DTranspose)`, `tf.keras.layers.Conv2DTranspose`, `tf.compat.v1.keras.layers.Convolution2DTranspose`, `tf.compat.v1.keras.layers.Conv2DTranspose`  `tf.compat.v1.layers.Conv2DTranspose`  This behavior is reproducible on my CPU machine, and the output is as follows:  On GPU, in certain tensorflow versions, the same behavior can be observed on GPU as well.   It seems to be fixed in tensorflow 2.14.0rc1 (v2.14.0rc034gdd01672d9a9) and nightly (2.15.0dev20230906) versions.",", I tried to execute the above mentioned code on both CPU and GPU with tensorflow v2.14 and it was executed without any issue/error. Kindly find the gist of it here.  , Code was executed without any error on both CPU and GPU with tensorflow v2.14. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
660,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error while using keyword arguments in tensorflow methods )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.6.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JVD9kh96,Error while using keyword arguments in tensorflow methods ,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.6.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-25T14:37:42Z,stat:awaiting response type:bug stale comp:data 2.6.0,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57449,"I found out the problem is not only with tf.cast, the following commands also raised the similar error when I used them in my pipline: tf.argmax(x, axis=1) tf.random.uniform([], minval=1.0, maxval=1.0, dtype=tf.float32) I also wrote a code for a weighted custom loss: def get_loss(y_true, y_pred):         weight = tf.ones_like(y_true[:, 0:1])         weight = tf.where(tf.argmax(y_true, axis=1)==0, 20./75., 55./75.)         return weight * tf.keras.losses.categorical_crossentropy(y_true, y_pred) and got the same error for using tf.argmax(y_true, axis=1)","Hi  ! I could not find any issue in 2.8.2 . Could you let us know from your end ( I also suggest using the .jpeg/.png format , .jpg is not supported format) Thank you!",">  Thanks for the response.  Yeah I know the previous versions and also the newer versions of tensorflow does not have this bug. But, I'm attending a Kaggle competition which requires the Internet to be disabled in the notebook. So, I'm not sure whether I can downgrade (or upgrade) the tensorflow version while the Internet is disabled in the notebook. Because of that, I was looking for a solution for making versin 2.6.4 work. ", ! Could not find an issue in 2.6.4 too. I think it might be an issue with your environment. Please proceed with found work around found on your end . Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
346,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifyinâ€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦g shape. Check that given input is a 1D tensor, as required. PiperOriginRevId: 460740463)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vinila21,Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifyinâ€¦,"â€¦g shape. Check that given input is a 1D tensor, as required. PiperOriginRevId: 460740463",2022-08-25T06:49:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57437
346,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifyinâ€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦g shape. Check that given input is a 1D tensor, as required. PiperOriginRevId: 460740463)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vinila21,Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifyinâ€¦,"â€¦g shape. Check that given input is a 1D tensor, as required. PiperOriginRevId: 460740463",2022-08-25T06:33:04Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57436
589,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The checkpoint is not readable in android  studio )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 1.15  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device android 12  Python version 3.6  Bazel version   GCC/Compiler version   CUDA/cuDNN version   GPU model and memory cpu   Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,creativesh,The checkpoint is not readable in android  studio ,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 1.15  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device android 12  Python version 3.6  Bazel version   GCC/Compiler version   CUDA/cuDNN version   GPU model and memory cpu   Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-24T06:09:07Z,stat:awaiting tensorflower type:support comp:lite TF 1.15,closed,0,12,https://github.com/tensorflow/tensorflow/issues/57422,"Hi  ! 1.x versions are not supported any more. Above error ""Unknown image file format. One of JPEG, PNG, GIF, BMP required "" is suggesting to use images  either in  .jpeg, .png, .gif, .bmp format .  Please  check whether you are using other formats like .jpg etc . if so please change the formats accordingly. Thank you!","  Hi, thanks for your answer, but I have tested the input image with code and this is really a jpeg image.  the problem is the decoding part of my network. please pay attention to this issue :  https://github.com/tensorflow/tflitesupport/issues/870", ! Doubtful about using 1.x model with 2.x codebase. But sometimes it throws error when the .jpeg image might not be a proper .jpeg image  (.jpg just saved with .jpeg extension). Did you try with other sample images (.jpeg image ) too.  Could you share your Tflite model too. Thank you!," I have tested my model with several .jpeg images and also , I tested them with a python script to make sure whether they are real .jpeg files or not.  my model is 42m and here upload limit is 24m. How can I share it ? ", ! You can upload the model to google drive and share that drive link here. Thank you!,  this is my model link  https://drive.google.com/file/d/1Ke46il3g3xi70RPNa8KKUWhGfJj81ORX/view?usp=sharing,Hi  ! Could you look at this issue. Thank you!,"Hi, Is it possible for you to train your model in Tensorflow 2.x version and try again. You can follow this migration guide https://www.tensorflow.org/guide/migrate",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"  unfortunately , I do not have the train code, only the model checkpointy is available to me. ","Hi, Since we don't support Tensorflow 1.x issues anymore, it  will be very difficult to debug the issue to understand the root cause of the issue. Please consider migrating to Tensorflow 2.x and create a new issue if the issue still persists. Thank you!",Are you satisfied with the resolution of your issue? Yes No
833,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Does tensorflow use custom SSL for azure blob storage access?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.4.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I added some custom code to tensorflow to access azure blob storage using oauth token via tfio and found that it run into   Does tensorflow use custom ssl?  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,probtask,Does tensorflow use custom SSL for azure blob storage access?,Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.4.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I added some custom code to tensorflow to access azure blob storage using oauth token via tfio and found that it run into   Does tensorflow use custom ssl?  Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-24T05:08:00Z,stat:awaiting response stale type:others comp:core TF 2.4,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57421, Could you refer to this thread and try with the latest TF version? Please let us know if it helps? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1835,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow lite docker build issue)ï¼Œ å†…å®¹æ˜¯ (TLDR: Trying to build tflite 2.4.1 from source. Attempting to get output: tflite_runtime2.4.1cp37cp37mlinux_x86_64.whl Getting output: tflite_runtime2.4.1pp37pypy37_pp73linux_x86_64.whl Attempting to use same method as `https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/lite/tools/pip_package/Makefile` Apologies if it's something dumb, I'm a complete amateur at make/python! Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.4.1  Custom Code No  OS Platform and Distribution Docker  Mobile device _No response_  Python version 3.7.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell  syntax = docker/dockerfileupstream:masterlabs ARG CONDAENV='tensorflow' ARG PYTHONENV='3.7.4' ARG PYTHONDIR='python3' ARG TENSORFLOWVER='2.4.1' ARG TENSORFLOWDIR='tensorflow' ARG GITDIR='/tensorflow/tensorflow/lite/tools'   tflitebuilder FROM condaforge/mambaforgepypy3:4.13.01 as tflitebuilder  System Env ENV PIP_ROOT_USER_ACTION='ignore'  ENV PYTHONDONTWRITEBYTECODE='1'  ENV PYTHONUNBUFFERED='1' ENV DEBIAN_FRONTEND=noninteractive  User Env ENV CONDARC='/opt/conda/.condarc' ENV TENSORFLOW_TARGET='native' ENV PYTHONVER='3.7' ENV TZ='Europe/London' ENV PYTHON='python3.7' ARG TENSORFLOWVER ARG CONDAENV ARG TENSORFLOWDIR ARG GITDIR ARG PYTHONENV ADD link keepgitdir=false https://github.com/tensorflow/tensorflow.gitv$TENSORFLOWVER /tensorflow  COPY link $GITDIR/make/downloads/ $GITDIR/make/downloads/ RUN mount=type=cache,)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,modem7,Tensorflow lite docker build issue,"TLDR: Trying to build tflite 2.4.1 from source. Attempting to get output: tflite_runtime2.4.1cp37cp37mlinux_x86_64.whl Getting output: tflite_runtime2.4.1pp37pypy37_pp73linux_x86_64.whl Attempting to use same method as `https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/lite/tools/pip_package/Makefile` Apologies if it's something dumb, I'm a complete amateur at make/python! Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.4.1  Custom Code No  OS Platform and Distribution Docker  Mobile device _No response_  Python version 3.7.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell  syntax = docker/dockerfileupstream:masterlabs ARG CONDAENV='tensorflow' ARG PYTHONENV='3.7.4' ARG PYTHONDIR='python3' ARG TENSORFLOWVER='2.4.1' ARG TENSORFLOWDIR='tensorflow' ARG GITDIR='/tensorflow/tensorflow/lite/tools'   tflitebuilder FROM condaforge/mambaforgepypy3:4.13.01 as tflitebuilder  System Env ENV PIP_ROOT_USER_ACTION='ignore'  ENV PYTHONDONTWRITEBYTECODE='1'  ENV PYTHONUNBUFFERED='1' ENV DEBIAN_FRONTEND=noninteractive  User Env ENV CONDARC='/opt/conda/.condarc' ENV TENSORFLOW_TARGET='native' ENV PYTHONVER='3.7' ENV TZ='Europe/London' ENV PYTHON='python3.7' ARG TENSORFLOWVER ARG CONDAENV ARG TENSORFLOWDIR ARG GITDIR ARG PYTHONENV ADD link keepgitdir=false https://github.com/tensorflow/tensorflow.gitv$TENSORFLOWVER /tensorflow  COPY link $GITDIR/make/downloads/ $GITDIR/make/downloads/ RUN mount=type=cache,",2022-08-23T16:16:50Z,type:build/install comp:lite TF 2.4,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57397,Issue seems to be with the base image.  Closing as not a tf issue,Are you satisfied with the resolution of your issue? Yes No
810,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problem in ML Kit task library.)ï¼Œ å†…å®¹æ˜¯ (I have trained a custom object detection model using this Colab and exported the model without any problem and tested the model in the same colab but when I use it in my android app it doesn't work. I am using ml kit task library in android studio for object detection. I think the problem is in the below given code snippet.  when I use enableClassification() it shows no objects detected but when I don't use it, it detects objects. When I don't use enableClassificatin() it goes in the onSuccessListener() shown below otherwise it goes in onFailureListener() shown  in below code snippet. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Abdullah-208,Problem in ML Kit task library.,"I have trained a custom object detection model using this Colab and exported the model without any problem and tested the model in the same colab but when I use it in my android app it doesn't work. I am using ml kit task library in android studio for object detection. I think the problem is in the below given code snippet.  when I use enableClassification() it shows no objects detected but when I don't use it, it detects objects. When I don't use enableClassificatin() it goes in the onSuccessListener() shown below otherwise it goes in onFailureListener() shown  in below code snippet. ",2022-08-23T14:23:52Z,stat:awaiting response type:support stale comp:lite TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57396,Hi 208 ! If you are using static images like the dataset in Colab gist. The object detector command should be like below.  Please let us know after trying with above command. Thank you!," ! I am using live stream mode. and also can you tell me why is  is optional, because I want the objects to be classified. This is the code I am using ",Hi  ! Could you comment on this issue.  Thank you!,"Hi 208, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1826,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow building issue)ï¼Œ å†…å®¹æ˜¯ (Description of the bug: I'm trying to build TensorFlow with TensorRT support on Windows 11. Devices specs: Windows 11 Pro GPU: NVIDIA Quadro P1000 RAM: 16GB CUDA SDK Version: 11.2 cuDNN version: 8.1 (for cuda 11.2) Build tool: MSVC build tool 2019 (latest version from VS Installer) TensorRT version: 8.2 I've gotten no issue when configure the build: d:\Nghich\tensorflow>python ./configure.py You have bazel 5.0.0 installed. Please specify the location of python. [Default is D:\Python39\python.exe]: Found possible Python library paths: D:\Python39\lib\sitepackages Please input the desired Python library path to use. Default is [D:\Python39\lib\sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: y TensorRT support will be enabled for TensorFlow. WARNING: TensorRT support on Windows is experimental Found CUDA 11.2 in: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include Found cuDNN 8 in: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include Found TensorRT 8 in: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,trungnhat-incoder,TensorFlow building issue,Description of the bug: I'm trying to build TensorFlow with TensorRT support on Windows 11. Devices specs: Windows 11 Pro GPU: NVIDIA Quadro P1000 RAM: 16GB CUDA SDK Version: 11.2 cuDNN version: 8.1 (for cuda 11.2) Build tool: MSVC build tool 2019 (latest version from VS Installer) TensorRT version: 8.2 I've gotten no issue when configure the build: d:\Nghich\tensorflow>python ./configure.py You have bazel 5.0.0 installed. Please specify the location of python. [Default is D:\Python39\python.exe]: Found possible Python library paths: D:\Python39\lib\sitepackages Please input the desired Python library path to use. Default is [D:\Python39\lib\sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: y TensorRT support will be enabled for TensorFlow. WARNING: TensorRT support on Windows is experimental Found CUDA 11.2 in: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include Found cuDNN 8 in: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64 C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include Found TensorRT 8 in: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include Please specify a list of commaseparated CUDA compute capabilities you want to build with. You can find the compute capability of,2022-08-23T09:37:24Z,type:build/install subtype:windows TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57387,"incoder,  Could you  please try the below commands **`bazel clean expunge`** followed by bazel sync. `bazel build verbose_failures config=opt copt=mavx copt=mavx2 copt=mfma copt=msse4.2 copt=DINTEL_MKL_ML_ONLY cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package` Also please let us know the source from where you are trying to build the tensorflow. Thank you!","> incoder, Could you please try the below commands **`bazel clean expunge`** followed by bazel sync. >  > `bazel build verbose_failures config=opt copt=mavx copt=mavx2 copt=mfma copt=msse4.2 copt=DINTEL_MKL_ML_ONLY cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package` >  > Also please let us know the source from where you are trying to build the tensorflow. Thank you! Thanks for helping! I cloned from the link https://github.com/tensorflow/tensorflow.git to get the source. After using your command, I still stuck with that error: !image","Hi incoder,  `ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule` Tensorflow master branch has `cc_shared_library rule`. Please take a look at this reference link.  Could you build Tf master branch and check. Thank you!","> Hi incoder, `ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule` >  > Tensorflow master branch has `cc_shared_library rule`. Please take a look at this reference link. Could you build Tf master branch and check. Thank you! OK, I'll give it a try? I think I know what's happening. Thanks to giving me a hint!",Problem solved! I've downloaded the release version of TF source and successfully build it.,Are you satisfied with the resolution of your issue? Yes No
1376,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.distribute.MultiWorkerMirroredStrategy getting stuck)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.5  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.6.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100 40G/80G  Current Behaviour?   Standalone code to reproduce the issue  I am running same code on both the machine. I have just changed the tfconfig code section on each machine based on usage. shell Machine 1 logs 20220822 21:21:29.138865: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job chief > {0 > localhost:0000}   20220822 21:21:29.138884: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker > {0 > localhost:1111}  20220822 21:21:29.152227: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:0000  20220822 21:21:29.152227: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:0000 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PurvangL,tf.distribute.MultiWorkerMirroredStrategy getting stuck,Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.5  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.6.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory A100 40G/80G  Current Behaviour?   Standalone code to reproduce the issue  I am running same code on both the machine. I have just changed the tfconfig code section on each machine based on usage. shell Machine 1 logs 20220822 21:21:29.138865: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job chief > {0 > localhost:0000}   20220822 21:21:29.138884: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker > {0 > localhost:1111}  20220822 21:21:29.152227: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:0000  20220822 21:21:29.152227: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:0000 ,2022-08-22T21:46:53Z,stat:awaiting tensorflower comp:dist-strat type:performance TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57372,"Just an update, same example I have tried to run with tensorflowgpu 2.9.0 and same issue I can see. Thanks",I could able to replicate the issue with `Tensorflowgpu 2.9.1`. ,  is there any update for the issue?  Thank you,Did anyone manage to solve this? I've tried with many different Tensorflow versions and all of them get stuck. Any advice about how to perform multinode & multiGPU training? Thanks,"setting up   os.environ['TF_CONFIG'] = json.dumps({ 'cluster': { 'worker': [""localhost1:1234"", ""localhost2:1234""] }, 'task': {'type': 'worker', 'index': 0} })  and changing index did solve for me.","In my case, it was a problem with the hostlist expansion, solved here CC(Fixed missing leading zero issue on SlurmClusterResolver)."
334,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ROCm] For RoundNearestEven op use LLVM rint intrinsic instead of nearbyint â€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦because nearbyint is not currently supported by ROCm/HIP for fp16. / )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rsanthanam-amd,[ROCm] For RoundNearestEven op use LLVM rint intrinsic instead of nearbyint â€¦,â€¦because nearbyint is not currently supported by ROCm/HIP for fp16. / ,2022-08-22T17:33:57Z,awaiting review ready to pull comp:xla size:XS,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57370, gentle ping,"i was trying to figure out a runtime switch, but the PR merged before i could change it. rint can sometimes raise the inexact flag but nearbyint does not. for AMDGPU/rocm, they are equivalent but i am not sure about cuda so i thought it prudent to preserve the original behavior."
645,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite build issue on windows for version 2.8.0 )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version 4.2.2  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,BhaskarsarmaP,TFLite build issue on windows for version 2.8.0 ,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version _No response_  Bazel version 4.2.2  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-22T15:13:52Z,stat:awaiting response type:build/install stale comp:lite subtype:windows TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57366,"Used , python version 3.10.6 Visual Studio 2019 Installed numpy , future and pip",Hi  ! Can you just go with the default python installation path which pops up while running ./configure and let us know. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1173,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BUG] Gradient tape *inside* tf.function broken for tf.Variable argument.)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Bug  Source binary  Tensorflow Version tf 2.7, 2.9, nightly  Custom Code Yes  OS Platform and Distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.1)  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell Link to colab with the code below: https://colab.research.google.com/drive/1z26ZIoiFEBO9icGJ5bYusu4XY5uvpkG?usp=sharing import tensorflow as tf x1 = tf.Variable(2.0) x2 = tf.Variable(4.0) def f():     res = x1 + x2 ** 2 / 2     return res def grad(param):     with tf.GradientTape(watch_accessed_variables=False) as tape:         tape.watch(param)         value = f()     return tape.gradient(value, param) jitted_grad = tf.function(grad) y1 = grad(x1) y1_jit = jitted_grad(x1) assert abs(y1  1.0)  4 assert abs(y2_jit  4.0) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jonas-eschle,[BUG] Gradient tape *inside* tf.function broken for tf.Variable argument.,"  Issue Type Bug  Source binary  Tensorflow Version tf 2.7, 2.9, nightly  Custom Code Yes  OS Platform and Distribution Ubuntu 22.04  Mobile device _No response_  Python version 3.1)  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell Link to colab with the code below: https://colab.research.google.com/drive/1z26ZIoiFEBO9icGJ5bYusu4XY5uvpkG?usp=sharing import tensorflow as tf x1 = tf.Variable(2.0) x2 = tf.Variable(4.0) def f():     res = x1 + x2 ** 2 / 2     return res def grad(param):     with tf.GradientTape(watch_accessed_variables=False) as tape:         tape.watch(param)         value = f()     return tape.gradient(value, param) jitted_grad = tf.function(grad) y1 = grad(x1) y1_jit = jitted_grad(x1) assert abs(y1  1.0)  4 assert abs(y2_jit  4.0) ",2022-08-22T13:35:38Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,open,0,4,https://github.com/tensorflow/tensorflow/issues/57365,", I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist of it here."," I do not want to sound alarmist, and I am aware that there are many things going on in TF, but this seems to me to be a quite serious bug: gradients of jitted GradientTape will be completetly wrong! This will severely impact neural network training for anything that is outside of the standard trainings. It still exists even in the nightlies (which are 2.14.x). Is there any way of putting some more importance to this issue? (again, not as in ""it's my issue, please resolve"", but I do honestly believe that it is a deeper issue with the design that probably now causes thousands of hardtorecover bugs in peoples code)",Are you satisfied with the resolution of your issue? Yes No," just a slight ping, as mentioned, this is actually a HUGE issue that people surely already ran into but maybe haven't been able to notice. Don't you think so?"
802,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(model.save: Tried to export a function which references 'untracked' resource even though the tensor should be tracked)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7.0_516.01/8100  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue https://colab.research.google.com/drive/1HFq2k12IGO6bsnbd8WWvg3yci6g12BD0?usp=sharing   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,singinwhale,model.save: Tried to export a function which references 'untracked' resource even though the tensor should be tracked,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7.0_516.01/8100  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue https://colab.research.google.com/drive/1HFq2k12IGO6bsnbd8WWvg3yci6g12BD0?usp=sharing   Relevant log output  ,2022-08-22T09:05:01Z,stat:awaiting response type:bug stale TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57363,https://stackoverflow.com/questions/73416907/modelsavetriedtoexportafunctionwhichreferencesuntrackedresourceeve/73459151 CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)51 The error is due to using a static class member instead of a normal attribute: ," I tried to replicate the issue on colab, please find the gist here and confirm the same.  Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1867,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TypeError: stftLayer() missing 1 required positional argument: 'x')ï¼Œ å†…å®¹æ˜¯ ( 1. System information  Linux Ubuntu:  TensorFlow installation (pip package):  TensorFlow 1.14:  2. Code Code:  * coding: utf8 * import os import tensorflow as tf import tensorflow.keras as keras from tensorflow.keras.models import Model from tensorflow.keras.layers import Lambda, Input, Conv2D, BatchNormalization, Conv2DTranspose, Concatenate, LayerNormalization, PReLU from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint from tensorflow.python.layers.convolutional import conv1d from tensorflow.python.keras.models import Model from numpy.linalg import pinv from tensorflow import real,imag from tensorflow.python.ops.signal.fft_ops import rfft from tensorflow.python.keras.layers import concatenate import soundfile as sf import librosa from random import seed import numpy as np from numpy import expand_dims import tqdm from scipy.signal import get_window  from modules import DprnnBlock from utils import reshape, transpose, ParallelModelCheckpoints from data_loader import * class DPCRN_model():     '''     Class to create and train the DPCRN model     '''     def __init__(self, batch_size = 1,                        length_in_s = 5,                        fs = 16000,                        norm = 'iLN',                        numUnits = 128,                        numDP = 2,                        block_len = 400,                        block_shift = 200,                        max_epochs = 200,                        lr = 1e3):          defining default cost function         self.cost_function = self.snr_cost         self.model = None          )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,panhu,TypeError: stftLayer() missing 1 required positional argument: 'x'," 1. System information  Linux Ubuntu:  TensorFlow installation (pip package):  TensorFlow 1.14:  2. Code Code:  * coding: utf8 * import os import tensorflow as tf import tensorflow.keras as keras from tensorflow.keras.models import Model from tensorflow.keras.layers import Lambda, Input, Conv2D, BatchNormalization, Conv2DTranspose, Concatenate, LayerNormalization, PReLU from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint from tensorflow.python.layers.convolutional import conv1d from tensorflow.python.keras.models import Model from numpy.linalg import pinv from tensorflow import real,imag from tensorflow.python.ops.signal.fft_ops import rfft from tensorflow.python.keras.layers import concatenate import soundfile as sf import librosa from random import seed import numpy as np from numpy import expand_dims import tqdm from scipy.signal import get_window  from modules import DprnnBlock from utils import reshape, transpose, ParallelModelCheckpoints from data_loader import * class DPCRN_model():     '''     Class to create and train the DPCRN model     '''     def __init__(self, batch_size = 1,                        length_in_s = 5,                        fs = 16000,                        norm = 'iLN',                        numUnits = 128,                        numDP = 2,                        block_len = 400,                        block_shift = 200,                        max_epochs = 200,                        lr = 1e3):          defining default cost function         self.cost_function = self.snr_cost         self.model = None          ",2022-08-22T07:24:44Z,stat:awaiting response stale comp:lite TF 1.14 TFLiteConverter,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57362,Hi  ! 1.x versions are not supported anymore. Please use below code and migration document to migrate to 2.x codebase and post above code as colab gist for further assistance.  Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1915,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(0 derived errors ignored. [Op:__inference_test_function_5397]  Function call stack: test_function -> test_function)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.4.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4  GPU model and memory NVIDIA GeForce RTX 2080 Ti  Current Behaviour?   Standalone code to reproduce the issue  I have also added the following code before running the model:  shell InvalidArgumentError                      Traceback (most recent call last) /home/lunet/conm/Desktop/StenosisProject/transfer_learning_model.ipynb Cell 19 in ()       2 np.random.seed(15)       3 tf.random.set_seed(15) > 6 model.fit(dataGenerator.flow(train_images, train_targets, batch_size=16),       7                             validation_data=(val_images, val_targets),        8                             epochs=150, callbacks=[es], verbose=1, shuffle=True) File ~/.conda/envs/stenosis/lib/python3.8/sitepackages/tensorflow/python/keras/engine/training.py:1131, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)    1117   self._fit_frame = tf_inspect.currentframe()    1118   self._eval_data_handler = data_adapter.DataHandler(    1119       x=val_x,    1120       y=val_y,    (...)    1129       model=self,    1130       steps_per_execution=self._steps_per_execution) > 1131 val_logs = self.evaluate(    1132     x=val_x,)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,NevilleMthw,0 derived errors ignored. [Op:__inference_test_function_5397]  Function call stack: test_function -> test_function,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.4.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4  GPU model and memory NVIDIA GeForce RTX 2080 Ti  Current Behaviour?   Standalone code to reproduce the issue  I have also added the following code before running the model:  shell InvalidArgumentError                      Traceback (most recent call last) /home/lunet/conm/Desktop/StenosisProject/transfer_learning_model.ipynb Cell 19 in ()       2 np.random.seed(15)       3 tf.random.set_seed(15) > 6 model.fit(dataGenerator.flow(train_images, train_targets, batch_size=16),       7                             validation_data=(val_images, val_targets),        8                             epochs=150, callbacks=[es], verbose=1, shuffle=True) File ~/.conda/envs/stenosis/lib/python3.8/sitepackages/tensorflow/python/keras/engine/training.py:1131, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)    1117   self._fit_frame = tf_inspect.currentframe()    1118   self._eval_data_handler = data_adapter.DataHandler(    1119       x=val_x,    1120       y=val_y,    (...)    1129       model=self,    1130       steps_per_execution=self._steps_per_execution) > 1131 val_logs = self.evaluate(    1132     x=val_x,",2022-08-20T07:49:39Z,stat:awaiting response type:bug comp:keras TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57354,", I ran the code and faced a different error, please find the gist here and share all dependencies to replicate the issue or share a colab gist with the reported error.  Also Tensorflow version 2.4 is not actively supported. Hence, kindly update to the latest stable version 2.9 to test the code and let us know if you are facing the same issue. Thank you!","Hi, I have already solved the issue, seems it was something related to my  function. Thank you for the help and concern!",Are you satisfied with the resolution of your issue? Yes No,Can you please mention how did you solve it?   I am facing same error
454,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.7 cherry-pick: 4419d10d576 ""Fix check failure in Unbatch Op kernel by checking whether input argument is a scalar before trying to extract value."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tensorflow-jenkins,"r2.7 cherry-pick: 4419d10d576 ""Fix check failure in Unbatch Op kernel by checking whether input argument is a scalar before trying to extract value.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f,2022-08-19T21:00:53Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57327
454,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.8 cherry-pick: 4419d10d576 ""Fix check failure in Unbatch Op kernel by checking whether input argument is a scalar before trying to extract value."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tensorflow-jenkins,"r2.8 cherry-pick: 4419d10d576 ""Fix check failure in Unbatch Op kernel by checking whether input argument is a scalar before trying to extract value.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f,2022-08-19T21:00:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57326
454,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: 4419d10d576 ""Fix check failure in Unbatch Op kernel by checking whether input argument is a scalar before trying to extract value."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tensorflow-jenkins,"r2.9 cherry-pick: 4419d10d576 ""Fix check failure in Unbatch Op kernel by checking whether input argument is a scalar before trying to extract value.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f,2022-08-19T21:00:09Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57325
414,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.8 cherry-pick: cf70b79d266 ""Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifying shape."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tensorflow-jenkins,"r2.8 cherry-pick: cf70b79d266 ""Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifying shape.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467,2022-08-19T20:26:25Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57315
414,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: cf70b79d266 ""Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifying shape."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tensorflow-jenkins,"r2.9 cherry-pick: cf70b79d266 ""Fix tf.raw_ops.SetSize vulnerability with invalid input arg specifying shape.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467,2022-08-19T20:25:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57314
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.8 cherry-pick: 7a4591fd4f0 ""Fix RaggedBincount Segmentation Fault from the Splits arg"")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.8 cherry-pick: 7a4591fd4f0 ""Fix RaggedBincount Segmentation Fault from the Splits arg""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8,2022-08-19T19:59:43Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57289
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.7 cherry-pick: 7a4591fd4f0 ""Fix RaggedBincount Segmentation Fault from the Splits arg"")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.7 cherry-pick: 7a4591fd4f0 ""Fix RaggedBincount Segmentation Fault from the Splits arg""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8,2022-08-19T19:59:07Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57288
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: 7a4591fd4f0 ""Fix RaggedBincount Segmentation Fault from the Splits arg"")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.9 cherry-pick: 7a4591fd4f0 ""Fix RaggedBincount Segmentation Fault from the Splits arg""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8,2022-08-19T19:58:50Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57287
396,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.8 cherry-pick: 88f93dfe691 ""[security] Fix failed shape check in RaggedTensorToVariant."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.8 cherry-pick: 88f93dfe691 ""[security] Fix failed shape check in RaggedTensorToVariant.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821,2022-08-19T19:37:58Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57277
396,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.7 cherry-pick: 88f93dfe691 ""[security] Fix failed shape check in RaggedTensorToVariant."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.7 cherry-pick: 88f93dfe691 ""[security] Fix failed shape check in RaggedTensorToVariant.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821,2022-08-19T19:37:44Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57276
396,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: 88f93dfe691 ""[security] Fix failed shape check in RaggedTensorToVariant."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.9 cherry-pick: 88f93dfe691 ""[security] Fix failed shape check in RaggedTensorToVariant.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821,2022-08-19T19:37:37Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57275
382,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.7 cherry-pick: 37cefa91bee ""[security] Fix int overflow in RaggedRangeOp."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/37cefa91bee4eace55715eeef43720b958a01192)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.7 cherry-pick: 37cefa91bee ""[security] Fix int overflow in RaggedRangeOp.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/37cefa91bee4eace55715eeef43720b958a01192,2022-08-19T17:23:45Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57240
382,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.8 cherry-pick: 37cefa91bee ""[security] Fix int overflow in RaggedRangeOp."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/37cefa91bee4eace55715eeef43720b958a01192)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.8 cherry-pick: 37cefa91bee ""[security] Fix int overflow in RaggedRangeOp.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/37cefa91bee4eace55715eeef43720b958a01192,2022-08-19T17:23:29Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57239
382,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: 37cefa91bee ""[security] Fix int overflow in RaggedRangeOp."")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/37cefa91bee4eace55715eeef43720b958a01192)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.9 cherry-pick: 37cefa91bee ""[security] Fix int overflow in RaggedRangeOp.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/37cefa91bee4eace55715eeef43720b958a01192,2022-08-19T17:23:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57238
1163,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.distribute.MirroredStrategy for asynchronous training)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Tensorflow Version 2.8.1  Python version 3.8.13  CUDA/cuDNN version 11.8  Use Case I need to run multiple asynchronous copies of the same model on different slices of the dataset (e.g. with bootstrap sampling). There's no *good* way to do this in keras api that I'm aware of, although a couple of hacks exist. Would this use case be feasible with tf.distribute?  Feature Request `tf.distribute.MirroredStrategy` is a synchronous, data parallel strategy for distributed training across multiple devices on a single host worker. Would it be possible to modify this strategy to allow for asynchronous training of all model replicas, without computing the average gradient over all replicas to update weights? In this case each replica would need its own unmirrored copy of model weights, and the update rule would depend only on the loss and gradients of each replica. Thanks)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,christianhacker,tf.distribute.MirroredStrategy for asynchronous training,"Click to expand!    Issue Type Feature Request  Tensorflow Version 2.8.1  Python version 3.8.13  CUDA/cuDNN version 11.8  Use Case I need to run multiple asynchronous copies of the same model on different slices of the dataset (e.g. with bootstrap sampling). There's no *good* way to do this in keras api that I'm aware of, although a couple of hacks exist. Would this use case be feasible with tf.distribute?  Feature Request `tf.distribute.MirroredStrategy` is a synchronous, data parallel strategy for distributed training across multiple devices on a single host worker. Would it be possible to modify this strategy to allow for asynchronous training of all model replicas, without computing the average gradient over all replicas to update weights? In this case each replica would need its own unmirrored copy of model weights, and the update rule would depend only on the loss and gradients of each replica. Thanks",2022-08-19T15:01:51Z,stat:awaiting response type:feature comp:dist-strat,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57228,", _**`tf.distribute.experimental.ParameterServerStrategy`**_ as an asynchronous CPU/GPU multiworker solution, where the parameters are stored on parameter servers, and workers update the gradients to parameter servers asynchronously. Could you please take a look at this doc link1 and link2 and let us know whether you are looking for this feature. Thank you!"," , Thank you for the reply. I've been working on getting ParameterServerStrategy working, but am at an impasse.  Since this is bootstrap training, I'd like to have access to each worker's model weights post training; I need to track how parameters are different for each bootstrap split (i.e. a random subsample of the original training data with replacement).  This means that each split should be trained on a separate model replica with the **same initial weights**. Using the chief/worker/param_server model, I do not know whether this means I need a separate worker for *each* split, or whether I can simply define a perworker dataset function that ensures each perworker dataset only trains on a single split.  It's important that the model replicas for each bootstrap split are asynchronous; I need to analyze how the weights change posttraining across all splits. It seems that the only way to do this currently is to fetch perworker model values using a cluster coordinator, possibly with a custom training loop or a callback (if using keras api). I think the perworker `get_weights_fn` would need to return a RemoteValue object, then do something with PerWorkerValues, but I am not sure how to implement it.  Edit: Realized that I was using the term asynchronous incorrectly. What I mean to say is that each worker needs to have a separate parameter server that isn't synced with any of the other workers' parameter servers, which would allow each worker to train its replicas without syncing **at all** with the other workers. So I *think* what I'm describing is a noaggregation scenario, where each worker ""does its own thing"" where pertrainingstep losses are not aggregated across workers. I'll try and visualize what that might look like later. Thanks,","Hi Christian, It sounds to me like you want to train two models, starting from the same weights, on different datasets (that are subsets of the same dataset), and then compare their weights after training. Maybe a naive question, but is this something you can accomplish by simply training these models in two separate jobs? Is the weight comparison used to change how the models will be trained *during* training? If the answer to my naive question is no, then this type of use case could be supported by ParameterServerStrategy, with some customization of ClusterCoordinator. As a sketch: we would schedule closures for each worker using the tagged queues, so that each worker gets its own function to run as a training step, which modifies its own specific model. There are some implications of using the tagged queues, though; for instance for `join` to work as expected we'd have to modify it to wait on these tagged closures to finish (which might cause a small performance hit)","  You're right; I could just train the models on different jobs. It'd be nice to do so in a coordinated way, since I'm trying to run hundreds of models asynchronously, distributed over at least 10 gpu processes running on a single card. Autosharding the data would be nice too, so I thought `distribute` would be a good fit. I was able to get something working with Ray; there's some extra overhead but it'll do. Closing unless anyone wants to discuss further.","  I'm looking to do something similar to what you were trying. I want to train models asynchronously in parallel with some sparse communication between the models after some timesteps. A feature similar to the MirroredStrategy without the ""mirroring"" and aggregation of the gradients would probably be what I'm looking for. Did you ever find a solution?","  Yes, I wrote a script to automate launching training jobs with the ray framework. It launches several worker processes and handles resource allocation, callbacks, etc., works pretty well. Not sure about sparse communication between models; I only needed something that could run multiple models in parallel on different splits in the data. It's basically parallel kfolds CV; none of the models are ""interacting"" to update weights or anything. I'm not sure how to do that with ray."
661,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Saving and loading with an empty model layer)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.8.0rc132g3f878cff5b6 2.8.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,stridge-cruxml,Saving and loading with an empty model layer,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.8.0rc132g3f878cff5b6 2.8.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-19T02:21:32Z,stat:awaiting response type:bug stale comp:keras TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57222,"cruxml, Could you please try loading the model with **`tf.saved_model.load(""PATH"")`**. I tried the alternative approach and was able to load the model without any issue. Kindly find the gist of it here. Thank you!"," thanks for the quick response! Yes, I found that work around. However this is not ideal as it loads as tensorflow model not keras model. The other work around (if its just a shortcut layer) is to use `tf.keras.layer.Layer()` instead of Sequential(). But my purpose of reporting this, is this is a case that should work and should be supported but isn't.","cruxml, Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
728,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(openCL delegate issue with models that output results from their intermediate nodes)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1, nightly version  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865, 855  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bahar-BM,openCL delegate issue with models that output results from their intermediate nodes,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1, nightly version  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865, 855  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-08-18T18:01:22Z,type:bug comp:lite TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57211,"Hi, this is known limitation of GPU delegates. I don't think that you can expect that it will be fixed in the nearest time.  As a workaround you can add dummy operation like Add with zeroes or identity reshape to make intermediate tensor output not intermediate tensor. Sorry for this inconvenience.","Hi, thank you for your response. Yes, in the documentation of the tool in the attached link, we have pointed out the same workaround for this issue. We wanted to know if there is more systematic way to address this issue from your side but your explanation cleared everything up. Thank you",Are you satisfied with the resolution of your issue? Yes No
655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hexagon delegate for Qualcomm snapdragon chip APQ8098 (835))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf2.7  Custom Code No  OS Platform and Distribution _No response_  Mobile device Android 10  Python version 3.6  Bazel version 3.1.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,davwang2019,Hexagon delegate for Qualcomm snapdragon chip APQ8098 (835),Click to expand!    Issue Type Support  Source source  Tensorflow Version tf2.7  Custom Code No  OS Platform and Distribution _No response_  Mobile device Android 10  Python version 3.6  Bazel version 3.1.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-18T16:08:33Z,stat:awaiting response type:support stale TFLiteHexagonDelegate TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57210,Hi  ! Just quick observations based on template details before going into details . Could you switch to Python 3.7/3.8 (Python 3.7 minimum version ) and Bazel 3.7.2 for TF 2.7 and let us know the results . Please post the error stack trace for further assistance.  Reference.  Could you also close CC(Hexagon delegate for Qualcomm snapdragon chip APQ8098 (835)) as a duplicate to this issue. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
922,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hexagon delegate for Qualcomm snapdragon chip APQ8098 (835))ï¼Œ å†…å®¹æ˜¯ (Our product uses Qualcomm snapdragon chip 835 (APQ8098) with android 8.1. We ran our machine learning model on hexagon DSP using hexagon delegate library from Google: v1.20.0.1 Recently, we updated our android from 8.1 to 10 and hexagon delegation stopped working (our model could only run on CPU now). I have some question: libhexagon_nn_skel_v66.so ==> for 'v66' based DSP libhexagon_nn_skel_v65.so ==> for 'v65' based DSP libhexagon_nn_skel.so ==> ?? Could you please confirm the right library to use for ADSP in APQ8098? Our DSP version is v62 and the hexagon tools version are '8.0.08'. Please advise how to fix this hexagon delegation issue. Thanks a lot. Regards, David)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,davwang2019,Hexagon delegate for Qualcomm snapdragon chip APQ8098 (835),"Our product uses Qualcomm snapdragon chip 835 (APQ8098) with android 8.1. We ran our machine learning model on hexagon DSP using hexagon delegate library from Google: v1.20.0.1 Recently, we updated our android from 8.1 to 10 and hexagon delegation stopped working (our model could only run on CPU now). I have some question: libhexagon_nn_skel_v66.so ==> for 'v66' based DSP libhexagon_nn_skel_v65.so ==> for 'v65' based DSP libhexagon_nn_skel.so ==> ?? Could you please confirm the right library to use for ADSP in APQ8098? Our DSP version is v62 and the hexagon tools version are '8.0.08'. Please advise how to fix this hexagon delegation issue. Thanks a lot. Regards, David",2022-08-18T16:02:03Z,stat:awaiting response stale,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57209,"  In order to expedite the troubleshooting process here, could you please fill the issue template. Please refer this link for more details on Hexagon delegate and let us know if it helps? Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1857,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(It seems that `recompute_grad` is not saving GPU memory.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version conda `tensorflowgpu` 2.10.0rc1 pypi_0 pypi (I also tried: conda `tensorflow` 2.9.1   pypi_0    pypi)  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit  11.2.2   hbe64b41_10    condaforge cudnn          8.1.0.77   h90431f1_0    condaforge  GPU model and memory NVIDIA GeForce RTX 3090 24GiB  Current Behaviour? (originally from this thread: https://github.com/davisyoshida/tf2gradientcheckpointing/issues/1) I cannot tell whether the APIs (either the official one `tf.recompute_grad`, or those from GitHub by [](https://github.com/pidajay/tf2_gradient_checkpointing), someone from google  , someone that crop tfslim  , etc) are really saving GPU memory as intended. This is because I don't know what's the right tool to tell the difference in the results. These are my questions: (all under the assumption that tensorflow 2.x is used, not 1.x versions) 1. How to properly profile the GPU memory usage of tensorflow? I tried pidajay's method, where the unmaintained/buggy(negative memory in the report, e.g. issue) pythonprofilers/memory_profiler is used. From a reply by the author, it seems that the Repo. is for profiling CPU memory only (and pidajay also said he/she hadn't done the experiment for GPU/TPU in some PR for tensorflow  see the 2. of the link). 2. So instead, I decided to use tensorboard to profile my GPU memory usage. I followed the idea of the ipynbtutorial by)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,nyngwang,It seems that `recompute_grad` is not saving GPU memory.,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version conda `tensorflowgpu` 2.10.0rc1 pypi_0 pypi (I also tried: conda `tensorflow` 2.9.1   pypi_0    pypi)  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.10.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit  11.2.2   hbe64b41_10    condaforge cudnn          8.1.0.77   h90431f1_0    condaforge  GPU model and memory NVIDIA GeForce RTX 3090 24GiB  Current Behaviour? (originally from this thread: https://github.com/davisyoshida/tf2gradientcheckpointing/issues/1) I cannot tell whether the APIs (either the official one `tf.recompute_grad`, or those from GitHub by [](https://github.com/pidajay/tf2_gradient_checkpointing), someone from google  , someone that crop tfslim  , etc) are really saving GPU memory as intended. This is because I don't know what's the right tool to tell the difference in the results. These are my questions: (all under the assumption that tensorflow 2.x is used, not 1.x versions) 1. How to properly profile the GPU memory usage of tensorflow? I tried pidajay's method, where the unmaintained/buggy(negative memory in the report, e.g. issue) pythonprofilers/memory_profiler is used. From a reply by the author, it seems that the Repo. is for profiling CPU memory only (and pidajay also said he/she hadn't done the experiment for GPU/TPU in some PR for tensorflow  see the 2. of the link). 2. So instead, I decided to use tensorboard to profile my GPU memory usage. I followed the idea of the ipynbtutorial by",2022-08-18T10:23:08Z,stat:awaiting response type:bug stale comp:ops comp:gpu TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57205,"I would also suggest reopening issue CC(Memory Saving Gradients for TF2), because if you follow the thread you would know that the problem isn't resolved: 1. The thread got closed after  had shared the code from his research and the author thumbed it. But from the comment it seems that it doesn't work on tensorflow 2.9.1:       >  Hello! I've been trying to use your gradient checkpointing solution in tf 2.9.1 with jit compilation enabled, but keep running into an OOM that I do not face if I don't enable gradient checkpointing. I'm using a double stacked model: as in, I have a VAE with a separate encoder and decoder model. Do you have any easiertoread docs for your gradient checkpointing py, and have you verified that it works on 2.9.1? 2. I have tried all the Repos in the thread, none of them work. Even if some of it might work, since I might monitor the GPU memory usage incorrectly, the documentation of the current `recompute_grad` API can be improved so people know how to correctly apply it so that the current `recompute_grad` can achieve  the same thing as what Pytorch `torch.utils.checkpoint.checkpoint` does.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1822,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tutorial throws error)ï¼Œ å†…å®¹æ˜¯ (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb at   throws the following error.   UnimplementedError                        Traceback (most recent call last) [](https://localhost:8080/) in  > 1 result = classifier.predict(grace_hopper[np.newaxis, ...])       2 result.shape 1 frames /usr/local/lib/python3.7/distpackages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      53     ctx.ensure_initialized()      54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, > 55                                         inputs, attrs, num_outputs)      56   except core._NotOkStatusException as e:      57     if name is not None: UnimplementedError: Graph execution error: Detected at node 'predict/MobilenetV2/Conv/Conv2D' defined at (most recent call last):     File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main       ""__main__"", mod_spec)     File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code       exec(code, run_globals)     File ""/usr/local/lib/python3.7/distpackages/ipykernel_launcher.py"", line 16, in        app.launch_new_instance()     File ""/usr/local/lib/python3.7/distpackages/traitlets/config/application.py"", line 846, in launch_instance       app.start()     File ""/usr/local/lib/python3.7/distpackages/ipykernel/kernelapp.py"", line 612, in start       self.io_loop.start()     File ""/usr/local/lib/python3.7/distpackages/tornado/platform/asyncio.py"", line 132, in start       self.asyncio_loop.run_forever()     File ""/usr/lib/python3.7/asyncio)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ejboettcher,Tutorial throws error,"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb at   throws the following error.   UnimplementedError                        Traceback (most recent call last) [](https://localhost:8080/) in  > 1 result = classifier.predict(grace_hopper[np.newaxis, ...])       2 result.shape 1 frames /usr/local/lib/python3.7/distpackages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      53     ctx.ensure_initialized()      54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, > 55                                         inputs, attrs, num_outputs)      56   except core._NotOkStatusException as e:      57     if name is not None: UnimplementedError: Graph execution error: Detected at node 'predict/MobilenetV2/Conv/Conv2D' defined at (most recent call last):     File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main       ""__main__"", mod_spec)     File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code       exec(code, run_globals)     File ""/usr/local/lib/python3.7/distpackages/ipykernel_launcher.py"", line 16, in        app.launch_new_instance()     File ""/usr/local/lib/python3.7/distpackages/traitlets/config/application.py"", line 846, in launch_instance       app.start()     File ""/usr/local/lib/python3.7/distpackages/ipykernel/kernelapp.py"", line 612, in start       self.io_loop.start()     File ""/usr/local/lib/python3.7/distpackages/tornado/platform/asyncio.py"", line 132, in start       self.asyncio_loop.run_forever()     File ""/usr/lib/python3.7/asyncio",2022-08-18T00:01:35Z,stat:awaiting response type:support comp:apis TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/57201,I was able to get the tutorial to run if I forced the version to be 2.7.  I would like to know how to run this tutorial using tensorflow 2.9.  Thank you.,", I was able to execute the code on tensorflow v2.9 wiht out any errors/issues. Kindly find the gist of it here and please create a virtual environment and test your code again. Thank you!","That is weird.  I ran your code fine but from the Google Tutorials in Colab I can not run the ""same"" notebook. Google Tutorial https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb My Gist https://colab.research.google.com/gist/ejboettcher/941018debe5072ab638087ea2cc6d30a/transfer_learning_with_hub.ipynb I checked and it is the same TF version.  ",I found the difference!!!  I have GPU enabled and you do not.  If you switch your run time to use GPU you will run into the same issue. This is a bug in TF.,", I was able to execute the code without any issues on **tensorflowgpu v2.9**. Kindly find the gist of it here. As a workaround for the time being, you may try running **!apt install allowchangeheldpackages libcudnn8=8.1.0.771+cuda11.2** and let us know if that fixes your issues. Please take a look at this comment from the issue with a similar error and this bug is not related TF. Thank you!",  Please run your lab in google Colab with GPU enabled.   It does not work in Google's colab. That is the issue.  It will only run in Google's colab if you switch to version 2.7 (with GPU)  or run it without GPU in version 2.9.   The only control I have in Colab is to run with and without GPU.  I really do not have control of the environment :(.  I wish there was a requirement.txt file so that I can run it. ,", I tried to execute the code on GPU enabled only and was able to execute without any issues. Please try to execute  **!apt install allowchangeheldpackages libcudnn8=8.1.0.771+cuda11.2** before code. Thank you! !image","Thank you   That did fix the issue of running this in CoLab with TF 2.9 + GPU.   **BUT** The tutorial does not run by itself without you injecting code if you are using the GPU as default.   You either have to switch to Non GPU, switch to TF 2.7 or change  libcudnn8dev to libcudnn8.  The purpose of the tutorials is that it should just run in colabs so that you can focus of the material and not trouble shooting the environment.  It is really hard to trouble shoot the environment when you are not given a build eg requirements.txt file.   ",", Ideally it should run but it was an issue from google colab side and was failing with the error which is not related to tensorflow. Please take a look at this comment from the **GoogleColab/ColabTools** team.  As this is not from the TF related issue, I request please feel free to move this issue to closed status and raise the issue in **GoogleColab/ColabTools** repo for the assistance. Thank you!","  I thought that you and colab were the same group.  My bad. **BUT** could you add a colab work around in your tutorial until colabs get their act together. Take care, Issue open for me but closed for you.",Are you satisfied with the resolution of your issue? Yes No
1455,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Is the estimate of how big TFLite is on ARM incorrect for iOS?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device iOS  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When trying to link against TFLite for iOS based on the CocoaPods. We noticed a nearly 34MB increase in our binary size as opposed to the documentation which states that   Admittedly this was for 64bit ARM, but I don't believe that should make a difference? Per Bloaty, the top symbols came from  Looking into it further, it looks like the flex delegate is allowlisted though I wouldn't normally expect it to be. I think this leads to an incorrect estimate of the binary size on the TFLite docs linked above. Is there any reason why the FlexDelegate needs to be linked for iOS (similar question for XNNPack)? Can't they be extras to install like the Metal or CoreML Delegate? Making those two delegates optional would reduce the binary size by nearly 2MB, a near 50% decrease.  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,drubinstein,Is the estimate of how big TFLite is on ARM incorrect for iOS?,"Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device iOS  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When trying to link against TFLite for iOS based on the CocoaPods. We noticed a nearly 34MB increase in our binary size as opposed to the documentation which states that   Admittedly this was for 64bit ARM, but I don't believe that should make a difference? Per Bloaty, the top symbols came from  Looking into it further, it looks like the flex delegate is allowlisted though I wouldn't normally expect it to be. I think this leads to an incorrect estimate of the binary size on the TFLite docs linked above. Is there any reason why the FlexDelegate needs to be linked for iOS (similar question for XNNPack)? Can't they be extras to install like the Metal or CoreML Delegate? Making those two delegates optional would reduce the binary size by nearly 2MB, a near 50% decrease.  Standalone code to reproduce the issue   Relevant log output _No response_",2022-08-17T18:16:30Z,type:docs-bug stat:awaiting response stale comp:lite TF 2.9,closed,0,13,https://github.com/tensorflow/tensorflow/issues/57196,Hi  ! Sorry for the late response. Could you share any minimal reproducible code to replicate this issue. Thank you!,"Hi  . I attached a zipped up xcode project using the tflite 2.9.1 Pod and a tflite file that can be used to build a project that integrates tflite. You'll need to run `pod install` to download the tflite pods as I removed them due to upload blah.zip  size constraints from github. After building it and running  You'll see the output:  Mirroring my earlier concerns. In this case, the sum of the top 6 items which are tflite related sum to ~3MB. 3x more than what the documentation on the website estimates", !Thanks for the update. Hi  ! Could you look at this issue. Thank you!,"Since ARM64 architecture is different than ARM32 architecture the difference in size could be possible since under the hood there will be changes in the trace requests etc. Have you checked with the ARM32, if yes, does it come under the size mentioned in the document.","I've also encountered much bigger size than documented. It's around 2.2MB  without any delegates, ~4MB with metal delegate, ~5.2MB with metal + coreml delegates. The numbers are also for arm64. For sure arm32 would be smaller but common why do you measure it? All devices with iOS 11+ are arm64 and it's more than 99% of iOS users.", I haven't checked it and didn't think it was worth the effort since all iPhones since 2013 have been 64bit/arm64. I tried to get an armv7 build running on my M1 laptop but couldn't figure out a way to do it nor do I have a iPhone5  available to me unfortunately.,"Thanks for clarifying, since the documentation does not have a mention about `ARM64`, it would not be fair to take `ARM32` as a benchmark comparison, we would have to come up with the benchmark results for the `ARM64`.","Yes that would make sense, but then it'd also help in the documentation to specify what architecture _and_ operating system the estimate was made for (android vs. iOS). Speaking of iOS vs. Android, a little out of scope for the issue, but I tried to create a demo app in android (sorry no code), but I did see the following results when using Android Studio's APK Analyzer:  TFLite 2.9 with Arm64  1.2MB Compressed, 3.2MB Uncompressed  TFLite 2.9 with Armv7 (32bit)  1.1MB compressed, 2.2MB Uncompressed The uncompressed value comes from checking the file size of the JNI .so file inside the aar file distributed by Tensorflow. Unlike in Swift, in Android we dont have the ability to perform any link time optimization since the binary is distributed as a shared object. In the documentation it may be important to distinguish between the compressed and uncompressed file sizes. But also this matches up with what we see in Swift where for arm64, the binary is >=3MB.","Hi, sorry for the late reply. > Yes that would make sense, but then it'd also help in the documentation to specify what architecture and operating system the estimate was made for (android vs. iOS). Sorry the documentation is outdated (the benchmark was done a couple years ago), we are working on updating the documentations. Will update it with new benchmark results. > Is there any reason why the FlexDelegate needs to be linked for iOS (similar question for XNNPack)? Can't they be extras to install like the Metal or CoreML Delegate? Making those two delegates optional would reduce the binary size by nearly 2MB, a near 50% decrease. The TensorFlowLiteC framework doesn't depend on the flex delegate, so the size of tflite::AcquireFlexDelegate() looks suspicious. I'll investigate. The TFLite interpreter uses XNNPack by default if CPU kernels are used as it runs faster. I'll working on the size measurements and see if we can add continuous size watch for TFLite iOS library.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1868,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.9  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.7  Bazel version v2.9.0rc242g8a20d54a3c1 2.9.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory 2080Ti 11Gb  Current Behaviour?   Standalone code to reproduce the issue  shell WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting ImageProj)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mazatov,WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.9  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.7  Bazel version v2.9.0rc242g8a20d54a3c1 2.9.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory 2080Ti 11Gb  Current Behaviour?   Standalone code to reproduce the issue  shell WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting Bitcast WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 WARNING:tensorflow:Using a while_loop for converting ImageProj,2022-08-17T08:50:06Z,stat:awaiting response stale comp:keras type:performance TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57191,Hi  ! Could you post this issue in kerasteam/kerascv. It seems to be duplicate of 56242. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Why was this closed?
647,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault (core dumped))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version tf 2.6  Custom Code Yes  OS Platform and Distribution Linux 5.4.01089, 18.04.1Ubuntu SMP  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory v100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,sidml,Segmentation fault (core dumped),"Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version tf 2.6  Custom Code Yes  OS Platform and Distribution Linux 5.4.01089, 18.04.1Ubuntu SMP  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory v100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-08-16T14:10:05Z,stat:awaiting response type:support stale comp:keras TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57181,"Hi , Use below code snippet at the beginning of the your code to Get the detailed error log.  Complete error log of given code snippet.  Error message says shape mismatch. Check your input data shape and expected input shape of Model. Thank you!","  Thanks. Yes, the input shape should have been (1, 28, 28,1). I missed it while writing this toy example. I added the logger code snippet but I still get  only `Segmentation fault (core dumped)`.  Is there anything else that I can do to get a more detailed error log ? Thanks.","Hi , Could you try the workaround mentioned here.  To get the full logging information. Use below code snippet,  Thank you!",  I tried the above code and i still get `get only Segmentation fault (core dumped)`. Thanks.,"Hi , Could you try with Tf v2.9. I tried with `Tf v2.9.2`. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
890,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bazel - Download from ... returned 404 Not Found)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Docker  condaforge/mambaforgepypy3:4.13.01  Mobile device _No response_  Python version _No response_  Bazel version 5.1.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell PIP_ROOT_USER_ACTION=ignore CONDAENV='tensorflow' PYTHONENV='3.7' CONDARC='/opt/conda/.condarc' TENSORFLOWDIR='tensorflow_src/tensorflow/lite/tools/pip_package' BAZEL_VERSION='5.1.1' DISABLE_BAZEL_WRAPPER='1' cat > $CONDARC )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,modem7,Bazel - Download from ... returned 404 Not Found,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Docker  condaforge/mambaforgepypy3:4.13.01  Mobile device _No response_  Python version _No response_  Bazel version 5.1.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell PIP_ROOT_USER_ACTION=ignore CONDAENV='tensorflow' PYTHONENV='3.7' CONDARC='/opt/conda/.condarc' TENSORFLOWDIR='tensorflow_src/tensorflow/lite/tools/pip_package' BAZEL_VERSION='5.1.1' DISABLE_BAZEL_WRAPPER='1' cat > $CONDARC ,2022-08-16T13:48:32Z,type:build/install subtype:bazel TF 2.9,closed,1,6,https://github.com/tensorflow/tensorflow/issues/57179,"Hi  ! Could you look at this issue.  It was passing in Colab and throwing package conflicts (Screenshot 1, 2, 3) in Docker.","These are just warnings, not impacting the build artifact. Most TF dependencies use a mirror to not hit upstream (e.g., github) on every build and download the artifact from there. When the dependency gets updated, someone also needs to update the mirror. If something is not mirrored, Bazel then goes to upstream and fetches the dependency from there.","Hi , As  mentioned in the above comments, these are the warnings, you can ignore them. These wonâ€™t impact on build. Thank you!",Great news! Cheers chaps! Thanks for looking into it! ,Are you satisfied with the resolution of your issue? Yes No,"Hi, @gadagashwini I also encountered these problems above, also the same warning, and this warning caused the build to fail. Could you help look?  **error message** WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/13c6828bedeb815ee7748f82ca36073dbd55a9db.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: 0 packages loaded Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 0 targets configured) WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/9ca259d5092e9cf2c1fa0788a470df6a4fc95f0a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found Analyzing: target //tensorflow/tools/pip_package:build_pip_package (272 packages loaded, 4050 targets configured) Analyzing: target //tensorflow/tools/pip_package:build_pip_package (528 packages loaded, 28314 targets configured) ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/python/BUILD:3654:8: in cmd attribute of genrule rule //tensorflow/python:pywrap_tensorflow_import_lib_file: variable '$<' : no input file ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/python/BUILD:3654:8: Analysis of target '//tensorflow/python:pywrap_tensorflow_import_lib_file' failed ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:  INFO: Elapsed time: 533.961s INFO: 0 processes. FAILED: Build did NOT complete successfully (529 packages loaded, 29595 targets configured) FAILED: Build did NOT complete successfully (529 packages loaded, 29595 targets configured)"
693,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(how to use tfrecord data(dict format) (tf.data.dataset) in distribute training?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf2.2.0  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,wannianlou,how to use tfrecord data(dict format) (tf.data.dataset) in distribute training?,Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf2.2.0  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-16T10:05:50Z,stat:awaiting response stale comp:data type:performance TF 2.2,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57175,"def make_dataset_for_distribute_train(self, all_paths):    dataset = tf.data.TFRecordDataset(all_paths)    dataset = dataset.map(self.parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)    dataset = dataset.map(self.make_feed_dict,num_parallel_calls=tf.data.experimental.AUTOTUNE)    padding_dict = {""a"": [20], ""b"": [None], ""c"": [30],""d"": [13]}    dataset_for_train = dataset.padded_batch(self.batch_size, padded_shapes=padding_dict,                                                  drop_remainder=True).prefetch(             buffer_size=tf.data.experimental.AUTOTUNE)    return dataset_for_train def parse_dict2_turple(self,data_tensor):         d0 = data_tensor['a']         d1 = data_tensor['b']         d2 = data_tensor['c']         d3 = data_tensor['d']         return tf.data.Dataset.from_tensor_slices((d0, d1, d2, d3)) def distributed_train_step(inputs):     .function     def distributed_train_step_wrapped(ds):         loss=0.0         for k in ds:              tf.print(k)             per_replica_average_loss = strategy.run(train_step, args=(k,))             loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_average_loss, axis=None)         return loss     return distributed_train_step_wrapped(inputs)     for i in date:         all_paths = self.parse_input_path(i, epoch)         logger.info(all_paths)         dataset = self.make_dataset_for_distribute_train(all_paths)         total_loss = 0.0         num_train_batches = 0         for j in dataset:             dataset_parsed = self.parse_dict2_turple(j)             dataset_parsed = dataset_parsed.with_options(options)             train_dataset_distribute = strategy.experimental_distribute_dataset(dataset_parsed)             batch_loss = distributed_train_step(train_dataset_distribute)             num_train_batches += 1             total_loss += batch_loss Aligned code is like this ",", To reproduce the issue reported here, could you please provide the complete code and the dataset you are using.  Also TensorFlow **v2.2** is not actively supported. Could you please update TensorFlow to the latest stable version **_v2.9_** and check if you are facing the same issue. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
672,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.nn.embedding_lookup result on cpu inconsistent with gpu)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF2.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cheyennee,tf.nn.embedding_lookup result on cpu inconsistent with gpu,Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF2.4  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-16T09:23:36Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57174,"Same problem is found in tf.scatter_nd.  ''' The result is {'err_cpu': 'Error:indices[0] = [107, 249] does not index into shape [3,1,12,64] [Op:ScatterNd]', 'res_gpu': } '''","Hi   ! Could you look at this issue. Attached gist in 2.8, 2.9 and nightly for reference. Thank you!","I could able to replicate the issue on Ubuntu with Tf v2.9, CUDA 11.4. ","Since too many validations will impact the performance of operations, usually you don't notice the same behavior in CPU vs GPU."," But I think it's important for gpu to check the validity of input. If the input is invalid, then the calculation has no meaning.","We intentionally don't check on GPU, and this is usually documented.  On GPU, the check is not worth the extra kernel launch required.  Garbage in, garbage out.  The calculation has no meaning regardless. Specifically, quoting from the docs for `scatter_nd`: > Note that on CPU, if an out of bound index is found, an error is returned. On GPU, if an out of bound index is found, the index is ignored.",Are you satisfied with the resolution of your issue? Yes No
1695,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(LeakyRelu in Tensorflow lite with the Hexagon Delegate not supported)ï¼Œ å†…å®¹æ˜¯ (When using a tflite model (8bits quantized via TensorFlow lite conversion framework) that includes the activation function ""LeakyRelu"", the Hexagon delegate from tensorflow framework cannot perform the DNN inference on the whole graph, but rather it falls back to the CPU/XNNPack delegate. This is due to the fact that 'LeakyRelu' operation is not supported by the Hexagon Delegate (confirmed in TensorFlow doc: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md). When using Relu activation function (and Relu6 as well), we can see below that the TF Hexagon Delegate can process the whole DNN graph, unfortunately, the qualitative results I get are much worse, hence the need of having 'Leaky Relu' implemented in the Hexagon Delegate. **System information**  OS Platform and Distribution): Android 10, NDK R21e  TensorFlow installed from (source or binary): from source using the Release tag '2.9.1'  TensorFlow version (or github SHA if from source):  2.9.1 **Output of Tensorflow library when running an inference with a model that includes 'LeakyRelu' activation function**  As we can see below when replacing 'LeakyRelu' operation by 'Relu', then the TF Hexagon delegate can process the whole DNN graph. **Output of Tensorflow library when running an inference with a model that includes 'Relu' activation function**  Would it be possible to implement 'LeakyRelu' in TF Hexagon Delegate ?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,eelcoder,LeakyRelu in Tensorflow lite with the Hexagon Delegate not supported,"When using a tflite model (8bits quantized via TensorFlow lite conversion framework) that includes the activation function ""LeakyRelu"", the Hexagon delegate from tensorflow framework cannot perform the DNN inference on the whole graph, but rather it falls back to the CPU/XNNPack delegate. This is due to the fact that 'LeakyRelu' operation is not supported by the Hexagon Delegate (confirmed in TensorFlow doc: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md). When using Relu activation function (and Relu6 as well), we can see below that the TF Hexagon Delegate can process the whole DNN graph, unfortunately, the qualitative results I get are much worse, hence the need of having 'Leaky Relu' implemented in the Hexagon Delegate. **System information**  OS Platform and Distribution): Android 10, NDK R21e  TensorFlow installed from (source or binary): from source using the Release tag '2.9.1'  TensorFlow version (or github SHA if from source):  2.9.1 **Output of Tensorflow library when running an inference with a model that includes 'LeakyRelu' activation function**  As we can see below when replacing 'LeakyRelu' operation by 'Relu', then the TF Hexagon delegate can process the whole DNN graph. **Output of Tensorflow library when running an inference with a model that includes 'Relu' activation function**  Would it be possible to implement 'LeakyRelu' in TF Hexagon Delegate ?",2022-08-16T07:45:48Z,stat:awaiting response type:feature stale comp:lite TFLiteHexagonDelegate TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/57171,Hi  ! Could you also provide a minimal reproducible code . Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
897,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AutoGraph cannot handle python 3.10's structural pattern matching)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution macOS 12.5  Mobile device none  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version none  GPU model and memory Apple's METAL (though unlikely to be relevant here)  Current Behaviour? Trying to AutoGraph a function containing a PEP 634/635/636 structural pattern matching statement from python 3.10 (i.e. `match/case`) results in the `WARNING:tensorflow:AutoGraph could not transform  and will run it asis.`  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,burnpanck,AutoGraph cannot handle python 3.10's structural pattern matching,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution macOS 12.5  Mobile device none  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version none  GPU model and memory Apple's METAL (though unlikely to be relevant here)  Current Behaviour? Trying to AutoGraph a function containing a PEP 634/635/636 structural pattern matching statement from python 3.10 (i.e. `match/case`) results in the `WARNING:tensorflow:AutoGraph could not transform  and will run it asis.`  Standalone code to reproduce the issue   Relevant log output  ,2022-08-16T06:28:12Z,stat:awaiting tensorflower type:bug comp:autograph TF 2.9,open,2,6,https://github.com/tensorflow/tensorflow/issues/57166,"Hi , When I executed given code, it worked as expected. Tested with `Python3.10` and` Tensorflow 2.9.1` ","Hey , thanks for looking into this. Please note that in my case, the code does execute as well. The traceback I posted under ""relevant log output"" is really a logged message (right after the warning that I mention), not from an exception that reaches my code. Your code logs the same warning  although the reason it gives is different: `Original error: could not get source code`. It appears that you did run the example code from an interactive base interpreter, which apparently prevents AutoGraph from grabbing the source code. Please rerun the code from within a `.py` file or a jupyter notebook, as that seems to provide the source code via a tempfile (that is what I did). Either way, do not expect the code to throw an exception. The issue that I'm reporting here is the warning message both our runs are logging, and the implications: The example code is not being handled by AutoGraph, even though it still can execute.","Any updates here? Still present in TF 2.15.0. Structural pattern matching is part of standard Python and by now several years and python releases old. If AutoGraph cannot handle structural pattern matching, it basically means it is unable to read the python syntax. This is a pity, because pattern matching is very useful, even if it does not involve any tensors. That is, supporting the syntax is important even if the syntax does not unlock any tensor operations. As the feature becomes more wellknown, this will only grow in importance.", I think TensorFlow's AutoGraph may not yet have implemented support for converting this syntax into graph operations.Did u try another approach ?," You think right. AutoGraph does not support structural pattern matching, despite this being a standard python feature since almost three years now (python 3.10 was released in 2021)  that is what I am reporting here.     Each of you were assigned to this at some point, but it seems the issue has now fallen between the cracks. Implementing this shouldn't be too difficult. Judging from previous release schedules, the next TF release may well drop support for python 3.9, at which point basically doesn't support *any* version of python correctly at all anymore!","> Implementing this shouldn't be too difficult. I do see that AutoGraph in fact offers to convert some python control statements into TF graphs. To support that usecase, AutoGraph needs to ""read and understand"" the python AST at some point before running the TF graph (i.e. during the first phase that includes tracing). For example, the documentation states that `if` statements are transformed when the condition is a tensor. The same *could* be done for `match` statements, e.g. by transforming it into an equivalent set of `if` statements. Given the breath of possible matching patterns, this may still be a nonnegligible effort. Instead, I suggest not to support any transformations for `match` at all. IMHO, the majority of usecases for `match` are only relevant during the tracing phase. Therefore, AutoGraph can be fairly oblivious of the existence of `match` at all, just letting it execute normally during tracing. The only reason it causes issues in the first place is likely because AutoGraph needs to inspect the AST to support the transformation of `if`, `while` and `for`, causing it to choke on an unknown AST node. So I'm confident that 99 % of the usecases for `match` could be solved simply by teaching AutoGraph to recognise and (mostly) ignore `match` statements. As it stands today however, all code containing `match` is being rejected entirely by AutoGraph, even if the `match` statement has zero effect on the graph execution phase (apparently even when it appears in the surrounding scope of a definition  see CC(AutoGraph error when function is inside a match ))."
721,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: Data adapters should be mutually exclusive for handling inputs in Streamlit)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Windows 10 21H2  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Cuda 11.2.2/ cuDNN v8.1.0.77  GPU model and memory GeForce NVidia RTX 3060  16GB RAM  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bluetail14,RuntimeError: Data adapters should be mutually exclusive for handling inputs in Streamlit,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Windows 10 21H2  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Cuda 11.2.2/ cuDNN v8.1.0.77  GPU model and memory GeForce NVidia RTX 3060  16GB RAM  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-15T10:12:52Z,stat:awaiting response type:support stale comp:data TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57155,Hi  ! Could you try again after removing  the all zero columns from your train and test dataset and let us know. Attached relevant thread for reference. Thank you!,"I have images in my train and test datasets,x_train and x_test, and 1 and 0 as labels in y_train and y_test. so I cant really remove the 0 labels?  ",Hi  ! Did you try putting Pillow image instead of Stream lite image. Thank you!,"thank you, that might be the case. why is it not a PIL image? I'm using 'u_img = load_image(Image.open(uploaded_file))'."," ! You are using st.image widget to take the image to stream lite. `    img = st.image(u_img, 'Uploaded Image', use_column_width=True) ` Could you confirm whether it works or not after replacing below statement  ` pred_label = model_cnn_ltsm_.predict(img)[0]  ` with statement below `pred_label = model_cnn_ltsm_.predict(u_img)[0] ` Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1634,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow c++ api crashed during inferencing on arm64, but same codes is running well on x86_64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.6.5 or  tf2.9.1  Custom Code No  OS Platform and Distribution centos7  arm64  Mobile device no  Python version 3.8.10  Bazel version 3.7.2  GCC/Compiler version gcc (GCC) 7.3.1 20180303 (Red Hat 7.3.16)  CUDA/cuDNN version no  GPU model and memory no  Current Behaviour?   Standalone code to reproduce the issue ```shell include  include  include  include  include  include  include  include  include  include  include  include  include  include  std::string GetDimDescription(const std::vector &shape) {     std::ostringstream oss;     oss (std::chrono::system_clock::now())         .time_since_epoch()         .count(); } static const std::string input_mel_name = ""serving_default_mels:0""; static const std::string output_wave_name = ""StatefulPartitionedCall:0""; int main(int argc, char const *argv[]) {     if (argc != 3) {         std::cerr > inputs;     inputs.push_back(std::pair(input_mel_name, input_mel));     std::vector output_node_names = {output_wave_name};     std::vector outputs;     bundle.GetSession()>Run(inputs, output_node_names, {}, &outputs);     //æ¨ç†æ¨¡å‹     int64_t start_ms = GetNowMs();     for (int32_t i = 0; i Run(inputs, output_node_names, {}, &outputs);     }     double use_time = (GetNowMs()  start_ms) * 1.0 / run_time;     std::cout )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,TsinghuaTop,"tensorflow c++ api crashed during inferencing on arm64, but same codes is running well on x86_64","Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.6.5 or  tf2.9.1  Custom Code No  OS Platform and Distribution centos7  arm64  Mobile device no  Python version 3.8.10  Bazel version 3.7.2  GCC/Compiler version gcc (GCC) 7.3.1 20180303 (Red Hat 7.3.16)  CUDA/cuDNN version no  GPU model and memory no  Current Behaviour?   Standalone code to reproduce the issue ```shell include  include  include  include  include  include  include  include  include  include  include  include  include  include  std::string GetDimDescription(const std::vector &shape) {     std::ostringstream oss;     oss (std::chrono::system_clock::now())         .time_since_epoch()         .count(); } static const std::string input_mel_name = ""serving_default_mels:0""; static const std::string output_wave_name = ""StatefulPartitionedCall:0""; int main(int argc, char const *argv[]) {     if (argc != 3) {         std::cerr > inputs;     inputs.push_back(std::pair(input_mel_name, input_mel));     std::vector output_node_names = {output_wave_name};     std::vector outputs;     bundle.GetSession()>Run(inputs, output_node_names, {}, &outputs);     //æ¨ç†æ¨¡å‹     int64_t start_ms = GetNowMs();     for (int32_t i = 0; i Run(inputs, output_node_names, {}, &outputs);     }     double use_time = (GetNowMs()  start_ms) * 1.0 / run_time;     std::cout ",2022-08-15T09:48:01Z,stat:awaiting response type:build/install stale subtype:centos TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57154,when i edit source code to close MetaOptimizer::OptimizeGraph in  https://github.com/tensorflow/tensorflow/blob/6b54e9fa35d6261adae9565f18cde359003b551b/tensorflow/core/grappler/optimizers/meta_optimizer.ccL606 like this  it is running ok.,"Hi , Can we move this to closed status since you found workaround. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1032,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`Transformers` : Error while fitting TFBertForSequenceClassification model (via tensorflow-directml-plugin))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tensorflowcpu 2.9.1  Custom Code Yes  OS Platform and Distribution windows 11 PRO 64bit : 21H2  Mobile device _No response_  Python version 3.8.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version tensorflowdirectmlplugin 0.0.1.dev220621  GPU model and memory RX 6800 16Go  Current Behaviour?   Standalone code to reproduce the issue python history=bert_model.fit([X_train, Mask_Train],                         y_train,                        batch_size=32,                        epochs=EPOCHS,                        validation_data=([X_test, Mask_test], y_test),                        callbacks=callbacks)   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,ImRobot777,`Transformers` : Error while fitting TFBertForSequenceClassification model (via tensorflow-directml-plugin),"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tensorflowcpu 2.9.1  Custom Code Yes  OS Platform and Distribution windows 11 PRO 64bit : 21H2  Mobile device _No response_  Python version 3.8.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version tensorflowdirectmlplugin 0.0.1.dev220621  GPU model and memory RX 6800 16Go  Current Behaviour?   Standalone code to reproduce the issue python history=bert_model.fit([X_train, Mask_Train],                         y_train,                        batch_size=32,                        epochs=EPOCHS,                        validation_data=([X_test, Mask_test], y_test),                        callbacks=callbacks)   Relevant log output  ",2022-08-14T23:34:14Z,stat:awaiting response type:bug stale comp:apis TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57149," , I ran the code and faced a different error, please find the gist here and share all dependencies to replicate the issue or share a colab gist with the reported error.  Could you please try to run the below command before executing the code and let us know if the issue still persists. **!apt install allowchangeheldpackages libcudnn8=8.1.0.771+cuda11.2** Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
678,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build Failure re: Intel MKL (building with ""--config=mkl"" flag); Missing ""mkl_cblas.h""?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version Docker Image: tensorflow/tensorflow:latestdevelgpu TF 2.10  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version CUDA 11.2, cuDNN 8.1.0  GPU model and memory RTX 2070 Super 8GB  Current Behaviour?  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,NeilPandya,"Build Failure re: Intel MKL (building with ""--config=mkl"" flag); Missing ""mkl_cblas.h""?","Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version Docker Image: tensorflow/tensorflow:latestdevelgpu TF 2.10  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version CUDA 11.2, cuDNN 8.1.0  GPU model and memory RTX 2070 Super 8GB  Current Behaviour?  ",2022-08-14T15:52:11Z,type:build/install comp:mkl subtype: ubuntu/linux TF 2.9,closed,0,19,https://github.com/tensorflow/tensorflow/issues/57146,"I think you haven't installed intel_mkl_ml dependencies. Could you find ""mkl_cblas.h"" file in your system? ","> I think you haven't installed intel_mkl_ml dependencies. Could you find ""mkl_cblas.h"" file in your system? I searched for it, and it itsn't present. I was under the impression that I installed MKL libraries via Intel OneAPI, installed with the following command: `sudo apt install intelbasekit` as per these instructions.","Hi , From the error log, it seems issue with GCC  `ERROR: /tensorflow_src/tensorflow/compiler/xla/service/cpu/BUILD:345:11: Compiling tensorflow/compiler/xla/service/cpu/simple_orc_jit.` Can you try with `GCC 9.3.1`. Thank you","> Hi , From the error log, it seems issue with GCC >  > `ERROR: /tensorflow_src/tensorflow/compiler/xla/service/cpu/BUILD:345:11: Compiling tensorflow/compiler/xla/service/cpu/simple_orc_jit.` >  > Can you try with `GCC 9.3.1`. Thank you Hi , thank you for helping. I am building via tensorflow's docker container, labeled with tags as `tensorflow/tensorflow:latestdevelgpu`. Is there a specific container I can pull that has gcc9.3.1 or must I install manually? Also, forgive me in advance, as this may be a dumb question, but if the error is `No such file or directory` regarding `mkl_cblas.h`, `In file included from tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:48: ./tensorflow/compiler/xla/service/cpu/runtime_matmul_mkl.h:22:10: fatal error: third_party/intel_mkl_ml/include/mkl_cblas.h: No such file or directory    22 | include ""third_party/intel_mkl_ml/include/mkl_cblas.h""` will changing compiler versions really solve anything if the path `third_party/intel_mkl_ml/include/mkl_cblas.h` isn't in tensorflow's source code? As it stands, from when I pulled from the repo, the folder `intel_mkl_ml` doesn't exist in the parent folder `third_party`. There are folders, however, named `mkl` and `mkl_dnn`. Neither have the folder `include` or the header file `mkl_cblas.h`.",Would you have a look at this CC(How to enable INTEL_MKL_ML?) ?  ,"> Would you have a look at this CC(How to enable INTEL_MKL_ML?) ?  Thank you, , I'm now compiling from source again with the following bazel build command, as per the solution found in  CC(How to enable INTEL_MKL_ML?): `bazel build verbose_failures config=mkl config=opt copt=mavx copt=mavx2 copt=mfma copt=msse4.2 copt=DINTEL_MKL_ML_ONLY cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package` Will reply again with results once it's finished.","I have the same issue but for `tensorflow2.10.0rc1`. Versions `2.9.1` and `2.8.2` compile fine. My GCC version is 9.4.0 (from Ubuntu 20.04). The above hint with adding `copt=DINTEL_MKL_ML_ONLY` doesn't help, it can't find the file `""third_party/intel_mkl_ml/include/mkl_cblas.h""`","Hi , Can you try other Docker image such as `tensorflow/tensorflow:nightlygpu`. Thank you!","> Hi , Can you try other Docker image such as `tensorflow/tensorflow:nightlygpu`. Thank you!  I am pulling the nightlygpu image now and will post results using the following build command.  `bazel build verbose_failures config=mkl config=opt copt=""mfpmath=both"" copt=mavx copt=mavx2 copt=mfma  copt=msse4.2 cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package ` Also, , I think I made a mistake on the labeling of this issue. I think we need to change the tag of this post to TF 2.10, as the original docker image I was using was `tensorflow/tensorflow:latestdevelgpu`. Please let me know if that's the case, or I'm mistaken. I've updated my original bug report above to reflect which version of TF is being discussed.  Unsuccessful when using the `config=mkl` flag. The system was still searching for `""third_party/intel_mkl_ml/include/mkl_cblas.h""`. The following was the build command and error log for the unsuccessful attempt: `apt install intelbasekit` `bazel build verbose_failures config=mkl config=opt copt=mavx copt=mavx2 copt=mfma copt=DINTEL_MKL_ML_ONLY cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package `  However, the following build command **did** compile successfully after removing the `config=mkl` flag. I've also included the log. That being said, I will test whether I can make use of MKL with my model(s) to see if this is a proper workaround for this bug. I'm skeptical to that end. `bazel build verbose_failures config=opt copt=mavx copt=mavx2 copt=mfma copt=msse4.2 copt=DINTEL_MKL_ML_ONLY cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package `  My mind says a header file named `mkl_cblas.h` would be very important in utilizing Intel's MKL. I agree with b in that the core issue of this bug is a discrepancy in the folder structure within the source code and the lack of said file, or the restructuring of how MKL is called within tensorflow in this release. MKL, I assume, was successfully installed beforehand when I ran `apt install intelbasekit`, as per these instructions. To repeat myself a little, below is the current condition of the folder structure I pulled once I spun up a container from the official `latestdevelgpu` image: > As it stands, from when I pulled from the repo, the folder `intel_mkl_ml` doesn't exist in the parent folder `third_party`. There are folders, however, named `mkl` and `mkl_dnn`. Neither have the folder include or the header file `mkl_cblas.h`.","PR CC(Remove `tf_copts` that were added by XLA ACL changes) fixes the compilation issue for me. However, I don't know if this has any implication on the outcome. I don't know if it's related or another issue but the output at import time is:   (I have built it with cuda enabled as well.)",cc: MKL ,"b Thank you for bringing up the PR! We believe CC(Remove `tf_copts` that were added by XLA ACL changes) should fix the issue.  > E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered I'm positive this is unrelated because PR https://github.com/tensorflow/tensorflow/pull/57186 only reverted XLA target copts to what they used to be before CC(Update unit tests for xla cpu changes). Could you please help check if you see this error message at commit cd17810b2c21c843fe9a605ef314f01077cf87f7 (the commit before https://github.com/tensorflow/tensorflow/commit/30069385eaffe6de51be5737d9798aeb02345c8c, the merge commit for CC(Update unit tests for xla cpu changes) which caused this mkl_cblas.h issue.)?","> I'm positive this is unrelated because PR CC(Remove `tf_copts` that were added by XLA ACL changes) only reverted XLA target copts to what they used to be before CC(Update unit tests for xla cpu changes). Could you please help check if you see this error message at commit cd17810 (the commit before 3006938, the merge commit for CC(Update unit tests for xla cpu changes) which caused this mkl_cblas.h issue.)? Agreeing with , the cuBlas error is not related to the mkl error.  's PR https://github.com/tensorflow/tensorflow/pull/57213 should fix the mkl error in 2.10","b:  thinks the message belongs to an unrelated issue, which has recently been fixed in https://github.com/tensorflow/tensorflow/pull/56691.  Thank you for the quick reply!  Does https://github.com/tensorflow/tensorflow/pull/57186 fix the issue for you?",">  Does CC(Remove `tf_copts` that were added by XLA ACL changes) fix the issue for you? I will checkout CC(Remove `tf_copts` that were added by XLA ACL changes), compile, and report back accordingly, thank you.","> I will checkout https://github.com/tensorflow/tensorflow/pull/57186, compile, and report back accordingly, thank you.  Thank you! CC(Remove `tf_copts` that were added by XLA ACL changes) has been merged into master, so you can just pull the latest commit from master as well. (Or use the PR merge commit https://github.com/tensorflow/tensorflow/commit/6909580015657cf0e0ecc623bc1838cf7263fef1.)",">  Thank you! CC(Remove `tf_copts` that were added by XLA ACL changes) has been merged into master, so you can just pull the latest commit from master as well. (Or use the PR merge commit 6909580.)  Build was successful with the following command:  I will install and test against my smaller models I'm working on which utilize MKL to be sure, but I think your fix detailed in CC(Remove `tf_copts` that were added by XLA ACL changes) did the trick. To be clear, I used the `tensorflow/tensorflow:latestdevelgpu` image; the `nightlygpu` image  suggested was fraught with several other issues (e.g. bazel not installed, CUDA and cuDNN not located when running `./configure`, needing to install `git`, and others). I will experiment more with getting the nightly container to a state where tensorflow will compile without problems. There were some warnings thrown in the successful build, so I've included the log, in case that may be useful you or someone else.  Thanks for your assistance!", Thank you for the detailed feedback and for the output log!  These warning messages are safe to ignore. It's just saying it couldn't find the mirrored packages on storage.googleapis.com (and went and download those packages directly from github instead). I just fixed that and those mirror links (beginning with storage.googleapis.com) should work now. I'm closing this issue since it's resolved. :),Are you satisfied with the resolution of your issue? Yes No
737,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TF2] Trying to fit an array of dictionaries into my model as prediction or training data to a DenseFeatures layer)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version v2.9.0rc242g8a20d54a3c1 2.9.0  Custom Code No  OS Platform and Distribution Windows 11 Pro  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,boroicamarius,[TF2] Trying to fit an array of dictionaries into my model as prediction or training data to a DenseFeatures layer,Click to expand!    Issue Type Support  Source source  Tensorflow Version v2.9.0rc242g8a20d54a3c1 2.9.0  Custom Code No  OS Platform and Distribution Windows 11 Pro  Mobile device _No response_  Python version 3.10.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-14T12:41:47Z,stat:awaiting response type:support stale comp:keras TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57145,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here.  I am getting different error while reproducing  this issue, as given below;   Could you refer this issue and let us know if it helps? Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
668,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF2 run out of GPU memory when doing GradientTape.jacobian)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.5  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dibbla,TF2 run out of GPU memory when doing GradientTape.jacobian,Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.5  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` ,2022-08-14T06:40:40Z,stat:awaiting response stale comp:gpu type:performance TF 2.5,closed,0,4,https://github.com/tensorflow/tensorflow/issues/57144,"One more thing, even after I comment all Conv layers, the jacobian still exceeds the GPU memory. "," I tried to replicate the issue on colab, please find the gist here and confirm the same. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
641,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Win] fft2d tries to link m.lib)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kcoul,[Win] fft2d tries to link m.lib,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-12T20:00:38Z,type:bug subtype:windows awaiting PR merge TF 2.9,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57133,", This issue will move to closed status once the respected PR is merged. Thank you!",Are you satisfied with the resolution of your issue? Yes No
1844,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Android]: Internal error: Cannot create interpreter: tensorflow/lite/core/subgraph.cc BytesRequired number of elements overflowed.)ï¼Œ å†…å®¹æ˜¯ (**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible):     (1) google/sdk_gphone64_arm64/emu64a:13/TPB4.220624.004/8808248:userdebug/devkeys   (2) Nokia/TA1012_00WW/NB1:9/PPR1.180610.011/00WW_5_15K:user/releasekeys  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): 2.9.0  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`):     (1) 22.18.21 (190400453244992)   (2) 22.26.15 (100400461192076) **Standalone code to reproduce the issue** I'm trying to load a tflite model with dynamic input size of (1, None, None, 1) on _Android_ as follows:  When instantiating the `Interpreter` I get the following error:  When I run the exact same model using _Python_ it works as intended:  When I skip the `resize_tensor_input()` operation in _Python_ (2. line) I get the **exact same error message** as in _Android_ which leads me to believe that the problem might be linked to the _Android_ interpreter's allocate tensors step within its constructor before resizing the input tensor. **Any other info / logs** Using _Android's_ ML Binding leads to the exact same error message.  TFLite model input details:  Extracted from _Python's_ interpreter:  I do however succeed to instantiate the _Android_ interpreter when I use a static input tensor size, e.g. `(1, 50, 150, 1)`. Resizing the input tensor then to the desired size works as well:  Any idea how to solve this (maybe except using static input tensor shape)?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jhabr,[Android]: Internal error: Cannot create interpreter: tensorflow/lite/core/subgraph.cc BytesRequired number of elements overflowed.,"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible):     (1) google/sdk_gphone64_arm64/emu64a:13/TPB4.220624.004/8808248:userdebug/devkeys   (2) Nokia/TA1012_00WW/NB1:9/PPR1.180610.011/00WW_5_15K:user/releasekeys  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): 2.9.0  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`):     (1) 22.18.21 (190400453244992)   (2) 22.26.15 (100400461192076) **Standalone code to reproduce the issue** I'm trying to load a tflite model with dynamic input size of (1, None, None, 1) on _Android_ as follows:  When instantiating the `Interpreter` I get the following error:  When I run the exact same model using _Python_ it works as intended:  When I skip the `resize_tensor_input()` operation in _Python_ (2. line) I get the **exact same error message** as in _Android_ which leads me to believe that the problem might be linked to the _Android_ interpreter's allocate tensors step within its constructor before resizing the input tensor. **Any other info / logs** Using _Android's_ ML Binding leads to the exact same error message.  TFLite model input details:  Extracted from _Python's_ interpreter:  I do however succeed to instantiate the _Android_ interpreter when I use a static input tensor size, e.g. `(1, 50, 150, 1)`. Resizing the input tensor then to the desired size works as well:  Any idea how to solve this (maybe except using static input tensor shape)?",2022-08-12T13:35:46Z,stat:awaiting response type:bug comp:lite TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57131,"Hi  ! Sorry for the inconvenience. Dynamic shape, Tensor and Batch size is not supported yet and work in progress feature . Present work around is go with resize the input according the input data as you mentioned . Attached relevant thread for reference. Thank you!","Hi   Thanks for your answer. I was able to make it run. If someone else encounters this problem, here is one way how to solve this: 1. Train the original model with dynamic input size, e.g. `(1, None, None, 1)` 2. During conversion to TFLite model fix the size of the input tensor:  This will create a TFLite model with the following input details:  3. On _Android_ load the TFLite model using the _Interpreter_ class:  4. Resize the input tensor of the _Android_ interpreter to the size of the input image (bitmap):  5. Convert the input image (bitmap) to `ByteBuffer`:  6. Run inference:  However, for convenience reasons, I would like to use _Android's_ ML Binding to load the model. It would be potentially useful for cases like this to be able to resize the input tensor of the `Interpreter` as well, e.g.:  ... or encapsulate the calls in appropriate method names.  is there any way to use the ML Binding and change the input tensor size of the interpreter (that I missed)?", ! You can format the input and output size through Firebase ML kit and TF hub model binding. Feel free to close this issue if it is resolved. Thank you!,Thank you for the update .,Are you satisfied with the resolution of your issue? Yes No
1903,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Test //tensorflow/python/tools:saved_model_cli_test fails on s390x arch due to incorrect target_triple)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubunu 18.04  Mobile device NA  Python version 3.10.4  Bazel version 5.1.1  GCC/Compiler version 7.5.0  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue The full command used is as follows:  I checked the target_cpu and target_triple values which are getting set for s390x, it takes default values viz. target_triple=x86_64pclinux target_cpu="""" //tensorflow/python/tools:saved_model_cli_test test passes if we set the target_cpu and target_triple values explicitly at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/saved_model_cli.pyL891 This is not the right location to add this, but this confirms that the above error is somehow bypassed when we provide the s390x archspecific target_cpu and target_triple values. Maybe the littleendian check was encountered because the target_triple is set to x86_64pclinux? Also, I observed that the above check is considered only in subtest cases where we have variables_to_feed as an empty string. I tried setting  c opt cpu s390x  as well as copt=mtune=z14 copt=mzarch copt=march=z14 options in bazel test command. However, it didn't help either. Due to some reason, the LLVM compiler cannot use the underlying host CPU by default and in turn, the target_triple is set incorrectly. The subtest which is responsible for the failure of //tensorflow/python/tools:saved_model_cli_test is testAOTCompileCPUFreezesAndCompiles   https://github.com/tensorflow/tens)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vibhutisawant,Test //tensorflow/python/tools:saved_model_cli_test fails on s390x arch due to incorrect target_triple,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubunu 18.04  Mobile device NA  Python version 3.10.4  Bazel version 5.1.1  GCC/Compiler version 7.5.0  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue The full command used is as follows:  I checked the target_cpu and target_triple values which are getting set for s390x, it takes default values viz. target_triple=x86_64pclinux target_cpu="""" //tensorflow/python/tools:saved_model_cli_test test passes if we set the target_cpu and target_triple values explicitly at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/saved_model_cli.pyL891 This is not the right location to add this, but this confirms that the above error is somehow bypassed when we provide the s390x archspecific target_cpu and target_triple values. Maybe the littleendian check was encountered because the target_triple is set to x86_64pclinux? Also, I observed that the above check is considered only in subtest cases where we have variables_to_feed as an empty string. I tried setting  c opt cpu s390x  as well as copt=mtune=z14 copt=mzarch copt=march=z14 options in bazel test command. However, it didn't help either. Due to some reason, the LLVM compiler cannot use the underlying host CPU by default and in turn, the target_triple is set incorrectly. The subtest which is responsible for the failure of //tensorflow/python/tools:saved_model_cli_test is testAOTCompileCPUFreezesAndCompiles   https://github.com/tensorflow/tens",2022-08-12T11:08:11Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57126,Hi   Could you please take a look at this? Thank you very much!,Hi   Could you please take a look at this issue? Thanks ,"Hi  , I am eager to know the current status. Could you please confirm whether this is still an issue. Please confirm if this is an issue with latest versions like TF 2.13. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Hi , Thanks for checking. This issue still persists, and we have raised the following PR to fix the same.",Above mentioned PR fixes this TC. Closing this issue,Are you satisfied with the resolution of your issue? Yes No
1277,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hexagon delegate (tflite) functionality broken after rebuilding project)ï¼Œ å†…å®¹æ˜¯ (I had the hexagon delegate working fine for weeks, but yesterday I clicked ""Rebuild Project"" in Android Studio, and now I cannot get it working again. I have scrubbed every place on my disk where I can find the files `libhexagon_interface.so` and `libtensorflowlite_hexagon_jni.so`, and made sure that the only copy of those files present on my computer is one that I downloaded and extracted manually from oss.sonatype.org. I place these files into their location in `.grade/caches/...`. I'm using the latest `0.0.0nightlySNAPSHOT` from there, which is dated `Fri Aug 12 2022`. I delete all intermediate build directories, and wipe Android studio caches. Still, I get this log dump when trying to load the hexagon delegate:  Prior to hitting ""Rebuild Project"", this worked fine for weeks. Neither my Android hardware nor my software has changed. This is a realtime project we're working on, and without hexagon support, inference time goes from 200ms to 1600ms, which makes this project dead in the water. Please help!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bmharper,Hexagon delegate (tflite) functionality broken after rebuilding project,"I had the hexagon delegate working fine for weeks, but yesterday I clicked ""Rebuild Project"" in Android Studio, and now I cannot get it working again. I have scrubbed every place on my disk where I can find the files `libhexagon_interface.so` and `libtensorflowlite_hexagon_jni.so`, and made sure that the only copy of those files present on my computer is one that I downloaded and extracted manually from oss.sonatype.org. I place these files into their location in `.grade/caches/...`. I'm using the latest `0.0.0nightlySNAPSHOT` from there, which is dated `Fri Aug 12 2022`. I delete all intermediate build directories, and wipe Android studio caches. Still, I get this log dump when trying to load the hexagon delegate:  Prior to hitting ""Rebuild Project"", this worked fine for weeks. Neither my Android hardware nor my software has changed. This is a realtime project we're working on, and without hexagon support, inference time goes from 200ms to 1600ms, which makes this project dead in the water. Please help!",2022-08-12T08:02:54Z,type:support comp:lite TFLiteHexagonDelegate TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57120,Hi  ! Can you check below options and let us know. 1. Switch to 2.9/2.8 version of GPU delegates  2. You also root your device to resolve this issue.   Attached relevant thread for reference.  Thank you!,"Hi , I've tried 2.8.0 and 2.9.0 of the hexagon delegate, and I get the exact same error messages. The GPU delegate runs without errors, but I specifically need the hexagon support. My device is rooted, and I run with `setenforce 0`. Like I said, I had this working for weeks.", ! Sorry for the inconvenience.  ! Could you please look at this issue.,"OK... an update for posterity: I finally managed to get the NNAPI delegate working, so I no longer need hexagon support. The key thing that I was doing wrong, was calling `options.setUseNNAPI(true)`. Instead, the correct thing to do is just this: ",Are you satisfied with the resolution of your issue? Yes No, ! Thanks for sharing your fix.
652,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(cmake build is not installing library)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version master  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8.5  Bazel version _No response_  GCC/Compiler version 9.4  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sconde,cmake build is not installing library,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version master  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8.5  Bazel version _No response_  GCC/Compiler version 9.4  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output _No response_,2022-08-11T15:20:22Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,0,8,https://github.com/tensorflow/tensorflow/issues/57105,Could you please refer this document here https://cmake.org/cmake/help/latest/command/find_package.html to find the package. Let us know if this helps. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,There are no installed files/shared objects in the CMAKE_INSTALL_PREFIX path for the configuration. ,   Is there any help you can offer here? I'm still not able to build from the source using CMake.,"Hi , Thanks for reporting the issue, we don't have access to your exact environment and you are using environment variables in your command, can you please let us know what those are? specifically: CXX, CC, pkg_install, you can output them with this command: ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1127,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite in Play Services issue)ï¼Œ å†…å®¹æ˜¯ (**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): Lenovo/A6020a46/A6020a46:5.1.1/LMY47V/A6020a46_S105_161124_ROW:user/releasekeys  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): playservicestflitejava:16.0.0beta03  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): 22.26.15 (020408461192076) **Standalone code to reproduce the issue** TfLite.initialize(this, TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(true).build()); **Any other info / logs** Any reliable way to find if GPU Delegate is supported in given device? Currently I am relying on addOnFailureListener() callback with ""Error loading TFLite GPU delegate module"" message. Should I go back to CompatibilityList class of org.tensorflow.lite.gpu package for this one particular requirement? )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sandeep5193,TensorFlow Lite in Play Services issue,"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): Lenovo/A6020a46/A6020a46:5.1.1/LMY47V/A6020a46_S105_161124_ROW:user/releasekeys  TensorFlow Lite in Play Services SDK version (found in `build.gradle`): playservicestflitejava:16.0.0beta03  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): 22.26.15 (020408461192076) **Standalone code to reproduce the issue** TfLite.initialize(this, TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(true).build()); **Any other info / logs** Any reliable way to find if GPU Delegate is supported in given device? Currently I am relying on addOnFailureListener() callback with ""Error loading TFLite GPU delegate module"" message. Should I go back to CompatibilityList class of org.tensorflow.lite.gpu package for this one particular requirement? ",2022-08-11T07:01:31Z,stat:awaiting response type:support comp:lite TFLiteGooglePlayServices,closed,0,40,https://github.com/tensorflow/tensorflow/issues/57099,"Hi  ! You can check instructions in this document to check available GPU delegate. You can also build your GPU delegate through below Bazel commands.  I am doubtful about NNAPI implementation with Android 5.1 (Android 8.1 minimum requirment). You can check with other available delegate ( I see Snapdragon 616 in Lenovo/A6020a46 , Hexagon delegate might work , not sure though)","Hi , The document you have linked talks about using the delegate through org.tensorflow:tensorflowlitegpu module, while I am inclined with using them through Play Services. TensorFlow Lite available in the Google Play services API doesn't have any way to check if the device is compatible for GPU delegate.","We're not recommending to use the `CompatibilityList` class with TensorFlow Lite in Google Play services, because the list is a static copy built into the SDK, whereas the GPU delegate can get updated over time, so compatibility might actually change. (Also, currently the `CompatibilityList` class is bundled into the static GPU delegate SDK, so depending on it would include a full copy of the GPU delegate implementation, which adds unnecessary binary size). We hear your feedback about lack of an explicit API to query compatibility though. Stay tuned :)",Thanks for clarifying.,Are you satisfied with the resolution of your issue? Yes No,"FYI, we had a new release (16.0.0, first stable release) that adds a new API to check if the GPU delegate is available on a device ahead of time: TfLiteGpu.isGpuDelegateAvailable) Instead of having to trywithdelegate/catch/trywithoutdelegate, you can now write something like:  Hope this helps!","Hi , While this worked on our devices in development, recently we pushed an update, and we're seeing below error in Crashlytics:  any decent way to handle this?","Thanks! I've filed an internal bug (http://b/248231641), and we'll investigate this.","Now I'm not sure if this error is coming for TfLiteGpu.isGpuDelegateAvailable(context) method, will update when I have more info.","We haven't been able to reproduce the issue, so we'll welcome any new info you may have. On the other hand, we think we may have a solution to safeguard the code against this issue in the future, but it would require us to publish an SDK update, which may take a bit."," Any hint on how we can catch it for the time being? We already tried to catch it for TfLiteGpu.isGpuDelegateAvailable(Context) method, still it's throwing exceptions in prod. TfLite.initialize(Context, TfLiteInitializationOptions) is next suspect.","Yeah, the problem is that the exception is being thrown on the UI thread in a callback, not in the immediate call to `isGpuDelegateAvailable()` or `initialize()`, so catching exceptions there won't help.","Got it, thanks for clarification.","Since we haven't been able to reproduce it: do you have any data on how widespread the problem is for your app? Is it impacting all users or a fraction of them? If it is a fraction, any metric that would differentiate crashing vs noncrashing populations? Is there a way for me to access the app and see it crash in person so I can collect more data?","Don't have exact figures, but definitely below 5 per cent, and real count can be way less than it. Currently we have disabled this feature through Firebase Remote Config because of this issue, will share app once we enable it again.",Thanks for the data! <5% does explain why we didn't run into it earlier and why we have found it challenging to reproduce. We are still investigating for the root cause and will keep you updated on an eventual resolution,"FYI, we thought we can skip using `isGpuDelegateAvailable()` and use `initialize()` without GPU delegate support for now. But now we are getting similar error here as well:  Again, I am not sure where exactly it is failing, `initialize()` or `InterpreterApi.create()` or `runForMultipleInputsOutputs()`. Do you think ensuring API availability with ModuleInstallClient before using it will solve this?","I don't think it would hurt. FYI: internal usage of ModuleInstallClient is what seems to be causing the issue. It seems that for the users experiencing issues, the progress listener we set up on our instance ModuleInstallClient receives multiple terminal state updates (CANCELED, FAILED, COMPLETED) which was not expected and gives us our DuplicateTaskCompletionException. If your code handles this gracefully (for example by unregistering the listener after receiving the first terminal state update), you could know ahead of time which scenario you are in:  if it's successful, the code from the stacktrace above would be skipped (since the modules would already be available)  if it's unsuccessful, you can avoid running the offending code altogether (which if run would trigger the issue above) Meanwhile, we are still working towards a more stable fix, that would not require this extra code, but the solution above may unblock you faster",Thanks for the tip.,"To my surprise, there are no errors in past 14 hours. Last error was at Sep 23, 2022, 14:56:58 UTC. Any idea what could have changed? Can dynamically loading modules solve it?","You mentioned in an earlier comment that you had disabled the crashing code via Firebase Remote Config, so wouldn't that explain why you're no longer getting errors? Or does this mean you've reenabled it again and are not seeing new errors?","We have enabled it again, but this time, not using GPU delegate. See below comment, it was enabled again but wasn't calling `isGpuDelegateAvailable()`. We were then getting another error for TfLite module but then it stopped. > FYI, we thought we can skip using `isGpuDelegateAvailable()` and use `initialize()` without GPU delegate support for now. But now we are getting similar error here as well: >  >  >  > Again, I am not sure where exactly it is failing, `initialize()` or `InterpreterApi.create()` or `runForMultipleInputsOutputs()`. >  > Do you think ensuring API availability with ModuleInstallClient before using it will solve this?","Ah I see, thanks for explaining. Because TFLite for Play Services overall relies on downloadable APIs/modules, it is possible that just giving it more time may have helped by giving more opportunity for all devices to be uptodate. While the error number going down could be explained by that, I'm still surprised it would go to 0. Either way, we'll probably want to go ahead with an SDK update that handles this situation more gracefully. Couple of thoughts:  is it possible that your app is used on devices that don't have the Play Services? (for example, distributed outside of the Play Store)?  is it possible that some of your users may try to use your app in countries where they would not be able to get Play Services updates/downloads?","We have not distributed this anywhere other than Play Store. But we can see it listed in various unofficial stores when we search by name on Google, and many of them do not redirect it to Play Store. So yes, it's possible to install it on devices where Play Services is not supported.","> I don't think it would hurt. >  > FYI: internal usage of ModuleInstallClient is what seems to be causing the issue. It seems that for the users experiencing issues, the progress listener we set up on our instance ModuleInstallClient receives multiple terminal state updates (CANCELED, FAILED, COMPLETED) which was not expected and gives us our DuplicateTaskCompletionException. If your code handles this gracefully (for example by unregistering the listener after receiving the first terminal state update), you could know ahead of time which scenario you are in: >  > * if it's successful, the code from the stacktrace above would be skipped (since the modules would already be available) > * if it's unsuccessful, you can avoid running the offending code altogether (which if run would trigger the issue above) >  > Meanwhile, we are still working towards a more stable fix, that would not require this extra code, but the solution above may unblock you faster TFLite module's availability can be checked with `TfLiteClient`, but I do not see a class extending `OptionalModuleApi` for GPU delegate module. `isGpuDelegateAvailable()` is using an internal class for the same.","> TFLite module's availability can be checked with `TfLiteClient`, but I do not see a class extending `OptionalModuleApi` for GPU delegate module. `isGpuDelegateAvailable()` is using an internal class for the same. That's a good point! We should expose an `OptionalModuleApi` implementation for the GPU delegate module.","We just released an update for the GPU SDK: https://maven.google.com/web/index.htmlcom.google.android.gms:playservicestflitegpu:16.1.0 This should resolve the crash your users were experiencing and also exposes an `OptionalModuleApi` for the GPU delegate module (via `TfLiteGpu.getClient(context)`). Note that since we haven't been able to reproduce the initial, we can't guarantee that this specific issue no longer happens, but it should be a step in the right direction either way.","Great, that was super fast. On Thu, 29 Sep, 2022, 9:50 pm Adrien Couque, ***@***.***> wrote: > We just released an update for the GPU SDK: > https://maven.google.com/web/index.htmlcom.google.android.gms:playservicestflitegpu:16.1.0 > This should resolve the crash your users were experiencing and also > exposes an OptionalModuleApi for the GPU delegate module (via > TfLiteGpu.getClient(context)). > > Note that since we haven't been able to reproduce the initial, we can't > guarantee that this specific issue no longer happens, but it should be a > step in the right direction either way. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you modified the open/close state.Message > ID: ***@***.***> >","Recently we updated the app with the latest version of this lib, and now we are seeing this exception:  I assume this is for TFLite module and not GPU Delegate.","Ah, that's frustrating... Thanks for letting us know. I was hoping the issue was only impacting the GPU delegate optional module, but your error points to the problem also impacting the main TFLite optional module. So we'll have to publish SDK updates of playservicestflitejava, playservicestflitesupport, ... I'll let you know when those are available"
1224,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CMake and Bazel documentations for compiling tensorflow-lite are incomplete and/or incorrect)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04 LTS   Mobile deice _No response_  Python version 3.10  Bazel version N/A  GCC/Compiler version gccarm8.32019.03x86_64aarch64linuxgnu  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? Following the instructions on https://www.tensorflow.org/lite/guide/build_cmake_arm to compile the shared library for Raspberry Pi, neither 64 bit nor 32 bit libraries for ARMv8 / ARMv7 respectively can be compiled. Using the commands as provided to build for AArch64 (ARM64), compilation fails with the provided log. If NEON is disabled by passing `DTFLITE_ENABLE_XNNPACK=OFF` to CMAKE, compilation fails with another error message:  Furthermore, trying to build tensorflowlite with Bazel for ARMhf causes a similar problem:   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ibiglari,CMake and Bazel documentations for compiling tensorflow-lite are incomplete and/or incorrect,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.11.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04 LTS   Mobile deice _No response_  Python version 3.10  Bazel version N/A  GCC/Compiler version gccarm8.32019.03x86_64aarch64linuxgnu  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? Following the instructions on https://www.tensorflow.org/lite/guide/build_cmake_arm to compile the shared library for Raspberry Pi, neither 64 bit nor 32 bit libraries for ARMv8 / ARMv7 respectively can be compiled. Using the commands as provided to build for AArch64 (ARM64), compilation fails with the provided log. If NEON is disabled by passing `DTFLITE_ENABLE_XNNPACK=OFF` to CMAKE, compilation fails with another error message:  Furthermore, trying to build tensorflowlite with Bazel for ARMhf causes a similar problem:   Standalone code to reproduce the issue   Relevant log output  ",2022-08-11T05:23:19Z,stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux TF 2.9,closed,0,21,https://github.com/tensorflow/tensorflow/issues/57097,"Without analyzing too deeply, I see in your first log: `armlinuxgnueabihfg++ march=armv7a mfpu=neonvfpv4` Which seems to imply you want to build for the Armv7a, which is a 32bits platform, with hardware floatingpoint support. Yet in your 'Standalone code to reproduce the issue' I see you use `aarch64linuxgnu`, which is a 64bits Arm compiler for the Armv8 platform. This may explain your ""Exec format error"".","> Without analyzing too deeply, I see in your first log: `armlinuxgnueabihfg++ march=armv7a mfpu=neonvfpv4` Which seems to imply you want to build for the Armv7a, which is a 32bits platform, with hardware floatingpoint support. Yet in your 'Standalone code to reproduce the issue' I see you use `aarch64linuxgnu`, which is a 64bits Arm compiler for the Armv8 platform. This may explain your ""Exec format error"". I need both 32bit and 64 bits of the library, hence trying both commands."," ! Thanks for pointing it out. Bazel build is passing with Bazel 5.0.0 , Master and GCC 7.3.1 .  Attached gist for reference. Not Sure about CMake documentation part. But it is failing in Colab too with 2.8, 2.9 and nightly Passing to  to have better look. Thank you!", Thanks for your update. I am using Bazel 5.1.1 (as I was getting an error message that project workspace specifies 5.1.1 needs to be used).,Could you please follow the steps from the Gist here and let us know if you're still facing an issue. Thanks!," I can run those commands to create the library on Ubuntu, but the result is a library for the CPU architecture that's executing those commands, and not for ARM.","A few months back I successfully used the following commands to build Arm64 libraries with CMake. Give it a try.   N.B. Back then I cloned the master branch, nowadays I would recommend to clone a stable branch (then you know what worked for you, instead of hoping that the master branch still works for you). E.g. `git clone singlebranch branch r2.9 https://github.com/tensorflow/tensorflow tensorflow_src` toolchain_arm64.cmake: ","  Finally had a chance to try it, and worked like a charm for ARM64. Thanks for that :) Then I tried to apply the same method to get the 32bit library, so I changed the toolchain to armlinuxgnueabihf. Looks like XNN doesn't like to be compiled for it. I tried both `arm` and `armhf` as target processor, but in both cases below message was produced:  If I understand it correctly, XNN should be available for ARM EABI HF, right? On another note, it might be worth updating tensorflowlite documentations on how to correctly build the ARM libraries.","Glad you got the Arm64 libraries built. https://github.com/google/XNNPACK/ says about supported architectures: > ARM64 on Android, Linux, macOS, and iOS (including WatchOS and tvOS) > ARMv7 (with NEON) on Android > ARMv6 (with VFPv2) on Linux Strangely enough ARMv6 and ARM64 are mentioned for Linux, but not your ARMv7. You can try to disable Xnnpack during building, e.g. with the CMake macro DTFLITE_ENABLE_XNNPACK=OFF. I'm not sure whether that macro is still respected though, because as you have said correctly, the documentation is outdated.  As for the outdated documentation, you're not the first to mention: https://discuss.tensorflow.org/t/instructionsforcmakeonraspberrypizeroareinaccurate","  Yes that works. XNN can be switched off for ARM 7. For other people who stumble across this, you need to install a version of GCC and G++ that matches what you have on your Raspberry Pi. After building the libraries using GCC11, I was not able to link against them on RPi because latest available version of GCC on RPi (at this time) is 10. I had to slightly modify the CMake script and download GCC10 and G++10 for AARCH64","> For other people who stumble across this, you need to install a version of GCC and G++ that matches what you have on your Raspberry Pi. After building the libraries using GCC11, I was not able to link against them on RPi because latest available version of GCC on RPi (at this time) is 10. I had to slightly modify the CMake script and download GCC10 and G++10 for AARCH64 You may correct me if I'm wrong, but I suspect you encountered a GLIBC(XX) mismatch. And you resolved it by downgrading the build tools on your build machine, so that the build tools on your build machine won't use a newer GLIBC(XX) than available on your target machine (Raspberry Pi).", You are correct. Ended up compiling it on RPi direct. Simpler and without all the headache.,I think you can close this issue now.," , If your issue is resolved, could you please close this request. Thanks!",Are you satisfied with the resolution of your issue? Yes No,I was trying to follow your steps of building libtensorflowlite.a for aarch64 but getting the following linking error when I try to build my application that used the edge tpu as well. I was wondering if I was missing a step. Thanks! /usr/lib/gcccross/aarch64linuxgnu/9/../../../../aarch64linuxgnu/bin/ld: /home/gsosnow/doc/tflite_buildbr/libtensorflowlite.a(spectrogram.cc.o): in function `tflite::internal::Spectrogram::ProcessCoreFFT()': spectrogram.cc:(.text+0xf4): undefined reference to `rdft',"This is a linker error. The linker cannot find `rdft`. Is this an error you got from building the Tensorflow Lite libraries or from linking the Tensorflow Lite libraries to your application? In case of the former, I don't know why this happens. In case of the latter, make sure you link your application to the library that contains `rdft`. The term `FFT` in `ProcessCoreFFT` seems to imply 'Fast Fourier Transform'. Are you sure you are linking your application to the libraries in `_deps/fft2dbuild` in your build dir? N.B. Tensorflow Lite comprises several libraries. libtensorflowlite.a is just one of them.","Thanks, yes I was linking my application with libtensorflowlite.a, I see that I am not referencing all the libraries in _deps/!! Thanks for the pointer this makes sense now. I will give this a try, I really appreciate it!","I am pretty close to being able to build the application. I have been able to get rid of most of the linker errors but some persist for some reason. Thanks for any guidance! `$ make f Makefile_edgetpu_native g++ std=c++17 o tfedgetpu tfedgetpu.cc \ I/home/gsosnow \ I/home/gsosnow/include \ L/home/gsosnow \ L/home/gsosnow/_deps \ ltensorflowlite l:libedgetpu.so.1.0 lpthread lm ldl lfft2d_fftsg lfft2d_fftsg3d lfft2d_fftsg2d lfft2d_fft4f2d lflatbuffers lruy_allocator lruy_apply_multiplier lruy_blocking_counter lruy_block_map lruy_context lruy_context_get_ctx lruy_cpuinfo lruy_ctx lruy_denormal lruy_frontend lruy_kernel_arm lruy_kernel_avx2_fma lruy_kernel_avx512 lruy_kernel_avx lruy_pack_arm lruy_pack_avx2_fma lruy_pack_avx512 lruy_pack_avx lruy_prepacked_cache lruy_prepare_packed_matrices lruy_system_aligned_alloc lruy_thread_pool lruy_trmul lruy_tune lruy_wait lcpuinfo lcpuinfo_internals lclog lfarmhash labsl_synchronization /usr/bin/ld: /home/gsosnow/_deps/libruy_thread_pool.a(thread_pool.cc.o): in function `ruy::ThreadPool::CreateThreads(int)': thread_pool.cc:(.text+0x1d0): undefined reference to `ruy::BlockingCounter::Reset(int)' /usr/bin/ld: thread_pool.cc:(.text+0x2dc): undefined reference to `ruy::BlockingCounter::Wait(std::chrono::duration >)' /usr/bin/ld: /home/gsosnow/_deps/libruy_thread_pool.a(thread_pool.cc.o): in function `ruy::ThreadPool::ExecuteImpl(int, int, ruy::Task*)': thread_pool.cc:(.text+0x3d0): undefined reference to `ruy::BlockingCounter::Reset(int)' /usr/bin/ld: thread_pool.cc:(.text+0x478): undefined reference to `ruy::BlockingCounter::Wait(std::chrono::duration >)' /usr/bin/ld: /home/gsosnow/_deps/libruy_thread_pool.a(thread_pool.cc.o): in function `ruy::Thread::ThreadFunc(ruy::Thread*)': thread_pool.cc:(.text._ZN3ruy6Thread10ThreadFuncEPS0_[_ZN3ruy6Thread10ThreadFuncEPS0_]+0x70): undefined reference to `ruy::BlockingCounter::DecrementCount()' /usr/bin/ld: thread_pool.cc:(.text._ZN3ruy6Thread10ThreadFuncEPS0_[_ZN3ruy6Thread10ThreadFuncEPS0_]+0xbc): undefined reference to `ruy::BlockingCounter::DecrementCount()' /usr/bin/ld: /home/gsosnow/_deps/libruy_trmul.a(trmul.cc.o): in function `ruy::(anonymous namespace)::TrMulTask::Run()': trmul.cc:(.text+0xf4): undefined reference to `ruy::GetBlockByIndex(ruy::BlockMap const&, int, ruy::SidePair*)' /usr/bin/ld: trmul.cc:(.text+0x108): undefined reference to `ruy::GetBlockMatrixCoords(ruy::BlockMap const&, ruy::SidePair const&, ruy::SidePair*, ruy::SidePair*)' /usr/bin/ld: trmul.cc:(.text+0x2b0): undefined reference to `ruy::GetBlockMatrixCoords(ruy::Side, ruy::BlockMap const&, int, int*, int*)' /usr/bin/ld: /home/gsosnow/_deps/libruy_trmul.a(trmul.cc.o): in function `ruy::TrMul(ruy::Ctx*, ruy::TrMulParams*)': trmul.cc:(.text+0x5a0): undefined reference to `ruy::MakeBlockMap(int, int, int, int, int, int, int, int, ruy::CpuCacheParams const&, ruy::BlockMap*)' /usr/bin/ld: trmul.cc:(.text+0x7d8): undefined reference to `ruy::IsObviouslyLinearTraversal(int, int, int, int, int, ruy::CpuCacheParams const&)' collect2: error: ld returned 1 exit status make: *** [Makefile_edgetpu_native:40: tfedgetpu] Error 1 `","This issue was originally about building TfLite for Arm. Therefore we should not discuss here indepth how to link TfLite to an application. For the latter I refer to https://stackoverflow.com/a/72100557/7268445, where I was once helped somebody to accomplish that.","Thanks for the pointer!! I got it working, much appreciated!!"
752,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA:GPU] Fuse copy added by copy-insertion)ï¼Œ å†…å®¹æ˜¯ (   A follow up to https://github.com/tensorflow/tensorflow/pull/57020/ This add the hloopfusion optimization pass after the copyinsertion pass. This is to be able to fuse the introduced copies. An example HLO before optimizations when copyinsertion add copies:   Current after_optimizations:  New hlo after this PR:  I disable that optimization pass when there is control flow in the graph. This pass doesn't know how to handle that. I do not need that case now, so I do not plan to make it works when controlflow is used.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nouiz,[XLA:GPU] Fuse copy added by copy-insertion,"   A follow up to https://github.com/tensorflow/tensorflow/pull/57020/ This add the hloopfusion optimization pass after the copyinsertion pass. This is to be able to fuse the introduced copies. An example HLO before optimizations when copyinsertion add copies:   Current after_optimizations:  New hlo after this PR:  I disable that optimization pass when there is control flow in the graph. This pass doesn't know how to handle that. I do not need that case now, so I do not plan to make it works when controlflow is used.",2022-08-10T20:27:35Z,ready to pull comp:xla size:M,closed,0,40,https://github.com/tensorflow/tensorflow/issues/57087,I did the 2 comments and added one for discussion., Can you please resolve conflicts? Thank you!,rebased.,i rebased this PR to be on top of https://github.com/tensorflow/tensorflow/pull/57020/ and https://github.com/tensorflow/tensorflow/pull/57249. The only new commit is: ff10b929170b208b559cda347b882d1aaea8596c,Rebased to fix the conflicts.,"Can you please rebase to head , thank you !",Rebased without conflict.  can you approve again?,The added test is failing:  The error is: ,Ah I missed the fact that this PR relies on CC([XLA:GPU] HorizontalLoopFusion now generates a cleaner graph). That is probably way this PR is failing.,The dependent PR is merged. I rebased it.  or  can you approve again?,"This should be merged pretty soon, assuming no tests fail. Reapproval is not necessary in this case.","The added test is failing when TF is compiled *without* GPU support.  The error is  I'm not sure why we run tests for GPU functionality without a GPU, but we do.  do you know why? In any case, maybe wrap the test in an `if GOOGLE_CUDA` block, or fix it some other way.","We would ideally like to be able to *compile* for GPU without a GPU present, so would be good to understand why is it failing. On Wed, Aug 31, 2022 at 21:44 Reed ***@***.***> wrote: > The added test is failing when TF is compiled *without* GPU support. > > yes '' | TF_NEED_CUDA=0  ./configure   Compile without GPU support > bazel build c opt //tensorflow/compiler/xla/service/gpu:horizontal_loop_fusion_test > bazelbin/tensorflow/compiler/xla/service/gpu/horizontal_loop_fusion_test > > The error is > > [ RUN      ] HorizontalLoopFusionTest.CopyInsertionFusion > 20220831 12:37:49.036858: I tensorflow/compiler/xla/service/platform_util.cc:69] platform Host present but no XLA compiler available: could not find registered compiler for platform Host  was support for that platform linked in? > tensorflow/compiler/xla/service/gpu/horizontal_loop_fusion_test.cc:629: Failure > Expected equality of these values: >   total_fusion_instrs >     Which is: 0 >   1 > tensorflow/compiler/xla/service/gpu/horizontal_loop_fusion_test.cc:634: Failure > Value of: entry_root > Expected: tuple(gettupleelement(fusion), gettupleelement(fusion), gettupleelement(fusion), gettupleelement(fusion)) >   Actual: %tuple_out = (f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0}) tuple(f32[1]{0} %cst, f32[1]{0} %cst, f32[1]{0} %cst, f32[1]{0} %cst) (of type xla::HloInstruction const*), (%tuple_out = (f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0}) tuple(f32[1]{0} %cst, f32[1]{0} %cst, f32[1]{0} %cst, f32[1]{0} %cst)) > operand 0: > 	%cst = f32[1]{0} constant({0}) > doesn't match expected: > 	gettupleelement(fusion), (%cst = f32[1]{0} constant({0})) > [  FAILED  ] HorizontalLoopFusionTest.CopyInsertionFusion (6 ms) > > I'm not sure why we run tests for GPU functionality without a GPU, but we > do.   do you know why? In any case, > maybe wrap the test in an if GOOGLE_CUDA block, or fix it some other way. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","This test shouldn't be executed on CPU. It execute, so it mean the build on CPU worked. The BUILD file contains this:  `tf_cuda_tests_tags()` include the tag `requiregpu`. So this should be an issue with the CPU builder that include this tests while it shouldn't. Who can investigate that?",The test `VariableOpsTest.testReadWrite` in `//tensorflow/compiler/tests:variable_ops_test_gpu_mlir_bridge_test` is failing with this PR:  ,Hi  Can you please check 's comments and keep us posted ? Thank you!,Done. In all the changes something got lost. I added it back as a new commit to this PR.,"Any update? It was approved 4 days ago, but isn't merged yet. One CI failed, but the test failure doesn't seem related to my PR. It is a grappler error: //tensorflow/python/grappler:remapper_test_gpu", ,"I confirmed that this test fail here with upstream/master. So this isn't related to my PR. I rebased it,  can you approve it again? Maybe it will pass this time.",> I confirmed that this test fail here with upstream/master Could be both? We only test internally on Volta/Pascal now =/,"It seems this change is triggering a test error in a Jax test: https://github.com/google/jax/blob/main/tests/pmap_test.py PythonPmapTest.testCollectiveConstantNested F0914 15:08:28.717310    6476 hlo_live_range.cc:114] Check failed: instruction_schedule_.insert({instruction, time}).second  *** Check failure stack trace: ***     @     0x55a4708487c4  absl::log_internal::LogMessage::SendToLog()     @     0x55a470847f5d  absl::log_internal::LogMessage::Flush()     @     0x55a470848b89  absl::log_internal::LogMessageFatal::~LogMessageFatal()     @     0x55a46ccddf03  xla::HloLiveRange::FlattenSchedule()     @     0x55a46ccdd98f  xla::HloLiveRange::Run()     @     0x55a46ccce13e  xla::HeapSimulator::Run()     @     0x55a46cccdb7c  xla::HeapSimulator::MinimumMemoryForComputation()     @     0x55a465d5ee4a  xla::ListMemoryScheduler()     @     0x55a465d5f3fe  xla::DefaultMemoryScheduler()     @     0x55a4583ad38c  std::__u::__function::__policy_invoker::__call_impl()     @     0x55a465d61b1c  xla::(anonymous namespace)::ScheduleComputationHelper()     @     0x55a465d65657  std::__u::__function::__policy_invoker::__call_impl()     @     0x55a465d614c4  xla::ScheduleModule()"," We could provide HLO dump, but I expect this situation to be quite frequent in the future, as JAX tends to have better test coverage than we do. I think it would be worthwhile to figure out how to run JAX tests with a given revision, do you think this is a reasonable process?",">  We could provide HLO dump, but I expect this situation to be quite frequent in the future, as JAX tends to have better test coverage than we do. >  > I think it would be worthwhile to figure out how to run JAX tests with a given revision, do you think this is a reasonable process? It makes sense. But we need to find a way that doesn't slow down the workflow.","I think figuring out how to do it on your side will be significantly quicker than us importing the PR, noting the breakage, and then reporting to you? In general I think it might be a good idea to run JAX test suite on all nontrivial XLA PRs. If it passes, the chance of internal test breakages will be significantly smaller.","I have a different error:  But I have this error with and without this PR (on the commit this PR is based on) and with upstream TF and with upstream JAX. Are you able to make this test pass without my PR? If so, what is the setup?"," for any pointers, ""Bus error"" looks like some kind of setup issue (?)","I'm trying to reproduce the JAX error, but I'm not able on a V100 and a A100 DGX node. Do you have instruction on how to do that? I'm not even able to get the test to pass with the TOT of jax and TF without my changes. My last try was like this:  I have this error:  Any idea how to reproduce the test working test? When this pass, to test my code change, I would try: ","For anyone else reading this, ""Bus error"" was from running in a docker container without enough SHM space for NCCL.","Unfortunately this change got rolled back, because an internal test started failing with a verification error: Nonroot instruction %tuple.284 = (f32[512,128]{1,0}, f32[512,128]{1,0}) tuple(f32[512,128]{1,0} %divide.240, f32[512,128]{1,0} %divide.161.clone.2) in %copy_fusion = (f32[65536]{0}, f32[65536]{0}, f32[65536]{0}, f32[65536]{0}) fusion(f32[] %gettupleelement.763, f16[512,128]{1,0} %cublasgemm.55, f16[512,128]{1,0} %cublasgemm.89, f16[128,512]{1,0} %cublasgemm.53, f16[128,512]{1,0} %cublasgemm.87), kind=kInput, calls=%copy_horizontally_fused_computation must have users. I will try to extract a reproducer."
678,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Installation of tflite-model-maker 0.4.1. takes forever)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tflitemodelmaker==0.4.1  Custom Code No  OS Platform and Distribution Ubuntu 18.04.3  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,piyushmundhra,Installation of tflite-model-maker 0.4.1. takes forever,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tflitemodelmaker==0.4.1  Custom Code No  OS Platform and Distribution Ubuntu 18.04.3  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-10T19:34:54Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux,closed,0,14,https://github.com/tensorflow/tensorflow/issues/57086,Update: installing the nightly version works fine and I am able to import searcher. I wonder why this installation was easily successful and not the stable package version?,"Hi  ! To use a specific branch, Please follow below syntax  Attached gist for reference.  Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"To me, neither the stable version nor nightly version was working. However, installations on colab would go through fine.  Ended up checking python version on colab, created a virtual env with the same python version as in colab. Could install successfully.",Hi  ! Could you tell your python version and system details to replicate this issue. Thank you!,"Local system is on python 3.10. With this, model maker nightly version installation goes into recursive mode. Created a conda virtual env with python version 3.7. Model maker was installed successfully.",Having the same issue Python 3.10.6 It seems like it is downloading all the archived nightly builds: Collecting tfnightly   Downloading tf_nightly2.12.0.dev20221216cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (556.9 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 556.9/556.9 MB 1.5 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221215cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (556.5 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 556.5/556.5 MB 3.1 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221214cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (556.4 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 556.4/556.4 MB 3.3 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221213cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (556.6 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 556.6/556.6 MB 3.2 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221212cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (558.1 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 558.1/558.1 MB 2.5 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221211cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (558.1 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 558.1/558.1 MB 2.5 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221210cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (558.1 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 558.1/558.1 MB 3.3 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221209cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (558.0 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 558.0/558.0 MB 2.8 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221208cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (557.9 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 557.9/557.9 MB 3.1 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221207cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (554.1 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 554.1/554.1 MB 1.7 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221206cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (553.9 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 553.9/553.9 MB 2.4 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221205cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (553.9 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 553.9/553.9 MB 2.4 MB/s eta 0:00:00   Downloading tf_nightly2.12.0.dev20221204cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (553.8 MB)      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 553.8/553.8 MB 3.0 MB/s eta 0:00:00,"as of 27.04.2023 it's still not fixed (or it occurs again), I am not able to install on colab  this takes 25min+ !image",Hi  . I am facing the same issue right now.. Did you manage to solve this? Thanks.,Same isssue cannnot install !pip install q tflitemodelmaker,> Same isssue cannnot install !pip install q tflitemodelmaker hey same issue here .. Please tell me what is the solution . Becouse previous month i train and it works fine . but now it took all the disk on colab .. my project now stucks ," unfortunately no, no progress so far, created a separate ticket: https://github.com/tensorflow/tensorflow/issues/60431issuecomment1530981946"
1076,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow-lite: ""ValueError: Invalid tensor size"" when output is a zero-size tensor)ï¼Œ å†…å®¹æ˜¯ (**Summary  when the output has zerosize, I get ""ValueError: Invalid tensor size"" when trying to retrieve the output value  even when the output is supposed to have zerosize.**  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **MacOS Monterrey 12.2.1**  TensorFlow installation (pip package or built from source): **`2.8.1`, installed with `conda install c apple tensorflow==2.8.1`**  2. Code Provide code to help us reproduce your issues using one of the following options:  Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong: The model gives the error when outputs have zerosize  even when they are expected to have zerosize.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,petered,"TensorFlow-lite: ""ValueError: Invalid tensor size"" when output is a zero-size tensor","**Summary  when the output has zerosize, I get ""ValueError: Invalid tensor size"" when trying to retrieve the output value  even when the output is supposed to have zerosize.**  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **MacOS Monterrey 12.2.1**  TensorFlow installation (pip package or built from source): **`2.8.1`, installed with `conda install c apple tensorflow==2.8.1`**  2. Code Provide code to help us reproduce your issues using one of the following options:  Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong: The model gives the error when outputs have zerosize  even when they are expected to have zerosize.",2022-08-10T18:14:40Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.11,closed,0,7,https://github.com/tensorflow/tensorflow/issues/57084,"Hi  ! Thanks for reporting this bug.  ! Could you look at this issue. Attached gist in 2.8, 2.9 and nightly for reference. Thank you!",I was able to reproduce this issue in TF 2.11 and TF Nightly 2.12.0.dev20230121 as well.  Please find the gist here. Thank you.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,Come on  this is no way to track issues.   Why close it if nothing was solved?
1255,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Header tensorflow/tsl/platform/stack_frame.h not included with tf-nightly pip package)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v1.12.179472g4aaefec710e 2.11.0dev20220810  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Horovod does not build with `tfnightly` anymore, see https://github.com/horovod/horovod/issues/3641 The code includes `tensorflow/core/framework/op.h` at https://github.com/horovod/horovod/blob/master/horovod/tensorflow/mpi_ops.ccL34. Header files in `tensorflow/core` are included with the pip package. However, this ultimately includes `tensorflow/tsl/platform/stack_frame.h`, which does not come with the package. This appears to have changed with https://github.com/tensorflow/tensorflow/commit/399e4071c471f6dc47bf245f8aeb8ab0c0374fce,   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,maxhgerlach,Header tensorflow/tsl/platform/stack_frame.h not included with tf-nightly pip package,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v1.12.179472g4aaefec710e 2.11.0dev20220810  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Horovod does not build with `tfnightly` anymore, see https://github.com/horovod/horovod/issues/3641 The code includes `tensorflow/core/framework/op.h` at https://github.com/horovod/horovod/blob/master/horovod/tensorflow/mpi_ops.ccL34. Header files in `tensorflow/core` are included with the pip package. However, this ultimately includes `tensorflow/tsl/platform/stack_frame.h`, which does not come with the package. This appears to have changed with https://github.com/tensorflow/tensorflow/commit/399e4071c471f6dc47bf245f8aeb8ab0c0374fce,   Standalone code to reproduce the issue   Relevant log output  ",2022-08-10T18:12:06Z,type:bug comp:core,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57083,Are you satisfied with the resolution of your issue? Yes No,"We likely need to wait for the next nightly package to confirm it is solved, will track that!",Thanks for getting on this so quickly ğŸ‘ , tfnightly 2.11.0.dev20220812 comes with the header file and that fixes the Horovod build,Thanks for confirming!
715,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(raise ValueError( ValueError: Given `time_step`: TimeStep( {'discount': array(1., dtype=float32),)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution MacOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Shinwazu,"raise ValueError( ValueError: Given `time_step`: TimeStep( {'discount': array(1., dtype=float32),",Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution MacOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-10T09:23:51Z,stat:awaiting response type:bug stale TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57082,", I do not have access to the link you have provided. Could you please provide the required permissions to view the files or the colab gist. Thank you !image","tilakrayal, I'm online now and I have given you full access to the files. https://codewithme.global.jetbrains.com/9Q5ZV_wWhCtt14FRmGi_cwp=PC&fp=711A53B05D7D46196DCF40C0C122FBE9EAFCAF76EA142EF150833CBDB7FAFD60 Thank you. On Thu, Aug 11, 2022 at 7:17 AM tilakrayal ***@***.***> wrote: >  , > I do not have access to the link you have provided. Could you please > provide the required permissions to view the files or the colab gist. Thank > you > > [image: image] >  > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","The access link keeps changing. Please mail me or comment when you want access so that I can provide you with the latest link. I hope you understand.  Kind regards, SHINWAZU",", I  was facing the same issue and do not have access to the link you have provided. Could you please provide the required permissions to view the files or the colab gist. Thank you","tilakrayal, https://gist.github.com/Shinwazu/3056cf83254bdc8a65249957f757f0b8 I'm providing a link to my github gist. I have written the sort of error I'm dealing with. Please help!!! Kind regards, SHINWAZU On Fri, Aug 12, 2022 at 12:59 PM tilakrayal ***@***.***> wrote: >  , > I was facing the same issue and do not have access to the link you have > provided. Could you please provide the required permissions to view the > files or the colab gist. Thank you > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >",", This issue is not related to `tensorflow`. It is more related to `tensorflowagents`. Could you please try to post this issues in `tensorflowagents repo` for the assistance. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
796,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow getting length when using tf.data.Dataset as numpy iterators)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version v2.9.018gd8ce9f9c301 2.9.1  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When using tensorflow Dataset as numpy iterator I want length/cardinality to work.   Standalone code to reproduce the issue Colab Link  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pbk0,Allow getting length when using tf.data.Dataset as numpy iterators,Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version v2.9.018gd8ce9f9c301 2.9.1  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When using tensorflow Dataset as numpy iterator I want length/cardinality to work.   Standalone code to reproduce the issue Colab Link  Relevant log output _No response_,2022-08-10T06:05:08Z,type:feature comp:data TF 2.9,open,0,2,https://github.com/tensorflow/tensorflow/issues/57080," I was able to replicate the issue, please find the gist here. Thank you!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space."
1358,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix endianness issue in Tensor::BitcastFrom() on s390x)ï¼Œ å†…å®¹æ˜¯ (The following Bitcast related test cases would fail on s390x (BE machines):  `//tensorflow/python/ops/ragged:ragged_bitcast_op_test`  `//tensorflow/compiler/tests:cast_ops_test_cpu`  `//tensorflow/compiler/tests:cast_ops_test_cpu_mlir_bridge_test` I found the root cause is that the basic Bitcast operation in `Tensor::BitcastFrom()` only reuses the input buffer, see https://github.com/tensorflow/tensorflow/blob/55782dc6b8276eddce25f56fe97aacd0fc3c6fe7/tensorflow/core/framework/tensor.ccL757, which would cause the endianness issue on BE machines when bitcasting between types of different sizes. This PR made code changes in `tensor.cc` which use `tensor::DeepCopy()` function to allocate a new buffer for the output in `BitcastFrom()` function on BE machines when the types are of different sizes, since the output buffer needs to be swapped to solve the endianness issue and the input buffer shouldnâ€™t be affected. After applying this PR, the above test cases could pass on BE machines. This code change would not cause any regressions on existing test cases and it won't affect LE machines. Signedoffby: KunLu )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kun-lu20,Fix endianness issue in Tensor::BitcastFrom() on s390x,"The following Bitcast related test cases would fail on s390x (BE machines):  `//tensorflow/python/ops/ragged:ragged_bitcast_op_test`  `//tensorflow/compiler/tests:cast_ops_test_cpu`  `//tensorflow/compiler/tests:cast_ops_test_cpu_mlir_bridge_test` I found the root cause is that the basic Bitcast operation in `Tensor::BitcastFrom()` only reuses the input buffer, see https://github.com/tensorflow/tensorflow/blob/55782dc6b8276eddce25f56fe97aacd0fc3c6fe7/tensorflow/core/framework/tensor.ccL757, which would cause the endianness issue on BE machines when bitcasting between types of different sizes. This PR made code changes in `tensor.cc` which use `tensor::DeepCopy()` function to allocate a new buffer for the output in `BitcastFrom()` function on BE machines when the types are of different sizes, since the output buffer needs to be swapped to solve the endianness issue and the input buffer shouldnâ€™t be affected. After applying this PR, the above test cases could pass on BE machines. This code change would not cause any regressions on existing test cases and it won't affect LE machines. Signedoffby: KunLu ",2022-08-09T20:18:08Z,awaiting review ready to pull size:S,closed,0,36,https://github.com/tensorflow/tensorflow/issues/57069,I've updated this PR to fix the object scope issue for `ts_.buf_`. The above mentioned test cases all passed on master branch.,"Hi   , Could you please kindly review this PR? Thank you very much!",Hi KunLu thanks for the fix! Just a quick comment  the official bitcast doc https://www.tensorflow.org/api_docs/python/tf/bitcast mentions that there is no data copying so the buffer reuse is a critical requirement. If we wanted to break backwards compatibility then there would have to be an RFC process for it. Is there a need to create a copy of the in buffer for the out buffer  can we not do the transformations on the out buffer?,"Hi  , Thanks so much for your quick reply. If we skip the data swapping on the outer buffer, the result on BE platforms will differ from LE platforms due to different endian orderings. If we reuse the input buffer and keep the data swapping, the content of input buffer will be changed as well. However, we can make a slight adjustment in `cast_ops_test.py` (as shown in the following patch), so that all the above test cases could still pass.  I've also noticed the following note from the documentation: > Note: Bitcast is implemented as a lowlevel cast, so machines with different endian orderings will give different results.  Although the documentation has mentioned that, I think it would be better to make BE machines get the same results as LE machines in these test cases. For example, in test case `//tensorflow/compiler/tests:cast_ops_test_cpu`, I think it's comparing the results between the JIT Compiled XLA Bitcast operation with the basic tensor Bitcast operation, as shown in the code below: https://github.com/tensorflow/tensorflow/blob/6cd264ba65e6161164d7f1b463b7d2415ac0aeb0/tensorflow/compiler/tests/cast_ops_test.pyL35L47  I found that the JIT Compiled XLA Bitcast operation (`compiled_f()`) will get the same result with Intel (LE platform) on s390x (BE platform), so I wonder if it has used separate input and output buffers, otherwise after the execution of `compiled_f(x)`, the data in `x` would be changed. Since `f(x)` which invokes the basic tensor Bitcast operation will give BE specific result, the original test would fail on s390x. That's the reason why I think we should do the buffer swapping in `tensor.cc` to make the two different versions of Bitcast operation to get the same result, which aligns with Intel platform as well. Please let me know what you think about this issue, thanks again! ",Thanks for your explanation KunLu. Could you please update the documentation at   tensorflow/tensorflow/tensorflow/go/op/wrappers.go  tensorflow/tensorflow/tensorflow/compiler/mlir/tensorflow/ir/tf_generated_ops.td  tensorflow/tensorflow/tensorflow/core/api_def/base_api/api_def_Bitcast.pbtxt to indicate the a copy is made on BE machines.,Thanks  !  I've updated the above mentioned documentation. Could you please take a look?,"Hi  , Hope all is well.  has approved this PR and I've fixed the BUILD file format issue as per the suggestion by `Code Check  Changed Files`. Could you please take a look? Thank you very much! ","CI checks showed several gpu related test cases will fail due to the invocation of `tensor::DeepCopy()`, so I moved this function again into the `else` block to ensure the copy only happens on BE machines. Also removed the entry `""//tensorflow/core:framework"",` from `tensorflow/core/framework/BUILD` to fix the `cycle in dependency graph` issue. Hi  , could you please take a look again? Thank you so much!","Hi  , Sure, I've updated the PR description accordingly. Could you please take a look? Thank you!",lu20 this PR now has a cyclic dependency and fails to build.  Can you rebase and address it?,"Thanks  !  Sure, I'll update this PR to fix it and let you know once I'm done.","Hi  , I've updated the code to reduce the dependency complexity in `tensorflow/core/framework/BUILD`. Now the `ByteSwapArray()` function only depends on `//tensorflow/core/util/tensor_bundle:byteswaparray` target, which would not cause cyclic dependency.  Please take a look at the latest code change when you have some time. Thank you very much!","Hi  , Hope all is well. I saw that all relevant CI checks had passed. Do you have any idea when this PR could be merged? Please let me know if there is anything I should do on my side. Thank you so much!","We're still seeing internal build failures related to visibility of `tensor_util.h`, specifically for android builds and tflite.  I'll need to investigate later today.","It is because `tensor_util.h` and its dependencies are not in `framework_internal_private_hdrs`.  We can add `tensor_util.h` in there, but now that depends on `tensor_bundle/byte_swap_array.h` and the like, which are in a different package.  This is causing a layering issue.  We shouldn't have `tensorflow/core/framework:tensor` depend on `tensor_bundle` headers. The byte swap headers might need to be moved around, and need to be included in `framework_internal_private_hdrs`.  Maybe move them to `util` and add them there?","Hi  , I have a few questions: 1. I checked and found that `tensor_util.h` is already in `//tensorflow/core/framework:framework_internal_private_hdrs`: https://github.com/tensorflow/tensorflow/blob/1d49a934a6e5bab4e89be1d6d54ddff47834b6eb/tensorflow/core/framework/BUILDL244     Do we still need to adjust it? If the answer is yes, where should we add it to? 2. Reg the byte swap headers, do you mean that we should move `byte_swap_array.h` and  `byte_swap_array.cc` into `tensorflow/core/util` folder and add `byte_swap_array.h` to `//tensorflow/core/util:framework_internal_private_hdrs` ? Thank you very much!","> 1. I checked and found that `tensor_util.h` is already in `//tensorflow/core/framework:framework_internal_private_hdrs`: Ah, sorry, that one seems to need to be in `mobile_srcs_no_runtime` as well.  Maybe the others? > 2. Reg the byte swap headers, do you mean that we should move `byte_swap_array.h` and  `byte_swap_array.cc` into `tensorflow/core/util` folder and add `byte_swap_array.h` to `//tensorflow/core/util:framework_internal_private_hdrs` ? Yes, anything that is reference by something in `framework` needs to be added as a header there.  These may also need to be included in the `mobile_srcs_no_runtime`, but I haven't verified  likely anywhere `tensor.h` is added you will need to add any other headers that it requires.  Can you refactor a bit to simplify the layering?  If the byte swap utilities are generalpurpose and are used within `framework:tensor`, then the layout should reflect that. Thanks!","Sure, I'll refactor the code and get back to you once it's ready. Thank you  !","Hi  , Thanks for your detailed explanation. I've made the following changes: 1. Added `tensor_util.h` to `mobile_srcs_no_runtime` in `tensorflow/core/framework/BUILD`. 2. Moved `byte_swap_array.h` and `byte_swap_array.cc` into `tensorflow/core/util` folder, added these two files to `mobile_srcs_no_runtime` and added  `byte_swap_array.h` to `framework_internal_private_hdrs` in `tensorflow/core/util/BUILD`. 3. Updated other code or BUILD files which reference `byte_swap_array.h` or `byteswaparray` target. Please take a look when you have some time. Thanks again!","Hi  , Hope all is well. Could you please help me check the status of this PR? Are there any adjustments I should make on my side before it could be merged? Thank you very much!","lu20 sorry, I've been away for a week. We still have header issues.  Now it's complaining about  I'm not sure why we're getting this now, since AFAIK we always included that header.  Maybe we actually rely on it now though with your additions.","> lu20 sorry, I've been away for a week. >  > We still have header issues. Now it's complaining about >  >  >  > I'm not sure why we're getting this now, since AFAIK we always included that header. Maybe we actually rely on it now though with your additions. Hi  , No problem. Thanks for the response. I'm not very sure how to solve this issue, should I add `"":tensor_shape_proto_cc"",` in the `deps` of tf_cuda_library `""tensor""` here? https://github.com/tensorflow/tensorflow/blob/c755d039138e6a248170b61d4454096ff219c339/tensorflow/core/framework/BUILDL788L827 Thank you!","> should I add `"":tensor_shape_proto_cc"",` in the `deps` of tf_cuda_library `""tensor""` here? Yes, that should fix it, thanks.","Hi  , I just checked the master branch and found that this commit https://github.com/tensorflow/tensorflow/commit/2d3eb4071b63ae55cad1c198b8292d90ae761b87 has already moved core/util/tensor_bundle/byte_swap_array.{cc,h} to TSL and it didn't move byte_swap_tensor.h with them.  Should we follow it and keep byte_swap_array.{cc,h} in TSL? Thank you!","> Should we follow it and keep byte_swap_array.{cc,h} in TSL? Thank you! Yes, everything should be kept together, thanks.","OK, I'll follow the practice in master branch. Thank you.","Hi  , Thanks for your advice. The code changes of this round include: 1. Fix the header issue by adding `:tensor_shape_proto_cc` to `deps`. 2. Remove the trailing spaces from the description lines. 3. Use the way consistent with master branch to deal with `byte_swap_array` dependency. Please review the code change when you have some time. Thanks again!","Hi  , I've moved `byte_swap_array.h` and `byte_swap_array.cc` to `tsl/util`, please take a look when you have some time. Thank you!","Hi  , Could you please review the code change when you have some time? Thank you very much!","Hi  , I just checked the `Code Check  Full` result and realized that I forgot to update the `//tensorflow/tsl/util/tensor_bundle:mobile_srcs` target in `tensorflow/core/util/tensor_bundle/BUILD`. Since the `tensor_bundle` folder under `tsl/util` was removed, I added a file group in `tensorflow/tsl/util/BUILD` and used it to replace the missing target in `tensorflow/core/util/tensor_bundle/BUILD`. Please review the latest code change when you have some time. Thanks very much for your time and effort!"
1262,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix endianness issue in BitcastConvert operation of HloEvaluator on s390x)ï¼Œ å†…å®¹æ˜¯ (Test case `//tensorflow/compiler/xla/tests:bitcast_convert_test_cpu` would fail on s390x (BE machines). This test case applies two XLA backends (HOST/CPU and Interpreter) to execute the `BitcastConvertHloTest` and compare the results. However, the `Interpreter` backend which executes the graph with builtin `HloEvaluator` uses direct memory copy when performing Bitcast operations, which causes the endianness issue on BE machines. This patch will do two rounds of endianness swap after direct memory copy in `LiteralBase::BitcastConvert()` function, which would be invoked by `HloEvaluator` to evaluate HLO modules containing `Bitcast` instructions. The first swap is based on the input data type, while the second one is based on the output data type. After applying the code change, all HLO subtests in `//tensorflow/compiler/xla/tests:bitcast_convert_test_cpu` could pass. This code change would not cause any regressions on existing test cases and it won't affect LE machines. Signedoffby: KunLu )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kun-lu20,Fix endianness issue in BitcastConvert operation of HloEvaluator on s390x,"Test case `//tensorflow/compiler/xla/tests:bitcast_convert_test_cpu` would fail on s390x (BE machines). This test case applies two XLA backends (HOST/CPU and Interpreter) to execute the `BitcastConvertHloTest` and compare the results. However, the `Interpreter` backend which executes the graph with builtin `HloEvaluator` uses direct memory copy when performing Bitcast operations, which causes the endianness issue on BE machines. This patch will do two rounds of endianness swap after direct memory copy in `LiteralBase::BitcastConvert()` function, which would be invoked by `HloEvaluator` to evaluate HLO modules containing `Bitcast` instructions. The first swap is based on the input data type, while the second one is based on the output data type. After applying the code change, all HLO subtests in `//tensorflow/compiler/xla/tests:bitcast_convert_test_cpu` could pass. This code change would not cause any regressions on existing test cases and it won't affect LE machines. Signedoffby: KunLu ",2022-08-09T19:40:56Z,ready to pull comp:xla size:S,closed,0,12,https://github.com/tensorflow/tensorflow/issues/57067,lu20 Could you give a bit more background about the change? Are all other tests passing on BE machines?,"Thanks  , Sure. Test case `//tensorflow/compiler/xla/tests:bitcast_convert_test_cpu` uses two XLA backends (HOST/CPU and Interpreter) to execute the `BitcastConvertHloTest`. However, on BE machines these two backends got different results. The HOST backend which lowers the HLO module further to LLVM IR before execution got the identical result to LE machines (such as amd64), while the Interpreter backend which executes the graph with builtin `HloEvaluator` gave BE specific result.  This PR is to swap the buffer to address the endianness issue in this process, it will fix this test case and won't cause any regressions. There are several other test case failures on BE machines which are also caused by the endianness issue, we'll address them in different PRs, such as https://github.com/tensorflow/tensorflow/pull/57060, https://github.com/tensorflow/tensorflow/pull/57065 and https://github.com/tensorflow/tensorflow/pull/57069.","Could you provide some rationale for this work? In particular, what the longer term plan for big endian support is. https://github.com/openxla/xla/discussions would be a great place to start a discussion and create more visibility for this effort.","Hi  , Thanks! From my understanding, the rationale behind this issue is that some tensor buffers may contain endian specific data when performing model input/output or serialization/deserialization in TensorFlow code. If the endianness issue isn't taken into consideration, these codes may run correctly on LE platform but fail on BE platform. For example, trying to load a LE specific model data directly on a BE machine would cause error. And for this work, Bitcast operation could not be done in the same way (i.e., direct memory copy) on BE platform as on LE platform. We've contributed several PRs to tensorflow repo to address the endianness issue in model saving/loading process on BE machines, such as CC(Fix Const op tensor_content on s390x during save/load) and CC(Fix swapping of tensor_content when loading a SavedModel on s390x). However, there are still some test case failures caused by the similar issue, especially in the `lite` and `compiler` modules, such as certain `FlatBuffers` related test cases and `quantization` test cases.  Having a holistic solution for endianness issue in TensorFlow code base would be great, but I guess it will take some time. For now I suggest we can fix these issues one by one. Please let me know if you have any suggestions. Thanks again.", We are porting TensorFlow code on s390x for each major release and will keep testing on this code. We will raise PRs to update them if necessary. You can also let us know whenever you want to update the code and we'll do the test and change accordingly. ,"Hi   , Hope all is well. Just now I found this PR was closed by `copybaraservice` because PR https://github.com/tensorflow/tensorflow/pull/56578 was merged. However, these two PRs are unrelated.  I checked and found that this PR was incorrectly referenced in the comment of a57ba6c, I think that's why it was closed when PR https://github.com/tensorflow/tensorflow/pull/56578 was merged. Could you please reopen this PR? It was approved before being closed accidentally. Thank you very much! ",lu20 Can you please check 's comments and keep us posted ? Thank you!, Sure. Sorry I've been on vacation for the past few days. I'll update this PR as per 's comment ASAP. Thank you!, Thank you :),"Hi  , Hope all is well with you. I saw the `Code Check  Changed Files` in CI check showed a code format issue in `tensorflow/core/util/tensor_bundle/BUILD`. Should I fix it first? Please let me know if there are any other adjustments which I should make at my end. Thank you very much!","> I saw the `Code Check  Changed Files` in CI check showed a code format issue in `tensorflow/core/util/tensor_bundle/BUILD`. Should I fix it first? Please let me know if there are any other adjustments which I should make at my end. Thank you very much! The dependencies are not sorted IIRC but this is fixed automatically on import, so no need on your side to change something. This is blocked by some internal merge issues but I keep working on it. Hope to get it landed this week.",Got it. Thank you so much  
610,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to load model)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ArthurBesse,Unable to load model,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-09T18:00:12Z,stat:awaiting response type:bug stale comp:keras TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57061,", Could you please try setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device.  Also Adding `./` in front of the file name like this filename = open(""./model3.pkl"", ""rb"") should help.   If you are able to load the file in linux, please make sure the file was not corrupted in Windows. Thank you!","  > Could you please try setting the experimental_io_device option in tf.saved_model.LoadOptions to the io_device. Hm, I think I can't. `pickle.load` does not have a suitable argument to pass `tf.saved_model.LoadOptions` . > Also Adding ./ in front of the file name like this filename = open(""./model3.pkl"", ""rb"") should help. Nothing changed. > If you are able to load the file in linux, please make sure the file was not corrupted in Windows. I don't understand what you mean. My code saves the model to a file and then tries to load the model from the same file. So, even if the file is corrupted, I think it will be another subject for investigation. Thank you, Arthur",", I am unable to reproduce the reported issue. I was able to save and load model. Please find the gist here. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
726,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GlobalAveragePooling2D-layer in mixed-precision mode and using CPU produces incorrect results)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version v2.8.0rc132g3f878cff5b6 2.8.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cgebbe,GlobalAveragePooling2D-layer in mixed-precision mode and using CPU produces incorrect results,Click to expand!    Issue Type Bug  Source source  Tensorflow Version v2.8.0rc132g3f878cff5b6 2.8.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-09T14:38:59Z,stat:awaiting response type:bug stale comp:keras TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57057,", I tried to execute the provided code in an alternative approach and it was executed without any issues. Kindly find the gist of it here. Also could you please try passing `dtype=tf.float32` and let us know if it works. Thank you!"," : Just to understand you correctly: Do you mean the example ran through? If yes, this is expected. However, the second test asserts that the output is close to the `wrong_result`, whereas it should be close to the `correct_result`. Maybe that clears things up?",", Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
711,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,elina-israyelyan, Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-09T11:41:24Z,stat:awaiting tensorflower type:bug comp:keras TF 2.9,open,0,30,https://github.com/tensorflow/tensorflow/issues/57052,"israyelyan  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"I also got a `Type inference failed` error when running `tf.keras.Model.fit()` in tf 2.9. I didn't see this kind error in tf 2.8 with the identical code.  Source binary  Tensorflow Version 2.9.1  OS Platform and Distribution Linux Ubuntu 18.04  Python version 3.7  Current behaviour Run tf.keras.Model.fit() and the error shows up.  Standalone code to reproduce the issue Link to source code: https://drive.google.com/file/d/1k78lpGVthB7nthEkYgUs3JNJTuR79r5E/view?usp=sharing To reproduce the error, start up Jupyter with a terminal and run all cells. The error will show up in the terminal. Note: In the source code, I used numpy to generate random training data because the dataset is private and can't be shared at this moment.  Relevant log output ","> israyelyan In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you! I mentioned the architecture of the model. Those warnings can also be seen in https://www.tensorflow.org/guide/migrate/evaluator . So the warning doesn't come from only my architecture. Further information on details of the code I cannot provide. ",israyelyan I tried to replicate the issue using latest TF versions but colab is crashing here. Could you please find the attached gist and confirm the same? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,israyelyan  Sorry that I didn't confirmed it with limited resources on colab. I've modified the code. Please use this notebook. The issue can be replicated with TF 2.9 running on CPUs. You can see the error message in runtime logs. !image,israyelyan  This issue seems to be Keras issue.  Please post this issue on kerasteam/keras repo. as Keras development is fully moving to github.com/kerasteam/keras. All issues and PRs related to keras will be addressed in that repo. To know more see this TF forum discussion ;  https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,  Ok! I've posted the issue on Keras repo. Link Thank you.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,The Keras guys wrote in https://github.com/kerasteam/tfkeras/issues/103 that it looks like a TF issue and that we should report it here. israyelyan Could you please reopen the issue?,It is still not clear from what part is the issue :/ ,"israyelyan No, it is not, but the Keras guys asked us to open an issue back on Tensorflow repo, which you probably can do (since you are the creator of this issue).",Cannot reopen from my side will open a new issue,"Met similar problem on TF 2.9/2.10 20221005 14:05:12.123396: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_BOOL     }   } }  is neither a subtype nor a supertype of the combined inputs preceding it: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_LEGACY_VARIANT     }   } }         while inferring type of node 'while_3/body/_5262/while_3/while/body/_16348/while_3/while/cond_1/output/_22035'","Met similar problem on TF 2.9.2 [1,2]:20221021 06:40:24.709289: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: [1,2]:type_id: TFT_OPTIONAL [1,2]:args { [1,2]:  type_id: TFT_PRODUCT [1,2]:  args { [1,2]:    type_id: TFT_TENSOR [1,2]:    args { [1,2]:      type_id: TFT_INT32 [1,2]:    } [1,2]:  } [1,2]:} [1,2]: is neither a subtype nor a supertype of the combined inputs preceding it: [1,2]:type_id: TFT_OPTIONAL [1,2]:args { [1,2]:  type_id: TFT_PRODUCT [1,2]:  args { [1,2]:    type_id: TFT_TENSOR [1,2]:    args { [1,2]:      type_id: TFT_HALF [1,2]:    } [1,2]:  } [1,2]:} [1,2]: [1,2]:	while inferring type of node 'cond_39/output/_24' [1,0]:20221021 06:40:25.581602: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: [1,0]:type_id: TFT_OPTIONAL [1,0]:args { [1,0]:  type_id: TFT_PRODUCT [1,0]:  args { [1,0]:    type_id: TFT_TENSOR [1,0]:    args { [1,0]:      type_id: TFT_INT32 [1,0]:    } [1,0]:  } [1,0]:} [1,0]: is neither a subtype nor a supertype of the combined inputs preceding it: [1,0]:type_id: TFT_OPTIONAL [1,0]:args { [1,0]:  type_id: TFT_PRODUCT [1,0]:  args { [1,0]:    type_id: TFT_TENSOR [1,0]:    args { [1,0]:      type_id: TFT_HALF [1,0]:    } [1,0]:  } [1,0]:} [1,0]: [1,0]:	while inferring type of node 'cond_39/output/_24' [1,1]:20221021 06:40:26.918500: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: [1,1]:type_id: TFT_OPTIONAL [1,1]:args { [1,1]:  type_id: TFT_PRODUCT [1,1]:  args { [1,1]:    type_id: TFT_TENSOR [1,1]:    args { [1,1]:      type_id: TFT_HALF [1,1]:    } [1,1]:  } [1,1]:} [1,1]: is neither a subtype nor a supertype of the combined inputs preceding it: [1,1]:type_id: TFT_OPTIONAL [1,1]:args { [1,1]:  type_id: TFT_PRODUCT [1,1]:  args { [1,1]:    type_id: TFT_TENSOR [1,1]:    args { [1,1]:      type_id: TFT_INT32 [1,1]:    } [1,1]:  } [1,1]:} [1,1]: [1,1]:	while inferring type of node 'cond_39/output/_23' [1,0]:[2cf69587bce7:07009] Read 1, expected 7865, errno = 1 [1,2]:[2cf69587bce7:07011] Read 1, expected 9529, errno = 1 [1,1]:[2cf69587bce7:07010] Read 1, expected 9529, errno = 1 [1,3]:[2cf69587bce7:07012] Read 1, expected 9529, errno = 1 [1,3]:20221021 06:40:27.716307: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: [1,3]:type_id: TFT_OPTIONAL [1,3]:args { [1,3]:  type_id: TFT_PRODUCT [1,3]:  args { [1,3]:    type_id: TFT_TENSOR [1,3]:    args { [1,3]:      type_id: TFT_LEGACY_VARIANT [1,3]:    } [1,3]:  } [1,3]:} [1,3]: is neither a subtype nor a supertype of the combined inputs preceding it: [1,3]:type_id: TFT_OPTIONAL [1,3]:args { [1,3]:  type_id: TFT_PRODUCT [1,3]:  args { [1,3]:    type_id: TFT_TENSOR [1,3]:    args { [1,3]:      type_id: TFT_HALF [1,3]:    } [1,3]:  } [1,3]:} [1,3]: [1,3]:	while inferring type of node 'cond_39/output/_19' [1,0]:[2cf69587bce7:07009] Read 1, expected 7921, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 18841, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 16417, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 16441, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 7953, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 16233, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 15193, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 21305, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 12473, errno = 1 [1,2]:[2cf69587bce7:07011] Read 1, expected 11097, errno = 1 [1,1]:[2cf69587bce7:07010] Read 1, expected 11097, errno = 1 [1,3]:[2cf69587bce7:07012] Read 1, expected 11097, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 32257, errno = 1 [1,0]:[2cf69587bce7:07009] Read 1, expected 16441, errno = 1 [1,2]:[2cf69587bce7:07011] Read 1, expected 60177, errno = 1 [1,3]:[2cf69587bce7:07012] Read 1, expected 60177, errno = 1 [1,1]:[2cf69587bce7:07010] Read 1, expected 60177, errno = 1",Could you please refer this comment and let us know if that helps you. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"The following is my trace of error when using tf2.11 in M1 Mac: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_INT32     }   } }  is neither a subtype nor a supertype of the combined inputs preceding it: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_FLOAT     }   } } 	while inferring type of node 'cond_18/output/_21'",Can anyone gives a hint?,"I'm getting something very similar in TF 2.11, on Mac M1.  Can anyone reopen?","I am getting exactly the same warning as  and , and I am running my code on an Ubuntu 22.04 server, i.e., the issue is definitely not limited to (M1) Macs. Interestingly, the warning is only displayed if I have a masking layer with the generator of my GAN. The Masking layer within the discriminator does not produce such a warning. In the generator, as soon as I comment out the Masking layer, the warning disappears. The warning message: ",I'm also seeing this warning when trying to train a custom image segmentation model with pixellib,The same warning always occur. I am training a model made of LSTMs with masks and with one dense layer before the output.  I use miniconda. Python 3.8 TF 2.11 cudatoolkit 11.1.1 ,Any update? ,"**I have solved a similar error in my code and here's how I did it.** TF=2.16.2 keras = 3.4.1 python 3.11.5 I think the problem lies when using .function or using any function with a condition while running tf graph. In my case during model.fit method. problem indicates that invalid graph escaped type checking. when using ifelse statement in .function code keras API converts ifelse conditions into tf.cond ([AutoGraph converts ifstatement to tf.cond().][1]) however, during model.fit() tensorflow gives a warning when using **""elif""** but if you want to avoid that error remove ""elif"" statements with normal ifelse statements and I think that might solve this problem.  Implimentation of function before error and it was used in loss function which was used in mode.compile and later model.fit method     import tensorflow as tf     class RescaleImage():         def __init__(self) > None:             super().__init__()         .function         def normalize(self, x:tf.Tensor, min_val: float=0.0, max_val: float=1.0)>tf.Tensor:             min_val = tf.cast(min_val,tf.float32)             max_val = tf.cast(max_val, tf.float32)             if tf.reduce_max(x)>1.0 and tf.reduce_min(x)>=0.0:                 if min_val==0.0 and max_val==1.0:                     x = x/255.0                 elif min_val==1.0 and max_val==1.0:                     x = (x  127.5)/127.5             elif tf.reduce_max(x)=1.0 and tf.reduce_min(x)=0.0:                 if min_val==1.0 and max_val==1.0:                     x = (x0.5)/0.5                 elif min_val==0.0 and max_val==255.0:                     x = x*255.0             return x         .function         def normalize_individual(self, x:tf.Tensor, min_val: float=0.0, max_val: float=1.0)>tf.Tensor:             min_val = tf.cast(min_val,tf.float32)             max_val = tf.cast(max_val, tf.float32)             if tf.reduce_max(x)>1.0 and tf.reduce_min(x)>=0.0:                 factor = (max_valmin_val)/(tf.math.reduce_max(x)tf.math.reduce_min(x))                 x = factor*(x  tf.math.reduce_min(x))+min_val             elif tf.reduce_max(x)=1.0 and tf.reduce_min(x)=0.0:                 if min_val==1.0 and max_val==1.0:                     x = (x0.5)/0.5                 elif min_val==0.0 and max_val==255.0:                     x = x*255.0             return x  code after solving the error (using normal if statements)     import tensorflow as tf     class RescaleImage():         def __init__(self) > None:             super().__init__()         .function         def normalize(self, x:tf.Tensor, min_val: float=0.0, max_val: float=1.0)>tf.Tensor:             min_val = tf.cast(min_val,tf.float32)             max_val = tf.cast(max_val, tf.float32)             if tf.reduce_max(x)>1.0 and tf.reduce_min(x)>=0.0:                 if min_val==0.0 and max_val==1.0:                     x = x/255.0                 if min_val==1.0 and max_val==1.0:                     x = (x  127.5)/127.5             if tf.reduce_max(x)=1.0 and tf.reduce_min(x)=0.0:                 if min_val==1.0 and max_val==1.0:                     x = (x0.5)/0.5                 if min_val==0.0 and max_val==255.0:                     x = x*255.0             return x         .function         def normalize_individual(self, x:tf.Tensor, min_val: float=0.0, max_val: float=1.0)>tf.Tensor:             min_val = tf.cast(min_val,tf.float32)             max_val = tf.cast(max_val, tf.float32)             if tf.reduce_max(x)>1.0 and tf.reduce_min(x)>=0.0:                 factor = (max_valmin_val)/(tf.math.reduce_max(x)tf.math.reduce_min(x))                 x = factor*(x  tf.math.reduce_min(x))+min_val             if tf.reduce_max(x)=1.0 and tf.reduce_min(x)=0.0:                 if min_val==1.0 and max_val==1.0:                     x = (x0.5)/0.5                 if min_val==0.0 and max_val==255.0:                     x = x*255.0             return x   [1]: https://www.tensorflow.org/api_docs/python/tf/cond:~:text=Note%3A%20This,numpy()%0A4"
369,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Propose file change)ï¼Œ å†…å®¹æ˜¯ (I am looking to start contributing to OSS on GitHub and trying it out first with some simple grammar fixes. Also, I signed the Contributor License Agreement (CLA).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,NoelBram,Propose file change,"I am looking to start contributing to OSS on GitHub and trying it out first with some simple grammar fixes. Also, I signed the Contributor License Agreement (CLA).",2022-08-09T05:12:09Z,size:S,closed,0,2,https://github.com/tensorflow/tensorflow/issues/57048,Hi  Can you please review this PR ? Thank you!,+1 I'm not sure I get it either.
1125,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Shifted/reordered YOLO output layers when converting with TfLite 2.8 or higher)ï¼Œ å†…å®¹æ˜¯ (System info: Ubuntu 22.04 LTS Python: Python 3.10.4 Tensorflow Lite: 2.8 and higher Our YOLO network has three output layers. The (correct) output from Keras (on the .h5), TfLite2.5.0, and my companyâ€™s custommade inference engine is:  Converting a YOLO network from .h5 to to .tflite with TfLite2.8 or higher, and then perform inference incorrectly outputs:  As if the output layers have shifted circularly or have reordered. Workaround: `converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file("".h5"")` Still, I consider downgrading TfLite or use `tf.compat.v1.lite` not a futureproof fix. Python code is below. I cannot share the network file, because itâ€™s company IP.  Similar issues: https://github.com/tensorflow/tensorflow/issues/51858 https://github.com/tensorflow/tensorflow/issues/47927 https://github.com/tensorflow/tensorflow/issues/33303)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,misterBart,Shifted/reordered YOLO output layers when converting with TfLite 2.8 or higher,"System info: Ubuntu 22.04 LTS Python: Python 3.10.4 Tensorflow Lite: 2.8 and higher Our YOLO network has three output layers. The (correct) output from Keras (on the .h5), TfLite2.5.0, and my companyâ€™s custommade inference engine is:  Converting a YOLO network from .h5 to to .tflite with TfLite2.8 or higher, and then perform inference incorrectly outputs:  As if the output layers have shifted circularly or have reordered. Workaround: `converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file("".h5"")` Still, I consider downgrading TfLite or use `tf.compat.v1.lite` not a futureproof fix. Python code is below. I cannot share the network file, because itâ€™s company IP.  Similar issues: https://github.com/tensorflow/tensorflow/issues/51858 https://github.com/tensorflow/tensorflow/issues/47927 https://github.com/tensorflow/tensorflow/issues/33303",2022-08-08T14:44:27Z,stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/57043,Hi  ! Thanks for bring it up .   !  Could you look at these issues.  Thank you!,Could you please try specifying the output signature in your keras saved model as mentioned in the issues which you have mentioned.,"With `tf.lite.TFLiteConverter.from_keras_model(keras_model)`: `'shape_signature': array([1, 16, 16, 18], dtype=int32), 'shape_signature': array([1,  4,  4, 18], dtype=int32), 'shape_signature': array([1,  8,  8, 18]` The full output details: `[{'name': 'StatefulPartitionedCall:0', 'index': 332, 'shape': array([ 1, 16, 16, 18], dtype=int32), 'shape_signature': array([1, 16, 16, 18], dtype=int32), 'dtype': , 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:1', 'index': 298, 'shape': array([ 1,  4,  4, 18], dtype=int32), 'shape_signature': array([1,  4,  4, 18], dtype=int32), 'dtype': , 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 315, 'shape': array([ 1,  8,  8, 18], dtype=int32), 'shape_signature': array([1,  8,  8, 18], dtype=int32), 'dtype': , 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]` With `tf.compat.v1.lite.TFLiteConverter.from_keras_model_file("".h5"")`: `'shape_signature': array([1,  4,  4, 18], dtype=int32), 'shape_signature': array([1,  8,  8, 18], dtype=int32), 'shape_signature': array([1, 16, 16, 18], dtype=int32)` The full output details: `[{'name': 'Identity', 'index': 298, 'shape': array([ 1,  4,  4, 18], dtype=int32), 'shape_signature': array([1,  4,  4, 18], dtype=int32), 'dtype': , 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 315, 'shape': array([ 1,  8,  8, 18], dtype=int32), 'shape_signature': array([1,  8,  8, 18], dtype=int32), 'dtype': , 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 332, 'shape': array([ 1, 16, 16, 18], dtype=int32), 'shape_signature': array([1, 16, 16, 18], dtype=int32), 'dtype': , 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]` You can see that both versions have: tensor_index=298 > shape=[1,4,4,18] tensor_index=315 > shape=[1,8,8,18] tensor_index=332 > shape=[1,16,16,18] Yet `tf.lite.TFLiteConverter.from_keras_model` stores them in the order [tensor_index=332, tensor_index=298, tensor_index=315], in contrast to `tf.compat.v1.lite.TFLiteConverter.from_keras_model_file` storing them as [tensor_index=298, tensor_index=315, tensor_index=332]","Based on my last reply, a workaround could be:  I'm not very fond of this workaround though, the code is cleaner and more efficient if the output order is correct in the .tflite.","Hmm, more than a week has passed. I believe my post is easy to follow. Several related issues exist about this reordered/shifted output layers, so I would deem it has some importance. Is everybody on holidays?","From many tries, I can understand the output order is related with the layer name. So my workaround is,  1. Set A B C to names of yolo three output layers. 2. Convert model. 3. Check real output order, then adjust the names to fit my expected order. (In my case , the final names order is C A B for output0, output1, output2) 4. Reconvert model with new names","Hi  , Could you please try using `ExportArchive` https://www.tensorflow.org/api_docs/python/tf/keras/export/ExportArchive which could solve the issue of retain output name in the Keras model. The other way to make it work is to convert the output value from `tf.Tensor` to `tf.keras.layers` type using `identity` layer, which will retain the output names in the converter model. Below is the sample example.  ",More than eight months have passed since my last post. Meanwhile I had to move on. We currently use tf.compat.v1.lite.TFLiteConverter. When that solution is no longer supported I am allowed to look into this issue again.,Are you satisfied with the resolution of your issue? Yes No
646,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot compile Hexagon delegate properly)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Android NDK r21e  Mobile device Android 10  Python version 3.9  Bazel version _No response_  GCC/Compiler version clang 9.0.9  CUDA/cuDNN version _No response_  GPU model and memory QQcom Adreno 650  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,eelcoder,Cannot compile Hexagon delegate properly,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Android NDK r21e  Mobile device Android 10  Python version 3.9  Bazel version _No response_  GCC/Compiler version clang 9.0.9  CUDA/cuDNN version _No response_  GPU model and memory QQcom Adreno 650  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-07T22:00:28Z,type:build/install comp:lite TFLiteHexagonDelegate TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57038,Hi ! Could you provide the steps followed too. Thank you!,"Hello, Finally, I found the issue, it was due to the fact that ops.def was not found and because of that was not able to generate the enum type during the compilation. I think we can consider it as fixed. Thanks!", ! Thanks for the update. Feel free to close this issue if it is resolved from your end. Thank you!,ok,Are you satisfied with the resolution of your issue? Yes No
1876,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reducing Ops in Hexagon Delegate Use Wrong Axis Index When Input Tensor <4D)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10  Bazel version 5.0.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? If there is a meanreducing op that is executed by the Hexagon Delegate, the axis indices are unchanged from the original inputs.  This causes a problem if the tensor being reduced is of rank  [1, 48, 768] Axis = 2 Output tensor shape > [1, 48, 1] but in the hexagon delegate we pad the dimensions to make it 4D so we get: Input tensor shape > [1, 1, 48, 768] Axis = 2 Output tensor shape > [1, 1, 1, 768] I've started on a fix in https://github.com/tensorflow/tensorflow/pull/57021 but I need some help with it  this approach creates a new tensor from the original axes tensor, increments the value by the number of dummy dimensions, and then tries to add this new tensor instead of the original. This runs (as long as the axes_tensor is a list, not a scalar), but it leaks memory, and there is an error relating to trying to add a duplicate tensor (`ERROR: Trying to add duplicate tensor without overwrite, tflite_tensor_id 1, hexagon_node_id 16, hexagon_node_output_id 0`).  I can't modify the original axes tensor directly because it has a readonly allocation type, and trying causes a segmentation fault.  Standalone code to reproduce the issue 1. Create dummy tflite model using https://colab.research.google.com/drive/1cyjF6dx31TNxmPgFwOdAXPQcUYD7685?usp=sharing 2. Run with `./benchmark_model_plus_f)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tc-wolf,Reducing Ops in Hexagon Delegate Use Wrong Axis Index When Input Tensor <4D,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04 LTS  Mobile device _No response_  Python version 3.10  Bazel version 5.0.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? If there is a meanreducing op that is executed by the Hexagon Delegate, the axis indices are unchanged from the original inputs.  This causes a problem if the tensor being reduced is of rank  [1, 48, 768] Axis = 2 Output tensor shape > [1, 48, 1] but in the hexagon delegate we pad the dimensions to make it 4D so we get: Input tensor shape > [1, 1, 48, 768] Axis = 2 Output tensor shape > [1, 1, 1, 768] I've started on a fix in https://github.com/tensorflow/tensorflow/pull/57021 but I need some help with it  this approach creates a new tensor from the original axes tensor, increments the value by the number of dummy dimensions, and then tries to add this new tensor instead of the original. This runs (as long as the axes_tensor is a list, not a scalar), but it leaks memory, and there is an error relating to trying to add a duplicate tensor (`ERROR: Trying to add duplicate tensor without overwrite, tflite_tensor_id 1, hexagon_node_id 16, hexagon_node_output_id 0`).  I can't modify the original axes tensor directly because it has a readonly allocation type, and trying causes a segmentation fault.  Standalone code to reproduce the issue 1. Create dummy tflite model using https://colab.research.google.com/drive/1cyjF6dx31TNxmPgFwOdAXPQcUYD7685?usp=sharing 2. Run with `./benchmark_model_plus_f",2022-08-06T04:06:13Z,stat:awaiting response type:bug comp:lite TFLiteHexagonDelegate TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57026,Hi wolf  I'm having 70c50e734eac50e67317614413da04ea53acd528 to fix the axes issue when input dimension is less than 4. Let me know if it works for you. Thanks!, That looks perfect!  I tested with the rminimum reprex and that works.  I'll test with a larger model and then should be good to close.,"Thanks for confirming that the issue is resolved with the above fix, please use. tfnightly version to use the above fix, till it becomes part of upcoming stable release. Feel free to close the issue once you test with the larger model. Thanks!",Confirmed that this works with the nontoyexample / larger model!,Are you satisfied with the resolution of your issue? Yes No
838,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.10 cherry-pick: Update oneDNN version for aarch64, increase aarch64 test coverage)ï¼Œ å†…å®¹æ˜¯ (oneDNN version update is necessary for performance and bug fixes. This cherrypick combines commits from several PRs, because all of them modify cpu_arm64_pip.sh:  * https://github.com/tensorflow/tensorflow/pull/56933 * https://github.com/tensorflow/tensorflow/pull/56811 * https://github.com/tensorflow/tensorflow/pull/56972 * https://github.com/tensorflow/tensorflow/pull/56975 * https://github.com/tensorflow/tensorflow/pull/57009 If we have separate cherrypick PRs, we will need to keep track of the PR merge order, so it's better to just combine them into one PR.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,penpornk,"r2.10 cherry-pick: Update oneDNN version for aarch64, increase aarch64 test coverage","oneDNN version update is necessary for performance and bug fixes. This cherrypick combines commits from several PRs, because all of them modify cpu_arm64_pip.sh:  * https://github.com/tensorflow/tensorflow/pull/56933 * https://github.com/tensorflow/tensorflow/pull/56811 * https://github.com/tensorflow/tensorflow/pull/56972 * https://github.com/tensorflow/tensorflow/pull/56975 * https://github.com/tensorflow/tensorflow/pull/57009 If we have separate cherrypick PRs, we will need to keep track of the PR merge order, so it's better to just combine them into one PR.",2022-08-05T23:16:46Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/57023
692,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Attempted workaround for reduce_mean with Hexagon delegates)ï¼Œ å†…å®¹æ˜¯ (Attempted workaround for reduce_mean  bump the index for axes reducing over by the number of 'padding' dimensions of size one we add when the input tensor is rank  [1, 48, 768] Axis = 2 Output tensor shape > [1, 48, 1] but in the hexagon delegate we pad the dimensions to make it 4D for nnlib, so we have: Input tensor shape > [1, 1, 48, 768] Axis = 2 Output tensor shape > [1, 1, 1, 768] and so get messages like:  from nnlib when trying to run.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tc-wolf,Attempted workaround for reduce_mean with Hexagon delegates,"Attempted workaround for reduce_mean  bump the index for axes reducing over by the number of 'padding' dimensions of size one we add when the input tensor is rank  [1, 48, 768] Axis = 2 Output tensor shape > [1, 48, 1] but in the hexagon delegate we pad the dimensions to make it 4D for nnlib, so we have: Input tensor shape > [1, 1, 48, 768] Axis = 2 Output tensor shape > [1, 1, 1, 768] and so get messages like:  from nnlib when trying to run.",2022-08-05T21:47:07Z,comp:lite size:M,closed,0,5,https://github.com/tensorflow/tensorflow/issues/57021,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",wolf Can you please resolve conflicts? Thank you!,"  No problem, I rebased off of master and AFAICT there shouldn't be merge conflicts with master!  Is that what we want, or should I rebase vs. the `v2.9.1` tag?",I think a better approach would be to add a reshape op like we do in the builder for `Rsqrt`.  I've tested and manually reshaping to rank4 inputs *does* work as a workaround.,Looks like fixed by 70c50e734eac50e67317614413da04ea53acd528
664,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problems objection detection api on MacBook M1)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution MacOs M1  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sepi83,Problems objection detection api on MacBook M1,Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution MacOs M1  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-05T07:06:10Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/57016,"Hi , Could you share the steps that you have followed to install Tensorflow Object detection api on MAC. Follow the steps mentioned in this link. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"I am also having this problem. It's not possible to install tensorflowio on Macbook M1. Running tensorflowmacos 2.9.2.  Option 1: pip install tensorflowio ERROR: Could not find a version that satisfies the requirement tensorflowio (from versions: none) ERROR: No matching distribution found for tensorflowio Option 2: cp object_detection/packages/tf2/setup.py . pip install usefeature=2020resolver . ERROR: Ignored the following versions that require a different python version: 1.6.2 RequiresPython >=3.7,=3.7,=3.7,=3.7,=3.7,=3.7,=2.7,=2.7,=2.7,=2.7,=2.7,=2.7,=2.7,=2.7,<3.0 ERROR: Could not find a version that satisfies the requirement tensorflow_io (from objectdetection) (from versions: none) ERROR: No matching distribution found for tensorflow_io Option 3: python setup.py build  sudo python setup.py install  error: Could not find suitable distribution for Requirement.parse('tensorflow_io')",Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"HI , Could you create new issue by providing all the required details. Thank you!"
1039,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Runtime Error: Node number 93 (CONCATENATION) failed to prepare)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18 LTS  TensorFlow installation (pip package or built from source): TF 2.9 installed using pip  TensorFlow library (version, if pip package or github SHA, if built from source): TF 2.9 installed using pip  2. Code   3.Error:  The conversion fails with an error:   The model is a Resnet and upscale image to image network originally saved from PyTorch. I have read in similar issues that there might be a problem with the way that PyTorch models have a dynamic input dimension which has to be fixed during conversion, but I am not sure on how to diagnose this any further than that. Does anyone have an idea on how to solve the issue of the dimension mismatch? Thank you very much for any help on this.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Alexanderbayer,Runtime Error: Node number 93 (CONCATENATION) failed to prepare," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18 LTS  TensorFlow installation (pip package or built from source): TF 2.9 installed using pip  TensorFlow library (version, if pip package or github SHA, if built from source): TF 2.9 installed using pip  2. Code   3.Error:  The conversion fails with an error:   The model is a Resnet and upscale image to image network originally saved from PyTorch. I have read in similar issues that there might be a problem with the way that PyTorch models have a dynamic input dimension which has to be fixed during conversion, but I am not sure on how to diagnose this any further than that. Does anyone have an idea on how to solve the issue of the dimension mismatch? Thank you very much for any help on this.",2022-08-04T07:21:34Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/57006,"Hi  ! It seems to be multi output  model and You can arrange the ascertain the sequence of input and output by implementing signatures in model itself. Can you check with below code and let us know.  Ref 1, 2 Thank you!","Hi  , Than you so much for your fast response! I had the chance to try converting it with your code now, but unfortunately still the same error occurs when converting the model. Also I have implemented signatures in the model and I printed the signatures using the following code (after conversion without quantization):  ` interpreter = tf.lite.Interpreter(model_content=tflite_model_quant) signatures = interpreter.get_signature_list() print(signatures) ` The model I am trying to convert is a ResNet18 encoder with skip connections to form a UNet with this depth decoder: https://github.com/nianticlabs/monodepth2/blob/master/networks/depth_decoder.py So it should only be single input and single output. I am not sure however about whether is the ordering of the dimensions is correct, as mentioned in the reference you provided. Here you can find the onnx model, the tensorflow model, as well as the unquantized tensorflow lite model: https://drive.google.com/file/d/19l4gARzdYTbckeiGAiCWc1GnIjY72n/view?usp=sharing Best reagards, Alex",Are you satisfied with the resolution of your issue? Yes No,Have you tried with float conversion instead of int8 conversion to see if it is failing on both the cases.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"> Have you tried with float conversion instead of int8 conversion to see if it is failing on both the cases. Yes, I have tried that, and this works as there is no inference on sample images necessary for conversion without quantization.","This could also be due to the different batch size during training and inference time, could you please check it and make sure you are providing the same batch size. During training, if you have not provided any batch size, converter will assume batch size as 1 and same has to be provided during inference. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No
1349,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tflite.interpreter problem its not opening the model in raspberry pi 4)ï¼Œ å†…å®¹æ˜¯ ( Hi guys, I am trying to implement pose estimation with TFlite in raspberry pi so its working in my desktop but when I tried to run it in my raspberry pi its not working. this is my original libraries: matplotlib==3.5.2 numpy==1.19.5 opencv_python==4.6.0.66 setuptools==57.0.0 tensorflow==2.4.1 when I tried to install my dependencies and libraries using requirements, its not accepting the tensorflow 2.4.1 so I downloaded 2.8 instead . my raspberry pi: bullseye 11 python 3.9 64bits this is my error: File ""/home/pi/pose1/main.py"", line 7, in      interpreter = tf.lite.Interpreter(model_path=""thunder.tflite "")   File ""/home/pi/pose1/pose11env/lib/python3.9/sitepackages/tensorflow/lite/python/interpreter.py"", line 456, in __init__     _interpreter_wrapper.CreateWrapperFromFile( ValueError: Could not open 'thunder.tflite '. this is the code for my interpreter: interpreter = tf.lite.Interpreter(model_path=""thunder.tflite "") interpreter.allocate_tensors() the model is also in my directory , again its working in my desktop but not in my raspberry pi.  please help thank you so much!!!!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Nikoed,tflite.interpreter problem its not opening the model in raspberry pi 4," Hi guys, I am trying to implement pose estimation with TFlite in raspberry pi so its working in my desktop but when I tried to run it in my raspberry pi its not working. this is my original libraries: matplotlib==3.5.2 numpy==1.19.5 opencv_python==4.6.0.66 setuptools==57.0.0 tensorflow==2.4.1 when I tried to install my dependencies and libraries using requirements, its not accepting the tensorflow 2.4.1 so I downloaded 2.8 instead . my raspberry pi: bullseye 11 python 3.9 64bits this is my error: File ""/home/pi/pose1/main.py"", line 7, in      interpreter = tf.lite.Interpreter(model_path=""thunder.tflite "")   File ""/home/pi/pose1/pose11env/lib/python3.9/sitepackages/tensorflow/lite/python/interpreter.py"", line 456, in __init__     _interpreter_wrapper.CreateWrapperFromFile( ValueError: Could not open 'thunder.tflite '. this is the code for my interpreter: interpreter = tf.lite.Interpreter(model_path=""thunder.tflite "") interpreter.allocate_tensors() the model is also in my directory , again its working in my desktop but not in my raspberry pi.  please help thank you so much!!!!",2022-08-03T11:15:08Z,stat:awaiting response type:support stale comp:micro TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56999,"Hi  ! Can you put the full path of Tflite model and let us know the results. If it still persists , Please change the name of TFlite file too. Ref Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1444,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MirroredStrategy does not use GPUs simultaneously)ï¼Œ å†…å®¹æ˜¯ ( Issue Type Performance  Source binary  Tensorflow Version 2.6,2.7,2.8,2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory DGX1 V100   Current Behaviour? Hi, I have observed some bad performance when trying to run single machine / multiGPU jobs. I have reported a minimum example below. When looking at the GPU performance (with NVIDIA NSight System), I can see that the GPUs run almost sequentially, i.e., GPU0 will do some work then when it is almost done, GPU1 will start.  !image I have executed a similar version of the code on PyTorch and it exhibits the expected behaviour, i.e., the GPUs run in parallel. !image This issue on TF dramatically slows down model training. Can someone help me with the issue? Am I not using MirroredStrategy correctly? I used `.function` to disable the eager execution of the graphs. I am posting the code below and to remove any impact of data loading, each GPU creates their own tensor to work on. Thanks!  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nicolas-mng,MirroredStrategy does not use GPUs simultaneously," Issue Type Performance  Source binary  Tensorflow Version 2.6,2.7,2.8,2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory DGX1 V100   Current Behaviour? Hi, I have observed some bad performance when trying to run single machine / multiGPU jobs. I have reported a minimum example below. When looking at the GPU performance (with NVIDIA NSight System), I can see that the GPUs run almost sequentially, i.e., GPU0 will do some work then when it is almost done, GPU1 will start.  !image I have executed a similar version of the code on PyTorch and it exhibits the expected behaviour, i.e., the GPUs run in parallel. !image This issue on TF dramatically slows down model training. Can someone help me with the issue? Am I not using MirroredStrategy correctly? I used `.function` to disable the eager execution of the graphs. I am posting the code below and to remove any impact of data loading, each GPU creates their own tensor to work on. Thanks!  Standalone code to reproduce the issue   Relevant log output _No response_",2022-08-02T16:55:42Z,comp:dist-strat type:performance TF 2.9,open,2,2,https://github.com/tensorflow/tensorflow/issues/56990,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.","The issue can be closed, thanks! I switched my workflow to PyTorch. "
664,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite convert BatchMatMulV2 into Split+multi FullyConnected + Pack)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ub18.04  TensorFlow installation (pip package or built from source):tf2.9 When I convert a transformer model with BatchMatMulV2 into tflite,  the first input shape of BatchMatMulV2 is [40x1x64],the second input shape   is [64x64], the BatchMatMulV2 is converted into Split+multi FullyConnected + Pack in the tflite model.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,luchangli03,TensorFlow Lite convert BatchMatMulV2 into Split+multi FullyConnected + Pack," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ub18.04  TensorFlow installation (pip package or built from source):tf2.9 When I convert a transformer model with BatchMatMulV2 into tflite,  the first input shape of BatchMatMulV2 is [40x1x64],the second input shape   is [64x64], the BatchMatMulV2 is converted into Split+multi FullyConnected + Pack in the tflite model.",2022-08-02T09:34:12Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56984,"Hi  ! TFliteconverter converts model operation into simple supported operations that can be run in memory constrained devices like Android and micro controllers. If you are facing any issue or want to disable quantisation for any node , you can refer QuantizationDebugger. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
689,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite gpu delegate incorrect fusion for layernorm reduce+elemwise)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,luchangli03,TensorFlow Lite gpu delegate incorrect fusion for layernorm reduce+elemwise,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-08-02T09:01:05Z,stat:awaiting response type:bug stale comp:lite TFLiteGpuDelegate TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56983,Hi  ! Could you share a minimal stand alone code too. Thank you!, ! GPU delegates are meant to break complex operations to simple operations for faster processing. Could you confirm performance difference through benchmarking. Please post the  lite model  along the transformer model  used for further assistance.  Thank you!,"  Since I can not upload binary file, you can simply save the txt format pbtxt model and convert it to pb model and finally convert it to tflite model with fp32 inference. The model is extracted from BERT layernorm for convenient testing. tflite will automaticly fuse reduce+a sequence of elemwise nodes, however, the elemwise nodes must not contiain shape broadcast that dst shape larger than reduce output shape. However, it happens in layernorm but tflite gpu delegate does not check it. For example, for input shape [512,768], the reduce output shape is [512,1], there is a mul that broadcast the shape again to [512,768], tflite fuse the reduce with the mul, which results in significantly performance degradation and incorrect result. The reason is tflite gpu delege simply use output shape to calculate the grid size of fused op.", please download the pbtxt model from https://github.com/luchangli03/test_models/blob/main/sub_layernorm.pbtxt then you can simply convert it into binary pb and tflite with fp32 inference., !  Thanks for the update.   Could you look at this issue. Thank you!,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
633,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Could not load dynamic library 'cudart64_110.dll')ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,FenrirVonDerNebelungs,Could not load dynamic library 'cudart64_110.dll',Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-02T00:52:10Z,type:build/install subtype:windows TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56976,", There are at least 3 possible scenarios:  Also `pip install tensorflow` will install a version thats compatible with `GPU` and `CPU`. So it gives you that warning message.  If you don't want to see warning messages and want to install the `CPU` only version, you could  `pip install tensorflowcpu` that's a smaller wheel file for `CPU` only version.  Thank you!","Seems to be working however had to use Conda. In all the awful detail.... _Installed the MSVC 2019(etc) redistributable. This initially did not make a difference. Following the instructions: https://www.tensorflow.org/install/pipwindows_1 installed Miniconda with the default options rebooted Opened the newly installed Anaconda Command prompt_ >conda create name tf python=3.9 _answered yes where necessary..._ >conda deactivate >conda activate tf >conda install c condaforge cudatoolkit=11.2 cudnn=8.1.0 >y >pip install upgrade pip  **Requirement already satisfied: pip in c:\programdata\miniconda3\envs\tf\lib\sitepackages (22.1.2) Collecting pip   Downloading pip22.2.2py3noneany.whl (2.0 MB)       2.0/2.0 MB 10.0 MB/s eta 0:00:00 ERROR: To modify pip, please run the following command: C:\ProgramData\Miniconda3\envs\tf\python.exe m pip install upgrade pip** > _Error message did not seem to have an effect_ _tried uninstalling tensorflow_ >pip uninstall tensorflow _Appearently tensorflow was not installed_ >pip install tensorflow ok... _Tried the recommended test command_ >python3 c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"" _This generate the previous ""Could not load dynamic library 'cudart64_110.dll'..."" error message_ >conda deactivate  _closed Anaconda prompt opened Anaconda Prompt_ >conda activate tf >python **Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.** >>>import tensorflow as tf >>>print(tf.reduce_sum(tf.random.normal([1000, 1000]))) **20220804 18:39:10.303527: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220804 18:39:10.694257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3995 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5 tf.Tensor(954.0616, shape=(), dtype=float32)** _This seems to be working fine_",Are you satisfied with the resolution of your issue? Yes No
739,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.lookup.StaticHashTable with TextFileInitializer uses a cached version of the file even after the file is changed)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution Ubuntu 18.04  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell 0,0 1,1 2,2 0,1 1,2 2,3 ((,   ),  (,   )) ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rjchee,tf.lookup.StaticHashTable with TextFileInitializer uses a cached version of the file even after the file is changed,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution Ubuntu 18.04  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell 0,0 1,1 2,2 0,1 1,2 2,3 ((,   ),  (,   )) ``` ",2022-08-01T21:32:59Z,stat:awaiting response type:bug stale comp:ops TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56974,"Hi  ! Could you look at this issue . Attached gist in 2.8, 2.9 and nightly for reference. Thank you!","Hi , Looks like it is working as expected. There is no cashing   **Output**   **Output** ","Hmm, that's strange. The colab notebooks  linked also show the issue. Does it depend on the system you're on?","Hi , On Colab if you are executing both the cases in same cell, we can observe cashing, since both are printing from single cell. You should execute it in different cell to and print the tables accordingly. Thank you! "," I observe this behavior even if I use different cells in colab. https://colab.research.google.com/drive/1mJ2rTixQXxaePjkdoEfbRMfNbJjlAFSe?usp=sharing. Also, I don't see why the behavior should be different if they were in the same cell.","Hi , Even if we execute on single cell, there is no cashing,  Code is working as expected, Please find the gist here. Thank you!","It looks like in the gist, you use a different `NamedTemporaryFile` each time. These are different files with different names, so we would not expect any caching. The case I am looking at is if you use the **same** filename for two different `StaticHashTables`. Could you please check that case? ","HI , I am using same file name called `f` in both case. Initialising freshly for second case. Can you verify. Thank you!","`f` is a variable name, not the file name. If you do `f = tempfile.NamedTemporaryFile`, it creates a **new** file with a different name, so writes to it won't affect the first file. I modified your gist so that it prints the names of the files being used. I also added a version which doesn't create a new `NamedTemporaryFile`. Please refer to the last cell which demonstrates the bug.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
725,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot build branch r2.9 on Windows 11)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Windows 11 Enterprise version 21H2 OS build 22000.795  Mobile device _No response_  Python version 3.9.7  Bazel version 5.0.0  GCC/Compiler version Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30146 for x64  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,cidzerda-unity,Cannot build branch r2.9 on Windows 11,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Windows 11 Enterprise version 21H2 OS build 22000.795  Mobile device _No response_  Python version 3.9.7  Bazel version 5.0.0  GCC/Compiler version Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30146 for x64  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-01T21:09:36Z,stat:awaiting response type:build/install,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56973,"Hi unity, Tensorflow build need Developer Mode to be turned on. The command prompt must be â€œRun as Adminâ€ for the bazel build.  Build the TensorFlow C++ DLL with this command: `bazel build tensorflow:tensorflow.dll ` Follow the workaround mentioned on similar issue here. Thank you!",The output is different but it still fails. ,"Hi unity, You need to install `MSYS2`. Install MSYS2 for the bin tools needed to build TensorFlow. Make sure all the prerequired software installed. Kindly follow the instructions mentioned here. Thank you!",We have a different solution now so this is no longer a problem.,Are you satisfied with the resolution of your issue? Yes No
762,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Model Maker Object Detection Tutorial crashes in colab at model.export line (TypeError: EndVector() missing 1 required positional argument: 'vectorNumElems'))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Google Colab  Mobile device _No response_  Python version 3.7.14  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rolfvandam,Model Maker Object Detection Tutorial crashes in colab at model.export line (TypeError: EndVector() missing 1 required positional argument: 'vectorNumElems'),Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Google Colab  Mobile device _No response_  Python version 3.7.14  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-08-01T19:20:13Z,stat:awaiting response type:bug stale comp:lite-examples TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/56970,Hi  ! Thanks for reporting this bug. It is replicating even when a valid path is given .   ! Could you look at this issue . Attached gist in 2.8 for reference. Thank you!,I am currently facing this issue. any solution or alternative?,> I am currently facing this issue. > any solution or alternative? I've switched to a different platform for object detection (YOLOv5 which is pytorch based)., actually I am training a custom object detection model to use it in android application. I have already trained the model but can't export it in tflite format!, ! have you found anything to solve this problem? I need the solution ASAP!,"Hi  ! Please Check this comment and resolved  gist as a workaround . Raised PR  CC(Fixes model export issue in 57386, 56970) too upon this. Thank you!","Hi! Sorry for the delayed response. Will https://github.com/tensorflow/tensorflow/pull/57394 address this issue? If so, I will reassign it to  .",Hi    ! I think this issue has been fixed now (might be because of flatbuffer version upgrade ). I ran the notebook today and did not find any issue with the model saving part. Attached gist for reference. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Thanks,  !  Hi,  , I will close this issue as the PR seemed to fix it, but please feel free to open it again if the issue is still present.",Are you satisfied with the resolution of your issue? Yes No
1034,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Model does not train properly when explicitly applying the gradients)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.9  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np from time import time tf.random.set_seed(0) lb = tf.constant([0,0], dtype=np.float32) ub = tf.constant([1,1], dtype=np.float32) N_0     = 4092 Nepochs = 1500 lr      = 0.001 def fun_u_0(xx):     c_0  = 0.5*(lb+ub)     rr   = 0.25*tf.reduce_min(ublb)         dsq  = tf.math.reduce_sum( (xxc_0)*(xxc_0),axis=1)     return(tf.where( dsq)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cesare-corrado,Model does not train properly when explicitly applying the gradients,"Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.9  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue ```shell import tensorflow as tf import numpy as np from time import time tf.random.set_seed(0) lb = tf.constant([0,0], dtype=np.float32) ub = tf.constant([1,1], dtype=np.float32) N_0     = 4092 Nepochs = 1500 lr      = 0.001 def fun_u_0(xx):     c_0  = 0.5*(lb+ub)     rr   = 0.25*tf.reduce_min(ublb)         dsq  = tf.math.reduce_sum( (xxc_0)*(xxc_0),axis=1)     return(tf.where( dsq",2022-08-01T17:24:45Z,stat:awaiting response type:bug stale comp:model TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56969,I found that expanding the dimension of the targets as follows:  the model training properly and the loss functions are correctly evaluated. I don't know if this is still a bug or just something that needs to be specified in the doc.,corrado ! Thanks for sharing your  fix  . Could we consider this resolved then. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
754,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(build failure on Debian)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version current master  Custom Code No  OS Platform and Distribution Linux Debian Unstable  Mobile device _No response_  Python version 3.10  Bazel version 5.1.1  GCC/Compiler version 9.5  CUDA/cuDNN version 11.4/8.2  GPU model and memory I configured Cuda compute level 6.1 (GTX 1060)  Current Behaviour?   See the Relevant log output below for more of the build log. shell bazel build config=nogcp //tensorflow/tools/pip_package:build_pip_package   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,hans-ekbrand,build failure on Debian,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version current master  Custom Code No  OS Platform and Distribution Linux Debian Unstable  Mobile device _No response_  Python version 3.10  Bazel version 5.1.1  GCC/Compiler version 9.5  CUDA/cuDNN version 11.4/8.2  GPU model and memory I configured Cuda compute level 6.1 (GTX 1060)  Current Behaviour?   See the Relevant log output below for more of the build log. shell bazel build config=nogcp //tensorflow/tools/pip_package:build_pip_package   Relevant log output  ,2022-07-31T21:53:17Z,stat:awaiting response stat:awaiting tensorflower type:build/install stale subtype:bazel,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56963,"ekbrand, Could you please provide the source from where you are trying to install the tensorflow. It helps us to analyse and provide the CUDA, cudNN and the bazel version which are compatible for the respective tensorflow version.  Also please take a look at this comment from the developer for the similar error. Thank you!","> Could you please provide the source from where you are trying to install the tensorflow. It helps us to analyse and provide the CUDA, cudNN and the bazel version which are compatible for the respective tensorflow version. The source I tried to build has this as the last log entry  > Also please take a look at this comment from the developer for the similar error.  Indeed the error looks similar. My question in https://github.com/tensorflow/tensorflow/issues/41490 about which versions of icu is compatible was never answered, and here are the currently installed versions: ","Hi ekbrand, Looks like issue with Python module. Could you try to install python3 module `request`.  `pip3 install requests`. Thank you!","> Hi ekbrand, Looks like issue with Python module. Could you try to install python3 module `request`. > `pip3 install requests`. Thank you! requests is already installed: ","Hi ekbrand , Could you please confirm if this is still an issue for you.  Can you confirm adding the flag `noincompatible_do_not_split_linking_cmdline` to your command helps you which seems resolved for few users as per the source from here. Also please find the tested configurations from here. Please go through the above resources and confirm if this is still having same problem. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1864,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(INVALID_ARGUMENT:  assertion failed: [predictions must be <= 1])ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source binary  Tensorflow Version tf2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.7 / cuDNN 8.4  GPU model and memory Geforce RTX 2080TI  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output ```shell 20220731 22:35:39.354415: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_INT32     }   } }  is neither a subtype nor a supertype of the combined inputs preceding it: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_FLOAT     }   } } 	while inferring type of node 'cond_21/output/_23' 20220731 22:35:39.756638: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100  InvalidArgumentError                      Traceback (most recent call last) Input In [9], in () > 1 history = student_model.fit(dataset = train_set,       2                             epochs = epochs,       3                             verbose = verbose,       4                             validation_data = val_set,       5                             callbacks = [       6                                 tf.keras.callback)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Blackbird95x,INVALID_ARGUMENT:  assertion failed: [predictions must be <= 1],"Click to expand!    Issue Type Others  Source binary  Tensorflow Version tf2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.7 / cuDNN 8.4  GPU model and memory Geforce RTX 2080TI  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output ```shell 20220731 22:35:39.354415: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_INT32     }   } }  is neither a subtype nor a supertype of the combined inputs preceding it: type_id: TFT_OPTIONAL args {   type_id: TFT_PRODUCT   args {     type_id: TFT_TENSOR     args {       type_id: TFT_FLOAT     }   } } 	while inferring type of node 'cond_21/output/_23' 20220731 22:35:39.756638: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100  InvalidArgumentError                      Traceback (most recent call last) Input In [9], in () > 1 history = student_model.fit(dataset = train_set,       2                             epochs = epochs,       3                             verbose = verbose,       4                             validation_data = val_set,       5                             callbacks = [       6                                 tf.keras.callback",2022-07-31T20:42:34Z,stat:awaiting response stale type:others comp:ops TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56962," I was able to replicate the issue, please find the gist here. Thank you!","Hi , `InvalidArgumentError: Graph execution error:` says that you need to increase your batch_size while training model. The data huge to fit the model. Hence use batch_size. Thank you!",Hey   I just tried to train the model with batch_ size = 64 and 128 both with the same result. ,"Hi , While I was trying to replicate issue, I am getting an error `ModuleNotFoundError: No module named 'deep_Knowledge_Tracing'`. Kindly provide more information to reproduce the issue. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
436,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update version numbers for TensorFlow 2.10.0-rc0)ï¼Œ å†…å®¹æ˜¯ (Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,tensorflow-jenkins,Update version numbers for TensorFlow 2.10.0-rc0,"Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: ",2022-07-30T00:54:12Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/56953
385,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update version numbers for TensorFlow 2.10.0-rc0)ï¼Œ å†…å®¹æ˜¯ (Same as CC(Update version numbers for TensorFlow 2.10.0rc0) but with the version change also in `tensorflow/tools/pip_package/setup_partner_builds.py`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,nitins17,Update version numbers for TensorFlow 2.10.0-rc0,Same as CC(Update version numbers for TensorFlow 2.10.0rc0) but with the version change also in `tensorflow/tools/pip_package/setup_partner_builds.py`,2022-07-29T22:46:29Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/56951,Not needed anymore after https://github.com/tensorflow/tensorflow/pull/56953 that has all the changes necessary
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom op's quantized version is not called, custom op's input and output are still floating-point)ï¼Œ å†…å®¹æ˜¯ (_TL;DR_: All the convolution operators are quantized in the TFLite model as expected, but the custom operator is not. I built TensorFlow from source (SHA: e0032f9d64d22e4b6f20102c35045247fe704596) after adding a new custom operator in `tensorflow/core/user_ops/dummy.cc` (see code below). I then created the following model with a few convolutions and the custom op. The goal is to have the whole TFLite model being quantized, as showed in this diagram:  Instead, I'm getting a TFLite model in which all the convolutions are indeed quantized, but the custom op is still floatingpoint and the model includes Dequantize/Quantize operators before/after the custom op. **Are quantized custom ops supported? If so, what's missing here to make sure the quantized version of the custom operator ends up in the TFLite model?**  Python code to build and convert model (three TFLite models will be created: one with no optimization, one with floatingpoint input/outputs with optimizations, and the last one with quantized input/outputs with optimizations, which is the one most relevant to this issue):  C++ code for custom operator in `tensorflow/core/user_ops/dummy.cc`: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hguihot,"Custom op's quantized version is not called, custom op's input and output are still floating-point","_TL;DR_: All the convolution operators are quantized in the TFLite model as expected, but the custom operator is not. I built TensorFlow from source (SHA: e0032f9d64d22e4b6f20102c35045247fe704596) after adding a new custom operator in `tensorflow/core/user_ops/dummy.cc` (see code below). I then created the following model with a few convolutions and the custom op. The goal is to have the whole TFLite model being quantized, as showed in this diagram:  Instead, I'm getting a TFLite model in which all the convolutions are indeed quantized, but the custom op is still floatingpoint and the model includes Dequantize/Quantize operators before/after the custom op. **Are quantized custom ops supported? If so, what's missing here to make sure the quantized version of the custom operator ends up in the TFLite model?**  Python code to build and convert model (three TFLite models will be created: one with no optimization, one with floatingpoint input/outputs with optimizations, and the last one with quantized input/outputs with optimizations, which is the one most relevant to this issue):  C++ code for custom operator in `tensorflow/core/user_ops/dummy.cc`: ",2022-07-29T21:59:10Z,stat:awaiting response type:support stale comp:lite TFLiteConverter ModelOptimizationToolkit TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/56950,Here is what the TFLite model looks like: !dummy_int8,Hi  ! Could you use a representative dataset and use int8 ops and inference type to get integer quantized model. s  tflite_model_quant = converter.convert(),"_TL;DR_: the problem still exists when using a representative dataset and calibration. I used the following function (from the TensorFlow posttraining quantization page):  I also added the following 2 lines in my Python code in the `tflite_model()` function:  With only those 2 changes it failed to convert the model because `DummyOp` was unknown. I then added the registration of the `DummyOp` TFLite operator in both register.cc and register_ref.cc:  And where some other custom ops are currently registered (`Mfcc` for example) I added:  After updating the environment the conversion succeeded (and took a bit longer because of the calibration), but the model still shows the same problem, i.e. the model includes Dequantize/Quantize operators before/after the custom op."," ! Thanks for the update.  Could you give one more try with below changes  You can also find quantization issue from your side using our recently introduced feature ""quantization_debugger"" Thank you!","Using `int8` data for the representative dataset does not work, I get the following error:  Regarding the quantization debugger, I couldn't find anything that was relevant to this issue. It seems to be mostly about identifying problematic layers as far as quality/precision is concerned.",Ok  ! Thanks for the update.  !  Could you look at this issue., has you been able to resolve the issue? I am facing the same issue trying to quantize a custom op.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
436,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update version numbers for TensorFlow 2.10.0-rc0)ï¼Œ å†…å®¹æ˜¯ (Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,tensorflow-jenkins,Update version numbers for TensorFlow 2.10.0-rc0,"Before merging this PR, please double check that it has correctly updated `core/public/version.h`, `tools/pip_package/setup.py`, and `tensorflow/tensorflow.bzl`. Also review the execution notes below: ",2022-07-29T21:24:42Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/56949,Closing this one as it is invalid. https://github.com/tensorflow/tensorflow/pull/56953 has the version changes for rc0
1871,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.lite.OpsSet.SELECT_TF_OPS` makes Interpreter invoke function crash)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04    **TensorFlow installed from (source or binary)**: Binary (Anaconda)    **TensorFlow version (use command below)**: 2.4.1    **Python version**: 3.9.12    **cudatoolkit version**: 10.1.243    **cuDNN version**: 7.6.5    **GPU model and memory**: NVIDIA GeForce GTX 1650 Mobile / MaxQ   **nvidiasmi**:     **Anacondaenv**: anacondaenv.zip  Describe the problem Hello, I am starting working with Tensorflow and I am facing a problem that seems similar to this issue: https://github.com/tensorflow/tensorflow/issues/25231  Context I have a trained model I cannot really change and I have to deploy it on Android.   My first thought was to use Tensorflow Lite to convert it and use it on Android. So I have converted it and I have tried to load it on my computer first (before trying to deploy it on Android). While the `invoke` function of the interpreter is called, it crashes (or the execution is aborted) with no error.   It seems it cannot handle 5D tensor as Oktai15 says but I cannot find any documentation that explains this constraint. Moreover, even if it cannot handle the case, it should return an error message and it should be explained in the documentation.   In order to be sure the problem is the fact that the interpreter doesn't handle 5D tensors, I have created a simple script that trains a model on the equation `y = 2*x`.  Source code working with 4D tensor Here is the code that)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tgeffroyModuleus,`tf.lite.OpsSet.SELECT_TF_OPS` makes Interpreter invoke function crash," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04    **TensorFlow installed from (source or binary)**: Binary (Anaconda)    **TensorFlow version (use command below)**: 2.4.1    **Python version**: 3.9.12    **cudatoolkit version**: 10.1.243    **cuDNN version**: 7.6.5    **GPU model and memory**: NVIDIA GeForce GTX 1650 Mobile / MaxQ   **nvidiasmi**:     **Anacondaenv**: anacondaenv.zip  Describe the problem Hello, I am starting working with Tensorflow and I am facing a problem that seems similar to this issue: https://github.com/tensorflow/tensorflow/issues/25231  Context I have a trained model I cannot really change and I have to deploy it on Android.   My first thought was to use Tensorflow Lite to convert it and use it on Android. So I have converted it and I have tried to load it on my computer first (before trying to deploy it on Android). While the `invoke` function of the interpreter is called, it crashes (or the execution is aborted) with no error.   It seems it cannot handle 5D tensor as Oktai15 says but I cannot find any documentation that explains this constraint. Moreover, even if it cannot handle the case, it should return an error message and it should be explained in the documentation.   In order to be sure the problem is the fact that the interpreter doesn't handle 5D tensors, I have created a simple script that trains a model on the equation `y = 2*x`.  Source code working with 4D tensor Here is the code that",2022-07-29T20:21:30Z,type:support comp:lite TF 2.4,closed,0,14,https://github.com/tensorflow/tensorflow/issues/56946,"Hi  ! Input shape has to be = [1] instead of [1,1,1,1,1] (i.e one output for one input) in model side. Attached resolved gist for reference.  Thank you!","Hi,   The purpose of the code was to test 5D tensor in a very simple code sample. I know the input shape [1] is working without `tf.lite.OpsSet.SELECT_TF_OPS`, it is not the point here. If you have a better sample to propose which takes 5D tensor I will be pleased to test it. However it doesn't explain either why it is working with input shape [1,1,1,1] and not with [1,1,1,1,1].  Also your colab is not running the correct environment /tensorflow version specified in the issue.  You also could test input shape [1] but with `tf.lite.OpsSet.SELECT_TF_OPS` and the right env.  Thanks,", ! Thanks for the clarification. Yes! 5D tensors are not supported yet in Lite side. Proper syntax to use select ops syntax is   Attached update gist in 2.8 for reference. Thank you!,"Ok, thanks!   Is there a documentation that explains this constraint please?   Whis is tensorflow lite crashing without any error message? "," ! Actually I can get results after using np.expand_dims with input dataset.  But the results were not as expected. Attached gist for reference.  5D might be  partially supported for linear regression. Could not find the exact documentation on 5D and 6D tensors constraints. But got some pointers here. 1 , 2 .  ! Could you look at this issue. Thank you!"," , Thanks for reporting the issue. As of now it is not documented about the support on 5D tensors, we will look into that. Meanwhile, you could try with 4D tensors, alternatively you could try flattening the array and then reshaping it back to 5D tensors after conversion?","Thank you, I am looking forward to learn more about your tensor support and constraints.  I am not able to look at ""my"" neural network code right now. I will see what is possible to do when I get back (August 17th), but as I have explained in the issue, I am not owner of the neural network code, it can be difficult for me to make changes accepted on it. Is there an other way than Tensorflow Lite to deploy a trained model on Android? ",In order to find a solution to deploy a trained model using 5D tensors on Android I have opened this question on stackoverflow: https://stackoverflow.com/questions/73474682/howtodeployatensorflowmodelonandroidwithouttfliteandwithoutusinga,"TFLite now supports 5D in selectV2, could you please try in the latest nightly version or the Tensorflow version 2.10 and confirm. More details can be found here https://github.com/tensorflow/tensorflow/blob/master/RELEASE.mdmajorfeaturesandimprovements1","The very basic sample seems to work while using np.expand_dims. However, I have others operator in my model, and I cannot test the solution right now. I have tried to install my packages with this new version of tensorflow and packages tensorflowbase, kerasbase, and kerasgpu cannot be installed with this latest version. Without them I cannot launch my model that uses other operators:  Conv3D  MaxPooling3D  Dropout  Dense  GlobalAveragePooling3D","Sure, After you test with the new setup, please let us know the progress. Thanks!",Hi   Did you get a chance to check with your setup?  I have tested with TF 2.12 with 5D tensor along with `SELECT_TF_OPS` and was able to run without any error. Please find the gist here and let us know if it helps. Thanks.,"Hi, I have met several issues that have pushed me to abandon tensorflow lite solution.     I have converted my model in onnx format to make it work on android.   Unfortunately, I will not be able to continue testing on tensorflow lite. Thanks a lot for your help and precious time. I think we can close this issue.",Are you satisfied with the resolution of your issue? Yes No
692,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Broken link to model mitigation guide for variable_scope (python))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,afdreher,Broken link to model mitigation guide for variable_scope (python),Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-07-29T14:54:04Z,type:docs-bug stat:awaiting response type:bug stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56945, Thank you for raising this issue. The right link is already reflecting in the master branch https://github.com/tensorflow/tensorflow/blob/03e6e2af40f53f69e5fab1151235b0880252c21c/tensorflow/python/ops/variable_scope.pyL204 It will reflect soon in the TF documentation. Please let us know if this information helps? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(build tensorflow-lite pip package arch x86_64 script error)ï¼Œ å†…å®¹æ˜¯ ( System information    **OS Platform and Distribution  Linux Ubuntu 22.04 LTS    X86_64    **TensorFlow installed from git clone https://github.com/tensorflow/tensorflow.git tensorflow_src        **Python version**: Python 3.10.4     **GCC/Compiler version gcc version 11.2.0 (Ubuntu 11.2.019ubuntu1)    **CUDA/cuDNN version**: n/a    **GPU model and memory**: n/a    **Exact command to reproduce**:  user:~$ cd tensorflow_src/ user:~/tensorflow_src$ python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native   File ""/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh"", line 71     armhf)          ^ SyntaxError: unmatched ')' user:~/tensorflow_src$  correcting this with an opening '(' on line 70 results in  user:~/tensorflow_src$ python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native   File ""/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh"", line 18     SCRIPT_DIR=""$(cd ""$(dirname ""${BASH_SOURCE[0]}"")"" && pwd)""                       ^ SyntaxError: invalid syntax user:~/tensorflow_src$  trying to build tensorflowruntime for x86 platform as package is required by preconfigured code originally destined for an Arm platform, rest of the code works ok. I'm not a bash expert so didn't want to play further.... Grateful for any assistance you can give...)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,edharman,build tensorflow-lite pip package arch x86_64 script error," System information    **OS Platform and Distribution  Linux Ubuntu 22.04 LTS    X86_64    **TensorFlow installed from git clone https://github.com/tensorflow/tensorflow.git tensorflow_src        **Python version**: Python 3.10.4     **GCC/Compiler version gcc version 11.2.0 (Ubuntu 11.2.019ubuntu1)    **CUDA/cuDNN version**: n/a    **GPU model and memory**: n/a    **Exact command to reproduce**:  user:~$ cd tensorflow_src/ user:~/tensorflow_src$ python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native   File ""/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh"", line 71     armhf)          ^ SyntaxError: unmatched ')' user:~/tensorflow_src$  correcting this with an opening '(' on line 70 results in  user:~/tensorflow_src$ python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native   File ""/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh"", line 18     SCRIPT_DIR=""$(cd ""$(dirname ""${BASH_SOURCE[0]}"")"" && pwd)""                       ^ SyntaxError: invalid syntax user:~/tensorflow_src$  trying to build tensorflowruntime for x86 platform as package is required by preconfigured code originally destined for an Arm platform, rest of the code works ok. I'm not a bash expert so didn't want to play further.... Grateful for any assistance you can give...",2022-07-29T08:42:57Z,stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56944,Hi  ! Could you check with below command with gcc 7.3 after cloning the repo. `PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native` You can also build using Cmake through below command.  Thank you!,"Hi,  PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native ...seems to fix the issue!, Many Thanks",Are you satisfied with the resolution of your issue? Yes No,"Yes, impressed with the speedy resolution,  Good Work!!"
1376,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(io_ops.py docstrings for serialize_tensor method generates different output on s390x architecture)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 7.5.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  Should a little blurb be added to serialize_tensor def to indicate that example docstring results will depend on system architecture on which testcase is executed? Thanks! shell x86: >>> import tensorflow as tf >>> t = tf.constant(1) 20220728 10:43:31.641256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> tf.io.serialize_tensor(t)  s390x: >>> import tensorflow as tf >>> t = tf.constant(1) >>> tf.io.serialize_tensor(t)    Relevant log output  on s390x:  ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rposts,io_ops.py docstrings for serialize_tensor method generates different output on s390x architecture,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 7.5.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  Should a little blurb be added to serialize_tensor def to indicate that example docstring results will depend on system architecture on which testcase is executed? Thanks! shell x86: >>> import tensorflow as tf >>> t = tf.constant(1) 20220728 10:43:31.641256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. >>> tf.io.serialize_tensor(t)  s390x: >>> import tensorflow as tf >>> t = tf.constant(1) >>> tf.io.serialize_tensor(t)    Relevant log output  on s390x:  ``` ",2022-07-28T18:07:02Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56937,", I was able to execute the provided code without any issues and the output also as expected. Kindly find the gist of it here. Thank you!",Hi   behavior is different on `s390x` arch (bigendian) where bytes are reversed  see output from this arch (notice the presence of x01 at the end vs x00 on `x86` arch: ,Hi   any insights appreciated  tx!,Closing  no response.,Are you satisfied with the resolution of your issue? Yes No
463,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow re-init on same stream in cudamallocasync allocator.)ï¼Œ å†…å®¹æ˜¯ (This fixes a bug in which destroying and recreating a session causes the cuda_malloc_async allocator to fail. For example, when `TF_GPU_ALLOCATOR` is set to `cuda_malloc_async` without this patch  results in the error )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nluehr,Allow re-init on same stream in cudamallocasync allocator.,"This fixes a bug in which destroying and recreating a session causes the cuda_malloc_async allocator to fail. For example, when `TF_GPU_ALLOCATOR` is set to `cuda_malloc_async` without this patch  results in the error ",2022-07-28T14:27:23Z,size:XS comp:core,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56935, Is the error above the same that you saw with real model in JAX with cudaMallocAsync?," No, it wasn't. That symptom was memory corruption (missing synchronization?)",">  No, it wasn't. That symptom was memory corruption (missing synchronization?) Ok. I was taking a chance.","Thanks for the change! But, someone else also has a PR to fix this, which should be merged soon: https://github.com/tensorflow/tensorflow/pull/56136."
806,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Efficiently get an equal number of per class data point continuously with `tf.data` API. )ï¼Œ å†…å®¹æ˜¯ ([Info]   Current Behaviour I am trying to get equal number of sample per class within a batch of data from `tf.data` API. With `batch_size = 14` and `sample_per_class = 3`, I'm expecting to get the following output for `num_classes = 5`:   Standalone code to reproduce the issue Here is the standalone code and approach so far.   The issue arises when `num_classes` gets bigger, for example, **imagenet** (1000 classes). It gets too slow to train the model, GPU/TPU both. I'm looking for an efficient solution with `tf.data` API. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,innat,Efficiently get an equal number of per class data point continuously with `tf.data` API. ,"[Info]   Current Behaviour I am trying to get equal number of sample per class within a batch of data from `tf.data` API. With `batch_size = 14` and `sample_per_class = 3`, I'm expecting to get the following output for `num_classes = 5`:   Standalone code to reproduce the issue Here is the standalone code and approach so far.   The issue arises when `num_classes` gets bigger, for example, **imagenet** (1000 classes). It gets too slow to train the model, GPU/TPU both. I'm looking for an efficient solution with `tf.data` API. ",2022-07-28T13:39:06Z,stat:awaiting tensorflower comp:data type:performance 2.6.0,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56934," I tried to replicate the issue on colab, could you please have a look at the gist here and confirm the same. Thank you!"," Thanks for checking.  Yes, the code I've shared above, should be run without giving any error.  The main problem is that the process of gathering the same class side by side (`[1, 1, 1, 2, 2, 3, 3, ..]`) is too slow in actual training. For example, if you run this code on ImageNet (number of class 1000), you would be able to see the effect. Especially, what would be an efficient alternative to the following line of code for the larger number of classes like ImageNet? ",Any update on this?,"The use of `filter` in `dataset_for_class` means that you will need to iterate through all of the data once per class. Each iteration through the data will drop around 99.9% of the data, so that it can filter out just the one class you are looking for. Some ideas to make this more efficient:  Use Dataset.rejection_resample to get the desired distribution.  Store each class separately, so that you don't need to do any filtering."
1820,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable NNAPI Crash )ï¼Œ å†…å®¹æ˜¯ (**System information**  Xamarin.Android Android 12.0   Xamarin Tensorflow Lite 2.6.0.1  Visual Studio 2022 c **Standalone code to reproduce the issue** Enableing NNAPI (options.SetUseNNAPI(true);) interpreter = new Interpreter(mappedByteBuffer, options); https://tfhub.dev/tensorflow/litemodel/ssd_mobilenet_v1/1/metadata/1?liteformat=tflite That works  all my trained models Crash  Work when i don't Enable NNAPI (I Comment options.SetUseNNAPI(true) but its should be slower) Even when i take  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu8.tar.gz tflite_directory = '/content/tfliteExport' pipeline_file = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu8/pipeline.config' last_model_path = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu8/checkpoint' !python /content/models/research/object_detection/export_tflite_graph_tf2.py \     pipeline_config_path={pipeline_file} \     trained_checkpoint_dir={last_model_path} \     output_directory={tflite_directory} import cv2  image_path = '/content/gdrive/MyDrive/Map/foto/Test' image_path = '/content/img' def representative_dataset_gen():     for f_name in os.listdir(image_path):       file_path = os.path.normpath(os.path.join(image_path, f_name))       print(file_path)       img = cv2.imread(file_path)       img = cv2.resize(img, (640,640))       img = img / 255.0       img = np.reshape(img, (1, 640, 640, 3))       image = img.astype(np.float32)       yield [image] converter = tf.lite.TFLiteConverter.from_saved_model('/content/tfliteExport/sav)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Whrothus,Enable NNAPI Crash ,"**System information**  Xamarin.Android Android 12.0   Xamarin Tensorflow Lite 2.6.0.1  Visual Studio 2022 c **Standalone code to reproduce the issue** Enableing NNAPI (options.SetUseNNAPI(true);) interpreter = new Interpreter(mappedByteBuffer, options); https://tfhub.dev/tensorflow/litemodel/ssd_mobilenet_v1/1/metadata/1?liteformat=tflite That works  all my trained models Crash  Work when i don't Enable NNAPI (I Comment options.SetUseNNAPI(true) but its should be slower) Even when i take  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu8.tar.gz tflite_directory = '/content/tfliteExport' pipeline_file = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu8/pipeline.config' last_model_path = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu8/checkpoint' !python /content/models/research/object_detection/export_tflite_graph_tf2.py \     pipeline_config_path={pipeline_file} \     trained_checkpoint_dir={last_model_path} \     output_directory={tflite_directory} import cv2  image_path = '/content/gdrive/MyDrive/Map/foto/Test' image_path = '/content/img' def representative_dataset_gen():     for f_name in os.listdir(image_path):       file_path = os.path.normpath(os.path.join(image_path, f_name))       print(file_path)       img = cv2.imread(file_path)       img = cv2.resize(img, (640,640))       img = img / 255.0       img = np.reshape(img, (1, 640, 640, 3))       image = img.astype(np.float32)       yield [image] converter = tf.lite.TFLiteConverter.from_saved_model('/content/tfliteExport/sav",2022-07-27T19:16:47Z,stat:awaiting response type:support stale comp:lite TFLiteNNAPIDelegate 2.6.0,closed,0,17,https://github.com/tensorflow/tensorflow/issues/56928, if you need help just let me know," , Apologies for the delayed response, I'm looking into your issue. I will get back to you if I need any details from you. Thanks",Could you try the option to `setUseNnapiCpu` to `true` in the `NnApiDelegate.Options` object and let us know if it is still crashing.,"So i did  Interpreter.Options options = new Interpreter.Options(); options.SetUseNNAPI(true); NnApiDelegate.Options NnApiOption  = new NnApiDelegate.Options(); NnApiOption.SetUseNnapiCpu(true); NnApiDelegate nnApiDelegate = new NnApiDelegate(NnApiOption); options.AddDelegate(nnApiDelegate); try { interpreter = new Interpreter(mappedByteBuffer, options); .TFlite = https://drive.google.com/file/d/1RZfTMnxOdKnDyBUBFb0kk4EJoxdUGlXr/view?usp=sharing Crash  {Java.Lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1053 while adding operation. Node number 158 (TfLiteNnapiDelegate) failed to prepare. Restored original execution plan after delegate application failure.   at Java.Interop.JniEnvironment+InstanceMethods.CallNonvirtualVoidMethod (Java.Interop.JniObjectReference instance, Java.Interop.JniObjectReference type, Java.Interop.JniMethodInfo method, Java.Interop.JniArgumentValue* args) [0x00088] in /Users/runner/work/1/s/xamarinandroid/external/Java.Interop/src/Java.Interop/Java.Interop/JniEnvironment.g.cs:12324    at Java.Interop.JniPeerMembers+JniInstanceMethods.FinishCreateInstance (System.String constructorSignature, Java.Interop.IJavaPeerable self, Java.Interop.JniArgumentValue* parameters) [0x0003e] in /Users/runner/work/1/s/xamarinandroid/external/Java.Interop/src/Java.Interop/Java.Interop/JniPeerMembers.JniInstanceMethods.cs:142    at Xamarin.TensorFlow.Lite.Interpreter..ctor (Java.Nio.ByteBuffer byteBuffer, Xamarin.TensorFlow.Lite.Interpreter+Options options) [0x0009d] in :0    at CameraTF.TensorflowLiteService.Initialize () [0x00034] in X:\Snelle TF Xam\CameraTFmaster\src\CameraTF\AR\TensorflowLiteService.cs:68     End of managed Java.Lang.IllegalArgumentException stack trace  java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1053 while adding operation. Node number 158 (TfLiteNnapiDelegate) failed to prepare. Restored original execution plan after delegate application failure. 	at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method) 	at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:480) 	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:88) 	at org.tensorflow.lite.NativeInterpreterWrapper.(NativeInterpreterWrapper.java:66) 	at org.tensorflow.lite.Interpreter.(Interpreter.java:247) 	at crc649b31f1000968ddb3.MainActivity.n_onCreate(Native Method) 	at crc649b31f1000968ddb3.MainActivity.onCreate(MainActivity.java:31) 	at android.app.Activity.performCreate(Activity.java:7994) 	at android.app.Activity.performCreate(Activity.java:7978) 	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309) 	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3422) 	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3601) 	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85) 	at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135) 	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95) 	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2066) 	at android.os.Handler.dispatchMessage(Handler.java:106) 	at android.os.Looper.loop(Looper.java:223) 	at android.app.ActivityThread.main(ActivityThread.java:7656) 	at java.lang.reflect.Method.invoke(Native Method) 	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592) 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947) }","Hi , thanks for reporting the issue. Could you share the following information to help us analyze the problem?  Which device were you using when the issue occured?  What is the Android build number / version of that device?",  One plus 7 Oxygen 10.0.9.1 and pixel 5 emulator. sdk_gphone_x86_64_arm64userdebug 11 RSR1.210722.013 7800151 devkeys. Is there a emulator you prefer ? ,tf Do you need something else ?, Do you have a emulator you prefer ?,"For both the OnePlus 7 and the emulator, could you try `adb shell setprop debug.nn.vlog 1` and rerun your test and share the logcat through `adb logcat`? Also, I just realized that you were using Xamarin Tensorflow Lite 2.6.0.1, which seems to be out of date regarding NNAPI delegate implementation. I wonder if you have tried to use the latest official TFLite packages to run your models? E.g. use the TFLite benchmark app (https://www.tensorflow.org/lite/performance/measurement): ",Think i f up att converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8],Is there any way you could include the select ops as well in your converter and check the outcome. ,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Not resolved but cant make the ""adb shell"" Command to work ","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
736,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failure to use selectively build TFLite frameworks on iOS)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.6  Custom Code No  OS Platform and Distribution iOS 14.1  target build, MacOSX 10.15.7 and XCode 12.4 for building  Mobile device Apple iPad, iOS 15.3  Python version 3.9  Bazel version 3.4.0  GCC/Compiler version Apple clang version 12.0.0 (clang1200.0.32.29)  CUDA/cuDNN version no CUDA used  GPU model and memory no GPU  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,emenshoff,Failure to use selectively build TFLite frameworks on iOS,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.6  Custom Code No  OS Platform and Distribution iOS 14.1  target build, MacOSX 10.15.7 and XCode 12.4 for building  Mobile device Apple iPad, iOS 15.3  Python version 3.9  Bazel version 3.4.0  GCC/Compiler version Apple clang version 12.0.0 (clang1200.0.32.29)  CUDA/cuDNN version no CUDA used  GPU model and memory no GPU  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-07-27T15:43:00Z,stat:awaiting response type:build/install comp:lite subtype:macOS 2.6.0,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56925,"Hi  ! Sorry for the late reply. Could you check with below steps and let us know. 1.  Please  checkout 2.8/2.9 Branch or higher and make sure that you have said yes to IOS build while running configure.py and   declared   in your pod file. If still does not resolve Goto step 2.  2. Go to your Project 3. Select your Target 4. Go to Build Phases 5. Open Link Binary With Libraries 6. Press + to Add the SelectOps.framework . Attached relevant thread for reference. 1, 2. Thank you!","Thank you, ! Should I remove ""force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/iosarm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"" from Other linker options after adding the framework to linker targets manually? Thanks in advance!","After removing force_load option and adding framework manually (step 6) the project has been built successfully, but I've got runtime error when loading my .tflite model: ""Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. TensorFlow Lite Error: Node number 0 (FlexResizeBilinear) failed to prepare."" Seems like the Custom Ops framework was not initialised correctly on runtime :(", ! You should not remove the force load step too. Above steps are addons for trouble shooting. Thank you!,"I tried all the options above. And got another errors: Undefined symbols for architecture arm64:   ""tflite::logging_internal::MinimalLogger::LogFormatted(tflite::LogSeverity, char const*, char*)"", referenced from:       tflite::StderrReporter::Report(char const*, char*) in TensorFlowLiteSelectTfOps(stderr_reporter.o)   ""tflite::ConvertTensorType(tflite::TensorType, TfLiteType*, tflite::ErrorReporter*)"", referenced from:       tflite::InterpreterBuilder::ParseTensors(flatbuffers::Vector > const*, flatbuffers::Vector > const*, tflite::Subgraph*) in TensorFlowLiteSelectTfOps(interpreter_builder.o)   ""tflite::ParseOpData(tflite::Operator const*, tflite::BuiltinOperator, tflite::ErrorReporter*, tflite::BuiltinDataAllocator*, void**)"", referenced from:       tflite::InterpreterBuilder::ParseNodes(flatbuffers::Vector > const*, tflite::Subgraph*) in TensorFlowLiteSelectTfOps(interpreter_builder.o) ****and  100+ same errors referencing tflite:: and ruy::  namespaces  :(","Hi, For the build flags could you try specifying `target_archs=x86_64,armv7,arm64` and see if you're able to build static `TensorFlowLiteSelectTfOps` library. Thanks!","target_archs=x86_64,armv7,arm64 option does not work, because x86 platform for TensorFlowLiteSelectTfOps are not supported by Google anymore.","I've found the solution: in file: /tensorflow/lite/ios/build_frameworks.sh, in function: generate_flex_framework, just add option ""config=monolithic"" to bazel build options at the end of function:  bazel build c opt config=ios config=monolithic  ios_multi_cpus=""${TARGET_ARCHS}"" ""${target}"" After that all build errors were gone (tensorflow v2.9.1), and my app starts and works fine! Thank you guys!, Have a nice day!","I'm glad that your issue is resolved, could you please close this issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No
1404,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite C API on Android Studio)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**:2.9.0    **Python version**:    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**:  Describe the problem I'm currently run tflite model on Android app by C API, I download version 2.9.0 shared library from TensorFlow Lite AAR hosted at MavenCentral and include requied header files as DOC mentioned. Load tflite model and create interpreter works perfectly. And `TfLiteInterpreterGetSignatureCount` return number is also correct. But `TfLiteInterpreterGetSignatureKey` shows **undefined reference to `TfLiteInterpreterGetSignatureKey`**. Does C API support these two function currently?  Source code / logs _nativelib.cpp_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,acebenson0704,TFLite C API on Android Studio," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**:2.9.0    **Python version**:    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**:  Describe the problem I'm currently run tflite model on Android app by C API, I download version 2.9.0 shared library from TensorFlow Lite AAR hosted at MavenCentral and include requied header files as DOC mentioned. Load tflite model and create interpreter works perfectly. And `TfLiteInterpreterGetSignatureCount` return number is also correct. But `TfLiteInterpreterGetSignatureKey` shows **undefined reference to `TfLiteInterpreterGetSignatureKey`**. Does C API support these two function currently?  Source code / logs _nativelib.cpp_ ",2022-07-27T10:10:16Z,stat:awaiting response type:support stale comp:lite TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56922,"Hi  ! Would it be possible to give a reproducible code in colab too (including the built c lib , running with dependent Tensorflow lib files and the above cpp file ).","Hi thank you for your reply. Sorry I cannot provide source code because of company policies. But It's a basic Android Studio Project based on **Use TFLite C API** in this doc. And as the above _native.lib_  code shows, it simply load pretrained tflite model and get some properties through c api . More information:  _CMakeLists.txt_: Add following lines for tflite and add tflitelib into `target_link_libraries`  _app/src/main/jni/_: Add all header files and shared library as doc (Most of header files is unused here.)  _build.gradle(:app)_: Add abiFilters to cmake  The rest part is **exactly** the same as Android Studio Native C/C++ Template.",Hi   TFLite C API should support the `TfLiteInterpreterGetSignatureKey` as given in the experimental api: https://github.com/tensorflow/tensorflow/blob/515dbdd45fdbc288667b3b3c7840b8ab2a527501/tensorflow/lite/core/c/c_api_experimental.ccL157L164 Can you please check in latest version and see if the behaviour still exists? Thanks.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow, llvmlite, mac M1 and poetry)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Apple Mac OS 12.5  Mobile device _No response_  Python version 3.9.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,oscar-defelice,"Tensorflow, llvmlite, mac M1 and poetry",Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Apple Mac OS 12.5  Mobile device _No response_  Python version 3.9.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-27T08:20:11Z,stat:awaiting response type:build/install subtype:macOS TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56921,"Hi defelice, To install llvmlite, follow the workarounds mentioned on similar thread here.  For users of Apple M1 computers, it is highly recommended to follow the instructions found here for smooth installation.Thank you! ","Hello , thank you very much for your answer. I do not have installation problems on my machine. I have working versions of tensorflowmetal, tensorflowmacos and llvmlite installed by pip. I got issues when I install it through poetry (as a dependency for my project).  The main problem is that I cannot add ""tensorflowmacos"" as the project has to be multiplatform.",defelice add this to your `pyproject.toml` to make it multiplatform ,"defelice , Whether the comment above is of any help to you?", Thank you for pointing this out. I missed the comment since I simply got rid of poetry as dependency manager because of these difficulties. I tried just now and it seems to work fine with the comment by . Thank you both!,Are you satisfied with the resolution of your issue? Yes No,"I am joining late this party, however, thanks to precious comment from  this worked on my Macbook Pro M1 and Python 3.10.6 (via pyenv).  I then run the script on step 4 of https://developer.apple.com/metal/tensorflowplugin/ to make sure it worked. It didn't with latest TF 2.11, but it did with the versions above (as per Apple compatibility table)."
564,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Differentiate a tuple with `tape.jacobian`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution MacOs  Mobile device   Python version 3.9  Bazel version   GCC/Compiler version   CUDA/cuDNN version   GPU model and memory   Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,rmoyard,Differentiate a tuple with `tape.jacobian`,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution MacOs  Mobile device   Python version 3.9  Bazel version   GCC/Compiler version   CUDA/cuDNN version   GPU model and memory   Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-26T18:03:56Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,open,0,3,https://github.com/tensorflow/tensorflow/issues/56912,", I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist of it here.",Any update on this issue?  ,I suppose a workaround is  Is the above a reasonable workaround? :thinking: How about performance of this repeated differentiation? Are there situations where this would lead to excessive retracing? :thinking: 
1010,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(unexpected value of binary_crossentropy loss function in network with )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8.2  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell [[0.5152152  0.48478484]] 1/1 [==============================]  0s 92ms/step  loss: 0.7085 Evaluated loss =  0.7084982991218567 Function loss =  0.72405 Manual loss =  0.7240501642227173 The evaluated loss value makes no sense compared to the function value and the manually calculated value (which is just equal to log(0.48478484) as it should be ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,will-cern,unexpected value of binary_crossentropy loss function in network with ,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8.2  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell [[0.5152152  0.48478484]] 1/1 [==============================]  0s 92ms/step  loss: 0.7085 Evaluated loss =  0.7084982991218567 Function loss =  0.72405 Manual loss =  0.7240501642227173 The evaluated loss value makes no sense compared to the function value and the manually calculated value (which is just equal to log(0.48478484) as it should be ``` ,2022-07-26T16:46:27Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56910,Hi cern ! I got same result when the activation function in last layer was changed from 'softmax' to 'sigmoid' and bit different result on the given case. Attached gist for reference. Thank you!,"Hi   a sigmoid activation isn't the right thing to be using here though because then the sum of the two network outputs is not equal to 1 i.e. the two outputs are not the respective probabilities of each of the two classes. Furthermore I'm not entirely unconvinced that they just appear to be agreeing because using the sigmoid activation you get outputs always very close to 0.5 so it looks like the numbers are the same but in fact they aren't. I think another way to ask my question is this: from the two probabilities of [0.5152152,  0.48478484] (which are the output of the network), what calculation is being performed that results in 0.7084982991218567 ??  I can see how to get the number from the ""function"" call to the loss function, 0.72405, because that's equal to log(0.48478484). But I cannot figure out what calculation the ""evaluate"" call is doing to get 0.70849... ???",cern ! Evaluating loss came close to manual loss and functional loss after reducing the no of neurons in input layers to 64 and adding more hidden layers.Attached gist for reference. Please post in Keras repo for further assistance. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,(cern in the meantime posted this in the TF Forum and in the Keras tracker),Hi  cern ! I suggested to use 64 neurons instead of 2**8 as it will overfit the results.  Attached a resolved gist to explain it. I thought it would be a good enough explanation then told to post in Keras repo for further assistance. Thank you!,"That wasn't a satisfactory solution to this issue for me, because the issue is about how the loss function is actually being calculated. Unless I've seriously misunderstood what 'evaluate' does, I believe we should be able to reproduce *exactly* whatever number is calculated by the 'evaluate' method, completely independently of whatever the network architecture is. The loss is simply a function of the network output, so if I take the network output, whatever the output is, I should be able to calculate the loss for myself.  This does work for other loss functions as far as I can recall, it was just in this particular case with binary crossentropy on a network with more than 1 output that it is producing something that I cannot verify. "
659,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AttributeError: module 'keras.layers' has no attribute 'experimental': M1 Mac Tensorflow Metal)ï¼Œ å†…å®¹æ˜¯ (I am trying to run a TensorFlow model on M1 Mac with the following settings: 1. MacBook Pro M1 2. macOS 12.4 3. tensorflowdeps **&** tensorflowestimator > 2.9.0 4. tensorflowmacos > 2.9.2 5. tensorflowmetal > 0.5.0 6. keras > 2.9.0 7. keraspreprocessing  > 1.1.2 8. Python 3.8.13 When resizing and rescaling from keras.layers, I got the following error:  Any suggestions? Thanks)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mtoseef99,AttributeError: module 'keras.layers' has no attribute 'experimental': M1 Mac Tensorflow Metal,"I am trying to run a TensorFlow model on M1 Mac with the following settings: 1. MacBook Pro M1 2. macOS 12.4 3. tensorflowdeps **&** tensorflowestimator > 2.9.0 4. tensorflowmacos > 2.9.2 5. tensorflowmetal > 0.5.0 6. keras > 2.9.0 7. keraspreprocessing  > 1.1.2 8. Python 3.8.13 When resizing and rescaling from keras.layers, I got the following error:  Any suggestions? Thanks",2022-07-25T11:08:08Z,stat:awaiting response type:support stale comp:keras,closed,0,14,https://github.com/tensorflow/tensorflow/issues/56889,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi , I could able to use `layers.experimental.preprocessing` with Tensorflow 2.9.2.  I changed `layers.experimental.preprocessing` to `tf.keras.layers.experimental.preprocessing`. Working as expected. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No," AttributeError                            Traceback (most recent call last) Cell In[111], line 12      10 from object_detection.utils import config_util       11 from object_detection.utils import visualization_utils as viz_utils  > 12 from object_detection.builders import model_builder   File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\model_builder.py:26      24 from object_detection.builders import box_coder_builder      25 from object_detection.builders import box_predictor_builder > 26 from object_detection.builders import hyperparams_builder      27 from object_detection.builders import image_resizer_builder      28 from object_detection.builders import losses_builder File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\hyperparams_builder.py:27      25  pylint: disable=gimportnotattop      26 if tf_version.is_tf2(): > 27   from object_detection.core import freezable_sync_batch_norm      28  pylint: enable=gimportnotattop      31 class KerasLayerHyperparams(object): File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\core\freezable_sync_batch_norm.py:20      16 """"""A freezable batch norm layer that uses Keras sync batch normalization.""""""      17 import tensorflow as tf > 20 class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization      21                             ):      22   """"""Sync Batch normalization layer (Ioffe and Szegedy, 2014).      23       24   This is a `freezable` batch norm layer that supports setting the `training`    (...)      48         Internal Covariate Shift](https://arxiv.org/abs/1502.03167)      49   """"""      51   def __init__(self, training=None, **kwargs): AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental'","import matplotlib  import matplotlib.pyplot as plt   import io  import os import numpy as np  from six import BytesIO  from PIL import Image, ImageDraw, ImageFont   import tensorflow as tf   from object_detection.utils import label_map_util  from object_detection.utils import config_util  from object_detection.utils import visualization_utils as viz_utils  from object_detection.builders import model_builder  ","> AttributeError Traceback (most recent call last) Cell In[111], line 12 10 from object_detection.utils import config_util 11 from object_detection.utils import visualization_utils as viz_utils > 12 from object_detection.builders import model_builder >  > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\model_builder.py:26 24 from object_detection.builders import box_coder_builder 25 from object_detection.builders import box_predictor_builder > 26 from object_detection.builders import hyperparams_builder 27 from object_detection.builders import image_resizer_builder 28 from object_detection.builders import losses_builder >  > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\hyperparams_builder.py:27 25  pylint: disable=gimportnotattop 26 if tf_version.is_tf2(): > 27 from object_detection.core import freezable_sync_batch_norm 28  pylint: enable=gimportnotattop 31 class KerasLayerHyperparams(object): >  > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\core\freezable_sync_batch_norm.py:20 16 """"""A freezable batch norm layer that uses Keras sync batch normalization."""""" 17 import tensorflow as tf > 20 class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization 21 ): 22 """"""Sync Batch normalization layer (Ioffe and Szegedy, 2014). 23 24 This is a `freezable` batch norm layer that supports setting the `training` (...) 48 Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 49 """""" 51 def **init**(self, training=None, **kwargs): >  > AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental' I am getting the same error, have you fixed it?","> > AttributeError Traceback (most recent call last) Cell In[111], line 12 10 from object_detection.utils import config_util 11 from object_detection.utils import visualization_utils as viz_utils > 12 from object_detection.builders import model_builder > > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\model_builder.py:26 24 from object_detection.builders import box_coder_builder 25 from object_detection.builders import box_predictor_builder > 26 from object_detection.builders import hyperparams_builder 27 from object_detection.builders import image_resizer_builder 28 from object_detection.builders import losses_builder > > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\hyperparams_builder.py:27 25  pylint: disable=gimportnotattop 26 if tf_version.is_tf2(): > 27 from object_detection.core import freezable_sync_batch_norm 28  pylint: enable=gimportnotattop 31 class KerasLayerHyperparams(object): > > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\core\freezable_sync_batch_norm.py:20 16 """"""A freezable batch norm layer that uses Keras sync batch normalization."""""" 17 import tensorflow as tf > 20 class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization 21 ): 22 """"""Sync Batch normalization layer (Ioffe and Szegedy, 2014). 23 24 This is a `freezable` batch norm layer that supports setting the `training` (...) 48 Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 49 """""" 51 def **init**(self, training=None, **kwargs): > > AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental' >  > I am getting the same error, have you fixed it? I solved it by limiting the versions of tfmodelsofficial >=2.5.1, <2.16.0 and comment out keras in the research/object_detection/packages/tf2/setup.py   I run the code in colab and colab's keras is version 2.15.0.  I suspect the newest keras messed up the object detection API","> > > AttributeError Traceback (most recent call last) Cell In[111], line 12 10 from object_detection.utils import config_util 11 from object_detection.utils import visualization_utils as viz_utils > 12 from object_detection.builders import model_builder > > > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\model_builder.py:26 24 from object_detection.builders import box_coder_builder 25 from object_detection.builders import box_predictor_builder > 26 from object_detection.builders import hyperparams_builder 27 from object_detection.builders import image_resizer_builder 28 from object_detection.builders import losses_builder > > > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\hyperparams_builder.py:27 25  pylint: disable=gimportnotattop 26 if tf_version.is_tf2(): > 27 from object_detection.core import freezable_sync_batch_norm 28  pylint: enable=gimportnotattop 31 class KerasLayerHyperparams(object): > > > File ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\core\freezable_sync_batch_norm.py:20 16 """"""A freezable batch norm layer that uses Keras sync batch normalization."""""" 17 import tensorflow as tf > 20 class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization 21 ): 22 """"""Sync Batch normalization layer (Ioffe and Szegedy, 2014). 23 24 This is a `freezable` batch norm layer that supports setting the `training` (...) 48 Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 49 """""" 51 def **init**(self, training=None, **kwargs): > > > AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental' > >  > >  > > I am getting the same error, have you fixed it? >  > I solved it by limiting the versions of tfmodelsofficial >=2.5.1, <2.16.0 and comment out keras in the research/object_detection/packages/tf2/setup.py I run the code in colab and colab's keras is version 2.15.0. I suspect the newest keras messed up the object detection API And how does one limit the versions of tfmodelsofficial? Thank you","  > And how does one limit the versions of tfmodelsofficial? Thank you Modify: `research/object_detection/packages/tf2/setup.py`  For me, this fixed the issue","Thank you very much, Mr.Markham. Let me check it. On Fri, Mar 22, 2024 at 7:07â€¯AM Tony Markham ***@***.***> wrote: > AttributeError Traceback (most recent call last) Cell In[111], line 12 10 > from object_detection.utils import config_util 11 from > object_detection.utils import visualization_utils as viz_utils > 12 from > object_detection.builders import model_builder > File > ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\model_builder.py:26 > 24 from object_detection.builders import box_coder_builder 25 from > object_detection.builders import box_predictor_builder > 26 from > object_detection.builders import hyperparams_builder 27 from > object_detection.builders import image_resizer_builder 28 from > object_detection.builders import losses_builder > File > ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\builders\hyperparams_builder.py:27 > 25  pylint: disable=gimportnotattop 26 if tf_version.is_tf2(): > 27 > from object_detection.core import freezable_sync_batch_norm 28  pylint: > enable=gimportnotattop 31 class KerasLayerHyperparams(object): > File > ~\Desktop\Tensor\RealTimeObjectDetection\Tensorflow\models\research\object_detection\core\freezable_sync_batch_norm.py:20 > 16 """"""A freezable batch norm layer that uses Keras sync batch > normalization."""""" 17 import tensorflow as tf > 20 class > FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization > 21 ): 22 """"""Sync Batch normalization layer (Ioffe and Szegedy, 2014). 23 24 > This is a freezable batch norm layer that supports setting the training > (...) 48 Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 49 > """""" 51 def *init*(self, training=None, **kwargs): > AttributeError: module 'keras._tf_keras.keras.layers' has no attribute > 'experimental' > > I am getting the same error, have you fixed it? > > I solved it by limiting the versions of tfmodelsofficial >=2.5.1, >  research/object_detection/packages/tf2/setup.py I run the code in colab and > colab's keras is version 2.15.0. I suspect the newest keras messed up the > object detection API > > And how does one limit the versions of tfmodelsofficial? Thank you > > Modify: research/object_detection/packages/tf2/setup.py > > """"""Setup script for object_detection with TF2.0."""""" > import os > from setuptools import find_packages > from setuptools import setup > > REQUIRED_PACKAGES = [ >      Required for apachebeam with PY3 >     'avropython3', >     'apachebeam', >     'pillow', >     'lxml', >     'matplotlib', >     'Cython', >     'contextlib2', >     'tfslim', >     'six', >     'pycocotools', >     'lvis', >     'scipy', >     'pandas', >     'tfmodelsofficial >=2.5.1,      'tensorflow_io', >     'keras', >     'pyparsing==2.4.7',   TODO(b/204103388) >     'sacrebleu ] > > setup( >     name='object_detection', >     version='0.1', >     install_requires=REQUIRED_PACKAGES, >     include_package_data=True, >     packages=( >         [p for p in find_packages() if p.startswith('object_detection')] + >         find_packages(where=os.path.join('.', 'slim'))), >     package_dir={ >         'datasets': os.path.join('slim', 'datasets'), >         'nets': os.path.join('slim', 'nets'), >         'preprocessing': os.path.join('slim', 'preprocessing'), >         'deployment': os.path.join('slim', 'deployment'), >         'scripts': os.path.join('slim', 'scripts'), >     }, >     description='Tensorflow Object Detection Library', >     python_requires='>3.6', > ) > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >",">  >  > > And how does one limit the versions of tfmodelsofficial? Thank you >  > Modify: `research/object_detection/packages/tf2/setup.py` >  >  >  > For me, this fixed the issue Yes this is what I did! "
671,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(collective_ops.all_reduce_v2 with ordering_token does not work correctly)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.5 or tf 2.8  Custom Code No  OS Platform and Distribution Centos 72.  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",pangu,chengmengli06,collective_ops.all_reduce_v2 with ordering_token does not work correctly,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.5 or tf 2.8  Custom Code No  OS Platform and Distribution Centos 72.  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-25T04:13:57Z,stat:awaiting response type:bug stale comp:dist-strat comp:ops TF 2.8,closed,0,16,https://github.com/tensorflow/tensorflow/issues/56885,Could  give some explanations on ordering_token?,"Hi , I tried to replicate the issue with Tf v2.9, but I donâ€™t see any error  Could you confirm the original issue still persists. Thank you!","https://github.com/alibaba/EasyRec/tree/fix_mirrored_bug, it can be reproduced with this branch using tensorflow 2.9.1 . The test is skipped in master branch temporarily.","Hi , I tried with Tensorflow 2.9.1, but i didnâ€™t see any error. Thank you!",could you post your logs here?  ,!image My output.,"Hi , I tried with Tf 2.9.1 and CUDA 11.4. ","As can see from the log, the test case is skipped, could you checkout fix_mirrored_bug branch, and run the test again?",any progress?, , any progress?,"Could you explain what is the purpose of ordering_token, the order of communication? Is it related to nccl?  Maybe we could help with it.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
656,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(keras.metrics.Mean does not support cross-replica context)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubunto 18.04.6 LTS  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tylerpayne,keras.metrics.Mean does not support cross-replica context,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubunto 18.04.6 LTS  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-23T23:54:03Z,type:bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/56875,Closing because moved to the keras repo: https://github.com/kerasteam/tfkeras/issues/505,Are you satisfied with the resolution of your issue? Yes No
1872,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bilinear upsampling layer in tflite cannot be 8-bit quantized correctly)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04  TensorFlow installation (pip package or built from source):  pip   TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0  TensorFlow Model Optimization version (installed from source or binary): pip  0.7.2  Abstract Hello, I want to do full 8bit quantization(input, weight all 8bit) to the network with a bilinear upsampling layer. The fake QAT result in validation set is closed to FP32 result, but when I converted the QAT model to full 8bit tflite model, the result in validation set decreased significantly, almost no accuracy. So I wonder whether I made a mistake or tflite doesn't support int8 bilinear upsampling correctly. The details are as follows.  2. Code I aim to do 8bit quantization to the network below, which has a bilinear upsample branch  I had done FP32 training before, then I loaded FP32 checkpoint and did Quantizationaware training like below  the QAT model was correct, and the performance in validation set was closed to FP32's. Then I converted QAT model to tflite  I loaded tflite model and evaluated as below, but the performance decrease significantly.    3. Failure after conversion the conversion is successful, but the int8 tflite model has a low accuracy performance.  5. (optional) Any other info / logs the tflite model is below, though there are some unexpected op before bilinear upsampling because of the dynamic input Tensor, I thought it has nothing to do with performance degradation, beacause the psnr performance is still low when the input )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,touristourist,Bilinear upsampling layer in tflite cannot be 8-bit quantized correctly," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04  TensorFlow installation (pip package or built from source):  pip   TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0  TensorFlow Model Optimization version (installed from source or binary): pip  0.7.2  Abstract Hello, I want to do full 8bit quantization(input, weight all 8bit) to the network with a bilinear upsampling layer. The fake QAT result in validation set is closed to FP32 result, but when I converted the QAT model to full 8bit tflite model, the result in validation set decreased significantly, almost no accuracy. So I wonder whether I made a mistake or tflite doesn't support int8 bilinear upsampling correctly. The details are as follows.  2. Code I aim to do 8bit quantization to the network below, which has a bilinear upsample branch  I had done FP32 training before, then I loaded FP32 checkpoint and did Quantizationaware training like below  the QAT model was correct, and the performance in validation set was closed to FP32's. Then I converted QAT model to tflite  I loaded tflite model and evaluated as below, but the performance decrease significantly.    3. Failure after conversion the conversion is successful, but the int8 tflite model has a low accuracy performance.  5. (optional) Any other info / logs the tflite model is below, though there are some unexpected op before bilinear upsampling because of the dynamic input Tensor, I thought it has nothing to do with performance degradation, beacause the psnr performance is still low when the input ",2022-07-23T14:49:20Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TF 2.5,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56871,Hi  ! Could you please post the above code in a gist along the model file.  Have you tried in 2.8/2.9 version yet? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Loss stagnates after first episode on random dataset)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  TL:DR Loss stagnated after first episode on realworld dataset. Tried same model on a random dataset. Loss stagnated as well. Seems to be related to kernel size. Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold? Test code/notebook link below. Notebook link  Context I'm trying to solve a realworld problem with a simple 1D CNN implemented in TensorFlow. However, the loss ""converged""/stayed constant after the first episode for some reason. After tinkering around with the actual dataset, I tried training the model with a randomly created dataset. And, low and behold, the loss stagnated at the exact same value as it would with the actual dataset. This seems to be related to the kernel size. However, I can't really make sense of why it results in the exact same constant loss despite being fed two completely different datasets.  Question What is the intuition behind this? Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold? Note that I get the exact same loss for a realworld dataset, so the problem isn't related to the fact that the example dataset is made up and random. Thank you for taking the t)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lucasfbn,Loss stagnates after first episode on random dataset,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  TL:DR Loss stagnated after first episode on realworld dataset. Tried same model on a random dataset. Loss stagnated as well. Seems to be related to kernel size. Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold? Test code/notebook link below. Notebook link  Context I'm trying to solve a realworld problem with a simple 1D CNN implemented in TensorFlow. However, the loss ""converged""/stayed constant after the first episode for some reason. After tinkering around with the actual dataset, I tried training the model with a randomly created dataset. And, low and behold, the loss stagnated at the exact same value as it would with the actual dataset. This seems to be related to the kernel size. However, I can't really make sense of why it results in the exact same constant loss despite being fed two completely different datasets.  Question What is the intuition behind this? Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold? Note that I get the exact same loss for a realworld dataset, so the problem isn't related to the fact that the example dataset is made up and random. Thank you for taking the t",2022-07-22T20:25:10Z,stat:awaiting response type:bug comp:apis TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56867,"In my opinion is more related to the activation function than with the loss... probably all the relus are dead... I can reproduce your issue perfectly, but changing the activation function to `""selu""` every works fine","Indeed, this seems to be the reason. Thank you for the hint and the quick response! Any intuition behind why a greater kernel size causes a gradient update that zeroes out all neurons? I can't get my head around that yet. "," I believe that at this point we can only speculate over why this might happen... but there is no correct answer... for example, given your code, changing the `kernel_inizializer` to something that is not the default, choice, for example `kernel_initializer=""random_normal""` everything works, but if you use `kernel_initializer=""he_normal""` the learning is extremely faster, but it might be just a case, who knows At this point, considering that the problem might just be the initialization and not even the activation function, I don't think that we can do much reasoning without getting in the detail of the initializer ",", Could you please take a look at the comment provided and let us know if the issue still persists. Thank you!","Closing as this appears to be related to deep learning theory and not necessarily to TensorFlow. Thank you for your comments, . If anyone got an explanation of why this is happening, I'd still appreciate it, as I couldn't find anything in the literature. ",Are you satisfied with the resolution of your issue? Yes No
725,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Loss is significantly worse when calculated manually in train_step compared to passed into model.compile)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04.5  Mobile device _No response_  Python version Python 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Yacalis,Loss is significantly worse when calculated manually in train_step compared to passed into model.compile,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.2  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04.5  Mobile device _No response_  Python version Python 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-07-22T17:30:28Z,stat:awaiting response type:bug stale comp:keras TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56866," I was able to replicate this issue, please find the gist here and   post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!"," Thank you, I have opened a new issue at kerasteam/keras, and it actually looks like you were automatically assigned to it: https://github.com/kerasteam/tfkeras/issues/501", Thank you for the update! Please move this issue to closed status as we will track the other ticket there. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
673,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF v2.9 Failed to build from source Error: crosstool_wrapper_driver_is_not_gcc failed: error executing command )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution 20.04  Mobile device _No response_  Python version 3.8  Bazel version 5.0  GCC/Compiler version 9.4  CUDA/cuDNN version 11.2/8.1  GPU model and memory 3090Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SuperPauly,TF v2.9 Failed to build from source Error: crosstool_wrapper_driver_is_not_gcc failed: error executing command ,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution 20.04  Mobile device _No response_  Python version 3.8  Bazel version 5.0  GCC/Compiler version 9.4  CUDA/cuDNN version 11.2/8.1  GPU model and memory 3090Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-22T14:38:18Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux subtype:bazel TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56862,"Hi  ! Either you can switch to GCC 7.3 or  rebuild with below command after pointing your Cuda file location during configure.py process. `bazel build  config=cuda  per_file_copt=//tensorflow/.*\.,O0 tensorflow/tools/pip_package:build_pip_package` Ref 1, 2. Thank you!",">  Thanks for the response. I ran a 'bazel clean' and then ./configure again and I did not need to set my Cuda dir's as it finds them automatically. I tryied to compile GCC 7.2 sionce it wasn't in my repo and came up with some problems and it did not compile so i ran the line you have above and now I get this error: `INFO: Found applicable config definition build:short_logs in file /home/heales/tensorflow/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/heales/tensorflow/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:tensorrt in file /home/heales/tensorflow/.bazelrc: repo_env TF_NEED_TENSORRT=1 INFO: Found applicable config definition build:cuda in file /home/heales/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda INFO: Found applicable config definition build:cuda in file /home/heales/tensorflow/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda INFO: Found applicable config definition build:linux in file /home/heales/tensorflow/.bazelrc: copt=w host_copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++14 host_cxxopt=std=c++14 config=dynamic_kernels distinct_host_configuration=false experimental_guard_against_concurrent_changesINFO: Found applicable config definition build:dynamic_kernels in file /home/heales/tensorflow/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS INFO: Build options compilation_mode, cxxopt, and per_file_copt have changed, discarding analysis cache. INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 35421 targets configured). INFO: Found 1 target... ERROR: /home/heales/tensorflow/tensorflow/compiler/mlir/hlo/BUILD:645:11: Compiling tensorflow/compiler/mlir/hlo/lib/Dialect/lhlo/IR/lhlo_ops.: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8optexec50AE0418/bin/tensorflow/compiler/mlir/hlo/_objs/lhlo/lhlo_ops.d ... (remaining 148 arguments skipped) x86_64linuxgnugcc9: fatal error: Killed signal terminated program cc1plus compilation terminated. Target //tensorflow/tools/pip_package:build_pip_package failed to build Use verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 1769.815s, Critical Path: 239.58s INFO: 8670 processes: 3170 internal, 5500 local. FAILED: Build did NOT complete successfully`","Hi , Looks like version incompatibility.  Follow these versions For Tensorflow 2.9. Version  Bazel 5.0.0 Let us know if you still face an issue. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
746,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0 [Op:IteratorGetNext] when reading from S3 storage)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.8.110g2ea19cbb575 2.8.2  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,omrialmog,tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0 [Op:IteratorGetNext] when reading from S3 storage,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.8.110g2ea19cbb575 2.8.2  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-21T21:06:45Z,stat:awaiting tensorflower type:bug comp:data TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56855,", Could you please try using `tf.data.experimental.ignore_errors()` which would solve the problem here.  Also please take a look at this doc link for the reference. Thank you!","Hi , we can try that but its not really a permanent solution since it ignores any errors even if they are not this particular error. We dont want to always ignore errors, we want to make sure we still catch errors in the future. Do you have any tips on why this might be occurring? Thanks!",", As the error suggested, the `record 0` was corrupted. Could you please check that record which was corrupted and try to execute the code again. Thank you!",", record 0 usually indicates the iterator didn't get anything and not a corrupted error. We are thinking for some reason the iterator is not receiving any items at some point in time during the loading and thats what this error is hitting.  Another data point that supports this is that all the records are being repeated over and over forever (until the error), and by the time we get to the error the records themselves have been used over many times. The records themselves are not corrupted in this scenario otherwise they would fail the first time we load them."," Did you use compression for this dataset? If you use GZIP compression, you should use `raw_dataset = tf.data.TFRecordDataset( output_path, compression_type = 'GZIP' )`. Also can you please look into this explanation and let me know if it helps. Also can you please provide us the code you used to convert images to tfrecords.  Also, can you read data from s3 like this: `filenames = [""s3://bucketname/path/to/file1.tfrecord"",              ""s3://bucketname/path/to/file2.tfrecord""]`  `dataset = tf.data.TFRecordDataset(filenames)` Attaching the link here","Hi , Thanks for the reply with more information. We do not use any compression for the tfrecords unfortunately, we thought the same looking at other issues but we dont use any Gzip. I will try to find the code we used to do the image conversion  Im pretty sure its standard use for that. We read the data like you listed, please see the line below: train_dataset = tf.data.TFRecordDataset(filenames=filenames)","Hi, maybe you can change your code as this `raw_dataset = tf.data.TFRecordDataset(VALID_FILENAMES, compression_type=""ZLIB"")`.  Actually, my dataset are TFRecord files. It also is not the compressed file. When I did not add `compression_type=""ZLIB""`, it has the same problem as yours. But when I add `compression_type=""ZLIB""`, it works. ",Are you satisfied with the resolution of your issue? Yes No
1992,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(assertion failed: [Trying to access a placeholder that is not supposed to be executed. This means you are executing a graph generated from the cross-replica context in an in-replica context.])ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:  Yes.    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 18.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No.    **TensorFlow installed from (source or binary)**: No.    **TensorFlow version (use command below)**: 2.8.0    **Python version**: 3.9    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**: Running on CPU    **GPU model and memory**: Running on CPU    **Exact command to reproduce**:  You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_c)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Feynman27,assertion failed: [Trying to access a placeholder that is not supposed to be executed. This means you are executing a graph generated from the cross-replica context in an in-replica context.],"Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:  Yes.    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 18.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No.    **TensorFlow installed from (source or binary)**: No.    **TensorFlow version (use command below)**: 2.8.0    **Python version**: 3.9    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**: Running on CPU    **GPU model and memory**: Running on CPU    **Exact command to reproduce**:  You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_c",2022-07-21T15:18:18Z,stat:awaiting tensorflower type:bug comp:dist-strat TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56852,"Hi , Could you share the saved model to replicate the issue. Thank you!",I've uploaded a toy model in the tarball attached in the issue description.,toy_model.tar.gz,I could able to reproduce the issue with Tensorflow 2.9 on Ubuntu.  Thank you!,"Hi, Is the model which you are trying to load is trained on different Tensorflow version, if so could you please mention the version. Thanks!",The model was saved in TF 2.8,"Hi , could you give us more information about the pretrained model? Is there an assert defined in the `model` layer?","No, there shouldn't be any assert defined in any model layer. ","I did notice that if you remove the lines below, the error disappears:  So it has something to do with the model initialization under the distributed strategy scope?",That might be it. Does the model behave as expected without the dist strat scope (are the losses and metrics as expected)?,"Yes, removing the `distributed_strategy.scope` works as expected.","Sorry for extremely late reply.  Use `strategy.run` to call the model, or use `model.fit`. Here is the guide: https://www.tensorflow.org/guide/distributed_traininguse_tfdistributestrategy_with_custom_training_loops Only model creation should be done inside of the distribution scope.",Are you satisfied with the resolution of your issue? Yes No
1239,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorflowLite model run on Hexagon DSP different from CPU)ï¼Œ å†…å®¹æ˜¯ (Hi, I convert a ""resnetlike"" .tflite model(quantized by int8) and find it has a certain precision loss on hexagon compared to cpu,  then i print the output probability,  they are really different. I used to discover the ""inference_diff"" tool (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) that you guys provided and now test with it, the result is as follows:    It looks like little difference, but in general, the output probability of the quantized model are mostly 0, so this probably doesn't mean much. Then I remove the softmax operation at the end of the model, and try again:    Now the difference is much bigger. I wonder whether these are normal, and what is the approximate level of accuracy loss of the model generally. I upload these two models which named resnet_quantized.tflite.tar.gz and resnet_quantized_without_softmax.tflite. Thanks ! resnet_quantized.tflite.tar.gz resnet_quantized_without_softmax.tflite.tar.gz)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,GinHIT,TensorflowLite model run on Hexagon DSP different from CPU,"Hi, I convert a ""resnetlike"" .tflite model(quantized by int8) and find it has a certain precision loss on hexagon compared to cpu,  then i print the output probability,  they are really different. I used to discover the ""inference_diff"" tool (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) that you guys provided and now test with it, the result is as follows:    It looks like little difference, but in general, the output probability of the quantized model are mostly 0, so this probably doesn't mean much. Then I remove the softmax operation at the end of the model, and try again:    Now the difference is much bigger. I wonder whether these are normal, and what is the approximate level of accuracy loss of the model generally. I upload these two models which named resnet_quantized.tflite.tar.gz and resnet_quantized_without_softmax.tflite. Thanks ! resnet_quantized.tflite.tar.gz resnet_quantized_without_softmax.tflite.tar.gz",2022-07-21T09:01:09Z,stat:awaiting response stale comp:lite type:performance comp:lite-xnnpack TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56848,Hi  ! resnet_quantized lite model is expected to have faster inference time /lower latency compared to resnet_quantized_without_softmax.tflite. Are you saying the for both models results different behavior for CPU and DSP hexagaon delegate respectively. Could you provide the benchmarking results for both model with CPU and DSP GPU delegate . Attached relevant thread for reference. Thank you!,"> Hi  ! resnet_quantized lite model is expected to have faster inference time /lower latency compared to resnet_quantized_without_softmax.tflite. Are you saying the for both models results different behavior for CPU and DSP hexagaon delegate respectively. >  > Could you provide the benchmarking results for both model with CPU and DSP GPU delegate . Attached relevant thread for reference. >  > Thank you! Thank you very much for your kind reply :) In fact resnet_quantized_without_softmax.tflite is faster than resnet_quantized.tflite, but that's not my concern. I follow your suggestion and compile ""benchmark_model"", then test on hexagon. It seems that the printed result is the **timeconsuming** of each op of the model during the inference, while what I am confused about  is the difference of  model output **values**.  Thanks againï¼", ! Could you please look at this issue. Thank you!,"In our case, some building block, e.g. the residuallike conv. building block, may cause standalone ReLU operator in TFLite. When this kind of operators occurs in graph, the results on CPU and Hexagon are usually different.  You can also check out CC(TFLite with Hexagon delegate produces wrong results for a particular model).","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Feature Request] GELU activation with the Hexagon delegate)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (or github SHA if from source): 2.9.1 I think I'd be able to implement this myself, but wanted to see if there was any interest in including this upstream.  Most of this I'm writing out to make sure my own understanding is correct.  The problem I'd like to add support for the GELU op to the Hexagon Delegate.  The motivation for this is mostly for use with DistilBERT, which uses this activation function in its feedforward network layers.  (Also used by BERT, GPT3, RoBERTa, etc.) Adding this as a supported op for the Hexagon delegate would avoid creating a graph partition/transferring between DSPCPU each time the GELU activation function is used.  How I'd implement this GELU in TF Lite is implemented as a lookup table when there are integer inputs (here and here). This same approach could be used for the Hexagon delegate, as it has int8/uint8 data types and also supports lookup tables. I'd plan to do this by adding a new op builder in the delegate, populating a lookup table for each node as is currently done for the CPU version of the op, and then using the Gather_8  nnlib library function to do the lookup.  Possible workaround A workaround I thought of: I'm going to try removing the pattern matching for approximate GELU in MLIR, and then using the approximate version of GELU (so that using tanh and not Erf).  This will probably be slower, but should let me keep execution on the DSP. Since this will then be tanh, addition, m)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,tc-wolf,[Feature Request] GELU activation with the Hexagon delegate,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (or github SHA if from source): 2.9.1 I think I'd be able to implement this myself, but wanted to see if there was any interest in including this upstream.  Most of this I'm writing out to make sure my own understanding is correct.  The problem I'd like to add support for the GELU op to the Hexagon Delegate.  The motivation for this is mostly for use with DistilBERT, which uses this activation function in its feedforward network layers.  (Also used by BERT, GPT3, RoBERTa, etc.) Adding this as a supported op for the Hexagon delegate would avoid creating a graph partition/transferring between DSPCPU each time the GELU activation function is used.  How I'd implement this GELU in TF Lite is implemented as a lookup table when there are integer inputs (here and here). This same approach could be used for the Hexagon delegate, as it has int8/uint8 data types and also supports lookup tables. I'd plan to do this by adding a new op builder in the delegate, populating a lookup table for each node as is currently done for the CPU version of the op, and then using the Gather_8  nnlib library function to do the lookup.  Possible workaround A workaround I thought of: I'm going to try removing the pattern matching for approximate GELU in MLIR, and then using the approximate version of GELU (so that using tanh and not Erf).  This will probably be slower, but should let me keep execution on the DSP. Since this will then be tanh, addition, m",2022-07-21T01:07:38Z,stat:awaiting response type:feature stale comp:lite TFLiteHexagonDelegate,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56845,wolf ! Feel free to raise a PR.   ! Could you please look at this feature request.,"Sorry for the delay! wolf , thank you for raising this issue. Implementation w/ lookup table LGTM. We'd like to keep the pattern matching as kernels and other delegates can take advantage of the fused GELU activation information to improve performance (instead of each delegate replicating the logic to fuse the tanh/erf, add and mul ops). Feel free to raise a PR! Thanks!","No worries!  I haven't too much time to look at implementing the lookup table but will probably get a chance to start this weekend.  I'll probably have some questions once I get going. Sorry  yeah, removing the pattern matching is just a temporary workaround that I'm using for a very specific use case.  Totally agree it wouldn't make sense to remove in general.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1301,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.config.set_logical_device_configuration` not working when attempting to create multiple virtual GPUs)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflowmacos v2.8.0 tensorflowmetal v0.5.0   Custom Code No  OS Platform and Distribution macOS12.2.1  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I'm developing on a system with a single GPU (Apple M1 Pro) and trying to simulate multiple GPUs with virtual devices. Using the examples found here: https://www.tensorflow.org/guide/gpuusing_multiple_gpus I get the following output:  where I was expecting the Logical GPUs to be greater than the Physical GPU count. I have tested `tf.config.set_logical_device_configuration` for increasing Logical device count for CPUs, and this indeed _does_ work for CPU virtualisation.  Standalone code to reproduce the issue Taken from the code snippet found at: https://www.tensorflow.org/guide/gpuusing_multiple_gpus   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tallamjr,`tf.config.set_logical_device_configuration` not working when attempting to create multiple virtual GPUs,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflowmacos v2.8.0 tensorflowmetal v0.5.0   Custom Code No  OS Platform and Distribution macOS12.2.1  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I'm developing on a system with a single GPU (Apple M1 Pro) and trying to simulate multiple GPUs with virtual devices. Using the examples found here: https://www.tensorflow.org/guide/gpuusing_multiple_gpus I get the following output:  where I was expecting the Logical GPUs to be greater than the Physical GPU count. I have tested `tf.config.set_logical_device_configuration` for increasing Logical device count for CPUs, and this indeed _does_ work for CPU virtualisation.  Standalone code to reproduce the issue Taken from the code snippet found at: https://www.tensorflow.org/guide/gpuusing_multiple_gpus   Relevant log output  ",2022-07-20T10:12:32Z,stat:awaiting response type:bug stale comp:gpu TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56837,"Hi , I tried on Ubuntu20.04. I could not reproduce the issue. Its working as expected. Could you try on other OS platform ","Hi  , thanks for looking into this. I tried on a Red Hat Enterprise Linux Server / Fedora machine and it seems to work fine:  Unfortunately I do not have access to a Windows machine for testing, but I wonder if it's a problem with `tensorflowmacos` / `tensorflowmetal` specifically. Is this repo still the best place to log such an issue? I tried again locally on macOS and bumped versions of `tensorflowmacos` to be:  and used Python 3.9, but still same problem as reported before, i.e. ","Hi , It is working as expected on Windows machine also. Looks like issue with tensorflowmacos multiple GPU. Thank you!","Thanks . I see you have added ""awaiting response"" label, please can you advise of next steps if this needs to be reported elsewhere, or if this is currently being looked into now we have determined it to be a macOS specific problem? **Update** I've now posted the issue here: https://developer.apple.com/forums/thread/710890", As mentioned here this issue is specific to tensorflowmacos.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,Any update on this?
720,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Wrong gradient calculated by `tf.autodiff.ForwardAccumulator` for API `tf.optimizers.schedules.ExponentialDecay`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,eeDigitalSeal,Wrong gradient calculated by `tf.autodiff.ForwardAccumulator` for API `tf.optimizers.schedules.ExponentialDecay`,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-20T09:55:16Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56836,", I tried to execute the code with various `initial_learning_rate`, `decay_steps` and `decay_rate`. I was able to fetch the result as expected. Kindly find the gist of it here.", This bug only happened when setting `decay_rate=0.0`. You can reproduce it here,", I was able to reproduce the issue on tensorflow v2.8, v2.9, nightly. Kindly find the gist of it here.",", A value too close to 1 (e.g: 0.999) might lead to the learning rate decaying very slowly, and difficult in the optimization process. and the rates extremely close to 0 (e.g: 0.0001) can cause the learning rate to decay to very small values, potentially causing numerical issues due to underflow. A common range for decay_rate is between 0.1 and 0.9. The optimal value depends on your specific dataset and model architecture. I tried with the different **decay_rate** for `tf.optimizers.schedules.ExponentialDecay` and observed that with the `decay_rate = 0.00000000000000000000000000000000000001 ` the output was different with the another. Kindly find the gist of it here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,>  The question here is with decay_rate=0 the problem arising. Why can't it be addressed? any reason for this? may be a validation check resolves the issue.,"Since the forward and backward gradients are computed using different mechanisms, there is no guarantee they produce the same result  particularly for degenerate edgecases.  In the backward case we get a 0 * 0, but in the forward case a 0 / 0, which is why one produces NaN and the other 0.  They should be _nearly_  identical in all nondegenerate cases. Unless you have a real usecase, we will no longer respond to these kinds of reported ""issues"".",Are you satisfied with the resolution of your issue? Yes No
734,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Forward AD threw error in `tf.autodiff.ForwardAccumulator` for `tf.keras.layers.MaxPooling3D`, but backward AD succeeded with same input)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,eeDigitalSeal,"Forward AD threw error in `tf.autodiff.ForwardAccumulator` for `tf.keras.layers.MaxPooling3D`, but backward AD succeeded with same input",Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-20T07:20:12Z,stat:awaiting response type:bug stale comp:ops TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56833,", I tried to execute the code in an alternative approach and was able to execute without any errors. Kindly find the gist of it here. Also please take a look at this issue and comment for the similar error. Thank you!"," Please kindly note that this bug only happened when the  `padding=""valid""`, not `padding=""same""`. In this case,  the `pool_size = [2,1,1]` is invalid, so `MaxPooling3D` should throw `ValueError`. However now it excuted successfully when direct invocation and calculating gradient in reverse mode. ",", I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist of it here.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1931,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Different outputs at inference when different versions of TensorFlow are used to perform quantization during conversion to .tflite)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colaboratory  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): Issue with various versions of TenrorFlow  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab Perform conversion and Dynamic range quantization:  Perform inference with the generated model   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  The issue here can be reproduced by using different versions of TensorFlow to convert and quantize the model. Check below for the versions and the results: Version 2.3 result: [[0.3330792  0.27843374]  [0.40366906 0.250709  ]  [0.48135388 0.24179175]  [0.5636318  0.24119636]  [0.6400584  0.2480475 ]  [0.6005912  0.3724205 ]  [0.5628794  0.3754966 ]  [0.48778155 0.40332332]  [0.3985254  0.3810564 ]  [0.34678322 0.3575413 ]  [0.52223235 0.3023828 ]  [0.5825569  0.30930337]] Version 2.4 result: [[0.3341408  0.28175732]  [0.40583873 0.25392115]  [0.48184934 0.24735713]  [0.55686283 0.2499246 ]  [0.63230157 0.26033315]  [0.6035842  0.37444225] )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,farmaker47,Different outputs at inference when different versions of TensorFlow are used to perform quantization during conversion to .tflite," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colaboratory  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): Issue with various versions of TenrorFlow  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab Perform conversion and Dynamic range quantization:  Perform inference with the generated model   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  The issue here can be reproduced by using different versions of TensorFlow to convert and quantize the model. Check below for the versions and the results: Version 2.3 result: [[0.3330792  0.27843374]  [0.40366906 0.250709  ]  [0.48135388 0.24179175]  [0.5636318  0.24119636]  [0.6400584  0.2480475 ]  [0.6005912  0.3724205 ]  [0.5628794  0.3754966 ]  [0.48778155 0.40332332]  [0.3985254  0.3810564 ]  [0.34678322 0.3575413 ]  [0.52223235 0.3023828 ]  [0.5825569  0.30930337]] Version 2.4 result: [[0.3341408  0.28175732]  [0.40583873 0.25392115]  [0.48184934 0.24735713]  [0.55686283 0.2499246 ]  [0.63230157 0.26033315]  [0.6035842  0.37444225] ",2022-07-20T04:40:38Z,type:bug comp:lite TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56832, may you please help take a look at this issue? Many thanks!,"Hi  ! With Option A, I was getting a lot of attribute error saying ""AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no copy/astype' initially. So I made changes after merging both model and lite notebooks and got new errors for int8 quantization .  But the results in other quantization looked same.  Attached gist in 2.8, 2.9 and nightly for reference. Thank you!",Jaesung can you please check this,"I suspect even for the TFLite floating point conversion, these kind of small numerical differences can be found. There are many reasons to introduce such behaviors due to 1) conversion logic change 2) kernel logic change and so on. It is really impossible to guarantee the numerical exactness. For example, the numerical exactness is hard to achieve in (a+b) * c != a*c + b*c. Those differences seem minor and looks like the behavior is working as intended.",Hi  ! Could you reply on above comment.,"Hi   I see that you are mentioning something about the option A but I did not provide a colab to check, I just put reference code at option B to show you how simple I convert and perform inference. Apologies for that. I should have deleted the A fields. Based on  answer I understand the behavior. But I am asking both of you since I see that the results at versions 2.4 and 2.7 are exactly the same...which version of TensorFlow to use to quantize the model during conversion?? Or just upgrade TensorFlow every time there is one so I get the latest results?"," , It is always preferred to use the latest stable version, we as Tensorflow constantly improve the library with bug fixes and added features. Even though you notice the same behavior in both the mentioned versions above, it is suggested to use the latest stable version. Hope this clarifies your doubt. Thanks!",Thanks you  . I will use the latest version from now on. You can close the issue.,Are you satisfied with the resolution of your issue? Yes No
647,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RaggedTensors should have a 'name' attribute)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version 2.9.x  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,comckay,RaggedTensors should have a 'name' attribute,Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version 2.9.x  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-19T13:46:50Z,stat:awaiting response type:feature stale comp:core TF 2.9,closed,0,18,https://github.com/tensorflow/tensorflow/issues/56819,"Just so my request is a bit easier to read:  Currently, `tf.RaggedTensor` objects have no `name` attribute like other tensor objects. Further, `__slots__` for `tf.RaggedTensor` is declared, so adding attributes is not allowed either. This is an issue particularly when saving a model to be promoted into a tf serving environment. When the `SavedModelBuilder` encounters `tf.RaggedTensor` in an input signature it decomposes that ragged tensor into two dense tensors with names like `args_0` and `args_0_1`. If `tf.RaggedTensor` had a name attribute and this was respected by `SavedModelBuilder` serving models with multiple ragged inputs would be easier/possible.","  I was able to replicate the issue reported, please have a look at this gist here. Thank you!","Where do you call the loaded saved model? If you call from python, you could directly do this: ", we load our models in TF Serving and serve them that way. We do not want to have to intermix our API dependencies with TF dependencies and therefore prefer separation. We also enjoy the performance we get from TF serving and have invested a bunch in making that pattern work. ,"We in our team are also struggling this issue. We are trying to find a solution where we perhaps can overwrite some of the signatures within the model, but we have only managed to do so on the output for now. It's quite errorprone to rely on the args_0..args_x naming using tensorflow serving","The plan is to support RaggedTensor in serving so we don't need to supply individual components. That requires presence of RaggedTensor in C++ and serving. This is one of our high priority projects. Before that happens, one not pretty workaround is to add a layer in front of your model. The layer will simply accept individual components, compose them into RaggedTensor and pass the RaggedTensor to the your original model. ","> The plan is to support RaggedTensor in serving so we don't need to supply individual components. That requires presence of RaggedTensor in C++ and serving. This is one of our high priority projects. >  > Before that happens, one not pretty workaround is to add a layer in front of your model. The layer will simply accept individual components, compose them into RaggedTensor and pass the RaggedTensor to the your original model. Thanks a lot for the quick response. It sounds really good that you're planning to support RaggedTensors in TFserving. Could you perhaps provide a minimum working example of your suggested solution? I'm having a hard time trying to wrap my head around how to split of the ragged tensor inside of for example a map function in a tf.dataset :) edit  managed to break them up using the following code ğŸ‘      And then creating the raggedtensor in my model, as per your suggestion. Thank you", Any update on supporting RaggedTensor within TFServing? Thanks!,Thanks for following up. RaggedTensor for serving is an important OKR for RaggedTensor this year. We are currently planning to make preliminary investigations and designs on Q1 and hopefully finish designs and start implementations on Q2. , Hello. How is TFServing's support for RaggedTensor being progressed within the team?,"Try using CoreMLTools to convert the Tensorflow model to CoreML Model to on Verizon Tensorflow 2.14, it ends with RaggerTensor does not have 'name' when tracing the graph.", can you share an update concerning the Ragged Tensor support on ML serving and whether the OKR was met last year? Can you also comment on whether you are seeing different priorities for ragged input vs output support?,"Hi, I have reviewed the code and made some changes,changed [1] to tf.constant([1]). It is working fine for me. Please find the gist here for reference. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.," the ticket was marked as stale by the bot. I understand that previously you considered the feature important, can you provide an update on whether it is still in scope for your roadmap?", Sorry for the late reply. Unfortunately the priority shifted in the past year. I am now assigned to work on other things. Would something like https://github.com/tensorflow/tensorflow/issues/56819issuecomment1241549712 work for you? (I understand it is not ideal...) ,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1868,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation Fault when using NCCL with MultiWorkerMirroredStrategy)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu    **TensorFlow installed from (source or binary)**:     **TensorFlow version (use command below)**: 2.6.5    **Python version**: 3.8.10    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:     **CUDA/cuDNN version**:  11.6    **GPU model and memory**: V100    **Exact command to reproduce**:   Describe the problem I am using 2 VMs each running a docker container. Each container uses 1 GPU. I am trying to use TF's MutliWorkerMirroredStrategy. Currently if I choose the RING, the training successfully runs on 2 VMs (1 GPU each): com_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.RING) strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=com_options) BUT  When I switch to NCCL: com_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.NCCL) strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=com_options) I get a Segmentation fault on my second VM, and then my first VM's training also stops.  Source code / logs  When I run the training script on VM1: 20220718 18:00:15.159072: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker > {0 > VM_1_IP:6006, 1 > VM_2_IP3:6006} 20220718 18:00:15.159629: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bluepra,Segmentation Fault when using NCCL with MultiWorkerMirroredStrategy," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu    **TensorFlow installed from (source or binary)**:     **TensorFlow version (use command below)**: 2.6.5    **Python version**: 3.8.10    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:     **CUDA/cuDNN version**:  11.6    **GPU model and memory**: V100    **Exact command to reproduce**:   Describe the problem I am using 2 VMs each running a docker container. Each container uses 1 GPU. I am trying to use TF's MutliWorkerMirroredStrategy. Currently if I choose the RING, the training successfully runs on 2 VMs (1 GPU each): com_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.RING) strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=com_options) BUT  When I switch to NCCL: com_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.NCCL) strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=com_options) I get a Segmentation fault on my second VM, and then my first VM's training also stops.  Source code / logs  When I run the training script on VM1: 20220718 18:00:15.159072: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker > {0 > VM_1_IP:6006, 1 > VM_2_IP3:6006} 20220718 18:00:15.159629: ",2022-07-18T18:07:12Z,stat:awaiting response type:bug stale comp:dist-strat 2.6.0,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56812,"When I try using NCCL outside my docker containers, it works. So I think its something with the container IP's that is going wrong, but I am not sure what to change.","It seems like it is failing to establish all connections. Is there anyway you can initialize NCCL environment to try the solution to make sure if the error is caused by NCCL. Also, run the docker with `export NCCL_DEBUG=INFO` to check the NCCL logs.","What do you mean by ""initialize NCCL environment""? And where exactly does ""export NCCL_DEBUG=INFO"" go? In the docker run command? Sorry I am very new to all this.",So its something to do with containers. I stopped using Docker containers and just used NCCL with 2 VMs and it works!," , If your issue is resolved, could you please close this issue. Thanks",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
705,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(openCL delegate generates '0' and 'random' values with 'tf.stack')ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1, nightly version  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bahar-BM,openCL delegate generates '0' and 'random' values with 'tf.stack',"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1, nightly version  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-07-18T14:31:27Z,type:bug comp:lite TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56808,Hi BM ! Does it resolve in this commit too? Thank you!,"Hi, can you check it with this commit: https://github.com/tensorflow/tensorflow/commit/61014f655a18c7de906ad42cb4d95de8252ab0f5","If you want to use only GPU delegate, I would recommend to use explicit 4d shapes everywhere.  Internally every shape converted to 4d shape(and no support of 5d or higher) for GPU delegates. If you will use 4d explicitly you can omit many problems/issues that can be with non4d shapes.","Hello  and ! The new commit (61014f6) has resolved my issue with `tf.stack`. Thank you! Working with explicit 4d shapes everywhere is not always possible, because in some models we have to add multiple reshape nodes in the model structure and it compromises the performance which is very important for us. ",Are you satisfied with the resolution of your issue? Yes No
964,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build failed in Android Studio)ï¼Œ å†…å®¹æ˜¯ (Tensorflow is a great work. I want to study arm64 computation from it. What is a good IDE to read TensorFlow Source Code. I try to build it with cmake in Android Studio. How to fix the error? `Execution failed for task ':lite:configureCMakeDebug'. > [CXX1402] /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/src/main/cpp/CMakeLists.txt debug|arm64v8a : Target all_microkernels:: produces multiple outputs /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/.cxx/Debug/482v1h6t/arm64v8a/_deps/xnnpackbuild/CMakeFiles/all_microkernels.dir/src/f16f32vcvt/gen/vcvtscalarx1.c.o, /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/.cxx/Debug/482v1h6t/arm64v8a/_deps/xnnpackbuild/CMakeFiles/all_microkernels.dir/src/f16f32vcvt/gen/vcvtscalarx2.c.o,`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Yoline777,Build failed in Android Studio,"Tensorflow is a great work. I want to study arm64 computation from it. What is a good IDE to read TensorFlow Source Code. I try to build it with cmake in Android Studio. How to fix the error? `Execution failed for task ':lite:configureCMakeDebug'. > [CXX1402] /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/src/main/cpp/CMakeLists.txt debug|arm64v8a : Target all_microkernels:: produces multiple outputs /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/.cxx/Debug/482v1h6t/arm64v8a/_deps/xnnpackbuild/CMakeFiles/all_microkernels.dir/src/f16f32vcvt/gen/vcvtscalarx1.c.o, /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/.cxx/Debug/482v1h6t/arm64v8a/_deps/xnnpackbuild/CMakeFiles/all_microkernels.dir/src/f16f32vcvt/gen/vcvtscalarx2.c.o,`",2022-07-18T07:02:46Z,stat:awaiting response type:build/install stale comp:lite TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56801,Hi  ! Could you confirm that you have followed instructions from the document. Please share your  code changes along TF version and OS details for further assistance.  Thank you!,  I want to build source in Android Studio. There is my change. I modified the gradle and cmakelists of the project.  ,Hi  ! Could you please look at this issue. Thank you!," , Could you please follow the steps from the document here for the android build.", I tried this method and it builds successfully. But I want to put compile and call together so I can learn tflite framework better with breakpoints and code jumps.,"Hi , it seems like your issue is resolved, if you have no more open items please feel free to close the issue as completed.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1827,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build issue on Apple M1/M2)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1, 2.8.2, 2.7.3, 2.7.0  Custom Code No  OS Platform and Distribution macOS 12.4 (Apple M1 Pro)  Mobile device _No response_  Python version 3.9.13  Bazel version 5.2.0, 4.2.2  GCC/Compiler version Apple Clang 13.1.6  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? I'm trying to update our Spack build recipe with the latest versions of TensorFlow but I'm unable to get anything to build. I've tried multiple versions of both TF and Bazel. With TensorFlow 2.9.1 and Bazel 5.2.0 I get:  With TensorFlow 2.9.1 and Bazel 4.2.2 I get:  With TensorFlow 2.8.2/2.7.3/2.7.0 and Bazel 4.2.2 I get:  Multiple users have reported this last build issue: https://github.com/spack/spack/issues/31229.  Standalone code to reproduce the issue https://github.com/spack/spack/pull/31615 is the PR where I'm working on our build recipe. This issue can be reproduced by first cloning my PR branch:  Then, try to install TF using one of the following commands:  Unfortunately, all of these fail at the moment. You don't have to try my PR branch, the develop branch also fails. But the develop branch hasn't been updated since 2.7.0 since none of our developers have managed to get newer versions of TF to build on any system, let alone M1. Would any TF developers be interested in helping maintain this build recipe? You can see the full build recipe and patches we currently require by running `spack edit pytensorflow` or by looking at the above PR.  Relevant log output TensorFlow 2.9.1 and Bazel 5.2.0: * build log * build env)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,adamjstewart,Build issue on Apple M1/M2,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1, 2.8.2, 2.7.3, 2.7.0  Custom Code No  OS Platform and Distribution macOS 12.4 (Apple M1 Pro)  Mobile device _No response_  Python version 3.9.13  Bazel version 5.2.0, 4.2.2  GCC/Compiler version Apple Clang 13.1.6  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? I'm trying to update our Spack build recipe with the latest versions of TensorFlow but I'm unable to get anything to build. I've tried multiple versions of both TF and Bazel. With TensorFlow 2.9.1 and Bazel 5.2.0 I get:  With TensorFlow 2.9.1 and Bazel 4.2.2 I get:  With TensorFlow 2.8.2/2.7.3/2.7.0 and Bazel 4.2.2 I get:  Multiple users have reported this last build issue: https://github.com/spack/spack/issues/31229.  Standalone code to reproduce the issue https://github.com/spack/spack/pull/31615 is the PR where I'm working on our build recipe. This issue can be reproduced by first cloning my PR branch:  Then, try to install TF using one of the following commands:  Unfortunately, all of these fail at the moment. You don't have to try my PR branch, the develop branch also fails. But the develop branch hasn't been updated since 2.7.0 since none of our developers have managed to get newer versions of TF to build on any system, let alone M1. Would any TF developers be interested in helping maintain this build recipe? You can see the full build recipe and patches we currently require by running `spack edit pytensorflow` or by looking at the above PR.  Relevant log output TensorFlow 2.9.1 and Bazel 5.2.0: * build log * build env",2022-07-18T04:10:03Z,stat:awaiting tensorflower type:build/install subtype:macOS TF 2.9,closed,0,31,https://github.com/tensorflow/tensorflow/issues/56799,"Hi , Could you try the workaround mentioned on the similar thread here. Thank you!","Thanks , specifying `macos_sdk_version` slightly changes the error message:  Full logs: * build log * build env Any idea where to go next?","Also, specifying `macos_sdk_version` actually breaks TF 2.8.2/Bazel 4.2.2: ","Hi , Isnâ€™t  binary Tensorflow installation helpful. "," unfortunately not. Spack is a fromsource package manager. We try to build all of our software from source whenever possible. Many of our users are at HPC centers building for esoteric supercomputer architectures like aarch64 and power9 or for the Cray programming environment. Even if binaries are available, they won't be optimized for the hardware users are running on. Spack automatically detects the microarchitecture users are building on and injects optimization flags based on compiler version. This results in significant speedups compared to the unoptimized binaries provided by pip or conda.",Facing the same issue ,"Just wanted to update this and say that I'm still having issues, this time with: * TensorFlow 2.12.0 * macOS 13.2.1 (Apple M2 Pro) * Python 3.10.10 * Bazel 5.3.0 * Apple Clang 14.0.0 * Protobuf 3.17.3 New error message: ","Hi, Could you please try with the protobuf 3.20.3 or higher since that is the minimum version requirement for Tensorflow 2.12. You can check any version from below. `'protobuf>=3.20.3,<5.0.0dev,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5'.`","Thanks, I think https://github.com/spack/spack/pull/36263 is correct now. Unfortunately we're having other concretization problems now due to conflicting gast requirements between TF and scipy. Still trying to see if we can solve that, will report back here if we ever get that working.  is also working on this with me, he may report back sooner.","Okay, using protobuf 4.21.7, I still see exactly the same issue:  Full dependency graph:    Click to expand   Build logs: * spackbuildout.txt * spackbuildenvmods.txt"," is also reporting the above protobuf issue on Linux, so that specific issue isn't specific to Apple Silicon. But once we solve that, I imagine we'll be back to the original Apple Silicon issue. P.S. The specific line of code giving us trouble was added by google in https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38, maybe they can comment.","Can you please verify that the actual used protobuf version in bazel is 3.21.x? Note, protobuf has many different occurrences in the build and has a confusing versioning model (it is a very long story why). Basically, protoc (protobuf compiler) used for build is 3.21.9, which corresponds to protobuf runtime (pip python package) 4.21.9. Note, there is no 4.x version for protoc, so  you using `protobuf 4.21.7` suggests me that it points to the pip package. The error message you see (`no such target '//:well_known_types_py_pb2_genproto'`) is comming from `protoc` part, and the fact that this target is missing should mean that you are depending on an older version. The simples way to check the actually resolved version of protoc in your build would be: `bazel run //:protoc  version`",Running:  gives:  This matches the protobuf 3.21.12 (C++) and 4.21.7 (Python) libraries I was expecting.,"Nit: Ideally the major and incremental versions should match (3.21.12 coresponds to 4.21.12). I don't think that causes your problem though. In that case can you build just `//:well_known_types_py_pb2_genproto`?  It must exist, and your error message says it does not. For what it worth, TF builds uses 3.21.9 but again, I do not anticipate differences like that between 3.21.9 and 3.21.12.  Also, can you please check what's inside `/private/var/folders/jv/cgkfvslj6nq1l7cw0c8c_8gm0000gn/T/spackzlw44jee/73416f5c6b515a686e32ac2e3981f9d3/external/com_google_protobuf/version.json`? (the actual path may be different if you rebuilt your project (different hashes for the cache path). For example in my build the `version.json` file looks like the following: ",Built with protobuf 4.21.9 (Python) and 3.21.9 (C++). Running:  gives the same error:  I could not find any `version.info` file anywhere recursively in `/private/var/folders/jv/cgkfvslj6nq1l7cw0c8c_8gm0000gn/T/spackugzi7i1y/_bazel_Adam/f4163c14beee47247db8eb86c7c4a797`.,version.info or version.json?,version.json file should be in the same directory as the `external/com_google_protobuf/BUILD.bazel` build file it complains about in the error message. I basically mean this file: https://github.com/protocolbuffers/protobuf/blob/main/version.json The healthy output of `$ bazel build //:well_known_types_py_pb2_genproto` should look like the following: ,"Sorry, I meant `version.json`. The directory in question does not contain this file:  At the moment, I'm just installing the autogenerated tarball from GitHub. Should I instead be checking out the repo and any submodules with git, or does that not matter?","It should be pulled similarly to how TF itself does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzlL455 The patch applied is for python 3.11 support (so it should work on 3.10 python without the patch. `tf_http_archive` is just a wrapper around `http_archive()` with a tf mirror for dependencies, so is also optional. `system_build_file`  that is a legacy irrelevant thing (so does not matter here).",I tried letting bazel download and install its own protobuf during the TF install but that didn't work either. Same error msg as before.,"Bazel does not install protobuf python runtime dependency (if we are talking about protobuf pip package, and which is the one which is supposed to be >= 3.20.3, e.g. this one: https://pypi.org/project/protobuf/4.21.9/). The build just assumes that it is already installed. Our CI makes sure that all the python packages arepreinstalled in the build environment before the build even starts. On the other hand, bazel is responsible for downloading protoc (i.e. generaitontime dependency on protobuf). That one must come from bazel (the way I described in my previous comment). The build does not really have a chance to work otherwise. So i'm confused.  What exactly is your build environment? How exactly is protoc and protobuf pypi package are supplied to your build environment? If inside your bazel cache directory (`external/com_google_protobuf`) there are only a few files, then something is fundamentally wrong with your setup. This is how that cached dir supposed to look like:  Please do not try to build it by providing protoc in some custom way, as it will most likely give you a lot of frustration, and even if you manage to build it that way, it will super likely break again on any significant update on TF/proto side anyways.","Progress! I started removing components from our build system one by one and discovered a:  environment variable that was the culprit! This was introduced way back in https://github.com/spack/spack/pull/16258 in order to allow us to build against externally installed protobuf to solve a different issue with horovod. Removing it probably breaks horovod, but let's ignore that for now. This gets me to a new error:  Full build log: spackbuildout.txt > What exactly is your build environment? I'm a developer for the Spack package manager. We have a rather complex build recipe for TF and its dependencies. In https://github.com/spack/spack/pull/36263, I'm attempting to update this build recipe with the latest versions of TF. That's when we encountered this issue. I realize that your team might not be interested in supporting ""yet another package manager"", but Spack is pretty much the de facto package manager in use at every national lab and HPC center around the world. We have a lot of unique needs (aarch64 and ppc64le support, airgapped networks that prevent downloading dependencies, security requirements, etc.) that make our build recipes quite complex. If anyone from the build team would be willing to help us maintain these recipes we would be eternally grateful.","This bug needs to change its name. It is formulated as a generic build issue on Apple M1/M2 architecture, when it turns out to be a build issue in a completely custom environment for an independent (Spack) product. To be completely honest, this bug needs to be in Spack's repository, not TF repository. I do understand and can relate to the importance of Spack and you trying to make new TF work there. It is also beneficial to TF as a project to be available in as many places as possible, so thank you for doing it. The problem for us is that we don't have enough resources to properly support even the platforms that Google officially declares to support.  For the current clang error you have  the message seems to be quite self explanatory (not the case for most build errors we get here...). I don't know where exactly that option comes from, but I would search for it in both TF and your custom env. Also please make sure that the version of clang  you are using is a correct one. Unfortunately I can't suggest much here as Mac is literally the platform I know the least about (never even owned a single Apple product in my entire life).  "," is `mcpu=vortex` something Spack's compiler wrappers might be injecting? I see no mention of `vortex` in either TF or in our build recipe. I was hoping to frame this issue in a generic ""I'm having trouble building TF from source"" kind of way. If there is documentation for the recommended way to build TF from source on macOS I'm happy to follow that outside of Spack and see if I hit the same issue. I can handle any of the intricacies of Spack if I can manage to get TF to build from source outside of Spack.",Hey! Also interested in this issue. It turns out that no prebuilt wheel is available for `macos` with `arm64` right now on PyPI: https://pypi.org/project/tensorflow/files Do you know if the CI for `macos` under `arm64` is running?," Yes, unfortunately. This is going to change soon, but to give some context:  We added the flag here https://github.com/archspec/archspecjson/pull/55issuecomment1276552320, when adding initial support for M2  The flag as been reported wrong later, see https://github.com/spack/spack/issues/36481  This will be fixed soon in `archspec` and ported to Spack in `develop` and v0.19.3 It seems that people reporting `mcpu=vortex` and `mcpu=cyclone` etc. are using clang from `homebrew`, while latest AppleClang is moving towards the ""saner"" `mcpu=applem2`.","Also, this issue should be specific with Apple M2. For Apple M1 the flag has been stable since a while `mcpu=applem1`.",Thanks for the fix ! This gets me further through the build before a new failure:  Full build log is too large to upload here.,"We managed to make it through the protobuf issues, but now I'm back to the issue from CC(Build error: Exec failed due to IOException: xcrun failed with code 1.):  Not sure if this is relevant, but I have the following set: ","Managed to get 2.13.0 to build from source with the help of https://github.com/tensorflow/tensorflow/issues/60179issuecomment1491238631. I'll try to get the Spack recipe working, but this is no longer a TF issue. Thanks for the help everyone!"
677,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GPU much slower than CPU for text processing using tensorflow-macos)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version TF 2.8  Custom Code Yes  OS Platform and Distribution MacOS 12.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vgkortsas,GPU much slower than CPU for text processing using tensorflow-macos,Click to expand!    Issue Type Performance  Source source  Tensorflow Version TF 2.8  Custom Code Yes  OS Platform and Distribution MacOS 12.4  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-07-15T02:03:38Z,stat:awaiting response stale comp:gpu type:performance TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56782,", I tried to execute the `Text classification with an RNN` on colab with both gpu, cpu and was able to execute without any issues. Kindly find the gist of it here.","  thanks for executing it. The problem with the GPU is when trying to run it locally on the MacBook, not on colab. ", GPU kernels are way more slower than CPU cores but the main advantage of GPU is that you can run thousands of threads simultaneously. On CPU you're limited by number of cores and besides even if M1 has 8 cores only 4 of them can work simultaneously. Please take a look at the explanation here," Thank you very much for your response! I am aware of the post in StackOverflow.  I have some questions/comments: 1. Did you have the chance to run the Text classification with an RNN in an M1based MacBook and did you also observe what I saw? 2. When running the Text classification with an RNN, the GPU is not fully utilized 3. I do not see the same issue when I run a notebook with CNNs. The GPU is fully utilized and is 34 times faster then the CPU.  4. In the case of sequence models, when I am using the GPU it takes more than 1 hour/epoch, while when I am using the CPU only it takes around 250 sec/epoch. That is really not expected. 5. I do not see the same issue when I run the notebooks using NVIDIA GPUs or the Colab.  All these make me to believe that there must be  a problem with the tensorflowmetal.", I agree. This issue is specific to M1based MacBook. Please post this issue here. Thanks!,  Thanks for your response! The repository in the link you attached has been archived by the owner and is now readonly. Is there any other link I can post MacOS related TensorFlow issues?, Please post the issue here. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
762,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error during fit on distributed dataset with multiple GPUs. ""ValueError: When providing a distributed dataset, you must specify the number of steps to run."")ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Linux Debian 4.19.1943  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7 / 8.3  GPU model and memory Tesla V100PCIE32GB x2  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pimorandi,"Error during fit on distributed dataset with multiple GPUs. ""ValueError: When providing a distributed dataset, you must specify the number of steps to run.""",Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Linux Debian 4.19.1943  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.7 / 8.3  GPU model and memory Tesla V100PCIE32GB x2  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-14T15:17:12Z,type:bug comp:dist-strat TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56773,Hi  ! You can just add steps_per_epoch in model.fit to resolve this issue. Attached relevant thread  for reference.  Thank you!,"Thank you for your response  I just tried to add the steps_per_epoch argument to the fit method, now it looks like this: h = mdl.fit(         train_dataset,          validation_data=val_dataset,         verbose=0,         callbacks=[tb_callback],          epochs=7,         batch_size=batch_size,         steps_per_epoch=50         )  But the error still occurs: Traceback (most recent call last):   File ""main.py"", line 79, in      h = mdl.fit(   File ""/usr/local/lib/python3.8/distpackages/keras/utils/traceback_utils.py"", line 67, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/usr/local/lib/python3.8/distpackages/keras/engine/data_adapter.py"", line 755, in _validate_args     raise ValueError(""When providing a distributed dataset, you must "" ValueError: When providing a distributed dataset, you must specify the number of steps to run.  To better give the context I must say that I execute the code with a singularity container, I don't know if this adds any clue.","Update: Just found out that also the argument validation_steps needs to be set, now the training starts and does not give any error. The problem now is that, no matter how many epochs and how many steps I set, the fit method performs only one iteration and then stops without any error. Any advice on how to debug this?"," ! Thanks for the update . Could  you change verbose=0 to verbose=1 . It might be happening because of following conditions. 1. You might be using small dataset  or model is going through complete dataset in one swap. ( Reduce the batch size ,decide epochs and other parameter upon that) 2. Proper image processing through augmentation. ( Keras image generator, or layer.augmentation) 3. Model is not robust enough ( use more hidden layers with proper activation function / weights from a Pretrained model) For debugging , You can use Tensorboard or debugger in model.fit operation through callback to see origin of issue. You can also use a custom callback which mentions that it won't stop at certain epoch or accuracy has been achieved. Attached relevant thread for reference. 1 , 2 Thank you!","Thank you for the hint  ! I have done some experiments changing the steps_per_epoch argument, following are the logs during and after the fit: Here, steps_per_epoch is set to 10_000 which is a number not related to the actual size of the dataset  In this experiment, steps_per_epoch is set to the exact number  And here is multiplied by 10  The epoch period in these three experiments stays the same even if the number of steps varies a lot. What's more strange is that only the first epoch is performed even if the total number of epochs is 10. To further monitor the experiments I used the tensorboard callback, which plotted only one dot in the loss graphs of the scalar sections. Any idea on this?",Hi  ! Could you please look at this issue. Thank you!,"Just found out that the datasets have to be able to provide steps_per_epochs * epochs rounds of data. Now the code to create the datasets looks like this:  With these changes the code works as expected, thank you",Are you satisfied with the resolution of your issue? Yes No,Why not adaptively traverse to the end like dataset instead of adding a step parameter.  
1844,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error Tensorflow lite model -android studio)ï¼Œ å†…å®¹æ˜¯ (I'm trying to import a tensorflow lite model ( inception V3 model for detecting a lesions in a retinal images) into my android studio. I've followed a tutorial . i got this erreur , i don't know how to solve it . my input image : !0f96c358a250 i got this error  java.lang.ArrayIndexOutOfBoundsException             at android.graphics.Bitmap.checkPixelsAccess(Bitmap.java:1706)             at android.graphics.Bitmap.getPixels(Bitmap.java:1647)             at com.example.micro_model.MainActivity.classifyImage(MainActivity.java:133)             at com.example.micro_model.MainActivity$2.onManagerConnected(MainActivity.java:225)             at com.example.micro_model.MainActivity$1.onClick(MainActivity.java:192)             at android.view.View.performClick(View.java:6256)             at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1119) here is my code :  **public class MainActivity extends AppCompatActivity {     ImageView imageView;     Button button;     TextView result;     int imageSize=299;     private static final String TAG = ""MainActivity"";     static {         if (OpenCVLoader.initDebug()) {             Log.d(TAG, ""opencv not load"");         } else {             Log.d(TAG, ""opencv loaded: "");         }     }     public void classifyImage(Bitmap image)     {         try {         ConvertedModel model = ConvertedModel.newInstance(getApplicationContext());         TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 299, 299, 3}, DataType.FLOAT32);         int width =image.getWidth();         int height =image.getHeight();         )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yasminemrad,Error Tensorflow lite model -android studio,"I'm trying to import a tensorflow lite model ( inception V3 model for detecting a lesions in a retinal images) into my android studio. I've followed a tutorial . i got this erreur , i don't know how to solve it . my input image : !0f96c358a250 i got this error  java.lang.ArrayIndexOutOfBoundsException             at android.graphics.Bitmap.checkPixelsAccess(Bitmap.java:1706)             at android.graphics.Bitmap.getPixels(Bitmap.java:1647)             at com.example.micro_model.MainActivity.classifyImage(MainActivity.java:133)             at com.example.micro_model.MainActivity$2.onManagerConnected(MainActivity.java:225)             at com.example.micro_model.MainActivity$1.onClick(MainActivity.java:192)             at android.view.View.performClick(View.java:6256)             at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1119) here is my code :  **public class MainActivity extends AppCompatActivity {     ImageView imageView;     Button button;     TextView result;     int imageSize=299;     private static final String TAG = ""MainActivity"";     static {         if (OpenCVLoader.initDebug()) {             Log.d(TAG, ""opencv not load"");         } else {             Log.d(TAG, ""opencv loaded: "");         }     }     public void classifyImage(Bitmap image)     {         try {         ConvertedModel model = ConvertedModel.newInstance(getApplicationContext());         TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 299, 299, 3}, DataType.FLOAT32);         int width =image.getWidth();         int height =image.getHeight();         ",2022-07-14T12:38:06Z,stat:awaiting response type:support stale comp:lite TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56770,"Hi  ! Sorry for the late response.  Could you make the changes as below and let us know  Attached relevant threads for reference. 1 , 2  Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU pod  v4 ""RuntimeError: TPU cores on each host is not same."" )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version Python 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,naumanjaved,"TPU pod  v4 ""RuntimeError: TPU cores on each host is not same."" ",Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version Python 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-13T19:45:58Z,stat:awaiting response type:bug stale comp:tpus TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/56760,", I ran the provided code and faced a different error, Kindly find the gist of it here and share all dependencies to replicate the issue. Thank you! ",Thanks for looking into this. The error I am describing seems to be specific to a TPU v4 VM running version tpuvmtf2.9.1podv4 and only occurs with a V416 configuration or above.  I am not sure how to link a TPU VM pod to the collab notebook to recreate the issue exactly  any recommendation for how to go about doing this? I believe that the HTTP error in the linked collab notebook is resulting from there not being an attached TPU pod(unless I am missing something).  Thanks! ,Hello! Just wondering if there are any recommendations for this error. Thanks so much! ,"Hi, As per the guide here, make sure you setup the TPU correctly https://cloud.google.com/tpu/docs/v4usersguide. In your TPU custer resolver,` resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')` for the argument tpu, could you try mentioning different values  from below and see if that resolves your issue. ` tpu | A string corresponding to the TPU to use. It can be the TPU name or TPU worker gRPC address. If not set, it will try automatically resolve the TPU address on Cloud TPUs. If set to ""local"", it will assume that the TPU is directly connected to the VM instead of over the network.`","Thanks so much with tpu name set to local, as follows:  I receive:  20220719 19:03:28.488280: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:116] Libtpu path is: libtpu.so Missing key: 'TWIST' in 'tpuenv' (yaml) instance metadata. 20220719 19:03:30.138621: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220719 19:03:30.195013: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x678b500 initialized for platform TPU (this does not guarantee that XLA will be used). Devices: 20220719 19:03:30.195057: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): TPU, 2a886c8 20220719 19:03:30.195063: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (1): TPU, 2a886c8 20220719 19:03:30.195089: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (2): TPU, 2a886c8 20220719 19:03:30.195095: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (3): TPU, 2a886c8 20220719 19:03:30.261256: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at tpu_configuration_ops.cc:342 : FAILED_PRECONDITION: 'CLOUD_TPU_TASK_ID' not specified for a multihost job. Traceback (most recent call last):   File ""test.py"", line 13, in      tf.tpu.experimental.initialize_tpu_system(resolver)   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/tpu/tpu_strategy_util.py"", line 110, in initialize_tpu_system     output = _tpu_init_fn()   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/eager/function.py"", line 2453, in __call__     return graph_function._call_flat(   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/eager/function.py"", line 1860, in _call_flat     return self._build_call_outputs(self._inference_function.call(   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/eager/function.py"", line 497, in call     outputs = execute.execute(   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/eager/execute.py"", line 54, in quick_execute     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error: 'CLOUD_TPU_TASK_ID' not specified for a multihost job. 	 [[{{node configure_tpu_host/_2}}]] [Op:__inference__tpu_init_fn_4]"," , Did you try specifying grpc address or by leaving the tpu="""" for it to identify the available TPU.","Using the grpc address leads to the code stalling at ""starting server at target""  Leaving tpu ="""" leads to   Looks similar to  CC(Cloud TPU VM with v332 fails while connecting to the cluster)",", AFAIK you will get the following error if you don't set **TPU_LOAD_LIBRARY=0**. Could you please set as mentioned and try to follow the process to execute the code.  Also please have a look at this comment for the similar issue. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1289,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cant use GPU when using Tensorflow Lite Model on Custom Object Detection)ï¼Œ å†…å®¹æ˜¯ (Hi, im trying to use transferlearning on MobileNetV2 SSD for facial detection. When i run the model on the example provided for Object Detection using CPU it works well but when i use GPU, Android shows me this error: org.tensorflow.lite.examples.objectdetection E/tflite: Following operations are not supported by GPU delegate:     CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess     PACK: OP is supported, but tensor type/shape isn't compatible.     RESHAPE: OP is supported, but tensor type/shape isn't compatible.     111 operations will run on the GPU, and the remaining 46 operations will run on the CPU.  And the App stops working. The code use for converting the saved model to tflite its the following: import tensorflow as tf saved_model_dir = 'url' converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) tflite_model = converter.convert() open(""url"", ""wb"").write(tflite_model) I use libraries and functions: tfnightly export_tflite_graph_tf2.py tflite_support_nightly Thanks in advance.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JuanJoseMoralesCalvo,Cant use GPU when using Tensorflow Lite Model on Custom Object Detection,"Hi, im trying to use transferlearning on MobileNetV2 SSD for facial detection. When i run the model on the example provided for Object Detection using CPU it works well but when i use GPU, Android shows me this error: org.tensorflow.lite.examples.objectdetection E/tflite: Following operations are not supported by GPU delegate:     CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess     PACK: OP is supported, but tensor type/shape isn't compatible.     RESHAPE: OP is supported, but tensor type/shape isn't compatible.     111 operations will run on the GPU, and the remaining 46 operations will run on the CPU.  And the App stops working. The code use for converting the saved model to tflite its the following: import tensorflow as tf saved_model_dir = 'url' converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) tflite_model = converter.convert() open(""url"", ""wb"").write(tflite_model) I use libraries and functions: tfnightly export_tflite_graph_tf2.py tflite_support_nightly Thanks in advance.",2022-07-13T11:31:43Z,stat:awaiting response type:support stale comp:lite TFLiteGpuDelegate,closed,0,23,https://github.com/tensorflow/tensorflow/issues/56753,The model was trained with a batch of 64 images just if you need more info about it.  ,"Hi  ! Could you try with select ops syntax , Tensorflowlitegpu:2.8.0 or higher  dependency in build.gradle file and let us know the difference . ","Same error with those changes: org.tensorflow.lite.examples.objectdetection E/tflite: Following operations are not supported by GPU delegate:     CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess     PACK: OP is supported, but tensor type/shape isn't compatible.     RESHAPE: OP is supported, but tensor type/shape isn't compatible.     111 operations will run on the GPU, and the remaining 46 operations will run on the CPU.","can you attach the model file? On Wed, Jul 13, 2022 at 11:26 PM JuanJoseMoralesCalvo  wrote: > Same error with those changes: > > org.tensorflow.lite.examples.objectdetection E/tflite: Following > operations are not supported by GPU delegate: > CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess > PACK: OP is supported, but tensor type/shape isn't compatible. > RESHAPE: OP is supported, but tensor type/shape isn't compatible. > 111 operations will run on the GPU, and the remaining 46 operations will > run on the CPU. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Yes, here it is the tflite model with the metadata needed: detectlast.zip","The reason why PACK isn't working is probably due to the fact that you're trying to feed the same tensor twice.  On the GPU, you can't use the same tensor (underlying implementation is GPU texture) twice or more.  Thus, PACK(a, a) doesn't really work.  If you really want, you have to create a temporary variable and do something like PACK(a, b) where b = ADD(a, 1e12) (maybe 0 also works). I don't know why RESHAPE isn't working, but you can probably find out easily why it's not working in the `model_builder.cc`.  Chances are high it doesn't know how to deal with 5D tensors, but I'm not 100% sure. On Thu, Jul 14, 2022 at 4:26 PM JuanJoseMoralesCalvo  wrote: > Yes, here it is the tflite model with the metadata needed: > detectlast.zip >  > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Im a bit knew with these :D, could you guide me a little bit with the solutions? Im sorry for taking your time.","TFLite now supports 5D in selectV2, could you please try in the latest nightly version or the Tensorflow version 2.10 and confirm. More details can be found here https://github.com/tensorflow/tensorflow/blob/master/RELEASE.mdmajorfeaturesandimprovements1",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, Wanted to see if there was any update here because I am seeing the same issue with tensorflow 2.10.0 I am running tensorflow lite version 2.10.0 and for a native C++ object detection app and getting similar issues when running on the GPU with not only a custom model but a model downloaded straight from the TF2 model zoo. I have followed the instructions of taking any of the ssd models (mostly the mobilenet models) and running the export script for tf2 and then using python tf converter to create the model.tflite file.  The tflite model that doesn't given me those pack or reshape errors is the mobilenet_v1 tflite model (with metadata) downloaded from the tfhub site. Are there any steps to create a .tflite model for object detection on tensorflow 2.10.0 where the pack and reshape issues don't come up and most operations run on the GPU?,Please reopen this ticket as the issue is still present, I don't know enough about TF 2.10 or the converter.  Can this be assigned to the owner of TFLite convert?,"Not sure if this helps give a direction to look for the issue, but I found it one modifies the default pretrained config file (I was using mobilenetV2) and you change the following area to look like this: `     anchor_generator {       multiscale_anchor_generator {         min_level: 3         max_level: 3         anchor_scale: 4.0         aspect_ratios: 1.0         aspect_ratios: 2.0         aspect_ratios: 0.5         scales_per_octave: 2       }     } ` i.e change the max_level from 7 to 3 and you change  The fpn info to look like the following: `  fpn {         min_level: 3         max_level: 3         additional_layer_depth: 128 } ` i.e. change max level from 7 to 3 it didn't give me those errors and in c++ i ended up with the following GPU delegate output: ` Setting up GPU Delegate Created! Checking Delegate ERROR: Following operations are not supported by GPU delegate: CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess 78 operations will run on the GPU, and the remaining 1 operations will run on the CPU. INFO: Created 1 GPU delegate kernels. ` Not doing this or even just stepping down from 7 to 6 or 7 to 5 caused the model to create a nearest_neighbor_upsampling/stack_1 node that had 6 dimensions and appeared to be an issue or cause those PACK and RESHAPE issues: Here is a part of the model html file created from the visualize.py in the lite/tools folder: 203   ` `",Could this issue result from using cuda 11.6? Just asking because I switched back to a commit for the models repo that was back around the time of tensorflow 2.5.0 thinking this would make things work and was seeing about the same issue. Only difference was that the model that was created and then exported to a .tflite file always seemed to run everything on the GPU except the last output layer where as in 2.10.0 it always seemed to split things between the gpu and cpu.,So found out that downgrading to cura 11.2 and cudnn8.101 does resolve the issues. Any thoughts?,"   I think I have found the issue. It appears that in the first version of the the export_tflite_ssd_graph.py it called the export_tflite_ssd_graph_lib.py which and a pruning of the h_stack and w_stack tensors that had the shapes I mentioned above. The export_tflite_graph_lib_tf2.py doesn't seem to have this. I modifyed the ops.py file found in the models repo (models/research/object_detection/utils/ops.py) so that the nearest_neighbor_upsampling function which was causing the problem looked like the following:  My question here is, doing a tf.concat the same as doing a tf.stack and then a tf.reshape? Also I noticed that the docstring states talks about being TPU compatible, so is what i did above TPU compatable?",  any thoughts on this???,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
670,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(some numpy operation(FFT,FFT2d,RFFT,RFFT2d) failed when building a model )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.2  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.0/8.0  GPU model and memory NVIDIA A6000 /48G  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TWTcodeKing,"some numpy operation(FFT,FFT2d,RFFT,RFFT2d) failed when building a model ",Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.2  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.0/8.0  GPU model and memory NVIDIA A6000 /48G  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-13T04:47:11Z,stat:awaiting response type:bug comp:apis TF 2.2,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56748, Older version of TF is not actively supported. Could you upgrade to the latest version and please provide a code snippet to reproduce the issue reported here if the issue still persists? Thank you!,">  Older version of TF is not actively supported. Could you upgrade to the latest version and please provide a code snippet to reproduce the issue reported here if the issue still persists? Thank you! Yes! I upgrade the tf version to 2.9.1, which is the newest. And I have a brief test on my desktop computer whose Os is Windows and the error still exists, shown below: before FFT:(None, None, None, 32) after FFT:(None, None, None, None) 20220714 16:27:34.262034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220714 16:27:34.610717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1661 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6 Traceback (most recent call last):   File ""C:/Users/19356/Desktop/DeepRFT_tf/layers.py"", line 228, in      model = FFT_Block()   File ""C:/Users/19356/Desktop/DeepRFT_tf/layers.py"", line 224, in FFT_Block     out = ResBlock_do_fft_bench(input,32)   File ""C:/Users/19356/Desktop/DeepRFT_tf/layers.py"", line 82, in ResBlock_do_fft_bench     y = BasicConv_do(y_f,filters*2,kernel_size=1,stride=1,relu=True)   File ""C:/Users/19356/Desktop/DeepRFT_tf/layers.py"", line 34, in BasicConv_do     x = DOConv2D(filters, kernel_size, padding='same', strides=stride, use_bias=bias, groups=groups)(inputs)   File ""D:\Anaconda\envs\TF29\lib\sitepackages\keras\utils\traceback_utils.py"", line 67, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""C:\Users\19356\Desktop\DeepRFT_tf\doconv_tf.py"", line 66, in build     input_channel = self._get_input_channel(input_shape) ValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis 1 (0based) is the channel dimension, which found to be `None`.  I think it's a tf bug because it appears on both Linux and Windows","  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you!",">  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thank you! Thank you! I think I've already solved this problem by myself. I am a noob in using tensorflow, my original code was: def ResBlock_do_fft_bench(inputs,filters):     axis = 1     x = inputs     x = tf.keras.layers.Lambda(lambda v:tf.signal.rfft2d(v))(x)     y_imag = tf.compat.v1.imag(x)     y_real = tf.compat.v1.real(x)     y_f = tf.concat([y_real, y_imag], axis=axis)     y_f = tf.keras.layers.Permute((2,3,1))(y_f)     y = BasicConv_do(y_f,filters*2,kernel_size=1,stride=1,relu=True)     y = BasicConv_do(y,filters*2,kernel_size=1,stride=1,relu=False)     y = tf.keras.layers.Permute((3,1,2))(y)     y_real, y_imag = tf.split(y, 2, axis=axis)     y = tf.complex(y_real, y_imag)     y = tf.keras.layers.Lambda(lambda v:tf.raw_ops.IRFFT2D(input=v,fft_length=(1,1)))(y)     y = tf.keras.layers.Permute((2,3,1))(y)     res = BasicConv_do(inputs,filters,kernel_size=3,stride=1,relu=True)     res = BasicConv_do(res,filters,kernel_size=3,stride=1,relu=False)     return res + inputs + y And later I realize that tf can't convince the input shape during compiling but only during training or testing, so it is not right to use tf.signal.rfft2d with a simple function, so I modify my code as below: class ResBlock_do_fft_bench(keras.models.Model):     def __init__(self,filters):         super(ResBlock_do_fft_bench,self).__init__()         self.filters = filters     def build(self, input_shape):         assert len(input_shape) == 4         shape = (input_shape[1],)         self.BasicConvY1 = BasicConv_do(self.filters*2,kernel_size=1,stride=1,relu=True)         self.BasicConvY2 = BasicConv_do(self.filters*2,kernel_size=1,stride=1,relu=False)         self.BasicConvRes1 = BasicConv_do(self.filters,kernel_size=3,stride=1,relu=True)         self.BasicConvRes2 = BasicConv_do(self.filters,kernel_size=3,stride=1,relu=False)     def call(self,inputs):         if inputs.shape[1] == None:             return inputs         else:             axis = 1             x = tf.signal.rfft2d(inputs)             y_imag = tf.compat.v1.imag(x)             y_real = tf.compat.v1.real(x)             y_f = tf.concat([y_real, y_imag], axis=axis)             y_f = tf.transpose(y_f,(0,2,3,1))             y = self.BasicConvY1(y_f)             y = self.BasicConvY2(y)             y = tf.transpose(y,(0,3,1,2))(y)             y_real, y_imag = tf.split(y, 2, axis=axis)             y = tf.complex(y_real, y_imag)             y = tf.signal.irfft2d(y)             y = tf.keras.layers.Permute((2,3,1))(y)             res = self.BasicConvRes1(y)             res = self.BasicConvRes2(res)         return res + inputs + y Then it finally works, thanks a lot for your comment!", Thank you for the update! Please move this issue to closed status if it is resolved ? Thank you!,Are you satisfied with the resolution of your issue? Yes No
1710,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorflowLite model can't run on Hexagon DSP device. Failed to apply delegate: Failed: Failed to execute graph.. ERROR: Node number 557 (TfLiteHexagonDelegate) failed to invoke.)ï¼Œ å†…å®¹æ˜¯ (I convert the tfmodel to tflite, and when inference with python, it can run success.  But in DSP, using the Hexagon C API (https://www.tensorflow.org/lite/performance/hexagon_delegate), in cpu run normal, but in DSP i get the below error.  The hexagon_delegate so I used is https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run model only  include 2 conv and 1 conformer layers Log hexagon/ops/src/op_split.c:87:uneven split: 1 / 2 hexagon/src/execute.c:167:execute() failed on node id=181 err=1 hexagon/src/interface.c:1297:fail in execute_inner()  ERROR: Failed: Failed to execute graph.. ERROR: Node number 139 (TfLiteHexagonDelegate) failed to invoke. model can find here. encoder_6.zip python code as this, and is running in tf2.4.0, the dsp's tf is 2.4.0 too. import tensorflow as tf import sys import os import numpy as np tfmodel=sys.argv[1]'temp.tflite' wav_path='../../asr/BAC009S0764W0121.wav' interpreter = tf.lite.Interpreter(model_path=tfmodel) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_wav = np.random.rand(8000).astype(np.float32) interpreter.set_tensor(input_details[0]['index'], input_wav) interpreter.invoke() output_data = interpreter.get_tensor(output_details[0]['index']).astype(np.float32))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,sdkdzq1,TensorflowLite model can't run on Hexagon DSP device. Failed to apply delegate: Failed: Failed to execute graph.. ERROR: Node number 557 (TfLiteHexagonDelegate) failed to invoke.,"I convert the tfmodel to tflite, and when inference with python, it can run success.  But in DSP, using the Hexagon C API (https://www.tensorflow.org/lite/performance/hexagon_delegate), in cpu run normal, but in DSP i get the below error.  The hexagon_delegate so I used is https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run model only  include 2 conv and 1 conformer layers Log hexagon/ops/src/op_split.c:87:uneven split: 1 / 2 hexagon/src/execute.c:167:execute() failed on node id=181 err=1 hexagon/src/interface.c:1297:fail in execute_inner()  ERROR: Failed: Failed to execute graph.. ERROR: Node number 139 (TfLiteHexagonDelegate) failed to invoke. model can find here. encoder_6.zip python code as this, and is running in tf2.4.0, the dsp's tf is 2.4.0 too. import tensorflow as tf import sys import os import numpy as np tfmodel=sys.argv[1]'temp.tflite' wav_path='../../asr/BAC009S0764W0121.wav' interpreter = tf.lite.Interpreter(model_path=tfmodel) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_wav = np.random.rand(8000).astype(np.float32) interpreter.set_tensor(input_details[0]['index'], input_wav) interpreter.invoke() output_data = interpreter.get_tensor(output_details[0]['index']).astype(np.float32)",2022-07-13T02:58:20Z,stat:awaiting tensorflower type:support comp:lite TF 2.4 TFLiteHexagonDelegate,closed,0,15,https://github.com/tensorflow/tensorflow/issues/56747,"when i convert it to tflite, it has warnning: like this, WARNING:absl:Importing a function (__inference_inference_101322) with ops with custom gradients. Will likely fail if a gradient is requested. convert code and saved model as this  Documents.zip","I used the  conv + transfomer, to do the same work, it can run success. so the error can find in some conv op  **Another thing** the infer result is differ between CPU and DSP. as this !image",Hi  ! Could you confirm that your model follows  8bit symmetric quantization spec. In case It finds any other unsupported op it will back to CPU. Can you give it a try in 2.8/2.9 version too?  It would be handy to have device details too.  Thank you!,"1. The convert code uesd the Convert using float fallback quantization in https://www.tensorflow.org/lite/performance/post_training_integer_quant converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] convert.py is here https://github.com/tensorflow/tensorflow/files/9098623/Documents.zip 2. I have tried in 2.9, the error is diff 2.9 is  `Log hexagon/ops/src/op_split.c:87:uneven split: 1 / 2 hexagon/src/execute.c:167:execute() failed on node id=35 err=1 hexagon/src/interface.c:1297:fail in execute_inner()  ERROR: Failed: Failed to execute graph.. ERROR: Node number 95 (TfLiteHexagonDelegate) failed to invoke. ` 2.4 is `Log hexagon/ops/src/op_split.c:87:uneven split: 1 / 2 hexagon/src/execute.c:167:execute() failed on node id=181 err=1 hexagon/src/interface.c:1297:fail in execute_inner()  ERROR: Failed: Failed to execute graph.. ERROR: Node number 139 (TfLiteHexagonDelegate) failed to invoke. ` 3.  device details device is the internal version **/bigsur/bigsur:11/RQ3A.211001.001/1.130.72.3290020220622:userdebug/testkeys > Hi  ! Could you confirm that your model follows 8bit symmetric quantization spec. In case It finds any other unsupported op it will back to CPU. Can you give it a try in 2.8/2.9 version too? >  > It would be handy to have device details too. Thank you!",Ok  ! Thanks for the update.   ! Could you look at this issue. Thank you!,"Hi   5dd4a61d6ed82f4f04f1d748d0c9e59a6e701b55 and 73cc8621b9c33424919d773f480d0b744c38c6c5 are in to fix the aforementioned issues in hexagon delegate. You may pull the latest code and verify on your side. Let me know if you have further questions. Thanks, Weiyi",Are you satisfied with the resolution of your issue? Yes No,"Thanks! I have built the latest version, the tflite model can run on the device, but the CPU usage is very high ï¼ˆ100%), I confirmed it is running on the CPU. Which is not what I originally intended. I expect the supported operators run on DSP and unsupported operators run on CPU, not all operators run on CPU. > Hi  >  > 5dd4a61 and 73cc862 are in to fix the aforementioned issues in hexagon delegate. You may pull the latest code and verify on your side. >  > Let me know if you have further questions. Thanks, Weiyi","Hi , > I confirmed it is running on the CPU. Which is not what I originally intended. Could you share the detailed logs, or the steps to reproduce it? Or you might use the inference diff tool to reproduce. For reference, I run the inference diff tool against your model (encoder_6.zip) with hexagon and shows most of the nodes are delegated to hexagon:  The test interpreter is initialized with 136 nodes delegated out of 139 nodes by hexagon delegate. Thanks!","Hi,   With the model(encoder_6.zip), the detailed logs as  The loaded so is libcdsprpc.so, which different with libadsprpc.so in your log. And the device is from qualcomm. My Hexagon libraries is generated from https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run, Is it the same?",The hexagon libraries are good. From your log > hexagon/ops/src/op_split.c:87:uneven split: 1 / 2 This should be fixed in 5dd4a61d6ed82f4f04f1d748d0c9e59a6e701b55. Could you check whether your code is up to date? Thanks!,"I have used the inference diff tool to reproduce. The result is same to your log. Thanks! But when the number of Conformer layers is four, a new error is happened. While the CPU can run success. It seems that there is a big gap between the calculation result of the middle layer between the hexagon and the CPU. Model is here. tmp.aa.zip tmp.ab.zip  Could you give me a hand. Thanks!","Hi  sorry for the delay. For this model, the issue comes from the softmax node with a too large quantization range. See hexagon nnlib for the restrictions. Could you check your model quantization and see if it's expected?","Hi , I have trained several different layers of the conformer model, and such errors will not occur in layers 1, 2 and 3. Therefore, I use netron to check the model quantization of the fourth softmax. The message as blow, !image I guess the error may be caused by excessive cumulative error of TfLiteHexagonDelegate. The convert code uesd the Convert using float fallback quantization in https://www.tensorflow.org/lite/performance/post_training_integer_quant, converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.representative_dataset is the feature extraction from training data.",Are you satisfied with the resolution of your issue? Yes No
1620,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([`tf.data` - `from_generator()` and `.interleave()`] - Funky Behavior)ï¼Œ å†…å®¹æ˜¯ (We are trying to combine `N_DATASETS_TO_INTERLEAVE` `from_generator()` Datasets using dataset Interleaving. And it seems that it's absolutely not working. For some reason, it looks like the print is happening only once. Which could be due to some form of autograph running, but we tried everything we could to stop that, but nothing I could think of works. You will say, it's just a `print()` what's the problem here. The problem is that if you execute any python code in this `from_generator()` it seems to be executed only once. Google Colab: https://colab.research.google.com/drive/1WFvSvU8CCc1A1bfVbUb6ck_EzvE9o3sS?usp=sharing  Output:  Why this matter ? We are trying to read and interleave multiple files that have a funky format. No native TF OPs can help to do that. We need to use a generator because the files are massive (dataset 100GB+), we can't load the whole file at once. We need to use interleaving because we have multiple files to interleave together. Annnnd ... As you can see ... The python part of this code is only executed once. We would expect to see (`[INFO] Calling Interleave Fn` printed `N_DATASETS_TO_INTERLEAVE` times).  My hunch is that `tf.data.from_generator()` does not allow python generator. Only pure tensorflow generators. Any idea how to maybe combine `tf.py_function` in a generator fashion ?  in case you have any idea ;) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DEKHTIARJonathan,[`tf.data` - `from_generator()` and `.interleave()`] - Funky Behavior,"We are trying to combine `N_DATASETS_TO_INTERLEAVE` `from_generator()` Datasets using dataset Interleaving. And it seems that it's absolutely not working. For some reason, it looks like the print is happening only once. Which could be due to some form of autograph running, but we tried everything we could to stop that, but nothing I could think of works. You will say, it's just a `print()` what's the problem here. The problem is that if you execute any python code in this `from_generator()` it seems to be executed only once. Google Colab: https://colab.research.google.com/drive/1WFvSvU8CCc1A1bfVbUb6ck_EzvE9o3sS?usp=sharing  Output:  Why this matter ? We are trying to read and interleave multiple files that have a funky format. No native TF OPs can help to do that. We need to use a generator because the files are massive (dataset 100GB+), we can't load the whole file at once. We need to use interleaving because we have multiple files to interleave together. Annnnd ... As you can see ... The python part of this code is only executed once. We would expect to see (`[INFO] Calling Interleave Fn` printed `N_DATASETS_TO_INTERLEAVE` times).  My hunch is that `tf.data.from_generator()` does not allow python generator. Only pure tensorflow generators. Any idea how to maybe combine `tf.py_function` in a generator fashion ?  in case you have any idea ;) ",2022-07-13T02:36:29Z,stat:awaiting response type:bug stale comp:data TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56746,I think I found the problem:  So the question is: how to avoid autographing my `hello()` function above.,Looks like I found one way to force `autograph` to not execute: `tf.data.experimental.enable_debug_mode()`  do you know any other way ? It's kind of weird to be forced to use debug mode ... ," I was able to replicate the issue on colab, please find the gist here. Thank you!","Unfortunately I don't know how to solve this. Dataset methods taking callables like `map` and `interleave` always run the callable in graph mode unless debug_mode is enabled, as you noticed. I am not familiar with StructuredFunctionWrapper but tf.data.Dataset seems to use it to always run the function in graph mode unless debug_mode is used. I tried to modify the example in the first post, to create an interleave_fn that would use `tf.py_function` to run a function eagerly. It failed with the cryptic error ""Could not find callback with key=pyfunc_5 in the registry"". I'm not sure what the issue was  , any ideas how to run an interleave_fn eagerly?  as a workaround, it might be possible to define a custom iterator to pass to `tf.data.Dataset.from_generator`, where the `iterable.__next__` method calls `tf.py_function`. This would allow you to run Python code from the iterable passed to `tf.data.Dataset.from_generator`.","> ""Could not find callback with key=pyfunc_5 in the registry Oh yes ! I had the exact same problem, in many ways actually. The way to ""avoid"" this is to have `block_length=sys.maxsize` (max int). Which is making the whole behavior equivalent to a concatenate, and no more interleaving... Looks like it's failing to fetch a previously define dataset ""by key"". >  as a workaround, it might be possible to define a custom iterator to pass to tf.data.Dataset.from_generator, where the iterable.__next__ method calls tf.py_function. This would allow you to run Python code from the iterable passed to tf.data.Dataset.from_generator. I ended up implementing some funky mechanics with `tf.data.Dataset.sample_from_datasets` and funky `multiprocessing` for multi threaded lookahead. Nasty, but it sort of works ...","It isn't currently possible to dynamically create `from_generator`based datasets from inside of `interleave`. The problem is that `py_function` can only return Tensors, but we need it to produce a Python `tf.data.Dataset` object. Instead, you could work around the issue by creating the `from_generator` datasets ahead of time in Python, and then interleaving them. The example below does what the original post was attempting: "," , Could you please try the above workaround and let us know if that helps you. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
646,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Building `tflite` for `manylinux`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device NA  Python version 3.8, 3.9, 3.10  Bazel version 5.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pharmpy-dev-123,Building `tflite` for `manylinux`,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device NA  Python version 3.8, 3.9, 3.10  Bazel version 5.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-07-12T19:42:40Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56743,Hi dev123 ! Sorry for the late response. I checked this thread which says Tensorflow linux wheels have been upgrade  manylinux 2014 wheel.  To get a more compatible wheel for your workstation you can run `tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native `  Reference. Thank you!,"Hi . As I understand it, a `native` build would produce a less generic binary. I am looking for a more generic binary. I could find some references on how to obtain such a build (`manylinux`) for TensorFlow, but it seems instructions are missing for `tflite`.", !  Could you please look at this issue. Thank you!,"Hi dev123, To build a more generic binary you can omit the native parameter, is there a reason you need to build it? can you just use the prerelease package?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1292,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(_ragged_tensor_to_tensor_grad() fails in eager mode under some circumstances)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.6  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Training some models that use `RaggedTensor` with Keras works fine if the model is compiled with `run_eagerly=False`, but when `run_eagerly=True`, training fails with the following traceback:  I have found that this is most likely due to an error at https://github.com/tensorflow/tensorflow/blob/7730bb302615aee24bbad653dcf7f7698d2dae5d/tensorflow/python/ops/ragged/ragged_conversion_ops.pyL66: in that line, there is a comparison with a bytestring `b""FIRST_DIM_SIZE""`, but in eager mode `row_partition_types` contain `str`s. I have also found that the following snippet prevents the bug from happening:   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,dniku,_ragged_tensor_to_tensor_grad() fails in eager mode under some circumstances,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.6  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Training some models that use `RaggedTensor` with Keras works fine if the model is compiled with `run_eagerly=False`, but when `run_eagerly=True`, training fails with the following traceback:  I have found that this is most likely due to an error at https://github.com/tensorflow/tensorflow/blob/7730bb302615aee24bbad653dcf7f7698d2dae5d/tensorflow/python/ops/ragged/ragged_conversion_ops.pyL66: in that line, there is a comparison with a bytestring `b""FIRST_DIM_SIZE""`, but in eager mode `row_partition_types` contain `str`s. I have also found that the following snippet prevents the bug from happening:   Standalone code to reproduce the issue   Relevant log output _No response_",2022-07-12T15:23:43Z,stat:awaiting response type:bug stale comp:ops 2.6.0,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56740, Could you try with the latest TF version 2.9.0 and please provide a code snippet to reproduce the issue reported here if the issue still persists? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
375,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Simplify the fuzzers)ï¼Œ å†…å®¹æ˜¯ ( Make use of atheris.FuzzedDataProvider where possible  Enable_python_coverage isn't necessary anymore  Don't instrument `sys`  .instrument_func isn't necessary anymore)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jvoisin,Simplify the fuzzers, Make use of atheris.FuzzedDataProvider where possible  Enable_python_coverage isn't necessary anymore  Don't instrument `sys`  .instrument_func isn't necessary anymore,2022-07-12T14:04:05Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/56738
1304,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to flat the model about TFLite and remove the ""while"" loop?)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  Linux:  TensorFlow 2.9:  2. Code The code of converter: modelparh = r""model.h5"" model = tf.keras.models.load_model(modelparh, custom_objects = {'...'}) def representative_dataset_gen():     for audio in x_train[0:500,:]:         yield [audio.reshape(1,x_train.shape[1],x_train.shape[2]).astype(""float32"")] converter = TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.contrib.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset_gen converter.target_spec.supported_ops=[tf.contrib.lite.OpsSet.TFLITE_BUILTINS,tf.contrib.lite.OpsSet.SELECT_TF_OPS,tf.contrib.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.inference_input_type = tf.int8 converter.inference_output_type = tf.int8 tflite_model = converter.convert() savepath = r""model.tflite"" open(savepath, ""wb"").write(tflite_model) my issus is: i can converter and quantification the model.but when i check the file of tflite by netron,find the model can't flat and have some ""while"" loop in gru graph. the graph like this: while 1=0 2=0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,panhu,"How to flat the model about TFLite and remove the ""while"" loop?"," 1. System information  Linux:  TensorFlow 2.9:  2. Code The code of converter: modelparh = r""model.h5"" model = tf.keras.models.load_model(modelparh, custom_objects = {'...'}) def representative_dataset_gen():     for audio in x_train[0:500,:]:         yield [audio.reshape(1,x_train.shape[1],x_train.shape[2]).astype(""float32"")] converter = TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.contrib.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset_gen converter.target_spec.supported_ops=[tf.contrib.lite.OpsSet.TFLITE_BUILTINS,tf.contrib.lite.OpsSet.SELECT_TF_OPS,tf.contrib.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.inference_input_type = tf.int8 converter.inference_output_type = tf.int8 tflite_model = converter.convert() savepath = r""model.tflite"" open(savepath, ""wb"").write(tflite_model) my issus is: i can converter and quantification the model.but when i check the file of tflite by netron,find the model can't flat and have some ""while"" loop in gru graph. the graph like this: while 1=0 2=0",2022-07-12T03:18:50Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TF 2.9,closed,0,12,https://github.com/tensorflow/tensorflow/issues/56734,"Hi ! Contrib has been removed in 2.x version. Could you just remove ""Contrib"" from tf.contrib.lite and let us know the difference.  Please post a minimal stand alone code for the model to replicate the issue.  Thank you!","Ok,The previous code was sent incorrectlyï¼Œthis is my simple code of convert:  The same issus: Checking the file of tflite by netron,find the model can't flat and have some ""while"" loop in gru graph. the loop of graph like this: while 1=0 2=0 so,how to flat the model and remove the ""while"" loop","!  Integer Quantization is not possible without a representative dataset. So, I removed ""tf.lite.OpsSet.TFLITE_BUILTINS_INT8"" from supported_ops.  ! Could you please look at this issue. Attached gist  in 2.8 for reference.  Thank you!",", Could you please provide the screenshot of the graph in netron which you have mentioned above.","> , Could you please provide the screenshot of the graph in netron which you have mentioned above. ok, i look the gru cell like this: https://userimages.githubusercontent.com/11703018/1788646658a75a585809944f18291e50ae6c9cc0a.PNG",More completeï¼š https://userimages.githubusercontent.com/11703018/178866754305bfd0ec7784aaca5959377d22c381c.PNG,"Thanks for the detail. The TFlite converted model creates nodes in a optimized way, I don't think you can modify any of those. Instead, you can follow any of the alternative way mentioned here to convert your model and see if that works for you. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I'm having a similar problem (""while"" loop) when converting a RNN network, with a custom RNN cell, to a tflite model. This also happens when using the LSTMcell instead of the custom cell or when using the GRUcell or GRU layer. However, this does NOT happen when using the LSTM layer.  When I view my model in netron, this is what I get !netron_gru_model This is what is returned from tf.lite.experimental.Analyzer.analyze(tfmodel)  However, when I create my model with an LSTM layer I get what I: !netron_lstm_model and from tf.lite.experimental.Analyzer.analyze(tfmodel)  What can I do to remove the reshaping and while and get something similar to the second photo which was created by only changing  to ","I have the same problem, trying to convert a model with GRU layer."
694,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(openCL delegate generates '0' and 'inf' values with reduce_sum)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8, tf 2.9  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bahar-BM,openCL delegate generates '0' and 'inf' values with reduce_sum,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8, tf 2.9  Custom Code No  OS Platform and Distribution Android  Mobile device tested on Snapdragon 888, 865  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-07-11T22:07:14Z,type:bug comp:lite TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56732,Hi BM ! Sorry for the late response.  Could you share complete code including TFlite conversion and inference to replicate the issue. Thank you!,"I have similar issue with reduce_mean using OpenCL delegate. for a (1 x 2 x 2 x 1) input, reduce across axis=2 gives wrong result. axis=1 or axis=(1,2) is fine. also with keepDims=True the result is correct, so I think something might be wrong when keepDims=False.  (I didn't check for other input size, though) edit: I just tested reduce_sum for same input, and the result is same as reduce_mean,  keepDims=True is correct and keepDIms=False is wrong.","Hi , we have implemented a small tool for you to reproduce the mentioned issue with the reduce_sum node. Here is the link to the repository: https://github.com/BaharBM/tflitetest In this repository we have also included scripts for building and converting the sample model.",Hi  ! Could you please look at this issue. Thank you!,"Hi, Can you check that your models work correct after this commit: https://github.com/tensorflow/tensorflow/commit/56a417633249bef47f94b2754687731e228758d6","Hi , Thank you for your response. Yes! your fix has resolved the problem with the ""reduce_sum"" node. Thank you! However, I noticed that there are other nodes like ""tf.stack"" with the same issue. Should I close this issue and open a new one for the other node? ","BTW I've just created a new branch in the ""tflitetest"" repository and have added codes to reproduce the issue with the ""tf.stack"" node: https://github.com/BaharBM/tflitetest/tree/tf_stack_openCL The issue is very similar to the ""reduce_sum"" node. But if you want, I can close this issue and open a new one. Thanks.","BM , If the issue is fixed with the latest nightly version, could you please close this request. You can open the new request for the above commented issue. Thanks!"," Yes, commit 56a4176 has resolved the issue with the ""reduce_sum"" node. So, I'll close this issue and will open a new one for the other similar nodes. ",Are you satisfied with the resolution of your issue? Yes No
665,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Difference in accuracy between EDGE TFLite Model and Quant TFLite Model.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.4.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version 3.7.2  GCC/Compiler version 8.3.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,BhaskarsarmaP,Difference in accuracy between EDGE TFLite Model and Quant TFLite Model.,Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.4.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version 3.7.2  GCC/Compiler version 8.3.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-11T10:09:37Z,stat:awaiting response stale comp:lite type:performance TF 2.4,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56729,Hi  ! Have you checked the 2.8/2.9 version yet ? Please provide a Colab gist  with a link to TFLite model to replicate the issue. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
267,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fixing the incorrect link in backend.py)ï¼Œ å†…å®¹æ˜¯ (Added the correct link for `RaggedTensor`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tilakrayal,Fixing the incorrect link in backend.py,Added the correct link for `RaggedTensor`,2022-07-11T10:06:20Z,comp:keras size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/56728,"Hi  It looks like your PR relates to the Keras component. Please submit it to the github.com/kerasteam/keras repository instead. Thankyou. , "
1962,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor->type != output->type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke.)ï¼Œ å†…å®¹æ˜¯ (Hello TF Lite Cummunity!  I'm doing tests with TF Lite converter on a machine, that, as far as I know, is equipped as follow:  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux5.4.188+x86_64withUbuntu18.04bionic  TensorFlow installation (pip package or built from source): installed via pip   TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.2  2. Code Starting from a TensorFlow model, I perform the following steps: 1. I perform a pruning operation on the model (everything works fine); 2. I convert the pruned model into a TensorFlow Lite model using tf.lite.TFLiteConverter.from_saved_model (everything works fine); 3. I evaluate such a pruned lite model using tf.lite.Interpreter(model_content=tflite_model) and subsequently interpreter.invoke() (everything works fine); 4. I then try to perform posttraining full integer quantization and evaluate the model again; posttraining full integer quantization seems to work fine because the model is properly converted. However, when I try to perform model evaluation, I got the following error ""RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor>type != output>type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke."", but I do not understand what could be the reason, and where I go wrong. Here is the code:   3. Failure after conversion As I've said, the model is properly converted, but during inference time interpreter.invoke() generates the following error: RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor>type != output>)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ChicchiPhD,RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor->type != output->type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke.,"Hello TF Lite Cummunity!  I'm doing tests with TF Lite converter on a machine, that, as far as I know, is equipped as follow:  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux5.4.188+x86_64withUbuntu18.04bionic  TensorFlow installation (pip package or built from source): installed via pip   TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.2  2. Code Starting from a TensorFlow model, I perform the following steps: 1. I perform a pruning operation on the model (everything works fine); 2. I convert the pruned model into a TensorFlow Lite model using tf.lite.TFLiteConverter.from_saved_model (everything works fine); 3. I evaluate such a pruned lite model using tf.lite.Interpreter(model_content=tflite_model) and subsequently interpreter.invoke() (everything works fine); 4. I then try to perform posttraining full integer quantization and evaluate the model again; posttraining full integer quantization seems to work fine because the model is properly converted. However, when I try to perform model evaluation, I got the following error ""RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor>type != output>type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke."", but I do not understand what could be the reason, and where I go wrong. Here is the code:   3. Failure after conversion As I've said, the model is properly converted, but during inference time interpreter.invoke() generates the following error: RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor>type != output>",2022-07-11T09:36:44Z,comp:lite TFLiteConverter Fixed in Nightly TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56727,Hi  ! Have you provided a representative dataset and used  `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] ` during integer quantization as per this document. .  Please provide complete code as colab gist (model snippet + quantization + inference) to replicate the issue. Thank you!,"Hi  !  Thanks for your reply! Yes, I provided a representative dataset as per the documentation, and I also used converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]. Actually, I used converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS] because otherwise I would have got a ConverterError. I'm working with the UCI HAR Data set; here attached you can find my working directory, i.e., where the pruned model is saved, and .tflite files are saved as well after conversion (in the attached directory you can find all generated files). models.zip For sake of simplicity, the code is a subset of the whole code but is enough to reproduce the same error thanks to the attached directory that contains the saved pruned model that you can load directly. https://colab.research.google.com/gist/ChicchiPhD/3adf359417b68742f50e00025392186b/reproduceerror.ipynb","I forgot to mention that I've also tried different implementations of the representative dataset function, but I've always ended up with the same error.","Hi  ! It is replicating in 2.8, 2.9  but seems to be fixed in the nightly  (2.10.0dev)version. Could you let us know from your end. Thank you!",Hi  ! I confirm that with nightly version know it works! Thank you so much for your help!,Ok  ! Could we move this to closed status then. Thank you!,Are you satisfied with the resolution of your issue? Yes No
1246,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('No toolchains found' when building libtensorflow on FreeBSD 13)ï¼Œ å†…å®¹æ˜¯ (Trying to build libtensorflow on FreeBSD 13 I was getting this toolchain error:    Looks like something happens to the execution platform here: https://github.com/tensorflow/tensorflow/blob/9584c52ecfbbc33b3be48903cdadd41b4a2bb083/third_party/remote_config/remote_platform_configure.bzlL3L12 I'm not sure exactly how this all works, but adding a branch for freebsd seems to have fixed the issue:  After that the toolchain is selected properly but the library still won't build.  I had to add the following lines to .bazelrc:  Now the code will compile, but it won't link. The linker will fail with `ld: error: undefined symbol: environ`. After googling a bit, this seems to be a known issue when building shared objects referencing `environ` (link) In the end I just removed `z defs` in tensorflow/BUILD and after that libtensorflow built successfully. The mini hello_tf.c example from the official site worked fine too.  FreeBSD 13.1RELEASE clang 13.0.0 bazel 5.2.0 python 3.9.13 tensorflow2.9.1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,desuwa,'No toolchains found' when building libtensorflow on FreeBSD 13,"Trying to build libtensorflow on FreeBSD 13 I was getting this toolchain error:    Looks like something happens to the execution platform here: https://github.com/tensorflow/tensorflow/blob/9584c52ecfbbc33b3be48903cdadd41b4a2bb083/third_party/remote_config/remote_platform_configure.bzlL3L12 I'm not sure exactly how this all works, but adding a branch for freebsd seems to have fixed the issue:  After that the toolchain is selected properly but the library still won't build.  I had to add the following lines to .bazelrc:  Now the code will compile, but it won't link. The linker will fail with `ld: error: undefined symbol: environ`. After googling a bit, this seems to be a known issue when building shared objects referencing `environ` (link) In the end I just removed `z defs` in tensorflow/BUILD and after that libtensorflow built successfully. The mini hello_tf.c example from the official site worked fine too.  FreeBSD 13.1RELEASE clang 13.0.0 bazel 5.2.0 python 3.9.13 tensorflow2.9.1",2022-07-10T21:04:23Z,stat:awaiting response type:build/install stale subtype:bazel TF 2.9,closed,0,17,https://github.com/tensorflow/tensorflow/issues/56725,", Could you try `bazel clean expunge` followed by `bazel sync`. Also please provide the complete error log which helps to debug the issue. Thank you!", Full log after bazel clean expunge and bazel sync  ,", Could you please try to install with a compatible version as mentioned below and check if you are facing the same error. Version  Bazel 3.7.2 Thank you!","I'm using Python 3.9.13, this should be fine, right?",", Python v3.9.13 is compatible. Could you please the comment and let us know if you are facing the same issue. Thank you!",Yes. Python 3.9.13 is what I've been using initially. I'll edit the first post to mention this.,", Could you please try to install with a compatible version as mentioned here and let us know if you are facing the same error. Thank you!","Yes, same issue with compatible versions of everything.","Hi , Officially, Tensorflow build does not support FreeBSD OS. Custom builds are available via SIG Build (https://github.com/tensorflow/build), You can try SIG build. Thank you!","It would build fine on FreeBSD if the bazel file I linked in the issue didn't have this oversight of overwriting the ""freebsd"" platform tag to ""linux"".","Hi , Bazel Config has default OS mentioned as Linux. It will take all the default configurations of Linux to build Tenaorflow. You can try with replacing Linux by freebsd. Thank you! ", posted a patch that makes this work at https://github.com/bazelbuild/bazel/issues/13227issuecomment1185231470 Lets get it checked in! I was able to build jaxlib and tensorflow on freebsd just now.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,This is a genius way of closing issues!  Bravo!,"Hi , Feel free to reopen if this issue is still unresolved. Thank you!"
653,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(raw_rnn equivalent in tensorflow 2.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",text generation,DYSIM,raw_rnn equivalent in tensorflow 2.0,Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-07-10T10:51:06Z,stat:awaiting response type:support stale comp:apis TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56724,", `raw_rnn` function is a more primitive version of `dynamic_rnn` that provides more direct access to the inputs each iteration. It also provides more control over when to start and finish reading the sequence, and what to emit for the output. For example, it can be used to implement the dynamic decoder of a `seq2seq` model. Kindly please take a look at this doc link which delivers the information. **Note: This method is still in testing, and the API may change.**  Thank you!",", I have read the documentation before, thanks for linking it. May I know what is the equivalent of raw_rnn in TF2.0? Or whether it is possible to get ""more control over when to start and finish reading the sequence, and what to emit for the output"" in TF2.0? Thank you.",", >May I know what is the equivalent of raw_rnn in TF2.0? `tf.compat.v1.nn.raw_rnn` is the compatible version that works in `TF2.x` >whether it is possible to get ""more control over when to start and finish reading the sequence, and what to emit for the output"" in TF2.0? Yes Thank you. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1430,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(__array_interface__ support?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.6.0  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Hi. Over at https://github.com/pythonpillow/Pillow, our Image class has the `__array_interface__` attribute, to support converting Pillow images to NumPy. See https://numpy.org/doc/stable/reference/arrays.interface.htmlobject.__array_interface__ A recent discussion has revealed that TensorFlow's `reshape` method (and I have to imagine other methods as well) accepts an object that provides an `__array__` method, but not an object with an `__array_interface__` attribute. My question  is there any interest from TensorFlow in supporting objects with `__array_interface__`?  Standalone code to reproduce the issue Here is code that fails with the latest version of Pillow.  However, if my assertion that TensorFlow doesn't accept `__array_interface__` is at all in doubt, let me know and I'll put together a better example. ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,radarhere,__array_interface__ support?,"Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.6.0  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Hi. Over at https://github.com/pythonpillow/Pillow, our Image class has the `__array_interface__` attribute, to support converting Pillow images to NumPy. See https://numpy.org/doc/stable/reference/arrays.interface.htmlobject.__array_interface__ A recent discussion has revealed that TensorFlow's `reshape` method (and I have to imagine other methods as well) accepts an object that provides an `__array__` method, but not an object with an `__array_interface__` attribute. My question  is there any interest from TensorFlow in supporting objects with `__array_interface__`?  Standalone code to reproduce the issue Here is code that fails with the latest version of Pillow.  However, if my assertion that TensorFlow doesn't accept `__array_interface__` is at all in doubt, let me know and I'll put together a better example. ```  Relevant log output _No response_",2022-07-10T10:03:45Z,stat:awaiting tensorflower type:feature comp:apis TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56723,The issue is replicated in TF 2.9  Output: ,Thanks very much !," no problem.  At Google we oddly have a patched version of Pillow that replaces `__array_interface__` back with `__array__`, so we weren't reproducing this issue internally.  Maybe this was why."
653,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issues when updating the code from tf1 to tf2)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version tf2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nour397,Issues when updating the code from tf1 to tf2,Click to expand!    Issue Type Others  Source source  Tensorflow Version tf2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-07-08T19:17:28Z,stat:awaiting response stale type:others comp:keras,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56719, Could you please refer to the migration doc to rewrite the code in TF v2.x style and try with the latest TF version? For any further queries please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
2068,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('keras.api._v1.keras.layers' has no attribute 'conv2d' how can I fix this issue please? (I'm trying to change my code from tf1 to tf2, I changed all tf.layers to tf.keras.layers since in tf2 keras is the High level API and I had this error message, how can I fix it? )ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**:    **TensorFlow version (use command below)**:    **Python version**:    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You can obtain the TensorFlow version with:   Describe the proble)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nour397,"'keras.api._v1.keras.layers' has no attribute 'conv2d' how can I fix this issue please? (I'm trying to change my code from tf1 to tf2, I changed all tf.layers to tf.keras.layers since in tf2 keras is the High level API and I had this error message, how can I fix it? ","Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**:    **TensorFlow version (use command below)**:    **Python version**:    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**:    **Exact command to reproduce**: You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh You can obtain the TensorFlow version with:   Describe the proble",2022-07-08T19:03:30Z,stat:awaiting response type:support stale comp:keras,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56718,", Could you please refer to the migration doc to update the code in latest TF v2.x and try to execute with the latest TF version. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
692,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlowLiteTaskVision and TensorFlowLiteSwift duplicate symbols for architecture arm64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution MacOS 12.4  Mobile device iPhone 7  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Anbusekar-Scanflow,TensorFlowLiteTaskVision and TensorFlowLiteSwift duplicate symbols for architecture arm64,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution MacOS 12.4  Mobile device iPhone 7  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-08T08:00:07Z,stat:awaiting response type:build/install stale comp:lite subtype:macOS TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56712,Hi Optisol ! Errors point towards duplicate symbols present between TensorFlowLiteVisionSwift and TensorFlowLiteTaskVision. Could you  declare only TensorFlowLiteTaskVision in pod file (new environment and new pod file please)and let us know whether it works.  Reference. Thank you!,"Hi  ! Here my requirement is, I have to utilise both TensorFlowLiteTaskVision and TensorFlowLiteVisionSwift, If I give only TensorFlowLiteTaskVision, it works fine. but need to integrate both. while integrating shows error. is there another way to ignore duplicates?.",Hi  ! Could you please look at this issue.  Thank you!,"Hi  ,  ! I need to access TensorTaskVision for object_detection, while integrating both TensorFlowLiteC ans TensorFlowTaskVision it shows above error, is there another way to access  object_detection without taskvision or any delegateMethods to achieve this integration.","Hi wangg, do we support using both these task libraries simultaneously?","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
684,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(A error occured when using dense_to_ragged_batch in TFRecordDataset, is't a bug?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.9.0  Custom Code Yes  OS Platform and Distribution Windows 10 21H1  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,123mbcz123,"A error occured when using dense_to_ragged_batch in TFRecordDataset, is't a bug?",Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.9.0  Custom Code Yes  OS Platform and Distribution Windows 10 21H1  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-08T07:58:28Z,stat:awaiting response type:support comp:data TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56711,", To expedite the troubleshooting process, could you please provide the complete code to reproduce the issue reported here.  Thank you!","  I have organized my datasets and codes,and posted it to colab. Could you please have a look at it?  The colab link shows below. https://colab.research.google.com/drive/1upagbX5ejvr0J_KTrWa0Q4ccJORZsAN?usp=sharing",", I tried to modify the code and executed it in an alternative approach without any issues. Kindly find the gist of it here and let us know if the issue still persists. Thank you!","  Thanks! I have checked your code line by line, It seems like you just change the batch_size and the filepath,is it right?  The batch_size bigger than one  still doesn't work in TFRecordDataset.,howerver,it works in tf.data.Dataset.      It seems that dense_to_ragged_batch in TFRecordDataset tries stacking tensors into a normal tf.tensor batch instead of a raggedTensor.     dense_to_ragged_batch in tf.data.Dataset always stacks tensors into a raggedTensor even if batch_size equals to one,But in TFRecordDataset it returns a tf.Tensor when batch_size equals to one,and It throws errors when batch_size is bigger than one.  Sincerely thanks again.",">  Thanks! I have checked your code line by line, It seems like you just change the batch_size and the filepath,is it right? The batch_size bigger than one still doesn't work in TFRecordDataset.,howerver,it works in tf.data.Dataset. It seems that dense_to_ragged_batch in TFRecordDataset tries stacking tensors into a normal tf.tensor batch instead of a raggedTensor. dense_to_ragged_batch in tf.data.Dataset always stacks tensors into a raggedTensor even if batch_size equals to one,But in TFRecordDataset it returns a tf.Tensor when batch_size equals to one,and It throws errors when batch_size is bigger than one. Sincerely thanks again. Yes. Also please take a look at this doc link which delivers the information on `dense_to_ragged_batch`. Thank you!","  Thanks, I have realized  the same function  by tf.data.Dataset instead.",", Could you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No
818,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problem using Metal delegate on iPhone 13 mini)ï¼Œ å†…å®¹æ˜¯ ( System information Code written:   Using an iPhone 13 mini, on iOS 15.5  Tensorflow lite is compiled from source, using 2.9.1  Cross compiled from m1 mac using cmake, and bazel 5.0.0 for the required frameworks  Compiled using appleclang 13.1  XCode version 13.4.1  Describe the problem There is a problem trying to use Metal delegate on this specific phone (or perhaps the whole iPhone 13 family, confirmed to be happening on mini and pro). Have tested on iPhone 12 and below, and there isn't a problem with that.  Logs Error printed by tflite. Also, returned with TfLiteStatus of 2 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,zhihuiloke,Problem using Metal delegate on iPhone 13 mini," System information Code written:   Using an iPhone 13 mini, on iOS 15.5  Tensorflow lite is compiled from source, using 2.9.1  Cross compiled from m1 mac using cmake, and bazel 5.0.0 for the required frameworks  Compiled using appleclang 13.1  XCode version 13.4.1  Describe the problem There is a problem trying to use Metal delegate on this specific phone (or perhaps the whole iPhone 13 family, confirmed to be happening on mini and pro). Have tested on iPhone 12 and below, and there isn't a problem with that.  Logs Error printed by tflite. Also, returned with TfLiteStatus of 2 ",2022-07-08T04:31:35Z,stat:awaiting tensorflower type:bug comp:lite TF 2.9,closed,0,12,https://github.com/tensorflow/tensorflow/issues/56709,Hi  ! Could you try with below Gpu delegate initialization and let us know.  Reference  Thank you!,"Some additional info:  CMake is compiled with c++17, so .bazelrc is also patched the same way to avoid the `absl::string_view` issue Some stuff I have tried, but didn't work:  Adding these at the top of `GenerateConvolution`    Forced minimum iOS version to be 14.0, ensuring it is building for metal 2.3"," this is exactly how we are using the GPU delegate right now. Parts of it are redacted, but in general, the flow is exactly as described. Update: using default options didn't help either",Hi  ! Could you please look at this issue. Thank you!,"Update: I managed to fix it, so here is one for you guys. Looking at the newest Metal Shading Language specification, we see that `simdgroup_matrix` is only available from Metal 2.3 and above. I patched `common.mm` `CreateFunction` function to use Metal 2.3 for my use case, and it worked. I have only resolved the symptom on my side, have yet to know why it is happening for iPhone 13 family. Will be willing to provide more information if needed. ",Could you try with Minimum IOS version as 10 and let us know if that helps. Tensorflow's implementation with c++ 17 is very recent and that could be the problem as well.  Let us know your findings as well. Thanks! ,I don't think setting the Minimum iOS versioned to 10.0 worked. It is still having the same errors as above.,"Also, compiling tflite 2.9.1 with cpp14 didn't help either.","Experienced the same issue on an iPad mini (6th gen) running iOS 15.5 built using Xcode 13.4.1. I was running the Mobilenet_1.0_224(float) model from the iOS performance benchmarks page. In order to get it to work, I had to use the following patch on top of v2.9.1: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/lite/delegates/gpu/metal/common.mm  Looks like a similar patch is already in `master`: https://github.com/tensorflow/tensorflow/commit/af256daa1495792613156192c02e92685337173e Edit: actually, it looks like that patch fixes the bug that allows `simdgroup_matrix` to be used for Metal 2.2.","""Looks like a similar patch is already inÂ master:Â https://github.com/tensorflow/tensorflow/commit/af256daa1495792613156192c02e92685337173e Edit: actually, it looks like that patch fixes the bug that allowsÂ simdgroup_matrixÂ to be used for Metal 2.2."" This is correct. Initially similar patch was submitted together with kernel that uses simd matrix multiplication. But it was reverted due to internal reasons. I also would like to add that this kernel indicates that fp32 precision was used. Fp32 is fully supported, but fp16 has better performance and highly recommended when possible.",Closing as there seems to be a fix at 2.10 onwards,Are you satisfied with the resolution of your issue? Yes No
700,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow.python.compiler.tensorrt converter build Bert model error)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1 (git version: v2.9.018gd8ce9f9c301)  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.4  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",text generation,Jiayuli-CU,tensorflow.python.compiler.tensorrt converter build Bert model error,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1 (git version: v2.9.018gd8ce9f9c301)  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.4  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-08T04:17:26Z,stat:awaiting response type:bug stale comp:gpu:tensorrt TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56708,CU I tried to replicate the issue on colab and didnâ€™t see the reported error. Please find the gist here for reference and let me know if I'm missing something to reproduce the issue. Thank you!, Your code stops before converter.build() because certain packages are missing. You need to install TensorRT first. https://docs.nvidia.com/deeplearning/tensorrt/installguide/index.html,CU Please try using `tf.experimental.tensorrt.Converter` to convert your saved model. Please take a look at the doc here. Please let me know if you are still running into the same error. Thanks!,> CU Please try using `tf.experimental.tensorrt.Converter` to convert your saved model. Please take a look at the doc here. Please let me know if you are still running into the same error. Thanks! Got the same error message after using `tf.experimental.tensorrt.Converter` follwing the official doc.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
686,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom Op written in C API errors when requesting GPU_Device stream)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Feature Request  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux   Mobile device 07010841551  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,YogaVicky,Custom Op written in C API errors when requesting GPU_Device stream,Click to expand!    Issue Type Documentation Feature Request  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux   Mobile device 07010841551  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-07-07T19:14:02Z,stat:awaiting response type:feature stale comp:core type:docs-feature,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56703,", Could you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Saving and Loading a Dataset returns a different elements)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution MacOS Monterey  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to save and subsequently load a dataset. I am observing that after loading the dataset the elements of it are many times different from the saved one. In order to run the code I have two text files in the following structure with contents:  I am expecting that the two loops should print an identical Tensor.  Standalone code to reproduce the issue   Relevant log output )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,balvisio,Saving and Loading a Dataset returns a different elements,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution MacOS Monterey  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to save and subsequently load a dataset. I am observing that after loading the dataset the elements of it are many times different from the saved one. In order to run the code I have two text files in the following structure with contents:  I am expecting that the two loops should print an identical Tensor.  Standalone code to reproduce the issue   Relevant log output ,2022-07-07T17:36:43Z,type:bug comp:keras TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56700,There is no bug. `text_dataset_from_directory` shuffles the samples by default and that is why I would see different samples.,Are you satisfied with the resolution of your issue? Yes No, Thank you for the update!
752,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MBv3 conversion: How to turn off hard swish optimization)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  Windows 11  pip   version 2.8.0  2. Code   3. Failure after conversion The model is correctly converted. The hard swish layers (multiple layers ADD, MUL, RELU6 in TF) are combined into one layer `hard swish` in TFlite. !image I understand that this is by design.  Question: I have a use case where I need to turn off this conversion. That is, when the model is converted to tflite, the layers should not be converted to hard swish layer. Please advise on how to turn off this behavior.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mrtpk123,MBv3 conversion: How to turn off hard swish optimization," 1. System information  Windows 11  pip   version 2.8.0  2. Code   3. Failure after conversion The model is correctly converted. The hard swish layers (multiple layers ADD, MUL, RELU6 in TF) are combined into one layer `hard swish` in TFlite. !image I understand that this is by design.  Question: I have a use case where I need to turn off this conversion. That is, when the model is converted to tflite, the layers should not be converted to hard swish layer. Please advise on how to turn off this behavior.",2022-07-06T06:42:59Z,stat:awaiting response type:support comp:lite TFLiteConverter TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56686,Hi  !  Could you please look at this issue. Thank you!,Hi  ! Have you checked quantization_debugger documentation yet. You can skip quantization for some nodes through selective quantization now in 2.10 version. Thank you!,Thank you .,Are you satisfied with the resolution of your issue? Yes No
655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can we get PocketFFT ported to Tensorflow?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ddgonzal3,Can we get PocketFFT ported to Tensorflow?,Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-07-06T02:42:38Z,stat:awaiting tensorflower type:feature comp:apis comp:ops TF 2.9,closed,0,24,https://github.com/tensorflow/tensorflow/issues/56685,MKL FFT could be another alternative if pocketFFT is not possible., PocketFFt is still not integrated in tensorflow. jax has used PocketFFT as a workaround as mentioned here but tensorflow has not. ,"Hey , thanks for your response!  Yeah I've gotten the JAX fft to work using jax2tf.convert, but it is not able to be serialized into a protobuf (falls back to EigenFFT), so it's only usable in a python environment.  Do you know if there are currently any plans to integrate PocketFFT into tensorflow?",Eigen can use PocketFFT if we include the pocketfft header (`pcketfft_hdronly.h`) in tensorflow and define `EIGEN_POCKETFFT_DEFAULT=1`.  That was added in !356.  We'd probably be willing to accept a pullrequest to enable it. The header would need to be pulled in under `third_party` (similar to the relevant JAX change in 4699.,"Hey , thanks for your reply.  Do you know if that's all that would be needed? If so, that sounds like a very quick change for something that would give immense benefit to the community for many use cases that require STFT transformations in a Tensorflow model inference pipeline.  If one wants to deploy a Tensorflow model in C++ on a CPU that operates on STFTs, they're forced to implement the STFT in C++ and feed that data into Tensorflow, otherwise you face a 3x performance hit. The problem gets immensely more complicated when the STFT transformations are in the _middle_ of a TF inference pipeline. I haven't been involved in contributing to Tensorflow in the past, so I would likely need to invest quite some time to learning the build system to do it myself. I'm currently swamped with high priority work before a going on vacation, so won't have the time to do so any time soon.  Is this something someone at Google could quickly try to implement, given the huge benefit it would provide?  Based off of this issue, it's a highly requested feature, as expected considering how widely used FFTs are in ML (speech, music, video, imaging, etc).","Looks like JAX switched to ducc (which is an ""evolution"" of pocketfft written by the same author).  We may consider the same.  it's not high priority for anyone on our end because we typically do model serving with GPU/TPU.  I've asked around, as well as the JAX team about their experience.  Depending on what I hear back, I _may_ put it on my list, though again it would be near the bottom of my priority queue.  If anyone else here is interested, I'm happy to look at PRs.","Looks like the PocketFFT author now recommends using DUCC, and the JAX team is happy with the transition, so we should do the same."," Awesome! That sounds great to me. I really appreciate you taking the time to look into this and check in with the Jax team.  Please let me know if you're ever able to get around to implementing DUCC, and I would be very keen on testing and using it. Thank you! ","hey , happy new year! Just wanted to check in to see if you happen to have some bandwidth for pushing this feature onto your priority queue.  Looking forward to hearing from you soon. Thanks!","Hi, author of pocketfft here :) I'm happy to help if there are any questions. One thing to keep in mind regarding pocketfft vs. DUCC: pocketfft requires C++11 support in the compilers, DUCC requires C++17. Unless you can guarantee C++17 support on all target platforms, you may want to use pocketfft, but if this is not an issue, I strongly recommend DUCC.","> Hi, author of pocketfft here :) I'm happy to help if there are any questions. > One thing to keep in mind regarding pocketfft vs. DUCC: pocketfft requires C++11 support in the compilers, DUCC requires C++17. Unless you can guarantee C++17 support on all target platforms, you may want to use pocketfft, but if this is not an issue, I strongly recommend DUCC. Thanks  ! C++17 isn't an issue  TF now requires c++17 anyways.  I think the main blockers are: 1) We can't actually use `std::mutex` or `std::thread` directly in TF (or internally, at all, within Google) 2) We would like to be able to use TF's threadpool for `execParallel(...)` For JAX, we initially simply set `DUCC0_NO_THREADING`.  However, we later found that we ran into race conditions in the `get_plan` cache due to parallel FFTs.  We _could_ disable plan caching as well, but I'm not sure how negatively that would impact performance.  We should solve this before adding it to TF. I _think_ we can work around this with minor modifications to `ducc0`: add optional inputs with defaults for the parallelizer and plan cache.  I haven't looked into it too closely.  How open would you be to modifications like this?","> We could disable plan caching as well, but I'm not sure how negatively that would impact performance. Actually that should not be a problem at all ... unless someone is calling 1D FFTs of length 10 over and over :) If you have lengths above, say, 256, or multiD transforms, plan caching isn't an issue. > I think we can work around this with minor modifications to ducc0: add optional inputs with defaults for the parallelizer and plan cache. I haven't looked into it too closely. How open would you be to modifications like this? I'd have to see the changes to judge this. Can you point me to a code location where you do something equivalent to `ducc`'s `execParallel`?","> Actually that should not be a problem at all ... unless someone is calling 1D FFTs of length 10 over and over :) We have people doing many batches of small FFTs.  For example, we have an internal TF user that ran into an issue with cuFFT that was doing millions of training iterations, each computing 10k batches of 16x16 2D FFTs.  What was the motivation for implementing the `get_plan` LRU cache, and do you think it would apply here? > I'd have to see the changes to judge this. Can you point me to a code location where you do something equivalent to ducc's execParallel? TF's threadpool has `ParallelFor`.  We would probably need something like:  The defaults would be simple structs that call `execParallel(...)` and `get_plan(...)` directly, but for TF's usage we would pass in our own versions that use TF's threadpool, and our own threadsafe caching mechanism.","> We have people doing many batches of small FFTs. For example, we have an internal TF user that ran into an issue with cuFFT that was doing millions of training iterations, each computing 10k batches of 16x16 2D FFTs. If the 10k FFTs are done in a single call (i.e. you call with a 3D array, but transform only along two axes), then plan caching is irrelevant. The plan for a singe length16 FFT will be computed once at the start, and the cost for this is negligible compared to the actual transforms. (And even if the 16x16 transforms are called individually, the plan will be reused 32 times, so that the actual FFT dominates). It's important to realize that pocketfft plans are much faster to compute than even FFTW's FFTW_ESTIMATE plans. That said, if performance of 16x16 FFTs is really important for an application, I'd look into customgenerated transforms for exactly this size... they can be *much* faster than any general purpose approach. > What was the motivation for implementing the `get_plan LRU` cache, and do you think it would apply here? This mainly exists because `scipy` people wanted to have it. I even have a PR against `scipy` (https://github.com/scipy/scipy/pull/12307) which would have disabled it because it can cause too much memory consumption in edge cases. This is no longer a problem with the current `ducc` version, but I still don't see real benefit in it. Thanks for the code snippet! I'm sure we can work something out if necessary. BTW, we can work on a potential solution for this on https://github.com/mreineck/ducc; it should be easier to collaborate there than on the Gitlab instance where I have my master repo.","> This mainly exists because scipy people wanted to have it.... Perfect, this simplifies things  we can probably skip caching altogether.  Yes, the batch would be called all at once. > Thanks for the code snippet! I'm sure we can work something out if necessary... Great, I was wondering how best to work together on something like this.  I will try to create a prototype then and ping you on the github repo when I have something.",Thanks guys!! Excited to be hopefully seeing faster FFTs in Tensorflow soon! :D,any  unpdate on thisï¼Ÿ,In process as we speak.  We needed upstream changes to ducc0 fft to be able to use our custom threadpools.,"Hey  , happy new year!  Been a while  do you know if there has been any update for the ducc FFT integration?","Yes, this was completed a while ago.",This is awesome!! I'm using the nightly build from today and am seeing way faster times for the FFT and IFFT operations! ," Would these changes make there way to TF 2.16? And if so, any idea when it might get released?  Thanks again for the incredible work!! ",">  Would these changes make there way to TF 2.16? And if so, any idea when it might get released? >  > Thanks again for the incredible work!! It was included in the 2.14 release, and will continue to be in all future releases.","Ah whoops, I thought I didn't see the changes in the release but I just tried it with 2.15 and it's working well. Thanks again! "
1861,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Training with MultiWorkerMirrorredStrategy crashes in TF 2.9)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA=11.2.2 / CUDNN=8.2.4.151  GPU model and memory Nvidia V100  Current Behaviour?   Standalone code to reproduce the issue  shell /opt/ml/code/official/vision/tasks/semantic_segmentation.py:169: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?   if 'mask_scores_mse' is metric.name: I0701 03:25:47.730769 140097427990336 train_utils.py:356] Final experiment parameters: {'runtime': {'all_reduce_alg': None,              'batchnorm_spatial_persistent': False,              'dataset_num_private_threads': None,              'default_shard_dim': 1,              'distribution_strategy': 'multi_worker_mirrored',              'enable_xla': False,              'gpu_thread_mode': None,              'loss_scale': 'dynamic',              'mixed_precision_dtype': 'float16',              'num_cores_per_replica': 1,              'num_gpus': 1,              'num_packs': 1,              'per_gpu_thread_count': 0,              'run_eagerly': False,              'task_index': 1,              'tpu': None,              'tpu_enable_xla_dynamic_padder': None,              'worker_hosts': None},  'task': {'differential_privacy_config': None,           'evaluation': {'top_k': 5},           'init_checkpoint': None,           'init_checkpoint_modules': 'all',           'losses': {'l2_weight_decay': 0.0001,                      'label_smoothing':)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Lokiiiiii,Training with MultiWorkerMirrorredStrategy crashes in TF 2.9,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA=11.2.2 / CUDNN=8.2.4.151  GPU model and memory Nvidia V100  Current Behaviour?   Standalone code to reproduce the issue  shell /opt/ml/code/official/vision/tasks/semantic_segmentation.py:169: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?   if 'mask_scores_mse' is metric.name: I0701 03:25:47.730769 140097427990336 train_utils.py:356] Final experiment parameters: {'runtime': {'all_reduce_alg': None,              'batchnorm_spatial_persistent': False,              'dataset_num_private_threads': None,              'default_shard_dim': 1,              'distribution_strategy': 'multi_worker_mirrored',              'enable_xla': False,              'gpu_thread_mode': None,              'loss_scale': 'dynamic',              'mixed_precision_dtype': 'float16',              'num_cores_per_replica': 1,              'num_gpus': 1,              'num_packs': 1,              'per_gpu_thread_count': 0,              'run_eagerly': False,              'task_index': 1,              'tpu': None,              'tpu_enable_xla_dynamic_padder': None,              'worker_hosts': None},  'task': {'differential_privacy_config': None,           'evaluation': {'top_k': 5},           'init_checkpoint': None,           'init_checkpoint_modules': 'all',           'losses': {'l2_weight_decay': 0.0001,                      'label_smoothing':",2022-07-05T19:53:24Z,stat:awaiting response type:bug stale comp:dist-strat TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56684, Does it only crash in TF 2.9? Does it work in TF 2.8? Also can you please check it with latest version of tfnightly and let me know if the issue still persists? Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs.  Also Please reproduce this simple example and check the issue. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1859,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tflite_runtime problem on Arm64 linux with multiprocessing)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: no    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux (Arm64)    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: n/a    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: 2.4.1    **Python version**: 3.6 (PC), 3.7 (embedded Linux)    **Bazel version (if compiling from source)**: n/a    **GCC/Compiler version (if compiling from source)**: n/a    **CUDA/cuDNN version**: 9    **GPU model and memory**: Titan xp    **Exact command to reproduce**:  Describe the problem I trained a neural network using quantizationaware training from tensorflow_model_optimization and converted it to a quantized tflite model. I have a python script that runs on a device with Linux (Arm64, 4 cores) and a NPU. This script uses the tflite_runtime module to make inference on the NPU with the NNAPI delegate: > INFO: Created TensorFlow Lite delegate for NNAPI. > Applied NNAPI delegate. This script uses multiprocessing to perform other operations. The inference is made on the main process while other operations are executed like this:  The problem is that when I start a new process, the inference stops working: the neural network keeps returning the same output, no matter which input I provide. Even when the child process ends, the problem remains. This does not happen if I use threads instead of process.  Source code / logs Quantization (on the training PC):  Interpre)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FSet89,tflite_runtime problem on Arm64 linux with multiprocessing," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: no    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux (Arm64)    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: n/a    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: 2.4.1    **Python version**: 3.6 (PC), 3.7 (embedded Linux)    **Bazel version (if compiling from source)**: n/a    **GCC/Compiler version (if compiling from source)**: n/a    **CUDA/cuDNN version**: 9    **GPU model and memory**: Titan xp    **Exact command to reproduce**:  Describe the problem I trained a neural network using quantizationaware training from tensorflow_model_optimization and converted it to a quantized tflite model. I have a python script that runs on a device with Linux (Arm64, 4 cores) and a NPU. This script uses the tflite_runtime module to make inference on the NPU with the NNAPI delegate: > INFO: Created TensorFlow Lite delegate for NNAPI. > Applied NNAPI delegate. This script uses multiprocessing to perform other operations. The inference is made on the main process while other operations are executed like this:  The problem is that when I start a new process, the inference stops working: the neural network keeps returning the same output, no matter which input I provide. Even when the child process ends, the problem remains. This does not happen if I use threads instead of process.  Source code / logs Quantization (on the training PC):  Interpre",2022-07-05T15:32:04Z,stat:awaiting response type:bug stale comp:lite TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56683,Hi  ! I think you are not using multithreading in interpreter correctly during inference. You have to pass the number of threads in interpreter too.   Could you also test above syntax with 2.8/2.9 version and Python 3.7/3.8 (Python 3.7 is minimum version now) Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1904,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow predict() is accurate on single sample but giving strange results when predicting on dataset)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Issue on Windows 11  (with CUDA) and MacOS 12.4 (CPU)  Mobile device _No response_  Python version 3.8 (Windows) and 3.9.5 (MacOS)  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version CUDA 11.4, driver version 471.41  GPU model and memory RTX3070 / 8GB  Current Behaviour? I have trained a model on the Stanford Dogs dataset and get accuracy around 80% (training acc) and 74% (validation acc).  When I predict using the trained model using `tf_preds = model.predict(test_images)` I get bad predictions (see relevant log output below) If I iterate through the dataset using a `for` loop, I get very good predictions (see relevant log output below) I do not understand why this problem occurs. I have raised this on Stackoverflow but had no responses https://stackoverflow.com/questions/72801723/tensorflowpredictisaccurateonsinglesamplebutgivingstrangeresultswhe I get the same issue when predicting on Windows (env details above) or Mac OS (env details above) with the same trained model.  Standalone code to reproduce the issue Here is a link to the notebook in Colab: https://colab.research.google.com/drive/1y2T_D3bQOXbs3tg_UKyYKZVINxb5vF_?usp=sharing You can visibly see the issue in the confusion matrices at the bottom of the notebook.  Note it does not matter whether I run `model.predict()` or the `for` loop first, I get the same result. And I have checked that I am not shuffling the data too (shuffling leads to similar results with `model.predict()`, as in it does)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,markstrefford,Tensorflow predict() is accurate on single sample but giving strange results when predicting on dataset,"Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Issue on Windows 11  (with CUDA) and MacOS 12.4 (CPU)  Mobile device _No response_  Python version 3.8 (Windows) and 3.9.5 (MacOS)  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version CUDA 11.4, driver version 471.41  GPU model and memory RTX3070 / 8GB  Current Behaviour? I have trained a model on the Stanford Dogs dataset and get accuracy around 80% (training acc) and 74% (validation acc).  When I predict using the trained model using `tf_preds = model.predict(test_images)` I get bad predictions (see relevant log output below) If I iterate through the dataset using a `for` loop, I get very good predictions (see relevant log output below) I do not understand why this problem occurs. I have raised this on Stackoverflow but had no responses https://stackoverflow.com/questions/72801723/tensorflowpredictisaccurateonsinglesamplebutgivingstrangeresultswhe I get the same issue when predicting on Windows (env details above) or Mac OS (env details above) with the same trained model.  Standalone code to reproduce the issue Here is a link to the notebook in Colab: https://colab.research.google.com/drive/1y2T_D3bQOXbs3tg_UKyYKZVINxb5vF_?usp=sharing You can visibly see the issue in the confusion matrices at the bottom of the notebook.  Note it does not matter whether I run `model.predict()` or the `for` loop first, I get the same result. And I have checked that I am not shuffling the data too (shuffling leads to similar results with `model.predict()`, as in it does",2022-07-04T17:42:26Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56674,", While I was trying to reproduce the provided code, the execution was taking longer than expected. Kindly find the gist of it here.",Note that model.predict is not intended to use inside the loop or to iterate data and use it.  It is intended for batch processing and large number of inputs. For details check here.,"> Note that model.predict is not intended to use inside the loop or to iterate data and use it. > It is intended for batch processing and large number of inputs. > For details check here. Hi yes, I realise that I should use `prediction = model(input_image)` or similar. However, the `for` loop works and predicting on the dataset doesn't","For more clarification on the issue, please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1776,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AutoGraph could not transform function)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18 (colab)  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory NVIDIA Tesla T4  Current Behaviour?   Standalone code to reproduce the issue ```shell .function def contains_pad(inp: tf.Tensor):     tf.debugging.assert_type(inp, tf.int32)     bool_ten = tf.math.equal(inp, pad_ten)     nonzero_count = tf.math.count_nonzero(bool_ten)     return nonzero_count > 0 .function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),                               tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32))) def train_step(inp: tf.Tensor, outp: tf.Tensor) > tf.Tensor:     with tf.GradientTape() as tape:         pred: tf.Tensor = model([inp, outp], training=True)          loss_val: tf.Tensor = tf.keras.losses.sparse_categorical_crossentropy(y_true = outp, y_pred = pred, from_logits = False)     grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)     optimizer.apply_gradients(zip(grads, model.trainable_weights))     return tf.math.reduce_mean(loss_val) .function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)]) def train(batch: tf.Tensor) > tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):     per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())     i = 0     while i )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,yonikremer,AutoGraph could not transform function,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18 (colab)  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory NVIDIA Tesla T4  Current Behaviour?   Standalone code to reproduce the issue ```shell .function def contains_pad(inp: tf.Tensor):     tf.debugging.assert_type(inp, tf.int32)     bool_ten = tf.math.equal(inp, pad_ten)     nonzero_count = tf.math.count_nonzero(bool_ten)     return nonzero_count > 0 .function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),                               tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32))) def train_step(inp: tf.Tensor, outp: tf.Tensor) > tf.Tensor:     with tf.GradientTape() as tape:         pred: tf.Tensor = model([inp, outp], training=True)          loss_val: tf.Tensor = tf.keras.losses.sparse_categorical_crossentropy(y_true = outp, y_pred = pred, from_logits = False)     grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)     optimizer.apply_gradients(zip(grads, model.trainable_weights))     return tf.math.reduce_mean(loss_val) .function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)]) def train(batch: tf.Tensor) > tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):     per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())     i = 0     while i ",2022-07-04T11:31:18Z,stat:awaiting response type:support stale comp:autograph TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56672,link the drive folder containing the notebook and the data,It would be nice and it will speedup the triage if you can minimize a bit the code surface to reproduce this error also using dummy input data on a runnable Colab. If it is failing it is cause we don't have a test for this case so we need to think about code snippet for reproducibility like a proxy for writing a new test.,"  In order to expedite the troubleshooting process, please provide a minimal code snippet to reproduce the issue reported here. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,I finally realized the cause of the issue! (sorry that it took me so long) I used tf.TensorSpec as a type hint for my function's output simple code to replicate (tensorflow 2.9.1):  this prints (when verbosity is set to 10):  I would be happy if a maintainer will change the warning to: Warning: `tf.function` does not support TensorSpec as a type hint for function output. Thanks!,Can you try with:   Do you think that we could extend this test? https://github.com/tensorflow/tensorflow/blob/46152aac7417aa0d1506b782e055448f431e1629/tensorflow/python/autograph/pyct/static_analysis/activity_test.pyL820L836, Could you please refer to the comment above and let us know if it helps? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1266,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLiteConverter.experimental_from_jax on a working jax model: ""flax.errors.JaxTransformError: Jax transforms and Flax models cannot be mixed."")ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04 (Google Colab)  TensorFlow installation (pip package or built from source): Colab's version (Pip?)  TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0  2. Code The example linked below demonstrates that a working JAX model (which converts and saves successfully with `tf.saved_model.save` throws the error ""flax.errors.JaxTransformError: Jax transforms and Flax models cannot be mixed."" when trying to convert it to TFLite using `TFLiteConverter.experimental_from_jax`. https://colab.research.google.com/gist/josephrocca/b9b7b4e92cb693772cb937302b4946d9/jaxflaxtransformationscannotbemixed.ipynb The `experimental_from_jax` function is of course experimental, and I understand that the jax2tf > tf saved model > tflite is the current recommended route, but just reporting this in case it is not expected behavior.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,josephrocca,"TFLiteConverter.experimental_from_jax on a working jax model: ""flax.errors.JaxTransformError: Jax transforms and Flax models cannot be mixed."""," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04 (Google Colab)  TensorFlow installation (pip package or built from source): Colab's version (Pip?)  TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0  2. Code The example linked below demonstrates that a working JAX model (which converts and saves successfully with `tf.saved_model.save` throws the error ""flax.errors.JaxTransformError: Jax transforms and Flax models cannot be mixed."" when trying to convert it to TFLite using `TFLiteConverter.experimental_from_jax`. https://colab.research.google.com/gist/josephrocca/b9b7b4e92cb693772cb937302b4946d9/jaxflaxtransformationscannotbemixed.ipynb The `experimental_from_jax` function is of course experimental, and I understand that the jax2tf > tf saved model > tflite is the current recommended route, but just reporting this in case it is not expected behavior.",2022-07-02T16:13:44Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.8 TF 2.11,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56660,"Hi  !  Could you please at this issue. Attached gist in 2.8, 2.9 and nightly (2.10.0dev) for reference. Thank you!","I am not sure if this is a meaningful conversion since **`seed`** is not present in the INPUT OP of saved_model. >Could you please at this issue. Attached gist in 2.8, 2.9 and nightly (2.10.0dev) for reference. https://colab.sandbox.google.com/gist/mohantym/fc8ff3b5e7287d4a1eb43f0f6d2b1361/jaxflaxtransformationscannotbemixed.ipynbscrollTo=MVLK3QdhXT1L   !image !model_float32 tflite",Could you try again saving the model with `model.save` instead of `tf.saved_model.save` and follow the other steps as usual.," I think I might have confused you in the email I sent you  this Github issue is actually just about the encoder side of the model, and doesn't take a `seed` input. I isolated the encoder because I found it to be the source of this particular ""Jax transforms and Flax models cannot be mixed"" error, and I wanted to try to produce a minimal reproducible example of the error. It is my mistake for not explaining this in the email  sorry! Hey , this issue is about `tf.lite.TFLiteConverter.experimental_from_jax`, so I do not call `tf.saved_model.save` in the process. Per my original post, please see this notebook for the code that replicates the issue: https://colab.research.google.com/gist/josephrocca/b9b7b4e92cb693772cb937302b4946d9/jaxflaxtransformationscannotbemixed.ipynb Thanks!",I was able to replicate this on  TF 2.11. Please find the gist here. Thank you.,This looks like a issue from the model by seeing the error log here https://flax.readthedocs.io/en/latest/api_reference/flax.errors.htmlflax.errors.JaxTransformError which states `exception flax.errors.JaxTransformError.`  Could you please check if the issue is from the model side. Thanks!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1875,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OSError: symbolic link privilege not held  when use bazel build tensorflow)ï¼Œ å†…å®¹æ˜¯ (I got a error when i build tensorflow with bazel: My command is: bazel build config=opt //tensorflow:tensorflow_cc.dll local_ram_resources=1024   And i run it with admin ï¼Œ I got this : `INFO: Reading 'startup' options from f:\tensorflow\.bazelrc: output_user_root=F:/tf INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=120 INFO: Reading rc options for 'build' from f:\tensorflow\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe INFO: Reading rc options for 'build' from f:\tensorflow\.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false INFO: Reading rc options for 'build' from f:\tensorflow\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe action_env PYTHON_LIB_PATH=C:/Users/hx/AppData/Local/Programs/Python/Python36/Lib/sitepackages python_path=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_eigen_strong_inlin)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,alexHxun,OSError: symbolic link privilege not held  when use bazel build tensorflow,I got a error when i build tensorflow with bazel: My command is: bazel build config=opt //tensorflow:tensorflow_cc.dll local_ram_resources=1024   And i run it with admin ï¼Œ I got this : `INFO: Reading 'startup' options from f:\tensorflow\.bazelrc: output_user_root=F:/tf INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=120 INFO: Reading rc options for 'build' from f:\tensorflow\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe INFO: Reading rc options for 'build' from f:\tensorflow\.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false INFO: Reading rc options for 'build' from f:\tensorflow\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe action_env PYTHON_LIB_PATH=C:/Users/hx/AppData/Local/Programs/Python/Python36/Lib/sitepackages python_path=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_eigen_strong_inlin,2022-07-02T10:18:21Z,type:support,closed,0,1,https://github.com/tensorflow/tensorflow/issues/56658,Are you satisfied with the resolution of your issue? Yes No
1866,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Gradient Tape (tf.GradientTape) Returning All 0 Values in GradCam)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes (Custom Loss + GradCam)    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7    **TensorFlow installed from (source or binary)**: PIP    **TensorFlow version (use command below)**: 2.9.0    **Python version**: 3.7.0    **CUDA/cuDNN version**:  11.6    **GPU model and memory**: Nvidia RTX 2080Ti  Describe the problem I have trained 2 FCN segmentation models, with the same custom loss.   Model 1 >  Trained of 256x512 images, on TF==1.15.  Model 2 > Trained on 512x1024 images, on TF==2.9.0. **The same GradCam Script works fine on `Model 1`, but returns 0 (and eventually NaN) for `Model 2`.** **Model1.summary():**  **Model2.summary():**   Source code / logs **Custom Loss (common for both models)**  **GradCam Script (Common for both models)**  Predictions from both the model yield useful results. However, the heatmap generated via the `gradcam` script for Model 2 returns 0's, and `anything/max(zeros)=NaN` **In `Model 2`, there are no other changes, except minor ones, required to make the code compatible with TF>2, from TF==1.X. Why is it that the `tf.GradientTape()` is resulting in 0 values regardless of the image input, when the model (on the same images) is able to generate good predictions?** Kindly advise and help, Thanks. Edit 01 Kindly check a difference I noticed in the GradCam results of the two models. `Model 1` (Working Gradcam)  `Model 2` (NaN/0 heatmap)  Thus, the major difference seems to be in the 'class_channel' variable > )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,varungupta31,Gradient Tape (tf.GradientTape) Returning All 0 Values in GradCam," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes (Custom Loss + GradCam)    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7    **TensorFlow installed from (source or binary)**: PIP    **TensorFlow version (use command below)**: 2.9.0    **Python version**: 3.7.0    **CUDA/cuDNN version**:  11.6    **GPU model and memory**: Nvidia RTX 2080Ti  Describe the problem I have trained 2 FCN segmentation models, with the same custom loss.   Model 1 >  Trained of 256x512 images, on TF==1.15.  Model 2 > Trained on 512x1024 images, on TF==2.9.0. **The same GradCam Script works fine on `Model 1`, but returns 0 (and eventually NaN) for `Model 2`.** **Model1.summary():**  **Model2.summary():**   Source code / logs **Custom Loss (common for both models)**  **GradCam Script (Common for both models)**  Predictions from both the model yield useful results. However, the heatmap generated via the `gradcam` script for Model 2 returns 0's, and `anything/max(zeros)=NaN` **In `Model 2`, there are no other changes, except minor ones, required to make the code compatible with TF>2, from TF==1.X. Why is it that the `tf.GradientTape()` is resulting in 0 values regardless of the image input, when the model (on the same images) is able to generate good predictions?** Kindly advise and help, Thanks. Edit 01 Kindly check a difference I noticed in the GradCam results of the two models. `Model 1` (Working Gradcam)  `Model 2` (NaN/0 heatmap)  Thus, the major difference seems to be in the 'class_channel' variable > ",2022-07-02T09:18:51Z,stat:awaiting response type:bug stale comp:ops TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56657,", I ran the code shared and face a different error, please find the gist here and share all dependencies to replicate the issue or share a colab gist with the reported error. Thank you!"," Hey!  To run the code, you'll need to model file, which I can't share as of now. Here is a complete gist containing relevant outputs and reported error. Basically, the point is: When this gist is run using Model trained in TF==2.9.0, I get these results (NaN) but the same model, with same paramters, etc. is trained in TF==1.15, the gradcam works fine on the same image. ",", I ran the code and faced a different error, please find the gist here and share all dependencies to replicate the issue. Thank you!", please read the previous comment? ,", Without the required code, it would be difficult for us to debug the issue. In order to expedite the troubleshooting process, could you please provide a complete code and the dependencies. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to predict a TPU-distributed dataset?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.4.1  Custom Code Yes  OS Platform and Distribution kaggle (linux)  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I have some TPUdistributed dataset, created similar to the below.      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')     tf.config.experimental_connect_to_cluster(resolver)     tf.tpu.experimental.initialize_tpu_system(resolver)     print(""All devices: "", tf.config.list_logical_devices('TPU'))     strategy = tf.distribute.TPUStrategy(resolver) ___     20220701 07:06:09.546401: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set     20220701 07:06:09.549373: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib     20220701 07:06:09.549403: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)     20220701 07:06:09.549429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (758f6ae4e228): /proc/driver/nvidia/version does not exist     20220701 07:06:09.553473: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ghost,How to predict a TPU-distributed dataset?,"Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.4.1  Custom Code Yes  OS Platform and Distribution kaggle (linux)  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I have some TPUdistributed dataset, created similar to the below.      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')     tf.config.experimental_connect_to_cluster(resolver)     tf.tpu.experimental.initialize_tpu_system(resolver)     print(""All devices: "", tf.config.list_logical_devices('TPU'))     strategy = tf.distribute.TPUStrategy(resolver) ___     20220701 07:06:09.546401: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set     20220701 07:06:09.549373: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib     20220701 07:06:09.549403: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)     20220701 07:06:09.549429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (758f6ae4e228): /proc/driver/nvidia/version does not exist     20220701 07:06:09.553473: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to",2022-07-01T07:40:18Z,stat:awaiting response type:support stale comp:tpus TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56648,"Tensorflow version 2.4 not actively supported. Hence, kindly update to latest stable version 2.9 and let us know if you are facing the same issue. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
700,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to convert tf.data.Dataset column which is converted into byte, back to string?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dexter1729,"How to convert tf.data.Dataset column which is converted into byte, back to string?",Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8.2  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-07-01T06:51:48Z,stat:awaiting response type:support stale comp:data TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56646,"  In order to expedite the troubleshooting process, please provide a complete code snippet to reproduce the issue reported here. Please refer this link and let us know if it helps? Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build Error: cannot convert â€˜float*â€™ to â€˜float_t*â€™ {aka â€˜long double*â€™} in assignment)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version 5.0.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.3.1/8.2.1.32  GPU model and memory None, Using cuda stub  Current Behaviour?   Standalone code to reproduce the issue  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,akowalsk,Build Error: cannot convert â€˜float*â€™ to â€˜float_t*â€™ {aka â€˜long double*â€™} in assignment,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version 5.0.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.3.1/8.2.1.32  GPU model and memory None, Using cuda stub  Current Behaviour?   Standalone code to reproduce the issue  ",2022-06-30T17:03:14Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56639,"Hi , Could you lower the GCC version to 9.3.1 and try to build Tensorflow. Thank you!",I cannot find any way to install that version on ubuntu 20.04.  I tried installing per your comments here: https://github.com/tensorflow/tensorflow/issues/56432 but it didn't work.,"Hi , Could you try the workarounds mentioned on similar thread  CC(crosstool_wrapper_driver_is_not_gcc failed: error executing command). Thank you!","Adding those two options didn't change the error, but pointed me in the right direction.  It looks like I get the error happened because of the `copt=mfpmath=both` option.  When I use `copt=mfpmath=sse` or leave it out, it works.  Specifying `copt=mfpmath=387` or `copt=mfpmath=both` cause the error.  I don't personally need the 387 support, so I'm fine with this workaround.","Hi , Glad that it resolved, could you move this to closed status?",Are you satisfied with the resolution of your issue? Yes No
1890,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Keras saved model with custom layer under MirroredStrategy scope cannot be used correctly)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04.5  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory 4  NVIDIA V100 16GB GPUs  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output bash 20220630 12:58:39.305720: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  SSE3 SSE4.1 SSE4.2 AVX To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220630 12:58:41.151528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  > device: 0, name: Tesla V100SXM216GB, pci bus id: 0000:06:00.0, compute capability: 7.0 20220630 12:58:41.153120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  > device: 1, name: Tesla V100SXM216GB, pci bus id: 0000:07:00.0, compute capability: 7.0 20220630 12:58:41.154428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  > device: 2, name: Tesla V100SXM216GB, pci bus id: 0000:0a:00.0, compute capability: 7.0 20220630 12:58:41.155692: I tensorflow/core/common_runtime/gpu/g)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,KingsleyLiu-NV,Keras saved model with custom layer under MirroredStrategy scope cannot be used correctly,"Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04.5  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory 4  NVIDIA V100 16GB GPUs  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output bash 20220630 12:58:39.305720: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  SSE3 SSE4.1 SSE4.2 AVX To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220630 12:58:41.151528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  > device: 0, name: Tesla V100SXM216GB, pci bus id: 0000:06:00.0, compute capability: 7.0 20220630 12:58:41.153120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  > device: 1, name: Tesla V100SXM216GB, pci bus id: 0000:07:00.0, compute capability: 7.0 20220630 12:58:41.154428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  > device: 2, name: Tesla V100SXM216GB, pci bus id: 0000:0a:00.0, compute capability: 7.0 20220630 12:58:41.155692: I tensorflow/core/common_runtime/gpu/g",2022-06-30T13:06:56Z,stat:awaiting response type:support stale comp:dist-strat TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56638,"Hi, Could you try defining the mirrored strategy scope with all the GPU's you want to use like below and let us know if it works. You can also try defining the mirrored strategy scope at the beginning and bring all the code under single scope.  `ex: strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])` Also, refer this answer33) for some details.","It does not work if do like this:  I mean, I use a new script to load the saved model and try to run it with MirroredStrategy. In the log, I can still see:  While I am expecting to see: ","> You can also try defining the mirrored strategy scope at the beginning and bring all the code under single scope I cannot do this. I need to save the model in one script and load the model in another script, and they cannot share the same single scope. The real case is that I have a SavedModel from the training side and I want to use it in the inference side. I try to load the SavedModel within MirroredStrategy scope and expect it to work across the multiple GPUs, but actually only one device is found according to the log.  I want to know if there is anything wrong with saving the model or loading the model with MirroredStrategy. ", Any progress here? , Any progress here?, Any progress here?, Any progress here?,You guys have really bad management on the reported issues: no one is being responsible and no one is replying! ,  Any progress here?,This issue appears to be a duplicate of CC(Save and load model within MirroredStrategy) and Keras Issue kerasteam/tfkeras CC(Changing 1 kernel op requires recompile of the entire kernel package... slow : (); we can replicate updates between them or close one as progress is made.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
674,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory leak when using the optimizer iterations in `tf.data.Dataset`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 16.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  See Colab  Standalone code to reproduce the issue   Relevant log output )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gcuder,Memory leak when using the optimizer iterations in `tf.data.Dataset`,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 16.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  See Colab  Standalone code to reproduce the issue   Relevant log output ,2022-06-29T13:52:31Z,stat:awaiting response type:bug stale comp:data TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56624,", While I was trying to reproduce the issue on tensorflow v2.8, v2.9 and nightly, the code execution time was longer than expected. Kindly find the gist of it here.", any updates so far? ,Is this issue solvable? Do you need any input from my side? ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
741,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Get Input/Output shapes of node (ex: Conv, Add) during the inferencce)ï¼Œ å†…å®¹æ˜¯ (Hi, I would like to know how can we access input and output shapes of a node (ex: Convolution, Add) from the reampper file.  I am trying to access input shapes in function [IsAddWithNoBroadcast] (https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/core/grappler/optimizers/remapper.ccL843) but I am not successful in retrieving the input shapes of node. Can someone help me to get the input shapes of node with in a remapper file? Thanks!!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Alavandar08,"Get Input/Output shapes of node (ex: Conv, Add) during the inferencce","Hi, I would like to know how can we access input and output shapes of a node (ex: Convolution, Add) from the reampper file.  I am trying to access input shapes in function [IsAddWithNoBroadcast] (https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/core/grappler/optimizers/remapper.ccL843) but I am not successful in retrieving the input shapes of node. Can someone help me to get the input shapes of node with in a remapper file? Thanks!!",2022-06-29T12:29:09Z,stat:awaiting response stale type:others comp:grappler TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56622,"Hi , We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported. Thank you!",Hi   The type of issue is **others** This issue (i.e getting shapes is not specific to particular version. It is across all versions). For easier reference we can consider     (TF2.9). From my experiments I am trying to dump the shapes into a log file and verify if the logs been printed or not. I tried to get node input shapes from couple of places some of them are   1. IsAddwithNoBroadcast  We have access to node of type 'NodeDef' 2. SymbolicshapesEqual We have access to input properties of a node that are obtained from context of type 'TensorShapeProto',"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
765,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Could not find device for node: {{node Qr}} = Qr[T=DT_HALF])ï¼Œ å†…å®¹æ˜¯ (* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Â Ubuntu 20.04.2 * TensorFlow installed from (source or binary):Â Binary * TensorFlow version (use command below):Â Tensorflow 2.7 * Python version:Â 3.7 Iâ€™m running into errors when using `tf.linalg.qr()` with inputs of half precision. The documentation lists float16 as a supported type. Everything works fine for iterations of single precision. Testing with this snippet returns a similar error to what happens when I run the source code below.  Source Output:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,wellerbii,Could not find device for node: {{node Qr}} = Qr[T=DT_HALF],"* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Â Ubuntu 20.04.2 * TensorFlow installed from (source or binary):Â Binary * TensorFlow version (use command below):Â Tensorflow 2.7 * Python version:Â 3.7 Iâ€™m running into errors when using `tf.linalg.qr()` with inputs of half precision. The documentation lists float16 as a supported type. Everything works fine for iterations of single precision. Testing with this snippet returns a similar error to what happens when I run the source code below.  Source Output:  ",2022-06-28T15:22:09Z,stat:awaiting response type:bug stale comp:apis TF 2.7,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56610,", Thanks for opening this issue. This is a duplicate of issue https://github.com/tensorflow/tensorflow/issues/56441.  Can you please close this issue, since it is already being tracked there? ",Sure thing,Are you satisfied with the resolution of your issue? Yes No,Are you satisfied with the resolution of your issue? Yes No," The other thread has been closed but I am still experiencing the same issue for v2.7, v2.8, and release 2.9 even after fresh installs.",", >I am still experiencing the same issue for v2.7, v2.8, and release 2.9 even after fresh installs. This issue is fixed in `tfnightly` and it will be part of the next stable release `TF2.10`.  For now please try to install `tfnightly` as  `!pip install tfnightly` (Not the versions mentioned above)  Output:  Thank you.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
662,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(trap invalid opcode in libtensorflow_io_plugins.so)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v1.12.177459g4ff6b08be52 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.11  GCC/Compiler version 8.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FireCulex,trap invalid opcode in libtensorflow_io_plugins.so,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v1.12.177459g4ff6b08be52 2.10.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.11  GCC/Compiler version 8.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-28T13:52:37Z,type:build/install subtype: ubuntu/linux,closed,0,2,https://github.com/tensorflow/tensorflow/issues/56608,Recompiled tensorflow_io from instructions found here: https://www.tensorflow.org/io/development Had to install tfmodelsofficial which automatically downgrades Tensorflow and had to reinstall the pip package I made from source.  Appears to be running.,Are you satisfied with the resolution of your issue? Yes No
620,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MKL use cannot be disabled )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF 2.8.0  Custom Code No  OS Platform and Distribution Linux CentOS 8  Mobile device _No response_  Python version 3.8.8  Bazel version 4.2.1  GCC/Compiler version 8.2.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hshazly,MKL use cannot be disabled ,Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF 2.8.0  Custom Code No  OS Platform and Distribution Linux CentOS 8  Mobile device _No response_  Python version 3.8.8  Bazel version 4.2.1  GCC/Compiler version 8.2.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-28T08:57:19Z,stat:awaiting response type:bug comp:mkl TF 2.8,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56603,"Hi , Custom `TFoneDNN` ops are disabled by default, but can be enabled by setting an environment variable `TF_ENABLE_ONEDNN_OPTS=1`. You can build with just `config=opt`. Thank you!","Thank you . I built with `config=opt c dbg strip=never` to include the debug info in the binaries and it didn't call `jit_avx512_common_gemm_f32` anymore. It just calls `SetupOrDisableJit`, which I read that it deactivates JIT kernels until it is reenabled again. ",Are you satisfied with the resolution of your issue? Yes No
1874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Basic Keras model underperforming against Scikit-Learn MLPRegressor model)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.9.0  Custom Code Yes  OS Platform and Distribution macOS Monterey  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I've experimented with sklearn's MLPRegressor class and have seen that it does fairly well for the dataset I'm looking at without much tuning. However, I'd like to be able to build out a more complex model in Tensorflow/Keras using a split LSTM and Dense network. To fulfill that end, I'm trying to first replicate the performance of MLPRegressor in Tensorflow for a very basic architecture, but struggling so far. Here's an attempt to create identical models with each. The parameters in the TF implementation are intended to be based on the MLPRegressor documentation, including certain default values. Running the code below, there are two noticeable observations:  The loss during training is roughly half for MLPRegressor versus the TF model. This is also what I've observed on the real dataset.  The final MSE of the predictions on the training set is always lower for MLPRegressor (note: I'm not sure if the random seeds have the same effect on both models, but if you run it a few times, you should see this). Just a note  I've posted this on StackOverflow, Reddit, and sklearn's GitHub without any substantive responses, even though the code should be fully reproducible.  This is kind of a last resort, but I'm hoping that someone here might be intereste)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jhogg11,Basic Keras model underperforming against Scikit-Learn MLPRegressor model,"Click to expand!    Issue Type Performance  Source binary  Tensorflow Version 2.9.0  Custom Code Yes  OS Platform and Distribution macOS Monterey  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I've experimented with sklearn's MLPRegressor class and have seen that it does fairly well for the dataset I'm looking at without much tuning. However, I'd like to be able to build out a more complex model in Tensorflow/Keras using a split LSTM and Dense network. To fulfill that end, I'm trying to first replicate the performance of MLPRegressor in Tensorflow for a very basic architecture, but struggling so far. Here's an attempt to create identical models with each. The parameters in the TF implementation are intended to be based on the MLPRegressor documentation, including certain default values. Running the code below, there are two noticeable observations:  The loss during training is roughly half for MLPRegressor versus the TF model. This is also what I've observed on the real dataset.  The final MSE of the predictions on the training set is always lower for MLPRegressor (note: I'm not sure if the random seeds have the same effect on both models, but if you run it a few times, you should see this). Just a note  I've posted this on StackOverflow, Reddit, and sklearn's GitHub without any substantive responses, even though the code should be fully reproducible.  This is kind of a last resort, but I'm hoping that someone here might be intereste",2022-06-28T01:31:28Z,stat:awaiting response stale comp:keras type:performance TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56599,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,"Okay, thanks for the response.", Could you please confirm if you have posted the issue on  kerasteam/keras repo.. Thank you!,">  Could you please confirm if you have posted the issue on kerasteam/keras repo.. Thank you! The Keras GitHub states to post it on the Tensorflow forum, and I did that, so that's one more venue where I've posted the question without a meaningful response. https://discuss.tensorflow.org/t/basickerasmodelunderperformingagainstscikitlearnmlpregressor/10485",", I could able to reproduce the issue with `Tf 2.9`take a look at gist here. Thank you!","> , I could able to reproduce the issue with `Tf 2.9`take a look at gist here. Thank you! Thanks for posting this.  By the way, if the loop is changed from: `for _ in range(100):` to `for i in range(100):` And then the value of `i` is used as the seed, including adding `random_state=i` to `make_regression`, that should give a fully consistent result every time.  Sorry, I missed that the first time. When I run it like that, I get averages over 100 loops of:  So it gives a little bigger difference between the two than your result (and slightly smaller than what I originally posted). And just to clarify, my assumption with this has been that there is something in the construction of the `MLPRegressor` model that should be able to be replicated in TF (rather than a bug or otherwise in TF).  I just can't figure out what it is."," As mentioned here, KerasRegressor is deprecated and we are no longer maintaining/bug fixing it. Please forward this issue to https://github.com/adriangb/scikeras",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.
682,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(InvalidArgumentError: Removed XLA support for adding ragged Tensors in TF 2.9)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SebastianCa,InvalidArgumentError: Removed XLA support for adding ragged Tensors in TF 2.9,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-27T17:11:29Z,stat:awaiting response type:bug stale comp:xla TF 1.12 regression issue,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56595,", I was able to reproduce the issue. Code works fine with TF v2.8, whereas with TF v2.9 and TFnightly the code got executed with error. Kindly find the gist of it here. Thank you!",I don't think RaggedTensors ever worked in XLA  what HLO did they produce?,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I would like to reopen this issue as it has not been solved. > I don't think RaggedTensors ever worked in XLA  what HLO did they produce? The above example works in TF <=2.8, i.e., RaggedTensors have been supported in XLA in the past.",They've never been; it must have been a bug it did not error out before.
1059,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Clip weight values when converting to int8 tflite models)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): TF 1.14  2. Code Provide code to help us reproduce your issues using one of the following options:   3. Failure after conversion Hi,  I'm trying to limit the range the weight values of tflite models because there are limited supported range of weight values on the  hardware I use. Therefore, I have tried to apply `default_ranges_stats` when converting the trained model to an int8 tflite model. However, the weight values of converted model are not changed. Is there other methods to limit the range of tflite models during the conversion stage? Best Regards)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rahn80643,Clip weight values when converting to int8 tflite models," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): TF 1.14  2. Code Provide code to help us reproduce your issues using one of the following options:   3. Failure after conversion Hi,  I'm trying to limit the range the weight values of tflite models because there are limited supported range of weight values on the  hardware I use. Therefore, I have tried to apply `default_ranges_stats` when converting the trained model to an int8 tflite model. However, the weight values of converted model are not changed. Is there other methods to limit the range of tflite models during the conversion stage? Best Regards",2022-06-27T07:27:37Z,stat:awaiting response type:support stale comp:lite TF 1.14 TFLiteConverter,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56587,Hi  !  1.x versions are not supported any more Min and Max values  in the tuple of default_ranges_stats has to be integer ones. Please test with integer values and let us know if it works. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1587,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to build tensorflow from source, so that it works similar across INTEL and AMD CPUs (in terms of floating point math precision))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 1.15  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.6.9  Bazel version 0.26.1  GCC/Compiler version 7.5.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to build tensorflow from source(v1.15) as I was getting a difference in floating point math precision across INTEL and AMD CPUs(refer to issue CC(Tensor Flow gives different results on INTEL and AMD CPUs)). What are the possible options to provide for the build command to fix this? I am looking for a solution for either TF v1.15 or TF v2.3.  I tried the following build commands,   But the difference was still present. These commands are resulting in the following error,    The above error got resolved after making some changes to my ./configure file. But the resulting tensorflow packages are still resulting in the difference between AMD and INTEL. Please let me know what are the options that I can provide while building Tensorflow from source(for v1.15 or v2.3), so that the floating point math is consistent across AMD and INTEL   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,GChaitanya2001,"How to build tensorflow from source, so that it works similar across INTEL and AMD CPUs (in terms of floating point math precision)","Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 1.15  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.6.9  Bazel version 0.26.1  GCC/Compiler version 7.5.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to build tensorflow from source(v1.15) as I was getting a difference in floating point math precision across INTEL and AMD CPUs(refer to issue CC(Tensor Flow gives different results on INTEL and AMD CPUs)). What are the possible options to provide for the build command to fix this? I am looking for a solution for either TF v1.15 or TF v2.3.  I tried the following build commands,   But the difference was still present. These commands are resulting in the following error,    The above error got resolved after making some changes to my ./configure file. But the resulting tensorflow packages are still resulting in the difference between AMD and INTEL. Please let me know what are the options that I can provide while building Tensorflow from source(for v1.15 or v2.3), so that the floating point math is consistent across AMD and INTEL   Standalone code to reproduce the issue   Relevant log output _No response_",2022-06-27T06:14:37Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 1.15 TF 2.3,closed,0,17,https://github.com/tensorflow/tensorflow/issues/56586,I updated the issue. Please check. Thank you!,Hi  !  1.x versions are not supported any more. Could you check the below command with the 2.8/2.9 version and let us know the difference.  Command to build for AMD wheels is  `bazel build config=opt  define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package `Command to build for Intel OneDNN wheels  `bazel build config=mkl c opt copt=march=native //tensorflow/tools/pip_package:build_pip_package` Thank you!,"Hi , Thank you for the response. I actually tried installing TF v2.9 binary using pip. That version wasn't giving any difference across INTEL and AMD. Can I use the above commands for v2.3?", ! You can use the above commands in the 2.3 version too. Please let us know if results match.Thank you!,"Hi , I built TF v2.3 using above two commands and I am still able to see the difference between AMD and INTEL. Please let me know about any other flag I can provide. Thank you!",Hi   ! Could you look at this issue. Thank you!,"Hi all,  Can I know what does Tensorflow use in the background to perform floating point math operations like sqrt?  I found this, TF 2.3  https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/core/kernels/cwise_op_sqrt..9  https://github.com/tensorflow/tensorflow/blob/r2.9/tensorflow/core/kernels/cwise_op_sqrt.. Is that the reason why TF 2.3 and TF 2.9 are behaving differently? (As v2.9 wasn't behaving differently, but v2.3 does behave differently, I am comparing these two versions, in a hope that it will help identify the correct compiler flags that I can use to build TF from source for TF 2.3)  Please let me know your views on this. Thank you!",You can check the history for the file. Most likely you will be interested in 7c88fd1dc7ecf720739c4b39a62c81de32071fc3 which is available from TF 2.5,"Hi , Thank you for the response! But I am mostly looking around CPU based Op implementations, especially sqrt/rsqrt. I am trying to understand what instructions are used by tensorflow in the backend to perform those ops, as I wanted to build TF from source with proper compiler options to avoid mismatches between AMD and INTEL for floating point ops.  ( CC(Tensor Flow gives different results on INTEL and AMD CPUs)) Thank you!","Is there a way to build TF **without** using basic CPU optimizations like SSE, SSE2 etc on x8664 CPUs? Looks like using march=x8664 enables SSE, SSE2, SSE3 by default which might be resulting in the mismatch. Please let me know about any build options which does not allow SSE and other CPU optimizations on x8664 CPUs. Thank you!","You need to pass these flags to the compiler itself. Bazel has `cxxopts`, `copts`, etc., but you'll need to read your compiler's documentation to find out what flags to send to it to not do these optimizations."," , Could you please confirm whether the above comment1182124567 solved your issue. Eager to know if this is still an issue. Thanks!","  Yes in a way, I updated necessary workaround I found at CC(Tensor Flow gives different results on INTEL and AMD CPUs) Thanks!","Hi  , Thanks for coming back and confirming that the issue got resolved with a workaround. Please feel free to close the issue if resolved. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SavedModelBundle object instantiation raises protobuf error)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.9.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.0.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version CUDA 11.2 / cuDNN 8.1  GPU model and memory Tesla P100PCIE12GB  Current Behaviour?   Could some one please tell me why protobuf is throwing this error and how to resolve this. shell MAIN.CPP include  include ""tensorflow/cc/client/client_session.h"" include ""tensorflow/core/framework/tensor.h"" include ""tensorflow/cc/saved_model/loader.h"" using tensorflow::string; using tensorflow::Tensor; using tensorflow::tstring; using tensorflow::SavedModelBundle; using tensorflow::SessionOptions; using tensorflow::RunOptions; using tensorflow::ClientSession; int main(int argc, char* argv[]){ 	if (argc != 2){ 		std::cout  "" set_allow_growth(true);	     auto status = tensorflow::LoadSavedModel(session_options, run_options, model_path, {""serve""}, &bundle); 	if (status.ok()){ 		printf(""Model loaded successfully...\n""); 	} 	else { 		printf(""Error in loading model\n""); 	}     return 0; }  cmake_minimum_required(VERSION 3.16) project(tensorflow_v2_cpp) find_package(CUDA REQUIRED) set(TENSORFLOW_LIB_DIR ""/tensorflow/bazelbin/tensorflow"") add_executable(get_prediction main.cpp) target_include_directories(get_prediction PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include) target_include_directories(get_prediction PRIVATE ${TENSORFLOW_LIB_DIR}/include ${OpenCV_INCLUDE_DIRS}) target_link_libraries(get_prediction ${TENSORFLOW_LIB_DIR}/libtensorflow_cc.so ${TENSOR)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kamalbowselvam,SavedModelBundle object instantiation raises protobuf error,"Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.9.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 5.0.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version CUDA 11.2 / cuDNN 8.1  GPU model and memory Tesla P100PCIE12GB  Current Behaviour?   Could some one please tell me why protobuf is throwing this error and how to resolve this. shell MAIN.CPP include  include ""tensorflow/cc/client/client_session.h"" include ""tensorflow/core/framework/tensor.h"" include ""tensorflow/cc/saved_model/loader.h"" using tensorflow::string; using tensorflow::Tensor; using tensorflow::tstring; using tensorflow::SavedModelBundle; using tensorflow::SessionOptions; using tensorflow::RunOptions; using tensorflow::ClientSession; int main(int argc, char* argv[]){ 	if (argc != 2){ 		std::cout  "" set_allow_growth(true);	     auto status = tensorflow::LoadSavedModel(session_options, run_options, model_path, {""serve""}, &bundle); 	if (status.ok()){ 		printf(""Model loaded successfully...\n""); 	} 	else { 		printf(""Error in loading model\n""); 	}     return 0; }  cmake_minimum_required(VERSION 3.16) project(tensorflow_v2_cpp) find_package(CUDA REQUIRED) set(TENSORFLOW_LIB_DIR ""/tensorflow/bazelbin/tensorflow"") add_executable(get_prediction main.cpp) target_include_directories(get_prediction PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include) target_include_directories(get_prediction PRIVATE ${TENSORFLOW_LIB_DIR}/include ${OpenCV_INCLUDE_DIRS}) target_link_libraries(get_prediction ${TENSORFLOW_LIB_DIR}/libtensorflow_cc.so ${TENSOR",2022-06-26T17:36:39Z,stat:awaiting response stale type:others comp:runtime TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56582, Please take a look at this issue and also this discussion where they have discussed the cause of the issue and how it can be resolved. Can you please try with different versions of protobufs like 3.11 or 3.8 and let me know if you face the same issue. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
663,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(    ValueError: Shapes (None, 21) and (None, 1, 1, 21) are incompatible)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,MiraGaikwad,"    ValueError: Shapes (None, 21) and (None, 1, 1, 21) are incompatible",Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-25T17:25:48Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56577,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1266,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Auto-Applying XNNPACK Delegate)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.5.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Moving from TFLite 2.4.1 to 2.5.0 and compiling with `TFLITE_ENABLE_XNNPACK` using the Cmake build, it seems that in the latter the tflite delegate is automatically applied to the model. If you attempt to apply the xnnpack delegate explicitly in your own usage of the API, you see the error indicating a delegate has already been applied to the graph, making it immutable if another delegate is applied.  Is there a way to compile using CMake with xnnpack support such that the xnnpack delegate is not automatically applied starting with tf 2.5.0? I would like to explicitly apply the xnnpack delegate with my own options.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jishminor,Auto-Applying XNNPACK Delegate,"Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.5.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Moving from TFLite 2.4.1 to 2.5.0 and compiling with `TFLITE_ENABLE_XNNPACK` using the Cmake build, it seems that in the latter the tflite delegate is automatically applied to the model. If you attempt to apply the xnnpack delegate explicitly in your own usage of the API, you see the error indicating a delegate has already been applied to the graph, making it immutable if another delegate is applied.  Is there a way to compile using CMake with xnnpack support such that the xnnpack delegate is not automatically applied starting with tf 2.5.0? I would like to explicitly apply the xnnpack delegate with my own options.  Standalone code to reproduce the issue   Relevant log output  ",2022-06-24T21:46:03Z,stat:awaiting tensorflower type:build/install type:support comp:lite TF 2.5 comp:lite-xnnpack,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56571,Hi  !  You can disable the XNNpack behavior which is enabled by default using the below flag while building your cmake files. `TFLITE_ENABLE_XNNPACK = OFF` Attached relevant thread for reference. Thank you!,"Thanks for the response.  I was looking for a build option which would include the xnnpack delegate, but not automatically apply it when a model is loaded. I would like to have control in my application logic which determines whether or not to create and apply the xnnpack delegate.",Hi  ! Could you please look at this issue. Thank you!,"As suggested above, you can build the tflite with `TFLITE_ENABLE_XNNPACK = OFF` which disables `XNNPACK` by default, you can enable it in your code using the steps mentioned here.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"The option `TFLITE_ENABLE_XNNPACK = OFF` used with the cmake build of tflite v2.5.0 does not include xnnpack in the resultant library for tflite it seems. The tutorial referenced, uses the bazel build of tflite with the bazel build option: `//tensorflow/lite:tflite_with_xnnpack`. The ""with"" xnnpack option is not available for the cmake build.",Is the above correct  ?,"You can have the same options for Cmake also, check the details here https://www.tensorflow.org/lite/guide/build_cmakeavailable_options_to_build_tensorflow_lite","The issue is that if I build with the cmake flag TFLITE_ENABLE_XNNPACK = OFF, xnnpack is not included in the build, and application logic will not be able to apply the xnnpack delegate explicitly, which is the behavior i'm looking for.  You can verify this yourself via running the cmake build for tflite and passing `TFLITE_ENABLE_XNNPACK = OFF` as one of the cmake args. libXNNPACK.a is not built.",The solution to get the xnnpack delegate to not autoapply when tflite is built with xnnpack support is to use: tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates as the op resolver. This fixes the problem.,Are you satisfied with the resolution of your issue? Yes No
627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Indexing a ragged tensor over a non-ragged index fails in tf.keras.Model)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1, nightly  Custom Code No  OS Platform and Distribution colab  Mobile device _No response_  Python version 3.7  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pstjohn,Indexing a ragged tensor over a non-ragged index fails in tf.keras.Model,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1, nightly  Custom Code No  OS Platform and Distribution colab  Mobile device _No response_  Python version 3.7  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-06-24T14:46:09Z,stat:awaiting response type:bug stale comp:keras TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56565,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
649,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with Tensorflow Binary and oneAPI while flask deployment)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version tensorflowcpu  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Siddharth1India,Issue with Tensorflow Binary and oneAPI while flask deployment,Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version tensorflowcpu  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-24T10:20:32Z,stat:awaiting response type:build/install,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56561,", In order to expedite the troubleshooting process, could you please provide the following information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: TensorFlow installed from (source or binary): TensorFlow version: Python version: Installed using virtualenv? pip? conda?: Bazel version (if compiling from source): GCC/Compiler version (if compiling from source): CUDA/cuDNN version: GPU model and memory: and the exact sequence of commands / steps that you executed before running into the problem and the error log.  Please have a look at the compatible tested build configurations and try to install tensorflow v2.9 with compatible version.  Thank you!"," Issue turned out to be RAM of machine.  Changing machine to higher end solved issue, Thanks.",Are you satisfied with the resolution of your issue? Yes No
1853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with building Tensorflow with NCCL from source)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS (Focal Fossa)  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version Cuda 11.6 / cuDNN 8.4.1  GPU model and memory Tesla P100PCIE12GB  Current Behaviour?  I get the following error:   but when I run with config=nonccl, everything works fine. Could some one tell me why I getting this error for NCCL and how to fix.  I also installed nccllocalrepoubuntu20042.12.12cuda11.6_1.01_amd64 on my machine successfully but still I am not able to build with NCCL. shell 1) Download Tensorflow and configure it with bazel for CUDA  2) Run bazel build c opt //tensorflow:tensorflow_cc shell INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=161 INFO: Reading rc options for 'build' from /data/softs/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /data/softs/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false INFO: Reading rc options for 'build' from /data/softs/te)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kamalbowselvam,Issue with building Tensorflow with NCCL from source,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS (Focal Fossa)  Mobile device _No response_  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version Cuda 11.6 / cuDNN 8.4.1  GPU model and memory Tesla P100PCIE12GB  Current Behaviour?  I get the following error:   but when I run with config=nonccl, everything works fine. Could some one tell me why I getting this error for NCCL and how to fix.  I also installed nccllocalrepoubuntu20042.12.12cuda11.6_1.01_amd64 on my machine successfully but still I am not able to build with NCCL. shell 1) Download Tensorflow and configure it with bazel for CUDA  2) Run bazel build c opt //tensorflow:tensorflow_cc shell INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=161 INFO: Reading rc options for 'build' from /data/softs/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /data/softs/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library experimental_link_static_libraries_once=false INFO: Reading rc options for 'build' from /data/softs/te",2022-06-23T17:09:14Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56554,", Could you try with tested build configurations as shown below and let us know? !image Thank you.","Hi,  I tried to build with the above mentioned configuration and i am getting this error:  Tensorflow  2.9.0 Python  3.8  GCC  9.4.0 Bazel  5.0.0  CUDA  11.2 cuDNN  8.1 ` The given http link given for llvm seems to be not available. Could you please tell how to fix this ? ",", The timeout issue is due to github. Note that the .zip version of the same URL works fine. Try changing this line to .zip should work. Thank you.","It worked, Thank you for helping me with this. ",Are you satisfied with the resolution of your issue? Yes No
889,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Duplicate logging messages after calling `model.save()`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.3  GPU model and memory _No response_  Current Behaviour? When using a logger from python's `logging` module, messages are duplicated after a model is saved. For example:  Will yield (some output not shown):  Logging is not duplicated if `logging.basicConfig()` is called before model saving:  This behavior is only observed in Tensorflow 2.9 and does not occur with version )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jamesdolezal,Duplicate logging messages after calling `model.save()`,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.3  GPU model and memory _No response_  Current Behaviour? When using a logger from python's `logging` module, messages are duplicated after a model is saved. For example:  Will yield (some output not shown):  Logging is not duplicated if `logging.basicConfig()` is called before model saving:  This behavior is only observed in Tensorflow 2.9 and does not occur with version ",2022-06-23T14:19:01Z,stat:awaiting response type:bug comp:keras regression issue TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56549," I was able to replicate the issue on colab, please find the gist here. Thank you!",", Thank you for opening this issue.  Development of keras moved to separate repository https://github.com/kerasteam/keras/issues.  Please post this issue on `kerasteam/keras repo`. For more details please refer here. ",", Thanks, I have posted the issue on the keras repo.",Are you satisfied with the resolution of your issue? Yes No
899,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow lite GPU Delegate error with JNI(C++) on android 10(API 29))ï¼Œ å†…å®¹æ˜¯ (**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): Galaxy S10 `(samsung/beyond1lteks/beyond1:10/QP1A.190711.020/G973NKSU4CTE9:user/releasekeys)`  TensorFlow version : 2.9.0  Bazel version (if compiling from source): 5.2.0 **Describe the problem** I tried to segmentation inference with tflite model on mobile. I have placed the tflite model in `main_app/src/main/assets` and run my code. I succeeded in running tflite model with cpu, but I has encountered error when running with GPU delegate. **Source code / logs** In Kotlin,   In C++ JNI  Logcat  Dependcy & Asset  tflite model)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jyh2378,Tensorflow lite GPU Delegate error with JNI(C++) on android 10(API 29),"**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible): Galaxy S10 `(samsung/beyond1lteks/beyond1:10/QP1A.190711.020/G973NKSU4CTE9:user/releasekeys)`  TensorFlow version : 2.9.0  Bazel version (if compiling from source): 5.2.0 **Describe the problem** I tried to segmentation inference with tflite model on mobile. I have placed the tflite model in `main_app/src/main/assets` and run my code. I succeeded in running tflite model with cpu, but I has encountered error when running with GPU delegate. **Source code / logs** In Kotlin,   In C++ JNI  Logcat  Dependcy & Asset  tflite model",2022-06-23T07:40:55Z,stat:awaiting response type:support stale comp:lite TFLiteGpuDelegate TF 2.9,closed,0,16,https://github.com/tensorflow/tensorflow/issues/56545,"Hi  !  Above Error >  ""CAST: Not supported Cast case. Input type: UINT8 and output type: FLOAT32""  points towards incompatibility of input as UINT8 and output type as FLOAT32. Could you set as inference_input and inference_output as either UINT8 or FLOAT32 and let us know the result.  Thank you!",", Thanks for reply! It is hard to generate new tflite because its saved_model seems to be made from tf 1(and has no tag), so I changed to the model that has same type of input and output (float32). new model  input shape is [1 x 2 x 512 x 640 x 3] After that, cast error has gone, but another error has appeared. ", ! All operations are not supported by the GPU delegate. Could you check with the below options and let us know. 1. Test Uint8 or int8 (both inference input or output type) (Skip to next step if still persits) 2. Switch to NNAPI delegate in case operations fail in GPU delegate. Reference Thank you!,", Thank you for adivce! Before trying to test what you saying , I have a some questions. 1. NN inference with gpu delegate should have same input type and output type? 2. What is the difference of NNAPI and GPU? I understand that NNAPI also uses gpu. 3. NNAPI delegate can be run on the C++? I tried to find some references about NNAPI, but there isn't any guide that compile lib file(.so or .a) or example with C++.",Hi  ! Could you look at this issue.,"In the log which you have provided, I was able to see that the GPU delegate is not initialized, could you refer the comment here and the issue thread for more details on the issue and let us know if this helps. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,", sorry for delay! I have made a simple network which has 1x512x512x3 input shape and 1x512x512x2 output shape. Then I ran the network on my phone. model: link !image result log:   The error was raised in `interpreter>AllocateTensors()`. It could be seen that the GPU delegate is initialized, isn't? What else can I do?","I'm not sure why it's failing, but ... the whole path with SHAPE > STRIDED_SLICE > PACK is a static thing you can replace with a static tensor. On Thu, Jul 21, 2022 at 5:22 AM Sachin Prasad ***@***.***> wrote: > Assigned CC(Tensorflow lite GPU Delegate error with JNI(C++) on android 10(API 29))  > to  . > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were assigned.Message ID: >  ***@***.***> >", Thanks for advice. But same code (same model and same inputs) on cpu work well. I don't know what I missed...,"CPU and GPU have completely separate implementations.  Something that runs on CPU is not guaranteed to work on GPU.  Often, we ask people to modify the network so that it runs on the GPU. On Tue, Jul 26, 2022 at 2:00 PM Jin YeongHwa ***@***.***> wrote: >   Thanks for advice. > But same code (same model and same inputs) on cpu work well. I don't know > what I missed... > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >", Thanks for information. But same error occurs with a model that have just one convolution layer. This problem does not seem to be related to the structure of the neural network. Could it be related to the Android version or other env setting?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1713,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(HloReachabilityMap does not update the BitVectors properly when there is a cycle in the HloComputation)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? In the HloReachabilityMap, the BitVectors are updated in topological order starting from the root node. However, if there is a cycle in the graph, e.g. A>B>C>A, with A being closer to the root, the following occurs: 1) A gets updated (ORed) with BitVector of C 2) At this point C's BitVector, has not been initialized, so this operation does nothing. Basically, if there's a cycle whichever instruction in the cycle is processed earlier will not be updated with the actual reachability BitVectors of the later instructions. Since the HloReachabilityMap is used in ""InstructionFusion::MultiOutputFusionCreatesCycle"" to detect cycles, so I believe it will no work properly in this use case. A trivial fix would be to just perform the BitVector update twice in HloReachabilityMap::Build, i.e. just repeat the following code twice:  By running it twice, we can ensure that e.g. in the second loop when we load A's BitMap with C's, C's BitMap already denotes that it's reachable from A, B and C (itself)  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,iamohcy,HloReachabilityMap does not update the BitVectors properly when there is a cycle in the HloComputation,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? In the HloReachabilityMap, the BitVectors are updated in topological order starting from the root node. However, if there is a cycle in the graph, e.g. A>B>C>A, with A being closer to the root, the following occurs: 1) A gets updated (ORed) with BitVector of C 2) At this point C's BitVector, has not been initialized, so this operation does nothing. Basically, if there's a cycle whichever instruction in the cycle is processed earlier will not be updated with the actual reachability BitVectors of the later instructions. Since the HloReachabilityMap is used in ""InstructionFusion::MultiOutputFusionCreatesCycle"" to detect cycles, so I believe it will no work properly in this use case. A trivial fix would be to just perform the BitVector update twice in HloReachabilityMap::Build, i.e. just repeat the following code twice:  By running it twice, we can ensure that e.g. in the second loop when we load A's BitMap with C's, C's BitMap already denotes that it's reachable from A, B and C (itself)  Standalone code to reproduce the issue   Relevant log output _No response_",2022-06-23T04:58:35Z,stat:awaiting response type:bug stale comp:xla TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56542,"Hi , To expedite the troubleshooting process, could you please provide the complete code to reproduce the issue reported here. Thank you!","Hi , to reproduce this issue, you can add this test to ""hlo_reachability_test.cc"" and it should fail ","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
642,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(An error occurred during the fetch of repository 'llvm-project':)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.4.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version 3.1.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.6/8  GPU model and memory RTX3080  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,danielgusland,An error occurred during the fetch of repository 'llvm-project':,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.4.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version 3.1.0  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.6/8  GPU model and memory RTX3080  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-22T18:42:53Z,type:build/install subtype: ubuntu/linux TF 2.4,closed,0,20,https://github.com/tensorflow/tensorflow/issues/56540,I've also seen this today on builds of the 2.9.1 branch and master. A simple `wget` times out too...  I suspect this is a GitHub issue?  No trouble getting to the GitHub llvm/llvmproject page though.,I'm also affected by this! ,"Hi , Clear the cache by calling `bazel clean â€“expunge` Command prompt must be `Run as Admin` for the bazel build of TensorFlow to be successful. Thank you!","Same issue here. Note that the .zip version of the same URL works, so this does look like a GitHub issue. I haven't tried it, but changing this line to .zip should work around it: https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/workspace.bzlL16","> Hi , Clear the cache by calling `bazel clean â€“expunge` Command prompt must be `Run as Admin` for the bazel build of TensorFlow to be successful. Thank you! Thank you for the response. Unfortunately, neither clean or run as admin helps in this case.  As mentioned by  mentions; a simple wget times out too.  Most likely a github server issue, but all their status flags are green. ","Clarifying my earlier comment, here is a patch that can be used to work around this issue: ","> Clarifying my earlier comment, here is a patch that can be used to work around this issue: >  Thank you for the workaround! I did get a similar warning but my build now running : ","Thanks , that work around has my build working again.  I found the same issue on a build off of the r2.9 branch, which is a different archive, but a similar fix works there too.",For anyone meet the problem about GZIP , ok it seems to work also for https://github.com/google/yggdrasildecisionforests/issues/23 I still see the following issue  seems not directly related... any idea?  full output: ,>  I think the 404 error was caused by simply there is no mirror on bazel or googleapis. You could't find the bazel script there is always a mirror and original repo url. So it would be fine.,you mean its a transient warning/error?,"We (OpenCE team)  are also facing the same problem since yesterday. I can't build TF 2.8 and TF 2.9.1 due to this. .zip format is being downloaded but not .tar.gz format. The recent commits of llvmproject are being downloaded in .tar.gz format. I'd raised these two issues   1. https://github.com/llvm/llvmproject/issues/56177 2. https://support.github.com/ticket/personal/0/1681014 Patching workspacebzl file as mentioned in one of the comments works. This issue is seen even with tensorflowtext and tensorflowserving builds. So, we would need to patch all of these packages of previous release and retag. ","The latest update to the commit hash in `third_party/llvm/workspace.bzl`  https://github.com/tensorflow/tensorflow/commit/c1b086bfce8ef6bd67082be3f1e70b835ad00236 now tries to download a tarball that _does_ exist, so the fixup shouldn't be needed for nightly builds anymore (for now at least!) However, that still leaves the 2.9 release branch broken  will this branch be reopened and patched to pickup the .zip?",We are also facing the issue with 2.9 release branch. Patching workspacebzl file as mentioned in one of the comments works.,The tars are fixed by Github. I could download both the commits of llvmproject in the form of .tar.gz.,"As  noted, the tars are fixed. Closing issue. ",Are you satisfied with the resolution of your issue? Yes No,"The .tar.gz downloads are still not completely fixed; I get random download failures, then starts working again when retrying a few minutes later.","Yes, I too see it intermittently. But Github support confirmed that llvm project was unstable last weekend and they have fixed it."
655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(6d transpose not supported by TFLite GPU delegate)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04 LTS  TensorFlow installed from (source or binary): binary (pip)  TensorFlow version (or github SHA if from source): 2.9.1 **Provide the text output from tflite_convert**  if SELECT_TF_OPS is enabled:  **Standalone code to reproduce the issue**   **Any other info / logs** 6D transform is a part of an image transformer model.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,vbogach,6d transpose not supported by TFLite GPU delegate,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04 LTS  TensorFlow installed from (source or binary): binary (pip)  TensorFlow version (or github SHA if from source): 2.9.1 **Provide the text output from tflite_convert**  if SELECT_TF_OPS is enabled:  **Standalone code to reproduce the issue**   **Any other info / logs** 6D transform is a part of an image transformer model.",2022-06-22T14:37:56Z,stat:awaiting tensorflower type:bug comp:lite TFLiteConverter TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56537,"Hi  ! Could you please look at this issue. Attached 2.8, 2.9 and nightly for reference.","As mentioned in the tflite_convert's error & warning messages, 6D tensors are not really supported in TFLite; only through Flex delegate which pulls in (nonmobile) TF code. My team mates were able to get away with a hack folding / unfolding tensors into lower ranks to be able to use the GPU delegate.  See whether you can do that.  When you do it naively, you will indeed end up with 6D tensors which is not TFLitefriendly, especially not TFLite GPU.",Are you satisfied with the resolution of your issue? Yes No,"I have tried to implement a way to avoid `Transpose` over 6D being converted to `FlexTranspose`. I even generated a random tensor in 8 dimensions 1,000 times to make sure the values matched Numpy's Transpose perfectly. The implementation is quite verbose. I don't know if this is a meaningful implementation or not. However, I have successfully converted the Swin Transformer. https://github.com/PINTO0309/onnx2tf/issues/93"
1502,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(New quantizer seems to ignore TFLITE_BUILTINS_INT8)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.1 and 2.9.1  2. Code The following script can be run directly to show the issue. A SavedModel is saved before the conversion and a TFLite file afterwards.   3. Failure after conversion I would expect (and want) the above script to fail, because the model intentionally contains ops that can not be quantized and I am using the following options:  `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`  `converter.target_spec.supported_types = [tf.int8]` From the TensorFlow docs:  Instead there are dequant/quant nodes inserted around the unsupported operator and the model is converted without any complaints. In other words: The used options seem to have no effect.  4. Tests and observations  When I use the old quantizer (`converter.experimental_new_quantizer = False`) I am getting the expected error.  After a quick look into the code there seems to be an option `fully_quantize` for `mlir_quantize()` here, but it seems to be not used here.  Might be related to this TODO in the cpp code.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,benkli01,New quantizer seems to ignore TFLITE_BUILTINS_INT8," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.1 and 2.9.1  2. Code The following script can be run directly to show the issue. A SavedModel is saved before the conversion and a TFLite file afterwards.   3. Failure after conversion I would expect (and want) the above script to fail, because the model intentionally contains ops that can not be quantized and I am using the following options:  `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`  `converter.target_spec.supported_types = [tf.int8]` From the TensorFlow docs:  Instead there are dequant/quant nodes inserted around the unsupported operator and the model is converted without any complaints. In other words: The used options seem to have no effect.  4. Tests and observations  When I use the old quantizer (`converter.experimental_new_quantizer = False`) I am getting the expected error.  After a quick look into the code there seems to be an option `fully_quantize` for `mlir_quantize()` here, but it seems to be not used here.  Might be related to this TODO in the cpp code.",2022-06-22T12:18:58Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter ModelOptimizationToolkit TF 2.9,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56535,"Hi  ! Could you please look at this issue . Attached gist in 2.8, 2.9 and nightly for reference. Thank you!","Hi,   It seems you are already assigned to this issue.  Could you take a look at this issue?  This is a blocker of ARM side. ",Thank you for reporting the issue! This behavior just does not throw an exception in the newer version. I wonder how the ARM side can be blocked because of this one.,"Hi .  Thanks for looking into this. The issue here is that there is no ""strict mode"". We can not rely on the converter to produce a TFLite graph that only contains int8 operations (which might be required by the hardware).",Hi. I just tested this with TensorFlow 2.10 and the issue seems to be still present. Are there any updates on this?,Hi. This is still an issue in TensorFlow 2.11. Any updates?,Hi   Th int8 support for `floormod` op has been added with commit https://github.com/tensorflow/tensorflow/commit/2ba376283d387b63d6c24bd48bea4dace386b017. It will be reflected in 2.13 version. Thanks.,"Hi , thanks for the headsup. I just tried the latest prerelease version which should have support for int8 in `floor_mod`, but I can still see the error: !image And if I switch to the old quantizer, I get the usual error:  `RuntimeError: Quantization not yet supported for op: 'FLOOR_MOD'.`","I just tested this with TensorFlow 2.13.0 and it is the same result as above, i.e. the error is still there (strangely enough, now that FLOOR_MOD should be supported for int8),  .","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1510,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow lite GPUDelegate with Android API 31)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes     **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: Pixel 6 pro    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: 2.9.0    **Python version**:  NA    **Bazel version (if compiling from source)**: NA    **GCC/Compiler version (if compiling from source)**: NA    **CUDA/cuDNN version**: NA    **GPU model and memory**: NA    **Exact command to reproduce**: NA  Describe the problem My model is a classifier trained using EfficientNet and runs on the camera feed using CameraX APIs. It expects a FloatBuffer of size 768 and returns a list of labels. This was working fine upto API 30, but once I update the `targetSdk` to 31 and run on a device with Android 12, I get this crash:   Source code / logs Initialization code:   The crash occurs on the line `interpreter = Interpreter(it, options)`  Inference code (never gets called in case of crash) :   where `passportInputImageConverter` would crop and convert the image to FloatBuffer using :    Dependency list: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,amol-tw,Tensorflow lite GPUDelegate with Android API 31," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes     **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: Pixel 6 pro    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: 2.9.0    **Python version**:  NA    **Bazel version (if compiling from source)**: NA    **GCC/Compiler version (if compiling from source)**: NA    **CUDA/cuDNN version**: NA    **GPU model and memory**: NA    **Exact command to reproduce**: NA  Describe the problem My model is a classifier trained using EfficientNet and runs on the camera feed using CameraX APIs. It expects a FloatBuffer of size 768 and returns a list of labels. This was working fine upto API 30, but once I update the `targetSdk` to 31 and run on a device with Android 12, I get this crash:   Source code / logs Initialization code:   The crash occurs on the line `interpreter = Interpreter(it, options)`  Inference code (never gets called in case of crash) :   where `passportInputImageConverter` would crop and convert the image to FloatBuffer using :    Dependency list: ",2022-06-22T10:09:10Z,stat:awaiting tensorflower type:bug comp:lite TFLiteGpuDelegate TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56533,Hi tw !  Sorry for the late response. Could you share the inference code snippet too along GPU delegate initialization. Thank you!," I have updated the issue with the inference code and more details, but I am afraid that it's never called as the crash happens during the initialization. ",Hi  ! Could you please look at this issue. Thank you!,"The error message tells you what's wrong.  Expected batch dimension of 768 but found 1.  The GPU delegate has some restrictions with the batch dimensions and requires that dimension to be consistent throughout the network.  In your case, you omitted the batch dimension, so your input is 768 instead of 1x768 which confuses the delegate.  I suggest you rearchitect the model to have an explicit batch size of 1 in every input & intermediate tensors. On Tue, Jul 12, 2022 at 8:44 AM Sachin Prasad ***@***.***> wrote: > Assigned CC(Tensorflow lite GPUDelegate with Android API 31)  > to  . > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were assigned.Message ID: > ***@***.***> >"," Thanks for the response.  Were the restrictions to GPUDelegate introduced recently? Can you point to me to some documentation around it? Asking because this does not explain that the model runs fine when `targetSDK` is set to 30 or lower. I tried a couple of other models also and got errors like the following:  and   Again, these models seem to run absolutely fine on `targetSDK` 30 but crash on 31. ","""Can not open OpenCL library on this device"" This is root problem with Android API 31. Looks like with Android API 30 it was able to open libOpenCL.so and use OpenCL delegate. In Android API 31 settings were changed probably and now libOpenCL not available for usual users. As result used OpenGL backend. But the problem is that OpenCL has much more ops covered and features(for example support of batch != 1). In your case OpenCL backend was able to run model and OpenGL can not. I don't think that in nearest future OpenGL will have similar feature parities to OpenCL backend. So I explained why you have this problem, but I don't know how to help you.","If it's the matter of libOpenCL.so not being detected, then it's about the Android manifest.  You have to add the following:  On Thu, Jul 14, 2022 at 5:23 PM Roman Sorokin ***@***.***> wrote: > ""Can not open OpenCL library on this device"" > This is root problem with Android API 31. > Looks like with Android API 30 it was able to open libOpenCL.so and use > OpenCL delegate. > In Android API 31 settings were changed probably and now libOpenCL not > available for usual users. > As result used OpenGL backend. But the problem is that OpenCL has much > more ops covered and features(for example support of batch != 1). In your > case OpenCL backend was able to run model and OpenGL can not. > I don't think that in nearest future OpenGL will have similar feature > parities to OpenCL backend. > So I explained why you have this problem, but I don't know how to help you. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Thanks for the support. For anyone looking at this thread in the future,   Added `libOpenCLpixel.so` as suggested above as a temporary fix  Will be retraining the models to use batch size 1 to run within OpenGL constraints.",Are you satisfied with the resolution of your issue? Yes No
703,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(C API Tensor data type doesn't have flat() to convert into a Eigen::Tensor)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Feature Request  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux   Mobile device 07010841551  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,YogaVicky,C API Tensor data type doesn't have flat() to convert into a Eigen::Tensor,Click to expand!    Issue Type Documentation Feature Request  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux   Mobile device 07010841551  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-22T09:02:26Z,stat:awaiting response type:feature stale comp:ops type:docs-feature,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56531, Can you PTAL. Thanks!,"I am not sue I understand the highlight. I haven't opened this bug, and I am not a developer of tensorflow. How is that related to me?"," unfortunately I don't think we currently have a method of converting directly from a c `TF_Tensor` to a c++ `tensorflow::Tensor`, which would get you what you want.  You can use the c api to extract the datatype, element count, and data pointer, then use this to manually construct your own `Eigen::TensorMap`.  That would look something like: ","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(""Cannot convert a symbolic Tensor ({}) to numpy array"")ï¼Œ å†…å®¹æ˜¯ (I tried to use the interface from DeepXDE as mentioned here ""https://github.com/tensorflow/tensorflow/issues/48167issuecomment966596156"" but encountered an error.  While I understand the error, I'm not sure how to resolve it. Any help in this regards is appreciated.  I'm using:                                                       The error: ""Cannot convert a symbolic Tensor ({}) to numpy array"" It seems like the function only accepts numpy arrays but I have a TF symbolic tensor. Any way to resolve this? _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/48167issuecomment1162519845_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rohitvuppala,"""Cannot convert a symbolic Tensor ({}) to numpy array""","I tried to use the interface from DeepXDE as mentioned here ""https://github.com/tensorflow/tensorflow/issues/48167issuecomment966596156"" but encountered an error.  While I understand the error, I'm not sure how to resolve it. Any help in this regards is appreciated.  I'm using:                                                       The error: ""Cannot convert a symbolic Tensor ({}) to numpy array"" It seems like the function only accepts numpy arrays but I have a TF symbolic tensor. Any way to resolve this? _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/48167issuecomment1162519845_",2022-06-22T01:38:46Z,stat:awaiting response type:support stale comp:keras TF 2.4,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56527,"Accidentally opened an issue in the wrong repo where the comment originated, but this repo/package might be unrelated to the issue. ","Hi !  In 2.6 or earlier versions, The error on symbolic tensor was getting resolved when eager execution was disable or numpy version was upgraded to 1.21.5 or downgraded to 1.19.5 . From the 2.8 version onwards, Numpy version has upgraded to 1.21 which solved these issues in most cases. Please create a new issue if you need further assistance.  Thank you!"," Thank you for the detailed comment! My tensorflow version is 2.8.1, numpy version is 1.22.4 and I did disable eager execution. Is the bug still related to tensorflow?", ! Sometimes it can get fixed through Numpy version upgrade/downgrade But Few issues need a code fix on the model side too. Could you update the template  with minimal stand alone code which will help us expedite the issue.  Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
635,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(UNKNOWN: KeyError: 331 in `tf.data.Dataset`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory NVIDIA GTX 2080Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gcuder,UNKNOWN: KeyError: 331 in `tf.data.Dataset`,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory NVIDIA GTX 2080Ti  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-21T07:06:32Z,stat:awaiting response type:support stale comp:dist-strat TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56517,"While reproducing your issue I'm getting` ModuleNotFoundError: No module named 'itranslate.text_models'` even after installing `pip install itranslate`, could you please share the colab gist. Thanks! ","Sorry, my bad. Here's the updated code. I removed the dependencies, as they are not needed for this code example.  ","I'm able to run without any error for the initial epoch, please find the attached gist here. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
371,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add ops_coverage python script)ï¼Œ å†…å®¹æ˜¯ ( Do you want to support me to integrate this script in the CI? Please check https://github.com/tensorflow/tensorflow/issues/14798issuecomment1047796247 / )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,bhack,Add ops_coverage python script, Do you want to support me to integrate this script in the CI? Please check https://github.com/tensorflow/tensorflow/issues/14798issuecomment1047796247 / ,2022-06-20T11:33:46Z,comp:xla size:M,closed,0,20,https://github.com/tensorflow/tensorflow/issues/56510,"Check also my ""extra"" comments: https://github.com/tensorflow/tensorflow/commit/8fccf8a7728315d1cf2c538a3bb55029103addaccommitcomment75913049 https://github.com/tensorflow/tensorflow/commit/a099818c97085a7174da52dc05cf5b49ec7105cfr76511015",!thesong,/,  Can you share a little bit the plan/roadmap after you have removed all the support protos with this commit https://github.com/tensorflow/tensorflow/commit/38582d06e9acd5108ce943af1e793aa493282cad ?,/ , Any news on this?,"Can we use/rely on `XlaOpRegistry` to generate this table?  https://github.com/tensorflow/tensorflow/blob/814e437df95b8b1455452feb47eee5c5e37ce229/tensorflow/compiler/tf2xla/xla_op_registry.ccL57 If this could be ok Is this only valid for the ""standard"" TF brdige?","Yes.  I actually have an internal change for this that I'll send for review that integrates the information with the raw ops table (sorry for the long delay). Current approach *would* use the opset defined by the registry of the non milr bridge.  That is currently the default bridge for GPU/CPU so we'd use that as ground truth in the docs. We'd switch over to the MLIR opset construction when that bridge becomes default. On Mon, Jan 23, 2023 at 12:27 PM bhack ***@***.***> wrote: > Can we use/rely on XlaOpRegistry to generate this table? > > https://github.com/tensorflow/tensorflow/blob/814e437df95b8b1455452feb47eee5c5e37ce229/tensorflow/compiler/tf2xla/xla_op_registry.ccL57 > > If this could be ok Is this only valid for the ""standard"" TF brdige? > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Thanks for the update, as it seems an internal change I suppose that it will be pushed directly on GitHub without a PR. I will close this PR when you are ready as I cannot interact with a CL right now.",Hi  Any update on this PR? Please. Thank you!,I don't know.  What is the status of your mentioned CL?,I think  is looking into it.,"Sorry for the delay.  Draft is written and I've synced with Mark on the documentation updates; I need to get someone to help with an internal issue but can submit after it is resolved. On Tue, Mar 21, 2023 at 11:59â€¯AM Mark Daoust ***@***.***> wrote: > I think   is looking into it. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >",Hi  Any update on this CL? Please. Thank you!," submitted the changes to the doc generator, I'm not 100% sure why this hasn't showed up on the raw_ops page yet: https://www.tensorflow.org/api_docs/python/tf/raw_ops",Hi  Any update on this PR? Please. Thank you!,Fixed: https://www.tensorflow.org/api_docs/python/tf/raw_ops, thanks. It would be really nice to know what is planned to be covered and what it will be never covered by design. I think it will help a lot to choice on what ops to rely if you need to compile your whole graph.  Do you think is it possible to add this info in the page?,Hi  Any update on above comment from ? Please. Thank you!,> Do you think is it possible to add this info in the page? No that information is not available.
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.keras.backend.ctc_decode merging repeated characters by default)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.7.0  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DLumi,tf.keras.backend.ctc_decode merging repeated characters by default,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.7.0  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-20T07:36:04Z,stat:awaiting response type:bug stale comp:keras TF 2.7,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56506,"Doublechecked documentation, and it turns out there's no 'merge_repeated' argument in tf.nn.ctc_beam_search_decoder anymore! I wonder if it was intentional to remove it, as I simply don't see why.","Hi , I tried to execute the code with an alternative approach. Kindly find the gist of it here and let us know if the workaround helped in this case. Thank you!","> Hi , I tried to execute the code with an alternative approach. Kindly find the gist of it here and let us know if the workaround helped in this case. Thank you! As the matter of fact, I did came up with something similar for my test purposes. Speaking of the code you provided  yes, it certainly works as I intend it to. ",", As mentioned above, if the provided workaround helped in your case, could you feel free to close this issue. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1287,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Type INT16 is unsupported by op Rsqrt.Node number 12 (RSQRT) failed to prepare.)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10  TensorFlow installed from (source or binary):                   tensorflow 2.9.1  TensorFlow version (or github SHA if from source): **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  First,I convert the input/output of the model from float32 into int16. Second,I load the new model.Third,I allocate tensors.Then the error reports after run the code ""interpreter_2.allocate_tensors()"" in real_time_processing_tf_lite.py.  The reported error is as following: `RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 12 (RSQRT) failed to prepare.` **Also, please include a link to a GraphDef or the model if possible.** The target model is here https://github.com/breizhn/DTLN The inputs format of the model in the link are float32. **Any other info / logs** The following are my codes for converting.It has no problem: In DTLN_model.py:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FragrantRookie,Type INT16 is unsupported by op Rsqrt.Node number 12 (RSQRT) failed to prepare.,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10  TensorFlow installed from (source or binary):                   tensorflow 2.9.1  TensorFlow version (or github SHA if from source): **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  First,I convert the input/output of the model from float32 into int16. Second,I load the new model.Third,I allocate tensors.Then the error reports after run the code ""interpreter_2.allocate_tensors()"" in real_time_processing_tf_lite.py.  The reported error is as following: `RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 12 (RSQRT) failed to prepare.` **Also, please include a link to a GraphDef or the model if possible.** The target model is here https://github.com/breizhn/DTLN The inputs format of the model in the link are float32. **Any other info / logs** The following are my codes for converting.It has no problem: In DTLN_model.py:  ",2022-06-20T02:27:35Z,stat:awaiting response type:support comp:lite TF 2.9,closed,0,14,https://github.com/tensorflow/tensorflow/issues/56505,"Hi  !  I was able to load both lite files in the above github link  but was facing an select ops error in Colab for 2.8, 2.9 and nightly.  Could you change the conversion code snippet as below and let us know. Thank you! ",">  OK, I'll try it right away",Hi       It works well with the code you gave me.I can get the converted output with float32 intput and output.,"  I don't use ""tf.lite.TFLiteConverter.from_saved_model"". I used this code  `def create_tf_lite_model(self, weights_file, target_name, use_dynamic_range_quant=False):`   define in DTLN_model.py to convert the model into tflite format. The reason that we don't use ""tf.lite.TFLiteConverter.from_saved_model"" has been written in the comments of the function.  `Tf lite does not support complex numbers yet. Some processing must be done outside the model.` Therefore, using this file `convert_weights_to_tf_lite.py` for conversion.", ! Thanks for the update. I was facing issues while converting the loading model (using from_keras_model method) . Attached gist for reference . Could you confirm whether it is working with tf.float32 as inference input and output type with tf.lite.TFLiteConverter.from_keras_model . Thank you!,  I think I gave an unclear reply. Let me reconfirm.,Hi  My computer reported the same error as yours when using `tf.lite.TFLiteConverter.from_keras_model`.No matter using this codes or not: , ! Thanks for the update. Could you add the select ops code snippet and let us know if it works. Thank you! ,"Hi ,  Using your codes,the reported error is as following:  The thing should be noticed is that I modified the function `representative_dataset` as bellow: ", ! Thanks for confirmation on the errors . Passing to  for faster resolution. Thank you!,"As the error message says, for some of the OP INT16 is not supported yet in TFLITE. You can try the solution suggested by  or you can try with different dtype precision like INT8 and see if that works. Thanks!","  Thanks for your repoly! You are the light in the night.  Thanks for your help.Since the model can't convet to the model with int16 input/output, I plan to use the model with float32 intput/output and int8 weights  to deploy to my target device by using `tensorflow for microcontroller`. Based on the fact that fully_connected layer consumes most of the computing power,I want to write c++ codeï¼ˆBecause `tensorflow for microcontroller` is written in c++ï¼‰ to quantize and de quantize the input and output of fully_connected layer. Then I use the int16 version of fully_connected to run the relevant calculations. I am not sure if it is feasible. If you have any suggestions, I would be very happy!",The above specific op for INT16 has been implemented for tflitemicro here in this PR https://github.com/tensorflow/tflitemicro/pull/1072/files. See if this helps you. Thanks!,Are you satisfied with the resolution of your issue? Yes No
1148,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Question on the implementation of BMM (batched matmul))ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Irrelevant  TensorFlow installation (pip package or built from source): Irrelevant  TensorFlow library (version, if pip package or github SHA, if built from source): Irrelevant  2. Code Hi, I am currently working on the transformer models on mobile GPUs. Because transformerbased models have multiheaded attention, batched matrix multiplication is crucial. However, I found out that TOCO simply replaces such BMM operators into a bunch of matmul operators. Is there any rationale behind this? If there is, can you provide some benchmark or evaluation on this? My guess is that the capacity of the mobile GPU cache or register file is not enough to perform BMM efficiently without register spill or something, so performance would be degraded, but I have not tried such experiments so cannot be sure on this. Thanks in advance.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,widiba03304,Question on the implementation of BMM (batched matmul)," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Irrelevant  TensorFlow installation (pip package or built from source): Irrelevant  TensorFlow library (version, if pip package or github SHA, if built from source): Irrelevant  2. Code Hi, I am currently working on the transformer models on mobile GPUs. Because transformerbased models have multiheaded attention, batched matrix multiplication is crucial. However, I found out that TOCO simply replaces such BMM operators into a bunch of matmul operators. Is there any rationale behind this? If there is, can you provide some benchmark or evaluation on this? My guess is that the capacity of the mobile GPU cache or register file is not enough to perform BMM efficiently without register spill or something, so performance would be degraded, but I have not tried such experiments so cannot be sure on this. Thanks in advance.",2022-06-19T07:35:13Z,stat:awaiting response type:support stale comp:lite TFLiteConverter TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56501,"Hi  ! You can refer to this link to benchmark your lite models and see supported ops here.To use custom ops, you have register them in kernel and use custom_ops = true during model to lite file conversion.  Please provide a minimal stand alone code for further assistance. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
617,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( OverflowError: int too large to convert to float)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Windows 1  Mobile device _No response_  Python version 3.9.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,GravermanDev, OverflowError: int too large to convert to float,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution Windows 1  Mobile device _No response_  Python version 3.9.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ,2022-06-18T09:42:48Z,type:bug comp:ops TF 2.9,closed,0,2,https://github.com/tensorflow/tensorflow/issues/56500,"I found the issue, my environment had a bug and the observation was too big of a number",Are you satisfied with the resolution of your issue? Yes No
965,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bincount doesn't check the tensor type)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version master  Custom Code Yes  OS Platform and Distribution linux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? With `axis=None` and `binary_output=False` it doesn't check the input tensor type so it is failing calling directly the `bincount` kernel that it doesn't support sparse and ragged inputs: https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/ops/bincount_ops.pyL137  Standalone code to reproduce the issue  ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,bhack,Bincount doesn't check the tensor type,Click to expand!    Issue Type Bug  Source source  Tensorflow Version master  Custom Code Yes  OS Platform and Distribution linux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? With `axis=None` and `binary_output=False` it doesn't check the input tensor type so it is failing calling directly the `bincount` kernel that it doesn't support sparse and ragged inputs: https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/ops/bincount_ops.pyL137  Standalone code to reproduce the issue  ```  Relevant log output _No response_,2022-06-17T22:21:23Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56499,I don't know why we have the XLA label but it Is a python only bug.,"Hi , Associated PR got closed, could you confirm issue still persists or not. Thank you!", What PR?,"Hi , Is this PR associated with this issue. Thank you! ",No that one is something different. Check my code gist.,"https://github.com/tensorflow/tensorflow/commit/caf5b4d890095df814d8b39d2ac9717aa32ecf1a fixes this, axis=0 and axis=None now consistently use the newer kernel. If there are still any remaining Bitcount issues, please feel free to file a new bug and bring it to my attention.",Are you satisfied with the resolution of your issue? Yes No
399,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix zero-point addition in PortableCwiseMul)ï¼Œ å†…å®¹æ˜¯ (This bug caused errors for the int8 `unidirectional_sequence_lstm` layer.    Python example to reproduce the bug in the LSTM layer, fixed by this PR (click to expand).  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tombana,Fix zero-point addition in PortableCwiseMul,"This bug caused errors for the int8 `unidirectional_sequence_lstm` layer.    Python example to reproduce the bug in the LSTM layer, fixed by this PR (click to expand).  ",2022-06-17T11:07:22Z,comp:lite size:XS,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56493,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",  Can you please review this PR ? Thank you!,Pushed an additional fix based on debugging from  that should hopefully mean all the internal tests pass. Let's go through the PR merging process one more time.,", are you ok with us making the same fixes as an internal commit and then having that be reflected in the GitHub tree? That may be a more efficient path to getting this fix in.","> , are you ok with us making the same fixes as an internal commit and then having that be reflected in the GitHub tree? That may be a more efficient path to getting this fix in. That's totally fine :+1:, thanks for the additional fixes.",The changes from this PR have been merged with https://github.com/tensorflow/tensorflow/commit/59fb183f7e7153a91b5ec0b054d3ecdea432a50a,", please note that the fix had to be reverted with https://github.com/tensorflow/tensorflow/commit/db1dd64675e4cb471390ba2aacdedf27b842399a due to some internal regressions. We're going to see how we can reconcile the different moving pieces but its going to take some time.",", thanks again for finding the issue. Your fix is now merged with https://github.com/tensorflow/tensorflow/commit/9b4997f7b0c13972c03362f94768e8f8e4d046f4 (from ). The corresponding change is in the TFLM repo as well with https://github.com/tensorflow/tflitemicro/pull/1553",Thanks!
1158,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.data.experimental.service does not stop with repeated dataset)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.7.0 and 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.8.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? After consuming a repeated dataset, the `DispatchServer` and `WorkerServer` are shutdown, but some thread keeps running that tries to reconnect to either the dispatcher or worker. This prevents the entire process from terminating (thread seems not to be a daemon thread, likely the gRPC thread: https://github.com/grpc/grpcjava/issues/80). Setting `WorkerConfig(dispatcher_timeout_ms=1000)` (timeout is 1s) makes the thread giving up at some point, but shutdown still takes 20 seconds.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,EnricoMi,tf.data.experimental.service does not stop with repeated dataset,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.7.0 and 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 22.04  Mobile device _No response_  Python version 3.8.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? After consuming a repeated dataset, the `DispatchServer` and `WorkerServer` are shutdown, but some thread keeps running that tries to reconnect to either the dispatcher or worker. This prevents the entire process from terminating (thread seems not to be a daemon thread, likely the gRPC thread: https://github.com/grpc/grpcjava/issues/80). Setting `WorkerConfig(dispatcher_timeout_ms=1000)` (timeout is 1s) makes the thread giving up at some point, but shutdown still takes 20 seconds.  Standalone code to reproduce the issue   Relevant log output  ",2022-06-17T09:55:49Z,stat:awaiting response type:bug comp:data TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56490,", I was facing a different issue while executing the given code. Kindly find the gist of it here and share the complete code and dependencies. Thank you!",I have updated the gist and code above.,", I have tried in colab with the v2.9 version and noticed that the session is being crashed. While I was trying on tf nightly, the code executed without any issues/warnings. Kindly find the gist of it here. Thank you!",I can confirm that I can no longer reproduce this with latest `tfnightlycpu`. Thanks for looking into this.,Are you satisfied with the resolution of your issue? Yes No
540,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TF-TRT] Disable TF32 for TF-TRT unittests that needs it)ï¼Œ å†…å®¹æ˜¯ (TF32 computation may require in some cases a higher threshold to pass some unittests. Instead of implementing some complex logic, the standard procedure for these few cases is to test in FP32 instead of TF32 and not modifying the threshold. This PR corrects TFTRT python unittests with this logic.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DEKHTIARJonathan,[TF-TRT] Disable TF32 for TF-TRT unittests that needs it,"TF32 computation may require in some cases a higher threshold to pass some unittests. Instead of implementing some complex logic, the standard procedure for these few cases is to test in FP32 instead of TF32 and not modifying the threshold. This PR corrects TFTRT python unittests with this logic.",2022-06-16T18:53:58Z,ready to pull size:M comp:gpu:tensorrt,closed,0,12,https://github.com/tensorflow/tensorflow/issues/56485, for review," I pushed the changes requested, can you reapprove ? Thanks", Can you please check 's comments and keep us posted ? Thank you!, Any update on this PR? Please. Thank you!, Any update on this PR? Please. Thank you!, Any update on this PR? Please. Thank you!," comments fixed, good for review",I am waiting for the author to address my comments., Any update on this PR? Please. Thank you!, Any update on this PR? Please. Thank you!, Any update on this PR? Please. Thank you!, could you review again ? All conflicts have been solved
1855,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RNN] LSTM after conversion to TFLite not run on GPU. )ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow 2.5, tensorflow 2.6, tensorflow 2.9, tfnightly  Mobile device: Huawei Mate 10 Pro with Android 10.  2. Code Provide code to help us reproduce your issues using one of the following options: You can see the attched files.   3. Failure after conversion In most cases I can complete the conversion to TFLite, with and without Quantization.  4. Any other info / logs The problem exists after the conversion; I am currently unable to understand if LSTM or GRU layers are actually supported on GPU and NNAPI_GPU Delegates. I test my models using the BenchmarkTools through the Andoid apk, setting the use_gpu or use_nnapi flags to True.  From my tests, I was able to find two different cases: 1) When I set in conversion as supported_ops  tf.lite.OpsSet.TFLITE_BUILTINS e tf.lite.OpsSet.SELECT_TF_OPS which leads to  UNPACK: Operation is not supported. 0616 17:31:52.176 27077 27077 E tflite  : 294 operations will run on the GPU, and the remaining 296 operations will run on the CPU. 0616 17:31:52.177 27077 27077 I tflite  : Replacing 294 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions. 0616 17:31:53.098 27077 27077 I tflite  : Initialized OpenCLbased API. 0616 17:31:53.242 27077 27077 I tflite  : Created 1 GPU delegate kernels. 0616 17:31:53.245 27077 27077 I tflite  : Explicitly applied GPU delegate, and the model graph will be)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,francescoyou97,[RNN] LSTM after conversion to TFLite not run on GPU. ," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow 2.5, tensorflow 2.6, tensorflow 2.9, tfnightly  Mobile device: Huawei Mate 10 Pro with Android 10.  2. Code Provide code to help us reproduce your issues using one of the following options: You can see the attched files.   3. Failure after conversion In most cases I can complete the conversion to TFLite, with and without Quantization.  4. Any other info / logs The problem exists after the conversion; I am currently unable to understand if LSTM or GRU layers are actually supported on GPU and NNAPI_GPU Delegates. I test my models using the BenchmarkTools through the Andoid apk, setting the use_gpu or use_nnapi flags to True.  From my tests, I was able to find two different cases: 1) When I set in conversion as supported_ops  tf.lite.OpsSet.TFLITE_BUILTINS e tf.lite.OpsSet.SELECT_TF_OPS which leads to  UNPACK: Operation is not supported. 0616 17:31:52.176 27077 27077 E tflite  : 294 operations will run on the GPU, and the remaining 296 operations will run on the CPU. 0616 17:31:52.177 27077 27077 I tflite  : Replacing 294 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions. 0616 17:31:53.098 27077 27077 I tflite  : Initialized OpenCLbased API. 0616 17:31:53.242 27077 27077 I tflite  : Created 1 GPU delegate kernels. 0616 17:31:53.245 27077 27077 I tflite  : Explicitly applied GPU delegate, and the model graph will be",2022-06-16T16:03:57Z,stat:awaiting response stale comp:lite type:performance TFLiteGpuDelegate TFLiteNNAPIDelegate TF 2.9,closed,0,21,https://github.com/tensorflow/tensorflow/issues/56482,example.zip train.zip,"Hi  ! Sorry for the late response. It is suggested to use both built_ins ops and select_ops in supported_ops when you are using any select ops operator. Could you confirm the below points. 1.  Are you using use_gpu = true flag in benchmarking for gpu . 2. Are you using  use_nnapi = true flag in benchmarking for nnapi Attached relevant threads for reference . 1, 2 Please  share your benchmarking commands and output logs for CPU, GPU and NNAPI for further assistance. Thank you!","0618 10:11:07.371  4320  4320 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: graph=/data/local/tmp/model.tflite warmup_runs=1 num_runs=50 enable_op_profiling=true use_xnnpack=false use_gpu=true 0618 10:11:07.683  4320  4320 I tflite  : Log parameter values verbosely: [0] 0618 10:11:07.683  4320  4320 I tflite  : Min num runs: [50] 0618 10:11:07.683  4320  4320 I tflite  : Min warmup runs: [1] 0618 10:11:07.683  4320  4320 I tflite  : Graph: [/data/local/tmp/model.tflite] 0618 10:11:07.683  4320  4320 I tflite  : Enable op profiling: [1] 0618 10:11:07.684  4320  4320 I tflite  : Use gpu: [1] 0618 10:11:07.684  4320  4320 I tflite  : Use xnnpack: [0] 0618 10:11:07.684  4320  4320 I tflite  : Loaded model /data/local/tmp/model.tflite 0618 10:11:07.685  4320  4320 I tflite  : Initialized TensorFlow Lite runtime. 0618 10:11:07.692  4320  4320 I tflite  : Created TensorFlow Lite delegate for GPU. 0618 10:11:07.693  4320  4320 I tflite  : GPU delegate created. 0618 10:11:07.713  4320  4320 E tflite  : Following operations are not supported by GPU delegate: 0618 10:11:07.713  4320  4320 E tflite  : UNPACK: Operation is not supported. 0618 10:11:07.713  4320  4320 E tflite  : 316 operations will run on the GPU, and the remaining 318 operations will run on the CPU. 0618 10:11:07.713  4320  4320 I tflite  : Replacing 316 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions. 0618 10:11:09.161  4320  4320 I tflite  : Initialized OpenCLbased API. 0618 10:11:09.310  4320  4320 I tflite  : Created 1 GPU delegate kernels. 0618 10:11:09.317  4320  4320 I tflite  : Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels. 0618 10:11:09.317  4320  4320 I tflite  : The input model file size (MB): 0.203088 0618 10:11:09.317  4320  4320 I tflite  : Initialized session in 1633.14ms. 0618 10:11:09.318  4320  4320 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds. 0618 10:11:09.835  4320  4320 I tflite  : count=22 first=33813 curr=23757 min=20690 max=33813 avg=23480.3 std=2652 0618 10:11:09.836  4320  4320 I tflite  : Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds. 0618 10:11:11.248  4320  4320 I tflite  : count=50 first=27669 curr=20747 min=20747 max=31841 avg=22956.2 std=2400 0618 10:11:11.248  4320  4320 I tflite  : Inference timings in us: Init: 1633136, First inference: 33813, Warmup (avg): 23480.3, Inference (avg): 22956.2 0618 10:11:11.248  4320  4320 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion. 0618 10:11:11.248  4320  4320 I tflite  : Memory footprint delta from the start of the tool (MB): init=31.75 overall=33.832 0618 10:11:11.248  4320  4320 I tflite  : Profiling Info for Benchmark Initialization: 0618 10:11:11.248  4320  4320 I tflite  : ============================== Run Order ============================== 0618 10:11:11.248  4320  4320 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0618 10:11:11.248  4320  4320 I tflite  :       ModifyGraphWithDelegate         1624.189        1624.189        99.600%         99.600%         31572.000              1       ModifyGraphWithDelegate/0 0618 10:11:11.248  4320  4320 I tflite  :               AllocateTensors            6.510           3.265         0.400%        100.000%             0.000              2       AllocateTensors/0 0618 10:11:11.248  4320  4320 I tflite  : 0618 10:11:11.248  4320  4320 I tflite  : ============================== Top by Computation Time ============================== 0618 10:11:11.248  4320  4320 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0618 10:11:11.248  4320  4320 I tflite  :       ModifyGraphWithDelegate         1624.189        1624.189        99.600%         99.600%         31572.000              1       ModifyGraphWithDelegate/0 0618 10:11:11.248  4320  4320 I tflite  :               AllocateTensors            6.510           3.265         0.400%        100.000%             0.000              2       AllocateTensors/0 0618 10:11:11.248  4320  4320 I tflite  : 0618 10:11:11.248  4320  4320 I tflite  : Number of nodes executed: 2 0618 10:11:11.248  4320  4320 I tflite  : ============================== Summary by node type ============================== 0618 10:11:11.248  4320  4320 I tflite  :                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [ti 0618 10:11:11.263  4320  4320 I tflite  : Operatorwise Profiling Info for Regular Benchmark Runs: 0618 10:11:11.263  4320  4320 I tflite  : ============================== Run Order ============================== 0618 10:11:11.263  4320  4320 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0618 10:11:11.263  4320  4320 I tflite  :                      QUANTIZE            0.050           0.047         0.213%          0.213%             0.000              1       [tfl.quantize]:0 0618 10:11:11.263  4320  4320 I tflite  :                       RESHAPE            0.007           0.007         0.031%          0.244%             0.000              1       [sequential/gru/transpose1]:1 0618 10:11:11.263  4320  4320 I tflite  :                        UNPACK            0.028           0.021         0.095%          0.339%             0.000              1       [sequential/gru/unstack, sequential/gru/unstack1, sequential/gru/unstack2, sequential/gru/unstack3, sequential/gru/unstack4, sequential/gru/unstack5, sequential/gru/unstack6, sequential/gru/unstack7, sequential/gru/unstack8, sequential/gru/unstack9, sequential/gru/unstack10, sequential/gru/unstack11, sequential/gru/unstack12, sequential/gru/unstack13, sequential/gru/unstack14, sequential/gru/unstack15, sequential/gru/unstack16, sequential/gru/unstack17, sequential/gru/unstack18, sequential/","This is the output from Benchmark after a conversion using:      run_model = tf.function(lambda x: model(x))     concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, model.inputs[0].shape[1]], model.inputs[0].dtype))     converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])       converter.optimizations = [tf.lite.Optimize.DEFAULT]     converter.experimental_new_converter = True     converter.experimental_new_quantizer = True     converter.allow_custom_ops = True     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]     tflite_quantized_model = converter.convert() And using the ""unroll=True"" for the LSTM layers.",There is another key point. I download the prebuilded apk from this page. https://www.tensorflow.org/lite/performance/measurement You passed me the github link. There is some difference. Maybe the one on the page is out of date? Or it can be used. Perhaps the nonuse could be due to an outdated Benchmark apk.,"When I use the NNAPI delegate.  0618 10:20:41.572  4669  4669 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: graph=/data/local/tmp/model.tflite warmup_runs=1 num_runs=50 enable_op_profiling=true use_xnnpack=false use_nnapi=true 0618 10:20:41.892  4669  4669 I tflite  : Log parameter values verbosely: [0] 0618 10:20:41.892  4669  4669 I tflite  : Min num runs: [50] 0618 10:20:41.893  4669  4669 I tflite  : Min warmup runs: [1] 0618 10:20:41.893  4669  4669 I tflite  : Graph: [/data/local/tmp/model.tflite] 0618 10:20:41.893  4669  4669 I tflite  : Enable op profiling: [1] 0618 10:20:41.893  4669  4669 I tflite  : Use NNAPI: [1] 0618 10:20:41.915  4669  4669 I tflite  : NNAPI accelerators available: [ipuadaptor,nnapireference] 0618 10:20:41.915  4669  4669 I tflite  : Use xnnpack: [0] 0618 10:20:41.916  4669  4669 I tflite  : Loaded model /data/local/tmp/model.tflite 0618 10:20:41.917  4669  4669 I tflite  : Initialized TensorFlow Lite runtime. 0618 10:20:41.926  4669  4669 I tflite  : Created TensorFlow Lite delegate for NNAPI. 0618 10:20:41.926  4669  4669 I tflite  : NNAPI delegate created. 0618 10:20:42.206  4669  4669 I tflite  : Though NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate. 0618 10:20:42.206  4669  4669 I tflite  : The input model file size (MB): 0.203088 0618 10:20:42.206  4669  4669 I tflite  : Initialized session in 290.328ms. 0618 10:20:42.207  4669  4669 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds. 0618 10:20:42.708  4669  4669 I tflite  : count=249 first=4486 curr=1946 min=1943 max=4486 avg=2006.8 std=236 0618 10:20:42.708  4669  4669 I tflite  : Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds. 0618 10:20:43.719  4669  4669 I tflite  : count=82 first=5339 curr=4325 min=4118 max=6021 avg=4451.4 std=364 0618 10:20:43.720  4669  4669 I tflite  : Inference timings in us: Init: 290328, First inference: 4486, Warmup (avg): 2006.8, Inference (avg): 4451.4 0618 10:20:43.720  4669  4669 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion. 0618 10:20:43.720  4669  4669 I tflite  : Memory footprint delta from the start of the tool (MB): init=1.90625 overall=2.84766 0618 10:20:43.720  4669  4669 I tflite  : Profiling Info for Benchmark Initialization: 0618 10:20:43.720  4669  4669 I tflite  : ============================== Run Order ============================== 0618 10:20:43.720  4669  4669 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0618 10:20:43.720  4669  4669 I tflite  :       ModifyGraphWithDelegate          279.218         279.218        94.616%         94.616%          1024.000              1       ModifyGraphWithDelegate/0 0618 10:20:43.720  4669  4669 I tflite  :               AllocateTensors           15.862           7.944         5.384%        100.000%             0.000              2       AllocateTensors/0 0618 10:20:43.720  4669  4669 I tflite  : 0618 10:20:43.720  4669  4669 I tflite  : ============================== Top by Computation Time ============================== 0618 10:20:43.720  4669  4669 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0618 10:20:43.720  4669  4669 I tflite  :       ModifyGraphWithDelegate          279.218         279.218        94.616%         94.616%          1024.000              1       ModifyGraphWithDelegate/0 0618 10:20:43.720  4669  4669 I tflite  :               AllocateTensors           15.862           7.944         5.384%        100.000%             0.000              2       AllocateTensors/0 0618 10:20:43.720  4669  4669 I tflite  : 0618 10:20:43.720  4669  4669 I tflite  : Number of nodes executed: 2 0618 10:20:43.720  4669  4669 I tflite  : ============================== Summary by node type ============================== 0618 10:20:43.720  4669  4669 I tflite  :                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [ti 0618 10:20:43.750  4669  4669 I tflite  : Operatorwise Profiling Info for Regular Benchmark Runs: 0618 10:20:43.750  4669  4669 I tflite  : ============================== Run Order ============================== 0618 10:20:43.750  4669  4669 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0618 10:20:43.750  4669  4669 I tflite  :                      QUANTIZE            0.024           0.032         1.062%          1.062%             0.000              1       [tfl.quantize]:0 0618 10:20:43.750  4669  4669 I tflite  :                       RESHAPE            0.004           0.004         0.146%          1.208%             0.000              1       [sequential/gru/transpose1]:1 0618 10:20:43.750  4669  4669 I tflite  :                        UNPACK            0.010           0.013         0.448%          1.656%             0.000              1       [sequential/gru/unstack, sequential/gru/unstack1, sequential/gru/unstack2, sequential/gru/unstack3, sequential/gru/unstack4, sequential/gru/unstack5, sequential/gru/unstack6, sequential/gru/unstack7, sequential/gru/unstack8, sequential/gru/unstack9, sequential/gru/unstack10, sequential/gru/unstack11, sequential/gru/unstack12, sequential/gru/unstack13, sequential/gru/unstack14, sequential/gru/unstack15, sequential/gru/unstack16, sequential/gru/unstack17, sequential/gru/unstack18, sequential/","Sorry but in both the case i use tge GRU layers, but the results are almost the same. The RESHAPE  and UNPACK ops have some problems. ", ! Sorry for the inconvenience .  ! Could you please look at this issue. Thank you!,You can help me?     Can you at least tell me if LSTM can run on GPU delegates or NNAPI with GPU. Or is it a useless claim?,"Hi, can you share .tflite model? Empty/random weights are fine. If not, at least part of the tflite model with unpack reshape that do not supported on gpu. Not all models faster on GPU delegate, especially when used cpu>gpu>cpu pipeline.","I thank you infinitely for an answer. Here is the full model. You can do your own checks. model.zip   I was already aware that the mixed gpu / cpu pipeline on mobile devices could be slower than using the gpu alone, especially with the use of NNAPI delegates. But then you are saying that, what is indicated here https://www.tensorflow.org/lite/performance/gpu_advanced is not correct? Here it is indicated that ""LSTM v2 (Basic LSTM only)"" and ""RESHAPE"" layers are supported on GPU. In my case not all operations (Unpack, Reshape, ...) are performed on GPU. Am I wrong or am I missing something? Now I would not like the device (Huawei Mate 10 Pro) to be too oldfashioned to perform such operations on GPU or NNAPI Delegate. I hope for your comprehensive answer . Thanks again.","""Here is the full model"" In this model I can see while operation and operations with int tensors. This is not supported on GPU. Is it unrolled version of the model?",Sorry. I make a mistake. This is an old version of the model. Here you can find a model with unroll.  model.zip,. This is an unrolled model. ,"Your unrolled model should be executed fully on gpu after this commit: https://github.com/tensorflow/tensorflow/commit/6bca0adc394a29eed9cd6e0a7d2143c46ba8e5b4 But this model still much slower on gpu, comparing to cpu Not all models faster on gpu, gpu needs very big models to be efficient and faster than cpu","Ok , from what I understand, after the changes with the given commit, the problem should be fixed, but this shouldn't improve performance. Ok. I am interested in experiencing it myself on my device. Could you please tell me if the aftta change affects the conversion process to tflite or in the next use step. That is what I need to update to get it working on the GPU. Thanks again.","No need to regenerate the model. It will work with current unrolled model. But you need to compile latest(after commit) version of code. Or wait until it will be compiled and included in the library, but I don't know when it happen.","Hi   Have you tried with commit https://github.com/tensorflow/tensorflow/commit/6bca0adc394a29eed9cd6e0a7d2143c46ba8e5b4 ? Using tflite model analyzer api, I could see that all ops in your model are now supported on GPU except UNPACK.  Please find the gist here. Thanks.",This issue has been marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No
508,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow precompiled AOT XLA compiler.)ï¼Œ å†…å®¹æ˜¯ (I am trying to compile a Saved Model to a linkable library. But compiling the compiler takes forever on my computer. Is it possible to provide the compiler binaries and possibly have it in the releases. Perhaps also make it freeze the model automatically for simplicity. Thank you!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,weisrc,Tensorflow precompiled AOT XLA compiler.,I am trying to compile a Saved Model to a linkable library. But compiling the compiler takes forever on my computer. Is it possible to provide the compiler binaries and possibly have it in the releases. Perhaps also make it freeze the model automatically for simplicity. Thank you!,2022-06-15T17:57:35Z,stat:awaiting response type:feature stale comp:runtime,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56474,Hi  ! Could you fill the template and share the steps along a minimal stand alone code to replicate the issue. Thank you!,"Thank you for the swift response! I am trying to compile this model https://tfhub.dev/vasudevgupta7/wav2vec2/1 into binary using `tf_library()`, but doing so requires compiling most of TensorFlow. I am following this tutorial online: https://gist.github.com/carlthome/6ae8a570e21069c60708017e3f96c9fd, but for a different saved model. Are there precompiled https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile_main.? If not, having one will be a great addition to make XLA AOT more accessible! The reason I am falling back to this is that I am experiencing issues using the regular TF. There is a feature which is not implemented for CPU (Group Conv2D if I recall correctly). EDIT: When running on CPU: `Fused conv implementation does not support grouped convolutions for now.` Running on GPU: everything is fine https://colab.research.google.com/drive/1NCOS_HGGoTal2DLBpF6u20fFBAnXCjUV?usp=sharing","Furthermore, I am trying to use it with Rust, this explains why I prefer having it as a linkable library.","Sorry, I will try to get the template filled when I have access to my computer.",Hi  ! Could you please look at this feature request. Thank you!,Any updates on this issue please? We need working recent examples of AOT compilation from a saved model. Having `tfcompile` available with tensorflow distribution will be a great help. ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
637,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow-Macos 2.9.2 Missing OP files)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution MacOS 13  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,abhimanyuchadha96,Tensorflow-Macos 2.9.2 Missing OP files,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.2  Custom Code No  OS Platform and Distribution MacOS 13  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-15T16:03:27Z,type:bug type:build/install subtype:macOS TF 2.9,closed,0,19,https://github.com/tensorflow/tensorflow/issues/56473,CC  toplay,"Hi  , is this on M1 machine ?. Can you please provide steps you used to reproduce this issue. Also is this on a M1 machine ?", Yes it is the M1 machine. I just did `import horovod.tensorflow` and the error appeared. Are you referring to installation steps? To setup Tensorflow and Horovod on M1?,"Hi , Did you follow the steps mentioned here. Thank you!","Yes, I used the `pip` installation method. On Fri, Jun 17, 2022 at 6:10 AM gadagashwini ***@***.***> wrote: > Hi  , Did you > follow the steps mentioned here > . Thank you! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >   Regards, Abhimanyu Chadha",These are the steps I used to setup tensorflow and horovod ` conda install c apple tensorflowdeps y python m pip install tensorflowmacos python m pip install tensorflowmetal HOROVOD_WITH_TENSORFLOW=1 pip install horovod `,  Had a similar conversation with the creators of Horovod.  They pointed out a few libraries missing when building TF.  Check it out: https://github.com/horovod/horovod/issues/3579,Thanks  . We will build the tf with missing libs and update the wheel ,"Thank you for the quick response. Is there an estimated timeline so that I can plan accordingly? On Tue, Jun 21, 2022 at 8:08 AM Kulin Seth ***@***.***> wrote: > Thanks   . We > will build the tf with missing libs and update the wheel > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >   Regards, Abhimanyu Chadha"," Hey Kulin, just wanted to follow up on the timeline, so that I can communicate that to my PM. "," Pinging again, in case the previous message got lost. Could really use the new build.","Hi  , unfortunately I can't discuss the timeline, but we are working on this with highpriority. I will update here, as soon as ready. One minor issue, is that we will have to bump the TF base version to release the fixes. But we will take a look.", Thank you for getting back to me. Looking forward to the new build., Any luck with this?,">  Any luck with this? Hi  , we are working on the 2.10 release. I will update here.", Thank you. I understand a detailed timeline can't be revealed but do we have an estimated date? I can update my project managers accordingly.," , we have updated the Tensorflow base and Tensorflow metal binaries. I tested with following setup and was able to use Horovod: Step 1: Environment Setup Install Horovod dependencies:  Setup Conda env and Tensorflow Metal:  https://developer.apple.com/metal/tensorflowplugin/ Step 2:  Install Horovod  Then works fine. ",Thank you.,Are you satisfied with the resolution of your issue? Yes No
675,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Gfile raises wrong error when trying to delete non existing file on HDFS)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.5.3  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jccarles,Tensorflow Gfile raises wrong error when trying to delete non existing file on HDFS,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.5.3  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-15T15:05:39Z,stat:awaiting response type:bug stale comp:ops TF 2.5,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56469,"Not directly relevant to this project, but this bug cascades into the Transform component of TFX failing because it tries to delete non existing files and does not catch ""UnknownError"".", Could you please find the reproducible gist here and let me know if I am missing something to replicate the issue? Thank you!,"Hello  ! I think to replicate in tensorflow >= 2.6 it is needed to install and import tensorflowio (https://stackoverflow.com/questions/68876693/howcaniaccesstohdfsfilesysteminthelatesttensorflow260). But in both cases (tf >= 2.6 and < 2.6) you need to have hadoop installed and to export its CLASSPATH in the container which tries to access hdfs (https://stackoverflow.com/questions/42307908/runningtensorflowwithfileonhdfscannotfindlibhdfsso), you also need to start an hdfs and try to delete a nonexisting file inside it... It requires quite a bit of setup... ","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
955,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(InactiveRpcError of RPC that terminated with: status = StatusCode.FAILED_PRECONDITION, details = ""Attempting to use uninitialized value dense_2/bias)ï¼Œ å†…å®¹æ˜¯ (  [InactiveRpcError of RPC that terminated with: status = StatusCode.FAILED_PRECONDITION, details = ""Attempting to use uninitialized value dense_2/bias] Getting following error from my python grpc client when trying predicting wuth the tfserving server:  I found description of problem but it is not informative for me Operation was rejected because the system is not in a state required for the operationâ€™s execution. (https://grpc.github.io/grpc/python/grpc.htmlgrpcstatuscode)  System information python 3.9  tensorflowservingapi==2.8.0 tensorflow==2.8.0 What can be the reason for this error and how it could be fix?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ViktorKhLoopMe,"InactiveRpcError of RPC that terminated with: status = StatusCode.FAILED_PRECONDITION, details = ""Attempting to use uninitialized value dense_2/bias","  [InactiveRpcError of RPC that terminated with: status = StatusCode.FAILED_PRECONDITION, details = ""Attempting to use uninitialized value dense_2/bias] Getting following error from my python grpc client when trying predicting wuth the tfserving server:  I found description of problem but it is not informative for me Operation was rejected because the system is not in a state required for the operationâ€™s execution. (https://grpc.github.io/grpc/python/grpc.htmlgrpcstatuscode)  System information python 3.9  tensorflowservingapi==2.8.0 tensorflow==2.8.0 What can be the reason for this error and how it could be fix?",2022-06-15T12:24:41Z,stat:awaiting response type:bug stale comp:keras TF 2.8,closed,0,18,https://github.com/tensorflow/tensorflow/issues/56698, what means this stat? 'awaiting tensorflower',", could you redirect this question to TF repo? It looks more like a TF issue. ",Hi  ! Sorry for the late response. Could you share the link to gist to your model file.   Did you try setting this code snippet in your code.  `tf.global_variables_initializer().run() `.  Attached relevant thread for reference. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Hi  Where should exactly I need use `tf.global_variables_initializer().run() `? running this line I got error : `AttributeError: module 'tensorflow' has no attribute 'global_variables_initializer'` Analysing relevant thread I decided update versions to 2.9.0 And After updating I got the same error,Hi  ! Could you please provide a Colab gist or reproduceable code again. Thank you!,  Here is code how I set up serving and how I predict https://gist.github.com/ViktorKhLoopMe/bd4a7ef6f605f96253cb7537ccd15ae5," This is a user error. In order to debug, Can you please attach the code used build the model and  save it and also the plat form details. Also can you please attach the full stack trace. Thanks!!"," Hello! Ok, I'll prepare code But can you tell me where exactly should I use: `tf.global_variables_initializer().run() `?", Since this looks like a user error I need the entire code in colab to see where the issue is. I can't help with just the stack trace. Thanks!, Hello! Here is code with model training and save https://gist.github.com/ViktorKhLoopMe/54f5584f77c5c030c835f9678c02da8d, The issue here is that you are using `saved_model.Builder` which is from tensorflow 1.x.  Please try using `tf.saved_model.save()` or `tf.keras.saved_model.save()` for keras which is preferred way of saving model in tensorflow 2.x. For more details you can look at the following migration guide. Thanks!,"  Can you produce how I suppose save model with signatures in V2? like I used to saved in V1 `builder = tf.saved_model.builder.SavedModelBuilder(export_path)     model_input = tf.saved_model.utils.build_tensor_info(model.input)     model_output = tf.saved_model.utils.build_tensor_info(model.output)     prediction_signature = (         tf.saved_model.signature_def_utils.build_signature_def(             inputs={'inputs': model_input},             outputs={'output': model_output},             method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))     with K.get_session() as sess:         builder.add_meta_graph_and_variables(             sess=sess, tags=[tf.saved_model.tag_constants.SERVING],             signature_def_map={                 'predict':                     prediction_signature,             })         builder.save() ` ", any comments?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1847,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Keras model Can't pickle _thread.RLock objects)ï¼Œ å†…å®¹æ˜¯ (`from __future__ import absolute_import from __future__ import print_function import numpy as np from keras.applications.vgg16 import VGG16 from keras.layers import * from keras.models import Model, load_model from keras.optimizers import SGD from keras.preprocessing.image import load_img, img_to_array import tensorflow as tf from keras import backend as K config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) K.set_session(sess) def convnet_model_():     vgg_model = VGG16(weights=None, include_top=False)     x = vgg_model.output     x = GlobalAveragePooling2D()(x)     x = Dense(4096, activation='relu')(x)     x = Dropout(0.6)(x)     x = Dense(4096, activation='relu')(x)     x = Dropout(0.6)(x)     x = Lambda(lambda x_: K.l2_normalize(x, axis=1))(x)     convnet_model = Model(inputs=vgg_model.input, outputs=x)     return convnet_model def deep_rank_model():     convnet_model = convnet_model_()     first_input = Input(shape=(224, 224, 3))     first_conv = Conv2D(96, kernel_size=(8, 8), strides=(16, 16), padding='same')(first_input)     first_max = MaxPool2D(pool_size=(3, 3), strides=(4, 4), padding='same')(first_conv)     first_max = Flatten()(first_max)     first_max = Lambda(lambda x: K.l2_normalize(x, axis=1))(first_max)     second_input = Input(shape=(224, 224, 3))     second_conv = Conv2D(96, kernel_size=(8, 8), strides=(32, 32), padding='same')(second_input)     second_max = MaxPool2D(pool_size=(7, 7), strides=(2, 2), padding='same')(second_conv)     second_max = Flatten()(second_max)     second_max = Lambda(lambda x: K.l2_normalize(x, axis=)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,gootacatchitall,Keras model Can't pickle _thread.RLock objects,"`from __future__ import absolute_import from __future__ import print_function import numpy as np from keras.applications.vgg16 import VGG16 from keras.layers import * from keras.models import Model, load_model from keras.optimizers import SGD from keras.preprocessing.image import load_img, img_to_array import tensorflow as tf from keras import backend as K config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) K.set_session(sess) def convnet_model_():     vgg_model = VGG16(weights=None, include_top=False)     x = vgg_model.output     x = GlobalAveragePooling2D()(x)     x = Dense(4096, activation='relu')(x)     x = Dropout(0.6)(x)     x = Dense(4096, activation='relu')(x)     x = Dropout(0.6)(x)     x = Lambda(lambda x_: K.l2_normalize(x, axis=1))(x)     convnet_model = Model(inputs=vgg_model.input, outputs=x)     return convnet_model def deep_rank_model():     convnet_model = convnet_model_()     first_input = Input(shape=(224, 224, 3))     first_conv = Conv2D(96, kernel_size=(8, 8), strides=(16, 16), padding='same')(first_input)     first_max = MaxPool2D(pool_size=(3, 3), strides=(4, 4), padding='same')(first_conv)     first_max = Flatten()(first_max)     first_max = Lambda(lambda x: K.l2_normalize(x, axis=1))(first_max)     second_input = Input(shape=(224, 224, 3))     second_conv = Conv2D(96, kernel_size=(8, 8), strides=(32, 32), padding='same')(second_input)     second_max = MaxPool2D(pool_size=(7, 7), strides=(2, 2), padding='same')(second_conv)     second_max = Flatten()(second_max)     second_max = Lambda(lambda x: K.l2_normalize(x, axis=",2022-06-15T04:41:29Z,stat:awaiting response type:support stale comp:keras TF 1.15,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56466,", From the provided code, we can assume that you are using tensorflow 1.x which is not actively supported. I tried to convert the code for the 2.x version and I was able to execute without any issues.  Kindly find the gist here and please update to the 2.x version and let us know if you are facing the same issue. Thank you!"," I tried ur method but come out with new bug... !image I am using tf 1.x version because this bug  CC(Graph ""Training/Test"" flag & global_flags) Btw,I just put my gist here so u can have full view on the code. https://colab.research.google.com/drive/1CEogpAk3MAbF3uDXVKiUd2h9rQXkLWU?usp=sharing","> , From the provided code, we can assume that you are using tensorflow 1.x which is not actively supported. I tried to convert the code for the 2.x version and I was able to execute without any issues. Kindly find the gist here and please update to the 2.x version and let us know if you are facing the same issue. Thank you!   I can't use tensor 2.x version as my code cant support it",", Unfortunately TensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.9.0 and check if you are facing the same issue.  Also it would be difficult for us to debug the issue with 1.x code. Kindly provide the version 2.x compatible code. Thank you!","> , Unfortunately TensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.9.0 and check if you are facing the same issue. Also it would be difficult for us to debug the issue with 1.x code. Kindly provide the version 2.x compatible code. Thank you! !image i got a new error when trying to use tensorflow 2.x...it seems like I can't put a layer with keras.k.l2.normalize Kindly find the gist as below https://colab.research.google.com/github/gootacatchitall/WID3003NeuralComputing/blob/main/56466.ipynb?authuser=2",", Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"The code posted in the colab by  does not work, there are only definition there.. as soon as you call them you have the same error encountered by . Look here for the solution.",Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
699,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reduction in tflite performance between v2.5 and v2.9)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version tflite 2.5.0rc3 to 2.9.1  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version n/a  Bazel version 4.2.1 for earlier tflite versions, 5.1.1 for v2.9.1  GCC/Compiler version 9.4.01ubuntu1~20.04.1  CUDA/cuDNN version n/a  GPU model and memory NVIDIA RTX 2070  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,parnham,Reduction in tflite performance between v2.5 and v2.9,"Click to expand!    Issue Type Performance  Source source  Tensorflow Version tflite 2.5.0rc3 to 2.9.1  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version n/a  Bazel version 4.2.1 for earlier tflite versions, 5.1.1 for v2.9.1  GCC/Compiler version 9.4.01ubuntu1~20.04.1  CUDA/cuDNN version n/a  GPU model and memory NVIDIA RTX 2070  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-06-14T22:29:03Z,stat:awaiting tensorflower comp:lite type:performance TF 2.9,closed,0,12,https://github.com/tensorflow/tensorflow/issues/56463,Hi  !  Thanks for reporting this issue.  ! Could you look at this . Thank you!,"I also have a question related to the GPU performance.  If I run two instances of our benchmark simultaneously each runs at the same speed as a single one (utilising ~66% of the GPU instead of ~33%). Therefore it stands to reason that running two threads within a process, each with their own model, interpreter and delegate, should provide an inferencing speed increase by classifying two samples at a time. However this is not the case  is there something in the GPU delegate which is preventing concurrent OpenCL pipelines within the same process?",Can you please elaborate on the building process with opencl?,"Hi, If you want better performance statistics, you can try to use this script: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/cl/testing/run_performance_profiling.sh ./run_performance_profiling.sh m path_to_the_model d HOST",> Can you please elaborate on the building process with opencl? We have an earthly script for building and packaging the dependencies that our system uses. These are the commands that are used to build the tensorflowlite and delegate for release:  You can see from the logs in the first post that OpenCL is being initialised. If I check the /proc maps for the benchmark_model tool while it's running it shows that the nvidia provided opencl implementation is being loaded:  and the graphics card is showing activity. Additionally this is the command used to build the benchmark_model tool ,"> Hi, If you want better performance statistics, you can try to use this script: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/cl/testing/run_performance_profiling.sh ./run_performance_profiling.sh m path_to_the_model d HOST Have attempted to run this script for both the v2.5.0rc3 and v2.9.1 tags and it fails as follows  What am I doing wrong?",What is the result of this command for you: bazel build c opt third_party/tensorflow/lite/delegates/gpu/cl/testing:performance_profiling,"> What is the result of this command for you: bazel build c opt third_party/tensorflow/lite/delegates/gpu/cl/testing:performance_profiling It fails with   But that path doesn't exist, so if I drop the `third_party` at the beginning then it builds.","Having built the tool using the correct path I was able to produce a comparison of v2.5.0rc3.log and v2.9.1.log I've also put it in a spreadsheet so that it's easier to compare the timings: https://docs.google.com/spreadsheets/d/1xmcY5TMXFH3RvSE91ZPuvLe2OZxqfGNbw5h7SR0ibug/edit?usp=sharing It seems that later tflite implementations use winograd in place of certain 2D convolutions, but according to the profiles both have similar ideal total times (with older tflite being slightly quicker).  But then during inferencing v2.5.0rc3 beats the ideal time considerably whereas v2.9.1 is much slower than the ideal time.","Hi, Thank you for your information From my side I can see that 'usual inference time' higher than ideal total time until this CL: https://github.com/tensorflow/tensorflow/commit/14dd3a808ded246021a110ee7cf8b174214ed6c2 and much smaller after. I don't know if this change included in v2.9.1 I hope this CL will fix performance for you. ""It seems that later tflite implementations use winograd in place of certain 2D convolutions, but according to the profiles both have similar ideal total times (with older tflite being slightly quicker)."" Nope, both of them use Winograd, but in version 2.5 this profiling tool was not so detailed and used convolution name for Winograd transformations.","> I also have a question related to the GPU performance. >  > If I run two instances of our benchmark simultaneously each runs at the same speed as a single one (utilising ~66% of the GPU instead of ~33%). Therefore it stands to reason that running two threads within a process, each with their own model, interpreter and delegate, should provide an inferencing speed increase by classifying two samples at a time. However this is not the case  is there something in the GPU delegate which is preventing concurrent OpenCL pipelines within the same process? I don't know why it works as you described and can not help with this question.","> Hi, Thank you for your information From my side I can see that 'usual inference time' higher than ideal total time until this CL: 14dd3a8 and much smaller after. I don't know if this change included in v2.9.1 I hope this CL will fix performance for you. >  > ""It seems that later tflite implementations use winograd in place of certain 2D convolutions, but according to the profiles both have similar ideal total times (with older tflite being slightly quicker)."" Nope, both of them use Winograd, but in version 2.5 this profiling tool was not so detailed and used convolution name for Winograd transformations. You're right  your patch in April did fix this issue but unfortunately did not make it in to the v2.9.1 release! I've updated the spreadsheet with performance results from the master branch and it has indeed returned to the inference speeds we used to see. We'll just have to wait for the next tagged release and then we can update our dependencies. Many thanks."
376,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Re-applying the Jit indexing build path)ï¼Œ å†…å®¹æ˜¯ (Resubmitting the build path for JIT 64 bit indexing (rolling back the b6569d7) and fixing the name length (to fix the the Winodws build issue). Attn: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kushanam,Re-applying the Jit indexing build path,Resubmitting the build path for JIT 64 bit indexing (rolling back the b6569d7) and fixing the name length (to fix the the Winodws build issue). Attn: ,2022-06-14T19:17:21Z,ready to pull size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/56460
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(assemble two model into one, but encount  ValueError: Graph disconnected)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fjzhangcr,"assemble two model into one, but encount  ValueError: Graph disconnected",Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution windows 10  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-14T16:00:11Z,stat:awaiting response type:bug stale comp:keras TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56459,"by the way, the purpose i want to assemble the two model together is i want to flatten the mobile net with my GlobalAveragePooling2D and Dense layer. Otherwise the moble net will be nested layer ,my model summary will looks like: ","Hi , Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1854,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Demo example Segmentation fault with tf version 2.9.1)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 22.04  Mobile device _No response_  Python version Python 3.10.4  Bazel version _No response_  GCC/Compiler version 11.2.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  Installed python packages:  Installed c library:  Created model:  Code:  shell root:~/demo gcc I/usr/local/include L/usr/local/lib main.c ltensorflow o main; ./main 20220614 15:26:32.875436: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/ 20220614 15:26:32.876544: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve } 20220614 15:26:32.876598: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: model/ 20220614 15:26:32.876660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220614 15:26:32.886525: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 20220614 15:26:32.887156: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle. 20220614 15:26:32.903110: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/ 20220614 15:26:32.909285: I tenso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Arnold1,Demo example Segmentation fault with tf version 2.9.1,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Ubuntu 22.04  Mobile device _No response_  Python version Python 3.10.4  Bazel version _No response_  GCC/Compiler version 11.2.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  Installed python packages:  Installed c library:  Created model:  Code:  shell root:~/demo gcc I/usr/local/include L/usr/local/lib main.c ltensorflow o main; ./main 20220614 15:26:32.875436: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/ 20220614 15:26:32.876544: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve } 20220614 15:26:32.876598: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: model/ 20220614 15:26:32.876660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220614 15:26:32.886525: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 20220614 15:26:32.887156: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle. 20220614 15:26:32.903110: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/ 20220614 15:26:32.909285: I tenso",2022-06-14T15:33:38Z,stat:awaiting response type:support stale comp:runtime TF 2.9,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56458,"Hi ! Sorry for the late response. I could not find any issue in Ubuntu 18 with TF 2.8.2 ,TF 2.9.1 and nightly. Issue might be with the Ubuntu 22 environment.  ! Could you please look at this issue.", did you run the same code and model  what is the output of the code? ,! Thanks for pointing out. It is  neither throwing segmentation fault nor running from  TF_SessionRun on wards.," what does it do exactly  it crashes and skips printf(""%f\n"",offsets[0]);? can you prepare a docker image for reproducibility?", ! It is not crashing just skipping the statements from TF_SessionRun onwards.,"Hi, I'm not sure if it's somehow related, but I too was experiencing segmentation faults when attempting to train my model using tf2.9 After a monthlong process of checking NVidia drivers, installed libraries,  and a long list of etc, I tried to download a docker container with tf2.11 and was able to train with no segmentation faults, using almost the exact same code (only change necessary was changing `tf.keras.optimizers.Adam()` to `tf.optimizers.Adam()`)"," , Thanks for confirming the working example, although changing from `tf.keras.optimizers.Adam()` to `tf.optimizers.Adam()` is also not necessary, since all these are aliases for Adam optimizer.  , Could you please try as suggested above or try downloading the latest file for Tensorflow 2.11 from the below link and let us know the outcome. Thanks! https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflowcpulinuxx86_642.11.0.tar.gz",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
677,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF Lite NNAPI fails to load a model)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04 (host), Android 11 (mobile device)  Mobile device Device based on Qualcomm Snapdragon 6490  Python version 3.8  Bazel version 4.2.1  GCC/Compiler version 8.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,MM2932,TF Lite NNAPI fails to load a model,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04 (host), Android 11 (mobile device)  Mobile device Device based on Qualcomm Snapdragon 6490  Python version 3.8  Bazel version 4.2.1  GCC/Compiler version 8.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-06-14T15:00:38Z,stat:awaiting response type:bug stale comp:lite TFLiteNNAPIDelegate TF 2.9,closed,0,15,https://github.com/tensorflow/tensorflow/issues/56457,Hi  !  I tracked the error here  Here are a few things I need to know. 1. Could you confirm whether Android studio is detecting your device in a live state.  2. Commands to implement qtidsp delegate specificially. ,"Hi , thank you for your answer. > 1. Could you confirm whether Android studio is detecting your device in a live state. Yes, the device is live all the time. > 2. Commands to implement qtidsp delegate specificially. Our current implementation is written in C++ and we simply select `accelerator_name = ""qtidsp""` for `tflite::StatefulNnApiDelegate::Options`. But the issue can also be reproduced with your own tools, for example with your Inference Diff tool selecting the same accelerator. A couple other comments:  This error is triggered by the model  linked in the ticket. We tested other models that did not cause this error.  In case this is helpful for your diagnosis, we actually tracked the error and it looks like it is happening here: https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/lite/delegates/nnapi/nnapi_delegate.ccL4644", ! Could you please look at this issue.,"Hi  ,  ,  ! Do you have any update on this issue? Thanks!","Miao, can you please have a look Thanks",Hi  ! Are you taking care of this issue? Thanks! Massimo.,"> E/VersionedInterfaces: DeathHandler::serviceDied  service unexpectedly died! This indicates that the model somehow has caused the qtidsp NNAPI driver to crash. Since it is an issue with proprietary driver, I'll forward the issue to our vendor partner. Could you share the following information: Device Name, Manufacture, Android version?",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi  , Sorry to keep you waiting. We are trying to reach out to Qualcomm to report this issue. Please keep the ticket open, so that we can share further updates with you. Thanks! Massimo.","Hi , has Qualcomm gotten back to you?","Hi, we contacted Qualcomm last year, but a fix is not available yet. They have been working on a new delegate (QNN), that will replace the Hexagon. We hope QNN will enable us to run this model soon. Thanks!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
625,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA performance drop when use --xla_dump_to  flag)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.0rc0  Custom Code No  OS Platform and Distribution _No response_  Mobile device Linux Ubuntu 18.04  Python version 3.8.8  Bazel version 5.0.0  GCC/Compiler version 7.5.0  CUDA/cuDNN version 11.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,shenh10,XLA performance drop when use --xla_dump_to  flag,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.0rc0  Custom Code No  OS Platform and Distribution _No response_  Mobile device Linux Ubuntu 18.04  Python version 3.8.8  Bazel version 5.0.0  GCC/Compiler version 7.5.0  CUDA/cuDNN version 11.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-14T08:04:06Z,stat:awaiting tensorflower type:bug comp:xla TF 2.9,closed,0,18,https://github.com/tensorflow/tensorflow/issues/56454," I tried to replicate the reported issue, could you find the gist here and confirm the same? Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,">  yes, did you reproduce the problem? I am not able to edit the gist.", Thank you for the response! Reopening this issue as it still persists. Thank you!,"Hi  , I try to run the gist you provided before, the program is running on CPU which cannot reproduce the situation I proposed. Could you reproduce the code on GPU and tell me your result?"," I was able to replicate the issue, could you please have a look at this gist and confirm the same. Thank you!"," yeah, the test code doesn't really matter, I found the issue during running various model. The difference is the code of the gist doesn't use xla. You might add environment variables such as os.environ['TF_XLA_FLAGS'] and os.environ['XLA_FLAGS'] to the code. ", Can you please modify the colab gist and provide the working gist. Thanks!,>  Can you please modify the colab gist and provide the working gist. Thanks! It's quite strange that I cannot use the gist to reproduce the problem. Could you please run the demo on your local machine? I cannot figure out how colab works with TF environment variables. Colab prints nothing even when I set TF_CPP_VMODULE=mark_for_compilation_pass=4 to os.environ or by %env methods thus I cannot make sure whether XLA is actually worked when running., Can you check with the nightly version and check if the issue still persists? Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"To better showcase the performance diff, I used the script below. I suggest TF devs  use it to reproduce in your environment. I tested the `test_mlp.py` example as before on three TF versions ( 2.9.0, 2.9.1 and tfnightly(pip install tfnightly) to demonstrate the performance result.   pip install tensorflow==2.9.1  pip install tfnightly ", Thank you for reporting this. I was able to reproduce your issue on colab. I am attaching the gist here.  Summary of the issue: When use xla_dump_toï¼Œthe performance drop to 50% of original TensorFlow runtime. Thanks!,"We saw this issue internally, it should have been fixed by https://github.com/tensorflow/tensorflow/commit/7ba8b410015df3025c49e9e65200c7218a719c99, could you verify?","> We saw this issue internally, it should have been fixed by 7ba8b41, could you verify? Yes, the commit fixed this issue.  ",Are you satisfied with the resolution of your issue? Yes No
840,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to load delegate from libedgetpu.so.1 M.2 Accelerator with Dual Edge TPU)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Support  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Debian GNU/Linux 11 (bullseye)  Mobile device _No response_  Python version 3.9.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? The image classifier test from https://www.coral.ai/docs/m2/getstarted/ fails. I had this coral device installed and working in Frigate for months before it quit.  Standalone code to reproduce the issue   Any other info / logs   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tehniemer,Failed to load delegate from libedgetpu.so.1 M.2 Accelerator with Dual Edge TPU,  Issue Type Support  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Debian GNU/Linux 11 (bullseye)  Mobile device _No response_  Python version 3.9.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? The image classifier test from https://www.coral.ai/docs/m2/getstarted/ fails. I had this coral device installed and working in Frigate for months before it quit.  Standalone code to reproduce the issue   Any other info / logs   ,2022-06-14T00:47:54Z,stat:awaiting response type:bug comp:micro TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56451,Hi ! Could you follow these things and let us know.  1. Ensure the Coral device is plugged in while inference is happening. 2. Check the permissions on your /dev/apex_0 device. 3. Add the user to plugdev group. Attached  issues 1 and 2 with similar error stack trace for reference. Please post on Coral Edge Tpu support for further assistance.  Thank you!, see https://github.com/googlecoral/edgetpu/issues/611, ! Could you please close this thread here as it will be tracked in coral repository. Thank you!,Are you satisfied with the resolution of your issue? Yes No
746,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow load_weights(0 function working in jupyter ntoebook but not in a script)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  OS Platform and Distribution Windows  Python version 3.8.10  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output   **Solved** Tensorflow does not work well with relative paths. The solution was to provide an absolute path to the checkpoint when running it in the terminal. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,shehroz218,Tensorflow load_weights(0 function working in jupyter ntoebook but not in a script,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.1  OS Platform and Distribution Windows  Python version 3.8.10  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output   **Solved** Tensorflow does not work well with relative paths. The solution was to provide an absolute path to the checkpoint when running it in the terminal. ,2022-06-13T19:26:28Z,type:bug comp:apis TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56447,", I tried to execute the given code with an alternate approach. Kindly find the workaround gist of it here and let us know if the issue still persists. Thank you!", The error still persists.  The code works as intended in a notebook both locally and on colabs. The error arises when running the code in a .py file.  I've updated my environment to match the version of the libraries as in the code on google colabs. for some reason the function (myenv\lib\sitepackages\tensorflow\python\training\py_checkpoint_reader.py) throws an error and is not able to find the checkpoints. The code should run fine because it is executing in a notebook but for some reason it does not replicate the output in a python script. ,Are you satisfied with the resolution of your issue? Yes No
823,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.linalg.qr does not support ""half"" datatype in the latest version)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 16.04  Mobile device _No response_  Python version 3.7.13  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  But the documentation describes that this API supports `half` datatype: https://www.tensorflow.org/api_docs/python/tf/linalg/qr shell import tensorflow as tf a = tf.random.uniform((10,10), dtype=""half"") tf.linalg.qr(a)   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,maybeLee,"tf.linalg.qr does not support ""half"" datatype in the latest version","Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 16.04  Mobile device _No response_  Python version 3.7.13  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  But the documentation describes that this API supports `half` datatype: https://www.tensorflow.org/api_docs/python/tf/linalg/qr shell import tensorflow as tf a = tf.random.uniform((10,10), dtype=""half"") tf.linalg.qr(a)   Relevant log output  ",2022-06-13T10:49:34Z,type:bug comp:ops awaiting PR merge TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56441,", I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist of it here.","Hi, Thanks for reporting the issue, I have created code fix internally.", This problem still persists for me. I've closed the issue to the thread I've created but hopefully you can help me with getting things back on track,Are you satisfied with the resolution of your issue? Yes No
662,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Core Dump in model.save_weights() (ARM))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution ARM8 (Raspberry Pi 4) with 8GB RAM  Mobile device Raspberry Pi 4 Bullseye  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,josuuribe,Core Dump in model.save_weights() (ARM),Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution ARM8 (Raspberry Pi 4) with 8GB RAM  Mobile device Raspberry Pi 4 Bullseye  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-12T18:29:35Z,stat:awaiting response type:bug stale comp:micro TF 2.8,closed,0,18,https://github.com/tensorflow/tensorflow/issues/56434, It is not recommended to directly install TF in Raspberry pi due to the memory constraints. Could you refer this link to know more on this . Thank you!," thanks for your response, I have tried this it in a RPI with 8GB RAM, so I think this is not a memory constraint, in particular the model it's very simple and works perfectly except when I try to save it. So the main problem is that seems is not possible use **model.save_weights(..)** function, this is a problem in most scenarios and I think not related to memory constraints.",Hi  ! Sorry for the late response . Could you update Threading=0  or Threading = 1 in /etc/odbcinst.ini file . Attached relevant thread for reference. Thank you!,"Hi   I have read your comment and the stackoverflow thread, I do not hace such file **/etc/odbcinst.ini** because this is a ODBC specific configuration file, not related to my problem.", ! Sorry for the inconvenience .Could you share the steps followed on installing Tensorflow in your device. Thank you!," nothing special, I downloaded the binary from Qengineering repo  and then install it via pip as usual **pip install tensorflow**", ! Could you please look at this issue . Thank you! , I was not able to reproduce the error. Can you please add all the necessary imports and give the complete code for me to reproduce the issue. Thanks!,"Hello   _This is my **pip list**_ Package                      Version   abslpy                      1.1.0 alepy                       0.7.5+8f3bc3b astunparse                   1.6.3 cachetools                   5.2.0 certifi                      2022.5.18.1 charsetnormalizer           2.0.12 cloudpickle                  2.1.0 cycler                       0.11.0 flatbuffers                  20181003210633 fonttools                    4.33.3 gast                         0.5.3 googleauth                  2.7.0 googleauthoauthlib         0.4.6 googlepasta                 0.2.0 grpcio                       1.46.3 gym                          0.24.1 gymnotices                  0.0.7 h5py                         3.7.0 idna                         3.3 importlibmetadata           4.11.4 importlibresources          5.7.1 keras                        2.8.0 KerasPreprocessing          1.1.2 kerasrl2                    1.0.5 kiwisolver                   1.4.3 libclang                     14.0.1 Markdown                     3.3.7 matplotlib                   3.5.2 numpy                        1.22.4 oauthlib                     3.2.0 opencvpython                4.6.0.66 opteinsum                   3.3.0 packaging                    21.3 Pillow                       9.1.1 pip                          20.3.4 pkgresources                0.0.0 protobuf                     3.20.0 pyasn1                       0.4.8 pyasn1modules               0.2.8 pyparsing                    3.0.9 pythondateutil              2.8.2 PyVirtualDisplay             3.0 requests                     2.28.0 requestsoauthlib            1.3.1 rsa                          4.8 setuptools                   44.1.1 six                          1.16.0 tensorboard                  2.8.0 tensorboarddataserver      0.6.1 tensorboardpluginwit       1.8.1 tensorflow                   2.8.0 tensorflowiogcsfilesystem 0.26.0 termcolor                    1.1.0 tfestimatornightly         2.8.0.dev2021122109 typingextensions            4.2.0 urllib3                      1.26.9 Werkzeug                     2.1.2 wheel                        0.37.1 wrapt                        1.14.1 zipp                         3.8.0 _This is a source code example, I have simplifed it, the first **save_weights** works the second no:_  The **nb_steps** has been intentionally reduced to 100 to reproduce the error (it's not relevant the number of steps).  The original code has a custom Callback with several calls to **save_weights**, this example has been simplified. As commented the first call to save_weights works, but second fails with different errors like:  ", I am not able to reproduce the issue on google colab. I was able to save the weights successfully multiple times. Please take a look at the gist here. Thanks!," please, read the bug description, this bug is on ARM (Rpi 4)", Sorry. Got it. Thanks. Also can you please attach the full stack trace. Thanks!,"There is not stack trace, the error output is the shown in bug description and previous messages.", Got it. Thanks!,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1820,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Experimental rostam)ï¼Œ å†…å®¹æ˜¯ ( The cosine embedding loss in torch is given by: loss(x,y)={1âˆ’cos(x1,x2),max(0,cos(x1,x2)âˆ’margin),if y=1if y=âˆ’1 The idea is to give it a pair of vectors and and a response value 1 or âˆ’1 depending on if they belong to the same group or not. First letâ€™s generate large dataset of 20 observations, each one being a length 100 vector. We also sample for each obs if it belongs to group â€˜aâ€™ or group â€˜bâ€™: library(torch) library(ggplot2)  going to use for plotting x < torch_randn(20, 100) y < sample(c(""a"", ""b""), replace = TRUE, size = x$shape[1]) Next, we crate a torch dataset that will do the following everytime we ask for a new item i: Take the observation corresponding to the item i in the dataset we created previously. With prob=0.5 select an observation from the same group to be itâ€™s pair, otherwise select an observation from a different group. Return both selected observations and the objective value âˆ’1 of they are obs are not from the same group and 1 if they are. data < dataset(   initialize = function(x, y, rate = 0.5) {     self x < x     self y < y     self rate < rate   },   .getitem = function(i) {     lab < self y[i]     if (self rate < runif(1)) {       i_ < sample(which(self y = lab), 1)       obj < 1     } else {       i_ < sample(which(self y = lab), 1)       obj < 1     }     list(x = self x[i,], x_ = self x[i_,], obj = obj)   },   .length = function() {     x shape[1]   } )  Initialize the dataset and dataloaders d < data(x, y) dl < dataloader(d, batch_size = 5) The model we are going to define is a dimension reduction model. It will take the observation space from 100 dimensio)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Danielfab1981,Experimental rostam," The cosine embedding loss in torch is given by: loss(x,y)={1âˆ’cos(x1,x2),max(0,cos(x1,x2)âˆ’margin),if y=1if y=âˆ’1 The idea is to give it a pair of vectors and and a response value 1 or âˆ’1 depending on if they belong to the same group or not. First letâ€™s generate large dataset of 20 observations, each one being a length 100 vector. We also sample for each obs if it belongs to group â€˜aâ€™ or group â€˜bâ€™: library(torch) library(ggplot2)  going to use for plotting x < torch_randn(20, 100) y < sample(c(""a"", ""b""), replace = TRUE, size = x$shape[1]) Next, we crate a torch dataset that will do the following everytime we ask for a new item i: Take the observation corresponding to the item i in the dataset we created previously. With prob=0.5 select an observation from the same group to be itâ€™s pair, otherwise select an observation from a different group. Return both selected observations and the objective value âˆ’1 of they are obs are not from the same group and 1 if they are. data < dataset(   initialize = function(x, y, rate = 0.5) {     self x < x     self y < y     self rate < rate   },   .getitem = function(i) {     lab < self y[i]     if (self rate < runif(1)) {       i_ < sample(which(self y = lab), 1)       obj < 1     } else {       i_ < sample(which(self y = lab), 1)       obj < 1     }     list(x = self x[i,], x_ = self x[i_,], obj = obj)   },   .length = function() {     x shape[1]   } )  Initialize the dataset and dataloaders d < data(x, y) dl < dataloader(d, batch_size = 5) The model we are going to define is a dimension reduction model. It will take the observation space from 100 dimensio",2022-06-12T14:19:11Z,invalid,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56433,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."," torch_cheatsheet A repo aiming at creating, sharing and improving the mlverse/torch cheatsheet.  Building To build the cheatsheet you will need to install a few packages:  Next you can run  "," List  [ CC([PluggableDevice] AsyncOpKernel) , CC(I need to pass a buffer in the tensorFlow library when the buffer is of the type byteArray) ,"
663,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`crosstool_wrapper_driver_is_not_gcc` error while building tf v2.8.0 and v2.9.0)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.8.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04 LTS  Mobile device   Python version 3.8.10  Bazel version 4.2.1  GCC/Compiler version GCC 7.5.0  CUDA/cuDNN version 11.2/8.1  GPU model and memory GTX 1050 2GB Mobile  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Scheggetta,`crosstool_wrapper_driver_is_not_gcc` error while building tf v2.8.0 and v2.9.0,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.8.0  Custom Code No  OS Platform and Distribution Ubuntu 20.04 LTS  Mobile device   Python version 3.8.10  Bazel version 4.2.1  GCC/Compiler version GCC 7.5.0  CUDA/cuDNN version 11.2/8.1  GPU model and memory GTX 1050 2GB Mobile  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-12T07:15:43Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 2.8,closed,0,22,https://github.com/tensorflow/tensorflow/issues/56432,", Could you try `bazel clean expunge` followed by `bazel` sync. Also kindly try to install the tensorflow `v2.8` with `GCC 7.3.1` compiler which is compatible. Thank you!","I'm having difficulties to find `GCC 7.3.1` from PPAs for Ubuntu 20.04. As I have written in the first post, also tf v2.9.0 gives the same problem with `GCC 9.4.0`. I'm also considering to try compiling tf with docker as witten here. Do you think I will get the same error?","Hi ,From the error traceback, it looks like issue is with compiler. Tf 2.8.0 is tested and configured with GCC 7.3.1. Tf 2.9.0 tested and configured with GCC v9.3.1. Request you to install right version of GCC  and build Tensorflow. Take a look at tested configuration here. Thank you!",Thanks for the comment. Do you have available a link or guide to install GCC 9.3.1? On my system there is GCC 9.4.0 and with `sudo apt install gcc9` I can't install the desired version.,"Hi , Did you check this https://linuxize.com/post/howtoinstallgcconubuntu2004/. Thank you!","Sorry for not having answered yet. I will try finding a way to install the right version of gcc. Thanks for the link, but it didn't work since that guide was published on June 4, 2020 when there was available only gcc 9.3.","Hi , Can you try   Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi , I tried your suggestion but the terminal gives me this error when executing the last command:  Am I doing something wrong?","Hi , Try this `sudo apt install gcc9 g++9`. I could able find gcc9 package. ","> Hi ,From the error traceback, it looks like issue is with compiler. Tf 2.8.0 is tested and configured with GCC 7.3.1. Tf 2.9.0 tested and configured with GCC v9.3.1. Request you to install right version of GCC and build Tensorflow. Take a look at tested configuration here. Thank you! Hi, but how you said above in a previous post, gcc 9.4.0 is not the right version. I had already tried with gcc 9.4.0 but it didn't work. Here is my version of gcc:  As you can see, it's the same as yours.","Hi , I could able build `Tensorflow v2.9.1` with `GCC v9.4.0` successfully. Thank you","So why do I have this `crosstool_wrapper_driver_is_not_gcc` error? This error occured with both tf versions (i.e., 2.8 and 2.9).",", I think you should add the TF 2.9 label as well.","> Hi , I could able build `Tensorflow v2.9.1` with `GCC v9.4.0` successfully. Thank you Ok, thanks. I am gonna try with `v2.9.1`.","> Hi , I could able build `Tensorflow v2.9.1` with `GCC v9.4.0` successfully. Thank you Hi , I tried with this configuration but I get the same error: ","Hi, since I gave up in making the build work out with GPU support, I decided to stick with CPU only. However, when I try the same commands to compile tf, I get a different error:  Specifically, `gcc failed: error executing command /usr/bin/gcc U_FORTIFY_SOURCE fstackprotector ...`","Hi , For the original issue, could you try the workarounds mentioned on similar thread https://github.com/tensorflow/tensorflow/issues/44692issuecomment917543931. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"> Hi , For the original issue, could you try the workarounds mentioned on similar thread  CC(crosstool_wrapper_driver_is_not_gcc failed: error executing command) (comment). Thank you! Ok , thanks for the tip. I am going to try it soon.","Hi  , Could you please spare some time to close the issue if got resolved.Please feel free to comeback if the issue still persists so that we can try for a workaround. Thankyou!",Are you satisfied with the resolution of your issue? Yes No
1898,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(macos build error: no matching function for call to 'LsbMask'     return LsbMask<uint64_t>(bits);)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution macos   Mobile device n/a  Python version 3.7, 3.8, 3.9, 3.10  Bazel version 4.2.2  GCC/Compiler version clang 13  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue  shell ./tensorflow/compiler/xla/util.h:502:5: error: variable of nonliteral type '::tensorflow::internal::CheckOpString' cannot be defined in a constexpr function     DCHECK_GE(exponent, 0);     ^ ./tensorflow/core/platform/default/logging.h:472:31: note: expanded from macro 'DCHECK_GE' define DCHECK_GE(val1, val2) CHECK_GE(val1, val2)                               ^ ./tensorflow/core/platform/default/logging.h:459:30: note: expanded from macro 'CHECK_GE' define CHECK_GE(val1, val2) CHECK_OP(Check_GE, >=, val1, val2)                              ^ ./tensorflow/core/platform/default/logging.h:452:40: note: expanded from macro 'CHECK_OP' define CHECK_OP(name, op, val1, val2) CHECK_OP_LOG(name, op, val1, val2)                                        ^ ./tensorflow/core/platform/default/logging.h:445:48: note: expanded from macro 'CHECK_OP_LOG'   while (::tensorflow::internal::CheckOpString _result{        \                                                ^ ./tensorflow/core/platform/default/logging.h:306:8: note: 'CheckOpString' is not literal because it is not an aggregate and has no constexpr constructors other than copy or move constructors struct CheckOpString {        ^ In file included from tensorflow/compiler/xla/util.cc:16: ./tens)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ngam,macos build error: no matching function for call to 'LsbMask'     return LsbMask<uint64_t>(bits);,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9.1  Custom Code No  OS Platform and Distribution macos   Mobile device n/a  Python version 3.7, 3.8, 3.9, 3.10  Bazel version 4.2.2  GCC/Compiler version clang 13  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue  shell ./tensorflow/compiler/xla/util.h:502:5: error: variable of nonliteral type '::tensorflow::internal::CheckOpString' cannot be defined in a constexpr function     DCHECK_GE(exponent, 0);     ^ ./tensorflow/core/platform/default/logging.h:472:31: note: expanded from macro 'DCHECK_GE' define DCHECK_GE(val1, val2) CHECK_GE(val1, val2)                               ^ ./tensorflow/core/platform/default/logging.h:459:30: note: expanded from macro 'CHECK_GE' define CHECK_GE(val1, val2) CHECK_OP(Check_GE, >=, val1, val2)                              ^ ./tensorflow/core/platform/default/logging.h:452:40: note: expanded from macro 'CHECK_OP' define CHECK_OP(name, op, val1, val2) CHECK_OP_LOG(name, op, val1, val2)                                        ^ ./tensorflow/core/platform/default/logging.h:445:48: note: expanded from macro 'CHECK_OP_LOG'   while (::tensorflow::internal::CheckOpString _result{        \                                                ^ ./tensorflow/core/platform/default/logging.h:306:8: note: 'CheckOpString' is not literal because it is not an aggregate and has no constexpr constructors other than copy or move constructors struct CheckOpString {        ^ In file included from tensorflow/compiler/xla/util.cc:16: ./tens",2022-06-11T23:28:23Z,type:build/install subtype:macOS TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56430,Relevant related issues: https://github.com/tensorflow/tensorflow/issues/56021 https://github.com/condaforge/tensorflowfeedstock/pull/240 https://github.com/pytorch/xla/issues/3589," and  seem to be the ones with the relevant commits here, e.g. https://github.com/tensorflow/tensorflow/commit/ce901649beec92845480b6bcdf6851baeff83daa, https://github.com/tensorflow/tensorflow/commit/0b6cc85863c4484586d1317b08d7c42bc8a0640c https://github.com/tensorflow/tensorflow/search?q=LsbMask&type=commits","Hi , As per the Tensorflow tested build configurations,  tensorflow2.9.0   Request you to downgrade Clang version to 10.14 and try to build. Thank you!"," thanks, testing in https://github.com/condaforge/tensorflowfeedstock/pull/240/commits/866a2fa745373da3d24b4adbffc914b18c5aef4b. Will report back later today.","Hi , I tried and it doesn't work. I am now going ahead and reverting daaed07d1056c522035f602e16196091eb371534 and 0b6cc85863c4484586d1317b08d7c42bc8a0640c to see  if this helps. Is there anyway for us to hear from  about this? The author of these two commits may be able to diagnose this better, especially as these two commits weren't made in a PR (direct commits to master as far as I could tell)",Are you satisfied with the resolution of your issue? Yes No,"Thanks all, this is now truly resolved by https://github.com/tensorflow/tensorflow/commit/bc4521dd193290f86bd5de8a56cefbcbfeae3213"
596,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Confusing wranings when `tf.data.dataset`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows 11  Mobile device N/A  Python version 3.10.4  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version 11.7/8.4  GPU model and memory RTX3090 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Zhaopudark,Confusing wranings when `tf.data.dataset`,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows 11  Mobile device N/A  Python version 3.10.4  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version 11.7/8.4  GPU model and memory RTX3090 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-11T12:46:10Z,stat:awaiting response type:bug comp:data TF 2.9,closed,1,4,https://github.com/tensorflow/tensorflow/issues/56427,", I was able to execute the code without any error warnings. Kindly find the gist of it here and please create a virtual environment and test your code again. Thank you!",  I have test in a  virtual environment  for test and didn't find any error warnings> So it's maybe a environment problem in my original environment. Thanks again and this issue can be closed!ğŸ˜Š,", Glad the workaround helped to resolve the issue for you, kindly feel free to move the issue to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No
714,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash.exe failed: error executing command)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Windows 11 10.0.22000  Mobile device _No response_  Python version 3.10.5  Bazel version 5.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Loldude0,Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash.exe failed: error executing command,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Windows 11 10.0.22000  Mobile device _No response_  Python version 3.10.5  Bazel version 5.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-11T05:37:10Z,stat:awaiting response type:build/install subtype:windows TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56426,"Hi , Follow the steps mentioned on official website of Tensorflow to build Tensorflow from source. Tensorflow suggest to install all the prerequisite packages to avoid errors. Thank you! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hello  , I wanted to take advantage of Intel's performance tweaks and MKL and AVX512 instructions in its Tensorflow fork hence I did not try building the official Tensorflow. Thanks.","Hi , You use Intel MKL to build Tensorflow. In the configure you can mention MKL `config=mkl` â€”Support for the IntelÂ® MKLDNN. Take a look at Preconfigured configurations. Thank you!",I'll try building it again that way. Thanks!,"HI , Did you try to build Tensorflow with `config=mkl`. Thank you!","> HI , Did you try to build Tensorflow with `config=mkl`. Thank you! Hello, due to frustration on failed builds, I simply installed the prebuilt version of IntelÂ® Optimization for TensorFlow and moved on. Thanks a lot for the help!",Are you satisfied with the resolution of your issue? Yes No,"Hi, Is there any solution for this error if we are building from source"
589,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite - different inference results between Android and python)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version python: 2.7.0; android: 2.4.0  Custom Code No  OS Platform and Distribution Windows 10; Android 10  Mobile device Redmi Note 9  Python version 3.7.9  CUDA/cuDNN version cuda_11.2.r11.2  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ivankrylatskoe,TFLite - different inference results between Android and python,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version python: 2.7.0; android: 2.4.0  Custom Code No  OS Platform and Distribution Windows 10; Android 10  Mobile device Redmi Note 9  Python version 3.7.9  CUDA/cuDNN version cuda_11.2.r11.2  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-10T21:34:10Z,stat:awaiting response type:bug comp:lite TF 2.7,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56425,Hi  ! The results look approximately the same to me.  Could you try Android version 4.2.1 or above with TF 2.8 version for better results. You can also refer quantization documentation to improve the performance .Thank you!,"Hi, ! ""Approximately same"" is like ""almost jumped across an abyss"". It doesn't work that way :) The results are either equal or not equal.  Actually, it means that the model results are either reproducible across different supported platforms, or they are not. Unless it is not explicitly told otherwise in the documentation, it is reasonable to suppose that normal behaviour is to get the same results on the same inputs using the same model. Returning to the issue, I admit that my description was ambiguous. Actually, Android version is 10.  2.4.0 is the version of tensorflow library for Android.  I rechecked the results for tensorflow library 2.8.0 and the results didn't change. So they are still different.",  ! Thanks for the update.  ! Could you please look at this issue?, Please take a look at this issue which describes why the results are different in python and android. Thanks!,", thank you for your reply. Actually, the solution described by  worked for me.  I made a mistake during a recheck of results on a different version of TF library. So, Tensorflow library 2.8.0 gives the same results as the python.  Thank you.",Are you satisfied with the resolution of your issue? Yes No
631,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue on building tensorflow-2.9.0 on centos 7.3)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution centos 7.3.1  Mobile device _No response_  Python version 3.8  Bazel version 5.0.0  GCC/Compiler version 9.3.1  CUDA/cuDNN version 11.2/8.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rajeev921,Issue on building tensorflow-2.9.0 on centos 7.3,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution centos 7.3.1  Mobile device _No response_  Python version 3.8  Bazel version 5.0.0  GCC/Compiler version 9.3.1  CUDA/cuDNN version 11.2/8.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` ,2022-06-10T10:02:25Z,stat:awaiting response type:build/install stale subtype:centos TF 2.9,closed,0,15,https://github.com/tensorflow/tensorflow/issues/56422,Are you satisfied with the resolution of your issue? Yes No,"Hi , Could you try to clear Bazel cache before building Tensorflow. `bazel clean expunge.`   And also kindly make sure to follow the steps mentioned here. Thank you!",Hi   I already did. Even I deleted all the cache and using it. Getting the same issue. I am following the steps as mentioned the the blog. I am using tensorflow 2.5.3 from long time and it is working fine now my requirements made me to upgrade it to the latest one  tensorflow 2.9.0 as it supports GCC 9.3.1 . I am able to build bazel 5.0.0 without any issue but only tensorflow didn't work. Please let me know if you need additional info to help me to resolve the issue.,"Adding the log as I tried after clean and removing cache and run from scratch.  As I am running everything on centos 7.3.1 But see it tries to pull win_1803 code doesn't make sense to me. **bazel output_user_root=/SCRATCH/rrajeev_bazelCache/ build  config=cuda config=v2 config=opt verbose_failures c opt cxxopt=std=c++17 cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:tensorflow_cc //tensorflow:install_headers tensorflow:tensorflow_framework //tensorflow/tools/...** Extracting Bazel installation... Starting local Bazel server and connecting to it... WARNING: The following configs were expanded more than once: [cuda, v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=149 INFO: Reading rc options for 'build' from/home/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from/home/.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library INFO: Reading rc options for 'build' from/home/.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=/remote/home/rrajeev/Software/anaconda3/envs/tf_9/bin/python3 action_env PYTHON_LIB_PATH=/remote/home/rrajeev/Software/anaconda3/envs/tf_9/lib/python3.8/sitepackages python_path=/remote/home/rrajeev/Software/anaconda3/envs/tf_9/bin/python3 action_env CUDA_TOOLKIT_PATH=/usr/local/cuda11.2 action_env TF_CUDA_COMPUTE_CAPABILITIES=7.0,7.0,7.0,7.0 action_env LD_LIBRARY_PATH=/usr/local/cuda11.2/lib64/:/usr/local/cuda11.2/lib64/:/opt/rh/devtoolset9/root/usr/lib64:/opt/rh/devtoolset9/root/usr/lib:/opt/rh/devtoolset9/root/usr/lib64/dyninst:/opt/rh/devtoolset9/root/usr/lib/dyninst:/opt/rh/devtoolset9/root/usr/lib64:/opt/rh/devtoolset9/root/usr/lib action_env GCC_HOST_COMPILER_PATH=/opt/rh/devtoolset9/root/usr/bin/gcc config=cuda INFO: Reading rc options for 'build' from/home/.bazelrc:   'build' options: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils INFO: Found applicable config definition build:short_logs in file/home/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file/home/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:cuda in file/home/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda INFO: Found applicable config definition build:cuda in file/home/.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda INFO: Found applicable config definition build:v2 in file/home/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:opt in file/home/.tf_configure.bazelrc: copt=Wnosigncompare host_copt=Wnosigncompare INFO: Found applicable config definition build:linux in file/home/.bazelrc: copt=w host_copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++14 host_cxxopt=std=c++14 config=dynamic_kernels distinct_host_configuration=false experimental_guard_against_concurrent_changes INFO: Found applicable config definition build:dynamic_kernels in file/home/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS **WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/093ed77f7d50f75b376f40a71ea86e08cedb8b80.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found DEBUG:/cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:  AutoConfiguration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ERROR:/home/tensorflow/tools/toolchains/win_1803/py36/BUILD:9:11: in interpreter_path attribute of py_runtime rule //tensorflow/tools/toolchains/win_1803/py36:py3_runtime: must be an absolute path. ERROR:/home/tensorflow/tools/toolchains/win_1803/py36/BUILD:9:11: Analysis of target '//tensorflow/tools/toolchains/win_1803/py36:py3_runtime' failed** INFO: Repository stylize instantiated at:  /home/WORKSPACE:23:14: in   /home/tensorflow/workspace0.bzl:81:17: in workspace Repository rule http_archive defined at:  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/build_defs/repo/http.bzl:364:31: in  INFO: Repository remote_java_tools instantiated at:   /DEFAULT.WORKSPACE.SUFFIX:360:6: in   /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/build_defs/repo/utils.bzl:233:18: in maybe Repository rule http_archive defined at:  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/build_defs/repo/http.bzl:364:31: in  INFO: Repository inception_v1 instantiated at:  /home/WORKSPACE:23:14: in   /home/tensorflow/workspace0.bzl:54:17: in workspace Repository rule http_archive defined at:  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/build_defs/repo/http.bzl:364:31: in  INFO: Repository mobile_ssd instantiated at:  /home/WORKSPACE:23:14: in   /home/tensorflow/workspace0.bzl:63:17: in workspace Repository rule http_archive defined at:  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/build_defs/repo/http.bzl:364:31: in  INFO: Repository go_sdk instantiated at:  /home/WORKSPACE:23:14: in   /home/tensorflow/workspace0.bzl:134:20: in workspace  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk Repository rule _go_download_sdk defined at:  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in  INFO: Repository speech_commands instantiated at:  /home/WORKSPACE:23:14: in   /home/tensorflow/workspace0.bzl:90:17: in workspace Repository rule http_archive defined at:  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/build_defs/repo/http.bzl:364:31: in  INFO: Repository rules_java instantiated at:  /home/WORKSPACE:23:14: in   /home/tensorflow/workspace0.bzl:134:20: in workspace  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/com_google_protobuf/protobuf_deps.bzl:34:21: in protobuf_deps Repository rule http_archive defined at:  /cache/5b5ea9016a850e6e8c5bdf2178058fa5/external/bazel_tools/tools/build_defs/repo/http.bzl:364:31: in  ERROR: Analysis of target '//tensorflow/tools/toolchains/win_1803/py36:py3_runtime' failed; build aborted:  INFO: Elapsed time: 81.593s INFO: 0 processes. FAILED: Build did NOT complete successfully (106 packages loaded, 1597 targets configured)     currently loading: // ... (10 packages)     Fetching ; fetching     Fetching ; fetching     Fetching ; Cloning 9bfcd7dbf0294ed9d11a99da6363fc28df904502 of https://github.com/bazelbuild/rules_docker.git     Fetching https://storage.googleapis.com/download.tensorflow.org/models/stylize_v1.zip     Fetching https://storage.googleapis.com/download.tensorflow.org/models/inception_v1.zip     Fetching https://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip     Fetching https://storage.googleapis.com/download.tensorflow.org/models/speech_commands_v0.01.zip     Fetching https://mirror.bazel.build/bazel_java_tools/releases/java/v11.6/java_toolsv11.6.zip ... (10 fetches)","Can someone please reply, How to resolve the issue as my whole development is stuck due to this build ?  ",Why highlight me even?,"Hi , I couldnâ€™t replicate your issue. Could you build master branch Tensorflow and check if you still getting same issue.  Make sure, you meet compatibility between versions. Thank you!","Hi ,  I tried below steps  1. clone the latest code. 2. git checkout v2.9.1  facing the same issue, so  3. git checkout origin/master and when I tried same it is throwing the same issue. Always making this link not found and then error and build stop. https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/da74868905d61b0e82dace3f833e8f411fae27bc.tar.gz",  any update on it?,    Hi Will anyone help me to resolve this. It been long time but no reply ?,"   Hi, Its been a long time on this thread without a reply. Can anyone look into that and help me.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
665,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tflite.invoke()` restart the kernel for transformer model (vision))ï¼Œ å†…å®¹æ˜¯ (Source: binary Tensorflow Version: 2.8 Custom Code: Yes  Current Behaviour? I've trained a vision transformer model and after training, I saved the model in `.tflite` format. Everything works without any issue until I reload the tflite model and try to make a prediction with it. It just restarts the kernel without showing any particular error message.  Standalone code to reproduce the issue HERE IS THE GIST.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,innat,`tflite.invoke()` restart the kernel for transformer model (vision),"Source: binary Tensorflow Version: 2.8 Custom Code: Yes  Current Behaviour? I've trained a vision transformer model and after training, I saved the model in `.tflite` format. Everything works without any issue until I reload the tflite model and try to make a prediction with it. It just restarts the kernel without showing any particular error message.  Standalone code to reproduce the issue HERE IS THE GIST.",2022-06-10T06:40:50Z,stat:awaiting response type:bug comp:lite TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56418,Hi !  This issue is not replicating in the 2.9 version. Attached gist for reference.  Thank you!," cool, thanks for testing. It does solve n tf 2.9. However, unfortunately, I'm running my actual test in the kaggle platform where the version of tf is 2.6 (maybe they'll upgrade to 2.8 soon). Now, is it possible to solve this issue with a lower tf version? ",I'm trying to spot a possible difference between tf 2.8 and tf 2.9 in the release note regarding `tf.lite`. I didn't find any bug fix in tf 2.9 for `tf.lite`.  Any thoughts? , ! You can use the git diff command to see the difference between 2.8 and 2.9.  Could we move this issue to closed status then? Thank you!," I need a solution that works less or equal to tf 2.8. And if there is any known bug in tflite (that fixes in tf 2.9), I like to adopt a workaround for tf <= 2.8.","Ok  ! Sorry for the inconvenience .  ! Could you please look at this issue. Attached gists in 2.8, 2.9 and nightly(2.10.0dev) for reference.Thank you!"," , This could be the commit which you are looking for https://github.com/tensorflow/tensorflow/commit/3a6a6465c45797bd573c118a535405da43ab03fa which was made in the Tensorflow 2.9 version, which is related to `.invoke()` method.", Is there any python API to adopt it? ,", I don't think we have any API do adapt that, could you please close the issue, since it is already included in the latest Tensorflow version. Thanks!",Are you satisfied with the resolution of your issue? Yes No
643,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.device('cpu') does not work under distributed strategies)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0dev20220520  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device N/A  Python version 3.8  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version 11.2/8.1  GPU model and memory T4/16GB  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lightingghost,tf.device('cpu') does not work under distributed strategies,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.10.0dev20220520  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device N/A  Python version 3.8  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version 11.2/8.1  GPU model and memory T4/16GB  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output _No response_,2022-06-09T22:37:50Z,stat:awaiting response type:bug stale comp:dist-strat,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56416,", I am able to replicate the issue with `Tf 2.10` and `tfnightly2.10.0.dev20220613`, please find gist here.Thank you! ","If the GPU is visible to the environment MirroredStrategy will use GPU. Check the detail here which states > A variable created under a MirroredStrategy is a MirroredVariable. If no devices are specified in the constructor argument of the strategy then it will use all the available GPUs. If no GPUs are found, it will use the available CPUs. Note that TensorFlow treats all CPUs on a machine as a single device, and uses threads internally for parallelism. You can disable the visible GPU devices with any one of the below line in your code at the starting, it will use CPU with warning message `WARNING:tensorflow:There are nonGPU devices in `tf.distribute.Strategy`, not using nccl allreduce. `  or  "," Thank you for replying. The scenarios is that I want to put a very large embedding on memory instead of GPU. I still want to use GPU for the rest part of the model. According to https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding, we are able to place the embedding on CPU. But we are not able to do so under a distributed strategy","You can also mention the specific device for mirrored strategy like below.   If you want to use both CPU and GPU you can define it using. `strategy = tf.distribute.MirroredStrategy(['CPU:0','GPU:0'])` So that all the operations done within strategy.scope() will be run on the specified device.",  If I use   I got   The variable is still ignoring `tf.device('cpu')` and gets assigned to all devices. Let me clarify the problem. The scenarios is that I want to put a very large embedding on memory instead of GPU. I still want to use GPU for the rest part of the model.,"You can try creating the variable outside the `strategy.scope()`. Then, when you use `tf.device(...)` inside of the `strategy.scope()`, the ops should only execute on the target device(s). For example, this code:  Generates this log:  (Where the `Mul` op correctly executes on the CPU.)",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
652,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error Running TFLite Mobile Benchmark Tool for iOS)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Mac OS 12.4  Mobile device _No response_  Python version 3.10.4  Bazel version 5.1.1homebrew  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gKurbis,Error Running TFLite Mobile Benchmark Tool for iOS,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Mac OS 12.4  Mobile device _No response_  Python version 3.10.4  Bazel version 5.1.1homebrew  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-08T19:23:33Z,stat:awaiting response type:bug stale comp:lite TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56405,Hi  !  Sorry for the late response.  Just wanted to check whether you said yes to IOS support when you run ./configure (After cloning Tensorflow repo) ? Could you please share the complete error stack trace for more clarity. Thank you!,"Hi !  I did say yes to IOS support with the ./configure was run. (See below)  When running build_benchmark_framework.sh , I get the following output, but the TFLiteBenchmark/TFLiteBenchmark/Frameworks directory remains empty:   ""Saving session...   ...copying shared history...   ...saving history...truncating history files...   ...completed.   [Process completed]"" Additionally, I have attached the full error in Xcode below. Cheers! ",ok  ! Thanks for the update.  ! Could you please look at this issue. Thank you!,"Hi ,  Were you able to build this on your end? I see there was a recent commit (commit 8eb1cba939c2ffaf0474d53f3d63debd458c9a3c) that seems to address this, but I am still unable to build the benchmark framework on my end after applying these changes. ",Hi   Just wondering if there is any update on this. Thanks!,Hi   I was able to build the benchmark framework on latest stable version r2.12 successfully without error. Please find the screenshot below.  Could you please on r2.12 and let us know if the issue still persists. Thanks.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
668,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu fails on manylinux2014_aarch64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,elfringham,//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu fails on manylinux2014_aarch64,Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-08T11:08:05Z,stat:awaiting tensorflower type:build/install subtype:centos subtype:bazel,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56402,"gdb session log below. The abort is triggered by a call to std::string::replace with __pos being massively positive. This is despite the two pointers to the string being the same which should result in __pos = 0. `gdb ./bazelbin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu GNU gdb (GDB) 12.1 Copyright (C) 2022 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later  This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type ""show copying"" and ""show warranty"" for details. This GDB was configured as ""aarch64unknownlinuxgnu"". Type ""show configuration"" for configuration details. For bug reporting instructions, please see: . Find the GDB manual and other documentation resources online at:     . For help, type ""help"". Type ""apropos word"" to search for commands related to ""word""... Reading symbols from ./bazelbin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu... (gdb) dir /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c Source directories searched: /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c:$cdir:$cwd (gdb) b re2::RE2::Replace Breakpoint 1 at 0x54fd94 (gdb) r Starting program: /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazelout/aarch64opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu  [Thread debugging using libthread_db enabled] Using host libthread_db library ""/lib64/libthread_db.so.1"". [==========] Running 2 tests from 1 test suite. [] Global test environment setup. [] 2 tests from SessionCoordinationTests/SingleClientTest [ RUN      ] SessionCoordinationTests/SingleClientTest.SetGetConfigInOpTest/EnableClusterSpecPropagation [New Thread 0xffff9ce8ecf0 (LWP 2396985)] [New Thread 0xffff97ffecf0 (LWP 2396986)] [New Thread 0xffff8f7eecf0 (LWP 2396987)] [New Thread 0xffff977eecf0 (LWP 2396988)] [New Thread 0xffff96fdecf0 (LWP 2396989)] [New Thread 0xffff967cecf0 (LWP 2396990)] [New Thread 0xffff95fbecf0 (LWP 2396991)] [New Thread 0xffff957aecf0 (LWP 2396992)] [New Thread 0xffff94f9ecf0 (LWP 2396993)] [New Thread 0xffff8fffecf0 (LWP 2396994)] [New Thread 0xffff8efdecf0 (LWP 2396995)] [New Thread 0xffff8e7cecf0 (LWP 2396996)] [New Thread 0xffff8dfbecf0 (LWP 2396997)] [New Thread 0xffff8d7aecf0 (LWP 2396998)] [New Thread 0xffff8cf9ecf0 (LWP 2396999)] [New Thread 0xffff57ffecf0 (LWP 2397000)] [New Thread 0xffff4f7eecf0 (LWP 2397001)] [New Thread 0xffff577eecf0 (LWP 2397002)] [New Thread 0xffff56fdecf0 (LWP 2397003)] [New Thread 0xffff567cecf0 (LWP 2397004)] [New Thread 0xffff55fbecf0 (LWP 2397005)] [New Thread 0xffff557aecf0 (LWP 2397006)] [New Thread 0xffff54f9ecf0 (LWP 2397007)] [New Thread 0xffff4fffecf0 (LWP 2397008)] [New Thread 0xffff4efdecf0 (LWP 2397009)] [New Thread 0xffff4e7cecf0 (LWP 2397010)] [New Thread 0xffff4dfbecf0 (LWP 2397011)] [New Thread 0xffff4d7aecf0 (LWP 2397012)] [New Thread 0xffff4cf9ecf0 (LWP 2397013)] [New Thread 0xffff17ffecf0 (LWP 2397014)] [New Thread 0xffff177eecf0 (LWP 2397015)] [New Thread 0xffff16fdecf0 (LWP 2397016)] [New Thread 0xffff167cecf0 (LWP 2397017)] [New Thread 0xffff15fbecf0 (LWP 2397018)] [New Thread 0xffff157aecf0 (LWP 2397019)] [New Thread 0xffff14f9ecf0 (LWP 2397020)] [New Thread 0xfffef7ffecf0 (LWP 2397021)] [New Thread 0xfffef77eecf0 (LWP 2397022)] [New Thread 0xfffef6fdecf0 (LWP 2397023)] [New Thread 0xfffef67cecf0 (LWP 2397024)] [New Thread 0xfffef5fbecf0 (LWP 2397025)] [New Thread 0xfffef57aecf0 (LWP 2397026)] [New Thread 0xfffef4f9ecf0 (LWP 2397027)] [New Thread 0xfffed7ffecf0 (LWP 2397028)] [New Thread 0xfffecfffecf0 (LWP 2397029)] [New Thread 0xfffed77eecf0 (LWP 2397030)] [New Thread 0xfffed6fdecf0 (LWP 2397031)] [New Thread 0xfffed67cecf0 (LWP 2397032)] [New Thread 0xfffed5fbecf0 (LWP 2397033)] [New Thread 0xfffed57aecf0 (LWP 2397034)] [New Thread 0xfffed4f9ecf0 (LWP 2397035)] [New Thread 0xfffecf7eecf0 (LWP 2397036)] [New Thread 0xfffecefdecf0 (LWP 2397037)] [New Thread 0xfffece7cecf0 (LWP 2397038)] [New Thread 0xfffecdfbecf0 (LWP 2397039)] [New Thread 0xfffecd7aecf0 (LWP 2397040)] [New Thread 0xfffeccf9ecf0 (LWP 2397041)] [New Thread 0xfffe97ffecf0 (LWP 2397042)] [New Thread 0xfffe8fffecf0 (LWP 2397043)] [New Thread 0xfffe977eecf0 (LWP 2397044)] [New Thread 0xfffe96fdecf0 (LWP 2397045)] [New Thread 0xfffe967cecf0 (LWP 2397046)] [New Thread 0xfffe95fbecf0 (LWP 2397047)] [New Thread 0xfffe957aecf0 (LWP 2397048)] [New Thread 0xfffe94f9ecf0 (LWP 2397049)] [New Thread 0xfffe8f7eecf0 (LWP 2397050)] [New Thread 0xfffe8efdecf0 (LWP 2397051)] [New Thread 0xfffe8e7cecf0 (LWP 2397052)] [New Thread 0xfffe8dfbecf0 (LWP 2397053)] [New Thread 0xfffe8d7aecf0 (LWP 2397054)] [New Thread 0xfffe8cf9ecf0 (LWP 2397055)] [New Thread 0xfffe57ffecf0 (LWP 2397056)] [New Thread 0xfffe4fffecf0 (LWP 2397057)] [New Thread 0xfffe577eecf0 (LWP 2397058)] [New Thread 0xfffe56fdecf0 (LWP 2397059)] [New Thread 0xfffe567cecf0 (LWP 2397060)] [New Thread 0xfffe55fbecf0 (LWP 2397061)] [New Thread 0xfffe557aecf0 (LWP 2397062)] [New Thread 0xfffe54f9ecf0 (LWP 2397063)] [New Thread 0xfffe4f7eecf0 (LWP 2397064)] [New Thread 0xfffe4efdecf0 (LWP 2397065)] [New Thread 0xfffe4e7cecf0 (LWP 2397066)] [New Thread 0xfffe4dfbecf0 (LWP 2397067)] [New Thread 0xfffe4d7aecf0 (LWP 2397068)] [New Thread 0xfffe4cf9ecf0 (LWP 2397069)] [New Thread 0xfffe17ffecf0 (LWP 2397070)] [New Thread 0xfffe0fffecf0 (LWP 2397071)] [New Thread 0xfffe177eecf0 (LWP 2397072)] [New Thread 0xfffe16fdecf0 (LWP 2397073)] [New Thread 0xfffe167cecf0 (LWP 2397074)] [New Thread 0xfffe15fbecf0 (LWP 2397075)] [New Thread 0xfffe157aecf0 (LWP 2397076)] [New Thread 0xfffe14f9ecf0 (LWP 2397077)] [New Thread 0xfffe0f7eecf0 (LWP 2397078)] [New Thread 0xfffe0efdecf0 (LWP 2397079)] [New Thread 0xfffe0e7cecf0 (LWP 2397080)] [New Thread 0xfffe0dfbecf0 (LWP 2397081)] [New Thread 0xfffe0d7aecf0 (LWP 2397082)] [New Thread 0xfffe0cf9ecf0 (LWP 2397083)] [New Thread 0xfffdd7ffecf0 (LWP 2397084)] [New Thread 0xfffdd77eecf0 (LWP 2397085)] [New Thread 0xfffdd6fdecf0 (LWP 2397086)] [New Thread 0xfffdd67cecf0 (LWP 2397087)] [New Thread 0xfffdd5fbecf0 (LWP 2397088)] [New Thread 0xfffdd57aecf0 (LWP 2397089)] [New Thread 0xfffdd4f9ecf0 (LWP 2397090)] [New Thread 0xfffdb7ffecf0 (LWP 2397091)] [New Thread 0xfffdb77eecf0 (LWP 2397092)] [New Thread 0xfffdb6fdecf0 (LWP 2397093)] [New Thread 0xfffdb67cecf0 (LWP 2397094)] [New Thread 0xfffdb5fbecf0 (LWP 2397095)] [New Thread 0xfffdb57aecf0 (LWP 2397096)] [New Thread 0xfffdb4f9ecf0 (LWP 2397097)] [New Thread 0xfffd97ffecf0 (LWP 2397098)] [New Thread 0xfffd977eecf0 (LWP 2397099)] [New Thread 0xfffd96fdecf0 (LWP 2397100)] [New Thread 0xfffd967cecf0 (LWP 2397101)] [New Thread 0xfffd95fbecf0 (LWP 2397102)] [New Thread 0xfffd957aecf0 (LWP 2397103)] [New Thread 0xfffd94f9ecf0 (LWP 2397104)] [New Thread 0xfffd77ffecf0 (LWP 2397105)] [New Thread 0xfffd777eecf0 (LWP 2397106)] [New Thread 0xfffd76fdecf0 (LWP 2397107)] [New Thread 0xfffd767cecf0 (LWP 2397108)] [New Thread 0xfffd75fbecf0 (LWP 2397109)] [New Thread 0xfffd757aecf0 (LWP 2397110)] [New Thread 0xfffd74f9ecf0 (LWP 2397111)] [New Thread 0xfffd57ffecf0 (LWP 2397112)] [New Thread 0xfffd577eecf0 (LWP 2397113)] [New Thread 0xfffd56fdecf0 (LWP 2397114)] [New Thread 0xfffd567cecf0 (LWP 2397115)] [New Thread 0xfffd55fbecf0 (LWP 2397116)] [New Thread 0xfffd557aecf0 (LWP 2397117)] [New Thread 0xfffd54f9ecf0 (LWP 2397118)] [New Thread 0xfffd37ffecf0 (LWP 2397119)] [New Thread 0xfffd377eecf0 (LWP 2397120)] [New Thread 0xfffd36fdecf0 (LWP 2397121)] [New Thread 0xfffd367cecf0 (LWP 2397122)] [New Thread 0xfffd35fbecf0 (LWP 2397123)] [New Thread 0xfffd357aecf0 (LWP 2397124)] [New Thread 0xfffd34f9ecf0 (LWP 2397125)] [New Thread 0xfffd17ffecf0 (LWP 2397126)] [New Thread 0xfffd177eecf0 (LWP 2397127)] [New Thread 0xfffd16fdecf0 (LWP 2397128)] [New Thread 0xfffd167cecf0 (LWP 2397129)] [New Thread 0xfffd15fbecf0 (LWP 2397130)] [New Thread 0xfffd157aecf0 (LWP 2397131)] [New Thread 0xfffd14f9ecf0 (LWP 2397132)] [New Thread 0xfffcf7ffecf0 (LWP 2397133)] [New Thread 0xfffcf77eecf0 (LWP 2397134)] [New Thread 0xfffcf6fdecf0 (LWP 2397135)] [New Thread 0xfffcf67cecf0 (LWP 2397136)] [New Thread 0xfffcf5fbecf0 (LWP 2397137)] [New Thread 0xfffcf57aecf0 (LWP 2397138)] [New Thread 0xfffcf4f9ecf0 (LWP 2397139)] [New Thread 0xfffcd7ffecf0 (LWP 2397140)] [New Thread 0xfffcd77eecf0 (LWP 2397141)] [New Thread 0xfffcd6fdecf0 (LWP 2397142)] [New Thread 0xfffcd67cecf0 (LWP 2397143)] [New Thread 0xfffcd5fbecf0 (LWP 2397144)] [New Thread 0xfffcd57aecf0 (LWP 2397145)] [New Thread 0xfffcd4f9ecf0 (LWP 2397146)] [New Thread 0xfffcb7ffecf0 (LWP 2397147)] [New Thread 0xfffcb77eecf0 (LWP 2397148)] [New Thread 0xfffcb6fdecf0 (LWP 2397149)] [New Thread 0xfffcb67cecf0 (LWP 2397150)] [New Thread 0xfffcb5fbecf0 (LWP 2397151)] [New Thread 0xfffcb57aecf0 (LWP 2397152)] [New Thread 0xfffcb4f9ecf0 (LWP 2397153)] [New Thread 0xfffc97ffecf0 (LWP 2397154)] [New Thread 0xfffc977eecf0 (LWP 2397155)] [New Thread 0xfffc96fdecf0 (LWP 2397156)] [New Thread 0xfffc967cecf0 (LWP 2397157)] [New Thread 0xfffc95fbecf0 (LWP 2397158)] [New Thread 0xfffc957aecf0 (LWP 2397159)] [New Thread 0xfffc94f9ecf0 (LWP 2397160)] [New Thread 0xfffc77ffecf0 (LWP 2397161)] [New Thread 0xfffc777eecf0 (LWP 2397162)] [New Thread 0xfffc76fdecf0 (LWP 2397163)] [New Thread 0xfffc767cecf0 (LWP 2397164)] [New Thread 0xfffc75fbecf0 (LWP 2397165)] [New Thread 0xfffc757aecf0 (LWP 2397166)] [New Thread 0xfffc74f9ecf0 (LWP 2397167)] [New Thread 0xfffc57ffecf0 (LWP 2397168)] [New Thread 0xfffc577eecf0 (LWP 2397169)] [New Thread 0xfffc56fdecf0 (LWP 2397170)] [New Thread 0xfffc567cecf0 (LWP 2397171)] [New Thread 0xfffc55fbecf0 (LWP 2397172)] [New Thread 0xfffc557aecf0 (LWP 2397173)] [New Thread 0xfffc54f9ecf0 (LWP 2397174)] [New Thread 0xfffc37ffecf0 (LWP 2397175)] [New Thread 0xfffc377eecf0 (LWP 2397176)] [New Thread 0xfffc36fdecf0 (LWP 2397177)] [New Thread 0xfffc367cecf0 (LWP 2397178)] [New Thread 0xfffc35fbecf0 (LWP 2397179)] [New Thread 0xfffc357aecf0 (LWP 2397180)] [New Thread 0xfffc34f9ecf0 (LWP 2397181)] [New Thread 0xfffc17ffecf0 (LWP 2397182)] [New Thread 0xfffc177eecf0 (LWP 2397183)] [New Thread 0xfffc16fdecf0 (LWP 2397184)] [New Thread 0xfffc167cecf0 (LWP 2397185)] [New Thread 0xfffc15fbecf0 (LWP 2397186)] [New Thread 0xfffc157aecf0 (LWP 2397187)] [New Thread 0xfffc14f9ecf0 (LWP 2397188)] [New Thread 0xfffbf7ffecf0 (LWP 2397189)] [New Thread 0xfffbf77eecf0 (LWP 2397190)] [New Thread 0xfffbf6fdecf0 (LWP 2397191)] [New Thread 0xfffbf67cecf0 (LWP 2397192)] [New Thread 0xfffbf5fbecf0 (LWP 2397193)] [New Thread 0xfffbf57aecf0 (LWP 2397194)] [New Thread 0xfffbf4f9ecf0 (LWP 2397195)] [New Thread 0xfffbd7ffecf0 (LWP 2397196)] [New Thread 0xfffbd77eecf0 (LWP 2397197)] [New Thread 0xfffbd6fdecf0 (LWP 2397198)] [New Thread 0xfffbd67cecf0 (LWP 2397199)] [New Thread 0xfffbd5fbecf0 (LWP 2397200)] [New Thread 0xfffbd57aecf0 (LWP 2397201)] [New Thread 0xfffbd4f9ecf0 (LWP 2397202)] [New Thread 0xfffbb7ffecf0 (LWP 2397203)] [New Thread 0xfffbb77eecf0 (LWP 2397204)] [New Thread 0xfffbb6fdecf0 (LWP 2397205)] [New Thread 0xfffbb67cecf0 (LWP 2397206)] [New Thread 0xfffbb5fbecf0 (LWP 2397207)] [New Thread 0xfffbb57aecf0 (LWP 2397208)] [New Thread 0xfffbb4f9ecf0 (LWP 2397209)] [New Thread 0xfffb9fffecf0 (LWP 2397210)] [New Thread 0xfffb9f7eecf0 (LWP 2397211)] [New Thread 0xfffb9efdecf0 (LWP 2397212)] [New Thread 0xfffb9e7cecf0 (LWP 2397213)] [New Thread 0xfffb9dfbecf0 (LWP 2397214)] [New Thread 0xfffb9d7aecf0 (LWP 2397215)] [New Thread 0xfffb9cf9ecf0 (LWP 2397216)] [New Thread 0xfffb83ffecf0 (LWP 2397217)] [New Thread 0xfffb837eecf0 (LWP 2397218)] [New Thread 0xfffb82fdecf0 (LWP 2397219)] [New Thread 0xfffb827cecf0 (LWP 2397220)] [New Thread 0xfffb81fbecf0 (LWP 2397221)] [New Thread 0xfffb817aecf0 (LWP 2397222)] [New Thread 0xfffb80f9ecf0 (LWP 2397223)] [New Thread 0xfffb63ffecf0 (LWP 2397224)] [New Thread 0xfffb637eecf0 (LWP 2397225)] [New Thread 0xfffb62fdecf0 (LWP 2397226)] [New Thread 0xfffb627cecf0 (LWP 2397227)] [New Thread 0xfffb61fbecf0 (LWP 2397228)] [New Thread 0xfffb617aecf0 (LWP 2397229)] [New Thread 0xfffb60f9ecf0 (LWP 2397230)] [New Thread 0xfffb43ffecf0 (LWP 2397231)] [New Thread 0xfffb437eecf0 (LWP 2397232)] [New Thread 0xfffb42fdecf0 (LWP 2397233)] [New Thread 0xfffb427cecf0 (LWP 2397234)] [New Thread 0xfffb41fbecf0 (LWP 2397235)] [New Thread 0xfffb417aecf0 (LWP 2397236)] [New Thread 0xfffb40f9ecf0 (LWP 2397237)] [New Thread 0xfffb23ffecf0 (LWP 2397238)] [New Thread 0xfffb237eecf0 (LWP 2397239)] [New Thread 0xfffb22fdecf0 (LWP 2397240)] [New Thread 0xfffb227cecf0 (LWP 2397241)] [New Thread 0xfffb21fbecf0 (LWP 2397242)] [New Thread 0xfffb217aecf0 (LWP 2397243)] [New Thread 0xfffb20f9ecf0 (LWP 2397244)] [New Thread 0xfffb03ffecf0 (LWP 2397245)] [New Thread 0xfffb037eecf0 (LWP 2397246)] [New Thread 0xfffb02fdecf0 (LWP 2397247)] [New Thread 0xfffb027cecf0 (LWP 2397248)] [New Thread 0xfffb01fbecf0 (LWP 2397249)] [New Thread 0xfffb017aecf0 (LWP 2397250)] [New Thread 0xfffb00f9ecf0 (LWP 2397251)] [New Thread 0xfffae3ffecf0 (LWP 2397252)] [New Thread 0xfffae37eecf0 (LWP 2397253)] [New Thread 0xfffae2fdecf0 (LWP 2397254)] [New Thread 0xfffae27cecf0 (LWP 2397255)] [New Thread 0xfffae1fbecf0 (LWP 2397256)] [New Thread 0xfffae17aecf0 (LWP 2397257)] [New Thread 0xfffae0f9ecf0 (LWP 2397258)] [New Thread 0xfffac3ffecf0 (LWP 2397259)] [New Thread 0xfffac37eecf0 (LWP 2397260)] [New Thread 0xfffac2fdecf0 (LWP 2397261)] [New Thread 0xfffac27cecf0 (LWP 2397262)] [New Thread 0xfffac1fbecf0 (LWP 2397263)] [New Thread 0xfffac17aecf0 (LWP 2397264)] [New Thread 0xfffac0f9ecf0 (LWP 2397265)] [New Thread 0xfffaa3ffecf0 (LWP 2397266)] [New Thread 0xfffaa37eecf0 (LWP 2397267)] [New Thread 0xfffaa2fdecf0 (LWP 2397268)] [New Thread 0xfffaa27cecf0 (LWP 2397269)] [New Thread 0xfffaa1fbecf0 (LWP 2397270)] [New Thread 0xfffaa17aecf0 (LWP 2397271)] [New Thread 0xfffaa0f9ecf0 (LWP 2397272)] [New Thread 0xfffa83ffecf0 (LWP 2397273)] [New Thread 0xfffa837eecf0 (LWP 2397274)] [New Thread 0xfffa82fdecf0 (LWP 2397275)] [New Thread 0xfffa827cecf0 (LWP 2397276)] [New Thread 0xfffa81fbecf0 (LWP 2397277)] [New Thread 0xfffa817aecf0 (LWP 2397278)] [New Thread 0xfffa80f9ecf0 (LWP 2397279)] [New Thread 0xfffa63ffecf0 (LWP 2397280)] [New Thread 0xfffa637eecf0 (LWP 2397281)] [New Thread 0xfffa62fdecf0 (LWP 2397282)] [New Thread 0xfffa627cecf0 (LWP 2397283)] [New Thread 0xfffa61fbecf0 (LWP 2397284)] [New Thread 0xfffa617aecf0 (LWP 2397285)] [New Thread 0xfffa60f9ecf0 (LWP 2397286)] [New Thread 0xfffa43ffecf0 (LWP 2397287)] [New Thread 0xfffa437eecf0 (LWP 2397288)] [New Thread 0xfffa42fdecf0 (LWP 2397289)] [New Thread 0xfffa427cecf0 (LWP 2397290)] [New Thread 0xfffa41fbecf0 (LWP 2397291)] [New Thread 0xfffa417aecf0 (LWP 2397292)] [New Thread 0xfffa40f9ecf0 (LWP 2397293)] [New Thread 0xfffa23ffecf0 (LWP 2397294)] [New Thread 0xfffa237eecf0 (LWP 2397295)] [New Thread 0xfffa22fdecf0 (LWP 2397296)] [New Thread 0xfffa227cecf0 (LWP 2397297)] [New Thread 0xfffa21fbecf0 (LWP 2397298)] [New Thread 0xfffa217aecf0 (LWP 2397299)] [New Thread 0xfffa20f9ecf0 (LWP 2397300)] [New Thread 0xfffa03ffecf0 (LWP 2397301)] [New Thread 0xfffa037eecf0 (LWP 2397302)] [New Thread 0xfffa02fdecf0 (LWP 2397303)] [New Thread 0xfffa027cecf0 (LWP 2397304)] [New Thread 0xfffa01fbecf0 (LWP 2397305)] [New Thread 0xfffa017aecf0 (LWP 2397306)] [New Thread 0xfffa00f9ecf0 (LWP 2397307)] [New Thread 0xfff9e3ffecf0 (LWP 2397308)] [New Thread 0xfff9e37eecf0 (LWP 2397309)] [New Thread 0xfff9e2fdecf0 (LWP 2397310)] [New Thread 0xfff9e27cecf0 (LWP 2397311)] [New Thread 0xfff9e1fbecf0 (LWP 2397312)] [New Thread 0xfff9e17aecf0 (LWP 2397313)] [New Thread 0xfff9e0f9ecf0 (LWP 2397314)] [New Thread 0xfff9c3ffecf0 (LWP 2397315)] [New Thread 0xfff9c37eecf0 (LWP 2397316)] [New Thread 0xfff9c2fdecf0 (LWP 2397317)] [New Thread 0xfff9c27cecf0 (LWP 2397318)] [New Thread 0xfff9c1fbecf0 (LWP 2397319)] [New Thread 0xfff9c17aecf0 (LWP 2397320)] [New Thread 0xfff9c0f9ecf0 (LWP 2397321)] [New Thread 0xfff9a3ffecf0 (LWP 2397322)] [New Thread 0xfff9a37eecf0 (LWP 2397323)] [New Thread 0xfff9a2fdecf0 (LWP 2397324)] [New Thread 0xfff9a27cecf0 (LWP 2397325)] [New Thread 0xfff9a1fbecf0 (LWP 2397326)] [New Thread 0xfff9a17aecf0 (LWP 2397327)] [New Thread 0xfff9a0f9ecf0 (LWP 2397328)] [New Thread 0xfff983ffecf0 (LWP 2397329)] [New Thread 0xfff9837eecf0 (LWP 2397330)] [New Thread 0xfff982fdecf0 (LWP 2397331)] [New Thread 0xfff9827cecf0 (LWP 2397332)] [New Thread 0xfff981fbecf0 (LWP 2397333)] [New Thread 0xfff9817aecf0 (LWP 2397334)] [New Thread 0xfff980f9ecf0 (LWP 2397335)] [New Thread 0xfff963ffecf0 (LWP 2397336)] [New Thread 0xfff9637eecf0 (LWP 2397337)] [New Thread 0xfff962fdecf0 (LWP 2397338)] [New Thread 0xfff9627cecf0 (LWP 2397339)] [New Thread 0xfff961fbecf0 (LWP 2397340)] [New Thread 0xfff9617aecf0 (LWP 2397341)] [New Thread 0xfff960f9ecf0 (LWP 2397342)] [New Thread 0xfff943ffecf0 (LWP 2397343)] [New Thread 0xfff9437eecf0 (LWP 2397344)] [New Thread 0xfff942fdecf0 (LWP 2397345)] [New Thread 0xfff9427cecf0 (LWP 2397346)] [New Thread 0xfff941fbecf0 (LWP 2397347)] [New Thread 0xfff9417aecf0 (LWP 2397348)] [New Thread 0xfff940f9ecf0 (LWP 2397349)] [New Thread 0xfff923ffecf0 (LWP 2397350)] [New Thread 0xfff9237eecf0 (LWP 2397351)] [New Thread 0xfff922fdecf0 (LWP 2397352)] [New Thread 0xfff9227cecf0 (LWP 2397353)] [New Thread 0xfff921fbecf0 (LWP 2397354)] [New Thread 0xfff9217aecf0 (LWP 2397355)] [New Thread 0xfff920f9ecf0 (LWP 2397356)] [New Thread 0xfff903ffecf0 (LWP 2397357)] [New Thread 0xfff9037eecf0 (LWP 2397358)] [New Thread 0xfff902fdecf0 (LWP 2397359)] [New Thread 0xfff9027cecf0 (LWP 2397360)] [New Thread 0xfff901fbecf0 (LWP 2397361)] [New Thread 0xfff9017aecf0 (LWP 2397362)] [New Thread 0xfff900f9ecf0 (LWP 2397363)] [New Thread 0xfff8e3ffecf0 (LWP 2397364)] [New Thread 0xfff8e37eecf0 (LWP 2397365)] [New Thread 0xfff8e2fdecf0 (LWP 2397366)] [New Thread 0xfff8e27cecf0 (LWP 2397367)] [New Thread 0xfff8e1fbecf0 (LWP 2397368)] [New Thread 0xfff8e17aecf0 (LWP 2397369)] [New Thread 0xfff8e0f9ecf0 (LWP 2397370)] [New Thread 0xfff8c3ffecf0 (LWP 2397371)] [New Thread 0xfff8c37eecf0 (LWP 2397372)] [New Thread 0xfff8c2fdecf0 (LWP 2397373)] [New Thread 0xfff8c27cecf0 (LWP 2397374)] [New Thread 0xfff8c1fbecf0 (LWP 2397375)] [New Thread 0xfff8c17aecf0 (LWP 2397376)] [New Thread 0xfff8c0f9ecf0 (LWP 2397377)] [New Thread 0xfff8a3ffecf0 (LWP 2397378)] [New Thread 0xfff8a37eecf0 (LWP 2397379)] [New Thread 0xfff8a2fdecf0 (LWP 2397380)] [New Thread 0xfff8a27cecf0 (LWP 2397381)] [New Thread 0xfff8a1fbecf0 (LWP 2397382)] [New Thread 0xfff8a17aecf0 (LWP 2397383)] [New Thread 0xfff8a0f9ecf0 (LWP 2397384)] [New Thread 0xfff883ffecf0 (LWP 2397385)] [New Thread 0xfff8837eecf0 (LWP 2397386)] [New Thread 0xfff882fdecf0 (LWP 2397387)] [New Thread 0xfff8827cecf0 (LWP 2397388)] [New Thread 0xfff881fbecf0 (LWP 2397389)] [New Thread 0xfff8817aecf0 (LWP 2397390)] [New Thread 0xfff880f9ecf0 (LWP 2397391)] [New Thread 0xfff863ffecf0 (LWP 2397392)] [New Thread 0xfff8637eecf0 (LWP 2397393)] [New Thread 0xfff862fdecf0 (LWP 2397394)] [New Thread 0xfff8627cecf0 (LWP 2397395)] [New Thread 0xfff861fbecf0 (LWP 2397396)] [New Thread 0xfff8617aecf0 (LWP 2397397)] [New Thread 0xfff860f9ecf0 (LWP 2397398)] [New Thread 0xfff843ffecf0 (LWP 2397399)] [New Thread 0xfff8437eecf0 (LWP 2397400)] [New Thread 0xfff842fdecf0 (LWP 2397401)] [New Thread 0xfff8427cecf0 (LWP 2397402)] [New Thread 0xfff841fbecf0 (LWP 2397403)] [New Thread 0xfff8417aecf0 (LWP 2397404)] [New Thread 0xfff840f9ecf0 (LWP 2397405)] [New Thread 0xfff823ffecf0 (LWP 2397406)] [New Thread 0xfff8237eecf0 (LWP 2397407)] [New Thread 0xfff822fdecf0 (LWP 2397408)] [New Thread 0xfff8227cecf0 (LWP 2397409)] [New Thread 0xfff821fbecf0 (LWP 2397410)] [New Thread 0xfff8217aecf0 (LWP 2397411)] [New Thread 0xfff820f9ecf0 (LWP 2397412)] [New Thread 0xfff803ffecf0 (LWP 2397413)] [New Thread 0xfff8037eecf0 (LWP 2397414)] [New Thread 0xfff802fdecf0 (LWP 2397415)] [New Thread 0xfff8027cecf0 (LWP 2397416)] [New Thread 0xfff801fbecf0 (LWP 2397417)] [New Thread 0xfff8017aecf0 (LWP 2397418)] [New Thread 0xfff800f9ecf0 (LWP 2397419)] [New Thread 0xfff7e3ffecf0 (LWP 2397420)] [New Thread 0xfff7e37eecf0 (LWP 2397421)] [New Thread 0xfff7e2fdecf0 (LWP 2397422)] [New Thread 0xfff7e27cecf0 (LWP 2397423)] [New Thread 0xfff7e1fbecf0 (LWP 2397424)] [New Thread 0xfff7e17aecf0 (LWP 2397425)] [New Thread 0xfff7e0f9ecf0 (LWP 2397426)] [New Thread 0xfff7c3ffecf0 (LWP 2397427)] [New Thread 0xfff7c37eecf0 (LWP 2397428)] [New Thread 0xfff7c2fdecf0 (LWP 2397429)] [New Thread 0xfff7c27cecf0 (LWP 2397430)] [New Thread 0xfff7c1fbecf0 (LWP 2397431)] [New Thread 0xfff7c17aecf0 (LWP 2397432)] [New Thread 0xfff7c0f9ecf0 (LWP 2397433)] [New Thread 0xfff7a3ffecf0 (LWP 2397434)] [New Thread 0xfff7a37eecf0 (LWP 2397435)] [New Thread 0xfff7a2fdecf0 (LWP 2397436)] [New Thread 0xfff7a27cecf0 (LWP 2397437)] [New Thread 0xfff7a1fbecf0 (LWP 2397438)] [New Thread 0xfff7a17aecf0 (LWP 2397439)] [New Thread 0xfff7a0f9ecf0 (LWP 2397440)] [New Thread 0xfff78bffecf0 (LWP 2397441)] [New Thread 0xfff78b7eecf0 (LWP 2397442)] [New Thread 0xfff78afdecf0 (LWP 2397443)] [New Thread 0xfff78a7cecf0 (LWP 2397444)] [New Thread 0xfff789fbecf0 (LWP 2397445)] [New Thread 0xfff7897aecf0 (LWP 2397446)] [New Thread 0xfff788f9ecf0 (LWP 2397447)] [New Thread 0xfff767ffecf0 (LWP 2397448)] [New Thread 0xfff7677eecf0 (LWP 2397449)] [New Thread 0xfff766fdecf0 (LWP 2397450)] [New Thread 0xfff7667cecf0 (LWP 2397451)] [New Thread 0xfff765fbecf0 (LWP 2397452)] [New Thread 0xfff7657aecf0 (LWP 2397453)] [New Thread 0xfff764f9ecf0 (LWP 2397454)] [New Thread 0xfff747ffecf0 (LWP 2397455)] [New Thread 0xfff7477eecf0 (LWP 2397456)] [New Thread 0xfff746fdecf0 (LWP 2397457)] [New Thread 0xfff7467cecf0 (LWP 2397458)] [New Thread 0xfff745fbecf0 (LWP 2397459)] [New Thread 0xfff7457aecf0 (LWP 2397460)] [New Thread 0xfff744f9ecf0 (LWP 2397461)] [New Thread 0xfff727ffecf0 (LWP 2397462)] [New Thread 0xfff7277eecf0 (LWP 2397463)] [New Thread 0xfff726fdecf0 (LWP 2397464)] [New Thread 0xfff7267cecf0 (LWP 2397465)] [New Thread 0xfff725fbecf0 (LWP 2397466)] [New Thread 0xfff7257aecf0 (LWP 2397467)] [New Thread 0xfff724f9ecf0 (LWP 2397468)] [New Thread 0xfff707ffecf0 (LWP 2397469)] [New Thread 0xfff7077eecf0 (LWP 2397470)] [New Thread 0xfff706fdecf0 (LWP 2397471)] [New Thread 0xfff7067cecf0 (LWP 2397472)] [New Thread 0xfff705fbecf0 (LWP 2397473)] [New Thread 0xfff7057aecf0 (LWP 2397474)] [New Thread 0xfff704f9ecf0 (LWP 2397475)] [New Thread 0xfff6e7ffecf0 (LWP 2397476)] [New Thread 0xfff6e77eecf0 (LWP 2397477)] [New Thread 0xfff6e6fdecf0 (LWP 2397478)] [New Thread 0xfff6e67cecf0 (LWP 2397479)] [New Thread 0xfff6e5fbecf0 (LWP 2397480)] [New Thread 0xfff6e57aecf0 (LWP 2397481)] [New Thread 0xfff6e4f9ecf0 (LWP 2397482)] [New Thread 0xfff6c7ffecf0 (LWP 2397483)] [New Thread 0xfff6c77eecf0 (LWP 2397484)] [New Thread 0xfff6c6fdecf0 (LWP 2397485)] [New Thread 0xfff6c67cecf0 (LWP 2397486)] [New Thread 0xfff6c5fbecf0 (LWP 2397487)] [New Thread 0xfff6c57aecf0 (LWP 2397488)] [New Thread 0xfff6c4f9ecf0 (LWP 2397489)] [New Thread 0xfff6a7ffecf0 (LWP 2397490)] [New Thread 0xfff6a77eecf0 (LWP 2397491)] [New Thread 0xfff6a6fdecf0 (LWP 2397492)] [New Thread 0xfff6a67cecf0 (LWP 2397493)] [New Thread 0xfff6a5fbecf0 (LWP 2397494)] [New Thread 0xfff6a57aecf0 (LWP 2397495)] [New Thread 0xfff6a4f9ecf0 (LWP 2397496)] [New Thread 0xfff687ffecf0 (LWP 2397497)] [New Thread 0xfff6877eecf0 (LWP 2397498)] [New Thread 0xfff686fdecf0 (LWP 2397499)] [New Thread 0xfff6867cecf0 (LWP 2397500)] [New Thread 0xfff685fbecf0 (LWP 2397501)] [New Thread 0xfff6857aecf0 (LWP 2397502)] [New Thread 0xfff684f9ecf0 (LWP 2397503)] [New Thread 0xfff667ffecf0 (LWP 2397504)] [New Thread 0xfff6677eecf0 (LWP 2397505)] [New Thread 0xfff666fdecf0 (LWP 2397506)] [New Thread 0xfff6667cecf0 (LWP 2397507)] [New Thread 0xfff665fbecf0 (LWP 2397508)] [New Thread 0xfff6657aecf0 (LWP 2397509)] [New Thread 0xfff664f9ecf0 (LWP 2397510)] [New Thread 0xfff647ffecf0 (LWP 2397511)] [New Thread 0xfff6477eecf0 (LWP 2397512)] [New Thread 0xfff646fdecf0 (LWP 2397513)] [New Thread 0xfff6467cecf0 (LWP 2397514)] [New Thread 0xfff645fbecf0 (LWP 2397515)] [New Thread 0xfff6457aecf0 (LWP 2397516)] [New Thread 0xfff644f9ecf0 (LWP 2397517)] [New Thread 0xfff627ffecf0 (LWP 2397518)] [New Thread 0xfff6277eecf0 (LWP 2397519)] [New Thread 0xfff626fdecf0 (LWP 2397520)] [New Thread 0xfff6267cecf0 (LWP 2397521)] [New Thread 0xfff625fbecf0 (LWP 2397522)] [New Thread 0xfff6257aecf0 (LWP 2397523)] [New Thread 0xfff624f9ecf0 (LWP 2397524)] [New Thread 0xfff607ffecf0 (LWP 2397525)] [New Thread 0xfff6077eecf0 (LWP 2397526)] [New Thread 0xfff606fdecf0 (LWP 2397527)] [New Thread 0xfff6067cecf0 (LWP 2397528)] [New Thread 0xfff605fbecf0 (LWP 2397529)] [New Thread 0xfff6057aecf0 (LWP 2397530)] [New Thread 0xfff604f9ecf0 (LWP 2397531)] [New Thread 0xfff5e7ffecf0 (LWP 2397532)] [New Thread 0xfff5e77eecf0 (LWP 2397533)] [New Thread 0xfff5e6fdecf0 (LWP 2397534)] [New Thread 0xfff5e67cecf0 (LWP 2397535)] [New Thread 0xfff5e5fbecf0 (LWP 2397536)] [New Thread 0xfff5e57aecf0 (LWP 2397537)] [New Thread 0xfff5e4f9ecf0 (LWP 2397538)] [New Thread 0xfff5c7ffecf0 (LWP 2397539)] [New Thread 0xfff5c77eecf0 (LWP 2397540)] [New Thread 0xfff5c6fdecf0 (LWP 2397541)] [New Thread 0xfff5c67cecf0 (LWP 2397542)] [New Thread 0xfff5c5fbecf0 (LWP 2397543)] [New Thread 0xfff5c57aecf0 (LWP 2397544)] [New Thread 0xfff5c4f9ecf0 (LWP 2397545)] [New Thread 0xfff5a7ffecf0 (LWP 2397546)] [New Thread 0xfff5a77eecf0 (LWP 2397547)] [New Thread 0xfff5a6fdecf0 (LWP 2397548)] [New Thread 0xfff5a67cecf0 (LWP 2397549)] [New Thread 0xfff5a5fbecf0 (LWP 2397550)] [New Thread 0xfff5a57aecf0 (LWP 2397551)] [New Thread 0xfff5a4f9ecf0 (LWP 2397552)] [New Thread 0xfff587ffecf0 (LWP 2397553)] [New Thread 0xfff5877eecf0 (LWP 2397554)] [New Thread 0xfff586fdecf0 (LWP 2397555)] [New Thread 0xfff5867cecf0 (LWP 2397556)] [New Thread 0xfff585fbecf0 (LWP 2397557)] [New Thread 0xfff5857aecf0 (LWP 2397558)] [New Thread 0xfff584f9ecf0 (LWP 2397559)] [New Thread 0xfff567ffecf0 (LWP 2397560)] [New Thread 0xfff5677eecf0 (LWP 2397561)] [New Thread 0xfff566fdecf0 (LWP 2397562)] [New Thread 0xfff5667cecf0 (LWP 2397563)] [New Thread 0xfff565fbecf0 (LWP 2397564)] [New Thread 0xfff5657aecf0 (LWP 2397565)] [New Thread 0xfff564f9ecf0 (LWP 2397566)] [New Thread 0xfff547ffecf0 (LWP 2397567)] [New Thread 0xfff5477eecf0 (LWP 2397568)] [New Thread 0xfff546fdecf0 (LWP 2397569)] [New Thread 0xfff5467cecf0 (LWP 2397570)] [New Thread 0xfff545fbecf0 (LWP 2397571)] [New Thread 0xfff5457aecf0 (LWP 2397572)] [New Thread 0xfff544f9ecf0 (LWP 2397573)] [New Thread 0xfff527ffecf0 (LWP 2397574)] [New Thread 0xfff5277eecf0 (LWP 2397575)] [New Thread 0xfff526fdecf0 (LWP 2397576)] [New Thread 0xfff5267cecf0 (LWP 2397577)] [New Thread 0xfff525fbecf0 (LWP 2397578)] [New Thread 0xfff5257aecf0 (LWP 2397579)] [New Thread 0xfff524f9ecf0 (LWP 2397580)] [New Thread 0xfff507ffecf0 (LWP 2397581)] [New Thread 0xfff5077eecf0 (LWP 2397582)] [New Thread 0xfff506fdecf0 (LWP 2397583)] [New Thread 0xfff5067cecf0 (LWP 2397584)] [New Thread 0xfff505fbecf0 (LWP 2397585)] [New Thread 0xfff5057aecf0 (LWP 2397586)] [New Thread 0xfff504f9ecf0 (LWP 2397587)] [New Thread 0xfff4e7ffecf0 (LWP 2397588)] [New Thread 0xfff4e77eecf0 (LWP 2397589)] [New Thread 0xfff4e6fdecf0 (LWP 2397590)] [New Thread 0xfff4e67cecf0 (LWP 2397591)] [New Thread 0xfff4e5fbecf0 (LWP 2397592)] [New Thread 0xfff4e57aecf0 (LWP 2397593)] [New Thread 0xfff4e4f9ecf0 (LWP 2397594)] [New Thread 0xfff4c7ffecf0 (LWP 2397595)] [New Thread 0xfff4c77eecf0 (LWP 2397596)] [New Thread 0xfff4c6fdecf0 (LWP 2397597)] [New Thread 0xfff4c67cecf0 (LWP 2397598)] [New Thread 0xfff4c5fbecf0 (LWP 2397599)] [New Thread 0xfff4c57aecf0 (LWP 2397600)] [New Thread 0xfff4c4f9ecf0 (LWP 2397601)] [New Thread 0xfff4a7ffecf0 (LWP 2397602)] [New Thread 0xfff4a77eecf0 (LWP 2397603)] [New Thread 0xfff4a6fdecf0 (LWP 2397604)] [New Thread 0xfff4a67cecf0 (LWP 2397605)] [New Thread 0xfff4a5fbecf0 (LWP 2397606)] [New Thread 0xfff4a57aecf0 (LWP 2397607)] [New Thread 0xfff4a4f9ecf0 (LWP 2397608)] [New Thread 0xfff487ffecf0 (LWP 2397609)] [New Thread 0xfff4877eecf0 (LWP 2397610)] [New Thread 0xfff486fdecf0 (LWP 2397611)] [New Thread 0xfff4867cecf0 (LWP 2397612)] [New Thread 0xfff485fbecf0 (LWP 2397613)] [New Thread 0xfff4857aecf0 (LWP 2397614)] [New Thread 0xfff484f9ecf0 (LWP 2397615)] [New Thread 0xfff467ffecf0 (LWP 2397616)] [New Thread 0xfff4677eecf0 (LWP 2397617)] [New Thread 0xfff466fdecf0 (LWP 2397618)] [New Thread 0xfff4667cecf0 (LWP 2397619)] [New Thread 0xfff465fbecf0 (LWP 2397620)] [New Thread 0xfff4657aecf0 (LWP 2397621)] [New Thread 0xfff464f9ecf0 (LWP 2397622)] [New Thread 0xfff447ffecf0 (LWP 2397623)] [New Thread 0xfff4477eecf0 (LWP 2397624)] [New Thread 0xfff446fdecf0 (LWP 2397625)] [New Thread 0xfff4467cecf0 (LWP 2397626)] [New Thread 0xfff445fbecf0 (LWP 2397627)] [New Thread 0xfff4457aecf0 (LWP 2397628)] [New Thread 0xfff444f9ecf0 (LWP 2397629)] [New Thread 0xfff427ffecf0 (LWP 2397630)] [New Thread 0xfff4277eecf0 (LWP 2397631)] [New Thread 0xfff426fdecf0 (LWP 2397632)] [New Thread 0xfff4267cecf0 (LWP 2397633)] [New Thread 0xfff425fbecf0 (LWP 2397634)] [New Thread 0xfff4257aecf0 (LWP 2397635)] [New Thread 0xfff424f9ecf0 (LWP 2397636)] [New Thread 0xfff407ffecf0 (LWP 2397637)] [New Thread 0xfff4077eecf0 (LWP 2397638)] [New Thread 0xfff406fdecf0 (LWP 2397639)] [New Thread 0xfff4067cecf0 (LWP 2397640)] [New Thread 0xfff405fbecf0 (LWP 2397641)] [New Thread 0xfff4057aecf0 (LWP 2397642)] [New Thread 0xfff404f9ecf0 (LWP 2397643)] [New Thread 0xfff3e7ffecf0 (LWP 2397644)] [New Thread 0xfff3e77eecf0 (LWP 2397645)] [New Thread 0xfff3e6fdecf0 (LWP 2397646)] [New Thread 0xfff3e67cecf0 (LWP 2397647)] [New Thread 0xfff3e5fbecf0 (LWP 2397648)] [New Thread 0xfff3e57aecf0 (LWP 2397649)] [New Thread 0xfff3e4f9ecf0 (LWP 2397650)] [New Thread 0xfff3c7ffecf0 (LWP 2397651)] [New Thread 0xfff3c77eecf0 (LWP 2397652)] [New Thread 0xfff3c6fdecf0 (LWP 2397653)] [New Thread 0xfff3c67cecf0 (LWP 2397654)] [New Thread 0xfff3c5fbecf0 (LWP 2397655)] [New Thread 0xfff3c57aecf0 (LWP 2397656)] [New Thread 0xfff3c4f9ecf0 (LWP 2397657)] [New Thread 0xfff3a7ffecf0 (LWP 2397658)] [New Thread 0xfff3a77eecf0 (LWP 2397659)] [New Thread 0xfff3a6fdecf0 (LWP 2397660)] [New Thread 0xfff3a67cecf0 (LWP 2397661)] [New Thread 0xfff3a5fbecf0 (LWP 2397662)] [New Thread 0xfff3a57aecf0 (LWP 2397663)] [New Thread 0xfff3a4f9ecf0 (LWP 2397664)] [New Thread 0xfff387ffecf0 (LWP 2397665)] [New Thread 0xfff3877eecf0 (LWP 2397666)] [New Thread 0xfff386fdecf0 (LWP 2397667)] [New Thread 0xfff3867cecf0 (LWP 2397668)] [New Thread 0xfff385fbecf0 (LWP 2397669)] [New Thread 0xfff3857aecf0 (LWP 2397670)] [New Thread 0xfff384f9ecf0 (LWP 2397671)] [New Thread 0xfff367ffecf0 (LWP 2397672)] [New Thread 0xfff3677eecf0 (LWP 2397673)] [New Thread 0xfff366fdecf0 (LWP 2397674)] [New Thread 0xfff3667cecf0 (LWP 2397675)] [New Thread 0xfff365fbecf0 (LWP 2397676)] [New Thread 0xfff3657aecf0 (LWP 2397677)] [New Thread 0xfff364f9ecf0 (LWP 2397678)] [New Thread 0xfff347ffecf0 (LWP 2397679)] [New Thread 0xfff3477eecf0 (LWP 2397680)] [New Thread 0xfff346fdecf0 (LWP 2397681)] [New Thread 0xfff3467cecf0 (LWP 2397682)] [New Thread 0xfff345fbecf0 (LWP 2397683)] [New Thread 0xfff3457aecf0 (LWP 2397684)] [New Thread 0xfff344f9ecf0 (LWP 2397685)] [New Thread 0xfff327ffecf0 (LWP 2397686)] [New Thread 0xfff3277eecf0 (LWP 2397687)] [New Thread 0xfff326fdecf0 (LWP 2397688)] [New Thread 0xfff3267cecf0 (LWP 2397689)] [New Thread 0xfff325fbecf0 (LWP 2397690)] [New Thread 0xfff3257aecf0 (LWP 2397691)] [New Thread 0xfff324f9ecf0 (LWP 2397692)] [New Thread 0xfff307ffecf0 (LWP 2397693)] [New Thread 0xfff3077eecf0 (LWP 2397694)] [New Thread 0xfff306fdecf0 (LWP 2397695)] [New Thread 0xfff3067cecf0 (LWP 2397696)] [New Thread 0xfff305fbecf0 (LWP 2397697)] [New Thread 0xfff3057aecf0 (LWP 2397698)] [New Thread 0xfff304f9ecf0 (LWP 2397699)] [New Thread 0xfff2e7ffecf0 (LWP 2397700)] [New Thread 0xfff2e77eecf0 (LWP 2397701)] [New Thread 0xfff2e6fdecf0 (LWP 2397702)] [New Thread 0xfff2e67cecf0 (LWP 2397703)] [New Thread 0xfff2e5fbecf0 (LWP 2397704)] [New Thread 0xfff2e57aecf0 (LWP 2397705)] [New Thread 0xfff2e4f9ecf0 (LWP 2397706)] [New Thread 0xfff2c7ffecf0 (LWP 2397707)] [New Thread 0xfff2c77eecf0 (LWP 2397708)] [New Thread 0xfff2c6fdecf0 (LWP 2397709)] [New Thread 0xfff2c67cecf0 (LWP 2397710)] [New Thread 0xfff2c5fbecf0 (LWP 2397711)] [New Thread 0xfff2c57aecf0 (LWP 2397712)] [New Thread 0xfff2c4f9ecf0 (LWP 2397713)] [New Thread 0xfff2a7ffecf0 (LWP 2397714)] [New Thread 0xfff2a77eecf0 (LWP 2397715)] [New Thread 0xfff2a6fdecf0 (LWP 2397716)] [New Thread 0xfff2a67cecf0 (LWP 2397717)] [New Thread 0xfff2a5fbecf0 (LWP 2397718)] [New Thread 0xfff2a57aecf0 (LWP 2397719)] [New Thread 0xfff2a4f9ecf0 (LWP 2397720)] [New Thread 0xfff287ffecf0 (LWP 2397721)] [New Thread 0xfff2877eecf0 (LWP 2397722)] [New Thread 0xfff286fdecf0 (LWP 2397723)] [New Thread 0xfff2867cecf0 (LWP 2397724)] [New Thread 0xfff285fbecf0 (LWP 2397725)] [New Thread 0xfff2857aecf0 (LWP 2397726)] 20220614 10:16:27.271595: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init > {0 > localhost:58347, 1 > localhost:33741, 2 > localhost:54957} [New Thread 0xfff284f9ecf0 (LWP 2397727)] [New Thread 0xfff267ffecf0 (LWP 2397728)] [New Thread 0xfff266fdecf0 (LWP 2397730)] [New Thread 0xfff2677eecf0 (LWP 2397729)] [New Thread 0xfff2667cecf0 (LWP 2397731)] [New Thread 0xfff265fbecf0 (LWP 2397732)] 20220614 10:16:27.273116: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:58347 [New Thread 0xfff2657aecf0 (LWP 2397733)] [New Thread 0xfff264f9ecf0 (LWP 2397734)] [New Thread 0xfff247ffecf0 (LWP 2397735)] [New Thread 0xfff2477eecf0 (LWP 2397736)] [New Thread 0xfff246fdecf0 (LWP 2397737)] [New Thread 0xfff2467cecf0 (LWP 2397738)] [New Thread 0xfff245fbecf0 (LWP 2397739)] [New Thread 0xfff2457aecf0 (LWP 2397740)] [New Thread 0xfff244f9ecf0 (LWP 2397741)] [New Thread 0xfff22fffecf0 (LWP 2397742)] [New Thread 0xfff22f7eecf0 (LWP 2397743)] [New Thread 0xfff22efdecf0 (LWP 2397744)] [New Thread 0xfff22e7cecf0 (LWP 2397745)] [New Thread 0xfff22dfbecf0 (LWP 2397746)] [New Thread 0xfff22d7aecf0 (LWP 2397747)] [New Thread 0xfff22cf9ecf0 (LWP 2397748)] [New Thread 0xfff20bffecf0 (LWP 2397749)] [New Thread 0xfff20b7eecf0 (LWP 2397750)] [New Thread 0xfff20afdecf0 (LWP 2397751)] [New Thread 0xfff20a7cecf0 (LWP 2397752)] [New Thread 0xfff209fbecf0 (LWP 2397753)] [New Thread 0xfff2097aecf0 (LWP 2397754)] [New Thread 0xfff208f9ecf0 (LWP 2397755)] [New Thread 0xfff1ebffecf0 (LWP 2397756)] [New Thread 0xfff1eb7eecf0 (LWP 2397757)] [New Thread 0xfff1eafdecf0 (LWP 2397758)] [New Thread 0xfff1ea7cecf0 (LWP 2397759)] [New Thread 0xfff1e9fbecf0 (LWP 2397760)] [New Thread 0xfff1e97aecf0 (LWP 2397761)] [New Thread 0xfff1e8f9ecf0 (LWP 2397762)] [New Thread 0xfff1cbffecf0 (LWP 2397763)] [New Thread 0xfff1cb7eecf0 (LWP 2397764)] [New Thread 0xfff1cafdecf0 (LWP 2397765)] [New Thread 0xfff1ca7cecf0 (LWP 2397766)] [New Thread 0xfff1c9fbecf0 (LWP 2397767)] [New Thread 0xfff1c97aecf0 (LWP 2397768)] [New Thread 0xfff1c8f9ecf0 (LWP 2397769)] [New Thread 0xfff1abffecf0 (LWP 2397770)] [New Thread 0xfff1ab7eecf0 (LWP 2397771)] [New Thread 0xfff1aafdecf0 (LWP 2397772)] [New Thread 0xfff1aa7cecf0 (LWP 2397773)] [New Thread 0xfff1a9fbecf0 (LWP 2397774)] [New Thread 0xfff1a97aecf0 (LWP 2397775)] [New Thread 0xfff1a8f9ecf0 (LWP 2397776)] [New Thread 0xfff18bffecf0 (LWP 2397777)] [New Thread 0xfff18b7eecf0 (LWP 2397778)] [New Thread 0xfff18afdecf0 (LWP 2397779)] [New Thread 0xfff18a7cecf0 (LWP 2397780)] [New Thread 0xfff189fbecf0 (LWP 2397781)] [New Thread 0xfff1897aecf0 (LWP 2397782)] [New Thread 0xfff188f9ecf0 (LWP 2397783)] [New Thread 0xfff16bffecf0 (LWP 2397784)] [New Thread 0xfff16b7eecf0 (LWP 2397785)] [New Thread 0xfff16afdecf0 (LWP 2397786)] [New Thread 0xfff16a7cecf0 (LWP 2397787)] [New Thread 0xfff169fbecf0 (LWP 2397788)] [New Thread 0xfff1697aecf0 (LWP 2397789)] [New Thread 0xfff168f9ecf0 (LWP 2397790)] [New Thread 0xfff14bffecf0 (LWP 2397791)] [New Thread 0xfff14b7eecf0 (LWP 2397792)] [New Thread 0xfff14afdecf0 (LWP 2397793)] [New Thread 0xfff14a7cecf0 (LWP 2397794)] [New Thread 0xfff149fbecf0 (LWP 2397795)] [New Thread 0xfff1497aecf0 (LWP 2397796)] [New Thread 0xfff148f9ecf0 (LWP 2397797)] [New Thread 0xfff12bffecf0 (LWP 2397798)] [New Thread 0xfff12b7eecf0 (LWP 2397799)] [New Thread 0xfff12afdecf0 (LWP 2397800)] [New Thread 0xfff12a7cecf0 (LWP 2397801)] [New Thread 0xfff129fbecf0 (LWP 2397802)] [New Thread 0xfff1297aecf0 (LWP 2397803)] [New Thread 0xfff128f9ecf0 (LWP 2397804)] [New Thread 0xfff10bffecf0 (LWP 2397805)] [New Thread 0xfff10b7eecf0 (LWP 2397806)] [New Thread 0xfff10afdecf0 (LWP 2397807)] [New Thread 0xfff10a7cecf0 (LWP 2397808)] [New Thread 0xfff109fbecf0 (LWP 2397809)] [New Thread 0xfff1097aecf0 (LWP 2397810)] [New Thread 0xfff108f9ecf0 (LWP 2397811)] [New Thread 0xfff0ebffecf0 (LWP 2397812)] [New Thread 0xfff0eb7eecf0 (LWP 2397813)] [New Thread 0xfff0eafdecf0 (LWP 2397814)] [New Thread 0xfff0ea7cecf0 (LWP 2397815)] [New Thread 0xfff0e9fbecf0 (LWP 2397816)] [New Thread 0xfff0e97aecf0 (LWP 2397817)] [New Thread 0xfff0e8f9ecf0 (LWP 2397818)] [New Thread 0xfff0cbffecf0 (LWP 2397819)] [New Thread 0xfff0cb7eecf0 (LWP 2397820)] [New Thread 0xfff0cafdecf0 (LWP 2397821)] [New Thread 0xfff0ca7cecf0 (LWP 2397822)] [New Thread 0xfff0c9fbecf0 (LWP 2397823)] [New Thread 0xfff0c97aecf0 (LWP 2397824)] [New Thread 0xfff0c8f9ecf0 (LWP 2397825)] [New Thread 0xfff0abffecf0 (LWP 2397826)] [New Thread 0xfff0ab7eecf0 (LWP 2397827)] [New Thread 0xfff0aafdecf0 (LWP 2397828)] [New Thread 0xfff0aa7cecf0 (LWP 2397829)] [New Thread 0xfff0a9fbecf0 (LWP 2397830)] [New Thread 0xfff0a97aecf0 (LWP 2397831)] [New Thread 0xfff0a8f9ecf0 (LWP 2397832)] [New Thread 0xfff08bffecf0 (LWP 2397833)] [New Thread 0xfff08b7eecf0 (LWP 2397834)] [New Thread 0xfff08afdecf0 (LWP 2397835)] [New Thread 0xfff08a7cecf0 (LWP 2397836)] [New Thread 0xfff089fbecf0 (LWP 2397837)] [New Thread 0xfff0897aecf0 (LWP 2397838)] [New Thread 0xfff088f9ecf0 (LWP 2397839)] [New Thread 0xfff06bffecf0 (LWP 2397840)] [New Thread 0xfff06b7eecf0 (LWP 2397841)] [New Thread 0xfff06afdecf0 (LWP 2397842)] [New Thread 0xfff06a7cecf0 (LWP 2397843)] [New Thread 0xfff069fbecf0 (LWP 2397844)] [New Thread 0xfff0697aecf0 (LWP 2397845)] [New Thread 0xfff068f9ecf0 (LWP 2397846)] [New Thread 0xfff04bffecf0 (LWP 2397847)] [New Thread 0xfff04b7eecf0 (LWP 2397848)] [New Thread 0xfff04afdecf0 (LWP 2397849)] [New Thread 0xfff04a7cecf0 (LWP 2397850)] [New Thread 0xfff049fbecf0 (LWP 2397851)] [New Thread 0xfff0497aecf0 (LWP 2397852)] [New Thread 0xfff048f9ecf0 (LWP 2397853)] [New Thread 0xfff02bffecf0 (LWP 2397854)] [New Thread 0xfff02b7eecf0 (LWP 2397855)] [New Thread 0xfff02afdecf0 (LWP 2397856)] [New Thread 0xfff02a7cecf0 (LWP 2397857)] [New Thread 0xfff029fbecf0 (LWP 2397858)] [New Thread 0xfff0297aecf0 (LWP 2397859)] [New Thread 0xfff028f9ecf0 (LWP 2397860)] [New Thread 0xfff00bffecf0 (LWP 2397861)] [New Thread 0xfff00b7eecf0 (LWP 2397862)] [New Thread 0xfff00afdecf0 (LWP 2397863)] [New Thread 0xfff00a7cecf0 (LWP 2397864)] [New Thread 0xfff009fbecf0 (LWP 2397865)] [New Thread 0xfff0097aecf0 (LWP 2397866)] [New Thread 0xfff008f9ecf0 (LWP 2397867)] [New Thread 0xffefebffecf0 (LWP 2397868)] [New Thread 0xffefeb7eecf0 (LWP 2397869)] [New Thread 0xffefeafdecf0 (LWP 2397870)] [New Thread 0xffefea7cecf0 (LWP 2397871)] [New Thread 0xffefe9fbecf0 (LWP 2397872)] [New Thread 0xffefe97aecf0 (LWP 2397873)] [New Thread 0xffefe8f9ecf0 (LWP 2397874)] [New Thread 0xffefcbffecf0 (LWP 2397875)] [New Thread 0xffefcb7eecf0 (LWP 2397876)] [New Thread 0xffefcafdecf0 (LWP 2397877)] [New Thread 0xffefca7cecf0 (LWP 2397878)] [New Thread 0xffefc9fbecf0 (LWP 2397879)] [New Thread 0xffefc97aecf0 (LWP 2397880)] [New Thread 0xffefc8f9ecf0 (LWP 2397881)] [New Thread 0xffefabffecf0 (LWP 2397882)] [New Thread 0xffefab7eecf0 (LWP 2397883)] [New Thread 0xffefaafdecf0 (LWP 2397884)] [New Thread 0xffefaa7cecf0 (LWP 2397885)] [New Thread 0xffefa9fbecf0 (LWP 2397886)] [New Thread 0xffefa97aecf0 (LWP 2397887)] [New Thread 0xffefa8f9ecf0 (LWP 2397888)] [New Thread 0xffef8bffecf0 (LWP 2397889)] [New Thread 0xffef8b7eecf0 (LWP 2397890)] [New Thread 0xffef8afdecf0 (LWP 2397891)] [New Thread 0xffef8a7cecf0 (LWP 2397892)] [New Thread 0xffef89fbecf0 (LWP 2397893)] [New Thread 0xffef897aecf0 (LWP 2397894)] [New Thread 0xffef88f9ecf0 (LWP 2397895)] [New Thread 0xffef6bffecf0 (LWP 2397896)] [New Thread 0xffef6b7eecf0 (LWP 2397897)] [New Thread 0xffef6afdecf0 (LWP 2397898)] [New Thread 0xffef6a7cecf0 (LWP 2397899)] [New Thread 0xffef69fbecf0 (LWP 2397900)] [New Thread 0xffef697aecf0 (LWP 2397901)] [New Thread 0xffef68f9ecf0 (LWP 2397902)] [New Thread 0xffef4bffecf0 (LWP 2397903)] [New Thread 0xffef4b7eecf0 (LWP 2397904)] [New Thread 0xffef4afdecf0 (LWP 2397905)] [New Thread 0xffef4a7cecf0 (LWP 2397906)] [New Thread 0xffef49fbecf0 (LWP 2397907)] [New Thread 0xffef497aecf0 (LWP 2397908)] [New Thread 0xffef48f9ecf0 (LWP 2397909)] [New Thread 0xffef2bffecf0 (LWP 2397910)] [New Thread 0xffef2b7eecf0 (LWP 2397911)] [New Thread 0xffef2afdecf0 (LWP 2397912)] [New Thread 0xffef2a7cecf0 (LWP 2397913)] [New Thread 0xffef29fbecf0 (LWP 2397914)] [New Thread 0xffef297aecf0 (LWP 2397915)] [New Thread 0xffef28f9ecf0 (LWP 2397916)] [New Thread 0xffef0bffecf0 (LWP 2397917)] [New Thread 0xffef0b7eecf0 (LWP 2397918)] [New Thread 0xffef0afdecf0 (LWP 2397919)] [New Thread 0xffef0a7cecf0 (LWP 2397920)] [New Thread 0xffef09fbecf0 (LWP 2397921)] [New Thread 0xffef097aecf0 (LWP 2397922)] [New Thread 0xffef08f9ecf0 (LWP 2397923)] [New Thread 0xffeeebffecf0 (LWP 2397924)] [New Thread 0xffeeeb7eecf0 (LWP 2397925)] [New Thread 0xffeeeafdecf0 (LWP 2397926)] [New Thread 0xffeeea7cecf0 (LWP 2397927)] [New Thread 0xffeee9fbecf0 (LWP 2397928)] [New Thread 0xffeee97aecf0 (LWP 2397929)] [New Thread 0xffeee8f9ecf0 (LWP 2397930)] [New Thread 0xffeecbffecf0 (LWP 2397931)] [New Thread 0xffeecb7eecf0 (LWP 2397932)] [New Thread 0xffeecafdecf0 (LWP 2397933)] [New Thread 0xffeeca7cecf0 (LWP 2397934)] [New Thread 0xffeec9fbecf0 (LWP 2397935)] [New Thread 0xffeec97aecf0 (LWP 2397936)] [New Thread 0xffeec8f9ecf0 (LWP 2397937)] [New Thread 0xffeeabffecf0 (LWP 2397938)] [New Thread 0xffeeab7eecf0 (LWP 2397939)] [New Thread 0xffeeaafdecf0 (LWP 2397940)] [New Thread 0xffeeaa7cecf0 (LWP 2397941)] [New Thread 0xffeea9fbecf0 (LWP 2397942)] [New Thread 0xffeea97aecf0 (LWP 2397943)] [New Thread 0xffeea8f9ecf0 (LWP 2397944)] [New Thread 0xffee8bffecf0 (LWP 2397945)] [New Thread 0xffee8b7eecf0 (LWP 2397946)] [New Thread 0xffee8afdecf0 (LWP 2397947)] [New Thread 0xffee8a7cecf0 (LWP 2397948)] [New Thread 0xffee89fbecf0 (LWP 2397949)] [New Thread 0xffee897aecf0 (LWP 2397950)] [New Thread 0xffee88f9ecf0 (LWP 2397951)] [New Thread 0xffee6bffecf0 (LWP 2397952)] [New Thread 0xffee6b7eecf0 (LWP 2397953)] [New Thread 0xffee6afdecf0 (LWP 2397954)] [New Thread 0xffee6a7cecf0 (LWP 2397955)] [New Thread 0xffee69fbecf0 (LWP 2397956)] [New Thread 0xffee697aecf0 (LWP 2397957)] [New Thread 0xffee68f9ecf0 (LWP 2397958)] [New Thread 0xffee4bffecf0 (LWP 2397959)] [New Thread 0xffee4b7eecf0 (LWP 2397960)] [New Thread 0xffee4afdecf0 (LWP 2397961)] [New Thread 0xffee4a7cecf0 (LWP 2397962)] [New Thread 0xffee49fbecf0 (LWP 2397963)] [New Thread 0xffee497aecf0 (LWP 2397964)] [New Thread 0xffee48f9ecf0 (LWP 2397965)] [New Thread 0xffee2bffecf0 (LWP 2397966)] [New Thread 0xffee2b7eecf0 (LWP 2397967)] [New Thread 0xffee2afdecf0 (LWP 2397968)] [New Thread 0xffee2a7cecf0 (LWP 2397969)] [New Thread 0xffee29fbecf0 (LWP 2397970)] [New Thread 0xffee297aecf0 (LWP 2397971)] [New Thread 0xffee28f9ecf0 (LWP 2397972)] [New Thread 0xffee0bffecf0 (LWP 2397973)] [New Thread 0xffee0b7eecf0 (LWP 2397974)] [New Thread 0xffee0afdecf0 (LWP 2397975)] [New Thread 0xffee0a7cecf0 (LWP 2397976)] [New Thread 0xffee09fbecf0 (LWP 2397977)] [New Thread 0xffee097aecf0 (LWP 2397978)] [New Thread 0xffee08f9ecf0 (LWP 2397979)] [New Thread 0xffedebffecf0 (LWP 2397980)] [New Thread 0xffedeb7eecf0 (LWP 2397981)] [New Thread 0xffedeafdecf0 (LWP 2397982)] [New Thread 0xffedea7cecf0 (LWP 2397983)] [New Thread 0xffede9fbecf0 (LWP 2397984)] [New Thread 0xffede97aecf0 (LWP 2397985)] [New Thread 0xffede8f9ecf0 (LWP 2397986)] [New Thread 0xffedcbffecf0 (LWP 2397987)] [New Thread 0xffedcb7eecf0 (LWP 2397988)] [New Thread 0xffedcafdecf0 (LWP 2397989)] [New Thread 0xffedca7cecf0 (LWP 2397990)] [New Thread 0xffedc9fbecf0 (LWP 2397991)] [New Thread 0xffedc97aecf0 (LWP 2397992)] [New Thread 0xffedc8f9ecf0 (LWP 2397993)] [New Thread 0xffedabffecf0 (LWP 2397994)] [New Thread 0xffedab7eecf0 (LWP 2397995)] [New Thread 0xffedaafdecf0 (LWP 2397996)] [New Thread 0xffedaa7cecf0 (LWP 2397997)] [New Thread 0xffeda9fbecf0 (LWP 2397998)] [New Thread 0xffeda97aecf0 (LWP 2397999)] [New Thread 0xffeda8f9ecf0 (LWP 2398000)] [New Thread 0xffed8bffecf0 (LWP 2398001)] [New Thread 0xffed8b7eecf0 (LWP 2398002)] [New Thread 0xffed8afdecf0 (LWP 2398003)] [New Thread 0xffed8a7cecf0 (LWP 2398004)] [New Thread 0xffed89fbecf0 (LWP 2398005)] [New Thread 0xffed897aecf0 (LWP 2398006)] [New Thread 0xffed88f9ecf0 (LWP 2398007)] [New Thread 0xffed6bffecf0 (LWP 2398008)] [New Thread 0xffed6b7eecf0 (LWP 2398009)] [New Thread 0xffed6afdecf0 (LWP 2398010)] [New Thread 0xffed6a7cecf0 (LWP 2398011)] [New Thread 0xffed69fbecf0 (LWP 2398012)] [New Thread 0xffed697aecf0 (LWP 2398013)] [New Thread 0xffed68f9ecf0 (LWP 2398014)] [New Thread 0xffed4bffecf0 (LWP 2398015)] [New Thread 0xffed4b7eecf0 (LWP 2398016)] [New Thread 0xffed4afdecf0 (LWP 2398017)] [New Thread 0xffed4a7cecf0 (LWP 2398018)] [New Thread 0xffed49fbecf0 (LWP 2398019)] [New Thread 0xffed497aecf0 (LWP 2398020)] [New Thread 0xffed48f9ecf0 (LWP 2398021)] [New Thread 0xffed2bffecf0 (LWP 2398022)] [New Thread 0xffed2b7eecf0 (LWP 2398023)] [New Thread 0xffed2afdecf0 (LWP 2398024)] [New Thread 0xffed2a7cecf0 (LWP 2398025)] [New Thread 0xffed29fbecf0 (LWP 2398026)] [New Thread 0xffed297aecf0 (LWP 2398027)] [New Thread 0xffed28f9ecf0 (LWP 2398028)] [New Thread 0xffed0bffecf0 (LWP 2398029)] 20220614 10:16:27.366195: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init > {0 > localhost:58347, 1 > localhost:33741, 2 > localhost:54957} [New Thread 0xffed0b7eecf0 (LWP 2398030)] [New Thread 0xffed0afdecf0 (LWP 2398031)] [New Thread 0xffed09fbecf0 (LWP 2398033)] [New Thread 0xffed0a7cecf0 (LWP 2398032)] [New Thread 0xffed08f9ecf0 (LWP 2398035)] [New Thread 0xffed097aecf0 (LWP 2398034)] 20220614 10:16:27.367787: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:33741 [New Thread 0xffecf3ffecf0 (LWP 2398036)] [New Thread 0xffecf37eecf0 (LWP 2398037)] [New Thread 0xffecf2fdecf0 (LWP 2398038)] [New Thread 0xffecf27cecf0 (LWP 2398039)] [New Thread 0xffecf1fbecf0 (LWP 2398040)] [New Thread 0xffecf17aecf0 (LWP 2398041)] [New Thread 0xffecf0f9ecf0 (LWP 2398042)] [New Thread 0xffecd7ffecf0 (LWP 2398043)] [New Thread 0xffecd77eecf0 (LWP 2398044)] [New Thread 0xffecd6fdecf0 (LWP 2398045)] [New Thread 0xffecd67cecf0 (LWP 2398046)] [New Thread 0xffecd5fbecf0 (LWP 2398047)] [New Thread 0xffecd57aecf0 (LWP 2398048)] [New Thread 0xffecd4f9ecf0 (LWP 2398049)] [New Thread 0xffecafffecf0 (LWP 2398050)] [New Thread 0xffecaf7eecf0 (LWP 2398051)] [New Thread 0xffecaefdecf0 (LWP 2398052)] [New Thread 0xffecae7cecf0 (LWP 2398053)] [New Thread 0xffecadfbecf0 (LWP 2398054)] [New Thread 0xffecad7aecf0 (LWP 2398055)] [New Thread 0xffecacf9ecf0 (LWP 2398056)] [New Thread 0xffec8fffecf0 (LWP 2398057)] [New Thread 0xffec8f7eecf0 (LWP 2398058)] [New Thread 0xffec8efdecf0 (LWP 2398059)] [New Thread 0xffec8e7cecf0 (LWP 2398060)] [New Thread 0xffec8dfbecf0 (LWP 2398061)] [New Thread 0xffec8d7aecf0 (LWP 2398062)] [New Thread 0xffec8cf9ecf0 (LWP 2398063)] [New Thread 0xffec6fffecf0 (LWP 2398064)] [New Thread 0xffec6f7eecf0 (LWP 2398065)] [New Thread 0xffec6efdecf0 (LWP 2398066)] [New Thread 0xffec6e7cecf0 (LWP 2398067)] [New Thread 0xffec6dfbecf0 (LWP 2398068)] [New Thread 0xffec6d7aecf0 (LWP 2398069)] [New Thread 0xffec6cf9ecf0 (LWP 2398070)] [New Thread 0xffec4fffecf0 (LWP 2398071)] [New Thread 0xffec4f7eecf0 (LWP 2398072)] [New Thread 0xffec4efdecf0 (LWP 2398073)] [New Thread 0xffec4e7cecf0 (LWP 2398074)] [New Thread 0xffec4dfbecf0 (LWP 2398075)] [New Thread 0xffec4d7aecf0 (LWP 2398076)] [New Thread 0xffec4cf9ecf0 (LWP 2398077)] [New Thread 0xffec2fffecf0 (LWP 2398078)] [New Thread 0xffec2f7eecf0 (LWP 2398079)] [New Thread 0xffec2efdecf0 (LWP 2398080)] [New Thread 0xffec2e7cecf0 (LWP 2398081)] [New Thread 0xffec2dfbecf0 (LWP 2398082)] [New Thread 0xffec2d7aecf0 (LWP 2398083)] [New Thread 0xffec2cf9ecf0 (LWP 2398084)] [New Thread 0xffec0fffecf0 (LWP 2398085)] [New Thread 0xffec0f7eecf0 (LWP 2398086)] [New Thread 0xffec0efdecf0 (LWP 2398087)] [New Thread 0xffec0e7cecf0 (LWP 2398088)] [New Thread 0xffec0dfbecf0 (LWP 2398089)] [New Thread 0xffec0d7aecf0 (LWP 2398090)] [New Thread 0xffec0cf9ecf0 (LWP 2398091)] [New Thread 0xffebefffecf0 (LWP 2398092)] [New Thread 0xffebef7eecf0 (LWP 2398093)] [New Thread 0xffebeefdecf0 (LWP 2398094)] [New Thread 0xffebee7cecf0 (LWP 2398095)] [New Thread 0xffebedfbecf0 (LWP 2398096)] [New Thread 0xffebed7aecf0 (LWP 2398097)] [New Thread 0xffebecf9ecf0 (LWP 2398098)] [New Thread 0xffebcfffecf0 (LWP 2398099)] [New Thread 0xffebcf7eecf0 (LWP 2398100)] [New Thread 0xffebcefdecf0 (LWP 2398101)] [New Thread 0xffebce7cecf0 (LWP 2398102)] [New Thread 0xffebcdfbecf0 (LWP 2398103)] [New Thread 0xffebcd7aecf0 (LWP 2398104)] [New Thread 0xffebccf9ecf0 (LWP 2398105)] [New Thread 0xffebafffecf0 (LWP 2398106)] [New Thread 0xffebaf7eecf0 (LWP 2398107)] [New Thread 0xffebaefdecf0 (LWP 2398108)] [New Thread 0xffebae7cecf0 (LWP 2398109)] [New Thread 0xffebadfbecf0 (LWP 2398110)] [New Thread 0xffebad7aecf0 (LWP 2398111)] [New Thread 0xffebacf9ecf0 (LWP 2398112)] [New Thread 0xffeb8fffecf0 (LWP 2398113)] [New Thread 0xffeb8f7eecf0 (LWP 2398114)] [New Thread 0xffeb8efdecf0 (LWP 2398115)] [New Thread 0xffeb8e7cecf0 (LWP 2398116)] [New Thread 0xffeb8dfbecf0 (LWP 2398117)] [New Thread 0xffeb8d7aecf0 (LWP 2398118)] [New Thread 0xffeb8cf9ecf0 (LWP 2398119)] [New Thread 0xffeb6fffecf0 (LWP 2398120)] [New Thread 0xffeb6f7eecf0 (LWP 2398121)] [New Thread 0xffeb6efdecf0 (LWP 2398122)] [New Thread 0xffeb6e7cecf0 (LWP 2398123)] [New Thread 0xffeb6dfbecf0 (LWP 2398124)] [New Thread 0xffeb6d7aecf0 (LWP 2398125)] [New Thread 0xffeb6cf9ecf0 (LWP 2398126)] [New Thread 0xffeb4fffecf0 (LWP 2398127)] [New Thread 0xffeb4f7eecf0 (LWP 2398128)] [New Thread 0xffeb4efdecf0 (LWP 2398129)] [New Thread 0xffeb4e7cecf0 (LWP 2398130)] [New Thread 0xffeb4dfbecf0 (LWP 2398131)] [New Thread 0xffeb4d7aecf0 (LWP 2398132)] [New Thread 0xffeb4cf9ecf0 (LWP 2398133)] [New Thread 0xffeb2fffecf0 (LWP 2398134)] [New Thread 0xffeb2f7eecf0 (LWP 2398135)] [New Thread 0xffeb2efdecf0 (LWP 2398136)] [New Thread 0xffeb2e7cecf0 (LWP 2398137)] [New Thread 0xffeb2dfbecf0 (LWP 2398138)] [New Thread 0xffeb2d7aecf0 (LWP 2398139)] [New Thread 0xffeb2cf9ecf0 (LWP 2398140)] [New Thread 0xffeb0fffecf0 (LWP 2398141)] [New Thread 0xffeb0f7eecf0 (LWP 2398142)] [New Thread 0xffeb0efdecf0 (LWP 2398143)] [New Thread 0xffeb0e7cecf0 (LWP 2398144)] [New Thread 0xffeb0dfbecf0 (LWP 2398145)] [New Thread 0xffeb0d7aecf0 (LWP 2398146)] [New Thread 0xffeb0cf9ecf0 (LWP 2398147)] [New Thread 0xffeaefffecf0 (LWP 2398148)] [New Thread 0xffeaef7eecf0 (LWP 2398149)] [New Thread 0xffeaeefdecf0 (LWP 2398150)] [New Thread 0xffeaee7cecf0 (LWP 2398151)] [New Thread 0xffeaedfbecf0 (LWP 2398152)] [New Thread 0xffeaed7aecf0 (LWP 2398153)] [New Thread 0xffeaecf9ecf0 (LWP 2398154)] [New Thread 0xffeacfffecf0 (LWP 2398155)] [New Thread 0xffeacf7eecf0 (LWP 2398156)] [New Thread 0xffeacefdecf0 (LWP 2398157)] [New Thread 0xffeace7cecf0 (LWP 2398158)] [New Thread 0xffeacdfbecf0 (LWP 2398159)] [New Thread 0xffeacd7aecf0 (LWP 2398160)] [New Thread 0xffeaccf9ecf0 (LWP 2398161)] [New Thread 0xffeaafffecf0 (LWP 2398162)] [New Thread 0xffeaaf7eecf0 (LWP 2398163)] [New Thread 0xffeaaefdecf0 (LWP 2398164)] [New Thread 0xffeaae7cecf0 (LWP 2398165)] [New Thread 0xffeaadfbecf0 (LWP 2398166)] [New Thread 0xffeaad7aecf0 (LWP 2398167)] [New Thread 0xffeaacf9ecf0 (LWP 2398168)] [New Thread 0xffea8fffecf0 (LWP 2398169)] [New Thread 0xffea8f7eecf0 (LWP 2398170)] [New Thread 0xffea8efdecf0 (LWP 2398171)] [New Thread 0xffea8e7cecf0 (LWP 2398172)] [New Thread 0xffea8dfbecf0 (LWP 2398173)] [New Thread 0xffea8d7aecf0 (LWP 2398174)] [New Thread 0xffea8cf9ecf0 (LWP 2398175)] [New Thread 0xffea6fffecf0 (LWP 2398176)] [New Thread 0xffea6f7eecf0 (LWP 2398177)] [New Thread 0xffea6efdecf0 (LWP 2398178)] [New Thread 0xffea6e7cecf0 (LWP 2398179)] [New Thread 0xffea6dfbecf0 (LWP 2398180)] [New Thread 0xffea6d7aecf0 (LWP 2398181)] [New Thread 0xffea6cf9ecf0 (LWP 2398182)] [New Thread 0xffea4fffecf0 (LWP 2398183)] [New Thread 0xffea4f7eecf0 (LWP 2398184)] [New Thread 0xffea4efdecf0 (LWP 2398185)] [New Thread 0xffea4e7cecf0 (LWP 2398186)] [New Thread 0xffea4dfbecf0 (LWP 2398187)] [New Thread 0xffea4d7aecf0 (LWP 2398188)] [New Thread 0xffea4cf9ecf0 (LWP 2398189)] [New Thread 0xffea2fffecf0 (LWP 2398190)] [New Thread 0xffea2f7eecf0 (LWP 2398191)] [New Thread 0xffea2efdecf0 (LWP 2398192)] [New Thread 0xffea2e7cecf0 (LWP 2398193)] [New Thread 0xffea2dfbecf0 (LWP 2398194)] [New Thread 0xffea2d7aecf0 (LWP 2398195)] [New Thread 0xffea2cf9ecf0 (LWP 2398196)] [New Thread 0xffea0fffecf0 (LWP 2398197)] [New Thread 0xffea0f7eecf0 (LWP 2398198)] [New Thread 0xffea0efdecf0 (LWP 2398199)] [New Thread 0xffea0e7cecf0 (LWP 2398200)] [New Thread 0xffea0dfbecf0 (LWP 2398201)] [New Thread 0xffea0d7aecf0 (LWP 2398202)] [New Thread 0xffea0cf9ecf0 (LWP 2398203)] [New Thread 0xffe9efffecf0 (LWP 2398204)] [New Thread 0xffe9ef7eecf0 (LWP 2398205)] [New Thread 0xffe9eefdecf0 (LWP 2398206)] [New Thread 0xffe9ee7cecf0 (LWP 2398207)] [New Thread 0xffe9edfbecf0 (LWP 2398208)] [New Thread 0xffe9ed7aecf0 (LWP 2398209)] [New Thread 0xffe9ecf9ecf0 (LWP 2398210)] [New Thread 0xffe9cfffecf0 (LWP 2398211)] [New Thread 0xffe9cf7eecf0 (LWP 2398212)] [New Thread 0xffe9cefdecf0 (LWP 2398213)] [New Thread 0xffe9ce7cecf0 (LWP 2398214)] [New Thread 0xffe9cdfbecf0 (LWP 2398215)] [New Thread 0xffe9cd7aecf0 (LWP 2398216)] [New Thread 0xffe9ccf9ecf0 (LWP 2398217)] [New Thread 0xffe9afffecf0 (LWP 2398218)] [New Thread 0xffe9af7eecf0 (LWP 2398219)] [New Thread 0xffe9aefdecf0 (LWP 2398220)] [New Thread 0xffe9ae7cecf0 (LWP 2398221)] [New Thread 0xffe9adfbecf0 (LWP 2398222)] [New Thread 0xffe9ad7aecf0 (LWP 2398223)] [New Thread 0xffe9acf9ecf0 (LWP 2398224)] [New Thread 0xffe98fffecf0 (LWP 2398225)] [New Thread 0xffe98f7eecf0 (LWP 2398226)] [New Thread 0xffe98efdecf0 (LWP 2398227)] [New Thread 0xffe98e7cecf0 (LWP 2398228)] [New Thread 0xffe98dfbecf0 (LWP 2398229)] [New Thread 0xffe98d7aecf0 (LWP 2398230)] [New Thread 0xffe98cf9ecf0 (LWP 2398231)] [New Thread 0xffe96fffecf0 (LWP 2398232)] [New Thread 0xffe96f7eecf0 (LWP 2398233)] [New Thread 0xffe96efdecf0 (LWP 2398234)] [New Thread 0xffe96e7cecf0 (LWP 2398235)] [New Thread 0xffe96dfbecf0 (LWP 2398236)] [New Thread 0xffe96d7aecf0 (LWP 2398237)] [New Thread 0xffe96cf9ecf0 (LWP 2398238)] [New Thread 0xffe94fffecf0 (LWP 2398239)] [New Thread 0xffe94f7eecf0 (LWP 2398240)] [New Thread 0xffe94efdecf0 (LWP 2398241)] [New Thread 0xffe94e7cecf0 (LWP 2398242)] [New Thread 0xffe94dfbecf0 (LWP 2398243)] [New Thread 0xffe94d7aecf0 (LWP 2398244)] [New Thread 0xffe94cf9ecf0 (LWP 2398245)] [New Thread 0xffe92fffecf0 (LWP 2398246)] [New Thread 0xffe92f7eecf0 (LWP 2398247)] [New Thread 0xffe92efdecf0 (LWP 2398248)] [New Thread 0xffe92e7cecf0 (LWP 2398249)] [New Thread 0xffe92dfbecf0 (LWP 2398250)] [New Thread 0xffe92d7aecf0 (LWP 2398251)] [New Thread 0xffe92cf9ecf0 (LWP 2398252)] [New Thread 0xffe90fffecf0 (LWP 2398253)] [New Thread 0xffe90f7eecf0 (LWP 2398254)] [New Thread 0xffe90efdecf0 (LWP 2398255)] [New Thread 0xffe90e7cecf0 (LWP 2398256)] [New Thread 0xffe90dfbecf0 (LWP 2398257)] [New Thread 0xffe90d7aecf0 (LWP 2398258)] [New Thread 0xffe90cf9ecf0 (LWP 2398259)] [New Thread 0xffe8efffecf0 (LWP 2398260)] [New Thread 0xffe8ef7eecf0 (LWP 2398261)] [New Thread 0xffe8eefdecf0 (LWP 2398262)] [New Thread 0xffe8ee7cecf0 (LWP 2398263)] [New Thread 0xffe8edfbecf0 (LWP 2398264)] [New Thread 0xffe8ed7aecf0 (LWP 2398265)] [New Thread 0xffe8ecf9ecf0 (LWP 2398266)] [New Thread 0xffe8cfffecf0 (LWP 2398267)] [New Thread 0xffe8cf7eecf0 (LWP 2398268)] [New Thread 0xffe8cefdecf0 (LWP 2398269)] [New Thread 0xffe8ce7cecf0 (LWP 2398270)] [New Thread 0xffe8cdfbecf0 (LWP 2398271)] [New Thread 0xffe8cd7aecf0 (LWP 2398272)] [New Thread 0xffe8ccf9ecf0 (LWP 2398273)] [New Thread 0xffe8afffecf0 (LWP 2398274)] [New Thread 0xffe8af7eecf0 (LWP 2398275)] [New Thread 0xffe8aefdecf0 (LWP 2398276)] [New Thread 0xffe8ae7cecf0 (LWP 2398277)] [New Thread 0xffe8adfbecf0 (LWP 2398278)] [New Thread 0xffe8ad7aecf0 (LWP 2398279)] [New Thread 0xffe8acf9ecf0 (LWP 2398280)] [New Thread 0xffe88fffecf0 (LWP 2398281)] [New Thread 0xffe88f7eecf0 (LWP 2398282)] [New Thread 0xffe88efdecf0 (LWP 2398283)] [New Thread 0xffe88e7cecf0 (LWP 2398284)] [New Thread 0xffe88dfbecf0 (LWP 2398285)] [New Thread 0xffe88d7aecf0 (LWP 2398286)] [New Thread 0xffe88cf9ecf0 (LWP 2398287)] [New Thread 0xffe86fffecf0 (LWP 2398288)] [New Thread 0xffe86f7eecf0 (LWP 2398289)] [New Thread 0xffe86efdecf0 (LWP 2398290)] [New Thread 0xffe86e7cecf0 (LWP 2398291)] [New Thread 0xffe86dfbecf0 (LWP 2398292)] [New Thread 0xffe86d7aecf0 (LWP 2398293)] [New Thread 0xffe86cf9ecf0 (LWP 2398294)] [New Thread 0xffe84fffecf0 (LWP 2398295)] [New Thread 0xffe84f7eecf0 (LWP 2398296)] [New Thread 0xffe84efdecf0 (LWP 2398297)] [New Thread 0xffe84e7cecf0 (LWP 2398298)] [New Thread 0xffe84dfbecf0 (LWP 2398299)] [New Thread 0xffe84d7aecf0 (LWP 2398300)] [New Thread 0xffe84cf9ecf0 (LWP 2398301)] [New Thread 0xffe82fffecf0 (LWP 2398302)] [New Thread 0xffe82f7eecf0 (LWP 2398303)] [New Thread 0xffe82efdecf0 (LWP 2398304)] [New Thread 0xffe82e7cecf0 (LWP 2398305)] [New Thread 0xffe82dfbecf0 (LWP 2398306)] [New Thread 0xffe82d7aecf0 (LWP 2398307)] [New Thread 0xffe82cf9ecf0 (LWP 2398308)] [New Thread 0xffe80fffecf0 (LWP 2398309)] [New Thread 0xffe80f7eecf0 (LWP 2398310)] [New Thread 0xffe80efdecf0 (LWP 2398311)] [New Thread 0xffe80e7cecf0 (LWP 2398312)] [New Thread 0xffe80dfbecf0 (LWP 2398313)] [New Thread 0xffe80d7aecf0 (LWP 2398314)] [New Thread 0xffe80cf9ecf0 (LWP 2398315)] [New Thread 0xffe7efffecf0 (LWP 2398316)] [New Thread 0xffe7ef7eecf0 (LWP 2398317)] [New Thread 0xffe7eefdecf0 (LWP 2398318)] [New Thread 0xffe7ee7cecf0 (LWP 2398319)] [New Thread 0xffe7edfbecf0 (LWP 2398320)] [New Thread 0xffe7ed7aecf0 (LWP 2398321)] [New Thread 0xffe7ecf9ecf0 (LWP 2398322)] [New Thread 0xffe7cfffecf0 (LWP 2398323)] [New Thread 0xffe7cf7eecf0 (LWP 2398324)] [New Thread 0xffe7cefdecf0 (LWP 2398325)] [New Thread 0xffe7ce7cecf0 (LWP 2398326)] [New Thread 0xffe7cdfbecf0 (LWP 2398327)] [New Thread 0xffe7cd7aecf0 (LWP 2398328)] [New Thread 0xffe7ccf9ecf0 (LWP 2398329)] [New Thread 0xffe7afffecf0 (LWP 2398330)] [New Thread 0xffe7af7eecf0 (LWP 2398331)] [New Thread 0xffe7aefdecf0 (LWP 2398332)] 20220614 10:16:27.477869: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init > {0 > localhost:58347, 1 > localhost:33741, 2 > localhost:54957} [New Thread 0xffe7ae7cecf0 (LWP 2398333)] [New Thread 0xffe7adfbecf0 (LWP 2398334)] [New Thread 0xffe7acf9ecf0 (LWP 2398336)] [New Thread 0xffe7ad7aecf0 (LWP 2398335)] [New Thread 0xffe78f7eecf0 (LWP 2398338)] [New Thread 0xffe78fffecf0 (LWP 2398337)] 20220614 10:16:27.479662: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:54957 [New Thread 0xffe787ffecf0 (LWP 2398339)] [New Thread 0xffe78efdecf0 (LWP 2398340)] [New Thread 0xffe78e7cecf0 (LWP 2398341)] [New Thread 0xffe78dfbecf0 (LWP 2398342)] [New Thread 0xffe78d7aecf0 (LWP 2398343)] [New Thread 0xffe78cf9ecf0 (LWP 2398344)] [New Thread 0xffe7877eecf0 (LWP 2398345)] [New Thread 0xffe786fdecf0 (LWP 2398346)] [Switching to Thread 0xffe786fdecf0 (LWP 2398346)] Thread 1363 ""c_api_session_c"" hit Breakpoint 1, 0x0000ffffa680cc6c in re2::RE2::Replace(std::string*, re2::RE2 const&, std::basic_string_view >) () from /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazelout/aarch64opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/libtensorflow_Score_Sdistributed_Uruntime_Slibmaster.so (gdb) c Continuing. Thread 1363 ""c_api_session_c"" hit Breakpoint 1, re2::RE2::Replace (str=str=0xffe786fdde80, re=..., rewrite=...) at external/com_googlesource_code_re2/re2/re2.cc:408 408	                  absl::string_view rewrite) { (gdb) n 409	  absl::string_view vec[kVecSize]; (gdb)  410	  int nvec = 1 + MaxSubmatch(rewrite); (gdb)  411	  if (nvec > 1 + re.NumberOfCapturingGroups()) (gdb)  413	  if (nvec > static_cast(ABSL_ARRAYSIZE(vec))) (gdb)  415	  if (!re.Match(*str, 0, str>size(), UNANCHORED, vec, nvec)) (gdb)  133	      basic_string_view(const _CharT* __str, size_type __len) noexcept (gdb)  418	  std::string s; (gdb)  419	  if (!re.Rewrite(&s, rewrite, vec, nvec)) (gdb)  424	  str>replace(vec[0].data()  str>data(), vec[0].size(), s); (gdb) p vec[0] $1 = {static npos = , _M_len = 7, _M_str = 0x68da6b8 ""grpc://localhost:58347""} (gdb) p *str $2 = {static npos = 18446744073709551615,    _M_dataplus = {> = {> = {}, },      _M_p = 0x68da6b8 ""grpc://localhost:58347""}} (gdb) s std::basic_string_view >::data (this=0xffe786fddca0)     at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/string_view:239 239	      data() const noexcept (gdb)  re2::RE2::Replace (str=str=0xffe786fdde80, re=..., rewrite=...) at external/com_googlesource_code_re2/re2/re2.cc:424 424	  str>replace(vec[0].data()  str>data(), vec[0].size(), s); (gdb)  std::string::data (this=0xffe786fdde80)     at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:5232 5232		_M_leak(); (gdb)  std::string::_M_leak (this=0xffe786fdde80)     at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:3384 3384		if (!_M_rep()>_M_is_leaked()) (gdb)  std::string::_M_rep (this=0xffe786fdde80)     at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:3369 3369	      { return &((reinterpret_cast (_M_data()))[1]); } (gdb)  std::string::_M_data (this=0xffe786fdde80)     at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:3360 3360	      _M_data() const _GLIBCXX_NOEXCEPT (gdb)  std::string::_M_leak (this=0xffe786fdde80) at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:3384 3384		if (!_M_rep()>_M_is_leaked()) (gdb)  std::string::_Rep::_M_is_leaked (this=0x68da6a0)     at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:3230 3230		_M_is_leaked() const _GLIBCXX_NOEXCEPT (gdb)  std::string::data (this=0xffe786fdde80) at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:5233 5233		return _M_data(); (gdb)  std::string::_M_data (this=0xffe786fdde80)     at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:3360 3360	      _M_data() const _GLIBCXX_NOEXCEPT (gdb)  std::string::replace (__str=..., __n=7, __pos=18446462704539241408, this=0xffe786fdde80) at /opt/rh/devtoolset10/root/usr/lib/gcc/aarch64redhatlinux/10/../../../../include/c++/10/bits/basic_string.h:3913 3913	      size() const _GLIBCXX_NOEXCEPT (gdb)  terminate called after throwing an instance of 'std::out_of_range'   what():  basic_string::replace Thread 1363 ""c_api_session_c"" received signal SIGABRT, Aborted. 0x0000ffff9d3a5238 in raise () from /lib64/libc.so.6 `",Also affects //tensorflow/python/client:session_list_devices_test //tensorflow/python/data/kernel_tests:iterator_test_cpu //tensorflow/python/training:server_lib_test,manylinux2014_aarch64 is no longer used for testing,Are you satisfied with the resolution of your issue? Yes No
1476,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Warnings (or any stderr writes) cause Python configuration to fail)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.5.0 (and many others)  Custom Code No  OS Platform and Distribution Ubuntu 22.04  Mobile device N/A  Python version 3.10  Bazel version 4.2.2  GCC/Compiler version 11.2.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? The current definition of the `execute()` helper for TensorFlow treats any writes to `stderr` as a fatal error. This causes CC(Locating the numpy include path can fail incorrectly), and can also cause the `third_party/py/python_configure.bzl` helpers `_get_python_include(), _get_python_import_lib_name()` to fail if the Python runtime issues any warnings, which is not an uncommon event. The `Dockerfile` attached to this issue illustrates a downstream failure caused by a warning issued in Python 3.10 because of the deprecation of `distutils` (PEP 632) I have observed this behavior in 2.5.0, but this failure mode should be common to most extant versions of TensorFlow.  Standalone code to reproduce the issue Sorry about the state of this reproduction, I did try to make a simpler one that depended only on the `tensorflow` repo but couldn't whittle it down any further.   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,SnoopJ,Warnings (or any stderr writes) cause Python configuration to fail,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version v2.5.0 (and many others)  Custom Code No  OS Platform and Distribution Ubuntu 22.04  Mobile device N/A  Python version 3.10  Bazel version 4.2.2  GCC/Compiler version 11.2.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour? The current definition of the `execute()` helper for TensorFlow treats any writes to `stderr` as a fatal error. This causes CC(Locating the numpy include path can fail incorrectly), and can also cause the `third_party/py/python_configure.bzl` helpers `_get_python_include(), _get_python_import_lib_name()` to fail if the Python runtime issues any warnings, which is not an uncommon event. The `Dockerfile` attached to this issue illustrates a downstream failure caused by a warning issued in Python 3.10 because of the deprecation of `distutils` (PEP 632) I have observed this behavior in 2.5.0, but this failure mode should be common to most extant versions of TensorFlow.  Standalone code to reproduce the issue Sorry about the state of this reproduction, I did try to make a simpler one that depended only on the `tensorflow` repo but couldn't whittle it down any further.   Relevant log output  ",2022-06-08T06:38:39Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.5,closed,0,11,https://github.com/tensorflow/tensorflow/issues/56399,"Do any of the maintainers know why `execute()` treats `stderr` writes as failures of the invoked process? I tried to follow the blame back as far as I could, but it looks like this function's history extends longer than the record visible in this repository, and there has never been a rationale given in the source code for this behavior. It seems like the `execute()` helper should probably use the `return_code` of the process to work out if the invoked process succeeded. If there isn't a specific reason to ignore the return code and focus on the content of the streams, I am willing to write a PR for the necessary changes.","Alternatively, here is a patch that fixes **only** the failure reported in this issue, leaving the fairly antisocial behavior of `execute()` alone and fixing only some of the places that Python warnings can cause failures (I'm sure there are others): ", Could you refer to the tested build configurations and try with the latest TF version 2.9.0. Please let us know if it helps? Thank you!,"I tried to update the example `Dockerfile` to 2.9.0 but wasn't successful. I will try to make another reproduction with 2.9.0 if I get the chance. I am however quite confident that this issue is present in 2.9.0 and the current `HEAD` of the repository, because the issue is that the `execute()` function treats stderr writes as fatal errors, and this is going to cause a lot of artificial failures with misleading error messages. More specifically to the reported reproduction, any project that uses Python 3.10, TensorFlow 2.9.0 and `distutils` is going to cause the `DeprecationWarning` that is causing the Python configuration step to fail.","Hi  , Could you please confirm if this is still an issue ? I have gone through the issue and I couldn't find reproducible code to check this issue.I am not quite follow through the issue and request you to check whether it is still same issue and if so request you to confirm and discuss. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"> Hi  , >  > Could you please confirm if this is still an issue ? I have gone through the issue and I couldn't find reproducible code to check this issue.I am not quite follow through the issue and request you to check whether it is still same issue and if so request you to confirm and discuss. Thanks! The code responsible for this bug has not changed since I filed my report, this bug is still present in TensorFlow's build system. To reproduce the bug, any build that includes a write to `stderr` from something called by the `execute()` function will trigger the bug. I am afraid I do not have time to put more effort into updating the reproducer I attached to this issue, but a good way to do that would be to add an extra call to `execute()` that echos something to `stderr`, which should be enough to trigger the faulty behavior.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
717,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(put two models in another model, the two models variable names aren't under the another model name scope)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.5.0  Custom Code Yes  OS Platform and Distribution mac os/ linux  Mobile device _No response_  Python version 3.8.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,xiongma,"put two models in another model, the two models variable names aren't under the another model name scope",Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.5.0  Custom Code Yes  OS Platform and Distribution mac os/ linux  Mobile device _No response_  Python version 3.8.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output _No response_,2022-06-08T03:00:50Z,type:bug comp:keras TF 2.5,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56398,variable names in CLIP model as follow. !image,"such as first layer in the picture, the layer name should be `clip123/text_transformer/embeddings/word_embeddings`",Are you satisfied with the resolution of your issue? Yes No,this scenario happen when you put some weights initial in build funtion
815,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(No gradients provided for any variable)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.4  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? distilbertbaseuncased.txt Loss is provided and I get an error. To make it work, modify the commented line and replace `return loss` with `return 'mse'` and it should work fine.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,ghost,No gradients provided for any variable,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.4  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? distilbertbaseuncased.txt Loss is provided and I get an error. To make it work, modify the commented line and replace `return loss` with `return 'mse'` and it should work fine.  Standalone code to reproduce the issue   Relevant log output  ",2022-06-08T02:07:21Z,stat:awaiting response type:bug comp:keras TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56396, Could you please confirm if it is a duplicate ticket of 16663. Thank you!,I created 2 issues in case it's irrelevant somewhere, Could you please move this issue to closed status as we will track this issue there in kerasteam/keras repo ? Thank you!,"Thanks, I will. For the time being, are there any known workarounds?",Are you satisfied with the resolution of your issue? Yes No
632,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.data.experimental.load() no longer working)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory NVIDIA Quadro T2000  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lcs-crr,tf.data.experimental.load() no longer working,Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory NVIDIA Quadro T2000  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-07T14:25:08Z,stat:awaiting response type:support comp:data TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56391,Hi  ! Could you pass the element spec of the dataset instead of None and let us know (also suggests to remove read mode and provide path only). Attached relevant thread for reference. Thank you!,"Hi, thanks for getting back to me so quickly. I've obtained the element spect through `tf_train.element_spec` which output the following:  How do I specify this in the load function? Just copy as is like below?  If I try the snippet above it tells me TensorSpec is not defined, hence I believe I've got the wrong syntax... What's the right way? EDIT: Also, trying to removing read mode and providing path only as shown:  leads to the same error as before.", ! It should be tf.TensorSpec instead of TensorSpec. Please let us know if it works.  ,Hey! I've tried specifying the tensorspec with read mode   and without read mode  but in both cases a new different error is shown:  Hope this helps?,Ok  !  Could you check this thread  with a similar error stack trace . Please provide a minimal stand alone code with dataset for further assistance. Thank you!,"I managed to fix it. I'm really embarassed but the problem was that I saved the tf.data objects with a typo in the name, hence the load function could not fetch the data.",Are you satisfied with the resolution of your issue? Yes No, ! Thanks for the update. 
620,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dataset Pipeline determinism when reload, using `batch` `prefetch`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows 11  Mobile device No  Python version 3.10.4  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version 11.7/8.4  GPU model and memory RTX3090 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Zhaopudark,"Dataset Pipeline determinism when reload, using `batch` `prefetch`",Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows 11  Mobile device No  Python version 3.10.4  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version 11.7/8.4  GPU model and memory RTX3090 24GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-07T12:33:53Z,stat:awaiting tensorflower type:bug comp:data TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56385," I was able to replicate the issue on colab, please find the gist here. Thank you!","What is the purpose of defining a separate `dataset_B` instead of just applying `.map(mapfunc)` to `dataset_A`?  Creating and checkpointing a single dataset/iterator should fix the issue. Right now you're creating two iterators (one for `dataset_A` and one for `dataset_B`) but only checkpointing the iterator for `dataset_A`. Even if you checkpointed both iterators, there would still be a race condition between checkpointing the iterators and transferring data from one iterator to the other.","  Because I need to use `tf.py_function`  in .map(mapfunc) and save an  `dataset`'s iter state, as the sample codes above. But its can not be saved, since dataset guidline.  So,  cascading `dataset` (saving previous `daraset` and putting `.map(mapfunc)` in posterior  `daraset` ) is the only way that I can figure out within my ability. But the problem is, it works only `tf.config.experimental.enable_op_determinism()` is used as precondition.ğŸ˜Š","Thanks!      I find an easier way to traceback dataset from checkpoint and without cascade 2 tf.data.dataset. After all, my needs and problems are too rare.   Any way, thank you very much. ",Are you satisfied with the resolution of your issue? Yes No
1853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Locating the numpy include path can fail incorrectly)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version Many versions. Affects 2.9.1 and HEAD (as of ee96662)  Custom Code Yes  OS Platform and Distribution Most. Reproduced on Ubuntu 20.04  Mobile device N/A  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  NOTE: This is a refiling of CC(Locating the numpy include path can ""fail"" incorrectly) to use the issue template and add a complete reproduction `_get_numpy_include()` will fail if importing `numpy` causes _anything_ to be written to stderr, even if the command succeeds. I found this while helping a user in python on the Libera.chat IRC network debug a failure when building `yggdrasildecisionforests`. When they import `numpy` on their system, they get the following nonfatal warning:  Which causes `execute()` to fail. It looks like the `allow_failure` parameter for that function is meant to address this, so adding this argument should fix the issue in this specific case, but I'm not sure what the other implications might be. (as an aside, this parameter is confusingly documented, I have opened CC(Change allow_failure description to match its functionality) to address that) Insight from someone who is more familiar with TF's build system would be appreciated, but inspecting stdout/stderr strikes me as a very strange way to work out whether or not the executed process failed. Is there a reason this bit of code does not check the `return_code` of the raw execution?  Standalone code to reproduce the issue Below is a `Docker)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SnoopJ,Locating the numpy include path can fail incorrectly,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version Many versions. Affects 2.9.1 and HEAD (as of ee96662)  Custom Code Yes  OS Platform and Distribution Most. Reproduced on Ubuntu 20.04  Mobile device N/A  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?  NOTE: This is a refiling of CC(Locating the numpy include path can ""fail"" incorrectly) to use the issue template and add a complete reproduction `_get_numpy_include()` will fail if importing `numpy` causes _anything_ to be written to stderr, even if the command succeeds. I found this while helping a user in python on the Libera.chat IRC network debug a failure when building `yggdrasildecisionforests`. When they import `numpy` on their system, they get the following nonfatal warning:  Which causes `execute()` to fail. It looks like the `allow_failure` parameter for that function is meant to address this, so adding this argument should fix the issue in this specific case, but I'm not sure what the other implications might be. (as an aside, this parameter is confusingly documented, I have opened CC(Change allow_failure description to match its functionality) to address that) Insight from someone who is more familiar with TF's build system would be appreciated, but inspecting stdout/stderr strikes me as a very strange way to work out whether or not the executed process failed. Is there a reason this bit of code does not check the `return_code` of the raw execution?  Standalone code to reproduce the issue Below is a `Docker",2022-06-06T17:55:03Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56373, Thank you for reporting this issue! It will be closed once the PR is merged. Thank you!,"As described in CC(Warnings (or any stderr writes) cause Python configuration to fail), this failure goes beyond the numpyspecific case.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
981,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.saved_model.save() excessive function tracing)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3,8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  Output with tensorflow 2.3.2:  Output with tensorflow 2.4.4:  Why are the functions traced a second time with TF>=2.4 ? TF2.3 colab: https://colab.research.google.com/drive/19iniJKEcFmhkNDVe6sbseOyASWb3oCYC?usp=sharing TF2.4 colab:  https://colab.research.google.com/drive/1NVCSiSednJnnZqk_s1VmOsIMSUluaLP7?usp=sharing  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",large language model,thierryherrmann,tf.saved_model.save() excessive function tracing,"Click to expand!    Issue Type Performance  Source binary  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3,8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  Output with tensorflow 2.3.2:  Output with tensorflow 2.4.4:  Why are the functions traced a second time with TF>=2.4 ? TF2.3 colab: https://colab.research.google.com/drive/19iniJKEcFmhkNDVe6sbseOyASWb3oCYC?usp=sharing TF2.4 colab:  https://colab.research.google.com/drive/1NVCSiSednJnnZqk_s1VmOsIMSUluaLP7?usp=sharing  Relevant log output _No response_",2022-06-06T13:46:52Z,stat:awaiting response stale type:performance comp:tf.function TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56370," tf.function has a mechanism, that lets you control this function tracing, by defining a method __tf_trace_type__. Please have a look at this link for more details and let us know if it helps? Thank you!","Thanks but unfortunately that link only covers retracing when calling the function several times with different arguments. In this issue, tracing (with TF2.4) is done several times for a single invocation (or by calling `pretty_printed_concrete_signatures()` or `get_concrete_function()` like in the colabs). Things become weirder when we add a `Embedding` layer in the custom model. This is shown by this simple code, pastable in any colab:  Output with TF2.3.2:  Output with TF2.4.4 (same with TF2.8.x): ","  I was able to replicate the issue on colab, please find the gist here for reference. Thank you!"," , Could you try to save  the model using `model.save` instead of `saved_model.save`  and load it using `tf.keras.models.load_model`","Thanks for the suggestion and that indeed solves the extra tracings but we have a `tf.Module` (containing a keras model), not a keras Model. Moving all our custom `tf.Module` code into the Model subclass is not impossible but would be a big amount of work. And I had mixed success with signatures with `tf.keras.models.Model.save(..., signatures=...)`. Thanks for suggesting solutions, this issue's question is only why extra tracings (with `tf.saved_model.save()`) appeared when going from TF2.3 to TF2.4.","ww , Any inputs on the above comment. Thanks!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1436,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TensorFlow Lite label_image]  Abort occurs with xnnpack_delegate option.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version v2.9.1 (or master)  Custom Code No  OS Platform and Distribution Raspberry Pi OS (bullseye)  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version gcc version 10.2.1 20210110 (Debian 10.2.16)   CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell Clone repository. git clone b v2.9.1 https://github.com/tensorflow/tensorflow.git  mkdir build && cd build cmake ../tensorflow/tensorflow/lite/ cmake build . j$(nproc) cmake build . j$(nproc) t label_image  wget https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz tar xf mobilenet_v1_1.0_224.tgz wget  https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz tar xf mobilenet_v1_1.0_224_frozen.tgz cp mobilenet_v1_1.0_224/labels.txt ./  ./examples/label_image/label_image \   tflite_model ./mobilenet_v1_1.0_224.tflite \   labels ./labels.txt \   image ../tensorflow/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,NobuoTsukamoto,[TensorFlow Lite label_image]  Abort occurs with xnnpack_delegate option.,Click to expand!    Issue Type Bug  Source source  Tensorflow Version v2.9.1 (or master)  Custom Code No  OS Platform and Distribution Raspberry Pi OS (bullseye)  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version gcc version 10.2.1 20210110 (Debian 10.2.16)   CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  shell Clone repository. git clone b v2.9.1 https://github.com/tensorflow/tensorflow.git  mkdir build && cd build cmake ../tensorflow/tensorflow/lite/ cmake build . j$(nproc) cmake build . j$(nproc) t label_image  wget https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz tar xf mobilenet_v1_1.0_224.tgz wget  https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz tar xf mobilenet_v1_1.0_224_frozen.tgz cp mobilenet_v1_1.0_224/labels.txt ./  ./examples/label_image/label_image \   tflite_model ./mobilenet_v1_1.0_224.tflite \   labels ./labels.txt \   image ../tensorflow/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp   Relevant log output _No response_,2022-06-06T12:16:36Z,type:bug comp:lite-examples TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56367,"Abort is caused by the following code in label_image. https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/lite/examples/label_image/label_image.ccL129 The correct type of num_threads is i`nt32_t`, not `bool`. https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/lite/tools/delegates/default_execution_provider.ccL30 This error occurs on Raspberry Pi OS 64bit (bullseye) and Fedora 36. On Ubuntu 22.04, the process is completed without Abort. For unknown reasons, xnnpack delegate does not take effect. It may be one of the possibilities of CC(TFLite performance difference between python and c++).",Hi  ! Thanks for reporting this bug. This issue will be closed once PR CC(Fixed a bug in the xnnpack delegate option for label_image.) is merged. ,Are you satisfied with the resolution of your issue? Yes No
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(There was a problem quantifying the model with two inputs which have different dimensions.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution windows10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FragrantRookie,There was a problem quantifying the model with two inputs which have different dimensions.,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution windows10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-06T10:42:52Z,type:bug comp:lite TFLiteConverter TF 2.9,closed,0,11,https://github.com/tensorflow/tensorflow/issues/56365,Hi  ! Could you try with the below code snippet and let us know. Thank you! ,"Hi @mohantym!It's very encouraging to get your replyï¼ The error reports I get from using your code are as followsï¼š File ""D:\ProgramFiles\Anaconda3\lib\sitepackages\tensorflow\lite\python\optimize\calibrator.py"", line 138, in _feed_tensors     self._calibrator.FeedTensor(input_array) ValueError: Cannot set tensor: Got value of type FLOAT64 but expected type FLOAT32 for input 0, name: serving_default_input_3:0 ","Ok  ! Thanks for the update. We should cast both data types back to float32 then. if it still throws error , you can comment out the inference input and output line. Please let us know if it works. Thank you! `yield [data1.astype(tf.float32), data2.astype(tf.float32)] `",Hi @mohantym The error reports I get from using tf.float32 are as followsï¼š      The error reports I get from using np.float32 are the same as using tf.float32ï¼š  My codes are as follows: , ! Please comment out inference input type snippets from above. Thank you. ,"Hi @mohantym Greatï¼It works! My codes are as follows:   or  But,if the codes are writed as followsï¼ŒIt doesn't workï¼š   or  The error reports I get from using these codes are the same as followsï¼š ",> Hi @mohantym >  > The error reports I get from using tf.float32 are as followsï¼š >  >  >  >  >  >  >  > My codes are as follows: >  >  Using np.float is the right wayï¼Œbut other codes may wrong.,Are you satisfied with the resolution of your issue? Yes No,">  ! Please comment out inference input type snippets from above. Thank you. >  >  @mohantym It's very kind of you. Without your reply, I might have given up. I've been trying for days.", ! Thanks for the update. Glad that you were able to resolve this issue.,">  ! Thanks for the update. Glad that you were able to resolve this issue. Hi mohantym     Thank you!     This morning I ran the code on the company's computer, I opened the model with netron, and confirmed that the input to the model has been quantized to int16.     But I got a warning message. I'm not sure what effect this warning will have. If it affects, I will ask for help later. The warning message is as follows: `WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.`"
672,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Keras/tensorflow runs only one epoch)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.6.0  Custom Code Yes  OS Platform and Distribution Windows 10 21H2  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Cuda 11.2.2_461.33/ cuDNN v8.1.1  GPU model and memory GeForce NVidia RTX 3060  16GB RAM  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,bluetail14,Keras/tensorflow runs only one epoch,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.6.0  Custom Code Yes  OS Platform and Distribution Windows 10 21H2  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Cuda 11.2.2_461.33/ cuDNN v8.1.1  GPU model and memory GeForce NVidia RTX 3060  16GB RAM  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-06T07:45:09Z,stat:awaiting response type:support comp:keras 2.6.0,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56363,"Hi  ! Could you let us know after setting epochs = 2 or 4 with TF 2.8 version in the below code snippet. Thank you! `history_lstm = model_lstm.fit(sent_tok_train, labels_train,  validation_data=(sent_tok_val, labels_val), verbose =2, epochs = 4)`","Hi    yes,  I can get the 4 epochs. see the output. ",Ok  ! Could you move this issue to closed status then as it seems to be resolved from the above comment..    Thank you!,Are you satisfied with the resolution of your issue? Yes No
757,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([intel cpu(Sapphire Rapids) ] TF_ENABLE_ONEDNN_OPTS=0 performance is better(intra_op_parallelism_threads=1,  inter_op_parallelism_threads=1))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version Python 3.8.10  Bazel version _No response_  GCC/Compiler version GCC 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gemma,cuisonghui,"[intel cpu(Sapphire Rapids) ] TF_ENABLE_ONEDNN_OPTS=0 performance is better(intra_op_parallelism_threads=1,  inter_op_parallelism_threads=1)",Click to expand!    Issue Type Performance  Source binary  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version Python 3.8.10  Bazel version _No response_  GCC/Compiler version GCC 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-03T15:17:50Z,comp:ops type:performance TF 2.9,closed,0,0,https://github.com/tensorflow/tensorflow/issues/56350
1513,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow argsort sometimes produces non-deterministic results)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Mac OS Monterey 12.3.1 with M1 Pro CPU   Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  Which gives me a 1D tensor with shape (n,). Then I want to sort those values in descending order and return their indexes in sort order  hence I use the `tf.argsort function`. But I sometimes get very weird results. Sometimes it works, sometimes the results just don't make any sense. Here's an example:  Sometimes the argsort function returns:  But sometimes it also returns this which makes no sense:  Can someone help me out with this or try if you can reproduce this error?  Thanks! :) shell sample_constant = tf.constant([1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 9, 9, 9, 9, 5.171316e32, 0.000000e+00, 0.000000e+00, 0.000000e+00, 2, 3, 4, 1.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00])  Needs to be performed many times: for i in range(0,10):    tf.argsort(sample_constant, axis=0) ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,com98,Tensorflow argsort sometimes produces non-deterministic results,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Mac OS Monterey 12.3.1 with M1 Pro CPU   Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  Which gives me a 1D tensor with shape (n,). Then I want to sort those values in descending order and return their indexes in sort order  hence I use the `tf.argsort function`. But I sometimes get very weird results. Sometimes it works, sometimes the results just don't make any sense. Here's an example:  Sometimes the argsort function returns:  But sometimes it also returns this which makes no sense:  Can someone help me out with this or try if you can reproduce this error?  Thanks! :) shell sample_constant = tf.constant([1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 9, 9, 9, 9, 5.171316e32, 0.000000e+00, 0.000000e+00, 0.000000e+00, 2, 3, 4, 1.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00])  Needs to be performed many times: for i in range(0,10):    tf.argsort(sample_constant, axis=0) ```  Relevant log output _No response_",2022-06-03T14:11:36Z,stat:awaiting response type:bug stale comp:ops TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56349, Could you try with the latest TF version 2.9.0 and have a look at this gist for reference. Please let us know if it helps? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,>  Could you try with the latest TF version 2.9.0 and have a look at this gist for reference. Please let us know if it helps? Thank you! Hi!  Sorry for the late response! At the end it turned out to be a Mac OS GPU problem. I was using the GPU chip of my M1 chip and this somehow produced the weird results above. After switching to CPU it works just fine. Not a big issue for me as we train our models on SageMaker anyhow. Thanks a lot for your help anyways! Best regards!, Thank you for the update!
984,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow non-empty stderr when locating numpy include path (closes #56373))ï¼Œ å†…å®¹æ˜¯ (This PR passes `allow_failure=True` in `_get_numpy_include()` to avoid incorrectly treating any output from numpy (or its dependencies, namely OpenBLAS) on stderr as fatal errors. There are at least some cases where benign warnings will cause the existing behavior to fail even though `numpy.get_include()` succeeds. This change does mean that if `python3 c ""import numpy; print(numpy.get_include())""` were to return an empty stdout, an exception would occur when indexing the resulting empty list after calling `splitlines()`. I don't think there is any way the `numpy` implementation (which appears to be the same for the last 15 years) can output an empty string, but it's worth mentioning for the reviewer's consideration.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SnoopJ,Allow non-empty stderr when locating numpy include path (closes #56373),"This PR passes `allow_failure=True` in `_get_numpy_include()` to avoid incorrectly treating any output from numpy (or its dependencies, namely OpenBLAS) on stderr as fatal errors. There are at least some cases where benign warnings will cause the existing behavior to fail even though `numpy.get_include()` succeeds. This change does mean that if `python3 c ""import numpy; print(numpy.get_include())""` were to return an empty stdout, an exception would occur when indexing the resulting empty list after calling `splitlines()`. I don't think there is any way the `numpy` implementation (which appears to be the same for the last 15 years) can output an empty string, but it's worth mentioning for the reviewer's consideration.",2022-06-03T06:38:26Z,size:XS,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56347,"CI failure does not look related to these changes, seeing the same failure on CC(Change allow_failure description to match its functionality) (which has no code changes) so I assume this is noise. ","Note: title change and forcepush are because the bug has been refiled as CC(Locating the numpy include path can fail incorrectly) to comply with a request for using the issue template. Branch name is now slightly misleading, but semantics of the changes are unaffected by this namejuggling.",Hi  Can you please review this PR ? Thank you!,Hi  Can you please resolve conflicts? Thank you!,"> Hi  Can you please resolve conflicts? Thank you! Done, this PR is old enough that it was simpler to reset to current `master` and apply the oneline change to the new location of the code responsible for the bug. ROCm CI failure appears to be unrelated to these changes, something is broken in that build, seems to be failing for other PRs as well. ",Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  /  Can you please review this PR ? Thank you!,Hi  /  Can you please review this PR ? Thank you!,Hi  Can you please confirm if this PR is still valid? Thank you!,"> Hi  Can you please confirm if this PR is still valid? Thank you! This PR is still valid, the flaw in TensorFlow's build process that incorrectly treats NumPy warnings as fatal is still present.",Hi  Can you please resolve the conflicts? Thank you!,"3bf2ac3 has removed the file that the patch originally applied to. That changeset adds enough additional layers of indirection to the build that I cannot tell if the problem persists. If there is a maintainer who cares to check, the artificialwarning approach taken in the reproduction given CC(Locating the numpy include path can ""fail"" incorrectly) is still a good way to evaluate if a warning emitted by `numpy` will fail the build. As for this PR, I have waited two years for review of a single line of code paired with a thorough bug report and justification of the change. I do not think it is a very good use of my time to put any further work into this. Closing."
1288,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Locating the numpy include path can ""fail"" incorrectly)ï¼Œ å†…å®¹æ˜¯ ( Description `_get_numpy_include()` will fail if importing `numpy` causes _anything_ to be written to stderr, even if the command succeeds. I found this while helping a user in python on the Libera.chat IRC network debug a failure when building `yggdrasildecisionforests`. When they import `numpy` on their system, they get the following nonfatal warning:  Which causes `execute()` to fail. It looks like the `allow_failure` parameter for that function is meant to address this, so adding this argument should fix the issue in this specific case, but I'm not sure what the other implications might be. (as an aside, this parameter is confusingly documented, I have opened CC(Change allow_failure description to match its functionality) to address that) Insight from someone who is more familiar with TF's build system would be appreciated, but inspecting stdout/stderr strikes me as a very strange way to work out whether or not the executed process failed. Is there a reason this bit of code does not check the `return_code` of the raw execution?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SnoopJ,"Locating the numpy include path can ""fail"" incorrectly"," Description `_get_numpy_include()` will fail if importing `numpy` causes _anything_ to be written to stderr, even if the command succeeds. I found this while helping a user in python on the Libera.chat IRC network debug a failure when building `yggdrasildecisionforests`. When they import `numpy` on their system, they get the following nonfatal warning:  Which causes `execute()` to fail. It looks like the `allow_failure` parameter for that function is meant to address this, so adding this argument should fix the issue in this specific case, but I'm not sure what the other implications might be. (as an aside, this parameter is confusingly documented, I have opened CC(Change allow_failure description to match its functionality) to address that) Insight from someone who is more familiar with TF's build system would be appreciated, but inspecting stdout/stderr strikes me as a very strange way to work out whether or not the executed process failed. Is there a reason this bit of code does not check the `return_code` of the raw execution?",2022-06-03T06:23:58Z,stat:awaiting response,closed,0,2,https://github.com/tensorflow/tensorflow/issues/56346,", We see that the issue template has not been filled, could you please do so as it helps us analyse the issue. Thank you!",Refiled as CC(Locating the numpy include path can fail incorrectly) to comply with the above request because there is not a tidy way to add the templated information to an existing ticket.
1864,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to use tf.data API with multiple inputs of different shapes)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to make a tensorflow model with two different inputs, one will have shape [9,10], the other will just have shape [8]. I am furthermore trying to use tf.dataset to iterate over my inputs. However, whenever I try to do so it fails with the following error:  But surely it is possible to have differently shaped inputs into different branches! This is exactly the case in the example in tensorflow's functional API guide, however they do not use tf.dataset so I can't simply follow their example. To give a little more specifics into the problem I am trying to solve and why I am using the tf.dataset api: I am doing a timeseries problem over multiple sites where my inputs are of two types: those that vary with time, and those that do not but do vary by site. For the time being, I'm just trying to estimate the next time step. First, I get my dynamic covariates and targets in a sliding window using the timeseries_dataset_from_array util.  This works perfectly and I can train models using this dataset as is. However, I want to also use the static covariates from the specific site that the time series data is coming from. The site id is included in the window input data in its own column, though it gets removed before training. Thus, what I am trying)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,winthropharvey,How to use tf.data API with multiple inputs of different shapes,"Click to expand!    Issue Type Support  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to make a tensorflow model with two different inputs, one will have shape [9,10], the other will just have shape [8]. I am furthermore trying to use tf.dataset to iterate over my inputs. However, whenever I try to do so it fails with the following error:  But surely it is possible to have differently shaped inputs into different branches! This is exactly the case in the example in tensorflow's functional API guide, however they do not use tf.dataset so I can't simply follow their example. To give a little more specifics into the problem I am trying to solve and why I am using the tf.dataset api: I am doing a timeseries problem over multiple sites where my inputs are of two types: those that vary with time, and those that do not but do vary by site. For the time being, I'm just trying to estimate the next time step. First, I get my dynamic covariates and targets in a sliding window using the timeseries_dataset_from_array util.  This works perfectly and I can train models using this dataset as is. However, I want to also use the static covariates from the specific site that the time series data is coming from. The site id is included in the window input data in its own column, though it gets removed before training. Thus, what I am trying",2022-06-03T05:26:28Z,stat:awaiting response type:support stale comp:apis TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56343," I was able to replicate the issue on colab, please find the gist here for reference. Thank you!"," As mentioned here, try using `tf.data.Dataset.from_generator()` or try to zip the dataset to combine various inputs using `tf.data.Dataset.zip`.","The link you included doesn't seem related On Wed, Jul 13, 2022, 9:29 AM gowthamkpr ***@***.***> wrote: >   As mentioned here > , try using > tf.data.Dataset.from_generator() or try to zip the dataset to combine > various inputs using tf.data.Dataset.zip. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >",I'm sorry. Just edited it. PTAL. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
641,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Incorrect gradient of model output with respect to inputs using GradientTape)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.1.3  Custom Code No  OS Platform and Distribution Windows 10 Home  Mobile device _No response_  Python version 3.7  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version CPU  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bzdjordje,Incorrect gradient of model output with respect to inputs using GradientTape,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.1.3  Custom Code No  OS Platform and Distribution Windows 10 Home  Mobile device _No response_  Python version 3.7  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version CPU  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-03T03:41:19Z,stat:awaiting response type:bug stale comp:ops TF 2.9,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56341,", TensorFlow 2.1 is not actively supported. Could you update TensorFlow to the latest stable version v2.9 and check if you are facing the same issue. Thanks!",  Unfortunately the error still seems to persist if I run the same script in v2.9 as presented below !image !image,"Hi  and , This isn't a bug.  Your target function $f(x_1, x_2) \equiv cos(x_1) + x_2 / 5$ is a smooth surface over $\mathbb{R}^2$ which has a constant slope of 0.2 along the $x_2$ direction and is wavy along the $x_1$ direction.  You are training a network to fit just a slice of points along the line $x_1 = x_2$ from 0 to 100000.  So, even if `model()` learns to fit all the values along this line, it has a lot of freedom in the shape of the actual surface itself, so it is not surprising that it has learned a totally different surface. If you instead train it to fit points spread out in the whole square $0 \le x_1, x_2 \le 100000$, then it's likely that `model()` will have gradients much closer to the target function.",", Could you please take a look at this comment from the user and let us know if it still an issue. Thank you!","   Thank you for the responses. The data is randomly shuffled prior to training so the model is not trained on one slice as opposed to another, the validation/training datasets are fairly uniformly distributed across. zoomed in !image (plotting only every 100th point) !image If I numerically integrated the GradientTape differentiated model I am able to recover the original data, but what I need are derivatives that match those of the training data surface. I would like to extend this to models for which I do not have simple analytical expressions to compare to. I need to be able to trust this approach with analytical support, that I am getting the derivatives with respect to the input data and not just with respect to a response surface that exists only in the model. !image","Hi ,   When I said 'slice', I mean a subset of $\mathbb{R}^2$, which is the domain of your target function.  Actually, I mistook the domain.  You are taking 100000 points evenly spaced along the line segment from point (0, 2) to (4pi, 20).  What you should be calculating is the gradient projected to that line.  If you do that, you see that the autodiff gradient according to the model very nearly matches the analytical gradient of the function. To calculate this gradient, you take the dot product with a normalized vector in the direction of that line.  Here is the result: !image What I did is edited your code as follows: ",", Could you try to execute the above alternative workaround and confirm if it is working as expected. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"   Hello, the correction to my script provided by Henry worked on my computer, thank you!",",  Glad the issue is resolved for you, please feel free to move this to closed status. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Character-level seq2seq model for translation and beam search. )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Colab GPU  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,VallabhMahajan1,Character-level seq2seq model for translation and beam search. ,Click to expand!    Issue Type Documentation Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Colab GPU  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-02T19:23:53Z,stat:awaiting response type:feature stale comp:apis type:docs-feature,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56337,"Hi , I was able to execute the given code without any issues. Kindly find the gist of it here. Could you share a reproducible code that supports your statement so that the issue can be easily understood? Thank you!","I changed this one parameter  'char_level = True' in tf.keras tokenizer. class NMTDataset:     def __init__(self, problem_type='enspa'):         self.problem_type = 'enspa'         self.inp_lang_tokenizer = None         self.targ_lang_tokenizer = None     def unicode_to_ascii(self, s):         return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')      Step 1 and Step 2      def preprocess_sentence(self, w):         w = self.unicode_to_ascii(w.lower().strip())          creating a space between a word and the punctuation following it          eg: ""he is a boy."" => ""he is a boy .""          Reference: https://stackoverflow.com/questions/3645931/pythonpaddingpunctuationwithwhitespaceskeepingpunctuation         w = re.sub(r""([?.!,Â¿])"", r"" \1 "", w)         w = re.sub(r'["" ""]+', "" "", w)          replacing everything with space except (az, AZ, ""."", ""?"", ""!"", "","")         w = re.sub(r""[^azAZ?.!,Â¿]+"", "" "", w)         w = w.strip()          adding a start and an end token to the sentence          so that the model know when to start and stop predicting.         w = ' ' + w + ' '         return w     def create_dataset(self, path, num_examples):          path : path to spaeng.txt file          num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)         lines = io.open(path, encoding='UTF8').read().strip().split('\n')         word_pairs = [[self.preprocess_sentence(w) for w in l.split('\t')]  for l in lines[:num_examples]]         return zip(*word_pairs)     def convert_list_to_string(sentences):         text = """"          for s in sentences:            text += s + "" ""          return text      Step 3 and Step 4     def tokenize(self, lang):          lang = list of sentences in a language         print(len(lang), ""example sentence: {}"".format(lang[0]))         lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='', char_level = True)         lang_tokenizer.fit_on_texts(lang)          tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn)           to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)         tensor = lang_tokenizer.texts_to_sequences(lang)           tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences           and pads the sequences to match the longest sequences in the given input         tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')         return tensor, lang_tokenizer     def load_dataset(self, path, num_examples=None):          creating cleaned input, output pairs         targ_lang, inp_lang = self.create_dataset(path, num_examples)         input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)         target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)         return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer     def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):         file_path = download_nmt()         input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)         input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)         train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))         train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)         val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))         val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)         return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer",", Here the tutorial describes Neural Machine Translation(NMT) using word level sequencetosequence model using TF Addons. Please refer to this tutorial for character level sequencetosequence using Tensorflow.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
666,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ZeroDivisionError on GradientTape.gradient when computing a loss from concatting scalar values)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution 18.04.1Ubuntu  Mobile device _No response_  Python version 3.8.10  Bazel version   GCC/Compiler version   CUDA/cuDNN version cuda11.3/cudnn8.2.1  GPU model and memory Tesla V100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,str4si,ZeroDivisionError on GradientTape.gradient when computing a loss from concatting scalar values,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution 18.04.1Ubuntu  Mobile device _No response_  Python version 3.8.10  Bazel version   GCC/Compiler version   CUDA/cuDNN version cuda11.3/cudnn8.2.1  GPU model and memory Tesla V100  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-02T13:42:01Z,stat:awaiting response type:bug stale comp:ops TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56335,"Hi , I tried to execute the given code in an alternative approach. Kindly find the gist of it here. Thank you! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
599,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unix wheel URLs are incorrect in TF2.9)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Ubuntu  Mobile device NA  Python version 3.7, 3.8, 3.9, 3.10  Bazel version NA  GCC/Compiler version NA  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,chunduriv,Unix wheel URLs are incorrect in TF2.9,"Click to expand!    Issue Type Documentation Bug  Source binary  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Ubuntu  Mobile device NA  Python version 3.7, 3.8, 3.9, 3.10  Bazel version NA  GCC/Compiler version NA  CUDA/cuDNN version NA  GPU model and memory NA  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ``` ",2022-06-02T05:52:40Z,type:docs-bug awaiting PR merge TF 2.9,closed,0,1,https://github.com/tensorflow/tensorflow/issues/56330,Submitted PR to fix this issue. 
696,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Character-level seq2seq model for translation and beam search.)ï¼Œ å†…å®¹æ˜¯ (I was trying to implement seq2seq translation model at character level along with beam search by referring the tensorflow documentation. [https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt](url) For this, I tried to change parameter, 'char_level = True' in tf.keras tokenizer. There was no issue during model training, but I'm getting error for inferences.  Can someone please help me to solve this issue. Thank you in advance !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,VallabhMahajan1,Character-level seq2seq model for translation and beam search.,"I was trying to implement seq2seq translation model at character level along with beam search by referring the tensorflow documentation. [https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt](url) For this, I tried to change parameter, 'char_level = True' in tf.keras tokenizer. There was no issue during model training, but I'm getting error for inferences.  Can someone please help me to solve this issue. Thank you in advance !image",2022-06-02T02:24:17Z,stat:awaiting response stale,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56327,"  In order to expedite the troubleshooting process here, could you please fill the issue template, Thank you!","It isn't working because the input to the tokenize function is a list of sentences.  char_level=True helps only if it is a string, from my experience. You can concatenate the list to a string and then tokenize with char_level=True  Hope this helps.","Thanks for the reply. I'm a beginner in this field. I tried to concatenate the list to a string inside the tokenize function and I'm getting this error.      Step 3 and Step 4     def tokenize(self, lang):          lang = list of sentences in a language         text = ''         for s in lang:           text += s + """"         print(len(text), ""example sentence: {}"".format(text[5]))         lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='', char_level = True)         lang_tokenizer.fit_on_texts(text)         print(text)          tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn)           to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)         tensor = lang_tokenizer.texts_to_sequences(text)           tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences           and pads the sequences to match the longest sequences in the given input         tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')         return tensor, lang_tokenizer  ValueError                                Traceback (most recent call last)  in ()       5        6 dataset_creator = NMTDataset('enspa') > 7 train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE) 3 frames /usr/local/lib/python3.7/distpackages/sklearn/utils/validation.py in check_consistent_length(*arrays)     332         raise ValueError(     333             ""Found input variables with inconsistent numbers of samples: %r"" > 334             % [int(l) for l in lengths]     335         )     336  ValueError: Found input variables with inconsistent numbers of samples: [1038789, 981044] Can you please help me with this.  Thank you so much."," Thank you for the response! Could you refer this link  to know more on Characterlevel recurrent sequencetosequence model and let us know if it helps? In order to expedite the troubleshooting process, please provide a complete code snippet to reproduce the issue reported here and let us know the TF version you are using. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
707,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Linking to static libtensorflow-lite.a gives linking errors on Debian x64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Debian 11 x64  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version g++ (Debian 10.2.16) 10.2.1 20210110  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mrdeveloperdude,Linking to static libtensorflow-lite.a gives linking errors on Debian x64,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Debian 11 x64  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version g++ (Debian 10.2.16) 10.2.1 20210110  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-01T21:49:03Z,type:build/install comp:lite TF 2.9,closed,0,2,https://github.com/tensorflow/tensorflow/issues/56325,"I found the answer; In the very documentation I was following this was noted: > Note: This generates a static library libtensorflowlite.a in the current directory but the library isn't selfcontained since all the transitive dependencies are not included. To use the library properly, you need to create a CMake project. Please refer the ""Create a CMake project which uses TensorFlow Lite"" section. Unfortunately my project is not based on cmake and so I really only see 4 ways to link to tflite at this point: 1. Convert to CMake (not feasible) 2. Create a CMake library project that wraps tflite and produces one single static library that can be linked from my noncmake project (feasible but a major PITA 3. Ask niceely for tf to support non cmake projects (probably not going to happen) 4. Hack the hell out of this and just make it workâ„¢ In the interest of saving time and not wasting my brilliance on this stuff I went wit option 4. Here is the workaround if you find yourself in a similar situation; To get additional lib folders ( use with L during link) find . type f name ""*.a"" exec  dirname {} \; | sort u To get the list of lib files (use with l during link) find . type f name ""lib*.a"" exec  basename {} \; NOTE: you have to remove ""lib"" prefix and "".a"" suffix for this to work The final result for me (Qt6 qmake syntax) looked like this: LIBS += L$$PWD/libs/tflite_build ltensorflowlite INCLUDEPATH += $$PWD/libs/tensorflow INCLUDEPATH += $$PWD/libs INCLUDEPATH += $$PWD/libs/tflite_build DEPENDPATH += $$PWD/libs/tflite_build PRE_TARGETDEPS += $$PWD/libs/tflite_build/libtensorflowlite.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/pthreadpool/libpthreadpool.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/cpuinfobuild/libcpuinfo.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/flatbuffersbuild/libflatbuffers.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/clogbuild/libclog.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/xnnpackbuild/libXNNPACK.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/farmhashbuild/libfarmhash.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/fft2dbuild/libfft2d_fftsg2d.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/fft2dbuild/libfft2d_fftsg.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/types/libabsl_bad_optional_access.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/types/libabsl_bad_variant_access.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/status/libabsl_status.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/time/libabsl_civil_time.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/time/libabsl_time_zone.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/time/libabsl_time.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/base/libabsl_throw_delegate.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/base/libabsl_raw_logging_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/base/libabsl_malloc_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/base/libabsl_spinlock_wait.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/base/libabsl_log_severity.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/base/libabsl_base.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_commandlineflag_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_marshalling.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_config.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_program_name.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_commandlineflag.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_private_handle_accessor.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags/libabsl_flags_reflection.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/hash/libabsl_city.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/hash/libabsl_low_level_hash.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/hash/libabsl_hash.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/numeric/libabsl_int128.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/debugging/libabsl_stacktrace.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/debugging/libabsl_symbolize.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/debugging/libabsl_debugging_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/debugging/libabsl_demangle_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/synchronization/libabsl_synchronization.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/synchronization/libabsl_graphcycles_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/profiling/libabsl_exponential_biased.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/container/libabsl_raw_hash_set.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/container/libabsl_hashtablez_sampler.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_cordz_functions.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_cordz_handle.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_cord_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_strings.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_strings_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_cord.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_cordz_info.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings/libabsl_str_format_internal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_ctx.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_block_map.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_apply_multiplier.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/profiler/libruy_profiler_instrumentation.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_allocator.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_tune.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_blocking_counter.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_denormal.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_cpuinfo.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_pack_avx2_fma.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_frontend.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_context.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_thread_pool.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_trmul.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_wait.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_prepacked_cache.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_kernel_avx512.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_kernel_arm.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_pack_arm.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_system_aligned_alloc.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_prepare_packed_matrices.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_pack_avx.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_context_get_ctx.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_have_built_path_for_avx512.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_have_built_path_for_avx.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_kernel_avx.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_pack_avx512.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_kernel_avx2_fma.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/_deps/ruybuild/ruy/libruy_have_built_path_for_avx2_fma.a PRE_TARGETDEPS += $$PWD/libs/tflite_build/libtensorflowlite.a LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/base LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/container LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/debugging LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/flags LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/hash LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/numeric LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/profiling LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/status LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/strings LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/synchronization LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/time LIBS += L$$PWD/libs/tflite_build/_deps/abseilcppbuild/absl/types LIBS += L$$PWD/libs/tflite_build/_deps/clogbuild LIBS += L$$PWD/libs/tflite_build/_deps/cpuinfobuild LIBS += L$$PWD/libs/tflite_build/_deps/farmhashbuild LIBS += L$$PWD/libs/tflite_build/_deps/fft2dbuild LIBS += L$$PWD/libs/tflite_build/_deps/flatbuffersbuild LIBS += L$$PWD/libs/tflite_build/_deps/ruybuild/ruy LIBS += L$$PWD/libs/tflite_build/_deps/ruybuild/ruy/profiler LIBS += L$$PWD/libs/tflite_build/_deps/xnnpackbuild LIBS += L$$PWD/libs/tflite_build/pthreadpool LIBS += lpthreadpool LIBS += lpthreadpool LIBS += lcpuinfo LIBS += lflatbuffers LIBS += lclog LIBS += lXNNPACK LIBS += lfarmhash LIBS += lfft2d_fftsg2d LIBS += lfft2d_fftsg LIBS += labsl_bad_optional_access LIBS += labsl_bad_variant_access LIBS += labsl_status LIBS += labsl_civil_time LIBS += labsl_time_zone LIBS += labsl_time LIBS += labsl_throw_delegate LIBS += labsl_raw_logging_internal LIBS += labsl_malloc_internal LIBS += labsl_spinlock_wait LIBS += labsl_log_severity LIBS += labsl_base LIBS += labsl_flags_commandlineflag_internal LIBS += labsl_flags_marshalling LIBS += labsl_flags_internal LIBS += labsl_flags_config LIBS += labsl_flags_program_name LIBS += labsl_flags LIBS += labsl_flags_commandlineflag LIBS += labsl_flags_private_handle_accessor LIBS += labsl_flags_reflection LIBS += labsl_city LIBS += labsl_low_level_hash LIBS += labsl_hash LIBS += labsl_int128 LIBS += labsl_stacktrace LIBS += labsl_symbolize LIBS += labsl_debugging_internal LIBS += labsl_demangle_internal LIBS += labsl_synchronization LIBS += labsl_graphcycles_internal LIBS += labsl_exponential_biased LIBS += labsl_raw_hash_set LIBS += labsl_hashtablez_sampler LIBS += labsl_cordz_functions LIBS += labsl_cordz_handle LIBS += labsl_cord_internal LIBS += labsl_strings LIBS += labsl_strings_internal LIBS += labsl_cord LIBS += labsl_cordz_info LIBS += labsl_str_format_internal LIBS += lruy_ctx LIBS += lruy_block_map LIBS += lruy_apply_multiplier LIBS += lruy_profiler_instrumentation LIBS += lruy_allocator LIBS += lruy_tune LIBS += lruy_blocking_counter LIBS += lruy_denormal LIBS += lruy_cpuinfo LIBS += lruy_pack_avx2_fma LIBS += lruy_frontend LIBS += lruy_context LIBS += lruy_thread_pool LIBS += lruy_trmul LIBS += lruy_wait LIBS += lruy_prepacked_cache LIBS += lruy_kernel_avx512 LIBS += lruy_kernel_arm LIBS += lruy_pack_arm LIBS += lruy_system_aligned_alloc LIBS += lruy_prepare_packed_matrices LIBS += lruy_pack_avx LIBS += lruy_context_get_ctx LIBS += lruy_have_built_path_for_avx512 LIBS += lruy_have_built_path_for_avx LIBS += lruy_kernel_avx LIBS += lruy_pack_avx512 LIBS += lruy_kernel_avx2_fma LIBS += lruy_have_built_path_for_avx2_fma LIBS += ldl",Are you satisfied with the resolution of your issue? Yes No
1848,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can't build tensorflow with cuda_clang ubuntu20)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.10  Custom Code No  OS Platform and Distribution Ubuntu 20.04 (from develgpu docker)  Mobile device _No response_  Python version 3.8  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2  GPU model and memory RTX2080Ti  Current Behaviour?  and running command  Got the fallowing error:  This seems to be problem with installing clang from apt and has been reported before without any other solution as building clang from source. When build clang12 from source it didn't have an error, but that version does not support sm75+ architectures.  Downloading clang was also not an option as chromium versions are not built against `nvptx` target and produces error during build with sm75+. How to properly link clang13/14 as cuda compiler to build tensorflow? shell Building from source at master branch and in develgpu docker image. Using instructions from https://apt.llvm.org/ to get clang14. Adding to tf_configure_bazel.rc:   action_env CLANG_CUDA_COMPILER_PATH=""/usr/bin/clang14"" shell WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.bazelrc:   'build' options: define framework_shared_obje)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,eduardinjo,Can't build tensorflow with cuda_clang ubuntu20,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.10  Custom Code No  OS Platform and Distribution Ubuntu 20.04 (from develgpu docker)  Mobile device _No response_  Python version 3.8  Bazel version 5.1.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2  GPU model and memory RTX2080Ti  Current Behaviour?  and running command  Got the fallowing error:  This seems to be problem with installing clang from apt and has been reported before without any other solution as building clang from source. When build clang12 from source it didn't have an error, but that version does not support sm75+ architectures.  Downloading clang was also not an option as chromium versions are not built against `nvptx` target and produces error during build with sm75+. How to properly link clang13/14 as cuda compiler to build tensorflow? shell Building from source at master branch and in develgpu docker image. Using instructions from https://apt.llvm.org/ to get clang14. Adding to tf_configure_bazel.rc:   action_env CLANG_CUDA_COMPILER_PATH=""/usr/bin/clang14"" shell WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.bazelrc:   'build' options: define framework_shared_obje",2022-06-01T20:48:17Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56324, Could you try to use the `latest stable TF version 2.9.0` and refer to the link to know more on the tested build configuration. Please let us know if it helps? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
793,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can't build tflite for Java)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution macos  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  I get the error  Not sure what it means though a similar issu?e I saw in mediapipe claims it has to do with my ANDROID_HOME and ANDROID_NDK_HOME. Any ideas?  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,drubinstein,Can't build tflite for Java,Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution macos  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  I get the error  Not sure what it means though a similar issu?e I saw in mediapipe claims it has to do with my ANDROID_HOME and ANDROID_NDK_HOME. Any ideas?  Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-01T20:09:54Z,stat:awaiting response type:build/install comp:lite subtype:macOS TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56323,"Hi  !  The above error is pointing towards unavailability of SDK tool chain and NDK tool chain during the build process(./configure). If you already have Android studio installed , it will take an existing tool chain from respective folders  else you can download NDK (19 min version)and SDK tool (23 min version )chain and export the path before running build process.  Alternatively you can follow the below process. 0. install Bazel 5.0.0 in macos  1. git clone https://github.com/tensorflow/tensorflow 2. cd tensorflow 3. git checkout r2.9 4. ./configure or python configure.py 5. say yes to android build when it asks and set path to SDK and NDK. 6. bazelisk build c opt fat_apk_cpu=x86,x86_64,arm64v8a,armeabiv7a \   host_crosstool_top=//tools/cpp:toolchain \   //tensorflow/lite/java:tensorflowlite Could you close the issue CC(Unable to use tflite java on macos) if duplicate to this issue. Thank you!",Thanks  . I went through `./configure` again and realized that I had an older version  than the default minimum  android SDK installed (32). I installed 32.0.0 and 32.1.0rc1 and ran into the error `missing input file 'external/androidsdk/buildtools/32.0.0/lib/dx.jar'`. I then installed SDK version 30.0.3 and went through the configuration again to use it and after that I was able to build the target successfully. This isn't a duplicate of CC(Unable to use tflite java on macos) as that's more about us getting jni errors when we try to use tflitejava from maven in local unit testing. I made this issue cause I was trying to do some initial work to see how hard it would be to add a .dylib output and build locally. I'll close this issue ,Are you satisfied with the resolution of your issue? Yes No,Thanks for the update! Glad it resolved this issue.
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to use tflite java on macos)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution MacOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,drubinstein,Unable to use tflite java on macos,Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution MacOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-01T20:07:25Z,stat:awaiting response type:feature stale comp:lite subtype:macOS,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56322,Any update?,Hi there! I'm following up on this issue! We're looking for `.dylib` files for Android development unit testing,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
661,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dataset created from large ragged tensor doesn't copy values)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux  Mobile device _No response_  Python version 3.9.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,svenhsia,Dataset created from large ragged tensor doesn't copy values,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.9.1  Custom Code Yes  OS Platform and Distribution Linux  Mobile device _No response_  Python version 3.9.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-01T09:56:59Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56319,Are you satisfied with the resolution of your issue? Yes No,"Reproduced on macos python 3.10.4 (condaforge) tf 2.9.1 (pypi) This seems to depend on the size of the data and works ok as long as the total size is < 2**31  1. e.g. `x.shape == (2 ** 31  2, 1)` is ok but `x.shape == (2 ** 30, 2)` is not However for `x.shape == (2 ** 31, 1)`  the construction of the RaggedTensor throws an exception complaining that `Shape output type is 32bit but dim 0 is 2**31 [Op:Shape]`. It does not work but at least it does not produce a random output silently",", I tried to execute the given code in colab with v 2.8, 2.9, nightly version and noticed that session is being crashed. Please find the gist of it here. Thank you!",The problem is caused by improper int types used in the ragged tensor data preparation. Will look at how to fix it.,We are working on a fix in ragged_tensor_to_variant_op.,The issue was fixed in this commit. Please see if there is any other problems.,Are you satisfied with the resolution of your issue? Yes No
660,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unit test //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test fails on Python 3.7, 3.8)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,elfringham,"Unit test //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test fails on Python 3.7, 3.8",Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-06-01T08:32:43Z,type:bug comp:core,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56317,  ,Fixed by https://github.com/tensorflow/tensorflow/commit/f98fbf2e484e7cc34e208ee5f3bb499925b8cb15,Are you satisfied with the resolution of your issue? Yes No
671,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Building Tflite compiling out pthread)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Linux Ubuntu 16.04.2 LTS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,supratimc239,Building Tflite compiling out pthread,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Linux Ubuntu 16.04.2 LTS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-06-01T06:50:06Z,stat:awaiting response type:feature type:build/install stale comp:lite TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56315,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
658,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to full int 8 quantize a yamnet model?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux, Mac  Mobile device Linux Ubuntu, Mac m1, Mac intel  Python version 3.7, 3.10, 3.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,MATTYGILO,How to full int 8 quantize a yamnet model?,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux, Mac  Mobile device Linux Ubuntu, Mac m1, Mac intel  Python version 3.7, 3.10, 3.6  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-05-31T20:55:14Z,stat:awaiting response type:feature stale comp:lite ModelOptimizationToolkit TF 2.8,closed,0,17,https://github.com/tensorflow/tensorflow/issues/56313,I have left an issue here for days now: https://github.com/tensorflow/modeloptimization/issues/974 There is not info online about full int 8 quantisation for yamnet,Hi  ! Sorry for the late response. Could you just share the TFlite conversion part of yamnet model as Colab gist. Thank you!, This is my gist https://gist.github.com/MATTYGILO/9f32ab8837a6ef794360ce1e128e7cc9,"I've also done some research into just quantising the spectrogram layer. Spectrogram output without any quantisation, No QAT: !IMG_4987 Spectrogram output with quantisation, No QAT: !IMG_4986 QAT spectrogram without quantisation: !IMG_4988 QAT spectrogram with quantisation !IMG_4989 As you can see with quantisation and QAT the output are just 0's. ï¿¼"," ! Actually ""Quant ""  seems not to be working in Colab. Could you please update on this. Attached gist  for reference. Thank you!", Oh sorry that was a file I made. I'll try and provide a better example, I'm really stuck is it possible if you could provide an example for a working full int 8 quantisation of a yamnet," ! I think int8 quantization might not be possible yet. Found a similar issue here. But , Could you make these changes in the above code and let us know. Thank you! "," Just a quick question, when training a full int 8 model using QAT. Do I train on quantised data or normal data?  Or does the QAT model auto convert the normal data to quantised? Just uncertain from the the `yield [data.astype(np.uint8)]`",Ok using the `data.astype(np.int8)` I get a `ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0`, ! Quantized data is not needed normally in the representative dataset.  You should revert to float32 datatype as the error suggests. I am afraid integer quantization might not be possible for Yamnet model yet. , Ok the only problem is that I require integer quantisation for tflite micro. So I'm a bit stuck I guess.,Hi  ! Could you please look at this issue. Thank you!,"On Tensorflow hub there is an int8 model, any examples to go with it? https://tfhub.dev/google/litemodel/yamnet/classification/tflite/1","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
714,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Training the same model on the same data yielding extremely different test accuracy)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0 (From Google Colab)  Custom Code No  OS Platform and Distribution Google Colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Google Colab  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Mirandatz,Training the same model on the same data yielding extremely different test accuracy,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0 (From Google Colab)  Custom Code No  OS Platform and Distribution Google Colab  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Google Colab  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-31T20:40:57Z,stat:awaiting response type:bug comp:keras TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56312,"Hi , I was able to execute the code without any issues and also the accuracy is also the same for every run. Please find the gist of it here.  Also, setting the seeds is not enough. If you are working on a small dataset you have to switch from GPU to CPU, as the first one has a nondeterministic order of computations. Moreover, you should get rid of any parallelism in feeding data, as that can also cause problems. Thank you!","Thank you for the quick feedback  I will run your notebook soon, but did you manage to run mine using a GPU? I ask because I understand that there will be some differences when we use GPUs due to differences in execution order, how the numerical errors accumulate, etc. But I did not expect such extreme differences. Allow me to clarify what I mean by ""extreme differences"":  The difference is as large as 50% between runs 4 and 3. That doesn't look reasonable. I expected differences <10%. Also, I believe I have found two things that appear to cause (or influence) this inconsistency (when using GPUs): the shuffling (using buffer sizes smaller than the entire dataset) and batching of the data (dropping remainders). I will write a new notebook and post it here ASAP.",I also asked for help on TensorFlow and it appears other have also reproduced the issue.,"Using the following parameters to shuffle and batch makes the issue disappear, but I don't know why: "," I tried to run your notebook, but it errors out. I created a copy of mine and reran it, again using a GPU runtime. https://colab.research.google.com/drive/1OvUkM_Mf1eUwY3LDRUp6Tk5l8alRi9Oa?usp=sharing Issue persists. Had accuracies as low as 0.10 and as high as 0.55 :( theres something really weird going on here. Output:  I have also created a new notebook that tries to emulate yours, i.e., keeps the dataset on /content instead of /dev/shm: https://colab.research.google.com/drive/16qi8SI94KpGXzHCIEyThWI3Z4xlNk5Q?usp=sharing The issue still occurs. I don't know why I didn't happen when you tried to run.",", I was able to reproduce the issue on tensorflowgpu v2.8 and v2.9. In tensorflowcpu v2.8, i was not able to find the issue. Please find the gist.","As mentioned here, these are the reasons for the variance in test accuracy and how can you work around it: 1. Use` tf.keras.utils.set_random_seed(some_seed)` control all random behavior is needed 2. Optimize the algorithm as much as possible to mitigate the impact of calculation accuracy. That is, let the algorithm not produce or accumulate small errors in accuracy. There are many adjustable keys, not only the optimizer or the optimizer's parameters, but also other stuff such as bacth_size, gradient_clip, gradient_norm etc. 3. Use `tf.config.experimental.enable_op_determinism` if you want to get completely consistent reproducible results and accept its side effects(deceleration)."," Yes, my mistake. I did not realize that test accuracy varying between 0.1 to 0.55, using the same model on the same dataset, was not only acceptable but expected.",Are you satisfied with the resolution of your issue? Yes No
699,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow 2.2 build from source: Executing genrule @local_config_python//:numpy_include failed (Exit 1))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.2  Custom Code No  OS Platform and Distribution Ubuntu   Mobile device Ubuntu 20.04.4  Python version 3.8.10  Bazel version Bazel 2.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bergduivel,Tensorflow 2.2 build from source: Executing genrule @local_config_python//:numpy_include failed (Exit 1),Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.2  Custom Code No  OS Platform and Distribution Ubuntu   Mobile device Ubuntu 20.04.4  Python version 3.8.10  Bazel version Bazel 2.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-31T16:17:54Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.2,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56310, Could you try with the latest TF version 2.9.0 as older versions are not actively supported. We recommend you to kindly upgrade to the latest TF versions and let us know the issue still persists? Thank you!,"I did that that earlier and it works with the latest version, but I specifically need TF 2.2?"," TF 2.2 does not really contain all the changes to prevent code using invalid shapes from crashing the interpreter via CHECKfailures. It's unlikely for TF 2.2 version to receive any bug fixes except when we have security patches. As TF version 2.2 is not actively supported please post this issue on TF forum, there is a larger community to get you the right help. Thank you!","Ok, I'll try that. Thanks!", Thank you for the update! Could you please let us know if we can move this issue to closed status? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1875,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.data shard malfunction (drops data) when using from_generator as source)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.8.0rc132g3f878cff5b6 2.8.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.6 / cuDNN 8.3.2 (using 22.02 TensorRT NGC container)  GPU model and memory Quadro RTX 4000 8GB  Current Behaviour? For prediction pipelines I am using tf.data.Dataset fed from queues by using from_generator. I chose this way since I get data from queues and I want the dataset never to terminate. I need to shard datasets to feed parallel prediction instances and tried using the shard function. It does not work as expected when using from_generator as source. It drops some data and fails with indices.  It works perfectly fine when using e.g. from_tensor_slices or range to define the dataset. Minimal example in colab: https://colab.research.google.com/drive/18SLBxl2Qws1GdA7YJ9GOI8DyPNgv_LTn?usp=sharing I am currently working around this by extracting the tf.data.Dataset and putting it into multiple queues to manually shard and putting it back into tf.data.Dataset  this works but feels very wrong and produces overhead as I assume. Is this behaviour desired?  Am I maybe missing something? Are there other solutions getting infinite Datasets from Queue with shard working? Would appreciate some help   Standalone code to reproduce the issue   Relevant log output Sharding from range as source even from range: 0 even from range: 2 odd from range: 1 even from range: 4 odd from range: 3 even fro)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Daniel595,tf.data shard malfunction (drops data) when using from_generator as source,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.8.0rc132g3f878cff5b6 2.8.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.6 / cuDNN 8.3.2 (using 22.02 TensorRT NGC container)  GPU model and memory Quadro RTX 4000 8GB  Current Behaviour? For prediction pipelines I am using tf.data.Dataset fed from queues by using from_generator. I chose this way since I get data from queues and I want the dataset never to terminate. I need to shard datasets to feed parallel prediction instances and tried using the shard function. It does not work as expected when using from_generator as source. It drops some data and fails with indices.  It works perfectly fine when using e.g. from_tensor_slices or range to define the dataset. Minimal example in colab: https://colab.research.google.com/drive/18SLBxl2Qws1GdA7YJ9GOI8DyPNgv_LTn?usp=sharing I am currently working around this by extracting the tf.data.Dataset and putting it into multiple queues to manually shard and putting it back into tf.data.Dataset  this works but feels very wrong and produces overhead as I assume. Is this behaviour desired?  Am I maybe missing something? Are there other solutions getting infinite Datasets from Queue with shard working? Would appreciate some help   Standalone code to reproduce the issue   Relevant log output Sharding from range as source even from range: 0 even from range: 2 odd from range: 1 even from range: 4 odd from range: 3 even fro,2022-05-31T08:38:00Z,stat:awaiting tensorflower type:bug comp:data TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56306,"Hi , From the `tf.data` side, it is expected that the cardinality of `Dataset.from_generator()` is unknown. There's isn't a good way to determine how much data a given generator will produce.  The generator could be infinite or produce random amounts of data. Thank you!"," Thank you for the quick response.  So, just to make sure  it is not expected that `shard` will work correctly for datasets generated by `Dataset.from_generator()`? Could you then maybe imagine a way of feeding a dataset from a Queue which contigously supplies images where the cardinality is not unknown?  The final amount of images the Queue will deliver could be known but the data will be only available during runtime. This excludes defining the dataset by something like `from_tensors()`, doesn't it? Thanks for the help!",", I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist of it here.","The official docs for `shard` say > Creates a Dataset that includes only 1/num_shards of this dataset. > > shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. Shard doesn't partition a dataset  rather it reads all the elements from the dataset and drops all but  `1/num_shards` of them. `shard` may be used to partition a dataset **if the dataset can be deterministically iterated through multiple times**. Each `.shard(num_shards, i)` transformation will read its entire input dataset and return only the part of the dataset that belongs to the specified ""shard"". In your case, the dataset created by `from_generator` is not deterministic since all iterations share the same Queue. That is why shard isn't working as you expected. Your workaround is reasonable and I wouldn't expect it to add a detectable amount of overhead. Alternatively, you could consider using a single (unsharded) dataset, and having prediction instances all pull from the same iterator over that dataset. That would result in processing every element exactly once, though it wouldn't be deterministic which thread processes which element. ","Thank you for the good explanations. I think I slightly unterstood the idea.  In my mind `shard` would have partitioning the dataset by kind of a counter logic or something like that. Reading all elements first actually seems unintuitive to me. I guess there could be a couple of methods more not working as expected on non deterministic datasets, like `concatenate`. I can imagine that cases like this are common pitfalls for developers and believe now that non deterministic datasets are kind of restricted. Maybe if there aren't some cases where this behaviour could be required, some warnings/errors could help here. In the minimal example I don't expect a detectable overhead by the workaround either. In my real application I put back the prediction data into datasets. My entire datapipeline uses `as_numpy_iterator()` and `from_generator()` about 10 times or so and I once read instanciation of `Dataset` is quite some overhead. Your example seems to be usefull to optimize througput. I'll keep that in mind.  I have 4 parallel instances each expecting to get elements with their indices (03) from the queue. My newest approach is to get the 4 elements zipped and run the prediction mapping a numpy_function wrapping the model prediction. This saves me all those custom 'cuts' and right now seems to perform better than the Thread/Queue approaches.   I have no idea if and how AUTOTUNE takes any effect here. The single prediction instances are getting very unbalanced workload (1/12/48). But I think it doesn't really matter, since I have to gather the 4 prediction results for postprocessing anyway. Anyway, thanks a lot for the help and for resolving my issue!",Are you satisfied with the resolution of your issue? Yes No
618,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fail to link unit tests for AARCH64 with --config=mkl_aarch64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,elfringham,Fail to link unit tests for AARCH64 with --config=mkl_aarch64,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-30T17:08:36Z,stat:awaiting response type:build/install stale subtype:centos subtype:bazel,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56302,  ,Hi  ! Could you look at this issue. Attached gist for reference. Thank you!,"  The referenced gist above does not match what I am seeing, not sure why that would be, possibly a different default in your environment. A full build log can be seen at https://ci.linaro.org/view/All/job/ldcgpythonmanylinuxtensorflowonednnnightly/68/consoleText","I'm getting the same error in ManyLinux2014 builds with Python 3.10, but not 3.8.",Hi   ! We are checking to see whether you still need help in this issue .  Have you checked this issue in 2.10/2.11 version lately. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
659,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tflite model not work correctly on NNAPI-GPU delegate)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.5, 2.6  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device Huawei Mate 10 Pro  Python version 3.6.0  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Non disponibile   GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,francescoyou97,Tflite model not work correctly on NNAPI-GPU delegate,"Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.5, 2.6  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device Huawei Mate 10 Pro  Python version 3.6.0  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version Non disponibile   GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-05-30T13:20:08Z,stat:awaiting response type:support stale comp:lite TFLiteNNAPIDelegate TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56301,"example.zip train.zip The example.zip contains the code, while the train.zip contains the data for training. ",Hi  ! Did you get a chance to test in 2.8/2.9 version too? Thank you!,"Ok I test using tf2.9, but the results are almost the same. 0531 10:31:13.006 21767 21767 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: graph=/data/local/tmp/model.tflite warmup_runs=1 num_runs=50 enable_op_profiling=true use_xnnpack=false use_nnapi=true 0531 10:31:13.022 21767 21767 I tflite  : Log parameter values verbosely: [0] 0531 10:31:13.023 21767 21767 I tflite  : Min num runs: [50] 0531 10:31:13.023 21767 21767 I tflite  : Min warmup runs: [1] 0531 10:31:13.023 21767 21767 I tflite  : Graph: [/data/local/tmp/model.tflite] 0531 10:31:13.023 21767 21767 I tflite  : Enable op profiling: [1] 0531 10:31:13.023 21767 21767 I tflite  : Use NNAPI: [1] 0531 10:31:13.036 21767 21767 I tflite  : NNAPI accelerators available: [ipuadaptor,nnapireference] 0531 10:31:13.036 21767 21767 I tflite  : Use xnnpack: [0] 0531 10:31:13.037 21767 21767 I tflite  : Loaded model /data/local/tmp/model.tflite 0531 10:31:13.037 21767 21767 I tflite  : Initialized TensorFlow Lite runtime. 0531 10:31:13.041 21767 21767 I tflite  : Created TensorFlow Lite delegate for NNAPI. 0531 10:31:13.042 21767 21767 I tflite  : NNAPI delegate created. 0531 10:31:13.157 21767 21767 I tflite  : Though NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate. 0531 10:31:13.157 21767 21767 I tflite  : The input model file size (MB): 0.104336 0531 10:31:13.157 21767 21767 I tflite  : Initialized session in 120.712ms. 0531 10:31:13.157 21767 21767 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds. 0531 10:31:13.658 21767 21767 I tflite  : count=946 first=4668 curr=512 min=510 max=4668 avg=525.8 std=137 0531 10:31:13.658 21767 21767 I tflite  : Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds. 0531 10:31:14.661 21767 21767 I tflite  : count=226 first=995 curr=807 min=793 max=1438 avg=854.973 std=85 0531 10:31:14.661 21767 21767 I tflite  : Inference timings in us: Init: 120712, First inference: 4668, Warmup (avg): 525.8, Inference (avg): 854.973 0531 10:31:14.661 21767 21767 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion. 0531 10:31:14.661 21767 21767 I tflite  : Memory footprint delta from the start of the tool (MB): init=2.12109 overall=2.12109 0531 10:31:14.662 21767 21767 I tflite  : Profiling Info for Benchmark Initialization: 0531 10:31:14.662 21767 21767 I tflite  : ============================== Run Order ============================== 0531 10:31:14.662 21767 21767 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0531 10:31:14.662 21767 21767 I tflite  :       ModifyGraphWithDelegate          115.047         115.047        96.029%         96.029%          1216.000              1       ModifyGraphWithDelegate/0 0531 10:31:14.662 21767 21767 I tflite  :               AllocateTensors            4.739           2.379         3.971%        100.000%             0.000              2       AllocateTensors/0 0531 10:31:14.662 21767 21767 I tflite  : 0531 10:31:14.662 21767 21767 I tflite  : ============================== Top by Computation Time ============================== 0531 10:31:14.662 21767 21767 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0531 10:31:14.662 21767 21767 I tflite  :       ModifyGraphWithDelegate          115.047         115.047        96.029%         96.029%          1216.000              1       ModifyGraphWithDelegate/0 0531 10:31:14.662 21767 21767 I tflite  :               AllocateTensors            4.739           2.379         3.971%        100.000%             0.000              2       AllocateTensors/0 0531 10:31:14.662 21767 21767 I tflite  : 0531 10:31:14.662 21767 21767 I tflite  : Number of nodes executed: 2 0531 10:31:14.662 21767 21767 I tflite  : ============================== Summary by node type ============================== 0531 10:31:14.662 21767 21767 I tflite  :                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [ti 0531 10:31:14.676 21767 21767 I tflite  : Operatorwise Profiling Info for Regular Benchmark Runs: 0531 10:31:14.676 21767 21767 I tflite  : ============================== Run Order ============================== 0531 10:31:14.676 21767 21767 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0531 10:31:14.676 21767 21767 I tflite  :                       RESHAPE            0.005           0.009         1.234%          1.234%             0.000              1       [sequential/lstm/transpose1]:0 0531 10:31:14.676 21767 21767 I tflite  :                        UNPACK            0.007           0.011         1.564%          2.798%             0.000              1       [sequential/lstm/unstack, sequential/lstm/unstack1, sequential/lstm/unstack2, sequential/lstm/unstack3, sequential/lstm/unstack4, sequential/lstm/unstack5, sequential/lstm/unstack6, sequential/lstm/unstack7, sequential/lstm/unstack8, sequential/lstm/unstack9, sequential/lstm/unstack10, sequential/lstm/unstack11, sequential/lstm/unstack12, sequential/lstm/unstack13, sequential/lstm/unstack14, sequential/lstm/unstack15, sequential/lstm/unstack16, sequential/lstm/unstack17, sequential/lstm/unstack18, sequential/lstm/unstack19, sequential/lstm/unstack20]:1 0531 10:31:14.676 21767 21767 I tflite  :               FULLY_CONNECTED            0.005 ================================================================================================================================================================================================================================================ Using GPU delegate  0531 10:38:42.569 22206 22206 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: graph=/data/local/tmp/model.tflite warmup_runs=1 num_runs=50 enable_op_profiling=true use_xnnpack=false use_gpu=true 0531 10:38:42.582 22206 22206 I tflite  : Log parameter values verbosely: [0] 0531 10:38:42.583 22206 22206 I tflite  : Min num runs: [50] 0531 10:38:42.583 22206 22206 I tflite  : Min warmup runs: [1] 0531 10:38:42.583 22206 22206 I tflite  : Graph: [/data/local/tmp/model.tflite] 0531 10:38:42.583 22206 22206 I tflite  : Enable op profiling: [1] 0531 10:38:42.583 22206 22206 I tflite  : Use gpu: [1] 0531 10:38:42.583 22206 22206 I tflite  : Use xnnpack: [0] 0531 10:38:42.584 22206 22206 I tflite  : Loaded model /data/local/tmp/model.tflite 0531 10:38:42.585 22206 22206 I tflite  : Initialized TensorFlow Lite runtime. 0531 10:38:42.589 22206 22206 I tflite  : Created TensorFlow Lite delegate for GPU. 0531 10:38:42.589 22206 22206 I tflite  : GPU delegate created. 0531 10:38:42.607 22206 22206 E tflite  : Following operations are not supported by GPU delegate: 0531 10:38:42.607 22206 22206 E tflite  : UNPACK: Operation is not supported. 0531 10:38:42.607 22206 22206 E tflite  : 294 operations will run on the GPU, and the remaining 296 operations will run on the CPU. 0531 10:38:42.608 22206 22206 I tflite  : Replacing 294 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions. 0531 10:38:43.825 22206 22206 I tflite  : Initialized OpenCLbased API. 0531 10:38:43.975 22206 22206 I tflite  : Created 1 GPU delegate kernels. 0531 10:38:43.978 22206 22206 I tflite  : Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels. 0531 10:38:43.978 22206 22206 I tflite  : The input model file size (MB): 0.104336 0531 10:38:43.978 22206 22206 I tflite  : Initialized session in 1394.55ms. 0531 10:38:43.978 22206 22206 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds. 0531 10:38:44.487 22206 22206 I tflite  : count=32 first=25465 curr=13471 min=13109 max=25465 avg=15866.6 std=2987 0531 10:38:44.487 22206 22206 I tflite  : Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds. 0531 10:38:45.495 22206 22206 I tflite  : count=60 first=13655 curr=13218 min=12861 max=15281 avg=13796.8 std=605 0531 10:38:45.495 22206 22206 I tflite  : Inference timings in us: Init: 1394548, First inference: 25465, Warmup (avg): 15866.6, Inference (avg): 13796.8 0531 10:38:45.495 22206 22206 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion. 0531 10:38:45.495 22206 22206 I tflite  : Memory footprint delta from the start of the tool (MB): init=35.0312 overall=36.9297 0531 10:38:45.496 22206 22206 I tflite  : Profiling Info for Benchmark Initialization: 0531 10:38:45.496 22206 22206 I tflite  : ============================== Run Order ============================== 0531 10:38:45.496 22206 22206 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0531 10:38:45.496 22206 22206 I tflite  :       ModifyGraphWithDelegate         1388.938        1388.938        99.810%         99.810%         34928.000              1       ModifyGraphWithDelegate/0 0531 10:38:45.496 22206 22206 I tflite  :               AllocateTensors            2.633           1.323         0.190%        100.000%             0.000              2       AllocateTensors/0 0531 10:38:45.496 22206 22206 I tflite  : 0531 10:38:45.496 22206 22206 I tflite  : ============================== Top by Computation Time ============================== 0531 10:38:45.496 22206 22206 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0531 10:38:45.496 22206 22206 I tflite  :       ModifyGraphWithDelegate         1388.938        1388.938        99.810%         99.810%         34928.000              1       ModifyGraphWithDelegate/0 0531 10:38:45.496 22206 22206 I tflite  :               AllocateTensors            2.633           1.323         0.190%        100.000%             0.000              2       AllocateTensors/0 0531 10:38:45.496 22206 22206 I tflite  : 0531 10:38:45.496 22206 22206 I tflite  : Number of nodes executed: 2 0531 10:38:45.496 22206 22206 I tflite  : ============================== Summary by node type ============================== 0531 10:38:45.496 22206 22206 I tflite  :                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [ti 0531 10:38:45.504 22206 22206 I tflite  : Operatorwise Profiling Info for Regular Benchmark Runs: 0531 10:38:45.504 22206 22206 I tflite  : ============================== Run Order ============================== 0531 10:38:45.504 22206 22206 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name] 0531 10:38:45.504 22206 22206 I tflite  :                       RESHAPE            0.031           0.021         0.157%          0.157%             0.000              1       [sequential/lstm/transpose1]:0 0531 10:38:45.504 22206 22206 I tflite  :                        UNPACK            0.070           0.035         0.255%          0.413%             0.000              1       [sequential/lstm/unstack, sequential/lstm/unstack1, sequential/lstm/unstack2, sequential/lstm/unstack3, sequential/lstm/unstack4, sequential/lstm/unstack5, sequential/lstm/unstack6, sequential/lstm/unstack7, sequential/lstm/unstack8, sequential/lstm/unstack9, sequential/lstm/unstack10, sequential/lstm/unstack11, sequential/lstm/unstack12, sequential/lstm/unstack13, sequential/lstm/unstack14, sequential/lstm/unstack15, sequential/lstm/unstack16, sequential/lstm/unstack17, sequential/lstm/unstack18, sequential/lstm/unstack19, sequential/lstm/unstack20]:1 0531 10:38:45.504 22206 22206 I tflite  :               FULLY_CONNECTED            0.028 As you can see only 294 operations are executed on GPU.",Hi  ! Could you please look at this issue. Thank you!,You can healp me?,Hi  ! We are checking to see whether you still need help in this issue . Could you build the lite model with Fusion Code lab method too (Please check with 3.8 version ) and post the benchmarking results. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
628,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Ragged Tensor in 'batch_jacobian')ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,PatReis,Ragged Tensor in 'batch_jacobian',Click to expand!    Issue Type Feature Request  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-30T11:44:30Z,stat:awaiting tensorflower type:feature comp:ops,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56300,"Any update? It says `experimental_use_pfor`, so I understand that there is a lot of development done in this direction. I just would like to know if something like this will be possible in the future or whether this is a misconception on my side. ","`tape.batch_jacobian` and `tf.vectorized_map` (API of pfor) both don't support RaggedTensor at this moment. We are planning to support both of them. Before that happen, you may try to pad RaggedTensor to tensor.",Okay thanks for the update. I am looking forward to RaggedTensor support.
1837,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Matmul not working on ragged tensors)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.8.01  GPU model and memory _No response_  Current Behaviour? The matmul function on ragged tensors gets the row splits dtype inferred incorrectly, leading to unreadable errors with Keras. I suppose this could be fixed somehow like this:  shell import tensorflow as tf import keras from tensorflow.python.ops.numpy_ops import np_config from scipy.optimize import linear_sum_assignment np_config.enable_numpy_behavior() def apply_normalization(layer, x):     """"""     Supports ragged tensors     """"""     s = tf.shape(x)     x = tf.reshape(x, [1, tf.shape(x)[1]])     y = layer(x)     return tf.reshape(y, s) def foo(*args):     n_elements = tf.reduce_prod(args)     return tf.range(n_elements, dtype=tf.float32).reshape(args) class Attention(keras.layers.Layer):     def __init__(self):         super().__init__()     def build(self, input_shape):         queries, keys, values = input_shape         assert queries[1] == keys[1] == values[1]         model_dim = queries[1]         self.keys_projector = keras.layers.Dense(model_dim)         self.query_projector = keras.layers.Dense(model_dim)         self.value_projector = keras.layers.Dense(model_dim)     def call(self, x, *args, **kwargs) > tf.Tensor:         queries, keys, values = x         queries = self.query_projector(queries)         keys = self.keys_projector(keys)         v)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,misha-antonenko,Matmul not working on ragged tensors,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.8.01  GPU model and memory _No response_  Current Behaviour? The matmul function on ragged tensors gets the row splits dtype inferred incorrectly, leading to unreadable errors with Keras. I suppose this could be fixed somehow like this:  shell import tensorflow as tf import keras from tensorflow.python.ops.numpy_ops import np_config from scipy.optimize import linear_sum_assignment np_config.enable_numpy_behavior() def apply_normalization(layer, x):     """"""     Supports ragged tensors     """"""     s = tf.shape(x)     x = tf.reshape(x, [1, tf.shape(x)[1]])     y = layer(x)     return tf.reshape(y, s) def foo(*args):     n_elements = tf.reduce_prod(args)     return tf.range(n_elements, dtype=tf.float32).reshape(args) class Attention(keras.layers.Layer):     def __init__(self):         super().__init__()     def build(self, input_shape):         queries, keys, values = input_shape         assert queries[1] == keys[1] == values[1]         model_dim = queries[1]         self.keys_projector = keras.layers.Dense(model_dim)         self.query_projector = keras.layers.Dense(model_dim)         self.value_projector = keras.layers.Dense(model_dim)     def call(self, x, *args, **kwargs) > tf.Tensor:         queries, keys, values = x         queries = self.query_projector(queries)         keys = self.keys_projector(keys)         v",2022-05-30T09:22:19Z,stat:awaiting response type:bug stale comp:ops TF 2.9,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56299,antonenko I tried to replicate this issue on colab using TF v2.8.0 and faced a different error. Could you have a look at this gist and confirm the same? Thank you!,"Just to confirm: `TypeError: object of type 'RaggedTensor' has no len()`? So, the version is different. May this be the reason?",antonenko Could you have a look at the updated gist here in TF v2.9.1 where I have faced the below output:  Please let me know if I am missing something to replicate the issue? Thank you!,This `block` refers to an instance of a `ResidualAttentionBlock`. Please excuse me for missing this: ,antonenko Could you have a look at the gist here and confirm the issue? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
661,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to batch dataset using `.batch` and `.padded_batch`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.3.1  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghost,Unable to batch dataset using `.batch` and `.padded_batch`,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.3.1  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-28T18:19:08Z,stat:awaiting response type:bug comp:apis TF 2.9,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56293,", I tried to execute the given code in a different possible way. Kindly find the attached gist and let us know if it works in your case. Thank you",The difference between your version and mine is that you set the batch size to 1 which temporarily fixes the problem but does not allow the use of any batch size other than 1 which is inconvenient as the whole point of storing the dataset in such format is to enable efficient batching.,"I was able to fix the problem by appending the following to `read_example`:         pad_size = max_length  tf.shape(f1)[0]         f1 = tf.pad(f1, [[0, pad_size]])",Are you satisfied with the resolution of your issue? Yes No
743,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error in Ragged Tensor Documentation)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output ValueError: Attempt to convert a value (Ellipsis) with an unsupported type () to a Tensor. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,aflah02,Error in Ragged Tensor Documentation,Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  ```  Relevant log output ValueError: Attempt to convert a value (Ellipsis) with an unsupported type () to a Tensor. ,2022-05-28T16:24:14Z,type:docs-bug stat:awaiting response comp:ops,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56292,"Hi , Kindly feel free to submit a PR for the requested change or share the link where requested change is to be made. Thank you!",Hey  The error is on this page  https://www.tensorflow.org/api_docs/python/tf/RaggedTensor I'm a bit busy would be great if someone else can take up the fix!,", Kindly feel free to submit a PR for the requested change to be made. Thank you!",Hey   I don't know what's the fix here. Would be great if someone else takes this,"Hi , Tf.RaggedTensor does take argument in the form `[...]`. As error says Ellipsis are unsupported. Argument `row_lengths` from `tf.RaggedTensor.from_row_lengths` takes A 1D integer tensor with shape [nrows]. Must be nonnegative. sum(row_lengths) must be nvals.  **Output** ``",Ohhh this makes sense   Thanks a lot,"Hi , Glad that it helps. Can we close this issue. Thank you!", Yeah sure closing it
664,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to decode numpy ndarray stored as `tf.train.BytesList`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.3.1  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghost,How to decode numpy ndarray stored as `tf.train.BytesList`,Click to expand!    Issue Type Others  Source binary  Tensorflow Version 2.9.1  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.3.1  Mobile device _No response_  Python version 3.10.4  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-27T17:09:52Z,type:others,closed,0,1,https://github.com/tensorflow/tensorflow/issues/56288,Hi! Did you find a solution? I am struggling with the same problem in static graph mode. 
639,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Library .so file path Custom Ops using C API)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux   Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,YogaVicky,Library .so file path Custom Ops using C API,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Linux   Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-27T10:34:30Z,stat:awaiting response type:support stale comp:lite TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56286,"Hi, Please follow the instructions mentioned here, which explains the path for .so file and the relevant kernel and header files. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
596,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(bazel build tensorflow but failed)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution ubuntu 20.04  Mobile device none  Python version 3.6.9  Bazel version 5.1.1  GCC/Compiler version 9.3.0  CUDA/cuDNN version none  GPU model and memory none  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Nullptr65535,bazel build tensorflow but failed,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.10.0  Custom Code No  OS Platform and Distribution ubuntu 20.04  Mobile device none  Python version 3.6.9  Bazel version 5.1.1  GCC/Compiler version 9.3.0  CUDA/cuDNN version none  GPU model and memory none  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-27T07:22:28Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux subtype:bazel,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56284,"Hi , We see that, you are trying to install the `tensorflow v2.10` with python `3.6.9 `which is not compatible. Could you try to install with `python >= 3.7 `version and let us know if you are facing same issue. Thank you!","Some error occur when I update python3 to python3.8.10: ERROR: /home/lvhang/.cache/bazel/_bazel_lvhang/ce216ca3f79222179e2eb092d40384de/external/cpuinfo/BUILD.bazel:100:11: Compiling src/arm/linux/chipset.c failed: (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/linuxx86_64/bin/clang gcctoolchain external/androidndk/ndk/toolchains/aarch64linuxandroid4.9/prebuilt/linuxx86_64 target ... (remaining 50 arguments skipped) external/cpuinfo/src/arm/linux/chipset.c:330:11: error: implicitly declaring library function 'tolower' with type 'int (int)' [Werror,Wimplicitfunctiondeclaration]                 int d = tolower((unsigned char)*a)  tolower((unsigned char)*b);                         ^ external/cpuinfo/src/arm/linux/chipset.c:330:11: note: include the header  or explicitly provide a declaration for 'tolower' 1 error generated. Target //tensorflow/lite/examples/label_image:label_image failed to build Use verbose_failures to see the command lines of failed build steps. Maybe include  is one of solutions, but I don't want to do this.","Hi , Have you tried the above query after trying the `bazel clean expunge` command. Can you try once and try repeating the above steps. And also make sure you followed steps mentioned here.Thank you!","OK, problem solved, thanks a lot","Hi , Glad that this issue resolved. Could you move this to closed status. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
707,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The _get_tensor_details() method was crashed while trying to read a tflite model.)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1 (v2.9.018gd8ce9f9c301)  Custom Code No  OS Platform and Distribution Ubuntu 18.04.5 LTS  Mobile device Ubuntu 18.04.5 LTS  Python version 3.8.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fatcat-z,The _get_tensor_details() method was crashed while trying to read a tflite model.,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.9.1 (v2.9.018gd8ce9f9c301)  Custom Code No  OS Platform and Distribution Ubuntu 18.04.5 LTS  Mobile device Ubuntu 18.04.5 LTS  Python version 3.8.5  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-27T05:45:17Z,stat:awaiting tensorflower type:bug comp:lite comp:runtime TF 2.11,closed,0,12,https://github.com/tensorflow/tensorflow/issues/56283,"Hi  ! Could you look at this issue. Attached 2.8, 2.9 and nightly (2.12 dev) for reference. Thank you!","for `get_tensor_details` method, the tensors has to be valid tensor details.  Tensors where required information about the tensor is not found are not added to the list. This includes temporary tensors without a name.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> for `get_tensor_details` method, the tensors has to be valid tensor details. Tensors where required information about the tensor is not found are not added to the list. This includes temporary tensors without a name. Do you mean the data in attached tflite models are incomplete?",z !  ! I am able to replicate this issue in  2.11 version too. Thank you!,"Yes... 2.12 and in last night builds too... It's the same problem with sparse tensors. At all, in this line: https://github.com/tensorflow/tensorflow/blob/0010d29d742350048206a312d485ee1245522f2b/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.ccL135 `param.block_map` is `nullptr`. When it tries to resolve it, a `segmentation fault` is happened.  here i can read https://github.com/tensorflow/tensorflow/blob/0010d29d742350048206a312d485ee1245522f2b/tensorflow/lite/schema/schema.fbsL184 > For an ndimensional tensor with a kdimensional block (0 data,                                              param.traversal_order>size));   PyDict_SetItemString(       result, ""block_map"",       PyArrayFromIntVector(param.block_map>data, param.block_map>size)); + +  if(param.block_map!=nullptr){ +     PyDict_SetItemString( +         result, ""block_map"", +         PyArrayFromIntVector(param.block_map>data, param.block_map>size)); +  }else{ +     PyDict_SetItemString(result, ""block_map"", PyArrayFromIntVector(nullptr,0)); +  } +    PyObject* dim_metadata = PyList_New(param.dim_metadata_size);    for (int i = 0; i      main()   File ""/home/buldonnx/.conda/envs/condatest/lib/python3.10/sitepackages/tf2onnx/convert.py"", line 269, in main     model_proto, _ = _convert_common(   File ""/home/buldonnx/.conda/envs/condatest/lib/python3.10/sitepackages/tf2onnx/convert.py"", line 164, in _convert_common     g = process_tf_graph(tf_graph, const_node_values=const_node_values,   File ""/home/buldonnx/.conda/envs/condatest/lib/python3.10/sitepackages/tf2onnx/tfonnx.py"", line 453, in process_tf_graph     main_g, subgraphs = graphs_from_tflite(tflite_path, input_names, output_names)   File ""/home/buldonnx/.conda/envs/condatest/lib/python3.10/sitepackages/tf2onnx/tflite_utils.py"", line 153, in graphs_from_tflite     parse_tflite_graph(tfl_graph, opcodes, model, prefix, tensor_shapes_from_interpreter)   File ""/home/buldonnx/.conda/envs/condatest/lib/python3.10/sitepackages/tf2onnx/tflite_utils.py"", line 355, in parse_tflite_graph     np_data = tensor_util.MakeNdarray(t)   File ""/home/buldonnx/.local/lib/python3.10/sitepackages/tensorflow/python/framework/tensor_util.py"", line 663, in MakeNdarray     dtype=dtype).copy().reshape(shape)) ValueError: cannot reshape array of size 96 into shape (16,1,1,24) ``` So far, I do not understand how it works, and what is happening here... I will look later. UPD: Really my modification fix this bug. tflite2onnx converter is not working, becouse here is not implemented sparse tensors at all...","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",!image ğŸ˜¢ ,"Hi z, Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here. Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
1874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Potential race condition in nsync library that causes SIGSEGV or deadlock)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.4  Custom Code No  OS Platform and Distribution Linux Redhat 7.6  Mobile device _No response_  Python version 3.7  Bazel version 3.1.0  GCC/Compiler version 7  CUDA/cuDNN version 11.0/8.0.2  GPU model and memory _No response_  Current Behaviour? Our TensorFlow application which uses nsync for mutex/cv implementation failed because of SIGSEGV. The issue can be reproduced during thread pool destruction with threadlocal TraceMeRecorder::ThreadLocalRecorderWrapper used in each thread. After adding logs into nsync library, we are able to identify the root cause.  Root cause Destruction order of threadlocal variables is not deterministic in POSIX. Mutex or conditional variable in TensorFlow is a thread local object. If another thread local object e.g. perthread TraceMe recorder use a lock in its destructor, the lock can already be destructed and the behavior is undefined.  Details For example, the following logs show two threads(0x7f4118748700 and 0x7f4100718700) share the same nsync waiter object (0x7f465c2c2290) and cause SIGSEGV. A nsync waiter object is a thread local object used by nsync to implement a lock. First, the waiter_destroy i.e. lock destructor was called on thread 0x7f4118748700 and the waiter 0x7f465c2c2290 had been recycled into free_waiters. At this moment, the other thread 0x7f4100718700 enter and take the waiter from free_waiters. Then, the old thread 0x7f4118748700 tries to take a lock again as the other threadlocal object use a lock in its destructor. Because the threadlocal state is not cleaned u)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,plliao,Potential race condition in nsync library that causes SIGSEGV or deadlock,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.4  Custom Code No  OS Platform and Distribution Linux Redhat 7.6  Mobile device _No response_  Python version 3.7  Bazel version 3.1.0  GCC/Compiler version 7  CUDA/cuDNN version 11.0/8.0.2  GPU model and memory _No response_  Current Behaviour? Our TensorFlow application which uses nsync for mutex/cv implementation failed because of SIGSEGV. The issue can be reproduced during thread pool destruction with threadlocal TraceMeRecorder::ThreadLocalRecorderWrapper used in each thread. After adding logs into nsync library, we are able to identify the root cause.  Root cause Destruction order of threadlocal variables is not deterministic in POSIX. Mutex or conditional variable in TensorFlow is a thread local object. If another thread local object e.g. perthread TraceMe recorder use a lock in its destructor, the lock can already be destructed and the behavior is undefined.  Details For example, the following logs show two threads(0x7f4118748700 and 0x7f4100718700) share the same nsync waiter object (0x7f465c2c2290) and cause SIGSEGV. A nsync waiter object is a thread local object used by nsync to implement a lock. First, the waiter_destroy i.e. lock destructor was called on thread 0x7f4118748700 and the waiter 0x7f465c2c2290 had been recycled into free_waiters. At this moment, the other thread 0x7f4100718700 enter and take the waiter from free_waiters. Then, the old thread 0x7f4118748700 tries to take a lock again as the other threadlocal object use a lock in its destructor. Because the threadlocal state is not cleaned u",2022-05-26T21:07:09Z,type:bug comp:core TF 2.4 awaiting PR merge,closed,1,8,https://github.com/tensorflow/tensorflow/issues/56280,", In order to expedite the troubleshooting process, could you provide the complete code to reproduce the issue reported here. Thank you!",TF 2.4 is no longer supported.,"Hi , Tensorflow version 2.4 not actively supported. Hence, kindly update to latest stable version 2.9 and let us know if you are facing the same issue. Thank you!","Hi    It is not a bug in TensorFlow but an issue from upstream. The nsync owner had released a new version and I have created a PR to upgrade the nsync library. Please take a look. Thank you so much,",", The related PR has been assigned for reviewing and once it is merged, this issue will move to closed status. Thank you!",Are you satisfied with the resolution of your issue? Yes No,The fix was merged. Close the issue.,Are you satisfied with the resolution of your issue? Yes No
673,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bad file descriptor error during cleanup of MirroredStrategy model)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Linux Red Hat  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.1 / cuDNN 8.2.1.32  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,alexarmstrongvi,Bad file descriptor error during cleanup of MirroredStrategy model,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Linux Red Hat  Mobile device _No response_  Python version 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.1 / cuDNN 8.2.1.32  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-26T18:24:32Z,type:bug comp:dist-strat TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56279,"i meet it too, dose it has any solution?",The error is because of the multiprocessing Threadpool created is not closed properly before the program exits. A fix has already been submitted," , Could you try with the latest nightly version to see the changes from the above linked fix. Thanks!",", the issue is fixed when I run on tf_nightly2.10.0.dev20220616. Thanks!",Are you satisfied with the resolution of your issue? Yes No
1443,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Is it possible to view or dump full tensor values in Debugger V2 GUI (in TensorBoard)?)ï¼Œ å†…å®¹æ˜¯ (I am trying to dump/view intermediate tensor values in a model created using the TF2 ObjectDetector API  I am trying to use the `tf.debugging.experimental.enable_dump_debug_info` for this as follows:  I understand that none of the 5 `tensor_debug_mode` options allow for displaying or dumping tensors produced during the Graph execution. At most we can see the datatype, dimesions, shape and size of these tensors. For example, in the image below, we can see these 4 attributes of the tensor `Postprocessor/Decode/get_center_coordinates_and_sizes/sub_1:0`, (which are: float32 1D shape:[1917]  size:1917) : !image I am wondering if it is possible to somehow view or dump the actual value of these tensors, i.e.: 1. Is there a way to view or dump the 1917 numeric values present in the tensor: `Postprocessor/Decode/get_center_coordinates_and_sizes/sub_1:0` ?  2. Is it  possible to do this using the Debugger V2 and TensorBoard? 3. Is it possible to do this in some other way, without modifying the `object_detection` API code? https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/debug/lib/dumping_callback.pyL683L871)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,meghendra,Is it possible to view or dump full tensor values in Debugger V2 GUI (in TensorBoard)?,"I am trying to dump/view intermediate tensor values in a model created using the TF2 ObjectDetector API  I am trying to use the `tf.debugging.experimental.enable_dump_debug_info` for this as follows:  I understand that none of the 5 `tensor_debug_mode` options allow for displaying or dumping tensors produced during the Graph execution. At most we can see the datatype, dimesions, shape and size of these tensors. For example, in the image below, we can see these 4 attributes of the tensor `Postprocessor/Decode/get_center_coordinates_and_sizes/sub_1:0`, (which are: float32 1D shape:[1917]  size:1917) : !image I am wondering if it is possible to somehow view or dump the actual value of these tensors, i.e.: 1. Is there a way to view or dump the 1917 numeric values present in the tensor: `Postprocessor/Decode/get_center_coordinates_and_sizes/sub_1:0` ?  2. Is it  possible to do this using the Debugger V2 and TensorBoard? 3. Is it possible to do this in some other way, without modifying the `object_detection` API code? https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/debug/lib/dumping_callback.pyL683L871",2022-05-26T16:46:08Z,stat:awaiting response type:bug comp:model,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56277, Please post this issue on models repo to get the right help on this. Thank you!,> sushreebarsa I posted this on models repo: https://github.com/tensorflow/models/issues/10659issue1251067967 But I think this is more of a feature request for `debugging.experimental.enable_dump_debug_info` ? What do you think?," Thank you for the update. This issue is more related to TF models. Please refer this link, it may help! Could you move this ticket to closed status as it can be addressed here. Thank you!",Are you satisfied with the resolution of your issue? Yes No
391,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix SplitV implementation for big tensors)ï¼Œ å†…å®¹æ˜¯ (When an input tensor meets specific conditions, runtime falls into ParallelSplitByInputData which has a little bug in implementation. This PR aims to fix this bug.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,stancelrobert,Fix SplitV implementation for big tensors,"When an input tensor meets specific conditions, runtime falls into ParallelSplitByInputData which has a little bug in implementation. This PR aims to fix this bug.",2022-05-26T15:48:46Z,size:M,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56275,"This change is causing segfaults for internal testing, so something is off with the index math.  Can you doublecheck?","Hi , thanks for your response. What do you mean by internal testing? Are these some hidden tests not publicly available? If so, can you give me some hints about these tests that would help debugging?"," by ""internal"" I mean that with this change, a bunch of TF models at Google start segfaulting at this exact place, so there's probably something off with the index math.  I was hoping you could just take a quick look to doublecheck. If it's not an obvious error, I could try to dig in deeper to create a reproducer.  Or, since you've identified a case where it's wrong, it *could* be that the previous PR that modified this code (and inserted that failing path) was always wrong to begin with, and needs to be rolled back  if it's failing to copy blocks of the input to the output, that likely means the benchmarks used to justify it are likely all wrong.", Any update on this PR? Please. Thank you!,"Hi, sorry for late reply, I was on vacation. I was trying to see if there's a mistake in the logic, should have some updates in a few days.","I looked into this, and fixing the original PR that added this optimized path slows it down considerably, making that PR no longer worth it.  I'll revert it but add your new tests to prevent backslide.  Thanks for catching this!","Sounds good, thanks"
397,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix SplitV implementation for big input tensors)ï¼Œ å†…å®¹æ˜¯ (When an input tensor meets specific conditions, runtime falls into ParallelSplitByInputData which has a little bug in implementation. This PR aims to fix this bug.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,stancelrobert,Fix SplitV implementation for big input tensors,"When an input tensor meets specific conditions, runtime falls into ParallelSplitByInputData which has a little bug in implementation. This PR aims to fix this bug.",2022-05-26T15:41:40Z,size:M,closed,0,1,https://github.com/tensorflow/tensorflow/issues/56274,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
681,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow core operator ExtractImagePatches , not supported)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.6.2 /cuDNN 8.4.0  GPU model and memory RTX 1660Ti and 16 GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ManishwarG,"TensorFlow core operator ExtractImagePatches , not supported",Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Windows 11  Mobile device _No response_  Python version 3.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.6.2 /cuDNN 8.4.0  GPU model and memory RTX 1660Ti and 16 GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-26T11:32:06Z,stat:awaiting response type:feature comp:lite,closed,0,2,https://github.com/tensorflow/tensorflow/issues/56270,"Hi  ! This ticket seems to be a duplicate of this issue, could you confirm and close this one? We will track the other one. Thank you! ", Thank you!
666,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting the bounding box coordinates on tensorflow lite )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device Android  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DemoLV,Getting the bounding box coordinates on tensorflow lite ,Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device Android  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-25T21:28:14Z,type:support comp:lite TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56262,"Hi  ! You can get the bounding box coordinates from the output of  > vis_util.visualize_boxes_and_labels_on_image_array()   .  Attached are relevant threads for reference. 1, 2. Thank you!",Could you advise on how to implement this code in MultiBoxTracker.java ? If needed I can send a screenshot on how I get the output of the count of classes on the screen. Thank you !,"Hello I resolved this issue, you can close it !", ! Thanks for the update. Moving this issue to closed status then.,Are you satisfied with the resolution of your issue? Yes No
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Using front camera for android with tensorflow lite)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device Android  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,DemoLV,Using front camera for android with tensorflow lite,Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device Android  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-25T21:14:37Z,stat:awaiting response type:bug comp:lite TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56260,"Maybe someone have some ideas on how to resolve the issue ?  The problem is with the bounding boxes, the front camera does not detect objects precisely if it detects, then it floats somewhere. Whenever I toggle camera to back, everything works like it should.  Thank you again ! I will wait for the reply.","Hello, You can close this issue. I resolved it !"," , Could you please mention what changes solved your issue, if it is a workaround, we can keep this issue open and look at the issue in detail.  Feel free to close the issue if the issue is not a bug from our end. Thanks!",Sorry for the late response. I was away. So basically for now it seems that it was a phone issue. I tested it on newest Xiomi and everything works.  But could that be true ?  I have samsung galaxy S9 and it did not work.,"    I have an idea on this one. Maybe I can create this simple app ""Wake Lock ON"" and in this app I choose to press the button wake Lock on and then I open the Tensorflow Lite application and then I can lock the phone and everything will work ?  Or that will not work as I imagined.","Since you have already created a new issue on the below point here https://github.com/tensorflow/tensorflow/issues/56261, could you please close this issue and track the below request in the new issue, Thanks! >    I have an idea on this one. Maybe I can create this simple app ""Wake Lock ON"" and in this app I choose to press the button wake Lock on and then I open the Tensorflow Lite application and then I can lock the phone and everything will work ? Or that will not work as I imagined.",So close issue CC(Using front camera for android with tensorflow lite) ?  Sorry did not quite understand.,If your issue is resolved in this thread then you can close this issue. ,"Ok, closing this issue. Please go to this issue where it is opened CC(Detecting objects while android phone is locked/sleep) about detecting objects while the phone is locked/sleep.",Are you satisfied with the resolution of your issue? Yes No
1345,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dataset from generator is far slower than from tensor slices, anything I can improve?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version tf 1.15.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output When using from_tensor_slices, average get batch time:  0.001715s When using from_generator, average get batch time:  0.10747s [0.14395s for tf2.8] If I change the generator function as follows and rerun the above program:  Namely if I get the value 10 times in the generator, but only yield one element, the performance is much faster compared with yield 10 features: average get batch time:  0.02014s, even though it is still much slower than from_tensor_slices. Why is that? I suppose the computation is the same, the only difference is how many elements I output? (BTW if I use tf2.8, in this case, it is 0.3935s, unreasonable... don't know why...))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hkvision,"Dataset from generator is far slower than from tensor slices, anything I can improve?","Click to expand!    Issue Type Performance  Source binary  Tensorflow Version tf 1.15.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output When using from_tensor_slices, average get batch time:  0.001715s When using from_generator, average get batch time:  0.10747s [0.14395s for tf2.8] If I change the generator function as follows and rerun the above program:  Namely if I get the value 10 times in the generator, but only yield one element, the performance is much faster compared with yield 10 features: average get batch time:  0.02014s, even though it is still much slower than from_tensor_slices. Why is that? I suppose the computation is the same, the only difference is how many elements I output? (BTW if I use tf2.8, in this case, it is 0.3935s, unreasonable... don't know why...)",2022-05-25T13:12:18Z,stat:awaiting response comp:data type:performance TF 2.8,closed,0,22,https://github.com/tensorflow/tensorflow/issues/56258," , I was able to execute the code without any issues in tensorflow v2.8 and also the average batch time for both are similar.   Please find the gist of it here. Thank you! ","Hi   Thanks for your reply! But in your notebook, the two code segments are both based on from_tensor_slices, you forgot to change to from_generator...ğŸ˜‚ I changed to from_generator:  and I get the result: Average get batch time:  0.3002346827059376, more than 100x slower... could you help verify this and give some help on how to improve?",", I was facing different error while trying to execute the mentioned code. Please find the gist of it here.",Changing  to   should work. Or change the generator function to return 10 results. Take a further look?," , I was able to reproduce the issue in tensorflow v2.8, v2.9 and nightly. Kindly find the attached gist.","Hi, arguments like `output_shapes, output_types` will be deprecated in future version, can you check with `output_signature` and let us know if you find similar behavior. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,I will check `output_signature`... mlbutler Please wait and not close this issue at this moment :)," , Any update from the above comment, did you find any difference in performance, also try using tfnightly and latest Tensorflow 2.9.1 version.",I will test and give you the result as soon as possible.,"`Dataset.from_generator` is generally not as performant as other ways of using tf.data, since it executes the input generation logic in Python instead of C++. In particular, the generator will always run singlethreaded, while computation expressed with other tf.data transformations can be run with many threads in parallel. If performance is critical, prefer other APIs like `Dataset.from_tensor_slices`"," Okay, I understand, thank you for your reply!"," , If your concern is addressed, could you please close the issue. Thanks!",Sure. Thanks for the help.,"> `Dataset.from_generator` is generally not as performant as other ways of using tf.data, since it executes the input generation logic in Python instead of C++. In particular, the generator will always run singlethreaded, while computation expressed with other tf.data transformations can be run with many threads in parallel. If performance is critical, prefer other APIs like `Dataset.from_tensor_slices`     The problem with`Dataset.from_tensor_slices` is that it is unsuitable for big datasets, while `Dataset.from_generator` will work. There are many technical problems with `from_generator`, and the main one, in my opinion, is that loading time is extremely slow compared to `Dataset.from_tensor_slices`. in my case, using generators slows the training in x3. This is a big problem! Therefore, I think this issue should be open and treated as for big datasets, `Dataset.from_generator` should provide an easy solution.","If you care about performance with large datasets try using a source dataset appropriate for your data format, such as `tf.data.TFRecordDataset`. Trying to make Python input processing with `from_generator` as performant as C++ processing is going to be an uphill battle.","> If you care about performance with large datasets try using a source dataset appropriate for your data format, such as `tf.data.TFRecordDataset`. Trying to make Python input processing with `from_generator` as performant as C++ processing is going to be an uphill battle.   I used tf.data.TFRecordDataset, however, the writingtofile process is not scaling up well. I tried to write 400k samples into a TFRecord file, which took me a couple of minutes. However, when I wrote 40 million samples, It took me around 10 hours. I think that there is a problem with the way tf.io.TFRecordWriter is writing. We cannot control the buffer which determines when to perform disk access  meaning that I do not know how to write in bulks of specific size. Note that this is my assumption for not being able to scale up well.","It might help to write to multiple files in parallel. If you use Dataset.save/Dataset.load, it will automatically parallelize the writing for you.","> It might help to write to multiple files in parallel. If you use Dataset.save/Dataset.load, it will automatically parallelize the writing for you.   This is indeed one option; however, it seems that even without parallelizing the process, the execution time should be linear as a function of the number of samples, but it is not linear. Therefore, I assume that the bottleneck is the number of disk access that costs too much. If that is the problem, I think that allowing to write to disk in bulks will improve the execution time significantly (and control the bulk/buffer size). It might be that I'm wrong, and it already writes in bulks, but I'm not familiar with the interior implementation. Thank you"," I did a little test. This is part of my code (I know that each time I overwrite the file repeatedly). I followed the file size during the process and found that the increase in size in each update is very small (probably less than MB). This confirms what I thought, and the bottleneck is in the number of times `tf.data.experimental.TFRecordWriter` writes. This is also the case with `tf.io.TFRecordWriter`. ","> It might help to write to multiple files in parallel. If you use Dataset.save/Dataset.load, it will automatically parallelize the writing for you.   I got additional questions regarding this answer. I tried to divide the dataset and parallelize the writing using multiprocessing (each worker writes with `tf.io.TFRecordWriter`), but it did not work for me. Would you happen to have a template for how to do that? Regarding the `tf.data.Dataset.save/load`, Does it perform the same as TFrecords? The advantage of using TFrecords is being able to load part of the data each time and, therefore, not getting memory allocation problems.","> I got additional questions regarding this answer. I tried to divide the dataset and parallelize the writing using multiprocessing (each worker writes with tf.io.TFRecordWriter), but it did not work for me. Would you happen to have a template for how to do that? Sorry, I don't have a template for that. > Regarding the tf.data.Dataset.save/load, Does it perform the same as TFrecords? The advantage of using TFrecords is being able to load part of the data each time and, therefore, not getting memory allocation problems. The `tf.data.Dataset.save/load` implementation currently uses TFRecords under the hood, so the performance will be very similar to using TFRecords directly."
1025,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tflite nightly/2.9.1 on iOS (CocoaPods) linker/build issue)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Build/Install  Source binary  Tensorflow Version 2.9.1 and 0.0.1nightly.20220524  Custom Code No  OS Platform and Distribution MacOS 12.1, XCode 13.4, CocoaPod 1.11.3  Mobile device iOS 15  Python version n/a  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour? Compiling a simple iOS project with this podfile:  and adding `force_load ""$(PROJECT_DIR)Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/iosarm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps""` results in linker error (see below).  Standalone code to reproduce the issue n/a  Relevant log output  This seems similar to: https://github.com/tensorflow/tensorflow/issues/52042issuecomment1098076930)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sschaetz,tflite nightly/2.9.1 on iOS (CocoaPods) linker/build issue,"  Issue Type Build/Install  Source binary  Tensorflow Version 2.9.1 and 0.0.1nightly.20220524  Custom Code No  OS Platform and Distribution MacOS 12.1, XCode 13.4, CocoaPod 1.11.3  Mobile device iOS 15  Python version n/a  Bazel version n/a  GCC/Compiler version n/a  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour? Compiling a simple iOS project with this podfile:  and adding `force_load ""$(PROJECT_DIR)Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/iosarm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps""` results in linker error (see below).  Standalone code to reproduce the issue n/a  Relevant log output  This seems similar to: https://github.com/tensorflow/tensorflow/issues/52042issuecomment1098076930",2022-05-25T09:36:00Z,stat:awaiting tensorflower type:build/install comp:lite subtype:macOS,closed,2,22,https://github.com/tensorflow/tensorflow/issues/56255,"Update: I'm able to reproduce this with the sound classification example. Steps to reproduce:  `git clone https://github.com/tensorflow/examples.git`  `cd lite/examples/sound_classification/ios`  `pod update`  `pod install`  `open SoundClassification.xcworkspace`  fix development team  build > error ""Could not find `TensorFlowLiteSelectTfOps`""  fix build error by patching project file with    ```patch    a/lite/examples/sound_classification/ios/SoundClassification.xcodeproj/project.pbxproj   +++ b/lite/examples/sound_classification/ios/SoundClassification.xcodeproj/project.pbxproj   @@ 525,11 +525,11 @@                                         ""weak_framework"",                                         ""\""CoreML\"""",                                         ""force_load"",                                          ""$(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"",   +                                       ""$(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/iosarm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"",                                 );   @@ 561,11 +561,11 @@                                         ""weak_framework"",                                         ""\""CoreML\"""",                                         ""force_load"",                                          ""$(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"",   +                                       ""$(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/iosarm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"",  rebuild  error: tflite_ios_build_fail.log",If I force the pods to 0.0.1nightly.20201122 it works fine. So some time between `20201122` and `20220524` things broke.,"For what it's worth, I hit on this issue.   I also changed the force_load settings as [sschaetz] did above.  Now what I'm faced with are a large set of undefined symbols, mostly around a 3rd party matrix multiplication package for arm called 'ruy'. Undefined symbol: ruy::ThreadPool::ExecuteImpl(int, int, ruy::Task*) Additionally it has undefined symbols in tflite: Undefined symbol: tflite::CheckedLog2(float, int*) I've tried nightly and  0.0.1nightly.20201122 for both Pods.    Any suggestion?",Note the documentation indicates the wrong path should be used for force_load https://www.tensorflow.org/lite/guide/ops_selectios force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps,"> For what it's worth, I hit on this issue. I also changed the force_load settings as [sschaetz] did above. Now what I'm faced with are a large set of undefined symbols, mostly around a 3rd party matrix multiplication package for arm called 'ruy'. >  > Undefined symbol: ruy::ThreadPool::ExecuteImpl(int, int, ruy::Task*) >  > Additionally it has undefined symbols in tflite: >  > Undefined symbol: tflite::CheckedLog2(float, int*) >  > I've tried nightly and 0.0.1nightly.20201122 for both Pods. >  > Any suggestion? I just remove the force_load flag and the build is ok"," I also had this issue. My model need to use flexDelegate so i added ""force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"" to ""Other Linker Flags"" but when i built an app there was many ""Undefined symbols for architecture arm64"" bug. Downgrade `TensorFlowLiteSwift ` and `TensorFlowLiteSelectTfOps ` to 2.6.0 work fine on IOS but my model need to use tf 2.9.x to be able to convert GroupConvolution to tflite.", have you fixed the issue yet?., can you take a look ? ,Any update on this issue?,", Could you try to build with the latest version and check if you're able to build successfully. Refer the document here https://www.tensorflow.org/lite/guide/ops_selectusing_bazel_xcode. If it is failing, as suggested in above comments, could you try building by removing `force_load` flag. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,sorry guys I was not able to find a work around.,"Hi really sorry for the late reply. I'm working on a fix. In the meantime, can you try tensorflow/lite/ios/build_frameworks.sh? You might need to add `config=monolithic` when building TensorFlowLiteSelectTfOps_framework.","Hi, the fix should be in the latest nightly version (0.0.1nightly.20220830), could you give it a try? Thanks!",Thanks YishuangP!  Can you share a command/build line for iOS?,"Hi, you just need to update your pod file to depend on the latest nightly build version.  Or your could hardcode the build version to 0.0.1nightly.20220830  Then run `pod update` and `pod install`","Hi sorry we had an issue in our script that builds the xcframework. Submitting the fix, hopefully tonight's build will resolve the issue.","Hi, I just verified that this build issue is resolved in build `0.0.1nightly.20220831`. Unfortunately the fix won't be in 2.9.1, if you want to use 2.9.1, you can try building the framework using tensorflow/lite/ios/build_frameworks.sh. Besides, make sure you add this build flag `define=tflite_with_xnnpack=false` when building the flex delegate framework. ",Are you satisfied with the resolution of your issue? Yes No,I still have the same issue. Is there any fix that exist ? 
1864,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF Lite Ondevice Training - Training Problem With Fashion MNIST)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 4.2.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version   GPU model and memory   Current Behaviour? refer site : https://www.tensorflow.org/lite/examples/on_device_training/overview I'm Converting model to Tensorflow Lite and Testing on Linux Ubuntu 20.04. Trying ReTraining with signature('train'), Fashion MNIST I remade the model several times, checked the shape of the data, but it was the same as in Python. However, proceed with the training, I can see the ""nan"" value, incorrect data as shown in the log below. What's the problem? Here is my Build Command  ADD below to tensorflow/lite/BUILD   Standalone code to reproduce the issue ```shell include  include  include  include  include  include  include  include  include  include  include  include  include ""tensorflow/lite/interpreter.h"" include ""tensorflow/lite/interpreter_builder.h"" include ""tensorflow/lite/kernels/register.h"" include ""tensorflow/lite/model.h"" include ""tensorflow/lite/optional_debug_tools.h"" include ""tensorflow/lite/signature_runner.h"" include ""mnist/mnist_reader.hpp"" define BATCH_SIZE 100 define EPOCH 100 inline bool is_little_indian() { 	int val = 1; 	return *reinterpret_cast(&val) != 0; } uint32_t reverse_indian(uint32_t val) { 	val = ((val > 8) & 0xFF00FF); 	return (val > 16); } int main(int argc, char* argv[]) { 	const char* model_filename = argv[1];     const int num_epochs = EPOCH;     mnist::MNIST_dat)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DonghakPark,TF Lite Ondevice Training - Training Problem With Fashion MNIST,"Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 4.2.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version   GPU model and memory   Current Behaviour? refer site : https://www.tensorflow.org/lite/examples/on_device_training/overview I'm Converting model to Tensorflow Lite and Testing on Linux Ubuntu 20.04. Trying ReTraining with signature('train'), Fashion MNIST I remade the model several times, checked the shape of the data, but it was the same as in Python. However, proceed with the training, I can see the ""nan"" value, incorrect data as shown in the log below. What's the problem? Here is my Build Command  ADD below to tensorflow/lite/BUILD   Standalone code to reproduce the issue ```shell include  include  include  include  include  include  include  include  include  include  include  include  include ""tensorflow/lite/interpreter.h"" include ""tensorflow/lite/interpreter_builder.h"" include ""tensorflow/lite/kernels/register.h"" include ""tensorflow/lite/model.h"" include ""tensorflow/lite/optional_debug_tools.h"" include ""tensorflow/lite/signature_runner.h"" include ""mnist/mnist_reader.hpp"" define BATCH_SIZE 100 define EPOCH 100 inline bool is_little_indian() { 	int val = 1; 	return *reinterpret_cast(&val) != 0; } uint32_t reverse_indian(uint32_t val) { 	val = ((val > 8) & 0xFF00FF); 	return (val > 16); } int main(int argc, char* argv[]) { 	const char* model_filename = argv[1];     const int num_epochs = EPOCH;     mnist::MNIST_dat",2022-05-25T02:54:22Z,stat:awaiting tensorflower comp:lite subtype: ubuntu/linux type:performance TF 2.8,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56251,I solved this issue by using different data feeding method.,what was the approach can u share the code?,"> what was the approach can u share the code? Hi ankit  Unfortunately, I do not currently possess any code. but as i remember you can call input tensor from tflite file and feed sequencially then they work correctly"
664,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multiple problems building TFLite 2.9.1)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution iOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  Compared to 2.7.1:  ``` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,w3sip,Multiple problems building TFLite 2.9.1,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.1  Custom Code No  OS Platform and Distribution iOS  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  Compared to 2.7.1:  ``` ,2022-05-25T00:45:29Z,stat:awaiting response type:build/install stale comp:lite TF 2.9,closed,0,16,https://github.com/tensorflow/tensorflow/issues/56250,"P.S. Would it ever be (or is it already) possible to build TFLite for iOS with straight CMake, avoiding bazel?",It looks like the symlinks bazel creates point to a wrong folder:  while the actual build product is found here ,Hi  ! Thanks for reporting those bugs.  ! Could you please look at this issue. Thank you!,"It appears there was a regression in commit 848e84c5a3acda31847457cbd52742436e024984. tensorflow/lite/c/common.c was renamed to common.cc, but tensorflow/lite/c/CMakeLists.txt hasn't been updated to reflect that.","Additionally, once all of the above are patched, and tflite builds, my project (previously successfully linking and running with 2.7.1), now fails to link with     (more similar errors wrt absl::lts_20211102 namespace omitted for brevity).","Any help with this, guys? What changed between 2.7.1 and 2.9.1 to cause the absl errors? ",Here's an example of symbols in 2.7.1 vs 2.9.1 resulting frameworks: 2.9.1: ," I'm looking into it, for the first 2 points which you have mentioned, I have created a PR.","Not sure if related, but after adding the changes in CC(Update TFL_MINIMUM_OS_VERSION) I still get the following errors   When running `bazel build c opt config=ios_fat //tensorflow/lite/swift:TensorFlowLite_framework`. Does anyone have any advice? I'm trying to run tests locally for some changes I'm working on.","> It looks like the symlinks bazel creates point to a wrong folder: >  >  >  > while the actual build product is found here >  >  FYI. I ran into this problem couple months ago, it was caused by bazel 5.x. My workaround is to use `ipa_post_processor` to copy what I wanted to place with definite name, see what I did at https://github.com/mlcommons/mobile_app_open/pull/268/commits/00cace81827a6927f4531ff2aa4ca2419c536f1a","> Here's an example of symbols in 2.7.1 vs 2.9.1 resulting frameworks: >  > 2.9.1: >  >  ABSL when compiled with different C++ standard (e.g., std=c++14 vs. std=c++17) may result in different binary. Some TFLite component changed to used C++17 (e.g., metal delegate), so a quick solution is to add `cxxopt=std=c++17 host_cxxopt=std=c++17` to you bazel build flags.",">  > ABSL when compiled with different C++ standard (e.g., std=c++14 vs. std=c++17) may result in different binary. Some TFLite component changed to used C++17 (e.g., metal delegate), so a quick solution is to add `cxxopt=std=c++17 host_cxxopt=std=c++17` to you bazel build flags. Thanks  , that fixed it!"," , If your issue is resolved, could you please close this issue. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
818,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove or bump upper bound for `gast` dependency)ï¼Œ å†…å®¹æ˜¯ (In https://github.com/tensorflow/tensorflow/commit/c762c4501ca017994c1fa5554c3c8e47b7c80b66  said: > I also know that gast 0.5.2 breaks one of our tests, so I've pinned the gast version to 0.4.0 or below, which is what our CI currently installs for testing. and pinned gast to 0.4.0, which is by now almost two years old. The added `TODO(angerson): File a bug for these incompatible tests and the limitation` eventually got removed in https://github.com/tensorflow/tensorflow/commit/0c8b4f2bed6e22e797f576fb553a9a9a9145711e, but I cannot find an issue for it, hence opening this one.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,h-vetinari,Remove or bump upper bound for `gast` dependency,"In https://github.com/tensorflow/tensorflow/commit/c762c4501ca017994c1fa5554c3c8e47b7c80b66  said: > I also know that gast 0.5.2 breaks one of our tests, so I've pinned the gast version to 0.4.0 or below, which is what our CI currently installs for testing. and pinned gast to 0.4.0, which is by now almost two years old. The added `TODO(angerson): File a bug for these incompatible tests and the limitation` eventually got removed in https://github.com/tensorflow/tensorflow/commit/0c8b4f2bed6e22e797f576fb553a9a9a9145711e, but I cannot find an issue for it, hence opening this one.",2022-05-24T15:21:42Z,stat:awaiting tensorflower type:bug type:build/install TF 2.9,closed,0,13,https://github.com/tensorflow/tensorflow/issues/56244,I am starting working on it. I will get back to you soon,  Any update on this?,">  Any update on this? This would be very helpful: this is preventing the installation of SageMath (through pythran) side by side with tensorflow, which we would want to have on our JupyterHub. For a silly practical reason (we have a fat monoimage for now) and a good reason: encouraging users to combine machine learning and computer algebra. Thanks in advance!",gast 0.4.0 is coming up on three years old in July and the pin still exists on master. Is there some way this can be prioritized?,Yes. It's getting more and more problematic to install tensorflow side by side with other packages...,"Could someone please have a look at raising the cap here? I know things in FOSS can take time, but this TODO (and issue) is now over a year old, and is creating everlarger installation problems in a wide variety of scenarios. CC   ","Can you send a PR please? This way people can run the tests and if all passes it will be taken in. I think checking the status of dependencies, sadly, is a low priority for the team.",> Can you send a PR please? This way people can run the tests and if all passes it will be taken in. Done: https://github.com/tensorflow/tensorflow/pull/61134,Thank you! ğŸ‰ ,Thank you for fixing this in CC(unpin gast dependency for pip_package)!,Are you satisfied with the resolution of your issue? Yes No,Thank you for this update ! Now waiting for a new release of tensorflow with this change.,"Branch cut is in a few weeks, after which the final release should arrive in a month (though the releases in the past year have been delayed more and more, almost 3 months between branch cut and final release for the last release)"
1479,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(non-DMA-copy attempted of tensor type: string error when trying to run roberta-base example from tfhub)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.5  Custom Code No  OS Platform and Distribution Ubuntu 18.04.5  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to run the following example for robertabase, copied from the documentation on tfhub. However, it seems unhappy with how the input is being fed into the model. I am getting `Invalid argument: During Variant Host>Device Copy: nonDMAcopy attempted of tensor type: string`.   If I instead use the preprocessor and encoder from BERT instead of RoBERTa, the code above works.   In addition, the code above with the RoBERTa preprocessor/encoder seems to work if I use CPU instead of GPU (adding `with tf.device('/cpu:0')`), but this is not feasible because I need to finetune a model on lots of data.    Standalone code to reproduce the issue I am using tensorflow version 2.5 and tensoflow_hub version 0.12. Here is a link to the model on tfhub: https://tfhub.dev/jeongukjae/roberta_en_cased_L12_H768_A12/1.  Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,preethiseshadri518,non-DMA-copy attempted of tensor type: string error when trying to run roberta-base example from tfhub,"Click to expand!    Issue Type Documentation Bug  Source source  Tensorflow Version 2.5  Custom Code No  OS Platform and Distribution Ubuntu 18.04.5  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I am trying to run the following example for robertabase, copied from the documentation on tfhub. However, it seems unhappy with how the input is being fed into the model. I am getting `Invalid argument: During Variant Host>Device Copy: nonDMAcopy attempted of tensor type: string`.   If I instead use the preprocessor and encoder from BERT instead of RoBERTa, the code above works.   In addition, the code above with the RoBERTa preprocessor/encoder seems to work if I use CPU instead of GPU (adding `with tf.device('/cpu:0')`), but this is not feasible because I need to finetune a model on lots of data.    Standalone code to reproduce the issue I am using tensorflow version 2.5 and tensoflow_hub version 0.12. Here is a link to the model on tfhub: https://tfhub.dev/jeongukjae/roberta_en_cased_L12_H768_A12/1.  Relevant log output  ",2022-05-23T23:33:53Z,stat:awaiting response type:bug TF 2.5,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56230, I was able to replicate the issue on cpu and gpu runtime. Please find the attached gists and for any further queries kindly post this issue on hub  repo to get the right help there. Thank you!,"Thank you, will post there. ",Are you satisfied with the resolution of your issue? Yes No
693,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Ragged reduce_variance returns negative value & reduce_std -> NaN)ï¼Œ å†…å®¹æ˜¯ (Click to expand!  This is not a contribution.   Issue Type Bug  Source binary  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Linux, macOS  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,peteboothroyd,Ragged reduce_variance returns negative value & reduce_std -> NaN,"Click to expand!  This is not a contribution.   Issue Type Bug  Source binary  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution Linux, macOS  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_",2022-05-23T12:19:17Z,stat:awaiting tensorflower type:bug comp:ops TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56222, Could you please have a look at this gist and confirm the issue? Thank you!, yes that's the issue,Are you satisfied with the resolution of your issue? Yes No
727,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('Unable to map file to memory buffer' - audioClassifier runtimeError)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.9  Custom Code Yes  OS Platform and Distribution Mac OSX Monterey 12.2.1  Mobile device _No response_  Python version 3.9.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue (`classifier` towards the end fails to get defined)   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,harryl-MoD,'Unable to map file to memory buffer' - audioClassifier runtimeError,Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.9  Custom Code Yes  OS Platform and Distribution Mac OSX Monterey 12.2.1  Mobile device _No response_  Python version 3.9.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue (`classifier` towards the end fails to get defined)   Relevant log output  ,2022-05-22T18:29:10Z,stat:awaiting response type:support stale comp:lite TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56210,"Hi  ! Could you please look at this issue. Attached gist in 2.8, 2.9 and nightly for reference. Thank you!",Thanks    if it helps  I've put some more wip context here in a stackoverflow question: https://stackoverflow.com/questions/72352129/customaudiotensorflowmodelhowtoruninference,"Ah, looks as though the runtimeError was being caused by a typo in my code.... the base options should take a filename, not just the model path. ie this snippet: `base_options = core.BaseOptions(file_name=models_path)` has to change to this: `base_options = core.BaseOptions(file_name=f'./{models_path}/{cries_model_filename}')`   are you able to update this page to reflect these changes please? (I can't seem to find this page in the repo to request a PR) https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifierstep_2_using_the_model_2"," , To make it more generic changing it to `models_file_path` from `models_path` would be more relevant. Also, have you tried the below option with `model_file_path`? ` classifier = audio.AudioClassifier.create_from_file(model_path)`",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
660,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('only supports NHWC tensor format' in Conv3D on M1 Mac)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution MacOS Monterey  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mj-mayank,'only supports NHWC tensor format' in Conv3D on M1 Mac,Click to expand!    Issue Type Support  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution MacOS Monterey  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-22T04:18:11Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,11,https://github.com/tensorflow/tensorflow/issues/56205,"Hi mayank, In order to expedite the troubleshooting process, could you provide the complete code to reproduce the issue reported here.Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"> Hi mayank, > In order to expedite the troubleshooting process, could you provide the complete code to reproduce the issue reported here.Thank you! This is the complete code. Could you please elaborate on what is needed for complete code?","mayank, I was facing a different issue while trying to execute the given code. Kindly find the gist of it here and provide the complete code to reproduce the issue. Thank you!"," below is the full code: `input_shape = (30, 100, 100, 3) write your model here model_1 = Sequential()        model_1.add(Conv3D(8,kernel_size=(3,3,3),padding='same',input_shape=(30, 120, 120, 3))) model_1.add(BatchNormalization()) model_1.add(Activation('relu')) model_1.add(Conv3D(16, (3, 3, 3), padding='same')) model_1.add(Activation('relu')) model_1.add(BatchNormalization()) model_1.add(MaxPooling3D(pool_size=(2, 2, 2))) model_1.add(Conv3D(32, (2, 2, 2), padding='same')) model_1.add(Activation('relu')) model_1.add(BatchNormalization()) model_1.add(MaxPooling3D(pool_size=(2, 2, 2))) model_1.add(Conv3D(64, (2, 2, 2), padding='same')) model_1.add(Activation('relu')) model_1.add(BatchNormalization()) model_1.add(MaxPooling3D(pool_size=(2, 2, 2))) model_1.add(Conv3D(128, (2, 2, 2), padding='same')) model_1.add(Activation('relu')) model_1.add(BatchNormalization()) model_1.add(MaxPooling3D(pool_size=(2, 2, 2)))        Flatten layer  model_1.add(Flatten()) model_1.add(Dense(1000, activation='relu')) model_1.add(Dropout(0.5)) model_1.add(Dense(500, activation='relu')) model_1.add(Dropout(0.5)) Softmax layer model_1.add(Dense(5, activation='softmax')) optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e6, momentum=0.7, nesterov=True) write your optimizer optimiser = tf.keras.optimizers.Adam(lr=0.001)write your optimizer model_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy']) print (model_1.summary()) train_generator = generator(train_path, train_doc, batch_size) val_generator = generator(val_path, val_doc, batch_size) model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/' if not os.path.exists(model_name):     os.mkdir(model_name) filepath = model_name + 'model{epoch:05d}{loss:.5f}{categorical_accuracy:.5f}{val_loss:.5f}{val_categorical_accuracy:.5f}.h5' checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1) LR = ReduceLROnPlateau(monitor='val_accuracy',      patience=5,      verbose=1,      factor=0.5,      min_lr=0.00001) write the REducelronplateau code here callbacks_list = [checkpoint, LR] if (num_train_sequences%batch_size) == 0:     steps_per_epoch = int(num_train_sequences/batch_size) else:     steps_per_epoch = (num_train_sequences//batch_size) + 1 if (num_val_sequences%batch_size) == 0:     validation_steps = int(num_val_sequences/batch_size) else:     validation_steps = (num_val_sequences//batch_size) + 1 model_1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, callbacks=callbacks_list, validation_data=val_generator, validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0) `","mayank, Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!", opened issue at new reposatory: https://github.com/kerasteam/tfkeras/issues/554,"mayank, Could you feel free to move this issue to closed status, so that we can track the issue there. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
689,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DenseNet PB and TFLite models produce inaccurate results)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source binary  Tensorflow Version TF 2.8.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04 4LTS  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Intel UHD Graphics 620  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,luludak,DenseNet PB and TFLite models produce inaccurate results,Click to expand!    Issue Type Performance  Source binary  Tensorflow Version TF 2.8.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04 4LTS  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory Intel UHD Graphics 620  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-20T11:18:02Z,stat:awaiting response stale comp:lite type:performance TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56183,Hi  ! Could you please share a minimal standalone code to replicate the issue.,"Hi  . My code follows precisely the model compilation and execution procedure defined in TVM (tutorials). In addition, I prepared and setup preprocessing as set in the VGG Preprocessing, unchanged. Please note that this was the suggestion for this specific model wherever I searched on that. In practice, I load the model, and for each image from its validation dataset, I convert it to a NumPy tensor and use VGG Preprocessing. Then I execute it using TVM and I extract the predictions. I tend to believe there is 1. either a problem with the model version or 2. preprocessing causes problems to the models. Can you please provide some insight on this?",Hi  ! Could you please look at this issue?,"TVM isn't part of the TensorFlow ecosystem so unfortunately we don't have anyone familiar with it at Google. If you want to run inference with TF models, I'd suggest checking out: 1. TF Serving if you want to run inference on the server. 2. TFLite if you want to run inference on edge devices.",Hi  and thank you for your reply. My main concern in terms of TensorFlow is on the use of preprocessing. Is the one I have found the correct one for these specific models?,"Hi , can you please take a look? Thanks.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
682,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.image.resize` different result when inside a `tf.function`)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.9.0rc242g8a20d54a3c1 2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fredo838,`tf.image.resize` different result when inside a `tf.function`,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.9.0rc242g8a20d54a3c1 2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-20T08:57:12Z,type:bug comp:tf.function TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/56181,I tested it with tensorflow `2.10` as well (`tfnightly`). ,Can you check if It Is the same case as: https://github.com/tensorflow/tensorflow/issues/55814issuecomment1115034411 / ,"E.g:  Then, you could play to turn optimizations if you want to identify the root cause of the numerical difference.  The list is available at: https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/framework/config.pyL203L241",P.s. for you specific case the root cause of the numerical difference was the constant folding:  See more at https://www.tensorflow.org/guide/graph_optimization,Thanks for testing it out for me! ,Are you satisfied with the resolution of your issue? Yes No,"A note that might be less obvious  with the arithmetic simplifications made by the optimizer, the results should be closer to the real mathematical results. So in practice, I would recommend using tf.function with the arithmetic optimizer enabled and noting that eager's results are slightly different (and likely less accurate), rather than disabling it the optimizer to bring the results in line. ","> So in practice, I would recommend using tf.function with the arithmetic optimizer enabled and noting that eager's results are slightly different (and likely less accurate), rather than disabling it the optimizer to bring the results in line. Yes thanks I think that it is something important to repeat as many things that it is just an approximation to make things faster (fooled by the `optimizer` name).  But also this is the 2nd time in a week and I have seen many of these in my sparse triage activity over the years.  So probably it is time to find a place to better better promote this knowledge. If an users/developer have some doubt I think it could be ok to test:  I don't know if we could have numerical bugs in these optimizations but eventually It would be very hard that we have this kind of report by users.","Right, so that's my point  the optimizations should never be approximating. It's more like transformations of the kind `exp(log(x)) > x`, or `3 * x / x > 3`. So if we see discrepancies, it's likely that the eager code is numerically unstable, whereas the optimized code is more correct. That's why disabling the optimizer should only be used for testing."
748,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.7  Custom Code Yes  OS Platform and Distribution linux  Mobile device _No response_  Python version python3.7.12  Bazel version 3.7.2  GCC/Compiler version 7.3.1  CUDA/cuDNN version rocm5.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,south-ocean,C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.7  Custom Code Yes  OS Platform and Distribution linux  Mobile device _No response_  Python version python3.7.12  Bazel version 3.7.2  GCC/Compiler version 7.3.1  CUDA/cuDNN version rocm5.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-20T07:20:28Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.7,closed,0,8,https://github.com/tensorflow/tensorflow/issues/56179,ocean Could you try with the latest TF v2.9.0 and let us know the outcome? Thank you!,"  I can try this, but TF v2.7.0 is the version  what I need despite the outcome of TF2.9,  on the same time ,I had build TF1.15 success with of the same environment of the TF v2.7.0 , So what  I want to solve is  this error on the version of TF v2.7.0, So can you help me to solve this error, thanks.",ocean Tensorflow doesn't officially support ROCm. You can follow the instructions here. Also this is similar to these issues issue1 and issue2. Follow all the suggestions in issue2 but this is all I can do as we don't support AMD ROCm," Thanks, I will try it.",ocean Also Please post this issue in this forum. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory Leak with Tensorflow Experimental Save)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Jesse-Kerr,Memory Leak with Tensorflow Experimental Save,Click to expand!    Issue Type Performance  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-20T04:03:31Z,stat:awaiting tensorflower comp:data type:performance TF 2.8,open,5,8,https://github.com/tensorflow/tensorflow/issues/56177," , I was able to reproduce the issue in tensorflow v2.8, v2.9 and nightly.Please find the gist of it here.",This is occurring also when using tf.data.Dataset.save() on version '2.10.0dev20220531',Any news? I am seeing the same issue. ,Seeing same issue,"   is there any work being done to address this issue? It seems multiple people are experiencing this memory leak, myself included","A year has passed, and I can reproduce under `tensorflow==2.12.0`.  Furthermore, there may be more features harmed downstream, like caching?","For anyone still following this, there is a hacky work around that allows you to avoid the memory leak. You can use Ray to create an Actor (https://docs.ray.io/en/latest/raycore/actors.html) that performs the call to save(). The critical bit from the documentation is ""Each actor runs in its own Python process""... meaning that the process will be terminated and the memory freed after saving a chunk. This also lets you save chunks of data in parallel, which makes your program faster. Beware of launching too many actors in parallel if you are working on large datasets. Some very rough pseudocode: ",This issue is still occurring in v2.16.1
652,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(LLVM 15 MLIR compilation bug)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution ArchLinux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version GCC 12.1 & LLVM 15  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bulvara,LLVM 15 MLIR compilation bug,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution ArchLinux  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version GCC 12.1 & LLVM 15  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-19T17:00:45Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.9,closed,0,14,https://github.com/tensorflow/tensorflow/issues/56171,"Hi  , Could you try to install with `GCC 9.3.1 and Bazel 5.0.0` which is compatible for tensorflow latest stable v2.9. Version  Bazel 3.7.2 Also could you take a look at this link for the compatible tested build configurations. Thank you!","Hi Tilakrayal, do you know what the _true_ upper limit for GCC version is? * You are testing with GCC 9.3.1 * ArchLinux is successfully using GCC 11 ** I'm having the same MLIR issue even with GCC11 :(","Hi , You can get the supporting GCC version information here. Thank you! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"> Could you try to install with `GCC 9.3.1 and Bazel 5.0.0` which is compatible for tensorflow latest stable v2.9. >  > Version	Python version	Compiler	Build tools > tensorflow2.9.0	3.73.10	GCC 9.3.1	Bazel 5.0.0 Great, but I'm compiling git master and not a stable version. I've tried with https://aur.archlinux.org/packages/gcc9 and it's giving the exact same failure with my LLVM, which is needed for _XLA_.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Hi , Currently, upper limit for GCC version is 9.3.1. Request you to downgrade GCC and build Tensorflow using steps mentioned here. Thank you!", These are the software versions available to me.,"Hi  , Please confirm whether you have g++ also which may be required by the Bazel. Please refer the attached requirements/steps here. Request you to use the tested configurations only and let us know if the issue still persists. Thankyou! Version  11.2",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
2015,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell  ConverterError                            Traceback (most recent call last) Input In [4], in ()       5 converter.target_spec.supported_ops = [       6   tf.lite.OpsSet.TFLITE_BUILTINS,  enable TensorFlow Lite ops.       7   tf.lite.OpsSet.SELECT_TF_OPS  enable TensorFlow ops.       8 ]       9 converter.experimental_enable_resource_variables = True > 10 tflite_model = converter.convert()      12  Save the model.      13 with open('model.tflite', 'wb') as f: File ~\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow\lite\python\lite.py:803, in _export_metrics..wrapper(self, *args, **kwargs)     800 .wraps(convert_func)     801 def wrapper(self, *args, **kwargs):     802    pylint: disable=protectedaccess > 803   return self._convert_and_export_metrics(convert_func, *args, **kwargs) File ~\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow\lite\python\lite.py:789, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)     787 self._save_conversion_params_metric()     788 start_time = time.process_time() > 789 result = convert_func(self, *args, **kwargs)     790 elapsed_time_ms = (time.process_time()  start_time) * 1000     791 if res)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,leeflix,"Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True","Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell  ConverterError                            Traceback (most recent call last) Input In [4], in ()       5 converter.target_spec.supported_ops = [       6   tf.lite.OpsSet.TFLITE_BUILTINS,  enable TensorFlow Lite ops.       7   tf.lite.OpsSet.SELECT_TF_OPS  enable TensorFlow ops.       8 ]       9 converter.experimental_enable_resource_variables = True > 10 tflite_model = converter.convert()      12  Save the model.      13 with open('model.tflite', 'wb') as f: File ~\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow\lite\python\lite.py:803, in _export_metrics..wrapper(self, *args, **kwargs)     800 .wraps(convert_func)     801 def wrapper(self, *args, **kwargs):     802    pylint: disable=protectedaccess > 803   return self._convert_and_export_metrics(convert_func, *args, **kwargs) File ~\AppData\Local\Programs\Python\Python39\lib\sitepackages\tensorflow\lite\python\lite.py:789, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)     787 self._save_conversion_params_metric()     788 start_time = time.process_time() > 789 result = convert_func(self, *args, **kwargs)     790 elapsed_time_ms = (time.process_time()  start_time) * 1000     791 if res",2022-05-19T16:43:10Z,stat:awaiting response type:bug TFLiteConverter TF 2.8,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56170,Hi  ! This issue is not replicating in 2.9 and nightly (2.10.0dev). Thank you!,Can confirm. Thank you! ,Are you satisfied with the resolution of your issue? Yes No
796,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Image name supported by generate_tf_record.py script)ï¼Œ å†…å®¹æ˜¯ (Hello, There are now many scripts on the Internet that can be used to create TFRecord files. However, I think I have read from time to time that certain names such as `image_001.jpg `or` image001.jpg` are more likely to lead to errors than `001.jpg`. I have a large data set and want to get everything right when I take the picture.  So I wonder whether it generally doesn't matter as long as the label files (XML files) are correct, i.e. that the image endings (.jpg or .png) in the CSV files are correct everywhere? Many thanks for clarifying this mystery)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Petros626,Image name supported by generate_tf_record.py script,"Hello, There are now many scripts on the Internet that can be used to create TFRecord files. However, I think I have read from time to time that certain names such as `image_001.jpg `or` image001.jpg` are more likely to lead to errors than `001.jpg`. I have a large data set and want to get everything right when I take the picture.  So I wonder whether it generally doesn't matter as long as the label files (XML files) are correct, i.e. that the image endings (.jpg or .png) in the CSV files are correct everywhere? Many thanks for clarifying this mystery",2022-05-19T05:45:34Z,stat:awaiting response type:others comp:ops,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56164," Could you please have a look at this similar SO thread to know more on the label files.  This is not a bug or feature request, for any further queries you may open this issue in TF discussion forum as there is a larger community there to get you the right help. Thank you!",">  Could you please have a look at this similar SO thread to know more on the label files. This is not a bug or feature request, for any further queries you may open this issue in TF discussion forum as there is a larger community there to get you the right help. Thank you! Thank you I will take a look and post it in the TF discussion forum", Could you please move this issue to closed status if you have already posted this issue on TF Forum. Thank you!," I will delete it there, the support isn't good enough. I will better let it here ;)","Hi , Use `tf.io.decode_image()`, which Detects whether an image is a BMP, GIF, JPEG, or PNG, and performs the appropriate operation to convert the input bytes string into a Tensor of type dtype. To convert to `tf.data` use  `tf.data.Dataset.from_tensor_slices`.Finally, write all the features into Tfrecord format using `tf.data.experimental.TFRecordWriter()`. Thank you!","> Hi , Use `tf.io.decode_image()`, which Detects whether an image is a BMP, GIF, JPEG, or PNG, and performs the appropriate operation to convert the input bytes string into a Tensor of type dtype. To convert to `tf.data` use `tf.data.Dataset.from_tensor_slices`.Finally, write all the features into Tfrecord format using `tf.data.experimental.TFRecordWriter()`. Thank you! Thanks for answering. That's okay I guess."
675,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(cufft plan fails with tf.signal.fft2d)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.1.0 gpu_py37h7db9008_0  Custom Code Yes  OS Platform and Distribution Windows  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit=10.1.243 cudnn=7.6.5  GPU model and memory NVIDIA Quatro RTX 8000 48GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kt-kbr,cufft plan fails with tf.signal.fft2d,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.1.0 gpu_py37h7db9008_0  Custom Code Yes  OS Platform and Distribution Windows  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit=10.1.243 cudnn=7.6.5  GPU model and memory NVIDIA Quatro RTX 8000 48GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-18T22:14:11Z,stat:awaiting response type:bug stale comp:apis TF 2.1,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56159,kbr We see that you are using an older version of TF(2.1.0) which is not actively supported.Please try to upgrade to the latest TF version(2.9.0) and let us know the outcome. Thank you!,My library compatibilities require me use 2.1,kbr Thank you for the update! TF v2.1 does not really contain all the changes to prevent code using invalid shapes from crashing the interpreter via CHECKfailures.Its unlikely for TF 2.1 version to receive any bug fixes except when we have security patches. We recommend you to use TF v2.4 or later versions as older versions are not actively supported. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
723,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( tflite_convert --saved_model_dir command throws error ""SavedModel file does not exist {saved_model.pbtxt|saved_model.pb}"" )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gauravxgolu," tflite_convert --saved_model_dir command throws error ""SavedModel file does not exist {saved_model.pbtxt|saved_model.pb}"" ",Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.9.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7.13  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-18T08:40:23Z,type:bug TFLiteConverter TF 2.9,closed,0,3,https://github.com/tensorflow/tensorflow/issues/56149,"I have got the reason to throw this error. Actually, I am trying to convert tf1 generated model in tf2 installed version.",Are you satisfied with the resolution of your issue? Yes No,Hi  ! Please check the 1.x and 2.x liteconverter scripts from this repo too which are normally preferred for pretrained models.  Thank you!
959,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Mix precision leads to nan loss)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source pip install  Tensorflow Version tf2.8  Custom Code Yes  OS Platform and Distribution windows  Mobile device windows 21h2 (19044, 1706)  Python version python 3.9.7  Bazel version unkonwn  GCC/Compiler version unknown  CUDA/cuDNN version 11.2  GPU model and memory RTX3070/8Gb  Problem Discription? Loss is nan when using mix precision API even though use (get_scaled_loss, get_unscaled_gradients) or scale loss manually.  Model works well in float32.  Mix precision API with tensorflow guide works well in my computer.  My model input start from Cond2D > ... > Resblock > ... > Dense > SoftMax, it's to solve a classification problem.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ZhenShuo2021,Mix precision leads to nan loss,"Click to expand!    Issue Type Bug  Source pip install  Tensorflow Version tf2.8  Custom Code Yes  OS Platform and Distribution windows  Mobile device windows 21h2 (19044, 1706)  Python version python 3.9.7  Bazel version unkonwn  GCC/Compiler version unknown  CUDA/cuDNN version 11.2  GPU model and memory RTX3070/8Gb  Problem Discription? Loss is nan when using mix precision API even though use (get_scaled_loss, get_unscaled_gradients) or scale loss manually.  Model works well in float32.  Mix precision API with tensorflow guide works well in my computer.  My model input start from Cond2D > ... > Resblock > ... > Dense > SoftMax, it's to solve a classification problem.  Standalone code to reproduce the issue   Relevant log output  ",2022-05-17T20:24:42Z,stat:awaiting response type:bug stale comp:keras TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56142,"  In order to expedite the troubleshooting process, please provide a complete code snippet to reproduce the issue reported here. Thank you!","Complete code update, please take a look. Thank you!", Thank you for the update! Could you try to change the output layer and for further queries please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
616,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(COLAB : DNN library is not found. )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Colab  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SanjayGhanagiri,COLAB : DNN library is not found. ,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code Yes  OS Platform and Distribution Colab  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-17T11:25:46Z,stat:awaiting response type:bug stale comp:lite TF 2.8,closed,2,14,https://github.com/tensorflow/tensorflow/issues/56135,"I also couldn't run the colab example on this link, the github repo link also seems to be broken https://www.tensorflow.org/lite/tutorials/model_maker_object_detection", ! Thank for reporting the bugs !  ! Could you please look at this issue?,"I too am facing a similar issue in colab, with Conv1D. I have been using the following code and it was never an issue until before few days. Note that this works fine on CPU. But issue occurs only when I switch to GPU in colab. Tried rolling back from latest tensorflow version to tensorflow==2.7.0. Still facing same issue.   Error as follows, ",Model maker examples have been moved to separate repository.  You can check the colab and github source using this path https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb and try running the example in Tensorflow 2.9 and let us know if you face the problem again. I will file a PR for broken link fix. Thanks!,> 2.9  Even after trying `!pip install  tensorflow==2.9` and   It fails to import necessary  tflite model maker imports. from tflite_model_maker.config import QuantizationConfig ,"Hi the issue has been resolved by installing the below required dependencies, please find the gist here for reference. Link to the colab in the website has been updated, it will reflect soon. Till then you can access the notebook using the link here. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> Hi the issue has been resolved by installing the below required dependencies, please find the gist here for reference. Link to the colab in the website has been updated, it will reflect soon. Till then you can access the notebook using the link here. >  >  Its now unable to import scann. Could you try running it again?",Hi  ! I could run the Colab notebook on object detection in GPU run time without any issue. Could you let us know from your end. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1399,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GpuCudaMallocAsyncAllocator fails if the same GPU device is initialised multiple times)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.7.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.2, cuDNN 8.1.1  GPU model and memory NVIDIA GTX 1080  Current Behaviour? When https://github.com/tensorflow/tensorflow/blob/v2.7.1/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.ccL391 is used instead of the default one (either via environment variable or config setting), and the same GPU device is initialised more than once (e.g. if there are multiple TensorFlow sessions within a single process) it fails with the ""Trying to set the stream twice. This isn't supported."" error. Apparently this is happening because the `GpuCudaMallocAsyncAllocator` is global and its `cuda_stream_` field is filled on the first `SetStreamAndPreallocateMemory`. I was able to fix this by making some changes in the code, and I am going to submit a PR containing those.  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dev0x13,GpuCudaMallocAsyncAllocator fails if the same GPU device is initialised multiple times,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.7.1  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.2, cuDNN 8.1.1  GPU model and memory NVIDIA GTX 1080  Current Behaviour? When https://github.com/tensorflow/tensorflow/blob/v2.7.1/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.ccL391 is used instead of the default one (either via environment variable or config setting), and the same GPU device is initialised more than once (e.g. if there are multiple TensorFlow sessions within a single process) it fails with the ""Trying to set the stream twice. This isn't supported."" error. Apparently this is happening because the `GpuCudaMallocAsyncAllocator` is global and its `cuda_stream_` field is filled on the first `SetStreamAndPreallocateMemory`. I was able to fix this by making some changes in the code, and I am going to submit a PR containing those.  Standalone code to reproduce the issue   Relevant log output  ",2022-05-17T11:19:41Z,type:bug comp:gpu TF 2.7 awaiting PR merge,closed,0,2,https://github.com/tensorflow/tensorflow/issues/56134,"Hello  , The PR has been assigned for reviewing and once it is merged this issue will move to closed status.Thank you!",Are you satisfied with the resolution of your issue? Yes No
678,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF 2.9 - CombinedNonMaxSuppression conversion to TFLite not working anymore)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rovinellimarco,TF 2.9 - CombinedNonMaxSuppression conversion to TFLite not working anymore,Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.9  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2/8.1  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-17T11:12:15Z,stat:awaiting response type:support comp:lite TFLiteConverter TF 2.9,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56133,"Hi  ! Could you please look at this issue ? Attached gist in 2.8, 2.9 and nightly. Thank you!",Any updates on this?,Hi  ! I was able to convert the model to a Tflite model  by skipping this line. `converter.target_spec.supported_types = [tf.int8] ` Attached resolved gist in nightly for reference. Thank you!,"As far as I know, that line is needed to have fully integer models so it is not an option to remove it.","Ok  ! Actually original select ops issue was getting resolved with following syntax (skipping `converter.target_spec.supported_types = [tf.int8]` ) before lite conversion and asking a 4D box tensor when not skipped.  Inference input and output type are not int8 types though . But they are int32 type (Check input and output details in above gist ), will that work for your use case. Thank you!","It seems like the generated TFLite matches the result that I was getting on TF 2.8 so yes this solves the problem, thank you! ",Are you satisfied with the resolution of your issue? Yes No
680,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite 2.9 ARM cross-compilation failed: Cannot find source file: common.c)ï¼Œ å†…å®¹æ˜¯ (**System information** * Linux Ubuntu 20.04 * TensorFlow 2.9 * CMake 3.16.3 * gccarm8.32019.03x86_64armlinuxgnueabihf (here) **Describe the problem** I trying to crosscompile TensorFlow Lite 2.9 for ARM using CMake. I got error ""Cannot find source file: common.c"":  I seet that `.c` files has been renamed to `.cc` but CMakeLists.txt file left unchanged: CMakeLists.txt I using build instructions provided here)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,distlibs,TensorFlow Lite 2.9 ARM cross-compilation failed: Cannot find source file: common.c,"**System information** * Linux Ubuntu 20.04 * TensorFlow 2.9 * CMake 3.16.3 * gccarm8.32019.03x86_64armlinuxgnueabihf (here) **Describe the problem** I trying to crosscompile TensorFlow Lite 2.9 for ARM using CMake. I got error ""Cannot find source file: common.c"":  I seet that `.c` files has been renamed to `.cc` but CMakeLists.txt file left unchanged: CMakeLists.txt I using build instructions provided here",2022-05-17T02:26:38Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux TF 2.9,closed,0,12,https://github.com/tensorflow/tensorflow/issues/56125,Hi  ! Build is passing for Ubuntu 18 and CMake 3.23 in Colab. Could you check this gist once ? Thank you!,I get the same issue when compiling on x86 Linux. Inside the `contrib/tensorflow/tensorflow/lite/c/CMakeLists.txt:63` the nonexisting file `common.c` is added. The file is now named `common.cc` This only happens when using the C API though as described here: https://www.tensorflow.org/lite/guide/build_cmakebuild_tensorflow_lite_c_library,Compilation is working fine after changing `common.c` to `common.cc` in the CMakeLists.txt,"Well, that's not really an option when using `tensorflow` as a submodule",Ok  ! Thanks for update ! This issue will be closed once above PR is merged. Thank you!,Can confirm this is a problem on Linux 5.10.013amd64 CC(Add support for Python 3.x) SMP Debian 5.10.1061 (20220317) x86_64 GNU/Linux. Renaming changing from 'common.c' to 'common.cc' in tensorflow/lite/c/CMakeLists.txt fixes the problem.,oneline workaround for GNU sed systems.  Hopefully the CMakeLists.txt in the release has been fixed.,"The suggested solution is already merged in the PR here, could you please check it in the nightly version and let us know. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"Note: There has been few changes made in the CMakeLists.txt since my last comment on this thread, refer the latest commit here https://github.com/tensorflow/tensorflow/commit/17e3646683f0149091384d250b27d6f52556ee78 for the changes and let us know if you still face an issue in the nightly build. Thanks!"
917,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.random.uniform()` always picking the same value when metal plugin is installed)ï¼Œ å†…å®¹æ˜¯ ( System information * OS: macOS Montererey 12.4 * Tensorflow installed from: `pip` commands inside a conda environment * Tensorflow version: `tensorflowmacos` 2.8.0 * Metal plugin version: `tensorflowmetal` 0.4.0 * Python version: 3.9.12 * GPU model and memory: Apple M1 Pro with 16 GB memory * Exact command to reproduce: see below  Describe the problem When the `tensorflowmetal` plugin is installed, the `tf.random.uniform()` function always picks the same value on every call. Other random functions like `tf.random.normal()` do not have this problem. Neither does this problem occur when the Metal plugin is not installed.  Source code / logs )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yunhao-qian,`tf.random.uniform()` always picking the same value when metal plugin is installed," System information * OS: macOS Montererey 12.4 * Tensorflow installed from: `pip` commands inside a conda environment * Tensorflow version: `tensorflowmacos` 2.8.0 * Metal plugin version: `tensorflowmetal` 0.4.0 * Python version: 3.9.12 * GPU model and memory: Apple M1 Pro with 16 GB memory * Exact command to reproduce: see below  Describe the problem When the `tensorflowmetal` plugin is installed, the `tf.random.uniform()` function always picks the same value on every call. Other random functions like `tf.random.normal()` do not have this problem. Neither does this problem occur when the Metal plugin is not installed.  Source code / logs ",2022-05-16T19:45:09Z,type:bug comp:ops TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56122,"It is because the random.uniform in apple metal is not able to initialize the op seed or global seed, when both of these seeds values are set to None, you'll get the same number generated.  To avoid this you can enable `tf.config.experimental.enable_op_determinism()` and set the `random_seed` to generate different results. Refer the below code. ","Thank you! ğŸ˜Š After the global seed is set, `tf.random.uniform()` behaves as expected. When I read the documentation of `tf.random.set_seed()`, I found the following sentence which complements your explanation: > If neither the global seed nor the operation seed is set: A randomly picked seed is used for this op. So here, how the seed is ""randomly picked"" is not clearly specified. Should I understand it as: since Metal is unable to initialize the global seed or the operation seed, the current behaviour (without seeding) is an expected ""sharp edge"" instead of a bug? This can be very surprising for many people like me, so maybe it should be documented somewhere more explicitly. Thank you again for your help. You can close this issue if you think no further action is needed.","Yes, you can consider it as a edge case, random seed works on a kernel level and maintains the counter in the kernel level itself. apple metal fails to initialize the random seed, `tensorflowmacos` is not officially supported by Tensorflow to document this behavior.  Closing the issue as per your above comment. Thanks!",Are you satisfied with the resolution of your issue? Yes No
695,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(problem with batch production)ï¼Œ å†…å®¹æ˜¯ (Hi All, Inspiring by the tutorial called â€œText generation with an RNNâ€ (https://www.tensorflow.org/text/tutorials/text_generation), I managed to train a model to generate new molecules in SMILES string format. It has been trained very well and I could generate new molecules either consecutively (one after another) and batch production. However, when I saved the trained model and loaded it again, the batch production fails and ends up with an error. could someone help please?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",text generation,Jalil-Mahdizadeh,problem with batch production,"Hi All, Inspiring by the tutorial called â€œText generation with an RNNâ€ (https://www.tensorflow.org/text/tutorials/text_generation), I managed to train a model to generate new molecules in SMILES string format. It has been trained very well and I could generate new molecules either consecutively (one after another) and batch production. However, when I saved the trained model and loaded it again, the batch production fails and ends up with an error. could someone help please?",2022-05-16T17:50:41Z,stat:awaiting response type:support stale comp:apis TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56121,"Hello Mahdizadeh , We see that the issue template has not been filled, could you do so as it helps us to analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible please share a colab gist with the issue reported.","Hi, I just directly pressed the button ""Run in Google Colab"" from this page : **https://www.tensorflow.org/text/tutorials/text_generation**  The steps that I followed: 1 I trained the model successfully 2 I was able to generate text with a single input using the trained model 3 I was able to generate text with a batched input using the trained model 4 I saved the onestep model successfully 5 I loaded the saved model successfully  6 I was able to generate text with a single input using the loaded model 7 I **COULD NOT** generate text with a batched input using the loaded model","Here is the error I got: ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in ()       4        5 for n in range(1000): > 6   next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)       7   result.append(next_char)       8  1 frames /usr/local/lib/python3.7/distpackages/tensorflow/python/saved_model/function_deserialization.py in restored_function_body(*args, **kwargs)     282           .format(index + 1, _pretty_format_positional(positional), keyword))     283     raise ValueError( > 284         ""Could not find matching concrete function to call loaded from the ""     285         f""SavedModel. Got:\n  {_pretty_format_positional(args)}\n  Keyword ""     286         f""arguments: {kwargs}\n\n Expected these arguments to match one of the "" ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:   Positional arguments (2 total):     * Tensor(""inputs:0"", shape=(2,), dtype=string)     * None   Keyword arguments: {}  Expected these arguments to match one of the following 4 option(s): Option 1:   Positional arguments (2 total):     * TensorSpec(shape=(1,), dtype=tf.string, name='inputs')     * None   Keyword arguments: {} Option 2:   Positional arguments (2 total):     * TensorSpec(shape=(1,), dtype=tf.string, name='inputs')     * TensorSpec(shape=(1, 1024), dtype=tf.float32, name='states')   Keyword arguments: {} Option 3:   Positional arguments (2 total):     * TensorSpec(shape=(5,), dtype=tf.string, name='inputs')     * None   Keyword arguments: {} Option 4:   Positional arguments (2 total):     * TensorSpec(shape=(5,), dtype=tf.string, name='inputs')     * TensorSpec(shape=(5, 1024), dtype=tf.float32, name='states')   Keyword arguments: {}","Mahdizadeh , I was able to execute the given code without any issues. Please find the gist of it here.","Hi  It is still not working when I tried to batched generate text by ""one_step_reloaded"" https://colab.research.google.com/gist/tilakrayal/1e3b0be6e3530f5bd0a50ce0e88da509/text_generation.ipynb"," , I was able to reproduce the issue in tensorflow v2.8, and in v2.7 and nightly the code was failing with different error. Please find the gist.",Mahdizadeh Please take a look at this issue and try all the suggestions mentioned in it and let me know if it helps.  Also take a look at the solutions provided here. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
509,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add lamba var loop test)ï¼Œ å†…å®¹æ˜¯ (Add a test to cover and then fix CC([Autograph] Inconsistent behaviour with lambda variable in loop) ~This is based on https://github.com/tensorflow/tensorflow/pull/56106 and it will be semplified/rebased when merged.~ /.  When we are satisfied by the test coverage we could start to try some fixes.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,bhack,Add lamba var loop test,Add a test to cover and then fix CC([Autograph] Inconsistent behaviour with lambda variable in loop) ~This is based on https://github.com/tensorflow/tensorflow/pull/56106 and it will be semplified/rebased when merged.~ /.  When we are satisfied by the test coverage we could start to try some fixes.,2022-05-16T13:26:37Z,ready to pull size:S,closed,0,14,https://github.com/tensorflow/tensorflow/issues/56119,P.s. Just a reminder for the test coverage. The original case was a tuple: https://github.com/kerasteam/kerascv/pull/428/filesdiff3b6e0d9a251194173d62c81502cd90685cad4845f2b76908ad4488fff0b2ec59L92L94, Some small progress here. We have still some edge case to handle like when we don't execute the loop ([] or range with 0) as we cannot initialize the iter in these cases.,P.s. I think that there was already your TODO: https://github.com/tensorflow/tensorflow/blob/79eee8e0493fb443a3fe6f247c3f55cec40b11e7/tensorflow/python/autograph/operators/control_flow.pyL196,,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB, I've done a small hack that it seems enough to cover all the public/OSS tests we have. Let me know if the test params are enough to cover the original case at: https://github.com/kerasteam/kerascv/pull/428/filesdiff3b6e0d9a251194173d62c81502cd90685cad4845f2b76908ad4488fff0b2ec59L92L94,"To summarize I think that all is working correctly and the code in master was working cause the old local scope of the lambda args but it was ""wrong"".","(copying offline discussion) Here's another relevant test  I think this one is worth ardding to the suite because it's quite minimal:  So the main issue is that this might break existing code, but it's worth a try. If we see too many breakages, we'll need to place the fix behind a flag and give users time for a minor version or two until we change the behavior permanently (sending notifications to test with the flag on in the mean time). I'll approve to test this change internally and post back the results.", Let me know if you have other internal problems as this is still ambiguous: https://github.com/tensorflow/tensorflow/blob/b5388b212fe50dc5615147ac410b1b0df27a7b02/tensorflow/python/autograph/pyct/static_analysis/activity.pyL637L640 Do we need to remove it?,"Probably, but I don't know exactly which use case would change behavior as a result of that.","> Probably, but I don't know exactly which use case would change behavior as a result of that. If you can reexecute internal tests try to remove it later. We need to modify: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.mdvariablesclosedoverbylambdafunctions","> > Probably, but I don't know exactly which use case would change behavior as a result of that. > > If you can reexecute internal tests try to remove it later. >  > We need to modify: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.mdvariablesclosedoverbylambdafunctions Done",/. Formally I am not changing an API here as in the signature.  I am just changing the behaviour of an API with autograph to be aligned with the expected output we have in python. Then I could not help to fix failures that are internal only as contributor cause I have no visibility., We could not merge this.  We lost the PR... What you want to do?
637,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bert ModelSpec Export to TFLITE does not work)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version GCC 9.3.0  CUDA/cuDNN version V11.6.55  GPU model and memory RTX5000, 16 GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,philippHeitzer,Bert ModelSpec Export to TFLITE does not work,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version GCC 9.3.0  CUDA/cuDNN version V11.6.55  GPU model and memory RTX5000, 16 GB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-05-15T16:03:05Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:lite TFLiteConverter TF 2.8,closed,0,20,https://github.com/tensorflow/tensorflow/issues/56110,Hi  ! Can you check this gist for required changes? Thanks!,"> Hi  ! Can you check this gist for required changes? Thanks! Thank you for your help. The code seems to work on Google Colab, but not on https://console.paperspace.com/pheitzer/notebook/rsnqh1w7dvedob3 where nothing happens after : ""INFO:tensorflow:Assets written to: /tmp/tmp3l2ik0s_/saved_model/assets"". Unfortunately, it is not possible to use Google Colab Pro+ from Austria to get the possibility to use better GPUs.","While installing Tensorflow Lite Model maker by: !pip install tflitemodelmaker Following error message appears: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. daskcuda 21.12.0 requires numba>=0.53.1, but you have numba 0.53.0 which is incompatible. cudf 21.12.0a0+293.g0930f712e6 requires numba>=0.53.1, but you have numba 0.53.0 which is incompatible. Maybe is this a reason for the problem?","And at importing things from tflite_model_maker: /usr/local/lib/python3.8/distpackages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported).   The versions of TensorFlow you are currently using is 2.8.0 and is not supported.  Some things might work, some things might not. If you were to encounter a bug, do not file an issue. If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. "," ! Thanks for the update! I think the above Error is suggesting that Paperspace does not have support for 2.8 and above version yet. Can you try again in 2.7 version(also use `""pip install tflite_model_maker tensorflow==2.7""` to select compatible tflite_model_maker version according to 2.7) let us know ?  Thank you!","> And at importing things from tflite_model_maker: >  > /usr/local/lib/python3.8/distpackages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). The versions of TensorFlow you are currently using is 2.8.0 and is not supported. Some things might work, some things might not. If you were to encounter a bug, do not file an issue. If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. To solve this problems I installed:  tensorflowaddons==0.16.1,so that this error message does not occur again. I also followed your instructions with using tensorflow v2.7.0 (""pip install tflite_model_maker tensorflow==2.7""). But unfortunately this also didn't solve the original problem of not exporting the Bert_Model to .tflite.  It never comes to the step: ""INFO:tensorflow:Vocab file and label file are inside the TFLite model with metadata."" !image Also, I always have to install these two libraries manually via terminal: apt y install libusb1.00dev apt y install libportaudio2","New findings: all other export formats work, just not ExportFormat.TFLITE",Ok ! Thanks for the update.  Could you skip the tflite format and try with below commands and let us know. Thank you! ,"Unfortunately, nothing changed. !image Just the empty directory ""models"" was created.","Without executing ""apt y install libusb1.00dev"" I get this error at importing tflite_model_maker libs: ImportError: libusb1.0.so.0: cannot open shared object file: No such file or directory !image ... !image Maybe there is something wrong with that metadata. Can this be related?",Hi  ! Could you please look at this issue? Attaching gist for reference. Thank you!,"> Without executing ""apt y install libusb1.00dev"" I get this error at importing tflite_model_maker libs: >  > ImportError: libusb1.0.so.0: cannot open shared object file: No such file or directory >  > !image ... !image >  > Maybe there is something wrong with that metadata. Can this be related? This doesn't seem to cause the problem either, because I have now installed version 0.3.1 of tflitesupport (which was updated to v0.4.0 8 days ago), but even after this nothing still happens...", take a look.,"Can you try first export it to saved_model and then use tf.lite.converter directly to see if it can be converted successfully?  Meanwhile, may I ask why you want BERT model instead of mobileBERT? We usually recommend mobileBERT model instead since it can achieve similar accuracy result while have much smaller model size and lower latency.","> model.export('./bert/', export_format=ExportFormat.SAVED_MODEL) > converter =  tf.lite.TFLiteConverter.from_saved_model('./bert/saved_model') > converter.optimizations = [tf.lite.Optimize.DEFAULT] > tflite_model = converter.convert() Do these log entries mean that the conversion has been performed? !image I am currently analyzing these three proposed models from tflitemodelmaker for my bachelor thesis. And since I am already done with data collection for Word Embedding and mobileBert, it is necessary to test BERT under the same circumstances.",Does the process finish? Or could you see if you can write the tflite_model?   ,"> with open('./bert/model.tflite', 'wb') as f: >   f.write(tflite_model) Yes that works, and for my experiments that is enough for now. Nevertheless it would be interesting to know what the problem is in this case for ""model.export()"" for the TFLITE format.",", I tried with the approach for the mentioned code and was executed without any issue/error and also I can see the model.tflite file was available. Kindly find the gist of it here.  !image",Closing this as stale. Please reopen if this is still a valid request. Thank you!,Are you satisfied with the resolution of your issue? Yes No
1838,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow local_init_op use all GPUs)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: No    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04 LTS    **TensorFlow installed from (source or binary)**: miniconda with condaforge channel    **TensorFlow version (use command below)**: 2.7.0    **Python version**: 3.8.13  ++++ py from __future__ import absolute_import, print_function, unicode_literals import numpy as np import pandas as pd import matplotlib.pyplot as plt from IPython.display import clear_output from six.moves import urllib import tensorflow.feature_column as fc import tensorflow as tf _GPU = tf.config.list_physical_devices('GPU')[1] tf.config.experimental.set_memory_growth(_GPU, True) tf.config.set_visible_devices([_GPU], device_type='GPU') os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID"" os.environ[""CUDA_VISIBLE_DEVICES""]=""1"" print(tf.config.list_logical_devices('GPU')) print(tf.config.get_visible_devices('GPU')) dftrain = pd.read_csv('https://storage.googleapis.com/tfdatasets/titanic/train.csv') dfeval = pd.read_csv('https://storage.googleapis.com/tfdatasets/titanic/eval.csv') y_train = dftrain.pop('survived') y_eval = dfeval.pop('survived') print(dict(dftrain)['age']) CATEGORICAL_COLUMNS = ['sex','n_siblings_spouses', 'parch','class','deck','embark_town','alone'] NUMERIC_COLUMNS = ['age','fare'] feature_columns = [] for feature_name in CATEGORICAL_COLUMNS:     vocabulary = dftrain[feature_name].unique()     feature_columns.append(fc.categorical_column_with_vocabulary_list(feature_name,vocabulary)) for feature)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,nyngwang,tensorflow local_init_op use all GPUs," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: No    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04 LTS    **TensorFlow installed from (source or binary)**: miniconda with condaforge channel    **TensorFlow version (use command below)**: 2.7.0    **Python version**: 3.8.13  ++++ py from __future__ import absolute_import, print_function, unicode_literals import numpy as np import pandas as pd import matplotlib.pyplot as plt from IPython.display import clear_output from six.moves import urllib import tensorflow.feature_column as fc import tensorflow as tf _GPU = tf.config.list_physical_devices('GPU')[1] tf.config.experimental.set_memory_growth(_GPU, True) tf.config.set_visible_devices([_GPU], device_type='GPU') os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID"" os.environ[""CUDA_VISIBLE_DEVICES""]=""1"" print(tf.config.list_logical_devices('GPU')) print(tf.config.get_visible_devices('GPU')) dftrain = pd.read_csv('https://storage.googleapis.com/tfdatasets/titanic/train.csv') dfeval = pd.read_csv('https://storage.googleapis.com/tfdatasets/titanic/eval.csv') y_train = dftrain.pop('survived') y_eval = dfeval.pop('survived') print(dict(dftrain)['age']) CATEGORICAL_COLUMNS = ['sex','n_siblings_spouses', 'parch','class','deck','embark_town','alone'] NUMERIC_COLUMNS = ['age','fare'] feature_columns = [] for feature_name in CATEGORICAL_COLUMNS:     vocabulary = dftrain[feature_name].unique()     feature_columns.append(fc.categorical_column_with_vocabulary_list(feature_name,vocabulary)) for feature",2022-05-14T12:02:16Z,stat:awaiting response type:bug stale comp:dist-strat comp:gpu TF 2.7,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56107,The problem is temporarily solved by moving these lines immediately following all the `import`s:  and then change one line:  But I still don't know the reason. Why the following configs are not enough? ,"Hello  , Could please try the following script for TensorFlow not allocating memory for ""all of the GPUs""  This allows Tensorflow to make visible/use of 0th GPU only. Also could you please try to use range[0, 2], then 0th and 2nd GPUs  both will be visible.Also i request to please take a look at this document and SO link1 and link2 with the similar error.It helps.Thanks!","  >     import os >     os.environ[""CUDA_VISIBLE_DEVICES""]=""0"" >  > This allows Tensorflow to make visible/use of 0th GPU only. Why `tf.config.set_visible_devices` cannot replace this command? I assume that `os.environ` is the TF1style way to do this. So I chose to set up everything with `tf.config`. > Also i request to please take a look at this document and SO link1 and [..] 1. I did try using `with tf.device(...)` in the code I provided in my OP, but it didn't work. 2. The answer in link1 is good but since it is not tagged with `tensorflow2.0` and it's answered 3 years ago, so even if I did find that page myself, I would probably skip it. 3. Thanks for link2, but my usage is different, I'm only interested in using one GPU. Thanks for your help and time reading my feedback.",condaforge is a community build which is not officially supported by Tensorflow.  Have you tried with the Tensorflow official build with the supported configurations mentioned here. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1909,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(File size & memory constantly increasing for keras models with StringLookup layer after reloading & resaving)ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: No    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 12.3.1    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: N/A    **TensorFlow installed from (source or binary)**: binary from pip    **TensorFlow version (use command below)**: 2.8.0    **Python version**: 3.8.3    **Bazel version (if compiling from source)**: N/A    **GCC/Compiler version (if compiling from source)**: N/A    **CUDA/cuDNN version**: N/A    **GPU model and memory**: Intel UHD Graphics 630 1536MB    **Exact command to reproduce**: 1) pip install psutil 2) python test.py train 3) python test.py You can collect some of this information using our environment capt)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,atticusfyj,File size & memory constantly increasing for keras models with StringLookup layer after reloading & resaving,"Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: No    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 12.3.1    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: N/A    **TensorFlow installed from (source or binary)**: binary from pip    **TensorFlow version (use command below)**: 2.8.0    **Python version**: 3.8.3    **Bazel version (if compiling from source)**: N/A    **GCC/Compiler version (if compiling from source)**: N/A    **CUDA/cuDNN version**: N/A    **GPU model and memory**: Intel UHD Graphics 630 1536MB    **Exact command to reproduce**: 1) pip install psutil 2) python test.py train 3) python test.py You can collect some of this information using our environment capt",2022-05-13T07:47:01Z,stat:awaiting response type:support comp:keras TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56095,"Hello  , Thanks for opening this issue. Development of keras moved to another repository.  Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!","> Hello  , Thanks for opening this issue. Development of keras moved to another repository. >  > Could you please post this issue on kerasteam/keras repo. To know more please refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you! Thanks! Moved to here."," , Could you please feel free to move this issue to closed status, since it is already being tracked there? Thanks!",Are you satisfied with the resolution of your issue? Yes No
1811,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(cuDNN path)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.7  Custom Code No  OS Platform and Distribution RHEL 7  Mobile device _No response_  Python version 3.9.5  Bazel version 3.7.2  GCC/Compiler version 10.3.0  CUDA/cuDNN version 11.2.1/8.2.1.39  GPU model and memory 2 nvidia v100 32gb each  Current Behaviour?  I do not have `sudo` access to generate symlinks. I was wondering if there is a way to specify the cuDNN library path separately to the CUDA library path. shell Have cuDNN and CUDA installed in separate directories $ git clone https://github.com/tensorflow/tensorflow.git $ cd tensorflow $ git checkout v2.7.0 $ ./configure You have bazel 3.7.2 installed. Please specify the location of python. [Default is /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python3]:  Found possible Python library paths:   /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/easybuild/python   /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/lib/python3.9/sitepackages Please input the desired Python library path to use.  Default is [/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/easybuild/python] Do you wish to build TensorFlow with ROCm support? [y/N]: N No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: N No TensorRT support will be enabled for TensorFlow. Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'        )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ozencgungor,cuDNN path,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.7  Custom Code No  OS Platform and Distribution RHEL 7  Mobile device _No response_  Python version 3.9.5  Bazel version 3.7.2  GCC/Compiler version 10.3.0  CUDA/cuDNN version 11.2.1/8.2.1.39  GPU model and memory 2 nvidia v100 32gb each  Current Behaviour?  I do not have `sudo` access to generate symlinks. I was wondering if there is a way to specify the cuDNN library path separately to the CUDA library path. shell Have cuDNN and CUDA installed in separate directories $ git clone https://github.com/tensorflow/tensorflow.git $ cd tensorflow $ git checkout v2.7.0 $ ./configure You have bazel 3.7.2 installed. Please specify the location of python. [Default is /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python3]:  Found possible Python library paths:   /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/easybuild/python   /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/lib/python3.9/sitepackages Please input the desired Python library path to use.  Default is [/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/easybuild/python] Do you wish to build TensorFlow with ROCm support? [y/N]: N No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for TensorFlow. Do you wish to build TensorFlow with TensorRT support? [y/N]: N No TensorRT support will be enabled for TensorFlow. Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'        ,2022-05-13T07:14:51Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.7,closed,0,10,https://github.com/tensorflow/tensorflow/issues/56093, Could you refer to this link to know more on the tested build configurations . Please try with the latest TF v2.8.0  and let us know if it helps ?  Thank you!, I will try but the cluster I'm working with has Bazel v3.7.2 as the latest version. I'll try and update the Bazel version and build Tensorflow v2.8.0. The issue seems that my cuDNN library files are not under the CUDA directory `/usr/local/easybuild/software/CUDA/11.3.1` but rather at `/usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1`. I do not have `sudo` access to create symlinks but maybe the newer bazel version and the newer tensorflow version will solve this issue.," Tried with Bazel v5.1.1 insalled through bazelisk with the latest tf version obtained from github through a `git clone` and no `git checkout` and it still is the same issue. Because cuDNN libraries are in a different directory, the configuration script fails to find the cuDNN libraries and the `cudnn.h` file. I have added all the relevant paths to my `$LD_LIBRARY_PATH` and my `$PATH` in my `~/.bashrc` file and sourced it to no avail. As a note, I am able to build the latest `jax` and `jaxlib` versions on this system without any issues so it seems the issue is that `configure.py` and `configure.cmd` are only able to look for cudnn files under `.../CUDA/11.3.1` even though I supply the correct path when prompted. Here is an example configure step:  The paths to CUDA and cuDNN for me are: for CUDA: /usr/local/easybuild/software/CUDA/11.3.1/ for cuDNN: /usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/ for NCCL: /usr/local/easybuild/software/NCCL/2.10.3GCCcore10.3.0CUDA11.3.1 I do not have `sudo` access to create a symlink or anything similar. Any help would be appreciated.","I figured it out. Here is how I did it:  With the `export` statement, `configure.py`Â was able to automatically locate CUDA, cuDNN and NCCL libraries and the build completed successfully.  It might not be a bad idea to update the official instructions as I'm guessing this is an issue for other people working with remote clusters configured with easybuild.",", Glad that this issue is resolved. `export` statement is already available in GPU support doc. Please take a look at this link. Thank you! `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64`", My `$LD_LIBRARY_PATH` variable already contains the relevant paths for cuDNN and CUDA libraries but for some reason `configure` fails to find them. `export TF_CUDA_PATH` was more reliable for me for some reason but thanks for the link.,"Hi , Since the issue is resolved can we move this to closed status. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
723,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Autograph] Inconsistent behaviour with lambda variable in loop)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version master  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue    Relevant log output `test_b` is wrongly working ""as expected"" in graph mode:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bhack,[Autograph] Inconsistent behaviour with lambda variable in loop,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version master  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue    Relevant log output `test_b` is wrongly working ""as expected"" in graph mode:  ",2022-05-12T22:17:32Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:autograph,closed,5,27,https://github.com/tensorflow/tensorflow/issues/56089,See more at https://github.com/kerasteam/kerascv/pull/432 /,"As referenced in the official Python FAQ: > Note that this behaviour is not peculiar to lambdas, but applies to regular functions too.","I think it's because we pass the loop variable through a function argument, which is enough to avoid the closure aliasing:  In the code above, the lambda would close over the local `i` which has a copy of the value. This only happens for the `for_stmt` operator, which passes the iterate as an argument. If we rewrite the test as a while loop, so that `i` is closedver by the function body, things are once again quirky as intended:  This means that the fix is also to avoid passing the iterate as argument to the loop body, and instead rely on the `get_state`/`set_state` functions, as is the case of the while loop.","Yes is what I have suspected:   And modify your while example to correctly get the output we ""expect"":  ", I've manually rewritten the transformation output. Do we need to produce something like this for the `lambda:` case? ,"Yes, something like that. And then the loop_body function would be a regular thunk: `def loop_body():`. We may also need to initialize `i` with Undefined, rather than 0, and might also need to replace `(), {'iterate_names': 'i'}` with just `('i',)`, but not sure.",Do we want to handle `iter` as `nolocal`? As in the helper function above it is only in the `undefined` bucket: https://github.com/tensorflow/tensorflow/blob/44e84cebef5a89a0840f8d401819c25af41ec3db/tensorflow/python/autograph/converters/control_flow.pyL170L199,"Yes, I think we do.","I don't know if it makes sense: I've changed:    And   So the output is correct but the transformation, surely, it is still quite ugly: ","P.s. as a side note, in the `BUILD` the loop scoping integration test is not registered. How the CI is running this currently?: https://github.com/tensorflow/tensorflow/blob/f2edfb6331c0563957bee6f73da9b2e09a7a8750/tensorflow/python/autograph/tests/BUILDL26L27 If it is activated some loop_scoping tests are failing: ","Ah, that's not intended. Can add it to the build file and mark the failing ones with self.skipTest, then file an issue to get them to pass? For the transformation, I'm not sure, the rules are quite finnicky, and I'm not sure I'd change them without extensive testing. Likely still safer to manually add the iterate to the list of loop_vars.",Side note  I just realized the limitations section of autograph does seem to document this case: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.mdvariablesclosedoverbylambdafunctions,Are you satisfied with the resolution of your issue? Yes No,"Hello, I have just discovered this in my app logs:  Is there anything I should look for in my code or is this a generic warning that displays to anyone? To be more precise, does the fact that I am seeing this warning mean that I am actually using something which is now deprecated?", We hope to align to the python behavior after the deprecation: https://docs.python.org/3/faq/programming.htmlwhydolambdasdefinedinaloopwithdifferentvaluesallreturnthesameresult,">  We hope to align to the python behavior after the deprecation: >  > https://docs.python.org/3/faq/programming.htmlwhydolambdasdefinedinaloopwithdifferentvaluesallreturnthesameresult My question rather is, if seeing that warning on my screen necessarily means that I am triggering that deprecated behavior (i.e. I am using lambda somewhere in .function or similar).",Do you have a small code gist to reproduce this?,"> Do you have a small code gist to reproduce this? Sorry, I don't understand what I should be reproducing. I just have a pretty big code base and I just want to know how much relevant the warning is for me :). ",I guess the answer is here: https://github.com/tensorflow/tensorflow/commit/6197fa37555b710a35e84c1b8e1aab2bcce9d46b,If it was internal someone introduced a new deprecation case internally ignoring the warning.,Are there any instructions on how to hide this warning? :P  ,It is suppressed in nightly and in the next release ,"Hi, How can one disable this warning so that it is not printed? I am using TF 2.11.1 and I cant really change my TF version. > It is suppressed in nightly and in the next release","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
647,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow docker image has outdated keys)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflow:latest  Custom Code No  OS Platform and Distribution Docker  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Naegionn,Tensorflow docker image has outdated keys,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflow:latest  Custom Code No  OS Platform and Distribution Docker  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-12T16:20:57Z,stat:awaiting response type:build/install TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56085, Could you please refer to this link and let us know if it helps? Thank you!,Ideally this fix will be added to the DockerHub images (it looks like the fix has been commited already) but in the meantime you can create a new Dockerfile that looks like:  and you should no longer see this error when you run `apt update` in that image. Alternatively if you don't want the intermediate Docker image you can jump into the tensorflow image shell and run the aptkey command above before doing anything else with it. The source of this issue is in this Nvidia article, Could you please refer to the comments above and let us know the outcome? Thank you!,"Hi, sorry for the late respone. Adding fetchkeys does solve the issue.", Thank you for the update! Could you please move this issue to closed status if it is resolved ? Thank you!,Are you satisfied with the resolution of your issue? Yes No
722,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Wrong prediction every time using the person detection model on ESP32-WROVER-IE board )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version Microlite esp32 firmware   Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version Python 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ASabovic,Wrong prediction every time using the person detection model on ESP32-WROVER-IE board ,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version Microlite esp32 firmware   Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version Python 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-12T11:01:27Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:micro TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56079,Hi  ! Thanks for reporting the bug.  ! Could you please look at this issue?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
623,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unit test //tensorflow/tools/docs:tf_doctest is broken by protobuf exception)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,elfringham,Unit test //tensorflow/tools/docs:tf_doctest is broken by protobuf exception,Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version 5.1.1  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-12T10:47:13Z,type:docs-bug type:bug type:build/install,closed,0,4,https://github.com/tensorflow/tensorflow/issues/56078,   ,CC toplay ,Fixed: https://github.com/tensorflow/tensorflow/commit/44e84cebef5a89a0840f8d401819c25af41ec3db,Are you satisfied with the resolution of your issue? Yes No
870,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Model fails to predict using float16)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.8.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue For instance, the model below gives `NaN`:  However, the following gives the correct result:  Same result if `tf.keras.backend.set_floatx('float16')` was called instead. Does anyone know why this might be the case? ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hoonkai,Model fails to predict using float16,"Click to expand!    Issue Type Support  Source source  Tensorflow Version tf 2.8.0  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue For instance, the model below gives `NaN`:  However, the following gives the correct result:  Same result if `tf.keras.backend.set_floatx('float16')` was called instead. Does anyone know why this might be the case? ```  Relevant log output _No response_",2022-05-11T17:25:13Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/56065," , I was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist of it here.",Thanks for opening this issue. Development of keras moved to separate repository https://github.com/kerasteam/keras/issues Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1990,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow 2.8.0 autograph transformation failure,  issues with : 'arguments' object has no attribute 'posonlyargs' and, decorate the function with @tf.autograph.experimental.do_not_convert)ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: yes, based on this tutorial : https://github.com/nmarincic/machineintelligence/blob/master/Exercise%2014%20%20Classifying%20HandWritten%20Digits.ipynb    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: source    **TensorFlow version (use command below)**: 2.8.0    **Python version**: 3.7.3    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**: Intel(R) HD Graphics , VRAM 64MB    **Exact command to reproduce**: Running)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,barkaPiy,"Tensorflow 2.8.0 autograph transformation failure,  issues with : 'arguments' object has no attribute 'posonlyargs' and, decorate the function with @tf.autograph.experimental.do_not_convert","Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: yes, based on this tutorial : https://github.com/nmarincic/machineintelligence/blob/master/Exercise%2014%20%20Classifying%20HandWritten%20Digits.ipynb    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: source    **TensorFlow version (use command below)**: 2.8.0    **Python version**: 3.7.3    **Bazel version (if compiling from source)**:    **GCC/Compiler version (if compiling from source)**:    **CUDA/cuDNN version**:    **GPU model and memory**: Intel(R) HD Graphics , VRAM 64MB    **Exact command to reproduce**: Running",2022-05-11T08:56:06Z,stat:awaiting response type:support stale comp:autograph TF 2.8,closed,0,17,https://github.com/tensorflow/tensorflow/issues/56059," , I was facing different error while trying to execute the mentioned code.Please find the gist of it here. Also i request can you please take a look at the comment from the issue and thread1 which helps to deliver the similar error.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,> train_labels_new = np.array([convert_label(l) for l in train_labels]) > validation_labels_new = np.array([convert_label(l) for l in validation_labels]) > test_labels_new = np.array([convert_label(l) for l in test_labels]) Please may i know why you didnt add the above to the code you tried executing? ,"Hi  , I had tried to reproduce the issue from the mentioned above link where the code doesn't contain `train_labels_new` and the code shared is full of indentation errors.  Could you share a colab gist with issue reported or simple stand alone indented code with all dependencies such that we can replicate the issue reported.Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"> Hi  , I had tried to reproduce the issue from the mentioned above link where the code doesn't contain `train_labels_new` and the code shared is full of indentation errors. >  > Could you share a colab gist with issue reported or simple stand alone indented code with all dependencies such that we can replicate the issue reported.Thank you! Ok, thanks for your concern, i just ran a test on colab and here is the link  https://colab.research.google.com/drive/1udpgHvdKt8EeVWgHkVgtQ4_Rr1xOEChr?usp=sharing On Colab, it shows a different error after executing  Epoch 1/8  ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in ()       6 train_labels_new,       7 validation_data=(validation_data,validation_labels_new), > 8 epochs=8       9 ) 1 frames /usr/local/lib/python3.7/distpackages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)    1145           except Exception as e:   pylint:disable=broadexcept    1146             if hasattr(e, ""ag_error_metadata""): > 1147               raise e.ag_error_metadata.to_exception(e)    1148             else:    1149               raise ValueError: in user code:     File ""/usr/local/lib/python3.7/distpackages/keras/engine/training.py"", line 1021, in train_function  *         return step_function(self, iterator)     File ""/usr/local/lib/python3.7/distpackages/keras/engine/training.py"", line 1010, in step_function  **         outputs = model.distribute_strategy.run(run_step, args=(data,))     File ""/usr/local/lib/python3.7/distpackages/keras/engine/training.py"", line 1000, in run_step  **         outputs = model.train_step(data)     File ""/usr/local/lib/python3.7/distpackages/keras/engine/training.py"", line 864, in train_step         return self.compute_metrics(x, y, y_pred, sample_weight)     File ""/usr/local/lib/python3.7/distpackages/keras/engine/training.py"", line 957, in compute_metrics         self.compiled_metrics.update_state(y, y_pred, sample_weight)     File ""/usr/local/lib/python3.7/distpackages/keras/engine/compile_utils.py"", line 459, in update_state         metric_obj.update_state(y_t, y_p, sample_weight=mask)     File ""/usr/local/lib/python3.7/distpackages/keras/utils/metrics_utils.py"", line 70, in decorated         update_op = update_state_fn(*args, **kwargs)     File ""/usr/local/lib/python3.7/distpackages/keras/metrics.py"", line 178, in update_state_fn         return ag_update_state(*args, **kwargs)     File ""/usr/local/lib/python3.7/distpackages/keras/metrics.py"", line 729, in update_state  **         matches = ag_fn(y_true, y_pred, **self._fn_kwargs)     File ""/usr/local/lib/python3.7/distpackages/keras/metrics.py"", line 4078, in sparse_categorical_accuracy         y_true = tf.squeeze(y_true, [1])     ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[1]](remove_squeezable_dimensions/Squeeze)' with input shapes: [?,10].   PLS HOW CAN WE RESOLVE THE ISSUE THAT OCCURED ON MY COMPUTER AND COLAB? THANK YOU.",", If your model has one output/input layer then you can use Sequential API to construct your model, regardless of the number of neurons in the output and input layers.  On the other hand, if your model has multiple output/input layers, then you must use the Functional API to define your model (no matter how many neurons the input/output layers might have). Now, you have stated that your model has two output values and for each output value you want to use a different sample weighting.  To be able to do that, your model must have two output layers, and then you can set the sample_weight argument as a dictionary containing two weight arrays corresponding to two output layers. Thank you!","> , If your model has one output/input layer then you can use Sequential API to construct your model, regardless of the number of neurons in the output and input layers. >  > On the other hand, if your model has multiple output/input layers, then you must use the Functional API to define your model (no matter how many neurons the input/output layers might have). >  > Now, you have stated that your model has two output values and for each output value you want to use a different sample weighting. To be able to do that, your model must have two output layers, and then you can set the sample_weight argument as a dictionary containing two weight arrays corresponding to two output layers. Thank you! Thank you. Please based on the code i provided via colab, can you show me a version that would make it work? Thank you.","Hi , I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist here.","> Hi , I was able to reproduce the issue on tensorflow v2.8, v2.9 and nightly. Kindly find the gist here. I am glad you were able to spot the issue. I pray we find all the solutions to the issue in it's various forms.  Thank you.",",  There are few things to correct from your code.  1.  In the output layer, instead of having `sigmoid` activation function( which is used for binary classification) change it to `softmax` activation function.  2. Instead of creating the one hot label, you can retain the normal indexed sparse label and in loss, you can use ""sparse_categorical_crossentropy"" which expects sparse indexed label, not the one hot encoded label which you are doing with `convert_label()` function. 3. Learning rate(3.0) is too high, reduce it to fraction value(e.g 0.01) for your model to learn. All the above changes have been made in the gist here, and you can see the model. getting trained and the accuracy is increasing.","> , There are few things to correct from your code. >  > 1. In the output layer, instead of having `sigmoid` activation function( which is used for binary classification) change it to `softmax` activation function. > 2. Instead of creating the one hot label, you can retain the normal indexed sparse label and in loss, you can use ""sparse_categorical_crossentropy"" which expects sparse indexed label, not the one hot encoded label which you are doing with `convert_label()` function. > 3. Learning rate(3.0) is too high, reduce it to fraction value(e.g 0.01) for your model to learn. >  > All the above changes have been made in the gist here, and you can see the model. getting trained and the accuracy is increasing. Thanks for your input i am gradually going through it. Please in the meantime, the original values were used in the video  https://www.youtube.com/watch?v=YASRu_eT0Y0&t=3525s of the tutorial as i early posted and it worked. Why are we having issues with using the values as in the video/tutorial? Can we make it work like in the video for tensorflow 2.8.0 like on my computer and the version on colab? Thank you.","> , There are few things to correct from your code. >  > 1. In the output layer, instead of having `sigmoid` activation function( which is used for binary classification) change it to `softmax` activation function. > 2. Instead of creating the one hot label, you can retain the normal indexed sparse label and in loss, you can use ""sparse_categorical_crossentropy"" which expects sparse indexed label, not the one hot encoded label which you are doing with `convert_label()` function. > 3. Learning rate(3.0) is too high, reduce it to fraction value(e.g 0.01) for your model to learn. >  > All the above changes have been made in the gist here, and you can see the model. getting trained and the accuracy is increasing. Hello, i am still expecting a positive feedback, thank you and God bless","With the constant changes in the Tensorflow API, the error could be causing due to some of the changes made under the hood, but in video also he explains in the later part that no need to change the labels when using Tensorflow and above changes which I had mentioned earlier is the optimal way. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
913,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(No matching distribution found for tensorflow==2.5.3)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version tensorflow 2.5.3  Custom Code Yes  OS Platform and Distribution Ubuntu 18.04 and Monterey 12.3.1  Mobile device _No response_  Python version 3.7/3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I was trying to install tensorflow 2.5.3 by doing pip install as below, but it failed with no matching version   However, I do see 2.5.3 is a Pypi public released version https://pypi.org/project/tensorflow/2.5.3/  Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,RuofanKong,No matching distribution found for tensorflow==2.5.3,"Click to expand!    Issue Type Build/Install  Source binary  Tensorflow Version tensorflow 2.5.3  Custom Code Yes  OS Platform and Distribution Ubuntu 18.04 and Monterey 12.3.1  Mobile device _No response_  Python version 3.7/3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? I was trying to install tensorflow 2.5.3 by doing pip install as below, but it failed with no matching version   However, I do see 2.5.3 is a Pypi public released version https://pypi.org/project/tensorflow/2.5.3/  Standalone code to reproduce the issue   Relevant log output _No response_",2022-05-11T02:26:58Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 2.5,closed,0,7,https://github.com/tensorflow/tensorflow/issues/56057, Could you please follow the steps as mentioned here and refer to the tested build configurations for matching distribution of environments? Thank you!,"Hi , our workflow requires installing Tensorflow from binary (https://www.tensorflow.org/install/pip), rather than building from source.","1. Try pip install tensorflow==2.5 2. If 1 not work, check your python version by : python V"," `pip install tensorflow==2.5` installs 2.5.0 only, while the need here is 2.5.3.",Check if ur python version >= 3.7  !image," Ah I think I accidentally switched to 3.6 env which I didn't realize. After switching to my original 3.8 env, it's working now. Thank You! Closing the ticket now.",Are you satisfied with the resolution of your issue? Yes No
687,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AssertionError: Bazel does not support execution of Python interpreters via labels yet)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf1.5  Custom Code No  OS Platform and Distribution ubuntu20  Mobile device ubuntu20  Python version 3.7  Bazel version bazel0.26.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,liuyang77886,AssertionError: Bazel does not support execution of Python interpreters via labels yet,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf1.5  Custom Code No  OS Platform and Distribution ubuntu20  Mobile device ubuntu20  Python version 3.7  Bazel version bazel0.26.1  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-09T23:51:50Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 1.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/56041,"Hello  , We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable version 2.8 and let us know if you are facing same issue.Thanks!",æˆ‘æƒ³ç¼–è¯‘æˆ1.xç‰ˆæœ¬ã€‚æˆ‘æœ€å¼€å§‹ä½¿ç”¨tf2.8æºä»£ç ï¼Œä½¿ç”¨å‘½ä»¤./bazel5.1.1linuxx86_64 build config=v1 copt=mavx copt=mavx2 copt=msse copt=msse2 copt=mssse3   k //tensorflow/tools/pip_package:build_pip_packageï¼Œç¼–è¯‘å‡ºæ¥çš„è¿˜æ˜¯tf2ï¼Œä½¿ç”¨tf2.8æºä»£ç config=v1ä¸èƒ½ç¼–è¯‘å‡ºv1ç‰ˆæœ¬å—," , As suggested, can you please try to install latest stable tensorflow v2.8 from here.Also provide the error log.It helps to analyse the issue.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
659,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problem with gpu memory allocation)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory notebook version of nvidia gtx 1060 (6GB)  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JamesJacquesDiego,Problem with gpu memory allocation,Click to expand!    Issue Type Others  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.9.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.2  GPU model and memory notebook version of nvidia gtx 1060 (6GB)  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-09T00:45:37Z,stat:awaiting response stale type:others comp:gpu TF 2.8,closed,0,14,https://github.com/tensorflow/tensorflow/issues/56019," , Usually, this error raises when GPU memory is full.Please clear the memory and test your code.Also please take a look at this issue1 and issue2 with the similar error.Thanks!",Thank you for your suggestions. But I've checked gpu usage in task manager when script was running. And the usage was from 1% to 5%. It looks like tensorflow is not using gpu memory at all. In the info there was information that tensorflow tried to allocate about 10GB of memory (but I have only 6GB gpu). I've also tried memory_limit=4096 and allow_growth=True. PS When I use the same code but with cpu there is no problem.,This looks like a GPU related OOM. Here is a more reproducible example based on what you provided earlier. You could try varying the input size and see if reducing the input size starts the network training. ,"I've added three lines of code after y variable: config = tf.compat.v1.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction=0.5 sess = tf.compat.v1.Session(config=config)  I've run your code and first 5 times it worked, but after that I got error. Here is what i get when it works: 20220511 02:29:31.044577: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220511 02:29:31.044577: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 20220511 02:29:37.276618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3071 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: xxxx:xx:xx:x, compute capability: 6.1 20220511 02:29:37.415072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3071 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: xxxx:xx:xx:x, compute capability: 6.1 Epoch 1/4  And here is what I get when it doesn't work:  I used OHM to check gpu memory values. I've added images. Unfortunately my script with image classification still not working at all.","The lines you added have no effect, I assume you read this issue, you need to use the instructions related to TensorFlow 2. However, it seems that you don't have enough memory on the GPU.  To test this assertion you could change the input size( e.g.  `x = np.random.rand(1000, 200, 256, 3)` to  `x = np.random.rand(1000, 100, 100, 3)` ) and see if the error persis","So those lines don't work?  The code you've pasted  (with input size you set) works first few times and after 4th or 5th time I get error.  It works after I've change input size to `x = np.random.rand(1000, 100, 100, 3)` as you advised but reducing resolution of my images in my network is not a solution. What if I will need more images in the future to train my network or if smaller resolution would be bad for classification results? When I tried to run the script with my network then I get info that tensorflow tries to allocate about 10GB. I have gtx 1060 so I have only 6GB.  Don't get me wrong. Thanks for your suggestions and help. I am grateful but I'm trying to find the best solution. I have only about 25000 images in my dataset. Some people train networks with few millions images. I don't know how they do it. Do they divide their data somehow into chunks?"," , In order to expedite the troubleshooting process, could you please provide the complete code to reproduce the issue reported here.Thanks!","Well, it's almost a complete code. Above I load my train and test images with labels. Below I generate plots to display data. I've decreased resolution of my images and now it works. But it should be some way to load only part of dataset to gpu memory or create additional memory like with ram we can do (virtual ram)?"," , I was facing different error while executing the mentioned code.Please find the gist of it here.","train_images and train_labels are images and classes which i load. Here is used `x = np.random.rand(1000, 200, 256, 3) y = np.array([rand.integers(20) for i in range(1000)])` to simulate those data"," , I donot find any error while trying to execute the given code. Kindly find the gist of it here. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,no solution ?
660,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA]  Regression in NLP Models w/ error Tile Op must be a compile time constant)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.9  Bazel version 4.2.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2  GPU model and memory Tesla V100 , 16384MiB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,codeislife99,[XLA]  Regression in NLP Models w/ error Tile Op must be a compile time constant,"Click to expand!    Issue Type Bug  Source source  Tensorflow Version TF 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.9  Bazel version 4.2.1  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2  GPU model and memory Tesla V100 , 16384MiB  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-05-08T02:59:06Z,stat:awaiting response type:bug stale comp:ops comp:xla TF 2.8,closed,0,16,https://github.com/tensorflow/tensorflow/issues/55973,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!","run_summarization.py code to be run with XLA Flags `export XLA_FLAGS=""xla_gpu_autotune_level=2 xla_gpu_enable_async_all_reduce""  export TF_XLA_FLAGS=""tf_xla_auto_jit=2  tf_xla_async_compilation""`","  If you check out the Steps to Reproduce section I have mentioned how to reproduce the issue, which I have further elaborated on in the above comment. Is there anything specific you want ? ",Has there been any progress on this issue ? Were you able to replicate the issue ? ," Sorry for the late response!  I tried to replicate the issue reported here, please find the gist here and confirm the same ? Thank you!","Hi,      No this is not the issue. Can you run it on a GPU? Right now the script is not able to run because you are running it on a CPU. You can find the CUDA versions in the issue description but that shouldn't matter for the error. ",Hi  were you able to replicate the issue? This is a regression from TF2.6 and it seems to be really problematic as an user.  ,"Hi, following up on this issue. ","Hi, another ping on this issue. please let me know if there is somewhere else where I need to post this issue to get a response? ","Hello,  any progress on this issue ? ","Hello, is there anyone willing to take a look at this issue ? ",Hi  ! I am getting different error  (dataset error )in 2.9 . Could you provide a colab gist from your end. Thank you!,"Hi, The error which you have reported seems to have resolved, now it it throwing the error `TypeError: Dataset argument should be a datasets.Dataset!` which means the issue is with the wrong input of the data provided, could you please provide the input of type `datasets.Dataset` to fix the issue. Attached the gist here for reference. Thanks!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1870,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite build with CMake does not produce a binary on Windows)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Windows 10  Mobile device   Python version   Bazel version   GCC/Compiler version MSVC 19.31.31104.0  CUDA/cuDNN version   GPU model and memory   Current Behaviour?   Standalone code to reproduce the issue  When now checking the build directory with `ls a tflite_build/` there is no output file. shell PS C:\Users\Dario\dev\tflite_build> cmake ../tensorflow_src/tensorflow/lite/c  Building for: Visual Studio 17 2022  Selecting Windows SDK version 10.0.19041.0 to target Windows 10.0.19044.  The C compiler identification is MSVC 19.31.31104.0  The CXX compiler identification is MSVC 19.31.31104.0  Detecting C compiler ABI info  Detecting C compiler ABI info  done  Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe  skipped  Detecting C compile features  Detecting C compile features  done  Detecting CXX compiler ABI info  Detecting CXX compiler ABI info  done  Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe  skipped  Detecting CXX compile features  Detecting CXX compile features  done  Setting build type to Release, for debug builds use'DCMAKE_BUILD_TYPE=Debug'.  Looking for pthread.h  Looking for pthread.h  not found  Found Threads: TRUE  Performing Test standard_math_library_linked_to_automatically  Performing Test standard_math_library_linked_to_automatically  Success  Standard libraries to)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,CaptainDario,TensorFlow Lite build with CMake does not produce a binary on Windows,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution Windows 10  Mobile device   Python version   Bazel version   GCC/Compiler version MSVC 19.31.31104.0  CUDA/cuDNN version   GPU model and memory   Current Behaviour?   Standalone code to reproduce the issue  When now checking the build directory with `ls a tflite_build/` there is no output file. shell PS C:\Users\Dario\dev\tflite_build> cmake ../tensorflow_src/tensorflow/lite/c  Building for: Visual Studio 17 2022  Selecting Windows SDK version 10.0.19041.0 to target Windows 10.0.19044.  The C compiler identification is MSVC 19.31.31104.0  The CXX compiler identification is MSVC 19.31.31104.0  Detecting C compiler ABI info  Detecting C compiler ABI info  done  Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe  skipped  Detecting C compile features  Detecting C compile features  done  Detecting CXX compiler ABI info  Detecting CXX compiler ABI info  done  Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe  skipped  Detecting CXX compile features  Detecting CXX compile features  done  Setting build type to Release, for debug builds use'DCMAKE_BUILD_TYPE=Debug'.  Looking for pthread.h  Looking for pthread.h  not found  Found Threads: TRUE  Performing Test standard_math_library_linked_to_automatically  Performing Test standard_math_library_linked_to_automatically  Success  Standard libraries to",2022-05-07T14:42:14Z,stat:awaiting tensorflower type:build/install comp:lite subtype:windows TF 2.8,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55970,Hi  ! Can you check with CMake 3.23 and Visual Studio 2019 once ? Please give full path if error suggests CMake GUI or ask about Correct path. Thank you!,I can try this but I also tried building it on github actions with no success. The workflow can be found here. There it also just stops.,"Ok  ! Check in local machine once ( I commented bash part in local windows machine , Please try same) and let us know. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you., I tried it with vs 2019 and cmake 3.23 and it is still stuck at `generating code...`,Any new on this because I need to build a binary on windows., ,  could I get any comment on this?,Hi   I have tried with the latest stable version `r2.12` on windows and was able to build the `tensorflowlite_c.dll` successfully without any error. Please find the screenshots below.   Can you check with r2.12 and let us know if issue still persists. Thanks.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,"I checked it today, and there is no file in the current directory. You are right there is a `.dll` being generated in `debug/`, but the documentation clearly states  which happens on MacOS and Linux. Therefore I assume(d) the file in the debug folder is for some kind of debugging. If that is not the case the documentation should be updated.",Are you satisfied with the resolution of your issue? Yes No
1763,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segfualt in tf.ragged.segment_ids_to_row_splits)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.1.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.6.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell 20220506 18:49:54.747657: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 20220506 18:49:54.747681: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303) 20220506 18:49:54.747723: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist 20220506 18:49:54.747996: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA 20220506 18:49:54.779246: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz 20220506 18:49:54.786014: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ebcefb66d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 20220506 18:49:54.786061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version Segmentation fault (core dumped)  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,DNXie,Segfualt in tf.ragged.segment_ids_to_row_splits,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf2.1.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.6.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue  shell 20220506 18:49:54.747657: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 20220506 18:49:54.747681: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303) 20220506 18:49:54.747723: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist 20220506 18:49:54.747996: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA 20220506 18:49:54.779246: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz 20220506 18:49:54.786014: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ebcefb66d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 20220506 18:49:54.786061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version Segmentation fault (core dumped)  ",2022-05-06T18:53:11Z,stat:awaiting response type:bug stale comp:apis TF 2.1,closed,1,7,https://github.com/tensorflow/tensorflow/issues/55964,2.1.0 is no longer supported. Does this happen in TF 2.8 too?, I tried to replicate the issue on colab using TF v2.8.0 and faced InvalidArgumentError.Please have a look at this gist and confirm the same?  Thank you!,"In TF head this is a proper error for invalid call:  As such, I don't think this is a relevant issue anymore.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,   Yes. It looks like the bug has been fixed. Thank you!
760,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GlobalAveragePooling2D causes the inconsistency of output between frameworks)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.2 LTS (GNU/Linux 5.4.074generic x86_64)  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version gcc (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0  CUDA/cuDNN version CUDA Version: 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Crystal-yyy,GlobalAveragePooling2D causes the inconsistency of output between frameworks,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf2.0  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04.2 LTS (GNU/Linux 5.4.074generic x86_64)  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version gcc (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0  CUDA/cuDNN version CUDA Version: 11.2  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-06T13:30:47Z,stat:awaiting response type:bug stale TF 2.0,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55947,"yyy  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Please make sure that you're using TF v2.4 or later as older versions are not actively supported.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1870,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build tensorflow failed using GCP RBE for python numpy_include module)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf master, 2.8, 2,7.3  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version bazel 5.1.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  but the file exist in local build machine  shell  bazel build config=rbe_cpu_linux //tensorflow/tools/pip_package:build_pip_package config=tensorflow_testing_rbe_linux shell  bazel build config=rbe_cpu_linux //tensorflow/tools/pip_package:build_pip_package config=tensorflow_testing_rbe_linux WARNING: Option 'project_id' is deprecated: Use bes_instance_name instead INFO: Invocation ID: 6a945e3f3528413ea11058f8d1156089 INFO: Streaming build results to: https://source.cloud.google.com/results/invocations/6a945e3f3528413ea11058f8d1156089 INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=237 INFO: Reading rc options for 'build' from /data/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /data/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lshmouse,Build tensorflow failed using GCP RBE for python numpy_include module,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf master, 2.8, 2,7.3  Custom Code No  OS Platform and Distribution Ubuntu 20.04.4 LTS  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version bazel 5.1.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?  but the file exist in local build machine  shell  bazel build config=rbe_cpu_linux //tensorflow/tools/pip_package:build_pip_package config=tensorflow_testing_rbe_linux shell  bazel build config=rbe_cpu_linux //tensorflow/tools/pip_package:build_pip_package config=tensorflow_testing_rbe_linux WARNING: Option 'project_id' is deprecated: Use bes_instance_name instead INFO: Invocation ID: 6a945e3f3528413ea11058f8d1156089 INFO: Streaming build results to: https://source.cloud.google.com/results/invocations/6a945e3f3528413ea11058f8d1156089 INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=237 INFO: Reading rc options for 'build' from /data/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /data/tensorflow/.bazelrc:   'build' options: define framework_shared_object=true define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library ",2022-05-06T12:46:53Z,stat:awaiting response type:build/install stale subtype:bazel TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55946,"google RBE use docker image: gcr.io/tensorflowtesting/noslacuda11.2cudnn8.1ubuntu20.04manylinux2014multipython for remote build. In the docker image,  numpy souce files are in /usr/local/lib/python3.8/sitepackages/numpy/, and no file: numpy/core/include/numpy/.doxyfile bazel verbose build logs are followingsï¼š "," , Can you please provide the source link from where you are trying to install the tensorflow.Also i request,please take a look at this issue1 and issue2 with the similar error.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
808,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(hexagon delegate fails with -- dlopen failed: cannot locate symbol ""dlopen"")ï¼Œ å†…å®¹æ˜¯ (I'm trying to get hexagon (Qualcomm's NN accelerator DSP) working on Android, but following the instructions leads me here. When I run my app, I get the following crash:  I'm trying to run this on the `tflite` `object_detection` example app from `github.com/tensorflow/examples`, on Android. My Android device:  If I `adb shell` into `/data/app/~~oHfy74UV0cmPBaaworOtcA==/org.tensorflow.lite.examples.detectionKxIoTs7gEXEEvawBkjkh0A==/lib/arm64`, and `ls`, then I see the following files:  It seems very strange that `dlopen` itself is not found!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bmharper,"hexagon delegate fails with -- dlopen failed: cannot locate symbol ""dlopen""","I'm trying to get hexagon (Qualcomm's NN accelerator DSP) working on Android, but following the instructions leads me here. When I run my app, I get the following crash:  I'm trying to run this on the `tflite` `object_detection` example app from `github.com/tensorflow/examples`, on Android. My Android device:  If I `adb shell` into `/data/app/~~oHfy74UV0cmPBaaworOtcA==/org.tensorflow.lite.examples.detectionKxIoTs7gEXEEvawBkjkh0A==/lib/arm64`, and `ls`, then I see the following files:  It seems very strange that `dlopen` itself is not found!",2022-05-05T09:43:31Z,stat:awaiting tensorflower type:bug comp:lite TFLiteHexagonDelegate,closed,0,50,https://github.com/tensorflow/tensorflow/issues/55934,Hi  ! Can you check the instruction in this thread once? Thanks!,"Hi , Thanks for the help. I've read through that Stack Overflow thread, but I can't find any specific instructions in there. I'm not explicitly doing any NDK build steps.. so I don't know what I can change. These are the first few lines of my `build.gradle` file: ", ! I was asking to lower minSdkVersion to 16 or 18 once and try again.  ! Could you please look at this issue? Thanks!,"OK... I've tried minSdkVersion at 16 and 18, but I still get the same issue.", Can you share how to reproduce this.  Thanks," , i am also seeing the same issue. i can run the tflite model using benchmark_model on adb command line and using nnapi delegage as well. however when i use hexagon delegate in my android app (java code), get the above error. simply following instructions here https://www.tensorflow.org/lite/android/delegates/hexagon pls note that there are no instructions on how to add the lib hexagons (.so files) to the android app. some clear instructions would be helpful","Sorry for the late reply on this . Instructions for reproducing: 1. Start from this example: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android 2. Switch from lib_task_api to lib_interpreter 3. Follow the instructions at https://www.tensorflow.org/lite/android/delegates/hexagon to include the hexagon delegate into the app 4. Inside `TFLiteObjectDetectionAPIModel.java`, create the hexagon delegate. A snippet of my code: ","    java.lang.UnsatisfiedLinkError: dlopen failed: ""/data/app/com.example.imageclassificationlivefeedhCcRkwcAyiruoow2ar2RJA==/base.apk!/lib/arm64v8a/libhexagon_nn_skel_v66.so"" is 32bit instead of 64bit when i am explicitly loading the /libhexagon_nn_skel_v66 library which i downloaded from the tensorflow page  https://www.tensorflow.org/lite/android/delegates/hexagon  , the error says these libraries are 32 bit (and not 64 bit) and our app has to be built for arm64v8a. ",Weiyi Wang Can you please have a look,Are you satisfied with the resolution of your issue? Yes No,"  , is there any resolution ? why is this getting closed?"," A fix has been sent and closed the issue. Please try using the newest nightlies, and let us know if you are having issues.",bmharper Please retry ," Thanks for the update, but I'm still getting the exact same issue:  Perhaps you could share an Android project that successfully loads the hexagon delegate? I don't need a comprehensive example  just enough structure to get the library loaded. I'm probably doing something stupid with my project structure, but I can't tell what it is.", i am also seeing the same issue as reported by . its not solved or we are missing something in the build,Hi  and   I retried with object detection example and hexagon delegate succeeded to load and function. See commit for changes switching to hexagon delegate. You also need to * Change the build variant to interpreter * Add hexagon libraries to your app. Instructions here. Let me know if it works for you.," Thanks for the example code. I am doing precisely that. I think the problem is my inclusion of the `.so` libraries. Is it sufficient to extract the files so that they appear like this?  I notice that no matter where I place the `.so` files, my build doesn't seem to change, and I get the same exception.","I think the problem is because we didn't release a new nightly, so you are still picking the old libraries without the fix. Will check this tomorrow, and update here. Sorry for the trouble.", thanks for the note!! will wait for your update,"   We have a fresh nightly released, could you retry and see if it's fixed? Thanks!","    incorporating the new nightly and your changes above is making things worse with hexagon delegate. 1. with the old libraries, the delegate was getting created and partitioning had some issues. see dump below ======================================================================== 20220603 08:18:51.446 65096509/com.example.imageclassificationlivefeed I/tflite: TfLiteHexagonDelegate delegate: 63 nodes delegated out of 64 nodes with 1 partitions. 20220603 08:18:51.446 65096553/com.example.imageclassificationlivefeed W/com.example.imageclassificationlivefeed: vendor/qcom/proprietary/commonsysintf/adsprpc/src/apps_std_imp.c:749: Warning: fopen returned 0xd for file libhexagon_nn_skel_v66.so. (Permission denied) 20220603 08:18:51.446 65096509/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror cannot open libhexagon_nn_skel_v66.so) 20220603 08:18:51.446 65096509/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp 20220603 08:18:51.448 65096509/com.example.imageclassificationlivefeed E/AndroidRuntime: FATAL EXCEPTION: main     Process: com.example.imageclassificationlivefeed, PID: 6509     java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.imageclassificationlivefeed/com.gmac.cameradetection.MainActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: hexagon_nn_config failed. Error: 1     Delegate kernel was not initialized     Node number 64 (TfLiteHexagonDelegate) failed to prepare.     Restored original execution plan after delegate application failure. ===================================================================== 2. with new libraries  incorporated using the following gradle file changes as indicated in your example:     implementation 'org.tensorflow:tensorflowlite:0.0.0nightlySNAPSHOT'     implementation 'org.tensorflow:tensorflowlitemetadata:0.0.0nightlySNAPSHOT'     implementation 'org.tensorflow:tensorflowlitehexagon:0.0.0nightlySNAPSHOT' the error is incompatibility of hexagon libraries as shown below and hexagon delegate is not even created: ======================================================= 20220603 08:29:54.347 77987842/com.example.imageclassificationlivefeed W/com.example.imageclassificationlivefeed: vendor/qcom/proprietary/commonsysintf/adsprpc/src/apps_std_imp.c:749: Warning: fopen returned 0xd for file libhexagon_nn_skel_v66.so. (Permission denied) 20220603 08:29:54.347 77987798/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror cannot open oemconfig.so) 20220603 08:29:54.348 77987798/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp 20220603 08:29:54.348 77987798/com.example.imageclassificationlivefeed W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide. 20220603 08:29:54.348 77987798/com.example.imageclassificationlivefeed I/tflite: Hexagon Delegate is not supported. ======================================================== as reported earlier, these tests are run on Snapdragon 855+, where we are able to run benchmark_model using hexagon delegate, hence functionality is possible on this device.","I've tried running this project again, but I get the exact same error as before:   `java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""dlopen"" referenced by ""/data/app/...` To invalidate my gradle cache (I'm building from Android Studio), is it sufficient to `rm rf ~/.gradle/caches/`? Can somebody please post a concrete example of where their `hexagon` `.so` files are located. I still have a suspicion that mine are in the wrong place, because regardless of where I put them in the project directory tree, I get the same error.","  1) Which device are you trying on ? 2) Can you please verify that all the libraries are correctly bundled with the App and share the files that are bundled here. From the error message it might be wrong file access. 3) can you run adb shell cat /sys/devices/soc0/soc_id and share the result  This error is not related to where the libraries should be, it was an incorrect linking flag, which was fixed. I think you are probably not picking the latest. gradle cache might be a reason. Try ""File > Invalidate Cache"" from android studio.","1. Xiaomi Redmi Note 9 Pro 2. Files in `lib` extracted out of my `apk`:  3. Contents of `soc_id` is 443 I have tried `File > Invalidate Cache` from Android Studio. Perhaps you can post a sha256 of `libtensorflowlite_hexagon_jni.so`, so that we can verify that I'm pulling the right one?",">  >  > 1. Which device are you trying on ? Snapdragon 855+ Oneplus 7t device.  > 2. Can you please verify that all the libraries are correctly bundled with the App and share the files that are bundled here. From the error message it might be wrong file access. pls see attached the tf libraries in the apk, (the files were downloaded from the v20.0.1 on tf page) arm64v8a.zip also when above files are bundled in the apk , get the following error that libhexagon_nn_skel_v66 cannot be loaded. 20220603 15:51:41.054 2071920764/com.example.imageclassificationlivefeed W/com.example.imageclassificationlivefeed: vendor/qcom/proprietary/commonsysintf/adsprpc/src/apps_std_imp.c:749: Warning: fopen returned 0xd for file libhexagon_nn_skel_v66.so. (Permission denied) 20220603 15:51:41.055 2071920719/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror cannot open oemconfig.so) 20220603 15:51:41.055 2071920719/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp 20220603 15:51:41.055 2071920719/com.example.imageclassificationlivefeed W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide. 20220603 15:51:41.055 2071920719/com.example.imageclassificationlivefeed I/tflite: Hexagon Delegate is not supported. > 3. can  you run >    adb shell cat /sys/devices/soc0/soc_id >    and share the result OnePlus7T:/ $ cat /sys/devices/soc0/soc_id 339 Also note that even with the above errors, the apk is running the model (object detection). but not on hexagon"," After you clears your gradle cache (usually ~/.gradle/cache) and android studio cache, you might need to restart your android studio and uninstall the app installed on your device and rebuild / reinstall it. > Perhaps you can post a sha256 of libtensorflowlite_hexagon_jni.so since it's a nightly build, sha256 won't provide much value. One way to verify you are having the fixed .so is to check the dl symbols For either arm64 or armeabi build  shall give you a linked dlopen if your lib is updated > ... dlopen instead of an unlinked one > ... dlopen You can also check latest libs from OSS Sonatype Nexus repository.  your files includes outdated libs. See comments above for verification steps."," thanks for your comments. can you pls specify which libs are outdated?  I am including the latest libs (libhexagon_nn_skel.so, libhexagon_nn_skel._v65.so,libhexagon_nn_skel_v66.so) from the tensorflow page here https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run   can you pls also specify the verification steps ? which comments should i look for? Can you pls share a object detection app github link which has working hexagon delegate, we will just try to replicate that to see if it works.",  Thanks for the information. The hexagon_nn libs (downloaded from the tf page) are fine.  It's `libhexagon_interface.so` and `libtensorflowlite_hexagon_jni.so` that requires updating. Those are managed / downloaded by gradle dependency `implementation 'org.tensorflow:tensorflowlitehexagon:0.0.0nightlySNAPSHOT'` so probably your apps are still using the cached libs that are downloaded before the new release. Instructions for verifying if your libs are up to date is in this comment. Example commit and instructions for object detection example w/ hexagon is in this comment.,"  , thanks for your instructions. I uninstalled the app on device, invalidated cache, removed all build artefacts , restarted android studio, rebuilt, but still see the same issue (cannot open oemconfig.so and load libhexagon_nn_skel_v66.so) 0220604 08:35:58.919 90389082/com.example.imageclassificationlivefeed W/com.example.imageclassificationlivefeed: vendor/qcom/proprietary/commonsysintf/adsprpc/src/apps_std_imp.c:749: Warning: fopen returned 0xd for file libhexagon_nn_skel_v66.so. (Permission denied) 20220604 08:35:58.919 90389038/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror cannot open oemconfig.so) 20220604 08:35:58.919 90389038/com.example.imageclassificationlivefeed E/com.example.imageclassificationlivefeed: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp also checked for dlopen using the command below. amitmate1GSNMRDR:$ readelf s libtensorflowlite_hexagon_jni.so | grep dlopen      5: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND dlopen pls see attached all the arm64v8a libs from the apk. libhexagon_apks.zip","   , unless i am missing something, the issue seems to be that in the object detection github code example that you folks shared earlier with hexagon delegate changes does not instantiate the code for Hexagon delegate. I recompiled that app with the mentioned changes and i did not see any hexagon ""delegate"" logs . pls double check. my suspicion is that the following line in the build.gradle file is causing the apk to use an alternative path (non hexagon delegate) "" taskApiImplementation project("":lib_task_api"")"" if you force the above path through the lib_interpreter where you have made the changes for hexagon delegate, you will see the same errors that rest of us are seeing"
753,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registered error in load_library.py)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version '2.4.1'  Custom Code No  OS Platform and Distribution Linux Kubuntu 18.04  Mobile device _No response_  Python version Python 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit: 10.1.243 ; cudnn:7.6.5  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,edehino,tensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registered error in load_library.py,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version '2.4.1'  Custom Code No  OS Platform and Distribution Linux Kubuntu 18.04  Mobile device _No response_  Python version Python 3.9.12  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cudatoolkit: 10.1.243 ; cudnn:7.6.5  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-05T07:48:53Z,stat:awaiting response type:bug stale TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55932," I tried to replicate the issue on colab using TF v2.8.0 ,please find the gist here and confirm the same? Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
618,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build error)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution GNU C17 (Gentoo x86_64pclinuxgnu)  Mobile device _No response_  Python version 3.10  Bazel version 4.2.2  GCC/Compiler version 11.2.1_p20220115 p4  CUDA/cuDNN version no  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,malk90,Build error,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution GNU C17 (Gentoo x86_64pclinuxgnu)  Mobile device _No response_  Python version 3.10  Bazel version 4.2.2  GCC/Compiler version 11.2.1_p20220115 p4  CUDA/cuDNN version no  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-05T06:08:37Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55931,You need a more recent bazel for that. Try `5.1.1.`," , Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"I've also encountered this error (GPU support tho, and for Windows), both with bazel 4.2.1 and 4.2.2. when I tried what  suggested and tried bazel 5.1.1, I got `ERROR: An error occurred during the fetch of repository 'local_config_cuda':` this is using a tested build configuration, so I don't know what is up with this. please help. ","Sadly, I don't know what error occurred during that fetch. Might have been just a transient error and retrying would work.",Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1568,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TfLite for Microcontrollers giving hybrid error)ï¼Œ å†…å®¹æ˜¯ (I converted my keras .h5 file to a quantized tflite in order to run on the new stm32f746   but when I run it I get an error saying ""Hybrid Models are not supported on TFLite Micro."" I'm not sure why my model is appearing as hybrid; the code I used to convert is below: `      model = keras.models.load_model('2D_2CHAN_32_T1_model_20210917_112755.h5')        converter = tf.lite.TFLiteConverter.from_keras_model(model)        converter.optimizations = [tf.lite.Optimize.DEFAULT]        converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]        tflite_model = converter.convert()        source_text, header_text = convert_bytes_to_c_source(tflite_model,""best_2Duint8"")            with  open('best_2Duint8.h',  'w')  as  file:                  file.write(header_text)            with  open('best_2Duint8.cc',  'w')  as  file:                  file.write(source_text)` If I didn't do the quantization, I could have used the input of Float32 to get the right result, but the model was too big and too slow. If I quantify it, I get the same error whether I change the input to uint8_t or uint16_t. Error message:../tensorflow/tensorflow/lite/micro/kernels/conv.. Node CONV_2D (number 4) failed to invoke with status 1 I'd appreciate if someone could guide me if I'm doing something wrong or if there is a better way to convert it.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ttxxdd123,TfLite for Microcontrollers giving hybrid error,"I converted my keras .h5 file to a quantized tflite in order to run on the new stm32f746   but when I run it I get an error saying ""Hybrid Models are not supported on TFLite Micro."" I'm not sure why my model is appearing as hybrid; the code I used to convert is below: `      model = keras.models.load_model('2D_2CHAN_32_T1_model_20210917_112755.h5')        converter = tf.lite.TFLiteConverter.from_keras_model(model)        converter.optimizations = [tf.lite.Optimize.DEFAULT]        converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]        tflite_model = converter.convert()        source_text, header_text = convert_bytes_to_c_source(tflite_model,""best_2Duint8"")            with  open('best_2Duint8.h',  'w')  as  file:                  file.write(header_text)            with  open('best_2Duint8.cc',  'w')  as  file:                  file.write(source_text)` If I didn't do the quantization, I could have used the input of Float32 to get the right result, but the model was too big and too slow. If I quantify it, I get the same error whether I change the input to uint8_t or uint16_t. Error message:../tensorflow/tensorflow/lite/micro/kernels/conv.. Node CONV_2D (number 4) failed to invoke with status 1 I'd appreciate if someone could guide me if I'm doing something wrong or if there is a better way to convert it.",2022-05-05T02:33:53Z,stat:awaiting response type:support comp:micro TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55928,Ok  ! Here are two things I suggest you to try (Assuming you are using 2.8 version for conversion to lite file) 1. Use xxd converter to get the C files from lite models. 2. Fully quantize the model using post training quantization (use a representative Dataset to do so) Attaching relevant thread for reference. Thanks!,"> Ok  ! Here are two things I suggest you to try (Assuming you are using 2.8 version for conversion to lite file) >  > 1. Use xxd converter to get the C files from lite models. > 2. Fully quantize the model using post training quantization (use a representative Dataset to do so) >  > Attaching relevant thread for reference. Thanks! I've run it successfully with full integer quantization, and I'm still using float32 for input and output, reducing the size of the model by more than half and improving the speed by about 30%", ! Thanks for the confirmation . Feel free to move this issue to closed status if it helped . Thanks!,Are you satisfied with the resolution of your issue? Yes No,"> > Ok  ! Here are two things I suggest you to try (Assuming you are using 2.8 version for conversion to lite file) > >  > > 1. Use xxd converter to get the C files from lite models. > > 2. Fully quantize the model using post training quantization (use a representative Dataset to do so) > >  > > Attaching relevant thread for reference. Thanks! >  > I've run it successfully with full integer quantization, and I'm **still using float32 for input and output**, reducing the size of the model by more than half and improving the speed by about 30% Hi , I have the same issue in using hybrid quantization in tflite micro   intput & output = float32   filter = int8, could you please share some information of how to use float32 for input and output Thanks!"
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.6 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`"")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.6 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e,2022-05-04T05:12:48Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55909
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`"")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.9 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e,2022-05-04T05:12:34Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55908
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.7 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`"")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.7 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e,2022-05-04T05:12:33Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55907
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.8 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`"")ï¼Œ å†…å®¹æ˜¯ (Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tensorflow-jenkins,"r2.8 cherry-pick: bd4d5583ff9 ""Prevent denial of service in `tf.ragged.constant`""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e,2022-05-04T05:12:30Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55906
1668,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.data.Dataset.from_generator fails when Tensorflow is imported inside an Abseil app)ï¼Œ å†…å®¹æ˜¯ (  Issue Type Bug  Source binary  Tensorflow Version https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.8.0cp38cp38manylinux2010_x86_64.whl  Custom Code No  Python version 3.8  Current Behaviour? `tf.data.Dataset.from_generator` fails with `Unable to find FunctionDef for __inference_Dataset_from_generator_generator_next_fn_24 in the registry. [Op:MakeIterator]` when tensorflow is imported within the body of a function when called from an Abseil app. Works when tensorflow is imported the standard way in the script header then called from within the Abseil app. Works when imported within a nonAbseil app function called from `__main__`. Only seems to fail when the import occurs inside a function called within an Abseil app. Only `tf.data.Dataset.from_generator` seems to be affected. Other pipelines can be used without issue and models can be trained on them. Tested with CPUonly Tensorflow 2.8.0 (also tested with 2.7.0) installed from prebuilt wheel.  `https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.8.0cp38cp38manylinux2010_x86_64.whl`  `https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.7.0cp38cp38manylinux2010_x86_64.whl`   Standalone code to reproduce the issue  Tensorflow deferred import in absl app function     Tensorflow deferred import in normal python function     Tensorflow standard import in header before absl app  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JossWhittle,tf.data.Dataset.from_generator fails when Tensorflow is imported inside an Abseil app,  Issue Type Bug  Source binary  Tensorflow Version https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.8.0cp38cp38manylinux2010_x86_64.whl  Custom Code No  Python version 3.8  Current Behaviour? `tf.data.Dataset.from_generator` fails with `Unable to find FunctionDef for __inference_Dataset_from_generator_generator_next_fn_24 in the registry. [Op:MakeIterator]` when tensorflow is imported within the body of a function when called from an Abseil app. Works when tensorflow is imported the standard way in the script header then called from within the Abseil app. Works when imported within a nonAbseil app function called from `__main__`. Only seems to fail when the import occurs inside a function called within an Abseil app. Only `tf.data.Dataset.from_generator` seems to be affected. Other pipelines can be used without issue and models can be trained on them. Tested with CPUonly Tensorflow 2.8.0 (also tested with 2.7.0) installed from prebuilt wheel.  `https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.8.0cp38cp38manylinux2010_x86_64.whl`  `https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.7.0cp38cp38manylinux2010_x86_64.whl`   Standalone code to reproduce the issue  Tensorflow deferred import in absl app function     Tensorflow deferred import in normal python function     Tensorflow standard import in header before absl app  ,2022-05-04T03:36:17Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:data TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55889," I tried to replicate the issue on colab ,could you please find this gist and confirm the same? Thanks!", I dont think this can be replicated directly in collab since the environment is different than when executing the app from the command line.  If you place those three snippets into separate .py files and then !python test1.py to run them from collab that may be enough. , Please see the following colab gist: https://colab.research.google.com/gist/JossWhittle/1010a5941911507eb82ac8838f7ccedc/55889.ipynb which reproduces the bug by writing the test scripts to files before executing them on the command line.," Thank you for the update!  I was able to reproduce the issue on colab , please find the gist here.Thank you!"," , I tried to execute the test cases test1, test2, test3 .py files on tensorflow v2.15 and observed that all the 3 test cases were executed without any issue/error. Kindly find the gist of it here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
602,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(//tensorflow/tools/docs:tf_doctest unit test broken)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,elfringham,//tensorflow/tools/docs:tf_doctest unit test broken,Click to expand!    Issue Type Bug  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution Ubuntu 20.04  Mobile device n/a  Python version 3.8.10  Bazel version 5.1.1  GCC/Compiler version 10.3.0  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-03T14:24:58Z,stat:awaiting tensorflower type:bug type:build/install subtype:bazel,closed,1,6,https://github.com/tensorflow/tensorflow/issues/55845,   ,Seems to have been introduced with https://github.com/tensorflow/tensorflow/commit/9fbc898fc61efd5ec06f18c423a8d2f57910c56e, Could you please help tag Rostam? Thank you!,"Yes, just tagged toplay (Rostam)",fixed by https://github.com/tensorflow/tensorflow/commit/8cf2f3c6a685a141c35e9ba2eb433fbe0bfa3174,Are you satisfied with the resolution of your issue? Yes No
368,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix typo and build error in MLIR-HLO's bazel BUILD file.)ï¼Œ å†…å®¹æ˜¯ (The previous version results in the following error:  This fixes build bug in `bazel build //tensorflow/compiler/mlir/hlo:*`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,JueonPark,Fix typo and build error in MLIR-HLO's bazel BUILD file.,The previous version results in the following error:  This fixes build bug in `bazel build //tensorflow/compiler/mlir/hlo:*`.,2022-05-03T13:20:59Z,,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55843,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). For more information, open the CLA check for this pull request.",Please open against master and then cherrypick from there,"I made a same pull request to master. As you said, I will close with this pull request from now on."
1365,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The `tf.linalg.svd` bug report!  Serious!)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.5 AND tf 2.8  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04 AND Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? A bug happened! I make a example net that contains a simple `keras.Sequence` of a `Conv3D` keras layer, a `tf.linalg.svd` operation that consists of a trainable parameter `thres` and another `Conv3D` keras layer.  When I calculate the gradients of the `Conv3D` layers, the `tf2.5` returns `nan` for the first `Conv3D` layer while the `tf2.8` seems to be normal but sometimes `tf2.8` reports error: `Segmentation Fault (core dumped)`, which I think may due to the `tf.linalg.svd`. please check it.  run the Standalone code to reproduce the issue, and we choose a very simple input (tf.ones) and the bug can be reproduced too.  We test it on Nvidia GV100 GPU and on `tf2.5` and `tf2.8`.  Standalone code to reproduce the issue  ```  Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,yhao-z,The `tf.linalg.svd` bug report!  Serious!,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.5 AND tf 2.8  Custom Code Yes  OS Platform and Distribution Linux Ubuntu 20.04 AND Windows 10  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? A bug happened! I make a example net that contains a simple `keras.Sequence` of a `Conv3D` keras layer, a `tf.linalg.svd` operation that consists of a trainable parameter `thres` and another `Conv3D` keras layer.  When I calculate the gradients of the `Conv3D` layers, the `tf2.5` returns `nan` for the first `Conv3D` layer while the `tf2.8` seems to be normal but sometimes `tf2.8` reports error: `Segmentation Fault (core dumped)`, which I think may due to the `tf.linalg.svd`. please check it.  run the Standalone code to reproduce the issue, and we choose a very simple input (tf.ones) and the bug can be reproduced too.  We test it on Nvidia GV100 GPU and on `tf2.5` and `tf2.8`.  Standalone code to reproduce the issue  ```  Relevant log output _No response_",2022-05-03T06:35:46Z,stat:awaiting response type:bug stale comp:apis TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/55840,> `tf2.8 reports error: Segmentation Fault (core dumped)`  I cannot reproduce this on Colab GPU (TF 2.8) Please note also that usually the fix in the older versions (<=1 year) will be done if that is a security bug or if the bug is critical and impacting larger community. In this case it is working fine or not reproducible with Tensorflow 2.8 you need to continue using this version.,"* Yes, you're right. The standalone code can be running without any error as the input of the `exam_net` being allone tensor on `tf 2.8`.  * When I use a large dataset to train the `exam_net`, __sometimes__ `Segmentation Fault` occurs. * Maybe I should find __the specific tensor__ that cause the `Segmentation Fault` on `tf 2.8`, and issue it later. * Thanks very much","> Maybe I should find the specific tensor that cause the Segmentation Fault on tf 2.8, and issue it later. Yes if you can isolate this it would be ideal. Extending the gist in a ""test like"" form it is a better solution: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/linalg/svd_op_test.py Also in the case you can reproduce it only on your local env but not on Colab (e.g. it is specific of your env/hardware) it would be ideal to test it locally in a reproducible env: https://www.tensorflow.org/install/docker","yes, thanks. I'll try it.",It could be nice if could extend the mentioned test contributing a PR to cover your case if you will find a way to reproduce it (on master/nightly).,"z I was not able to reproduce this issue on colab using TF v2.8.0,please find the gist here.Could you please refer to this comment above and and let us know the update on this?Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
718,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exporting LSTM with TFlite Converter yields model which makes bad predictions in contrast to keras model)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,leeflix,Exporting LSTM with TFlite Converter yields model which makes bad predictions in contrast to keras model,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.8  Custom Code No  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-05-02T21:49:13Z,stat:awaiting response type:support TFLiteConverter TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55835,Hi  ! Could you let us know reason behind not using Select Ops(As it is giving already correct predictions right)? ,When using Select Ops I have to link way bigger binaries.,You can reduce the binary size of your model using selective builds https://www.tensorflow.org/lite/guide/reduce_binary_size. Selective builds skip unused operations in your model set and produce a compact library with just the runtime and the op kernels required for the model to run on your mobile device.,"Thanks for the suggestion. I will look into it, but can someone explain why option 2 produces a model that does bad predictions?","This could be due to some of the OPS which are not supported in TFLite runtime, so having selective OPS has advantage to fallback to TF OPS, you can see the more detail here. You can check this comparison of binary file size when using different builds.",Are you satisfied with the resolution of your issue? Yes No
736,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF Windows CI test TIMEOUT error (intermittent) //py_test_dir/tensorflow/python/kernel_tests/nn_ops:xent_op_d9m_test_cpu TIMEOUT)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Windows Server 2019 32core  Mobile device _No response_  Python version 3.10  Bazel version 5.2.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bani-intelaipg,TF Windows CI test TIMEOUT error (intermittent) //py_test_dir/tensorflow/python/kernel_tests/nn_ops:xent_op_d9m_test_cpu TIMEOUT,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.9  Custom Code No  OS Platform and Distribution Windows Server 2019 32core  Mobile device _No response_  Python version 3.10  Bazel version 5.2.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-02T17:25:59Z,stat:awaiting response type:build/install stale subtype:windows TF 2.9,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55827,Tagging  ,"Will disable the test for now, unsure why it also runs on CPU (it should only be GPU)","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
623,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(unable to pickle keras.layers.StringLookup)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution colab  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ophiry,unable to pickle keras.layers.StringLookup,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution colab  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-02T14:32:56Z,stat:awaiting response type:bug comp:keras comp:ops TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55825," , I was able to reproduce the issue in tensorflow v2.7, v2.8 and nightly.Please find the gist of it here.",", Development of keras moved to separate repository https://github.com/kerasteam/keras/issues Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",", Can you please close this issue, since it is tracked here. Thank you.",Are you satisfied with the resolution of your issue? Yes No
828,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tflite model crashes before loading .tflite file in Google Colab, with no error or exception)ï¼Œ å†…å®¹æ˜¯ (I create a Visual Transformer (VIT) for image classification. After that i create the model using Tensorflow, i obtain a .pb folder with these files inside:   2 folders ""assets"" and ""variables""  2 files ""keras_metadata"" and ""saved_model"" I am now trying to convert this file into a "".tflite"" file but when i try to do the conversion and load the file .tfite, google colab crash and restart the run.  I am using the version 2.6.0 of both Tensorflow and Keras. This is my VIT:             Now this is ho i try to transform the model from PB to TFLITE )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llava,dgrnd4,"tflite model crashes before loading .tflite file in Google Colab, with no error or exception","I create a Visual Transformer (VIT) for image classification. After that i create the model using Tensorflow, i obtain a .pb folder with these files inside:   2 folders ""assets"" and ""variables""  2 files ""keras_metadata"" and ""saved_model"" I am now trying to convert this file into a "".tflite"" file but when i try to do the conversion and load the file .tfite, google colab crash and restart the run.  I am using the version 2.6.0 of both Tensorflow and Keras. This is my VIT:             Now this is ho i try to transform the model from PB to TFLITE ",2022-05-02T08:21:02Z,stat:awaiting response type:bug stale comp:lite comp:runtime 2.6.0,closed,0,11,https://github.com/tensorflow/tensorflow/issues/55819, i saw that you solved a problem that looks like mine. Do you have any recommendation?,Hi  ! Could you share the dataset used in above code too ? Please Check in 2.8/2.9.0rc1 too and let us know.Thanks!, cannot use it on versions > 2.7 because this NET will be implemented on a microcrontroller that doesn't support these new versions.  However because of privacy and stuff I cannot share the Dataset but it's a dataset of images with 4 classes! So whatever dataset you find with just 4 classes it's ok! There are 4k images for the TR and 1k images for the TE.,Hi  ! Could you please look at this issue? Thanks!,I solved the problem changing a little bit the net but however if you can solve the problem with the net above would be better! Many thanks!,"Hi , how did you solve the problem by changing the net? Why is solving the original better than your change? That is probably good information for resolving the original issue."," Hi, itâ€™s been a long time and to be honest I donâ€™t remember which were the changes I made. However probably the change I made was about memory so I changed the way I was giving in input the input sets (TR,TE,VAL). Another thing I made was reduce the way I work on the images processing: reduce che windows of the filter I think that can help reduce the RAM used.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1793,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow registering GPU with a lot less memory than available)ï¼Œ å†…å®¹æ˜¯ (I recently installed a new GPU in my workstation  the EVGA Nvidia RTX 3080 with 12Gb. After successfully installing the corresponding drivers on my Ubuntu 18.04, as well as installing the latest CUDA and cuDNN, I opened Python in a terminal to test Tensorflow. However, despite my GPU having exactly 12288MiB according to nvidiasmi (and only 435 Mb in usage by running processes Xorg and GnomeShell), tensorflow registered a device with only 9663 Mb. The exact message is: 20220501 22:59:37.900290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9663 MB  memory:  > device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:81:00.0, compute capability: 8.6 Note that while related to the closed issue **https://github.com/tensorflow/tensorflow/issues/22623**, mine cannot be explained in the same fashion because I am on Ubuntu (so the Windows issue described there does not apply) and also I do have nearly the entire 12Gb of GPU memory available per nvidiasmi  so no secondary processes are occupying the missing memory that Tensorflow is not allocating. Details: Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory RTX 3080 12Gb  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,baozzhao,Tensorflow registering GPU with a lot less memory than available,"I recently installed a new GPU in my workstation  the EVGA Nvidia RTX 3080 with 12Gb. After successfully installing the corresponding drivers on my Ubuntu 18.04, as well as installing the latest CUDA and cuDNN, I opened Python in a terminal to test Tensorflow. However, despite my GPU having exactly 12288MiB according to nvidiasmi (and only 435 Mb in usage by running processes Xorg and GnomeShell), tensorflow registered a device with only 9663 Mb. The exact message is: 20220501 22:59:37.900290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9663 MB  memory:  > device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:81:00.0, compute capability: 8.6 Note that while related to the closed issue **https://github.com/tensorflow/tensorflow/issues/22623**, mine cannot be explained in the same fashion because I am on Ubuntu (so the Windows issue described there does not apply) and also I do have nearly the entire 12Gb of GPU memory available per nvidiasmi  so no secondary processes are occupying the missing memory that Tensorflow is not allocating. Details: Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.8.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.6  GPU model and memory RTX 3080 12Gb  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-05-02T03:38:24Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:gpu TF 2.8,closed,0,11,https://github.com/tensorflow/tensorflow/issues/55818,"Hi I'm not sure what is going on with your new Nvidia RTX 3080 set up,  But  while looking at the file:> tensorflow/core/common_runtime/gpu/gpu_device  line 1536 There is a note that states: line 1536:   // 'memory_limit  // 'memory_limit' is the required memory size, but if the allocator with   // given tf_device_id was created before, we'll use it instead of creating a   // new one (as TF gpu device is a shared resource), in which case the actual   // memory limit represented by 'stats.bytes_limit' used by that allocator   // may be different (which should be an error).   //   // TODO(laigd): report error if memory_limit doesn't match   // stats>bytes_limit. I'm not sure if this helps. You could try rebooting your computer,", Could you please try with the comment above and let us know if it helps?Thanks!," do you mean simply restarting the computer? I tried and it did not help much: now my nvidiasmi shows there are only 281 Mb being used by other processes (which are Xorg and gnomeshell) out of the 12288 Mb available in the GPU. So, there are 12007 Mb available. Yet, after starting a Python prompt and entering:  > I get this output: > 20220505 13:31:33.835176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 9849 MB memory:  > device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:81:00.0, compute capability: 8.6 > '/device:GPU:0' That is, TensorFlow is allocating 12007  9849 = 2158 Mb less than actually available. I also tried setting tf.GPUOptions(per_process_gpu_memory_fraction=1.0) to see if that helped, but it did not."," Thank you for the update! I tried to replicate the issue reported here on colab,could you please have a look at this gist  and confirm the same?Please refer this link and let us know if it helps? Thanks!"," thanks for checking on this issue. I checked your gist, and yeah, if I run either code snippet on my machine, the problem originally described shows up: Tensorflow registers the GPU device allocating around 2 Gb less of the available GPU memory. As per the link you suggested, I read it all but I can't see how could it be of any help. That link is about profiling and optimizing the usage of GPUs. My problem comes clearly from a Tensorflow bug: Tensorflow is registering my GPU with around 2 Gb less memory than the GPU has available (already discounting the few Mb being used by system processes). "," Thank you for the update!  I was able to replicate the issue on colab , please have a look at the gist . Thank you!"," As mentioned in this forum, this issues persists for both linux and windows for Nvidia RTX 3080. ","Hi,   Apologies for the delay and I found similar issue 56661 and  provided workaround please check this comment, even I replicated the same issue on Google Colab It's working fine, for your reference I have attached Gist file here, please refer this official documentation about tf.config.LogicalDeviceConfiguration Could you please try above workaround and check, Is it resolving your issue ? If it's resolving your issue please feel free to close this issue ? If issue still persists, please let us know and if possible help us with error log to do further investigation to find out root cause for your issue ? Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
716,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Windows Build error with --cofig=mkl => ImportError: DLL load failed while importing _pywrap_tensorflow_internal)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows Server 2019  Mobile device _No response_  Python version 3.10  Bazel version 5.2.0  GCC/Compiler version Windows VC++ 2019  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bani-intelaipg,Windows Build error with --cofig=mkl => ImportError: DLL load failed while importing _pywrap_tensorflow_internal,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.9  Custom Code No  OS Platform and Distribution Windows Server 2019  Mobile device _No response_  Python version 3.10  Bazel version 5.2.0  GCC/Compiler version Windows VC++ 2019  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-05-01T20:03:09Z,stat:awaiting response stat:awaiting tensorflower type:build/install stale subtype:windows TF 2.9,closed,0,15,https://github.com/tensorflow/tensorflow/issues/55817,Tagging ,"Rostam, this looks like another cc_shared_library issue","We were using a nonLTS Bazel at that commit, to work around some other `cc_shared_library` issues. Can you try with 3637a0245a2b44f2de2aa84b6f8f40cac5600109 (the commit after the one you tested, which reverts back to LTS Bazel)? This way we'd know if the issue is Bazel or some `cc_shared_library` changelist.",Same error is there with commit b5fa6a4eecdcf69408708e98ba4de6debb880596,"Thanks for letting us know. We're working on the issue. Will update. On Sun, May 1, 2022 at 11:35 PM Banikumar Maiti (Intel AIPG)  wrote: > Same error is there with commit b5fa6a4 >  > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were assigned.Message ID: > ***@***.***> >","Penporn (), can you bisect this if this still happens? I couldn't find a cc_shared_library last week last week that could have cause this",intelaipg Is this a recent failure or preexisting one? When was the last buildable commit for you? I looked at the TFoneDNN nightly CI and saw a similar error message since 3/10/22.  ," I believe this has been broken for a while. I tried to go back to find a passing build with ""config=mkl"" setting, but haven't found one with my limited rebuild effort. I can use some guidance on a potential analysis of the why this import is potentially failing in Windows. Do we have to explicitly set some PATH/PYTHONPATH env var to help the build find the module? The pipeline you mentioned was a recent temporary effort and not a mainline pipeline.","Oh, I somehow got the impression that the failure was recent. If it was an older one then the theory that this is caused by `cc_shared_library` becomes viable again","intelaipg Debugging shared library failures can be tricky.  Do you see the error when NOT building with `config=mkl`?  If you can find a commit that builds and one that doesn't, it will help narrow down the underlying cause. Can you try building a green commit from 1 week ago, 2 weeks ago, and so on until you find one that builds? Then we can use gitbisect to narrow down which change caused this.","I got hit by exact same issue while building with `config=mkl` . I tried ProcessMonitor downloaded from Microsoft site to identify which DLL is missing and got below error. `8:44:18.8425549	python.exe	14196	CreateFile	C:\Python\Python310\libomp.dll	NAME NOT FOUND	Desired Access: Read Attributes, Disposition: Open, Options: Open For Backup, Open Reparse Point, Attributes: n/a, ShareMode: Read, Write, Delete, AllocationSize: n/a` `libomp.dll` is in the PATH but seems not loaded using PATH so I put it into `C:\Python\Python310` (python.exe installed directory) as work around, then build went through. Not sure this apply for everybody, and not sure how to resolve this from root, though hope this info help to resolve this issue.",intelaipg Could you please let us know if you tried to put the path into C:\Python\Python310 (python.exe installed directory) using the latest TF version and if the issue still persists? Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
685,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error when making directory to a google cloud storage bucket path)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution CentOS Linux release 7.4.1708  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cuda/11.2.2  cudnn/8.1a11  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,kunfang98927,Error when making directory to a google cloud storage bucket path,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution CentOS Linux release 7.4.1708  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version cuda/11.2.2  cudnn/8.1a11  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-30T19:01:09Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:cloud TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55816,", Looks like issue is with redirecting http to https was not clearly defined, so the url could change when the request for https for a specific service was made, this caused an error. Could you check the Cloud instance rule for HTTP and HTTPS. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Thank you for your reply.  I have no idea how to get the information of 'Cloud instance rule for HTTP and HTTPS' you mentioned above... Could you please give some hints? By the way, I add this line: `os.environ['NO_GCE_CHECK'] = 'true'` and the first line of the the error message I mentioned above disappears And the error message now is: ",", It seems that you're working with GCS. Was that your intention? Perhaps `TFDS_DATA_DIR` variable is not set and dataset builder falls back to: https://github.com/tensorflow/datasets/blob/8ec8cc47bc5bc406198a96e1353f0d581fed7a2d/tensorflow_datasets/core/dataset_builder.pyL287 Also can you confirm that this message is related to your dataset preparation not working? Seems like it could be just a warning that shouldn't prevent the script from executing (see https://github.com/tensorflow/datasets/issues/2761) You can try using:  https://github.com/tensorflow/datasets/issues/2761 Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1839,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.data batching slows down on Windows)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.2 cuDNN 8.1.0  GPU model and memory Nvidia A40 48GB, 128GB System RAM  Current Behaviour? While training a keras model using a custom tensorflow dataset, the training steps slow down once the remaining sample count in the first epoch is smaller than the shuffle buffer size. The slowdown carries over to the following epochs and takes longer and longer. I would expect the opposite behavior since all remaining samples should already be in the buffer and do not need to be loaded anymore. I have tested it on multiple Windows systems and this slowdown occurred on all of them. However, it does not happen on Linuxbased systems. I then used the tensorboard profiling plugin to investigate what is causing the slowdown. As you can see here, it seems to be an inputrelated problem: !profiler_1 From the input operations you can see that `Iterator::Root::Prefetch::BatchV2` takes the most time (this is also the case when removing the prefetching): !profiler_2 The slowdown can also be seen in the trace viewer: !profiler_3 Here is a more detailed comparison of `BatchV2` durations from different steps: !profiler_4 !profiler_5 Here is the profiling_data.zip.  Standalone code to reproduce the issue In order to reproduce and profile this issue the following code can be used. When running the `main.py` for the first time it will generate t)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,michaeldietz,tf.data batching slows down on Windows,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.2 cuDNN 8.1.0  GPU model and memory Nvidia A40 48GB, 128GB System RAM  Current Behaviour? While training a keras model using a custom tensorflow dataset, the training steps slow down once the remaining sample count in the first epoch is smaller than the shuffle buffer size. The slowdown carries over to the following epochs and takes longer and longer. I would expect the opposite behavior since all remaining samples should already be in the buffer and do not need to be loaded anymore. I have tested it on multiple Windows systems and this slowdown occurred on all of them. However, it does not happen on Linuxbased systems. I then used the tensorboard profiling plugin to investigate what is causing the slowdown. As you can see here, it seems to be an inputrelated problem: !profiler_1 From the input operations you can see that `Iterator::Root::Prefetch::BatchV2` takes the most time (this is also the case when removing the prefetching): !profiler_2 The slowdown can also be seen in the trace viewer: !profiler_3 Here is a more detailed comparison of `BatchV2` durations from different steps: !profiler_4 !profiler_5 Here is the profiling_data.zip.  Standalone code to reproduce the issue In order to reproduce and profile this issue the following code can be used. When running the `main.py` for the first time it will generate t",2022-04-30T15:34:42Z,stat:awaiting tensorflower type:bug comp:data TF 2.8,open,0,5,https://github.com/tensorflow/tensorflow/issues/55815, I tried to replicate the issue on colab and faced a different error. Could you please have a look at the gist here.Thanks!," The error in your gist happens because it uses an older version of tensorflowdatasets. I already tried to replicate the problem on colab with this gist but it does not happen there. As mentioned, it only happens on systems with a Windows operating system.","The difference may be in the malloc implementation. `batch` needs to allocate a large amount of memory to copy the batch data into. Linux and Windows have different malloc implementations, and it's possible that the Windows heap becomes increasingly fragmented, degrading malloc performance over time. To investigate further, I suggest profiling the malloc performance of the process."," , Could you please respond to the above comment and provide your insights. Thanks!","Oh I thought the comment was meant for someone from the tensorflow team since the status was still on ""awaiting tensorflower"". I don't know how to profile the malloc performance. However, I did observe that the problem always occurred at the same time, regardless of the memory of the system (I tested it with 128GB and 256GB RAM)."
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Value Assignment by specific indices error in Tensorflow (Assign values using mask))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Muhammad-Kaleem-Ullah,Value Assignment by specific indices error in Tensorflow (Assign values using mask),Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.10.2  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-30T12:39:08Z,stat:awaiting response type:feature stale comp:ops,closed,0,13,https://github.com/tensorflow/tensorflow/issues/55812,KaleemUllah  Can you please elaborate about your feature and please specify the use cases for this feature. Thanks!,"I'm building a custom loss function while converting Tensors into NumPy arrays by using .numpy(), which results in ""Gradients loss"". So, I must have to use Tensorflow to manipulate data. Here is what I'm trying:  > Output:  > Output: (10, 1000), (10, 1000) > (10, 1000) > Error values:  Then, I apply the mask to retrieve indices where y_true > 5 & y_false  5) i = mask[0] j = mask[1] print(i[:5]) print(j[:5])  error = y_true  y_false print(y_true.shape, y_pred.shape) print(error.shape) print(""Error values: "", error)  mask = tf.where(y_true > 5) i = mask[0] j = mask[1] print(i[:5]) print(j[:5])  TypeError                                 Traceback (most recent call last) Input In [193], in () > 1 error[i, j] File ~\AppData\Local\Programs\Python\Python310\lib\sitepackages\tensorflow\python\util\traceback_utils.py:153, in filter_traceback..error_handler(*args, **kwargs)     151 except Exception as e:     152   filtered_tb = _process_traceback_frames(e.__traceback__) > 153   raise e.with_traceback(filtered_tb) from None     154 finally:     155   del filtered_tb File ~\AppData\Local\Programs\Python\Python310\lib\sitepackages\tensorflow\python\ops\array_ops.py:899, in _check_index(idx)     894 dtype = getattr(idx, ""dtype"", None)     895 if (dtype is None or dtypes.as_dtype(dtype) not in _SUPPORTED_SLICE_DTYPES or     896     idx.shape and len(idx.shape) == 1):     897    TODO(slebedev): IndexError seems more appropriate here, but it     898    will break `_slice_helper` contract. > 899   raise TypeError(_SLICE_TYPE_ERROR + "", got {!r}"".format(idx)) TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got  ```","There are multiple reasons for Gradients to become None, in your case it might be due to the operation you are doing could be outside Tensorflow operation or `tf.Variable` gets replaced with `tf.tensor` which is used for gradient calculation. More details on this can be found here. Regarding updating specific index values, you can take a look into tf.Variable operation scatter_update which assigns tf.IndexedSlices values.  Let me know if this helps. Thanks!","I used run_eagerly in model compile and then call the custom loss function. Then, I just used the above calculation for Tensors. And I got stuck there. Regarding your suggestion to use `scatter_update`, it did not work here.",I think you want a combination of `tf.gather_nd` and `tf.tensor_scatter_nd_update`: ,"KaleemUllah , Could you please refer the above comment and let us know if that helps. For more details refer, tf.gather_nd, tf.tensor_scatter_nd_update","Or, you could use the multiplexing mask: ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,"An RFC to address this has been up for a bit.  Adding a link here in case anyone wants to provide input. Comments are invited to the pull request. You can view the design doc here, and also leave comments inline on the document source.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1691,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to resize (downsample) 5D samples in tensorflow? )ï¼Œ å†…å®¹æ˜¯ (Issue Type: Feature Request Source: binary Tensorflow Version: tf 2.8 Custom Code: Yes OS Platform and Distribution: Windows 10 Python version: 3.9   Note. Reposting from here. It's closed in keras and recommended to post it on the tensorflow side because this functionality is not precisely available in tensorflow. It may need to write lowlevel ops in tf for best performance.  Current Behaviour? Currently, for **5D** data, `(batch_size, h, w, depth, channel)`, the `tf.keras.backend.resize_volumes` or `UpSampling3D` can be used to **upsampling purpose**.  For example, I can do   These `*_factor` values (above), should be an **integer**, and are coded here: https://github.com/kerasteam/keras/blob/master/keras/backend.pyL3441L3444.  In that case, how can we **downsample** the input sample? For example:  [ HERE https://stackoverflow.com/q/57341504/9215780, another scenario where the factor needed to be fractional. ]  Candidate Solutions  scipy.ndimage.zoom  in pytorch (didn't test)  Others  Such downsampling feature needs to be implemented in lowlevel.   In the `depth` part of volumetric data, it might be hard to decide the appropriate strategy to drop the slices depending on the domain. For example, in medical data, if we drop the slice blindly, we might lose information. FYI, in CT/MRI images, most of the information appears mainly in the **middle range**.   Currently a workaround for **medical data (CT/MRI)**, we're following:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,innat,How to resize (downsample) 5D samples in tensorflow? ,"Issue Type: Feature Request Source: binary Tensorflow Version: tf 2.8 Custom Code: Yes OS Platform and Distribution: Windows 10 Python version: 3.9   Note. Reposting from here. It's closed in keras and recommended to post it on the tensorflow side because this functionality is not precisely available in tensorflow. It may need to write lowlevel ops in tf for best performance.  Current Behaviour? Currently, for **5D** data, `(batch_size, h, w, depth, channel)`, the `tf.keras.backend.resize_volumes` or `UpSampling3D` can be used to **upsampling purpose**.  For example, I can do   These `*_factor` values (above), should be an **integer**, and are coded here: https://github.com/kerasteam/keras/blob/master/keras/backend.pyL3441L3444.  In that case, how can we **downsample** the input sample? For example:  [ HERE https://stackoverflow.com/q/57341504/9215780, another scenario where the factor needed to be fractional. ]  Candidate Solutions  scipy.ndimage.zoom  in pytorch (didn't test)  Others  Such downsampling feature needs to be implemented in lowlevel.   In the `depth` part of volumetric data, it might be hard to decide the appropriate strategy to drop the slices depending on the domain. For example, in medical data, if we drop the slice blindly, we might lose information. FYI, in CT/MRI images, most of the information appears mainly in the **middle range**.   Currently a workaround for **medical data (CT/MRI)**, we're following:  ",2022-04-30T09:27:56Z,stat:awaiting tensorflower type:feature comp:ops,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55809,/ , adding `keras` tag may not be proper. Please read this https://github.com/kerasteam/keras/issues/16260issuecomment1070950476 and https://github.com/kerasteam/keras/issues/16260issuecomment1071146844.," medical data often needs to be treated quite specially.  Our `tf.image` toolbox is designed for 2D images, and does an interpolation or convolution (depending on the supplied method).  A generalized 3D image resize primitive would mirror this, and apply the interpolation/convolution in all 3 dimensions.  Is this what you're looking for? In MR images, the zdirection usually has a much lower resolution that the others, and correlation between slices is much lower (since slices have further separation and have larger ""thickness"").   You often don't want to interpolate between these.  More commonly, you resize the 2D image slices and then stack them together, as you have done (i.e. only resize in xy).  To reduce the number of slices, you can either pick a region of interest (e.g. choose the middle 10 slices as in your workaround), or use `tf.strided_slice` to skip between them (e.g. `a5 = a4[...,  ::2, :]`). If you're content with your workaround, I don't think we could squeeze much more efficiency out of it by introducing a new op.  In graph mode, the shaperesizeshape will be just as efficient as if we had done the same thing internally (the reshapes won't actually move any data around). If you *do* want a 3d interpolation, there are tools like tfg.math.interpolation.trilinear.  I don't think it's common enough that we would put it into core tensorflow."," Thanks for your details answer. I did a pause of a project that raised such above issue. I will revisit the problem soon. But below are some quick queries.  1. > A generalized 3D image resize primitive would mirror this, and apply the interpolation/convolution in all 3 dimensions.  Is this https://www.tensorflow.org/graphics/api_docs/python/tfg/math/interpolation/trilinear/interpolate function doing a similar operation? Is it possible to show an example with code? 2. > I don't think it's common enough that we would put it into core tensorflow. Agree. But IMHO, general interpolation methods should be available from core tensorflow.  3. > In MR images, the zdirection usually has a much lower resolution than the others, and the correlation between slices is much lower (since slices have further separation and have larger ""thickness""). You often don't want to interpolate between these. Could you please elaborate on this? 4. > Our `tf.image` toolbox is designed for 2D images and does an interpolation or convolution (depending on the supplied method).  My main approach was not with `tf.image` though, I wanted to use `tf.keras.backend.resize_volumes`, which meant for the volumetric data. Now, unable to downsampling with it was a bit disappointing. To address this is the actual feature request.  In `scipy` and `pytorch`, I can do   Now, if I reform my query, like, how can I achieve this with core `tensorflow`. The `layers.UpSampling3D` which uses `tf.keras.backend.resize_volumes` in backed lacks interpolation methods for 3D volumetric data.",I want to add 3 extra points:   I suppose that `tf.keras.backend` namespace has not a so great planning future right?   Is TF graphics actively maintained/staffed? As we had a quite cyclic activity flow on that repository https://github.com/tensorflow/graphics/issues/340issuecomment708611214      oss  More in general I think it would be more great if we could tell to the TF ecosystem community if we want to separate CV 2d stuffs in Kerascv and CV 3d stuff in TF graphics (if it is actively maintained).,"  1. I have no direct experience with tfg, but you can check out the source for `trilinear` here.  It looks like you give it a set of sampling points (e.g. your desired locations within the new interpolated grid), and it will use trilinear interpolation to extract values at those points. 2. Interpolation is usually viewed as an input preprocessing stage.  It's not *really* part of the ML model.  For 2D images, it is very common to have training images of different sizes (e.g. a collection of downloaded pictures of horses).  And in model serving, if you want a generic inference model that can work for multiple camera resolutions, it makes sense to have a builtin tool to allow these to be resampled to a fixed input size.  3D images, however, are a lot more rare  the most common being medical images.  It is more likely that these will already be of identical size, and if not, you can use whatever tool you need to reinterpolate them ahead of time; it's unlikely you will have a steady stream of 3D images coming directly from a camera, for example.  Similarly with general ND inputs  it's also not as common to need to interpolate onthefly. 3. Interpolation works well when nearby pixels/voxels are highly correlated, so that the new interpolated pixel/voxel is highly representative of those around it.  In MR slices, the slice direction is often separated by larger gaps, the signal is an estimated average over the slice thickness, and it is often captured at a slightly different time than other slices.  For CT it depends on how it's collected.  Many applications also use slices of 2D CT, so you will have similar issues.  Hence there is often a much lower image resolution in the slice direction, and more variation between slices.  In medical applications we often want to keep all slices (at least those in the region of interest). 4. It looks like `resize_volumes` just repeats values along each spatial axis.  The inverse of this is `tf.strided_slice`.  If you have an input of fixed known size, and you just want to decrease the resolution to make the model run faster, it's much more common to add convolution+pooling layers (e.g. `Conv3d` + `MaxPool3d`).  The convolution will try to pick out ""features"", and the pooling will keep only the ""important"" ones while cutting down the number of samples.  That way you're not throwing away any potentially important information. If you just want to cut down the resolution of your fixedsize input images, you could try `tf.nn.avg_pool3d`, which will take the average of a collection of voxels.  This will be similar in nature to trilinear interpolation on a regular grid (and equivalent if you're cutting each dimension down by half), but is already builtin and will be much faster."," not by Google, it's communitydriven.  The interpolate function is pythononly though, and they just use exising `tf` functions underthehood.","> The interpolate function is pythononly though, and they just use exising tf functions underthehood. Yes I saw. It was more related to clearup the contribution route in the ecosystem and to know if there were full time resources allocated to sustain the core repo activities."
902,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error building custom tensorflow op on Ubuntu and Windows)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04, Windows 10  Mobile device _No response_  Python version 3.8, 3.9  Bazel version _No response_  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2  GPU model and memory RTX 3080  Current Behaviour? I am trying to build a custom op following this https://www.tensorflow.org/guide/create_op. First, I compiled .cu file using nvcc into .cu.o successfully. Than I was trying to build a resulting .so file using g++, but I am constantly getting errors from compiler screaming on tensorflow internal src files. For example:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dimentary,Error building custom tensorflow op on Ubuntu and Windows,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04, Windows 10  Mobile device _No response_  Python version 3.8, 3.9  Bazel version _No response_  GCC/Compiler version 9.4.0  CUDA/cuDNN version 11.2  GPU model and memory RTX 3080  Current Behaviour? I am trying to build a custom op following this https://www.tensorflow.org/guide/create_op. First, I compiled .cu file using nvcc into .cu.o successfully. Than I was trying to build a resulting .so file using g++, but I am constantly getting errors from compiler screaming on tensorflow internal src files. For example:   ",2022-04-29T19:11:24Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/55803,"**Update** Despite throwing errors my command generates the required .so file. But, when I try to load it with `tf.load_op_library` I get `tensorflow invalid ELF header` error."," , `tf.sysconfig.get_link_flags()` returns `['L/usr/local/lib/python3.7/distpackages/tensorflow', 'l:libtensorflow_framework.so.2']` Here,` library directory` is `L` and library is `l`. Now replace the `L` and `l` with `library_dirs` and `libraries` arguments to your Extension object in `setup.py`. Can you please set your `LD_LIBRARY_PATH` environment variable to point to the directory with the `libtensorflow_framework.so` library is located.Thanks!"," thank you for the answer, Do you mean this `setup.py`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py Do I have to make those changes followed by `pip install .`? Please describe it in more detail.  If you mean my own `setup.py`  I do not have a local project with it  just .h .cpp and .cu files for the build. I am attaching them here upfirdn2d.zip Regarding second point, I found `libtensorflow_framework.so.2` under `/home/dmitry/.local/lib/python3.8/sitepackages/tensorflow` and added this folder to the `LD_LIBRARY_PATH`, but the errors are still present.","To sum up, I have 3 main errors on Windows (for Ubuntu only the paths will differ):   ",here is my Makefile to reproduce everything (based on this): ,Can you check if your op file is following this folder structure under respective kernels. Did you try building the sample custom op `zero_out_ops` mentioned in the above link.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
705,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFlite model with dilated convolution yields different results than TF model )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.049g85c8b2a817f 2.4.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.0, cuDNN 6.5.0  GPU model and memory NVIDIA GeForce RTX 3090  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hartmannma,TFlite model with dilated convolution yields different results than TF model ,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version v2.4.049g85c8b2a817f 2.4.1  Custom Code No  OS Platform and Distribution Windows 10  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version CUDA 11.0, cuDNN 6.5.0  GPU model and memory NVIDIA GeForce RTX 3090  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-04-29T15:04:22Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.4,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55799,"Today, I tested on the latest released TF version: I could not reproduce the issue in 2.8.0. The issue seems to be resolved here, or at least it does not occur with the configuration and the parameters of my test code. Also my original, much larger model, from which the test code above was extrated, works in TF 2.8.0. Nevertheless, I would highly appreciate a fix in 2.4."," ! Thanks for confirmation . But I am getting same relative error in 2.8 , 2.9 and nightly .  ! Could you please look into this issue ? Thanks!",", Usually the fix in the older versions will be done if that is a security bug or if the bug is critical and impacting larger community. In this case since you have mentioned that it is working fine with Tensorflow 2.8. You can continue using this version. Also, Tensorflow 2.9 will be released anytime soon. Thanks!",",  thanks for your answer. I agree, I can continue with Tensorflow 2.8, which seems to resolve the problem.  Neverthless, it might be good to identify (not fix) the issue in the converter of 2.4, just to know that it is no more relevant in the newer releases. My concern is also triggered by the fact that  reported to get the error also in 2.8, 2.9 and nightly!? Thanks! ","I can see the output in Tensorflow is very close to TFLite output. The small difference in both could be due to the floating point precision difference or when they are processed by the TensorFlow Lite Optimizing Converter, those operations may be elided or fused, before the supported operations are mapped to their TensorFlow Lite counterparts.","Yes, I agree with you. When the error does not occur, differences are in the order of 10e2. But in the error case differences were above 90%, which is basically a completley different output:  `relative error:  tf.Tensor(0.9494969, shape=(), dtype=float32) %` I assumed,  did also get an error in this order of magnitude in TF 2.8. Maybe I misinterpreted his comment?"," , Could you please check the Gists provided in the below links for different versions and let us know if you think that as an error. Thanks!  >  ! Thanks for confirmation . But I am getting same relative error in 2.8 , 2.9 and nightly .  ! Could you please look into this issue ? Thanks!","I had not seen, that there are linked Gists in that post, sorry.  The results in these Gists are okay, which also was expected in TF 2.4, since the code was run with `dilation=1` (1st line in main() ). The error can be reproduced with `dilation = 2` (uncommenting 2nd line in in main() ). Thus I assume that  obtained only the results with (acceptable) numerical deviation, but not the erroneous result I obtained in 2.4 => **This is no indication for the bug still beeing relevant in 2.8!** Below you can see the plots comparing the results for dilation = 2 (completely different) and dilation = 1 (numerical deviation): **dilation=(2,1)**: !image **dilation=(1,1)**: !image"," I did check with `dilation=2` and `filters=32` on TF 2.11 and TF nightly 2.12.0dev20230109. I have observed that `relative error:  tf.Tensor(0.009144009, shape=(), dtype=float32) %`  which I think is acceptable. Please check the gist here and let us know if it helps. Thank you.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
625,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Change to rolling bazel release is broken on AARCH64)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version last_downstream_green  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,elfringham,Change to rolling bazel release is broken on AARCH64,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version git HEAD  Custom Code No  OS Platform and Distribution CentOS 7  Mobile device n/a  Python version 3.8.13  Bazel version last_downstream_green  GCC/Compiler version 10.2.1  CUDA/cuDNN version n/a  GPU model and memory n/a  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-29T09:51:48Z,type:build/install,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55797,   ,"`file /root/.cache/bazelisk/downloads/bazelbuild/bazeldf96163124d220a04abde24f8302e77349ae5194linuxarm64/bin/bazel /root/.cache/bazelisk/downloads/bazelbuild/bazeldf96163124d220a04abde24f8302e77349ae5194linuxarm64/bin/bazel: ELF 64bit LSB executable, x8664, version 1 (GNU/Linux), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=1af60e5fe9fa0c2ec7780f7093d998be9a9c584f, not stripped`",This should have been resolved by https://github.com/tensorflow/tensorflow/commit/a969f2bf9196b534f2acc47ca11e963c729bb80d,Are you satisfied with the resolution of your issue? Yes No
1852,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Autograph crashes when using pythonw.exe on Windows)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When using pythonw.exe, if local sources are not available, (e.g. only .pyc files are available), autograph causes a crash by trying to flush sys.stdout. https://github.com/tensorflow/tensorflow/blob/f39905619ddf75b31e209264673f76ac2c72030e/tensorflow/python/autograph/core/ag_ctx.pyL100L105 If the source code to Autograph itself isn't available, it emits this warning using ag_logging.warning: https://github.com/tensorflow/tensorflow/blob/f39905619ddf75b31e209264673f76ac2c72030e/tensorflow/python/autograph/utils/ag_logging.pyL141L145 For some reason, `warning()` is different than any of the other emitters and flushes the stream.  But when running in pythonw.exe, `sys.stdout` is `None`.  It checks for this using `echo_log_to_stdout` to ensure that it doesn't write to stdout, but then immediately tries to flush it. This can be remediated to indenting the call to `sys.stdout.flush()` to inside the `if echo_log_to_stdout:` check.  Standalone code to reproduce the issue 1. Create a virtual environment and install TensorFlow 2. Compile all of your py files in sitepackages using the `compileall` module 3. Delete py files from your sitepackages 4. Create an example tkbased script or something else that shows a GUI 5. Import tensorflow in your script and execute it usin)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jgentil,Autograph crashes when using pythonw.exe on Windows,"Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Windows  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? When using pythonw.exe, if local sources are not available, (e.g. only .pyc files are available), autograph causes a crash by trying to flush sys.stdout. https://github.com/tensorflow/tensorflow/blob/f39905619ddf75b31e209264673f76ac2c72030e/tensorflow/python/autograph/core/ag_ctx.pyL100L105 If the source code to Autograph itself isn't available, it emits this warning using ag_logging.warning: https://github.com/tensorflow/tensorflow/blob/f39905619ddf75b31e209264673f76ac2c72030e/tensorflow/python/autograph/utils/ag_logging.pyL141L145 For some reason, `warning()` is different than any of the other emitters and flushes the stream.  But when running in pythonw.exe, `sys.stdout` is `None`.  It checks for this using `echo_log_to_stdout` to ensure that it doesn't write to stdout, but then immediately tries to flush it. This can be remediated to indenting the call to `sys.stdout.flush()` to inside the `if echo_log_to_stdout:` check.  Standalone code to reproduce the issue 1. Create a virtual environment and install TensorFlow 2. Compile all of your py files in sitepackages using the `compileall` module 3. Delete py files from your sitepackages 4. Create an example tkbased script or something else that shows a GUI 5. Import tensorflow in your script and execute it usin",2022-04-28T23:51:54Z,type:bug comp:autograph TF 2.8 awaiting PR merge,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55795,"Thanks for the bug, I have created a PR for the same.",Are you satisfied with the resolution of your issue? Yes No
687,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build C++ interface with CUDA on CentOS 8)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.3.4, 2.4.4, 2.5.3, 2.6.3, 2.7.1, 2.8.0  Custom Code No  OS Platform and Distribution CentOS 8.5  Mobile device _No response_  Python version 3.7.9  Bazel version 3.1.0, 3.7.2, 4.2.1 depending on tf version  GCC/Compiler version gcc 8.5.0  CUDA/cuDNN version 11.4 / 8  GPU model and memory A10  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hoba87,Build C++ interface with CUDA on CentOS 8,"Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.3.4, 2.4.4, 2.5.3, 2.6.3, 2.7.1, 2.8.0  Custom Code No  OS Platform and Distribution CentOS 8.5  Mobile device _No response_  Python version 3.7.9  Bazel version 3.1.0, 3.7.2, 4.2.1 depending on tf version  GCC/Compiler version gcc 8.5.0  CUDA/cuDNN version 11.4 / 8  GPU model and memory A10  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ",2022-04-28T09:05:04Z,stat:awaiting response type:build/install stale subtype:centos TF 2.8,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55786,", Could you try `bazel clean expunge` followed by `bazel sync`. Thanks!",for `bazel sync` I get a bunch of errors like e.g.: ,", > ERROR: An error occurred during the fetch of repository 'ubuntu18.04gcc8_manylinux2014cuda11.2cudnn8.1tensorrt7.2_config_python3.7': Tensorflow master branch has the configuration   Could you try to build Tensorflow master branch and let us know. Thanks!",master branch: ," , Could you share the complete Log. Thanks!",bazel.log,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi , Have you tried `config=nccl ` command during Tensorflow build run. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> Hi , Have you tried `config=nccl ` command during Tensorflow build run. Thank you! That gives only: ERROR: Config value 'nccl' is not defined in any .rc file At the end my workaround was to use a ubuntu docker image with fitting libc version and using that then."
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to load TF1 `.ckpt` weights into keras model?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Others  Source binary  Tensorflow Version 1.14  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.3.1  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ghost,How to load TF1 `.ckpt` weights into keras model?,Click to expand!    Issue Type Others  Source binary  Tensorflow Version 1.14  Custom Code Yes  OS Platform and Distribution macOS Monterey 12.3.1  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-27T06:24:43Z,stat:awaiting response type:support comp:keras TF 1.14,closed,0,11,https://github.com/tensorflow/tensorflow/issues/55763,"Hello  , We see that you are using tf version 1.14, 1.x is not actively supported, please update to latest stable v2.8 and let us know if you are facing same issue.Thanks!","I'm converting 1.14 code to 2.8 and to do so, I need to run it in 1.14 and it doesn't really matter, the same issue will happen at any given version since it seems there is no predefined way of converting from/to tf1.x models/tf2.x, which is what the issue is about."," , The provided code is related to tf1 version.Can you please provide the latest version code to reproduce the issue.It helps to debug the issue.Thanks!","I provided both codes for creating 1.x model, 2.x keras model and a demonstration of what I'm trying to achieve which again is loading weights produced by tf1 code into keras model. If you think something is preventing you from reproducing the issue, please be specific about what it is."," , I was facing different issue while reproducing the mentioned code.Please find the gist of it here and provide the reproducible tf v2.x code.It helps to analyse the issue.Thanks!","I don't know how many times I'll have to repeat the same information over and over. The colab notebook you sent runs tensorflow 2.8, and for the 623754th time: **TO RUN THE CODE YOU NEED TF 1.X**. Once you obtain the weights produced by **TF 1.X**, you can then load the weights in **TF2.x**."," , Thanks for opening this issue. Development of keras moved to another repository.  Can you please post this issue on kerasteam/keras repo. To know more refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,here's an activity," , As suggested, Thanks for opening this issue. Development of keras moved to another repository. Can you please post this issue on kerasteam/keras repo. To know more refer: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",Are you satisfied with the resolution of your issue? Yes No
662,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Undefined reference to function FreezeSavedModel with C++ API)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.6  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 3.7.2  GCC/Compiler version 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,GosuPaper,Undefined reference to function FreezeSavedModel with C++ API,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf 2.6  Custom Code Yes  OS Platform and Distribution Ubuntu 20.04  Mobile device _No response_  Python version 3.8.10  Bazel version 3.7.2  GCC/Compiler version 9.4.0  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output _No response_,2022-04-26T09:37:07Z,stat:awaiting response type:bug stale comp:runtime 2.6.0,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55755,"Hello  , Can you please provide the complete error log which helps to debug/analyse the issue.Thanks!","Yes, but there is nothing else I think. ====================[ Build  Debug ]================================== /snap/clion/189/bin/cmake/linux/bin/cmake build /home/gosu/CLionProjects/TestIA/cmakebuilddebug target TestIA [1/1] Linking CXX executable TestIA FAILED: TestIA  : && /usr/bin/c++ g  CMakeFiles/TestIA.dir/main.cpp.o CMakeFiles/TestIA.dir/CatDogCNN.cpp.o o TestIA  Wl,rpath,/usr/local/lib:/home/gosu/Bureau/IA/tensorflow/bazelbin/tensorflow  /usr/local/lib/libprotobuf.so  /usr/local/lib/libprotoc.so  /home/gosu/Bureau/IA/tensorflow/bazelbin/tensorflow/libtensorflow.so  /home/gosu/Bureau/IA/tensorflow/bazelbin/tensorflow/libtensorflow_cc.so  /home/gosu/Bureau/IA/tensorflow/bazelbin/tensorflow/libtensorflow_framework.so && : /usr/bin/ldÂ : CMakeFiles/TestIA.dir/CatDogCNN.cpp.oÂ : dans la fonction Â«Â CatDogCNN::FreezeSave(std::__cxx11::basic_string, std::allocator >&)Â Â»Â : /home/gosu/CLionProjects/TestIA/cmakebuilddebug/CatDogCNN.cpp:367Â : rÃ©fÃ©rence indÃ©finie vers Â«Â tensorflow::FreezeSavedModel(tensorflow::SavedModelBundle const&, tensorflow::GraphDef*, std::unordered_set, std::allocator >, std::hash, std::allocator > >, std::equal_to, std::allocator > >, std::allocator, std::allocator > > >*, std::unordered_set, std::allocator >, std::hash, std::allocator > >, std::equal_to, std::allocator > >, std::allocator, std::allocator > > >*)Â Â» collect2: error: ld returned 1 exit status ninja: build stopped: subcommand failed. ""rÃ©fÃ©rence indÃ©finie vers"" translate as ""undefined reference to"". + As I mentionned in my first post, I saw that freezing model might not be the way in TF2.x but I can't find how to do it otherwise. And I find it odd that method that are in source files can't be used anymore.",https://github.com/tensorflow/tensorflow/labels/type%3Abuild%2Finstall," , We see you are using gcc compiler v9.4.0 which is not compatible for tensorflow v2.6. Please try to install with the compatible gcc compiler 7.3.1.Thanks!",  I did try to use this version of gcc... Still exactly the same log error ,"Since it appears that freeze_save_model.h and ., I have managed a simple workaround for those two : Include them directly in the project next to my main and obviously in the makefile. This way I am able to use the function FreezeSavedModel without any trouble and it is working great. Still, I would really like to know how to save and load a model using the C++ API in TF2.x. There has to be a way included in the library !",",  You can load the saved model in C++ using LoadSavedModel. Thanks!"," Indeed, but how do I save it ?","Hi, could you please refer this answer which is similar to yours. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1882,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Huge Accuracy drop after re-training my model on device using tensorflow-lite 2.8)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device Custom Linux Distribution on Raspberry Pi  Python version _No response_  Bazel version 3.7.2  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? * After converting my model to TensorFlow Lite and deploying it with your app, I am trying to retrain the model on a device using the fashion MNIST Testset and the train signature method of my model. During the training on the device for 5 epochs, I can print the current Loss of each epoch. The https://www.tensorflow.org/lite/examples/on_device_training/overviewretrain_the_model_on_a_device article states that the training on target should start from the last loss value and the loss curve should be continuous as shown in the following figure: !figure. The weird behaviour I am noticing is mainly related to this, these are the loss values I get on my target after reproducing the same steps as the article.  The Loss value from the initial training on Google Colab seems to end at 0.241, but it starts at 0.0003855.  * The second behavior is the accuracy drop in my model after retraining it on my device. The initial accuracy after training the model on Google Colab is around 92% after 100 epochs of training. But after retraining the model, saving the new weights and testing it, I notice a huge accuracy drop down to 83% after 5 epochs of training on device. That's a bit odd in my opinion. Any help to understand this behav)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OAHLSTM,Huge Accuracy drop after re-training my model on device using tensorflow-lite 2.8,"Click to expand!    Issue Type Performance  Source source  Tensorflow Version tf 2.8  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device Custom Linux Distribution on Raspberry Pi  Python version _No response_  Bazel version 3.7.2  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? * After converting my model to TensorFlow Lite and deploying it with your app, I am trying to retrain the model on a device using the fashion MNIST Testset and the train signature method of my model. During the training on the device for 5 epochs, I can print the current Loss of each epoch. The https://www.tensorflow.org/lite/examples/on_device_training/overviewretrain_the_model_on_a_device article states that the training on target should start from the last loss value and the loss curve should be continuous as shown in the following figure: !figure. The weird behaviour I am noticing is mainly related to this, these are the loss values I get on my target after reproducing the same steps as the article.  The Loss value from the initial training on Google Colab seems to end at 0.241, but it starts at 0.0003855.  * The second behavior is the accuracy drop in my model after retraining it on my device. The initial accuracy after training the model on Google Colab is around 92% after 100 epochs of training. But after retraining the model, saving the new weights and testing it, I notice a huge accuracy drop down to 83% after 5 epochs of training on device. That's a bit odd in my opinion. Any help to understand this behav",2022-04-26T09:30:15Z,stat:awaiting response comp:lite type:performance TF 2.8,closed,0,34,https://github.com/tensorflow/tensorflow/issues/55754,Haoliang can you please have a look Thanks,"The Loss value from the initial training on Google Colab seems to end at 0.241, but it starts at 0.0003855. Hi, From your C++ code, I didn't see you load the initial checkpoint from the previous training in colab. This will cause the model to use random weights. Could you try calling the restore signature as described in this article? https://www.tensorflow.org/lite/examples/on_device_training/overviewrestore_the_trained_weights","Hello   Thank you for your helpful feedback. As you suggested, I tried to invoke the restore runner by my interpreter in order to load the checkpoint from the initial training in colab. This is what I added to my code before invoking the train runner.  INFO: Created TensorFlow Lite delegate for select TF ops. INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 17 nodes with 0 partitions. [DEBUG][METADATA] Signatures available in this Model: infer   restore   save   train   20220425 17:01:42.591196: W /local/home/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 44866235472 exceeds 10% of free system memory. Segmentation fault ``` What does it need to allocate 44866235472 bytes knowing that my checkpoint file is only 407.6kB ? Any help would be appreciated. Thank you again.","Hi, Did you call `restore_runner>AllocateTensors()` before calling invoke?","Hi , Yes, I did, just like this:  Also, I'm facing the same issue when I try saving the weights after a training on target with save runner from the signature save. ","Hello ,  Have you been able to reproduce this issue on your side ? For your information, I tried setting **input_tensor>data.data** which is a void* and it doesn't seem to work either.  This is what I tried to do:  Although, I am pretty convinced that the issue is related to this variable assignement. ","Hi, could you share your tflite model file? ","Hi , Yes sure, here it is: https://drive.google.com/file/d/1i_emRlbfM_cSVltgkP81JqiN1rltM5Et/view?usp=sharing",is there a checkpoint file associated with this tflite model? could you also upload it? thanks.,"Hello , This link points to a zip containing the checkpoint of the weights and the dataset used for ondevicetraining: https://drive.google.com/file/d/1t5iahmzMIsMyn1v46dDvXSQqZUc6vZj/view?usp=sharing Let me know if anything else is missing. Thank you,","Hi, The reason for the crash when you run the `restore` signature is that, you didn't properly initialize the string input tensor. In TF Lite, you need to use `DynamicBuffer` to initialize string tensor. Replace `input_tensor>data.raw = restore_path;` with: `  tflite::DynamicBuffer buf;   buf.AddString(restore_path, strlen(restore_path));   buf.WriteToTensor(input_tensor, nullptr);` The `DynamicBuffer` is defined in this file.","Hi , Thank you for your feedback, this definitely helped me solve my save and restore signatures issues. With DynamicBuffer I am now able to load and save my checkpoints before and after the training on device. Though, I am still noticing some huge loss dropping even when I load the weights from Google Colab training. I am not able to notice a continuous decreasing curve for loss function. There are the new loss I get after a 5epoch training:  Do you have any idea what would cause this weird behavior ? let me know if you face any trouble while trying to reproduce the issue. The same tflite and ckpt files has been used. Thank you,","Hi, A few questions: 1) What's the difference between the training data on the server v.s. mobile? 2) Can you calculate the prediction accuracy  with the last training checkpoint on the server, loads the checkpoint in TF Lite and then predict with same input image? Those two accuracy should match.","Hello ,  To answer your questions: 1. On the server, I'm using the Training Set of the fashion MNIST to train the model. Here is a link to the dataset used: https://storage.googleapis.com/tensorflow/tfkerasdatasets/trainlabelsidx1ubyte.gz https://storage.googleapis.com/tensorflow/tfkerasdatasets/trainimagesidx3ubyte.gz On the mobile, I'm using the Test Set of the Fashion MNIST dataset, Here is a link to the dataset used: https://storage.googleapis.com/tensorflow/tfkerasdatasets/t10klabelsidx1ubyte.gz https://storage.googleapis.com/tensorflow/tfkerasdatasets/t10kimagesidx3ubyte.gz Do you think that the difference of the dataset between the server and the mobile might have an effect of the loss gap ? 2. I have done some tests with several input images and apparently those accuracy values do not match. I made sure to load the checkpoint file with restore signature before running the inference. Here is a log of the results:  **On the server:**  **On the mobile :** ","Hi, Could you update your colab notebook to better reproduce this issue? For the current issue, I think there might be some issues when the interpreter reloads the checkpoint so you are not seeing consistent predictions. For the ondevice part, you can still run it in colab. Instead of using C++ interpreter APIs, you can use the python API. So I guess the next steps will be: 1) Train the model for a few epochs 2) Write to a checkpoint 3) Convert to tflite model 4) Make a prediction using the TF model. Compare that with the tflite model (After running restore signature) If the two results don't agree, I think we need to debug what happens during checkpoint reloading.","Hi , I tried to update the Colab Notebook as you suggested. Here is a Gist. I haven't been able to run the restore signature on google Colab, It keeps crashing with an error **AttributeError: 'str' object has no attribute 'shape'**. I guess I'm not giving the correct input to the interpreter.  Anyway, I commented those 2 lines and ran my inference with **NO CHECKPOINT FILE LOADED** and the surprise was that the predictions were exactly the same for the 4 input images as in the inferences using the TF Model. The test procedure is well explained in the Google Colab Gist I shared. Another point, I performed another test ondevice by computing prediction of inference by: 1. Running inference ondevice by calling restore signature to load checkpoint file. 2. Then running inference ondevice without calling restore signature, (I generated a new bin of course) => In both cases, I get the exact same predictions probabilities ... Weird ... Which makes me join you in the idea that something might be wrong with the checkpoint weights loading in the interpreter. ",> It keeps crashing with an error AttributeError: 'str' object has no attribute 'shape'. I guess I'm not giving the correct input to the interpreter. I think you need to construct a string using numpy. Something like (an example) ,"Thank you for your answe , This seems to be doing the trick.    Here is an updated Google Colab Gist. This gets more interesting, it seems that the predictions are still the same between the 3 benchmarking subtests: 1. Running inference using the TF Model 2. Running inference using the TFLite Model without loading checkpoints, (I ran this one before the next one) 3. Running inference using the TFLite Model while loading checkpoints.",",  Any updates on the subject please ? From my side, I wrote a simple application with the same model that loads the checkpoint file with the restore signature, run inference on the Test Set of the Fashion MNIST Dataset (10k Images) and compute the average accuracy based on the predictions, at this stage I get 87% of images predicted correctly. I trained then the same model with the same Dataset for just 5 epochs and ran another batchinferencing on the same Dataset, I got 85% of images predicted well. I didn't call the save and restore signature again since I'm using the same interpreter from the starting of the program. This might be another issue, but I cannot see to what this global accuracy dropping is due. Thank you, ","Hi  Sorry for the delay! I was quite busy with other stuff last week. I will take a look on this issue today and update, thanks!","Hi  I could refer to a lot of things through this issue.  I'm conducting a similar test. In the training process, output continues to appear as ""nan"", so could you share the C++ code if you don't mind?"," Okay, looking forward to it, thank you !  if you're output continues to appear as ""nan"", it simply means that you're not feeding your training signature runner with correct data. My source code has been shared at the first comment of this issue. Maybe if you share yours, I could help you identify the root cause of your problem. "," Thank you for your Response. i'm conducting Test Based on the code you shared, and the output still ""Nan"" my Testing Environment  OS : Ubuntu 20.04  Tensorflow : v2.8  Bazel : 4.2.1  build command  : bazel build config=monolithic config=noaws config=nogcp config=nohdfs config=nonccl fat_apk_cpu=x86_64 experimental_ui_max_stdouterr=1073741819 c opt cxxopt=std=c++14 //tensorflow/lite:libtensorflowlite.so with same DataSet and tensorflow exmaple model  do you think OS Difference can cause ""nan"" output  as i know ubuntu and raspberry pi both little Endian","Hello , I have just rerun my code on a Ubuntu 20.04 based system and I'm not getting these ""nan"" output, I get actual loss values, I don't think it's a little Endian issue because in my code I test whether the system is Little Endian before applying the reverse function. I suggest that you add some debugging traces on **fpixels** and **labels** variables to check if your input is correctly fed to the signature function.   Any updates on the restore function eventual bug ?","Hi   Yes. Let me reply separately: >I trained then the same model with the same Dataset for just 5 epochs and ran another batchinferencing on the same Dataset, I got 85% of images predicted well It seems that the model is overfitting. Did you witness loss is continuing dropping in the 5 epochs? > Here is an updated Google Colab Gist. This gets more interesting, it seems that the predictions are still the same between the 3 benchmarking subtests: 1) Running inference using the TF Model 2)Running inference using the TFLite Model without loading checkpoints, (I ran this one before the next one) 3)Running inference using the TFLite Model while loading checkpoints. The reason that 2) gives you the same prediction result is that, the converted TF Lite model has a special op called `CallOnce`, and this op will automatically run a subgraph which initializes the variables with the value at model export time. This is why you get same prediction without loading checkpoint at front. !image I updated your colab, adding a section which continues TF Lite training for 10 more epochs. And I do see the loss dropping from the previous servertrained baseline. I used TF to train 10 epochs initially, with losses: Finished 2 epochs   loss: 0.230 Finished 4 epochs   loss: 0.168 Finished 6 epochs   loss: 0.131 Finished 8 epochs   loss: 0.119 Finished 10 epochs   loss: 0.117 And with TF. Lite, training another 10 epochs, losses: Finished 2 epochs   loss: 0.114 Finished 4 epochs   loss: 0.114 Finished 6 epochs   loss: 0.114 Finished 8 epochs   loss: 0.106 Finished 10 epochs   loss: 0.106 As you see here, the TF Lite loss continue dropping. I think in your original colab, you are training TF for 100 epochs, this will probably lead to overfitting. The only change I made here is to reduce the TF training to 10 epochs, and batch_size = 10.  See the updated colab: https://colab.research.google.com/drive/1AabhKW_qCW_UER8NzR9ehoohErCZCcrc?usp=sharing Cheers.","Hello ,  Thank you for your feedback, it was really helpful and I managed to reproduce it on the updated colab you sent. I have been able to see the loss values decreasing continuously from the initial training to the ondevicetraining. Although, I haven't been able to reproduce this behavior ondevice using C++ code. I'm still noticing decreasing but too low values of the loss function, which seems illogical to me. The only code difference between the python script from google Colab and my C++ minimalist application is the dataloader. In Python, we can use the **tf.Dataset.from _tensor_slices()** function, whereas in the C++ application the data is fed manually to the train signature input tensor as follow:  This example has been written based on my analysis and understanding of the signature features from Tensorflow 2.9 source code, so I might be mistaken. Do you have any working C++ code example that will help me reproduce the same issue as in python script ? Thank you for your help and support,","For a given image/label input pair, have you checked whether the floating numbers are the same between the C++ code and the python colab? Let's ensure the input x/y in your C++ code is consistent with the colab.","Hello ,  The input pair was consistent with google's colab input but only for the first epoch. I just figured out what was the problem. In my code, when I read the bytes from the dataset file, I forget to reset the reading position in the second epoch so it keeps training the model with zero arrays for the second and third epochs and that what causes the accuracy dropping. My bad, it was an inattention mistake.  Thank you for this interesting exchange, It helped me understand a lot about this ondevicetraining feature, I will close the issue. ",Great that you solved the issue!,"I found this feature very interesting, too bad there isn't any detailed documentation on the topic. I'm ready to contribute on the topic if needed. Although, I still have some optional questions, I'm trying to leverage the use case to a more realistic one (Image classification using MobileNet Models). I have found this example using Tranfer Learning on android. What took my attention is that they are adding a new signature for loading the bottleneck features and a train signature that fine tunes only the classifier (the last FC layers). My questions are: Is it possible to retrain the convolutional layers ondevice ? Or the ondevicetraining supports only the FC layers for now ?  If it is possible can you provide any example on how to that ? Would **model.fit()** function work on device by calling the TensorFlow Eager executor ? Thank you again for your valuable support."
1612,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom MLIR Tensorflow Pass)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution Linux Centos 7  Mobile device _No response_  Python version 3.10.4  Bazel version 4.2.1  GCC/Compiler version 11.2.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Hi, I am currently working a compiler project that identifies when complex arithmetic is being cast as real arithmetic. For example, say we want to compute   where A, B, C are tf.complex64. But, due to whatever reason it is not allowed, or the person writing is a bad developer. Thus, instead we use real arithmetic to compute   where Ar, Ai, Br, Bi, Cr, Ci are tf.float32. I would like to catch this in a compiler pass and modify the program to use a high performance complex matmul kernel instead. As well as convert the data format into the one expected by the kernel. I was looking into the Tensorflow MLIR passes and was wondering how could I go about writing my own pass to implement this. So far I have been trying to convert `tf` MLIR to `linalg` MLIR and then writing an `OperationPass` for this. It would be a lot easier if I could just add a custom pass such that I can use it within `tfopt` TLDR: Are there any docs on writing custom tensorflow passes? If not, can these be added?  Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nicholaiTukanov,Custom MLIR Tensorflow Pass,"Click to expand!    Issue Type Feature Request  Source source  Tensorflow Version 2.8.0  Custom Code Yes  OS Platform and Distribution Linux Centos 7  Mobile device _No response_  Python version 3.10.4  Bazel version 4.2.1  GCC/Compiler version 11.2.1  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour? Hi, I am currently working a compiler project that identifies when complex arithmetic is being cast as real arithmetic. For example, say we want to compute   where A, B, C are tf.complex64. But, due to whatever reason it is not allowed, or the person writing is a bad developer. Thus, instead we use real arithmetic to compute   where Ar, Ai, Br, Bi, Cr, Ci are tf.float32. I would like to catch this in a compiler pass and modify the program to use a high performance complex matmul kernel instead. As well as convert the data format into the one expected by the kernel. I was looking into the Tensorflow MLIR passes and was wondering how could I go about writing my own pass to implement this. So far I have been trying to convert `tf` MLIR to `linalg` MLIR and then writing an `OperationPass` for this. It would be a lot easier if I could just add a custom pass such that I can use it within `tfopt` TLDR: Are there any docs on writing custom tensorflow passes? If not, can these be added?  Standalone code to reproduce the issue   Relevant log output  ",2022-04-26T05:22:35Z,stat:awaiting response type:feature stale comp:ops,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55750," , I was able to reproduce the issue in tf v2.7, v2.8 and nightly.Please find the gist here.",Good question and quite timely with the new function rewrite API being worked on. Could you clarify if you are interested in having this just in offline tool (a la tfopt or tfg_graph_transforms)?,"An offline tool would be great, since it makes it easier for me to test and play around with.  When thinking for my project, I thought it would be something like tfopt. But, I wasn't aware of tfg_graph_transforms. Are you referring to the Graph Transform Tool?","No, https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/tfg_graph_transforms (which uses TFG dialect which is more isomorphic with GraphDef than TF). But there isn't guide to either unfortunately, but depending on your use it's easier to point to different folks and ask. The passes themselves are standard MLIR ones and you seem like you know that part (the ops I can point you to their definitions in either dialect too).  tfopt is more of a testing tool, so the UI is more basic/sharp edges but it's fairly simple to add a pass. There isn't a registration mechanism to add a pass beyond just modifying it to register your pass and changing the BUILD file (alternatively one can just create a new fooopt tool and register the dialects and passes one want/copy those from tfopt and add new ones, this should be possible wherever you can add bazel dep). But it mostly works well enough for experimentation/playing. For the other one I could have sworn we had some mechanism recently, but if not it would work same as above.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
598,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF Lite (CMake): neon2sse missing license file location)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution Linux  Mobile device _No response_  Python version 3.8  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,stewartmiles,TF Lite (CMake): neon2sse missing license file location,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.9.0  Custom Code No  OS Platform and Distribution Linux  Mobile device _No response_  Python version 3.8  Bazel version N/A  GCC/Compiler version N/A  CUDA/cuDNN version N/A  GPU model and memory N/A  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-25T21:32:39Z,stat:awaiting tensorflower type:build/install comp:lite awaiting PR merge TF 2.9,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55741,Hi  ! This issue will be closed once PR CC(Add licenses for neon2sse and OpenGL in TF Lite CMake build.) is merged . Thanks!,"Hi  ! Just wanted to let you know that I could not find any issue in Colab earlier.  Might have been clang issue then. > but required is exact version ""9"" (found CLANG_FORMAT_EXECUTABLENOTFOUND) Could you check with GCC 7.3 or 9 and let us know. Attached Colab gist for reference. Thank you!"," I'm not sure what you're referring to w.r.t Colab. This is regarding TF lite not TensorFlow. The issue is at build time, if license checking is enabled (logic I wrote while at Google) the neon2sse module is missing a reference to a license file, I have a pull https://github.com/tensorflow/tensorflow/pull/55743 open to fix this could someone please review?",Ok  ! I found similar results for Tensorflow/lite build too. Attached gist for reference. will connect with Team internally on this (just wanted to let you on clang pointer from stacktrace). Thank you !,Are you satisfied with the resolution of your issue? Yes No
783,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: Fix `linalg:self_adjoint_eig_op_test` on Linux aarch64)ï¼Œ å†…å®¹æ˜¯ (The tolerance on self_adjoint_eig_op_test seems a bit tight. The test is currently failing on aarch64 ( CC(TensorFlow unit test failure python/kernel_tests:self_adjoint_eig_op_test)). Playing around with small perturbations of the inputs and step size `delta` on x86_64, the max error seems to be in the range 0.0080.016. Increasing the test tolerance therefore seems reasonable to account for this error range. Fixes CC(TensorFlow unit test failure python/kernel_tests:self_adjoint_eig_op_test). PiperOriginRevId: 439758034)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,penpornk,r2.9 cherry-pick: Fix `linalg:self_adjoint_eig_op_test` on Linux aarch64,"The tolerance on self_adjoint_eig_op_test seems a bit tight. The test is currently failing on aarch64 ( CC(TensorFlow unit test failure python/kernel_tests:self_adjoint_eig_op_test)). Playing around with small perturbations of the inputs and step size `delta` on x86_64, the max error seems to be in the range 0.0080.016. Increasing the test tolerance therefore seems reasonable to account for this error range. Fixes CC(TensorFlow unit test failure python/kernel_tests:self_adjoint_eig_op_test). PiperOriginRevId: 439758034",2022-04-25T17:52:31Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55737
613,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can not build from source)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution windows 10  Mobile device _No response_  Python version 3.7  Bazel version 5.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,onurberkay,Can not build from source,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.9  Custom Code No  OS Platform and Distribution windows 10  Mobile device _No response_  Python version 3.7  Bazel version 5.0.0  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-25T17:28:55Z,stat:awaiting response type:build/install stale subtype:windows TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/55733,"`ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/tools/pip_package/BUILD:277:10: Target '//tensorflow/tools/common:traverse' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package' ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/tools/pip_package/BUILD:277:10: Target '//tensorflow/python/distribute:parameter_server_strategy_v2' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package' ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/tools/pip_package/BUILD:277:10: Target '//tensorflow/python/distribute/coordinator:cluster_coordinator' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package' ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/tools/pip_package/BUILD:277:10: Target '//tensorflow/python/distribute/coordinator:remote_eager_lib' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package' ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/tools/pip_package/BUILD:277:10: Target '//tensorflow/python/distribute/coordinator:metric_utils' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package' ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/tools/pip_package/BUILD:277:10: Target '//tensorflow/python/distribute/experimental/rpc:rpc_ops' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package' ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/tools/pip_package/BUILD:277:10: Target '//tensorflow/tools/docs:tf_doctest_lib' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package' ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed INFO: Elapsed time: 0.309s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)`", Could you please make sure that you have followed the mentioned steps here and refer to the tested build configurations? Thanks!,  yes I have already followed this tutorial,btw I have same problem with master branch,", Can you try the workaround mentioned on similar thread  CC(Error building from source).  Let us know if that helps. Thanks!","> ERROR: C:/users/onurb/downloads/tensorflow2.9.0rc1/tensorflow2.9.0rc1/tensorflow/python/tools/api/generator/BUILD:40:8: In rule 'create_python_api_test', size 'medium' is not a valid size. At what commit are you building? There is no `medium` test size currently https://github.com/tensorflow/tensorflow/blob/1cca89708c5b218e38cd68d341c98c2d06f3870b/tensorflow/python/tools/api/generator/BUILDL40L53",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
633,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU Execution Failure)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.6  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device 20.04.1 Ubuntu SMP  Python version Python 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mosicr,TPU Execution Failure,Click to expand!    Issue Type Support  Source binary  Tensorflow Version tf 2.6  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device 20.04.1 Ubuntu SMP  Python version Python 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-22T11:55:04Z,stat:awaiting response type:support stale comp:dist-strat comp:tpus 2.6.0,closed,0,13,https://github.com/tensorflow/tensorflow/issues/55713," , While trying to reproduce the issue in colab environment, the .pynb file has been corrupted.Can you please share the file and dependencies to reproduce the issue.Thanks!","occup5.zip Hi,  I am not executing in Colab. I am running it on TPU VM. I put it all in one python file. If TPU related statements are omitted, it runs ok ( trains on CPU ). But if I add TPU related statements ( strategy.scope, .function decorator , etc ), it blows up / doesn't start training.  This version fails with: 2252  File ""occup5.py"", line 367, in  2253    loss_value = train_step(inputs) 2254  File ""occup5.py"", line 354, in train_step 2255    optimizer.apply_gradients(zip(grads, model.trainable_weights)) 2256  File ""/usr/local/lib/python3.8/distpackages/keras/optimizer_v2/optimizer_v2.py"", line 62\     8, in apply_gradients 2257    self._create_all_weights(var_list) 2258  File ""/usr/local/lib/python3.8/distpackages/keras/optimizer_v2/optimizer_v2.py"", line 81\     5, in _create_all_weights 2259    self._create_slots(var_list) 2260  File ""/usr/local/lib/python3.8/distpackages/keras/optimizer_v2/adam.py"", line 117, in _c\     reate_slots 2261    self.add_slot(var, 'm') 2262  File ""/usr/local/lib/python3.8/distpackages/keras/optimizer_v2/optimizer_v2.py"", line 89\     2, in add_slot 2263    raise ValueError( 2264ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Stra\     tegy (), \     which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/\     kernel:0' shape=(7, 7, 23, 64) dtype=float32, numpy=     array([[[[ 7.60472193e03,  1.16682872e02,  6.13643974e03"," , I was facing different error while executing the code.Please find the gist of it here.","Hi, Your dataset is empty. You need to copy at least one tfrecord to local location.",https://github.com/waymoresearch/waymoopendataset/issues/420,", To replicate the above issue, can you please share all required data files like `training_tfexample`, `validation_scenario_ids.txt`, `testing_scenario_ids.txt`, `validation.tfrecord` etc. If possible, please feel free to share colab gist of TPU and CPU execution. Thank you!","Hi,  This is the input file location:  gs://waymo_open_dataset_motion_v_1_1_0/uncompressed/tf_example/training/training_tfexample.tfrecord00000of01000 The rest of files ( scenario_ids etc ) are not used , you can safely disregard.  Please find attached CPU exec log.  TPU exec fails in various modalities depending on what code change I introduce. I mostly played with strategy.scope and tf.function placement.  occup.zip ( occup.zip file is the original code and execution log).  occup5.zip occup5.zip is an example of TPU modification and execution. It seems significant rewrite is needed to run this logic on TPU; simple tf.function decorator won't do. ",", The given `tf record` input file is available in google storage which we are unable to access.To reproduce the issue, can you please attach the tf record file here. So that we can download and try to replicate the issue in our local environment.Thanks!","Hi, TF record is 500M, can't upload to github compressed. I enabled public access to  gs://avwaymo/training_tfexample.tfrecord00000of01000 You should be able to download from the Google Cloud ( gsutil cp gs://avwaymo/training_tfexample.tfrecord00000of01000 . ); or perhaps use GCP Storage Browser and download the file . ","> ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy, which is different from the scope used for the original variable A potential solution to this error may be consolidating model compilation and optimizer creation under `strategy.scope()`:  Then, after deleting the outofscope declaration of `optimizer` (before `train_step`), you should be able to train with:  You can try making these changes from a ""fresh"" copy of `tutorial_occupancy_flow.ipynb`.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
755,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([compiler/mhlo] fix lib/CAPI cmake when using python binding)ï¼Œ å†…å®¹æ˜¯ (There are some problems which are import to discuss: 1. There is a spelling mistake `AllMhLoPasses ` => `AllMhloPasses` 2. `AllMhloPasses` is a library `interface` which not defined by `add_mlir_library`. This causes that `AllMhloPasses` doesn't has property `MLIR_AGGREGATE_DEPS`. Without  `MLIR_AGGREGATE_DEPS` property, the python binding compiling process will failed with many `undefined reference`. 3. I think this PR may not be the best fixing method. If you have more suitable method, tell me please.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,qingyunqu,[compiler/mhlo] fix lib/CAPI cmake when using python binding,"There are some problems which are import to discuss: 1. There is a spelling mistake `AllMhLoPasses ` => `AllMhloPasses` 2. `AllMhloPasses` is a library `interface` which not defined by `add_mlir_library`. This causes that `AllMhloPasses` doesn't has property `MLIR_AGGREGATE_DEPS`. Without  `MLIR_AGGREGATE_DEPS` property, the python binding compiling process will failed with many `undefined reference`. 3. I think this PR may not be the best fixing method. If you have more suitable method, tell me please.",2022-04-22T11:01:47Z,size:S,closed,0,1,https://github.com/tensorflow/tensorflow/issues/55712, Could you please look at this?
652,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(adam optimizer does not work in tensorflow-macos)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflowmacos2.8  Custom Code Yes  OS Platform and Distribution macos 12.3.1  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hijkzzz,adam optimizer does not work in tensorflow-macos,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version tensorflowmacos2.8  Custom Code Yes  OS Platform and Distribution macos 12.3.1  Mobile device _No response_  Python version 3.8  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-22T03:22:38Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.8,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55707, I tried to replicate the issue on colab and didn't face any error.Please find the gist here for reference.Thanks!,">  I tried to replicate the issue on colab and didn't face any error.Please find the gist here for reference.Thanks! Hi, you should run this python script on macOS 12 with AMD GPU~ Then pip install tensorflowmacos and pip install tensorflowmetal and you can reproduce this bug.",Hi  Thanks for reporting this issue. Can you try the workaround mentioned on similar thread. Thanks! ,"> Hi  Thanks for reporting this issue. Can you try the workaround mentioned on similar thread. Thanks! Hi, they are not the same problem. My bug can be fixed by replacing the Adam Optimizer with SGD Optimizer. So the problem is that tensorflowmetal is not compatible with Adam Optimizer ? The key error is "" 93125 segmentation fault""",", Could you share us about CUDA and cuDNN version that you are using. Thanks!","> , Could you share us about CUDA and cuDNN version that you are using. Thanks! Hi , I run this script on macOS / AMD GPU / Apple Metal API~ So I don't use any CUDA and cuDNN API~ My ENV: miniconda 3 macOS 12.3.1 MacBookPro16,1 GPU AMD Radeon Pro 5300M Cmd for installing tensorflowmacos 2.8: SYSTEM_VERSION_COMPAT=0 pip install tensorflowmacos tensorflowmetal Install Scripts from: https://developer.apple.com/metal/tensorflowplugin/ Relevant issues: https://developer.apple.com/forums/thread/691917",", For Tensorflow AMD ROCm, you need to use https://github.com/ROCmSoftwarePlatform/tensorflowupstreamtensorflowrocmport. Also take a look at this link.Thanks!",">  Hi, AMD ROCm only support Linux. For macOS, we should use Apple Metal API. See tensorflow metal plugin  https://developer.apple.com/metal/tensorflowplugin/ https://blog.tensorflow.org/2021/06/pluggabledevicedevicepluginsforTensorFlow.html","I was able to run successfully in Apple M1, could you please try again with the latest Tensorflow version. If you still face an issue, you can replace adam API with the `tf.keras.optimizers.experimental.Adam`. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
680,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.nn.embedding_lookup_sparse doesn't work with @tf.function(jit_compile=True))ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Colab Notebook  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pritamdamania87,tf.nn.embedding_lookup_sparse doesn't work with @tf.function(jit_compile=True),Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Colab Notebook  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standalone code to reproduce the issue   Relevant log output  ,2022-04-21T07:15:37Z,stat:awaiting response type:feature stale comp:dist-strat TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55696,"`embedding_lookup_sparse` is not registered with XLA, `XLA_TPU_JIT` and `XLA_GPU_JIT` as well. As the error message suggests you can perform the unsupported OPs compilation in CPU by setting `tf.config.set_soft_device_placement(True)`. In your case, since you are using only one operation inside tf.function, it makes sense if you use only eager mode, also `experimental_get_compiler_ir` is limited to XLA operations only.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,I think this a valid feature request.,"Triage Notes: `tf.nn.embedding_lookup_sparse` is not supported on TPU, we have set of experimental apis which supports running embedding lookup on TPU, https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingV0 supports embedding lookup on TensorCore and https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbedding supports embedding lookup on Barnacore. They requires some changes to the user's code but should be familiarly easy to do if following the documentation on the page. Note that the first API will only be available in TF 2.9 release.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1156,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Replacing Add Layer with Linear Combination of Learnable Weights)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Feature Request The Add() Layer can be expressed as a linear combination of inputs where each coefficient value is 1.  I was trying to make a custom layer that performs the same as Add, except uses trainable coefficients as we assume a coefficient of 1 is optimal. Unfortunately, I was unable to do so on my own, specifically the gradients.  I do not think this would be too hard to implement as all the code is kind of already there, just need to calculate gradients and then update.  Source source  Tensorflow Version 2.7  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OUStudent,Replacing Add Layer with Linear Combination of Learnable Weights,"Click to expand!    Issue Type Feature Request The Add() Layer can be expressed as a linear combination of inputs where each coefficient value is 1.  I was trying to make a custom layer that performs the same as Add, except uses trainable coefficients as we assume a coefficient of 1 is optimal. Unfortunately, I was unable to do so on my own, specifically the gradients.  I do not think this would be too hard to implement as all the code is kind of already there, just need to calculate gradients and then update.  Source source  Tensorflow Version 2.7  Custom Code Yes  OS Platform and Distribution _No response_  Mobile device _No response_  Python version _No response_  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output _No response_",2022-04-20T22:25:36Z,stat:awaiting response type:feature stale comp:ops,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55693," , Can you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,">  , Can you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks! Imagine we have a skip connection and a connection from the previous block. We can add these two together as follows: skip = .... prev = .... next = Add()([skip, prev]) This add statement is mathematically equal to: next = 1*skip + 1 * prev As we can see, the coefficient for each input path is 1; however, perhaps we could train these coefficients to ""learn"" some type of attention mechanism : next = alpha*skip + beta*next   where alpha and beta are learned during training.  I Don't know how to add use cases, but the use case would to ""learn"" attention between input paths, either for a standard skip and next situation, or if we have multiple paths. ","Replacing an existing functionality will not be a good option since it breaks lots of user code. Rather we can consider it as a new feature request. Since, development of keras moved to separate repository https://github.com/kerasteam/keras/issues Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1134,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GPT-J saved_model conversion to fp16 TF-TRT errors with ``Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 23499914257"")ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4/8.1.0  GPU model and memory A100 40GB  Current Behaviour? I tried converting a saved_model of GPTJ using TFTRT to an FP16 version. I ran into a problem with the protobuf size limit (log below), it seems the model is too large even though saving works correctly. The standalone code was executed using the tensorflow:latestgpu image. The saved_model can be produced with the following snippet  Expected Behaviour: The saved_model should be converted into a TFTRT version.  Standlone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,andoorve,"GPT-J saved_model conversion to fp16 TF-TRT errors with ``Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 23499914257""","Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.9  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4/8.1.0  GPU model and memory A100 40GB  Current Behaviour? I tried converting a saved_model of GPTJ using TFTRT to an FP16 version. I ran into a problem with the protobuf size limit (log below), it seems the model is too large even though saving works correctly. The standalone code was executed using the tensorflow:latestgpu image. The saved_model can be produced with the following snippet  Expected Behaviour: The saved_model should be converted into a TFTRT version.  Standlone code to reproduce the issue   Relevant log output  ",2022-04-20T22:13:37Z,stat:awaiting response type:bug stale comp:gpu:tensorrt TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55692,    any updates on thisï¼Ÿ,"Hi, thanks for the reminder! The error caused by the  protobuf size limit, it occurs for other large models as well (e.g. https://github.com/tensorflow/tensorrt/issues/255).  We are currently working around this issue, by removing the graph freezing step. To enable the workaround, you can add  on top of your conversion script. The `disable_graph_freezing` feature is experimental, and it needs actually two more PRs to be fully functional ( CC([TFTRT] Variable converters followup: INT8 support and edge cases) and CC([TFTRT] ResourceGather converter)).  Currently it depends on the actual representation of variables in the model, whether this will fix the issue. Please give it a try and let us know how it goes.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, Thank you for this workaround. Will try it out! convert_variables_to_constants_v2 also hit this 2BG limitation when converting variables in saved model into constants. Do we have any workaround for this? Related issue: https://github.com/tensorflow/tensorflow/issues/51870
480,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(r2.9 cherry-pick: Fix crash in TF Lite Java API on Android API <= 19)ï¼Œ å†…å®¹æ˜¯ (Work around crash in dlsym when trying to check for the presence of a XNNPACK delegate symbols on Android API <= 19 by detecting the respective Android versions and opting out of XNNPack inference. PiperOriginRevId: 441825578)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Maratyszcza,r2.9 cherry-pick: Fix crash in TF Lite Java API on Android API <= 19,Work around crash in dlsym when trying to check for the presence of a XNNPACK delegate symbols on Android API <= 19 by detecting the respective Android versions and opting out of XNNPack inference. PiperOriginRevId: 441825578,2022-04-20T20:34:12Z,comp:lite,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55691
691,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA: softmax with numerically masked inputs does not match its non-XLA counterpart on CPU)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4  GPU model and memory T4 (16 GB)  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",text generation,gante,XLA: softmax with numerically masked inputs does not match its non-XLA counterpart on CPU,Click to expand!    Issue Type Bug  Source binary  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04.3 LTS  Mobile device _No response_  Python version 3.8.10  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN version 11.4  GPU model and memory T4 (16 GB)  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output _No response_,2022-04-20T17:49:29Z,stat:awaiting response type:bug stale comp:xla TF 2.8,closed,6,7,https://github.com/tensorflow/tensorflow/issues/55682,: https://github.com/huggingface/transformers/issues/16838,"I forgot to add an important detail to this issue: if we add a constant to the input of the XLAcompiled softmax, which should result in the exact same output, then CPU+XLA behaves correctly. () As an example, the following script runs well on CPU and GPU. The only difference is a `+1` in the softmax. Adding `+1` to the input before the XLA function yields the same CPUrelated problem. ","  I was able to replicate the issue on colab using TF v2.8.0, tfnightly on both cpu and gpu runtime.Please refer to the attached  gists. Thanks!",", I tried to execute the mentioned code on the latest tensorflow v2.14 and observed that the code was providing the output which was matching on both GPU and CPU environments. Kindly find the attached gist. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
646,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(build tflite-2.7.1 android_arm64 shared library failed)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.7  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Android  Python version  3.8.10  Bazel version 5.1.0  GCC/Compiler version clang version 9.0.9  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,snowuyl,build tflite-2.7.1 android_arm64 shared library failed,Click to expand!    Issue Type Bug  Source source  Tensorflow Version tf 2.7  Custom Code No  OS Platform and Distribution Linux Ubuntu 20.04  Mobile device Android  Python version  3.8.10  Bazel version 5.1.0  GCC/Compiler version clang version 9.0.9  CUDA/cuDNN version _No response_  GPU model and memory _No response_  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output  ,2022-04-20T14:17:35Z,stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55680,"Hi  ! Could you please look at this issue? It is replicating in 2.6.3 , 2.7  and 2.8 . Thanks!",Hi  ! Could you build with the below command and let us know. `bazel build jobs 6 verbose_failures c opt  config=elinux_aarch64 fat_apk_cpu= arm64v8a define=tflite_convert_with_select_tf_ops=true define=with_select_tf_ops=true //tensorflow/lite:libtensorflowlite.so` Attached gist for reference. Ref Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
648,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Would like to build for ppc64el, is this possible?)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Redhat Enterprise Linux 8  Mobile device _No response_  Python version 3.9  Bazel version bazel4.2.1  GCC/Compiler version 8.5  CUDA/cuDNN version 11.2/8.4  GPU model and memory Nvidia P100 x4 16GB  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hicotton02,"Would like to build for ppc64el, is this possible?",Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version 2.8  Custom Code No  OS Platform and Distribution Redhat Enterprise Linux 8  Mobile device _No response_  Python version 3.9  Bazel version bazel4.2.1  GCC/Compiler version 8.5  CUDA/cuDNN version 11.2/8.4  GPU model and memory Nvidia P100 x4 16GB  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output  ,2022-04-20T05:58:21Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55676, Could you please have a look at this link and  check the tested build configurations.Please let us know if it helps? Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
828,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TRT] TF 2.8.0 EfficientDet D0 TRT conversion failed - Unable to save gradient functions)ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN/TensorRT version 11.2 / 8.1.1.33 / 7.2.31  GPU model and memory Nvidia Tesla T4 16GB (aws g4dn.2xlarge)  Current Behaviour?   Standlone code to reproduce the issue Download EfficientDet D0 model from TensorFlow Hub Extract model tar.gz to efficientdet_d0 folder    Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,apivovarov,[TRT] TF 2.8.0 EfficientDet D0 TRT conversion failed - Unable to save gradient functions,Click to expand!    Issue Type Bug  Source source  Tensorflow Version 2.8.0  Custom Code No  OS Platform and Distribution Linux Ubuntu 18.04  Mobile device _No response_  Python version 3.7  Bazel version _No response_  GCC/Compiler version _No response_  CUDA/cuDNN/TensorRT version 11.2 / 8.1.1.33 / 7.2.31  GPU model and memory Nvidia Tesla T4 16GB (aws g4dn.2xlarge)  Current Behaviour?   Standlone code to reproduce the issue Download EfficientDet D0 model from TensorFlow Hub Extract model tar.gz to efficientdet_d0 folder    Relevant log output  ,2022-04-20T04:41:51Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:gpu:tensorrt TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/55675,Adding `options=tf.saved_model.SaveOptions(experimental_custom_gradients=False)` to `saved_model.save()` method in trt_convert.py helped. But why it is not needed in 2.7.1 and 2.9.0rc0?,"The reason why `save()` works in `2.9.0rc0` is because trt_convert.py uses new list of graph optimizers in `2.9.0rc0`. Inserting `""pruning""` optimization before the original list helps to solve the issue with `Unable to save gradient functions` in TF 2.8.0 ",https://github.com/NVIDIA/TensorRT/tree/master/samples%2Fpython%2Fefficientdet I suggest use above transfer script instead of tftrt.,"Converting saved_model to ONNX and then to TRT might work.  But it is much easier to patch TF 2.8.0 and Inserting `pruning` optimization before `""constfold"", ""layout"", ""constfold""` in trt_convert.py The optimization list is updated in 2.9.0 anyway.",It's much faster than tf trt and it's recommended by efficientdet author.,"There are many ways on how EfficientDet D0 can be executed on Nvidia HW. This particular issue is not asking on how to get the max performance for EfficientDet D0. TFTRT is a generic solution. It can convert and save EfficientDet D0 in 2.7.1 and 2.9.0rc0 This issue reports the problem with saving converted EfficientDet D0 in 2.8.0. I also found a workaroud for the issue in 2.8.0 by adding optimizer ""pruning"" to the list in 2.9.0rc0.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
607,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(build tensorflow2.8 failed )ï¼Œ å†…å®¹æ˜¯ (Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.8  Custom Code Yes  OS Platform and Distribution centos7.6  Mobile device _No response_  Python version 3.7.12  Bazel version 4.2.2  GCC/Compiler version 7.3.1  CUDA/cuDNN version rocm5.0.2  GPU model and memory _No response_  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,south-ocean,build tensorflow2.8 failed ,Click to expand!    Issue Type Build/Install  Source source  Tensorflow Version tf2.8  Custom Code Yes  OS Platform and Distribution centos7.6  Mobile device _No response_  Python version 3.7.12  Bazel version 4.2.2  GCC/Compiler version 7.3.1  CUDA/cuDNN version rocm5.0.2  GPU model and memory _No response_  Current Behaviour?   Standlone code to reproduce the issue   Relevant log output  ,2022-04-19T05:36:29Z,stat:awaiting response type:build/install stale subtype:centos TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55662,Are you satisfied with the resolution of your issue? Yes No,"Hi ocean, Try to clear Bazel cache before building Tensorflow `bazel clean expunge`. Thanks!","I had try that, but it didi't success.  Lastly, i success through change the gcc. thanks","ocean, Glad that its resolved. Can you move this to closed status. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1227,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Google Colab ValueError: faster_rcnn_inception_v2 is not supported for tf version 2. See `model_builder.py` for features extractors compatible with different versions of Tensorflow)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Default)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Google Colab (Default)  TensorFlow version: 2.8.0  Python version: 3.7.13  Installed using virtualenv? pip? conda?: Google Colab (so pip)  Bazel version (if compiling from source):  Google Colab (Default)  GCC/Compiler version (if compiling from source):  (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0  CUDA/cuDNN version:  V11.1.105  GPU model and memory:  Google Colab (Default) **Describe the problem** Don't know why faster_rcnn_inception_v2 is not supported checked the `model_builder_tf2_test.py` inside builder, can't seem to figure out what is wronf Ran the blocks one after other      > Output   > Ouput    > Output )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,santhoshnumberone,Google Colab ValueError: faster_rcnn_inception_v2 is not supported for tf version 2. See `model_builder.py` for features extractors compatible with different versions of Tensorflow,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Default)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Google Colab (Default)  TensorFlow version: 2.8.0  Python version: 3.7.13  Installed using virtualenv? pip? conda?: Google Colab (so pip)  Bazel version (if compiling from source):  Google Colab (Default)  GCC/Compiler version (if compiling from source):  (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0  CUDA/cuDNN version:  V11.1.105  GPU model and memory:  Google Colab (Default) **Describe the problem** Don't know why faster_rcnn_inception_v2 is not supported checked the `model_builder_tf2_test.py` inside builder, can't seem to figure out what is wronf Ran the blocks one after other      > Output   > Ouput    > Output ",2022-04-18T17:59:11Z,stat:awaiting response type:support comp:model TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55654,"Hello  , I request you,please download the models from the below link and let us know if the issue still persists. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md Also i request to look at this reference  thread1 and thread2 with the similar issue.Thanks!"," , Thanks for opening this issue.This issue is more suitable for TensorFlow Models repo. I request you,please post it on Tensorflow Models repo from here. Thanks!","> Hello  , >  > I request you,please download the models from the below link and let us know if the issue still persists. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md Also i request to look at this reference thread1 and thread2 with the similar issue.Thanks!  Thank you, will try your suggestions then raise the issue again if I encounter and more issues under Tensorflow Models",Are you satisfied with the resolution of your issue? Yes No,"> Hello  , >  > I request you,please download the models from the below link and let us know if the issue still persists. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md Also i request to look at this reference thread1 and thread2 with the similar issue.Thanks! I downloaded this model http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_1024x1024_coco17_tpu32.tar.gz and I still have this problem with uncompatiblity. How I can solve it?"
766,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(hlo-legalize-to-linalg is incorrect for mhlo::DotGeneralOp)ï¼Œ å†…å®¹æ˜¯ (The usage of `i` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.ccL2945 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.ccL2970 is incorrect.  You need another index, say `j` that only increments when `assigned_dims[i]` is false. Suggested fix  It might be worth considering adding even more test cases, as obviously the current coverage isn't enough.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,srcarroll,hlo-legalize-to-linalg is incorrect for mhlo::DotGeneralOp,"The usage of `i` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.ccL2945 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.ccL2970 is incorrect.  You need another index, say `j` that only increments when `assigned_dims[i]` is false. Suggested fix  It might be worth considering adding even more test cases, as obviously the current coverage isn't enough.",2022-04-18T15:24:16Z,stat:awaiting response type:bug comp:core awaiting PR merge,closed,0,9,https://github.com/tensorflow/tensorflow/issues/55652,"Hello  , I request you,can you please feel free to submit a PR for the requested change is to be made.Thanks!","Yes I can do that.  However, the issue is that I need to contribute from my work account which will require some amount of bureaucracy.  This is why I decided to post the fix here, since it's a very small change.  If this can wait, I'd be happy to submit a PR after we get things sorted out."," , As suggested I request you,can you please feel free to submit a PR for the requested change is to be made.Thanks!",I have been given the go ahead to do this from my personal account so I will submit a PR soon.," , Can you please refer this document link which delivers the infomation for  contribution to Tensorflow as PR and please feel free to submit a PR for the requested change is to be made.Thanks!",I submitted a PR here already https://github.com/tensorflow/tensorflow/pull/55722.  Sorry I should have notified you," , The PR has been assigned for reviewing and once it is merged this issue will move to closed status.Thanks!"," , Related PR has been closed.Can you please feel free to move this issue to closed status.Thanks!",Are you satisfied with the resolution of your issue? Yes No
1037,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Not able to encode tensor in any type of image format that other libraries can understand)ï¼Œ å†…å®¹æ˜¯ (I am trying to write a tensor to an image format As seen in colab in bottom, when I take the encoded bytes and write it a file using python, it doesn't work. Trying opening the image it gets corrupted. I have used gif, png, jpeg When I use tf.io.write_file it works. I also tried taking a png and using tf.io_decode_png and than tf.io.encoding_png and the image string bytes differ from the original png. I am using tensorflow serving and using this png result through the rest api, where I don't have access to tf.io.write_file so I was wondering what encode  does differently, than traditional encodings. https://www.loom.com/share/4b32b330a0e742cab2ea3a1a9af11b22 https://colab.research.google.com/drive/13Y4ET8jAHZDq7qwUYgg8nuBkxqniUqGuscrollTo=IcXVZepnhkW4)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rohanmuplara,Not able to encode tensor in any type of image format that other libraries can understand,"I am trying to write a tensor to an image format As seen in colab in bottom, when I take the encoded bytes and write it a file using python, it doesn't work. Trying opening the image it gets corrupted. I have used gif, png, jpeg When I use tf.io.write_file it works. I also tried taking a png and using tf.io_decode_png and than tf.io.encoding_png and the image string bytes differ from the original png. I am using tensorflow serving and using this png result through the rest api, where I don't have access to tf.io.write_file so I was wondering what encode  does differently, than traditional encodings. https://www.loom.com/share/4b32b330a0e742cab2ea3a1a9af11b22 https://colab.research.google.com/drive/13Y4ET8jAHZDq7qwUYgg8nuBkxqniUqGuscrollTo=IcXVZepnhkW4",2022-04-18T14:13:16Z,stat:awaiting response type:bug stale,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55651, anything I can do to make the issue more clear,  We could see a duplicate ticket in the serving repo of this issue. Could you please move this issue to closed status as it  will be addressed in the serving repo?  Thanks!, I actually think this more of a tensorflow core issue.  I can remove it from there if you prefer as this is a problem I am seeing in tensorflow core and people might encounter this even if they are not using tensorflow serving; ," Thank you for the update! In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1143,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FusedBatchNormV3 split/fuse to/from Cheaper primitive ops with TF-1.15/TF-2.8)ï¼Œ å†…å®¹æ˜¯ (Hi,  I am doing Inference with Bert Large & Bert Base models with TF1.15 and TF2.8.  **Inference with Checkpoints:** For TF2.8, I observed set of primitive ops are Fused to single node FusedBatchNormV3 whereas in TF1.15 fusion of primitive ops is not happening(The graph consists of Primitive ops). **Inference with Frozen Graph:** With frozen graph(.pb) the graph itself has FusedBatchNormV3 node. For TF1.15 the FusedBatchNormV3 is split into set of primitive ops. whereas in TF2.8 split of ops is not happening (The graph consists of Fused op). I am trying to find where this fusion/split is happening when it differs TF versions. After going through fusions that happens at grappler phase(remapper.cc) I found the fusion is happening before it reaches grappler phase.  Can someone help me to point out where the fusion/split of FusedBatchNormV3 is being taken place? Thanks!!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Alavandar08,FusedBatchNormV3 split/fuse to/from Cheaper primitive ops with TF-1.15/TF-2.8,"Hi,  I am doing Inference with Bert Large & Bert Base models with TF1.15 and TF2.8.  **Inference with Checkpoints:** For TF2.8, I observed set of primitive ops are Fused to single node FusedBatchNormV3 whereas in TF1.15 fusion of primitive ops is not happening(The graph consists of Primitive ops). **Inference with Frozen Graph:** With frozen graph(.pb) the graph itself has FusedBatchNormV3 node. For TF1.15 the FusedBatchNormV3 is split into set of primitive ops. whereas in TF2.8 split of ops is not happening (The graph consists of Fused op). I am trying to find where this fusion/split is happening when it differs TF versions. After going through fusions that happens at grappler phase(remapper.cc) I found the fusion is happening before it reaches grappler phase.  Can someone help me to point out where the fusion/split of FusedBatchNormV3 is being taken place? Thanks!!",2022-04-18T09:40:32Z,stat:awaiting response stale type:others comp:grappler TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55650,"Hello  , I request you, please specify the Use Cases/colab gist or share the link for the mentioned statement.It helps to debug the issue.Thanks!",Hi   As mentioned above one of the use case is set of primitive ops are Fused into single op FusedBatchNormV3 in Bert Large with TF2.8 Here I am adding the before and after fusion of FusedBatchNormV3 with checkpoints snapshots for the reference **Before Fusion:**  **After Fusion:**  The file that I referred to in grappler   https://github.com/tensorflow/tensorflow/blob/r2.8/tensorflow/core/grappler/optimizers/remapper.cc," , The provided grappler file link is redirecting to github homepage.Can you please provide the correct link or code to reproduce the issue.It helps to debug the issue.Thanks!",  Here is the correct url of the grappler file https://github.com/tensorflow/tensorflow/blob/r2.8/tensorflow/core/grappler/optimizers/remapper.cc For use cases we are trying to improve FusedBatchNorm performance so we want to know where the fusion happens.,"In Tensorflow 1.x with the graph mode as default it optimizes the graph computations and the same reason you see mainly primitive ops. With Tensorflow 2.x with eager execution as default, in order to optimize the overall computation and to reduce the memory footprints fused operations are mainly used.  Apart from the code link which you shared, you can refer this documentation for FusedBatchNormV3. For other options to optimize graphs you can refer this document. Here is an other code implementation of NN ops with `FusedNormV3`. Hope this detail helps you. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Gradients not computed upon `tf.concat()`)ï¼Œ å†…å®¹æ˜¯ (Hi! I am trying to rewrite MAVNet (https://github.com/sudakshin/imitation_learning/blob/master/2.train_model/MavNet.py) into genuine Tensorflow (instead of TFLearn) and train it. Note that MAVNet consists of numerous `tf.keras.layers.Concatenate()`, which involves `tf.concat()`. For some reason, gradient flow seems to break... and I just found out that the flow breaks whenever it passes `tf.concat()`. Please pardon if this issue is a duplicate of CC(Gradients do not exist for variables after tf.concat().) , but I feel that the previous issue has been closed too early without being thoroughly investigated and resolved. The details of this issue is as follows: **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.3.02.8.0 (issue reproduced from all versions included in this range)  Python version: 3.8.3  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: CUDA 11.4 + cuDNN 8.1.1  GPU model and memory: NVIDIA GeForce GTX Titan X You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.vers)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,joonjeon,Gradients not computed upon `tf.concat()`,"Hi! I am trying to rewrite MAVNet (https://github.com/sudakshin/imitation_learning/blob/master/2.train_model/MavNet.py) into genuine Tensorflow (instead of TFLearn) and train it. Note that MAVNet consists of numerous `tf.keras.layers.Concatenate()`, which involves `tf.concat()`. For some reason, gradient flow seems to break... and I just found out that the flow breaks whenever it passes `tf.concat()`. Please pardon if this issue is a duplicate of CC(Gradients do not exist for variables after tf.concat().) , but I feel that the previous issue has been closed too early without being thoroughly investigated and resolved. The details of this issue is as follows: **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.3.02.8.0 (issue reproduced from all versions included in this range)  Python version: 3.8.3  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: CUDA 11.4 + cuDNN 8.1.1  GPU model and memory: NVIDIA GeForce GTX Titan X You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.vers",2022-04-18T06:20:48Z,type:bug comp:keras TF 2.8,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55648, Thank you for raising this issue! Please refer to this link and for any further queries  please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thanks!,"So... this is an issue with Keras, right? If I understood your response correctly, for all Tensorflow""native"" functions (i.e. the Tensorflow functions that are not part of Keras) for constructing neural network layers, almost of them including `tf.concat()` can have their gradients computed when properly put within `tf.GradientTape()` context, is that right? If so, then I suppose I can close this issue for a while and mention it to the Keras Team as you have mentioned. Thanks.",Are you satisfied with the resolution of your issue? Yes No
911,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add TF_AssignRefVariable)ï¼Œ å†…å®¹æ˜¯ (The Kernel Extension for Variable Operations API added pluggable device support for resource variables and incomplete support for ref variables (TF_OpKernelContext_ForwardRefInputToRefOutput), but it misses one endpoint to make ref variables usable: TF_AssignRefVariable. This change attempts to fill the gaps in the RFC by adding TF_AssignRefVariable, which is analogous to TF_AssignVariable but for ref variables instead of resource variables. It uses the same semantics where the user has to pass a copy function to be called when copying the value tensor to the ref tensor. **Note: This is a remake of a previous PR that got reverted, with a fix for the `DCHECK` on line 77 in ref_var. `CHECK`.**)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PatriceVignola,Add TF_AssignRefVariable,"The Kernel Extension for Variable Operations API added pluggable device support for resource variables and incomplete support for ref variables (TF_OpKernelContext_ForwardRefInputToRefOutput), but it misses one endpoint to make ref variables usable: TF_AssignRefVariable. This change attempts to fill the gaps in the RFC by adding TF_AssignRefVariable, which is analogous to TF_AssignVariable but for ref variables instead of resource variables. It uses the same semantics where the user has to pass a copy function to be called when copying the value tensor to the ref tensor. **Note: This is a remake of a previous PR that got reverted, with a fix for the `DCHECK` on line 77 in ref_var. `CHECK`.**",2022-04-16T02:26:25Z,ready to pull size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55640
618,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA:GPU] Failed to determine best cudnn convolution algorithm)ï¼Œ å†…å®¹æ˜¯ (I got `Failed to determine best cudnn convolution algorithm`  error when running `facebook/wav2vec2base960h` model using torch_xla on GPU in fp16 mode. This error only occurs when using fp16 and fp32 works fine. Minimal HLO to reproduce:  The following is the full log file.  Tested with tensorflow commit 75861c43005523e2552bb3f85b2f0defc16ea9cf, CUDA 11.4, CUDNN 8.2.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ymwangg,[XLA:GPU] Failed to determine best cudnn convolution algorithm,"I got `Failed to determine best cudnn convolution algorithm`  error when running `facebook/wav2vec2base960h` model using torch_xla on GPU in fp16 mode. This error only occurs when using fp16 and fp32 works fine. Minimal HLO to reproduce:  The following is the full log file.  Tested with tensorflow commit 75861c43005523e2552bb3f85b2f0defc16ea9cf, CUDA 11.4, CUDNN 8.2.",2022-04-15T00:15:40Z,stat:awaiting tensorflower type:bug comp:xla,open,0,5,https://github.com/tensorflow/tensorflow/issues/55633,It looks like this bug is due to `CudnnPadForConvolutions` pass. Disabling this pass solves the error. The above mentioned HLO graph is extracted from the following torch_xla script.  It looks like the `CudnnPadForConvolutions` pass pads filter c_in from 44 to 48 and makes it no longer a divisor of 704 (c_out/groups*batch).  Dump of cudnn debugging information. ,Tracked internally in b/232516873, does the error still reproduce ? Is it on Ampere?,Update: still repros on Ampere.,"Yes, here's what I got on A100: "
1261,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NotImplementedError: Cannot convert a symbolic Tensor (cond_2/strided_slice:0) to a numpy array.)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google Colab  TensorFlow installed from (source or binary): Google Colab (default)  TensorFlow version: 1.15.2  Python version: 3.7.13  Installed using virtualenv? pip? conda?: Google Colab (no env)  Bazel version (if compiling from source): Google Colab (default)  GCC/Compiler version (if compiling from source): Google Colab (default)  CUDA/cuDNN version: Google Colab (default)  GPU model and memory: Google Colab (default) **Describe the problem** Trying to train custom object detector referring blog Custom object detection in the browser using TensorFlow.js **Provide the exact sequence of commands / steps that you executed before running into the problem** Ran these blocks one after other inside Google colab         Error I get  If I run this  Error I get   What is wrong here?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,santhoshnumberone,NotImplementedError: Cannot convert a symbolic Tensor (cond_2/strided_slice:0) to a numpy array.,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google Colab  TensorFlow installed from (source or binary): Google Colab (default)  TensorFlow version: 1.15.2  Python version: 3.7.13  Installed using virtualenv? pip? conda?: Google Colab (no env)  Bazel version (if compiling from source): Google Colab (default)  GCC/Compiler version (if compiling from source): Google Colab (default)  CUDA/cuDNN version: Google Colab (default)  GPU model and memory: Google Colab (default) **Describe the problem** Trying to train custom object detector referring blog Custom object detection in the browser using TensorFlow.js **Provide the exact sequence of commands / steps that you executed before running into the problem** Ran these blocks one after other inside Google colab         Error I get  If I run this  Error I get   What is wrong here?",2022-04-14T17:08:00Z,stat:awaiting response type:support comp:apis TF 1.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55626, We  see that you are using older version of TF (v1.15) which is not actively supported so we recommend you to kindly upgrade  to the latest TF versions(2.4 or later) and let us know if it is still an issue ?Please refer this link for migration  from TF v1.x to 2.x . Thanks!,">  We see that you are using older version of TF (v1.15) which is not actively supported so we recommend you to kindly upgrade to the latest TF versions(2.4 or later) and let us know if it is still an issue ?Please refer this link for migration from TF v1.x to 2.x . Thanks! Thank you for your response, I have raised the same here Google Colab ValueError: faster_rcnn_inception_v2 is not supported for tf version 2. See model_builder.py for features extractors compatible with different versions of Tensorflow",Are you satisfied with the resolution of your issue? Yes No
1938,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow 2.8 distributed training for neural network model (designed by subclass) cannot improve the training runtime on multicore CPUs)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (the code is too long)  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu (18.04.6)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no  TensorFlow installed from (source or binary): pip install tensorflow==2.8.0  TensorFlow version (use command below): 2.8.0  Python version: 3.7.6  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):   CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the current behavior** I am trying to train a neural network model built by tf2.8 and keras 2.8. I would like to train the model with distributed training on multiple CPUs (e.g. EC2 m4.4 with 16 cpu cores) to improve the training performance. I am using MirroredStrategy. I am following the example at https://www.tensorflow.org/tutorials/distribute/keras and https://www.tensorflow.org/tutorials/distribute/custom_training My neural network model is built by subclass (not functional API) of TF2.8 and keras. But, the training run time  (50 mins per epoch) is not improved significantly compared with nondistributed training (65 mins per epoch) on EC2 m4.4 (16 cpu cores). During the training process, most cores are idle or have very low utilizations (< 5%). I also got a warning at the end of each epoch:      **WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_fo)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,umusa,tensorflow 2.8 distributed training for neural network model (designed by subclass) cannot improve the training runtime on multicore CPUs,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (the code is too long)  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu (18.04.6)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no  TensorFlow installed from (source or binary): pip install tensorflow==2.8.0  TensorFlow version (use command below): 2.8.0  Python version: 3.7.6  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):   CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the current behavior** I am trying to train a neural network model built by tf2.8 and keras 2.8. I would like to train the model with distributed training on multiple CPUs (e.g. EC2 m4.4 with 16 cpu cores) to improve the training performance. I am using MirroredStrategy. I am following the example at https://www.tensorflow.org/tutorials/distribute/keras and https://www.tensorflow.org/tutorials/distribute/custom_training My neural network model is built by subclass (not functional API) of TF2.8 and keras. But, the training run time  (50 mins per epoch) is not improved significantly compared with nondistributed training (65 mins per epoch) on EC2 m4.4 (16 cpu cores). During the training process, most cores are idle or have very low utilizations (< 5%). I also got a warning at the end of each epoch:      **WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_fo",2022-04-14T14:05:34Z,stat:awaiting response stale comp:dist-strat type:performance TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55622,"Hello  , You have provided multiple links for the code.Can you please confirm the reproducible code link or provide the complete here to reproduce the issue.Thanks!"," , please use  https://www.tensorflow.org/tutorials/distribute/keras, my code is based on this example. ", Can you please follow the solutions provided here and let me know. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1845,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build issue r2.6 with Cuda Compatibility 3.0)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6.3  Python version: 3.7  Installed using virtualenv? pip? conda?: No  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): MSVC2019  CUDA/cuDNN version: 10.1/7.6  GPU model and memory: Nvidia Quadro K1100M 2GB **Describe the problem** Hi, I'm trying to build Tensorflow 2.6.3 from source on Windows, to support Cuda Compatibility 3.0.` I follow this procedure 53062, but after few tries i continue to get this error during the compiling: > Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=146 INFO: Reading rc options for 'build' from c:\users\priva\tensorflow\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/Users/priva/AppData/Local/Programs/Python/Python37/python.exe INFO: Reading rc options for 'build' from c:\users\priva\tensorflow\.bazelrc:   'build' options: define framework_shared_object=true java_toolchain=//toolchains/java:tf_java_toolchain host_java_toolchain=//toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_spec)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OrazioLombardi,Build issue r2.6 with Cuda Compatibility 3.0,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6.3  Python version: 3.7  Installed using virtualenv? pip? conda?: No  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): MSVC2019  CUDA/cuDNN version: 10.1/7.6  GPU model and memory: Nvidia Quadro K1100M 2GB **Describe the problem** Hi, I'm trying to build Tensorflow 2.6.3 from source on Windows, to support Cuda Compatibility 3.0.` I follow this procedure 53062, but after few tries i continue to get this error during the compiling: > Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=146 INFO: Reading rc options for 'build' from c:\users\priva\tensorflow\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/Users/priva/AppData/Local/Programs/Python/Python37/python.exe INFO: Reading rc options for 'build' from c:\users\priva\tensorflow\.bazelrc:   'build' options: define framework_shared_object=true java_toolchain=//toolchains/java:tf_java_toolchain host_java_toolchain=//toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_spec",2022-04-14T11:43:32Z,stat:awaiting response type:build/install stale subtype:windows 2.6.0,closed,0,13,https://github.com/tensorflow/tensorflow/issues/55621," Could you please try to upgrade to latest TF v2.8.0 and refer this tested build configurations , please let us know if it helps?Thanks!", the problem is that cuda 11.2 doesn't support compute capability 3.0. I can tra to upgrade to 2.8 but I cannot use a tested build configuration as suggest. I also try the tested build configuration of TF v2.4 but the result it's quite the same.," Thank you for the update!  From the error log it seems to be an issue with MSVC2019 , could you please  make sure that you are using the right GCC version as mentioned in the tested build configuration ? Thanks!","Hi . I have GCC installed by Cygwin. Today I update GCC to the latest version and try to build tensorflow but nothing seems to change in the output. By the way, in the tested build configuration for Windows, GCC is not metioned (GCC is mentioned only in Linux guide), are you sure that this could influence the building process? How can I check that the right version of GCC is installed on my computer and that works correctly with MSVC2019?",", Hi, Issue with CUDA compatibility. Tensorflow 2.6 doesnâ€™t support Cuda compatibility 3.  You need to add the compatibility factor. `ComputeCapabilityFromString(""3.0""), ComputeCapabilityFromString(""3.5""), ComputeCapabilityFromString(""5.2"")};` Do changes in this file. Thanks!  "," Hi, thank you for your answer. I already edit the `gpu_device.cc` file adding the `ComputeCapabilityFromString(""3.0"")` statement as already mentioned in my issue description. I think that the problem is related to MSVC that has problem to compile some of the files.",",  Looks issue with MSVC2019. Make sure you followed steps mentioned here and install required packages. Thanks ! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"  Hi guy, I'm enabling tensorflow 2.8 on mac 10.13.6 with cuda 10.1, cudnn7.3.6 and meeting with the same issue. After investigating with code logic, I figured out the root cause: you check the implementation of absl::lts_20210324::span_internal::GetDataImpl in https://github.com/abseil/abseilcpp/blob/1ae9b71c474628d60eb251a3f62967fe64151bb2/absl/types/internal/span.hL38, you will find   this function is used to call typename with remember function data(). with keywords decltype and constexpr auto, the compiler could delegate the instantiation phase for type compatibility checkup, saying whether such type has data() function. However, the trick lies during the instantiation phase. That is if and only if the typename could be found with detailed declaration, otherwise it will fail. OK, back to the instantiation phase of our case:  check the implementation of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_shape.hL375  how does TensorShapeProto generate? it is a sort of proto file (protobuf protocol declaration) which will be converted to pb.h file together with its default implementation based on protobuf protocol. But there is no declaration in resize_nearest_neighbor_op_gpu.cu..  so like https://github.com/tensorflow/tensorflow/blob/6b7b98fa154d57260337e5e5f929f8907f4b23f1/tensorflow/lite/kernels/shim/tf_tensor_view_test.cc,  just include  include ""tensorflow/core/framework/tensor_shape.pb.h""  to your resize_nearest_neighbor_op_gpu.cu.. PS: Regarding why cuda 11.0+ doesn't have such pain, I guess during compilation resize_nearest_neighbor_op_gpu.cu.pic.d may insert tensor_shape.pb.h in advance, so the class type is ready to use during compilation. Enjoy your hack anyway. ^^",">  >  > Hi guy, I'm enabling tensorflow 2.8 on mac 10.13.6 with cuda 10.1, cudnn7.3.6 and meeting with the same issue. After investigating with code logic, I figured out the root cause: >  > you check the implementation of absl::lts_20210324::span_internal::GetDataImpl in https://github.com/abseil/abseilcpp/blob/1ae9b71c474628d60eb251a3f62967fe64151bb2/absl/types/internal/span.hL38, you will find >  >  >  > this function is used to call typename with remember function data(). with keywords decltype and constexpr auto, the compiler could delegate the instantiation phase for type compatibility checkup, saying whether such type has data() function. >  > However, the trick lies during the instantiation phase. That is if and only if the typename could be found with detailed declaration, otherwise it will fail. >  > OK, back to the instantiation phase of our case: >  >  >  > check the implementation of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_shape.hL375 >  >  >  > how does TensorShapeProto generate? it is a sort of proto file (protobuf protocol declaration) which will be converted to pb.h file together with its default implementation based on protobuf protocol. >  > But there is no declaration in resize_nearest_neighbor_op_gpu.cu.. so like https://github.com/tensorflow/tensorflow/blob/6b7b98fa154d57260337e5e5f929f8907f4b23f1/tensorflow/lite/kernels/shim/tf_tensor_view_test.cc, just include >  > include ""tensorflow/core/framework/tensor_shape.pb.h"" >  > to your resize_nearest_neighbor_op_gpu.cu.. >  > PS: Regarding why cuda 11.0+ doesn't have such pain, I guess during compilation resize_nearest_neighbor_op_gpu.cu.pic.d may insert tensor_shape.pb.h in advance, so the class type is ready to use during compilation. >  > Enjoy your hack anyway. ^^ Hi , thank's a lot for your answer, it's a very rich explanation of your similar problem, and there are a lot of things to think about. I'll do some tests and let you know the results. Probably it's not the resize_nearest_neighbor_op_gpu.cu., but the inplace_ops_functor_gpu.cu.cpp1 that is not compiling. I don't understand if https://github.com/tensorflow/tensorflow/blob/6b7b98fa154d57260337e5e5f929f8907f4b23f1/tensorflow/lite/kernels/shim/tf_tensor_view_test.  include ""tensorflow/core/framework/tensor_shape.pb.h"" Thank's a lot for your time, and have a good day!"," I checked with Line 22 of https://github.com/tensorflow/tensorflow/blob/6b7b98fa154d57260337e5e5f929f8907f4b23f1/tensorflow/lite/kernels/shim/tf_tensor_view_test.cc include ""tensorflow/core/framework/tensor.pb.h"" check readme.md in https://github.com/tensorflow/tensorflow/tree/6b7b98fa154d57260337e5e5f929f8907f4b23f1/tensorflow/lite/kernels/shim See **the unit tests tf_tensor_view_test..cc** for more usage. 1. you don't have to include for *_test.cc, as they are unit tests which could be included in framework ^^. They just serve for the purpose of checking up if the functionalities as expected. Well, also a showcase about how to use the class, check the link above about how to use TensorView, one exactly case.  2.  inplace_ops_functor_gpu.cu.. but unfortunately no TensorShapeProto declaration has been found during compilation phase.  I think you need to add include ""tensorflow/core/framework/tensor_shape.pb.h"" in it 3. Regarding why cuda 10.1 or older version miss the supposed operation of including proto files in , I still need to compare the default behaviors of old nvcc compilers and the latest one (I suppose so). Unfortunately, now there is no clear clue....  Tensorflow compilation via bazel is always a big monster.... ..you know......"
918,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.io.encode png doesn't work when trying to write array to png)ï¼Œ å†…å®¹æ˜¯ (Please see here.  https://colab.research.google.com/drive/1ayBIErWSDAZGJ6B_CjeqraJyMAzEWSyv?usp=sharing I am trying to write a tensor to a png. As seen here, when I take the encoded bytes and write it a file using python, it doesn't work. Trying opening the image it gets corrupted  When I use tf.io.write_file it works. I also tried taking a png and using tf.io_decode_png and than tf.io.encoding_png and the image string bytes differ from the original png. I am using tensorflow serving and using this png result through the rest api, where I don't have access to tf.io.wrte_file so I was wondering what encode_png does differently, than traditional png encodings.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rohanmuplara,tf.io.encode png doesn't work when trying to write array to png,"Please see here.  https://colab.research.google.com/drive/1ayBIErWSDAZGJ6B_CjeqraJyMAzEWSyv?usp=sharing I am trying to write a tensor to a png. As seen here, when I take the encoded bytes and write it a file using python, it doesn't work. Trying opening the image it gets corrupted  When I use tf.io.write_file it works. I also tried taking a png and using tf.io_decode_png and than tf.io.encoding_png and the image string bytes differ from the original png. I am using tensorflow serving and using this png result through the rest api, where I don't have access to tf.io.wrte_file so I was wondering what encode_png does differently, than traditional png encodings.  ",2022-04-14T02:40:03Z,stat:awaiting tensorflower type:bug comp:ops TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/55615,"Hello  , The error log stating that `write() takes exactly one argument (0 given)` which will be in bytes, not in str type.I request you please try to execute the code by providing the expected input.Thanks!", please try again. I was just playing around in colab and forgot to return it to proper state. I apologize for that. The error is one I was describing above., sorry for repining but I haven't been able to get it to work for any other image formats. I have also tried other formats here and they are still not working. I have added this https://colab.research.google.com/drive/13Y4ET8jAHZDq7qwUYgg8nuBkxqniUqGuscrollTo=kTeKBHgkcaG.," , On running the given code snippet, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory`. Please find the gist of it here.",You have to upload a sample image for that cell because it reads it locally. Wasn't sure how it was supposed to work otherwise.  https://userimages.githubusercontent.com/47044724/1638086242f435116f8cd4cb29be0116c16fe56fe.png, I have gotten rid of having to upload a sample image to make it easier. I also added a video https://www.loom.com/share/4b32b330a0e742cab2ea3a1a9af11b22 here. I am also happy to jump on google meet or whatever is required," , I was able to reproduce the issue in tf v2.7, v2.8 and nightly.Please find the attached gist.","The returned value from `tf.io.encode_png` is a `Tensor` of type `string`, where the string value is the image bytes. Trying to write this directly to a file in python attempts to write the `Tensor` object itself, which is a container and pointer to the underlying data.  It does not write the string contents.  Using `tf.io.write_file` will write the underlying input `Tensor` contents to the file, which is why it works. To write the contents without `tf.io.write_file`, you need to extract the string contents and write that directly. ",Are you satisfied with the resolution of your issue? Yes No
1189,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA] Different JIT compile behavior from TF2.7)ï¼Œ å†…å®¹æ˜¯ (For the customized code below, I have seen such a error at runtime when xla is turned on. This does NOT appear in TF2.7. `20220413 19:49:36.873241: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:436 : INVALID_ARGUMENT: Fail to proof the equality of two dimensions at compile time: %multiply.144 = s32[] multiply(s32[] %constant.142, s32[] %add.1), metadata={op_type=""Reshape"" op_name=""Reshape_3""} vs %add = s32[] add(s32[] %reduce.109, s32[] %constant.17)` **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution: Ubuntu 20.04.4 LTS  TensorFlow installed from (source or binary): binary  TensorFlow version:2.8.0  Python version: 3.8  GCC/Compiler version (if compiling from source): gcc 10  CUDA/cuDNN version: 11.6  GPU model and memory: V100, 32G **Standalone code to reproduce the issue**  Tracked down to commit ac4575.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,wenscarl,[XLA] Different JIT compile behavior from TF2.7,"For the customized code below, I have seen such a error at runtime when xla is turned on. This does NOT appear in TF2.7. `20220413 19:49:36.873241: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:436 : INVALID_ARGUMENT: Fail to proof the equality of two dimensions at compile time: %multiply.144 = s32[] multiply(s32[] %constant.142, s32[] %add.1), metadata={op_type=""Reshape"" op_name=""Reshape_3""} vs %add = s32[] add(s32[] %reduce.109, s32[] %constant.17)` **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution: Ubuntu 20.04.4 LTS  TensorFlow installed from (source or binary): binary  TensorFlow version:2.8.0  Python version: 3.8  GCC/Compiler version (if compiling from source): gcc 10  CUDA/cuDNN version: 11.6  GPU model and memory: V100, 32G **Standalone code to reproduce the issue**  Tracked down to commit ac4575.",2022-04-13T19:56:19Z,type:bug comp:xla TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/55610, can you triage to check where this error is coming from?,"Right, what has changed is that `tf.where` became compilable in autoclustering, and then the compiler tries at compile time to prove equality of dimensions from two dynamic outputs and fails. Do you need a general solution, or a workaround? Switching this line (https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/service/gpu/gpu_compiler.cc;l=440?q=padder%20f:gpu_compiler) to `kIgnore`  would solve the issue by ignoring the check. We can also give you a flag to switch those without modifying TF. Or do you need a general solution which always works?","Thanks . Making `tf.where` XLA compilable in autoclustering may results into a perf regression. Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size.  See the snapshot from profiler. !recompile  Reverting this commit resolve this.  I would prefer to have a switch flag to turn it off. "," OK it might make sense to not compile tf.where in autoclustering environment altogether then. I can make this change. > Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size Do you have a repro I could try? XLA compiles up to the upper bound of the `tf.where` output, not to the concrete bound seen at runtime, so it seems bizarre it would recompile.", Actually do you want to send a patch disabling tf.where autoclustering?,"  > Do you have a repro I could try? I don't have a lite repro. The regression is observed from wideanddeep model. With this commit, XLA compiled 3 times as indicated by the above snapshot. Turning off, only 1 XLA jit compile as shown below. !image. The recompilation may not be directly caused by tf.where autoclustering, but turningoff is a WAR.","For the sample code above, the compile time for cluster_1 which include tf.where, is about 10sec while for other clusters on ms level. Recompilation couldn't be reproduced by the sample code, but once happen, it would stall the cpu for 10 sec long. ",  > Actually do you want to send a patch disabling tf.where autoclustering? Yes. `tf_xla_ops_to_cluster` serves a similar purpose but not quite the granularity we want.  I would like to propose a `tf_xla_ops_cluster_blacklist` under `TF_XLA_FLAG` which takes comma separated op names like where or unique. Does that sound good?  ,"Thanks! We already have a struct for enabling/disabling such ops, in compilability_check_util. Readding tf.where there and disabling it by default for autoclustering could make sense. On Wed, Apr 27, 2022 at 21:36 wenscarl ***@***.***> wrote: >   > > Actually do you want to send a patch disabling tf.where autoclustering? > Yes. tf_xla_ops_to_cluster serves a similar purpose but not quite the > granularity we want. I would like to propose a > tf_xla_ops_cluster_blacklist under TF_XLA_FLAG which takes comma > separated op names like where or unique. Does that sound good? > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >",This is also causing an issue for me. Do you have a pointer in TF code where I can hack together an addition to not compile tf.where?,  A PR is opened here.,PR merged. Closing.,Are you satisfied with the resolution of your issue? Yes No
1941,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow 2.8 neural network model training cannot be run as multiprocess even though the training data is created by keras.utils.Sequence )ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu (18.04.6)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no  TensorFlow installed from (source or binary): pip install tensorflow==2.8.0  TensorFlow version (use command below): 2.8.0  Python version: 3.7.6  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):   CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the current behavior** I am trying to train a neural network model built by tf2.8 ans keras 2.8. I would like to train the model with **""multiprocess"".** to improve the training performance. The following url shows that only keras.utils.Sequence is supported for multiprocess. https://www.tensorflow.org/api_docs/python/tf/keras/Modelfit "" ""use_multiprocessing""  Boolean. **Used for generator or keras.utils.Sequence input only**. If True, use processbased threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass nonpicklable arguments to the generator as they can't be passed easily to children processes."" **My code (tf2.8):**    If I run model.fit as:  I got error:    If I run model.fit as:   The whole training process is frozen and made no progress and no errors popped. It seems that there is a deadlock ?  I have also tried tf2.9, got the same error.  Could anybody let me know what I did wrong ?  I )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,umusa,tensorflow 2.8 neural network model training cannot be run as multiprocess even though the training data is created by keras.utils.Sequence ,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu (18.04.6)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no  TensorFlow installed from (source or binary): pip install tensorflow==2.8.0  TensorFlow version (use command below): 2.8.0  Python version: 3.7.6  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):   CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the current behavior** I am trying to train a neural network model built by tf2.8 ans keras 2.8. I would like to train the model with **""multiprocess"".** to improve the training performance. The following url shows that only keras.utils.Sequence is supported for multiprocess. https://www.tensorflow.org/api_docs/python/tf/keras/Modelfit "" ""use_multiprocessing""  Boolean. **Used for generator or keras.utils.Sequence input only**. If True, use processbased threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass nonpicklable arguments to the generator as they can't be passed easily to children processes."" **My code (tf2.8):**    If I run model.fit as:  I got error:    If I run model.fit as:   The whole training process is frozen and made no progress and no errors popped. It seems that there is a deadlock ?  I have also tried tf2.9, got the same error.  Could anybody let me know what I did wrong ?  I ",2022-04-13T19:19:21Z,stat:awaiting response type:bug stale comp:keras TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55609,"Hello , On running the given code snippet, I am facing an different error.Can you please find the gist of it here and requesting to provide the complete code to debug the issue.Thanks!"," , this code needs to access some data files, which are not available to access here. Can you please check the logic to confirm that I do not do anything wrong. thanks"," , Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the troubleshooting process, could you please provide a complete code and the dependencies you are using.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
740,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(right after install TensorFlowLiteObjC , by pod install, cause error in Xcode)ï¼Œ å†…å®¹æ˜¯ (my Podfile is   right after install pod file by commanding  arch x86_64 pod install then build keep failed saying  diff: /Podfile.lock: No such file or directory diff: /Manifest.lock: No such file or directory error: The sandbox is not in sync with the Podfile.lock. Run 'pod install' or update your CocoaPods installation. I did many things that on web says...delete pods file...clean .... I did almost everything but no worth it.  it just happened when install pod.  m1 Mac )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kotran88,"right after install TensorFlowLiteObjC , by pod install, cause error in Xcode",my Podfile is   right after install pod file by commanding  arch x86_64 pod install then build keep failed saying  diff: /Podfile.lock: No such file or directory diff: /Manifest.lock: No such file or directory error: The sandbox is not in sync with the Podfile.lock. Run 'pod install' or update your CocoaPods installation. I did many things that on web says...delete pods file...clean .... I did almost everything but no worth it.  it just happened when install pod.  m1 Mac ,2022-04-13T01:53:01Z,stat:awaiting response type:bug stale comp:lite subtype:macOS,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55599,"Hi  ! Is this issue duplicate to CC(on xcode,  not found for architecture arm64 caused) ?",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1217,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ImportError: cannot import name 'deepmac_meta_arch' from 'object_detection.meta_architectures')ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google colab  TensorFlow installed from (source or binary): Google colab (default)  TensorFlow version: 1.15.2  Python version: 3.7.13  Installed using virtualenv? pip? conda?: Google Colab (no env)  Bazel version (if compiling from source): Google colab (default)  GCC/Compiler version (if compiling from source): Google colab (default)  CUDA/cuDNN version: Google colab (default)  GPU model and memory: Google colab (default) **Describe the problem** Trying to train custom object detector referring blog Custom object detection in the browser using TensorFlow.js **Provide the exact sequence of commands / steps that you executed before running into the problem** Ran these blocks one after other inside Google colab        Error in this block )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,santhoshnumberone,ImportError: cannot import name 'deepmac_meta_arch' from 'object_detection.meta_architectures',"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google colab  TensorFlow installed from (source or binary): Google colab (default)  TensorFlow version: 1.15.2  Python version: 3.7.13  Installed using virtualenv? pip? conda?: Google Colab (no env)  Bazel version (if compiling from source): Google colab (default)  GCC/Compiler version (if compiling from source): Google colab (default)  CUDA/cuDNN version: Google colab (default)  GPU model and memory: Google colab (default) **Describe the problem** Trying to train custom object detector referring blog Custom object detection in the browser using TensorFlow.js **Provide the exact sequence of commands / steps that you executed before running into the problem** Ran these blocks one after other inside Google colab        Error in this block ",2022-04-12T17:12:40Z,stat:awaiting response comp:apis TF 1.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55591,"Since you are using `TF1.x`,  1. Can you try to install `TF1.x` Object Detection API instead of `TF2.x` Object Detection API as shown below `cp object_detection/packages/tf1/setup.py  ` 2. Can you try to change `model_builder_tf2_test.py` to `model_builder_tf1_test.py` as shown below `!python /content/drive/MyDrive/Tensorflow/models/research/object_detection/builders/model_builder_tf1_test.py ` Or simple workaround is try to use `%tensorflow_version 2.x` instead of `%tensorflow_version 1.x`.  For more details please refer here. Thanks!","> Since you are using `TF1.x`, >  > 1. Can you try to install `TF1.x` Object Detection API instead of `TF2.x` Object Detection API as shown below >  > `cp object_detection/packages/tf1/setup.py ` >  > 2. Can you try to change `model_builder_tf2_test.py` to `model_builder_tf1_test.py` as shown below >  > `!python /content/drive/MyDrive/Tensorflow/models/research/object_detection/builders/model_builder_tf1_test.py ` >  > Or simple workaround is try to use `%tensorflow_version 2.x` instead of `%tensorflow_version 1.x`. >  > For more details please refer here. Thanks! Thank you it started working ",Are you satisfied with the resolution of your issue? Yes No,"For those using Tensorflow 2.2 or above, Manually Downloading the missing files from https://github.com/tensorflow/models/tree/master/research/object_detection/meta_architectures and putting them in python packages folder can fix it"
844,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(how to define representative_data_gen function when model has two inputs?)ï¼Œ å†…å®¹æ˜¯ (https://www.tensorflow.org/lite/performance/post_training_integer_quant: def representative_data_gen():   for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):      Model has only one input so each data point has one element.     yield [input_value] converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_data_gen tflite_model_quant = converter.convert() My question is, how to modify the code listed above, when model has two or more inputs?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,njuhang,how to define representative_data_gen function when model has two inputs?,"https://www.tensorflow.org/lite/performance/post_training_integer_quant: def representative_data_gen():   for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):      Model has only one input so each data point has one element.     yield [input_value] converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_data_gen tflite_model_quant = converter.convert() My question is, how to modify the code listed above, when model has two or more inputs?",2022-04-12T12:17:32Z,type:support TFLiteConverter TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55586,Hi  ! You have to add the other input in the yield line of the representative Dataset. Please use the input signature to keep the inputs in order . Attaching relevant thread for reference. Thanks!,"yes, I solve the problem", ! Thanks for confirmation . Moving this issue to closed status then.,Are you satisfied with the resolution of your issue? Yes No
1051,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot install tensorflow-gpu==1.15.0)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: Linux Ubuntu 20.04  TensorFlow version: tensorflowgpu==1.15.0  Python version: 3.17.13  Installed using virtualenv? pip? conda?: pip  CUDA/cuDNN version:   GPU model and memory:  Describe the problem I get the following error when trying to install tensorflowgpu in version  1.15.0:  After I try change this line in file `requirements.txt`.  to   And I've got this error:  After that I install `cmake` by `sudo apt install cmake`. And I've got this error:  I changed keras version in file `requirements.txt`.  to  And then I 've got error:     I changed numpy version in file `requirements.txt`.  to  then i have got error:  and second error  For improve fail I run this command:  `sudo aptget install protobufcompiler libprotocdev` and eventually  `Installation Succeeded`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dvogureckiy99,Cannot install tensorflow-gpu==1.15.0,**System information**  OS Platform and Distribution: Linux Ubuntu 20.04  TensorFlow version: tensorflowgpu==1.15.0  Python version: 3.17.13  Installed using virtualenv? pip? conda?: pip  CUDA/cuDNN version:   GPU model and memory:  Describe the problem I get the following error when trying to install tensorflowgpu in version  1.15.0:  After I try change this line in file `requirements.txt`.  to   And I've got this error:  After that I install `cmake` by `sudo apt install cmake`. And I've got this error:  I changed keras version in file `requirements.txt`.  to  And then I 've got error:     I changed numpy version in file `requirements.txt`.  to  then i have got error:  and second error  For improve fail I run this command:  `sudo aptget install protobufcompiler libprotocdev` and eventually  `Installation Succeeded`,2022-04-11T22:55:22Z,type:build/install subtype: ubuntu/linux TF 1.15,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55582,TF 1.15 is too old. When it was released there was no python 3.8. This is not a TF issue.,Are you satisfied with the resolution of your issue? Yes No
602,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(macOS wheel URLs are incorrect)ï¼Œ å†…å®¹æ˜¯ ( URL(s) with the issue: https://www.tensorflow.org/install/pip  Description of issue (what needs changing): The Mac wheels that the docs page purport to exist do not, in fact, exist:  Where are these wheels actually located?  Submit a pull request? I would submit a PR, but I am not aware of what the correct URLs are. The storage bucket does not have any index as far as I could tell.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,samuela,macOS wheel URLs are incorrect," URL(s) with the issue: https://www.tensorflow.org/install/pip  Description of issue (what needs changing): The Mac wheels that the docs page purport to exist do not, in fact, exist:  Where are these wheels actually located?  Submit a pull request? I would submit a PR, but I am not aware of what the correct URLs are. The storage bucket does not have any index as far as I could tell.",2022-04-11T21:25:49Z,type:docs-bug stat:awaiting response type:build/install stale subtype:macOS,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55581,", Thank you for reporting.   For macOS (CPUonly) : All URL's are not working For Linux : Python 3.10 CPUonly URL is not working", what are the correct URLs for those wheels?,", I have submitted the fix internally which should reflect the updated changes. Please refer correct path for macOS Python 3.7  https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow2.8.0cp310cp310macosx_10_14_x86_64.whl Thanks!","Thanks ! What is the correct path for linux, python 3.10, CPU only?",", We have submitted fix internally for `Linux Python 3.10 CPU only` wheel. Thanks! ",", Now macOS wheel URL's are updated and are working. Please refer the same in below !image >Thanks ! What is the correct path for linux, python 3.10, CPU only? Path : https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.8.0cp310cp310manylinux2010_x86_64.whl Currently there is no wheel available for TF2.8. Thank you. ", Thanks! Is there a technical reason that the linux/python 3.10/CPUonly wheel does not exist?,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,not stale,", Sorry for the late response. `TF2.8` binaries were uploaded to the wrong bucket.  `TF2.8.1` is released. Could you try to use 2.8.1 wheel https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu2.8.1cp310cp310manylinux2010_x86_64.whl Thank you.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
1873,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Difference in Inference result in JAVA and Swift using same TFLite Model)ï¼Œ å†…å®¹æ˜¯ (I have trained a Mobile Bert Custom TFLite model for Text Classification and the model works as expected in Android. The output values match the expected values from running the TFLite model in Python. However, while running the same model in iOS with same input, I am getting different results.  I am new to iOS, this is the first time I'm working on iOS so my gut feeling is that I'm doing something wrong. The only clue I currently have is the slight difference in my Java Code and Swift Code to extract the result from Model Output. Java `float[][][] bert_output = new float[1][MAX_SEQ_LEN][BERT_HIDDEN_LAYERS];`  `bert_model.run(inputIds, bert_output); ` inputIds here is an Int Array In Java I directly call **run** in the interpreter and pass the Input in inputIds as well as the output array object and at the end of it I get a populated Output array in bert_output. Swift In Swift the inference seems to work a little different. You have to convert Your input(Int Array) to Tensor format and also fetch the output in Tensor format and convert it back to Float array. I feel this is the part where things are going wrong but no way to confirm. Convert Input into Tensor `let input_tensor = inputIds.withUnsafeBufferPointer(Data.init)` Copy Input to Model Input Layer 0 `try interpreter_bert?.copy(input, toInputAt: 0)` Inference `try interpreter_bert?.invoke()` Copy Output Tensor from Output Layer 0 `let temp_res = try interpreter_bert?.output(at: 0)` Convert Output Tensor Data to Float Array `let mResult = BertHelper.TensorToArray(tensor: temp_res!)` You can find the Conversion code here After the Co)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vipintom,Difference in Inference result in JAVA and Swift using same TFLite Model,"I have trained a Mobile Bert Custom TFLite model for Text Classification and the model works as expected in Android. The output values match the expected values from running the TFLite model in Python. However, while running the same model in iOS with same input, I am getting different results.  I am new to iOS, this is the first time I'm working on iOS so my gut feeling is that I'm doing something wrong. The only clue I currently have is the slight difference in my Java Code and Swift Code to extract the result from Model Output. Java `float[][][] bert_output = new float[1][MAX_SEQ_LEN][BERT_HIDDEN_LAYERS];`  `bert_model.run(inputIds, bert_output); ` inputIds here is an Int Array In Java I directly call **run** in the interpreter and pass the Input in inputIds as well as the output array object and at the end of it I get a populated Output array in bert_output. Swift In Swift the inference seems to work a little different. You have to convert Your input(Int Array) to Tensor format and also fetch the output in Tensor format and convert it back to Float array. I feel this is the part where things are going wrong but no way to confirm. Convert Input into Tensor `let input_tensor = inputIds.withUnsafeBufferPointer(Data.init)` Copy Input to Model Input Layer 0 `try interpreter_bert?.copy(input, toInputAt: 0)` Inference `try interpreter_bert?.invoke()` Copy Output Tensor from Output Layer 0 `let temp_res = try interpreter_bert?.output(at: 0)` Convert Output Tensor Data to Float Array `let mResult = BertHelper.TensorToArray(tensor: temp_res!)` You can find the Conversion code here After the Co",2022-04-11T10:20:18Z,stat:awaiting response stale comp:lite type:performance TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55574,Hi   Did you get a chance to look at bert_qa ios example which used MobileBert where model Interpreter is called? https://github.com/tensorflow/examples/blob/master/lite/examples/bert_qa/ios/BertQACore/Models/ML/BertQAHandler.swift Thanks.," , could you please share how do you build your custom  on Android? Passed week I have been working on integrating mobile BERT into Android and got several issues I haven't resolved yet. https://stackoverflow.com/questions/76889244/ifmobilebertmodelrequiresdatatypeint32isitpossibletorunonandroida ",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, Did you get a chance to look at bert_qa android example? Thanks.," , yes, I had checked it, but it didn't give me a clue for this task.  Further work shown for my task I need just a tokenizer / word segmentation. It is part of BERT, but I hadn't managed to make it work as a standalone function. I ended up with Python script which utilises ckiptransformers which is served via API.  Later this week I discovered yours  package under the hood also tokenize words before making translation (console logs shown it). Sadly, I haven't found open API to use this feature for Chinese text.  I solved my task, but I am happy to know more how to do it better with ondevice offline ML kit"
1874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Lite build for Android results in issue - class file not found)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version: master repo / nightly repo  Python version: 3.7.3  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): 5.1.1  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version: not using cuda  GPU model and memory: not using GPU **Describe the problem** I am trying to use custombuilt tensorflow lite AAR on model personalization example of official tensorflow repo from the link Before I change anything from the master repository, I followed the steps in this link to build Tensorflow Lite AAR with bazel. Actually the bazel build did not flawlessly work at once; I had to change the visibility option of 'op_resolver_internal' to public in tensorflow/lite/core/api/BUILD to successfully build the AAR. Anyways, when I substituted the tensorflowlite api with my built AARs, I get the following error: Error 1: android\transfer_api\src\main\java\org\tensorflow\lite\examples\transfer\api\LiteMultipleSignatureModel.java:57: error: cannot access InterpreterApi     this.interpreter.runSignature(inputs, outputs, ""load"");                     ^   class file for org.tensorflow.lite.InterpreterApi no)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jaemin-shin,Tensorflow Lite build for Android results in issue - class file not found,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version: master repo / nightly repo  Python version: 3.7.3  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): 5.1.1  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version: not using cuda  GPU model and memory: not using GPU **Describe the problem** I am trying to use custombuilt tensorflow lite AAR on model personalization example of official tensorflow repo from the link Before I change anything from the master repository, I followed the steps in this link to build Tensorflow Lite AAR with bazel. Actually the bazel build did not flawlessly work at once; I had to change the visibility option of 'op_resolver_internal' to public in tensorflow/lite/core/api/BUILD to successfully build the AAR. Anyways, when I substituted the tensorflowlite api with my built AARs, I get the following error: Error 1: android\transfer_api\src\main\java\org\tensorflow\lite\examples\transfer\api\LiteMultipleSignatureModel.java:57: error: cannot access InterpreterApi     this.interpreter.runSignature(inputs, outputs, ""load"");                     ^   class file for org.tensorflow.lite.InterpreterApi no",2022-04-11T05:26:28Z,stat:awaiting response type:build/install stale comp:lite subtype:bazel,closed,0,10,https://github.com/tensorflow/tensorflow/issues/55572,Hi shin ! Could you please share your command on bazel build  method too?,"**Hello! Here is the bazel command that I used:** bazel build c opt fat_apk_cpu=arm64v8a  host_crosstool_top=//tools/cpp:toolchain //tensorflow/lite/java:tensorflowlite **For ./configure in the tensorflow root directory, I configured as follows:** (base) jmshin:~/tensorflow > ./configure You have bazel 5.1.1 installed. Please specify the location of python. [Default is /home/jmshin/miniconda3/bin/python3]: Found possible Python library paths:   /home/jmshin/miniconda3/lib/python3.7/sitepackages Please input the desired Python library path to use.  Default is [/home/jmshin/miniconda3/lib/python3.7/sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: n No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: n No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: n Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""config=opt"" is specified [Default is Wnosigncompare]: O3 Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y Searching for NDK and SDK installations. Please specify the home path of the Android NDK to use. [Default is /home/jmshin/Android/Sdk/ndkbundle]: /home/jmshin/Android/Sdk/ndk/19.2.5345600 Please specify the (min) Android NDK API level to use. [Available levels: ['16', '17', '18', '19', '21', '22', '23', '24', '26', '27', '28']] [Default is 21]: Please specify the home path of the Android SDK to use. [Default is /home/jmshin/Android/Sdk]: Please specify the Android SDK API level to use. [Available levels: ['32']] [Default is 32]: Please specify an Android build tools version to use. [Available versions: ['30.0.3', '32.0.0', '32.1.0rc1']] [Default is 32.1.0rc1]: 30.0.3 Preconfigured Bazel build configs. You can use any of the below by adding ""config="" to your build command. See .bazelrc for more details.         config=mkl             Build with MKL support.         config=mkl_aarch64     Build with oneDNN and Compute Library for the Arm Architecture (ACL).         config=monolithic      Config for mostly static monolithic build.         config=numa            Build with NUMA support.         config=dynamic_kernels         (Experimental) Build kernels into separate shared objects.         config=v1              Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:         config=nogcp           Disable GCP support.         config=nonccl          Disable NVIDIA NCCL support. Configuration finished","Ok shin ! I think it is missing a dependency  of lite support in the build.gradle file.   Did you try after adding aar files to the project and local maven repository?  If that does not work , please try again after adding more architecture in bazel config and let us know the results. ` bazel build c opt fat_apk_cpu=arm64v8a armeabiv7a host_crosstool_top=//tools/cpp:toolchain //tensorflow/lite/java:tensorflowlite` Thanks!","Dear , Thank you for the suggestion! Unfortunately, it all did not work; I tried adding them by aar file, or adding them through local maven repository, and repeated all process with adding more architecture in bazel config. All was out of luck. I think the aar is well recognized by the Android Studio, since it does not show on error on `import org.tensorflow.lite.Interpreter;` but shows error on followings: Error 1: android\transfer_api\src\main\java\org\tensorflow\lite\examples\transfer\api\LiteMultipleSignatureModel.java:57: error: cannot access InterpreterApi this.interpreter.runSignature(inputs, outputs, ""load""); ^ class file for org.tensorflow.lite.InterpreterApi not found Error 2: android\transfer_api\src\main\java\org\tensorflow\lite\examples\transfer\api\LiteMultipleSignatureModel.java:105: error: cannot access Tensor return this.interpreter.getInputTensorFromSignature(""bottleneck"", ""train"").shape()[1]; ^ class file for org.tensorflow.lite.Tensor not found I also tried bazel build on release branch r2.8 as they should be stable, but the same error occurred...",Hi  ! Could you please look at this issue?,Hi ! May I ask if there's any update on this?! Have a nice day :) ,Hi shin  The model personalization example has been updated with commit https://github.com/tensorflow/examples/commit/7cb2b4dd5858c44cf8ab2c691bc646eaaf28713e Could you please try with latest version and let us know if you are still facing the issue? Thanks.,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
692,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bug in tf.saved_model.load(): 'NormalizeUTF8' not in self._op_def_cache)ï¼Œ å†…å®¹æ˜¯ (I load my model with just these two lines of code:  But I get the error below. HOWEVER! When I also import tensorflow_text, everything works:  I don't actually call anything from tensorflow_text. I believe when tensorflow_text is imported, it updates self._op_def_cache in the Graph class in `tensorflow/python/framework/ops.py`. I tested this on both Mac and Windows, in jupyter notebook and an IDE. Behavior is the same. The error: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,oksanatkach,Bug in tf.saved_model.load(): 'NormalizeUTF8' not in self._op_def_cache,"I load my model with just these two lines of code:  But I get the error below. HOWEVER! When I also import tensorflow_text, everything works:  I don't actually call anything from tensorflow_text. I believe when tensorflow_text is imported, it updates self._op_def_cache in the Graph class in `tensorflow/python/framework/ops.py`. I tested this on both Mac and Windows, in jupyter notebook and an IDE. Behavior is the same. The error: ",2022-04-10T21:09:50Z,type:support comp:ops,closed,2,2,https://github.com/tensorflow/tensorflow/issues/55568,"TensorFlow uses custom ops that can be implemented by other package, as it is not ideal to have TF repo be a monorepo of all possible implementation of ML models and the C++ kernels they need (for reasons including for example the question of who would maintain the code after it gets submitted, or for the reason of limiting the amount of dependencies that need to be present when compiling or users would need to install to use TF). Instead, if you need to use features implemented in custom ops, you have to import the corresponding package. In most cases this is done via the side effect of an import. Even if you don't use the imported name anywhere else in your code, the import also results in loading shared objects in the C++ runtime, some of which objects are just implementations of these kernels. This way, users that don't care about these custom ops (because their code doesn't use that) are not impacted and all other users can use these ops by just adding one single import or a minimal amount of code changes. This is similar to how we now have the cloud filesystem support: you would import tensorflow_io and get access to reading files from these filesystems, with minimal changes to your code."," makes sense, thank you! Hope this will help some lost soul out there."
467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix issue with tf.ragged.constant when empty pylist is provided)ï¼Œ å†…å®¹æ˜¯ (`tf.ragged.constant` when provided empty `pylist` is provided as an input , all RAM is consumed, causing the notebook to crash. Below is the reference issue: Fixes https://github.com/tensorflow/tensorflow/issues/55199)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,rthadur,Fix issue with tf.ragged.constant when empty pylist is provided,"`tf.ragged.constant` when provided empty `pylist` is provided as an input , all RAM is consumed, causing the notebook to crash. Below is the reference issue: Fixes https://github.com/tensorflow/tensorflow/issues/55199",2022-04-10T15:16:23Z,comp:ops size:XS prtype:bugfix,closed,0,10,https://github.com/tensorflow/tensorflow/issues/55567,"Please use a proper commit message / PR title, not `Update `. See https://cbea.ms/gitcommit/","> Please use a proper commit message / PR title, not `Update `. See https://cbea.ms/gitcommit/ Sorry , updated description.",Still see the same title. That would be the first line of the description after this gets merged.,"> Still see the same title. That would be the first line of the description after this gets merged. done, thank you",Can you check the build failures?,"Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor.","> Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor. I agree ,any idea how we can fix this issue ?",There needs to be a fix in C++ code where we check the number of elements in list/tensor and return a `Status` back to the user if it's 0.,"Apparently the fix was still in Python, but lower after the length of the list has been determined. See https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e",Thank you Mihai 
1870,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow ERROR - Testing a linear regression model using TensorFlow)ï¼Œ å†…å®¹æ˜¯ (I'm trying to resolve an error in tensoflow but I can't understand what the error is. I can't understand what the type [('resource', 'u1')] is and what that means. I am studying the book Mastering OpenCV 4 with Python by the author Alberto Fernandez Villan, but the code is outdated, I have already updated everything according to version 2.8 of tensorflow, but now this error appears. can you help me? Here is the complete code and the error. numpy version: 1.22.3 tensorflow version: 2.8.0  import numpy as np import tensorflow as tf import matplotlib.pyplot as plt tf.compat.v1.disable_eager_execution()  Number of points: N = 50  Make random numbers predictable: np.random.seed(101) tf.compat.v1.set_random_seed(101)  Generate random data composed by 50 (N = 50) points: x = np.linspace(0, N, N) float64 y = 3 * np.linspace(0, N, N) + np.random.uniform(10, 10, N) float64  Number of points to predict: M = 3  Define 'M' more points to get the predictions using the trained model: new_x = np.linspace(N + 1, N + 10, M) float64  Restore the model.  First step when loading a model is to load the graph from '.meta': tf.compat.v1.reset_default_graph() imported_meta = tf.compat.v1.train.import_meta_graph(""linear_regression.meta"")  The second step when loading a model is to load the values of the variables:  Note that values only exist within a session with tf.compat.v1.Session() as sess:     imported_meta.restore(sess, './linear_regression')      Run the model to get the values of the variables W, b and new prediction values:     W_estimated = sess.run('W:0')     b_estimated = sess.run('b:0')     new_pred)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,NathFarinha237,Tensorflow ERROR - Testing a linear regression model using TensorFlow,"I'm trying to resolve an error in tensoflow but I can't understand what the error is. I can't understand what the type [('resource', 'u1')] is and what that means. I am studying the book Mastering OpenCV 4 with Python by the author Alberto Fernandez Villan, but the code is outdated, I have already updated everything according to version 2.8 of tensorflow, but now this error appears. can you help me? Here is the complete code and the error. numpy version: 1.22.3 tensorflow version: 2.8.0  import numpy as np import tensorflow as tf import matplotlib.pyplot as plt tf.compat.v1.disable_eager_execution()  Number of points: N = 50  Make random numbers predictable: np.random.seed(101) tf.compat.v1.set_random_seed(101)  Generate random data composed by 50 (N = 50) points: x = np.linspace(0, N, N) float64 y = 3 * np.linspace(0, N, N) + np.random.uniform(10, 10, N) float64  Number of points to predict: M = 3  Define 'M' more points to get the predictions using the trained model: new_x = np.linspace(N + 1, N + 10, M) float64  Restore the model.  First step when loading a model is to load the graph from '.meta': tf.compat.v1.reset_default_graph() imported_meta = tf.compat.v1.train.import_meta_graph(""linear_regression.meta"")  The second step when loading a model is to load the values of the variables:  Note that values only exist within a session with tf.compat.v1.Session() as sess:     imported_meta.restore(sess, './linear_regression')      Run the model to get the values of the variables W, b and new prediction values:     W_estimated = sess.run('W:0')     b_estimated = sess.run('b:0')     new_pred",2022-04-09T21:09:24Z,stat:awaiting response stale type:others TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55564,"Updated training code that works. Required for the test code that shows an error to work.   Training a linear regression model using TensorFlow import numpy as np import tensorflow as tf import matplotlib.pyplot as plt tf.compat.v1.disable_eager_execution()  sugestÃ£o encontrada na internet para evitar erro  Path to the folder that we want to save the logs for Tensorboard: logs_path = ""./logs""  Number of points: N = 50  Make random numbers predictable: np.random.seed(101) tf.compat.v1.set_random_seed(101)  Generate random data composed by 50 (N = 50) points: x = np.linspace(0, N, N) y = 3 * np.linspace(0, N, N) + np.random.uniform(10, 10, N)  You can check the shape of the created training data: print(x.shape) print(y.shape)  Create the placeholders in order to feed our training examples into the optimizer while training: X = tf.compat.v1.placeholder(""float"", name='X') Y = tf.compat.v1.placeholder(""float"", name='Y')  Declare two trainable TensorFlow Variables for the Weights and Bias  We are going to initialize them randomly. Another way can be to set '0.0': W = tf.Variable(np.random.randn(), name=""W"") b = tf.Variable(np.random.randn(), name=""b"")  Define the hyperparameters of the model: learning_rate = 0.01 training_epochs = 1000  This will be used to show results after every 25 epochs: disp_step = 100  Construct a linear model: y_model = tf.add(tf.multiply(X, W), b, name=""y_model"")  Define cost function, in this case, the Mean squared error  (Note that other cost functions can be defined) cost = tf.reduce_sum(tf.pow(y_model  Y, 2)) / (2 * N)  Create the gradient descent optimizer that is going to minimize the cost function modifying the  values of the variables W and b: optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(cost)  Initialize all variables: init = tf.compat.v1.global_variables_initializer()  Create a Saver object: saver = tf.compat.v1.train.Saver()  Start the training procedure inside a TensorFlow Session: with tf.compat.v1.Session() as sess:      Run the initializer:     sess.run(init)      Uncomment if you want to see the created graph      summary_writer = tf.summary.FileWriter(logs_path, sess.graph)      Iterate over all defined epochs:     for epoch in range(training_epochs):          Feed each training data point into the optimizer:         for (_x, _y) in zip(x, y):             sess.run(optimizer, feed_dict={X: _x, Y: _y})          Display the results every 'display_step' epochs:         if (epoch + 1) % disp_step == 0:              Calculate the actual cost, W and b:             c = sess.run(cost, feed_dict={X: x, Y: y})             w_est = sess.run(W)             b_est = sess.run(b)             print(""epoch {}: cost = {} W = {}  b = {}"".format(epoch + 1, c, w_est, b_est))      Save the final model     saver.save(sess, './linear_regression')      Storing necessary values to be used outside the session     training_cost = sess.run(cost, feed_dict={X: x, Y: y})     weight = sess.run(W)     bias = sess.run(b) print(""Training finished!"")  Calculate the predictions: predictions = weight * x + bias  Create the dimensions of the figure and set title: fig = plt.figure(figsize=(8, 5)) plt.suptitle(""Linear regression using TensorFlow"", fontsize=14, fontweight='bold') fig.patch.set_facecolor('silver')  Plot training data: plt.subplot(1, 2, 1) plt.plot(x, y, 'ro', label='Original data') plt.xlabel('x') plt.ylabel('y') plt.title(""Training Data"")  Plot results: plt.subplot(1, 2, 2) plt.plot(x, y, 'ro', label='Original data') plt.plot(x, predictions, label='Fitted line') plt.xlabel('x') plt.ylabel('y') plt.title('Linear Regression Result') plt.legend()  Show the Figure: plt.show()","Hello  , On running the given code snippet, I am facing an error stating `OSError: File does not exist. Received: {filename}`. Please find the gist of it here.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
805,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build failed: undeclared inclusion(s) in rule '@llvm-project//mlir:GPUTransforms')ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code: No  OS Platform and Distribution: Ubuntu 20.04.4 LTS x86_64  TensorFlow version: Latest commit `55645ca`  Python version: 3.10.4  Bazel version (if compiling from source): 5.1.1  GCC/Compiler version (if compiling from source): 9.4.0  CUDA/cuDNN version: No CUDA  GPU model and memory: No GPU I am trying to build Tensorflow with TPU support on a Cloud TPU VM. I have successfully built v2.8.0 before, but now it shows an error:  I am willing to provide more information on this issue.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ayaka14732,Build failed: undeclared inclusion(s) in rule '@llvm-project//mlir:GPUTransforms',"**System information**  Have I written custom code: No  OS Platform and Distribution: Ubuntu 20.04.4 LTS x86_64  TensorFlow version: Latest commit `55645ca`  Python version: 3.10.4  Bazel version (if compiling from source): 5.1.1  GCC/Compiler version (if compiling from source): 9.4.0  CUDA/cuDNN version: No CUDA  GPU model and memory: No GPU I am trying to build Tensorflow with TPU support on a Cloud TPU VM. I have successfully built v2.8.0 before, but now it shows an error:  I am willing to provide more information on this issue.",2022-04-09T17:02:04Z,stat:awaiting response type:build/install comp:tpus subtype:bazel,closed,6,12,https://github.com/tensorflow/tensorflow/issues/55563,I am experiencing a similar problem.  **System information**  Have I written custom code: No  OS Platform and Distribution: Arch Linux x86_64 (rolling release)  TensorFlow version: master branch @ c44d14f2194  Python version: 3.10.4  Bazel version (if compiling from source): 5.1.1  GCC/Compiler version (if compiling from source): Clang 13.0.1  Compiling without GPU support,"I am seeing same, compiling with GPU support.",", Looks like Bazel version, can you try with Bazel 5.0.0 Version. Thanks!","> , Looks like Bazel version, can you try with Bazel 5.0.0 Version. Thanks! I manually changed `.bazelversion` to 5.0.0 and built with Bazel 5.0.0, but there are even more build errors.","I met the same error, but it is automatically fixed after I clean up the bazel cache.", Thanks! This also works for me.  Is this a Bazel bug or a Tensorflow bug?,", Can you try to clear the bazel cache before building Tensorflow `bazel clean expunge`"," As I replied before, I tried to clean the cache and it works for me.",", Glad that its resolved. Can we close this issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No,"Hi, I had a similar issue but `bazel clean expunge` did not fix my problem. Instead I had to use the workaround below I had my env variables set CC=/usr/bin/clang CXX=/usr/bin/clang++ But bazel (v5.1.1) and tensorflow (93b2ef6e27c63f48f7bab67a630ee226b78e74c6) have some issues when compiling the with clang: https://github.com/bazelbuild/bazel/issues/15359 To fix this issue I cleared my CC and CXX env variables and compiled tensorflow with gcc9",add `spawn_strategy=sandboxed` solved this problem for me
649,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Question: how to implement tf.gather with some other tf ops ?)ï¼Œ å†…å®¹æ˜¯ (I am trying to implement tf.gather with some simple and common tf ops, because tf.gather is not supported in the ncnn deployment framework currently, for now I can only think of implementing it with numpy:   If I don't convert the type of tensor to numpy by `params = params.numpy()`, it will reports error:  So how to implement tf.gather with some other tf ops and  without numpy, thanks in advance !)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,wwdok,Question: how to implement tf.gather with some other tf ops ?,"I am trying to implement tf.gather with some simple and common tf ops, because tf.gather is not supported in the ncnn deployment framework currently, for now I can only think of implementing it with numpy:   If I don't convert the type of tensor to numpy by `params = params.numpy()`, it will reports error:  So how to implement tf.gather with some other tf ops and  without numpy, thanks in advance !",2022-04-09T05:37:09Z,stat:awaiting response stale type:others comp:ops TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55560,"or please tell me where is the minimal c++ implementation code of tf.gather, thanks !", Could you please have a look at this link to get more details on Gather and let us know if it helps?Thanks!," I'd rather know the codelevel implementation than the superficial documentation. I notice the gather_op.cc and gather_functor.cc, are they the lowlevel implementation of tf.gather ?"," I was able to reproduce this issue on colab using TF v2.7.0 , 2.8.0 and tfnightly ,please find the gist here.Thanks!",", Hi, Are you looking for https://github.com/tensorflow/tensorflow/blob/aa64f94acc83889d5e165d14f2e5c1dbe71bef3b/tensorflow/python/ops/array_ops.pyL5066",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,I got the same issue. Any help please!!
420,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add DEVICE_DEFAULT registration for DataFormatDimMap and DataFormatVecPermute)ï¼Œ å†…å®¹æ˜¯ (`DataFormatDimMap` and `DataFormatVecPermute` are necessary for pluggable devices with the `GPU` type that want to leverage existing grappler optimizations.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,PatriceVignola,Add DEVICE_DEFAULT registration for DataFormatDimMap and DataFormatVecPermute,`DataFormatDimMap` and `DataFormatVecPermute` are necessary for pluggable devices with the `GPU` type that want to leverage existing grappler optimizations.,2022-04-08T21:14:49Z,ready to pull size:S,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55558, Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Bumping this to hopefully get some eyes on it. `DataFormatDimMap` and `DataFormatVecPermute` are necessary for pluggable devices with the `GPU` type that want to leverage existing grappler optimizations. Thanks!
1021,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (1280 != 0))ï¼Œ å†…å®¹æ˜¯ ( 1. System information OS: Windows 10: TensorFlow: 2.7.1:  2. Code python version : 3.7.13  / 64 bit  Code used to create and convert transfer learning tflite model   Android Dependency: implementation 'org.tensorflow:tensorflowlite:2.7.0' I have to specify that the inputs map will load an object of size [1][32][32][3] and outputs of [1][1280] as i've seen while debugging. Android method called:  Error got :  Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (1280 != 0)     Node number 70 (RESHAPE) failed to prepare. Node 70 (the reshape one) from netron visualization: !image !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rares926,Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (1280 != 0), 1. System information OS: Windows 10: TensorFlow: 2.7.1:  2. Code python version : 3.7.13  / 64 bit  Code used to create and convert transfer learning tflite model   Android Dependency: implementation 'org.tensorflow:tensorflowlite:2.7.0' I have to specify that the inputs map will load an object of size [1][32][32][3] and outputs of [1][1280] as i've seen while debugging. Android method called:  Error got :  Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (1280 != 0)     Node number 70 (RESHAPE) failed to prepare. Node 70 (the reshape one) from netron visualization: !image !image,2022-04-08T10:57:11Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.7,closed,0,11,https://github.com/tensorflow/tensorflow/issues/55551,"Hi  ! Sorry for the late response. I was getting **shape errors while saving the model** and getting an **empty array** while executing  **from the get_input_tensor details command**.  ! Could you please look at this issue?  Attaching gist in 2.6.3 , 2.7.1 , 2.8 and nightly  for reference . Thanks!"," is it possible to also add code that shows TFLite inference using the `tf.lite.Interpreter`? Refer to this code. If you are using samples images, please upload them as well.",Do you mean this one ?  ,https://www.dropbox.com/s/coeixr4kh8ljw6o/cifar10.zip?dl=1     > the dataset i use can be downloaded from here ,Could you provide a colab where you convert the model and run inference on it using the Python interpreter? ,You can find the colab here colab .  My questions are the following .  >  Firstly why is the Interpreter crashing when i try to add the extract function?  >  Secondly why is the load output on the test_image an array of zeros ?  Is it possible for the feature extractor to extract nothing from an image??,Any idea? I got the same problem.,"Hi, After looking into your code, you have defined  `BATCH_SIZE` as `32`, but you have not used it anywhere in your model.  Could you please define the batch size in your model.  If the loaded model in TFLite does not have any defined batch size, Converter will take the batch size as 1, and when you evaluate it with the different batch size, you are likely to end up with the problem which you are facing. If defining batch_size in your model does not work, try reshaping the input tensor while doing shape inference.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1637,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.io.decode_image(img, channels=1) has different results for a RGB image and a RGBA image where A is set to 255. )ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Build 19044  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  /  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.3.0  Python version: 3.8.8  Bazel version (if compiling from source): /  GCC/Compiler version (if compiling from source): /  CUDA/cuDNN version: /  GPU model and memory: / **Describe the current behavior** `tf.io.decode_image(img, channels=1)` should convert the input image to grayscale (see link), the results for an RGB image and a RGBA image where the A dimensions is just 255 should be the same but they are not. `tf.image.rgb_to_grayscale(img)` also does not yield the same result as the `decode_image` version.  **Describe the expected behavior** It seems to me that both the RGB and the RGBA image (with A=255) should yield the same result when using `decode_image `with channels=1?  The output from `rgb_to_grayscale`  is also different which is odd, shouldn't the implementation be very similar? **Standalone code to reproduce the issue** Example image and notebook in attachment: decode_image_bug.zip **Other info / logs**  /)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,StefRommes,"tf.io.decode_image(img, channels=1) has different results for a RGB image and a RGBA image where A is set to 255. ","**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Build 19044  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  /  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.3.0  Python version: 3.8.8  Bazel version (if compiling from source): /  GCC/Compiler version (if compiling from source): /  CUDA/cuDNN version: /  GPU model and memory: / **Describe the current behavior** `tf.io.decode_image(img, channels=1)` should convert the input image to grayscale (see link), the results for an RGB image and a RGBA image where the A dimensions is just 255 should be the same but they are not. `tf.image.rgb_to_grayscale(img)` also does not yield the same result as the `decode_image` version.  **Describe the expected behavior** It seems to me that both the RGB and the RGBA image (with A=255) should yield the same result when using `decode_image `with channels=1?  The output from `rgb_to_grayscale`  is also different which is odd, shouldn't the implementation be very similar? **Standalone code to reproduce the issue** Example image and notebook in attachment: decode_image_bug.zip **Other info / logs**  /",2022-04-08T08:23:40Z,stat:awaiting response type:bug stale comp:ops TF 2.3,closed,0,17,https://github.com/tensorflow/tensorflow/issues/55550," I tried to replicate this issue on colab using TF v2.8.0 ,could you please find the gist and confirm the same? Please refer to this thread as well. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.," the gist has an issue in the visualisations that I didn't have locally. It can be solved by wrapping the input for `plt.imshow` with `tf.squeeze`.  For example: `plt.imshow(imDecodedRGBgray, cmap='gray', vmin=0, vmax=255)`  Would become: `plt.imshow(tf.squeeze(imDecodedRGBgray), cmap='gray', vmin=0, vmax=255)` For your convenience a gist where this is solved, I also added back in interesting comments.  gist", Thank you for the update! Could you please move this issue to closed status if it is resolved? Thanks!," this isn't solved, the bug/quirk in Tensorflow persists, I just patched a small error in your gist so you can easier replicate.","Hi  for reporting this issue. Looks like its intended behaviour.  Take a look at below test scenarios. **Decoding them in 3 channels/dimensions**   **Comparing them, they are equal as expected** `>>>print(tf.reduce_all(tf.equal(imDecodedRGB.shape, imDecodedRGBA.shape)))` `tf.Tensor(True, shape=(), dtype=bool)` **Decoding them in 1 channel, changing to grayscale see link)**   **Comparing them, they are equal  as expected** `>>>print(tf.reduce_all(tf.equal(imDecodedRGBgray.shape, imDecodedRGBAgray.shape)))` `tf.Tensor(True, shape=(), dtype=bool)`",Hi  thank for taking the time to answer. The dimensions are indeed the same but the contents are not! They are also supposed to be the same.,"Also make sure to use the RGBA image provided, with just an RGB the issue doesn't appear.",", Content appears to be same  When i tried to see the images on Tensorboard      ",Visually there are very minimal to no differences but numerical there are.,> tf.image.rgb_to_grayscale(img) also does not yield the same result as the decode_image version. Are you sure that the weights are exactly the same? As `decode_image` in your case is `libpng`: https://github.com/glennrp/libpng/blob/master/pngrtran.cL1013L1049 Instead `tf.image.rgb_to_grayscale(img)` is: https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/image_ops_impl.pyL2514,"Interesting, that is probably the answer to the second part of the question! Thanks a lot! Maybe the first part can also be answered by finding a discrepancy in `libpng`. Will look into it!",> Maybe the first part can also be answered by finding a discrepancy in libpng It is `png_set_strip_alpha` in `libpng` https://github.com/tensorflow/tensorflow/blob/b0fcd5091c83256e05213d16d8f3c26c19097dfa/tensorflow/core/lib/png/png_io.ccL255L268 I suppose this is your case right?  https://github.com/glennrp/libpng/blob/master/libpng.3L1595L1598,", Did you get a chance to look at  comments. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1266,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Dataset has not correct shape)ï¼Œ å†…å®¹æ˜¯ (Hi,    I think the issue here is from the dataset, which doesn't populate the shape info for the sequence dim.   It is becouse print(dataset) gives me: ` `. The model knows from the dataset object only the last two axis, but I need to it knows the last 3 axis.  The shape must be defined for example: `shape=(None, 7, 25, 81)` or `shape=(None, 7, 25, 1)`. Thanks.   System information  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12.3 / Debian 5.10.701 (20210930)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0 / 2.9.0dev20220329  Python version: Python 3.9.7 / Python 3.9.2  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source): Clang 11.1.0 / GCC 10.2.1  CUDA/cuDNN version: No  GPU model and memory: Apple M1, 16GB / No  Code )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,markub3327,TensorFlow Dataset has not correct shape,"Hi,    I think the issue here is from the dataset, which doesn't populate the shape info for the sequence dim.   It is becouse print(dataset) gives me: ` `. The model knows from the dataset object only the last two axis, but I need to it knows the last 3 axis.  The shape must be defined for example: `shape=(None, 7, 25, 81)` or `shape=(None, 7, 25, 1)`. Thanks.   System information  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12.3 / Debian 5.10.701 (20210930)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0 / 2.9.0dev20220329  Python version: Python 3.9.7 / Python 3.9.2  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source): Clang 11.1.0 / GCC 10.2.1  CUDA/cuDNN version: No  GPU model and memory: Apple M1, 16GB / No  Code ",2022-04-08T05:27:10Z,stat:awaiting response type:bug stale comp:ops comp:data TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/55546,"Hello  , I tried to execute the given code snippet, I am facing different error stating. Please find the gist of it here.",  It's the same error as I saw.   The problem is with `print(dataset)` that gives me:   ` ` And I need to propagate the last 3 dims from Dataset to model during build process. More info about this you found here: https://github.com/kerasteam/tfkeras/issues/114," , I was able to reproduce the issue in tf v2.7, v2.8 and nightly.Please find the gist here.","It looks like `tf.keras.preprocessing.timeseries_dataset_from_array` builds the sequences in such a way that their length can't be inferred automatically. If the second dimension is guaranteed to be equal to the sequence length, you can use ensure_shape to communicate the knowledge to the shape inference system:   would it make sense for `timeseries_dataset_from_array` to call `ensure_shape` for the elements produced by `timeseries_dataset_from_array`?", Is it a true solution? Can I rely this solution? I need it know with academical precision. Thanks., Can you please create an issue in Keras repo as all the Keras issues are tracked separately in keras repo. Thanks!,"  In Keras this issue exists, it's here. Please reopen it. Thanks.",  I reopened the issue. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
812,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Converter for LogicalNot operation)ï¼Œ å†…å®¹æ˜¯ ( Converter for LogicalNot operation is using the same base class (ConvertUnaryImpl) as other Unary operations.  The special converter for Rsqrt was removed and now for this operation regular Unary Op converter is used.  New templated class implemented for testing ConvertUnary, ConvertBooleanUnary, ConvertActivation.  A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any Unary and UnaryBoolean operation. (Similar subtest for ConvertActivation operations is blocked and after refactoring of ConvertActivation it will be activated))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,drivanov,Converter for LogicalNot operation," Converter for LogicalNot operation is using the same base class (ConvertUnaryImpl) as other Unary operations.  The special converter for Rsqrt was removed and now for this operation regular Unary Op converter is used.  New templated class implemented for testing ConvertUnary, ConvertBooleanUnary, ConvertActivation.  A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any Unary and UnaryBoolean operation. (Similar subtest for ConvertActivation operations is blocked and after refactoring of ConvertActivation it will be activated)",2022-04-07T23:22:50Z,awaiting review ready to pull size:L comp:gpu:tensorrt,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55542,Replacement for PR CC(Converter for LogicalNot operation),"still seeing the same error: In file included from third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:16: In file included from ./third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26: In file included from ./third_party/tensorflow/compiler/tf2tensorrt/convert/op_converter.h:25: In file included from ./third_party/tensorflow/compiler/tf2tensorrt/convert/weights.h:22: ./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:98:30: error: no matching function for call to 'DebugString'     StrAppend(&tmp_s, StrCat(DebugString(el), "", ""));                              ^~~~~~~~~~~ third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:1706:58: note: in instantiation of function template specialization 'tensorflow::tensorrt::DebugString' requested here                        ' requested here       AddTestTensor(""input"", p.input_dims, input_tf_type, input_values);       ^ third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:7057:3: note: in instantiation of function template specialization 'tensorflow::tensorrt::convert::OpConverter_UnaryTest::RunTests' requested here   RunTests(""LogicalUnary"", ops_to_test, *UnaryBooleanOperationMap(), op_map,   ^ ./third_party/tensorflow/compiler/tf2tensorrt/convert/trt_parameters.h:46:8: note: candidate function not viable: no known conversion from 'const std::__bit_iterator, true, 0>::reference' (aka 'const std::__bit_const_reference>') to 'const tensorflow::tensorrt::TrtPrecisionMode' for 1st argument string DebugString(const TrtPrecisionMode mode);        ^ ./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:95:8: note: candidate template ignored: could not match 'vector' against '__bit_const_reference' string DebugString(const std::vector& vector) {        ^ ./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:86:8: note: candidate template ignored: requirement 'std::is_arithmetic>>>::value' was not satisfied [with CType = std::__bit_const_reference>] string DebugString(const CType& el) {        ^ 1 error generated.", : It looks like **bool**'s are represented differently by the compilers we are using. After replacement **bool** by **int** the compilation should be fine.,I am fine with change bool in the test or having another print method.,Test failure: [] 3 tests from OpConvTestInstantiation/OpConverter_BOOL_Test [ RUN      ] OpConvTestInstantiation/OpConverter_BOOL_Test.ConvertBoolean/0 F0418 09:07:39.785729    2094 convert_nodes_test.cc:1202] Cannot create tensor with type bool *** Check failure stack trace: ***     @     0x560d281e6d38  absl::log_internal::LogMessage::SendToLog()     @     0x560d281e6602  absl::log_internal::LogMessage::Flush()     @     0x560d281e6fc9  absl::log_internal::LogMessageFatal::~LogMessageFatal()     @     0x560d1c9e4f49  tensorflow::tensorrt::convert::OpConverterTest::AsTensor()     @     0x560d1c967556  tensorflow::tensorrt::convert::OpConverterTest::AddTestWeights()     @     0x560d1ca11c03  tensorflow::tensorrt::convert::OpConverter_UnaryTest::runExpectedToFailTest()     @     0x560d1c9b39a7  tensorflow::tensorrt::convert::OpConverter_UnaryTest::RunTests()     @     0x560d1c9b334d  tensorflow::tensorrt::convert::OpConverter_BOOL_Test_ConvertBoolean_Test::TestBody()     @     0x560d1cb684a2  testing::Test::Run()     @     0x560d1cb6972f  testing::TestInfo::Run()     @     0x560d1cb6a495  testing::TestSuite::Run()     @     0x560d1cb7b69c  testing::internal::UnitTestImpl::RunAllTests()     @     0x560d1cb7abfb  testing::UnitTest::Run()     @     0x560d1cb3a8d3  main     @     0x7f75d3dbd8d3  __libc_start_main     @     0x560d1c93002a  ../sysdeps/x86_64/start.S:120 _start,: What version of TRT are you using? It looks like DT_BOOL is not fully supported for TRT < 8.2.  I added  statements in two places (in `ConverBooleanUnary::Validate()` and in `ConvertBoolean` test). I hope this will solve the problem.," : Could you, please try with the latest changes?"
1467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(imdeu.htmi)ï¼Œ å†…å®¹æ˜¯ ( 1. System information git adt OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github gtr SHA, if built comdlim t):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,mdmilm,imdeu.htmi," 1. System information git adt OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github gtr SHA, if built comdlim t):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-04-07T21:51:42Z,type:others TFLiteConverter TF 2.3,closed,0,1,https://github.com/tensorflow/tensorflow/issues/55538,Hi  ! Closing this issue as it is duplicated to CC(Alom) . Thanks!
1452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Alom)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,mdmilm,Alom," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  TensorFlow installation (pip package or built from source):  TensorFlow library (version, if pip package or github SHA, if built from source):  2. Code Provide code to help us reproduce your issues using one of the following options:  Option A: Reference colab notebooks 1)  Reference TensorFlow Model Colab: Demonstrate how to build your TF model. 2)  Reference TensorFlow Lite Model Colab: Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).   Option B: Paste your code here or provide a link to a custom endtoend colab   3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong:  Model produces wrong results and/or has lesser accuracy.  Model produces correct results, but it is slower than expected.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-04-07T21:39:07Z,stat:awaiting response type:others TFLiteConverter TF 2.3,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55537,Hi  ! I see the TFLite converter notebook  attached here got 97% accuracy during evaluation. Could you point out the exact issue? Thanks!,Template not filled.
1000,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XNNPack delegate support for quantized models, no latency improvement)ï¼Œ å†…å®¹æ˜¯ (I built tensorflow with  define tflite_with_xnnpack=true define xnn_enable_qs8=true to have acceleration for quantized models. But I don't gain any improvement in the latency of the quantized models compared to when I build tensorflow with define tflite_with_xnnpack=true alone. I tested both in windows and linux. I am testing on desktop with intel CPU.  Is there anything I am doing wrong? I tested with trained models too. I used the following code to quantize the model.: ` **System information**  OS: Windows 10 Pro  Desktop, Intel(R) Core(TM) i79700K CPU   Tensorflow 2.9 built from source (tested with tensoflow 2.8 built from source on linux too)  python 3.10  Bazel 5.1.0  Did not include gpu support on the tensorflow build in windows)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Soleimani64,"XNNPack delegate support for quantized models, no latency improvement","I built tensorflow with  define tflite_with_xnnpack=true define xnn_enable_qs8=true to have acceleration for quantized models. But I don't gain any improvement in the latency of the quantized models compared to when I build tensorflow with define tflite_with_xnnpack=true alone. I tested both in windows and linux. I am testing on desktop with intel CPU.  Is there anything I am doing wrong? I tested with trained models too. I used the following code to quantize the model.: ` **System information**  OS: Windows 10 Pro  Desktop, Intel(R) Core(TM) i79700K CPU   Tensorflow 2.9 built from source (tested with tensoflow 2.8 built from source on linux too)  python 3.10  Bazel 5.1.0  Did not include gpu support on the tensorflow build in windows",2022-04-07T04:09:38Z,stat:awaiting response stale type:performance comp:lite-xnnpack TF 2.8,closed,0,15,https://github.com/tensorflow/tensorflow/issues/55525,"Hi  ! According to this document, Limited support for operators with asymmetric quantization is available via the define xnn_enable_qu8=true Bazel option. So **""define tflite_with_xnnpack=true define xnn_enable_qs8=true""** is  correct command to build with bazel here . Attaching relevant thread for reference. Thanks!","Hi   Thanks for the reply. but I used  ""**define tflite_with_xnnpack=true define xnn_enable_qs8=true**"" to build tensorflow. I did **not** use define xnn_enable_qu8=true"," To make it clear, the problem here is that I was expecting to see lower latency for quantized models when I use ""define xnn_enable_qs8=true"" option. But the latency doesn't change at all. I was wondering if you can give me an example when ""define xnn_enable_qs8=true"" actually reduces the latency of a quantized model.  I assumed that with this option we can use fixedpoint operations on a desktop cpu. Is that correct?","Sorry  ! According to this document, **BatchNormalization** is not supported operator yet in XNNPack yet. I think you have to stay with **define tflite_with_xnnpack=true** for now to use the default XNNPack engine . Thanks!","Thanks   I tried without BatchNormalization, the latency became a little bit smaller with define xnn_enable_qs8=true, but the difference is very small. I will try with bigger models. However, if I don't quantize the model, the latency is always much smaller than the quantized model! How about my other question: does tensorflow do 8bit operations when we activate define xnn_enable_qs8=true, or it still uses the floating point (I mean on a desktop cpu)?",Hi  ! Could you please look at this issue?, use `define tflite_with_xnnpack_qs8=true` and/or `define tflite_with_xnnpack_qu8=true` Bazel flags. See XNNPack delegate documentation for details.," I did that, it doesn't improve the latency.","Do you use the latest version of TensorFlow Lite? Do you see ""Created TensorFlow Lite XNNPACK delegate for CPU."" message when you run the model?"," , yes to both questions.  I can see improvement in the latency of both tf32 and int8 models when comparing XNN pack with regular tensorflow. but I don't see any difference in the latency of the int8 models if I use define tflite_with_xnnpack_qs8=true and define tflite_with_xnnpack_qu8=true compared to when I don't use them.","> Do you use the latest version of TensorFlow Lite? Do you see ""Created TensorFlow Lite XNNPACK delegate for CPU."" message when you run the model? Actually I encountered the same issue. I can see ""Created Tensorflow Lite XNNPACK delegate for CPU"" but the thing is that when running on the floating model, there are multiple lines of ""Created Tensorflow Lite XNNPACK ..."", for the int8 model, there is only one such line printed. And, the quantized (int8) model obtains much more latency than the floating point model. Another thing is that I enabled ""enable_op_profiling"" when I run the tflite model on the Android phone (with xnnpack being enabled). The results show that for the floating point model, the node type is ""TfLiteXNNPackDelegate"" in the ""Run Order"" section, however, for the quantized model, the node type keeps the original operators, like SUB, CONV_2D, ADD, etc in the ""Run Order"" section. From this perspective, looks like xnnpack has not been applied on the quantized model successfully in my case. I linked the detail log here https://drive.google.com/drive/folders/1bsFfjtI3V0iVyvH4IhcmCZZXugmOkCvH?usp=sharing Please help!",Hi could you please compile the package with debug mode with something like below to get more verbose logs so that we can investigate further on the issue. `bazel build c dbg copt=g  jobs=8 //tensorflow/tools/pip_package:build_pip_package`,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1006,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hi)ï¼Œ å†…å®¹æ˜¯ (Hi I've been trying to train tflite pose classifiers to work with your movenet pi example https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi I've used the suggested tutorial and colab  https://www.tensorflow.org/lite/tutorials/pose_classification and carefully followed the example yoga pose dataset (with train and test examples) but the classification results seem almost random (2 classifiers out 4 successfully classify a pose about 50% of the time). My dataset is here https://github.com/roddicki/movenetpidataset The training in the colab works well with a confusion matrix with no errors. The model accuracy is also high (0.9977) and the increasing accuracy through the training epochs looks as expected. I wondered if anyone has used this colab with success? Thanks!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,roddicki,Hi,Hi I've been trying to train tflite pose classifiers to work with your movenet pi example https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi I've used the suggested tutorial and colab  https://www.tensorflow.org/lite/tutorials/pose_classification and carefully followed the example yoga pose dataset (with train and test examples) but the classification results seem almost random (2 classifiers out 4 successfully classify a pose about 50% of the time). My dataset is here https://github.com/roddicki/movenetpidataset The training in the colab works well with a confusion matrix with no errors. The model accuracy is also high (0.9977) and the increasing accuracy through the training epochs looks as expected. I wondered if anyone has used this colab with success? Thanks!,2022-04-06T23:02:59Z,comp:lite type:others,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55521,Hi. Please use TF forum for these types of questions. GitHub is for bugs related to the code or feature requests.,Ok thanks
592,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow for M1 mac)ï¼Œ å†…å®¹æ˜¯ (I have a macbook air m1. I am trying to use tensorflow (for darkflow) in my mac. I need to get tensorflow 1.13.2 but brew and conda always installs the latest version. Now it gives me error ' tensorflow.contrib' not found as it was removed in 1.14.  I am importing this: 'from darkflow.net.build import TFNet' Could someone please explain what commands to replace for the new version.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,akshatmanohar21,Tensorflow for M1 mac,I have a macbook air m1. I am trying to use tensorflow (for darkflow) in my mac. I need to get tensorflow 1.13.2 but brew and conda always installs the latest version. Now it gives me error ' tensorflow.contrib' not found as it was removed in 1.14.  I am importing this: 'from darkflow.net.build import TFNet' Could someone please explain what commands to replace for the new version.,2022-04-06T21:39:03Z,stat:awaiting response type:build/install stale subtype:macOS,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55519,", You can follow commands below to install Tensorflow=1.13.2 ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom Gradient for Sparse Weight Tensors)ï¼Œ å†…å®¹æ˜¯ (I am trying to create a custom layer which computes W\*x + b where W is a sparse tensor. It is important that I don't ever form the dense version of W because it would be too large to store in memory. It is my understanding that the computation of W\*x, using `tf.sparse.sparse_dense_matmul(W, x),` does not have a supported gradient. Is such a gradient expected to be supported anytime soon? It seems that many others would want this functionality as well. To make this work, I am trying to implement a custom gradient using the following code:     .custom_gradient           def sparse_weight_multiply(self, w):     	      compute the product sparse_W * inputs, where sparse_W is a sparse tensor formed from the entries in w               self.sparse_W = tf.sparse.SparseTensor(self.indices, w, self.shape)               w_inputs = tf.sparse.sparse_dense_matmul(self.sparse_W, self.inputs)                define gradient for this function               def sparse_weight_grad(upstream_grad):                   '''                   upstream_grad is the gradient computed thus far in the computational graph.                    The output of this function will be the gradient of the function sparse_weight_multiply                    times upstream_grad, due to the product rule in differentiation.                    '''                    check the shape of upstream_grad                   print(""Shape of upstream grad: {}"".format(upstream_grad.shape))                   print(""Shape of inputs: {}"".format(inputs.shape))                   print(""Shape of weight: {}"".format(self.shape))                    map)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jezvonek,Custom Gradient for Sparse Weight Tensors,"I am trying to create a custom layer which computes W\*x + b where W is a sparse tensor. It is important that I don't ever form the dense version of W because it would be too large to store in memory. It is my understanding that the computation of W\*x, using `tf.sparse.sparse_dense_matmul(W, x),` does not have a supported gradient. Is such a gradient expected to be supported anytime soon? It seems that many others would want this functionality as well. To make this work, I am trying to implement a custom gradient using the following code:     .custom_gradient           def sparse_weight_multiply(self, w):     	      compute the product sparse_W * inputs, where sparse_W is a sparse tensor formed from the entries in w               self.sparse_W = tf.sparse.SparseTensor(self.indices, w, self.shape)               w_inputs = tf.sparse.sparse_dense_matmul(self.sparse_W, self.inputs)                define gradient for this function               def sparse_weight_grad(upstream_grad):                   '''                   upstream_grad is the gradient computed thus far in the computational graph.                    The output of this function will be the gradient of the function sparse_weight_multiply                    times upstream_grad, due to the product rule in differentiation.                    '''                    check the shape of upstream_grad                   print(""Shape of upstream grad: {}"".format(upstream_grad.shape))                   print(""Shape of inputs: {}"".format(inputs.shape))                   print(""Shape of weight: {}"".format(self.shape))                    map",2022-04-06T13:58:04Z,stat:awaiting response stale type:others,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55515,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here.  Please refer to this thread and let us know if it helps?Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please feel free to reopen if you'd like to work on this further.Thanks!
834,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(2.7.0: memory leak in TFLite's tflite::Interpreter::Invoke())ï¼Œ å†…å®¹æ˜¯ (Seeing a memory leak in tflite::Interpreter::Invoke(). The leak is observed while running our software, wrapping TFLite 2.7.0 (built from source at that tag), on iOS 15.4 (EDIT: confirmed still leaking with 2.7.1 and 15.4.1), and using CoreML delegate. The following leaks seem to occur roughly with every call to Invoke: 80 bytes chunk with the following stack:  48 bytes chunk with the following stack:  16 bytes chunk with the following stack:  32 bytes chunk with the following stack   Last time I went hunting for memory leaks, we were using 2.5.0, and there had been no leak there.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,w3sip,2.7.0: memory leak in TFLite's tflite::Interpreter::Invoke(),"Seeing a memory leak in tflite::Interpreter::Invoke(). The leak is observed while running our software, wrapping TFLite 2.7.0 (built from source at that tag), on iOS 15.4 (EDIT: confirmed still leaking with 2.7.1 and 15.4.1), and using CoreML delegate. The following leaks seem to occur roughly with every call to Invoke: 80 bytes chunk with the following stack:  48 bytes chunk with the following stack:  16 bytes chunk with the following stack:  32 bytes chunk with the following stack   Last time I went hunting for memory leaks, we were using 2.5.0, and there had been no leak there.",2022-04-06T00:46:56Z,stat:awaiting response type:bug stale comp:lite TF 2.7,closed,0,11,https://github.com/tensorflow/tensorflow/issues/55503,Hi  ! Can you please share the steps or lite model to reproduce this issue? Did you get a chance to check in TF 2.8 or nightly version too? Thanks!," Unfortunately can't share the model, but it's a yolo v3 model.  The leak is obvious, when running an inference under Instruments. It also seems to persist even after destroying the interpreter with TfLiteInterpreterDelete)  Haven't checked in 2.8 (we'd have to upgrade CI to build it, hard dependency on Bazel makes things complex)  did check in 2.6 (seems like the leak was already present there)",Hi  ! Could you please look at this issue?,One more note: the leak is gone if CoreML delegate is disabled.,This seems relevant: https://developer.apple.com/forums/thread/692425,"Looks like this had been addressed: https://github.com/tensorflow/tensorflow/commit/9f2e9d1e58f85a2b603baa1e682c0987fa49b203 Any chance this could be cherrypicked into a 2.7 update? In the meantime, we'll patch it locally and retest.","Thanks for the issue report, I have created a PR for cherrypicking into r2.8 update.","Hi, I see that you want this fix to be included in 2.7 version, generally cherrypicks will be done if it is security fix or a large community is effected with the bug.  Since the fix is available in 2.9 and it's release is around the corner. You can use Tfnightly for the immediate fix and then use the Tensorflow 2.9 when it is released. You can close this issue. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1164,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Using model.predict(X) gives me ""InternalError: Failed copying input tensor from CPU to GPU in order to run _EagerConst: Dst tensor is not initialized."")ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (Linux Ubuntu 20.04):  TensorFlow installed from Docker (tensorflow/tensorflow):  TensorFlow version: 2.8.0  Python version: Python 3.8.10  GPU model and memory: NVIDIA GeForce MX110 2048 MB I have built a neural network model with TensorFlow Probability and when trying to predict values as in  `pred_train = model.predict(X_train)`  it gives me this error, most of the times  `InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.` and sometimes it runs ok without errors. The same data/code was used with different models and works just fine. The problem seems to appear only with models built with TensorFlow Probability )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,caliari-italo,"Using model.predict(X) gives me ""InternalError: Failed copying input tensor from CPU to GPU in order to run _EagerConst: Dst tensor is not initialized.""","**System information**  OS Platform and Distribution (Linux Ubuntu 20.04):  TensorFlow installed from Docker (tensorflow/tensorflow):  TensorFlow version: 2.8.0  Python version: Python 3.8.10  GPU model and memory: NVIDIA GeForce MX110 2048 MB I have built a neural network model with TensorFlow Probability and when trying to predict values as in  `pred_train = model.predict(X_train)`  it gives me this error, most of the times  `InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.` and sometimes it runs ok without errors. The same data/code was used with different models and works just fine. The problem seems to appear only with models built with TensorFlow Probability ",2022-04-05T19:56:19Z,stat:awaiting response type:bug TF 2.8,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55502,"italo  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!",I manage to contour the problem by disabling my GPU by removing `gpus all` when running the docker image `sudo docker run it rm gpus all image_name python3 code.py` to `sudo docker run it rm image_name python3 code.py`,Are you satisfied with the resolution of your issue? Yes No
1862,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pip package build broken by removal of declare of RUY LICENSE)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.9.2  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source): 5.1.0  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Build fails with error **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel build config=nonccl verbose_failures  //tensorflow/tools/pip_package:build_pip_package **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. $ bazel build config=nonccl verbose_failures  //tensorflow/tools/pip_package:build_pip_package Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=131 INFO: Reading rc options for 'build' from /home/debianj13x8605/src/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/debianj13x860)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,elfringham,Pip package build broken by removal of declare of RUY LICENSE,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.9.2  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source): 5.1.0  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Build fails with error **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel build config=nonccl verbose_failures  //tensorflow/tools/pip_package:build_pip_package **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. $ bazel build config=nonccl verbose_failures  //tensorflow/tools/pip_package:build_pip_package Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=131 INFO: Reading rc options for 'build' from /home/debianj13x8605/src/tensorflow/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/debianj13x860",2022-04-05T09:06:26Z,type:build/install subtype: ubuntu/linux,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55494,  ,Resolved by commit https://github.com/tensorflow/tensorflow/commit/8f1a282ce58853a5152d2edda2f56a055d2ca419,Are you satisfied with the resolution of your issue? Yes No
768,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory Leak in Example/Tutorial Documentation)ï¼Œ å†…å®¹æ˜¯ ( URL(s) with the issue: https://www.tensorflow.org/tensorboard/image_summarieslogging_arbitrary_image_data  Description of issue (what needs changing): Listed example needs to be corrected as to avoid memory leaks.  Clear description It appears that the example is creating `tf.image.decode_png`  and `tf.expand_dims` layers within `plot_to_image` which is being repeatedly called by a `tf.keras.callbacks.LambdaCallback`.  These layers are filling VRAM or system RAM, depending on whether or not the example is running on GPU or not.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tokotchd,Memory Leak in Example/Tutorial Documentation," URL(s) with the issue: https://www.tensorflow.org/tensorboard/image_summarieslogging_arbitrary_image_data  Description of issue (what needs changing): Listed example needs to be corrected as to avoid memory leaks.  Clear description It appears that the example is creating `tf.image.decode_png`  and `tf.expand_dims` layers within `plot_to_image` which is being repeatedly called by a `tf.keras.callbacks.LambdaCallback`.  These layers are filling VRAM or system RAM, depending on whether or not the example is running on GPU or not.  ",2022-04-04T18:40:22Z,type:docs-bug stat:awaiting response stale,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55486,"  Could you please share more details on the issue and provide some use cases on this ? I was able to run the example tutorial  successfully on colab using TF v2.8.0 ,please find the gist here for reference.Thanks! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
515,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Are the instructions for building TFLite examples outdated?)ï¼Œ å†…å®¹æ˜¯ (I am trying to build an image classifier for android, and I am following the instructions provided here. Following the instructions to run the given example, I am getting this error:  **System Info** * System: mac * Android Studio version: 3.2 * Android SDK version: 23)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AakashKumarNain,Are the instructions for building TFLite examples outdated?,"I am trying to build an image classifier for android, and I am following the instructions provided here. Following the instructions to run the given example, I am getting this error:  **System Info** * System: mac * Android Studio version: 3.2 * Android SDK version: 23",2022-04-04T18:06:03Z,type:docs-bug stat:awaiting tensorflower comp:lite subtype:macOS,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55485, ! Thanks for reporting the bug . Raised PR  CC(wishlist: provide alternative build system) to address this issue. Please try again in Android Studio's latest version and let us know. Attaching relevant document for reference. Thanks!,Thanks  Let me try to build it with the updated instructions in your PR,Hi  I tried with Android Studio 4.1 and the given examples still fails on build step. Here is the stacktrace: ,Hi  ! Could you please look at this issue?,Any updates on this?,", could you please take a look? Thanks!",Upgrading to Android Gradle Plugin v4.2 and Android Studio 4.2 will solve this problem. * PR to upgrade Android Gradle Plugin: https://github.com/tensorflow/examples/pull/370 * I'll create a commit to update README files to suggest using Android Studio 4.2 and above.
1144,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How can we use gradients accumulation in tf.keras by defining the custom training step?)ï¼Œ å†…å®¹æ˜¯ (I found that when I used a very large model like robertlarge,the implementation of gradients accumulation like this https://gist.github.com/innat/ba6740293e7b7b227829790686f2119c may be very expensive for the gpu memory because  I need to store an additional copy of the parameters of the entire roberta model  in here ""        self.gradient_accumulation = [tf.Variable(tf.zeros_like(v, dtype=tf.float32),                                                    trainable=False) for v in self.trainable_variables] "" The below is my implementation , just  an imitation of the gradient accumulation of pytorchï¼Œbut this implementation is not valid because I found that the loss just did not decrese . I think it is a problem of the design of tf.gradienttape,when one batch size is over ,the tape did not record the gradients in last batch size . How can I resolve this problem? )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,wangbingnan136,How can we use gradients accumulation in tf.keras by defining the custom training step?,"I found that when I used a very large model like robertlarge,the implementation of gradients accumulation like this https://gist.github.com/innat/ba6740293e7b7b227829790686f2119c may be very expensive for the gpu memory because  I need to store an additional copy of the parameters of the entire roberta model  in here ""        self.gradient_accumulation = [tf.Variable(tf.zeros_like(v, dtype=tf.float32),                                                    trainable=False) for v in self.trainable_variables] "" The below is my implementation , just  an imitation of the gradient accumulation of pytorchï¼Œbut this implementation is not valid because I found that the loss just did not decrese . I think it is a problem of the design of tf.gradienttape,when one batch size is over ,the tape did not record the gradients in last batch size . How can I resolve this problem? ",2022-04-04T01:48:33Z,stat:awaiting response type:bug stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55478,"  In order to expedite the troubleshooting process here,could you please fill the issue template,and refer to this thread .Please let us know if it helps?Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite performance difference between python and c++)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Linux buster  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): 2.8.0  TensorFlow version (use command below): 2.8.0  Python version: 3.7.3  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): 8.3.0  CUDA/cuDNN version: NA  GPU model and memory: NA  TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` v2.8.0rc132g3f878cff5b6 2.8.0 **Describe the current behavior** Python performance of tflite is much better than C++. When number of threads is set to 1, Not getting best performance in C++. Manual setting the number of threads to max is giving improvement in C++ API performance and still its less than python. As per this issue(https://github.com/tensorflow/tensorflow/issues/46272) It is mentioned, number of threads in c++ are automatically set to 1 and all threads will be used, But its not happening and there is performance difference. Performance is not modified proportionately based on the threads. Suppose when threads are set to 2, we are not getting 2x performance than threads as 1 **Describe the expected behavior** Match the performance of python with C++. Give an API or directly automate the setting the threads without manual change. What is the backend used for python and C++ ? Are they same ? Can we expect the performanc)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,BhaskarsarmaP,TFLite performance difference between python and c++,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Linux buster  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): 2.8.0  TensorFlow version (use command below): 2.8.0  Python version: 3.7.3  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): 8.3.0  CUDA/cuDNN version: NA  GPU model and memory: NA  TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` v2.8.0rc132g3f878cff5b6 2.8.0 **Describe the current behavior** Python performance of tflite is much better than C++. When number of threads is set to 1, Not getting best performance in C++. Manual setting the number of threads to max is giving improvement in C++ API performance and still its less than python. As per this issue(https://github.com/tensorflow/tensorflow/issues/46272) It is mentioned, number of threads in c++ are automatically set to 1 and all threads will be used, But its not happening and there is performance difference. Performance is not modified proportionately based on the threads. Suppose when threads are set to 2, we are not getting 2x performance than threads as 1 **Describe the expected behavior** Match the performance of python with C++. Give an API or directly automate the setting the threads without manual change. What is the backend used for python and C++ ? Are they same ? Can we expect the performanc",2022-04-03T17:19:41Z,stat:awaiting response stale comp:lite type:performance TF 2.8,closed,0,14,https://github.com/tensorflow/tensorflow/issues/55476,"> **Describe the expected behavior** Match the performance of python with C++. Give an API or directly automate the setting > the threads without manual change. What is the backend used for python and C++ ? Are they same ? Can we expect the  > performance proportionately based on the threads.  > Suppose when threads are set to 2, can we expect 2x performance than 1 thread ? 1. Python binding actually uses the C++ implementation of Tensorflow Lite. Considering the Python runtime overhead itself, I would expect, under the same running environment, the Python performance should be no better than that of the C++ one. 2. Linear scaling w/ threads isn't guaranteed at all in practice due to various factors. For example, if some TFLite op in the graph isn't parallelized, then running with multiple threads on the op doesn't help at all. In addition, the multithreading performance could suffer if there's resource contention on CPU cores. Btw, you could use the TFLite benchmark tool to measure the performance of your model. In practice, the overall performance could be further impacted by other components of your inference binary, including the data preprocessing, data postprocessing etc.","But based on the attached files, Its clearly having difference between python and C++",I have run the benchmark tool for tensorflow 2.8.0 and still got less performance than python. !Capture2 !Capture1,"> I have run the benchmark tool for tensorflow 2.8.0 and still got less performance than python. !Capture2 !Capture1 Could you also paste the performance of the corresponding Python program here? In addition, what are the performances when num_threads is set to 1, 2, 4 respectively? Thx! Btw, which hardware platform are you running the performance test?"," OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Linux buster  TensorFlow installed from (source or binary): 2.8.0  TensorFlow version (use command below): 2.8.0  Python version: 3.7.3  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): 8.3.0 `cat /sys/devices/system/cpu/smt/active`       1       Hyper threading is enabled. ` sudo dmidecode t processor | grep Count` 	Core Count: 6 	Thread Count: 12 For C++, I profiled using benchmark tool and also with label example, with loop count set to 100 and warmup runs zero and threads ranging from 1 to 12 and  >12 . `bazel4.2.1 build c opt //tensorflow/lite/examples/label_image:label_image` `bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model detect.tflite labels labelmap.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp`  C++ (2.8.0) 1  306.917 ms   0   122.151 ms (Randomly getting cores ) 1   293.365 ms   2   155.565 ms  3   105.54 ms   4   82.3551 ms 5   98.8991 ms 6   64.5327 ms  7   103.385 ms  8   98.1083 ms   9    87.2203 ms  10   76.0722 ms  11   63.1864 ms  12   71.3735 ms 13  131.074 ms  (Randomly getting cores when greater than threads given) 14  133.658 ms  15  134.247 ms  Python Performance for threads (2.8.0) (Ran for 100 runs) TFLite 2.8.0 Python 1  unable to set 0  unable to set 1   67.28704053 ms 2   37.27101533 ms 3   24.89689852 ms 4   19.27222278 ms  5   16.82612416 ms 6   14.75332775 ms  7   26.53925619 ms 8   22.71062904 ms 9   20.09229901 ms 10  17.41771392 ms 11  14.67389033 ms 12  14.79799008 ms 13   81.4597831 ms (Randomly pick threads) 14    76.92194408 ms  15   76.67167669 ms These are the observations when all are closed in my pc except profiler. I have the following queries 1) **As per this issue(https://github.com/tensorflow/tensorflow/issues/46272) It is mentioned when the number of threads in c++ is set to 1 all threads will be used, But it's not happening and there is a clear performance difference.** 2) **What is meant by threads set as zero, what is ideal behavior in c++?**  3) **Why the performance in C++ is random, and in my case, there is a drop in performance after 6 threads (My pc has 6 cores) and an increase happened when it reaches almost 11 or 12 threads. Is this behavior expected? There is no proportional increase in performance w.r.t to threads set .**   **This is similar even in python until 6 there is an increase, But there is a drop in performance, and eventually, performance increases when it reaches the max number of threads which is 12. What is the reason behind it?** 4) **Can we expect a proportional increase in performance based on the threads set?** lscpu info file and scripts for python benchmarking and model are attached in this thread.",Any update,"> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Linux buster > * TensorFlow installed from (source or binary): 2.8.0 > * TensorFlow version (use command below): 2.8.0 > * Python version: 3.7.3 > * Bazel version (if compiling from source): 4.2.1 > * GCC/Compiler version (if compiling from source): 8.3.0 >  > `cat /sys/devices/system/cpu/smt/active` 1 Hyper threading is enabled. >  > ` sudo dmidecode t processor | grep Count` Core Count: 6 Thread Count: 12 >  > For C++, I profiled using benchmark tool and also with label example, with loop count set to 100 and warmup runs zero and threads ranging from 1 to 12 and >12 . >  > `bazel4.2.1 build c opt //tensorflow/lite/examples/label_image:label_image` `bazelbin/tensorflow/lite/examples/label_image/label_image tflite_model detect.tflite labels labelmap.txt image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp` >  >  C++ (2.8.0) > 1  306.917 ms 0  122.151 ms (Randomly getting cores ) 1  293.365 ms 2  155.565 ms 3  105.54 ms 4  82.3551 ms 5  98.8991 ms 6  64.5327 ms 7  103.385 ms 8  98.1083 ms 9  87.2203 ms 10  76.0722 ms 11  63.1864 ms 12  71.3735 ms 13  131.074 ms (Randomly getting cores when greater than threads given) 14  133.658 ms 15  134.247 ms >  >  Python Performance for threads (2.8.0) (Ran for 100 runs) > TFLite 2.8.0 Python >  > 1  unable to set 0  unable to set 1  67.28704053 ms 2  37.27101533 ms 3  24.89689852 ms 4  19.27222278 ms 5  16.82612416 ms 6  14.75332775 ms 7  26.53925619 ms 8  22.71062904 ms 9  20.09229901 ms 10  17.41771392 ms 11  14.67389033 ms 12  14.79799008 ms 13  81.4597831 ms (Randomly pick threads) 14  76.92194408 ms 15  76.67167669 ms >  > These are the observations when all are closed in my pc except profiler. I have the following queries >  > 1. **As per this issue(use of interpreter>SetNumThreads. Do we need to invoke setNumThreads always to improve performance CC(use of interpreter>SetNumThreads. Do we need to invoke setNumThreads always to improve performance)) It is mentioned when the number of threads in c++ is set to 1 all threads will be used, But it's not happening and there is a clear performance difference.** When it's set to 1, in general, the actual number of threads used is 1 because it's the safest choice on mobile phones in practice so that we could avoid resource contention with other activities on the phone. > 2. **What is meant by threads set as zero, what is ideal behavior in c++?** When it's set to 0, it should mean to disable multithreading as noted here. > 3. **Why the performance in C++ is random, and in my case, there is a drop in performance after 6 threads (My pc has 6 cores) and an increase happened when it reaches almost 11 or 12 threads. Is this behavior expected? There is no proportional increase in performance w.r.t to threads set .** >  > **This is similar even in python until 6 there is an increase, But there is a drop in performance, and eventually, performance increases when it reaches the max number of threads which is 12. What is the reason behind it?** >  I think this is largely because of the oversubscription on the underlying cores (i.e. severe computing resource contention) by the computing inference threads. As a result, performance decrease is expected. > 4. **Can we expect a proportional increase in performance based on the threads set?** I hope my earlier comment could clarify this. >  > lscpu info file and scripts for python benchmarking and model are attached in this thread. Lastly, regarding the intriguing performance difference reported here, after looking at the additional details you've provided, I'm now thinking whether it could be caused by the compiler or not as there's a performancedifference issue before that's solved by upgrading the compiler to gcc 9.0+ (you used gcc 8.3 here in this case?).   My guess is based on the assumption that you compiled the C++ lib/binary from source while using the Python library from the official TF 2.8.0 release. To help validate the guess, could you try the following (from being easy to being complex): * Download the prebuilt latest linux_x8664 benchmark binary to benchmark the detect model and check whether there are performance differences? More options for the benchmark tool could be found here. * As your model is a quantized model (uint8), could you try run with the corresponding nonquantized fp32 model? Just wondering the performance difference also occurs with the fp32 model. * Also build the TFLite python library from source using the same building env? Or use the TF dev/nightly docker (which includes newer compilers) to build the lib/binary? Again, many thanks for reporting the issue and the detailed description earlier!","label_image (C ++) doesn't seem to have xnnpack delegate enabled. You must specify the `xnnpack_delegate` option. (`INFO: Applied XNNPACK delegate.` Is logged) However, the xnnpack delegate does not take effect when the option is specified due to a label_image issue ( CC([TensorFlow Lite label_image]  Abort occurs with xnnpack_delegate option.)). Perhaps the python benchmark has xnnpack delegate enabled. I think you need to make sure that the xnnpack delegate option is working properly. "," , Could you please try the solution mentioned here and let us know if this works for you. > label_image (C ++) doesn't seem to have xnnpack delegate enabled. You must specify the `xnnpack_delegate` option. (`INFO: Applied XNNPACK delegate.` Is logged) However, the xnnpack delegate does not take effect when the option is specified due to a label_image issue ( CC([TensorFlow Lite label_image]  Abort occurs with xnnpack_delegate option.)). Perhaps the python benchmark has xnnpack delegate enabled. >  > I think you need to make sure that the xnnpack delegate option is working properly.","FYI, https://github.com/tensorflow/tensorflow/issues/56367 is fixed",Will update this by testing with xnnpack_delegate,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,was this resolved?
1583,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.map_fn on RaggedTensors crash during gradient computation on a GPU)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**  TensorFlow installed from (source or binary): **binary**  TensorFlow version (use command below): **2.8**  Python version: **3.7** **Describe the current behavior** When some loss (`tf.losses.SparseCategoricalCrossentropy`, `tf.losses.CategoricalCrossentropy`, `tf.losses.BinaryCrossentropy`, or `tf.losses.MeanSquaredError`) is used on Ragged tensors, which is computed via a `tf.map_fn` on a `RaggedTensor`, that the gradient computation on a GPU crashes with  The computation does not crash on a CPU and it does not crash when `tf.function`s are executed eagerly. Also, if the `tf.map_fn` is circumvented by using the following argument to compile  it works on GPU without a crash. **Describe the expected behavior** The code does not crash on a GPU.  Do you want to contribute a PR? (yes/no): **no** **Standalone code to reproduce the issue** A simple Colab reproducing the error is here: https://colab.research.google.com/drive/1OELAhvpQHhaz3sOYabf4SdBqKlQCjNjs?usp=sharing **Other info / logs** The `map_fn` used is here: https://github.com/kerasteam/keras/blob/2db5acf3e3c5904b014cb409d3c514bef44f9640/keras/losses.pyL1408 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,foxik,tf.map_fn on RaggedTensors crash during gradient computation on a GPU,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**  TensorFlow installed from (source or binary): **binary**  TensorFlow version (use command below): **2.8**  Python version: **3.7** **Describe the current behavior** When some loss (`tf.losses.SparseCategoricalCrossentropy`, `tf.losses.CategoricalCrossentropy`, `tf.losses.BinaryCrossentropy`, or `tf.losses.MeanSquaredError`) is used on Ragged tensors, which is computed via a `tf.map_fn` on a `RaggedTensor`, that the gradient computation on a GPU crashes with  The computation does not crash on a CPU and it does not crash when `tf.function`s are executed eagerly. Also, if the `tf.map_fn` is circumvented by using the following argument to compile  it works on GPU without a crash. **Describe the expected behavior** The code does not crash on a GPU.  Do you want to contribute a PR? (yes/no): **no** **Standalone code to reproduce the issue** A simple Colab reproducing the error is here: https://colab.research.google.com/drive/1OELAhvpQHhaz3sOYabf4SdBqKlQCjNjs?usp=sharing **Other info / logs** The `map_fn` used is here: https://github.com/kerasteam/keras/blob/2db5acf3e3c5904b014cb409d3c514bef44f9640/keras/losses.pyL1408 ",2022-04-03T11:15:36Z,type:bug comp:ops TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55475,"Note that I also opened an issue in the Keras repository https://github.com/kerasteam/tfkeras/issues/638 , where we discuss whether we should avoid the `tf.map_fn` on the RaggedTensors, because it can probably be avoided  the metrics with ragged tensors take a different approach, and instead of a ragged map, they use `flat_values`, see https://github.com/kerasteam/keras/blob/2db5acf3e3c5904b014cb409d3c514bef44f9640/keras/utils/metrics_utils.pyL800 ."," I was able to reproduce the issue on colab using TF v2.8.0 ,tfnightly on both gpu and cpu , please find the attached gists for reference.Thanks!","Oh, I was just pointed to me (by djoshea) that this is a duplicate of 46635, so closing.","Closing as a duplicate of CC(Missing GPU op for zeros_like for RaggedTensorVariant, error occurs when Ragged Tensor fed thru tf.map_fn) .",Are you satisfied with the resolution of your issue? Yes No
510,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix tf.scatter_nd documentation)ï¼Œ å†…å®¹æ˜¯ (I experienced quite a lot of confusion while trying to understand this documentation. The part that confused me the most was ""`indices` is an integer of shape `shape`"", which is definitely not true. I ended up scrubbing through and fixing or improving the documentation in several other ways.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,duncanriach,Fix tf.scatter_nd documentation,"I experienced quite a lot of confusion while trying to understand this documentation. The part that confused me the most was ""`indices` is an integer of shape `shape`"", which is definitely not true. I ended up scrubbing through and fixing or improving the documentation in several other ways.",2022-04-01T03:02:41Z,size:S comp:core,closed,1,1,https://github.com/tensorflow/tensorflow/issues/55460,"I just also removed the nondeterminism warning from the documentation. Since TF 2.7.0, this op is deterministic on both CPU and GPU (though not when using XLA), due to this commit from ."
1872,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GRU performance severely degraded inside tf.function with Apple m1 chip)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Monterey 12.3, Metal device set to: Apple M1 Pro  TensorFlow installed from (source or binary): binary  TensorFlow version: tensorflowdeps           2.7.0                 tensorflowmacos          2.8.0               tensorflowmetal          0.4.0   Python version: 3.9.12  GPU model and memory:  Apple M1 Pro **Describe the current behavior** I run this simple code with a GRU layer with a `tf.function` decorator:  its much slower (~510x times) than running in the eager mode. However, this bug only shows up for recurrent layers. When using Dense, the `tf.function` mode is faster than the eager mode as expected. The issue also disappeared outside `tf.GradientTape()`. I only encountered this problem in my Apple Macbook Pro with M1 chip. I tried it on a linux machine and it's ok. **Describe the expected behavior** `tf.function` should be faster (at least not several times slower) than the eager mode. **Standalone code to reproduce the issue** It cannot be reproduced on a linux machine, so no Colab notebook is available. **Other info / logs**  FYI the code above runs with the warning message as follows: > 20220331 14:40:34.462151: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz 20220331 14:40:34.463604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled. 20220331 14:40:34.480917: E tensorflo)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,David-Mao,GRU performance severely degraded inside tf.function with Apple m1 chip,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Monterey 12.3, Metal device set to: Apple M1 Pro  TensorFlow installed from (source or binary): binary  TensorFlow version: tensorflowdeps           2.7.0                 tensorflowmacos          2.8.0               tensorflowmetal          0.4.0   Python version: 3.9.12  GPU model and memory:  Apple M1 Pro **Describe the current behavior** I run this simple code with a GRU layer with a `tf.function` decorator:  its much slower (~510x times) than running in the eager mode. However, this bug only shows up for recurrent layers. When using Dense, the `tf.function` mode is faster than the eager mode as expected. The issue also disappeared outside `tf.GradientTape()`. I only encountered this problem in my Apple Macbook Pro with M1 chip. I tried it on a linux machine and it's ok. **Describe the expected behavior** `tf.function` should be faster (at least not several times slower) than the eager mode. **Standalone code to reproduce the issue** It cannot be reproduced on a linux machine, so no Colab notebook is available. **Other info / logs**  FYI the code above runs with the warning message as follows: > 20220331 14:40:34.462151: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz 20220331 14:40:34.463604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled. 20220331 14:40:34.480917: E tensorflo",2022-03-31T19:41:40Z,stat:awaiting response type:bug stale type:performance comp:tf.function TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/55455,"Mao I tried to reproduce the issue on colab using 2.8.0 , tfnightly and didn't face the error reported.Could you please have a look at the gist here and confirm the same?Thanks!",">   As I said in the problem description, this problem is reproducible on Apple m1 chip machine (like a recent macbook Pro).  I think Colab uses Linux machine, which works fine. ","Mao,  Thanks for reporting this issue.  I tried on macOS Monterey v12.3.  **.function**  **Output** `16.70040798187256 seconds` **Eager execution**  **Output** `17.127399921417236 seconds`"," On my computer they took 2.0382091999053955 seconds vs 11.459597826004028 seconds respectively. In fact, for such a small code, if it took you 17 seconds, then there is a performance bug anyway, as it's way too slower than normal.  Please notice that in the gist above (given by  ) it took 2.979365348815918 seconds on a colab machine. I think if there weren't any bug it shouldn't take 17 seconds on any modern machine, no matter on which OS.","Mao,  Time 16 and 17 seconds is because i executed code on terminal directly.  When I run .py file it took **.function** 1.9312949180603027 seconds **Eager execution**  4.352757930755615 seconds ",>  Sorry I didn't know that. Are you running on a M1 chip machine? I encounter this problem on the M1 arm chip Mac.,"Mao, Yes I am running on Mac M1 chip.  Please do check with tfnightly version. Let us know if you observe same behaviour. Thanks!","> Mao, Yes I am running on Mac M1 chip. Please do check with tfnightly version. Let us know if you observe same behaviour. Thanks!   Is there a nightly tensorflow for M1 chip? I installed my tensorflow from https://pypi.org/project/tensorflowmacos/, which only has 2.8.0 as the newest version. I checked https://pypi.org/project/tfnightly/, but that's not for M1 chip. Could you please tell me what's the correct channel to install nightly version for m1 chip? Thank you so much.","Mao, Now the latest stable Tensorflow version is 2.8.0. Once the new release done we can expect Tf 2.9. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> > Mao, Yes I am running on Mac M1 chip. Please do check with tfnightly version. Let us know if you observe same behaviour. Thanks! >  >  Is there a nightly tensorflow for M1 chip? I installed my tensorflow from https://pypi.org/project/tensorflowmacos/, which only has 2.8.0 as the newest version. I checked https://pypi.org/project/tfnightly/, but that's not for M1 chip. Could you please tell me what's the correct channel to install nightly version for m1 chip? Thank you so much. Did you find a solution to this? I have the same error on M1 Mac, using Tensorflow 2.8.0 and R 4.1.2"
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorRT conversion fails with 5D input data)ï¼Œ å†…å®¹æ˜¯ (Hi, I'm having an issue with tensorrt conversion of model which uses 3D convolutions and processes 5D input. Code to reproduce error (I cut the model to minimal example):  The output:  Other observations:  Removing activation layer or swapping it with BatchNorm makes script to execute without errors, but doesn't produce valid trt engine. It writes the following warning: ""TFTRT Warning: Engine creation for TRTEngineOp_0_0 failed. The native segment will be used instead. Reason: INVALID_ARGUMENT: Rank of perm for transpose does not match with that of the input.""  Removing Conv3D layer does not change the result.  Trying to tweak conversion parameters did not help.  Using 4D input data and 2D convolutions seem to work fine. **System information** (Using NVidia NGC container for Tensorflow, version 22.03)  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**  TensorFlow installed from (source or binary): **binary**  TensorFlow version (use command below): **2.8.0**  Python version: **3.8.10**  CUDA/cuDNN version: **11.6/8.3.3**  GPU model and memory: **NVidia Tesla T4 16Gb**)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AlexM4,TensorRT conversion fails with 5D input data,"Hi, I'm having an issue with tensorrt conversion of model which uses 3D convolutions and processes 5D input. Code to reproduce error (I cut the model to minimal example):  The output:  Other observations:  Removing activation layer or swapping it with BatchNorm makes script to execute without errors, but doesn't produce valid trt engine. It writes the following warning: ""TFTRT Warning: Engine creation for TRTEngineOp_0_0 failed. The native segment will be used instead. Reason: INVALID_ARGUMENT: Rank of perm for transpose does not match with that of the input.""  Removing Conv3D layer does not change the result.  Trying to tweak conversion parameters did not help.  Using 4D input data and 2D convolutions seem to work fine. **System information** (Using NVidia NGC container for Tensorflow, version 22.03)  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**  TensorFlow installed from (source or binary): **binary**  TensorFlow version (use command below): **2.8.0**  Python version: **3.8.10**  CUDA/cuDNN version: **11.6/8.3.3**  GPU model and memory: **NVidia Tesla T4 16Gb**",2022-03-31T12:39:13Z,stat:awaiting response type:bug stale comp:gpu:tensorrt TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55452," I was able to reproduce this issue on colab using TF v2.8.0 & tfnightly ,please find the gist here.Thanks!",Are there any updates on the issue?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Python Import error for tensorflow, tflearn and nltk libraries!)ï¼Œ å†…å®¹æ˜¯ (   I have even changed the path and tried setting up the envn paths to anaconda and python3 but it still shows this error.   If I comment the imports of the libraries mentioned I don't get any errors.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,virajsabhaya23,"Python Import error for tensorflow, tflearn and nltk libraries!",   I have even changed the path and tried setting up the envn paths to anaconda and python3 but it still shows this error.   If I comment the imports of the libraries mentioned I don't get any errors.,2022-03-31T10:25:26Z,stat:awaiting response type:build/install stale,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55450,"From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:    * For TFGPU  See point 1    * For TFCPU  See point 2  **1. Installing **TensorFlowGPU** (TF) prebuilt binaries** Make sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.    * If you have above configuration and using _**Windows**_ platform      * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.     * Refer windows setup guide.   * If you have above configuration and using _**Ubuntu/Linux**_ platform      * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.     * Refer linux setup guide.   * If error still persists then, apparently your CPU model does not support AVX instruction sets.     * Refer hardware requirements.  **2. Installing **TensorFlow** (TF) CPU prebuilt binaries** *TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.* Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load. Apparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:    * Try Google Colab to use TensorFlow.       * The easiest way to use TF will be to switch to google colab. You get preinstalled latest stable TF version. Also you can use   to install any other preferred TF version.       * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.       * All you need is a good internet connection and you are all set.    * Try to build TF from sources by changing CPU optimization flags. *Please let us know if this helps.*"," , In order to expedite the troubleshooting process, could you please provide the following information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: TensorFlow installed from (source or binary): TensorFlow version: Python version: Installed using virtualenv? pip? conda?: Bazel version (if compiling from source): GCC/Compiler version (if compiling from source): CUDA/cuDNN version: GPU model and memory: and the exact sequence of commands / steps that you executed before running into the problem.Thanks!",Also i request you to follow the steps mentioned here for installing the tensorflow with the compatible tested build configurations.Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Are you satisfied with the resolution of your issue? Yes No
1396,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Notebooks deleted when following beginner tutorial)ï¼Œ å†…å®¹æ˜¯ (Hi! This issue is related to a notebook which is a part of the beginner tutorial focusing on text classification.  Links: https://www.tensorflow.org/tutorials/keras/text_classification https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb I'm a beginner so I might be doing something wrong or something that is discouraged, but I figured that a warning may be appropriate if this is the case since this notebook is part of the beginner learning material.  Description of issue (reproduction steps): In short, the issue is that when executing a few steps in a specific order, .ipynb files with a specific prefix in the working directory are deleted. Repro steps: 1. Run cell 1 (imports) 1. Run cell 2 (check version) 1. Run cell 3 (get IMDB dataset) 1. Restart kernel **before cell 3 is done** 1. Run cell 1 again Now all .ipynb files in the working directory with a prefix of ""x_"" where x is a digit are deleted.  Environment I'm running the notebook in a local jupyterlab (version 3.3.2) on a windows 10 (version 19042.1586) machine.  If there is anything else anyone would like to know I'm happy to answer! Thanks!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,KristalAlfred,Notebooks deleted when following beginner tutorial,"Hi! This issue is related to a notebook which is a part of the beginner tutorial focusing on text classification.  Links: https://www.tensorflow.org/tutorials/keras/text_classification https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb I'm a beginner so I might be doing something wrong or something that is discouraged, but I figured that a warning may be appropriate if this is the case since this notebook is part of the beginner learning material.  Description of issue (reproduction steps): In short, the issue is that when executing a few steps in a specific order, .ipynb files with a specific prefix in the working directory are deleted. Repro steps: 1. Run cell 1 (imports) 1. Run cell 2 (check version) 1. Run cell 3 (get IMDB dataset) 1. Restart kernel **before cell 3 is done** 1. Run cell 1 again Now all .ipynb files in the working directory with a prefix of ""x_"" where x is a digit are deleted.  Environment I'm running the notebook in a local jupyterlab (version 3.3.2) on a windows 10 (version 19042.1586) machine.  If there is anything else anyone would like to know I'm happy to answer! Thanks!",2022-03-31T10:07:55Z,type:support comp:keras TF 2.8,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55449," , I have followed the mentioned steps and executed the code in latest tf v2.8.I haven't faced any issues/errors during the execution.Please find the gist here. Also this issue in more related to jupyterlab, i request you to refer this link and check with the respective repo.Thanks!",", Alright! The SOquestion you linked wasn't quite what I was describing, but you're right. This is probably more of a jupyterlab issue. I'll raise the issue over there instead. Thanks for checking!",Are you satisfied with the resolution of your issue? Yes No
812,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Converter for LogicalNot operation)ï¼Œ å†…å®¹æ˜¯ ( Converter for LogicalNot operation is using the same base class (ConvertUnaryImpl) as other Unary operations.  The special converter for Rsqrt was removed and now for this operation regular Unary Op converter is used.  New templated class implemented for testing ConvertUnary, ConvertBooleanUnary, ConvertActivation.  A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any Unary and UnaryBoolean operation. (Similar subtest for ConvertActivation operations is blocked and after refactoring of ConvertActivation it will be activated))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,drivanov,Converter for LogicalNot operation," Converter for LogicalNot operation is using the same base class (ConvertUnaryImpl) as other Unary operations.  The special converter for Rsqrt was removed and now for this operation regular Unary Op converter is used.  New templated class implemented for testing ConvertUnary, ConvertBooleanUnary, ConvertActivation.  A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any Unary and UnaryBoolean operation. (Similar subtest for ConvertActivation operations is blocked and after refactoring of ConvertActivation it will be activated)",2022-03-29T16:39:44Z,size:L comp:gpu:tensorrt,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55428,Replacement for PR CC(Converter for LogicalNot operation),fail1.log,"> fail1.log FIXED Sorry, while resolving a merge conflict online, I didn't notice that the line `template ` disappeared from  "," build errors: In file included from third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:16: In file included from ./third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26: In file included from ./third_party/tensorflow/compiler/tf2tensorrt/convert/op_converter.h:25: In file included from ./third_party/tensorflow/compiler/tf2tensorrt/convert/weights.h:22: ./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:98:30: error: no matching function for call to 'DebugString'     StrAppend(&tmp_s, StrCat(DebugString(el), "", ""));                              ^~~~~~~~~~~ third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:1706:58: note: in instantiation of function template specialization 'tensorflow::tensorrt::DebugString' requested here                        ' requested here       AddTestTensor(""input"", p.input_dims, input_tf_type, input_values);       ^ third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:7057:3: note: in instantiation of function template specialization 'tensorflow::tensorrt::convert::OpConverter_UnaryTest::RunTests' requested here   RunTests(""LogicalUnary"", ops_to_test, *UnaryBooleanOperationMap(), op_map,   ^ ./third_party/tensorflow/compiler/tf2tensorrt/convert/trt_parameters.h:46:8: note: candidate function not viable: no known conversion from 'const std::__bit_iterator, true, 0>::reference' (aka 'const std::__bit_const_reference>') to 'const tensorflow::tensorrt::TrtPrecisionMode' for 1st argument string DebugString(const TrtPrecisionMode mode);        ^ ./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:95:8: note: candidate template ignored: could not match 'vector' against '__bit_const_reference' string DebugString(const std::vector& vector) {        ^ ./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:86:8: note: candidate template ignored: requirement 'std::is_arithmetic>>>::value' was not satisfied [with CType = std::__bit_const_reference>] string DebugString(const CType& el) {"," : I don't see these compilation issues on my side. To make things easier, I just created PR CC(Converter for LogicalNot operation) and am closing it."
1106,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bazel build error. Warning on retrieving an archive which is not found and then fails.)ï¼Œ å†…å®¹æ˜¯ (Hi fellow developers, I am trying to build Tensorflow from source on Windows using the tutorial on the website (https://www.tensorflow.org/install/source_windows) but I have an issue when i get to the bazel build of tensorflow I am using this command to build since I have cuda and cudnn installed on my computer. (When i did the ./configure it found them and I could specify the compute capability for example) : `bazel build config=opt config=cuda define=no_tensorflow_py_deps=true copt=nvcc_options=disablewarnings //tensorflow/tools/pip_package:build_pip_package` Here is the trace starting a little before it could not found an archive and then fails.  I am really sorry if it appears to be a problem on my side but since I can't retrieve the archive I think it is best to start here first. Thank you for your help and your job.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,GosuPaper,Bazel build error. Warning on retrieving an archive which is not found and then fails.,"Hi fellow developers, I am trying to build Tensorflow from source on Windows using the tutorial on the website (https://www.tensorflow.org/install/source_windows) but I have an issue when i get to the bazel build of tensorflow I am using this command to build since I have cuda and cudnn installed on my computer. (When i did the ./configure it found them and I could specify the compute capability for example) : `bazel build config=opt config=cuda define=no_tensorflow_py_deps=true copt=nvcc_options=disablewarnings //tensorflow/tools/pip_package:build_pip_package` Here is the trace starting a little before it could not found an archive and then fails.  I am really sorry if it appears to be a problem on my side but since I can't retrieve the archive I think it is best to start here first. Thank you for your help and your job.",2022-03-29T13:23:37Z,stat:awaiting response type:build/install subtype:windows,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55426,Problem solved here : https://discuss.tensorflow.org/t/errorbuildingtensorflow28inwindows10/7984/3 Sorry for the inconvenience," , Please refer this link 1 and 2 with the similar error.It helps.Thanks!",> Problem solved here : https://discuss.tensorflow.org/t/errorbuildingtensorflow28inwindows10/7984/3 >  > Sorry for the inconvenience Here is the answer to my problem  . Sorry for the incovenience and thanks for your help," , Glad the issue is resolved for you, please feel free to move this to closed status.",Are you satisfied with the resolution of your issue? Yes No
1866,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite allocate tensors fails: (CONCATENATION) failed to prepare )ï¼Œ å†…å®¹æ˜¯ ( 1. System information There is a problem in the pytorch model > tflite quantization process, the model is a multiinput multioutput structure The model inference process is shown in the code `    def forward(self, input_x: torch.Tensor, tdnn_s0, tdnn_s1, tdnn_s1_2):          model_in = input_x         conv1_inputs = torch.cat([tdnn_s0, model_in], 2)          conv1 = self.norm0).permute(0, 2, 1)).permute(0, 2, 1)         output_conv1 = conv1         conv2_inputs = torch.cat([tdnn_s1, conv1], 2)          conv2 = self.convs1          output_conv2 = conv2         conv2_2_inputs = torch.cat([tdnn_s1_2, conv2], 2)          conv2_2 = self.norm1).permute(0, 2, 1)).permute(0, 2, 1)         conv2_2 = torch.add(conv2_inputs[:,:,:1], conv2_2)         dense_input = conv2_2.permute(0, 2, 1)         output = self.dense(dense_input)         return  output, output_conv1, output_conv2` When I quantize the model I get an error like the following, but when I comment out the quantization in the code, it works; I don't yet know where the problem is 20220329 20:37:26.144668: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format. 20220329 20:37:26.144820: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency. 20220329 20:37:26.144875: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored change_concat_input_ranges. 20220329 20:37:26.146905: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: tmp 20220329 20:37:26.149434: I tensorflow/cc/saved_model/reader.cc:90] Reading meta grap)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ldb1026,TFLite allocate tensors fails: (CONCATENATION) failed to prepare ," 1. System information There is a problem in the pytorch model > tflite quantization process, the model is a multiinput multioutput structure The model inference process is shown in the code `    def forward(self, input_x: torch.Tensor, tdnn_s0, tdnn_s1, tdnn_s1_2):          model_in = input_x         conv1_inputs = torch.cat([tdnn_s0, model_in], 2)          conv1 = self.norm0).permute(0, 2, 1)).permute(0, 2, 1)         output_conv1 = conv1         conv2_inputs = torch.cat([tdnn_s1, conv1], 2)          conv2 = self.convs1          output_conv2 = conv2         conv2_2_inputs = torch.cat([tdnn_s1_2, conv2], 2)          conv2_2 = self.norm1).permute(0, 2, 1)).permute(0, 2, 1)         conv2_2 = torch.add(conv2_inputs[:,:,:1], conv2_2)         dense_input = conv2_2.permute(0, 2, 1)         output = self.dense(dense_input)         return  output, output_conv1, output_conv2` When I quantize the model I get an error like the following, but when I comment out the quantization in the code, it works; I don't yet know where the problem is 20220329 20:37:26.144668: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format. 20220329 20:37:26.144820: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency. 20220329 20:37:26.144875: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored change_concat_input_ranges. 20220329 20:37:26.146905: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: tmp 20220329 20:37:26.149434: I tensorflow/cc/saved_model/reader.cc:90] Reading meta grap",2022-03-29T12:49:10Z,stat:awaiting response type:support TFLiteConverter TF 2.8,closed,0,9,https://github.com/tensorflow/tensorflow/issues/55424,"Hi  ! I think you should use ONNX to convert Pytorch models to TF model/TF lite models . Attaching relevant threads 1, 2 for reference . Please revert back with a Colab gist for further assistance. Thank you!","hi     https://colab.research.google.com/drive/19TLv1SEtZWcE6Vzb1Nig8JSjwL6uICqscrollTo=L_PMowpPmaFx I have successfully converted the model to a pb model, it's just that I get an error during quantization The problem I encountered is that when I use the code to convert the pb to tflite model if I quantify the configuration, like this  there will be corresponding problemsï¼Œ that is to  paper   > RuntimeError: tensorflow/lite/kernels/concatenation.cc:80 t>dims>data[d] != t0>dims>data[d] (20 != 40)Node number 0 (CONCATENATION) failed to prepare. but if it is not quantized, the conversion will be successful.  Is this problem caused by the fact that the model cannot recalculate the shape due to operator collapse?",Are you satisfied with the resolution of your issue? Yes No,"The problem is solved, the reason is that the pb model is a multiinput model, and the data returned by the function onverter.representative_dataset is not in the correct order",  ! I did not have access to above colab notebook but  suspected the same (order or concatenation of inputs as single input upon how model was trained) in the representative dataset aside suggesting to remove the .numpy() operation.  Shall we move this to closed status then?,"The problem is solved, it can be closed, thank you", ! Thanks for confirmation. Moving to closed status as it was marked resolved from this comment.,Are you satisfied with the resolution of your issue? Yes No,"Thanks , I had the same problem and it indeed was the order of the inputs in the representative dataset. For anyone that needs extra clarification: the input order of the actual graph (after conversion) might be different than the order as defined in your python code, and, apparently, the representative dataset needs input ordering of the postconversion graph..."
645,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Parameter init_epoch can not be identified when calling model.train())ï¼Œ å†…å®¹æ˜¯ (Hi,     I am trying to implement incremental training (when the model trained is stopped due to other reasons, I can retrain the model from the latest checkpoint). There, I used the parameter 'init_epoch' when calling model.train(). However, parameter 'init_epoch' can not be identified in Tensorflow2.4.0.    I want to know which version of Tensorflow support parameter 'init_epoch'? !scr)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,v3551G,Parameter init_epoch can not be identified when calling model.train(),"Hi,     I am trying to implement incremental training (when the model trained is stopped due to other reasons, I can retrain the model from the latest checkpoint). There, I used the parameter 'init_epoch' when calling model.train(). However, parameter 'init_epoch' can not be identified in Tensorflow2.4.0.    I want to know which version of Tensorflow support parameter 'init_epoch'? !scr",2022-03-29T10:53:10Z,stat:awaiting response stale type:others TF 2.4,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55422," In order to expedite the troubleshooting process,could you please provide the code and share the error in  text format  instead of the screenshot which will help  us to analyze the issue?Thanks!","Thank you. The parameter is misspelled, and the correct parameter should be 'initial_epoch'"," Currently the BackupAndRestore callback only supports eager mode. In graph mode, consider using  Save/Restore Model as mentioned here, and by providing initial_epoch in Model.fit. We recommend you to use TF v2.4 or later versions as older versions are not actively supported. Please let us know if it helps? Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1044,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to update tf.Variable inplace using custom op?)ï¼Œ å†…å®¹æ˜¯ (Hi, I want to write a custom op which updates the contents of `tf.Variable` inplace on a GPU (cuda) device. I haven't found any relevant documentation to achieve this, hence creating this issue. It'll be great if you can point me out to any relevant doc/code implementation for this.  I can do the same with assign op, but that requires creation of extra tensor and will prefer to update it inplace. I looked at the implementation of Adam op registration from here and noticed that the variables to be updated are copied to host memory. Also, it appears the same approach is followed here in assign op implementation.  Is this really needed to update variables and can this be avoided? I suspect this might have some performance implications of copying data to CPU every time it needs to be updated.  Thanks)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranavladkat,How to update tf.Variable inplace using custom op?,"Hi, I want to write a custom op which updates the contents of `tf.Variable` inplace on a GPU (cuda) device. I haven't found any relevant documentation to achieve this, hence creating this issue. It'll be great if you can point me out to any relevant doc/code implementation for this.  I can do the same with assign op, but that requires creation of extra tensor and will prefer to update it inplace. I looked at the implementation of Adam op registration from here and noticed that the variables to be updated are copied to host memory. Also, it appears the same approach is followed here in assign op implementation.  Is this really needed to update variables and can this be avoided? I suspect this might have some performance implications of copying data to CPU every time it needs to be updated.  Thanks",2022-03-29T00:42:06Z,stat:awaiting response type:support stale comp:ops,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55417," TensorFlow variable is the recommended way to represent a shared, persistent state your program manipulates.For more details could you please have a look at this link and refer this thread? Please let us know if it helps? Thanks!","Hi, Thanks for the response. However it does not answer my question of how to update `tf.Variable` inplace using custom C++ op. If I were to use `var.assign()` op, I will need to return a tf.Tensor from the op and then use that to update variable. That consumes more memory of my use case and training fails with out of memory errors. Thanks",",  Hi, Thanks for reporting this issue. If you'd like to create an op that isn't covered by the existing TensorFlow library,  you can create a custom C++ op. Here are some reference documents which help community to create custom of with GPU support  Create custom op and building setting up custom op. Thanks!","Hi, thanks for the response. I'm using the resourced that you have linked to create custom op. But my question is `How to update tf.Variable inplace using custom op?` Thanks","The links which you mentioned does the gpu kernel registration for resource variables.  It can impact the performance little bit but you can create custom op without registering GPU kernel.  You can refer this document about `ResourceVariable`. Also, refer this document on implementing kernel for the custom op.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
694,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([oneDNN] Softmax with oneDNN library.)ï¼Œ å†…å®¹æ˜¯ (This PR enables softmax forward op using oneDNN library. It uses inplace computation whenever possible. It improves performance of BERT/Transformer like models wherein the softmax is used for selfattention. The following are performance data on some microbenchmarks and BERT inference. The performance data were collected on Intel Xeon CPUs using Eigen ThreadPool.  Microbenchmarks Tensor Dims  2.85x This PR improves performance by 12% for some models that use softmax.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,mdfaijul,[oneDNN] Softmax with oneDNN library.,This PR enables softmax forward op using oneDNN library. It uses inplace computation whenever possible. It improves performance of BERT/Transformer like models wherein the softmax is used for selfattention. The following are performance data on some microbenchmarks and BERT inference. The performance data were collected on Intel Xeon CPUs using Eigen ThreadPool.  Microbenchmarks Tensor Dims  2.85x This PR improves performance by 12% for some models that use softmax.,2022-03-28T16:54:17Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55408
531,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable pylint warnings)ï¼Œ å†…å®¹æ˜¯ (It is quite annoying to check some warning only with the internal CI (Kokoro). This is going to slowdown the review rate, increase reviewers noise/rounds and wasting CI cycles (as currently we cannot invoke isolated CI Linting jobs). E.g. see https://github.com/tensorflow/tensorflow/pull/55363pullrequestreview922485535 /)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bhack,Enable pylint warnings,"It is quite annoying to check some warning only with the internal CI (Kokoro). This is going to slowdown the review rate, increase reviewers noise/rounds and wasting CI cycles (as currently we cannot invoke isolated CI Linting jobs). E.g. see https://github.com/tensorflow/tensorflow/pull/55363pullrequestreview922485535 /",2022-03-27T20:13:40Z,size:XS,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55396,/ ,Copybara again.., Can we rollback this? The problem was that we cannot check space errors anymore  with pylint only since 2.6.  We need to intorduce  Black or something else. https://github.com/PyCQA/pylint/commit/28a5c2e417ebdf239712859c9f699d602411233b More in general here the full list of deprecated checks https://github.com/PyCQA/pylint/blob/main/pylint/constants.pyL90L180 / 
796,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add TF_AssignRefVariable)ï¼Œ å†…å®¹æ˜¯ (The Kernel Extension for Variable Operations API added pluggable device support for resource variables and incomplete support for ref variables (`TF_OpKernelContext_ForwardRefInputToRefOutput`), but it misses one endpoint to make ref variables usable: `TF_AssignRefVariable`. This change attempts to fill the gaps in the RFC by adding `TF_AssignRefVariable`, which is analogous to `TF_AssignVariable` but for ref variables instead of resource variables. It uses the same semantics where the user has to pass a copy function to be called when copying the value tensor to the ref tensor.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PatriceVignola,Add TF_AssignRefVariable,"The Kernel Extension for Variable Operations API added pluggable device support for resource variables and incomplete support for ref variables (`TF_OpKernelContext_ForwardRefInputToRefOutput`), but it misses one endpoint to make ref variables usable: `TF_AssignRefVariable`. This change attempts to fill the gaps in the RFC by adding `TF_AssignRefVariable`, which is analogous to `TF_AssignVariable` but for ref variables instead of resource variables. It uses the same semantics where the user has to pass a copy function to be called when copying the value tensor to the ref tensor.",2022-03-26T02:31:34Z,size:L,closed,0,21,https://github.com/tensorflow/tensorflow/issues/55379,I need more context. Please add a PR description and (in it) a link to the original RFC(s) that introduced APIs such as `TF_AssignVariable`.,"> I need more context. Please add a PR description and (in it) a link to the original RFC(s) that introduced APIs such as `TF_AssignVariable`.  Got it, thanks! I updated the description.","I'm not sure why the Windows Bazel GPU build is failing. I built it locally yesterday and it completed just fine, and the Linux GPU build completed successfully. Is it possible to look at the failures or is it unrelated to this PR?","Hi  , is there a way to see the Windows Bazel GPU error messages?", Usually I just rerun the tests and hope the log appears. Will rerun them now since it's evening anyway.,"For Googlers, if you retrigger the job and click on the details link before the job finishes you are taken to the internal log page (external people will get an invalid link). Once the job finishes, the link switched to the public version, but if the job fails in certain ways there is no public log. This is usually infra failure and could be ignored.","The build failed again without a ""Details"" link. Is there anything I can do to expedite this PR? I successfully built it and ran the tests locally but maybe I'm missing something. Is there a way to confirm whether it is an infra issue or not?", I will manually import the PR and run the tests internally. I'll let you know if anything fails there.,This PR was reverted because it broke internal tests. We will try to follow up with failure details soon. ,"Why was the PR merged in the first place? I thought you would only merge it if the internal tests passed. Please let us know what tests where failing when you have the info, I can debug it locally if I know what they are."," We only run TensorFlow presubmit tests before merging. After merging, PRs could still be reverted if they broke TensorFlow nightly tests or other internal tests (e.g., unit / integration tests of internal applications that use TensorFlow). In this case, it broke an internal test.","Here is the complete workflow for a PR: !7XNWZfbTauSmBXn We have this many steps due to CI complexity and needing to support both multiple platforms x python versions x {GPU,CPU} and a requirement to have short presubmits to not negatively impact developer velocity.","Hi, I'm bumping this to know if it's possible to get the failure details, or the command line that was used to run the internal tests that failed. We are committed to investigate the issue and resolve it.",Sorry for the delay. It took a while to extract a proxy test. Please add the following test case to variable_ops_test.py  To run: ,"Thank you for the snippet. What kind of error am I supposed to see? When I run the test command from the master branch, I get the following error so I'm not able to compare with my branch:  But otherwise, when I manually install the python package built from my branch and run the test manually, it seems to pass without issues. I'm just not sure what error I'm supposed to see here since my AssignRefVariable PR merely refactored the existing code around and didn't handle strings differently or change the registration.","Oops sorry. I forgot to post the error message. I'm not sure why it's happening either. I don't see anything apparently wrong in your refactor code.  I could reproduce the error with the TensorFlow docker image here. The error happened with commit da9a9d69bb1c98f0e88883b3d0274b139d455e49 (merged pull request) but not the commit before (2d040ce3dde8553125eaaa49a97355cc2e05a445). To summarize, these series of commands should be able to reproduce the error: ","I found the problem, but it wasn't caused by me. In my PR, line 76 correctly uses the `CHECK` macro. But somehow, when committing, the TensorFlow gardener decided to change the `CHECK` for a `DCHECK` on line 76, which obviously gets removed in release builds. And when we look at the git blame, we see that all lines are attributed to me, except for that infamous `CHECK` line. What could cause this behavior? Is there a script that converts `CHECK` statements to `DCHECK` for performance purposes and it erroneously changed this line, or is there something I have to do to force it to not be removed? I created a brand new PR where I replaced the `CHECK` statement with an `OP_REQUIRES` statement, but I'd still like to understand why the `CHECK` seems to have magically been replaced with a `DCHECK`.","`CHECK`s have been causing issues in the past and are discouraged (there is automation that blocks submit with them, unless there are additional steps taken). `DCHECK` is a workaround with some differences `OP_REQUIRES` is the best approach to use.","It would have been fine if the submission had been blocked, but the automation silently changing `CHECK` to `DCHECK` can be hard to debug when the change gets reverted, especially when refactoring old code that already contained them.","It wasn't automation, it was a deliberate change during PR import.","Sorry for the late reply and thank you for looking into it!  Yes, it was me changing `CHECK` to `DCHECK` during merge. Sorry I forgot to mention that. :("
1885,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('Activation' object has no attribute 'kernel' applying regularization to MobileNetV3)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.7.0rc169gc256c071bb2, tfnightly2.9.0.dev20220325  Python version: 3.9.7  CUDA/cuDNN version: N/A   GPU model and memory: N/A **Describe the current behavior** Attempting to add regularization to MobileNetV3. The regularization appears to be applied correctly (applied to 43 Conv2D and Dense layers), but calling `model.fit` fails with the error `AttributeError: 'Activation' object has no attribute 'kernel'`. It appears that calling `model.fit` is reapplying the regularization, but on the last `layer`, which is the output Activation. Adding regularization this way works on MobileNetV2 and custom models I've built, but only fails on MobileNetV3 (small or large). If a default argument is used in the lambda (`layer.add_loss(lambda l=layer: regularizer(l.kernel))`), I can build and train the model, but saving the model with the Saved Model API fails with `tensorflow.python.saved_model.nested_structure_coder.NotEncodableError: No encoder for object  of type .` **Describe the expected behavior** The sample script should run without fail, or at least fail similarly between MobieNetV2 and MobileNetV3. **Standalone code to rep)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mattpotma,'Activation' object has no attribute 'kernel' applying regularization to MobileNetV3,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.7.0rc169gc256c071bb2, tfnightly2.9.0.dev20220325  Python version: 3.9.7  CUDA/cuDNN version: N/A   GPU model and memory: N/A **Describe the current behavior** Attempting to add regularization to MobileNetV3. The regularization appears to be applied correctly (applied to 43 Conv2D and Dense layers), but calling `model.fit` fails with the error `AttributeError: 'Activation' object has no attribute 'kernel'`. It appears that calling `model.fit` is reapplying the regularization, but on the last `layer`, which is the output Activation. Adding regularization this way works on MobileNetV2 and custom models I've built, but only fails on MobileNetV3 (small or large). If a default argument is used in the lambda (`layer.add_loss(lambda l=layer: regularizer(l.kernel))`), I can build and train the model, but saving the model with the Saved Model API fails with `tensorflow.python.saved_model.nested_structure_coder.NotEncodableError: No encoder for object  of type .` **Describe the expected behavior** The sample script should run without fail, or at least fail similarly between MobieNetV2 and MobileNetV3. **Standalone code to rep",2022-03-25T17:18:05Z,stat:awaiting response type:bug comp:keras TF 2.7,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55376,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thanks!,Sorry I missed that. Cross posted to https://github.com/kerasteam/keras/issues/16316. Closing this now.,Are you satisfied with the resolution of your issue? Yes No
1151,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.load_op_library doesn't load .so file and throws file not found error although the file exists)ï¼Œ å†…å®¹æ˜¯ ( **Describe the current behavior** When the code below is executed  The first line prints `True` which means that the file exists. But the second line throws the following error. `  File ""/home/ubuntu/venvpipe1v1/lib/python3.7/sitepackages/tensorflow/python/framework/load_library.py"", line 54, in load_op_library     lib_handle = py_tf.TF_LoadLibrary(library_filename) tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory ` **Describe the expected behavior** The .so file should load  Do you want to contribute a PR? (yes/no): no **Standalone code to reproduce the issue**  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nirnayr,tf.load_op_library doesn't load .so file and throws file not found error although the file exists," **Describe the current behavior** When the code below is executed  The first line prints `True` which means that the file exists. But the second line throws the following error. `  File ""/home/ubuntu/venvpipe1v1/lib/python3.7/sitepackages/tensorflow/python/framework/load_library.py"", line 54, in load_op_library     lib_handle = py_tf.TF_LoadLibrary(library_filename) tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory ` **Describe the expected behavior** The .so file should load  Do you want to contribute a PR? (yes/no): no **Standalone code to reproduce the issue**  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. ",2022-03-25T15:40:34Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55375," , Can you please take a look at this issue1, 2 and also TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations. ","Tried linking libtensorflow_framework.so and libtensorflow_framework.so.2 using ln as suggested by https://stackoverflow.com/questions/56888781/tensorflownotfounderrorlibtensorflowframeworksocannotopensharedfileor?noredirect=1&lq=1 I have tried appending LD_LIBRARY_PATH with the libtensorflow_framework.so file as suggested by this thread https://github.com/tensorflow/tensorflow/issues/30488 I have tried downgrading to tensorflow 1.14, with Python 3.7. As that was the latest compatible according to https://www.tensorflow.org/install/sourcegpu",",  `tf.sysconfig.get_link_flags()`. returns   Here, `library directory` is  `L` and `library` is `l`. Now replace the `L` and `l`  with `library_dirs` and `libraries `arguments to your Extension object in `setup.py`. Finally, set your `LD_LIBRARY_PATH` environment variable to point to the directory with the `libtensorflow_framework.so` library is located.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1836,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to call tf.convert_to_tensor for a tf.RaggedTensor with a PlaceHolder value)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.3  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): https://developer.apple.com/metal/tensorflowplugin/  TensorFlow version (use command below): unknown 2.8.0  Python version: 3.8.12  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: Apple M1 16.00 GB **Describe the current behavior** related issue in tensorflowtext github : https://github.com/tensorflow/text/issues/867  **Describe the expected behavior**  **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. This eager execution equivalent returns expected output. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,dandelin,Failed to call tf.convert_to_tensor for a tf.RaggedTensor with a PlaceHolder value,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.3  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): https://developer.apple.com/metal/tensorflowplugin/  TensorFlow version (use command below): unknown 2.8.0  Python version: 3.8.12  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: Apple M1 16.00 GB **Describe the current behavior** related issue in tensorflowtext github : https://github.com/tensorflow/text/issues/867  **Describe the expected behavior**  **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. This eager execution equivalent returns expected output. ",2022-03-25T12:54:23Z,stat:awaiting response type:bug stale comp:ops TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55373," , I was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist here.",", I tried to execute the mentioned code on tensorflow 2.17 which consists of Keras3.0, and observed the code was executed without any error/fail, also the output is available. Kindly find the gist of it here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
625,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow android throw java.lang.OutOfMemoryError when building project, but there's free 20GB memory.)ï¼Œ å†…å®¹æ˜¯ (!image My process:  Clone project from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/android/test  Click build button (My PC memory : 11.7/31.9 GB (37%))  When building the memory only use 2GB, then it throwed error :    My PC has hyperv Did I miss some key steps?  Really appreciate Tensorflow team's efforts.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shps951023,"TensorFlow android throw java.lang.OutOfMemoryError when building project, but there's free 20GB memory.","!image My process:  Clone project from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/android/test  Click build button (My PC memory : 11.7/31.9 GB (37%))  When building the memory only use 2GB, then it throwed error :    My PC has hyperv Did I miss some key steps?  Really appreciate Tensorflow team's efforts.",2022-03-25T00:49:28Z,stat:awaiting response stale type:others comp:gpu,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55368," , In order to expedite the troubleshooting process, could you please provide the following information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: TensorFlow installed from (source or binary): TensorFlow version: Python version: Installed using virtualenv? pip? conda?: Bazel version (if compiling from source): GCC/Compiler version (if compiling from source): CUDA/cuDNN version: GPU model and memory: and the exact sequence of commands / steps that you executed before running into the problem",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1631,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DeeplabV3 custom dataset, inference problem black images)ï¼Œ å†…å®¹æ˜¯ (Good morning, I want to train a custom dataset using deeplabV3. I'm following this tutorial (https://sanjayparajuli27.medium.com/howtotraindeeplaboncustomdataseta40c41c4c6a3) for this dataset (https://www.kaggle.com/datasets/dansbecker/cityscapesimagepairs) that I found on Kaggle, based on cityscapes. There are images of 256x256 pixel in RGB colors, divided in 2975 imgs for training and 500 for validation, and I created the respective mask using this script  Each image in the dataset contain its same mask, so before to launch the new notebook I divided the image and the mask to have a situation like in the tutorial. You can find my code here: https://drive.google.com/drive/folders/105JMDmujY6lknH3D74WM8R8S7jTb51qX?usp=sharing and this is the notebook https://drive.google.com/file/d/1xmUtLBXPj4mZdqbx9SAQOXKSwxtxCLX/view?usp=sharing I have a problem with the inference. Every time I launch the notebook with few epochs (less then 10) I receive good results, but trying to increase the number of epoch I have all black images. These are the parameters that I used for the train:  I edited the data_generator.py file putting  I'm using a pretrained model downloaded from here: http://download.tensorflow.org/models/deeplabv3_cityscapes_train_2018_02_06.tar.gz I keep the batch size at 4, and I don't know if it is correct or not. Can you tell me where could be the possible error?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,paolodavid,"DeeplabV3 custom dataset, inference problem black images","Good morning, I want to train a custom dataset using deeplabV3. I'm following this tutorial (https://sanjayparajuli27.medium.com/howtotraindeeplaboncustomdataseta40c41c4c6a3) for this dataset (https://www.kaggle.com/datasets/dansbecker/cityscapesimagepairs) that I found on Kaggle, based on cityscapes. There are images of 256x256 pixel in RGB colors, divided in 2975 imgs for training and 500 for validation, and I created the respective mask using this script  Each image in the dataset contain its same mask, so before to launch the new notebook I divided the image and the mask to have a situation like in the tutorial. You can find my code here: https://drive.google.com/drive/folders/105JMDmujY6lknH3D74WM8R8S7jTb51qX?usp=sharing and this is the notebook https://drive.google.com/file/d/1xmUtLBXPj4mZdqbx9SAQOXKSwxtxCLX/view?usp=sharing I have a problem with the inference. Every time I launch the notebook with few epochs (less then 10) I receive good results, but trying to increase the number of epoch I have all black images. These are the parameters that I used for the train:  I edited the data_generator.py file putting  I'm using a pretrained model downloaded from here: http://download.tensorflow.org/models/deeplabv3_cityscapes_train_2018_02_06.tar.gz I keep the batch size at 4, and I don't know if it is correct or not. Can you tell me where could be the possible error?",2022-03-24T09:14:30Z,stat:awaiting response type:support stale comp:model,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55360, Could you please post this issue in the tensorflow/models repo to get the right help there? Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
558,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(@tensorflow/tfjs: Unable to load model)ï¼Œ å†…å®¹æ˜¯ (Using Webpack to build a node app with /tfjs included, when using tf.loadLayersModel() the following error is received:  When looking into the node_module, it does appear that this is a bug in the tensorflowjs core code. This happens when trying to load a model via an http request or a direct file (since this is a node application))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mcmanussean,@tensorflow/tfjs: Unable to load model,"Using Webpack to build a node app with /tfjs included, when using tf.loadLayersModel() the following error is received:  When looking into the node_module, it does appear that this is a bug in the tensorflowjs core code. This happens when trying to load a model via an http request or a direct file (since this is a node application)",2022-03-24T03:45:51Z,stat:awaiting response type:bug stale,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55357, Could you please post this issue in tensorflow/tfjs repo to get the right help there? Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,  Could you please confirm if you have posted this issue in tfjs repo and please refer to this thread with similar error? Thanks!,Closing this issue due to lack of recent activity.Please feel free to reopen the issue if you still have a concern. Thanks!,Are you satisfied with the resolution of your issue? Yes No
276,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix pfor defunc issue)ï¼Œ å†…å®¹æ˜¯ (This is trying to fix the indirect defunc case with a vectorized_map)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bhack,Fix pfor defunc issue,This is trying to fix the indirect defunc case with a vectorized_map,2022-03-23T20:09:34Z,comp:ops size:S,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55350,  I've tried to add a dummy failing test (we could improve it as you like). The goal here is to fix https://github.com/tensorflow/tensorflow/issues/55340 I see that we are checking if we are in eager mode and we wrap the function in that case. https://github.com/tensorflow/tensorflow/blob/f62c8863b3e19b81d2484a95e96ddb854ea66b2f/tensorflow/python/ops/parallel_for/control_flow_ops.pyL183L205 /," I've tried to insert  `f = autograph.tf_convert(f, autograph_ctx.control_status_ctx())` there but it is failing with the same error",I've moved `autograph.tf_convert` internally. I don't know if the coverage is complete but locally the test is passing. Let me know if and how we want to improve this., Can you connect this PR to https://github.com/tensorflow/tensorflow/issues/55340. So when it is merged it will close the ticket., Let me know if you like this test or you want to test something else.,/
1129,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TF-lite conversion of complex abs layer not working with integer quantization with fallbacks)ï¼Œ å†…å®¹æ˜¯ (I'm trying to optimize a speechtotext Conformer model for tflite usage. Quantization of the model is working for _default_ optimization mode, but not when a _representative dataset_ is used. In this case it fails in inference with: `type != kTfLiteFloat32 (INT8 != FLOAT32) Node number 46 (COMPLEX_ABS) failed to prepare` Shouldn't the model use the _float32_ fallback in this case, since I'm not enforcing _int8_ operators?  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20:04  TensorFlow installation (pip package or built from source): NvidiaDocker  TensorFlow library (version, if pip package or github SHA, if built from source): 2.7  2. Failure after conversion   3. Code snippets   Related issue: https://github.com/tensorflow/tensorflow/issues/53393issuecomment1009703703 (was closed without solution))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DanBmh,TF-lite conversion of complex abs layer not working with integer quantization with fallbacks,"I'm trying to optimize a speechtotext Conformer model for tflite usage. Quantization of the model is working for _default_ optimization mode, but not when a _representative dataset_ is used. In this case it fails in inference with: `type != kTfLiteFloat32 (INT8 != FLOAT32) Node number 46 (COMPLEX_ABS) failed to prepare` Shouldn't the model use the _float32_ fallback in this case, since I'm not enforcing _int8_ operators?  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20:04  TensorFlow installation (pip package or built from source): NvidiaDocker  TensorFlow library (version, if pip package or github SHA, if built from source): 2.7  2. Failure after conversion   3. Code snippets   Related issue: https://github.com/tensorflow/tensorflow/issues/53393issuecomment1009703703 (was closed without solution)",2022-03-23T19:34:34Z,stat:awaiting response type:bug stale TFLiteConverter TF 2.7,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55349,Fails as well when I'm replacing the `abs` with `spectrogram = tf.math.real(stfts) ** 2 + tf.math.imag(stfts) ** 2`:  `RuntimeError: /workspace/tensorflow/lite/kernels/complex_support.cc:43 output>type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (IMAG) failed to prepare.`,Hi  ! Can you please share the model along with code containing representative dataset to replicate the issue? I was going to propose these changes for integer quantization.  Thanks! ,"The actual model is quite complicated, but I could implement a shorter example: ",Ok  ! I used tf.lite.interpreter from version 2.8 instead of tflite.interpreter from tflite_runtime. Attaching resolved gist for reference. Thanks!,"Thanks, it is working for me with `tf.lite` instead of `tflite_runtime` and `v=2.8` as well. (I kept float32 input and output) Do you know when `tflite_runtime=2.8` will be released? My target is a RaspberryPi and using the full tensorflow installation isn't really feasible.","I also tried the export with the full model, which is working too, but the outputs are now incorrect.  Instead of predicting some text the model now only predicts _blank_ labels (normally the most common label), so it returns an empty text... There was no problem with the default quantization approach without the representative dataset (I tried different sizes between 20 and 2000 samples, resulting in slight changes of the softmax output, but no real changes of the argmax label). Do you have an idea about this? Is it common that _int8_ quantization can break the model?"," ! Regarding the Raspberry pi deployment and tflite_run_time 2.8 , You can check this repo. Can you check the wave to spectrogram  part of document mentioned in these documents 1 , 2 for debugging the prediction part? ","I think I could solve my problem with the full model. For melspectrogram and featurenormalization I'm adding very small values to the tensors to prevent _log_ and _div_ with zero, but with the _int8quantization_ those ""zeroguards"" are just set to zero themselves, so I'm getting _nan_ values inbetween... Do you know the smallest value I can use? Currently I'm using `2**5`, and `2**6` was to small. Thanks for the tip to the spectrogram debugging documents. You can find my complete debugging script here: https://gitlab.com/DANBER/Scribosermo//blob/cfc/extras/misc/complex_tflite_test.py Regarding the version problem, it seems that _2.8_ is only required for exporting. I also found that, presumably only in the case of the example script, using a larger number (>= 100) of representative examples decreases the performance of quantization. I think the effect could be smaller if there is a real model after the feature calculation. What do you think about this?"," ! Glad you sorted out the issue .  Try with ""1.001e5"" as the minimum value to avoid zero division error.  You can batch the data in the representative dataset if you are worried about performance( which seems to be implemented already in gitlab link). Please move this to closed status if it worked. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"The smallest value working as zeroguard for `log` was `2 ** 5`, smaller values sometimes resulted in `inf` results. The smallest value for `div` was `1/255`, smaller values sometimes resulted in `nan` results. Batching might work here, but I think it won't work for my real representative dataset, because the audio inputs have quite different lengths.",Are you satisfied with the resolution of your issue? Yes No
745,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Opening GUIs on host machine using the TensorFlow docker image)ï¼Œ å†…å®¹æ˜¯ (Hello everyone  I'm using TensorFlow gpu docker image and it is working just fine on my python scripts.  My issue is trying to open some GUI applications like spyderide. The _Dockerfile_ used to build the image is:  and the docker arguments used to start the image are  But I'm getting the following error...  Any ideas how could I open GUIs on the host machine using the TensorFlow docker image ? I tried reinstalling the application and getting these plugins but had no success in opening GUIs.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,caliari-italo,Opening GUIs on host machine using the TensorFlow docker image,Hello everyone  I'm using TensorFlow gpu docker image and it is working just fine on my python scripts.  My issue is trying to open some GUI applications like spyderide. The _Dockerfile_ used to build the image is:  and the docker arguments used to start the image are  But I'm getting the following error...  Any ideas how could I open GUIs on the host machine using the TensorFlow docker image ? I tried reinstalling the application and getting these plugins but had no success in opening GUIs.,2022-03-23T12:09:06Z,stat:awaiting response type:others comp:gpu,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55346,"italo , Can you please take a look at this link 1, 2 and 3 with the similar error.It helps.Thanks!",Unfortunately any of these approaches could solve my problem. I managed to open Firefox app from this docker image without problems but spyder is giving me a hard time with ,The problem was solved following  `https://github.com/spyderide/spyder/issues/17542`
1193,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Normalization and BatchNormalization layer does not rescale or normalize inputs, am I missing anything?)ï¼Œ å†…å®¹æ˜¯ (**System information** colab with tf2.8 oldtf_env.txt  **Describe the current behavior** Normalization and Batch Normalization layer does not rescale or normalize inputs as excepted, the inputs barely not changing.  **Standalone code to reproduce the issue**  Tensorflow:   Normalization layer  >tfdata: [[1. 2. 3. 4. 5.]] avg:  3.0 var:  2.0  **output:**  >[[1. 2. 3. 4. 5.]] normalized avg:  3.0 normalized var:  2.0 **This output is unchanged**  BatchNormalization layer  **output:** >[[0.9995004 1.9990008 2.9985013 3.9980016 4.997502 ]] batch normalized avg:  2.9985013 batch normalized var:  1.9980018  **This output still unchanged!**  Torch:  LayerNorm  **output** >[1.4142101  0.70710504 0.00000006  0.7071049   1.4142098 ] normalized avg:  4.7683717e08 normalized var:  0.9999949 normalized av closed to 0, and normalized var close to 1, correct answer. and BatchNormalization ... all the same)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jayagami,"Normalization and BatchNormalization layer does not rescale or normalize inputs, am I missing anything?","**System information** colab with tf2.8 oldtf_env.txt  **Describe the current behavior** Normalization and Batch Normalization layer does not rescale or normalize inputs as excepted, the inputs barely not changing.  **Standalone code to reproduce the issue**  Tensorflow:   Normalization layer  >tfdata: [[1. 2. 3. 4. 5.]] avg:  3.0 var:  2.0  **output:**  >[[1. 2. 3. 4. 5.]] normalized avg:  3.0 normalized var:  2.0 **This output is unchanged**  BatchNormalization layer  **output:** >[[0.9995004 1.9990008 2.9985013 3.9980016 4.997502 ]] batch normalized avg:  2.9985013 batch normalized var:  1.9980018  **This output still unchanged!**  Torch:  LayerNorm  **output** >[1.4142101  0.70710504 0.00000006  0.7071049   1.4142098 ] normalized avg:  4.7683717e08 normalized var:  0.9999949 normalized av closed to 0, and normalized var close to 1, correct answer. and BatchNormalization ... all the same",2022-03-23T07:09:20Z,stat:awaiting response type:bug comp:data TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55343," , Can you please elaborate about your issue here.Also please confirm which output you are mentioning as the right one in both? It helps to debug the issue.Thanks!",">  , Can you please elaborate about your issue here.Also please confirm which output you are mentioning as the right one in both? It helps to debug the issue.Thanks! Thanks for replying. Well,  both Normalization and BatchNormalization layer in tf are not working. according to BatchNormalization definition: !image In the outputs, the mean should be close to 0, and the variance should be close to 1. (ignore the gamma, betaâ€¦â€¦) However, as you can see, the outputs are unchanged in tensorflow. As a comparison, Torch gives the expected answer: >normalized avg: 4.7683717e08 normalized var: 0.9999949 By the way, Layer Normalization is working as excepted in tf 2.8. This is a very basic layer, and I doubt I'm missing something.","besides, this output is different with examples in  Normalization docs  the outputs: >","I checked the code, could be a keras problem. The call method is quite short, seems mean and variance are never calculated https://github.com/kerasteam/keras/blob/v2.8.0/keras/layers/preprocessing/normalization.pyL300L306  and the mean is initialized as 0, variance is initialized as 1, unless you manually set it. https://github.com/kerasteam/keras/blob/v2.8.0/keras/layers/preprocessing/normalization.pyL244L298","I rechecked the documentation and found it right in front of me, surprised I hadn't noticed this paragraph before. I've used BatchNormalization with unprocessed data and got unexpected results, which is confusing me. I focus on the Normalization issue and ignore the differences in their implementation. I understand the need for this design, which is very different from torch. I should have paid more attention to these details.",Are you satisfied with the resolution of your issue? Yes No
1856,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Autovectorization fail with tf.vectorized_map and range)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:   TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): Nightly  Python version: Colab  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior**   **Describe the expected behavior** **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and fi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bhack,Autovectorization fail with tf.vectorized_map and range,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:   TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): Nightly  Python version: Colab  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior**   **Describe the expected behavior** **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and fi",2022-03-22T21:46:45Z,stat:awaiting response type:bug comp:ops,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55340,Just to expose a few reformulations it seems that we have some side effect with `tf.vectorized_map` in an autovectorizzation region. This is working correctly instead:  This is failing   This is failing:  ` Moving Autovectorizzation to execlude `tf.vectorized_map`: This is working  This is working:  This is working:  / ,P.s. just a note the `tf.vectorized_map` example was derived from official example in docstring: https://www.tensorflow.org/api_docs/python/tf/vectorized_mapexamples,"Looking at the detailed logs, this raw error points at the culprit:  The failure happens in pure Python code, which means that `_pfor_impl` fails to call autograph. The solution should be relatively straightforward, there is an internal `tf_convert` call designed to do the right thing (i.e. apply autograph if enabled in the parent function, ensure the error message is accurate, etc.). I think that's the reason why the error message is incorrect (it says that ""AutoGraph did convert this function"", when it in fact didn't). A good example of how `tf_convert` is called can be found in the `tf.data` code: https://github.com/tensorflow/tensorflow/blob/0a8a781e597b18ead006d19b7d23d0a369e9ad73/tensorflow/python/data/ops/structured_function.pyL177",Thanks for the hint /," Can we close this issue, since associated PR got merged. Thanks!",Are you satisfied with the resolution of your issue? Yes No
1815,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow installation in venv results in RuntimeError when importing the library)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Using pip  TensorFlow version: Latest (cannot determine version after installation)  Python version: 3.8.10  Installed using virtualenv? pip? conda?: Using pip, inside venv  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the problem** When trying to install Tensorflow using pip inside a virtual environment, installation completes, but importing the library results in a RuntimeError. **Provide the exact sequence of commands / steps that you executed before running into the problem** 1. `python3 m venv venv` 2. `. venv/bin/activate` 3. `python3 m pip install tensorflow` 4. `python3 c ""import tensorflow as tf; print(tf.__version__)""` **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. First 3 steps work as expected, 4th step results in the following errors:  I have tried manually upgrading numpy, but that didn't resolve my issue.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,andrei-micuda,Tensorflow installation in venv results in RuntimeError when importing the library,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Using pip  TensorFlow version: Latest (cannot determine version after installation)  Python version: 3.8.10  Installed using virtualenv? pip? conda?: Using pip, inside venv  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the problem** When trying to install Tensorflow using pip inside a virtual environment, installation completes, but importing the library results in a RuntimeError. **Provide the exact sequence of commands / steps that you executed before running into the problem** 1. `python3 m venv venv` 2. `. venv/bin/activate` 3. `python3 m pip install tensorflow` 4. `python3 c ""import tensorflow as tf; print(tf.__version__)""` **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. First 3 steps work as expected, 4th step results in the following errors:  I have tried manually upgrading numpy, but that didn't resolve my issue.",2022-03-22T21:04:00Z,type:build/install subtype: ubuntu/linux,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55338,"Finally got a solution. Not sure what I did different, but here are the steps that worked: 1. `python3 m venv systemsitepackages ./venv` 2. `source ./venv/bin/activate` 3. `pip install upgrade pip` 4. `pip install upgrade tensorflow` After that, the previous import step worked.",Are you satisfied with the resolution of your issue? Yes No
1095,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(undefined references on tflite x86_64 cross compiled with NDK Android toolchain)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: Linux Ubuntu 21.04  TensorFlow version: 2.8.0  GCC/Compiler version: Clang 9.0.8, from Android NDK 21.3.6528147 **Describe the problem** I'm able to build `libtensorflowlite.a` for Android x86_64, but when I try to link the library I get  If I build the lib without XNN delegate support, I got an error on NNAPI. If also disable NNAPI, I got undefined reference errors for ryu related functions. It seems like that all the delegate symbols in the built static library are undefined. **Provide the exact sequence of commands / steps that you executed before running into the problem** to build  `libtensorflowlite.a`:  later linked using CMake on an Android project with `target_link_libraries(...)`. **Any other info / logs** output of `nm C libtensorflowlite.a | grep xnn`: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,mtamburrano,undefined references on tflite x86_64 cross compiled with NDK Android toolchain,"**System information**  OS Platform and Distribution: Linux Ubuntu 21.04  TensorFlow version: 2.8.0  GCC/Compiler version: Clang 9.0.8, from Android NDK 21.3.6528147 **Describe the problem** I'm able to build `libtensorflowlite.a` for Android x86_64, but when I try to link the library I get  If I build the lib without XNN delegate support, I got an error on NNAPI. If also disable NNAPI, I got undefined reference errors for ryu related functions. It seems like that all the delegate symbols in the built static library are undefined. **Provide the exact sequence of commands / steps that you executed before running into the problem** to build  `libtensorflowlite.a`:  later linked using CMake on an Android project with `target_link_libraries(...)`. **Any other info / logs** output of `nm C libtensorflowlite.a | grep xnn`: ",2022-03-22T18:48:46Z,stat:awaiting response type:build/install stale comp:lite TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55336,Hi  ! Sorry for the late response. Could you rebuild  your Cmake project with below changes and let us know the difference? ,"hi   I'm sorry but I'm not sure what are you trying to suggest. My project is an Android app, that uses some JNI cpp modules that I build through CMake, but I don't see the point to add `DTFLITE_ENABLE_XNNPACK=ON` there because of course that CMakeList is not aware on that flag. Instead, if you wanted to make me to build tflite for `arm64v8a` arch instead than for `x86_64` like I was doing previously, I tried but the outcome is the same, I got a lot of undefined symbols in the built `libtensorflowlite.a` that then cause undefined reference when I try to link that library in my android project","I managed to figure out what the issue is. I was supposing that after building all the third party dependencies, all the static libraries would have been combined in a single one (libetensorflowlite.a). Instead there are a bunch of libraries that need to be linked in the `_deps/` folder. Is it not possible to produce a single fat static library with everything needed to link an executable, instead having to rely to all those different *.a files? I can produce it by merging all the libraries together with AR commands, but I wonder if this functionality is already supported somehow by the current CMakeLists"," ! you can add a static library using add_library (..)and use STATIC keyword along with desired libraries in it. Attaching relevant thread for reference. 1, 2 . Feel free to move this to closed status if it helped .Thanks!  ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No," I've combined all the libraries in /_deps into one folder, the number of errors reduced, but there are still 60 errors. for example, have you met it?"
1866,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AttributeError: 'arguments' object has no attribute 'posonlyargs')ï¼Œ å†…å®¹æ˜¯ (summary: while running locally the efficient net tutorial, the logs showed and error and asked to report to the tf team. ""AttributeError: 'arguments' object has no attribute 'posonlyargs' WARNING:tensorflow:AutoGraph could not transform .get_dataset.. at 0x7ff6443d2e60> and will run it asis. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 'arguments' object has no attribute 'posonlyargs'"" **System information**  Code is an official example of how to run efficientnet:(https://github.com/tensorflow/tpu/blob/master/models/official/detection/GETTING_STARTED.md)   OS Platform Linux Ubuntu 20.04, Lenovo T14, no gpu.  TensorFlow installed from binary.  TensorFlow version v2.5.0rc3213ga4dfb8d1a71 2.5.0.  Python version 3.8 log: `tpu/models/official/detection/main.py model=retinanet model_dir=/home/dannyb/Documents/Research/copy_paste/model_output mode=train eval_after_training=True use_tpu=False ""params_override={ train: { checkpoint: { path: gs://cloudtpucheckpoints/retinanet/resnet50checkpoint20180207/model.ckpt112603, prefix: resnet50/ }, train_file_pattern: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco }, eval: { val_json_file: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco/rawdata/annotations/image_info_testdev2017.json, eval_file_pattern: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco } }"" Connected to pydev debugger (build 213.7172.26) 20220322 13:26:39.711187: E tensorflow/stream_executor/cuda/cuda_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,dannybarash7,AttributeError: 'arguments' object has no attribute 'posonlyargs',"summary: while running locally the efficient net tutorial, the logs showed and error and asked to report to the tf team. ""AttributeError: 'arguments' object has no attribute 'posonlyargs' WARNING:tensorflow:AutoGraph could not transform .get_dataset.. at 0x7ff6443d2e60> and will run it asis. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 'arguments' object has no attribute 'posonlyargs'"" **System information**  Code is an official example of how to run efficientnet:(https://github.com/tensorflow/tpu/blob/master/models/official/detection/GETTING_STARTED.md)   OS Platform Linux Ubuntu 20.04, Lenovo T14, no gpu.  TensorFlow installed from binary.  TensorFlow version v2.5.0rc3213ga4dfb8d1a71 2.5.0.  Python version 3.8 log: `tpu/models/official/detection/main.py model=retinanet model_dir=/home/dannyb/Documents/Research/copy_paste/model_output mode=train eval_after_training=True use_tpu=False ""params_override={ train: { checkpoint: { path: gs://cloudtpucheckpoints/retinanet/resnet50checkpoint20180207/model.ckpt112603, prefix: resnet50/ }, train_file_pattern: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco }, eval: { val_json_file: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco/rawdata/annotations/image_info_testdev2017.json, eval_file_pattern: /home/dannyb/Documents/Research/copy_paste/cocoapi/PythonAPI/data/coco } }"" Connected to pydev debugger (build 213.7172.26) 20220322 13:26:39.711187: E tensorflow/stream_executor/cuda/cuda_",2022-03-22T11:47:11Z,stat:awaiting response type:bug stale comp:autograph TF 2.5,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55325," , Can you please take a look at this link 1 and 2 with the similar error.It helps.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Auto comment generation project - model training loop crash)ï¼Œ å†…å®¹æ˜¯ (Trying to train a model in an automatic comment generation project, the training loop throws an exception. This error has occured with 3 combinations of python/tensorflow versions, which I note below as (1), (2) and (3), as well as on both a conda env in Powershell and in the WSL Ubuntu command line. It also happens on 2 different machines, both with the official (https://github.com/techsrl/code2seq) and unofficial (https://github.com/Kolkir/code2seq) versions of the project.  **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not sure if the project devs did, I assume not.  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64, Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 1.12 (1), 2.8.0 (2), 2.8.0 (3)  Python version: 3.6.5 (1), 3.9.7 (2), 3.8.10 (3)  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: RTX 2080 SUPER 8GB, but N/A **Describe the current behavior** Running the training loop with `bash train.sh` both in Powershell (conda) and Ubuntu WSL command line :  Code snippet being triggered in module.py of tensorflow :  **Describe the expected behavior** The traning loop should terminate without errors. **Standalone code to reproduce the issue** Best I can do is give the project git:  https://github.com/techsrl/code2seq (official version, tens)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gbaulard,Auto comment generation project - model training loop crash,"Trying to train a model in an automatic comment generation project, the training loop throws an exception. This error has occured with 3 combinations of python/tensorflow versions, which I note below as (1), (2) and (3), as well as on both a conda env in Powershell and in the WSL Ubuntu command line. It also happens on 2 different machines, both with the official (https://github.com/techsrl/code2seq) and unofficial (https://github.com/Kolkir/code2seq) versions of the project.  **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not sure if the project devs did, I assume not.  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64, Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 1.12 (1), 2.8.0 (2), 2.8.0 (3)  Python version: 3.6.5 (1), 3.9.7 (2), 3.8.10 (3)  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: RTX 2080 SUPER 8GB, but N/A **Describe the current behavior** Running the training loop with `bash train.sh` both in Powershell (conda) and Ubuntu WSL command line :  Code snippet being triggered in module.py of tensorflow :  **Describe the expected behavior** The traning loop should terminate without errors. **Standalone code to reproduce the issue** Best I can do is give the project git:  https://github.com/techsrl/code2seq (official version, tens",2022-03-22T11:46:02Z,stat:awaiting response type:support stale comp:model TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55324,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!","Here's a link with the minimal amount of files to launch the training : https://we.tl/tlrwIEVS9Dw Run this in a terminal at the root with python 3.6+, tensorflow 2.1+ installed. `python u code2seq.py data data/javasmall/javasmall test data/javasmall/javasmall.val.c2s save_path models/javasmallmodel/model`",  Could you please  open this issue in TF discussion forum as there is a larger community  to get you the right help. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"I've got the same error, have you fixed it?  "
710,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TFLite] Failed to build iOS framework: thread-local storage is not supported for the current target)ï¼Œ å†…å®¹æ˜¯ (Hi I use the official guide to build a framework for iOS, however, when I run this command:  I'm getting this error:  My System Info: + tensorflow: from master branch, the latest code + bazel: 5.0.0 I think a similar issue is CC(Failed to build for iOS using Xcode 9.3: threadlocal storage is not supported for the current target), however the solution didn't work for me, perhaps it works for tensorflow version below 2.0.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,BILLXZY1215,[TFLite] Failed to build iOS framework: thread-local storage is not supported for the current target,"Hi I use the official guide to build a framework for iOS, however, when I run this command:  I'm getting this error:  My System Info: + tensorflow: from master branch, the latest code + bazel: 5.0.0 I think a similar issue is CC(Failed to build for iOS using Xcode 9.3: threadlocal storage is not supported for the current target), however the solution didn't work for me, perhaps it works for tensorflow version below 2.0.",2022-03-22T11:04:32Z,stat:awaiting response type:build/install subtype:macOS subtype:bazel,closed,2,6,https://github.com/tensorflow/tensorflow/issues/55322,", Please post this issue on Eigen repo for faster resolution: https://gitlab.com/libeigen/eigen/issues","I had the same issue. After so many trials and continuous errors, I've finally generated the framework simply by updating  TFL_MINIMUM_OS_VERSION = ""12.0"" from ""9.0"" in path 'tensorflow/lite/ios/ios.bzl' file."," , Could you try the above solution and let us know the outcome. Thanks!","Solution proposed by  worked for me with min version 12.0 and even 10.0.      I built the static lib from commit b6585b6a2e4cecde3650978354bb30bbcec9d559 using bazel, individually for all architectures compatible with iOS 10 (`arm64`/`armv7` for iOS and `x86_64`/`i386`/`arm64` for iOS simulator) with bitcode activated.",">  , Could you try the above solution and let us know the outcome. Thanks! Yes, it worked for me, thanks!",Are you satisfied with the resolution of your issue? Yes No
596,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distributed training with tensorflow on 'x' gpu makes loss 1/x)ï¼Œ å†…å®¹æ˜¯ (I was trying to run a model on multiple gpu with `mirror strategy` of tensorflow. I used a custom loss function like this:  with mirror strategy I train the model like this:  But if I run this on 4 GPU, my loss value becomes 1/4 times than the loss I get when run on single GPU. Does it fail to sum up the different losses from the different GPUs?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,st3inum,distributed training with tensorflow on 'x' gpu makes loss 1/x,"I was trying to run a model on multiple gpu with `mirror strategy` of tensorflow. I used a custom loss function like this:  with mirror strategy I train the model like this:  But if I run this on 4 GPU, my loss value becomes 1/4 times than the loss I get when run on single GPU. Does it fail to sum up the different losses from the different GPUs?",2022-03-22T10:24:34Z,stat:awaiting response comp:keras,closed,0,1,https://github.com/tensorflow/tensorflow/issues/55321,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thanks!
585,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Redundant `create_padding_mask(inp)` masks)ï¼Œ å†…å®¹æ˜¯ (Hello,   in your tutorial about Transormer I think there are redundant `create_padding_mask(inp)` masks. The same mask is created for Encoder and Decoder too. Please look here:  It's the same padding mask created from `inp`. It's memory inefficient. Link: https://www.tensorflow.org/text/tutorials/transformercreate_the_transformer Thanks. Have a nice day.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,markub3327,Redundant `create_padding_mask(inp)` masks,"Hello,   in your tutorial about Transormer I think there are redundant `create_padding_mask(inp)` masks. The same mask is created for Encoder and Decoder too. Please look here:  It's the same padding mask created from `inp`. It's memory inefficient. Link: https://www.tensorflow.org/text/tutorials/transformercreate_the_transformer Thanks. Have a nice day.",2022-03-21T17:25:52Z,type:others comp:model TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/55313,Hi  ! Could you please post this text repository?,"Yes, I can create issue in tensorflow/text. Thanks.",https://github.com/tensorflow/text/issues/863,Hi  ! Could you please look at this issue?,", `dec_padding_mask` is expected. This padding mask is used to mask the encoder outputs.  dec_padding_mask is for input and target language  `dec_padding_mask = self.create_masks(inp, tar)`",  Now the snippet look like here:  The same thing with lower memory consumption and better way is: ,", Would you like to contribute to this issue?  Or any suggestion or workaround to fix this issue.Thanks! ", Yes I can create PR.," I create PR at https://github.com/tensorflow/text/pull/869, is it OK? Thanks.",", Thanks for the PR. "
1631,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DeeplabV3 custom dataset, inference problem black images)ï¼Œ å†…å®¹æ˜¯ (Good morning, I want to train a custom dataset using deeplabV3. I'm following this tutorial (https://sanjayparajuli27.medium.com/howtotraindeeplaboncustomdataseta40c41c4c6a3) for this dataset (https://www.kaggle.com/datasets/dansbecker/cityscapesimagepairs) that I found on Kaggle, based on cityscapes. There are images of 256x256 pixel in RGB colors, divided in 2975 imgs for training and 500 for validation, and I created the respective mask using this script  Each image in the dataset contain its same mask, so before to launch the new notebook I divided the image and the mask to have a situation like in the tutorial. You can find my code here: https://drive.google.com/drive/folders/105JMDmujY6lknH3D74WM8R8S7jTb51qX?usp=sharing and this is the notebook https://drive.google.com/file/d/1xmUtLBXPj4mZdqbx9SAQOXKSwxtxCLX/view?usp=sharing I have a problem with the inference. Every time I launch the notebook with few epochs (less then 10) I receive good results, but trying to increase the number of epoch I have all black images. These are the parameters that I used for the train:  I edited the data_generator.py file putting  I'm using a pretrained model downloaded from here: http://download.tensorflow.org/models/deeplabv3_cityscapes_train_2018_02_06.tar.gz I keep the batch size at 4, and I don't know if it is correct or not. Can you tell me where could be the possible error?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,paolodavid,"DeeplabV3 custom dataset, inference problem black images","Good morning, I want to train a custom dataset using deeplabV3. I'm following this tutorial (https://sanjayparajuli27.medium.com/howtotraindeeplaboncustomdataseta40c41c4c6a3) for this dataset (https://www.kaggle.com/datasets/dansbecker/cityscapesimagepairs) that I found on Kaggle, based on cityscapes. There are images of 256x256 pixel in RGB colors, divided in 2975 imgs for training and 500 for validation, and I created the respective mask using this script  Each image in the dataset contain its same mask, so before to launch the new notebook I divided the image and the mask to have a situation like in the tutorial. You can find my code here: https://drive.google.com/drive/folders/105JMDmujY6lknH3D74WM8R8S7jTb51qX?usp=sharing and this is the notebook https://drive.google.com/file/d/1xmUtLBXPj4mZdqbx9SAQOXKSwxtxCLX/view?usp=sharing I have a problem with the inference. Every time I launch the notebook with few epochs (less then 10) I receive good results, but trying to increase the number of epoch I have all black images. These are the parameters that I used for the train:  I edited the data_generator.py file putting  I'm using a pretrained model downloaded from here: http://download.tensorflow.org/models/deeplabv3_cityscapes_train_2018_02_06.tar.gz I keep the batch size at 4, and I don't know if it is correct or not. Can you tell me where could be the possible error?",2022-03-21T16:26:10Z,stat:awaiting response type:support comp:model,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55312,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!",Are you satisfied with the resolution of your issue? Yes No,Are you satisfied with the resolution of your issue? Yes No
1854,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Node: 'model/conv1d/Conv1D' DNN library is not found.)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS: Linux Ubuntu 20.04:  TensorFlow installed from pip  TensorFlow version: v2.8.0rc132g3f878cff5b6 2.8.0  Python version: Python 3.8.10  CUDA/cuDNN version: NVIDIASMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     GPU model : [GeForce GTX 1650]  Hello, everyone. I'm trying to run a  convolutional neural network on tensorflow but I'm receiving the current error: > 20220321 10:40:20.665473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero > 20220321 10:40:20.687599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero > 20220321 10:40:20.687804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero > 20220321 10:40:22.544819: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 FMA > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. > 20220321 10:40:22.545204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUM)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,caliari-italo,Node: 'model/conv1d/Conv1D' DNN library is not found.,"**System information**  OS: Linux Ubuntu 20.04:  TensorFlow installed from pip  TensorFlow version: v2.8.0rc132g3f878cff5b6 2.8.0  Python version: Python 3.8.10  CUDA/cuDNN version: NVIDIASMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     GPU model : [GeForce GTX 1650]  Hello, everyone. I'm trying to run a  convolutional neural network on tensorflow but I'm receiving the current error: > 20220321 10:40:20.665473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero > 20220321 10:40:20.687599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero > 20220321 10:40:20.687804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero > 20220321 10:40:22.544819: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX2 FMA > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. > 20220321 10:40:22.545204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUM",2022-03-21T13:56:05Z,stat:awaiting response type:bug comp:gpu TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55309,Hi italo ! Could you switch to Cuda 11.2 and CuDNN 8.1 and let us know the results ? Attaching tested configuration for reference. Thanks!,Unfortunately it didn't worked. The error is still the same even changing Cuda and CuDNN.,Ok. Please use 2.8.0 stable version with Cuda 11.2 and CudNN 8.1 . Are you loading Pytorch and Tensorflow simultaneously as this thread suggests? Could you provide a simple stand alone code to reproduce this issue too?,I ended up using the TensorFlow Docker image and it worked just fine. https://www.tensorflow.org/install/docker,italo ! Shall we move this issue to closed status then?,Are you satisfied with the resolution of your issue? Yes No
1884,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TPU] TPUClusterResolver Can't Resolve TPU Metadata When Using Regional GKE Cluster)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:2.7.1, tensorflow/tensorflow:2.8.0  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.1, 2.8.0  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: cloudtpus.google.com/preemptiblev2: ""8"" **Describe the current behavior** We have a GKE regional cluster with TPU support. When I tried to create a ResNetRS using the official TPU guide, I encountered the issue with TPUClusterResolver.  **Describe the expected behavior** The ResNetRS job should run in TPU core without any issues. **Contributing**  Do you want to contribute a PR? (yes/no): No  Briefly describe your candidate solution(if contributing): N/A **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. https://cloud.google.com/tpu/docs/tutorials/resnetrs2.x Run this guide in a regional GKE cluster with TPU support. **Other info / logs** Include any logs or source cod)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ZeroExistence,[TPU] TPUClusterResolver Can't Resolve TPU Metadata When Using Regional GKE Cluster,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:2.7.1, tensorflow/tensorflow:2.8.0  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.1, 2.8.0  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: cloudtpus.google.com/preemptiblev2: ""8"" **Describe the current behavior** We have a GKE regional cluster with TPU support. When I tried to create a ResNetRS using the official TPU guide, I encountered the issue with TPUClusterResolver.  **Describe the expected behavior** The ResNetRS job should run in TPU core without any issues. **Contributing**  Do you want to contribute a PR? (yes/no): No  Briefly describe your candidate solution(if contributing): N/A **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. https://cloud.google.com/tpu/docs/tutorials/resnetrs2.x Run this guide in a regional GKE cluster with TPU support. **Other info / logs** Include any logs or source cod",2022-03-21T11:04:30Z,stat:awaiting response type:bug comp:tpus comp:cloud TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55307,"Okay, I just realized that I can use the endpoint IP directly, instead of name. Just change the `tpu=$TPU_NAME` to `tpu=$KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS`. It works on my side now within GKE regional cluster. I will just leave this ticket as it is, if it is needed to troubleshoot the TPUClusterResolver. Thank you!"," , Can you please take a look at this issue link with the similar error.It helps.",Also please move this issue to closed status as it has been resolved.Thanks!,Thank you  for checking! Moving this issue to close.,Are you satisfied with the resolution of your issue? Yes No
1883,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Code under tf.init_scope behaves differently than code manually lifted from graph.)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab with TPU  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0  Python version: 3.7.12  CUDA/cuDNN version:   GPU model and memory: TPUv2 **Describe the current behavior** I'm trying to implement sharding algorithm from paper https://arxiv.org/pdf/2010.05222.pdf. To emulate sharded variable I wrote wrappers for SaveableObject and tf.Variable with tf.VariableSynchronization.ON_READ and tf.VariableAggregation.NONE. At the restore from checkpoint moment I need to run Variable.assign in replica context to assign different values to shards on different TPU replicas. SaveableObject.restore called under tf.init_scope: train_function > step_function > train_step > optimizer.minimize > apply_gradients > tf.init_scope > optimizer._create_all_weights > _create_slots > add_slot > _restore_slot_variable > restore. There are two problems with tf.init_scope: 1) ReplicaContext.replica_id_in_sync_group is unusable. Attempt to use it leads to an error ""TypeError:  is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it."". This colab demonstrates the issue https://colab.research.google.com/drive/1BjRHdrWG9mM75zHe43rpOlppfjE0kta?usp=sharing 2) Strategy.run (after merge_call) leads to an error ""NotImplementedError: tpu_shard_context cannot be nested.If you're using TPUEstimator with inference_on_tpu, make sure yo)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,API92,Code under tf.init_scope behaves differently than code manually lifted from graph.,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab with TPU  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0  Python version: 3.7.12  CUDA/cuDNN version:   GPU model and memory: TPUv2 **Describe the current behavior** I'm trying to implement sharding algorithm from paper https://arxiv.org/pdf/2010.05222.pdf. To emulate sharded variable I wrote wrappers for SaveableObject and tf.Variable with tf.VariableSynchronization.ON_READ and tf.VariableAggregation.NONE. At the restore from checkpoint moment I need to run Variable.assign in replica context to assign different values to shards on different TPU replicas. SaveableObject.restore called under tf.init_scope: train_function > step_function > train_step > optimizer.minimize > apply_gradients > tf.init_scope > optimizer._create_all_weights > _create_slots > add_slot > _restore_slot_variable > restore. There are two problems with tf.init_scope: 1) ReplicaContext.replica_id_in_sync_group is unusable. Attempt to use it leads to an error ""TypeError:  is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it."". This colab demonstrates the issue https://colab.research.google.com/drive/1BjRHdrWG9mM75zHe43rpOlppfjE0kta?usp=sharing 2) Strategy.run (after merge_call) leads to an error ""NotImplementedError: tpu_shard_context cannot be nested.If you're using TPUEstimator with inference_on_tpu, make sure yo",2022-03-20T18:04:27Z,stat:awaiting response type:bug stale comp:tpus comp:ops TF 2.8,closed,0,10,https://github.com/tensorflow/tensorflow/issues/55303," ,  In order to expedite the troubleshooting process, could you please provide  code dependencies you are using as I was facing error to provide `input for the checkpoints`.Thanks!"," , you should enter path to your bucket in Google Cloud Storage, because tensorflow doesn't support storing checkpoints locally on TPU. This folder may be empty. For example, I used this gs://dc795ab990a24cb9b1ddbadcf556350b/init_scope_replica_id , but it deleted now. There is no other code dependencies for notebooks."," , Unfortunately we can reproduce the issue only in colab. Can you please provide the required checkpoints which are required for avoiding the above mentioned error.It helps to debug the issue.Thanks!","model.zip This checkpoints generated by notebooks itself in first call `train(strategy, False)` and used in second call `train(strategy, True)`. There is no need to use external checkpoints. But here is those checkpoints in attached model.zip.\ Don't forget to give ""Storage Admin"" role to ""allUsers"" in permissions of your bucket to use it with TPU, else you'll get access error. As you understand I can't give you my bucket because publishing bucket with read/write access to any user in public discussion would be too expensive for me.","TPUEstimator will be deprecated, could you please use TPUStrategy instead and report the problem if you still face an error. Refer the migration guide here https://www.tensorflow.org/guide/migrate/tpu_estimator . Thanks!","I am using TPUStrategy. There is no TPUEstimator in my code. The word ""TPUEstimator"" only appears in the exception message thrown by TensorFlow in what I consider to be a bug.","Is this still an issue? If so, I can offer some advice on this matter. First, we recommend sharding variables via XLA. Using TPUStrategy, the relevant builtin code pointers are `experimental_spmd_xla_partitioning`, `experimental_split_to_logical_devices`, and `TPUReplicatedVariable`; the latter is what one could extend to add a sharded variable type. Second, the ""cannot be accessed from here"" error may be resolved by mirroring the code here. Suffice to say, there are complex graph interactions surrounding TPU replication; one must move operations like `TPUReplicatedInput` and `TPUPartitionedInput(V2)` to the `_enclosing_tpu_context_and_graph`.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1134,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Installation Issue - OSError: [Errno 2] No such file or directory: ...\\BufferizableOpInterface.cpp.inc)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: Windows 11 Professional   TensorFlow installed from: binary (`pip install tensorflow`)  TensorFlow version: 2.8.0  Python version: 3.10.3  Installed using virtualenv: `pip`  CUDA/cuDNN version: 11.6  GPU model and memory: NVIDIA RTX A2000 / RAM 32 GB Trying to install tensorflow on my PC (system described above) and found an issue as follows at the end of `pip install tensorflow`:  `tfenv` is the name of the virtual environment I've created using the command `python m venv systemsitepackages tfenv`. I've installed the prerequisite (Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019) from this link. I'm guessing the error is somehow related to the MS Visual C++ or the virtual environment, but I'm not sure how. Could you help me please? Thank you in advance.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ahmad-alkadri,Installation Issue - OSError: [Errno 2] No such file or directory: ...\\BufferizableOpInterface.cpp.inc,"**System information**  OS Platform and Distribution: Windows 11 Professional   TensorFlow installed from: binary (`pip install tensorflow`)  TensorFlow version: 2.8.0  Python version: 3.10.3  Installed using virtualenv: `pip`  CUDA/cuDNN version: 11.6  GPU model and memory: NVIDIA RTX A2000 / RAM 32 GB Trying to install tensorflow on my PC (system described above) and found an issue as follows at the end of `pip install tensorflow`:  `tfenv` is the name of the virtual environment I've created using the command `python m venv systemsitepackages tfenv`. I've installed the prerequisite (Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019) from this link. I'm guessing the error is somehow related to the MS Visual C++ or the virtual environment, but I'm not sure how. Could you help me please? Thank you in advance.",2022-03-20T06:56:45Z,stat:awaiting response type:build/install subtype:windows TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55300,Similar to CC(Cannot Install Tensorflow on any version of Python 3.8),Hi alkadri ! Can you check this thread as workaround and let us know the results?  Thank you!,> Hi alkadri ! Can you check this thread as workaround and let us know the results? Thank you! Alright the installation succeeded! You're right the problem is the character limit on the path. Deactivating this limit on Windows is the solution. Thanks again!,Are you satisfied with the resolution of your issue? Yes No,å·²æ”¶åˆ°ï¼Œæˆ‘ä¼šå°½å¿«å›å¤ä½ ï¼ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  â€”â€”åˆ˜åŠ²å­—
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Using Numpy BitGenerator with TF-1.15 (?))ï¼Œ å†…å®¹æ˜¯ (Hi I am running minigo code from here. Although TF1.15 and Numpy 1.17.3 are installed, I get the following error  Any idea about that? Am I missing something?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mahmoodn,Using Numpy BitGenerator with TF-1.15 (?),"Hi I am running minigo code from here. Although TF1.15 and Numpy 1.17.3 are installed, I get the following error  Any idea about that? Am I missing something?",2022-03-19T15:16:21Z,stat:awaiting response type:others TF 1.15,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55299,", We see that you are using old version of tensorflow (1.x) which is not actively supported. We recommend that you upgrade to 2.8.0 and let us know if the issue still persists in newer versions .Thanks! Can you take a look at this link1, link2 which discusses about the similar issue? It may help you. Thanks!","OK. Based on the solution proposed in link2, `np.random.bit_generator.BitGenerator` fixed the error. Thanks.",Are you satisfied with the resolution of your issue? Yes No
1863,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(org.tensorflow:tensorflow-lite-support:0.3.1 has missing files)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below):  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** When trying to deploy a TFLite model in Android Studio, I received the error ClassNotFound: about org.tensorflow.lite.support.common.SupportPreconditions. **Describe the expected behavior** No error. **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. modelPath = ""local_path_of_model"" AudioClassifi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,IoanaKitsune,org.tensorflow:tensorflow-lite-support:0.3.1 has missing files,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below):  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** When trying to deploy a TFLite model in Android Studio, I received the error ClassNotFound: about org.tensorflow.lite.support.common.SupportPreconditions. **Describe the expected behavior** No error. **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. modelPath = ""local_path_of_model"" AudioClassifi",2022-03-19T13:28:17Z,stat:awaiting response type:bug type:build/install comp:lite,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55298," , Can you please refer this link which helps to upgrade TFLite Support 0.3.1. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,I face the same error and i figure out the problem. Actually the problem is `SupportPreconditions.java` is in `/org/tensorflow/lite/support/common/internal/SupportPreconditions.java` not in `/org/tensorflow/lite/support/common/SupportPreconditions.java` With lastest version of `tensorflowlitesupport:0.3.1` And this cause a problem with `implementation 'org.tensorflow:tensorflowlitetaskaudio:0.3.0'` And here's the full stacktrace:  I hope you understand me Thanks in advance.,"Oh, I see, thank you so much. I temporarily downgraded and it worked, but I think your solution helps a lot! On Fri, Apr 1, 2022, 03:03 Agono0 ***@***.***> wrote: > I face the same error and i figure out the problem. > Actually the problem is SupportPreconditions.java is in > /org/tensorflow/lite/support/common/internal/SupportPreconditions.java > not in /org/tensorflow/lite/support/common/SupportPreconditions.java > With lastest version of tensorflowlitesupport:0.3.1 > And this cause a problem with implementation > 'org.tensorflow:tensorflowlitetaskaudio:0.3.0' > And here's the full stacktrace: > > java.lang. NoClassDefFoundError: Failed resolutionof:Lorg/tensorflow/lite/support/common/SupportPreconditions;atorg.tensorflow.lite.task.audio.classifier.AudioClassifier.createAudioRecord (AudioClassifier.java:473)at com.test.noplv.MainActivity.onCreate (MainActivity.java:45)at android.app.Activity.performCreate(Activity.java:7335)atandroid.app.Activity.performCreate (Activity.java:7326)atandroid.app.Instrumentation.callActivityOnCreate(Inst rumentation.java:1275) atandroid.app.ActivityThread.performLaunchActivity (ActivityThread.java:3119) > atandroid.app.ActivityThread.handleLaunchActivity (ActivityThread.java:3282)atandroid.app.servertransaction.LaunchActivityItem.exec ute (LaunchActivityItem.java:78)atandroid.app.servertransaction.TransactionExecutor.exe cuteCallbacks (TransactionExecutor.java:108) atandroid.app.servertransaction.Transaction Executor.exe cute(TransactionExecutor.java:68)atandroid.app.Activity Thread$H.handleMessage (ActivityTh read.java:1970)atandroid.os.Handler.dispatchMessage(Handler.java:106)at android.os.Looper.loop (Looper.java:214) atandroid.app.ActivityThread.main(ActivityThread.java: 7156) at java.lang.reflect.Method.invoke (Native Method)atcom.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:494) > > I hope you understand me > Thanks in advance. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >"," , Could you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status ",Are you satisfied with the resolution of your issue? Yes No,"Properly not solved for me, any help. I describe the issue . But I didn't solve it",I must modify the library files or what? Then if yes. how l can move `SupportPreconditions.java` then?
1221,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(how to change tensorflow version if build from source?)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 lts  TensorFlow installed from (source or binary): source  TensorFlow version: master  Python version: 3.10.2  Bazel version (if compiling from source): 5.0.0  GCC/Compiler version (if compiling from source): 9.3.0  CUDA/cuDNN version: 11.2/8.1  GPU model and memory: RTX 3090 **Describe the problem** I am trying to build tensorflow from source. Everything works good and I successfully build it based on master branch.  After I get the `.whl` package, I installed it and checked the version number. I get the following output:  I get the version number as `'2.9.0'`. it makes sense as I build based on master branch and the current release is `2.8.0`. However, I wish to customize the version number, for example, as `2.9.0xxx` so that I know what features I added into this build. I thought there should be a setup file or something I can change but I cannot find it. Any ideas? Thx!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,WingsOfPanda,how to change tensorflow version if build from source?,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 lts  TensorFlow installed from (source or binary): source  TensorFlow version: master  Python version: 3.10.2  Bazel version (if compiling from source): 5.0.0  GCC/Compiler version (if compiling from source): 9.3.0  CUDA/cuDNN version: 11.2/8.1  GPU model and memory: RTX 3090 **Describe the problem** I am trying to build tensorflow from source. Everything works good and I successfully build it based on master branch.  After I get the `.whl` package, I installed it and checked the version number. I get the following output:  I get the version number as `'2.9.0'`. it makes sense as I build based on master branch and the current release is `2.8.0`. However, I wish to customize the version number, for example, as `2.9.0xxx` so that I know what features I added into this build. I thought there should be a setup file or something I can change but I cannot find it. Any ideas? Thx!",2022-03-19T12:10:31Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux subtype:bazel,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55297,"Hi  ! Once you cloned Tensorflow repo and entered into tensoflow folder through cd command , you can use `git checkout branch_name `  r2.2, r2.3,r2.8 etc to select a release branch. Attaching relevant thread for reference. Thanks!", please refer to how the master bump the version number from 2.8.0 to 2.9.0 2719182f888c16b44dba504c2f2d2240ddc9396d,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"You can do as suggested beforehand and search for available version in tesorflow site but you can't access versions older than available there. So if you want an earlier version: go to https://github.com/tensorflow/tensorflow. search for the version you want under branches How to Upgrade or Downgrade TensorFlow Python 3.63.9 installed and configured (check the Python version before starting). TensorFlow 2 installed. ... The command shows information about the package, including the version. ... Make sure to select a version compatible with your Python release. ... How to Downgrade TensorFlow.","You just need to clone the corresponding branch. For example, for TF 2.8, you need to clone `r2.8` and build from that branch."
1900,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Upstream Gradient of variables in tf.custom_gradient is (unexpectedly and seemingly generally) zero)ï¼Œ å†…å®¹æ˜¯ (**System information** TF 2.8, via Google Colab (so none of the other information should be relevant here?) **Describe the current behavior** Using tf.custom_gradient the upstream of a variable currently seems to be zero generally. Exemplary code:  (background: Trying to implement an equivalent to PyTorch higher library, to enable metalearning research also on TF, where it currently strongly lacks behind) I have come upon this while trying to implement a wrapper that calls a function as a function of weights. I observe that generally, the upstream gradient of any variable seems to be zero. I'm reporting this as a bug, because the behavior is (1) unexpected, (2) undocumented (from what I could find at least), and (3) the expected behavior would generally be more useful from what I understand.  A full colab notebook reproducing the problem can be found here: https://colab.research.google.com/drive/1kSQhgtxaIhwwpzp0VObZQbqGC0dmxeNc?usp=sharing I hope this is not just some other stupid coding mistake I made somewhere, but I've been trying to eliminate any other possible reasons for this behavior I could come up with. **Describe the expected behavior** The upstream gradient of a variable in tf.custom_gradient should be the actual upstream gradient, not a vector of zeros of the same shape  Do you want to contribute a PR? (yes/no): Since I'm not sure how thiis could be fixed, no; If I get pointed in the correct direction I will happily do that!   Briefly describe your candidate solution(if contributing): See above **Standalone code to reproduce the issue** Link again: https://colab.research.goo)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,LJKS,Upstream Gradient of variables in tf.custom_gradient is (unexpectedly and seemingly generally) zero,"**System information** TF 2.8, via Google Colab (so none of the other information should be relevant here?) **Describe the current behavior** Using tf.custom_gradient the upstream of a variable currently seems to be zero generally. Exemplary code:  (background: Trying to implement an equivalent to PyTorch higher library, to enable metalearning research also on TF, where it currently strongly lacks behind) I have come upon this while trying to implement a wrapper that calls a function as a function of weights. I observe that generally, the upstream gradient of any variable seems to be zero. I'm reporting this as a bug, because the behavior is (1) unexpected, (2) undocumented (from what I could find at least), and (3) the expected behavior would generally be more useful from what I understand.  A full colab notebook reproducing the problem can be found here: https://colab.research.google.com/drive/1kSQhgtxaIhwwpzp0VObZQbqGC0dmxeNc?usp=sharing I hope this is not just some other stupid coding mistake I made somewhere, but I've been trying to eliminate any other possible reasons for this behavior I could come up with. **Describe the expected behavior** The upstream gradient of a variable in tf.custom_gradient should be the actual upstream gradient, not a vector of zeros of the same shape  Do you want to contribute a PR? (yes/no): Since I'm not sure how thiis could be fixed, no; If I get pointed in the correct direction I will happily do that!   Briefly describe your candidate solution(if contributing): See above **Standalone code to reproduce the issue** Link again: https://colab.research.goo",2022-03-19T11:51:51Z,stat:awaiting response type:feature stale comp:ops TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55296,"  I was able to reproduce the issues on colab using TF v2.8.0 and tfnightly ,please find the attached gists. Thanks!","Hi , thanks for reporting! I think there are two issues here: (1) TF doesn't support differentiating through variable writes; (2) your workaround doesn't work for its own reason. About (2), because `copy_gradient`'s first return value isn't used, `var_up` in `grad` will also be zeros. I can't think of a working workaround though. So the current situation is that differentiating through variable writes is impossible in TF. We are exploring some ways to properly support it.","TensorFlow is an open source software library developed by Google that is widely used for machine learning and deep learning applications. One of the key features of TensorFlow is its ability to create custom gradients for differentiable functions. In this article, we will explore the concept of upstream gradient of variables in tf.custom_gradient and how it can impact the performance of machine learning models. When working with machine learning models, it is important to understand the concept of gradients. Gradients are used to optimize the weights and biases of a model by minimizing the loss function. TensorFlow provides a mechanism for creating custom gradients using the tf.custom_gradient function. This function allows users to define their own gradients for a given function.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1837,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Plugin implementation of AssignVarOp)ï¼Œ å†…å®¹æ˜¯ (I am struggling a lot to understand how plugins (pluggable device) should implement `AssignVarOp`. I created a post on the discussion forum but it seems like no one is able to answer pluggable device questions. I am trying to run a `Conv2D` on a separate device (only dummy for now) using the pluggable device interface. I implemented the `Conv2D` kernel, and when running tensorflow asked for an implementation of `AssignVarOp`. So I started implementing `AssignVarOp`, it has two input tensors (variable and value), variable is a tensor of type `TF_RESOURCE`. I imagine I should modify the data of the resource tensor, but what does it contain? After further research, it looks like `VarHandleOp` is the kernel responsible for the creation of the variable (and it's resource).  So I wrote a kernel for `VarHandleOp`. Checking the parameters at runtime, they are all uninitialized at the first call (`shared_name` is a default value, `shape` unitialized, etc). So I am left with two choices:  Either I would update the variable attributes later (doesn't seem possible as the C api doesn't provide any function for modifying an input tensor dimensions)  Either I should store all dimensions, etc in the resource tensor data in the form of a `ResourceHandle`. Is this `Resourcehandle` type arbitrarily defined by the user? I have been following this basic tutorial for writing tensorflow plugins, but the logic for variable handling is missing and it seems like a necessary step for plugins. **I will be really thankful if the tensorflow team (or anyone) could help me understand the mechanism of variable handling )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,slai-nick,Plugin implementation of AssignVarOp,"I am struggling a lot to understand how plugins (pluggable device) should implement `AssignVarOp`. I created a post on the discussion forum but it seems like no one is able to answer pluggable device questions. I am trying to run a `Conv2D` on a separate device (only dummy for now) using the pluggable device interface. I implemented the `Conv2D` kernel, and when running tensorflow asked for an implementation of `AssignVarOp`. So I started implementing `AssignVarOp`, it has two input tensors (variable and value), variable is a tensor of type `TF_RESOURCE`. I imagine I should modify the data of the resource tensor, but what does it contain? After further research, it looks like `VarHandleOp` is the kernel responsible for the creation of the variable (and it's resource).  So I wrote a kernel for `VarHandleOp`. Checking the parameters at runtime, they are all uninitialized at the first call (`shared_name` is a default value, `shape` unitialized, etc). So I am left with two choices:  Either I would update the variable attributes later (doesn't seem possible as the C api doesn't provide any function for modifying an input tensor dimensions)  Either I should store all dimensions, etc in the resource tensor data in the form of a `ResourceHandle`. Is this `Resourcehandle` type arbitrarily defined by the user? I have been following this basic tutorial for writing tensorflow plugins, but the logic for variable handling is missing and it seems like a necessary step for plugins. **I will be really thankful if the tensorflow team (or anyone) could help me understand the mechanism of variable handling ",2022-03-19T10:49:55Z,stat:awaiting response type:support stale comp:ops,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55295,"nick, This post, introduces the PluggableDevice architecture which offers a plugin mechanism for registering devices with TensorFlow without the need to make changes in TensorFlow code. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"With AssignVarOp, the OpKernelContext has two inputs, but expect no output. The first input is the variable, the second input is the value. This is analogous to me as a C++ function with a signature like AssignVarOp(TF_Tensor &variable, TF_Tensor value) ( this is not meant to represent the C++ implementation in tensorflow, just an analogy where I expose the concept of an input being a reference that will get modified using the second input which contains the value). The value input is a well formed tensor, with well defined dimensions and data, however the variable doesnâ€™t seem well defined (no dimensions, etc). There is no functions to modify a tensor object in the C API, nor to (re)set an input tensor. Therefore, it is unclear to me how to assign the value to the variable just by accessing those two tensors."
887,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Crashed if using different types of grad in adadelta optimizer)ï¼Œ å†…å®¹æ˜¯ (I'm trying to use adadelta optimizer in my training process but it crashed. It came to that I used different types of value in grad parameters. **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0  Python version: 3.7.11 **Describe the current behavior** Crashed with error info. **Describe the expected behavior** Error info could show the wrong type and would not crash. **Standalone code to reproduce the issue**  **Other info / logs** Include any logs or source code that would be helpful to )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,GundamBen,Crashed if using different types of grad in adadelta optimizer,"I'm trying to use adadelta optimizer in my training process but it crashed. It came to that I used different types of value in grad parameters. **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0  Python version: 3.7.11 **Describe the current behavior** Crashed with error info. **Describe the expected behavior** Error info could show the wrong type and would not crash. **Standalone code to reproduce the issue**  **Other info / logs** Include any logs or source code that would be helpful to ",2022-03-18T15:36:03Z,stat:awaiting response type:bug stale comp:core TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55285,"Hi  ! Could you please look at this issue ? It is replicating in 2.7,2.8 and nightly .",Could you please try with the experimental Adadelta from Keras here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/Adadelta and let us know if you find the similar behavior. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
747,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implemented RaggedTensor.from_sparse for multi-dimensional SparseTensor)ï¼Œ å†…å®¹æ˜¯ (Implemented a more general version of `RaggedTensor.from_sparse`, now allowing conversion from multidimensional `SparseTensor`s to `RaggedTensors`. To compare the performance with the original implementation specifically for the case of 2D sparse matrices, I simulated a random (1000, 1000) sparse tensor, with 50% sparsity. Under eager mode, the run time for the original implementation is 17.1  ms +/ 589 us, and the new implementation is 15.4 ms +/ 420 us. The performance wasn't worse.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,sapphire008,Implemented RaggedTensor.from_sparse for multi-dimensional SparseTensor,"Implemented a more general version of `RaggedTensor.from_sparse`, now allowing conversion from multidimensional `SparseTensor`s to `RaggedTensors`. To compare the performance with the original implementation specifically for the case of 2D sparse matrices, I simulated a random (1000, 1000) sparse tensor, with 50% sparsity. Under eager mode, the run time for the original implementation is 17.1  ms +/ 589 us, and the new implementation is 15.4 ms +/ 420 us. The performance wasn't worse.",2022-03-18T02:33:26Z,comp:ops size:M,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55280,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). For more information, open the CLA check for this pull request.",Found a problem. Close for now.
1787,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.function condition with tensor)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): colab  TensorFlow version (use command below): default colab  Python version: default colab  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior**  **Describe the expected behavior** I suppose that it will give `2` as with this little modified version:   **Contributing**  Do you want to contribute a PR? (yes/no): With some hints  Briefly describe your candidate solution(if contributing): I need some pointer **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bhack,tf.function condition with tensor,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): colab  TensorFlow version (use command below): default colab  Python version: default colab  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior**  **Describe the expected behavior** I suppose that it will give `2` as with this little modified version:   **Contributing**  Do you want to contribute a PR? (yes/no): With some hints  Briefly describe your candidate solution(if contributing): I need some pointer **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-03-17T22:49:23Z,stat:awaiting tensorflower type:bug comp:autograph comp:tf.function TF 2.8,closed,0,20,https://github.com/tensorflow/tensorflow/issues/55278,Just to share more debugging info for the failing and working case: ,The 2nd autograph is valid so I think that there is a false positive:  ,The second form seems to pass with: `.function(experimental_autograph_options=tf.autograph.experimental.Feature.EQUALITY_OPERATORS)` As it is converted in the same form as the first impl: https://github.com/tensorflow/tensorflow/blob/87462bfac761435a46641ff2f10ad0b6e5414a4b/tensorflow/python/autograph/converters/logical_expressions.pyL89L90  / ,https://github.com/tensorflow/tensorflow/blob/87462bfac761435a46641ff2f10ad0b6e5414a4b/tensorflow/python/autograph/converters/logical_expressions.pyL89L90,"Odd, I thought we had enabled that feature by default. But I think if was left off as Tensor implemented its own `__eq__` , `__lt__`, etc. This use case indicates that we should turn it on anyway.","> Odd, I thought we had enabled that feature by default. But I think if was left off as Tensor implemented its own `__eq__` , `__lt__`, etc. This use case indicates that we should turn it on anyway. Is https://github.com/tensorflow/tensorflow/issues/55278issuecomment1073058951 running fine cause we are in eager mode?","Yes. When evaluating `a < b < c` Python calls something in the lines of `(a < b).__bool__()`, which only works in eager mode.","> Yes. When evaluating `a < b < c` Python calls something in the lines of `(a < b).__bool__()`, which only works in eager mode. Yes honestly it is so hard to explain this kind of underline machinery to our users/developers. I needed to point another user to the `Cpython` source for a ""similar"" case","Yep, Python has uses a bit of magic in this case. I find is easiest to explain by crafting a class with overloaded operators and running it:  BTW, the `max` case could be a feature requerst for autograph to replace `tf.maximum`  we do that for a few builtins like `len`, and it's a fairly straightforward change: https://github.com/tensorflow/tensorflow/blob/b75de1cb47551a5a43f0c8d095252d0a125c4a8a/tensorflow/python/autograph/operators/py_builtins.pyL630","> Yep, Python has uses a bit of magic in this case. I find is easiest to explain by crafting a class with overloaded operators and running it: It is still hard for an avg developer to understand that we probably want to collect this case as a new `Autograph` ticket.  Do you think that we could improve the string: `OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.`",P.s. I would like to make a PR for both the cases but I will go for sure to abuse the CI with a blackbox impl as I've described in https://github.com/tensorflow/tensorflow/pull/55192issue1165231429. And in this case it will require for sure  more CI cycles so this will go to slowdown the PR too much without locally executing tests.,"Oops, edited. Yes, improving the error message makes sense too; one challenge is that it comes from outside of autograph, and can be triggered from other places as well."," Let me know how I could help you. If required I could spent/invest again many computing hours to rebuild TF with a fresh commit. I still think it will be faster for you with your build infra but if you don't have free slots give me some pointers I would try to rebuild ""again"" TF.  I see 3 activities here:  Default enable `tf.autograph.experimental.Feature.EQUALITY_OPERATORS`  Extend buitlin ops for `min` and `max` + tests.  Improve `OperatorNotAllowedInGraphError: using a tf.Tensoras a Pythonbool is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.`","It's definitely easier for me to test, though I'm not sure when I'll have the bandwidth. For very simple changes, I think it'd be ok to rely on the CI instead of building locally. BTW, I think basic building is a bit more practical on linux nowadays  especially if you use the docker container; you don't need to build the whole pip package, just `bazel test` the unit test straight after `./configure`. For now, how about we keep this issue for the first bullet, and file separate ones for the other two bullets?","> For now, how about we keep this issue for the first bullet, and file separate ones for the other two bullets? Ok > especially if you use the docker container; you don't need to build the whole pip package, just bazel test the unit test straight after ./configure. This will not speedup my contribution, I've built TF multiple times in the last years investing many hours of (waiting) compile time between a PR and the next one.  Are you aware of: https://discuss.tensorflow.org/t/llvmupdatesandbazelcache/2060 https://github.com/tensorflow/build/pull/48 https://github.com/tensorflow/build/issues/72","As we are not going to care/solve these build topics quite soon I need to invest many hours to rebuild again TF.  For the fist point do you have a specific pointer for the tests I need to run/pass? Is `bazel test tensorflow/python/autograph/operator` enough? As just `bazel test tensorflow/python/autograph/operator` already requires  more then 27k targets if the bazel estimation is correct: `INFO: Analyzed target //tensorflow/python/autograph/operators:operators (372 packages loaded, 27441 targets configured).`","> As we are not going to care/solve these build topics quite soon I need to invest many hours to rebuild again TF. I admit it's frustrating :( Most PRs rely on the internal build system, making it hard to get traction to improve the external one. > For the fist point do you have a specific pointer for the tests I need to run/pass? Is bazel test tensorflow/python/autograph/operator enough? I'd say, just `tensorflow/python/autograph/operators:py_builtins_test` and `tensorflow/python/autograph/converterd:logical_expressions_test` (especially if you add a test in there). The main idea is to test that the code builds and runs, and let the CI catch any other corner cases. > As just bazel test tensorflow/python/autograph/operator already requires more then 27k targets if the bazel estimation is correct: Looks about right, though IIRC it can grow up to 50k. It practically builds the whole TF, it's just that you'll run far fewer tests.","> I admit it's frustrating :( Most PRs rely on the internal build system, making it hard to get traction to improve the external one. It is one of the ""chicken or the egg"" problems we have for the community growing. I will ping you when the compilation is done.","Done.. for the the record: `bazel test tensorflow/python/autograph/operator`  `INFO: Elapsed time: 15290.045s, Critical Path: 298.10s`  Quad core with  6K bogomips (just to give a reference.)",Are you satisfied with the resolution of your issue? Yes No
1459,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Model Compiler Metrics Error: NaN Outputs)ï¼Œ å†…å®¹æ˜¯ (I am having a problem using Tensorflow when trying to use two custom metric functions which calculate the mean and maximum fractional errors. My code for these functions are the following:  When I try to use these metrics, all of the loss outputs go to NaN. If I use my meanFE and maxFE outside of Tensorflow, I don't encounter this issue. The only way I have been able to use these functions in Tensorflow is to remove the denominator term in the fractional errors, but this defeats the purpose of using a fractional error in the first place. I suspect this has something to do with the fact that my arrays get turned into Tensor objects by default when the system trains. My inputs are astronomical spectra (i.e., each spectrum is an array of shape (500 x 100), or there are 500 spectral arrays with 100 wavelength bins per array. I have tried using other built in functions for my metrics, namely the MAPE function (mean absolute percentage error), but this generates bad spectra (output spectra do not look like input spectra), so I am trying to implement my own shown above. Has anyone encountered NaN outputs as your loss and metric output when using functions such as these? I am using Tensorflow 1.14.0 on macOS.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,abojanich,Tensorflow Model Compiler Metrics Error: NaN Outputs,"I am having a problem using Tensorflow when trying to use two custom metric functions which calculate the mean and maximum fractional errors. My code for these functions are the following:  When I try to use these metrics, all of the loss outputs go to NaN. If I use my meanFE and maxFE outside of Tensorflow, I don't encounter this issue. The only way I have been able to use these functions in Tensorflow is to remove the denominator term in the fractional errors, but this defeats the purpose of using a fractional error in the first place. I suspect this has something to do with the fact that my arrays get turned into Tensor objects by default when the system trains. My inputs are astronomical spectra (i.e., each spectrum is an array of shape (500 x 100), or there are 500 spectral arrays with 100 wavelength bins per array. I have tried using other built in functions for my metrics, namely the MAPE function (mean absolute percentage error), but this generates bad spectra (output spectra do not look like input spectra), so I am trying to implement my own shown above. Has anyone encountered NaN outputs as your loss and metric output when using functions such as these? I am using Tensorflow 1.14.0 on macOS.",2022-03-17T21:55:49Z,stat:awaiting response type:support stale comp:keras TF 1.14,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55277,"Hi  ! 1.x versions are not supported anymore. I think there might be NaN values in Dataset itself as you are already using np.abs to convert the negative fraction values to positive ones.  Please let us know after replacing the above metrics with the below one.  If it is a zero division error in your loss function , you can resolve it by adding k.epsilon in Denominator.  Please post on TF forum for further assistance. Thank you! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1546,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Event file summary metric type changed, breaking the example in summary_iterator.py)ï¼Œ å†…å®¹æ˜¯ (Thank you for submitting a TensorFlow documentation issue. Per our GitHub policy, we only address code/doc bugs, performance issues, feature requests, and build/installation issues on GitHub. The TensorFlow docs are open source! To get involved, read the documentation contributor guide: https://www.tensorflow.org/community/contribute/docs  URL(s) with the issue: Please provide a link to the documentation entry, for example: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/  Description of issue (what needs changing):  Clear description  Some version between TF 2.3 and TF 2.8, it seems the behavior of logging metrics in TB event files changed. Before, values were logged as `simple_value` in the proto. Now, it is logged as a tensor that requires additional parsing with `tf.make_ndarray()` for example. As a result, this example in the documentation here no longer works. I'm not sure if the change to metrics writing was intentional (wasn't able to find release notes on that) but we should update the documentation here to a working one. TF 2.3 gist: https://colab.research.google.com/gist/timatim/067edda865dc9c0899bda0befd97930c/beginner.ipynb  TF 2.8 gist: https://colab.research.google.com/gist/timatim/2828cf9aa67e77d20baa22fc9935a684/beginner.ipynb )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,timatim,"Event file summary metric type changed, breaking the example in summary_iterator.py","Thank you for submitting a TensorFlow documentation issue. Per our GitHub policy, we only address code/doc bugs, performance issues, feature requests, and build/installation issues on GitHub. The TensorFlow docs are open source! To get involved, read the documentation contributor guide: https://www.tensorflow.org/community/contribute/docs  URL(s) with the issue: Please provide a link to the documentation entry, for example: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/  Description of issue (what needs changing):  Clear description  Some version between TF 2.3 and TF 2.8, it seems the behavior of logging metrics in TB event files changed. Before, values were logged as `simple_value` in the proto. Now, it is logged as a tensor that requires additional parsing with `tf.make_ndarray()` for example. As a result, this example in the documentation here no longer works. I'm not sure if the change to metrics writing was intentional (wasn't able to find release notes on that) but we should update the documentation here to a working one. TF 2.3 gist: https://colab.research.google.com/gist/timatim/067edda865dc9c0899bda0befd97930c/beginner.ipynb  TF 2.8 gist: https://colab.research.google.com/gist/timatim/2828cf9aa67e77d20baa22fc9935a684/beginner.ipynb ",2022-03-17T21:11:02Z,type:docs-bug,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55276,", Thanks for reporting. Created CL to fix above issue.",Are you satisfied with the resolution of your issue? Yes No
1400,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hexagon delegate doesn`t work on Android devices, cannot be loaded library UnsatisfiedLinkError (CMake))ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: build with ubuntu:18.04(docker image)   Samsung S7 tab (SMT875, Android12, Snapdragon 865), Samsung S21 (SMG9910, Snapdragon 888, Android 12), Vivotek FT9392EHTV0 (Smart Camera, Qualcomm SOC, Custom Android OS API 27  Azena OS)  TensorFlow version: 2.5.0  Python version: 3.6  Installed using bazel build  Bazel version: bazel3.7.2  Compiler version: clang(ndk;18.1.5063045)  CUDA/cuDNN version: not used  GPU model and memory:  not used **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** TF build with this command:  Linkage:  CMake:  CppCode (based on https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/utils.ccL132):  Aar contains: libhexagon_interface.so; libhexagon_delegate.so; libtensorflowlite.so; libhexagon_nn_skel(_v66/_v65/).so **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,YevhenServetnykSD,"Hexagon delegate doesn`t work on Android devices, cannot be loaded library UnsatisfiedLinkError (CMake)","**System information**  OS Platform and Distribution: build with ubuntu:18.04(docker image)   Samsung S7 tab (SMT875, Android12, Snapdragon 865), Samsung S21 (SMG9910, Snapdragon 888, Android 12), Vivotek FT9392EHTV0 (Smart Camera, Qualcomm SOC, Custom Android OS API 27  Azena OS)  TensorFlow version: 2.5.0  Python version: 3.6  Installed using bazel build  Bazel version: bazel3.7.2  Compiler version: clang(ndk;18.1.5063045)  CUDA/cuDNN version: not used  GPU model and memory:  not used **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** TF build with this command:  Linkage:  CMake:  CppCode (based on https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/utils.ccL132):  Aar contains: libhexagon_interface.so; libhexagon_delegate.so; libtensorflowlite.so; libhexagon_nn_skel(_v66/_v65/).so **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. ",2022-03-17T14:20:30Z,stat:awaiting response type:build/install stale TF 2.5 TFLiteHexagonDelegate,closed,4,4,https://github.com/tensorflow/tensorflow/issues/55269,"I am wondering if this is related to open sourcing the hexagon interface code, where some refactoring caused an issue here. Few suggestions / questions: 1) Do you get the same issue when you run evaluation tool, benchmark tool using adb ? 2) Do you face same issue when running on older versions ? 3) Do you face same issue when using the precompiled aar ? Thanks",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
613,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build TensorFlow Lite for iOS on macOS Mojave)ï¼Œ å†…å®¹æ˜¯ (Hello, I'm trying to build TensorFlow Lite 2.8.0 for iOS on an Intel MacBook Pro with macOS Monterey.  I'm following this link but whan I execute the command: `bazel build config=ios_fat c opt //tensorflow/lite/ios:TensorFlowLiteC_framework` I get this error:  It seems that python is not found when executing the script bundletool. I installed both python 3.9 and 2.7. Please help)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,raziel2k,Build TensorFlow Lite for iOS on macOS Mojave,"Hello, I'm trying to build TensorFlow Lite 2.8.0 for iOS on an Intel MacBook Pro with macOS Monterey.  I'm following this link but whan I execute the command: `bazel build config=ios_fat c opt //tensorflow/lite/ios:TensorFlowLiteC_framework` I get this error:  It seems that python is not found when executing the script bundletool. I installed both python 3.9 and 2.7. Please help",2022-03-17T13:11:05Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.8,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55268,"Hi, looks like this is a python location issue. When you ran configure.py, did you specify the location of python? You should specify it, otherwise the default one is used. One possible scenario is that python 3 is used, but it is finding `/usr/bin/python` folder. Find your python3 path with `whereis python3` and maybe create a symlink to it: `sudo ln s /usr/bin/python3 /usr/bin/python`. Hope this could help!","Hi yishuangP, thank you for your response. I have already tried to specify the location of python (trying different paths) but I don't think that's the problem since many python scripts are correctly executed before that problem occurs. I have also already tried to create the symbolic link but it didn't help. I tried to run the command `exec env  bazelout/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool` from command line and still got the problem but if I write `python bazelout/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool`, the script is executed (but I get a different error at runtime), so python is not found only when I use `exec env `. How can I solve this?",This is weird. I wonder if this is because you have two versions of python installed. And I think this is a bazel build issue. How about `python3 bazelout/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool`? Does it execute? `exec env ` clears the environment variables before executing. Can you try removing the old version? Sorry I only have python 3 installed and I can execute the command fine.,"Yes, that command executes without `exec env `. Anyhow, the problem is not related to the fact that I have python 2 installed because there was also before I installed it (actually, I installed python 2 to try to solve it). I think, maybe, the problem is related to the fact that I have a fresh installation of macOS Mojave (that does not come with python preinstalled anymore) and I installed python 3.9 from the official web site. Maybe this setup is different from an update to Mojave from a previous version.", Hi is your problem resolved? Maybe the following commands could help   ,"I also have a similar problem. Unable to create symblic link in /usr/bin path on macOS. ` exec env  ..... ` Because of that part, the environment variable disappears. And if the environment variable disappears, python cannot be used, only python3 can be used. This is my guess, but because of this I can't run bundletool during build. Is there a way to fix the build other than bypassing the OS?",I also have the similar issue. Is it the problem of bazel 4? Someone mentioned it on https://stackoverflow.com/questions/71778867/bazelbuildfailswithmsgenvpythonnosuchfileordirectorymacosmonte,Can you try updating to bazel 5?," , As per the below comment, could you please try in Bazel 5 and let us know if you still face an issue. Thanks! > Could you please try updating to bazel 5?",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1378,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RNN] Max and min for dynamic tensors should be recorded during calibration: Failed for tensor arg1 Empty min/max for tensor arg1)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0  2. Code Provide code to help us reproduce your issues using one of the following options: Colab Link   Output: `RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor arg1 Empty min/max for tensor arg1` I'm trying to quantize this model using 8 bit for weights and 16 bit for activations. Code works perfectly if full 8 bit quantization (including activations) is used or if the GRU layer is removed. So the experimental 8x16 bit quantization seems to fail for GRUs.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs flatbuffers downgraded to version 1.12, got error otherwise (same as in https://github.com/tensorflow/tensorflow/issues/51590issuecomment912614740 ). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,schmiph2,[RNN] Max and min for dynamic tensors should be recorded during calibration: Failed for tensor arg1 Empty min/max for tensor arg1," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  TensorFlow installation (pip package or built from source): pip  TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0  2. Code Provide code to help us reproduce your issues using one of the following options: Colab Link   Output: `RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor arg1 Empty min/max for tensor arg1` I'm trying to quantize this model using 8 bit for weights and 16 bit for activations. Code works perfectly if full 8 bit quantization (including activations) is used or if the GRU layer is removed. So the experimental 8x16 bit quantization seems to fail for GRUs.  4. (optional) RNN conversion support If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.  5. (optional) Any other info / logs flatbuffers downgraded to version 1.12, got error otherwise (same as in https://github.com/tensorflow/tensorflow/issues/51590issuecomment912614740 ). ",2022-03-17T12:56:05Z,stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.11,closed,2,9,https://github.com/tensorflow/tensorflow/issues/55267," , Can you please take a look at this SO link and the issue with the similar error.It helps.Thanks!","Unfortunately, these links didn't help me. The problem is really just with the 16x8 quantization of the GRU layer. If I remove the GRU layer, I can use the 16x8 quantization. Furthermore, when I switch to 8x8 quantization, everything works fine (including the GRU layer). If you want to test it yourself without the GRU layer, please comment out the line `tf.keras.layers.GRU(5,return_sequences=True)` in the colab notebook.","Hi  ! Could you please look at this issue? It is replicating in  2.8 and throwing different error in 2.7, nightly .",The issue still exists with TF 2.11. Please find the gist here. Thank you.,"I also encountered this problem. From tensorflow 2.8 to 2.11, this problem still exists. Does the official plan to make any fixes?","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
868,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exact Same outputs with 2 different datasets)ï¼Œ å†…å®¹æ˜¯ (I was trying to reimplement the paper https://arxiv.org/pdf/1901.11196.pdf and created 2 models which have same structure one takes in the augmented data and one doesn't to see which one performs better but for some weird reason both of them have exact same loss and other metrics and it doesn't even change after every epoch This is the link for my implementation  https://colab.research.google.com/drive/192mGhABi1n51cg8SFLvuUCwvsYIMNvx1?usp=sharing The model training is at the very end I tried to check and the inputs, encoders etc. are all different to both models but still this issue persists. Any ideas what could be going wrong?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,aflah02,Exact Same outputs with 2 different datasets,"I was trying to reimplement the paper https://arxiv.org/pdf/1901.11196.pdf and created 2 models which have same structure one takes in the augmented data and one doesn't to see which one performs better but for some weird reason both of them have exact same loss and other metrics and it doesn't even change after every epoch This is the link for my implementation  https://colab.research.google.com/drive/192mGhABi1n51cg8SFLvuUCwvsYIMNvx1?usp=sharing The model training is at the very end I tried to check and the inputs, encoders etc. are all different to both models but still this issue persists. Any ideas what could be going wrong?",2022-03-17T12:35:47Z,type:others comp:keras,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55266
1537,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.compat.v1.signal.rfft2d` and `rfft3d` lacks input validation leading to crashes)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0  Python version:3.7.12  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.2 (based on a colab notebook)  GPU model and memory: Tesla T4, 15109MiB (based on a colab notebook) **Describe the current behavior** The following code snippets lead to crashes when executed:  and  In either case, the inputs do not quite make sense, and tensorflow should throw. **Describe the expected behavior** Tensorflow should throw exceptions instead of crashing. **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Here is a colab notebook: https://colab.research.google.com/drive/168jYG6MqnW4jpJdIXFMUBkyiaweA43aP?usp=sharing Edit: the notebook has to be run with GPU  The code snippets above should also reproduce the issue.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kanghj,`tf.compat.v1.signal.rfft2d` and `rfft3d` lacks input validation leading to crashes,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0  Python version:3.7.12  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.2 (based on a colab notebook)  GPU model and memory: Tesla T4, 15109MiB (based on a colab notebook) **Describe the current behavior** The following code snippets lead to crashes when executed:  and  In either case, the inputs do not quite make sense, and tensorflow should throw. **Describe the expected behavior** Tensorflow should throw exceptions instead of crashing. **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Here is a colab notebook: https://colab.research.google.com/drive/168jYG6MqnW4jpJdIXFMUBkyiaweA43aP?usp=sharing Edit: the notebook has to be run with GPU  The code snippets above should also reproduce the issue.",2022-03-17T05:41:45Z,type:bug comp:ops TF 2.8,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55263,Added PR CC(Add necessary check in fft ops to fix crash) for the fix.,Are you satisfied with the resolution of your issue? Yes No
513,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Facing Issues with tf.io.TFRecordWriter while trying to write string)ï¼Œ å†…å®¹æ˜¯ (I'm trying to write a dataset to a tfrecords file however the string portion keeps raising an error  Error    I looked at several doc pages but it seems BytesList is the wat to go with string but for some reason it isn't working here. Am I missing something?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,aflah02,Facing Issues with tf.io.TFRecordWriter while trying to write string,I'm trying to write a dataset to a tfrecords file however the string portion keeps raising an error  Error    I looked at several doc pages but it seems BytesList is the wat to go with string but for some reason it isn't working here. Am I missing something?,2022-03-16T22:45:10Z,stat:awaiting response type:others,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55259," , In order to expedite the troubleshooting process, could you please provide a complete code and the TensorFlow version you are using.",I was using the default version which comes with colab currently not sure which one it is and this is the part which was causing the issue I've now removed it and used a workaround but I'm still interested to see what was going wrong I also tried reordering and saw all 3 have issues be it float or bytes," , Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the troubleshooting process, could you please provide complete code and the TensorFlow version you are using.",Hey  I no longer have that code and i used a different process from scratch. I guess it's best to close the issue as your point is correct but I don't have the code as i overwrote it,"I use the following code to write images to TFRecord format: def image_feature(imageSample):   feature = {     'image': bytes_feature(imageSample.tobytes())   }   return tf.train.Example(features=tf.train.Features(feature=feature)) def WriteTFRecord(images, save_directory, fileNumber):   """""" Writes a TFRecord file containing one or multiple training examples """"""   with tf.io.TFRecordWriter(save_directory + 'batch{0:08d}.tfrecords'.format(fileNumber)) as writer:     for sample in images:       tf_example = image_feature(sample)       writer.write(tf_example.SerializeToString())     First define image_feature().     Open your writer within with (automatically opens and closes the stream)     write your sample to TFRecord file image is of shape  that are (num_images, height, width, channel), save_directory is the path where you want to save and fileNumber is a postfix when writing several files."
972,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([INTEL oneDNN] Resubmitting scratch pad optimization for matmul primitive : PR number :54381)ï¼Œ å†…å®¹æ˜¯ (This PR has changes from PR 54381 (reverted back) + fix for Windows build failure. Changes to add user scratch pad for matmul primitive to fix OOM issue in Transformer LT. This reduces memory footprint of the primitive. It fixes an out of memory issue when running Transformer LT with multiple instances and total thread count is large. Managing scratch pad for the primitive from the framework, fixes the out of memory issue, reduces memory footprint and does not affect performance. The changes : Creates a new struct that hold the Tensor for scratch pad arg. Allocates memory based on scratch pad size queried from primitive description. Sets user scratch pad in post ops for the primitive.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,jojivk73,[INTEL oneDNN] Resubmitting scratch pad optimization for matmul primitive : PR number :54381,"This PR has changes from PR 54381 (reverted back) + fix for Windows build failure. Changes to add user scratch pad for matmul primitive to fix OOM issue in Transformer LT. This reduces memory footprint of the primitive. It fixes an out of memory issue when running Transformer LT with multiple instances and total thread count is large. Managing scratch pad for the primitive from the framework, fixes the out of memory issue, reduces memory footprint and does not affect performance. The changes : Creates a new struct that hold the Tensor for scratch pad arg. Allocates memory based on scratch pad size queried from primitive description. Sets user scratch pad in post ops for the primitive.",2022-03-16T18:55:44Z,comp:mkl size:M,closed,0,2,https://github.com/tensorflow/tensorflow/issues/55256, . There are 3 code style failures(Code Check). The first failure is often seen with clangformat style fix. It switches the location of the header from first to last or vice versa. The changes in second the third failures are copied from clangformat style=Google file. Please advice what to do for these failures. Wondering if I am using a different version than the one run by Internal CI. Thanks, Don't worry about that. We have already imported the PR. I'm just running Windows tests at a different snapshot to make sure there is no more failures. (The code at HEAD already failed the Windows tests because some unrelated LLVM errors so we can't tell.)
1839,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Generalise `xla::Map` to functions over arbitrary shapes)ï¼Œ å†…å®¹æ˜¯ (**System information**  TensorFlow version (you are using): 2.8  Are you willing to contribute it (Yes/No): No **Describe the feature and the current behavior/state.** Currently `xla::Map` only allow functions from shape [] to []. I'd like to use functions with arbitrary shapes. For example, apply a function with shapes [2, 3] > [5] to a tensor [100, 4, 2, 3] to get a [100, 4, 5]. I've not yet thought about how it would work when mapping multiple input tensors at once. **Will this change the current api? How?** It would extend `xla::Map`, either internally, or as an overload. It's possible additional information would need to be passed to a more general implementation of map, in which an overload may be preferable to maintain backwards compatibility. **Who will benefit with this feature?** Anyone using XLA who'd like to apply a function over sections of a tensor. It would effectively be an alternative to (some portion of) broadcasting, allowing people to use functions that don't allow leading dimensions to tensors with leading dimensions. It is particularly useful for me as I'm working with dependent types and adding leading dimensions is nontrivial (and verbose) in type signatures. **Any Other info.** It is already possible to do this by indexing into the tensor and iteratively applying the function to the contents, then concatenating the results, but I expect this is significantly slower than could be achieved within XLA. I have considered using `xla::While` for this but I still expect the slicing and concatenation would still come with a significant performance cost.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,joelberkeley,Generalise `xla::Map` to functions over arbitrary shapes,"**System information**  TensorFlow version (you are using): 2.8  Are you willing to contribute it (Yes/No): No **Describe the feature and the current behavior/state.** Currently `xla::Map` only allow functions from shape [] to []. I'd like to use functions with arbitrary shapes. For example, apply a function with shapes [2, 3] > [5] to a tensor [100, 4, 2, 3] to get a [100, 4, 5]. I've not yet thought about how it would work when mapping multiple input tensors at once. **Will this change the current api? How?** It would extend `xla::Map`, either internally, or as an overload. It's possible additional information would need to be passed to a more general implementation of map, in which an overload may be preferable to maintain backwards compatibility. **Who will benefit with this feature?** Anyone using XLA who'd like to apply a function over sections of a tensor. It would effectively be an alternative to (some portion of) broadcasting, allowing people to use functions that don't allow leading dimensions to tensors with leading dimensions. It is particularly useful for me as I'm working with dependent types and adding leading dimensions is nontrivial (and verbose) in type signatures. **Any Other info.** It is already possible to do this by indexing into the tensor and iteratively applying the function to the contents, then concatenating the results, but I expect this is significantly slower than could be achieved within XLA. I have considered using `xla::While` for this but I still expect the slicing and concatenation would still come with a significant performance cost.",2022-03-15T21:25:46Z,stat:awaiting tensorflower type:feature comp:xla,closed,0,1,https://github.com/tensorflow/tensorflow/issues/55246,"I have remembered that it may be possible to use `xla::While` with `xla::Slice` and `xla::DynamicUpdateSlice`, thus avoiding the concatenation at the end. I don't know if this would be performant. **EDIT** Having spoken to the XLA devs, this would apparently **not** be performant"
1352,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Windows Build From Source, executing ./config responded 404 is a different issue.)ï¼Œ å†…å®¹æ˜¯ (Had applied a potential fix for CC(Build fails to make use of bazelisk, if available.), however, I think this 404 is a different issue. _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/55218issuecomment1066932121_ below is the console message, I think this 404 is a different issue. INFO: Found applicable config definition build:monolithic in file c:\users\gomath776\christwork\tensorflow_renamed\.bazelrc: define framework_shared_object=false Loading: Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/e83168170a0d0bdf856a109187936bc44853c1b8.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/55218issuecomment1066932121_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Gelesh,"Windows Build From Source, executing ./config responded 404 is a different issue.","Had applied a potential fix for CC(Build fails to make use of bazelisk, if available.), however, I think this 404 is a different issue. _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/55218issuecomment1066932121_ below is the console message, I think this 404 is a different issue. INFO: Found applicable config definition build:monolithic in file c:\users\gomath776\christwork\tensorflow_renamed\.bazelrc: define framework_shared_object=false Loading: Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/e83168170a0d0bdf856a109187936bc44853c1b8.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found _Originally posted by  in https://github.com/tensorflow/tensorflow/issues/55218issuecomment1066932121_",2022-03-15T13:52:18Z,type:build/install subtype:windows TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55240," , Looks like this is duplicate of issue CC(Build fails to make use of bazelisk, if available.).Can you please close this issue, since it is already being tracked there? Thanks!","this is not duplicate of CC(Build fails to make use of bazelisk, if available.), this issue is about  error 404, file not found. Where as 55218 is about , bazelisk is not attempted to build when bazel is not installed.  Some are even attempting to set the path of bazel == path of bazelisk as a work arround "," , Looks like error is coming from MSVC. Make sure you have installed 1. Visual C++ Build Tools 2019. Thanks!","This issue is from the URL , noting is being fetched from URL, even when we hit the url from browser. I am sorry, I dont understand how is it even related to Visual Studio C++. However, I do have that in my pc","This 404 is just a warning which can be ignored. We have multiple mirrors from which build deps are downloaded, this just says that one of them does not have the sought for target.",Are you satisfied with the resolution of your issue? Yes No
1070,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix softmax bug for ragged tensors)ï¼Œ å†…å®¹æ˜¯ (The current implementation of softmax for ragged tensors is numerically unstable. In particular, the `exp()` function is computed on the raw logits. This causes an overflow when the logits are large positive. When the logits are large negative, this leads to an underflow and division by zero. The problem manifests through `nan` entries in the ragged tensor. In this PR I (hopefully) fix the ragged tensor implementation and implement a corresponding test. For reference of a numerically stable implementation, consider the numpy implementation in the test suite. (Recall that the softmax function is invariant under adding an overall constant to the input logits. Before applying `exp()` it is advantageous to subtract the max so that one logit component is zero and the rest are negative. This ensures that the denominator is numerically finite.))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,maresb,Fix softmax bug for ragged tensors,"The current implementation of softmax for ragged tensors is numerically unstable. In particular, the `exp()` function is computed on the raw logits. This causes an overflow when the logits are large positive. When the logits are large negative, this leads to an underflow and division by zero. The problem manifests through `nan` entries in the ragged tensor. In this PR I (hopefully) fix the ragged tensor implementation and implement a corresponding test. For reference of a numerically stable implementation, consider the numpy implementation in the test suite. (Recall that the softmax function is invariant under adding an overall constant to the input logits. Before applying `exp()` it is advantageous to subtract the max so that one logit component is zero and the rest are negative. This ensures that the denominator is numerically finite.)",2022-03-14T20:08:23Z,ready to pull comp:ops size:S,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55231,"Oops, I seem to have messed up something. :frowning_face:  ","Ok, it seems to be succeeding now. ğŸ˜… ",  Can you please review this PR ? Thank you!,", thanks so much for the review! Have I satisfactorily fixed everything?","I think so  we'll let it run through the whole test suite.  If everything looks good, it will be merged."," the test Ubuntu GPU, Python 3.9 is failing. I found a few other recent PRs where it seems to be succeeding, so I'm wondering if my PR is to blame? I was looking through what I could access from the logs, and I didn't manage to find any sort of explanation for the failure such as a stack trace. Am I missing something, or is that info only Googleaccessible? I found `kokorosisyphus_resultstore_prod_tensorflow_rel_docker_github_presubmit_pycpp_gpu_py39_dfdf902b73bc4129a7bd541ec0226cb2_build.log`, which contains: ",I think a test or two timed out.  Doesn't look related.,"Is there anything blocking a merge here? (It'd be nice to merge while this is fresh and before any conflicts arise, but I understand if there's a release cycle or if the team has other priorities.) Let me know if I can help with anything. Thanks so much!!!"
832,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Converter for LogicalNot operation)ï¼Œ å†…å®¹æ˜¯ ( Converter for LogicalNot operation is using the same base class (`ConvertUnaryImpl`) as other Unary operations.   The special converter for `Rsqrt ` was removed and now for this operation regular Unary Op converter is used.  New templated class implemented for testing `ConvertUnary`, `ConvertBooleanUnary`, `ConvertActivation`.  A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any `Unary` and `UnaryBoolean` operation. (Similar subtest for `ConvertActivation` operations is blocked and after refactoring of `ConvertActivation` it will be activated))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,drivanov,Converter for LogicalNot operation," Converter for LogicalNot operation is using the same base class (`ConvertUnaryImpl`) as other Unary operations.   The special converter for `Rsqrt ` was removed and now for this operation regular Unary Op converter is used.  New templated class implemented for testing `ConvertUnary`, `ConvertBooleanUnary`, `ConvertActivation`.  A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any `Unary` and `UnaryBoolean` operation. (Similar subtest for `ConvertActivation` operations is blocked and after refactoring of `ConvertActivation` it will be activated)",2022-03-14T17:34:37Z,size:L comp:gpu:tensorrt,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55229,Should be merged AFTER [PR CC([TF:TRT] Update test helpers to handle bool tensors and weights)](https://github.com/tensorflow/tensorflow/pull/55177): **Update test helpers to handle bool tensors and weights**,I fixed some formatting issues and noticed that this PR was closed.,Would you please rebase this PR?  ,"> Would you please rebase this PR?  Rebase is done.   : Just one note... On Friday (03/18/2022) I noticed that my latest change for [PR CC([TF:TRT] Update test helpers to handle bool tensors and weights)](https://github.com/tensorflow/tensorflow/pull/55177/) fixes the issue with `ConvertMatMul` you mentioned, but is not valid for `LogicalNot` and other boolean operations. I fixed this and updated [PR CC([TF:TRT] Update test helpers to handle bool tensors and weights)](https://github.com/tensorflow/tensorflow/pull/55177/), but it seems that it was already merged at that point. After that, I got the ""merge conflict"" messages for all my 4 PRs and I fixed them yesterday (03/20/22). Long story short, the changes in that fragments is a correct fix for the the issue with `ConvertMatMul` you mentioned. ","After the merge with the master branch, I see a lot of new ""changed"" files. I am closing that PR and I creating a new one PR CC(Converter for LogicalNot operation)."
1832,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tracker for TensorFlow-DirectML)ï¼Œ å†…å®¹æ˜¯ (cc:   This issue tracks pending PRs, issues, and possible cherrypicks necessary for TensorFlowDirectML for each TF release. Please post a comment with new things to track and I will update this post to reflect the changes. New PRs: * https://github.com/tensorflow/tensorflow/pull/56707     * Awaiting internal discussion in the TensorFlow team. * https://github.com/tensorflow/tensorflow/pull/55677 PRs that need more investigation.  CC([PluggableDevice] Add DEVICE_DEFAULT int32 registration for strided slice ops) [A similar PR broke tests before so this can't go in until the issue is resolved.] * https://github.com/tensorflow/tensorflow/pull/55381      * Reverted because it broke internal tests.  PRs that made it into TF nightly (post r2.10 branch cut):  CC([PluggableDevice] Add TF_IsHostMemoryInput and TF_IsHostMemoryOutput to the C API)  * https://github.com/tensorflow/tensorflow/pull/55544  * https://github.com/tensorflow/tensorflow/pull/55557 * https://github.com/tensorflow/tensorflow/pull/55579 * https://github.com/tensorflow/tensorflow/pull/52157 PRs that made it into TF 2.10: * https://github.com/tensorflow/tensorflow/pull/55379     * Resubmitted in https://github.com/tensorflow/tensorflow/pull/55640 * https://github.com/tensorflow/tensorflow/pull/55678 * https://github.com/tensorflow/tensorflow/pull/55645 * https://github.com/tensorflow/tensorflow/pull/55558 PRs that made it into TF 2.9:  CC([PluggableDevice] Add DEVICE_DEFAULT registration for ZerosLike and OnesLike)   CC([PluggableDevice] Enable tensor list kernels on Windows)   CC([PluggableDevice] Make `candidate_input_indices` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,penpornk,Tracker for TensorFlow-DirectML,"cc:   This issue tracks pending PRs, issues, and possible cherrypicks necessary for TensorFlowDirectML for each TF release. Please post a comment with new things to track and I will update this post to reflect the changes. New PRs: * https://github.com/tensorflow/tensorflow/pull/56707     * Awaiting internal discussion in the TensorFlow team. * https://github.com/tensorflow/tensorflow/pull/55677 PRs that need more investigation.  CC([PluggableDevice] Add DEVICE_DEFAULT int32 registration for strided slice ops) [A similar PR broke tests before so this can't go in until the issue is resolved.] * https://github.com/tensorflow/tensorflow/pull/55381      * Reverted because it broke internal tests.  PRs that made it into TF nightly (post r2.10 branch cut):  CC([PluggableDevice] Add TF_IsHostMemoryInput and TF_IsHostMemoryOutput to the C API)  * https://github.com/tensorflow/tensorflow/pull/55544  * https://github.com/tensorflow/tensorflow/pull/55557 * https://github.com/tensorflow/tensorflow/pull/55579 * https://github.com/tensorflow/tensorflow/pull/52157 PRs that made it into TF 2.10: * https://github.com/tensorflow/tensorflow/pull/55379     * Resubmitted in https://github.com/tensorflow/tensorflow/pull/55640 * https://github.com/tensorflow/tensorflow/pull/55678 * https://github.com/tensorflow/tensorflow/pull/55645 * https://github.com/tensorflow/tensorflow/pull/55558 PRs that made it into TF 2.9:  CC([PluggableDevice] Add DEVICE_DEFAULT registration for ZerosLike and OnesLike)   CC([PluggableDevice] Enable tensor list kernels on Windows)   CC([PluggableDevice] Make `candidate_input_indices` ",2022-03-14T16:28:25Z,type:others,open,0,25,https://github.com/tensorflow/tensorflow/issues/55226," How can pluggable devices implement the `Assign` op which uses the legacy variables? `TF_AssignVariable` already exists to implement `AssignVariableOp` which uses resources, but there doesn't seem to be an equivalent for the legacy variables. Should we add a `TF_AssignRefVariable` function?","  >  How can pluggable devices implement the `Assign` op which uses the legacy variables? `TF_AssignVariable` already exists to implement `AssignVariableOp` which uses resources, but there doesn't seem to be an equivalent for the legacy variables. Should we add a `TF_AssignRefVariable` function? Looping in  for more info on variables.","Resource and ref vars are very different, so my first thought is that `TF_AssignRefVariable` is probably needed.","I have a working prototype in a fork that I put in the kernels_experimental header. I can submit a PR if that's ok with everyone. Also, my understanding is that ref vars are deprecated in TF2 and are replaced by resources at the python API level. Is that correct? But even though they are deprecated, some popular benchmarks like AIBenchmark use a frozen TF1 model when running on TF2, which yield bad results for pluggable devices since they don't currently support them.","> my understanding is that ref vars are deprecated in TF2 and are replaced by resources at the python API level. Is that correct? But even though they are deprecated, some popular benchmarks like AIBenchmark use a frozen TF1 model when running on TF2. Yes, ref vars are deprecated at the Python level. Existing TF1 models may still be using them, so TF2 internals still support them.","> I have a working prototype in a fork that I put in the kernels_experimental header. I can submit a PR if that's ok with everyone.  Oh, that's great! Please submit the PR and we can continue the discussion there (e.g., whether this needs to be an RFC, etc). Thank you very much! :)", I submitted a PR here: https://github.com/tensorflow/tensorflow/pull/55379,New PRs to add to the list: https://github.com/tensorflow/tensorflow/pull/55381 https://github.com/tensorflow/tensorflow/pull/55382 https://github.com/tensorflow/tensorflow/pull/55384 https://github.com/tensorflow/tensorflow/pull/55385 https://github.com/tensorflow/tensorflow/pull/55386 https://github.com/tensorflow/tensorflow/pull/55387,3 new PRs: https://github.com/tensorflow/tensorflow/pull/55392 https://github.com/tensorflow/tensorflow/pull/55393 https://github.com/tensorflow/tensorflow/pull/55395, A simple PR that was opened a month ago. I think it flew under the radar. https://github.com/tensorflow/tensorflow/pull/54746,New PR: https://github.com/tensorflow/tensorflow/pull/55544,New PR: https://github.com/tensorflow/tensorflow/pull/55557,New PR: https://github.com/tensorflow/tensorflow/pull/55558,New PR: https://github.com/tensorflow/tensorflow/pull/55579," We noticed that support for `TF_VARIANT` is not really available for pluggable devices through the API since it's very hard (or even impossible) to read the content of a C++ object through ABIs. Even if we were to use the exact same headers as TensorFlow uses for all C++ objects, we would most likely need to use the exact same compiler. Since some models depend heavily on variant tensors which contain TensorList objects, we'd like to propose 2 new APIs that address this issue:  Like the `TF_AssignVariable` and `TF_AssignUpdateVariable` functions currently available in `kernels_experimental.h`, the goal of `TF_AddNVariant` and `TF_ZerosLikeVariant` would be to allow plugins to implement those 2 key operations by treating the Variant objects as a black box. The Variant objects would be unwrapped within TensorFlow core, which would then call the `binaryAddFunc` or `zerosLikeFunc` functions provided by the user, which are guaranteed to contain tensors of primitives (e.g. `TF_FLOAT`). Is this something that the TensorFlow team would like to see an RFC or PR for? For one of the RNN models that we track (PixelRNN), we see up to a 5x performance improvement by not having to do those operations on the CPU.",">  We noticed that support for `TF_VARIANT` is not really available for pluggable devices through the API since it's very hard (or even impossible) to read the content of a C++ object through ABIs. Even if we were to use the exact same headers as TensorFlow uses for all C++ objects, we would most likely need to use the exact same compiler. Yes, we currently don't have a generic way to support `TF_VARIANT`/`DT_VARIANT`. Even if you use the exact same header, compiler, and compiler flags, it's possible something else could still go wrong. The support so far has been on an opbyop basis, e.g., Kernel C API extension for Variable ops, TensorList support, and your recent PRs. > Since some models depend heavily on variant tensors which contain TensorList objects, we'd like to propose 2 new APIs that address this issue: >  >  >  > Like the `TF_AssignVariable` and `TF_AssignUpdateVariable` functions currently available in `kernels_experimental.h`, the goal of `TF_AddNVariant` and `TF_ZerosLikeVariant` would be to allow plugins to implement those 2 key operations by treating the Variant objects as a black box. The Variant objects would be unwrapped within TensorFlow core, which would then call the `binaryAddFunc` or `zerosLikeFunc` functions provided by the user, which are guaranteed to contain tensors of primitives (e.g. `TF_FLOAT`). >  > Is this something that the TensorFlow team would like to see an RFC or PR for? For one of the RNN models that we track (PixelRNN), we see up to a 5x performance improvement by not having to do those operations on the CPU. Thank you for the suggestion! The team thinks this sounds reasonable. If you already have a prototype code for this, would you mind opening a PR? We can take it from there. (If there are some points that need more discussion, we can start an RFC.)",Sure! I created a PR here: https://github.com/tensorflow/tensorflow/pull/55645,2 new PRs: https://github.com/tensorflow/tensorflow/pull/55677 https://github.com/tensorflow/tensorflow/pull/55678," I have a new PR to track: https://github.com/tensorflow/tensorflow/pull/56707 Do you want to create a new tracker for TF 2.10, or is this one fine?"," Thank you! Added to the list.  > Do you want to create a new tracker for TF 2.10, or is this one fine? Let's just reuse this one. :)",Would it be possible to ensure that 2.10 has CC(Constrain pytree type annotations to produce lists) in it?," Thank you for monitoring the other PR! Another PR that is important for us and we believe is a pretty significant bug in the pluggable device implementation is this one: https://github.com/tensorflow/tensorflow/pull/56707 The Pluggable Device RFC says that pluggable devices should be able to name themselves ""GPU"" and overwrite the builtin CUDA GPU, but in practice what happens is a lot of duplicate registration errors are being thrown because the previous registrations for the CUDA GPU device are not removed. This forces users to use the `tensorflowcpu` package instead of the `tensorflow` package that GPU users are more familiar with."," Happy to report that CC(Add DEVICE_DEFAULT registration for DataFormatDimMap and DataFormatVecPermute) went into TF 2.10. > Another PR that is important for us and we believe is a pretty significant bug in the pluggable device implementation is this one: https://github.com/tensorflow/tensorflow/pull/56707 Unfortunately, we need more time to carefully consider possible side effects of this one. I have replied on the PR. I would like to introduce tf. In the future, please help . I'll also try to go through PRs that are tracked in the top post soon. Apologies again for the delay! :("," Yes, CC(Constrain pytree type annotations to produce lists) is in v2.10. We have just cut the branch on Wednesday so anything that was merged before that is in the release.", We have 2 new small PRs that we'd like to get into 2.11: https://github.com/tensorflow/tensorflow/pull/58097 https://github.com/tensorflow/tensorflow/pull/58207
558,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix invalid examples for `tf.RaggedTensor.__abs__`)ï¼Œ å†…å®¹æ˜¯ (This PR tries to address the issue raised in  CC(Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear) where tf.tensor Examples was added to tf.RaggedTensor __abs__ Replacing the tensor examples with tf.RaggedTensor examples This PR fixes  CC(Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,gadagashwini,Fix invalid examples for `tf.RaggedTensor.__abs__`,This PR tries to address the issue raised in  CC(Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear) where tf.tensor Examples was added to tf.RaggedTensor __abs__ Replacing the tensor examples with tf.RaggedTensor examples This PR fixes  CC(Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear).,2022-03-14T09:42:10Z,type:docs-bug stale comp:ops size:S,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55222,tf.RaggedTensor represents usage and examples for RaggedTensor. Since few methods of tf.RaggedTensor such as https://www.tensorflow.org/api_docs/python/tf/RaggedTensor__abs__ has Dense Tensor examples.  This PR fixes issue https://github.com/tensorflow/tensorflow/issues/53828.,"The symbol you are changing is `tf.abs`, for dense tensors",`tf.RaggedTensor` doc has symbol `__abs__`. Can you please take a look https://www.tensorflow.org/api_docs/python/tf/RaggedTensor__abs__. Thanks!, This means this docstring is attached to `tf.math.abs` and `tf.abs`,"Thank you ! Only ask here is symbols like `__abs__`, `__add__`, `__and__` ,`__bool__` and etc, are appearing under tf.RaggedTensor document. How do we justify the examples given there. Since examples has Dense tensor but document is explaining about the RaggedTensor ","A RaggedTensor is a subclass of Tensor. If the method is not redefined, the parent class method is used.",Since https://github.com/tensorflow/tensorflow/pull/55222issuecomment1084771301 showed up in CC(Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear): the comment here is to help in solving the issue properly: you need to subclass the methods for the ragged tensor and attach docstring to the new methods.,"My bad, I misinterpreted the comment."
466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Model.compute_loss documentation section is not correct. compute_loss function is not called)ï¼Œ å†…å®¹æ˜¯ (Hello, Model.compute_loss documentation section is not correct compute_loss function is not called for some reason.  'compute_loss is called:' is not printed.  Checked on tensorflow 2.6.2)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,snarb,Model.compute_loss documentation section is not correct. compute_loss function is not called,"Hello, Model.compute_loss documentation section is not correct compute_loss function is not called for some reason.  'compute_loss is called:' is not printed.  Checked on tensorflow 2.6.2",2022-03-13T15:37:00Z,stat:awaiting response type:bug comp:keras 2.6.0,closed,0,7,https://github.com/tensorflow/tensorflow/issues/55216," I tried to replicate the code on  colab using `TF 2.8.0` ,and didn't face any error reported . Could you please have a look at the gist here and let us know if it helps?Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"  Hi. I have checked. The mentioned problem exists on 2.6 and 2.7, but not on 2.8. Thank you!", Thank you for the update! Could you please move this issue to closed status as it is resolved for you? Thanks!,  yes.  Resolved for me. Thanks, Closing this issue as it is fixed in latest version of TensorFlow.  Thanks!,Are you satisfied with the resolution of your issue? Yes No
1892,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.matmul() fails with ragged inputs of shape [batch_size, None, dims] and transpose_b=True)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11.2 ""bullseye"" (kerasdev docker image)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v1.12.172504gfeb9095a53f 2.9.0dev20220311  Python version: 3.9.10  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the current behavior** When A and B are two ragged tensors, both of shape [batch_size, None, dims], then `tf.matmul(A, B, transpose_b=True)` fails. However, when A and B are ragged tensors of shape [batch_size, None, None], with the exact same values, then it works fine. **Describe the expected behavior** I expect the first scenario to work, and it should return the same result as the second scenario. **Contributing**  Do you want to contribute a PR? (yes/no): not sure I'll have time, but if it's not too long ok  Briefly describe your candidate solution(if contributing): I'm guessing there's a bug in the shape checks in `tf.matmul()` since I see no reason why [batch_size, None, dims] would fail while [batch_size, None, None] would succeed. If anything, I would have expected the opposite. **Standalone code to reproduce the issue** See this gist. This code fails:  But this one succeeds:  The only difference I can see is that the shape of `A` and `B`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ageron,"tf.matmul() fails with ragged inputs of shape [batch_size, None, dims] and transpose_b=True","**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11.2 ""bullseye"" (kerasdev docker image)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v1.12.172504gfeb9095a53f 2.9.0dev20220311  Python version: 3.9.10  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the current behavior** When A and B are two ragged tensors, both of shape [batch_size, None, dims], then `tf.matmul(A, B, transpose_b=True)` fails. However, when A and B are ragged tensors of shape [batch_size, None, None], with the exact same values, then it works fine. **Describe the expected behavior** I expect the first scenario to work, and it should return the same result as the second scenario. **Contributing**  Do you want to contribute a PR? (yes/no): not sure I'll have time, but if it's not too long ok  Briefly describe your candidate solution(if contributing): I'm guessing there's a bug in the shape checks in `tf.matmul()` since I see no reason why [batch_size, None, dims] would fail while [batch_size, None, None] would succeed. If anything, I would have expected the opposite. **Standalone code to reproduce the issue** See this gist. This code fails:  But this one succeeds:  The only difference I can see is that the shape of `A` and `B`",2022-03-12T02:40:21Z,stat:awaiting tensorflower type:bug comp:apis TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55209,"Until this issue is fixed, here's a workaround: the following function sets the last dimension to `None`, so the `matmul()` operation succeeds.  Here's how it can be used (you only need to apply it to the second `matmul()` input):  I just checked the performance: `unset_outer_dimension()` does not seem to impact performance. However, I also benchmarked `tf.matmul()` between two regular tensors: it took about 28 Âµs microseconds per loop on CPU, versus 8100 Âµs (8.1 ms) for ragged tensors. Yikes! `tf.matmul()` with ragged tensors is almost 300 times slower than with regular tensors! On GPU (Colab), the difference is not as large, about 20 times slower. But performance is a separate issue."," Was able to replicate the issue on colab using TF v2.8.0 , nightly(2.9.0.dev20220313), please find the attached gists for reference. Thanks!",Thanks for reporting this!  Fixed in abc6f6ec7869d846b33b3d8e5e33284aac1600dc.,Are you satisfied with the resolution of your issue? Yes No
832,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Saving Resource Variables without Copy)ï¼Œ å†…å®¹æ˜¯ (Provides a solution for the increase in memory usage when saving models with large sparsely accessed resource variables by avoiding the copying of the variables. The kernel of `ReadVariableOp` is modified to convert variables that were sparsely accessed from copyonread mode to copyonwrite mode under an exclusive lock. The conversion is disabled in the default setting and only occurs when the Python function `_read_variable_op` is called from class `ResourceVariableSaveable` during the saving of resource variables. See issue  CC(adding additional support for completions related to lazyloaded modules).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,philipphack,Saving Resource Variables without Copy,Provides a solution for the increase in memory usage when saving models with large sparsely accessed resource variables by avoiding the copying of the variables. The kernel of `ReadVariableOp` is modified to convert variables that were sparsely accessed from copyonread mode to copyonwrite mode under an exclusive lock. The conversion is disabled in the default setting and only occurs when the Python function `_read_variable_op` is called from class `ResourceVariableSaveable` during the saving of resource variables. See issue  CC(adding additional support for completions related to lazyloaded modules).,2022-03-11T17:55:11Z,size:S,closed,0,11,https://github.com/tensorflow/tensorflow/issues/55206,I don't like that `ReadVariableOp` does this CORtoCOW conversion as a side effect (even if it only happens when `no_copy` is true). How about adding a new op `TurnOffCopyOnRead` to do this job?,"Thanks for the comment. I've introduced a new `ReadVariableWithoutCopyOp`, which is exclusively called when saving resource variables. For copyonwrite variables, the op behaves like `ReadVariableOp`. For copyonread variables, the kernel again acquires an exclusive mutex, but avoids the side effect by directly reading the variable instead of changing its type.",The new `DisableCopyOnReadOp` is called when a resource variable is saved.,"Thanks for the update! Overall looks good, except for some minor issues.","I can't reproduce the PyLint error, and I don't see any lines that are longer than 80 characters in the diff.",Maybe the test was against an older version. Or maybe it's caused by whitespaces. (CCing  who may know more.),"Rerunning the pylint job, seems to have been using an old version? Anyway, if still broken, please ignore.",Hi   Can you please fix build failures? Thank you!,There are some `api_compatibility_test` CI failures. Please run `bazel run //tensorflow/tools/api/tests:api_compatibility_test  update_goldens True` to update the API golden files.,This fails with `ModuleNotFoundError: No module named 'tensorflow.core.kernels'`.,Seems your Python paths are not set up properly. I'll try fixing the CI errors internally.
684,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(QAT model conversion adds extra RESHAPE layers compared to PTQ)ï¼Œ å†…å®¹æ˜¯ (Hello, When converting a QAT FullyConnected model to TFLite, the Reshape layers before and after the FullyConnected layer remain. This is due to the fact that there are Quant/Dequant stat nodes before and after the Reshapes therefore the patterns do not match. The solution to this is to run  the CreateOptimizePass after the Quantization passes. Added test to verify this. Code to reproduce:   Model produced:  !image20211208095717159)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SaoirseARM,QAT model conversion adds extra RESHAPE layers compared to PTQ,"Hello, When converting a QAT FullyConnected model to TFLite, the Reshape layers before and after the FullyConnected layer remain. This is due to the fact that there are Quant/Dequant stat nodes before and after the Reshapes therefore the patterns do not match. The solution to this is to run  the CreateOptimizePass after the Quantization passes. Added test to verify this. Code to reproduce:   Model produced:  !image20211208095717159",2022-03-11T17:21:50Z,size:M,closed,0,12,https://github.com/tensorflow/tensorflow/issues/55205, Can you please resolve conflicts? Thank you!,">  Can you please resolve conflicts? Thank you! Thank you. I have resolved the conflict. Best regards, Saoirse", Can you please address PyLint errors? Thank you!,">  Can you please address PyLint errors? Thank you! Thanks, I have now fixed those PyLint errrors. Best regards,  Saoirse",Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,Hi  Can you please review this PR ? Thank you!,"> Hi  Can you please review this PR ? Thank you! Sorry for late reply, our team member   is reviewing this CL. After approval, we can merge this. ", Can you please resolve conflicts? Thank you!,"Coinstantaneously, a similar fix has been merged.",https://github.com/tensorflow/tensorflow/commit/ad35289082513cddd7cdebeb2a836267c6924ab3
537,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(google.protobuf.message.DecodeError: Error parsing message with type 'tensorflow.GraphDef')ï¼Œ å†…å®¹æ˜¯ (I trained the model and saved it, now I am trying to load but unable to do. I have seen in previous post as well, but reference links are not working. **Code**  **Error:**  Note: I am using, python = 3.7.6 Tensorflowgpu=1.15 tensorboard=1.15.0 protobuf =3.19.4)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shubhambagwari,google.protobuf.message.DecodeError: Error parsing message with type 'tensorflow.GraphDef',"I trained the model and saved it, now I am trying to load but unable to do. I have seen in previous post as well, but reference links are not working. **Code**  **Error:**  Note: I am using, python = 3.7.6 Tensorflowgpu=1.15 tensorboard=1.15.0 protobuf =3.19.4",2022-03-11T14:27:05Z,stat:awaiting response type:others TF 2.7,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55202," , We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest tensorflow v2.8 and let us know if you are facing same issue.","  I updated tensorflow version to 2.7, still not resolved.   File ""stylize.py"", line 107, in      main()   File ""stylize.py"", line 52, in main     graph = load_graph(args.model)   File ""stylize.py"", line 15, in load_graph     graph_def.ParseFromString(f.read())   File ""C:\Users\shubham\env\lib\sitepackages\google\protobuf\message.py"", line 199, in ParseFromString     return self.MergeFromString(serialized)   File ""C:\Users\shubham\env\lib\sitepackages\google\protobuf\internal\python_message.py"", line 1145, in MergeFromString     if self._InternalParse(serialized, 0, length) != length:   File ""C:\Users\shubham\env\lib\sitepackages\google\protobuf\internal\python_message.py"", line 1198, in InternalParse     (data, new_pos) = decoder._DecodeUnknownField(   File ""C:\Users\shubham\env\lib\sitepackages\google\protobuf\internal\decoder.py"", line 993, in _DecodeUnknownField     raise _DecodeError('Wrong wire type in tag.') google.protobuf.message.DecodeError: Wrong wire type in tag."," , In order to expedite the troubleshooting process, could you please provide a complete code snippet  you are using.",Can you please take a look at this SO link with the similar error.It helps.Thanks!,Can you mail me @ subbu.bagwari.com," , Can you please provide the code and the dependencies here or in colab gist.Thanks!"
1849,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Missing input validation on `tf.ragged.constant`)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):   TensorFlow version (use command below):2.8.0  Python version: 3.7.12  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: using a colab notebook  GPU model and memory: using a colab notebook You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** If I pass an empty list with a large ragged_rank to `tf.ragged.constant`, all RAM is consumed, causing the notebook to crash. The docs indicate that ragged_rank should be between 0 and the rank of pylist, so the large value of ragged_rank should be rejected **Describe the expected behavior** Some input validation should be done and an exception thrown. **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reprodu)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,kanghj,Missing input validation on `tf.ragged.constant`,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):   TensorFlow version (use command below):2.8.0  Python version: 3.7.12  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: using a colab notebook  GPU model and memory: using a colab notebook You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** If I pass an empty list with a large ragged_rank to `tf.ragged.constant`, all RAM is consumed, causing the notebook to crash. The docs indicate that ragged_rank should be between 0 and the rank of pylist, so the large value of ragged_rank should be rejected **Describe the expected behavior** Some input validation should be done and an exception thrown. **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reprodu",2022-03-11T09:56:38Z,type:bug comp:ops TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55199," In order to expedite the troubleshooting process ,could you please provide the access to the colab ? Thanks!","Sorry, my bad, . I have updated the access permissions to the colab. Could you try again?  https://colab.research.google.com/drive/1OyQNTCiqHKjmHKfYbSOmVt4EfkLEgsNA?usp=sharing"," I was able to replicate the issue on colab using TF v2.8.0 and tfnightly , please find the gist here for reference.Thanks!",Are you satisfied with the resolution of your issue? Yes No
1242,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fail to convert trained model to TensorFlow Lite (integer only and unsigned integer))ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution: Linux Ubuntu 16.04  TensorFlow installation: pip package  TensorFlow library: 2.4.0  tensorflowmodeloptimization: 0.7.1.  2. Code (int8 conversion)  (uint8 conversion)  I'm trying to apply posttraining quantization to the trained model, the model is built and trained through TensorFlow2 functional API. However, for conversion of integer only, the converter generates the error message: `Model output is not dquntized.` when executing converter.convert(); for conversion of unsigned integer, the converter generates the error message: `the inference_input_type and inference_output_type must be tf.float32.` if I assigned `converter.target_spec.supported_ops = [tf.uint8]`, `converter.inference_input_type = tf.uint8`, and `converter.inference_output_type = tf.uint8`. !keras_to_int8_error !keras_to_uint8_error Are there settings required to convert the trained model to int8 and uint8 precision? Best Regards, Rahn)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rahn80643,Fail to convert trained model to TensorFlow Lite (integer only and unsigned integer)," 1. System information  OS Platform and Distribution: Linux Ubuntu 16.04  TensorFlow installation: pip package  TensorFlow library: 2.4.0  tensorflowmodeloptimization: 0.7.1.  2. Code (int8 conversion)  (uint8 conversion)  I'm trying to apply posttraining quantization to the trained model, the model is built and trained through TensorFlow2 functional API. However, for conversion of integer only, the converter generates the error message: `Model output is not dquntized.` when executing converter.convert(); for conversion of unsigned integer, the converter generates the error message: `the inference_input_type and inference_output_type must be tf.float32.` if I assigned `converter.target_spec.supported_ops = [tf.uint8]`, `converter.inference_input_type = tf.uint8`, and `converter.inference_output_type = tf.uint8`. !keras_to_int8_error !keras_to_uint8_error Are there settings required to convert the trained model to int8 and uint8 precision? Best Regards, Rahn",2022-03-11T08:04:40Z,stat:awaiting response type:bug stale TFLiteConverter TF 2.4,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55198," , In order to expedite the troubleshooting process, could you please provide a complete code and also please try to test your the code in latest tensorflow version 2.8 and let us know if you are facing same issue.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
815,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA support for ImageProjectiveTransformV3)ï¼Œ å†…å®¹æ˜¯ (**System information**  TensorFlow version (you are using): master  Are you willing to contribute it (Yes/No): I don't know, only if I have a clear contribution path **Describe the feature and the current behavior/state.** XLA support for `ImageProjectiveTransformV3`  **Will this change the current api? How?** No **Who will benefit with this feature?** Performance on image preprocessing **Any Other info.**   Extra: Please also note that the CPU/GPU TF2XLA supported ops tables are probably outdated (2018): https://github.com/tensorflow/tensorflow/issues/14798issuecomment1047796247)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,bhack,XLA support for ImageProjectiveTransformV3,"**System information**  TensorFlow version (you are using): master  Are you willing to contribute it (Yes/No): I don't know, only if I have a clear contribution path **Describe the feature and the current behavior/state.** XLA support for `ImageProjectiveTransformV3`  **Will this change the current api? How?** No **Who will benefit with this feature?** Performance on image preprocessing **Any Other info.**   Extra: Please also note that the CPU/GPU TF2XLA supported ops tables are probably outdated (2018): https://github.com/tensorflow/tensorflow/issues/14798issuecomment1047796247",2022-03-10T17:14:17Z,stat:awaiting response type:feature stale comp:xla,closed,1,10,https://github.com/tensorflow/tensorflow/issues/55194,/," , I was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist here.",Assigning to hinsu for triage.,We don't have a plan to support ImageProjectiveTransform at the moment so for now the best option will be to put this op outside of the XLA cluster.,Ok but It Is quite recurrent in image augmentation  layers,"Also, I don't know if you could expose some steps for a contributor to add support to this op trought a PR.",See also https://github.com/kerasteam/tfkeras/issues/89,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Got this warning while using @tf.function on tensorflow_probability.)ï¼Œ å†…å®¹æ˜¯ (I am trying to train a reinforcment learning agent on BipedalWalker. To Speed things up I used .function wrapper on my gradient calculations and then I got this warning. **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04LTS  Device: Lenovo Legion Y540  TensorFlow installed from (source or binary): conda  TensorFlow version (use command below): v2.4  TensorFlowProbability: v0.12.2  Python version: v3.9  Bazel version (if compiling from source):   GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 10.1/7.6.5  GPU model and memory: Nvidia Geforce Gtx 1650 4GB **Describe the current behavior**  **Describe the expected behavior**    Tensorflow should not give these warnings.  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): None **Standalone code to reproduce the issue**   Link to the jupyter notebook can be found here.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,HimGautam,Got this warning while using @tf.function on tensorflow_probability.,"I am trying to train a reinforcment learning agent on BipedalWalker. To Speed things up I used .function wrapper on my gradient calculations and then I got this warning. **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04LTS  Device: Lenovo Legion Y540  TensorFlow installed from (source or binary): conda  TensorFlow version (use command below): v2.4  TensorFlowProbability: v0.12.2  Python version: v3.9  Bazel version (if compiling from source):   GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 10.1/7.6.5  GPU model and memory: Nvidia Geforce Gtx 1650 4GB **Describe the current behavior**  **Describe the expected behavior**    Tensorflow should not give these warnings.  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): None **Standalone code to reproduce the issue**   Link to the jupyter notebook can be found here.",2022-03-10T11:30:11Z,stat:awaiting response type:bug stale TF 2.4 comp:tf.function,closed,0,8,https://github.com/tensorflow/tensorflow/issues/55190,Hi  ! Did you check in TF 2.8 version and tensorflow probability nightly? You can add this line at the beginning of code to disable the warnings. Thanks! `os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'`,"No, v2.8 is not supported by CUDA 10.1. So that's why I have not checked it.",Can you switch to Cuda 11.2 and CudNN 8.1 for TF 2.8 and let us know whether the issue still persists.,"I tried but my machine when I install these versions of cuda and cudnn, tensorflow is not able to access gpu on my system.",", There are the Deprecation Warnings, which we can suppress using one of the below code snippets   OR ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Ok got it, I think when I will update to higher versions of tensorflow then these warning will go away automatically. Thanks for the assist.",Are you satisfied with the resolution of your issue? Yes No
1392,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Will 64-bit support ever be added to the TPU driver API?)ï¼Œ å†…å®¹æ˜¯ (Hi, this is my first GitHub issue so please let me know if this is an innapropriate place to ask this question. When I try to create an array of type `float64` in JAX I get the following message: `RuntimeError: INVALID_ARGUMENT: 64bit data types are not yet supported on the TPU driver API. Convert inputs to float32/int32_t before using.` which from what I can tell is from the file `tensorflow/compiler/xla/python/tpu_driver/client/tpu_client.cc` in the Tensorflow repo, which I assume is part of the TPU driver API code. The verbiage here suggests this support may one day be added to the TPU driver API, so I'd like to know if this is something that's planned to be added in the future. If it is planned, is there an ETA on when this feature may be available? For a university project I'm trying to implement a Mersenne prime number search program which runs on the TPU. These calculations are very sensitive to precision errors, and we've found that although we may be able to use the TPU to improve performance over existing GPUbased programs, the precision of `bfloat16` or even `float32` are too low for the calculations we're trying to run.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,willfarris,Will 64-bit support ever be added to the TPU driver API?,"Hi, this is my first GitHub issue so please let me know if this is an innapropriate place to ask this question. When I try to create an array of type `float64` in JAX I get the following message: `RuntimeError: INVALID_ARGUMENT: 64bit data types are not yet supported on the TPU driver API. Convert inputs to float32/int32_t before using.` which from what I can tell is from the file `tensorflow/compiler/xla/python/tpu_driver/client/tpu_client.cc` in the Tensorflow repo, which I assume is part of the TPU driver API code. The verbiage here suggests this support may one day be added to the TPU driver API, so I'd like to know if this is something that's planned to be added in the future. If it is planned, is there an ETA on when this feature may be available? For a university project I'm trying to implement a Mersenne prime number search program which runs on the TPU. These calculations are very sensitive to precision errors, and we've found that although we may be able to use the TPU to improve performance over existing GPUbased programs, the precision of `bfloat16` or even `float32` are too low for the calculations we're trying to run.",2022-03-10T04:20:35Z,stat:awaiting response type:others,closed,1,6,https://github.com/tensorflow/tensorflow/issues/55187,Hi  ! Can you please share a simple stand alone code to replicate this issue? ," sure thing, running the code below in a Google Colab notebook with the TPU runtime selected should produce the error:  The error message: ", ! I think this issue will suit more in Jax repo .  Could you please post your query there?,"Sure thing, thanks! Should I go ahead and close this issue then?",Ok  . Moving this to closed status. Thanks!,"It appears the answer is no, this feature is not planned. See the issue below for details. https://github.com/google/jax/issues/9862"
1842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow core while do stdthread create)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (yes):  OS Platform and Distribution (e.g., Linux Red Hat 4.8.516) (GCC) ):  Mobile device (no) if the issue happens on mobile device:  TensorFlow installed from (source):  TensorFlow version (use command below):1.15.0  Python version:no  Bazel version (3.1.0):  GCC/Compiler version (5.4.0):  CUDA/cuDNN version:no  GPU model and memory:no You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** while I load savedmodel,tensorflow core dump,the detail gdb info at below. **Describe the expected behavior** load saved model success **Contributing**  Do you want to contribute a PR? (no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** core dump stack info: CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  0x0000000000000000 in ?? () CC(Add support for Python 3.x)  0x00007f201fbfbaf3 in __gthread_create (__args=, __func=0x7f201fbfb990 , __threadid=0x2ab77d8)     at /usr/gcc5.4.0/gccbui)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,goddie1,tensorflow core while do stdthread create,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (yes):  OS Platform and Distribution (e.g., Linux Red Hat 4.8.516) (GCC) ):  Mobile device (no) if the issue happens on mobile device:  TensorFlow installed from (source):  TensorFlow version (use command below):1.15.0  Python version:no  Bazel version (3.1.0):  GCC/Compiler version (5.4.0):  CUDA/cuDNN version:no  GPU model and memory:no You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** while I load savedmodel,tensorflow core dump,the detail gdb info at below. **Describe the expected behavior** load saved model success **Contributing**  Do you want to contribute a PR? (no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** core dump stack info: CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  0x0000000000000000 in ?? () CC(Add support for Python 3.x)  0x00007f201fbfbaf3 in __gthread_create (__args=, __func=0x7f201fbfb990 , __threadid=0x2ab77d8)     at /usr/gcc5.4.0/gccbui",2022-03-10T02:57:54Z,stat:awaiting response type:support stale comp:core TF 1.15,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55186,"  We see that you are using old version of tensorflow 1,x(1.15)  which is not actively supported, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Thanks!","ok,I will try  new version.", Could you please move this issue to closed status if it is resolved for you ?Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1126,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensor evaluating to None)ï¼Œ å†…å®¹æ˜¯ (**System information**  TensorFlow version (you are using): TF1 and TF2  Are you willing to contribute it (Yes/No): Depends on the extent of the change necessary. **Describe the feature and the current behavior/state.**  I have TF datasets (both TF1 and TF2) that I would like to transform (dataset.map). Transforms package the datasets into a new structure and this structure might contain some None elements. If I try to make elements None, I get some ""fetches errors"" saying None is not supported. I'm wondering if there is a value that would evaluate to None? If not, could we add support for one? Alternative would be to create an iterator and manually doing the map on fetched items, but that messes up external API (as we no longer work on tf.Datasets but Iterators instead). **Will this change the current api? How?** Not sure. **Who will benefit with this feature?** DeepMind internal. **Any Other info.**)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hamzamerzic,Tensor evaluating to None,"**System information**  TensorFlow version (you are using): TF1 and TF2  Are you willing to contribute it (Yes/No): Depends on the extent of the change necessary. **Describe the feature and the current behavior/state.**  I have TF datasets (both TF1 and TF2) that I would like to transform (dataset.map). Transforms package the datasets into a new structure and this structure might contain some None elements. If I try to make elements None, I get some ""fetches errors"" saying None is not supported. I'm wondering if there is a value that would evaluate to None? If not, could we add support for one? Alternative would be to create an iterator and manually doing the map on fetched items, but that messes up external API (as we no longer work on tf.Datasets but Iterators instead). **Will this change the current api? How?** Not sure. **Who will benefit with this feature?** DeepMind internal. **Any Other info.**",2022-03-10T01:18:24Z,stat:awaiting response type:feature stale comp:data,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55184,Hi  ! Could you please share a use case or a simple code snippet that explain the above feature request?,"Thanks a lot for the quick reply! Here is an example:  This gives an error `TypeError: Argument `fetch` = None has invalid type ""NoneType"". Cannot be None` This simple reproducer seems to actually work well with TF2. But, in my particular use case the dataset that I'm trying to use seems to fail elsewhere when using TF2 though.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
496,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Targets in `tensorflow/core/kernels/fuzzing` directory.)ï¼Œ å†…å®¹æ˜¯ (Hello. I'm trying to build fuzzing targets from `tensorflow/core/kernels/fuzzing` directory, and by default all targets from there are built as libraries. Was there some idea in this way of building, and is there any way to build them not like a library?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kobrineli,Targets in `tensorflow/core/kernels/fuzzing` directory.,"Hello. I'm trying to build fuzzing targets from `tensorflow/core/kernels/fuzzing` directory, and by default all targets from there are built as libraries. Was there some idea in this way of building, and is there any way to build them not like a library?",2022-03-09T19:10:35Z,stat:awaiting tensorflower type:others comp:core,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55183,"The question is, how to link build .so or .lo files with libfuzzer, tried something like `clang o bin fsanitize=fuzzer libdecode_csv_fuzz_lib.so`, but got linking errors. How it was conceived to fuzz with such targets?","Hi. These targets build internally with additional configs. We have integrated them in OSSFuzz in the past, see https://github.com/google/ossfuzz/pull/1937 for the additional things we've done to make it buildable in OSS. We are in the process of migrating these to something that can be properly built."," Hi. We have already found this patch and we succeeded to build these targets. We added `cc_test` like you did in this patch and added `linkopts = [""fsanitize=fuzzer,address""]` into it. Also we used `config=monilithic`, `dynamic_mode=off` and added `libclang_rt.asan_cxxx86_64.a` and `libclang_rt.asanx86_64.a` to link options and succeeded to build targets only with `bazel build` command without linking targets by ourselves after linking fail (so we don't have linking fail). The only problem is `decode_base64` target, we decided not to build it, because it is linking too long. To minimize logs from TF we used environment variable `TF_CPP_MIN_LOG_LEVEL=3`.","Awesome. We're planning to move these to rules that are better suited for OSS development, but it would take some time"
1876,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make computation of higher order gradients through apply_gradients possible)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.7 / 2.8  Are you willing to contribute it (Yes/No): No **Describe the feature and the current behavior/state.** Currently, TF can not compute gradients through optimizer.apply_gradients calls  I suspect this would be caused by the underlying assign operation not being differentiable. However, this operation is in itself definitely differentiable and computing a gradient over this operation is a staple in multiple important works, especially in MetaLearning and other fields actually making use of higher order gradients.  Current behavior: While higherorder gradients can easily be implemented using e.g. nested gradient tapes (https://www.tensorflow.org/guide/advanced_autodiffhigherorder_gradients), they can not be achieved through applying gradient updates  generally done via optimizer.apply_gradients().  While in theory one can create workarounds by stitching together gradients by hand ( let 'theta' be parameters before update, theta_dash parameters after update, phi some term dependent on the updated parameters one wants the gradient for: [d_L/d_theta_dash] x [d_theta_dash/d_theta] x [d_theta/d_phi] can be computed because TF can compute [d_L/d_theta_dash] and [d_theta/d_phi] naturally, for SGD [d_theta_dash/d_theta] is just vec(1) really. However this workaround really only works for SGD without Momentum, for anything used i)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,LJKS,Make computation of higher order gradients through apply_gradients possible,"Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.7 / 2.8  Are you willing to contribute it (Yes/No): No **Describe the feature and the current behavior/state.** Currently, TF can not compute gradients through optimizer.apply_gradients calls  I suspect this would be caused by the underlying assign operation not being differentiable. However, this operation is in itself definitely differentiable and computing a gradient over this operation is a staple in multiple important works, especially in MetaLearning and other fields actually making use of higher order gradients.  Current behavior: While higherorder gradients can easily be implemented using e.g. nested gradient tapes (https://www.tensorflow.org/guide/advanced_autodiffhigherorder_gradients), they can not be achieved through applying gradient updates  generally done via optimizer.apply_gradients().  While in theory one can create workarounds by stitching together gradients by hand ( let 'theta' be parameters before update, theta_dash parameters after update, phi some term dependent on the updated parameters one wants the gradient for: [d_L/d_theta_dash] x [d_theta_dash/d_theta] x [d_theta/d_phi] can be computed because TF can compute [d_L/d_theta_dash] and [d_theta/d_phi] naturally, for SGD [d_theta_dash/d_theta] is just vec(1) really. However this workaround really only works for SGD without Momentum, for anything used i",2022-03-09T12:05:35Z,stat:awaiting response type:feature stale comp:eager comp:ops,closed,1,4,https://github.com/tensorflow/tensorflow/issues/55181,"I think this feature is crucial to be competitive against PyTorch for all those who want to research in metalearning field. After almost 1 year, I think no progress has been made and currently, as  highlighted, there are only workarounds feasbile with SGD only. Also, to encapsulate the training logic in a `.function`, you need to create N copies of the model where N is the number of `inner_update_steps` (see https://github.com/siavashkhodadadeh/UMTRARelease/blob/master/models/maml/maml.pyL63), resulting in a high memory consumption. Right now, there is no way to use TF in an efficient way for metalearning ğŸ˜”","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1936,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.distribute.MultiWorkerMirroredStrategy() not getting initialized(RuntimeError: Collective ops must be configured at program startup))ï¼Œ å†…å®¹æ˜¯ (I am running a training job on vertex AI.  Versions: TF = 2.7 python = 3.8 The model runs fine for mirrored strategy where we have only one node with multiple gpu attached to it. For MultiWorkerMirroredStrategy I am using chief  n1 32 with 4V100 gpu  count 1(default) worker n1 32 with 4V100 gpu  count 1 When I am trying to run the code using multiple nodes using MultiWorkerMirroredStrategy, It is giving following error RuntimeError: Collective ops must be configured at program startup I found some suggestions to put the strategy at the program beginning  but that also didn't help. Vertex AI is setting the TF_CONFIG correctly. But it is not able to instantiate the MultiWorkerMirroredStrategy The stack trace  mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"" "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/util/deprecation.py"", line 348, in new_func"" workerpool00 "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 253, in __init__"" "" super(_CollectiveAllReduceStrategyExperimental,"" workerpool00 "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__"" workerpool00 "" CollectiveAllReduceExtended("" workerpool00 "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 330, in __init__"" workerpool00 "" self._initialize_strategy(self._cluster_resolver)"" Error 20220309 09:57:09.622 IST workerpool00 "" File ""/opt/conda/env)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ranvirdesai,tf.distribute.MultiWorkerMirroredStrategy() not getting initialized(RuntimeError: Collective ops must be configured at program startup),"I am running a training job on vertex AI.  Versions: TF = 2.7 python = 3.8 The model runs fine for mirrored strategy where we have only one node with multiple gpu attached to it. For MultiWorkerMirroredStrategy I am using chief  n1 32 with 4V100 gpu  count 1(default) worker n1 32 with 4V100 gpu  count 1 When I am trying to run the code using multiple nodes using MultiWorkerMirroredStrategy, It is giving following error RuntimeError: Collective ops must be configured at program startup I found some suggestions to put the strategy at the program beginning  but that also didn't help. Vertex AI is setting the TF_CONFIG correctly. But it is not able to instantiate the MultiWorkerMirroredStrategy The stack trace  mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"" "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/util/deprecation.py"", line 348, in new_func"" workerpool00 "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 253, in __init__"" "" super(_CollectiveAllReduceStrategyExperimental,"" workerpool00 "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__"" workerpool00 "" CollectiveAllReduceExtended("" workerpool00 "" File ""/opt/conda/envs/py38/lib/python3.8/sitepackages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 330, in __init__"" workerpool00 "" self._initialize_strategy(self._cluster_resolver)"" Error 20220309 09:57:09.622 IST workerpool00 "" File ""/opt/conda/env",2022-03-09T05:47:24Z,stat:awaiting response type:bug comp:dist-strat TF 2.7,closed,0,13,https://github.com/tensorflow/tensorflow/issues/55178,Can you put a breakpoint before this line `mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()` and check if any gpu memory is allocated already by TF? if so you accidentally created a tf.Var or Tensor which you are not allowed to before creating MultiWorkerMirroredStrategy,"Instantiating the strategy is first line of code, I don't think there are any tensors created before.",",  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!","bauer Thanks for the reply, any other issue you could think of....","  Its just this code raises error while initializing strategy. if __name__==""__main__"":     mirrored_strategy = tf.distribute.MultiWorkerMirroredStrategy()",This is not the first line of code... Did you check the GPU memory at this line?  you could have   somewhere above main or in any imported module and it would already fail ,", Could you please refer to the comment and let us know ?Thanks!"," bauer  Thanks for the response. I tried running vertex ai custom job using suggested method. I initialized strategy right after importing tf, it did not give error but training was also not done. training was for only one epoch and takes 2min in my local, but I think it went in never ending loop on vertex ai.","Make sure you have the proper configuration, follow this document for reference. Thanks!", bauer  The comment helped. I rearranged imports and it worked," , If your issue is resolved, could you please close this issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No,"I have a similar problem , I am trying to distribute the load accross multiple nodes , if I initiate the Strategy first then it's not using the TF_CONFIG so as a result it's using only one GPU, but if I initiate the config first then getting error ""Collective ops must be configured at program startup"""
1638,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Silent reset_states() error in custom model)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Monterey 12.1  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.8  Python version: 3.9 **Describe the current behavior** I am developing a custom model by inheriting from the `tf.keras.Model` class, in particular I am developing a custom LSTM residual network. Following the documentation and code, if calling `model.reset_states()` on a stateless layer should trigger an exception present in the layer class (in my case LSTM).  However, such call is obscured by a conditional statement that prevents the raising of such exception in any case (at least the one I was trying). So basically model.reset_states() runs silently without errors and without resetting the states.  This is the function present inside tf.keras.Model class. The conditional statement prevents the condition from layer.reset_states() to be raised. By manually forcing the evaluation  of `layer.reset_states()` inside a debugger, the exception is correctly raised.  **Describe the expected behavior** Raise the exception and make explicit whether reset_states() is called on layers that are not stateful  Do you want to contribute a PR? (yes/no): no **Standalone code to reproduce the issue** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mattiasu96,Silent reset_states() error in custom model,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Monterey 12.1  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.8  Python version: 3.9 **Describe the current behavior** I am developing a custom model by inheriting from the `tf.keras.Model` class, in particular I am developing a custom LSTM residual network. Following the documentation and code, if calling `model.reset_states()` on a stateless layer should trigger an exception present in the layer class (in my case LSTM).  However, such call is obscured by a conditional statement that prevents the raising of such exception in any case (at least the one I was trying). So basically model.reset_states() runs silently without errors and without resetting the states.  This is the function present inside tf.keras.Model class. The conditional statement prevents the condition from layer.reset_states() to be raised. By manually forcing the evaluation  of `layer.reset_states()` inside a debugger, the exception is correctly raised.  **Describe the expected behavior** Raise the exception and make explicit whether reset_states() is called on layers that are not stateful  Do you want to contribute a PR? (yes/no): no **Standalone code to reproduce the issue** ",2022-03-08T21:00:13Z,stat:awaiting response type:bug comp:keras TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55174,", Thanks for opening this issue. Development of keras moved to separate repository https://github.com/kerasteam/keras/issues Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999. Thanks!","Oh sorry, should I copy the issue to Keras then? ",", Yes, please post your issue in https://github.com/kerasteam/keras/issues repo and close this issue. Thanks!",Done!,Are you satisfied with the resolution of your issue? Yes No
1912,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MultiWorkerMirroredStrategy reinitializes reduce operations multiple times at start and also from time to time.)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:   TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.7.0rc169gc256c071bb2 2.7.0  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.4   GPU model and memory: V100 16GB You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** Fyi: I use a custom training loop.  On a multi worker setup with `MultiWorkerMirroredStrategy` I get this output for the first 3 train steps and they are also super slow:   Then the training runs as expected but it seems that the above init/retracing can happen randomly in the middle of the training again with the same logs. So an intermediate step after 1000 iterations can again take minutes (the more workers/devices the longer) The step at)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,andre-bauer,MultiWorkerMirroredStrategy reinitializes reduce operations multiple times at start and also from time to time.,"Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:   TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.7.0rc169gc256c071bb2 2.7.0  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.4   GPU model and memory: V100 16GB You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** Fyi: I use a custom training loop.  On a multi worker setup with `MultiWorkerMirroredStrategy` I get this output for the first 3 train steps and they are also super slow:   Then the training runs as expected but it seems that the above init/retracing can happen randomly in the middle of the training again with the same logs. So an intermediate step after 1000 iterations can again take minutes (the more workers/devices the longer) The step at",2022-03-08T17:02:15Z,stat:awaiting response comp:dist-strat type:performance TF 2.7,closed,0,3,https://github.com/tensorflow/tensorflow/issues/55147,"bauer, In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!"," Thx for the reply I actually figured it out myself and it was my fault. Still documenting the problem I created here: This output above happens either on model build or on ``tf.function`` (re)tracing. Apparently I was passing some python object into the ``tf.function`` that could change from time to time cause function retracing. With one worker there is no output (default logging verbosity, relax_shapes=True) and it also does not take a crazy amount of time. That is the reason why I only noticed it when using ``MultiWorkerMirroredStrategy``. So to everyone having something similar, check if your ``tf.function`` is retraced for some reason.",Are you satisfied with the resolution of your issue? Yes No
1864,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bug in feature_column.embedding_column based on vocabulary size)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  AL2  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.3.3137gea90cf44f73 2.3.4  Python version: 3.6  CUDA/cuDNN version: replicable on CPU  GPU model and memory: replicable on CPU  **Describe the current behavior** While trying to use `tf.feature_column.embedding_column` API. While I don't think is relevant I'm generating the input data via `tf.data.Dataset.from_generator`. Code     **Describe the expected behavior** For small vocabulary sizes everything works fine but the moment I try to add more elements to my vocabulary I weirdly get the following exception:  **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,h1t35h,Bug in feature_column.embedding_column based on vocabulary size,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  AL2  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.3.3137gea90cf44f73 2.3.4  Python version: 3.6  CUDA/cuDNN version: replicable on CPU  GPU model and memory: replicable on CPU  **Describe the current behavior** While trying to use `tf.feature_column.embedding_column` API. While I don't think is relevant I'm generating the input data via `tf.data.Dataset.from_generator`. Code     **Describe the expected behavior** For small vocabulary sizes everything works fine but the moment I try to add more elements to my vocabulary I weirdly get the following exception:  **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-03-08T13:05:56Z,stat:awaiting response type:bug comp:apis TF 2.3,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55139,", In order to expedite the troubleshooting process, please provide a standalone code to reproduce the issue reported here. Thanks!",My sincere apologies it seemed my vocabulary was getting dirtied upon fixing that the behavior no longer present. Closing this.,Are you satisfied with the resolution of your issue? Yes No,æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç¾Šå³»éœ„ï¼Œæˆ‘å·²ç»æ”¶åˆ°æ‚¨çš„é‚®ä»¶ï¼Œè°¢è°¢
1853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.image.adjust_jpeg_quality gives unexpected result)ï¼Œ å†…å®¹æ˜¯ (This issue is about the function `tf.image.adjust_jpeg_quality`. In particular, I am concerned about the docs example and I am surprised by the result with quality 100. 1. The example provided in the documentation is misleading. The example uses a float tensor with values in range 112. The result of `tf.image.adjust_jpeg_quality` are all one. I assume this is happening because the data type conversion sees a float tensor and clips all values larger than 1. If the input was a uint8 tensor, then the result would differ significantly. In my opinion, the docs should clarify the expected input data type. 2. The output of `tf.image.adjust_jpeg_quality` with quality 100 looks suspicious to me. I know that even with quality 100 JPEG compression is not necessarily lossless due to rounding errors. Nevertheless, I am surprised how much information is lost. In comparison, the results with Pillow at the same quality are far less lossy. Please take a look at the example below. The input is a 8x8 grayscale image with values from 0 through 63. The pixel values after TensorFlow's JPEG compression show quite some difference to the input, whereas JPEG compression with Pillow preserves the input array. Can anyone verify whether TensorFlow is behaving as expected here? Is there any documentation on how TensorFlow implements JPEG compression?  Output:  **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see example above  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20  TensorFlow installed from (source or binary): bin)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,btlorch,tf.image.adjust_jpeg_quality gives unexpected result,"This issue is about the function `tf.image.adjust_jpeg_quality`. In particular, I am concerned about the docs example and I am surprised by the result with quality 100. 1. The example provided in the documentation is misleading. The example uses a float tensor with values in range 112. The result of `tf.image.adjust_jpeg_quality` are all one. I assume this is happening because the data type conversion sees a float tensor and clips all values larger than 1. If the input was a uint8 tensor, then the result would differ significantly. In my opinion, the docs should clarify the expected input data type. 2. The output of `tf.image.adjust_jpeg_quality` with quality 100 looks suspicious to me. I know that even with quality 100 JPEG compression is not necessarily lossless due to rounding errors. Nevertheless, I am surprised how much information is lost. In comparison, the results with Pillow at the same quality are far less lossy. Please take a look at the example below. The input is a 8x8 grayscale image with values from 0 through 63. The pixel values after TensorFlow's JPEG compression show quite some difference to the input, whereas JPEG compression with Pillow preserves the input array. Can anyone verify whether TensorFlow is behaving as expected here? Is there any documentation on how TensorFlow implements JPEG compression?  Output:  **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see example above  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20  TensorFlow installed from (source or binary): bin",2022-03-08T12:20:50Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:ops TF 2.9,closed,0,9,https://github.com/tensorflow/tensorflow/issues/55138,"Hi  ! Could you please look at this issue? It's replicating in 2.7 , 2.8 and nightly. ","It looks like this discrepancy is due to the fact that TensorFlow uses the fast inverse DCT by default for decoding JPEG images. However, this fact is completely opaque to someone using `tf.image.adjust_jpeg_quality`. I was able to reproduce the wrong output tensor by running libjpegturbo's `djpeg dct fast` command. I see two options to solve this issue: 1. Use the accurate integer DCT by default. 2. Provide an argument to choose the DCT method in `tf.image.adjust_jpeg_quality` or at least mention this caveat in the method's documentation. Besides from that, it is very unsatisfactory that the example provided with `tf.image.adjust_jpeg_quality` uses a float tensor with values above 1, although `convert_image_dtype` states that float tensors are expected to have values in the range \[0, 1).","On a side note, it also seems inconsistent to me that TensorFlow uses the accurate DCT (JDCT_ISLOW) for JPEG compression and the inaccurate IDCT (JDCT_IFAST) for decompression by default.",> Provide an argument to choose the DCT method in tf.image.adjust_jpeg_quality Do you have any interest in contributing a PR for this? > at least mention this caveat in the method's documentation. I can add something to the documentation and add an example without clipping. > Use the accurate integer DCT by default. Changing the default (which would change behavior for existing users) is unlikely to be accepted.,"I also observed the following API aliases have the same behavior that produces different outputs on PIL and TensorFlow under the same jpeg quality setting.  `(tf.image.adjust_jpeg_quality)`, `tf.compat.v1.image.adjust_jpeg_quality` This behavior still exists in tensorflow nightly (2.15.0dev20230907), and users should be cautious when using them on both CPU and GPU.    Code to reproduce the issue in tf.compat.v1.image.adjust_jpeg_quality  Here is the output of the above code on my GPU machine, which shows the different outputs on PIL and TensorFlow under the same jpeg quality setting.  It also produces the same unexpected output on CPU:  ","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The PR which was raised for the respective issue has been reviewed and merged. https://github.com/tensorflow/tensorflow/pull/55427  The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1907,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Setting `converter.inference_type=uint8` does not produce quantized TFLite model of uint8 weight data type)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.8.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** I was trying to convert a TF model to TFLite model and quantize by the way. With `converter.optimizations = [tf.lite.Optimize.DEFAULT]` set, I could get a quantized model of int8 weight data type. I would also want a quantized model of uint8 type, so I tried `converter.inference_type=uint8`, but the produced model is still in int8 data type. **Describe the expected behavior** Set `converter.inference_type=uint8` to produce a quantized TFLite model of **uint8 weight data type**)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fengyuentau,Setting `converter.inference_type=uint8` does not produce quantized TFLite model of uint8 weight data type,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.8.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** I was trying to convert a TF model to TFLite model and quantize by the way. With `converter.optimizations = [tf.lite.Optimize.DEFAULT]` set, I could get a quantized model of int8 weight data type. I would also want a quantized model of uint8 type, so I tried `converter.inference_type=uint8`, but the produced model is still in int8 data type. **Describe the expected behavior** Set `converter.inference_type=uint8` to produce a quantized TFLite model of **uint8 weight data type**",2022-03-08T11:53:32Z,stat:awaiting tensorflower type:bug comp:lite TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55135,https://github.com/tensorflow/modeloptimization/issues/775issuecomment894908619 this comment mentioned that TF2 converter does not support `uint8` quantization any more. Is this the reason why setting `converter.inference_type=uint8` does not help?,Hi  ! Could you please share the model file too? I was suggesting to use a representative dataset and use supported_ops as mentioned in this link  for post integer quantization. Thanks!," Link to the model: http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NHWC.tar.gz, which is the same hyper link in the description.", ! I replicated this issue in 2.8 . Attaching gist for reference. Thanks!,"> tensorflow/modeloptimization CC(TensorBoard ""Split on underscores"" does not split on underscores) (comment) this comment mentioned that TF2 converter does not support `uint8` quantization any more. Is this the reason why setting `converter.inference_type=uint8` does not help? Yes, you're right. TF2 converter does not support `uint8` quantization. ",Are you satisfied with the resolution of your issue? Yes No
1937,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superresolution), pid 9093 (superresolution))ï¼Œ å†…å®¹æ˜¯ (**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible):  TensorFlow Lite in Play Services SDK version (found in `build.gradle`):  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): ` D/goldfishaddressspace: allocate: Ask for block of size 0x100 D/goldfishaddressspace: allocate: ioctl allocate returned offset 0x3fb9b8000 size 0x2000 D/HostConnection: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_null_optional_strings ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_YUV_Cache ANDROID_EMU_async_unmap_buffer ANDROID_EMU_vulkan_ignored_handles ANDROID_EMU_vulkan_free_memory_sync ANDROID_EMU_vulkan_shader_float16_int8 ANDROID_EMU_vulkan_async_queue_submit GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_host_side_tracing ANDROID_EMU_gles_max_version_2  I/tflite: Initialized TensorFlow Lite runtime. I/super_resolution::: Interpreter is created successfully I/System.out: s A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superresolution), pid 9093 (superresolution)` when i run a image(250 * 250), a very lagre image(default size 50 * 50), an error is  A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superreso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",large language model,douzaikongcheng,"A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superresolution), pid 9093 (superresolution)","**System information**  Android Device information (use `adb shell getprop ro.build.fingerprint`   if possible):  TensorFlow Lite in Play Services SDK version (found in `build.gradle`):  Google Play Services version   (`Settings` > `Apps` > `Google Play Services` > `App details`): ` D/goldfishaddressspace: allocate: Ask for block of size 0x100 D/goldfishaddressspace: allocate: ioctl allocate returned offset 0x3fb9b8000 size 0x2000 D/HostConnection: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_null_optional_strings ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_YUV_Cache ANDROID_EMU_async_unmap_buffer ANDROID_EMU_vulkan_ignored_handles ANDROID_EMU_vulkan_free_memory_sync ANDROID_EMU_vulkan_shader_float16_int8 ANDROID_EMU_vulkan_async_queue_submit GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_host_side_tracing ANDROID_EMU_gles_max_version_2  I/tflite: Initialized TensorFlow Lite runtime. I/super_resolution::: Interpreter is created successfully I/System.out: s A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superresolution), pid 9093 (superresolution)` when i run a image(250 * 250), a very lagre image(default size 50 * 50), an error is  A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superreso",2022-03-08T05:19:19Z,stat:awaiting response type:support stale comp:lite,closed,0,25,https://github.com/tensorflow/tensorflow/issues/55123,Hi  ! Can you check the instructions in this thread to attach the log cat here too? Please update device information and SDK version in template too to help expedite the issue . Thanks!,@mohantymï¼Œokï¼Œ SDK version: 28 device informationï¼š huawei p40 log https://github.com/douzaikongcheng/log/blob/main/test.txt,"Hi , if you change the size of the input image, you will also need to retrain/reconvert the model (see https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/ml/super_resolution.ipynb) and change the corresponding values in the native code. I suspect the mismatch between these is what is causing the crash.",Hi@mohantym.This error was encountered when I was testing my training model. I think it may be a memory problem. Please test it again. Thank you,"I don't have the model that you've been using, but you will definitely also have to change the size in native code (https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.hL35).", ! Did you check with 's comment on retraining the model with input size too?,"Hi@mohantym,Hi@sheepmaster I have uploaded my model and the whole code to GitHub. The link is as follows, https://github.com/douzaikongcheng/test_tflite. Please check it. thank you","Hi@mohantym,Hi@sheepmaster Your reply would be appreciated!","Sorry, I haven't had a chance yet to try the sample (I'm not directly on the TF team, I just came across this bug and found an obvious issue). Maybe  can take a look?","@sheepmaster,Thanks.@chunduriv,Can you help me?",", Triage Notes : Here you are trying to use different dimensions than the original example (50x50 > 200x200). Can you try to use the example model with different dimensions?  If it failed while reading a bigger image than the model would actually return (250x250 instead of 200x200), then you would need to train the model using a dataset of 250x250 images. Thanks!","@chunduriv I didn't encounter this error when trying to deploy my model using pytorch mobile, but I encountered it when using tflite. I think it may be a memory problem. My model is not trained and does not need training. I just test whether it can run.Can you check it for me again? The model doesn't need to output the correct superresolution image, as long as it can run, thank you.","@chunduriv I have uploaded my model and the whole code to GitHub. The link is as follows, https://github.com/douzaikongcheng/test_tflite.", could you please help take a look at this issue?, Could you take a look? Thanks!," are you using the ESRGAN model from TF Hub? That model can only take [50,50] input image (you can see it in Netron) because the model author fixed the dimensions when he did the conversion. If you want to use another dimension (or dynamic shape), you are going to have to retrain and reconvert the model (it's nontrivial work). The source code link is on TF Hub page. If you are using a model trained by PyTorch and then somehow converted to TFLite, there is not much we can do, since we are not responsible for the PyTorch > TF/TFLite conversion. If you suspect it's a memory issue, most likely it's OOM, in which case you can check memory usage using 'top' or the like. Hope this helps.",@windmaple Thanks,", Could you please confirm if this issue is resolved for you? Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"@windmapleRecently, I used tensorflow to train a superresolution model, but this error still occurs, that is, an error will be reported when the image input size is too large, but there is no problem when the image is small",I'm pretty you ran out of memory. You can confirm this by using benchmark tool to see how much memory it is using.  Super resolution models are quite memoryhungry and you should look into model optimization techniques like distillation.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,I got same issue
1846,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFHub Object Detection Models Cannot Quantize)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.8.2  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: N/A  GPU model and memory: Nvidia GeForce GTX 1050 8GB You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** When taking the Object Detection Saved Models provided on https://tfhub.dev/tensorflow/collections/object_detection/1 and converting them to TFLite with quantization enabled, the model outputted remains as a fully float model. **Describe the expected behavior** When converting these Saved Models using quantization enabled, one can produce a quantized TFLite. **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to genera)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,msquigle,TFHub Object Detection Models Cannot Quantize,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.8.2  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: N/A  GPU model and memory: Nvidia GeForce GTX 1050 8GB You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** When taking the Object Detection Saved Models provided on https://tfhub.dev/tensorflow/collections/object_detection/1 and converting them to TFLite with quantization enabled, the model outputted remains as a fully float model. **Describe the expected behavior** When converting these Saved Models using quantization enabled, one can produce a quantized TFLite. **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to genera",2022-03-07T18:14:33Z,stat:awaiting response type:bug stale comp:lite TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55073," Was able to replicate the issue on colab using TF v2.8.0 and tfnightly(2.9.0.dev20220313) , please find the gist here.Thanks!",Hi  ! You can get an integer quantized model by adding the three lines after the representative dataset. Attaching gist for reference. ,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1743,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow/core/kernels/conv_ops_gpu.cc:336] None of the algorithms provided by cuDNN frontend heuristics worked)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04  TensorFlow installed from (source or binary): source  TensorFlow version: master  Python version: 3.9.9  Bazel version (if compiling from source): bazel 5.0.0  GCC/Compiler version (if compiling from source): g++ (Ubuntu 7.5.03ubuntu1~16.04) 7.5.0  CUDA/cuDNN version: 11.4  GPU model and memory: NVIDIA GeForce RTX 3090 sm_8.6 with 25447170048B RAM and 82 cores **Describe the problem** after I build and installed, when I run model with conv3d ops, it will return the following msg: > 20220225 09:26:56.521222: W tensorflow/core/kernels/conv_ops_gpu.cc:336] None of the algorithms provided by cuDNN frontend heuristics worked; trying fallback algorithms.  Conv: batch: 1 > in_depths: 1 > out_depths: 40 > in: 256 > in: 256 > in: 256 > data_format: 1 > filter: 3 > filter: 3 > filter: 3 > dilation: 1 > dilation: 1 > dilation: 1 > stride: 1 > stride: 1 > stride: 1 > padding: 2 > padding: 2 > padding: 2 > dtype: DT_FLOAT > group_count: 1 > device_identifier: ""NVIDIA GeForce RTX 3090 sm_8.6 with 25447170048B RAM and 82 cores"" > version: 1 >  **Provide the exact sequence of commands / steps that you executed before running into the problem** anything that use conv3d ops **Any other info / logs** the model, like unet, can be finished even with the above msg. But I think this msg shows that cuDNN is not envolved? and hence the gpu is not used?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,WingsOfPanda,tensorflow/core/kernels/conv_ops_gpu.cc:336] None of the algorithms provided by cuDNN frontend heuristics worked,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04  TensorFlow installed from (source or binary): source  TensorFlow version: master  Python version: 3.9.9  Bazel version (if compiling from source): bazel 5.0.0  GCC/Compiler version (if compiling from source): g++ (Ubuntu 7.5.03ubuntu1~16.04) 7.5.0  CUDA/cuDNN version: 11.4  GPU model and memory: NVIDIA GeForce RTX 3090 sm_8.6 with 25447170048B RAM and 82 cores **Describe the problem** after I build and installed, when I run model with conv3d ops, it will return the following msg: > 20220225 09:26:56.521222: W tensorflow/core/kernels/conv_ops_gpu.cc:336] None of the algorithms provided by cuDNN frontend heuristics worked; trying fallback algorithms.  Conv: batch: 1 > in_depths: 1 > out_depths: 40 > in: 256 > in: 256 > in: 256 > data_format: 1 > filter: 3 > filter: 3 > filter: 3 > dilation: 1 > dilation: 1 > dilation: 1 > stride: 1 > stride: 1 > stride: 1 > padding: 2 > padding: 2 > padding: 2 > dtype: DT_FLOAT > group_count: 1 > device_identifier: ""NVIDIA GeForce RTX 3090 sm_8.6 with 25447170048B RAM and 82 cores"" > version: 1 >  **Provide the exact sequence of commands / steps that you executed before running into the problem** anything that use conv3d ops **Any other info / logs** the model, like unet, can be finished even with the above msg. But I think this msg shows that cuDNN is not envolved? and hence the gpu is not used?",2022-03-07T13:35:50Z,stat:awaiting response type:build/install subtype: ubuntu/linux subtype:bazel,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55059,Hi  ! Could you please try again after a building from a source with Cuda 11.2 and CudNN 8.1   with 2.8 Branch. Please use Bazel 4.2.1 while building from the source. Thank you!,Hi  . Thank you for your suggestions. I changed to coda 11.2 and cudnn 8.1 and it worked! I still use Bazel 5.0.0 btw. ,Are you satisfied with the resolution of your issue? Yes No,"I solve this problem by install tensorflow using pip rather than conda. try ""pip install tensorflowgpu"""
371,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TestExecutionEnvironment extended to provide info about storages that support in/uint types.)ï¼Œ å†…å®¹æ˜¯ (TestExecutionEnvironment extended to provide info about storages that support in/uint types.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],TestExecutionEnvironment extended to provide info about storages that support in/uint types.,TestExecutionEnvironment extended to provide info about storages that support in/uint types.,2022-03-07T11:39:36Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55056
704,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA] Fix the bug in Literal storage: make it consistent with on-device buffer representation)ï¼Œ å†…å®¹æ˜¯ ([XLA] Fix the bug in Literal storage: make it consistent with ondevice buffer representation Otherwise, copies using TransferManager may result in OOB read/writes. As a bonus, the resulting code is also considerably simpler. We have to overallocate Literals (always allocate the ""margin"" for the dynamic sizes), but given that this allocation is tiny and is not used on performancecritical path, this should not be an issue.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],[XLA] Fix the bug in Literal storage: make it consistent with on-device buffer representation,"[XLA] Fix the bug in Literal storage: make it consistent with ondevice buffer representation Otherwise, copies using TransferManager may result in OOB read/writes. As a bonus, the resulting code is also considerably simpler. We have to overallocate Literals (always allocate the ""margin"" for the dynamic sizes), but given that this allocation is tiny and is not used on performancecritical path, this should not be an issue.",2022-03-07T06:12:37Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55052
1784,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom operation on extracted volume patches)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are ): I'm using TensorFlow version 2.8  Are you willing to contribute it (Yes/No): **Describe the feature and the current behavior/state.** Custom operation on sliding kernels on volumes of data would be really helpful. I know off the function _tf.extract_volume_patches_ but there are some drawbacks to this. It collects the patches but consumes lots of memory resource. I mean what I had in mind was performing some kind of custom operation on data collected through a sliding window and using this function would eventually lead to exhausting my available resource. And also it would be great to add a kernel mask to specify how the sampling would be performed. In short two I recommend more arguments to this function : 1) An argument to specify a function or a lambda to call upon every sampled window right after sampling and return and save the result instead of the sampled window itself 2) A sampling mask. Specifying which elements should be sampled ( I know it can be done using the this very function but again the constrain is the memory usage)  **Will this change the current api? How?** I think it wouldn't change anything  **Who will benefit with this feature?** This would be very helpful for image processing and feature extraction out of large volumes of data  **Any Other info.**)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ShowStopperTheSecond,Custom operation on extracted volume patches,"Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are ): I'm using TensorFlow version 2.8  Are you willing to contribute it (Yes/No): **Describe the feature and the current behavior/state.** Custom operation on sliding kernels on volumes of data would be really helpful. I know off the function _tf.extract_volume_patches_ but there are some drawbacks to this. It collects the patches but consumes lots of memory resource. I mean what I had in mind was performing some kind of custom operation on data collected through a sliding window and using this function would eventually lead to exhausting my available resource. And also it would be great to add a kernel mask to specify how the sampling would be performed. In short two I recommend more arguments to this function : 1) An argument to specify a function or a lambda to call upon every sampled window right after sampling and return and save the result instead of the sampled window itself 2) A sampling mask. Specifying which elements should be sampled ( I know it can be done using the this very function but again the constrain is the memory usage)  **Will this change the current api? How?** I think it wouldn't change anything  **Who will benefit with this feature?** This would be very helpful for image processing and feature extraction out of large volumes of data  **Any Other info.**",2022-03-06T14:05:44Z,stat:awaiting response stat:awaiting tensorflower type:feature stale comp:data,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55043,Hi  ! Could you please look at this feature request?,", Would you like to raise PR for proposed feature request? ",", I'd love to but unfortunately I don't have the technical knowledge and skills necessary to do :(","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1803,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite Python:  interpreter._get_tensor_details(index) can't read tensor with sparsity)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Source  TensorFlow version (use command below): 2.8.0  Python version: 3.7.0  Bazel version (if compiling from source):  5.0.0  GCC/Compiler version (if compiling from source): MSVC 2019  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** In the Python code below, load the Mediapipe pose_detection tflie model, and try to get details of tensor index 15. The program crashes during `interpreter._get_tensor_details(15)`, and the `print(details)` never executes.  I analysed it and found the call stack is `interpreter._get_tensor_details(index)` > `_interpreter.TensorSparsityParameters()` > `InterpreterWrapper::TensorSparsityParameters()` > `PyDictFromSparsityParam()` > `PyArrayFromIntVector()` tensorflow\lite\python\interpreter.py  tensorflow\lite\python\interpreter_wrapper\interpreter_wrapper.cc  In the tensor index 15 of pose_detection.tflite, `tensor>sparsity` is not null, but `tensor>sparsity>traversal_order>data` is null. So the `memcpy()` in `PyArrayFromIntVector()` crashed with null pointer exception. Could you check why pose_detection.tflite can be inferred normally in Mediapipe, but can't be read by `interpreter._get_tensor_details(index)`? Thanks.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,zk-talentech,TFLite Python:  interpreter._get_tensor_details(index) can't read tensor with sparsity,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Source  TensorFlow version (use command below): 2.8.0  Python version: 3.7.0  Bazel version (if compiling from source):  5.0.0  GCC/Compiler version (if compiling from source): MSVC 2019  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** In the Python code below, load the Mediapipe pose_detection tflie model, and try to get details of tensor index 15. The program crashes during `interpreter._get_tensor_details(15)`, and the `print(details)` never executes.  I analysed it and found the call stack is `interpreter._get_tensor_details(index)` > `_interpreter.TensorSparsityParameters()` > `InterpreterWrapper::TensorSparsityParameters()` > `PyDictFromSparsityParam()` > `PyArrayFromIntVector()` tensorflow\lite\python\interpreter.py  tensorflow\lite\python\interpreter_wrapper\interpreter_wrapper.cc  In the tensor index 15 of pose_detection.tflite, `tensor>sparsity` is not null, but `tensor>sparsity>traversal_order>data` is null. So the `memcpy()` in `PyArrayFromIntVector()` crashed with null pointer exception. Could you check why pose_detection.tflite can be inferred normally in Mediapipe, but can't be read by `interpreter._get_tensor_details(index)`? Thanks.",2022-03-06T07:57:58Z,stat:awaiting response type:bug stale comp:lite TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/55040,I could replicate this issue in 2.8 . Used two different pose net models  (one from Tensorflow site and another from mediapipe) . I saw there is a difference on index value coming from gist and  Ones with Netron view. Attaching gist for reference. Thanks!,"Hi talentech  I was checking if the issue still persists. Seems like mediapipe `tflite` model is unavailabe at pose detection. The `interpreter._get_tensor_details()` require tensor index as well the sub graph index. If the model has only one subgraph, try `interpreter._get_tensor_details(15,0)` that should give the tensor details at the index 15. Indexing beyond sub graph indices might lead to the program crash. I have tested with Movenet multipose tflite model available in tf hub and was able to get the tesnor details at an index. Please find the gist here. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
730,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AutoGraph could not transform when fine-tuning HuggingFace Pretrained Model)ï¼Œ å†…å®¹æ˜¯ (**System information** Macbook OS 11.6  **Describe the current behavior** During` .fit()`, receiving warnings:   and  _Would not have submitted except for the ""please report"" phrase in first warning. Happy to close quickly if this is resolved/trivial._ **Describe the expected behavior** Neither of these warnings to appear prior to training. **Standalone code to reproduce the issue**  **Other info / logs**  Verbose==10 logs leading up to warning: tf_verbose_log.txt)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,adam1brownell,AutoGraph could not transform when fine-tuning HuggingFace Pretrained Model,"**System information** Macbook OS 11.6  **Describe the current behavior** During` .fit()`, receiving warnings:   and  _Would not have submitted except for the ""please report"" phrase in first warning. Happy to close quickly if this is resolved/trivial._ **Describe the expected behavior** Neither of these warnings to appear prior to training. **Standalone code to reproduce the issue**  **Other info / logs**  Verbose==10 logs leading up to warning: tf_verbose_log.txt",2022-03-06T02:03:06Z,stat:awaiting response type:support stale comp:autograph TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55038,", You can suppress all debugging logs using below code snippet. Try setting log level before importing tf  OR ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Mulitple inputs not parsing properly with dictionary)ï¼Œ å†…å®¹æ˜¯ (I'm getting an error with multiple inputs  something like this `         customer = Input(shape=(1,), name=""customer_id"")   1length sequence of ints     age = Input(shape=(1,), name=""age"")   1length sequence of ints     other_customer_input = Input(shape=(None,7), name=""customer_info"") 7length sequence of ints     other_customer_input = layers.Dense(7,kernel_regularizer='l1')(other_customer_input)     cust_embedding = layers.Embedding(num_of_cx, 256)(customer)     dem_embedding = layers.Embedding(num_of_age, 16)(age)     x = layers.concatenate(axis=2,inputs=[customer_embedding,postal_code_embedding,other_customer_input])     x = layers.Dense(450,kernel_regularizer='l1')(x)     pref = layers.Dense(619, name=""pref"",kernel_regularizer='l1')(x)     model = tf.keras.Model(         inputs=[age, customer, other_customer_input],         outputs=[pref]     )     x = layers.concatenate(axis=2,inputs=[customer_embedding,postal_code_embedding,other_customer_input])     x = layers.Dense(450,kernel_regularizer='l1')(x)     model.compile()     X= {""customer_id"": data['customer_id'], ""age"": data['age'], ""customer_info"": data[cols]}     Y={""pref"": data[cols])}     pref_model.fit(X,Y,         epochs=2,         batch_size=64) ` This is the full error message  It's not accepting a dictionary as an input INFO:tensorflow:Allowlisted: .slice_batch_indices at 0x7fb010938940>: DoNotConvert rule for keras INFO:tensorflow:Converted call: .grab_batch at 0x7faf82c8cd30>     args: (, ({'customer_id': , 'age': , 'customer_info': }, {'pref': }))     kwargs: {} INFO:tensorflow:Allowlisted: .grab_batch at 0x7faf82c8cd30>)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tjaqu787,Mulitple inputs not parsing properly with dictionary,"I'm getting an error with multiple inputs  something like this `         customer = Input(shape=(1,), name=""customer_id"")   1length sequence of ints     age = Input(shape=(1,), name=""age"")   1length sequence of ints     other_customer_input = Input(shape=(None,7), name=""customer_info"") 7length sequence of ints     other_customer_input = layers.Dense(7,kernel_regularizer='l1')(other_customer_input)     cust_embedding = layers.Embedding(num_of_cx, 256)(customer)     dem_embedding = layers.Embedding(num_of_age, 16)(age)     x = layers.concatenate(axis=2,inputs=[customer_embedding,postal_code_embedding,other_customer_input])     x = layers.Dense(450,kernel_regularizer='l1')(x)     pref = layers.Dense(619, name=""pref"",kernel_regularizer='l1')(x)     model = tf.keras.Model(         inputs=[age, customer, other_customer_input],         outputs=[pref]     )     x = layers.concatenate(axis=2,inputs=[customer_embedding,postal_code_embedding,other_customer_input])     x = layers.Dense(450,kernel_regularizer='l1')(x)     model.compile()     X= {""customer_id"": data['customer_id'], ""age"": data['age'], ""customer_info"": data[cols]}     Y={""pref"": data[cols])}     pref_model.fit(X,Y,         epochs=2,         batch_size=64) ` This is the full error message  It's not accepting a dictionary as an input INFO:tensorflow:Allowlisted: .slice_batch_indices at 0x7fb010938940>: DoNotConvert rule for keras INFO:tensorflow:Converted call: .grab_batch at 0x7faf82c8cd30>     args: (, ({'customer_id': , 'age': , 'customer_info': }, {'pref': }))     kwargs: {} INFO:tensorflow:Allowlisted: .grab_batch at 0x7faf82c8cd30>",2022-03-05T21:25:02Z,stat:awaiting response type:support stale comp:keras,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55037,Hi  ! Could you please share the above code as a Colab gist? Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1726,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to make pipeline parallelism in Tensorflow runtime with custom CUDA steam)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.5.0  Python version: 3.8.7  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 7.3.1  CUDA/cuDNN version: cuda11.0, cudnn8.2  GPU model and memory: A100, 40GB HBM.  Description I'm trying to run a recommendersystem for advertisement application with TensorFlow, on A100 X 16, and run a sparse embedding_lookup on a modelparallized embedding layer (embedding layer is splitted to 16 partitions on each GPU, and every worker send the lookup indices to others, and gather the lookup  results to local). One problem is: I'm trying to split a whole batch input into several minibatches, and let the kth minibatch be parallized to the (k+1)th minibatch, with a stage step bebind, to overlap the elementwise computations between minibatches. But I haven't found a method to implement the idea, since the TensorFlow runtime run an op in graph when its upstream ops are finished. Another problem is in an AsyncOpKernel, I found that a cudaStream created by cudaStreamCreate(), not d.stream(), is not synchronizable to the host code:  I struggled with it for several weeks, I will be really appreciated if someone could answer the issue (X0X) !)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Lifann,How to make pipeline parallelism in Tensorflow runtime with custom CUDA steam,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.5.0  Python version: 3.8.7  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 7.3.1  CUDA/cuDNN version: cuda11.0, cudnn8.2  GPU model and memory: A100, 40GB HBM.  Description I'm trying to run a recommendersystem for advertisement application with TensorFlow, on A100 X 16, and run a sparse embedding_lookup on a modelparallized embedding layer (embedding layer is splitted to 16 partitions on each GPU, and every worker send the lookup indices to others, and gather the lookup  results to local). One problem is: I'm trying to split a whole batch input into several minibatches, and let the kth minibatch be parallized to the (k+1)th minibatch, with a stage step bebind, to overlap the elementwise computations between minibatches. But I haven't found a method to implement the idea, since the TensorFlow runtime run an op in graph when its upstream ops are finished. Another problem is in an AsyncOpKernel, I found that a cudaStream created by cudaStreamCreate(), not d.stream(), is not synchronizable to the host code:  I struggled with it for several weeks, I will be really appreciated if someone could answer the issue (X0X) !",2022-03-05T13:02:07Z,stat:awaiting response stale type:performance TF 2.5,closed,0,4,https://github.com/tensorflow/tensorflow/issues/55030,", In order to expedite the troubleshooting process here, could you please give more information on this issue and fill the issue template. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further., Any latest progress?
261,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(trying to print the fusing decisions.)ï¼Œ å†…å®¹æ˜¯ (trying to print the fusing decisions.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],trying to print the fusing decisions.,trying to print the fusing decisions.,2022-03-05T08:35:40Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55028
485,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Developer Certificate error sign in on pycharm )ï¼Œ å†…å®¹æ˜¯ (I am able to successfully sign in to the Certification Assistant. However, I keep getting this error on pycharm ""An error occurred while trying to sign in: Connection reset"". I don't know why it happened !Screenshot 20220305 at 2 43 15 PM .)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,callmekofi,TensorFlow Developer Certificate error sign in on pycharm ,"I am able to successfully sign in to the Certification Assistant. However, I keep getting this error on pycharm ""An error occurred while trying to sign in: Connection reset"". I don't know why it happened !Screenshot 20220305 at 2 43 15 PM .",2022-03-05T06:43:45Z,stat:awaiting response stale type:others,closed,0,6,https://github.com/tensorflow/tensorflow/issues/55027,Hi  ! Did you try again after disabling your antivirus/VPN or whitelisting the IP address linked with Tensorflow exam ? Can you also confirm that you are on Pycharm version  2021.3?,I am using pycharm version 2021.3 on apple macbook. I turned off my vpn once i sign in my google account but the result is the same. ,"Ok  !  1. Please uninstall then reinstall the Tensorflow certification plugin  and accept policies wherever needed.  2. Please check from different devices aside macbook( There are performance issue in Mac OS .You can refer https://developer.apple.com/metal/tensorflowplugin/ for installation in Mac) . For general support and questions regarding the Google Tensorflow Developer Certification, please contact tensorflowcertificatesupport.com.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,"I meet the same problem. I successfully login via the opened browser, but the Pycharm occurs the Time out Error. I have sent the mail the tensorflowcertificatesupport.com, but still get no reply. My test only left 3 days,.Please help me.  "
231,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Internal changes only.)ï¼Œ å†…å®¹æ˜¯ (Internal changes only.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],Internal changes only.,Internal changes only.,2022-03-05T01:23:46Z,size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55020
697,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Generalize einsum to dot_general lowering to allow transposed results.)ï¼Œ å†…å®¹æ˜¯ (Generalize einsum to dot_general lowering to allow transposed results. This allows ops like this to be lowered properly (was generated illegal dot_general ops):  I found the existing lowering quite buggy, and I believe that two of the tests are incorrect, generating dot_general ops that are actually illegal. MHLO is so underspecified and underverified, though, so that it is next to impossible to say, so I am just applying best judgment.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],Generalize einsum to dot_general lowering to allow transposed results.,"Generalize einsum to dot_general lowering to allow transposed results. This allows ops like this to be lowered properly (was generated illegal dot_general ops):  I found the existing lowering quite buggy, and I believe that two of the tests are incorrect, generating dot_general ops that are actually illegal. MHLO is so underspecified and underverified, though, so that it is next to impossible to say, so I am just applying best judgment.",2022-03-04T20:37:02Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/55004
1826,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build of unit tests fails)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.7.5  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source): 5.0.0  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Building unit tests fails with ================================================================================ ERROR: /tmp/workspace/tensorflowgit/tensorflow/compiler/mlir/lite/quantization/BUILD:152:20: Compiling tensorflow/compiler/mlir/lite/quantization/tools/tflite_op_coverage_spec_getters_gen.: (Exit 1): gcc failed: error executing command    (cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \   exec env  \     LD_LIBRARY_PATH=/opt/rh/devtoolset10/root/usr/lib64:/opt/rh/devtoolset10/root/usr/lib:/opt/rh/devtoolset10/root/usr/lib64/dyninst:/opt/rh/devtoolset10/root/usr/lib/dyninst:/usr/local/lib64:/usr/lib \     PATH=/tmp/workspace/venvcp310cp310/bin:/opt/rh/devtoolset10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \     PWD=/proc/self/cwd \   /opt/rh/devtoolset10/root/usr/bin/gcc U_F)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,elfringham,Build of unit tests fails,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.7.5  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source): 5.0.0  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Building unit tests fails with ================================================================================ ERROR: /tmp/workspace/tensorflowgit/tensorflow/compiler/mlir/lite/quantization/BUILD:152:20: Compiling tensorflow/compiler/mlir/lite/quantization/tools/tflite_op_coverage_spec_getters_gen.: (Exit 1): gcc failed: error executing command    (cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \   exec env  \     LD_LIBRARY_PATH=/opt/rh/devtoolset10/root/usr/lib64:/opt/rh/devtoolset10/root/usr/lib:/opt/rh/devtoolset10/root/usr/lib64/dyninst:/opt/rh/devtoolset10/root/usr/lib/dyninst:/usr/local/lib64:/usr/lib \     PATH=/tmp/workspace/venvcp310cp310/bin:/opt/rh/devtoolset10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \     PWD=/proc/self/cwd \   /opt/rh/devtoolset10/root/usr/bin/gcc U_F",2022-03-04T15:26:28Z,type:build/install subtype: ubuntu/linux,closed,0,2,https://github.com/tensorflow/tensorflow/issues/54988,"Not sure why this was tagged TF 2.8, it does not affect TF 2.8 AFAIK. This was introduced by https://github.com/tensorflow/tensorflow/commit/559586be3d2c43a15d68b0bae2493cabf52f430b This seems to have been fixed by https://github.com/tensorflow/tensorflow/commit/32cc6df68dcc26cc2958f8bf557c77ac623baf9c",Are you satisfied with the resolution of your issue? Yes No
1846,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Not able to build tensorflow2.5.3 from source)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:  Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Centos 7    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No     **TensorFlow installed from (source or binary)**: source    **TensorFlow version (use command below)**: 2.5.3    **Python version**: 3.8    **Bazel version (if compiling from source)**: 3.7.2    **GCC/Compiler version (if compiling from source)**: 9.3.0    **CUDA/cuDNN version**: not applied    **GPU model and memory**: not applied    **Exact command to reproduce**:  bazel output_user_root=/cache/ build config=v2 copt=m64 copt=march=native config=monolithic config=opt verbose_failures c opt cxxopt=std=c++17 cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:tensorflow_cc //tensorflow:install_headers tensorflow:tensorflow_framework //tensorflow/tools/... //tensorflow/examples/...  Describe the problem Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. I have built bazel 3.7.0 from the source and now building TensorFlow 2.5.3 from the source with GCC 9.3.0 from devtoolset in centos 7. I am getting the below error.    Source code / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. Error Messa)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rajeev921,Not able to build tensorflow2.5.3 from source," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:  Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Centos 7    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: No     **TensorFlow installed from (source or binary)**: source    **TensorFlow version (use command below)**: 2.5.3    **Python version**: 3.8    **Bazel version (if compiling from source)**: 3.7.2    **GCC/Compiler version (if compiling from source)**: 9.3.0    **CUDA/cuDNN version**: not applied    **GPU model and memory**: not applied    **Exact command to reproduce**:  bazel output_user_root=/cache/ build config=v2 copt=m64 copt=march=native config=monolithic config=opt verbose_failures c opt cxxopt=std=c++17 cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:tensorflow_cc //tensorflow:install_headers tensorflow:tensorflow_framework //tensorflow/tools/... //tensorflow/examples/...  Describe the problem Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. I have built bazel 3.7.0 from the source and now building TensorFlow 2.5.3 from the source with GCC 9.3.0 from devtoolset in centos 7. I am getting the below error.    Source code / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. Error Messa",2022-03-04T14:21:01Z,stat:awaiting response type:build/install stale subtype:bazel TF 2.5,closed,0,6,https://github.com/tensorflow/tensorflow/issues/54986,", > ERROR: /tensorflow2.5.3/tensorflow/compiler/mlir/lite/BUILD:627:20: Linking of rule '//tensorflow/compiler/mlir/lite:convertergen' failed (Exit 1): gcc failed: error executing command > (cd /cache/1c211944fffc93afc232f5e22d2690de/execroot/org_tensorflow && > exec env  This issue is regarding GCC compiler. Compatibility issue. Tensorflow 2.5 supports GCC 7.3.1. Take a look at Tested and Build configuration . Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> when i compile tensorflow2.8 with bazel 4.2.2ã€GCC7.3.1ã€python3.7.12, it also occur the eame error? do you have any suggestions to solve this problem ? the output log as follows: ERROR: /home/tensorflow2.8.0/tensorflow/compiler/mlir/xla/BUILD:676:20: Linking tensorflow/compiler/mlir/xla/operator_writer_gen failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command   (cd /root/.cache/bazel/_bazel_root/7e60a61be90f35a88e89e062446eed75/execroot/org_tensorflow && \   exec env  \     LD_LIBRARY_PATH=/opt/rh/devtoolset7/root/usr/lib64:/opt/rh/devtoolset7/root/usr/lib:/opt/rh/devtoolset7/root/usr/lib64/dyninst:/opt/rh/devtoolset7/root/usr/lib/dyninst:/opt/rh/devtoolset7/root/usr/lib64:/opt/rh/devtoolset7/root/usr/lib \     PATH=/home/tensorflow2.8.0/Depend/bin:/home/tensorflow/Depend/bin:/opt/git2.30.0/bin:/opt/cmake3.19.3Linuxx86_64/bin:/opt/rh/devtoolset7/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \     PWD=/proc/self/cwd \   external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc out/k8optexec50AE0418/bin/tensorflow/compiler/mlir/xla/operator_writer_gen2.params) Execution platform: //:platform bazelout/k8optexec50AE0418/bin/external/llvmproject/mlir/_objs/TableGen/Operator.o:Operator.cpp:function llvm::detail::provider_format_adapter::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/mlir/_objs/TableGen/Operator.o:Operator.cpp:function llvm::detail::provider_format_adapter::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/mlir/_objs/TableGen/Operator.o:Operator.cpp:function llvm::detail::provider_format_adapter::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/mlir/_objs/TableGen/Operator.o:Operator.cpp:function llvm::detail::provider_format_adapter::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const*, bool (*)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const*, bool (*)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/CommandLine.o:CommandLine.cpp:function ExpandResponseFile(llvm::StringRef, llvm::StringSaver&, void (*)(llvm::StringRef, llvm::StringSaver&, llvm::SmallVectorImpl&, bool), llvm::SmallVectorImpl&, bool, bool, llvm::vfs::FileSystem&): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/ConvertUTFWrapper.o:ConvertUTFWrapper.cpp:function llvm::convertUTF16ToUTF8String(llvm::ArrayRef, std::string&): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function (anonymous namespace)::ErrorErrorCategory::~ErrorErrorCategory(): error: undefined reference to 'std::_V2::error_category::~error_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function (anonymous namespace)::ErrorErrorCategory::~ErrorErrorCategory(): error: undefined reference to 'std::_V2::error_category::~error_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function llvm::object_deleter::call(void*): error: undefined reference to 'std::_V2::error_category::~error_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function llvm::errorToErrorCode(llvm::Error): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:typeinfo for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'typeinfo for std::_V2::error_category' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::_M_message(int) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::default_error_condition(int) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::equivalent(int, std::error_condition const&) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::equivalent(std::error_code const&, int) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::write_impl(char const*, unsigned long): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function getFD(llvm::StringRef, std::error_code&, llvm::sys::fs::CreationDisposition, llvm::sys::fs::FileAccess, llvm::sys::fs::OpenFlags): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::seek(unsigned long): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_stream::raw_fd_stream(llvm::StringRef, std::error_code&): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/StringRef.o:StringRef.cpp:function llvm::APFloat::Storage::~Storage(): error: undefined reference to 'operator delete[](void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/StringRef.o:StringRef.cpp:function llvm::APFloat::Storage::~Storage(): error: undefined reference to 'operator delete[](void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/StringRef.o:StringRef.cpp:function llvm::APFloat::Storage::~Storage(): error: undefined reference to 'operator delete[](void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/StringRef.o:StringRef.cpp:function llvm::APFloat::Storage::~Storage(): error: undefined reference to 'operator delete[](void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/StringRef.o:StringRef.cpp:function unsigned int llvm::ComputeEditDistance(llvm::ArrayRef, llvm::ArrayRef, bool, unsigned int): error: undefined reference to '__cxa_throw_bad_array_new_length' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/SourceMgr.o:SourceMgr.cpp:function llvm::SMDiagnostic::print(char const*, llvm::raw_ostream&, bool, bool) const: error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)' collect2: error: ld returned 1 exit status",">  System information > * **Have I written custom code (as opposed to using a stock example script >   provided in TensorFlow)**:  Yes > * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Centos 7 > * **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue >   happens on a mobile device**: No > * **TensorFlow installed from (source or binary)**: source > * **TensorFlow version (use command below)**: 2.5.3 > * **Python version**: 3.8 > * **Bazel version (if compiling from source)**: 3.7.2 > * **GCC/Compiler version (if compiling from source)**: 9.3.0 > * **CUDA/cuDNN version**: not applied > * **GPU model and memory**: not applied > * **Exact command to reproduce**:  bazel output_user_root=/cache/ build config=v2 copt=m64 copt=march=native config=monolithic config=opt verbose_failures c opt cxxopt=std=c++17 cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:tensorflow_cc //tensorflow:install_headers tensorflow:tensorflow_framework //tensorflow/tools/... //tensorflow/examples/... >  >  Describe the problem > Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. >  > I have built bazel 3.7.0 from the source and now building TensorFlow 2.5.3 from the source with GCC 9.3.0 from devtoolset in centos 7. I am getting the below error. >  >  Source code / logs > Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. >  > Error Message  >  > bazel output_user_root=/cache/ build config=v2 copt=m64 copt=march=native config=monolithic config=opt verbose_failures c opt cxxopt=std=c++17 cxxopt=""D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:tensorflow_cc //tensorflow:install_headers tensorflow:tensorflow_framework //tensorflow/tools/... //tensorflow/examples/... Extracting Bazel installation... Starting local Bazel server and connecting to it... WARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior. INFO: Options provided by the client: Inherited 'common' options: isatty=1 terminal_columns=138 INFO: Reading rc options for 'build' from /tensorflow2.5.3/.bazelrc: Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'build' from /tensorflow2.5.3/.bazelrc: 'build' options: define framework_shared_object=true java_toolchain=//toolchains/java:tf_java_toolchain host_java_toolchain=//toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive noincompatible_prohibit_aapt1 enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 INFO: Reading rc options for 'build' from /tensorflow2.5.3/.tf_configure.bazelrc: 'build' options: action_env PYTHON_BIN_PATH=/remote/home/rrajeev/Software/anaconda3/envs/tf/bin/python3 action_env PYTHON_LIB_PATH=/remote/home/rrajeev/Software/anaconda3/envs/tf/lib/python3.8/sitepackages python_path=/remote/home/rrajeev/Software/anaconda3/envs/tf/bin/python3 INFO: Found applicable config definition build:short_logs in file /tensorflow2.5.3/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /tensorflow2.5.3/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:v2 in file /tensorflow2.5.3/.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:monolithic in file /tensorflow2.5.3/.bazelrc: define framework_shared_object=false INFO: Found applicable config definition build:opt in file /tensorflow2.5.3/.tf_configure.bazelrc: copt=Wnosigncompare host_copt=Wnosigncompare INFO: Found applicable config definition build:linux in file /tensorflow2.5.3/.bazelrc: copt=w host_copt=w define=PREFIX=/usr define=LIBDIR=$(PREFIX)/lib define=INCLUDEDIR=$(PREFIX)/include define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include cxxopt=std=c++14 host_cxxopt=std=c++14 config=dynamic_kernels INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow2.5.3/.bazelrc: define=dynamic_loaded_kernels=true copt=DAUTOLOAD_DYNAMIC_KERNELS DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 0400"" DEBUG: Repository io_bazel_rules_docker instantiated at: /tensorflow2.5.3/WORKSPACE:23:14: in /tensorflow2.5.3/tensorflow/workspace0.bzl:105:34: in workspace /cache/1c211944fffc93afc232f5e22d2690de/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories Repository rule git_repository defined at: /cache/1c211944fffc93afc232f5e22d2690de/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in INFO: Analyzed 193 targets (453 packages loaded, 36344 targets configured). INFO: Found 193 targets... ERROR: /tensorflow2.5.3/tensorflow/compiler/mlir/lite/BUILD:627:20: Linking of rule '//tensorflow/compiler/mlir/lite:convertergen' failed (Exit 1): gcc failed: error executing command (cd /cache/1c211944fffc93afc232f5e22d2690de/execroot/org_tensorflow && exec env  LD_LIBRARY_PATH=/usr/path PATH= /usr/path PWD=/proc/self/cwd /opt/rh/devtoolset9/root/usr/bin/gcc out/k8optexec50AE0418/bin/tensorflow/compiler/mlir/lite/convertergen2.params) Execution platform: //:platform bazelout/k8optexec50AE0418/bin/tensorflow/compiler/mlir/lite/_objs/convertergen/converter_gen.o:converter_gen.cc:function llvm::detail::provider_format_adapter::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/tensorflow/compiler/mlir/lite/_objs/convertergen/converter_gen.o:converter_gen.cc:function llvm::detail::provider_format_adapter::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/tensorflow/compiler/mlir/lite/_objs/convertergen/converter_gen.o:converter_gen.cc:function llvm::detail::provider_format_adapterllvm::StringRef::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/tensorflow/compiler/mlir/lite/_objs/convertergen/converter_gen.o:converter_gen.cc:function llvm::detail::provider_format_adapterllvm::StringRef&::~provider_format_adapter(): error: undefined reference to 'operator delete(void*, unsigned long)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const*, bool (_)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const_, bool (_)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/CommandLine.o:CommandLine.cpp:function ExpandResponseFile(llvm::StringRef, llvm::StringSaver&, void (_)(llvm::StringRef, llvm::StringSaver&, llvm::SmallVectorImpl&, bool), llvm::SmallVectorImpl&, bool, bool, llvm::vfs::FileSystem&): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/ConvertUTFWrapper.o:ConvertUTFWrapper.cpp:function llvm::convertUTF16ToUTF8String(llvm::ArrayRef, std::string&): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function (anonymous namespace)::ErrorErrorCategory::~ErrorErrorCategory(): error: undefined reference to 'std::_V2::error_category::~error_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function (anonymous namespace)::ErrorErrorCategory::~ErrorErrorCategory(): error: undefined reference to 'std::_V2::error_category::~error_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function llvm::object_deleter::call(void*): error: undefined reference to 'std::_V2::error_category::~error_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:function llvm::errorToErrorCode(llvm::Error): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:typeinfo for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'typeinfo for std::_V2::error_category' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::_M_message(int) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::default_error_condition(int) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::equivalent(int, std::error_condition const&) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::equivalent(std::error_code const&, int) const' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function getFD(llvm::StringRef, std::error_code&, llvm::sys::fs::CreationDisposition, llvm::sys::fs::FileAccess, llvm::sys::fs::OpenFlags): error: undefined reference to 'std::_V2::system_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::write_impl(char const*, unsigned long): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::seek(unsigned long): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/MemoryBuffer.o:MemoryBuffer.cpp:function getMemBufferCopyImpl(llvm::StringRef, llvm::Twine const&): error: undefined reference to 'std::_V2::generic_category()' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/SmallPtrSet.o:SmallPtrSet.cpp:function llvm::SmallPtrSetImplBase::Grow(unsigned int): warning: memset used with constant zero length parameter; this could be due to transposed parameters bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/SourceMgr.o:SourceMgr.cpp:function llvm::SMDiagnostic::print(char const*, llvm::raw_ostream&, bool, bool) const: error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)' bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/_objs/Support/StringRef.o:StringRef.cpp:function unsigned int llvm::ComputeEditDistance(llvm::ArrayRef, llvm::ArrayRef, bool, unsigned int): error: undefined reference to '__cxa_throw_bad_array_new_length' collect2: error: ld returned 1 exit status ERROR: /tensorflow2.5.3/tensorflow/python/BUILD:5209:24 Linking of rule '//tensorflow/compiler/mlir/lite:convertergen' failed (Exit 1): gcc failed: error executing command (cd /cache/1c211944fffc93afc232f5e22d2690de/execroot/org_tensorflow && exec env  LD_LIBRARY_PATH=/usr/path PATH= /usr/path PWD=/proc/self/cwd /opt/rh/devtoolset9/root/usr/bin/gcc out/k8optexec50AE0418/bin/tensorflow/compiler/mlir/lite/convertergen2.params) Execution platform: //:platform I also meet the question, do you have resolved it?"
1024,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(gradient tape with rank 0 tensors causes integer division or modulo by zero)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code? Yes  OS Platform and Distribution: Linux Ubuntu 20.04  TensorFlow installed from: binary (via aptget)  TensorFlow version: 2.3.1  Python version: 3.8.10  GPU model: GeForce GTX 1050 Ti Mobile **Describe the current behavior** The simple repro below throws the following exception:  **Describe the expected behavior** It should either compute a gradient (or throw a meaningful exception if that is not possible). **Contributing**  Do you want to contribute a PR? No **Standalone code to reproduce the issue**  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mikegashler,gradient tape with rank 0 tensors causes integer division or modulo by zero,"**System information**  Have I written custom code? Yes  OS Platform and Distribution: Linux Ubuntu 20.04  TensorFlow installed from: binary (via aptget)  TensorFlow version: 2.3.1  Python version: 3.8.10  GPU model: GeForce GTX 1050 Ti Mobile **Describe the current behavior** The simple repro below throws the following exception:  **Describe the expected behavior** It should either compute a gradient (or throw a meaningful exception if that is not possible). **Contributing**  Do you want to contribute a PR? No **Standalone code to reproduce the issue**  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-03-04T12:01:34Z,stat:awaiting response type:bug comp:ops TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54984,Hi  ! Could you check this resolved gist? Thanks!,"Yes, this works around the problem in my simplified repro. And I see that the relevant difference is that you removed ""[0,0]"" from the code. I found that changing it to ""[0]"" also seems to work around the problem. Unfortunately, I'm having trouble fixing my larger project because I don't understand what specific condition triggers this problem. Is it inappropriate to index a tensor all the way down to a scalar inside of gradient tape? I tried auditing each variable in model.trainable_variables to make sure there are no scalars, but the problem still persists. Is it possible to articulate what I did wrong?","Hi  ! I have used [0,0] to instantiate the MyModel class in this line. `model = MyModel([0,0])` then model(x) instead of `theta = model(x)[0,0]` Attaching relevant thread for reference. 1, 2. Thank you!","Yes, I can follow the recommended structure better. Sorry. I have fixed my code to do that. But this is not relevant to the bug. This example does not throw any exceptions:  But this example does throw an exception:  The only difference is the first (working) example uses ""model(x)[0]"" and the second (broken) example uses ""model(x)[0,0]"". My question is, why does the second (broken) example throw an exception that says ""ZeroDivisionError: integer division or modulo by zero""? I did not do an integer division or modulo. I only indexed an element in a tensor.",Hi ! Could you please look at this issue ? Attaching Gist for reference.,",  Loss calculation was concatenating a bunch of scalars together, by changing them from scalars to rank 1 tensors with keep_dims, the error no longer occurs.  Since `tf.concat()` doesnâ€™t have keepdims argument, replace it with `tf.math.reduce_mean().` Thanks!","Yes, that fixes it. Thanks so much! (I still don't understand why concatenating scalars inside gradient tape is bad behavior. I hope you'll improve the error message to reflect the problem, but I suppose that's up to you.)",Are you satisfied with the resolution of your issue? Yes No
1864,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update Eigen to commit:0ae94456a0e6dd5e20ca65ba2f405964f6931faf)ï¼Œ å†…å®¹æ˜¯ (Update Eigen to commit:0ae94456a0e6dd5e20ca65ba2f405964f6931faf CHANGELOG ========= 0ae94456a  Remove duplicate IsRowMajor declaration. 0e6f4e43f  Fix a few confusing comments in psincos_float. f1b9692d6  Removed EIGEN_UNUSED decorations from many functions that are in fact used 27d8f29be  Update vectorization_logic tests for all platforms. c9ff739af  Fix JacobiSVD_LAPACKE bindings d0b1aef6f  Speed lscg by using .noalias 55c7400db  Fix enum conversion warnings in BooleanRedux. 711803c42  Skip denormal test if `Cond` is false. d819a33bf  Remove poor nonconvergence checks in NonLinearOptimization. 9c07e201f  Modified sqrt/rsqrt for denormal handling. 1c2690ed2  Adjust tolerance of matrix_power test for MSVC. b48922cb5  Fix SVD for MSVC+CUDA. bf6726a0c  Fix any/all reduction in the case of rowmajor layout f03df0df5  Fix SVD for MSVC. 19c39bea2  Fix mixingtypes for g++11. 2ed4bee78  Fix frexp packetmath tests for MSVC. d58e62913  Disable deprecated warnings for SVD tests on MSVC. 3d7e2d0e3  Fix packetmath compilation error. 897071977  Fix gcc5 packetmath_12 bug. f0b81fefb  Disable deprecated warnings in SVD tests. 8b875dbef  Changes to fast SQRT/RSQRT f9b7564fa  E2K: initial support of LCC MCST compiler for the Elbrus 2000 CPU architecture ae86a146b  Modify test expression to avoid numerical differences ( CC(Problem with documentation regarding SparseTensors and TFRecord.)). cd80e04ab  Add assert for edge case if Thin U Requested at runtime 35727928a  Fix test macro conflicts with STL headers in C++20 2dd879d4b  [SYCL] Fix CMake for SYCL support 550af3938  Fix for crash bug in SPQRSupport: I)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Update Eigen to commit:0ae94456a0e6dd5e20ca65ba2f405964f6931faf,Update Eigen to commit:0ae94456a0e6dd5e20ca65ba2f405964f6931faf CHANGELOG ========= 0ae94456a  Remove duplicate IsRowMajor declaration. 0e6f4e43f  Fix a few confusing comments in psincos_float. f1b9692d6  Removed EIGEN_UNUSED decorations from many functions that are in fact used 27d8f29be  Update vectorization_logic tests for all platforms. c9ff739af  Fix JacobiSVD_LAPACKE bindings d0b1aef6f  Speed lscg by using .noalias 55c7400db  Fix enum conversion warnings in BooleanRedux. 711803c42  Skip denormal test if `Cond` is false. d819a33bf  Remove poor nonconvergence checks in NonLinearOptimization. 9c07e201f  Modified sqrt/rsqrt for denormal handling. 1c2690ed2  Adjust tolerance of matrix_power test for MSVC. b48922cb5  Fix SVD for MSVC+CUDA. bf6726a0c  Fix any/all reduction in the case of rowmajor layout f03df0df5  Fix SVD for MSVC. 19c39bea2  Fix mixingtypes for g++11. 2ed4bee78  Fix frexp packetmath tests for MSVC. d58e62913  Disable deprecated warnings for SVD tests on MSVC. 3d7e2d0e3  Fix packetmath compilation error. 897071977  Fix gcc5 packetmath_12 bug. f0b81fefb  Disable deprecated warnings in SVD tests. 8b875dbef  Changes to fast SQRT/RSQRT f9b7564fa  E2K: initial support of LCC MCST compiler for the Elbrus 2000 CPU architecture ae86a146b  Modify test expression to avoid numerical differences ( CC(Problem with documentation regarding SparseTensors and TFRecord.)). cd80e04ab  Add assert for edge case if Thin U Requested at runtime 35727928a  Fix test macro conflicts with STL headers in C++20 2dd879d4b  [SYCL] Fix CMake for SYCL support 550af3938  Fix for crash bug in SPQRSupport: I,2022-03-04T05:24:58Z,size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54971
1900,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.distribute.experimental.CentralStorageStrategy does not work with tf.keras.layers.Discretization)ï¼Œ å†…å®¹æ˜¯ ( System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.2    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: N/A    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: v2.8.0rc132g3f878cff5b6 2.8.0    **Python version**: 3.8.8    **Bazel version (if compiling from source)**: N/A    **GCC/Compiler version (if compiling from source)**: N/A    **CUDA/cuDNN version**: CUDA 11.2/cuDNN 8.2.1    **GPU model and memory**: two A100 GPUs, each with 40GB GPU memory    **Exact command to reproduce**:  Describe the problem I wrote a subclassed Keras model and use tf.keras.layers.Discretization to preprocessing two numerical features. In the preprocessing layer, the code also uses Normalization/StringLookup/IntegerLookup to preprocess numerical and categorical features. The training data is in TFRecord format and read using TFRecordDataset API. The model training works fine with default strategy, tf.distribute.OneDeviceStrategy(device=""/gpu:0""), and tf.distribute.MirroredStrategy(). However, the following error occurred if the distributed strategy is switched to CentralStorageStrategy.  This vocabulary size of the uage_dis_embedding is 10, and the index is computed by the Discretization. I do not quite understand why Discretization output 1 under CentralStorageStrategy, and since the training code works fine with other distributed strat)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,cockroachzl,tf.distribute.experimental.CentralStorageStrategy does not work with tf.keras.layers.Discretization," System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**: Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.2    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: N/A    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: v2.8.0rc132g3f878cff5b6 2.8.0    **Python version**: 3.8.8    **Bazel version (if compiling from source)**: N/A    **GCC/Compiler version (if compiling from source)**: N/A    **CUDA/cuDNN version**: CUDA 11.2/cuDNN 8.2.1    **GPU model and memory**: two A100 GPUs, each with 40GB GPU memory    **Exact command to reproduce**:  Describe the problem I wrote a subclassed Keras model and use tf.keras.layers.Discretization to preprocessing two numerical features. In the preprocessing layer, the code also uses Normalization/StringLookup/IntegerLookup to preprocess numerical and categorical features. The training data is in TFRecord format and read using TFRecordDataset API. The model training works fine with default strategy, tf.distribute.OneDeviceStrategy(device=""/gpu:0""), and tf.distribute.MirroredStrategy(). However, the following error occurred if the distributed strategy is switched to CentralStorageStrategy.  This vocabulary size of the uage_dis_embedding is 10, and the index is computed by the Discretization. I do not quite understand why Discretization output 1 under CentralStorageStrategy, and since the training code works fine with other distributed strat",2022-03-04T04:52:58Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:dist-strat TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54969,", Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. Could you please try with the latest tensorflow v2.17 which contains the Keras3.0 and supports the disitribution strategy for the Keras. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Expand test coverage of `ConvertPyObjectToAttributeType` which can be used to speed up `apply_op_helper`)ï¼Œ å†…å®¹æ˜¯ (Expand test coverage of `ConvertPyObjectToAttributeType` which can be used to speed up `apply_op_helper`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Expand test coverage of `ConvertPyObjectToAttributeType` which can be used to speed up `apply_op_helper`,Expand test coverage of `ConvertPyObjectToAttributeType` which can be used to speed up `apply_op_helper`,2022-03-03T22:10:40Z,size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54952
509,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Removes extra call to native.genrule() because underlying bugs requiring the call are now resolved.  This cleans up the code and reduces Forge impact a tiny bit.)ï¼Œ å†…å®¹æ˜¯ (Removes extra call to native.genrule() because underlying bugs requiring the call are now resolved.  This cleans up the code and reduces Forge impact a tiny bit.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],Removes extra call to native.genrule() because underlying bugs requiring the call are now resolved.  This cleans up the code and reduces Forge impact a tiny bit.,Removes extra call to native.genrule() because underlying bugs requiring the call are now resolved.  This cleans up the code and reduces Forge impact a tiny bit.,2022-03-03T21:36:14Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54949
685,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(* Define a new ""CompositeTensorGradientProtocol"", which can be used by CompositeTensors to indicate how gradients should be computed.)ï¼Œ å†…å®¹æ˜¯ (* Define a new ""CompositeTensorGradientProtocol"", which can be used by CompositeTensors to indicate how gradients should be computed. * Extend tf.GradientTape.gradient and tf.gradients to support CompositeTensor sources (xs) and targets (ys) that implement the CompositeTensorGradientProtocol. * Update RaggedTensor to implement the CompositeTensorGradientProtocol.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],"* Define a new ""CompositeTensorGradientProtocol"", which can be used by CompositeTensors to indicate how gradients should be computed.","* Define a new ""CompositeTensorGradientProtocol"", which can be used by CompositeTensors to indicate how gradients should be computed. * Extend tf.GradientTape.gradient and tf.gradients to support CompositeTensor sources (xs) and targets (ys) that implement the CompositeTensorGradientProtocol. * Update RaggedTensor to implement the CompositeTensorGradientProtocol.",2022-03-03T20:42:22Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54942
742,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([tf][tfg] Add a dialect interface to the TF op registry)ï¼Œ å†…å®¹æ˜¯ ([tf][tfg] Add a dialect interface to the TF op registry Adds a dialect interface to the TFG dialect for querying the TF op registry. Passes should be querying the op registry through this interface instead of directly querying the global registry. The default implementation of the registry interface returns conservative values for op properties. A concrete implementation using an op registry must be injected by the user. This is to avoid having the TFG dialect depend on the TF framework library.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],[tf][tfg] Add a dialect interface to the TF op registry,[tf][tfg] Add a dialect interface to the TF op registry Adds a dialect interface to the TFG dialect for querying the TF op registry. Passes should be querying the op registry through this interface instead of directly querying the global registry. The default implementation of the registry interface returns conservative values for op properties. A concrete implementation using an op registry must be injected by the user. This is to avoid having the TFG dialect depend on the TF framework library.,2022-03-03T19:34:43Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54937
1833,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(problem with the eager execution)ï¼Œ å†…å®¹æ˜¯ (I retrain the resnet152v2 from keras to calassify my own set of image. I did so using a data generator that looks like this:  for training the model the code looks like this:  till there there was no problem, I've a couple of thousand images to validate and produce the output. The issue is that I've not been able to get the same prediction with and without the batch generator.  when I do model.predict(img, verbose=0, use_multiprocessing=True, workers=4) where img is a numpy array that is the same as generated by the batch I get this: >  > in user code: >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1801, in predict_function  * >         return step_function(self, iterator) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1790, in step_function  ** >         outputs = model.distribute_strategy.run(run_step, args=(data,)) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1783, in run_step  ** >         outputs = model.predict_step(data) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1751, in predict_step >         return self(x, training=False) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\utils\traceback_utils.py"", line 67, in error_handler >         raise e.with_traceback(filtered_tb) from None >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\input_spec.py"", line 264, in assert_input_compatibility >         raise ValueError(f'Input {input_index} of )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,hapicatto,problem with the eager execution,"I retrain the resnet152v2 from keras to calassify my own set of image. I did so using a data generator that looks like this:  for training the model the code looks like this:  till there there was no problem, I've a couple of thousand images to validate and produce the output. The issue is that I've not been able to get the same prediction with and without the batch generator.  when I do model.predict(img, verbose=0, use_multiprocessing=True, workers=4) where img is a numpy array that is the same as generated by the batch I get this: >  > in user code: >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1801, in predict_function  * >         return step_function(self, iterator) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1790, in step_function  ** >         outputs = model.distribute_strategy.run(run_step, args=(data,)) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1783, in run_step  ** >         outputs = model.predict_step(data) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\training.py"", line 1751, in predict_step >         return self(x, training=False) >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\utils\traceback_utils.py"", line 67, in error_handler >         raise e.with_traceback(filtered_tb) from None >     File ""C:\Users\dsyr\anaconda3\envs\intel\lib\sitepackages\keras\engine\input_spec.py"", line 264, in assert_input_compatibility >         raise ValueError(f'Input {input_index} of ",2022-03-03T10:40:47Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54916," , Can you please take a look at this SO link 1 and 2 with the similar error.It helps.Thanks!","Hello, thanks for your response. I've looked at those before and I'm not declaring the batch size in the network architecture.  In fact I can feed the network without the data generator but the image has to be transposed. if you notice, the dimensions are flipped in the response.   I can provide access to a colab notebook so you can check code.",Link to the colab code: https://colab.research.google.com/drive/1GdzxZn9LjE4JdzW1n0qGmubsTFpDuTTo?usp=sharing Link to the data and model file: https://drive.google.com/drive/folders/1ZInGRY0FB25LaFcE9vCG1vgqqaGBS2nW?usp=sharing, could you check what I mentioned? the entrances to SO that you referenced were not useful. I don't think the bug label was removed correctly thanks," , Sorry for the delay response. Please post this issue on kerasteam/keras repo. To know more refer to: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
962,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFT convert error: ValueError: Input 0 of node StatefulPartitionedCall/my_model/conv_root/AssignVariableOp was passed float from Func/StatefulPartitionedCall/input/_1:0 incompatible with expected resource.)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS is Ubuntu 18.04  CUDA is 11.5  Code is being run under `nvcr.io/nvidia/tensorflow:21.12tf2py3` (`TensorFlow` version in it is `2.6.2`) **Describe the current behavior** I'm trying to convert a `TensorFlow` saved model to `TFTRT` using tf.experimental.tensorrt.Converter I'm getting the following error:  **Standalone code to reproduce the issue** Here's a fully reproducible code  I've tried this with a couple of other tensorflow versions, and have also tried using `trt_convert.TrtGraphConverterV2` from `tensorflow.python.compiler.tensorrt`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nikshar-symbio,TFT convert error: ValueError: Input 0 of node StatefulPartitionedCall/my_model/conv_root/AssignVariableOp was passed float from Func/StatefulPartitionedCall/input/_1:0 incompatible with expected resource.,"**System information**  OS is Ubuntu 18.04  CUDA is 11.5  Code is being run under `nvcr.io/nvidia/tensorflow:21.12tf2py3` (`TensorFlow` version in it is `2.6.2`) **Describe the current behavior** I'm trying to convert a `TensorFlow` saved model to `TFTRT` using tf.experimental.tensorrt.Converter I'm getting the following error:  **Standalone code to reproduce the issue** Here's a fully reproducible code  I've tried this with a couple of other tensorflow versions, and have also tried using `trt_convert.TrtGraphConverterV2` from `tensorflow.python.compiler.tensorrt`",2022-03-03T08:29:05Z,stat:awaiting response stat:awaiting tensorflower type:bug stale comp:gpu:tensorrt 2.6.0,closed,0,6,https://github.com/tensorflow/tensorflow/issues/54911,"symbio, > Sorry for encountering this issue in your side. Both TFRT and TFLite converter do not support mutable resource variable use cases yet. We are working on supporting the missing features in the MLIR converter.   CC(TF2.3 converter.convert ValueError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.). Workaround, convert your `""TF model to ONNX to TRT""` instead of `""TF to TRT""`. Hope this method can help you. Thanks!","Thanks for that suggestion. I tried it, and it works, but the model prediction time is significantly slower. The model I'm using is a modified version of ResNet that uses the `StdConv` block show in the code above. Using the standard `model.call` method, it takes `0.02 sec` per prediction, but with the `TRT` model it takes about `3 sec`. I don't know what's causing the issue. As another test, I repeated the same comparison with a standard ResNet50, and I get about `0.004 sec` per prediction in both cases. Interestingly, if I convert this standard ResNet50 model to `TensorFlowTRT` (using the code in my original question), it takes about `0.003 sec`!","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. Both TFRT and TFLite converter do not support mutable resource variable use cases yet. We are working on supporting the missing features in the MLIR converter. For TFLite converter, please use the V2 saved model converter if you have an issue related to resource variables since it has the better support in dealing with immutable resource variables than other graph representations (e.g., keras model, concrete function, and frozen graphdef). The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1840,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to find valid certification path)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04  TensorFlow installed from (source or binary): source  TensorFlow version: 2.5.0  Python version: 3.7.0  Installed using virtualenv? pip? conda?: source build  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version:  GPU model and memory: **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** ~/tensorflow$ ./configure You have bazel 3.7.2 installed. Please specify the location of python. [Default is /home/user/miniconda3/envs/tf2.5/bin/python3]: /home/user/miniconda3/envs/tf2.5/bin/python3]^H Invalid python path: /home/user/miniconda3/envs/tf2.5/bin/python3 cannot be found. Please specify the location of python. [Default is /home/user/miniconda3/envs/tf2.5/bin/python3]: /home/user/miniconda3/envs/tf2.5/bin/python3 Found possible Python library paths:   /home/user/miniconda3/envs/tf2.5/lib/python3.7/sitepackages Please input the desired Python library path to use.  Default is [/home/user/miniconda3/envs/tf2.5/lib/python3.7/sitepackages] /home/user/miniconda3/envs/tf2.5/lib/python3.7/sitepackages Do you wish to build TensorFlow with ROCm support? [y/N]: N No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA su)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,taegeonum,Unable to find valid certification path,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04  TensorFlow installed from (source or binary): source  TensorFlow version: 2.5.0  Python version: 3.7.0  Installed using virtualenv? pip? conda?: source build  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version:  GPU model and memory: **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** ~/tensorflow$ ./configure You have bazel 3.7.2 installed. Please specify the location of python. [Default is /home/user/miniconda3/envs/tf2.5/bin/python3]: /home/user/miniconda3/envs/tf2.5/bin/python3]^H Invalid python path: /home/user/miniconda3/envs/tf2.5/bin/python3 cannot be found. Please specify the location of python. [Default is /home/user/miniconda3/envs/tf2.5/bin/python3]: /home/user/miniconda3/envs/tf2.5/bin/python3 Found possible Python library paths:   /home/user/miniconda3/envs/tf2.5/lib/python3.7/sitepackages Please input the desired Python library path to use.  Default is [/home/user/miniconda3/envs/tf2.5/lib/python3.7/sitepackages] /home/user/miniconda3/envs/tf2.5/lib/python3.7/sitepackages Do you wish to build TensorFlow with ROCm support? [y/N]: N No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA su",2022-03-03T03:01:49Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 2.5,closed,0,15,https://github.com/tensorflow/tensorflow/issues/54899,"Hi  ! Can you try following steps ? Step1: bazel clean expunge ( it will clean previous builds and cache) Step2: Uninstall and Reinstall JDK latest version. Step3: Try again  Bazel build with 3.7.2  or 4.2.2 for 2.5 or Bazel 4.2.2 (with   with branch 2.8). Attaching relevant threads for reference. 1, 2. Thanks!", Thanks for the reply! I will share the result after trying the steps. ," Hmm, I reinstalled my jdk (jdk 11) but got the following error   The error message is different from the previous one, but also has the same exception (unable to find valid certification path).  Perhaps is this because of firewall? but I'm using proxy and `wget https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz` is successful.  The attatched reference explains registering certifcate in java toolchain, but I don't have `sudo` permission so cannot execute the keytool command that requires `sudo` permission. ",Hi  ! Could you please look at this issue?,", Add these lines in Tensorflow WORKSPACE file ", I got the following error ,",  Try  "," I added the code in WORKSPACE, but still got this error: ",", You need to do `HTTPS SSL certificate` setup on your browser. Follow the workaround mention  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)0. Thanks!"," I added the certificate to java keystore as suggested in the link, but still got PKIX error (but the download file is different). `"," I also added github.com cert to java keystore, but still got the same error. Is there any way  to change `https` to `http`?","Or is there a workaround for downloading all dependencies in my local machine, and using the downloaded files during bazel build? ",I resolved this issue by changing my proxy server. Thanks for your help,Are you satisfied with the resolution of your issue? Yes No,"hi, I meet the same problem. I add github.com cer in java cacerts and set proxy. There is still has the problem: ""PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target"". How do you set your proxy ? "
286,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Internal document update)ï¼Œ å†…å®¹æ˜¯ (Internal document update On the TFLite 16x8 quantization operator coverage.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Internal document update,Internal document update On the TFLite 16x8 quantization operator coverage.,2022-03-03T02:50:34Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54898
290,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Internal document update)ï¼Œ å†…å®¹æ˜¯ (Internal document update On the TFLite full int quantization operator coverage.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Internal document update,Internal document update On the TFLite full int quantization operator coverage.,2022-03-03T01:34:24Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54896
448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dynamic bounds storage.)ï¼Œ å†…å®¹æ˜¯ (Dynamic bounds storage. This adds bounds.{cc,h}, which allow to store dynamic bound information in the ""encoding"" attribute of RankedTensorType, via either function call or by attaching an (external) attribute interface to IntegerSetAttr.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Dynamic bounds storage.,"Dynamic bounds storage. This adds bounds.{cc,h}, which allow to store dynamic bound information in the ""encoding"" attribute of RankedTensorType, via either function call or by attaching an (external) attribute interface to IntegerSetAttr.",2022-03-02T22:51:13Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54883
1065,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.ragged.segment_ids_to_row_splits` lack check for `out_type`)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  **Describe the current behavior** `tf.ragged.segment_ids_to_row_splits` should check `out_type` is a valid DType. In the example code, `out_typt` is `1`, so it should raise an error instead of silently pass.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ArrowIntoTheSky,`tf.ragged.segment_ids_to_row_splits` lack check for `out_type`,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  **Describe the current behavior** `tf.ragged.segment_ids_to_row_splits` should check `out_type` is a valid DType. In the example code, `out_typt` is `1`, so it should raise an error instead of silently pass.",2022-03-02T18:05:05Z,stat:awaiting response type:bug comp:ops TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54851," , I was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist of it here.", can you look into this as this should have been solved in https://github.com/tensorflow/tensorflow/pull/54441 however the issue is being reproduced. If not I can add the necessary python validations,"The `out_type` relies on `tf.dtypes.as_dtype` for dtype validation. In case of integer, out_type will inteperete the integer as enum of the dtype (`enum 1 == float32`), so a float32 is taken: https://www.tensorflow.org/api_docs/python/tf/dtypes/as_dtype Overall, passing `1` is working as expected in this case.",Thanks  ! I didn't know that before! I will close this issue.,Are you satisfied with the resolution of your issue? Yes No
999,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.ragged.row_splits_to_segment_ids` lack input validation)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  **Describe the current behavior** `tf.ragged.row_splits_to_segment_ids` should check `splits` starts with `0`, and throw `ValueError` if invalid.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ArrowIntoTheSky,`tf.ragged.row_splits_to_segment_ids` lack input validation,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  **Describe the current behavior** `tf.ragged.row_splits_to_segment_ids` should check `splits` starts with `0`, and throw `ValueError` if invalid.",2022-03-02T18:00:46Z,stat:awaiting response type:bug stale comp:ops TF 2.7,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54849,Thanks  ! Added a PR CC(Fixed Input check in 54849) to address this issue. ,Hi  ! Could you please look at this issue?,Was able to reproduce the issue with `tfnightly2.11.0dev20220901` . Please find the gist here. Thank you! ,"I also observed the following API aliases have the same behavior that accepts the `splits` not started from 0 without throwing any Exception.  `(tf.ragged.row_splits_to_segment_ids)`, `tf.compat.v1.ragged.row_splits_to_segment_ids` This behavior still exists in tensorflow nightly (2.15.0dev20230907), and users should be cautious when using them on both CPU and GPU.    Code to reproduce the issue in tf.compat.v1.ragged.row_splits_to_segment_ids  On my GPU machine, it produces the following output without throwing any Exceptions:  The same behavior can be observed on CPU as well.  ",", I tried to execute the code on tensorflow v2.17 and observed that the code was executed with the output ""Tensor(""RaggedSplitsToSegmentIds/Repeat/boolean_mask/GatherV2:0"", shape=(None,), dtype=int64)"" Kindly find the gist of it here. Thank you!",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
472,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to arrange tensorboard's graphs horizontally in tensorflow 2.x?)ï¼Œ å†…å®¹æ˜¯ (I use the following code, the drawing is arranged vertically, how to change it to horizontal arrangement?  I didn't find it in the homepage of tensorflow. https://www.tensorflow.org/api_docs/python/tf/summary/scalar !1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jaried,How to arrange tensorboard's graphs horizontally in tensorflow 2.x?,"I use the following code, the drawing is arranged vertically, how to change it to horizontal arrangement?  I didn't find it in the homepage of tensorflow. https://www.tensorflow.org/api_docs/python/tf/summary/scalar !1",2022-03-02T08:31:13Z,comp:tensorboard stat:awaiting response type:others,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54828,"I changed it this way:  But the name on the tag shows the same as the tag, how to make the name only show the part after '/'? !1"," , Can you please take a look at this link which delivers the similar information.It helps,Thanks!!",Thanks for your answer.
1853,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom LSTMCell has cell state gradients being zeros)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.3.1  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.6.0  Python version: 3.6.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** Let c_t be the cell state at step t, c_0 the initial cell state of the same shape. I am trying to compute dc_t/dc_0 for every step as an indicative statistic for the extent of vanishing gradients in LSTM. On a text comment dataset, where each text sequence is labeled with a binary index as target, I have constructed an LSTM and computed and plotted the gradients over steps.  For the first 20 steps the gradients are decreasing drastically, demonstrating behaviour of vani)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DagonArises,Custom LSTMCell has cell state gradients being zeros,"Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.3.1  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.6.0  Python version: 3.6.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** Let c_t be the cell state at step t, c_0 the initial cell state of the same shape. I am trying to compute dc_t/dc_0 for every step as an indicative statistic for the extent of vanishing gradients in LSTM. On a text comment dataset, where each text sequence is labeled with a binary index as target, I have constructed an LSTM and computed and plotted the gradients over steps.  For the first 20 steps the gradients are decreasing drastically, demonstrating behaviour of vani",2022-03-02T05:34:53Z,comp:keras type:performance TF 2.8,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54817, ! It is replicating in 2.8 version too. Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thanks!, done it's here,Ok!  ! Moving this issue to closed status as it will be tracked down in Keras repo. Thanks!
249,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(internal visibility change only)ï¼Œ å†…å®¹æ˜¯ (internal visibility change only)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],internal visibility change only,internal visibility change only,2022-03-01T21:31:45Z,size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54782
367,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update RaggedTensorSpec._to_legacy_output_class to return the class instead of the object.)ï¼Œ å†…å®¹æ˜¯ (Update RaggedTensorSpec._to_legacy_output_class to return the class instead of the object.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Update RaggedTensorSpec._to_legacy_output_class to return the class instead of the object.,Update RaggedTensorSpec._to_legacy_output_class to return the class instead of the object.,2022-03-01T18:11:49Z,size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54772
1616,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation Fault (core dumped) Issue)ï¼Œ å†…å®¹æ˜¯ (**System information**  I have written custom code  OS Platform and Distribution: CentOS Linux release 7.7.1908 (AltArch), Power9 PC with 4 GPU nodes, 1 login node and 1 compute node and it has little endian architecture  TensorFlow installed from (source or binary): https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibmai/conda/  TensorFlow version (use command below): 2.1.3 (tensorflowgpu)  Python version:3.6.13  Bazel version (if compiling from source):   python compiler version: GCC 7.3.0  CUDA/cuDNN version:10.2  GPU model and memory: NVIDIA Tesla V100, Total memory: 379140608 kB =379.140608 GB (in one of the node) I use Supercomputer at our organization for training. I am facing the Segmentation Fault (core dumped) Issue, when I attempt to train a CNN model. The model gets trained for a smaller number of Epochs. Eg if epochs=50 the model runs fine and outputs the trained model file (using callbacks in model.fit). But when epochs are increased, to 75 then segmentation fault occurs at 54th epoch for the same dataset. I ran .py file by submitting it as a job through slurm script. Dataset size is 80,000*5600, around 9GB. **Code**  **Following is the logs generated after enabling fault handler() function:** !fault handler **Slurm Script used to submit the job** !/bin/bash SBATCH partition=GPU_three SBATCH nodelist=Node_03 SBATCH output=output python3 train.py)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,peter-jp,Segmentation Fault (core dumped) Issue,"**System information**  I have written custom code  OS Platform and Distribution: CentOS Linux release 7.7.1908 (AltArch), Power9 PC with 4 GPU nodes, 1 login node and 1 compute node and it has little endian architecture  TensorFlow installed from (source or binary): https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibmai/conda/  TensorFlow version (use command below): 2.1.3 (tensorflowgpu)  Python version:3.6.13  Bazel version (if compiling from source):   python compiler version: GCC 7.3.0  CUDA/cuDNN version:10.2  GPU model and memory: NVIDIA Tesla V100, Total memory: 379140608 kB =379.140608 GB (in one of the node) I use Supercomputer at our organization for training. I am facing the Segmentation Fault (core dumped) Issue, when I attempt to train a CNN model. The model gets trained for a smaller number of Epochs. Eg if epochs=50 the model runs fine and outputs the trained model file (using callbacks in model.fit). But when epochs are increased, to 75 then segmentation fault occurs at 54th epoch for the same dataset. I ran .py file by submitting it as a job through slurm script. Dataset size is 80,000*5600, around 9GB. **Code**  **Following is the logs generated after enabling fault handler() function:** !fault handler **Slurm Script used to submit the job** !/bin/bash SBATCH partition=GPU_three SBATCH nodelist=Node_03 SBATCH output=output python3 train.py",2022-03-01T10:04:31Z,stat:awaiting response stale comp:keras type:performance TF 2.1,closed,0,11,https://github.com/tensorflow/tensorflow/issues/54747,"Hi jp! Thanks for reporting this issue. You can reduce reduce the batch size to 30 or 60  and input size to (100,1) to see the difference in TF  2.8 version. You can also use tf.device to force run the entire process in GPU mode.  Please post this issue on kerasteam/keras repo. for further assistance. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thanks!","I can reduce the batch size but cannot reduce the input size (as it must be equal to the number of columns in the dataset).  In IBM conda, the latest version available for Tensorflowgpu is 2.1.3, no other versions like TF 2.8 is available for ppcle64 arch. I will use tf.device and will update the result. FYI: Meanwhile, I used Tensorflow Mirrored strategy within one GPU, but that also resulted in segmentation fault after some more epochs than normal implementation.",jp ! Did it get resolved in GPU mode in 2.8 version?,There is no TF2.8 version available for IBM power 9 PPCLE64 architecture. I use the following channel for installing packages conda config prepend channels \ https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibmai/conda/,Ok  ! Please let us know after reducing batch size to 30 in GPU mode .Hi  ! Could you please look at this issue?,yes sure,Thanks for opening this issue. Development of keras moved to separate repository https://github.com/kerasteam/keras/issues Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!," I tried with 30 as batch size for 400 epochs but it also resulted in segmentation fault at 130th epoch. Following is the result of enabling fault handler Current thread 0x0000200000044cd0 (most recent call first):   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/h5py/_hl/files.py"", line 443 in close   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 120 in save_model_to_hdf5   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/saving/save.py"", line 112 in save_model   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/engine/network.py"", line 1011 in save   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/callbacks.py"", line 1040 in _save_model   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/callbacks.py"", line 992 in on_epoch_end   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/callbacks.py"", line 302 in on_epoch_end   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/engine/training_v2.py"", line 771 in on_epoch   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/contextlib.py"", line 88 in __exit__   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/engine/training_v2.py"", line 397 in fit   File ""/home/peter/.conda/envs/peter_env/lib/python3.6/sitepackages/tensorflow_core/python/keras/engine/training.py"", line 819 in fit   File ""/home/peter/analysis/for_Training_gpu.py"", line 100 in  /var/spool/slurm/d/job03321/slurm_script: line 8:  3921 Segmentation fault      (core dumped) python3 ~/analysis/for_Training_gpu.py","Hi, Could you please refer this comment and post the issue in Keras repo for faster resolution. Thanks! > Thanks for opening this issue. Development of keras moved to separate repository https://github.com/kerasteam/keras/issues >  > Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
486,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update RaggedTensor._to_components to add shape information to RaggedTensor.)ï¼Œ å†…å®¹æ˜¯ (Update RaggedTensor._to_components to add shape information to RaggedTensor. When a RaggedTensor is returned from tf.py_function in graph mode, its shape information is unknown and should be recovered from RaggedTensorSpec.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Update RaggedTensor._to_components to add shape information to RaggedTensor.,"Update RaggedTensor._to_components to add shape information to RaggedTensor. When a RaggedTensor is returned from tf.py_function in graph mode, its shape information is unknown and should be recovered from RaggedTensorSpec.",2022-03-01T00:36:26Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54725
503,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix failed test cases for the ragged tensor issue with py_function.)ï¼Œ å†…å®¹æ˜¯ (Fix failed test cases for the ragged tensor issue with py_function. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/54521 from PatriceVignola:addinplacedevicedefaultregistration 2e5af637d89b180031100530cdbd9b3ef34ec176)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Fix failed test cases for the ragged tensor issue with py_function.,Fix failed test cases for the ragged tensor issue with py_function. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/54521 from PatriceVignola:addinplacedevicedefaultregistration 2e5af637d89b180031100530cdbd9b3ef34ec176,2022-02-28T22:24:38Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54717
1285,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Linking process takes forever)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: Ubuntu18.04 (x86_64)  TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow.git  TensorFlow version: latest  Python version: 3.8  Installed using virtualenv? pip? conda?: pip3  Bazel version (if compiling from source): bazel5.0.0linuxarm64  GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu/Linaro 7.5.03ubuntu1~18.04)  CUDA/cuDNN version: 8.2.1.321+cuda10.2  CPU model and memory: CPU:       32 core Intel Xeon E52667 v2 (MTMCP) speed: 3292 MHz (max) / 40GB RAM I'm trying to crosscompile Tensorflow for my NVIDIA Jetson Nano device, from a source code, on an Intel platform server machine. Got by the git clone. It seems like compilation process is finished, or the part of is finished, but the linking process stuck. It gives a tiny load on 1 of 32 cpu threads, but it already took almost a day, but nothing happens. Linking.... Also i checked, IO load, it's minimal. Is it ok, and it takes so much time, or i do something wrong ? >  !image !image !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gmernov,Linking process takes forever,"**System information**  OS Platform and Distribution: Ubuntu18.04 (x86_64)  TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow.git  TensorFlow version: latest  Python version: 3.8  Installed using virtualenv? pip? conda?: pip3  Bazel version (if compiling from source): bazel5.0.0linuxarm64  GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu/Linaro 7.5.03ubuntu1~18.04)  CUDA/cuDNN version: 8.2.1.321+cuda10.2  CPU model and memory: CPU:       32 core Intel Xeon E52667 v2 (MTMCP) speed: 3292 MHz (max) / 40GB RAM I'm trying to crosscompile Tensorflow for my NVIDIA Jetson Nano device, from a source code, on an Intel platform server machine. Got by the git clone. It seems like compilation process is finished, or the part of is finished, but the linking process stuck. It gives a tiny load on 1 of 32 cpu threads, but it already took almost a day, but nothing happens. Linking.... Also i checked, IO load, it's minimal. Is it ok, and it takes so much time, or i do something wrong ? >  !image !image !image",2022-02-28T21:01:21Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.8,closed,0,23,https://github.com/tensorflow/tensorflow/issues/54714," , Can you please try installing TensorFlow v2.8 with GCC 7.3.1 and Bazel 4.2.1 as suggested in tested build configurations here and check if you are facing the same error. Thanks!",Actually i tried multiple version of Bazel. But only 5.0 did the trick. All other versions gave me an error. rootSRV:/tmp/tensorflowmaster/build ../configure  You have bazel 4.2.1 installed. Please upgrade your bazel installation to version 4.2.2 or higher to build TensorFlow!,I also tried to build with 4.2.2 It segfaulted in the start of. Before the actuall compilation process started. Somewhere after bazel started and Download of something was initiated:  !image,"As for now, it doesn't want to build even with 5.0.0 bazel. !image /root/.cache/bazel/_bazel_root/2b5c4e51f9e1005b4128ae187b9d110f/server/jvm.out   has nothing in","Tensorflow 2.7.1, bazel 4.2.2 the same thing: I also tried to install clang6.0 and clang10.0, but clang compilation provides an errors. !image",", Instead of screenshot/image of an error, could you share the complete error log in the comment section. And also provide the `./configure` list. Thanks!","It's not possible to share the complete error log because of: if i compile tensor using GCC, it gives me no error. Simply endless linking... i have to stop the compilation process using CTRL+C(3 times). My ./configure   Also the build command looks like:  `bazel build progress_report_interval=30 show_loading_progress show_progress local_ram_resources=1500 local_cpu_resources=32 j 32 verbose_failures=true config=cuda //tensorflow/tools/pip_package:build_pip_package ` Actually i started with 2.8.0 and bazel 5.0.0 versions. and a build string: `bazel build  config=cuda //tensorflow/tools/pip_package:build_pip_package `",Anything you can advice ?,", As per the Tensorflow Tested build configuration, Tensorflow v2.8 supports CUDA 11.2 and cuDNN 8.1.  Linking to specific CUDA version is taking long time. Upgrade CUDA version and try. Thanks!","I think it's not possible to upgrade CUDA version, because of i'm using latest Jetson JetPack. And it has it's certain version in. But i tried to build TF 2.7.1 using bazel =<3.99.0. The situation changed. Now it doesn't wait to the end to start linking process, but it does it on the run. Compiling,linking,compiling,linking. But it ""hangs"" somewhere in the middle of as well as TF 2.8.0 does. So, still no go...","Can i ask you to help me to compile python3.8 module for aarch64 architechture with GPU CUDA support, if you can help me with ? ",", You can use binary package with GPU support `pip install tensorflowaarch64`","Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown).","> Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown). Yes, but the linking doesn't want to end. It produces no load. I don't think that it's gonna end while it does no load... I'm using 32 cores, and 40GB or RAM","> , You can use binary package with GPU support `pip install tensorflowaarch64` This build has a very dated version. I need something like 2.42.8 version for aarch64 with GPU CUDA support","Trying to build TF 2.4.1 using bazel 3.1.0(built from sources as well). The same crap. hangs on linking... But now, i've enabled debug messages by subcommands. !image That's the system monitor, to see the ""load"" on the system:  !image","Maybe it's not related exactly  for Tensorflow, but for linker. IDK. But i suffer this for multiple versions of tensorflow code.",", Latest release  `pip install tensorflowaarch64==2.8.0`","> , Latest release `pip install tensorflowaarch64==2.8.0` Does this version support CUDA ? I'm trying to build a version from sources because of i need tensorflow GPU support.",", `tensorflowaarch64==2.8.0` it does support GPU. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
313,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Switching over row_partitions and rank to rely on ragged_shape.)ï¼Œ å†…å®¹æ˜¯ (Switching over row_partitions and rank to rely on ragged_shape.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Switching over row_partitions and rank to rely on ragged_shape.,Switching over row_partitions and rank to rely on ragged_shape.,2022-02-28T20:56:20Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54713
455,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add additional methods for constructing DynamicRaggedShape.Spec.)ï¼Œ å†…å®¹æ˜¯ (Add additional methods for constructing DynamicRaggedShape.Spec. In particular, adding the ability to: * create a spec from a spec of a different type. * change the dtype of the spec. * truncate the spec.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Add additional methods for constructing DynamicRaggedShape.Spec.,"Add additional methods for constructing DynamicRaggedShape.Spec. In particular, adding the ability to: * create a spec from a spec of a different type. * change the dtype of the spec. * truncate the spec.",2022-02-28T20:43:22Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54710
493,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( multi label `class_weight` )ï¼Œ å†…å®¹æ˜¯ (hi  i was trying to deal with multioutput classification problem on an imbalanced dataset using class_weight parameter of fit method  ; but i faced this issue; i am looking for an alternative or solution  `class_weight` is only supported for Models with a single output. thanks )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Drlilou, multi label `class_weight` ,hi  i was trying to deal with multioutput classification problem on an imbalanced dataset using class_weight parameter of fit method  ; but i faced this issue; i am looking for an alternative or solution  `class_weight` is only supported for Models with a single output. thanks ,2022-02-28T17:55:32Z,stat:awaiting response type:support stale comp:keras TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/54697,"Hi  ! Could you please update the template too as it will help us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!","tensorflow version : 2.8.0 environnement : colab  model.fit( x_train,[y_train1,y_train2] , batch size=128, class_weight=....) ",you can see this documentation https://www.tensorflow.org/tutorials/structured_data/imbalanced_data .,"Hi  ! Did  you try again after passing weights as dictionaries ? Attaching relevant threads as reference. R 1, 2 .  Please post this issue on kerasteam/keras repo for further assistance. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1907,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TypeError: can't pickle _thread.RLock objects when using KerasClassifier with Keras and RandomizedSearchCV)ï¼Œ å†…å®¹æ˜¯ (I created a simple neural network for binary spam/ham text classification using pretrained BERT transformer. Now I want to apply randomized search for tuning the hyperparameters. For now the only hyperparameter I would like to tune is the dropout regularization probability. I set up a sklearn pipeline with scikeras's KerasClassifier that contains my custom `build_sequential_nn()` method. When merely fitting  the pipeline, everything is fine; however, when I pass the pipeline into a sklearn `RandomizedSearchCV`, the following error message pops up: `TypeError: can't pickle _thread.RLock objects` Full error message will be detailed at the end of the post. A full reproducible code is as follows, including sample data:   Versions:   Note:  A very similar issue was reported in 2020 where the issue was reproducible on TFv2.3 as mentioned in this comment in that thread. However there was no clear solution/workaround provided there, rather this link. Having compared the code snippets provided in that blog post with my code above, I don't see any obvious inconsistencies or mistakes in my solution. Note that the linked blog post is actually only about incorporating a KerasClassifier into a grid/randomized search logic, and not about incorporating an entire sklearn pipeline. This might be important.   Another time the same error message was reported here on github was this one. Yet, in this case the reason is seemingly unrelated.   In this thread on StackOverflow Marcin MoÅ¼ejko mentions that `keras` doesn't support parallelization via pickle which would be performed by default by grid/randomized se)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,leweex95,TypeError: can't pickle _thread.RLock objects when using KerasClassifier with Keras and RandomizedSearchCV,"I created a simple neural network for binary spam/ham text classification using pretrained BERT transformer. Now I want to apply randomized search for tuning the hyperparameters. For now the only hyperparameter I would like to tune is the dropout regularization probability. I set up a sklearn pipeline with scikeras's KerasClassifier that contains my custom `build_sequential_nn()` method. When merely fitting  the pipeline, everything is fine; however, when I pass the pipeline into a sklearn `RandomizedSearchCV`, the following error message pops up: `TypeError: can't pickle _thread.RLock objects` Full error message will be detailed at the end of the post. A full reproducible code is as follows, including sample data:   Versions:   Note:  A very similar issue was reported in 2020 where the issue was reproducible on TFv2.3 as mentioned in this comment in that thread. However there was no clear solution/workaround provided there, rather this link. Having compared the code snippets provided in that blog post with my code above, I don't see any obvious inconsistencies or mistakes in my solution. Note that the linked blog post is actually only about incorporating a KerasClassifier into a grid/randomized search logic, and not about incorporating an entire sklearn pipeline. This might be important.   Another time the same error message was reported here on github was this one. Yet, in this case the reason is seemingly unrelated.   In this thread on StackOverflow Marcin MoÅ¼ejko mentions that `keras` doesn't support parallelization via pickle which would be performed by default by grid/randomized se",2022-02-28T16:44:44Z,stat:awaiting response type:bug comp:keras,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54694,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,"Thanks, I will do so.",Are you satisfied with the resolution of your issue? Yes No
825,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(After finetuning a BERT model max_position_embeddings=128, my saved model is not reflecting the smaller sequence length)ï¼Œ å†…å®¹æ˜¯ (I used to finetune BERT with tf1 with original BERT code, now that I am using tfmodelsofficial, although I change max_position_embedding, my saved model file doesn't change. Previously when I limit max_seq_length and saved model with the associated signature (with the same max_seq_length) my saved model would be smaller. Here in tfmodelsofficial I don't need to explicitly pass a signature, so the only place that l put limitation is in max_position_embedding. The problem is my saved model is bigger than I expected.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,SohaK,"After finetuning a BERT model max_position_embeddings=128, my saved model is not reflecting the smaller sequence length","I used to finetune BERT with tf1 with original BERT code, now that I am using tfmodelsofficial, although I change max_position_embedding, my saved model file doesn't change. Previously when I limit max_seq_length and saved model with the associated signature (with the same max_seq_length) my saved model would be smaller. Here in tfmodelsofficial I don't need to explicitly pass a signature, so the only place that l put limitation is in max_position_embedding. The problem is my saved model is bigger than I expected.",2022-02-28T16:04:37Z,stat:awaiting response type:support stale comp:model TF 2.8,closed,0,14,https://github.com/tensorflow/tensorflow/issues/54687,"Hi  !  Could you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!","TF version: 2.7 I am almost following https://www.tensorflow.org/text/tutorials/fine_tune_bert but with my own BERT model and data bert_config: {'hidden_size': 512,  'hidden_act': 'gelu',  'initializer_range': 0.02,  'vocab_size': 47644,  'hidden_dropout_prob': 0.1,  'num_attention_heads': 8,  'type_vocab_size': 2,  'max_position_embeddings': 128,  'num_hidden_layers': 12,  'intermediate_size': 2048,  'attention_probs_dropout_prob': 0.1,  'max_seq_length': 128} with tf.saved_model.save(reloaded, export_dir=export_dir) I get the variables and pb files the variable file are 753 MB, but I expected to be 251 MB (based on older versions on TF1), the saved_model.pb is 4.65 MB while on TF1 I used to get 764kB. with bert_classifier.save(export_dir, include_optimizer=False), I get the variables with the size I expect, and I get two pb files: saved_model.pb and keras_metadata.pb the saved_model.pb is 3.99 MB. Do these different saved models affect the runtime behavior and speed?",Also bert_classifier = tf.saved_model.load(import_dir) bert_classifier.summary() on both way of saving gives me '_UserObject' object has no attribute 'summary',", When i tried to save the Fine_tune_bert model with same configuration as you mentioned, it has only one .pb file `saved_model.pb`. Size of the model ~3.4MB.  Use keras load model to load your model. ","Thank you   from tensorflow import keras import_dir=""AF_128_NQ_3"" reloaded = keras.models.load_model(import_dir) reloaded.summary() gives me:  TypeError                                 Traceback (most recent call last)  in        1 from tensorflow import keras       2 import_dir=""./AF_128_NQ_3"" > 3 reloaded = keras.models.load_model(import_dir)       4  reloaded = tf.saved_model.load(import_dir)       5 reloaded.summary() ~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/sitepackages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)      65     except Exception as e:   pylint: disable=broadexcept      66       filtered_tb = _process_traceback_frames(e.__traceback__) > 67       raise e.with_traceback(filtered_tb) from None      68     finally:      69       del filtered_tb ~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/sitepackages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)     528     self._self_setattr_tracking = False   pylint: disable=protectedaccess     529     try: > 530       result = method(self, *args, **kwargs)     531     finally:     532       self._self_setattr_tracking = previous_value   pylint: disable=protectedaccess TypeError: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'"," I also need to be able to reload a bert classifier_model checkpoint to retrain further, but when I do: bert_classifier, bert_encoder = bert.bert_models.classifier_model(bert_config, num_labels=2) checkpoint = tf.train.Checkpoint(model=bert_classifier) checkpoint.read(os.path.join(path +'/model00000001.ckpt/variables/', 'variables')).assert_existing_objects_matched() I get this error:  AssertionError                            Traceback (most recent call last)  in        1 checkpoint = tf.train.Checkpoint(model=bert_classifier)       2 checkpoint.read( > 3     os.path.join(efs_path +""AF2022/model00000001.ckpt/variables/"", 'variables')).assert_existing_objects_matched() ~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/sitepackages/tensorflow/python/training/tracking/util.py in assert_existing_objects_matched(self)     845       num_variables_to_show = min(10, num_unused_python_objects)     846       raise AssertionError( > 847           f""Found {num_unused_python_objects} Python objects that were ""     848           ""not bound to checkpointed values, likely due to changes in the ""     849           f""Python program. Showing {num_variables_to_show} of "" AssertionError: Found 201 Python objects that were not bound to checkpointed values, likely due to changes in the Python program. Showing 10 of 201 unmatched objects: [<tf.Variable 'transformer/layer_5/output/kernel:0' shape=(2048, 512) dtype=float32, numpy=...",", Could you share complete code to reproduce the issue. Thanks!","  This is the complete code: !pip install tensorflow==2.8 !pip install U tensorflowtext==2.8 !pip install tfmodelsofficial==2.7 import os import json import numpy as np import tensorflow as tf from official.nlp import bert from official import nlp  Load the required submodules import official.nlp.optimization import official.nlp.bert.run_classifier efs_path =  a path data_path =  efs_path +  a path gs_folder_bert = efs_path +  a path tf.io.gfile.listdir(gs_folder_bert) bert_config_file = os.path.join(gs_folder_bert, ""bert_config.json"") config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read()) bert_config = bert.configs.BertConfig.from_dict(config_dict) bert_classifier, bert_encoder = bert.bert_models.classifier_model(     bert_config, num_labels=2) train_data_output_path=""./AF_NQ_train.tf_record"" eval_data_output_path=""./AF_NQ_eval.tf_record"" max_seq_length = 128 batch_size = 32 eval_batch_size = 32 training_dataset = bert.run_classifier.get_dataset_fn(     train_data_output_path,     max_seq_length,     batch_size,     is_training=True)() evaluation_dataset = bert.run_classifier.get_dataset_fn(     eval_data_output_path,     max_seq_length,     eval_batch_size,     is_training=False)()  Set up epochs and steps epochs = 1 batch_size = 32 eval_batch_size = 32 TRAIN_SIZE =  a number train_data_size = TRAIN_SIZE steps_per_epoch = int(train_data_size / batch_size) num_train_steps = steps_per_epoch * epochs warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)  creates an optimizer with learning rate schedule optimizer = nlp.optimization.create_optimizer(     2e5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps) checkpoint = tf.keras.callbacks.ModelCheckpoint('model{epoch:08d}_test.ckpt', save_freq=steps_per_epoch, save_format=""tf"")  log = tf.keras.callbacks.TensorBoard(log_dir='./logs',update_freq=10) metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)] loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) bert_classifier.compile(     optimizer=optimizer,     loss=loss,     metrics=metrics) bert_classifier.fit(       training_dataset,       validation_data=evaluation_dataset,       batch_size=32,       steps_per_epoch=steps_per_epoch,       epochs=epochs,       callbacks=[checkpoint, log]) export_dir='./AF_128_NQ_3' tf.saved_model.save(bert_classifier, export_dir=export_dir) import_dir=""./AF_128_NQ_3"" bert_classifier = tf.keras.models.load_model(import_dir)  WARNING:absl:Found untraced functions such as dropout_5_layer_call_fn, dropout_5_layer_call_and_return_conditional_losses, logits_layer_call_fn, logits_layer_call_and_return_conditional_losses, self_attention_layer_call_fn while saving (showing 5 of 364). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: ./AF_128_NQ_3/assets INFO:tensorflow:Assets written to: ./AF_128_NQ_3/assets  TypeError                                 Traceback (most recent call last) /tmp/ipykernel_1252/896310302.py in        3        4 import_dir=""./AF_128_NQ_3"" > 5 bert_classifier = tf.keras.models.load_model(import_dir) ~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/sitepackages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)      65     except Exception as e:   pylint: disable=broadexcept      66       filtered_tb = _process_traceback_frames(e.__traceback__) > 67       raise e.with_traceback(filtered_tb) from None      68     finally:      69       del filtered_tb ~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/sitepackages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)     627     self._self_setattr_tracking = False   pylint: disable=protectedaccess     628     try: > 629       result = method(self, *args, **kwargs)     630     finally:     631       self._self_setattr_tracking = previous_value   pylint: disable=protectedaccess TypeError: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'",", Thanks for providing code to reproduce. Could you please provide the required documents like Json and dataset file. Thanks!",As I said I am following https://www.tensorflow.org/text/tutorials/fine_tune_bert. ,", When i tried to save the Fine_tune_bert model with same configuration as you mentioned, it has only one .pb file saved_model.pb. Size of the model ~3.3MB. If you are customising the model and training it again. Make sure you have the same size data set. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build failure with TF 2.7.1 on Linux ppc64le with high memory consumption)ï¼Œ å†…å®¹æ˜¯ (**System information**  Red Hat Enterprise Linux CoreOS 4.6:  Desktop ppc64le   TensorFlow installed from (source or binary): Source   TensorFlow version: 2.7.1  Python version: 3.9  Installed using conda:  Bazel version : 3.7.2  GCC/Compiler version (if compiling from source): 8.2 from Anaconda Compiler Toolchain  CUDA/cuDNN version: 112  GPU model and memory: 511GB RAM !image **Describe the problem**: Our Team has been building Tensorflow source code on ppc64le(511GB RAM total) Openshift CI/CD pipeline We have observing that while Tensorflow builds it tends to consume huge chunk of RAM memory like 110GB and then  the build fails .. on multiple retries like in the number of 1718 and sometimes even more the builds randomly pass In comparison to this same version of Tensorflow on x86(61GB RAM total) environment wherein the RAM is much less as compared to ppc64le the build completes successfully must faster and consistent with max reties of 1 as compared to ppc64le The above mentioned scenario is also been tested for Tensorflow v 2.4.4 on ppc64le and its performance has been much much better as compared to Tensorflow v 2.7.1. We are trying to understand what is it with Tensorflow v 2.7.1 that occupies so much memory on ppc64le and refuses to release memory which causes repeated build failures .. is there something we can do in code to correct this behavior on ppc64le **Provide the exact sequence of commands / steps that you executed before running into the problem** https://github.com/opence/tensorflowfeedstock/blob/main/recipe/meta.yaml **Any other info / logs** Include any logs or source)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ketank-new,Build failure with TF 2.7.1 on Linux ppc64le with high memory consumption,**System information**  Red Hat Enterprise Linux CoreOS 4.6:  Desktop ppc64le   TensorFlow installed from (source or binary): Source   TensorFlow version: 2.7.1  Python version: 3.9  Installed using conda:  Bazel version : 3.7.2  GCC/Compiler version (if compiling from source): 8.2 from Anaconda Compiler Toolchain  CUDA/cuDNN version: 112  GPU model and memory: 511GB RAM !image **Describe the problem**: Our Team has been building Tensorflow source code on ppc64le(511GB RAM total) Openshift CI/CD pipeline We have observing that while Tensorflow builds it tends to consume huge chunk of RAM memory like 110GB and then  the build fails .. on multiple retries like in the number of 1718 and sometimes even more the builds randomly pass In comparison to this same version of Tensorflow on x86(61GB RAM total) environment wherein the RAM is much less as compared to ppc64le the build completes successfully must faster and consistent with max reties of 1 as compared to ppc64le The above mentioned scenario is also been tested for Tensorflow v 2.4.4 on ppc64le and its performance has been much much better as compared to Tensorflow v 2.7.1. We are trying to understand what is it with Tensorflow v 2.7.1 that occupies so much memory on ppc64le and refuses to release memory which causes repeated build failures .. is there something we can do in code to correct this behavior on ppc64le **Provide the exact sequence of commands / steps that you executed before running into the problem** https://github.com/opence/tensorflowfeedstock/blob/main/recipe/meta.yaml **Any other info / logs** Include any logs or source,2022-02-28T12:07:08Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54671,"new,  You might try to clear Bazel cache before running Bazel build command  You can run Bazel with Limited RAM. Take a look https://docs.bazel.build/versions/main/memorysavingmode.html. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
777,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow constant folding to remove add instructions used by compares)ï¼Œ å†…å®¹æ˜¯ (If we have a compare with a constant on the RHS and and add(op, constant) on the left, then move the LHS constant to the right hand side so that constant folding can eliminate the add instruction. We could do the removing of the addition manually by modifying the constants literal but for the sake of  avoiding code duplication I think it's better to let the constant folding pass to remove the subtraction Eliminating the this pattern allows the pattern matching in the whilelooptripcountannotator to hit some extra cases.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Sam-Hornby,Allow constant folding to remove add instructions used by compares,"If we have a compare with a constant on the RHS and and add(op, constant) on the left, then move the LHS constant to the right hand side so that constant folding can eliminate the add instruction. We could do the removing of the addition manually by modifying the constants literal but for the sake of  avoiding code duplication I think it's better to let the constant folding pass to remove the subtraction Eliminating the this pattern allows the pattern matching in the whilelooptripcountannotator to hit some extra cases.",2022-02-26T17:40:16Z,stale comp:xla size:M,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54645, Can you please review this PR ? Thank you!,Hi Hornby Can you please resolve conflicts? Thank you!,Hi Hornby Any update on this PR? Please. Thank you!
281,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Verifying the verification order [do not merge])ï¼Œ å†…å®¹æ˜¯ (Verifying the verification order [do not merge])è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],Verifying the verification order [do not merge],Verifying the verification order [do not merge],2022-02-25T18:04:58Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54603
486,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update RaggedTensor._to_components to add shape information to RaggedTensor.)ï¼Œ å†…å®¹æ˜¯ (Update RaggedTensor._to_components to add shape information to RaggedTensor. When a RaggedTensor is returned from tf.py_function in graph mode, its shape information is unknown and should be recovered from RaggedTensorSpec.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Update RaggedTensor._to_components to add shape information to RaggedTensor.,"Update RaggedTensor._to_components to add shape information to RaggedTensor. When a RaggedTensor is returned from tf.py_function in graph mode, its shape information is unknown and should be recovered from RaggedTensorSpec.",2022-02-25T17:46:10Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54601
483,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TF:TRT] Elide log messages when qdq mode is off)ï¼Œ å†…å®¹æ˜¯ (This change removes log messages notifying users that QDQ mode is not enabled. Since this is not surprising for the majority of users, and log messages appear to notify users that QDQ mode is enabled, the ""not enabled"" log messages are superfluous.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,christopherbate,[TF:TRT] Elide log messages when qdq mode is off,"This change removes log messages notifying users that QDQ mode is not enabled. Since this is not surprising for the majority of users, and log messages appear to notify users that QDQ mode is enabled, the ""not enabled"" log messages are superfluous.",2022-02-25T17:29:22Z,size:XS comp:gpu:tensorrt,closed,0,1,https://github.com/tensorflow/tensorflow/issues/54599, for review when available. Thanks!
402,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add StatusOr<bool> IsReady() to the PjRtBuffer API.)ï¼Œ å†…å®¹æ˜¯ (Add StatusOr IsReady() to the PjRtBuffer API. An error is returned:  if the buffer was deleted.  if it's not supported by the backend (discouraged, but may happen).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],Add StatusOr<bool> IsReady() to the PjRtBuffer API.,"Add StatusOr IsReady() to the PjRtBuffer API. An error is returned:  if the buffer was deleted.  if it's not supported by the backend (discouraged, but may happen).",2022-02-25T16:11:47Z,size:M,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54592
1860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(huggingface pre_trained model loading takes full gpu memory)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  N/A  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 x64  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below):  2.8.0  Python version: 3.9.7  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): visual studio 2019  CUDA/cuDNN version: 11.6/8.3.2  GPU model and memory: RTX3090 24GB You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** load from pretrained transformer takes full gpu memory, so oom happens after training started **Describe the expected behavior** appropriate memory allocation **Contributing**  Do you want to contribute a PR? (yes/no): yes  Briefly describe your candidate solution(if contributing): add options for loading pretrained model **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, p)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,alanpurple,huggingface pre_trained model loading takes full gpu memory,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  N/A  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 x64  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below):  2.8.0  Python version: 3.9.7  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): visual studio 2019  CUDA/cuDNN version: 11.6/8.3.2  GPU model and memory: RTX3090 24GB You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** load from pretrained transformer takes full gpu memory, so oom happens after training started **Describe the expected behavior** appropriate memory allocation **Contributing**  Do you want to contribute a PR? (yes/no): yes  Briefly describe your candidate solution(if contributing): add options for loading pretrained model **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, p",2022-02-25T11:47:03Z,stat:awaiting response type:bug comp:keras comp:gpu TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/54582,Hi  ! Can you try again after reducing max_length and input size /add more pooling layers in build_model  aside constraining memory growth in Gpu. You can also use tf.data to set a pipe line to load the data in batches.  Attaching relevant thread for reference.Thanks!,  same with tf.data.Dataset  of course no oom with max_len 256 and batch size 8. so is this not a bug? I don't understand since 14*16*1024*1024 with float32 is just under 1GB and RTX3090 has 24GB of memory,Hi  ! Its not only about one Tensor . It is iterating for all tensors with 1GB memory size while in model.fit() operation.  Thank you for confirmation though. Please move this issue to closed status if it helped. ,"  so almost all 24GB of GPU memory being taken right after load pretrained BERTLARGE is normal? In that case, this is should be closed",Loading the model should not cause OOM issue. Can you switch to Cuda 11.2 and CuDNN 8.1 and let us know the difference ? Thanks!,"  I think I did have some misconcept, as soon as the program load cuda context, it takes almost  all of gpu's memory, it's normal I guess.... sorry for misunderstanding( of cudnn )",Are you satisfied with the resolution of your issue? Yes No
387,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(while do evaluation in the runhook, the eval_metrics calculate each batch and the average the result or calculate after all evaluation dataset?)ï¼Œ å†…å®¹æ˜¯ ( like auc in eval_metrics. Anybody knows this?Please help)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,satyrswang,"while do evaluation in the runhook, the eval_metrics calculate each batch and the average the result or calculate after all evaluation dataset?", like auc in eval_metrics. Anybody knows this?Please help,2022-02-25T08:00:27Z,stat:awaiting response type:support stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54573," , Can you please take a look at this link 1, 2 and 3 which delivers the information for eval_metrics.It helps.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
359,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implement asynchronous checkpoint for the eager mode, i.e., context.running_eagerly().)ï¼Œ å†…å®¹æ˜¯ (Implement asynchronous checkpoint for the eager mode, i.e., context.running_eagerly().)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],"Implement asynchronous checkpoint for the eager mode, i.e., context.running_eagerly().","Implement asynchronous checkpoint for the eager mode, i.e., context.running_eagerly().",2022-02-25T06:23:40Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54571
1879,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite Converter: how to skip Dequantize node in FP16 models for GPU Delegate?)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto dunfell   Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: aarch64  TensorFlow installed from (source or binary):  TensorFlow version (use command below): 2.6.1  Python version: 3.9.9  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: Mali G72 **Describe the current behavior** I converted mobilenet_v2_1.0_224 https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz to fp16 tflite model using the steps mentioned here https://www.tensorflow.org/lite/performance/post_training_quantizationfloat16_quantization the converted mobilenet_v2_fp16.tflite model has multiple Dequantize steps inserted within which seem to convert float16 to float32, even though the model is supposed to be quantized to float16. Below is a snippet of the model visualization from netron. The inputs to the converted model are float32 and its outputs are also float32.  !test All the dequantize nodes have float16 input and float32 output. Node 0 dequantize node properties are visualized below: !image all dequantize nodes have similar properties where the inputs to the layer c)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,suyash-narain,TFLite Converter: how to skip Dequantize node in FP16 models for GPU Delegate?,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto dunfell   Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: aarch64  TensorFlow installed from (source or binary):  TensorFlow version (use command below): 2.6.1  Python version: 3.9.9  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: Mali G72 **Describe the current behavior** I converted mobilenet_v2_1.0_224 https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz to fp16 tflite model using the steps mentioned here https://www.tensorflow.org/lite/performance/post_training_quantizationfloat16_quantization the converted mobilenet_v2_fp16.tflite model has multiple Dequantize steps inserted within which seem to convert float16 to float32, even though the model is supposed to be quantized to float16. Below is a snippet of the model visualization from netron. The inputs to the converted model are float32 and its outputs are also float32.  !test All the dequantize nodes have float16 input and float32 output. Node 0 dequantize node properties are visualized below: !image all dequantize nodes have similar properties where the inputs to the layer c",2022-02-25T01:01:05Z,stat:awaiting tensorflower type:bug TFLiteConverter 2.6.0,open,0,8,https://github.com/tensorflow/tensorflow/issues/54559,"same question. I want to convert this quantized fp16 model using snpetflitetodlc and keep a bitwidth=16 structure. Thus, I would expect all the ops in .tflite to be directly executable on 16precision","Same question, any update?","Same questoin, 2022 years, tflite still can not fully do fp16 inference on armv8.2 above CPU? Even on CPU doesn't support fp16 natively, it should also using simulation to support fp16. Please add cpu fp16!"," please take a look, thanks!",any update ?,"> same question. I want to convert this quantized fp16 model using snpetflitetodlc and keep a bitwidth=16 structure. Thus, I would expect all the ops in .tflite to be directly executable on 16precision Hi  , may I know how do you use snpetflitetodlc to generate a fp16.dlc file? Could it be run on HTP and output correct result? Thanks.",Having the exact same issue?   or  any updates would be really appreciated.,"Same question, is there any update?"
467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(This CL reduces the high average looping count (~4,435,487) in the RandomDims() utility function which slows down dependent tests; i.e. RFFT)ï¼Œ å†…å®¹æ˜¯ (This CL reduces the high average looping count (~4,435,487) in the RandomDims() utility function which slows down dependent tests; i.e. RFFT)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],"This CL reduces the high average looping count (~4,435,487) in the RandomDims() utility function which slows down dependent tests; i.e. RFFT","This CL reduces the high average looping count (~4,435,487) in the RandomDims() utility function which slows down dependent tests; i.e. RFFT",2022-02-25T00:55:34Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54557
497,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Create a custom Spec for DynamicRaggedShape. This involves making functions in DynamicRaggedShape work for the spec too, e.g. functions such as _dimension.)ï¼Œ å†…å®¹æ˜¯ (Create a custom Spec for DynamicRaggedShape. This involves making functions in DynamicRaggedShape work for the spec too, e.g. functions such as _dimension.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,copybara-service[bot],"Create a custom Spec for DynamicRaggedShape. This involves making functions in DynamicRaggedShape work for the spec too, e.g. functions such as _dimension.","Create a custom Spec for DynamicRaggedShape. This involves making functions in DynamicRaggedShape work for the spec too, e.g. functions such as _dimension.",2022-02-24T21:34:39Z,size:L,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54541
1511,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(InaccessibleTensorError when running model as a .expand_dims call in the main model code)ï¼Œ å†…å®¹æ˜¯ (**System information**  I have written a custom model in TensorFlow as a modification of the transformer model. M1 Pro 32 GB Macbook Pro 12.1 Monterey(also ran on colab T4 GPU 16 GB)  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): 2.7.1 also tried 2.8  Python version: 3.7.1.2  CUDA/cuDNN version: N/A  GPU model and memory: 32 GB M1 PRO,  Tesla T4 16 GB **Describe the current behavior** Current when I run my model in eager execution model it is fine, but when I run it with .function wrapper I get an inaccessible tensor error when building the AutoGraph. This is coming in the model call method, when I .expand_dims on the call of another layer class. The error is:  I expect the model to compile, I have tried to move the tf.expand_dims to other layers/subclasses and still get the same bug no matter where. I guess this could be rooted from somewhere else as well but I cannot understand the error. **Standalone code to reproduce the issue** Reproducible test case can be found here:  https://colab.research.google.com/gist/ae20cg/fe1577f53f6e68db1e3429643f1e957a/tft2tf2graph.ipynb)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ae20cg,InaccessibleTensorError when running model as a .expand_dims call in the main model code,"**System information**  I have written a custom model in TensorFlow as a modification of the transformer model. M1 Pro 32 GB Macbook Pro 12.1 Monterey(also ran on colab T4 GPU 16 GB)  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): 2.7.1 also tried 2.8  Python version: 3.7.1.2  CUDA/cuDNN version: N/A  GPU model and memory: 32 GB M1 PRO,  Tesla T4 16 GB **Describe the current behavior** Current when I run my model in eager execution model it is fine, but when I run it with .function wrapper I get an inaccessible tensor error when building the AutoGraph. This is coming in the model call method, when I .expand_dims on the call of another layer class. The error is:  I expect the model to compile, I have tried to move the tf.expand_dims to other layers/subclasses and still get the same bug no matter where. I guess this could be rooted from somewhere else as well but I cannot understand the error. **Standalone code to reproduce the issue** Reproducible test case can be found here:  https://colab.research.google.com/gist/ae20cg/fe1577f53f6e68db1e3429643f1e957a/tft2tf2graph.ipynb",2022-02-24T21:25:30Z,stat:awaiting response type:bug comp:tf.function TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54540,"Hi  ! I was able to replicate in 2.7 and 2.8 , But Can you try again after changing empty lists to Tensor arrays ? Attaching relevant threads for reference. R 1 2. Thank you!","  Since the argument that is causing the code to fail is 'optional', I removed it and the code runs is able to run train. And the tensor that is causing an error does not come from a list. I have updated the code to get rid of the empty lists, still the same error.","I have fixed this issue.  For those this may apply to. This tensor was being called by another layer in the `call` method. This tensor was then being passed to another layer, however rather than calling it through the `call` method I was passing it in in initialization. I changed said method to accept the tensor in the `call` argument and it fixed the code.  Thanks!",Are you satisfied with the resolution of your issue? Yes No,"Hi  ,   I'm making visual transformer code with TensorFlow 2.x and have the same error like you, so I checked test case you uploaded but I can't find wrong part in the code. Can you tell me where's the problem in the test case code?  I read your comment above about how you solved but I think I didn't understand correctly.  Thank you!"
1326,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tape.gradient returns None when assign is used in a model for computation of a function)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Springdale  TensorFlow installed from (source or binary): pip  TensorFlow version (use command below): 2.8.9  Python version: 3.7.10+ **Describe the current behavior** When a function is calculated based on a weight of a model the gradient can't trace the connection and returns None. More precisely I am trying to define a log_posterior from a model that has multiple weights (a shallow neural network, if you will). But Hamiltonian Monte Carlo does not work because my defined log_posterior does not have a gradient. **Describe the expected behavior** TF should return the gradient of the log_posterior function defined. **Standalone code to reproduce the issue**  In the example below tape.gradient should return the value of w as the gradient. Here DummyModel's call supposed to replicate a log_postrior function, where I reassign its parameter ""w"" with ""x"".  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,naji-s,tape.gradient returns None when assign is used in a model for computation of a function,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Springdale  TensorFlow installed from (source or binary): pip  TensorFlow version (use command below): 2.8.9  Python version: 3.7.10+ **Describe the current behavior** When a function is calculated based on a weight of a model the gradient can't trace the connection and returns None. More precisely I am trying to define a log_posterior from a model that has multiple weights (a shallow neural network, if you will). But Hamiltonian Monte Carlo does not work because my defined log_posterior does not have a gradient. **Describe the expected behavior** TF should return the gradient of the log_posterior function defined. **Standalone code to reproduce the issue**  In the example below tape.gradient should return the value of w as the gradient. Here DummyModel's call supposed to replicate a log_postrior function, where I reassign its parameter ""w"" with ""x"".  ",2022-02-24T20:18:50Z,type:support TF 2.8,closed,0,1,https://github.com/tensorflow/tensorflow/issues/54532,Are you satisfied with the resolution of your issue? Yes No
378,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reduce flakyness of `//tensorflow/c/eager:c_api_test_cpu`.)ï¼Œ å†…å®¹æ˜¯ (Reduce flakyness of `//tensorflow/c/eager:c_api_test_cpu`. Applying a similar fix as before, just increasing length of backfill queue)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,copybara-service[bot],Reduce flakyness of `//tensorflow/c/eager:c_api_test_cpu`.,"Reduce flakyness of `//tensorflow/c/eager:c_api_test_cpu`. Applying a similar fix as before, just increasing length of backfill queue",2022-02-24T18:31:46Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54523
1789,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow-lite with Flex Delegates build fails for aarch64 source architecture using TF2.7)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 583dc6ac55ff 5.4.144+  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): Source  TensorFlow version (use command below): 2.7.0   Python version: 3.7.12  Bazel version (if compiling from source): 3.7.2 **Describe the current behavior** I'm trying to build a shared library for tensorflowlite including the Flex Delegate by Modifying the BUILD to add following dependency:  Since I'm targeting a Linux system running on my aarch64 board, the command I ran is the below:  I also made sure to hide my OpenSSL from my /usr/include as suggested in the following issue https://github.com/tensorflow/tensorflow/issues/48401. I noticed that the build fails at compiling **icu** external source code. **Describe the expected behavior** The expected behavior is a build completed successfully and a shared library libtensorflowlite.so generated and supporting Flex Delegates with TF selected Ops. **Standalone code to reproduce the issue** Here is a Google Colab gist to reproduce the Build issue. All you need to do is to modify the tensorflow/lite/BUILD by adding the following line to the libtensorflowlite.so deps.   **Other info / logs**  Here is the full traceback of the error that shows in the middle of the build and makes it fail. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OAHLSTM,Tensorflow-lite with Flex Delegates build fails for aarch64 source architecture using TF2.7,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 583dc6ac55ff 5.4.144+  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): Source  TensorFlow version (use command below): 2.7.0   Python version: 3.7.12  Bazel version (if compiling from source): 3.7.2 **Describe the current behavior** I'm trying to build a shared library for tensorflowlite including the Flex Delegate by Modifying the BUILD to add following dependency:  Since I'm targeting a Linux system running on my aarch64 board, the command I ran is the below:  I also made sure to hide my OpenSSL from my /usr/include as suggested in the following issue https://github.com/tensorflow/tensorflow/issues/48401. I noticed that the build fails at compiling **icu** external source code. **Describe the expected behavior** The expected behavior is a build completed successfully and a shared library libtensorflowlite.so generated and supporting Flex Delegates with TF selected Ops. **Standalone code to reproduce the issue** Here is a Google Colab gist to reproduce the Build issue. All you need to do is to modify the tensorflow/lite/BUILD by adding the following line to the libtensorflowlite.so deps.   **Other info / logs**  Here is the full traceback of the error that shows in the middle of the build and makes it fail. ",2022-02-24T14:30:34Z,stat:awaiting response type:build/install comp:lite subtype:bazel TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54517,Hello   Have you been able to take a look at this ?,"Hello   , Has anyone took a look at this issue ? is it about some missing arguments ? Thank you for your cooperation. Othmane,",I've verified the issue. Will take a look.,Hi  ! This issue is not replicating in 2.9 with Bazel 5.0.0 and Ubuntu 18 .Attaching gist for reference. Thank you!,Are you satisfied with the resolution of your issue? Yes No
1306,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Libtensorflowlite.so build failed for aarch64 with Flex Delegate included in dependencies)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 0be7898f8b6f 5.4.144+ from Google Colab**  Mobile device: **Raspberry Pi Aarch64 target architecture**  TensorFlow installed from (source or binary): **Source**  TensorFlow version (use command below): **2.7.0**  Python version: **3.7.12**  Bazel version (if compiling from source): **3.7.2** **Describe the current behavior** I'm trying to build a shared library for tensorflowlite including the Flex Delegate by Modifying the BUILD to add following dependency:  Since I'm targeting a Linux system running on my aarch64 board, the command I ran is the below:   **Describe the expected behavior** The expected behavior is a build completing successfully.  **Standalone code to reproduce the issue** I provide a gist from google Colab to make you able to reproduce the issue.  **Other info / logs**  Thank you in advance for your support.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OAHLSTM,Libtensorflowlite.so build failed for aarch64 with Flex Delegate included in dependencies,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 0be7898f8b6f 5.4.144+ from Google Colab**  Mobile device: **Raspberry Pi Aarch64 target architecture**  TensorFlow installed from (source or binary): **Source**  TensorFlow version (use command below): **2.7.0**  Python version: **3.7.12**  Bazel version (if compiling from source): **3.7.2** **Describe the current behavior** I'm trying to build a shared library for tensorflowlite including the Flex Delegate by Modifying the BUILD to add following dependency:  Since I'm targeting a Linux system running on my aarch64 board, the command I ran is the below:   **Describe the expected behavior** The expected behavior is a build completing successfully.  **Standalone code to reproduce the issue** I provide a gist from google Colab to make you able to reproduce the issue.  **Other info / logs**  Thank you in advance for your support.",2022-02-23T16:12:43Z,type:bug comp:lite TF 2.7,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54500, Here is a link to the Google Colab Notebook to reproduce the build issue. ,"It seems to be a clone of this issue https://github.com/tensorflow/tensorflow/issues/52018 Hence, I'm closing it, sorry for the convenience.",Are you satisfied with the resolution of your issue? Yes No
1878,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to remove TensorFlow CUDA messages? TensorFlow was installed for CPU only)ï¼Œ å†…å®¹æ˜¯ (I installed TensorFlow 2.8.0 in my Windows 10 PC with CPU only. No Nvidia/CUDA hardware/software installed at all. I run a simple Python image processing project with CNN and I got bunch of no sense messages: 20220223 07:31:55.511878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found  20220223 07:31:55.512119: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.  20220223 07:32:16.820716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found  20220223 07:32:16.820934: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 20220223 07:32:16.823066: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP7OCT2M4  20220223 07:32:16.823285: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP7OCT2M4  20220223 07:32:16.823612: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations: AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Why I'm getting these CUDA messages? I never installed anything for CUDA. I did try the following like everyone recommended in Google: os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' os.env)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ebonat,How to remove TensorFlow CUDA messages? TensorFlow was installed for CPU only,"I installed TensorFlow 2.8.0 in my Windows 10 PC with CPU only. No Nvidia/CUDA hardware/software installed at all. I run a simple Python image processing project with CNN and I got bunch of no sense messages: 20220223 07:31:55.511878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found  20220223 07:31:55.512119: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.  20220223 07:32:16.820716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found  20220223 07:32:16.820934: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 20220223 07:32:16.823066: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP7OCT2M4  20220223 07:32:16.823285: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP7OCT2M4  20220223 07:32:16.823612: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations: AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Why I'm getting these CUDA messages? I never installed anything for CUDA. I did try the following like everyone recommended in Google: os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' os.env",2022-02-23T16:10:11Z,stat:awaiting response type:build/install subtype:windows TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/54499,", For TensorFlow 2.x, CPU and GPU packages are same. There was no issue with Tensorflow installation and you can ignore all those warnings if you are not using GPU cards. They are just the information message as they are prefixed with `I`, if it is the error message they would be prefixed with `E` or `W` for warnings. To turn off the Tensorflow warnings use below code snippet  To turn of full log use below below code snippet  For more information please refer here. Thanks!","  `pip install tensorflow` will install a version thats compatible with GPU and CPU.  So it gives you that warning messages.  If you don't want to see warning messages and want to install CPU only version, you could  `pip install tensorflowcpu` that's a smaller wheel file for CPU only version",  thank you for letting me know. In this case the TensorFlow site did not mention that at all. what is the best way to uninstall this version now? ,_To turn off the Tensorflow warnings use below code snippet_  This code did not do anything at all. The CUDA messages remained the same  just to let you know. Thanks for your help.,  You can    to install a CPU only version.,All fixed.  Thank you  for your help!,Are you satisfied with the resolution of your issue? Yes No
1079,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TFLite] memory cost is much higher when enable xnnpack delegate)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OPPO Find X3Pro Dear I build TFLite benchmark_model from source code, based on commitID: 965c39fdf304a80eacd6cdca43241956084301c3 (CommitDate: Mon Feb 21 15:37:46 2022 0800) build command:   then I run benchmark_model on my Android Phone, I test with model  https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip when disable xnnpack, the memory cost is 8.8MB, see below: build command:   When enable xnnpack, the memory cost is 13.8MB, see below:  Could you please help check this ?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,huanyingjun,[TFLite] memory cost is much higher when enable xnnpack delegate,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OPPO Find X3Pro Dear I build TFLite benchmark_model from source code, based on commitID: 965c39fdf304a80eacd6cdca43241956084301c3 (CommitDate: Mon Feb 21 15:37:46 2022 0800) build command:   then I run benchmark_model on my Android Phone, I test with model  https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip when disable xnnpack, the memory cost is 8.8MB, see below: build command:   When enable xnnpack, the memory cost is 13.8MB, see below:  Could you please help check this ?",2022-02-23T11:26:14Z,stat:awaiting response stale comp:lite type:performance comp:lite-xnnpack,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54496,"> **System information** >  > * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No > * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 > * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OPPO Find X3Pro >  > Dear I build TFLite benchmark_model from source code, based on commitID: 965c39f (CommitDate: Mon Feb 21 15:37:46 2022 0800) >  > build command: >  >  >  > then I run benchmark_model on my Android Phone, I test with model https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip >  > when disable xnnpack, the memory cost is 8.8MB, see below: build command: >  >  >  > When enable xnnpack, the memory cost is 13.8MB, see below: >  >  >  > Could you please help check this ? Thx for reporting this! This is not surprising to us when we turned on XNNPACK by default, especially when the model is small and the relative memory footprint increase is quite large. We intentionally made a tradeoff between memory consumption increase and performance gains across a wide range of platforms.  Note we provide multiple options to disable using XNNPACKbydefault if the memory consumption increase doesn't fit your use cases. One could use any one of the following options: 1. Add define=tflite_with_xnnpack=false when compiling the code using Bazel. 2. In C++ code, use BuiltinOpResolverWithoutDefaultDelegates when resolving Tensorflow Lite ops. 3. In Python code, use BUILTIN_WITHOUT_DEFAULT_DELEGATES when creating the Interpreter.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1198,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Read only tensor is mapped as input to IF operation)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution: Google Colab Ubuntu 18.04  TensorFlow installed from binary  TensorFlow version: 2.8.0  2. Code Colab Notebook: link  3. Failure after conversion The readonly tensors containing the weights are mapped as inputs for the `IF` operation in Subgraph ` CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)`. I agree on the fact that these are indeed inputs for the subgraph but this behavior causes some difficulties in combination with TFLM. More specifically, every input tensor is copied over to the ""subgraph's execution environment"". Especially for these (large) readonly tensors, it's a huge unnecessary overhead because these could be linked to the OPs that use the tensors directly.   I already posted this issue in the TensorFlow lite micro repository over here https://github.com/tensorflow/tflitemicro/issues/903. Is there any option besides modifying the .tflite file to get these tensors out of the input tensor list of the `IF` operation?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ScholliYT,Read only tensor is mapped as input to IF operation," 1. System information  OS Platform and Distribution: Google Colab Ubuntu 18.04  TensorFlow installed from binary  TensorFlow version: 2.8.0  2. Code Colab Notebook: link  3. Failure after conversion The readonly tensors containing the weights are mapped as inputs for the `IF` operation in Subgraph ` CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)`. I agree on the fact that these are indeed inputs for the subgraph but this behavior causes some difficulties in combination with TFLM. More specifically, every input tensor is copied over to the ""subgraph's execution environment"". Especially for these (large) readonly tensors, it's a huge unnecessary overhead because these could be linked to the OPs that use the tensors directly.   I already posted this issue in the TensorFlow lite micro repository over here https://github.com/tensorflow/tflitemicro/issues/903. Is there any option besides modifying the .tflite file to get these tensors out of the input tensor list of the `IF` operation?",2022-02-23T00:36:40Z,stat:awaiting response stat:awaiting tensorflower stale type:performance comp:micro TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/54488," As mentioned here, the latest TFLM no longer requires copying back to input. This should be fixed in the latest version of TFLM. Please try and let me know. Thanks!","Hi , Sorry for my late response to this. I see the following 2 points with the state of this issue: 1. The issue of copying back into readonly memory regions indeed is resolved as  pointed out here https://github.com/tensorflow/tflitemicro/issues/903. 2. The overall main problem of unnecessary copying of readonly tensors is not resolved and still exists. For example: with the example linked above 3 tensors each of size 10x10 (x 4 bytes) are copied, i.e. the input, fc1_kernel and fc2_kernel. This happens for both subgraphs. As said in the other issue I made a script wich rewires the readonly tensors such that these are no longer copied from subgraph to subgraph. The code can be found here: https://colab.research.google.com/drive/1kMqLF985dpIUmxJnGI5ospCMQ0p8wH8?usp=sharing. Please note that this is not well documented, yet â˜ºï¸ I might write a self contained blogpost on this soon.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
340,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([oneDNN] Optimize CNMS implementation)ï¼Œ å†…å®¹æ˜¯ (Eliminate unnecessary data copying in per class NMS computation, this reduces memory usage and improves performance.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yimeisun123,[oneDNN] Optimize CNMS implementation,"Eliminate unnecessary data copying in per class NMS computation, this reduces memory usage and improves performance.",2022-02-22T17:24:13Z,size:M comp:core,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54482,"CNMS benchmark test result (run on  Xeon 28 cores/socket, 2 sockets sever)  Comparison of real time in ns between baseline vs with this PR change.         name  1.137008   When tested endtoend inference performance using SSDMobileNet frozen graphs with CNMS, int8 has about 12% gain, and float32 has about 7% gain. ",  please let me know when you may be able to review this PR. Thanks,"I checked the logs for ""Ubuntu CPU  Internal CI build failed"" and ""IntelÂ® oneDNN  Community CI Build"", they have the same two test case failures and the failures are not related to this PR change.","Yes, probably broken by c6531b8bcb83d1314acc96360156b549afecb377 LLVM/MLIR","If this PR is approved, can it be merged now? Thanks"
1584,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite with Hexagon delegate produces wrong results for a particular model)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (host), Android 11 (target device)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: device with Snapdragon 660  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): 8.4.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the current behavior** For a custom model I am trying to use, when I enable the Hexagon delegate, the output contains incorrect values. The Hexagon delegate seems to work fine with other models I have tried. **Describe the expected behavior** I expect the Hexagon delegate to produce the same results as CPU. **Standalone code to reproduce the issue** The difference between CPU and Hexagon delegate can be observed with your Inference Diff tool:  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Model used: test_model.zip)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AB5247,TFLite with Hexagon delegate produces wrong results for a particular model,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (host), Android 11 (target device)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: device with Snapdragon 660  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): 8.4.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the current behavior** For a custom model I am trying to use, when I enable the Hexagon delegate, the output contains incorrect values. The Hexagon delegate seems to work fine with other models I have tried. **Describe the expected behavior** I expect the Hexagon delegate to produce the same results as CPU. **Standalone code to reproduce the issue** The difference between CPU and Hexagon delegate can be observed with your Inference Diff tool:  **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Model used: test_model.zip",2022-02-22T16:53:55Z,stat:awaiting response stat:awaiting tensorflower type:bug stale TFLiteHexagonDelegate TF 2.7,closed,0,33,https://github.com/tensorflow/tensorflow/issues/54481,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!"," As I mentioned in my initial comment, the issue can easily be reproduced with the Inference Diff tool from this repository.","Hi, are there any updates? Has anyone been able to take a look at this issue?",   Can anyone provide an update on the status of this issue or if someone will be able to take a look at it soon? Thank you., Can you please check  's previous responses as he responded to many of the Hexagon related issues. Thanks," Sorry, but this is not related to anything reported before and answered by . TFlite with the Hexagon delegate is simply giving completely wrong results for the model I am working with, someone from your team will have to look at it and tell me what is going on. https://github.com/tensorflow/tensorflow/issues/36804 was a similar problem  and it was a bug that you eventually fixed.",Sorry for the late reply as i was out of office and came back today. Will check it and update the issue. Thanks, Any updates on this? Thanks.,"  Sorry to bother you again, but can this be assigned to someone else if  is not able to take a look? It has been almost 2 months since I opened this issue and no one has told me yet if you can even reproduce the same bug I experience. Thanks.","Hi   Sorry for the late update, we didn't ignore you. I was sick for some time. Thanks for reporting I can reproduce the issue, and found a bug related to relu layer and sent a fix for it. There is another issue related to depthwise conv when stride h/w > 1 and shape has ?x5x5x? don't have a fix but will be disabling this until fix is ready  sending a change to it too.. Will update the issue here when changes are in. Thanks","Thanks , glad you found the bugs. Please, keep me updated about the fixes. Thanks.","Hi  , are there any updates for this issue? Is a fix going to go in soon? Thanks, John.",The reshape fix is merged. The depthwise conv is not yet. Sorry for the delay Thanks,"Thanks  ! Do you have any update on the depthwise conv defect? Thanks, Massimo.","Hi  , I was wondering if there was any update for this issue? Thanks! Massimo.","Hi  , The depthwise conv issue is identified and it's a defect in Hexagon's depthwiseSupernode implementation. Unfortunately we can not modify hexagon nn lib. I'm trying to provide a workaround so we can avoid disabling it, but it requires some refactoring on the hexagon delegate code base. Will update in the thread next week.","Hi  , thanks for your update. Really appreciated! Talk to you next week then. Have a good weekend! Massimo.",Hi  ! Any new update you could share? Thanks! Massimo., The change in review and will be merged soon. Thanks for your patience.,Hi   Could you help validate if the pushed change 5d18974 fixes the issue?  Thanks!,"Hi  , thanks! We'll test the change and let you know soon. Thanks again! Massimo.","Hi   I built tensorflow from the current master branch and tried to run the model with the same Inference Diff tool and the hexagon delegate, this is what I get now:  Thanks",Hi   What's the device you are trying to run on? Also could you try different devices with the inference diff? Thanks.,"It's a device with Snapdragon 660 and it's the only device I have that supports the hexagon delegate.  When I try the inference diff built from v2.7 I get the high error values as reported originally, but it runs. When I build it from current master I get the error message mentioned before and does not even run. Thanks",Hi  ! Do you think you guys can run the change on a SnapDragon660 based device or something similar to reproduce our environment? Thanks! Massimo.,"Hi  , We are reproducing the crash on similar devices and investigating the cause of the crash. Will update in the thread. Thanks","Hi  , Thanks! Please keep us updated about the progress. Thanks again, Massimo.","Hi  , Any recent update you could share with us on this issue? Thanks! Massimo.","I use TF 2.7 for training and deploy to Proto Buffer. After that, I try to convert them with different TF version. So far, some models on Hexagon still have wrong value on different version of tensorflow, but all fine on CPU. I can reproduce this bug using TFLite converter in Tensorflow 2.4.0 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.1.  Troubleshooting In our case, some building block, e.g. the residuallike conv. building block, may cause ***standalone ReLU*** operator in TFLite. When this kind of operators occurs in graph, the results on CPU and Hexagon are usually different. For example, this kind of building block result in wrong value in Hexagon. !image  Consistent results on TF 2.2 I also find that using converter in Tensorflow 2.2.0 get correct result on both CPU and Hexagon.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space."
1002,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA tf.bincount support)ï¼Œ å†…å®¹æ˜¯ (**System information**  TensorFlow version (you are using): master  Are you willing to contribute it (Yes/No): only if we have a clear path and a reviewer on how to contribute this **Describe the feature and the current behavior/state.** `tf.bincount` isn't supported by XLA **Will this change the current api? How?** No **Who will benefit with this feature?** Speedup functions/loops that rely on `tf.bincount` **Any Other info.** How to reproduce it:   Without this we had problems to compile intermediate Ms Coco recall function in kerascv: https://github.com/kerasteam/kerascv/issues/141issuecomment1043692891 Extra:  Please also note that the CPU/GPU TF2XLA supported ops tables are probably outdated (2018): https://github.com/tensorflow/tensorflow/issues/14798issuecomment1047796247 / )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,bhack,XLA tf.bincount support,**System information**  TensorFlow version (you are using): master  Are you willing to contribute it (Yes/No): only if we have a clear path and a reviewer on how to contribute this **Describe the feature and the current behavior/state.** `tf.bincount` isn't supported by XLA **Will this change the current api? How?** No **Who will benefit with this feature?** Speedup functions/loops that rely on `tf.bincount` **Any Other info.** How to reproduce it:   Without this we had problems to compile intermediate Ms Coco recall function in kerascv: https://github.com/kerasteam/kerascv/issues/141issuecomment1043692891 Extra:  Please also note that the CPU/GPU TF2XLA supported ops tables are probably outdated (2018): https://github.com/tensorflow/tensorflow/issues/14798issuecomment1047796247 / ,2022-02-22T13:50:23Z,stat:awaiting tensorflower type:feature comp:xla,closed,0,16,https://github.com/tensorflow/tensorflow/issues/54479,Thanks for opening this.  This would be really helpful for the KerasCV project.,/.," is this still relevant?  1) If you are willing to write a TF2XLA kernel, I could help with review. 2) As a shorterterm solution, what do you think about using light outside compilation to achieve this, similarly to https://github.com/tensorflow/tensorflow/commit/1edb91b1cb4390189d77e15e491df035f4016fd2 ? In that solution, HLO will directly call into a TF kernel (which precludes fusing the TF kernel, but other optimizations remain viable).",What is the most similar thing to 1. for `tf.bincount` that we have in our portfolio: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2xla/kernels,Kernels for tf.where and tf.unique.,Ok i.e. where is the test for tf.unique?,"Is it in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function_xla_jit_test.py? Another question, is this going to be still valid when the mlir bridge will be enabled (as I see specific tests for this case) or will need need to write something totally different (just to understand if this effort will eventually have a minimal longevity).","Yes , it will be valid, mlir bridge can call tf2xla kernels. On Thu, May 26, 2022 at 00:49 bhack ***@***.***> wrote: > Is it in > https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function_xla_jit_test.py > ? > > Another question, is this going to be still valid when the mlir bridge > will be enabled (as I see specific tests for this case) or will need need > to write something totally different (just to understand if this effort > will eventually have a minimal longevity). > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >",Is the test I've pointed correct? In the end what Is the scope of a TF2XLA kernel? Is it  a c++ implementation that need to use  only XLA Op semantics? Any other constrain?,Just another info. I see that iterating the development over this TF2XLA kernels is quite slow also on a GCP instance with a modern CPU.  E.g. with every single edit in `unique_op.cc`:  ,  In the meant time I would try at least to prepare/merge a good test coverage with a dummy kernel.  We can discuss on the test early draft at https://github.com/tensorflow/tensorflow/pull/56276,> Thanks for opening this. This would be really helpful for the KerasCV project. As  didn't share any additional code/perf gist but we have merged yesterday the `bincount` XLA kernel in TF I've tried  again to `jit_compile` the Mscoco `MAP` `update` function with `tfnightly`: https://github.com/kerasteam/kerascv/blob/master/keras_cv/metrics/coco/mean_average_precision.pyL148L293 Running `pytest keras_cv/metrics/coco/mean_average_precision_test.py` we have:  As for two test the error output is truncated we have:  And for the last one.   Let me know if we could do something else here to cover these issues or I will close this as we have now the Bincount TF2XLA kernel in master.,"Ragged tensor is not supported in XLA, so it might not be feasible now? On Sat, Jul 9, 2022 at 15:57 bhack ***@***.***> wrote: > As   didn't share any additional > code/perf gist but we have merged yesterday the bincount XLA kernel in TF > I've tried again to jit_compile the Mscoco MAP update function with > tfnightly: > > https://github.com/kerasteam/kerascv/blob/master/keras_cv/metrics/coco/mean_average_precision.pyL148L293 > > Running pytest keras_cv/metrics/coco/mean_average_precision_test.py we > have: > > FAILED keras_cv/metrics/coco/mean_average_precision_test.py::COCOMeanAveragePrecisionTest::test_bounding_box_counting  tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 to node `while_1/while/while/bincount/Bincount` with op Bincount must be a compiletime constant.FAILED keras_cv/metrics/coco/mean_average_precision_test.py::COCOMeanAveragePrecisionTest::test_counting_with_missing_class_present_in_data  tensorflow.python.framework.errors_impl.UnimplementedError: Dynamic reshape is not implemented.FAILED keras_cv/metrics/coco/mean_average_precision_test.py::COCOMeanAveragePrecisionTest::test_mixed_dtypes  tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 to node `while_1/while/while/bincount/Bincount` with op Bincount must be a compiletime constant.FAILED keras_cv/metrics/coco/mean_average_precision_test.py::COCOMeanAveragePrecisionTest::test_runs_inside_model  tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:FAILED keras_cv/metrics/coco/mean_average_precision_test.py::COCOMeanAveragePrecisionTest::test_runs_with_confidence_over_1  tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_update_state_5982[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] > > As for two test the error output is truncated we have: > > 20220709 13:53:06.869179: W tensorflow/core/framework/op_kernel.cc:1777] OP_REQUIRES failed at bincount_op.cc:49 : UNIMPLEMENTED: Dynamic reshape is not implemented. >         while evaluating input 1 of Bincount operator as a compiletime constant.20220709 13:53:06.869839: W tensorflow/core/framework/op_kernel.cc:1777] OP_REQUIRES failed at while_op.cc:351 : UNIMPLEMENTED: Dynamic reshape is not implemented. >         while evaluating input 1 of Bincount operator as a compiletime constant. >          [[{{function_node while_1_while_while_body_494}}{{node while_1/while/while/bincount_1/Bincount}}]]20220709 13:53:06.870151: W tensorflow/core/framework/op_kernel.cc:1777] OP_REQUIRES failed at while_op.cc:351 : UNIMPLEMENTED: Dynamic reshape is not implemented. >         while evaluating input 1 of Bincount operator as a compiletime constant. >          [[{{function_node while_1_while_while_body_494}}{{node while_1/while/while/bincount_1/Bincount}}]] >          [[while_1/while/while_tfg_inlined_while_1/while_tfg_inlined_while_1_1_1]]20220709 13:53:06.870502: W tensorflow/core/framework/op_kernel.cc:1777] OP_REQUIRES failed at while_op.cc:351 : UNIMPLEMENTED: Dynamic reshape is not implemented. >         while evaluating input 1 of Bincount operator as a compiletime constant. >          [[{{function_node while_1_while_while_body_494}}{{node while_1/while/while/bincount_1/Bincount}}]] >          [[while_1/while/while_tfg_inlined_while_1/while_tfg_inlined_while_1_1_1]] >          [[while_1/while_tfg_inlined_while_1_1]]20220709 13:53:06.877054: W tensorflow/core/framework/op_kernel.cc:1777] OP_REQUIRES failed at xla_ops.cc:296 : UNIMPLEMENTED: Dynamic reshape is not implemented. >         while evaluating input 1 of Bincount operator as a compiletime constant. >          [[{{function_node while_1_while_while_body_494}}{{node while_1/while/while/bincount_1/Bincount}}]] >          [[while_1/while/while_tfg_inlined_while_1/while_tfg_inlined_while_1_1_1]] >          [[while_1/while_tfg_inlined_while_1_1]] >          [[while_1]] > > And for the last one. > > XLA_CPU_JIT: RaggedTensorToTensor (No registered 'RaggedTensorToTensor' OpKernel for XLA_CPU_JIT devices compatible with node {{node RaggedToTensor/RaggedTensorToTensor}}){{node RaggedToTensor/RaggedTensorToTensor}} > >   Let me know if we could do > something else here to cover these issues or I will close this as we have > now the Bincount TF2XLA kernel in master. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","> Ragged tensor is not supported in XLA, so it might not be feasible now? We know this but If you check is just one test case. What do you think about the other issues?",Some of these  `Dynamic reshape is not implemented` are: https://github.com/tensorflow/tensorflow/blob/262777e9f9304c7df6b694934af819c820954ef5/tensorflow/compiler/xla/literal.ccL963L965,"As XLA support it is finally merged, please followup Kerascv use case (excluding `Raggedtensor` input that is not supported in XLA) at https://github.com/tensorflow/tensorflow/issues/56769"
307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Web Page problem)ï¼Œ å†…å®¹æ˜¯ (When trying to access some pages of tensorflow it downloads an xml instead of showing the webpage !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PeraltaFede,Web Page problem,When trying to access some pages of tensorflow it downloads an xml instead of showing the webpage !image,2022-02-22T12:42:10Z,stat:awaiting response type:bug,closed,4,4,https://github.com/tensorflow/tensorflow/issues/54478,Many pages especially keras related pages all have this issue. Changing browsers does not work.,   We are able to reproduce the issue . We will resolve this soon.Thank you!, This should be fixed now.Could you please confirm the same?Thanks!,Are you satisfied with the resolution of your issue? Yes No
1848,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Building tensorflowlite.so for aarch_64 failed )ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (Windows 10):  Mobile device ( for RPI4) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source (cloned the repo)  TensorFlow version: latest  Python version: 3.7.9  Installed using virtualenv? pip? conda?: none  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): g++  CUDA/cuDNN version:   GPU model and memory: gtx 1060 6gb **Describe the problem** I followed this tutorial for cross compilation of aarch64  https://www.tensorflow.org/lite/guide/build_arm 1 cloned tensorflow repo  2 ran `bazel build config=elinux_aarch64 c opt //tensorflow/lite:libtensorflowlite.so` **Any other info / logs** INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=211 INFO: Reading rc options for 'build' from c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/Users/Mohamed_Gamal/AppData/Local/Microsoft/WindowsApps/python.exe INFO: Reading rc options for 'build' from c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc:   'build' options: define framework_shared_object=true java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_p)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mohamedgamal7,Building tensorflowlite.so for aarch_64 failed ,**System information**  OS Platform and Distribution (Windows 10):  Mobile device ( for RPI4) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source (cloned the repo)  TensorFlow version: latest  Python version: 3.7.9  Installed using virtualenv? pip? conda?: none  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): g++  CUDA/cuDNN version:   GPU model and memory: gtx 1060 6gb **Describe the problem** I followed this tutorial for cross compilation of aarch64  https://www.tensorflow.org/lite/guide/build_arm 1 cloned tensorflow repo  2 ran `bazel build config=elinux_aarch64 c opt //tensorflow/lite:libtensorflowlite.so` **Any other info / logs** INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=211 INFO: Reading rc options for 'build' from c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/Users/Mohamed_Gamal/AppData/Local/Microsoft/WindowsApps/python.exe INFO: Reading rc options for 'build' from c:\users\mohamed_gamal\downloads\tensorflow_src\.bazelrc:   'build' options: define framework_shared_object=true java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_p,2022-02-21T21:53:49Z,stat:awaiting response type:build/install stale comp:lite subtype:bazel,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54473,"Hi  ! Could you try with Bazel 4.2.2/5.0.0 ? if you face any error on  llvm package ,you can download and install it locally using Cmake . ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1881,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Internal error `Blas xGEMV launch failed` on Tensorflow v2.8.0 for the same block of codes that runs perfectly well on Tensorflow v2.4.1)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.8  Bazel version (if compiling from source): N.A.  GCC/Compiler version (if compiling from source): N.A.  CUDA/cuDNN version: CUDA 11.2.1  GPU model and memory: Tesla T4 / 16 GB **Describe the current behavior** Running a block of code with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1 returns an internal error `Blas xGEMV launch failed` when it runs perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0. **Describe the expected behavior** Return the same output as Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0. **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): N.A. **Standalone code to reproduce the issue** The following block of code works perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0, but not with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1.  An important point to note is that when I reduce the `shape` of `empty_image` to `[512, 512, 3]`, there is no issue. However, I believe this is not a device memory issue as I can reproduce this with GeForce RTX 2080 Ti 11 GB as well as Tesla T4 16 GB. **Other info / logs**  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,arvindrajan92,Internal error `Blas xGEMV launch failed` on Tensorflow v2.8.0 for the same block of codes that runs perfectly well on Tensorflow v2.4.1,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.8  Bazel version (if compiling from source): N.A.  GCC/Compiler version (if compiling from source): N.A.  CUDA/cuDNN version: CUDA 11.2.1  GPU model and memory: Tesla T4 / 16 GB **Describe the current behavior** Running a block of code with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1 returns an internal error `Blas xGEMV launch failed` when it runs perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0. **Describe the expected behavior** Return the same output as Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0. **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): N.A. **Standalone code to reproduce the issue** The following block of code works perfectly well with Tensorflow v2.4.1 / Cuda 11.0 / CuDNN 8.0, but not with Tensorflow v2.8.0 / Cuda 11.2 / CuDNN 8.1.  An important point to note is that when I reduce the `shape` of `empty_image` to `[512, 512, 3]`, there is no issue. However, I believe this is not a device memory issue as I can reproduce this with GeForce RTX 2080 Ti 11 GB as well as Tesla T4 16 GB. **Other info / logs**  ",2022-02-21T01:08:35Z,type:bug type:build/install TF 2.8,closed,0,12,https://github.com/tensorflow/tensorflow/issues/54463,"Currently, the workaround for me is to use CUDA 11.1 with cuDNN 8.1.1. I arrived at this after finding out that Google Colab has TensorFlow 2.8.0 installed but runs on CUDA 11.1, although TensorFlow's compatibility matrix recommends CUDA 11.2. When I installed CUDA 11.1, which is usually bundled with cuDNN 8.0.x, TensorFlow threw an error saying it requires cuDNN 8.1.x. Hence, upgrading cuDNN to 8.1.1 does the trick.  Having said that, I believe the reported bug is something to be looked at and addressed. I have a feeling this problem would appear in all TensorFlow versions that recommends CUDA 11.2 and cuDNN 8.1, i.e., TensorFlow >= 2.5.0, and I am saying this because I was getting the same error after downgrading to TensorFlow 2.7.0 on CUDA 11.2 and cuDNN 8.1. For those who has CUDA 11.1 installed with cuDNN 8.0.x on Ubuntu 18.04 / 20.04, the following commands would upgrade your cuDNN version from 8.0.x to 8.1.1. "," , Google Colab has TensorFlow 2.8.0 installed and runs on CUDA 11.2 and I was able to execute the given code without any issues.Please find the gist here.Thanks!",">  , Google Colab has TensorFlow 2.8.0 installed and runs on CUDA 11.2 and I was able to execute the given code without any issues.Please find the gist here.Thanks! hi , thank you for getting back to me. your gist brings me back to this issue though. could you check your link please? also, this is my google colab notebook which says CUDA 11.1 when i execute `nvcc version`",",  Given configurations Tested build configuration were tested on different platforms.  This error is due to   OOM error GPU is running out of memory  Doesn't have enough compute capacity  There's a driver issue. Can you verify the memory usage with nvidiasmi? If you have any other processes using the GPU. And also check CUDA compute capability for the given nvidia drivers. ","> , Given configurations Tested build configuration were tested on different platforms. >  > This error is due to >  > * OOM error GPU is running out of memory > * Doesn't have enough compute capacity > * There's a driver issue. >  > Can you verify the memory usage with nvidiasmi? If you have any other processes using the GPU. And also check CUDA compute capability for the given nvidia drivers. Thank you for taking a look at this issue . Please allow me to address your points. **OOM error GPU is running out of memory** Below is the block of codes and outputs when ran on AWS Deep Learning AMI GPU CUDA 11.2.1 (Ubuntu 20.04) 20220208. I have attached below the screenshot from nvidiasmi after running the codes. I can confirm that the GPU did not run out of memory and there are no other processes using the GPU as you can see from nvidiasmi. Furthermore, running these codes has not used more than 1GB of GPU memory.  !image **Doesn't have enough compute capacity** From the code block above, you can see that the compute capability is 7.5. Is this not enough? **There's a driver issue.** I can reproduce this on any of the AWS's AMI with GPU and CUDA 11.2.1 installed. Similarly, I can reproduce this on my local machine with Geforce RTX 3060 which has compute capability of 8.6 where the NVIDIA driver, CUDA 11.2.1 and cuDNN 8.1 are freshly installed. However, I don't see this issue on any of AWS's AMI with CUDA 11.1.1 installed after upgrading cuDNN to version 8.1 (from version 8.0)  I observe the same behaviour when installing CUDA 11.1.1 and cuDNN 8.1 on my local machine with Geforce RTX 3060. Are you able to run this on a physical machine with CUDA 11.2.1 and cuDNN 8.1 without issues? ","Hi , are you still looking into this issue? Thanks.",", > However, I don't see this issue on any of AWS's AMI with CUDA 11.1.1 installed after upgrading cuDNN to version 8.1 (from version 8.0)  I observe the same behaviour when installing CUDA 11.1.1 and cuDNN 8.1 on my local machine with Geforce RTX 3060. Indeed this is expected behaviour. As per the Tensorflow document, CUDA 11.2 and cuDNN 8.1 are compatible versions. I could run the given code on CUDA 11.2 with cuDNN 8.1. Thanks!","It may be a bug of cublas. cublas 11.4 resolved an issue: > Some gemv cases were producing incorrect results if the matrix dimension (n or m) was large, for example 2^20. In your case, m=1638400>2^20. As cublas is not opensource, it's unclear what versions of cublas have this issue.","Thank you , looking at `tensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,1638400,3], b.shape=[1,3,1], m=1638400, n=1, k=3 [Op:MatMul]`, seems like it is probably due to the bug in cuBLAS.  When I change `shape` to `[512, 512, 3]`, I am getting the expected output. From trying out different versions of CUDA, seems like the bug is introduced in CUDA 11.2 and only resolved in CUDA 11.4. I don't see TensorFlow throwing the error in CUDA 11.1. Hi , I am happy to close the issue since it is a bug in cuBLAS 11.2. I suppose this is something to keep in mind so that upcoming TensorFlow versions are not built against CUDA 11.3, which may also have the same bug in cuBLAS.",", Thanks for confirming. Since the issue is more related to cuBLAS, will move this to closure. Thanks! ",Are you satisfied with the resolution of your issue? Yes No,"Blas xGEMV launch failed : a.shape=[1,8696332,3], b.shape=[1,3,1], m=8696332, n=1, k=3 [Op:MatMul] I am getting error as above i am using CUDA 11.2 and tensorflow version 2.11.1"
1917,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(InvalidArgumentError: Node '.../while_grad': Connecting to invalid output 4 of source node while which has 4 outputs)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Colab  TensorFlow version (use command below): 2.8.0 (Colab)  Python version: 3.7.12  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** I get the exception  when executing a `NoOp` without any control dependencies. But also when executing my real code, I get the same exception. I'm not sure if these are two separate issues or the same. **Describe the expected behavior** A NoOp without control dependencies should not depend on anything, so I would never expect such exception. For my real code, I also would not expect such exception.  **Standalone code to reproduce the issue** Code (to be executed with disabled eager mode):  Colab: https://colab.research.google.com/drive/1jjXpz8SAeU8Cg6nuWu7tjxK_sAU5lVc?usp=sharing **Other info / logs** When executing locally, I additionally see this log output:  My hypothesis is that the first session call `session.run(tf.compat.v1.global_variables_initializer())` will somehow freeze the while loop function graph (which is strange though as it would not depend on it), and then the further code which adds the gradients causes this warning `Operation ... StatelessWhile ... was changed by setting attribute after it wa)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,albertz,InvalidArgumentError: Node '.../while_grad': Connecting to invalid output 4 of source node while which has 4 outputs,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): Colab  TensorFlow version (use command below): 2.8.0 (Colab)  Python version: 3.7.12  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** I get the exception  when executing a `NoOp` without any control dependencies. But also when executing my real code, I get the same exception. I'm not sure if these are two separate issues or the same. **Describe the expected behavior** A NoOp without control dependencies should not depend on anything, so I would never expect such exception. For my real code, I also would not expect such exception.  **Standalone code to reproduce the issue** Code (to be executed with disabled eager mode):  Colab: https://colab.research.google.com/drive/1jjXpz8SAeU8Cg6nuWu7tjxK_sAU5lVc?usp=sharing **Other info / logs** When executing locally, I additionally see this log output:  My hypothesis is that the first session call `session.run(tf.compat.v1.global_variables_initializer())` will somehow freeze the while loop function graph (which is strange though as it would not depend on it), and then the further code which adds the gradients causes this warning `Operation ... StatelessWhile ... was changed by setting attribute after it wa",2022-02-19T22:24:17Z,stat:awaiting tensorflower type:bug comp:ops TF 2.8,open,0,9,https://github.com/tensorflow/tensorflow/issues/54458,"There are some related issues here, like: https://github.com/tensorflow/tensorflow/issues/39908. Some suggestion is to use eager mode, but I want to keep using graph mode.","Hi , I noticed you recently enabled `tf.compat.v1.experimental.output_all_intermediates(True)` in your application. Be aware that the flag may increase the memory usage. Another workaround to try, is to move all calls to `session.run` after mutations of the graph, e.g., (in your colab),   Depending on your application's structure, this change may be infeasible. (Also refer to the Keras situation mentioned in docstring of output_all_intermediates).","Hi , Did you get a chance to look at  comment . Thank you!","  I did. But  just made a comment on the `output_all_intermediates` usage. However, as long as this issue here is not fixed, I don't see the alternative? Even the error message suggests that: > Try using tf.compat.v1.experimental.output_all_intermediates(True). The other suggested workaround to rearrange the code is not possible in my case. So, as you see, it is important for me that you fix this.",Hi  ! I was able to resolve this issue by putting  `tf.compat.v1.experimental.output_all_intermediates(True) `after   `tf.compat.v1.disable_eager_execution() ` Attached resolved gist for reference. Thank you!,"But as you can read above: ""Be aware that the flag may increase the memory usage."" So this is not really a solution.",Ok  ! Thanks for the update.  ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",The error is still there in TF 2.17.0.
1896,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unrecognized term used by the documentation of the class ""tensorflow.compat.v1.variable_scope"".)ï¼Œ å†…å®¹æ˜¯ (The paragraph following:                       ""A note about using variable scopes in multithreaded environment: Variable                         scopes are thread local, so one thread will not see another thread's current                         scope. Also, when using `default_name`, unique scopes names are also generated                         only on a per thread basis. If the same name was used within a different                         thread, that doesn't prevent a new thread from creating the same scope.                         However, the underlying variable store is shared across threads (within the                         same graph). As such, if another thread tries to create a new variable with                         the same name as a variable created by a previous thread, it will fail unless                         reuse is True."" which is found in both the documentation (https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) and the source code (https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/variable_scope.pyL2144L2579). My question(s): For the last sentence of this paragraph, what is the point it talks about?  It seems to talk about a condition when two threads cause an error since one tries to have a variable of the same name as the other, previous one, although I am not sure what is the mentioned 'underlying variable store' referring to. But after some tests, I haven't found the condition yet, while it is always the case that each thread has a variable scope separated from others. Does anyone have learned about the docume)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,A1795066711,"Unrecognized term used by the documentation of the class ""tensorflow.compat.v1.variable_scope"".","The paragraph following:                       ""A note about using variable scopes in multithreaded environment: Variable                         scopes are thread local, so one thread will not see another thread's current                         scope. Also, when using `default_name`, unique scopes names are also generated                         only on a per thread basis. If the same name was used within a different                         thread, that doesn't prevent a new thread from creating the same scope.                         However, the underlying variable store is shared across threads (within the                         same graph). As such, if another thread tries to create a new variable with                         the same name as a variable created by a previous thread, it will fail unless                         reuse is True."" which is found in both the documentation (https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) and the source code (https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/variable_scope.pyL2144L2579). My question(s): For the last sentence of this paragraph, what is the point it talks about?  It seems to talk about a condition when two threads cause an error since one tries to have a variable of the same name as the other, previous one, although I am not sure what is the mentioned 'underlying variable store' referring to. But after some tests, I haven't found the condition yet, while it is always the case that each thread has a variable scope separated from others. Does anyone have learned about the docume",2022-02-18T13:27:45Z,stat:awaiting response type:bug stale comp:ops TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54449,"Hi  ! Could you please look at this issue? It is replicating in 2.7, 2.8 and nightly.","Variable store is nothing but the thread local store for the current variable scope and scope counts. variable store will be maintained on a thread level so that it will not create any problem during multithread environments. In your code when you change the `reuse=None`( which is equivalent to reuse = False) instead of `reuse=tf.AUTO_REUSE`, you will get the expected error outcome, if you want your variable store's scope to be shared to other threads  then you can use `reuse = True`. Below is the code with reuse=None which does not allow variable scope to be shared to other thread. ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"  Thank you very much for your answer! For your answer, I have two more questions as following. (1) Is the variable store mentioned in your answer the variable '_var_store' (should be initialized here) or the variable '_var_scope_store' (should be initialized here) inside the object of the class _pure_variable_scope (should be defined here) which could be found after the scope (should be an object of the class variable_scope, which should be defined here) containing, or ready to initialize it, is entered? (2) Is the mechanism of deciding whether or not to reuse a variable in a variable scope (variable_scope), **only** based on the values of the reuse fields, composed of the corresponding argument of the function whereby it should be defined here, as well as the value of the corresponding member variable (should be initialized here) priorly set to the scope, as well as the existence and the content of such store (for example, for _var_scope_store, as it should be an object of the class _VariableScopeStore, check its member variable(s), either 'variable_scopes_count' (should be defined here) or 'current_scope' (should be defined here), or both)?","`_var_scope_store` is a thread local store for the current variable scope and scope counts, see here. In multithread environment by default the variable scope mechanism will be in thread local, when the reuse option is set to `True`, then you can create the variable with the same name as the another thread.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1855,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problems compiling TensorFlow on M1 Mac within Rosetta)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Monterey 12.1, MacBook Pro M1 2021**  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**  TensorFlow installed from (source or binary): **source**  TensorFlow version (use command below): **2.7.1 (couldn't use the command because the import fails)**  Python version: **3.8.5 (Miniconda)**  Bazel version (if compiling from source): **Build label: 5.0.0 Build target: bazelout/darwinopt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Wed Jan 19 14:15:55 2022 (1642601755) Build timestamp: 1642601755 Build timestamp as int: 1642601755**  GCC/Compiler version (if compiling from source): **Configured with: prefix=/Library/Developer/CommandLineTools/usr withgxxincludedir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1 Apple clang version 13.0.0 (clang1300.0.29.30) Target: x86_64appledarwin21.2.0 Thread model: posix InstalledDir: /Library/Developer/CommandLineTools/usr/bin**  CUDA/cuDNN version: **n/a**  GPU model and memory: **n/a** You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; pr)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dalkio,Problems compiling TensorFlow on M1 Mac within Rosetta,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Monterey 12.1, MacBook Pro M1 2021**  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**  TensorFlow installed from (source or binary): **source**  TensorFlow version (use command below): **2.7.1 (couldn't use the command because the import fails)**  Python version: **3.8.5 (Miniconda)**  Bazel version (if compiling from source): **Build label: 5.0.0 Build target: bazelout/darwinopt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Wed Jan 19 14:15:55 2022 (1642601755) Build timestamp: 1642601755 Build timestamp as int: 1642601755**  GCC/Compiler version (if compiling from source): **Configured with: prefix=/Library/Developer/CommandLineTools/usr withgxxincludedir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1 Apple clang version 13.0.0 (clang1300.0.29.30) Target: x86_64appledarwin21.2.0 Thread model: posix InstalledDir: /Library/Developer/CommandLineTools/usr/bin**  CUDA/cuDNN version: **n/a**  GPU model and memory: **n/a** You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; pr",2022-02-18T13:06:35Z,stat:awaiting response type:bug type:build/install stale subtype:macOS TF 2.7,closed,1,15,https://github.com/tensorflow/tensorflow/issues/54448,", Can you please refer to similar issues CC(TensorFlow support for Apple Silicon (M1 Chips)), CC(M1 arm64 release binaries), CC(How to run tensorflow 2.4.0 on ARM Mac using Rosetta 2) for installation and let us know if it help? Thanks!"," it looks like  isn't trying to install TF, they are trying to compile it from source.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> , > > Can you please refer to similar issues https://github.com/tensorflow/tensorflow/issues/44751,https://github.com/tensorflow/tensorflow/issues/47782, https://github.com/tensorflow/tensorflow/issues/46044 for installation and let us know if it help? Thanks! Thanks for the reply. I'm indeed trying to compile it from source, not import it from a release. Can we reopen the issue please?","M1 related build issues are officially not supported by Tensorflow, since we currently don't have Tensorflow build for M1 officially from us. Please refer to this document for building Tensorflow. If you have any additional queries, you can post your issues on https://developer.apple.com/forums/. Thanks!","The document you linked does not talk about building TensorFlow at all. Apple's M1 TensorFlow release is closedsource, as you surely must know. Your build page https://www.tensorflow.org/install/sourcemacos does not say anything about not supporting M1. This is all it says about the macOS build requirements:  Maybe if you don't support Apple M1 machines (which is a large and growing segment of the developer market) you should say that explicitly on the TF buildfromsource website.",Please refer the Note section in the document here.installthepythondevelopmentenvironmentonyoursystem) and let us know if this helps. Thanks!,"That document is about installing TensorFlow, not building it, and as we have already discussed in this GitHub thread, the OP is asking about building TF, not installing it.",I have the exact same issues. Are there any updates on this? I'm interested in building Tensorflow within Rosetta on an M1 Macbook.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. Also M1 related build issues are officially not supported by Tensorflow, since we currently don't have Tensorflow build for M1 officially from us. Please refer to this document for building Tensorflow. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1716,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(libXNNPACK.so: internal symbol `xnn_f16_ibilinear_ukernel__neonfp16arith_c8' isn't defined)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): source  TensorFlow version: 56e2e916929746cd6c415c2a402a46349bffa7e0 (commit id)  Python version: N/A  Installed using virtualenv? pip? conda?:  N/A  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): 9.3.0  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the problem** I was trying to build TFLite with XNNPACK delegate natively on an ARMbased board using CMake. Previously I successfully built TFLite as static library, but when I tried to link it to my program, many errors of undefined references related to `ruy` happened. So I turned to compile TFLite as shared library instead, but got the following error:  **Provide the exact sequence of commands / steps that you executed before running into the problem**  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fengyuentau,libXNNPACK.so: internal symbol `xnn_f16_ibilinear_ukernel__neonfp16arith_c8' isn't defined,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): source  TensorFlow version: 56e2e916929746cd6c415c2a402a46349bffa7e0 (commit id)  Python version: N/A  Installed using virtualenv? pip? conda?:  N/A  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): 9.3.0  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Describe the problem** I was trying to build TFLite with XNNPACK delegate natively on an ARMbased board using CMake. Previously I successfully built TFLite as static library, but when I tried to link it to my program, many errors of undefined references related to `ruy` happened. So I turned to compile TFLite as shared library instead, but got the following error:  **Provide the exact sequence of commands / steps that you executed before running into the problem**  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-02-18T03:19:46Z,stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux,closed,0,12,https://github.com/tensorflow/tensorflow/issues/54437,Hi  ! Can you check below command  after declaring ARM 64 SDK tool chain in path and removing DBUILD_SHARED_LIBS=ON flag ? Thanks! `cmake DCMAKE_TOOLCHAIN_FILE= ../tensorflow/lite/`," Thanks for the reply, but TFLite is compiled natively on ARM in my case.","> Hi  ! Can you check below command after declaring ARM 64 SDK tool chain in path and removing DBUILD_SHARED_LIBS=ON flag ? Thanks! >  > `cmake DCMAKE_TOOLCHAIN_FILE= ../tensorflow/lite/` I met the same problem and I can compile static library successfully, but failed with ""DBUILD_SHARED_LIBS=ON"" on Android64.  ""error: undefined reference to `xnn_f16_ibilinear_ukernel__neonfp16arith_c8'""", Did you have undefined reference related to `ruy` when linking TFLite static library to your programe?,">  Did you have undefined reference related to `ruy` when linking TFLite static library to your programe?  ......I was wrong,  this error appeared again when linking TFLite static library. !image","This was fixed in XNNPACK in google/XNNPACK, and the latest revision of TensorFlow shouldn't have this issue.", Let me check which version of XNNPACK is using when I compiled TLite.," The last compilation I tried was 3 days ago, but somehow CMake scripts downloaded XNNPACK at https://github.com/google/XNNPACK/commit/d605aa76904aa1c5ea98bb7a0a4bafc4886b8a0a, which was 6 days ago (The fix you mention was 5 days ago). Do you know how to force using latest XNNPACK?","I wonder if `pip install tfliteruntime` comes with XNNPACK delegate for aarch64, which can be a workaround for now.",XNNPACK revision used in the build is specified here. You'd probably need to remove all configuration artifacts and reconfigure TFLite after updating this line., Thanks for your information! I pulled the latest Tensorflow which has a newer XNNPACK with the patch you mentioned previously. It can finish compilation without errors.,Are you satisfied with the resolution of your issue? Yes No
1587,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Using `+` in custom residual Keras `Layer` does not create correct model graph)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.7.12 Also reproducible on various hosted Jupyter environments (Kaggle, Colab) with and without GPU. **Describe the current behavior** In custom residual block implementation with keras APIs, `+` yields broken graph.  This is the resulting graph: !resblockbroken **Describe the expected behavior** This code produces the correct graph, where each `add` is a separate instance of `keras.layers.Add`. The `+` operator should produce the same graph.  !resblockworking I will add that this isn't just a visualization issue. My model would not train until after I identified this problem and applied the fixed implementation described above. This was causing serious issues with my gradients and the model could not learn because it was too deep without the skip connections. **Contributing**  Do you want to contribute a PR? (yes/no): I would consider it, but leaning towards no.  Briefly describe your candidate solution(if contributing): N/A **Standalone code to reproduce the issue** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jacoblubecki,Using `+` in custom residual Keras `Layer` does not create correct model graph,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): v2.8.0rc132g3f878cff5b6 2.8.0  Python version: 3.7.12 Also reproducible on various hosted Jupyter environments (Kaggle, Colab) with and without GPU. **Describe the current behavior** In custom residual block implementation with keras APIs, `+` yields broken graph.  This is the resulting graph: !resblockbroken **Describe the expected behavior** This code produces the correct graph, where each `add` is a separate instance of `keras.layers.Add`. The `+` operator should produce the same graph.  !resblockworking I will add that this isn't just a visualization issue. My model would not train until after I identified this problem and applied the fixed implementation described above. This was causing serious issues with my gradients and the model could not learn because it was too deep without the skip connections. **Contributing**  Do you want to contribute a PR? (yes/no): I would consider it, but leaning towards no.  Briefly describe your candidate solution(if contributing): N/A **Standalone code to reproduce the issue** ",2022-02-18T02:15:20Z,stat:awaiting response type:bug comp:keras TF 2.8,closed,0,7,https://github.com/tensorflow/tensorflow/issues/54436," I guess you cannot add layers using the '+' operator, as I think it would be confusing if that was allowed"," Sorry if it was a bit unclear  the `+` operator is being used to add the output tensors of the layers (_NOT_ adding the layers themselves). Both versions of the code work during the forward pass. When setting random seeds and `TF_DETERMINISTIC_OPS=1`, _**the outputs of each version are identical**_. Problems only appear during training because the gradients are not computed correctly for the `+`version due to the malformed computation graph, and the training problems are entirely nonobvious because the backwards pass executes and the model's weights do get updated, but the model fails to converge because the gradients aren't correct.",  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!," I have filed a ticket here:  https://github.com/kerasteam/keras/issues/16098 I replaced my `ConvBnAct` with the following:  Then I used tensorboard to trace two versions of `ResBlock`. Keras version using only `tensorflow` APIs (still inherits from Keras `Layer`, but otherwise only using `tf` namespace):  And pure tensorflow version (with no reference to any Keras APIs):  The Keras version is still broken on tensorboard, but the pure version with no Keras APIs appears to work fine.", Could you please let us know if we can move this issue to closed status as we are addressing the other ticket in  kerasteam/keras repo. ? Thanks!," Sure thing, thanks for the redirect.",Are you satisfied with the resolution of your issue? Yes No
1456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow-gpu doesn't pack CUDA and cuDNN anymore through conda-forge)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution : Windows 10  TensorFlow installed from (source or binary): Binary  TensorFlow version: 2.3.0  Python version: 3.8.12  Installed using virtualenv? pip? conda?: conda  CUDA/cuDNN version: 10.1/7.6  GPU model and memory: NVIDIA Quadro P400, 2Gb **Describe the problem** A few months ago, I installed TensorFlow on a machine through condaforge. After running into some issues installing it at the system level, I found the simplest solution was to install the tensorflowgpu package in a conda environment.  That metapackage came in with cudatoolkit and cudnn, and it worked out of the box. However, as I tried to replicate that install on another machine, I found that the list of packages provided when trying to install tensorflowgpu through condaforge did not contain cudatoolkit or cudnn.  I also tried again on the machine where I had a successful install, in another environment, and the issue persisted. **Provide the exact sequence of commands / steps that you executed before running into the problem** `conda install tensorflowgpu==2.3.0 c condaforge` conda outputs the following :   Then, running those lines :   Outputs the following :  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,b-grimaud,tensorflow-gpu doesn't pack CUDA and cuDNN anymore through conda-forge,"**System information**  OS Platform and Distribution : Windows 10  TensorFlow installed from (source or binary): Binary  TensorFlow version: 2.3.0  Python version: 3.8.12  Installed using virtualenv? pip? conda?: conda  CUDA/cuDNN version: 10.1/7.6  GPU model and memory: NVIDIA Quadro P400, 2Gb **Describe the problem** A few months ago, I installed TensorFlow on a machine through condaforge. After running into some issues installing it at the system level, I found the simplest solution was to install the tensorflowgpu package in a conda environment.  That metapackage came in with cudatoolkit and cudnn, and it worked out of the box. However, as I tried to replicate that install on another machine, I found that the list of packages provided when trying to install tensorflowgpu through condaforge did not contain cudatoolkit or cudnn.  I also tried again on the machine where I had a successful install, in another environment, and the issue persisted. **Provide the exact sequence of commands / steps that you executed before running into the problem** `conda install tensorflowgpu==2.3.0 c condaforge` conda outputs the following :   Then, running those lines :   Outputs the following :  ",2022-02-17T17:53:38Z,stat:awaiting response type:build/install stale subtype:windows TF 2.3,closed,0,11,https://github.com/tensorflow/tensorflow/issues/54423,"Looks like you have to be careful you match the correct Nvidia drivers, could be an issue? https://www.tensorflow.org/install/gpu", ! Could you please check with above comment ?,"> Looks like you have to be careful you match the correct Nvidia drivers, could be an issue? https://www.tensorflow.org/install/gpu The drivers I have installed can support CUDA 11.2. The TensorFlow install I'm looking for is compatible with CUDA 10.1. Is there a problem with retrocompatibility that would require downgrading my GPU drivers ? Also, as I stated before, even on the machine that already has a functional TF install, and thus presumably correct drivers, condaforge doesn't provide cudatoolkit or cudnn either.","Please try with ""conda install c anaconda cudatoolkit "" command . If that does not resolve the issue please check with instructions from here  with TF 2.8 after activating Conda environment?","The GPU still isn't recognized, and cuDNN still isn't installed either through conda with cudatoolkit or pip with tensorflow.",", Follow these steps to install Tensorflowgpu version with CUDA and cuDNN using Conda package  ","I have been able to install TF like this previously.  My issue was that those packages used to be wrapped in a single metapackage that, if I remember correctly, would install the correct verions of everything needed.  It was, in my opinion, an easier and safer way to install TF, and I was wondering why it isn't available anymore.",", It used be earlier. When all the compatible packages release happen at the same time, in that case we no need to explicitly mention any of the versions. Binary installer takes the latest stable version. Since when the CUDA and cuDNN and Tensorflow compatibility is required, we need to mention each package version explicitly. Thanks!  ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
714,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow not detecting GPU RTX 2070)ï¼Œ å†…å®¹æ˜¯ (I am trying to get tensorflow to detect my RTX 2070. I am using Ubuntu with the nvidia510 drivers. Pytorch is detecting the GPU but tensorflow is not detecting it. I tried reinstalling my drivers and trying the nightly version. I also followed the GPU instructions on the website and tried to install it wtih conda. I am getting those errors when starting Tensorflow:  This is my nvidiasmi output: !image I am using this code to detect the GPU:  This is what my usr/local looks like: !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Cheeseboy8020,Tensorflow not detecting GPU RTX 2070,I am trying to get tensorflow to detect my RTX 2070. I am using Ubuntu with the nvidia510 drivers. Pytorch is detecting the GPU but tensorflow is not detecting it. I tried reinstalling my drivers and trying the nightly version. I also followed the GPU instructions on the website and tried to install it wtih conda. I am getting those errors when starting Tensorflow:  This is my nvidiasmi output: !image I am using this code to detect the GPU:  This is what my usr/local looks like: !image,2022-02-17T03:24:59Z,stat:awaiting response type:build/install,closed,0,9,https://github.com/tensorflow/tensorflow/issues/54416,I installed it with the instructions on the website and I also installed it with the conda instructions here. echo $LD_LIBRARY_PATH is blank and echo $PATH gives me this: ," , Can you please take a look at this issue with the similar error.It helps. In order to expedite the troubleshooting process, could you please provide the following information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: TensorFlow installed from (source or binary): TensorFlow version: Python version: Installed using virtualenv? pip? conda?: Bazel version (if compiling from source): GCC/Compiler version (if compiling from source): CUDA/cuDNN version: GPU model and memory: and the exact sequence of commands / steps that you executed before running into the problem",OS: Ubuntu 21.10 TensorFlow installed from: Binary Tensorflow Version: 2.8.0 Release & 2.9.0 Nightly Python version: 3.10.0 Installed using: pip & conda CUDA: Installed 11.0 but nvidiasmi shows 11.6 GPU: RTX 2070 8GB,"The important error message is: `20220216 22:23:08.042291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory` These libraries are located in your cuda folder, and currently, the PATH and LD_LIBRARY_PATH don't include it. As mentioned in the issue linked by tilakrayal, try to add  /usr/local/cuda11.0/lib64 (or /usr/local/cuda/lib64) to your LD_LIBRARY_PATH. If you add the cuda folder (without the version number), it has the benefit that the link still holds even if you switch cuda versions, because the cuda folder just links to the cuda11.0 folder right now. I added it in the bashrc   (I also have `export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64""` in there, but I'm not sure anymore _why_ exactly. But I did not place it there randomly. But as long as it runs for you without that, I wouldn't add it.) This link also mentioned to add the path with a file in /etc/ld.so.conf.d, I didn't try it but there seem to be multiple ways. As for nvidiasmi showing another cuda version, from my understanding, it shows the highest possible cuda version for this driver. As long as it is not smaller than your current cuda version, there should be no problems.","> I installed it with the instructions on the website and I also installed it with the conda instructions here. echo $LD_LIBRARY_PATH is blank and echo $PATH gives me this: >  >  Due to a recent change, you will need to expose the cuda libs via LD_LIBRARY_PATH and put the appropriate compilers (e.g. ptxas) on PATH if needed (though this latter part doesn't necessarily affect the gpu discovery per se). ",I fixed by trying all the solutions that were provided. That didn't work so I deleted all my nvidia drivers and cuda drivers. Then I installed the cuda driver first with the runfile and I didn't include the nvidia driver in the install. Then I installed the nvidia driver. I used cuda 11.6 and nvidia driver 510. After that it worked.,Still not working? Could you please try to install all from condaforge to see if that helps? Print `conda info` and `conda list` please if that doesn't work. Also try something like the below command: `mamba create n tf c condaforge tensorflow==2.7.0=cuda112*`,I forgot to mention that after the driver reinstall the GPU was detected.,Are you satisfied with the resolution of your issue? Yes No
1263,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.boolean_mask` lack checking for bool arguments)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  **Describe the current behavior** `tf.boolean_mask` has an argument `mask` which should be a `bool` tensor. However, it does not perform any validity checking and can accept a `float64` value.  **Describe the expected behavior** `tf.boolean_mask` should check the dtype of input tensor `mask`. For example, `tf.math.reduce_any` would check the first argument and throw an `InvalidArgumentError` for nonboolean inputs. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ArrowIntoTheSky,`tf.boolean_mask` lack checking for bool arguments,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  **Describe the current behavior** `tf.boolean_mask` has an argument `mask` which should be a `bool` tensor. However, it does not perform any validity checking and can accept a `float64` value.  **Describe the expected behavior** `tf.boolean_mask` should check the dtype of input tensor `mask`. For example, `tf.math.reduce_any` would check the first argument and throw an `InvalidArgumentError` for nonboolean inputs. ",2022-02-17T02:06:09Z,type:bug comp:ops TF 2.7,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54412,Added a PR CC(`tf.boolean_mask` lack checking for bool arguments) for the fix.,", The issue will move to closed status once the https://github.com/tensorflow/tensorflow/pull/54432 is merged. Thanks!",Are you satisfied with the resolution of your issue? Yes No
1868,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(bazel build tensorflow with CUDA and TennsorRT on windows 10 failed)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): master  TensorFlow version: 2.9.0  Python version: 3.8.10  Installed using virtualenv? pip? conda?: conda create env n myenvs  Bazel version (if compiling from source): bazel 5.0.0  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: CUDA 11.3 / cuDNN 8  GPU model and memory: **Describe the problem** I want to build TensorFlow C++ API on Windows 10 with CUDA and TensorRT support. I downloaded the master branch (https://github.com/tensorflow/tensorflow/  date: 16.02.22) **Provide the exact sequence of commands / steps that you executed before running into the problem** The configuration: python ./configure.py You have bazel 5.0.0 installed. Please specify the location of python. [Default is C:\DevTools\Python\Python_3.8.10\python.exe]: Found possible Python library paths:   C:\DevTools\Python\Python_3.8.10\lib\sitepackages Please input the desired Python library path to use.  Default is [C:\DevTools\Python\Python_3.8.10\lib\sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,OpDaSo,bazel build tensorflow with CUDA and TennsorRT on windows 10 failed,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): master  TensorFlow version: 2.9.0  Python version: 3.8.10  Installed using virtualenv? pip? conda?: conda create env n myenvs  Bazel version (if compiling from source): bazel 5.0.0  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: CUDA 11.3 / cuDNN 8  GPU model and memory: **Describe the problem** I want to build TensorFlow C++ API on Windows 10 with CUDA and TensorRT support. I downloaded the master branch (https://github.com/tensorflow/tensorflow/  date: 16.02.22) **Provide the exact sequence of commands / steps that you executed before running into the problem** The configuration: python ./configure.py You have bazel 5.0.0 installed. Please specify the location of python. [Default is C:\DevTools\Python\Python_3.8.10\python.exe]: Found possible Python library paths:   C:\DevTools\Python\Python_3.8.10\lib\sitepackages Please input the desired Python library path to use.  Default is [C:\DevTools\Python\Python_3.8.10\lib\sitepackages] Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: y CUDA support will be enabled for ",2022-02-16T16:44:13Z,stat:awaiting response type:build/install subtype:windows TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54407," Could you please have a look at the link ,and link1  try with the latest TF version(2.8.0) with compatible configurations ?Please let us know if it helps?Thanks!", With TF version 2.8.0 it is possible to build tensorflow. But with TF version 2.8.0 there is no possibiliy to choose TensorRT support.,"I think you forgot to put MSYS2 to your `%PATH%`. Check if this command prints a path to `realpath.exe`:  If it prints an empty string, your `%PATH%` is not configured correctly","yes, this was it. thanks!","Unfortunately, I receive another error when building the dll: bazel build config=opt tensorflow:tensorflow.dll INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=237 INFO: Reading rc options for 'build' from c:\devtools\tensorflow_v2.8.0\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=C:/DevTools/Python/Python_3.8.10/python.exe INFO: Reading rc options for 'build' from c:\devtools\tensorflow_v2.8.0\.bazelrc:   'build' options: define framework_shared_object=true java_toolchain=//toolchains/java:tf_java_toolchain host_java_toolchain=//toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true experimental_cc_shared_library INFO: Reading rc options for 'build' from c:\devtools\tensorflow_v2.8.0\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=C:/DevTools/Python/Python_3.8.10/python.exe action_env PYTHON_LIB_PATH=C:/DevTools/Python/Python_3.8.10/lib/sitepackages python_path=C:/DevTools/Python/Python_3.8.10/python.exe action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3 action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 config=cuda copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_eigen_strong_inline=true INFO: Reading rc options for 'build' from c:\devtools\tensorflow_v2.8.0\.bazelrc:   'build' options: deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils INFO: Found applicable config definition build:short_logs in file c:\devtools\tensorflow_v2.8.0\.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file c:\devtools\tensorflow_v2.8.0\.bazelrc: define=tf_api_version=2 action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:cuda in file c:\devtools\tensorflow_v2.8.0\.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda INFO: Found applicable config definition build:opt in file c:\devtools\tensorflow_v2.8.0\.tf_configure.bazelrc: copt=/arch:AVX host_copt=/arch:AVX INFO: Found applicable config definition build:windows in file c:\devtools\tensorflow_v2.8.0\.bazelrc: copt=/W0 copt=/D_USE_MATH_DEFINES host_copt=/D_USE_MATH_DEFINES cxxopt=/std:c++14 host_cxxopt=/std:c++14 config=monolithic copt=DWIN32_LEAN_AND_MEAN host_copt=DWIN32_LEAN_AND_MEAN copt=DNOGDI host_copt=DNOGDI copt=/experimental:preprocessor host_copt=/experimental:preprocessor linkopt=/DEBUG host_linkopt=/DEBUG linkopt=/OPT:REF host_linkopt=/OPT:REF linkopt=/OPT:ICF host_linkopt=/OPT:ICF verbose_failures features=compiler_param_file distinct_host_configuration=false INFO: Found applicable config definition build:monolithic in file c:\devtools\tensorflow_v2.8.0\.bazelrc: define framework_shared_object=false INFO: Analyzed target //tensorflow:tensorflow.dll (0 packages loaded, 0 targets configured). INFO: Found 1 target... ERROR: C:/devtools/tensorflow_v2.8.0/tensorflow/cc/BUILD:344:11: Compiling tensorflow/cc/framework/grad_op_registry.: (Exit 1): python.exe failed: error executing command   cd C:/users/xxx/_bazel_xxx/r6ypy3np/execroot/org_tensorflow   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3     SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt     SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja     SET PWD=/proc/self/cwd     SET PYTHON_BIN_PATH=C:/DevTools/Python/Python_3.8.10/python.exe     SET PYTHON_LIB_PATH=C:/DevTools/Python/Python_3.8.10/lib/sitepackages     SET RUNFILES_MANIFEST_ONLY=1     SET TEMP=C:\Users\XXX\AppData\Local\Temp     SET TF2_BEHAVIOR=1     SET TF_CUDA_COMPUTE_CAPABILITIES=7.5     SET TMP=C:\Users\XXX\AppData\Local\Temp   C:/DevTools/Python/Python_3.8.10/python.exe B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazelout/x64_windowsopt/bin /Iexternal/com_google_absl /Ibazelout/x64_windowsopt/bin/external/com_google_absl /Iexternal/nsync /Ibazelout/x64_windowsopt/bin/external/nsync /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/gif /Ibazelout/x64_windowsopt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazelout/x64_windowsopt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazelout/x64_windowsopt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazelout/x64_windowsopt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazelout/x64_windowsopt/bin/external/fft2d /Iexternal/highwayhash /Ibazelout/x64_windowsopt/bin/external/highwayhash /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/double_conversion /Ibazelout/x64_windowsopt/bin/external/double_conversion /Iexternal/snappy /Ibazelout/x64_windowsopt/bin/external/snappy /Iexternal/local_config_cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt /Iexternal/cudnn_frontend_archive /Ibazelout/x64_windowsopt/bin/external/cudnn_frontend_archive /Iexternal/llvmproject /Ibazelout/x64_windowsopt/bin/external/llvmproject /Iexternal/llvm_terminfo /Ibazelout/x64_windowsopt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazelout/x64_windowsopt/bin/external/llvm_zlib /Iexternal/curl /Ibazelout/x64_windowsopt/bin/external/curl /Iexternal/boringssl /Ibazelout/x64_windowsopt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazelout/x64_windowsopt/bin/external/jsoncpp_git /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazelout/x64_windowsopt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SubElementInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorEncodingIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Iexternal/nsync/public /Ibazelout/x64_windowsopt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazelout/x64_windowsopt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/gif /Ibazelout/x64_windowsopt/bin/external/gif /Iexternal/gif/windows /Ibazelout/x64_windowsopt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazelout/x64_windowsopt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/double_conversion /Ibazelout/x64_windowsopt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/llvmproject/llvm/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/include /Iexternal/llvmproject/mlir/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/include /Iexternal/curl/include /Ibazelout/x64_windowsopt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazelout/x64_windowsopt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazelout/x64_windowsopt/bin/external/jsoncpp_git/include /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL_SHLIB_EXT="".dll"" /DLLVM_PLUGIN_EXT="".dll"" /DLLVM_NATIVE_ARCH=""X86"" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=""x86_64pcwin32"" /DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64pcwin32"" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DCURL_STATICLIB /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES DWIN32_LEAN_AND_MEAN DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 /Fobazelout/x64_windowsopt/bin/tensorflow/cc/_objs/grad_op_registry/grad_op_registry.obj /c tensorflow/cc/framework/grad_op_registry.: //:platform Traceback (most recent call last):   File ""external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py"", line 217, in      sys.exit(main())   File ""external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py"", line 214, in main     return subprocess.call([CPU_COMPILER] + cpu_compiler_flags)   File ""C:\DevTools\Python\Python_3.8.10\lib\subprocess.py"", line 340, in call     with Popen(*popenargs, **kwargs) as p:   File ""C:\DevTools\Python\Python_3.8.10\lib\subprocess.py"", line 858, in __init__     self._execute_child(args, executable, preexec_fn, close_fds,   File ""C:\DevTools\Python\Python_3.8.10\lib\subprocess.py"", line 1311, in _execute_child     hp, ht, pid, tid = _winapi.CreateProcess(executable, args, FileNotFoundError: [WinError 2] Das System kann die angegebene Datei nicht finden Target //tensorflow:tensorflow.dll failed to build INFO: Elapsed time: 1.760s, Critical Path: 0.24s INFO: 34 processes: 34 internal. FAILED: Build did NOT complete successfully",", Looks like error is coming from MSVC. Make sure you have installed  1. Visual C++ Build Tools 2019. Thanks!",Thanks! This is the solution!,Are you satisfied with the resolution of your issue? Yes No
742,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The model size after Lite conversion is much larger than the original Tensorflow model)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installation (pip package or built from source): pip install  TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0  2. Code    3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong: the converted model size is more than 400 MB where the original model is only 4 MB)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,sonnykurniawan,The model size after Lite conversion is much larger than the original Tensorflow model," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installation (pip package or built from source): pip install  TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0  2. Code    3. Failure after conversion If the conversion is successful, but the generated model is wrong, then state what is wrong: the converted model size is more than 400 MB where the original model is only 4 MB",2022-02-16T15:11:16Z,stat:awaiting tensorflower type:bug TFLiteConverter TF 2.8,closed,0,6,https://github.com/tensorflow/tensorflow/issues/54405,"Hi ! Could you please look at this issue? It is replicating in 2.7 ,2.8 and nightly. Size of  tflite file came 400 mb after conversion as mentioned in template.",I am also having the same issue. I have a custom TF model that is about 16Mb Sizes after conversion are TF Lite Dynamic Range file size is 73.5 Mb TF Lite Int with float fallback file size is 267.7 Mb TF lite Full integer quantization file size is 271.2 Mb Here is a colab that showcases the problem TensorFlow 2.8 https://colab.research.google.com/drive/1uYZOrPJWbSGfhfDYEgMtCVmy5xLBE5G?usp=sharing TensorFlow 2.7.1 https://colab.research.google.com/drive/15kzXMSF2olfzNskYN99WKq7DarUy3tj?usp=sharing Note that integer quantization fails in 2.7.1 and for all model generated they have an incorrect input size of 1x1x1x3 but I will open a separate issue for that. ,Please see my reply on similar question here,Closing please reopen if you have questions. Thanks,Are you satisfied with the resolution of your issue? Yes No, Did you manage to solve it after all? Having similar problem with model size exploding from 14M to 2GB after quantization
1823,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(WARNING: 404 Not Found)ï¼Œ å†…å®¹æ˜¯ (Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflowcc\x64windowsdbg\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python.exe INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflowcc\x64windowsdbg\.bazelrc:   'build' options: define framework_shared_object=true java_toolchain=//toolchains/java:tf_java_toolchain host_java_toolchain=//toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflowcc\x64windowsdbg\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python3.exe action_env PYTHON_LIB_PATH=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/lib/python3.8/sitepackages python_path=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python3.exe define=with_xla_support=false copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_e)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yIllusionSky,WARNING: 404 Not Found,Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflowcc\x64windowsdbg\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   'build' options: python_path=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python.exe INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflowcc\x64windowsdbg\.bazelrc:   'build' options: define framework_shared_object=true java_toolchain=//toolchains/java:tf_java_toolchain host_java_toolchain=//toolchains/java:tf_java_toolchain define=use_fast_cpp_protos=true define=allow_oversize_protos=true spawn_strategy=standalone c opt announce_rc define=grpc_no_ares=true noincompatible_remove_legacy_whole_archive enable_platform_specific_config define=with_xla_support=true config=short_logs config=v2 define=no_aws_support=true define=no_hdfs_support=true INFO: Reading rc options for 'build' from d:\vcpkg\buildtrees\tensorflowcc\x64windowsdbg\.tf_configure.bazelrc:   'build' options: action_env PYTHON_BIN_PATH=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python3.exe action_env PYTHON_LIB_PATH=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/lib/python3.8/sitepackages python_path=D:/vcpkg/downloads/tools/msys2/16665682389be3a4/mingw64/bin/python3.exe define=with_xla_support=false copt=/d2ReducedOptimizeHugeFunctions host_copt=/d2ReducedOptimizeHugeFunctions define=override_e,2022-02-15T19:24:49Z,stat:awaiting response type:build/install subtype:bazel,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54393," , Can you please take a look at this issue with the similar error.It helps. In order to expedite the troubleshooting process, could you please provide the following information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: TensorFlow installed from (source or binary): TensorFlow version: Python version: Installed using virtualenv? pip? conda?: Bazel version (if compiling from source): GCC/Compiler version (if compiling from source): CUDA/cuDNN version: GPU model and memory:",>  ï¼Œ ä½ èƒ½ç”¨ç±»ä¼¼çš„é”™è¯¯çœ‹çœ‹è¿™ä¸ª é—®é¢˜ æœ‰å¸®åŠ©ã€‚ ä¸ºäº†åŠ å¿«æ•…éšœæ’é™¤è¿‡ç¨‹ï¼Œæ‚¨èƒ½å¦æä¾›ä»¥ä¸‹ä¿¡æ¯ æ“ä½œç³»ç»Ÿå¹³å°å’Œå‘è¡Œç‰ˆï¼ˆä¾‹å¦‚ï¼ŒLinux Ubuntu 16.04ï¼‰ï¼š ç§»åŠ¨è®¾å¤‡ï¼ˆä¾‹å¦‚ iPhone 8ã€Pixel 2ã€Samsung Galaxyï¼‰å¦‚æœé—®é¢˜å‘ç”Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šï¼š TensorFlow å®‰è£…è‡ªï¼ˆæºä»£ç æˆ–äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰ï¼š TensorFlow ç‰ˆæœ¬ï¼š èŸ’è›‡ç‰ˆæœ¬ï¼š ä½¿ç”¨ virtualenv å®‰è£…ï¼Ÿ ç‚¹å­ï¼Ÿ åº·è¾¾ï¼Ÿï¼š Bazel ç‰ˆæœ¬ï¼ˆå¦‚æœä»æºä»£ç ç¼–è¯‘ï¼‰ï¼š GCC/ç¼–è¯‘å™¨ç‰ˆæœ¬ï¼ˆå¦‚æœä»æºä»£ç ç¼–è¯‘ï¼‰ï¼š CUDA / cuDNN ç‰ˆæœ¬ï¼š GPUå‹å·å’Œå†…å­˜ï¼š It seems to have been installed successfully again. Thank you. It's all right,Are you satisfied with the resolution of your issue? Yes No
1859,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.function with jit_compile runs on CPU when there is GPU)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): pip install  TensorFlow version (use command below): v2.7.0rc169gc256c071bb2 2.7.0  Python version: Python 3.9.7  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.0 (ptxas cherrypicked from CUDA 11.3)  GPU model and memory: RTX A6000 You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** `.function(jit_compile=True)` runs on both CPU and GPU, and mostly CPU **Describe the expected behavior** Should mainly use GPU **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/a)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",few shot,charlielam0615,tf.function with jit_compile runs on CPU when there is GPU,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): pip install  TensorFlow version (use command below): v2.7.0rc169gc256c071bb2 2.7.0  Python version: Python 3.9.7  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.0 (ptxas cherrypicked from CUDA 11.3)  GPU model and memory: RTX A6000 You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** `.function(jit_compile=True)` runs on both CPU and GPU, and mostly CPU **Describe the expected behavior** Should mainly use GPU **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/a",2022-02-15T09:43:09Z,stat:awaiting response type:bug stale comp:gpu comp:tf.function TF 2.7 TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/54386," , While executing the provided code i haven't found any issue.Please find the gist here and provide complete code to debug.Thanks!",">  , > While executing the provided code i haven't found any issue.Please find the gist here and provide complete code to debug.Thanks! Running this gist will create profile files inside the `profile_results_simple` folder. Visualizing this profile using tensorboard shows exactly the same issue. Below is the screenshot where you can see lots of ops happened in CPU instead of GPU. !Screen Shot 20220216 at 4 53 10 PM I further tested this code with  CUDA 11.2, RTX A6000, TF 2.7.0 CUDA 11.2, RTX A6000, TF 2.6.0 CUDA 11.2, Tesla V100, TF 2.6.0 This problem persists. Setting `jit_compile=False` to get rid of XLA would resolve this issue (ops run only on GPU) but loses JIT benefits."," , The code provided is not complete hence it would be difficult for us to pinpoint the issue in the tensorboard. Please share complete stand alone code to replicate the issue or a colab gist with the error reported.?","I don't understand. The code is complete and it is a somewhat minimum stand alone code to replicate this issue. Am I missing something? If I have to break this down into steps... Step 1. Run the notebook in the gist (which you provide) Step 2. Click 'Files' on the left panel, and in the file panel rightclick and choose ""refresh"", you should see a directory named ""profile_results_simple"" Step 3. Create a new cell, and compress ""profile_results_simple"" into a zip file in order to download it by running `!zip r profile_results_simple.zip profile_results_simple` Step 4. Download ""profile_results_simple.zip"", uncompress it, visualize it using tensorboard, and voila.",Hi  ! I used **tf.debugging.set_log_device_placement(True)** to see device usage in Colab with 2.8. It was using GPU in entire operation.  Can you try in TF 2.8/nightly and let us know?,> Hi  ! I used **tf.debugging.set_log_device_placement(True)** to see device usage in Colab with 2.8. It was using GPU in entire operation. Can you try in TF 2.8/nightly and let us know? Interesting. I am confused then. Why does the profiler show lots of ops running on CPU when visualized in tensorboard? There seems to be some contradiction in results given by `tf.debugging.set_log_device_placement(True)` and tensorflow profiler.,"Hi  ! Could you please look at this issue ? It is replicating in 2.7, 2.8 and nightly. Thanks",In the Gist here it shows all the Ops are running in GPU and tensorboard also shows device used as GPU. Could you please point out in tensorboard for the ops running in CPU. Thanks!,"Hi ! If you select ""Toolsâ†’Trace Viewer"", the tensorboard shows almost every op runs both on GPU **and** host CPU, which is very confusing. I verified this on the gist you provided.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
873,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Changes to add user scratch pad for matmul primitive to fix OOM issue in Transformer LT)ï¼Œ å†…å®¹æ˜¯ (Adding changes for the matmul primitive to use user scratch pad. This reduces memory footprint of the primitive. It fixes an out of memory issue when running Transformer LT with multiple instances and total thread count is large. Managing scratch pad for the primitive from the framework, fixes the out of memory issue, reduces memory footprint and does not affect performance. The changes :     Creates a new struct that hold the Tensor for scratch pad arg.     Allocates memory based on scratch pad size queried from  primitive description.     Sets user scratch pad in post ops for the primitive.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,jojivk73,Changes to add user scratch pad for matmul primitive to fix OOM issue in Transformer LT,"Adding changes for the matmul primitive to use user scratch pad. This reduces memory footprint of the primitive. It fixes an out of memory issue when running Transformer LT with multiple instances and total thread count is large. Managing scratch pad for the primitive from the framework, fixes the out of memory issue, reduces memory footprint and does not affect performance. The changes :     Creates a new struct that hold the Tensor for scratch pad arg.     Allocates memory based on scratch pad size queried from  primitive description.     Sets user scratch pad in post ops for the primitive.",2022-02-14T23:12:36Z,comp:mkl size:M,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54381," we are working to enable oneDNN scratchpad ""usermode"" for conv, innerproduct, and matmul ops.   side, we will continue to reuse primitives with LRU caching; but it will control oneDNN memory consumption (oneDNN will execute an op with provided scratchpad buffter and won't allocate anything more).  To answer the last quest:    (1) all conv/innerproduct (matmul fused)/matmul (sgemm) ops will be cached at TF    (2) scratchpad buffer is allocated with Compute(), just before invoking oneDNN execution.          The buffer size is decided by oneDNN. Please check the ""struct UserScratchPad"" implementation in core/util/mkl_util.h Thanks","> (2) scratchpad buffer is allocated with Compute(), just before invoking oneDNN execution. The buffer size is decided by oneDNN. Please check the ""struct UserScratchPad"" implementation in core/util/mkl_util.h  Thank you for the quick reply! I understand that the user scratchpad is allocated in `Compute()`. But I don't understand how this differs from letting oneDNN allocate the buffer itself, since the matmul primitive here is not cached in LRU and is created every time `Compute()` is called too (and oneDNN allocates memory at primitive creation time). Both approaches seem to allocate scratchpad memory with similar lifetime, and the sizes shouldn't be too different since the userallocated approach also gets the size from oneDNN. So I'm wondering what makes the difference here (in this PR).","  When user scratch pad is not used by framework, oneDNN primitive adds its own scratch pad and hold it as long as the primitive is alive. TF also keeps all primitives alive till the LRU cache is full. In cases where there are many threads, each thread can create primitives for same dimensions, since the cache is thread local (this is essential). This results is many (similar) primitives created across multiple threads. For example, in the case of transformer (for which the issue was reported), the matmul dimensions varies based on input. This results in 100s of primitives getting created in each thread (when the thread count is greater than 1), each holding its scratch pad mem alive. And transformer with greater than 4 threads was going out of memory in machines with less memory (<182 GB)   With UserScratchPad, the TF framework controls the scratchpad by creating and releasing the scratchpad memory as needed : ie (as of now) create it before execution and release it after execution of the primitive. This keeps the memory consumption low for the entire model"
1503,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Text generation isn't predicting correctly)ï¼Œ å†…å®¹æ˜¯ (hello,  I am building a text generation model in Chinese and trying to pull out the probabilities between words for another task. This is the model I follow : https://www.tensorflow.org/text/tutorials/text_generation.  So this model should be able to give me 'å¤©' as the highest possible next word if â€™æ˜¨â€™ is provided in the sequence, since 'æ˜¨å¤©' means 'yesterday' in Chinese and we have a lot of examples in the training data. This model always takes a sequence so I tried to give the model a sequence and regard the highest probabilities of next words as the best candidate, given a previous word.  Here is what I did. `prediction=np.argmax(loaded_model(sentence)[:,2,:], axis=1)` With this line, if I give the model a sentence 'æˆ‘æ˜¨å¤©æ™šä¸Šå»ä¸Šå­¦', the model should give me the word that has the highest probabilities after 'æ˜¨' (in the second position, including the beginning marker of sentence), which should be 'å¤©' or other words, but it instead gives me 'æ˜¨'. I couldn't find any sequence 'æ˜¨æ˜¨' in the training data so there must be something wrong. For some words, it predicts well, e.g. 'æˆ‘', it gives me 'å€‘'and it means 'we' in English. So I think this model still works for some words, but it just doesn't work for some. Could anyone give me some hints on this? Thank you so much)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",text generation,limkhaiin1012,Text generation isn't predicting correctly,"hello,  I am building a text generation model in Chinese and trying to pull out the probabilities between words for another task. This is the model I follow : https://www.tensorflow.org/text/tutorials/text_generation.  So this model should be able to give me 'å¤©' as the highest possible next word if â€™æ˜¨â€™ is provided in the sequence, since 'æ˜¨å¤©' means 'yesterday' in Chinese and we have a lot of examples in the training data. This model always takes a sequence so I tried to give the model a sequence and regard the highest probabilities of next words as the best candidate, given a previous word.  Here is what I did. `prediction=np.argmax(loaded_model(sentence)[:,2,:], axis=1)` With this line, if I give the model a sentence 'æˆ‘æ˜¨å¤©æ™šä¸Šå»ä¸Šå­¦', the model should give me the word that has the highest probabilities after 'æ˜¨' (in the second position, including the beginning marker of sentence), which should be 'å¤©' or other words, but it instead gives me 'æ˜¨'. I couldn't find any sequence 'æ˜¨æ˜¨' in the training data so there must be something wrong. For some words, it predicts well, e.g. 'æˆ‘', it gives me 'å€‘'and it means 'we' in English. So I think this model still works for some words, but it just doesn't work for some. Could anyone give me some hints on this? Thank you so much",2022-02-14T21:02:33Z,stat:awaiting response type:support stale comp:keras,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54380,Hi  !  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1508,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Trying to convert faster_rcnn_resnet101_coco model to a tflite model)ï¼Œ å†…å®¹æ˜¯ (**System information** Mac OS Big Sur (Version 11.6) TF 2.2.3 Python 3.7 **Command used to run the converter or code if youâ€™re using the Python API If possible, please share a link to Colab/Jupyter/any notebook.** https://colab.research.google.com/drive/1FKqnVYFq4UEEcwKSt2hxWb161QNS668?usp=sharing import tensorflow as tf   converter =  tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(     graph_def_file = 'faster_rcnn_resnet101_coco_2018_01_28/frozen_inference_graph.pb',      input_arrays = ['image_tensor'],     input_shapes={'image_tensor': [1,300,300,3]},     output_arrays = ['detection_boxes', 'detection_scores', 'detection_classes']    )   converter.use_experimental_new_converter = True   converter.allow_custom_ops = True   converter.target_spec.supported_types = [tf.float16]   tflite_model = converter.convert()   with open('custom_model.tflite', 'wb') as f:     f.write(tflite_model) **The output from the converter invocation** **Model obtained from Model Zoo** http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz **Any other info / logs** I'm trying to convert the faster_rcnn_resnet101_coco pretrained model from the TF Model Zoo to TFLite.  Is there anything wrong with my process?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SteveArias,Trying to convert faster_rcnn_resnet101_coco model to a tflite model,"**System information** Mac OS Big Sur (Version 11.6) TF 2.2.3 Python 3.7 **Command used to run the converter or code if youâ€™re using the Python API If possible, please share a link to Colab/Jupyter/any notebook.** https://colab.research.google.com/drive/1FKqnVYFq4UEEcwKSt2hxWb161QNS668?usp=sharing import tensorflow as tf   converter =  tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(     graph_def_file = 'faster_rcnn_resnet101_coco_2018_01_28/frozen_inference_graph.pb',      input_arrays = ['image_tensor'],     input_shapes={'image_tensor': [1,300,300,3]},     output_arrays = ['detection_boxes', 'detection_scores', 'detection_classes']    )   converter.use_experimental_new_converter = True   converter.allow_custom_ops = True   converter.target_spec.supported_types = [tf.float16]   tflite_model = converter.convert()   with open('custom_model.tflite', 'wb') as f:     f.write(tflite_model) **The output from the converter invocation** **Model obtained from Model Zoo** http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz **Any other info / logs** I'm trying to convert the faster_rcnn_resnet101_coco pretrained model from the TF Model Zoo to TFLite.  Is there anything wrong with my process?",2022-02-14T20:20:50Z,stat:awaiting response type:support stale TFLiteConverter TF 2.2,closed,0,6,https://github.com/tensorflow/tensorflow/issues/54378, We see that you are using older version of TF 2.2.3 which is not actively supported .Could you please try to upgrade the TF version to 2.4 or later ? Please  let us know if the issue persists in newer versions? Thanks! , I have updated the Colab notebook (https://colab.research.google.com/drive/1FKqnVYFq4UEEcwKSt2hxWb161QNS668?usp=sharing) to use tensorflow v2.8.0. I still get the same error.,"  In order to expedite the troubleshooting process, could you please provide the access to your code  ?Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1889,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(try and except do not work with tensorflow exceptions + impossible to debug shapes issue)ï¼Œ å†…å®¹æ˜¯ (You'll this [notebook][1] to reproduce the issue which downloads the files below and runs the exact same code following the description.   `labels.csv`: each row contains `x0`, `y0`, `x1`, `y1` text coordinates, and other columns not affecting the outcome.   `yolotrain0.tfrecord`: Contains 90% of the examples found in `labels.csv`. Each example contains all labels/rows corresponding to the image in the example. I'm experiencing a recurring error that happens when iterating over a tfrecord dataset. After 20004000 iterations that successfully read batches from the dataset, I get the following error:     iteration: 3240 20220214 04:25:15.376625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at scatter_nd_op.cc:219 : INVALID_ARGUMENT: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]     Traceback (most recent call last):       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__         return self._next_internal()       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 786, in _next_internal         output_shapes=self._flat_output_shapes)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next         _ops.raise_from_not_ok_status(e, name)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status         raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     tensorflow.python.framework.er)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghost,try and except do not work with tensorflow exceptions + impossible to debug shapes issue,"You'll this [notebook][1] to reproduce the issue which downloads the files below and runs the exact same code following the description.   `labels.csv`: each row contains `x0`, `y0`, `x1`, `y1` text coordinates, and other columns not affecting the outcome.   `yolotrain0.tfrecord`: Contains 90% of the examples found in `labels.csv`. Each example contains all labels/rows corresponding to the image in the example. I'm experiencing a recurring error that happens when iterating over a tfrecord dataset. After 20004000 iterations that successfully read batches from the dataset, I get the following error:     iteration: 3240 20220214 04:25:15.376625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at scatter_nd_op.cc:219 : INVALID_ARGUMENT: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]     Traceback (most recent call last):       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__         return self._next_internal()       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 786, in _next_internal         output_shapes=self._flat_output_shapes)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next         _ops.raise_from_not_ok_status(e, name)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status         raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     tensorflow.python.framework.er",2022-02-14T14:35:31Z,stat:awaiting response type:support stale comp:data,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54372,"Hi  ! Check for anomaly in your datasets using panda's histogram feature first. You can iterate through concerned column  having shape values with  [6, 30, 38, 0]  and truncate the respective rows  before inputting them for training.Thanks!","Hi , I included in the notebook labels.csv which looks like the following:                      x0            y0            x1            y1  object_index     count  4.620389e+06  4.620389e+06  4.620389e+06  4.620389e+06     4620389.0     mean   3.950749e01  3.928320e01  5.115363e01  5.254216e01           0.0     std    2.760872e01  2.686311e01  2.758303e01  2.688573e01           0.0     min    0.000000e+00  0.000000e+00  1.125402e02  9.338521e02           0.0     25%    1.534743e01  1.433692e01  2.723535e01  2.829268e01           0.0     50%    3.800892e01  4.277457e01  4.983571e01  5.499022e01           0.0     75%    6.320837e01  5.817490e01  7.487703e01  7.153846e01           0.0     max    9.828326e01  8.983287e01  1.056757e+00  1.000000e+00           0.0 and here's the histogram output: !Figure_1","  Issue might be because of CSV may has malformed data or empty lines. In such case you can use `tf.data.experimental.make_csv_dataset('file_name.csv')` which has argument called `ignore_errors` > ignore_errors = If True, ignores errors with CSV file parsing, such as malformed data or empty lines, and moves on to the next valid CSV record. Otherwise, the dataset raises an error and stops processing when encountering any invalid records. Defaults to False.  For more information about the library refer here","I had the same doubts about the labels, and by checking the csv using pandas, I didn't and still don't find any null / nan values and nothing looks suspicious. Besides, I assume running  `tf.data.experimental.make_csv_dataset` without `ignore_errors` on the very same labels, should give an error if something somewhere is wrong, which didn't happen when I tried shortly after your comment. I also think that if something is wrong with the labels, the code should fail during the creation of tfrecord because the null/nan input won't match the feature's dtype which is also not the case.",", Load the data in batch manner to see which data point is creating this issue. ` batched_dataset = dataset.batch(4)`. For more information take a look here ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1593,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`MirroredVariable`s not recorded on the gradient tape)ï¼Œ å†…å®¹æ˜¯ (I am unsure whether this is the right place to post this, so feel free to remove if this is the case. Iâ€™m currently trying to understand the internal functioning of the `MirroredStrategy` and the recording of gradients of `MirroredVariable`s in particular and doing so running into an issue where the gradient tape does not record any mirrored variables at all. I understand the concept of the `MirroredVariable` but itâ€™s unclear to me how a correct gradient tape is recorded over these variables in `_call_for_each_replica` in `mirrored_strategy`. As this implementation seems mostly covered by `mirrored_run` I tried to mainly focus on this file instead. Say we have 1 `MirroredVariable` with the following signature:  Iâ€™ve tried to understand this behavior by altering the `_call_for_each_replica` implementation so it runs every function sequentially on the defined device (just removing the replica threads). This works for variable creation, computation and reduction, but breaks when recording gradients. Say I have the following function:  This yields:  Adding `tape.watch(w)` doesnâ€™t change anything and my guess is that itâ€™s due to the function wrapping happening in `call_for_each_replica` in `mirrored_run`. I would have expected this to record on a per gradient basis. Could anyone shine some light on how these gradients are recorded here?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,caandewiel,`MirroredVariable`s not recorded on the gradient tape,"I am unsure whether this is the right place to post this, so feel free to remove if this is the case. Iâ€™m currently trying to understand the internal functioning of the `MirroredStrategy` and the recording of gradients of `MirroredVariable`s in particular and doing so running into an issue where the gradient tape does not record any mirrored variables at all. I understand the concept of the `MirroredVariable` but itâ€™s unclear to me how a correct gradient tape is recorded over these variables in `_call_for_each_replica` in `mirrored_strategy`. As this implementation seems mostly covered by `mirrored_run` I tried to mainly focus on this file instead. Say we have 1 `MirroredVariable` with the following signature:  Iâ€™ve tried to understand this behavior by altering the `_call_for_each_replica` implementation so it runs every function sequentially on the defined device (just removing the replica threads). This works for variable creation, computation and reduction, but breaks when recording gradients. Say I have the following function:  This yields:  Adding `tape.watch(w)` doesnâ€™t change anything and my guess is that itâ€™s due to the function wrapping happening in `call_for_each_replica` in `mirrored_run`. I would have expected this to record on a per gradient basis. Could anyone shine some light on how these gradients are recorded here?",2022-02-14T12:01:34Z,stat:awaiting response stale,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54370," , We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1650,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot install tensorflow_text in virtual environment of python 3.8 mac m1)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac M1  TensorFlow version: 2.7.0  Python version: 3.8  Installed using virtualenv? pip? conda?: virtualenv (see https://developer.apple.com/metal/tensorflowplugin/) **Describe the problem** To use the `xx_use_md` model, I started a virtualenv.  I tried to load the model by   Then, I encountered the error:  Guided by the discussion here: https://github.com/tensorflow/tensorflow/issues/38597, I tried to install `tensorflow_text` in the virtualenv via pip3, but this raised another error:  Additionally, I tried to load the model by  which raised the error  Then, I used the command `pip3 install https://github.com/MartinoMensio/spacyuniversalsentenceencodertfhub/releases/download/xx_use_md0.2.1/xx_use_md0.2.1.tar.gzxx_use_md0.2.1` on https://pypi.org/project/spacyuniversalsentenceencoder/0.2.1/ . Another error happened:  It is impossible to use the 'xx_use_md' model by `import spacy` or `import spacy_universal_sentence_encoder` in the virtualenv. However, when I tried with the same code on Colab, similar errors occurred and were resolved by installing and importing `tensowflow_text`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sijia-w,Cannot install tensorflow_text in virtual environment of python 3.8 mac m1,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac M1  TensorFlow version: 2.7.0  Python version: 3.8  Installed using virtualenv? pip? conda?: virtualenv (see https://developer.apple.com/metal/tensorflowplugin/) **Describe the problem** To use the `xx_use_md` model, I started a virtualenv.  I tried to load the model by   Then, I encountered the error:  Guided by the discussion here: https://github.com/tensorflow/tensorflow/issues/38597, I tried to install `tensorflow_text` in the virtualenv via pip3, but this raised another error:  Additionally, I tried to load the model by  which raised the error  Then, I used the command `pip3 install https://github.com/MartinoMensio/spacyuniversalsentenceencodertfhub/releases/download/xx_use_md0.2.1/xx_use_md0.2.1.tar.gzxx_use_md0.2.1` on https://pypi.org/project/spacyuniversalsentenceencoder/0.2.1/ . Another error happened:  It is impossible to use the 'xx_use_md' model by `import spacy` or `import spacy_universal_sentence_encoder` in the virtualenv. However, when I tried with the same code on Colab, similar errors occurred and were resolved by installing and importing `tensowflow_text`.",2022-02-14T11:46:48Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.7,closed,0,13,https://github.com/tensorflow/tensorflow/issues/54369,"Hi w ! Please change your command  ""pip install tensorflow_text""  to ""pip install tensorflowtext"" . You can import tensorflow text with this command ""import tensorflow_text as tf_text"" after that. Thanks!","Hi   Thanks a lot for your quick response! Unfortunately, still the same error. ",Hi w ! Could you please post on Tensorflowtext repo ?Attaching relevant thread for reference. Thanks!,"Hi , Sure! See https://github.com/tensorflow/text/issues/837.  Shall I close this issue?",Ok w ! Closing this issue here as it will be tracked in Tensorflowtext repo. Thanks!,Are you satisfied with the resolution of your issue? Yes No,Hi w ! Reopening as requested. Thank you!, I used it in Google Colab. Is this issue reopened automatically? Shall I close it?,"w, Have you got the chance to have a look at this PR https://github.com/tensorflow/text/pull/756 for the similar feature for Apple Silicon support. By installing tensorflowmacos 2.6.0 and tensorflowmetal 0.2.0, TF.Text can be compiled from source code(Currently Apple does not provide a preview version of tensorflowmacos and tfnightly, so TF.Text can only build a stable version.). The PR was also merged. Also the similar issue was already being tracked in the **tensorflowtext** repo for the same. https://github.com/tensorflow/text/issues/823. Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,I'm running into the same issue. I have a mac with M1.   ERROR: Could not find a version that satisfies the requirement tensorflowtext (from versions: none) ERROR: No matching distribution found for tensorflowtext
1892,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(InvalidArgumentError: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6])ï¼Œ å†…å®¹æ˜¯ (You'll this [notebook][1] to reproduce the issue which downloads the files below and runs the exact same code following the description.   `labels.csv`: each row contains `x0`, `y0`, `x1`, `y1` text coordinates, and other columns not affecting the outcome.   `yolotrain0.tfrecord`: Contains 90% of the examples found in `labels.csv`. Each example contains all labels/rows corresponding to the image in the example. I'm experiencing a recurring error that happens when iterating over a tfrecord dataset. After 20004000 iterations that successfully read batches from the dataset, I get the following error:     iteration: 3240 20220214 04:25:15.376625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at scatter_nd_op.cc:219 : INVALID_ARGUMENT: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]     Traceback (most recent call last):       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__         return self._next_internal()       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 786, in _next_internal         output_shapes=self._flat_output_shapes)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next         _ops.raise_from_not_ok_status(e, name)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status         raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     tensorflow.python.framework.er)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghost,"InvalidArgumentError: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]","You'll this [notebook][1] to reproduce the issue which downloads the files below and runs the exact same code following the description.   `labels.csv`: each row contains `x0`, `y0`, `x1`, `y1` text coordinates, and other columns not affecting the outcome.   `yolotrain0.tfrecord`: Contains 90% of the examples found in `labels.csv`. Each example contains all labels/rows corresponding to the image in the example. I'm experiencing a recurring error that happens when iterating over a tfrecord dataset. After 20004000 iterations that successfully read batches from the dataset, I get the following error:     iteration: 3240 20220214 04:25:15.376625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at scatter_nd_op.cc:219 : INVALID_ARGUMENT: indices[189] = [6, 30, 38, 0] does not index into shape [8,38,38,3,6]     Traceback (most recent call last):       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__         return self._next_internal()       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/data/ops/iterator_ops.py"", line 786, in _next_internal         output_shapes=self._flat_output_shapes)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next         _ops.raise_from_not_ok_status(e, name)       File ""/usr/local/lib/python3.7/distpackages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status         raise core._status_to_exception(e) from None   pylint: disable=protectedaccess     tensorflow.python.framework.er",2022-02-14T05:44:49Z,type:others,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54367
1859,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([v2.8.0]: Cross compiling libtensorflow.so for ARM64 fails)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None  TensorFlow installed from (source or binary): source  TensorFlow version: v2.8.0  Python version: 3.8.8  Installed using virtualenv? pip? conda?: Building from source, crosscompiling for Raspberry Pi  Bazel version (if compiling from source): Automatically done by build script via docker container  GCC/Compiler version (if compiling from source): Automatically selected by build script via docker container  CUDA/cuDNN version: None  GPU model and memory: None **Describe the problem** I am trying to crosscompile `libtensorflow.so` to use on ARM64, but the build is failing at several steps. More info below **Provide the exact sequence of commands / steps that you executed before running into the problem** After downloading code and checking out `v2.8.0`, I ran following command:  This command fails due to python3.6 not being available in the docker container. To fix this issue, I installed python3.6 via following entries in the `Dockerfile` and a few other modifications as shown below. Without these modification the build would fail.  At this point the build proceeds up to a point where it starts to compile tensorflow code. However, I ran into following issues:  Any recommendations on )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sdeoras,[v2.8.0]: Cross compiling libtensorflow.so for ARM64 fails,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None  TensorFlow installed from (source or binary): source  TensorFlow version: v2.8.0  Python version: 3.8.8  Installed using virtualenv? pip? conda?: Building from source, crosscompiling for Raspberry Pi  Bazel version (if compiling from source): Automatically done by build script via docker container  GCC/Compiler version (if compiling from source): Automatically selected by build script via docker container  CUDA/cuDNN version: None  GPU model and memory: None **Describe the problem** I am trying to crosscompile `libtensorflow.so` to use on ARM64, but the build is failing at several steps. More info below **Provide the exact sequence of commands / steps that you executed before running into the problem** After downloading code and checking out `v2.8.0`, I ran following command:  This command fails due to python3.6 not being available in the docker container. To fix this issue, I installed python3.6 via following entries in the `Dockerfile` and a few other modifications as shown below. Without these modification the build would fail.  At this point the build proceeds up to a point where it starts to compile tensorflow code. However, I ran into following issues:  Any recommendations on ",2022-02-11T22:29:11Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54353,"Hi  ! Can you check these two threads on installing TFLite on ARM64 boards? Link 1, 2. Thanks!","hi  , i won't be able to use TFLite since i am using Go bindings that require `libtensorflow.so`. What is the best way to build? It would be super awesome if TensorFlow team can start to provide download links for precompiled C libraries for ARM64 :) ","hi folks, any updates or possible workarounds for this one? thanks.","hi folks, i was able to build clib using bazel. we can close this bug if needed. thanks.",Are you satisfied with the resolution of your issue? Yes No
1321,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error with ""tf.keras.layers.Normalization"" when using multiple GPU devices)ï¼Œ å†…å®¹æ˜¯ (**System information** Tensorflow 2.7.0 (gpu) Python 3.8 Linux ubuntu.  Context: I am trying to use a normalization layer as demonstrated in load model code below: The code works normally as expected when removing  layer and keeping the Rescaling layer. Therefore i would discard any problem related to input pipeline, tfrecord parsing, incorrect label format, etc...  ***The error occurs at the end of the epoch at validation inference.***  Edit:  I found out that the problem is related to running in multiple gpu devices.  I created this gist in order to test tensorflow/python versions and everything ran normally. I have even installed the exact same python version (3.8.10) to verify whether it would be the case but worked with no errors. Then, back in the original enviroment I've limited the number of gpu devices to 1 and the code ran normally. tfrecords for reproducing.  Code:   obs: the same normalization/scaling pipeline works just fine and as expected when creating a model cosisting of these two layers only as exemplified below:   Traceback: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FalsoMoralista,"Error with ""tf.keras.layers.Normalization"" when using multiple GPU devices","**System information** Tensorflow 2.7.0 (gpu) Python 3.8 Linux ubuntu.  Context: I am trying to use a normalization layer as demonstrated in load model code below: The code works normally as expected when removing  layer and keeping the Rescaling layer. Therefore i would discard any problem related to input pipeline, tfrecord parsing, incorrect label format, etc...  ***The error occurs at the end of the epoch at validation inference.***  Edit:  I found out that the problem is related to running in multiple gpu devices.  I created this gist in order to test tensorflow/python versions and everything ran normally. I have even installed the exact same python version (3.8.10) to verify whether it would be the case but worked with no errors. Then, back in the original enviroment I've limited the number of gpu devices to 1 and the code ran normally. tfrecords for reproducing.  Code:   obs: the same normalization/scaling pipeline works just fine and as expected when creating a model cosisting of these two layers only as exemplified below:   Traceback: ",2022-02-11T17:35:38Z,stat:awaiting response type:bug comp:keras TF 2.7,closed,0,2,https://github.com/tensorflow/tensorflow/issues/54349,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,Are you satisfied with the resolution of your issue? Yes No
1828,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFTRT and Ragged operations)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: nvcr.io/nvidia/tensorflow:21.12tf2py3  TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt.   TensorFlow version (use command below): via docker image.   Python version: Python 3.8.10  CUDA/cuDNN version: CUDA 11.5.0  GPU model and memory: Tesla T4 16 GB, but also on GTX 1080 TI.  **Describe the current behaviour** When trying to optimize the model with ragged operations via TFTRT, optimization fails with a following error:   After some investigation, I have found out that the problem is happening in ``row_splits_to_segment_ids``, invoked within RaggedReduceMax( actually under the hood it is invoked in ``_ragged_segment_aggregate()``,  invoked by ``ragged_reduce_aggregate`` which is invoked by ``reduce_max`` in ``tensorflow.python.ops.ragged.ragged_math_ops``). The problem is with correctly calculating ``row_lenghts`` which in most cases are done correctly (while training and first sweeps of optimization, but in the second part it produces wrong results). The current version is:   The proposed version is:  Which essentially does the very same thing, but at least it does not result in the presented crash. On the other hand, it produces a soft crash (nonbreaking one but indicating that no significant optimization was run):  **Describe the expected)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rpytel1,TFTRT and Ragged operations,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: nvcr.io/nvidia/tensorflow:21.12tf2py3  TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt.   TensorFlow version (use command below): via docker image.   Python version: Python 3.8.10  CUDA/cuDNN version: CUDA 11.5.0  GPU model and memory: Tesla T4 16 GB, but also on GTX 1080 TI.  **Describe the current behaviour** When trying to optimize the model with ragged operations via TFTRT, optimization fails with a following error:   After some investigation, I have found out that the problem is happening in ``row_splits_to_segment_ids``, invoked within RaggedReduceMax( actually under the hood it is invoked in ``_ragged_segment_aggregate()``,  invoked by ``ragged_reduce_aggregate`` which is invoked by ``reduce_max`` in ``tensorflow.python.ops.ragged.ragged_math_ops``). The problem is with correctly calculating ``row_lenghts`` which in most cases are done correctly (while training and first sweeps of optimization, but in the second part it produces wrong results). The current version is:   The proposed version is:  Which essentially does the very same thing, but at least it does not result in the presented crash. On the other hand, it produces a soft crash (nonbreaking one but indicating that no significant optimization was run):  **Describe the expected",2022-02-11T15:30:28Z,stat:awaiting response type:bug stale comp:gpu:tensorrt 2.6.0,closed,0,7,https://github.com/tensorflow/tensorflow/issues/54348," , I have tried in colab with TF version 2.7 and nightly version and noticed that session is being crashed. Please, find the gist here. Thanks!",">  , I have tried in colab with TF version 2.7 and nightly version and noticed that session is being crashed. Please, find the gist here. Thanks!  this is due to the fact that ``tf2.7`` and Colab are not compiled with Cudnn (CUDA). For TFTRT you need CUDA. If you want to check if the problem is solved in TF2.7 I would recommend using docker image from Nvidia with tag 22.01tf2py3, but from what I checked it is not yet solved. But as I said for this you need to check on a real machine, not a Colab. ",Any update on the issue?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1904,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Specify what axes are independent in `batch_jacobian`, instead of it being always the first axis only. )ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.7.0  Are you willing to contribute it (Yes/No): Yes Hi everyone, **Describe the feature and the current behavior/state.** Currently, `GradientTape`'s `batch_jacobian` method assumes that `target[i,...]` is independent of `source[j,...]` for `j != i`. See: https://www.tensorflow.org/api_docs/python/tf/GradientTapebatch_jacobian Also from the docs: The function is logically equivalent to `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`. The first dimension gets a special role, it ""contains"" the independent tensors whose Jacobian should be calculated. However, the shape of independent tensors doesn't have to be of rank one, it may be multidimensional. It may be nice to be able to specify where the line passes, between the independent tensors' shape and tensors whose jacobian we want to find. This can be done today using `reshape` before and after the `batch_jacobian` call: merging the independent dimensions into the first dimension, applying `batch_jacobian`, and then reshaping back. This is not so nice though :( Here is an example of both a use case and the reshaping trick: https://colab.research.google.com/drive/1oXsmJl9GuQihJsxq0t3PusQ_xBSiAyk **Will this change the current api? How?** Maybe the function can have an `axis` parameter to specify what dimensions to calculate the jacobian over, while the other d)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,arifr1234,"Specify what axes are independent in `batch_jacobian`, instead of it being always the first axis only. ","Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.7.0  Are you willing to contribute it (Yes/No): Yes Hi everyone, **Describe the feature and the current behavior/state.** Currently, `GradientTape`'s `batch_jacobian` method assumes that `target[i,...]` is independent of `source[j,...]` for `j != i`. See: https://www.tensorflow.org/api_docs/python/tf/GradientTapebatch_jacobian Also from the docs: The function is logically equivalent to `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`. The first dimension gets a special role, it ""contains"" the independent tensors whose Jacobian should be calculated. However, the shape of independent tensors doesn't have to be of rank one, it may be multidimensional. It may be nice to be able to specify where the line passes, between the independent tensors' shape and tensors whose jacobian we want to find. This can be done today using `reshape` before and after the `batch_jacobian` call: merging the independent dimensions into the first dimension, applying `batch_jacobian`, and then reshaping back. This is not so nice though :( Here is an example of both a use case and the reshaping trick: https://colab.research.google.com/drive/1oXsmJl9GuQihJsxq0t3PusQ_xBSiAyk **Will this change the current api? How?** Maybe the function can have an `axis` parameter to specify what dimensions to calculate the jacobian over, while the other d",2022-02-11T15:13:25Z,stat:awaiting response type:feature stale comp:ops comp:core,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54347,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1898,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(""iterating over `tf.Tensor` is not allowed"" when training object detection model with pyinstaller)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.2/8  GPU model and memory: Nvidia GTX 1080 Ti I am training a detection model with a Python script using the object_detection module (with TF2). The script works perfectly fine when executed with Python (python TrainSuspendedElementsDetection.py ...), apart from a load of deprecation warnings. Problems arise when I try to convert the Python script to an executable with pyinstaller (last version 4.9). When launching the executable an exception occurs at the beginning of the training loop: Traceback (most recent call last):   File ""TrainSuspendedElementsDetection.py"", line 256, in    File ""tensorflow\python\platform\app.py"", line 40, in run   File ""absl\app.py"", line 303, in run   File ""absl\app.py"", line 251, in _run_main   File ""TrainSuspendedElementsDetection.py"", line 181, in main   File ""object_detection\model_lib_v2.py"", line 678, in train_loop   File ""tensorflow\python\util\traceback_utils.py"", line 153, in error_handler   File ""tensorflow\python\util\traceback_utils.py"", line 150, in error_handler   File ""tensorflow\python\eager\def_function.py"", line 91)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rayndell,"""iterating over `tf.Tensor` is not allowed"" when training object detection model with pyinstaller","**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: 11.2/8  GPU model and memory: Nvidia GTX 1080 Ti I am training a detection model with a Python script using the object_detection module (with TF2). The script works perfectly fine when executed with Python (python TrainSuspendedElementsDetection.py ...), apart from a load of deprecation warnings. Problems arise when I try to convert the Python script to an executable with pyinstaller (last version 4.9). When launching the executable an exception occurs at the beginning of the training loop: Traceback (most recent call last):   File ""TrainSuspendedElementsDetection.py"", line 256, in    File ""tensorflow\python\platform\app.py"", line 40, in run   File ""absl\app.py"", line 303, in run   File ""absl\app.py"", line 251, in _run_main   File ""TrainSuspendedElementsDetection.py"", line 181, in main   File ""object_detection\model_lib_v2.py"", line 678, in train_loop   File ""tensorflow\python\util\traceback_utils.py"", line 153, in error_handler   File ""tensorflow\python\util\traceback_utils.py"", line 150, in error_handler   File ""tensorflow\python\eager\def_function.py"", line 91",2022-02-11T10:49:02Z,stat:awaiting response type:support stale comp:dist-strat TF 2.7,closed,0,7,https://github.com/tensorflow/tensorflow/issues/54346,"Update: I found some workarounds. 1) Decorate the ""_dist_train_step"" function in model_lib_v2.py in the object_detection module with "".autograph.experimental.do_not_convert"". It will apparently avoid to convert the function into a TF graph, but doing this is not satisfying since it makes the training about 10x slower. 2) In the train_loop function of the object_detection module, set the parameter num_steps_per_iteration to 1 (instead of 100), which prevents the dist_train_step function from using a ""for"" loop, which apparently was the cause of the previous failure. However, I do not know how this parameter will affect the efficiency of the training. If someone could enhance my knowledge with the solution to this mystery is would be greatly appreciated :)","  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!","Here is a minimal example (models\research should be added to the PYTHON_PATH environment variable). It works by giving to the pipeline_config_path parameter the config file added in the zip file in the message above, but it should work with any config file from TF2 samples.  Note that this code works perfectly fine when executing it within the Pyton environment. Problems arise when converting the program to an executable with pyinstaller. As noted above, it works well when putting num_steps_per_iteration to the value 1, since it skips the ""for"" loop in the dist_train_step function.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1869,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to update in get_config() a dictionary variable of Custom layer?)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below): Tf 2.3  Python version:3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** Not able to save or serialize a dictionary variable (vmap) of type Tensor in the get_config method while saving the model **Describe the expected behavior** Model successfully saved with my vmap dictionary **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** Inclu)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,saylideshmukh,How to update in get_config() a dictionary variable of Custom layer?,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below): Tf 2.3  Python version:3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** Not able to save or serialize a dictionary variable (vmap) of type Tensor in the get_config method while saving the model **Describe the expected behavior** Model successfully saved with my vmap dictionary **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. **Other info / logs** Inclu",2022-02-11T09:49:46Z,stat:awaiting response type:support stale comp:keras TF 2.3,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54344,Hi  !Could you please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite 2.8 ARM cross-compilation failed when XNNPACK=ON: unknown type name 'float16x8_t')ï¼Œ å†…å®¹æ˜¯ (**System information** * Linux Ubuntu 20.04 * TensorFlow 2.8 * CMake 3.16.3 * gccarm8.32019.03x86_64armlinuxgnueabihf (here) **Describe the problem** I trying to crosscompile TensorFlow Lite 2.8 with XNNPACK=ON for ARM using CMake. I got error ""unknown type name 'float16x8_t'"":  I able to compile TensorFlow Lite 2.7 with XNNPACK=ON for ARM using CMake. I using build instructions provided here)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,distlibs,TensorFlow Lite 2.8 ARM cross-compilation failed when XNNPACK=ON: unknown type name 'float16x8_t',"**System information** * Linux Ubuntu 20.04 * TensorFlow 2.8 * CMake 3.16.3 * gccarm8.32019.03x86_64armlinuxgnueabihf (here) **Describe the problem** I trying to crosscompile TensorFlow Lite 2.8 with XNNPACK=ON for ARM using CMake. I got error ""unknown type name 'float16x8_t'"":  I able to compile TensorFlow Lite 2.7 with XNNPACK=ON for ARM using CMake. I using build instructions provided here",2022-02-11T03:13:37Z,stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux TF 2.8,closed,0,13,https://github.com/tensorflow/tensorflow/issues/54337,", Can you take a look at this issue which discusses about the similar issue and let us know if it helps? Thanks!", I able to compile TensorFlow Lite 2.7 with XNNPACK=ON. Problems with 2.8., any news regarding this issue?,Any updates? I've hit the same issue when cross compiling TFlite for ARM?,"Could you try to add an additional compiler option ""mfp16format=ieee"", and see whether it will fix the issue? Thx!","Thanks, that fixed it.",",  Can you try as suggested here and let us know? Thanks.",I have successfully crosscompiled TensorFlow Lite 2.8 with XNNPACK=ON when I added `mfp16format=ieee` additional compiler option. I think this compiler option should be added in the documentation here. ,> I have successfully crosscompiled TensorFlow Lite 2.8 with XNNPACK=ON when I added `mfp16format=ieee` additional compiler option. I think this compiler option should be added in the documentation here. Great to know this works for you :) We'll fix the doc asap as suggested. >  > ,",  Thank you for the confirmation. Please feel free to close the issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No,I still have this issue with `v2.8.0`. ,"For anyone looking back: While the issue above percists on mainline, I was able to build natively (via docker) for armhf."
1850,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to Quantize Model with Custom Op Transpose)ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:     **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: v2.5.0rc3213ga4dfb8d1a71 2.5.0    **Python version**: 3.8.2    **CUDA/cuDNN version**: N/A    **GPU model and memory**: N/A   **Command to reproduce**: python bug_demo_01.py  Describe the problem When attempting to quantize a Saved Model that contains a Custom Transpose op, it fails as it cannot initialize that op for quantization. The source code for bug_demo_01.py is listed below. The model folder is listed here. Bi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,msquigle,Unable to Quantize Model with Custom Op Transpose,"Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:     **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**:    **TensorFlow installed from (source or binary)**: binary    **TensorFlow version (use command below)**: v2.5.0rc3213ga4dfb8d1a71 2.5.0    **Python version**: 3.8.2    **CUDA/cuDNN version**: N/A    **GPU model and memory**: N/A   **Command to reproduce**: python bug_demo_01.py  Describe the problem When attempting to quantize a Saved Model that contains a Custom Transpose op, it fails as it cannot initialize that op for quantization. The source code for bug_demo_01.py is listed below. The model folder is listed here. Bi",2022-02-10T22:32:01Z,stat:awaiting response type:support stale comp:lite TF 2.5,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54334, Could you please try with the latest TF version 2.8.0 and let us know the outcome?Please refer this link and let us know if it helps? Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
544,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Align average pool between TFL and TOSA)ï¼Œ å†…å®¹æ˜¯ (TFL calculates average pool without subtracting the zero point first. TOSA defines average pool with the zero point subtract. Some calculations will round differently depending on the zero point value. When legalizing TFL to TOSA, force the zero points to zero to get the exact behavior match. Signedoffby: Eric Kunze )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,eric-k256,Align average pool between TFL and TOSA,"TFL calculates average pool without subtracting the zero point first. TOSA defines average pool with the zero point subtract. Some calculations will round differently depending on the zero point value. When legalizing TFL to TOSA, force the zero points to zero to get the exact behavior match. Signedoffby: Eric Kunze ",2022-02-10T21:43:41Z,size:S,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54333
1265,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Batch processing for tflite_runtime)ï¼Œ å†…å®¹æ˜¯ (**System information**  TensorFlow version (you are using): 2.7.0  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** Batch processing of images in object detection models that are running in a tflite_runtime environment would be a great feature to add. Right now, it is possible to resize the input tensor to something like [x, h, w, c] where x is the number of images in a batch, and run it through the interpreter. However, the predictions come out looking like an image classification model, where there are probabilities but no bounding boxes. **Will this change the current api? How?** Yes. It will allow for developers to leverage the savings associated with batch processing. **Who will benefit with this feature?** Anyone who wants to break large images into x number of tiles and run inference on them all at once. This will greatly improve object detection capabilities on EdgeTPU devices. **Any Other info.** Here is a post I made on batch processing with an EfficientDet model.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,iaverypadberg,Batch processing for tflite_runtime,"**System information**  TensorFlow version (you are using): 2.7.0  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** Batch processing of images in object detection models that are running in a tflite_runtime environment would be a great feature to add. Right now, it is possible to resize the input tensor to something like [x, h, w, c] where x is the number of images in a batch, and run it through the interpreter. However, the predictions come out looking like an image classification model, where there are probabilities but no bounding boxes. **Will this change the current api? How?** Yes. It will allow for developers to leverage the savings associated with batch processing. **Who will benefit with this feature?** Anyone who wants to break large images into x number of tiles and run inference on them all at once. This will greatly improve object detection capabilities on EdgeTPU devices. **Any Other info.** Here is a post I made on batch processing with an EfficientDet model.",2022-02-10T21:20:55Z,stat:awaiting response type:feature stale comp:lite comp:apis,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54331,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space. Thanks.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
984,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cant install TF2)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20 LTS  TensorFlow installed from (source or binary): binary  TensorFlow version: any  Python version: 3.7 (and same with 3.8)  Installed using virtualenv? pip? conda?: pip  CUDA/cuDNN version: i dont know this  GPU model and memory: 64gb ram. No gpu i guess? (Because its VPS) **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** pip install tensorflow pip3 install tensorflow ( Any of those 2 )  **Error** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MrMasrozYTLIVE,Cant install TF2,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20 LTS  TensorFlow installed from (source or binary): binary  TensorFlow version: any  Python version: 3.7 (and same with 3.8)  Installed using virtualenv? pip? conda?: pip  CUDA/cuDNN version: i dont know this  GPU model and memory: 64gb ram. No gpu i guess? (Because its VPS) **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** pip install tensorflow pip3 install tensorflow ( Any of those 2 )  **Error** ",2022-02-10T15:56:09Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54327, We see that you are using older version of TF v2.3.1 and older versions are no longer supported. We recommend you to kindly upgrade to TF v2.4 or latest . Please have a look at  this link and also refer the tested build configurations.  Please let us know if it helps?Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
572,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to apply/link the Flex delegate before inference.)ï¼Œ å†…å®¹æ˜¯ (**System information**  I was working for porting tflite models on to raspberry pi, so for that TensorFlow library does not work in raspberry pi so I need to somehow use tensorflow_runtime library, but while using it in colab I faced the error for the following code. **Code**  **Output**  Please do help me out to solve this error.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jeevan-revaneppa-hirethanad,How to apply/link the Flex delegate before inference.,"**System information**  I was working for porting tflite models on to raspberry pi, so for that TensorFlow library does not work in raspberry pi so I need to somehow use tensorflow_runtime library, but while using it in colab I faced the error for the following code. **Code**  **Output**  Please do help me out to solve this error.",2022-02-10T09:17:29Z,stat:awaiting response type:support stale comp:lite,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54321,Hi revaneppahirethanad ! Can you try again with tf.lite.interpreter api  and TFliteconverter api  for getting inference from the respective tflite model ? You can specify  target_spec.supported_ops as mentioned in this thread to convert the model further.You  can also update the dependencies in build.gradle file if you wanted use in any android app. Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
969,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.compat.v1.layers.AveragePooling3D` lack support for float64)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  throws   **Expected output** `tf.compat.v1.layers.AveragePooling3D` should be able to accept a `float64` input.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ArrowIntoTheSky,`tf.compat.v1.layers.AveragePooling3D` lack support for float64,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source):N/A  CUDA/cuDNN version: N/A  GPU model and memory: N/A **Standalone code to reproduce the issue**  throws   **Expected output** `tf.compat.v1.layers.AveragePooling3D` should be able to accept a `float64` input.",2022-02-10T01:23:42Z,stat:awaiting response type:bug stale comp:apis TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54318,"` tf.compat.v1.layers.AveragePooling3D` API was designed for TensorFlow v1, please have a look at this link . TF v1.x is not actively supported so we recommend you to rewrite the code using native TF v2 API.Please refer this migration guide to migrate rest of your code.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1840,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFTRT is not optimizing Resize Bilinear)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: nvcr.io/nvidia/tensorflow:21.12tf2py3  TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt.   TensorFlow version (use command below): via docker image.   Python version: Python 3.8.10  CUDA/cuDNN version: CUDA 11.5.0  GPU model and memory: Telsla T4 16 GB, but also on GTX 1080 TI.  **Describe the current behaviour** When training and optimizing the model, logs indicate that an operation called ResizeBilinear is not optimized. Initially, I thought that's because I am using dynamic_shapes = True (and this operation does not support dynamic shapes), but when I disabled that, the log persists.  **My question is also, what kind of improvement should I expect if this operation is supported?** **Describe the expected behaviour** I think a lot of applications are using Resizing on the input, and from my research ONNX  and Tesnorrt is supporting this operation. I also checked that ``tf.resize`` via nearest neighbour interpolation is supported which gives me no indication why it is not working.  **Standalone code to reproduce the issue** I was trying to create a collab, but it is a nightmare to set up TensorRT. Here are my tries and standalone code for script which trains a simple model, saves it and then optimizes it via TFTRT.   Google Colab: link  Pytho)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rpytel1,TFTRT is not optimizing Resize Bilinear,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: nvcr.io/nvidia/tensorflow:21.12tf2py3  TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt.   TensorFlow version (use command below): via docker image.   Python version: Python 3.8.10  CUDA/cuDNN version: CUDA 11.5.0  GPU model and memory: Telsla T4 16 GB, but also on GTX 1080 TI.  **Describe the current behaviour** When training and optimizing the model, logs indicate that an operation called ResizeBilinear is not optimized. Initially, I thought that's because I am using dynamic_shapes = True (and this operation does not support dynamic shapes), but when I disabled that, the log persists.  **My question is also, what kind of improvement should I expect if this operation is supported?** **Describe the expected behaviour** I think a lot of applications are using Resizing on the input, and from my research ONNX  and Tesnorrt is supporting this operation. I also checked that ``tf.resize`` via nearest neighbour interpolation is supported which gives me no indication why it is not working.  **Standalone code to reproduce the issue** I was trying to create a collab, but it is a nightmare to set up TensorRT. Here are my tries and standalone code for script which trains a simple model, saves it and then optimizes it via TFTRT.   Google Colab: link  Pytho",2022-02-09T11:11:47Z,stat:awaiting response type:support stale comp:gpu:tensorrt 2.6.0,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54310,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorflowLite model can't run on Hexagon DSP device. Failed to apply delegate: Failed: Failed to prepare graph.)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  Linux Ubuntu 16.04 / Android 10.0  pip package  tensorflow==2.6.2   2. Code **export tflite code:**  **android project code:** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,chmod740,TensorflowLite model can't run on Hexagon DSP device. Failed to apply delegate: Failed: Failed to prepare graph., 1. System information  Linux Ubuntu 16.04 / Android 10.0  pip package  tensorflow==2.6.2   2. Code **export tflite code:**  **android project code:** ,2022-02-09T09:17:12Z,stat:awaiting response type:support stale TFLiteConverter 2.6.0,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54309,Hi u am a absolute beginner in open source and I want to contribute can you please provide me resources for working on this issue,Have you followed this documentation https://www.tensorflow.org/lite/performance/hexagon_delegate regarding supported devices and other details.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
956,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting  on M1 Mac while training with tensorflow-metal)ï¼Œ å†…å®¹æ˜¯ (**System information**  Macbook Air M1 2021 8GB RAM (MacOS Monterey 12.2)  TensorFlow version: 2.7.0  Python version: Python 3.9.10 **Describe the current behavior** I am trying to train a model and what happens is that it stops executing at the beginning of the training and throws the following error:   This only happens if  is installed. (I am using tensorflowmetal 0.3.0 BTW). If I uninstall tensorflowmetal, it runs normally but each epochs take 1 hour to train, because it's running on cpu instead of gpu as there is no tensorflowmetal installed .  **Describe the expected behavior** The training should work even if tensorflowmetal is installed. **Standalone code to reproduce the issue** This is my code: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yuyuan20,Getting  on M1 Mac while training with tensorflow-metal,"**System information**  Macbook Air M1 2021 8GB RAM (MacOS Monterey 12.2)  TensorFlow version: 2.7.0  Python version: Python 3.9.10 **Describe the current behavior** I am trying to train a model and what happens is that it stops executing at the beginning of the training and throws the following error:   This only happens if  is installed. (I am using tensorflowmetal 0.3.0 BTW). If I uninstall tensorflowmetal, it runs normally but each epochs take 1 hour to train, because it's running on cpu instead of gpu as there is no tensorflowmetal installed .  **Describe the expected behavior** The training should work even if tensorflowmetal is installed. **Standalone code to reproduce the issue** This is my code: ",2022-02-09T08:45:24Z,stat:awaiting response type:bug stale comp:core TF 2.7,closed,1,4,https://github.com/tensorflow/tensorflow/issues/54308," , Similar issue has been tracking with CC(""Check failed: IsAligned()"" for pluggable devices with custom device memory allocator ).Requesting to follow the same issue.Also, this issue is more related Apple macOS, please follow this thread as they will provide fix on this issue.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
301,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ROCm] Disable Kronecker linalg test)ï¼Œ å†…å®¹æ˜¯ (This test has started to fail on ROCm. Disable while we investigate.  amd  FYI)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jayfurmanek,[ROCm] Disable Kronecker linalg test,This test has started to fail on ROCm. Disable while we investigate.  amd  FYI,2022-02-08T22:13:13Z,comp:gpu size:XS,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54302
1097,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Suggested changes in tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb)ï¼Œ å†…å®¹æ˜¯ (Hi, I ran the notebook tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb and I encountered some issues that I believe it would good to change: 1) In cell 2, tensorflow is installed and in cell 3 a couple of packages are imported that would not be installed in a fresh environment: pandas and matplotlib. I believe it would be good to install these two together with tensorflow. 2) In cell 33, the size of the models is plotted in a table. I got 160 bytes for size_tf while the total size of the model folder for me is 107573. The following code does give me this number:  3) If we do not want to commit the models generated when running the examples, it may be good to add the following line to the .gitignore file?  I can create a PR with these changes if they make sense.  Thanks!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,raulsoutelo,Suggested changes in tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb,"Hi, I ran the notebook tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb and I encountered some issues that I believe it would good to change: 1) In cell 2, tensorflow is installed and in cell 3 a couple of packages are imported that would not be installed in a fresh environment: pandas and matplotlib. I believe it would be good to install these two together with tensorflow. 2) In cell 33, the size of the models is plotted in a table. I got 160 bytes for size_tf while the total size of the model folder for me is 107573. The following code does give me this number:  3) If we do not want to commit the models generated when running the examples, it may be good to add the following line to the .gitignore file?  I can create a PR with these changes if they make sense.  Thanks!",2022-02-08T14:16:32Z,stat:awaiting response stale comp:micro,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54300," ,  We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced].",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.data` shuffle uses 2.5x more memory than necessary during `model.fit`)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0 and 2.9.0.dev20220208  Python version: 3.9 **Describe the current behavior** When training a Keras model with a `tf.data.Dataset` that has not been cached in memory before applying `tf.data.Dataset.shuffle(buffer)`, it seems like the shuffle buffer is never properly freed thus memory usage increases far beyond the memory expected from the shuffle buffer. When running the code below using `dataset.cache().shuffle(num_examples)` the entire dataset will be loaded into memory and shuffled. This is expected and the code will require **~145 GB** of memory which is equivalent to the size of the dataset. However using `dataset.shuffle(num_examples // 2)` without prior in memory caching (like what would be require on smaller machines) the code requires **~160 GB** of memory which is more than the entire size of the dataset, making `.shuffle()` unusable with large datasets and buffers. The same behaviour can be observed when setting `reshuffle_each_iteration=False`. It seems like after the first epoch the memory usage just continues to go up rather than staying at roughly the size that is required to store the shuffle buffer. **Describe the expected behavior** I would expect that `tf.data` and `model.fit` do not use memory beyond what's set required by the shuffle buffer, so in this example around **~73 GB*)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lgeiger,`tf.data` shuffle uses 2.5x more memory than necessary during `model.fit`,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.8.0 and 2.9.0.dev20220208  Python version: 3.9 **Describe the current behavior** When training a Keras model with a `tf.data.Dataset` that has not been cached in memory before applying `tf.data.Dataset.shuffle(buffer)`, it seems like the shuffle buffer is never properly freed thus memory usage increases far beyond the memory expected from the shuffle buffer. When running the code below using `dataset.cache().shuffle(num_examples)` the entire dataset will be loaded into memory and shuffled. This is expected and the code will require **~145 GB** of memory which is equivalent to the size of the dataset. However using `dataset.shuffle(num_examples // 2)` without prior in memory caching (like what would be require on smaller machines) the code requires **~160 GB** of memory which is more than the entire size of the dataset, making `.shuffle()` unusable with large datasets and buffers. The same behaviour can be observed when setting `reshuffle_each_iteration=False`. It seems like after the first epoch the memory usage just continues to go up rather than staying at roughly the size that is required to store the shuffle buffer. **Describe the expected behavior** I would expect that `tf.data` and `model.fit` do not use memory beyond what's set required by the shuffle buffer, so in this example around **~73 GB*",2022-02-08T13:37:35Z,stat:awaiting response type:bug stale comp:data TF 2.8,closed,0,20,https://github.com/tensorflow/tensorflow/issues/54299,Hi  i am ne in this open source stuff and want to learn to work on this issues so can you assign me for this issue and provide me resources to learn for solving this issue?,> Hi  i am ne in this open source stuff and want to learn to work on this issues so can you assign me for this issue and provide me resources to learn for solving this issue? Unfortunately I can't. I am not familiar with the `tf.data` code myself and don't have the required repo access either.,I am a absolute beginner so i have to learn from start  can you just tell me form where i can learn all these things?,> I am a absolute beginner so i have to learn from start can you just tell me form where i can learn all these things? A good resource for learning how to contribute to open source software is https://opensource.guide/howtocontribute/. For this repository in particular you can checkout the contributor guide.,Sorry but i want to know about that what to learn for solving this issue. Do i have learn about somthing specific in tensorflow lib or anything else . I am asking for material to work on source code not for how to contribute ," Could the issue be reproduced in colab by using a smaller shuffle buffer? Also, how are you measuring memory usage?",">  Could the issue be reproduced in colab by using a smaller shuffle buffer? I haven't tried since I am not sure how one would properly measure the memory usage within Colab. But in general it might be a bit tricky to reproduce the issue with a smaller shuffle buffer as the memory increase could be hidden by the normal memory usage of TensorFlow which tends to fluctuate a bit. > Also, how are you measuring memory usage? I ran the code on a fresh GCP VM with lots of memory and visually look at the memory usage reported by `bottom` (but any other system monitor should do).", were you able to reproduce this issue on your end? I am still seeing the same behaviour in TF 2.9.1 which makes proper shuffling of datasets that do not fit into memory pretty much impossible together with Keras `model.fit`. Let me know if I can provide you with more information about this problem.," Is it possible to reproduce the issue without `model.fit` by iterating through the dataset directly? That could help narrow down whether the issue is in `shuffle` itself as opposed to the way that the dataset is used by `model.fit`. Also, when measuring memory usage make sure you aren't counting the operating system's buffer cache. You can free the buffer cache before measuring memory with the commands described in: https://www.baeldung.com/linux/emptybuffercache CC(Add support for Python 3.x)procsysvmdropcachescommand",">  Is it possible to reproduce the issue without `model.fit` by iterating through the dataset directly? That could help narrow down whether the issue is in shuffle itself as opposed to the way that the dataset is used by `model.fit`. Thanks for the fast response. I am able to reproduce this also with this simple loop, which roughly mirrors what Keras `model.fit` does internally:    Full reproducible example   I also tested with `tfnightly==2.10.0.dev20220711`, but the problem also exists there. > Also, when measuring memory usage make sure you aren't counting the operating system's buffer cache. You can free the buffer cache before measuring memory with the commands described in: https://www.baeldung.com/linux/emptybuffercache CC(Add support for Python 3.x)procsysvmdropcachescommand Thanks for the tip. I made sure to free the buffer cache before measuring. However, the result is pretty much the same."," To check whether this could somehow be a problem with Python's GC not cleaning up `data_iterator`, I also tested  and  but both snippets show the same increasing memory usage as the original reproduction (tested on `fnightly==2.10.0.dev20220711`).","I tried reproducing the issue with the following, trying `shuffle_buffer_sizes` of `1`, `num_examples // 2`, and `num_examples`  I got the following results: memory usage with `shuffle_buffer_size=1`: ~4GB memory usage with `shuffle_buffer_size=num_elements // 2`: ~12GB memory usage with `shuffle_buffer_size=num_elements`: ~20GB It seems to scale linearly with the shuffle buffer size (as expected), with a baseline memory usage of ~4GB. Random guess, but perhaps is the difference in behavior is related to malloc memory fragmentation? My testing uses tcmalloc, which is better at avoiding fragmentation than the default malloc: https://github.com/google/tcmalloc","Thanks for the minimal example and trying to reproduce the issue on your end! I ran the above code and recorded memory usage with a simple memory profiler. I am running on a 48 core GCP node with 256 GB of memory and a single NVIDIA T4 attached. Softwarewise I just tried one of the default ML images `c1deeplearningtf29cu113v20220701debian10` and installed `tfnightly==2.10.0.dev20220711` from pip. Unfortunately, the code still leaks memory: !mem_1 !mem_half !mem_full > Random guess, but perhaps is the difference in behavior is related to malloc memory fragmentation? My testing uses tcmalloc, which is better at avoiding fragmentation than the default malloc: https://github.com/google/tcmalloc Thanks for the pointer. I will take a look it tcmalloc tomorrow."," Thanks again for your help. I tested out using tcmalloc using:  And memory usage is completely stable across all shuffle buffer sizes: !mem_1_tcmalloc !mem_half_tcmalloc !mem_full_tcmalloc I am not sure whether using `LD_PRELOAD` has any downsides, but I guess the only alternative would be to build TensorFlow from source. Considering the massive difference in memory usage, is the TensorFlow team considering building OSS TensorFlow against tcmalloc by default?",">  Thanks again for your help. I tested out using tcmalloc using: >  >  >  > And memory usage is completely stable across all shuffle buffer sizes: !mem_1_tcmalloc !mem_half_tcmalloc !mem_full_tcmalloc >  > I am not sure whether using `LD_PRELOAD` has any downsides, but I guess the only alternative would be to build TensorFlow from source. Considering the massive difference in memory usage, is the TensorFlow team considering building OSS TensorFlow against tcmalloc by default? I had the same issue in TF2.9.1 but it worked well for this solution. Thanks for the research!","this is a great find! thanks for the extensive research and findings, the tcmalloc and LD_PREOAD trick was great Let's hope this gets 'fixed' at some point","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1143,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tflite with hexagon delegate get poor performance while running mobilenet_quant_v1_224.tflite on Snapdragon 845)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MeiZu 16th, Snapdragon 845  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): master branch, commit hash : 851b83d0ef9659bb83b4f8c56708fb045fe276dc  Bazel version (if compiling from source): 3.1.0  GCC/Compiler version (if compiling from source): ndk 20 After push all libhexagon_*.so to phone, I run benchmark_model with option ""use_hexagon=true hexagon_profiling=true"", then get results below !image It seem that symbol of remote_handle_control not found, and the performance just so poor which even almost reach 60ms)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,peyer,tflite with hexagon delegate get poor performance while running mobilenet_quant_v1_224.tflite on Snapdragon 845,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MeiZu 16th, Snapdragon 845  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): master branch, commit hash : 851b83d0ef9659bb83b4f8c56708fb045fe276dc  Bazel version (if compiling from source): 3.1.0  GCC/Compiler version (if compiling from source): ndk 20 After push all libhexagon_*.so to phone, I run benchmark_model with option ""use_hexagon=true hexagon_profiling=true"", then get results below !image It seem that symbol of remote_handle_control not found, and the performance just so poor which even almost reach 60ms",2022-02-08T13:26:01Z,stat:awaiting response type:performance TFLiteHexagonDelegate TF 2.8,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54298,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, and please provide a code snippet to reproduce the issue reported here. Thanks!","ok. As I have said, I just use master branch with newest commit 851b83d0ef9659bb83b4f8c56708fb045fe276dc Below is my configure results !image Then I just use bazel to compile tflie !image After all, I push all libhexagon_*.so and mobilenet_quant_v1_224.tflite, benchmark_model, then got !image I think it is just too simple to reproduce similar results as well as running on Snapdragon 845 device","For the remote handle log message, it is harmless in your case. As for the latency, i think you are mistaken. From the log you shared, the inference is taking 6358 us, which is 6.358 ms. not 60ms. This looks working correctly to me."," Sorry to brother you for my mistake. Another question, dose tflite support inference on SnaDragon 888 with Hexagon dsp backend ? "," np. Sadly no, Snapdragon 888 doesn't leverage Hexagon NN API. They had a new APIs for executing on the DSP, and hexagon delegate doesn't work with it."
1077,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(saved_model_aot_compile.py removes unwanted tensors from signature)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): 2.4.1  Python version: 3.6.9  CUDA/cuDNN version:  11.1  GPU model and memory: 12 GB **Describe the current behavior** When I load the frozen graph using `tf.saved_model.load(""saved_model.pb"")`, the signature shows all the tensor names, but when `.local/lib/python3.6/sitepackages/tensorflow/python/tools/saved_model_aot_compile.py` does the `_prune_removed_feed_nodes(signature_def, graph_def)`, it prunes multiple tensors that it does not find in `graph_dev.node` and as a result my final deployed model is incorrect:   Here are my other TF related packages:  Also, `2.4.1` was used to create the model:   Here is the debug output: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,amirjamez,saved_model_aot_compile.py removes unwanted tensors from signature,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04  TensorFlow installed from (source or binary): Binary  TensorFlow version (use command below): 2.4.1  Python version: 3.6.9  CUDA/cuDNN version:  11.1  GPU model and memory: 12 GB **Describe the current behavior** When I load the frozen graph using `tf.saved_model.load(""saved_model.pb"")`, the signature shows all the tensor names, but when `.local/lib/python3.6/sitepackages/tensorflow/python/tools/saved_model_aot_compile.py` does the `_prune_removed_feed_nodes(signature_def, graph_def)`, it prunes multiple tensors that it does not find in `graph_dev.node` and as a result my final deployed model is incorrect:   Here are my other TF related packages:  Also, `2.4.1` was used to create the model:   Here is the debug output: ",2022-02-08T02:27:45Z,stat:awaiting response type:bug comp:runtime TF 2.4,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54296," , In order to expedite the troubleshooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", Received an answer from the group they are working on this project: https://github.com/google/mlcompileropt/issues/14,Are you satisfied with the resolution of your issue? Yes No
1884,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory leak after model.fit is called in tf 2.7 and 2.8 and training does not start)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code: custom code  OS Platform and Distribution: Ubuntu 18 (google colab)  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7, 2.8, 2.9.0dev20220203  Python version: 3.7, 3.9, 3.10  CUDA/cuDNN version: Build cuda_11.1.TC455_06.29190527_0  GPU model and memory: varies You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** I'm using my yolo implementation which used to work fine on tensorflow versions prior to 2.5. I tried recently training yolo3 on a small dataset (which uses `tf.keras.Model.fit`). Here's a colab notebook which you can use to reproduce the issue. Shortly after `model.fit` is called, the following 2 messages keep repeating in no particular order:     /usr/local/lib/python3.7/distpackages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.       layer_config = serialize_layer_fn(layer) and     INFO:tensorflow:Assets written to: ram://eefa3127ad7d4445a18675fd8f0b81e1/as)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghost,Memory leak after model.fit is called in tf 2.7 and 2.8 and training does not start,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code: custom code  OS Platform and Distribution: Ubuntu 18 (google colab)  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7, 2.8, 2.9.0dev20220203  Python version: 3.7, 3.9, 3.10  CUDA/cuDNN version: Build cuda_11.1.TC455_06.29190527_0  GPU model and memory: varies You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** I'm using my yolo implementation which used to work fine on tensorflow versions prior to 2.5. I tried recently training yolo3 on a small dataset (which uses `tf.keras.Model.fit`). Here's a colab notebook which you can use to reproduce the issue. Shortly after `model.fit` is called, the following 2 messages keep repeating in no particular order:     /usr/local/lib/python3.7/distpackages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.       layer_config = serialize_layer_fn(layer) and     INFO:tensorflow:Assets written to: ram://eefa3127ad7d4445a18675fd8f0b81e1/as",2022-02-07T06:19:15Z,stat:awaiting response type:support comp:keras TF 2.8,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54285,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,"I will, thanks", Could you please confirm if you have posted this issue on kerasteam/keras repo. ? Thanks!,  Here's the issue url, Thank you for the confirmation! Can you please move this issue to close status as we will track the issue in  kerasteam/keras repo.. Thanks!,Are you satisfied with the resolution of your issue? Yes No,"I can confirm this same error is still encountered in tensorflow 2.8 even with the use of the latest nightly version of 2.8.0 (2.8.0dev20211222). This was encountered with several tf.keras models (where some work fine in tensorflow 2.2 version but simply stop training after few epochs in 2.8 version). Even with very small datasets. Decreasing the batch size improves a bit the training by delaying the error explosion to later epochs, but it doesn't solve the issue. I have tried to use os.environ[""TF_GPU_ALLOCATOR""]=""cuda_malloc_async"" but in vain. Here is an example of an error output when training (memory usage keeps increasing during training, until the 60th epoch where an OOM memory burst problem happens). > 20220311 16:33:20.651586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11433 MB memory:  > device: 0, name: NVIDIA TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1 20220311 16:33:35.198049: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101 20220311 16:42:26.891801: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 98.00MiB (rounded to 102760448)requested by op gradient_tape/model_2/conv_3_2/Conv2D/Conv2DBackpropInput If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.  Current allocation summary follows. Current allocation summary follows. 20220311 16:42:26.891975: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc 20220311 16:42:26.892028: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): 	Total Chunks: 179, Chunks in use: 173. 44.8KiB allocated for chunks. 43.2KiB in use in bin. 16.7KiB clientrequested in use in bin. 20220311 16:42:26.892060: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): 	Total Chunks: 58, Chunks in use: 52. 29.2KiB allocated for chunks. 26.2KiB in use in bin. 26.0KiB clientrequested in use in bin. 20220311 16:42:26.892093: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): 	Total Chunks: 203, Chunks in use: 202. 212.5KiB allocated for chunks. 211.5KiB in use in bin. 202.0KiB clientrequested in use in bin. 20220311 16:42:26.892639: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): 	Total Chunks: 1, Chunks in use: 0. 2.0KiB allocated for chunks. 0B in use in bin. 0B clientrequested in use in bin. 20220311 16:42:26.892864: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B clientrequested in use in bin. 20220311 16:42:26.892915: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): 	Total Chunks: 22, Chunks in use: 22. 179.0KiB allocated for chunks. 179.0KiB in use in bin. 176.0KiB clientrequested in use in bin. 20220311 16:42:26.892954: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): 	Total Chunks: 21, Chunks in use: 18. 518.8KiB allocated for chunks. 446.2KiB in use in bin. 442.0KiB clientrequested in use in bin. 20220311 16:42:26.892989: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): 	Total Chunks: 4, Chunks in use: 3. 184.0KiB allocated for chunks. 134.0KiB in use in bin. 81.0KiB clientrequested in use in bin. 20220311 16:42:26.893018: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B clientrequested in use in bin. 20220311 16:42:26.893050: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): 	Total Chunks: 6, Chunks in use: 5. 948.5KiB allocated for chunks. 804.5KiB in use in bin. 720.0KiB clientrequested in use in bin. 20220311 16:42:26.893079: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): 	Total Chunks: 5, Chunks in use: 5. 1.58MiB allocated for chunks. 1.58MiB in use in bin. 1.41MiB clientrequested in use in bin. 20220311 16:42:26.893297: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): 	Total Chunks: 18, Chunks in use: 14. 10.70MiB allocated for chunks. 8.12MiB in use in bin. 7.88MiB clientrequested in use in bin. 20220311 16:42:26.893332: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): 	Total Chunks: 8, Chunks in use: 5. 9.51MiB allocated for chunks. 6.06MiB in use in bin. 5.65MiB clientrequested in use in bin. 20220311 16:42:26.893367: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): 	Total Chunks: 19, Chunks in use: 19. 44.25MiB allocated for chunks. 44.25MiB in use in bin. 42.75MiB clientrequested in use in bin. 20220311 16:42:26.893402: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): 	Total Chunks: 12, Chunks in use: 10. 68.09MiB allocated for chunks. 54.09MiB in use in bin. 53.22MiB clientrequested in use in bin. 20220311 16:42:26.893431: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B clientrequested in use in bin. 20220311 16:42:26.893464: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): 	Total Chunks: 15, Chunks in use: 14. 360.07MiB allocated for chunks. 335.83MiB in use in bin. 311.38MiB clientrequested in use in bin. 20220311 16:42:26.893497: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): 	Total Chunks: 13, Chunks in use: 12. 615.82MiB allocated for chunks. 578.18MiB in use in bin. 563.50MiB clientrequested in use in bin. 20220311 16:42:26.893709: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): 	Total Chunks: 78, Chunks in use: 78. 6.23GiB allocated for chunks. 6.23GiB in use in bin. 5.94GiB clientrequested in use in bin. 20220311 16:42:26.893742: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): 	Total Chunks: 7, Chunks in use: 7. 1.29GiB allocated for chunks. 1.29GiB in use in bin. 1.24GiB clientrequested in use in bin. 20220311 16:42:26.893774: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): 	Total Chunks: 6, Chunks in use: 6. 2.56GiB allocated for chunks. 2.56GiB in use in bin. 2.45GiB clientrequested in use in bin. 20220311 16:42:26.895086: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 98.00MiB was 64.00MiB, Chunk State:  20220311 16:42:26.895129: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 11988434944 20220311 16:42:26.895161: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000000 of size 1280 next 1 20220311 16:42:26.895380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000500 of size 256 next 2 20220311 16:42:26.895411: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000600 of size 256 next 3 20220311 16:42:26.895433: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000700 of size 256 next 4 20220311 16:42:26.895620: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000800 of size 256 next 5 20220311 16:42:26.895656: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000900 of size 256 next 6 20220311 16:42:26.895680: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000a00 of size 256 next 7 20220311 16:42:26.895701: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000b00 of size 256 next 8 20220311 16:42:26.895721: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000c00 of size 256 next 11 20220311 16:42:26.895742: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000d00 of size 256 next 12 20220311 16:42:26.895951: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000e00 of size 256 next 81 20220311 16:42:26.895986: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc000f00 of size 256 next 14 20220311 16:42:26.896010: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001000 of size 256 next 15 20220311 16:42:26.896032: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001100 of size 256 next 16 20220311 16:42:26.896054: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001200 of size 256 next 17 20220311 16:42:26.896076: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001300 of size 256 next 20 20220311 16:42:26.896098: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001400 of size 256 next 21 20220311 16:42:26.896120: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001500 of size 256 next 343 20220311 16:42:26.896140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001600 of size 256 next 23 20220311 16:42:26.896161: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc001700 of size 8192 next 123 20220311 16:42:26.896182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc003700 of size 8192 next 126 20220311 16:42:26.896201: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc005700 of size 8192 next 127 20220311 16:42:26.896399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc007700 of size 8192 next 128 20220311 16:42:26.896424: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc009700 of size 8192 next 129 20220311 16:42:26.896447: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc00b700 of size 11264 next 9 20220311 16:42:26.896469: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc00e300 of size 27648 next 10 20220311 16:42:26.896491: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc014f00 of size 256 next 24 20220311 16:42:26.896513: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015000 of size 256 next 27 20220311 16:42:26.896535: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015100 of size 256 next 28 20220311 16:42:26.896557: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015200 of size 256 next 534 20220311 16:42:26.896579: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015300 of size 256 next 30 20220311 16:42:26.896601: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015400 of size 256 next 31 20220311 16:42:26.896620: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015500 of size 256 next 32 20220311 16:42:26.896643: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015600 of size 512 next 33 20220311 16:42:26.896663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015800 of size 512 next 36 20220311 16:42:26.896682: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015a00 of size 512 next 37 20220311 16:42:26.896702: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8cc015c00 of size 512 next 424 20220311 16:42:26.896912: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015e00 of size 256 next 157 20220311 16:42:26.896941: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc015f00 of size 256 next 39 20220311 16:42:26.896964: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016000 of size 512 next 40 20220311 16:42:26.896988: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016200 of size 512 next 42 20220311 16:42:26.897010: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016400 of size 512 next 43 20220311 16:42:26.897034: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016600 of size 1024 next 45 20220311 16:42:26.897056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016a00 of size 256 next 48 20220311 16:42:26.897078: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016b00 of size 256 next 49 20220311 16:42:26.897100: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016c00 of size 256 next 50 20220311 16:42:26.897121: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8cc016d00 of size 512 next 52 20220311 16:42:26.897143: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc016f00 of size 256 next 53 20220311 16:42:26.897163: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017000 of size 256 next 54 20220311 16:42:26.897183: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017100 of size 512 next 55 20220311 16:42:26.897202: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017300 of size 512 next 58 20220311 16:42:26.897390: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017500 of size 512 next 59 20220311 16:42:26.897421: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8cc017700 of size 512 next 447 20220311 16:42:26.897444: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017900 of size 256 next 512 20220311 16:42:26.897465: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017a00 of size 256 next 61 20220311 16:42:26.897487: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017b00 of size 256 next 62 20220311 16:42:26.897509: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017c00 of size 256 next 63 20220311 16:42:26.897531: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc017d00 of size 1024 next 64 20220311 16:42:26.897553: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc018100 of size 1024 next 67 20220311 16:42:26.897574: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc018500 of size 1024 next 68 20220311 16:42:26.897597: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc018900 of size 1024 next 579 20220311 16:42:26.897618: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc018d00 of size 1024 next 70 20220311 16:42:26.897639: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc019100 of size 1024 next 71 20220311 16:42:26.897659: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc019500 of size 1024 next 73 20220311 16:42:26.897678: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc019900 of size 1024 next 74 20220311 16:42:26.897698: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc019d00 of size 1024 next 454 20220311 16:42:26.897907: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01a100 of size 1024 next 76 20220311 16:42:26.897942: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01a500 of size 256 next 78 20220311 16:42:26.897965: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01a600 of size 256 next 79 20220311 16:42:26.897987: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01a700 of size 256 next 80 20220311 16:42:26.898010: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01a800 of size 256 next 548 20220311 16:42:26.898032: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01a900 of size 256 next 82 20220311 16:42:26.898057: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01aa00 of size 256 next 83 20220311 16:42:26.898079: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01ab00 of size 256 next 84 20220311 16:42:26.898101: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01ac00 of size 1024 next 87 20220311 16:42:26.898122: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01b000 of size 1024 next 88 20220311 16:42:26.898145: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01b400 of size 1024 next 89 20220311 16:42:26.898165: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01b800 of size 1024 next 25 20220311 16:42:26.898195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc01bc00 of size 27648 next 26 20220311 16:42:26.898215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc022800 of size 1024 next 90 20220311 16:42:26.898408: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc022c00 of size 1024 next 93 20220311 16:42:26.898437: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc023000 of size 1024 next 94 20220311 16:42:26.898460: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc023400 of size 1024 next 95 20220311 16:42:26.898481: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc023800 of size 256 next 348 20220311 16:42:26.898504: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc023900 of size 1024 next 144 20220311 16:42:26.898526: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc023d00 of size 512 next 576 20220311 16:42:26.898548: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8cc023f00 of size 256 next 97 20220311 16:42:26.898570: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc024000 of size 256 next 98 20220311 16:42:26.898593: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc024100 of size 256 next 99 20220311 16:42:26.898615: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc024200 of size 1024 next 100 20220311 16:42:26.898636: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc024600 of size 1024 next 103 20220311 16:42:26.898658: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc024a00 of size 1024 next 104 20220311 16:42:26.898678: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc024e00 of size 1024 next 444 20220311 16:42:26.898698: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc025200 of size 1024 next 106 20220311 16:42:26.898718: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc025600 of size 1024 next 108 20220311 16:42:26.898938: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc025a00 of size 1024 next 109 20220311 16:42:26.898966: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc025e00 of size 1024 next 110 20220311 16:42:26.898987: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026200 of size 256 next 535 20220311 16:42:26.899009: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026300 of size 256 next 494 20220311 16:42:26.899030: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026400 of size 256 next 517 20220311 16:42:26.899052: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026500 of size 256 next 352 20220311 16:42:26.899074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026600 of size 256 next 384 20220311 16:42:26.899096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026700 of size 256 next 394 20220311 16:42:26.899118: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026800 of size 256 next 383 20220311 16:42:26.899140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026900 of size 256 next 112 20220311 16:42:26.899161: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026a00 of size 256 next 113 20220311 16:42:26.899182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026b00 of size 256 next 114 20220311 16:42:26.899203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026c00 of size 512 next 115 20220311 16:42:26.899221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc026e00 of size 512 next 117 20220311 16:42:26.899240: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027000 of size 512 next 118 20220311 16:42:26.899259: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027200 of size 1024 next 120 20220311 16:42:26.899279: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027600 of size 256 next 121 20220311 16:42:26.899299: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027700 of size 256 next 122 20220311 16:42:26.899498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027800 of size 256 next 130 20220311 16:42:26.899526: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027900 of size 256 next 131 20220311 16:42:26.899549: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027a00 of size 256 next 138 20220311 16:42:26.899571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027b00 of size 256 next 139 20220311 16:42:26.899593: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027c00 of size 256 next 140 20220311 16:42:26.899614: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc027d00 of size 256 next 143 20220311 16:42:26.899636: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8cc027e00 of size 512 next 145 20220311 16:42:26.899658: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028000 of size 256 next 146 20220311 16:42:26.899679: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028100 of size 256 next 362 20220311 16:42:26.899700: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028200 of size 256 next 149 20220311 16:42:26.899722: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028300 of size 256 next 150 20220311 16:42:26.899742: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028400 of size 256 next 151 20220311 16:42:26.899762: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028500 of size 256 next 152 20220311 16:42:26.899782: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028600 of size 256 next 153 20220311 16:42:26.899976: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028700 of size 256 next 154 20220311 16:42:26.900011: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028800 of size 256 next 155 20220311 16:42:26.900034: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028900 of size 256 next 156 20220311 16:42:26.900056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028a00 of size 256 next 165 20220311 16:42:26.900077: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028b00 of size 256 next 158 20220311 16:42:26.900099: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028c00 of size 256 next 159 20220311 16:42:26.900121: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028d00 of size 256 next 160 20220311 16:42:26.900143: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028e00 of size 256 next 161 20220311 16:42:26.900165: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc028f00 of size 256 next 163 20220311 16:42:26.900189: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc029000 of size 256 next 164 20220311 16:42:26.900221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc029100 of size 256 next 167 20220311 16:42:26.900241: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc029200 of size 256 next 168 20220311 16:42:26.900261: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc029300 of size 256 next 47 20220311 16:42:26.900280: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc029400 of size 27648 next 46 20220311 16:42:26.900303: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc030000 of size 27648 next 77 20220311 16:42:26.900477: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc036c00 of size 8192 next 132 20220311 16:42:26.900509: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc038c00 of size 8192 next 135 20220311 16:42:26.900531: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc03ac00 of size 8192 next 136 20220311 16:42:26.900551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc03cc00 of size 8192 next 137 20220311 16:42:26.900571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc03ec00 of size 1024 next 547 20220311 16:42:26.900591: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc03f000 of size 1024 next 166 20220311 16:42:26.900613: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc03f400 of size 30720 next 142 20220311 16:42:26.900634: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc046c00 of size 16384 next 141 20220311 16:42:26.900654: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc04ac00 of size 16384 next 148 20220311 16:42:26.900674: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc04ec00 of size 256 next 169 20220311 16:42:26.900695: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc04ed00 of size 256 next 170 20220311 16:42:26.900714: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc04ee00 of size 256 next 172 20220311 16:42:26.900733: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc04ef00 of size 27648 next 173 20220311 16:42:26.900752: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc055b00 of size 256 next 174 20220311 16:42:26.900940: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc055c00 of size 256 next 175 20220311 16:42:26.900971: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc055d00 of size 256 next 176 20220311 16:42:26.900992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc055e00 of size 256 next 177 20220311 16:42:26.901012: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc055f00 of size 256 next 178 20220311 16:42:26.901032: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc056000 of size 512 next 179 20220311 16:42:26.901054: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc056200 of size 27904 next 19 20220311 16:42:26.901076: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc05cf00 of size 147456 next 18 20220311 16:42:26.901097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc080f00 of size 589824 next 57 20220311 16:42:26.901119: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc110f00 of size 294912 next 56 20220311 16:42:26.901139: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc158f00 of size 147456 next 171 20220311 16:42:26.901160: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17cf00 of size 256 next 180 20220311 16:42:26.901180: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17d000 of size 512 next 181 20220311 16:42:26.901199: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17d200 of size 512 next 182 20220311 16:42:26.901217: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17d400 of size 256 next 183 20220311 16:42:26.901236: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17d500 of size 256 next 184 20220311 16:42:26.901254: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17d600 of size 512 next 185 20220311 16:42:26.901439: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17d800 of size 512 next 187 20220311 16:42:26.901462: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17da00 of size 512 next 188 20220311 16:42:26.901483: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17dc00 of size 512 next 189 20220311 16:42:26.901503: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17de00 of size 512 next 190 20220311 16:42:26.901522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17e000 of size 512 next 191 20220311 16:42:26.901543: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc17e200 of size 27648 next 192 20220311 16:42:26.901563: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc184e00 of size 256 next 193 20220311 16:42:26.901583: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc184f00 of size 256 next 194 20220311 16:42:26.901603: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc185000 of size 256 next 195 20220311 16:42:26.901623: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc185100 of size 1024 next 196 20220311 16:42:26.901641: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc185500 of size 1024 next 198 20220311 16:42:26.901660: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc185900 of size 1024 next 199 20220311 16:42:26.901678: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc185d00 of size 1024 next 200 20220311 16:42:26.901878: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc186100 of size 1024 next 201 20220311 16:42:26.901904: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc186500 of size 1024 next 202 20220311 16:42:26.901924: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc186900 of size 1024 next 204 20220311 16:42:26.901944: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc186d00 of size 1024 next 206 20220311 16:42:26.901964: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc187100 of size 1024 next 207 20220311 16:42:26.901984: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc187500 of size 1024 next 208 20220311 16:42:26.902004: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc187900 of size 1024 next 209 20220311 16:42:26.902024: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc187d00 of size 1024 next 210 20220311 16:42:26.902044: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc188100 of size 1024 next 212 20220311 16:42:26.902064: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc188500 of size 1024 next 213 20220311 16:42:26.902084: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc188900 of size 1024 next 214 20220311 16:42:26.902104: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc188d00 of size 1024 next 215 20220311 16:42:26.902123: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc189100 of size 1024 next 216 20220311 16:42:26.902141: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc189500 of size 1024 next 217 20220311 16:42:26.902159: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc189900 of size 512 next 218 20220311 16:42:26.902337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc189b00 of size 512 next 219 20220311 16:42:26.902367: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc189d00 of size 512 next 220 20220311 16:42:26.902389: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc189f00 of size 8192 next 221 20220311 16:42:26.902409: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc18bf00 of size 8192 next 222 20220311 16:42:26.902428: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc18df00 of size 8192 next 223 20220311 16:42:26.902448: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc18ff00 of size 8192 next 224 20220311 16:42:26.902468: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc191f00 of size 8192 next 225 20220311 16:42:26.902488: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc193f00 of size 8192 next 226 20220311 16:42:26.902508: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc195f00 of size 16384 next 227 20220311 16:42:26.902529: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc199f00 of size 28672 next 35 20220311 16:42:26.902550: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc1a0f00 of size 589824 next 34 20220311 16:42:26.902569: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc230f00 of size 589824 next 41 20220311 16:42:26.902588: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc2c0f00 of size 589824 next 86 20220311 16:42:26.902606: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc350f00 of size 589824 next 85 20220311 16:42:26.902626: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc3e0f00 of size 1179648 next 116 20220311 16:42:26.902806: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc500f00 of size 294912 next 186 20220311 16:42:26.902830: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc548f00 of size 589824 next 197 20220311 16:42:26.902878: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc5d8f00 of size 1474560 next 66 20220311 16:42:26.902899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc740f00 of size 2359296 next 65 20220311 16:42:26.902918: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cc980f00 of size 2359296 next 72 20220311 16:42:26.902938: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ccbc0f00 of size 2359296 next 92 20220311 16:42:26.902957: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cce00f00 of size 2359296 next 91 20220311 16:42:26.902977: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cd040f00 of size 2359296 next 107 20220311 16:42:26.902997: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cd280f00 of size 2359296 next 203 20220311 16:42:26.903018: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cd4c0f00 of size 2359296 next 205 20220311 16:42:26.903037: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cd700f00 of size 2359296 next 102 20220311 16:42:26.903058: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cd940f00 of size 4718592 next 101 20220311 16:42:26.903077: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cddc0f00 of size 4718592 next 211 20220311 16:42:26.903097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ce240f00 of size 28835840 next 134 20220311 16:42:26.903115: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8cfdc0f00 of size 16777216 next 133 20220311 16:42:26.903135: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8d0dc0f00 of size 52428800 next 125 20220311 16:42:26.903317: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8d3fc0f00 of size 51380224 next 124 20220311 16:42:26.903346: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8d70c0f00 of size 77070336 next 162 20220311 16:42:26.903367: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba40f00 of size 256 next 228 20220311 16:42:26.903387: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba41000 of size 256 next 229 20220311 16:42:26.903407: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba41100 of size 256 next 230 20220311 16:42:26.903428: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba41200 of size 147456 next 231 20220311 16:42:26.903449: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba65200 of size 256 next 232 20220311 16:42:26.903469: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba65300 of size 27648 next 233 20220311 16:42:26.903490: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba6bf00 of size 256 next 234 20220311 16:42:26.903510: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba6c000 of size 256 next 235 20220311 16:42:26.903530: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba6c100 of size 256 next 236 20220311 16:42:26.903550: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba6c200 of size 256 next 237 20220311 16:42:26.903569: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba6c300 of size 256 next 238 20220311 16:42:26.903588: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dba6c400 of size 589824 next 239 20220311 16:42:26.903607: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbafc400 of size 512 next 240 20220311 16:42:26.903789: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbafc600 of size 27648 next 241 20220311 16:42:26.903815: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb03200 of size 256 next 242 20220311 16:42:26.903848: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb03300 of size 512 next 243 20220311 16:42:26.903871: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb03500 of size 512 next 244 20220311 16:42:26.903891: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb03700 of size 256 next 245 20220311 16:42:26.903912: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb03800 of size 256 next 246 20220311 16:42:26.903933: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb03900 of size 589824 next 247 20220311 16:42:26.903954: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb93900 of size 512 next 248 20220311 16:42:26.903974: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbb93b00 of size 294912 next 249 20220311 16:42:26.903994: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbdbb00 of size 512 next 250 20220311 16:42:26.904014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbdbd00 of size 512 next 251 20220311 16:42:26.904033: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbdbf00 of size 512 next 252 20220311 16:42:26.904051: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbdc100 of size 512 next 253 20220311 16:42:26.904070: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbdc300 of size 512 next 254 20220311 16:42:26.904254: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbdc500 of size 27648 next 255 20220311 16:42:26.904278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbe3100 of size 256 next 256 20220311 16:42:26.904299: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbe3200 of size 256 next 257 20220311 16:42:26.904320: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbe3300 of size 256 next 258 20220311 16:42:26.904340: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbbe3400 of size 2359296 next 259 20220311 16:42:26.904360: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbe23400 of size 1024 next 260 20220311 16:42:26.904381: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbe23800 of size 589824 next 261 20220311 16:42:26.904401: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbeb3800 of size 1024 next 262 20220311 16:42:26.904421: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbeb3c00 of size 1024 next 263 20220311 16:42:26.904441: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbeb4000 of size 1024 next 264 20220311 16:42:26.904461: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbeb4400 of size 1024 next 265 20220311 16:42:26.904479: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbeb4800 of size 1024 next 266 20220311 16:42:26.904497: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dbeb4c00 of size 2359296 next 267 20220311 16:42:26.904516: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc0f4c00 of size 1024 next 268 20220311 16:42:26.904697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc0f5000 of size 2359296 next 269 20220311 16:42:26.904722: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc335000 of size 1024 next 270 20220311 16:42:26.904743: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc335400 of size 1024 next 271 20220311 16:42:26.904764: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc335800 of size 1024 next 272 20220311 16:42:26.904784: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc335c00 of size 1024 next 273 20220311 16:42:26.904803: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc336000 of size 1024 next 274 20220311 16:42:26.904823: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc336400 of size 4718592 next 275 20220311 16:42:26.904856: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc7b6400 of size 1024 next 276 20220311 16:42:26.904879: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc7b6800 of size 1024 next 277 20220311 16:42:26.904899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc7b6c00 of size 1024 next 278 20220311 16:42:26.904919: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc7b7000 of size 2359296 next 279 20220311 16:42:26.904939: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc9f7000 of size 1024 next 280 20220311 16:42:26.904958: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc9f7400 of size 1024 next 281 20220311 16:42:26.904976: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc9f7800 of size 1024 next 282 20220311 16:42:26.904995: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dc9f7c00 of size 1179648 next 283 20220311 16:42:26.905176: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dcb17c00 of size 512 next 284 20220311 16:42:26.905200: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dcb17e00 of size 512 next 285 20220311 16:42:26.905219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dcb18000 of size 512 next 286 20220311 16:42:26.905238: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dcb18200 of size 51380224 next 287 20220311 16:42:26.905260: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dfc18200 of size 8192 next 288 20220311 16:42:26.905280: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dfc1a200 of size 8192 next 289 20220311 16:42:26.905301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dfc1c200 of size 8192 next 290 20220311 16:42:26.905322: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8dfc1e200 of size 16777216 next 291 20220311 16:42:26.905342: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c1e200 of size 8192 next 292 20220311 16:42:26.905363: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c20200 of size 8192 next 293 20220311 16:42:26.905383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c22200 of size 8192 next 294 20220311 16:42:26.905404: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c24200 of size 16384 next 295 20220311 16:42:26.905424: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c28200 of size 256 next 296 20220311 16:42:26.905444: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c28300 of size 256 next 297 20220311 16:42:26.905464: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c28400 of size 256 next 298 20220311 16:42:26.905483: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c28500 of size 27648 next 299 20220311 16:42:26.905502: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f100 of size 256 next 300 20220311 16:42:26.905520: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f200 of size 256 next 301 20220311 16:42:26.905704: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f300 of size 256 next 302 20220311 16:42:26.905728: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f400 of size 256 next 303 20220311 16:42:26.905749: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f500 of size 256 next 304 20220311 16:42:26.905769: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f600 of size 512 next 305 20220311 16:42:26.905790: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f800 of size 256 next 306 20220311 16:42:26.905810: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2f900 of size 1024 next 307 20220311 16:42:26.905831: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2fd00 of size 256 next 308 20220311 16:42:26.905877: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2fe00 of size 256 next 309 20220311 16:42:26.905898: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c2ff00 of size 256 next 310 20220311 16:42:26.905918: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30000 of size 256 next 311 20220311 16:42:26.905937: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30100 of size 256 next 312 20220311 16:42:26.905956: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30200 of size 256 next 313 20220311 16:42:26.905975: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30300 of size 256 next 314 20220311 16:42:26.905993: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30400 of size 256 next 315 20220311 16:42:26.906175: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30500 of size 256 next 316 20220311 16:42:26.906197: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30600 of size 256 next 317 20220311 16:42:26.906218: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30700 of size 256 next 318 20220311 16:42:26.906238: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30800 of size 256 next 319 20220311 16:42:26.906258: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30900 of size 256 next 320 20220311 16:42:26.906279: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30a00 of size 512 next 321 20220311 16:42:26.906299: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30c00 of size 256 next 322 20220311 16:42:26.906319: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30d00 of size 256 next 323 20220311 16:42:26.906339: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30e00 of size 256 next 324 20220311 16:42:26.906359: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c30f00 of size 256 next 325 20220311 16:42:26.906380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c31000 of size 256 next 326 20220311 16:42:26.906399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c31100 of size 256 next 327 20220311 16:42:26.906418: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c31200 of size 147456 next 328 20220311 16:42:26.906437: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0c55200 of size 589824 next 329 20220311 16:42:26.906611: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0ce5200 of size 2359296 next 330 20220311 16:42:26.906641: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e0f25200 of size 4718592 next 331 20220311 16:42:26.906662: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e13a5200 of size 1179648 next 332 20220311 16:42:26.906683: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e14c5200 of size 294912 next 333 20220311 16:42:26.906703: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e150d200 of size 589824 next 334 20220311 16:42:26.906724: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e159d200 of size 256 next 514 20220311 16:42:26.906743: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e159d300 of size 256 next 513 20220311 16:42:26.906763: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e159d400 of size 256 next 335 20220311 16:42:26.906783: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8e159d500 of size 1210880 next 13 20220311 16:42:26.906803: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c4f00 of size 1024 next 610 20220311 16:42:26.906823: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c5300 of size 1024 next 438 20220311 16:42:26.906868: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c5700 of size 1024 next 580 20220311 16:42:26.906889: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c5b00 of size 1024 next 614 20220311 16:42:26.906909: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c5f00 of size 1024 next 342 20220311 16:42:26.906927: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c6300 of size 1024 next 419 20220311 16:42:26.906945: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c6700 of size 256 next 659 20220311 16:42:26.906963: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c6800 of size 256 next 544 20220311 16:42:26.907194: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c6900 of size 1024 next 382 20220311 16:42:26.907223: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c6d00 of size 1024 next 645 20220311 16:42:26.907245: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16c7100 of size 256 next 524 20220311 16:42:26.907266: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8e16c7200 of size 147456 next 696 20220311 16:42:26.907290: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e16eb200 of size 801792 next 608 20220311 16:42:26.907311: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17aee00 of size 1280 next 464 20220311 16:42:26.907332: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17af300 of size 1024 next 380 20220311 16:42:26.907353: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17af700 of size 1024 next 479 20220311 16:42:26.907372: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17afb00 of size 512 next 704 20220311 16:42:26.907392: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17afd00 of size 47104 next 527 20220311 16:42:26.907411: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bb500 of size 1024 next 449 20220311 16:42:26.907429: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bb900 of size 1024 next 361 20220311 16:42:26.907448: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bbd00 of size 1024 next 581 20220311 16:42:26.907467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bc100 of size 1024 next 450 20220311 16:42:26.907485: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bc500 of size 1536 next 592 20220311 16:42:26.907503: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bcb00 of size 1024 next 442 20220311 16:42:26.907522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bcf00 of size 1024 next 445 20220311 16:42:26.907541: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bd300 of size 1024 next 486 20220311 16:42:26.907560: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bd700 of size 1024 next 482 20220311 16:42:26.907579: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bdb00 of size 1024 next 344 20220311 16:42:26.907598: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bdf00 of size 1024 next 599 20220311 16:42:26.907617: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17be300 of size 1024 next 420 20220311 16:42:26.907637: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17be700 of size 1536 next 562 20220311 16:42:26.907657: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bed00 of size 1280 next 473 20220311 16:42:26.907677: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bf200 of size 256 next 643 20220311 16:42:26.907697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bf300 of size 256 next 368 20220311 16:42:26.907718: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bf400 of size 256 next 583 20220311 16:42:26.907738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bf500 of size 256 next 613 20220311 16:42:26.907758: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bf600 of size 1024 next 437 20220311 16:42:26.907778: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bfa00 of size 256 next 674 20220311 16:42:26.907797: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8e17bfb00 of size 256 next 378 20220311 16:42:26.907816: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17bfc00 of size 1024 next 594 20220311 16:42:26.907848: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c0000 of size 1280 next 429 20220311 16:42:26.907869: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c0500 of size 1024 next 338 20220311 16:42:26.907888: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c0900 of size 1024 next 408 20220311 16:42:26.907907: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c0d00 of size 1280 next 585 20220311 16:42:26.907929: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c1200 of size 1024 next 350 20220311 16:42:26.907948: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8e17c1600 of size 256 next 687 20220311 16:42:26.907967: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c1700 of size 256 next 656 20220311 16:42:26.907987: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c1800 of size 256 next 472 20220311 16:42:26.908006: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c1900 of size 1024 next 471 20220311 16:42:26.908027: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e17c1d00 of size 3608576 next 658 20220311 16:42:26.908046: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1b32d00 of size 1536 next 370 20220311 16:42:26.908063: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1b33300 of size 1024 next 446 20220311 16:42:26.908082: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1b33700 of size 512 next 683 20220311 16:42:26.908100: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1b33900 of size 1536 next 379 20220311 16:42:26.908120: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1b33f00 of size 1024 next 595 20220311 16:42:26.908140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8e1b34300 of size 589824 next 492 20220311 16:42:26.908158: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1bc4300 of size 589824 next 460 20220311 16:42:26.908177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8e1c54300 of size 776704 next 508 20220311 16:42:26.908195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1d11d00 of size 1536 next 553 20220311 16:42:26.908215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e1d12300 of size 77070336 next 386 20220311 16:42:26.908236: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8e6692300 of size 90040832 next 539 20220311 16:42:26.908257: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc70d00 of size 1024 next 533 20220311 16:42:26.908277: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc71100 of size 1024 next 69 20220311 16:42:26.908298: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8ebc71500 of size 17408 next 400 20220311 16:42:26.908319: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc75900 of size 41984 next 546 20220311 16:42:26.908340: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc7fd00 of size 1024 next 377 20220311 16:42:26.908360: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8ebc80100 of size 512 next 353 20220311 16:42:26.908380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc80300 of size 1024 next 498 20220311 16:42:26.908400: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc80700 of size 1024 next 604 20220311 16:42:26.908418: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc80b00 of size 256 next 387 20220311 16:42:26.908438: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc80c00 of size 256 next 465 20220311 16:42:26.908456: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc80d00 of size 1024 next 398 20220311 16:42:26.908475: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc81100 of size 1280 next 606 20220311 16:42:26.908494: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc81600 of size 1024 next 487 20220311 16:42:26.908513: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc81a00 of size 512 next 648 20220311 16:42:26.908533: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc81c00 of size 256 next 529 20220311 16:42:26.908551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc81d00 of size 256 next 347 20220311 16:42:26.908571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc81e00 of size 1024 next 642 20220311 16:42:26.908590: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc82200 of size 256 next 729 20220311 16:42:26.908609: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc82300 of size 256 next 376 20220311 16:42:26.908628: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc82400 of size 1280 next 351 20220311 16:42:26.908646: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ebc82900 of size 1536 next 478 20220311 16:42:26.908663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8ebc82f00 of size 25417216 next 485 20220311 16:42:26.908681: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c0500 of size 1536 next 29 20220311 16:42:26.908699: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c0b00 of size 1024 next 413 20220311 16:42:26.908718: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c0f00 of size 1024 next 557 20220311 16:42:26.908737: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8ed4c1300 of size 512 next 497 20220311 16:42:26.908757: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c1500 of size 1024 next 336 20220311 16:42:26.908777: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8ed4c1900 of size 256 next 699 20220311 16:42:26.908796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c1a00 of size 256 next 596 20220311 16:42:26.908815: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c1b00 of size 1024 next 532 20220311 16:42:26.908833: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c1f00 of size 256 next 721 20220311 16:42:26.908867: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c2000 of size 256 next 518 20220311 16:42:26.908887: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c2100 of size 1024 next 540 20220311 16:42:26.908907: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c2500 of size 1024 next 456 20220311 16:42:26.908929: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ed4c2900 of size 25690112 next 720 20220311 16:42:26.908950: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8eed42900 of size 40036096 next 496 20220311 16:42:26.908970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f1371000 of size 1024 next 526 20220311 16:42:26.908989: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8f1371400 of size 256 next 609 20220311 16:42:26.909009: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f1371500 of size 1024 next 600 20220311 16:42:26.909029: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f1371900 of size 1024 next 626 20220311 16:42:26.909048: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f1371d00 of size 1024 next 435 20220311 16:42:26.909068: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f1372100 of size 1792 next 358 20220311 16:42:26.909088: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f1372800 of size 77070336 next 385 20220311 16:42:26.909107: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5cf2800 of size 1024 next 505 20220311 16:42:26.909126: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5cf2c00 of size 1024 next 577 20220311 16:42:26.909146: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5cf3000 of size 1024 next 730 20220311 16:42:26.909166: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8f5cf3400 of size 51200 next 443 20220311 16:42:26.909185: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5cffc00 of size 1024 next 578 20220311 16:42:26.909205: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d00000 of size 1024 next 430 20220311 16:42:26.909224: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d00400 of size 1024 next 466 20220311 16:42:26.909244: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d00800 of size 1536 next 75 20220311 16:42:26.909263: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d00e00 of size 1024 next 550 20220311 16:42:26.909282: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d01200 of size 1024 next 433 20220311 16:42:26.909301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d01600 of size 1024 next 636 20220311 16:42:26.909320: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d01a00 of size 1024 next 147 20220311 16:42:26.909339: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d01e00 of size 1536 next 490 20220311 16:42:26.909357: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8f5d02400 of size 77070336 next 418 20220311 16:42:26.909377: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8fa682400 of size 89632256 next 523 20220311 16:42:26.909395: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ffbfd200 of size 1024 next 366 20220311 16:42:26.909413: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ffbfd600 of size 1280 next 501 20220311 16:42:26.909432: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ffbfdb00 of size 1024 next 401 20220311 16:42:26.909452: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ffbfdf00 of size 256 next 707 20220311 16:42:26.909472: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ffbfe000 of size 256 next 561 20220311 16:42:26.909491: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd8ffbfe100 of size 256 next 569 20220311 16:42:26.909511: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd8ffbfe200 of size 108818432 next 96 20220311 16:42:26.909532: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9063c5200 of size 99255296 next 588 20220311 16:42:26.909552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd90c26d600 of size 99264512 next 363 20220311 16:42:26.909572: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd912117e00 of size 95168768 next 520 20220311 16:42:26.909592: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd917bda700 of size 87380992 next 423 20220311 16:42:26.909614: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd91cf2fb00 of size 82456576 next 440 20220311 16:42:26.909635: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd921dd2b00 of size 1024 next 355 20220311 16:42:26.909655: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd921dd2f00 of size 1024 next 504 20220311 16:42:26.909675: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd921dd3300 of size 1024 next 405 20220311 16:42:26.909697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd921dd3700 of size 82451456 next 360 20220311 16:42:26.909717: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd926c75300 of size 77070336 next 506 20220311 16:42:26.909737: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd92b5f5300 of size 77070336 next 452 20220311 16:42:26.909760: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd92ff75300 of size 77070336 next 372 20220311 16:42:26.909780: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9348f5300 of size 104173312 next 570 20220311 16:42:26.909800: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93ac4e200 of size 1024 next 364 20220311 16:42:26.909819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93ac4e600 of size 2387968 next 624 20220311 16:42:26.909864: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93ae95600 of size 1024 next 615 20220311 16:42:26.909886: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93ae95a00 of size 4816896 next 426 20220311 16:42:26.909906: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93b32da00 of size 6422528 next 552 20220311 16:42:26.909927: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93b94da00 of size 7341056 next 105 20220311 16:42:26.909946: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c04de00 of size 1024 next 632 20220311 16:42:26.909966: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c04e200 of size 472064 next 718 20220311 16:42:26.909986: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c1600 of size 1024 next 719 20220311 16:42:26.910005: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c1a00 of size 1024 next 538 20220311 16:42:26.910024: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c1e00 of size 1024 next 432 20220311 16:42:26.910043: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c2200 of size 512 next 119 20220311 16:42:26.910060: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c2400 of size 512 next 684 20220311 16:42:26.910078: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c2600 of size 512 next 635 20220311 16:42:26.910107: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c2800 of size 1536 next 409 20220311 16:42:26.910125: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c2e00 of size 1024 next 605 20220311 16:42:26.910143: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c3200 of size 1024 next 607 20220311 16:42:26.910160: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c3600 of size 1024 next 434 20220311 16:42:26.910178: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c3a00 of size 1024 next 511 20220311 16:42:26.910195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c3e00 of size 1024 next 715 20220311 16:42:26.910213: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c4200 of size 1024 next 649 20220311 16:42:26.910232: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c4600 of size 1024 next 491 20220311 16:42:26.910250: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c4a00 of size 1024 next 627 20220311 16:42:26.910270: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c4e00 of size 1024 next 462 20220311 16:42:26.910289: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd93c0c5200 of size 2048 next 717 20220311 16:42:26.910308: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0c5a00 of size 512 next 710 20220311 16:42:26.910327: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd93c0c5c00 of size 29184 next 427 20220311 16:42:26.910346: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0cce00 of size 27648 next 620 20220311 16:42:26.910363: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd93c0d3a00 of size 27648 next 670 20220311 16:42:26.910383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0da600 of size 48128 next 705 20220311 16:42:26.910402: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0e6200 of size 256 next 428 20220311 16:42:26.910420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0e6300 of size 256 next 415 20220311 16:42:26.910438: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0e6400 of size 1024 next 457 20220311 16:42:26.910455: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0e6800 of size 1024 next 563 20220311 16:42:26.910474: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c0e6c00 of size 1024 next 660 20220311 16:42:26.910491: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd93c0e7000 of size 585472 next 669 20220311 16:42:26.910509: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c175f00 of size 1024 next 367 20220311 16:42:26.910527: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93c176300 of size 25690112 next 500 20220311 16:42:26.910545: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd93d9f6300 of size 39464192 next 111 20220311 16:42:26.910563: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93ff99000 of size 1024 next 421 20220311 16:42:26.910581: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd93ff99400 of size 32231680 next 463 20220311 16:42:26.910598: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd941e56500 of size 119567360 next 566 20220311 16:42:26.910616: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd94905d900 of size 77070336 next 536 20220311 16:42:26.910633: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd94d9dd900 of size 77070336 next 537 20220311 16:42:26.910651: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd95235d900 of size 77070336 next 475 20220311 16:42:26.910669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd956cdd900 of size 77070336 next 510 20220311 16:42:26.910687: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd95b65d900 of size 77070336 next 568 20220311 16:42:26.910707: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd95ffdd900 of size 77115392 next 525 20220311 16:42:26.910725: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd964968900 of size 77114880 next 593 20220311 16:42:26.910742: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9692f3700 of size 77070336 next 556 20220311 16:42:26.910760: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd96dc73700 of size 77070336 next 559 20220311 16:42:26.910777: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9725f3700 of size 77070336 next 549 20220311 16:42:26.910796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd976f73700 of size 77070336 next 468 20220311 16:42:26.910815: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd97b8f3700 of size 77070336 next 60 20220311 16:42:26.910834: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd980273700 of size 77070336 next 411 20220311 16:42:26.910878: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd984bf3700 of size 77070336 next 531 20220311 16:42:26.910897: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd989573700 of size 77070336 next 587 20220311 16:42:26.910916: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd98def3700 of size 77070336 next 522 20220311 16:42:26.910935: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd992873700 of size 77070336 next 51 20220311 16:42:26.910955: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9971f3700 of size 2390528 next 575 20220311 16:42:26.910974: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd99743b100 of size 1024 next 499 20220311 16:42:26.910992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd99743b500 of size 1048576 next 708 20220311 16:42:26.911011: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd99753b500 of size 1340416 next 389 20220311 16:42:26.911028: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd997682900 of size 1024 next 371 20220311 16:42:26.911047: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd997682d00 of size 105158656 next 22 20220311 16:42:26.911065: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd99dacc500 of size 127390208 next 542 20220311 16:42:26.911084: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9a5449700 of size 96234496 next 390 20220311 16:42:26.911102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd9ab010300 of size 1361920 next 484 20220311 16:42:26.911120: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab15cb00 of size 1024 next 392 20220311 16:42:26.911137: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd9ab15cf00 of size 760832 next 572 20220311 16:42:26.911155: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab216b00 of size 1024 next 571 20220311 16:42:26.911173: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab216f00 of size 1024 next 375 20220311 16:42:26.911190: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab217300 of size 1024 next 341 20220311 16:42:26.911208: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab217700 of size 256 next 495 20220311 16:42:26.911225: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab217800 of size 256 next 682 20220311 16:42:26.911241: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab217900 of size 1280 next 470 20220311 16:42:26.911258: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab217e00 of size 1024 next 586 20220311 16:42:26.911275: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab218200 of size 512 next 681 20220311 16:42:26.911295: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab218400 of size 768 next 356 20220311 16:42:26.911313: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab218700 of size 1024 next 530 20220311 16:42:26.911330: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab218b00 of size 1024 next 647 20220311 16:42:26.911348: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab218f00 of size 1024 next 664 20220311 16:42:26.911365: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab219300 of size 1024 next 354 20220311 16:42:26.911383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab219700 of size 1024 next 603 20220311 16:42:26.911401: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab219b00 of size 1024 next 502 20220311 16:42:26.911420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab219f00 of size 1024 next 625 20220311 16:42:26.911439: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21a300 of size 1024 next 574 20220311 16:42:26.911457: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21a700 of size 1024 next 461 20220311 16:42:26.911476: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21ab00 of size 1536 next 402 20220311 16:42:26.911494: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21b100 of size 1024 next 565 20220311 16:42:26.911512: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21b500 of size 1024 next 697 20220311 16:42:26.911532: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21b900 of size 1024 next 690 20220311 16:42:26.911552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21bd00 of size 1024 next 349 20220311 16:42:26.911570: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21c100 of size 256 next 458 20220311 16:42:26.911588: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21c200 of size 256 next 724 20220311 16:42:26.911605: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21c300 of size 512 next 480 20220311 16:42:26.911623: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21c500 of size 1024 next 700 20220311 16:42:26.911640: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21c900 of size 1024 next 602 20220311 16:42:26.911657: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21cd00 of size 1024 next 44 20220311 16:42:26.911675: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21d100 of size 1024 next 528 20220311 16:42:26.911693: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21d500 of size 512 next 406 20220311 16:42:26.911711: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21d700 of size 512 next 672 20220311 16:42:26.911729: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21d900 of size 512 next 573 20220311 16:42:26.911746: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21db00 of size 1024 next 543 20220311 16:42:26.911766: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab21df00 of size 233984 next 633 20220311 16:42:26.911784: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab257100 of size 1024 next 541 20220311 16:42:26.911802: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ab257500 of size 77070336 next 621 20220311 16:42:26.911819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9afbd7500 of size 77070336 next 598 20220311 16:42:26.911847: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9b4557500 of size 77070336 next 640 20220311 16:42:26.911867: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9b8ed7500 of size 77070336 next 493 20220311 16:42:26.911885: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9bd857500 of size 77070336 next 403 20220311 16:42:26.911903: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9c21d7500 of size 77070336 next 662 20220311 16:42:26.911921: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9c6b57500 of size 77070336 next 396 20220311 16:42:26.911938: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9cb4d7500 of size 77070336 next 515 20220311 16:42:26.911955: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9cfe57500 of size 77070336 next 422 20220311 16:42:26.911973: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9d47d7500 of size 77070336 next 337 20220311 16:42:26.911992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9d9157500 of size 77070336 next 654 20220311 16:42:26.912011: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9ddad7500 of size 77070336 next 397 20220311 16:42:26.912030: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9e2457500 of size 77070336 next 453 20220311 16:42:26.912049: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9e6dd7500 of size 77070336 next 373 20220311 16:42:26.912068: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9eb757500 of size 77070336 next 551 20220311 16:42:26.912088: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9f00d7500 of size 25690112 next 675 20220311 16:42:26.912109: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9f1957500 of size 26314240 next 431 20220311 16:42:26.912130: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9f326fb00 of size 77690368 next 477 20220311 16:42:26.912149: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9f7c87100 of size 629760 next 345 20220311 16:42:26.912168: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9f7d20d00 of size 77097984 next 441 20220311 16:42:26.912186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a7900 of size 1024 next 554 20220311 16:42:26.912203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a7d00 of size 256 next 694 20220311 16:42:26.912221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a7e00 of size 256 next 628 20220311 16:42:26.912238: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a7f00 of size 1024 next 698 20220311 16:42:26.912256: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a8300 of size 1536 next 483 20220311 16:42:26.912274: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fd9fc6a8900 of size 1024 next 414 20220311 16:42:26.912292: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a8d00 of size 1024 next 650 20220311 16:42:26.912309: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a9100 of size 1536 next 665 20220311 16:42:26.912327: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fd9fc6a9700 of size 77070336 next 692 20220311 16:42:26.912345: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda01029700 of size 77070336 next 451 20220311 16:42:26.912363: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda059a9700 of size 77070336 next 597 20220311 16:42:26.912380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda0a329700 of size 77070336 next 521 20220311 16:42:26.912396: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda0eca9700 of size 77070336 next 688 20220311 16:42:26.912413: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda13629700 of size 77070336 next 488 20220311 16:42:26.912430: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda17fa9700 of size 77070336 next 623 20220311 16:42:26.912448: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda1c929700 of size 77070336 next 38 20220311 16:42:26.912466: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda212a9700 of size 77097984 next 693 20220311 16:42:26.912484: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda25c30300 of size 2359296 next 436 20220311 16:42:26.912504: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda25e70300 of size 2621440 next 612 20220311 16:42:26.912522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda260f0300 of size 2359296 next 667 20220311 16:42:26.912540: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fda26330300 of size 8257536 next 686 20220311 16:42:26.912560: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda26b10300 of size 411041792 next 646 20220311 16:42:26.912579: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda3f310300 of size 411041792 next 619 20220311 16:42:26.912598: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda57b10300 of size 411041792 next 357 20220311 16:42:26.912617: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda70310300 of size 411041792 next 388 20220311 16:42:26.912636: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fda88b10300 of size 411041792 next 590 20220311 16:42:26.912656: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdaa1310300 of size 102760448 next 695 20220311 16:42:26.912677: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdaa7510300 of size 102760448 next 448 20220311 16:42:26.912696: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdaad710300 of size 102760448 next 725 20220311 16:42:26.912713: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdab3910300 of size 102760448 next 685 20220311 16:42:26.912731: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdab9b10300 of size 102760448 next 584 20220311 16:42:26.912748: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdabfd10300 of size 102760448 next 652 20220311 16:42:26.912766: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdac5f10300 of size 102760448 next 678 20220311 16:42:26.912784: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdacc110300 of size 51380224 next 410 20220311 16:42:26.912802: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdacf210300 of size 25690112 next 663 20220311 16:42:26.912821: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdad0a90300 of size 6422528 next 417 20220311 16:42:26.912851: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdad10b0300 of size 6422528 next 407 20220311 16:42:26.912871: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdad16d0300 of size 6422528 next 727 20220311 16:42:26.912888: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fdad1cf0300 of size 6422528 next 346 20220311 16:42:26.912907: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdad2310300 of size 102760448 next 638 20220311 16:42:26.912924: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdad8510300 of size 51380224 next 651 20220311 16:42:26.912940: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdadb610300 of size 51380224 next 404 20220311 16:42:26.912958: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdade710300 of size 205520896 next 359 20220311 16:42:26.912976: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdaeab10300 of size 205520896 next 339 20220311 16:42:26.912995: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdaf6f10300 of size 205520896 next 560 20220311 16:42:26.913013: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb03310300 of size 205520896 next 622 20220311 16:42:26.913030: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb0f710300 of size 205520896 next 611 20220311 16:42:26.913047: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb1bb10300 of size 205520896 next 503 20220311 16:42:26.913065: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb27f10300 of size 51380224 next 516 20220311 16:42:26.913083: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb2b010300 of size 51380224 next 582 20220311 16:42:26.913102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb2e110300 of size 51380224 next 637 20220311 16:42:26.913121: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb31210300 of size 51380224 next 564 20220311 16:42:26.913140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb34310300 of size 51380224 next 474 20220311 16:42:26.913159: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb37410300 of size 102760448 next 666 20220311 16:42:26.913179: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb3d610300 of size 102760448 next 489 20220311 16:42:26.913198: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb43810300 of size 25690112 next 476 20220311 16:42:26.913217: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb45090300 of size 102760448 next 668 20220311 16:42:26.913236: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb4b290300 of size 102760448 next 680 20220311 16:42:26.913254: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb51490300 of size 25690112 next 616 20220311 16:42:26.913272: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb52d10300 of size 102760448 next 399 20220311 16:42:26.913289: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb58f10300 of size 25690112 next 340 20220311 16:42:26.913307: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb5a790300 of size 25690112 next 393 20220311 16:42:26.913325: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb5c010300 of size 154140672 next 723 20220311 16:42:26.913343: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb65310300 of size 25690112 next 567 20220311 16:42:26.913362: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb66b90300 of size 110231552 next 641 20220311 16:42:26.913382: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fdb6d4b0300 of size 692452608 next 18446744073709551615 20220311 16:42:26.913402: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of inuse Chunks by size:  20220311 16:42:26.913432: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 173 Chunks of size 256 totalling 43.2KiB 20220311 16:42:26.913456: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 51 Chunks of size 512 totalling 25.5KiB 20220311 16:42:26.913478: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 768 totalling 768B 20220311 16:42:26.913499: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 179 Chunks of size 1024 totalling 179.0KiB 20220311 16:42:26.913519: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 9 Chunks of size 1280 totalling 11.2KiB 20220311 16:42:26.913539: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 13 Chunks of size 1536 totalling 19.5KiB 20220311 16:42:26.913561: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1792 totalling 1.8KiB 20220311 16:42:26.913584: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 21 Chunks of size 8192 totalling 168.0KiB 20220311 16:42:26.913607: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 11264 totalling 11.0KiB 20220311 16:42:26.913629: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 16384 totalling 64.0KiB 20220311 16:42:26.913650: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 11 Chunks of size 27648 totalling 297.0KiB 20220311 16:42:26.913671: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 27904 totalling 27.2KiB 20220311 16:42:26.913692: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 28672 totalling 28.0KiB 20220311 16:42:26.913714: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 30720 totalling 30.0KiB 20220311 16:42:26.913737: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 41984 totalling 41.0KiB 20220311 16:42:26.913760: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 47104 totalling 46.0KiB 20220311 16:42:26.913783: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 48128 totalling 47.0KiB 20220311 16:42:26.913806: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 147456 totalling 576.0KiB 20220311 16:42:26.913828: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 233984 totalling 228.5KiB 20220311 16:42:26.913876: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 294912 totalling 1.12MiB 20220311 16:42:26.913899: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 472064 totalling 461.0KiB 20220311 16:42:26.913921: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 12 Chunks of size 589824 totalling 6.75MiB 20220311 16:42:26.913945: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 629760 totalling 615.0KiB 20220311 16:42:26.913966: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 801792 totalling 783.0KiB 20220311 16:42:26.913987: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 1179648 totalling 3.38MiB 20220311 16:42:26.914008: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1340416 totalling 1.28MiB 20220311 16:42:26.914028: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1474560 totalling 1.41MiB 20220311 16:42:26.914050: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 15 Chunks of size 2359296 totalling 33.75MiB 20220311 16:42:26.914071: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2387968 totalling 2.28MiB 20220311 16:42:26.914092: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2390528 totalling 2.28MiB 20220311 16:42:26.914113: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2621440 totalling 2.50MiB 20220311 16:42:26.914134: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3608576 totalling 3.44MiB 20220311 16:42:26.914155: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 4718592 totalling 18.00MiB 20220311 16:42:26.914175: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4816896 totalling 4.59MiB 20220311 16:42:26.914195: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 6422528 totalling 24.50MiB 20220311 16:42:26.914215: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7341056 totalling 7.00MiB 20220311 16:42:26.914236: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 16777216 totalling 32.00MiB 20220311 16:42:26.914258: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 9 Chunks of size 25690112 totalling 220.50MiB 20220311 16:42:26.914282: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 26314240 totalling 25.09MiB 20220311 16:42:26.914305: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 28835840 totalling 27.50MiB 20220311 16:42:26.914326: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32231680 totalling 30.74MiB 20220311 16:42:26.914347: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 40036096 totalling 38.18MiB 20220311 16:42:26.914368: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 10 Chunks of size 51380224 totalling 490.00MiB 20220311 16:42:26.914391: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 52428800 totalling 50.00MiB 20220311 16:42:26.914413: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 45 Chunks of size 77070336 totalling 3.23GiB 20220311 16:42:26.914437: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 77097984 totalling 147.05MiB 20220311 16:42:26.914460: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 77114880 totalling 73.54MiB 20220311 16:42:26.914483: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 77115392 totalling 73.54MiB 20220311 16:42:26.914506: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 77690368 totalling 74.09MiB 20220311 16:42:26.914528: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 82451456 totalling 78.63MiB 20220311 16:42:26.914551: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 82456576 totalling 78.64MiB 20220311 16:42:26.914573: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 87380992 totalling 83.33MiB 20220311 16:42:26.914594: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 89632256 totalling 85.48MiB 20220311 16:42:26.914617: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 90040832 totalling 85.87MiB 20220311 16:42:26.914639: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 95168768 totalling 90.76MiB 20220311 16:42:26.914660: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 96234496 totalling 91.78MiB 20220311 16:42:26.914682: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 99255296 totalling 94.66MiB 20220311 16:42:26.914705: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 99264512 totalling 94.67MiB 20220311 16:42:26.914726: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 13 Chunks of size 102760448 totalling 1.24GiB 20220311 16:42:26.914747: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 104173312 totalling 99.35MiB 20220311 16:42:26.914770: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 105158656 totalling 100.29MiB 20220311 16:42:26.914792: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 108818432 totalling 103.78MiB 20220311 16:42:26.914812: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 110231552 totalling 105.12MiB 20220311 16:42:26.914832: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 119567360 totalling 114.03MiB 20220311 16:42:26.914877: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 127390208 totalling 121.49MiB 20220311 16:42:26.914900: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 154140672 totalling 147.00MiB 20220311 16:42:26.914921: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 6 Chunks of size 205520896 totalling 1.15GiB 20220311 16:42:26.914943: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 411041792 totalling 1.91GiB 20220311 16:42:26.914965: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 692452608 totalling 660.37MiB 20220311 16:42:26.914986: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of inuse chunks: 11.08GiB 20220311 16:42:26.915006: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 11988434944 memory_limit_: 11988434944 available bytes: 0 curr_region_allocation_bytes_: 23976869888 20220311 16:42:26.915035: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:  Limit:                     11988434944 InUse:                     11902258688 MaxInUse:                  11902527488 NumAllocs:                     1785814 MaxAllocSize:               1866465280 Reserved:                            0 PeakReserved:                        0 LargestFreeBlock:                    0 20220311 16:42:26.915131: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***************************************************************************************************x 20220311 16:42:26.916684: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at conv_grad_input_ops.cc:408 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[128,256,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc Is there any clue on how to prevent this and if this error has or is been addressed ? I would be happy to provide further details if needed. Thanks.",  Please post your issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 It is difficult for us to track here in a closed issue. Thank you!
1837,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow unsupported within MSYS2?)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installed from (source or binary): wanted binary .whl via `python3 m pip install tensorflow`  TensorFlow version: any would be fine, was trying for 2.6.3  Python version: 3.9  Installed using virtualenv? pip? conda?: pip  CUDA/cuDNN version: no GPU **Describe the problem** On MSYS2, using pip via the mingw64compiled Python, the message is  It's the same if I just use the name eg `python3 m pip install tensorflow`. I have an extensive codebase that currently depends on MSYS2 and gcc in the Windows context, and we have a module that successfully uses Tensorflow on our Linux version of the code. As above, however, I can't use Tensorflow via Python under MSYS2 using the version Python that MSYS2 provides. I wonder if I could get some clarity on what the options are for Windows/MSYS2 therefore:  can tensorflow be built on Windows using gcc in MSYS2? The instructions, even if they use MSYS2, seem to be referring to MSVC as the compilier. What are the reasons why 'full' MSYS2 support is not possible? *  should I instead switch, if possible, to building my software to link against Windows Python instead of MSYS2 Python. This would be less convenient for users, but might work.  would it be possible for the situation regarding MSYS2 support to be clarified in the MSYS2 documentation somehow? [*] I note that https://www.tensorflow.org/install/source_windows says that there is some issue with path support in Windows by Bazel, but obviously Tensorflow builds find on Linux, where such paths exist. I)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jdpipe,Tensorflow unsupported within MSYS2?,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installed from (source or binary): wanted binary .whl via `python3 m pip install tensorflow`  TensorFlow version: any would be fine, was trying for 2.6.3  Python version: 3.9  Installed using virtualenv? pip? conda?: pip  CUDA/cuDNN version: no GPU **Describe the problem** On MSYS2, using pip via the mingw64compiled Python, the message is  It's the same if I just use the name eg `python3 m pip install tensorflow`. I have an extensive codebase that currently depends on MSYS2 and gcc in the Windows context, and we have a module that successfully uses Tensorflow on our Linux version of the code. As above, however, I can't use Tensorflow via Python under MSYS2 using the version Python that MSYS2 provides. I wonder if I could get some clarity on what the options are for Windows/MSYS2 therefore:  can tensorflow be built on Windows using gcc in MSYS2? The instructions, even if they use MSYS2, seem to be referring to MSVC as the compilier. What are the reasons why 'full' MSYS2 support is not possible? *  should I instead switch, if possible, to building my software to link against Windows Python instead of MSYS2 Python. This would be less convenient for users, but might work.  would it be possible for the situation regarding MSYS2 support to be clarified in the MSYS2 documentation somehow? [*] I note that https://www.tensorflow.org/install/source_windows says that there is some issue with path support in Windows by Bazel, but obviously Tensorflow builds find on Linux, where such paths exist. I",2022-02-06T23:06:03Z,stat:awaiting tensorflower type:build/install subtype:windows 2.6.0,closed,1,11,https://github.com/tensorflow/tensorflow/issues/54284," , Can you please confirm is there any specific reason to use tf v2.6.3, please try with stable v2.7 and let us know if you are facing same issue.Thanks!","Hi , I tried the following instead:  and also   and also   Note here that I am using the Python that is provided by MSYS2 (MinGW64), which is not the same as 'regular' CPython for Windows. It seems that the PyPI packages for Tensorflow are being deliberately blocked for use on MSYS2 somehow, and I'm not sure if that's deliberate (nothing in the documentation that I could see) or on purpose (would be nice to know the reason).","Further to the above, I note that I typed `python3 m pip debug verbose` and got the following output:  Meanwhile, if I type ` python3 m pip download tensorflow platform win32 pythonversion 3.9 vv nodeps` then I get this output:  Which clearly indicates that it is the 'platform tag' that is causing the problem here. In short, `cp39cp39win_amd64` doesn't match with `cp39cp39mingw_x86_64`.  The question is, does the platform need to be so restrictive here? If Python/Wheel wasn't blocking me, would this module actually work, or not? Is this a MSYS2 issue or a TensorFlow issue, I wonder? https://www.python.org/dev/peps/pep0425/platformtag","One more thing. If I manually unzip the .whl file  and then 'trick' Python into importing that package:  I opened `_pywrap_tensorflow_internal.pyd` using Dependency Walker, and found that the file that was missing was `PYTHON39.DLL`. On MinGW64, this file is called `libpython3.9.dll`, so I made a copy of the MinGW64 file, and renamed it to the expected name `python39.dll`. I then made a tiny bit more progress, namely:  This looks like it's getting a little bit further, and attempting to `dlopen` some CUDArelated stuff, and my system doesn't have CUDA, so I guess it fails. But presumably, it should not be crashing out of Python in this case. All in all, it looks like the `mingw_x86_64` 'platform tag' makes sense, because the DLL for Python has a different name on this platform compared to `win_amd64`, and that means that platformsensitive Python modules need to be compiled differently on this platform.  Having established that, are there any suggestions for how to compile Tensorflow such that it works with the MSYS2MinGW64 version of Python?"," , Can you please take a look at this link for the issue with the similar DLL error.It helps.Thanks!","Hi Tilakrayal  thanks for the link. However, my comments above were basically ""deep hacking"" of the available TensorFlow PyPI package to try to identify why it was refusing to install. It obviously still refuses to install on MSYS2, and our first need here is to figure out why that is, and then secondly, assuming that's correct, to figure out how to install it otherwise. I think that the issue is that MSYS2 doesn't support CUDA (AFAICT) and that, hence, the TensforFlow build process basically doesn't use the MSYS2 compiler, and tries to mostly not use MSYS2 at all, except for some bash tricks perhaps. I don't actually want/need CUDA for my application, so this is annoying for me, but I do understand. Our only path forward seems to be to attempt to install Windows Python, then install TensorFlow with the `pip` from Windows Python, then compile our local codebase against Windows Python instead of MinGW64 Python.","Hi  , The prebuilt binaries for windows are of win_amd64.whl type and seems not supporting for mingw_x86_64 architecture. You have option for Build from source using MSYS shell. Please refer to the documentation here for same. May be this can be helpful for your case. Please check and confirm if this solves your purpose. Thank you!","Hi , Thanks for the documentation link. It says that one can build using the MSYS2 bash shell. But would this produce a version of Tensorflow that is built using MINGW's GCC and links to the MINGW version of Python?  The instructions say to use the MSYS2 shell, but then all of the instructions for installing dependencies, specifically MS Visual C++ Build Tools, etc. (ie ""then follow the previous instructions for the Windows native command line""). I feel that the instructions are not really sufficiently clear. For example, if using MSYS2, and I want to use the MSYS2 version of Python, why would I install Windows Python? It would be helpful to have an endtoend set of instructions for building on MSYS2, using native MSYS2 GCC and Python, etc.  It would be even better if PyPI could host the resulting packages ;) But as noted, there may be issues with CUDA support in MSYS2 (not a big deal for me, but certainly it would be for others).","I'm in a similar situation. I need to adapt an open source application that was build under Windows using MYSYS2. Now we want to add a tensorflow model to the application. Two things are now bothering us. First tensorflow doesn't work under MYSYS2 (at least we can't get it to build/work). Secondly, From Tensorflow 2.11 GPU support is only under WSL2, but not anymore under Windows directly. GPU accelerated computing is really crucial.",Most likely this should be closed as duplicate of CC(Help! I have an issue when importing TF!) ,Are you satisfied with the resolution of your issue? Yes No
1775,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AutoGraph could not transform <function ...>)ï¼Œ å†…å®¹æ˜¯ ( **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary (`pip3 install tensorflowmacos`)  TensorFlow version (use command below): `unknown 2.7.0`  Python version: 3.9.9  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):   CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** **Describe the expected behavior** **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. Sorry, but I can't provide the full source code. **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,andmis,AutoGraph could not transform <function ...>," **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary (`pip3 install tensorflowmacos`)  TensorFlow version (use command below): `unknown 2.7.0`  Python version: 3.9.9  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):   CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** **Describe the expected behavior** **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook. Sorry, but I can't provide the full source code. **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-02-04T16:30:01Z,type:bug WIP comp:autograph TF 2.7,closed,0,14,https://github.com/tensorflow/tensorflow/issues/54274,"I did some investigation and the problem seems to be on the line `log_Ï€_Î¸: tf.Tensor` giving a type annotation for `log_Ï€_Î¸`. If I comment that line out, the issue goes away.","  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!","I cannot provide you with the entire Python program. As I said, the line that introduces the problem is the type annotation `log_Ï€_Î¸: tf.Tensor`, and it seems that the parser in TensorFlow cannot handle that line. To reproduce the problem, I think it should be enough to add a line that looks like `foo_bar: int` or `foo_bar: tf.Tensor` to any `.function`.","  When i tried to replicate your issue, I got `SyntaxError: invalid syntax` at `log_Ï€_Î¸: tf.Tensor`.  Help us in replicating your issue to investigate the root cause. Thanks!","What version of Python are you using? That line of code is a variable type annotation. Python type hinting support was introduced in 3.5, and variable type annotations specifically in 3.6. You can read more about Python type hints at . See also this StackOverflow thread: . Do you get that error with Python >= 3.7?",", I am using Python version 3.7. I get the syntax error when i am using `log_Ï€_Î¸: tf.Tensor`. Thanks!",Do you get a syntax error if you run the following Python program? ,", I didnâ€™t get syntax error on Python ~=3.8 for both below code snippet.  ","Ok, now try this: ",", I am able to replicate the issue with Tensorflow 2.8. Please find the Colab gist here.  Seems Autograph function failed to read `n: int`. Thanks!",The operation which you are trying is not supported in tf.function. You can silence this warning by decorating with `.autograph.experimental.do_not_convert` like below.  import tensorflow as tf ," It's not an operation, it's a type annotation.  The error message says, `Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, 'export AUTOGRAPH_VERBOSITY=10') and attach the full output.`  Type annotations like `n: int = 3` work fine, with no error. Why is `n: int = 3` ok but `n: int` not ok?  This is obviously a bug in the parser code.","Yes, this is a bug. A temporary workaround would be to initialize it, e.g. `log_Ï€_Î¸: tf.Tensor = None`.",Are you satisfied with the resolution of your issue? Yes No
1880,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(model.fit() bug when using a zipped Dataset as input for a multiple-input model)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  TensorFlow installed from (source or binary): pip  TensorFlow version (use command below): 2.9.0.dev20220202  Python version: 3.10.2 **Describe the current behavior** I have a custom model which takes 3 images as input I have 3 separate (currently unbatched as I debug this error) datasets, classes encoded as categorical, meaning each input tensor has shape ((x, y, z), (c,)) Trying to input the 3 datasets separately fails, either by inputting them as a dict mapping each ds to a named input `{""Input1"": ds1, {""Input2"": ds2, {""Input3"": ds3}`, or using a list `[ds, ds2, ds3]` I zip the three datasets. Testing the resulting dataset with (using the docs as guidance):  Outputs:  Seems to work, right? Every call to the iterator returns 3 elements. Well, when I use the zipped dataset as input of model_fit(), the first element in the tuple returned by the dataset object is treated as the input for the whole model, meaning that instead of using [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] as the input to the model, it uses [[x1, y1, z1], [c1,]], and the training fails. I've tried many approaches, like using `zipped_ds.as_numpy_iterator()` or `([ds1, ds2, ds3] for idx, (ds1, ds2, ds3) in enumerate(zipped_ds))`, but both fail as the returned item is empty **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the prob)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghylander,model.fit() bug when using a zipped Dataset as input for a multiple-input model,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  TensorFlow installed from (source or binary): pip  TensorFlow version (use command below): 2.9.0.dev20220202  Python version: 3.10.2 **Describe the current behavior** I have a custom model which takes 3 images as input I have 3 separate (currently unbatched as I debug this error) datasets, classes encoded as categorical, meaning each input tensor has shape ((x, y, z), (c,)) Trying to input the 3 datasets separately fails, either by inputting them as a dict mapping each ds to a named input `{""Input1"": ds1, {""Input2"": ds2, {""Input3"": ds3}`, or using a list `[ds, ds2, ds3]` I zip the three datasets. Testing the resulting dataset with (using the docs as guidance):  Outputs:  Seems to work, right? Every call to the iterator returns 3 elements. Well, when I use the zipped dataset as input of model_fit(), the first element in the tuple returned by the dataset object is treated as the input for the whole model, meaning that instead of using [[[x1, y1, z1], [c1,]], [[x2, y2, z2], [c2,]], [[x3, y3, z3], [c3,]]] as the input to the model, it uses [[x1, y1, z1], [c1,]], and the training fails. I've tried many approaches, like using `zipped_ds.as_numpy_iterator()` or `([ds1, ds2, ds3] for idx, (ds1, ds2, ds3) in enumerate(zipped_ds))`, but both fail as the returned item is empty **Standalone code to reproduce the issue** Provide a reproducible test case that is the bare minimum necessary to generate the prob",2022-02-04T14:17:54Z,type:bug comp:keras TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54271,I would like to work on this. I am new to this community could anyone please guide me how to go about solving this issue. ,Hi  ! You can go through model documentation to start contributing in this issue., ! Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thanks!,Ok  ! Closing this issue here as it will be tracked in Keras repo . Thanks!,Are you satisfied with the resolution of your issue? Yes No
1869,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GPU delegate for tflite not finding libOpenCL when deployed on linux)ï¼Œ å†…å®¹æ˜¯ (**System information**  *Have I written custom code (as opposed to using a stock example script provided in TensorFlow):* yes  *OS Platform and Distribution (e.g., Linux Ubuntu 16.04):* Built on and deployed to Ubuntu 18.04 x86_64   *TensorFlow installed from (source or binary):* Package of tflite built from source, deployed as deb  *TensorFlow version (use command below):* tflite v2.5.0 (2.6 was slower and we've not yet had time to assess 2.7)  *Python version:* n/a  *Bazel version (if compiling from source):* 4.2.1  *GCC/Compiler version (if compiling from source):* 7.5.03ubuntu1~18.04  *CUDA/cuDNN version:* n/a  *GPU model and memory:* Deployed to systems with nvidia GTX 1070 **Describe the current behavior** On a linux system the GPU delegate attempts to dynamically load libOpenCL but uses the ""dev"" name of the library.  When deploying a solution on an Ubuntu system the filename for the installed OpenCL library from package ""oclicdlibopencl1"" is `libOpenCL.so.1`. The symlink `libOpenCL.so` would only be present via the ""oclicdopencldev"" package which would usually be installed when building not deploying.  **Describe the expected behavior** When deployed it should load OpenCL from the library package and not the dev package. **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. https://github.com/tensorflow/tensorflow)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,parnham,GPU delegate for tflite not finding libOpenCL when deployed on linux,"**System information**  *Have I written custom code (as opposed to using a stock example script provided in TensorFlow):* yes  *OS Platform and Distribution (e.g., Linux Ubuntu 16.04):* Built on and deployed to Ubuntu 18.04 x86_64   *TensorFlow installed from (source or binary):* Package of tflite built from source, deployed as deb  *TensorFlow version (use command below):* tflite v2.5.0 (2.6 was slower and we've not yet had time to assess 2.7)  *Python version:* n/a  *Bazel version (if compiling from source):* 4.2.1  *GCC/Compiler version (if compiling from source):* 7.5.03ubuntu1~18.04  *CUDA/cuDNN version:* n/a  *GPU model and memory:* Deployed to systems with nvidia GTX 1070 **Describe the current behavior** On a linux system the GPU delegate attempts to dynamically load libOpenCL but uses the ""dev"" name of the library.  When deploying a solution on an Ubuntu system the filename for the installed OpenCL library from package ""oclicdlibopencl1"" is `libOpenCL.so.1`. The symlink `libOpenCL.so` would only be present via the ""oclicdopencldev"" package which would usually be installed when building not deploying.  **Describe the expected behavior** When deployed it should load OpenCL from the library package and not the dev package. **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. https://github.com/tensorflow/tensorflow",2022-02-04T11:17:23Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TFLiteGpuDelegate TF 2.13,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54269,Hi   Th commit https://github.com/tensorflow/tensorflow/commit/6e58460909fdcb9b64e606eca9590014576f4caf which updates build rule to use opencl icd loader to run OpenCL tests under gLinux. Can you please check in the latest version and confirm the same? Thanks.,"Hi ,  I'm afraid I can't check since we're unable to build the v2.13.0rc2 GPU delegate under linux due to this change (I think) which causes a linker error: https://github.com/tensorflow/tensorflow/commit/33722bc185e676c99f738790ef35db8479f2f7d4  I'm not very familiar with Bazel and our build of the tflite libraries occurs in a container so it's not that easy for me to hack around with. Kind regards, Dan","I managed to hack our build system to create a package for v2.13.0rc2 and have tested it. Unfortunately it still does not dynamically load the opencl library correctly  unless I install the `oclicdopencldev` package manually when deploying to other systems. It looks like it's still `dlopen`ing the dev version of the opencl icd loader library ""libOpenCL.so"" instead of ""libOpenCL.so.1"" File lists: Library package on Ubuntu: https://packages.ubuntu.com/jammy/amd64/oclicdlibopencl1/filelist Dev package on Ubuntu: https://packages.ubuntu.com/jammy/amd64/oclicdopencldev/filelist I tested a change in our package build script which modifies the opencl_wrapper as follows:  which makes it work as expected.","  Thanks for your `sed` solution, because I experienced the same problem. The simple program:  outputted before your `sed` fix:  And outputted after your `sed` fix:  Tested with TfLite 2.13 on Ubuntu 23.04 with an Nvidia Geforece GT1030. ","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
805,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can't build libtensorflowlite.so with `--config=elinux_armhf` - external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:36:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int')ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): source  TensorFlow version: r2.8, e994fb9c3ad250d38fd07511aaa445eda728f9af  Python version: Python 3.8.10  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 **Describe the problem** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,reuben,Can't build libtensorflowlite.so with `--config=elinux_armhf` - external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:36:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int',"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): source  TensorFlow version: r2.8, e994fb9c3ad250d38fd07511aaa445eda728f9af  Python version: Python 3.8.10  Bazel version (if compiling from source): 4.2.1  GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 **Describe the problem** ",2022-02-04T11:11:34Z,stat:awaiting response type:build/install stale comp:lite TF 2.8,closed,0,12,https://github.com/tensorflow/tensorflow/issues/54268,FWIW the build works fine on the `master` branch.,,"Hm, but that change is already applied on r2.8... Let me try commenting it out.","Oh wait, it isn't! Applying 4ec84aa99372ca7899fb12715edd2bfe3c947c88 on r2.8 fixes the build for me!",Tensorflow 2.8 branch cut of rc0 and rc1 was made before the above commit on Dec 29th.  You need to wait for the next branch release for these changes to reflect. Till then you can build using master branch. Thanks!,"I understand that it missed the branch, but considering 2.8.0 is already out as a stable release and that target is documented explicitly, you might want to consider uplifting the change.","Hi, Since the fix will be available in Tensorflow 2.9 and it will be released soon and I'm closing the linked PR, since this issue is not critical for cherrypick.  You can close this issue. Thanks!",I'd say being able to build the project for a supported target is quite critical ğŸ˜…,You can build against the master branch and release of Tensorflow 2.9 is around the corner.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1867,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build with exception ""no such package '@llvm-raw//utils/bazel':"" .)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No TensorFlow installed from (source or binary):source TensorFlow version: r2.8 Python version: python3.9 Installed using virtualenv? pip? conda?:pip Bazel version (if compiling from source): bazel 3.7.2 GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang1300.0.29.3) CUDA/cuDNN version: no enabled GPU model and memory:: no enabled(Just CPU model) **Describe the problem** The ""bazel build config=dbg strip=never c dbg copt='g' cxxopt='g' //tensorflow/tools/pip_package:build_pip_package"" always failed at the step pull and install the package from url ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz"" with msg ""no such package 'raw//utils/bazel':"" , maybe  it is too large , can it be solved by download the package  to local and install from local ? If so ,  I want to download it to local firstly and install from local , but how to install it  from local ? any file need to be modified ? **Provide the exact sequence of commands / steps that you executed before running into the problem** **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If includin)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,shenyufengt,"Build with exception ""no such package '@llvm-raw//utils/bazel':"" .","Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No TensorFlow installed from (source or binary):source TensorFlow version: r2.8 Python version: python3.9 Installed using virtualenv? pip? conda?:pip Bazel version (if compiling from source): bazel 3.7.2 GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang1300.0.29.3) CUDA/cuDNN version: no enabled GPU model and memory:: no enabled(Just CPU model) **Describe the problem** The ""bazel build config=dbg strip=never c dbg copt='g' cxxopt='g' //tensorflow/tools/pip_package:build_pip_package"" always failed at the step pull and install the package from url ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz"" with msg ""no such package 'raw//utils/bazel':"" , maybe  it is too large , can it be solved by download the package  to local and install from local ? If so ,  I want to download it to local firstly and install from local , but how to install it  from local ? any file need to be modified ? **Provide the exact sequence of commands / steps that you executed before running into the problem** **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If includin",2022-02-03T17:11:43Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.8,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54258,", Issue is with Bazel version. Latest Tensorflow version r2.8 requires Bazel 4.2.1 or 5.0.0. For Xcode version is 13, download llvm 12 from brew and tried to compile using the llvm 12 as per build configuration. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to install llvm-project from local ? )ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No TensorFlow installed from (source or binary):source TensorFlow version: r2.7 Python version: python3.9 Installed using virtualenv? pip? conda?:pip Bazel version (if compiling from source): bazel 3.7.2 GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang1300.0.29.3) CUDA/cuDNN version: no enabled GPU model and memory:: no enabled(Just CPU model) **Describe the problem** When I run the command "" bazel build config=dbg strip=never c dbg copt='g' cxxopt='g' //tensorflow/tools/pip_package:build_pip_package"", it would pull package from ""https://github.com/llvm/llvmproject/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz"" ,maybe it is too large,  the bazel build  command always failed at this step, so how to install this package from local ? As I can download this package  to local successfully. **Provide the exact sequence of commands / steps that you executed before running into the problem** **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (mos)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,shenyufengt,How to install llvm-project from local ? ,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No TensorFlow installed from (source or binary):source TensorFlow version: r2.7 Python version: python3.9 Installed using virtualenv? pip? conda?:pip Bazel version (if compiling from source): bazel 3.7.2 GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang1300.0.29.3) CUDA/cuDNN version: no enabled GPU model and memory:: no enabled(Just CPU model) **Describe the problem** When I run the command "" bazel build config=dbg strip=never c dbg copt='g' cxxopt='g' //tensorflow/tools/pip_package:build_pip_package"", it would pull package from ""https://github.com/llvm/llvmproject/archive/55c71c9eac9bc7f956a05fa9258fad4f86565450.tar.gz"" ,maybe it is too large,  the bazel build  command always failed at this step, so how to install this package from local ? As I can download this package  to local successfully. **Provide the exact sequence of commands / steps that you executed before running into the problem** **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (mos",2022-02-03T17:09:42Z,stat:awaiting response type:build/install stale subtype:bazel TF 2.7,closed,0,7,https://github.com/tensorflow/tensorflow/issues/54257," , Can you please take a look at this issue with the similar error and it is fixed in the latest build.It helps.Thanks!","> tilakrayal The issue also happened at r2.8, which has the fix.so they are different issue. My question is is there is any way to install the llvmproject from local ? such as download it to local manually and install it from there ."," , Can you please take a look at this link to install llvmproject.It helps.Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"> > tilakrayal > > The issue also happened at r2.8, which has the fix.so they are different issue. My question is is there is any way to install the llvmproject from local ? such as download it to local manually and install it from there . A solution to ""download it to local manually and install it from there"": add an additional attr(e.g. ""local_files"") to ""_tf_http_archive"" in third_party/repo.bzl, and call ""ctx.extract"" instead of ""ctx.download_and_extract"" in ""_tf_http_archive_impl"". Below is a quick patch based on v2.12.1, which also should be compatible with other release versions: 0001patchsupporttoextractlocaldependenciesdirectly.patch Then, add the local file path for llvm > \\\ a/third_party/llvm/workspace.bzl > \+\+\+ b/third_party/llvm/workspace.bzl > @@ 15,6 +15,9 @@ def repo(name): >              ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/{commit}.tar.gz"".format(commit = LLVM_COMMIT), >              ""https://github.com/llvm/llvmproject/archive/{commit}.tar.gz"".format(commit = LLVM_COMMIT), >          ], > \+       local_files = [ > \+            ""/dl/llvm10939d1d580b9d3c9c2f3539c6bdb39f408179c0.tar.gz"", > \+        ], >          build_file = ""//third_party/llvm:llvm.BUILD"", Have a try!"
1985,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(InvalidArgumentError:  Size 1 must be non-negative, not -13 	 [[{{node gradient_tape/mean_squared_error/sub-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]] [Op:__inference_train_function_7208])ï¼Œ å†…å®¹æ˜¯ (I am trying to train my neuronal Network but get this error everytime and i dont know what it means. I am using tenserfow 2.7.0 in google colab. Here is my NN architecture (i am training a CNN to output two informations  steering and speed. Is something wrong with the mean_squared_error?  `class v2Model():     def __init__(self):         self.logdir = os.path.join(TRAIN_LOGS_PATH, datetime.datetime.now().strftime(""runV2_%d%b%Y__%H%M%S_%f""))         self.epochs = 10         self.batchSizeTrain = 100         self.batchSizeVal = 25     def createModel(self) > None:         inputs = Input(shape=(80, 320, 1))         self.model = Rescaling(scale=1./127.5, offset=1.)(inputs)         self.model = Conv2D(filters=16, kernel_size=(5, 5), padding='VALID', activation='elu')(self.model)         self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)         self.model = Conv2D(filters=32, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)         self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)         self.model = Conv2D(filters=64, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)         self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)         self.model = Dense(19456, use_bias=True)(self.model)         self.model = Dropout(rate=0.2)(self.model)         self.model = Dense(500, use_bias=True)(self.model)         dropV2 = Dropout(rate=0.2)(self.model)         headSteeringV2 = Dense(1, activation=""linear"", name=""output_ster"")(dropV2)         headSpeedV2 = Dense(1, activation=""linear"", name=""output_acc"")()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Salman-F,"InvalidArgumentError:  Size 1 must be non-negative, not -13 	 [[{{node gradient_tape/mean_squared_error/sub-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]] [Op:__inference_train_function_7208]","I am trying to train my neuronal Network but get this error everytime and i dont know what it means. I am using tenserfow 2.7.0 in google colab. Here is my NN architecture (i am training a CNN to output two informations  steering and speed. Is something wrong with the mean_squared_error?  `class v2Model():     def __init__(self):         self.logdir = os.path.join(TRAIN_LOGS_PATH, datetime.datetime.now().strftime(""runV2_%d%b%Y__%H%M%S_%f""))         self.epochs = 10         self.batchSizeTrain = 100         self.batchSizeVal = 25     def createModel(self) > None:         inputs = Input(shape=(80, 320, 1))         self.model = Rescaling(scale=1./127.5, offset=1.)(inputs)         self.model = Conv2D(filters=16, kernel_size=(5, 5), padding='VALID', activation='elu')(self.model)         self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)         self.model = Conv2D(filters=32, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)         self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)         self.model = Conv2D(filters=64, kernel_size=(3, 3), padding='VALID', activation='elu')(self.model)         self.model = MaxPooling2D(pool_size=(2, 2), padding='VALID')(self.model)         self.model = Dense(19456, use_bias=True)(self.model)         self.model = Dropout(rate=0.2)(self.model)         self.model = Dense(500, use_bias=True)(self.model)         dropV2 = Dropout(rate=0.2)(self.model)         headSteeringV2 = Dense(1, activation=""linear"", name=""output_ster"")(dropV2)         headSpeedV2 = Dense(1, activation=""linear"", name=""output_acc"")(",2022-02-03T13:55:31Z,stat:awaiting response type:support stale comp:model TF 2.7,closed,0,10,https://github.com/tensorflow/tensorflow/issues/54255,"F   In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!","  **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no  TensorFlow installed from (source or binary): google colab pre installed  TensorFlow version (use command below): 2.7.0  Python version: 3.6.9  Bazel version (if compiling from source): no idea  GCC/Compiler version (if compiling from source): no idea  CUDA/cuDNN version: google colab pre installed  GPU model and memory: google colab switches sometimes You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` 3. v2.7.00gc256c071bb2 2.7.0 **Describe the current behavior** I can't train my CNN because of an error i can't explain (or see where it is) **Describe the expected behavior** The training of my CNN should start and the epochs should work **Contributing**  Do you want to contribute a PR? (yes/no): no idea  Briefly describe your candidate solution(if contributing): no idea **Standalone code to reproduce the issue** Here is my notebook https://gist.github.com/SalmanF/015a7658333e8afe8d6c80c7b8ce1d2a **Other info / logs**  Epoch 1/10  InvalidArgumentError                      Traceback (most recent call last) [](https://localhost:8080/) in ()      74 v2CNN = v2Model()      75 v2CNN.createModel() > 76 v2CNN.trainModel(x_train, y_train_ster, y_train_acc, x_val, y_val_ster, y_val_acc)      77 v2CNN.uploadTraining()      78 v2CNN.saveModel() 2 frames /usr/local/lib/python3.7/distpackages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      57     ctx.ensure_initialized()      58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, > 59                                         inputs, attrs, num_outputs)      60   except core._NotOkStatusException as e:      61     if name is not None: InvalidArgumentError:  Size 1 must be nonnegative, not 12 	 [[{{node gradient_tape/mean_squared_error_1/sub1ReshapeNHWCToNCHWLayoutOptimizer}}]] [Op:__inference_train_function_5310]",Sorry for not filling it out immediately :)    The problem just appears when i try to train the v2 CNN. The CNN v1 is working,"F Thank you for the update! I tried to run your code on colab using TF v2.7.0,2.8.0  and faced a different error .Could you please have a look at the gist1,  gist2 and confirm the same? Let me know if I'm missing something to replicate the issue.Thanks!"," Hi, yes i think you are missing my data. I can try to give you some data that you have to load into your google colab enviorment. A problem may accur with the paths i decleared at the beginning. You have to change them to access the data (or if you put them in your google drive  and conect it to google colab it should be the same) Download a few data samples here: https://nextcloud.dhbwstuttgart.de/index.php/s/akmrSWTFA6zDiQm Maybe you can also create some dummy data. Please feel free to contact me, if you need more :)"," Was able to reproduce the issue on colab using TF v2.7.0 and 2.8.0 ,please find the attached gists for reference.Thanks!","F, Looks like issue is with your input data shape I tried to replicate your issue with minimal code. Below sample code works  **Output** ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1657,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dataset error when using with XLA device (No unary variant device copy function found))ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3 LTS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA  TensorFlow installed from (source or binary): binary   TensorFlow version (use command below):  v2.8.0rc132g3f878cff5b6 2.8.0  tensorflow2.8.0cp38cp38manylinux2010_x86_64.whl  Python version: 3.8  Bazel version (if compiling from source): NA  GCC/Compiler version (if compiling from source): NA  CUDA/cuDNN version: NA  GPU model and memory: NA **Describe the current behavior** error in script:  tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:XLA_CPU:0 in order to run __inference_f_27: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper [Op:__inference_f_27] **Describe the expected behavior** script runs with no errors and produces values as expected  **Contributing**  Do you want to contribute a PR? (yes/no): no  internal error   Briefly describe your candidate solution(if contributing): no  internal error  **Standalone code to reproduce the issue** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mmaor123,Dataset error when using with XLA device (No unary variant device copy function found),"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3 LTS  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA  TensorFlow installed from (source or binary): binary   TensorFlow version (use command below):  v2.8.0rc132g3f878cff5b6 2.8.0  tensorflow2.8.0cp38cp38manylinux2010_x86_64.whl  Python version: 3.8  Bazel version (if compiling from source): NA  GCC/Compiler version (if compiling from source): NA  CUDA/cuDNN version: NA  GPU model and memory: NA **Describe the current behavior** error in script:  tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:XLA_CPU:0 in order to run __inference_f_27: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper [Op:__inference_f_27] **Describe the expected behavior** script runs with no errors and produces values as expected  **Contributing**  Do you want to contribute a PR? (yes/no): no  internal error   Briefly describe your candidate solution(if contributing): no  internal error  **Standalone code to reproduce the issue** ",2022-02-03T13:26:35Z,stat:awaiting response type:bug stale comp:data comp:xla TF 2.8,closed,1,7,https://github.com/tensorflow/tensorflow/issues/54254," , I was able to reproduce the issue in tf v2.8, v2.7 and  nightly.Please find the gist here.","adding more context:  it appears like this failure is related to the fact that XLA is (rightfully) not supporting dataset structs and operations. however, the failure is not clear and it would be better to try and generate a clear failure statement that explains that.  the user is expected to explicitly place all dataset related work on the CPU and hand over only pure tensors to the function, when mustcompile is used (in this case, the mustcompile is implicitly triggered because of the explicit placement on an XLA device). ", ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
265,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update ops.py)ï¼Œ å†…å®¹æ˜¯ (In this removing `'s'` doesn't make any change in the explanation)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tilakrayal,Update ops.py,In this removing `'s'` doesn't make any change in the explanation,2022-02-03T09:33:35Z,size:XS invalid,closed,0,1,https://github.com/tensorflow/tensorflow/issues/54252,"We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest. CC ,  "
1847,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation Fault After Canonicalization Pass)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source.   TensorFlow version (use command below):  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):GCC 8.3  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** Segment Fault. **Describe the expected behavior** Run canonicalization pass successfully. **Contributing**  Do you want to contribute a PR? (yes/no): I have some findings but I can't solve it independently.  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue**  First create the `reproduce.mlir` as below.   `bazel  build //tensorflow/compiler/mlir/hlo:mlirhloopt`  `./bazelbin/tensorflow/compiler/mlir/hlo/mlirhloopt canonicalize reproduce.mlir` **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  backtrace: `readBits (rawData=0x0, bitPos=0, bitWidth=1)`  If `mlirhloopt` is built by CMake, the command could run successfully.  I)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,yaochengji,Segmentation Fault After Canonicalization Pass,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source.   TensorFlow version (use command below):  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):GCC 8.3  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** Segment Fault. **Describe the expected behavior** Run canonicalization pass successfully. **Contributing**  Do you want to contribute a PR? (yes/no): I have some findings but I can't solve it independently.  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue**  First create the `reproduce.mlir` as below.   `bazel  build //tensorflow/compiler/mlir/hlo:mlirhloopt`  `./bazelbin/tensorflow/compiler/mlir/hlo/mlirhloopt canonicalize reproduce.mlir` **Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  backtrace: `readBits (rawData=0x0, bitPos=0, bitWidth=1)`  If `mlirhloopt` is built by CMake, the command could run successfully.  I",2022-02-03T00:03:28Z,stat:awaiting response type:build/install subtype:bazel,closed,0,11,https://github.com/tensorflow/tensorflow/issues/54249,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!"," Thanks for your reply. This content is already from the `bug issue template`. It is a `mlir` and `bazel` related bug, I hope the additional infomation could help you find the right person to assign the issue to.","Is this still happening? I just tried it, and for me it seems to work now.","> Is this still happening? I just tried it, and for me it seems to work now.  Thanks. But this bug still appears on my side. I'm using the latest commit `3362b358bbad2e6d`.","My guess is that it depends on which compiler you use. Are you using gcc by any chance? And if yes, which version?",I'm using gcc 8.3.0. ,I think we have seen segfaults also in other cases when using the gcc 8.3 compiler. Can you please try a newer version? gcc9.3.1 should work for example: https://github.com/tensorflow/build/blob/nitin/manylinux2014/tf_sig_build_dockerfiles/builder.devtoolset/build_devtoolset.shL115,"Thanks,  . I tried gcc11 and it works.","Hi  , As the issue seems resolved with latest GCC version can we mark it as closed? Please feel free to close if resolved already and let us know if still facing any issue. Thanks!"," it is closed, thanks.",Are you satisfied with the resolution of your issue? Yes No
1478,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow accesses .hdf5 weights file after it's been loaded)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  Redhat 7.9  TensorFlow installed from (source or binary): binary (conda)  TensorFlow version (use command below): On all versions from 2.2 to 2.7 (2.7.0 for the latest)  Python version: 3.8  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 10.2.0 **Describe the current behavior** When loading a keras model from a temporary `.hdf5` weights file, the weights are read successfully read into the model, but Tensorflow outputs a DATA_LOSS warning after the `load_weights` method has finished executing indicating that Tensorflow is still trying to read the file. **Describe the expected behavior** I expect Tensorflow to not touch the weights file once the `load_weights` function as returned. **Contributing**  Do you want to contribute a PR? (yes/no): no **Standalone code to reproduce the issue**  **Other info / logs** This the output I get by running this code locally:  I tried running this code on colab, however I can't see any of the tensorflow logging lines I see on my local machine.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,krafczyk,Tensorflow accesses .hdf5 weights file after it's been loaded,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  Redhat 7.9  TensorFlow installed from (source or binary): binary (conda)  TensorFlow version (use command below): On all versions from 2.2 to 2.7 (2.7.0 for the latest)  Python version: 3.8  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 10.2.0 **Describe the current behavior** When loading a keras model from a temporary `.hdf5` weights file, the weights are read successfully read into the model, but Tensorflow outputs a DATA_LOSS warning after the `load_weights` method has finished executing indicating that Tensorflow is still trying to read the file. **Describe the expected behavior** I expect Tensorflow to not touch the weights file once the `load_weights` function as returned. **Contributing**  Do you want to contribute a PR? (yes/no): no **Standalone code to reproduce the issue**  **Other info / logs** This the output I get by running this code locally:  I tried running this code on colab, however I can't see any of the tensorflow logging lines I see on my local machine.",2022-02-02T22:03:21Z,stat:awaiting response type:bug comp:keras comp:ops TF 2.5,closed,0,10,https://github.com/tensorflow/tensorflow/issues/54248,"This might be expected behavior, but then I'd like a way to determine that tensorflow is finished reading the file before releasing the temporary file."," , I was able to execute the mentioned code with out any Data loss warnings/issues.Please find the gist of it here."," Yes, I think its because none of the TF logging messages are visible on colab. I ran it on colab before submitting as well, and also didn't see the warnings. That said, every version of tensorflow I've tried on my local machine produces this error message."," I worked around the colab logging issue by first writing the script to a file, and then using `!` magic commands to execute the script. If you know a proper way to show logging on colab please let me know. Here is my gist"," , I was able to reproduce the issue in tf v2.5, v2.7 and nightly.Please find the gist of it here.",", Please post this issue on kerasteam/keras repo. To know more see https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999.Thanks!"," Sure. Since the error message is originating from the tensorflow C library, i figured this would be a better place to start.",I've opened the issue at kerasteam/keras here: kerasteam/keras CC(Cannot compile with Visual Studio 15),", Can you please close this issue, since it is tracked there. Thanks!",Are you satisfied with the resolution of your issue? Yes No
1896,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(On-device-training fails using Tflite_runtime: Node number 54 (FlexReluGrad) failed to prepare.)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Custom embedded Linux distribution (Kernel 5.15)**  Mobile device: **Raspberry Pi 4B**  TensorFlow installed from (source or binary): **Source (Building on Tflite_runtime using pip_package scripts)**  TensorFlow version (use command below): **2.7.0**  Python version: **3.8**  Bazel version (if compiling from source): **N/A**  GCC/Compiler version (if compiling from source): **9.3 (OpenEmbedded GNU Toolchain)**  CUDA/cuDNN version: **N/A**  GPU model and memory: **N/A** **Describe the current behavior** I am following the tutorial on how to do ondevicetraining. The first step was to create and train the Fashion_mnist model on google Colab which was successful since I managed to download as an output the tflite model after converting (I made sure to mention all the necessary signatures while saving the model). The documentation is providing an example only on Java for android Apps, but I'm trying to explore whether this is feasible with tflite_runtime wheel. I sent then the tflite model to the target (raspberry pi) where the Tflite_runtime has been installed using this article and I'm trying to run the training function by feeding my neural network with arrays of zeros just to prove that it is working. This is the code snippet I'm running on my target.     The script fails at the train function call and return the following traces:  **Describe the expected behavior** The expected behavior is to get a comp)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OAHLSTM,On-device-training fails using Tflite_runtime: Node number 54 (FlexReluGrad) failed to prepare.,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Custom embedded Linux distribution (Kernel 5.15)**  Mobile device: **Raspberry Pi 4B**  TensorFlow installed from (source or binary): **Source (Building on Tflite_runtime using pip_package scripts)**  TensorFlow version (use command below): **2.7.0**  Python version: **3.8**  Bazel version (if compiling from source): **N/A**  GCC/Compiler version (if compiling from source): **9.3 (OpenEmbedded GNU Toolchain)**  CUDA/cuDNN version: **N/A**  GPU model and memory: **N/A** **Describe the current behavior** I am following the tutorial on how to do ondevicetraining. The first step was to create and train the Fashion_mnist model on google Colab which was successful since I managed to download as an output the tflite model after converting (I made sure to mention all the necessary signatures while saving the model). The documentation is providing an example only on Java for android Apps, but I'm trying to explore whether this is feasible with tflite_runtime wheel. I sent then the tflite model to the target (raspberry pi) where the Tflite_runtime has been installed using this article and I'm trying to run the training function by feeding my neural network with arrays of zeros just to prove that it is working. This is the code snippet I'm running on my target.     The script fails at the train function call and return the following traces:  **Describe the expected behavior** The expected behavior is to get a comp",2022-02-02T13:03:15Z,stat:awaiting tensorflower type:bug comp:lite TF 2.7,closed,0,15,https://github.com/tensorflow/tensorflow/issues/54244,"Another (probably helpful) and relevant information, delegate/flex sources seems to integrated in the  **`_pywrap_tensorflow_interpreter_wrapper.so`** as it can be shown below: "," , I was facing different error while executing the code.Please find the gist of it here.Please provide the complete code and dependencies to reproduce the issue.Thanks!"," , Thank your for your answer, I have prepared a a gist here with the whole code.  Unfortunately, it is working just fine in the google colab environment and the train function returns the expected value, so that's not really the way to go. If you want to reproduce exactly the same issue, I would suggest you run the code on target by doing the following: 1. Run the notebook in the link I sent you,  2. then from **Files** section download the **fashion_mnist_model.tflite** file.  3. Send this model file to your board (Raspberry Pi for example) via scp or any other file transfer protocol. 4. From your board's terminal, run the following command. `BOARD$ pip3 install tfliteruntime` 5. Then you will need to open a Python prompt  `BOARD$ python3` 6. Type the following instructions:  That's the output showing the error: "," , I was using colab to reproduce the issue.Can you please provide the  tflite file along with the code which you are facing issue.It helps to debug the issue.Thanks!"," , The issue is not reproduceable on google colab, it will work as expected with no errors. You will need to run it on an armv7 or aarch64 architecture target to reproduce the issue, since the goal of this is to do ondevicetraining. Here is a link to the tflite file."," , I was able to execute the code in colab without any issues.Please find the gist of it here.","   I already mentioned that the code will run without any issues on Google Colab and the issue is not reproduceable on its environement, I'm actually facing the issue while running the code on device (on a raspberry Pi aarch64 architecture). The bug might be in the tfliteruntime python wheel of aarch64. ","Hello  , Have you been able to reproduce the issue on some device ? I can confirm you that it is reproduceable on both armv8 and armv7 architectures. Thank you in advance,","Hello  ,    I'm doubting this is isn't a bug but more of a missing feature. I have tried an interesting experimentation, I tried running the same script on my x86 host machine with no tensorflow package installed, only the tfliteruntime Python pip_package installed, this reproduces the same issue. After installing Tensorflow package, the error traces disappear. This made me think about some operation features missing in the interpreter I build using CMake. I was trying to add debug some traces into the tensorflow source code to be able to figure it out. I have reached some conclusions:  The traces I added were mainly near the intepreter function and allowed me to notice that the FlexReluGrad had no prepare function as you may notice below:    Based on the documentation, in order to do ondevicetraining, it is necessary to add the **Flex Delegate** to the interpreter during the build and select some tensorflow ops so the FlexOps could be understood by the interpreter (Hence the error, I'm getting each time : **_Node number 54 (FlexReluGrad) failed to prepare._**   By getting a closer look inside the libtensorflowlite.a generated by  CMake, I can see that none of the flex delegate function has been integrated in the build since the variable TFLITE_DELEGATES_FLEX_SRCS is empty.  By digging deeper inside the Bazel BUILD scripts, I was trying to duplicate the same build dependencies and sources to build using CMake. I have also noticed that many dependencies from tensorflow/core/framework are used. => To make it short, I'm trying to build a shared library containing the flex delegates with the _with_select_tf_ops=true_ but using **Cmake** instead of **Bazel** since it's not an option for me. This may be pushed as a new feature in the tensorflow/lite/CMakeList.txt  upon completion, so I'm looking for some guidance on the dependencies and sources to build for that purpose. ", triaging for ondevice related issues.,"Hi, Sorry for the late reply. ReluGrad isn't a supported builtin op of TF Lite. So you will need to link in the Flex delegate to make it able to run ondevice.","Hello ,  First of all, thank you for your reply. That's what I figured out, then I tried to build a new libtensorflowliteflex.so library with the Flex Delegates and the necessary TF Ops kernels (ReluOp, SaveOp and RestoreOp in this case) using SELECTIVE_REGISTRATION Mode, but the build does not seem to be working properly for aarch64 as stated in this issue. Also, do you have a clearer example on how to link the flex delegate to my interpreter ? I tried doing this :   For now, I cannot tell if it is working properly or not because I still don't have a properly working shared library libtensorflowliteflex.so to link against. Thank you for your support, Othmane",Will https://github.com/tensorflow/tensorflow/issues/52018 help you resolve the build issue?,"Hello ,  Sorry, I was mistaken in the link of the issue. The one I mentioned was actually closed and the CC(libtensorflowlite target compilation failed) did help, hence it was closed. The issue I'm facing during the build is related to some dependency ""icu"". A ticket has been opened in here but no answers so far.  Thank you again for your support, ",Are you satisfied with the resolution of your issue? Yes No
775,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FIFOQueue as a part of custom layer)ï¼Œ å†…å®¹æ˜¯ (Hello,    I need a queue as a part of my custom input layer in Transformer for Shortterm memory purposes. I need to store the coming features ""in first in first out"" manner and remove the oldies features when it's full and clean all content if it's needed. I see here something like that for working with Tensor, but is it a good choice for me and contains all that I need? Is the FIFOQueue faster than using Python's deque or Numpy based queue directly in a custom layer? Compare Python's deque collection with TF's FIFOQueue:  Thanks, have a nice day.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,markub3327,FIFOQueue as a part of custom layer,"Hello,    I need a queue as a part of my custom input layer in Transformer for Shortterm memory purposes. I need to store the coming features ""in first in first out"" manner and remove the oldies features when it's full and clean all content if it's needed. I see here something like that for working with Tensor, but is it a good choice for me and contains all that I need? Is the FIFOQueue faster than using Python's deque or Numpy based queue directly in a custom layer? Compare Python's deque collection with TF's FIFOQueue:  Thanks, have a nice day.",2022-02-02T06:42:45Z,type:feature comp:ops,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54241, ,Thanks for opening this feature request. Development of keras moved to separate repository https://github.com/kerasteam/keras/issues Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,"Okej, I create a feature request there: https://github.com/kerasteam/keras/issues/16015. Have a nice day."
1878,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tensorflow.python.keras.api._v1.keras.layers' has no attribute 'experimental')ï¼Œ å†…å®¹æ˜¯ (**System information**  Linux Ubuntu 16.04  TensorFlow version: 1.15.5  Python version: 3.7  Installed using virtualenv **Describe the problem** Command  import tensorflow_model_optimization Error that I get File ""test.py"", line 2, in      import tensorflow_model_optimization   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/__init__.py"", line 86, in      from tensorflow_model_optimization.python.core.api import clustering   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/api/__init__.py"", line 19, in      from tensorflow_model_optimization.python.core.api import sparsity   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/api/sparsity/__init__.py"", line 16, in      from tensorflow_model_optimization.python.core.api.sparsity import keras   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/api/sparsity/keras/__init__.py"", line 18, in      from tensorflow_model_optimization.python.core.sparsity.keras.prune import prune_low_magnitude   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/sparsity/keras/prune.py"", line 22, in      from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 33, in      from tensorflow_model_optimization.python.core.sparsity.keras import prune_registry   File ""/home/marlin/.local/lib/python)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,prmudgal,tensorflow.python.keras.api._v1.keras.layers' has no attribute 'experimental',"**System information**  Linux Ubuntu 16.04  TensorFlow version: 1.15.5  Python version: 3.7  Installed using virtualenv **Describe the problem** Command  import tensorflow_model_optimization Error that I get File ""test.py"", line 2, in      import tensorflow_model_optimization   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/__init__.py"", line 86, in      from tensorflow_model_optimization.python.core.api import clustering   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/api/__init__.py"", line 19, in      from tensorflow_model_optimization.python.core.api import sparsity   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/api/sparsity/__init__.py"", line 16, in      from tensorflow_model_optimization.python.core.api.sparsity import keras   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/api/sparsity/keras/__init__.py"", line 18, in      from tensorflow_model_optimization.python.core.sparsity.keras.prune import prune_low_magnitude   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/sparsity/keras/prune.py"", line 22, in      from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper   File ""/home/marlin/.local/lib/python3.7/sitepackages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 33, in      from tensorflow_model_optimization.python.core.sparsity.keras import prune_registry   File ""/home/marlin/.local/lib/python",2022-02-02T00:31:54Z,stat:awaiting response type:build/install subtype: ubuntu/linux TF 1.15,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54239,"Hi  ! After installing  Model Optimization 0.6.0 , I was able to  execute import command. you can select suitable version for quantization from here. Attaching gist for reference.Thank you"," It worked, Thanks ",Are you satisfied with the resolution of your issue? Yes No,"> Hi  ! After installing Model Optimization 0.6.0 , I was able to execute import command. you can select suitable version for quantization from here. Attaching gist for reference.Thank you Did you mean (pip install tensorflowmodeloptimization) ?"
1556,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot build tflite wheels for Windows)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows (github actions)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA  TensorFlow installed from (source or binary): cloned from github  TensorFlow version: from github  Python version: 3.8  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): using cmake  GCC/Compiler version (if compiling from source): Visual Studio 17.0  CUDA/cuDNN version: NA  GPU model and memory: NA **Describe the problem** I am trying to build tensorflow lite wheels for Windows on the github actions runner. I am using the `build_pip_package_with_cmake.sh` I can get compiler to run, but in the end it fails with 1738 errors. Here is a small snippet of these:  Building wheels for MacOS and Linux works fine. I would be open for any suggestions. The goal is to make it possible to get wheels for Windows. I know that there are wheels available from coral, but not for Python 3.10. **Provide the exact sequence of commands / steps that you executed before running into the problem**  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rikardn,Cannot build tflite wheels for Windows,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows (github actions)  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA  TensorFlow installed from (source or binary): cloned from github  TensorFlow version: from github  Python version: 3.8  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): using cmake  GCC/Compiler version (if compiling from source): Visual Studio 17.0  CUDA/cuDNN version: NA  GPU model and memory: NA **Describe the problem** I am trying to build tensorflow lite wheels for Windows on the github actions runner. I am using the `build_pip_package_with_cmake.sh` I can get compiler to run, but in the end it fails with 1738 errors. Here is a small snippet of these:  Building wheels for MacOS and Linux works fine. I would be open for any suggestions. The goal is to make it possible to get wheels for Windows. I know that there are wheels available from coral, but not for Python 3.10. **Provide the exact sequence of commands / steps that you executed before running into the problem**  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.",2022-02-01T17:04:52Z,type:build/install comp:lite subtype:windows,closed,0,11,https://github.com/tensorflow/tensorflow/issues/54235,Looks like recentlyintroduced breakage due to a recent change to the flatbuffers library: https://github.com/google/flatbuffers/commit/a42e898979cc828a35aefcc469fec896175fa8e6diffb141302ec1fbb79cf5dcc83cc8a211f34556c807361e58b18f3dc2b2f33faf06 That change modified the declaration of flatbuffers::Table::VerifyField to take an extra argument. So one possible workaround is to use an earlier version of the flatbuffers library.,"Ah, this is a mismatch between the file `tensorflow/lite/schema/schema_generated.h` and the new version of the flatbuffers library. That file is automatically generated by the flatbuffers tool, and I guess it may be using internal details in the flatbuffer headers that might not be considered part of the flatbuffers API? So another, better workaround would be to regenerate that file, which I think you can probably do using a command something along the lines of  (possibly with some additional ""I"" or ""o"" options).",Thanks for looking into this ! I will try to look into your suggestions. I failed to mention that I first had to patch the flatbuffers CMakeLists.txt to turn off the `/WX` (warnings becomes errors) flag. Some warnings were emitted that failed the compilation. Perhaps these are clues:  Context for one of these: ,Isn't the tflite build always using flatbuffers v1.12 by the way? `tensorflow/lite/tools/cmake/modules/flatbuffers.cmake` is using that tag and I checked that this is the one being used.,"Hi ! Could you check with  TF 2.9, Python 3.8, and Visual Studio 2019 with the below command. `PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native ` Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Thanks for the suggestion. I will test this and get back with the results, ! Could you confirm the results from your end.  Thank you!,Your suggestion resolved the issue. We can now build tflite on Windows. This issue can be closed now.,Ok  ! Thanks for the confirmation. Moving this issue to resolved status then. Thank you!,Are you satisfied with the resolution of your issue? Yes No
1589,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tf.data.Dataset.from_generator() does not work with tf.data.experimental.enable_debug_mode())ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  TensorFlow version (use command below): `v2.7.00gc256c071bb2 2.7.0`  Python version: 3.7.12 **Describe the current behavior** `tf.data.Dataset.from_generator()` does not work when `tf.data.experimental.enable_debug_mode()` is called in advance.  Results in  From a small investigation it seems that the ""obvious"" solution â€”Â changing https://github.com/tensorflow/tensorflow/blob/ffe6f62b7f8e57177c26ca3b38c0929d5f64b43f/tensorflow/python/data/ops/dataset_ops.pyL833 to  does not fix the issue, even though `np.int64(0)` is hashable, while `np.array(0, dtype=np.int64)` is not. Additionally, I tried a workaround and added to `_GeneratorState` the following method:  and added calls to it from `get_iterator()` and `iterator_completed()`. However, this triggered yet another issue: in `tf.python.ops.script_ops`, `FuncRegistry` started throwing an error `ValueError('callback pyfunc_63 is not found')`. **Describe the expected behavior** No crash. **Standalone code to reproduce the issue** https://colab.research.google.com/gist/dniku/80456bc9d30fbaaadca4a22469c2c1df/tf_dataset_from_generator_with_debug_mode_crash.ipynb)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,dniku,tf.data.Dataset.from_generator() does not work with tf.data.experimental.enable_debug_mode(),"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab  TensorFlow version (use command below): `v2.7.00gc256c071bb2 2.7.0`  Python version: 3.7.12 **Describe the current behavior** `tf.data.Dataset.from_generator()` does not work when `tf.data.experimental.enable_debug_mode()` is called in advance.  Results in  From a small investigation it seems that the ""obvious"" solution â€”Â changing https://github.com/tensorflow/tensorflow/blob/ffe6f62b7f8e57177c26ca3b38c0929d5f64b43f/tensorflow/python/data/ops/dataset_ops.pyL833 to  does not fix the issue, even though `np.int64(0)` is hashable, while `np.array(0, dtype=np.int64)` is not. Additionally, I tried a workaround and added to `_GeneratorState` the following method:  and added calls to it from `get_iterator()` and `iterator_completed()`. However, this triggered yet another issue: in `tf.python.ops.script_ops`, `FuncRegistry` started throwing an error `ValueError('callback pyfunc_63 is not found')`. **Describe the expected behavior** No crash. **Standalone code to reproduce the issue** https://colab.research.google.com/gist/dniku/80456bc9d30fbaaadca4a22469c2c1df/tf_dataset_from_generator_with_debug_mode_crash.ipynb",2022-02-01T12:35:17Z,stat:awaiting tensorflower type:bug comp:data TF 2.7,closed,2,4,https://github.com/tensorflow/tensorflow/issues/54232," , I was able to reproduce the issue in tf v2.5, v2.7 and nightly.Please find the gist of it here."," , If you have identified the issue in the code and the probable fix, could you please raise a PR. Thanks!","I have not, and my small investigation only hints at what will _not_ resolve this issue.",Are you satisfied with the resolution of your issue? Yes No
1655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow to change thread timeout in collective_ops with an ENV variable)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.6.2  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** This issue happens with XLA when using JAX in a multihost system, where I get `This thread has been waiting for 5000ms for and may be stuck` when the pmap function is getting compiled and rapidly followed by `Thread is unstuck!  Warning above was a falsepositive.`. This gets annoying when I know that it is not really stuck and I get it on most of the devices (so the log gets repeated `num_devices` times).  **Will this change the current api? How?** Looking at the code, the 5000ms are hardcoded. Because I understand the need to log these cases, I would propose adding an ENV var where you could change the 5000ms to be another value, and if the variable is not set, then keep the 5000ms which works on most cases.  It would only require to modify this function at `tensorflow/tensorflow/compiler/xla/service/collective_ops_utils.h`  **Who will benefit with this feature?** Cleaner logs and avoid known false positive. **Any Other info.** Also, there's a `for` repeated on the log which could be fixed in the same PR :))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,alonfnt,Allow to change thread timeout in collective_ops with an ENV variable,"Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.6.2  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** This issue happens with XLA when using JAX in a multihost system, where I get `This thread has been waiting for 5000ms for and may be stuck` when the pmap function is getting compiled and rapidly followed by `Thread is unstuck!  Warning above was a falsepositive.`. This gets annoying when I know that it is not really stuck and I get it on most of the devices (so the log gets repeated `num_devices` times).  **Will this change the current api? How?** Looking at the code, the 5000ms are hardcoded. Because I understand the need to log these cases, I would propose adding an ENV var where you could change the 5000ms to be another value, and if the variable is not set, then keep the 5000ms which works on most cases.  It would only require to modify this function at `tensorflow/tensorflow/compiler/xla/service/collective_ops_utils.h`  **Who will benefit with this feature?** Cleaner logs and avoid known false positive. **Any Other info.** Also, there's a `for` repeated on the log which could be fixed in the same PR :)",2022-01-31T10:31:54Z,stat:awaiting response type:feature comp:xla,closed,0,3,https://github.com/tensorflow/tensorflow/issues/54221,I can happily do a quick PR if this is something you may want and I get the env var name :),"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",ğŸ˜† win by attrition.
1863,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OpenMPI issue causes import tensorflow to hang intermittently?)ï¼Œ å†…å®¹æ˜¯ (Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: n/a    **TensorFlow installed from (source or binary)**: Binary    **TensorFlow version (use command below)**:tf.version.VERSION = 2.7.0    **Python version**: (3, 8, 10, 'final', 0)    **Bazel version (if compiling from source)**: n/a    **GCC/Compiler version (if compiling from source)**: n/a    **CUDA/cuDNN version**:CUDA Version: 11.4     **GPU model and memory**: 4 x A6000 48GB    **Exact command to reproduce**:  import tensorflow as tf You can collect some of this information using our environment capture script:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,camda03,OpenMPI issue causes import tensorflow to hang intermittently?,"Please go to Stack Overflow for help and support: https://stackoverflow.com/questions/tagged/tensorflow If you open a GitHub issue, here is our policy: 1.  It must be a bug, a feature request, or a significant problem with the     documentation (for small docs fixes please send a PR instead). 2.  The form below must be filled out. 3.  It shouldn't be a TensorBoard issue. Those go     here. **Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.   System information    **Have I written custom code (as opposed to using a stock example script     provided in TensorFlow)**:Yes    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04    **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue     happens on a mobile device**: n/a    **TensorFlow installed from (source or binary)**: Binary    **TensorFlow version (use command below)**:tf.version.VERSION = 2.7.0    **Python version**: (3, 8, 10, 'final', 0)    **Bazel version (if compiling from source)**: n/a    **GCC/Compiler version (if compiling from source)**: n/a    **CUDA/cuDNN version**:CUDA Version: 11.4     **GPU model and memory**: 4 x A6000 48GB    **Exact command to reproduce**:  import tensorflow as tf You can collect some of this information using our environment capture script:",2022-01-31T00:57:34Z,stat:awaiting response type:others TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54218," , In order to expedite the troubleshooting process, could you please provide the complete code and dataset to reproduce the issue reported here.","tf_strace_fail.txt tf_strace_success.txt All it takes is the strace command that I've provided. strace ttt python c 'import tensorflow as tf; print(tf.version)' | tee a stracepython.txt I've seen cases in which I reboot the machine, run this command, and it hangs as shown. If this fails in a Jupyter notebook, all it takes is ""import tensorflow as tf""' to make the notebook hang. The notebook never gets to our code in these cases. This fails randomly, for no reason that we understand. Previously some thought that this was a hardware issue. However, the hardware has been replaced and this still occurs. For the past two days I've been running our code without incident. (Big surprise to me.) Why is this working at the moment?  How long will it keep working? I wish I knew. It's like flipping a coin or throwing dice, whether our notebooks and strace will work or hang. Here are two logfiles. Both were done using the strace command that I've provided. One worked, one hung. Will these help? Thanks! P.S. This is our model.  We run this on one A6000.  This lets us run (if nothing hangs) up to four at a time. Model: ""model"" _________________________________________________________________ Layer (type)                 Output Shape              Param     ================================================================= layer_0 (InputLayer)         [(None, 23963)]           0          _________________________________________________________________ layer_1 (Dense)              (None, 23963)             574249332  _________________________________________________________________ layer_2 (Dense)              (None, 23963)             574249332  _________________________________________________________________ layer_3 (Dense)              (None, 23963)             574249332  _________________________________________________________________ layer_4 (Dense)              (None, 23963)             574249332  _________________________________________________________________ output_layer (Dense)         (None, 22)                527208     ================================================================= Total params: 2,297,524,536 Trainable params: 2,297,524,536 Nontrainable params: 0","Here is ompi_info FYI. Thanks! ompi_info                  Package: Debian OpenMPI                 Open MPI: 4.0.3   Open MPI repo revision: v4.0.3    Open MPI release date: Mar 03, 2020                 Open RTE: 4.0.3   Open RTE repo revision: v4.0.3    Open RTE release date: Mar 03, 2020                     OPAL: 4.0.3       OPAL repo revision: v4.0.3        OPAL release date: Mar 03, 2020                  MPI API: 3.1.0             Ident string: 4.0.3                   Prefix: /usr  Configured architecture: x86_64pclinuxgnu           Configure host: lcy01amd64020            Configured by: buildd            Configured on: Wed Apr 15 13:14:35 UTC 2020           Configure host: lcy01amd64020   Configure command line: 'build=x86_64linuxgnu' 'prefix=/usr'                           'includedir=${prefix}/include'                           'mandir=${prefix}/share/man'                           'infodir=${prefix}/share/info'                           'sysconfdir=/etc' 'localstatedir=/var'                           'disablesilentrules'                           'libdir=${prefix}/lib/x86_64linuxgnu'                           'runstatedir=/run' 'disablemaintainermode'                           'disabledependencytracking'                           'disablesilentrules'                           'disablewrapperrunpath'                           'withpackagestring=Debian OpenMPI'                           'withverbs' 'withlibfabric' 'withpsm2'                           'withjdkdir=/usr/lib/jvm/defaultjava'                           'enablempijava'                           'enableopalbtlusnicunittests'                           'withlibevent=external'                           'withpmix=/usr/lib/x86_64linuxgnu/pmix'                           'disablesilentrules' 'enablempicxx'                           'withhwloc=/usr' 'withlibltdl'                           'withdevelheaders' 'withslurm' 'withsge'                           'withouttm' 'sysconfdir=/etc/openmpi'                           'libdir=${prefix}/lib/x86_64linuxgnu/openmpi/lib'                           'includedir=${prefix}/lib/x86_64linuxgnu/openmpi/include'                 Built by: buildd                 Built on: Wed Apr 15 13:20:16 UTC 2020               Built host: lcy01amd64020               C bindings: yes             C++ bindings: yes              Fort mpif.h: yes (all)             Fort use mpi: yes (full: ignore TKR)        Fort use mpi size: deprecatedompiinfovalue         Fort use mpi_f08: yes  Fort mpi_f08 compliance: The mpi_f08 module is available, but due to                           limitations in the gfortran compiler and/or Open                           MPI, does not support the following: array                           subsections, direct passthru (where possible) to                           underlying Open MPI's C functionality   Fort mpi_f08 subarrays: no            Java bindings: yes   Wrapper compiler rpath: rpath               C compiler: gcc      C compiler absolute: /usr/bin/gcc   C compiler family name: GNU       C compiler version: 9.3.0             C++ compiler: g++    C++ compiler absolute: /usr/bin/g++            Fort compiler: gfortran        Fort compiler abs: /usr/bin/gfortran          Fort ignore TKR: yes (!GCC$ ATTRIBUTES NO_ARG_CHECK ::)    Fort 08 assumed shape: yes       Fort optional args: yes           Fort INTERFACE: yes     Fort ISO_FORTRAN_ENV: yes        Fort STORAGE_SIZE: yes       Fort BIND(C) (all): yes       Fort ISO_C_BINDING: yes  Fort SUBROUTINE BIND(C): yes        Fort TYPE,BIND(C): yes  Fort T,BIND(C,name=""a""): yes             Fort PRIVATE: yes           Fort PROTECTED: yes            Fort ABSTRACT: yes        Fort ASYNCHRONOUS: yes           Fort PROCEDURE: yes          Fort USE...ONLY: yes            Fort C_FUNLOC: yes  Fort f08 using wrappers: yes          Fort MPI_SIZEOF: yes              C profiling: yes            C++ profiling: yes    Fort mpif.h profiling: yes   Fort use mpi profiling: yes    Fort use mpi_f08 prof: yes           C++ exceptions: no           Thread support: posix (MPI_THREAD_MULTIPLE: yes, OPAL support: yes,                           OMPI progress: no, ORTE progress: yes, Event lib:                           yes)            Sparse Groups: no   Internal debug support: no   MPI interface warnings: yes      MPI parameter check: runtime Memory profiling support: no Memory debugging support: no               dl support: yes    Heterogeneous support: no  mpirun default prefix: no        MPI_WTIME support: native      Symbol vis. support: yes    Host topology support: yes             IPv6 support: no       MPI1 compatibility: no           MPI extensions: affinity, cuda, pcollreq    FT Checkpoint support: no (checkpoint thread: no)    C/R Enabled Debugging: no   MPI_MAX_PROCESSOR_NAME: 256     MPI_MAX_ERROR_STRING: 256      MPI_MAX_OBJECT_NAME: 64         MPI_MAX_INFO_KEY: 36         MPI_MAX_INFO_VAL: 256        MPI_MAX_PORT_NAME: 1024   MPI_MAX_DATAREP_STRING: 128            MCA allocator: bucket (MCA v2.1.0, API v2.0.0, Component v4.0.3)            MCA allocator: basic (MCA v2.1.0, API v2.0.0, Component v4.0.3)            MCA backtrace: execinfo (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA btl: self (MCA v2.1.0, API v3.1.0, Component v4.0.3)                  MCA btl: openib (MCA v2.1.0, API v3.1.0, Component v4.0.3)                  MCA btl: vader (MCA v2.1.0, API v3.1.0, Component v4.0.3)                  MCA btl: tcp (MCA v2.1.0, API v3.1.0, Component v4.0.3)             MCA compress: bzip (MCA v2.1.0, API v2.0.0, Component v4.0.3)             MCA compress: gzip (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA crs: none (MCA v2.1.0, API v2.0.0, Component v4.0.3)                   MCA dl: dlopen (MCA v2.1.0, API v1.0.0, Component v4.0.3)                MCA event: external (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA hwloc: external (MCA v2.1.0, API v2.0.0, Component v4.0.3)                   MCA if: linux_ipv6 (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                   MCA if: posix_ipv4 (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)          MCA installdirs: env (MCA v2.1.0, API v2.0.0, Component v4.0.3)          MCA installdirs: config (MCA v2.1.0, API v2.0.0, Component v4.0.3)               MCA memory: patcher (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA mpool: hugepage (MCA v2.1.0, API v3.0.0, Component v4.0.3)              MCA patcher: overwrite (MCA v2.1.0, API v1.0.0, Component                           v4.0.3)                 MCA pmix: flux (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA pmix: isolated (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA pmix: ext3x (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA pstat: linux (MCA v2.1.0, API v2.0.0, Component v4.0.3)               MCA rcache: grdma (MCA v2.1.0, API v3.3.0, Component v4.0.3)            MCA reachable: weighted (MCA v2.1.0, API v2.0.0, Component v4.0.3)            MCA reachable: netlink (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA shmem: mmap (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA shmem: posix (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA shmem: sysv (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA timer: linux (MCA v2.1.0, API v2.0.0, Component v4.0.3)               MCA errmgr: default_app (MCA v2.1.0, API v3.0.0, Component                           v4.0.3)               MCA errmgr: default_tool (MCA v2.1.0, API v3.0.0, Component                           v4.0.3)               MCA errmgr: default_hnp (MCA v2.1.0, API v3.0.0, Component                           v4.0.3)               MCA errmgr: default_orted (MCA v2.1.0, API v3.0.0, Component                           v4.0.3)                  MCA ess: singleton (MCA v2.1.0, API v3.0.0, Component                           v4.0.3)                  MCA ess: env (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA ess: hnp (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA ess: tool (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA ess: pmi (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA ess: slurm (MCA v2.1.0, API v3.0.0, Component v4.0.3)                MCA filem: raw (MCA v2.1.0, API v2.0.0, Component v4.0.3)              MCA grpcomm: direct (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA iof: tool (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA iof: hnp (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA iof: orted (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA odls: pspawn (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA odls: default (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA oob: tcp (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA plm: isolated (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA plm: slurm (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA plm: rsh (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA ras: slurm (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA ras: gridengine (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                  MCA ras: simulator (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                 MCA regx: reverse (MCA v2.1.0, API v1.0.0, Component v4.0.3)                 MCA regx: fwd (MCA v2.1.0, API v1.0.0, Component v4.0.3)                 MCA regx: naive (MCA v2.1.0, API v1.0.0, Component v4.0.3)                MCA rmaps: mindist (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA rmaps: round_robin (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                MCA rmaps: ppr (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA rmaps: resilient (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                MCA rmaps: seq (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA rmaps: rank_file (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                  MCA rml: oob (MCA v2.1.0, API v3.0.0, Component v4.0.3)               MCA routed: direct (MCA v2.1.0, API v3.0.0, Component v4.0.3)               MCA routed: radix (MCA v2.1.0, API v3.0.0, Component v4.0.3)               MCA routed: binomial (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA rtc: hwloc (MCA v2.1.0, API v1.0.0, Component v4.0.3)               MCA schizo: slurm (MCA v2.1.0, API v1.0.0, Component v4.0.3)               MCA schizo: ompi (MCA v2.1.0, API v1.0.0, Component v4.0.3)               MCA schizo: orte (MCA v2.1.0, API v1.0.0, Component v4.0.3)               MCA schizo: flux (MCA v2.1.0, API v1.0.0, Component v4.0.3)                MCA state: novm (MCA v2.1.0, API v1.0.0, Component v4.0.3)                MCA state: orted (MCA v2.1.0, API v1.0.0, Component v4.0.3)                MCA state: hnp (MCA v2.1.0, API v1.0.0, Component v4.0.3)                MCA state: app (MCA v2.1.0, API v1.0.0, Component v4.0.3)                MCA state: tool (MCA v2.1.0, API v1.0.0, Component v4.0.3)                  MCA bml: r2 (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA coll: libnbc (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA coll: sm (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA coll: inter (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA coll: self (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA coll: tuned (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA coll: basic (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA coll: monitoring (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                 MCA coll: sync (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA fbtl: posix (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA fcoll: two_phase (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                MCA fcoll: dynamic (MCA v2.1.0, API v2.0.0, Component v4.0.3)                MCA fcoll: individual (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                MCA fcoll: dynamic_gen2 (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                MCA fcoll: vulcan (MCA v2.1.0, API v2.0.0, Component v4.0.3)                   MCA fs: ufs (MCA v2.1.0, API v2.0.0, Component v4.0.3)                   MCA io: romio321 (MCA v2.1.0, API v2.0.0, Component v4.0.3)                   MCA io: ompio (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA mtl: ofi (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA mtl: psm2 (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA mtl: psm (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA osc: rdma (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA osc: monitoring (MCA v2.1.0, API v3.0.0, Component                           v4.0.3)                  MCA osc: pt2pt (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA osc: sm (MCA v2.1.0, API v3.0.0, Component v4.0.3)                  MCA pml: v (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA pml: monitoring (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)                  MCA pml: ob1 (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA pml: cm (MCA v2.1.0, API v2.0.0, Component v4.0.3)                  MCA rte: orte (MCA v2.1.0, API v2.0.0, Component v4.0.3)             MCA sharedfp: individual (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)             MCA sharedfp: lockedfile (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)             MCA sharedfp: sm (MCA v2.1.0, API v2.0.0, Component v4.0.3)                 MCA topo: basic (MCA v2.1.0, API v2.2.0, Component v4.0.3)                 MCA topo: treematch (MCA v2.1.0, API v2.2.0, Component                           v4.0.3)            MCA vprotocol: pessimist (MCA v2.1.0, API v2.0.0, Component                           v4.0.3)"," , This issue is not related to tensorflow. Please move to respective respository which helps to resolve the issue.Thanks!",Please see the ticket below for additional information on this issue. https://github.com/openmpi/ompi/issues/10025 Thanks! Dave
1907,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(partially initialized module 'tensorflow' has no attribute 'Tensor' (most likely due to a circular import))ï¼Œ å†…å®¹æ˜¯ (**System information**  Running code from: https://github.com/jpiedrafita/ai_number_read/blob/main/numbers.py â¯pyhton3 numbers.py  OS Platform and Distribution: OSX 10.15.7  TensorFlow installed from: pip install tensorflow As described in other ""has no attribute"" issues I tried other tf versions using pip install tensorflow==x.x.x ignoreinstalled  TensorFlow version (use command below): â¯ pip show tensorflow Name: tensorflow Version: 2.7.0 Summary: TensorFlow is an open source machine learning framework for everyone. Homepage: https://www.tensorflow.org/ Author: Google Inc. Authoremail: packages.org License: Apache 2.0 Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sitepackages Requires: abslpy, astunparse, flatbuffers, gast, googlepasta, grpcio, h5py, keras, keraspreprocessing, libclang, numpy, opteinsum, protobuf, six, tensorboard, tensorflowestimator, tensorflowiogcsfilesystem, termcolor, typingextensions, wheel, wrapt Requiredby: â¯ pip show tensorflow_datasets Name: tensorflowdatasets Version: 4.5.0 Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow. Homepage: https://github.com/tensorflow/datasets Author: Google Inc. Authoremail: packages.org License: Apache 2.0 Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sitepackages Requires: abslpy, dill, numpy, promise, protobuf, requests, six, tensorflowmetadata, termcolor, tqdm Requiredby:  Python version 3.9.6 **Describe the current behavior** Console error:  https://github.com/jpiedrafita/ai_number_read/blob/main/traceback.txt File ""/Library/Frameworks/)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jpiedrafita,partially initialized module 'tensorflow' has no attribute 'Tensor' (most likely due to a circular import),"**System information**  Running code from: https://github.com/jpiedrafita/ai_number_read/blob/main/numbers.py â¯pyhton3 numbers.py  OS Platform and Distribution: OSX 10.15.7  TensorFlow installed from: pip install tensorflow As described in other ""has no attribute"" issues I tried other tf versions using pip install tensorflow==x.x.x ignoreinstalled  TensorFlow version (use command below): â¯ pip show tensorflow Name: tensorflow Version: 2.7.0 Summary: TensorFlow is an open source machine learning framework for everyone. Homepage: https://www.tensorflow.org/ Author: Google Inc. Authoremail: packages.org License: Apache 2.0 Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sitepackages Requires: abslpy, astunparse, flatbuffers, gast, googlepasta, grpcio, h5py, keras, keraspreprocessing, libclang, numpy, opteinsum, protobuf, six, tensorboard, tensorflowestimator, tensorflowiogcsfilesystem, termcolor, typingextensions, wheel, wrapt Requiredby: â¯ pip show tensorflow_datasets Name: tensorflowdatasets Version: 4.5.0 Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow. Homepage: https://github.com/tensorflow/datasets Author: Google Inc. Authoremail: packages.org License: Apache 2.0 Location: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sitepackages Requires: abslpy, dill, numpy, promise, protobuf, requests, six, tensorflowmetadata, termcolor, tqdm Requiredby:  Python version 3.9.6 **Describe the current behavior** Console error:  https://github.com/jpiedrafita/ai_number_read/blob/main/traceback.txt File ""/Library/Frameworks/",2022-01-30T10:53:26Z,stat:awaiting response type:bug TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54208," , Can you please confirm whether you have installed tensorflow from this link.If not please try to follow the doc and let us know if you are facing same issue.Thanks!",Hi   I installed tensorflow without upgrading pip (21.3.1). So:  I uninstalled tensorflow and tensorflow_datasets.  Then I upgraded pip according to the link (22.0.2).  I installed tensorflow and tensorflow_datasets using:   Current versions are: tensorflow 2.7.0 tensorflow_datasets 4.5.2  Now the traceback changed to a different attribute error.," , I was facing different issue while executing the mentioned code.Please find the gist of it here.",Are you satisfied with the resolution of your issue? Yes No
1908,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(No details on the CPU chip AVX requirement, pls give details up front. Not all folks have ivory tower HW.  )ï¼Œ å†…å®¹æ˜¯ (Your documentation tells of the Nvidia GPU/CUDA requirement but not the CPU chip AVX requirement.   I have spent three days swapping video cards, now to find my dual Xeon(R) CPU  X5450 Dell cannot run your 2.0 version.  There should be a clear grid/listing of requirements and a lookup function.  It will take me a week and $ to build out of stored Dell t5600 with the Sandbridge CPUs.  Why are there no switches to disable advance features to use latest SW.  No all folks have ivory tower HW.  I run multiple old Dells with multiple configurations. I do not care if a job takes 2 days for a POC/hobby projects, just needs to run.  I find it very odd to think/expect a person would load of of this SW onto their main PC/Laptop.  Also, a large amount sugar coating and theory on the YouTube Tensorflow guides  (Josh G. nice detail, grateful you have no tin cup out).  But, there is a disservice in hiding technical details and steps. Do you think nontech folks going to pick this up, not likely.  FYI..if you do not know structured tables vs non, do not tell them to jump into the pool. Question Can you advise if I build from source, will the CPU chip AVX be accounted for?  I am running Tensorflow 1.5 successfully , but not sure is Sahre prject will support that version Thank you for submitting a TensorFlow documentation issue. Per our GitHub policy, we only address code/doc bugs, performance issues, feature requests, and build/installation issues on GitHub. The TensorFlow docs are open source! To get involved, read the documentation contributor guide: https://www.tensorflow.org/community/contribute/docs )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tomthumb99,"No details on the CPU chip AVX requirement, pls give details up front. Not all folks have ivory tower HW.  ","Your documentation tells of the Nvidia GPU/CUDA requirement but not the CPU chip AVX requirement.   I have spent three days swapping video cards, now to find my dual Xeon(R) CPU  X5450 Dell cannot run your 2.0 version.  There should be a clear grid/listing of requirements and a lookup function.  It will take me a week and $ to build out of stored Dell t5600 with the Sandbridge CPUs.  Why are there no switches to disable advance features to use latest SW.  No all folks have ivory tower HW.  I run multiple old Dells with multiple configurations. I do not care if a job takes 2 days for a POC/hobby projects, just needs to run.  I find it very odd to think/expect a person would load of of this SW onto their main PC/Laptop.  Also, a large amount sugar coating and theory on the YouTube Tensorflow guides  (Josh G. nice detail, grateful you have no tin cup out).  But, there is a disservice in hiding technical details and steps. Do you think nontech folks going to pick this up, not likely.  FYI..if you do not know structured tables vs non, do not tell them to jump into the pool. Question Can you advise if I build from source, will the CPU chip AVX be accounted for?  I am running Tensorflow 1.5 successfully , but not sure is Sahre prject will support that version Thank you for submitting a TensorFlow documentation issue. Per our GitHub policy, we only address code/doc bugs, performance issues, feature requests, and build/installation issues on GitHub. The TensorFlow docs are open source! To get involved, read the documentation contributor guide: https://www.tensorflow.org/community/contribute/docs ",2022-01-30T08:53:29Z,stat:awaiting response type:build/install stale,closed,1,9,https://github.com/tensorflow/tensorflow/issues/54207,"  In order to expedite the troubleshooting process here,Could you please fill the issue template,Please make sure you are using TF v2.4 and later as TF v1.x is no longer actively supported.Please refer this link as well. Thanks!",">Can you advise if I build from source, will the CPU chip AVX be accounted for? AFAIK, that was broken before (the build was failing when AVX was disabled in the build configuration). Not sure whether it was fixed.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, Could you please refer to the above comment   and let us know the update on that ?Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1437,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MacOS build compilation error)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra (10.13.6)  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6  Python version: 3.9.10  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang1000.10.44.4) **Describe the problem** Hi. The problem i'm trying to solve is to build and install TF on early2009 mac mini with core2duo and 4gb ram inside. Firstly I tried to install TF through PIP (`pip install tensorflow/pip install tensorflowcpu`). But after running `python c 'import keras'` i got `illegal instruction 4` error. As far as I understand the pip package is build with AVX support which not supported on my workstation. So i'm trying to create package manually from the 'r2.6' branch but stucked on some wired c++ compilation errors. The build commands i'm trying:  Produces such kind of output  and after a couple of hours waiting i get  huge list of  errors like  Also I've tried with actual 'master' branch and also got   Honestly I'm not good in c++ templates and I don't understand how to solve this compilation error.  Maybe you could help me with this?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,i00lii,MacOS build compilation error,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra (10.13.6)  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6  Python version: 3.9.10  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang1000.10.44.4) **Describe the problem** Hi. The problem i'm trying to solve is to build and install TF on early2009 mac mini with core2duo and 4gb ram inside. Firstly I tried to install TF through PIP (`pip install tensorflow/pip install tensorflowcpu`). But after running `python c 'import keras'` i got `illegal instruction 4` error. As far as I understand the pip package is build with AVX support which not supported on my workstation. So i'm trying to create package manually from the 'r2.6' branch but stucked on some wired c++ compilation errors. The build commands i'm trying:  Produces such kind of output  and after a couple of hours waiting i get  huge list of  errors like  Also I've tried with actual 'master' branch and also got   Honestly I'm not good in c++ templates and I don't understand how to solve this compilation error.  Maybe you could help me with this?",2022-01-27T17:47:33Z,type:build/install subtype:macOS 2.6.0,closed,0,8,https://github.com/tensorflow/tensorflow/issues/54156,Also attaching whole 2.6 error error2.6.log r,"Also, is there any reason try to use gcc instead of clang? "," , Every TensorFlow release is compatible with a certain version, for more information please take a look at the tested build configurations.In this case, can you please try installing TensorFlow v2.6 with respective configurations and check if you are facing the same error. Thanks!", thanks for your response. I'll try to update llvm/clang up to required version (Clang from xcode 10.11) and then continue with tensorflow itself. ,"Also, here is minimal repo of compilation error.  Maybe you could know why this error occures?",It's me again.  The build process started after I updated llvm and clang up to latest version,Are you satisfied with the resolution of your issue? Yes No,Here is the result package  TF 2.6.2 macos 10.13 CPUonly no AVX https://drive.google.com/file/d/1Wndu6YEJIc7N__dERUaGjNm6wri6YhVs/view?usp=sharing
676,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(build tensorflow 2.9 using mingw-w64 on windows 10 )ï¼Œ å†…å®¹æ˜¯ (i am trying to build the tensorflow 2.9 using mingww64 on windows 10 .  i need the tensorflow c++ lib which i wanted to use it in mingww64 based projects. how to do ?  has any tutorial ï¼Ÿ  **System information**  OS Platform and Distribution: Window 10 x64  TensorFlow installed from: source   TensorFlow version: 2.9  Python version: 3.9  Bazel version : 4.2.2  Compiler : mingww64  CUDA/cuDNN version: 11.5  GPU model and memory: RTX3070)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,hujhcv,build tensorflow 2.9 using mingw-w64 on windows 10 ,i am trying to build the tensorflow 2.9 using mingww64 on windows 10 .  i need the tensorflow c++ lib which i wanted to use it in mingww64 based projects. how to do ?  has any tutorial ï¼Ÿ  **System information**  OS Platform and Distribution: Window 10 x64  TensorFlow installed from: source   TensorFlow version: 2.9  Python version: 3.9  Bazel version : 4.2.2  Compiler : mingww64  CUDA/cuDNN version: 11.5  GPU model and memory: RTX3070,2022-01-27T06:05:09Z,stat:awaiting response type:build/install stale subtype:windows,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54134," Please try with the latest TF stable version **2.7.0** and refer to TF releases ,tested build configurations ? Please let us know if it helps? Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1420,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Copy-on-read Limits Usable GPU Memory)ï¼Œ å†…å®¹æ˜¯ (When sparsely accessed individual variables occupy a large portion of available memory, the necessary copy during dense reads, e.g. while saving a checkpoint, can substantially increase the total memory usage. In the limit of a single variable (such as an embedding table) occupying all allocated memory, this behavior causes the memory usage to double when the model is saved. Therefore, a single sparsely accessed variable can at most occupy half the available memory to avoid OOM while saving the model. Possible solutions could include for instance allowing a conversion from copyonread to copyonwrite, or dense access without copy using an exclusive lock in some situations. [](url) **System information** Reproducer: copy_on_read_oom.zip  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): Source  TensorFlow version (use command below): 2.6  Python version: 3.8  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): 9.3.0  CUDA/cuDNN version: N/A  GPU model and memory: N/A)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,philipphack,Copy-on-read Limits Usable GPU Memory,"When sparsely accessed individual variables occupy a large portion of available memory, the necessary copy during dense reads, e.g. while saving a checkpoint, can substantially increase the total memory usage. In the limit of a single variable (such as an embedding table) occupying all allocated memory, this behavior causes the memory usage to double when the model is saved. Therefore, a single sparsely accessed variable can at most occupy half the available memory to avoid OOM while saving the model. Possible solutions could include for instance allowing a conversion from copyonread to copyonwrite, or dense access without copy using an exclusive lock in some situations. [](url) **System information** Reproducer: copy_on_read_oom.zip  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): Source  TensorFlow version (use command below): 2.6  Python version: 3.8  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): 9.3.0  CUDA/cuDNN version: N/A  GPU model and memory: N/A",2022-01-26T23:00:21Z,stat:awaiting response type:feature stale comp:gpu 2.6.0,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54105,Hi  ! Could you look at this feature request?, can you please take a look at this as well?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.
1394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow restore memory leak)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: MacOS Monterey 12.1  TensorFlow installed: from binary  TensorFlow version: The issue could be reproduced by v2.6.0  Python version: 3.8.3 **Describe the current behavior** I notice that Tensorflow (TF) leaks memory when I restore TF models. Specifically if I iteratively load model checkpoint from disk with TF restore API, the memory usage keeps growing no matter what I do `tf.reset_default_graph()` or `tf.keras.backend.clear_session()` or both (But it seems `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` slow down the memory leak speed rate). ** Standalone code to reproduce the issue ** The issue could be reproduced as below. Run `tf_save_model.py` to create and save a model in the folder `./test_model_repro/` with the model name `foo`. Then run `tf_restore_model.py` to load the model iteratively. You would see after each iteration calling `restore()`, even if `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` are being called, the log shows the memory usage keeps increasing. tf_save_model.py  tf.restore_model.py  The issue could be 100% reprod by the above Python scripts.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,RuofanKong,Tensorflow restore memory leak,"**System information**  OS Platform and Distribution: MacOS Monterey 12.1  TensorFlow installed: from binary  TensorFlow version: The issue could be reproduced by v2.6.0  Python version: 3.8.3 **Describe the current behavior** I notice that Tensorflow (TF) leaks memory when I restore TF models. Specifically if I iteratively load model checkpoint from disk with TF restore API, the memory usage keeps growing no matter what I do `tf.reset_default_graph()` or `tf.keras.backend.clear_session()` or both (But it seems `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` slow down the memory leak speed rate). ** Standalone code to reproduce the issue ** The issue could be reproduced as below. Run `tf_save_model.py` to create and save a model in the folder `./test_model_repro/` with the model name `foo`. Then run `tf_restore_model.py` to load the model iteratively. You would see after each iteration calling `restore()`, even if `tf.reset_default_graph()` and `tf.keras.backend.clear_session()` are being called, the log shows the memory usage keeps increasing. tf_save_model.py  tf.restore_model.py  The issue could be 100% reprod by the above Python scripts.",2022-01-26T01:15:59Z,stat:awaiting response type:bug stale comp:apis 2.6.0,closed,0,9,https://github.com/tensorflow/tensorflow/issues/54086," , I was able to reproduce the issue in tf v2.5,v2.7 and nightly.Please find the gist here.",Hi did you try the same in eager mode and is there any specific reason you are testing it by using `tf.disable_v2_behavior()`,"Hi  We use the v1compatible one and don't use the eager mode, due to our product spec in the current phase.","PING, it also affects me.",555...have you solved the problem?,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1091,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(LSTM slow to calibrate with large observation period)ï¼Œ å†…å®¹æ˜¯ (I am reporting an issue in the calibration speed of LSTM models with large observation period even though none of my resource is saturated (CPU, GPU, RAM, SSD) **System information**  OS Platform and Distribution: Win11  TensorFlow installed from: `pip install tensorflow`  TensorFlow version: 2.7 / GPU  Python version: 3.9  CUDA/cuDNN version: cuDNN 8201 (installed via `pip install tensorflow`), CUDA 11.4  GPU model and memory: Geforce GTX 3060 4 Go / RAM 32 Go / CPU AMD Ryzen 5 / SSD **Describe the current behavior** None of the machine resource is saturated following LSTM calibration `print(tf.config.list_physical_devices(""GPU""))` shows that the calibration runs on the GPU as expected. **Describe the expected behavior** Any of the machine resource should be saturated **Standalone code to reproduce the issue**  **Other info / logs** !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Raa23,LSTM slow to calibrate with large observation period,"I am reporting an issue in the calibration speed of LSTM models with large observation period even though none of my resource is saturated (CPU, GPU, RAM, SSD) **System information**  OS Platform and Distribution: Win11  TensorFlow installed from: `pip install tensorflow`  TensorFlow version: 2.7 / GPU  Python version: 3.9  CUDA/cuDNN version: cuDNN 8201 (installed via `pip install tensorflow`), CUDA 11.4  GPU model and memory: Geforce GTX 3060 4 Go / RAM 32 Go / CPU AMD Ryzen 5 / SSD **Describe the current behavior** None of the machine resource is saturated following LSTM calibration `print(tf.config.list_physical_devices(""GPU""))` shows that the calibration runs on the GPU as expected. **Describe the expected behavior** Any of the machine resource should be saturated **Standalone code to reproduce the issue**  **Other info / logs** !image",2022-01-25T21:48:36Z,stat:awaiting response stale comp:keras type:performance TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/54085,"Hi  ! I was getting a slow processing time but memory usage  increased  and got saturated at end of training . Attaching gist  and screenshot for reference. From template, I can also see **dedicated gpu memory usage line** has flattened after a while  and Other devices are expected to work differently. !image Please post on TF forum for further assistance. Thank you!","Hello, thank you for your feedback but I see in the screenshot that the RAM (system and GPU) is not saturated, though I don't understand the reference. I have tested the gist and it has the same behavior as in my local machine: nothing is saturated and the process is slow. Is there any reason why to expect RAM to be saturated as the dataset is not cached ? Also, should I request assistance in the Forum instead ? Thanks", !  Please post this issue on kerasteam/keras repo or TF forum. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
658,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(iOS app size increasing)ï¼Œ å†…å®¹æ˜¯ (Hey dear TensorFlow team. I am using  following lib on my iOS application  `pod 'TensorFlowLiteSwift', '~> 0.0.1nightly', :subspecs => ['Metal','CoreML']` Every time when I am trying to read .tflite, my app size is increasing. I discovered and found some interesting think inside the app container AppData>tmp   Why does it create a Core ML Compiled Model each time and not be deleted after being freed? Please  help me , this is a really blocker ğŸ™)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PahlevanyanSamvel,iOS app size increasing,"Hey dear TensorFlow team. I am using  following lib on my iOS application  `pod 'TensorFlowLiteSwift', '~> 0.0.1nightly', :subspecs => ['Metal','CoreML']` Every time when I am trying to read .tflite, my app size is increasing. I discovered and found some interesting think inside the app container AppData>tmp   Why does it create a Core ML Compiled Model each time and not be deleted after being freed? Please  help me , this is a really blocker ğŸ™",2022-01-25T18:48:00Z,stat:awaiting response type:bug stale,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54083,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, please refer to this similar issues link, link1 and let us know if it helps? Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1902,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Building Tensorflow lite from source: cannot fix unresolved external symbols in Visual Studio project)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installed from (source or binary): Tensorflow lite (source)  TensorFlow version: 2.7.0  GCC/Compiler version (if compiling from source): MSVC 19.26.28806.0 **Describe the problem** Building Tensorflow lite from source appears to work, in so far as I get the static libraries. However, when I create a Visual Studio project I get several linker errors (unresolved externals  see end of issue for error messages) and whatever libs I try to link, I cannot seem to get anything to build. I have followed the instructions for building the minimal example CMake project and **this appears to work**. However, we need to be able to link tensorflow lite in our application code which isn't managed with CMake, so using CMake isn't an option for us. In a nutshell: the build process appears to work, but I can't seem to get a Visual Studio project set up. More details below. **Provide the exact sequence of commands / steps that you executed before running into the problem** 1. Used this link as a guide: https://www.tensorflow.org/lite/guide/build_cmake 2. Cloned the tensorflow repo 3. Created a directory called ""mybuild"" at the top level of the repo 4. Opened a command prompt in mybuild and ran: `cmake ../tensorflow_src/tensorflow/lite` 5. Then ran: `cmake build . j` 6. Created a visual studio project console application (see hello world program below) 7. Set up the include directory to point to the tensorflow repo 8. Added tensorflowlite.lib to the linker input options 9. Added the path to the lib in the )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,4scastro,Building Tensorflow lite from source: cannot fix unresolved external symbols in Visual Studio project,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  TensorFlow installed from (source or binary): Tensorflow lite (source)  TensorFlow version: 2.7.0  GCC/Compiler version (if compiling from source): MSVC 19.26.28806.0 **Describe the problem** Building Tensorflow lite from source appears to work, in so far as I get the static libraries. However, when I create a Visual Studio project I get several linker errors (unresolved externals  see end of issue for error messages) and whatever libs I try to link, I cannot seem to get anything to build. I have followed the instructions for building the minimal example CMake project and **this appears to work**. However, we need to be able to link tensorflow lite in our application code which isn't managed with CMake, so using CMake isn't an option for us. In a nutshell: the build process appears to work, but I can't seem to get a Visual Studio project set up. More details below. **Provide the exact sequence of commands / steps that you executed before running into the problem** 1. Used this link as a guide: https://www.tensorflow.org/lite/guide/build_cmake 2. Cloned the tensorflow repo 3. Created a directory called ""mybuild"" at the top level of the repo 4. Opened a command prompt in mybuild and ran: `cmake ../tensorflow_src/tensorflow/lite` 5. Then ran: `cmake build . j` 6. Created a visual studio project console application (see hello world program below) 7. Set up the include directory to point to the tensorflow repo 8. Added tensorflowlite.lib to the linker input options 9. Added the path to the lib in the ",2022-01-25T10:45:10Z,stat:awaiting tensorflower type:build/install subtype:windows TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/54066,"It could be due to the wrong choice of application type, make sure at the start of the application you choose Console Application instead of windows application for Visual Studio. Start the process again with option something like. File > New > Project > Win32 Console Application. ",Having checked the project properties I can confirm it is a console application. If I comment out the Tflite line/header and put in a standard cout statement the console app builds and runs fine.,Mystery solved  we were trying to use the C API as opposed to the C++ binding. I was able to build and link the example code provided here without any significant problems: https://stackoverflow.com/a/56868813/1618009,Are you satisfied with the resolution of your issue? Yes No
563,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prevent overflow in `CalculateTensorElementCount`)ï¼Œ å†…å®¹æ˜¯ (Grappler cost estimation sometimes computes the number of elements in a tensor by multiplying all the dimensions in a shape. However, these tensors can also be controlled by users so a malicious attacker can trigger overflow that can be exploited. PiperOriginRevId: 409575048 ChangeId: I7a958875ba6f3ad9cb5b9943fe5d459efcbe4557)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Prevent overflow in `CalculateTensorElementCount`,"Grappler cost estimation sometimes computes the number of elements in a tensor by multiplying all the dimensions in a shape. However, these tensors can also be controlled by users so a malicious attacker can trigger overflow that can be exploited. PiperOriginRevId: 409575048 ChangeId: I7a958875ba6f3ad9cb5b9943fe5d459efcbe4557",2022-01-25T08:19:21Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54048
563,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prevent overflow in `CalculateTensorElementCount`)ï¼Œ å†…å®¹æ˜¯ (Grappler cost estimation sometimes computes the number of elements in a tensor by multiplying all the dimensions in a shape. However, these tensors can also be controlled by users so a malicious attacker can trigger overflow that can be exploited. PiperOriginRevId: 409575048 ChangeId: I7a958875ba6f3ad9cb5b9943fe5d459efcbe4557)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Prevent overflow in `CalculateTensorElementCount`,"Grappler cost estimation sometimes computes the number of elements in a tensor by multiplying all the dimensions in a shape. However, these tensors can also be controlled by users so a malicious attacker can trigger overflow that can be exploited. PiperOriginRevId: 409575048 ChangeId: I7a958875ba6f3ad9cb5b9943fe5d459efcbe4557",2022-01-25T08:18:06Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54047
563,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prevent overflow in `CalculateTensorElementCount`)ï¼Œ å†…å®¹æ˜¯ (Grappler cost estimation sometimes computes the number of elements in a tensor by multiplying all the dimensions in a shape. However, these tensors can also be controlled by users so a malicious attacker can trigger overflow that can be exploited. PiperOriginRevId: 409575048 ChangeId: I7a958875ba6f3ad9cb5b9943fe5d459efcbe4557)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Prevent overflow in `CalculateTensorElementCount`,"Grappler cost estimation sometimes computes the number of elements in a tensor by multiplying all the dimensions in a shape. However, these tensors can also be controlled by users so a malicious attacker can trigger overflow that can be exploited. PiperOriginRevId: 409575048 ChangeId: I7a958875ba6f3ad9cb5b9943fe5d459efcbe4557",2022-01-25T08:15:59Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54046
629,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Validate real and expected type of arguments to cwise ops.)ï¼Œ å†…å®¹æ˜¯ (Without this validation, it is possible to trigger a `CHECK`fail denial of service. This is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum::v())`. PiperOriginRevId: 409340416 ChangeId: I96080b2796729a3a9b65e7c68307ac276070f2f0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Validate real and expected type of arguments to cwise ops.,"Without this validation, it is possible to trigger a `CHECK`fail denial of service. This is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum::v())`. PiperOriginRevId: 409340416 ChangeId: I96080b2796729a3a9b65e7c68307ac276070f2f0",2022-01-25T07:27:08Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54028
629,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Validate real and expected type of arguments to cwise ops.)ï¼Œ å†…å®¹æ˜¯ (Without this validation, it is possible to trigger a `CHECK`fail denial of service. This is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum::v())`. PiperOriginRevId: 409340416 ChangeId: I96080b2796729a3a9b65e7c68307ac276070f2f0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Validate real and expected type of arguments to cwise ops.,"Without this validation, it is possible to trigger a `CHECK`fail denial of service. This is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum::v())`. PiperOriginRevId: 409340416 ChangeId: I96080b2796729a3a9b65e7c68307ac276070f2f0",2022-01-25T07:25:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54027
629,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Validate real and expected type of arguments to cwise ops.)ï¼Œ å†…å®¹æ˜¯ (Without this validation, it is possible to trigger a `CHECK`fail denial of service. This is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum::v())`. PiperOriginRevId: 409340416 ChangeId: I96080b2796729a3a9b65e7c68307ac276070f2f0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Validate real and expected type of arguments to cwise ops.,"Without this validation, it is possible to trigger a `CHECK`fail denial of service. This is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum::v())`. PiperOriginRevId: 409340416 ChangeId: I96080b2796729a3a9b65e7c68307ac276070f2f0",2022-01-25T07:24:14Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/54026
1855,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Some operations didn't work in tflite train procedure.)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS  TensorFlow version (use command below): tensorflow 2.7.0  Python version: python 3.9  CUDA/cuDNN version: Cuda 11.2/ cudnn 8.2  GPU model and memory: rtx A6000 You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior**  I want to generate fully train able tflite model. I found mapped ops in 'tfl' Dialect. And then, I use these functions in my model.  All operation can convert to graph model use converter.convert(), but it dosen't work. Some operation can use in inference, but can't use in train without any error just stop.(ex. Avgpool2d).  **Describe the expected behavior** I have three questions in upper problem.  First, Is this operation(avgpool) can't use in  tflite?  Second, If first question is true, can i get trainable operation documentation in tflite?  Third, When can i use fully trainable tflite model in mobile? **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if c)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,SpicyYeol,Some operations didn't work in tflite train procedure.,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS  TensorFlow version (use command below): tensorflow 2.7.0  Python version: python 3.9  CUDA/cuDNN version: Cuda 11.2/ cudnn 8.2  GPU model and memory: rtx A6000 You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior**  I want to generate fully train able tflite model. I found mapped ops in 'tfl' Dialect. And then, I use these functions in my model.  All operation can convert to graph model use converter.convert(), but it dosen't work. Some operation can use in inference, but can't use in train without any error just stop.(ex. Avgpool2d).  **Describe the expected behavior** I have three questions in upper problem.  First, Is this operation(avgpool) can't use in  tflite?  Second, If first question is true, can i get trainable operation documentation in tflite?  Third, When can i use fully trainable tflite model in mobile? **Contributing**  Do you want to contribute a PR? (yes/no):  Briefly describe your candidate solution(if c",2022-01-25T05:53:19Z,stat:awaiting response type:bug stale comp:lite TF 2.7,closed,0,9,https://github.com/tensorflow/tensorflow/issues/54001,"  In order to expedite the troubleshooting process, please provide a  code snippet to reproduce the issue reported here. Thanks!","I use the modified example code. I added print(""train_finish) in train loop. but I can't see that. ","  Was able to reproduce this issue on colab using TF v2.7.0 and tfnightly(2.9.0dev20220130) ,please find the gist here for reference. Thanks!"," Thanks for the answer. I tested my code with tfnightly(2.9.0) and I confirmed training.  All 2D model works very well. Thanks. My final goal is  train a 3D conv model on mobile using tflite. however, It already doesn't work yet. Any plans for support to 3d models in the near future?", Triaging training related questions.,"Hi  If the 3D conv model's difference with the 2D one is just operators, then I think we can support them in TF Lite. Could you paste the error you see?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
602,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prevent copying uninitialized data in `AssignOp`.)ï¼Œ å†…å®¹æ˜¯ (This prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan/asan. PiperOriginRevId: 408654780 ChangeId: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Prevent copying uninitialized data in `AssignOp`.,"This prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan/asan. PiperOriginRevId: 408654780 ChangeId: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1",2022-01-25T05:34:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/53997
602,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prevent copying uninitialized data in `AssignOp`.)ï¼Œ å†…å®¹æ˜¯ (This prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan/asan. PiperOriginRevId: 408654780 ChangeId: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Prevent copying uninitialized data in `AssignOp`.,"This prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan/asan. PiperOriginRevId: 408654780 ChangeId: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1",2022-01-25T05:32:38Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/53996
602,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prevent copying uninitialized data in `AssignOp`.)ï¼Œ å†…å®¹æ˜¯ (This prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan/asan. PiperOriginRevId: 408654780 ChangeId: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Prevent copying uninitialized data in `AssignOp`.,"This prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan/asan. PiperOriginRevId: 408654780 ChangeId: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1",2022-01-25T05:30:15Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/53995
490,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Properly handle the case where `SpecializeType()` returns an error `Sâ€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦tatus`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object PiperOriginRevId: 408380069 ChangeId: If3c3fc876dcf9384d5ec7a4985adc68c23ea7318)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Properly handle the case where `SpecializeType()` returns an error `Sâ€¦,"â€¦tatus`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object PiperOriginRevId: 408380069 ChangeId: If3c3fc876dcf9384d5ec7a4985adc68c23ea7318",2022-01-25T05:16:01Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/53991
490,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Properly handle the case where `SpecializeType()` returns an error `Sâ€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦tatus`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object PiperOriginRevId: 408380069 ChangeId: If3c3fc876dcf9384d5ec7a4985adc68c23ea7318)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pranve,Properly handle the case where `SpecializeType()` returns an error `Sâ€¦,"â€¦tatus`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object PiperOriginRevId: 408380069 ChangeId: If3c3fc876dcf9384d5ec7a4985adc68c23ea7318",2022-01-25T05:14:10Z,,closed,0,0,https://github.com/tensorflow/tensorflow/issues/53990
787,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Lite: Using output index causes segfault. )ï¼Œ å†…å®¹æ˜¯ (I am using Tensorflow Lite C++ interface for running inference, commit hash `040585c0f25681b399c9087b53c982959bcca44f` (HEAD of master branch at the time of posting this).  I am using a model with 2 inputs, and a single output (array).  I am trying to run the following code as a sanity check:   The reason I even ask this question is because I expect to be able to obtain the inference results by calling:  However, this too causes a segmentation fault.  Running `float* out = interpreter>typed_output_tensor(0);` does give me the correct output.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cyrusbehr,Tensorflow Lite: Using output index causes segfault. ,"I am using Tensorflow Lite C++ interface for running inference, commit hash `040585c0f25681b399c9087b53c982959bcca44f` (HEAD of master branch at the time of posting this).  I am using a model with 2 inputs, and a single output (array).  I am trying to run the following code as a sanity check:   The reason I even ask this question is because I expect to be able to obtain the inference results by calling:  However, this too causes a segmentation fault.  Running `float* out = interpreter>typed_output_tensor(0);` does give me the correct output.",2022-01-25T01:06:41Z,stat:awaiting tensorflower type:bug comp:lite,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53951,"hi, the `interpreter>GetOutputName(i)` API expects the index to between 0 and the outputs.size()  1, it's not the tensor index. When you use `output` which returned from the `interpreter>outputs()`, this is the output tensor index in the graph, so it might be out of bound.","Makes sense, thank you! ",Are you satisfied with the resolution of your issue? Yes No
1730,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Not getting consistent results with .h5 and .tflite models on different machines)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tested on Windows 11, Intel Mac, Mac M1 v11.6  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary (tensorflow for windows and tensorflowmacos for Mac)  TensorFlow version (use command below): 2.5.0  Python version: 3.8.12  Bazel version (if compiling from source): No  GCC/Compiler version (if compiling from source): No  CUDA/cuDNN version: 11.0 (for training only)  GPU model and memory: Mac M1 **Describe the current behavior** The same h5 and/or TFLite model gives different outputs on different machines. For an image regression model where ground truth labels range between [0, 3] the results differ by ~0.20.3 on different machines, which causes thresholding issues. **Describe the expected behavior** The same model should theoretically perform the same on all machines or at least they should match upto more precision. **Standalone code to reproduce the issue** The current model arch and the compilation info:   The model was converted to the usual TFLite format as well, but the issue persisted.  The model, thus obtained, can be used to reproduce the issue on different machines. Any ideas on why it might be happening and how to deal with it would mean a lot. Thanks!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,rohanmishra21,Not getting consistent results with .h5 and .tflite models on different machines,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tested on Windows 11, Intel Mac, Mac M1 v11.6  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary (tensorflow for windows and tensorflowmacos for Mac)  TensorFlow version (use command below): 2.5.0  Python version: 3.8.12  Bazel version (if compiling from source): No  GCC/Compiler version (if compiling from source): No  CUDA/cuDNN version: 11.0 (for training only)  GPU model and memory: Mac M1 **Describe the current behavior** The same h5 and/or TFLite model gives different outputs on different machines. For an image regression model where ground truth labels range between [0, 3] the results differ by ~0.20.3 on different machines, which causes thresholding issues. **Describe the expected behavior** The same model should theoretically perform the same on all machines or at least they should match upto more precision. **Standalone code to reproduce the issue** The current model arch and the compilation info:   The model was converted to the usual TFLite format as well, but the issue persisted.  The model, thus obtained, can be used to reproduce the issue on different machines. Any ideas on why it might be happening and how to deal with it would mean a lot. Thanks!",2022-01-24T10:29:07Z,stat:awaiting response stale comp:lite type:performance TF 2.5,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53908, Could you please try on the latest version of TF v2.7.0 and let us know the outcome?Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.," Thanks for the reply! It seemed to be some issue with the tensorflowmetal plugin on the mac which led to inconsistencies, removing it solved the issue. Closing this issue now :)"
1879,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Node is not unique when frozen graph using convert_variables_to_constants_v2())ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Windows 10/11 and Ubutun 20  TensorFlow installed from pip  tf 2.5.0/2.7.0/2.9nightly  Python 3.6.9/3.8  NA  NA:  NA: **Describe the current behavior** From google/deeplab2/export_model.py, add the following line:  This issue also appears in tf2onnx tool which uses this convert_variables_to_constants_v2() call. Full command to run the above script.   Do you want to contribute a PR? (yes/no): no. I have no profound knowledge in this conversion, and it may take too much time for me to look into it. **Standalone code to reproduce the issue** export.py. Can get the save model from the following page, I used MaXDeepLabLBackbone https://github.com/googleresearch/deeplab2/blob/main/g3doc/projects/axial_deeplab.md **Other info / logs** >  > 20220121 17:28:22.472844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found > 20220121 17:28:22.473118: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. > Skipping registering GPU devices... > 20220121 17:28:33.104143: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the follow)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,posEdgeOfLife,Node is not unique when frozen graph using convert_variables_to_constants_v2(),"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Windows 10/11 and Ubutun 20  TensorFlow installed from pip  tf 2.5.0/2.7.0/2.9nightly  Python 3.6.9/3.8  NA  NA:  NA: **Describe the current behavior** From google/deeplab2/export_model.py, add the following line:  This issue also appears in tf2onnx tool which uses this convert_variables_to_constants_v2() call. Full command to run the above script.   Do you want to contribute a PR? (yes/no): no. I have no profound knowledge in this conversion, and it may take too much time for me to look into it. **Standalone code to reproduce the issue** export.py. Can get the save model from the following page, I used MaXDeepLabLBackbone https://github.com/googleresearch/deeplab2/blob/main/g3doc/projects/axial_deeplab.md **Other info / logs** >  > 20220121 17:28:22.472844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found > 20220121 17:28:22.473118: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. > Skipping registering GPU devices... > 20220121 17:28:33.104143: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the follow",2022-01-22T01:56:15Z,stat:awaiting response type:bug stale comp:model TF 2.5,closed,0,10,https://github.com/tensorflow/tensorflow/issues/53866,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!",">  hi, excuse me  for asking, i have already provided the code from deeplab2 and the code I added, but I don't have any snippet can run  independently. "," Thank you for the update!  Could you please try on the latest TF v2.7.0 ,tfnightly and let us know the outcome? Thanks!",">  Thank you for the update! Could you please try on the latest TF v2.7.0 ,tfnightly and let us know the outcome? Thanks! actually I just tested a code snippet that would run independently as long as you have the model downloaded.  ",">  Thank you for the update! Could you please try on the latest TF v2.7.0 ,tfnightly and let us know the outcome? Thanks! ignore my previous msg. tried with 2.7 and same error. new error log on 2.7: ",tried it on 2.9 nightly which returns same error. Logs: , Could you please post this issue in models repo to get the right help there ?Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1836,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Dataset: Image too large)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**  TensorFlow installed from (source or binary):  **binary**  TensorFlow version (use command below): **v2.7.0rc169gc256c071bb2 2.7.0**  Python version: **3.8.9**  Bazel version (if compiling from source): **N/A**  GCC/Compiler version (if compiling from source): **N/A**  CUDA/cuDNN version: **11.2 / 8.1**  GPU model and memory: **Tested on RTX 3080 10 GB & V100 16 GB** **Describe the current behavior** Hello, I am creating a TF Dataset from Gigapixel WSIs (whole slide images). The dimensions of them are 51968 x 37632 x 1 (grayscale) yielding a size of 1955659776.   This gives an error:  **Describe the expected behavior** Expected behavior is for tensorflow to load in the large images into the Dataset. **Contributing**  Do you want to contribute a PR? (yes/no): **No**  Briefly describe your candidate solution(if contributing): **N/A** **Other info / logs** I looked into the location of this error: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/jpeg/jpeg_mem.ccL182L186  The size of my images (1955659776) is 3.6 times larger than the maximum size (536870912). I'm curious as to why it is this specifically this number as the maximum size since I have access to 256 GB of memory. For large images like this would I have to create the dataset manually instead of `image_data)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,riteshahlawat,Tensorflow Dataset: Image too large,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**  TensorFlow installed from (source or binary):  **binary**  TensorFlow version (use command below): **v2.7.0rc169gc256c071bb2 2.7.0**  Python version: **3.8.9**  Bazel version (if compiling from source): **N/A**  GCC/Compiler version (if compiling from source): **N/A**  CUDA/cuDNN version: **11.2 / 8.1**  GPU model and memory: **Tested on RTX 3080 10 GB & V100 16 GB** **Describe the current behavior** Hello, I am creating a TF Dataset from Gigapixel WSIs (whole slide images). The dimensions of them are 51968 x 37632 x 1 (grayscale) yielding a size of 1955659776.   This gives an error:  **Describe the expected behavior** Expected behavior is for tensorflow to load in the large images into the Dataset. **Contributing**  Do you want to contribute a PR? (yes/no): **No**  Briefly describe your candidate solution(if contributing): **N/A** **Other info / logs** I looked into the location of this error: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/jpeg/jpeg_mem.ccL182L186  The size of my images (1955659776) is 3.6 times larger than the maximum size (536870912). I'm curious as to why it is this specifically this number as the maximum size since I have access to 256 GB of memory. For large images like this would I have to create the dataset manually instead of `image_data",2022-01-22T01:39:31Z,stat:awaiting response type:support stale comp:keras TF 2.7,closed,0,8,https://github.com/tensorflow/tensorflow/issues/53865,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!", Filled!,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!",">  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks! I have already provided a code snippet that loads in a dataset of images with an extremely large resolution, uploading the dataset would be an issue but any highresolution image over size 536870912 (width x height x channels) would work.",  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1031,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(normalization should be done using moving averages?)ï¼Œ å†…å®¹æ˜¯ ( URL(s) with the issue: https://www.tensorflow.org/tutorials/structured_data/time_seriesnormalize_the_data  Description of issue (what needs changing): Hello this is a small feature request. In the Time series tutorial at the normalization section in the 3rd paragraph there is a remark saying: ""and that this normalization should be done using moving averages."" I am very curious on how a moving average would be used here.  Should  a ""simple moving average"" for the whole dataset be computed and then on that new ""datasetMA"" just use the normalization technique that is described in the tutorial? The feature I am requesting is something like a remark on how this would be done or maybe a pointer to different tutorial if one exists or some further references/research users could look into.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,1knueller,normalization should be done using moving averages?," URL(s) with the issue: https://www.tensorflow.org/tutorials/structured_data/time_seriesnormalize_the_data  Description of issue (what needs changing): Hello this is a small feature request. In the Time series tutorial at the normalization section in the 3rd paragraph there is a remark saying: ""and that this normalization should be done using moving averages."" I am very curious on how a moving average would be used here.  Should  a ""simple moving average"" for the whole dataset be computed and then on that new ""datasetMA"" just use the normalization technique that is described in the tutorial? The feature I am requesting is something like a remark on how this would be done or maybe a pointer to different tutorial if one exists or some further references/research users could look into.",2022-01-21T19:39:23Z,stat:awaiting response type:feature stale,closed,0,6,https://github.com/tensorflow/tensorflow/issues/53858,  Can you please fill the issue template.Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thanks!,I created the issue with the relevant parts of that template.,did the tutorial just mean to use something like pandas.rolling function to create the data?,"The tutorial uses the simple average to normalize the data, the intention of the tutorial is to create a Time series forecasting using Tensorflow.  If you feel any of the description is irrelevant/inappropriate feel free to create a PR. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Expose half_pixel_center or anti-aliasing parameter in the keras resizing layer.)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): TF 2.6  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** The feature that I am proposing is basically a fix for the current resizing keras layer which sets the `half_pixel_center` to true by default which causes problem on nnapi (android) based platforms and also on snapdragon based platforms. There are 2 possible solution to this problem   1. You expose the parameters `antialias` or `half_pixel_center` via the keras Resizing layer and let the user explicitly set either of the 2 properties. Better would be to expose `antialias` which can be defaulted to `None` or `True` since resizing is used mostly in later layers and not for downsampling it should be fine. 2.  Add an automated check within the resize base function to check if the half_pixel_center is actually needed. From my understanding when you are resizing just check if the (inputsize 1)/(outputsize1) when downsampling or vice versa when upsampling, is fractional or int. If fractional set half_pixel_center to true else set it to false.  **Will this change the current api? How?** Yes this will change the current api by either exposing certain optional parameters or by adding an automated check. **Who will benefit with this feature?** Everyone running their NN on somekind of DSP. **Any Other info.**)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,bayesian-mind,Expose half_pixel_center or anti-aliasing parameter in the keras resizing layer.,"Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): TF 2.6  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** The feature that I am proposing is basically a fix for the current resizing keras layer which sets the `half_pixel_center` to true by default which causes problem on nnapi (android) based platforms and also on snapdragon based platforms. There are 2 possible solution to this problem   1. You expose the parameters `antialias` or `half_pixel_center` via the keras Resizing layer and let the user explicitly set either of the 2 properties. Better would be to expose `antialias` which can be defaulted to `None` or `True` since resizing is used mostly in later layers and not for downsampling it should be fine. 2.  Add an automated check within the resize base function to check if the half_pixel_center is actually needed. From my understanding when you are resizing just check if the (inputsize 1)/(outputsize1) when downsampling or vice versa when upsampling, is fractional or int. If fractional set half_pixel_center to true else set it to false.  **Will this change the current api? How?** Yes this will change the current api by either exposing certain optional parameters or by adding an automated check. **Who will benefit with this feature?** Everyone running their NN on somekind of DSP. **Any Other info.**",2022-01-21T19:03:40Z,stat:awaiting response type:feature stale comp:keras,closed,1,3,https://github.com/tensorflow/tensorflow/issues/53856,mind  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
865,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Is there a direct way to add Flex delegate to options in Android Tensorflow Lite 2.5 on C API ?)ï¼Œ å†…å®¹æ˜¯ (I'm trying to use tflite 2.5 and  a tflite model converted from tensorflow model and I need to use flex delegate  in android on a native level C api. There is a method in c header lite\c\c_api.h TFL_CAPI_EXPORT extern void TfLiteInterpreterOptionsAddDelegate(     TfLiteInterpreterOptions* options, TfLiteDelegate* delegate); But: there is no api for creating TfLiteDelegate object. Is it possible to use this TfLiteInterpreterOptionsAddDelegate method or  is this method just a prank? I need an api for creating flex TfLiteDelegate  object and If such a method exitsts, where is it?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fatihkiralioglu,Is there a direct way to add Flex delegate to options in Android Tensorflow Lite 2.5 on C API ?,"I'm trying to use tflite 2.5 and  a tflite model converted from tensorflow model and I need to use flex delegate  in android on a native level C api. There is a method in c header lite\c\c_api.h TFL_CAPI_EXPORT extern void TfLiteInterpreterOptionsAddDelegate(     TfLiteInterpreterOptions* options, TfLiteDelegate* delegate); But: there is no api for creating TfLiteDelegate object. Is it possible to use this TfLiteInterpreterOptionsAddDelegate method or  is this method just a prank? I need an api for creating flex TfLiteDelegate  object and If such a method exitsts, where is it?",2022-01-21T07:18:55Z,stat:awaiting response type:support comp:lite TF 2.5,closed,0,5,https://github.com/tensorflow/tensorflow/issues/53849,Hi  ! Could you please look at this query?, could you help take a look? Thanks!,"  You can use TfLiteInterpreterOptionsAddDelegate for sure. How to get TfLiteDelegate ? TfLite delegates expose C API functions to create TfLiteDelegate instances example,  GPU  XNNPACK Hexagon For Flex Delegate, you don't need to add it as long as you're using TFLite which already built with TF Select support  see here for android build that links Flex. We apply it automatically underneath. Let me know if you have more questions. Thanks",Closing. Please feel free to reopen if you have other questions/issues. Thanks,Are you satisfied with the resolution of your issue? Yes No
701,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Post Training Quantization doesn't work for tf.divide and tf.negative in TF 2.7)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution: Linux Ubuntu 18.04  TensorFlow installation: from pip  TensorFlow library: 2.7.0 (and 2.6.2 for reference)  2. Code   3. Failure after conversion Conversion passes succesfuly. In TF 2.7 the converted network isn't quantized, but in TF 2.6.2 the network is quantized correctly. output for TF 2.6.2:  output for TF 2.7.0:  The output of TF 2.7 is missing the QuantizeDequantize nodes)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,elad-c,Post Training Quantization doesn't work for tf.divide and tf.negative in TF 2.7," 1. System information  OS Platform and Distribution: Linux Ubuntu 18.04  TensorFlow installation: from pip  TensorFlow library: 2.7.0 (and 2.6.2 for reference)  2. Code   3. Failure after conversion Conversion passes succesfuly. In TF 2.7 the converted network isn't quantized, but in TF 2.6.2 the network is quantized correctly. output for TF 2.6.2:  output for TF 2.7.0:  The output of TF 2.7 is missing the QuantizeDequantize nodes",2022-01-20T14:43:16Z,stat:awaiting response type:bug stale regression issue TFLiteConverter TF 2.7,closed,0,12,https://github.com/tensorflow/tensorflow/issues/53837,"c , I was facing different error while trying to execute the mentioned code.Please find the gist of it here.",I've removed the last 2 lines which cuased the exception. I used them to save the models to disk and later open them with netron. It should work now,"for example, here's a Tensorflow model: !image This is the output of the converter with TensorFlow 2.6.2: !image and this is the converter's output with TensorFlow 2.7.0: !image","c , I was able to execute the mentioned code in colab.Please find the gist here.Can you please provide the colab gist where you are facing issue.It helps to debug the issue.Thanks!",Editted the code to print the difference in outputs. The difference matches the plots of the networks I added in the previous comment.,"c , With the updated code also i was not able to get the output which you are mentioned.Please find the gist.","Looks like you got exactly the output I got (for TF 2.7.0)  When I changed the gist to install TF 2.6.3, I got:  Note the TF 2.6.3 quantized the network (tfl.qantize, tfl.dequantize), and TF 2.7.0 did not. That is the bug I reported in this issue"," , I was able to reproduce the issue in tf v2.7 and nightly, where in v2.5 the code executed as expected.Please find the gist of it here.",Hi c !I am able to get an integer quantized model by setting the **converter.inference_input_type** and **converter.inference_ouput_type** to tf.uint8.  !image Attaching gist and relevant thread for reference.    Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
671,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ValueError: Failed to find data adapter that can handle input)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10  TensorFlow installed from (source or binary):pip  TensorFlow version (use command below):2.5.0  Python version:3.7.11  CUDA/cuDNN version:11.2  GPU model and memory: rtx3060 laptop 6g problem:  networks:  data generator:  train code: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghost,ValueError: Failed to find data adapter that can handle input,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10  TensorFlow installed from (source or binary):pip  TensorFlow version (use command below):2.5.0  Python version:3.7.11  CUDA/cuDNN version:11.2  GPU model and memory: rtx3060 laptop 6g problem:  networks:  data generator:  train code: ",2022-01-20T14:03:32Z,stat:awaiting response type:bug comp:keras TF 2.5,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53836,Hi  ! I did not have the relevant dataset. Can you try one of following  1. convert involved dataset to numpy arrays. 2. use tf.keras instead of keras.  Attaching relevant thread for reference .Thanks., thank you very much,Are you satisfied with the resolution of your issue? Yes No
542,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to find data adapter that can handle input: )ï¼Œ å†…å®¹æ˜¯ (Hi, I am getting the error while writing the code:  Below is the data generator I have definedï¼š  The code when training the model is as followsï¼š  The answer I found on stackoverflow didn't work for me, the data type returned by the data generator was already numpy. the version of my tensorflow is 2.5.0.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ghost,Failed to find data adapter that can handle input: ,"Hi, I am getting the error while writing the code:  Below is the data generator I have definedï¼š  The code when training the model is as followsï¼š  The answer I found on stackoverflow didn't work for me, the data type returned by the data generator was already numpy. the version of my tensorflow is 2.5.0.",2022-01-20T13:40:46Z,stat:awaiting response,closed,0,2,https://github.com/tensorflow/tensorflow/issues/53835,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!",i submit a new issue  
1902,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(wierd behavior while running the inference on 2 different deep learning models(pb graphs) at the same)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 1.15.0  Python version:3.6  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: 10.0.130/ 7.6.5  GPU model and memory: Nvidia GeForce GTX 1080 Ti  11GB **Describe the current behavior** I am trying to run 2 tensorflow models by loading the 2 different pb graphs.  However I also see that when I a load the 2 models together, the 2nd session is getting the values of the first session.  I am suspecting that as the both models have input_1 layer as their first layer in their graph, this could be somehow creating a  problem when they are loaded at the same time.  I have included 2 experiments to prove the same.  Experiment 1 > When I am running the `run_SS_model.py` file, below is the output for my first and last layer !image Experiment 2 > When I am running the `run_two_models.py` file, note that the dimensions of the first layer is overwritten to the dimensions of the 2nd pb graph. !image **Describe the expected behavior** 2 Models should be executed without any problem. **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a r)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sachinkmohan,wierd behavior while running the inference on 2 different deep learning models(pb graphs) at the same,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 1.15.0  Python version:3.6  Bazel version (if compiling from source): N/A  GCC/Compiler version (if compiling from source): N/A  CUDA/cuDNN version: 10.0.130/ 7.6.5  GPU model and memory: Nvidia GeForce GTX 1080 Ti  11GB **Describe the current behavior** I am trying to run 2 tensorflow models by loading the 2 different pb graphs.  However I also see that when I a load the 2 models together, the 2nd session is getting the values of the first session.  I am suspecting that as the both models have input_1 layer as their first layer in their graph, this could be somehow creating a  problem when they are loaded at the same time.  I have included 2 experiments to prove the same.  Experiment 1 > When I am running the `run_SS_model.py` file, below is the output for my first and last layer !image Experiment 2 > When I am running the `run_two_models.py` file, note that the dimensions of the first layer is overwritten to the dimensions of the 2nd pb graph. !image **Describe the expected behavior** 2 Models should be executed without any problem. **Contributing**  Do you want to contribute a PR? (yes/no): no  Briefly describe your candidate solution(if contributing): **Standalone code to reproduce the issue** Provide a r",2022-01-20T12:46:00Z,stat:awaiting response type:bug stale comp:model TF 2.7,closed,0,11,https://github.com/tensorflow/tensorflow/issues/53834," , We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable v2.7 and let us know if you are facing same issue.","I tested this against TF 2.7 now, but I am still getting the same error. Please refer the screenshot. I have updated my gist Please follow the README.md file to replicate the same in TF2.7 environment. !TF2 7 bug"," , Sorry for the delay response.This issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from here. Thanks!", Thanks for the response! But these models I tested are not part of tensorflow models repo. These are the 2 repositories I used. These code are build using tensorflow.   For Obj Detection  https://github.com/pierluigiferrari/ssd_keras  Semantic Segmentation  https://github.com/qubvel/segmentation_models Please advice!,The code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!,Quite disappointed despite providing so much information. You may close this. Hope you fix this bug sometime in the future. ," , While executing the given code in colab, i was facing different error and  noticed session is being crashed.Please find the gist here.", Looks like one of the graph was set as a default graph and the code is using that graph (instead of second graph). If the two graphs are not intended to be interacting with each other then you can create them under two different scope and perform the tasks. Please check this SO answer. Thanks! ,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
777,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear)ï¼Œ å†…å®¹æ˜¯ (Documentation bug for: https://www.tensorflow.org/api_docs/python/tf/RaggedTensor?hl=en__and__  Description of issue:  Usage examples are unmatched `tf.RaggedTensor` has a number of class methods, for example, `__abs__`, `__add__`, ... The usage examples for these methods are unmatched. For instance, the example code for `__add__` does not contain any **ragged tensors**, instead the examples are all normal tensors (of type `tf.Tensor`).  The example code is:  That is not expected for a `tf.RaggedTensor.__add__()` method.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ArrowIntoTheSky,Descriptions of `tf.RaggedTensor`'s methods '__xx__' are unclear,"Documentation bug for: https://www.tensorflow.org/api_docs/python/tf/RaggedTensor?hl=en__and__  Description of issue:  Usage examples are unmatched `tf.RaggedTensor` has a number of class methods, for example, `__abs__`, `__add__`, ... The usage examples for these methods are unmatched. For instance, the example code for `__add__` does not contain any **ragged tensors**, instead the examples are all normal tensors (of type `tf.Tensor`).  The example code is:  That is not expected for a `tf.RaggedTensor.__add__()` method.",2022-01-20T00:50:23Z,type:docs-bug stat:awaiting response stale comp:ops TF 2.8,closed,0,12,https://github.com/tensorflow/tensorflow/issues/53828,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, and provide more details on this? Please let us know if you want to contribute to this. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further., Sorry for the late response. I have updated the description of the issue to include more details. Can you please look into this?,Added a PR  CC(Fix invalid examples for `tf.RaggedTensor.__abs__`) for the fix.,", A RaggedTensor is a subclass of Tensor. If the method is not redefined, the parent class method is used. Hence the examples given are in dense tensor. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Reopening as https://github.com/tensorflow/tensorflow/issues/53828issuecomment1101038091 is a comment on CC(Fix invalid examples for `tf.RaggedTensor.__abs__`) which could be edited to incorporate the comment and add proper ragged tensor description,"Fix provided here https://github.com/tensorflow/tensorflow/commit/d0356e76730aee601a91f342d32a9671a487230a contains the example of Ragged Tensors, it is reflecting in the document as well here https://www.tensorflow.org/api_docs/python/tf/RaggedTensor?hl=en&version=nightly__abs__ ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1821,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(bazel compile errors)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template OS Platform and Distribution:  Linux centos 7.6 TensorFlow installed from source TensorFlow version: 2.7.0 Python version: 3.9.6 Bazel version: 4.2.2 GCC/Compiler version (if compiling from source): 7.3.1 **Describe the problem** Hello, I want to get libtensorflowlite.so with bazel, but I met a problem.  The llvm tar file is wrong, it can't be exacted. **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel build c opt cxxopt=std=c++11 config=mkl config=numa config=monolithic //tensorflow/lite:libtensorflowlite.so **Any other info / logs** Following is the error logs: Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz, https://github.com/llvm/llvmproject/archive/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz] to /home/cjh/.cache/bazel/_bazel_cjh/904c1e9488234a0ff2cc21c6287b3e45/external/llvmraw/temp9533793750467514112/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz: Premature EOF I try to extract the temp9533793750467514112/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz file, then a same problem occurs. The downloading file is wrong. [cjh tensorflowmaster]$ tar xf llvmproject2325f363010d3176e96579628cbb96b8fca003a1.tar.gz gzip: stdin: unexpected end of file tar: Unexpected )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,lantianguhong,bazel compile errors,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template OS Platform and Distribution:  Linux centos 7.6 TensorFlow installed from source TensorFlow version: 2.7.0 Python version: 3.9.6 Bazel version: 4.2.2 GCC/Compiler version (if compiling from source): 7.3.1 **Describe the problem** Hello, I want to get libtensorflowlite.so with bazel, but I met a problem.  The llvm tar file is wrong, it can't be exacted. **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel build c opt cxxopt=std=c++11 config=mkl config=numa config=monolithic //tensorflow/lite:libtensorflowlite.so **Any other info / logs** Following is the error logs: Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz, https://github.com/llvm/llvmproject/archive/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz] to /home/cjh/.cache/bazel/_bazel_cjh/904c1e9488234a0ff2cc21c6287b3e45/external/llvmraw/temp9533793750467514112/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz: Premature EOF I try to extract the temp9533793750467514112/2325f363010d3176e96579628cbb96b8fca003a1.tar.gz file, then a same problem occurs. The downloading file is wrong. [cjh tensorflowmaster]$ tar xf llvmproject2325f363010d3176e96579628cbb96b8fca003a1.tar.gz gzip: stdin: unexpected end of file tar: Unexpected ",2022-01-19T07:41:21Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.7,closed,0,6,https://github.com/tensorflow/tensorflow/issues/53814,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you., Could you please refer the build from source  and check the tested build configurations ?Please let us know if it helps?Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1832,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AttributeError: 'Tensor' object has no attribute '_keras_mask' TF 2.7)ï¼Œ å†…å®¹æ˜¯ (AttributeError: 'Tensor' object has no attribute '_keras_mask' Traceback (most recent call last):   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception     yield                      File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/distribute/mirrored_run.py"", line 346, in run     self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/autograph/impl/api.py"", line 699, in wrapper     raise e.ag_error_metadata.to_exception(e) AttributeError: in user code: **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below): 2.7  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** currently we use tf 2.2 and the code runs fine. but any version from 2.3 its failing)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,kabilan6,AttributeError: 'Tensor' object has no attribute '_keras_mask' TF 2.7,"AttributeError: 'Tensor' object has no attribute '_keras_mask' Traceback (most recent call last):   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception     yield                      File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/distribute/mirrored_run.py"", line 346, in run     self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)   File ""/usr/local/lib/python3.8/distpackages/tensorflow/python/autograph/impl/api.py"", line 699, in wrapper     raise e.ag_error_metadata.to_exception(e) AttributeError: in user code: **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below): 2.7  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` **Describe the current behavior** currently we use tf 2.2 and the code runs fine. but any version from 2.3 its failing",2022-01-18T22:06:47Z,stat:awaiting response type:bug stale comp:keras TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/53811,Hi !  Could you provide a stand alone code to reproduce this issue? Attaching relevant thread for reference. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1838,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Installing TF without Google OAuthLib)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution: Docker deployment with RedHat 8 base image  TensorFlow installed from (source or binary): binary  TensorFlow version: 2.7  Python version: 3.8.8  Installed using virtualenv? pip? conda?: virtualenv  CUDA/cuDNN version: TBA (Will get info, but don't think it's relevant to issue)  GPU model and memory: TBA (Will get info, but don't think it's relevant to issue) **Problem** Hi, Where I work, we use Twistlock, a container security analysis tool, in our current workflow. Twistlock reports this error with our current container: `There is no support for PKCE implementation in the oauthlib client. Clientside PKCE for OAuth2 RFC 7636 is required for applications to have secure communication with the authorization server. OAuth 2.0 public clients utilizing the Authorization Code Grant are susceptible to the authorization code interception attack.` We don't have the luxury to use another version of TF; we effectively need to use as close to the latest version as we can. That said, we need to be able to install TF but without this lib. Is this possible? I should mention it seems this lib is needed to train models on Google Cloud (or so I read), but we don't need that functionality. It would be helpful if this could be an optional feature instead of a required one. Currently, this is preventing us from deploying our container, as we can't deploy if this error yields. I also apologize if this is improper use of this ticket (or ticketing system), I was asked to create one in the mean time, so the people managing Twistlock can make an exception if th)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,x0734x,Installing TF without Google OAuthLib,"**System information**  OS Platform and Distribution: Docker deployment with RedHat 8 base image  TensorFlow installed from (source or binary): binary  TensorFlow version: 2.7  Python version: 3.8.8  Installed using virtualenv? pip? conda?: virtualenv  CUDA/cuDNN version: TBA (Will get info, but don't think it's relevant to issue)  GPU model and memory: TBA (Will get info, but don't think it's relevant to issue) **Problem** Hi, Where I work, we use Twistlock, a container security analysis tool, in our current workflow. Twistlock reports this error with our current container: `There is no support for PKCE implementation in the oauthlib client. Clientside PKCE for OAuth2 RFC 7636 is required for applications to have secure communication with the authorization server. OAuth 2.0 public clients utilizing the Authorization Code Grant are susceptible to the authorization code interception attack.` We don't have the luxury to use another version of TF; we effectively need to use as close to the latest version as we can. That said, we need to be able to install TF but without this lib. Is this possible? I should mention it seems this lib is needed to train models on Google Cloud (or so I read), but we don't need that functionality. It would be helpful if this could be an optional feature instead of a required one. Currently, this is preventing us from deploying our container, as we can't deploy if this error yields. I also apologize if this is improper use of this ticket (or ticketing system), I was asked to create one in the mean time, so the people managing Twistlock can make an exception if th",2022-01-18T21:24:04Z,stat:awaiting response type:build/install stale TF 2.7,closed,0,8,https://github.com/tensorflow/tensorflow/issues/53810,"  In order to expedite the troubleshooting process here,Could you please fill the issue template, Thanks!",I updated it a bit. Let me know if you need more info.," Could you please create another virtual env and try the same?Please refer to the build from source, install TF2, docker guide and let us know if it helps?Thanks!",I think there is a misunderstanding here... I'm not having trouble installing Tensorflow. I'm saying the installation of TF requires a library *that has a known security vulnerability* as described by Twistlock  a service we use to scan for vulnerabilities. So I'm asking is there a way to install TF without this Google OAuth lib? (Cause we don't need the library) I don't see anywhere in the installation guides where this lib can be made optional if installed from source either.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
316,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add missing return statement to AveragePool16)ï¼Œ å†…å®¹æ˜¯ (Fixes bug in AveragePool16 introduced by CC(Prevent a division by 0 in average ops.))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,NAgarwalla,Add missing return statement to AveragePool16,Fixes bug in AveragePool16 introduced by CC(Prevent a division by 0 in average ops.),2022-01-18T15:06:18Z,,closed,0,1,https://github.com/tensorflow/tensorflow/issues/53807,Duplicate of CC(Added missing 'return true')
1319,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation model C++ API's based inference output different from Python tensors)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code:No  OS Platform and Distribution (e.g., Linux Ubuntu 18.04):  Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): v2.3.0  Python version: 3.6.9  Bazel version (if compiling from source): 3.1.0  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version: 10.1cudnn7  GPU model and memory: **Describe the current behavior** We've trained a Segmentation model using Keras with Tensorflow backend. Now, We are trying to infer by using the TF C++ interface. I used the below code snippet to convert the .h5 model to a .pb file.  Our python pipeline,  our C++ pipeline,  The tensor value is different when I compare the result between Python and C++. And, C++ argmax API returning zero for all the tensor as well. **Describe the expected behavior** The output expected to be same in both Python and C++. Maybe with some precision changes. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nullbyte91,Segmentation model C++ API's based inference output different from Python tensors,"**System information**  Have I written custom code:No  OS Platform and Distribution (e.g., Linux Ubuntu 18.04):  Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): v2.3.0  Python version: 3.6.9  Bazel version (if compiling from source): 3.1.0  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version: 10.1cudnn7  GPU model and memory: **Describe the current behavior** We've trained a Segmentation model using Keras with Tensorflow backend. Now, We are trying to infer by using the TF C++ interface. I used the below code snippet to convert the .h5 model to a .pb file.  Our python pipeline,  our C++ pipeline,  The tensor value is different when I compare the result between Python and C++. And, C++ argmax API returning zero for all the tensor as well. **Describe the expected behavior** The output expected to be same in both Python and C++. Maybe with some precision changes. ",2022-01-18T04:48:50Z,stat:awaiting response type:bug stale comp:core TF 2.3,closed,0,5,https://github.com/tensorflow/tensorflow/issues/53802," ,  In order to expedite the troubleshooting process, could you please provide a complete code to reproduce the issue.Also could you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,"ArgMax doesnt work, I suppose it has a bug, it returns a list of zero for the tensor"
1529,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite GPU delegate crash)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): nightly  Python version: N/A  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): 8.4  CUDA/cuDNN version: 11.1  GPU model and memory: GTX 1650, 4GB There is a segfault in the destructor of `tflite::gpu::cl::Buffer::Release()`, the stack trace is as follows:  This problem is only producible with NVidia's OpenCL. However, other implementations of OpenCL (e.g. Qualcomm) is known to let undefined behaviors pass silently.  I believe the problem is related to OpenCL sub buffers. If I hack the tensorflow/lite/delegates/gpu/cl/inference_context. `use_offset_assignment` to `false` right before:   thus preventing `CreateReadWriteSubBuffer` from being ever called, then this problem goes away. Incidentally, I also noticed that the model I am trying to infer runs faster without using sub buffers (20ms with my hack disabling sub buffers, 30ms if I allow sub buffers, on my GTX 1650). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DwayneDuane,TFLite GPU delegate crash,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): nightly  Python version: N/A  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): 8.4  CUDA/cuDNN version: 11.1  GPU model and memory: GTX 1650, 4GB There is a segfault in the destructor of `tflite::gpu::cl::Buffer::Release()`, the stack trace is as follows:  This problem is only producible with NVidia's OpenCL. However, other implementations of OpenCL (e.g. Qualcomm) is known to let undefined behaviors pass silently.  I believe the problem is related to OpenCL sub buffers. If I hack the tensorflow/lite/delegates/gpu/cl/inference_context. `use_offset_assignment` to `false` right before:   thus preventing `CreateReadWriteSubBuffer` from being ever called, then this problem goes away. Incidentally, I also noticed that the model I am trying to infer runs faster without using sub buffers (20ms with my hack disabling sub buffers, 30ms if I allow sub buffers, on my GTX 1650). ",2022-01-17T21:29:28Z,stat:awaiting response type:bug stale TFLiteGpuDelegate TF 2.7,closed,0,11,https://github.com/tensorflow/tensorflow/issues/53800," , In order to expedite the troubleshooting process, could you please provide the complete code to reproduce the issue reported here.", You can reproduce this problem with:  and the floating point Inception V4 model from https://www.tensorflow.org/lite/guide/hosted_models Just a reminder that this problem is only reproducible on NVidia GPU and using TFLite v2.7 or more recent. I am using the nightly branch.,  Could you please try with 2.6 and let us know if this is still an issue.,"   This issue is not reproducible in 2.6.   In the current nightly branch, I am almost sure the problem is caused by the `GreedyBySizeAssignment` function in `greedy_by_size_assignment.cc`.  If I change the **memory strategy** in this line inside `AllocateMemoryForBuffers` function (inside `inference_context.cc`):  to `MemoryStrategy::EQUALITY`, the problem goes away and valgrind turns up clean as well.",  Can you please use 2.6 as its a stable version and nightly has is still worked upon and yest to be tested.," I might be able to use 2.6 for now. But will you be fixing this issue for 2.8? In the meantime, if you wish, I can make a PR to change MemoryStrategy::GREEDY_BY_SIZE to MemoryStrategy::EQUALITY (or some other strategy).",  Please feel free to create a pr for the same.,", Does this similar thread helps  to resolve this problem. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1120,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Feature Request] Support for convolutional layers for `tf.autodiff.ForwardAccumulator`)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.7  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** Currently, the implementation of `tf.autodiff.ForwardAccumulator` only officially supports Dense layers. **Will this change the current api? How?** Yes. The new API will be able to calculate the JVP also for convolutional layers. **Who will benefit from this feature?** Anyone who wants to implement a neural network with convolutional layers and needs forwardmode autodiff. **Any Other info.** Trying to use the current API results in a shape mismatch:  Summary:  Error: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,IAL32,[Feature Request] Support for convolutional layers for `tf.autodiff.ForwardAccumulator`,"Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template **System information**  TensorFlow version (you are using): 2.7  Are you willing to contribute it (Yes/No): Yes **Describe the feature and the current behavior/state.** Currently, the implementation of `tf.autodiff.ForwardAccumulator` only officially supports Dense layers. **Will this change the current api? How?** Yes. The new API will be able to calculate the JVP also for convolutional layers. **Who will benefit from this feature?** Anyone who wants to implement a neural network with convolutional layers and needs forwardmode autodiff. **Any Other info.** Trying to use the current API results in a shape mismatch:  Summary:  Error: ",2022-01-17T09:55:18Z,type:feature comp:eager,closed,0,1,https://github.com/tensorflow/tensorflow/issues/53793,"Closing as I was using the wrong shape for the `tangent` tensor. In this particular case, it should have been of shape =  `(2, 2, 1, 6)`."
1468,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Android Tensorflow ObjectDetection lib_task_api default number of threads?)ï¼Œ å†…å®¹æ˜¯ (Hello, We are working on implementing Tensorflow object detection based on the example provided here: https://github.com/tensorflow/examples/tree/eb925e460f761f5ed643d17f0c449e040ac2ac45/lite/examples/object_detection/android It seems that there are 2 implementations for that one:  We are using `lib_task_api` and we don't see the way how we can set the number of threads with it?  For the `lib_interpreter`  there is a variable that is being set up for that. https://github.com/tensorflow/examples/blob/a228a3460f3fdd8edee9e8b061a08ffc92629907/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.javaL138 On the other hand  for `lib_task_api` i see that this method is deprecated and it is not set up https://github.com/tensorflow/examples/blob/eb925e460f761f5ed643d17f0c449e040ac2ac45/lite/examples/object_detection/android/lib_task_api/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.javaL87 My question being  what is the default number of threads for `lib_task_api`?  I am asking this one because i would like to tune it up for different set of devices. Best Regards)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,AleksandarTokarev,Android Tensorflow ObjectDetection lib_task_api default number of threads?,"Hello, We are working on implementing Tensorflow object detection based on the example provided here: https://github.com/tensorflow/examples/tree/eb925e460f761f5ed643d17f0c449e040ac2ac45/lite/examples/object_detection/android It seems that there are 2 implementations for that one:  We are using `lib_task_api` and we don't see the way how we can set the number of threads with it?  For the `lib_interpreter`  there is a variable that is being set up for that. https://github.com/tensorflow/examples/blob/a228a3460f3fdd8edee9e8b061a08ffc92629907/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.javaL138 On the other hand  for `lib_task_api` i see that this method is deprecated and it is not set up https://github.com/tensorflow/examples/blob/eb925e460f761f5ed643d17f0c449e040ac2ac45/lite/examples/object_detection/android/lib_task_api/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.javaL87 My question being  what is the default number of threads for `lib_task_api`?  I am asking this one because i would like to tune it up for different set of devices. Best Regards",2022-01-16T19:40:43Z,stat:awaiting response stale comp:lite type:others comp:model,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53786," , This issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from here. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
880,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`tf.scatter_nd` document refers to deprecated apis)ï¼Œ å†…å®¹æ˜¯ ( URL(s) with the issue: https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/scatter_nd  Description of issue (what needs changing):  Clear description  Here is a paragraph describing the relationship between `tf.scatter_nd` and `tf.tensor_scatter_add`: > This operation is similar to tf.tensor_scatter_add, except that the tensor is zeroinitialized. Calling tf.scatter_nd(indices, values, shape) is identical to calling tf.tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values). However,  `tf.tensor_scatter_add` does not exist in TensorFlow 2.7, `tf.tensor_scatter_add` should be replaced with `tf.tensor_scatter_nd_add`. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ArrowIntoTheSky,`tf.scatter_nd` document refers to deprecated apis," URL(s) with the issue: https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/scatter_nd  Description of issue (what needs changing):  Clear description  Here is a paragraph describing the relationship between `tf.scatter_nd` and `tf.tensor_scatter_add`: > This operation is similar to tf.tensor_scatter_add, except that the tensor is zeroinitialized. Calling tf.scatter_nd(indices, values, shape) is identical to calling tf.tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values). However,  `tf.tensor_scatter_add` does not exist in TensorFlow 2.7, `tf.tensor_scatter_add` should be replaced with `tf.tensor_scatter_nd_add`. ",2022-01-15T22:57:25Z,type:docs-bug comp:ops,closed,0,7,https://github.com/tensorflow/tensorflow/issues/53783,I can fix this," ,  , Please feel free to submit a PR for the requested change or share the link where requested change is to be made."," cannot find the source file for the mentioned issue, can you help me a bit?",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, Thanks for raising this issue. This was merged internally and will show up soon on the TF website. Thanks again.
1933,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inference from frozen_graph yields all black result despite deeplab/vis.py works as intended (frozen_graph uses the same checkpoint))ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Stock code  Both on Windows 10 and Google colab  TensorFlow 1.x on google colab.  TensorFlow 2.6.2  TensorFlow version (use command below): You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` I used the deeplabibrary to train my models. This is the script i used to create the checkpoint ` %tensorflow_version 1.x !pip install tf_slim from google.colab import drive drive.mount('/content/drive') !python /content/drive/MyDrive/models/research/deeplab/train.py logtostderr \    training_number_of_steps=30000 \    train_split=""train"" \    model_variant=""xception_65"" \    atrous_rates=6 \    atrous_rates=12 \    atrous_rates=18 \    output_stride=16 \    decoder_output_stride=4 \   train_crop_size=""513,513"" \    train_batch_size=1 \    dataset=""pascal_voc_seg"" \    tf_initial_checkpoint=""/content/drive/MyDrive/models/deeplabv3_pascal_train_aug/model.ckpt"" \    train_logdir=""/content/drive/MyDrive/models/checkpoint_exc_all2"" \    dataset_dir=""/content/drive/MyDrive/models/tfrecord_all"" \    fine_tune_batch_norm=false \    initialize_last_layer=true \    last_layers_contain_logits_only=false` Then, this is the working vis.py: `%tensorfl)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tix00,Inference from frozen_graph yields all black result despite deeplab/vis.py works as intended (frozen_graph uses the same checkpoint),"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Stock code  Both on Windows 10 and Google colab  TensorFlow 1.x on google colab.  TensorFlow 2.6.2  TensorFlow version (use command below): You can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: `python c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` I used the deeplabibrary to train my models. This is the script i used to create the checkpoint ` %tensorflow_version 1.x !pip install tf_slim from google.colab import drive drive.mount('/content/drive') !python /content/drive/MyDrive/models/research/deeplab/train.py logtostderr \    training_number_of_steps=30000 \    train_split=""train"" \    model_variant=""xception_65"" \    atrous_rates=6 \    atrous_rates=12 \    atrous_rates=18 \    output_stride=16 \    decoder_output_stride=4 \   train_crop_size=""513,513"" \    train_batch_size=1 \    dataset=""pascal_voc_seg"" \    tf_initial_checkpoint=""/content/drive/MyDrive/models/deeplabv3_pascal_train_aug/model.ckpt"" \    train_logdir=""/content/drive/MyDrive/models/checkpoint_exc_all2"" \    dataset_dir=""/content/drive/MyDrive/models/tfrecord_all"" \    fine_tune_batch_norm=false \    initialize_last_layer=true \    last_layers_contain_logits_only=false` Then, this is the working vis.py: `%tensorfl",2022-01-14T20:53:43Z,type:support comp:model 2.6.0,closed,0,13,https://github.com/tensorflow/tensorflow/issues/53770,Hi ! Could you please share the model file or update the error trace in above template? Thanks!,> Hi ! Could you please share the model file or update the error trace in above template? Thanks! Do you need the .pb file or the checkpoint? And there is no error trace. It simply output and all blank(zeros) result. While it is working with the checkpoint model. https://drive.google.com/file/d/15ad_AcTyfEmUWh5Gu7_JCmVu7ZPa8P/view?usp=sharing,> Hi ! Could you please share the model file or update the error trace in above template? Thanks! Am i missing something? Do you need more information?, !  Its 1.x code actually which you might be trying to run compatibility mode in 2.6 which might be cause of issue. Can you upgrade your code base to 2.7 version and let us know ? Thanks!,"The whole code base(so redoing the preprocessing/training/export_graph) or just trying to use model .pb file?  Because otherwise I already tried using the latest version of tensor flow while trying to make an inference Frome the .pb Does it gives different results to you? Il mar 18 gen 2022, 05:10 mohantym ***@***.***> ha scritto: >   ! Its 1.x code actually which you might > be trying to run compatibility mode in 2.6 which might be cause of issue. > Can you upgrade your code base to 2.7 version and let us know ? Thanks! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > Triage notifications on the go with GitHub Mobile for iOS >  > or Android > . > > You are receiving this because you were mentioned.Message ID: > ***@***.***> >",Can you please share the colab gist with results? Thank you!,"Yes of course. https://gist.github.com/Tix00/156ab9226f951475d9684d3f4e7708c6 Thank you for your help On Tue, Jan 18, 2022 at 12:29 PM mohantym ***@***.***> wrote: > Can you please share the colab gist with results? Thank you! > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > Triage notifications on the go with GitHub Mobile for iOS >  > or Android > . > > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Did I miss anything? Should I provide something else? Il mar 18 gen 2022, 13:03 Angelo Italiano ***@***.***> ha scritto: > Yes of course. > https://gist.github.com/Tix00/156ab9226f951475d9684d3f4e7708c6 > > Thank you for your help > > On Tue, Jan 18, 2022 at 12:29 PM mohantym ***@***.***> > wrote: > >> Can you please share the colab gist with results? Thank you! >> >> â€” >> Reply to this email directly, view it on GitHub >> , >> or unsubscribe >>  >> . >> Triage notifications on the go with GitHub Mobile for iOS >>  >> or Android >> . >> >> You are receiving this because you were mentioned.Message ID: >> ***@***.***> >> >", ! Could you please look at this issue ? Attaching gist for reference. ," Session is `deprecated`. We see that you're using Session which does not work with  eager execution  (TF V2). To migrate code that uses sessions to TF2, rewrite the code without it. Please see the migration guide_replace_v1sessionrun_calls) to migrate from 1.x to 2.x. tf.compat.v1.GraphDef   is also designed for TensorFlow v1. Please refer this   . TF v1.x is currently  not actively supported so please try to upgrade to TF v2.4 or later versions. For any further queries regarding TF v1.x specific issues please post it in TF forum where there is a larger community to get you the right help. Thanks! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Thanks for your concern. I resolved the issue.  save_inference_graph=True \ Adding this flag in the export model script will do the trick. Have a nice day and thanks for your time,Are you satisfied with the resolution of your issue? Yes No
851,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added missing 'return true')ï¼Œ å†…å®¹æ˜¯ (When evaluating particularly sized uint8 quantized AveragPool2D operations, it's broken due to missing 'return true' statement here: https://github.com/tensorflow/tensorflow/blob/v2.4.4/tensorflow/lite/kernels/internal/optimized/optimized_ops.hL3424 This regression was originated here: https://github.com/tensorflow/tensorflow/commit/a5ceb2445d37d9d89a13de3fd0d0d991f4962522diffd7e3b0af29f6e121b2c30e4fc932dcb6c67ab073d759bd9786b60d4069637e78R3293 Which appears change specific for the r2.4 branch, master branch doesn't have this code any longer: https://github.com/tensorflow/tensorflow/commit/b44dd5e97df2f9aec5ad71e8122be98a59a35057)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,vsilyaev,Added missing 'return true',"When evaluating particularly sized uint8 quantized AveragPool2D operations, it's broken due to missing 'return true' statement here: https://github.com/tensorflow/tensorflow/blob/v2.4.4/tensorflow/lite/kernels/internal/optimized/optimized_ops.hL3424 This regression was originated here: https://github.com/tensorflow/tensorflow/commit/a5ceb2445d37d9d89a13de3fd0d0d991f4962522diffd7e3b0af29f6e121b2c30e4fc932dcb6c67ab073d759bd9786b60d4069637e78R3293 Which appears change specific for the r2.4 branch, master branch doesn't have this code any longer: https://github.com/tensorflow/tensorflow/commit/b44dd5e97df2f9aec5ad71e8122be98a59a35057",2022-01-14T19:00:09Z,comp:lite,closed,0,1,https://github.com/tensorflow/tensorflow/issues/53768,We no longer patch TF 2.4. Please make this against master branch if needed.
1379,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite Converter segfaults when trying to convert per-channel quantized transposed convolutions)ï¼Œ å†…å®¹æ˜¯ (When converting transposed convolutions using perchannel weight quantization the converter segfaults and crashes the Python process. Perchannel quantization is supported by TFLite Transposed convolutions: https://github.com/tensorflow/tensorflow/blob/f87be6c7de847017c48520649e3d771e5d6b81b6/tensorflow/lite/kernels/transpose_conv.ccL371L380 so the converter shouldn't segfault when trying to convert such a model. It looks like this issue has been introduced in TensorFlow 2.6 since the same model code produced a valid TFLite file in TensorFlow 2.5. This issue might also be related to CC(Constant folding fails when converting int8 transposed convolutions), but in any case the converter should never segfault.  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.6, 2.7, 2.8rc0 and 2.9.0dev20220114  2. Code A minimal reproduction of the issue and a workaround is available in this notebook. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,lgeiger,TFLite Converter segfaults when trying to convert per-channel quantized transposed convolutions,"When converting transposed convolutions using perchannel weight quantization the converter segfaults and crashes the Python process. Perchannel quantization is supported by TFLite Transposed convolutions: https://github.com/tensorflow/tensorflow/blob/f87be6c7de847017c48520649e3d771e5d6b81b6/tensorflow/lite/kernels/transpose_conv.ccL371L380 so the converter shouldn't segfault when trying to convert such a model. It looks like this issue has been introduced in TensorFlow 2.6 since the same model code produced a valid TFLite file in TensorFlow 2.5. This issue might also be related to CC(Constant folding fails when converting int8 transposed convolutions), but in any case the converter should never segfault.  1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu  TensorFlow installation (pip package or built from source): pip package  TensorFlow library (version, if pip package or github SHA, if built from source): 2.6, 2.7, 2.8rc0 and 2.9.0dev20220114  2. Code A minimal reproduction of the issue and a workaround is available in this notebook. ",2022-01-14T18:55:01Z,stat:awaiting response type:bug stale TFLiteConverter TF 2.7,closed,1,9,https://github.com/tensorflow/tensorflow/issues/53767,"  Was able to replicate the issue on colab using  TF v2.6.0, 2.7.0 and tfnightly(2.9.0dev20220114),please find the attached gists.Thank you!","> Was able to replicate the issue on colab using TF v2.6.0, 2.7.0 and tfnightly(2.9.0dev20220114),please find the attached gists   Thanks for confirming. Just for reference, your reproduction on TF 2.6.0 actually now fails due to an unrelated Keras version conflict. Changing the dependency from `v2.6.0` to `v2.6.2` will fix this and allow you to correctly reproduce the segfault mentioned in this issue.",This is still an issue in `2.9.0dev20220318`. Are there any updates on this? Being able to trigger a converter segfault seems to be quite problematic., I retested the above example `2.10.0dev20220427` and the converter still segfaults.,"I retested the above example with `2.10.0` and the segfault seems to be fixed now, however conversion still fails with:  See here.","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1870,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TFLite] Accumulator and bias types coherence for int16x8 FC operator)ï¼Œ å†…å®¹æ˜¯ (Hello, We noticed that int32 accumulator and int32 bias support was recently added for the int16x8 FULLY_CONNECTED operator (along with CONV_2D and TRANSPOSE_CONV_2D operators) through the `_experimental_full_integer_quantization_bias_type` option of the TFLiteConverter : * Add option to store bias as 32 bit in 16x8 Quant. * Update TFLite kernel to use Ruy 16x8 Gemm instead of reference kernel.  It seems though that these changes are leading to some incoherences with potential unexpected overflow and also created a bug. Before an int16x8 FC would always use an int64 accumulator to avoid any overflow and an int64 bias. The current status now seems to be: * converter._experimental_full_integer_quantization_bias_type = tf.int64 or None:     * OpResolverType.BUILTIN_REF         * use_bias = True             * int64 accumulator and int64 bias         * use_bias = False             * int64 accumulator     * OpResolverType.BUILTIN         * use_bias = True             * int64 accumulator and int64 bias         * use_bias = False             * int32 accumulator (would have expected an int64 accumulator)  * converter._experimental_full_integer_quantization_bias_type = tf.int32:     * OpResolverType.BUILTIN_REF         * use_bias = True             * int64 accumulator and int64 bias (read an int32 bias tensor as an int64 tensor => bug, need to be an int32 bias and would expect an int32 accumulator)         * use_bias = False             * int64 accumulator (would have expected an int32 accumulator)     * OpResolverType.BUILTIN         * use_bias = True             * int32 accumulator and int32 bia)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tessil,[TFLite] Accumulator and bias types coherence for int16x8 FC operator,"Hello, We noticed that int32 accumulator and int32 bias support was recently added for the int16x8 FULLY_CONNECTED operator (along with CONV_2D and TRANSPOSE_CONV_2D operators) through the `_experimental_full_integer_quantization_bias_type` option of the TFLiteConverter : * Add option to store bias as 32 bit in 16x8 Quant. * Update TFLite kernel to use Ruy 16x8 Gemm instead of reference kernel.  It seems though that these changes are leading to some incoherences with potential unexpected overflow and also created a bug. Before an int16x8 FC would always use an int64 accumulator to avoid any overflow and an int64 bias. The current status now seems to be: * converter._experimental_full_integer_quantization_bias_type = tf.int64 or None:     * OpResolverType.BUILTIN_REF         * use_bias = True             * int64 accumulator and int64 bias         * use_bias = False             * int64 accumulator     * OpResolverType.BUILTIN         * use_bias = True             * int64 accumulator and int64 bias         * use_bias = False             * int32 accumulator (would have expected an int64 accumulator)  * converter._experimental_full_integer_quantization_bias_type = tf.int32:     * OpResolverType.BUILTIN_REF         * use_bias = True             * int64 accumulator and int64 bias (read an int32 bias tensor as an int64 tensor => bug, need to be an int32 bias and would expect an int32 accumulator)         * use_bias = False             * int64 accumulator (would have expected an int32 accumulator)     * OpResolverType.BUILTIN         * use_bias = True             * int32 accumulator and int32 bia",2022-01-14T16:22:04Z,stat:awaiting tensorflower type:bug comp:lite comp:lite-kernels,closed,0,21,https://github.com/tensorflow/tensorflow/issues/53763,Hi ! Thanks for the reporting the issue in 2.9 . Please switch to stable version 2.7 for a while. Thank you!,thanks for reporting! I will work on it ,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No, Would it be possible to reopen the issue? Thanks!,Ok  ! Reopening as it is still replicating in 2.9 . Thank you!," Thank you for solving the bug with commit d088d70cc655a16360bd7926db3fe2842414255b With `use_bias = True` things are coherent now, an int32 accumulator is used with an int32 bias and an int64 accumulator with an int64 bias. But if `use_bias = False` is used, an int32 accumulator will be used in all cases, is it intentional? I would expect that setting `converter._experimental_full_integer_quantization_bias_type` to `tf.int64` or `None` would enable the usage of an int64 accumulator in all cases as even if an int16x8 layer has no bias, there's still a high risk of overflow with an int32 accumulator. It also changes the default numerical behaviour of the reference kernel, before an int16x8 FC layer without a bias would have an int64 accumulator but now an int32 one is used even if `converter._experimental_full_integer_quantization_bias_type` is not set to `tf.int32`.", I wonder whether you're interested in providing a fix for this case.,"I don't have much time to work on it for now but I may be looking to do it in a couple of weeks/months if you don't have to time to pick it up on your side. The main difficulty is that we may need to record the bias type (and thus implicitly the accumulator type) in the layer FlatBuffer options. Currently TFL uses the bias type to decide the accumulator type but it falls apart when the layer has no bias. If a model was created with `converter._experimental_full_integer_quantization_bias_type=tf.int64 or None` then an int64 accumulator should be used, even for layer without any bias, otherwise an int32 accumulator must be used if `tf.int32` was set in the flag. Edit: Though it seems having a bias tensor (even if all zeros) is enforced for the CONV operator (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/conv.ccL385). We could do the same for the other operators eventually.",One fundamental question: do we really want accumulators to be 32bits?  16 bits activation X 8 bits weights FC means that the maximum dimension we can have before integer overflow is 2^8 = 256. ,"> One fundamental question: do we really want accumulators to be 32bits? 16 bits activation X 8 bits weights FC means that the maximum dimension we can have before integer overflow is 2^8 = 256. I think that was considered when Dayeong added this feature at the first place. That is why we set the 64bit as the default to avoid overflowing. Users will need to check the accuray by themselves and set the `_experimental_full_integer_quantization_bias_type` if it works for them. From the description of that change: ""For 16x8 Quant, the default is to store the bias as 64 bit. However, 64 bit is 1.5~2 times slower than 32 bits so there is a feature request to enable 32 bit bias in 16x8 Quant. Users can test their model's accuracy and performance with both 32 bit and 64 bit and finalize which options to go by themselves. The default is {32bit for int8Quant, 64bit for int16Quant}.""","Understood. However, it it possible to have 64 bits accumulator but 32 bits bias? My concern is that integer overflow is really hard to debug. ", Could you please take a look at this? Thanks,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.","The issue is still present, there's a PR to fix it. We should wait for it to be merged before closing the issue.","Hi, , it seems the corresponding PR has merged, can you confirm the issue is fixed?",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.," Thanks, the PR was effectively merge but it seems there's still a bug remaining, see https://github.com/tensorflow/tensorflow/pull/58400issuecomment1723176172 comment.","Hi  , Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here. Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
869,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(print_selective_registration_header: No module named 'tensorflow.python.platform' in Offical Lastest Docker)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04  TensorFlow installation (pip package or built from source): source   TensorFlow library (version, if pip package or github SHA, if built from source): source  2. Code Provide code to help us reproduce your issues using one of the following options: Hi, as this issue CC(print_selective_registration_header: No module named 'tensorflow.python.platform')  mention, i meet this issue again even in lastest offical docker. Thanks! here is the code to reproduce the errror.  here is the log. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,didadida-r,print_selective_registration_header: No module named 'tensorflow.python.platform' in Offical Lastest Docker," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04  TensorFlow installation (pip package or built from source): source   TensorFlow library (version, if pip package or github SHA, if built from source): source  2. Code Provide code to help us reproduce your issues using one of the following options: Hi, as this issue CC(print_selective_registration_header: No module named 'tensorflow.python.platform')  mention, i meet this issue again even in lastest offical docker. Thanks! here is the code to reproduce the errror.  here is the log. ",2022-01-14T08:40:33Z,stat:awaiting response type:bug stale TFLiteConverter TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/53761," , Looks like this is duplicate of issue CC(print_selective_registration_header: No module named 'tensorflow.python.platform') .Can you please close this issue, since it is already being tracked there? Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1851,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(movenet fp16 not executed by TFLITE NNAPI DELEGATE)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): aarch64 Android 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below): 2.8  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** I have issues while working with movenet tflite models on android 11 NNAPI 1.3 the movenet models used are sourced from tfhub: https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/float16/4 https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/int8/4 the above two singlepose movenet lighting tflite models are float16 and INT8 respectively, and I was trying to perform benchmarking of the same using the prebuilt benchmark model for android_aarch64 sourced from tflite website (v 2.8): https://storage.googleapis.com/tensorflownightlypublic/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model When I run litemodel_movenet_singlepose_lightning_tflite_int8_4.tflite model on NNAPI using the following command: > ./benchmark_model graph=li)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,suyash-narain,movenet fp16 not executed by TFLITE NNAPI DELEGATE,"Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): aarch64 Android 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below): 2.8  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** I have issues while working with movenet tflite models on android 11 NNAPI 1.3 the movenet models used are sourced from tfhub: https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/float16/4 https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/int8/4 the above two singlepose movenet lighting tflite models are float16 and INT8 respectively, and I was trying to perform benchmarking of the same using the prebuilt benchmark model for android_aarch64 sourced from tflite website (v 2.8): https://storage.googleapis.com/tensorflownightlypublic/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model When I run litemodel_movenet_singlepose_lightning_tflite_int8_4.tflite model on NNAPI using the following command: > ./benchmark_model graph=li",2022-01-14T02:45:01Z,stat:awaiting tensorflower comp:lite type:performance,closed,0,8,https://github.com/tensorflow/tensorflow/issues/53758,"narain  Can you please confirm the tensorflow version you are using? In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here.Thanks!",i was using the tf version 2.8rc0 and tf nightly benchmark model for date 1/13/2022 since the issue occurred during benchmarking the models using tflite benchmark model the command used for benchmarking was: $chmod +x android_aarch64_benchmark_model $./android_aarch64_benchmark_model graph=litemodel_movenet_singlepose_lightning_tflite_float16_4.tflite use_nnapi=1 nnapi_accelerator_name=nnapireference,"Hi , We faced the same issue when trying to emulate pose estimation on Android Studio for Movenet Thunder/Lightning fp16 on NNAPI using tflite nightly build. To reproduce the error, 1)  Clone https://github.com/SuhridS/examples.git b suhrids/update_pose_estimation 2) Open ""https://github.com/SuhridS/examples/tree/suhrids/update_pose_estimation/lite/examples/pose_estimation/android"" on Android Studio and build. 3) Create an AVD for Pixel 5, API 30 and run pose estimation app on it. Observation: ", could you help take a look? thanks!,"Miao, can you please take a look.","Thanks for reporting the issue. The fix has been submitted and you should be able to build benchmark_model and run the fp16 one. (There are still ops like GatherNd not supported by NNAPI delegate, causing the model being partitioned into several sub models though)","Hi, I built the benchmark model using master branch of tensorflow. Steps followed were: 1. git clone https://github.com/tensorflow/tensorflow.git 2. bazel build c opt \   config=android_arm64 \   tensorflow/lite/tools/benchmark:benchmark_model 3. adb push benchmark_model /data/local/tmp 4. adb push litemodel_movenet_singlepose_lightning_tflite_float16_4.tflite /data/local/tmp 5. adb shell 6. chmod +x benchmark_model 7. ./benchmark_model graph=litemodel_movenet_singlepose_lightning_tflite_float16_4.tflite use_nnapi=1 use_xnnpack=false  output:  I got the same results using the nightly (dated:2/10/2022) But when i use the nightly (date 2/17/2022), the fp16 model seems to partially execute on NNAPI. Have the changes not been pushed to tensorflowmaster? thanks","Hi, same issue as above. probably the changes not been pushed to the master branch. Could you pls look into it!"
1876,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Convert tflite model with customerized QuantizeConfig for different layers.)ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  TensorFlow installation (pip package or built from source):pip package  TensorFlow library (version, if pip package or github SHA, if built from source): tf 2.5  2. Code import tensorflow as tf from tensorflow import keras import tensorflow_model_optimization as tfmot import numpy as np import tempfile from tensorflow.keras import layers  LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer class DefaultConv2DQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):      Configure how to quantize weights.     def get_weights_and_quantizers(self, layer):         return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]      Configure how to quantize activations.     def get_activations_and_quantizers(self, layer):         return [(layer.activation, MovingAverageQuantizer(num_bits=16, symmetric=False, narrow_range=False, per_axis=False))]     def set_quantize_weights(self, layer, quantize_weights):          Add this line for each item returned in `get_weights_and_quantizers`          , in the same order         layer.kernel = quantize_weights[0]     def set_quantize_activations(self, layer, quantize_activations):          Add this line for each item returned in `get_activations_and_quantizers`          , in the same order.         layer.activation = quantize_activations[0]      Configure how to quantize outputs (may be equivalent to activation)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JuanLei2019,Convert tflite model with customerized QuantizeConfig for different layers.," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  TensorFlow installation (pip package or built from source):pip package  TensorFlow library (version, if pip package or github SHA, if built from source): tf 2.5  2. Code import tensorflow as tf from tensorflow import keras import tensorflow_model_optimization as tfmot import numpy as np import tempfile from tensorflow.keras import layers  LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer class DefaultConv2DQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):      Configure how to quantize weights.     def get_weights_and_quantizers(self, layer):         return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]      Configure how to quantize activations.     def get_activations_and_quantizers(self, layer):         return [(layer.activation, MovingAverageQuantizer(num_bits=16, symmetric=False, narrow_range=False, per_axis=False))]     def set_quantize_weights(self, layer, quantize_weights):          Add this line for each item returned in `get_weights_and_quantizers`          , in the same order         layer.kernel = quantize_weights[0]     def set_quantize_activations(self, layer, quantize_activations):          Add this line for each item returned in `get_activations_and_quantizers`          , in the same order.         layer.activation = quantize_activations[0]      Configure how to quantize outputs (may be equivalent to activation",2022-01-13T08:15:55Z,stat:awaiting response type:support stale TFLiteConverter TF 2.11,closed,0,17,https://github.com/tensorflow/tensorflow/issues/53749, Could you please try to use latest TF v2.7.0 and let us know the outcome?Thanks!," Thanks for your response. We have tried the TF v2.7.0 and it outputs the same error.  By the way, we change the  class "" InputLayerQuantize"" of  ""default_8bit_transforms.py""  to make the output of ""InputLayer"" to int16 type. But we think it's not the reason which cause the above runtime error. ",">  Could you please try to use latest TF v2.7.0 and let us know the outcome?Thanks!  Thanks for your response. We have tried the TF v2.7.0 and it outputs the same error. By the way, we change the class "" InputLayerQuantize"" of ""default_8bit_transforms.py"" to make the output of ""InputLayer"" to int16 type. But we think it's not the reason which cause the above runtime error.","  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!",">  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!   Running the below code will output the error. Thanks. "," Was able to replicate the issue on colab using TF v2.7.0 and tfnightly,please find the attached gists for reference.Thanks!",">  Was able to replicate the issue on colab using TF v2.7.0 and tfnightly,please find the attached gists for reference.Thanks! ï¼Œï¼Œ hello, sorry to interruptï¼ŒI do not know how to check the ""attached gists for reference"". I can not find any attached files. Could you paste the reference here? or could you give me some advice how to solve the errors? Thanks.","The gist for v2.7.0 is https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynbscrollTo=utwlQejrw3Qq and for tfnightly is https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749nightly.ipynb, could you please verify the error message reproduced. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"> The gist for v2.7.0 is https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynbscrollTo=utwlQejrw3Qq and for tfnightly is https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749nightly.ipynb, could you please verify the error message reproduced. Thanks! Thanks for the reply. I checked the error message. The output error message is a little different. We change the class "" InputLayerQuantize"" of ""default_8bit_transforms.py"" to make the output of ""InputLayer"" to int16 type. The output error message is  ""interpreter.allocate_tensors()  File ""/usr/local/lib/python3.6/distpackages/tensorflow/lite/python/interpreter.py"", line 259, in allocate_tensors return self._interpreter.AllocateTensors() RuntimeError: tensorflow/lite/kernels/conv.cc:353 bias>type != kTfLiteInt64 (INT32 != INT64)Node number 1 (CONV_2D) failed to prepare."" And the link error message is ""RuntimeError: tensorflow/lite/kernels/conv.cc:357 output>type != input_type (INT16 != INT8)Node number 1 (CONV_2D) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0."".  But they are occured at the same ""conv.cc"" file. ","> The gist for v2.7.0 is https://colab.research.google.com/gist/sushreebarsa/994675facdb1d4d555a645168d112e5c/53749.ipynbscrollTo=utwlQejrw3Qq and for tfnightly is https://colab.research.google.com/gist/sushreebarsa/b21d3cb2b642848974d675632fad238c/53749nightly.ipynb, could you please verify the error message reproduced. Thanks! Hi, sorry to disturb you,  we have checked the error messages. And  could you give us some advice how to solve the errors?", Triaging to MOT team,I was able to reproduce this in TF 2.11. Please find the gist here. Thank you.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1887,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problems with xla and dynamic LinearOperatorBlockDiag. Issues developing custom XLA op)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution:  Linux Ubuntu 18.04  TensorFlow installed from (source or binary): r2.7  Python version: 3.6.10  Bazel version (if compiling from source):  3.7.2  GCC/Compiler version (if compiling from source): 7.3.1 **Describe the current behavior** I have a 2D sparse square matrix, where the elements are organized in blocks around the diagonal, similar to LinearOperatorBlockDiag.  At compilation time I know the total matrix size, but the size of each of its blocks its not known until execution time. I need to multiply that matrix by a 1D vector to generate a 1D vector, and similar to matvec method I would like to only compute the dense part of the matrix, i.e. block_0 x sub_vector_0, block_1 x sub_vector_1 ... block_n x sub_vector_n. And I would like to have it working with XLA. I have tried XLA with LinearOperatorBlockDiag, however it is not XLA compatible I would like to create a custom XLA op, which could deal with the computation and offsets internally:  it can receive as input a (NxN) matrix, a (N) vector and an array of M offsets (the size of each block), and generate the final (N) vector. I cannot express it with the available XLA ops: tried with xla::DynamicSlice + xla::Dot ,  but I do not know the size of the submatrices/subvectors, so I cannot pass the sizes to DynamicSlice. I have created my own Custom Call but I am not sure how to expose it to TensorFlow. The documentation does not include how to compile the Custom Call, integrate, and )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pemoi1982jpm,Problems with xla and dynamic LinearOperatorBlockDiag. Issues developing custom XLA op,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution:  Linux Ubuntu 18.04  TensorFlow installed from (source or binary): r2.7  Python version: 3.6.10  Bazel version (if compiling from source):  3.7.2  GCC/Compiler version (if compiling from source): 7.3.1 **Describe the current behavior** I have a 2D sparse square matrix, where the elements are organized in blocks around the diagonal, similar to LinearOperatorBlockDiag.  At compilation time I know the total matrix size, but the size of each of its blocks its not known until execution time. I need to multiply that matrix by a 1D vector to generate a 1D vector, and similar to matvec method I would like to only compute the dense part of the matrix, i.e. block_0 x sub_vector_0, block_1 x sub_vector_1 ... block_n x sub_vector_n. And I would like to have it working with XLA. I have tried XLA with LinearOperatorBlockDiag, however it is not XLA compatible I would like to create a custom XLA op, which could deal with the computation and offsets internally:  it can receive as input a (NxN) matrix, a (N) vector and an array of M offsets (the size of each block), and generate the final (N) vector. I cannot express it with the available XLA ops: tried with xla::DynamicSlice + xla::Dot ,  but I do not know the size of the submatrices/subvectors, so I cannot pass the sizes to DynamicSlice. I have created my own Custom Call but I am not sure how to expose it to TensorFlow. The documentation does not include how to compile the Custom Call, integrate, and ",2022-01-13T00:31:14Z,stat:awaiting response type:bug stale comp:xla TF 2.7,closed,0,10,https://github.com/tensorflow/tensorflow/issues/53746,"  In order to expedite the troubleshooting process, please provide a code snippet in colab gist ,notebook link to reproduce the issue reported here.Thanks!",Here you can find the gist: https://gist.github.com/pemoi1982jpm/382ef8c23622dcfd4c878cc403fe6c74 Thanks," I tried to replicate the issue on colab using TF v2.7.0 , tfnightly ,and faced a different error .Could you please find the gist here and confirm the same?Could you please refer to the  tutorial and similar issue1, issue2 ,let us know if it helps?Please refer to the tested build configurations to see version compatibility.Thanks!","That error you faced it is the same I got, and the reason why I said:  ""I have tried XLA with LinearOperatorBlockDiag, however it is not XLA compatible"" Those issues you pointed do not seem to apply here. It is not a performance issue, and I am running everything on CPU. The dynamic shapes of the blocks seem to be the issue. I would like to ask here:     Is it possible to implement that kind of op on TFXLA?     If yes, can you please provide some guidance on how?     if it is with a CustomCall, can you please provide help on how to compile, expose and use? Thank you",Any update on this? Thank you,"Please,  Any progress or update?","Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1883,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(hdf5_format.py method load_weights_from_hdf5_group() fails when you exclude layers)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **from https://github.com/ahmedfgad/MaskRCNNTF2** with some other TF2 fixes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows Pro**  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  **n/a**  TensorFlow installed from (source or binary):  **via pip install**  TensorFlow version (use command below):  **v2.7.0rc169gc256c071bb2 2.7.0**  Python version:   **v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32**  Bazel version (if compiling from source):   **n/a**  GCC/Compiler version (if compiling from source):   **n/a**  CUDA/cuDNN version:   **cuda_11.2.r11.2/compiler.29373293_0**  GPU model and memory:   **Quadro 2000 32GB** You can collect some of this information using our environment capture script **No windows version** **Describe the current behavior** When you call `load_weights()` using the `exclude=` parameter you get errors like ""**You are trying to load a weight file containing 233 layers into a model with 229 layers.**"". Several issues have been posted about this type error but none of them addressed the fundamental problem which is in the method `load_weights_from_hdf5_group()`.  **Describe the expected behavior** Depending on which way layers count goes either being greater than or less than we can either autocorrect the layers or report the actual layer that is missing when it checks for matching layer sizes: **Contributing**  Do you want to contribute a PR? (yes/no):)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,quaesitor-scientiam,hdf5_format.py method load_weights_from_hdf5_group() fails when you exclude layers,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **from https://github.com/ahmedfgad/MaskRCNNTF2** with some other TF2 fixes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows Pro**  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  **n/a**  TensorFlow installed from (source or binary):  **via pip install**  TensorFlow version (use command below):  **v2.7.0rc169gc256c071bb2 2.7.0**  Python version:   **v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32**  Bazel version (if compiling from source):   **n/a**  GCC/Compiler version (if compiling from source):   **n/a**  CUDA/cuDNN version:   **cuda_11.2.r11.2/compiler.29373293_0**  GPU model and memory:   **Quadro 2000 32GB** You can collect some of this information using our environment capture script **No windows version** **Describe the current behavior** When you call `load_weights()` using the `exclude=` parameter you get errors like ""**You are trying to load a weight file containing 233 layers into a model with 229 layers.**"". Several issues have been posted about this type error but none of them addressed the fundamental problem which is in the method `load_weights_from_hdf5_group()`.  **Describe the expected behavior** Depending on which way layers count goes either being greater than or less than we can either autocorrect the layers or report the actual layer that is missing when it checks for matching layer sizes: **Contributing**  Do you want to contribute a PR? (yes/no):",2022-01-12T18:21:28Z,stat:awaiting response type:bug stale comp:keras TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/53740,scientiam   Could please share complete code to replicate the issue or a gist with the error reported for us to help you resolve it.,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1132,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(hdf5_format.py bug/enhancement fix)ï¼Œ å†…å®¹æ˜¯ (When you call `load_weights()` using the `exclude=` parameter you get errors like ""**You are trying to load a weight file containing 233 layers into a model with 229 layers.**"". Several issues have been posted about this type error but none of them addressed the fundamental problem which is in the method `load_weights_from_hdf5_group()`. Depending on which way layers count goes either being greater than or less than we can either autocorrect the layers or report the actual layer that is missing when it checks for matching layer sizes:  with:  Now if we call with an empty exclude list we get this message **""You are trying to load a weight file containing 233 layers into a model with 234 layers. Missing layer names are: ['anchors']""** With enhanced message we now know the missing name and it can be added it to the exclude list and the call will work instead of wondering what is wrong. hdf5_format.zip)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,quaesitor-scientiam,hdf5_format.py bug/enhancement fix,"When you call `load_weights()` using the `exclude=` parameter you get errors like ""**You are trying to load a weight file containing 233 layers into a model with 229 layers.**"". Several issues have been posted about this type error but none of them addressed the fundamental problem which is in the method `load_weights_from_hdf5_group()`. Depending on which way layers count goes either being greater than or less than we can either autocorrect the layers or report the actual layer that is missing when it checks for matching layer sizes:  with:  Now if we call with an empty exclude list we get this message **""You are trying to load a weight file containing 233 layers into a model with 234 layers. Missing layer names are: ['anchors']""** With enhanced message we now know the missing name and it can be added it to the exclude list and the call will work instead of wondering what is wrong. hdf5_format.zip",2022-01-12T18:01:03Z,,closed,0,2,https://github.com/tensorflow/tensorflow/issues/53739,Check out this pull request on&nbsp;    See visual diffs & provide feedback on Jupyter Notebooks.    Powered by ReviewNB,Please open against `master` branch and make sure it only includes the relevant commits.
698,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix several grammar mistakes in c_api.h)ï¼Œ å†…å®¹æ˜¯ (I scanned over the entirety of c_api.h and found 4 errors that should be addressed. These are all coalesced into a single pull request, and this is not a â€œonelinerâ€ correcting one error.  When https://github.com/tensorflow/tensorflow/pull/53717 was closed, I was unable to request that it remain open or communicate that I intended to add more commits. If possible, please assign the reviewers from that PR so that it doesnâ€™t seem like Iâ€™m trying to bypass their decision.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,philipturner,Fix several grammar mistakes in c_api.h,"I scanned over the entirety of c_api.h and found 4 errors that should be addressed. These are all coalesced into a single pull request, and this is not a â€œonelinerâ€ correcting one error.  When https://github.com/tensorflow/tensorflow/pull/53717 was closed, I was unable to request that it remain open or communicate that I intended to add more commits. If possible, please assign the reviewers from that PR so that it doesnâ€™t seem like Iâ€™m trying to bypass their decision.",2022-01-12T17:12:22Z,size:XS,closed,0,1,https://github.com/tensorflow/tensorflow/issues/53737,Thanks
1177,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(print_selective_registration_header: No module named 'tensorflow.python.platform')ï¼Œ å†…å®¹æ˜¯ ( 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04  Tensorflow version: r2.3  TensorFlow installation (pip package or built from source): source  TensorFlow library (version, if pip package or github SHA, if built from source): source  2. Code Provide code to help us reproduce your issues using one of the following options: Hi, i follow the tflite doc and want to reduce the library size, but after i build the print_selective_registration_header tool,  i fail to run the bin because some import error is raised, and i try two way to fix it but both fail. Can you give me some ideas, Many thanks! 1. try to run print_selective_registration_header **not in tensorflow source dir**.  i check the pip version has installed and then try to run print_selective_registration_header, but it fails.  2. try to run print_selective_registration_header **in tensorflow source dir** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,didadida-r,print_selective_registration_header: No module named 'tensorflow.python.platform'," 1. System information  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04  Tensorflow version: r2.3  TensorFlow installation (pip package or built from source): source  TensorFlow library (version, if pip package or github SHA, if built from source): source  2. Code Provide code to help us reproduce your issues using one of the following options: Hi, i follow the tflite doc and want to reduce the library size, but after i build the print_selective_registration_header tool,  i fail to run the bin because some import error is raised, and i try two way to fix it but both fail. Can you give me some ideas, Many thanks! 1. try to run print_selective_registration_header **not in tensorflow source dir**.  i check the pip version has installed and then try to run print_selective_registration_header, but it fails.  2. try to run print_selective_registration_header **in tensorflow source dir** ",2022-01-12T09:06:43Z,type:support TFLiteConverter TF 2.3,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53732," We could see you are using TF v2.3.0 which is not actively supported ,please refer  this comment.Could you please try to upgrade TF version to latest `TF v2.7.0` and let us know the outcome? Thanks! ","After upgrade TF version to 2.7.0, it fail with other error.    Build CMD:  Here is the detailed log. ",Are you satisfied with the resolution of your issue? Yes No
840,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TF:TRT] EfficientDet D0 pre-build TRT engine failed. TF 2.7.0, TRT 7.2.3)ï¼Œ å†…å®¹æ˜¯ (Ubuntu 18.04 Tensorflow 2.7.0 Cuda 11.1.1 TenosrRT 7.2.3.4 CuDNN 8.1.1.33 Cuda Compute Capability 7.5 Hardware: ec2 g4dn.8xlarge (Tesla Turing T4 Tensor Core) TF2 Model is from TensorFlow 2 Detection Model Zoo  EfficientDet D0 512x512 I tried to convert saved model to TRT model and then prebuild TRT engine. The conversion worked fine but prebuild TRT engine step failed. Note: Other models such as ""Faster RCNN ResNet50 V1 640x640"" and ""SSD MobileNet v2 320x320"" work fine. So, most probably it is not ""cudnn installation issue"" as indicated in the error message below.  Error: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,apivovarov,"[TF:TRT] EfficientDet D0 pre-build TRT engine failed. TF 2.7.0, TRT 7.2.3","Ubuntu 18.04 Tensorflow 2.7.0 Cuda 11.1.1 TenosrRT 7.2.3.4 CuDNN 8.1.1.33 Cuda Compute Capability 7.5 Hardware: ec2 g4dn.8xlarge (Tesla Turing T4 Tensor Core) TF2 Model is from TensorFlow 2 Detection Model Zoo  EfficientDet D0 512x512 I tried to convert saved model to TRT model and then prebuild TRT engine. The conversion worked fine but prebuild TRT engine step failed. Note: Other models such as ""Faster RCNN ResNet50 V1 640x640"" and ""SSD MobileNet v2 320x320"" work fine. So, most probably it is not ""cudnn installation issue"" as indicated in the error message below.  Error: ",2022-01-12T06:09:32Z,type:bug comp:gpu:tensorrt TF 2.7,closed,0,4,https://github.com/tensorflow/tensorflow/issues/53730,Hi  ! Could you please look at this issue?,"This can happen if you have many processes running and run out of GPU memory, leaving other libraries like CUDNN and CUBLAS no available memory for initialization. Are you running multiple builds at the same time?","Chris, thank you for the info about possible GPU OOM issue. To answer your questions  I run one engine build. I was able to solve the issue with TRT engine build for EfficientDet D0 model by setting memory growth config param to True  I also checked used GPU memory during the engine build with `nvidiasmi pmon`. At one moment it showed `mem 99%` for 1 second.  I also checked peak memory usage after the engine was built. It showed only 4.06 GB peak usage.  I'm not sure how `99% mem` reported by nvidiasmi and only 4.06 GB peak usage reported by TF can be explained. GPU has 16GB of memory. But anyway, the problem with TRT engine build is solved. Saved TRT model (with prebuilt engine) works and it works slightly faster than the original (nonconverted) TF model.",Are you satisfied with the resolution of your issue? Yes No
1897,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The shell environmental variables are disabled in some actions.run as user local GCC is applied.)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6.2  Python version: 3.9  Installed using virtualenv? pip? conda?: under conda env  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version: 11.0  GPU model and memory: V100 32GiB **Describe the problem** I tried to compile TF 2.6.2 from the source with the GCC 7.5.0 compiled by myself under the conda env and failed. I may get messages just like the following one:  The complied `mlirtblgen` cannot get the library directory of my GCC for execution. The application of static build is no help as introducing `env BAZEL_LINKOPTS=staticlibstdc++:staticlibgcc BAZEL_LINKLIBS=l%:libstdc++.a:lm` that is suggested in the issue and the protocol. I am not familiar with Bazel but at last, I found that the default value of `use_default_shell_env` in the `actions.run` is false. Therefore, I added `use_default_shell_env = True,` in two files to solve this problem. File 1: third_party/mlir/tblgen.bzl  File 2: tensorflow/core/kernels/mlir_generated/build_defs.bzl  Although the compilation is successful and all environment variables are revealed after applying the above p)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,tirear,The shell environmental variables are disabled in some actions.run as user local GCC is applied.,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6.2  Python version: 3.9  Installed using virtualenv? pip? conda?: under conda env  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 7.5.0  CUDA/cuDNN version: 11.0  GPU model and memory: V100 32GiB **Describe the problem** I tried to compile TF 2.6.2 from the source with the GCC 7.5.0 compiled by myself under the conda env and failed. I may get messages just like the following one:  The complied `mlirtblgen` cannot get the library directory of my GCC for execution. The application of static build is no help as introducing `env BAZEL_LINKOPTS=staticlibstdc++:staticlibgcc BAZEL_LINKLIBS=l%:libstdc++.a:lm` that is suggested in the issue and the protocol. I am not familiar with Bazel but at last, I found that the default value of `use_default_shell_env` in the `actions.run` is false. Therefore, I added `use_default_shell_env = True,` in two files to solve this problem. File 1: third_party/mlir/tblgen.bzl  File 2: tensorflow/core/kernels/mlir_generated/build_defs.bzl  Although the compilation is successful and all environment variables are revealed after applying the above p",2022-01-12T04:20:46Z,stat:awaiting response type:build/install subtype:centos 2.6.0,closed,0,7,https://github.com/tensorflow/tensorflow/issues/53729, Could you please try with the latest TF version **2.7.0** and let us know the outcome?Thanks!,I just compiled TF 2.7.0 from the source without any patch on the same system. The entire process is very smooth! :smile: , Thank you for the update! Glad it worked for you. Can you please let us know if we can close this issue as it has been resolved? Thanks!,"Okay, if you also think that my solution is okay, please close this issue. Thanks!",I just found that I also have to apply similar patches for compiling TF 2.5.2 on the same system.,Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!,Are you satisfied with the resolution of your issue? Yes No
1844,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inference time jumps for varying batch size)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes [Link below]  OS Platform and Distribution (Ubuntu 18.04):  TensorFlow installed from (source or binary): binary  TensorFlow version: Tested on 2.6, 2.4  Python version: 3.6  CUDA/cuDNN version: Tested on 1) CUDA 10.1+cudnn 7, 2) CUDA 11.1 + cudnn 8  GPU model and memory: Tested on 1) Tesla P100, 16GB , 2) Quadro P4000, 8GB **Current behavior** When running classifier inference on GPU, if an input batch size is seen for the first time, the inference time is more than expected. In subsequent runs for the same input batch size, the inference time reduces. When the inference time jump is observed, the load shifts to CPU (GPU usage drops in nvidiasmi) while on subsequent inferences the load is on GPU. Example 1:  !figure1_tf_expt_1 For a random batch size, the inference time on `run 2` reduces because it is seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces. Example 2:  !figure2_tf_expt_2 For 10 random batch sizes, the inference time in `run 2` reduces because all these batches are seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces. **Why is this relevant?** Suppose we have a video sequence with varying number of objects every few frames (i.e., the batch size=number of objects varies every few frames). Every time there are total number of objects in a frame that have not been seen before, there is a jump in inference time. For example, if there are 10 objects in the fir)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,parneetk,Inference time jumps for varying batch size,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes [Link below]  OS Platform and Distribution (Ubuntu 18.04):  TensorFlow installed from (source or binary): binary  TensorFlow version: Tested on 2.6, 2.4  Python version: 3.6  CUDA/cuDNN version: Tested on 1) CUDA 10.1+cudnn 7, 2) CUDA 11.1 + cudnn 8  GPU model and memory: Tested on 1) Tesla P100, 16GB , 2) Quadro P4000, 8GB **Current behavior** When running classifier inference on GPU, if an input batch size is seen for the first time, the inference time is more than expected. In subsequent runs for the same input batch size, the inference time reduces. When the inference time jump is observed, the load shifts to CPU (GPU usage drops in nvidiasmi) while on subsequent inferences the load is on GPU. Example 1:  !figure1_tf_expt_1 For a random batch size, the inference time on `run 2` reduces because it is seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces. Example 2:  !figure2_tf_expt_2 For 10 random batch sizes, the inference time in `run 2` reduces because all these batches are seen in `run 1`. In `run 3`, all the batch sizes are seen in `run 2` and their inference time reduces. **Why is this relevant?** Suppose we have a video sequence with varying number of objects every few frames (i.e., the batch size=number of objects varies every few frames). Every time there are total number of objects in a frame that have not been seen before, there is a jump in inference time. For example, if there are 10 objects in the fir",2022-01-11T15:36:08Z,stat:awaiting response stale comp:gpu type:performance 2.6.0,closed,0,6,https://github.com/tensorflow/tensorflow/issues/53725," I tried to replicate this issue on colab using TF v2.7.0 , tfnightly(2.9.0dev20220111) and faced different error in   tfnightly, please find the gist here for reference. Please confirm the same.Thanks!", Thank you for looking into this!  I did not install tfnightly and used default TF of colab environment (v2.7.0). Not sure why colab shows an error for you and not me. I verified again and it seems to run without any errors.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.",This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1413,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFLite_Detection_PostProcess produces invalid bounding box coordinates)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code: yes   OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Mac Monterey 12.0.1`  TensorFlow version (use command below):   Python version:  `3.9.9` **Describe the current behavior** Model: I got the tflite model from https://tfhub.dev/tensorflow/litemodel/ssd_mobilenet_v1/1/default/1 Image: https://github.com/heeh/tflitetest/blob/main/buggy_image_2.jpg Bounding box produces negative coordinates as well as coordinates bigger than one.  **Describe the expected behavior** The bounding box should produce the output within `0.0` and `1.0` According to https://www.tensorflow.org/lite/examples/object_detection/overview  !image  Briefly describe your candidate solution(if contributing): The `TFLite_Detection_PostProcess` operator should enforce the output between zero and one. I believe that this has something to do with the following code. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc **Standalone code to reproduce the issue** https://colab.research.google.com/drive/1Ouyz_BUGSvvKG2Ib_fVMye6IjAmWuuRf?usp=sharing **Other info / logs**  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,heeh,TFLite_Detection_PostProcess produces invalid bounding box coordinates,"**System information**  Have I written custom code: yes   OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Mac Monterey 12.0.1`  TensorFlow version (use command below):   Python version:  `3.9.9` **Describe the current behavior** Model: I got the tflite model from https://tfhub.dev/tensorflow/litemodel/ssd_mobilenet_v1/1/default/1 Image: https://github.com/heeh/tflitetest/blob/main/buggy_image_2.jpg Bounding box produces negative coordinates as well as coordinates bigger than one.  **Describe the expected behavior** The bounding box should produce the output within `0.0` and `1.0` According to https://www.tensorflow.org/lite/examples/object_detection/overview  !image  Briefly describe your candidate solution(if contributing): The `TFLite_Detection_PostProcess` operator should enforce the output between zero and one. I believe that this has something to do with the following code. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc **Standalone code to reproduce the issue** https://colab.research.google.com/drive/1Ouyz_BUGSvvKG2Ib_fVMye6IjAmWuuRf?usp=sharing **Other info / logs**  ",2022-01-10T15:22:24Z,stat:awaiting tensorflower type:bug comp:lite comp:apis TF 2.7,closed,0,16,https://github.com/tensorflow/tensorflow/issues/53713,"Hi  ! Could you please look at this issue? Attaching gist in 2.6, 2.7 and nightly for reference.","   I am sure you are very busy, but I haven't heard back from you for a while. Any updates on this issue?",Hello   What is the status of this issue? Let me know if there is anything that I can help you with.  ,"Hi   For TF models, the bbox are clipped to [0,1] in the postprocess operation where a clip_window is calculated for every image based on its true shape, if true shape is not specified the default is [0,0,1,1]. This clip_window is passed into nms op where bbox are clipped.  Comparing the detections: **TF model: detected bbox > [0.  , 0.04112084, 0.44440162, 0.39392054]** !image **Equivalent TFlite model: detected bbox > [0.00404607,  0.04112093,  0.4444015 ,  0.39392054]** !image In the detection_preprocess operation of tflite, I could not find this clipping operation happening. Could this be a possible reason for the negative bbox?  ",   It has been 23 days since I reported this bug. Could you at least share the status?," , apologies for the delayed response, I could not find much insight on this issue,  , could you please look at this issue. Thanks!",    Could you update the status of this issue?,"Hi   It has been 26 days since this issue was assigned to you and I understand that this issue might not be your priority. However, I need to get your response for moving forward.  I don't mind even if this issue is not going to be fixed anytime soon.","Hi , Sorry for the trouble. Unfortunately, we haven't had a chance to look into this. Given this is a minor issue affecting some example use cases, it's unlikely we'll be able to look into it soon."," I was checking if this is still an issue. I have tried it on TF Nightly 2.12.0.dev20230121 and observed the bounding box coordinates between 0 and 1. I have tried using this image, as the image given in the gist was not accessible. The model is taken from TF hub. Please find the gist here and let us know if it helps. Thank you.","Thank you  , I just have tested the new version on your gist and it still contains coordinates outside 0 and 1. !image Here is the image that I tried (You can replace the following with the cell that downloads the image.)  I also made this gist accessible to public. Sorry for the trouble.", Thank you for the information.,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the TFLite space.","Thank you   I tried 2.13.* and still get the same problem. I modified only input from 's colab. https://colab.research.google.com/drive/1WFyW_eEaNTGrGW4mQE5y8S9KhosTkK44 I am not sure if it is good idea to suggest users try a new version without inspecting the cause. Thank you. Best, Hee","Hi  , Thanks for raising this issue. Are you aware of the migration to LiteRT? This transition is aimed at enhancing our project's capabilities and providing improved support and focus for our users. As we believe this issue is still relevant to LiteRT we are moving your issue there. Please follow progress here. Let us know if you have any questions. Thanks.",Are you satisfied with the resolution of your issue? Yes No
631,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(No matching distribution found for tensorflow==2.4.1)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS Big Sur 11.6  TensorFlow installed from (source or binary): via pip   TensorFlow version: trying to install 2.4.1  Python version: 3.9.6  Installed using virtualenv? pip? conda?: pip  **Describe the problem** Via jupyter notebooks on vs code, trying to run `!pip3 install tensorflow==2.4.1` but getting  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,zohimchandani,No matching distribution found for tensorflow==2.4.1,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS Big Sur 11.6  TensorFlow installed from (source or binary): via pip   TensorFlow version: trying to install 2.4.1  Python version: 3.9.6  Installed using virtualenv? pip? conda?: pip  **Describe the problem** Via jupyter notebooks on vs code, trying to run `!pip3 install tensorflow==2.4.1` but getting  ",2022-01-10T07:40:28Z,stat:awaiting response type:build/install stale subtype:macOS TF 2.4,closed,0,14,https://github.com/tensorflow/tensorflow/issues/53709," Could you please try to latest stable `TF v2.7.0` and refer to build from source, tested build configurations.Please have a look at the  similar issue1, issue2and  system requirements .Please let us know if it helps?Thank you!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"This is because you are trying to install tensorflow 2.4 with python 3.9. As you see here, tensorflow 2.4 is only supported with python 3.6 / 3.7 / 3.8 . Creates an environment with one of these python versions and then you could install tensorflow 2.4 .",Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,I am getting the same error could anyone give me detailed information on how to actually do it,I am also getting same error,  Reopening this ticket as the issue still persists.   Thank you!," hi, I am trying to install tensorflow==2.4.0   with python3.8, I got error like this:  Python 3.8.13 (default, Mar 28 2022, 06:13:39)  [Clang 12.0.0 ] :: Anaconda, Inc. on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. exit>>> exit() (test) hqzMacBookAir Documents % pip show tensorflow WARNING: Package(s) not found: tensorflow (test) hqzMacBookAir Documents % pip install tensorflow==2.4.0 ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.0 (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0) ERROR: No matching distribution found for tensorflow==2.4.0 (test) hqzMacBookAir Documents % pip install tensorflow==2.2.0 ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0) ERROR: No matching distribution found for tensorflow==2.2.0","On my machine, with Windows 10, when I try your command `pip install tensorflow==2.2.0` with an environment that has python 3.8, it works fine. I suspect that your environment **test** has not python 3.8 installed but another version. Can you do `python version` within your environment please. In your error, it shows the available tensorflow versions (2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0) which indicates that you probably have python 3.11. ",Please try to check the python version you have been using in your environment as it seems the incompatible python version with the TF version. Could you please try with the compatible one. You can use the following command to use any specific version and check this link to see the compatible configuration.   !mactest Thank you!,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TFlite docker build fail using bazel.)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version: 2.3  Python version: 2.8  Installed using virtualenv? pip? conda?: pip  Bazel version (if compiling from source): 3.1.0  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: no  GPU model and memory: no **Describe the problem** I had build the tflite successully without using docker, but when i try to use docker to build it automatically. It fails. Many Thanks for you help! **Provide the exact sequence of commands / steps that you executed before running into the problem**  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,didadida-r,TFlite docker build fail using bazel.,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): source  TensorFlow version: 2.3  Python version: 2.8  Installed using virtualenv? pip? conda?: pip  Bazel version (if compiling from source): 3.1.0  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version: no  GPU model and memory: no **Describe the problem** I had build the tflite successully without using docker, but when i try to use docker to build it automatically. It fails. Many Thanks for you help! **Provide the exact sequence of commands / steps that you executed before running into the problem**  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. ",2022-01-10T03:57:01Z,type:build/install comp:lite TF 2.3,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53707,! Could you check this thread and try again ?   ! Could you please look at this issue ?,"yeah, it works! Thanks",Are you satisfied with the resolution of your issue? Yes No
506,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault: 11)ï¼Œ å†…å®¹æ˜¯ (pardon me for being new veteran on this. i still cannot figure this error after i try to train the model, it end up with this error. after few search, have a clue that it might be Mac issue. My mac is macOS Catalina and version 10.15.2. Thank you in advance ^^ !Screen Shot 20220109 at 10 24 30 PM)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",few shot,gyurmey,Segmentation fault: 11,"pardon me for being new veteran on this. i still cannot figure this error after i try to train the model, it end up with this error. after few search, have a clue that it might be Mac issue. My mac is macOS Catalina and version 10.15.2. Thank you in advance ^^ !Screen Shot 20220109 at 10 24 30 PM",2022-01-09T22:39:45Z,stat:awaiting response stale type:others TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/53706,"  In order to reproduce the issue reported here, could you please provide the error in text format, complete code , the dataset , tensorflow version you are using?Thanks!", thanks for the reply ^^ error is Segmentation fault: 11  tensorflow version is 2.7.0,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
723,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Conflict between two libraries of source-built and prebuilt (conda) TensorFlow)ï¼Œ å†…å®¹æ˜¯ (Hi Folks, I installed prebuilt TensorFlow (TF) 2.6 using conda and compiled the same version from the source using bazel. I am able to call/import TF in Python but when adding the built shared libraries to the lib env variable (`LD_LIBRARY_PATH`), I got the following error about a conflict between two TF libraries. These codes work well:  But these codes yielded the error in question (suppose that shared libs are stored in `/usr/local/tensorflow/lib/`): )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rangsimanketkaew,Conflict between two libraries of source-built and prebuilt (conda) TensorFlow,"Hi Folks, I installed prebuilt TensorFlow (TF) 2.6 using conda and compiled the same version from the source using bazel. I am able to call/import TF in Python but when adding the built shared libraries to the lib env variable (`LD_LIBRARY_PATH`), I got the following error about a conflict between two TF libraries. These codes work well:  But these codes yielded the error in question (suppose that shared libs are stored in `/usr/local/tensorflow/lib/`): ",2022-01-09T15:46:46Z,stat:awaiting response type:support comp:gpu 2.6.0,closed,0,10,https://github.com/tensorflow/tensorflow/issues/53704,"Hi  ! The error is about configuring GPU support . You need to point the respective Cuda files after downloading them in your local machine. You can refer these thread to configure GPU support on your machine.Link 1 ,2  Thank you!","> Hi  ! The error is about configuring GPU support . You need to point the respective Cuda files after downloading them in your local machine. You can refer these thread to configure GPU support on your machine.Link 1 ,2  Thank you! IIUC, the error was at this line `ImportError: /home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/sitepackages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: descriptor_table_tensorflow_2fcore_2fprotobuf_2fdata_5fservice_2eproto` which I think that the conflict is relevant to the protobuf, not GPU support.","Ok  ! First suggestion on LD_LIBRARY_PATH was upon this thread. For the this error , You can try upgrading the protobuf version . Attaching relevant thread for  Reference . Please don't mix conda installation of Tensorflow with Build from source installation in one environment. Thank you.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,"Hi  Thanks for the suggestions. I've tried upgrading the protobuf to the newer version (4.x) but it didn't work, still get the same error.",Hi  ! Could you please look at this issue?,Could you please try creating a new environment and try only with build from source and let us know if you face any error. Thanks!,"Thanks for the reply. Yes, building from source in a newly created en works for me.","If your issue is resolved, could you please close this issue. Thanks!",Are you satisfied with the resolution of your issue? Yes No
1204,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Native support for StridedSlice in 6D and Transpose in 7D)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installed from (source or binary): source  TensorFlow version (or github SHA if from source): 804ef7223ef08fd14c274b4a4044cc4aeee68863 **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  TFLite Native support for `StridedSlice` for 6D, and `Transpose` for 7D would be very beneficial. It would be nice if we could avoid Flex operations if possible.  **`saved_model`** and converted **`tflite`** files  saved_model_and_float32tflite.zip  Conversion Script  **Any other info / logs**  Model Citation Repository  HITNet: Hierarchical Iterative Tile Refinement Network for Realtime Stereo Matching https://github.com/googleresearch/googleresearch/tree/master/hitnet !1483232087db28584ce78439894fa4ce93c9f8d4d  FlexStridedSlice (6D) !Screenshot 20220108 21:52:11  FlexTranspose (7D) !Screenshot 20220108 21:52:40)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PINTO0309,Native support for StridedSlice in 6D and Transpose in 7D,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04  TensorFlow installed from (source or binary): source  TensorFlow version (or github SHA if from source): 804ef7223ef08fd14c274b4a4044cc4aeee68863 **Provide the text output from tflite_convert**  **Standalone code to reproduce the issue**  TFLite Native support for `StridedSlice` for 6D, and `Transpose` for 7D would be very beneficial. It would be nice if we could avoid Flex operations if possible.  **`saved_model`** and converted **`tflite`** files  saved_model_and_float32tflite.zip  Conversion Script  **Any other info / logs**  Model Citation Repository  HITNet: Hierarchical Iterative Tile Refinement Network for Realtime Stereo Matching https://github.com/googleresearch/googleresearch/tree/master/hitnet !1483232087db28584ce78439894fa4ce93c9f8d4d  FlexStridedSlice (6D) !Screenshot 20220108 21:52:11  FlexTranspose (7D) !Screenshot 20220108 21:52:40",2022-01-08T12:57:44Z,stat:awaiting tensorflower type:feature comp:lite,closed,0,7,https://github.com/tensorflow/tensorflow/issues/53702,zè¿™æ˜¯ä¸ªä»€ä¹ˆç©æ„ï¼Œæˆ‘åªæ˜¯è¯„è®ºä¸€ä¸‹ï¼Œé‡åˆè‚‰ä¸å°±æ˜¯å°±," , Please take a look at this SO link and issue with the similar error.It helps.Thanks!"," Thank you. CUDA warnings have nothing to do with this issue. I have transcribed all the logs without omitting them, but you can ignore the warnings. Also, this is not a bug. The .tflite has been generated successfully. This issue is an OP request for TensorFlow Lite. A **`feature request`** has been made to implement **`FlexStridedSlice`** and **`FlexTranspose`** to replace the standard operators. `StridedSlice`, `Transpose` I used the following issue template to create the issue. https://github.com/tensorflow/tensorflow/issues/new/choose !Screenshot 20220110 19:49:45","is there any  update? well, i got the same issues with tensorflowlite version 2.8.1.  I found an option TFLITE_ENABLE_FLEX in tensorflowlite 2.5.3 but the option was gone from 2.6.1 I took the part of TFLITE_ENABLE_FLEX in tensorflow 2.5.3 and applied them to 2.8.1. It still doesn't work. anyone share your tips if you got progress?",I solved the problem on my own., can you please share how you solved this problem ?,"For exactly one year this issue got no response, so I addressed the `Transpose` only âˆ dimension transposition myself. `StridedSlice` should also succeed with an implementation similar to `Transpose`.  I have implemented two patterns of dimensional compression algorithms. 1. Pattern that actively compresses a dimension of size 1, then transposes it, and finally reexpands the compressed dimension 2. Pattern of `Split` and `Transpose` of a dimension with a small size when there are few dimensions with a size of 1 I can't explain it in words, so please look at the test implementation. https://github.com/PINTO0309/onnx2tf/issues/93"
626,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Why is tensorflow2.6 so much slower than pytorch1.10.0+cu113?)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (Linux Ubuntu 18.04):  TensorFlow installed from binary:  TensorFlow version 2.6:  Python 3.8:  GPU model R3090 and memory 24G: tensorflow 2.6 codeï¼š  output:  pytorch code:  output:  Is this my problem or the tensorflow problem?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nuaasxr,Why is tensorflow2.6 so much slower than pytorch1.10.0+cu113?,**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  OS Platform and Distribution (Linux Ubuntu 18.04):  TensorFlow installed from binary:  TensorFlow version 2.6:  Python 3.8:  GPU model R3090 and memory 24G: tensorflow 2.6 codeï¼š  output:  pytorch code:  output:  Is this my problem or the tensorflow problem?,2022-01-08T06:54:08Z,comp:apis type:performance 2.6.0,closed,2,2,https://github.com/tensorflow/tensorflow/issues/53700,Check CC(Tensorflow 2.3.0 is much slower than PyTorch 1.4.0 in backward propagation. (20 times slower)).,> Check CC(Tensorflow 2.3.0 is much slower than PyTorch 1.4.0 in backward propagation. (20 times slower)). thanks.
1982,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ValueError: Exception encountered when calling layer ""transformer_decoder_3"" (type TransformerDecoder).  Could not find matching concrete function to call loaded from the SavedModel)ï¼Œ å†…å®¹æ˜¯ (I got this error, while trying to load my transformer model.  Model: >Layer (type)          ==         Output Shape    ==     Param    ==  Connected to                      >=============================================================================================>===== > encoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                                 >                                                                                                   >positional_embedding_6 (Positi    == (None, None, 256)==   645120  ==    ['encoder_inputs[0][0]']           >onalEmbedding)                                                                                      >                                                                                                  >decoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                                  >                                                                                                  >transformer_encoder_3 (Transfo    == (None, None, 256) ==  3155456   ==  ['positional_embedding_6[0][0]']   >rmerEncoder)                                                                                       >                                                                                                   >model_7 (Functional)      ==     (None, None, 2500) ==  6547140  ==   ['decoder_inputs[0][0]',            >                                                                                                                 'transformer_encoder_3[0][0]']      >                                           )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MrBuzzy4,"ValueError: Exception encountered when calling layer ""transformer_decoder_3"" (type TransformerDecoder).  Could not find matching concrete function to call loaded from the SavedModel","I got this error, while trying to load my transformer model.  Model: >Layer (type)          ==         Output Shape    ==     Param    ==  Connected to                      >=============================================================================================>===== > encoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                                 >                                                                                                   >positional_embedding_6 (Positi    == (None, None, 256)==   645120  ==    ['encoder_inputs[0][0]']           >onalEmbedding)                                                                                      >                                                                                                  >decoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                                  >                                                                                                  >transformer_encoder_3 (Transfo    == (None, None, 256) ==  3155456   ==  ['positional_embedding_6[0][0]']   >rmerEncoder)                                                                                       >                                                                                                   >model_7 (Functional)      ==     (None, None, 2500) ==  6547140  ==   ['decoder_inputs[0][0]',            >                                                                                                                 'transformer_encoder_3[0][0]']      >                                           ",2022-01-08T05:30:41Z,stat:awaiting response type:support stale comp:keras TF 2.7,closed,1,5,https://github.com/tensorflow/tensorflow/issues/53699," , Please post this issue on kerasteam/keras repo. To know more refer to: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No," I'm facing with the same problem, did you solved it?"
1862,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NNAPI/Hexagon Support Through CMAKE on RB5 Arm Linux Platform)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  Ubuntu 18.04 Qualcomm Linaro Release  Qualcomm RB5  Source  TensorFlow version: v2.6  Python version: 3.6  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source):    GCC/Compiler version (if compiling from source): gcc 7.5  CUDA/cuDNN version: n/a  GPU model and memory: Adreno GPU 650 Hello, I am having trouble understanding the tutorials and getting delegate support for Tensorflow lite c++/c library on the Qualcomm RB5. I have been using  installation natively on board. I have the following flags enabled  And I am able to compile a working(ish) library and do inferencing. However the only delegate that works is the XNN_PACK delegate. The GPU delegate causes the program to fail when specifying it (maybe not linking correct adreno library to the inferencing code)...could use some help there) But mostly I am concerned about building NNAPI and HEXAGON delegate support. The RB5 has hexagon libraries already installed and I will also install the libhexagon_nn_skel.so libraries. There is no documentation using cmake to enable hexagon support in the TensorFlow lite library and I see no flags in the CMakeLists.txt. I have downloaded the Hexagon SDK and will copy the android_ndk _rc19 workspace to somewhere on board and set the path. Will this enable NNAPI correctly? Because now it does not believe it skips that compilation because it)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mrawding,NNAPI/Hexagon Support Through CMAKE on RB5 Arm Linux Platform,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  Ubuntu 18.04 Qualcomm Linaro Release  Qualcomm RB5  Source  TensorFlow version: v2.6  Python version: 3.6  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source):    GCC/Compiler version (if compiling from source): gcc 7.5  CUDA/cuDNN version: n/a  GPU model and memory: Adreno GPU 650 Hello, I am having trouble understanding the tutorials and getting delegate support for Tensorflow lite c++/c library on the Qualcomm RB5. I have been using  installation natively on board. I have the following flags enabled  And I am able to compile a working(ish) library and do inferencing. However the only delegate that works is the XNN_PACK delegate. The GPU delegate causes the program to fail when specifying it (maybe not linking correct adreno library to the inferencing code)...could use some help there) But mostly I am concerned about building NNAPI and HEXAGON delegate support. The RB5 has hexagon libraries already installed and I will also install the libhexagon_nn_skel.so libraries. There is no documentation using cmake to enable hexagon support in the TensorFlow lite library and I see no flags in the CMakeLists.txt. I have downloaded the Hexagon SDK and will copy the android_ndk _rc19 workspace to somewhere on board and set the path. Will this enable NNAPI correctly? Because now it does not believe it skips that compilation because it",2022-01-07T16:16:47Z,stat:awaiting response type:support stale TFLiteGpuDelegate 2.6.0,closed,0,6,https://github.com/tensorflow/tensorflow/issues/53692,Hi  ! Could you please look at this issue?,Can you have a look into this document on Hexagon delegate C API and also check these FAQs on Hexagon delegate. Let us know if there is anything missing which you are looking for. Thanks! ,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No,How can Ä± run hexagon delegate on tensorflow that which version? I am working on Qt Creator c++ aarch64 for object detection model's display. But FPS is not enough for my homework. Also which shared library should I add? Can you please help me?
1263,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Training time is varying between docker and without docker )ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.   Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:11.4/8.2  GPU model and memory:A6000 & 48GB **Describe the current behavior** If I use docker image tensorflow/tensorflow:latestgpu container for training Convolution based neural network it is taking less time compared to Tensorflow installed using pip install tensorflowgpu==2.7.  **Describe the expected behavior** If we use docker image/ installed using pip also the training should not change much I am using tf.distribute.MirroredStrategy() to use all GPU's. How to resolve this issue?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rameshkunasi,Training time is varying between docker and without docker ,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.   Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): 2.7.0  Python version: 3.8  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:11.4/8.2  GPU model and memory:A6000 & 48GB **Describe the current behavior** If I use docker image tensorflow/tensorflow:latestgpu container for training Convolution based neural network it is taking less time compared to Tensorflow installed using pip install tensorflowgpu==2.7.  **Describe the expected behavior** If we use docker image/ installed using pip also the training should not change much I am using tf.distribute.MirroredStrategy() to use all GPU's. How to resolve this issue?",2022-01-07T11:47:08Z,stat:awaiting response stale comp:dist-strat type:performance TF 2.7,closed,0,5,https://github.com/tensorflow/tensorflow/issues/53690,"  In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!","Sorry... We are using in built database, We cannot give that data and code here. If you suggest any experiment I can do and share the results"," If you're unable to provide  a code snippet of the reported issue , then it would be difficult for us to reproduce this one from our end .Could you please post this issue in TF discussion forum where there is a larger community to get you the right help? Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1890,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(LocallyConnected layers default initializer glorot_uniform determines fanning incorrectly)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7 running NGC tensorflow_21.07tf2py3 container  TensorFlow installed from (source or binary): binary, Nvidia container  TensorFlow version (use command below): 2.5.0+nv  Python version: 3.8.10  CUDA/cuDNN version: 11.5  GPU model and memory: Nvidia V100, 32 GB **Describe the current behavior** The Keras LocallyConnectednD layers all use default kernel_initializer glorot_uniform, just like their Conv counterparts. However, with the default implementation (1), the parameter tensor for e.g. LocallyConnected1D ends up being threedimensional inputsize * kernelsize * filters. In short, this means that the initialization values for a layer are vastly different if you just switch from Conv1D to LocallyConnected1D, to the point where gradients can easily vanish. **Describe the expected behavior** While a locally connected layer and a convolutional layer will have different training dynamics, the underlying argument for the Xavier initialization should result in initial tensors of similar magnitude, since the number of terms in the sum for each activation is identical. GlorotUniform is a wrapper for VarianceScaling. VarianceScaling will use _compute_fans internally, which just stacks additional input dimensions on top in a multiplicative manner (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/initializers/initializers_v2.pyL1011). This has been confirmed in the master branch, as indi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cnettel,LocallyConnected layers default initializer glorot_uniform determines fanning incorrectly,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7 running NGC tensorflow_21.07tf2py3 container  TensorFlow installed from (source or binary): binary, Nvidia container  TensorFlow version (use command below): 2.5.0+nv  Python version: 3.8.10  CUDA/cuDNN version: 11.5  GPU model and memory: Nvidia V100, 32 GB **Describe the current behavior** The Keras LocallyConnectednD layers all use default kernel_initializer glorot_uniform, just like their Conv counterparts. However, with the default implementation (1), the parameter tensor for e.g. LocallyConnected1D ends up being threedimensional inputsize * kernelsize * filters. In short, this means that the initialization values for a layer are vastly different if you just switch from Conv1D to LocallyConnected1D, to the point where gradients can easily vanish. **Describe the expected behavior** While a locally connected layer and a convolutional layer will have different training dynamics, the underlying argument for the Xavier initialization should result in initial tensors of similar magnitude, since the number of terms in the sum for each activation is identical. GlorotUniform is a wrapper for VarianceScaling. VarianceScaling will use _compute_fans internally, which just stacks additional input dimensions on top in a multiplicative manner (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/initializers/initializers_v2.pyL1011). This has been confirmed in the master branch, as indi",2022-01-07T10:05:44Z,stat:awaiting response type:bug stale comp:keras TF 2.5,closed,0,4,https://github.com/tensorflow/tensorflow/issues/53688,Hi  !  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 . Thanks!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1342,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(API generation makes inheritance relationship confusing after r2.6)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.6.0rc232g919f693420e 2.6.0  Python version: Python 3.6.8 **Describe the current behavior** I'm trying to select different algorithms by different optimizer, and got following unexpected behavior:  Before r2.6, everything works fine. But after r2.6, it will get false instead. I noticed that there is an APIgen process when exporting the APIs. And I make other tests.  It seems that the APIs have an independent type system after r2.6. It makes code hard to manage when we need to extend some functionalities.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Lifann,API generation makes inheritance relationship confusing after r2.6,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No  TensorFlow installed from (source or binary): binary  TensorFlow version (use command below): v2.6.0rc232g919f693420e 2.6.0  Python version: Python 3.6.8 **Describe the current behavior** I'm trying to select different algorithms by different optimizer, and got following unexpected behavior:  Before r2.6, everything works fine. But after r2.6, it will get false instead. I noticed that there is an APIgen process when exporting the APIs. And I make other tests.  It seems that the APIs have an independent type system after r2.6. It makes code hard to manage when we need to extend some functionalities.",2022-01-07T06:31:22Z,stat:awaiting response type:bug comp:keras 2.6.0,closed,0,2,https://github.com/tensorflow/tensorflow/issues/53686,  Please post this issue on kerasteam/keras repo. To know more see; https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999 Thank you!,Are you satisfied with the resolution of your issue? Yes No
1867,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NNAPI on android 11 fails with movenet fp16 and int8 tflite models)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): aarch64 Android 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below):  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** I have issues while working with movenet tflite models on android 11 NNAPI 1.3 the movenet models used are sourced from tfhub: 1. https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/float16/4 2. https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/int8/4  the above two singlepose movenet lighting tflite models are float16 and INT8 respectively, and I was trying to perform benchmarking of the same using the prebuilt benchmark model for android_aarch64 sourced from tflite website: https://storage.googleapis.com/tensorflownightlypublic/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model I am easily able to benchmark the models on CPU and GPU, but when I try to run it on NNAPI, the benchmarking fails, which is interesting because eve)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,suyash-narain,NNAPI on android 11 fails with movenet fp16 and int8 tflite models,"Please make sure that this is an issue related to performance of TensorFlow. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template **System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): aarch64 Android 11  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  TensorFlow installed from (source or binary):  TensorFlow version (use command below):  Python version:  Bazel version (if compiling from source):  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the current behavior** I have issues while working with movenet tflite models on android 11 NNAPI 1.3 the movenet models used are sourced from tfhub: 1. https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/float16/4 2. https://tfhub.dev/google/litemodel/movenet/singlepose/lightning/tflite/int8/4  the above two singlepose movenet lighting tflite models are float16 and INT8 respectively, and I was trying to perform benchmarking of the same using the prebuilt benchmark model for android_aarch64 sourced from tflite website: https://storage.googleapis.com/tensorflownightlypublic/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model I am easily able to benchmark the models on CPU and GPU, but when I try to run it on NNAPI, the benchmarking fails, which is interesting because eve",2022-01-07T01:36:00Z,comp:lite type:performance TF 2.4,closed,0,11,https://github.com/tensorflow/tensorflow/issues/53682,"narain , Can you please confirm the tensorflow version you are using.Thanks","Hi  , The tensorflow version being used is ""2.1.0""",Hi s  The 2.1.0 version was already more than 2 years old (it was released back in Jan 2020).  Could you try to upgrade to the newest version and see if the problem still exists?,"if the benchmark_model is downloaded recently, it should be Tensorflow 2.8 or later.","narain , As mentioned can you please try to execute the code in latest v2.7 and let us know if you are facing same issue.Thanks!","what is the best method to upgrade tensorflow on android 11? I am using the preinstalled version 2.1 on the same. from what I know, in android 11, the benchmark_model should have same tf runtime as that installed on the android, right? or does the tf runtime of the binary or app doesn't matter?","narain if you download a recent nightly build of benchmark_model, you are using post2.8 tflite. 1. there is not tf runtime in Android. Yes, there is a tflite shared library in Android, but current benchmark_model doesn't use it. 2. currently, tflite benchmark_model use statically linked tflite interpreter/runtime. So, get benchmark_model nightly from https://www.tensorflow.org/lite/performance/measurementnative_benchmark_binary, then you can use newer tflite. ","narain , Please take a look at this link to update to latest stable version .Thanks","I tried the same int8 and float16 models with the latest nightly benchmark_model android_arch64 for android 11, and i used the following commands for nnapicpu, and i get the following results: 1. litemodel_movenet_singlepose_lightning_tflite_int8_4.tflite > $ ./benchmark_model graph=litemodel_movenet_singlepose_lightning_tflite_int8_4.tflite use_nnapi=1 nnapi_accelerator_name=nnapireference STARTING! Log parameter values verbosely: [0] Graph: [litemodel_movenet_singlepose_lightning_tflite_int8_4.tflite] Use NNAPI: [1] NNAPI accelerator name: [nnapireference] NNAPI accelerators available: [nnapireference] Loaded model litemodel_movenet_singlepose_lightning_tflite_int8_4.tflite INFO: Initialized TensorFlow Lite runtime. INFO: Created TensorFlow Lite delegate for NNAPI. NNAPI delegate created. INFO: Replacing 141 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 15 partitions. Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 8 delegate kernels. INFO: Created TensorFlow Lite XNNPACK delegate for CPU. The input model file size (MB): 2.89484 Initialized session in 59.758ms. 2.  litemodel_movenet_singlepose_lightning_tflite_float16_4.tflite > $ ./benchmark_model graph=litemodel_movenet_singlepose_lightning_tflite_float16_4.tflite use_nnapi=1 nnapi_accelerator_name=nnapireference STARTING! Log parameter values verbosely: [0] Graph: [litemodel_movenet_singlepose_lightning_tflite_float16_4.tflite] Use NNAPI: [1] NNAPI accelerator name: [nnapireference] NNAPI accelerators available: [nnapireference] Loaded model litemodel_movenet_singlepose_lightning_tflite_float16_4.tflite INFO: Initialized TensorFlow Lite runtime. INFO: Created TensorFlow Lite delegate for NNAPI. NNAPI delegate created. Though NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate. INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: Replacing 269 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 13 partitions. The input model file size (MB): 4.75851 Initialized session in 109.192ms. for INT8, I see the model is partially executed by NNAPI delegate, whereas the float16 model is not at all executed by the NNAPI delegate even though I am explicitly applying NNAPI CPU. Is there a reason? ","I face the same issue if i run this model https://tfhub.dev/sayakpaul/litemodel/mobilenetv2coco/fp16/1?liteformat=tflite, i see the same message as before for fp16 models: > Graph: [litemodel_mobilenetv2coco_fp16_1.tflite] Use NNAPI: [1] NNAPI accelerator name: [nnapireference] NNAPI accelerators available: [nnapireference] Loaded model litemodel_mobilenetv2coco_fp16_1.tflite INFO: Initialized TensorFlow Lite runtime. INFO: Created TensorFlow Lite delegate for NNAPI. NNAPI delegate created. Though NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate. INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: Replacing 211 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 43 partitions. The input model file size (MB): 4.2551 Initialized session in 133.522ms. is it the same for every fp16 model that it won't be supported on NNAPI? i don't see this issue with an old nightly benchmark model for android, which i downloaded on 1/28/2021, which i think used tf 2.4.1 I even used the disable_nnapi_cpu=0 argument, and yet still got the same result with nightly",i will create a new ticket for fp16 issue https://github.com/tensorflow/tensorflow/issues/53758issue1102815789 and close this one
466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OSError: SavedModel file does not exist at: final_model_weights.hdf5/{saved_model.pbtxt|saved_model.pb})ï¼Œ å†…å®¹æ˜¯ (Hey a have a problem whn i try to load the module : my code : index.py  when i upload the image to my code i get this error :  structure of my code :   any solution for that ??)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,okba-bouziane,OSError: SavedModel file does not exist at: final_model_weights.hdf5/{saved_model.pbtxt|saved_model.pb},Hey a have a problem whn i try to load the module : my code : index.py  when i upload the image to my code i get this error :  structure of my code :   any solution for that ??,2022-01-05T22:03:40Z,stat:awaiting response stale type:others comp:keras TF 2.7,closed,0,9,https://github.com/tensorflow/tensorflow/issues/53661," , Can you please look at this comment from the issue with similar error.It helps.Thanks",">  , Can you please look at this comment from the issue with similar error.It helps.Thanks i looked before to all this issues but i didn't get the solution that's why i made this issues, So you have any solution for that ? ",Any solution?," , In order to expedite the troubleshooting process, could you please provide the complete code and tensorflow version you are using to reproduce the issue reported here.",Removing comp:lite because this seems irrelevant to TFLite. ,sorry for late reply :  tensorflow version 2.7.0 all code :  main.py  index.py  ," , Please post this issue on kerasteam/keras repo. To know more refer to: https://discuss.tensorflow.org/t/kerasprojectmovedtonewrepositoryinhttpsgithubcomkerasteamkeras/1999",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
1841,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unit test builds broken by recent commit)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.8.10  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Commit https://github.com/tensorflow/tensorflow/commit/fea79a29ad16aa3081e2c0c1c6cf8b81771d46b1 introduced an error that prevents the unit test build from completing **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel test test_timeout=300,500,1,1 flaky_test_attempts=3 test_output=all cache_test_results=no remote_http_cache=""""  remote_cache_proxy="""" noremote_accept_cached config=nonccl build_tag_filters=no_oss,oss_serial,gpu,tpu,benchmarktest,v1only,no_aarch64 test_tag_filters=no_oss,oss_serial,gpu,tpu,benchmarktest,v1only,no_aarch64 copt=ffpcontract=off verbose_failures  //tensorflow/python/... //tensorflow/python/tools/... //tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test //tensorflow/python/ops/ragged:ragged_dispatch_test //tensorflow/python:quantized_ops_test **Any other in)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,elfringham,Unit test builds broken by recent commit,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.8.10  Installed using virtualenv? pip? conda?: no  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Commit https://github.com/tensorflow/tensorflow/commit/fea79a29ad16aa3081e2c0c1c6cf8b81771d46b1 introduced an error that prevents the unit test build from completing **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel test test_timeout=300,500,1,1 flaky_test_attempts=3 test_output=all cache_test_results=no remote_http_cache=""""  remote_cache_proxy="""" noremote_accept_cached config=nonccl build_tag_filters=no_oss,oss_serial,gpu,tpu,benchmarktest,v1only,no_aarch64 test_tag_filters=no_oss,oss_serial,gpu,tpu,benchmarktest,v1only,no_aarch64 copt=ffpcontract=off verbose_failures  //tensorflow/python/... //tensorflow/python/tools/... //tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test //tensorflow/python/ops/ragged:ragged_dispatch_test //tensorflow/python:quantized_ops_test **Any other in",2022-01-05T11:22:55Z,type:build/install subtype: ubuntu/linux subtype:bazel,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53648,   ,Fixed by https://github.com/tensorflow/tensorflow/commit/4a3e89423182a3fbf008eaedc86b169bd3461c64,Are you satisfied with the resolution of your issue? Yes No
901,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to check the total no of neurons of model?)ï¼Œ å†…å®¹æ˜¯ (My question is about the Neural Network neuron's numbers. No of neurons in input layer is equal to no of input features..say 32,32,1 if MNIST dataset  and no of neurons in Output layer is equal to no of target variable say,,in MNIST it is 10 but how can we know about the neurons of hidden layers? https://colab.research.google.com/github/AviatorMoser/kerasmnisttutorial/blob/master/MNIST%20in%20Keras.ipynbscrollTo=BL0AhUGJwT6M in this colab notebook, there are 2 network defined. How can we know the total no of neurons of the architecure? Same question is for TFLite model too. Also, How to check the performance in terms of TOPs/Power consumption/GPUCPU usage etc?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,neso613,How to check the total no of neurons of model?,"My question is about the Neural Network neuron's numbers. No of neurons in input layer is equal to no of input features..say 32,32,1 if MNIST dataset  and no of neurons in Output layer is equal to no of target variable say,,in MNIST it is 10 but how can we know about the neurons of hidden layers? https://colab.research.google.com/github/AviatorMoser/kerasmnisttutorial/blob/master/MNIST%20in%20Keras.ipynbscrollTo=BL0AhUGJwT6M in this colab notebook, there are 2 network defined. How can we know the total no of neurons of the architecure? Same question is for TFLite model too. Also, How to check the performance in terms of TOPs/Power consumption/GPUCPU usage etc?",2022-01-05T07:23:59Z,stat:awaiting response type:support stale comp:keras,closed,0,10,https://github.com/tensorflow/tensorflow/issues/53644,"Hi  ! Can you check these  threads 1 ,2  for counting neurons in a keras model ? You can check memory usage of CPU/GPU  using memory_usage api . You can use a USB Power Meter along your microcontrollers to check the power usage during inference . Please post in TF forum/SO for further assistance. Thank you!","I will assume that, you already have some knowledge about Neural Nets. So, when you write this:  This line means your Input neurons is = total number of pixels in MNIST (here) And this is `connected` to a hidden layer of `512` neurons. So the next hidden layer `h1` will have `512` neurons. Now suppose after this you add this line.  So, this means that the next layer `h2` will have `64` number of neurons. And `h1` is connected to `h2`. And this is how it goes on. So, the first hidden layer has `h1` has `512` neurons. Second layer `h2` has `64` neurons. And similarly, you can count number of neurons for other layers.    About total number of neurons: Suppose your NN architecture is defined as:  So, here Input layer `I` has `784` Neurons. Hidden Layer `h1` has `512` neurons. Hidden Layer `h2` has `64` neurons. Hidden Layer `h3` has `32` neurons. Last layer `h4` has `10` neurons. So  total number of hidden neurons in this architectures is :  Now I am excluding the input and the last layer. As these two layer `input` and `last layer` is independent of our choice, as `784` neurons is due to flatten size of image, and there are `10` classes. So hidden neuron number would be here 608. There are some sources, where we see last layer (and sometime input is also considered). Hope this helps. Please feel free to correct me if I missed out at some thing.","> Hi  ! Can you check these threads 1 ,2  for counting neurons in a keras model ? You can check memory usage of CPU/GPU using memory_usage api . You can use a USB Power Meter along your microcontrollers to check the power usage during inference . >  > Please post in TF forum/SO for further assistance. Thank you! Thanks for reply but can USB Power Meter works with rasp?","> I will assume that, you already have some knowledge about Neural Nets. So, when you write this: >  >  >  > This line means your Input neurons is = total number of pixels in MNIST (here) And this is `connected` to a hidden layer of `512` neurons. So the next hidden layer `h1` will have `512` neurons. Now suppose after this you add this line. >  >  >  > So, this means that the next layer `h2` will have `64` number of neurons. And `h1` is connected to `h2`. And this is how it goes on. So, the first hidden layer has `h1` has `512` neurons. Second layer `h2` has `64` neurons. And similarly, you can count number of neurons for other layers. >  > About total number of neurons: >  > Suppose your NN architecture is defined as: >  >  >  > So, here Input layer `I` has `784` Neurons. Hidden Layer `h1` has `512` neurons. Hidden Layer `h2` has `64` neurons. Hidden Layer `h3` has `32` neurons. Last layer `h4` has `10` neurons. So total number of hidden neurons in this architectures is : >  >  >  > Now I am excluding the input and the last layer. As these two layer `input` and `last layer` is independent of our choice, as `784` neurons is due to flatten size of image, and there are `10` classes. So hidden neuron number would be here 608. There are some sources, where we see last layer (and sometime input is also considered). Hope this helps. Please feel free to correct me if I missed out at some thing. So, how model parameters are different from no of neurons?"," ! You can check this thread for Raspberry pi power meter. Thanks for the lucid explanation  . You can check more on model parameters and HyperParameters from below threads. Link 1 ,2 .Thank you!",">  ! You can check this thread for Raspberry pi power meter. Thanks for the lucid explanation  . You can check more on model parameters and HyperParameters from below threads. Link 1 ,2 .Thank you! My Model parameters means, after model.summary() in Tensorflow, it shows model architecture and below to it prints abput parameter..say..trainable parameters, nontrainable parameters. What is these trainable parameters, nontrainable parameters? Is it models neurons information?","Generally, what the `model.summary()` shows the architecture of the model. Like how many dense blocks u have used, how many convolutional blocks u have used etc. At the very end you will see that there is written about this much `trainable param` and this much `nontrainable` parameters. Trainable parameters are those parameters, which will be updated over time, during the time of back propagation. If you know that the hidden neurons, which we set in the dense layers, are nothing but some sort of `weight` matrix. The weights (elements of that matrix) are the trainable parameters. At the same time, suppose nontrainable params are those, which are not updated over time. Some of them are `Dropout layer`, `MaxPool layer` etc. As these parameters do not provide a learnable paradigm in the models. So this combination of trainable and nontrainable parameters, both constitutes as parameters of the model, and that whole information of that model architecture is being provided by the summary.  Also, fun fact, Do not get confused with hyperparameter, and nontrainable parameter. Hyper parameter are some sort of user defined values, which are tweakable and are set to the models. Like the number of hidden neurons in layers, the learning rate, the value of `p` in dropout etc. Non trainable parameters originates from the model, and have some dependency of set up of hyper parameters. Like suppose in case of dropout layer, the value of p. Or the strides taken in maxpool etc.  Go through this link, to have more indepth understanding of trainable and nontrainable parameters.  Also, correct me, if I missed out at some thing. Thank you.",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1798,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed build: Problem getting numpy include path.)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monterey 12.1  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:   TensorFlow installed from (source or binary): source  TensorFlow version: 2.6.2(latest  cloned yesterday)  Python version: 3.7.2  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the problem** I wan to build GPU Metal Delegate and get the error `Problem getting numpy include path.`. I'm pretty sure I have numpy installed for python3 and python2. **Provide the exact sequence of commands / steps that you executed before running into the problem** I've tried to fix the problem with this suggestion, but still get the same output. Running on this command: `bazel build action_env PYTHON_BIN_PATH=/usr/bin/python3 verbose_failures config=ios_arm64 c opt //tensorflow/lite/ios:TensorFlowLiteCMetal_framework` **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. **NumPy logs on python3 and python2**  **Configure**  **Output when run bazel build**  CC:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Miko2x,Failed build: Problem getting numpy include path.,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monterey 12.1  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:   TensorFlow installed from (source or binary): source  TensorFlow version: 2.6.2(latest  cloned yesterday)  Python version: 3.7.2  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source):  CUDA/cuDNN version:  GPU model and memory: **Describe the problem** I wan to build GPU Metal Delegate and get the error `Problem getting numpy include path.`. I'm pretty sure I have numpy installed for python3 and python2. **Provide the exact sequence of commands / steps that you executed before running into the problem** I've tried to fix the problem with this suggestion, but still get the same output. Running on this command: `bazel build action_env PYTHON_BIN_PATH=/usr/bin/python3 verbose_failures config=ios_arm64 c opt //tensorflow/lite/ios:TensorFlowLiteCMetal_framework` **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. **NumPy logs on python3 and python2**  **Configure**  **Output when run bazel build**  CC:  ",2022-01-05T05:39:29Z,stat:awaiting response type:build/install comp:lite subtype:macOS,closed,0,8,https://github.com/tensorflow/tensorflow/issues/53640,"Have you run the `./configure` script from the repository root directory, before executing the `bazel build` command? The way you specify `PYTHON_BIN_PATH` is nonstandard. Usually, you'd specify the Python executable path as well as the Python package path when running the `./configure` script, and those settings will be used for all of your subsequent `bazel build` command. (i.e., you shouldn't be manually specifying `action_env PYTHON_BIN_PATH=...` from the command line)",I've run `./configure` once again:  And when i take a look at the sitepackages it doesn't have numpy:  Is that possible to running the command `/Applications/Xcode.app/Contents/Developer/usr/bin/python3 m pip install numpy` to install numpy inside Xcode Contents? Or it must be reinstalling the Command Line Tools?,And when I use different Python library paths `/usr/local/lib/python3.7/sitepackages` which has numpy in it. It's give me another another error message: ,"In your screenshot, the Python lib path definitely doesn't look right: It's pointing to a python module directory contained within your Xcode installation. It should be something like `/Library/Python/3.7/sitepackages` or something along this line. For the following error:  Try running `bazel build` with the suggested flag: ","> In your screenshot, the Python lib path definitely doesn't look right: It's pointing to a python module directory contained within your Xcode installation. It should be something like `/Library/Python/3.7/sitepackages` or something along this line. >  > For the following error: >  >  >  > Try running `bazel build` with the suggested flag: >  >  Yes it works! Successfully build with no error, the problem it's just wrong Python path library. But I can't use TensorFlowLiteCMetal_framework ğŸ˜… which brings up another problem. There might be a problem with the framework I'm using. Thanks  ",Closing the issue,Are you satisfied with the resolution of your issue? Yes No,This isn't working for me 
1862,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Updating bazel causes python to abort during build on AARCH64)ï¼Œ å†…å®¹æ˜¯ (Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.7  Installed using virtualenv? pip? conda?: n/a  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Build fails with an abort from Python when using Bazel 4.2.2. The same build works with bazel 3.7.2 **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel build config=nonccl //tensorflow/tools/pip_package:build_pip_package verbose_failures copt=ffpcontract=off cxxopt=ffpcontract=off **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. https://ci.linaro.org/job/ldcgpythonmanylinuxtensorflownightly/225/console 18:51:45     Execution platform: //:platform 18:51:45     *** Error in `/tmp/workspace/venvcp37cp37m/bin/python3': free(): invalid pointer: 0x0000fffef8184fa8 *** 18:51:45     ======= Backtrace: ========= 18:51:45     /lib64/libc.so.6(+0x7d1ec)[0xffff9694d1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,elfringham,Updating bazel causes python to abort during build on AARCH64,"Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a  TensorFlow installed from (source or binary): source  TensorFlow version: git HEAD  Python version: 3.7  Installed using virtualenv? pip? conda?: n/a  Bazel version (if compiling from source): 4.2.2  GCC/Compiler version (if compiling from source): 10.3.0  CUDA/cuDNN version: n/a  GPU model and memory: n/a **Describe the problem** Build fails with an abort from Python when using Bazel 4.2.2. The same build works with bazel 3.7.2 **Provide the exact sequence of commands / steps that you executed before running into the problem** bazel build config=nonccl //tensorflow/tools/pip_package:build_pip_package verbose_failures copt=ffpcontract=off cxxopt=ffpcontract=off **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. https://ci.linaro.org/job/ldcgpythonmanylinuxtensorflownightly/225/console 18:51:45     Execution platform: //:platform 18:51:45     *** Error in `/tmp/workspace/venvcp37cp37m/bin/python3': free(): invalid pointer: 0x0000fffef8184fa8 *** 18:51:45     ======= Backtrace: ========= 18:51:45     /lib64/libc.so.6(+0x7d1ec)[0xffff9694d1",2022-01-04T16:39:04Z,stat:awaiting tensorflower type:build/install subtype: ubuntu/linux subtype:bazel,closed,0,15,https://github.com/tensorflow/tensorflow/issues/53632,   ,I have same bug while building in manylinux2014 aarch64 container.,I'm able to build with Bazel 4.2.2 and Python 3.8  is this failure restricted to 3.7?,  Could you please try with python 3.8 and let us know if this is still an issue., it looks like it is only a problem inside the manylinux2014 docker image but it does fail on Python 3.8 in that environment.,"I used 'git bisect' to try and isolate the issue. This was not straight forward due to two things. First there is a range of commits that will not build in the period of interest which may be affecting the result. Secondly the signature of the problem changes at some point from 'free of invalid pointer and abort' to segfault. The result of the bisect was https://github.com/tensorflow/tensorflow/commit/367cf9968c3d14d9d41d1775d8ceeb8e7bf3190f which introduces cpu_feature_guard. However if I remove the call to that .so from the latest git HEAD, the build still fails.",I used git bisect to isolate the point where the signature of the failure changed from segfault to abort. The result was this commit about protobuf https://github.com/tensorflow/tensorflow/commit/7fbc5be3b5a3cc17c05f1ae5b0577fdca8a0e0e2,If I go back to https://github.com/tensorflow/tensorflow/commit/367cf9968c3d14d9d41d1775d8ceeb8e7bf3190f where the segfault first appeared and cherry pick https://github.com/tensorflow/tensorflow/commit/7fbc5be3b5a3cc17c05f1ae5b0577fdca8a0e0e2 then the segfault turns into the abort.,If I go back to https://github.com/tensorflow/tensorflow/commit/098fc5e8c58ed0582cd0121839cddbff0a1c89d5 which is the commit before https://github.com/tensorflow/tensorflow/commit/367cf9968c3d14d9d41d1775d8ceeb8e7bf3190f the build works. But if I then cherry pick https://github.com/tensorflow/tensorflow/commit/7fbc5be3b5a3cc17c05f1ae5b0577fdca8a0e0e2 it results in the abort. So I think this shows that protobuf is definitely implicated in the abort.,"Moving now to git HEAD, if I revert the change from https://github.com/tensorflow/tensorflow/commit/7fbc5be3b5a3cc17c05f1ae5b0577fdca8a0e0e2 the failure changes from abort to segfault. Furthermore if I remove the call to _cpu_feature_guard.so from tensorflow/python/platform/self_check.py which effectively undoes the change from https://github.com/tensorflow/tensorflow/commit/367cf9968c3d14d9d41d1775d8ceeb8e7bf3190f then the build completes successfully. So the segfault and the abort are two separate issues and both need to be addressed.","If I understand correctly, if you revert these two old commits you can build on the latest version of master and not encounter any issues?","Pretty much yes, but not quite so simple as that. There are later commits that build on the use of _cpu_feature_guard.so so simply reverting https://github.com/tensorflow/tensorflow/commit/367cf9968c3d14d9d41d1775d8ceeb8e7bf3190f is not possible which is why I just removed the call from self_check.py. Also https://github.com/tensorflow/tensorflow/commit/7fbc5be3b5a3cc17c05f1ae5b0577fdca8a0e0e2 looks to be a bug fix and probably should not be reverted. Running the failure through pdb in both cases the failure happens on this line of python `from tensorflow.python.tools.api.generator import doc_srcs` in tensorflow/python/__init__.py Also latest version of master is broken in an additional way since this morning. https://github.com/tensorflow/tensorflow/issues/54273",Tagged the author of the XNNPACK dep on that PR. Will try to get some help on this issue too.,This seems to have been caused by the use of BAZEL_LINKLIBS to add 'l%:stdc++' to the build. This was introduced in our initial CI scripts and has since just been carried forward. I am not sure of the initial need for this usage but it is not needed now.,Are you satisfied with the resolution of your issue? Yes No
1823,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Building with Bazel failed during fetching of repository ""go sdk"")ï¼Œ å†…å®¹æ˜¯ (Happy New Year 2022! I've been trying to build TF 2.6 from source using Bazel on my Ubuntu machine. Unfortunately, I got stuck at the very beginning step of installing dependencies for TF. The error says that bazel failed to fetch (and also extract?) GO SDK package (see log below). My system details and building commands are also provided. **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04 // 16 cores AMD Ryzen 7 1700 processors with 16 GB of RAM.  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6  Python version: 3.9.9  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 10.3  CUDA/cuDNN version: no  GPU model and memory: no **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** I had cloned TF from my repo forked from the official TF repo, checkouted `r2.6`, and then configured the build (`./configure`) and left answers empty (answered No to all questions).  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  Any ideas to tackle this issue? Hope that the info above is sufficient, but let me know if you need further info for investigation.  Thank you very much. Rangsiman)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rangsimanketkaew,"Building with Bazel failed during fetching of repository ""go sdk""","Happy New Year 2022! I've been trying to build TF 2.6 from source using Bazel on my Ubuntu machine. Unfortunately, I got stuck at the very beginning step of installing dependencies for TF. The error says that bazel failed to fetch (and also extract?) GO SDK package (see log below). My system details and building commands are also provided. **System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04 // 16 cores AMD Ryzen 7 1700 processors with 16 GB of RAM.  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no  TensorFlow installed from (source or binary): source  TensorFlow version: 2.6  Python version: 3.9.9  Installed using virtualenv? pip? conda?:  Bazel version (if compiling from source): 3.7.2  GCC/Compiler version (if compiling from source): 10.3  CUDA/cuDNN version: no  GPU model and memory: no **Describe the problem** **Provide the exact sequence of commands / steps that you executed before running into the problem** I had cloned TF from my repo forked from the official TF repo, checkouted `r2.6`, and then configured the build (`./configure`) and left answers empty (answered No to all questions).  **Any other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  Any ideas to tackle this issue? Hope that the info above is sufficient, but let me know if you need further info for investigation.  Thank you very much. Rangsiman",2022-01-04T11:02:43Z,stat:awaiting response type:build/install stale subtype: ubuntu/linux 2.6.0,closed,0,6,https://github.com/tensorflow/tensorflow/issues/53629, Could you please let us know if this issue still persists in  latest stable TF v2.7.0 ? Thanks!," >  Could you please let us know if this issue still persists in latest stable TF v2.7.0 ? Thanks! Yes, I also faced the same issue in v2.7.",Hi  ! This issue is not replicating with 2.9 and Bazel 5.00 (Ubuntu 18)  . Attaching gist for reference. Thank you!,This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1540,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorFlow Lite C API TfLiteInterpreter misleading documentation)ï¼Œ å†…å®¹æ˜¯ (This is documentation bug in TFLite C API. I am not sure if I chose a proper issue tag.  Description of issue (what needs changing):  Clear description See the documentation of `TfLiteInterpreterCreate`. According to this documentation, the snippet below is valid:  From now on, according to the documentation of `TfLiteModelCreate`, `model_data`  can be modified. Like the snippet below:  This is just a demo to show that current or another process may modify the memory region. However, this is not valid, and corrupts the interpreter severely. Deleting a `TfLiteModel` object is ok, if one reads it from a file, because the interpreter relies on the file which is _assumed_ to stay unmodified during the lifetime of the interpreter. On the other hand, lifetime of an Interpreter must be bounded by the model which is bounded by the lifetime of the model data to avoid errors. Similar statement can be found in the documentation of the C++ API (see the usage snippet). I think  is the one who can fix the documentation properly, but my suggestion is to replace `... and can destroy it immediately after creating the interpreter; the interpreter will maintain its own reference to the underlying model data` part with ` and the model data must outlive the interpreter` to avoid confusion.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ebraraktas,TensorFlow Lite C API TfLiteInterpreter misleading documentation,"This is documentation bug in TFLite C API. I am not sure if I chose a proper issue tag.  Description of issue (what needs changing):  Clear description See the documentation of `TfLiteInterpreterCreate`. According to this documentation, the snippet below is valid:  From now on, according to the documentation of `TfLiteModelCreate`, `model_data`  can be modified. Like the snippet below:  This is just a demo to show that current or another process may modify the memory region. However, this is not valid, and corrupts the interpreter severely. Deleting a `TfLiteModel` object is ok, if one reads it from a file, because the interpreter relies on the file which is _assumed_ to stay unmodified during the lifetime of the interpreter. On the other hand, lifetime of an Interpreter must be bounded by the model which is bounded by the lifetime of the model data to avoid errors. Similar statement can be found in the documentation of the C++ API (see the usage snippet). I think  is the one who can fix the documentation properly, but my suggestion is to replace `... and can destroy it immediately after creating the interpreter; the interpreter will maintain its own reference to the underlying model data` part with ` and the model data must outlive the interpreter` to avoid confusion.",2022-01-04T10:03:53Z,type:docs-bug stat:awaiting response comp:lite,closed,0,4,https://github.com/tensorflow/tensorflow/issues/53628," , Can you please feel free to submit a PR for the requested change or share the link where requested change is to be made?"," , it seems  has fixed the issue. You can close it if you want."," , Can you please feel free to move this issue to closed status as related PR has been merged.Thanks!","> See the documentation > of TfLiteInterpreterCreate. 1. That link stopped working due to subsequent changes in that file. Here's a corrected link to the original documentation of TfLiteInterpreterCreate. 2. This fix in https://github.com/tensorflow/tensorflow/pull/53711/commits/c109cfb2921b4d8d1340de83987646e4b607cce7 wasn't quite right; it didn't properly distinguish the lifetime of the buffer from the lifetime of the TfLiteModel object. It's fine to destroy the TfLiteModel object before the lifetime of the TfLiteInterpreter has ended (since internally the TfLiteInterpreter has a shared_ptr to the underlying FlatBufferModel object), it's only the byte buffer whose lifetime needs to last beyond the lifetime of the TfLiteInterpreter. We will make a further fix to correct the docs."
862,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CUDA_ERROR_ILLEGAL_ADDRESS when enable unified memory in multi-GPUs training)ï¼Œ å†…å®¹æ˜¯ (I perform data parallel training on nvidia v100 based on horovod+tensorflow. Since a single gpu cannot accommodate the size of my model, I am trying to use tensorflow's unified memory through `per_process_gpu_memory_fractio`. When I use 4 gpus for training and enable unified memory, everything is fine, and the performance loss is acceptable. But when I used 8 gpus for training and turned on unified memory, I encountered the following error:   Tensorflow version: 1.13.1  CUDA 10.0  CentOS 7.2 I donâ€™t know the reason for this error, any troubleshooting suggestions will be very much appreciated.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,firejq,CUDA_ERROR_ILLEGAL_ADDRESS when enable unified memory in multi-GPUs training,"I perform data parallel training on nvidia v100 based on horovod+tensorflow. Since a single gpu cannot accommodate the size of my model, I am trying to use tensorflow's unified memory through `per_process_gpu_memory_fractio`. When I use 4 gpus for training and enable unified memory, everything is fine, and the performance loss is acceptable. But when I used 8 gpus for training and turned on unified memory, I encountered the following error:   Tensorflow version: 1.13.1  CUDA 10.0  CentOS 7.2 I donâ€™t know the reason for this error, any troubleshooting suggestions will be very much appreciated.",2022-01-04T07:50:36Z,stat:awaiting response type:support stale comp:gpu TF 1.13,closed,0,4,https://github.com/tensorflow/tensorflow/issues/53626,"Hi  !  1.x versions of Tensorflow is not supported any more. Could you please try in latest version 2.7 and let us know ?Attaching relevant threads . 1, 2.  Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1862,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SelfAdjointEigV2 GPU operation takes a lot of temporary memory.)ï¼Œ å†…å®¹æ˜¯ (**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.9.0  Python version: 3.8.10  CUDA/cuDNN version: 11.5  GPU model and memory: GTX 1660 Ti **Describe the current behavior** A single call to `tf.linalg.eigh()` takes a linear amount of memory in batch size, despite being processed matrix by matrix. I think the ScratchSpace is only freed in the end of the call after ALL matrices in the batch are processed, instead of on the fly matrix, per matrix. This I conclude from the enormous amount of allocations reported by the allocator during the OOM, and looking at the code. **Contributing**  Do you want to contribute a PR? (yes/no): maybe  Briefly describe your candidate solution(if contributing):    **Best solution:** reuse the ScratchSpace for every matrix.      **Next best solution:** free the ScratchSpace after every matrix in the batch. **Standalone code to reproduce the issue**  **Other info / logs** See below the summary of the allocator:  Especially this line:    So here, I conclude that it crashes when it's trying to process matrix number 11842 out of 16384. Pointers:   ScratchSpace request in the `HeevdImpl` call (at line 635): https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/util/cuda_solvers.ccL620L643   Batch processing matrix by matrix: https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/kernels/linalg/self_adjoint_eig_v2_op_gpu.ccL130L140)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,mcourteaux,SelfAdjointEigV2 GPU operation takes a lot of temporary memory.,"**System information**  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04  TensorFlow installed from (source or binary): source  TensorFlow version (use command below): 2.9.0  Python version: 3.8.10  CUDA/cuDNN version: 11.5  GPU model and memory: GTX 1660 Ti **Describe the current behavior** A single call to `tf.linalg.eigh()` takes a linear amount of memory in batch size, despite being processed matrix by matrix. I think the ScratchSpace is only freed in the end of the call after ALL matrices in the batch are processed, instead of on the fly matrix, per matrix. This I conclude from the enormous amount of allocations reported by the allocator during the OOM, and looking at the code. **Contributing**  Do you want to contribute a PR? (yes/no): maybe  Briefly describe your candidate solution(if contributing):    **Best solution:** reuse the ScratchSpace for every matrix.      **Next best solution:** free the ScratchSpace after every matrix in the batch. **Standalone code to reproduce the issue**  **Other info / logs** See below the summary of the allocator:  Especially this line:    So here, I conclude that it crashes when it's trying to process matrix number 11842 out of 16384. Pointers:   ScratchSpace request in the `HeevdImpl` call (at line 635): https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/util/cuda_solvers.ccL620L643   Batch processing matrix by matrix: https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/kernels/linalg/self_adjoint_eig_v2_op_gpu.ccL130L140",2022-01-03T15:35:17Z,stat:awaiting tensorflower type:bug comp:gpu comp:core TF 2.9,open,0,5,https://github.com/tensorflow/tensorflow/issues/53615," , I was able to execute the code in tf v2.7 and nightly.Please find the gist here.",  The Google Cloud collab thing has more memory than me. Changing it to  demonstrates the issue (note that the total number of floats is even less than before: the batch size is what matters here).,Was able to reproduce the issue with Tensorflow 2.9.2.  ,"Hi, Thank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base. The Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow version with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate. Please follow the release notes to stay up to date with the latest developments which are happening in the Tensorflow space.","No. Surprise: It's still open because nobody fixed it. You can still reproduce this with 4 lines (just reproduced it with TF 2.17 in the Google Collab link above):  Please stop trying to close my issues. This is absolutely counterproductive. I pointed exactly to the code causing the issue. This is a good issue, and should not be recreated. Pointless policies gets contributors upset."
1888,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(INVALID_ARGUMENT: transpose expects a vector of size 0 (when GPU units are more than 1))ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04.2 LTS  TensorFlow installed from (source or binary): Yes  TensorFlow version (use command below): 2.7.0  Python version: 3.8.5  CUDA/cuDNN version &  GPU model and memory: !image I am using a pretrained model to train an image classifier. Below Code is running fine on CPU and single unit GPU (i.e. when GPU=1)  But If I use a system when the number of GPU > 1 then it is throwing the below error. Epoch 1/2 6/Unknown  44s 150ms/step  loss: 19.2255  categorical_accuracy: 0.0625  recall: 0.0000e+00  precision: 0.0000e+00 /bwz_venv/lib/python3.8/sitepackages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument. layer_config = serialize_layer_fn(layer) 288/Unknown  84s 141ms/step  loss: 13.7873  categorical_accuracy: 0.1788  recall: 0.0080  precision: 0.770820211230 15:08:31.404434: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 Traceback (most recent call last): File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main return _run_code(code, main_globals, None, File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code exec(code, run_globals) File ""/ssd/custom_mnet_v2.py"", line 536, in history = model.fit(train_dat)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ravinderkhatri,INVALID_ARGUMENT: transpose expects a vector of size 0 (when GPU units are more than 1),"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04.2 LTS  TensorFlow installed from (source or binary): Yes  TensorFlow version (use command below): 2.7.0  Python version: 3.8.5  CUDA/cuDNN version &  GPU model and memory: !image I am using a pretrained model to train an image classifier. Below Code is running fine on CPU and single unit GPU (i.e. when GPU=1)  But If I use a system when the number of GPU > 1 then it is throwing the below error. Epoch 1/2 6/Unknown  44s 150ms/step  loss: 19.2255  categorical_accuracy: 0.0625  recall: 0.0000e+00  precision: 0.0000e+00 /bwz_venv/lib/python3.8/sitepackages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument. layer_config = serialize_layer_fn(layer) 288/Unknown  84s 141ms/step  loss: 13.7873  categorical_accuracy: 0.1788  recall: 0.0080  precision: 0.770820211230 15:08:31.404434: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 Traceback (most recent call last): File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main return _run_code(code, main_globals, None, File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code exec(code, run_globals) File ""/ssd/custom_mnet_v2.py"", line 536, in history = model.fit(train_dat",2022-01-03T06:45:33Z,stat:awaiting response type:bug stale comp:dist-strat comp:gpu TF 2.7,closed,0,6,https://github.com/tensorflow/tensorflow/issues/53612," , I was able to execute the code without any issues in tf v2.7 and nightly(gpu and cpu).Please find the gist here.1,2,3,4","  Have you checked for more than one GPU? Also, can you please run it for more than one epochs? From my obs, it is breaking after the execution of First Epochs.  Also in the main code, I forgot to add then I am using the below strategery for TPU|GPU. Apology for the miss from my end.  I have added the below line of code in the notebook. Can you please try it with this also? "," The issue here is that lambda layers are not serializable and inorder for you to run on multiple GPUs, your model should be serializable. Please take a look at the detailed explanation here. Thanks!",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.,Are you satisfied with the resolution of your issue? Yes No
1110,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow consuming too much GPU memory?)ï¼Œ å†…å®¹æ˜¯ (**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Windows 10  Tensorflow 2.5.0 (from pip)  Python version: 3.8.9  CUDA/cuDNN version: CUDA 11.2 / CuDNN 8.1  GPU model and memory: NVIDIA RTX 3080 (10GB) **Describe the current behavior** I am running a basic ResNet50 model w/ 23.6M params (~.1GB). This model takes up 8.2GB of GPU memory upon being loaded (after calling `model.compile`). **However, the largest batchsize I can run is 48 before I get OOM errors**. During training w/ batchsize 48, it takes up 9.1/10GB GPU memory. This batchsize seems very low to me for a 10GB RTX 3080 GPU. Is this expected or is this indeed a performance issue?    **Describe the expected behavior** I would imagine I can run the model with a much larger batchsize.  **Standalone code to reproduce the issue**  **Output log** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,rsandler00,Tensorflow consuming too much GPU memory?,"**System information**  Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Windows 10  Tensorflow 2.5.0 (from pip)  Python version: 3.8.9  CUDA/cuDNN version: CUDA 11.2 / CuDNN 8.1  GPU model and memory: NVIDIA RTX 3080 (10GB) **Describe the current behavior** I am running a basic ResNet50 model w/ 23.6M params (~.1GB). This model takes up 8.2GB of GPU memory upon being loaded (after calling `model.compile`). **However, the largest batchsize I can run is 48 before I get OOM errors**. During training w/ batchsize 48, it takes up 9.1/10GB GPU memory. This batchsize seems very low to me for a 10GB RTX 3080 GPU. Is this expected or is this indeed a performance issue?    **Describe the expected behavior** I would imagine I can run the model with a much larger batchsize.  **Standalone code to reproduce the issue**  **Output log** ",2022-01-02T05:17:38Z,stat:awaiting response stale comp:keras type:performance TF 2.5,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53608,"Hi  !  It is not replicating in TF 2.7 Colab environment  though.     I see this message ""This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performancecritical operations:  AVX AVX2 .To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.""  in  error stack trace.  Can you try again after building Tensorflow 2.7 referring these threads and let us know if the issue still persist? Ref 1 , 2 ,3 Thanks! ",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
565,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(What can I use instead of ""tf.contrib.layers.embed_sequence"")ï¼Œ å†…å®¹æ˜¯ (I try to build a chatbot for studying purpose, but it seems like the lesson is quite outdated, they used tensorflow v1 and there are some problems with that. What can I use instead of ""tf.contrib.layers.embed_sequence"" since ""contrib"" is no longer available and I can't find anything in tf_slim/layers and addons, too.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,BuuDien,"What can I use instead of ""tf.contrib.layers.embed_sequence""","I try to build a chatbot for studying purpose, but it seems like the lesson is quite outdated, they used tensorflow v1 and there are some problems with that. What can I use instead of ""tf.contrib.layers.embed_sequence"" since ""contrib"" is no longer available and I can't find anything in tf_slim/layers and addons, too.",2022-01-01T10:07:08Z,stat:awaiting response stale type:others,closed,0,3,https://github.com/tensorflow/tensorflow/issues/53605," , Please take a look at this comment from the issue and the SO link.It helps.Thanks",This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.,Closing as stale. Please reopen if you'd like to work on this further.
553,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„tensorflowä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(raggedtensor: how to convert a list of numpy array with a uniform dim in a certain axis to a ragged tensor)ï¼Œ å†…å®¹æ˜¯ (suppose I have three numpy array with shape (1,3) and Stack them to group with shape (2,3) and (1,3). Then I stack them with tf.ragged.stack to get a ragged tensor  I expect its shape to be (2, None, 3) but (2, None, None). How to implement it?? Using tf 2.5.2)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,zwh14,raggedtensor: how to convert a list of numpy array with a uniform dim in a certain axis to a ragged tensor,"suppose I have three numpy array with shape (1,3) and Stack them to group with shape (2,3) and (1,3). Then I stack them with tf.ragged.stack to get a ragged tensor  I expect its shape to be (2, None, 3) but (2, None, None). How to implement it?? Using tf 2.5.2",2022-01-01T03:11:06Z,type:others,closed,0,0,https://github.com/tensorflow/tensorflow/issues/53600
