https://gitee.com/mindspore/mindformers/issues/IC3H37
这是一个bug报告类型的issue，主要涉及mindformers中mindspore开源软件的第三方copyright notice需要更新的问题。原因可能是当前notice版本过时，导致用户需要更新。

https://gitee.com/mindspore/mindformers/issues/IC2YD7
这是一个用户提出需求的issue，主要对象是deepseekv3模型门禁用例补充。由于功能需求变更或者初期设计不完善，导致用户提出补充门禁用例的请求。

https://gitee.com/mindspore/mindformers/issues/IC2UL5
这是一个bug报告类型的issue，主要涉及到GLM4请求OpenAI格式接口报错问题。由于请求中的messages数组存在两个元素而非一个，导致报错，可能是与OpenAI接口格式兼容性有问题。

https://gitee.com/mindspore/mindformers/issues/IC2S9M
这是一个bug报告，主要涉及模型转换过程中遇到的数据格式错误问题。由于numpy不直接支持bf16的数据格式，导致出现了报错。

https://gitee.com/mindspore/mindformers/issues/IC2QL4
这是一个用户提出需求的issue，主要涉及的对象是MindIE+MindSpore Transformers模型，用户希望增加函数调用支持以提升模型的实用性和交互性。

https://gitee.com/mindspore/mindformers/issues/IC24VI
这是一个bug报告，主要涉及数据集读取引入并行配置读取不合理问题，由于去掉并行配置读取后出现FA算子提示actual_seq_len需要是一个递增序列的错误。

https://gitee.com/mindspore/mindformers/issues/IC20ZM
这是一个bug报告，涉及到mindformers下的首页README中模型列表未刷新的问题。由于模型列表未刷新，可能导致用户无法获取到最新的模型信息。

https://gitee.com/mindspore/mindformers/issues/IC20VI
这个issue是一个需求类型，涉及的主要对象是moev3模块。由于缺乏版本校验，可能导致程序无法正确识别模块版本信息，需要增加版本校验功能来解决这一问题。

https://gitee.com/mindspore/mindformers/issues/IC1XN6
这是一个bug报告，该问题涉及到deepseek网络中smoothquant量化rmsnorm和quant算子未成功融合，导致预期结果中rmsnorm和quant算子未成功融合。

https://gitee.com/mindspore/mindformers/issues/IC1X7N
这是一个用户提出需求的issue，主要对象涉及到seqpipe。原因可能是当前用例优先级不符合预期，用户希望将其调整为level0。

https://gitee.com/mindspore/mindformers/issues/IC1PTK
这是一个用户提出需求的issue，主要对象是deepseekv3文档中的环境搭建篇章。由于篇章冗余，用户担心与其他模型的环境搭建不一致，希望文档能统一安装流程。

https://gitee.com/mindspore/mindformers/issues/IC1N6M
这是一个bug报告，主要涉及"majun"问题未清零。由于未清零导致了bug或者用户提出需要进行清零操作的问题或者寻求相关帮助。

https://gitee.com/mindspore/mindformers/issues/IC1GF4
这是一个bug报告类型的issue，主要涉及GLM4权重下载存在问题，用户询问是否有其他下载方式以及一键启动mindie和普通启动mindie的区别。这个问题可能由于git lfs下载权重时报错导致。

https://gitee.com/mindspore/mindformers/issues/IC187V
这是一个bug报告类型的issue，主要涉及mindformers r1.5.0分支中API中英文不一致的问题。可能是由于未正确同步中英文内容导致的翻译错误。

https://gitee.com/mindspore/mindformers/issues/IC0M7R
这是一个bug报告，涉及接口看护用例中的mindformers_version未更新的问题。原因可能是由于遗漏或更新流程中的失误导致该版本未更新，从而影响了接口看护用例的有效性。

https://gitee.com/mindspore/mindformers/issues/IC0FNY
这是一个bug报告类型的issue，主要涉及到mindformers.models.LlamaConfig和mindformers.TrainingArguments两个对象，由于中英文不一致导致了参数缺失和额外问题。

https://gitee.com/mindspore/mindformers/issues/IC01PC
这个issue属于bug报告类型，主要涉及权重离线切分脚本的问题。由于代码逻辑问题导致多权重合单权重和多进程切分报错，可能是由于多个python进程同时启动以及进程内再启动多进程所致。

https://gitee.com/mindspore/mindformers/issues/IC00HZ
这是一个Bug报告，主要涉及的对象是官网API中的mindformers.run_check功能，由于展示效果错误导致。

https://gitee.com/mindspore/mindformers/issues/IBZWRO
这是一个bug报告，涉及的主要对象是基于MF推理老流程进行权重量化时的输入格式问题，由于aclnn FA算子不支持TH格式，导致流程报错。

https://gitee.com/mindspore/mindformers/issues/IBZVAC
这是一个bug报告，涉及llama模型在全自动并行场景下内存突增的问题，原因是修改时未考虑全自动场景导致。

https://gitee.com/mindspore/mindformers/issues/IBZHSL
这是一个bug报告，主要对象是名为majun的问题未清零。由于某种原因导致了问题未被清零，需要进一步处理。

https://gitee.com/mindspore/mindformers/issues/IBZEZJ
这是一个bug报告，涉及的主要对象是qwen2.5tokenizer，可能由于测试用例报错导致。

https://gitee.com/mindspore/mindformers/issues/IBZ4LX
这是一个bug报告，涉及对象是mindformers.core.TrainingStateMonitor接口英文注释缩进错误，需要调整格式。这个问题出现的原因是注释的缩进错误导致API文档展示效果错误。

https://gitee.com/mindspore/mindformers/issues/IBZ471
这个issue类型为需求提出，主要对象是mindformers仓库中的mindspore门禁版本，由于当前版本0322需要更新到0404版本。

https://gitee.com/mindspore/mindformers/issues/IBYR84
这是一个bug报告，主要涉及mindformers在modelart上多机多卡拉起时报错找不到rank table file，可能由于缺少RANK_TABLE_FILE文件导致。

https://gitee.com/mindspore/mindformers/issues/IBYMFB
这是一个bug报告，涉及mindformers 1.3.2版本2机16卡跑13B模型报错，可能由于pipeline stages配置错误导致loss异常。

https://gitee.com/mindspore/mindformers/issues/IBYK7R
这是一个bug报告类型的issue，主要涉及到使用qwen2.5转换脚本转换lora后推理会影响网络精度的问题。

https://gitee.com/mindspore/mindformers/issues/IBYK6J
这是一个Bug报告，涉及主要对象是deepseekv3模型，由于配置项norm_topk_prob未与开源保持一致，可能导致精度对不齐。

https://gitee.com/mindspore/mindformers/issues/IBYGDC
这是一个bug报告，涉及到Qwen2.5多机推理功能的问题，可能是由于缺少QwenToken导致的报错。

https://gitee.com/mindspore/mindformers/issues/IBYDHJ
这是一个Bug报告，主要涉及PetConfig参数支持范围未明确，可能导致使用时产生不确定行为。

https://gitee.com/mindspore/mindformers/issues/IBY4XV
这是一个Bug报告类型的Issue，主要涉及到packinghandler中未使用变量可能导致数据丢失的问题。

https://gitee.com/mindspore/mindformers/issues/IBY39R
这是一个bug报告，涉及主要对象是dev分支下的mindformers项目。由于tokenizers版本要求与python版本不匹配，导致非python3.9环境下安装依赖失败。

https://gitee.com/mindspore/mindformers/issues/IBY39J
这是一个关于bug报告的issue，主要涉及dev分支编译生成的build目录中文件权限问题，导致第二次编译时无法更新文件而出现error。

https://gitee.com/mindspore/mindformers/issues/IBXH45
这是关于代码缺失的bug报告，涉及到项目中的base_schema.py文件。这个问题由误操作导致了base_schema.py文件为空，需要删除。

https://gitee.com/mindspore/mindformers/issues/IBX1QC
这是一个bug报告，涉及llama模型在线加载获取qkv_concat变量为None和多卡加载生成的json文件报错的问题。由于软件环境中MindSpore版本为2.6.0和MindFormers版本为dev、r1.5.0，导致了这个问题的出现。

https://gitee.com/mindspore/mindformers/issues/IBWV4D
这是一个bug报告，涉及的主要对象是mindformers项目中的开源声明书写位置。原因可能是开源声明中的声明出现在Written Offer之后，导致位置不正确。

https://gitee.com/mindspore/mindformers/issues/IBWV1Q
这是一个bug报告，涉及的主要对象是从obs下载权重功能。由于当前无法下架权重功能，需要回退pr并添加日志，推测可能是版本更新或代码问题导致该bug。

https://gitee.com/mindspore/mindformers/issues/IBWJKE
这个issue是用户提出需求，询问有关内部接口调用方式的问题，主要涉及mindspore库的内部接口调用，用户由于缺乏对应的使用文档或开放接口，想获取相关信息。

https://gitee.com/mindspore/mindformers/issues/IBWIVZ
这是一个bug报告，涉及qwen2.5模型HF权重在线加载时，qkv_concat为true情况下拿不到qkv_config的问题。原因可能是代码实现中的逻辑错误导致无法获取必要的配置信息。

https://gitee.com/mindspore/mindformers/issues/IBWGLJ
这是一个Bug报告，涉及到moe模型在开启共享专家时缺少return_extra_loss入参，导致训练报错。

https://gitee.com/mindspore/mindformers/issues/IBW8OY
这是一个bug报告，主要涉及到glm4在训练前向时调用set_dynamic_inputs报错。导致这个问题可能是由于代码逻辑错误或参数设置不正确。

https://gitee.com/mindspore/mindformers/issues/IBW5DA
这是一个bug报告issue，主要涉及的对象是混合模式CP的名称拼写错误。由于拼写错误导致了不符合预期的命名状况。

https://gitee.com/mindspore/mindformers/issues/IBW3L5
这个issue类型是代码质量问题报告，主要涉及的对象是代码，由于不符合Python编码规范导致了代码不规范的情况。

https://gitee.com/mindspore/mindformers/issues/IBW3IV
这个issue类型是bug报告，主要涉及推理时同时打开量化和SLora特性，影响网络精度。导致症状的原因可能是配置中开启了量化和SLora特性导致网络精度受影响。

https://gitee.com/mindspore/mindformers/issues/IBW328
这是一个用户提出问题的类型的issue，主要涉及的对象是1.3.2版本中的select_recompute和select_comm_recompute功能。这个问题是由于用户想了解在该版本中这两个功能是否已经实现而提出的。

https://gitee.com/mindspore/mindformers/issues/IBW1AS
这是一个关于缺少开源声明的bug报告，主要涉及开源依赖tensorboardX和deprecated。由于缺少声明，可能导致使用这些依赖时出现无法预期的问题。

https://gitee.com/mindspore/mindformers/issues/IBVY2Q
这是一个bug报告，主要涉及EOD压缩训练运行失败的问题，可能是由于actual_seq_qlen和actual_seq_kvlen参数不符合要求导致的。

https://gitee.com/mindspore/mindformers/issues/IBVL16
这是一个代码质量问题报告，主要涉及mindformers仓库中滥用pylint注释屏蔽告警的情况，需要进行整改。

https://gitee.com/mindspore/mindformers/issues/IBVKDR
这个issue属于需求提出类型，主要对象是mindformers的代码。该问题出现的原因是缺少用例看护，导致代码覆盖率较低，需要增加用例看护。

https://gitee.com/mindspore/mindformers/issues/IBV9AS
这是一个bug报告，涉及主要对象为qwen2_5微调，导致报错的原因是pad_token_id应该传入一个列表。

https://gitee.com/mindspore/mindformers/issues/IBV94T
这是一个bug报告，主要对象是模型训练过程中出现的ValueError，由于pad_token_id参数未以列表形式提供，导致报错。

https://gitee.com/mindspore/mindformers/issues/IBV35D
这是一个bug报告，主要涉及到资料中提供的WikiText2数据集的下载链接失效，导致无法成功下载。

https://gitee.com/mindspore/mindformers/issues/IBV2T0
这是一个bug报告，主要涉及MindForger修改导致profile新接口的自动化测试流程受阻。这个问题的原因可能是MindForger侧未适配修改，从而影响到了测试流程。

https://gitee.com/mindspore/mindformers/issues/IBUTY1
这是一个bug报告，涉及的主要对象是动态shape开cp后跑不通的IR图。这个问题可能是由于代码实现错误或者某些特定情况下的错误引起，导致无法正确执行动态shape开cp操作。

https://gitee.com/mindspore/mindformers/issues/IBUNZA
这个issue是关于缺少中文文档的问题，属于文档编写类的问题，主要对象是llama新增的参数。导致这个问题的原因可能是开发人员忘记撰写中文文档，导致用户无法准确理解新增参数的用途。

https://gitee.com/mindspore/mindformers/issues/IBUNZ6
这个issue是一个bug报告，涉及的主要对象是在dev分支下运行ms2.4.10的run_check()方法。由于某些原因导致该方法无法正常运行，用户预期推理精度正常，因此提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/IBUFFQ
这个issue类型是建议完善文档，涉及的主要对象是kv_channels参数的使用。原因是该参数在官方文档中没有解释，导致用户可能会感到困惑和误导。

https://gitee.com/mindspore/mindformers/issues/IBUERN
这是一个缺少测试用例的Bug报告，主要涉及到qwen2_5项目中的qwen2_5lora转换脚本，由于缺乏测试用例看护导致该问题被提出。

https://gitee.com/mindspore/mindformers/issues/IBTTMZ
这个issue类型是缺陷报告，主要对象是有关Mindformer的文档。由于缺乏对only_save_strategy参数开启后的行为的清晰描述，用户不清楚训练会在保存策略文件后退出以及其他输出是否会被保存，导致了这个问题的存在。

https://gitee.com/mindspore/mindformers/issues/IBTMCU
这是一个bug报告，主要涉及HF数据集加载后Alpaca不支持动态shape输出的问题，导致无法正常使用。

https://gitee.com/mindspore/mindformers/issues/IBTGR9
这是一个bug报告，涉及到mindformersv1.5.0beta2中使用wikitext2进行数据预处理预训练时出现ImportError的问题，可能是由于导入模块出错导致的。

https://gitee.com/mindspore/mindformers/issues/IBTG4A
这是一个bug报告，涉及的主要对象是ASCEND_HOME_PATH的硬编码设置，导致了门禁失败的问题。

https://gitee.com/mindspore/mindformers/issues/IBTBVK
这是一个关于配置代理导致Docker拉取镜像失败的bug报告，主要涉及Docker镜像拉取过程中的代理设置与 DNS 解析问题。

https://gitee.com/mindspore/mindformers/issues/IBTBJK
这是一个bug报告，主要涉及tensorboard写入内容有误的问题。原因可能是代码中的错误或者数据格式问题导致了内容有误。

https://gitee.com/mindspore/mindformers/issues/IBT9ZS
这是一个bug报告，涉及的主要对象是CausalLanguageModelDataset，由于在Repeat之后断点续训会导致数据集性能降低，可能是由于RepeatDataset在部分配置场景下会超时的原因。

https://gitee.com/mindspore/mindformers/issues/IBT80K
这是一个 bug 报告，主要涉及的对象是 HF 数据集的 packing 过程。问题可能由于 HF 数据集 packing 过程无法配置 shuffle 而导致。

https://gitee.com/mindspore/mindformers/issues/IBT6K1
这是一个用户提出需求的issue，主要涉及到加载HuggingFace权重时的路径配置问题，导致需要二次修改yaml文件的情况。

https://gitee.com/mindspore/mindformers/issues/IBT6BT
这是一个bug报告，涉及的主要对象是eod压缩场景，由于设置runner_config.batch_size大于1导致报错。

https://gitee.com/mindspore/mindformers/issues/IBT199
这是一个bug报告，主要涉及MindSpore框架中的流水线并行模式下分离保存LoRA权重导致合并权重时的shape不匹配的问题。

https://gitee.com/mindspore/mindformers/issues/IBT0IP
这是一个bug报告，涉及主要对象是处理数据集时的注册路径。由于使用register_path注册处理数据集时出现错误，导致用户无法成功处理数据集，需要修复此问题。

https://gitee.com/mindspore/mindformers/issues/IBT0EM
这是一个用户提出需求的issue，主要对象是指标监控功能的监控频率参数设置，由于监控频率参数需要手动添加在callbacks配置项中，与统一的指标监控配置项分离，用户使用不便。

https://gitee.com/mindspore/mindformers/issues/IBT0AW
这个issue类型是bug报告，涉及的主要对象是配置项monitor_config未添加至template，由于新增的配置项monitor_config未添加到template中进行维护导致的。

https://gitee.com/mindspore/mindformers/issues/IBSUVQ
这是一个关于bug报告的issue，主要涉及internlm2模型存在oom（out of memory）问题。

https://gitee.com/mindspore/mindformers/issues/IBSU2T
这是一个需求更改的类型，该问题单涉及的主要对象是1.5.0版本的模型、代码和文档，由于评审结论的决定，需要废弃这些内容。

https://gitee.com/mindspore/mindformers/issues/IBSTYH
这是一个bug报告，主要涉及MindFormers在安装后缺少experimental/models目录，导致MCore推理流程报错的问题。

https://gitee.com/mindspore/mindformers/issues/IBSR8K
这是一个关于bug报告的issue，涉及到CommonDataLoader中labels未进行偏移导致微调后模型输出乱码的问题。

https://gitee.com/mindspore/mindformers/issues/IBSMIF
这是一个bug报告，涉及的主要对象是SAM（Semantic Adaptation Module），出现报错是因为节点输出索引越界。

https://gitee.com/mindspore/mindformers/issues/IBSLNT
这是一个bug报告，主要涉及Llama网络在单机多卡场景下出现的问题。由于重计算配置为bool类型时，Layersetting类中会走入swap初始化分支，导致出现不符合预期的报错。

https://gitee.com/mindspore/mindformers/issues/IBSLNI
这是一个bug报告，主要涉及deepseek3精度对齐问题。该问题由于配置不当导致精度不一致。

https://gitee.com/mindspore/mindformers/issues/IBSICE
这是一个bug报告，主要涉及DPO训练开PP并行过程中报错的问题。原因可能是并行处理时出现了错误导致报错。

https://gitee.com/mindspore/mindformers/issues/IBSHOW
这是一个bug报告类型的issue，主要涉及多机多卡跑模型时加载权重出现找不到策略文件的问题。这可能是由于文件路径配置错误或者部署环境设置不正确所致。

https://gitee.com/mindspore/mindformers/issues/IBSE8D
这是一个bug报告，主要对象是DeepseekV3模型在开启use_fused_swiglu=True后出现性能劣化的问题。导致这个问题可能是因为开启use_fused_swiglu后导致精度一致，但性能下降。

https://gitee.com/mindspore/mindformers/issues/IBSC40
这是一个bug报告，涉及配置yaml的quant_config中的layer_policy参数不生效的问题。由于选择PYNATIVE模式初始化set_context后构建前段并行llama网络时，mode变为GRAPH模式，导致layer_policy参数不生效。

https://gitee.com/mindspore/mindformers/issues/IBS8EU
这是一个用户提出需求的issue，主要涉及swap策略调整和警告提示功能。原因是当前swap默认策略和优先级设置不够符合需求，同时缺少重复使能的警告提示。

https://gitee.com/mindspore/mindformers/issues/IBS7EO
这是一个用户提出需求的issue，主要对象是CogVLM2Image，由于缺乏训练微调的示例指导，客户需要补充这部分信息。

https://gitee.com/mindspore/mindformers/issues/IBS7BG
这个issue是关于bug报告，涉及DeepSeek3配置中的YARN extend_method，导致scaling_factor中配置original_max_position_embeddings=4096时出现报错，要求original_max_position_embeddings必须小于max_position_embeddings，但实际上应该可以等于max_position_embeddings。

https://gitee.com/mindspore/mindformers/issues/IBS785
这个issue类型是用户提出需求，主要涉及的对象是CogVLM2Video模型。由于缺少多机多卡的微调指导，客户无法评估CogVLM2Video多机多卡微调需要消耗的电力数据，需要更完整的指导。

https://gitee.com/mindspore/mindformers/issues/IBRZOF
这是一个bug报告，涉及mcore接口中参数初始化存在冗余操作的问题，导致初始权重参数被调用两次导致冗余操作。

https://gitee.com/mindspore/mindformers/issues/IBRMMD
这个issue是关于bug报告，涉及到对gpt_dataset的函数_get_ltor_masks_and_position_ids的侵入式修改。原因可能是因为这个修改导致了不正常的行为或功能受损。

https://gitee.com/mindspore/mindformers/issues/IBRLPL
这是一个bug报告，主要涉及Megatron数据集加载功能的编译问题，导致易用性不佳。

https://gitee.com/mindspore/mindformers/issues/IBRKW9
该issue为bug报告类型，主要涉及对齐开源deepseek3和mtp的输出层norm不应共享的问题，导致了实现错误。

https://gitee.com/mindspore/mindformers/issues/IBRFXX
该issue是一个bug报告，主要涉及DeepseekV3 gmm下开启extra loss后报错的问题。造成报错的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/IBRDER
这是一个bug报告，涉及CommonDataLoader中mask压缩参数与megatron数据集不一致的问题。原因可能是参数设置不一致导致推理精度异常。

https://gitee.com/mindspore/mindformers/issues/IBRCS6
这是一个文档反馈类型的issue，主要涉及到MindSpore Transformers中的CheckpointMonitor模块，用户反馈易用性问题，包括错误步骤、缺失功能描述等。

https://gitee.com/mindspore/mindformers/issues/IBRCHL
这是一个针对代码逻辑不统一的bug报告，主要涉及到version_control.py文件中get_lazy_inline函数的disable_lazy_inline取值逻辑不统一的问题。

https://gitee.com/mindspore/mindformers/issues/IBR7YU
这是一个用户提需求的issue，主要对象是有关"cogvlm2_image"模型在910B上是否支持训练的问题，用户想知道后续是否会支持训练，并寻求具体的时间节点。

https://gitee.com/mindspore/mindformers/issues/IBR2BJ
这是一个bug报告，主要涉及到harness工具的易用性问题以及模型初始化速度缓慢的情况。

https://gitee.com/mindspore/mindformers/issues/IBQUGE
这是一个bug报告类型的issue，主要涉及到deepseek3模型的yaml配置文件。由于缺少对gmm算子的支持，导致无法运行，需要删除对应的配置文件。

https://gitee.com/mindspore/mindformers/issues/IBQM85
这是一个bug报告，涉及到qwen2.5系列模型转换脚本缺少lora模型转换脚本的问题，由于缺少对应的转换脚本，导致用户无法将lora模型转换为所需的格式。

https://gitee.com/mindspore/mindformers/issues/IBQL3E
这是一个bug报告类型的issue，主要涉及DeepseekV3中seqpp冗余Mul计算的问题。由于未能正确计算冗余Mul，导致了特定的症状或结果不符合预期。

https://gitee.com/mindspore/mindformers/issues/IBQIQI
这是一个用户需求提出的类型，主要对象是权重保存及加载功能，由于在推理场景中不需要加载优化器权重，用户希望在合并权重时能够选择性过滤部分权重参数。

https://gitee.com/mindspore/mindformers/issues/IBPUX1
这是一个bug报告，主要涉及的对象是软件"deprecated"。由于安装1.2.0以下版本时出现TypeError，可能是由于不支持version参数导致的。

https://gitee.com/mindspore/mindformers/issues/IBPRTF
这是一个bug报告，问题涉及MindFormers多轮推理时在使用do_sample=True参数时输出结果缺乏随机性。

https://gitee.com/mindspore/mindformers/issues/IBPQOV
这个issue是一个bug报告，主要涉及YARN中的scaling_factor配置参数，由于配置整数或者将original_max_position_embedding设置为max_position_embedding时会导致报错。

https://gitee.com/mindspore/mindformers/issues/IBPNGG
这个issue属于bug报告类型，主要涉及mindformers的推理过程中出现的报错问题。可能是由于环境配置或软件适配性问题导致的。

https://gitee.com/mindspore/mindformers/issues/IBPJ74
这是一个bug报告，涉及的主要对象是DeepSeekV3的seqpp编译。由于某种原因导致编译报错。

https://gitee.com/mindspore/mindformers/issues/IBPBWR
这是一个bug报告，主要涉及mindformers下的qwen2_5.md文档中脚本参数错误导致的问题。

https://gitee.com/mindspore/mindformers/issues/IBP3JU
这是一个Bug报告，涉及的主要对象是TopkRouterV2中的balance_via_topk_bias功能。由于未执行group_limited_topk，导致开启balance_via_topk_bias时出现了逻辑问题。

https://gitee.com/mindspore/mindformers/issues/IBOX0Y
该issue属于bug报告类型，涉及的主要对象是mindformers在开启并行训练时自动生成无效的rank文件夹。由于dump_local_norm_path默认值非None，导致这一问题的症状。

https://gitee.com/mindspore/mindformers/issues/IBOPV2
这是一个bug报告，主要涉及的对象是readme文件。由于readme文件无法正常跳转，导致了这个bug报告。

https://gitee.com/mindspore/mindformers/issues/IBOCJD
这是一个bug报告类型的issue，涉及的主要对象是在使用dp=2，batch size=1进行分布式边训练边推理功能时，出现了eval数据集传入模型数据shape为1导致的错误。

https://gitee.com/mindspore/mindformers/issues/IBO7XU
这是一个bug报告类型的issue，主要涉及tensorboard记录信息不恰当的问题，可能是由于配置参数设置不正确导致tensorboard未记录iterationtime和throughput等信息。

https://gitee.com/mindspore/mindformers/issues/IBO364
这是一个bug报告，涉及到deeppseekv3中的check_rule校验失败问题，由于未将mtp_depth纳入num_layers中导致了pp切分校验不通过。

https://gitee.com/mindspore/mindformers/issues/IBNT2Y
这个issue是关于需求的，主要涉及的对象是deepseek2模型，用户提出需要为该模型编写UT用例以保证其可用性。

https://gitee.com/mindspore/mindformers/issues/IBNPS2
这个issue属于bug报告类型，涉及的主要对象是qwen系列文档，由于步骤序号和拼写有误导致症状表现为错误的文档内容。

https://gitee.com/mindspore/mindformers/issues/IBNORE
这是一个bug报告类型的issue，主要涉及MFLossMonitor文档、basetrainer注释、trainer注释和变量命名等的错误。由于注释和文档有误，导致了命名和描述不准确的问题。

https://gitee.com/mindspore/mindformers/issues/IBNOJJ
这是一个bug报告，涉及DeepSeek3权重转换脚本在处理多个Huggingface权重文件时出现的读取转换错误。

https://gitee.com/mindspore/mindformers/issues/IBNOJC
这是一个bug报告，涉及到ProfilerMonitor注释和import出错，可能由于注释错误和import缺失导致了问题的产生。

https://gitee.com/mindspore/mindformers/issues/IBNO08
这是一个功能需求类型的 issue，涉及到软件文档的更新，主要对象是用户。由于缺乏驱动固件版本的提示，导致用户使用功能遇到依赖版本不匹配的问题。

https://gitee.com/mindspore/mindformers/issues/IBNMJS
这是一个关于bug报告的issue，涉及到使用TextStreamer开启do_sample推理时出现报错的问题。由于对numpy.int的支持有问题，导致了程序逻辑判断错误。

https://gitee.com/mindspore/mindformers/issues/IBNGT8
这是一个bug报告，涉及的主要对象是并行解码场景下的FA算子。该问题由于某种原因导致了RuntimeError，输出了Tiling error for FlashAttentionScore的错误信息。

https://gitee.com/mindspore/mindformers/issues/IBNGR5
这是一个bug报告，该问题涉及到MindSpore库中的deepseek3模块，由于优化器并行参数导致的错误，导致yaml dryrun报错。

https://gitee.com/mindspore/mindformers/issues/IBN6CM
这是一个bug报告，涉及的主要对象是在Atlas 800T A2集群上使用mindspore版本为2.4.0和CANN版本进行deepseekv3预训练时出现的Launch kernel failed错误。

https://gitee.com/mindspore/mindformers/issues/IBN6BR
这是一个bug报告类型的issue，主要涉及llavanext文档中的链接失效问题，可能是由于链接错误或失效导致用户无法访问相关内容。

https://gitee.com/mindspore/mindformers/issues/IBMXTJ
这个issue属于bug报告类型，主要涉及到冷热专家回调函数中命名错别字的问题。由于mointor应当为monitor，可能导致程序出现错误或者不符合预期的行为。

https://gitee.com/mindspore/mindformers/issues/IBMXSL
这个issue属于bug报告类型，涉及的主要对象是deepseekv3文档的配置文件修改中autoregister参数，由于readme中的autoregister修改不准确而导致配置文件设置错误。

https://gitee.com/mindspore/mindformers/issues/IBMWFX
这是一个bug报告，涉及日志输出问题，主要对象是在线转换合并权重时的日志记录和共享存储的识别。这个问题源于日志输出不够详细，导致用户无法准确识别共享存储路径的情况。

https://gitee.com/mindspore/mindformers/issues/IBMW4W
这是一个bug报告，主要涉及到mindformers下的deepseekv3预训练，由于helpers.cpp未被正确编译导致ImportError。

https://gitee.com/mindspore/mindformers/issues/IBMVV4
这是一个bug报告，主要涉及deepseek3的文档更新和开源配置修正，问题涉及到auto_register、recompute、topk_bias_update_rate和aux_loss等配置不一致的情况。

https://gitee.com/mindspore/mindformers/issues/IBMSI4
这是一个bug报告，主要涉及deepseek3库的配置问题，用户发现未打开return_extra_loss选项。

https://gitee.com/mindspore/mindformers/issues/IBMSHY
这是一个需求类型的issue，主要涉及到deepseek3的UT用例看护可用性。由于缺乏UT用例看护，可能导致测试覆盖不全面或者无法准确评估功能可用性。

https://gitee.com/mindspore/mindformers/issues/IBMS5S
这是一个bug报告，涉及到deepseek3模型续训过程中出现loss异常增大的问题。据描述，不同设置下进行续训会导致loss无法对齐或随机变化，可能原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/IBMMXR
这是一个Bug报告，涉及CANN没有安装在默认目录时会导致run_check报错，可能会让用户以为环境有问题。

https://gitee.com/mindspore/mindformers/issues/IBMK9H
这是一个用户提出需求的Issue，主要涉及大规模模型读取时缺乏读取进度条显示，导致使用体验不佳。

https://gitee.com/mindspore/mindformers/issues/IBMK9A
这是一个bug报告，该问题涉及MindIE推理服务启动时可能存在的文件路径冲突导致的启动报错。

https://gitee.com/mindspore/mindformers/issues/IBMIWA
这是一个bug报告，涉及Qwen2.57BInstruct单卡推理结果出现乱码的问题，可能是由于字符编码或模型输出处理问题导致的。

https://gitee.com/mindspore/mindformers/issues/IBMH65
这是一个Bug报告，涉及到主要对象deepseekv3预训练，由于推理合入的PR修改训练文件导致训练无法执行。

https://gitee.com/mindspore/mindformers/issues/IBMDKC
这是一个用户提出需求的issue，主要涉及神经网络模型训练中aux loss计算过程中的输入归一化问题。原因可能是深度神经网络在sigmoid激活函数下的收敛困难导致。

https://gitee.com/mindspore/mindformers/issues/IBMBVL
这是一个bug报告，涉及到deepseekv3权重合并过程中出现的数值重塑错误。原因可能是无法将数组大小重新调整为所需形状导致报错。

https://gitee.com/mindspore/mindformers/issues/IBMAIF
这是一个bug报告，涉及的主要对象是权重转换命令中的Qwen2.57BInstruct模型。由于mindspore版本2.3.0中缺少'comm_func'，导致权重转换命令出错报错。

https://gitee.com/mindspore/mindformers/issues/IBM8XW
这个issue类型是功能需求，主要涉及到深度学习模型中权重初始化的标准差配置，用户希望增加初始化权重标准差的配置，并修改默认值为0.006，以优化模型表现。

https://gitee.com/mindspore/mindformers/issues/IBM679
这是一个bug报告，主要涉及deepseekv3训练过程中出现global norm和loss为nan的问题。

https://gitee.com/mindspore/mindformers/issues/IBM63N
这是一个bug报告，涉及ChatGLM3Tokenizer API文档中eos_token和end_token描述雷同的问题，可能由于文档错误或描述不清导致。

https://gitee.com/mindspore/mindformers/issues/IBM605
这是一个性能调优的issue，该问题单主要涉及deepseek模块。原因可能是在使用过程中遇到了性能不佳的问题，希望对其进行优化。

https://gitee.com/mindspore/mindformers/issues/IBM4U4
这是一个Bug报告，涉及的主要对象是deepseek2 128卡，问题可能由于下发性能慢而导致。

https://gitee.com/mindspore/mindformers/issues/IBM4RK
这个issue是关于bug报告，主要涉及yaml地址写错导致的问题。

https://gitee.com/mindspore/mindformers/issues/IBM4EP
这是一个bug报告，涉及主要对象是万卡去冗余权重合并出错，导致 RuntimeError。发现问题的原因是has_redundancy的含义与merge_with_redundancy的含义一致，不需要取反。

https://gitee.com/mindspore/mindformers/issues/IBM41S
这是关于bug报告类型的issue，主要涉及deepseekv2模型在设置full batch=false、top k method为greedy_limited时出现loss为nan的情况。原因可能是设置不当导致。

https://gitee.com/mindspore/mindformers/issues/IBM3FQ
这是一个bug报告，涉及主要对象为deepseekv2推理功能。该问题导致症状为推理时显示shape不一致的报错信息。

https://gitee.com/mindspore/mindformers/issues/IBLY42
这是一个bug报告，主要涉及到deepseekv2模型中的Tensor shape与dp挂钩，导致大dp场景下显存异常的问题。

https://gitee.com/mindspore/mindformers/issues/IBLWSX
这是一个bug报告，问题涉及deepseekV2权重转换脚本参数不匹配，可能由于参数设置错误导致。

https://gitee.com/mindspore/mindformers/issues/IBLSTG
这是一个bug报告，涉及的主要对象是deepseek3的显存优化。由于logits的split功能切分不足，导致最终pp最后一个stage的显存占用过大，建议修改。

https://gitee.com/mindspore/mindformers/issues/IBLOB9
这个issue属于文档修改类型，涉及到项目deepseek3的Readme微调部分，需要更正。

https://gitee.com/mindspore/mindformers/issues/IBLLPU
这是一个bug报告，主要涉及的对象是用于训练cogvlm2模型时开启pp功能时报错的问题。导致这个bug的原因可能是在CogVLM2ForCausalLM模型配置中出现了不支持的操作类型。

https://gitee.com/mindspore/mindformers/issues/IBLLPC
这是一个宣传推广贴，用户提供了关于DeepSeek R1和DeepSeek V3的更新信息，类型为非bug报告。

https://gitee.com/mindspore/mindformers/issues/IBLL9P
这是一个bug报告类型的issue，涉及MindForcers下的run_check功能，在部分运行过程中产生不合适的warning，用户期望日志中不应该出现不匹配或者使用废弃接口的WARNING。

https://gitee.com/mindspore/mindformers/issues/IBLIR8
这是一个需求提交，主要对象是deepseek3权重转换脚本。

https://gitee.com/mindspore/mindformers/issues/IBLBMC
该issue为需求提出类型，主要对象是deepseek微调相关文档上库，由于文档上库存在问题，需要进行相关文档的更新。

https://gitee.com/mindspore/mindformers/issues/IBLBM2
这是一个需求提出类型的issue，主要对象是deepseek3数据集预处理脚本。由于缺少具体的内容描述，无法分析导致的bug或问题原因。

https://gitee.com/mindspore/mindformers/issues/IBL0X5
这是一个用户提出需求的类型，主要对象是基于昇腾AI硬件与昇思MindSpore AI框架的DeepSeekV3。由于希望开发者体验和了解DeepSeekV3的预训练和推理能力，发帖者提供了相应的训练和推理手把手教程，并欢迎大家进行交流和探讨。

https://gitee.com/mindspore/mindformers/issues/IBKR91
该问题类型为用户提出需求，询问Altas 300 Duo I 推理卡是否支持Qwen2.5和DeepSeekR1的时间计划。这是因为用户关注设备支持情况，希望获得相关信息。

https://gitee.com/mindspore/mindformers/issues/IBKHLZ
这是一个bug报告类型的issue，涉及主要对象是生成策略文件的配置操作。由于配置项名称错误导致无法生成对应的策略文件。

https://gitee.com/mindspore/mindformers/issues/IBK7T7
这是一个bug报告，涉及弃用接口无提示日志的问题。原因是调用弃用接口未打印弃用日志。

https://gitee.com/mindspore/mindformers/issues/IBJTJK
这是一个用户提出需求的issue，主要涉及的对象是safetensor分布式加载策略。用户希望在safetensors分布式加载过程中不需要执行unify操作，以提高效率和用户体验。

https://gitee.com/mindspore/mindformers/issues/IBJSSM
这是一个bug报告，涉及MultiSourceDataloader加载多源数据集在full_batch=False和full_batch=True场景下数据采样不一致的问题。原因是full_batch参数设置不同导致数据采样方式出现差异。

https://gitee.com/mindspore/mindformers/issues/IBJOGL
这是一个bug报告，主要涉及到kvcache维度修改的问题。由于kvcache输入目前为4维，但需要修改为3维，否则会造成空间浪费。

https://gitee.com/mindspore/mindformers/issues/IBJL5Y
这是一个bug报告，涉及的主要对象是deepseek2_model.py文件中的Linear参数，可能由于参数被误删除导致了bug。

https://gitee.com/mindspore/mindformers/issues/IBJKUV
这是一个bug报告，涉及门禁用例test_api_compatibility.py的base_schema.json校验错误导致门禁报错，导致了门禁报错的症状。

https://gitee.com/mindspore/mindformers/issues/IBJKTT
这个issue是用户提出的需求，主要涉及增加统计性能时延的工具，其中记录了各个`batch_size`, `in_seq_length`, `out_seq_length`的prepare/predict/post阶段的时延。

https://gitee.com/mindspore/mindformers/issues/IBJIUZ
这是一个bug报告，主要涉及run_check和快速启动的日志优化建议。产生这个问题的原因可能是日志中存在不合适的WARNING级别信息，并缺乏启动成功的文字提示。

https://gitee.com/mindspore/mindformers/issues/IBJIIP
这是一个关于文档错误的bug报告，报告者发现预训练命令无法正常运行，原因在于run_llama3.py不在根目录下。

https://gitee.com/mindspore/mindformers/issues/IBJH0U
这是一个bug报告，涉及到文档描述中关于fastchat工具的内容不统一，导致使用时产生混淆。

https://gitee.com/mindspore/mindformers/issues/IBJFCX
该issue类型为用户提出需求，主要涉及对象是Qwen2 VL模型，用户想了解是否可以使用Qwenvl的训练脚本来训练Qwen2 VL模型。

https://gitee.com/mindspore/mindformers/issues/IBJCMH
这是一个bug报告，用户反馈在MindFormers项目中TensorBoard无法记录iterationtime和throughput信息。

https://gitee.com/mindspore/mindformers/issues/IBJBO1
这是一个用户询问类型的issue，主要涉及Llama 2模型只支持使用based model进行评测任务而不支持微调后的模型，用户想了解是否只能使用初始权重进行评测的问题。

https://gitee.com/mindspore/mindformers/issues/IBJ2MF
这是一个用户提出的问题报告，涉及MindForger中推理文档存在的一些问题，导致用户使用时遇到困难。

https://gitee.com/mindspore/mindformers/issues/IBIZHX
这是一个关于bug报告的issue，涉及对象是流式推理用例。由于流式推理卡住问题导致的，用户提出了删除该用例的问题。

https://gitee.com/mindspore/mindformers/issues/IBIY6O
这是一个bug报告，主要涉及确定性计算环境变量在8卡下设置不生效的问题。导致这个问题的原因可能是环境变量设置方式不正确或者硬件限制。

https://gitee.com/mindspore/mindformers/issues/IBIV1K
这是一个bug报告类型的issue，主要涉及运行mindformer预训练时出现的数值不匹配错误。

https://gitee.com/mindspore/mindformers/issues/IBINED
这是一则bug报告，主要涉及到mindformers书籍中glm2/3的权重链接的删除。原因可能是链接错误或相关内容需要更新。

https://gitee.com/mindspore/mindformers/issues/IBIJX8
这是一个bug报告，该问题涉及的主要对象是mindformers项目中的llama_layer.py文件中的mul算子初始化错误导致type错误的问题。

https://gitee.com/mindspore/mindformers/issues/IBIAHT
这是一个关于修改推理profiler等级以获取aclnn算子信息的需求。问题涉及到mindspore的profiler等级变更，原因是需要使用Level1才能获取到aclnn算子信息。

https://gitee.com/mindspore/mindformers/issues/IBI99E
该issue为用户提出需求类型，主要涉及qwen2微调训练如何指定卡的问题，由于启动脚本和参数设置不明确，导致用户无法成功指定多张卡进行微调训练。

https://gitee.com/mindspore/mindformers/issues/IBI0A8
这个issue类型是bug报告，主要涉及DynamicNTK推理过程中输入大小超过max_position_embedding时出现报错，可能由于边界条件检查不完善导致。

https://gitee.com/mindspore/mindformers/issues/IBHVEG
这是一个用户提出需求的问题，主要涉及mindformers是否支持商用的并发推理，用户想了解现在是否支持并发请求。

https://gitee.com/mindspore/mindformers/issues/IBHT6I
这是一个bug报告，主要涉及盘古7b并行解码服务化推理时延劣化和精度异常，可能由于算法实现或模型设计导致。

https://gitee.com/mindspore/mindformers/issues/IBHRMC
这是一个bug报告，涉及mindformer1.2.0应用官方例程的RoundToNearest后量化算法报错EH9999，以及Predict run mode显示False的问题。

https://gitee.com/mindspore/mindformers/issues/IBHM5U
这个issue类型是bug报告，主要对象是ProfileMonitor文档格式，缺少空格导致关键字格式未能正确配置。

https://gitee.com/mindspore/mindformers/issues/IBHKOB
这是一个bug报告，主要涉及到Qwenvl模型在推理过程中效果不佳的问题。原因可能是推理配置或权重文件的问题，导致模型无法正确进行推理。

https://gitee.com/mindspore/mindformers/issues/IBHH74
这是一个bug报告，涉及的主要对象是MindSpore文档中的安装和快速启动流程。由于Python版本不一致和模块缺失导致了报错的问题。

https://gitee.com/mindspore/mindformers/issues/IBHD2G
这是一个bug报告，涉及api兼容性用例报错，可能由于用例执行时的兼容性问题导致。

https://gitee.com/mindspore/mindformers/issues/IBHAW9
这个issue类型是bug报告，涉及的主要对象是项目中的测试用例。由于测试用例位置不正确，导致了dev分支的测试用例无法正常使用。

https://gitee.com/mindspore/mindformers/issues/IBH9K4
这是一个bug报告，主要涉及mindformers 1.2.0版本在进行Llama27b单卡Lora微调过程中出现的错误。原因可能是使用了即将不支持的callback方法导致。

https://gitee.com/mindspore/mindformers/issues/IBH8EE
这是一个bug报告，单卡推理时出现了问题。导致这个问题的原因是由于配置文件中的max_device_memory设置不正确。

https://gitee.com/mindspore/mindformers/issues/IBH48K
这是一个bug报告，主要涉及MindFormers中QwenVL模型推理效果差的问题。由于可能是模型配置或参数设置不正确导致生成文本与问题不相关的症状。

https://gitee.com/mindspore/mindformers/issues/IBH48I
这是一个bug报告，该问题涉及到Qwenvl模型在推理时出现Unsupported op [ReshapeAndCache]算子缺失错误。这可能是由于模型推理所需的部分算子在硬件环境Atlas 800T A2服务器上不支持导致的。

https://gitee.com/mindspore/mindformers/issues/IBGYDN
这是一个bug报告，涉及的主要对象是Mindformers1.4beta和MindSpore 2.4.1，由于MindSpore 2.4.1中的Profile类增加的判断条件与Mindformers接口不同步，导致在收集stack信息时失败。

https://gitee.com/mindspore/mindformers/issues/IBGX94
这个issue类型为需求提出，涉及主要对象为新增ring attention负载均衡支持，用户提出了对该功能的需要。

https://gitee.com/mindspore/mindformers/issues/IBGU96
这是一个bug报告，涉及的主要对象是_PipeLineConfig类，由于重复触发了default_transformer_config的实例化流程，导致调用set_auto_parallel_contex接口时改写了auto_parallel_context中pipeline_stages的值，进而导致校验报错。

https://gitee.com/mindspore/mindformers/issues/IBGQMO
这是一个bug报告，涉及mindformers在gitee上的llama2文档的编写错误，导致yaml与实际运行脚本不一致。

https://gitee.com/mindspore/mindformers/issues/IBGPZM
这个issue是一个bug报告，涉及的主要对象是dev分支callback的init未导入StressDetectCallBack接口。导致的症状是test_stress_detect.py测试用例无法通过。

https://gitee.com/mindspore/mindformers/issues/IBGHBO
这是一个关于软件操作问题的issue，主要涉及到Modelarts云平台的训练作业。用户提出了llama训练报错的问题，原因可能是启动指令错误或者路径配置问题。

https://gitee.com/mindspore/mindformers/issues/IBGD43
这是一个bug报告，涉及离线权重转换指令与脚本路径不一致的问题。由于路径更改但文档未更新导致无法正确运行转换权重指令。

https://gitee.com/mindspore/mindformers/issues/IBGCLY
这是一个bug报告，该问题涉及单机多卡推理时出错，可能是由于输入布局模式错误导致的RuntimeError。

https://gitee.com/mindspore/mindformers/issues/IBGBVP
这是一个bug报告，问题涉及的主要对象是接口调用顺序。由于调用顺序不正确，导致校验失败的bug。

https://gitee.com/mindspore/mindformers/issues/IBGBNA
这是一个Bug报告，主要涉及Mindformers 1.2.0版本在进行单卡全量微调时出现报内存错误的问题。Bug的原因可能是代码中的问题导致内存错误。

https://gitee.com/mindspore/mindformers/issues/IBGB1G
这是一个bug报告，报告了调用glm4_tokenizer.py中save_vocabulary方法时报错的情况，可能由于代码实现问题或参数传递错误导致。

https://gitee.com/mindspore/mindformers/issues/IBGAJH
这是一个关于在aicc云平台训练llama时报错的问题，涉及到配置参数和报错信息，用户询问如何解决该错误。

https://gitee.com/mindspore/mindformers/issues/IBG9I9
这是一个关于bug报告的issue，主要涉及mindformers 1.2.0中的微调训练，用户在单卡运行时出现失败的问题。原因可能是配置参数需要调整。

https://gitee.com/mindspore/mindformers/issues/IBG8EQ
这是一个bug报告，主要涉及MindSpore的模型预测过程中出现了文件缺失错误。

https://gitee.com/mindspore/mindformers/issues/IBG86R
这个issue是关于配置微调oom的bug报告，涉及MindSpore版本2.5.0及相关配置的调整，导致OOM（Out of Memory）问题。

https://gitee.com/mindspore/mindformers/issues/IBG7EV
这个issue属于bug报告类型，主要涉及mindformers工具中的Chatglm36b模型全量微调过程中出现报错的问题，可能是由于配置文件或代码实现的问题导致的。

https://gitee.com/mindspore/mindformers/issues/IBG5FO
这是一个bug报告类型的issue, 主要涉及的对象是使用openai推理接口时出现的错误。原因可能是无法正确解析请求上下文为json导致报错。

https://gitee.com/mindspore/mindformers/issues/IBG2UK
这个issue类型是bug报告，该问题涉及的主要对象是ModelAr，由于提供的镜像无法创建成功，导致用户在使用过程中遇到问题。

https://gitee.com/mindspore/mindformers/issues/IBG2I2
这是一个bug报告, 该问题涉及的主要对象是在使用mindformer进行GLM36b推理时出现了报错。出现报错的原因可能是由于numpy库中的警告导致的。

https://gitee.com/mindspore/mindformers/issues/IBG1V8
这个issue类型是bug报告，涉及的主要对象是关于使用npu_device_id拉起PD分离和PD混布的模型实例。由于消除ASCEND_RT_VISIBLE_DEVICES环境变量，用户遇到了相关问题或需要进一步的帮助。

https://gitee.com/mindspore/mindformers/issues/IBG02H
这是一个bug报告，涉及的主要对象是MindSpore的qkv校验流程。导致这个问题的原因是使用单safetensors文件作为权重文件时，出现了异常报错"./qwen2_7b.safetensors is not a directory"。

https://gitee.com/mindspore/mindformers/issues/IBFYEJ
这是一个bug报告，涉及的主要对象是权重转换脚本。原因是命令行参数错误导致出现AttributeError。

https://gitee.com/mindspore/mindformers/issues/IBFMCU
这是一个bug报告，该issue涉及callback中对micro_batch_interleave_num的日志输出为%s，可能是由于代码中的逻辑错误导致的。

https://gitee.com/mindspore/mindformers/issues/IBFBO1
该issue是一个关于bug报告的类型，涉及到版本控制中日志打印错误的问题，可能由于代码编写或配置错误导致日志打印不准确。

https://gitee.com/mindspore/mindformers/issues/IBFBAF
这个issue是用户提出的需求，要求在Qwen2.5文档中新增对transformer版本描述，导致了转换权重操作无法顺利进行。

https://gitee.com/mindspore/mindformers/issues/IBFBA3
这是一个bug报告，主要涉及Qwen2.5权重转换指令运行失败的问题。导致该bug的原因可能是缺失参数导致程序无法正常运行。

https://gitee.com/mindspore/mindformers/issues/IBFB9U
这是一个文档反馈类型的issue，主要涉及MindSpore框架下mindformers项目的readme文档。问题是存在部分命令可能令人误解和存在错漏，导致用户在使用单机或多机训练时遇到了参数设置问题。

https://gitee.com/mindspore/mindformers/issues/IBFB6Y
这是一个bug报告类型的issue，主要涉及mindie启动安装问题，可能是安装过程出现了问题导致/usr/local/Ascend/mindie/1.0.RC3/mindieservice/security/下缺少文件。

https://gitee.com/mindspore/mindformers/issues/IBFB3U
这个issue是关于文档错误的bug报告，涉及到LLAMA2阅读理解评测的配置文件问题，原因可能是配置文件名与指令中示例不一致。

https://gitee.com/mindspore/mindformers/issues/IBF3DQ
这是一个bug报告，涉及AutoModel类在构建前段并行llama网络时将模式重置为静态图，导致模式不符合预期的问题。

https://gitee.com/mindspore/mindformers/issues/IBF32P
这是一个bug报告，涉及主要对象为FreqsMgr类模型训练时显存溢出，由于调用该类导致部分模型在分布式训练时出现显存溢出的问题。

https://gitee.com/mindspore/mindformers/issues/IBEZVX
这是一个bug报告，涉及chat_web项目中运行python run_chat_server.py时出现错误的问题，可能由于环境配置问题或代码逻辑错误导致。

https://gitee.com/mindspore/mindformers/issues/IBEWYI
这是一个bug报告类型的issue，主要涉及云平台modelarts的训练internlm7B功能，用户遇到无法运行的错误并寻求帮助。

https://gitee.com/mindspore/mindformers/issues/IBEP9Z
这是一个bug报告，涉及MindSpore框架下的模型推理过程中出现的错误。由于同步流失败导致报错。

https://gitee.com/mindspore/mindformers/issues/IBENFO
这是一个优化需求，主要涉及到配置项`run_mode`的读取方式优化。由于目前`run_mode`配置项通过环境变量进行传递，但实际上已记录在全局变量`Context()`中，因此希望仅支持从`Context()`中获取`run_mode`，不再支持直接通过环境变量控制。

https://gitee.com/mindspore/mindformers/issues/IBEGKH
这是一个bug报告，主要涉及llama3.1推理时无法提前停止输出的问题，可能是由于stop_words_ids无法起作用所导致。

https://gitee.com/mindspore/mindformers/issues/IBEEB4
这是一个bug报告，涉及FlopsUtilizationCollector CallBack和sink_size配置同时报错的问题。由于这两个配置同时配置时会导致训练报RuntimeError异常，用户提出了关于此错误的问题。

https://gitee.com/mindspore/mindformers/issues/IBE8CZ
这是一个bug报告，涉及的主要对象是docker多机多卡hccl配置。由于docker配置了ssh的2222端口免密登录后，在容器内部进行多机多卡操作时出现报错。

https://gitee.com/mindspore/mindformers/issues/IBE6RI
这是一个bug报告，涉及对象为mindie启动失败。由于缺少libpython3.10.so.1.0库文件导致启动报错。

https://gitee.com/mindspore/mindformers/issues/IBE1H1
这是一个bug报告，涉及mindformers下的一个issue，用户在执行awq w4a16量化过程中遇到数值拆分错误(ValueError: not enough values to unpack)，导致量化推理出现问题。

https://gitee.com/mindspore/mindformers/issues/IBDT9S
这是一个Bug报告类型的Issue，主要涉及的对象是llama3.1预训练配置文件。由于无法找到名为AdamWeightDecayX的优化器类，导致了报错数值错误。

https://gitee.com/mindspore/mindformers/issues/IBDPT3
这是一个用户提出需求的issue，主要涉及llama3.1预训练文档缺失问题，用户想了解如何进行多机和单机训练，以及与llama3配置文件的关系，希望解决报错问题。

https://gitee.com/mindspore/mindformers/issues/IBDL9S
这是一个bug报告，涉及主要对象是权重转换过程中出现的错误。导致这个问题的原因是版本不兼容和缺少配置文件导致转换失败。

https://gitee.com/mindspore/mindformers/issues/IBDJIY
这个issue是一个bug报告，涉及的主要对象是打印时延数据。由于缺乏有效性校验，可能导致打印时延数据影响推理流程。

https://gitee.com/mindspore/mindformers/issues/IBDF9D
这是一个bug报告，问题涉及到在单卡上微调时出现报错。由于numpy版本兼容性问题，导致出现了给定的报错信息。

https://gitee.com/mindspore/mindformers/issues/IBDBD5
这是一个bug报告，涉及到华为昇腾altls300i单卡运行llama2_7b报错的问题，原因可能是硬件兼容性或软件配置错误导致的症状。

https://gitee.com/mindspore/mindformers/issues/IBD9AZ
这个issue是bug报告，涉及到基于910A的GLM3微调报错的问题，可能是由于程序错误或者不完整的模型导致的错误。

https://gitee.com/mindspore/mindformers/issues/IBD6YS
这是一个用户询问支持某项功能的问题，主要对象是针对软件支持知识。

https://gitee.com/mindspore/mindformers/issues/IBD0T8
这是一个bug报告，涉及Mindformers的Llama模型使用了Cell类的self.cast，导致运行测试用例时出现No Attribute of cast的错误。

https://gitee.com/mindspore/mindformers/issues/IBCJCE
这是关于模型推理验证过程中加载权重失败的bug报告，主要涉及模型权重加载的功能。可能由于权重加载路径或格式错误导致了权重未成功加载的警告。

https://gitee.com/mindspore/mindformers/issues/IBCJBR
这是一个bug报告，涉及到mindformers项目中的权重转换功能，由于传入的参数格式不正确导致了数值错误的问题。

https://gitee.com/mindspore/mindformers/issues/IBCJ8I
这是一个bug报告，主要涉及到使用mindformers/convert_weight.py进行权重转换时出现的错误。导致这个问题的原因是缺少/config.json文件。

https://gitee.com/mindspore/mindformers/issues/IBCGPV
这是一个bug报告，涉及到mindformers项目下的合并权重后推理乱码的问题。此问题可能由于loar微调后引起的错误导致。

https://gitee.com/mindspore/mindformers/issues/IBCE3X
这是一个关于deepseekai/deepseekcoder7bbasev1.5模型在Atlas800 9000支持微调的问题，报告了全参微调报OOM和lora微调报错的情况。

https://gitee.com/mindspore/mindformers/issues/IBC3ML
这是一个bug报告，主要涉及到双卡推理baichuan213B报错的问题。由于配置文件中的一些设置可能存在问题，导致运行时报错。

https://gitee.com/mindspore/mindformers/issues/IBBY4S
这是一个bug报告，主要涉及GLM2样例中缺少了predict_glm2_6b.yaml文件，导致找不到该文件。

https://gitee.com/mindspore/mindformers/issues/IBBUP1
这是一个bug报告，主要涉及的对象是在已经跑通qwen27b微调的环境下进行原权重推理报错。导致这个问题的原因可能是代码实现上的逻辑错误或环境配置问题。

https://gitee.com/mindspore/mindformers/issues/IBBOOS
这是一个关于兼容性问题的用户提问，主要涉及mindformers 1.2.0和mindspore 2.4之间的兼容性。

https://gitee.com/mindspore/mindformers/issues/IBBONW
这是一个Bug报告类型的Issue，主要涉及到数据集下载的问题，由于使用wget下载Wikitext103数据只能下载部分数据，用户希望寻求下载全部数据集的帮助。

https://gitee.com/mindspore/mindformers/issues/IBBJHV
该issue属于用户问题咨询类型，主要涉及的对象是whisperlargev3模型，用户询问该模型是否支持中文识别。

https://gitee.com/mindspore/mindformers/issues/IBBJGU
这是一个性能问题报告，主要涉及到qwenvl的推理速度在800T A2上的表现。可能是用户对qwenvl在特定环境下的性能表现感到疑惑或不满意。

https://gitee.com/mindspore/mindformers/issues/IBBH4S
这是一个bug报告，涉及到模型转换的问题。由于huggingface到mindformers转换和mindformers到huggingface转换存在问题，可能是转换不是严格的1:1转换导致的损失。

https://gitee.com/mindspore/mindformers/issues/IBBGLU
这是一个bug报告，主要涉及mindformers推理qwen2.572b功能，由于设备显存不足导致推理报错。

https://gitee.com/mindspore/mindformers/issues/IBBFER
这是一个bug报告类型的issue，主要涉及mindformers运行时输出的内容问题，可能是由于环境配置问题导致的。

https://gitee.com/mindspore/mindformers/issues/IBBDSK
这是关于软件bug的报告，涉及mindformers部署带权重的训练任务时出现loss跑飞的问题。

https://gitee.com/mindspore/mindformers/issues/IBBC40
这个issue是bug报告，主要涉及llava模型在example例子中无法正常调用的问题，可能是由于缺少相关实现导致的。

https://gitee.com/mindspore/mindformers/issues/IBBC31
这是一个缺少关键方法使用说明的文档反馈类型的issue，主要涉及到mindformers下的EntityScore, EmF1Metric, PerplexityMetric, PromptAccMetric评估类。由于缺少主要功能描述和关键步骤，导致用户无法完成相关任务。

https://gitee.com/mindspore/mindformers/issues/IBBC1L
这是一个bug报告类型的issue，主要涉及mindformers中Pipline资料中task支持范围描述有误导致技术原理和软件实现不一致的问题。

https://gitee.com/mindspore/mindformers/issues/IBBC1I
这个issue属于bug报告类型，主要涉及mindformers下的AutoModelForZeroShotImageClassification类，可能由于技术原理与软件实现不一致导致描述错误，需要修正。

https://gitee.com/mindspore/mindformers/issues/IBBC1F
这个issue是一个bug报告，涉及到mindformers下的AutoModelForCausalLM文档内容。原因是文档中描述与实际软件实现存在不一致。

https://gitee.com/mindspore/mindformers/issues/IBBC15
这个issue类型是bug报告，涉及主要对象为mindformers.AutoModel文档页面，并由于描述和软件实现不一致导致出现问题。

https://gitee.com/mindspore/mindformers/issues/IBBC13
这是一个bug报告类型的issue，涉及的主要对象是TrainingArguments参数说明中的错别字问题。由于文档中出现了错别字，可能会导致用户在理解和使用TrainingArguments参数时出现困惑。

https://gitee.com/mindspore/mindformers/issues/IBB1D4
这是一个bug报告类型的issue，涉及到MindFormers中Llama2_7b模型在本地自验结果与官网数据差异很大的问题。可能由于模型配置、数据集处理或评测命令等方面的问题导致了该现象。

https://gitee.com/mindspore/mindformers/issues/IBAV8S
这是一个bug报告，主要涉及Biachuan2模型2*8集群训练中权重加载报错的问题，可能由于权重shape不一致导致。

https://gitee.com/mindspore/mindformers/issues/IBAV68
这是一个bug报告，主要涉及Biachuan2模型在2*8集群训练过程中出现"set device failed"错误，可能是由于程序执行命令时参数设置或环境配置问题所导致。

https://gitee.com/mindspore/mindformers/issues/IBAV3O
这是一个bug报告，涉及主要对象是Biachuan2模型2*8集群训练主节点，由于init hccl graph adapter初始化失败导致报错。

https://gitee.com/mindspore/mindformers/issues/IBAV0N
这是一个bug报告，问题涉及的主要对象是MindIE大模型推理，由于权限问题导致了无法初始化endpoint，进而导致报错。

https://gitee.com/mindspore/mindformers/issues/IBAUZR
这是一个bug报告，涉及的主要对象是MindIE大模型推理。导致报错的原因是Backend library路径权限设置问题。

https://gitee.com/mindspore/mindformers/issues/IBAUXV
这是一个bug报告，主要涉及MindIE镜像在创建容器时出现的错误。导致该错误的原因是缺少必要的tini文件。

https://gitee.com/mindspore/mindformers/issues/IBAURY
这是一个bug报告，涉及MindSpore模型llama27b2在多卡推理过程中出现LC0c MatmulAllReduce失败的错误。

https://gitee.com/mindspore/mindformers/issues/IBATIL
这个issue是bug报告，主要涉及llama的训练loss值在rank0中显示为0，可能是由于并行计算导致的问题。

https://gitee.com/mindspore/mindformers/issues/IBANJE
这是一个bug报告issue，主要对象是在微调Qwen27b模型时，在执行alpaca_converter.py转换自有中文数据集为静态shape格式时出现乱码情况。原因可能是数据转换过程中的编码或格式不兼容所致。

https://gitee.com/mindspore/mindformers/issues/IBAKZ9
这是一个bug报告，涉及的主要对象是在华为昇腾910B使用mindIE部署telechat27b，导致Import模型报错无法识别TelechatForCausalLM。

https://gitee.com/mindspore/mindformers/issues/IBAHKY
这是一个用户提出需求的issue，主要对象是适配Telechat2系列模型。

https://gitee.com/mindspore/mindformers/issues/IBAD6M
这是一个bug报告，涉及的主要对象为运行Whisper时出现报错。由于代码中调用了错误的函数或方法，导致程序无法正常执行。

https://gitee.com/mindspore/mindformers/issues/IBA8GL
这是一个bug报告，涉及mindformers项目下的推理乱码问题，可能由于环境配置不兼容导致。

https://gitee.com/mindspore/mindformers/issues/IBA4T4
这个issue类型是bug报告，主要对象是dryrun工具，由于部分条件下吞吐率显示不正常(显示30+samples/p/s，实际小于3 samples/p/s)，需要工具修复。

https://gitee.com/mindspore/mindformers/issues/IBA1LK
这是一个bug报告，涉及的主要对象是权重模型转换尺寸不匹配。导致这个问题的原因是转换权重的python脚本报告了尺寸不匹配的错误。

https://gitee.com/mindspore/mindformers/issues/IB9XUM
这是一个bug报告，主要涉及到MindSpore Transformers文档中加载模型权重的问题。由于配置文件加载模型不会载入模型权重，导致用户期望的结果和实际结果不符。

https://gitee.com/mindspore/mindformers/issues/IB91W7
这是一个bug报告，主要涉及的对象是Glm4在使用MF1.3全参微调时出现报错"'numpy.float64' object cannot be interpreted as an integer"。由于可能传入了错误类型的数据导致的类型转换错误，引发了该BUG。

https://gitee.com/mindspore/mindformers/issues/IB8WEU
这是一个bug报告，涉及的主要对象是mindformers/mindformers/core/callback/callback.py脚本中的world_size参数。由于脚本中获取world_size数值的方法不正确，导致训练过程中无法正确获取world_size数值，进而影响其他组件的数据处理，出现错误报告。

https://gitee.com/mindspore/mindformers/issues/IB8SWW
这是一个bug报告，涉及的主要对象是在华为昇腾910b部署qwen2_57b进行权重模型转换时出现版本不兼容问题。由于transformers、mindformers与tokenizers版本不兼容，导致用户不知道mindspore==2.3.0、mindformers==1.2.0应该对应什么版本的transformers。

https://gitee.com/mindspore/mindformers/issues/IB8QRQ
这是一个bug报告，主要涉及mindspore在推理过程中出现的错误。可能的原因是不支持在CPU上使用ReshapeAndCache操作。

https://gitee.com/mindspore/mindformers/issues/IB8QG5
这是一个bug报告，主要涉及llama3单机并行训练中，当pp > 1时会卡死，可能是由于多进程训练导致的问题。

https://gitee.com/mindspore/mindformers/issues/IB8H7Z
这个issue类型是功能需求，主要涉及的对象是AI修图业务交付项目。由于需求变更或技术实现问题，用户希望支持vit+internlm27b推理。

https://gitee.com/mindspore/mindformers/issues/IB8E8B
这是一个bug报告，主要涉及到文档中的链接无效的问题。由于链接无效，导致用户无法正常访问相关内容。

https://gitee.com/mindspore/mindformers/issues/IB8ASR
这是关于技术实现的问题，涉及到权重拼接方式不一致导致的精度问题和框架间权重迁移需要额外适配的bug报告。

https://gitee.com/mindspore/mindformers/issues/IB8AOP
这是一个用户寻求帮助的问题，涉及主要对象为在使用fastapi框架中调用多卡推理的原理。用户提出了多进程持久化模型时出现的推理错误，希望找到在不使用MindIe的情况下解决这个问题的方法。

https://gitee.com/mindspore/mindformers/issues/IB82IX
这是一个bug报告，涉及到mindformers项目下的internlm27b模块，该问题可能由于软件版本不匹配或者代码bug导致推理报错。

https://gitee.com/mindspore/mindformers/issues/IB7WX6
这是一个bug报告，主要涉及到静态图并行执行下开关Ulysees序列并行精度不一致的问题。原因可能是程序执行参数设置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/IB7WVY
这是一个关于部署模型的问题，涉及的主要对象是模型使用的不同硬件加速器，原因可能是硬件兼容性或转换问题。

https://gitee.com/mindspore/mindformers/issues/IB7N9E
这是一个bug报告，问题涉及微调参数设置以避免8卡微调显存溢出，可能由于配置参数未正确设置导致显存溢出的问题。

https://gitee.com/mindspore/mindformers/issues/IB7MTS
这是一个bug报告，主要涉及对象是mindformers中的软件版本2.3.0.rc2，在Altlas 800TA2设备上微调报错。由于jit_config注释掉后可以正常运行，可能是与jit_config配置相关的问题导致报错。

https://gitee.com/mindspore/mindformers/issues/IB767W
这是一个bug报告，主要涉及权重保存时打印的epoch信息有误，可能由于代码中打印逻辑错误导致。

https://gitee.com/mindspore/mindformers/issues/IB71OI
这是一个用户提出需求的issue，涉及主要对象是mindformers的dev分支下的数据处理脚本。该问题的产生可能是因为在qwen2.5 research下面缺少相应的数据处理脚本。

https://gitee.com/mindspore/mindformers/issues/IB70YV
这是一个用户提出需求的issue，涉及主要对象是qwen1.572bchat的get_model和get_tokenizer函数，由于官方目前只提供了baichuan213b而没有qwen系列，用户在封装chat_web时遇到了填写get_model和get_tokenizer函数的问题。

https://gitee.com/mindspore/mindformers/issues/IB70LQ
这是一个bug报告，主要涉及的对象是glm36b训练卡住。由于某种原因导致启动8卡训练时会卡住，并且没有输出任何错误信息。

https://gitee.com/mindspore/mindformers/issues/IB6S5S
这是一个bug报告，主要涉及的对象是mindformers中进行推理时遇到的设备兼容性问题，报错信息表明Ascend设备不支持，需要检查环境配置和构建mindspore wheel package的方式。

https://gitee.com/mindspore/mindformers/issues/IB6L96
这个issue是一个bug报告，涉及到infer_boost flag 默认为on导致CPU部分算子计算错误的问题。

https://gitee.com/mindspore/mindformers/issues/IB6GQI
这是一个bug报告，涉及主要对象为使用qwen1.57b或者14b进行lora微调时出现的错误，可能是由于配置文件中添加了lora配置而导致的。

https://gitee.com/mindspore/mindformers/issues/IB676B
这是一个bug报告，主要对象是mindformers工具中的hccl_tools.py文件。该问题可能是由环境配置问题导致的，导致无法成功生成RANK_TABLE_FILE文件。

https://gitee.com/mindspore/mindformers/issues/IB617H
这是一个bug报告，主要涉及模型推理过程中使用参数设置时产生错误结果的问题。

https://gitee.com/mindspore/mindformers/issues/IB5ZTO
这是一个bug报告，涉及主要对象为Qwen14BChat的转换程序。由于执行convert_weight.py时报错Segmentation fault，可能是由于程序中的某些限制值被设置为零导致的问题。

https://gitee.com/mindspore/mindformers/issues/IB5T4A
这个issue类型是用户询问问题，主要涉及对象是Mindspore测试工具。由于缺乏具体描述和背景信息，用户提出了关于Mindspore测试精度工具的疑问，希望得到帮助解决问题。

https://gitee.com/mindspore/mindformers/issues/IB5KIM
这是一个Bug报告，涉及mindformers微调过程中出现loss震荡不收敛的问题。导致该问题的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/IB5F26
这是用户提出的需求。该问题单涉及的主要对象是在mindformers项目中实现的权重保存过滤功能。由于缺乏过滤指定关键词权重不保存的配置选项，用户希望通过yaml配置实现此功能。

https://gitee.com/mindspore/mindformers/issues/IB5ETI
这是一个bug报告，主要涉及的对象是权重保存命名格式，由于权重保存命名方式改变导致长时间训练时挤爆磁盘空间。

https://gitee.com/mindspore/mindformers/issues/IB5EOL
这是一个bug报告，主要涉及llama模型中的infer_attention算子初始化与训练相关算子初始化的冲突，导致模型只能进行推理或者训练，无法同时进行。

https://gitee.com/mindspore/mindformers/issues/IB5EJX
这个issue类型为功能需求，主要涉及pet中存在的冗余代码不利于类方法扩展。

https://gitee.com/mindspore/mindformers/issues/IB5EGC
这是一个关于bug的报告，主要涉及的对象是数据集加载功能。由于当前的方式不能正常加载已保存的数据集，需要调用load_from_disk接口来临时支持。

https://gitee.com/mindspore/mindformers/issues/IB5ED1
这是一个用户提出需求的issue，涉及主要对象是加载datasets数据集时未设置环境变量导致报错，希望改进环境变量获取方式。

https://gitee.com/mindspore/mindformers/issues/IB5EBY
这是一个bug报告，涉及到saveckpt打印信息有问题，可能是由于错误的输出信息导致用户混淆。

https://gitee.com/mindspore/mindformers/issues/IB5APZ
这是一个bug报告，主要涉及visualglm模型转换的问题，可能由于输入数据异常或者转换工具版本不兼容导致转换报错。

https://gitee.com/mindspore/mindformers/issues/IB5AOK
这是一个bug报告，主要涉及Qwen/Qwen7B模型在转换成ckpt后推理结果异常的问题。

https://gitee.com/mindspore/mindformers/issues/IB5AKM
这是关于bug报告的issue， 主要涉及mindspore中qwen21.5B模型微调后推理乱码的问题，可能是由于部分权重没有加载导致的。

https://gitee.com/mindspore/mindformers/issues/IB5AGP
这是一个bug报告，涉及到llama27B模型推理过程中的错误。由于加载权重时出现问题，导致推理过程报错。

https://gitee.com/mindspore/mindformers/issues/IB55UI
这是一个性能问题报告，主要涉及特征值检测和非动态loss场景的性能损耗。可能是由于某种原因导致性能下降，用户需要解决性能损耗问题。

https://gitee.com/mindspore/mindformers/issues/IB550M
这是一个bug报告，主要涉及关于兼容关系是否写反的问题。导致该问题可能是由于写错了兼容关系的逻辑导致了不符合预期的结果。

https://gitee.com/mindspore/mindformers/issues/IB52UR
这是一个bug报告，涉及到mindformers下的mindformer1.3微调qwen_7B出现RuntimeError导致编译图失败的问题。

https://gitee.com/mindspore/mindformers/issues/IB505F
这是一个bug报告，涉及到mindformers的Baichuan27B模型推理报错910B。由于推理命令执行时报错，可能是权重加载或模型配置出现了问题。

https://gitee.com/mindspore/mindformers/issues/IB4OHX
这个issue类型是bug报告，涉及的主要对象是MindFormers的README文档。由于版本配套关系写反，导致了用户提出了疑惑并认为可能是错误。

https://gitee.com/mindspore/mindformers/issues/IB4G4Q
这是一个关于功能兼容性问题的用户提问，涉及主要对象为mindformers1.2分支中的Qwen1.514b以及910A设备。由于用户想确认在910A设备上是否可以运行mindformers1.2分支的Qwen1.514b版本，可能由于平台兼容性或系统要求等原因导致此疑问。

https://gitee.com/mindspore/mindformers/issues/IB4D5A
这是一个bug报告，主要涉及qwen7Bbase在使用chat web时出现的"Ascend operator selection failed"错误。可能是由于运行chat web的相关代码时出现了操作符选择失败导致的问题。

https://gitee.com/mindspore/mindformers/issues/IB3TUS
这是一个bug报告，主要涉及的对象是Qwen2.1.5BInstruct模型。原因是预定义模型结构和加载的ckpt无法完全匹配，导致输出结果全是"！"。

https://gitee.com/mindspore/mindformers/issues/IB3RNT
这是一个用户提出需求的issue，主要涉及mindformers高版本对Altas800训练服务器的支持问题。由于最近的几个版本都没有写Altas800（9000/9010）的支持，用户想了解是否后续不支持该训练服务器了。

https://gitee.com/mindspore/mindformers/issues/IB3GQ3
这是一个bug报告，主要涉及mindformers r1.3.0版本模型并行过程中出现的错误。由于模型拆分和合并的过程中出现了维度不匹配的问题，导致最终合并后的模型大小异常且推理结果错误。

https://gitee.com/mindspore/mindformers/issues/IB2SWT
这是一个bug报告，涉及到mindformers在使用mindspore版本2.4.0和mindformers版本1.3时，在Yi6BBase单机微调过程中出现报错的问题。

https://gitee.com/mindspore/mindformers/issues/IB2PON
这是一个Bug报告，涉及主要对象为在Atlas服务器上训练llama38b时出现的EZ9999内部错误，可能是由于开启离线Profile采集性能数据导致的问题。

https://gitee.com/mindspore/mindformers/issues/IB2G71
这是一个bug报告，涉及对象是LLAMA2_7b模型的sft功能，由于某种原因导致容器中执行sft时报错。

https://gitee.com/mindspore/mindformers/issues/IB2A8Y
这是一个bug报告，涉及的主要对象是telechat235B模型，在dev分支中启动失败。

https://gitee.com/mindspore/mindformers/issues/IB29N9
这是一个bug报告，涉及主要对象为Qwen1.57b单机单卡，问题原因可能是'Gather'操作初始化失败导致出现错误信息。

https://gitee.com/mindspore/mindformers/issues/IB29LN
这是一个bug报告类型的issue，涉及对象是MindSpore中的rmsnorm模块。由于安装的是NPU版本，但报告显示rmsnorm不支持CPU，可能是版本兼容性问题导致的。

https://gitee.com/mindspore/mindformers/issues/IB29IJ
这是一个bug报告，主要涉及到glm4推理乱码的问题，可能是由于数据集、机器类型或环境不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/IB29IG
这个issue类型属于bug报告，主要涉及的对象是glm4模型推理的乱码问题，可能由于数据集、机器或环境的配置不当导致。

https://gitee.com/mindspore/mindformers/issues/IB1YRR
这是一个bug报告，涉及的主要对象是atlas 800TA2 dev分支 ms2.3.0，原因是推理过程中出现了Resize失败的错误。

https://gitee.com/mindspore/mindformers/issues/IB1YPG
这个issue属于Bug报告类型，涉及的主要对象是Atlas 800TA2，由于指针`mem_manager_`为空指针导致训练报错。

https://gitee.com/mindspore/mindformers/issues/IB1YJ2
这是一个用户提出需求的问题，涉及对象为LLAMA3中的lora训练配置，由于LLAMA3可能不支持lora训练导致训练报错，用户想知道什么时候会支持。

https://gitee.com/mindspore/mindformers/issues/IB1WI5
这是一个用户提出需求的issue，主要涉及到大模型训练配置中sink size参数需要改成1。用户反馈该需求是因为当sink size为2时，每两步才会打印一次loss，不方便用户比对精度。

https://gitee.com/mindspore/mindformers/issues/IB1LTX
这是一个bug报告，涉及到MindSpore中使用LLAMA2模型微调时出现的命令执行错误问题，可能是由于未正确设置日志路径和环境变量导致命令无法执行。

https://gitee.com/mindspore/mindformers/issues/IB1ALS
这是一个bug报告，涉及的主要对象是llam213四卡Lora微调后生成的文件夹。由于文件夹中的权重未正确合并，导致生成的文件混乱无法正常使用。

https://gitee.com/mindspore/mindformers/issues/IB15EJ
这个issue属于bug报告类型，涉及的主要对象是MindForMers框架中的Qwen2.5模型推理过程。由于使用转换模型进行推理时出现了"Unsupported op [ReshapeAndCache] on CPU"的报错，可能是由于模型转换过程中的某些操作在CPU上不受支持导致的。

https://gitee.com/mindspore/mindformers/issues/IB13HG
这是一个bug报告，涉及的主要对象是mindformers下的mindie服务。由于权重转化时出现了多个文件冲突，导致mindieservice启动报错。

https://gitee.com/mindspore/mindformers/issues/IB11DA
这是一个bug报告，涉及主要对象是mindformers中的SummaryMonitor模块。由于可能是SummaryMonitor功能代码存在问题，导致用户无法成功可视化mindinsight训练过程中的loss数据。

https://gitee.com/mindspore/mindformers/issues/IB0ZKW
这是一个Bug报告，涉及的主要对象是在部署qwen7b模型时出现ImportError错误。这可能是由于代码中对'mindspore.ops.auto_generate.gen_ops_prim'模块中inner_comm_irecv_op的导入错误导致的。

https://gitee.com/mindspore/mindformers/issues/IB0VFV
这是一个bug报告类型的issue，涉及到mindformers中SummaryMonitor无法使用的问题。由于加入SummaryMonitor导致报错，用户想实现可视化训练过程中的loss，但使用mindinsight也未能成功，希望找到调整方法解决该问题。

https://gitee.com/mindspore/mindformers/issues/IB0TZG
该issue类型为bug报告，涉及主要对象为mindformer数据转换功能。由于命令中存在问题，导致数据转换时出现错误。

https://gitee.com/mindspore/mindformers/issues/IB0QJB
这是一个bug报告，涉及的主要对象是mindformers0.8.0在跑llama2_7b时报错，可能是由于使用文档镜像时产生的错误导致。

https://gitee.com/mindspore/mindformers/issues/IB0F5G
这是一个bug报告，主要涉及mindformers项目中glm4chat模型推理过程出现的错误。导致这个bug可能是由于部分依赖库版本不匹配或配置错误所致。

https://gitee.com/mindspore/mindformers/issues/IB02H1
这是一个bug报告，主要涉及mindformers/experimental/parallel_core/pynative/pipeline_parallel/p2p_communication.py文件中引用了不存在的算子导致ImportError。

https://gitee.com/mindspore/mindformers/issues/IB01ID
这是一个bug报告，涉及llama3.1多npu推理过程中的Tokenizer错误问题，可能由于tokenizer.json文件路径指定错误导致。

https://gitee.com/mindspore/mindformers/issues/IAZYQU
这是一个bug报告，主要问题是internlm2训练过程中出现掉卡问题，导致部分卡没有进程。

https://gitee.com/mindspore/mindformers/issues/IAZWUA
这个issue类型是bug报告，涉及到的主要对象是mindformers项目下的代码。由于图形核心的编译失败导致了RuntimeError错误。

https://gitee.com/mindspore/mindformers/issues/IAZS0M
这是一个bug报告，涉及的主要对象是qwen20.5B模型的lora微调。由于网络结构shape报错，导致了该bug的发生。

https://gitee.com/mindspore/mindformers/issues/IAZL51
这是一个bug报告，问题涉及的主要对象是训练qwen2 1.5b模型时出现的类型错误。导致这个问题的原因是在转换权重后训练时输入参数类型不匹配，引发了类型错误的报错。

https://gitee.com/mindspore/mindformers/issues/IAZJD9
这是一个bug报告，涉及到baichuan2推理报错的问题。这个问题可能是由于ReduceOperation操作无法获取最佳tactic导致的Segmentation fault错误。

https://gitee.com/mindspore/mindformers/issues/IAZJ00
这是一个Bug报告类型的Issue，主要涉及Mindformers仓库中1.3.0版本的代码和Readme文档不一致的问题。由于配置文件与教程不符，用户希望相关内容得到及时更新。

https://gitee.com/mindspore/mindformers/issues/IAZFIL
这是一个bug报告，问题涉及到模型权重转换的过程中出现Segmentation fault错误，可能是由于版本兼容性或代码逻辑错误导致的。

https://gitee.com/mindspore/mindformers/issues/IAYZOV
这是一个bug报告类型的issue，涉及到使用mindformers推理时first token生成时间与其他token生成时间相差不大的问题。原因可能是推理过程中的某些设置或逻辑导致该现象。

https://gitee.com/mindspore/mindformers/issues/IAYST6
这是一个关于查询支持openai的服务接口的类型为询问问题的issue，主要涉及的对象是mindformers1.0版本。用户可能因为想要了解版本支持的功能范围而提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/IAYSIN
这是一个需求提出的issue，主要涉及mindformers1.0在推理时无法动态改变batch数量的问题，用户希望了解如何部署支持动态batch的解决方案。

https://gitee.com/mindspore/mindformers/issues/IAYP86
这是一个bug报告，主要对象是执行权重转化时出现无法找到config.json文件的问题，可能是由于文件路径缺少相关文件导致的。

https://gitee.com/mindspore/mindformers/issues/IAYNG4
这是一个bug报告，涉及的主要对象是Qwenvl在910B3上启动单机8卡微调任务时遇到无法合并权重的问题。

https://gitee.com/mindspore/mindformers/issues/IAYGUT
这是一个bug报告，主要涉及mindformers包中的Qwen2Tokenizer出现了"'module' object is not callable"错误，可能是由于调用方式不正确导致的。

https://gitee.com/mindspore/mindformers/issues/IAY9BW
这是一个bug报告，主要涉及的对象是权重转换过程中出现Segmentation fault导致任务退出，可能由于某些Python异常导致。

https://gitee.com/mindspore/mindformers/issues/IAY93Z
这是一个bug报告，主要涉及Llama234B推理过程中出现的问题。由于vocab文件未被正确使用，导致运行generate_llama2_34B_ptd.sh报错。

https://gitee.com/mindspore/mindformers/issues/IAY65V
这是一个bug报告，涉及对象为使用Atlas 800T A2设备进行推理的用户。导致该问题的原因可能是在微调模型后转换权重时出现了错误，导致推理过程中对于微调训练集的提问方式一直重复。

https://gitee.com/mindspore/mindformers/issues/IAY5ZL
这是一个bug报告，涉及到部署模型时出现的问题，由于内部异常没有在外层捕获打印，导致根因缺失。

https://gitee.com/mindspore/mindformers/issues/IAY4O6
这是一个bug报告，涉及到LLAMA7B模型微调过程中出现的数值错误，导致无法在CPU/GPU上广播输入形状的问题。

https://gitee.com/mindspore/mindformers/issues/IAY2JD
这是一个bug报告，主要涉及mindformers中的数据集转化和微调训练过程中出现的写入错误。由于配置或数据集设置问题，导致了无法写入的错误。

https://gitee.com/mindspore/mindformers/issues/IAY0ZU
这是一个bug报告类型的issue，涉及分布式优化器文档样例执行报错，可能是由于文档中的样例代码或说明存在错误或不完整导致的。

https://gitee.com/mindspore/mindformers/issues/IAXZCJ
这是一个bug报告，主要涉及魔乐社区体验空间。由于推理过程中不支持np.int64类型数据和文件路径含“.”时报错，导致流式推理遇到问题。

https://gitee.com/mindspore/mindformers/issues/IAXW86
这是一个用户提出的问题报告，涉及主要对象是llama3模型和wiki3数据集，用户因无法找到tokenizer.model文件和下载数据集失败而寻求帮助。

https://gitee.com/mindspore/mindformers/issues/IAXM7I
这个issue属于bug报告类型， 主要涉及到yaml文件配置问题，由于配置文件中存在某些参数设置不正确，可能导致模型并行计算出现问题。

https://gitee.com/mindspore/mindformers/issues/IAXG65
这个issue属于用户提出需求类型，主要对象是mindspore官方最高版本和依赖要求版本不匹配，可能由于版本更新引起的依赖问题。

https://gitee.com/mindspore/mindformers/issues/IAXA24
这是一个关于bug报告的issue，主要涉及预训练 qwen1.5 7b 模型时显示错误码 EI0006，可能是由软件环境中的MindSpore版本2.3.0和mindformers版本1.3.0导致的。

https://gitee.com/mindspore/mindformers/issues/IAX98T
这个issue属于用户提出需求类型，主要涉及了在Mixtral8x7B模型在alpaca_data数据集后微调后在WikiText2和SQuAD 1.1上的评测结果。

https://gitee.com/mindspore/mindformers/issues/IAX8DX
这是一个bug报告，涉及的主要对象是mindformers下的llama3项目。由于调用的函数参数错误，导致训练过程中出现了报错。

https://gitee.com/mindspore/mindformers/issues/IAX647
这是一个bug报告，主要涉及mindformers下的一个issue，用户反馈在进行lora微调后推理会出现重复或回答不相关的情况。由于Generation Config配置中存在一些参数设置不当，导致推理结果出现异常。

https://gitee.com/mindspore/mindformers/issues/IAX4KD
这是一个bug报告，涉及的主要对象是llama27b模型的权重转换。原因可能是数据类型转换问题导致了转换权重过程中出错。

https://gitee.com/mindspore/mindformers/issues/IAX113
这是一个bug报告，涉及的主要对象是mindsormers 1.2.0版本，可能由于硬件环境和软件配置不匹配导致运行报错。

https://gitee.com/mindspore/mindformers/issues/IAWVZ2
这是一个bug报告，用户提到r1.0版本缺少mindformers/models/llama目录分支，导致难以找到conberter.py等文件位置。

https://gitee.com/mindspore/mindformers/issues/IAWSP0
这是一个bug报告，涉及的主要对象是mindformers/mindformers1.0.2_mindspore2.2.13镜像。由于无法安装sympy模块导致的报错，用户在该镜像中验证mindspore时遇到了问题。

https://gitee.com/mindspore/mindformers/issues/IAWGXZ
这个issue是bug报告类型，主要涉及的对象是lora微调服务，由于调用aclnnCast失败导致报错，可能是由于网络或数据传输问题导致的。

https://gitee.com/mindspore/mindformers/issues/IAW9MC
这是一个bug报告，问题涉及启动单机多卡训练脚本时报错，可能由于脚本参数设置不正确导致。

https://gitee.com/mindspore/mindformers/issues/IAW8V7
这是一个bug报告，问题涉及到权重文件切分时device_num参数的连续性要求，可能导致操作执行失败。

https://gitee.com/mindspore/mindformers/issues/IAW8IO
这是一个bug报告类型的issue，涉及对象是无法下载的数据集链接。由于链接出现403错误导致了用户无法下载数据集。

https://gitee.com/mindspore/mindformers/issues/IAW7VE
这是一个bug报告，涉及主要对象为支持qwen2.5的训练，用户报告了结构错误的问题。

https://gitee.com/mindspore/mindformers/issues/IAW651
这是一个bug报告，主要涉及到mindformer的llama2 7b在硬件910b1上使用r1.2.0版本训练时出现的错误。可能是由于seq设置不当导致显存溢出错误。

https://gitee.com/mindspore/mindformers/issues/IAVS9Q
这是一个bug报告，主要涉及mindformers项目中的baichuan213B模型，由于yaml文件缺少参数导致启动8卡微调时报错。

https://gitee.com/mindspore/mindformers/issues/IAV95I
这是一个bug报告，主要涉及的对象是设备内存不足导致的问题。

https://gitee.com/mindspore/mindformers/issues/IAURX4
这是一个bug报告，主要涉及GLM26B的全参微调评估分数极低。由于在微调后得到的权重用于评估时，得分很差，远不如官网效果。

https://gitee.com/mindspore/mindformers/issues/IAUHC8
这是一个用户提出需求的issue，主要涉及的对象是名为"glm3lora"的功能。由于该用户想要了解目前是否支持"glm3lora"的微调功能，希望得到相关帮助。

https://gitee.com/mindspore/mindformers/issues/IAUDXV
这是一个bug报告类型的issue，主要涉及到qwen 1.5 7b模型的使用。导致报错的原因可能是`src_strategy`和`dst_strategy`参数同时为None所引起。

https://gitee.com/mindspore/mindformers/issues/IAU5AV
这是一个用户提出需求的issue，主要对象是支持 Qwen2VL 72B 模型。用户希望在已支持 QWEN2 的基础上，额外支持 Qwen2VL 72B 的模型。

https://gitee.com/mindspore/mindformers/issues/IAU3DH
这是一个bug报告，主要涉及Mindformers中CodeGeex2模型在进行推理时出现乱码的问题。导致这个问题的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/IAU29E
这是一个bug报告，涉及主要对象为在昇腾910b部署时权重文件转换报错，报错原因是模型权重转换指令中出现了转换错误。

https://gitee.com/mindspore/mindformers/issues/IATX7R
这是一个bug报告，主要涉及Mindformers中llama38b训练处理数据集时出现的错误，可能是由于numpy版本兼容性或数据集处理步骤中的某些设置问题导致。

https://gitee.com/mindspore/mindformers/issues/IATOTH
这是一个bug报告issue，主要涉及的对象是qwen模型参数转换过程。造成报错的原因可能是在参数转换过程中出现了异常情况。

https://gitee.com/mindspore/mindformers/issues/IATKBP
这是一个bug报告，涉及主要对象为在使用mindformers==1.10分支时遇到的错误。由于内存分配问题导致出现"OSError: /root/miniconda3/envs/mindspore2.2.13_py39/lib/python3.9/sitepackages/torch/lib/libgompd22c30c5.so.1: cannot allocate memory in static TLS block"提示。

https://gitee.com/mindspore/mindformers/issues/IATJPG
这是一个bug报告，主要涉及mindformers==r0.8分支下的llama2模型在单卡多卡推理时出现的参数形状不匹配导致的RuntimeError错误。

https://gitee.com/mindspore/mindformers/issues/IATJNA
这是一个bug报告，涉及的主要对象是在跑llama2模型时出现的TypeError错误。由于传入了一个未知的关键字参数'sparse_mode'导致了此错误的出现。

https://gitee.com/mindspore/mindformers/issues/IATJM7
这是一个bug报告，涉及到在跑glm3模型时遇到"Invalid data,column name: labels con not found in schema"的错误。

https://gitee.com/mindspore/mindformers/issues/IATJK8
这是一个bug报告，主要涉及ChatGLM2forCouditionalGeneration模型，在运行时出现数值错误导致 checkpoint 文件不受支持。

https://gitee.com/mindspore/mindformers/issues/IATJJH
这个issue类型是bug报告，该问题涉及到在跑glm2模型时出现TypeError错误，是因为比较了一个NoneType和int类型导致的。

https://gitee.com/mindspore/mindformers/issues/IATJDG
这是一个bug报告，涉及到在跑mindformers(llama2)时出现的ValueError: The accumulate of x_shape must be equal to out_shape的错误。这个问题可能是由于参数设置或代码逻辑问题导致的。

https://gitee.com/mindspore/mindformers/issues/IATI1H
这是一个Bug报告类型的Issue，主要涉及到mindformers(llama2)中的类注册问题，导致报错"ValueError: Can't find class type models class name glm2 in class registry"。

https://gitee.com/mindspore/mindformers/issues/IATI1G
这是一个bug报告，主要对象是在跑mindformers(llama2)时出现了RuntimeError: predict failed错误，可能是由于模型预测失败导致的。

https://gitee.com/mindspore/mindformers/issues/IATHW1
这是一个bug报告，涉及对象是模型参数加载过程中出现的维度不匹配错误，可能是由于加载了不正确的检查点或者输入数据维度不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/IAT7SG
这是一个Bug报告，涉及优化器参数写法格式不正确，由于参数格式不符合预期导致问题。

https://gitee.com/mindspore/mindformers/issues/IAT1EG
这是一个用户提出需求或疑问的类型，该问题涉及docker配置环境中使用不同版本的cann。这可能是由于项目需要不同版本的cann导致的需求或疑问。

https://gitee.com/mindspore/mindformers/issues/IASKS5
这是一个bug报告，主要涉及脚本执行出错的问题，原因是出现了图循环存在的运行时错误。

https://gitee.com/mindspore/mindformers/issues/IASI7J
这个issue类型属于bug报告，涉及的主要对象是mindformers项目中的qwen1.514b训练，报错是因为jit_config关键字参数不被识别。

https://gitee.com/mindspore/mindformers/issues/IASDMF
这是一个用户提出需求的类型的issue，主要涉及的对象是在atlas800A2硬件上训练自定义数据集，用户想要了解是否可以通过lora方式实现。

https://gitee.com/mindspore/mindformers/issues/IAR9R8
这是一个用户提出需求的issue，主要涉及的对象是在训练日志中增加TFLOPs、每次迭代的数据加载时长、前反向计算时长、参数更新时长，以及checkpoint保存和加载的时长的打点信息。由于需要根据这些指标进行性能和稳定性测试，希望在Mindformers仓中加入对应的打点信息，以便进行准确的统计和交流。

https://gitee.com/mindspore/mindformers/issues/IAR6XJ
这是一个bug报告，主要涉及mindformers中模型权重转换过程中的问题，导致推理时出现参数维度不匹配的错误。

https://gitee.com/mindspore/mindformers/issues/IAR4FX
这个issue类型属于bug报告，涉及mindformers框架下文本推理停止后内存占用无法释放的问题，可能由程序未正确释放内存导致。

https://gitee.com/mindspore/mindformers/issues/IAR2BS
这个issue是用户提出需求类型的，主要涉及Modellink仓中训练日志中缺少关于TFLOPs、数据加载时长等信息的打点信息。

https://gitee.com/mindspore/mindformers/issues/IAQYN9
这是一个bug报告，主要涉及 test_train.py 文件中的代码运行报错的问题。由于什么样的原因导致了什样症状的bug或者用户提出了关于什么的问题或者寻求什么样的帮助需要进一步详细信息才能确定。

https://gitee.com/mindspore/mindformers/issues/IAQXYR
这是一个bug报告，该问题涉及Mindformers1.2版本下QwenVL微调失败的问题，可能是由于调用GE RunGraphWithStreamAsync失败导致的。

https://gitee.com/mindspore/mindformers/issues/IAQWWS
这个issue是一个bug报告，涉及主要对象为运行脚本的程序。导致报错的原因可能是脚本编写错误或者环境配置问题。

https://gitee.com/mindspore/mindformers/issues/IAQW2T
这是一个bug报告，主要对象是runqwen.vl中的use_parallel参数，默认设置成None会影响config配置文件中的配置。由于未正确设置use_parallel参数，导致程序出现配置文件设置不生效的问题。

https://gitee.com/mindspore/mindformers/issues/IAQPXU
这是一个关于bug报告的issue，主要涉及到mindformers 910B3单卡lora微调14B模型时出现显存不足的问题。该问题可能是由于模型微调过程中对显存使用的不当导致。

https://gitee.com/mindspore/mindformers/issues/IAQHHS
这是一个bug报告，涉及到在Ascend310上部署推理过程中出现的问题，包括ChatGLM36B和LLAMA27B的显存不足和不支持相关算子的错误。

https://gitee.com/mindspore/mindformers/issues/IAQEOE
这是一个bug报告，涉及主要对象为AICC的GLM2模块，由于镜像配置错误导致报错提示。

https://gitee.com/mindspore/mindformers/issues/IAQ91D
这是关于使用mindformers进行chatglm36b模型微调遇到问题的报告类型issue。主要涉及的对象是mindformers工具及相关配置和数据格式。用户可能无法成功运行大模型微调的原因可能是配置参数设置不正确或环境搭建存在问题。

https://gitee.com/mindspore/mindformers/issues/IAQ2WX
这是一个bug报告，问题涉及的主要对象是alpaca数据集包，由于alpaca_data.json文件已经不存在，导致用户无法访问数据集包。

https://gitee.com/mindspore/mindformers/issues/IAQ2HC
这是一个bug报告类型的issue，主要涉及MindFormers大模型套件中AutoClass使用教程链接404的问题。

https://gitee.com/mindspore/mindformers/issues/IAQ2GL
这是一个bug报告类型的issue，主要涉及的对象是MindFormers大模型套件中Trainer API的使用教程链接失效。原因可能是链接地址错误或者页面被删除导致用户无法访问教程内容。

https://gitee.com/mindspore/mindformers/issues/IAPCHJ
这是一个bug报告，主要涉及到使用mindformers单卡推理baichuan2_7B时报错 rtKernelLaunchWithHandleV2 execute failed。由于某种原因导致这样的报错。

https://gitee.com/mindspore/mindformers/issues/IAPB5U
这是一个bug报告类型的issue，主要涉及从openmind下载qwenvl模型时出现网络通讯问题，导致下载失败。

https://gitee.com/mindspore/mindformers/issues/IAP74T
这个issue是关于bug报告，涉及的主要对象是Mixtral8x7B模型的双机训练配置问题，由于默认配置导致双机16卡出现OOM错误。

https://gitee.com/mindspore/mindformers/issues/IAP252
这是一个bug报告，涉及的主要对象是Qwen1.5。原因可能是推理后输出重复的问题导致了bug。

https://gitee.com/mindspore/mindformers/issues/IAOD0J
这是一个bug报告，涉及的主要对象是使用clip对visa数据集进行训练时出现的错误。这个问题可能是由于numpy版本或依赖库的兼容性问题导致的。

https://gitee.com/mindspore/mindformers/issues/IAOBUN
这是一个关于用户提出需求的issue，主要对象是计划适配mt5模型。

https://gitee.com/mindspore/mindformers/issues/IAO74M
这是一个bug报告，主要涉及Mindformers项目中单机多卡推理过程中出现的报错。由于yaml文件配置不当，导致程序报错提示缺少数据集文件。

https://gitee.com/mindspore/mindformers/issues/IANTS8
这个issue类型是bug报告，主要涉及鹏城盘古13B大模型预测时内存爆满的问题。由于该模型占用内存远超同等参数类型，导致了内存溢出的症状。

https://gitee.com/mindspore/mindformers/issues/IANO2I
这是一个bug报告，该问题涉及MA上多机多卡时会少保存几个checkpoint_network，可能是由于网络配置或代码逻辑错误导致。

https://gitee.com/mindspore/mindformers/issues/IANHNN
这是一个bug报告，涉及qwen模型在单机8卡训练时出现内存不足错误的问题。造成这个问题的原因可能是配置文件中的内存设置不正确。

https://gitee.com/mindspore/mindformers/issues/IANDLE
这是一个针对predict过程中文件路径问题的bug报告，主要涉及mindformers下的QwenVL模型。由于文件路径设置错误导致无法正常进行预测。

https://gitee.com/mindspore/mindformers/issues/IANBD9
这是一个bug报告，涉及的主要对象是代码中的数据预处理函数`alpaca_data_preprocess.py`。原因是在`preprocess`函数中，当输入的`token_ids`长度等于`seq_length`时，却被执行了`token_ids += tokenizer.encode(eos_token, add_special_tokens=False)`操作，导致最终数据长度超过预期。

https://gitee.com/mindspore/mindformers/issues/IAN5HK
这是一个bug报告，主要涉及mindformers中chat_web模块运行时出现数值类型错误导致数值处理异常的问题。

https://gitee.com/mindspore/mindformers/issues/IAN4AR
这是一个bug报告，用户提到在使用910B服务器执行指定代码时只支持CP而不支持Ascend，导致了问题。

https://gitee.com/mindspore/mindformers/issues/IAMTUV
这是一个bug报告，涉及对象是GLM3全参微调，由于某些原因导致了出现RuntimeError报错信息。

https://gitee.com/mindspore/mindformers/issues/IAMS70
这是一个用户需求咨询类的issue，主要涉及的对象是mamba模型，用户在询问该模型是否已经适配以及寻求对标模型的推荐。

https://gitee.com/mindspore/mindformers/issues/IAMKI9
这是一个bug报告，主要涉及mindformers中的qwen14b模型微调后推理结果输出重复信息。由于微调后推理结果出现异常、重复信息，请求处理该问题。

https://gitee.com/mindspore/mindformers/issues/IAMIO0
这是一个bug报告，主要涉及QwenVL单卡微调过程中出现的错误，可能由于参数设置不正确导致训练过程报错。

https://gitee.com/mindspore/mindformers/issues/IAMGS2
这是一个bug报告，涉及到mindformers项目下的一个卡训练出错的问题，可能是由于rank下面的日志导致无法拉起训练。

https://gitee.com/mindspore/mindformers/issues/IAMF1N
这个issue是一个bug报告，主要涉及对象是将qwen27BInstruction转为ckpt权重时出现的报错。由于输入数据类型错误导致Tensor类型异常，触发了TypeError。

https://gitee.com/mindspore/mindformers/issues/IAMDKW
这是一个bug报告，涉及hccl_tools.py运行失败，原因是在读取设备ip时发生了KeyError。

https://gitee.com/mindspore/mindformers/issues/IAM2N2
这是一个用户提出需求的issue，主要对象是适配llavanext110b或其他千亿级参数的多模态模型。用户提出了需要了解MindFormers是否有计划适配这些模型的问题。

https://gitee.com/mindspore/mindformers/issues/IALVWK
这是一个bug报告，针对llama27b在设置use_past=True时进行推理时出现Segmentation fault错误。

https://gitee.com/mindspore/mindformers/issues/IALUY8
这是一个bug报告，涉及的主要对象是mindformers下的visualglm模型。由于embedding_table未出现在图中，导致了Segmentation fault错误。

https://gitee.com/mindspore/mindformers/issues/IALT36
这是一个bug报告类型的issue，主要涉及的对象是当前数据缓存服务在多节点中的使用。由于数据缓存服务在多节点中开启时出现了未找到session的报错，导致用户提出了关于是否支持多节点的数据缓存的问题。

https://gitee.com/mindspore/mindformers/issues/IALNFH
这是一个Bug报告，涉及的主要对象是在使用MindSpore版本2.2.14时，在执行推理过程中出现了TypeError错误，缺少了'dtype'位置参数。

https://gitee.com/mindspore/mindformers/issues/IALMKI
这是一个bug报告，主要涉及的对象是运行qwen1.5的lora训练时报错。由于配置了RANK_TABLE_FILE但报卡间通信问题，导致出现报错。

https://gitee.com/mindspore/mindformers/issues/IALDL1
这是一个bug报告，涉及Qwen1.57B部署基于高阶接口的单卡推理报错，可能由于系统环境和版本不匹配导致。

https://gitee.com/mindspore/mindformers/issues/IALDJZ
这是一个bug报告类型的issue，主要涉及推理速度较慢问题，可能由于设备或软件版本不匹配导致。

https://gitee.com/mindspore/mindformers/issues/IAKU8K
这是一个bug报告，涉及Mindformers的LoRA模型微调失败的问题。原因可能是模型运行环境配置或参数设置不正确导致报错。

https://gitee.com/mindspore/mindformers/issues/IAKRS4
这是一个bug报告，涉及mindformers0.8在混合并行时出现的报错问题。问题可能由于不同并行方式同时开启导致，具体原因需进一步分析。

https://gitee.com/mindspore/mindformers/issues/IAKD6U
这是一个bug报告类型的issue，主要涉及的对象是"qwen2 72B web chat"。可能是由于部署问题导致无法正常使用。

https://gitee.com/mindspore/mindformers/issues/IAKBY0
这是一个Bug报告，主要涉及MindFormer中的glm3进行训练和微调时报错的问题，原因是'ChatGLM3Tokenizer'对象没有 'build_prompt'属性。

https://gitee.com/mindspore/mindformers/issues/IAK9U2
这是一个bug报告类型的issue，主要涉及的对象是在910b上运行QwenVL9.6b的全参微调。导致该bug的原因可能是运行环境配置或代码实现的问题。

https://gitee.com/mindspore/mindformers/issues/IAK8II
这是一个bug报告类型的issue，主要涉及mindformers下的qwen27b推理精度问题，由于use_past参数关闭后导致推理生成文本无法对齐及乱码。

https://gitee.com/mindspore/mindformers/issues/IAK730
这是一个bug报告，涉及到使用910A推理qwen27B时出现的加载权重报错，可能是由于模型参数不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/IAK0AV
这是一个bug报告，主要涉及到安装基于已适配的flash attention包后开启Flash Attention特性仍然报错的问题。由于可能是版本适配或类型转换错误导致的RuntimeError，用户寻求解决该错误并希望适配高版本mindspore的建议。

https://gitee.com/mindspore/mindformers/issues/IAJYW1
这是一个 bug 报告，用户在使用微调后的分布式权重进行 chat_web 推理时出错的问题。

https://gitee.com/mindspore/mindformers/issues/IAJTFS
这是一个bug报告，涉及到物理机使用run_chat_server.py推理llama2_13b双卡启动推理报错。原因可能是配置或运行步骤中出现了错误。

https://gitee.com/mindspore/mindformers/issues/IAJCRE
这是一个bug报告，涉及mindformers下的lite推理GPT2报错。可能是由于lite推理执行指令中的参数配置或模型路径设置不正确所导致的问题。

https://gitee.com/mindspore/mindformers/issues/IAJ8QD
这是一个bug报告类型的issue，涉及到MindFormers下的LLama3多卡推理。由于硬件及软件环境配置等原因，导致无法正常执行多卡推理操作。

https://gitee.com/mindspore/mindformers/issues/IAJ0Z2
这是一个bug报告，涉及的主要对象是静态图基础接口。由于mindspore的ops.outer算子产生误差，对应megatron算子为torch.outer，导致精度未100%对齐的问题。

https://gitee.com/mindspore/mindformers/issues/IAIRMF
这是一个用户提出需求的issue，主要涉及mindformers如何将cpkt或者bin文件转成safetensors文件。由于缺乏转成safetensors的脚本工具，用户无法完成相应的文件转换操作。

https://gitee.com/mindspore/mindformers/issues/IAIKWC
这是一个bug报告，用户在使用两张Ascend910B卡训练qwen1_5模型时遇到内存不足的问题。

https://gitee.com/mindspore/mindformers/issues/IAIDMA
这是一个用户请求获取model与tokenizer的issue，涉及到Qwen2ForCausalLM模型和Qwen2Tokenizer。由于未能正确获取model与tokenizer，用户提出了获取这两个对象的问题。

https://gitee.com/mindspore/mindformers/issues/IAHPEL
这是一个bug报告，涉及的主要对象是使用MindFormers提供的QwenVL推理命令行的用户。由于在单卡和多卡推理时均出现报错，可能由于程序错误或环境配置问题导致的Linux段错误。

https://gitee.com/mindspore/mindformers/issues/IAH8WR
这是一个bug报告，主要涉及在910B上对mixtral8x7B进行4卡Lora微调时出现BrokenPipeError报错。

https://gitee.com/mindspore/mindformers/issues/IAH68A
这是一个bug报告，涉及主要对象是mindformers中的权重转换功能。由于输入数据类型错误导致了转换权重时出现报错异常。

https://gitee.com/mindspore/mindformers/issues/IAH5UN
这是一个bug报告，涉及"qwen2 72B"的部署推异常。可能是由于部署过程中的某些原因导致了异常情况。

https://gitee.com/mindspore/mindformers/issues/IAH0UX
这是一个bug报告，涉及主要对象为MindSpore预训练模型qwen1.5，在特定环境下预训练报错。原因可能是执行模式选择错误导致报错。

https://gitee.com/mindspore/mindformers/issues/IAGAZ9
这是一个bug报告，主要涉及QWEN7B模型在进行LORA微调后推理结果不佳的问题。

https://gitee.com/mindspore/mindformers/issues/IAG5RN
这个issue类型是用户需求，主要涉及的对象是部署名称为qwen2的72版本，用户想知道是否有其他人已经成功部署了这个版本。

https://gitee.com/mindspore/mindformers/issues/IAFXB0
这是一个bug报告，涉及的主要对象是mindformers动态组网模块。这个问题可能是由于内存资源耗尽导致的报错。

https://gitee.com/mindspore/mindformers/issues/IAFX8M
这是一个bug报告，主要涉及mindformers 1.0版本在ModelArts云服务器上进行Blip2模型的二阶段训练时出现的内存池不足的报错。

https://gitee.com/mindspore/mindformers/issues/IAFX7E
这是一个bug报告，该问题涉及的主要对象是llama3.1全参微调，由于部分代码调用错误导致程序报错。

https://gitee.com/mindspore/mindformers/issues/IAFWNQ
这是一个bug报告，涉及到mindformers/research仓库的README内容未及时更新的问题，由于未更新Baichuan和Interlm支持模型信息导致用户提出更新请求。

https://gitee.com/mindspore/mindformers/issues/IAFV17
这是一个用户提出需求的issue，主要涉及对象是qwen2，用户在询问qwen2是否需要切分重权以及是否有相关的配置文件。

https://gitee.com/mindspore/mindformers/issues/IAFTSH
这是一个bug报告类型的issue，涉及到策略文件生成过程。由于一些代码错误导致了程序出现了错误的异常情况。

https://gitee.com/mindspore/mindformers/issues/IAFPSB
这是一个bug报告，涉及到生成rank文件时出现报错的问题。可能是由于输入图片的某些特性或配置错误导致的。

https://gitee.com/mindspore/mindformers/issues/IAFN2C
这是一个bug报告，主要涉及到用户在运行mindformers中的推理模型时出现了错误。由于模块导入错误，导致了该bug的症状。

https://gitee.com/mindspore/mindformers/issues/IAFGYZ
这个issue是关于bug报告的，主要涉及到Qwen1.5 72B 910A模型在处理带有复杂格式的prompt时出现推理精度问题。

https://gitee.com/mindspore/mindformers/issues/IAFFNR
这是一个用户报告的issue，主要涉及Hugging Face在国内无法正常下载模型权重，导致无法访问mindformers和modellink提供的下载链接。

https://gitee.com/mindspore/mindformers/issues/IAFED1
这是一个bug报告，用户在Ascend910A上微调VisualGLM模型时出现了问题。

https://gitee.com/mindspore/mindformers/issues/IAFAVU
这是一个bug报告，涉及到HTTP 500错误导致无法拉取镜像，用户需要帮助解决这个问题。

https://gitee.com/mindspore/mindformers/issues/IAF961
这是一个bug报告，涉及到mindformers1.1.0版本和ms2.2.14版本的推理时报错问题。可能是由于命令执行错误导致图片上传中断，需要用户输入图片说明解决。

https://gitee.com/mindspore/mindformers/issues/IAEQ5J
这是一个Bug报告类型的issue，主要涉及MindSpore在使用mindformers拉起训练时报错"TbeInitialize running failed"。可能是由于环境配置或软件版本等原因导致的问题。

https://gitee.com/mindspore/mindformers/issues/IAEPMN
这是一个bug报告，涉及GLM3全参微调报错，可能由于代码执行过程中出现了异常导致。

https://gitee.com/mindspore/mindformers/issues/IAEN5W
这是一个bug报告，涉及的主要对象是Mindformer的sam示例。由于aicore利用率为0，导致推理速度较慢。

https://gitee.com/mindspore/mindformers/issues/IAE6A8
这是一个bug报告，涉及的主要对象是Atlas 300I Pro推理卡和MindSpore/MindFormers软件，导致报错的原因可能是qwen7b推理时出现了错误。

https://gitee.com/mindspore/mindformers/issues/IAE5LC
这是一个用户提出需求的issue，主要涉及的对象是适配qwen2。用户提出这个问题是因为希望用中国的计算卡跑qwen2的7b和72b，并表示对1.5B和0.5B的适配没有那么大需求。

https://gitee.com/mindspore/mindformers/issues/IADRHG
这是一个bug报告类型的issue，主要涉及软件环境下ChatGLM3推理报错的问题，可能由于软件版本不匹配或配置错误导致了推理错误。

https://gitee.com/mindspore/mindformers/issues/IADPJJ
这是一个bug报告类型的issue，主要涉及到glm6b模型微调过程中出现的报错。可能是由于模型微调过程中的某些操作或参数设置不正确导致的问题。

https://gitee.com/mindspore/mindformers/issues/IADFL0
这是一个用户提出配置文档错误的问题，涉及的主要对象是MindFormers项目。原因可能是配置文档中自动并行策略未按照MindSpore推荐配置。

https://gitee.com/mindspore/mindformers/issues/IACWOP
这是一个bug报告，主要涉及的对象是在Atlas8009000上运行glm3全参微调时出现的E30008错误，可能是由于AI CPU操作执行超时导致。

https://gitee.com/mindspore/mindformers/issues/IACW8M
这是一个bug报告，涉及mindformers单机多卡运行时报错的问题，可能是由于配置或脚本错误导致。

https://gitee.com/mindspore/mindformers/issues/IACO88
这个issue是一个bug报告，主要涉及代码中的转换函数出现了错误，导致一些参数名未能正确替换。

https://gitee.com/mindspore/mindformers/issues/IACKM3
这是一个用户提出需求的类型，用户询问是否有计划适配cogvlm这个多模态模型。

https://gitee.com/mindspore/mindformers/issues/IACGSQ
这是一个bug报告，主要涉及的对象是mindformers_qwen1.5 7B的微调训练。由于OOM（Out of Memory）导致服务器宕机。

https://gitee.com/mindspore/mindformers/issues/IACETX
这是一个Bug报告，主要涉及Llama3中的Ascend功能，用户遇到了EZ1001错误提示self not implement for DT_BFLOAT16，可能是由于数据类型不支持而导致的。

https://gitee.com/mindspore/mindformers/issues/IACDI4
这个issue类型为文档问题报告，主要涉及use_clip_grad配置在readme中的层级关系描述错误。原因可能是文档内容描述不清导致用户无法正确设置相关配置。

https://gitee.com/mindspore/mindformers/issues/IACDI5
这是一个bug报告，涉及的主要对象是MindForers项目中的training_args.py文件。导致这个问题的原因是参数use_clip_grad的默认值和描述不一致。

https://gitee.com/mindspore/mindformers/issues/IACCOA
这是一个bug报告，主要对象涉及MindFormers的分布式推理功能。由于变量内存设置超出有效范围导致Ascend内核运行初始化失败。

https://gitee.com/mindspore/mindformers/issues/IACCBT
这是一个bug报告，涉及对象是代码中权重切分报错，可能由于NPU驱动版本和NPU固件版本不匹配导致报错。

https://gitee.com/mindspore/mindformers/issues/IAC9AA
这是一个bug报告，主要对象是Mindformers中的权重合并过程，由于输入维度不匹配导致出现了MatMul运算错误。

https://gitee.com/mindspore/mindformers/issues/IAC6RJ
这是一个bug报告，主要涉及对象是mindspore包中的hal属性，由于属性异常导致出现bug。

https://gitee.com/mindspore/mindformers/issues/IAC0QZ
这是一个bug报告，主要涉及文档手册中的错误，提出了关于权重转换、百川7B文档描述错误以及依赖软件torch和transformer的问题。

https://gitee.com/mindspore/mindformers/issues/IABYPR
这是一个bug报告，涉及mindspore的分布式任务在调度时只在0号卡上分配的问题，疑似参数配置错误导致分布式任务未成功分配显卡。

https://gitee.com/mindspore/mindformers/issues/IABVQ6
这是一个bug报告类型的issue，主要涉及mindformer_book.py中缺少llama2_7b_lora和llama2_13b_lora模型及配置，导致微调时调用Trainer类接口出错。

https://gitee.com/mindspore/mindformers/issues/IABQVV
这是一个bug报告，主要涉及mindformers 1.1.0 版本在执行推理任务时出现的报错问题，可能由于HCCL初始化模式与当前执行模式不匹配而导致报错。

https://gitee.com/mindspore/mindformers/issues/IABNJK
这是一个bug报告，涉及Mindspore进行baichuan2_7b模型数据集预处理时运行belle_preprocess.py脚本报错的问题。可能是由于环境配置或代码错误导致的报错。

https://gitee.com/mindspore/mindformers/issues/IABJBZ
这是一个bug报告，该问题涉及mindspore中离线权重切分功能，由于模型并行数量设置不正确导致生成的策略在检测时报错。

https://gitee.com/mindspore/mindformers/issues/IABCDB
这是一个bug报告，涉及对象为在CANN版本7.0.0上使用mindformers进行lora微调后模型推理时出现重复问题。可能由于某种原因导致模型推理出现重复现象。

https://gitee.com/mindspore/mindformers/issues/IAB412
这是一个bug报告，主要涉及对象是使用mindformers库时下载预训练权重失败导致模型加载错误的问题。

https://gitee.com/mindspore/mindformers/issues/IAAYYN
这是一个bug报告，涉及到MindFormers项目中的权重文件下载和指定问题，用户提出了执行单卡推理时指定了已有的权重路径，但程序重新下载权重文件的疑问。

https://gitee.com/mindspore/mindformers/issues/IAAWAZ
这个issue属于bug报告类型，涉及的主要对象是llama3在910A设备上的运行情况。由于使用指定版本的mindformers、mindspore和cann在910A设备上运行llama3时出现错误，用户询问这个版本的Llama3是否可以在该设备上运行。

https://gitee.com/mindspore/mindformers/issues/IAAQ6R
这是一个bug报告，涉及mindspore版本升级后出现的代码缺失和编译错误。

https://gitee.com/mindspore/mindformers/issues/IAAJ8P
这是一个bug报告，主要涉及的对象是glm3推理功能。由于未知原因导致程序偶发性报错，即使配置参数及资源未满也无法正常运行。

https://gitee.com/mindspore/mindformers/issues/IAAIZM
这是一个bug报告，涉及的主要对象是数据加载器（data_loader）。由于用户在使用MultiSourceDataLoader时未传入train_dataset_dir参数，导致程序报错。

https://gitee.com/mindspore/mindformers/issues/IAAGWT
这是一个bug报告，涉及到Qwen + Lora微调推理结果不符合预期的问题。可能由于Qwen_14b + Lora微调推理出现错误，导致结果与输入内容相同。

https://gitee.com/mindspore/mindformers/issues/IA9YCL
这是一个bug报告，主要涉及GPU单机四卡推理Llama2 Mul需要进行广播，导致出现了问题。

https://gitee.com/mindspore/mindformers/issues/IA9L6X
这是一个bug报告，涉及到"llama2"经过lora训练后权重转换bin出现问题，导致转换后出现无法识别的权重层。

https://gitee.com/mindspore/mindformers/issues/IA9JG7
这是一个bug报告，涉及主要对象为mindformers 1.0软件配置，由于显卡内存溢出OOM导致推理 qwen1.5_72B 报错。

https://gitee.com/mindspore/mindformers/issues/IA99IU
这是一个bug报告，涉及对象为llama213b两节点全参微调，报错现象是在指定运行环境下出现错误。

https://gitee.com/mindspore/mindformers/issues/IA99FN
这是一个bug报告，涉及到Baichuan213B模型在微调后推理时不写入checkpoint_name_or_path导致推理效果不佳的问题。

https://gitee.com/mindspore/mindformers/issues/IA99E7
这是一个bug报告类型的issue，涉及到CANN版本、mindie版本、Python版本、torch版本以及硬件atlas 800T A2，由于json数据类型错误导致了RuntimeError异常。

https://gitee.com/mindspore/mindformers/issues/IA94VD
这是一个bug报告，涉及mindformers的模型推理过程中出现乱码的问题，可能是由于权重加载报错导致的。

https://gitee.com/mindspore/mindformers/issues/IA8Z2Z
这是一个bug报告，主要涉及mindformers1.1中微调baichuan2_7b安装文档报错的问题。原因可能是环境配置或脚本参数设置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/IA8ULX
这个issue类型是用户提出需求，主要对象是单卡推理和分布式推理的主要步骤和区别，用户提出了关于这两种推理方式的问题或寻求帮助。

https://gitee.com/mindspore/mindformers/issues/IA8ULF
该issue类型为疑问提出，主要涉及“增量推理”和“自回归推理”，探讨它们的区别和适用场景。

https://gitee.com/mindspore/mindformers/issues/IA8UB8
这是一个用户提出需求的类型issue，主要涉及将huggingface的torch模型转换成mindspore支持的模型。这个问题的原因是用户想要在mindspore中使用由huggingface训练的模型，需要进行模型格式的转换。

https://gitee.com/mindspore/mindformers/issues/IA8UAJ
这个issue类型是用户提出需求，问题主要涉及mindformers中如何实现RLHF微调，由于用户对于这一功能实现方式不清楚而提出了疑问。

https://gitee.com/mindspore/mindformers/issues/IA8UA6
这是一个用户提出需求的类型，主要对象是mindformers支持的数据集格式，用户想了解mindformers支持哪些数据集格式。

https://gitee.com/mindspore/mindformers/issues/IA8U8Q
这个issue是关于如何在chat_web中运行多机多卡推理的问题，属于用户提出需求且寻求帮助类型。

https://gitee.com/mindspore/mindformers/issues/IA8U7D
这是一个询问问题类型的issue，主要涉及对象是chat_web模块加载research模型，用户可能遇到无法正常加载模型的问题。

https://gitee.com/mindspore/mindformers/issues/IA8U6K
这是一个用户询问是否有关于chatweb的说明文档的问题，属于用户提出疑问类型的issue。该问题主要涉及chatweb的说明文档缺失。

https://gitee.com/mindspore/mindformers/issues/IA8U51
这是一个用户提出问题的issue，主要对象是chatweb，由于用户想了解chatweb是否支持并发。

https://gitee.com/mindspore/mindformers/issues/IA8U3A
这是一个用户提出需求的类型，用户想要了解如何区分configs中配置的yaml文件。这个问题的根本原因在于缺乏清晰的文件区分指南导致用户难以确定何时应该使用哪个yaml配置文件。

https://gitee.com/mindspore/mindformers/issues/IA8U2S
这是一个用户提出问题的issue，主要涉及的对象是代码库中的两个参数load_checkpoint和checkpoint_name_or_path，用户想要了解它们之间的区别。

https://gitee.com/mindspore/mindformers/issues/IA8TXP
该issue属于用户询问问题的类型，主要涉及对象是设备910a和Mixtral8x7B，用户想知道是否可以在910a上运行Mixtral8x7B。

https://gitee.com/mindspore/mindformers/issues/IA8REI
这是一个bug报告，主要涉及mindformers在modelarts 910B上推理不完整的问题，可能由于适配问题导致。

https://gitee.com/mindspore/mindformers/issues/IA8ORK
这是一个bug报告，涉及到基于mindspore/mindformers项目中Chat_Web模块封装推理服务时出现的错误。由于配置错误导致运行run_chat_server.py时报错，仅第一张npu被占用。

https://gitee.com/mindspore/mindformers/issues/IA8J0W
这是一个关于设备兼容性的问题，用户想知道是否910b能够运行llama3 8b，若不能运行可能会导致应用程序无法正常工作。

https://gitee.com/mindspore/mindformers/issues/IA8J03
这是一个由用户提出的问题单，主要涉及对象是910B和qwen1.570b，用户想知道910B是否能够运行qwen1.570b。

https://gitee.com/mindspore/mindformers/issues/IA8IR8
这是一个bug报告，主要涉及mindformers框架下推理过程中权重加载报错的问题，可能是由于CANN版本和硬件不兼容导致的。

https://gitee.com/mindspore/mindformers/issues/IA8IPS
这是一个bug报告类型的issue，涉及主要对象为mindformers项目。由于yaml文件加载错误导致推理过程出现问题。

https://gitee.com/mindspore/mindformers/issues/IA8IOQ
这是一个bug报告，涉及mindformers在权重转换过程中报错的问题，可能由于版本不匹配或代码逻辑错误导致。

https://gitee.com/mindspore/mindformers/issues/IA8IN6
这是一个bug报告类型的issue，主要涉及到mindformers项目中的qwen1.5数据转换报错，可能是由于数据转换代码存在问题或者环境配置不兼容所导致的。

https://gitee.com/mindspore/mindformers/issues/IA8IFX
这是一个bug报告，涉及对象是拉取镜像时出现的网络请求失败，可能是由于镜像仓库地址无法访问导致的。

https://gitee.com/mindspore/mindformers/issues/IA8IFE
这是一个bug报告，涉及主要对象为Mindformers项目中的qwen1.5模块。报错现象由于硬件为800T A2卡，使用CANN版本 (7.0.0beta1)、mindformers版本(r1.0.0)、torch版本(2.3)、Python版本（3.9.19）运行时出现了报错。

https://gitee.com/mindspore/mindformers/issues/IA8I58
这个issue类型是用户提出需求，询问910A是否能够运行qwen2.0 70b。【注意：根据提供的信息无法进一步分析原因和症状】

https://gitee.com/mindspore/mindformers/issues/IA8G8A
这是一个硬件配置相关的问题，类型为bug报告。用户在使用高阶推理脚本时，由于内存占用过高而导致进程被杀，寻求帮助解决这一问题。

https://gitee.com/mindspore/mindformers/issues/IA8DX4
这是一个bug报告，涉及到mindformers中运行llama27b权重转换时出现报错的问题。可能是由于运行环境和参数配置不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/IA8DWT
这是一个bug报告类型的issue，涉及主要对象是yolov5模型训练过程中出现的错误信息。由于dcmi module初始化失败导致的报错信息EE9999和EL9999，需要查看npusmi info定位问题。

https://gitee.com/mindspore/mindformers/issues/IA8DWC
这是一个bug报告类型的issue，主要涉及到qwen1.5模型自己的数据转换报错，由于未确认具体原因导致了报错现象。

https://gitee.com/mindspore/mindformers/issues/IA8DVQ
这是一个bug报告类型的issue，主要涉及自动模型权重合并操作。出现问题的原因可能是代码中的参数传递导致生成的文件格式不正确。

https://gitee.com/mindspore/mindformers/issues/IA88YU
这是一个用户需求问题，涉及主要对象为在chatweb中添加qwen1.572B进行推理，由于需要在chat_web中进行推理千问1.5_72B，用户提出了如何实现chat_web的predict_process.py中的get_model和get_tokenizer的问题。

https://gitee.com/mindspore/mindformers/issues/IA86IF
这是一个bug报告，涉及多机多卡训练时权重自动转换目录识别失败的问题，需要在utils.py文件中修改代码才能进行下一步权重转换。

https://gitee.com/mindspore/mindformers/issues/IA868A
这是一个bug报告，主要涉及模型转换过程中的异常问题，用户在进行torch权重转mindspore权重时直接killed并无异常提示。

https://gitee.com/mindspore/mindformers/issues/IA7WZ0
这个issue类型是bug报告，涉及主要对象为模型微调过程中出现EI99999报错。这可能是由于训练到75%时发生了意外错误，导致模型训练无法继续进行。

https://gitee.com/mindspore/mindformers/issues/IA7WOF
这是一个bug报告，主要涉及到MindSpore进行推理时出现的错误EI40021，可能是由于软件版本或硬件问题导致的。

https://gitee.com/mindspore/mindformers/issues/IA7WN9
这是一个bug报告，涉及mindformers下的llama2 70b续训遇到fp32imm报错的问题。该问题由于升级到mindspore2.2.14和mf 1.0版本后，在续训过程中遇到了无法定位具体step及数据的报错。

https://gitee.com/mindspore/mindformers/issues/IA7WIX
这是一个bug报告，主要涉及mindformers仓库下的模型转换脚本的问题。由于内存分配问题，在执行转换模型脚本时出现了"cannot allocate memory in static TLS block"的错误。

https://gitee.com/mindspore/mindformers/issues/IA7WFW
这是一个bug报告，主要涉及mindformers下的千问1.5训练遇到attention_mask问题，可能是由于缺少attention_mask参数导致的错误。

https://gitee.com/mindspore/mindformers/issues/IA7WAX
这是一个bug报告类型的issue，涉及到ceval运行时遇到mul算子报错的问题，导致两个value不一致。

https://gitee.com/mindspore/mindformers/issues/IA7UT3
这是一个bug报告，涉及的主要对象是数据转化脚本 llame_preprocess.py，由于参数设置或者代码逻辑错误导致数据转化时出现报错。

https://gitee.com/mindspore/mindformers/issues/IA7T83
这是一个bug报告issue，主要涉及到codellama ckpt文件转换失败的问题，可能是由于缺少output文件夹导致。

https://gitee.com/mindspore/mindformers/issues/IA7R12
这是一个向开发者请教问题的issue，主要涉及的对象是300I DUO和GLM36B推理。用户问询在300I DUO上是否可以使用mindformers跑GLM3的推理，或者使用哪个仓库来运行大模型推理，问题可能是由于未知的兼容性或配置问题导致。

https://gitee.com/mindspore/mindformers/issues/IA7PHK
这个issue类型是性能优化，涉及的主要对象是mindformers中的模型训练过程。原因是修改`gradient_aggregation_group`参数后，训练速度显著提升，说明之前参数设置影响了训练效率。

https://gitee.com/mindspore/mindformers/issues/IA7NVX
这是一个bug报告，涉及mindformers下的callback.py中print_output_info函数的输出小数位问题，由于小数位只有3位，当loss较小时会导致输出的loss全为0。

https://gitee.com/mindspore/mindformers/issues/IA7CBA
这是一个bug报告类型的issue，涉及mindformers库中的qwen1.5数据转换功能，由于未确认具体原因导致转换自定义数据时出错。

https://gitee.com/mindspore/mindformers/issues/IA7C14
这是一个bug报告，涉及到算子编译过程中部分模块与numpy2.0版本不兼容的问题。

https://gitee.com/mindspore/mindformers/issues/IA77VT
这是一个bug报告，涉及MindSpore环境下模型微调过程中出现数值错误的问题。

https://gitee.com/mindspore/mindformers/issues/IA6ZYB
这个issue类型为bug报告，涉及主要对象是LLama13b设备，由于同时开启pp和lora配置导致通信超时问题。

https://gitee.com/mindspore/mindformers/issues/IA6YCN
这是一个bug报告，涉及的主要对象是MindSpore在Ascend910A设备上的执行模式问题。由于存在动态shape，在编译时挂掉导致了这个问题。

https://gitee.com/mindspore/mindformers/issues/IA6VFG
这是一个bug报告，主要涉及Baichuan213B模型的GPU上训练配置文件参数缺失问题，导致用户寻求关于参数对应和配置方式的帮助。

https://gitee.com/mindspore/mindformers/issues/IA6VAC
这是一个bug报告，问题涉及到Mindformer1.0服务器训练时突然中断并报错，提示"EE9999,EL9999 halQueQueryInfo failed:deviceId=7, qid=5,drvRetCode=1"，以及执行“npusmi info”提示"dcmi module initialize failed. ret is 8005"。原因可能是设备初始化或驱动模块问题导致的异常现象。

https://gitee.com/mindspore/mindformers/issues/IA6V83
这是一个bug报告，主要涉及安装mindspore过程中出现的错误。由于PIL库中不存在'ANTIALIAS'属性，导致报错。

https://gitee.com/mindspore/mindformers/issues/IA6V0S
这个issue是bug报告，涉及主要对象是安装MindSpore时出现的undefined symbol错误，可能是由于依赖库版本问题导致。

https://gitee.com/mindspore/mindformers/issues/IA6UZZ
这是一个bug报告，主要涉及到mindformers版本1.0.0中的chat_web/run_chat_server.py服务。由于并发请求模型导致流式输出结果互相串。

https://gitee.com/mindspore/mindformers/issues/IA6UTY
这是一个bug报告，主要涉及GLM7B模型训练时出现卡间通信问题导致报错"Get gateway failed,because no gateway was preset there!"。

https://gitee.com/mindspore/mindformers/issues/IA6SVM
这个issue类型为技术支持问题，主要涉及对象是华为云的910b服务器和glm26b模型，用户询问能否在910b服务器上运行glm26b模型的全参微调。

https://gitee.com/mindspore/mindformers/issues/IA6SPA
这是一个bug报告类型的issue，涉及主要对象为执行qwen1_5_preprocess.py脚本时出错。原因可能是版本兼容性或代码逻辑错误导致无法正常执行。

https://gitee.com/mindspore/mindformers/issues/IA6R4L
这是一个bug报告，涉及的主要对象是在mindspore 2.2.14和mindformers 1.0环境下运行llama2 7b lora微调时出现的RuntimeError。造成这个问题的原因是operation 'adam_opt'中存在不受支持的语法，在图模式下会回退到Python解释器，而Python解释器不支持操作'adam_opt'。

https://gitee.com/mindspore/mindformers/issues/IA6NTY
这是一个关于模型训练过程中loss反复波动的bug报告，用户需求在于获取关于全参微调和lora微调的例子中loss曲线图作为参考。

https://gitee.com/mindspore/mindformers/issues/IA6JWB
这是一个bug报告，主要涉及到mindformers项目中的GenerateBatchStrategiesBySplitFlag函数。该问题的原因是Split_flag_list和inputs shape的大小不一致，导致了1 : 4的错误症状。

https://gitee.com/mindspore/mindformers/issues/IA6IXL
这是一个问题询问类型的issue，主要涉及对象是"altas 300I单卡"，用户想知道是否支持lora微调。

https://gitee.com/mindspore/mindformers/issues/IA6HT5
这是一个bug报告，主要涉及mindformers框架在运行时出现指针空指异常的问题。

https://gitee.com/mindspore/mindformers/issues/IA6ABC
这个issue类型为bug报告，涉及主要对象为Qwen7B模型微调，由于可能配置文件设置不正确导致报错。

https://gitee.com/mindspore/mindformers/issues/IA5VB3
这是一个bug报告，涉及将torch权重转换成mindformers权重时出现报错。由于转换处理过程中出现错误导致报错。

https://gitee.com/mindspore/mindformers/issues/IA5M77
这是一个bug报告，主要涉及在运行4卡微调时执行张量乘法时出现的形状不符合广播规则的问题。

https://gitee.com/mindspore/mindformers/issues/IA5LKU
这个issue类型是bug报告，涉及到llama2的lara文档指引路径有误。可能是由于路径配置错误导致用户无法找到正确的文档指引。

https://gitee.com/mindspore/mindformers/issues/IA5LI5
这是一个关于需求的问题单类型，主要对象为文档设计不合理，需要区分开llama和llama2的模型关联文件。原因可能是现有文档未清晰标明两者的区分导致混淆和困惑。

https://gitee.com/mindspore/mindformers/issues/IA5IJJ
这是一个请教问题类型的issue，涉及到EpochCtrlOp线程数设置，由于用户对算子间队列关系不清楚而提出。

https://gitee.com/mindspore/mindformers/issues/IA5I0W
这是一个bug报告，主要涉及到"qwen1.514B chat权重转换问题"。问题可能由于权重转换不正确导致某些功能无法正常工作。

https://gitee.com/mindspore/mindformers/issues/IA5HX9
这是一个用户提出需求的类型，主要涉及多卡推理的问题。原因可能是使用Python脚本调用多卡推理导致部分卡进程异常死亡。

https://gitee.com/mindspore/mindformers/issues/IA5GDD
这个issue是一个bug报告，主要涉及mindformers中的使用PIPELINE分布式训练。由于mindspore内置模型训练速度较慢，用户尝试使用自定义的训练套路结合gpt2的transformer部分进行训练，但在使用pipeline时遇到了问题。

https://gitee.com/mindspore/mindformers/issues/IA5E4C
这是一个bug报告，涉及主要对象为一个名为"冒烟"的功能，用户遇到了无法重新启动的问题。

https://gitee.com/mindspore/mindformers/issues/IA56XB
这个issue类型为寻求帮助，主要涉及初学者在安装过程中未找到作用指引的问题。

https://gitee.com/mindspore/mindformers/issues/IA56L0
这是一个bug报告类型的issue，涉及的主要对象是GLM36B训练无法调用NPU。由于环境中缺少必要的软件依赖，导致无法启动GLM36B训练，出现了该问题。

https://gitee.com/mindspore/mindformers/issues/IA559F
这是一个类型为bug报告的issue，主要涉及到将huggingface下载的pytorch模型转换为mindspore时出现报错ImportError，可能是由于内存无法在静态TLS块中分配导致的。

https://gitee.com/mindspore/mindformers/issues/IA51P3
这是一个bug报告，涉及MindSpore在Ascend910A上推理时出现乱码的问题。可能由于模型转换或代码执行过程中的某些问题导致了输出乱码的现象。

https://gitee.com/mindspore/mindformers/issues/IA4W2A
这个issue是关于bug报告的，涉及到GLM26B在910A上Lora微调执行脚本run_standalone.sh报错的问题，可能是由于传入参数不符合要求导致的。

https://gitee.com/mindspore/mindformers/issues/IA4TR0
这是一个bug报告，涉及的主要对象是LLaMa7b模型推理过程。由于文件'kernel_operator.h'未找到，导致在LLaMa7b模型进行完整权重切分为2卡分布式推理时出现了fatal error。

https://gitee.com/mindspore/mindformers/issues/IA4T6I
这是一个bug报告类型的issue，主要涉及到拼写错误。由于单词"!satge"拼写错误，导致了此问题的产生。

https://gitee.com/mindspore/mindformers/issues/IA4QJ1
这是一个bug报告，主要涉及MindSpore / mindformers仓库中BLIP2模型文档未提供二阶段推理模型权重。这个问题可能是由于文档遗漏或者权重文件未正确添加而导致用户无法找到二阶段推理模型权重。

https://gitee.com/mindspore/mindformers/issues/IA4PPT
这是一个bug报告，主要涉及训练模型过程中出现的TypeError错误。造成该错误的原因可能是选择了无效的内核信息导致的。

https://gitee.com/mindspore/mindformers/issues/IA44ZO
这是一个bug报告类型的issue，主要涉及MindForger项目下的deepseek33b推理报错问题。由于权重文件转换等原因导致的推理报错。

https://gitee.com/mindspore/mindformers/issues/IA3URH
这是一个bug报告，主要涉及的对象是mindformers1.1_mindspore2.3rc2:20240511镜像，报错可能由于参数dtype不支持BFLOAT16类型而导致。

https://gitee.com/mindspore/mindformers/issues/I9W29F
这是一个用户提出需求的问题单，主要对象是GLM4库，用户在询问GLM4适配的时间。

https://gitee.com/mindspore/mindformers/issues/I9W1RX
这是一个bug报告，主要涉及llama27b转换权重时出错。可能是由于路径或文件错误导致转换权重的过程中出现了报错。

https://gitee.com/mindspore/mindformers/issues/I9VWSI
这是一个bug报告，主要涉及mindformers项目中QwenConfig类的注册问题，可能由于缺少对应的配置文件路径或者配置类名称错误导致这个bug。

https://gitee.com/mindspore/mindformers/issues/I9VW31
这是一个bug报告，主要涉及Atlas800T A2的运行时报错问题，由于环境变量和Python路径配置问题导致无法导入Python模块。

https://gitee.com/mindspore/mindformers/issues/I9VVUX
这是用户提出需求的类型，主要涉及到对于支持计划和在特定硬件上进行训练、微调、推理的问题。

https://gitee.com/mindspore/mindformers/issues/I9VUSM
这是一个bug报告，涉及主要对象为mindformers/tools/cloud_adapter/cloud_adapter.py文件。导致这个bug的原因是在文件中缺少导入moxing as mox模块，从而导致无法识别'mox'变量。

https://gitee.com/mindspore/mindformers/issues/I9VUHZ
这是一个bug报告，主要涉及执行模式设置和启动方式问题，由于执行模式为kernelbykernel模式，要求使用OpenMPI或Dynamic Cluster启动进程，导致出现错误提示。

https://gitee.com/mindspore/mindformers/issues/I9VQZ3
这是一个bug报告，涉及mindformers在使用AICC训练任务训练时出现的问题，主要表现为日志打印信息提示等待文件生成的情况，重试多次后才生成。

https://gitee.com/mindspore/mindformers/issues/I9VM05
这是一个bug报告，涉及mindspore仓库中baichuan2推理过程中对话推理结果异常的问题。可能由于模型设置或代码逻辑错误导致对话推理结果错乱。

https://gitee.com/mindspore/mindformers/issues/I9VKYO
这是一个bug报告，涉及的主要对象是mindformers中经过lora训练后推理结果混乱。由于终止符未生效导致推理结果出错。

https://gitee.com/mindspore/mindformers/issues/I9V87Q
这是一个bug报告类型的issue，主要涉及 ModelArts 上运行 qwen1_5_72b 在 910B 8卡上推理时编译报错。产生此问题的原因是调用 GE CompileGraph 失败，导致报错。

https://gitee.com/mindspore/mindformers/issues/I9V70S
这是一个bug报告类型的issue，主要涉及GLM3全参微调过程中loss直接变为0的问题。可能是由于模型训练过程中的某些设置或参数配置不正确，导致loss无法正确计算。

https://gitee.com/mindspore/mindformers/issues/I9V671
这是一个bug报告，涉及到mindspore和mindformers中的filewriter.py文件，问题可能是由于代码版本不匹配或者调用出错导致的。

https://gitee.com/mindspore/mindformers/issues/I9UXRG
这是一个bug报告，主要涉及的对象是在进行qwen72B全量微调时的Building model阶段耗时过长。导致这个问题可能是由于程序在该阶段卡住没有进入训练。

https://gitee.com/mindspore/mindformers/issues/I9UON5
这是一个bug报告，主要涉及到Blip2二阶段推理问题。由于未改动代码和配置文件，导致一阶段结果正确但二阶段结果错误，用户想知道这种情况出现的原因。

https://gitee.com/mindspore/mindformers/issues/I9UHW2
这个issue类型是用户提出需求，主要对象是ceval评测脚本，用户寻求chatglm36B和llama7B的ceval评测脚本。

https://gitee.com/mindspore/mindformers/issues/I9U7Y4
这是一个bug报告，主要涉及到qwen7b多卡推理时报错，可能由于配置参数不正确导致报错。

https://gitee.com/mindspore/mindformers/issues/I9U3AO
这是一个bug报告，涉及llama7b chat_web推理报错，报错信息中提到硬件环境、软件环境和配置文件，可能是由配置或软件环境不匹配导致的推理报错。

https://gitee.com/mindspore/mindformers/issues/I9U2FJ
这是一个bug报告，涉及到在910A环境下单机8卡的bloom大模型微调训练失败，导致出现"Out of Memory"错误。

https://gitee.com/mindspore/mindformers/issues/I9TL00
这是一个bug报告，主要涉及mindspore中的运算图数据重分布问题，可能由于缺乏运算符信息导致报错。

https://gitee.com/mindspore/mindformers/issues/I9TKU6
这是一个bug报告类型的issue，主要涉及的对象是910B4 32GB内存设备。这个问题由于内存不足导致Out of Memory错误，用户寻求减少内存占用的方法。

https://gitee.com/mindspore/mindformers/issues/I9TJEI
这是一个bug报告，主要涉及mindformers中的llama38b模型在推理过程中出现"Segmentation fault (core dumped)"错误。可能的原因是模型转换或推理环境的配置问题导致的。

https://gitee.com/mindspore/mindformers/issues/I9T8YK
这是一个用户提出问题的issue，主要涉及MindSpore /mindformers项目中基于Generate推理可设置的参数及文档情况。由于用户不清楚如何指定推理参数，导致他在部署推理服务时遇到困惑。

https://gitee.com/mindspore/mindformers/issues/I9T82E
这是一个bug报告类型的issue，主要涉及Mindspore网络迁移过程中输出的loss值为负数，可能是由于某种原因导致的。

https://gitee.com/mindspore/mindformers/issues/I9T80K
这是一个bug报告，主要涉及mindformers项目下的qwen1.5推理报错，由于目录结构问题导致找不到resarch，造成推理报错问题。

https://gitee.com/mindspore/mindformers/issues/I9T7W8
这是一个bug报告，主要对象是AlphaPose案例在800/3000服务器上部署失败，可能是由于缺少软件依赖"live555"而导致。

https://gitee.com/mindspore/mindformers/issues/I9T7TQ
这是一个bug报告，主要涉及到运行mindformers中qwen1_5推理时出现报错的问题。原因可能是运行脚本参数设置错误或者代码实现逻辑问题导致的。

https://gitee.com/mindspore/mindformers/issues/I9T7TM
这是一个bug报告，主要涉及 mindformers 代码在数据集转换过程中出现报错的问题。由于可能是数据路径或参数设置错误导致的报错。

https://gitee.com/mindspore/mindformers/issues/I9T7TC
这是一个bug报告，涉及Mindformer在分布式权重拉起lora微调过程中遇到的问题，可能是由于修改超参导致加载分布式预训练权重时报错的原因。

https://gitee.com/mindspore/mindformers/issues/I9T7RY
这是一个Bug报告，涉及到Mindformers项目下的qwen1_5推理报错，可能导致的问题是运行脚本报错，可能原因是命令参数错误。

https://gitee.com/mindspore/mindformers/issues/I9T7O4
这是一个bug报告，涉及到qwen1.5在特定环境下运行报错的问题。原因可能是CANN、mindformers、torch和python等版本不兼容或不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/I9T7M1
这是一个bug报告，主要涉及代码无法导入名为"swap_cache"的模块，可能是由于代码版本不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/I9T7E0
这是一个bug报告，针对glm32k模型微调时出现的错误。可能由于代码版本兼容性或参数设置问题导致运行报错。

https://gitee.com/mindspore/mindformers/issues/I9T211
这个issue类型是bug报告，该问题单涉及的主要对象是网络推理功能。这是由于索引超出边界导致的错误。

https://gitee.com/mindspore/mindformers/issues/I9T1UT
这是一个bug报告，涉及到网络性能劣化的问题，由于某些原因导致生成速度下降到6.66，低于标准值45.32。

https://gitee.com/mindspore/mindformers/issues/I9SYZ2
这是一个bug报告，涉及的主要对象是baichuan2单机8卡910微调。由于获取IP的代码可能存在问题，导致节点重复注册的报错。

https://gitee.com/mindspore/mindformers/issues/I9SS77
这是一个bug报告，涉及对象为在集成web服务时遇到了Malloc device memory failed报错的用户。这个问题可能是由于web服务的改动导致内存分配错误所致。

https://gitee.com/mindspore/mindformers/issues/I9SQ9K
这是一个bug报告类型的issue，主要涉及chat_web调用接口异常，由于传入格式不符合要求导致TextStreamer出现异常。

https://gitee.com/mindspore/mindformers/issues/I9SKPY
这是一个bug报告，针对mindformers下的一个issue，主要涉及增量/batch推理功能，由于特定条件下出现推理回答异常。

https://gitee.com/mindspore/mindformers/issues/I9SI2E
这是一个bug报告类型的issue，主要涉及到使用V1.1.0版本的代码对qwen7b进行单卡推理时出现的问题和保存的txt乱码。导致这个问题的原因可能是代码版本不匹配或者环境配置问题。

https://gitee.com/mindspore/mindformers/issues/I9SEXT
这是一个bug报告，主要涉及MindFormers项目中的通义千问1.5模块，用户提出了910B推理结果错误的问题。

https://gitee.com/mindspore/mindformers/issues/I9SCE8
这是一个bug报告类型的issue，涉及主要对象为纯数据微调时出现OOM报错，可能由于内存不足导致。

https://gitee.com/mindspore/mindformers/issues/I9SC1D
这个issue类型是bug报告，该问题单涉及的主要对象是设备加载错误信息。原因可能是设备加载过程中出现了错误，导致用户无法正常使用设备或者获取相关信息。

https://gitee.com/mindspore/mindformers/issues/I9SBZ8
这是一个bug报告，该问题涉及的主要对象是DS（数据存储），可能由于硬盘空间不足导致了"No space left on device"错误信息的出现。

https://gitee.com/mindspore/mindformers/issues/I9SBXL
这是一个bug报告，主要涉及对象是选择数值时出现的数值错误。原因是用户没有选择预先设定的数值选项，导致程序报错。

https://gitee.com/mindspore/mindformers/issues/I9RQGT
这是一个bug报告类型的issue，涉及主要对象是在modelarts云平台基础镜像mindspore_2.1.0cann_6.3.2py_3.7euler_2.8.3aarch64d910上安装ascend_toolkit后出现的报错。这个问题可能由于numpy库的某些设置问题导致了报错提示的症状。

https://gitee.com/mindspore/mindformers/issues/I9RDCX
这是一个bug报告，主要涉及mindformers项目中运行finetune时出现JSONDecodeError报错，可能是由于输入参数缺失导致的。

https://gitee.com/mindspore/mindformers/issues/I9RBX8
这是一个用户提出需求的issue，主要对象是支持多卡交互式推理。原因可能是因为当前无法使用10B以上的模型。

https://gitee.com/mindspore/mindformers/issues/I9R9U4
这是一个bug报告，主要涉及GLM2 Lora在分布式训练自动切分过程中未生成策略文件导致索引越界报错的问题。

https://gitee.com/mindspore/mindformers/issues/I9R5LA
这是一个bug报告，主要涉及mindspore中的FileWriter对象，由于未知原因导致出现AttributeError错误。

https://gitee.com/mindspore/mindformers/issues/I9QZOH
这个issue是关于bug报告，主要涉及到MindForbers中的GLM3推理实例化模型报错问题，导致报错的原因是在配置文件中pre_seq_len参数为None而非0或False。

https://gitee.com/mindspore/mindformers/issues/I9QXJ3
这是一个bug报告，涉及对象是baichuan2模型微调时出现的ImportError，可能是因为版本不匹配导致无法正确引入函数check_valid_paged_attention。

https://gitee.com/mindspore/mindformers/issues/I9QGBR
这是一个bug报告，涉及MindSpore Lite推理的问题，原因可能是执行计算图失败导致的报错。

https://gitee.com/mindspore/mindformers/issues/I9QCXN
这是一个bug报告类型的issue，主要涉及到用户在使用qwen1.5时遇到推理结果乱码的问题。可能是由于版本或配置问题导致的。

https://gitee.com/mindspore/mindformers/issues/I9Q7JZ
这是一个bug报告，涉及到MindSpore框架在进行并行泛化测试时出现报错“RuntimeError: Failure:operator RmsNorm init failed”的问题。原因是最后一张卡挂掉并出现报错，其余卡进程卡住，不能及时退出。

https://gitee.com/mindspore/mindformers/issues/I9Q6P9
这是一个bug报告，涉及的主要对象是分布式训练中的ranktable配置。这个问题可能由于ranktable配置中的devId与本地devId不一致导致了错误信息"The ranktable config devId is inconsistent with the local devId"的产生。

https://gitee.com/mindspore/mindformers/issues/I9Q6KG
这是一个bug报告，主要涉及qwen1.5模型在推理过程中速度缓慢的问题。可能是由于单卡推理速度过慢导致的。

https://gitee.com/mindspore/mindformers/issues/I9Q4E1
这是一个bug报告，主要涉及到执行llama2推理时出现了数值错误（ValueError），原因可能是无法找到类类型为LlamaProcessor的tokenizer类名。

https://gitee.com/mindspore/mindformers/issues/I9PLJF
这是一个bug报告，该问题涉及的主要对象是codegeex2_6b单机两卡开启增量(use_past: True)推理，由于未开启增量推理正常，导致了推理报错。

https://gitee.com/mindspore/mindformers/issues/I9PLI1
这是一个关于环境配置版本缺失的bug报告，涉及Mindformers仓库中r1.0.a分支的问题。

https://gitee.com/mindspore/mindformers/issues/I9PKC9
这是一个Bug报告，主要涉及软件配置、文档可用性和模型部署的对应关系问题，可能是由于文档不清晰导致的用户在运行模型时出现错误。

https://gitee.com/mindspore/mindformers/issues/I9PJFR
这是一个bug报告，主要对象是mindformers r1.0.a分支dockers镜像无法拉取。由于某种原因导致用户无法拉取镜像，需要寻求帮助解决这个问题。

https://gitee.com/mindspore/mindformers/issues/I9PH5G
这是一个bug报告，涉及Baichuan13B推理缺少resume的问题。由于代码中resume变量未被正确设置为resume_training，导致训练过程中出现报错。

https://gitee.com/mindspore/mindformers/issues/I9P8VG
这是一个bug报告，主要涉及clip processor静态图推理在Ascend 910环境下出错的问题。可能由于环境配置或代码逻辑错误导致的 bug。

https://gitee.com/mindspore/mindformers/issues/I9P8K2
这是一个bug报告，涉及到MindFormers在转换数据集为mindrecode时出现导入tik异常的问题，可能是由于代码中缺少了对`tik`的引入而导致的。

https://gitee.com/mindspore/mindformers/issues/I9P5OI
这是一个bug报告，主要涉及ChatGLM2模型推理过程中的错误，由于参数pre_seq_len设置为None时，导致在自回归推理时出现了TypeError。

https://gitee.com/mindspore/mindformers/issues/I9P2GJ
这是一个bug报告类型的issue，主要涉及到mindformers项目中的文档链接错误。该问题可能是由于文档中的链接写错或指向错误路径导致无法访问到相关文件。

https://gitee.com/mindspore/mindformers/issues/I9OZ05
这个issue属于bug报告类型，涉及对象为使用Bloom 560m推理模型时关闭增量推理后出现异常。该问题可能是由于关闭增量推理导致答案异常。

https://gitee.com/mindspore/mindformers/issues/I9OS39
这是一个bug报告，主要涉及对象为chatglm26b单卡lora微调，其报错原因可能是服务器910a cann版本问题导致的。

https://gitee.com/mindspore/mindformers/issues/I9ORZN
这是一个问题报告，涉及到分布式训练过程中出现"Exec Graph Failed"的错误。可能是由于参数设置或软件版本的原因导致的。

https://gitee.com/mindspore/mindformers/issues/I9OQ0A
这是一个bug报告，该问题涉及baichuan2大模型在910A环境下单机8卡微调失败，出现数值错误导致训练失败。

https://gitee.com/mindspore/mindformers/issues/I9OIKT
这个issue类型是bug报告，涉及的主要对象是mindformers中的llama234b模型推理过程。由于超参"temperature"设置为0.9时，导致推理出现病句。

https://gitee.com/mindspore/mindformers/issues/I9OFXW
这个issue属于用户提出需求类型，主要涉及mindformers是否有对internlm220B的支持计划。用户提出该问题可能是因为目前使用internlm20B没有继续与训练的方法。

https://gitee.com/mindspore/mindformers/issues/I9OF44
这是一个bug报告，主要涉及到使用mindformers教程进行internlm的推理时出现了import error。原因是`partially initialized module 'tvm' has no attribute '_ffi'`，可能是由于循环导入导致的。

https://gitee.com/mindspore/mindformers/issues/I9O8JO
这是一个需求提出类型的issue，主要涉及mindformers模型转换为huggingface模型的功能。由于当前只支持单向转换，导致无法在推理阶段使用基于英伟达卡的huggingface模型做推理。

https://gitee.com/mindspore/mindformers/issues/I9O5HH
这是一个bug报告，主要涉及GLM3 mslite推理报错的问题，可能是由于软件bug或者配置问题导致的推理错误。

https://gitee.com/mindspore/mindformers/issues/I9NWRF
这是一个bug报告，涉及到"llama27b"推理时出现报错的问题，可能是由于配置或代码问题导致的。

https://gitee.com/mindspore/mindformers/issues/I9NRNI
这是一个bug报告，涉及主要对象为qwen1.5_1.8B推理出现回答混乱的情况。由于推理步骤参考可能存在问题，导致了回答混乱的bug。

https://gitee.com/mindspore/mindformers/issues/I9NPTQ
这个issue属于性能问题报告，涉及llama3模型在转换精度过程中出现的问题，可能由于转换过程中的参数设置或代码逻辑不正确导致测试结果的精度与官方测试结果不符。

https://gitee.com/mindspore/mindformers/issues/I9NNQ2
这个issue是一个bug报告，涉及到mindformers项目中运行脚本时的配置问题，可能是由于文件路径和软件版本不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/I9NL4D
这是一个bug报告，涉及的主要对象是mindformers中的代码。导致此问题的原因是无法从'mindspore._c_expression'模块中导入名为'swap_cache'的函数。

https://gitee.com/mindspore/mindformers/issues/I9NI10
这是一个bug报告，主要涉及mindformers框架在单机4卡分布式推理时出现out of memory报错，可能是由于内存不足导致的。

https://gitee.com/mindspore/mindformers/issues/I9NHTW
这是一个bug报告，涉及mindformers中做qwen1.5_0.5B推理时出现out of vocabulary错误的问题，可能是由于模型使用910A进行多卡推理导致。

https://gitee.com/mindspore/mindformers/issues/I9NHEQ
这是一个bug报告，涉及到mindformers下的llama2模型，在单卡推理过程中出现异常。导致这个问题的原因可能与软件环境或执行模式有关。

https://gitee.com/mindspore/mindformers/issues/I9N6Z7
这是一个bug报告，涉及mindformers下的readme文档中关于推理启动命令描述的问题。由于推理启动命令描述双引号重复，未提示修改ckpt路径，导致无法启动推理。

https://gitee.com/mindspore/mindformers/issues/I9N6V3
这是一个bug报告，涉及MindSpore在大模型启动方式、生成、流水线、多批量推理失败的问题，由于启动方式需要改成msrun，导致出现报错。

https://gitee.com/mindspore/mindformers/issues/I9N0Y6
这个issue类型是用户提出需求，请教问题。问题涉及的主要对象是mindspore以及mindformers仓库。由于较低版本的HDK在昇腾社区已下架无法下载，用户想了解是否能够正常使用23.0.3版本的HDK和8.0以上版本的CANN，以及何时能够配套。

https://gitee.com/mindspore/mindformers/issues/I9N0B4
这是一个bug报告类型的issue，主要涉及run_chat_server请求时出现的input_ids超出max length错误。这个问题可能是由于未释放上次请求的上下文而导致。

https://gitee.com/mindspore/mindformers/issues/I9MW51
这是一个bug报告，涉及的主要对象是在pull dev分支的mindspore中准备微调llama2的前期模型转换工作时出现的ImportError。这个问题可能是由于dev分支的代码中引入了model_runner.py文件导致的。

https://gitee.com/mindspore/mindformers/issues/I9MO1R
这是一个bug报告，该问题涉及MindSpore在推理过程中文档描述不清晰导致单卡推理失败。

https://gitee.com/mindspore/mindformers/issues/I9MKIQ
这是一个bug报告，涉及对象为baichuan213Bchat，推理回答乱码，可能由于自动权重转换导致。

https://gitee.com/mindspore/mindformers/issues/I9MJJR
这是一个用户提出需求的issue，主要涉及mindformers中数据集加载功能的问题，由于只能从OBS加载mindrecord导致用户需要费时转换大数据集成mindrecord，希望能添加直接加载数据集目录的功能。

https://gitee.com/mindspore/mindformers/issues/I9MJ1R
这是一个bug报告类型的issue，主要涉及mindformers下的readme文档不清晰，启动方式需改成msrun启动，以及单卡推理失败的问题。由于文档描述不清晰导致启动方式错误，进而导致单卡推理失败。

https://gitee.com/mindspore/mindformers/issues/I9MD2S
该issue是关于需求的问题，主要涉及qwen1.514B chat版的适配问题，用户询问何时适配。

https://gitee.com/mindspore/mindformers/issues/I9KNPC
这是一个bug报告类型的issue，主要涉及的对象是Baichuan27B的推理性能。由于910B上的推理性能表现比910A差，并且多卡测试表现不如单卡，用户想知道当前情况是否正常。

https://gitee.com/mindspore/mindformers/issues/I9KEEW
这是一个用户需求类型的问题单，主要涉及到mindformers中的logger工具以及如何在模型中添加时间信息。这个问题的原因是尝试使用logger.info导出时间信息时出错，用户想要在模型中添加时间戳来定位代码运行到哪个结构。

https://gitee.com/mindspore/mindformers/issues/I9KED2
这是一个bug报告，涉及的主要对象是Qwen1.5数据处理程序。这个问题是由于无法从'mindspore._c_expression'中导入'swap_cache'导致的错误信息报告。

https://gitee.com/mindspore/mindformers/issues/I9KC4C
这是一个bug报告，主要涉及chatglm36b推理的问题，可能由于操作系统环境或输入错误导致报错。

https://gitee.com/mindspore/mindformers/issues/I9KA3Z
这是一个bug报告，主要涉及的对象是cann7.2, ms2.3, qwen1.5训练bf16，导致loss为nan的问题。

https://gitee.com/mindspore/mindformers/issues/I9K8AS
这是一个bug报告，涉及多机多卡配置问题，由于Ascend collective communication初始化失败导致启动报错。

https://gitee.com/mindspore/mindformers/issues/I9K86M
这是一个bug报告，涉及的主要对象是MindFormers下的llama27b推理。由于MindFormers版本更新后，导致在运行predict_custom.py时出现了P没有auto_generate的attribute报错。

https://gitee.com/mindspore/mindformers/issues/I9JUFC
这是一个bug报告，涉及主要对象是mindformers代码中的推理运行，出现了 Cast failed, original value: 0.0883883, type: FP32Imm 报错。

https://gitee.com/mindspore/mindformers/issues/I9JTN6
这是一个bug报告类型的issue，涉及到docker镜像无法拉取的问题，由于镜像的标签不存在而导致无法拉取。

https://gitee.com/mindspore/mindformers/issues/I9JTLL
这是一个bug报告类型的issue，主要涉及Llama2分布式预训练模型权重合并后大小异常的问题，原因可能是合并操作导致权重大小增大，用户希望得到权重大小异常以及合并后每个rank大小的正常性问题的解答。

https://gitee.com/mindspore/mindformers/issues/I9JO77
这是一个bug报告，涉及使用大模型训练设置流水线并行时出错。可能是由于设置了流水线导致报错。

https://gitee.com/mindspore/mindformers/issues/I9JKC9
这是一个bug报告，主要涉及Llama27b在mindspore分布式推理时报错，原因是出现了不期望的关键字参数'sparse_mode'。

https://gitee.com/mindspore/mindformers/issues/I9JHVD
这是一个关于如何使用ModelArts OBS上的数据的问题，主要涉及mindformers对大数据在OBS上的使用。由于用户需要在mindformers中使用12TB左右的数据，所以提出了如何在ModelArts OBS上使用数据并请求示例代码的问题。

https://gitee.com/mindspore/mindformers/issues/I9J8OP
这个issue类型是用户提出需求，主要涉及的对象是glm3库，用户提出了关于支持lora微调的需求。

https://gitee.com/mindspore/mindformers/issues/I9J2M9
这是一个bug报告，主要涉及test_mindformers_auto_trans_ckpt_parallel_check_4，由于某种原因导致进程报错但不退出。

https://gitee.com/mindspore/mindformers/issues/I9J0VU
这是一个bug报告，涉及的主要对象是Qwen7B昇腾适配问题。由于numpy库中的警告导致出现数值为零的问题。

https://gitee.com/mindspore/mindformers/issues/I9IXP8
这是一个关于需求的问题，主要对象是mindformers软件中的并发推理功能。用户想了解mindformers是否支持并发推理以及相关组件信息。

https://gitee.com/mindspore/mindformers/issues/I9IWC2
这个issue是一个Bug报告，主要涉及mindformers的qwen1_5模型转换成ckpt后在执行推理时报权重权限问题。可能是由于权重文件的权限设置问题导致的。

https://gitee.com/mindspore/mindformers/issues/I9IVQ3
这是一个bug报告，涉及主要对象为在baichuan2base13b上进行推理时出现错误。原因可能是转换权重格式后使用mindformers进行推理结果为空。

https://gitee.com/mindspore/mindformers/issues/I9IU17
这是一个bug报告，涉及的主要对象是运行Python脚本时出现的类型错误，导致出现了"TypeError: must be real number, not NoneType"的报错信息。

https://gitee.com/mindspore/mindformers/issues/I9ITZJ
这个issue是关于bug报告，涉及MindSpore代码中出现的TypeError错误。原因是当前代码中出现了对NoneType的处理，导致必须是实数的要求无法满足。

https://gitee.com/mindspore/mindformers/issues/I9ITOT
这是一个bug报告类型的issue，涉及主要对象为baichuan213B推理引擎。由于缺少一个类，导致在运行lora微调时报错。

https://gitee.com/mindspore/mindformers/issues/I9ITBD
这是一个bug报告，涉及到单卡推理报错的问题，由于14b报错后刷新环境变量无法解决。

https://gitee.com/mindspore/mindformers/issues/I9IPTJ
这是一个bug报告类型的issue，主要涉及LLaMA27B推理速度慢的问题，可能由于硬件设备或软件配置等原因导致推理速度只有3.75token每秒。

https://gitee.com/mindspore/mindformers/issues/I9IOA5
这是一个bug报告，主要涉及到MindSpore在开启flash attention后推理报错的问题。原因可能是flash attention导致的加速推理时出现的错误。

https://gitee.com/mindspore/mindformers/issues/I9IKUC
这是一个bug报告类型的issue，涉及主要对象为mindformers的llama 7b模型在modelarts平台上的训练作业提交出现报错，可能由于代码执行过程中出现了异常导致报错。

https://gitee.com/mindspore/mindformers/issues/I9IK5X
这是一个bug报告，主要涉及到mindformers下的telechat模型。该问题由于分词问题导致脚本运行报错第109行长度没有对齐。

https://gitee.com/mindspore/mindformers/issues/I9IK0P
这是一个bug报告，主要涉及的对象是baichuan2进行chat_web时出现的报错。由于predict配置文件中存在参数值错误或者格式问题，导致了程序报错。

https://gitee.com/mindspore/mindformers/issues/I9I65A
这是一个bug报告，主要涉及模型在设置top_k和top_p参数时多次推理结果完全一致的问题，可能是由于模型在采样过程中出现了错误导致的。

https://gitee.com/mindspore/mindformers/issues/I9I5NP
这是一个bug报告，涉及的主要对象是llama213b双动态lite推理模块。由于精度有误差导致推理结果不正确。

https://gitee.com/mindspore/mindformers/issues/I9I2ZQ
这个issue类型是bug报告，涉及的主要对象是模型转换说明的参数设置。由于参数设置不符合用户的预期，导致了用户对于模型转换说明的写法感到困惑和不满。

https://gitee.com/mindspore/mindformers/issues/I9I2YR
这是一个bug报告，主要涉及MindSpore在910B3环境下单机微调失败的问题，原因是内存池空间不足。

https://gitee.com/mindspore/mindformers/issues/I9I2PW
这个issue类型是bug报告，主要涉及多卡推理功能，由于缺少hccl库导致无法进行多卡推理。

https://gitee.com/mindspore/mindformers/issues/I9I12A
这个issue是关于telechat在web_chat中遇到问题的bug报告，主要涉及mindformers和mindspore环境下的配置文件和代码。报错可能是因为文件路径导致的。

https://gitee.com/mindspore/mindformers/issues/I9HZUP
这是一个用户提出需求的类型的issue，主要涉及对象是telechat项目中的tokenizer.model文件。用户希望在README.md中提供tokenizer.model的下载链接。

https://gitee.com/mindspore/mindformers/issues/I9HV42
这是一个Bug报告类型的Issue，主要涉及MindSpore中的AI辅助内部业务。存在多次输入相同问题，do_sample=False配置情况下得到不一致结果的问题，以及在线推理与离线推理性能下降的情况。

https://gitee.com/mindspore/mindformers/issues/I9HUUF
这是一个bug报告，涉及的主要对象是mindformers 0.8与mindspore 2.2.1配套时出现的调用问题，导致缺少head_num传参。

https://gitee.com/mindspore/mindformers/issues/I9HT40
这个issue是关于bug报告，主要涉及mindformers库中Generate推理设置参数num_beams在设置为3时出错的问题，可能是由于代码实现或配置问题导致。

https://gitee.com/mindspore/mindformers/issues/I9HS15
这个issue类型是bug报告，涉及的主要对象是脚本参数名train_data不正确，由于参数名应为`train_dataset`的原因导致了这个bug。

https://gitee.com/mindspore/mindformers/issues/I9HGAZ
这是一个bug报告，用户在尝试使用最新的Telechat12b模型进行分割时遇到了问题，可能由于环境配置或命令操作不当导致报错。

https://gitee.com/mindspore/mindformers/issues/I9HETE
这是一个bug报告，涉及的主要对象是运行在Ascend910A上的MindSpore程序。可能原因是运行过程中出现了"RuntimeError: Failure: operator Gather SetCostUnderStrategy failed"错误。

https://gitee.com/mindspore/mindformers/issues/I9HADU
这是一个bug报告，用户在运行`run_glm2_6b_lora_eval.yaml`时出现了报错。原因可能是某些依赖库的版本不兼容导致的问题。

https://gitee.com/mindspore/mindformers/issues/I9H6PG
这是一个关于需求讨论的issue，主要涉及到视觉大模型vit和swin在使用自定义数据集标签时需要做怎样的修改。这个问题的提出可能是由于缺乏关于如何自定义数据集标签的文档或指导，导致用户不清楚需要修改哪些文件以及如何做出相应的更改。

https://gitee.com/mindspore/mindformers/issues/I9H0PG
这是一个用户提出需求的issue，主要涉及到Mindformers项目的初学者使用问题。由于文档不清晰，导致用户对于模型加载、转换以及环境安装等方面产生困惑。

https://gitee.com/mindspore/mindformers/issues/I9GZX3
这是一个bug报告，涉及到Qwen14B在使用Generate多卡推理时报错的问题。由于GE RunGraphWithStreamAsync调用失败导致了报错。

https://gitee.com/mindspore/mindformers/issues/I9GYUV
这是一个bug报告类型的issue，主要涉及mindspore中gpt2模型优化改进的问题。导致这个问题的原因是缺少对于不同参数量模型的说明和适配问题。

https://gitee.com/mindspore/mindformers/issues/I9GYA5
这是一个bug报告，该问题涉及mindformers模型在使用动态组网时出现内存资源耗尽的错误。原因可能是环境配置导致内存溢出，用户希望将解决方法添加到相关资料中。

https://gitee.com/mindspore/mindformers/issues/I9GXJ0
这是用户提出需求的问题，主要涉及MindSpore模型中缺乏动态组网的相关说明和脚本，导致用户在使用动态组网时缺乏指导和支持。

https://gitee.com/mindspore/mindformers/issues/I9GXF1
这个issue属于Bug报告类型，涉及主要对象是MindSpore / mindformers中的数据集预处理脚本belle_preprocess.py，由于tokenizers语法错误导致报错，需要添加tokenizers升级说明来解决问题。

https://gitee.com/mindspore/mindformers/issues/I9GU5M
这个issue是bug报告，涉及的主要对象是在图模式中使用Ascend910A上的SoftmaxCrossEntropyWithLogits出现RuntimeError。原因可能是缺少适用于SparseSoftmaxCrossEntropyWithLogits的内核信息导致的错误。

https://gitee.com/mindspore/mindformers/issues/I9GRZD
这是一个缺陷报告，主要涉及推理过程中baichuan213b模型显存占用过高的问题。

https://gitee.com/mindspore/mindformers/issues/I9GRFX
这是一个bug报告，涉及对象是vit预训练，在mindspore2.2.12环境下运行时报错'ViTConfig' object has no attribute 'depth'，可能是由于ViTConfig对象缺少depth属性导致的问题。

https://gitee.com/mindspore/mindformers/issues/I9GQK9
这是一个bug报告，涉及主要对象是使用mindformers库和nn.SoftmaxCrossEntropyWithLogits函数的用户。由于输入的features和labels形状长度不匹配，导致出现了ValueError。

https://gitee.com/mindspore/mindformers/issues/I9GPI5
这是一个bug报告，涉及Qwen1.5转化权重时出现的报错，可能由于转化脚本或参数设置问题导致。

https://gitee.com/mindspore/mindformers/issues/I9GLA8
这是一个bug报告类型的issue，主要涉及"nn.CrossEntropyLoss"算子在Ascend910A上使用时的问题，可能由于Graph模式下无法使用"shard"接口导致无法进行算子划分。

https://gitee.com/mindspore/mindformers/issues/I9GHXG
这是一个用户提出需求的issue，主要对象是mindformers的gitee仓库，由于当前主页支持的大模型列表中缺少GLM3导致用户提出需求添加GLM3及其文档链接。

https://gitee.com/mindspore/mindformers/issues/I9GD1R
这是一个bug报告，涉及的主要对象是llama2175B模型。由于设置load_checkpoint为''，并且只保存切分策略，导致报错。

https://gitee.com/mindspore/mindformers/issues/I9G6WV
这个issue类型为功能需求，主要涉及的对象是LoRa权重合并。由于模型微调需要将已训练的权重与新权重合并后再进行评估，用户在询问如何进行权重合并。

https://gitee.com/mindspore/mindformers/issues/I9G6BG
这是一个bug报告类型的issue，主要涉及到测试用例[test_mindformers_auto_trans_ckpt_path_error_1](https://gitee.com/mindspore/mindformers/issues/I1KJW5)，问题原因可能是未找到rank_0文件夹或ckpt文件引起的数值错误。

https://gitee.com/mindspore/mindformers/issues/I9G3TW
这是一个bug报告，主要涉及到训练run_glm2_6b_ptuning2.yaml设置do_eval时，tokenizer文件报错的问题。由于网络连接错误导致无法下载tokenizer文件，造成了该问题。

https://gitee.com/mindspore/mindformers/issues/I9G2PJ
这是一个bug报告，主要对象是在使用mindformers库的过程中遇到了加载本地模型文件进行推理时出现报错的问题，可能是因为硬件NPU限制导致无法使用mindspore框架。

https://gitee.com/mindspore/mindformers/issues/I9G2CB
这是一个bug报告，主要涉及AutoTokenizer.from_pretrained和AutoProcessor.from_pretrained两种方式加载tokenizer时出现的数值错误。造成该问题的原因可能是缺少相应的文件或配置。

https://gitee.com/mindspore/mindformers/issues/I9G2CA
该issue类型为bug报告，涉及到mindformer.core.callback.CheckpointMonitor类的拼写错误问题，由于拼写错误导致了代码逻辑上的问题。

https://gitee.com/mindspore/mindformers/issues/I9G1GJ
这是一个bug报告，主要涉及多卡lora微调，出现原因可能是传入的数据维度不匹配导致数值计算错误。

https://gitee.com/mindspore/mindformers/issues/I9G0LD
这是一个bug报告，主要涉及KeyWordGenDataset和ChatGLM2Tokenizer，由于路径错误导致出现了AssertionError。

https://gitee.com/mindspore/mindformers/issues/I9FWBQ
这是一个bug报告，主要对象是iflytekspark13B模型训练后推理出现重复字符的问题，可能是由于数据使用错误导致的。

https://gitee.com/mindspore/mindformers/issues/I9FV6Q
这是一个bug报告，主要涉及mindformers下的baichuan213bchat双卡推理显存溢出的问题，可能是由于模型在使用双卡推理时占用显存过高导致显存溢出的情况。

https://gitee.com/mindspore/mindformers/issues/I9FU9A
这是关于bug报告的issue，主要涉及的对象是使用chat权重进行推理时出现的报错问题。由于版本信息不匹配或者模型权重加载出错导致了报错信息的出现。

https://gitee.com/mindspore/mindformers/issues/I9FQ75
这是一个bug报告。主要涉及对象是使用昇腾处理器进行codellama34b 8卡分布式推理时出现失败。可能由于环境配置、驱动信息或者软件版本等原因导致了推理失败。

https://gitee.com/mindspore/mindformers/issues/I9FN8A
这是一个bug报告，主要涉及Mindformers中Qwen14B模型推理样例无法正常运行的问题。原因是在更新至最新commit后出现了缺少必需的位置参数'layer_id'的错误。

https://gitee.com/mindspore/mindformers/issues/I9FN3K
这是一个bug报告，主要涉及的对象是warmup_ratio参数。导致这个问题的原因可能是设置的warmup_ratio参数未生效，需要转为warmup_steps才能生效。

https://gitee.com/mindspore/mindformers/issues/I9FL4E
这是一个bug报告，涉及的主要对象是书生20b双动态，lite单卡推理，由于未知原因导致推理报错。

https://gitee.com/mindspore/mindformers/issues/I9FI71
这是一个bug报告，涉及的主要对象是baichuan2_7b在1*8卡910A跑lora微调过程中出现了TypeError错误。原因可能是参数传递错误导致的。

https://gitee.com/mindspore/mindformers/issues/I9FHXQ
这是一个bug报告，主要涉及的对象是mindspore中的参数。由于参数`vision_model.encoder.layers.0.attn.v_bias`存在多个用户，但是TensorInfo不同导致的问题。

https://gitee.com/mindspore/mindformers/issues/I9FH9F
这是一个bug报告类型的issue，主要涉及了llama27b预训练模型在更换成自己数据集时损失不下降且学习率一直为0的问题。原因可能是数据集迁移导致模型无法正常训练。

https://gitee.com/mindspore/mindformers/issues/I9FFH7
这是一个bug报告，涉及到mindspore库中缺少ReshapeAndCache方法导致的报错。

https://gitee.com/mindspore/mindformers/issues/I9FADI
这是一个bug报告，主要涉及的对象是llama27b全量训练过程中出现的内存不足问题，导致了无法分配内存而失败的症状。

https://gitee.com/mindspore/mindformers/issues/I9F5N8
这是一个bug报告，涉及主要对象为在8卡910a上使用mindformers进行微调时设置seq_length为2048导致内存溢出（OOM），而设置为512可以成功运行。

https://gitee.com/mindspore/mindformers/issues/I9EYM9
这是一个bug报告，涉及MindFormers中chatglm26b, lite推理报错的问题。由于某种原因导致推理失败，出现了相关错误日志。

https://gitee.com/mindspore/mindformers/issues/I9EY1R
这是一个bug报告，涉及到Baichuan213b和Baichuan27b模型在进行推理时输出异常结果，可能是由于模型推理脚本中没有支持流式推理方法导致。

https://gitee.com/mindspore/mindformers/issues/I9EXTL
这是一个bug报告， 主要涉及到Yi34B模型的内存泄露问题。原因可能是配置文件中的一些参数设置引发了内存泄露的情况。

https://gitee.com/mindspore/mindformers/issues/I9EWP0
这是一个bug报告，涉及主要对象是qwen7bchat模型lora微调训练，由于参数传递错误导致初始化函数接收到重复数值的参数而报错。

https://gitee.com/mindspore/mindformers/issues/I9EWKI
这是一个bug报告issue，主要涉及llama2 13B+PA + dynamic lite推理，由于is_dynamic设置为true导致推理报错。

https://gitee.com/mindspore/mindformers/issues/I9EUNE
这个issue是关于bug报告，主要涉及百川13b lite双动态推理在精度和资源占用方面存在问题，导致部分推理失败。

https://gitee.com/mindspore/mindformers/issues/I9ESIZ
这是一个bug报告，主要涉及mindformers在训练llama2 7B模型时出现内存分配问题，可能是由于环境变量设置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I9ELP5
该issue类型是用户提出问题，主要涉及部署Qwen1.514B和推理结果模块，可能是由于重复的最后几条内容导致问题。

https://gitee.com/mindspore/mindformers/issues/I9EGRX
这是一个bug报告类型的issue，主要涉及转换模型框架时出现的输出问题，可能是由于不同框架之间参数转换导致的。

https://gitee.com/mindspore/mindformers/issues/I9E41X
这是一个bug报告，主要涉及的对象是在执行数据预处理和Mindrecord数据生成时遇到TypeError错误。这个问题可能是由于特殊token格式不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I9E0P4
这是一个用户提出需求的问题，主要涉及Dataset、Sampler以及DatasetHelper，由于采样顺序无法控制导致每个进程获得的input_ids不一致。

https://gitee.com/mindspore/mindformers/issues/I9DXJD
这是一个bug报告类型的issue，主要涉及到llama2 13B+PA lite推理过程中出现的推理报错问题。可能是由于某种原因导致在特定场景下无法成功完成推理。

https://gitee.com/mindspore/mindformers/issues/I9DIMC
这是一个bug报告类型的issue，主要涉及Lora训练完保存ckpt时出错。由于图形获取失败导致保存图执行失败。

https://gitee.com/mindspore/mindformers/issues/I9DGVK
这是一个bug报告，主要涉及glm3推理在最新代码下出现报错的问题，可能由代码更新造成。

https://gitee.com/mindspore/mindformers/issues/I9D8CV
这是一个bug报告，该问题涉及mindformers版本1.0的使用问题，由于将该包下载后放在引用文件根目录下会导致报错，可能是由于路径引用或环境配置问题导致运行错误。

https://gitee.com/mindspore/mindformers/issues/I9D54G
这是一个bug报告，涉及MindFormers 1.1中came优化器支持优化器并行功能的问题。开启梯度累积后执行报错，cam优化器内部报错，但不开启梯度累积时执行正常。可能原因是梯度累积功能与cam优化器内部不兼容导致报错。

https://gitee.com/mindspore/mindformers/issues/I9D1Z3
这是一个关于模型训练过程中Loss异常的bug报告，用户反馈最后全程loss为0，微调效果奇差。

https://gitee.com/mindspore/mindformers/issues/I9CUQ9
这是一个bug报告，涉及的主要对象是AICC中的chatglm36b模块。导致这个问题的原因是在glm3_tokenizer.py文件的build_batch_input函数中缺少了padding参数，导致在进行batch推理时报错。

https://gitee.com/mindspore/mindformers/issues/I9CQ8Q
这个issue是一个bug报告，主要涉及LoRA模块中的参数处理问题，导致部分参数未被正确考虑到，同时也反映了lora_adapter中的一个bug未正确调用freeze_pretrained_model函数。

https://gitee.com/mindspore/mindformers/issues/I9CN9L
这个issue属于bug报告类型，涉及主要对象是baichuan213B模型参数提取。由于GPU不一致导致出现参数提取结果与预期不一致的情况。

https://gitee.com/mindspore/mindformers/issues/I9CMCZ
这是一个bug报告，主要涉及的对象是mindformers下的一个运行错误"llama70b910b"。原因可能是由于 StridedSlice 初始化失败导致的程序运行错误。

https://gitee.com/mindspore/mindformers/issues/I9CLBX
这是一个bug报告，主要对象涉及多机多卡训练过程中出现的数值错误"ValueError: CPU number 42 is not eligible"。由于CPU亲缘性设置时发生了异常，导致训练中出现了错误。

https://gitee.com/mindspore/mindformers/issues/I9CKV8
这是一个bug报告类型的issue，主要涉及的对象是Baichuan27B pipeline及chat。由于传参重复导致出现dim报错。

https://gitee.com/mindspore/mindformers/issues/I9CKQ0
这是一个bug报告，主要涉及的对象是llama2推理过程中出现的错误。由于加载的参数字典在过滤或指定后为空，导致了在推理时出现报错。

https://gitee.com/mindspore/mindformers/issues/I9CH0U
这是一个bug报告，涉及的主要对象为mindformers下的全参微调模型qwen7b，由于优化器并行分片与模型并行分片大小不相等导致报错。

https://gitee.com/mindspore/mindformers/issues/I9CFQ0
这是一个bug报告类型的issue，主要涉及mindformers在r1.0版本训练LLam27B时出现的错误。导致这个问题的原因是输入的策略与要求的不匹配。

https://gitee.com/mindspore/mindformers/issues/I9CFNU
这是一个bug报告，涉及主要对象是llama2，可能由于细粒度多副本和concat优化引起了报错。

https://gitee.com/mindspore/mindformers/issues/I9CCYN
这是一个bug报告，主要涉及Modelarts开发环境中运行pangu模型报错EI9999的问题。由于未能指明具体错误信息，可能是由于环境配置、依赖版本或代码兼容性等问题引起。

https://gitee.com/mindspore/mindformers/issues/I9C9B8
这是一个bug报告，主要涉及到mindformers中llama27b模型生成推理时报错的问题。导致这个问题的原因是配置文件中存在错误的optimizer参数。

https://gitee.com/mindspore/mindformers/issues/I9C987
这是一个bug报告类型的issue，主要涉及到模型转换过程中的报错。原因是参数加载时出现错误，需要使用torch.jit.load替代torch.load来解决。

https://gitee.com/mindspore/mindformers/issues/I9C8O8
这是一个bug报告类型的issue，主要涉及的对象是mindformers r1.0 mindspore 2.2.11中的qwen17b chatweb api。问题可能是由于部署qpi时的特定情况导致，使得qwen回答异常，只能续写而非正常回答问题。

https://gitee.com/mindspore/mindformers/issues/I9C50J
这是一个bug报告类型的issue，涉及LLaMa27B全参微调README描述有误，导致微调报错。

https://gitee.com/mindspore/mindformers/issues/I9C4MH
这是一个bug报告，涉及到基于Mindformer的CodeLlama7bInstructhf单卡推理精度不对齐的问题。导致该问题的原因是在当前输出中生成了大量换行符。

https://gitee.com/mindspore/mindformers/issues/I9C3YL
这是一个bug报告类型的issue，主要涉及chatglm3模型微调失败问题，可能由于显存不足导致。

https://gitee.com/mindspore/mindformers/issues/I9BSNJ
这是一个Bug报告，涉及MindFormers 1.1中Came优化器在设置auto_parallel后导致训练报错的问题。可能由于优化器并行设置不正确导致。

https://gitee.com/mindspore/mindformers/issues/I9BP6L
这是一个bug报告，主要涉及的对象是基于mindformers跑baichuan213b lora微调的系统。问题由于出现了图循环而导致RuntimeError，用户需要帮助解决这个问题。

https://gitee.com/mindspore/mindformers/issues/I9BG1N
这是一个bug报告，涉及Baichuan27b lora微调功能，由于环境配置或者脚本执行问题导致进程自动退出。

https://gitee.com/mindspore/mindformers/issues/I9BETP
这是一个bug报告，主要涉及MindSpore中PolynomialWithWarmUpLR的问题，可能是由于参数传递或计算逻辑错误导致WarmUpLR计算结果与预期不一致。

https://gitee.com/mindspore/mindformers/issues/I9BE4U
这是一个bug报告，主要对象是LR学习率模块，由于缺少参数校验导致了参数与数据类型不一致时未报错或报错信息不合理的问题。

https://gitee.com/mindspore/mindformers/issues/I9B4KY
这个issue类型是bug报告，主要涉及到CANN社区版8.0.rc1、mindformer预训练模型以及运行环境配置，可能由于版本不匹配或其他原因导致训练进程突然停止，没有报错信息。

https://gitee.com/mindspore/mindformers/issues/I9B32O
这是一个bug报告，涉及MindFormers 1.1中came优化器支持优化器并行的问题，compression设置为True时报错ValueError: probabilities contain NaN。这个问题可能是由于概率值中包含NaN导致的。

https://gitee.com/mindspore/mindformers/issues/I9B301
这是一个bug报告，涉及到MindFormers 1.1版本中came优化器支持优化器并行时的入参校验和资料不一致问题。可能由于参数类型不符合文档要求，导致了一系列报错症状。

https://gitee.com/mindspore/mindformers/issues/I9ATOK
这是一个bug报告类型的issue，涉及到mindspore baichuan2模块中命令参数拼写错误和数据集下载说明不清晰的问题。由于命令参数拼写错误和缺少数据下载说明，可能导致用户在数据预处理过程中遇到问题。

https://gitee.com/mindspore/mindformers/issues/I9ASUK
这是一个bug报告类型的issue，主要涉及Baichuan7B使用配套的tokenizer.model处理数据集时报错的问题。可能是由于数据集处理过程中的配置或代码问题导致了RuntimeError。

https://gitee.com/mindspore/mindformers/issues/I9AQZE
这是一个bug报告，主要涉及mindformers下的Baichuan13B全参微调时进行权重切分时报错的问题。由于权重切分时出现数值错误，导致报错"ValueError: Can't find class type models class name Baichuan13BForCausalLM in class registry"。

https://gitee.com/mindspore/mindformers/issues/I9APP4
这是一个bug报告，涉及分布式通讯信息导出时日志打印不正确的问题。由于某种原因导致日志打印Skip dump parallel info，用户希望修复这一问题。

https://gitee.com/mindspore/mindformers/issues/I9AP6R
这是一个用户询问问题类型的issue，主要涉及的对象是910A与baichuan213b的配置兼容性。用户可能由于混淆配置选项而产生困惑，想知道如何在使用910A卡时选择正确的配置。

https://gitee.com/mindspore/mindformers/issues/I9AOJC
这是一个关于模型微调效果问题的用户提问，主要涉及使用baichuan13Bchat进行全参微调时，数据较少导致模型效果不佳的情况。

https://gitee.com/mindspore/mindformers/issues/I9AIKR
这是一个bug报告，主要涉及到codellama 34B 启动serving报错。导致该问题的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/I9AIHG
这是一个bug报告，问题单涉及的主要对象是`mf 1.0+ serving dev，llama70B`，由于未指定的环境信息和执行模式，导致推理结果不正确。

https://gitee.com/mindspore/mindformers/issues/I9AEPJ
这是一个bug报告类型的issue，涉及主要对象是IFlytekSpark的运行脚本run_lite.sh。由于执行/bin/bash run_lite.sh时报错，可能是由于脚本中的错误或Docker环境设置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I9AD21
这是关于bug报告类型的issue，涉及主要对象为文档内容。原因是文档中部分内容复制粘贴导致信息错误。

https://gitee.com/mindspore/mindformers/issues/I9A3Y7
这是一个bug报告，涉及主要对象为llama213B训练过程中在Atlas800T A2设备上遇到bfloat16训练报错的问题。错误可能由于CANN 7.0.0.beta1和mindspore2.2环境不兼容导致。

https://gitee.com/mindspore/mindformers/issues/I9A3S6
这是一个bug报告类型的issue，主要涉及到训练过程中出现loss为NaN的问题，可能是由于某些原因导致了模型训练过程中出现了异常。

https://gitee.com/mindspore/mindformers/issues/I9A3GS
这是一个bug报告，主要涉及Mindformers中GLM2推理的错误。导致这个bug的原因可能是开启FA功能时出现问题，关闭FA功能后可以正常推理。

https://gitee.com/mindspore/mindformers/issues/I9A194
这是一个关于使用mindspore中trainer模块时遇到错误的bug报告，问题出现在数据pipeline不是树形结构导致。

https://gitee.com/mindspore/mindformers/issues/I99XWP
这是一个bug报告，涉及到MindSpore框架中的异常错误，由于框架的异常导致了此问题。

https://gitee.com/mindspore/mindformers/issues/I99U8B
这是一个bug报告类型的issue，主要涉及CHATGLM3精度调整的问题，用户询问如何调整精度以及如何确认当前使用的精度，由于修改配置文件中的参数没有效果。

https://gitee.com/mindspore/mindformers/issues/I99U4J
这是一个bug报告类型的issue，主要涉及iFlytekSpark的离线推理过程中运行bin/bash run_lite.sh时报错，可能是因为model path设置错误导致的。

https://gitee.com/mindspore/mindformers/issues/I99SHW
这是一个bug报告，涉及主要对象是iFlytekSpark。由于执行离线推理时报错，用户寻求如何处理该报错的帮助。

https://gitee.com/mindspore/mindformers/issues/I99S13
这是一个bug报告类型的issue，主要涉及到mindspore lite在Atlas800T A2训练服务器上推理报错的问题，导致报错的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/I99OC7
这是一个bug报告，主要对象是WikiText2数据集链接不可用。由于给出的数据集链接无效，导致无法获取预期的结果。

https://gitee.com/mindspore/mindformers/issues/I99JR1
This is a bug report related to exporting a mindir model failure in the context of serving development without post-training quantization in MindSpore environment due to unknown reasons.

https://gitee.com/mindspore/mindformers/issues/I99JPU
这是一个bug报告类型的issue，主要涉及mindformers中训练llama时出现的RuntimeError: Failed to get level by input node错误，根据描述推测可能是由于设置不同的参数dp、mp、pp导致的问题。

https://gitee.com/mindspore/mindformers/issues/I99I55
这个issue是一个bug报告，涉及到裸金属训练glm3时出现加载任务失败的错误，可能由于某种原因导致导致该任务无法成功加载。

https://gitee.com/mindspore/mindformers/issues/I99GWI
这个issue类型是bug报告，主要涉及对象是模型推理过程。由于输入长度超过了模型配置的最大长度，导致报错提示长度超出限制。

https://gitee.com/mindspore/mindformers/issues/I99CCW
这是一个需求问题，用户提出GLM2是否支持多轮对话的问题，以及关于文档生成不全的疑问。

https://gitee.com/mindspore/mindformers/issues/I999QK
这是一个bug报告，涉及到llama7b预训练过程中运行脚本出现问题，可能由于配置或脚本错误导致分布式训练失败。

https://gitee.com/mindspore/mindformers/issues/I98YYY
这是一个Bug报告issue，涉及mindspore 2.2和mindformers 1.0.0，在全量微调llama2时开启flashattention后报错TypeError: For primitive[FlashAttentionScore], the input type must be same。导致报错的原因可能是输入数据类型不一致。

https://gitee.com/mindspore/mindformers/issues/I98WVQ
该issue为bug报告类型，涉及的主要对象是internlm_20b多卡双动态lite推理导出mindir报错，可能是由于环境配置或代码逻辑的问题导致了该bug的出现。

https://gitee.com/mindspore/mindformers/issues/I98VMF
这个issue是用户提出需求，主要涉及到如何将mindformers的日志输出到k8s的pod准输出打印，使用kubectl logs查看日志。

https://gitee.com/mindspore/mindformers/issues/I98R66
这是一个询问问题类型的issue，主要涉及对象是ERNIE3.0大模型华为昇腾，用户询问是否已经适配。由于适配情况不明确，用户提出了相关问题。

https://gitee.com/mindspore/mindformers/issues/I98PZO
这个issue类型为bug报告，涉及主要对象为gitee上的mindformers下的一个算子rmsnorm。由于rmsnorm算子不支持手动配并行，调用shard接口不生效，导致了这个bug的症状。

https://gitee.com/mindspore/mindformers/issues/I98LKT
这是一个Bug报告，用户反映微调模型效果几乎为0，怀疑权重未加载导致。

https://gitee.com/mindspore/mindformers/issues/I98LBX
这是一个bug报告，主要涉及mindformers项目下运行Qwen7b全参微调时出现RuntimeError的问题，可能是由于配置或依赖版本不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/I98KRS
这是一个bug报告，涉及的主要对象是iflytekspark的运行环境。由于缺少mpi4py依赖项，导致无法成功运行相关脚本，用户在寻求解决方案。

https://gitee.com/mindspore/mindformers/issues/I98IQG
这个issue类型是bug报告，涉及的主要对象是run_mindformer和pipleine接口支持Batch推理特性。由于代码逻辑错误或者参数设置不当，导致test_mindformers_predict_batch_size_2的结果与预期不符。

https://gitee.com/mindspore/mindformers/issues/I98FE6
这是一个bug报告，主要涉及的对象是在推理glm3时生成的内容与问题不相关且大量重复，可能是由于模型推理过程中的某些问题导致的。

https://gitee.com/mindspore/mindformers/issues/I98BAJ
这是一个关于如何查看模型训练日志的需求问题，主要涉及到用户对于大模型训练日志查询的困惑。

https://gitee.com/mindspore/mindformers/issues/I98B7U
这是一个bug报告，涉及ModelArts下的run_distribute.sh脚本，由于脚本问题导致运行时出现错误。

https://gitee.com/mindspore/mindformers/issues/I98B6E
这是一个bug报告，涉及到百川7b ckpt权重转换过程缺少必要步骤以及相关库，可能导致缺少torch和transformers库导致转换错误的问题。

https://gitee.com/mindspore/mindformers/issues/I98B57
这是一个bug报告类型的issue，主要涉及百川7B文档的描述问题，由于缺少配置文件导致无法提供正确的输入图片说明。

https://gitee.com/mindspore/mindformers/issues/I98934
这是一条bug报告，涉及iflytekspark模型相关问题咨询。问题可能是由于参数格式转换不完整导致的TypeError错误。

https://gitee.com/mindspore/mindformers/issues/I987V7
这个issue是一个bug报告，主要涉及的对象是chatweb脚本和glm6b的权重，导致识别效果异常的bug。

https://gitee.com/mindspore/mindformers/issues/I987T3
这是一个bug报告，主要涉及mindformers工具中运行hccl_tools.py生成RANK_TABLE_FILE文件时出现的报错。可能是由于环境设置或代码逻辑问题导致的生成文件错误。

https://gitee.com/mindspore/mindformers/issues/I97K7E
这是一个bug报告，涉及到mindspore在推理过程中出现显存不足的问题。

https://gitee.com/mindspore/mindformers/issues/I97CH2
这是一个bug报告类型的issue，主要涉及到在进行推理过程中无法找到modelfoundry_hub模块，可能是由于环境配置或代码调用错误导致的。

https://gitee.com/mindspore/mindformers/issues/I97BNF
这是一个bug报告类型的issue，主要涉及MindSpore在910A上使用glm3全参微调后的权重文件进行推理效果不好的问题。造成这种情况可能是由于软件版本或配置不匹配所致。

https://gitee.com/mindspore/mindformers/issues/I97AOO
这是一个bug报告，涉及Llama2推理现象时出现重复和无法停止的问题，可能是由于程序逻辑错误或者数据处理异常导致。

https://gitee.com/mindspore/mindformers/issues/I97982
这是一个用户提出需求的问题，主要涉及web_chat中推理服务相关的适配问题。由于当前chatglm2不支持glm2_6b_lora和llama213b不支持llama2_13b_lora推理，导致用户无法使用微调后的权重进行推理，因此提出了如何自主适配的疑问。

https://gitee.com/mindspore/mindformers/issues/I9707W
这是一个bug报告，主要涉及MindSpore模型导出失败的问题。可能由于代码错误或环境配置问题导致。

https://gitee.com/mindspore/mindformers/issues/I96XK1
这是一个bug报告，涉及MindFormer模型导出过程中出现的错误。由于代码中涉及的模型配置问题导致无法正常导出模型。

https://gitee.com/mindspore/mindformers/issues/I96WVE
这是一个bug报告，涉及到ChatGLM3模型训练报错的问题，原因可能是在预处理阶段出现错误导致无法成功运行图表。

https://gitee.com/mindspore/mindformers/issues/I96RSS
这是一个bug报告，涉及的主要对象是VisualGLM的版本依赖环境。原因是tokenizers和transformers的版本不兼容，导致无法安装合适的tokenizers版本。

https://gitee.com/mindspore/mindformers/issues/I96PYL
这是一个关于运行配置问题的bug报告，涉及到模型 glm32k，用户询问max_device_memory参数设置是否有问题导致内存不足的报错。

https://gitee.com/mindspore/mindformers/issues/I96PPZ
这是一个bug报告，涉及ChatGLM3模型训练时出现的数据对不齐错误，由于加载的checkpoint与模型参数的shape不匹配导致。

https://gitee.com/mindspore/mindformers/issues/I96PMW
这是一个bug报告，主要涉及到ChatGLM3Tokenizer对象，可能由于代码中缺少build_prompt属性导致了该bug。

https://gitee.com/mindspore/mindformers/issues/I96KHN
这是一个bug报告，主要涉及MindSpore在单卡推理中显示killed导致推理异常，可能由于版本不支持某些特性导致。

https://gitee.com/mindspore/mindformers/issues/I96KHF
这是一个bug报告，该问题涉及MindSpore中的推理异常，由于不支持的操作符导致了报错信息。

https://gitee.com/mindspore/mindformers/issues/I96K5E
这是一个bug报告，该问题涉及Qwen14B全参微调功能，运行时出现配置文件错误导致报错。

https://gitee.com/mindspore/mindformers/issues/I96JMP
这是一个bug报告，涉及到使用双机训练微调后合并权重时缺少rank_15权重的问题。由于缺少了rank_15权重导致执行报错。

https://gitee.com/mindspore/mindformers/issues/I96BO9
这是一个bug报告，主要涉及qwen_7B模型在alpaca数据集上推理效果需要优化的问题。由于受训练数据的影响，导致推理效果差，需要进一步优化。

https://gitee.com/mindspore/mindformers/issues/I96ARM
这是一个bug报告，涉及到mindformers运行qwen微调时出现的报错。这个问题可能是由于模型编译阶段的错误导致的。

https://gitee.com/mindspore/mindformers/issues/I9671P
这是一个bug报告，涉及到gitee上mindformers项目中的deepseek llama233b模型，在多卡推理过程中生成的token都是相同的，而不同输入文本应生成不同的token，可能是由于推理过程中出现了重复的逻辑或者配置错误导致的。

https://gitee.com/mindspore/mindformers/issues/I965ND
这是一个Bug报告，涉及MindSpore Lite在mindformers r1.0分支下导出和推理出现问题，可能由于导出错误导致推理报错。

https://gitee.com/mindspore/mindformers/issues/I965GB
这是一个bug报告，涉及对象是llama7b_lora模型在alpaca数据集上的训练过程。问题是由于收敛不够导致loss无法降低，微调后准确率还不如原先的llama模型。

https://gitee.com/mindspore/mindformers/issues/I964GA
这是一个bug报告类型的issue，涉及主要对象为mindformers中的blip2模块。由于参数类型转换不一致以及部分权重未加载导致输出结果错误。

https://gitee.com/mindspore/mindformers/issues/I961GT
这是一个用户提出需求的issue，主要涉及mindformers库中的Streamer接口，用户希望该接口支持自定义StoppingCriteria功能。

https://gitee.com/mindspore/mindformers/issues/I95UZF
这是一个bug报告，主要涉及MS2.3在动态lite推理时出现的shape size overflow错误。这个问题可能由于参数的无效性导致，最终导致报错overflow。

https://gitee.com/mindspore/mindformers/issues/I95TAD
这是一个关于bug报告的issue，涉及主要对象是Qwen14b精度设置bfloat16，在转换权重后在运行时报错。

https://gitee.com/mindspore/mindformers/issues/I95QHF
这是一个bug报告，涉及MindSpore 2.2.12版本训练GPT3模型失败，报错空指针。可能是由于软件环境或代码逻辑错误导致的。

https://gitee.com/mindspore/mindformers/issues/I95B6J
这是一个bug报告，主要对象是mindspore运行在ModelArts的开发环境notebook下的Ascend910环境。由于缺少特定的符号"_ZN2ge7PromoteC1ERKSt16initializer_listIPKcE"，导致加载动态库libmindspore_ascend.so.1失败，出现报错信息。

https://gitee.com/mindspore/mindformers/issues/I95B6I
这是关于bug报告的issue，主要涉及mindspore在Ascend910环境下出现undefined symbol错误，可能由于版本兼容性问题导致该bug。

https://gitee.com/mindspore/mindformers/issues/I953YD
这是一个bug报告，涉及模型转换后的运行结果出现异常。原因可能是合成权重导致所有样本的推理结果相同。

https://gitee.com/mindspore/mindformers/issues/I952NR
这是一个bug报告类型的issue，主要涉及的对象是BLIP2模型在使用AutoClass时出现了错误。由于代码中存在错误或不完整的操作，导致了模型在生成文本过程中出现了错误。

https://gitee.com/mindspore/mindformers/issues/I94WO6
这是一个需求询问类型的issue，涉及模型转换的问题，由于评估时参数配置不同于训练时，用户询问如何进行模型转换。

https://gitee.com/mindspore/mindformers/issues/I94V4U
这是一个bug报告类型的issue，涉及主要对象是llama2模型。由于开启细粒度多副本后，loss收敛速度比不开慢，需要分析原因并解决。

https://gitee.com/mindspore/mindformers/issues/I94V1T
这是一个bug报告，主要对象是llama213b在开启细粒度多副本后出现内存不足的问题。这可能是由于参数设置不当导致的bug。

https://gitee.com/mindspore/mindformers/issues/I94U37
这个issue类型是用户询问问题，主要涉及gradient_aggregation_group参数与dp/mp/pp值之间的关系，可能由于对这些参数之间的关系和影响不清楚而引发提问。

https://gitee.com/mindspore/mindformers/issues/I94U1F
这是一个bug报告，涉及到不同的并行配置导致的收敛性问题，用户在使用不同的并行配置时，发现在特定配置下模型不收敛，但在另一配置下却可以收敛。

https://gitee.com/mindspore/mindformers/issues/I94TUX
这是一个bug报告，主要涉及MindSpore在尝试双动态推理时无法成功导出WizardCoder 34B为mindir模型。原因可能是配置中添加了use_kvcache_op: True导致的问题。

https://gitee.com/mindspore/mindformers/issues/I94S6K
这是一个bug报告类型的issue，主要涉及到百川2多卡推理过程中模型转换报错无法找到ckpt文件，可能由于软链接的ckpt不是一个模型文件导致第一张卡进程down掉，其他卡卡在模型转换且进度一直停留在0%。

https://gitee.com/mindspore/mindformers/issues/I94RUB
这是一个Bug报告，涉及MindFormer中llama2 7b模型在推理过程中出现乱码的问题。这个问题可能是由于推理过程中的某些环境设置或配置错误导致的。

https://gitee.com/mindspore/mindformers/issues/I94RQG
这是一个bug报告，涉及mindformers下的一个issue，用户使用lora微调后的权重进行推理时出现无结果的问题。

https://gitee.com/mindspore/mindformers/issues/I94MGW
这个issue属于用户提出问题类型，主要涉及的对象是在Ascend平台安装torch后能否在atlas平台上使用deepspeed/megatron等并行策略。由于用户想知道在atlas平台上是否能够使用torch的dp/mp/pp，并行策略，表明用户希望实现在atlas平台上利用相关并行策略进行深度学习任务。

https://gitee.com/mindspore/mindformers/issues/I94M5S
这是一个bug报告，主要涉及到ms2.1.1 + python3.9 + Ascend910A环境下的推理评估，出现数值长度不匹配的数值错误。

https://gitee.com/mindspore/mindformers/issues/I94I7O
这是一个bug报告，该问题涉及的主要对象是baichuan2推理结果为None。由于生成速度过慢，推理结果为None，可能是由于程序执行时间过长导致的。

https://gitee.com/mindspore/mindformers/issues/I94GCM
这是一个bug报告，涉及iflytekSpark13b模型单卡推理出现error code：1343225857的问题。可能由于环境配置、版本不兼容等原因导致的报错。

https://gitee.com/mindspore/mindformers/issues/I94G9L
这是一个bug报告，主要涉及到Qwen14B在生成推理时出现的报错。这个问题可能是由于代码中的初始化错误导致的，需要进一步调查原因。

https://gitee.com/mindspore/mindformers/issues/I94BL4
这是一个Bug报告，涉及主要对象为InternLMForCausalLM模块，由于InternLMConfig对象缺少'pop'属性导致AttributeError错误。

https://gitee.com/mindspore/mindformers/issues/I945FY
该issue属于用户提出需求类型，主要涉及对象为使用glm2作为网页前端和后端模型的桥连接。用户询问是否能通过glm2生成json文件，并希望实现类似告诉小爱设定闹钟的功能，但是以文本形式进行，希望得到大佬们的意见和建议。

https://gitee.com/mindspore/mindformers/issues/I93WAN
这是一个bug报告，主要涉及MindSpore中llama2mindir格式推理功能的使用问题，由于infer_seq_length与max_length参数设置不当导致了RuntimeError错误。

https://gitee.com/mindspore/mindformers/issues/I93TP9
这是一个bug报告类型的issue，涉及Qwen14B在mindformers下的generate和batch推理速度差异的问题。

https://gitee.com/mindspore/mindformers/issues/I93SGG
这是一个bug报告，主要涉及docker pull以及ctr images pull拉取镜像失败的问题。这个问题可能由于http请求失败导致无法拉取指定版本的镜像。

https://gitee.com/mindspore/mindformers/issues/I93SB1
这是一个bug报告类型的issue，主要涉及的对象为量化工具包的存放地址。由于只有最新版2.3在910A测试有问题，2.2版本测试正常，用户希望将历史版本也放在链接里。

https://gitee.com/mindspore/mindformers/issues/I93P2O
这是一个bug报告类型的issue，主要涉及的对象是Ascend910A GPU。因为overflow一直是True且lr一直是0，导致模型无法得到更新，用户寻求解决该问题的帮助。

https://gitee.com/mindspore/mindformers/issues/I93H16
这是一个bug报告类型的issue，主要涉及MindFormers中的CausalLanguageModelDataset，在使用特定模型进行微调训练时出现了TypeError，导致参数错误。

https://gitee.com/mindspore/mindformers/issues/I93E8Q
这是一个用户提出需求的issue，主要对象是Mindformer trainer功能，用户想了解其是否支持amp。

https://gitee.com/mindspore/mindformers/issues/I93D6G
这是一个bug报告，主要涉及MindSpore升级到版本ms2.2.11后在编译阶段直接挂了，没有输出错误信息。可能的原因是与安装canntoolkits和cannkernels包相关。

https://gitee.com/mindspore/mindformers/issues/I931W3
这是一个bug报告，主要涉及llama7b_lora在MMLU数据集上单卡推理评估时空间溢出的问题，可能是由于循环推理过程中缓慢增长导致。

https://gitee.com/mindspore/mindformers/issues/I92YVZ
这是一个bug报告类型的issue，主要涉及到使用PyInstaller工具打包mindspore_lite项目后无法使用的问题。

https://gitee.com/mindspore/mindformers/issues/I92Y0E
这是一个bug报告，涉及MindSpore Lite在使用多batch推理时，流式输出结果混乱的问题。原因可能是接口还未支持这种方式导致的。

https://gitee.com/mindspore/mindformers/issues/I92XVK
这是一个关于输出胡言乱语的bug报告，主要对象是推理过程中输出异常的问题。由于训练时使用了较长的seq长度，可能导致推理阶段模型无法正确解码，从而输出了无意义的文本。

https://gitee.com/mindspore/mindformers/issues/I92TCK
这是一个bug报告，主要涉及mindspore2.1.1在Ascend910A llama7b_lora上进行推理时遇到EL0009错误，原因是流资源不足。

https://gitee.com/mindspore/mindformers/issues/I92QUQ
这是一个bug报告类型的issue，主要涉及mindformers仓库中的Qwen模型推理执行过程中的报错问题，由于QwenConfig对象缺少属性'compute_in_2d'导致推理脚本无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I92PPZ
这是一个关于软件镜像下载的需求提出类型的issue，主要涉及mindformerr1.0 x86官方镜像问题，由于缺乏x86版本的官方镜像，用户询问如何在x86机器上运行mindformerr1.0 lite推理的代码。

https://gitee.com/mindspore/mindformers/issues/I92KUU
这是一个用户询问问题类型的issue，主要对象是关于mindformer r1.0的官方镜像及使用问题，由于x86机器无法找到对应镜像，用户询问如何在x86机器上运行mindformer r1.0代码。

https://gitee.com/mindspore/mindformers/issues/I92K70
这是一个bug报告，涉及到Qwen7b按照文档进行CEval评测报错的问题，可能是由于代码运行错误或者参数设置问题导致报错。

https://gitee.com/mindspore/mindformers/issues/I92IJG
这是一个用户询问类型的issue，主要涉及对象是Mindformers是否支持昇腾310P卡的chatglm36b大模型的推理，用户在此寻求关于兼容性的帮助。

https://gitee.com/mindspore/mindformers/issues/I92E0V
这是一个bug报告，主要涉及了mindformer版本的Baichuan213B模型精度低于pytorch版本的情况。原因可能是模型实现或参数设置不一致导致不同框架下的精度差异。

https://gitee.com/mindspore/mindformers/issues/I92AMA
这是一个用户提出需求类型的issue，主要涉及的对象是如何将pytorch版本的GPT-2使用的inputs_embeds转换成mindformers的GPT-2要求的input_ids，并将新的数据放入mindformers的GPT-2进行训练。

https://gitee.com/mindspore/mindformers/issues/I9298X
这是一个bug报告，涉及问题类型为模型全参微调错误，主要对象为baichuan213b模型运行环境。可能由于数据集或模型配置问题导致了RuntimeError: Preprocess failed before run graph 1错误。

https://gitee.com/mindspore/mindformers/issues/I927NI
这是用户提出的关于需求的问题，涉及到MindFormers和Ascend Transformer Boost库之间的联系，询问是否会集成相关的量化推理支持。

https://gitee.com/mindspore/mindformers/issues/I926MO
这是一个bug报告，涉及qwen全参微调过程中报memory not enough的问题。由于内存不足导致此bug。

https://gitee.com/mindspore/mindformers/issues/I922VT
这是一个bug报告，主要对象为Qwen1.5的7B和14B在910A推理过程中出现乱码。这可能是由于新发布的Qwen1.5与MindSpore版本2.2.11在特定环境下的不兼容性所导致的。

https://gitee.com/mindspore/mindformers/issues/I91T78
这是一个bug报告，主要对象是llama7b断点续训时日志显示的epoch与所加载ckpt的epoch不一致，可能由于加载最新ckpt进行断点续训时出现了epoch不对应的情况。

https://gitee.com/mindspore/mindformers/issues/I91SZB
这是一个bug报告，主要涉及llama7b断点续训加载不完整的ckpt导致训练失败的问题。

https://gitee.com/mindspore/mindformers/issues/I91QL6
这是一个bug报告，主要涉及mindformers/mindformers/dataset/dataloader/training_dataloader.py代码中的LLM数据在线加载功能问题。问题出现在_read_dataset函数中，在处理不同格式数据时出现了错误。

https://gitee.com/mindspore/mindformers/issues/I91MX8
这是一个bug报告，涉及的主要对象是MindForers中的llama13b模型。由于代码运行过程中出现了train函数错误导致的报错，可能是参数传递或调用过程中的问题。

https://gitee.com/mindspore/mindformers/issues/I91GBI
这是一个Bug报告，主要涉及MindSpore 2.3在并行模式下出现"init_context failed in parallel mode"错误。可能是由于未正确进行D.init操作导致的。

https://gitee.com/mindspore/mindformers/issues/I91DW2
这是一个bug报告，涉及主要对象为预训练模型llama13b，由于loss scale下降为1后lr不再变化，导致训练吞吐量低以及lr变化异常。

https://gitee.com/mindspore/mindformers/issues/I91AOQ
这是一个bug报告，涉及主要对象是deepseekcoder模块。推理结果出现乱码的症状可能是由于权重转换存在问题导致的。

https://gitee.com/mindspore/mindformers/issues/I919Z6
This is a bug report for the issue that inference results are not guaranteed to be unique when using a large model with `do_sample=False` parameter.

https://gitee.com/mindspore/mindformers/issues/I918CH
这是一个关于功能需求的issue，主要涉及mindformers对模型推理量化的支持问题，用户想知道这种方法是否支持分布式推理。由于mindformers目前是否支持模型推理的量化尚不明确，用户提出了此问题。

https://gitee.com/mindspore/mindformers/issues/I9182Y
这是一个bug报告，涉及主要对象是使用GPT2.15B模型进行单卡推理时出现的错误信息。由于加载的参数字典在过滤或指定后为空，可能是由于设置的 'filter_prefix' 或 'specify_prefix' 不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I917TO
这是一个bug报告类型的issue，涉及主要对象是clip模型转换流程，由于openai官方给的模型pt文件是serialization的torchscript，导致无法成功使用原先的模型转换代码。

https://gitee.com/mindspore/mindformers/issues/I915YC
这是一个bug报告，主要涉及的对象是使用高阶接口进行多卡推理的用户。由于参数设置错误导致出现了"pp only support training process for now, set pp=1 to {mode}"的报错提示。

https://gitee.com/mindspore/mindformers/issues/I915SW
这是一个 bug 报告，主要涉及 qwen14b 在推理时出现报错的问题。原因可能是使用环境为 Ascend 910 并且使用的是 MindSpore 2.2.11 版本。

https://gitee.com/mindspore/mindformers/issues/I913V4
这是一个Bug报告，主要涉及的对象是GPT-2 13B模型权重合并的过程。由于使用`max_shard_size`参数时出现错误，导致生成的文件是文件夹而非文件。

https://gitee.com/mindspore/mindformers/issues/I9116L
这是一个bug报告，涉及到baichuan2 13b lite模型在单卡推理精度和预期不一致的问题。导致该问题的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/I90SBK
这是一个bug报告，主要涉及mindformers中使用llama13b进行pipeline推理时出现的错误。由于参数设置或者代码逻辑问题导致了报错。

https://gitee.com/mindspore/mindformers/issues/I90O0U
这个issue属于需求提出类型，主要涉及Qwen72b多卡并行推理任务配置的上库时间，由于之前提及的上库时间不同步而提出希望同步的需求。

https://gitee.com/mindspore/mindformers/issues/I90NK3
这是一个bug报告，涉及到mindformers的chatglm模型微调时出现的报错。导致该问题的原因可能是模型连接或代码中的错误。

https://gitee.com/mindspore/mindformers/issues/I90I7L
这个issue类型是bug报告，涉及的主要对象是mindformers和mindspore。由于mindformers r0.8和mindspore版本不匹配，导致双机无法启动，但升级为mindformers r1.0后问题解决。

https://gitee.com/mindspore/mindformers/issues/I90GRS
这是一个Bug报告，涉及对象为Baichuan213BChat项目，在推理时生成结果异常，可能由于环境配置或模型导出的问题导致。

https://gitee.com/mindspore/mindformers/issues/I90G01
这是一个bug报告，该问题涉及GLM3微调过程中出现的“E30008: AI CPU operator execution time out.”错误，可能是由于程序执行超时导致的。

https://gitee.com/mindspore/mindformers/issues/I9093O
这是一个bug报告，涉及mindformers项目下的baichuan213b全量微调功能，主要问题可能是在运行`run_singlenode.sh`时出现了报错。

https://gitee.com/mindspore/mindformers/issues/I908WT
这是一个bug报告类的issue，涉及主要对象为llama2 70B模型的权重自动切分为8卡分布式权重报错。问题是由于type字段配置错误导致的。

https://gitee.com/mindspore/mindformers/issues/I90895
该issue是用户提出需求的类型，主要涉及多机多卡的分布式训练任务。用户询问如何在不使用万兆组网的情况下实现数据并行功能，希望通过千兆以太网传递数据，主要困扰在于卡间不直连的情况下如何处理传递流程。

https://gitee.com/mindspore/mindformers/issues/I907ZQ
这是一个bug报告，主要涉及到baichuan2模块在调整seq_length和max_decode_length为4096后出现报错的问题。

https://gitee.com/mindspore/mindformers/issues/I9079Y
这是一个bug报告，涉及对象为MindSpore中的lite导出和推理功能。由于lite导出和推理出现问题，导致了报错和推理失败的症状。

https://gitee.com/mindspore/mindformers/issues/I8ZYTI
这是一个bug报告，主要涉及的对象是glm2 lora微调功能。问题可能是由于操作不当或软件bug导致./output/strategy目录为空。

https://gitee.com/mindspore/mindformers/issues/I8ZSQV
这是一个bug报告，涉及主要对象是在Windows11环境下安装MindSpore时遇到编译错误，可能由于MindFormers未在昇腾设备上安装而导致。

https://gitee.com/mindspore/mindformers/issues/I8ZRU9
这个issue类型是关于需求的问题，主要涉及的对象是atlas 800推理服务器（型号3000），用户想要知道是否支持部署mindformers用于完成微调的大模型部署。

https://gitee.com/mindspore/mindformers/issues/I8ZM00
这是一个bug报告，主要涉及的对象是torch和mindspore。由于torch和mindspore版本冲突导致内存分配错误，产生了OSError。

https://gitee.com/mindspore/mindformers/issues/I8ZKUC
这是一个bug报告，涉及到`llama7b_lora`模型在不同`batch_size`下编译时出现错误的问题。可能是由于`batch_size`为16时导致模型构建过程中出现环路，而`batch_size`为4时则没有出现这个错误。

https://gitee.com/mindspore/mindformers/issues/I8ZK63
这是一个bug报告，涉及到使用InternLM20B lite进行多卡双动态推理导出mindir报错的问题，可能由于软件环境或代码实现的问题导致此bug。

https://gitee.com/mindspore/mindformers/issues/I8ZK3Y
这是一个bug报告，涉及到baichuan2 lite 多卡双动态推理导出mindir时遇到的报错问题，可能由于执行模式为graph导致。

https://gitee.com/mindspore/mindformers/issues/I8ZA8I
这是一个bug报告类型的issue，主要涉及到mindformers库中的llama2模块。由于非autoparallel模式下训练llama2时出现报错，可能是某种环境配置或代码逻辑问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8ZA0Z
这是一个bug报告，主要涉及llama2训练的默认配置中学习率不符合原始论文。用户提出疑问原因为配置中的学习率值为5e5，而原论文使用3e4，用户想知道未遵循原始原则的原因。

https://gitee.com/mindspore/mindformers/issues/I8Z7EX
这是一个用户提出需求的issue，主要涉及GLM3框架缺乏预训练功能。由于青岛AICC头部客户采购算力，需要进行GLM36b模型的增量预训练，因此提出了该需求。

https://gitee.com/mindspore/mindformers/issues/I8Z201
这是一个bug报告，涉及对象是baichuan2的预训练数据集构造labels的方式，用户希望修复计算loss报错的问题。

https://gitee.com/mindspore/mindformers/issues/I8YXUT
这是一个bug报告，涉及神经网络模型参数设置导致OOM问题。

https://gitee.com/mindspore/mindformers/issues/I8YXL9
这是一个bug报告类型的issue，涉及到完整权重自动切分为2卡分布式权重失败的问题。原因可能是离线切分Baichuan213BChat.ckpt权重时出现了错误。

https://gitee.com/mindspore/mindformers/issues/I8YUY1
这是一个bug报告类型的issue，主要对象是qwen7b模型在进行lora微调时出现了"Preprocess failed before run graph 1"错误。

https://gitee.com/mindspore/mindformers/issues/I8YLMM
这是一个bug报告，涉及的主要对象是昇腾310P部署chatglm26b推理报错。由于MindSpore版本与Ascend AI软件包版本不匹配，导致出现了报错信息。

https://gitee.com/mindspore/mindformers/issues/I8YLMA
这是一个用户提出需求的issue，主要涉及到mindformers/pipeline/text_generation_pipeline.py文件，由于可能存在逻辑不统一的问题导致用户提出需要统一写一个逻辑。

https://gitee.com/mindspore/mindformers/issues/I8YLJT
这是一个关于bug报告的issue，主要涉及mindspore对qwen14Bchat模型训练的支持问题。由于loss下降不正常，用户寻求帮助。

https://gitee.com/mindspore/mindformers/issues/I8YLIA
这是一个bug报告，主要涉及到Baichuan213Bbase模型在训练128卡自有数据集时出现失败的问题，可能是由于客户自有数据集大小较大导致一键重建时出现报错。

https://gitee.com/mindspore/mindformers/issues/I8YKR1
该issue类型为用户提出需求，请教问题，主要涉及对象为在gitee上的mindformers下的mindspore serving。由于缺少大模型权重转换和配置文件写法等问题，用户寻求关于如何在mindspore serving中运行llama系、qwen、百川的指导。

https://gitee.com/mindspore/mindformers/issues/I8YHHA
这是一个用户请教问题的issue，主要涉及到梯度通信算子融合组参数gradient_aggregation_group的具体功能问题，用户想了解其作用是否与gradient accumulation steps相同。

https://gitee.com/mindspore/mindformers/issues/I8YGZP
这是一个用户提出需求的问题单，涉及主要对象是项目代码中的codellama适配。用户询问是否可以参考llama的文档来进行codellama的训练、推理和部署操作。

https://gitee.com/mindspore/mindformers/issues/I8YCYQ
这是一个bug报告，涉及对象为mindformers中的llama_lora并行推理，由于VirtualOutput初始化失败导致了此问题。

https://gitee.com/mindspore/mindformers/issues/I8YC1B
这是一个关于模型权重使用方式的问题，属于用户请教问题类型，主要涉及到MAE模型在分布式训练下，权重保存与使用的问题。导致该问题产生可能是因为用户对多卡训练的结果和权重保存方式不清楚。

https://gitee.com/mindspore/mindformers/issues/I8YAJ3
这是一个bug报告，主要对象是llama2在使用mindsporelite进行推理时出现的报错问题。可能由于代码实现不当或者mindsporelite配置问题导致了该bug的出现。

https://gitee.com/mindspore/mindformers/issues/I8Y7P6
这是一个性能问题的用户反馈，涉及到在mindspore lite（910A）上运行Qwen 7B/14B模型的推理速度异常缓慢，可能是由于硬件资源利用不足或算法优化问题导致。

https://gitee.com/mindspore/mindformers/issues/I8Y5MF
这是一个bug报告类型的issue，涉及的主要对象是GLM2模型的8卡并行增量推理。由于开启半自动并行和full_batch后，推理失败，需要分析原因并解决问题。

https://gitee.com/mindspore/mindformers/issues/I8Y5JF
这是一个功能需求提出的issue，主要涉及GLM模型支持dp/mp并行推理的功能。可能由于当前版本尚不支持，导致用户提出该需求。

https://gitee.com/mindspore/mindformers/issues/I8Y2XO
这是一个bug报告，主要涉及Baichuan213BChat模型基于generate对已经切分权重多卡推理时报错。原因可能是使用已经切分好的8卡权重进行推理时出现了错误。

https://gitee.com/mindspore/mindformers/issues/I8Y1IK
这是一个用户提出需求的issue，主要涉及单机四卡执行Baichuan27BChat单卡多轮对话推理时，控制chat多轮对话模型返回文本长度的参数。导致用户提出这个问题可能是长回答文本只返回部分结果。

https://gitee.com/mindspore/mindformers/issues/I8Y0VU
该issue属于用户提出需求的类型，主要涉及对象是数据集转换方式。用户提出问题是如何将中英文PICO标注数据转换成可以进行baichuan27b全参微调的格式。

https://gitee.com/mindspore/mindformers/issues/I8XY7W
这是一个bug报告，主要涉及mindformers在更新到最新版本后训练失败的问题，可能是由于训练任务自动中断导致进程终止，但用户没有收到任何提示。

https://gitee.com/mindspore/mindformers/issues/I8XVES
这是一个bug报告，涉及mindformers下的一个issue，报告在使用两卡进行baichuan213b的"基于Generate推理"时出现RuntimeError。Bug的原因可能是参数lm_head.weight存在多个用户，但TensorInfo不同。

https://gitee.com/mindspore/mindformers/issues/I8XUCF
这是一个bug报告，主要涉及mindsporelite在推理过程中报错的问题，可能由于生成内容为空导致。

https://gitee.com/mindspore/mindformers/issues/I8XIYY
这是一个bug报告，涉及chatglm2模型在推理过程中结果被截断的问题，可能是由于推理时出现了截断导致。

https://gitee.com/mindspore/mindformers/issues/I8XGIR
这是一个bug报告类型的issue，主要涉及baichuan213bchat模型推理时出现的异常，报错信息显示`'LowerTriangularMaskWithDynamic' object has no attribute 'bmm'`。原因可能是模型13b版本的代码在运行中出现了相关错误。

https://gitee.com/mindspore/mindformers/issues/I8XF78
这是一个用户提出需求的issue，主要涉及对象是对华为300i系列DUO 96G卡的支持。造成这个issue的原因是用户想了解Mindformers和Lite是否计划支持该系列卡，以及如何在300i没有HCCS情况下进行分布式多卡推理。

https://gitee.com/mindspore/mindformers/issues/I8X891
这是一个bug报告，主要涉及mindspore 2.1.0版本下的mindformers 0.8模块在采用usepast时报错的问题。可能是由于代码逻辑问题导致的错误或者不完整的使用说明引起用户困扰。

https://gitee.com/mindspore/mindformers/issues/I8X42E
这是一个bug报告，主要涉及Baichuan13bchat模型在lite推理结果异常的问题。导致该问题的原因是模型在进行Mindsporelite推理时出现了生成重复子串现象。

https://gitee.com/mindspore/mindformers/issues/I8X3O8
这是一个bug报告，涉及主要对象为mindformers下的16卡跑不起来的问题。可能是因为安装了cann 7.0导致的报错。

https://gitee.com/mindspore/mindformers/issues/I8X14P
这是一个bug报告，主要对象是chatglm3推理速度慢，可能由于mindspore版本和Ascend AI软件版本不匹配导致推理速度慢。

https://gitee.com/mindspore/mindformers/issues/I8WZ1C
这是一个关于技术功能需求的issue，主要涉及到wizardcoder通过chatweb接口只支持聊天模式而不支持FIM模式的问题。

https://gitee.com/mindspore/mindformers/issues/I8WX4L
这是一个bug报告，涉及到mindformers下的telechat项目，用户反馈在处理数据成mindrecord格式时需要的文件tokenizer.model不存在。

https://gitee.com/mindspore/mindformers/issues/I8WV2Z
这是一个bug报告，涉及的主要对象是使用streamlit加载MindSpore模型时出现报错。这个问题可能是由于模型加载过程中出现了异常导致的。

https://gitee.com/mindspore/mindformers/issues/I8WOZQ
这是一个用户提出需求和问题的类型，主要涉及如何启动多卡推理并构建服务的内容。原因可能是用户想了解如何使用多卡推理并将其作为服务的推理入口，以及如何处理固定的输入在每次运行时都传入pipeline的情况。

https://gitee.com/mindspore/mindformers/issues/I8WLFF
这是一个bug报告，涉及主要对象为mindformer的GPT2模型，由于缺少输入数据类型导致了报错。

https://gitee.com/mindspore/mindformers/issues/I8W8I0
这是一个bug报告类型的issue，涉及主要对象为Baichuan213BChat模型在使用MS Lite静态和动态推理时报错。出现此问题可能是由于Mindformers 1.0.0版本与Mindspore 2.2.10及Mindspore_lite 2.2.10版本之间的兼容性问题所致。

https://gitee.com/mindspore/mindformers/issues/I8W6A0
这个issue是一个bug报告，主要涉及baseconfig类的getattr重载实现方法导致baseconfig的hasattr只会返回True，可能是由于重载实现方法的原因导致的。

https://gitee.com/mindspore/mindformers/issues/I8W50T
这是一个bug报告，主要涉及到在910B环境下使用llama模型进行8卡训练时出现了RuntimeError: Graph cycle exists的问题。这可能是由于分布式配置设置或其他原因导致的。

https://gitee.com/mindspore/mindformers/issues/I8W49O
这是一个bug报告，主要涉及的对象是Baichuan27BChat在Atlas 800 9000上推理耗时长。导致这个问题的原因可能是AICore使用率为0，需要进行优化。

https://gitee.com/mindspore/mindformers/issues/I8W3SY
这是一个bug报告，主要涉及mindspore2.1.1和mindformers1.0.0之间的静态图生成analyze_fail.ir文件的问题，可能由于配置修改或其他原因导致无法生成所需的analyze_fail.ir文件。

https://gitee.com/mindspore/mindformers/issues/I8W33X
这是一个bug报告，主要涉及mindsporelite推理过程中出现的报错问题，可能由模型转换过程或输入数据错误导致。

https://gitee.com/mindspore/mindformers/issues/I8VZIJ
这是一个bug报告，问题涉及的主要对象是在910B上微调Baichuan2_13b模型过程中flash_attention参数无法正常开启所导致的问题。

https://gitee.com/mindspore/mindformers/issues/I8VXWK
这是一个bug报告，该问题涉及的主要对象是Lite推理功能。由于多线程无法支持，尝试双动态推理失败，导致出现乱码和报错。

https://gitee.com/mindspore/mindformers/issues/I8VXUG
这是一个bug报告，涉及MindSpore推理过程中出现的内存分配失败问题，导致无法成功运行图。

https://gitee.com/mindspore/mindformers/issues/I8VUY7
这是一个bug报告，涉及到执行llama_preprocess.py脚本时卡住的问题。原因是MindSpore版本与Ascend AI软件包版本不匹配，导致警告信息显示不匹配的版本。

https://gitee.com/mindspore/mindformers/issues/I8VQTF
这是一个bug报告类型的issue，主要涉及到mindformers/tools/hccl_tools.py文件的运行报错问题，可能是由于代码逻辑错误或参数设置不当导致的。

https://gitee.com/mindspore/mindformers/issues/I8VQHW
这是一个bug报告，主要涉及的对象是使用AI加速卡910A进行推理时OOM（Out of Memory）报错的问题。这个问题可能是由于模型参数量过大导致内存不足而引起。

https://gitee.com/mindspore/mindformers/issues/I8VOCO
这是一个bug报告，主要涉及mindformers官网样例代码存在格式问题，可能是由于代码格式不统一导致的。

https://gitee.com/mindspore/mindformers/issues/I8VLA9
这是一个bug报告，涉及的主要对象是mindformers下的lora微调模型。由于没有设置include/exclude字段，导致所有模块都参与微调，从而增大了空间消耗和训练时间。

https://gitee.com/mindspore/mindformers/issues/I8VKWI
这是一个bug报告，主要涉及到mindformers下的llama7b lora微调功能，由于某种原因导致lr一直显示为0.0。

https://gitee.com/mindspore/mindformers/issues/I8VJZY
这是一个Bug报告，主要涉及LLAMA代码中deviceid和RANKID写反导致模型训练失败。

https://gitee.com/mindspore/mindformers/issues/I8VJQ3
这是一个bug报告，主要对象是MindSpore社区中的其他成员。由于代码没有适配训练任务，添加了set_remote_save_url(remote_save_url)，导致ckpt权重无法完全保存或者最后一次不会保存权重文件。

https://gitee.com/mindspore/mindformers/issues/I8VJMD
这是一个询问如何使用LoRA微调后的权重进行推理并合并权重的问题，主要涉及的对象是Ascend910A以及模型的权重处理，用户可能对如何将微调后的权重应用于推理操作以及如何合并权重产生了疑惑。

https://gitee.com/mindspore/mindformers/issues/I8VGTZ
这是一个bug报告，主要涉及到mindspore框架在数据集评估时动态shape导致速度慢的问题。该问题由于数据集中样本长度不一，无法统一使用Model.infer_layerout进行编译，导致每个样本都需要编译，进而导致推理过程耗时。

https://gitee.com/mindspore/mindformers/issues/I8VGN9
这个issue属于问题询问类型，主要涉及对象是获取下一个词的生成概率，可能是由于用户需要执行相关任务而寻求帮助。

https://gitee.com/mindspore/mindformers/issues/I8VG9B
这是一个bug报告，主要涉及Qwen 14b lite 2卡推理报错的问题。由于环境软件配置的因素，导致了推理报错的现象。

https://gitee.com/mindspore/mindformers/issues/I8VBMH
这是一个关于软件bug报告的问题单，主要涉及mindformers新版本更新导致的错误，用户希望知道应该使用哪个版本的mindformers来解决问题。

https://gitee.com/mindspore/mindformers/issues/I8V5FS
这是一个bug报告，涉及到将mt5模型转换为mindspore权重的问题，由于转换脚本中指定的层在mt5模型中不存在，导致转权重失败。

https://gitee.com/mindspore/mindformers/issues/I8V5D9
这是一个bug报告，主要涉及的对象是Llama2_7b 910A单卡，可能由于内存溢出导致使用run_llama2_7b_lora_910b.yaml时报OOM错误。

https://gitee.com/mindspore/mindformers/issues/I8V0XK
这是一个bug报告，涉及的主要对象是GLM2模型。由于开启边训边推模式训练完成后，切分策略文件成为空文件，无法执行权重合并。

https://gitee.com/mindspore/mindformers/issues/I8URBL
这是一个bug报告，主要涉及数据集在半自动并行+full_batch下仍错误shard导致数据拆分不正确的问题。

https://gitee.com/mindspore/mindformers/issues/I8UQP6
这是一个bug报告，主要涉及到在使用mindformers的Chat_Web拉起两卡的baichuan2 13B推理时，不会生成推理结果的问题。原因可能是代码或配置文件存在问题导致此异常症状。

https://gitee.com/mindspore/mindformers/issues/I8UO7V
这个issue属于bug报告，主要涉及Graph模式训练时添加print/logger.info会出现问题，可能是编译器在处理这些函数时需要改进导致的。

https://gitee.com/mindspore/mindformers/issues/I8UI5V
这是一个bug报告，涉及mindformers库中算子错误问题导致推理代码无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I8UFKM
这是一个描述精度问题的bug报告，涉及到baichuan13b模型，在graph执行模式下出现预测准确度不符合预期的情况。

https://gitee.com/mindspore/mindformers/issues/I8UEUN
这是一个用户提出需求类型的问题，主要涉及Mindformers在处理多个MindRecord文件时是否能正确读取并训练的功能。由于用户希望能够让Mindformers正确识别和读取不同数据集分别生成的多个MindRecord文件，因此可能存在对MindRecord文件集成性的需求或者对训练数据的批量处理需求。

https://gitee.com/mindspore/mindformers/issues/I8UCF6
这是一个bug报告，主要涉及mindformers dev mindpore 2.2.0 微调Baichuan213B配置文件run_baichuan2_13b_910b.yaml，由于rank_6、rank_7的error.log报错，用户遇到了报错问题寻求帮助。

https://gitee.com/mindspore/mindformers/issues/I8UAGF
这是一个bug报告，涉及的主要对象是在Ascend910A上使用mindformers进行全参微调时出现不收敛的问题。导致这个问题的原因可能是参数配置或代码实现上的错误。

https://gitee.com/mindspore/mindformers/issues/I8U7H6
这是一个bug报告，该问题涉及mindformers在Linux系统上出现ModuleNotFoundError，并可能是由于运行时环境或依赖配置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I8U785
这是一个bug报告，主要涉及Gitee上Mindformers仓库中的Qwen generate功能在多卡推理时存在问题，导致无法支持预拆分权重。

https://gitee.com/mindspore/mindformers/issues/I8U0L9
这个issue是一个bug报告，涉及的主要对象是千问14B lora微调过程中loss始终为0，可能是由于代码中的某些问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8TYVD
这是一个bug报告，该问题涉及mindformers仓库中readme中的批处理代码执行报错。可能的原因是代码中的错误或环境配置问题导致执行失败。

https://gitee.com/mindspore/mindformers/issues/I8TV58
这是一个功能需求的issue，主要涉及到模型推理性能问题，希望能够增加stop_words_ids参数以提前结束推理，减少推理时延。

https://gitee.com/mindspore/mindformers/issues/I8TUYV
这是一个Bug报告，主要涉及MindFormer中的llama2模型导出错误，原因是报错"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"。

https://gitee.com/mindspore/mindformers/issues/I8TTD0
该issue是关于用户提出需求的，主要对象是mindformers，用户想要自定义evaluation过程，但在readme部分没有找到相关内容，可能是因为缺乏相关文档或指导导致用户无法实现自定义evaluation的功能。

https://gitee.com/mindspore/mindformers/issues/I8TPNK
这是一个bug报告，涉及到MindSpore的模型网络skywork中md，ms方式的开源数据集评测测试报错。由于代码中索引操作`outputs[:, input_len  1, :]`导致了Tensor索引错误，进而触发了报错。

https://gitee.com/mindspore/mindformers/issues/I8TPIT
这是一个bug报告，该问题涉及的主要对象是qwen chat模型。导致该问题的原因是模型无法在generate生成特殊token时直接停止，从而影响了推理性能。

https://gitee.com/mindspore/mindformers/issues/I8TP7L
该issue类型为bug报告，主要涉及swintransformer和visiontransformer的单机多卡和多机多卡运行设置问题，可能由于设置参数错误导致报错。

https://gitee.com/mindspore/mindformers/issues/I8TP5J
这是一个bug报告，涉及SFTDataLoader接口参数校验不在初始化时进行导致的问题。原因是参数类型和范围未经校验可能导致程序异常。

https://gitee.com/mindspore/mindformers/issues/I8TP53
这个issue是一个bug报告，主要涉及TrainingDataLoader接口参数校验不足，在接口初始化时需进行参数校验，可能导致程序运行错误或异常。

https://gitee.com/mindspore/mindformers/issues/I8TP0W
这是一个bug报告，涉及SFTDataLoader接口在使用read_function参数创建数据集时出现错误。

https://gitee.com/mindspore/mindformers/issues/I8TOJ4
这是一个bug报告，主要涉及的对象是在modelarts平台上测试chatglm3模型推理，由于推理速度极慢（0.1tokens/s不到），用户寻求帮助提供排查思路。

https://gitee.com/mindspore/mindformers/issues/I8TNEO
该issue类型属于bug报告，涉及主要对象为MindFormer项目中的"llama2"模块。由于部分参数需要适配，导致run_infer_main.py脚本在lite双动态推理时出现问题。

https://gitee.com/mindspore/mindformers/issues/I8TMIS
这是一个bug报告，涉及的主要对象是导出llama2 7B模型时报错，原因可能是export_llama2_7b.yaml文件中缺少output_dir字段。

https://gitee.com/mindspore/mindformers/issues/I8TKQV
这是一个bug报告，主要涉及MindSpore2.2.0和Cann7.0rc1系统，用户在进行qwen7b全参微调时遇到报错。

https://gitee.com/mindspore/mindformers/issues/I8TJY5
这是一个用户提出疑问的issue，涉及主要对象为glm2/3的maxlen配置。用户疑问的原因可能是对模型配置的疑惑，希望了解为何官方训练配置文件中的maxlen参数未使用到8k的情况。

https://gitee.com/mindspore/mindformers/issues/I8TJ81
这是一个需求类型的issue，主要涉及到希望适配大模型的openai API。由于openai_api.py在mindspore上无法运行，影响了昇腾服务器上的大模型私有化部署。

https://gitee.com/mindspore/mindformers/issues/I8THC3
这是一个bug报告，涉及的主要对象是baichuan2_7b网络模型。由于加载权重时报找不到权重目录，导致卡6加载权重失败，其他卡加载成功。

https://gitee.com/mindspore/mindformers/issues/I8TG1I
这是一个bug报告，主要涉及qwen14B推理阶段出现的参数错误问题，可能导致无法正确运行推理操作。

https://gitee.com/mindspore/mindformers/issues/I8TF36
这是一个bug报告，涉及的主要对象是mindformers的wiki4096.mindrecord数据集训练，由于某种原因导致训练时出现错误。

https://gitee.com/mindspore/mindformers/issues/I8TCWT
这是一个bug报告，主要涉及mindformers源码构建中的问题。由于使用mindspore2.2.0和2.2.1构建后，推理结果混乱，用户需求解决关于乱结果的问题。

https://gitee.com/mindspore/mindformers/issues/I8TCGH
这是一个bug报告，涉及mindformers dev mindspore 2.2.0中chat_web的多卡推理启动时报错的问题，原因是不支持使用yaml文件参数auto_trans_ckpt=True，导致无法使用已切分好的权重文件进行配置。

https://gitee.com/mindspore/mindformers/issues/I8TAO2
This issue is a bug report related to the "Ascend collective communication initialization failed" error on multiple inference cards baichuan214b.

https://gitee.com/mindspore/mindformers/issues/I8T91Y
这是一个bug报告，涉及mindformers中的多卡增量推理功能，由于最新版本下输入出现报错。

https://gitee.com/mindspore/mindformers/issues/I8T7PY
这是一个流式输出问题的bug报告，涉及的主要对象是gml2_6b模型在推理过程中使用TextIteratorStreamer和Flask实现流式输出时出现重复输出问题。这可能是由于与Flask结合使用时出现反复重复输出的实现方式引起的。

https://gitee.com/mindspore/mindformers/issues/I8T5VU
这是一个bug报告，涉及对象是mindformers下的一个issue，用户在使用wizardcoder推理时遇到了参数不匹配的问题。

https://gitee.com/mindspore/mindformers/issues/I8T2EY
这是一个bug报告，涉及到hccl算子切分不均匀导致lite推理报错的问题，由于batch维切分算子只切到两卡而不是四卡，导致生成了不支持的hccl group，从而引发了错误。

https://gitee.com/mindspore/mindformers/issues/I8SLI3
这是一个bug报告，主要涉及GLM2在进行lora微调100轮时出现x_shape和out_shape不等的错误。原因可能是微调轮数增加导致的问题。

https://gitee.com/mindspore/mindformers/issues/I8SFYL
这是一个bug报告，主要涉及模型导出过程中设置batch_size不生效的问题。可能是由于`run_mindformer.py`脚本中设置的batch_size参数未能正确生效导致预期的batch_size为8，实际却为1。

https://gitee.com/mindspore/mindformers/issues/I8SEK6
这是一个bug报告，主要涉及多卡推理功能，在启动过程中遇到报错。

https://gitee.com/mindspore/mindformers/issues/I8SDMQ
这是一个bug报告，涉及主要对象是使用百川2模型进行多卡推理的配置问题，导致推理不出结果。

https://gitee.com/mindspore/mindformers/issues/I8SDLR
这是一个bug报告，涉及的主要对象是MindSpore Lite中的ChatGLM36B模型，在增量batch1和batch4 lite推理性能不达标问题上。原因可能是转测版本上的问题。

https://gitee.com/mindspore/mindformers/issues/I8S978
这是一个bug报告，涉及的主要对象是在集成fastapi服务时出现了内存分配失败的问题，可能是由于模型初始化部分的代码问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8S6DM
这是一个bug报告，涉及的主要对象是MindFormers项目中的llama2模块。由于lite推理时出现了Check shape failed报错，可能是由于输入数据的shape与预期不符导致的。

https://gitee.com/mindspore/mindformers/issues/I8S6BN
这是一个bug报告，主要涉及lite推理报内存不足的问题，可能由于batch_size设置为16在Ascend910 B1机器上导致。

https://gitee.com/mindspore/mindformers/issues/I8S5O8
这是一个bug报告，主要涉及Chatglm26b 910b 单机8卡微调，可能是由于代码逻辑或参数设置不当导致的报错。

https://gitee.com/mindspore/mindformers/issues/I8S4SS
这是一个bug报告，涉及主要对象是按照案例里的百川模型推理时出现错误。导致此问题的原因可能是生成的output文件夹中没有内容，transformed_checkpoint也为空，导致推理没有结果。

https://gitee.com/mindspore/mindformers/issues/I8S1QS
这是一个bug报告，主要涉及MindFormers仓库下的大模型推理脚本和导出脚本不合理的问题，原因是推理脚本参数设置不合理且导出脚本存在冗余代码。

https://gitee.com/mindspore/mindformers/issues/I8S1KC
这是一个bug报告类型的issue，主要涉及visualglm在A100上缺少batch推理的精度和性能数据，可能由于开发自验报告缺少相关信息导致。

https://gitee.com/mindspore/mindformers/issues/I8S13G
这是一个bug报告，主要涉及run_chat_server.py启动服务调用时的问题，由于在中途终止请求导致卡住现象，run_chat_server.py不再执行预测。

https://gitee.com/mindspore/mindformers/issues/I8S0NF
这是关于查询mindformers在Python中的运行版本的问题，属于用户提问类型，主要涉及对象为mindformers库。可能由于用户想要确认如何查看mindformers在Python中的运行版本而产生。

https://gitee.com/mindspore/mindformers/issues/I8RZLC
这是一个bug报告，主要涉及mindformers库中base_trainer.py文件中create_eval_dataset函数中计算batch_size的问题，导致构建数据集失败。

https://gitee.com/mindspore/mindformers/issues/I8RYV1
这是一个bug报告类型的issue，主要涉及LlamaTokenizer中的属性错误。导致该错误的原因可能是tokenizers.AddedToken对象缺少'special'属性。

https://gitee.com/mindspore/mindformers/issues/I8RXII
这是一个bug报告类型的issue，主要涉及使用run_baichuan2.py进行eval模式评估时出现的问题，原因是评估数据集在处理时维度不匹配导致错误。

https://gitee.com/mindspore/mindformers/issues/I8RWSI
这是一个用户寻求帮助的issue，主要涉及到调用KeyWordGenDataset函数时发生的错误，原因可能是无法找到对应的函数位置或GPT分析错误。

https://gitee.com/mindspore/mindformers/issues/I8RTZK
这个issue类型为用户提问，主要涉及多级多卡训练操作，由于文档未提供相关操作指导，导致用户不清楚如何处理分布式权重。

https://gitee.com/mindspore/mindformers/issues/I8RS5E
这是一个用户需求问题，用户在寻求关于适配flash attention的docker镜像的帮助。

https://gitee.com/mindspore/mindformers/issues/I8RQKM
这是一个bug报告，涉及的主要对象是Baichuan27b模型推理过程中出现报错，可能由于配置文件中存在一些参数设置错误导致。

https://gitee.com/mindspore/mindformers/issues/I8RQIW
这是一个bug报告，涉及使用基于pipeline的推理llama27b模型时出现报错，可能是由于环境和软件版本不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/I8RO4Z
这是一个bug报告，主要涉及Mindformers项目中描述微调需要4机，实际双机即可启动的问题，可能是由于readme文档描述不准确导致的。

https://gitee.com/mindspore/mindformers/issues/I8RL47
这是一个bug报告，涉及的主要对象是mindformers中的Skywork模型。由于代码中存在问题，导致在lite推理时会报错。

https://gitee.com/mindspore/mindformers/issues/I8RK9S
这是一个Bug报告，涉及主要对象为配置文件。由于配置文件中写的是'float16'，但程序中要求的关键字是'float16'，导致引发KeyError错误。

https://gitee.com/mindspore/mindformers/issues/I8RIM8
这是一个bug报告类型的issue，主要涉及mindformers库中动态shape功能的异常运行问题。

https://gitee.com/mindspore/mindformers/issues/I8RDAG
这个issue是关于bug报告，涉及到chatweb应用功能包中的token调用错误问题。由于base_tokenizer.py文件替换导致了调用报错，对比代码发现主要差异在报错位置上。

https://gitee.com/mindspore/mindformers/issues/I8RCY9
这是一个用户提出需求的类型的issue，主要对象是流式推理代码，用户希望了解如何在异常时终止流式推理，是否有终止推理的接口。

https://gitee.com/mindspore/mindformers/issues/I8R582
这是一个bug报告，涉及主要对象是在使用MindSpore 2.2.0进行推理时出现权限错误的问题。造成该问题的原因可能是权限设置不当导致的。

https://gitee.com/mindspore/mindformers/issues/I8R4SD
这是一个bug报告，涉及到qwen14b lite推理性能方面的问题，由于性能仅达到转测报告的50%，且与A100相比有较大差距，原因尚需进一步分析。

https://gitee.com/mindspore/mindformers/issues/I8R48T
这是一个关于bug报告的issue，主要涉及到qwen14b lite推理精度和标杆结果不一致的问题。可能由于模型推理过程中的算法或实现细节问题导致了这一症状。

https://gitee.com/mindspore/mindformers/issues/I8R3GM
这是一个bug报告，涉及qwen14b lite推理时导出mindir报错的问题。可能由于软件环境中MindSpore版本为2.2.10、执行模式为graph所导致。

https://gitee.com/mindspore/mindformers/issues/I8R3FR
这是一个关于需求/问题探讨的issue，主要涉及mindformers套件在推理阶段出现重复内容推理的问题。可能是由于推理时的循环回答现象导致，用户希望找到强制禁止推理时重复的方法或特殊配置技巧。

https://gitee.com/mindspore/mindformers/issues/I8QZYR
这是一个bug报告，涉及主要对象为Qwen_7b单卡推理，由于无法看到具体内容，可能是程序报错导致无法正常进行推理。

https://gitee.com/mindspore/mindformers/issues/I8QNAW
这是一个bug报告，该问题涉及到baichuan213B基于高阶接口的推理运行到generation阶段报错，原因是在执行到Generation阶段时出现了RuntimeError。

https://gitee.com/mindspore/mindformers/issues/I8QN8T
这是一个bug报告，涉及MindSpore Lite在双卡推理过程中出现错误的情况。这个问题可能是由环境配置或代码逻辑导致的。

https://gitee.com/mindspore/mindformers/issues/I8QMD9
这是一个用户提出需求的问题，主要涉及Baichuan213BChat对话推理功能，用户想了解为什么仅支持单卡多轮对话推理以及是否可以支持多卡及何时支持。

https://gitee.com/mindspore/mindformers/issues/I8QM0B
这是一个Bug报告，涉及到llama construct代码出现无效代码的问题。可能是由于编码错误或者代码逻辑问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8QLXS
这是一个bug报告类型的issue，主要涉及到README文件中的拼写错误，可能是由于手误导致的。

https://gitee.com/mindspore/mindformers/issues/I8QKGM
这是一个bug报告，主要涉及到chatglm全参微调时报错，TypeError。由于编码错误导致UnicodeDecodeError，无法解码utf8编码的字节数据，出现了异常情况。

https://gitee.com/mindspore/mindformers/issues/I8QJW4
这是一个Bug报告，问题主要涉及到Mindformers项目下的hccl_tools.py文件，导致出现了设备号参数错误的问题。

https://gitee.com/mindspore/mindformers/issues/I8QJT6
这是一个bug报告，主要涉及到将完整权重切分成2卡推理时没有生成output文件的问题。可能是由于运行指令时的配置或参数设置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I8QJHV
这个issue类型为Bug报告，主要涉及的对象是glm3模型代码。导致这个问题的原因可能是缺少相应的配置类名称。

https://gitee.com/mindspore/mindformers/issues/I8QFP8
这个issue类型是bug报告，主要涉及基于generate的推理过程中遇到的问题。由于选择的seq_length可能不符合模型期望的大小，导致出现错误，同时对于推理过程中为何下载了模型而非直接使用训练权重参数提出疑问。

https://gitee.com/mindspore/mindformers/issues/I8QE8I
这是一个bug报告，涉及到MindSpore Lite在双卡推理过程中出现错误，可能是由于推理过程中的环境配置或代码逻辑错误导致的。

https://gitee.com/mindspore/mindformers/issues/I8QDXJ
这是一个bug报告，主要对象是mindformers项目。由于缺少scripts文件夹和run_distribute.bash文件，导致进行微调时出现问题。

https://gitee.com/mindspore/mindformers/issues/I8Q9RG
这个issue属于bug报告类型，主要涉及ChatGLM3的tokenizer和build_model_config的错误，可能是由于最新dev分支的更新导致的功能异常。

https://gitee.com/mindspore/mindformers/issues/I8Q7BA
这是一个bug报告，涉及Llama2 13B双动态lite推理单卡执行的问题，由于相关转测报告未提供，导致发现问题的症状。

https://gitee.com/mindspore/mindformers/issues/I8Q4TN
这是一个bug报告，涉及主要对象是Mindformers项目下的910A设备，在微调过程中出现持续溢出的情况，即使打开infnan模式也无效。

https://gitee.com/mindspore/mindformers/issues/I8Q0WX
这是一个bug报告类型的issue，主要涉及到CONTRIBUTING_CN.md文档中的笔误问题。原因可能是文档内容有误导致用户无法正确理解单卡启动流程和贡献要求。

https://gitee.com/mindspore/mindformers/issues/I8PXMK
这是一个bug报告类型的Issue，主要对象是ChatGLM模块，由于不明原因导致全参微调启动报错。

https://gitee.com/mindspore/mindformers/issues/I8PWI6
这是一个bug报告，涉及到baichuan2_13b模型双卡推理结果异常的问题。由于某种原因导致双卡推理结果异常，但单卡推理没有问题。

https://gitee.com/mindspore/mindformers/issues/I8PVWN
这是一个bug报告类型的issue，涉及到模型在设置不正确的并行参数情况下产生了加载权重错误的问题。

https://gitee.com/mindspore/mindformers/issues/I8PVLL
这是用户提出的需求，希望文档中解释配置文件中的参数 warmup_ratio。由于缺乏这一参数的解释，用户无法理解其含义。

https://gitee.com/mindspore/mindformers/issues/I8PP4G
这是一个bug报告，涉及baichuan2_13b启动eval评测时出现错误的问题。由于特定测试数据集导致产生错误，用户请求协助解决。

https://gitee.com/mindspore/mindformers/issues/I8PP1S
这是一个bug报告，涉及mindformers下的base_infer模块，在调用_lite接口时设置config为空导致调用lite报错。

https://gitee.com/mindspore/mindformers/issues/I8PORF
这是一个bug报告，主要涉及的对象是模型glm2和对话系统。由于设置中的字数限制不一致（128和1024），导致在多轮对话中出现了“exceed max length”报错，无法正确获取历史对话信息。

https://gitee.com/mindspore/mindformers/issues/I8PNP3
这是一个包含关于llama2训练、推理精度相关问题的用户提问，主要涉及llama2模型评测、迁移存在的不一致性和性能指标低下等主题。

https://gitee.com/mindspore/mindformers/issues/I8PC4L
这个issue类型是bug报告，主要涉及的对象是Chat对话文档，由于没有对参数device_range做出说明导致了单卡启动时出现问题。

https://gitee.com/mindspore/mindformers/issues/I8PB8P
这是一个用户询问新功能使用文档的问题，涉及的主要对象是动态seq和动态batch功能。用户询问如何使用新特性llama双动态lite推理和KVCacheMgr增量推理接口，是因为缺乏更新后的使用文档。

https://gitee.com/mindspore/mindformers/issues/I8P9O1
这是一个bug报告，主要涉及到gitee上代码仓提供的链接无法访问的问题。原因可能是链接指向的内容无效或者访问权限受限。

https://gitee.com/mindspore/mindformers/issues/I8P39E
这是一个bug报告，涉及的主要对象是chatglm2 API。由于在使用"基于AutoClass快速使用"时结束后没有打印出(response)，用户想知道为什么会出现这种情况。

https://gitee.com/mindspore/mindformers/issues/I8OW7H
这是一个bug报告，涉及的主要对象是qwen_72b多卡推理，可能由于输入图片引起的报错导致了910b推理失败。

https://gitee.com/mindspore/mindformers/issues/I8OSL9
这是一个bug报告，涉及问题类型是运行百川2报warning，发生原因可能是配置问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8OS1J
这是一个bug报告，涉及到在LoRA微调或全量微调时出现No module named 'mindpet'错误。 由于缺少'mindpet'模块导致此问题。

https://gitee.com/mindspore/mindformers/issues/I8ORV9
这是一个bug报告类型的issue，主要涉及到MindSpore框架中glm Lora模型在不同epoch下推理结果出现问题的情况。这可能是由于训练过程中的参数调整或模型优化不当导致的。

https://gitee.com/mindspore/mindformers/issues/I8ONJE
这是一个bug报告，涉及物理机上chat web推理服务使用LLaMA213B进行2卡推理时报错的问题。可能由于启动chat web过程中配置有误导致报错信息的问题。

https://gitee.com/mindspore/mindformers/issues/I8OJAL
这是一个bug报告类型的issue，涉及的主要对象是baichuan7B微调。由于8卡910A微调老是报OOM，导致bug报告。

https://gitee.com/mindspore/mindformers/issues/I8OIXP
这是一个用户提出问题的issue，主要涉及MindSpore框架中是否可以在910B上运行glm，并且用户环境符合要求。原因可能是用户遇到了运行glm时的问题或者疑问。

https://gitee.com/mindspore/mindformers/issues/I8OIOU
这是一个bug报告，涉及到AICC 910A 8P MS2.1和MS2.2模块中的配置问题，由于无法配置并行策略811导致训练吞吐性能较低，客户反馈训练吞吐较差。

https://gitee.com/mindspore/mindformers/issues/I8OHTI
这是一个bug报告，问题主要涉及LLama2_7b官方推理代码在特定环境下报错。可能的原因是环境配置或代码实现存在问题。

https://gitee.com/mindspore/mindformers/issues/I8OGRN
这是一个bug报告，涉及主要对象为ChatGLM6B模型，在长文上下文推理时出现了推理速度慢的问题。

https://gitee.com/mindspore/mindformers/issues/I8OA1V
这是一个关于性能劣化的bug报告，主要涉及BaiChuan2 7B/13B模型在多batch下性能劣化的问题。据分析，可能是由于2.2.10版本相比2.2.1版本在特定条件下的执行模式导致的。

https://gitee.com/mindspore/mindformers/issues/I8O490
这是一个bug报告，主要涉及在进行权重切分时无法正确输出结果。可能原因是执行的命令或代码中存在错误。

https://gitee.com/mindspore/mindformers/issues/I8O2AY
这是一个bug报告类型的issue，主要涉及的对象是baichuan7B模型数据转换为mindrecord格式时报错。问题可能是由于数据格式转换的错误或者转换工具的bug导致。

https://gitee.com/mindspore/mindformers/issues/I8O0NS
这是一个用户提出需求的类型，该问题单涉及的主要对象是baichuan2在训练作业中的训练参数配置。通过用户提问的方式，试图了解如何设置训练参数以进行训练。

https://gitee.com/mindspore/mindformers/issues/I8O0DB
这是一个bug报告，主要涉及到llama2 13b全参微调过程中出现的报错问题。由于输入数据为alpaca数据集时，在进行全参微调运行train model时出现了错误。

https://gitee.com/mindspore/mindformers/issues/I8NTBM
这是一个关于需求提出的issue，主要涉及的对象是开源MOE模型Mixtral8x7B，用户希望知道是否有适配该模型的计划。

https://gitee.com/mindspore/mindformers/issues/I8NSXO
这是一个 bug 报告，涉及对象为 llama2 7B 全参微调，在运行到第 6 个 step 时报错，可能由于环境信息配置不当导致。

https://gitee.com/mindspore/mindformers/issues/I8NSEF
这是一个bug报告类型的issue，主要涉及的对象是在华为ModelArts Notebook上使用BERT模型。更改seq_length长度为512后导致模型内部报错，用户询问应如何更改seq_length才能使模型正常输出。

https://gitee.com/mindspore/mindformers/issues/I8NS3S
这是一个bug报告类型的issue，主要涉及到`BERT`模型对`masked_lm_ids`和`masked_lm_weights`参数传入形式的问题，可能是由于环境或代码变动导致出现错误。

https://gitee.com/mindspore/mindformers/issues/I8NQWT
这是一个bug报告类型的issue，涉及到将pytorch权重合并转换到mindspore上时出现精度异常的问题。

https://gitee.com/mindspore/mindformers/issues/I8NMLT
这是一个Bug报告，问题涉及的主要对象是MS_DEV_FORCE_ACL环境变量设置导致chatglm2_6b模型在运行时出错。

https://gitee.com/mindspore/mindformers/issues/I8NMJD
这是一个bug报告，涉及的主要对象是MIMDataset。由于关闭shuffle在相同种子下，MIMDataset两次生成的数据不一致。

https://gitee.com/mindspore/mindformers/issues/I8NHO5
这是一个bug报告，主要涉及GLM2网络的微调加载ckpt失败，导致loss异常，网络无法评估。可能原因是加载ckpt失败导致的网络异常。

https://gitee.com/mindspore/mindformers/issues/I8NFGP
这是一个bug报告，问题涉及到baichuan2模块微调过程中出现的日志信息学习率为0的情况。由于尝试了多个模型配置都出现相同情况，用户询问是什么原因导致此问题。

https://gitee.com/mindspore/mindformers/issues/I8NERA
这是一个bug报告，涉及MindSpore在910B环境下使用llama模型进行分布式训练时出现的问题。由于配置中的参数设置不当，导致在8P训练过程中出现错误。

https://gitee.com/mindspore/mindformers/issues/I8NDXD
这是一个寻求帮助类的问题单，涉及主要对象为moxing模块的引用。由于用户需要指导如何正确导入moxing模块，故提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I8NDKO
这是一个关于功能需求的issue，主要对象是mindformer，用户询问是否将来支持GPU环境以及可能的时间表。

https://gitee.com/mindspore/mindformers/issues/I8NDDJ
这是一个bug报告，涉及mindformersdev的套件中2卡分布式训练llama2 13b时出现报错的问题，可能是由于脚本运行有误导致。

https://gitee.com/mindspore/mindformers/issues/I8NCHE
这个issue属于用户提出需求类型，主要对象涉及微调和合并llama27b模型的任务。原因是用户想要在run_distribute.sh微调任务结束后自动执行权重合并，需要找到判断微调任务是否结束的方法。

https://gitee.com/mindspore/mindformers/issues/I8NANP
这是一个bug报告，主要涉及llama分布式预训练后权重合并操作。由于合并后权重过大，用户提出是否需要裁剪权重以及如何进行权重裁剪的问题。

https://gitee.com/mindspore/mindformers/issues/I8N8R3
这是一个bug报告，涉及主要对象为在使用Baichuan213B进行多卡推理时出错。由于配置修改后运行`python run_chat_server.py`报错，可能由配置文件错误或代码逻辑问题导致。

https://gitee.com/mindspore/mindformers/issues/I8N7IL
这是一个bug报告，涉及对象是mindformers中的skywork13B lite推理结果与标杆不一致，可能是由于执行lite推理时出现了西安旅游计划内容而导致的。

https://gitee.com/mindspore/mindformers/issues/I8MXVS
这是一个bug报告，主要涉及到了在使用mindformers中的qwen_7b模型进行batch推理时，设置do_sample参数为False，在多个batch中间结果出现差异的问题。可能是由于某些原因导致造成了这种结果。

https://gitee.com/mindspore/mindformers/issues/I8MXV0
这个issue是一个bug报告，涉及的主要对象是模型推理过程中的结果不一致，可能是由于增量推理 + do_sample=False 在部分场景下导致与 A100 的对比结果不一致。

https://gitee.com/mindspore/mindformers/issues/I8MU4E
这是一个bug报告，主要涉及WizardCode15B lite推理结果与标杆不一致的问题。可能由于编码逻辑不正确导致了这一状况。

https://gitee.com/mindspore/mindformers/issues/I8MSQQ
这是一个bug报告，涉及对象是mindformers文档中的chat web章节。原因是文档中指向的dev分支代码与PYPI库中发布的版本不匹配，导致出现load_checkpoint配置错误的问题。

https://gitee.com/mindspore/mindformers/issues/I8MS7Y
这个issue属于用户提出需求类型，主要涉及对Chat Web中对自定义权重和分词器model的说明不清晰导致设置自定义checkpoint_download目录时无法找到tokenizer.model文件的问题。

https://gitee.com/mindspore/mindformers/issues/I8MNT6
这是一个bug报告issue，主要涉及运行mindformers中llama27b八卡lora微调时出现报错的问题。报错原因可能是由于长时间运行后部分卡没有进程占用，导致报错情况发生。

https://gitee.com/mindspore/mindformers/issues/I8MJPT
这是一个bug报告issue，主要对象是gpt2_7b模型在batch推理中性能不达标，可能是由于推理时的速度只有35tokens/s导致的。

https://gitee.com/mindspore/mindformers/issues/I8MIG9
这是一个Bug报告，主要涉及到"langchain加载自有数据库"。由于缺少libGL.so.1文件导致环境报错，影响了对大模型加载知识数据库的操作。

https://gitee.com/mindspore/mindformers/issues/I8MIFM
这是一个bug报告，涉及主要对象是langchain加载自有知识数据库时出现报错。导致报错的原因是缺少libGL.so.1文件。

https://gitee.com/mindspore/mindformers/issues/I8MAJ7
这是一个Bug报告，描述了使用chat_web部署baichuan213b时，多个请求中包含流式和非流式请求时，会导致非流式请求的回答可能是之前流式请求的结果。原因是流式请求的结果被保存在输出队列中，导致下一次非流式请求直接读取之前队列里的结果。

https://gitee.com/mindspore/mindformers/issues/I8MAA7
这是一个关于代码优化的issue，主要对象是MSLITE推理。这个问题可能是由于性能优化需要，将gather操作移到最后的词表投影MatMul前。

https://gitee.com/mindspore/mindformers/issues/I8M6VR
这是一个bug报告，主要涉及单卡lora微调报错问题。由于代码执行过程中出现了异常，导致程序报错并无法正确执行。

https://gitee.com/mindspore/mindformers/issues/I8M351
这是一个bug报告类型的issue，主要涉及到mindsporelite推理过程中出现的RuntimeError，原因可能是配置或代码逻辑问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8M1LR
这是一个bug报告，主要涉及gpt2_13b.batch推理，模型导出设置不生效的问题。可能由于设置参数错误或软件环境配置不正确导致了这一问题。

https://gitee.com/mindspore/mindformers/issues/I8M1AT
这个issue是一个bug报告，涉及主要对象是MindSpore在mindformers仓库下的一个模型（qwen_7b）。主要原因可能是官网demo缺少输入输出结果，device_id设定无法生效，导致推理结果不符合预期。

https://gitee.com/mindspore/mindformers/issues/I8M18S
这是一个bug报告，该问题涉及mindformers下的gpt_13b模型导出失败，可能是由于执行过程中遇到了错误导致的。

https://gitee.com/mindspore/mindformers/issues/I8LY0E
这个issue属于用户提出需求类型，主要涉及到支持GLIP网络的请求。

https://gitee.com/mindspore/mindformers/issues/I8LW9M
这是一个bug报告类型的issue，涉及主要对象是使用Baichuan27B进行推理的初次请求速度较慢。出现这个问题的原因可能是模型初始化和加载操作耗时过长，用户希望优化首次推理请求的速度。

https://gitee.com/mindspore/mindformers/issues/I8LSBN
这是一个性能问题报告，主要涉及的对象是在gitee上的mindformers下的qwen14b模型在910A双卡上推理速度过慢的情况。

https://gitee.com/mindspore/mindformers/issues/I8LRV7
这是一个bug报告，涉及主要对象是使用mindformers运行llama2时遇到了类型为FP32Imm的BUG。由于多方面因素如镜像版本、驱动版本、固件版本等的影响，导致了这个具体的BUG症状的出现。

https://gitee.com/mindspore/mindformers/issues/I8LPTK
这是一个关于llm engine以及build_from_file的问题，问题类型是用户提出需求，请教问题，主要涉及的对象是mslite的llm engine和mindformers库的推理代码。由于self._model断开导致无法加载模型，用户提出了关于如何调用llm engine和build_from_file的问题。

https://gitee.com/mindspore/mindformers/issues/I8LOU3
这是一个bug报告，涉及的主要对象是执行千问7b模型的推理代码，用户遇到了代码执行报错的问题。

https://gitee.com/mindspore/mindformers/issues/I8LIGK
这是一个关于性能问题的需求提出，主要涉及到推理加速方案不足，导致无法支持商用并发推理需求。

https://gitee.com/mindspore/mindformers/issues/I8LFFY
该issue类型为用户提出需求，涉及主要对象为在modelarts环境中进行全量微调时的并行配置。由于硬件资源不匹配以及多机训练无法顺利运行，用户提出了配置建议和测试需求。

https://gitee.com/mindspore/mindformers/issues/I8LFDS
这是一个bug报告，涉及到Yi34B模型权重转换后遇到的问题，导致wq和wk两层layer的shape需要重新reshape。

https://gitee.com/mindspore/mindformers/issues/I8L7VP
这是一个Bug报告类型的Issue，主要对象是910A服务器和baichuan213B多卡部署系统。由于推理速度非常缓慢，可能是部署过程中配置或硬件性能不足导致的。

https://gitee.com/mindspore/mindformers/issues/I8L4LZ
这是一个bug报告，该问题涉及MindSpore框架下的模型训练。由于在加载ckpt文件后设置resume_training=True会导致重新执行加载的epoch，引起了训练过程中的异常行为。

https://gitee.com/mindspore/mindformers/issues/I8L3F1
这是一个bug报告，涉及chat_web 2.0调用2卡模型推理时出现报错的问题，报错原因是在调用chat_glm2模型时直接使用了错误的device_id。

https://gitee.com/mindspore/mindformers/issues/I8L1ZJ
这个issue是一个bug报告，主要涉及MultiSourceDataLoader中配置nums_per_dataset参数时超出数据总量会导致报错，可能是由于参数配置错误导致的。

https://gitee.com/mindspore/mindformers/issues/I8KXN5
这个issue是一个bug报告，主要涉及mindformers项目中yaml文件与官方文档配置不一致导致部分配置项说明缺失，用户希望获得有关optimizer_shard配置项更详细的解释。

https://gitee.com/mindspore/mindformers/issues/I8KXLT
这是一个用户提出需求的issue，主要涉及到mindformers中的yaml文件解释不够详细，导致使用上的困惑。

https://gitee.com/mindspore/mindformers/issues/I8KRM4
该issue属于用户提出需求类型，主要涉及generate方法的添加校验flash attention开关，由于需求背景信息不完整导致的。

https://gitee.com/mindspore/mindformers/issues/I8KPF1
这是一个bug报告，主要涉及LLama 7B单机单卡资源不足的问题，可能是由于内存不足导致运行LLama 27b的单卡Lora微调时出现报错。

https://gitee.com/mindspore/mindformers/issues/I8KOCZ
该issue类型为用户提出需求，主要对象是适配Qwen 7b/14b模型预训练任务。由于缺乏适配计划，用户希望了解近期是否会实现此需求。

https://gitee.com/mindspore/mindformers/issues/I8KNTQ
这是一个bug报告，涉及的主要对象是baichuan213b按照data_parallel=2自动切分权重时出现报错。可能由于配置参数设置不正确导致报错。

https://gitee.com/mindspore/mindformers/issues/I8KNNI
这是一个bug报告，主要涉及mindformers项目中使用llama_13b进行generate单卡推理时出现的报错。导致这个问题的原因是输入的类型不匹配导致的报错。

https://gitee.com/mindspore/mindformers/issues/I8KKV9
这是一个bug报告类型的issue，涉及主要对象为mindformers下的llama项目。由于可能未正确配置权重合并参数或其他原因，导致训练结束后合并的权重与原始预训练权重无变化，用户对此提出疑问并寻求解释。

https://gitee.com/mindspore/mindformers/issues/I8KHKD
这是一个bug报告，涉及lite推理过程中出现bmm执行失败异常的问题，可能是由于算子问题导致。

https://gitee.com/mindspore/mindformers/issues/I8KGEH
这是一个关于模型注册与调用方式的问题，属于用户提出需求类型，主要涉及到MindFormerRegister中的模型注册逻辑。这个问题可能由于缺乏对类注册逻辑的了解，导致无法正确初始化生成实例化对象，并在训练代码的运行过程中未能正确执行相应逻辑。

https://gitee.com/mindspore/mindformers/issues/I8KEUR
这是一个bug报告，涉及的主要对象是TextGeneratorInfer模块。导致这个问题的原因可能是传入多个字符串时出现了错误。

https://gitee.com/mindspore/mindformers/issues/I8KEAL
这是一个用户提出需求的issue，主要涉及LLAMA2文档中关于API使用的说明不够详细，需要补充说明在未联网环境下如何下载权重文件以及权重文件的放置位置。

https://gitee.com/mindspore/mindformers/issues/I8KBLT
这个issue类型为bug报告，涉及的主要对象是自动切分权重过程，由于auto_parallel导致了Integrated_save参数被误设置为False，可能导致了权重卡死的bug。

https://gitee.com/mindspore/mindformers/issues/I8KBJD
这是一个bug报告，主要涉及LlamaForCausalLM无法读取checkpoint文件的权限问题，可能由于文件访问权限设置不正确而导致。

https://gitee.com/mindspore/mindformers/issues/I8KBIO
这是一个bug报告，主要涉及MindSpore在运行sam示例代码`sam_by_image.py`时出现的报错。原因可能是模型推理过程中出现了错误。

https://gitee.com/mindspore/mindformers/issues/I8KAAX
这是一个用户询问问题的类型的issue，主要涉及Mindformers框架在性能方面与torch的比较，以及多用户同时请求并发推理功能和fastchat支持情况。由于用户对框架性能和功能的关注，导致提出了相关问题。

https://gitee.com/mindspore/mindformers/issues/I8K7JY
这是一个用户提出需求的issue，主要涉及lite推理在物理机上是否支持多卡，用户想要了解多卡支持是否可行。

https://gitee.com/mindspore/mindformers/issues/I8K72Z
这是一个bug报告，涉及单卡评测过程中出现数值错误的问题。导致该问题的原因是数值形状不匹配。

https://gitee.com/mindspore/mindformers/issues/I8K5HP
这是一个关于LLama27b训练的用户提问，涉及LLama2和CodeLlama的参数转换和训练操作。问题是由于参数输入类型不一致导致的。

https://gitee.com/mindspore/mindformers/issues/I8K2X3
这是一个关于性能优化的bug报告，主要涉及ChatGLM2量化模型的Process Memory占用问题。原因可能是未能成功开启全量化模式导致。

https://gitee.com/mindspore/mindformers/issues/I8K0C1
这是一个bug报告，涉及的主要对象是Chat Web在910A服务器上多卡部署baichuan213B，其中包括模型权重加载错误、部署后对话能力问题以及文档细节错误。

https://gitee.com/mindspore/mindformers/issues/I8JYN0
这是一个 Bug 报告类型的 issue，主要涉及到 ChatGLM6B 的 Lora 微调过程中，NPU 内存计算错误导致实际空余内存与显示的内存不一致。

https://gitee.com/mindspore/mindformers/issues/I8JVTM
这是一个bug报告，问题涉及MindFormers中Trainer接口直接配置梯度累积导致无法正确执行GradAccumulationCell流程，原因可能是配置参数设置不正确。

https://gitee.com/mindspore/mindformers/issues/I8JVHZ
这是一个bug报告，主要涉及yaml文件的内容。由于某些原因导致llama213b 2机16卡一直卡在上面，需要对yaml文件进行微调来解决这个问题。

https://gitee.com/mindspore/mindformers/issues/I8JS21
这是一个bug报告，主要涉及对象是使用910A两卡推理百川13B的用户。由于开启增量推理导致报错，用户寻求关于报错信息的帮助。

https://gitee.com/mindspore/mindformers/issues/I8JS0F
这是一个bug报告类型的issue，涉及主要对象是模型微调/LORA微调过程中的错误加载和权重加载问题，可能由于模型加载失败导致错误信息和权重未加载信息的现象。

https://gitee.com/mindspore/mindformers/issues/I8JMQH
这是一个关于模型权重下载相关的问题，类型是需求提出，主要涉及AutoModel.from_pretrained函数以及模型权重的命名和下载问题，可能由于模型权重实际内容相同导致用户疑惑是否需要提供两份权重。

https://gitee.com/mindspore/mindformers/issues/I8JKE2
该issue为用户提出需求。主要对象是baichuan213B模型在910A服务器上的多卡推理部署问题。该问题由于目前baichuan213B在910A上仅支持多卡推理，但不支持交互，导致无法在910A服务器上部署。

https://gitee.com/mindspore/mindformers/issues/I8JG42
这是一个用户提出需求的 issue，主要涉及到 ChatGLM2 的量化推理配置文件缺失问题，用户寻求获取配置文件的帮助。

https://gitee.com/mindspore/mindformers/issues/I8JDER
这是一个bug报告，主要涉及代码规范问题，建议使用logging而不是print来避免打印大量信息影响开发使用。由于代码中使用print打印大量信息，导致终端输出过多信息且无法通过更改调用参数关闭，影响正常开发使用。

https://gitee.com/mindspore/mindformers/issues/I8JCQF
这是一个bug报告，主要涉及mindformers对GPT2模型推理测试时出现的报错问题。原因可能是模型推理过程中出现了加载op信息json失败的错误。

https://gitee.com/mindspore/mindformers/issues/I8JCP7
这是一个bug报告类型的issue，主要涉及mindformers在安装时无法找到pyarrow 12.0.1版本，导致无法完成安装。

https://gitee.com/mindspore/mindformers/issues/I8JCP6
该issue类型为技术支持问题，涉及主要对象为mindformer软件的支持情况。这个问题可能源于用户想要确认mindformer软件是否可以兼容310系列的推理卡，以满足他们使用atlas 300（8000）服务器的需求。

https://gitee.com/mindspore/mindformers/issues/I8J93M
这个issue属于bug报告类型，涉及mindformers中chatglm26b自有数据集lora微调后推理异常，可能是由于执行命令参数设置不正确导致无法正常进行推理。

https://gitee.com/mindspore/mindformers/issues/I8J8VS
这是一个Bug报告，涉及大模型bloom_7.1b在batch size为8时推理性能不达标的情况。可能是由于模型推理过程中的某些问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8J8C0
这是一个bug报告，涉及的主要对象是910B(x86)硬件和ChatGLM2模型。更新了代码后，进行lora微调会导致报错，尚未找到具体原因。

https://gitee.com/mindspore/mindformers/issues/I8J3MF
这是一个bug报告，主要涉及Mindformers库中的模型chatglm26b在分布式推理过程中出现错误。原因可能是基于完整预训练权重使用2卡进行自动切分权重导致的。

https://gitee.com/mindspore/mindformers/issues/I8J1HL
这是用户提出的一个问题，主要涉及如何在yaml配置文件中指定训练数据集使用何种sampler，可能是由于缺乏相关接口文档而导致用户困惑。

https://gitee.com/mindspore/mindformers/issues/I8IWY2
这是一个Bug报告，问题涉及Mindformers项目中的LLama27b模型微调过程中的错误。原因可能是yaml配置文件中的lora参数设置不正确，导致在运行finetune时出现错误。

https://gitee.com/mindspore/mindformers/issues/I8IQIN
这是一个用户提出需求的问题，主要对象是Mindformers的模型和套件。用户询问有关Mindformers是否有支持多机多卡推理的模型和套件的相关信息。

https://gitee.com/mindspore/mindformers/issues/I8IPGN
这个issue属于bug报告类型，主要涉及Baichuan2单机两卡分布式启动中推理服务无法正常工作的问题，可能是由部署配置或代码逻辑错误导致。

https://gitee.com/mindspore/mindformers/issues/I8IFZC
这是一个bug报告，主要涉及的对象是Mindformers下的一个issue，由于使用了错误的Python文件导致了输入batch_size超出了允许的范围而出现数值错误的报错。

https://gitee.com/mindspore/mindformers/issues/I8IE20
这是一个bug报告类型的issue，主要涉及文档打开显示违规信息，可能是由于链接指向内容不当导致无法打开文档。

https://gitee.com/mindspore/mindformers/issues/I8I5NN
这是一个bug报告，涉及的主要对象是Atlas 800T a2服务器。原因可能是第一台服务器的npu0进程挂掉导致的问题。

https://gitee.com/mindspore/mindformers/issues/I8I4NF
这个issue属于用户提问类型，主要涉及分布式并行方式中几种配置方式的选择原则，由于用户想了解configs/*.yaml配置文件中的并行方式semiauto，auto和hybird的区别以及如何选择适合的方式。

https://gitee.com/mindspore/mindformers/issues/I8I195
这是一个bug报告，主要涉及模型推理结果和速度异常的问题。由于自定义实例化配置文件无法成功运行demo，并且在CPU环境下生成速度非常慢，用户提出了关于模型配置和推理速度的问题。

https://gitee.com/mindspore/mindformers/issues/I8HYLL
这是一个bug报告，主要涉及使用镜像跑baichuan2的模型推理时报cell_reuse相关错误，可能是由于默认镜像未解决该问题导致。

https://gitee.com/mindspore/mindformers/issues/I8HYH4
这是一个用户提出需求的issue，主要涉及日志打印级别的控制。由于目前日志打印级别无法通过参数控制，用户希望能够实现这一功能。

https://gitee.com/mindspore/mindformers/issues/I8HYFC
这是一个bug报告，主要涉及dataset预处理逻辑，由于未包含截断逻辑导致超长报错。

https://gitee.com/mindspore/mindformers/issues/I8HYD5
这是一个bug报告，涉及到910B服务器上的Lora微调启动报错问题。原因可能是两台服务器内网互通时导致的故障。

https://gitee.com/mindspore/mindformers/issues/I8HVMG
这个issue类型是bug报告，该问题涉及的主要对象是代码中的ChatGLM2WithLora函数。由于更新后的版本中glm2.py中没有ChatGLM2WithLora函数，导致用户不知道如何生成微调后的推理model。

https://gitee.com/mindspore/mindformers/issues/I8HVM1
这是一个bug报告，主要涉及的对象是权重转换脚本，导致的症状是无法支持lora微调权重的转换。

https://gitee.com/mindspore/mindformers/issues/I8HV4W
这是一个bug报告，该问题涉及共享磁盘场景下多节点启动任务时出现策略文件丢失的情况。由于任务拉起的时机不对导致部分节点的策略文件被后拉起任务的节点删除。

https://gitee.com/mindspore/mindformers/issues/I8HUQI
这个issue属于用户提出需求的类型，主要涉及的对象是BERT模型的数据处理功能。原因可能是用户想了解BERT模型是否支持TFRECORD类型输入以及对于MINDRECORD等其他类型输入的支持情况。

https://gitee.com/mindspore/mindformers/issues/I8HU01
这是一个bug报告，该问题涉及百川2 13b generate功能，导致输出结果总是带着输入。

https://gitee.com/mindspore/mindformers/issues/I8HQF3
这是一个bug报告类型的issue，主要涉及的对象是Ascend 910B芯片的不同版本内存配置。由于所有910B相关配置都是64GB版本，而实际情况下需要支持32GB版本，导致可能发生内存溢出的问题。

https://gitee.com/mindspore/mindformers/issues/I8H5XH
这是一个bug报告，涉及问题导致推理失败。

https://gitee.com/mindspore/mindformers/issues/I8H5R4
这是一个用户提出需求的类型，该问题涉及的主要对象是ChatGLM36b模型的支持和开源大模型的Serving支持。用户提出了关于支持特定模型和开源大模型Serving计划的问题。

https://gitee.com/mindspore/mindformers/issues/I8H4PM
这是一个bug报告类型的issue，主要涉及MindSpore框架中模型推理执行出错的问题。可能由于环境配置、模型版本、执行模式等原因导致推理执行失败。

https://gitee.com/mindspore/mindformers/issues/I8H1V5
这个issue属于需求类问题，主要涉及对象是权重转换，由于使用baichuan27b进行lora微调后得到了分布式的权重，用户想知道如何将其转换为完整权重进行推理。

https://gitee.com/mindspore/mindformers/issues/I8GVCP
这是一个用户提出需求的类型。该问题单涉及主要对象是Trainer高阶接口配置内容打印不一致的问题。原因是在评测和推理阶段打印的配置内容与实际输入不匹配，造成冗长且观察困难。

https://gitee.com/mindspore/mindformers/issues/I8GQI9
这是一个用户提出需求的类型，主要涉及对象是训练模型所需的计算资源，用户关注在两台910A能够支持最大训练模型的大小和是否能支持34B模型。

https://gitee.com/mindspore/mindformers/issues/I8GOSF
这是一个用户提出需求的issue，主要涉及Baichuan213BChat4bits。由于用户想要支持Baichuan213BChat4bits，因此提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I8GMY8
这是一个用户提出需求的问题单，主要涉及大模型的API部署。用户询问是否支持在Baichuan2微调后提供API接口方式去访问模型，希望实现类似vllm上的web或API接口方式的部署。

https://gitee.com/mindspore/mindformers/issues/I8GHLJ
这是一个性能问题报告，与glm2_6b+环境910b3相关，由于测试性能不达标，导致实际tokens性能不符预期。

https://gitee.com/mindspore/mindformers/issues/I8GAEC
这是一个bug报告，主要涉及LLaMA2推理速度慢的问题，原因可能是由于性能数据异常导致。

https://gitee.com/mindspore/mindformers/issues/I8G4P9
这是一个bug报告，涉及的主要对象是运行在model arts环境中的baichuan2 13b_910b程序。由于设置双节点环境时出现报错，可能是yaml文件配置或数据集制作的问题导致。

https://gitee.com/mindspore/mindformers/issues/I8G1B3
这是一个bug报告，主要涉及在mindformers中使用Swin大模型训练自定义数据集后，识别类别标签不对的问题。由于模型训练完成后，使用模型进行识别时返回的类别是imagenet1k的标签，可能是因为在训练过程中数据集处理、配置文件设置或者模型加载等环节出现了问题。

https://gitee.com/mindspore/mindformers/issues/I8G0TS
这是一个用户提出需求和问题的issue，主要涉及mindformers和昇腾310卡，由于昇腾310卡只支持正向推理，无法进行反向传播训练，导致用户想要将mindformers和chatglm3移植到310卡上时遇到困难。

https://gitee.com/mindspore/mindformers/issues/I8FXW5
这是一个bug报告，主要涉及的对象是使用llama.md提供的脚本进行多batch推理时出现显存不足的问题。

https://gitee.com/mindspore/mindformers/issues/I8FW6X
这是一个bug报告类型的issue，主要涉及baichuan2_7b_lora_910b的报错问题，由于执行命令和ymal配置导致了报错情况。

https://gitee.com/mindspore/mindformers/issues/I8FL60
这是一个Bug报告类型的Issue，主要涉及llama27b单机多卡运行出现问题，可能由于配置文件、运行代码、运行脚本或运行命令错误导致了问题。

https://gitee.com/mindspore/mindformers/issues/I8FJ7Z
该issue属于Bug报告类型，主要涉及到代码库中的`BinaryDiceLoss`类及其相关方法。发起该Issue是因为代码中的`dice_loss`方法在计算损失时存在一个错误，导致了错误的计算结果。

https://gitee.com/mindspore/mindformers/issues/I8FIPC
这是一个bug报告，主要涉及的对象是llama33B的训练作业。问题出现的原因是在代码中变量'device_num'在被赋值前被引用，导致了UnboundLocalError错误。

https://gitee.com/mindspore/mindformers/issues/I8FHH0
这是一个bug报告，主要涉及的对象是读取图片的过程。由于出现了ZeroDivisionError，推测是因为在计算中出现了除以零的情况导致的问题。

https://gitee.com/mindspore/mindformers/issues/I8FH5S
这个issue属于bug报告类型，主要涉及的对象是支持glm2_6b_ptuning2的web服务。这个问题可能是由于命令执行时的报错信息导致，用户希望得到web服务是否支持的解答。

https://gitee.com/mindspore/mindformers/issues/I8FGHU
这是一个bug报告，涉及模型"llama_13b"的推理结果错误。原因可能是模型配置或其他参数设置有误导致结果不符预期。

https://gitee.com/mindspore/mindformers/issues/I8FDVU
这是一个bug报告，涉及Mindformers在同步sink stream时出现错误。原因可能是由于代码中某些模块的异常导致的问题。

https://gitee.com/mindspore/mindformers/issues/I8FDVI
这是一个关于用户提出需求的issue，主要涉及到边微调边评估的功能。用户提出这个问题是因为开启了do_eval=True，但在任务状态为finetune时无法实时看到评估结果。

https://gitee.com/mindspore/mindformers/issues/I8FCAC
这是一个关于功能支持的问题，主要涉及设备型号910ProB和Mindformers平台。由于设备型号和文档中支持的型号不匹配，导致在推理过程中报oom（内存不足）错误，用户想要确认Mindformers是否支持量化推理。

https://gitee.com/mindspore/mindformers/issues/I8FA8I
这是一个bug报告，主要涉及大模型bloom_7.1b在batch size为8时推理结果不正确，可能由于模型实现或推理过程中的问题所致。

https://gitee.com/mindspore/mindformers/issues/I8F9TD
这个issue属于bug报告类型，主要涉及text_generator在Baichuan2上设置了random seed导致do_sample参数失效的问题。

https://gitee.com/mindspore/mindformers/issues/I8F9SL
这是一个bug报告，涉及对象为MindSpore中的推理精度问题。由于推理精度不正确，用户提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I8F9RM
这是一个bug报告，主要涉及LITE模块下的llama2_13b模型缺少export_llama2_13b.yaml文件以及使用910B lite.ini配置后导致性能降低的问题。

https://gitee.com/mindspore/mindformers/issues/I8F844
这是一个bug报告，涉及的主要对象是模型微调的过程。由于内存不足，导致模型微调失败。

https://gitee.com/mindspore/mindformers/issues/I8F81M
这是一个功能需求类的issue，主要涉及MindFormers的各个功能特性不够解耦，需要保持核心代码稳定以实现不同特性之间的代码界限清晰和新增特性代码更加模块化。

https://gitee.com/mindspore/mindformers/issues/I8F7O8
这是一个用户提出需求的issue，主要对象是数据集加载功能。由于支持的数据集太少，并且无法在线直接加载，用户希望增强数据集加载功能。

https://gitee.com/mindspore/mindformers/issues/I8F7CS
这是一个关于mindformers是否支持海光DCU部署的需求提出类型的issue，主要涉及对象是mindformers平台。由于用户可能需要在海光DCU环境中使用mindformers，因此提出了这个问题以了解是否支持相关部署。

https://gitee.com/mindspore/mindformers/issues/I8F58V
这是一个bug报告，该问题涉及多线程并发执行时出现的BrokenPipeError，导致程序报错。

https://gitee.com/mindspore/mindformers/issues/I8F2VB
这是一个bug报告issue，出现在mindformers安装的develop版本中，主要对象是生成rank_table脚本。由于缺少hccn_tool命令导致的KeyError错误引发了此问题。

https://gitee.com/mindspore/mindformers/issues/I8F1JP
这是一个bug报告，涉及hccn.conf文件内容为空的问题，可能是由于文件不存在导致的。

https://gitee.com/mindspore/mindformers/issues/I8F1E8
这个issue类型为内容违规报告，主要涉及的对象是上传的文件"qwen.md"，可能由于文件名或内容含有违规信息导致此问题产生。

https://gitee.com/mindspore/mindformers/issues/I8EVXO
这是一个bug报告，涉及主要对象为mindformersr0.6下的T5推理速度问题。由于增大推理的batch_size并未加快推理速度，可能是代码实现上的问题导致了这一症状。

https://gitee.com/mindspore/mindformers/issues/I8EVU5
这个issue是关于bug报告，主要涉及mindformersr0.2下的T5模型推理过程中，加padding和不加padding导致输出结果不同。原因可能是padding造成了模型输入数据的变化，影响了输出结果。

https://gitee.com/mindspore/mindformers/issues/I8ESQB
这是一个bug报告，涉及主要对象是baichuan2模型。问题来源于缺少local_rank参数导致在modelarts训练时出现问题。

https://gitee.com/mindspore/mindformers/issues/I8EO2N
这个issue是关于bug报告，主要涉及多机多卡训练的参数设置问题，可能由于参数配比不当导致出现问题。

https://gitee.com/mindspore/mindformers/issues/I8EKOR
这是一个bug报告，主要涉及mindformers中安装mindspore时出现的警告问题，可能是由于镜像环境设置或导入问题导致。

https://gitee.com/mindspore/mindformers/issues/I8EABR
这是一个bug报告，主要涉及对象是mindspore软件，可能由于某些原因导致了tbe报错的症状。

https://gitee.com/mindspore/mindformers/issues/I8EA71
这是一个bug报告，涉及SAM部分代码的运行问题，报错信息提示运行sam_by_prompt.py时出现错误。可能是由于代码逻辑错误或依赖库配置问题导致。

https://gitee.com/mindspore/mindformers/issues/I8E6X5
这是一个bug报告，涉及主要对象是"baichuan2_7b"，由于执行推理进行增量模型推理时，预期模型推理成功，实际推理失败。

https://gitee.com/mindspore/mindformers/issues/I8E0J6
这是一个bug报告，主要涉及到baichuan213b高阶接口推理和pipeline4卡报错。导致bug的原因可能是mindspore版本2.2和2.1.1在推理过程中出现报错。

https://gitee.com/mindspore/mindformers/issues/I8E0I6
这是一个用户提出需求的issue，主要涉及Mindformers团队对310p推理卡在实现mindspore lite框架推理的适配问题。原因是用户想通过310p推理卡完成该工作，需要了解如何进行适配。

https://gitee.com/mindspore/mindformers/issues/I8DWMR
这是一个bug报告，主要涉及自动权重转换步骤中生成分布式策略文件时出现的报错。由于配置参数错误或程序逻辑问题，导致生成的日志显示报错信息。

https://gitee.com/mindspore/mindformers/issues/I8DT5Y
这是一个关于bug报告的issue，针对MindFormers下910B平台上的ChatGLM2模型微调无效的问题。由于lora微调和ptuning2微调均未产生效果，无论是在单卡还是8卡的情况下都没有改善模型效果。

https://gitee.com/mindspore/mindformers/issues/I8DS9Z
这是一个bug报告，涉及主要对象是llama213b单卡推理，由于推理报错导致bug或寻求帮助。

https://gitee.com/mindspore/mindformers/issues/I8DS61
该issue类型是用户询问问题，主要涉及llama2权重转换和离线权重切分，用户询问是否需要联网进行自动权重转换以及如何在离线状态下对llama2权重进行切分，可能由于用户需要在没有网络连接的情况下进行相关操作或者对权重处理流程不清楚而提出这些问题。

https://gitee.com/mindspore/mindformers/issues/I8DMVN
这个issue为用户提出需求类型，主要涉及Embedding模型，由于用户关心在910b上能否运行Embedding模型，可能由于资源限制或者性能要求导致用户提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I8DLMM
这是一个bug报告，主要涉及mindformers下的大模型bloom_7.1b在lite推理结果不正确的问题。原因可能是ckpt导出mindir模型后进行推理时出现了错误。

https://gitee.com/mindspore/mindformers/issues/I8DI8K
这是一个bug报告，主要涉及到baichuan213b双机16卡训练，导致训练阻塞在collecting strategy并显示进度条50%。

https://gitee.com/mindspore/mindformers/issues/I8DD7J
这是一个bug报告类型的issue，主要涉及mindformers下的llama2_7b模型执行batch推理时存在预期结果正确但实际结果有随机性的问题。

https://gitee.com/mindspore/mindformers/issues/I8DCLT
这是一个bug报告，主要涉及模型转换失败的问题，由于某种原因导致。

https://gitee.com/mindspore/mindformers/issues/I8DCJV
这是一个bug报告，涉及的主要对象是llama2_7b模型，由于执行export.py进行batch推理时，实际推理结果为乱码。

https://gitee.com/mindspore/mindformers/issues/I8D7BF
这是一个bug报告，涉及的主要对象是llama2_13模型转换。由于执行export.py进行增量模型转换时，预期模型转换成功，但实际转换失败。

https://gitee.com/mindspore/mindformers/issues/I8CVGQ
这是一个bug报告，主要涉及的对象是gpt2单卡增量推理自动化，由于x_shape与out_shape不匹配导致了报错。

https://gitee.com/mindspore/mindformers/issues/I8CV4X
这是一个关于bug报告的issue，主要涉及llama7b在8卡训练使用master版本mindspore时报错的问题。由于什么样的原因导致了这样的bug需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/I8CV1Q
这是一个bug报告issue，主要涉及MindSpore模型运行出错，可能是由于环境配置错误导致。

https://gitee.com/mindspore/mindformers/issues/I8CPL8
这是一个bug报告，问题涉及的主要对象是使用baichuan2 13b进行双卡分布式推理时，模型推理结果始终与输入内容相同。这个问题可能是由于推理过程中的错误操作或者代码逻辑问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8CMZ3
这是一个bug报告，涉及bloom7.1B增量推理不同问题之间性能差距过大的问题。

https://gitee.com/mindspore/mindformers/issues/I8CMOE
这是一个bug报告，涉及的主要对象是ChatGLM2 ptuning v2代码，由于load_checkpoint和dataset部分的修改导致单卡执行时出现异常。

https://gitee.com/mindspore/mindformers/issues/I8CLTD
该issue类型为需求提出，主要对象是qwen14b，用户提出关于适配时间的问题。可能由于该项目尚未适配qwen14b导致用户询问适配时间。

https://gitee.com/mindspore/mindformers/issues/I8CJHH
这是一个bug报告，涉及的主要对象是MindformerConfig。由于无法从'mindformers'导入'MindformerConfig'，导致进行llama推理时出现ImportError错误。

https://gitee.com/mindspore/mindformers/issues/I8CHIW
这个issue属于用户提出需求类型，涉及主要对象为使用mindspore实现语料向量化的功能。用户提出的问题是如何通过阅读mindspore的文档来实现与指定代码相同的功能。

https://gitee.com/mindspore/mindformers/issues/I8CH2N
这是一个bug报告，涉及主要对象是Baichuan13B模型推理。这个问题可能由于合并分布式训练模型时出现的维度错误导致，需要排查从何处出现问题。

https://gitee.com/mindspore/mindformers/issues/I8CFWV
这是一个用户提出需求的issue，主要对象是"自动设置hccl_connect_time"，导致这个问题可能是由于缺乏自动设置hccl_connect_time的功能而引起的。

https://gitee.com/mindspore/mindformers/issues/I8CAPI
这是一个需求提出类型的issue，主要对象为MSLite优化中的Llama后处理gather入图功能。

https://gitee.com/mindspore/mindformers/issues/I8C8KM
这是一个bug报告，涉及对象是Baichuan2中的Lora微调和推理功能。由于动态内存分配失败和推理结果缺失，可能导致报错或者无法获取到正确的推理结果。

https://gitee.com/mindspore/mindformers/issues/I8C8AF
这是一个bug报告，该问题涉及到分布式推理代码在启动过程中内存占用异常增加的问题。可能由于代码实现方式不当导致每张卡内存成倍递增。

https://gitee.com/mindspore/mindformers/issues/I8C4QL
这是一个关于讨论多机多卡训练数据和权重上传方式的问题，类型为用户请教。主要涉及的对象是多机多卡训练的数据和权重上传。

https://gitee.com/mindspore/mindformers/issues/I8C3N4
这是一个bug报告，主要涉及ChatGLM26B低参微调异常，可能是由于代码更新或参数设置错误导致的问题。

https://gitee.com/mindspore/mindformers/issues/I8BZCH
这是一个关于用户提出需求的issue，主要涉及mindformers中大模型部署方案的问题。用户询问是否有推荐的部署方案，以及如何在mindspore serving中部署类似llama模型的问题。

https://gitee.com/mindspore/mindformers/issues/I8BUL0
这个issue类型为用户提出需求，主要涉及的对象是chatglm3，由于用户想知道是否支持chatglm3而提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I8BO16
这是一个bug报告，主要涉及到LlamaForCausalLM中train模式下token和label维度不匹配的问题，可能是由于self.slice步骤导致的。

https://gitee.com/mindspore/mindformers/issues/I8BJVX
这是一个用户就单机多卡推理生成rank file的问题寻求帮助的issue。该问题涉及的主要对象是机器上的NPU。由于用户想进行单机多卡推理，但遇到了生成rank file的问题，不清楚如何解决以及是否需要将两个卡插到一个group中。

https://gitee.com/mindspore/mindformers/issues/I8BIB5
这是一个bug报告，主要涉及llama_transformer.py中使用Concat算子问题，由于使用了Add算子而引发了bug。

https://gitee.com/mindspore/mindformers/issues/I8BIA2
这是一个用户提出的问题类型的issue，主要涉及的对象是软件环境的迁移。由于从910A测试环境迁移到910B环境，可能需要修改相关配置，导致用户遇到紧急情况求助。

https://gitee.com/mindspore/mindformers/issues/I8BI31
这是一个bug报告，主要对象是pipeline中的get_pet_model接口，由于未调用最新的pet框架，导致缺少微调结构。

https://gitee.com/mindspore/mindformers/issues/I8ATAV
这个issue属于优化需求，主要对象是当前注册机制。由于每次导入全量类导致导入速度慢且所有依赖库必须全部安装，导致了臃肿的问题。

https://gitee.com/mindspore/mindformers/issues/I8ASM8
这是一个bug报告，涉及运行llama2的generate推理时出现报错，主要涉及对象是软件和硬件环境。由于某些原因导致报错，用户寻求帮助解决这个问题。

https://gitee.com/mindspore/mindformers/issues/I8ANGM
这是一个bug报告，涉及主要对象为运行baichaun2的Pipeline推理的软件系统。由于代码报"not a string"错误，可能是由于输入数据类型问题导致的。

https://gitee.com/mindspore/mindformers/issues/I8AJR7
这是一个需求讨论类型的issue，主要涉及多卡推理下如何切分ckpt，用户希望了解如何在llama27b模型中进行模型切分。

https://gitee.com/mindspore/mindformers/issues/I8AHV4
这是一个关于用户需求的issue，涉及主要对象是大模型多轮对话中的history构建，用户询问如何在模型推理中添加多轮对话的history。

https://gitee.com/mindspore/mindformers/issues/I8AGTH
这是一个bug报告类型的issue，主要涉及fastchat npu推理大模型使用报错，可能由于代码bug或环境配置问题导致。

https://gitee.com/mindspore/mindformers/issues/I8AFET
这是一个用户询问问题的类型，主要涉及对象是在昇腾910服务器上使用CPU执行LLaMA 2的推理，原因可能是相关文档缺失导致用户不清楚如何在CPU上执行推理。

https://gitee.com/mindspore/mindformers/issues/I8AE0X
该issue为bug报告类型，主要涉及LLaMA 2微调过程中处理多轮对话数据格式错误导致数值处理异常。

https://gitee.com/mindspore/mindformers/issues/I8AARX
这是一个bug报告，主要涉及llama213B 2卡generate推理报错的问题，由于指针manager为空导致错误报错。

https://gitee.com/mindspore/mindformers/issues/I8A6EW
这是一个bug报告，涉及对象为llama2中的pipeline。由于无法找到特定class name导致数值报错。

https://gitee.com/mindspore/mindformers/issues/I89XNM
这是一个用户需求问题，涉及主要对象为分布式微调，用户想要在分布式微调的同时完成自动权重转换，但执行时出现报错。

https://gitee.com/mindspore/mindformers/issues/I89OQV
这是一个bug报告，主要涉及blip2网络推理性能小于指标值的问题，可能由于某些原因导致推理性能不达预期。

https://gitee.com/mindspore/mindformers/issues/I89NWF
这是一个bug报告，主要涉及baichuan2的readme里url拼写错误导致无法下载文件的问题。

https://gitee.com/mindspore/mindformers/issues/I89N04
该issue类型为用户提出需求，主要涉及对象是数据集格式。由于ChatGLM26B目前仅支持.json格式数据集，用户希望了解平台标准数据集格式对于csv或tsv的要求。

https://gitee.com/mindspore/mindformers/issues/I89KSZ
这是一个bug报告，主要对象是参数问题。导致该问题的原因是缺少输入图片说明。

https://gitee.com/mindspore/mindformers/issues/I89K80
这是一个bug报告，涉及到glm低参微调任务，报错原因可能是训练过程中出现了异常情况。

https://gitee.com/mindspore/mindformers/issues/I89IXP
这是一个bug报告类型的issue，主要关于blip2 readme中“基于pipeline的推理”中“单卡pipeline推理”的报错，用户受影响的环境是使用MindSpore 2.2.0.20231016版本进行推理时出现的问题。

https://gitee.com/mindspore/mindformers/issues/I89IO1
这是一个bug报告，涉及的主要对象是Mindformers库中的blip2 readme中的示例代码。导致报错的原因是在运行示例代码时出现了ValueError: too many values to unpack (expected 2)。

https://gitee.com/mindspore/mindformers/issues/I89FQ8
这是一个文档更新类型的issue，主要涉及mslite推理910b推荐配置，提出了配置更新要求及相关说明。

https://gitee.com/mindspore/mindformers/issues/I89DGX
这是一个bug报告，主要涉及mindformers库中的模块导入错误。可能是由于导入路径或依赖关系问题导致的。

https://gitee.com/mindspore/mindformers/issues/I89D5U
这是一个用户需求类的issue，主要涉及多卡推理支持交互式接口的实现问题，由于采用的模型需要多卡推理且无法同时拉起server服务，导致提供对外服务时存在问题。

https://gitee.com/mindspore/mindformers/issues/I89CH1
这是一个bug报告，主要涉及MindForgers在Ascend 910b上推理速度慢的问题。原因可能是硬件适配或代码优化不足导致。

https://gitee.com/mindspore/mindformers/issues/I897WD
这个issue类型是用户提出需求，涉及的主要对象是MindFormers的构建过程。由于无法方便地定位源码位置，测试人员提出了需求希望构建时能够附带commit id信息。

https://gitee.com/mindspore/mindformers/issues/I897LA
这是一个bug报告，主要涉及的对象是NPU（神经处理单元），由于NPU未被正确使用，导致用户在text_generation任务中无法生成文本。

https://gitee.com/mindspore/mindformers/issues/I896ZJ
这是一个bug报告类型的issue，涉及的主要对象是mindformers软件安装过程。导致这个bug的原因是两个不同版本的mindformers软件存在冲突依赖关系，引发了安装错误。

https://gitee.com/mindspore/mindformers/issues/I895U5
这是一个需求提出类型的issue，主要涉及Mindformer项目是否会集成Vicuna算法。这个问题由用户对Mindformer在近期是否增加Vicuna算法的集成进行了询问。

https://gitee.com/mindspore/mindformers/issues/I894BN
这是一个Bug报告，涉及的主要对象是`glm2_6b`模型。由于`glm2_6b`模型在加载过程中出现了KeyError导致的错误。

https://gitee.com/mindspore/mindformers/issues/I892AJ
这是一个bug报告，主要涉及的对象是baichuan13B 6机48卡训练。由于配置不同导致训练过程中出现OOM（内存溢出）的问题。

https://gitee.com/mindspore/mindformers/issues/I891MM
这是一个bug报告，主要涉及模型chatglm在华为云ModelArts环境中启动报错的问题，可能由于模型加载特别慢导致推理结果缓慢。

https://gitee.com/mindspore/mindformers/issues/I88YOB
这是一个关于功能使用的问题，主要涉及ChatGLM26B单机多卡评估，由于对命令参数理解有误导致提出了关于单机多卡评估和权重文件合并的问题。

https://gitee.com/mindspore/mindformers/issues/I88WPS
这是一个bug报告，涉及到在ckpt落盘时时间较长且存储开销过大的问题。这是由于每次在saveckpt时会存储两份权重，一份带有优化器参数，一份不带，且无法选择导致的。

https://gitee.com/mindspore/mindformers/issues/I88UEH
这是一个bug报告，涉及到Bloom模型在使用mindir+mindspore_lite后端推理时出现错误。原因可能是配置文件和模型之间的不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/I88S4Z
该issue类型是关于功能支持的询问，主要涉及对象是在nvidia jetson嵌入式系统上使用mindspore和mindformer。询问是由于用户想知道这两个平台是否支持在nvidia jetson上使用，以及如果不支持的话，是全部功能不支持还是部分功能不支持。

https://gitee.com/mindspore/mindformers/issues/I88R4T
这是一个bug报告，主要涉及LLaMA2文档链接错误问题，可能导致用户无法访问正确的文档页面。

https://gitee.com/mindspore/mindformers/issues/I88QHM
这是一个Bug报告类型的Issue，主要涉及mindformers中Python报错的问题，用户遇到了Python报错导致程序出现问题。

https://gitee.com/mindspore/mindformers/issues/I88ACJ
这是一个用户提出需求的类型，主要涉及对象是保存检查点（checkpoint）时缺少hasattr函数。

https://gitee.com/mindspore/mindformers/issues/I8857D
这是一个关于需求的问题，涉及主要对象是脚本运行和参数设定，用户想要知道如何在运行脚本时加入device_num参数。

https://gitee.com/mindspore/mindformers/issues/I87YCW
这是一个关于bug报告的issue，主要涉及到llama在转换数据格式时缺少fschat第三方库时出现报错。这可能是由于缺少fschat库导致的问题。

https://gitee.com/mindspore/mindformers/issues/I87YAX
这是一个bug报告类型的issue，主要涉及llama2 finetune操作，报告了显存溢出的错误。

https://gitee.com/mindspore/mindformers/issues/I87XD2
这是一个bug报告，主要对象是mindformers项目下的边训练边评估页面资料存在格式问题。这个问题可能是由于页面格式错误导致的。

https://gitee.com/mindspore/mindformers/issues/I87QI3
这是一个bug报告，涉及问题是在使用llama进行预训练时出现了shape问题。由于输入的shape无法在CPU/GPU上广播，导致出现数学计算异常的ValueError。

https://gitee.com/mindspore/mindformers/issues/I87GSG
这是一个bug报告，主要涉及的对象是昇腾910执行run_chat_web.py加载glm2_6b模型后无法请求到web端，可能是由于网络请求受防火墙限制导致。

https://gitee.com/mindspore/mindformers/issues/I87A3T
这是一个bug报告，涉及mindformers在进行分布式训练时出现的TypeError。可能由于转换相同类型未生效导致的错误。

https://gitee.com/mindspore/mindformers/issues/I87968
这是一个bug报告，涉及到运行`run_glm2_6b`的evaluate出现报错。原因是在`eval_dataset`的配置中，`max_source_length`和`max_target_length`应该设置成都小于`seq_length`，否则会导致报错。

https://gitee.com/mindspore/mindformers/issues/I8767X
这是一个Bug报告类型的issue，主要对象是MindSpore框架下的操作类BroadcastTo。由于输入的初始化值应为常量，但实际传入的是AbstractTuple，导致出现了TypeError。

https://gitee.com/mindspore/mindformers/issues/I875ML
这是一个bug报告，该问题涉及Baichuan2文档链接失效，导致404页面错误。

https://gitee.com/mindspore/mindformers/issues/I87135
这是一个bug报告，主要涉及Llama2的pad_token未更新为(r0.8分支)，导致处理数据集到mindrecord格式输出错误。

https://gitee.com/mindspore/mindformers/issues/I86YMR
该issue类型为bug报告，主要涉及mindformers对GPU支持的问题，由于之前回复表示所有模型不适用于GPU，但实际情况下在GPU上成功预训练和微调GPT2。

https://gitee.com/mindspore/mindformers/issues/I86WQY
这是一个Bug报告，涉及对象为MindSpore推理过程中使用mindir进行增量推理时出现的错误。由于运行`run_infer_main.py`时出现了Graph执行失败的错误，可能是由于模型推理过程中的运算图执行出现了异常导致的问题。

https://gitee.com/mindspore/mindformers/issues/I86W57
这个issue类型是链接无法打开的bug报告，主要涉及到文档链接的访问问题，可能是由于链接地址错误或目标页面不存在所导致。

https://gitee.com/mindspore/mindformers/issues/I86UNE
这是一个用户请教问题的issue，主要涉及到auto_trans_ckpt参数的使用。由于Baichuan2模型要求使用该参数，而其他模型没有相应的说明，用户想了解该参数应该如何使用。

https://gitee.com/mindspore/mindformers/issues/I86TZ4
这是一个bug报告，涉及到分布式策略设置的问题，由于未进行校验导致实际同时运行了两个模型。

https://gitee.com/mindspore/mindformers/issues/I86TD3
这是一个bug报告，涉及主要对象是llama增量推理模型导出，由于传入的参数类型错误导致了TypeError异常。

https://gitee.com/mindspore/mindformers/issues/I86RRR
这是一个bug报告，主要涉及的对象是mindformers项目中的输出路径自定义问题。导致该问题的原因可能是在配置文件中指定了自定义的输出路径，但实际输出文件夹仍然保持为默认的output路径。

https://gitee.com/mindspore/mindformers/issues/I86R9V
这是一个bug报告，涉及到mindformers/auto_class中的AutoConfig对llama2的处理存在问题，导致llama2不在支持列表里，需要将llama2单独作为一个key值。

https://gitee.com/mindspore/mindformers/issues/I86KKX
这是一个bug报告，涉及的主要对象是llama模型的导出增量图功能。由于model.add_flags_recursive(is_first_iteration=False)设置不成功，导致导出增量图时出现错误报告。

https://gitee.com/mindspore/mindformers/issues/I86IXA
这是一个bug报告，主要涉及Mindformers中日志的性能数据在数据并行模式下存在错误。这个问题可能是由于并行模式下数据处理逻辑错误导致性能数据异常而引起的。

https://gitee.com/mindspore/mindformers/issues/I86CQM
这是一个bug报告，涉及主要对象为AICC（Auto AI Computing Cluster）。由于将use_past设置为True进行分布式推理时，导致了运行infer_predict_layout时报错“operator ExpandDims init failed”。

https://gitee.com/mindspore/mindformers/issues/I86C3R
这是一个用户提出需求的issue，主要涉及mindformers在GPU上的支持问题，用户想了解是否可以在GPU上实现llama训练及部署。

https://gitee.com/mindspore/mindformers/issues/I7ZIP7
这个issue是关于提Issue指导的，主要对象是MindFormers开源仓库的用户。 

https://gitee.com/mindspore/mindformers/issues/I860M0
这是一个bug报告issue，涉及Mslite pipeline接口配置ge_config无效的问题，由于相关pipeline接口没有使用该传参，导致只能从配置文件中读取。

https://gitee.com/mindspore/mindformers/issues/I85T3I
这个issue是一个bug报告，涉及到gitee首页llama2的链接指向了llama的model_cards，可能是由于链接指向错误导致的。

https://gitee.com/mindspore/mindformers/issues/I85LT3
这是一个用户请教问题的issue，主要涉及yaml文件中自定义输出需求的操作方式，由于源码中是从callback中操作的，用户想知道如何入手修改，并需要注意哪些问题。

https://gitee.com/mindspore/mindformers/issues/I85LHP
这是一个用户提出需求的类型issue，主要对象是华为mindspore是否支持英伟达GPU部署llama2_7B，由于目前华为mindspore的教程只提供了Ascend 910A部署llama2的方法，用户希望得知能否使用英伟达GPU进行部署。

https://gitee.com/mindspore/mindformers/issues/I85C54
这个issue类型是bug报告，主要涉及的对象是LLAMA2全量微调过程中出现的推理效果差问题。由于未知原因导致微调后模型性能开始变差，进而出现模型推理无输出的情况。

https://gitee.com/mindspore/mindformers/issues/I850NM
这是一个技术问题报告issue，涉及到mindformers的安装及hccn_tool的找不到问题，导致无法获取hccn.conf文件。

https://gitee.com/mindspore/mindformers/issues/I84WGK
这是一个Bug报告，主要涉及"baichuan7B"使用pipeline推理时出现乱码的问题，可能是由于数据格式不匹配或编码问题导致的。

https://gitee.com/mindspore/mindformers/issues/I84VUM
这个issue类型是bug报告，涉及的主要对象是Baichuan213b版本推理精度问题。原因可能是推理结果与torch存在差异导致逻辑不一致。

https://gitee.com/mindspore/mindformers/issues/I84TBZ
这是一个 bug 报告，主要涉及到日志信息打印问题，由于未打印出具体配置信息，导致了用户反馈的问题。

https://gitee.com/mindspore/mindformers/issues/I84T9H
这个issue类型是需求提出，主要涉及Mindformer对310P卡推理的支持情况。原因可能是用户想了解Mindformer何时开始支持310P卡的推理功能。

https://gitee.com/mindspore/mindformers/issues/I84QIL
该issue类型为bug报告，主要涉及dev分支glm2 mslite推理设置is_sample_accelation为false时失败。由于设置is_sample_accelation为False导致推理失败。

https://gitee.com/mindspore/mindformers/issues/I84QDO
这是一个bug报告，主要涉及llama2推理应用在910B云环境上部署时出现的报错。问题可能由部署环境不兼容或配置错误导致。

https://gitee.com/mindspore/mindformers/issues/I84PII
该issue是一个功能需求提出，涉及的主要对象是程序中的规则校验操作，由于当前流水线并行切分数目大于模型层数时报错不明显，需要增加校验功能来抛出错误。

https://gitee.com/mindspore/mindformers/issues/I84NO9
这是一个用户提出需求的issue，主要涉及到模型文档缺失分布式权重加载说明，导致用户无法进行模型训练。

https://gitee.com/mindspore/mindformers/issues/I84JRI
这是一个用户需求的issue，主要涉及集群支持NFS网络存储模式下因手动生成策略文件、手动切分权重和加载权重微调步骤繁琐导致操作不友好的问题。

https://gitee.com/mindspore/mindformers/issues/I84I8Z
这个issue是关于bug报告，主要对象是MindSpore 2.1.1版本中的存ckpt内存泄漏问题，可能由于未设置keep_checkpoint_max参数导致内存溢出症状。

https://gitee.com/mindspore/mindformers/issues/I84HMQ
这是一个用户提出需求的issue，主要对象是model cards，由于需要支持lite推理，所以提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I84H1Y
这是一个bug报告，主要涉及对象是代码中的'self.key_past'变量。原因是'self.key_past'未在初始化函数中作为'Parameter'类型被初始化，导致出现'TypeError'症状。

https://gitee.com/mindspore/mindformers/issues/I84GAM
这个issue类型为用户提出需求，主要涉及对象是模型适配问题。由于用户想了解哪些模型适配了910B，导致提出这个问题。

https://gitee.com/mindspore/mindformers/issues/I84DYJ
这是一个bug报告，涉及主要对象是GLM2 MSlite设置后处理入图推理的功能。由于设置is_sample_acceleration为True时导致MSLite GE推理报错，需要分析原因并修复该bug。

https://gitee.com/mindspore/mindformers/issues/I849UX
这是一个用户提出需求的类型，主要涉及LLama支持动态shape及动态分档lite推理的问题。由于原因未提及，用户可能提出了关于新功能实现的问题或寻求相关帮助。

https://gitee.com/mindspore/mindformers/issues/I8486A
这是一个bug报告，主要涉及的对象是mindformers软件安装后导致mindspore在Ascend 910b平台上导入报错的问题。

https://gitee.com/mindspore/mindformers/issues/I847HO
这是一个bug报告，该问题涉及到训练过程中per step time时间异常变化的情况，可能是由于存储checkpoint操作导致的。

https://gitee.com/mindspore/mindformers/issues/I842KQ
这是一个bug报告类型的issue，主要涉及Mindformers库中的MFLossMonitor类，由于不支持'global_batch_size'参数导致出现TypeError错误。

https://gitee.com/mindspore/mindformers/issues/I83ZR2
这是一个bug报告，主要涉及chat_web_demo项目在对话轮数过多时会因长度超出而报错，建议添加长度截断逻辑。

https://gitee.com/mindspore/mindformers/issues/I83ZIS
这是一个bug报告，涉及的主要对象是Baichuan2在设置不同seq_length时报错。导致这个bug的原因是在yaml文件中设置其他长度时未能正确处理，默认值导致shape不匹配的错误。

https://gitee.com/mindspore/mindformers/issues/I83ZEV
这是一个bug报告类型的issue，主要涉及到mindformers下research仓库中模型多机物理机训练指导文档中指令错误的问题。

https://gitee.com/mindspore/mindformers/issues/I83QZD
这是一个用户提出需求的issue，主要涉及到Xverse13B推理适配需求。由于客户业务需要卡不够用且希望转向信创用国产化，所以需要进行昇腾适配，目标是拿下客户2000卡空间。

https://gitee.com/mindspore/mindformers/issues/I83MVM
这是一个bug报告，主要涉及chat_web_demo中use_past设置无效的问题，由于数据类型定义错误导致。

https://gitee.com/mindspore/mindformers/issues/I83L3I
这是关于模块导入错误的bug报告，涉及到MindSpore2.0py39版本下的`mindformers.models.utils`模块。可能由于模块内部的`cell_reuse`无法被正确导入，导致了此问题的产生。

https://gitee.com/mindspore/mindformers/issues/I83KS3
这是一个Bug报告，主要涉及的对象是安装mindspore过程中遇到的libprotobuf错误。这个问题可能是因代码冲突导致的。

https://gitee.com/mindspore/mindformers/issues/I83HY9
这是一个用户提出需求的类型，主要涉及mindformers适配xverse13b推理，由于需求适配FasterTransformer进行推理以及处理xverse模型的参数差异等限制导致。

https://gitee.com/mindspore/mindformers/issues/I83A16
这是一个关于API文档整改规则的问题。类型是用户提出需求，并主要涉及文档内容的统一规范。由于文档内容不规范，可能导致用户阅读困难或者理解不清晰。

https://gitee.com/mindspore/mindformers/issues/I835FG
这是一个bug报告，主要对象是2.6b版本的盘古单卡训练无法启动，可能是因为教程未提供相应说明导致的。

https://gitee.com/mindspore/mindformers/issues/I830E7
这是一个用户在提出需求的类型的issue，主要涉及支持llama34B的计划或codellama34B。用户可能由于缺乏支持或信息，提出该需求以获取相关帮助。

https://gitee.com/mindspore/mindformers/issues/I830A6
这个issue是关于文档规范的需求，主要对象是模型卡片（model cards）。由于缺乏统一的文档规范和模板，需要制定规范并提供模板以便更好地管理模型文档内容。

https://gitee.com/mindspore/mindformers/issues/I82ZLL
这是一个bug报告，主要涉及的对象是chatglm26B单卡推理速度慢，可能原因是推理速度稳定在19 tokens/s。

https://gitee.com/mindspore/mindformers/issues/I82ZGS
这个issue是一个bug报告类型，主要涉及推理模块中推理输出日志写入模式的问题，由于当前写入模式为覆盖，导致只能显示最后一条结果。

https://gitee.com/mindspore/mindformers/issues/I82XLX
这个issue是一个易用性问题，主要涉及到训练权重加载功能，用户易忘记打开或关闭自动转换权重的开关，导致报错频繁，造成使用上的困扰。

https://gitee.com/mindspore/mindformers/issues/I82USW
这是一个bug报告，涉及vit在8卡时运行报错的情况，可能是由于mindspore版本1.10.0和服务器型号、芯片不兼容导致的。

https://gitee.com/mindspore/mindformers/issues/I82QAD
这个issue是一个用户提出的需求，主要涉及套件易用性的提升，由于当前参数设置存在限制，导致在不同运行模式下会产生报错，需增加前序校验来解决这个问题。

https://gitee.com/mindspore/mindformers/issues/I82PMX
这是一个bug报告类型的issue，主要涉及mindspore2.0.0环境下在Ascend910b机器上进行lora微调报错的问题。原因可能是编译错误导致的。

https://gitee.com/mindspore/mindformers/issues/I82LU7
这个issue类型为bug报告，涉及到在执行bash build.sh安装时出现"Invalid version"报错。原因可能是版本不匹配或格式不正确导致安装失败。

https://gitee.com/mindspore/mindformers/issues/I82JCL
这是一个bug报告，该问题涉及到GPT2文本分类数据处理缺失参数arg.tokenizer，导致代码无法正确执行。

https://gitee.com/mindspore/mindformers/issues/I82IAD
这个issue属于用户提出问题类型，主要涉及BERT模型的权重保存和加载问题，可能由于缺乏对应的加载接口导致加载失败。

https://gitee.com/mindspore/mindformers/issues/I82GZD
这是一个bug报告类型的issue，主要涉及chatglm/chatglm2推理性能较差，可能由于性能低下导致对话推理时间较长。

https://gitee.com/mindspore/mindformers/issues/I82CU6
这是一个bug报告，主要涉及到在北京AICC的modelarts进行chatglm2全参微调时遇到报错的问题。原因可能是参数设置的问题导致报错，需要进一步解决。

https://gitee.com/mindspore/mindformers/issues/I82BIA
这是一个用户提出问题的issue，主要涉及n卡机器上单机多卡训练报错，用户询问在这种情况下应该如何处理。原因是使用ascend机器的hccl无法在n卡机器上正常执行训练导致报错。

https://gitee.com/mindspore/mindformers/issues/I82ABM
这是一个bug报告，主要涉及下载模型时遇到网络中断导致无法续传或重新读取权限错误问题。

https://gitee.com/mindspore/mindformers/issues/I828IZ
这是一个bug报告，涉及对象是baichuan2模块，由于输入数据维度不匹配导致数学运算出错。

https://gitee.com/mindspore/mindformers/issues/I824ZS
这是一个用户提出需求的问题，主要涉及对象是mindrecord文件。由于数据集在已有mindrecord文件基础上做了新增，用户希望了解如何在不重新生成mindrecord的情况下增量添加新的数据。

https://gitee.com/mindspore/mindformers/issues/I81V3U
这是一个bug报告类型的issue，主要涉及glm2推理报错的问题，由于某种原因导致推理功能出现了错误。

https://gitee.com/mindspore/mindformers/issues/I81V32
这是一个bug报告类型的issue，主要涉及到glm库。由于推理过程出现错误导致报错。

https://gitee.com/mindspore/mindformers/issues/I81OIR
这是一个bug报告，主要涉及到Baichuan2项目在进行部署时出现了运行时错误和推理问题。这可能是由于未正确配置Ascend环境以及模型路径未正确指定所致。

https://gitee.com/mindspore/mindformers/issues/I81N6G
这个issue类型为需求提出，主要涉及自己数据集格式转换问题，由于数据格式与大模型的输入格式不匹配导致需要微调和评估的困扰。

https://gitee.com/mindspore/mindformers/issues/I81G3K
这个issue是一个bug报告，主要对象涉及mindformers项目下的glm2模型。由于未指定推理结果保存路径或格式，并且没有显示推理的结果，导致用户无法找到推理结果。

https://gitee.com/mindspore/mindformers/issues/I81ERG
该issue为用户提出需求类型的问题单，涉及主要对象为Llama2和codellama结构，用户想了解Llama2是否支持codellama结构。

https://gitee.com/mindspore/mindformers/issues/I8159H
这是一个bug报告，涉及到T5模型在转换为mindspore过程中出现错误的问题。原因可能是转换脚本或环境配置的问题导致转换出错。

https://gitee.com/mindspore/mindformers/issues/I8153B
这个issue是关于需求的，主要对象是在已训练bert模型迁移工作中需要yaml和json配置项的对应。可能由于参数在torch的json文件和mindspore中yaml文件的命名方式不同，导致需要整合为对应的yaml文件或文档。

https://gitee.com/mindspore/mindformers/issues/I8138Z
这是一个bug报告，主要涉及LLAMA推理过程中出现的问题。由于BatchMatMul算子的广播机制只支持ascend，导致出现数值错误并报错。

https://gitee.com/mindspore/mindformers/issues/I80WTK
这是用户提出关于Mindpet套件中微调算法使用及如何进一步优化模型的问题。

https://gitee.com/mindspore/mindformers/issues/I80WD6
这个issue属于用户提出需求，主要对象是模型训练过程中的数据集。导致问题的原因是用户可能未清楚需要手动添加标签，导致微调效果差或浪费资源。

https://gitee.com/mindspore/mindformers/issues/I80U3N
这是一个bug报告，涉及LLaMATokenizer配置错误导致推理报错。

https://gitee.com/mindspore/mindformers/issues/I80R8V
这是一个需求提出的issue，主要对象是LoRA微调功能，用户提出的问题是希望优化存储ckpt时仅存可训练参数的功能，以及加载时通过yaml文件配置加载原始权重和lora可训权重，原因可能是为了减少磁盘占用空间和提升使用方便性。

https://gitee.com/mindspore/mindformers/issues/I80Q4M
该issue属于bug报告类型，主要对象可能是一个名为"快速推理"的功能，原因可能是快速推理功能没有给出正确的结果。

https://gitee.com/mindspore/mindformers/issues/I80NB5
这是一个bug报告类型的issue，涉及mindformers在ascend910上微调后输出和问题相同的句子，由于未能正确安装dev环境，导致出现该问题。

https://gitee.com/mindspore/mindformers/issues/I80MEO
这是一个用户对于分布式权重自动转换方案的问题，需要解决的主要对象是Mindformers项目。用户提出了对于全量微调后权重文件生成、流水线并行参数配置的疑问。

https://gitee.com/mindspore/mindformers/issues/I80JJ4
这是一个Bug报告，主要涉及的对象是 AICC 中的 baichuan27b 模型，在执行过程中出现了无法引用包的错误，可能是由于代码更新或依赖问题导致的。

https://gitee.com/mindspore/mindformers/issues/I80F1W
这是一个用户需求问题，涉及到GLM26B模型的分布式评估和推理路径选择问题。由于无法选择分布式全量微调后合并的merge.ckpt文件路径和评估数据集路径，用户无法按需执行评估和推理。

https://gitee.com/mindspore/mindformers/issues/I80BJX
这个issue属于用户提出需求类型，主要涉及的对象是VideoLLaMA软件的适配计划。用户提出这个问题是想了解VideoLLaMA是否有适配的计划。

https://gitee.com/mindspore/mindformers/issues/I808UP
这是一个bug报告，主要涉及的对象是"glm2 lora微调"，该问题可能由于环境设置或驱动版本不兼容而导致该报错情况。

https://gitee.com/mindspore/mindformers/issues/I7ZW0D
这个issue类型为bug报告，主要涉及mindformers在ascend910上微调后推理时间过长的问题，可能原因是代码逻辑问题或硬件适配性不佳。

https://gitee.com/mindspore/mindformers/issues/I7ZV1I
这是一个Bug报告，主要问题是LLAMA 7B无法在单个GPU上启动，可能是由于输入形状无法在CPU/GPU上进行广播导致的数值错误。

https://gitee.com/mindspore/mindformers/issues/I7ZNL2
这是一个bug报告，涉及的主要对象是GLM2模型推理结果被截断，可能是由于设置的参数导致输出结果被截断。

https://gitee.com/mindspore/mindformers/issues/I7ZMKG
这是一个bug报告，主要涉及mindformers项目中的ChatGLM6B模型在web_chat demo推理结果异常的问题。由于启动时配置文件内容异常，导致系统无法正确加载模型和tokenizer，进而造成推理结果异常。

https://gitee.com/mindspore/mindformers/issues/I7ZM0Y
这是一个bug报告，涉及的主要对象是baichuan模型在转换数据集格式时出错。这个问题很可能是由于脚本将数据集中的utf码当成文本导致无法正常转换。

https://gitee.com/mindspore/mindformers/issues/I7Z76I
这个issue类型是用户技术问题，涉及主要对象为使用者尝试进行微调后进行推理。由于权重文件命名导致推理错误，用户寻求解决方案。

https://gitee.com/mindspore/mindformers/issues/I7Z1ZY
这个issue属于bug报告类型，涉及的主要对象是数据预处理模块，由于缺少'tk.graph'模块导致了报错。

https://gitee.com/mindspore/mindformers/issues/I7Z0V2
这是一个bug报告，涉及对象为原始模型和微调后模型推理infer，导致错误的原因可能是配置和标记器中的参数未正确设置。

https://gitee.com/mindspore/mindformers/issues/I7YMRY
这是一个bug报告，涉及使用LoRA低参微调MindFormers模型执行单卡评估报错的问题，可能是由于参数配置错误导致的。

https://gitee.com/mindspore/mindformers/issues/I7Y2O4
这个issue属于bug报告类型，主要涉及的对象是llama13B模型在昇腾环境下训练报错。可能由于MindSpore版本和Python版本不匹配，导致报错截图中出现了错误信息。

https://gitee.com/mindspore/mindformers/issues/I7Y0YH
这是一个bug报告，主要涉及mindformers在启智平台安装时出现的包冲突问题。

https://gitee.com/mindspore/mindformers/issues/I7XQPO
这是一个关于环境配置或使用问题的issue，主要涉及的对象是mindformers1.0.0.dev202307和mindspore 1.10.1，可能由于固件版本不清楚或镜像配置导致无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I7XL9T
这是一个bug报告，主要涉及GLM模型在单机多卡环境下使用给定数据集时报错缺少"label"，可能是由于数据集中缺少"label"字段导致的。

https://gitee.com/mindspore/mindformers/issues/I7XGBE
这是一个bug报告，涉及的主要对象是Ascend910B双机16卡GLM网络进行finetune微调时默认配置文件中的参数设置问题，导致训练耗时长。

https://gitee.com/mindspore/mindformers/issues/I7XEZD
这是一个bug报告，涉及vit执行预训练报错编号EE9999的问题。可能由于软件环境中使用的MindSpore版本和其他参数设置不兼容所导致的错误。

https://gitee.com/mindspore/mindformers/issues/I7XEA2
这是一个bug报告，主要涉及MindSpore和MindFormers在单机8卡运行Glm时合并权重后报错的问题。由于操作步骤中可能存在某些步骤或参数设置不正确，导致了报错问题的发生。

https://gitee.com/mindspore/mindformers/issues/I7XDU4
这是一个bug报告，主要涉及全量微调权重合并操作的问题。由于切分策略文件路径下有多个文件，导致执行多次后无法将这些ckpt合成为一个的困扰。

https://gitee.com/mindspore/mindformers/issues/I7XALW
这是一个bug报告，主要涉及训练过程中的问题。由于resume training功能从0开始而不是接着上次的epoch step处进行训练，导致无法按预期继续训练。

https://gitee.com/mindspore/mindformers/issues/I7WXQX
这是一个bug报告，涉及MindFormers在配置2机8卡跑llama时出现了hccl通信失败的问题，可能是由于配置文件或参数设置错误导致的。

https://gitee.com/mindspore/mindformers/issues/I7WINK
这是一个BUG报告，主要涉及模型推理时web服务崩溃和web demo推理速度变慢的问题，可能由中断web连接导致服务崩溃和特定commit导致推理速度变慢。

https://gitee.com/mindspore/mindformers/issues/I7WESV
这是一个关于Baichuan13B支持的需求问题，该问题涉及的主要对象是Baichuan13B。

https://gitee.com/mindspore/mindformers/issues/I7WERQ
这是一个需求提交类型的 issue，主要涉及 GLM2 适配 lite 推理，用户反馈需要相关适配的功能。

https://gitee.com/mindspore/mindformers/issues/I7W5HY
这是一个用户提出需求的类型，主要涉及昇腾训练好的模型能否部署在GPU上以及提供权重转换脚本的问题。该问题由于算力成本高、昇腾模型无法在GPU上部署的困扰，用户希望寻求昇腾模型在GPU平台上部署的解决方案。

https://gitee.com/mindspore/mindformers/issues/I7VZKX
这个issue是一个bug报告，涉及的主要对象是ChatGLM26B微调的结果验证。由于某种原因导致微调后评估报错，用户寻求关于该报错的帮助。

https://gitee.com/mindspore/mindformers/issues/I7VYV8
这是一个bug报告，主要涉及的对象是使用ModelArts上vit进行训练时出现的错误。由于Aicpu kernel执行失败，导致训练过程中出现错误。

https://gitee.com/mindspore/mindformers/issues/I7VPVB
这是一个需求提出类型的issue，主要涉及盘古模型处理数据集脚本无法处理非标注文本的问题，可能是由于文档描述与实际脚本需求不一致导致。

https://gitee.com/mindspore/mindformers/issues/I7VHHL
这是一个用户提出需求的issue，主要涉及mindformers单卡训练脚本的使用方式是否更合理的讨论，原因是README中介绍的使用方式与仓库中实际脚本的命名不一致，导致用户疑惑。

https://gitee.com/mindspore/mindformers/issues/I7VE63
这是一个bug报告，主要涉及到ChatGLM6B模型在配置和启动LoRA低参微调脚本时报错。造成该问题可能是输入参数不匹配所致。

https://gitee.com/mindspore/mindformers/issues/I7V28J
该issue类型为需求提出，主要涉及对象为BLIP2的二阶段支持。用户询问什么时候会支持BLIP2的二阶段可能是因为用户希望了解项目的发展规划和进度。

https://gitee.com/mindspore/mindformers/issues/I7UYOP
这是一个bug报告，主要涉及Mindspore在训练LLaMA13B模型到step200时出现EE9999错误的问题，可能是由于环境变量未设置导致的节点转储文件保存错误。

https://gitee.com/mindspore/mindformers/issues/I7UXU3
这是一个bug报告，主要涉及到MindForMers项目中的Python脚本运行时出现错误。这个问题可能是由于Ascend310P3 NPU卡不受支持而导致的。

https://gitee.com/mindspore/mindformers/issues/I7UVFS
这是一个用户提出需求的issue，主要涉及pipeline接口加载指定权重的功能。用户希望能够在使用pipeline接口时指定加载特定的权重，由于当前功能不支持导致用户提出需求。

https://gitee.com/mindspore/mindformers/issues/I7USPX
这是一个用户提出需求的类型，主要涉及对象为模型mae，用户想了解mae是否仅支持vitbase，是否可以支持更大参数量的模型（如vitlarge或vithuge），原因可能是用户希望在使用mae时能够选择更大的参数量模型。

https://gitee.com/mindspore/mindformers/issues/I7U5JV
这个issue类型为用户提出需求，主要对象是权重合并功能。这个问题可能源于用户希望在多线程或小内存设备上进行权重合并，但目前系统可能尚不支持此功能。

https://gitee.com/mindspore/mindformers/issues/I7TQN1
这是一个用户提出需求的issue，主要涉及到需要增加flanT5模型的增量适配，由于游戏npc场景需要支持，估计用卡数在几百卡。

https://gitee.com/mindspore/mindformers/issues/I7TPE0
这是一个用户提出需求的 issue，主要涉及base_model中load_checkpoint方法的功能改进，由于传入参数限制导致子模型权重未正确加载。

https://gitee.com/mindspore/mindformers/issues/I7TKI3
这是一个bug报告，用户提到在华为云上使用MindSpore时，obs频繁上传文件导致hccl超时的问题。

https://gitee.com/mindspore/mindformers/issues/I7TCY9
这是一个bug报告，涉及到auto_class.py中support_list中key存在性判断逻辑的问题，可能由于存在不可到达的语句导致在595行报KeyError。

https://gitee.com/mindspore/mindformers/issues/I7T7UY
这是一个bug报告，涉及主要对象为ModelArts上预训练MAE，由于软件运行时出错导致RuntimeError。

https://gitee.com/mindspore/mindformers/issues/I7T71E
这是一个bug报告类型的issue，主要涉及相对引用和绝对引用的统一问题，由混用这两种引用导致了难以排查的问题。

https://gitee.com/mindspore/mindformers/issues/I7T5V7
这是一个bug报告，涉及GLM2大模型配置文件中缺失了run_glm2_6b_finetune.yaml文件，用户寻求获取该文件的帮助。

https://gitee.com/mindspore/mindformers/issues/I7T0GH
这是一个Bug报告，主要涉及glm/glm2模型在lite ge后端在线推理时出现Stack报错"index_i_desc is null"。导致这个问题的原因是将list改成tuple可以规避此问题。

https://gitee.com/mindspore/mindformers/issues/I7SZPO
这是一个bug报告，该问题单涉及MindSpore中llama模块的文档链接失效、脚本函数部分权重未加载、函数中的permute报错以及分布式增量推理报错等问题。

https://gitee.com/mindspore/mindformers/issues/I7STBY
这是一个Bug报告，主要涉及的对象是Llama推理softmax算子编译错误问题。原因可能是使用了README上的代码，但转换过的ckpt文件导致编译错误。

https://gitee.com/mindspore/mindformers/issues/I7SPF3
这是一个用户提出问题的类型issue，主要涉及的对象是glm模型中的position_ids参数，用户询问该参数的含义如何以及如何正确给定其数值。

https://gitee.com/mindspore/mindformers/issues/I7SNKU
这是一个关于训练模型过程中出现损失为0没有变化的bug报告，主要涉及到LoRa微调训练GLM模型的问题。可能由于模型结构、数据集质量或超参数设置等原因导致损失一直为0，需要进一步分析排查。

https://gitee.com/mindspore/mindformers/issues/I7SJ0Y
这是一个bug报告，主要涉及到在chatglm模型的lora微调训练过程中出现的多个ckpt文件和strategy ckpt文件的问题，以及关于单机多卡模型合一写法和yaml参数含义的疑惑。这可能是由于配置文件中参数设置不当导致的。

https://gitee.com/mindspore/mindformers/issues/I7SEXQ
这是一个关于高阶接口开发问题的issue报告，主要涉及到mindformers开发中的一些适配性和配置方面的问题，导致在使用高阶接口时出现训练和微调模型时的逻辑错误。

https://gitee.com/mindspore/mindformers/issues/I7SAJS
这是一个Bug报告，涉及到Glm模型自制数据集mindrecord保存时遇到的问题。这个问题可能是由于多头注意力机制中的mask和score不能broadcast而导致。

https://gitee.com/mindspore/mindformers/issues/I7SA85
这是一个需求问题，用户在使用ModelArts上预训练MAEBASE模型时遇到需要长时间运行的情况，询问是否还需要额外配置。

https://gitee.com/mindspore/mindformers/issues/I7S7FU
这是一个bug报告，问题涉及到多头注意力机制的mask和score无法broadcast。原因可能是在训练时出现了错误，导致输出的数据格式不匹配。

https://gitee.com/mindspore/mindformers/issues/I7S3A2
这是一个bug报告，主要对象涉及到训练/微调时的use_past开关，在yaml配置中设置为True导致报错，可能是由于训练/微调过程中对use_past参数的感知和判断导致的。

https://gitee.com/mindspore/mindformers/issues/I7S2TD
这是一个bug报告，主要问题是"aicc云道上 only_save_strategy功能存在问题"，由于开启only_save_strategy后通过raise system exit退出导致任务识别失败、obs monitor功能未生效、strategy文件未回传。

https://gitee.com/mindspore/mindformers/issues/I7RP8N
该issue类型为用户询问问题类型，涉及主要对象是LLAMA模型。由于用户想了解LLAMA模型是否能用于翻译任务，所以提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I7R3K1
该issue类型为用户请教问题，主要涉及对象为如何使用sentence Transformers库中的嵌入模型。由于用户不清楚如何操作及实例化模型，导致提出了关于如何获取和使用该模型的问题。

https://gitee.com/mindspore/mindformers/issues/I7R23Z
这是一个bug报告，涉及主要对象是在使用alpaca_data数据集进行训练\微调时，出现了模型训练报错。原因可能是数据集形状大小不匹配导致的报错。

https://gitee.com/mindspore/mindformers/issues/I7QOWQ
这是一个bug报告，主要涉及llama 7b lora使用aicc进行训练时npu使用率突然降低的问题。

https://gitee.com/mindspore/mindformers/issues/I7QN2M
这是一个bug报告，涉及mindformers的llamaba 7b lora推理出现问题，原因是Single op compile failed导致推理代码无法成功运行。

https://gitee.com/mindspore/mindformers/issues/I7QGZW
这是一个bug报告，涉及到文件缺失导致训练任务中断。

https://gitee.com/mindspore/mindformers/issues/I7QDEB
这是一个bug报告，主要涉及到仓库中的文档链接失效问题，可能由于链接本身修改或失效导致无法访问文档。

https://gitee.com/mindspore/mindformers/issues/I7QDEC
这是一个bug报告，该问题涉及chatglm6b mindformers全参数微调，导致报错。

https://gitee.com/mindspore/mindformers/issues/I7QCAE
这是一个bug报告，主要涉及LoRA低参微调过程中出现的错误。这可能是由于参数设置问题或操作错误导致的。

https://gitee.com/mindspore/mindformers/issues/I7QC2S
这是一个bug报告类型的issue，主要涉及GLM文档上的代码，可能是由于代码的错误导致用户提出疑问。

https://gitee.com/mindspore/mindformers/issues/I7Q848
这是一个bug报告，涉及的主要对象是LLaMA tokenizer，由于pad token id不一致导致在推理时出现问题。

https://gitee.com/mindspore/mindformers/issues/I7Q6QN
该issue类型为bug报告，主要涉及base trainer默认配置导致错误判断，由于误写model_name导致走的是GPT模型的默认分支，用户误以为跑通。

https://gitee.com/mindspore/mindformers/issues/I7Q50Y
这是一个bug报告，涉及到gpt2模型微调过程中出现的训练数据错误问题。

https://gitee.com/mindspore/mindformers/issues/I7Q4TN
这个issue类型是用户问题，主要对象是如何在Pipline推理时加载自己的模型权重，可能由于缺少相关文档或指导而导致用户提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I7Q1GE
这是一个bug报告，主要涉及训练平台Ascend 910和mindspore 1.10，用户反馈日志显示模型参数未加载后又加载完成，询问预训练权重是否成功加载。

https://gitee.com/mindspore/mindformers/issues/I7PUQ8
这是一个bug报告类型的issue，主要涉及mindformers中SWIN模型训练过程中出现的错误。可能是由于环境配置、代码逻辑或参数设置错误导致的问题。

https://gitee.com/mindspore/mindformers/issues/I7PUN4
这个issue是关于版本管理的需求问题，涉及主要对象为mindformers的ms版本管理，由于版本散落在各个代码中，导致难以维护。

https://gitee.com/mindspore/mindformers/issues/I7PR5R
这个issue是一个bug报告，主要对象是Mindarmour优化器，由于某些原因导致了the model stream execute failed错误。

https://gitee.com/mindspore/mindformers/issues/I7PIID
这个issue是用户提出的需求类型，主要涉及到在模型文件的construct中打印信息的问题。由于直接print无法打印信息，用户试图添加打印代码进行调试。

https://gitee.com/mindspore/mindformers/issues/I7PHM9
这是一个bug报告，针对Mindformers VIT单机八卡（notebook）训练报错的问题。原因可能是环境配置或代码实现问题引起的报错。

https://gitee.com/mindspore/mindformers/issues/I7P0YN
这个issue属于bug报告类型，主要涉及mindformers在modelarts上使用8卡微调ChatGLM时遇到报错问题，可能是由于代码中的某些错误导致了训练过程中出现异常。

https://gitee.com/mindspore/mindformers/issues/I7OVVI
这是一个用户需求提出类的issue，主要涉及的对象是MindFormers团队，由于需要增加对StarCoder模型的支持而提出。

https://gitee.com/mindspore/mindformers/issues/I7ONL2
该issue类型为用户提出需求，关注于昇腾910上自己创建数据集做Lora微调训练所需数据量和训练轮数。

https://gitee.com/mindspore/mindformers/issues/I7ONA3
这是一个bug报告，主要涉及T5模型推理速度过慢的问题，可能是由于ms框架推理机制导致的。

https://gitee.com/mindspore/mindformers/issues/I7OMTY
这是一个用户提出需求的issue，主要涉及到Pipeline对外API的功能不完善。由于版权问题，用户无法将第三方模型上传至obs，希望能够支持对本地repo中的权重和模型配置进行加载和推理。

https://gitee.com/mindspore/mindformers/issues/I7OLJE
这是一个建议修改类型的issue，涉及主要对象为config文件中的batch size参数。原因是config中存在多个batch size可能导致使用时的混淆。

https://gitee.com/mindspore/mindformers/issues/I7O9BH
这个issue是一个bug报告，主要问题是callback存在bug，导致epoch_upload_frequence=1和sinksize相关，以及epoch end通过step_num判断。

https://gitee.com/mindspore/mindformers/issues/I7O8ZP
该issue属于用户提出需求类型，主要涉及并行推理需求，由于需要支持来自不同线程的输入并并发返回结果给相应线程，用户提出该需求。

https://gitee.com/mindspore/mindformers/issues/I7O217
这是一个bug报告，涉及到Mindformers中Llama模型文档中的链接失效问题，导致无法下载分词模型文件。

https://gitee.com/mindspore/mindformers/issues/I7NW4N
这是一个bug报告，涉及到mindformers库中的LlamaForCausalLM模型和LlamaTokenizer模型的使用问题。由于模型加载和调用过程中出现了错误，导致了模型无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I7NKCH
该issue属于用户提出需求类型，主要对象是Mindformers项目。由于用户希望Mindformers可以支持cv图像重建大模型，可能是由于当前该功能在项目中尚未被支持所致。

https://gitee.com/mindspore/mindformers/issues/I7NE9L
这是一个bug报告，涉及类GPT模型的loss计算问题。由于input_mask在计算loss时出现了逻辑错误，导致和huggingface对比结果时出现差异。

https://gitee.com/mindspore/mindformers/issues/I7MX2Q
这是一个bug报告，该问题涉及的主要对象是训练llama13B模型，由于算子初始化错误导致了训练过程中的异常。

https://gitee.com/mindspore/mindformers/issues/I7MRVN
这是一个bug报告类型的issue，主要涉及GLM6b模型微调不起效果的问题，可能是因为loss不下降导致微调过和未微调过输出结果相同。

https://gitee.com/mindspore/mindformers/issues/I7MRDD
这个issue是一个需求提出类型，主要涉及到缺少RLHF例子的问题。由于缺少相关例子，导致用户可能无法解决reward model两个模型的加载问题。

https://gitee.com/mindspore/mindformers/issues/I7MIDT
这是一个用户提出问题的类型，主要涉及LLAMA模型在单机8卡训练过程中保存计算图的问题，可能由于参数设置或并行训练限制导致保存的图未能正确生成或保存在期望路径。

https://gitee.com/mindspore/mindformers/issues/I7MGX5
这是一个bug报告，主要涉及到在单机单卡微调时出现了报错No module named 'tk'且无法root安装。可能是由于缺少tk模块导致的问题。

https://gitee.com/mindspore/mindformers/issues/I7MEB0
这是一个bug报告类型的issue，主要涉及到aicc在大规模集群断点续训时对load checkpoint功能的不友好性。由于下载不是顺序的，加载到的权重不是最新的，导致可能加载了不同训练轮次的ckpt。

https://gitee.com/mindspore/mindformers/issues/I7M65F
这是一个bug报告，涉及的主要对象是MindSpore的ParallelTransformerConfig类。由于初始化时修改了enable_parallel_optimizer的默认配置为False，导致在实例化config过程中直接将False作为属性设置，进而引发报错的问题。

https://gitee.com/mindspore/mindformers/issues/I7LZL2
这是一个bug报告，主要涉及的对象是执行bloom7.1b教程示例代码时遇到的数值计算错误。造成这种情况的原因可能是输入张量的形状不匹配而引发的数值计算错误。

https://gitee.com/mindspore/mindformers/issues/I7LXQQ
这个issue类型是bug报告，涉及对象是fastapi调用模型时显存占用问题。可能由于调用修改后的chatglm导致显存无法释放，导致无法使用npu。

https://gitee.com/mindspore/mindformers/issues/I7LQ0H
该issue类型是用户请教问题，涉及主要对象为分布式训练参数设置。由于用户对模型并行训练中模型切分方式和参数设置理解有疑问，需更清楚了解在单机八卡环境下模型与卡之间的分配关系。

https://gitee.com/mindspore/mindformers/issues/I7LLWD
这是一个用户提出需求的issue，主要涉及数据集的input_columns配置问题，由于按照顺序而非key值匹配导致了模型传参的错误。

https://gitee.com/mindspore/mindformers/issues/I7LLD8
这是一个bug报告，涉及MindSpore在推理过程中使用盘古Alpha模型在不同batch size下出现RuntimeError的问题。

https://gitee.com/mindspore/mindformers/issues/I7LE76
这是一个bug报告类型的issue，主要涉及mindformers跑glm过程中出现的报错。由于使用了即将被弃用的'mindspore.ms_function'和'TensorAdd'，导致出现了特定的报错信息，需要更新为'mindspore.jit'和'Add'来解决问题。

https://gitee.com/mindspore/mindformers/issues/I7LBDN
这是一个bug报告，主要涉及gpt2微调lora在32G V100机器上显存不足的问题。

https://gitee.com/mindspore/mindformers/issues/I7L8VZ
这是一个bug报告，该问题涉及MindSpore在Llama 7b执行加载分布式断点权重续训时出现RuntimeError的情况。原因可能是无法找到pipeline的结束节点。

https://gitee.com/mindspore/mindformers/issues/I7L8LL
这个issue类型是用户提出需求，主要涉及到增加训练时间的预估打印，以及一次迭代中训练和评估时间的打印。用户提出这个需求是为了方便了解整个训练过程中的时间分配情况。

https://gitee.com/mindspore/mindformers/issues/I7L6EO
这是一个bug报告，涉及MindSpore Serving部署LLama7b分布式推理服务失败的问题。由于硬件环境为Ascend910A，软件环境为MindSpore2.0.0和MindSpore Serving 2.0.0，Python 3.9.13，导致部署分布式推理服务时出现错误。

https://gitee.com/mindspore/mindformers/issues/I7L58S
这是一个用户提出问题的类型，主要关注mindsporegpt2微调过程中数据格式的理解。可能由于用户对于数据格式理解存在歧义或不清楚，导致提问这个问题。

https://gitee.com/mindspore/mindformers/issues/I7L3EY
这个issue类型是bug报告，涉及的主要对象是mindformers库的导入操作。由于mindpet版本号为1.0.0时导入mindformers失败，出现了AttributeError: module 'mindspore._checkparam' has no attribute 'INC_LEFT'的错误。

https://gitee.com/mindspore/mindformers/issues/I7KZLN
这是一个用户提出需求的issue，涉及的主要对象是ChatGLM26B模型，用户询问该模型是否支持。

https://gitee.com/mindspore/mindformers/issues/I7KY3G
这是一个bug报告，主要涉及的对象是mindformers中的SyncStream功能；这个问题可能是由于CUDA内存访问非法导致的内存同步失败。

https://gitee.com/mindspore/mindformers/issues/I7KY3A
这是一个用户提出需求的issue，主要涉及GPT2教程中缺少eval_dataset的制作方法。这个问题之所以出现，是因为无法复现对应的效果截图。

https://gitee.com/mindspore/mindformers/issues/I7KT9B
这是一个用户提出需求的issue，主要涉及增量推理支持多batch的功能。可能由于当前模型和流程都不支持增量推理多batch，用户希望在两方面都得到支持。

https://gitee.com/mindspore/mindformers/issues/I7KLJL
这是一个bug报告，主要涉及Mindformers在训练时出现了Ascend collective communication初始化失败的错误。

https://gitee.com/mindspore/mindformers/issues/I7KJPB
这是一个用户提出需求的issue，主要涉及的对象是增加cosinelr最后为水平的学习率功能。原因可能是用户希望在模型训练中设置一种类似cosine学习率退火的功能，使得学习率在训练后期保持稳定水平。

https://gitee.com/mindspore/mindformers/issues/I7KIPU
这是一个bug报告类型的issue，主要涉及配置文件中的save_graph_path参数设置不当，导致在modelarts环境下存图内容无法回传的问题。

https://gitee.com/mindspore/mindformers/issues/I7K92U
这是一个bug报告类型的issue，主要涉及chatglm6b在使用ms2.0进行lora微调时生成的策略切分文件异常，可能由于版本兼容性或参数设置的问题导致无法正常合并。

https://gitee.com/mindspore/mindformers/issues/I7K5SM
这是一个bug报告，涉及到mindformers库中的API调用相关问题，由于数据类型不匹配导致了TypeError异常。

https://gitee.com/mindspore/mindformers/issues/I7K3B4
这是一个用户提出的问题，主要涉及mindformers中BertModel的使用问题，导致无法对齐mindformers和transformers的模型结构和数据形状。

https://gitee.com/mindspore/mindformers/issues/I7K2BR
这是一个用户提出需求的类型issue，主要涉及bloom7.1b版本在ascend910单机8卡上进行微调后如何合并权重文件的问题。该问题是由于用户需要合并微调后的权重文件且指导文档未提供相关步骤或代码而导致的。

https://gitee.com/mindspore/mindformers/issues/I7K0T8
这是一个需求类型的issue，主要涉及到CI测试用例的补齐及其在门禁中的监控，用户提出希望完善边训练边评估功能的需求。

https://gitee.com/mindspore/mindformers/issues/I7K0HF
这是一个bug报告issue，主要涉及LLaMA并行推理结果较差的问题。原因是硬件环境为Ascend910A，软件环境为MindSpore 2.0.0，Python版本为3.9.13。

https://gitee.com/mindspore/mindformers/issues/I7K096
这是一个功能优化建议，主要涉及AICC拉取切分权重文件或超大数据集时的易用性问题。原因是当前界面功能拉取权重只能整量拉取，导致在特定场景下造成大量时间和空间浪费。

https://gitee.com/mindspore/mindformers/issues/I7JY86
这个issue是一个bug报告，主要涉及的对象是采用教程示例代码对预训练的llama权重进行评估时出现的shape问题。由于传入的参数或数据格式不符合期望，导致了mindspore在评估过程中出现异常。

https://gitee.com/mindspore/mindformers/issues/I7JUR3
该issue属于需求提出类型，主要涉及日志结构及内容的优化；用户要求增加输出内容、调整输出顺序、增加性能信息呈现、配置化控制日志输出、增加日志说明文档等，旨在改善日志的易用性。

https://gitee.com/mindspore/mindformers/issues/I7JJMM
这个issue类型为用户提问，涉及对象是mindformer模块。由于requirements.txt未列明依赖和Python版本要求，用户提出是否需要依赖mindspore以及Python版本的问题。

https://gitee.com/mindspore/mindformers/issues/I7JH9T
这是一个bug报告，涉及的主要对象是ChatGLM6B模型的Lora微调结果。由于微调结果差异与不微调结果相似，可能由于微调过程中的某些错误或参数配置不当导致。

https://gitee.com/mindspore/mindformers/issues/I7JF66
这是一个bug报告，主要涉及对象是llama单卡推理速度过慢，可能由于算法优化问题导致。

https://gitee.com/mindspore/mindformers/issues/I7J4WB
这是一个bug报告，主要涉及紫东太初大模型推理下游任务零样本跨模态检索报错，可能由于模型训练过程中出现错误导致。

https://gitee.com/mindspore/mindformers/issues/I7J302
这是一个bug报告，主要涉及mindformers在特定系统环境下调用报错的问题，可能由于版本兼容性问题导致无法成功导入mindformers模块。

https://gitee.com/mindspore/mindformers/issues/I7J28K
这是一个bug报告，主要涉及mindspore的版本问题，导致出现了'AttributeError: 'mindspore._c_expression.ParamInfo' object has no attribute 'cloned_obj''错误。

https://gitee.com/mindspore/mindformers/issues/I7IOHF
这是一个bug报告，用户在执行gpt2的权重转换脚本时遇到了报错。

https://gitee.com/mindspore/mindformers/issues/I7IO06
该问题单属于用户提出需求类型，涉及主要对象为mindformer支持的推理卡，用户询问是否支持310的卡。由于用户需要确定mindformer是否支持特定型号的推理卡，因此提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I7IGYX
这是一个bug报告，涉及主要对象是mindformers下的llama_7b模型，在训练过程中出现loss值为负数的问题，可能由于模型选择或数据准备等原因导致。

https://gitee.com/mindspore/mindformers/issues/I7IASZ
这是一个bug报告，涉及对象为llama 7B模型的推理过程。推测由于加载微调后的ckpt导致推理速度缓慢，需要找出原因加以解决。

https://gitee.com/mindspore/mindformers/issues/I7IAPV
这个issue类型是bug报告，主要涉及Trainer.predict接口加载lora微调的ckpt造成回答有很多重复句子的问题。

https://gitee.com/mindspore/mindformers/issues/I7I5NU
这是一个用户提出需求的类型的issue，主要涉及到mindformers库中的temperature设置问题。用户询问是否有类似huggingface中temperature参数的设置选项。

https://gitee.com/mindspore/mindformers/issues/I7HSEP
这是一个bug报告，主要涉及到在使用MFPipelineWithLossScaleCell并行流水线时出现的类型错误导致的报错。

https://gitee.com/mindspore/mindformers/issues/I7HRU9
这是一个bug报告，涉及主要对象为llama 13B模型的NPU资源调用问题。这个问题可能是由于软件或配置错误导致的NPU资源占用为0%，使得训练作业无法正常调用NPU资源。

https://gitee.com/mindspore/mindformers/issues/I7H5R4
这是一个用户提出需求的issue，主要涉及logger记录日志时能否将文件和代码行数记录下来，快速定位的功能。由于用户希望能快速定位日志记录的文件和代码行数，因此提出了这个需求。

https://gitee.com/mindspore/mindformers/issues/I7GNXV
这个issue类型是bug报告，该问题涉及图片显示问题，由于图片无法显示导致用户提出了该问题请求获取帮助。

https://gitee.com/mindspore/mindformers/issues/I7GG54
这个issue属于需求类型，主要涉及到类GPT模型下游任务评测体系方案的设计和开发。由于缺乏完整的下游任务评测体系，导致模型迁移验证和对外展示受到影响。

https://gitee.com/mindspore/mindformers/issues/I7G7DO
这是一个bug报告，主要涉及到mindformers项目中的多卡训练过程中出现的AttributeError。由于缺少了is_first_iteration属性，导致了训练过程中的报错。

https://gitee.com/mindspore/mindformers/issues/I7FMCL
这是一个bug报告，涉及主要对象是进行数据预处理时出现的模块导入错误，导致无法使用特定模块进行处理数据。

https://gitee.com/mindspore/mindformers/issues/I7EWEM
这是一个Bug报告，主要对象是在运行bloom7.1B时遇到了运行报算子错误的问题，可能是由于CumSum算子初始化失败导致的。

https://gitee.com/mindspore/mindformers/issues/I7DSG1
这是一个bug报告类型的issue，主要涉及Atlas 300I Model 3000进行模型推理报错问题。导致此问题的原因可能是模型推理过程中出现了错误。

https://gitee.com/mindspore/mindformers/issues/I7DOGH
此issue为bug报告，涉及T5base模型转换后推理结果错误，可能是由于模型转换后出现了编码器输出错误导致。

https://gitee.com/mindspore/mindformers/issues/I7DNGS
这是一个bug报告，主要涉及mindformers第三方依赖库版本支持问题，导致mindformers不支持sentencepiece==0.1.91版本。

https://gitee.com/mindspore/mindformers/issues/I7DCE9
这是一个用户提出需求的issue，主要涉及MindFormers套件的易用性问题。由于缺乏相关功能或文档支持，用户提出了合并权重、修改文档、增加分布式推理能力、文档说明等改进建议。

https://gitee.com/mindspore/mindformers/issues/I7DA2F
这是一个bug报告类型的issue，主要涉及MindForgers套件上GPT代码在开启流水线并行时加载分布式权重续训和评测报错。原因可能是配置参数设置或代码逻辑问题导致的报错。

https://gitee.com/mindspore/mindformers/issues/I7CVRG
这个issue类型为bug报告，主要涉及的对象是llama模型训练，可能是由于训练12个迭代后出现报错，导致训练无法正常进行或完成。

https://gitee.com/mindspore/mindformers/issues/I7CVRC
这是一个用户提出需求的issue，主要对象是Bart这个模型。用户希望尽快更新该模型的转换，并因为当前模型可能存在问题或过时导致功能不符合预期。

https://gitee.com/mindspore/mindformers/issues/I7CO22
这是一个关于bug的报告，主要对象是trainer模块，由于即使设置了resume_checkpoint=False，trainer仍然自动加载权重，导致了这个问题。

https://gitee.com/mindspore/mindformers/issues/I7CFG6
这个issue类型是特性需求提议，涉及对象是Trainer模块的功能接口。由于MindPet功能需要新增高阶接口方法，用户提出了增加Trainer.finetune接口以适配该功能。

https://gitee.com/mindspore/mindformers/issues/I7AB2Q
这是一个bug报告，主要对象是MindSpore自动加载训练得到的最后一个权重失败。造成这一问题的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/I7A3RF
这是一个bug报告，主要涉及GPT2在数据读取和预测时间方面出现问题，可能是由于数据类型错误导致。

https://gitee.com/mindspore/mindformers/issues/I79WJG
这是一个bug报告，涉及的主要对象是在t4 gpu上运行mindformers 0.3.0时出现的OSError: [Errno 9] Bad file descriptor错误。

https://gitee.com/mindspore/mindformers/issues/I79WAI
该issue属于用户提出需求类型，涉及对象为关于新增模型的实现机制。由于模型权重转换、权重下载与转换链接问题以及多模态任务代码耦合的困难，开发者寻求关于Mindformers的回复与解答。

https://gitee.com/mindspore/mindformers/issues/I79UW0
这是一个升级适配问题，主要涉及mindspore 2.0版本适配的问题。可能是因为mindformers需要进行mindspore 2.0的适配，但仍需要兼容1.10版本，因此提出这个issue。

https://gitee.com/mindspore/mindformers/issues/I79RWC
这是一个bug报告，主要涉及的对象是modelarts训练gpt13b模型，在ms1.9.0版本中出现了多个问题，包括数据集路径设置、eval dataset逻辑冲突、optimizer_shard报错等。

https://gitee.com/mindspore/mindformers/issues/I79RHM
这是一个bug报告，主要涉及Mindformer代码中的optimizer模块，由于FP32StateAdamWeightDecay类缺少"global_step"属性，导致与断点续训功能冲突。

https://gitee.com/mindspore/mindformers/issues/I79EVI
这是一个关于bug报告的issue，主要涉及武大Luojia团队的权重转换脚本，可能是由于权重转换脚本的问题导致转换后的shape不匹配。

https://gitee.com/mindspore/mindformers/issues/I79ESP
这是一个用户需求类型的issue，主要涉及到武大LuojiaNet团队的交流和答疑遗留问题，涉及对MAE模型图像重构结果可视化、冻结ViT的backbone部分权重、提供UperNet模型作为像素级任务参考以及推理输入数据集标签映射等内容。

https://gitee.com/mindspore/mindformers/issues/I79C01
这是一个bug报告，主要涉及text generator模块的在线增量推理功能。导致这个问题的原因是部分流程未能正确支持增量推理。

https://gitee.com/mindspore/mindformers/issues/I79BNR
该issue是一个用户提出需求的类型，主要涉及MindBook查询类，由于分表过多导致新增模型需要多处修改子表，降低易用性。

https://gitee.com/mindspore/mindformers/issues/I79BLF
这是一个bug报告，主要涉及大模型的预训练权重加载时不支持自动切片加载，导致host侧内存OOM的问题。

https://gitee.com/mindspore/mindformers/issues/I796JB
这是一个bug报告，涉及主要对象为Predict模块，由于不支持分布式推理导致单卡在线推理时内存不够，无法完成推理。

https://gitee.com/mindspore/mindformers/issues/I796EM
这是一个bug报告类型的issue，主要涉及到gpt213b并行场景优化器切分使用FusedAdamWeightDecay保存ckpt时报错。问题可能由于模型优化器配置不当导致。

https://gitee.com/mindspore/mindformers/issues/I78U4R
这是一个bug报告，主要涉及文本生成解码时无法支持输入字符形式的eod，由于tokenizer model中的结束符是字符表示而非token id，导致解码过程繁琐。

https://gitee.com/mindspore/mindformers/issues/I78TLG
这是一个bug报告类型的issue，主要涉及并行模式切换为数据并行后full_batch不能为True的问题。原因可能是程序在切换并行模式后未正确处理相关逻辑导致报错。

https://gitee.com/mindspore/mindformers/issues/I78RSC
这是一个bug报告类型的issue，主要涉及MindFormerBook索引方式整改，由于索引内容过于复杂和校验条件不合理，导致维护困难和错误报告。

https://gitee.com/mindspore/mindformers/issues/I78P7S
这是一个bug报告，涉及trainer中修改resume_or_finetune_checkpoint时写错成了resume_or_finetune_from_checkpoint。这可能是由于拼写错误导致的代码调用错误。

https://gitee.com/mindspore/mindformers/issues/I78JC7
这个issue是一个bug报告，涉及的主要对象是使用Trainer接口进行推理时出现了TBE Single op compile failed的错误。

https://gitee.com/mindspore/mindformers/issues/I78G80
这是一个bug报告，主要对象是output_dir不支持自定义修改，可能由于未知原因导致无法进行所需的自定义修改。

https://gitee.com/mindspore/mindformers/issues/I77NWV
这是一个Bug报告，涉及的主要对象是最新代码中的gpt2_13b多卡数据并行+优化器切分，导致内存占用大幅增加，最终导致训练报错。

https://gitee.com/mindspore/mindformers/issues/I77N9Z
这是一个Bug报告类型的issue，主要涉及MindSpore 2.1版本下MindFormer缺少loss package的问题。由于MindFormer在2.1版本不支持没有出包，导致此问题的产生。

https://gitee.com/mindspore/mindformers/issues/I771QR
这是一个bug报告，主要涉及CheckpointMonitor的self.rank_id设置错误导致保存节点0的checkpoint问题。

https://gitee.com/mindspore/mindformers/issues/I76TL7
这是一个bug报告，主要问题是断点续训（如训练10epochs中断）失败。可能由于软件环境中的某些因素导致这种症状。

https://gitee.com/mindspore/mindformers/issues/I76SXM
这是一个用户提出需求的issue，主要涉及的对象是希望能提供类似modelzoo仓库中的export.py脚本，将.ckpt文件导出为.onnx，用于后续推理。

https://gitee.com/mindspore/mindformers/issues/I76S6O
这是一个需求类型的issue，主要涉及如何将训练好的GPT2模型应用到推理应用开发中，用户希望了解如何生成.air或者.onnx文件来部署接口。

https://gitee.com/mindspore/mindformers/issues/I76OW5
这是一个用户提出需求的issue，主要对象是配置文件内容的修改，由于需要频繁修改配置文件来调试参数，希望支持命令行修改配置文件内容以方便调试。

https://gitee.com/mindspore/mindformers/issues/I73U52
这是一个bug报告，涉及的主要对象是在modelarts环境中运行多机多卡的GPT2模型训练失败。由于配置并行模式出现错误导致训练在第一个epoch的step 23/2721时发生错误。

https://gitee.com/mindspore/mindformers/issues/I731RG
这是一个bug报告，主要对象是MAE模型流水线并行在MindSpore环境下报错。由于在使用流水线并行时出现报错，可能是代码中对梯度累积的处理方式不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I72ZQO
这个issue类型为需求提出，涉及的主要对象是MindFormers中的PR评审流程。由于需要规范PR评审流程，因此提出了相关的评审要求和人员一览表。

https://gitee.com/mindspore/mindformers/issues/I72ZN3
这个issue类型是用户提出需求，主要涉及MindFormers关键模型的大规模分布式能力，由于用户需要了解核心关键模型的分布式并行能力一览表。

https://gitee.com/mindspore/mindformers/issues/I72UAS
这是一个bug报告，主要涉及到使用CausalLanguageModelDataset生成数据时，由于不支持fullbatch为false导致实际batchsize偏大，与预期不符。

https://gitee.com/mindspore/mindformers/issues/I72RPA
这是一个bug报告类型的issue，主要涉及到`bert.py`文件中存在冗余的代码。原因是在判断是否训练状态时，重复执行了部分代码。

https://gitee.com/mindspore/mindformers/issues/I72O1W
这是用户提出的功能需求，主要对象是增加边训练边评估特性。由于用户希望在训练过程中执行评估脚本实时查看模型精度表现，所以提出了这个需求。

https://gitee.com/mindspore/mindformers/issues/I72KSC
这是一个bug报告，涉及的主要对象是日志打印中设置了不同的perbatchsize，导致打印行为不一致，用户不知道看哪个值。

https://gitee.com/mindspore/mindformers/issues/I7268V
这是一个用户提出需求的类型issue，主要涉及需求完善自定义开发指南，因为缺乏指导导致用户不清楚如何开发自定义模型。

https://gitee.com/mindspore/mindformers/issues/I71Q0D
这是一个bug报告，主要涉及对象为lr_scheduler，由于未对total_steps参数的存在进行判断，在使用ConstantWarmUpLR时出现了初始化参数报错。

https://gitee.com/mindspore/mindformers/issues/I71I8A
这是一个bug报告，主要涉及Trainer高阶接口在模型实例传入时，并行配置不生效的问题。

https://gitee.com/mindspore/mindformers/issues/I71HHY
该issue类型为用户提问，涉及主要对象为如何微调模型，用户想了解如何微调特定模型（例如gpt2）的具体步骤。

https://gitee.com/mindspore/mindformers/issues/I71FOV
这是一个bug报告，涉及到gpt2在ms 2.0beta分布式并行模式下训练时的问题，报错信息显示在半自动并行模式下开启模型并行会出错。

https://gitee.com/mindspore/mindformers/issues/I70VYT
这是一个bug报告，该问题涉及MindFormers版本号与实际版本不匹配的情况，导致了版本匹配关系错误。

https://gitee.com/mindspore/mindformers/issues/I6ZZ9R
这是一个bug报告，用户反馈GPT2模型在本地推理非常慢，几分钟都没有结果。

https://gitee.com/mindspore/mindformers/issues/I6ZZ9C
这是一个用户提出需求的问题，主要涉及盘古大模型的API调用问题。由于用户对于盘古大模型的API调用有需求，因此提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I6YSPR
这是一个bug报告，涉及字节现场适配问题。原因可能是当前代码逻辑不符合预期，导致页面在适配时出现了问题。

https://gitee.com/mindspore/mindformers/issues/I6YH0T
这是一个用户提出需求的issue，主要涉及的对象是在ModelArts的训练任务中支持多级多卡训练。由于目前该模型未提供对应的ModelArts的训练任务中支持多级多卡训练gpt2的支持，用户希望工程师能提供相应支持以便于使用人工智能计算中心的算力资源进行多机多卡训练。

https://gitee.com/mindspore/mindformers/issues/I6Y2HV
这个issue是关于bug报告，在ModelArts上云训练过程中存在eval数据集泄露问题导致数据集混淆，影响模型训练结果。

https://gitee.com/mindspore/mindformers/issues/I6X1SJ
这是一个bug报告，涉及的主要对象是FP32StateAdamWeightDecay接口。由于代码中存在错误，导致接口报错。

https://gitee.com/mindspore/mindformers/issues/I6X0OD
这是一个bug报告类型的issue，涉及的主要对象是tqdm未指定版本，导致下载部分报错并提示数值错误。

https://gitee.com/mindspore/mindformers/issues/I6X0KT
这个issue类型是bug报告，主要涉及到mindformers安装成功后无法使用common库。由于可能存在导入路径或配置参数错误，导致无法正确调用common库中的功能。

https://gitee.com/mindspore/mindformers/issues/I6WPFN
这是一个Bug报告类型的Issue，涉及的主要对象是SQuADDataLoader。由于缺少必要的参数'tokenizer'，导致出现TypeError，这是引起问题的原因。

https://gitee.com/mindspore/mindformers/issues/I6WOUW
这是一个bug报告，涉及到mindformers库中build_dataloader.py模块的导入失败问题。原因可能是模块导入路径或代码中的错误。

https://gitee.com/mindspore/mindformers/issues/I6WO68
这是一个bug报告，涉及的主要对象是mindspore框架中的优化器模块。原因是接口案例代码存在错误，导致无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I6WNW9
这是一个bug报告，主要涉及build_optim接口运行失败，可能由于接口实现问题导致。

https://gitee.com/mindspore/mindformers/issues/I6WL8R
这是一个用户提出需求的类型issue，主要涉及模型复现以及需要获得相关技术和路线上的帮助。

https://gitee.com/mindspore/mindformers/issues/I6WKKN
这是一个bug报告，涉及的主要对象是`build_callback`接口在GPU模式下报错，原因可能是接口不支持GPU。

https://gitee.com/mindspore/mindformers/issues/I6WBGZ
这是一个用户提出需求的issue，主要涉及的对象是增加T5 Pegasus模型和中文文本摘要的下游任务，用户要求在项目中增加这些内容。

https://gitee.com/mindspore/mindformers/issues/I6UZ75
这是一个bug报告，涉及的主要对象是Dataset API Examples。由于缺少环境信息和重现步骤，导致无法使用问题。

https://gitee.com/mindspore/mindformers/issues/I6UUOA
这是一个bug报告，问题涉及的主要对象是build_dataset接口。由于代码中config参数传递错误路径导致报错，需要修复该问题。

https://gitee.com/mindspore/mindformers/issues/I6UUKZ
这是一个bug报告类型的issue，涉及到接口异常报错。原因是参数parallel_config.model_parallel不存在导致的问题。

https://gitee.com/mindspore/mindformers/issues/I6UNG5
这是一个bug报告，涉及Swin模型中ape配置项不匹配的问题，由于模型内与config定义不一致导致该问题产生。

https://gitee.com/mindspore/mindformers/issues/I6UFCU
这个issue属于用户提出需求类型，主要对象是Trainer高阶接口，用户希望该接口支持各类分布式并行能力调用。

https://gitee.com/mindspore/mindformers/issues/I6UEXR
这是一个bug报告，该问题单涉及的主要对象是功能模块check_dataset_config。由于check_dataset_config接口中进行了config的赋值和修改，导致开发者在不调用该接口时会报错，因此需要避免在此处进行配置的赋值和修改，仅单纯进行检查。

https://gitee.com/mindspore/mindformers/issues/I6TMWP
这是一个bug报告，涉及到在使用ModelArts跑微调过程中报OBS错误的问题。由于可能是由于代码中对OBS操作的错误引起的，导致了报错信息中提到的问题。

https://gitee.com/mindspore/mindformers/issues/I6TGMO
该issue类型为bug报告，涉及主要对象为config中的checkpoint_name_or_path和resume_or_finetune_checkpoint参数。由于这两个参数容易混淆且逻辑问题导致run_mindformers.py中出现异常行为，需要统一参数、修复逻辑以简化用户使用。

https://gitee.com/mindspore/mindformers/issues/I6TGMA
这是一个bug报告类型的issue，涉及主要对象为版本判断逻辑。由于版本判断逻辑问题，导致测试用例无法通过并出现代码逻辑错误。

https://gitee.com/mindspore/mindformers/issues/I6SYHJ
这个issue属于bug报告类型，主要涉及的对象是昇腾大模型，由于sink_size默认为1导致训练时loss出来太慢，影响用户体验。

https://gitee.com/mindspore/mindformers/issues/I6S30W
这是一个用户提出的功能需求，主要对象是数据集的generator接口。由于部分API注释和测试用例中有generator接口，用户建议将其抽取出来成一个独立的接口。

https://gitee.com/mindspore/mindformers/issues/I6RRND
这是一个bug报告，涉及的主要对象是Mindformers项目中的trainer.py和image_classification.py文件。由于代码中使用了默认数据集路径，而当前路径下缺少ImageNet数据集，导致无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I6RRM9
这是一个bug报告，问题涉及到mindformers项目中的question_answering.py、text_classification.py和token_classification.py文件，由于使用了已隐藏的build_lr接口，导致了无法正常运行的症状。

https://gitee.com/mindspore/mindformers/issues/I6RRKZ
这个issue属于bug报告类型，主要涉及masked_language_modeling_pretrain.py用例无法正常运行，导致没有创建数据集实例，可能的原因是代码逻辑错误或配置问题。

https://gitee.com/mindspore/mindformers/issues/I6RRKU
这是一个bug报告，涉及主要对象是"MindForMers"项目中的"masked_image_modeling_pretrain.py"脚本。由于代码对"ViTMAEForPreTraioning"对象的调用出错，导致程序报错并无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I6RRKK
该问题是关于bug报告，涉及的主要对象是ViTForMaskedImageModeling模型运行报错，可能由于config.num_channels参数为None而导致报错。

https://gitee.com/mindspore/mindformers/issues/I6RREH
这个issue属于bug报告类型，主要涉及mindformers底下的bert.py代码文件。报告称运行时报input_ids没有所指定的键，预期结果是能正常运行。原因可能是代码中未正确处理输入键。

https://gitee.com/mindspore/mindformers/issues/I6RQM3
这是一个bug报告，涉及T5模型配置项与实际入参名不一致的问题，导致部分配置项无效。

https://gitee.com/mindspore/mindformers/issues/I6RESU
这个issue是一个bug报告，涉及到mindformers项目中的general_task_trainer，在用例运行过程中失败。造成这个问题的原因尚未明确，用户希望能够得到预期结果正常运行。

https://gitee.com/mindspore/mindformers/issues/I6RB7L
该issue是关于bug报告类型，涉及t5脚本启动失败问题，由于没有指定Python 3.7版本导致了 'str' object has no attribute 'decode' 错误。

https://gitee.com/mindspore/mindformers/issues/I6RB4A
这是一个bug报告，涉及MindSpore在运行`run_clip_vit_l_14_pretrain_flickr8k.yaml`时出错。可能由于软件环境和执行模式不匹配导致的。

https://gitee.com/mindspore/mindformers/issues/I6RACB
这是一个bug报告，涉及MindSpore在Ascend硬件环境上运行时报错的问题。可能是由于配置文件或环境设置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I6R94J
这是一个bug报告，涉及运行`run_clip_vit_b_32_pretrain_flickr8k.yaml`时出现报错。由于环境信息显示硬件为Ascend，软件环境为MindSpore版本1.7.0，可能导致了此问题的出现。

https://gitee.com/mindspore/mindformers/issues/I6R8L5
这是一个bug报告，涉及运行"run_clip_vit_b_16_pretrain_flickr8k.yaml"时出现报错，可能导致的原因应当是环境信息不匹配所导致的。

https://gitee.com/mindspore/mindformers/issues/I6R684
这是一个bug报告issue，涉及的主要对象是在运行`run_clip_vit_l_14_pretrain_flickr8k.yaml`时出现了报错。可能是由于环境配置或代码逻辑问题导致的。

https://gitee.com/mindspore/mindformers/issues/I6R65H
这是一个bug报告，用户在运行mindformers中的一个yaml文件时遇到了报错问题。

https://gitee.com/mindspore/mindformers/issues/I6R62G
这是一个bug报告，涉及的主要对象是在运行`run_clip_vit_b_32_pretrain_flickr8k.yaml`脚本时出现报错。由于环境信息中提供的硬件和软件环境不匹配或配置有误，导致了此问题的出现。

https://gitee.com/mindspore/mindformers/issues/I6R43J
这是一个bug报告，涉及的主要对象是运行 run_clip_vit_b_16_pretrain_flickr8k.yaml 文件时出现的错误。导致该问题的原因需要进一步分析。

https://gitee.com/mindspore/mindformers/issues/I6R3YF
这个issue是bug报告，主要涉及到QA微调、txtcls微调和tokcls微调快速任务没有自动加载权重的问题，导致启动报错。

https://gitee.com/mindspore/mindformers/issues/I6R3Q7
这是一个bug报告，涉及Trainer接口开启训练和评估/推理过程中报错，原因是使用了弃用的构建数据集方法导致。

https://gitee.com/mindspore/mindformers/issues/I6R3GV
这个issue类型是bug报告，主要涉及到以API方式启动MAE训练和SWIN训练失败，导致训练启动失败，报错卡被占用，应该是起了多个进程引起的。

https://gitee.com/mindspore/mindformers/issues/I6R250
这是一个bug报告类型的issue，主要涉及到Mindformers项目中关于API方式启动clip评估和推理时出现的数值错误报错问题。产生该问题的原因是配置文件中写死了数据集路径，且readme中未告知用户将数据集存放在指定位置。

https://gitee.com/mindspore/mindformers/issues/I6R1PB
这是一个bug报告，主要涉及MindSpore在运行T5模型时在8p和1p train过程中出现报错的问题。原因可能是硬件环境、软件环境或执行模式等方面的设置不正确。

https://gitee.com/mindspore/mindformers/issues/I6QZEO
这是一个关于软件bug的报告，主要涉及日志输出的epoch和step数不一致的问题，可能是由于数据下沉方式导致的不一致。

https://gitee.com/mindspore/mindformers/issues/I6QUH0
这是一个bug报告issue，主要涉及的对象是ImageClassificationTrainer接口。由于接口异常报错，导致了该问题的发生。

https://gitee.com/mindspore/mindformers/issues/I6QRJY
这是一个bug报告，涉及MindForcers项目中的QuestionAnsweringTrainer API调用问题，由于未正确设置metrics导致出现TypeError报错。

https://gitee.com/mindspore/mindformers/issues/I6QQIE
这是一个bug报告，涉及到run_txtcls_bert_base_uncased_mnli.yaml在train和eval过程中出现的错误。由于环境信息和执行模式不匹配，导致了该问题的发生。

https://gitee.com/mindspore/mindformers/issues/I6QLIE
这是一个用户提出需求的issue，请求向mindformer适配体系中新增适配 xlmrobertbase、xlmrobertalarge、flant5base、flant5large四个模型。

https://gitee.com/mindspore/mindformers/issues/I6Q8RB
这是一个bug报告，涉及的主要对象是在mindformers项目中调用API启动Trainer接口时出现的问题，需要注释掉build_dataset和MindFormerConfig才能正常运行。原因可能是这两个API的使用方式与当前环境或配置不兼容。

https://gitee.com/mindspore/mindformers/issues/I6Q7MV
这是一个bug报告，主要涉及对象是导入MaskLanguageModelingTrainer失败，可能是因为无法从'mindformers'模块中导入'MaskLanguageModelingTrainer'导致的。

https://gitee.com/mindspore/mindformers/issues/I6Q7KP
这是一个bug报告，主要涉及的对象是ImageClassificationTrainer接口无法适配MS=2.0，导致产生了数值错误。

https://gitee.com/mindspore/mindformers/issues/I6PX60
这是一个Bug报告，涉及的主要对象是在使用`clip_vit_l_14_zero_shot_image_classification_cifar100.yaml`文件进行predict时出现的报错。导致该问题的原因可能是环境配置或代码实现方面的问题。

https://gitee.com/mindspore/mindformers/issues/I6PTQM
这是一个bug报告，用户在运行`run_clip_vit_l_14.yaml`文件时遇到了predict报错的问题，可能是由于软件环境中的某些配置不正确导致的。

https://gitee.com/mindspore/mindformers/issues/I6PSJN
这是一个需求提出类的issue，主要涉及对象是代码中的新建和加载操作，由于无法准确判断是否需要加载优化器的权重而导致了问题。

https://gitee.com/mindspore/mindformers/issues/I6POKA
这是一个用户提出配置环境变量相关的问题，主要涉及到环境变量的设置；用户提出了关于环境配置的问题，寻求如何设置特定的环境变量。

https://gitee.com/mindspore/mindformers/issues/I6PMM5
这是用户提出的需求问题，主要涉及分布式启动方式以及相应文档的更新，用户可能希望通过mpirun来统一更新各种分布式启动方式。

https://gitee.com/mindspore/mindformers/issues/I6OFH0
这个issue类型是bug报告，涉及到API文档相关接口优化的问题，由于未暴露需要对外的接口，导致产生了需要对外的接口暴露和类私有函数设置的症状。

https://gitee.com/mindspore/mindformers/issues/I6OFGE
这是一个用户提出需求的类型，主要涉及TrainingArguments类，由于配置入参指令不全导致易用性不佳。

https://gitee.com/mindspore/mindformers/issues/I6OFFU
这是一个bug报告，主要涉及run_mindformer.py脚本支持入参覆盖所有除模型配置外所有指令的需求。由于现有功能无法满足需求，导致了此问题的产生。

https://gitee.com/mindspore/mindformers/issues/I6OFEZ
这是一个需求类型的issue，主要对象是模型的batch size设置。由于目前模型的batch size与网络construct实时获取时未解耦，导致用户无法动态调整模型的batch size，需求解耦以便更灵活地设置batch size。

https://gitee.com/mindspore/mindformers/issues/I6LWMQ
该issue属于用户提出需求，希望添加并行配置的文档指导。

https://gitee.com/mindspore/mindformers/issues/I6L9MF
这是一个bug报告，涉及对象为MindFormers项目中的T5模型训练，在执行文档示例中出现AttributeError报错。

https://gitee.com/mindspore/mindformers/issues/I6KTSE
这是一个用户提出需求的issue，主要对象是支持SOTA预训练模型，由于当前不支持这些大模型导致用户期望能够集成这些模型以提升NLP建模的效果。

https://gitee.com/mindspore/mindformers/issues/I6JZNU
这是一个bug报告，涉及mindformers项目中删除冗余且后续不支持的模块的问题。由于部分模块已不支持并引发冗余导致的bug。

https://gitee.com/mindspore/mindformers/issues/I6JZA1
这是一个bug报告，主要对象是修改common文件夹名称为core引起的问题。

https://gitee.com/mindspore/mindformers/issues/I6JYEO
这是一个用户提出需求的issue，主要对象是支持昇思羲和生态平台权重自动下载及加载。

https://gitee.com/mindspore/mindformers/issues/I6JYDE
这是一个bug报告，涉及主要对象是MindFormers库的__all__导入包统一规范，用户遇到了由于不统一导致的问题。

https://gitee.com/mindspore/mindformers/issues/I6JYD4
这是一个用户提出需求的类型，该问题单涉及的主要对象是README文档及特性文档，用户可能希望更新至r0.3版本以获取最新的信息。

https://gitee.com/mindspore/mindformers/issues/I6JYCJ
这个issue是用户提出需求类型，主要对象是MindFormers GPT2支持MindSpore全部并行策略，用户提出了需要支持全部并行策略的功能。

https://gitee.com/mindspore/mindformers/issues/I6J5BM
这是一个用户提出需求的问题单，主要涉及generate()接口适配gpt的问题。由于缺少具体的描述内容，无法准确分析用户的具体需求或问题。

https://gitee.com/mindspore/mindformers/issues/I6IXRX
这是一个bug报告，主要对象是BERT模型在并行跑起来时出现的问题，可能是因为配置文件的修改导致分布式训练脚本报错。

https://gitee.com/mindspore/mindformers/issues/I6IXE3
这是一个bug报告，主要涉及MindFormers支持多线程和断点续传特性下载大文件时出现的问题，由于客户网络不稳定导致下载缓慢且下载失败后需重新下载，希望改进下载体验。

https://gitee.com/mindspore/mindformers/issues/I6DNC0
这个issue属于用户提出需求类型，主要涉及MindFormers的易用性及通用性完善任务列表，由于缺乏标准的面向任务的数据集类规范和复杂的yaml配置，导致数据集部分规范方案和models部分存在一些缺陷。

https://gitee.com/mindspore/mindformers/issues/I6DK1G
这是一个bug报告，涉及translation_pipeline适配问题。由于非encoderdecoder结构的模型在调用model.generate方法时报错，需要修复相关逻辑。

https://gitee.com/mindspore/mindformers/issues/I6DH2V
这是一个用户提出需求的issue，主要涉及的对象是增加通用的train wrapper实现以及clip grad标准实现。该问题产生的原因是缺乏这些功能导致用户无法高效地进行模型训练。

https://gitee.com/mindspore/mindformers/issues/I6DGW6
这是一个bug报告，主要涉及门禁系统在代码中不能识别错误的情况，导致含有问题代码的PR可以通过门禁。

https://gitee.com/mindspore/mindformers/issues/I6DG38
这是一个功能需求类型的issue，主要涉及的对象是t5.py中的TransformerEncoder模块。由于代码分散在不同地方导致需要整理并放在一起，用户提出需求统一整改这一部分API。

https://gitee.com/mindspore/mindformers/issues/I6DDV6
该issue属于需求提出类型，主要对象是与huggingface对标的学习策略模块，用户提出了需要增加该模块并保持逻辑对齐的需求。

https://gitee.com/mindspore/mindformers/issues/I6D8SH
这是一个bug报告，主要涉及build.sh程序未能自动安装mindformers包，导致该包未被成功安装。

https://gitee.com/mindspore/mindformers/issues/I6D87I
这个issue属于需求提出类型，主要对象是对标HuggingFace的TrainingArguments类，并适配Trainer高阶接口，由于Trainer args入参行为不一致而导致需要对齐huggingface Trainer args入参行为。

https://gitee.com/mindspore/mindformers/issues/I6D1YM
这是一个bug报告，主要涉及的对象是用户安装的whl包，问题可能是由于文件权限过低导致。

https://gitee.com/mindspore/mindformers/issues/I6CRDJ
这是一个代码重构类型的issue，涉及到mindformers下的trainer中eval部分代码的重构。可能由于代码结构不清晰或有优化空间而提出该问题。

https://gitee.com/mindspore/mindformers/issues/I6CQZ9
这是一个需求提出类型的issue，主要涉及到mindformers项目中的日志功能改进。

https://gitee.com/mindspore/mindformers/issues/I6CBUC
这个issue类型是bug报告，主要涉及的对象是BertConfig部分属性命名不合理，在保存和加载时存在问题。原因是属性名中的下划线导致保存的yaml文件和加载时的属性名称不一致，导致加载失败。

https://gitee.com/mindspore/mindformers/issues/I6C1EP
这个issue是一个bug报告类型，涉及到mindformers库中的配置文件路径问题，导致在某些情况下会出现报错。

https://gitee.com/mindspore/mindformers/issues/I6BLVE
这是一个用户提问类型的issue，涉及mindformer中ckpt文件数量限制的设置，问题主要询问如何调整保存ckpt文件数量的限制。

https://gitee.com/mindspore/mindformers/issues/I6BE1X
这是用户询问问题类型的issue，主要涉及如何显示网络编译info日志的配置问题，由于设置GLOG_v=1无效，用户希望了解如何正确配置config以显示Info日志。

https://gitee.com/mindspore/mindformers/issues/I6BE1J
这是一个bug报告，涉及到t5模型中的dataset batchsize设置无效的问题。由于设置了batchsize为2后，实际运行却打印出batchsize为1，可能是由于代码逻辑错误导致。

https://gitee.com/mindspore/mindformers/issues/I6B3C6
这是一个bug报告，涉及的主要对象是运行Mindformer dataset时出现的报错，可能因为配置文件或代码问题导致了该问题。

https://gitee.com/mindspore/mindformers/issues/I6AIVR
该issue类型为bug报告，涉及的主要对象是Model中的Config默认入参。由于默认值为None导致用户无法正确实例化模型配置，使得部分模型无法正常运行。

https://gitee.com/mindspore/mindformers/issues/I6AHNI
这是一个用户提出需求的类型，主要对象是增加直接pip wheel安装包方式。原因可能是用户希望能够更方便快捷地安装包。

https://gitee.com/mindspore/mindformers/issues/I6AHNH
这是一个用户提出需求的issue，主要涉及对象是修改obs云上存储默认文件夹名称为MindFormers。

https://gitee.com/mindspore/mindformers/issues/I6ACMD
这是一个关于构建时需要在setuptools打包时去除tests文件夹的需求，主要对象是项目的构建配置。

https://gitee.com/mindspore/mindformers/issues/I6AC3X
这个issue是一个bug报告类型，主要涉及GPT效果复现问题，可能由于数据生成部分未对齐导致无法复现期望的效果。

https://gitee.com/mindspore/mindformers/issues/I6AARK
这是一个bug报告，涉及到AutoTokenizer和BaseTokenizer模块中showsupportlist功能与其他模块对齐的问题，由于展示内容不对齐和support-list校验需修改导致。

https://gitee.com/mindspore/mindformers/issues/I6AA73
这个issue是一个需求类型，涉及MindFormers 大模型套件 330 版本的优化升级计划，用户提出了待办项列表。

https://gitee.com/mindspore/mindformers/issues/I6A6EU
这个issue属于功能需求类型，主要对象是优化器参数预处理和权重预处理功能，用户提出了如何将这些功能作为模块调用到通用的task trainer中的问题。

https://gitee.com/mindspore/mindformers/issues/I6A6AQ
这个issue类型是bug报告，该问题单涉及的主要对象是Task Trainer Examples，由于缺乏规范，导致调用不一致性，用户提出需要规范用例以确保一致性。

https://gitee.com/mindspore/mindformers/issues/I6A648
这是一个bug报告，主要涉及的对象是所有模型支持半自动并行模式运行。原因是出现了无法重复的问题或报错信息。


https://gitee.com/mindspore/mindformers/issues/I6A642
这是一个需求提出类型的issue，涉及主要对象为Mindformers的models文件夹及Model类。

https://gitee.com/mindspore/mindformers/issues/I6A63O
这是一个用户提出需求的类型，主要涉及数据集API整改和规范化，兼容2.0版本。

https://gitee.com/mindspore/mindformers/issues/I6A4AP
这是一个需求更改类型的issue，该问题单涉及的主要对象是变量命名。原因是修改变量名导致代码中使用该变量的地方需要做相应的更改。

https://gitee.com/mindspore/mindformers/issues/I69RHO
这个issue是关于训练imagenet1k数据集时出现的bug报告，使用全量数据跑训练会报错，使用小样本却可以正常运行。

https://gitee.com/mindspore/mindformers/issues/I69MAV
这是一个bug报告，问题涉及Trainer高阶接口中eval时回调函数使用了train的callback而导致无效的打印输出。

https://gitee.com/mindspore/mindformers/issues/I69MAD
这是一个bug报告，涉及的主要对象是Trainer高阶接口。由于模型实例无法加载输入的权重，导致出现了无法加载权重的症状。

https://gitee.com/mindspore/mindformers/issues/I69KB4
该issue类型为用户提出需求，涉及的主要对象是兼容Huggingface接口。由于与Huggingface接口不对齐，导致需要在主要接口上进行调整。

https://gitee.com/mindspore/mindformers/issues/I69G59
这是一个bug报告，主要涉及到gitee上的mindformers项目中的bert和T5的readme格式对齐与校验。由于readme文件格式不对齐并且部分内容有错误，需要进行修复和校验。

https://gitee.com/mindspore/mindformers/issues/I698IW
这是一个bug报告，涉及run_mindformer接口无法有效指定数据集路径，可能是由于代码逻辑错误导致的。

https://gitee.com/mindspore/mindformers/issues/I698IG
这是一个bug报告，主要涉及Trainer高阶接口train属性的resume_or_finetune_from_checkpoint参数无法有效启用，可能由于某些原因导致无法正确实现从checkpoint恢复或微调的功能。

https://gitee.com/mindspore/mindformers/issues/I69222
这是一个bug报告，主要涉及模型配置中的batch_size移除导致的问题，用户寻求关于模型下游任务Head可替换模块的帮助。

https://gitee.com/mindspore/mindformers/issues/I68UAP
这是一个用户提出需求的issue，主要涉及词表的配置文件应该存放在哪里的问题。由于现在需要手动给每个model的config中添加tokenizer的内容，因此用户在寻求一个更合理的存放词表配置信息的方式。

https://gitee.com/mindspore/mindformers/issues/I68U5R
这是一个需求提出类型的issue，主要涉及到LR组件重复导致的功能重叠问题，提出合并组件并统一命名的建议。

https://gitee.com/mindspore/mindformers/issues/I68OA5
这是一个bug报告类型的issue，主要涉及到Trainer高阶接口的train/evaluate/predict同时使用时会报错。由于某种原因导致了报错的症状，用户希望得到相关帮助解决这个问题。

https://gitee.com/mindspore/mindformers/issues/I688YK
这是一个bug报告，涉及的主要对象是代码在CPU服务器上测试时发现是旧的。由于代码未及时更新，导致在CPU服务器上测试时出现问题。

https://gitee.com/mindspore/mindformers/issues/I688B2
这是一个用户提出的需求。该问题单涉及的主要对象是项目中的run_mindformer功能。用户提出需求是为了给run_mindformer添加finetune功能，可能是为了使模型能够在特定任务上进行微调。

https://gitee.com/mindspore/mindformers/issues/I67W0U
这个issue类型是需求提出，主要涉及Trainer高阶接口API的注释和TaskTrainer逻辑的修复。

https://gitee.com/mindspore/mindformers/issues/I67NVC
这是一个用户提出需求的issue，主要涉及到配置文件加载逻辑的改动。根据描述，用户希望将配置文件加载方式从云端切换为本地离线默认yaml加载逻辑。

https://gitee.com/mindspore/mindformers/issues/I67NUU
这是一个bug报告类型的issue，主要涉及推理相关的Base接口。问题是由于修改推理相关的Base接口，去除FeatureExtrator逻辑而引起的。

https://gitee.com/mindspore/mindformers/issues/I67LG1
这是一个用户提出需求的类型，主要涉及到mindformers项目中的opt模块。用户希望尽快完善opt模块与相关文档，以便进行文本摘要生成相关工作。

https://gitee.com/mindspore/mindformers/issues/I67JMV
这是一个用户提出需求类型的issue，主要涉及新增Predict模板。由于用户需要使用Predict模板来进行某种预测分析，因此提出了这个问题。

https://gitee.com/mindspore/mindformers/issues/I67JMS
该issue类型为需求提出，涉及的主要对象是通过resume_checkpoint_path恢复训练功能支持。

https://gitee.com/mindspore/mindformers/issues/I67JMQ
这个issue类型为用户提出需求，涉及的主要对象是新增的zeroshotimageclassification Trainer API接口。

https://gitee.com/mindspore/mindformers/issues/I67JML
这个issue类型是需求提出，涉及的主要对象是MindSpore的数据增强算子接口。

https://gitee.com/mindspore/mindformers/issues/I67JMI
该issue属于用户提出需求类型，主要涉及Task Trainer规范接口和模板代码。

https://gitee.com/mindspore/mindformers/issues/I67JC4
这是一个用户提出需求的issue，涉及 MindFormers 套件的API文档注释。该需求可能是为了改进文档的完整性或准确性。

https://gitee.com/mindspore/mindformers/issues/I67ESZ
这个issue是用户提出的需求，主要涉及TranslationPipeline需要支持BatchSize输入。由于nlp的generate方法以及pipeline的输入目前不支持batch输入，用户希望该功能能够被支持，以提升处理效率。

https://gitee.com/mindspore/mindformers/issues/I67D5B
这是一个bug报告类型的issue，涉及的主要对象是ParallelConfig存储方式。由于使用yaml保存后出现不易阅读的python字符串，导致了用户提出了关于存储方式优化的问题。

https://gitee.com/mindspore/mindformers/issues/I67C4K
这是一个用户提出需求的类型，主要涉及的对象是代码中的T5Porcessor，由于使用了错误的T5Porcessor导致了功能无法正常运行，用户希望将其替换为正确的T5Tokenizer。

https://gitee.com/mindspore/mindformers/issues/I67C00
这是一个bug报告类型的issue，涉及到套件API名称与MindSpore重复导致注册时疯狂打印重复告警。

https://gitee.com/mindspore/mindformers/issues/I67B6N
这个issue类型为bug报告，主要涉及MindFormer的modules模块规范修改，由于对规范的修改引起了问题的发生。

https://gitee.com/mindspore/mindformers/issues/I679ZW
这是一个用户提出需求的 issue，涉及的主要对象是 pipeline 接口。

https://gitee.com/mindspore/mindformers/issues/I674YK
这个issue属于用户提出需求类型，主要对象是Trainer组件。由于缺乏保存配置文件的功能，用户提出了需要Trainer支持配置文件保存的请求。

https://gitee.com/mindspore/mindformers/issues/I67487
这个issue属于bug报告类型，涉及的主要对象是T5精度对齐。可能由于模型训练或部署过程中的问题导致T5模型的精度出现偏差，用户在请求帮助解决精度对齐的问题。

https://gitee.com/mindspore/mindformers/issues/I672E7
这是一个关于需求的issue，主要对象是支持默认的通用task_name类型，由于某种原因导致用户提出了关于该问题如何引起、重现步骤和报错信息的问题。

https://gitee.com/mindspore/mindformers/issues/I66LPR
这是一个用户提出需求的类型，该问题单涉及的主要对象是Trainer接口。由于Trainer接口未提供指定Wrapper为DynamicLossScale的方法，用户在使用时无法修改相关参数，导致了无法灵活调整动态损失缩放功能的问题。

https://gitee.com/mindspore/mindformers/issues/I66LJL
这是一个bug报告，该问题主要涉及模型加载权重的问题，由于from_pretrained方法只是实例化参数并未加载权重，导致无法成功加载checkpoint。

https://gitee.com/mindspore/mindformers/issues/I66KD6
这个issue属于用户提出需求的类型，主要涉及到的对象是`build_tokenizer`的内部实现。由于Yaml文件中没有清晰指定传入的词表文件路径，导致需要提供一个词表文件路径的问题。

https://gitee.com/mindspore/mindformers/issues/I66D1A
这是一个bug报告，主要涉及Import导入方式导致自定义API未覆盖MS的原始API，可能原因是导入方式不正确导致自定义API未能正确覆盖MS的原始API。

https://gitee.com/mindspore/mindformers/issues/I66CQT
这是一个bug报告，涉及主要对象为Trainer测试用例。因CI门禁需求变更，导致需要对Trainer测试用例进行修正。

https://gitee.com/mindspore/mindformers/issues/I662I3
这是一个用户提出需求的类型的issue，主要涉及到Trainer高阶接口的predict属性函数。由于缺乏这一功能，用户提出了希望Trainer高阶接口能够支持predict属性函数的需求。

https://gitee.com/mindspore/mindformers/issues/I662HM
这是一个需求提出类型的issue，主要涉及支持Swinv2分类骨干网络的功能。由于目前系统不支持该功能，用户提出需求希望能够新增对Swinv2分类骨干网络的支持。

https://gitee.com/mindspore/mindformers/issues/I662HI
这是用户提出需求类型的issue，主要涉及的对象是支持simmim预训练模型及swinV1分类网络。由于用户希望增加对simmim预训练模型和swinV1分类网络的支持，因此提出了这个需求。

https://gitee.com/mindspore/mindformers/issues/I662HA
这是一个功能需求类型的issue，主要涉及支持MAE预训练模型及Vit分类网络。可能是用户需求增加对这两种模型的支持和集成。

https://gitee.com/mindspore/mindformers/issues/I662H5
这个issue属于文档改进类型，主要涉及到规范trainer和common模块的docs注释。

https://gitee.com/mindspore/mindformers/issues/I662B1
这是一个用户提出需求的类型，主要涉及MindFormers仓库的Import方式规范和统一。

https://gitee.com/mindspore/mindformers/issues/I65UUD
这是一个bug报告，主要对象是Profile模块，该问题可能由于Profile模块无法正确收集性能数据导致。

https://gitee.com/mindspore/mindformers/issues/I65Q2X
这个issue类型是用户提出需求，该问题单涉及的主要对象是BERT模型。由于最新版代码中没有支持BERT模型，用户希望尽快添加该支持。

https://gitee.com/mindspore/mindformers/issues/I65F27
这是一个需求类型的issue，主要涉及的对象是README文档使用说明，并由于文档使用说明不清晰引起了问题。

https://gitee.com/mindspore/mindformers/issues/I64HWS
这是一个bug报告，涉及的主要对象是添加T5模型时调用from_pretrained(xxx)报类型不匹配错误。由于保存时未正确存储类型信息，导致加载模型时类型不匹配报错。

https://gitee.com/mindspore/mindformers/issues/I64H1K
这是一个用户提出需求的issue，主要涉及到Trainer接口的完善和功能改进。

https://gitee.com/mindspore/mindformers/issues/I645DK
这是一个bug报告，主要涉及对象是OPTTrainingConfig，在推理过程中出现了属性错误，并且缺少特定参数导致程序无法正确运行。

https://gitee.com/mindspore/mindformers/issues/I63TGU
这个issue类型是需求提出，涉及的主要对象是MindFormers设计文档。由于文档内容描述不清晰，导致用户提出了关于框架对外接口和网络调用设计的问题。

https://gitee.com/mindspore/mindformers/issues/I636CY
这是一个 bug 报告，主要涉及的对象是仓库包更新。问题是由于仓库包未更新为mindformers而导致的。

https://gitee.com/mindspore/mindformers/issues/I630CX
这是一个用户提出的需求类型的issue，涉及的主要对象是各个模块的通用build接口。这个问题的提出可能是为了提升项目的代码可维护性和规范性，增加License用于标识各个模块的知识产权归属。

https://gitee.com/mindspore/mindformers/issues/I62OB5
这是一个需求类型的issue，主要涉及到打包wheel和测试安装的过程。

https://gitee.com/mindspore/mindformers/issues/I62O22
该issue类型是任务需求，主要涉及的对象是接口测试用例的开发，用户希望编写import、register、build接口的测试用例。

https://gitee.com/mindspore/mindformers/issues/I62O1H
这是一个关于需求的问题，涉及的主要对象是Trainer接口，由于需要与huggingface transformers接口使用方式进行对齐，因此产生了该问题。

https://gitee.com/mindspore/mindformers/issues/I61KF7
这是一个用户提出需求的issue，主要涉及Tokenzier的分析。由于需要增强易用性并减少用户学习成本，用户希望实现通用的Tokenzier，参考HuggingFace库的实现。


https://gitee.com/mindspore/mindformers/issues/I61HAK
这是一个bug报告，涉及到执行GPT模型训练时出现的错误，可能是由于数据处理过程中出现了某些问题导致模型无法找到特定的列名所致。

https://gitee.com/mindspore/mindformers/issues/I5YYL8
这是一个用户提出需求的类型，主要涉及对象是T5和GPT模型的脚本推理功能。由于现有情况下无法进行T5和GPT模型的脚本推理，用户希望增加这一功能。

https://gitee.com/mindspore/mindformers/issues/I5WHA9
这是一个bug报告，涉及的主要对象是使用125m模型进行推理时在transformer/models/opt/opt.py中的EvalNet构造方法参数数量不匹配导致无法运行。

https://gitee.com/mindspore/mindformers/issues/I5WBLV
这是一个bug报告，涉及主要对象是运行transformer/models/t5中的t5_predict.py时出现错误。由于加载的checkpoint与网络结构不匹配导致报错。

https://gitee.com/mindspore/mindformers/issues/I5VDGV
这是一个bug报告，涉及预训练模型权重转换后使用预测时出现的TypeError。原因是在构造'Cell'时，给定的参数数量不符合要求。

https://gitee.com/mindspore/mindformers/issues/I5U65O
这个issue是一个bug报告，涉及的主要对象是OPT在使用转换后的预训练模型权重进行预测时报错，可能由于模型权重转换或者OPT实现逻辑的问题导致了此bug。

https://gitee.com/mindspore/mindformers/issues/I5QEUW
这是一个Bug报告，主要涉及Mindformers项目下的transformer文件中的requirements缺少依赖模块yaml，可能由于疏忽导致了该问题。

https://gitee.com/mindspore/mindformers/issues/I5Q5NV
这是一个用户提出需求的issue，主要对象是mindformers下的transformer库，由于当前transformer库推理流程不支持BeamSearch，导致用户请求优先适配这一功能。

https://gitee.com/mindspore/mindformers/issues/I5Q1MI
这个issue类型是bug报告，主要涉及 Mindformers 项目中 BERT 数据预处理库中一个拼写错误导致的问题。

https://gitee.com/mindspore/mindformers/issues/I5Q148
这是一个bug报告类型的issue，涉及的主要对象是无法找到名为'tranformer'的模块导致的报错。

https://gitee.com/mindspore/mindformers/issues/I5PJS7
这是一个关于代码bug报告，主要问题是在Windows平台无法正确识别路径名，可能是由于引号问题导致的。

https://gitee.com/mindspore/mindformers/issues/I5OBJ3
该issue属于用户提出需求类型，主要对象为项目仓库中的ReadMe.md文件。用户提出在ReadMe.md中增加介绍如何为transformer仓库做贡献和添加自己的网络，旨在方便开发者为仓库做贡献并扩大其影响力。

https://gitee.com/mindspore/mindformers/issues/I5OBIK
这个issue类型是用户提出需求，该问题单涉及的主要对象是transformer仓库。由于开发者希望通过pip安装whl包的方式来使用transformer仓库，因此提出了这个需求。

https://gitee.com/mindspore/mindformers/issues/I5O5ZO
这是一个需求反馈，主要对象是Transformer库的整体接口，问题源于接口修改复杂、配置文件臃肿以及文档结构不佳。

https://gitee.com/mindspore/mindformers/issues/I5KM8W
这是一个建议性问题，用户提出了关于MindSpore框架中transformer中_Linear参数has_bias不可选的问题，希望增加可选性，允许设置has_bias为False。

https://gitee.com/mindspore/mindformers/issues/I5KM65
这是关于缺失推理代码Beam_Search的bug报告，主要涉及mindspore.nn.transformer中T5模型的推理功能缺失，导致无法进行模型上线推理。

https://gitee.com/mindspore/mindformers/issues/I5J6O3
这是一个bug报告，用户指出pretrain_t5.py文件中的get_model_config函数缺少输入，导致其中的seq_length与position_embedding不一致。

https://gitee.com/mindspore/mindformers/issues/I5J2QX
这个issue类型是bug报告，涉及目标是优化器中的参数调用错误，导致在r1.6之前版本无法识别该参数。

https://gitee.com/mindspore/mindformers/issues/I4VE50
这是一个提出需求的issue，主要涉及项目的代码结构设计。

https://gitee.com/mindspore/mindformers/issues/I4UTG1
这是一个需求提出的issue，主要涉及T5模型实现中的一些差异和参考链接。由于Megatron中T5模型的实现与官方实现存在一些区别，用户提出了需要增加T5模型以及指出了不同之处，并提供了参考链接。

