https://github.com/NVIDIA/TensorRT-LLM/issues/3296
这个issue类型是功能需求，涉及的主要对象是TensorRT-LLM，提出了新增对Phi-4-MM的支持的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3295
该issue属于测试类问题，主要对象是单GPU分散测试。由于缺少这些测试，可能导致对于单GPU分散场景的代码不够健壮。

https://github.com/NVIDIA/TensorRT-LLM/issues/3294
这是一个特性增加的请求，主要涉及Blackwell和Hopper MoE Gemm2+Finalize fusion实现，由于需要更新internal_cutlass_kernels，因此暂时不合并。

https://github.com/NVIDIA/TensorRT-LLM/issues/3293
这个issue属于用户提出需求，主要涉及的对象是更新 speculative-decoding.md 文件。由于原文表述不清，用户希望对文档进行更清晰的表达。

https://github.com/NVIDIA/TensorRT-LLM/issues/3292
这是一个需求提出的issue，主要涉及的对象是TensorRT-LLM下的lm_eval_tensorrt_llm.py脚本，主要原因是需要支持自动部署、提高Torch编译缓存大小、启用聊天模板以及升级lm_eval到最新版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/3291
这是一个清理工作类型的issue，主要涉及删除评论中的用户名。

https://github.com/NVIDIA/TensorRT-LLM/issues/3290
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM的服务功能。由于高并发时出现py_decoding_iter错误，推测可能是在使用64到100个并发请求时出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3289
这是一个用户提出需求的 issue，主要涉及将 ENABLE_MULTI_DEVICE 注册为 CMake 选项，由于该选项目前在 cpp/CMakeLists.txt 中是隐藏的，用户想了解是否有将其注册为 CMake 选项的潜在影响。

https://github.com/NVIDIA/TensorRT-LLM/issues/3288
这是一个bug报告，主要涉及修复overlap dp测试的问题。由于测试中存在重叠的数据包，导致了此问题的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/3287
这是一个bug报告，主要涉及TensorRT-LLM库的构建和安装问题，用户在尝试从源代码构建并与wheel文件一起安装时遇到了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3286
这是一个bug报告，主要涉及对MLA中一些未释放的张量进行释放，由于未释放这些不再使用的张量，可能导致内存没有及时释放。

https://github.com/NVIDIA/TensorRT-LLM/issues/3285
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM的multi GPU测试，由于导致超时问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3284
这个issue类型是提出需求，涉及的主要对象是用于TensorRT-LLM的ZMQ socket IPC通信。创建该issue的原因可能是用户希望改进目前的通信方式，使用pydantic json序列化来实现通信。

https://github.com/NVIDIA/TensorRT-LLM/issues/3283
这是一个特性新增的issue，主要对象是在TensorRT-LLM中添加对量化MoE的支持。由于模式匹配器需要更新来处理不同的模型结构，导致需要一系列步骤来实现对量化MoE的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/3282
这是一个基础设施改进的issue，主要涉及将 nvrtc_wrapper 迁移到 Conan 包。因缺少 Rocky8 容器镜像中的 sqlite3 python 模块，导致无法通过 pip 安装 Conan，故采用了官方发布的独立 Conan 可执行文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/3281
这个issue属于用户提出需求，主要涉及支持DeepseekV3在AutoDeploy中的使用。原因是需要对DeepseekV3进行模型补丁以及其他支持，以实现自动部署功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3280
这个issue为bug报告类型，主要涉及到TensorRT-LLM中trtllm-serve对openai chunk streaming的bug。由于部分chunk返回整个响应而非增量，导致出现异常症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/3279
这个issue类型是更新维护（chore），涉及的主要对象是更新内部的"internal_cutlass"版本号。

https://github.com/NVIDIA/TensorRT-LLM/issues/3278
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM下的Redrafter模型，导致问题的原因是使用了未归一化的draft logits 导致sampling出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3277
这是一个bug报告，主要涉及TensorRT-LLM中的`--gather_generation_logits`功能无法在A100上使用。导致该问题可能是由`int4_wo`引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3276
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM中的lookahead decoding以及multimodal input支持。原因是由于PromptTuningEmbedding层假设了prompt_tokens的形状，而启用speculative decoding时会导致形状不兼容的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3275
这是一个空白的issue，没有包含具体内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/3274
这是一个类型为bug报告的issue，主要涉及的对象是TensorRT-LLM库。由于无法找到适用于特定形状的GEMM配置，导致出现此错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3273
这是一个用户询问相关硬件支持的问题，主要涉及对象为NVIDIA的Jetson Orin Nano 8 GB开发套件和TensorRT的Vision Language Model。由于用户需要确认该硬件和软件环境是否支持特定的TensorRT模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/3272
这是一个bug报告类型的issue，涉及的主要对象是 deepseek-v3 模块的 mtp 命令。这个问题可能由于 deepseekv3 的 mtp 命令存在问题而导致错误或无法使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/3271
这个issue类型是用户提出需求，请教问题，主要涉及的对象是attention backend interface，用户寻求对其进行完善。

https://github.com/NVIDIA/TensorRT-LLM/issues/3270
这是一个bug报告，涉及线程泄漏和内存泄漏的问题。这个问题可能由于未关闭线程或未释放内存导致CI系统出现OOM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3269
这是一个功能增强（feature enhancement）类型的issue，主要涉及的对象是Hopper XQA模块。由于之前只有Ampere XQA支持推测解码，而Hopper XQA未支持，因此在Hopper GPU上性能存在改进空间。

https://github.com/NVIDIA/TensorRT-LLM/issues/3268
这是一个关于基础设施问题的bug报告，主要涉及bot在pull request触发时出现的错误检查，可能是由于配置问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3267
这是一个用户提出需求的issue，主要涉及后端类型过滤支持。原因可能是用户希望能够根据后端类型来过滤和筛选相关功能或模块。

https://github.com/NVIDIA/TensorRT-LLM/issues/3266
这是一个功能增强类型的issue，涉及的对象是TensorRT-LLM中的`tensorrt_llm` package。由于需要增加工具来支持json_schema对LLM中的函数调用进行检测和验证，从而帮助提高代码的可读性和可维护性。

https://github.com/NVIDIA/TensorRT-LLM/issues/3265
该issue类型是功能增强（feature enhancement），主要涉及的对象是`tensorrt_llm/serve`模块。由于用户希望增强`tensorrt_llm/serve`模块中的功能和对`tool_call`的OpenAI协议处理。

https://github.com/NVIDIA/TensorRT-LLM/issues/3264
这个issue类型是bug报告，该问题单涉及的主要对象是测试用例。由于测试用例执行失败，用户希望重新运行失败的测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/3263
这是一个Bug报告，主要涉及对象为Jetson Orin Nano Super 8GB。由于Jetson Orin Nano Super在构建TensorRT库时出现重启，可能是由于软件版本不兼容或配置问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3262
这是一个用户提出需求的issue，主要对象是TensorRT-LLM中的DS V3文档。由于缺少"Serving"部分，用户希望文档中添加该部分内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/3261
这是一个bug报告，主要涉及测试脚本"test_autotuner.py" 中的延迟和缓存结果检查，并解决了导致 CI 测试中出现非确定性失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3260
这个issue是关于功能改进的建议，主要涉及到迁移DeepSeek测试，可能由于测试准确性不佳导致的需改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/3259
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的FP4模型构建，在特定环境下失败。原因可能是与FP8 FMHA相关的编译问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3258
这个issue属于用户提出需求，并且主要涉及的对象是支持 AWQ 模型优化检查点。

https://github.com/NVIDIA/TensorRT-LLM/issues/3257
这是一个功能特性新增的issue，主要涉及的对象是TensorRT-LLM下的pytorch flow。原因是为了更方便地管理UserBuffers，并优化UB的分配和使用方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/3256
这个issue类型是测试相关的其它问题，主要对象是测试列表中的测试，由于一些原因导致需要移除qa测试列表中的测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/3255
这是一个类型是bug报告的issue，主要涉及的对象是测试模块。由于参数设置错误，导致了测试失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/3254
这是一个bug报告，主要涉及修复警告问题的问题。由于代码中存在警告，需要进行修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/3253
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中无法成功转换特定模型的问题。原因可能是代码中的错误导致无法从Hugging Face模型中转换所需的模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/3252
这是一个维护类的issue，主要涉及更新模型优化工具至0.27版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/3251
这个issue类型是用户提出需求，主要对象是TensorRT-LLM下的层级融合功能。由于最后一层PP等级无法使用融合，导致用户提出需要为非边界PP层启用后MLP/MOE融合的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3250
这是一个功能特性增强（Feature Enhancement）类型的issue，主要涉及的是在TensorRT-LLM中添加压力测试（stress test）功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3249
这是一个用户提出需求的issue，主要涉及测试超时阈值设置。由于测试运行时间长，用户希望将检测时间限制提高到5400秒。

https://github.com/NVIDIA/TensorRT-LLM/issues/3248
这是一个功能增强类型的issue，主要对象是新增对SM 120的FP8支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/3247
这个issue类型是用户提出需求，涉及的主要对象是"feat: Gemma"。由于缺少具体的描述内容，无法确定具体问题或需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3246
这是一个bug报告。问题涉及的主要对象是TensorRT-LLM中的测试代码。这个问题由于测试代码中缺少延迟而导致部分测试失败，通过增加延时解决了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3245
该issue类型为功能增强请求，主要对象是AutoTuner缓存的序列化和反序列化功能。这个问题的根本原因是需要实现一个简单的dump选项来实现确定性调优缓存。

https://github.com/NVIDIA/TensorRT-LLM/issues/3244
这个issue是关于bug报告，涉及的主要对象是PP + MTP配置。由于不支持的配置导致了bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3243
这是一个功能需求，单涉及的主要对象是TensorRT-LLM，用户提出了添加一个选项来实现无上下文服务器的分解服务运行，并提到设置环境变量`TRTLLM_DISAGG_BENCHMARK_GEN_ONLY`为1来实现该功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3242
这是一个文档更新的issue，主要对象是GitHub仓库的gh-pages分支。可能由于文档内容需要更新或修复，导致用户提交了这个issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/3241
这是一个 bug 报告，涉及的主要对象是在 TensorRT-LLM 中的调度器行为。由于重叠调度器在处理请求时可能忽略计算第二个生成的令牌，导致了此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3240
这是一个bug报告，该问题涉及TensorRT-LLM中的pytorch workflow的接受率问题。这个问题可能是由于代码逻辑错误或者实现不完整导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3239
这个issue类型是功能增强请求，主要涉及使用NVRTC来进行DeepGEMM JIT编译。

https://github.com/NVIDIA/TensorRT-LLM/issues/3238
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的torch nvsmall模型加载和生成过程。由于未正确设置dtype和num_key_value_heads参数，以及未使用正确的Linear模块，导致了模型加载和TP支持上的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3237
这是一个Bug报告，主要涉及对象是Scaffolding MajorityVote模块，由于不支持负数结果，导致出现了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3236
这是一个bug报告，主要涉及Scaffolding MajorityVote的失败，可能是由于负数结果导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3235
这是一个bug报告，主要涉及"Qwen2-VL"模型在TensorRT-LLM和huggingface两个环境下输出不一致的问题，可能是由于模型引擎或推理环境的差异导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3234
这是一个重构类型的issue，主要涉及到Executor和Decoder状态的数据类重构。可能由于代码结构不够清晰或者需要优化，导致用户提出了进行重构的建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/3232
这个issue为需求类型，主要涉及文档内容。由于缺少具体描述，用户提出需要添加ds1性能文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/3231
这是一个版本发布通知类型的issue，主要涉及的对象是TensorRT-LLM软件。

https://github.com/NVIDIA/TensorRT-LLM/issues/3230
这个issue类型是用户提出需求，请教问题， 主要对象是在TensorRT-LLM下的DSR1，由于缺少最佳性能实践，用户可能遇到性能方面的问题或需求更好的指导。

https://github.com/NVIDIA/TensorRT-LLM/issues/3229
这是一个关于修复构建错误的问题单，主要涉及TensorRT-LLM项目的构建流程。原因是构建GH200镜像时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3228
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的一个测试用例。这个问题由于测试依赖于执行时间，容易受线程调度和延迟影响，导致测试不稳定。

https://github.com/NVIDIA/TensorRT-LLM/issues/3227
这是一个用户提出需求的issue，该问题单涉及的主要对象是TensorRT-LLM，导致了这个问题的原因可能是用户希望进行测试或者草稿编写。

https://github.com/NVIDIA/TensorRT-LLM/issues/3226
这是一个bug报告，主要涉及TensorRT-LLM下的一个issue，并由于添加了优化导致运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3225
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM中的deepseek功能。由于pipeline并行性问题导致了deepseek功能失败，用户寻求对此问题的修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/3224
这是一个需求类型的 issue，主要涉及到贡献者的代码合并。由于缺乏贡献者指导文件，需要添加一个目录来帮助贡献者更方便地将他们的代码合并到项目中。

https://github.com/NVIDIA/TensorRT-LLM/issues/3223
这是一个功能需求的issue，主要对象是Qwen2.5-vl vit模型在转换成TensorRT过程中出现了精度问题，并且通过重写乘法操作可以解决精度问题，但会降低运行速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/3222
该issue是一个清除无用文件的任务，并非bug报告，主要涉及的对象是项目中的一个Python脚本文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/3221
这是一个用户提出需求的issue，主要涉及的对象是PyTorch flow。原因是用户希望在PyTorch flow中返回logits。

https://github.com/NVIDIA/TensorRT-LLM/issues/3220
这是一个bug报告类型的issue，主要涉及的对象是GPT-Next的转换失败问题。由于可能的编码错误或转换逻辑不完整所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/3219
这个issue属于需求类型，主要涉及在测试中移动Qwen测试，可能由于测试精度问题或者性能优化需求而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/3218
这是一个用户提出需求的issue，主要涉及到在TensorRT-LLM中添加纯PyTorch实现的MLA以进行准确性调试。 由于当前实现不能满足用户的需求，因此用户希望添加这个功能来提高模型的准确性 debugging。

https://github.com/NVIDIA/TensorRT-LLM/issues/3217
这是一个bug报告，提到要修复测试中的L0问题，主要涉及TensorRT-LLM项目。可能是因为L0测试出现了问题，需要关注和修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/3216
这个issue类型是更新需求，涉及的主要对象是Docker镜像。由于依赖更新导致的版本不匹配，需要更新到新的基础Docker镜像。

https://github.com/NVIDIA/TensorRT-LLM/issues/3215
这个issue类型是文档改进（非bug报告），主要涉及到项目的集成测试指南。原因可能是测试指南需要进一步细化和完善。

https://github.com/NVIDIA/TensorRT-LLM/issues/3214
这是一个bug报告，主要涉及TensorRT-LLM中请求被取消后继续处理的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3213
这个issue为用户提出需求类型，涉及到fetch new requests方法的优化。由于当前方法不够精细，用户需要对其进行改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/3212
这是一个用户提出的需求。该问题单主要涉及TensorRT-LLM下的性能指标需要新增总token吞吐量的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3211
这是一条关于减少针对deepseek的测试用例的CI操作的bug报告，主要涉及TensorRT-LLM项目。由于测试用例过多导致CI运行时间过长，用户希望能减少测试用例的数量以优化CI流程。

https://github.com/NVIDIA/TensorRT-LLM/issues/3210
这个issue是一个chore类型的问题，主要涉及TensorRT-LLM下的Pytorch `LLM`实例不再需要`build_config`选项的变化。原因是最近CC的变化导致功能参数移动到了llmargs中，所以需要移除`.

https://github.com/NVIDIA/TensorRT-LLM/issues/3209
这是一个功能提议，主要涉及GPU直接存储，需要在构建容器中包含cuFile库。

https://github.com/NVIDIA/TensorRT-LLM/issues/3208
这个issue类型是升级请求，并涉及的主要对象是cmake版本。原因是为了正确支持最新的架构，可能导致在较低版本的cmake上出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3207
这个issue属于用户提出需求类型，涉及主要对象是EAGLE模型的量化支持。由于`smoothquant`参数在转换模型时出错，`use_weight_only`参数会导致部分查询错误，因此用户在寻找一个可行的量化解决方案并寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/3206
这是一个性能优化类型的issue，主要涉及到使用Llama 3.2 1B模型来加速Llama C++测试套件。可能由于测试套件运行速度较慢，提出采用更小的模型来提高性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3205
这个issue是一个功能增强的变更，主要涉及的对象是TensorRT-LLM模型的生成过程，由于输出维度未匹配导致生成结果不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/3203
这是一个缺少具体描述的issue，类型是需求提出。该问题单主要涉及的对象是TensorRT-LLM中的测试功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3202
这是一个类型为维护任务（chore）的issue，主要涉及到TensorRT-LLM项目中的源代码文件结构。这个问题由于未将`*.cubin.h`加入到忽略文件中，导致在进行版本控制时可能会涉及到这些文件，需要添加到ignorefile以避免不必要的干扰。

https://github.com/NVIDIA/TensorRT-LLM/issues/3201
这个issue是一个功能测试的反馈报告，主要涉及的对象是LoRa模块。由于某些原因导致了测试失败，需要进一步调查和修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/3200
这是一个空内容的bug报告issue，主要涉及的对象是名为"lora_manager"的模块。

https://github.com/NVIDIA/TensorRT-LLM/issues/3199
该issue属于代码重构类型，主要涉及将OpenAIServer的tokenizer替换为llm.tokenizer。原因是不再需要初始化另一个tokenizer并将其传递给服务器。

https://github.com/NVIDIA/TensorRT-LLM/issues/3198
这个issue属于用户提出需求类型，主要对象是优化广播新请求方法，可能是为了改善性能或功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3197
该issue为功能请求类型，提出了为参与者提供框架的目录，便于他们将代码合并到主分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/3196
这是一个空内容的 issue，无法确定其类型及关注对象。

https://github.com/NVIDIA/TensorRT-LLM/issues/3195
这个issue属于代码重构类型，主要涉及的对象是Decoder类，由于需要将DecoderFinishedEvent替换为CudaEvent，可能是为了优化性能或提高可维护性。

https://github.com/NVIDIA/TensorRT-LLM/issues/3194
该issue类型为文档需求，主要对象是开发人员，由于缺少关于在云端开发或者运行pod上的文档，用户提出了需要相关信息的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3193
这个issue类型是用户提出需求，该问题单涉及的主要对象是"update user list"。

https://github.com/NVIDIA/TensorRT-LLM/issues/3192
这是一个用户提出需求的issue，主要对象是为TensorRT-LLM项目添加Bot帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/3191
这是一个用户提出需求的issue，主要对象是增加机器人命令帮助和检查机器人命令功能。预计用户希望在TensorRT-LLM项目中添加机器人命令的帮助功能以及检查机器人命令的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3190
该issue属于功能增强类型，主要涉及的对象是在Hopper和Blackwell上添加对FP8 MLA的支持，包括实现pertensor FP8 e4m3量化以及Q和latent KV的quantization。此问题的产生是为了实现更广泛的硬件支持以及提高模型性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3189
这是一则用户提出需求的issue，主要涉及的对象是测试。原因可能是需要修改测试脚本来适配新的功能或场景。

https://github.com/NVIDIA/TensorRT-LLM/issues/3188
这个issue属于代码优化或者项目管理类型，主要涉及的对象是提交记录。原因可能是为了合并、整理或调整提交历史。

https://github.com/NVIDIA/TensorRT-LLM/issues/3187
这个issue属于用户提出需求类型，主要对象是TensorRT-LLM下的trtllm-serve，用户想要增加新的API示例和文档来增强功能和使用说明。

https://github.com/NVIDIA/TensorRT-LLM/issues/3186
这是一个功能需求的issue，主要对象是支持PeftCacheManager在Torch中的使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/3185
这是一个bug报告，涉及TensorRT-LLM中的"split prefill and decode"功能。原因是在该功能中出现了某种症状的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3184
这是一个bug报告，主要涉及到nvbug 5196515中与glm-4-9b路径有关的问题，可能由于路径设置错误导致该bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3183
这个issue是针对一个bug的报告，主要对象是TensorRT-LLM下的Decoder类。这个bug可能是由于某次提交引入了问题，导致在替换DecoderFinishedEvent为CudaEvent后出现了不可预料的结果，需要撤销该次提交。

https://github.com/NVIDIA/TensorRT-LLM/issues/3182
这是一个测试相关的issue，主要涉及LLM-API的测试情况，通过标题和内容无法准确判断具体问题的症状或需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3181
这是一个bug报告。问题涉及主要对象是TensorRT-LLM中的Decoder类。由于将DecoderFinishedEvent替换为CudaEvent导致多个测试用例中出现`bad optional access`错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3180
这是一个用户提出需求的类型，主要涉及的对象是项目的基础设施。由于需要自动同步分叉与上游的代码同步，用户提出了添加同步分叉操作的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3179
这个issue属于需求提出类型，主要涉及TensorRT-LLM库在PyTorch示例中添加支持的模型，可能是用户希望了解当前支持的模型列表并了解如何在Torch工作流程中使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/3178
这是一个bug报告，涉及Gemma示例安装问题，由于Python版本、pip版本和Ubuntu版本不兼容而导致安装依赖出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/3177
这个issue类型是需求提出，主要涉及的对象是Phi4MM。由于名称中包含"Draft"，推测用户正在起草关于Phi4MM的变更，可能寻求相关讨论或意见。

https://github.com/NVIDIA/TensorRT-LLM/issues/3176
这是一个 feature 请求。该问题单涉及的主要对象是支持 CUDA graphs for EAGLE3。由于需要支持CUDA graphs for EAGLE3，用户请求特性增加。

https://github.com/NVIDIA/TensorRT-LLM/issues/3174
这是一个用户提出需求的类型。该问题单涉及的主要对象是TensorRT-LLM中的modernBERT模型支持情况。由于缺乏关于modernBERT在TensorRTLLM中的支持情况的清晰说明，用户提出了关于该模型支持情况的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3173
该issue是关于bug报告，涉及到TensorRT-LLM下的CMake失败问题，由于CMake版本过低导致无法自动设置CMAKE_CUDA_ARCHITECTURES。

https://github.com/NVIDIA/TensorRT-LLM/issues/3172
这是一个bug报告，主要涉及到TensorRT-LLM中对LongT5的支持。这个问题是由于convert_checkpoint.py警告信息中提到的模型类型不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3171
这是一个版本升级的issue，主要对象是TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/3170
该issue是一个用户提出需求的问题，主要涉及TensorRT-LLM下的auto-regressive prefill-phase，用户希望得到所有token的hidden_states。由于现有实现仅返回最后一个token的hidden_states而非所有token，用户想要了解是否只能逐一生成token。

https://github.com/NVIDIA/TensorRT-LLM/issues/3169
该issue属于用户请教问题类型，主要涉及了TensorRT-LLM中的Batch Scheduling Policies。用户提出了关于`Guaranteed_no_evict`和`Max_Utilization`两种Batch Scheduling Policies的具体行为及代码实现细节的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/3168
这是一个bug报告，涉及TensorRT-LLM下greedy search在并发请求情况下出错的问题。原因可能是在并发情况下采样操作导致结果不一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/3167
这是一个功能改进的issue，主要涉及TensorRT-LLM的测试准确性的扩展以及新的API trtllmeval的引入。

https://github.com/NVIDIA/TensorRT-LLM/issues/3166
这是一个用户提出的需求类型的issue，涉及主要对象为TensorRT-LLM下的scaffolding worker和openai api worker，原因是需要对Worker进行重构以支持openai api，并实现相关功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3165
这是一个chore类型的issue，主要涉及Cutlass库的清理，要删除`internal_cutlass_type_conversion.h`文件。原因是在更新Cutlass到3.8.0版本后不再需要该文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/3164
这个issue是一个功能需求，主要涉及到GitHub上的TensorRT-LLM项目，用户提出需要添加一个同步fork操作的工作流程。

https://github.com/NVIDIA/TensorRT-LLM/issues/3163
这是一个 debug 的 issue，主要对象是 Fp8 block gemm 的开源代码。由于代码中注明了 "Don‘t merge，file this PR for debug only"，推测用户可能正在检查该功能的正确性，而非为了合并到主分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/3162
该issue类型为文档更新请求，涉及主要对象为TensorRT-LLM的主README文档。由于文档需要更新，用户请求审核并直接更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/3160
这个issue是一个用户提出需求的类型，主要涉及的对象是TensorRT-LLM中的Scaffolding模块。这个需求是为了支持基于奖励模型的best_of_n方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/3159
这是一个基础设施问题，涉及部分对象是连接到 urm-rn.nvidia.com 的问题。由于连接问题，提交了切换到 urm.nvidia.com 解决的问题单。

https://github.com/NVIDIA/TensorRT-LLM/issues/3158
这是一个需求类型的问题单，主要涉及StreamGenerationTask的支持在scaffolding上。原因是在推理时的计算方法中，scaffolding控制器可能需要检查已经生成的标记而不中断正在进行的工作请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3157
这是一个bug报告，涉及TensorRT-LLM的多GPU测试问题，由于测试被忽略，导致了问题的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/3156
这是一个功能增加的issue，主要涉及TensorRT-LLM的Qwen2.5VL和Qwen2VL。由于Qwen2.5VL需要transformers版本 >= 4.49.0，因此存在等待版本升级的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3155
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的cuda graph功能。这个问题可能是由于cuda graph导致输出不正确而被提出，因此建议暂时禁用cuda graph和MTP以进行重叠测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/3154
这是一个改进代码结构的issue，主要对象是`test_disaggregated.py`脚本。

https://github.com/NVIDIA/TensorRT-LLM/issues/3153
该issue类型为文档改进，主要对象是项目文档。这个问题可能是由于文档中格式不规范，导致部分内容没有正确使用alert formatting。

https://github.com/NVIDIA/TensorRT-LLM/issues/3152
这是一个版本更新类型的issue，主要对象是库中的版本号。

https://github.com/NVIDIA/TensorRT-LLM/issues/3151
这是一个特性新增的issue，主要涉及TensorRT-LLM下的Autotuner在Fused MoE和NVFP4 Linear操作上的应用。

https://github.com/NVIDIA/TensorRT-LLM/issues/3150
这是一个bug报告，涉及主要对象为TensorRTLLM中的SamplingConfig，由于参数设定为`no_repeat_ngram_size=0`导致触发了一个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3149
这个issue是关于如何在更强大的主机上构建TensorRT-LLM引擎并部署到Jetson Orin Nano Super的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3148
这个issue是关于需求反馈的，主要对象是TensorRT-LLM 1.0 Release Planning and API Compatibility Commitment。用户提出了关于1.0版本API兼容性的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3147
这是一个优化性能的 issue，主要涉及到使用 pinned H2D 以减少 CPU 端的阻塞。

https://github.com/NVIDIA/TensorRT-LLM/issues/3146
这是一个bug报告，主要涉及的对象是使用MTP时遇到的一个错误，由于未设置`max_num_draft_tokens`导致dummy请求出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/3145
这个issue类型是功能新增提议，相关对象是PyTorch backend的LogitsProcessor。由于项目需要完善测试和更新公共示例，以及未更新相关内容，导致需要继续进行工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/3144
这是一个bug报告，主要涉及的对象是Blackwell FP8 blockwise gemm kernel，由于缩放因子布局不匹配导致了一些断言可能放错位置的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3143
这是一个用户提出需求的类型，该问题涉及Gemmas 3支持的情况。由于Gemmas 3尚未得到支持，用户提出询问何时将其支持的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3142
这是一个用户提出需求的issue， 主要对象是Executor API，由于缺乏获取吞吐量的相关信息，用户无法通过get_latest_iteration_stats()方法获得正确的数据。

https://github.com/NVIDIA/TensorRT-LLM/issues/3141
这是一个bug报告，涉及主要对象是测试列表中使用文件名。原因是不支持在测试列表中使用文件名，用户提出了该功能需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3140
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于无法在Slurm上正常退出导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3139
这个issue类型为“功能需求”，主要涉及对象是针对TensorRT-LLM中DecoderState的重构。由于需要通过绑定暴露DecoderState并集成到TRTLLMDecoder中，用户可能提出了对DecoderState在代码中的结构和功能进行优化的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3138
这个issue属于功能需求类型，涉及的主要对象是TensorRT-LLM的API。由于目前LLM API尚未支持prompt lookup speculative decoding，用户提出了关于此功能缺失的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3137
这是一个bug报告，主要涉及TensorRT-LLM中的lookahead decoding和multimodal input支持的相关问题。由于特定条件下引发错误，可能是参数设置不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3136
这是一个与Pull Request相关的问题，用户请求撤销 PR #3053。

https://github.com/NVIDIA/TensorRT-LLM/issues/3135
这是一个用户提出需求的issue，主要对象是为TensorRT-LLM添加新文档，由于缺乏这些文档，用户希望添加关于使用`trtllmllmapilaunch`在Slurm或其他MPI系统上的示例文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/3134
这是一个优化性能的issue，主要涉及TensorRT-LLM中的PP和attention DP，由于同步请求完成点和移除MPI world broadcast导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3133
这个issue是一个功能特性提议，主要涉及Variable-Beam-Width-Search（VBWS），目的是实现VBWS支持的第二部分。

https://github.com/NVIDIA/TensorRT-LLM/issues/3132
这是一个代码清理（chore）类型的issue，涉及到TensorRT-LLM项目中的py_executor部分的代码调整。

https://github.com/NVIDIA/TensorRT-LLM/issues/3131
这是一个Bug报告，涉及的主要对象是TensorRT-LLM模型，由于参数设置不当导致模型输出无法正常停止。

https://github.com/NVIDIA/TensorRT-LLM/issues/3130
这是一个关于需求提出的issue，主要对象是TensorRT-LLM下的KV Cache Offload功能。由于用户想要强制TensorRT仅使用CPU进行KV Cache，以便比较在GPU和CPU上KV Cache对吞吐量/延迟的影响。

https://github.com/NVIDIA/TensorRT-LLM/issues/3129
这个issue类型是基础设施改进（非bug报告），主要涉及的对象是GitHub actions的配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/3128
这是一个功能增强（feature enhancement）类型的issue，主要涉及的对象是在TensorRT-LLM中添加对Cohere2ForCausalLM架构的支持。造成这个issue的原因可能是某些功能尚未完成，如无法从模型配置加载正确的滑动窗口配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/3127
这是一个特性增强的issue，主要涉及到在Cohere模型中添加FP8支持的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3126
这个issue属于更新类型，涉及主要对象为libcutlass library。由于NVIDIA在内部仓库上进行的更改，在TensorRT-LLM中需要同步更新libcutlass库的内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/3125
这是一个关于模型质量问题的Bug报告，用户遇到了使用ReDrafter生成的文本质量明显下降的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/3124
这是一个用户提出需求的类型，主要对象是TensorRT-LLM团队。由于团队希望组织在线见面会话，他们正在寻求社区成员提供讨论话题和反馈意见。

https://github.com/NVIDIA/TensorRT-LLM/issues/3123
这个issue是关于bug报告，主要涉及CUDA Device Binding Runtime Error，由于系统配置错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/3122
这是一个关于修改trtllm-bench CLI中批量调度策略的问题，用户提出是否成功应用了新的调度策略。

https://github.com/NVIDIA/TensorRT-LLM/issues/3121
这是一个用户提出问题类型的issue，主要涉及到在TensorRT中实现query和value具有不同隐藏维度的注意力机制。用户询问如何在导出注意力层时处理这种情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/3120
该issue是一个基础设施更新类型的问题单，涉及到并发控制，可能是由于多个操作同时访问导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3119
这个issue是一个bug报告，涉及TensorRT-LLM的mgmn组件，由于某种原因导致了使用trtllm-llmapi-launch命令时程序hang住的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3118
这是一个bug报告，用户遇到在运行Deepseek R1模型时出现的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3117
该issue类型为技术改进（chore），主要涉及的对象是内部核心库的ABI边界稳定性，问题由于需要对内部核心库的ABI边界进行稳定性优化。

https://github.com/NVIDIA/TensorRT-LLM/issues/3116
这是一个bug报告，主要涉及TensorRT-LLM下的CUDA图大小排列错误导致图大小频繁波动，使总图大小变大的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3115
这是一个用户提出需求的issue，主要涉及到AutoDeploy中RoPE支持的增强。由于transformers.models.llama.modeling_llama.apply_rotary_pos_emb存在两种变体，目前仅支持其中一种，用户可能提出了对第二种变体的支持需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3114
该问题属于Feature请求，主要涉及Rocky容器。导致这个问题的原因是需要安装sqlite才能在Conan中使用pip。

https://github.com/NVIDIA/TensorRT-LLM/issues/3113
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的find_library_create_target() cmake macro。由于目前该macro继续执行即使找不到库，导致在后续构建过程中出现难以理解的构建失败，这个issue提出了修复这一问题的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3112
这是一个bug报告，主要涉及到Triton Docker容器中import tensorrt_llm时出现的错误。原因可能是mlx5设备连接失败导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3111
这个issue属于bug报告，主要涉及.devcontainer文件指向不可公开访问的内部Docker镜像，导致外部开发人员无法访问。

https://github.com/NVIDIA/TensorRT-LLM/issues/3110
这是一个空issue，类型为无效issue，涉及主要对象未提及，可能是因为用户未填写任何内容导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/3109
这是一个bug报告，主要涉及到TensorRT-LLM下的find_library_create_target() cmake macro，由于当前的实现在没有找到库时仍然继续，导致后续构建失败，用户希望在该macro中添加合适的错误信息提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/3108
这是一个bug报告，用户提出了关于如何使用TensorRT-LLM在H200上达到150 TPS的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3107
这个issue是关于提出需求的类型，主要对象是增加机器人帮助功能。这个问题的原因是缺乏机器人帮助功能，用户需求更方便和快速的交互方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/3106
这是一个类型为bug报告的issue，涉及的主要对象是TensorRT-LLM中的C++ decoders。原因是因为在disaggregated serving方面仍然存在问题，所以C++ decoders尚未被默认使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/3105
这是一个bug报告，主要涉及对象是TensorRT-LLM中的QWenConfig对象。这个问题的症状是 AttributeError: 'QWenConfig' object has no attribute 'seq_length'，可能是由于代码中缺少对'seq_length'属性的定义或者赋值导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3104
这个issue类型是功能增强（feature enhancement），涉及的主要对象是TensorRT-LLM中的MoE（Mixture of Experts）模块，由于MoE workspace size很大导致GPU内存不足，故提出将MoE输入拆分为多个块来减少GPU内存使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/3103
这个issue类型是重构请求，主要涉及DecoderState对象的属性调整，由于需要简化disableLookahead和改进numDecodingEngineTokens处理。

https://github.com/NVIDIA/TensorRT-LLM/issues/3102
这是一个性能优化的问题，涉及TensorRT-LLM API，由于当前性能不佳，用户希望对API进行优化。

https://github.com/NVIDIA/TensorRT-LLM/issues/3101
这个issue是一个技术改进性质的提交，涉及到移除TensorRT-LLM中与MPI相关的依赖，原因是为了减少对MPI的依赖性。

https://github.com/NVIDIA/TensorRT-LLM/issues/3100
这是一个CI相关的问题，需要豁免一个链接为https://nvbugspro.nvidia.com/bug/5189673的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3098
这是一个关于GitHub PR文件更改支持的功能需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3097
这是一个改进性质的issue，主要涉及到测试列表名称检查的添加。原因可能是为了增强代码质量和测试覆盖率。

https://github.com/NVIDIA/TensorRT-LLM/issues/3096
这是一个功能需求类型的issue，主要对象是TensorRT-LLM中的MTP，用户提出需要添加对MTP+cuda_graph_padding=True的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/3095
这个issue是关于bug报告的，主要涉及TensorRT-LLM中的context server和KV Cache。原因是在某些情况下，context会因为等待所有inflight context传输完成而hang，导致了这个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3094
这个issue是针对自动从waives.txt中移除已关闭的bug的类型为自动问题处理，主要涉及的对象是测试中发现的已经关闭的 bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3093
这是一个用户要求性能优化的问题，主要涉及TensorRT-LLM中的DeepSeek功能。由于目前的最小延迟模式下DeepSeek表现不佳，用户希望增加一些优化来提升性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/3092
这个issue是关于bug报告，主要涉及的对象是TensorRT-LLM中的kv_cache_manager。由于之前的内存使用测试不准确，导致新增的API更新kv_cache_manager时可能会出现CacheTransceiver hang的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3091
这是一个需求类型的issue，主要涉及Blossom debug hook功能。由于开发者可能需要在调试过程中添加某些特定的调试挂钩，因此提出了该需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3090
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的测试集集成，由于错误的testdb上下文导致了空的测试列表输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/3089
这是一个用户提出需求的类型，主要对象是代码中的一些修改。这个issue可能由于某些重要的改动或功能需求导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/3088
该issue类型为需求提出，主要对象是CI测试。这个问题的原因可能是开发者希望将一些重要的变化合并进项目中。

https://github.com/NVIDIA/TensorRT-LLM/issues/3087
这是一个bug报告，主要涉及到在benchmarking distserving时出现返回HTTP 500错误的问题，可能是由于服务器返回非200 HTTP状态码导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/3086
这是一个用户提出需求类型的issue，涉及的主要对象为CI demo。由于可能有重要且优秀的更改，用户建议将这些更改合并到项目中。

https://github.com/NVIDIA/TensorRT-LLM/issues/3085
这是一个特性需求，用户提出需要在PyTorch工作流中加入不带缓存的注意力机制。

https://github.com/NVIDIA/TensorRT-LLM/issues/3084
这是一个bug报告，涉及到TensorRT-LLM中scaffolding shutdown过程的优化。问题由于避免死锁而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/3083
这是一个迁移模型并清理CI测试的维护类型的issue，涉及的主要对象是代码库中的模型和持续集成测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/3082
这是一个功能特性新增（feat）类型的issue，主要涉及TensorRT-LLM中引入Variable-Beam-Width-Search（VBWS），用户提出了对于beam search进行批处理时对beam width进行动态调整的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3081
这是一个升级维护类的Issue，涉及主要对象是将nvrtc_wrapper替换为Conan packages。缺乏sqlite3 python模块导致无法通过pip安装Conan，解决方案是从官方Conan GitHub发布中安装独立的Conan可执行文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/3080
这是一个维护类型的issue，涉及的主要对象是TensorRT-LLM。由于Transformers在处理过程中已经扩展了input_ids，因此在TRTLLM代码中不再需要进行这样的操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/3079
这是一个特性新增的issue，涉及Phi3/4模型系列的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/3078
这个issue是关于代码重构，并不是bug报告。主要涉及到将`DecoderFinishedEvent`替换为`CudaEvent`，以简化事件处理，并提高解码流程的清晰性和效率。

https://github.com/NVIDIA/TensorRT-LLM/issues/3077
这个issue类型是需求提升，涉及主要对象为decoder finalize函数的重构。由于未提供具体的变更细节，用户可能需要查看提交详情来了解改动的具体内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/3076
这是一个用户提出需求的类型，主要涉及 GptDecoderBatched 的前向方法的简化。由于代码复杂性或不便使用等原因，用户想要简化这些前向方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/3075
这是一个用户提交需求改进的issue，主要涉及的对象是项目的开发容器（devcontainer），提出的问题是关于提高开发环境的生产力和便利性。

https://github.com/NVIDIA/TensorRT-LLM/issues/3074
这个issue类型是更新CI allowlist，主要涉及的对象是持续集成系统，用户可能遇到无法通过CI检查的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3073
该issue属于改进需求类型，涉及到TensorRT-LLM下的disaggregated serving脚本的重构。此问题主要是为了简化disaggregated serving部署并减少重复代码，提出了启动disaggregated workers和server的新方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/3072
这是一个用户提出需求的issue，主要涉及修改README.md文件来更新支持矩阵和添加切换列表。

https://github.com/NVIDIA/TensorRT-LLM/issues/3071
这是一个用户提出需求的issue，主要涉及的对象是开源项目TensorRT-LLM。由于该需求提出了开源fp8_blockscale_gemm功能的请求，说明用户想要对该功能进行开源。

https://github.com/NVIDIA/TensorRT-LLM/issues/3070
这是一个用户提出的需求。该问题涉及要添加请求带宽测量功能。这个需求的提出可能是为了更好地了解系统中每个请求的带宽使用情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/3069
这是一个用户提出需求的issue，该问题涉及的主要对象是API stability references，由于缺乏1.0 criteria scope的创建，用户提出了相关需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3068
这是一个用户提出需求的issue，主要对象是为TensorRT-LLM添加LoRA支持的gemma。

https://github.com/NVIDIA/TensorRT-LLM/issues/3067
这是一个bug报告，主要涉及消息在不同进程上不对齐的问题，导致了融合消息错误的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/3066
这个issue属于bug报告类型，主要涉及的对象是twoshot。原因可能是twoshot导致了准确率问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3065
这是一个功能需求，主要涉及到对UCX支持的缓存传输器进行功能性扩展。

https://github.com/NVIDIA/TensorRT-LLM/issues/3064
这是一个特性开发的issue，主要涉及TensorRT-LLM下的allreduce和fusion kernel的开发。原因可能是新增功能或者对现有功能进行改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/3063
这个issue类型是需求提出，主要涉及的对象是DeepSeek模型。用户提出了添加gpqa准确性测试脚本和测试的需求，并更新了相应文档和测试清单。

https://github.com/NVIDIA/TensorRT-LLM/issues/3062
这是一条关于标记为flaky的测试的issue，属于Bug报告类型，主要对象是test_kv_cache_event_async_api。这个问题可能是由该测试在某些情况下出现不稳定的行为，导致测试结果不确定。

https://github.com/NVIDIA/TensorRT-LLM/issues/3061
这是一个需求更改类型的issue，主要涉及将GPTJ特征测试移至LLaMA模型。由于可能需要整合或调整测试框架，所以作出了这个改动。

https://github.com/NVIDIA/TensorRT-LLM/issues/3060
这个issue类型是用户提出需求，主要涉及文档中如何在本地运行CI阶段，用户希望了解如何在本地运行CI脚本。

https://github.com/NVIDIA/TensorRT-LLM/issues/3059
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的测试案例fp4_gemm，由于其中提到的bug编号nvbugs/5100633导致了某些情况下无法执行相关测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/3058
这是一个bug报告，该问题涉及到DeepSeekR1FP4模型在B200上的部署和推理速度问题。导致这个问题可能是由于部署过程中遇到的技术限制或配置问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3057
这是一个删除测试用例的issue，类型为功能调整，涉及主要对象为test_llm_gptj_fp8_manage_weights测试用例。原因可能是该测试用例不再需要，或者需要进行修改调整。

https://github.com/NVIDIA/TensorRT-LLM/issues/3056
这是一个bug报告，涉及的主要对象是测试列表。由于测试列表需要更正，导致了需要修复的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3055
这是一个bug报告，主要涉及的对象是LLama-3.2-11b-vision模型，用户添加了一个随机图像测试以解决多模态交叉注意力结果不正确的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3054
这是一个功能增强的issue，涉及主要对象为TensorRT-LLM中的模型管理。原因是为了增加新发布的EXAONE-Deep模型，并更改默认目录以匹配新模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/3053
这是一个 Bug 报告，主要涉及的对象是 TensorRT-LLM， 提issue者提示在 torch 编译中使用 MTP 时需要设置正确的 draft_token_nums。

https://github.com/NVIDIA/TensorRT-LLM/issues/3052
这是一个用户提出需求的issue，主要涉及更新DeepSeekV3文档，因为需要包含flashMLA和DeepGEMM的信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/3051
这个issue是一个升级库版本的任务，主要涉及到的对象是项目中的transformers库。

https://github.com/NVIDIA/TensorRT-LLM/issues/3050
这是一个bug报告，主要涉及测试脚本中的错误设置导致不必要的依赖安装问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3049
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的allreduce CUDA图。这个问题产生的原因是allreduce kernel在启用cuda图时可能导致结果错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3048
这是一个需求类型的issue， 主要对象是"doc"。由于缺少README.md文件，用户希望添加用于生成模板的说明文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/3046
这是一个功能增强类型的 issue，涉及主要对象是支持从 ModelOpt 导入 prequantized FP8 ckpt 到 nemotron-mini-4b-instruct 模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/3045
这是一个功能请求，涉及支持 FP4 量化中的线性块模式布局。

https://github.com/NVIDIA/TensorRT-LLM/issues/3044
这是一个用户提出需求的issue，主要涉及Pytorch PP + attention DP支持。由于缺乏此功能，用户请求添加对应支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/3043
这是一个需求修改类型的issue，主要涉及TensorRT-LLM下的llava模块，用户可能希望添加第二个可能的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/3042
这是一个bug报告，涉及主要对象是vila test。由于NVBUG 5087143的问题导致vila test出现bug，需要修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/3041
这是一个功能增强类型的issue，涉及主要对象为TensorRT-LLM中的AutoDeploy功能。 

https://github.com/NVIDIA/TensorRT-LLM/issues/3040
这是一个优化建议，主要涉及到使用TensorRT-LLM中的通信方式，建议在rank 0节点上使用`gather`而不是`allgather`。

https://github.com/NVIDIA/TensorRT-LLM/issues/3039
这个issue类型属于功能增强（feature enhancement），主要涉及到TensorRT-LLM下的trtllm-bench工具的迭代日志记录。这个功能增强的需求可能是为了更详细地统计每次迭代的性能数据。

https://github.com/NVIDIA/TensorRT-LLM/issues/3038
这是一个bug报告，涉及主要对象为trtGptModelInflightBatching.h，由于之前的重构移动了一些方法但没有删除对应的定义，导致了描述中提到的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3037
该issue属于用户提出需求类型，主要涉及TensorRT-LLM中关于NVFP4量化的权重和激活范围校准的问题。导致该问题的原因是用户对于NVFP4量化中的激活范围校准和全局尺度的理解不清晰。

https://github.com/NVIDIA/TensorRT-LLM/issues/3036
这是一个chore类型的issue，主要涉及到TensorRT-LLM的功能参数移动的问题，由于与Torch后端共享的参数需要被移到LLMArgs中，导致需要进行相应调整。

https://github.com/NVIDIA/TensorRT-LLM/issues/3035
这是一个功能更新的issue，主要涉及TensorRT-LLM中EAGLE-3的初始实现。由于CUDA graphs未与EAGLE3兼容，导致特定性能差异，用户需要进行比较与benchmark测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/3034
这是一个用户提出需求的issue，涉及的主要对象是LlmRequest、KVCacheManager和Scheduler。由于pybind调用比纯Python调用慢，所以决定重新实现LlmRequest和Scheduler。

https://github.com/NVIDIA/TensorRT-LLM/issues/3033
这是一个文档更新类的issue，主要涉及到项目的贡献者。由于可能相关文档需要更新或补充，导致用户提出这个issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/3032
这是一个类型为功能改进（Feature Improvement）的issue，涉及主要对象为TensorRT-LLM中的AllReduce custom op。原因可能是为了统一Module和custom op级别的两个版本的AllReduce操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/3031
这是关于软件部署中的bug报告，主要涉及TensorRT-LLM引擎文件在不同GPU架构上导致的不兼容性问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3030
这个issue是关于文档更新的请求，主要对象是TensorRT-LLM下的internal cutlass library和nvrtc_wrapper。

https://github.com/NVIDIA/TensorRT-LLM/issues/3029
这个issue类型是优化建议，针对的主要对象是CI pipeline执行过程中多次检出Git源代码的重复操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/3028
这个issue类型属于功能增强，主要涉及TensorRT-LLM中的内存分配问题，由于需要为每个窗口大小分配最小的内存块，可能出现了内存管理方面的症状或者用户提出了此需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3027
这个issue是关于改进多GPU测试的问题，涉及CI测试的优化。

https://github.com/NVIDIA/TensorRT-LLM/issues/3026
这是一个用户提出需求的 issue，主要涉及的对象是 LoRa 模块。

https://github.com/NVIDIA/TensorRT-LLM/issues/3025
这个issue属于任务类型，涉及的主要对象是TensorRT-LLM下的LlmArgs。由于对LlmArgs进行重构和迁移，导致了API稳定性和错误提示方面的改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/3024
这是一个功能改进的issue，主要涉及到TensorRT-LLM下的stateful decoders的重构。通过简化构造函数和设置方法签名，去除了一些冗余参数，以提高代码的简洁性和易用性。

https://github.com/NVIDIA/TensorRT-LLM/issues/3023
这是一个维护类型的issue，涉及到移除无用参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/3022
这个issue属于改进类型，主要涉及构建脚本中的git lfs操作，导致在未执行`git lfs pull`时报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/3021
这是一个bug报告，涉及主要对象是TensorRT-LLM中的KV cache。这个问题可能由于使用attention sink时出现了KV cache重用问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3020
这个issue类型是功能增强（feature enhancement），主要涉及TensorRT-LLM下的cos_sin_cache支持更新。由于需要优化rope实现以及改善用户体验，需要添加新的功能并做性能优化。

https://github.com/NVIDIA/TensorRT-LLM/issues/3019
这是一个版本更新的问题，涉及的主要对象是TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/3018
这个issue属于Bug报告，主要对象是TensorRT-LLM的dataset generator。由于文件路径问题导致的bug，无法确定父目录导致输出文件创建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/3017
这是一个bug报告，涉及主要对象是TensorRT-LLM中的cudaDriverWrapper，因为动态加载库中不存在的符号导致了segfault，隐藏了内核启动失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3016
这是一个优化类型的issue，主要涉及TensorRT-LLM项目的代码导入方式。由于当前使用的绝对导入方式，用户建议替换为相对导入方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/3015
这是一个代码重构（chore）类型的issue，主要涉及到TensorRT-LLM下的_torch模块的import重构。根据描述，可能是为了提高代码可维护性和移植性而进行的变更。

https://github.com/NVIDIA/TensorRT-LLM/issues/3014
这个issue是一个功能改进类型，主要涉及TensorRT-LLM项目中的trt engine构建时间问题。这个问题是由于autotune的默认最大tactic数量设置导致测试时间过长引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/3013
这是一个bug报告，主要涉及TensorRT-LLM下的测试功能。由于设置了`ONLY_PYTORCH_FILE_CHANGED`为true，导致禁用了`CPP`和`TensorRT`阶段，可能还会强制启用多GPU测试阶段。

https://github.com/NVIDIA/TensorRT-LLM/issues/3012
这个issue为功能新增（feature addition），主要针对条件分解测试（conditional disaggregation test），用户需要添加该功能以便进行相关测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/3011
这是一个bug报告，主要涉及的对象是TensorRT-LLM项目中的confset.py文件，发现了模型路径问题导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3010
这个issue是一个性能优化类型的问题，主要对象是CUDA graphs以及在不同GPU上活动请求不均匀。由于活动请求不均匀会导致无法使用CUDA图形，并且引起性能问题，所以提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/3009
这个issue属于feature功能更新，主要涉及TensorRT-LLM下logits bitmask kernel的更新，由于采用了新的kernel版本(v3)，在不同场景下带来了不同的性能影响。

https://github.com/NVIDIA/TensorRT-LLM/issues/3008
这是一个bug报告，主要涉及的对象是MLA（Machine Learning Accelerator）的fp8 MLA kernels在Blackwell上的bug。原因可能是由于在Blackwell平台上存在某种特定情况下的错误导致了bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/3007
该issue属于bug报告类型，涉及的主要对象为TensorRT-LLM下的trtllm-bench工具。由于world_size小于device_count时，在设定gpus_per_node参数时出现问题，导致程序运行不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/3006
这是一个bug报告，涉及主要对象为TensorRT-LLM。原因是由于mtp eagle模式导致的非法内存访问。

https://github.com/NVIDIA/TensorRT-LLM/issues/3005
这是一个特性更新（feature update）的issue，主要涉及到TensorRT-LLM中的allreduce benchmark。这个更新是为了将当前的allreduce benchmark从TRT流程改为PyTorch流程，同时支持cuda图形和norm融合。

https://github.com/NVIDIA/TensorRT-LLM/issues/3004
这是一个用户提出需求的类型，主要对象涉及到MLA FP8 KV Cache on Blackwell。由于目前不支持fp8 kv cache on blackwell，用户提出了添加该支持的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/3003
这是一个bug报告，主要涉及的对象为TensorRT-LLM中的路径问题。由于文件路径发生变化，导致在安装示例依赖时出现文件找不到的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/3002
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM中的MQA模块。这个问题的原因是当cp_size * tp_size的乘积大于kv_head_num时，kv_head_num会变为0，导致了bug出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/3001
这是一个增加流量限制功能的任务类型的issue，主要对象是GitHub上的TensorRT-LLM项目。

https://github.com/NVIDIA/TensorRT-LLM/issues/3000
这是一个用户提出需求的issue，主要对象是PyTorch flow，可能由于现有快速启动流程过于复杂而希望简化的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/2999
这是一个类型为"维护性任务"（chore）的issue，主要涉及的对象是模型 API 示例，由于在维护过程中决定移除示例和对应的端到端测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/2998
这个issue类型为bug报告，涉及的主要对象是测试程序，由于等待很长时间导致了测试的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2997
这是一个功能增强（feature enhancement）类型的issue，涉及的主要对象是LlmArgs。由于用户希望添加PeftCacheConfig和SchedulerConfig等纯Python配置，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2996
这个issue是关于重组测试文件夹层次结构的，属于重构改进类问题，主要涉及到测试工作流单元，旨在整理测试文件夹结构并改善一些单元测试代码的导入使用方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/2995
这个issue是一个功能请求，主要对象是TensorRT-LLM的UB AR NORM FP16/BF16模块，用户提出了添加一次性版本的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2994
这是一个更新审批人列表的类型。该问题单涉及的主要对象是项目的审批人列表。原因可能是需要更新或调整项目的审批流程。

https://github.com/NVIDIA/TensorRT-LLM/issues/2993
这是一个bug报告，涉及到TensorRT-LLM中的requirements.txt相对路径问题。这个问题因为最近将requirements.txt移动位置导致路径不再有效，需要更新路径或调整文件结构。

https://github.com/NVIDIA/TensorRT-LLM/issues/2992
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的setuptools模块。产生这个问题的原因是setuptools版本77.0.1存在bug，导致需要更新到最新发布的版本77.0.3来修复问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2991
该issue属于测试添加类别，主要涉及Eagle支持未训练头部模型的单元测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/2990
这个issue属于功能新增类型，主要涉及的对象是Phi-4-mini模型。由于用户希望在TensorRT-LLM中添加对Phi-4-mini模型的支持，因此提出了这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2989
这是一个bug报告，主要涉及TensorRT-LLM下的synthetic data generation和driver wrapper的问题。这个问题导致了程序在特定情况下出现了segfault错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2988
这是一个测试添加类型的issue，主要涉及的对象是测试数据集和模型支持情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2987
该issue类型为功能性改进，涉及删除旧模型的测试用例。

https://github.com/NVIDIA/TensorRT-LLM/issues/2986
这是一个优化改进类型的issue，涉及主要对象为PD（可能为某个API或模块），旨在优化生成的token的返回逻辑。

https://github.com/NVIDIA/TensorRT-LLM/issues/2985
该issue类型为功能增强（feature enhancement），主要对象为TensorRT-LLM中的chat completion功能。原因可能是缺乏此功能导致用户无法进行对话完成。

https://github.com/NVIDIA/TensorRT-LLM/issues/2984
这是一个功能增强类型的issue，主要涉及TensorRT-LLM中的KV缓存传输度量，用户添加了一种精细化的KV缓存传输度量，并提供了设置环境变量来输出指标的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2983
这是一个bug报告，主要对象是TensorRT-LLM中的Sliding Window Attention (SWA)功能。这个问题由于实现逻辑与kv缓存重用机制冲突，导致在VSWA情况下不能正确判断何时禁用缓存重用功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2982
这是一个功能改进类的issue，主要针对Accuracy test suite的改进。原因可能是为了提高测试的准确性和可靠性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2981
这个issue类型是功能更新，主要涉及的对象是cutlass库，由于需要更新cutlass库来改进TensorRT-LLM的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2980
这个issue属于用户提出需求类型，主要对象是贡献指南(CONTRIBUTING.md)，由于缺少具体描述，用户可能需要更新TensorRT-LLM项目的贡献指南，为第一个PR做准备。

https://github.com/NVIDIA/TensorRT-LLM/issues/2979
这是一个用户提出需求的类型，主要对象是文档更新。由于内容为空，用户可能希望更新文档内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2978
这是一个缺少具体内容的问题跟踪报告issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/2977
这是一个bug报告，主要涉及对象是TensorRT-LLM构造函数中的条件检查。这个问题发生的原因是构造函数中对dynamicTreeMaxTopK参数的验证与useDynamicTree参数的关联性不严谨，可能导致潜在的错误配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/2976
这是一个bug报告，主要涉及TensorRT-LLM下的trtllm-build命令出现内存不足问题导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2975
这个issue类型是维护性修改（chore），涉及主要对象为Clangd，由于编译命令默认路径需要与`build_wheel.py`保持一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2974
这是一个Bug报告，主要涉及对象是TensorRT-LLM中Gemma2在Jetson AGX Orin平台上的支持问题。原因可能是代码转换过程中出现了错误导致了报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2973
这是一个CI测试的issue，主要对象是构建自动化测试系统，原因是进行提交触发了CI测试，目的是验证代码变更正确性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2972
这个issue类型是CI测试问题，涉及主要对象是TensorRT-LLM。由于某些原因导致CI测试无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2971
这是一个bug报告，该问题涉及TensorRT-LLM。原因可能是代码中存在错误导致了bug的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/2970
这个issue属于用户提问类型，主要涉及TensorRT-LLM下的GPT模型参数删除导致的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2969
这个issue是关于修复setuptools bug的bug报告，主要涉及的对象是TensorRT-LLM。由于出现了setuptools的bug，导致了安装或者构建过程中出现错误，用户希望修复这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2968
这是一则关于bug报告的issue，涉及主要对象为Deepseek R1生成乱码字符。由于某种原因导致在H20机器上使用pytorch后端测试Deepseek R1时，生成了乱码字符。

https://github.com/NVIDIA/TensorRT-LLM/issues/2967
这是一个用户提出需求的问题，主要关注TensorRT-LLM的支持情况，希望实现模型在多个GPU上的分布式部署来提高推断速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/2966
这是一个用户提出需求的issue，主要对象是软件仓库中的 requirements.txt 文件。可能是由于软件版本更新或者新增依赖导致的需求更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2965
这是一个需要移除示例的issue，主要对象是测试Deepseek v1的例子。

https://github.com/NVIDIA/TensorRT-LLM/issues/2964
这个issue类型是用户提出需求，主要涉及到请求复制DeepSeek-R1在H200和B200上的配置，用户希望获得在特定硬件上复现详细配置以达到博客中展示的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2963
这是一个类型为bug报告的issue，主要涉及的对象是TensorRT-LLM。根据标题为空内容，可能是用户意外创建了一个空的issue导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2962
这个issue类型是文档修正，主要涉及LLM API logits processor example的注释。原因可能是之前的注释有误或不清晰。

https://github.com/NVIDIA/TensorRT-LLM/issues/2961
这个issue类型是CI测试问题，主要涉及的对象是TensorRT-LLM。 由于CI测试流程中的某些原因，导致测试未通过或者出现问题，需要进行修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/2960
这是一个用户提出需求的类型，主要涉及的对象是项目的路线图（roadmap）。由于路线图可能过时或不清晰，用户请求更新以获取最新信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2959
这是一个bug报告，该问题涉及的主要对象是Test failed tests。由于测试失败，用户在该issue中提出了相关问题或寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2958
这个issue类型属于CI测试失败，涉及主要对象为TensorRT-LLM项目。这种情况通常由于代码更改或环境配置问题导致自动化测试失败，需要开发人员查找和解决问题以保证代码质量。

https://github.com/NVIDIA/TensorRT-LLM/issues/2957
这是一个类型为Bug报告的GitHub issue，主要涉及的对象是测试Kill。原因可能是测试中出现了无法kill的问题，导致测试无法执行完成。

https://github.com/NVIDIA/TensorRT-LLM/issues/2956
这是一个bug报告，涉及测试多GPU功能，可能由于多GPU环境下的测试问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2955
这是一个bug报告类型的issue，主要涉及Test failed tests。原因可能是测试未通过，导致出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2954
这是一个空issue，无法确定是什么类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2953
这是一个bug报告，主要涉及TensorRT-LLM下的分布式推理工具trtllmbench在多节点场景下使用时出现"double free detected in tcache 2"错误。原因可能是由于内存管理问题导致的重复释放内存引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2952
这是一个bug报告，主要涉及对象是TensorRT-LLM中的convert_checkpoint.py脚本，可能是由于数据类型错误导致无法成功进行模型转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/2951
这是一个测试类型的issue，涉及到自动运行测试功能。由于未提供具体内容，无法确定导致的问题或需要解决的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2950
这是一个标题为"Test"的空issue，类型未知，主要对象未提及，无法确定问题来源。

https://github.com/NVIDIA/TensorRT-LLM/issues/2949
这是一个类型为文档/注释改进的 issue，主要关注于TensorRT-LLM项目中的 license comment 中嵌入的 reference URL 问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2948
这是一个bug报告，主要涉及到TensorRT-LLM下的l0 pipeline，可能由于代码逻辑错误或者功能不完善导致了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2947
这是一个文档增强类型的问题，主要关注于TensorRT-LLM的资料整理和补充。

https://github.com/NVIDIA/TensorRT-LLM/issues/2946
这是一个bug报告，问题涉及状态测试。导致此问题可能是由于代码实现不正确或环境配置问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2945
这是一个功能需求提出的issue，主要涉及NeMo的conformer encoder-transformer decoder模型（canary1b和canary1b flash）的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2944
这是一个测试相关的issue，具体对象未提及清楚。由于Ci测试出现问题，导致可能无法正常进行持续集成测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/2943
这个issue属于更新工作流程的需求类型，主要涉及TensorRT-LLM项目的工作流程控制。

https://github.com/NVIDIA/TensorRT-LLM/issues/2942
这是一个bug报告类型的issue，主要涉及的对象是bot testing。由于issue内容为空，用户可能在尝试进行bot testing时遇到了问题或者寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2941
这是一个空白的issue，类型可能为bug报告，针对的主要对象是TensorRT-LLM。由于未提供具体内容，无法分析导致了何种bug症状或用户提出了何种问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2940
这是一个文档更新的需求，涉及主要对象是项目的文档，可能是由于文档内容过时或者不准确导致用户需要更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2939
这是一个用户提出需求的issue，主要涉及到测试机器人和持续集成。这个问题可能是由于测试机器人和持续集成系统在项目中尚未完全集成或配置正确而引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2938
这是一个CI（持续集成）相关的issue，主要对象是测试机器人（Test Bot）。由于该issue的标题为"[NOT MERGE]Test Bot and CI"，推测可能是关于持续集成环境中测试机器人或者CI流程的问题，需要进行排查与修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/2936
这个issue是关于更新TensorRT-LLM的，类型为功能需求提升。主要涉及的对象是TensorRT-LLM的功能和性能优化。由于新增功能支持和内存优化，带来了新增功能和性能优化方面的更新和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/2935
这个issue属于文档更新类型，主要对象是TensorRT-LLM项目的文档。由于文档内容过时或不完整，用户提出了需要更新文档的帮助请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2934
这是一个缺失具体内容的issue，无法确定其类型或描述。

https://github.com/NVIDIA/TensorRT-LLM/issues/2933
这是一个标题为"Test"的bug报告，该问题单涉及的主要对象是TensorRT-LLM代码库。由于issue内容为空，无法确定具体问题描述。

https://github.com/NVIDIA/TensorRT-LLM/issues/2932
这个issue是关于用户提出需求的类型，主要对象是TensorRT-LLM下的trtllm-serve，用户询问是否trtllmserve能自动启用前缀缓存功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2931
这是一个用户请求更新GitHub工作流程的issue，主要对象是TensorRT-LLM。由于工作流程可能需要改进或更新，用户请求更新以提高工作效率或符合最新规范。

https://github.com/NVIDIA/TensorRT-LLM/issues/2930
这个issue类型是merge请求，涉及主要对象为TensorRT-LLM项目。由于作者提出了一个合并请求测试，想要将更改合并到主分支中。

https://github.com/NVIDIA/TensorRT-LLM/issues/2929
这是一个空内容的Merge request测试issue，类型为技术问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2928
这个issue类型是用户提出需求，该问题涉及的主要对象是R1 671B，由于未提供定量，用户想了解在bs=1情况下，R1 671B使用h200模型的吞吐量是多少。

https://github.com/NVIDIA/TensorRT-LLM/issues/2927
这个issue是由于代码库中的rebase操作导致的问题，属于代码质量问题，主要对象为代码库维护者。

https://github.com/NVIDIA/TensorRT-LLM/issues/2926
这个issue是一个rebase testing的类型，主要涉及到代码的修改和测试，可能由于代码冲突或者测试结果不符合预期导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2925
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的Qwen MOE模块。由于Qwen MOE不支持FP8，导致用户无法使用该数据类型进行相关操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2924
这个issue是一个bug报告，主要涉及的对象是TensorRT-LLM，由于压力测试导致的线程11错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2923
这是一个 bug 报告，主要对象是 TensorRT-LLM，在 stress testing 过程中会遇到线程相关的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2922
这个issue属于bug报告类型，主要涉及TensorRT-LLM的stress testing。由于在thread-11中出现了问题，可能是由于线程处理或并发性能方面的bug导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2921
这是一个关于性能测试的bug报告，主要对象是TensorRT-LLM。由于压力测试时出现了线程数量异常的问题，导致产生了"stress testing thread-12"的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2920
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM库中的"stress testing"模块。由于负载压力测试导致的线程问题，可能会导致程序崩溃或者性能下降。

https://github.com/NVIDIA/TensorRT-LLM/issues/2919
这是一个bug报告，主要涉及TensorRT-LLM的stress testing功能，由于线程14的问题导致了某种症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2918
这是一个bug报告，问题涉及的主要对象是TensorRT-LLM。这个问题很可能由于程序中的压力测试导致的线程冲突或竞争条件而产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/2917
这个issue是一个bug报告，主要涉及的对象是stress testing功能。由于并发性测试中使用16个线程导致的问题，用户寻求关于stress testing的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2916
这是一个bug报告，涉及的主要对象是TensorRT-LLM库。原因可能是压力测试导致线程17出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2915
这个issue类型是bug报告，主要对象是stress testing thread-18，由于什么样的原因导致了发生了一个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2914
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM。由于压力测试导致线程19出现异常，用户需求解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2913
这个issue类型是用户提出需求，主要涉及的对象是TensorRT-LLM模型支持，用户关注的是是否会有对phi4和Gemma3模型的支持，以及对于AI领域进展速度慢的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2912
这是一个用户提出需求的issue，主要对象是支持 Cohere Command-A 模型。这是由于该模型采用了独特的架构，导致用户需要帮助来添加该模型到代码库中。

https://github.com/NVIDIA/TensorRT-LLM/issues/2911
这是一个bug报告，主要涉及TensorRT-LLM中的enable_kv_cache_reuse功能，在Qwen L40模型上未观察到预期的时间消耗优化。问题可能是由于配置不正确或遗漏导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2910
这是一个bug报告，涉及主要对象是TensorRT-LLM，用户在进行压力测试时出现了线程问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2909
这个issue是一个bug报告，主要涉及TensorRT-LLM在stress testing时出现的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2908
这是一个bug报告，主要涉及TensorRT-LLM中的stress testing功能。这个issue可能是由于多线程测试导致的程序崩溃或错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2907
这是一个bug报告，该问题涉及的主要对象是TensorRT-LLM。原因是由于压力测试时使用了4个线程，导致了某种类型的问题或错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2906
这个issue属于bug报告类型，主要对象是TensorRT-LLM。由于可能的原因导致了stress testing时线程导致的问题或bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2905
这个issue是一个bug报告，涉及到stress testing thread-6的问题。原因导致了stress testing thread-6出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2904
这个issue是一个bug报告，主要涉及TensorRT-LLM的stress testing thread-7功能。由于某种原因导致了该功能出现了问题或者错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2903
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM。由于压力测试线程过多导致的问题，用户报告了与线程相关的错误或异常行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/2902
这是一个Bug报告类型的Issue，涉及主要对象为TensorRT-LLM。由于压力测试（stress testing）导致的线程问题，可能出现了线程崩溃或死锁等症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2901
该issue是GitHub上的CI验证问题，涉及主要对象为TensorRT-LLM。原因可能是为了进行github CI验证而做的次要更改。

https://github.com/NVIDIA/TensorRT-LLM/issues/2900
这是一个bug报告，主要涉及到代码中添加调试信息的问题。由于缺少具体描述内容，难以确定具体的问题表现或帮助需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2899
这是一个bug报告，涉及的主要对象是代码中的debug信息输出。由于可能调试信息的输出缺失或错误，导致需要在特定线程（thread 3）中进行调试信息测试，以找出问题根源。

https://github.com/NVIDIA/TensorRT-LLM/issues/2898
这是一个bug报告，涉及到调试信息的添加操作。由于缺少具体内容，无法确定具体问题或帮助需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2897
这是一个bug报告，主要涉及Debug消息测试线程1，在代码中添加了错误的消息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2896
这是一个Bug报告，主要涉及Whisper在LLM下批处理功能不支持，导致结果只会输出30秒的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2895
这是一个bug报告，主要对象是代码中的debug信息。由于缺少具体内容，无法确定具体问题导致的bug或者用户需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2894
这个issue类型是bug报告，主要涉及TensorRT-LLM下的模型优化问题。由于enable_kv_cache_reuse和use_paged_context_fmha参数未能显著减少时间消耗，可能是由于参数配置或引擎构建过程中的问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2893
这个issue类型是bug报告，涉及主要对象是TensorRT-LLM下的代码。由于缺少详细信息的debug消息，用户提出了需要添加更多调试信息以解决问题的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2892
该issue类型为用户提出需求，主要对象是机器人。由于用户想要触发机器人运行，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2891
这个issue类型是bug报告，主要涉及的对象是代码中缺少了debug信息的地方。原因可能是开发者忘记添加debug信息导致用户无法准确地定位问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2890
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM项目。由于缺少具体的内容，用户可能在添加debug信息方面遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2889
这是一个bug报告，涉及的主要对象是用户guomingz的ci robust 1。可能由于代码实现问题导致了某种症状的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2888
这是一个用户请求运行bot的类型的问题，该问题涉及的主要对象是项目中的CI流程。由于用户想要触发bot的运行，可能是希望自动执行一些操作或检查，或者触发一些特定的流程。

https://github.com/NVIDIA/TensorRT-LLM/issues/2887
这是一个CI测试的issue，主要对象是Github的CI功能。由于缺乏描述，无法确定具体问题或需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2886
这是一个Bug报告的issue，涉及的主要对象是TensorRT-LLM中使用官方示例构建decoder时发生core dump，可能是由于参数配置不当导致程序崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/2885
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的Encoder Only Executor，用户在使用encoder engine创建一个新的executor并排队请求时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2884
这个issue是一个bug报告，涉及的主要对象是dev-main初始化，可能是由于初始化过程出现问题导致了某种bug或错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2883
这个issue是一个bug报告，主要涉及TensorRT-LLM中Qwen 32B在启用lora时变慢的问题，可能由于lora功能导致推断速度慢8倍。

https://github.com/NVIDIA/TensorRT-LLM/issues/2882
这是一个bug报告，主要涉及TensorRT-LLM的dev-main初始化问题，可能是由于初始化过程中出现的错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2881
这是一个bug报告，主要涉及TensorRT-LLM在使用cudaHostAlloc函数时出现内存不足的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2880
这个issue类型为用户提出需求，该问题涉及的主要对象是添加gemma 3 architecture。这个需求是由于用户希望TensorRT-LLM项目中加入对gemma 3 architecture的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2879
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM下的lookahead speculative decoding在使用gpt2 0.1B时出错，可能是由于版本兼容性或配置问题导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2878
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM和MiniCPM-V2.6模型。用户提出是否有计划适配minicpmv系列模型，可能是因为希望TensorRT-LLM能够支持MiniCPM-V2.6模型，从而获得更好的性能或功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2877
这个issue是关于功能需求的，主要对象是TensorRT-LLM系统中QwQ 32B的支持更新，由于需要增加QwQ 32B的支持，用户提出了更新readme的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2876
这个issue是关于更新`setup.py`中脚本路径的更新维护工作，不是bug报告。主要涉及的对象是项目文件`setup.py`。

https://github.com/NVIDIA/TensorRT-LLM/issues/2874
这是一个bug报告，主要涉及TensorRT-LLM下的Codestral22Bv0.1模型，用户询问Trt是否支持Codestral22B FP8量化，可能是由于Quantize过程中出现的错误导致了该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2873
这个issue是关于更新TensorRT-LLM的，不是bug报告，主要涉及的对象是TensorRT-LLM的功能更新和改进。该更新主要是为了增加了一些新特性和修复了一些问题，旨在提升模型支持和用户体验。

https://github.com/NVIDIA/TensorRT-LLM/issues/2872
这是一个bug报告，涉及主要对象为TensorRT-LLM中的模型转换功能。由于没有设置指定的环境变量导致对EXAONE模型进行转换时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2871
这是一个用户提出需求的issue，主要涉及对象为executor API。用户提出自定义采样器以加快生成速度的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2870
这是一个用户需求问题单，关于DeepSeek R1在TensorRT中的支持问题。关闭问题未提供有效答复可能是由于相关团队尚未提供支持或解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/2869
这是一个bug报告类型的issue，主要涉及到使用TensorRT-LLM API代码无法运行FP4格式的问题。由于架构不支持CUTLASS FP4 GEMM，导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2868
这是一个bug报告，涉及TensorRT-LLM在GB200上运行python benchmark时遇到非法内存访问的问题，可能是由于批量大小为16时引发的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2867
这个issue类型为用户提出需求，询问是否支持internvl2.5，主要涉及对象为TensorRT-LLM。由于缺少internvl2.5支持，用户可能遇到功能无法实现或性能问题，因此提出了询问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2866
这是一个关于并行请求被阻塞的bug报告，主要涉及TensorRT-LLM的max_tokens参数设置。这个问题导致当max_tokens设置为非常大时，其他请求被阻塞。

https://github.com/NVIDIA/TensorRT-LLM/issues/2865
这个issue是一个bug报告，主要涉及TensorRT-LLM下在设置max_tokens参数时出现的并行请求队列处理问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2864
该issue为bug报告类型，涉及主要对象为TensorRT-LLM在双RTX 5090 GPU、WSL2和Docker环境下无法运行。造成这一问题的原因可能是系统配置或软件版本的兼容性问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2863
这是一个bug报告，涉及的主要对象是无法使用Nvidia Triton Server在8xH200上运行Deepseek-R1 Engine的问题。由于TensorRT版本不匹配导致无法成功运行引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/2862
这是一个bug报告类型的issue，主要涉及内容是quantization/README.md文件中的拼写错误。由于存在这些拼写错误，需要修复以提高文档的质量。

https://github.com/NVIDIA/TensorRT-LLM/issues/2861
这个issue类型为需求提出，主要对象是TensorRT-LLM中的技术优化，用户询问关于是否有计划采纳类似DeepEP、EPLB、DeepGEMM、FlashMLA等优化技术，以及未来版本可能考虑的优化方向。

https://github.com/NVIDIA/TensorRT-LLM/issues/2860
这是一个bug报告，主要涉及TensorRT-LLM中回答相同问题时结果相同的问题，可能是由于模型在连续输入相同问题时返回相同答案的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2859
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM for Qwen2.5-VL模型，用户想要发布更新版本以支持Qwen2.5VL模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2858
这是一个用户提出需求的issue，主要涉及的对象是测试用例的使能。用户想要启用名为l0-test.yml的测试用例。

https://github.com/NVIDIA/TensorRT-LLM/issues/2857
这是一个bug报告，主要涉及对象是TensorRT-LLM中的CUDA运行时错误，由于内存耗尽导致生成logits时发生运行时错误并无法回到正常状态。

https://github.com/NVIDIA/TensorRT-LLM/issues/2856
这个issue是关于bug报告，主要涉及到TensorRT-LLM中的模型转换错误，导致了映射错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2855
这是一个bug报告，问题涉及的主要对象是在5110上构建TensorRT引擎时出现错误，可能是由于硬件不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2854
这是一个关于bug报告的issue，主要对象是在TensorRT-LLM下无法成功转换InternVL2-1B模型。由于读者尝试遵循指南进行模型转换却失败，可能导致的原因是转换脚本不适用该模型或者与transformers包版本相关的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2853
这是一个bug报告类型的issue，主要涉及到Fix googletest github。由于googletest的问题导致了bug或者用户提出了关于googletest的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2852
这是一个bug报告，该问题单涉及的主要对象是.gitmodules文件。由于.gitmodules文件出现问题，导致了该issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/2850
这是一个bug报告，主要涉及DeepSeek-V3模型在使用TensorRT-LLM时遇到的Integer Overflow错误。由于权重计数超出int32_t范围导致的转换失败，导致构建引擎过程中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2849
这是一个bug报告。该问题涉及主要对象为TensorRT-LLM。由于对`addCumLogProbs` kernel中批次槽使用的错误导致了这个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2847
这个issue属于bug报告类型，主要涉及TensorRT-LLM在5080 GPU上无法运行Whisper的问题，可能是由于Torch版本不兼容导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2846
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的LlmArgs类。造成这个bug的原因是LlmArgs初始化时传入了一个未预期的关键字参数'enable_attention_dp'。

https://github.com/NVIDIA/TensorRT-LLM/issues/2845
这是一个Bug报告类型的Issue，主要涉及的对象是TensorRT-LLM中的DeepSeek V3模型。用户遇到了一个关于Unknown architecture for AutoModelForCausalLM的数值错误，可能是由于模型未被正确识别导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2844
这是一个关于软件版本发布计划和支持不同功能的询问类型的issue，主要涉及到TensorRT-LLM的各个分支之间的差异和支持的TRT引擎功能，由于分支间的差异和信息不清晰，用户寻求关于版本发布计划和当前分支是否支持所需功能的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2843
这是一个bug报告，涉及使用TensorRT-LLM来服务deepseekR1distillqwen14B时出现的错误，错误原因是未定义的名称'_flash_supports_window_size'导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2842
这是一个bug报告，涉及TensorRT-LLM下的ModelRunnerCpp.from_dir()方法调用时出现的TypeError，原因可能是该方法不支持所传递的'gather_generation_logits'参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/2841
这是一个测试规则审查的问题单，主要对象是测试规则。由于尚未提供具体内容，原因可能是用户提出需要审查测试规则的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2840
这是关于Docker在Ubuntu 24.04.2 VM上安装时出现的APT-GET问题的报告，类型为bug报告，主要涉及对象是Docker安装过程中的APT-GET更新无法解析 DNS，可能的原因是DNS解析失败导致无法完成脚本执行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2839
这是一个需求类型的issue，主要涉及文档所有者添加，目的是使文档组成为/docs/的所有者。

https://github.com/NVIDIA/TensorRT-LLM/issues/2838
这个issue类型是关于性能问题的用户报告，主要涉及TensorRT-LLM下的Orchestrator模式中多个服务导致性能下降的情况。根据描述，可能是由于多个服务同时运行时共享GPU资源导致性能下降，用户希望获得关于调试或配置调整的建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/2837
这是一个关于修复blossom-ci的issue，类型为bug报告，涉及的主要对象是TensorRT-LLM。由于blossom-ci存在问题，导致需要对其进行修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/2836
这个issue类型是bug报告，涉及的主要对象是修复blossom-ci。由于当前blossom-ci存在问题，需要修复以解决相关bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2835
这是一个功能需求的讨论。它涉及到对团队成员资格进行检查的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2834
这是一个Bug报告，用户提出了在Windows x86_64系统中缺少tensorrt_llm_ucx_wrapper.dll和tensorrt_llm_ucx_wrapper.lib文件的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2833
这是一个bug报告issue，主要对象是TensorRT-LLM下的InternVL2模型。由于InternVL2推断的输出文本出现了问题，可能是由于模型设置或输入数据不正确导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2832
这是一个用户提出需求的问题，主要涉及的对象是xgrammar库的更新。由于当前版本的xgrammar在Executor中不支持StructuralTagItem功能，导致无法提供利用该特性的EBNF语法。

https://github.com/NVIDIA/TensorRT-LLM/issues/2831
这个issue是关于用户提出需求，询问是否计划通过旧的（非pytorch）工作流程支持DeepSeek v3，以及询问旧工作流是否将被废弃，pytorch工作流是否将成为新模型的主要方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/2830
该issue是关于需求的，主要涉及使用NVIDIA-gha runners来收集测试结果在CI中。这个问题的产生可能是为了优化CI流程以便更有效地收集和分析测试结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2829
这是一个bug报告，涉及的主要对象是将RoBERTa模型转换为TensorRTLLM引擎。由于未通过huggingface测试，可能是由于转换过程中的某些错误或问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2828
这是一个新功能需求的issue，涉及到添加CODEOWNERs文件进行规则测试，由于缺乏该文件可能导致团队在规则测试中的管理和决策方面存在一些不便。

https://github.com/NVIDIA/TensorRT-LLM/issues/2827
这是一个关于性能瓶颈的bug报告issue， 主要涉及TensorRT-LLM中的_initialize_and_fill_output函数，问题出现在该函数中导致每次查询耗时超过1.18秒。

https://github.com/NVIDIA/TensorRT-LLM/issues/2826
这是一个关于构建Tensorrt-LLM时出现错误的bug报告，主要涉及构建过程中出现的错误以及无法成功构建的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2825
这是一个bug报告类型的issue，涉及的主要对象是在TensorRT-LLM下调用pytorch backend运行fp8 hf模型时出现的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2824
这是一个Bug报告，主要涉及Baichuan2模型在量化为FP8后运行时core dumped的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2823
这是一个用户建议需求的issue，主要涉及的对象是在TensorRT-LLM中添加R1性能数据到最新消息页面。可能由于用户希望更方便地查看最新的性能数据而提出此需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2822
这是一个bug报告，该问题涉及TensorRT LLM中使用R1 FP4 checkpoint时在GSM8K数据集上出现的准确性问题，可能是由于预先量化的模型导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2820
这个issue是关于bug报告的，主要涉及TensorRT-LLM的功能更新和bug修复，其中包括新增特性、bug修复和已知问题。由于PyTorch workflow在SBSA上与裸机环境不兼容，用户可能会遇到问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2819
这是一个用户提出关于TensorRT-LLM中的Qwen模型是否支持FP8转换的问题，主要涉及convert_checkpoint.py脚本中的配置选项是否支持FP8。

https://github.com/NVIDIA/TensorRT-LLM/issues/2818
这是一个bug报告，涉及TensorRT-LLM中quantize.py文件中的错误，导致出现NoneType object not subscriptable的异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/2817
这是一个bug报告，涉及的主要对象是TensorRT-LLM 0.17版本。由于从0.16版本升级到0.17版本，使用了fp8 + llama + L4组合时，出现了reduce_fusion和user_buffer结合的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2816
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下Executor API的使用问题，请求添加关于External Draft model speculative decoding的Python示例和询问encoder-decoder模型支持的Speculative decoding模式。

https://github.com/NVIDIA/TensorRT-LLM/issues/2815
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM下的VLMs。由于KV cache quantization似乎在VLMs上没有提供预期的速度提升，用户怀疑自己的实现是否正确或者KV cache quantization在VLMs上是否不太适用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2814
这是一个bug报告，涉及到TensorRT-LLM下的cross_kv_cache_fraction设置问题导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2813
该issue是一个bug报告，主要涉及多GPU Triton部署时出现的MPI错误，请求寻求帮助解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/2812
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Executor API，用户希望实现数据并行性以避免由于负载均衡而导致的性能下降问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2811
这是一个关于安装TensorRT-LLM时遇到MPI错误的问题报告，用户在Grace black平台上尝试安装TensorRT-LLM时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2810
这是一个bug报告，涉及TensorRT-LLM中的一个问题，用户在执行模型量化过程中遇到了转换代码与实现之间的差异导致的输出结果错误的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2809
这是一个功能需求，主要涉及TensorRT-LLM下的Per-Request Logits Post-Processor Registration。由于当前logits后处理器必须在初始化时注册，无法在每个请求上进行运行时注册，导致应用与模型部署之间紧密耦合，需要已知并命名完整的验证模式集合，并且应用逻辑更改需要模型重新部署。

https://github.com/NVIDIA/TensorRT-LLM/issues/2808
这是一个功能需求类的issue，主要涉及到TensorRT-LLM项目中的workflow comment trigger。由于原始实现检查整个注释而不是注释的开头，导致无法添加任意参数到触发注释的末尾。

https://github.com/NVIDIA/TensorRT-LLM/issues/2807
这是一个空内容的测试CI的issue，类型为CI测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/2806
这个issue类型为功能优化，主要涉及TensorRT-LLM下的模型量化参数设置和自动推断，用户希望改进模型优化流程中的批处理大小设置。

https://github.com/NVIDIA/TensorRT-LLM/issues/2805
这是一个bug报告，主要涉及TensorRT-LLM下的Qwen2VL，报错信息表明出现了类型错误。原因可能是在执行python3 run.py时出现的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2804
这是一个bug报告，涉及到TensorRT-LLM下的speculative decoding功能，由于未能生成预期文本导致问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2803
这个issue是关于bug报告，涉及到TensorRT-LLM中的Qwen convert_checkpoint.py脚本中的量化方法映射错误问题。由于代码中将`awq`映射到`gptq`，导致了不正确的量化方法匹配问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2802
这是一个关于性能问题的issue，主要涉及TensorRT-LLM在新版本下性能下降的情况，可能由于参数配置或版本更新导致GPU利用率低下。

https://github.com/NVIDIA/TensorRT-LLM/issues/2801
这是一个关于在使用TensorRT-LLM转换模型时遇到Open MPI错误的bug报告，主要涉及模型转换过程中出现的错误现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/2800
这是一个用户提出需求的问题，主要涉及TensorRT-LLM框架中GptManager接口被Executor接口替代的变更。用户询问为何替换了GptManager接口以及Executor接口作为替代的优势，表达了对此变化的困惑并寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2799
这是一个寻求帮助的问题，涉及主要对象是如何在docker上设置TensorRT-LLM以便将模型从huggingface转换为TensorRTLLM并执行推理。

https://github.com/NVIDIA/TensorRT-LLM/issues/2798
这是一个bug报告，主要涉及运行TensorRT-LLM的Speculative Decoding with Draft model时出现的bug，导致run.py卡住不动。

https://github.com/NVIDIA/TensorRT-LLM/issues/2797
这是一个bug报告，涉及的主要对象是DeepSeek-R1-Distill-Qwen-1.5B推理偏差。由于结果出现偏差，用户寻求关于使用TensorRT-LLM进行推理时的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2796
这是一个bug报告，涉及的主要对象是Multimodal Cross-attention功能，问题可能是由于TensorRT-LLM在执行过程中产生了不正确的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2795
这是一个功能需求类型的Issue，主要涉及的对象是TensorRT-LLM库。由于更新后的TensorRT-LLM库在aarch64平台下使用的库要求GLIBC_2.38，导致在旧版本系统上无法使用，希望更新构建过程以支持旧版本的GLIBC，以便在旧版系统上运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2794
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下的Qwen2.5-VL架构。这个问题出现的原因是在TensorRTLLM中的MODEL_MAP中尚未包含Qwen2.5VL，导致无法支持该架构的模型转换和引擎构建。

https://github.com/NVIDIA/TensorRT-LLM/issues/2792
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的更新和功能新增，其中涉及到的问题是修复了PluginField的参数长度错误以及改善了LoRA在处理多个具有不同任务id的请求时的性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2791
这是一个bug报告，涉及主要对象为TensorRT-LLM项目中的deepseek分支，由于缺少std::ios_base_library_init符号导致链接错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2789
这是一个bug报告类型的issue，涉及的主要对象是在Qwen2上运行Speculative decoding时遇到的不兼容的缓冲区大小，可能是由于TensorRT-LLM版本、模型配置等问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2788
这是一个安装问题，用户需要一次性安装多个缺失的模块，主要涉及到TensorRT-LLM下的一些模块。这个问题可能是由于缺少必要模块或依赖导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2787
这个issue是关于bug报告，主要涉及了addCumLogProbs Kernel在处理Batch Slot时出现错误导致输出不正确的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2786
这是一个问题报告，主要涉及Deepseek-v3在2xH100节点上性能问题，可能是由于batch size为4时性能急剧下降。

https://github.com/NVIDIA/TensorRT-LLM/issues/2785
这是一个关于性能问题的bug报告，针对TensorRT-LLM中Qwen1.5-7B模型在trtllm-bench测试中表现非常差的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2784
该问题类型为用户询问问题，主要涉及对象是针对使用TensorRT-LLM来为ComfyUI实现功能的可行性验证。由于用户觉得TensorRT-LLM在SDXL上速度很快，希望了解是否可以将其应用到ComfyUI，以提高在diffusers方面的效率。

https://github.com/NVIDIA/TensorRT-LLM/issues/2783
这是一个需求更新的issue，主要涉及TensorRT-LLM的更新以及添加新功能，如DeepSeek V3/R1支持、批处理logits处理支持等。

https://github.com/NVIDIA/TensorRT-LLM/issues/2782
这是一个bug报告，涉及TensorRT-LLM中使用LoRA通过LLM API加载引擎时出现错误。原因可能是在加载引擎时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2781
这是一个bug报告；该问题涉及TensorRT-LLM中的用户缓冲和减少融合功能未按预期启用的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2779
这个issue是一个空白issue，类型为用户未提供任何具体内容的问题报告，无法确定主要涉及的对象或具体问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2778
这是一个关于bug报告的issue，涉及的主要对象是TensorRT-LLM中的Executor API。由于GPU和内存利用率逐渐下降导致推理速度下降，可能是由于某种内部资源管理问题引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2777
这是一个关于bug报告的issue，主要问题是在使用TensorRT-LLM的decoupled模式时，输出的batch索引顺序不一致，可能是由于此模式下导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2776
这是一个bug报告，主要涉及DeepSeek-V3的fp8 tp32权重转换问题，由于无法成功转换权重，用户正在寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2775
这是一个bug报告，主要涉及的对象是Qwen2-VL，该问题可能由于multirope op 的bug导致在处理多个并发请求时时间消耗增加，即使设置了max_batch_size=4。

https://github.com/NVIDIA/TensorRT-LLM/issues/2774
这个issue是关于安装问题的bug报告，主要涉及到TensorRT-LLM下的安装过程，由于Git和Flash Infer依赖导致安装中出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2773
这是一个用户提出需求的issue，主要问题是用户想要控制TensorRT使用的最大GPU内存量，但当前的参数不够直接控制，希望能找到更直接的方式，或者能够估计给定参数下TensorRT引擎会使用多少内存。

https://github.com/NVIDIA/TensorRT-LLM/issues/2772
这是一个bug报告，涉及主要对象为TensorRT-LLM中的checkpoint创建。由于无法成功转换下载的Llama3.2 1B和3B版本导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2771
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的quantize_and_export()函数。由于函数调用时传入了一个未知的关键字参数'cp_size'，导致出现了TypeError异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/2770
这是一个bug报告，涉及的主要对象是TensorRT-LLM项目中的torch.md文件。由于提供的链接无效，导致无法访问相关内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2769
这是一个bug报告，涉及的主要对象是在Amazon Linux 2上安装TensorRT-LLM遇到的安装失败问题。导致这个问题的原因是无法成功安装tensorrt_cu12_libs和tensorrtcu12库，用户提出了安装过程中遇到的错误以及尝试解决方案无效的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2768
这是一个bug报告，主要涉及到TensorRT-LLM中的`WeightOnlyQuantRowLinear`模块缺少了`is_expert`参数，导致MoE模型在INT8 weightonly量化期间执行不必要的`allreduce`操作，进而导致了Deepseek V2.5在INT8 weightonly量化情况下运行`run.py`产生不正确的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2767
这是一个关于bug报告的issue，主要涉及到TensorRT-LLM中的Whisper模型，因为某些输入大小会导致CUDA非法内存访问错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2766
这个issue属于性能改进类型，主要涉及FP8的性能改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/2765
这个issue类型为用户提出需求，主要涉及的对象是对于TensorRT-LLM是否计划实现DeepSeek中介绍的DualPipe并行机制。

https://github.com/NVIDIA/TensorRT-LLM/issues/2764
这是一个空issue，类型无法确定。

https://github.com/NVIDIA/TensorRT-LLM/issues/2763
这个issue是关于bug报告，主要涉及TensorRT-LLM中使用bfloat16转换模型时出现OSError的问题，导致无法找到有效的模型标识符。

https://github.com/NVIDIA/TensorRT-LLM/issues/2762
这是一个需要更新说明文档的问题，主要涉及DeepSeek V3的README。可能由于README文档过时或不完整导致用户需要更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2761
这是一个bug报告类型的issue，主要涉及修复TensorRT-LLM在github io pages上的问题。原因可能是页面显示异常或者无法正常访问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2760
这是一个bug报告类型的issue，涉及的主要对象是在TensorRT-LLM下使用T4 GPU构建whisper时出现core dumped的问题。原因可能是版本不兼容导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2759
这是一个用户询问是否支持Mixtral8x7B平滑量化的问题，属于功能需求类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2758
这是一个bug报告，主要涉及TensorRT-LLM中protobuf输入数据类型不匹配导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2757
这是一个bug报告，涉及到TensorRT-LLM的构建问题。问题可能是由于CUDA_ARCHS参数设置错误导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2755
这是一个功能更新的issue，涉及主要对象为TensorRT-LLM。由于新功能加入、支持和一些限制，用户可能会关注特定平台的pip安装支持问题、新硬件支持、PyTorch工作流程实验特性等内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2753
这个issue是一个bug报告，主要对象是TensorRT-LLM下的Executor Api。由于CPU内存泄漏导致，频繁运行benchmark请求会导致内存泄漏。

https://github.com/NVIDIA/TensorRT-LLM/issues/2752
这是一个bug报告，主要涉及TensorRT-LLM下的moenormalization模式问题，由于mixtral和arctic的归一化模式设置不正确导致症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2751
这个issue为更新说明文档的问题，涉及主要对象为TensorRT-LLM项目的README.md文件。原因可能是0.17版本的更新引起了部分内容需要补充或修改。

https://github.com/NVIDIA/TensorRT-LLM/issues/2750
这是一个用户询问关于新的pytorch workflow的类型为用户提出需求的issue，主要涉及对象是TensorRT-LLM。用户想了解关于使用TRTLLM的推荐方式以及性能表现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2749
这是一个bug报告，针对TensorRT-LLM中在Ubuntu 22.04.4 LTS上安装0.17.0.post1版本时出现的安装失败问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2748
这是一个bug报告，主要涉及TensorRT-LLM中KV Cache quantization功能与Whisper集成时出现的问题。这个bug可能是由于KV Cache的fp8或int8量化导致的，会在第一次推理请求中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2747
这是一个优化建议，针对代码中重复读取JSON文件的问题，提出只在必要时读取一次。

https://github.com/NVIDIA/TensorRT-LLM/issues/2746
这个issue是关于文档页面更新的请求，属于用户提出需求类型，主要涉及到TensorRT-LLM项目的文档页面。由于可能的原因可能是文档内容过时或有错误，导致用户请求更新文档页面。

https://github.com/NVIDIA/TensorRT-LLM/issues/2745
这是一个关于软件安装问题的bug报告，涉及的主要对象是TensorRT-LLM。由于pip安装时仍指向NVIDIA的私有存储库，导致安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2744
这是一个bug报告类型的issue，涉及主要对象是文档更新。这个问题由于extra-index-url导致，在更新文档时需要解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2743
这是一个关于更新Github Pages的issue，类型为用户提出需求，相关对象为TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/2742
这是一个用户提出需求并添加备注的类型，主要涉及对象为黑韦尔（Blackwell）用户。由于缺乏具体内容，很难判断用户需求背后的原因或具体问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2741
这是一个请求更新GitHub页面的issue，属于其他类型（非bug报告）。主要涉及项目的GitHub页面显示。

https://github.com/NVIDIA/TensorRT-LLM/issues/2740
这是一个用户提出需求的issue，主要涉及到fp8 kv cache 和 FP8_PER_CHANNEL_PER_TOKEN quantization。用户询问是否能让llamalike模型使用fp8 kv cache进行FP8_PER_CHANNEL_PER_TOKEN量化，或者是否有计划支持此功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2739
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中Whisper模型解码的问题，用户遇到了解码引擎不在`eos_token_id`处停止生成的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2738
这是一个关于功能实现的问题，主要涉及TensorRT-LLM下的KV cache。用户询问TensorRTLLM在KV缓存中存储的内容是何种形式。

https://github.com/NVIDIA/TensorRT-LLM/issues/2737
这是一个bug报告类型的issue，针对TensorRT-LLM库中的 AttributeError 错误，主要涉及对象是`tensorrt.fp4`模块，引发错误的原因可能是模块属性不存在。

https://github.com/NVIDIA/TensorRT-LLM/issues/2736
这个issue属于bug报告类型，涉及TensorRT-LLM的安装问题和缺失的Blackwell kernels，可能由于缺失的静态库和更新不完整导致安装问题或功能缺失。

https://github.com/NVIDIA/TensorRT-LLM/issues/2735
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中的DeepSeek-R1-Distill-Llama-70B模型生成垃圾数值的问题。导致这个问题的原因可能与int4量化版本的模型权重处理有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/2734
这是一个关于需求更新的问题，主要涉及到更新cutlass kernel库。由于更新不及时或者缺少某些功能，用户希望对cutlass kernel库进行更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2733
这是一个bug报告，涉及的主要对象是在构建TensorRT LLM引擎时出现了Lora错误。由于使用的库版本、环境配置等原因导致了这个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2732
这是一个用户提出更新文档的需求，可能是因为当前的文档不够清晰或者需要添加新的信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2731
该issue类型是功能增强需求，主要涉及的对象是trtllm-serve。由于新功能需求，用户提出了需要向trtllm-serve添加分块上下文/预填充运行时选项的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2730
这是一个bug报告，涉及TensorRT-LLM quantization script中的AttributeError问题，主要是由于TensorRT-LLM中缺少'info'属性导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2729
这个issue类型是bug报告，该问题单涉及的主要对象是无法安装tensorrt_llm 17版本，由于flashinfer git克隆失败导致安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2728
这是一个用户需求的类型issue，该问题单涉及到在TensorRT-LLM中添加deepseek模型。因为用户想要向项目中添加deepseek模型，所以提出了这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2727
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的一个问题。原因可能是由于特定操作的处理不正确而导致了报错信息"Trying to remove block n by 0 that is not in hash map"。

https://github.com/NVIDIA/TensorRT-LLM/issues/2725
这是一个关于TensorRT-LLM v0.17版本发布的issue，类型为新功能和增强特性的通知，涉及的主要对象是该软件的更新内容和支持的平台、模型和特性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2724
这是一个bug报告，涉及到TensorRT-LLM中的ifb issue。原因可能是深度搜索中ifb存在问题导致症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2723
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM代码。由于创建了一个超出max_size的std::vector，导致了错误信息"[TensorRTLLM][ERROR] Encountered an error in forwardSync function: cannot create std::vector larger than max_size()"。

https://github.com/NVIDIA/TensorRT-LLM/issues/2722
这是一个用户提出需求的类型的issue，主要对象是TensorRT-LLM中的性能问题。由于性能没有按预期增加，用户想要确定是什么导致了计算或内存带宽等方面的瓶颈。

https://github.com/NVIDIA/TensorRT-LLM/issues/2721
这是一个bug报告，主要问题是Whisper示例未返回原始语言的转录，仅返回英语翻译。可能是由于设置问题导致的。 

https://github.com/NVIDIA/TensorRT-LLM/issues/2720
这个issue是关于需求问题，主要涉及的对象是TensorRT-LLM项目。用户提出了关于更新频率的疑问，可能由于长时间没有更新导致用户关注并期待新版本的发布。

https://github.com/NVIDIA/TensorRT-LLM/issues/2719
这是一个功能需求的issue，关于为DeepSeek V3添加FP8支持的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2718
这是一个用户提出需求的问题，主要涉及的对象是如何在使用自定义模型时在TensorRT-LLM上对模型进行量化。这个问题的根本原因是用户想了解在TensorRT-LLM上使用自定义模型时如何进行模型量化，是否需要编写C++代码以及是否有相关示例。

https://github.com/NVIDIA/TensorRT-LLM/issues/2717
这是一个bug报告，主要涉及到TensorRTLLM中的输入长度限制以及Qwen2.57Binstruct模型的tokens数量支持与实际设定值不符的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2716
这是一个bug报告，主要涉及TensorRT-LLM下的Memory Leak问题，可能由于未提供所需输入或请求量过大导致GPU利用率过高和内存泄漏。

https://github.com/NVIDIA/TensorRT-LLM/issues/2715
这是一个bug报告，涉及deepseek_v1和norm_topk_prob参数的配置问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2714
这是一个关于功能需求的问题，主要涉及TensorRT-LLM下的trtllmserve是否支持部署多模态模型。造成这个问题的原因可能是trtllmserve目前不支持部署多模态模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2713
这个issue是关于bug报告，主要涉及TensorRT-LLM下的模型推理结果与HuggingFace Python模型推理结果不一致的问题，可能由于版本0.11.0的问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2712
这是一个bug报告，主要涉及TensorRT-LLM中gptattentionplugin的ONNXParser兼容性问题，由于缺少对`tensorrt.OnnxParser`转换API字段的声明以及未使用的`in_flight_batching`插件字段，导致问题出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/2711
这是一个bug报告，涉及主要对象是如何在TensorRT-LLM中编译deepseekv3。由于在运行编译命令时出现了权限问题，推荐使用虚拟环境来解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/2710
这是一个关于需求的issue，用户提出了关于Blackwell架构的支持问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2709
这是一个关于部署TensorRT-LLM中遇到问题的Bug报告，主要涉及的对象是Triton inference Server。由于部署过程中的步骤或配置可能存在错误，导致服务器无法正常启动运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2708
这是一个bug报告，主要涉及TensorRT-LLM下的forwardAsync函数遇到了Assertion failed错误，可能是由于负载过重导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2707
这个issue类型为功能需求，主要涉及NVILA模型的支持。由于当前示例中展示了VILA + Llama（而NVILA是基于Qwen2的），因此用户询问何时会支持新的NVILA模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2706
这是一个bug报告，涉及的主要对象是NVILA模型的转换。由于AutoConfig无法加载huggingface配置，导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2705
这是一个关于性能问题的issue报告，涉及到TensorRT-LLM中的模型在forward过程中加速效果的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2704
这是一个关于用户提出需求的issue，主要涉及到TensorRT-LLM中对 INT2/INT3 量化的支持，用户想了解是否计划在未来支持INT2/INT3量化，以及了解支持INT2/INT3量化所需的工作量。

https://github.com/NVIDIA/TensorRT-LLM/issues/2703
这是一个用户询问问题的issue，主要涉及Quantized Model using AWQ and Lora Weights。询问是否TensorRTLLM支持使用AWQ和lora weights量化的模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2702
这是一个bug报告，涉及主要对象是TensorRT-LLM下的Qwen2.57Binstruct模型，由于输入长度超过8192 tokens导致了限制错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2701
这是一个空白issue，类型为更新请求，涉及主要对象为GitHub Pages。

https://github.com/NVIDIA/TensorRT-LLM/issues/2700
这是一个撤销之前文档修复的issue，涉及的主要对象是TensorRT-LLM文档。可能由于之前的文档修复导致了一些问题，需要撤销这次修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/2699
这是一个Bug报告，涉及TensorRT-LLM中使用FP8 kv_cache reuse时出现的输出错误。导致这种症状的原因可能是kv_cache的重复利用导致了错误的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2698
这是一个用户提出的问题类型的issue，主要涉及的对象是TensorRT-LLM中的`execution context memory`。由于相同模型、相同推断图片和相同配置，在不同设备上模型的`execution context memory`显示出巨大的差异，猎取是因为什么原因导致这种情况以及是否可配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/2697
这是一个bug报告，主要涉及到TensorRT-LLM下的kv cache config配置问题。由于kv cache配置的问题，导致了某种症状的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2696
这是一个性能优化建议，主要涉及TensorRT-LLM中的自定义allreduce功能。原因是作者发现当使用自定义allreduce内核进行性能测试时，特别是在使用cuda图形并且批量大小较小时，观察到了可观的延迟。提议通过优化索引计算来改善性能，并认为为一次性allreduce添加__launch_bounds__(512, 1)也能提高性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2695
这个issue属于用户提出需求，涉及主要对象是在TensorRT-LLM中操作动态形状的输入张量并替换为动态形状的zerolike张量。原因在于在运行时需要替换具有动态形状（1,1）的输入张量，而使用Numpy会出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2694
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM。这个问题的症状是无法正常运行benchmark，可能是由于Docker命令运行不正确导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2693
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的Qwen2-72B模型，在更新版本后执行Qwen SQint8时遇到空输出的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2692
这是一个bug报告，主要涉及的对象是TensorRT-LLM。导致这个问题的原因可能是在将auto参数更改为'cpu'时仍报告先前的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2691
这是一个bug报告，主要涉及对象是TensorRT-LLM下的tokenizer初始化时kwargs参数的更新问题，导致可能会出现错误的行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/2690
这是一个关于bug报告的issue，主要涉及的对象是TensorRT-LLM下的trtllm-build命令。导致该问题的原因是似乎存在内存泄漏，导致在拥有超过1TB内存的机器上编译失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2689
这是一个Bug报告类型的问题，涉及到TensorRT-LLM中Whisper编码器的错误输出形状问题。由于输入总长度正确为572，但TRT生成的输出长度为571，导致了错误的输出形状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2688
这个issue是一个bug报告，主要涉及的对象是TensorRT-LLM的构建过程。由于工作空间大小不足导致构建失败，用户询问如何增加工作空间大小。

https://github.com/NVIDIA/TensorRT-LLM/issues/2687
这个issue属于bug报告类型，涉及的主要对象是Multi-LoRA的cpp推断过程。由于lora_weights数值不足导致的断言失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2686
这是一个关于需求的问题，主要涉及的对象是internvl 2.5版本，用户询问是否支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2685
这是一个关于缺少字段声明的bug报告，涉及主要对象是GitHub上的TensorRT-LLM中的`GPTAttentionPlugin`。由于缺少了一些关键字段声明，导致在加载含有`GPTAttentionPlugin`的onnx模型时失败，需要在creator类中添加缺失的字段声明。

https://github.com/NVIDIA/TensorRT-LLM/issues/2684
这是一个关于bug报告的issue，涉及主要对象是在使用TensorRT-LLM模型时遇到推断错误。原因可能是配置参数错误导致的断言错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2683
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM中的Deepseek-v3模型，由于使用int4 weight only engine进行推理时输出垃圾字符，可能是由于模型配置或实现问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2682
这是一个bug报告，主要涉及TensorRT-LLM中的continuous kv cache使用时的问题。由于未正确使用选定的past_key_value，导致计算结果错误当continuous kv caches的值不为零时。

https://github.com/NVIDIA/TensorRT-LLM/issues/2681
这个issue类型是用户提出需求，主要涉及TensorRT-LLM中的量化实现区别，用户询问如何通过不同命令实现量化及是否可以手动修改生成的配置文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/2680
这是用户提出需要请教关于如何在TensorRT-LLM中使用多个GPU进行推断的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2679
这是一个Bug报告，主要涉及TensorRT-LLM下的TRT引擎构建，并由于某种未知原因导致`TypeError`错误，用户寻求解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/2678
这个issue是关于bug报告，涉及TensorRT-LLM中使用Qwen2-0.5b + Medusa进行推断失败的问题。原因是在更新版本后构建引擎时出现错误消息，并且即使手动添加缺失的配置，推断结果也混乱。

https://github.com/NVIDIA/TensorRT-LLM/issues/2677
这是一个bug报告类型的issue，涉及TensorRT-LLM下SmoothQuant在转换checkpoint时出现的错误。问题可能出现在模型参数数值为none。

https://github.com/NVIDIA/TensorRT-LLM/issues/2676
这是一个关于用户提出需求的问题，主要涉及TensorRT-LLM项目在Jetson平台上的版本兼容性。用户想要了解是否会有与v0.15.0标签对应的Jetson版本，或者如何将v0.15.0转换为适用于Jetson的版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2675
这是一个bug报告，涉及到TensorRT-LLM中的attention模块存在输出差异问题，导致部分样本的F1分数受到影响。

https://github.com/NVIDIA/TensorRT-LLM/issues/2674
这是一个bug报告， 主要涉及的对象是TensorRT-LLM中的torch.distributed模块。这个问题是由于模块‘torch.distributed’缺少‘ReduceOp’属性引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2673
这是一个bug报告类型的issue，涉及的主要对象是EAGLE模型的部署和推断过程。由于部署成功但推断时出现错误，可能是由于模型参数或部署流程设置不正确导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2672
这是一个关于需求的问题，涉及到TensorRT-LLM中不同版本的InternVL2模型的问题。由于使用了不同的LLM架构，导致在使用multimodal_model_runner()时可能会出现低精度的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2671
这个issue属于用户提出需求类型，主要涉及TensorRT-LLM下的speculative decoding实现，由于缺乏清晰的文档或示例，用户无法配置后端服务以实现预测解码，且trtllmserve不支持该功能，导致用户请求指导和帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2670
这是一个用户提出需求的问题，涉及TensorRT-LLM中构建带有权重稀疏性的模型时加速计算的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2669
这个issue是关于bug报告，主要对象是TensorRT-LLM文档，可能是由于文档中缺失内容导致的空白问题产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/2668
这是一个需求更新的issue，主要对象是项目的README.md文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/2667
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM的trtllm-serve命令。由于命令运行时没有任何输出，导致无法获取预期的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2666
这个issue是关于bug报告，主要对象是TensorRT-LLM中的fp8量化功能。问题可能是由于在模型量化过程中输出的config.json中架构为null导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2665
这是一个关于API文档缺失的问题，类型为bug报告，主要涉及TensorRT-LLM的文档页面，可能是由于页面内容更新不及时导致文档空白。

https://github.com/NVIDIA/TensorRT-LLM/issues/2664
这是一个关于功能需求的问题，主要涉及在TensorRT-LLM中的MLP和Attention层中支持的低比特（int8/fp8/int4）数据类型。用户提出了关于各层中支持的数据类型和内部运行方式的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2663
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的量化支持。由于新的24位quant格式提供更好的结果，用户建议整合该格式至TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/2662
这是一个用户提出需求的issue，主要对象是TensorRT-LLM中的cpp executor。用户询问如何将生成的token ids转换为可读的单词。这可能是由于用户需要更详细的指导来完成此操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2661
这是一个bug报告，涉及主要对象是 HF ckpt BF16 conversion。这个问题由于需要更新HF ckpt BF16 conversion而产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/2660
这个issue是一个bug报告，主要涉及到TensorRT-LLM在使用guided decoding xgrammar和kv cache reuse时发生了Segmentation fault crash。原因可能是在这两种功能一起使用时引起了crash。

https://github.com/NVIDIA/TensorRT-LLM/issues/2659
这是一个提出问题的问题报告，主要涉及到TensorRT-LLM 中 f16xs8 混合gemm实现与本地cutlass 例子中不同之处。由于实现中加入了dequantization scale，用户询问了TRT-LLM相较于cutlass原生实现在性能或准确性上的优劣，并对使用LDS而非LDSM来加载操作数B(s8)的细节提出了疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2658
这是一个bug报告，主要涉及的对象是Qwen2 VL模型在TensorRT-LLM中无法转换为checkpoint的问题。原因可能是Qwen2VL模型不受支持导致转换过程中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2657
这是一个用户提出需求的issue，主要涉及TensorRT-LLM的Dockerfile中关于PyTorch Nightly支持的问题。原因是当前版本中稳定的PyTorch版本滞后于CUDA、cuDNN、NCCL等组件，用户希望增加对Nightly版本的支持以获得更多bug修复和功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2656
这是一个bug报告类型的issue，主要涉及的对象是在使用`pixi`库管理工具安装TensorRT-LLM时出现了`No module named 'tensorrt_llm.bindings'`错误消息。这可能是由于缺少输入或配置不正确导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2655
这个issue是一个bug报告，主要涉及到setuptools的依赖冲突问题，导致无法安装所需的软件包。

https://github.com/NVIDIA/TensorRT-LLM/issues/2654
这个issue属于更新网站文档的类型，主要对象是项目的文档页面。由于可能需要更新项目网站上的内容或修复页面显示的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2653
这个issue类型为用户提出需求，主要涉及对象为项目中的文档。由于文档需要更新或补充相关信息，用户希望对linux.md进行更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2652
这是一个bug报告类型的issue，主要涉及到使用TensorRT-LLM过程中出现的CUDA调用失败问题。导致该问题的原因可能是CUDA调用在初始化时出现错误，具体表现为'CUDA call failed lazily at initialization with error: 'NoneType' object is not iterable'。

https://github.com/NVIDIA/TensorRT-LLM/issues/2651
这是一个空内容的issue，类型为更新请求，主要涉及更新GitHub Pages页面操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2650
这个issue属于用户提出需求类型，主要涉及更新"disaggregated-service.md"文档。由于文档可能需要添加、修改或者更新内容，用户建议对该文档进行更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2649
这是一个bug报告，主要涉及对象为TensorRT-LLM中的Mpool Failure on H100 DGX node问题。导致这个问题的原因可能是UCX警告中的错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2648
这是一个关于性能测量的问题，用户在A100 GPU上进行了TensorRT-LLM模型的基准测试，发现与文档中所述的性能存在差异，希望能够获得针对输入输出128 tokens的模型转换和构建方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/2647
这是一个bug报告，主要涉及在TensorRT-LLM下的文件convert_checkpoint.py中的代码问题（line 254 to 267），导致加载模型的次数与预期不符。

https://github.com/NVIDIA/TensorRT-LLM/issues/2646
这个issue类型是更新请求，主要涉及的对象是github页面。由于需要更新gh-pages，用户提出了这个issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/2645
这是一个需求更新的issue，主要涉及到服务文档的更新。产生这个问题是因为服务文档需要添加或修改内容以满足新的需求或者更新信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2644
这是一个bug报告，主要对象是TensorRT-LLM模型名称映射问题。原因可能是模型名称映射存在错误，导致模型无法正确加载。

https://github.com/NVIDIA/TensorRT-LLM/issues/2643
这是一个功能需求相关的issue，涉及TensorRT-LLM中的采样层实现，主要问题是无法从前端控制另一个min_p参数，需求包括实现Min-P采样和后续温度调整功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2642
这是一个bug报告，主要涉及ExecutorInstanceBasic.cpp在推断qwen模型时无法正确响应系统提示标记作为输入的问题，原因可能是解码后不能正确获得预期的响应。

https://github.com/NVIDIA/TensorRT-LLM/issues/2641
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的构建引擎失败问题。由于CUDA toolkit版本、驱动版本、以及构建参数设置等原因导致了构建引擎失败的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2640
这是一个bug报告，涉及TensorRT-LLM在使用特定设置时每次推断结果都会有所变化，可能是由于算法实现或参数配置问题引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2639
这是一个关于需求的提问，主要涉及对象是TensorRT-LLM中的attention机制。由于TensorRTLLM代码复杂且一般化程度高，导致用户困惑并希望了解如何在部署新模型时选择合适的attention类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2638
这是一个bug报告，涉及到在aarch64架构上构建TensorRT-LLM的Multi-Modal引擎失败的问题。原因在于缺少某些必要的依赖库，需要更新README来解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2637
这是一个bug报告，用户提出不想在运行ExecutorInstanceBasic.cpp时显示info信息，可能是由于不需要这些提示信息而希望简化输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2636
这是一个Bug报告，主要涉及的对象是TensorRT-LLM下的ModelRunnerCpp(executor)。其原因是在进行inference过程中，执行了一个不允许在InferenceMode之外更新推理张量的操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2635
这个issue是一个bug报告，主要涉及的对象是TensorRT-LLM下的ModelRunnerCpp(executor)。由于inplace update在InferenceMode外部不被允许，导致了该错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2634
这是一个bug报告，涉及TensorRT-LLM的Cpp runner在使用lora + tensor parallelism时输出错误结果的问题。由于Cpp runner和Python runner给出的结果不一致，并且Cpp runner的结果明显错误，用户正在寻求帮助解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2633
这是一个bug报告，针对TensorRT-LLM的自定义采样配置添加的问题，代码更改后导致编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2632
这是一个关于调试模型问题的issue，主要涉及的对象是TensorRT-LLM中的mistral模型。由于缺乏调试设置，用户想要解决Biomistral模型在TensorRT引擎上与vLLM相比精度降低的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2631
这是一个bug报告，主要涉及TensorRT-LLM中的KV cache reuse功能。这个问题导致当批量大小大于1时，启用KV cache reuse会导致推断延迟增加，无论公共前缀的长度如何。

https://github.com/NVIDIA/TensorRT-LLM/issues/2630
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的Qwen模型。由于未得到响应，导致出现提示信息，用户在寻求有关YaRN BUG的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2629
这是一个用户提出需求的issue，主要对象是希望在TensorRT-LLM中支持DeepSeek-V3模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2628
这是一个bug报告，主要涉及的对象是TensorRT-LLM模型执行，由于LoRA权重数据类型在量化的TensorRT-LLM模型执行时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2627
这是一个性能问题的issue，主要涉及到TensorRT-LLM模型在性能上表现较差，并且用户提到了可能是因为CUDA操作符之间存在较多间隙导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2626
这是一个bug报告，主要涉及TensorRT-LLM中的前向推断功能，由于某种原因导致出现了"forwardAsync assertion failed: Unable to get batch slot for reqId"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2625
这是一个类型为文档更新的缺陷报告，该问题单涉及的主要对象是更新TensorRT-LLM的gh-pages分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/2624
这是一个用户提出需求的issue，主要涉及TensorRT-LLM的trtllm-serve是否支持toolparser和guided-decoding，可能是因为用户想了解是否计划支持这些功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2623
这是一个更新文档的issue，主要对象是Windows版本，原因可能是发布了新版本需要更新相应的文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/2622
这是一个bug报告，主要涉及TensorRT-LLM中的元素加法操作中数据类型不匹配导致构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2621
这是一个bug报告，主要涉及fairseq和TensorRTLLM在使用过程中对decoder_input_ids设置问题导致模型输出异常的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2620
这个issue是关于bug报告的，主要涉及到TensorRT-LLM在T4 GPU上识别问题，可能是由于T4 GPU不能被正确识别导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2619
这是一个Bug报告，主要涉及TensorRT-LLM下的构建错误。用户遇到SIGABRT错误当尝试为biomistral模型在T4上构建trtllm引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/2618
这个issue属于更新网站页面的类型，主要涉及的对象是项目的网站页面。由于更新网站时出现问题，需要更新 gh-pages 分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/2617
这是一个用户提出需求类的issue，主要涉及TensorRT-LLM模型编译的性能问题，反映出模型编译可能未能提高性能，需要帮助优化配置以减少响应时间。

https://github.com/NVIDIA/TensorRT-LLM/issues/2616
这是一个用户提出需求的issue，主要涉及的对象是Phi4支持，用户想知道是否TensorRT-LLM支持Phi4。

https://github.com/NVIDIA/TensorRT-LLM/issues/2615
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的gather_generation_logits功能。由于label数量与vocab大小不匹配，导致生成的logits出现异常，提问者请求关于该部分的实现细节。

https://github.com/NVIDIA/TensorRT-LLM/issues/2613
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的性能问题。由于开启了`KVcachereuse`，导致了在streaming模式下请求性能比非streaming模式差，并且存在请求超时和后处理阶段耗时过长的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2612
这是一个空内容的issue，类型为 bug报告，涉及主要对象为 PR测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/2611
该issue类型为发布信息，主要涉及TensorRT-LLM v0.16的更新内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2610
这是一个关于如何抑制WARNING日志记录的问题，主要涉及的对象是TensorRT-LLM中的Python绑定ModelRunnerCpp，用户希望找到一种方法来抑制这些警告信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2609
这是一个bug报告，涉及到自定义采样配置的添加，编译时出现错误。可能由于自定义配置引起的编译错误引发了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2608
这是一个用户提出需求/请教问题的issue，主要涉及的对象是TensorRT-LLM，用户询问关于是否支持VLM模型的前缀缓存及如何启用该功能的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2607
这是一个关于现有功能限制的需求，涉及TensorRT-LLM中的Redrafter fp8支持问题，用户希望能够同时转换fp8的基础模型和fp32 Redrafter。原因是转换脚本目前仅接受fp16/fp32/bf16格式的基础模型，无法处理fp8格式的基础模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2606
这是一个Bug报告，用户提出需要为Gemma 2 9b LoRA添加支持，由于设置`lora_plugin`时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2605
这个issue类型是用户提出需求，主要涉及的对象是w4a8 quantization支持，用户提出了希望在trtllm中增加更好的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2604
这是一个bug报告，主要涉及TensorRT-LLM下SmoothQuant模块无法与Lora配合使用的问题，可能是由于未得到有效响应而导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2603
这是一个bug报告类型的issue，主要对象是使用TensorRT-LLM中的Lora模块时出现了问题，可能是由于使用"--use_fp8_rowwise"参数导致构建引擎时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2602
这是一个bug报告，主要涉及TensorRT-LLM中的--use_fp8参数无法与llama 3.1 8b正常工作，可能由于软件版本兼容性或配置错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2601
这是一个需求类别的issue，主要涉及的对象是创建一个c-cpp.yml文件。可能由于缺乏该文件导致项目无法正常运行或配置问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2600
这是一个bug报告，主要涉及到TensorRT-LLM中Qwen 2.5版本的一个问题，用户反馈在输入助手名称时输出不符合预期。由于可能的原因是输入处理逻辑错误导致重复输出同一名称。

https://github.com/NVIDIA/TensorRT-LLM/issues/2599
这个issue属于bug报告类型，主要涉及到在TensorRT-LLM安装过程中出现找不到'tensorrt_llm.bindings'模块的问题。由于缺少bindings模块导致了无法正常导入包的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2598
这是一个性能问题报告，主要涉及TensorRT-LLM下的TTFT（time to first token）性能测试。由于A100和A100 MIG的推理速度未达预期，用户寻求优化建议或见解。

https://github.com/NVIDIA/TensorRT-LLM/issues/2597
这是一个关于安装TensorRT-LLM的bug报告，涉及主要对象为TensorRT-LLM安装过程。由于构建TensorRTLLM时出现错误，导致无法成功安装，可能是由于依赖项或构建过程中的某些问题所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2596
这是一个用户提出需求的问题，主要涉及如何传递原始图像张量数据给自定义模型的forward()函数，该问题可能是由于需要将原始图像张量数据作为参数传递给模型的forward()函数而产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/2595
这个issue是关于Chinese decoding garbled characters in stream mode的bug报告，在TensorRT-LLM项目下。原因可能是由于字符解码问题而导致错乱字符显示。

https://github.com/NVIDIA/TensorRT-LLM/issues/2594
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的trtllmserve命令。由于某种原因导致无法在多个拥有8个GPU的节点上启动openAI-API服务器，出现了运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2593
这是一个关于如何在TensorRT-LLM下使用continuous kv cache与prefix prompt caching在gpt attention插件中的一个issue，主要涉及gpt attention插件在context阶段未使用预先计算的kv缓存值导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2592
这是一个用户提出需求的类型问题，主要对象是关于DiT或Flux.1-dev相关模型的输入padding功能。这个问题可能是由于用户想要支持多种不同分辨率请求而产生的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2591
这是一个bug报告，主要涉及TensorRT-LLM中的internVL，用户尝试进行batched inference时遇到了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2590
该issue类型为用户提出需求，请教问题，主要涉及对象是转换qwen2vl的可视部分为TensorRT模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2589
这是一个bug报告，主要涉及TensorRT-LLM在多GPU环境下使用时出现的结果错误问题，可能是由于LORA模块导致的输出文本和生成logits不一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2588
这是一个bug报告，涉及TensorRT-LLM中使用eagle模型转换时出现的错误，问题可能是由于加载模型时缺少'fc.bias'导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2587
这个issue是一个bug报告，涉及到使用pip安装tensorrt-llm时出现的依赖冲突问题，导致安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2586
这是关于bug报告的issue，涉及主要对象是TensorRT-LLM中的`trtllmserve`工具。该问题可能是由于OOM（Out of Memory）错误导致的，用户在构建引擎时遇到了内存溢出问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2585
这个issue是一个bug报告，主要涉及到llava-onevision convert bug这个模型转换问题，由于未设置tie_word_embeddings=True导致lm_head weights转换失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2584
这是一个用户提出需求的类型issue，涉及在自定义encdec架构中切片具有动态形状的张量，可能是由于需要将编码器输出传递给解码器，并需要对动态形状的张量进行切片而导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2582
这个issue类型为更新反映，涉及主要对象为TensorRT-LLM。由于Windows构建出现问题，导致该bug需要修复或者用户寻求相关帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2581
这是一个关于功能需求的issue，主要涉及的对象是OpenAIServer。由于OpenAIServer目前只支持LLM而不支持VLM，因此用户提出了无法传递图片信息的问题，并希望后期能够解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/2580
这是一个bug报告，主要涉及TensorRT-LLM的配置问题，导致输出token长度不符合预期的情况。原因可能是配置中的pad_id和end_id设置不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/2579
这是一个关于代码开源性的问题，主要涉及到TensorRT-LLM下的bindings.cpython-310-x86_64-linux-gnu.so文件。由于代码是否开源的问题，导致用户提出了相关疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2578
这是一个关于需求的问题，主要涉及TensorRT-LLM版本1.5是否支持InternVL 2.0模型，可能是由于图片预处理的版本差异引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2577
这是一个bug报告，主要涉及TensorRT-LLM中trtllmbench工具缺少对moe_ep_size / moe_tp_size的支持，导致无法评估MoE模型的专家并行性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2576
这是一个关于性能优化的问题，主要涉及到GPU内存的使用情况。用户想要通过配置参数来减少TensorRT-LLM在运行Qwen2.5 7B qint4模型时的GPU内存占用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2575
这是一个关于代码逻辑理解的问题，用户在询问关于自定义allreduce算法无法选择`AllReduceStrategyType::TWOSHOT`导致`twoShotAllReduceKernel`未被调用的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/2574
这是一个bug报告，涉及主要对象为TensorRT-LLM下的Qwen2VL2BInstruct转换过程。由于配置文件中出现了不被识别的key `{‘mrope_section}`，导致报错信息中提到的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2573
这个issue属于bug报告，主要涉及TensorRT-LLM中的sampling_params设置问题，由于在某些情况下未正确处理tokenizer的情况，导致潜在的未定义行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/2572
这是一个可能是bug报告的issue，主要涉及Testing Actions的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2571
这是一个关于bug报告的issue，主要涉及的对象是TensorRT-LLM。问题可能是由于安装的TensorRT-LLM版本与环境不兼容导致代码运行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2570
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的`pynvml`模块。由于`pynvml`模块在版本`12.0.0`中被移除，导致出现错误，解决方法是将其版本约束在小于`12.0.0`。

https://github.com/NVIDIA/TensorRT-LLM/issues/2569
这是一个用户提出需求的类型，主要对象是关于获取存储代码的位置，可能是由于缺少相关代码而无法使用或查看。

https://github.com/NVIDIA/TensorRT-LLM/issues/2568
这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的tensorrtllm_backend，用户正在寻求针对InternVL2的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2567
这是一个用户提出需求的issue，主要涉及支持LLaMa3.3模型在TensorRT-LLM中的部署。用户希望更新现有示例以整合新模型，并确认其与现有设置的兼容性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2566
这是一个关于优化TensorRT-LLM的问题，用户提出了“Add issue triage workflows”的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2565
这是一个用户寻求帮助的问题，主要涉及TensorRT-LLM中模型部署的相关教程。用户完成了引擎生成步骤，现在希望找到下一步部署的相关示例教程。

https://github.com/NVIDIA/TensorRT-LLM/issues/2564
这是一个bug报告，针对TensorRT-LLM下的finetuned llama 3.2 instruct model的输出重复问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2562
这是一个功能更新的Issue，主要涉及TensorRT-LLM的功能更新和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/2561
这是一个关于软件功能理解的问题，主要涉及到TensorRT-LLM下的awq_w4a8模型，用户询问了关于safetensor结果中'scaling_factor'各项的含义和在w4a8 gemm中的使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2560
这是一个bug报告，主要涉及的对象是nccl allgather功能。由于长时间运行后系统hang，可能与链接https://github.com/NVIDIA/nccl/issues/311相关。

https://github.com/NVIDIA/TensorRT-LLM/issues/2559
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中以`w8a8`和`kv_cache_dtype`为`FP8`构建失败的问题。可能由于TensorRT-LLM不支持使用`FP8`或`w4a8`构建加载模型，导致了建立`w8a8` with `fp8` kv_cache时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2558
这是一个关于设置`--max_encoder_input_len`参数的问题，属于需求和潜在bug报告类型的issue。用户主要在问如何正确设置此参数以及如何计算特征数量，最终导致了在运行时得到了一个警告，需要进一步澄清和解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/2557
这是一个关于如何正确使用greedy search的问题，主要涉及的对象是机器翻译模型TensorRT-LLM中的生成模块。由于设置top_p、top_k和temperature参数无法得到正确的greedy search结果，用户寻求帮助调整参数设置。

https://github.com/NVIDIA/TensorRT-LLM/issues/2556
这是一个bug报告，涉及到TensorRT-LLM的batch-manager.md文档被移除导致链接错误，问是否还有相应的指标可查看。

https://github.com/NVIDIA/TensorRT-LLM/issues/2555
这是一个bug报告，涉及到无法加载已构建的Llama引擎的问题，导致由KeyError引发的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2554
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的Bert模型实现。由于实现BertStyle pooling时发生了`remove_input_padding` tensor t.region->getDataType() == DataType::kINT32 failed的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2553
这是一个bug报告，主要涉及TensorRT-LLM下的int8和bf16性能对比出现的问题。原因可能是A100 GPU上int8运算比bf16慢导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2552
这个issue属于用户提出需求类型，主要涉及的对象是`infer_cluster_key()`方法。由于当前方法处理的设备列表不包括H200，用户希望添加H200（可能还有GB200）到列表中。

https://github.com/NVIDIA/TensorRT-LLM/issues/2551
这是一个性能问题报告，主要涉及TensorRT-LLM版本`0.16.0.dev2024112600`在运行Qwen2_VL模型时的性能表现不佳。原因可能是模型实现的不同所导致的性能差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/2550
这是一个用户提出需求（feature request）的issue，主要涉及lm_head的量化问题。由于lm_head的权重大小超过了10GB，在一些LLMs中，而当前无法对lm_head进行量化处理。

https://github.com/NVIDIA/TensorRT-LLM/issues/2549
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下trtllm-serve不支持类似tritonserver动态批处理的功能。用户希望通过使用tritonserver（tensorrtbackend）来实现OpenAI API，以获得动态批处理的优势。

https://github.com/NVIDIA/TensorRT-LLM/issues/2548
这是一个关于性能问题的bug报告，主要涉及TensorRT-LLM在长上下文问题上的性能表现，用户反馈不符合预期的性能表现可能由于缺乏配置优化文档导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2547
这个issue类型为bug报告，主要涉及的对象是LayerInfo中不支持fp8和int4_awq dtype。问题可能是由于导出的layer information中缺少Int4描述导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2546
这是一个用户提出需求的issue，主要涉及TensorRT中Qwen2-VL FP8/INT8量化支持的实现。

https://github.com/NVIDIA/TensorRT-LLM/issues/2545
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM中的性能测试工具trtllmbench的执行失败。可能是由于程序执行过程中出现了错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2544
这是一个bug报告，主要涉及Triton server的stream response编码错误问题，可能是由于环境配置或代码逻辑问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2543
这是一个bug报告，主要涉及到TensorRT-LLM中的lora模块，问题是由于禁用了kv_cache导致无法正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2542
这是一个bug报告，主要涉及TensorRT-LLM的安装问题。原因是由于安装命令中出现了错误的写法导致无法成功安装。

https://github.com/NVIDIA/TensorRT-LLM/issues/2541
这是一个与性能下降相关的bug报告，涉及TensorRT-LLM模型在不同精度下准确率降低的问题，用户希望解决准确率下降的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2540
这是一个bug报告。该问题涉及TensorRT-LLM在Windows上运行时出现运行时错误，用户在使用TensorRTLLM 0.15.0版本的时候遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2539
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的convert_checkpoint功能。由于GPU有充足的空闲内存，但在执行convert_checkpoint时程序被终止了。

https://github.com/NVIDIA/TensorRT-LLM/issues/2538
这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持PaliGemma模型。由于PaliGemma模型已在transformers中得到支持，用户想了解TensorRT-LLM是否会添加对其的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2537
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的`load_calib_dataset()`函数中的`trust_remote_code`参数未被使用。导致的问题是在量化过程中出现了需要用户输入的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2536
这个issue类型为bug报告，涉及的主要对象是TensorRT-LLM。由于系统环境和版本不匹配，导致运行gptManagerBenchmark时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2535
这是一个bug报告。用户尝试转换自定义编码器模型时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2533
这是一个bug报告，主要涉及TensorRT-LLM下回复中标点错误缺失的问题。用户发现相同模型在vllm引擎上表现优于tensorrtllm，因为在后处理过程中出现了缺少标点的情况。用户询问有什么方法可以提高响应的准确性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2532
这是一个需求更新类型的issue，主要对象是TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/2530
这是一个类型是更新请求的issue，主要涉及的对象是Github上的TensorRT-LLM。由于网页需要更新，用户提出了更新Github页面的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2529
该issue类型为软件发布通知，涉及内容为TensorRT-LLM Release 0.15.0的新功能和增强。

https://github.com/NVIDIA/TensorRT-LLM/issues/2528
这是一个bug报告，主要涉及Qwen2VL模型在使用零visual_features时仍能正常生成结果的现象。原因可能是图像信息被其他方式包含在内。

https://github.com/NVIDIA/TensorRT-LLM/issues/2527
这是一个bug报告，主要涉及到TensorRT-LLM项目中的CUtensorMap相关类型未被声明导致编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2526
这是一个关于使用问题的报告，主要涉及Trtllmserve命令无法找到的问题。可能由于路径未设置而导致此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2525
这个issue是bug报告，主要涉及TensorRT-LLM的转换过程中出现的问题，由于执行命令后没有任何反应且未使用GPU，可能是由于转换脚本或环境设置问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2524
这个issue是关于bug报告，主要涉及pynvml版本问题，可能是由于版本不匹配导致的import错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2522
这个issue类型是用户提出需求，主要涉及的对象是如何在使用executor API时通过每个请求设置eagle_choices或medusa_choices，由于无法在运行时更改它们，用户寻求关于如何在每个请求中设置这些选项的指导。

https://github.com/NVIDIA/TensorRT-LLM/issues/2521
这是一个用户询问如何使用TensorRT-LLM MultiShot allreduce算法的问题，不属于bug报告。询问的主要对象是TensorRT-LLM，用户想知道如何使用这个算法，因为在这个仓库中找不到相关信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2520
这是一个bug报告，主要涉及的对象是模型在使用pytorch和TensorRTLLM两种方式下输出结果不一致。导致这个问题的原因可能是在模型部署过程中的不一致性导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2519
这是一个功能需求报告，主要涉及的对象是TensorRT-LLM中的模型定义。由于用户希望实现通过onnx等跟踪方式生成模型定义，以便支持那些对trtllm结构不太熟悉或者非通用模型结构的用户，从而提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2518
这是一个bug报告，涉及TensorRT-LLM中的MPI Abort Error问题，用户寻求关于如何解决该错误的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2517
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于运行命令时出现AttributeError，导致了这个问题的产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/2516
这是一个bug报告，主要涉及的对象是NV bench output len。此问题可能是由于输出长度的数据类型不正确导致了垃圾数值的使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2515
这个issue属于安装问题，主要对象是TensorRT-LLM的安装过程。由于缺少pywin32的安装候选者，无法成功通过poetry安装TensorRTLLM，导致了安装失败的报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2514
该issue是关于bug报告，主要涉及TensorRT-LLM处理高并发请求时出现的推理时间增加的问题，用户寻求帮助找出并发请求时间增加的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/2513
这个issue类型是测试相关，主要涉及测试CI。由于标题标记为"[DO NOT MERGE]"，推测可能是为了提醒不要合并该测试代码。

https://github.com/NVIDIA/TensorRT-LLM/issues/2512
这个issue是用户提交的类型为需求，要求添加一个名为"blossom-ci.yml"的文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/2511
这是一个bug报告，用户正在寻求帮助在Qwen模型上使用EAGLE时输出出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2510
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中是否支持GLM4Voice。用户希望知道是否有计划支持GLM-4-Voice，原因是希望在TensorRT-LLM中能够使用GLM4Voice这个端到端语音模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2509
这个issue是性能问题的报告，主要涉及TensorRT-LLM在NVIDIA H100上qserve比awq int4慢的情况。原因可能涉及代码实现或者算法优化方面的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2508
这是一个Bug报告，主要对象是在TensorRT-LLM下无法构建Whisper engines，可能由于Whisper LargeV2 encoder hangs导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2507
这是一个bug报告，涉及的主要对象是在将checkpoint转换时出现错误，可能是由于TensorRT-LLM在特定环境下的实现问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2506
这是一个bug报告，主要涉及TensorRTLLM中Medusa max_draft_len设置导致推理性能下降的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2505
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的convert_checkpoint.py在错误model_dir路径时错误提示不清晰导致误解。

https://github.com/NVIDIA/TensorRT-LLM/issues/2504
这是一个用户提出需求的issue，主要涉及到是否存在用于低延迟的基准数据，以及关于TensorRT-LLM是否会开源内部的PDL gemm的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2502
这个issue类型是一个包含了bug修复和新功能添加的更新，主要对象是TensorRT-LLM项目。由于之前可能存在的问题，导致需要修复不必要的批量logits后处理函数调用，并新增了一些新的功能和支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2501
这是一个用户需求报告，涉及的主要对象是如何可视化TensorRT-LLM网络。由于TREx不支持LLM并且`trtllmbuil visualize_network`命令无效，用户想知道是否有工具或选项可以可视化LLMs的trt引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/2500
这个issue类型为用户提出需求，主要涉及的对象是在使用C++运行时时如何获取准确的TTFT和TPOT值。原因是由于使用C++运行时，难以准确获取所需的预填充延迟和时间复杂度信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2499
这是一个用户提出需求的问题，主要涉及TensorRT-LLM模型中提取hidden_states的问题，由于TensorRTLLM没有提供接口访问hidden_states导致用户无法获取想要的参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/2498
这是关于GitHub上TensorRT-LLM的一个issue，类型为功能需求，主要涉及模块所有者以及GitHub actions。

https://github.com/NVIDIA/TensorRT-LLM/issues/2497
这是一个用户提出需求的issue，主要涉及到对MoE模型中自定义路由分布进行运行时配置的需求。由于当前方法需要重新构建引擎以支持不同的路由分布，用户希望找到一种动态配置路由分布而无需重建引擎的方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/2496
这是一个关于避免在使用TensorRT-LLM进行推断中遇到OOM（内存溢出）问题的Bug报告，主要涉及对象是如何避免内存溢出以及推断过程中产生的内存占用增加的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2495
这是一个bug报告，主要涉及Qwen2-VL Batch Bug的问题，由于batch prompt时只有第一个结果是正确的，其余结果均为空，导致了该issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/2494
这是一个bug报告问题，涉及主要对象是TensorRT-LLM下的一个Assertion失败问题，可能是由于`tensorrt_llm::batch_manager::TrtGptModelInflightBatching::setupDecoderStep`设置错误的`max_new_tokens`导致了Assertion失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2493
这个issue是用户提出的需求，主要涉及的对象是Qwen convert_checkpoint.py，由于数据集加载问题，用户需要在模型转换过程中添加trust_remote_code参数以避免交互提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/2492
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的1B模型。由于某种原因导致1B模型在运行时出现错误的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2491
该issue为用户提出需求类型，主要涉及修改Encoder Decoder模型架构以支持新的配置，寻求关于如何修改代码以支持新架构的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2490
该issue是关于用户需求的，涉及到如何在TensorRT-LLM中设置"max_num_tokens"和"max_batch_size"作为运行时参数。用户询问如何在Triton上定义这些值，或者在Sagemaker上传递这些值。这个问题实质上是在问如何在不同环境中配置TensorRT-LLM的运行时参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/2489
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在Windows系统上安装的问题。由于缺少特定的Python模块和CUDA库文件，导致安装过程中出现了模块未找到和文件未找到的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2488
这是一个bug报告，主要对象是TensorRT-LLM的输出流模式，由于一个中文字符使用多个令牌，每个令牌在输出流期间被分别解码，导致一些中文字符被解码为乱码。

https://github.com/NVIDIA/TensorRT-LLM/issues/2487
这个issue是一个bug报告，主要涉及TensorRT-LLM中int4性能不如预期的问题。原因可能是硬件或软件配置问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2486
这是一个Bug报告，主要问题涉及TensorRT-LLM中penaltyKernels.cu存在的不一致性，导致输出logits不符合预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/2485
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的clamp in-place操作。由于clamp in-place操作无法直接修改weight_scales张量，导致通过SmoothQuant获取的检查点中出现NaN值。

https://github.com/NVIDIA/TensorRT-LLM/issues/2484
这个issue是关于bug修复的，主要涉及的对象是TensorRT-LLM中的lora模块，作者提出了hidden_size不等于num_heads * head_size时lora功能无法正常工作的问题，最终确定是因为尺寸不匹配导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2483
这是一个bug报告，主要涉及TensorRT-LLM在构建QwenVL时失败的问题。原因可能是代码错误或环境配置问题导致了构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2482
这是一个bug报告，涉及的主要对象是Medusa在TensorRT-LLM上的性能随着批处理大小增大而下降的问题。由于批处理大小增大导致性能下降的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2481
这是一个关于安装TensorRT-LLM在Python3.11中的问题，属于用户提出需求类型，主要涉及TensorRT-LLM库的安装。由于TensorRT-LLM可能尚不支持Python3.11版本，导致用户无法顺利安装。

https://github.com/NVIDIA/TensorRT-LLM/issues/2480
这是一个 bug 报告，主要涉及 TensorRT-LLM 中的 model_spec 模块，导致出现了无法定位模块的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2479
这是一个bug报告，主要涉及TensorRT-LLM项目在构建过程中出现的错误。原因可能是在执行命令`make -C docker release_build`时，出现了非零退出代码的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2478
这是一个bug报告，讨论的主要对象是在TensorRT-LLM下的使用问题。由于使用vicuna 33B和其训练好的medusa heads时出现了转换错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2477
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中attention的使用。由于用户想要单独使用attention类，但受到了默认网络设置的影响，导致需要指导如何解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/2476
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的trtllmbench工具，导致问题的原因可能是MPI_Info key参数错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2475
这个issue类型是bug报告，主要涉及TensorRT-LLM在构建过程中出现的链接错误`undefined reference to '__libc_single_threaded'`，可能是由于环境配置不正确或依赖库版本不匹配导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2474
这是一个关于需求的issue，主要涉及TensorRT-LLM引擎中的确定性相关问题。用户提出了关于如何实现高吞吐量确定性生成的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2473
这是一个bug报告，主要涉及TensorRT-LLM中的设置问题。由于存在拼写错误，导致在使用张量并行性时设置不正确，需要修复该错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2472
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下是否支持部署4bit quantised unsloth Llama模型，用户想要知道是否可以使用tensorRTLLM后端来部署这个模型，是否有相关文档可供参考。

https://github.com/NVIDIA/TensorRT-LLM/issues/2471
这是一个bug报告，涉及TensorRT-LLM中convert_checkpoint功能无法成功运行的问题。由于convert_checkpoint脚本失败导致用户遇到错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2470
这是一个bug报告，涉及TensorRT-LLM中的.tensor shape错误和模型运行时的hidden_size计算错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2469
这是一个bug报告，主要涉及到在构建TensorRT-LLM时遇到的性能问题，由于编译过程中耗时过长且出现了与所需架构不相关的错误，导致构建时间超过预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/2468
这是一个缺少具体内容的issue，需进一步补充信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2467
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的模型转换问题。由于某种原因导致convert_checkpoint.py运行时出现了错误提示，用户希望了解Llama3.2 3B模型转换是否被支持以及何时可以支持该转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/2466
这个issue是关于性能问题的bug报告，主要涉及TensorRT-LLM下的批处理功能，由于批处理大小增加导致延迟线性增加。

https://github.com/NVIDIA/TensorRT-LLM/issues/2465
这是一个用户提出需求的issue，主要涉及vLLM和TensorRTLLM在`transformers`版本不一致导致无法共同使用的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2464
这是一个用户提出需求的issue，主要涉及添加一个名为"blossom-ci.yml"的文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/2463
这是一个关于bug报告的issue，主要涉及的对象是在尝试转换Deepseek-V2-Lite模型时遇到的错误。原因是Deepseek-V2-Lite中的q_lora_rank为None且与DeepSeekV2中不同，导致无法成功转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/2462
这个issue属于bug报告类型，涉及主要对象是TensorRT-LLM的enable_kv_cache_reuse功能。用户提出了如何验证enable_kv_cache_reuse是否正确工作的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2460
这个issue属于更新提议，主要涉及了TensorRT-LLM的模型支持和API更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2459
这是一个关于功能需求的问题，涉及的主要对象是TensorRT-LLM中的MultiModalRunner.py，用户尝试在其中使用enableBlockReuse参数但遇到错误，询问该功能在多模态情况下是否可用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2458
这是一个用户提出需求的问题，涉及主要对象是TensorRT-LLM下的模型加载和量化转换，用户希望能够直接从huggingface加载量化模型并转换为TensorRTLLM checkpoint或engine，而不需要经过校准过程。

https://github.com/NVIDIA/TensorRT-LLM/issues/2457
这个issue是一个bug报告，涉及的主要对象是在Windows系统上使用TensorRT-LLM的Executor功能。报告中提到使用`tensorrt_llm/executor/executor.h`时出现了`unresolved external symbol`错误，可能由于链接库相关的配置问题导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2456
这是一个关于性能测试的问题，主要涉及的对象是TensorRT-LLM中的KV cache re-use。原因可能是KV cache重用对于低请求速率下平均序列延迟的影响较小，用户对于这种现象感到困惑并寻求解释。

https://github.com/NVIDIA/TensorRT-LLM/issues/2455
这是一个bug报告，涉及的主要对象是使用TensorRT-LLM下的模型在更改CUDA版本后出现运行失败的情况。由于CUDA版本不匹配，导致triton server容器无法正常启动。

https://github.com/NVIDIA/TensorRT-LLM/issues/2454
这是一个bug报告，主要涉及TensorRT-LLM中使用INT4 GPTQ时转换模型失败的问题。由于transformer版本和TensorRTLLM版本不匹配，导致无法成功转换保存模型检查点。

https://github.com/NVIDIA/TensorRT-LLM/issues/2453
这是一个关于功能需求的问题，主要涉及TensorRT-LLM中的在线微调和In-flight batching功能如何同时启用的问题。由于TensorRT的Executor不兼容在线微调功能，而CPG Runtime不支持In-flight batching功能，用户想知道是否可以在当前版本中同时启用这两项功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2452
这是一个bug报告，涉及到TensorRT-LLM版本0.14.0中的一个bug，用户在运行TensorRT-LLM示例时遇到了无法调用runner.generate的错误。错误导致类型不兼容，可能由于参数传递错误引起。

https://github.com/NVIDIA/TensorRT-LLM/issues/2451
这是一个bug报告issue，主要涉及TensorRT-LLM下建立chatglm3_6b时出现内存不足的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2450
该issue类型为用户询问问题，主要涉及对象是重复使用Gemini模型，疑问是如何进行量化。

https://github.com/NVIDIA/TensorRT-LLM/issues/2449
这是一个bug报告，涉及TensorRT-LLM的安装和构建过程，出现了"AttributeError: 'PretrainedConfig' object has no attribute 'n_audio_ctx'"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2448
这是一个bug报告，主要涉及TensorRT-LLM下的NCCL错误导致无法进行cpp性能基准测试的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2447
这是一个bug报告，涉及主要对象为TensorRT-LLM下的使用--use_fp8_context_fmha选项的版本0.13.0和0.14.0。问题是在尝试使用--use_fp8_context_fmha选项时遇到错误，无法正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2446
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的INT8 KV Cache功能。由于之前的计算方式不适用于Qserve，并且将k和v的scale合并为一个kv_cache_scaling_factor导致了问题，作者修改计算方式后获得更高质量的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2445
这是一个bug报告，涉及TensorRT-LLM下的模型量化构建失败的问题。由于可能的软件版本不匹配或参数配置错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2444
这个issue是一个功能请求，主要涉及Qserve和int8_kv_cache的集成问题。由于int8_kv_cache与Qserve的集成尚未实现，用户希望团队能够检查并实现这一功能，以便在TensorRTLLM上使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2443
这个issue类型为bug报告，主要涉及TensorRT-LLM的后端在禁用kv缓存时出错的问题。由于禁用kv缓存导致triton出错，并且trtllm的批大小始终为1。

https://github.com/NVIDIA/TensorRT-LLM/issues/2442
这是一个bug报告，主要涉及TensorRT-LLM下的一个Assertion failed错误，用户想要将'no_repeat_ngram_size'设置为0，但导致了assertion错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2441
这是一个关于性能问题（Throughput dropped by nearly half with just 1 LoRA）的bug报告，主要涉及TensorRT-LLM下的服务器性能与LoRA相关的问题。由于使用LoRA时出现性能严重下降，用户请求有关使用多个LoRA时的benchmark报告以及为何性能下降的建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/2440
这是一个需求报告类型的 issue，主要涉及的对象是将 RoBERTa 模型部署到 Triton 服务器并进行推理。问题产生的原因是缺乏关于如何在 TensorRT-LLM 下进行 BERT 模型的推理的详细指南。

https://github.com/NVIDIA/TensorRT-LLM/issues/2439
这是一个bug报告，该问题涉及TensorRT-LLM中的模型构建过程，存在Executor多次调用logit processor导致输出异常的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2438
这是一个用户提出需求的issue，涉及的主要对象是TensorRT-LLM中的FA V2（Flash Attention Version 2），用户想了解为什么在预生成阶段使用FA V2，但在生成阶段却不使用，是否是因为Flash attention在解码阶段没有显著的性能提升。

https://github.com/NVIDIA/TensorRT-LLM/issues/2436
这是一个bug报告，主要涉及的对象是TensorRT-LLM的kernel `moeTopK()`出现了问题导致无法找到 "。"。

https://github.com/NVIDIA/TensorRT-LLM/issues/2435
这是一个bug报告，主要涉及tritonserver与`TensorRT-LLM/examples/run.py`性能差异大的问题。导致这种现象可能是由于系统环境、代码实现或参数配置等方面的原因造成的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2434
这是一个bug报告，主要涉及对象是TensorRT-LLM中的数据类型错误问题。由于将模型和lora同时转换为bfloat16时会出现错误，但将它们同时转换为float16时却能正常工作，这可能是由于数据类型不匹配导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2433
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的CUDA核心gemm计算。产生问题的原因是当权重维度n或k不是2的幂时，会导致cudaCoreGemm内核的精度异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/2432
这是一个用户提出需求的issue，主要涉及需要将structured text decoding library outlines集成为TensorRTLLM的一个dependency。问题的原因是希望通过 outlines 的 LogitsProcessor 解决 TensorRTLLM 在大批量大小下的吞吐问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2431
这是一个用户提出需求的issue，主要对象是为量化模型添加支持LORA推理的功能。提出需求的原因是用户希望在AWQ 4bit量化模型的推理时使用LORA，但目前该功能尚不支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2430
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中的`trtllmbuild`命令忽略了`model_cls_file`和`model_cls_name`导致无法完成引擎文件生成。导致问题的原因是缺少了在`MODEL_MAP`中相应条目的场景。

https://github.com/NVIDIA/TensorRT-LLM/issues/2429
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的trt_build操作失败的问题，可能由于CUDA错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2428
这是一个bug报告，主要涉及TensorRT-LLM中构建模型引擎失败的问题，可能是由于CUDA错误导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2427
这是一个bug报告，涉及的主要对象是Dit（TensorRT-LLM中的一个模块），由于Dit不支持pp_size大于1，导致了该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2426
这是一个关于TensorRT-LLM的bug报告，用户在初始化MPI时遇到问题，希望能成功导入tensorrt_llm。

https://github.com/NVIDIA/TensorRT-LLM/issues/2425
这是一个文档问题，主要涉及的对象是TensorRT-LLM文档。由于其中的ENABLE_FDL被误写成了ENABLE_PDL，导致了一个小的拼写错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2424
该issue是关于用户询问如何通过C++执行器API来支持在TensorRT-LLM中使用草稿模型进行推理。

https://github.com/NVIDIA/TensorRT-LLM/issues/2423
这是一个关于部署TensorRT-LLM的问题，用户提出了需求，寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2422
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中运行benchmark时出现错误，错误原因是在输入输出长度为1024，128且批量大小大于等于512时，张量容量超过限制2147483647。

https://github.com/NVIDIA/TensorRT-LLM/issues/2421
这是一个用户提出需求的issue，主要涉及的对象是是否支持FLUX。由于当前的TensorRT-LLM不支持多GPU并行，用户询问是否有计划在未来支持FLUX。

https://github.com/NVIDIA/TensorRT-LLM/issues/2420
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的qwen 2-1.5B模型构建错误。由于某些原因导致在加载模型权重时出现了错误，导致了构建错误的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2419
这是一个关于TensorRT-LLM下的一个bug报告issue，主要涉及到版本不匹配导致的问题，报错信息是"Assertion failed: Must set crossKvCacheFraction for encoder-decoder model"。

https://github.com/NVIDIA/TensorRT-LLM/issues/2418
这是一个缺少文件的bug报告，涉及的主要对象是TensorRT-LLM项目。由于缺少某些文件，导致用户在使用时遇到问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2417
这个issue是关于bug报告，涉及的主要对象是TensorRT-LLM中的CUDA运行时错误，导致在启用kv缓存重用时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2416
这是一个bug报告，涉及到在使用TensorRT-LLM时出现了模块导入错误的问题。原因可能是由于模块路径设置不正确或环境配置问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2415
这是一个用户提出需求的issue，主要涉及的对象是要在TensorRT-LLM中加入ColBERT模型。由于用户认为ColBERT的密集检索方法可能是一个有价值的补充，他想评估在现有框架中集成它的可行性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2413
这是一个包含bug修复、新特性添加和API更改的issue，主要涉及TensorRT-LLM模型的更新和改进。由于一些构建相关的问题导致了BERT模型的加载问题和执行示例的失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2412
这是用户提出需求的 issue，主要涉及将Finetuned Llama模型导出为TensorRT格式，但由于相关资源和信息有限，用户寻求相关实现方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/2411
这是一个关于模型输出一致性的Bug报告，主要涉及TensorRT-LLM中的生成模型的输出结果稳定性问题。原因在于用户希望了解如何在固定的temperature和top_p设置下生成不确定性结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2410
这是一个用户提出需求的issue，主要对象是关于更新LLM API参考页面。原因可能是现有API参考页面内容需要更新或完善。

https://github.com/NVIDIA/TensorRT-LLM/issues/2409
这是一个文档问题，主要涉及TensorRT-LLM下的文档内容。由于文档问题可能存在错误或不清晰之处，用户提出了修复文档问题的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2408
这个issue属于用户提出问题类型，主要涉及TensorRT-LLM框架中的`enable_context_fmha`和`use_paged_context_fmha`参数的工作原理及其对模型性能的影响，用户想了解为什么chunked context需要两者一同使用，以及`use_paged_context_fmha`是否影响解码阶段的attention kernel。

https://github.com/NVIDIA/TensorRT-LLM/issues/2407
这是一个bug报告，涉及到TensorRT-LLM下的`run.py`脚本，在运行性能对比时出现了不一致的结果，原因是`--run_profiling`选项无法正确禁用停止标记导致生成长度不一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2406
这是一个bug报告，主要涉及TensorRT-LLM下logprobs输出为0.000的问题，可能原因是代码或者模型推理逻辑中的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2405
这是一个关于软件bug报告的issue，涉及到TensorRT-LLM库的导入问题。由于使用conda安装mpi4py可能导致导入tensorrt_llm在初始化MPI时出现卡住的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2404
这个issue类型是文档更新请求，涉及主要对象为项目的Github Pages。由于项目文档或网页内容需要更新，所以提出这个请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2402
这是一个bug报告，主要涉及TensorRT-LLM在V100 GPU上出现Segmentation fault (11)的问题。可能是由于TensorRT版本不兼容Python 3.12导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2401
这是一个更新请求，涉及TensorRT-LLM的API改动和功能增强，主要对象是TensorRT-LLM的v0.14.0版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2400
这是一个bug报告，涉及的主要对象是TensorRT-LLM库。导致该问题的原因是目标GPU架构SM 70在此TensorRT版本中不受支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2399
这是一个关于bug报告的issue，涉及的主要对象是TensorRT-LLM下的llava模型。由于环境和版本不匹配导致执行失败，需要寻求针对此问题的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2398
这是一个Bug报告，涉及TensorRT-LLM中的T5模型运行期间出现内存溢出的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2397
这是一个用户需求问题，涉及主要对象为PyTorch，提出了停止在其命名空间内导出optional的要求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2396
这个issue是关于需求的，主要涉及的对象是kernel的重写，用户寻求在不参考cutlass实现的情况下如何重写这个kernel。

https://github.com/NVIDIA/TensorRT-LLM/issues/2395
该issue类型是bug报告，涉及TensorRT-LLM的性能问题，用户提出0.13.0版本性能比0.12.0版本差的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2394
这是一个需求报告，主要对象是在TensorRT-LLM中添加对interlvl2的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2393
这个issue类型是bug报告，涉及的主要对象是在TensorRT-LLM下使用mpirun和cpp示例时出现了非法内存访问。导致这个问题的原因可能是在构建引擎时参数设置不当导致内存不足，导致推理失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2392
这是一个bug报告，涉及主要对象为Qwen2-72B w4a8模型，用户在使用Quantization过程中遇到输出为空的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2391
该问题单属于更新需求，主要涉及的对象是TensorRT-LLM项目。由于缺少具体的更新内容，用户希望得到最新消息的更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2389
这是一个更新（feature update）类型的issue，涉及主要对象是TensorRT-LLM。由于TensorRT 10.5 中构建 MLLaMa model 存在已知问题，导致用户需要将 `tensorrt` 包版本降级到 10.4.0 作为解决方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/2388
这是一个bug报告，涉及TensorRT-LLM中的convert_checkpoint.py失败的问题，用户寻求关于转换模型的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2387
这是一个bug报告，主要涉及Medusa支持编码/解码模型的问题。由于Cross attention无法使用Medusa，用户遇到了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2386
这是一个Bug报告，涉及的主要对象是TensorRT-LLM下的一个Python脚本。由于缺少`init_all_reduce_helper()`函数在代码中的调用，导致无法正确运行allreduce操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2385
这是一个bug报告，主要涉及TensorRT-LLM中转换checkpoint时出现的Flash attention问题。造成这个问题的原因可能是环境配置或库版本不兼容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2384
这是一个功能需求提出的issue，主要对象是TensorRT-LLM中的attention mechanism。这个需求是为了让用户在加载模型时可以选择是否使用注意力机制，以适应旧版GPU的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2383
这是一个用户提出疑问的类型，主要涉及stop_words_list和end_id之间的关系。用户询问他们是否有必要同时使用stop_words_list和end_id，以及如果他们在stop_words_list中提到了end_id是否有价值。

https://github.com/NVIDIA/TensorRT-LLM/issues/2382
这是一个bug报告，主要对象是在TensorRT-LLM中的`qwen/convert_checkpoint.py`，问题是关于`load_model_on_cpu`选项未能在转换`qwen`时传播的缺陷。

https://github.com/NVIDIA/TensorRT-LLM/issues/2381
这是一个bug报告，主要涉及到在使用TensorRT-LLM在4个NVIDIA A100上运行Nemotron-51B时出现CUDA内存不足错误的问题。原因可能是推理过程中内存分配错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2380
这是一个bug报告，涉及的主要对象是TensorRT-LLM。这个问题产生的原因可能是安装或导入过程中缺少了正确的依赖项。

https://github.com/NVIDIA/TensorRT-LLM/issues/2379
这是一个bug报告，主要涉及TensorRT-LLM下的build bert模块无法读取模型的问题，可能是由于代码中的某一部分导致无法正确读取模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2378
这是一个bug报告，涉及到TensorRT-LLM中网络导出功能的问题，由于缺少必要的import导致无法正确使用`visualize_network` flag导出ONNX，从而产生问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2377
这是一个bug报告，主要涉及TensorRT-LLM下的FP8转换失败的问题，由于使用Mixtral 8x7B模型时在rowwise FP8转换过程中发生崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/2376
这是一个bug报告类型的issue，涉及的主要对象是TensorRT-LLM和NeMo Framework。由于在构建引擎时，无法使用"multi-rank nemo LoRA"检查点导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2375
这是一个关于安装TensorRT-LLM时出现的bug报告，主要涉及到TensorRT-LLM模块缺失的问题，导致无法完成安装和测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/2374
这是一个关于性能调优的问题，主要涉及TensorRT-LLM的benchmark及相关参数配置。由于某些设置，导致输出中的avg_inter_token_latency为0。

https://github.com/NVIDIA/TensorRT-LLM/issues/2373
这是一个bug报告，主要涉及TensorRT-LLM中构建BERT模型的问题。这个问题可能是由于TensorRT版本0.13存在bug或者不完整的功能导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2372
这是一个bug报告，涉及到TensorRT-LLM中XQA kernel在H100上使用fp8 kv比fp16 kv速度慢的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2371
这是一个用户提出需求的类型，主要涉及TensorRT-LLM的多LoRA设置集成问题。造成问题可能是由于引擎构建时的参数配置或文件路径设置不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/2370
这是一个bug报告，主要对象是在使用SmoothQuant对Qwen2模型进行量化时出现的错误。由于mlp.proj没有正确分割，导致bug的症状是无法正确拆分该部分模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2369
这个issue是一个bug报告，主要涉及的对象是TensorRT-LLM中的ONNX导出功能。导致该问题的原因是未识别的命名空间导致ONNX导出失败，用户寻求关于无法识别flash_attn::_flash_attn_forward命名空间的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2368
这是一个bug报告，主要涉及TensorRT-LLM中对于自定义操作符导出到ONNX格式时出现错误的问题。原因是相关操作符的命名空间未被正确注册所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2367
这是一个关于性能问题的bug报告，主要涉及TensorRT-LLM中生成速度下降的问题，可能是由于使用return_log_probs功能导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2366
这个issue是关于bug报告，涉及的主要对象是TensorRT-LLM中LoRA模块的适配器。这个问题是由于HuggingFace格式中加载/转换LoRA适配器时不支持不同维度的`r`属性，导致用户无法加载具有任意秩的LoRA适配器。

https://github.com/NVIDIA/TensorRT-LLM/issues/2365
该issue类型为功能需求，主要涉及TensorRT-LLM中的fast-forward tokens in logits post processor功能。由于目前无法在输出阶段追加多个token到序列中，用户提出需要增加这一功能以提高性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2363
这个issue类型是功能更新和Bug修复，主要涉及的对象是TensorRT-LLM模块。由于添加了新模型支持、新特性、修改了API默认设置以及修复了一些问题，用户可能提出了需求，或者报告了相关Bug，寻求更新和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/2362
这是一个关于bug报告的issue，涉及主要对象是TensorRT-LLM下的多模型运行。由于Python-binding-C++和Python runtime之间的结果不一致，导致C++结果偶尔出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2361
这是一个需求类型的issue，主要涉及对象为c++ inference example。由于缺乏具体的C++示例，用户询问是否可以提供一个包含输入和输出单词的示例。

https://github.com/NVIDIA/TensorRT-LLM/issues/2360
这是一个bug报告，问题涉及的主要对象是在运行TensorRT-LLM时出现的错误。可能是由于环境配置不正确或者依赖项缺失导致无法成功编译发布版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2359
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM库中的C++ API。用户想要了解如何使用C++ API设置ptuning提示嵌入表。由于C++ API中缺少设置提示嵌入表和词汇表大小的功能，用户希望找到解决方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/2358
这是一个bug报告，涉及主要对象是TensorRT-LLM。由于在使用多个GPU部署服务时，如果第一个请求是图像请求，会导致前向推理函数出现错误，出现的原因可能是CUDA运行时错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2357
这是一个bug报告，主要涉及TensorRT-LLM的OpenAPI服务器出现错误。原因可能是配置引擎模型后无法执行推理操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2356
这是一个bug报告。该问题涉及TensorRT-LLM下的convert_checkpoint.py脚本的转换错误。可能由于代码中对模型权重的量化处理出现了错误，导致了此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2355
这是一个bug报告，涉及的主要对象是尝试在单个A100 80GB GPU上构建和运行nvidia/Llama3_1Nemotron51BInstruct模型。由于A100 GPU上无法成功降低内存请求，导致了该问题的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/2354
该issue属于bug报告类型，主要涉及的对象是"C++测试脚本(test_cpp.py)"。导致这个问题的原因是在构建引擎时无法找到"model_spec.so"，最终导致了无法成功构建的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2353
这个issue类型是用户提出需求，主要涉及对象是在TensorRT-LLM中寻找enqueue_requests函数实现以及qwen模型的输入头文件代码，以及trtllmbuild执行的入口函数。用户提出问题的原因可能是希望找到相应的代码实现并缺少相关文档信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2352
这是一个bug报告，主要涉及的对象是模型转换（model conversion），由于未能正确处理用户传入的gpt_variant参数，导致模型转换出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2351
这是一个关于性能问题的bug报告，主要涉及了TensorRT-LLM下的Int8 Gemm算法性能在实际模型中下降的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2350
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中的free_gpu_memory_fraction参数不起作用的问题。这个问题可能由于代码实现上的错误或者配置设置不正确而导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2349
这是一个用户提出需求的 issue，主要涉及到TensorRT-LLM中的in-flight batch strategy功能。问题可能由于缺乏关于该策略实现的教程而导致用户难以理解这一技术。

https://github.com/NVIDIA/TensorRT-LLM/issues/2348
这是一个bug报告，涉及的主要对象是Docker构建过程。由于在构建docker镜像时使用了未知的标志"--trt_root"，导致产生了错误，并且提示了对应的帮助信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2347
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的trtllm-bench脚本。该问题可能是由于在v0.13.0版本中缺少'tensorrt_llm.bench.datamodels'模块而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2346
这是一个关于Bug报告的issue，主要涉及对象是TensorRT-LLM下的\_SyncQueue类属性错误，由于\_SyncQueue类中result方法中的代码逻辑问题，导致了出现了相关症状的Bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2345
这是一个关于功能需求的 issue，主要涉及到TensorRT-LLM中的Eagle实现状态。这个问题由于目前在最新版本中尚未看到与Eagle Speculative decoding 相关的运行时代码，导致用户提出了关于这种模式是否会很快得到支持的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2344
这是一个bug报告issue，涉及的主要对象是TensorRTLLM中的convert_checkpoint.py脚本。由于未得到预期的输出，用户报告在转换Gemma hf格式时出现了"Killed"信息，请求帮助解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/2343
这是一个用户提出需求的issue，主要涉及LLM下的readme文档，原因是需要指定LLama 3.x信息并更新VLM支持情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2342
这个issue属于用户请教问题类型，主要涉及TensorRT-LLM在TTS中的使用情况，用户想知道在TTS的llm代码中，是否适合使用TensorRT-LLM，建议参考哪个Demo。

https://github.com/NVIDIA/TensorRT-LLM/issues/2341
这是一个关于代码逻辑问题的issue，主要对象是代码中的MedusaDecodingLayer模块。由于Medusa并未在runner中执行dynamic_decoder，导致用户产生了疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2340
这是一个Bug报告，涉及对象是DeepseekV2ForCausalLM在TensorRT-LLM中的支持，用户提出了希望支持DeepseekV2的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2339
这是一个bug报告，主要涉及TensorRT-LLM下的checkpoint转换脚本（convert_checkpoint.py），由于UCP版本不兼容导致转换失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2338
这是一个bug报告，涉及Whisper Encoder 在使用 Executor API 时出现的问题。由于对位置嵌入的变化导致了错误的引入，可能导致无法成功完成请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2337
这个issue属于bug报告类型，主要涉及TensorRT-LLM中使用mpirun时hang up的问题，原因可能是MpiComm.local_init() 函数导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2336
这是一个用户提出需求的issue，主要涉及的对象是支持qwen2.5模型，由于目前不支持该模型，用户希望在TensorRT-LLM中添加对qwen2.5模型的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2335
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的GPU内存泄漏问题，可能由于max_tokens设置为1且gather_all_token_logits参数导致请求无法正常结束。

https://github.com/NVIDIA/TensorRT-LLM/issues/2333
这个issue是关于功能更新和bug修复的。主要涉及的对象是TensorRT-LLM模型库。这个issue的内容包括了对模型支持的更新、新增功能的介绍、API的变更、bug修复等方面的描述。

https://github.com/NVIDIA/TensorRT-LLM/issues/2332
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM中的版本v0.13.0，由于未找到 `builder_config` 导致了报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2331
这是一个关于性能比较和可能优化的issue，主要涉及Expert Parallelism (EP)和Tensor Parallelism (TP)的比较。原因是由于现有实现导致在某些情况下TP总是优于EP，但提出了可能通过专用alltoall EP实现来提高性能的猜想。

https://github.com/NVIDIA/TensorRT-LLM/issues/2330
这是一个bug报告类型的issue，涉及到TensorRT-LLM下的AWQ 4bit模型，用户反映量化后模型占用内存较多的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2329
这是一个bug报告，主要涉及TensorRT-LLM下的AWQ速度不增加的问题，用户希望在使用AWQ时获得更高的tokens per second。

https://github.com/NVIDIA/TensorRT-LLM/issues/2328
这是一个关于使用 Executor 替代 GptManager 的问题讨论，属于用户提问类型，主要涉及 GptManager 和 Executorl，原因可能是用户想了解为什么版本发布时采用 Executor 而不是 GptManager。

https://github.com/NVIDIA/TensorRT-LLM/issues/2327
这是一个bug报告，涉及的主要对象是在尝试使用awq对gemma 2 9B模型进行量化时出现了错误。原因可能是由于awq无法成功量化导致了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2326
这是一个用户需求问题，用户想知道在TensorRT-LLM中是否有办法从attention中获取QK分数。

https://github.com/NVIDIA/TensorRT-LLM/issues/2325
这个issue是一个bug报告，主要涉及的对象是在Linux系统下构建TensorRT镜像。由于某些原因导致问题无法解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/2324
该问题属于Bug报告类型，主要涉及进一步的TensorRT-LLM中的Gemina 2 9B模型构建问题。由于不支持激活函数gelu_pytorch_tanh而导致了该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2323
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于SyncQueue对象缺少'get'属性，导致了AttributeError异常的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2322
这个issue类型为用户提出需求，请教问题，主要涉及对象为C++ Executor Leader Mode。用户询问是否可能在一个GPU设备上有一个进程内多个处于Leader模式的Executor。

https://github.com/NVIDIA/TensorRT-LLM/issues/2321
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的LogitsPostProcessorConfig。用户提到配置未生效，怀疑初始化过程存在问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2320
这是一个bug报告，涉及到TensorRT-LLM对Llama3.2的支持问题，用户提到在尝试构建引擎时遇到了错误。可能与上下文长度有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/2319
这是一个bug报告，涉及TensorRT-LLM中Quantizate功能无法成功执行的问题，用户更改了convert.py文件后可以成功转换checkpoint，但在尝试使用trtllmbuild构建引擎时却遇到新的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2318
这是一个功能需求请求类型的issue，涉及主要对象为executor::Response。由于prepopulatedPromptLen字段为私有对象，导致用户无法正常访问，希望能在createResponse函数中添加mPrepopulatedPromptLen字段。

https://github.com/NVIDIA/TensorRT-LLM/issues/2317
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中用于Triton服务器的Llama3和Llama3.1模型在内存使用上出现显著差异的问题，用户想了解导致这种差异的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/2316
这是一个bug报告，主要涉及TensorRT-LLM在Slurm环境下的使用问题。这个问题可能是由于缺乏关键说明导致用户在Slurm环境中使用TensorRT-LLM时出现挂起的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2315
这是一个bug报告，主要涉及TensorRT-LLM下的代码编译问题。该问题由于GCC13需要包含一些额外的头文件才能成功构建，导致编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2314
这个issue类型是bug报告，涉及的主要对象是关于TensorRT-LLM中qwen2 0.5b模型的GPU内存使用和推理速度问题。这个问题可能由于模型转换过程中的参数设置不正确或者代码实现中的问题导致的不符合预期的推理结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2313
这是一个bug报告，涉及的主要对象是Phi-3-mini-128k。由于环境配置或依赖项不匹配导致了报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2312
这是一个关于需求的问题，主要涉及TensorRT-LLM中使用fused multi-head attention时数据精度下降的问题。由于需求是希望保持数据精度以提高准确性，用户提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2311
该issue类型为实验结果分析，主要涉及了TensorRT-LLM中的混合批处理功能。原因可能是混合批处理并没有按照预期的方式工作，导致实验结果与预期不符。

https://github.com/NVIDIA/TensorRT-LLM/issues/2310
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的模型转换问题。由于在执行转换模型的命令时出现了错误，导致了这个问题的提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2309
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM中的kv cache quant功能。原因可能是kv cache quant导致模型精度严重下降，相比之下其他功能的影响较小。

https://github.com/NVIDIA/TensorRT-LLM/issues/2308
该issue类型为用户提出需求，主要涉及对象为TensorRT-LLM，用户想了解是否能仅使用CPU进行推理。

https://github.com/NVIDIA/TensorRT-LLM/issues/2307
这是一个用户提出需求的issue，主要涉及到如何在TensorRT-LLM中实现最大GPU利用率以及如何使用多个LLM模型和嵌入模型。用户询问如何通过TensorRT-LLM来管理GPU资源以及是否有计划实现支持Triton托管模型的API。

https://github.com/NVIDIA/TensorRT-LLM/issues/2306
这是一个用户提出需求的issue，主要涉及Python Executor API的实现，需要添加request interruption功能，可能由于需要在程序执行过程中进行中断操作而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2305
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下推断出现Hang的问题，根据描述可能是由TP或PP参数设置不当导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2304
这是一个用户提出需求的类型的issue，主要涉及的对象是如何在TensorRT-LLM中实现和使用自定义的核心，或者如何有效地在两个GPU之间传输kv_cache。导致该问题的原因可能是用户想要了解如何实现类似于https://github.com/microsoft/mscclpp的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2303
这是一个bug报告，主要涉及TensorRT-LLM下的llama examples无法运行的问题。由于run.py文件出现错误，导致所有尝试运行的模型均失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2302
这是一个bug报告，主要涉及TensorRT-LLM中的LLM 3.2版本的转换失败问题，由于尚未支持3.2版本导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2301
这个issue属于bug报告类型，主要涉及TensorRT-LLM在退出时崩溃的问题，用户遇到了类似错误消息的结束。

https://github.com/NVIDIA/TensorRT-LLM/issues/2300
这是一个性能问题的issue，主要涉及的对象为TensorRT-LLM中的W4A8 throughput on Hopper GPU。由于W4A8_AWQ的吞吐量较低，用户怀疑是测试方法或W4A8_AWQ中计算核的性能较低所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2297
这个issue属于bug报告类型，涉及主要对象为TensorRT-LLM。原因是由于TensorRT 10 默认使用`strongly_typed=True`构建fp32 vision engines，即使输入ONNX文件是fp16，导致构建fp16 vision engines的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2296
该issue属于用户提出问题的类型，主要涉及AllReduce、AllGather和Send/Recv等异步操作的支持，用户希望了解Send/Recv kernel是否也是异步操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2295
这是一个bug报告，涉及TensorRT-LLM中kv cache重用在请求调度中未被考虑的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2294
这是一个描述bug类型的issue，主要涉及TensorRT-LLM中C++运行时的问题，用户在尝试使用executor API运行推理时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2293
这是一个关于修改权重数据格式的问题，需要适应row major格式。原因可能是在使用int8_sq_launcher时权重数据需要做适当的调整以正确处理数据。

https://github.com/NVIDIA/TensorRT-LLM/issues/2292
这是一个关于功能需求的issue，主要涉及对象是在deepstream pipeline中集成TensorRT-LLM。用户询问如何在deepstream中使用TensorRT-LLM引擎的具体方法及遇到的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2291
这是一个用户提出需求的issue，主要涉及的对象是在TensorRT-LLM项目中创建特定架构（sm87）的cubin.cpp文件。用户提问是由于缺少sm87架构的cubin.cpp文件，不知道如何创建这些文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/2290
该issue类型是文档更新，主要对象是高级文档。由于文档中存在一个小错误，导致需要进行修正。

https://github.com/NVIDIA/TensorRT-LLM/issues/2289
这是一个关于旧数据集需求的bug报告，问题涉及datasets版本要求是否仍然有效，可能是由于TensorRTLLM例子中对较旧版本的datasets有依赖，导致用户需要进行特殊设置以适应更更新的datasets版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2288
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM网站。由于缺少复制代码小部件，用户在网站上手动复制粘贴代码时出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2287
这是一个关于过时文档的issue，用户反馈文档内容过时且不适用于当前的cuda12时代，请求更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2286
这个issue是一个bug报告，主要涉及的对象是TensorRT-LLM项目中的mpi4py依赖版本问题，由于未固定mpi4py的版本导致加载TensorRTLLM时出现hang的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/2285
这个issue类型是用户提出需求，主要对象是文档。由于BF16缺失，用户建议添加，以完善文档内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2284
这是一个bug报告，主要涉及TensorRT-LLM中的ModelRunnerCpp模块，报告中指出出现了UnboundLocalError错误，原因是变量'vocab_size'在赋值前被引用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2283
这是一个bug报告，涉及的主要对象是在使用Lora adaptors与Mistral Nemo时出现的错误。由于使用最新的Transformer lib版本和应用于TRTLLM时出现的问题，该问题可能是由于软件版本不兼容或配置错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2282
这是一个bug报告，涉及的主要对象是BART LoRA的转换引擎，由于更改了相关参数后出现了“LoraParams and input dims don't match, lora tokens 1 input tokens 0”的错误提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/2281
这个问题是用户请教问题类型，涉及主要对象是Mistral和LLAMA，由于用户想知道是否可以将LLAMA的量化方法应用于Mistral，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2280
这是一个bug报告，主要涉及到无法下载TensorRT 10.4导致的404 Not Found错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2279
这是一个关于需求提问的issue，主要涉及的对象是TensorRT-LLM，用户想知道是否支持在RISCV上运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2278
这个issue属于bug报告，主要涉及TensorRT-LLM下构建INT8 Engine时遇到的问题，可能是由于量化模型过程中某些层未被正确量化导致无法成功构建TensorRT LLM Engine。

https://github.com/NVIDIA/TensorRT-LLM/issues/2277
这个issue是关于软件安装问题的bug报告，主要涉及TensorRT-LLM的安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2276
这是一个请求更新 gh-pages 页面的 issue，涉及的对象是项目的文档。原因可能是需要更新或改进项目的文档信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2275
这个issue属于bug报告类型，主要涉及Windows安装指南。由于未提供具体内容，无法分析具体原因及症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2273
这是一个包含Bug修复、更新和API支持的Issue，涉及TensorRT-LLM的更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2272
这是一个bug报告，涉及TensorRT-LLM下的C++运行时系统。由于输入在Whisper中被打包，导致输出结果错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2271
这是一个空内容的issue，类型为更新请求，涉及主要对象为Github Pages。

https://github.com/NVIDIA/TensorRT-LLM/issues/2269
这个issue类型是关于TensorRT-LLM v0.13版本更新的，主要涉及到新功能和增强。

https://github.com/NVIDIA/TensorRT-LLM/issues/2268
这个issue是关于bug报告类型，涉及的主要对象是C++ runtime中计算块数的代码。导致21G内存只有631块的情况可能是由于计算块数的算法或逻辑错误引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2267
这是一个bug报告，涉及TensorRT-LLM中的checkpoint转换功能，由于代码中可能存在错误，导致转换失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2266
这是一个bug报告类型的issue，主要涉及对象是在Jetson AGX Orin Developer Kit上构建TensorRT-LLM wheel时遇到的问题，原因是pynvml在Jetson上不起作用，导致无法进行测试和性能分析。

https://github.com/NVIDIA/TensorRT-LLM/issues/2265
这是一个关于bug报告的issue，主要涉及TensorRT-LLM对Python自定义层插件支持的问题。由于TensorRTLLM可能不支持Python自定义层插件，用户尝试注册Python自定义层插件时出现Segment Fault，需要进一步调试或示例以解决此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2264
这是一个bug报告，涉及的主要对象是在quantizing Llama模型时遇到冲突，导致`torch.Tensor`和`numpy`在代码中发生冲突，最终导致了进程中断。

https://github.com/NVIDIA/TensorRT-LLM/issues/2263
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM中的Lookahead decoding功能。由于多次调用runner.generate后，出现了非确定性和错误的响应。

https://github.com/NVIDIA/TensorRT-LLM/issues/2262
这是一个bug报告类型的issue，主要涉及的对象是FusedMHARunnerV2。由于参数构建可能不正确，导致无法产生正确的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2261
这是一个用户询问如何在TensorRT-LLM中运行多批次（multi-batch）的问题，请求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2260
这是一个bug报告，该问题涉及的主要对象是TensorRT-LLM下的模型Qwen27B。由于参数设置不正确，导致生成了重复的文本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2259
这是一个bug报告，涉及TensorRT-LLM中的echo功能缺陷导致"none prompt to string"的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2258
该issue类型为版本更新需求，涉及主要对象是代码库中的版本号。由于代码库需要升级至新版本，因此提出了更新版本号的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2257
该issue类型为用户提出需求，请教问题，主要涉及对象为如何向TensorRT-LLM中添加新的多模态模型。该问题由于缺乏相关文档或经验指导，导致用户寻求帮助以添加名为Qwen2的LLM模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2256
这是一个用户询问兼容性问题的类型，涉及主要对象是CUDA和TensorRT-LLM版本。问题是由于CUDA版本不匹配，用户想知道可以安装的最高版本的TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/2255
这是一个bug报告，主要涉及TensorRT-LLM中的"--use_paged_context_fmha"选项在特定配置下导致NaN值出现的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2253
这是一个功能更新的issue，主要涉及TensorRT-LLM模型的更新和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/2252
这是一个关于构造函数行为的问题，涉及的主要对象是`Executor`类。用户提出了关于对象生命周期管理的问题，是否需要手动管理传入构造函数的对象。

https://github.com/NVIDIA/TensorRT-LLM/issues/2251
这是一个bug报告，问题涉及TensorRT-LLM的输出重复的问题。导致这个问题的原因是设置max_output_len参数到一个较大的值时，模型会重复输出直到达到指定长度，而不是停止输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2250
这个issue属于用户提出需求的类型，主要涉及C++运行时如何支持新的多模态模型llava-one-vision，用户询问是否有参考文档，并提到了在执行指令时遇到的一些问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2249
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的encoder_repetition_penalty，用户询问是否在TensorRT-LLM的路线图中实现了该功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2248
这是一个bug报告，主要涉及TensorRT-LLM下的模型构建过程中出现的KeyError问题。由于缺少ChatGLMForConditionalGeneration模型的支持，导致程序无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2247
这是一个bug报告，涉及的主要对象是在TensorRT-LLM中运行模型转换时出现的错误。由于MPI的问题，导致出现"Invalid MIT-MAGIC-COOKIE-1 key"的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2246
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM代码，由于某种原因导致ModelRunneCpp的await_responses方法被阻塞。

https://github.com/NVIDIA/TensorRT-LLM/issues/2245
这是一个用户在询问功能支持的类型问题，主要涉及TensorRT-LLM是否支持input_embeds作为输入。原因可能是用户想要确认该功能在TensorRT-LLM中是否可用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2244
这是一个用户提出需求的issue，主要涉及的对象是在TensorRT-LLM中添加一份第三方推理速度仪表板。用户提出需求的原因是想要分享有关推理速度仪表板的社区链接资源，其中包括综合性的基准测试和各种优化技术。

https://github.com/NVIDIA/TensorRT-LLM/issues/2243
这是一个bug报告，主要涉及代码中未能支持使用自定义校准数据集导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2242
这是一个用户提出需求的issue，主要涉及对象为是否TensorRT-LLM支持llama3.1的序列分类。用户询问是否可以在TensorRT-LLM中使用llama3.1的序列分类，并询问在修改代码以添加此功能时是否会遇到障碍。

https://github.com/NVIDIA/TensorRT-LLM/issues/2241
这是一个关于模型准确率较低的bug报告，主要涉及TensorRT-LLM下的模型"QwenVLChat"。该问题可能是由模型对图片中的动作位置预测不准确导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2240
这是一个bug报告类型的issue，主要涉及TensorRT-LLM模型在不同batch size下的推理延迟线性增加的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2239
这是一个bug报告，主要涉及TensorRT-LLM中使用ModelRunnerCpp时设置early stopping为0时出现问题。由于设置了early stopping参数为0，导致无法正确进行生成文本的操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2238
这是一个bug报告，涉及到如何在TensorRT-LLM中设置使用Flash Attention 3的问题。用户安装了Flash Attention 2.6.3但似乎并未成功使用，导致在H100上运行TensorRTLLM时性能没有改善。

https://github.com/NVIDIA/TensorRT-LLM/issues/2237
该问题类型为用户提出需求，主要对象是在使用TensorRT-LLM时，用户发现与VLLM相比，使用TensorRT更加繁琐。原因是在使用TensorRT时需要进行多个步骤才能实现类似VLLM的简单功能，导致用户感受不便。

https://github.com/NVIDIA/TensorRT-LLM/issues/2236
这是一个bug报告，涉及的主要对象是gptSessionBench和gptManagerBench。由于处理batch_size的方式不同，导致了即使使用相同batch_size时，两者的行为也不同。

https://github.com/NVIDIA/TensorRT-LLM/issues/2235
这个issue是用户提出的需求。主要涉及的对象是TensorRT-LLM。用户希望在不同的GPU或不同的节点上自行配置预加载阶段和解码阶段，以支持自身应用需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2234
这是一个更新版本的issue，主要对象是TensorRT-LLM。由于开发人员想要将版本提升到`0.14.0.dev2024091700`。

https://github.com/NVIDIA/TensorRT-LLM/issues/2233
这是一个关于bug报告的issue，主要涉及到TensorRT-LLM下的gemm2-27b模型输出错误的问题，可能是由于建立gemm2-27b引擎和使用tensorrtllm后端发送请求时导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2232
这是一个bug报告类型的issue，主要对象是TensorRT-LLM中的"check_share_embedding"问题，由于tie_word_embedding设置为True导致lm_head.weight不存在于model.safetensors，进而出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2230
这个issue是一个bug报告，涉及到TensorRT-LLM在代码中存在的两个问题并进行了修复。原因是代码中出现了一处拼写错误和一处重复导入模块的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2229
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的fp8 rowwise支持，由于SM89不支持fp8 rowwise，导致在4090 GPUs上构建引擎失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2228
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM模型。由于程序输出中包含了"<|im_end|>"这样的标记，可能是因为模型在生成文本时出现了意外的结束标记导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2227
这是一个bug报告，主要涉及TensorRT-LLM中whisper-medium decoder编译阻塞的问题。这个问题的原因可能是转换和编译过程中的配置或代码错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2226
这是一个bug报告，涉及到TensorRT-LLM下的"use_embedding_sharing"选项不起作用的问题。导致这个问题的原因是在使用TensorRTLLM主分支时，部分权重不一致导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2225
这是一个bug报告，主要涉及LLM中的`host_cache_size`参数配置无效以及服务在持续推送推理请求后崩溃的问题。原因可能是内存受限导致配置不生效，并最终导致服务崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/2223
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的模型ChatGLM的多Lora支持，用户试图添加多Lora支持但遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2222
这是一个bug报告，涉及到TensorRT-LLM的构建问题，由于`epilogue_moe_finalize.hpp`中的常量值问题导致了编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2221
这是一个用户提出需求的issue，主要对象是支持florence2。可能是用户想要在TensorRT-LLM中使用florence2，但目前还不支持，所以提出了这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2220
这是一个关于使用nccl ops插件的需求问题，主要涉及到TensorRT-LLM的插件加载机制。用户想要了解如何在其项目中使用由TensorRT-LLM提供的nccl ops插件。

https://github.com/NVIDIA/TensorRT-LLM/issues/2219
这是一个bug报告，主要涉及`kv_cache_type`在CC中加载whisper解码器引擎时出现问题，导致无法加载。

https://github.com/NVIDIA/TensorRT-LLM/issues/2218
这个issue是关于用户提出需求，并询问如何处理在TensorRT-LLM中不支持的操作，主要涉及到TensorRT-LLM中无法直接支持的复杂数据类型和FFT操作的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2217
这是一个bug报告，主要涉及TensorRT-LLM下的GptManager和Executor，由于启用缓存重用导致生成异常标记并最终导致段错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2215
这个issue是一个功能更新的类型，涉及的主要对象是TensorRT-LLM。原因是更新添加了新功能并改变了默认设置，用户可能需要了解这些更改并进行相应的调整。

https://github.com/NVIDIA/TensorRT-LLM/issues/2214
这是一个bug报告，主要涉及TensorRT-LLM下无法构建Phi3 128k模型的量化int8模型。原因可能是与GPU虚拟化环境中使用的组件或设置有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/2213
这是一个优化提案，主要涉及TensorRT-LLM中的small-batched weight only quantization，通过使用shared memory和memcpy_async来减少全局内存负载延迟。

https://github.com/NVIDIA/TensorRT-LLM/issues/2212
这是一个bug报告，涉及的主要对象是Triton Inference Server。由于脚本无法启动 Triton Server，可能是由于命令错误或配置问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2211
这是一个用户提出需求的问题，主要对象是在TensorRT-LLM生成函数中是否能够直接传递input_embeds参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/2210
这是一个bug报告，涉及TensorRT-LLM中版本生成FP16模型的问题，问题可能与模型生成过程中的transformers更新有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/2209
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的multi-gpu error和MPI_Unknown_error问题。由于系统出现未知错误，导致chat.py运行时出现问题，用户请求解决方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/2208
这个issue是关于Bug报告的，涉及的主要对象是TensorRT-LLM中的Speculative Decoding模块。由于num_draft_tokens参数设置不同导致了模型输出不一致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2207
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的模型精度问题，用户在使用不同精度的模型引擎时发现输出结果不一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2206
这是一个bug报告，主要涉及到TensorRT-LLM中Qwen-VL-Chat出现错误的问题，可能是由于trtllm0.12.0与QwenVL在使用fp16和fp32精度时导致不一致的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2205
这是一个bug报告，主要涉及TensorRT-LLM安装和编译过程中出现的问题。由于在Docker中存在不同版本的CUDA，导致用户无法成功编译TensorRT Engine。

https://github.com/NVIDIA/TensorRT-LLM/issues/2204
这是一个bug报告，主要涉及TensorRT-LLM中的tensor体积超过限制的问题，导致无法使用FP8格式运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2203
这个issue属于bug报告，涉及的主要对象是TensorRT-LLM。由于多个进程同时发送请求导致CUDA runtime错误，出现" CUBLAS_STATUS_INTERNAL_ERROR"。

https://github.com/NVIDIA/TensorRT-LLM/issues/2202
这个issue类型是bug报告，涉及的主要对象为TensorRT-LLM下的一个转换脚本。由于内存溢出(Out of Memory, OOM)导致脚本无法成功转换模型，并寻求解决方案帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2201
这是一个bug报告，涉及到TensorRT-LLM中的一个已弃用参数导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2200
这是一个bug报告，主要涉及对象是TensorRT-LLM中的QWen模型，原因可能是在设置tp_size为8时出现了转换错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2199
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的参数设置。由于参数'paged_kv_cache=paged_kv_cache'需替换为'kv_cache_type=KVCacheType.PAGED'，可能导致程序出现错误或异常行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/2198
这个issue是一个Bug报告，涉及的主要对象是TensorRT-LLM下的convert_checkpoint.py脚本。导致此问题的原因可能是脚本在执行过程中出现了错误，导致无法成功转换指定的模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2197
这个issue是一个bug报告，涉及的主要对象是TensorRT LLM中的Bert模型构建脚本。这个bug的原因是当前构建脚本忽略了模型权重文件，只读取了config.json文件，导致无法从预训练模型生成可用的engine。

https://github.com/NVIDIA/TensorRT-LLM/issues/2196
这是一个关于TensorRT-LLM中参数use_paged_context_fmha引发的bug报告，主要涉及模型文件生成不一致导致结果差异的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2195
这是一个bug报告，涉及TensorRT-LLM下的模型输出不正确的问题。可能的原因导致模型未能输出预期的"singapore" 文本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2194
这是一个用户提出问题的issue，主要涉及的对象是TensorRT-LLM中的PDL机制，用户询问了PDL的含义以及为什么需要使用它。

https://github.com/NVIDIA/TensorRT-LLM/issues/2193
这是一个需求更改类型的用户提出的问题，涉及到模型输入格式的更改，由于用户希望改变输入数据的格式，导致无法正常使用模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2192
这是一个bug报告，针对TensorRT-LLM中Lora adapter加载失败的问题，具体原因是系统环境以及GPU相关库版本不匹配。

https://github.com/NVIDIA/TensorRT-LLM/issues/2191
这是一个bug报告issue，主要涉及TensorRT-LLM的Module和ModuleList模块，由于缺少`__repr__`方法可能导致调试时的困难。

https://github.com/NVIDIA/TensorRT-LLM/issues/2190
这是一个bug报告，问题涉及TensorRT-LLM中Executor API产生输出时排除了endId token，导致生成结果不包含此token。

https://github.com/NVIDIA/TensorRT-LLM/issues/2189
这是一个用户提出需求的issue，主要涉及TensorRT-LLM版本对应的CUDA、TensorRT、Python版本信息，并询问最低要求版本，原因可能是为了确保兼容性和最佳性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2188
这是一个bug报告，主要涉及到torch库的安装问题。导致这个问题的原因可能是extra-index-url设置不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/2187
这是一个 bug 报告，主要涉及 TensorRT LLM engine builder for Bert models，由于当前代码忽略加载模型权重，这导致无法从预训练模型生成可工作的引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/2185
这是一个bug报告，涉及到TensorRT-LLM的Executor线程安全性问题。由于未指定函数的线程安全性，可能导致调用cancelRequest时发生异常情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2184
这是一个功能更新的issue，主要涉及TensorRT-LLM的更新和API变更。用户提出了关于新增功能和API变更的需求或问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2183
这个issue类型是需求报告，涉及主要对象是支持Qwen2-VL。由于Qwen2-VL添加了新功能MROPE，而TensorRT-LLM目前不支持，因此用户提出希望支持该功能的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2182
这是一个Bug报告类型的Issue，主要涉及的对象是TensorRT-LLM代码中的`generation.py`文件。这个问题是由于在该文件中重复导入模块`tensorrt as trt`而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2181
这是一个关于安装TensorRT-LLM时遇到问题的bug报告，主要涉及的对象是TensorRT-LLM安装过程。由于命令`pip install extraindexurl https://pypi.nvidia.com tensorrtllm`可能出现写错，导致无法安装成功的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/2180
这是一个bug报告，涉及对象为TensorRT-LLM库。通过出现的"undefined symbol"错误信息可以推断出可能是由于缺少某个符号定义导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2179
这是一个性能问题报告，主要涉及TensorRT-LLM下的llama2like模型推理性能不达预期的情况。原因可能是由于使用vllm(fp16)作为推理引擎表现更好导致性能异常，希望专家能够帮助查看并提供帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2178
这是一个用户提出需求的issue，主要涉及的对象是Executor TP在多个GPU上是否可以在同一台主机上运行而不需要MPI。询问这个问题可能是由于用户想要在同一主机上利用多个GPU运行Executor TP而不依赖于MPI。

https://github.com/NVIDIA/TensorRT-LLM/issues/2177
这是一个关于如何区分模型中初始字符和非初始字符的问题，主要涉及到代码逻辑和条件判断部分，用户可能面临无法准确区分不同字符类型的困扰。

https://github.com/NVIDIA/TensorRT-LLM/issues/2175
这是一个类型为文档修正的issue，主要涉及修复README文档内容。原因可能是README文档存在错误或缺失，需要更新修正。

https://github.com/NVIDIA/TensorRT-LLM/issues/2174
这是一个用户提出需求的类型，主要对象是有关TensorRT-LLM中的batch_manager和executor的CPP代码。用户询问何时开源这两部分代码，可能是希望获得相关的开源支持或进一步研究。

https://github.com/NVIDIA/TensorRT-LLM/issues/2173
这是一个bug报告，主要涉及的对象是TensorRT-LLM的文档链接。该问题产生的原因是文档链接已经失效导致用户无法访问文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/2172
这个issue是一个bug报告，主要涉及的对象是在导入tensorrt_llm时出现了卡住的情况，可能是由于系统MPI库问题导致导入错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2171
这是一个用户提出需求并请教问题的类型，主要涉及TensorRT-LLM中的PromptTuningEmbedding参数。由于用户对任务和标记的含义以及特定标记的取值范围不清楚，导致了提出了问题并寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2169
这是一个用户提出需求的issue，主要涉及TensorRT-LLM的优化服务问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2168
这是一个需要更新文档页面的类型为“其它”的issue，主要涉及到项目的网页文档 更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2166
这是一个特性请求，主要涉及TensorRT-LLM中的KV缓存复用和Int8 KV 缓存的兼容性问题。由于当前的页上下文FMHA不支持Int8 KV缓存，导致用户无法同时使用KV缓存复用和Int8 KV缓存，限制了在大规模部署中优化性能和效率的能力。

https://github.com/NVIDIA/TensorRT-LLM/issues/2165
这个issue类型是一个功能需求，主要对象是在TensorRT-LLM下添加Windows库到版本0.12中。

https://github.com/NVIDIA/TensorRT-LLM/issues/2164
这是一个描述TensorRT-LLM v0.12 更新的issue，主要涉及的对象是更新的主要功能和增强，提供了对LLA和MoE等模型的支持，以及一系列功能和模型的增强。

https://github.com/NVIDIA/TensorRT-LLM/issues/2163
这是一个关于性能优化的问题，用户希望在GPU上获取包括context logit tensors的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2162
这个issue是关于bug报告，涉及的主要对象是TensorRT-LLM下的setup.py文件，造成该问题的原因是缺少了必要的bindings路径导致设置文件无法运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2161
这是一个Bug报告类型的issue，主要涉及TensorRT-LLM下生成引擎时出现的错误，可能是由于TensorRT版本和LLM版本不兼容导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2159
这是一个bug报告，主要对象是TensorRT-LLM下的trtexec工具。造成该问题的原因可能是trtexec未能正确注册插件，导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2158
这是一个bug报告，主要涉及TensorRT-LLM中的int4优化问题，由于键错误导致了'KeyError: 'llava_llama''这样的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2156
这个issue是关于需求提出的，主要涉及TensorRT-LLM模型的更新和改进，其中用户提出了对Model Support、API、Bug fixes、Benchmark等方面的具体需求和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/2155
这是一个关于Bug报告的issue，主要涉及TensorRT-LLM的Recurrent Drafter功能不正常工作的问题。由于转换/构建/运行TensorRT LLM引擎时出现了bug，导致生成token的速度慢且一次只生成一个token。

https://github.com/NVIDIA/TensorRT-LLM/issues/2154
该issue类型为功能需求，主要涉及到创建名为sync.yml的文件。原因可能是用户需要与指定的同步配置进行交互。

https://github.com/NVIDIA/TensorRT-LLM/issues/2153
这是一个bug报告，涉及TensorRT-LLM中模型推理结果混乱的问题，可能是由于模型转换或构建过程中的某些问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2152
这是一个bug报告，涉及的主要对象是"model_weights_loader.py"。由于更新问题，用户可能遇到了模型权重加载方面的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2151
这是一个需求更改类的issue，主要对象是更新快速入门指南文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/2150
这是一个bug报告，主要涉及TensorRT-LLM中生成TensorRT engines时出现的错误。由于输入尺寸不符合要求，导致出现了RuntimeError。

https://github.com/NVIDIA/TensorRT-LLM/issues/2149
这是一个bug报告类型的issue，涉及主要对象是如何将lora adapter添加到whisper模型中。由于构建命令时发生了语法错误，导致无法成功运行命令，因此用户寻求帮助解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2148
这是一个关于bug报告的issue，主要涉及TensorRT-LLM在每次构建后生成的引擎对相同输入有不同结果问题。原因可能是引擎构建时未成功使用模型缓存。

https://github.com/NVIDIA/TensorRT-LLM/issues/2147
这是一个bug报告，问题涉及的主要对象是TensorRT-LLM中的模型输出。出现这个问题的原因可能是使用了smoothquart量化器导致输出为空。

https://github.com/NVIDIA/TensorRT-LLM/issues/2146
这是一个bug报告，涉及的主要对象是TensorRT-LLM，由于v0.11在Windows上存在已知问题，需要提供一个解决方案说明。

https://github.com/NVIDIA/TensorRT-LLM/issues/2145
这是一个bug报告，提到了build_wheel.py出现错误，请求帮助解决。原因可能是构建wheel时出现了问题，导致无法成功构建。

https://github.com/NVIDIA/TensorRT-LLM/issues/2144
这是一个用户提出需求的issue，主要涉及TensorRT-LLM模型是否支持接受两个图像作为输入的功能。原因可能是用户想了解当前的TensorRT-LLM是否支持这种模型结构。

https://github.com/NVIDIA/TensorRT-LLM/issues/2143
这是一个bug报告，主要涉及在TensorRT-LLM中使用trtllm-build命令时出现未识别参数导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2142
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的GPT模型推断。用户希望得到一个C++示例代码，而当前官方例子只提供Python版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2141
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的Qwen2 checkpoint构建过程中出现KeyError的问题。原因可能是在构建TensorRT引擎时程序无法找到'Qwen2ForCausalLM'这个键值导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2140
这是一个bug报告，主要涉及了在4x L40S系统上难以复现NVidia宣称的性能数字，可能是由于硬件/驱动版本/环境配置等问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2139
这个issue是一个用户提出的问题类型，主要涉及到了使用TensorRT-LLM下的metallama370B构建engine时遇到OOM问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2138
这是关于bug报告的issue，主要涉及TensorRT-LLM下int4_awq量化时设置temperature=0.0001时出现token_ids为-1的问题。由于设置了较低的temperature值，导致出现了错误提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/2137
这是一个关于性能影响的问题，涉及TensorRT-LLM中的参数 `spec_decoding_is_generation_length_variable`，用户询问不同长度对性能的影响，以及该参数对注意力内核的影响。

https://github.com/NVIDIA/TensorRT-LLM/issues/2136
这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的Whisper models，用户希望实现对Whisper模型的多 Lora 支持。这个需求可能是由于用户有多个 Lora 设备需要支持，希望能够在 Whisper 模型中实现多个 Lora 的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/2135
这是一个bug报告，该问题涉及TensorRT-LLM的构建问题，由于--fast-build标志导致transformer layers被忽略。

https://github.com/NVIDIA/TensorRT-LLM/issues/2134
这是一个bug报告，主要涉及openai_triton/manual_plugin的安装问题，由于授权相关问题导致了"Authorization required, but no authorization protocol specified"这个症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/2133
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中使用W4A8、FP8 KV cache构建的TRT engine输出为空的问题，由w4a8_awq量化可能存在问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2132
这是一个用户报告bug的issue，主要涉及TensorRT-LLM下转换hf训练的Dora适配器时遇到了问题，请求解决如何转换hf训练的Dora适配器以用于Whisper模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2130
这个issue类型是bug报告，主要涉及TensorRT-LLM的更新。这些bug和需求改进是由于代码实现中涉及的各种功能缺陷和性能优化需求而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2129
这个issue是关于bug报告，主要涉及的对象是TensorRT-LLM中的WhisperEncoder模块。由于输入数据类型不匹配，导致了AssertionError。

https://github.com/NVIDIA/TensorRT-LLM/issues/2128
这是一个bug报告，关于在TensorRT-LLM上部署模型的问题，主要涉及到TensorRTLLM只支持fp16 transformer模型。由于转换脚本可能存在问题，导致推理结果错误，用户寻求帮助解决这一问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2127
这个issue是一个功能需求类型，主要涉及TensorRT-LLM下的Max Forward Passes支持，用户提出希望引入maxForwardPasses支持来缓解高延迟问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2126
这是一个bug报告类型的issue，主要涉及如何在TensorRT-LLM中添加gemm_plugin int8。可能由于某些设置或参数导致了Engine创建失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2125
这是一个bug报告，主要涉及TensorRT-LLM中Llama 3 70B FP8 engine构建失败的问题，可能是由于在trtllmbuild中包含'use_paged_context_fmha enable use_fp8_context_fmha enable'导致引擎构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/2124
这个issue属于bug报告类型，涉及TensorRT-LLM引擎构建失败的问题，可能由于环境设置不正确导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2123
这是一个bug报告，主要涉及的对象是在尝试为Qwen272B模型构建TensorRT引擎时出现了CUDA内存不足错误。原因可能是在模型编译阶段进行量化和拓扑优化时导致内存超额分配。

https://github.com/NVIDIA/TensorRT-LLM/issues/2122
这是一个需求报告，主要对象是TensorRT-LLM下无法支持32k context输入，根本原因是内存限制导致OOM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2121
这是一个关于bug报告的issue，主要涉及的对象是TensorRT-LLM中Llama3.1的支持情况。导致问题的原因是最新版本的TensorRT-LLM不再支持与transformers 4.43.1兼容，导致Llama3.1无法正常使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/2120
这是一个bug报告issue，针对的是TensorRT LLM在遇到结束标识符时没有停止生成的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2119
这是一个Bug报告，涉及到TensorRT-LLM中的multi_block_mode参数使用不正常，导致引发了错误提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/2118
这是一个Bug报告，涉及的主要对象是TensorRT-LLM。导致该问题的原因是`PluginConfig`对象缺少`_streamingllm`属性，可能由于版本不匹配或代码错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2117
这是一个bug报告，主要涉及TensorRT-LLM权重编译后运行效率较低的问题。由于v0.11.0版本GPU利用率降低导致并发性下降，与期望表现相比速度慢，用户寻求支援解决此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2116
这个issue是一个bug报告，主要涉及的对象是在编译ptx代码时出现的性能警告。原因是 wgmma.mma_async 指令由于在管道阶段的起始和结束之间定义了一个 wgmma 的累加寄存器的非 wgmma 指令而被序列化，用户希望了解如何改进 ptx 代码以消除警告。

https://github.com/NVIDIA/TensorRT-LLM/issues/2115
这是一个bug报告类型的issue，涉及TensorRT-LLM中使用FP8量化的Mixtral + Medusa heads模型构建失败的问题。可能由于某些原因导致构建的引擎出现了错误行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/2114
这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的C++ batch manager API的移除及替换问题，用户关注是否C++ executor API会与batch manager功能保持一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2113
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM中的代码。由于代码变更导致`convert_utils.py`中的`exclude_modules`匹配模式与`quantize.py`不一致，用户提出需要修正以保持一致性。

https://github.com/NVIDIA/TensorRT-LLM/issues/2112
该issue是关于bug报告，涉及到TensorRT-LLM下的`--use_fp8_context_fmha`功能与Llama models的使用问题，问题出现的原因可能是在构建engine时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2110
这是一则Bug报告，涉及TensorRT-LLM的更新和功能支持。该问题主要涉及功能更新、API变更、Bug修复以及基础设施更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2109
这是一个bug报告，主要涉及TensorRT-LLM中模型转换失败的问题。原因可能是程序运行过程中出现了错误或者缺少某些必要的组件导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2108
这个issue属于bug报告类型，主要涉及TensorRT-LLM下回答结果相同的问题，可能是由于模型或代码逻辑中的错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2107
该问题属于bug报告类型，涉及的主要对象是TensorRT-LLM。由于将pipeline代码和TensorRT-LLM分开并将pipeline代码映射为TensorRTLLM Docker容器的卷，导致了"Assertion failed: FMHA kernels are not found"的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2106
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM，用户询问是否会支持KOSMOS-2.5版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2105
这是一个bug报告，主要涉及TensorRT-LLM下的`convert_checkpoint.py`失败的问题。由于LLAMA 3.1 8B指令导致无法正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/2104
这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持直接传递final input_embeds，并由于组合不同的视觉特征和input_embeds导致无法满足需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2103
这个issue类型属于技术问题咨询，主要涉及对象是使用TensorRT-LLM进行推理时如何预处理一个int4模型。由于用户自行将模型量化为int4，现在不知道如何安排int4的权重布局。

https://github.com/NVIDIA/TensorRT-LLM/issues/2102
这是一个用户提出需求的issue，主要涉及TensorRT-LLM项目中添加新的量化方法，用户希望了解如何在代码中实现INT8 KV cache量化以及如何集成CUDA核函数和Python绑定。

https://github.com/NVIDIA/TensorRT-LLM/issues/2101
这是一个关于寻找protobuf文件的问题，涉及TensorRT-LLM版本升级对应配置文件变更的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2100
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中构建visual encoder时出现的错误。原因可能是脚本执行过程中的某些问题导致了无法将visual encoder转换为.engine格式。

https://github.com/NVIDIA/TensorRT-LLM/issues/2099
这是一个bug报告，主要涉及对象是TensorRT-LLM中的`oneShotAllReduceKernel`函数，由于在之前对`buffers`进行了偏移操作，导致了在GPU之间错误地读取数据缓冲区，引发了性能问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2098
这个issue是一个功能需求报告，涉及的主要对象是TensorRT-LLM，用户希望添加对Nougat模型的支持，但当前系统不支持，可能是由于TensorRT-LLM尚未集成Nougat相关功能导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2097
这是一个bug报告，主要涉及TensorRT-LLM中的quantization plugins，由于混淆了元素数量"n"和原始元素数量"originalN"导致计算工作空间大小出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2096
这是一个Bug报告，涉及的主要对象是TensorRT-LLM下的LLama2模型。由于混合使用LoRA和非LoRA请求在同一批次中导致模型生成结果随机，可能是由于请求的混合方式或者模型本身的逻辑问题所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2094
这是一个 bug 报告类的 issue，主要涉及TensorRT-LLM模型支持与功能更新以及修复了一些bug和文档问题。由于特定功能和模型支持方面的更改，导致了一些问题和错误需要修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/2093
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在模型转换过程中OOMed的问题，可能由于并行处理工作数过多导致内存溢出。

https://github.com/NVIDIA/TensorRT-LLM/issues/2092
这是一个用户提出问题的类型，涉及主要对象是TensorRT-LLM的cpp benchmarks，用户询问了gptManagerBenchmark和gptSessionBenchmark之间的区别及示例使用场景。

https://github.com/NVIDIA/TensorRT-LLM/issues/2091
这个issue是一个bug报告，主要涉及TensorRT-LLM下的量化过程中出现错误，导致无法成功量化模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2090
这是一个关于功能优化的问题，主要涉及TensorRT-LLM中的streaming推理过程，用户提出了生成的logits格式不够高效的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2089
这是一个bug报告，主要涉及的对象是在构建TensorRT-LLM引擎时出现的segmentation fault错误。导致这个问题的原因可能是环境中TensorRT和相关库的版本不兼容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2088
这是一个关于Bug报告的问题，主要涉及TensorRT-LLM下的运行结果出现了错误的情况。原因可能是在启用fp8量化时返回的generationLogits有问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2087
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM中的模型转换过程，由于smoothquant功能在转换Qwen2-0.5b-instruct模型时失败，导致无法成功转换和保存模型检查点。

https://github.com/NVIDIA/TensorRT-LLM/issues/2086
该issue属于代码合并类型的问题，主要涉及TensorRT-LLM项目中的分支合并操作。这个问题可能是由于开发过程中的代码变更或较大的功能更新引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2085
这是一个优化性能的issue，涉及到TensorRT-LLM下的decoder MMHA kernel支持INT8 SCALE_Q_INSTEAD_OF_K and SCALE_P_INSTEAD_OF_V，用户提出了性能优化的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2084
这是一个bug报告，涉及主要对象是TensorRT-LLM下的一个特定函数，导致所有令牌在填充阶段错误地使用相同的位置编码。

https://github.com/NVIDIA/TensorRT-LLM/issues/2083
这是一个用户提出需求的issue，主要涉及到TensorRT-LLM中的InternVL2支持问题。由于未收到响应，用户正在寻求帮助验证InternVL2的支持情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/2082
这是一个关于支持cuda12.5和tritonserver 24.07-py3的需求问题，讨论了支持的版本信息和相关环境配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/2081
这是一个bug报告，涉及TensorRT-LLM中构建BuildConfig对象时未正确传播use_fused_mlp选项导致加载JSON文件时数值不正确的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2080
这是一个bug报告，主要涉及TensorRT-LLM下的转换脚本convert_checkpoint.py无法使用的问题，可能是由于这个脚本无法正常处理llama3.1 405b模型所导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2079
这是一个关于bug报告的issue，主要涉及vllm backend在Triton中不支持MiniCPM-Llama3-V-2_5模型的问题。由于缺乏响应，用户在安装和配置过程中遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2078
该问题类型为bug报告，涉及主要对象是TensorRT-LLM中的模型优化工具nvidia-modelopt。由于版本问题导致无法安装nvidia-modelopt 0.9.3，用户需要求解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/2077
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM项目中的install_requirements.sh脚本文件。这个问题可能是由于安装脚本中存在多余的空格导致安装失败的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2076
这是一个bug报告，主要涉及TensorRT-LLM中如何支持将llama3.18b转换为smoothquant，由于llama3.1的rope scaling与llama3不同，导致无法成功转换为smoothquant。

https://github.com/NVIDIA/TensorRT-LLM/issues/2075
这是一个bug报告类型的issue，主要涉及的对象是快速入门指南（quick-start-guide.md）。该问题由于拼写错误导致了一个bug，需要进行修正。

https://github.com/NVIDIA/TensorRT-LLM/issues/2074
这是一个关于性能比较的问题，用户提出了关于为什么fp8速度优于int8 smoothquant的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2073
这是一个用户询问问题的类型，主要涉及的对象是TensorRT-LLM中的Executor构造函数，用户询问如何调用该构造函数。

https://github.com/NVIDIA/TensorRT-LLM/issues/2072
这是一个bug报告，涉及TensorRT-LLM下使用Mixtral FP8时出现的token id异常的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2071
这是一个bug报告，涉及TensorRT-LLM下的Lama 3 8B模型，在运行推理时出现了静态维度不匹配的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2070
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的GemmFpAIntB MMa::IteratorB::Layout，可能由于布局错误导致bug产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/2069
这是一个关于TensorRT-LLM的bug报告，涉及对象是在尝试手动创建GenerationSession并执行文本生成推理时遇到Assertion failed错误导致Python进程崩溃的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2068
这个issue类型是用户提出需求，主要涉及的对象是模型加载代码，用户想要修改加载模型的逻辑。

https://github.com/NVIDIA/TensorRT-LLM/issues/2066
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中在使用4个GPU设备时出现的运行时错误。导致这个问题的原因可能是GPU设置相关的配置问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2065
这是一个bug报告，主要涉及TensorRT-LLM下的一个fatal error，由于某个代码行导致版本0.12.0.dev2024073000出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2064
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的Llama 3 8B Instruct引擎配置问题，由于不清楚如何配置整个上下文大小导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2063
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的量化功能。由于模型数据类型不匹配导致的错误，用户在尝试使用TensorRT-LLM对llama 3 8b模型进行量化时出现了"Cannot copy out of meta tensor; no data error"的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2062
这是一个bug报告，主要涉及TensorRT-LLM脚本在运行时出现错误。由于Cython绑定的问题，导致了quantize.py脚本无法成功执行。

https://github.com/NVIDIA/TensorRT-LLM/issues/2060
这是一个关于TensorRT-LLM在NVIDIA Grace Hopper系统上不支持`arm64`架构的bug报告。

https://github.com/NVIDIA/TensorRT-LLM/issues/2059
这个issue是一个bug报告，主要涉及TensorRT-LLM中--use_custom_all_reduce选项的移除，导致运行时出现"No Peer Access error"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2058
这是一个bug报告，主要涉及TensorRT-LLM中MPI worldSize与tp*pp不一致导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2057
这个issue类型是bug报告，主要涉及Engine Building Command文档中参数错误的问题，导致trtllmbuild命令报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/2056
这个issue类型是用户提出需求，主要涉及的对象是Propagate QuantConfig.exclude_modules to weight only quantization。

https://github.com/NVIDIA/TensorRT-LLM/issues/2055
这是一个用户提出需求的问题，主要涉及对象是在TensorRT-LLM中添加图像嵌入到模型输入中的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2053
这个issue类型是更新通报，涉及TensorRT-LLM的功能更新和基础设施升级，而非bug报告或用户提出需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2052
这是一个bug报告，主要涉及LLAMA 3.1中由BF16量化失败到FP8的问题，用户在尝试量化时遇到了困难。

https://github.com/NVIDIA/TensorRT-LLM/issues/2051
这是关于代码开源性的问题，询问了TensorRT-LLM中executor部分是否是闭源的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2050
这是一个bug报告类型的issue，涉及的主要对象是TensorRT-LLM。由于未正确安装TensorRT-LLM导致的安装错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2049
这个issue是关于bug报告，涉及的主要对象是TensorRT-LLM中的Engine Building Command。由于参数错误导致的bug，用户报告了错误的命令处理和错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/2048
这是一个用户提出问题的issue，主要涉及如何使用prompt_table来解决在转换模型时出现的问题，因为GenerationSession似乎只接受input_ids作为输入，但用户想尝试使用prompt_table来结合input_ids和vit输出，但不知道在哪里合并它们。

https://github.com/NVIDIA/TensorRT-LLM/issues/2047
这个issue是关于TensorRT-LLM中运行引擎时出现错误的bug报告。

https://github.com/NVIDIA/TensorRT-LLM/issues/2046
这是一个用户提出需求的issue，主要涉及的对象是通过给定数据集来进行吞吐量基准测试。原因可能是用户想要比较不同框架的吞吐量并找到与 benchmarks/benchmark_throughput.py 相似的基准测试脚本。

https://github.com/NVIDIA/TensorRT-LLM/issues/2045
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的PluginConfig对象。这个问题是由于插件配置对象中缺少"_remove_input_padding"属性而导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2044
这是一个用户提出的特性需求，主要涉及Executor API中缺少灵活的logit后处理器API。这个问题存在的原因是当前的logit后处理器实现要求事先在一个映射表中注册后处理器，然后在请求时按名称引用后处理器，无法传递额外的每个请求参数给后处理器。

https://github.com/NVIDIA/TensorRT-LLM/issues/2043
这是一个bug报告，涉及的主要对象是trtllm-build Mixtral-8x7B-v0.1 fp16，由于内存不足导致OutOfMemory错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2042
这个issue类型是bug报告，主要对象是在TensorRT-LLM v0.10.0版本中使用tensorrtllm_backend的用户。由于CPU资源不足导致服务hang并出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2041
这是一个关于功能支持的问题，涉及主要对象是TensorRT-LLM和NVIDIA Jetson Orin Nano开发套件。用户想了解Jetson Orin Nano开发套件是否支持在上运行TensorRTLLM，可能由于缺乏相关信息而产生疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/2040
这是一个bug报告，主要涉及到TensorRT-LLM中的TopP层问题，由于这个问题导致了程序在主分支上出现段错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2039
这是一个关于bug的报告，主要涉及的对象是TensorRT-LLM中的TopP sampling layer。这个问题是由于调用了不匹配的数组大小而导致的segfault。

https://github.com/NVIDIA/TensorRT-LLM/issues/2038
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中关于使用多个Lora的问题，用户希望能够同时使用多个Lora进行推理。

https://github.com/NVIDIA/TensorRT-LLM/issues/2037
这是一个bug报告，该问题单涉及的主要对象是TensorRT-LLM。由于构建过程中出现错误导致的失败，可能是由于环境配置问题或脚本错误引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/2035
这个issue是关于bug报告，涉及的主要对象是TensorRT-LLM中的FP8Linear层，由于访问参数的.raw_value和.value导致不能连续调用FP8Linear.forward函数。

https://github.com/NVIDIA/TensorRT-LLM/issues/2034
这是一个标题为"done"的空issue，类型可能是用户确认任务已完成。

https://github.com/NVIDIA/TensorRT-LLM/issues/2033
这是一个需求更新的issue，主要对象是TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/2032
这是一个关于TensorRT-LLM下Qwen2-7B指令结果差异的bug报告。主要涉及的对象是Qwen2-7B指令执行的结果，可能由于generation_config参数设置不同导致了不同的输出结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2031
这是一个bug报告，主要涉及的对象是在H20设备上运行TensorRT-LLM时出现除零错误的问题。原因可能是网络模型Qwen14B在运行时发生了除零错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2029
这是一个bug报告，涉及主要对象是在AGX Orin devkit上编译TrTllm时遇到的编译错误。由于无法成功编译，用户提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2028
这个issue类型是bug报告，涉及的主要对象是README文件中的链接。原因是发现README中的链接已过时，导致需要更新以提供正确的链接。

https://github.com/NVIDIA/TensorRT-LLM/issues/2027
这是一个关于软件使用问题的询问，主要涉及TensorRT-LLM下的Python运行时是否支持inflight batching。

https://github.com/NVIDIA/TensorRT-LLM/issues/2026
这是一个优化建议类型的issue，主要涉及TensorRT-LLM中的交叉注意力计算，用户认为当前计算过程中存在冗余，建议优化以提高性能和量化效果。

https://github.com/NVIDIA/TensorRT-LLM/issues/2025
这是一个Bug报告，主要涉及TensorRT-LLM下smoothquant模型转换过程中出现的错误，用户正在寻求关于smoothquant转换错误的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/2024
这是一个用户提出需求的类型的issue，请求TensorRT-LLM支持Mistral Large 2模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/2023
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下使用Leader Mode时出现的延迟增加问题。造成这个问题的可能原因是在使用两个不同模型在两个GPU上运行时，出现了比预期更慢的延迟现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/2022
这个issue属于bug报告类型，主要涉及TensorRT-LLM在使用lora时输出结果与transformers lib不一致的问题。可能由于lora权重设置不正确导致了输出结果的差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/2021
这是一个bug报告，涉及的主要对象是Triton和TensorRT-LLM。由于参与者ID未指定，MPI的worldSize要求与tp*pp相等，导致了Triton在启动时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2020
这是一个bug报告，涉及TensorRT-LLM中的模型创建问题，用户遇到了"UNAVAILABLE: Internal: unexpected error when creating modelInstance"的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2019
这是一个Bug报告，涉及的主要对象是将multimodal Phi 3 Vision模型转换为TRT-LLM checkpoints。由于某种版本不匹配的原因导致了"undefined symbol"错误，用户在尝试转换模型时遇到了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2018
这是一个bug报告，主要涉及TensorRT-LLM中构建engine时出现了"TypeError: set_shape(): incompatible function arguments"错误。造成这个问题的原因可能是参数设置不兼容。

https://github.com/NVIDIA/TensorRT-LLM/issues/2017
这是一个Bug报告类型的issue，涉及的主要对象是在AWS G5/G6 NVIDIA A10和L4设备上运行的TensorRT-LLM。由于缺少`use_custom_all_reduce disable`参数，导致无法在节点上加载模型并发生"peer access is not supported between these two devices"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2016
这是一个类型是需求更新的issue，主要涉及更新TensorRT-LLM。由于未提供具体的描述内容，无法分析具体的问题或需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/2015
这个issue属于用户提出需求类型，主要涉及的对象是TensorRT-LLM下的支持Llama 3.1的功能。由于Llama 3.1中前向模型部分发生了小改动，导致当前版本的TensorRTLLM无法运行此版本，用户希望更新TensorRTLLM以支持Llama 3.1。

https://github.com/NVIDIA/TensorRT-LLM/issues/2014
这是一个关于无法使用TensorRT-LLM/examples/apps/fastapi_server.py的bug报告，主要涉及TensorRT-LLM的推断服务运行失败问题。这可能是由于代码参数传递错误或服务配置问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/2013
这是一个关于性能优化的咨询问题，主要涉及到TensorRT-LLM中fp8和fp16内核在H100平台上性能比较的情况。由于fp8内核表现不佳，导致在解码阶段的性能比fp16差。

https://github.com/NVIDIA/TensorRT-LLM/issues/2012
这是一个空内容的issue，类型为用户提出需求。主要涉及的对象是更新README文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/2011
这是一个bug报告，问题涉及CogVLM示例中的FP8量化和KV缓存支持未实际实现。这可能是因为相关代码未被正确实现或者未完成。

https://github.com/NVIDIA/TensorRT-LLM/issues/2010
这是一个功能需求报告类型的 issue，涉及支持加载 fp8 hf 权重。

https://github.com/NVIDIA/TensorRT-LLM/issues/2008
这是一个bug报告类型的issue，涉及的主要对象是TensorRT-LLM。由于缺少正确的pad token导致了bug，用户提出了关于修复CodeQwen模型中错误pad token的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2007
这个issue是一个bug报告，主要涉及的对象是在使用TensorRT-LLM中进行FP8量化后推理过程中出现的问题。导致出现这个问题的原因可能是量化过程中的配置问题或代码实现的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/2006
这个issue是关于bug报告，主要涉及的对象是TensorRT-LLM中的executor模块。由于升级到0.11.0版本后，原本可用的`TrtGptModelType::InflightFusedBatching`在executor绑定中不再被暴露，导致无法识别该选项。

https://github.com/NVIDIA/TensorRT-LLM/issues/2005
这是一个Bug报告，主要涉及TensorRT-LLM中的模型转换错误，用户尝试使用smoothquant功能转换模型时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2004
这是一个bug报告，涉及到TensorRT-LLM的部署问题，由于无法加载共享库 libtensorrt_llm.so，导致出现了"cannot open shared object file: No such file or directory"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/2003
这是一个用户提出需求的issue，主要涉及 Prefix Caching 是否支持CPU卸载，用户希望了解是否有计划支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/2002
这是一个bug报告，主要涉及TensorRT-LLM版本0.12.0.dev2024071600，由于无法成功构建引擎而导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/2001
这是一个用户提出需求的问题单，主要涉及TensorRT-LLM下的Context FMHA是否能够用来实现Transformer在视觉编码器中的应用，用户关注为什么示例中的多模态模型都直接使用TensorRT来部署视觉编码器，而不使用TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/2000
这是一个需求类型的issue，主要涉及TensorRT-LLM中缺乏支持interleaved moe模型架构的功能，用户提出需要添加对interleaved moe的支持以及调整权重命名方式的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1999
这个issue类型是bug报告，主要涉及的对象是T5模型的推理结果。由于enable/disable `remove_input_padding`参数的设置不同导致T5模型在TensorRT-LLM和Hugging Face环境下的推理结果有明显差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/1998
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在使用docker时的打包发布错误，导致出现了资源访问权限被拒绝的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1997
这是一个用户提出需求的问题，主要涉及FMHA kernels中是否支持自定义alibi slopes，可能由于功能限制或设计缺陷导致用户无法使用自定义alibi slopes。

https://github.com/NVIDIA/TensorRT-LLM/issues/1996
这个issue类型是用户请教问题，主要涉及auto_parallel中的node_sharding_weight和edge_resharding_weight，用户想了解这些权重的作用。

https://github.com/NVIDIA/TensorRT-LLM/issues/1995
这是一个bug报告，涉及对象为TensorRT-LLM，由于未加载正确的模型版本和后端库，导致模型加载失败并出现错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1994
这是一个bug报告，涉及到TensorRT-LLM下的一个测试文件无法找到modelSpec.h导致编译错误。造成这个问题的原因可能是文件路径配置错误或依赖关系缺失。

https://github.com/NVIDIA/TensorRT-LLM/issues/1993
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的一个C++测试文件。由于缺少modelSpec.h文件，导致编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1992
这是一个bug报告，涉及TensorRT-LLM中的qwen demo无法运行的问题，可能由于MPI size和TP size不匹配导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1991
这是一个关于迁移模型至TRTLLM的问题，涉及的主要对象是Q2_K量化方法。这个问题由于用户想迁移已在llama.cpp中运行的模型至TRTLLM，询问是否技术上可行以及完成迁移需要的工作量。

https://github.com/NVIDIA/TensorRT-LLM/issues/1990
这是一个关于性能问题的bug报告，主要涉及的对象是TensorRT-LLM下使用4个A10显卡通过PCIe连接时的设置问题。由于TP和PP参数设置不当导致推理速度显著降低，可能涉及参数设置或其他可能性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1989
这是一个关于需求咨询的issue，主要涉及对象是构建TensorRT-LLM Backend for Triton server时关于GPU要求的问题。该问题的产生是由于缺少GPU驱动而导致构建时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1988
这是一个bug报告，主要涉及CogVLM项目的构建问题，可能是由于错误的构建命令导致无法成功构建。

https://github.com/NVIDIA/TensorRT-LLM/issues/1987
这是一个bug报告，涉及使用auto_parallel时的类型错误。原因可能是用户在V100 SXM集群上使用auto_parallel时遇到了拼写错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1986
这是一个关于如何在tritonserver中使用tensorrt_llm后端的问题，涉及对象为TensorRT-LLM模型部署，可能是用户需要文档或指导，而由于缺乏相关文档或指导，用户表达了无法解决该问题的困惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1985
该issue属于用户提出需求类型，主要涉及支持Mistral Nemo模型在TensorRT-LLM中的问题，用户想了解在不久的将来是否会支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1984
这是用户提出的需求类型的issue，主要涉及到TensorRT-LLM下的模型支持问题，用户希望添加对GEMMA2的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1983
这是一个关于文档缺失的问题，用户在寻找TensorRT LLM的相关文档时遇到了空白页面，导致无法获取所需信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1982
这是一个Bug报告类型的Issue，主要涉及的对象是TensorRT-LLM中的gptSessionBenchmark功能。这个问题可能是由于OptProfilerSelector shape无效导致的，用户寻求关于该问题的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1981
这个issue类型属于用户提出问题，涉及的主要对象是TensorRT-LLM中的CUDA代码。问题在于为什么要将最小缩放因子设置为1.0f / (FP8_E4M3_MAX * 512.f)，用户询问这个值为何是512。

https://github.com/NVIDIA/TensorRT-LLM/issues/1980
这是一个Bug报告类型的issue，主要涉及TensorRT-LLM中的MOE-FP8量化功能，用户遇到了在H20环境中MOE-FP8量化整型除零错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1979
这是一个需要更新Windows部分文档的issue，主要涉及TensorRT-LLM项目的文档维护，可能是由于文档不完整或有过时内容导致用户反馈需更新文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/1978
这是一个bug报告，主要涉及TensorRT-LLM下的T5 model，在运行过程中遇到了编译和推理问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1977
这是一个用户提出需求的issue，主要涉及更新Windows部分文档。原因可能是旧文档不准确或缺失某些信息，用户希望更新以提供更准确的指导。

https://github.com/NVIDIA/TensorRT-LLM/issues/1976
这是一个关于如何在TensorRT-LLM中禁用KV缓存的问题，属于用户提出需求类型的issue；该问题涉及主要对象是LLM模型的推理过程；用户提出这个问题是因为在量化了Qwen20.5B模型后，推理过程占用了大量GPU内存，怀疑是由于KV缓存导致，希望能够禁用KV缓存以减少GPU内存的使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/1975
这个issue类型是bug报告，涉及到Windows文档的修复，由于发布标签错误导致文档内容需要快速修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1974
这是一个关于bug报告的issue，主要涉及TensorRT-LLM在NVIDIA L4 GPU上运行出错，导致无法确定是否有选择的配置的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1973
这是一个文档更新的issue，主要涉及更新GitHub页面中图片的超链接。原因可能是当前页面中的图片链接需要更新或修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1972
这是一个用户需求类型的issue，该问题单涉及的主要对象是更新GitHub页面。

https://github.com/NVIDIA/TensorRT-LLM/issues/1971
这是一个升级发布版本的需求类型的issue，主要涉及的对象是Github页面（gh-pages）。这个issue出现的原因可能是为了更新项目发布版本0.11在Github页面上的信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1969
这是一个更新通知类型的issue，主要涉及TensorRT-LLM v0.11版本的一些关键功能和增强，包括支持长上下文、低延迟优化和LoRA增强等新特性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1968
这是一个Bug报告，涉及主要对象是TensorRT-LLM中的mamba-codestral-7B-v0.1模型。由于在`convert_checkpoint.py`文件中尝试加载HuggingFace的配置时出现了类型错误，导致了无法使用该模型的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1967
这个issue是一个bug报告，主要涉及的对象是TensorRT-LLM中的QWen2模型。由于内存不足导致引擎构建失败，用户寻求解决方案在16G内存GPU上修复这一问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1966
这是一个关于更新最新消息的问题，属于需求提出类型，主要对象是TensorRT-LLM。由于缺乏详细信息，用户需要更新最新消息，可能是为了获取最新的功能、改进或者其他重要信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1965
这是一个 bug 报告 issue，主要涉及的对象是 TensorRT-LLM 下的 fused_multihead_attention_v2 模块，由于 CUDA_ERROR_INVALID_VALUE 错误导致无法正常处理请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1964
这是一个bug报告，主要涉及到在使用TensorRT-LLM进行模型量化时出现内存溢出错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1963
这个issue类型是文档错误报告，主要涉及到TensorRT-LLM的文档，用户发现在chunkedcontext中，某个文档链接不存在。

https://github.com/NVIDIA/TensorRT-LLM/issues/1962
这个issue类型是用户提出需求，该问题单涉及的主要对象是添加对minicpm模型的支持，因为minicpm模型在chatRTX中具有良好效果且参数较小，希望能够结合使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/1961
这是一个bug报告，主要涉及的对象是gitmodules文件。由于在正常下载环境下，.gitmodules文件中的链接会导致无法更新子模块，建议替换为ssh链接。

https://github.com/NVIDIA/TensorRT-LLM/issues/1960
这是一个bug报告，涉及TensorRT-LLM下的Whisper模型，由于在代码中变量'tokenizer_name'在被赋值前被引用，导致了程序崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/1959
这是一个bug报告，涉及的主要对象是TensorRT-LLM，用户提出了关于MPI是否需要的疑问，因为在编译时禁用了多设备支持导致链接错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1958
这是一个bug报告，涉及TensorRT-LLM中的一个代码错误导致了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1957
这个issue是关于bug报告，涉及的主要对象是在使用BFLOAT16 LoRa Adapters时，模型性能降低。导致这个问题的原因可能是在编译和使用LoRa权重时，生成的输出与在huggingface中相比存在显著差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/1956
这是一个用户提出需求的问题，主要涉及TensorRT-LLM下如何执行跨注意力机制与FMHA核的问题。由于两种模态可能具有不同的序列长度，用户想知道如何在这种情况下执行FMHA以及是否可以修改插件以确保使用FMHA。

https://github.com/NVIDIA/TensorRT-LLM/issues/1954
该issue是关于更新TensorRT-LLM的新特性和API变更，涉及到了软件功能和接口的更新和改动。

https://github.com/NVIDIA/TensorRT-LLM/issues/1953
这是一个bug报告类型的issue，涉及主要对象是TensorRT-LLM中的quantize_by_modelopt.py脚本。由于函数get_tokenizer未能正确设置pad_token，导致用户无法成功量化CodeQwen1.5 7B Chat模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1952
这是一个特性需求相关的问题，主要涉及TikToken的集成。原因可能是用户希望在TensorRT-LLM中集成TikToken功能以实现某种特定功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1951
这是一个用户提出需求的问题，主要涉及TensorRT-LLM中关于在V100上通过双缓冲加速weightOnlyGemm操作的相关讨论。原因是TRTLLM不支持在V100上进行组相关的weightOnlyGemm，导致用户在尝试加速该操作时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1950
这是一个关于LLAMA checkpoint ImportError的bug报告，用户在使用TensorRT-LLM时遇到了undefined symbol导致的ImportError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1949
这是一个用户提出需求的问题，询问是否TensorRT-LLM支持blip2与fp8量化。

https://github.com/NVIDIA/TensorRT-LLM/issues/1948
这是一个bug报告，涉及TensorRT-LLM库中使用H20运行Python基准测试时遇到了cudaDeviceSynchronize运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1947
这是一个用户提出需求的类型，该问题涉及的主要对象是支持FlashAttention 3，用户询问是否有支持的计划。

https://github.com/NVIDIA/TensorRT-LLM/issues/1946
这个issue是用户在寻求关于如何使用Medusa来支持非llama模型的帮助，涉及到的主要对象是Bloom7b1模型。由于用户需要修改bloom/model.py以支持其他类型的模型，但修改后的精度很差，因此提出了两个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1945
这是一个关于用户需求的问题，主要涉及如何在TensorRT-LLM中量化自定义模型，作者想知道是否可以定义自己的模型和校准过程，然后简单地使用modelopt.torch.quantization.quantize()。

https://github.com/NVIDIA/TensorRT-LLM/issues/1944
这个issue是一个功能请求，请求支持vAttention风格的分页功能以实现动态内存管理。

https://github.com/NVIDIA/TensorRT-LLM/issues/1943
这个Issue属于用户提出需求类型，主要对象是为TensorRT项目创建一个Discord频道。这可能是因为缺少Slack、Discord或IRC频道而导致用户希望在特定频道中讨论有关NVIDIA TensorRT、TensorRTLLM和Triton推理服务器的问题，并希望在频道中广告以吸引更多用户参与讨论。

https://github.com/NVIDIA/TensorRT-LLM/issues/1942
这是一个Bug报告，涉及TensorRT-LLM中Mixtral-8x7B模型产生重复答案的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1941
这是一个Bug报告，涉及主要对象是`tensorrt_llm.bindings.Request`类。由于`Request`类只接受文本输入，而对于非文本输入的情况无法正常使用，导致了用户无法使用多模态输入，需要对此进行修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1940
这是一个技术问题咨询，主要涉及TensorRT-LLM下的GEMM计算中的FP8类型使用。由于文档中指明FP8的compute_type必须为FP32，在使用cublasLtMatmul进行FP8 GEMM时，compute_type被配置为FP32，引发了用户关于操作类型opAopB是否仍然为FP8的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1939
这个issue类型属于文档修复，涉及主要对象是项目文档。这个问题由于存在拼写错误和语法错误，影响了文档的质量和可读性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1938
这个issue是关于TensorRT-LLM性能问题的报告，用户在使用TensorRT-LLM编译器生成模型引擎后发现性能比vLLM差，希望找到问题所在并改善延迟和吞吐量。

https://github.com/NVIDIA/TensorRT-LLM/issues/1937
这是一个bug报告，涉及到移除了重复标志`extraindexurl https://pypi.nvidia.com/`而引起的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1936
这是一个缺少具体内容的issue，类型为未明确问题，主要对象是版本信息。原因可能是未正确填写具体信息导致疏漏。

https://github.com/NVIDIA/TensorRT-LLM/issues/1935
这是一个bug报告，问题涉及的主要对象是TensorRT-LLM中的默认最小长度参数。这个问题产生的原因是当将1作为最小长度参数传递时，由于默认值为1，导致`minLengths`张量为nullptr，从而导致penalty无法生效。

https://github.com/NVIDIA/TensorRT-LLM/issues/1934
这个issue属于用户提出需求类型，主要涉及的对象是InternVL2.0和InternVL1.5模型。由于这两个模型具有相同的架构，用户认为下载统计可能需要重新考虑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1933
这是一个bug报告类型的issue，主要涉及TensorRT-LLM安装问题。导致这个问题的原因可能是安装过程中出现了错误，最终导致安装不成功。

https://github.com/NVIDIA/TensorRT-LLM/issues/1932
这是一个bug报告，涉及GPU OOM Error When Quantizing Llama 3 8b，由于GPU显存不足导致torch.cuda.OutOfMemoryError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1931
这是一个用户提出需求的issue，主要对象是支持谷歌的Paligemma多模态模型。由于用户希望了解是否有计划支持该模型，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1930
这是一个bug报告，主要涉及TensorRT-LLM的Whisper decoder engine加载失败的问题，用户寻求关于无法加载模型的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1929
这个issue是关于bug报告，主要涉及TensorRT-LLM中模型版本升级导致结果变化和速度下降的问题，可能是由于版本升级中温度参数变化引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1928
这是一个Bug报告，针对TensorRT-LLM下的版本升级导致结果不一致和速度下降的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1927
这是一个用户提出需求的issue， 主要对象是TensorRT-LLM。这个需求提出要添加对自定义分词器和批处理大小的支持，可能是为了增加模型的灵活性和适用性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1926
这是一个用户提出需求的类型issue，主要对象是TensorRT-LLM项目，用户要求增加对falcon2的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1925
这个issue类型是bug报告，主要涉及的对象是在使用trtllm 0.8.0模型带有moe支持和Trtllm_backend的用户。由于发送大量请求给tritonserver时，使用两个nvidiaA800时出现了Assertion failed错误，用户猜测可能是moe kernel中的一个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1924
这是一个用户提出需求的issue，涉及主要对象是TensorRT-LLM中的`masked_multihead_attention_kernel`。用户询问关于为何在该函数中的注意力查询（Q）没有使用FP8量化，以及是否有考虑到精度等方面的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/1923
这是一个bug报告，主要涉及TensorRT-LLM中的推理结果存在重复的问题，可能是由于TensorRT v0.10.0中FP8量化导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1922
这是一个用户提出需求的issue，涉及主要对象为TensorRT-LLM下的weight-only GEMM。由于weight_only_groupwise_quant_matmul只支持fp16类型的zeropoints作为输入，导致一些模型中保存的zeropoints为int4类型时需要进行数据类型转换，用户希望增加对int类型zeropoints的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1921
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的attention计算部分，由于Q*transpose(Key)或softmax * value在attention计算中不支持fp8类型的计算，需要先将fp8数据转换为fp16/bf16类型，导致用户反馈问题寻求解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/1920
这是一个bug报告，主要涉及TensorRT-LLM下的GPTQ模型转换运行失败的问题。由于转换过程中出现了运行失败的情况，用户寻求关于TensorRTLLM在将模型转换为Int4格式时的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1918
这是一条更新通知，不是bug报告，主要涉及TensorRT-LLM模型支持的增加以及功能更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/1917
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的InternLM2模型。由于将`max_batch_size`从16增加到32、64或128导致出现错误，问题可能是由于超过一定的批次大小限制而引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1916
这是一个bug报告，该问题涉及TensorRT-LLM中使用Phi-3 Model构建引擎时出现形状不匹配的问题。导致这个问题的原因可能是模型参数不匹配所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1915
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM下的kv-cache在Qwen-72B-Chat中复用时当tp>1时速度变慢的问题。原因可能是kv-cache在特定情况下性能下降导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1914
这是一个bug报告类型的issue，主要涉及NVIDIA L20 GPUs是否支持FP8量化，由于支持FP8 FMHA的CUDA失败在预Hopper架构上启用，导致用户寻求帮助解决这一问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1913
这是一个关于性能优化需求的问题，主要涉及TensorRT-LLM在RTX4090上使用Sparsity fp8 Llama38b时未能获得性能提升，可能是由于稀疏优化策略未被选择，导致用户提出了关于为何在RTX4090上无法获得性能提升以及关于4090支持计划的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1912
这是一个bug报告，主要涉及的对象是TensorRT-LLM在L40设备上编译时出现的错误。由于CUDA版本和TRT版本不兼容，导致执行编译命令时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1911
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中使用`tensorrt_llm.functional.expand`时内部错误抛出的问题。原因是在构建序列化网络时，出现了kOPT值违反形状约束导致的内部错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1910
这是一个关于功能需求的issue，主要涉及TensorRTLLM对fp32 LoRA支持的问题，用户希望TensorRTLLM能够接受fp32 LoRA并在fp32下对LoRA低秩矩阵进行乘法运算，然后将乘积矩阵量化为与基础模型的fp16相匹配，以减少量化损失。

https://github.com/NVIDIA/TensorRT-LLM/issues/1909
这个issue是在为TensorRT-LLM添加`chunk_length`参数时遇到的问题，属于功能需求类问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1908
这是一个Bug报告类型的issue，主要涉及TensorRT-LLM中Mixtral转换出现错误的问题。由于无法为GEMM策略分析分配临时工作空间，导致了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1907
这是一个bug报告，主要涉及TensorRT-LLM中的PromptTuning和block_reuse功能之间无法同时使用的问题。原因可能是由于代码逻辑或配置设置的问题，导致启用block_reuse功能时无法与PromptTuning同时正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1906
这是一个性能问题的报告，主要涉及到TensorRT-LLM中使用FP8xint4时在Ada GPUs上性能表现不佳的情况。该问题可能由于Ada GPUs对FP8算术限制为FP32累加，导致性能不佳，用户提出疑问是否不建议在这类GPU上部署FP8。

https://github.com/NVIDIA/TensorRT-LLM/issues/1905
这是一个关于功能支持问题的issue，主要涉及的对象是TensorRT-LLM中的RecurrentGemma模块。原因在于是否RecurrentGemma支持ModelRunnerCpp，导致用户提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1904
这是一个bug报告，主要涉及TensorRT-LLM的构建问题。问题是由于编译错误导致无法成功构建trt_llm镜像，可能是源代码中的错误或依赖关系错误所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1903
这个issue是一个bug报告，主要涉及对象是TensorRT-LLM中的LoRA模块。由于low_rank参数设置超过了最大值64导致出现错误信息，用户寻求关于如何调整最大允许的rank或实现更高rank的解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/1902
这个issue是关于bug报告，涉及更新设置脚本中一个版本号错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1901
这是一个bug报告，涉及到TensorRT-LLM下的混合gemm内核和缩放迭代器，问题在于在处理行数时使用了不必要的64常数，可能导致未来当组大小为32时出现除以零的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1900
这是一个用户提出需求的issue，主要对象是TensorRT-LLM，用户希望知道是否计划支持Llavanextimage和Llavanextvideo模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1899
这是一个用户提出需求的问题，主要涉及TensorRT-LLM中的批量推理结果的差异，用户希望了解如何在llavanextvideo模型中设置参数以达到类似于Hugging Face transformers模型generate方法中do_sample=False参数设置的效果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1898
这是一个用户提出需求的类型问题，主要涉及TensorRT-LLM是否计划支持Dual Chunk Attention (DCA)功能，用户关注是否能支持运行长输入上下文推断。

https://github.com/NVIDIA/TensorRT-LLM/issues/1897
这是用户提出的需求类型的issue，主要涉及TensorRT-LLM的激活函数支持问题，用户希望添加对`gelu_pytorch_tanh`激活函数的识别。

https://github.com/NVIDIA/TensorRT-LLM/issues/1896
这是一个用户需求类型的issue，涉及主要对象是TensorRT-LLM下的MInference功能。用户提出根据论文指出可以通过不同的attention机制实现更快10倍的预填充速率，希望实现更快速的预填充。

https://github.com/NVIDIA/TensorRT-LLM/issues/1895
这是一个bug报告，主要涉及Quantization guidelines无法正常执行的问题，导致pip安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1894
这是一个关于建议和未来维护的问题，主要涉及到了GPTSession在C++运行时的推理应用。用户提出了executor的灵活性限制的问题，例如无法进一步开发splitwise等方法进行测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/1893
这是一个关于bug报告的issue，主要涉及TensorRT-LLM的初始化过程中遇到了"0xC0000409"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1891
这个issue属于更新需求类型，主要涉及TensorRT-LLM在特定功能方面的更新。原因是添加了新功能，支持了更多的模型和参数配置选项。

https://github.com/NVIDIA/TensorRT-LLM/issues/1890
这是一个bug报告，问题涉及到TensorRT-LLM下的模型转换程序在运行时产生错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1889
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中对Gemma 1.1模型的支持问题，可能是由`gelu_pytorch_tanh`激活函数引起，导致1.1版本无法成功构建引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/1888
这是一个bug报告，主要涉及TensorRT-LLM下的"rank0.python"程序在修改UD QP时出现"Operation not permitted"错误，可能是由于权限限制导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1887
这个issue是一个关于bug报告的问题，涉及TensorRT-LLM中的动态形状张量相关的问题，由于`expand`和`slice`操作在处理具有尺寸为1的维度时出现问题，导致后续操作失败构建。

https://github.com/NVIDIA/TensorRT-LLM/issues/1886
这是一个类型为bug报告的issue，主要涉及对象是TensorRT-LLM中的smoothquant功能。由于smoothquant转换starcoder23b时出现错误，推测可能是参数设置或代码逻辑问题导致无法成功转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/1885
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM下的kv_cache_reuse功能在awq quantized model上出现问题，导致服务器无法正确处理请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1884
这是一个bug报告，涉及TensorRT-LLM下的enc_dec模型，用户在运行时发现prompt_embedding_table未传递给编码器模型导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1883
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的enc_dec模型，由于在特定模式下运行会导致assertion failed。

https://github.com/NVIDIA/TensorRT-LLM/issues/1882
这是用户提出需求的类型，用户在询问关于TensorRT-LLM支持glm4的问题。这个问题可能是由于目前TensorRT-LLM版本尚不支持glm4，用户希望了解是否计划在未来版本中添加此支持所导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1881
这是一个关于TensorRT-LLM中批处理推断的bug报告，涉及到由不同提示组成的批次导致结果不正确，而由相同提示组成的批次结果正确，可能是由于填充问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1880
这是一个需求类型的issue，主要涉及的对象是Jeston Orin MLPerf 4.1，由于开发需要而提交。

https://github.com/NVIDIA/TensorRT-LLM/issues/1879
这是一个bug报告，该问题涉及TensorRT-LLM中的批量推断功能，由于某些原因导致相同输入批量推断时得到了不同的输出结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1876
这是一个bug报告，主要涉及TensorRT-LLM库。用户遇到了导入模块时出现的ModuleNotFoundError错误，可能是由于依赖安装不正确或环境配置问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1875
这个issue属于性能问题，涉及TensorRT-LLM在AWS A10g上通过划分模型到多个GPU进行加速时遇到的困惑。由于划分模型到两个GPU后没有获得更快的推理速度，用户想寻求解释是否合理以及可能原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/1874
这是一个关于bug报告的 issue，主要涉及对象是TensorRT-LLM中的XQA kernel function，用户提到在特定序列长度下观察到生成速度略有增加，可能是由于内存传输大小的差异导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1873
这是一个关于性能问题的issue，主要涉及TensorRT-LLM在多模态模型中的性能表现的比较。由于TensorRT-LLM在测试中比4位整型量化的transformers效率更低，用户提出了是否正常的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1871
这是一个关于如何获取或计算Python运行时和C++运行时基准测试中的第一个令牌延迟和第二/下一个令牌延迟的问题。主要涉及TensorRT-LLM版本v0.10.0的GPU性能问题。这个问题的原因可能是用户想要衡量第一个和第二/下一个令牌的延迟数值，但在当前的基准测试输出中找不到相关数值。

https://github.com/NVIDIA/TensorRT-LLM/issues/1870
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于某些特殊标记的处理问题，导致了TensorRT的结果出现异常，而vLLM和transformers.generate却表现正常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1869
这是一个bug报告，主要涉及TensorRT-LLM下的Executor出现"Executor failed to await responses"的问题。由于某种情况下导致请求响应未能正确等待，造成程序出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1868
这是一个bug报告，主要涉及TensorRT-LLM中的llama2模型，由于内存不足导致CUDA运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1867
这是一个用户提出需求的issue，主要涉及了TensorRT-LLM是否支持类似huggingface transformer past_key_values的参数。用户希望能够提前计算kv缓存并传递给ModelRunner.generate()来加快解码速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/1866
这是一个用户提出需求的问题，主要涉及到如何在TensorRT-LLM中选择性地在推理过程中仅对模型的单个层或头运行前向传递的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1865
这是一个Bug报告类型的issue，主要涉及到TensorRT-LLM无法在Nvidia T4上运行whisper，可能由于软件版本或配置不兼容导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1864
这是一个Bug报告，主要涉及TensorRT-LLM在L40下无法启用FP8 FMHA功能的问题，导致用户无法使用特定命令。

https://github.com/NVIDIA/TensorRT-LLM/issues/1863
这个issue是关于提出需求的，主要涉及到的对象是代码中的参数命名问题，导致可能无法信任远程代码。

https://github.com/NVIDIA/TensorRT-LLM/issues/1862
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中运行LoRA模型时遇到的错误。由于运行LoRA模型时出现了错误，用户寻求帮助解决该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1861
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在GRID vGPU环境下出现nvml错误的问题，可能由于VM配置或驱动版本不匹配导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1860
这是一个bug报告，涉及到TensorRT-LLM中的模型转换问题。该问题由于量化转换导致结果不正确，用户想了解是什么原因导致了这种差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/1859
这是一个bug报告类型的GitHub issue， 主要涉及TensorRT-LLM下模型加载失败的问题。导致此问题可能是由于模型转换、引擎构建或推理过程中的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1858
这是一个关于功能需求的问题，主要涉及LLAMA 3 70B或LLama 3 8B是否支持XQA，用户询问原因是文档上没有明确提到LLAMA 3是否支持这个功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1855
这是一个bug报告，主要涉及TensorRT-LLM中导入llama模型时出现的代码错误。由于代码中默认值的处理不当，导致了无法正常初始化层的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1854
这是一个缺少具体内容的bug报告，主要涉及的对象是代码提交（commit），导致用户无法提供详细信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1853
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM。由于添加新的输入导致CUDA内存访问错误，用户寻求关于CUDA错误的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1852
这是一个bug报告，主要涉及TensorRT-LLM下的LoRA模型推理速度比基础模型慢的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1851
这是一个bug报告，涉及的主要对象是TensorRT-LLM代码中的条件编译逻辑结构。由于忽略了在 #endif 后添加 FAST_BUILD 注释，导致出现了 endif 丢失 FAST_BUILD 注释的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1850
这个issue类型是bug报告，涉及主要对象为TensorRT-LLM。由于int4 checkpoint下无法释放tmp workspace导致的Assertion错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1849
这是一个关于TensorRT-LLM中Quantization无法在消费级硬件上成功运行的bug报告。主要涉及的对象是模型Quantization过程中的CUDA内存不足问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1848
这个issue类型为用户提出需求，涉及主要对象为TensorRT-LLM。由于argument parser的设计不够灵活和可重用，用户提出了关于重构argument parser来实现更好的可读性和重用性的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1847
这是一个bug报告，主要涉及TensorRT-LLM中t5模型在批处理大小大于1时推理结果错误的问题。原因可能是在使用BERT和GPT插件时出现了特定情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1845
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM下的Medusa Weight Only Quantize功能。由于remove_input_padding参数启用，但opt_num_tokens参数未设置，导致了trtllmbuild命令运行时崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/1844
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的llava模型在不同batch情况下推理结果不一致。原因可能是数据批量处理方式不同导致结果差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/1843
这是一个Bug报告类型的Issue，主要涉及TensorRT-LLM下关于使用W4A(FP)8量化结合bf16数据类型的问题。由于缺少对零点值的条件检查，导致量化过程出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1842
这是一个bug报告，主要涉及到SwigLU在TensorRTLLM中的实现不同于主流方法的问题，可能导致异常结果和难以诊断的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1841
这个issue是一个bug报告，涉及主要对象是TensorRT-LLM项目。由于内存工作空间释放失败导致了Assertion failed错误，用户寻求解决在构建引擎时遇到的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1840
这是一个bug报告，主要涉及升级TensorRT-LLM版本时出现的错误，由于参数转换相关问题导致运行步骤时产生错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1839
这是一个关于运行LLM引擎文件时出现错误的bug报告，用户在安装TensorRT时遇到版本不兼容导致的undefined symbol错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1838
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的TensorRT engine与Triton推理服务器不兼容所导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1836
这个issue属于软件升级类，用户提出了升级Pillow库版本的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1835
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM。由于在加载Gemma时未设置`share_embedding_table`导致无法载入Gemma，因此用户提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1834
这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的BertForSequenceClassification模型。由于模型输入发生变化，需要支持`remove_input_padding`，并对`build.py`脚本进行改进，添加一个新的`run_remove_input_padding.py`演示脚本来展示如何构建模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1833
这是一个Bug报告，涉及的主要对象是TensorRT-LLM下的convert_checkpoint.py脚本。由于环境配置和脚本执行步骤的问题，导致无法成功运行int8权重量化的转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/1832
这个issue属于用户提出需求类型，主要涉及IFB支持添加到更多模型的时间表。由于目前只有少数模型支持IFB，缺乏对其他模型的支持严重影响了TRTLLM框架在生产环境中的实用性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1831
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM模块的导入问题，可能由于安装不完整或路径配置错误导致了无法导入tensorrt_llm的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1830
这是一个bug报告，涉及TensorRT-LLM中使用dtype=bf16时，在处理大文本长度时产生垃圾数据的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1829
这是一个bug报告，主要涉及到系统信息的测试。用户未收到响应可能是因为缺少必要的信息或系统设置问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1828
这是一个用户提出需求的issue，主要涉及在TensorRT-LLM上新增GLM-4系列模型的支持。原因是新的GLM4系列模型具有更好的性能和准确性，用户期望TensorRT能够官方支持这些模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1827
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中使用Qwen1.5-32B模型时出现的满足优化配置文件限制的问题。可能是由于输入Tensor的维度设置不符合优化配置文件导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1826
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM。由于MPI错误导致了程序终止并抛出异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1825
这是一个关于解决"Failed to build mpi4py"错误的bug报告，主要涉及TensorRT-LLM项目。由于缺少先决条件，导致构建过程中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1823
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在使用in-flight batching时性能下降。问题出现的原因可能是由于配置文件中的参数设置不正确导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1822
这是一个bug报告，涉及TensorRT-LLM下的MMLU script，由于代码中出现了TypeError导致问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1821
这是一个bug报告，涉及TensorRT-LLM在Windows上运行时出现CUDA运行时错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1820
这个issue是一个功能需求提问，涉及到TensorRT-LLM中的top_p和top_k采样机制的关系。其提问的核心是为什么在已设定top_k条件下会跳过top_p采样。

https://github.com/NVIDIA/TensorRT-LLM/issues/1819
这是一个bug报告，主要涉及TensorRT-LLM中的pipeline parallelism问题，用户希望改善LLM推理时的token生成速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/1818
该问题类型为用户提出需求，涉及主要对象为`GenerationSession`。由于用户需要在VAD标记音频片段为静音时，结束正在进行的解码，因此提出了如何在`GenerationSession`中取消解码的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1817
这是一个bug报告，涉及TensorRT-LLM下的internlm2-chat-20b模型转换时缺少"--int8_kv_cache"选项的问题。这个问题导致无法构建int8_kv_cache internlm2chat20b模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1816
这是一个关于构建错误的Bug报告，主要涉及TensorRT-LLM在构建llava-v1.6-34b模型时失败的问题。原因可能是模型不受支持或存在构建配置错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1815
这是一个bug报告，主要涉及对象是TensorRT-LLM中的ModelRunner。这个问题是由于token数组不在内存中连续排列导致的，使得创建的数组偏移量无效，进而影响了`stop_words_list`功能在batch_size > 1时的正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1814
这是一个bug报告，主要涉及TensorRT-LLM中的模型转换失败问题，可能由于参数设置错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1813
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中无法在Triton Server上部署多模型模型的问题。由于tensorrtllm_backend不支持多模型导致这一问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1812
这是一个bug报告，主要涉及TensorRT-LLM模型加载失败的问题，由于缺少'use_context_fmha_for_generation'键而导致加载失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1811
这是一个bug报告，涉及到了安装脚本中的冗余空格问题。由于安装脚本中存在不必要的空格，导致安装过程可能出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1810
这是一个关于bug的问题，主要涉及TensorRT-LLM中的权重精度配置，用户询问在设置"--use_weight_only --weight_only_precision int8 --qformat fp8"时，量化是否为INT8还是FP8，由于实际情况是权重为torch.int8，而期望的行为是权重为FP8，因此用户提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1809
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中executor API忽略了`prompt_vocab_size`参数导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1808
这个issue类型为bug报告，涉及的主要对象是TensorRT-LLM。由于key_size大于remaining_buffer_size导致了"gptSessionBenchmark"的失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1807
这是一个bug报告，涉及 TensorRT-LLM 中的 `clusterkey` 选项，用户尝试在不同硬件上部署模型时遇到了问题。因为 cluster key 似乎在部署时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1806
这个issue是关于bug报告，主要涉及TensorRT-LLM的CUDA运行时错误。由于尝试在不支持的硬件上进行前向兼容性尝试，导致了CUDA运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1805
这是一个bug报告，主要涉及TensorRT-LLM的模型测试时间问题，可能是由于MPI运行时无法访问或执行可执行文件导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1804
这是一个bug报告，主要涉及对象是pip安装过程中出现的错误。由于参数设置错误导致metadata生成失败，进而导致安装过程出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1803
这是一个bug报告，主要涉及对象为TensorRT-LLM模型加载失败的问题。由于缺少关键参数'use_context_fmha_for_generation'导致加载失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1802
这是一个bug报告，主要涉及TensorRT-LLM下的latency reduction问题，用户询问关于"--weight_sparsity"和稀疏性如何与量化一起使用的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1801
这个issue类型是用户提出需求，涉及到`batch_manager::GenericLlmRequest`的logitsPostProcessor处理批量请求的问题，需要解决如何处理一批请求的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1799
这是一个bug报告，主要涉及到TensorRT-LLM下的Gemma模块。原因是`from_hugging_face`未将`shared_embedding_table`设置为`True`，导致无法加载Gemma模块。

https://github.com/NVIDIA/TensorRT-LLM/issues/1798
这是一个关于需求提出的issue，主要涉及的对象是TensorRT-LLM中的Medusa with Mixtral 8x7B。导致这个问题的原因是当前的Medusa convert_checkpoint.py不支持Mixtral，用户希望获得关于支持Mixtral的相关提示和帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1797
这是一个用户提出需求的 issue，主要涉及如何自定义模型中的 position_ids，原因是模型中的一些 position_ids 相同，用户希望能够自行设置 position_ids。

https://github.com/NVIDIA/TensorRT-LLM/issues/1796
这是一个关于TensorRT-LLM中使用`examples/run.py`时出现`RuntimeError`的bug报告，问题涉及主要对象为TensorRT-LLM库。这个问题的症状是在运行脚本时出现了断言失败的错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1795
这是一个bug报告，涉及TensorRT-LLM下的Medusa模型解码结果与源模型不同的问题，可能是由于模型转换或推理过程中的参数设置不一致导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1793
这是一个包含bug修复、功能更新和API变更的issue，主要涉及TensorRT-LLM模型的更新和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/1792
这是一个 bug 报告类型的 issue，涉及主要对象为 TensorRT-LLM。由于系统环境和版本信息不匹配，导致在构建 w4a8_awq/int4_awq 时出现错误和警告信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1791
这个issue是关于bug报告，主要涉及TensorRT-LLM的安装问题，导致无法找到tensorrt模块。造成这个问题的原因可能是v0.10.0版本的安装未能安装所需的tensorrt依赖。

https://github.com/NVIDIA/TensorRT-LLM/issues/1790
这是一个功能需求的问题，主要涉及到CogVLM模型在处理多个图片输入位置时的限制。由于CogVLM固定了输入图片的数量及位置，导致用户无法灵活添加多个图片输入位置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1788
这是一个关于Bug报告的Issue，主要涉及TensorRT-LLM在长输入任务上输出重复问题。可能是由于TensorRT-LLM在特定任务中生成的输出中出现了重复现象，而vLLM却没有这种问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1787
这是一个bug报告，主要涉及TensorRT-LLM库安装时出现的构造函数匹配问题。由于构造函数的重复定义导致了安装错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1786
这是一个关于运行`convert_checkpoint.py`文件的bug报告，涉及到TensorRT-LLM库中的模型导入问题。该问题由于未能正确导入模型配置对象`LLaMAConfig`导致脚本无法正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1785
这是一个关于bug报告的issue，主要涉及Qwen2 1.5B checkpoint的转换问题。由于1.5B conversion无法正常工作，用户寻求帮助解决问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1784
这是一个bug报告，主要涉及TensorRT-LLM下的形状相关错误问题，由于批大小和束宽维度不相等导致的内部错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1783
这个issue是关于bug报告，主要涉及的对象是TensorRT-LLM，问题出现是由于CUDA内存耗尽导致torch.cuda.OutOfMemoryError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1781
This is a bug report related to the TensorRT-LLM model, where passing a specific LoRA as input causes the model to go into an unusable state, failing all subsequent requests.

https://github.com/NVIDIA/TensorRT-LLM/issues/1780
这个issue是关于文档问题的报告，用户需要了解如何设置输入和输出长度以达到模型默认长度的要求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1779
这是一个bug报告，主要涉及TensorRT-LLM下的编译错误问题。原因可能是由于wgmma指令在函数调用时跨越函数边界而导致的性能下降。

https://github.com/NVIDIA/TensorRT-LLM/issues/1778
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM下的参数未使用的问题，用户对输出质量存在疑虑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1777
这是一个关于功能需求的issue，主要涉及Quantizing Mixtral模型到fp8过程中的设备选择问题，用户寻求对通过CPU进行模型量化的结果以及选择适当方法的建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/1776
这是一个bug报告，主要涉及的对象是将LLaVa模型转换为TensorRT，导致无法成功转换的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1775
这是一个报告bug类型的issue，涉及的主要对象是TensorRT-LLM中的ChatGLM3 6B模型。原因是在特定条件下运行benchmark.py脚本时出现错误，可能是与输入输出长度有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/1774
这是一个bug报告，主要问题是在安装完TensorRT-LLM之后在虚拟环境中导入时出现ImportError，导致libnvinfer未找到。

https://github.com/NVIDIA/TensorRT-LLM/issues/1773
这是用户提出的关于功能需求的issue，主要涉及TensorRT-LLM输出约束为JSON或调用函数工具，由于用户希望能够在使用TensorRTLLM与Modal时实现类似于OpenAI API方式的输出和调用，以提高效率。

https://github.com/NVIDIA/TensorRT-LLM/issues/1771
这个issue是关于Bug报告，主要涉及到TensorRT-LLM中的InferenceRequest::serialize方法，由于不处理logits后处理器，导致在使用C++ GptManager时无法正确同步请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1770
这是一个bug报告，主要涉及TensorRT-LLM下的构建问题，由于设置的参数不符合预期导致trtllmbuild构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1769
这是一个发布版本相关的issue，主要对象是项目中的版本发布。

https://github.com/NVIDIA/TensorRT-LLM/issues/1768
这是一个bug报告类型的issue，涉及的主要对象是在TensorRT-LLM docker中使用fastapi_server.py作为服务器。由于缺乏响应，用户寻求关于在特定环境下运行fastapi_server.py时出错的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1767
这是一个关于bug报告的issue，主要涉及对象是C++ GptManager和TensorRTLLM。由于使用`setEndId`方法在并行计算过程中可能导致engine hang/frozen。

https://github.com/NVIDIA/TensorRT-LLM/issues/1766
这是一个bug报告， 主要涉及TensorRT-LLM在部署模型时出现的问题，可能是由于代码中的属性错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1764
这是一个bug报告，主要涉及在安装 `tensorrt-llm==0.11.0.dev2024060400` 时遇到了导入错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1763
这是一个关于更新TensorRT-LLM的issue，涉及的主要对象是Model Support, Features, API 和Bug fixes。由于引入了一系列破坏性更改和修复，可能会导致无法正确调用`convert_hf_mpt_legacy`函数以及其他API变更或修复问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1762
这是一个用户提出需求的issue，主要涉及在TensorRT-LLM中支持自定义标定数据集，用户希望能够提供一个带有`text`列的`json`或`jsonl`文件用于标定。

https://github.com/NVIDIA/TensorRT-LLM/issues/1761
这是一个bug报告，主要涉及到TensorRT-LLM的rest toke latency识别问题。造成这个问题的原因可能是模型在输入输出长度为1024和512时出现了不符合预期的延迟。

https://github.com/NVIDIA/TensorRT-LLM/issues/1760
这是一个bug报告类型的issue，主要涉及TensorRT-LLM库在使用过程中遇到的cudaGetDeviceCount()错误。可能是由于CUDA错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1759
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM中的Internlm2，问题出现的原因可能是在非相邻GPU上运行时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1758
这是一个用户提出需求的issue，主要涉及的对象是添加对DeepSeek MoE的支持。这个问题的原因是在TensorRT-LLM中添加了对DeepSeek MoE模型的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1757
这是一个关于AWQ性能问题的bug报告，涉及主要对象是TensorRT LLM。原因可能是AWQ在批处理大小大于等于8时吞吐量较低。

https://github.com/NVIDIA/TensorRT-LLM/issues/1756
这是一个bug报告，涉及TensorRT-LLM下的Whisper Decoder Batch Prompting Issue。由于不同批次具有不同的初始提示，导致了填充应用于context_length的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1755
这个issue是一个关于需求的问题，主要涉及的对象是TensorRT-LLM中针对BERT模型是否支持`--remove_input_padding`特性，用户询问是否有计划支持该功能，因为当前配置中没有找到相关参数设置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1754
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中启用对等访问失败的问题，导致在部署llama38B时出现错误提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/1753
这是一个bug报告，涉及TensorRT-LLM下Enc-Dec C++ Runtime Paged KV模块。由于使用多个输入文本进行推断时，输出结果异常，可能是由于C++ runtime + inflight batching的发布存在某些问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1752
这是一个用户提出需求的问题，主要涉及对象是在GCP上配置包含NVIDIA软件安装的虚拟机实例。由于缺少必要的NVIDIA安装，用户无法成功运行特定的docker命令。

https://github.com/NVIDIA/TensorRT-LLM/issues/1751
这是一个更新issue，涉及的主要对象是将gradio从4.19.2更新到4.36.0版本，原因是添加了监控仪表板和修复了一些问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1750
这是一个用户询问问题的issue，主要涉及TensorRT-LLM在构建过程中可调节的性能参数以及构建过程带来的性能优化的话题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1749
这是一个bug报告，主要涉及TensorRT-LLM下的ChatGLM3量化过程中INT8/INT4失败的问题。原因可能是由dtype参数引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1748
这是一个bug报告，涉及的主要对象是TensorRT-LLM项目。由于缺少对`cudaStream_t`的声明，导致了编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1747
这个issue是关于bug报告，涉及TensorRT-LLM下的 llama3-8b-instruct 运行失败的问题，可能是由于使用了 `use_py_session` 参数导致执行不成功。

https://github.com/NVIDIA/TensorRT-LLM/issues/1746
这个issue属于bug报告，主要涉及TensorRT-LLM中的Multi-Batch Error with Quantization Int8/Int4问题，可能由于版本升级或配置错误导致无法正确运行benchmark.py。

https://github.com/NVIDIA/TensorRT-LLM/issues/1745
这是一个bug报告类型的issue，涉及主要对象为TensorRT-LLM，由于高并发请求导致响应时间增大。

https://github.com/NVIDIA/TensorRT-LLM/issues/1744
这个issue类型是用户询问问题类型，主要涉及对象是TensorRT-LLM中的ModelRunner和ModelRunnerCpp，问题源于用户想了解这两者之间的区别并寻求相关文档资料。

https://github.com/NVIDIA/TensorRT-LLM/issues/1743
这是一个特性请求问题，主要涉及TensorRT-LLM中加载模型部分时只使用一个GPU的需求。因为用户想要测量模型的性能指标，但当前在TensorRT-LLM中必须使用所有GPU运行模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1742
这是一个bug报告，该问题主要涉及的对象是TensorRT-LLM中的模型编译和随机种子问题。由于未正确使用随机种子导致模型输出的不准确性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1741
这是一个bug报告，主要对象是Quantizing Phi-3 128k指令到FP8的过程。问题出现在将Quantized Phi3权重保存时出现了错误，并给出了'unknown:Phi3ForCausalLM'的详细导出错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1740
这是一个性能问题的bug报告，主要涉及TensorRT-LLM下的whisper模块。由于解码过程缓慢，导致延迟高、结果不正确且不一致，用户寻求帮助和建议来解决问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1739
这是一个bug报告，主要涉及TensorRT-LLM模块下的GEMM tactics profiling功能出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1738
这是一个关于TensorRT-LLM下"Inflight batching for fp8 Llama and Mixtral is broken"的bug报告，报告者在使用Triton Inference Server进行模型测试时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1737
这是一个更新网页页面的Issue，主要对象是项目的Github Pages。可能由于需要更新网页内容或修复已有页面的问题而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/1736
这是一个用户提出需求的问题，主要涉及TensorRT-LLM下的ModelRunner类，用户请求如何使用`enable_block_reuse`参数。可能由于缺乏文档指导，导致用户难以使用该参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/1734
这是一个发布新版本更新的issue，主要涉及的对象是TensorRT-LLM。通过添加新功能和改进，解决了一些先前版本中的问题，以提高性能和功能性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1733
这是一个bug报告，涉及TensorRT-LLM下的模型转换问题，原因是转换过程中遇到了特定的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1732
这是一个bug报告，涉及对象是TensorRT-LLM下的模型转换过程。由于未指定的错误文件不包含期望的张量模型，导致了无法成功转换模型的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1731
这是一个关于模型性能问题的问题，涉及对象为模型性能优化。这个问题可能是由于稀疏性设置未正确应用或优化未达到预期效果所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1730
这是一个bug报告，主要涉及的对象是TensorRT-LLM编译过程中出现的警告信息。由于函数过大导致生成的调试信息可能不准确，可能是由于编译器版本或配置设置不当导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1729
这是一个用户问问题的issue，主要涉及TensorRT-LLM中如何在v0.9.0版本中打开inflight batching功能。使用inflight batching的方法"use_inflight_batching"在trtllmbuild中已在v0.9.0版本中移除。

https://github.com/NVIDIA/TensorRT-LLM/issues/1728
这是一个bug报告，主要涉及TensorRT-LLM中convert_checkpoint.py转换LLama3 hf格式时出现了"killed"的打印输出。可能的原因是程序被强制终止导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1727
这是一个关于功能需求的issue，主要涉及的对象是TensorRT-LLM中的Whisper。由于特定词汇未能正确定位，用户在使用initial prompt时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1725
该issue属于bug报告类型，主要涉及TensorRT-LLM的更新和功能改进。Bug修复方面涉及解决了`qkv_bias` shape的问题以及Ada traits for `fpA_intB`的错误。这些问题可能由于代码bug或者需求变更引起。

https://github.com/NVIDIA/TensorRT-LLM/issues/1724
这个issue属于bug报告，主要涉及TensorRT-LLM中的条件语句在评估时机上存在问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1723
这是一个bug报告，涉及TensorRT-LLM中的nmt模型预归一化权重转换问题。原因是fairseq会在ffn之前使用原始的`final_layer_norm`，尽管他们称之为`final_layer_norm`。

https://github.com/NVIDIA/TensorRT-LLM/issues/1722
这是一个Bug报告，涉及主要对象为LLama370B模型以及AWQ优化性能问题。该问题可能由于模型在8 x H100 GPUs上，经过特定的batch size之后，AWQ性能下降导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1721
这是一个bug报告类型的issue，主要涉及Lora support with LLama3-70B and AWQ Quantization。由于Lora Adapters与AWQ quantized Llama370B模型的兼容性问题，导致构建过程中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1720
这是一个bug报告issue，主要涉及的对象是在Flask服务中运行Baichuan模型示例时出现的问题。由于在代码中初始化runner时出现了错误，导致runner值为False，导致进程卡在runner = runner_cls.from_dir(**runner_kwargs)这行代码。

https://github.com/NVIDIA/TensorRT-LLM/issues/1719
这是一个bug报告，主要涉及Triton server 服务执行大请求时响应时间过长的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1718
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM。用户提出了无法追踪每个请求的输入和输出标记计数以及无法添加新的响应变量的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1717
这个issue是关于bug报告，涉及到H20使用随机权重推断llama2-13B结果时出现除零错误的问题。这个问题是由于程序中某处使用了除以零的操作而引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1716
这是一个bug报告，主要涉及TensorRT-LLM的运行时维度问题，用户寻求关于优化不满足的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1715
这个issue是关于bug报告，涉及TensorRT-LLM在部署后每个请求异常都生成core.xxxx文件的问题，可能由于某些设置导致这种情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1714
这是一个Bug报告，主要涉及TensorRT-LLM中生成引擎时出现错误，可能是由于维度参数错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1713
这是一个bug报告，用户在部署LLaMA3模型到Triton服务器时遇到重复生成答案的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1712
这是一个bug报告，涉及的主要对象是SmoothQuantGatedMLP中的mlp_hidden_size参数，由于参数设置错误导致在Qwen model下出现了不符合预期的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1711
这是一个Bug报告，主要涉及到TensorRT-LLM下的输出生成问题，用户寻求关于模型输出不在停止标记</s>处停止的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1710
这是一个用户请求寻找Dockerfile以在TensorRT-LLM编译文件上运行推断的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1709
这是一个关于bug报告的问题，主要涉及TensorRT-LLM下的Llava multimodel example，用户遇到了segfault问题。导致该问题的原因可能是软件配置不匹配或代码逻辑错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1708
这是一个关于bug的报告，主要对象是ModelRunner.generate()方法缺少kwargs参数，导致程序运行时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1707
这个issue是关于功能需求的，主要涉及到TensorRT-LLM中的Grouped Diverse Beam Search功能。由于无法更改分组和大小以及设置beam_diversity_rate后输出没有明显变化，用户寻求如何实现更多样化的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/1706
这是一个Bug报告，涉及的主要对象是TensorRT-LLM下的Python 3.11支持。由于TensorRT仅支持Python 3.10，用户遇到了无法在Windows安装或使用Python 3.11导致的多个错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1705
这个issue类型是bug报告，主要涉及的对象是构建TensorRT-LLM的Docker镜像。由于构建出来的镜像大小为357GB，导致该问题被提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/1704
这是一个bug报告，主要涉及TensorRT-LLM版本升级后镜像大小急剧增加的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1702
这是一个bug报告，主要涉及TensorRT-LLM的Llama 3.0在tp尺寸为4时失败，可能由于mpirun失败导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1701
这是一个bug报告，涉及的主要对象是LLama 70B模型在生成结果时出现异常。由于使用LLama 70B模型生成结果异常，用户寻求帮助解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1700
这是一个bug报告，涉及 TensorRT-LLM 下运行时出现的 MPI 运行时错误，导致进程异常退出。

https://github.com/NVIDIA/TensorRT-LLM/issues/1699
这个issue类型是用户询问问题，主要对象是关于TensorRT-LLM下的部署模型在基准测试中的表现情况，原因是想了解以TensorRTLLM部署的Llama 3模型在评估基准中的结果是否与报告的Llama 3性能相一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1698
这是一则用户需求报告，主要涉及对象是TensorRT-LLM中的convert_checkpoint.py工具。用户想要在本地文件夹中使用已下载的模型，并希望让convert_checkpoint.py工作于这个本地文件夹。

https://github.com/NVIDIA/TensorRT-LLM/issues/1697
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的Whisper，由于某些原因导致了高Word Error Rate和不完整的音频转录。

https://github.com/NVIDIA/TensorRT-LLM/issues/1696
这是一个bug报告类型的issue，涉及TensorRT-LLM下构建失败的问题，由于worker数量大于1时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1695
这是一个bug报告，涉及到TensorRT-LLM与Tritonserver的版本不匹配的问题。由于TensorRT-LLM版本与Tritonserver版本不一致，导致用户在构建LLaMA引擎时遇到问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1694
这是一个bug报告，涉及到LLama3 sq(per_token + per_channel)在主分支上构建失败的问题。可能是由于代码中的某些问题引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1693
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的int4_awq和w4a8_awq是否支持deepseek的问题。由于使用w4a8_awq转换deepseek6.7bbase模型时遇到错误，导致命令无法成功运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1692
这是一个Bug报告issue，关于TensorRT-LLM项目中的安装要求发生了问题。由于最近的提交导致了安装的要求文件错误，用户在尝试安装时遇到了问题，并请求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1691
这是一个关于如何使用`use_paged_context_fmha`功能的问题，用户询问为什么在使用该功能构建引擎并启用`enable_kv_cache_reuse`后，随着QPS的增加，延迟并没有显著增加。

https://github.com/NVIDIA/TensorRT-LLM/issues/1689
这是一个需求汇报issue，主要涉及的对象是在TensorRT-LLM下的/examples/multimodal中更新transformers版本至4.38.0。因为更新了transformers版本，可能出现了新特性或优化，需要验证新版本在项目中的适用性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1688
这是一个更新内容的issue，主要涉及TensorRT-LLM的模型支持、特性更新和API变动。

https://github.com/NVIDIA/TensorRT-LLM/issues/1687
这是一个bug报告，涉及TensorRT-LLM中的Offloading to host memory功能导致错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1686
这是一个bug报告，主要涉及TensorRT-LLM中转换权重的问题，导致转换和构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1685
这是一则bug报告，涉及主要对象是TensorRT-LLM下的GenerationSession类。由于在python GenerationSession.setup方法中调用torch.empty导致在GPU内存中不断分配新内存，提出应该在GenerationSession的__init__方法中使用_init_cache_buffer方法来初始化缓存生成缓冲区，以避免内存碎片化和加快内存分配速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/1684
这是一个用户提出需求的issue，主要涉及控制ngram重复问题的问题。由于使用自定义Python代码控制重复效率太低，用户希望知道如何使用何种接口控制ngram重复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1683
该issue属于用户提出需求的类型，主要涉及TensorRT-LLM项目是否考虑采用min_p采样策略。根据用户提供的一些研究，min_p采样被认为优于其他采样策略，因此用户建议在项目中采用min_p采样方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/1682
这是一个bug报告，主要涉及TensorRT-LLM中llama3-70b模型转换失败的问题。由于在v0.9.0版本中使用了use_parallel_embedding和tp_size = 8时导致转换过程出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1681
这是一个特性请求，主要涉及到TensorRT-LLM中的logits processor，由于每个请求都会独立调用logits processor导致性能下降约10%。

https://github.com/NVIDIA/TensorRT-LLM/issues/1680
这个issue是一个“用户提出需求”的类型，主要涉及的对象是TensorRT-LLM中的C++ Executor API。由于C++ Executor API当前只支持函数而不支持类的方式来控制语言模型生成，因此用户提出希望添加LogitsProcessor类支持的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1679
这是一个bug报告，涉及到TensorRT-LLM中的run_profiling功能缺少了medusa参数。可能是由于未正确添加参数导致无法执行正确的分析操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1678
这是一个 bug 报告，主要涉及 TensorRT-LLM 中使用4个 GPU 运行 qwen1.5-72b-int4-gptq 在生成时出现 Segmentation fault 的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1677
这是一个bug报告，主要涉及TensorRT-LLM中将Meta-Llama-3-70B-Instruct模型转换为TensorRTLLM checkpoint格式时失败的问题。产生这个问题的原因可能是转换过程中的某些错误设置或步骤。

https://github.com/NVIDIA/TensorRT-LLM/issues/1676
这是一个bug报告，涉及到TensorRT-LLM的quantize.py在导出配置文件config.json时遗漏了重要数据（如rotary scaling），导致引擎构建时生成了无效结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1675
这是一个关于bug报告的issue，主要涉及的对象是在将Qwen1.50.5BChat模型转换为TensorRT格式时出现的错误。这个问题的原因可能是由于代码中的某些bug或配置错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1674
这个issue是关于用户提出需求，涉及的主要对象是为TensorRT-LLM添加Huggingface model zoo。原因是用户希望讨论如何确定适当的配置以上传模型到Model Zoo。

https://github.com/NVIDIA/TensorRT-LLM/issues/1673
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中fpA-int8 group&zero GEMV的移除问题，用户想要了解为何这一功能被移除并寻求帮助解决相关症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1672
这是一个bug报告，主要涉及TensorRT-LLM中如何在多次运行中更改`num_beams`参数，由于无法成功动态调整`num_beams`参数导致出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1671
这是一个关于功能需求的issue，主要涉及到TensorRT-LLM中的kv-reuse功能的使用情况。原因可能是文档缺失导致用户对功能的有效性和可用性产生疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1670
这是一个bug报告，主要涉及TensorRT-LLM下的一个issue，用户遇到了在8xV100上部署llama3到triton时，NCCL报告'out of memory'的问题，其中尝试设置特定参数能解决，但在设置其他参数时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1669
这是一个bug报告，主要涉及在训练时使用rslora scaling导致权重需不同的缩放比例，导致代码在处理方面的缺陷。

https://github.com/NVIDIA/TensorRT-LLM/issues/1668
这是一个bug报告，涉及到TensorRT-LLM中LoRA权重在使用rslora训练时alpha scaling计算不正确的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1667
这是一个关于bug报告的issue，主要涉及Multi-node inference中的设备顺序问题，由于设备映射配置错误导致脚本运行时出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1666
这是一个bug报告，主要涉及TensorRT-LLM中的模型转换过程，由于代码中的错误导致了无法成功转换特定模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1665
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM的Llava 1.5 7B转换过程失败，用户询问是否由于GPU内存不足而出现错误，并寻求解决方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/1664
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的ModelRunnerCpp.from_dir()方法出现了一个未预期的关键字参数 'gpu_weights_percent' 的问题。这个问题可能是由于方法调用中传入了不支持的参数导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1663
这个issue是关于需求的，主要涉及到TensorRT-LLM的版本兼容性问题，用户希望得到支持TRT10.1版本的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1662
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的checkpoint转换问题。根据描述，可能是由于Transformers版本不匹配导致了转换失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1661
这是一个bug报告，主要涉及TensorRT-LLM下的一个issue，由于设置了`use_cache=False`导致了运行时出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1660
这是一个bug报告，涉及TensorRT-LLM中nmt weight conversion的问题，主要原因是decoder的vocab size设置错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1659
这是一个关于部署Bert模型与TRT-LLM和Triton服务器相关的bug报告，涉及主要对象为部署过程中出现的问题。原因可能是部署过程中的配置或代码逻辑问题导致模型无法成功部署。

https://github.com/NVIDIA/TensorRT-LLM/issues/1658
这个issue类型是用户提出需求，并涉及了关于自定义内核实现与跨注意力相关的问题，用户寻求创建用于跨注意力的自定义内核的指南或参考。

https://github.com/NVIDIA/TensorRT-LLM/issues/1657
这是一个用户提出需求的类型的issue，主要涉及的对象是TensorRT-LLM 下的 Cohere family of models。用户请求支持这一系列的模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1656
这是一个Bug报告类型的Issue，主要涉及到TensorRT-LLM的安装问题。由于缺少特定的模块导致了DLL加载失败的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1655
这是一个bug报告，主要涉及TensorRT-LLM中Executor超时时间问题，由于超时时间设置不够，导致引擎在某些情况下无法使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/1654
这个issue是关于用户提出一个功能需求的类型，主要涉及的对象是TensorRT-LLM中的kv cache。原因是用户想通过增加"priority"来实现特定查询不留在缓存中的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1653
这是一个关于bug报告的issue，主要涉及Qwen-VL模型在加速过程中处理多张图片时输出结果不一致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1652
这是一个性能问题报告，主要涉及的对象是使用A100 GPU的TensorRT-LLM模型。这个问题由于性能（延迟和吞吐量）表现非常差，可能源于环境设置或代码实现的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1651
这是一个关于性能提升问题的bug报告，涉及TensorRT-LLM下的WOQ在Whisper模型中未获得性能提升的情况。原因可能与GPU架构有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/1650
这是一个bug报告，主要涉及TensorRT-LLM中的llama conversion with smooth quant功能。原因是在引擎转换过程中出现了错误，导致输出不符合预期或性能表现不如预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/1649
该issue类型为bug报告，主要涉及TensorRT-LLM下的模型转换问题，由于CUDA内存错误导致无法将模型转换为8位整型权重。

https://github.com/NVIDIA/TensorRT-LLM/issues/1648
这是一个bug报告类型的issue，主要涉及的对象是针对V100 w8 llama模型推理失败的问题。该问题导致了在进行引擎推理时出现了错误的行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/1647
这是一个bug报告，涉及的主要对象是在Docker镜像构建中遇到了"fatal error: mpi.h: No such file or directory"错误。该问题可能是由于缺少mpi.h文件导致的编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1646
这个issue是关于优化python基准测试日志记录的，主要包括对logging机制的更新和统一，以及新增的print_report_dict方法和优化后对空csv文件的检查，旨在避免重复写入重复的表头信息到同一个csv文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/1645
这是一个bug报告，涉及TensorRT-LLM中的FP8量化过程，导致出现"Error: FP8 quantize Integer divide-by-zero"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1644
这是一个用户提出需求的issue，主要对象是为TensorRT-LLM添加对CogVLM2模型的支持。这个需求出现的原因是CogVLM2被认为是一款优秀的开源多模态模型，社区希望为其提供TensorRT-LLM引擎以进一步提升模型应用的效率。

https://github.com/NVIDIA/TensorRT-LLM/issues/1643
这是一个用户请教问题类型的issue，主要涉及的对象是关于TensorRT-LLM中的`RowLinear`和`ColumnLinear`使用问题。用户提出了不清楚如何选择合适的`Linear`层的困惑，希望了解何时使用`RowLinear`和`ColumnLinear`的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1642
这个issue属于bug报告类型，主要涉及TensorRT-LLM库中的使用问题。由于传入的参数格式有误，导致停用词列表无法正确地被使用，最终造成了执行失败的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1640
这个issue是一个bug报告，主要涉及的对象是 gradio 库的版本更新。这个问题是由于更新版本导致一些功能异常或错误，用户需要修复这些问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1639
这是一个bug报告，涉及TensorRT-LLM更新中的问题。由于错误的`top_k`类型和`use_fp8_context_fmha`导致了输出错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1638
这是一个bug报告，主要涉及TensorRT-LLM下的int4 reasoning异常，可能由于量化过程中的设置或参数问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1637
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的qwen1.5-32b-gptq-int4模型。这个问题导致了qkv.bias错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1636
这是一个bug报告，涉及主要对象为TensorRT-LLM中的AWQ量化，问题出现在使用较高的张量并行度时导致INT4 AWQ量化失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1635
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM。由于参数校验失败和张量形状错误导致了API使用错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1634
这是一个bug报告，主要涉及TensorRT-LLM下构建特定模型时遇到的问题，可能由于环境配置或兼容性导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1633
这是一个bug报告类型的issue，涉及对象为TensorRT-LLM库是否支持A10 GPU。导致问题的原因可能是TensorRT-LLM不支持A10导致在A10服务器上运行时出现段错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1632
这是一个Bug报告，涉及的主要对象是TensorRT-LLM系统。由于零温度的curl请求影响了非零温度的请求，导致了系统出现异常行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/1631
该issue属于用户提出需求类型，主要涉及的对象是在TensorRT-LLM中进行fp8量化过程中如何切换量化形式（E4M3或E5M2）；用户提出了关于如何切换量化形式的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1630
这是一个bug报告类型的issue，涉及到在构建Mixtral-8x22B-v0.1 with w4a16时出现错误的问题。由于版本不匹配或支持，导致构建过程出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1629
这是一个bug报告，主要涉及的对象是Mixtral模型的Checkpoint创建过程。该问题由于未将部分张量移动到CPU导致CUDA内存溢出，需要将w1、w2和w3张量转移到CPU才能完成堆叠操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1628
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中转换模型时遇到的问题。由于转换时出现了错误的Traceback信息，导致无法成功转换和构建模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1627
这是用户提出的关于如何在TensorRT-LLM中使用自定义模型而不是基于HuggingFace的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1626
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的auto parallel + int4 quantization功能。由于某种原因在尝试应用这两个功能时出现了报错，导致无法成功构建引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/1625
这个issue属于bug报告，主要涉及TensorRT-LLM下的推理（Inference），用户在使用Qwen1.5-14B-Chat模型时遇到了CUDA runtime错误，具体涉及到peer access不支持的问题，而当使用use_custom_all_reduce参数时却能正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1624
这是一个关于构建TensorRT-LLM时遇到文件缺失错误的bug报告。

https://github.com/NVIDIA/TensorRT-LLM/issues/1623
这个issue是关于用户需求的，主要涉及到在流处理过程中增加每个chunk中发送的token数量。问题来源于用户希望了解如何实现增加chunk大小以及如何应用于tritoninferenceserver。

https://github.com/NVIDIA/TensorRT-LLM/issues/1622
这是一个bug报告，涉及TensorRT-LLM版本号不匹配的问题，导致用户期望版本为 0.10.0 实际版本为 0.11.0。

https://github.com/NVIDIA/TensorRT-LLM/issues/1621
这是一个bug报告，涉及主要对象为TensorRT-LLM下的Windows构建问题，由于trt_root路径包含空格导致cmake错误，并需要在cmakelist.txt中为tensorrt_llm_nvrtc_wrapper添加IMPORTED_IMPLIB属性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1620
这是一个bug报告，主要涉及TensorRT-LLM下的distil-whisper/distil-large-v2模型，在使用librispeech_asr任务时出现了100%以上的识别错误率。原因可能是模型保存和使用的过程中出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1619
这是一个关于用户需求和困惑的问题，主要涉及TensorRT中Prefix Caching的工作原理以及与普通kv缓存之间的区别。由于描述中存在一些模糊不清的逻辑，导致用户难以理解如何优化性能和进行prompt相关的调整。

https://github.com/NVIDIA/TensorRT-LLM/issues/1618
这是一个关于性能问题的bug报告，主要涉及到TensorRT-LLM中的LLama-2 13B模型和SmoothQuant量化策略的性能表现差的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1617
这是一个关于bug报告类型的issue，主要涉及TensorRT-LLM中T5模型的Encoder无法在构建引擎时更改max_input_len参数，导致推理时仅支持长度为1024的输入，可能是由于代码中某处固定了max_input_len参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/1616
这是一个Bug报告，涉及TensorRT-LLM，由于MPI线程模式设置不正确导致程序出现卡顿。

https://github.com/NVIDIA/TensorRT-LLM/issues/1615
该问题属于用户提出需求，主要对象为Medusa int8权重量化。由于用户希望支持仅对权重进行int8量化，目前的实现可能无法满足该需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1614
这是一个关于Unsupported model architecture导致RuntimeError的bug报告。

https://github.com/NVIDIA/TensorRT-LLM/issues/1613
这是一个用户提出需求的类型，主要涉及对象是TensorRT-LLM中的默认参数`opt_num_tokens`，原因是用户认为默认设置为`max_batch_size * max_beam_width`不符合批处理中token数量的比例。

https://github.com/NVIDIA/TensorRT-LLM/issues/1612
这是一个Bug报告，主要涉及TensorRT-LLM下 Llama3 模型转换检查点的问题。由于不支持的数据类型导致了转换失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1611
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的Alibi模块，由于原有代码对非2的幂次头部的支持不完整导致了生成的结果错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1610
这是一个bug报告，主要涉及动态尺度问题导致的TensorRT-LLM引擎文件无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1609
这是一个用户报告的bug类型的issue，主要涉及的对象是在使用RTX 4070super GPU时构建llama3:8b-instruct模型遇到不适合GPU的问题，用户寻求解决办法或者已经构建好的模型版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/1608
这是一个关于功能疑问的问题，主要涉及TensorRT-LLM中的Executor API中Leader和Orchestrator两种模式的实现方式以及性能比较。

https://github.com/NVIDIA/TensorRT-LLM/issues/1607
这是一个关于性能问题的问题，主要涉及到GPU和Tensor Core的利用率，用户想了解为什么在A10G卡上运行Whisper TRT时，GPU利用率达到90%而Tensor Core利用率只有不到25%。

https://github.com/NVIDIA/TensorRT-LLM/issues/1605
这是一个用户提出疑问的issue，涉及对象为如何构建LLM模型引擎过程中是否有中间转换为onnx文件，并询问相关文件的位置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1604
这是一个bug报告，主要涉及的对象是mpirun和gptSessionBenchmark，原因是mpirun导致gptSessionBenchmark产生了大量占用额外显存的进程。

https://github.com/NVIDIA/TensorRT-LLM/issues/1603
这是一个bug报告，主要涉及Qwen-VL模型推理过程出现的错误。可能是由模型推理逻辑或输入数据格式等问题导致推理结果错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1602
这是一个关于构建TensorRT-LLM下cogvlm引擎时终端卡住的bug报告，用户在CentOS系统上使用A10 GPU进行int8量化部署cogvlm时遇到了终端卡住的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1601
这个issue是关于bug报告，涉及主要对象是在使用TensorRT-LLM时出现hang indefinitely的问题，原因是当另一个进程使用相同GPU进行ONNX推理时导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1600
这是一个用户提出问题的issue，主要涉及部署经过剪枝的模型时遇到的困惑。由于pruned模型中每个层的qkv维度不同，并且使用了torch.save来存储模型而不是save_pretrained，导致用户不清楚如何在TensorRT-LLM中使用此模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1598
这个issue类型是功能更新，涉及的主要对象是TensorRT-LLM。由于功能更新添加了一些新特性和改变原有API，可能需要用户注意新的API用法或迁移旧的代码。

https://github.com/NVIDIA/TensorRT-LLM/issues/1597
该问题属于用户提出需求类型，主要涉及TensorRT-LLM下的benchmark.py脚本，用户询问如何在不在allowed_configs.py中的情况下测试不同模型的性能。这可能是由于缺乏指导或配置文件的更新而引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1596
这是一个bug报告，主要涉及TensorRT-LLM下的Mixtral 8x7b运行时出现错误的情况。由于使用tp size为4或8时均导致运行失败，希望能得到解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/1595
这是一个关于bug报告的issue，主要涉及的对象是TensorRT-LLM中的rope scaling功能。由于无法使用rotary_scaling命令以及手动更改config.json值无效，导致用户无法使用rope scaling功能，需要寻求解决方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/1594
这是一个关于bug报告类型的issue，在TensorRT-LLM下的ModelRunner中无法使用`enableBlockReuse`选项，用户希望能够启用常见前缀缓存功能，但实际上无法在该运行时中指定该选项。

https://github.com/NVIDIA/TensorRT-LLM/issues/1593
这个issue属于用户提出问题类型，主要涉及到支持输入log概率的问题，用户询问当前是否支持C++ inflight batching backend。造成此问题的原因可能是缺乏当前支持的功能或者实现在这个版本或者后续版本中的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1592
这是一个需求类型的issue，主要涉及了在TensorRT-LLM中使用LoRA模型进行分类任务时遇到的问题。由于模型运行脚本`run.py`只支持解码而不支持分类，用户希望能够得到一个示例，展示如何使用LoRA和LLM进行分类任务。

https://github.com/NVIDIA/TensorRT-LLM/issues/1591
这是一个用户提出需求的问题，主要涉及TensorRT-LLM中的量化问题，用户希望建立一个“模型动物园”来存储量化模型，以解决硬件资源不足的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1590
这是一个关于bug报告的issue，主要涉及的对象是TensorRT-LLM。issue的原因可能是Top-P sampling偶尔会生成无效的标记，导致输出中包含超出模型词汇范围的标记。

https://github.com/NVIDIA/TensorRT-LLM/issues/1589
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中在转换qwen 110b gptq checkpoint时，qkv_bias的shape不能被3整除的问题。可能是由于模型参数shape不符合要求导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1588
这是一个bug报告，涉及主要对象是TensorRT-LLM中的推理功能。该问题由于在使用main branch进行推理时出现了错误导致推理失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1587
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的perf-best-practices.md文件。原因可能是缺少了某个链接导致了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1586
这是一个Bug报告，涉及到TensorRT-LLM中建立Yi-6B fp8模型时的运行时错误。这个问题的原因可能是由于某些层次的问题导致了错误的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1585
这是一个bug报告，主要涉及到Quantization过程中出现的错误。由于无法从元张量中复制数据，导致了NotImplementedError错误的发生。

https://github.com/NVIDIA/TensorRT-LLM/issues/1584
这是一个bug报告，涉及的主要对象是TensorRT-LLM，由于版本不匹配导致无法加载引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/1583
这是一个bug报告，涉及TensorRT-LLM中Ada traits for fpA_intB的问题，由于在`ENABLE_FP8`未定义时缺少`>::type`，导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1582
这是一个bug报告，该问题涉及的主要对象是prepare_dataset.py脚本。由于用户在准备数据集时遇到了无法运行自定义代码导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1581
这是一个用户提出问题的issue，主要涉及对象是"int4_gptq on Mixtral 8x7b"。由于`convert_checkpoint.py`需要一个`modelopt_quant_ckpt_path`参数，用户不清楚如何生成这个参数而引发了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1580
这是一个bug报告，主要涉及TensorRT-LLM下的int4_awq构建失败的问题。由于构建过程中出现了错误，导致了trtllmbuild执行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1579
这是一个用户提出需求类型的issue，主要涉及到nvidia-ammo在Windows上的安装问题。由于nvidia-ammo并没有针对Windows系统的版本，导致用户在quantize功能时遇到了安装nvidia-ammo的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1578
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的Gpt (Starcoder2 variant)模型。该问题可能是由于量化过程中指定的参数或代码逻辑错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1577
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的kv cache长度设置问题，用户询问如何设置初始kv cache长度。导致这个问题的原因是设置初始kv cache长度后无法正确迭代生成输出tokens。

https://github.com/NVIDIA/TensorRT-LLM/issues/1576
这是一个关于安装TensorRT-LLM后找不到TensorRT模块的bug报告，涉及对象是在ngc容器上使用TensorRT-LLM的用户。原因可能是安装过程中出现了问题导致模块无法找到。

https://github.com/NVIDIA/TensorRT-LLM/issues/1575
这是一个bug报告，主要涉及TensorRT-LLM下的medusa推理过程中发生错误，可能是由于使用profiling功能而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1574
这是一个bug报告，主要涉及TensorRT-LLM中的Executor API，在使用beam search时出现问题导致请求失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1573
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下enc-dec模型支持inflight batching的问题，疑问是是否可以使用动态batching等方法来提高性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1572
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM版本llava1.5的计算结果与HF输出不同。由于未能解决计算结果不正确的问题，用户提出了寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1571
这是一个bug报告，主要涉及的对象是在Windows上构建Qwen-7B时出现问题。 升级到trtllm-0.9.0版本后，用户能够成功构建Qwen Engine，但在进行推理时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1570
这个issue是关于性能优化的问题，主要涉及TensorRT-LLM下H200和H100在Mistral-7B上表现的比较差异，提出了为什么H200相较H100显示仅有少量改进的疑问。原因可能是H200在内存受限情况下并未实现预期的显著改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/1569
这是一个bug报告，主要涉及TensorRT-LLM中Executor API的一个问题，由于生成循环提前终止导致logits张量中丢失一个完整生成步骤。

https://github.com/NVIDIA/TensorRT-LLM/issues/1568
这是一个功能支持请求，涉及主要对象为TensorRT-LLM下的gemm操作，由于需支持2bit计算，需要依赖cutlass仓库中两个pull requests的合并。

https://github.com/NVIDIA/TensorRT-LLM/issues/1567
这个issue属于用户提出需求，并希望添加InternVL-Chat-V1.5支持，意图是认为该模型功能强大且超越了许多专有的多模态模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1566
这是一个关于系统错误的bug报告，涉及的主要对象是TensorRT-LLM中的WeightOnlyQuantMatmultensorrt_llm插件。出现这个问题的原因可能是由于插件未正确注册导致引擎加载失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1565
这是一个bug报告；主要涉及对象是Quantization功能在TensorRT-LLM下执行时生成第一个token的延迟问题；导致该问题的可能原因是int4/8转换为fp16的耗时，使得int8和int4的延迟约比fp16长25%。

https://github.com/NVIDIA/TensorRT-LLM/issues/1564
这个issue属于bug报告类型，主要涉及TensorRT-LLM的segmentation fault(core dumped)错误。由于缺少了配置文件中所需的关键参数，导致程序无法正常运行并出现崩溃提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/1563
这是一个Bug报告问题，主要涉及TensorRT-LLM无法满足特定版本要求的问题，可能是由于依赖版本不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1562
这是一个使用bug报告 issue，主要涉及到TensorRT-LLM的gptManagerBenchmark功能，由于某些原因导致GPU利用率为0%，CPU利用率持续100%，导致代码进入死循环。

https://github.com/NVIDIA/TensorRT-LLM/issues/1561
这是一个bug报告，涉及到TensorRT-LLM下的推理结果与HF模型结果不一致的问题。由于TensorRT-LLM中的ModelRunner.generate()方法与HF框架中的.chat()方法之间存在差异，导致推理结果异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1560
这个issue是关于bug报告，涉及主要对象为TensorRT-LLM的cpp benchmark，由于原因导致在运行cpp benchmark时无法使用`nsys profile`。

https://github.com/NVIDIA/TensorRT-LLM/issues/1559
这是一个bug报告，主要涉及TensorRT-LLM中的weight_sparsity功能。由于设置了weight_sparsity后，TensorRT日志未显示任何与稀疏性相关的信息，并且在测试经过稀疏剪枝的模型时未获得任何加速效果，导致用户无法实现预期的推理加速。

https://github.com/NVIDIA/TensorRT-LLM/issues/1558
这是一个bug报告，涉及代码中描述矩阵维度的错误，可能由于block_barrier函数在代码中引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1557
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM中的int8_kv_cache模块。由于计算`q_proj`的`act_range`在`int8_kv_cache`的scale中，导致了症状为困惑的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1556
这是一个bug报告，涉及TensorRT-LLM的CUDA运行时错误导致设备序号无效，用户需要帮助解决此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1554
这是一个用户提出需求的issue，主要涉及TensorRT-LLM模型更新和支持列表的更新，涉及的对象是TensorRT-LLM工具。

https://github.com/NVIDIA/TensorRT-LLM/issues/1553
这是一个bug报告，主要涉及TensorRT-LLM下的代码修改问题导致的`AttributeError: 'NoneType' object has no attribute 'name'`错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1552
这个issue类型是bug报告，主要涉及TensorRT-LLM的错误处理，由于LoRA任务未在缓存中找到导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1551
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的ModelRunner类。这个问题是由于使用ModelRunner变量而不是cls变量导致的，影响了当类被继承时的方法调用。

https://github.com/NVIDIA/TensorRT-LLM/issues/1550
这是一个bug报告，主要涉及的对象是A10模型转换。由于无法设置最大输入/输出标记，导致了无法将qwen1.5-7b模型转换的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1549
这个issue是关于ModuleNotFoundError导致的bug报告，主要涉及TensorRT-LLM模块缺失的问题，可能是由于模块路径配置不正确导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1548
这是一个Bug报告，主要涉及TensorRT-LLM下的multi block mode在特定条件下性能问题。由于multi block mode仅在特定输入输出长度下工作，并且存在一些限制条件，用户希望确认此行为是否符合TensorRT-LLM的预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/1547
这是一个bug报告，涉及的主要对象是TensorRT-LLM，由于缺乏有效的权重配置组合策略导致了GEMM错误，导致某些TensorRTLLM功能无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1546
这是一个用户提出需求的issue，主要涉及的对象是 TensorRT-LLM 中的模型量化过程。原因可能是在量化模型时出现了OOM错误，用户寻求帮助解决该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1545
这是一个bug报告类型的issue，主要涉及的对象是perf-best-practices.md文件中的一些死链接。存在这个问题的原因可能是为了保证文档内容的正确性和完整性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1544
这是一个bug报告，主要涉及的对象是TensorRT-LLM的模型转换过程。这个问题是关于在使用fp16精度转换模型时出现的警告累积在fp16还是fp32，可能是由于某些layernorm节点的问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1543
这个issue是一个bug报告，主要涉及TensorRT-LLM的内存使用过多导致OOM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1542
这是一个关于bug报告的issue，主要涉及到TensorRT-LLM中的Mixtral 8x7B smoothquant功能无法正常运行。造成这个bug的原因可能是在使用0.9.0版本时在运行convert_checkpoint.py脚本时出现了KeyError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1541
这是一个关于命名规范的问题，用户询问在TensorRT-LLM中使用cuBLAS的MMA CUDA Kernel的命名约定及含义。

https://github.com/NVIDIA/TensorRT-LLM/issues/1540
该问题是关于需求的提出，主要涉及的对象是TensorRT-LLM中的张量操作，用户出于性能优化考虑，希望在不进行数据拷贝的情况下获取子张量。

https://github.com/NVIDIA/TensorRT-LLM/issues/1539
这是一个bug报告，关于TensorRT-LLM中"use_fp8_context_fmha"功能产生错误输出的问题。由于构建参数错误导致了文字输出异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1538
这是一个Bug报告，涉及的主要对象是在TensorRT-LLM中运行T5转换脚本时出现Bus error。通过用户提供的信息来看，可能是由于T5架构的问题或者与使用的GPU类型（a10g）相关而导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1537
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的CUDA driver和pynvml版本。由于CUDA driver版本低于526时在使用pynvml版本11.5.0时出现导出失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1536
这个issue是关于功能请求，主要涉及TensorRT-LLM中的SamplingConfig类，用户试图实现MinP采样层，因该类的构造函数不是开源的且存在与.a文件中导致无法修改而导致问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1535
这个issue类型是用户提出需求。主要对象是正在开发的Medusa Safetensors Lm heads。由于AWQ Conversion存在错误，用户希望能够加载safetensors Medusa Lm heads，需要进行AWQ Conversion的修正。

https://github.com/NVIDIA/TensorRT-LLM/issues/1534
这是一个bug报告，主要涉及到TensorRT-LLM中的convert_hf_mpt_legacy函数，问题导致调用该函数在非全局范围时可能会失败，原因是函数当前引用了全局作用域中的hf_config。

https://github.com/NVIDIA/TensorRT-LLM/issues/1533
这是一个bug报告类型的issue，主要涉及到Mixtral转换过程中的OOM错误修复。问题是由于GPU内存不足导致OOM错误，用户寻求帮助修复模型转换失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1532
这个issue是一个bug报告，主要涉及TensorRT-LLM工具的运行问题。由于参数输入的错误格式导致了新的错误提示信息的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/1530
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM。原因是由于Windows环境下nvidiacudnncu12 CC的需求规范错误导致出现了ImportError: DLL load failed while importing tensorrt等bug症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1529
这是一个bug报告，主要涉及TensorRT-LLM下的zephyr-7b-beta fp16 engine在输入较长的情况下输出异常的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1528
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM中的模型转换过程。由于代码中出现了KeyError导致了报错信息中提到的缺失参数' model.layers.0.self_attn.q_proj.qweight'，可能是因为加载的模型权重文件缺失或者代码逻辑错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1527
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM中的转换脚本。导致此问题的原因可能是在加载checkpoint shards时出现了无法复制元数据张量的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1526
这是一个bug报告，主要涉及的对象是TensorRT-LLM中customAllReduceKernels.cu文件中的代码，由于维度错误导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1525
这是一个用户提出需求的issue，主要对象是高级API。由于用户想要了解何时高级API将支持QWEN模型，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1524
这是一个bug报告，该问题涉及TensorRT-LLM的转换脚本错误，导致在加载权重并转换为numpy时出现了不支持的ScalarType BFloat16类型的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1523
这是一个bug报告类型的issue，主要涉及使用TensorRT-LLM在CPU上构建的问题，用户询问能否在CPU上执行trtllm-build process。这个问题可能是由于缺少相关设定或支持导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1522
这是一个用户提出需求的issue，主要涉及改进构建Llama v3时的README内容，原因是需要增加词汇量大小。

https://github.com/NVIDIA/TensorRT-LLM/issues/1521
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM，由于存在顶部的拼写错误导致了bug的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/1520
这是一个关于TensorRT-LLM的bug报告，主要涉及到无法在序列长度超过200K时进行推理的问题，可能是由于模型无法支持这么长的序列长度而引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1519
这是一个bug报告，涉及TensorRT-LLM上运行MOE 8*7b模型在4*A100上出现Cuda OOM问题。由于相同的问题出现在先前提到的另一个issue上，用户寻求解决方案建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/1518
这是一个用户提出需求的类型。主要对象是TensorRT-LLM下的BERT模型。用户想要了解是否可以使BERT支持可变长度的输入序列。

https://github.com/NVIDIA/TensorRT-LLM/issues/1517
这个issue属于用户提出需求类型，围绕着BERT-like模型是否可以应用类似GPT-like模型的特性展开讨论。由于用户对于BERT模型可以借鉴GPT模型的哪些特性存在疑问导致提出这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1516
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM中的Jetson Orin AGX，由于缺少针对sm_87架构的优化编译内核，导致提取头尺寸为128时无法使用优化的融合MHA内核。

https://github.com/NVIDIA/TensorRT-LLM/issues/1515
这是一个bug报告，涉及TensorRT-LLM下的Unsupported architecture问题，由于架构不支持导致断言错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1514
这是一个需求提出的issue，主要涉及SDXL框架，用户提出希望支持SDXL和其分布式推理，原因是为了实现更好的性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1513
这是一个bug报告，主要对象是Llama 3 70b的TensorRT-LLM引擎构建过程。导致报错的原因可能是缺少"Architecture"参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/1512
这是一个bug报告类型的issue，主要涉及对象是TensorRT-LLM中的模型转换过程。由于config.json文件可能存在问题，导致无法成功转换模型的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1511
这个issue是关于bug报告的，主要对象是在TensorRT-LLM下使用T5模型时出现GPU利用率高导致编码器输出为NaN值的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1510
这是一个bug报告，主要涉及TensorRT-LLM中Llama 2执行时出现的bug。由于在构建具有高tp值和int4量化的引擎时出现了问题，导致该bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1509
这个issue属于bug报告类型，涉及TensorRT-LLM的通过put Benchmark的复现问题，可能由于系统或环境配置问题导致Segmentation fault错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1508
这个issue是关于bug报告，涉及到TensorRT-LLM中的multimodal模块。由于tokenizer在生成input_ids时添加了特殊的起始标识符<s>，需要将这个token从post_input_ids中移除，以避免最终的prompt中出现意外的token。

https://github.com/NVIDIA/TensorRT-LLM/issues/1507
这是一个bug报告，涉及对象为在使用Phi-2 with A100 (160GB)和Triton server 24.02时出现无法启动的问题。原因可能是Triton server在此环境下无法正常启动导致hang indefinitely。

https://github.com/NVIDIA/TensorRT-LLM/issues/1506
这是一个用户需求问题，主要涉及TensorRT-LLM中如何在C++后端控制生成过程中分页kv缓存中的最大令牌数。用户想要指定`max_tokens_in_paged_kv_cache`属性，但不清楚如何实现。

https://github.com/NVIDIA/TensorRT-LLM/issues/1504
这个issue是一个bug报告，主要涉及TensorRT-LLM下的flopy数值比标准规格更高的问题，用户询问了关于FLOPS计算的疑惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1503
该issue属于用户询问支持是否支持Mistral-7B v0.2，涉及主要对象为TensorRT-LLM。原因可能是用户想要在系统中使用A100 GPU（40GB）的TensorRT-LLM模型，但存在一些不确定性导致需要确认支持性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1502
这是一个bug报告，主要涉及TensorRT-LLM中的参数--load_model_on_cpu被忽略的问题，导致脚本使用了GPU而引发OOM问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1501
这是一个关于性能问题的bug报告，主要涉及TensorRT-LLM中Llama-2-7b在fp8下性能低于预期的情况。由于使用w8a8引擎后，预期提升性能却没有明显改善，希望了解如何调整参数以实现预期的性能提升。

https://github.com/NVIDIA/TensorRT-LLM/issues/1500
这是一个关于参数设置问题的bug报告，涉及主要对象为TensorRT-LLM，在建模时设置的max_input_len和max_output_len与benchmark时的input_output_len关系引发了bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1499
这是一个bug报告类型的issue，涉及主要对象是TensorRT-LLM中的GPU设备，并由于peer access不支持在两个设备之间进行访问，导致了无法使用两个A30 GPU进行推理的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1498
这是一个bug报告，涉及主要对象为TensorRT-LLM，出现了“peer access is not supported between these two devices”的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1497
这是一个用户提出需求的issue，主要对象是CogVLM模型支持计划。用户询问是否有对该模型的支持计划。

https://github.com/NVIDIA/TensorRT-LLM/issues/1496
这是一个bug报告，主要涉及TensorRT-LLM中的W4A8 AWQ问题，由于W4A8_awq内核在先Hopper体系结构上不受支持，导致了无法在Ada Lovelace GPUs上成功运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1495
这是一个bug报告，主要涉及GPU在MPI InterNode处理中的错误分配问题，可能导致CUDA设备错误和单GPU节点的互相关联问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1494
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM在MPI跨节点处理中单GPU节点上的GPU分配错误问题。由于假设每个节点有8个GPU而实际只有1个GPU导致了GPU索引分配错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1492
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM的更新和改进，其中提到了修复了一些bug以及新增了一些功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1491
该问题类型为用户寻求帮助，主要涉及对象是Mistral在不同GPU（A100和A5500）上的运行，并困扰用户如何设置多GPU的参数以获得所需的配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1490
这是一个bug报告，涉及TensorRT-LLM中的构建问题。由于在0.9.0分支上构建时出现了错误，导致了构建失败的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1489
这个issue是关于bug报告，主要涉及TensorRT-LLM的运行时参数配置。由于配置的组合及单独开启不同功能产生意外效果，比如性能提升或执行错误，表明可能存在潜在的程序逻辑问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1488
这是一个用户提出需求的问题，主要涉及在使用inflight batching时如何将hidden_states直接传递给LLM。由于GenerationRequest类中缺少"prompt_table"属性，导致用户无法直接传递图像特征的hidden_states给LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/1487
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在运行时出现的性能问题。造成问题的原因可能是安装或配置过程中的某些错误或不当设置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1486
该issue类型为bug报告，涉及的主要对象为`ModelRunner`和`batch_size`。这个bug的原因是指针偏移没有考虑张量条目的大小，导致了`stop_words_list`功能在`ModelRunner`中与`batch_size` > 1时无法正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1485
这是一个bug报告，涉及TensorRT-LLM的`float32`精度引擎构建时遇到`TypeError`错误。可能是由于代码实现还未针对`float32`精度进行完善导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1484
这是一个用户需求相关的问题，主要涉及对象为TensorRT-LLM中的functional.Tensor，由于版本升级导致获取的数据类型变化，需要转换Tensor为List或numpy以满足原有接口。

https://github.com/NVIDIA/TensorRT-LLM/issues/1483
这是一个关于模型输出不稳定的bug报告，涉及对象为QwenVLChat模型。由于同一输入图像产生不同的输出嵌入向量，导致结果略差于原始模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1482
这是一个关于用户提出问题的issue，主要涉及对象是`gptManagerBenchmark`方法。用户想了解在`gptManagerBenchmark`中采样策略是如何设置的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1481
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中的`ModelRunnerCpp`和`gather_all_token_logits`的高内存消耗问题。由于升级到TensorRTLLM版本0.10.0后，与`gather_all_token_logits`结合使用时，`ModelRunnerCpp`消耗大量内存，而在0.7.1版本中没有这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1480
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的forward函数。这个问题的原因可能是slice操作超出缓冲区大小，导致出现错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1479
这是一个bug报告，主要涉及的对象是TensorRT-LLM工具。由于未正确处理输入数据类型异常，导致报错TypeError: a bytes-like object is required, not 'NoneType'。

https://github.com/NVIDIA/TensorRT-LLM/issues/1478
这是一个bug报告，主要涉及到使用70B sq_int模型构建引擎时出现错误。由于某种原因导致了引擎构建失败，用户正在寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1477
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的"hf_lora_convert.py"文件，用户怀疑"lora_alpha"值没有在将权重转换为tensorrt_llm输入时被使用，导致模型性能下降。

https://github.com/NVIDIA/TensorRT-LLM/issues/1476
这是一个bug报告类型的issue，主要涉及到AWQ Int4量化在使用--pp_size=2时导出失败的问题，可能是由于在导出时缺少相应的配置参数和API支持所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1475
这是一个Bug报告，涉及的主要对象是在导出到TensorRTLLM格式时出现了错误，原因是模型配置文件为空导致了AttributeError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1474
这是一个bug报告。该问题涉及TensorRT-LLM下的streamingllm模块。由于设置的max_seq_len超过了model_config.max_seq_len导致报告了长度相关的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1473
这个issue属于用户提出需求类型，主要涉及的对象是对TensorRT-LLM的Yi-VL模型支持。这个需求是因为Yi Vision Language (YiVL)模型的优秀性能以及在最新基准测试中排名第一，用户希望能够在TensorRT-LLM中集成Yi-VL模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1472
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM库在将Mixtral-8x7B转换为tensorrt格式时出现了错误。由于代码中出现了一些路径错误，导致了Quantization相关的函数调用时出现了异常，进而导致了这个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1471
这个issue属于用户提出需求类，主要涉及TensorRT-LLM中的auto_parallel功能，用户询问该功能的工作原理以及适用范围。

https://github.com/NVIDIA/TensorRT-LLM/issues/1470
这是一个特性请求，主要对象是支持 llama v3，用户请求nvidia公司提供帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1468
这是一个用户提出问题的类型，主要涉及对象是在使用TensorRT-LLM进行量化方法时是否需要替换数据集。 用户可能对是否需要使用自己的数据集来配置模型进行量化方法产生疑惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1467
这是一个bug报告，主要涉及TensorRT-LLM中获取nvlink_bandwidth时出现KeyError: 6的问题，可能是代码中无法找到NVLink版本6的带宽信息导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1465
该issue是一个用户提问类型的，并且涉及TensorRT-LLM中的<image> token添加问题。用户询问在使用transformers中的llava-hf时，是否需要添加<image> token到prompt。

https://github.com/NVIDIA/TensorRT-LLM/issues/1464
该issue为文档更新请求，涉及的主要对象是GitHub Pages，由于页面内容需要更新或修复，用户提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1463
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的FP8后训练量化。由于build选项中缺少 `use_fp8_context_fmha enable` 选项，导致用户无法按照文档中描述的方式启用fp8 context fmha加速。

https://github.com/NVIDIA/TensorRT-LLM/issues/1462
这个issue是关于bug报告，主要涉及TensorRT-LLM中的性能文档修复。由于性能文档的问题，用户提出了修复请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1461
这是一个文档更新类型的issue，涉及的主要对象是发布0.9版本的文档。由于发布版本的内容需要更新，用户提出了更新文档的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1460
这个issue是一个bug报告，主要对象是TensorRT-LLM下的 `convert_checkpoint.py` 脚本。由于新增的preload逻辑导致CPU标志丢失，导致在量化模型时无法加载只适合在VRAM中的模型，出现了加载问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1459
这个issue是关于bug报告，主要涉及TensorRT-LLM在构建T5模型时使用`--debug_mode`标志导致运行失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1458
这是一个bug报告，主要涉及TensorRT-LLM下的mpirun命令在8x4090设备上内存占用过高导致out of memory的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1457
该issue类型属于用户提出需求类型，主要涉及TensorRT-LLM下的embedding_bias特性。用户询问如何使用embedding_bias以及它的作用，特别是在解码过程中添加偏置的用例。

https://github.com/NVIDIA/TensorRT-LLM/issues/1455
这是一个关于TensorRT-LLM更新的issue，主要涉及Bug修复，其中涉及到Pipeline Parallelism和`gather_all_token_logits`功能引起的Segmentation fault问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1454
这是一个用户提出需求的issue，用户询问何时支持Qwen1.5版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/1453
这是一个bug报告，涉及的主要对象是关于TensorRT-LLM项目下LoRA模型转换到cpp格式时遇到的问题，由于LoRA权重文件夹中缺少`model_weights.ckpt`和`model_config.yaml`导致报错并无法运行脚本。

https://github.com/NVIDIA/TensorRT-LLM/issues/1452
这个issue是用户询问关于TRT-LLM 0.9.0是否支持A6000和相关性能评测的问题，属于用户提出需求及请教问题类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1450
这是一个用户提出问题的issue，主要涉及TensorRT-LLM中的指标解释，包括tokens_per_sec和generation_tokens_per_second，用户询问这两个指标是表示总tok/s还是tok/s/GPU。

https://github.com/NVIDIA/TensorRT-LLM/issues/1449
这是一个bug报告，主要对象涉及Medusa IFB在Triton Inference Server上运行失败，可能由于Medusa模型参数设置等原因导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1447
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM系统。由于某些原因导致了配置数值不一致，进而引发了samplingConfig.h:46处的崩溃和批处理大小限制为2的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1446
这是一个关于TensorRT安装问题的bug报告，主要涉及对象为TensorRT库。由于缺少指定的模块，导致了无法导入tensorrt的DLL加载失败的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1445
这个issue属于需求提出类型，主要涉及TensorRT-LLM的更新和新增功能，其中包括模型支持、功能新增、特性改进等内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/1444
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的运行脚本在多GPU下无法正常工作。ã

https://github.com/NVIDIA/TensorRT-LLM/issues/1443
这是一个用户询问如何在Transformer层中添加额外操作的问题，涉及TensorRT-LLM下的Cohere模型中的qknorm操作。由于TensorRT-LLM目前仅支持输入和后置归一化，用户不清楚如何将qknorm编译到引擎中，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1442
这是一个关于安装TensorRT-LLM的bug报告，主要涉及安装过程中出现找不到相应wheel文件的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1441
这个issue是一个bug报告，涉及的主要对象是在构建TensorRT-LLM时遇到了问题，可能是由于TensorRT与其他依赖或环境不兼容导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1440
这是一个bug报告，主要涉及TensorRT-LLM下的Mixtral转换权重时出现的错误。造成这个问题的原因是在转换权重过程中出现了NotImplementedError无法复制元数据张量导致无法读取数据。

https://github.com/NVIDIA/TensorRT-LLM/issues/1439
这是一个bug报告，涉及主要对象是TensorRT 9版本。由于无法从官方网站下载TensorRT 9版本，导致在运行convert_checkpoint.py时出现了"ImportError: libnvinfer.so.9: cannot open shared object file"的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1438
这是一个用户提出需求的问题，主要涉及如何控制最大并发性。用户希望了解如何限制在构建最大批处理大小为24的引擎时，最多同时运行16个请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1437
这是一个用户询问如何使用自己的提示生成数据集的问题，主要涉及的对象是生成tokens的脚本。由于用户无法找到在脚本中生成tokens的位置，因此提出了如何使用包含大量句子的文件生成tokens的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1435
这个issue是关于功能需求的，主要对象是TensorRT-LLM下的model_runner_cpp.py文件，用户建议将gather generation logits和gather context logits进行分开设置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1434
这个issue是关于bug报告，主要涉及到TensorRT-LLM中的编译错误，原因是缺少头文件导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1433
这是一个bug报告，主要涉及TensorRT-LLM下无法保存engine的问题。由于某种原因导致triton-build命令运行失败，未能成功构建引擎目录。

https://github.com/NVIDIA/TensorRT-LLM/issues/1432
这个issue类型是用户提出需求，该问题单涉及的主要对象是添加LoRa支持到Qwen或Qwen2模块。由于目前缺乏LoRa支持，用户提出了添加LoRa支持的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1430
这是关于用户提出需求的问题，主要涉及TensorRT-LLM中的multimoda模块支持GroundingDINO的相关内容。由于用户想了解TensorRT-LLM multimoda是否支持GroundingDINO以及如何支持，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1429
这是一个bug报告，涉及的主要对象是Medusa模型。由于批处理大小为2时引发了CUDA运行时错误，可能由于索引错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1428
这是一个BUG报告，主要涉及的对象是使用FP8精度网络需要具备FP8支持的硬件。由于硬件不支持FP8，导致出现无法构建引擎的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1427
这是一个功能更新的issue，主要涉及TensorRT-LLM模型的更新和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/1426
这是一个用户提出需求的问题，主要涉及到如何在版本0.8.0中使用System Prompt Caching来加速处理超过3000+ tokens的系统提示时的输出延迟问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1425
这是一个关于用户提出需求的问题，在GptManager中修改topK/topP值的可行性。这个问题主要涉及到在生成过程中动态更改topK/topP值的操作是否安全，用户想要根据特定条件调整这些数值，且疑问修改后会否被正确应用。

https://github.com/NVIDIA/TensorRT-LLM/issues/1424
这是一个bug报告，涉及TensorRT-LLM中窗口注意力机制的错误。由于使用大窗口大小和高内存使用，导致共享内存错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1423
这是一个Bug报告，涉及TensorRT-LLM的安装问题。由于缺少正确的依赖安装命令，导致无法安装tensorrt_llm库。

https://github.com/NVIDIA/TensorRT-LLM/issues/1422
这是一个bug报告，主要涉及TensorRT-LLM的`use_cache=False`设置问题，导致代码报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1421
该issue类型为用户提出需求，主要涉及TensorRT-LLM下benchmark相关的问题，用户想要了解如何进行基准测试以及如何使用lora进行性能基准测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/1420
这是一个bug报告，主要涉及TensorRT-LLM无法成功编译的问题。可能原因是Docker镜像构建或容器内部编译过程存在错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1419
这是一个功能请求，请求提供在TensorRT-LLM中收集稀疏logprobs的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1418
这是一个bug报告，涉及TensorRT-LLM中遇到的CUDA错误，由于在增加input_ids长度时出现异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1417
这是一个关于需求和帮助的问题，涉及TensorRT-LLM引擎的构建参数选择。用户对构建llama引擎的最佳配置和选项含义不清楚，导致不确定如何选择最佳参数配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1416
该issue类型为Bug报告，涉及TensorRT-LLM下的Weight-only quantization功能无法正常工作的问题，可能由于环境适配、功能测试需求或其他原因导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1415
这是一个bug报告，涉及LogitsPostProcessor BatchManager在tp值大于1时出现的问题，导致GptManager在第一次迭代时无法正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1414
这是一个bug报告，涉及TensorRT-LLM下的Streaming功能，在使用中出现了与空格相关的问题，导致输出不符合预期的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1413
这是一个bug报告，涉及到Official Triton Inference Server Image with TRT-LLM support缺少TRT-LLM的问题。用户可能因为导入module失败而遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1412
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM。导致该问题的原因可能是构建过程中出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1411
这是一个关于bug报告的issue，涉及的主要对象是TensorRT-LLM中的hf_lora_convert.py脚本。由于ChatGLM层权重的键名不被支持导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1410
这个issue属于用户提交问题类型，主要涉及TensorRT-LLM下benchmark输出中`tokens_per_sec`和`generation_tokens_per_second`含义的问题。用户提问的原因可能是缺乏相关文档解释导致的困惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1409
这个issue是一个bug报告，涉及主要对象是TensorRT-LLM下的cuda11.8兼容性问题，由于用户的设备无法安装cuda12.1，导致出现此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1408
这是一个bug报告，针对TensorRT-LLM的Import error。用户在导入`tensorrt_llm`时遇到了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1407
这是一个bug报告，主要涉及LLava在使用SamplingConfig时无法产生多样化输出句子的问题。原因可能是参数设置或代码实现上的问题导致生成的结果始终相同。

https://github.com/NVIDIA/TensorRT-LLM/issues/1406
这是一个类型为bug报告的issue，主要涉及的对象是summarize.py文件。由于代码问题导致"tokens per second"计算始终为0。

https://github.com/NVIDIA/TensorRT-LLM/issues/1405
这是一个功能需求的issue，主要涉及TensorRT-LLM中高级API中的SamplingConfig。用户希望通过高级API能够访问完整的SamplingParams集合，以便更容易迁移自vLLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/1404
此issue属于bug报告类型，关于TensorRT-LLM下的Mistral 7B v0.1模型推断延迟在使用滑动窗口注意力尺寸后增加的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1403
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的config创建，导致症状是缺失rotary base。

https://github.com/NVIDIA/TensorRT-LLM/issues/1402
这是一个bug报告类型的issue，涉及主要对象为在github上的TensorRT-LLM。导致此问题的原因可能是某些步骤缺失导致无法找到`tensorrt_llm`模块。

https://github.com/NVIDIA/TensorRT-LLM/issues/1401
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中如何同时整合自定义的Mistral LLM和ViT图像编码器模型，并希望得到如何结合和运行这两个组件的指导。

https://github.com/NVIDIA/TensorRT-LLM/issues/1400
这是一个bug报告，涉及的主要对象是如何控制TensorRT v0.8中KV cache占用的空间。由于TensorRT v0.8未按预期方式分配GPU全局存储空间，导致KV缓存占据大量存储空间。

https://github.com/NVIDIA/TensorRT-LLM/issues/1399
这是一个关于需求问题的issue，主要涉及对象是GptManager InferenceRequest中的Tensor数据类型，用户提出关于这些张量需要的内存类型的问题，可能是由于缺乏关于内存需求的信息而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1398
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的paged_kv_cache功能，用户在启用该功能时发现GPU内存使用量升高的问题，希望了解为何会出现这种情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1397
这是一个关于性能测试的issue，主要对象是int8_kv_cache，由于内存限制导致在A80040G设备上使用int8_kv_cache时出现oom问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1396
这个issue类型是bug报告，主要涉及的对象是C++ runtime中的gptSessionTest。导致该问题出现的原因是配置文件中缺少了关键参数，导致引擎无法正确构建。

https://github.com/NVIDIA/TensorRT-LLM/issues/1395
这是一个bug报告类型的issue，主要涉及TensorRTLLM中的模型量化问题，可能由于参数值的shape不匹配导致了该bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1394
这个issue是关于一个bug报告，主要涉及的对象是TensorRT-LLM中的lm_head量化问题。由于lm_head量化为fakequantized而非realquantized，导致模型压缩问题及输出错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1393
这是一个关于Smoothquant数据导出问题的bug报告。用户主要关注的对象是TensorRT-LLM中的attention.qkv数据，由于后续的rope嵌入过程导致无法正确导出中间值，希望找出导致数据异常的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/1392
这是一个功能需求请求，主要涉及TensorRT-LLM下的internlm2模型的转换支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1391
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的batch_manager和executor，用户寻求是否有开源计划。

https://github.com/NVIDIA/TensorRT-LLM/issues/1390
这是一个关于TensorRT-LLM中内存使用的问题，用户询问关于数据类型以及模型中 embedding 计算所使用的数据类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1389
这是一个用户提出需求的类型问题。该问题主要涉及TensorRT-LLM对异步allreduce的支持。用户想要实现异步操作并提升速度，可能需要涉及到使用cudagraphs或jit编译。

https://github.com/NVIDIA/TensorRT-LLM/issues/1387
这个issue类型是需求更新，涉及的主要对象是TensorRT-LLM。由于性能问题，用户提出了减少TensorRT引擎之间`enqueue`调用之间的开销以及一些小的更新和修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1386
这是一个bug报告，主要涉及TensorRT-LLM中的参数length_penalty出现问题。由于参数设置不当导致模型推断无法正确停止的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1385
这是一个bug报告，涉及主要对象为llama中的convert_checkpoint.py。由于缺少rotary_scaling参数，在某些情况下会导致推断过程中出现大量重复的token。

https://github.com/NVIDIA/TensorRT-LLM/issues/1384
这是一个bug报告，主要涉及的对象是AttentionMaskType.bidirectional参数。由于将参数chatglm2更改为AttentionMaskType.bidirectional后出现了功能无法正常工作的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/1383
这是一个类型为bug报告的issue，涉及的主要对象是TensorRT-LLM中的quantized model of mistral 7b使用ammo无法创建engine的问题。由于config.json文件中存在错误的格式导致了该问题的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1382
这是一个bug报告，涉及TensorRT-LLM在执行LLaVA 13B batch_size=2推理时报错的问题。原因是编译batch_size=4的engine后导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1381
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM中的LLaVA模块。由于将pad_token设置为与eos_token相同，导致了该bug症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1380
这是一个关于性能问题的bug报告，涉及TensorRT-LLM中使用fp8模型时随着批处理大小增加性能显著下降的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1379
这是一个bug报告，涉及TensorRT-LLM中的"multi_block_mode enable runtime crash"问题，由于在运行时启用多块模式导致引擎崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/1378
这是一个关于性能问题的bug报告，主要涉及TensorRT-LLM的int8模型在权重量化方面性能低于fp16模型的问题，可能与启用chunked context有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/1377
这是一个bug报告，主要涉及TensorRT-LLM中的Chunked context incomplete outputs问题。造成这个问题的原因可能是运行引擎时高负载导致输出不完整。

https://github.com/NVIDIA/TensorRT-LLM/issues/1376
这个issue是用户提出需求，针对如何向TensorRT-LLM添加自定义简单网络并生成引擎进行推断问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1375
这个issue是关于bug报告，主要对象是TensorRT-LLM中的ModelRunner。用户遇到的问题是在使用Python API时无法指定使用的GPU，导致所有推理实例都默认使用GPU 0。

https://github.com/NVIDIA/TensorRT-LLM/issues/1374
这是一个bug报告类型的issue，主要涉及到在使用TensorRT-LLM时在不同的GPU配置下出现了运行时错误。这个问题的原因可能是与0.8.0版本相关的某些功能变更或者配置设置的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1373
这是一个bug报告，主要涉及TensorRT-LLM下的mistral权重转换和构建指令错误，用户反馈的问题是由于指令错误导致无法正确转换和构建mistral权重。

https://github.com/NVIDIA/TensorRT-LLM/issues/1372
这是一个用户提出需求的 issue，主要涉及的对象是TensorRT-LLM，用户询问是否支持特定的模型，并询问是否有计划支持这个模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1371
这个issue属于用户提出需求类型，主要关注对象是generation.py脚本。由于缺乏时间戳打印功能，用户希望在generation.py中添加该功能。



https://github.com/NVIDIA/TensorRT-LLM/issues/1370
该issue属于用户提出需求类型，主要涉及TensorRT-LLM中支持HF的`early_stopping`启发式方法的问题，用户怀疑不同的early stopping行为可能导致HF和TRTLLM输出结果的巨大差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/1369
这是一个关于需求讨论的issue，主要涉及AttentionMaskType中两个选项的区别，用户想了解这两者之间的差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/1368
这是一个用户需求类型的issue，主要涉及NVIDIA AMMO工具箱的文档相关问题，用户希望了解文档中关于实现功能、支持的模型量化技术以及版本之间的更新内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/1367
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM下的moe.py文件中的代码。造成这个问题的原因可能是在tp_mode不等于MoeConfig.ParallelismMode.TENSOR_PARALLEL时错误地添加了一个allreduce操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1366
这个issue是一个bug报告，主要涉及TensorRT-LLM中SmoothQuant quantization for T5，在实现过程中遇到了一些问题，导致输出结果不符合预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/1365
这是一个Bug报告，用户遇到了安装TensorRT时出现的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1364
This issue is a modification related to the cutlass library in the TensorRT-LLM repository. It does not specify the type of issue or the reason behind the modification.

https://github.com/NVIDIA/TensorRT-LLM/issues/1363
这是一个用户提出需求的issue，主要涉及支持`DBRX`模型的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1362
这是一个关于安装问题的bug报告，涉及对象是TensorRT-LLM。由于构建wheel时出现了错误，导致安装未能成功完成。

https://github.com/NVIDIA/TensorRT-LLM/issues/1361
这是一个关于性能问题的bug报告，主要涉及LLaVA模型在int4量化中性能表现不佳。由于未达到预期的4倍加速，用户提出了相关问题并寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1360
这个issue类型是用户提出需求，主要涉及TensorRT-LLM下对Cohere Command-R模型的支持。由于Cohere发布了CommandR模型并取得良好的评估结果，用户希望TensorRT-LLM能够支持该模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1358
这是一个bug报告，主要涉及TensorRT-LLM中的ModelRunnerCpp的问题。这个bug导致ModelRunnerCpp没有正确传输SamplingConfig张量字段。

https://github.com/NVIDIA/TensorRT-LLM/issues/1357
这是用户提出需求，寻找某个功能源码的问题。该问题涉及的主要对象是`KVCacheManager`。用户无法找到 `KVCacheManager` 的cpp实现代码，只找到了头文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/1356
这是一个bug报告，主要涉及TensorRT-LLM中的mistral run AWQ功能。导致问题的原因可能是系统卡住，无法使用CPU和GPU。

https://github.com/NVIDIA/TensorRT-LLM/issues/1355
这是一个bug报告，涉及TensorRT-LLM中长输入导致输出异常的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1354
这是一个bug报告，主要涉及到TensorRT-LLM下的模型starcoder2在输出上与原始模型不一致的问题。这个问题可能是由于模型输出不符合预期行为以及模型运行速度慢的情况所导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1353
这是一个bug报告，涉及了在TensorRT-LLM下无法将finetuned llama-7b转换为TRT检查点的问题。由于配置文件不匹配导致该错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1352
这是一个关于模型推理结果差异的bug报告，主要涉及TensorRT-LLM下的beam search功能在不同版本之间出现结果差异的问题。由于某些优化选项的启用或禁用导致了推理结果的明显下降，用户希望得到关于如何提高准确性的建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/1351
这是一个用户提出需求的类型的issue，主要涉及到TensorRT-LLM中的量化功能。用户提出需求是希望增加一个选项用于指定量化校准数据集的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1350
这是一个关于功能需求的问题，主要涉及的对象是TensorRT-LLM中对BERT模型是否支持int8量化。用户在询问是否支持int8量化（权重量化或平滑量化），表达了对该功能的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1349
这是一个用户提出需求的issue，主要对象是对TensorRT-LLM的Lookahead Decoding支持。由于用户希望添加这一特性，并提供了相关文档链接。

https://github.com/NVIDIA/TensorRT-LLM/issues/1348
这个issue是用户提出的问题，主要涉及的对象是TensorRT-LLM中的 gptManagerBenchmark 和 gptSessionBenchmark。用户想要了解这两个 benchmark 之间的区别，并咨询在选择测试 Qwen7B 时应该使用哪一个，以及它们之间是否存在性能差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/1347
这是一个用户提出需求的问题，主要涉及对象是TensorRT-LLM中支持Qwen2模型的问题。由于当前版本可能尚未支持Qwen2模型，用户请求添加该模型的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1346
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM项目。由于严格固定Python依赖版本导致集成TensorRTLLM变得困难。

https://github.com/NVIDIA/TensorRT-LLM/issues/1345
这个issue类型是bug报告，主要涉及到ammo_cuda_ext和ammo_cuda_ext_fp8的构建失败，可能由于软件版本冲突导致了此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1344
这是一个bug报告，主要涉及的对象是Qwen-72B-chat-GPTQ TP=4模型，可能由于系统信息缺失导致了未能获得预期的行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/1343
这是一个关于TensorRT-LLM中Flan t5 xxl结果存在较大差异的bug报告，主要涉及GPU、docker容器、脚本运行以及文本输出的比较。原因可能是模型运行中的错误导致结果差异，并寻求帮助解决这一问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1342
这是一个bug报告，主要涉及TensorRT-LLM在Windows 11上通过pip安装时出现错误的情况。导致这一问题的可能原因是安装过程中的setup.py执行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1341
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的Sample Config设置不生效的问题，可能由于配置参数中的温度设置未能正确影响输出结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1340
这个issue是关于bug报告，涉及的主要对象是TensorRT-LLM中的运行脚本。由于参数配置错误导致了Assertion failed错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1339
这是一个bug报告，主要涉及TensorRT-LLM中Mixtral MoE引擎在8k上下文长度时出现错误的问题，可能是由于输入参数不满足特定条件导致引擎异常终止。

https://github.com/NVIDIA/TensorRT-LLM/issues/1338
这是一个bug报告，主要涉及TensorRT-LLM中Smoothquant在LLaMA-13B模型上创建checkpoint时出现的错误。造成此错误的原因是无法将大小为15360的数组调整为形状为(1,3840)。

https://github.com/NVIDIA/TensorRT-LLM/issues/1337
这是一个功能需求类型的issue，主要涉及的对象是对新发布的distilwhisper/distillargev3的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1336
这是一个bug报告，主要涉及TensorRT-LLM中相同输入下出现的非确定性问题。由于不同批次大小或使用不同计算核心导致输出不一致，用户希望了解如何确保结果的确定性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1335
这个issue是关于bug报告，主要涉及TensorRT-LLM中FP8量化失败的问题，可能由环境配置或代码实现导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1334
这是一个关于TensorRT-LLM中的prequant_scaling_factor问题的疑问，用户请求关于prequant_scaling_factor的解释。

https://github.com/NVIDIA/TensorRT-LLM/issues/1333
该issue是一个bug报告，主要涉及TensorRT-LLM中Mistral-7B smoothquant失败的问题。用户期望成功转换模型，但实际行为是输出乱码。原因可能是与TensorRTLLM版本或平台兼容性相关。

https://github.com/NVIDIA/TensorRT-LLM/issues/1332
这是一个bug报告，主要涉及TensorRT-LLM引擎的优化配置问题，由于测试数据维度与优化配置不匹配导致了Runtime dimension不满足任何优化配置的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1331
这是一个关于TensorRT-LLM下的bug报告，用户在尝试进行FP8量化时遇到CUDA的OOM错误，无法成功量化Llama 70B模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1330
该issue类型为用户提出需求，并涉及到使用SmoothQuant和int8 kv cache。这个问题涉及到如何同时使用这两个功能以及在kv缓存存储精度方面的疑惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1329
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的executor.py文件。由于类型错误导致runtime_top_k出现异常，在测试flaskapi_server.py时遇到问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1328
这个issue类型是bug报告，主要对象是TensorRT-LLM项目中的examples/whisper模块，由于其中存在一个拼写错误导致了需要修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1327
这是一个用户提出需求的issue，主要涉及自动下载模型转换至TensorRT-LLM模型并保存的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1326
这是一个bug报告，涉及主要对象是torch.ops.trtllm.symmetric_quantize_last_axis_of_batched_matrix。由于weight和scale在使用torch.ops.trtllm.symmetric_quantize_last_axis_of_batched_matrix后出现较大误差，导致了错误的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1325
这是一个关于bug报告的issue，涉及CentOS 7的构建问题。这个issue可能是由CentOS 7上的编译错误引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1324
这个issue属于bug报告类型，主要涉及TensorRT-LLM中"Failed to deserialize cuda engine"错误，可能由于模型量化过程中的错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1323
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的Python 3.11和release 0.8.0，由于Python 3.11中dataclasses标准库的更改导致数值类型默认值的可变性问题，进而触发了数值类型不被允许作为默认值的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1322
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的模型转换和构建引擎过程。由于输出差异过大，可能是由参数配置不当或代码实现问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1321
这是一个bug报告，涉及的主要对象是GenerationExecutor。由于某些原因，导致GenerationExecutor输出的结果没有意义。

https://github.com/NVIDIA/TensorRT-LLM/issues/1320
这是一个关于性能问题的bug报告，主要涉及TensorRT-LLM中使用fp16时gemm运算速度异常快的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1319
这是一个关于功能需求的issue，主要涉及TensorRT-LLM中使用FP8权重和FP16激活量化的方法，用户希望权重可以使用FP8格式而保留激活值为FP16格式，但目前只能将权重和激活值一同量化至FP8。

https://github.com/NVIDIA/TensorRT-LLM/issues/1318
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的Quantization功能。由于在使用 ammo.torch.quantization 过程中出现了 TypeError，可能是由于传入的参数组合错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1317
该issue类型为用户提出需求，主要涉及W8A8模型在TensorRT-LLM下是否支持混合精度推理，并由于精度和差异率问题导致单独使用FP16和INT8量化存在困难。

https://github.com/NVIDIA/TensorRT-LLM/issues/1315
该issue类型为功能更新，主要涉及TensorRT-LLM的更新内容及相关API。由于代码结构重构和参数变更，可能导致了BREAKING CHANGES，并且用户可能面临与之相关的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1314
这是一个bug报告，主要涉及到TensorRT-LLM下版本0.8.0出现的编译错误，可能是由于特定的提交ID和cutlass的版本导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1313
这是一个bug报告，涉及的主要对象是Mixtral engine bfloat16构建失败。由于预期的数据类型是BFloat16而实际上是Float，导致了这个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1312
这个issue是一个关于功能请求的问题，主要涉及TensorRT-LLM中支持INT4权重和FP8激活的需求。由于当前无法实现该功能，用户提出了这一需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1311
这是一个关于bug报告类型的issue，主要涉及的对象是TensorRT-LLM下的代码文档。问题可能是由缺少设置`promptTuningEnabled`参数导致的错误行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/1310
这是一个用户需求问题，涉及TensorRT-LLM模型中如何在模型生成过程中为每个token添加用户指定的embedding。因用户希望在生成模型时控制token的embedding。

https://github.com/NVIDIA/TensorRT-LLM/issues/1308
这是一个用户提出的关于如何设置`PromptTuningParams`中的参数的问题，主要涉及设置`embeddingTable`和`tasks`参数，由于处理多批次输入时，每个输入都包含多个图像，需要确保能够区分哪个图像属于哪个输入。

https://github.com/NVIDIA/TensorRT-LLM/issues/1307
这是一个bug报告，主要涉及Gemma在执行convert_checkpoint.py脚本时出现错误。这个问题出现的原因是在执行`quantize_fp8_weights()`方法时，某些HF参数被卸载到CPU上。

https://github.com/NVIDIA/TensorRT-LLM/issues/1306
这是一个bug报告，涉及TensorRT-LLM下的一个import错误。这个错误是因为在导入`types`模块时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1305
这是一个关于性能下降的bug报告，涉及到TensorRT-LLM中Mistral 7b和Mixtral 8x7b的使用体验。原因可能是使用某些特定格式的提示导致模型无法完整响应。

https://github.com/NVIDIA/TensorRT-LLM/issues/1304
这是一个bug报告，涉及的主要对象是使用Mixtral engine build时在8个40GB GPU上发生CUDA内存溢出。导致这个问题的原因可能是不能正确估计构建Mixtral引擎所需的GPU内存。

https://github.com/NVIDIA/TensorRT-LLM/issues/1303
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM在使用特定配置时生成垃圾tokens的问题。可能是由于输入参数配置不当导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1302
这是一个bug报告类型的issue，主要涉及TensorRT-LLM库无法导入的问题。由于未能打印出预期的tensorrt_llm版本号，用户提出了寻求帮助的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1301
这是一个bug报告，主要涉及TensorRT-LLM在安装过程中出现的错误，可能由于代码更新导致安装报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1300
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的QwenVL示例代码，用户遇到了无法正确读取图像嵌入的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1299
这是一个bug报告，涉及TensorRT-LLM版本v0.8.0在处理长文本输入时输出精度变差的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1298
这是关于如何使用trt_llm加速原始llava-liuhaotian/llava-v1.5-7b的问题，属于bug报告类型，主要涉及模型配置不匹配导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1297
这是一个Bug报告，涉及主要对象是TensorRT-LLM中的quantize_by_ammo模块。出错的原因是缺少对变量'args'的定义。

https://github.com/NVIDIA/TensorRT-LLM/issues/1296
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的server.py文件。该问题是由于参数混淆造成，导致max_new_tokens始终为1（布尔值True）。

https://github.com/NVIDIA/TensorRT-LLM/issues/1295
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的engine和executor代码，由于代码期望使用子类TokenizerBase，但实际在示例中使用了transformers.AutoTokenizer，导致传递HF目录的tokenizer出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1294
这是一个bug报告，涉及的主要对象是在执行docker commit后导致"import tensorrt_llm"失败，可能由于环境配置问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1293
这是一个bug报告，主要涉及TensorRT-LLM中的模块导入错误，导致了无法调用特定函数的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1292
这是一个用户提出需求的issue，主要涉及TensorRT-LLM的性能优化和吞吐量问题。由于当前的基准测试方法有限，并不符合实际情况，导致无法展示出模型优势在吞吐量场景下的表现。

https://github.com/NVIDIA/TensorRT-LLM/issues/1291
这个issue是关于用户提出需求的问题，主要涉及TensorRT-LLM下的MPT模型，由于缺乏FP8支持，用户寻求关于该特性的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1290
该issue是一个bug报告，涉及的主要对象是v0.8.0 tag下的TensorRT-LLM的trtllm-build命令，由于在该版本中缺少max_draft_len参数的支持而导致无法构建模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1289
这是一个bug报告，涉及的主要对象是TensorRT-LLM项目。由于安装新版本的tensorrt_llm包后，用户在导入模块时出现了ModuleNotFoundError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1288
这是一个bug报告，主要涉及缺少模块导致了无法正常运行的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1287
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的convert_checkpoint.py脚本。由于在加载预训练模型时出现了"InvalidHeaderDeserialization"错误，导致了该bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1286
这是一个用户报告bug的issue，主要涉及TensorRT-LLM中使用Flan-T5模型时的性能问题。用户试图构建具有不同配置和张量并行性的引擎，但遇到了问题，请求帮助解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/1285
这是一个bug报告类型的issue，涉及的主要对象是使用TensorRT-LLM进行模型量化。由于资源不足导致CUDA OOM，进而无法成功进行模型量化。

https://github.com/NVIDIA/TensorRT-LLM/issues/1284
这是一个bug报告，涉及TensorRT-LLM下的pipeline parallelism和gather_all_token_logits在使用过程中导致segmentation fault错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1283
这是一个bug报告，涉及TensorRT-LLM中无法成功转换模型的问题。由于未能成功运行第4步，导致出现错误行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/1282
这是一个bug报告。主要涉及对象是TensorRT-LLM中的Executor模块。由于Executor配置的最大beam大小为3，用户无法将beam大小设置为1，从而无法执行token healing操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1281
这个issue是关于bug报告，涉及主要对象是更新精度（precision）文档。由于多模型（multimodel）拼写错误，导致需要进行修正。

https://github.com/NVIDIA/TensorRT-LLM/issues/1279
这是一个bug报告，涉及到TensorRT-LLM中的Assertion错误，用户寻求关于系统运行中出现的错误以及如何解决该错误的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1278
这个issue类型为文档更新（Chore），主要对象是项目的README文件，由于链接错误导致需要修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1277
这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持指定模型"deepseekai/deepseekmoe16bbase"，并询问是否有计划支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1276
该问题类型为用户需求，主要涉及对象是TensorRT-LLM中的LORA功能。用户提问是否LORA支持除LLAMA之外的其他LLM，并是否需要自己进行适配。

https://github.com/NVIDIA/TensorRT-LLM/issues/1275
这是一个用户提出需求的类型，主要对象是更新链接到Nitro下载和LlamaCorn模型引擎，可能是因为链接已过时或指向错误导致用户希望更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/1274
这个issue属于更新issue，主要涉及TensorRT-LLM的功能更新和改进，包括模型支持、特性添加、API修改、Bug修复、性能优化和基础设施更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/1273
该问题是一个bug报告，涉及的主要对象是如何使用BF16构建Mistral模型。由于缺少具体细节导致了构建时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1272
这是一个bug报告，涉及的主要对象是Mistral AWQ quantization，由CUDA error触发，导致无法生成checkpoint文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/1271
这是一个bug报告，涉及到TensorRT-LLM模型转换中的`freq_cis`字段缺失导致转换失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1270
这是一个bug报告，涉及对象为TensorRT-LLM中NMT的embedding weights，问题原因是当前embedding weights没有转换为相同的数据类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1269
这是一个用户提出需求的issue，主要涉及到如何在0.8.0版本的LLaMA模型推理引擎中开启in-flight batching功能。这个问题可能是由于用户想要优化推理性能而提出的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1268
这是一个bug报告，主要涉及LLaMA INT8 KV Cache engine在构建时出现错误，可能是由于校准过程中出现故障导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1267
这是一个bug报告，涉及的主要对象是Smoothquant LLaMA builds。由于尝试保存一个非连续张量导致数值错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1266
这是一个关于用户需求的问题，主要涉及到自定义加载模型参数逻辑。用户希望通过定制加载逻辑来加载每个张量而非使用默认的加载机制，因为他们正在使用tensorrtllm库。

https://github.com/NVIDIA/TensorRT-LLM/issues/1265
这个issue类型为用户提出需求，主要对象是TensorRT-LLM，用户询问是否有计划支持Yi。可能是由于Yi目前不被支持，用户希望了解是否会在未来添加对Yi的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1263
这是一个文档改进类的issue，主要涉及TensorRT-LLM下的inflight batching支持文档。由于文档可能缺乏相关信息或者未清晰表达相关概念，用户可能需要更多关于inflight batching的指导或者说明。

https://github.com/NVIDIA/TensorRT-LLM/issues/1262
这是一个用户提出需求的类型，主要对象是是否支持Qwen Lora，可能是因为用户想要了解TensorRT-LLM是否计划支持Qwen Lora设备。

https://github.com/NVIDIA/TensorRT-LLM/issues/1261
这是一个bug报告，主要涉及TensorRT-LLM下的模型的准确性问题。由于某些输入序列，模型生成的tokens数量少于预期，但与TGI模型相比，在相同的环境中TGI模型的输出是正常的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1260
这是一个bug报告，主要涉及TensorRT-LLM在Windows 10下构建时缺失BatchManager导致链接错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1259
该issue类型为错误报告，涉及主要对象为文档集成。由于误将PR提交给了错误的仓库，导致产生了混乱。

https://github.com/NVIDIA/TensorRT-LLM/issues/1258
这是一个bug报告，该问题涉及到在包含空格的文件夹中运行Python时出现的问题。由于路径中包含空格字符，导致无法正确运行Python脚本。

https://github.com/NVIDIA/TensorRT-LLM/issues/1257
这是一个需求类型的issue，主要涉及的对象是TensorRT-LLM下的`nitro-tensorrt-llm`项目。由于缺乏README文件中一些关键信息，用户需要说明该项目的目标、与`nitro`的区别，以及如何快速开始、如何编译/导出以及如何使用可执行/编译二进制文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/1256
这是一个bug报告类型的issue，涉及TensorRT-LLM中的Code Llama 70b模型和XQA错误，由于发送多个推断请求导致模型崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/1255
这是一个bug报告，主要涉及TensorRT-LLM下的性能问题，用户在同样条件下未达到文档描述的吞吐量数值。

https://github.com/NVIDIA/TensorRT-LLM/issues/1254
这是一个缺少文件报告issue，主要涉及TensorRT-LLM中的build.py文件缺失问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1253
这是一个bug报告，涉及TensorRT-LLM下的Streaming功能不可用的问题，可能是因为开发者遗漏了实现Streaming支持导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1252
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于缺少llama.convert模块导致了报告的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1251
该issue属于bug报告类型，主要涉及TensorRT-LLM的Forward函数出现错误，原因是处理长度超过了shared memory限制所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1250
这是一个bug报告，涉及的主要对象是TensorRT-LLM模型的推理结果是否受批处理大小影响。由于批处理大小的改变可能导致推理结果的变化，用户询问批处理大小是否会影响LLM推理结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1249
这是一个bug报告，主要涉及TensorRT-LLM中的Qwen-72B-Chat-Int4模型在配置多个GPU时产生assertation错误并导致推理结果混乱的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1248
这是关于更新requirements.txt的bug报告，主要涉及缺少的packages导致qwenvl无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1247
这个issue类型是bug报告，主要涉及TensorRT-LLM版本v0.8.0中的一个KeyError bug，由于新版本config.json文件缺少builder_config导致benchmark测试出现异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1246
该issue类型是bug报告，涉及对象是TensorRT-LLM中的max_new_tokens参数。增加max_new_tokens参数显著降低了推理速度，由此产生了78倍的速度减慢，用户寻求解释是否是预期行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/1245
这是一个bug报告，涉及TensorRT-LLM下的AWQ Quantization，由于未知的格式导致文件生成错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1244
这是一个Bug报告，主要涉及TensorRT-LLM下的MMLU得分在使用FP8量化时出现意外行为。这可能是由最近的提交引入的错误所导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1243
这是一个bug报告类型的issue，主要涉及Tensorrt-LLM和transformers在推理baichuan27bchat模型时出现不一致的结果。原因可能是Tensorrt-LLM的推理过程中产生了一些token不完全相同并且含有一些逻辑错误的单词。

https://github.com/NVIDIA/TensorRT-LLM/issues/1242
这是一个bug报告，涉及TensorRT-LLM的examples中的文档有误，用户反映引擎生成工具和脚本输出不匹配。

https://github.com/NVIDIA/TensorRT-LLM/issues/1241
这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的SmoothQuant支持MoE模型（如Mixtral），用户询问其是否在开发路线图中并希望得到明确答复。

https://github.com/NVIDIA/TensorRT-LLM/issues/1240
这是一个性能测试的issue，涉及的主要对象是XQA在TensorRT-LLM中的表现，用户反馈XQA在A100 GPU上的性能不如预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/1239
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于TensorRTLLM v0.8.0分支下构建int8+kv8时失败，可能是由于代码实现或依赖环境配置问题所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1238
该issue类型为增加调试选项，主要对象是trtllm-build工具。这个增加的功能主要为了让开发者能够可视化TensorRT网络在构建Engine之前的结构，以便进行调试和分析。

https://github.com/NVIDIA/TensorRT-LLM/issues/1237
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下对于Jais模型的支持。由于Jais模型是一种基于transformer-based解码器的模型并使用了SwiGLU非线性，要求将其添加到TensorRT-LLM中以提供更好的上下文处理和模型精度。

https://github.com/NVIDIA/TensorRT-LLM/issues/1236
这个issue属于安装失败的bug报告，主要涉及的对象是TensorRT-LLM包0.8.0版本的安装。由于pypi下载时获取到了无后缀的tensorrtllm0.8.0.tar.gz文件，导致安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1235
这是一个bug报告issue，涉及TensorRT-LLM的问题。由于在推理过程中出现了"No valid weight only groupwise GEMM tactic"错误，用户寻求帮助解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1233
这个issue属于bug报告类型，涉及TensorRT-LLM工具中的模型支持、API变更、Bug修复以及文档更新。造成bug的原因可能是在Gemna模型导入和转换方面存在问题，导致一些错误的结果或链接问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1232
这是一个bug报告，主要涉及TensorRT-LLM的pipeline parallelism功能在pp_size大于1时出现Various CUDA errors。原因可能是代码实现中存在问题导致CUDA运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1231
这个issue是关于bug报告，涉及到TensorRT-LLM中的int8模型转换脚本缺失scale和zero-point警告问题。导致该问题可能是缺失了scale和zero-point参数导致模型编译时警告出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/1230
这是一个空内容的issue，类型无法确定。

https://github.com/NVIDIA/TensorRT-LLM/issues/1229
这是一个用户提出需求的问题，主要涉及TensorRT-LLM下的Mixtral（MoE）模型支持FP8量化的时间线问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1228
这是一个bug报告，用户遇到了获取context logits数值的问题，尝试了多种方法但未成功。

https://github.com/NVIDIA/TensorRT-LLM/issues/1227
这是一个关于性能问题的bug报告，主要涉及的对象是TensorRT-LLM中的Batch inference using Llava ModelRunner，由于批量推理任务的耗时远远超过了单个推理的预期时间，可能由于`tensorrt_llm.runtime.ModelRunner`存在问题或者CUDAStreamSynchronize问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1226
这是一个用户提出需求的类型的issue，主要涉及对象是TensorRT-LLM，用户询问是否TensorRT-LLM将会支持VIT模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1225
这是一个功能需求类型的issue，主要涉及TensorRT-LLM集成QUICK内核以实现AWQ量化。

https://github.com/NVIDIA/TensorRT-LLM/issues/1224
这是一个用户提出需求的类型，主要涉及的对象是多个LoRa设备的支持。由于功能需求，用户询问是否支持SLoRA机制，并寻求关于示例或未来计划的信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1223
这是一个用户提出需求的问题，主要对象是如何在两个GPU上初始化两个不同的模型，由于提供的命令导致两个模型都在cuda:0上初始化。

https://github.com/NVIDIA/TensorRT-LLM/issues/1222
这是一个用户提出需求的issue，主要涉及如何估算TensorRT-LLM模型在特定硬件、模型和量化设置下的最大批次大小，以及推荐的最大token数量。 导致这个问题的原因是用户需要更多关于如何根据硬件和设置来设置合理的最大批次大小相关信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/1221
这个issue类型是bug报告， 主要涉及的对象是TensorRT-LLM的cuda out of memory问题，原因是PyTorch尝试分配的内存过多导致CUDA内存不足。

https://github.com/NVIDIA/TensorRT-LLM/issues/1220
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM中GptSession的MPI初始化问题，导致用户在非MPI环境下无法进行升级操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1219
这是一个用户提需求的问题，主要需求是如何将Medusa解码添加到服务器中，并提出了在服务器示例中需要的选项不足的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1218
该issue属于用户提出需求类型，主要涉及如何在TensorRT-LLM中添加新模块，如alltoall或自定义模块。这可能是因为用户想要扩展TensorRT-LLM的功能，但不清楚如何实现。

https://github.com/NVIDIA/TensorRT-LLM/issues/1217
这是一个bug报告，涉及的主要对象是Mixtral示例中的run.py文件，因为链接失效导致无法展示引擎执行的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1216
这是一个关于bug报告的issue，主要涉及TensorRT-LLM在使用`enable_chunked_context`参数时导致的精度下降问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1215
这是一个bug报告，涉及TensorRT-LLM下的构建问题，由于缺少"libtensorrt_llm.so"文件导致wheel安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1214
这是一个用户提出的问题，涉及到在TensorRT-LLM下如何设置rotary embeddings的配置参数。用户提到在quantize.py工具中无法找到设置rotary embeddings的选项，询问如何在quantized模型中设置rotary embeddings。

https://github.com/NVIDIA/TensorRT-LLM/issues/1213
这是用户提出的需求。该问题单涉及的主要对象是TensorRT-LLM，用户希望支持文本嵌入模型以及来自句子转换器的重新排序模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1212
这是一个bug报告，涉及TensorRT-LLM在Windows上从源代码构建wheel时遇到问题，可能是由于CMake生成器执行失败导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1211
这是一个bug报告，主要涉及TensorRT-LLM下使用smoothquant时遇到的“shape cannot be computed”错误，可能由于代码中的某些问题导致无法计算输出形状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1210
这是一个用户请求帮助的issue，主要涉及如何为finetuned bert模型构建TensorRT引擎，用户寻求关于序列分类的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1209
这个issue是关于构建TensorRT-LLM在Windows上出错的bug报告，涉及到缺少MSVC版本的executor，导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1208
这是一个bug报告，主要涉及TensorRT-LLM中Qwen1.5模型的构建错误导致的KeyError问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1207
这是一个bug报告，主要涉及TensorRT-LLM的构建问题，用户报告了“trtllm-build: not found”的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1206
这是一个bug报告，涉及的主要对象是SmoothQuant在TensorRT-LLM下的错误。这个问题可能是由于平台兼容性问题导致的运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1205
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM项目中的代码文件"run.py"。由于在代码行309中未定义'hidden_size'变量，导致了这个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1204
这个issue属于用户提出的需求类型，主要涉及TensorRT-LLM中的Chunked context支持FP8的问题，由于FP8与paged_context_fmha不兼容导致无法使用该功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1203
这是一个关于如何在新的trt-build中启用inflight batching的问题报告，涉及的对象是TensorRT-LLM。由于原参数无法在新系统中找到对应的设置，导致用户无法启用inflight batching。

https://github.com/NVIDIA/TensorRT-LLM/issues/1202
这是一个需求类型的 issue，主要涉及到为 TensorRT-LLM 添加 0.8 batch manager 的静态库。

https://github.com/NVIDIA/TensorRT-LLM/issues/1201
这是一个bug报告，主要涉及TensorRT-LLM中转换模型时出现的错误，问题可能是由于未正确配置fp16导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1200
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的模型量化操作，用户提出了关于fp8导出类型硬编码为float16的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1199
这是一个bug报告，主要涉及TensorRT-LLM下的插件创建问题，由于getPluginCreator无法找到特定插件所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1198
这是一个关于安装TensorRT-LLM时出现错误的bug报告，主要涉及的对象是尝试在NVIDIA A100上安装TensorRT-LLM的用户。由于参数设置错误导致安装过程中出现异常，最终导致安装失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1197
这个issue类型是bug报告，主要涉及的对象是tritonserver与tensorrt_llm whisper的使用。由于可能在运行时出现了无效指针错误，导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1196
这个issue是关于更新gh-pages的，属于一般性维护问题，主要对象为项目的网页分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/1195
这个issue是关于bug报告，主要涉及TensorRT-LLM中使用AWQ(W4A8)无法量化LLaMA2模型的问题。原因可能是由于校准过程失败导致trtllmbuild执行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1194
这是一个bug报告issue，涉及的主要对象是TensorRT-LLM模块。由于缺少'tensorrt_llm.bindings'模块导致ModuleNotFoundError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1192
这个issue类型是请求更新发布分支，主要涉及TensorRT-LLM。由于发布分支需要更新，用户希望进行相关操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1191
这是一个bug报告，涉及TensorRT-LLM下的一个模型输出问题。由于某些原因导致int4权重量化版本的merged Llama2 70b模型存在输出异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1190
这是一个bug报告，在TensorRT-LLM下的一个issue，主要涉及到close_ipc_memory Error异常以及构建latest dev trtllm后出现的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1189
这个issue是关于用户提出需求的问题，主要涉及的对象是TensorRT-LLM，用户询问是否支持w4a8 (int4 * int8)，因为一些GPU不支持fp8。

https://github.com/NVIDIA/TensorRT-LLM/issues/1188
This is a bug report. The main issue is high GPU memory usage during the execution of the TensorRT-LLM demo, even though the expected behavior is much lower memory consumption.

https://github.com/NVIDIA/TensorRT-LLM/issues/1187
这是一个bug报告，涉及对象是TensorRT-LLM中的Llama2/Mistral，问题可能是由于CUDA错误导致在进行性能基准测试时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1186
这是一个用户就如何在构建模型时使用多个quantized_npz文件的疑问，属于用户提出问题类型。由于quantize.py工具在处理tp值大于1时会导致生成多个npz文件，用户想了解如何正确传递多个文件给build函数。

https://github.com/NVIDIA/TensorRT-LLM/issues/1185
这是一个关于功能需求的issue，主要涉及到TensorRT-LLM中的多模态示例。由于自定义分词器为embedding分词生成了不同的假prompt id，用户想知道这是否符合设计，以及如何通过调整参数来实现所需的效果。

https://github.com/NVIDIA/TensorRT-LLM/issues/1184
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM中无法达到相同的最大批处理大小（峰值吞吐量），可能是由于未知原因导致的进程挂起问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1183
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的ModelRunnerCpp。问题是由于在ModelRunnerCpp中对SamplingConfig的处理不正确，导致了无法设置每个batch条目的独立数值，导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1182
这是一个bug报告，涉及主要对象为在T4上运行TensorRT-LLM中的ModelRunnerCpp时出现的问题。这个问题可能由于硬件不匹配导致kernel死机的情况而产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/1181
这是一个bug报告，涉及到Mixtral项目中缺少 run.py 文件的问题，可能导致无法运行相关功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1179
这是一个用户提出需求的类型。该问题涉及TensorRT-LLM中的C++接口使用，并请求提供类似llama.cpp的示例代码。这可能是由于用户想要了解如何使用C++接口来初始化和运行LLM模型，而不需要关注分词器的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/1178
这是一个用户询问关于系统支持int16的issue，主要对象是TensorRT-LLM。用户询问是否TensorRT-LLM支持int 16量化，由于无人回应可能导致现象是用户无法确定系统对int16的支持情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1177
这个issue类型是bug报告，该问题涉及的主要对象是TensorRT-LLM下的`ParallelGenerationExecutor`，由于原始实现是LIFO而不是FIFO，导致在大批量请求时会出现不同MPI会话中请求顺序不一致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1176
这是一个bug报告，主要涉及TensorRT-LLM下的文件查找错误导致无法执行脚本。

https://github.com/NVIDIA/TensorRT-LLM/issues/1175
这个issue是关于bug报告，主要涉及TensorRT-LLM中logits_processor逻辑，在token生成过程中出现重复token生成的问题，可能是由于对logits引起的潜在错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1174
这是一个关于技术问题的请求帮助类型的issue，主要涉及将大型世界模型转换为TensorRT LLM的问题，由于权重是使用JAX进行处理，用户不熟悉JAX和TensorRT LLM，导致他们不清楚如何继续进行模型转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/1173
这是一个Bug报告，涉及的主要对象是TensorRT-LLM下的BERT模型。由于模型输出的余弦距离与预期不符，可能是由于模型不准确导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1172
这个issue是关于Failed to quantize Llama2 70b fine tuned model to AWQ Int4的bug报告，涉及的主要对象是TensorRT-LLM下的模型量化过程。原因可能是软件版本兼容性问题或者环境配置错误所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1171
这是一个bug报告，主要涉及Medusa convert cpt在处理safetensors版本时出现问题，可能是由于脚本期望找到一个名为"medusa_lm_head.pt"的文件而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1169
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的FP8量化功能。由于开启了"use_paged_context_fmha enable"参数，在某些情况下会导致结果错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1168
这个issue属于bug报告类型，主要涉及TensorRT-LLM中的模型支持、特性更新、bug修复和文档更新，其中包括了多个不同的问题和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/1167
这是一个可能与命令行工具 trtllm-build 不支持 "qwen model" 的功能或模型类型相关的功能需求问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1166
这个issue类型是用户提出需求，主要涉及到如何处理批量预测中变长的decoder_input_ids，由于模型Nougat家族中的模型不支持此功能，用户寻求如何在donutbasefinetuneddocvqa模型中应用。

https://github.com/NVIDIA/TensorRT-LLM/issues/1165
这个issue属于bug报告类型，主要涉及 pip 安装 tensorrt_llm 时出现的问题，可能是由于 pip 版本变化导致的 DEPRECATION 错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1164
这是一个bug报告，主要涉及TensorRT-LLM中的llama Engine构建错误。由于max_workers参数设置不正确导致数值错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1163
该issue属于用户提出问题类型，主要涉及TensorRT-LLM中的Medusa解码相关功能，用户对medusa_choices参数的含义和实现细节产生困惑，希望得到详细介绍。

https://github.com/NVIDIA/TensorRT-LLM/issues/1162
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的模型构建过程中出现失败的情况。导致这个bug的原因可能是构建脚本或参数设置的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1161
这是一个BUG报告，主要涉及TensorRT-LLM的Windows构建问题，由于缺少对Windows版本的batch_manager支持导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1160
这是一个功能请求，用户想要在nvidia orin平台上支持TensorRT-LLM，并尝试在该平台上编译TensorRT-LLM时遇到了编译失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1159
这个issue是关于用户需求的，涉及的主要对象是TensorRT-LLM中的`system prompt caching`功能。用户想了解如何使用这个功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1158
这是一个bug报告，该问题涉及的主要对象是TensorRT-LLM下的BERT或robert模型。由于尝试使用remove_padding功能时出现编译错误，导致报错信息中出现了内部错误的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1157
这个issue是关于Bug报告，单涉及的主要对象是TensorRT-LLM中的服务器示例。由于一些小问题在服务器示例中导致了bug的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1156
这是一个Bug报告，主要对象是在8xA10机器上构建Mixtral时出现CUDA内存不足的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1155
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM在V100上的WeightOnly测试，导致的问题是在V100上执行WeightOnly unittest时失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1154
该issue是一个用户提出的需求，主要对象是在TensorRT-LLM中添加Min-P采样层。由于Min-P采样在本地LLM中变得更受欢迎，并提供更有用的结果或感觉如此，因此用户希望这个库能够支持Min-P采样方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/1153
这是一个Bug报告，涉及主要对象为LLaMA + SmoothQuant checkpoint creation无法在bfloat16下工作。原因可能是代码中的错误导致了无法生成trtllm checkpoint。

https://github.com/NVIDIA/TensorRT-LLM/issues/1152
这是一个bug报告类型的issue，主要涉及TensorRT-LLM库在Windows平台下通过pip安装失败的问题，可能由于安装过程中某些依赖项无法正确下载导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1151
这是一个bug报告，主要涉及TensorRT-LLM High Level API，由于版本0.9.0.dev2024022000，在使用高级Python API时初始化构建的engine无法加载配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/1150
这是一个关于构建失败的bug报告，主要涉及TensorRT-LLM项目。可能由于无法成功运行`make C docker release_build`命令，导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1149
这是用户提出的一个功能需求，主要对象是对输出字典中的Option fields进行追加。原因是用户希望能够获得每个输入序列位置的TokenIDs的Topk对数概率。

https://github.com/NVIDIA/TensorRT-LLM/issues/1148
这是一个bug报告，主要涉及到在从Hugging Face导入Gemma模型时，由于注意力机制的输入输出大小不一致导致形状定义错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1147
这是一个功能需求的issue，主要涉及的对象是TensorRT-LLM下的Gemmi模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1146
这个issue属于bug报告类型，涉及主要对象为TensorRT-LLM。这个问题的症状是由于某些原因导致的，用户可能遇到了与模型加载相关的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1145
这个issue类型是bug报告，涉及主要对象是TensorRT-LLM代码中的mpmath模块。因为未锁定mpmath版本导致新版本1.4.0a0引发了'rational'属性错误，导致了AttributeError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1144
这是一个关于性能问题的bug报告，主要涉及到TensorRT-LLM下的Pipeline Parallel和Tensor Parallel引擎的对比，询问为何Pipeline parallelism表现低于Tensor parallelism。

https://github.com/NVIDIA/TensorRT-LLM/issues/1143
这个issue是关于bug报告，涉及TensorRT-LLM在DEBUG模式下构建时出错。在DEBUG模式下启用了`enable_debug_output`后出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1142
这是一个bug报告，主要涉及TensorRT-LLM下的`model_runner_cpp.py`文件，由于缺少修复，导致运行TRT engines时会出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1141
这是一个报告bug类型的issue，主要涉及TensorRT-LLM中在多个A30 TP上运行BLIP2-T5时遇到问题。由于单个A30内存不足以支持模型，使用2个A30 GPU结合Tensor Parallel运行时脚本会卡住。

https://github.com/NVIDIA/TensorRT-LLM/issues/1140
这是一个用户提出需求类型的issue，主要涉及TensorRT-LLM中的RelayAttention。

https://github.com/NVIDIA/TensorRT-LLM/issues/1139
这是一个bug报告，主要涉及到TensorRT-LLM中的脚本`convert_checkpoint.py`在`smoothquant llama2-7b`时运行缓慢的问题。原因可能是在运行`smoothquant`时无法继续执行`generate_int8()`函数，导致无法正确完成转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/1138
这个issue类型是用户询问问题，涉及主要对象为TensorRT-LLM中的Gemma模型。由于找不到`tmp_vocab.model`文件和版本兼容性问题，导致用户无法进行推断和选择合适的Triton Server版本运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1137
这个issue类型是bug报告，涉及的主要对象是AsyncLLMEngine，由于无法从张量中获取值导致了"Failed to get value from tensor: request_output_len"的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1136
This issue is a bug report for the project TensorRT-LLM under GitHub, involving upgrading the version of gradio library in a specific directory, "/examples/qwen".

https://github.com/NVIDIA/TensorRT-LLM/issues/1135
这是一个关于开源代码缺失问题的类型为需求问题的issue，主要涉及的对象是TensorRT-LLM中的batch_manager和executor模块。由于缺失源代码，用户提出了对应开源库中编译二进制文件的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1134
这是一个bug报告类型的issue，涉及的主要对象是TensorRT-LLM中的quantize llama2模型，由于lm_head.weight键错误导致了无法正确创建trt engine的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1133
这是一个bug报告，主要涉及对象为`xverse-65b`模型的推理过程出现错误。该问题的原因是Tensor 'past_key_value_0'的形状不合法，导致了推理过程中的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1132
这是一个用户提出需求的issue，主要涉及TensorRT-LLM模型中不同概率之间的相关性问题，用户希望找到详细的解释。

https://github.com/NVIDIA/TensorRT-LLM/issues/1131
这是一个bug报告，主要涉及TensorRT-LLM下运行run.py时出现NCCL错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1130
这是一个关于依赖冲突的bug报告，涉及的主要对象是在构建Docker容器时遇到的TensorRT-LLM版本的依赖冲突。由于依赖库版本不匹配，导致了构建过程失败的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1129
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的安装问题。由于参数错误导致setup.py无法成功运行，进而生成包元数据失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1128
这个issue是一则需求更新，涉及到gemma项目的README文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/1127
这是一个用户需求提出的问题，主要涉及了如何从生成会话中返回logprobs。用户希望能够获取whisper解码器的logprobs，但尝试过程中未能成功，导致所需的logprobs无法获取。

https://github.com/NVIDIA/TensorRT-LLM/issues/1126
这是一个需求更新类型的issue，涉及的主要对象是README.md文件。用户提出需要将ammo分支版本从0.7.0更新至0.7.3，可能是为了更新项目依赖或修复之前版本存在的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1124
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下moe gemm对w8a8的支持问题，用户询问是否有计划支持w8a8的moe。

https://github.com/NVIDIA/TensorRT-LLM/issues/1123
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在GPU上性能不如预期的问题，可能由于算法实现或代码修改导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1122
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的功能更新、API改进、bug修复以及性能优化。原因是修复了一个关于`Whisper`的权重量化bug，该bug导致在Python运行时未返回log probabilities。

https://github.com/NVIDIA/TensorRT-LLM/issues/1121
这是一个bug报告，主要涉及TensorRT-LLM中Smoothquant在Llama2-13B上出现的意外结果。由GPU内存不足导致的警告可能是导致这些异常结果的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/1120
这是一个关于代码逻辑错误的bug报告，主要涉及到AWQ_int4_group_128模式下的权重转换问题。由于权重被舍入为[-8,7]后又转换为int8，导致用户感到困惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/1119
这是一个用户询问类的问题，涉及对象是在TensorRT-LLM中部署qwen1.5模型。由于原因不明，用户想知道是否能够使用TensorRT-LLM部署qwen1.5模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1118
这是一个bug报告，涉及LLAVA在LLM部分输出了不必要的图像标记导致性能下降的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1117
这是一个bug报告，主要涉及TensorRT-LLM的转换脚本在执行过程中出现错误。由于某些参数被卸载到CPU导致部分参数在meta设备上，引起了该bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/1116
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM下的模型架构不被支持导致的RuntimeError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1115
这是一个用户提出需求的issue， 主要对象是项目中的某个特定修改，由于没有具体描述内容，无法分析导致问题的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/1114
这是一个关于需求探讨的问题，主要涉及的对象是AMMO，问题想要了解是否AMMO支持QAT训练流程。

https://github.com/NVIDIA/TensorRT-LLM/issues/1113
这个issue是关于bug报告，主要涉及TensorRT-LLM下的quantization模块，用户在使用int8_wo和kv_cache fp8模式执行trtllmbuild命令时出现了assert failed的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1112
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的编译失败问题。出现这个问题可能是由于CUDA 12.2、TensorRT 9.0版本以及其他环境配置不兼容引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1111
这是一个用户提出需求的issue，主要涉及TensorRT-LLM的支持Constrained Decoding的功能请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1110
这是一个Bug报告类型的issue，主要涉及到MixtRAL 模型使用tensor并行转换时出现了参数识别错误的问题，用户询问是否应该移除world_size参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/1109
这是一个bug报告，主要涉及TensorRT-LLM下的"trtllm-build"命令出现的错误。由于提供的张量名称与引擎所期望的张量名称不同，导致出现了"RuntimeError"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1108
这是一个用户提出需求的问题，主要涉及TensorRT-LLM中的pipeline parallelism模式。由于需求是指定每个pp层的gpu id，用户可能遇到了无法指定gpu id的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1107
这是一个用户提出需求的issue，主要对象是TensorRT-LLM，用户在请求关于int4权重的运行支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1106
这个issue类型是用户提出需求， 主要对象是关于AWQ配置参数的调整。用户询问如何通过AMMO来配置AWQ的权重分数比。

https://github.com/NVIDIA/TensorRT-LLM/issues/1105
这个issue是用户提出需求，请教问题类型，主要对象是创建对话的template。

https://github.com/NVIDIA/TensorRT-LLM/issues/1104
这是一条bug报告，主要涉及的对象是AWQ (W4A8 mode)转换llama-7b模型过程中出现的错误，用户希望得到帮助解决该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1103
这是一个涉及代码质量的bug报告，主要涉及到TensorRT-LLM中的torchAllocator.cpp文件，由于未同步当前流和指定的CUDA流导致PyTorch的CUDA语义被违反。

https://github.com/NVIDIA/TensorRT-LLM/issues/1102
这是一个bug报告，主要涉及TensorRT-LLM下的pipeline parallelism和单GPU之间的性能问题。原因可能是由于一些异常情况导致了pipeline parallelism略快于单GPU。

https://github.com/NVIDIA/TensorRT-LLM/issues/1101
这是一个关于功能询问的问题，主要涉及Quantization methods的支持情况。由于文档信息不一致，用户正在请求了解当前multi-modal (decoder only) pipeline是否支持smoothquant和awq方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/1100
这是一个关于bug的报告，主要涉及TensorRT-LLM中使用awq4bit量化后精度严重下降的问题，用户询问是否需要使用自己的训练集进行校准以及是否有可调参数来解决该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1099
这是一个bug报告类型的issue，主要涉及的对象是在使用TRT LLM适配器m2m100时遇到了SinusoidalPositionalEmbedding相关的问题。导致这个问题的原因可能是代码中的input_ids.size()返回了无法解包的值，从而导致数值不匹配的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1098
这是一个功能需求问题，用户提出了需要更新TensorRT-LLM的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1097
这是一个bug报告，主要涉及TensorRT-LLM中Mixtral-8x7B模型推理时间随每个新请求而变慢，可能由于模型处理请求后响应时间迅速增加而导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1096
这是一个bug报告，涉及的主要对象是NVIDIA/TensorRT-LLM CC，由于安装命令已被弃用导致了此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1095
这是一个bug报告，涉及的主要对象是TensorRT-LLM项目的安装说明。导致该问题的原因是安装指令中使用了已被弃用的nvidiadocker。

https://github.com/NVIDIA/TensorRT-LLM/issues/1094
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM下使用自定义模型文件并在并行构建时出现的import错误。约原因是原始实现会导致import错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1093
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的chatGLM3-6B模型构建TensorRT engine时出现错误。原因可能是代码中的某些错误导致了构建过程中的异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1092
这是一个bug报告，主要涉及TensorRT-LLM中的visual_encoder失败，可能是由于输入形状静态维度不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1091
这是一个性能优化的issue，主要涉及TensorRT-LLM中的moe router，通过移除TP实现加速，导致在Mixtral8x7Bv0.1中出现418%的解码速度提升。

https://github.com/NVIDIA/TensorRT-LLM/issues/1090
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中benchmarking文档中引用了不存在的build.py脚本，可能由于文档更新未同步脚本或者脚本位置发生变化导致了该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1089
这是一个bug报告，涉及到Benchmarking build docs的更新。原因是由于指示错误导致了使用特权时出现问题，提出了正确的使用方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/1088
这是一个bug报告类型的issue，主要涉及的对象是Nougat模型的构建过程。由于缺少`to_legacy_settings()`方法导致了无法成功构建模型的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1087
这是一个用户提出需求的issue，主要涉及到为TensorRT-LLM添加对CogVLM模型的支持。由于用户认为CogVLM是描述图像的最佳模型之一，希望能够在4位上运行该模型以加快图像的字幕生成速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/1086
这个issue是一个Bug报告，主要涉及的对象是TensorRT-LLM中的tritonserver。由于出现了free()无效指针错误，导致程序崩溃并显示Signal: Aborted。

https://github.com/NVIDIA/TensorRT-LLM/issues/1085
这是一个bug报告，主要涉及CUDA编译器未找到的问题。由于Dockerfile中未找到CUDA编译器，导致在构建过程中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1084
这是一个bug报告，涉及到LLama Int4 AWQ (W4A8)模型在TensorRT-LLM下运行失败的问题，用户期望量化模型，但实际上遇到了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1083
这是一个Bug报告，涉及对象为TensorRT Quantization中的`LlamaLinearScalingRotaryEmbedding`模块，在编译并在fp8上运行时出现“no .weight for this module”的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1082
这是一个功能请求，主要涉及TensorRT-LLM Python package中的类定义问题。用户希望可以通过pip安装后直接导入类，以提高TensorRTLLM的易用性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1081
这个issue是关于bug报告，主要涉及TensorRT-LLM下的Whisper构建失败的问题，可能是由于`--remove_input_padding`选项导致编码器注意力层中的断言失败所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1080
这是一个 bug 报告，涉及的主要对象是在构建 TensorRT-LLM 时出现的 CUDA 版本不兼容问题，导致了编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1079
这是一个关于无法将Llama-2-7b-chat-hf模型转换为TensorRT-LLM引擎的bug报告，主要涉及到TensorRT-LLM的模型转换功能。由于安装`tensorrt_llm` python包时出现错误导致该问题的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1078
这是一个bug报告，涉及到TensorRT-LLM的模型量化转换时的数据类型问题，导致无法构建引擎的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1077
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的Finetuned Mistral模型构建失败，可能由于合并模型时出现的异常导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1076
这是一个bug报告，主要涉及对象是TensorRT-LLM中的`addRelativeAttentionBiasUnaligned` kernel。由于使用了整数数据类型进行索引，导致当序列长度和最大序列长度均设置为32k时可能会出现溢出问题，因此需调整数据类型为int64_t并进行静态转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/1075
这是一个用户提出需求的issue，该问题单涉及的主要对象是更新MPT requirements脚本。由于一些MPTfamily模型需要einops库，用户希望将此库添加到requirements.txt中。

https://github.com/NVIDIA/TensorRT-LLM/issues/1074
这是一个 bug 报告 issue，主要涉及TensorRT-LLM项目中的依赖安装，用户提出需要将 libnccl2 添加到 README.md 中的安装命令中，因为缺少 libnccl2 会导致导入 TensorRTLLM python package 出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1073
这是一个关于安装说明问题的bug报告，涉及主要对象为cuDNN版本。原因是更新后的cuDNN 9.0.0版本导致TensorRT-LLM无法正常运行，需要使用旧版本的cuDNN v8.9.7。

https://github.com/NVIDIA/TensorRT-LLM/issues/1072
这是一个bug报告，涉及的主要对象是无法运行Mixtral7B的基准测试。由于构建命令中可能存在错误，导致无法成功运行基准测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/1071
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的模型 llama 和 phi，在使用4个GPU时出现了无响应的问题。造成这个问题的原因可能是在执行模型时出现了错误，导致处理输入token数无法增加。

https://github.com/NVIDIA/TensorRT-LLM/issues/1070
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM库。导致这个bug的原因是变量在赋值前被引用，导致了UnboundLocalError异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/1069
这是一个bug报告，涉及主要对象是在使用V100 GPU进行模型转换时出现的异常信息，可能由于GPU不支持bfloat16数据类型导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1068
这是一个bug报告，主要涉及TensorRT-LLM中LLAMA转换checkpoint时出现的错误。由于缺少必需的package 'psutil'导致了模块导入错误和相关功能无法正常运行的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1067
这个issue是关于bug报告，主要涉及到TensorRT-LLM中的weight_only_quantize()函数，由于输入了一个未知的关键字参数'group_size'导致了报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/1066
这是一个bug报告，主要涉及TensorRT-LLM下的weight_only_quantize函数调用异常。由于传入参数中出现了意外的关键字参数'group_size'，导致出现TypeError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1065
这是一个关于bug报告的问题，主要涉及TensorRT-LLM中pip安装失败的情况。用户遇到了无法通过pip安装TensorRT-LLM并导致导入错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1064
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM。由于升级到0.9版本后，用户在使用新的workflow时出现配置文件不兼容的错误，需要指导如何处理或等待TensorRTBackend的更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/1063
这是一个bug报告，主要涉及TensorRT-LLM中的Llama模型创建checkpoint时出现错误的问题。原因可能是命令执行时的参数或路径设置不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/1062
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在Windows 11上安装后出现奇怪错误消息的问题。可能是由于安装过程中的步骤或配置出现了问题导致模型转换错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1061
这是一个功能增强的issue，主要涉及将huggingface的distil-whisper模型权重转换为兼容的pytorch `.pt`文件，以便构建TensorRTLLM引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/1060
这个issue是一个bug报告，主要涉及的对象是TensorRT-LLM，由于更新到最新分支后，可能由于某些原因导致OOM错误，用户正在寻求解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1059
该问题为如何在Phi模型示例中启用多GPU使用的问题，主要涉及的对象为TensorRT-LLM。由于未收到响应，导致用户无法解决如何使用多个GPU的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1058
该issue属于用户提出需求类型，主要对象为TensorRT-LLM。导致这个问题的原因是用户对于TensorRT在TensorRT-LLM中的作用和参与方式产生了疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/1057
这是一个bug报告，用户在尝试在Windows上安装TensorRT-LLM时遇到了CUDA编译器未找到的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1055
该issue属于功能更新类型，主要涉及TensorRT-LLM模型支持和功能增强。由于新版本增加了更多头部尺寸支持和修复了一些小bug，以及增加了文档内容和博客，用户可以更方便地使用新的功能和优化技术来提升推断速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/1054
这是一个bug报告，涉及主要对象为TensorRT-LLM下的构建引擎过程，由于CUDA设备错误导致了无法构建引擎的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1053
这是一个用户提出需求的类型，主要涉及对象是TensorRT LLM下的bark模型。由于用户需要支持bark模型，因此提出了这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1052
这是一个用户提出需求的issue，主要涉及到TensorRTLLM下的ModelRunner类，用户希望能够在ModelRunner类中提供自定义特征遮罩的功能，但目前未看到相关功能。用户认为缺少这一功能会影响他们使用唯一特征遮罩进行操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1051
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的`GenerationSession`。这个问题是由于生成token时出现特定情况导致`outputs['generation_logits']`为空列表，进而在运行过程中导致会话崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/1050
这个issue类型为用户提出需求，针对的主要对象是TensorRT-LLM GitHub仓库。由于缺乏关于如何克隆指定分支的说明，用户提出了添加更具体的命令，以便克隆特定的`rel`分支而非默认的`main`分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/1049
这是一个bug报告，涉及的主要对象是在使用多个GPU执行流式推断时出现系统挂起的问题。导致这个问题的原因是在系统中进行流式推断时，只有被指定为'rank 0'的进程执行迭代，但使所有进程执行迭代似乎可以解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1048
这是一个bug报告类型的issue，主要涉及TensorRT-LLM项目在Windows平台上生成wheel时缺少batch manager文件，导致无法成功生成wheel。

https://github.com/NVIDIA/TensorRT-LLM/issues/1047
这是一个bug报告，涉及TensorRT-LLM中的一个bug，导致在AWS g5.12xlarge上加载模型时出现mpiSize与tp*pp不匹配的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1046
这个issue是一个bug报告，主要涉及TensorRT-LLM在Windows 10上构建引擎时出现的问题。造成该bug的原因可能是由于预构建的引擎版本不匹配或者构建脚本出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1045
这是一个Bug报告类型的Issue，主要涉及TensorRT-LLM项目中在RTX 4070Ti上构建Llama-2-13B-Chat模型时遇到的内存分配问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1044
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的Smoothquant功能。用户询问是否有计划在未来的版本中支持volta GPU上的int8 smoothquant，表明用户希望在将来能够在volta GPU上使用这个功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/1043
这是一个用户询问问题类型的issue，主要涉及对象是TensorRT-LLM中的"prefix caching"功能，用户询问是否有关于此功能的文档。

https://github.com/NVIDIA/TensorRT-LLM/issues/1042
这是一个关于Bug报告类型的Issue，主要涉及的对象是在使用TensorRT-LLM下的BERT模型时遇到了[unusedXXX] tokens在解码文本中的意外存在。由于潜在的Tokenizer词汇表与TensorRT引擎产生的logits之间可能存在不匹配，导致解码输出中出现了这些意外token。

https://github.com/NVIDIA/TensorRT-LLM/issues/1041
这是一个bug报告，主要涉及针对Mixtral模型进行转换时出现的错误。产生该问题的原因是尝试使用Mixtral类型的模型来实例化LLAMA类型的模型，这在所有模型配置中并不受支持，可能会导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1040
这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的GenerationSession，用户希望能够收集额外的模型输出来满足特定需求，如tokenwise分类和特定头部的注意力分数。

https://github.com/NVIDIA/TensorRT-LLM/issues/1039
这是一个关于bug报告的issue，主要涉及LLama模型在多GPU上无法正常工作的问题。问题可能是由于LLama 70B模型在AWQ上执行quantize.py时导致GPU内存溢出而产生。

https://github.com/NVIDIA/TensorRT-LLM/issues/1038
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM，由于序列化CUDA引擎失败，导致用户在部署mistral7b模型时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1037
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的modified QWenAttention。由于修改后的QWenAttention与原版在数值精度上存在不一致，用户希望了解如何让TensorRT-LLM支持这一修改。

https://github.com/NVIDIA/TensorRT-LLM/issues/1036
这是一个关于用户需求的问题，用户询问是否有计划添加对xverse模型的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/1035
这是一个bug报告，涉及到TensorRT-LLM下的runtime代码实现中关于多节点推断配置的问题。问题由于代码中的断言判断错误，导致无法正确设置管线并行和张量并行参数，从而造成程序无法运行的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/1034
这是一个bug报告，涉及主要对象为TensorRT-LLM中的多GPU问题，由于peer access不支持两个设备之间的访问，导致无法使用多个GPU。

https://github.com/NVIDIA/TensorRT-LLM/issues/1033
这是关于操作顺序的问题，用户询问应该在git clone后运行git lfs pull吗，可能涉及到TensorRT-LLM的文档不清晰或存在误导。

https://github.com/NVIDIA/TensorRT-LLM/issues/1032
这是一个需求报告，涉及主要对象是自动化cuDNN设置。可能由于用户需要简化TensorRT-LLM的cuDNN设置流程导致提出这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1031
这个issue类型是用户提出需求，请教问题，主要涉及的对象是使用TensorRT-LLM进行简单模型推理的用户。原因是用户想要使用简单输入和输出进行模型前向推理，但目前的ModelRunner实现中缺乏清晰的model.forward()函数。

https://github.com/NVIDIA/TensorRT-LLM/issues/1030
这是一个用户提出需求的issue，主要涉及的对象是QWen是否支持MLM（Masked Language Model）功能。 

https://github.com/NVIDIA/TensorRT-LLM/issues/1029
这是一个bug报告，涉及的主要对象是在TensorRT-LLM下无法通过pip package转换whisper为TensorRT Engine。原因可能是TensorRTLLM下载自pip时带有版本标签而导致失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1028
这个issue属于bug报告类型，主要涉及TensorRT-LLM的编译问题。由于环境配置或脚本错误导致编译失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/1027
这是一个关于需求的问题，用户希望在TensorRT-LLM中支持pipeline parallelism和tensor parallelism的组合。

https://github.com/NVIDIA/TensorRT-LLM/issues/1026
这是一个Bug报告，用户在TensorRT-LLM下遇到了"RuntimeError: Unsupported model architecture: LlamaForCausalLM"的问题。原因可能是模型架构不受支持，导致出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1025
这是一个bug报告，主要涉及的对象是TensorRT-LLM的CUDA内核。原因是未检查cudaFuncAttributeMaxDynamicSharedMemorySize的返回值，导致当其失败时，CUDA内核仍然尝试使用共享内存，从而导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1024
这是一个关于兼容性和性能问题的用户需求报告，主要涉及到T4 GPU在运行LLava时遇到内存不足错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1023
这是一个关于功能差异的问题，主要涉及TensorRT-LLM的In-flight Batching功能缺失和具体版本的支持情况，用户怀疑在不同版本中的功能差异是否正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/1022
这是一个bug报告，主要涉及到TensorRT-LLM中Blip2下的`decode_batch`方法的参数传递问题，由于参数传递错误导致了TypeError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1021
这是一个bug报告，主要涉及MPT在TensorRT-LLM下的适配问题，由于当前源代码不支持MPT，导致用户在特定环境下使用时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/1019
这个issue是关于更新TensorRT-LLM，并涉及bug修复、功能更新以及API迁移，主要涉及的对象是TensorRT-LLM这个项目。

https://github.com/NVIDIA/TensorRT-LLM/issues/1018
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM中的AWQ功能。该问题由于world_size大于1时执行AWQ失败，但world_size为1时成功，可能是由于多GPU环境下的某些配置或逻辑导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1017
这个issue是一个bug报告，涉及到TensorRT-LLM项目中的一个依赖错误，导致无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/1016
这是一个bug报告，涉及的主要对象是TensorRT-LLM。该问题可能是由于授权问题导致无法运行转换后的模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/1015
这是一个bug报告，主要涉及TensorRT-LLM下的API使用错误。由于使用了TensorRTLLM主分支，在example/llama中的示例尝试推理时出现异常行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/1014
这是一个修复拼写错误（typo）的问题，主要涉及文本内容。由于拼写错误导致了错误的表述，需要修正以提高文档准确性。

https://github.com/NVIDIA/TensorRT-LLM/issues/1013
这是一个Bug报告，涉及对象为在Python 3.10环境下导入`tensorrt_llm`包时出现的ImportError和OSError。导致该问题的原因可能是缺少依赖关系和共享对象文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/1012
这个issue是关于用户请求指南或优化提示，涉及优化KV缓存使用与inflight batcher，由于缺乏关于处理inflight批次设置的文档，导致KV缓存块数量停止增加，用户寻求模型服务器优化建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/1011
这是一个关于bug报告的issue，主要涉及到TensorRT-LLM的GPU内存使用。这个问题是由于int8使用的GPU内存高于fp16引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1010
这是一个bug报告，问题涉及`--fp8_kv_cache`选项中帮助信息的拼写错误导致与`--int8_kv_cache`选项无法正确区分。

https://github.com/NVIDIA/TensorRT-LLM/issues/1009
这是一个文档更新的issue，主要对象是TensorRT-LLM，由于没有具体的内容说明，无法分析具体原因或者提出的问题或需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/1008
这是一个bug报告，涉及主要对象为TensorRT-LLM安装过程。这个问题是因为在导入tensorrt_llm时出现了undefined symbol导致的ImportError。

https://github.com/NVIDIA/TensorRT-LLM/issues/1007
这个issue类型是功能请求，主要涉及的对象是Mixtral支持。由于TensorRT-LLM目前不支持AWQ with AMMO，导致用户无法使用该功能，用户希望能够跟踪该功能的支持情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/1006
这是一个bug报告，涉及的主要对象是TensorRT-LLM项目下的decoderMaskedMultiheadAttention相关代码文件。这个问题是由于函数命名不一致导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/1005
这是一个用户提出需求的问题，主要涉及如何获取模型的hidden_state值。由于TensorRT-LLM在构建后是静态的，用户想要获取模型的中间结果，但启用debug输出会导致推断变得非常缓慢。

https://github.com/NVIDIA/TensorRT-LLM/issues/1004
这是一个bug报告，涉及的主要对象是在TensorRT-LLM中使用Docker出现错误响应。由于配置参数错误或者Docker环境问题导致无法成功进入Docker。

https://github.com/NVIDIA/TensorRT-LLM/issues/1003
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中使用多GPU运行llava时出现的问题，可能是由于多GPU配置或程序逻辑出现错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/1002
这是一个bug报告，涉及到在TensorRT-LLM中使用int8_kv_cache时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/1001
这是一个用户提出需求的issue，主要涉及TensorRT-LLM的Windows设置脚本，用户指出运行脚本需要先导航到克隆的存储库。因此，用户在寻求如何运行Windows设置脚本的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/1000
这是一个bug报告，问题涉及TensorRT-LLM中进行抽样时出现结果相同的情况。导致这一问题的原因是默认情况下随机种子为None，因此导致每次抽样结果相同。

https://github.com/NVIDIA/TensorRT-LLM/issues/999
这是一个Bug报告类型的issue，主要涉及对象是在TensorRT-LLM中运行使用多GPU的问题。导致这个问题的原因可能涉及到使用多GPU时的配置或代码实现上的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/998
这是一个Bug报告，主要涉及的对象是BERT model在TensorRT-LLM中的转换问题。由于Myelin优化过程中的一个错误导致模型无法成功转换成ONNX模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/997
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在运行时出现segment fault和signal code 1的问题。原因可能是与使用fp16有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/996
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的模型构建器（Builder），由于构建器是在不同设备上创建，导致了内部错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/995
这个issue属于bug报告类型，主要涉及TensorRT-LLM中运行sqlcoder时出现的MPI_Init_thread错误。可能由于模型本身导致了此错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/994
这是一个关于构建错误的bug报告，涉及主要对象是TensorRT-LLM项目。由于某些原因导致了`make C docker release_build`命令失败并显示错误消息"make: *** [Make:55: release_build] Error 255"。

https://github.com/NVIDIA/TensorRT-LLM/issues/993
这是一个bug报告，主要涉及TensorRT-LLM中的Int8_kv_cache功能。产生此问题可能是由于操作符或驱动程序的问题导致无法在Python中捕获错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/992
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的enc_dec模型存在的一个bug。问题的症状是当模型中同时使用了cross_attention和weight_only_gemm_plugin时会导致构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/991
这个issue类型是用户提出问题，涉及的主要对象是关于TensorRT-LLM下的gpt_attention模块的输入参数。由于用户需要确认关于输入参数和实现Qwen's lognscaling插件的细节，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/990
这是一个bug报告，主要涉及TensorRT-LLM中使用torch.profiler.profile获取每个组件性能的问题。由于CUPTI初始化失败导致缺失CUDA性能分析器活动。

https://github.com/NVIDIA/TensorRT-LLM/issues/989
这是一个关于缺少Alltoall功能的Bug报告，涉及主要对象是TensorRT-LLM中的MoE实现，用户提出缺少Alltoall操作可能导致专家并行性不足的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/988
这是一个bug报告，主要涉及TensorRT-LLM下使用MMLU和Llama2-7b量化时在GPTQ group_size为32时无法运行的问题。由于未能成功运行上述操作的命令，用户寻求帮助以解决此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/987
这是一个bug报告，涉及主要对象为TensorRT-LLM项目中的end_id设置问题，导致无法在正确的位置停止。

https://github.com/NVIDIA/TensorRT-LLM/issues/986
这是一个 Bug 报告类型的 issue，主要涉及 Layernorm plugin 在 TensorRT-LLM 中即将被弃用的问题。用户询问由于插件即将被弃用，将会使用哪种 Layernorm kernel 替代。

https://github.com/NVIDIA/TensorRT-LLM/issues/985
这是一个功能需求，主要涉及添加T5模型的权重量化功能。导致问题的原因是现有的HF权重加载机制可能不适用，需要修复以使权重量化功能能够在生产中发挥作用。

https://github.com/NVIDIA/TensorRT-LLM/issues/984
这是一个关于如何在同一进程/服务器中提供多个TensorRT-LLM模型的问题，涉及的主要对象是TensorRT-LLM模型的服务方式。由于TensorRTLLM库中存在一些限制，用户在尝试并发运行多个模型时遇到了错误，因此需要了解如何正确地为多个模型提供服务。

https://github.com/NVIDIA/TensorRT-LLM/issues/983
这是关于缺失输出 log probabilities 的问题，属于用户提出需求或者询问问题的类型，主要涉及 SamplingConfig 和返回字典中的 log probabilities 缺失。由于 TensorRT-LLM 在返回字典中缺少 log probabilities，用户想知道是否有其他方法可以获取 log probabilities。

https://github.com/NVIDIA/TensorRT-LLM/issues/982
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的summarize.py脚本。由于baichuan模型不支持BetterTransformer，导致summarize.py在遇到该情况时会自动终止。

https://github.com/NVIDIA/TensorRT-LLM/issues/981
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中使用GptSession时出现的错误。这个问题是由于模型中存在启用和禁用GPT插件的层导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/980
这个issue是关于用户提出需求的类型，主要涉及TensorRT-LLM下的两个文件中关于优化配置的问题。由于用户对于如何通过设置`mRuntime.addContext`中的值来优化性能提出了疑问，可能是因为用户想要了解在上下文和生成阶段中FLOPs百分比受到什么控制。

https://github.com/NVIDIA/TensorRT-LLM/issues/979
这是一个Bug报告，涉及的主要对象是在TensorRT-LLM下使用smoothquant LLaMa2-70B模型时出现构建失败。这个问题可能是由于量化后在构建过程中出现错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/978
这是一个bug报告类型的issue，涉及的主要对象是在使用TensorRT-LLM时构建engine失败。由于CUDA版本不兼容所导致，用户在构建engine时遇到了AssertionError。

https://github.com/NVIDIA/TensorRT-LLM/issues/977
这是一个关于TensorRT-LLM中max_batch_size和max_input_len参数关系的问题。用户提出了对于请求长度大于设定值直接返回错误的情况下，需求处理超出长度请求而不截断的方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/976
该issue属于功能增强类型，主要针对TensorRT-LLM中引擎缓存的效率进行优化，主要涉及到引擎编译缓存机制的改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/975
这个issue属于用户提出需求类型，主要涉及TensorRT-LLM下的优化问题，用户希望在Context阶段缓存预先确定的提示词组K和V，以提高产品关联性检测效率。

https://github.com/NVIDIA/TensorRT-LLM/issues/974
这是一个bug报告，涉及对象为cuDNN版本不匹配。由于TensorRT与cuDNN版本不一致导致性能降低。

https://github.com/NVIDIA/TensorRT-LLM/issues/973
这是一个bug报告，主要涉及TensorRT-LLM中的streaming mode无法正常工作，用户希望能够在streaming模式下输出tokens，但实际上会引发错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/972
该issue属于bug报告类型，主要涉及TensorRT-LLM下的qwenvl在Tesla A10设备上运行时出现的问题。出现问题的原因可能是导致OOM（out of memory）的内存不足，用户提出了一些修改和解决方案来解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/971
该issue是一个需求提出类型的问题，主要涉及到如何在TensorRT-LLM中获取隐藏状态（hidden_states）。由于作者只简单提到了标题内容，故无法确定具体的问题原因或帮助需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/970
这个issue属于用户提出需求，询问TensorRT-LLM是否支持facebook/nlgb-200-3.3B，问题涉及主要对象是TensorRT-LLM。由于缺乏详细信息，用户提出了关于TensorRT-LLM是否支持特定模型的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/969
这是一个关于debug的bug报告，主要涉及TensorRT-LLM中的插件代码调试输出不成功的问题。通过在cpp文件中添加printf语句进行调试时，未能在终端输出"hello world"，可能是由于未成功编译或安装导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/968
这是一个功能需求类型的issue，主要涉及对象是TensorRT-LLM的模块（Module），用户提出需要添加打印模块的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/967
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的模型性能问题，导致使用int8-kv-cache + per-channel-int8-weight方式得到的结果比预期精度低。

https://github.com/NVIDIA/TensorRT-LLM/issues/966
这个issue是关于一个bug报告，主要涉及的对象是使用deepseek-6.7b模型推理时得到的结果为"\n\n\n"，可能是由于deepseek模型还未被支持导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/965
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM中的VLLM模型。由于测试结果显示TensorrtLLM的响应时间比VLLM0.2.7长，用户在寻求关于为什么TensorrtLLM比VLLM慢以及如何提高性能的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/964
这是一个关于潜在影响量化模型准确性的bug报告，涉及到TensorRT-LLM中线性层合并问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/963
这是一个关于bug报告的issue，主要涉及NVIDIA GeForce RTX 3090在部署13B LLaMA-V2时遇到的错误。原因可能是兼容性问题导致的运行时错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/962
这个issue是关于代码错误的bug报告，主要涉及TensorRT-LLM中的int4-awq功能，问题出现在vocab_size不一致导致引擎构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/961
这是一个bug报告，涉及的主要对象是在TensorRT-LLM下的FP8运行失败。导致这一问题的原因可能是使用Vicuna 13B V15构建引擎时引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/960
这是一个bug报告，涉及主要对象为TensorRT-LLM，由于缺少名为`utils.utils`的模块，导致无法成功运行QwenVL例子。

https://github.com/NVIDIA/TensorRT-LLM/issues/959
这个issue属于bug报告类型，主要涉及TensorRT-LLM中fp8运行失败的问题，可能是由于特定提交导致的运行错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/958
这个issue是关于bug报告，涉及神经网络模型Qwen14B在长提示下结果与另一个模型hf不同的问题，可能由于模型构建或实现细节导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/957
这是一个关于性能问题的bug报告，涉及主要对象是TensorRT-LLM中的layernorm kernel。可能由于代码实现或优化问题导致了layernorm运行速度比torch慢。

https://github.com/NVIDIA/TensorRT-LLM/issues/956
这是一个bug报告，主要涉及TensorRT-LLM中的inflight batcher，由于修改了预处理模型后，在并发发送请求时出现assert错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/955
这个issue是一个bug报告，主要涉及的对象是CodeLlama7B模型的配置文件和编译过程中的数据不匹配所导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/954
这是一个Bug报告，主要涉及到TensorRT-LLM在两个不同GPU上同时服务模型导致延迟增加的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/953
这个issue是一个bug报告，涉及的主要对象是NCCL errors导致LLAMA2 70b benchmark在4个H100 GPUs上运行时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/952
This is a bug report regarding an error that occurs during building the llama model in the TensorRT-LLM repository.

https://github.com/NVIDIA/TensorRT-LLM/issues/951
这是一个bug报告，涉及的主要对象是MBartForCausalLM模型的转换和构建过程，由于转换和构建过程中可能出现的错误，用户提出了需要帮助的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/950
这是一个bug报告，主要涉及了TensorRT-LLM中的"gptManagerBenchmark"工具，用户在反馈中提到了关于--max_num_sequences选项不存在的问题，以及关于max_batch_size对性能影响的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/949
这个issue是一个bug报告，主要涉及TensorRT-LLM项目中SmoothQuant功能的错误，由于缺少`medusa_packed_mask`和`medusa_position_offsets`参数导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/948
这是一个bug报告，涉及到TensorRT-LLM使用int8 kvcache时出现的"RuntimeWarning: overflow encountered in multiply"警告问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/947
这是一个bug报告，涉及主要对象是为starcoder添加gelu_pytorch_tanh，问题可能是由于缺少这个功能导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/946
这个issue是关于一个bug报告，涉及的主要对象是TensorRT-LLM下的gelu_pytorch_tanh激活函数。由于TensorRT-LLM没有适配starcoder模型的问题导致了这个bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/945
这个issue类型是用户提出需求，主要对象是TensorRT-LLM的smoothquant功能。由于`int8_sq_per_tensor`模式表现异常，用户在测试中发现了准确性比较问题，并询问llama系列模型是否支持在TensorRT-LLM中的pertensor smooth quant。

https://github.com/NVIDIA/TensorRT-LLM/issues/944
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的模型转换问题。原因是在运行转换模型的脚本时发生了错误，导致无法成功将模型转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/942
这是一个bug报告，主要涉及TensorRT-LLM下使用BLOOM 560M模型构建时的失败情况，由于用户在执行README中的步骤时出现了问题导致模型构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/941
该issue属于功能更新，涉及TensorRT-LLM模型支持、特性、API、bug修复、性能和文档更新，原因是需要增加新的功能和改进现有功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/940
这个issue属于bug报告类型，主要涉及TensorRTLLM在TP>1时ModelLoader._load_model_runner()方法的问题。由于在引擎初始化过程中，信息未传递给子进程，导致程序出现异常状况。

https://github.com/NVIDIA/TensorRT-LLM/issues/939
这是一个用户提出需求的issue，主要涉及TensorRT对JAIS模型的转换，用户询问如何处理SwiGlu权重的相应偏置。

https://github.com/NVIDIA/TensorRT-LLM/issues/938
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于构建PEFT训练的Llama模型时出现了错误，导致无法创建Llama Engine。

https://github.com/NVIDIA/TensorRT-LLM/issues/937
这是一个用户提出需求的类型，主要对象是是否计划支持Yuan2.0模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/936
这个issue是关于代码缺失的bug报告，涉及的主要对象是tensorrt_llm.models.llama.weight模块。由于在Docker容器中缺少weight.py文件导致出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/935
这个issue类型是bug报告，主要涉及的对象是int8 gemm的性能在A100上比fp16慢。具体原因可能是代码实现中存在性能瓶颈导致不稳定的计算速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/934
这是一个bug报告，涉及主要对象是在运行sqlcoder时出现的MPI_Init_thread错误。该问题可能是由于JSON文件中缺少某些必要参数导致，从而引发了MPI初始化错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/933
这是一个关于如何获取BERT模型的分类标签的问题。该问题涉及的主要对象是TensorRT-LLM下的BERT模型。由于修改了build.py和run.py文件后，在运行时输出的张量形状与预期不符，导致了输出不符合预期的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/932
这是一个关于调试问题的issue，主要涉及TensorRT-LLM转换模型过程中逐层调试结果以确定问题存在的主体。可能是由于模型转换过程中的某些错误导致的问题，用户希望了解如何逐层调试以找出问题所在。

https://github.com/NVIDIA/TensorRT-LLM/issues/931
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的一个节点实现问题，由于某些实施缺失导致了报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/930
这个issue类型是用户提出需求，请教问题，主要涉及对象是TensorRT和TensorRT-LLM，由于用户想了解原始TensorRT API和TensorRT-LLM之间的区别及其性能表现。

https://github.com/NVIDIA/TensorRT-LLM/issues/929
这是一个bug报告，涉及TensorRT-LLM下的Int4 AWQ模型保存问题。由于未知原因导致了CC(LLaMA 7B AWQ Quantaization)问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/928
这个issue是关于如何在运行LLaVA中使用batchsize的问题，类型是bug报告，主要涉及对象是LLaVA的batchsize设置问题，由于设置了batch size为32却出现了错误，导致代码无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/927
这是一个bug报告类型的issue， 主要涉及TensorRT-LLM在Windows PC上运行server示例时遇到的问题。 问题可能由于环境配置问题或代码适配性导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/926
这是一个用户需求类型的issue，主要对象是GptSession，用户关注是否多个线程能够共享GptSession并同时调用GptSession::generate()。可能由于服务需要并发处理请求，用户在询问多线程下的使用情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/925
这是一个bug报告类型的issue，主要涉及的对象是Layernorm plugin。由于有关deprecating layernorm plugin的警告，用户在询问TensorRTLLM是否将不再使用layernorm kernel并直接在模型中使用layernorm kernel。

https://github.com/NVIDIA/TensorRT-LLM/issues/924
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM下的Mixtral-8x7b-instruct-0.1模型构建过程。产生这个bug的原因是调用 LoraConfig.from_hf() 函数缺少了一个必需的参数 'trtllm_modules_to_hf_modules'。

https://github.com/NVIDIA/TensorRT-LLM/issues/923
这是一个bug报告，涉及主要对象是TensorRT-LLM下的paralle build example，导致错误的原因是`hf instance`未传递给`torch.mp`。

https://github.com/NVIDIA/TensorRT-LLM/issues/922
这是一个bug报告，涉及主要对象为TensorRT-LLM。由于在转换权重过程中出现了"NotImplementedError: Cannot copy out of meta tensor; no data!"错误，可能是由于数据复制操作无法对元数据张量进行复制所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/921
这是一个bug报告类型的issue，涉及TensorRT-LLM中找不到节点实现，用户提出由于内存不足导致错误并询问如何减少内存使用问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/920
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下的pypi包，用户希望预构建的包可以支持所有GPU架构。这个问题的原因可能是预构建包不包括T4 GPU的架构导致无法使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/919
这个issue是一个bug报告，主要涉及TensorRT-LLM在模块"transformers_modules"上出现的ModuleNotFoundError错误。原因可能是缺少相应的模块或路径配置错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/918
这是一个bug报告，用户在TensorRT LLM 0.7.1下进行推断时出现了失败的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/917
这是一个bug报告类型的问题，主要涉及的对象是GptManager。由于GptManager的初始化导致CPU占用率过高，即使没有请求，这可能是导致问题的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/916
这是一个其他类型的issue，主要对象是代码基础设施。这个问题可能是由于作者或维护者敲错了单词导致标题为空，需要进一步澄清或修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/915
这是一个用户提出需求的issue，主要涉及到TensorRT-LLM的内存需求信息缺失。

https://github.com/NVIDIA/TensorRT-LLM/issues/914
这是一个bug报告，主要涉及TensorRTLLM的构建过程中出现错误的问题。可能是由于nvidiaammo安装错误导致的构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/913
这是一个关于如何在triton中部署Llava的问题，属于用户寻求帮助类型的issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/912
这是一个bug报告，主要涉及TensorRT-LLM的模型运行时引发异常，原因是在多GPU部署时执行上下文步骤时出现GEMM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/911
该issue属于用户提出需求类型，主要涉及TensorRT-LLM的请求推理API以及是否支持在单个上下文中获取多个结果，用户询问如何实现获取多个结果的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/910
这是一个用户提出需求的issue，主要涉及的对象是Nvidia GeForce RTX 4070 GPU以及TensorRT-LLM中的Llama 7b模型。由于GPU VRAM（显存）仅有8GB，导致加载Llama 7b模型时出现错误，用户在寻求建议使用哪个模型来解决问题，或者是否有其他方式可以在RTX 4070 GPU上加载Llama 7b模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/909
这是一个用户提出需求的 issue，主要涉及到将 qwen model 支持 use_paged_context_fmha 特性，并使 enable_kv_cache 在 triton server 中可用。

https://github.com/NVIDIA/TensorRT-LLM/issues/908
这是一个bug报告，主要涉及到在编译源码时遇到了无法连接到GitHub下载googletest的问题，导致了连接超时的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/907
这是一个关于需求的问题，主要对象是TensorRT-LLM中的inflight_batching功能。询问直接从TensorRTLLM中是否可以进行inflight_batching，而不是使用triton，并提及了相关的测试脚本test_gpt_manager.py。

https://github.com/NVIDIA/TensorRT-LLM/issues/906
这是一个功能需求提问，用户关注的主要对象是TensorRT-LLM下的gptSessionBenchmark benchmark工具。由于缺少关于prompt text作为输入的支持说明，用户想知道gptSessionBenchmark benchmark工具是否支持这样的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/905
这是一个Bug报告，涉及TensorRT-LLM下的服务器示例无法正常运行，可能是由于代码中引用的模块或文件错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/904
这是一个bug报告，涉及TensorRT-LLM中的一个错误。用户在使用`build.py`脚本生成文本时，请求输出长度为11时出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/903
这是一个用户提出需求的问题单，主要涉及TensorRT-LLM模型加载的不同精度。用户想要了解在相同精度下是否可以加载不同精度的模型或者是否需要为不同精度编译不同的引擎文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/902
这是一个bug报告，涉及的主要对象是在进行TF weights到TensorRT LLTM权重转换时出现了问题，导致运行模型时生成了乱码。

https://github.com/NVIDIA/TensorRT-LLM/issues/901
这是一个bug报告，主要涉及TensorRT-LLM库中的AWQ量化功能，由于导入cuda_ext时出现了错误，导致出现了'NoneType' object has no attribute 'fake_tensor_quant_with_axis'的属性错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/900
这是一个bug报告，该问题单主要涉及的对象是软件中的拼写错误。这个问题是由于拼写错误导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/899
这是一个bug报告，涉及主要对象为TensorRT-LLM下的smoothquant功能。由于部分Tensor shape mismatch导致引擎构建失败，用户寻求帮助解决此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/898
这是一个bug报告，涉及主要对象是TensorRT-LLM，问题出现的原因是缺少'tensorrt_llm.bindings'模块导致了模块未找到的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/897
这是一个关于如何在构建引擎成功后运行GPT2模型的问题，主要涉及TensorRT-LLM下的使用。由于旧版本引擎构建问题导致的bug或用户寻求如何有效地在工作流程中使用引擎和测量推理速度的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/896
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的构建过程中出现的错误提示信息，可能是由于硬件不支持TF32导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/895
这个issue属于bug报告类型，主要涉及TensorRT-LLM中smoothquant功能的tensor shape mismatch问题。由于加载文件时出现了tensor shape mismatch导致报错，用户寻求关于smoothquant在4GPU运行时出错的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/894
这是一个bug报告，涉及TensorRT-LLM下的"Assertion failed: Number of bytes for rows and cols must be a multiple of 32"错误。由于rows和cols的字节数不能被32整除，导致了这个问题的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/893
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的Benchmarking功能。由于未定义的符号 CHECK_DEBUG_ENABLED 导致了基准测试相关的cpp无法正确运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/891
这是一个更新TensorRT-LLM的issue，主要涉及到特征更新、性能优化和文档更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/890
这是一个bug报告，主要涉及到在Python-Runtime中使用IA3的问题。由于某种原因导致了无法正常使用IA3，用户希望得到相关问题的解决帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/889
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下使用int8_kv_cache时Yi6B模型表现不佳的问题。原因可能是int8_kv_cache设置导致了模型结果明显变差。

https://github.com/NVIDIA/TensorRT-LLM/issues/888
这是一个用户对如何使用TensorRT加速基于Bloom560m的Reward Model的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/887
这是一个bug报告，主要对象是T5模型在TensorRT-LLM中缺少配置文件导致构建引擎时遇到错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/886
这个issue是一个bug报告，主要涉及TensorRT-LLM在Windows WSL2 docker环境中崩溃并引发内存不足错误，尽管有足够的空闲VRAM。

https://github.com/NVIDIA/TensorRT-LLM/issues/885
这个issue类型为bug报告，涉及主要对象是TensorRT-LLM下的Quantization模块，可能由于模型结构中带有bias导致了quantization错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/884
这是一个bug报告，涉及到TensorRT-LLM下的to_word_list_format函数的问题。由于函数在某些情况下无法顺利停止生成文本，导致生成了意外结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/883
这是一个bug报告，问题涉及TensorRT-LLM的使用，由于Runtime execution failed导致推理过程出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/882
这是一个bug报告，主要涉及TensorRT-LLM中的并行化问题。用户遇到的问题来源于对脚本运行时出现的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/881
这是一个Bug报告，涉及主要对象为Triton及Mistral 7B。该问题由于`Segmentation fault: invalid permissions for mapped object`错误导致程序崩溃，用户寻求关于Mistral7Bv0.1在NVIDIA RTX 6000 Ada Generation上运行Triton的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/880
这个issue是用户提出需求，主要对象是在TensorRT-LLM中支持文本和图像嵌入模型，用户询问是否有计划未来支持类似CLIP或sentencetransformers等模型，并询问如何能够实现对这些模型的支持，以加速嵌入模型应用在类似ChromaDB这样的场景中。

https://github.com/NVIDIA/TensorRT-LLM/issues/879
这是一个关于功能疑问的issue，主要对象是TensorRT-LLM中`generation_logits`的输出行为，问题出现的原因可能是在使用`top_p`参数时，`generation_logits`输出的值是否应该存储概率值而不是对数概率值。

https://github.com/NVIDIA/TensorRT-LLM/issues/878
这个issue是关于用户提出需求的问题，主要涉及的对象是`batch_manager`源代码。由于源代码未开源，用户提出了请求将`batch_manager`源代码推送到该repo的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/877
这是一个bug报告，涉及TensorRT-LLM在docker容器中无法运行的问题，由于缺少`ibnvinfer_plugin_tensorrt_llm.so`文件导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/876
这是一个请求指南或演示的issue，涉及到TensorRT-LLM在Mistral 7b上的并行性和部署。

https://github.com/NVIDIA/TensorRT-LLM/issues/875
这是一个Bug报告，主要涉及Mistral/Mixtral模型的基准测试，问题是由于输入数据维度不一致导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/874
这是一个Bug报告，涉及到LLaMAForCausalLM初始化时出现了意外的关键字参数问题。由于初始化时传入了未知的关键字参数'enable_pos_shift'，导致了TypeError异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/873
这是一个bug报告，主要对象是LLaVA项目中的multimodal示例。这个问题是由于一个小的拼写错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/872
这是一个bug报告，涉及的主要对象是TensorRT-LLM和tensorrtllm_backend，用户提出了关于Mixtral模型量化的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/871
这是一个bug报告，涉及的主要对象是在构建GPT2过程中出现错误。由于可能出现的问题或错误，用户遇到了一个名为"'GPTLMHeadModel' object has no attribute 'position_embedding'"的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/870
这是一个用户提出需求的issue，涉及主要对象是如何在llama模型中使用嵌入作为输入。由于用户想要根据trtllm自定义输入和输出，导致需要更好的方法来实现此功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/869
这是一个bug报告，主要涉及TensorRT-LLM下的llama13b模型在多GPU多批次大小下生成异常结果的问题。导致该问题的原因是在特定环境和参数设置下出现异常结果，但单批次运行时结果正常。

https://github.com/NVIDIA/TensorRT-LLM/issues/868
这是一个用户提出需求的 issue，主要涉及的对象是 GptManager。由于 GptManager 不够灵活，无法解决一些问题，用户希望使其更加可配置和灵活。

https://github.com/NVIDIA/TensorRT-LLM/issues/867
这是一个Bug报告类型的Issue，主要涉及TensorRT-LLM的安装问题，导致了与CUDA 12.3不兼容的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/866
这是一个bug报告，主要涉及TensorRT-LLM的GPTQ quant build engines过程出现了AssertionError。由于不明原因导致加载权重时出现了错误并警告TypedStorage即将被弃用的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/865
这是一个bug报告，主要涉及的对象是TensorRT-LLM中设置penalty时系统hang的问题，可能由于高并发情况导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/864
这是一个用户询问问题的issue， 主要涉及的对象是TensorRT-LLM中的"enableBlockReuse"配置，用户想知道如何将其设置为True。

https://github.com/NVIDIA/TensorRT-LLM/issues/863
这是一个bug报告，主要涉及到TensorRT-LLM下的build脚本运行时出现的CUDA runtime error导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/862
这是一个bug报告，涉及主要对象为TensorRT-LLM中的Triton推理服务，由于参数数值错误导致数值处理异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/861
这是一个包含技术问题的issue，主要围绕TensorRT-LLM中的"get_visual_features"函数和"prompt_table"参数展开讨论，用户提出了关于GPU计算和参数相关问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/860
这是一个关于环境要求的问题，用户询问关于TensorRT-LLM安装的详细指南和CUDA版本要求。

https://github.com/NVIDIA/TensorRT-LLM/issues/859
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的cutlass preprocessors模块。这个问题很可能是由于拼写错误导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/858
这是一个bug报告类型的issue，涉及到TensorRT-LLM下的Transformers版本冲突问题，导致在运行build_visual_engine.py时报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/857
这是一个bug报告类型的issue，主要涉及的对象是"perf_best_practices.md"。由于拼写错误，导致了需要修复的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/856
这个issue属于bug报告类型，主要涉及Deepseek模型的支持问题。导致这个问题的原因可能是模型输出异常，需要进一步调查解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/855
这是一个bug报告，问题涉及到TensorRT-LLM中的quantized int8kv cache。由于头部大小（headsize）不是8的倍数，导致出现Assertion failed错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/854
这是一个关于bug报告的issue，涉及主要对象是TensorRT-LLM下的Yi6Bchat模型。导致结果异常的原因是在使用`gather_all_token_logits=False`时导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/853
这是一个关于用户提出需求的类型的问题，主要涉及支持 DeciLM7Binstruct 的可能性。用户可能由于想了解是否将来会支持 DeciLM7Binstruct 而提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/852
这是一个bug报告，涉及TensorRT-LLM下的失败构建引擎的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/851
这是一个bug报告，主要涉及TensorRT-LLM在版本0.5.0和0.7.1之间的性能回归，用户希望找出导致这种回归的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/850
这个issue类型是bug报告，主要对象是TensorRT-LLM的构建过程。导致这个问题可能是由于构建过程中的错误，导致无法成功生成二进制文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/849
这是一个用户提出需求的issue，主要涉及的对象是深度学习模型优化工具TensorRT-LLM下的Mixtral Offloading功能。用户希望该工具可以支持一种新的缓存技术，以便在较小的GPU上运行时提高性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/848
这是一个用户提出需求的类型为feature request的issue，该问题单涉及的主要对象是TensorRT-LLM的HLAPI模块。由于用户希望在HLAPI的示例中添加构建命令，可能是为了方便使用和测试，因此提出了这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/846
这个issue类型是功能需求提出，主要涉及到对TensorRT-LLM模型更新和改进。由于功能要求涉及到添加新的模态模型支持、API增强、性能优化等，用户希望获得更多功能的支持和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/845
这是一个关于技术问题的提问，涉及到TensorRT-LLM中关于Smooth quant int8 gemm的处理过程中的疑问，主要涉及对象为int8 gemm输出的fake quant操作，用户询问是否这一步骤是必要的。

https://github.com/NVIDIA/TensorRT-LLM/issues/844
这是一个bug报告，主要涉及到TensorRT-LLM中benchmark测试时所有激活值为零的问题。造成这种症状的原因可能是参数配置或代码逻辑问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/843
这个issue是bug报告，涉及的主要对象是LLAMA-2-70B AWQ模型构建过程。由于输入行列的字节数必须是32的倍数，而此处的行字节数为4096、列字节数为2000，导致了该断言错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/842
这是一个bug报告，主要涉及TensorRT-LLM下的Mixtral引擎构建过程中使用'load_by_shard'选项时出现错误。Issue中提到无法使用'load_by_shard'选项，指示矛盾，并导致内存占用过大。

https://github.com/NVIDIA/TensorRT-LLM/issues/841
这是一个bug报告，该问题涉及的主要对象是TensorRT-LLM中的模型构建过程。由于内存不足导致构建引擎失败，提示需要增加工作空间大小以解决问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/840
这是一个关于构建TensorRT-LLM代码在Python中出现问题的bug报告。用户在尝试在Google Colab环境上设置TensorRT-LLM时，在运行特定代码后遇到错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/839
这是一个关于TensorRT-LLM下的性能问题，用户提出了关于GPU内存占用和推理批大小的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/838
这是一个bug报告，该问题涉及TensorRT-LLM下的模型推理引擎出错，可能是由于未正确标记输出张量以及启用调试输出导致推理时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/837
这是一个用户提出问题的issue，主要涉及到SmoothQuant在tp>1情况下的支持，由于Linear hook在tp>1时出现问题导致模型运行时内存溢出。

https://github.com/NVIDIA/TensorRT-LLM/issues/836
这是一个功能需求类型的issue，主要涉及的对象是Qwen模型中的logn-scaling attention功能。由于TensorRT-LLM实现的Qwen模型目前不支持logn-scaling，导致输出质量较低，用户提出需要在实现中加入这一功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/835
这个issue类型属于用户提出需求，主要涉及对象是在TensorRT-LLM下构建和在不同型号GPU上运行引擎的需求。提问者想知道是否能够在一个GPU上构建引擎，然后在另一个不同型号的GPU上加载和运行该引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/834
这是一个bug报告，主要涉及解决TensorRT-LLM中构建模型失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/833
这个issue属于用户提出问题类型，主要涉及对象是GptManager和GptSession之间的关系。原因是用户想了解GptManager和GptSession之间的关系以及GptManager是否使用GptSession来管理和调用TensorRT引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/832
这个issue是一个bug报告，涉及的主要对象是INT8量化范围。由于INT8量化范围为[-127,127]而不是[-128,127]，导致用户提出如何更改范围以提高模型准确性的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/831
这是一个bug报告，涉及的主要对象是TensorRT-LLM项目中的方法"get_engine_version"的缺失。原因是运行文件"examples/run.py"时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/830
这是一个用户询问类型的issue，询问了TensorRT-LLM中-这两种MultiHeadAttention的不同之处。

https://github.com/NVIDIA/TensorRT-LLM/issues/829
这是一个bug报告，该问题涉及TensorRT-LLM的stop_words功能，在设置多个stop_words时只有第一个会生效。

https://github.com/NVIDIA/TensorRT-LLM/issues/828
这是一个bug报告，主要涉及到在构建TensorRT-LLM时出现了模块未找到的错误。问题的原因可能是缺少特定的模块或依赖。

https://github.com/NVIDIA/TensorRT-LLM/issues/827
这个issue类型是bug报告，主要涉及到在使用多个GPU时系统hang的问题，可能是由于多GPU环境下的某些设置导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/826
这是一个关于构建benchmark的bug报告，主要涉及到TensorRT-LLM项目的构建脚本。由于依赖项已经安装，可能由于环境配置或依赖版本冲突导致无法构建benchmark。

https://github.com/NVIDIA/TensorRT-LLM/issues/825
这个issue属于bug报告类型， 主要涉及的对象是Mixtral-8x7b模型，问题可能由于程序中使用了未初始化的对象导致了'NoneType' object has no attribute 'trt_tensor'的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/824
该issue是关于bug报告，主要涉及的对象是Tensorrt-LLM的安装过程。由于安装Tensorrt-LLM python包时出现错误，用户寻求帮助解决安装问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/823
这个issue属于bug报告类型，主要涉及TensorRT-LLM下构建长输入长度Llama模型时出现的内存溢出错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/822
这是一个关于"t5-large speedup is low"的bug报告，涉及主要对象是TensorRT-LLM中的t5large模型运行速度问题。由于缺乏预热效应，导致在完整语料上运行速度提升较低，用户希望得到解释和帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/821
这是一个bug报告，主要涉及TensorRT-LLM下的chatglm3_6b模型，在使用虚拟权重构建模型时出现错误，原因是模型仍需要模型检查点才能成功构建。

https://github.com/NVIDIA/TensorRT-LLM/issues/820
这是一个bug报告，主要问题涉及TensorRT-LLM在Windows 11编译时出现的链接错误。由于导出函数的重复定义而导致链接失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/819
这是一个Bug报告，涉及主要对象是TensorRT-LLM中的参数request_output_len。由于request_output_len的不同取值会影响系统的吞吐量，可能导致输出速度异常波动，用户在询问是否存在配置问题或者是批处理问题造成的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/818
这是一个bug报告类型的issue，主要涉及到在使用Python benchmark时遇到的错误，可能由于TensorRTLLM版本升级至v0.7.0导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/817
这是一个Bug报告，主要涉及的对象是TensorRT-LLM下的权重比例计算bug，导致性能下降3~10%。

https://github.com/NVIDIA/TensorRT-LLM/issues/816
这是一个寻求帮助的问题，主要涉及TensorRT-LLM中的Context Phase和Generation Phase概念定义不明确，导致用户困惑并寻求标准定义。

https://github.com/NVIDIA/TensorRT-LLM/issues/815
这是一个bug报告，主要涉及的对象是在TensorRT-LLM下使用triton server image进行GPT模型benchmark时出现的断言错误，可能是由于0.7.1版本发布导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/814
这是一个需求类型的issue，主要涉及添加适用于Windows的批处理管理器静态库。原因可能是为了在Windows平台上提供更好的批处理管理功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/813
这是一个用户提出需求的issue，主要涉及到对句子重新表述的功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/812
这是一个Bug报告类型的issue，主要涉及TensorRT-LLM中smoothquant功能的性能问题。由于smoothquant导致的act_scales无线性mlp模块，导致0.7.1版本性能比0.6.1版本低8到10%。

https://github.com/NVIDIA/TensorRT-LLM/issues/811
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM下的aww-int4模型量化过程中出现的错误。原因可能是由于设置了tp_size为4导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/810
这是一个bug报告，涉及的主要对象是TensorRT-LLM项目中的smooth quant model构建过程。这个问题的原因是self.dtype未定义导致构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/809
这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的GptSession或GptManager。用户希望能够直接从pytorch.bin或safetensors模型进行推断，而不必先通过build.py构建并存储引擎到磁盘，从而提高加载模型的效率。

https://github.com/NVIDIA/TensorRT-LLM/issues/808
这是一个bug报告，涉及TensorRT-LLM软件的构建过程中出现了"Import Error: /libs/libth_common.so: Undefined Symbol"错误，用户寻求解决此问题的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/807
这是一个bug报告，主要涉及TensorRT-LLM中使用多个block模式未能提高速度的问题。原因可能是由于代码中的某些问题或配置设置不正确导致性能无法提升。

https://github.com/NVIDIA/TensorRT-LLM/issues/806
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM中Conv2D计算的结果精度差异，可能是由于代码实现问题或使用方式不当导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/805
这是一个关于资源占用问题的bug报告，涉及TensorRT-LLM中的whisper模型，问题是为什么whisper模型需要17GB的视频内存，而fastwhisper只需要4GB的视频内存。用户还提到找不到对whisper模型进行INT量化的方法，询问该功能是否目前不支持，并希望了解是否有优化的方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/804
这是一个用户提出需求的issue，主要对象是希望在TensorRT-LLM中的examples/stablediffusion文件夹下实现转换和构建代码。由于非LLM transformer模型已经开始在框架中得到支持，用户需要更多的转换和构建代码用于重新定义SD模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/803
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的channel-wise weight-only gemm实现，用户询问为什么不支持添加偏置项。原因可能是之前的实现默认设置了epilogue，但未考虑偏置。

https://github.com/NVIDIA/TensorRT-LLM/issues/802
这是一个bug报告，主要对象是TensorRT-LLM中的Ensemble模型在使用多GPU进行推理时遇到维度错误导致请求失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/801
这是一个关于依赖其它库支持的bug报告，主要涉及的对象是TensorRT-LLM中的smooth quant llama模块。这个问题出现的原因似乎是要求用户使用原生的bettertransformer而不支持引用外部依赖。

https://github.com/NVIDIA/TensorRT-LLM/issues/800
这是一个用户提出需求的Issue，主要涉及TensorRT-LLM下的TRTLLM Triton backend对encdec模型（例如T5）的支持更新。用户想了解是否有计划支持这一功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/799
这是一个bug报告，主要涉及TensorRT-LLM在GPU选项上无法运行的问题。导致这个问题的原因可能是无法启动容器进程或者无法创建任务容器。

https://github.com/NVIDIA/TensorRT-LLM/issues/798
这是一个用户提问类型的issue，主要涉及对象是TensorRT-LLM。由于用户不熟悉docker，提出了关于安装、部署和使用的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/797
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的模型构建。由于缺少'from_pretrained'属性导致了AttributeError，导致了构建模型失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/796
这个issue是关于bug报告，主要涉及TensorRT-LLM下构建Falcon-7B TRT-LLM引擎时出现KeyError的问题。由于某种原因导致构建过程中出现了KeyError错误堆栈。

https://github.com/NVIDIA/TensorRT-LLM/issues/795
这是一个bug报告，涉及TensorRT-LLM中的`build.py`脚本，描述了加载的模型无法正确传递到进程中的问题。原因可能是`hf_gpt`在进程中为`None`。

https://github.com/NVIDIA/TensorRT-LLM/issues/794
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Whisper模型添加支持weight-only功能。该需求的目的是通过简化构建命令来减少GPU内存使用、提高推理速度并保持准确性。

https://github.com/NVIDIA/TensorRT-LLM/issues/793
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM中的build.py工具以及PyTorch模型文件的加载，导致由于不兼容的格式或版本问题而无法成功加载模型的权重。

https://github.com/NVIDIA/TensorRT-LLM/issues/792
这个issue属于功能请求类型，主要对象是针对TensorRT-LLM的支持YaRN请求。由于YaRN模型具有更长的上下文长度，因此用户请求该模型的支持以便在涉及广泛上下文的任务中发挥作用。

https://github.com/NVIDIA/TensorRT-LLM/issues/791
这是一个用户提出需求的类型，主要涉及到TensorRT-LLM下的解码逻辑，用户希望能够改变解码逻辑以支持一些新的功能，例如推测性采样。

https://github.com/NVIDIA/TensorRT-LLM/issues/790
这个issue类型是用户提出需求，问题单涉及的主要对象是TensorRT-LLM，由于用户想要了解如何使用LLAMA 7b来运行示例而不需要构建引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/788
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM的Whisper示例。由于在运行`run.py`时出现了跟内存分配相关的错误，推测是由于内存相关的问题导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/787
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的更新，其中包括模型支持、功能特性、bug修复和文档更新等，其中提到了修复了`gptManagerBenchmark`和Blip2的构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/786
这个issue属于bug报告类型，涉及TensorRT-LLM工具。由于标题和内容为空，可能是用户误操作或未能正确描述问题所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/785
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的Qwen模型。问题是由于缺少'rotary_embedding_base'参数导致构建结果不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/784
这是一个bug报告，涉及的主要对象是在设置4GPU llama270b服务器时出现的地址绑定错误导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/783
这是一个bug报告，涉及TensorRT-LLM环境和复现步骤，由于环境配置或软件逻辑错误导致软件行为与预期不符。

https://github.com/NVIDIA/TensorRT-LLM/issues/782
这个issue是关于bug报告，涉及到TensorRT-LLM下的GptManager，由于输出中排除了输入文本导致段错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/781
这是一个bug报告，涉及的主要对象是在TensorRT-LLM中构建和运行的Docker镜像。由于在Mac上构建的镜像无法成功运行，并出现 "exec format error" 错误，用户提出了这个问题请求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/780
这是一个bug报告，主要涉及TensorRT-LLM中fastertransformer操作符的缺失导致的AttributeError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/779
这是一个bug报告，主要涉及于修复Mixtral文档中的一个`run.py`链接，并提出应该遵循与llama文件夹相同模式的建议。原因是当前的链接失效，导致无法访问正确的`run.py`文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/778
这是一个功能性问题，主要涉及TensorRT-LLM中的Bert和Roberta模型的支持。造成这个问题的原因是Bert模型在处理attention_mask时存在一些问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/777
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于安装过程中出现错误，可能是由于环境配置问题导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/776
这是一个bug报告，主要涉及TensorRT-LLM中构建qwen awq模型时出现的问题。由于加载权重时的数据类型不匹配，导致了出现加载权重错误的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/775
这是一个bug报告，主要涉及TensorRT-LLM库中运行GPT2模型时出现CUDA运行时错误的问题，导致运行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/774
这是一个Bug报告，该问题涉及的主要对象是在使用A10进行chatglm26b转换时发生错误。原因可能是代码中的某些问题导致了转换失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/773
这是一个关于bug报告的issue，主要涉及到TensorRT-LLM基于fastapi服务器生成过程中出现的卡住现象。可能是由于TensorRT-LLM引擎在处理http请求时出现了问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/772
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM中使用int8 weightonly quantization构建72B Qwen模型，由于显存不足导致编译过程中出现了Out of Memory (OOM)错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/771
这是一个关于构建TensorRT-LLM下T4 int4权重失败的bug报告，由于cuDNN版本不匹配导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/770
这是一个bug报告类型的issue，涉及到TensorRT-LLM工具中构建失败的问题，原因是缺少所需的scratch空间导致内部错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/769
这个issue类型为用户提出问题，主要涉及对象是summarize.py脚本运行时的得分计算，由于用户想要了解关于rouge指标得分的计算解释。

https://github.com/NVIDIA/TensorRT-LLM/issues/768
这是一个bug报告，关于如何在使用TensorRT-LLM调试72B模型在多个GPU上的问题。问题可能是因为使用管道并行加载模型到2个GPU导致的结果不如预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/767
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM模块下的bloom conversion错误。由于 ModuleNotFoundError 表明模块未找到，导致出现了 "tensorrt_llm.models.llama.utils" 的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/766
这是一个 Bug 报告，主要涉及 TensorRT-LLM 中的一个错误问题。其原因可能是由于代码中的属性调用问题而导致 AttributeError。

https://github.com/NVIDIA/TensorRT-LLM/issues/765
这是一个用户需求类型的问题，主要涉及的对象是在TensorRT-LLM中应用平滑量化和管道并行的可能性。用户由于模型过大无法在单个GPU上运行导致量化失败，通过询问是否可以使用管道并行来解决这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/764
这是一个bug报告，该问题涉及TensorRT-LLM的构建过程。导致错误的原因可能是环境配置或构建脚本中的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/763
这是一个bug报告，主要对象是TensorRT-LLM中的CodeLlama-7b模型。由于值的形状不匹配，导致了构建时出现错误，错误信息表明更新后的形状与原始形状不同。

https://github.com/NVIDIA/TensorRT-LLM/issues/762
这个issue属于Bug报告类型，涉及的主要对象是无法构建BERT模型。由于CUDA相关路径不存在，导致无法找到所需的库文件，出现了构建模型失败的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/761
这是一个关于功能需求的issue，主要涉及到如何在TensorRT-LLM中创建int4_awq类型的权重矩阵。用户提出了在quantize.py中无法直接支持int4_awq参数的问题，希望得到关于使用int4_awq的解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/760
这是一个bug报告，涉及的主要对象是在ORIN设备上构建TensorRT-LLM wheel时遇到错误。由于更新了CUDA到12.2版本，要求使用的TensorRT版本不匹配，导致了编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/759
这是一个bug报告类型的issue，主要涉及TensorRT-LLM在构建wheel时遇到Ubuntu版本错误，希望得到关于如何解决这个问题的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/758
这是一个bug报告，主要涉及TensorRT-LLM中的benchmark运行问题。由于NoneType对象缺少'trt_tensor'属性，导致程序在运行时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/757
这是一个bug报告，涉及主要对象为TensorRT-LLM中的bloom示例模型。由于在运行trtllm示例模型时出现"bash: trtllmbuild: command not found"错误，推测是环境变量配置或命令路径设置不正确导致无法找到对应命令。

https://github.com/NVIDIA/TensorRT-LLM/issues/755
这个问题是一个bug报告，主要涉及的对象是TensorRT-LLM中的一个示例脚本"fmha_triton.py"。由于某个bug导致了需要修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/754
这个issue是关于更新TensorRT-LLM主分支的，涉及的主要对象是TensorRT-LLM软件。由于编译错误和文档更新等原因导致了多个bug修复和功能更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/753
这是一个bug报告，主要涉及对象为TensorRT-LLM中的Engine world size与Runtime world size不匹配，导致错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/752
这是一个bug报告的issue，主要涉及的对象是TensorRTLLM中的llama2模型。由于运行时出现了Segmentation fault信号，可能是由于内存访问错误导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/751
这是一个用户提出需求的issue，主要涉及的对象是支持Swin Transformer。原因可能是用户希望在TensorRT-LLM中添加对Swin Transformer的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/750
这是一个需求更新的issue。该问题单主要涉及TensorRT-LLM项目。由于版本更新或功能改进的需求，用户提出了更新TensorRT-LLM的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/748
这是一个关于性能问题的bug报告，主要涉及到awq量化的时候速度慢的问题，用户想知道是否有人测试过awq的延迟。

https://github.com/NVIDIA/TensorRT-LLM/issues/747
这是一个bug报告，涉及的主要对象是MPI库。这个问题可能是由于缺少help文件导致的MPI初始化失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/746
这是一个关于如何构建TensorRT-LLM并在Python环境中使用C++运行时绑定的问题，用户在导入依赖于TensorRTLLM C++运行时Python绑定的Python模块时遇到了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/745
这是一个关于更新TensorRT-LLM Release分支的问题，类型属于需求提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/743
这是一个bug报告，用户遇到了在构建TensorRT引擎时程序被kill的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/742
这个issue是一个bug报告，主要涉及TensorRT-LLM中Gemm插件的参数顺序问题。这个bug导致参数顺序在调用cublas gemm函数时发生变化。

https://github.com/NVIDIA/TensorRT-LLM/issues/741
这是一个bug报告，该问题涉及的主要对象是TensorRT-LLM中的convert_checkpoint.py脚本。由于变量'int8_weights'在赋值前被引用，导致了报错现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/740
这是一个bug报告，涉及TensorRT-LLM下的W8A8与W8A16推理实现速度对比的问题，用户寻求帮助解决W8A8推理实现比W8A16慢的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/739
这是一个关于文档描述不符实际的问题反馈，主要涉及TensorRT-LLM下的INT4支持现实与文档的不一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/738
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM，用户需要支持多个LoRA同时进行推理，类似于s-lora，以用于并发推理服务器。

https://github.com/NVIDIA/TensorRT-LLM/issues/737
这是一个用户提出的问题，主要涉及TensorRT-LLM下的smooth-quant int8内存使用情况不符合预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/736
这个issue是一个bug报告，涉及的主要对象是TensorRT-LLM。由于显存不足导致OutOfMemory错误，用户希望知道是否能通过使用4块A6000显卡来解决问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/735
这是一个bug报告，主要涉及到在旧版本的NVIDIA驱动中无法查询"compute_cap"字段，导致需要使用torch代替"nvidiasmi"来获取设备的能力。

https://github.com/NVIDIA/TensorRT-LLM/issues/734
这个issue属于bug报告类型，主要涉及TensorRT-LLM中将模型转换为FP8时出现的错误。导致这个问题的原因是在转换过程中出现了“TypeError: Object of type Tensor is not JSON serializable”错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/733
这是一个bug报告，涉及内容是在执行llama2 70B benchmark test时出现了symbol lookup error导致程序无法正常运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/732
这是一个性能问题报告，主要涉及的对象是TensorRT-LLM和Triton，用户询问为什么Triton的显存占用量是TensorRT-LLM的两倍。

https://github.com/NVIDIA/TensorRT-LLM/issues/731
这是用户提出的需求问题，主要对象是想在不构建引擎的情况下运行LLAMA 7b示例，可能由于无法使用Docker，希望能直接通过build_wheel.py运行基准测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/730
这是一个bug报告，该问题涉及的主要对象是TensorRT-LLM下的Llama模型。由于构建引擎时指定了bfloat16和int8权重类型，但在加载权重时发生了错误导致程序崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/729
这个issue是关于bug报告的，主要涉及TensorRT-LLM下的INT8 KV Cache和量化引擎构建，出现了断言失败的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/728
这是一个用户请教问题的类型的issue，主要涉及对象为TensorRT-LLM中的qwen-vl模块。由于用户想要了解如何在TensorRT-LLM中构建qwenvl，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/727
这是一个关于性能优化和功能改进的Issue，主要涉及到TensorRT-LLM中重复惩罚内核的重写。该问题由于使用了全局内存分配的信号量进行跨块同步，导致新内核的执行时间增加了18%。

https://github.com/NVIDIA/TensorRT-LLM/issues/726
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM下的gptq量化准确度问题，问题可能是由量化操作导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/725
这是一个需求类型的issue，主要涉及TensorRT-LLM项目中的容器发布问题，用户由于网络问题无法构建Docker环境。

https://github.com/NVIDIA/TensorRT-LLM/issues/724
这是一个bug报告，涉及的主要对象是TensorRT-LLM和TensorRT之间函数签名相同的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/723
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下的支持的模型版本。由于该repo只支持qwen7b和qwen13b，用户询问是否支持qwen1.8b，可能是出于想要使用这个模型版本的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/722
这个issue是一个关于bug报告的问题，涉及到TensorRT LLM中Mixtral生成过程无法停止的情况。原因可能是参数配置或模型设置导致生成过程无法正确终止。

https://github.com/NVIDIA/TensorRT-LLM/issues/721
这是一个更新issue，涉及的主要对象是在/examples/qwen下的gradio版本更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/720
这是一个用户提出需求的问题，主要涉及到TensorRT-LLM是否支持Lora/Qlora模型的问题，用户因无法将Lora或Qlora模型与基础checkpoint合并而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/719
这是一个bug报告类型的issue，涉及的主要对象是TensorRT-LLM代码库。由于Docker构建错误导致问题，用户在此需要修复这一问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/718
这是一个关于bug报告的问题，主要对象是TensorRT-LLM。由于最大批处理大小大于8时请求失败，可能是由于版本0.5.0中的某些问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/717
该issue类型为用户请求教程，主要对象是如何在TensorRT-LLM中加载和运行一个小模型。这个问题是因为用户想测试TensorRT-LLM在小模型上的运行速度，但不清楚如何在其中加载和运行非语言模型的小模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/716
这是一个bug报告，主要涉及TensorRT-LLM的example program在使用--streaming参数配合mpirun -n 4时出现程序卡住的问题。可能由于某些原因导致程序在特定代码段处无法继续执行。

https://github.com/NVIDIA/TensorRT-LLM/issues/715
这是一个bug报告，主要涉及TensorRT-LLM的容器构建错误。由于缺少相关依赖或配置错误，导致了构建TensorRT-LLM容器时出现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/714
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于CUDA版本和配置的主机编译器不兼容，导致出现了nvcc警告提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/713
这个issue属于bug报告类型，主要涉及的对象是使用awq工具将quantized npz模型转换为TensorRT格式时出现的错误。导致这个问题的原因是更新后的值与原始值的形状不一致，从而触发了AssertionError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/712
这是一个升级依赖库版本的issue，涉及的主要对象是TensorRT-LLM。导致这个issue的原因是要将transformers库从版本4.33.1升级到4.36.0。

https://github.com/NVIDIA/TensorRT-LLM/issues/711
这是一个bug报告，涉及的主要对象是ModelRunnerCpp接口，用户在询问关于是否会在将来为ModelRunnerCpp接口添加流式响应支持。这个问题可能由于尝试在Python绑定的C++运行时的流式模式下进行推断时遇到的运行时错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/710
这是一个bug报告，该问题涉及TensorRT-LLM下int8_t quant模块，用户提出可能存在误导性排序bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/709
这是一个bug报告，主要涉及AWQ quantisation using NVIDIA AMMO toolkit，由于无效的文件名导致了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/708
这是一个需求更新的issue，主要对象是TensorRT-LLM更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/707
这个issue属于bug报告，涉及TensorRT-LLM在使用batch manager时性能问题，由于不同的max_batch_size导致性能出现明显差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/706
这是一个bug报告，主要涉及pynvml版本过低导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/705
这是一个bug报告，涉及TensorRT-LLM在0.6.1版本下的InternLM SmoothQuant无法正常工作的问题，导致bias属性未正确传递而导致初始化错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/704
这个issue属于用户提问类型，主要涉及TensorRT-LLM中关于`activation size`和`workspace`的问题，用户询问`activation size`是否等于`workspace`，以及关于多层计算中工作空间释放和复用的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/703
这是一个 bug 报告，涉及 TensorRT-LLM 中的性能问题，可能由于启用了一个无法识别的优化配置参数导致性能差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/702
该issue类型是用户提出需求，主要关注的对象是TensorRT-LLM项目，用户在询问是否TensorRT-LLM计划集成页面注意力功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/701
这个issue属于bug报告类型，涉及的主要对象是GptManager pybinding和tensorrt_llm.bindings.InferenceRequest对象，由于InferenceRequest对象在Python中无法序列化导致了TypeError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/700
这是一个bug报告，主要涉及到CUDA runtime error in cublasLtMatmul，由于CUBLAS_STATUS_EXECUTION_FAILED导致了此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/699
这是一个bug报告，主要涉及TensorRT-LLM中KV cache管理器的问题，由于mNextBlocks为空导致程序崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/698
这是一个关于软件bug的报告，主要涉及了TensorRT-LLM引擎加载内存大小大于原始模型大小的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/697
这是一个bug报告，主要对象是在使用Mixtral-7b-instruct weight-only engine with LORA构建模型时遇到的问题。原因是在使用LORA时出现了错误，可能涉及tensor.transpose()函数的错误使用。

https://github.com/NVIDIA/TensorRT-LLM/issues/696
这是一个关于性能问题的 bug 报告，主要涉及的对象是 TensorRT-LLM 中的主分支和 0.5.0 版本。原因可能是在不同的情况下，主分支的性能略低于 0.5.0 版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/695
这是一个bug报告，涉及TensorRT-LLM下的build error，用户遇到与trtllmbuild相关的错误并请求协助解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/694
这是一个关于bug报告的issue，涉及TensorRT-LLM在使用较大max_batch_size时出现内存分配问题，导致运行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/693
这是一个bug报告，用户在benchmarking时遇到了Assertion error。主要涉及的对象是TensorRT-LLM中的benchmark功能。该问题可能是由于引擎文件无法打开导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/692
这是一个用户提出需求的issue，主要涉及自定义采样层和Inflight批处理。由于文档中提到了自定义采样层，用户想知道是否可以为Inflight批处理实现自定义生成循环。

https://github.com/NVIDIA/TensorRT-LLM/issues/691
这个issue类型是bug报告，主要涉及TensorRT LLM在使用chatglm26b模型时输出结果与Hugging Face Transformers不一致的问题，可能由于num_beams参数设置不当导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/690
这是一个bug报告，涉及TensorRT-LLM中的fix banRepeatNgram问题，由于参数传递错误导致engine在特定条件下出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/689
这是一个bug报告，涉及主要对象为TensorRT-LLM下的engine在triton-inference-server中的使用。由于无法加载plan文件来自动完成配置，导致出现了UNAVAILABLE: Internal错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/688
这是一个bug报告，主要涉及的对象是在构建bloom 176b模型时出现整数溢出错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/687
这个issue类型为用户提出需求，主要涉及对象为支持llava，具体原因可能是用户希望TensorRT-LLM能够支持llava这个功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/686
这是一个用户提出需求的issue，主要涉及的对象是'TensorRT-LLM'中的 'tensorrt_llm/kernels/kvCacheUtils.h' 文件。问题是用户想将'int32_t'变量改为其他类型（例如'int64'），以支持更大的batch_size/seq_length。这个问题是由于现有数据类型限制导致的，用户希望解决MMHA kernel中的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/685
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的GPTQ或AWQ在V100上的支持情况。用户报告了在V100上运行AWQ时报错，在运行GPTQ时也报错，但并未提供具体的错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/684
这是一个bug报告，主要涉及TensorRT-LLM的模型构建过程中出现了错误，可能是由于新增和删除的tensor导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/683
这个issue是一个关于询问使用8位GPTQ量化模型的问题，主要涉及对象是在构建引擎时选择合适的权重精度选项，可能由于对不同精度选项的理解不清晰导致用户提出了关于选择int8、int4、int4_awq、int4_gptq之间的区别和最佳选择的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/682
这是一个关于需求提出的issue，主要涉及的对象是是否支持INT8 GEMM。由于TensorRT-LLM目前主要支持浮点数类型，因此用户提出了关于是否支持INT8/INT4 GEMM和W4Q4量化算法的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/681
这是一个bug报告，涉及GPU内存在模型删除后仍保留，可能由于遗留的中间张量或KV缓存值导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/680
这是一个bug报告，涉及TensorRT-LLM的版本转换脚本在python3.9下无法正常运行的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/679
这是一个bug报告，主要涉及TensorRT-LLM下的openllama3b模型，在设置kv缓存时出现"Size per head is not a multiple of X"的错误。由于X无法整除头部大小100，导致该错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/678
这是一个bug报告，主要涉及MPT-30B模型的处理失败。由于代码执行时出现异常，导致无法将引擎序列化成功。

https://github.com/NVIDIA/TensorRT-LLM/issues/677
这是一个bug报告类型的issue，主要涉及Triton server测试使用TensorRT-LLM作为后端时，HTTP端点不支持具有解耦事务策略的模型。导致的问题可能是无法构建配置文件和脚本执行错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/676
这是一个bug报告，涉及到Libnvinfer_plugin_tensorrt_llm.so库在Colab A100上无法打开共享对象文件的问题。这可能是由于文件路径错误或缺少依赖库导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/675
这是一个bug报告，涉及到TensorRT-LLM中CUDA错误，是由于设置的输入长度和输出长度过长导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/674
这是一个关于在Kubernetes中使用Triton Server和TensorRT-LLM后端以及Llama 2模型部署时发生的bug报告，出现了bus error。

https://github.com/NVIDIA/TensorRT-LLM/issues/673
这是一个bug报告，主要涉及了在构建Llama-2-13b-hf engine时遇到的形状不匹配AssertionError问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/672
这是一个关于性能变化的issue，主要涉及Mixtral优化和Latency的改变。原因可能是由于代码更改或优化导致的性能变化。

https://github.com/NVIDIA/TensorRT-LLM/issues/671
这是一个用户提出需求类型的issue，主要涉及到TensorRT-LLM中的"Speculative decoding"功能的使用问题，用户想了解如何使用此功能以及是否有相关文档可供参考。原因可能是用户想了解最新更新中提供的支持功能的具体细节。

https://github.com/NVIDIA/TensorRT-LLM/issues/669
此issue属于bug报告，主要涉及的对象是构建TensorRT-LLM Docker镜像时遇到的问题。原因可能是需要添加`allowchangeheldpackages`参数来移除已存在的libcudnn8库。

https://github.com/NVIDIA/TensorRT-LLM/issues/668
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中如何连续地主机llama 7b模型的问题。用户希望能够持续加载模型，而不仅仅在推断时加载，可能由于当前内存使用量只在推断时增加而导致此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/667
这是一个bug报告类型的issue，主要涉及TensorRT-LLM模型的更新，其中描述了多个功能支持、Bug修复和性能优化，提到了一系列更新和修复内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/666
这是一个bug报告，主要涉及对象是TensorRT-LLM下的summarize.py脚本。由于无法找到rouge.py模块，导致报错信息中提到的FileNotFoundError。

https://github.com/NVIDIA/TensorRT-LLM/issues/665
该issue类型为用户询问问题，主要涉及如何测试批处理运行的转换模型，用户想知道应该使用哪些命令或更改哪些代码。

https://github.com/NVIDIA/TensorRT-LLM/issues/664
该问题类型为功能需求提议，主要涉及TensorRT-LLM下新增的TensorLLM插件是否能进一步优化ViT和Q-Former模型的运行速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/663
这是一个bug报告，主要涉及到内存不足导致服务器崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/662
这是一个bug报告，涉及的主要对象是在TensorRT-LLM下使用AWQ 4-bit with 4 A6000(LLama 70B)时出现的错误。由于版本号为0.6.1，导致无法正常使用该配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/661
这个issue类型属于bug报告，涉及的主要对象是TensorRT-LLM中的输出字典。此问题可能出现是因为在输出字典中缺少了"output_ids"键值，导致该用户在使用时遇到未定义的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/660
这是一个关于部署TensorRT-LLM引擎时出现错误的bug报告，问题涉及主要对象为TensorRT-LLM、tritonserver和tensorrtllm_backend。

https://github.com/NVIDIA/TensorRT-LLM/issues/659
这是一个涉及bug报告类型的issue，主要涉及TensorRT-LLM下使用streaming=True进行推断时的问题。由于streaming推断的设置，导致rank 1模型无法检索数据，可能是出现此问题的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/658
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM下的模型构建程序。由于网络接口卡选择不公平导致MPI进程性能下降，出现了编译错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/657
这是一个用户询问问题类型的issue，主要涉及对象为获取`FLayerInfo`对象，可能由于缺少对 `FLayerInfoMemo.instance().create()` 和 `FLayerInfoMemo.instance().add()` 方法的调用而导致无法获取所需的 `FLayerInfo` 对象。

https://github.com/NVIDIA/TensorRT-LLM/issues/656
这是一个bug报告，主要对象是TensorRT-LLM下的tritonserver:23.11-trtllm-python-py3 Image。导致该问题的原因是23.11版本的triton server image导致benchmark无法工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/655
这个issue属于bug报告，主要涉及的对象是TensorRT-LLM的serving system。问题的根本原因是在高负载下，系统返回的响应长度逐渐变短，导致请求的响应不完整。

https://github.com/NVIDIA/TensorRT-LLM/issues/654
这是一个bug报告，主要涉及对象是在TensorRT-LLM下运行时遇到了无法识别的错误，问题可能是由于主分支无法在A800上正常工作，但在A10上可以正常运行，且对驱动版本和硬件要求较为敏感。

https://github.com/NVIDIA/TensorRT-LLM/issues/653
这是一个bug报告，该问题涉及的主要对象是TensorRT-LLM中的baichuan2引擎构建过程。导致这个错误的原因是参数类型错误，期望是numpy.ndarray或torch.Tensor类型，但实际传入的是NoneType。

https://github.com/NVIDIA/TensorRT-LLM/issues/652
这是一个bug报告类型的issue，涉及的主要对象是AWQ和LLM-awq的性能比较。原因可能是ammo中的awq实现使用的默认参数导致性能下降。

https://github.com/NVIDIA/TensorRT-LLM/issues/651
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM中的13b模型。由于输出结果异常，用户请求关于测试引擎输出不正确的建议和帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/650
这是一个bug报告，主要涉及TensorRT-LLM下的gptSessionBenchmark使用--enable_cuda_graph时出现性能问题的情况。造成该问题的原因可能是CUDA图（cuda graph）启用时导致内存消耗过大并降低了性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/649
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的gptManagerBenchmark，由于某种原因导致测试inflight batching时crash。

https://github.com/NVIDIA/TensorRT-LLM/issues/648
这是一个bug报告，涉及TensorRT-LLM下Llama2-7b引擎在4GPU和8GPU上构建失败的问题。这可能是由于构建命令参数配置错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/647
这是一个用户提出需求的issue，主要涉及Whisper中缺少时间戳特性的问题。用户询问是否有计划支持Whisper的时间戳特性。

https://github.com/NVIDIA/TensorRT-LLM/issues/646
这是一个用户提出问题的issue，主要涉及TensorRT-LLM下greedy sample的设置导致结果不含eos标记。

https://github.com/NVIDIA/TensorRT-LLM/issues/645
这是一个bug报告，涉及到TensorRT-LLM的构建错误，提示需要配置DOCKER_BUILD_ARGS这个参数。可能是由于参数配置不正确导致的构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/644
这

https://github.com/NVIDIA/TensorRT-LLM/issues/643
这是一个bug报告，涉及主要对象为SQ engine的构建过程，由于某些原因导致了 AttributeError: 'NoneType' object has no attribute 'trt_tensor' 的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/642
这是一个bug报告，主要涉及的对象是fp8量化。这个问题可能是由于参数设置不正确导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/641
这是一个用户提出需求的issue，主要对象是TensorRT-LLM中的模型。用户想知道是否可以在模型中传递多个输入层的数据。

https://github.com/NVIDIA/TensorRT-LLM/issues/640
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下使用多用户时出现的core dump问题。问题可能是由多人同时生成文本时对decoder进行调用导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/639
这是一个bug报告，主要涉及TensorRT-LLM中的'--gather_all_token_logits'选项，可能是由于构建引擎时使用该选项导致第一个token出现异常字符。

https://github.com/NVIDIA/TensorRT-LLM/issues/638
这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持SmoothQuant W8A8 for Mistral models的问题，用户想了解是否有预期的更新时间。

https://github.com/NVIDIA/TensorRT-LLM/issues/637
这个issue类型是bug报告，涉及主要对象为LLaMa模型，由于缺少modules_to_save数值导致的TypeError错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/636
这是一个bug报告，涉及到0.6.1版本下的TensorRT-LLM的运行脚本输出问题。原因可能是脚本整合和位置改变导致输出文本无法正确解码。

https://github.com/NVIDIA/TensorRT-LLM/issues/635
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM 0.6.1版本中的summarization benchmark脚本问题。由于更新后`summarize.py`被替换为`summarize_long.py`且存在错误，导致无法成功运行benchmark脚本。

https://github.com/NVIDIA/TensorRT-LLM/issues/634
这是一个用户提出需求的issue，主要涉及是否计划在TensorRT-LLM中支持Stable Diffusion模型，用户询问出于什么原因可以在TRTLLM中支持该模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/633
这个issue是关于功能需求的，主要涉及TensorRT-LLM中的pipeline parallelism功能，并由于持续批处理模式下未观察到预期性能差异而提出帮助请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/632
这是一个功能请求（Feature Request）issue，主要涉及了对TensorRT-LLM支持的模型进行增加或改进。由于用户请求支持不同的语言模型模型，包括Decoder Only、Encoder / EncoderDecoder和MultiModal类型的模型，以及一些相关功能的需求，并且此issue追踪了用户的请求和项目正在进行的工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/631
这是一个bug报告类型的issue，该问题涉及的主要对象是README.md文件。由于原先的gpu数量设置不准确，用户提出修改为4台GPU，可能是为了保证软件能够正确地利用全部的GPU资源。

https://github.com/NVIDIA/TensorRT-LLM/issues/630
这是一个bug报告，涉及主要对象是TensorRT-LLM中的模型引擎构建和执行过程。由于最大递归深度超出限制，导致出现问题或报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/629
这个issue类型为用户提出需求，主要对象是TensorRT-LLM中的图重写模块；用户想要了解图重写与计算图优化之间的关系，询问是否会在未来加入其他模式。

https://github.com/NVIDIA/TensorRT-LLM/issues/628
这是一个bug报告，主要涉及TensorRT-LLM的CUDA 11.7环境下安装时遇到libnccl2安装错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/627
这是一个 bug 报告，主要涉及的对象是在 Windows 11 PC 上运行的 TensorRT-LLM 的 build.py 脚本，导致在某个版本环境下出现导入 LoraConfig 出现问题的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/626
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的构建引擎失败问题，可能是由于版本不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/625
这是一个bug报告，主要涉及TensorRT-LLM项目，由于缺少`xqa_kernel_dt_fp16_d_128_beam_1_kvt_fp16_nqpkv_8_sm_90.cubin.cpp`文件导致编译报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/624
这是一个bug报告，主要涉及到TensorRT-LLM中的gptAttentionPlugin.cpp文件，编译时出现了模糊符号导致的编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/623
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的genai docker。由于测试过程中出现问题，导致需要关闭此issue。

https://github.com/NVIDIA/TensorRT-LLM/issues/622
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的smoothquant功能。由于内存不足导致OOM错误，用户正在寻找解决方案以在4个A100 GPU上对llama 70b进行量化。

https://github.com/NVIDIA/TensorRT-LLM/issues/621
这是一个bug报告，涉及TensorRT-LLM项目中的构建问题，由于某种原因导致了"Segmentation fault"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/620
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的生成过程中共享key value cache blocks的优化问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/619
这是一个关于TensorRT-LLM中安装问题的技术支持请求，用户询问在构建引擎时是否需要设置正确的LD_LIBRARY_PATH参数，以及在不同机器上出现错误的原因可能是与TensorRT安装版本不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/618
这个issue是关于用户需求的，主要对象是在Windows 11上构建TensorRT-LLM时缺少批处理管理器的静态库文件，导致无法完成构建。

https://github.com/NVIDIA/TensorRT-LLM/issues/617
这是一个关于构建错误的bug报告，主要涉及TensorRT-LLM的构建过程，用户遇到了链接错误，可能是由于未正确设置环境变量或缺少必要的依赖导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/616
这是一个新模型发布的issue，主要涉及到Mistral AI的Mixtral模型，属于新功能发布类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/615
这是一个修复bug的issue，涉及的主要对象是TensorRT-LLM的代码。由于变量命名错误导致的bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/613
这是一个包含bug修复、新功能和性能优化内容的issue，主要涉及TensorRT-LLM的更新和改进。由于不同模块的代码更新和添加导致了bug修复和新功能实现以及性能优化需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/612
这是一个bug报告，涉及的主要对象是TensorRT-LLM的enc_dec模型。由于模型在不同精度下的优化结果未能与HuggingFace模型对齐，导致在特定场景下出现输出差异，用户在寻求解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/611
这是一个用户提出需求的问题，主要涉及TensorRT-LLM下如何支持自定义解码器模型的部署。用户希望在Tritonserver中使用TensorRTLLM后端部署经过训练的1B/6B模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/610
这是一个关于如何在TensorRT-LLM中使用流式输出的bug报告，用户希望在生成token的迭代循环中实现inflight batch和流式输出，但设置为True的"is_streaming"属性未按预期调用回调函数"response_cb"，导致回调函数直到生成完成才被调用。

https://github.com/NVIDIA/TensorRT-LLM/issues/609
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中CUDA代码是否会在未来开源，问题涉及到实现的CUDA代码是否会对外公开。

https://github.com/NVIDIA/TensorRT-LLM/issues/608
这是一个用户提出需求的类型，主要对象是StoppingCriteria和LogitsProcessor，在最新版本的Python API中支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/607
该issue为bug报告类型，涉及TensorRT-LLM的AWQ quantization在构建engine时出现错误，导致需要调用load_from_awq_llama和load_from_hf_llama两个函数，用户询问此错误原因及解决方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/606
这是一个用户提出需求的issue，主要涉及TensorRT-LLM在Windows Visual Studio下使用cmake构建时遇到的问题。原因是目前的代码主要针对Linux，同时缺少对Windows下的NCCL库依赖处理方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/605
这是一个bug报告，涉及的主要对象是Docker image for CUDA-11.x。用户遇到了无法在CUDA11.6环境中正常使用Docker image的问题，可能是因为该Docker image实际上是基于CUDA12.x构建而非适配CUDA11.x导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/604
这是一个bug报告类型的issue，主要涉及在构建TensorRT-LLM下的Yi-34B-200K-Llamafied-GPTQ遇到的问题。由于缺少参数或config.json未能自动选择参数，导致出现错误信息和无法编译问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/603
这是一个bug报告，主要涉及对象是TensorRT-LLM下的QuantMode类。这个问题是因为QuantMode类缺少'none'选项导致的，导致在默认值中出现了魔法数字0。

https://github.com/NVIDIA/TensorRT-LLM/issues/602
这是一个关于脚本运行失败的bug报告，主要涉及TensorRT-LLM项目下的shell脚本运行问题，导致mpirun命令执行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/601
这个issue是关于bug报告，涉及到在rootless模式下使用docker时执行`make -C docker run LOCAL_USER=1`失败的问题，可能由于登录docker后仍无法解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/600
这是一个bug报告，主要涉及TensorRT-LLM模型在处理包含表情符号的文本时发生的断言失败错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/599
这是一个关于需求问题，主要涉及TensorRT-LLM中加载多个Lora权重和多个文本输入以进行推理的问题。这可能由于当前只支持单个Lora权重和输入标记，无法支持多个导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/598
这是一个用户提出需求的问题，主要涉及运行trtllm而无需triton，以及测试/基准inflightbatching。

https://github.com/NVIDIA/TensorRT-LLM/issues/597
这是一个bug报告，涉及TensorRT-LLM下的WMT Example，由于模型输出结果与期望输出不一致，可能是由于引擎插件或其它原因导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/596
这个issue类型为功能增强，主要涉及的对象是将OpenAI Whisper集成进TensorRT-LLM模型。由于TensorRT引擎优化问题，用户提出了加速推断性能、输出时间戳等建议和需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/595
这是一个用户提出需求的issue，主要涉及获取上下文隐藏状态的问题，可能导致用户无法按需访问所需的张量信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/594
这是一个关于性能问题的bug报告，涉及TensorRT-LLM中的`multi_block_mode`选项，用户期望通过该选项获得性能提升，但在运行性能基准测试时却没有看到任何差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/593
这个issue类型是用户提出需求，请教问题，主要涉及的对象是如何设置mpirun的进程数量，以及是否可以使用两个GPU来运行特定任务。由于用户需要了解如何设置mpirun的进程数量以及是否可以使用多个GPU来运行任务，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/592
这是一个bug报告，涉及TensorRT-LLM中构建引擎时出现错误的问题。可能是由于配置或代码错误导致引擎构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/590
这是一个类型为bug报告的issue，涉及的主要对象是'TensorRT-LLM'项目。由于警告消息引用了错误的库'psutil'，导致了错误的警告提示信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/589
这是一个用户提出需求的类型问题，主要涉及TensorRT-LLM engine是否支持在运行时使用if语句，用户困惑于模型中的if语句如何在构建和服务阶段保持一致性的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/588
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的tensor_parrallel功能。由于试图在chatglm26B模型上测试tensor_parrallel时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/587
这是一个bug报告，主要涉及的对象是PTQ INT8 calibration，在70b tp=2的情况下使用自定义数据集进行SQ的标定，但在模型推断时出现OOM错误，猜测可能是使用HF接口导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/586
这是一个功能需求提出问题，主要涉及TensorRT-LLM下的模型转换流程，用户希望能够直接从HF转换MPT模型到TRT引擎而不必经过FT的中间步骤。

https://github.com/NVIDIA/TensorRT-LLM/issues/585
这是一个bug报告类型的issue，主要涉及TensorRT-LLM的非Docker环境下构建问题。问题可能是由于跳过安装cudnn或者其他环境配置不一致引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/584
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的attention kernels，用户关注的问题是在generation phase中，kernel是否支持query length大于1。

https://github.com/NVIDIA/TensorRT-LLM/issues/583
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的GPTAttention插件。导致此问题的原因可能是在插件代码中的长度计算错误，导致断言失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/582
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的使用问题，用户提出关于"use_inflight_batching"不支持的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/581
这是一个用户提出需求的类型，主要涉及TensorRT-LLM的部署和测试，用户遇到安装TensorRTLLM时失败的问题，寻求关于如何使用TensorRTLLM进行部署以及如何构建自定义Docker镜像的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/580
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中的GPTQ模型更新数值形状不一致问题，可能由于忽略了`mapping.tp_rank`而导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/579
这是一个类型为bug报告的issue，主要涉及TensorRT-LLM中的chatglm36b模型。问题可能是由于模型生成的回复过多以及回复不一致的原因导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/578
这是一个询问类型的issue，用户想知道TensorRT-LLM是否支持在A10或4090上进行推断。

https://github.com/NVIDIA/TensorRT-LLM/issues/577
这是一个关于TensorRT-LLM的bug报告，用户在尝试构建一个baichuan27B模型时遇到了RAM不足的问题。原因可能是模型过大导致构建时需要更多的内存。

https://github.com/NVIDIA/TensorRT-LLM/issues/576
这是一个bug报告，涉及的主要对象是C++ benchmarking链接的失效。这个问题由于链接指向的页面不存在导致用户无法获取构建TensorRTLLM的步骤信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/575
这个issue类型是bug报告，主要涉及对象是TensorRT-LLM中的t5模型，可能是由于t5模型具有偏置参数和无法被TP大小整除的嵌入大小导致了构建问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/574
这是一个bug报告类型的issue，涉及到Python脚本在添加"engine_dir"选项时出现的错误，导致出现了张量名称不匹配的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/573
这是一个bug报告，主要涉及编译Docker时在安装mpi4py时失败。由于未说明具体错误信息，无法确定导致失败的具体原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/572
这是一个bug报告，主要涉及到TensorRT-LLM下的llama2模型转换问题，用户遇到了无法转换模型的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/571
这是一个bug报告，主要涉及的对象是无法加载 llama2 模型，可能由于转换后出现错误导致模型无法加载。

https://github.com/NVIDIA/TensorRT-LLM/issues/570
该issue类型为用户提出需求类型，主要涉及的对象是在Python中如何运行LLM模型的批量推断。这个问题可能是由于用户想要批量运行LLM模型进行推断而不知道如何操作或者缺乏相关文档或示例的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/569
这是一个功能需求类型的issue，主要对象是为Windows添加批处理管理器静态库。原因可能是当前缺少这个静态库，导致用户无法在Windows平台上有效地管理批处理任务。

https://github.com/NVIDIA/TensorRT-LLM/issues/568
这是一个用户提出需求的issue，主要涉及TensorRT是否支持`sequence_bias`和`renormalize_logits`功能。用户询问是否能够通过TensorRT实现类似的效果，以及提到了相关的GitHub链接。

https://github.com/NVIDIA/TensorRT-LLM/issues/567
这是一个需求类型的issue，主要涉及yi-34B llamafied model，用户想知道是否可以使用example/llama code。这可能是因为用户想利用llama code来应用在yi-34B llamafied model上，但不确定是否支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/566
这是一个bug报告，涉及主要对象为TensorRT-LLM中的decoding batch_size。由于设置max_num_sequences为8时，在生成阶段将批量大小划分为2个batch * 4的情况，用户希望了解如何将解码批量设置为8而不是分成2个批次 * 4。

https://github.com/NVIDIA/TensorRT-LLM/issues/565
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的AWQ权重量化。这个问题是由于`block_size=512`参数未生效，导致文本样本在应用`tokenizer.encode()`后被全部填充到长度为1195而不是被截断到512。

https://github.com/NVIDIA/TensorRT-LLM/issues/564
这是一个bug报告，主要涉及TensorRT-LLM在推理llama-7b-hf过程中产生的结果错误问题，可能由于某些原因导致embdstep*.txt中的数字为零。

https://github.com/NVIDIA/TensorRT-LLM/issues/563
这是一个关于如何通过MPI在多节点上分发推理的问题，属于用户寻求帮助的类型，主要涉及TensorRT-LLM在多节点上的推理分布设置及环境配置。

https://github.com/NVIDIA/TensorRT-LLM/issues/562
这是一个bug报告，主要对象是在使用GPU时出现了构建模型错误的问题，可能是由于命令转换错误导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/561
这是一个bug报告，涉及于LLAMA模型转换功能在版本V0.6.1下出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/560
这是一个关于构建问题的bug报告，问题涉及TensorRT-LLM项目。由于可能的CUDA版本不匹配，导致构建过程中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/559
这个issue是关于改进CUDA cast函数实现的，不属于bug报告类型，主要涉及到float到int8_t的转换方法优化。

https://github.com/NVIDIA/TensorRT-LLM/issues/558
这是一个bug报告。该问题涉及TensorRT-LLM库在Windows系统上通过pip安装后版本不更新的情况。这可能是由于更新组件后未正确升级导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/557
这个issue是关于编译错误的bug报告，主要涉及TensorRT-LLM在Windows上编译时缺少文件导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/556
这是用户提出需求的类型，主要对象是Mojo Integration。用户询问关于TensorRT-LLM是否有未来与Mojo集成的计划。

https://github.com/NVIDIA/TensorRT-LLM/issues/555
这是一个关于构建TensorRT-LLM下Llama 7B SmoothQuant引擎失败的bug报告，由于0.6.0或0.6.1标签下无法成功构建Llama 7B引擎，相比0.5.0标签出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/554
这是用户提出的需求类型的问题，主要涉及TensorRT-LLM中支持int4-awq/gptq for Qwen的功能。由于该需求可能与新技术或新功能相关，导致用户提出了关于支持int4-awq/gptq for Qwen的问题或需求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/553
这是一个功能需求类型的issue，涉及到GPTj TensorRT-LLM模型的精度选项限制。由于构建脚本中限制了只能选择'float16'和'float32'这两种精度选项，用户想了解是否可能在GPTj TensorRT-LLM引擎中使用bfloat16精度选项。

https://github.com/NVIDIA/TensorRT-LLM/issues/552
这是一个缺失内容的issue，类型为问题或需求，该问题单主要涉及项目的徽章更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/551
这是一个用户提出需求的issue，主要对象是项目的badge。由于badge信息需要更新，用户希望进行相关修改。

https://github.com/NVIDIA/TensorRT-LLM/issues/550
这是一个用户提出需求的issue，主要涉及到文档更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/549
这是一个寻求更新最新信息的issue，属于用户提出需求类型，主要涉及TensorRT-LLM项目。可能由于项目信息落后或者缺失导致用户需要更新最新内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/548
这个issue是一个关于内存利用率的问题，涉及对象是TensorRT-LLM中的Memory utilization，用户想了解余下的500M CUDA内存是如何占用的以及如何计算。

https://github.com/NVIDIA/TensorRT-LLM/issues/546
这是一个空白的issue，类型未知。

https://github.com/NVIDIA/TensorRT-LLM/issues/545
这是一个bug报告，主要涉及TensorRT-LLM模型推断过程中某些特殊token被跳过导致输出中出现多余的eos token。

https://github.com/NVIDIA/TensorRT-LLM/issues/544
这是一个用户提出需求的类型，主要对象是需要更新GitHub页面。由于GitHub页面可能过时或需要更新，用户提出了这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/543
这是一个关于功能问题的issue，主要涉及TensorRT-LLM下的sliding window attention是否可用，问题在于代码中发现其似乎被禁用了。

https://github.com/NVIDIA/TensorRT-LLM/issues/542
这是一个bug报告，涉及主要对象是TensorRT-LLM引擎，由于maxTokensInPagedKvCache参数设置不足以处理完整序列而导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/541
这是一个 bug 报告，涉及到在构建 TensorRT-LLM 时出现名称错误 'layers_range' 未定义的问题，用户建议在 GenerationMixin 类中将 'layers_range' 更改为 'self.layers_range'。

https://github.com/NVIDIA/TensorRT-LLM/issues/540
这是一个需求更新GitHub页面的类型为其他类型的issue，主要对象是项目的GitHub页面。导致此需求的原因可能是项目页面信息不全或需要更新特定内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/539
这个issue属于需求提出，主要对象是更新TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/538
这是一个用户提出需求的issue，主要涉及LLama 70B模型构建引擎，由于模型大小过大导致内存不足问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/537
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM中chatglm模型配置文件命名不被Triton后端识别的问题，可能因为文件名不符合通用规则导致无法部署在Triton中。

https://github.com/NVIDIA/TensorRT-LLM/issues/536
这个issue属于用户提出问题类型，主要涉及TensorRT-LLM中模型性能的疑问，可能是由于文档内容不一致导致的困惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/535
这是一个bug报告，涉及到TensorRT-LLM的构建过程中出现了文件缺失的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/534
这是一个用户需求问题，涉及TensorRT-LLM中如何使用embedding作为输入的主要对象是LLaVa模型。由于目前的模型仅支持input_ids作为输入，用户希望得到关于如何实现使用embedding作为输入的指导。

https://github.com/NVIDIA/TensorRT-LLM/issues/533
这个issue是一个关于构建脚本失败的bug报告，涉及的主要对象是TensorRT-LLM下的llama7b版本。由于在运行构建脚本时出现失败，导致了提问者无法成功构建该版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/532
这是一个bug报告，主要涉及TensorRT-LLM下的chatglm3模型，问题可能是因为未对`self.address`进行检查而导致的无效cuda地址错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/531
这个issue类型是用户提出需求，主要涉及的对象是Blip2在FlanT5xxl上的支持。用户提出了关于在FlanT5xxl上使用Blip2的需求，并寻求帮助解决相关技术问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/530
该issue属于用户提出问题类型，问题主要涉及到构建Docker时缺少特定版本信息。这可能是由于缺少特定版本的软件包而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/529
这是一个用户提出疑问的类型，涉及主要对象是TensorRT-LLM中的GatedMLP和swiglu。其中，用户想了解为什么存在两种等效实现而没有合并成一种，对于这种情况可能感到困惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/528
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM中的python binding for InferenceRequest。由于toTrtLlm()没有正确复制streaming属性，导致了当前无法进行流式处理的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/526
这是一个性能调优问题，涉及到了在使用TensorRT-LLM时设置max_batch_size参数的影响。造成症状的主要原因可能是参数配置不合理导致性能表现不佳。

https://github.com/NVIDIA/TensorRT-LLM/issues/525
这个issue类型是需求更新，涉及主要对象为aarch64 batch manager libraries，用户希望进行更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/524
这是一个用户需求更新的类型，主要涉及TensorRT-LLM模型支持、特性添加、内存优化和性能提升，用户可能是提出需求或建议进行更新和改进。

https://github.com/NVIDIA/TensorRT-LLM/issues/523
这是一个关于构建Docker环境时出现错误的bug报告，涉及的主要对象是项目的CMake配置。这个问题可能是由于硬件限制所导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/522
这个issue是一个bug报告，主要对象是在TensorRT-LLM下使用Llama-2-7b模型构建4个GPU的AWQ Engine时遇到了tensor维度不匹配错误。这个问题很可能是由于多GPU环境下的维度处理不正确引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/521
这个issue类型为性能问题，主要涉及的对象是gptManagerBenchmark，发生这个问题的原因可能是由于批处理大小不同导致性能表现不同。

https://github.com/NVIDIA/TensorRT-LLM/issues/520
这是一个关于构建T5 encoder时遇到错误的bug报告。主要涉及TensorRT-LLM下的FlanT5示例的构建问题，导致无法找到与输入/输出数据类型一致的支持格式。

https://github.com/NVIDIA/TensorRT-LLM/issues/519
这是一个bug报告，涉及主要对象是TensorRT-LLM的enc-dec engine构建，由于缺少gpt_attention_plugin导致错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/518
这是一个bug报告，主要涉及TensorRT-LLM下的GPTQ引擎构建过程中出现的张量维度不匹配错误。导致这一问题的原因是构建引擎时输入张量维度不对应，无法成功构建引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/517
这个issue属于用户提出需求的类型，主要涉及TensorRT LLM中如何找到第一个标记的延迟问题。用户询问如何在Jetson ORIN上报告不同上下文长度和生成步骤的第一个标记延迟，以供基准测试。

https://github.com/NVIDIA/TensorRT-LLM/issues/516
这是一个用户提出需求的issue，主要涉及的对象是A10图形卡。由于A10图形卡在fp32模式下产生异常结果，在fp16模式下结果几乎正确，用户想了解A10图形卡是否与TensorRTLLM兼容，并希望将其添加到未来版本的兼容图形卡列表中。

https://github.com/NVIDIA/TensorRT-LLM/issues/515
这个issue是关于软件包版本不匹配导致依赖冲突的bug报告，主要涉及TensorRT-LLM工具中的GPTQ实验内容。原因是protobuf和transformers版本要求不一致，导致安装时出现错误并影响实验进行。

https://github.com/NVIDIA/TensorRT-LLM/issues/514
这是一个用户请教问题类型的issue，涉及主要对象是关于调用weight_only_batched_gemv_launcher函数的问题，用户想知道如何直接从C++端调用该函数并寻求相关帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/513
这是一个用户提出需求的问题，主要对象是TensorRT-LLM。用户想知道是否在TensorRT-LLM中支持使用`stopping_criteria`功能，并请求相关示例。

https://github.com/NVIDIA/TensorRT-LLM/issues/512
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的paged kv cache使用情况展示，用户希望能够了解该缓存的利用情况以确定支持的批处理大小。

https://github.com/NVIDIA/TensorRT-LLM/issues/511
该issue类型是用户提出需求，主要涉及对象是TensorRT-LLM中的参数设置。由于在Volta架构平台上不支持enable_context_fmha，用户希望修改max_num_tokens以生成更长的序列，寻求如何设置的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/510
这是一个用户询问问题的issue，主要涉及如何在gptManagerBenchmark中设置采样参数，用户想知道如何设置topk topp参数。

https://github.com/NVIDIA/TensorRT-LLM/issues/509
这个issue是关于构建TensorRT-LLM模型总结代码时出现的bug报告，主要涉及对象是构建过程中的参数解析和配置，可能由于配置文件中的某些设置错误导致文件解析失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/508
这是一个bug报告，主要涉及在Windows环境下无法构建LLM模型，可能由于GPU内存不足导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/507
这是一个bug报告，主要对象是TensorRT-LLM中的Python benchmark功能。导致这个问题的原因是Tensor names在engine中与预期不相符，需要使用GPTLMHeadModel.prepare_inputs以创建TRT Network inputs。

https://github.com/NVIDIA/TensorRT-LLM/issues/506
这是一个需求更新的issue，主要对象是TensorRT-LLM。该问题可能是用户要求更新TensorRT-LLM项目的内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/505
这是一个bug报告，该问题涉及到TensorRT-LLM中的模型转换过程中出现的错误。造成这个问题的原因可能是在进行模型平滑量化时出现了空值错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/504
这个issue属于bug报告类型，主要涉及到Nccl errors在运行codellama 34b on 4 A100 gpus时出现。由于Nccl和CUDA版本不匹配导致了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/503
这是一个bug报告，主要涉及TensorRT-LLM项目下的模型 llama2 70b + int4_gptq 在解码输出方面出现无意义结果的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/502
这是一个bug报告，主要涉及TensorRT-LLM下的模型编译问题，导致同样输入时原始模型和编译模型的输出不一致。

https://github.com/NVIDIA/TensorRT-LLM/issues/501
这是一个关于bug报告的issue， 主要涉及TensorRT-LLM在Kubernetes环境中运行GPT示例脚本时产生僵尸进程的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/500
这是一个bug报告，涉及TensorRT-LLM中的cross attention功能，用户遇到了生成token时logits出现不正确的问题。可能是由于kv缓存导致的输出不符合预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/499
这是一个关于TensorRT LLM Docker的问题报告，涉及到构建docker容器时遇到的多个问题，包括无法连接互联网、构建失败和mpi4py编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/498
这是一个bug报告，涉及到TensorRT-LLM中的一个Failed to run benchmark的问题，由于描述不完整，无法确定具体原因和解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/497
该issue属于文档信息错误类型，主要对象涉及到TensorRT-LLM。由于文档中给出的信息有误，导致用户无法准确了解到"Mistral7Bv0.1"支持的起始版本是4.34.0。

https://github.com/NVIDIA/TensorRT-LLM/issues/496
这是一个bug报告，涉及TensorRT-LLM中无法成功构建per channel smoothquant模型的问题。由于此问题导致了量化结果异常，需要对此进行进一步分析和解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/495
这个issue类型为用户询问问题，主要涉及对象是TensorRT-LLM模型。用户询问是否可以在不同GPU上转移相同的模型，是否需要为不同GPU构建不同的模型。这个问题的出现主要是因为用户对模型的跨GPU可移植性存在疑惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/494
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的模型部署问题，由于maxTokensInPagedKvCache参数设置不正确导致创建模型实例时发生错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/493
这是一个bug报告，主要涉及的对象是TensorRT-LLM。由于内存不足导致某个tactic不能被执行，用户询问是否安全跳过该tactic。

https://github.com/NVIDIA/TensorRT-LLM/issues/492
这是一个用户在Github上针对TensorRT-LLM的issue，提出了关于`no_repeat_ngram_size`参数功能的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/491
这个issue是关于bug报告，涉及到TensorRT-LLM中的内存泄漏问题，导致在运行generate函数时，每次循环后GPU内存使用量会不断增加。

https://github.com/NVIDIA/TensorRT-LLM/issues/490
这是一个bug报告，涉及TensorRT-LLM中的GenerationSession.log_probs大小错误的问题，由于log_probs数组的大小设置不正确，导致写入数据时发生错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/489
这是一个bug报告，涉及的主要对象是TensorRT-LLM编译过程，问题可能源于无效的时间缓存导致错误消息的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/488
这是一个bug报告，主要涉及TensorRT-LLM在Jetson Orin NX上运行失败的问题，可能是由于NVIDIA驱动版本过旧导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/487
这是一个关于编译cpp版本基准测试的issue，类型为bug报告，主要对象是TensorRT-LLM的编译和基准测试功能。原因是安装文档中提到的文件或链接可能已经过时或不存在，导致用户无法找到cpp/build文件夹。

https://github.com/NVIDIA/TensorRT-LLM/issues/486
这是一个用户提出的需求类型的issue，主要涉及到TensorRT-LLM中如何控制GPU内存用量以限制KV缓存分配，用户希望能够为每个trtllm实例限制内存使用，由于在运行多个实例或模型时遇到内存使用问题而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/485
这是一个bug报告，主要涉及了stop_words无效和infilling功能的问题，用户询问最新版本何时发布可以完全解决stop_words不起作用的问题，并询问当前版本是否支持infilling。

https://github.com/NVIDIA/TensorRT-LLM/issues/484
这个issue属于bug报告，主要涉及TensorRT-LLM在推理llama-2-13b-chat-hf模型时出现"no kernel image is available for execution on the device"错误，可能是由于缺少必要的CUDA kernel image导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/483
这是一个关于代码实现的疑惑，主要涉及到 FP8Linear 和 FP8RowLinear 的量化和去量化操作的顺序问题，用户认为在执行 `multiply_gather` 或 `multiply_reduce` 前是否应该进行 GEMM 操作。可能由于操作顺序不符导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/482
这是一个用户提出需求的issue，主要涉及运行Mistral-7B模型时不同tag的差异情况。用户询问在release/v0.5.0标签下运行Mistral-7B是否只有FasterTransformer加速而不需要KV缓存，寻求相关帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/481
这是一个用户提出需求的问题，主要涉及到如何将多个prompt传递给run.py llama这个脚本。这个问题可能由于用户想同时处理多个prompt而导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/480
这是一个用户提出需求的问题，主要涉及的对象是TensorRT-LLM下的llama2模型，用户想要设置系统提示来生成输出。由于系统未提供相关设置选项，用户无法设置系统提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/479
这个issue是关于bug报告，涉及的主要对象是TensorRT-LLM中的Attention层。由于在使用`gpt_attention_plugin`时遇到了`Segmentation fault`错误，推测可能是在处理`cross_past_key_value_0`和`past_key_value_0`参数时出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/478
这是关于TensorRT-LLM中不同版本和批处理方式之间的区别的讨论，属于用户寻求功能解释和帮助的类型。该问题涉及到TensorRT-LLM中的V1版本、InflightBatching和InflightFusedBatching的区别，以及关于paged kv缓存的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/477
这是一个用户询问如何在已通过pip安装TensorRT的情况下构建wheel的问题，属于用户提出需求询问类型。用户遇到了由于已通过pip安装TensorRT而需要通过源文件构建wheel的困惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/476
这是一个bug报告，涉及TensorRT-LLM在与huggingface chatglm2-6b冲突时出现的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/475
这个issue是一个bug报告，涉及mpi4py。由于该issue没有具体描述具体的问题，只提供了相关链接，可能是在使用TensorRT-LLM过程中遇到了与mpi4py相关的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/474
这个问题是关于bug报告，涉及TensorRT-LLM中flan-t5-xxl的使用问题，用户遇到了无法获得正确输出的情况，怀疑可能是flan-t5-xxl的支持问题或者构建和运行代码的异常导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/473
这个issue类型为用户提出需求，关联的主要对象是统计TensorRT-LLM模型中第一个token和后续token的延迟，原因可能是用户想要了解模型在不同情况下的延迟统计情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/472
这是一个关于性能问题的bug报告，主要涉及TensorRT-LLM下的ATQ在使用tensor parallelism时的性能下降问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/471
这是一个用户提出安装TensorRT-LLM时权限问题的issue，涉及主要对象是安装TensorRT-LLM的过程。由于缺少docker权限，导致用户无法完成安装过程。

https://github.com/NVIDIA/TensorRT-LLM/issues/470
这是一个bug报告，该问题涉及的主要对象是GPTLMHeadModel模型在使用--use_embedding_sharing和--gather_all_token_logits两个标志构建engine时会出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/469
这是一个关于bug报告的issue，主要涉及TensorRT-LLM中处理slice操作时出现的问题，用户想实现的`x.slice[:, 1:, :]`操作导致了错误，可能是由于对数组形状中出现`-1`和`1`的处理不当所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/467
这个issue类型为用户提出需求，请教问题，主要涉及的对象为TensorRT-LLM下的`text_output`功能，用户希望能够让返回的值不再包含提示信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/466
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的Cross Attention layer。导致这个问题的原因是beam size大于1时在CUDA内存中出现非法访问错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/465
这是一个包含Bug修复和新功能支持的GitHub issue，主要涉及TensorRT-LLM项目的更新。由于某些问题导致了一些Bug和功能需求的提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/464
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM的构建过程。由于之前可成功构建，但在重新构建时出现了安装`mpi4py`时的错误，可能是由于依赖关系或环境配置问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/463
这是一个关于软件版本发布时间的用户需求类问题，用户想了解0.6.0版本的发布时间。

https://github.com/NVIDIA/TensorRT-LLM/issues/462
这是一个用户提出需求的issue，主要涉及如何实现在TensorRT-LLM中使用INT4 conv插件，其中问题包括INT4数据类型生成和张量格式转换。

https://github.com/NVIDIA/TensorRT-LLM/issues/461
这是一个关于功能问题的issue，主要涉及对象是TensorRT-LLM下的context_fmha设置和cross_attention。由于设置条件限制导致在特定情况下内存未正确释放，可能影响Gem操作的执行或功能性问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/460
这是一个bug报告，主要对象是LLaMA 7B AWQ Quantaization指令。由于quantization指令使用错误，导致生成的文件夹不正确，运行summarization时出现分数为0的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/459
这个issue类型是用户提出需求，请添加类似huggingface的StoppingCriteria和LogitsProcessor功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/458
这是一个用户就TensorRT-LLM是否支持L40 GPU Ada结构提出的问题，属于用户询问功能支持的类型。

https://github.com/NVIDIA/TensorRT-LLM/issues/457
这是一个关于功能运作机制的问题，涉及主要对象是TensorRT-LLM下的Attention kernels，由于文档描述和CUDA kernel源代码结构之间的混淆，用户想了解context和generation阶段是否调用不同的attention kernel实现。

https://github.com/NVIDIA/TensorRT-LLM/issues/456
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于在使用 mpirun 启动程序时出现了错误，导致 rank2 web 服务器抛出异常，可能是由于程序部署为 web 服务器时出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/455
这是一个bug报告，主要涉及的对象是TensorRT-LLM的benchmark测试。问题可能是由于Python运行时与C++运行时在构建过程中接口不一致导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/454
这是一个关于构建TensorRT-LLM wheel时遇到错误的问题，属于技术支持类型，涉及的主要对象是项目依赖和构建过程。可能是由于缺少依赖或配置错误导致构建失败，用户在寻求解决此问题所需的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/453
这是一个bug报告，主要涉及TensorRT-LLM模型 smoothquant 和 int8_weight_only 在人类评估中得分过低的问题，可能由于推断结果中出现了大量换行符造成。

https://github.com/NVIDIA/TensorRT-LLM/issues/452
这是一个bug报告，主要涉及TensorRT-LLM中的Plugin创建问题，导致无法找到Gemmtensorrt_llm插件的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/451
这是关于功能比较的问题，主要涉及到TensorRT和vLLM的批量处理技术差异。用户询问在什么场景下应该使用TensorRT，以及在什么场景下应该使用vLLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/450
这是一个bug报告，涉及的主要对象是在部署模型时遇到内存不足问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/449
这个issue是关于bug报告，涉及的主要对象是OpenChat 3.5 language model。由于未初始化权重导致TensorRT构建过程中出现了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/448
这个issue是关于bug报告的，主要涉及TensorRT-LLM在部署70B模型时出现NCCL错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/447
这是一个bug报告，涉及到TensorRT-LLM项目下的docker release_build命令执行失败的情况。导致问题的原因可能是环境配置或代码变动引起的兼容性问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/446
这是一个开发者提出需求的issue，涉及主要对象是TensorRT-LLM中的新插件实现和应用，开发者询问是否有更快的方法来验证新插件是否正常工作。

https://github.com/NVIDIA/TensorRT-LLM/issues/445
这是一个bug报告，主要涉及TensorRT-LLM项目的构建失败。由于某些原因导致无法成功构建llama，用户需要解决构建失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/444
这是一个关于bug报告的issue，主要涉及的对象是enc_dec例子中的context phase。由于修改相关代码导致运行时出现错误，用户提出是否context phase应该被添加到enc_dec例子中的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/443
这是一个bug报告类型的issue，涉及主要对象是TensorRT-LLM。原因可能是在调用SplitInfo类的初始化函数时传入了一个意外的关键字参数'shard_lengths'。

https://github.com/NVIDIA/TensorRT-LLM/issues/442
这是一个Bug报告类型的Issue，主要涉及libnvinfer_plugin_tensorrt_llm.so无法打开共享对象文件，可能由于文件路径错误或缺失导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/441
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的generation decode函数参数问题导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/440
这是一个bug报告，主要涉及TensorRT-LLM下的AWQ for llama 70B的使用问题，由于在8xA100上执行引擎时出现冲突导致无法正确运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/439
这个issue属于用户询问问题类型，主要涉及对象是TensorRT-LLM中的FlashAttention集成。用户询问是TensorRT-LLM中的FlashAttention集成使用的是版本1还是已更新至新版本FlashAttention 2，可能由于版本更新引起功能差异导致疑惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/438
这是一个bug报告，涉及TensorRT-LLM在使用docker构建过程中遇到的错误。由于切换源代码分支导致编译过程出现错误，反馈了构建TensorRT-LLM时的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/437
这是一个用户提出需求的issue，主要涉及的对象是在docker中构建TensorRT-LLM的过程中遇到的问题。由于没有传递cuDNN相关的环境参数给cmake，导致了构建时的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/436
这是一个bug报告类型的issue，主要涉及到在构建TensorRT-LLM模型时出现的设备内存不足问题，导致构建过程中被自动终止。

https://github.com/NVIDIA/TensorRT-LLM/issues/435
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM下的llama-2 engine。由于缺少有效的SQ GEMM tactic，导致出现了Assertion failed错误，用户寻求关于无法推理llama-2 engine的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/434
这是一个bug报告，主要对象是TensorRT-LLM下的安装过程。该问题可能由于Docker构建过程中出现的错误导致执行失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/433
这是一个用户提出需求的问题，主要关注了支持分布式推理在多个节点上运行的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/432
这是一个bug报告，主要涉及到在构建TensorRT-LLM时缺少对cuDNN相关环境的传递。

https://github.com/NVIDIA/TensorRT-LLM/issues/431
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM，在inflight-batching模式下Iteration Counter计数为生成的令牌数的两倍，可能是由于计算或逻辑错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/430
这是一个bug报告，涉及的主要对象是TensorRT-LLM的chatglm2-6b模型，由于函数`load_from_hf_chatglm2_6B`没有使用`tensor_parallel`参数，导致构建时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/429
这个issue类型是构建错误报告，主要对象是TensorRT-LLM项目。由于CMake生成步骤失败，导致无法正确重建构建文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/428
这是一个用户提出需求的issue，主要涉及添加一个说明TensorRT-LLM和TensorRT之间关系的图表。原因可能是为了提供更直观的文档帮助理解两者之间的关系。

https://github.com/NVIDIA/TensorRT-LLM/issues/427
这个issue属于bug报告类型，主要涉及TensorRT-LLM中使用medium batch sizes时出现非法内存访问问题，导致整个引擎冻结。

https://github.com/NVIDIA/TensorRT-LLM/issues/426
这是一个bug报告，主要涉及TensorRT-LLM下的量化功能，用户提到使用`INT4_AWQ_CFG`和`INT4_BLOCKWISE_WEIGHT_ONLY_CFG`导致较大的量化误差。

https://github.com/NVIDIA/TensorRT-LLM/issues/425
这是一个bug报告，主要涉及TensorRT-LLM下的llama模型，用户在使用过程中发现生成结果缺少预期的结束符号。

https://github.com/NVIDIA/TensorRT-LLM/issues/423
这是一个bug报告，主要涉及TensorRT-LLM中部署两个LLama2模型时遇到的MPI错误。原因可能是两个tensorrt_llm模型分别启动了MPI初始化，导致在同一模型存储库中启动两个模型时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/422
这个issue是关于Bug报告，主要涉及TensorRT-LLM的模型支持和功能更新，因为部分功能存在错误和需要修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/421
这个issue类型是性能问题报告，主要涉及TensorRT-LLM下H100 FP8性能测试与预期性能之间的差异。原因可能是使用了不适合的fp8模式导致性能表现不佳。

https://github.com/NVIDIA/TensorRT-LLM/issues/420
这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持数据并行或多进程推理，其问题在于无法正确映射TensorRT实例到不同GPU导致无法进行多GPU推理。

https://github.com/NVIDIA/TensorRT-LLM/issues/419
这个issue类型是Bug报告，涉及主要对象是`TensorRT-LLM`，由于未授权的错误导致了401 Unauthorized状态的出现。

https://github.com/NVIDIA/TensorRT-LLM/issues/418
这是一个关于性能问题的bug报告，用户在使用TensorRT-LLM的过程中发现获取上下文logits导致了80%的延迟增加，主要涉及的对象是llama7B模型和A10 GPU，可能由于特定操作导致了性能问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/417
这是一个bug报告，涉及到baichuan convert script缺少部分数值保存，导致了构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/416
该issue为bug报告。主要涉及TensorRT-LLM中构建chatglm-6b hf模型转换成trt engines时出现的错误。错误原因可能是网络中使用了相同值但不同数量的权重。

https://github.com/NVIDIA/TensorRT-LLM/issues/413
这是一个关于询问CUDA graph mode的类型为问题咨询，主要涉及TensorRT-LLM项目中关于CUDA graph mode的使用及其限制的问题，用户想了解这种模式的优势和劣势。

https://github.com/NVIDIA/TensorRT-LLM/issues/412
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的Bert模型。问题产生的原因是传入的position_ids参数导致模型推理出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/411
该issue属于用户提出问题类型，主要涉及Triton Server的最大输出长度的设置，可能是由于请求输出令牌数量超过最大输出长度限制所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/410
这是一个bug报告，涉及代码中一个变量名错误导致程序逻辑错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/409
这是一个bug报告，该问题涉及TensorRT-LLM中的MPI world size设置，用户询问如何修改 MPI world size 以解决引擎和运行时 world size 不匹配的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/408
这是一个bug报告，主要涉及TensorRT-LLM下运行时出现NCCL错误的问题。由于NCCL通信组件出现内部错误，导致运行时失败并提示报告该问题给NCCL开发者。

https://github.com/NVIDIA/TensorRT-LLM/issues/407
这个issue属于对代码优化的需求，主要对象是CUDA kernel performance相关的问题，提问者想了解是否有计划优化加载kvcache的次数。

https://github.com/NVIDIA/TensorRT-LLM/issues/406
这是一个用户提出问题的issue，询问有关如何获得第一个令牌延迟的最佳实践。

https://github.com/NVIDIA/TensorRT-LLM/issues/405
这是一个bug报告，涉及主要对象为TensorRT-LLM中的load_from_gptq_llama函数。由于未能正确处理dimension并进行拼接，导致出现了无法加载codefuse34bint4 model的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/404
这是一个bug报告，涉及到TensorRT-LLM下的MHA的E2E latency异常较大的问题，主要原因是inflighting batching中MQA/GQA/MHA的实际批处理大小不同。

https://github.com/NVIDIA/TensorRT-LLM/issues/403
这是一个bug报告，主要涉及AWQ量化过程中出现cuda_ext导入错误，导致无法成功运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/402
这是一个关于功能问题的issue，主要涉及到TensorRT-LLM的batch manager，用户想了解该功能是否支持cuda graph，并表达了针对高延迟的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/401
这是一个关于OOM错误的bug报告，主要涉及TensorRT-LLM模型加载时内存不足的问题。造成这一问题的原因可能是缺乏内存管理机制或者缓存策略不足以应对大批量数据加载时的内存需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/400
这是一个bug报告，涉及TensorRT-LLM在g5.12xlarge上构建docker镜像失败的问题，可能是由于系统环境不兼容导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/399
这是一个关于Dockerfile设置的问题，不是bug报告。该问题涉及到TensorRT-LLM的Dockerfile中最终阶段基于`devel`而不是`base`，导致最终发行镜像包含所有构建时的依赖项。

https://github.com/NVIDIA/TensorRT-LLM/issues/398
这是一个Bug报告，用户反映在启动TensorRT-LLM Docker容器时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/397
这是一个用户请求代码位置相关的问题，主要对象是TensorRT-LLM项目。用户可能因为想要查看代码而提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/396
这是一个bug报告，主要涉及Llama7b Int4在Nvidia T4上的输出问题，可能是由于构建和服务的过程中出现的错误导致输出不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/395
这是一个bug报告，用户在尝试在2个RTX4090上并行运行llama13b模型时出现了运行失败的问题，并试图在TensorRT-LLM中解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/394
这是一个用户提出的问题，主要涉及TensorRT-LLM中kv cache int8的效果检查和自定义数据集的操作指南。用户想要确认kv cache int8是否生效，并寻求如何在自定义数据集上进行kv cache量化的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/393
这个issue是关于bug报告，涉及到Triton Server与LLaMA2-13B模型的结果不匹配的问题，用户在传递采样参数给triton server模型实例时无效果，寻求解决方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/392
这是一个bug报告，主要涉及的对象是使用8 X V100 GPU运行bloomz2b6。由于不同batch size导致的线性增加的延迟可能是由于错误用法或者V100不够好地支持批处理功能导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/391
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的baichuan2-7b模型的量化过程中在启动tritonserver时出现问题。由于无法正确启动tritonserver，用户提出这个问题并寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/390
这是一个bug报告，主要涉及TensorRT-LLM下的chatglm3模型运行出错。导致这个问题的原因可能是模型参数配置错误或代码实现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/389
这是一个bug报告，涉及使用TensorRT-LLM时在多GPU环境下并行运行效率低于单GPU的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/388
这是一个关于库文件加载机制的问题，而非bug报告。提问者关注的是`libtensorrt_llm.so`在Python中的加载情况，却发现实际加载的是`libnvinfer_plugin_tensorrt_llm.so`，想要了解`libtensorrt_llm.so`的加载机制和时机。

https://github.com/NVIDIA/TensorRT-LLM/issues/387
这是一个bug报告，主要涉及TensorRT-LLM在编译autogptq quantized模型时出现的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/386
这是一个 bug 报告类型的 issue，主要涉及 TensorRT-LLM 中的 Rouge 数值不同。导致这一问题的可能原因是在不同 GPU 数量下进行 FP8 量化测试时出现的差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/385
这是一个用户提出需求的问题，涉及的主要对象是TensorRT-LLM和transformers库。由于requirements文件中指定了固定的transformers版本，用户询问是否有计划使用最新版本或者是否存在必须固定特定版本的限制。

https://github.com/NVIDIA/TensorRT-LLM/issues/384
这个issue类型是用户提出需求类型，主要对象涉及TensorRT-LLM下的添加rwkv模型，用户提出需要添加rwkv模型功能并进行相关工作分解，目前尚未完成部分功能的开发。

https://github.com/NVIDIA/TensorRT-LLM/issues/383
这是一个bug报告，主要涉及到TensorRT-LLM的Int8 weight转换过程中出现了无法reshape数组的数值大小错误，导致数值无法正确转换的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/382
这是一个性能问题报告，针对TensorRT-LLM中批处理时生成第一个令牌延迟随批处理大小线性增加的现象进行了描述和咨询。

https://github.com/NVIDIA/TensorRT-LLM/issues/381
这是一个关于构建TensorRT-LLM下Chatglm3-6b模型引擎失败的bug报告，主要涉及的对象是构建引擎时的`build.py`脚本。这个问题可能由于代码hang导致进程无法顺利执行，用户在运行过程中无法得到任何输出，最终不得不手动终止进程。

https://github.com/NVIDIA/TensorRT-LLM/issues/380
这是一个Bug报告，主要涉及到了GPT-J模型在使用两个GPU进行推理时出现的大小不匹配问题，可能是由于源代码中的tp_size导致了尺寸不匹配的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/379
这是一个用户提出需求的类型，主要对象是更新最新新闻，此问题可能由于信息更新不及时或者缺失导致用户需要更新最新的消息。

https://github.com/NVIDIA/TensorRT-LLM/issues/378
这个issue类型为用户提出需求，该问题单涉及的主要对象是项目的最新消息更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/377
这是用户提出需求的类型，主要涉及对象为构建HF格式模型。用户询问是否可以直接使用AWQ作为TensorRT引擎来构建已通过AWQ量化的模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/376
这个issue属于Bug报告类型，涉及的主要对象是TensorRT-LLM。由于值更新后与原始值的形状不同导致的AssertionError。

https://github.com/NVIDIA/TensorRT-LLM/issues/375
这是一个关于bug报告的issue，主要涉及的对象是TensorRT-LLM。由于出现错误"newSize <= getCapacity()"，用户在调用endpoint时遇到问题，需要寻求解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/374
这是一个bug报告，主要涉及TensorRT-LLM下的模型构建过程中world_size为2时出现错误。由于array split不会产生相等的分割，导致了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/373
这是一个bug报告，涉及对象是在构建AWQ格式codallama-13b-base引擎时遇到的问题；由于填充长度过大导致构建引擎失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/372
这个issue属于bug报告类型，主要涉及的对象是TensorRT-LLM下的Yi模型，可能由于最新main分支的更改导致inference出现错误，可能是TRTLLM框架中存在bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/371
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的groupwise_qweight_safetensors加载问题，由于weight.py仅支持.gptqint4而不支持.pt格式的safetensors，导致了无法加载.pt文件的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/370
这是一个 bug 报告，涉及的主要对象是 TensorRT-LLM。原因可能是 In-flight batching 和 PagedAttention 功能并未带来性能提升。

https://github.com/NVIDIA/TensorRT-LLM/issues/369
这是一个bug报告类型的issue， 主要涉及TensorRT-LLM的runtime error问题，可能是由于最新主分支上的命名不匹配导致了该错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/368
这个issue属于用户提出需求类型，主要涉及添加最新消息部分的功能要求。

https://github.com/NVIDIA/TensorRT-LLM/issues/367
这个issue类型为用户提出需求，希望添加最新消息部分，主要涉及到TensorRT-LLM项目。

https://github.com/NVIDIA/TensorRT-LLM/issues/366
这个issue类型是用户提出需求，该问题单涉及的主要对象是添加Latest News section。由于当前项目缺少最新动态信息展示，用户在此提出需求添加Latest News section。

https://github.com/NVIDIA/TensorRT-LLM/issues/365
这是一个需求类型的issue，主要对象是添加最新消息模块。

https://github.com/NVIDIA/TensorRT-LLM/issues/364
这是一个用户提出需求的问题，主要涉及到TensorRT-LLM中decode函数的参数设置。用户希望添加attention_mask参数来指示padding位置并进行padding token的mask处理，以适应不同长度句子的输入。

https://github.com/NVIDIA/TensorRT-LLM/issues/363
该issue类型为用户提出需求，主要涉及对象为如何将一个Tensor类型对象转换为torch.Tensor，用户提出了关于如何在TensorRTLLM中实现特定功能的问题，并寻求相应的API或编写代码的方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/362
这是一个用户提出需求的问题，主要对象是在TensorRT-LLM项目中添加一个"Latest News"部分。

https://github.com/NVIDIA/TensorRT-LLM/issues/361
这是一个用户提出需求的issue，主要涉及到在TensorRT-LLM项目中添加最新消息部分。可能是希望能够及时了解项目的新动态。

https://github.com/NVIDIA/TensorRT-LLM/issues/360
这是一个用户提出需求的issue，主要涉及 MBartForCausalLM 的支持。用户请求提供 MBartForCausalLM 的示例。

https://github.com/NVIDIA/TensorRT-LLM/issues/359
这是一个bug报告，涉及的主要对象是TensorRT-LLM。由于在执行build_wheel.py脚本时出现了cmake命令返回非零退出状态1的错误，导致无法成功构建TensorRT-LLM，用户寻求关于此问题的帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/358
这是一个bug报告，涉及的主要对象是Smoothquant conversion过程中转换Llama-2-70b模型失败。造成这个问题的原因可能是硬件环境为AWS EC2 p4de.24xlarge导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/357
这是一个文档错误修正的issue，涉及的主要对象是TensorRT-LLM文档。原因是在文档中出现了"TensorRTLMM"的错别字，用户提出了修正为"TensorRTLLM"的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/356
这个issue类型是用户提出疑问，主要对象是TensorRT-LLM中的`encoder_input_lengths`变量。根据描述，用户提出了关于`encoder_input_lengths`在解码器自回归生成中的作用以及不同长度导致不同输出的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/355
这是一个bug报告，主要涉及到TensorRT-LLM中的gptManagerBenchmark.cpp文件，用户提出了一个关于生成模型在输出tokens后继续生成的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/354
这是一个用户提出需求和寻求帮助的类型，主要涉及TensorRT-LLM中的inflight_batching功能。由于用户不清楚批处理管理器和inflight_batching之间的区别，以及源代码的开放情况，因此提出了这些问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/353
这是一个Bug报告，主要对象是TensorRT-LLM中的int4_gptq功能。由于无法找到合适的GEMM配置，导致了错误警告信息的输出。

https://github.com/NVIDIA/TensorRT-LLM/issues/352
这是一个bug报告类型的issue，涉及的主要对象是在构建TensorRT-LLM引擎时出现的内存不足问题。原因是插件节点所需的缓存空间超出了可用空间限制导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/351
这是一个用户提出需求的issue，主要涉及添加对Flan-T5模型的支持。这个需求的提出是为了适配FlanT5模型的特定结构和激活函数。

https://github.com/NVIDIA/TensorRT-LLM/issues/349
这是一个类型为更新请求的issue，主要涉及的对象是TensorRT-LLM。可能由于项目需要更新或者改进功能而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/348
这是一个bug报告，涉及的主要对象是llama2在V100 GPU上的不支持架构，可能是由于编译或硬件兼容性问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/347
这是一个Bug报告，主要涉及TensorRT-LLM中的推理结果和HF模型结果不一致的问题，可能由于输入文本格式不同导致了TRT模型部分推理结果为空的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/346
这是一个bug报告，问题涉及的主要对象是TensorRT-LLM中的build.py脚本。导致这个bug的原因是内存不足引起的OutOfMemory错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/345
这个issue属于用户提出需求类型，主要涉及的对象是AutoAWQ support。

https://github.com/NVIDIA/TensorRT-LLM/issues/344
这是一个关于如何构建GPU内核调试版本的问题报告，涉及主要对象为TensorRT-LLM项目。由于缺少正确的编译选项和链接设置，用户无法调试到内核函数并遇到了构建库时的链接错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/343
这是一个关于GPU内存占用问题的bug报告，主要涉及到TensorRT-LLM下的llama-13b模型。由于加载模型后占用过多的GPU内存，用户希望知道原因并寻求解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/342
这是一个用户询问如何在没有使用Docker容器的情况下构建项目的问题。该问题涉及到项目构建时依赖Docker容器的限制。

https://github.com/NVIDIA/TensorRT-LLM/issues/341
这个issue属于bug报告类型，涉及的主要对象是TensorRT-LLM项目，出现这个问题是由于fp16 pipeline在特定分支版本上出现错误，需要寻求帮助解决。

https://github.com/NVIDIA/TensorRT-LLM/issues/340
该issue是一个bug报告，主要涉及TensorRT-LLM下的Inflight batching功能，由于指定end_id或输入中包含转义双引号等导致整个服务器冻结，可能由于代码中的潜在问题导致这些症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/339
这是一个bug报告，涉及主要对象为TensorRT-LLM下的trtGptModelInflightBatching.cpp文件，由于缺少ompi_mpi_comm_world的引用导致链接错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/338
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的C++ Benchmark工具。用户在运行benchmark时遇到了无法打开引擎文件的错误，可能是由于引擎文件路径错误或生成引擎文件过程中出现了问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/337
这是一个用户提出需求的issue，主要涉及的对象是TensorRT 9和PyTorch NGC。由于PyTorch NGC容器23.10不支持TensorRT 9，用户询问是否23.11容器会支持TensorRT 9。

https://github.com/NVIDIA/TensorRT-LLM/issues/336
这是一个bug报告，主要涉及的对象是通过trtexec来对engine进行性能分析时出现coredump错误的问题。可能是由于无法下载任何文件，只能使用trtexec或nvprof等Profiler，因此用户寻求其他性能分析建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/335
该issue属于Bug报告类型，主要涉及的对象是TensorRT-LLM中的bfloat16解码器。由于bfloat16解码器在创建过程中出现问题，导致用户无法成功运行模型，最终引发了segment fault的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/334
这是用户提出的需求，希望提供与OpenAI API类似的简单接口。由于用户希望使用类似OpenAI API的接口，可能是为了方便使用TensorRT-LLM模型进行开发和部署。

https://github.com/NVIDIA/TensorRT-LLM/issues/333
这是一个bug报告，涉及主要对象是TensorRT-LLM下的llama27bhf model。由于转换成hf fast transformers格式时出现错误导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/332
这是关于 bug 报告，主要对象是 GPTBenchmark 对象，由于缺少属性 num_kv_heads 导致了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/331
这个issue是关于环境配置问题而非bug报告，主要涉及对象是TensorRT-LLM的模型编译过程。由于GPU驱动版本过低导致模型编译失败，用户寻求解决TensorRTLLM与较低驱动版本兼容的方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/330
这是一个关于构建中遇到形状不匹配错误的bug报告，涉及主要对象为在LLama213B模型上使用平滑量化。这个问题可能是由于参数配置或代码执行逻辑不正确导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/329
这是一个bug报告，主要涉及的对象是LLaMA 7b模型在H100SXM上无法达到所宣称的性能。导致这个问题的原因可能是代码实现或者配置参数的设置不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/328
这是一个bug报告，涉及到使用TensorRT-LLM时在tritonserver和summarize.py脚本测试相同模型时内存占用量不一致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/327
这是一个用户提出需求的问题，涉及的主要对象是TensorRT-LLM模型加载过程。用户想要在Python中释放GPU内存，而不需要终止Python进程。

https://github.com/NVIDIA/TensorRT-LLM/issues/326
这是一个bug报告，主要涉及Smoothquant model以及其构建过程中的空间和内存使用问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/325
这是一个用户提出需求的问题，主要涉及到TensorRT-LLM中gemm_config.in文件的生成问题。用户询问是否在TensorRT-LLM中仍然需要手动生成gemm_config.in文件，并希望了解是否TensorRTLLM能够自动选择最佳的GEMM算法。

https://github.com/NVIDIA/TensorRT-LLM/issues/324
这是一个用户提出需求的类型，主要涉及到TensorRT-LLM下StarCoder的SmoothQuant变体支持问题，用户询问当前构建StarCoder GPT变体在应用smooth quant时失败，并询问是否有计划支持该功能以及如何解决此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/323
这是一个用户提出需求的issue，主要涉及在TensorRT-LLM中支持非LLM transformer网络的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/322
这是一个bug报告，主要涉及TensorRT-LLM下int8 + vicuna-7b模型输出错误的问题，可能由于模型配置或代码实现错误导致输出不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/321
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的一个issue。由于context/output_len过长时，批处理大小为70时的吞吐量会下降。

https://github.com/NVIDIA/TensorRT-LLM/issues/320
这是一个bug报告，涉及主要对象是TensorRT-LLM下的FMHA功能在V100和T4上不被支持，可能由于代码中未支持V100架构导致此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/319
这是一个bug报告，涉及主要对象是在TensorRT-LLM项目中使用multiGPU时构建Chatglm26B engine失败的问题。导致该问题可能是由于设置world_size=2时出现了无法构建的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/318
这个issue类型是bug报告，主要涉及TensorRT-LLM下的memory leak问题。由于在转换和构建`llama7bhf`模型时使用了int8 kv cache和权重，导致在运行过程中存在内存泄漏，每次提交一个batch时观察到大约2~3GB内存增长。

https://github.com/NVIDIA/TensorRT-LLM/issues/317
这个issue是一个用户提出需求类型的问题，主要涉及的对象是TensorRT-LLM与Deepspeed之间的通过动态拆分融合实现2倍吞吐量的比较。通过问题描述可以看出用户想要实现类似于Deepspeed的动态拆分融合功能，但不确定是否能在TensorRT中实现，可能由于两者的批处理机制不同导致了这个问题的提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/316
这是一个关于Triton server启动失败的bug报告，主要涉及TensorRT-LLM模型的部署和启动Triton server过程中出现的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/315
这是一个用户提出需求的issue，主要对象是在TensorRT-LLM中添加一个最新消息（Latest News）部分。

https://github.com/NVIDIA/TensorRT-LLM/issues/314
这是一个用户提出需求的issue，主要对象是项目的Latest News部分。 由于该部分还未添加相关内容，用户希望在其中加入最新消息。

https://github.com/NVIDIA/TensorRT-LLM/issues/313
这是一个Bug报告，涉及主要对象为LLAMA 2 70B模型，由于某些原因导致无法使用命令行构建使用per channel smoothquant特性的引擎，因此出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/312
这个issue是用户提出需求。主要对象是将W8A8引擎转换为ChatGLM2-6b，但没有找到将huggface转换为fastertransformer的方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/311
这是一个报告bug的issue，主要涉及Baichuan2-7B-Chat-4bits模型构建时出现的错误。原因可能是模型权重未绑定导致的错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/310
这个issue类型是用户提出需求，主要涉及的对象是LLM模型。由于LLM作为多模态模型的一部分时，需要支持soft_prompt或inputs_embeds，因此用户提出了这个功能需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/309
这是一个类型为技术支持的issue，主要涉及到更新Windows上的Torch版本。由于旧版本存在问题或功能需求，用户希望更新Torch版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/308
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Batch Manager开源问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/307
这是一个bug报告，主要涉及TensorRT-LLM库中的"Import Error: from tensorrt_llm.builder import Builder"错误，用户询问如何解决这个错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/306
这是一个bug报告，涉及内容是在构建docker镜像时出现"Unauthorized error"的问题。造成这种症状的原因可能是权限相关的设置问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/304
这是一个bug报告，主要涉及TensorRT-LLM下的一个测试脚本运行失败的问题，可能是由于某些配置参数引发的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/303
这是一个关于需求的问题，主要涉及TensorRT-LLM中的python benchmark，用户询问是否支持pipeline parallel和在运行benchmark时是否使用了tensor parallel和rank是8。

https://github.com/NVIDIA/TensorRT-LLM/issues/302
这个issue属于更新请求类型，主要涉及TensorRT-LLM项目。由于项目版本可能需要更新或修复某些功能，用户提出了更新请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/301
这是一个bug报告，主要涉及到TensorRT-LLM中的codellama模型的word embedding层，由于wordembedding的大小不是64的倍数导致构建错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/300
这个issue类型是用户提出需求，请添加新的数据集支持。该问题单涉及的主要对象是在TensorRT-LLM下运行MMLU、TruthfulQA和ARC数据集。由于缺少后端支持，用户无法在LMevaluationharness中运行这些数据集。

https://github.com/NVIDIA/TensorRT-LLM/issues/299
这是一个bug报告，主要涉及TensorRT-LLM中的TRT engine，在测试生成过程中出现core dump和异常的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/298
这是一个bug报告，涉及TensorRT-LLM的release_build构建错误。造成这个问题的原因是构建过程中出现了一些无法识别的字符。

https://github.com/NVIDIA/TensorRT-LLM/issues/297
这是一个bug报告类型的issue，涉及的主要对象是TensorRT-LLM的容器构建步骤。由于缺少必要的库或依赖关系，导致构建过程中出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/296
这是一个关于CMake Generate步骤失败的bug报告，主要涉及TensorRT-LLM下的编译脚本。由于requirements.txt中指定的依赖已被满足，可能是由于其他环境配置或参数设置问题导致了CMake Generate步骤失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/295
这个issue是用户提出的一个需求，主要涉及到是否能够支持Ptuning功能，其中提到了Ptuning V2 for Chatglm，用户希望在TensorRT-LLM中实现这一功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/294
这是一个bug报告，主要涉及TensorRT-LLM下运行in-flight batching benchmark时遇到错误。由于缺少适合的数据集，导致无法成功运行benchmark。

https://github.com/NVIDIA/TensorRT-LLM/issues/293
这是一个bug报告，主要涉及TensorRT-LLM中的fp8 llama2量化脚本语法错误导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/292
这是一个用户提出需求的类型，主要对象是交互式生成功能。由于用户希望了解有关交互式生成功能的计划，因此提出了这个需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/291
这是一个用户提出需求的issue，主要涉及到TensorRT-LLM的BertModel示例构建脚本，用户提出了添加对加载预训练模型的支持的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/290
这是一个bug报告，用户在构建TensorRTLLM时遇到了问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/289
这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Python runtime，问题是关于如何查看在Python运行时中添加的日志打印，可能是由于日志打印未生效或未正确配置导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/288
这是一个bug报告，主要涉及TensorRT-LLM中的FP8量化问题，用户反馈在尝试量化70亿参数的模型时，即使使用4个80GB的A100显卡仍然会出现OOM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/287
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的llama7b模型。由于输入文本长度过长（16k），导致运行benchmark时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/286
这是一个用户提出需求的 issue，主要涉及到安装 TensorRT-LLM 时无法使用 Docker 的问题。由于某些原因，用户无法在系统上使用 Docker，因此寻求不使用 Docker 的安装教程。

https://github.com/NVIDIA/TensorRT-LLM/issues/285
这是一个bug报告，主要涉及TensorRT-LLM中的cross-attention机制返回错误结果的问题，可能是由于cross-attention层产生完全不同数值导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/284
这是一个关于TensorRT-LLM中量化偏好层级的讨论问题，主要涉及到量化推理的性能和质量损失选择，以及不同量化方法的偏好与适用场景。

https://github.com/NVIDIA/TensorRT-LLM/issues/283
这是一个关于bug报告的issue，本问题涉及到TensorRT-LLM中使用`--paged_kv_cache`选项时出现GPU内存耗尽错误。原因可能是引擎使用`paged_kv_cache`标志建立时泄露了GPU存储器。

https://github.com/NVIDIA/TensorRT-LLM/issues/282
这是一个关于性能需求的问题，主要涉及TensorRT-LLM中的KV缓存以及在特定硬件上运行时内存消耗的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/281
这是一个关于硬件规格的问题，用户询问了关于H100和A100 GPU在性能表中使用的接口和VRAM容量的具体信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/280
这是一个bug报告，涉及的主要对象是Quantized Llama 7b模型。由于模型在生成结束符后仍继续生成结束符，可能是脚本问题或正常行为导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/279
这是一个bug报告，涉及文件路径问题，可能导致程序无法正确运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/278
这是一个bug报告，主要涉及的对象是TensorRT-LLM模型在不同架构GPU上的量化问题。这一问题可能由于Turing架构GPU无法正确支持模型量化导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/277
这个issue属于bug报告类型，主要涉及到编译TensorRT-LLM时出现的链接错误，导致无法成功编译google-test和benchmarks。

https://github.com/NVIDIA/TensorRT-LLM/issues/276
这是一个关于问题解决（bug报告）的issue，涉及主要对象是在NVIDIA T4 GPU上编译CodeLlama-7B模型。导致问题的原因是尝试在资源有限的GPU和RAM上构建引擎文件时出现内存问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/275
这是一个用户提出需求类似的issue，主要涉及的对象是支持`frequency_penalty`，用户希望增加对此参数的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/274
这是一个用户提出的需求问题，主要涉及TensorRT-LLM中支持`repetition_penalty`和`presence_penalty`参数互相排斥的情况，导致无法创建符合OpenAI需求的API。

https://github.com/NVIDIA/TensorRT-LLM/issues/273
这是一个需求类型的issue，主要涉及支持`gather_all_token_logits`标志以构建Llama模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/272
这是一个bug报告，涉及主要对象是TensorRT-LLM中的权重量化操作，由于输入文本长度超过最大允许长度，导致出现数值错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/271
这个issue是用户提出需求，主要对象是关于TensorRT-LLM中生成token数量的设置或查询，用户想要知道如何获取每个提示生成的输出token数量，并询问是否有方法来设置这个变量。

https://github.com/NVIDIA/TensorRT-LLM/issues/270
该issue属于用户提出需求类型，主要涉及的对象是TensorRT-LLM项目中的ChatGLM3模型。由于用户想要测试使用chatglm26b来测试ChatGLM3或者了解是否有计划支持ChatGLM3。

https://github.com/NVIDIA/TensorRT-LLM/issues/269
这是一个用户需求类型的issue，主要涉及benchmarks/cpp/prepare_dataset.py脚本的使用。导致该问题的原因是用户在运行benchmarks/gptManagerBenchmark时需要生成input_ids文件，但不清楚如何准备这个文件的示例。

https://github.com/NVIDIA/TensorRT-LLM/issues/268
这是一个关于如何确认分页注意力是否启用的问题，主要涉及TensorRT-LLM中的`paged_kv_cache`功能。由于无法观察到使用`paged_kv_cache`引擎构建后的效果，用户寻求帮助确认其是否生效。

https://github.com/NVIDIA/TensorRT-LLM/issues/267
这是一个bug报告，涉及主要对象为在构建TensorRT-LLM下的llama7bhf模型时设置tp大小为2、pp大小为2，并使用smoothquant，但构建失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/266
这是一个需求提出的issue，主要涉及支持InternLM模型，由于要实现的功能是支持不同类型的权重和缓存，因此这可能涉及到模型优化和性能改进方面的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/265
这是一个bug报告，主要涉及TensorRT-LLM框架下运行baichu13b-chat时在设置[num_beams]为2时出现错误，原因是在设置输入形状时发生了静态维度不匹配导致的API使用错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/264
这个issue类型是关于性能测试的问题，主要对象是TensorRT-LLM下的gptSessionBenchmark。用户在问关于latency含义以及如何查看token throughput的问题，希望获得更详细的性能指标。

https://github.com/NVIDIA/TensorRT-LLM/issues/263
这是一个bug报告，涉及TensorRT-LLM下的llama 2 70B模型构建失败问题，可能由某个错误导致模型无法加载。

https://github.com/NVIDIA/TensorRT-LLM/issues/262
这是一个关于功能需求的issue，主要涉及的对象是TensorRT-LLM，用户询问是否支持arrch64。

https://github.com/NVIDIA/TensorRT-LLM/issues/261
这是一个bug报告，涉及到在安装TensorRT-LLM时出现的依赖冲突导致安装失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/260
这个issue类型是bug报告，主要涉及的对象是Triton server，由于Triton server使用更多内存导致启动失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/259
这是一个无内容的issue，类型为用户提出需求。主要对象是代码中的打印输出功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/257
这是一个技术问题，涉及TensorRT-LLM在A10G上编译quantized llama7B模型时内存限制引起的问题，用户询问如何规避内存不足导致的OOM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/256
这是一个关于Docker构建中Python安装无效的bug报告，用户指出在构建容器时安装的Python包在运行容器后消失。

https://github.com/NVIDIA/TensorRT-LLM/issues/255
这是一个bug报告，主要涉及对象是使用TensorRT-LLM进行量化时出现的异常。该问题可能是由于TensorRTLLM编译版本与Phind CodeLlama模型间的兼容性问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/254
这是一个bug报告，主要涉及TensorRT-LLM下的CodeLlama-7b模型，由于缺少config.json文件导致了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/253
这是一个bug报告，主要涉及TensorRT-LLM中构建LLM模型时发生Cuda内存溢出错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/252
这是一个关于内存泄漏的bug报告，涉及TensorRT-LLM中使用inference service进行stress test时GPU内存占用持续增加的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/251
这是一个用户请求帮助的issue，主要涉及使用flan-t5-base遇到的问题。由于无法加载和使用google/flant5base，用户寻求帮助解决问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/250
这是一个bug报告，主要涉及TensorRT-LLM下的生成接口无法正确在EOS处停止，而只在达到最大输出长度时停止。这个问题的症状是baichuan模型中，无论是在python测试脚本还是在tritonllm后端，生成接口 无法正确根据end_ids停止生成。

https://github.com/NVIDIA/TensorRT-LLM/issues/249
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下int8权重量化不准确的问题。可能由于weight.py文件修改不正确或配置未调整导致权重量化模型输出结果错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/248
这是一个bug报告，该问题涉及到TensorRT-LLM下的batch_size设定导致性能问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/247
这是一个bug报告，涉及TensorRT-LLM在推理阶段中GPU内存使用异常增加的问题。原因可能是内存泄漏或者资源释放不完全导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/246
这是一个性能问题报告，主要涉及TensorRT-LLM在不同V100设备上的表现。由于cutlass核心的差异导致了在不同设备上TRTLLM性能不同的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/245
这是一个bug报告类型的issue，主要涉及到代码提交f84d5fe，可能是由于该提交引入的问题导致了bug或错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/244
这是一个关于性能问题的bug报告，涉及主要对象为TensorRT-LLM中的int4和int8模型，可能由于某些原因导致int4模型的推理速度明显慢于int8模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/243
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的EOS功能在baichuan2示例中无法正常工作。可能是由于特定参数设置导致输出与预期不符。

https://github.com/NVIDIA/TensorRT-LLM/issues/242
这是一个关于bug的报告，涉及到TensorRT-LLM下使用tritonserver时stop_words无法正常工作的问题。原因可能是stop_words格式不符合原始fastertransformer的要求。

https://github.com/NVIDIA/TensorRT-LLM/issues/241
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的logits manipulators，用户想知道是否有计划添加类似transformers支持的typical_p功能，以便更容易地过渡到TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/240
这是一个用户提出需求的类型，该问题涉及request streaming功能，用户提出需要一个标志来指示生成结束。

https://github.com/NVIDIA/TensorRT-LLM/issues/239
这是一个关于代码功能问题的bug报告，主要涉及TensorRT-LLM下的quantize脚本无法处理CodeLlama 34B模型的注意力权重矩阵形状不被3整除的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/238
这是一个用户提出需求的issue，主要对象是希望从模型返回token的对数概率。

https://github.com/NVIDIA/TensorRT-LLM/issues/237
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的GenerationSession.setup()和GenerationSession.decode()函数反复调用时导致GPU内存耗尽的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/236
这个issue类型是用户提出需求，主要涉及对象是构建TensorRT引擎。由于内存不足导致Falcon的`build.py`脚本在g5.2xlarge实例上崩溃，用户想知道需要多少内存来构建引擎。

https://github.com/NVIDIA/TensorRT-LLM/issues/235
这是一个bug报告，涉及的主要对象是使用TensorRT-LLM中的Docker镜像构建引擎时遇到依赖安装问题。用户尝试在nvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3镜像中构建falcon7binstruct引擎，但安装依赖时遇到了错误，包括onnx和tensorrt_llm的安装问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/234
这是一个Bug报告，涉及的主要对象是TensorRT-LLM下的一个benchmark。问题可能是由于配置限制导致最大批处理大小出现错误，但实际服务端的并发较大时不会发生OOM。

https://github.com/NVIDIA/TensorRT-LLM/issues/233
这是一个bug报告，主要对象是TensorRT-LLM下的benchmarks cpp版本，由于缺少Makefile导致无法编译可执行文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/232
这是一个关于代码逻辑的问题，主要涉及Attention类中参数传递的逻辑，导致了关于qkv_bias参数的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/231
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的docker image构建问题。由于文件格式不被识别，导致了构建docker image失败的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/230
这是一个bug报告，主要涉及TensorRT-LLM在Colab环境下无法安装的问题。原因可能是环境配置导致无法使用pip安装。

https://github.com/NVIDIA/TensorRT-LLM/issues/229
这是一个关于bug报告类型的issue，主要涉及的对象是TensorRT-LLM下的模型构建（build.py）。由于模型只支持在CPU上运行，用户想知道在获取TensorRT的FP16或INT8引擎时需要多少最低内存，同时提到自己的内存为48GB。

https://github.com/NVIDIA/TensorRT-LLM/issues/228
这是一个bug报告，涉及主要对象为TensorRT-LLM，用户添加额外的停用词时未生效。

https://github.com/NVIDIA/TensorRT-LLM/issues/227
这是一个关于用户提出需求的问题，主要涉及的对象是TensorRT-LLM下的模型后处理。用户希望获取仅包含新生成 token 的功能，但由于后处理模型尝试接收来自 decouple 模型 tensorrt_llm 和非 decoupled 模型预处理的输入，导致出现错误，用户寻求解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/226
这是一个bug报告，主要涉及TensorRT-LLM中的Falcon-40b构建过程，问题是构建过程中出现了内存泄漏导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/225
这是一个关于并发性/线程安全问题的bug报告，主要涉及TensorRT-LLM中Python示例中的GenerationSession和ExecutionContext。导致该问题的原因是在GenerationSession中重复使用相同的ExecutionContext可能导致并发调用`GenerationSession::decode`函数不安全。

https://github.com/NVIDIA/TensorRT-LLM/issues/224
这个issue属于bug报告类型，主要涉及TensorRT-LLM中的paged KV cache的可用空间问题。由于kv_cache_free_gpu_mem_fraction设置不当，可能导致TRTLLM Triton backend中的paged KV cache大小与预期不符。

https://github.com/NVIDIA/TensorRT-LLM/issues/223
这是一个关于用户提出需求的问题，主要涉及TensorRT-LLM中是否能够在单个前向传递中处理多个标记的问题。由于用户希望能够同时处理多个输入标记并获得相应的注意力分数，因此提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/222
该issue类型是bug报告，涉及的主要对象是TensorRT-LLM。由于不同CUDA兼容性的设备导致了无法在不同架构设备上使用相同的engine plan文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/221
这个issue是一个功能需求请求，主要涉及的对象是TensorRT-LLM中的批处理管理库。这个需求是由于缺乏批处理管理库导致用户希望增加该功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/220
这是一个bug报告，问题主要涉及TensorRT-LLM的错误代码8，导致形状计算溢出的内部错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/219
这个issue类型是bug报告，主要涉及的对象是参数设置--ulimit memlock=-1和--ulimit stack=67108864。未设置这两个参数导致程序运行时出现freeze up的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/218
这个issue类型是bug报告，涉及的主要对象是TensorRT-LLM。出现该错误的原因是缺少libnvinfer.so.9共享对象文件，导致无法导入TensorRT模块。

https://github.com/NVIDIA/TensorRT-LLM/issues/217
这是一个bug报告，主要涉及TensorRT-LLM下的Baichuan模块，出现了输入类型不匹配导致的错误提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/216
这个issue是关于Bug的报告，主要涉及TensorRT-LLM中使用FP8 PTQ和AWQ进行量化时出现的错误，可能是由于量化过程中的某些问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/215
这是一个bug报告，主要涉及到TensorRT-LLM下的代码中出现了混淆，导致无法正确实现平滑功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/214
这是一个bug报告，主要涉及对象是TensorRT-LLM下的设置`max_output_len`参数时出现问题，由于将`max_output_len`设为400导致程序出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/213
这个issue属于bug报告类型，主要涉及Nccl errors导致的错误提示，问题可能是由于GPU集群上运行LLAMA 70b时出现NCCL错误而导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/212
这是一个bug报告，主要涉及到TensorRT-LLM模型中关于输出样本多样性的问题，用户在尝试生成固定输入提示的替代输出时遇到了输出缺乏多样性的现象。

https://github.com/NVIDIA/TensorRT-LLM/issues/211
这是一个关于性能问题的用户提问，主要涉及TensorRT-LLM的内存使用情况，用户想测试实际内存占用并确认是否成功启用kvint8功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/210
该issue类型为性能问题，主要涉及的对象是TensorRT-LLM的engine加载过程。由于max_input_len参数增加导致内存需求增加，进而可能触发OOM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/209
这是一个用户提出需求的issue，主要关注的对象是支持FlashDecoding来加快长序列推理速度。

https://github.com/NVIDIA/TensorRT-LLM/issues/208
这是一个bug报告，涉及的主要对象是TensorRT-LLM下的baichuan2模型转换和生成结果的不一致性。原因可能是模型转换和生成过程中的配置或参数设置引起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/207
这是一个关于模型转换生成结果不一致的bug报告，涉及的主要对象是TensorRT-LLM中的模型转换过程。由于模型转换生成的结果与原始模型的结果不一致，导致了与预期不符的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/206
这是一个bug报告，主要涉及TensorRT-LLM下的模型转换和生成结果的一致性问题，可能是由于转换过程中的参数设置或代码逻辑导致生成结果的差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/205
这是一个需求提议的issue，涉及将GQA支持添加到MPT和GPT模型中。原因是当前TensorRTLLM尚不支持GQA模型，但某些MPT模型可能会使用GQA，用户希望增加对GQA的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/204
这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的max capacity for paged KV cache models的指定方式。原因是希望能够通过其他方式指定最大内存容量，而不是当前的方式。

https://github.com/NVIDIA/TensorRT-LLM/issues/203
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的模型运行问题，由于某种原因导致无法在2xA100 80G上成功运行70B模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/202
这是一个bug报告，主要涉及TensorRT-LLM中GenerationSession对象，因为在创建GenerationSession时如果指定了`stream`关键字参数，会导致`self.stream`未初始化而在后续代码中崩溃。

https://github.com/NVIDIA/TensorRT-LLM/issues/201
这个issue类型属于bug报告，该问题涉及TensorRT-LLM下以bfloat16构建70B模型失败，原因可能是命令使用错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/200
这是一个关于bug报告的issue，主要涉及的对象是TensorRT-LLM中的4bit GPTQ Llama2引擎。由于enable_context_fmha操作在Volta上不受支持，导致在构建引擎时出现"Assertion failed: No valid SQ GEMM tactic"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/199
这是一个bug报告，该问题单涉及的主要对象是TensorRT-LLM中的GPT Variant StarCoder模型构建引擎过程。由于未指定use_gpt_attention_plugin和use_gemm_plugin的值，导致了assertation fault。

https://github.com/NVIDIA/TensorRT-LLM/issues/198
这是一个bug报告，涉及TensorRT-LLM下构建引擎时出现的断言失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/197
这是一个bug报告，涉及TensorRT-LLM的加载错误，由于未找到模型文件导致出现该错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/196
这是一个bug报告，主要涉及Baichuan模型在启用inflight batching时遇到错误，错误可能由于PLUGIN_V2操作不受支持而导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/195
这个issue类型是bug报告，该问题涉及的主要对象是KV-int8 calibration，由于内存不足导致OOM错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/194
这是一个性能问题报告，主要涉及到在2* V100上性能比FT差的问题。可能由于batch大小增加导致性能差距进一步扩大。

https://github.com/NVIDIA/TensorRT-LLM/issues/193
这是一个用户提出需求的issue，主要涉及的对象是GptManager。由于C++实现中没有提供设置sampling配置的接口，用户询问如何使用GptManager进行文本采样。

https://github.com/NVIDIA/TensorRT-LLM/issues/192
这是一个 bug 报告，涉及的主要对象是 TensorRT-LLM 中的 llama-7B 模型，由于无法成功运行 benchmark 导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/191
这个issue类型是bug报告，涉及的主要对象是Baichuan v1_13b build.py，在构建过程中由于内存分配问题导致程序被终止。

https://github.com/NVIDIA/TensorRT-LLM/issues/190
这个issue属于用户提出需求。该问题涉及的主要对象是TensorRT-LLM项目。用户在询问是否有计划在本地Linux环境下构建TensorRT-LLM而不是使用基于Docker的构建方法。

https://github.com/NVIDIA/TensorRT-LLM/issues/189
这是一个bug报告类型的issue，主要涉及的对象是在使用TensorRT-LLM的docker镜像时遇到了CUDA初始化失败的问题，导致GPU功能不可用。

https://github.com/NVIDIA/TensorRT-LLM/issues/188
该issue类型为更新请求，主要涉及的对象是TensorRT-LLM项目。

https://github.com/NVIDIA/TensorRT-LLM/issues/187
这是一个bug报告，主要涉及TensorRT-LLM项目中通过`make -C docker release_build`命令编译时出现的错误。原因可能是相关依赖项或配置有问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/186
这是一个Bug报告类型的issue，涉及TensorRT-LLM在构建过程中出现内存不足错误的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/185
这是一个用户提出问题的issue，主要涉及的对象是C++ benchmark的构建，可能是由于缺乏构建指导导致用户无法构建benchmark。

https://github.com/NVIDIA/TensorRT-LLM/issues/184
这是一个bug报告，主要涉及TensorRT-LLM的构建过程中出现了"Error: could not load cache"错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/183
这是一个bug报告，主要涉及TensorRT-LLM下无法使用ncu和nsys进行benchmark profiling的问题，可能是由于配置错误或使用方式不正确导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/182
这是一个bug报告，主要涉及TensorRT-LLM下的gptSessionBenchmark无法打开生成的engine文件，导致出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/181
这是一个Bug报告类型的Issue，涉及TensorRT-LLM下的int8 kv cache或smoothquant功能。原因可能是在保存权重时出现CUDA错误导致保存失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/180
该issue属于用户提出需求类型，主要对象是TensorRT-LLM下的支持ChatGLM3。因发布了ChatGLM3模型成为参数少于10b的最佳中文模型，用户请求添加对ChatGLM3的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/179
这是一个bug报告，涉及主要对象为在Windows 11上构建TensorRT-LLM时缺少了特定的AMD64-WINDOWS库文件，导致链接错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/178
这是一个bug报告，主要涉及到在安装`tensorrt_llm`时出现了找不到`torchdata==0.7.0.dev20230828`版本的错误。造成这个问题的原因可能是缺少对应版本的`torchdata`库。

https://github.com/NVIDIA/TensorRT-LLM/issues/177
这是一个用户提出需求的类型，主要涉及到项目的文档更新。由于README.md文件可能需要更新或修正，用户提出更新的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/176
这是一个bug报告，涉及的主要对象是TensorRT-LLM。用户无法将形状为torch.Size([7, 346])的input_ids分配给引擎，因为该引擎支持的形状范围为[(1, 1), (2, 1), (4, 1024)]，导致出现`ValueError`。

https://github.com/NVIDIA/TensorRT-LLM/issues/175
这是一个bug报告，涉及TensorRT-LLM的新构建失败问题，用户在尝试使用最新的triton主分支时尝试转换引擎时遇到了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/174
这是一个用户提出需求的类型，主要涉及的对象是TensorRT-LLM项目。由于DeBerta模型目前不受支持，用户请求添加对其的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/173
这个issue属于功能需求提议类型，主要涉及Activation Functions的实现。原因是用户提出了需要实现包括tanhshrink、logsoftmax、softmin、dimwise tensor sum、selu、logsigmoid以及relu6等激活函数的需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/172
这是一个关于功能需求的issue，主要涉及的对象是Activation Operators。原因是用户请求跟踪所有激活函数操作符的实现来源。

https://github.com/NVIDIA/TensorRT-LLM/issues/171
这是一个bug报告，涉及主要对象是TensorRT-LLM。用户询问关于SmoothQuant与Inflight batching的支持情况，报告在运行gptManagerBenchmark时遇到问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/170
这是一个bug报告，针对TensorRT-LLM下无法使用fp8精度运行falcon_180B benchmark的问题。由于环境配置和代码执行出现错误，用户希望得到问题解决方案。

https://github.com/NVIDIA/TensorRT-LLM/issues/169
这个issue类型是提出需求，涉及的主要对象是在TensorRT-LLM下实现 speculative sampling / assisted generation功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/168
这是一个Bug报告，该问题涉及将fintuned llama2-70B-chat hf模型转换为FT模型时遇到错误。导致此问题的原因可能是llama270B模型是否支持当前TRTLLM的int8 kvcache。

https://github.com/NVIDIA/TensorRT-LLM/issues/167
这是一个bug报告，主要涉及TensorRT-LLM库在docker环境中导入失败的问题。由于缺少libmpi.so.12共享对象文件造成了此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/166
这是一个bug报告，主要涉及的对象是在运行docker的make release_build过程中遇到问题。这个问题可能是由于APT执行的脚本出现问题而导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/165
这是一个bug报告，涉及到TensorRT-LLM的docker打包编译过程中出现的错误问题。由于某种原因导致出现了编译错误，用户希望知道如何更改以解决问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/164
这是一个bug报告，涉及主要对象为Baichuan v2_13b的run.py脚本。由于无法复制日志，导致在执行`run.py`时发生错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/163
这个issue主要是功能需求的提出，涉及到Conv操作在Pytorch API中的实现问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/162
这是一个 bug 报告类型的 issue，主要涉及的对象是 TensorRT-LLM 中的编译错误 `Compile with TORCH_USE_CUDA_DSA`，由于尝试按照README指南进行 INT8 weight only 和 INT8 KV cache 的转换时发生编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/161
该issue类型为用户提出需求，涉及主要对象是模型在量化前后在一系列常见基准测试中的性能比较。用户询问如何比较量化前后模型在常见基准测试中的性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/160
这是一个bug报告，该问题涉及TensorRT-LLM中使用smoothquant对llama33b模型进行量化过程中出现oom问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/159
这是一个bug报告，涉及主要对象是TensorRTLLM。由于执行`./tests/samplingTest`时出现"no kernel image is available for execution on the device"错误，可能是由于软件环境中的CUDA版本不兼容导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/158
这是一个用户请教问题类型的issue，主要涉及模型的前向传播操作。用户想要通过一次前向传播获取模型的logits，可能由于现有示例中直接调用生成函数获取结果而困惑。

https://github.com/NVIDIA/TensorRT-LLM/issues/157
这是一个用户提出需求的类型的issue，主要对象是支持Zephyr 7B模型，由于当前没有该模型的支持，用户请求添加对https://huggingface.co/HuggingFaceH4/zephyr7balpha的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/156
这是一个关于无法运行SmoothQuant for llama2-7b的bug报告，由于每一层都遇到错误，用户无法成功执行平滑量化操作。

https://github.com/NVIDIA/TensorRT-LLM/issues/155
这是一个关于TensorRT-LLM中In-flight Batching使用的问题，类型为使用问题，主要涉及到如何正确使用TensorRT-LLM和关于In-flight Batching在句子生成长度不同的批次中的应用。

https://github.com/NVIDIA/TensorRT-LLM/issues/154
这是一个bug报告，涉及TensorRT-LLM下的性能衰减问题，主要是1st token latency方面的表现不符合预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/152
这是一个更新需求，主要对象是batch manager，用户希望在release/0.5.0版本中对其进行更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/151
这是一个bug报告，涉及主要对象是TensorRT-LLM下的代码。由于在使用`--cpp-only`编译时，导致batch_manage.a选择错误，原因是编译时使用了不同的cxx11 abi版本造成的。

https://github.com/NVIDIA/TensorRT-LLM/issues/150
这是一个用户提出需求的类型的issue，主要涉及的对象是TensorRT-LLM中的batch manager组件。由于实现细节被封装在C++静态库中，导致用户难以基于batch manager实现一些优化技术，因此用户询问batch manager是临时还是永久闭源的。

https://github.com/NVIDIA/TensorRT-LLM/issues/149
这是一个关于Bug报告的issue，主要涉及对象为在单个3090 GPU上无法运行6B模型加载引擎时出现OOM问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/148
这是一个空白的issue，需要补充内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/147
这个issue类型是用户提出需求，主要涉及的对象是如何在多个节点之间构建模型。由于两台机器之间没有NVLink连接，用户想知道如何在两台机器之间通信以利用4个GPU构建和运行模型。

https://github.com/NVIDIA/TensorRT-LLM/issues/146
这是一个用户提出需求的问题，主要涉及TensorRT-LLM对哪些GeForce GPU的支持，以及是否计划扩展对其他NVIDIA GPU的支持。该问题由于不清楚TensorRT-LLM具体支持哪些单个NVIDIA GPU以及其相应的VRAM数量而引起。

https://github.com/NVIDIA/TensorRT-LLM/issues/145
这个issue类型是需求问题，主要涉及定义模型中的任意大小的Tensor，可能是由于模型定义中出现了'`default_net`'相关错误而导致问题的提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/144
这个issue是用户提出需求类型的问题，主要涉及的对象是GptManager，用户想要实现从RAM中加载数据而非文件系统导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/143
这是一个bug报告，涉及到TensorRT-LLM中运行gpt build.py时出现的错误。问题可能是由于函数参数不匹配导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/142
这是一个bug报告，涉及TensorRT-LLM项目中的make命令，用户反馈运行 "make -C docker release_build" 时出错。

https://github.com/NVIDIA/TensorRT-LLM/issues/141
这是一个bug报告，主要涉及TensorRT-LLM下的chatglm2-6b引擎，由于可能不支持bs维度导致无法运行benchmark。

https://github.com/NVIDIA/TensorRT-LLM/issues/140
这是一个用户提出需求的issue，主要涉及TensorRT-LLM生成不同输出长度句子的问题。这个问题可能是由于TensorRT-LLM在同一批次生成的句子长度相同，需要用户手动修改终止符来实现不同长度的句子生成。

https://github.com/NVIDIA/TensorRT-LLM/issues/139
这是一个bug报告，涉及TensorRT-LLM中smoothquantization功能，通过smoothquantization处理llama1B模型后内存增加并出现譥权告警，导致内存占用增加、譥权告警。

https://github.com/NVIDIA/TensorRT-LLM/issues/138
这是一个bug报告类型的issue，主要涉及chatglm2-6b模型的benchmark失败问题，可能是由于转换模型或者运行benchmark时出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/137
这是一个关于使用docker镜像的问题，用户询问能否使用特定的pytorch的docker镜像作为基础镜像来编译和运行TensorRTLLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/136
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM引擎。由于使用INT8权重和INT8 KV缓存在批大小为8时导致性能分析器失败，但批大小为1时正常，可能是由于批处理维度无效导致的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/135
这是一个bug报告，涉及主要对象为TensorRT-LLM中的'use_custom_all_reduce'功能。由于在weight_only quantization情况下，使用该功能会导致错误和不稳定的结果，可能是硬件环境与特定配置之间的兼容性问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/134
这是一个bug报告，涉及TensorRT-LLM中的bloom model不支持In-flight batching和paged kv cache功能的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/133
这是一个bug报告类型的issue，主要涉及到TensorRT-LLM中的kv-int8模型输出错误结果的问题，可能是由于转换脚本或运行方式等原因导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/132
这是一个关于性能问题的bug报告，主要涉及TensorRT-LLM的批量推理功能。由于GPU内存紧张，导致在使用较大批次大小时性能下降或出现CUDA内存溢出错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/131
这是一个用户需求类型的issue，主要涉及Docker容器中Flask应用与宿主机具有相同IP地址的问题。由于Docker网络隔离性导致，用户希望了解如何使Flask应用与宿主机拥有相同IP地址。

https://github.com/NVIDIA/TensorRT-LLM/issues/130
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的KV cache，原因是内存管理问题导致了内存泄漏和最终内存溢出。

https://github.com/NVIDIA/TensorRT-LLM/issues/129
这是一个用户提出需求的问题，涉及到TensorRT-LLM，用户询问是否可以运行调整来获得更好的性能。

https://github.com/NVIDIA/TensorRT-LLM/issues/128
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的文本生成模型。用户提出需要添加对抽样（sampling）功能的支持，以获得更好的文本生成效果（类似于T5模型）。

https://github.com/NVIDIA/TensorRT-LLM/issues/127
这个issue是关于用户提出需求的类型，主要涉及到T5模型类型支持。用户想要在TensorRT-LLM中使用T5large模型时遇到了权重定义的问题，希望得到关于T5模型支持的改进和优化建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/126
这是一个用户提出需求的issue，主要涉及的对象是AWQ quantization功能。 由于特定torch版本依赖性限制，导致用户维护自己的镜像时需要构建特定torch版本，提出希望增加灵活性的要求。

https://github.com/NVIDIA/TensorRT-LLM/issues/125
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的一个函数缺少返回语句，可能导致函数行为不符合预期。

https://github.com/NVIDIA/TensorRT-LLM/issues/124
这是一个用户提出需求的issue，主要涉及支持Robert模型，用户请求添加对Hugging Face Roberta模型的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/123
这是一个功能需求的issue，主要涉及支持InternLM模型。这个问题尚未完全开发和测试并行计算和量化功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/122
这是一个bug报告，主要涉及到TensorRT-LLM中的gather_all_token_logits功能，由于对模型输出的调整导致了错误的发生。

https://github.com/NVIDIA/TensorRT-LLM/issues/121
这是一个bug报告，主要涉及TensorRT-LLM下的文本生成功能，问题导致生成的文本未能正确结束。

https://github.com/NVIDIA/TensorRT-LLM/issues/120
这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的生成引擎，用户希望支持多个停止标识符。

https://github.com/NVIDIA/TensorRT-LLM/issues/119
这是一个bug报告，涉及主要对象为TensorRT-LLM，由于缺少'int64'属性导致该错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/118
这是一个关于bug报告的issue，主要涉及TensorRT-LLM下的`convert_hf_mpt_to_ft`工具无法正常下载shards导致进度卡住的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/117
这是一个bug报告，问题涉及到TensorRT-LLM中的推理结果每次相同的问题，可能是由于SamplingConfig参数设置不当导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/116
这是一个bug报告，涉及TensorRT-LLM下的vocode启动问题，可能由于vscode配置导致无法成功运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/115
这是一个bug报告，主要涉及的对象是TensorRT-LLM，问题由于潜在的拼写错误引起了bug。

https://github.com/NVIDIA/TensorRT-LLM/issues/114
这是一个关于文档错误修正的bug报告，主要涉及的对象是文档中涉及到的“qunatization”拼写错误。原因是由于拼写错误导致文档中的错别字问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/113
这是一个bug报告，主要涉及的对象是在使用TensorRT-LLM时构建docker镜像时遇到问题。由于执行`make C docker release_build`命令导致无法完成构建，需要帮助解决此问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/112
这是一个bug报告，主要涉及TensorRT-LLM中的pipeline parallel功能在benchmark中是否支持的问题。导致该问题的原因可能是benchmark代码中还没有针对pipeline parallel进行支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/111
这是一个用户提出需求的issue，主要涉及的对象是通过Python运行时实现的stream_decode生成器功能。由于实现非阻塞流处理需要修改源代码，用户在询问是否有其他方法实现非阻塞流处理。

https://github.com/NVIDIA/TensorRT-LLM/issues/110
这是一个bug报告，主要涉及的对象是TensorRT-LLM下的权重量化过程，导致了无法量化bf16到int8的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/109
这是一个bug报告，涉及TensorRT-LLM项目中 llama engine 运行时出现的错误。由于系统磁盘空间不足导致 inotify_add_watch(/tmp) 操作失败，进而引发了报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/108
这是一个bug报告，涉及TensorRT-LLM下在使用2个GPU进行两路张量并行时出现的引擎和运行时世界大小不匹配的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/107
这是一个bug报告类型的issue，主要涉及TensorRT-LLM中的Gemmtensorrt_llm插件，由于无法找到该插件导致出现错误信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/106
这是一个关于构建TensorRT-LLM时出现错误的bug报告，主要涉及到构建过程中出现的路径错误导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/105
这是一个关于bug报告的issue，主要涉及到TensorRT-LLM下的模型建立和生成过程。导致所有输出都是"</s>"的症状是在构建baihuan213bchat引擎时，设置的max_input_len为4096，但在生成时却无法输出正确结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/104
这是一个用户提出需求的issue，主要涉及的对象是Attention sink。用户询问有没有计划在未来几周或几个月内对Attention sink进行工作，表达了期待看到未来发展的想法。

https://github.com/NVIDIA/TensorRT-LLM/issues/103
这是一个关于Docker镜像大小的问题，类型为用户提出需求。该问题涉及的主要对象是构建TensorRT-LLM的Docker镜像。由于构建的Docker镜像大小为28GB，提问者想了解这个大小是否符合预期，是否有可能对构建的镜像进行优化以减小大小。

https://github.com/NVIDIA/TensorRT-LLM/issues/102
这是一个bug报告，主要涉及TensorRT-LLM下CodeLlama的运行问题，由于未知原因导致了High CPU memory usage以及Killed提示。

https://github.com/NVIDIA/TensorRT-LLM/issues/101
这个issue是一个bug报告，主要涉及的对象是在单个NVIDIA L4 GPU上运行GPTJ6B模型的Benchmarking脚本。原因可能是由于CUDA ERROR: 2导致了错误的结果。

https://github.com/NVIDIA/TensorRT-LLM/issues/100
这是一个bug报告，主要涉及的对象是TensorRT-LLM。由于文件格式错误，导致编译失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/99
这是一个bug报告，涉及到在4个RTX3090设备上并行运行TensorRT-LLM时出现的nccl通信失败问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/98
这是一个bug报告，主要涉及TensorRT-LLM中权重无法被tensor_parallel_size整除导致的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/97
这是一个bug报告，主要涉及TensorRT-LLM的编译问题，由于编译过程中卡在98%，导致了无法完成编译的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/96
这是一个关于使用TensorRT-LLM Python API 改善服务器吞吐量的问题，涉及到in-flight batch 和 decoder 之间的不一致，可能存在设计或实现上的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/95
这是一个bug报告，该问题涉及TensorRT-LLM下的构建错误，由于输入类型不一致导致报错。

https://github.com/NVIDIA/TensorRT-LLM/issues/94
这是一个bug报告，该问题涉及TensorRT-LLM下的编译错误。造成这个问题的原因是链接器脚本文件的格式不符合规范。

https://github.com/NVIDIA/TensorRT-LLM/issues/93
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的chatglm26B示例。由于代码geex26b的构建和运行与chatglm26B示例不兼容，导致了运行结果不正确。

https://github.com/NVIDIA/TensorRT-LLM/issues/92
这是一个bug报告，涉及主要对象是TensorRT-LLM项目，由于缺少Makefile文件导致出现gmake错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/91
这是一个bug报告类型的issue，主要涉及TensorRT-LLM下的量化过程中出现错误的问题。原因可能是由于构建和安装步骤没有完全遵循导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/90
这是一个关于bug报告的issue，涉及TensorRT-LLM中停用词无法正常工作的问题。由于停用词列表在传递给函数时变得异常，导致输出提前完成，并且引发了关于如何设置停用词的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/89
这个issue类型是bug报告，主要涉及对象是TensorRT-LLM下的Triton，原因是出现了TrtGptModelInflightBatching需要特定版本的GPT attention插件的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/88
这是一个bug报告，主要涉及TensorRT-LLM下设置max_input_len, max_output_len为4096却无法达到实际输入等级。导致bug的原因可能是输入长度需要大约2000以上才会出错，而max_output_len为4096。

https://github.com/NVIDIA/TensorRT-LLM/issues/87
这是一个关于用户提出需求的类型，主要是在询问是否支持Qwen-7B或Qwen-14B等其他模型。原因可能是用户希望了解更多可支持的模型的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/86
这是一个用户提出需求的类型，主要涉及到支持InternLM模型。这个问题是由于用户希望TensorRT-LLM支持InternLM模型造成的。

https://github.com/NVIDIA/TensorRT-LLM/issues/85
这是一个bug报告，主要涉及的对象是在TensorRT-LLM下运行LLaMA 13B模型时，批量大小超过8时出现错误。这个问题出现的原因是优化模型不支持超过8的批量大小。

https://github.com/NVIDIA/TensorRT-LLM/issues/84
该issue类型为bug报告，主要涉及的对象是在使用TensorRT-LLM的chatglm2-6b模型在单节点多GPU上构建时遇到失败。这可能是由于多GPU情况下出现了错误，导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/83
这是一个bug报告，该问题涉及TensorRT-LLM无法支持较低版本的cuda driver，导致了不兼容错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/82
这是一个bug报告，涉及的主要对象是TensorRTLLM的docker镜像构建。原因是构建过程中出现了错误导致构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/81
这是一个bug报告，问题涉及TensorRT-LLM的中文、韩文等亚洲语言解码过程中出现乱码的情况。这可能是由于在bfloat16运行时出现了编码问题，与Huggingface运行时不同所导致。

https://github.com/NVIDIA/TensorRT-LLM/issues/80
该问题属于用户提出需求，并涉及到如何在TRTLLM中输出模型的中间结果，以便定位精度错误所在。

https://github.com/NVIDIA/TensorRT-LLM/issues/79
这是一个用户提出需求的类型，主要关注于支持"group beam search"和"diverse beam search"方法的功能新增请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/78
这是一个bug报告，主要涉及的对象是"Donglu branch"。由于某些原因导致的bug或用户需求。

https://github.com/NVIDIA/TensorRT-LLM/issues/77
这是一个bug报告，主要涉及对象是在TensorRT-LLM中运行时出现的segmentation fault异常。造成这个问题的原因可能是在使用num_beams值大于1时引发异常。

https://github.com/NVIDIA/TensorRT-LLM/issues/76
这是一个增强功能的类型issue，主要涉及的对象是`GptManager`，由于需要手动构建`pygptmanager`模块以及与`BatchManager`符号存在链接问题，导致了相关症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/75
这是一个关于性能下降问题的bug报告，主要涉及标注了TensorRT-LLM下paged attention的使用表现不佳的情况，用户可能因此提出了性能方面的问题或寻求帮助。

https://github.com/NVIDIA/TensorRT-LLM/issues/74
这是一个用户提出需求的issue，涉及的主要对象是batch_manager。这个问题由于未知原因导致用户想知道未来是否会开源batch_manager。

https://github.com/NVIDIA/TensorRT-LLM/issues/73
这是一个功能需求的issue， 主要涉及TensorRT-LLM的离线吞吐量benchmark，用户在询问如何使用`use_inflight_batching`和`use_gpt_attention_plugin float16`构建选项的兼容性问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/72
这是一个bug报告，涉及TensorRT-LLM的构建失败问题。由于编译时出现了语法错误，导致了该问题的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/71
这是一个关于如何构建docker镜像的问题，涉及主要对象为GPU A800和L40s，可能是由于配置或依赖关系导致的构建失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/70
这个issue类型是用户提出需求， 主要对象是如何在不使用Docker的情况下构建TensorRT-LLM，可能是用户对构建过程或环境有特定需求或限制。

https://github.com/NVIDIA/TensorRT-LLM/issues/69
这是一个bug报告类型的issue，主要涉及TensorRT-LLM项目的构建过程中出现的CMake错误，由于缺少"/usr/local/tensorrt/include/NvInferVersion.h"文件而导致无法读取，用户询问是否需要通过源代码构建TensorRT来解决该问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/68
这是一个用户提出需求的issue，用户想要了解如何将使用LoRA微调的Llama 2模型转换为TensorRTLLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/67
这是一个bug报告，涉及的主要对象是TensorRT-LLM中的模型 llama-7b。由于模型的内存消耗过高，导致在A40 GPU上以8 batch size 运行时出现了 CUDA 内存不足的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/66
这是一个bug报告，主要涉及的对象是"GptManager"。由于GPU内存应该足够，但在启动GptManager时出现了std::bad_alloc错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/65
这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的max_batch_size和max_num_sequences的区别问题。最可能是由于缺乏文档解释导致用户需要明确了解两者之间的差异。

https://github.com/NVIDIA/TensorRT-LLM/issues/64
这是一个bug报告，主要涉及到TensorRT 9.1.0.4版本中缺少libnvparsers库，导致构建失败的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/63
这个issue属于用户请求类，主要对象是文档的可读性。造成这个问题的原因可能是贡献者希望改善Readme.md文件的可读性，并询问所需时间来合并他们的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/62
该issue是用户请求支持，主要对象是在Nvidia AGX Orin开发套件上运行TensorRTLLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/61
这个issue属于用户提出问题类型，主要涉及限制topk大小为1024这一设计选择。由于仅能使用1024大小的topk，用户想要了解设计者的原因。

https://github.com/NVIDIA/TensorRT-LLM/issues/60
这是一个用户提出需求类型的issue，主要涉及的对象是TensorRT-LLM项目的Windows相关文档。可能由于过时或不完整的文档，用户提出需要更新Windows相关文档到主分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/59
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM在Windows平台上的相关文档。由于文档需要更新，用户提出了更新Windows相关文档的请求。

https://github.com/NVIDIA/TensorRT-LLM/issues/58
这是一个bug报告类型的issue，主要涉及的对象是TensorRT-LLM。这个问题可能由于某些条件下产生了"Perceived"相关的错误或异常情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/57
这个issue类型是用户寻求关于TensorRT-LLM工作流程的澄清，主要涉及到如何扩展模型支持。由于提供的示例与用户的情况不符，用户希望理解创建新模型并映射权重的过程，以便生成TRT engine，但需要更深入的指导。

https://github.com/NVIDIA/TensorRT-LLM/issues/56
这是一个bug报告类型的issue，涉及到批处理调度器（batchScheduler.h）中的一个拼写错误。原因是在代码中将"requets"错误地拼写为"requests"，导致出现了拼写错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/54
这是一个bug报告，主要涉及的对象是TensorRT-LLM中的starcoder engine构建过程。由于PLUGIN_V2操作在此图中不受支持，导致了引擎构建失败的症状。

https://github.com/NVIDIA/TensorRT-LLM/issues/53
这个issue是关于bug报告，主要涉及的对象是TensorRT-LLM中的GPT2示例。这个问题可能是由于输入数据或模型配置错误导致输出结果异常的问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/52
该issue属于用户提出需求类型，主要对象是Docker image的发布进度。原因可能是用户想了解关于发布时间的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/51
这是一个用户提出需求的issue，主要涉及到如何处理kv-cache在多模态GPT中的问题。由于模型行为在初始和后续传递之间存在差异，导致了挑战。

https://github.com/NVIDIA/TensorRT-LLM/issues/50
这是一个关于安装TensorRT-LLM时遇到的bug报告，涉及到x86_64-conda_cos6-linux-gnu-cc命令找不到的问题，可能是由于环境配置不正确导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/49
这是一个用户提出需求的issue，主要对象是添加对Mistral 7B模型的支持。由于Mistral模型比Llama2模型表现更好，用户希望在TensorRT-LLM中添加对Mistral模型的支持。

https://github.com/NVIDIA/TensorRT-LLM/issues/48
这是一个bug报告，涉及主要对象是TensorRT-LLM的docker file build。由于某些原因导致构建docker文件时出现错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/47
这个issue是用户提出需求，主要涉及TensorRT-LLM对于RWKV项目的支持问题。用户询问是否TensorRT-LLM支持RWKV项目，描述了RWKV项目的特性以及希望得到对应支持的情况。

https://github.com/NVIDIA/TensorRT-LLM/issues/46
这个issue类型是bug报告，主要涉及的对象是TensorRT-LLM环境下的Forward Compatibility mode，导致由于ENV BASH_ENV覆盖了默认变量值而出现了Compatibility mode不可用的错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/45
这是一个bug报告，涉及主要对象为CUDA版本变更导致TensorRT-LLM编译错误。原因是CUDA 11.8和CUDA 12中cudaGraphExecUpdate参数发生变化引发的编译错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/44
这是一个Bug报告，主要涉及TensorRT-LLM下的一个benchmark程序 llama-7b 的运行失败。可能是由于代码中存在错误导致无法成功运行。

https://github.com/NVIDIA/TensorRT-LLM/issues/43
该issue类型为用户提出问题，涉及主要对象为评估Throughput (tokens/s)，用户想了解关于该评估的方法及计算过程是否包括上下文处理。

https://github.com/NVIDIA/TensorRT-LLM/issues/42
这是一个Bug报告，涉及TensorRT-LLM在批量推理时遇到的问题。由于最大批处理大小未正确设置为引擎中的值，导致批量推理失败。

https://github.com/NVIDIA/TensorRT-LLM/issues/41
这个issue是关于TensorRT-LLM Releases的发布说明，主要对象是TensorRTLLM用户。通过发布稳定版本和开发版本来满足用户需求，并根据反馈更新分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/40
这是一个文档更新的issue，主要对象是batch_manager.md文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/39
这个issue是一个bug报告，涉及主要对象是在运行Tritonserver时出现错误的chatglm2模型。原因可能是与Tritonserver集成时的配置或环境设置有关。

https://github.com/NVIDIA/TensorRT-LLM/issues/38
这个issue类型是代码贡献，主要涉及TensorRT-LLM项目的分支管理。

https://github.com/NVIDIA/TensorRT-LLM/issues/37
这是一个bug报告，涉及的主要对象是Baichuan V2 13B模型，可能由于INT4权重量化的配置问题导致输出结果有问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/36
这是一个bug报告，主要涉及TensorRT-LLM在TensorRT docker container中的构建问题，由于在更改了tensorrt头文件路径后构建出现了错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/35
这是一个bug报告，主要涉及对象是TensorRT软件。用户由于TensorRT 9.1.0.4版本尚不可用，无法找到符合要求的版本。

https://github.com/NVIDIA/TensorRT-LLM/issues/34
这是一个bug报告，主要涉及TensorRT-LLM下的build.py文件，可能是由于依赖未正确安装导致的导入错误。

https://github.com/NVIDIA/TensorRT-LLM/issues/33
该问题类型是用户提出需求，请教问题，涉及的主要对象是Triton Inference Server。由于当前发布的版本不支持 flashattention，用户询问是否有计划将其集成到 tritoninferenceserver 中，以及是否会在未来支持该功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/32
这是一个bug报告，主要涉及TensorRT-LLM项目中构建失败的问题，可能是由于编译时的C++ 11 ABI链接问题导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/31
这个issue类型为用户提出需求，主要涉及的对象是项目团队。用户想了解项目团队是否计划支持MiniGPT4。

https://github.com/NVIDIA/TensorRT-LLM/issues/30
这是一个bug报告，主要涉及到TensorRT-LLM在Windows上的readme.md文件中链接跳转错误的问题。可能是由于链接设置错误导致的。

https://github.com/NVIDIA/TensorRT-LLM/issues/29
此issue属于bug报告类型，主要涉及的对象是TensorRT-LLM下的Code Llama 34B构建过程。该问题可能由GPU内存利用不足而导致的tactic内存请求过大所致。

https://github.com/NVIDIA/TensorRT-LLM/issues/28
这是一个bug报告，主要涉及的对象是TensorRT-LLM的构建过程，由于构建命令出现语法错误导致了无法生成静态链接库libtensorrt_llm_batch_manager_static.a。

https://github.com/NVIDIA/TensorRT-LLM/issues/27
这是一个用户提出需求的类型。用户想知道他们能否在GPT-2上使用TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/26
这个issue属于用户提出需求类型，主要涉及的对象是支持的GPU类型。用户询问为什么A10 GPU没有列入支持列表，以及是否在将来会支持这个GPU。

https://github.com/NVIDIA/TensorRT-LLM/issues/25
这是一个用户提出需求的issue，主要关注支持Medusa Sampling。原因在于需求扩展。

https://github.com/NVIDIA/TensorRT-LLM/issues/24
这是一个关于性能比较的问题，不是bug报告。用户对TensorRT-LLM和VLLM的速度进行了比较，并可能寻求性能方面的帮助或建议。

https://github.com/NVIDIA/TensorRT-LLM/issues/23
这是一个bug报告，涉及的主要对象是TensorRT-LLM。原因可能是由于容器构建的NVIDIA驱动版本不兼容导致无法正常运行TensorRT-LLM。

https://github.com/NVIDIA/TensorRT-LLM/issues/22
这是一个关于安装问题的类型为用户提出需求的issue，主要涉及对象为在docker容器中安装TensorRT-LLM，由于用户已经在docker容器中，因此提出如何在此环境下安装TensorRT-LLM的疑问。

https://github.com/NVIDIA/TensorRT-LLM/issues/21
这是一个bug报告，主要涉及的对象是README.md文件中两个失效链接。可能原因是链接地址变更或失效，导致用户无法访问相关内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/20
这是一个缺失具体内容的文档修复类型的issue，主要涉及TensorRT-LLM项目文档的修改。

https://github.com/NVIDIA/TensorRT-LLM/issues/19
这是一个文档问题，涉及TensorRT-LLM项目。由于文档问题导致的小错误，需要修复。

https://github.com/NVIDIA/TensorRT-LLM/issues/18
这个issue类型是需求提出，用户提出了需要在Windows上发布可用的wheel链接。

https://github.com/NVIDIA/TensorRT-LLM/issues/17
这是一个bug报告，用户在寻找Windows平台下的TensorRT 9.1.0.4下载链接，但目前该版本在Windows平台下尚不可用。

https://github.com/NVIDIA/TensorRT-LLM/issues/16
这是一个bug报告类型的issue，主要涉及Huggingface Transformers版本不匹配导致的错误行为。

https://github.com/NVIDIA/TensorRT-LLM/issues/15
这是一个对文档连接进行修复的Issue，主要对象是Github上的TensorRT-LLM项目。由于链接错误导致无法访问相关文档，需要修复以提供正确的文档信息。

https://github.com/NVIDIA/TensorRT-LLM/issues/14
这是一个用户提出需求类型的issue，主要涉及的对象是TensorRT-LLM的项目主页，可能由于当前首页内容需要更新或修正而提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/13
这是一个用户提出需求的类型，主要对象是项目的首页。由于可能需要对项目首页进行修订或更新，用户提出了这个问题。

https://github.com/NVIDIA/TensorRT-LLM/issues/12
这是一个关于需求提出的issue，主要涉及的对象是为TensorRT-LLM添加git-lfs依赖的二进制文件。

https://github.com/NVIDIA/TensorRT-LLM/issues/11
这个issue类型是用户提出需求，请求为TensorRT-LLM添加git-lfs依赖以处理二进制文件的发布。

https://github.com/NVIDIA/TensorRT-LLM/issues/10
这是一个更新请求类型的issue，主要涉及更新aarch64 batch manager libraries到release/0.5.0版本，可能由于需要更新功能或修复bug而被提出。

https://github.com/NVIDIA/TensorRT-LLM/issues/9
这是一个更新操作系统相关的功能库至主要分支的issue，主要涉及aarch64 batch manager libraries，原因可能是需要修复bug或者添加新功能。

https://github.com/NVIDIA/TensorRT-LLM/issues/8
这是一个bug报告，主要涉及Falcon weight loader下的内存泄漏问题。这个问题的症状可能是程序运行时内存占用持续增长，导致内存泄漏。

https://github.com/NVIDIA/TensorRT-LLM/issues/7
这个issue属于用户提出需求，要求更新aarch64 libraries到release/0.5.0分支。

https://github.com/NVIDIA/TensorRT-LLM/issues/6
这是一个Bug报告类型的Issue，涉及主要对象是update aarch64 libraries到main分支。原因可能是aarch64 libraries版本滞后导致功能不同步。

https://github.com/NVIDIA/TensorRT-LLM/issues/5
这是一个需求更新的issue，涉及到项目主分支的更新。这个问题可能是由于需要引入新的功能、修复bug或其他一些更新而发起的。

https://github.com/NVIDIA/TensorRT-LLM/issues/4
这是一个缺少具体内容的issue，类型是需求更新。主要对象是更新TensorRT-LLM，由于缺少具体信息，无法确定用户具体要求的更新内容。

https://github.com/NVIDIA/TensorRT-LLM/issues/3
这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM代码更新。

https://github.com/NVIDIA/TensorRT-LLM/issues/2
这是一个用户提出需求的issue，主要涉及到添加静态库来管理批处理。根据issue标题和内容推测，用户希望将静态库添加到批处理管理器中。

https://github.com/NVIDIA/TensorRT-LLM/issues/1
这个issue类型为更新请求，主要涉及的对象是将TensorRT-LLM中的onnx版本从1.12.0升级到1.13.0。这个更新请求是由于新发布的ONNX版本1.13.0带来了一系列新特性和功能。

