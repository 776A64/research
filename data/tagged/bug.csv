DeepSpeed,这是一个关于功能支持问题的issue，主要涉及DeepSpeed Zero 0对于universal checkpoint 的支持不完整导致无法在不同GPU数量下继续训练。,https://github.com/deepspeedai/DeepSpeed/issues/7200
DeepSpeed,这是一个功能需求类型的issue，主要涉及在DeepSpeed中将`pytorch_model.bin`或`.safetensors`检查点转换为UCP，并且由于缺少特定文件而引起了一些问题。,https://github.com/deepspeedai/DeepSpeed/issues/7192
DeepSpeed,这是一个关于更新torch grad hook API至BF16Optimizer和Stage2的issue，类型为功能改进。,https://github.com/deepspeedai/DeepSpeed/issues/7189
DeepSpeed,这个issue属于需求提出类型，主要涉及DeepSpeed项目中的autotp功能，描述了添加qwen3 autotp支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/7187
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中的gather输出布局支持。原因可能是用户希望在列并行中实现gather操作。,https://github.com/deepspeedai/DeepSpeed/issues/7181
DeepSpeed,这个issue类型是更新问题，涉及的主要对象是DeepSpeed版本号文本文件。,https://github.com/deepspeedai/DeepSpeed/issues/7180
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中如何使用AutoTP进行训练而不依赖于huggingface Trainer和accelerate，可能由于当前不支持该操作导致用户提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/7179
DeepSpeed,"该issue属于优化需求，主要涉及DeepSpeed中的Domino模块，需要添加跨层通信重叠以实现通信""free""和优化transformer层内的通信重叠实现。原因可能是为了提高性能和效率。",https://github.com/deepspeedai/DeepSpeed/issues/7178
DeepSpeed,这是一个需求类型的issue，主要涉及到需要从Megatron-Deepspeed中将`parallel_state.py`(`mpu`)功能移植到Ulysses，由于在DeepSpeed中调用了该功能的多个方法，难以仅简单剪裁到SP groups，因此需要对代码进行移植。,https://github.com/deepspeedai/DeepSpeed/issues/7176
DeepSpeed,这是一个关于更新torch版本的需求报告问题，主要涉及DeepSpeed在Windows环境下的用户。,https://github.com/deepspeedai/DeepSpeed/issues/7172
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及DeepSpeed的前端页面。由于开发者希望将AutoTP的博客链接放在首页，因此提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/7167
DeepSpeed,这是一份需求提出类型的issue，主要涉及的对象是DataStates-LLM（Asynchronous Checkpointing Engine）。这个issue主要是关于团队在为LLMs和transformers提供低开销的异步检查点方法，通过提出的库DataStatesLLM实现异步复制和检查点功能，希望将其贡献给DeepSpeed社区。,https://github.com/deepspeedai/DeepSpeed/issues/7166
DeepSpeed,这个issue是关于添加DCO检查指导的建议，属于增强功能需求类型，主要涉及到预提交格式中包含DCO检查的功能。这是为了帮助新用户更容易参与贡献，并确保他们了解贡献指南的要求。,https://github.com/deepspeedai/DeepSpeed/issues/7162
DeepSpeed,这是一个关于功能需求的issue，主要涉及DeepSpeed MoE中的ZeRO优化支持问题，用户提出希望优化MoE层参数的内存消耗。,https://github.com/deepspeedai/DeepSpeed/issues/7156
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的DeepCompile功能。这个需求是为了将编译器优化与DeepSpeed功能有效集成，提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/7154
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及的对象是DeepSpeed，用户想知道当前版本是否支持在Mac设备（M2-M4）上进行分布式微调操作。,https://github.com/deepspeedai/DeepSpeed/issues/7148
DeepSpeed,该issue属于用户提出需求的类型，主要涉及DeepSpeed是否支持autotp和pp在llm训练中，并询问有无相关用例或示例。,https://github.com/deepspeedai/DeepSpeed/issues/7147
DeepSpeed,该issue类型为功能增强（需求）报告，主要涉及到Gaudi2 CI/Nightly覆盖率的提升，由于需要增加模型并行和线性测试，导致增加了CI时间和夜间作业时间。,https://github.com/deepspeedai/DeepSpeed/issues/7146
DeepSpeed,这是一个功能请求类型的问题，主要涉及DeepSpeed无法在Nvidia 50系列GPU上使用的情况，由于该系列GPU仅支持Pytorch版本为2.6及以上，以及CUDA版本为12.8，用户请求增加对Pytorch >= 2.6和CUDA 12.8的支持。,https://github.com/deepspeedai/DeepSpeed/issues/7144
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的Wan2.1支持。由于在使用Wan2.1时调用了deepspeed.zero.Init()导致NotImplementedError，可能是因为在WanModel的__init__中的自定义init_weights代码引起的。,https://github.com/deepspeedai/DeepSpeed/issues/7142
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed是否支持多节点推断，用户在文档中找不到相关内容并寻求指导。,https://github.com/deepspeedai/DeepSpeed/issues/7137
DeepSpeed,该issue类型为用户提出需求，涉及的主要对象是DeepSpeed下的(IA)^3 Fine-Tuning方法。由于一个误操作导致的问题。,https://github.com/deepspeedai/DeepSpeed/issues/7134
DeepSpeed,这个issue类型是用户提出需求类型的问题，主要涉及的对象是DeepSpeed下的混合精致评估（MOE）结构。由于用户对DeepSpeed的MOE内容有兴趣，希望了解是否有计划支持深度搜索（deepseek）v3的MOE结构，表达了对深度搜索和DeepSpeed结合的期望。,https://github.com/deepspeedai/DeepSpeed/issues/7129
DeepSpeed,这是一个用户提出需求的问题，主要关注DeepSpeed中的pipeline engine是否支持DualPipe，用户想知道DeepSpeed的pipeline engine是否属于DualPipe。,https://github.com/deepspeedai/DeepSpeed/issues/7126
DeepSpeed,这是一个用户提出需要功能需求的issue，主要涉及DeepSpeed中集成新通信库并测试ZeRO3的请求。由于RCCL在规模扩展时速度极慢且不稳定，用户希望替换现有的rccl allgathers和reducescatters。,https://github.com/deepspeedai/DeepSpeed/issues/7125
DeepSpeed,这个Issue是一个功能增强请求，主要针对CI测试中出现间歇性挂起的情况，需要添加pytest的超时设置。,https://github.com/deepspeedai/DeepSpeed/issues/7124
DeepSpeed,这是一个用户提出需求的 issue， 主要涉及对象是DeepSpeed下的代码。由于代码行未符合PEP 8长度限制，用户希望添加条件表达式以提高可读性并保留原始功能。,https://github.com/deepspeedai/DeepSpeed/issues/7119
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是XPU设备。由于未来将使用XCCL支持XPU设备，因此希望在DeepSpeed端提供支持，同时保留旧路径以启用torchCCL。,https://github.com/deepspeedai/DeepSpeed/issues/7113
DeepSpeed,这是一篇用户提出需求的issue，主要涉及DeepSpeed中的seq split功能，用户寻求在Domino中增加对其的支持。,https://github.com/deepspeedai/DeepSpeed/issues/7111
DeepSpeed,这是一个关于更新Twitter账号名称的issue，属于用户提出需求的类型。它涉及到DeepSpeed相关的社交媒体账号。其原因可能是加入Linux Foundation AI&Data所导致的账号名称变更需求。,https://github.com/deepspeedai/DeepSpeed/issues/7110
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的梯度聚合操作，用户希望增加一个选项来支持SUM梯度聚合而不是MEAN。,https://github.com/deepspeedai/DeepSpeed/issues/7107
DeepSpeed,这是一个关于需求的问题，主要涉及到深度学习模型中的变量批量大小和学习率调度。由于输入数据具有可变长度，在课程学习等情况下需要动态调整批处理大小，并相应调整学习率。,https://github.com/deepspeedai/DeepSpeed/issues/7104
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的XPU支持，用户希望添加对XCCL的支持。,https://github.com/deepspeedai/DeepSpeed/issues/7103
DeepSpeed,这是一个关于需求的问题，涉及的主要对象是DeepSpeed中的变量batch大小和LR调度器。由于许多应用场景中遇到变长输入，需要动态批次大小和学习率调度。,https://github.com/deepspeedai/DeepSpeed/issues/7102
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed，提出了集成DualPipe以增强训练效率的建议。,https://github.com/deepspeedai/DeepSpeed/issues/7100
DeepSpeed,这是一个关于ChatGPT在训练过程中提高响应质量的一个建议。这个问题主要涉及ChatGPT的响应质量，提出了一种方法来改善对话质量的逻辑精确性。问题可能是一个需求提案。CHATGPT以及对应的团队需要思考如何提高模型在训练阶段的响应质量，避免虚幻现象。,https://github.com/deepspeedai/DeepSpeed/issues/7097
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed的gaudi2 nightly和ci，用户希望更新到最新的1.20.0版本构建。,https://github.com/deepspeedai/DeepSpeed/issues/7093
DeepSpeed,这是一个功能需求类型的issue，主要涉及移除旧版本torch兼容性的工作流程。,https://github.com/deepspeedai/DeepSpeed/issues/7090
DeepSpeed,这个issue类型是更新请求，主要涉及更新DeepSpeed的README.md文件，用户在提出此issue可能是为了添加ICS '23 MoE paper的链接。,https://github.com/deepspeedai/DeepSpeed/issues/7087
DeepSpeed,这是一个关于更新并行性的issue，类型为用户提出需求。该问题主要涉及DeepSpeed中nv-torch-latest/nightly测试的并行性。,https://github.com/deepspeedai/DeepSpeed/issues/7086
DeepSpeed,这是一个需求提出类型的issue，涉及主要对象是Domino for Llama3的更新。,https://github.com/deepspeedai/DeepSpeed/issues/7084
DeepSpeed,这是一个类型为改进建议的issue，主要涉及的对象是DeepSpeed项目中的构建和打包过程。原因是因为未来pip/python等工具的变化，需要修改传统的构建方式来适应最新的标准。,https://github.com/deepspeedai/DeepSpeed/issues/7069
DeepSpeed,这是一个用户提出需求的issue，在更新DeepSpeed的whl构建命令时遇到问题。,https://github.com/deepspeedai/DeepSpeed/issues/7068
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及的对象是DeepSpeed的README文件。由于项目新增了最新的加速器，用户希望更新README文件以提供相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/7065
DeepSpeed,这是一个更新类的issue，涉及的主要对象是DeepSpeed软件的版本控制文件。,https://github.com/deepspeedai/DeepSpeed/issues/7063
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及DeepSpeed在Windows环境下无法满足特定版本要求的问题，用户请求发布Windows wheels构建工作流程以便自行尝试构建满足需求的环境。,https://github.com/deepspeedai/DeepSpeed/issues/7057
DeepSpeed,这是一个API更名的修改请求，涉及的主要对象是DeepSpeed库中的某个参数。由于API名称的变更，用户需要更新其代码以适应新的命名规范。,https://github.com/deepspeedai/DeepSpeed/issues/7056
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的PipelineModule，用户想知道如何在自己的module中设置不同的子模块在不同的GPU上运行，并询问如何处理多个输入输出参数的情况。,https://github.com/deepspeedai/DeepSpeed/issues/7053
DeepSpeed,这是关于更新DeepSpeed中ROCm cupy的处理方式的问题单，类型为需求更新，主要涉及对象为`setup.py`文件处理ROCm cupy。,https://github.com/deepspeedai/DeepSpeed/issues/7051
DeepSpeed,这是一个需求更新类型的issue，主要涉及的对象是DeepSpeed中的CUDA compute capability，由于需要支持Blackwell架构而进行更新。,https://github.com/deepspeedai/DeepSpeed/issues/7047
DeepSpeed,这是一个用户提出需求的issue，涉及主要对象为DeepSpeed的ZeRO分区状态和NVMe offload，由于NVMe offloading场景下性能至关重要，需要扩展API以支持debugging和修改ZeRO分区状态，添加向量化更新API。,https://github.com/deepspeedai/DeepSpeed/issues/7046
DeepSpeed,这个issue属于需求提出类型，主要涉及DeepSpeed库中的AutoTP功能。这个问题的提出可能是为了增加对DeepSpeedV3版本的AutoTP功能的支持。,https://github.com/deepspeedai/DeepSpeed/issues/7045
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是在Linux系统上使用RTX 5090 GPU以及特定版本的Linux Driver、Pytorch和Deepspeed来fine-tuning LLM（Lama38B），导致问题的原因可能是环境配置或软件版本不兼容。,https://github.com/deepspeedai/DeepSpeed/issues/7042
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed Ulysses论文中的模糊性问题，由于Figure 2和Section 3.1之间的不一致导致了关于方法具体实现细节的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/7041
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的activation checkpoint API，由于不支持关键字参数导致出现问题。,https://github.com/deepspeedai/DeepSpeed/issues/7038
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed推理中缺少列线性层的实现，导致在使用单个线性层时无法处理整个隐藏状态数据，需要实现列线性层与全聚合功能以解决问题。,https://github.com/deepspeedai/DeepSpeed/issues/7037
DeepSpeed,这个issue为用户提出需求类型，主要涉及添加关于DeepSpeed的中文博客。,https://github.com/deepspeedai/DeepSpeed/issues/7034
DeepSpeed,这是一个关于更新DeepSpeed成为PEP517 compliant的issue，主要涉及的对象是DeepSpeed软件包。原因是为了满足即将到来的pip 25.1更改和避免出现可编辑安装错误。,https://github.com/deepspeedai/DeepSpeed/issues/7033
DeepSpeed,这是一个更新请求，主要涉及DeepSpeed项目的支持更新至支持`pyproject.toml`，由于该项目需要符合PEP517规范，因此用户提出了这个更新请求。,https://github.com/deepspeedai/DeepSpeed/issues/7032
DeepSpeed,这是一个需要更新DeepSpeed以符合PEP 517标准的问题，涉及主要对象是DeepSpeed库。这个问题出现的原因是为了处理未来pip的变化，需要使DeepSpeed与PEP 517标准兼容。,https://github.com/deepspeedai/DeepSpeed/issues/7031
DeepSpeed,这是一个需求类型的issue，涉及的主要对象是DeepSpeed下的fp_quantizer扩展，用户提出了在ROCm上成功构建该扩展所需的更改。,https://github.com/deepspeedai/DeepSpeed/issues/7027
DeepSpeed,这是一个用户需求类型的issue，主要对象是DeepSpeed项目中的AIO library，并提出了为AMD GPU添加编译支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/7023
DeepSpeed,这是用户提出的一个需求请求，询问DeepSpeed是否会支持mamba模型。,https://github.com/deepspeedai/DeepSpeed/issues/7022
DeepSpeed,这个issue类型是用户提出需求，涉及的主要对象是实现动态批处理和学习速率调整。由于需要处理可变长度的输入数据（句子），在训练中需要根据token数量动态调整批处理大小，并相应调整学习速率。,https://github.com/deepspeedai/DeepSpeed/issues/7020
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed下Multinode training的性能问题，用户希望添加选项以在每个节点内部对权重进行分片。由于多节点训练中的阶段3速度过慢，用户提出了这样的解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/7019
DeepSpeed,这是一个功能需求提出的issue，主要涉及DeepSpeed中支持训练多个模型的问题。由于loss来源于多个DeepSpeed引擎，导致之前实现的backward方法无法正常工作。,https://github.com/deepspeedai/DeepSpeed/issues/7018
DeepSpeed,该issue属于用户提出需求类型，主要涉及如何控制模块的并行方式，用户想要对不同模块应用不同的并行策略。,https://github.com/deepspeedai/DeepSpeed/issues/7017
DeepSpeed,这是一个需求更新的问题，涉及主要对象为测试用例，用户寻求更新A6000测试用例的transformers版本。,https://github.com/deepspeedai/DeepSpeed/issues/7016
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed引擎在RLHF训练中的支持。原因可能是需要在RLHF中支持类似vLLM睡眠模式和OpenRLHF混合引擎模式的功能。,https://github.com/deepspeedai/DeepSpeed/issues/7013
DeepSpeed,这是一个功能需求相关的issue，主要涉及的对象是DeepSpeed库。由于新版本Python 3.11和3.12发布，用户希望在DeepSpeed中启用针对这些版本的测试。,https://github.com/deepspeedai/DeepSpeed/issues/7007
DeepSpeed,这是一个用户提出需求的issue，主要涉及将LongVU与DeepSpeed框架集成。,https://github.com/deepspeedai/DeepSpeed/issues/7006
DeepSpeed,这个issue类型为功能需求更新，涉及到DeepSpeed的XPU模块，用户提出更新intel oneAPI basekit和torch/ipex/oneccl的版本需求。,https://github.com/deepspeedai/DeepSpeed/issues/7003
DeepSpeed,该issue类型是更新GitHub组织引用，主要涉及的对象是GitHub组织引用。由于可能组织引用名称、链接发生变化或错误，用户需要对GitHub组织引用进行更新。,https://github.com/deepspeedai/DeepSpeed/issues/6998
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed模型训练过程中如何在每个epoch后以及最终记录模型准确率的问题。由于缺乏关于如何使用Monitor来记录准确率的具体指导，用户寻求帮助和建议。,https://github.com/deepspeedai/DeepSpeed/issues/6996
DeepSpeed,这是一个关于功能需求的issue，主要涉及了DeepSpeed中使用torch.autocast与ZeRO的集成。由于DeepSpeed与torch.autocast的混合精度训练行为差异，用户提出了需要对两者进行整合以解决性能、内存使用和精度方面的差异。,https://github.com/deepspeedai/DeepSpeed/issues/6993
DeepSpeed,这是一个功能改进建议，主要涉及到DeepSpeed项目中关于Triton的文件导入问题。由于有些系统虽然安装了 Triton 但不支持 Triton，在导入这些文件时可能会导致问题。,https://github.com/deepspeedai/DeepSpeed/issues/6989
DeepSpeed,这是一个用户提出的需求，涉及DeepSpeed库的类型提示和`py.typed`元数据问题，由于缺少类型提示和`py.typed`标记文件导致了`mypy`报告错误。,https://github.com/deepspeedai/DeepSpeed/issues/6988
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed项目在Windows平台上构建的版本更新需求。,https://github.com/deepspeedai/DeepSpeed/issues/6983
DeepSpeed,该issue属于用户提出需求的类型，主要涉及DeepSpeed在Windows系统上编译困难导致需要分享WHL文件的问题。,https://github.com/deepspeedai/DeepSpeed/issues/6981
DeepSpeed,这个issue为需求问题，主要涉及的对象是DeepSpeed下的cpu torch模块。由于torch 2.6的更新，需要更新cpu torch模块以适配最新版本。,https://github.com/deepspeedai/DeepSpeed/issues/6977
DeepSpeed,这是一个功能需求类型的issue，主要涉及的对象是DeepSpeed库。这个issue提出了为DeepSpeed添加对CUDA 12.8的支持以及关于CUDA 12.7的评论。,https://github.com/deepspeedai/DeepSpeed/issues/6975
DeepSpeed,这是一个关于更新Docker容器版本导致问题的issue，涉及主要对象为A6000工作流。出现问题的原因可能是与nvsd更新相关。,https://github.com/deepspeedai/DeepSpeed/issues/6967
DeepSpeed,这个issue属于更新版本信息的类型，涉及主要对象为版本控制文件version.txt，由于需要自动更新版本信息，作者是为了在DeepSpeed发布后更新版本号。,https://github.com/deepspeedai/DeepSpeed/issues/6965
DeepSpeed,这个issue属于功能改进类型，主要涉及到DeepSpeed中MoE模型的高性能自动张量并行性能优化。原因是为了减少每个MoE层中路由的专家的AllReduce操作次数，从而提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/6964
DeepSpeed,这是一个功能改进类的issue，主要涉及DeepSpeed库中的GPU upcast功能。原因是为了提高性能效率，替换隐式upcast为显式upcast。,https://github.com/deepspeedai/DeepSpeed/issues/6962
DeepSpeed,"这是一个用户提出需求的类型的issue，主要涉及DeepSpeed库中的""Hierarchical All-to-all""特性是否可用的问题。用户提出了关于这一特性是否会在将来支持的疑问。",https://github.com/deepspeedai/DeepSpeed/issues/6957
DeepSpeed,这个issue属于功能需求提议，主要涉及DeepSpeed框架中的FPDT功能基于Intel后端的支持。,https://github.com/deepspeedai/DeepSpeed/issues/6956
DeepSpeed,这是一个用户提出需求的 issue，请求添加一个 FPDT backward test。,https://github.com/deepspeedai/DeepSpeed/issues/6955
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的Linear类，通过给部分Linear类添加`extra_repr`方法来提供额外信息以便进行调试。这个需求的提出是为了帮助调试时更方便查看相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/6954
DeepSpeed,这是一个用户提出需求的issue，主要对象是希望DeepSpeed的Pipeline Parallelism支持多优化器训练。由于当前pipeline_module似乎不支持多优化器，用户提出了希望添加这一功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6951
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed的数据加载器（dataloader）。由于缺少配置选项，用户需要将dataloader的shuffle设置为true。,https://github.com/deepspeedai/DeepSpeed/issues/6950
DeepSpeed,这是一个功能增强的issue，主要涉及DeepSpeed下的sharded_moe.py文件，用户提出了关于top2 gate和Tutel之间冲突的问题。,https://github.com/deepspeedai/DeepSpeed/issues/6948
DeepSpeed,这是一个需求提出的issue，主要涉及DeepSpeed的Q1 2025路线图规划。,https://github.com/deepspeedai/DeepSpeed/issues/6946
DeepSpeed,该issue类型是用户提出需求，问题涉及如何优化一个类中包含两个模型时只优化其中一个模型的参数。这个问题的原因是DeepSpeed和Accelerate包只支持一个optimizer，导致用户需要将两个模型的参数放在同一个optimizer中，但用户希望仅优化其中一个模型的参数。,https://github.com/deepspeedai/DeepSpeed/issues/6943
DeepSpeed,这是一个用户提出需求的 issue，主要涉及的对象是 DeepSpeed 软件的安全性期望。用户希望能够获得关于使用 DeepSpeed 所需具备的安全期望的信息。,https://github.com/deepspeedai/DeepSpeed/issues/6941
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的OPTEmbedding模块，由于需要新增加一个`position_ids`参数，用户提出了更新forward函数的请求。,https://github.com/deepspeedai/DeepSpeed/issues/6939
DeepSpeed,这是一个需求更改类型的issue，涉及主要对象为DeepSpeed下的nv-a6000工作流程，由于Hugging Face Transformers库的更新（pull request #35235）导致需要修改工作流程的问题。,https://github.com/deepspeedai/DeepSpeed/issues/6938
DeepSpeed,这是一个需求提出的issue，涉及的主要对象是DeepSpeed中的Deepseek模块。由于新增的MLA和MoE功能需要特殊处理的层级，作者提出了对特定层级的跳过和默认设置问题。,https://github.com/deepspeedai/DeepSpeed/issues/6937
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed linear模块在非cuda系统下的实现问题。,https://github.com/deepspeedai/DeepSpeed/issues/6932
DeepSpeed,这是一个用户需求问题，主要涉及到如何在多个节点上分布式加载模型，其中用户提出了希望在多个节点之间分配模型加载而无需在单个节点上完全加载的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6927
DeepSpeed,这个issue类型是代码优化，涉及主要对象是DeepSpeed ops/transformer/inference测试的清理。,https://github.com/deepspeedai/DeepSpeed/issues/6925
DeepSpeed,该issue类型为功能需求提出，主要涉及DeepSpeed中的auto tensor parallel training功能。由于当前模型文件保存功能存在问题，导致用户需要设置特定的参数才能保存HF模型文件。,https://github.com/deepspeedai/DeepSpeed/issues/6922
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed Inference对视觉语言模型的支持问题，用户希望扩展`deepspeed.init_inference`以支持视觉语言模型，因为当前该功能不适用于此类模型。,https://github.com/deepspeedai/DeepSpeed/issues/6917
DeepSpeed,这是一个用户提出需求的类型，用户想知道是否支持使用fp8进行训练。由于DeepSpeed目前不支持fp8训练，用户提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/6908
DeepSpeed,这是一个用户提出需求的类型，涉及到更新Gaudi2作业到最新1.19版本构建。原因可能是用户希望能够使用最新的构建版本来继续进行项目开发。,https://github.com/deepspeedai/DeepSpeed/issues/6905
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed支持Tecorigin SDAA加速器的功能需求。,https://github.com/deepspeedai/DeepSpeed/issues/6903
DeepSpeed,这个issue是一个用户提出的需求请求，请求DeepSpeed支持XLA/TPU。用户希望DeepSpeed集成XLA作为后端，使TPU用户能够利用DeepSpeed的优化功能。,https://github.com/deepspeedai/DeepSpeed/issues/6901
DeepSpeed,这是一个需求提议，主要对象是DeepSpeed中的hpu_accelerator模块，用户提议使用formal API替换hpu.setDeterministic。,https://github.com/deepspeedai/DeepSpeed/issues/6897
DeepSpeed,这是一个自动更新版本号的Issue，类型为维护类任务，涉及的主要对象是DeepSpeed的版本控制文件。,https://github.com/deepspeedai/DeepSpeed/issues/6893
DeepSpeed,这是一个用户提出需求的issue，主要涉及使用DeepSpeed进行多GPU环境下在HuggingFace MoE模型上进行推断时遇到困难。原因可能是官方教程不够详细，文档也没有提供足够的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/6891
DeepSpeed,这是一个更新代码所有者（Code Owners）的issue，属于用户提出需求类型，主要对象是代码库中的贡献者。,https://github.com/deepspeedai/DeepSpeed/issues/6890
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在多节点上使用zero3时训练速度较慢的问题，原因是模型参数被分布到所有节点的所有GPU导致通信延迟高。,https://github.com/deepspeedai/DeepSpeed/issues/6889
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中的sequence parallel cross entropy功能，需要添加`ignore_index`参数来实现与Hugging Face的完全集成。,https://github.com/deepspeedai/DeepSpeed/issues/6882
DeepSpeed,这是一个用户需求类型的issue，主要涉及的对象是DeepSpeed项目的技术社区委员会（TSC）。由于缺乏具体的内容描述，无法确切分析导致了何种症状的问题或需求。,https://github.com/deepspeedai/DeepSpeed/issues/6867
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中导航功能的修改。由于缺少domino item的导航列表，用户请求将其添加进去。,https://github.com/deepspeedai/DeepSpeed/issues/6866
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中Domino的通信overlap实现问题。用户提出了关于代码中通信overlap部分缺失或者实现不完整的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/6858
DeepSpeed,该issue是一个功能需求，主要涉及DeepSpeed中的配置变量，用户提出需要添加一个配置变量来控制加载检查点到模型参数时是否将其移到设备上。,https://github.com/deepspeedai/DeepSpeed/issues/6846
DeepSpeed,这是一个用户提出需求的issue，主要涉及到对Ulysses重构的建议。由于当前代码中存在一些不必要的操作和复杂结构，导致代码难以理解和性能下降。,https://github.com/deepspeedai/DeepSpeed/issues/6843
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的Domino集成到Nanotron库来增强其张量并行能力。,https://github.com/deepspeedai/DeepSpeed/issues/6835
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的MLP/lm_head粒度设置，由于DNN库偏爱2的幂的张量大小，希望能够灵活设置MLP/lm_head的颗粒度大小。,https://github.com/deepspeedai/DeepSpeed/issues/6828
DeepSpeed,这是一个需求类型的issue，涉及DeepSpeed的README.md文档更新，主要是为了在News页面上添加Ulyssesoffload。,https://github.com/deepspeedai/DeepSpeed/issues/6825
DeepSpeed,这是一个用户提出需求的issue，主要对象是更新DeepSpeed中的pre-commit版本。可能是因为当前版本存在某些问题或者需要与其他组件进行适配导致用户希望更新pre-commit版本。,https://github.com/deepspeedai/DeepSpeed/issues/6821
DeepSpeed,该issue类型为需求更新，涉及到DeepSpeed的README.md文件，更新内容为Domino新闻。根据更新内容，用户可能希望了解最新的Domino新闻或相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/6815
DeepSpeed,这个issue是用户提出需求，并涉及到 UlyssesOffload (FPDT)。由于缺少具体描述，用户需要查看相关教程页面以获取更多信息。,https://github.com/deepspeedai/DeepSpeed/issues/6814
DeepSpeed,这个issue属于用户提出需求类型，主要涉及添加FPDT教程的问题。,https://github.com/deepspeedai/DeepSpeed/issues/6813
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed库中的lm_head模型，用户希望在未提供checkpoint的情况下支持lm_head的tp功能。,https://github.com/deepspeedai/DeepSpeed/issues/6812
DeepSpeed,这是一个关于功能逻辑的问题，主要涉及Autotuner的使用以及与ZeRO和Tensor Parallelism的结合。由于对GPU数量的处理存在疑惑，可能导致内存使用存在重复计算的问题。,https://github.com/deepspeedai/DeepSpeed/issues/6796
DeepSpeed,该issue类型是用户提出需求，涉及的主要对象是更新 DeepSpeed 的 Python 版本。这个问题产生的原因是更新 Python 版本后，需要手动将 setuptools 加入 requirements.txt 文件。,https://github.com/deepspeedai/DeepSpeed/issues/6787
DeepSpeed,这是一个更新版本信息的issue，涉及主要对象为DeepSpeed软件。,https://github.com/deepspeedai/DeepSpeed/issues/6786
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是发布工作流程。原因可能是之前的发布工作流程出现了问题，需要回滚到之前的版本。,https://github.com/deepspeedai/DeepSpeed/issues/6785
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO-offload使用CPU和GPU并行计算的功能。由于现有的计算模式导致CPU和GPU无法同时工作，用户希望能够实现这样的并行计算来提高效率。,https://github.com/deepspeedai/DeepSpeed/issues/6778
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed项目中的Domino Blog，问题可能是关于增加或更新博客内容。,https://github.com/deepspeedai/DeepSpeed/issues/6776
DeepSpeed,该issue类型为功能增强，涉及主要对象为DeepSpeed中的FlopsProfiler，由于需要支持einops.einsum操作而引发。,https://github.com/deepspeedai/DeepSpeed/issues/6755
DeepSpeed,这个issue类型是更新请求，涉及主要对象是DeepSpeedExamples中的BingBertSquad路径。这个issue是由DeepSpeedExamples目录结构的重构引起的，需要更新DeepSpeed示例以适应这些变化。,https://github.com/deepspeedai/DeepSpeed/issues/6746
DeepSpeed,该issue类型为用户提出需求，主要涉及DeepSpeed中的Deepspeed-Domino是否兼容其他并行策略，具体问题是关于Domino实现是否与sequence parallel或ring attention并行策略兼容性的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/6744
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是为DeepSpeed项目添加COMMITTER文件。,https://github.com/deepspeedai/DeepSpeed/issues/6741
DeepSpeed,这个issue类型是用户提出需求，请求更新flake8版本。由于原先的flake8版本可能存在一些问题或者不足，用户提出更新版本来改进代码质量检查。,https://github.com/deepspeedai/DeepSpeed/issues/6740
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的AMD apex版本更新。这个问题可能是因为用户想要更新AMD apex版本以解决某些bug或者获得新功能。,https://github.com/deepspeedai/DeepSpeed/issues/6739
DeepSpeed,这是一个关于更新格式工作流程的问题，类型为需求反馈，涉及的主要对象是DeepSpeed项目。,https://github.com/deepspeedai/DeepSpeed/issues/6738
DeepSpeed,这是一个用户请求帮助的issue，主要涉及DeepSpeed中offload stage3部分的源代码学习问题，用户寻找不到CPU和GPU之间梯度调度过程的代码。,https://github.com/deepspeedai/DeepSpeed/issues/6735
DeepSpeed,这个issue是关于性能优化的，主要是为了增加训练Llama-based MoE架构的性能。,https://github.com/deepspeedai/DeepSpeed/issues/6734
DeepSpeed,这是一个需求类型的issue，涉及的主要对象是添加Domino代码。,https://github.com/deepspeedai/DeepSpeed/issues/6733
DeepSpeed,这是一个更新版本信息的Issue，不是bug报告或用户需求。,https://github.com/deepspeedai/DeepSpeed/issues/6731
DeepSpeed,这个issue类型是一个需求，该问题单涉及的主要对象是CI coverage for Gaudi2。,https://github.com/deepspeedai/DeepSpeed/issues/6728
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中`ZeRO-3`和`MP8`模型转换为通用检查点的问题。由于转换工具未合并模型状态，导致无法将`ZeRO-3`和`MP8`检查点模型状态合并成单个文件。,https://github.com/deepspeedai/DeepSpeed/issues/6724
DeepSpeed,这个issue类型是功能需求，涉及的主要对象是更新DeepSpeed的flake8版本，由于需要更新flake8以在新版本的Python上运行，特别是在GitHub上更新到的较新的ubuntulatest版本中。,https://github.com/deepspeedai/DeepSpeed/issues/6722
DeepSpeed,这是一个关于更新GitHub托管工作流程至Ubuntu 24.04的建议类型的issue，涉及的主要对象是DeepSpeed项目的工作流程。该问题是由于Ubuntu版本变化而可能导致的工作流程功能受到影响，需要测试确认更新过程中是否会出现问题。,https://github.com/deepspeedai/DeepSpeed/issues/6717
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的DSChat工具支持最新的transformers模型。用户希望实现新的transformers模型在DSChat中的支持。,https://github.com/deepspeedai/DeepSpeed/issues/6711
DeepSpeed,这是一个用户提出需求的类型问题，主要对象是 DeepSpeed 中的 sequence parallel 和 MoE 功能。用户询问如何在纯 DeepSpeed 代码库中使用 sequence parallel 或 MoE，提到需要开发者创建特定的 process group。,https://github.com/deepspeedai/DeepSpeed/issues/6708
DeepSpeed,这是一个需求类型的issue，涉及的主要对象是DeepSpeed中的README页面。由于URL需要更新，导致Pipeline Status for Huawei Ascend NPU无法显示正确的信息。,https://github.com/deepspeedai/DeepSpeed/issues/6706
DeepSpeed,这个issue属于用户提出需求的类型，主要涉及DeepSpeed无法兼容非逐元素优化器，导致训练出现问题。,https://github.com/deepspeedai/DeepSpeed/issues/6701
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是DeepSpeed，由于没有重新使用分布式环境导致重复创建环境带来不必要的开销。,https://github.com/deepspeedai/DeepSpeed/issues/6697
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed如何避免对指定参数进行all_reduce，问题源于用户希望通过特定参数列表绕过all_reduce操作。,https://github.com/deepspeedai/DeepSpeed/issues/6690
DeepSpeed,这是一个关于DeepSpeed中ZeRO 1和ZeRO 2中all-reduce重叠的请求，涉及问题主要是在训练GPTBased模型时allreduce与backward计算未能重叠，可能由于allreduce中的copy操作阻塞了后续的backward计算。,https://github.com/deepspeedai/DeepSpeed/issues/6685
DeepSpeed,这是一个需求类的issue，主要涉及DeepSpeed项目中需要移除不再需要更新的软件包。原因可能是为了简化容器内的软件组件，并保持项目的最新状态。,https://github.com/deepspeedai/DeepSpeed/issues/6682
DeepSpeed,这是一个技术支持请求，关于如何在当前硬件上支持使用DeepSpeed进行6b模型微调的问题。由于GPU显存不足，导致在反向传播阶段出现CUDA内存不足的情况。,https://github.com/deepspeedai/DeepSpeed/issues/6679
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed下的Gaudi2 docker image。,https://github.com/deepspeedai/DeepSpeed/issues/6677
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中添加一个no_sync上下文管理器，由于缺乏torch等效的API导致问题产生。,https://github.com/deepspeedai/DeepSpeed/issues/6675
DeepSpeed,这个issue是一个功能需求的提交，主要对象是DeepSpeed，在PyTorch版本大于2.3时允许编译collective operations的功能。,https://github.com/deepspeedai/DeepSpeed/issues/6674
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中的8bit优化器问题，用户希望支持8bit优化器以减少CPU内存需求。,https://github.com/deepspeedai/DeepSpeed/issues/6666
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed的zero_to_fp32脚本的并行转换功能，由于模型规模增大导致转换耗时较长。,https://github.com/deepspeedai/DeepSpeed/issues/6655
DeepSpeed,这是一个关于功能需求的问题，涉及主要对象为DeepSpeed的性能优化。提问者想了解是否需要安装apex库来增强DeepSpeed在混合精度训练下的性能。,https://github.com/deepspeedai/DeepSpeed/issues/6653
DeepSpeed,这是一个更新请求，用于更新DeepSpeed版本号的issue，主要涉及到版本控制。,https://github.com/deepspeedai/DeepSpeed/issues/6652
DeepSpeed,该issue类型为功能增强提议，涉及主要对象为DeepSpeed中的zero optimization模块。由于逻辑复用困难，导致用户提议添加更精细的优化选择以减少主机开销和提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/6649
DeepSpeed,这是一个更新需求类型的issue，主要涉及DeepSpeed中的Gaudi2 docker版本更新到最新发布版（1.18），并需要更新Gaudi2节点的固件版本为1.18。,https://github.com/deepspeedai/DeepSpeed/issues/6648
DeepSpeed,这是一个关于需求提出的问题，主要涉及DeepSpeed中Zero3在部分模型上无法使用的问题。原因是模型输入/输出的大小和模型结构可能会变化，导致无法直接在整个模型上应用Zero3。,https://github.com/deepspeedai/DeepSpeed/issues/6642
DeepSpeed,这个issue属于用户提出需求类型，主要涉及AIO和GDS操作的读写函数添加文件偏移选项。,https://github.com/deepspeedai/DeepSpeed/issues/6641
DeepSpeed,这是一个用户提需求类型的issue，主要对象是关于DeepSpeed中的[XPU] host timer，用户提出了在Torch 2.6中支持Elapsed time的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6633
DeepSpeed,这个issue类型是功能需求，涉及的主要对象是测试运行日志记录。这个功能的目的是用于调试偶尔失败或卡住的测试，日志记录开启时，当设置`RUNNING_TEST_LOG_FILE`时可生效。,https://github.com/deepspeedai/DeepSpeed/issues/6627
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed框架中关于pipeline inference的演示或教程。由于官方教程只包含了有关PP用于训练的部分，因此用户在寻找关于混合pipeline inference策略的教程或经验。,https://github.com/deepspeedai/DeepSpeed/issues/6619
DeepSpeed,这是一个更新型issue，涉及DeepSpeed中的测试环境和依赖的更新，由于Hugging Face库中加速功能的修复，不再需要钉选定的Accelerate版本，同时支持最新的Ubuntu版本和Node.js版本。,https://github.com/deepspeedai/DeepSpeed/issues/6611
DeepSpeed,该issue为用户请求更改V100工作流程以适应cuda 12.1，主要涉及到在DeepSpeed中对V100 GPU的支持。,https://github.com/deepspeedai/DeepSpeed/issues/6607
DeepSpeed,该issue为用户提出需求类型，主要涉及DeepSpeed下的Domino代码发布的时间表询问。,https://github.com/deepspeedai/DeepSpeed/issues/6605
DeepSpeed,这个issue类型属于功能增强请求，主要涉及的对象是项目的最佳实践认证。,https://github.com/deepspeedai/DeepSpeed/issues/6604
DeepSpeed,这是一个功能需求类型的issue，涉及DeepSpeed日志控制的改进。原因是要禁用默认的`steps_per_print`。,https://github.com/deepspeedai/DeepSpeed/issues/6602
DeepSpeed,这个issue类型属于功能增强，主要对象是DeepSpeed项目中的图断点（graph break）功能，用户提出了显示图断点分布的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6601
DeepSpeed,这个issue是一个用户提出的需求，主要对象是`offload_states` API，由于目前无法在使用CPU-based optimizer的模型上实现优化，用户希望扩展支持DeepSpeedCPUAdam optimizer的模型。,https://github.com/deepspeedai/DeepSpeed/issues/6596
DeepSpeed,这是一个功能请求（Feature Request），主要涉及DeepSpeed的ZeRO-3模型推断支持。用户提出无法将已配置为ZeRO3推断的模型移动到CPU的问题，希望能拓展API以支持此功能。,https://github.com/deepspeedai/DeepSpeed/issues/6595
DeepSpeed,这是一个用户提出需求的issue，主要对象是AIO库中的CPU锁定张量功能，并需要恢复在CPU加速器中异步I/O操作符的可用性。,https://github.com/deepspeedai/DeepSpeed/issues/6592
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中ZeRO gradients更新的API，用户想要添加一个新的API用于更新ZeRO gradients。,https://github.com/deepspeedai/DeepSpeed/issues/6590
DeepSpeed,这是一个功能增强类型的issue，主要涉及添加README Pipeline Status以验证华为Ascend NPU。原因是实现了一个CI pipeline来验证该硬件。,https://github.com/deepspeedai/DeepSpeed/issues/6588
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed的offload states。由于循环导入关系，需要重构代码以解决问题。,https://github.com/deepspeedai/DeepSpeed/issues/6586
DeepSpeed,该issue是关于添加CI测试用例以验证华为Ascend NPU的功能，并不是bug报告。主要涉及的对象是DeepSpeed项目中的CI pipelines。,https://github.com/deepspeedai/DeepSpeed/issues/6580
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的支持safetensors导出的功能。,https://github.com/deepspeedai/DeepSpeed/issues/6579
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed的llama3.211b和llama3.290b vision模型和text模型，需动态设置num_kv_heads。,https://github.com/deepspeedai/DeepSpeed/issues/6577
DeepSpeed,这是一个关于功能改进的issue，主要涉及DeepSpeed中的LR调整和优化器步骤顺序的问题造成的相关症状。,https://github.com/deepspeedai/DeepSpeed/issues/6575
DeepSpeed,这是用户提出的功能需求。该问题单涉及的主要对象是AdvancedSecure项目中的加密技术服务。,https://github.com/deepspeedai/DeepSpeed/issues/6561
DeepSpeed,该issue为需求提出，主要涉及的对象是DeepSpeed中的NVMe性能优化工具，用户希望添加性能调优实用工具并更新教程以包含调优部分。,https://github.com/deepspeedai/DeepSpeed/issues/6560
DeepSpeed,这是一个用户提出的需求，主要涉及DeepSpeed中的Tensor Parallelism（TP）自动配置，由于一些特定线性模块需要特殊处理，如运行跨多个HPU/GPU卡后进行全局归约操作，权重不可分割或者不应分割到不同卡上以避免性能降低。,https://github.com/deepspeedai/DeepSpeed/issues/6553
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的CUDA Graphs和ZeRO-3/TP+PP的结合使用情况。用户疑惑是否能够在此情况下使用CUDA Graphs并探讨了可能的收益和限制。,https://github.com/deepspeedai/DeepSpeed/issues/6552
DeepSpeed,这是一个关于用户提出需求的issue，主要涉及在DeepSpeed框架下如何加载通用checkpoint的问题。用户希望提供一个仅包含deepspeed和pytorch的最小示例，以便与其他框架如pytorch lightning或huggingface集成。,https://github.com/deepspeedai/DeepSpeed/issues/6548
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是DeepSpeed下的pipeline engine，问题是提出使用msgpack进行P2P通信。,https://github.com/deepspeedai/DeepSpeed/issues/6547
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中关于在设备上保留检查点的配置设置。,https://github.com/deepspeedai/DeepSpeed/issues/6544
DeepSpeed,这是一个用户提出的需求。该问题单涉及的主要对象是加速器设置指南。由于缺少Intel Gaudi在加速器列表中的信息，用户提出需要将其添加到设置指南中。,https://github.com/deepspeedai/DeepSpeed/issues/6543
DeepSpeed,该issue类型为用户提出需求，主要涉及DeepSpeed中动态批量大小和梯度累积的功能，用户请求解释DeepSpeed如何处理梯度累积并解决关于不同分辨率数据集和批量大小冲突的问题。,https://github.com/deepspeedai/DeepSpeed/issues/6533
DeepSpeed,该issue类型为用户提出需求，主要涉及的对象是DeepSpeed库中的推断支持的dtype。由于缺乏bfloat16支持，用户无法运行推断任务使用bfloat16，因此提出添加bfloat16到推断支持dtype的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6528
DeepSpeed,这个issue属于用户提出的需求类型，主要涉及DeepSpeed中zero_to_fp32.py文件，并针对目前70B+模型转换耗时较长的问题，用户提出了希望该脚本能够并行化利用多个CPU核心和线程的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6526
DeepSpeed,这是一个用户提出的需求类型的问题，主要涉及DeepSpeed中的ZeRO3模块如何支持分别包装模型子组件进行训练。该问题由于需要在训练时有效处理模型的不同部分而提出。,https://github.com/deepspeedai/DeepSpeed/issues/6505
DeepSpeed,这个issue是一个功能需求提案，提议在编译时添加选项来禁用日志记录器，以避免图形断裂。,https://github.com/deepspeedai/DeepSpeed/issues/6496
DeepSpeed,这个issue为用户提出需求。主要对象为HPU加速器初始化环境变量。由于缺少所需的环境变量，在初始化加速器时出现了问题。,https://github.com/deepspeedai/DeepSpeed/issues/6495
DeepSpeed,这个issue类型为版本更新，涉及主要对象为DeepSpeed的版本信息，由于需要在0.15.1版本发布后更新version.txt文件。,https://github.com/deepspeedai/DeepSpeed/issues/6493
DeepSpeed,这是一个关于需求的issue，主要对象是DeepSpeed下的zero_optimization模式，用户提出了希望在zero_optimization模式下支持clip_grad_norm功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6480
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed下的Flops Profiler的使用问题，用户希望能够对包括自定义组件在内的复杂算法框架进行FLOPs总结分析，但由于算法的特殊设计和使用context manager引入了额外的复杂性。,https://github.com/deepspeedai/DeepSpeed/issues/6475
DeepSpeed,这是一个功能需求提议，主要涉及加速器支持范畴。,https://github.com/deepspeedai/DeepSpeed/issues/6472
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中Zero3的分区细粒度问题，用户想了解如何设置参数以实现指定的网络层在前向/反向传播中作为一个单元进行批处理。,https://github.com/deepspeedai/DeepSpeed/issues/6469
DeepSpeed,这是一个用户提出需求的issue，主要涉及到对于MiCS和Zero++ hpZ在Hybrid FSDP中的应用和区别的请求，原因是用户希望获得更好的文档、示例或教程来了解这些解决方案的差异和如何最佳地结合它们与Zero3在特定网络拓扑结构下使用。,https://github.com/deepspeedai/DeepSpeed/issues/6467
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中ZeRO 1/2/3模块的参数reduce操作优化。,https://github.com/deepspeedai/DeepSpeed/issues/6463
DeepSpeed,该issue类型为功能需求，主要涉及DeepSpeed中的新功能FPDT，用户可能因为FPDT只能在指定版本的MegatronDeepSpeed中使用而提出了该问题。,https://github.com/deepspeedai/DeepSpeed/issues/6462
DeepSpeed,这个issue类型是需求提出，主要对象是DeepSpeed中的launcher功能。由于缺少关于不使用SSH功能启动launcher的文档，用户提出了添加相关文档的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6455
DeepSpeed,这是一个优化性质的issue，主要涉及到grad norm计算和梯度裁剪的优化方法。,https://github.com/deepspeedai/DeepSpeed/issues/6453
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的Getting Started页面，用户提出在安装部分添加加速器设置指南链接以便用户可以轻松找到文档。,https://github.com/deepspeedai/DeepSpeed/issues/6452
DeepSpeed,这个issue类型是在测试过程中提出的需求，主要涉及如何在不需要Torch作为依赖项的情况下构建DeepSpeed，其原因是为了在发布前更好地进行测试。,https://github.com/deepspeedai/DeepSpeed/issues/6450
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed库中的DeepNVMe教程。用户可能由于需要更详细的操作指南或指导而提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/6449
DeepSpeed,这是一条关于软件版本支持的类型为用户提出需求的issue，主要涉及到DeepSpeed中的fp_quantizer模块，原因是Triton 2.3.x 在 arm64 上支持不好，而用户希望允许使用 Triton 3.0.x 版本以便在 arm 主机上更简化使用。,https://github.com/deepspeedai/DeepSpeed/issues/6447
DeepSpeed,这是一个功能需求提出的issue，主要对象是bf16_optimizer模块中的load_state_dict函数。由于缺少默认数值导致可能的兼容性问题。,https://github.com/deepspeedai/DeepSpeed/issues/6446
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed库中在`deepspeed.initialize`后重新加载模型参数的功能。用户想要在使用ZeRO3进行训练时，动态更新模型的权重。,https://github.com/deepspeedai/DeepSpeed/issues/6411
DeepSpeed,这个issue类型是更新版本信息，涉及的主要对象是DeepSpeed的版本信息文件version.txt。原因是发布了0.15.0版本需要更新版本信息文件。,https://github.com/deepspeedai/DeepSpeed/issues/6403
DeepSpeed,该issue类型为添加功能类型，主要涉及的对象是DeepSpeed项目下的Windows支持博客的日文翻译。,https://github.com/deepspeedai/DeepSpeed/issues/6394
DeepSpeed,这是一个feature需求的issue，主要涉及DeepSpeed中存储和加载CIFAR数据集的相关功能。由于需要在本地/离线使用CIFAR数据集，用户请求添加相关参数及功能以方便操作。,https://github.com/deepspeedai/DeepSpeed/issues/6390
DeepSpeed,这是一个需求提出类型的问题，主要涉及DeepSpeed下的Qwen2系列模型，用户请求支持Qwen2系列模型在DeepSpeedFastGen。,https://github.com/deepspeedai/DeepSpeed/issues/6028
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed框架中的API新增，用户提出了关于优化器、模型和引擎状态离线的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6011
DeepSpeed,这是一个需求类型的issue，主要涉及更新DeepSpeed版本号文件。,https://github.com/deepspeedai/DeepSpeed/issues/5982
DeepSpeed,这个issue类型是更新请求，主要涉及到DeepSpeed下的Gaudi2 CI容器版本更新。原因是要将版本从1.16.2更新到1.17.0。,https://github.com/deepspeedai/DeepSpeed/issues/5937
DeepSpeed,这个issue类型是需求更新，涉及到更新HPU Gaudi docker版本。由于需求变更或技术升级，用户提出寻求更新该docker版本。,https://github.com/deepspeedai/DeepSpeed/issues/5936
DeepSpeed,这个issue是用户提交的需求类型，该问题涉及DeepSpeed库中的class.MD文件，可能是为了增加对类（class）的说明文档。,https://github.com/deepspeedai/DeepSpeed/issues/5901
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed模型的checkpoint加载，用户希望能够在不使用DeepSpeed的情况下加载DeepSpeed的checkpoint，以避免额外的开销。,https://github.com/deepspeedai/DeepSpeed/issues/5895
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed框架下添加MoE模型中topk (k > 2) gates的支持。由于一些用户需要训练MoE模型时使用topk > 2，因此需求增加此功能。,https://github.com/deepspeedai/DeepSpeed/issues/5881
DeepSpeed,这是一个用户提出需求的issue， 主要涉及的对象是DeepSpeed代码库。 导致的问题是用户希望使用`torch.nan_to_num`替换掉之前的numpy包装器。,https://github.com/deepspeedai/DeepSpeed/issues/5877
DeepSpeed,这个issue是关于功能改进的，涉及到LR scheduler配置更新问题。由于当前的LR scheduler初始化行为与优化器初始化行为不一致，导致需要修改以保持一致性。,https://github.com/deepspeedai/DeepSpeed/issues/5846
DeepSpeed,这是一个需求提交类型的issue，主要涉及的对象是DeepSpeed项目中使用的clang-format版本。由于使用了较旧的clang-format版本导致安装最新版本时出现问题，用户提出更新版本和改进格式风格的需求。,https://github.com/deepspeedai/DeepSpeed/issues/5839
DeepSpeed,这个issue类型是功能需求，主要对象是对DeepSpeed加速器设置指南的需求。,https://github.com/deepspeedai/DeepSpeed/issues/5827
DeepSpeed,这是一个用户提出需求的issue，主要涉及将GDS博客链接到DeepSpeed网站。 由于缺少博客链接，用户希望能够在DeepSpeed网站上方便地访问到相关的GDS博客内容。,https://github.com/deepspeedai/DeepSpeed/issues/5820
DeepSpeed,这是一个需求类型的issue，主要涉及到GDS AIO Blog的README和媒体文件。可能由于需要更新blog的文档和媒体资源，用户提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/5817
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed对LLama 405B模型进行多节点多GPU分片的推理/训练操作。该问题出现的原因可能是用户在尝试中遇到了配置上的困难，需要指导如何正确完成操作。,https://github.com/deepspeedai/DeepSpeed/issues/5814
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及如何在导入DeepSpeed时屏蔽警告信息，可能是由于用户不想输出警告和信息而提出。,https://github.com/deepspeedai/DeepSpeed/issues/5801
DeepSpeed,这是一个关于测试更新的issue，主要涉及到MII tests和torchvision。问题可能是由于深度学习框架版本不匹配或依赖库更新导致的。,https://github.com/deepspeedai/DeepSpeed/issues/5800
DeepSpeed,这个issue是关于更新其他工作流以在Ubuntu 22.04上运行的任务，属于需求报告类型，主要涉及DeepSpeed下的工作流程对象。,https://github.com/deepspeedai/DeepSpeed/issues/5798
DeepSpeed,这个issue类型是功能更新，主要对象是DeepSpeed中的OptimizedLinear模块，用户提出了性能和可用性方面的改进需求。,https://github.com/deepspeedai/DeepSpeed/issues/5791
DeepSpeed,该issue是用户提出的需求类型问题，主要涉及DeepSpeed中如何在配置JSON中设置Ulysses。这个问题的产生是由于用户想要了解如何配置Ulysses，以实现特定的功能。,https://github.com/deepspeedai/DeepSpeed/issues/5787
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的auto_tp模块。由于缺少Qwen2RMSNorm类型的loaded layers，用户希望将其添加进去。,https://github.com/deepspeedai/DeepSpeed/issues/5786
DeepSpeed,这是一个用户提交需求的issue，主要涉及DeepSpeed中AutoTP的支持模型列表更新。原因可能是为了向AutoTP添加新支持的模型。,https://github.com/deepspeedai/DeepSpeed/issues/5785
DeepSpeed,这是一个关于需求的issue，涉及DeepSpeed是否支持纯bf16训练，主要问题是当训练70+B模型时，GPU内存成本过高，导致内存消耗严重。,https://github.com/deepspeedai/DeepSpeed/issues/5784
DeepSpeed,这是一个关于更新Ubuntu版本以进行Python测试的问题，属于用户提出需求的类型。,https://github.com/deepspeedai/DeepSpeed/issues/5783
DeepSpeed,该issue类型为功能增强，主要对象是DeepSpeed中的长序列并行化（Ulysses）与HuggingFace的集成。原因是为了增强DeepSpeed长序列（上下文）并行化的功能，支持HuggingFace等模型。,https://github.com/deepspeedai/DeepSpeed/issues/5774
DeepSpeed,这是一个需求更新类的issue，涉及DeepSpeed下的fastgen中支持模型列表的更新，由于最近一次中文README更新后，deepentfastgen中支持的三种模型发生了更新。,https://github.com/deepspeedai/DeepSpeed/issues/5773
DeepSpeed,这是一个功能改进的issue，主要涉及DeepSpeed中关于hpu tensors的包装器，目的是避免在使用torch.compile时导致图断裂。,https://github.com/deepspeedai/DeepSpeed/issues/5771
DeepSpeed,这个issue类型是需求提出，主要涉及DeepSpeed项目中的setup.py和runner.py文件，提出使用accelerator api、选择设备和设置环境变量的需求。,https://github.com/deepspeedai/DeepSpeed/issues/5769
DeepSpeed,这是一个新增功能需求的issue，主要涉及DeepSpeed中添加使用fp8quantized weight的融合内核。,https://github.com/deepspeedai/DeepSpeed/issues/5764
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的DataStatesLLM库的贡献，并针对异步检查点功能进行讨论。原因是团队希望将他们开发的DataStatesLLM库贡献给DeepSpeed社区，并希望得到反馈以进行进一步的合作。,https://github.com/deepspeedai/DeepSpeed/issues/5763
DeepSpeed,这个issue类型为更新需求，主要涉及的对象是DeepSpeed下的一个GitHub仓库。由于当前工作流需要更新以使用node20/checkout，导致该issue被提出。,https://github.com/deepspeedai/DeepSpeed/issues/5757
DeepSpeed,这是一个用户提出需求的问题，主要对象是 DeepSpeed 中的 Launcher mode，用户希望实现 SSH bypass。,https://github.com/deepspeedai/DeepSpeed/issues/5728
DeepSpeed,这是一个需求报告，涉及DeepSpeed中的学习率调度器配置问题，由于当前设计导致LR调度器无法在代码中覆盖配置文件中定义的调度器。,https://github.com/deepspeedai/DeepSpeed/issues/5726
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的异步Checkpoint功能。用户提出了希望实现在训练过程中异步保存Checkpoint的需求，因为当前的Checkpoint操作会阻塞训练过程。,https://github.com/deepspeedai/DeepSpeed/issues/5721
DeepSpeed,这个issue是用户提出需求，主要涉及DeepSpeed下的Universal Checkpoint是否支持MoE Checkpoint，用户想通过Universal Checkpoint来恢复不同GPU上的MoE checkpoint训练。导致该问题的原因是Universal Checkpoint转换MoE Checkpoint时出现错误。,https://github.com/deepspeedai/DeepSpeed/issues/5716
DeepSpeed,这是一个用户提出需求的issue，主要对象是修复教程链接，可能是由于链接失效或指向错误页面导致的。,https://github.com/deepspeedai/DeepSpeed/issues/5714
DeepSpeed,这个issue类型属于用户提出需求，主要涉及的对象是UCP Chinese Blog。由于缺乏详细内容，用户可能提出了关于中文博客的问题或寻求相关帮助。,https://github.com/deepspeedai/DeepSpeed/issues/5713
DeepSpeed,这是一个用户提出需求的issue，涉及更新XPU docker版本。 likely caused by changes in XPU docker version or need to match with specific requirements.,https://github.com/deepspeedai/DeepSpeed/issues/5712
DeepSpeed,该issue属于用户提出需求，主要涉及DeepSpeed Universal Checkpointing (UCP)，用户寻求了关于如何在低成本下训练各种模型的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/5711
DeepSpeed,这个issue类型是用户提出需求，主要对象是DeepSpeed软件的测试路径。这个问题由于需要进一步添加路径以触发特定的xpu测试。,https://github.com/deepspeedai/DeepSpeed/issues/5707
DeepSpeed,这是一个用户提出需求的问题，主要涉及Activation Checkpointing功能的配置设置，由于设置不生效导致GPU内存未能显著减少。,https://github.com/deepspeedai/DeepSpeed/issues/5704
DeepSpeed,这是关于技术改进的issue，主要涉及DeepSpeed的CPUAdam模块。由于CPUAdam移除了对CUDA的依赖，用户可以现在使用相同的源代码。,https://github.com/deepspeedai/DeepSpeed/issues/5703
DeepSpeed,这是一个需求提出类型的issue，主要涉及DeepSpeed在Windows平台下缺少脚本文件的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5699
DeepSpeed,这是一个更新版本信息的issue，涉及对象是DeepSpeed的版本控制，由于发行了版本0.14.4导致需要更新version.txt文件。,https://github.com/deepspeedai/DeepSpeed/issues/5694
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的sequence parallel with communication overlap。由于使用了特定的条件和策略，可以提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/5691
DeepSpeed,这是一个需求讨论类型的issue，涉及DeepSpeed框架中的环境变量设置，主要目的是用于重新缓存数据。,https://github.com/deepspeedai/DeepSpeed/issues/5688
DeepSpeed,该issue类型为需求提出，主要涉及的对象是DeepSpeed项目中的Python版本支持。这个问题由于需要对Python 3.11/3.12进行测试编译以及构建相关的Docker镜像导致。,https://github.com/deepspeedai/DeepSpeed/issues/5676
DeepSpeed,这是一个功能增强类型的issue，主要涉及DeepSpeed中SHM inference_all_reduce功能的FP16支持问题。,https://github.com/deepspeedai/DeepSpeed/issues/5669
DeepSpeed,该issue是一个需求更新类型，涉及的主要对象是更新DeepSpeed配置文件和测试。,https://github.com/deepspeedai/DeepSpeed/issues/5668
DeepSpeed,这是一个用户提出需求的issue，涉及如何为LLMs的不同参数设置不同的学习率，用户希望能为LLama3中的特定参数设置不同的学习率。,https://github.com/deepspeedai/DeepSpeed/issues/5665
DeepSpeed,这是一个关于功能需求的issue，主要涉及DeepSpeed是否支持一种名为AMSP的新DP shard策略。用户想知道DeepSpeed是否已支持或者是否有计划支持类似功能。,https://github.com/deepspeedai/DeepSpeed/issues/5661
DeepSpeed,这个issue类型是功能需求，主要涉及DeepSpeed中的ZeRO 3功能，用户希望能够通过上下文管理器来添加和移除ZeRO 3中的forward hooks以实现更好的推理效果。,https://github.com/deepspeedai/DeepSpeed/issues/5658
DeepSpeed,这个issue属于更新类型，涉及主要对象是DeepSpeed的版本文件version.txt，由于程序发布后需要更新版本号信息导致的相关问题。,https://github.com/deepspeedai/DeepSpeed/issues/5651
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed在使用多GPU时不允许单个GPU上的批量大小为1。原因是DeepSpeed阶段3要求批量大小至少等于GPU数量，导致无法实现按需调整批量大小以避免内存不足的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5645
DeepSpeed,这个issue类型是用户提出需求，要求将DeepSpeed中的Pin加速版本固定在0.30.1，主要对象是DeepSpeed库。此需求可能是由于新版本Pin加速出现了问题，需要将版本固定在0.30.1以避免潜在的错误或兼容性问题。,https://github.com/deepspeedai/DeepSpeed/issues/5628
DeepSpeed,这是一个优化问题，主要涉及DeepSpeed框架下的神经网络模型并行运行过程中的通信优化。,https://github.com/deepspeedai/DeepSpeed/issues/5626
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的Hybrid Offloading功能，该功能允许用户在使用ZeRO3模块时将部分参数转移到主机内存，以减少allgather的使用。,https://github.com/deepspeedai/DeepSpeed/issues/5625
DeepSpeed,这是一个关于功能支持的issue，涉及主要对象是hpu-gaudi2测试内容。由于量化模块尚不被支持，导致需要更新测试内容。,https://github.com/deepspeedai/DeepSpeed/issues/5622
DeepSpeed,这是针对DeepSpeed项目的一个需求类型的issue，主要涉及更新HPU docker镜像。,https://github.com/deepspeedai/DeepSpeed/issues/5621
DeepSpeed,这是一个用户提出的需求，主要涉及DeepSpeed模型在GPU和CPU之间移动时遇到的问题，因为优化器对模型参数的引用在移动时导致GPU内存未被清除。,https://github.com/deepspeedai/DeepSpeed/issues/5620
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中Zero optimizer的重置问题。导致用户提出该问题的原因是无法像在原生PyTorch中那样重置Zero optimizer，用户想知道是否有解决方法。,https://github.com/deepspeedai/DeepSpeed/issues/5615
DeepSpeed,这个issue类型是用户提出需求，主要对象是FastGen项目。该需求的原因可能是希望在FastGen中添加对Phi-3 small的支持。,https://github.com/deepspeedai/DeepSpeed/issues/5614
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed工具中与推理相关的功能。该需求是希望能够启用torch编译用于推理，可能由于现有功能限制或用户需求而导致。,https://github.com/deepspeedai/DeepSpeed/issues/5612
DeepSpeed,该issue属于用户提出需求类型，主要涉及的对象是Windows操作系统。由于Windows操作系统无法支持推断编译导致的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5609
DeepSpeed,这个issue类型是功能需求，并且涉及DeepSpeed的universal checkpoint特性。由于需要在转换通用检查点时注入必要信息，因此用户提出了这个功能需求。,https://github.com/deepspeedai/DeepSpeed/issues/5608
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的PaRO模块的上游修改。这个需求是由于当前的分布式策略在异构网络中存在局限性，需要更优化的分布式策略来提高训练效率。,https://github.com/deepspeedai/DeepSpeed/issues/5607
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是DeepSpeed代码库。由于用户可能对DeepSpeed的功能有特定需求或建议，因此提出了关于在特性分支上工作的建议。,https://github.com/deepspeedai/DeepSpeed/issues/5600
DeepSpeed,这个issue属于用户提出需求，并涉及在日本举办的meetup所需的幻灯片展示。,https://github.com/deepspeedai/DeepSpeed/issues/5598
DeepSpeed,该issue为一个关于用户提出需求的请求，主要涉及DeepSpeedHybridEngine中支持自定义生成循环的功能。由于transformers.PreTrainedModel.generate方法不支持除module.generate之外的其他生成循环，导致无法使用DeepSpeedHybridEngine执行引导生成。,https://github.com/deepspeedai/DeepSpeed/issues/5595
DeepSpeed,这是一个功能增强类型的issue，该问题涉及的主要对象是DeepSpeed库中的CUDA版本兼容性。这个问题的原因是需要将CUDA版本12.4和12.5添加到兼容性的列表中。,https://github.com/deepspeedai/DeepSpeed/issues/5591
DeepSpeed,这是用户提出需求的类型issue，主要涉及DeepSpeed下的FastGen H100 MoE支持，用户希望添加PyTorch中的多gemm MOE实现。由于需要增加新的功能特性，用户提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/5586
DeepSpeed,这是一个功能改进的类型，该问题单涉及的主要对象是DeepSpeed中的编译模块(wrapper），因为编译模块会给访问编译模型属性带来各种限制，因此需要移除编译模块以简化对编译模型的访问。,https://github.com/deepspeedai/DeepSpeed/issues/5581
DeepSpeed,这是一个优化建议类型的issue，主要涉及DeepSpeed下的CPU端SHM based allreduce在处理小消息大小时性能受同步延迟影响。,https://github.com/deepspeedai/DeepSpeed/issues/5571
DeepSpeed,这是一个用户提出需求的类型，主要涉及HPU docker版本更新。由于当前版本不适用，用户希望更新至新版本。,https://github.com/deepspeedai/DeepSpeed/issues/5566
DeepSpeed,这是一个用户提出需求的issue，主要对象是为DeepSpeed-FastGen添加对Microsoft Phi-3模型的支持。这个issue由于用户希望在FastGen中添加对该模型的支持，以便在输出中使用特定提示来展示DeepSpeed的功能和特点。,https://github.com/deepspeedai/DeepSpeed/issues/5559
DeepSpeed,这个issue类型是优化建议，主要涉及的对象是DeepSpeed的unscale_and_clip_grads函数。原因是使用if语句导致设备和主机之间同步，而采用clamp操作可以在设备上更高效地完成相同操作。,https://github.com/deepspeedai/DeepSpeed/issues/5547
DeepSpeed,该issue是用户提出需求类型，主要涉及DeepSpeed-Ulysses与DeepSpeed Zero整合的解决方案何时可用，用户寻求帮助确认可用时间。,https://github.com/deepspeedai/DeepSpeed/issues/5542
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及DeepSpeed的安装问题。由于用户希望只安装torch的CPU版本而不安装nvidia相关的包，导致了安装DeepSpeed时出现了意外的nvidia相关包安装。,https://github.com/deepspeedai/DeepSpeed/issues/5541
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中添加chatglm2和chatglm3 autotp功能，要求调整权重顺序以适配模型结构。,https://github.com/deepspeedai/DeepSpeed/issues/5540
DeepSpeed,该issue为需求类型，涉及DeepSpeed项目中的manifest文件，由于未覆盖.hpp文件导致需要更新manifest文件。,https://github.com/deepspeedai/DeepSpeed/issues/5532
DeepSpeed,这是一个用户请求改进类型的issue，主要涉及的对象是DeepSpeed中的pynvml模块。这个问题产生的原因是pynvml模块似乎已被弃用，希望转换为nvidia-ml-py模块，用户请求将pynvml更换为nvidia-ml-py。,https://github.com/deepspeedai/DeepSpeed/issues/5529
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中创建最小化的通用检查点信息用于客户端状态。这个问题的产生是为了实现通用检查点功能使其他平台如HuggingFace Trainer能够在不需要更改HuggingFace代码的情况下进行操作。,https://github.com/deepspeedai/DeepSpeed/issues/5526
DeepSpeed,这是一个需求报告，涉及DeepSpeed下的DistributedAttention模块的兼容性问题，导致无法兼容Flash Attention等常用场景需求。,https://github.com/deepspeedai/DeepSpeed/issues/5525
DeepSpeed,这是一个用户提交需求的类型，该问题涉及的主要对象是DeepSpeed的manifes文件更新，由于当前的manifest文件未包含csrc中的hpp文件，导致用户需求更新manifest文件来涵盖hpp文件。,https://github.com/deepspeedai/DeepSpeed/issues/5522
DeepSpeed,这个issue类型是用户提出需求，主要对象是DeepSpeed项目中的依赖包pynvml，由于pynvml已经停止更新一年，用户建议将requirements.txt中的pynvml替换为nvidia-ml-py。,https://github.com/deepspeedai/DeepSpeed/issues/5517
DeepSpeed,这是一个特性需求的issue，主要涉及DeepSpeed的启动机制问题，请求添加一个可通过SSH绕过的启动器模式。,https://github.com/deepspeedai/DeepSpeed/issues/5510
DeepSpeed,这是一个用户提出需求的 issue，主要对象是优化器的优化功能，用户希望能够同时启用 CPU 和 NVMe 进行优化。由于当前的实现方式导致无法同时将优化器分配到 GPU、CPU 和 NVMe，用户希望优化器状态可以根据指定比例分配到不同设备。,https://github.com/deepspeedai/DeepSpeed/issues/5508
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeedCheckpoint功能中支持自定义最后ln idx的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5506
DeepSpeed,这是一个代码优化类型的issue，涉及主要对象是DeepSpeed库中的推理功能。,https://github.com/deepspeedai/DeepSpeed/issues/5505
DeepSpeed,这是一个功能增强（enhancement）类型的issue，涉及DeepSpeed中与梯度计算和梯度裁剪相关的优化操作，主要对象是对梯度计算和裁剪功能的优化。,https://github.com/deepspeedai/DeepSpeed/issues/5504
DeepSpeed,这是一个用户提交需求的issue，主要涉及DeepSpeed中的phi3 mini autotp功能。,https://github.com/deepspeedai/DeepSpeed/issues/5501
DeepSpeed,这是一个提出需求类型的issue，主要涉及DeepSpeed下的Fused adam for HPU功能。由于需要对HPU进行优化，用户提出了关于Fused adam操作的需求。,https://github.com/deepspeedai/DeepSpeed/issues/5500
DeepSpeed,这是一个关于用户提出需求的问题，涉及DeepSpeed库中全局步数保存功能的建议。用户提出是否有禁用保存全局步数的参数的可能性，以减少时间和内存消耗。,https://github.com/deepspeedai/DeepSpeed/issues/5499
DeepSpeed,这个issue是关于用户需求提出的，希望DeepSpeed添加关于如何在ZeRO-3上快速运行`transformers`模型推断的文档。根源是在DPO训练中的一个错误，导致梯度累积步骤大于1时出现了一个加密性的AssertionError。,https://github.com/deepspeedai/DeepSpeed/issues/5498
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed官方网站上的内容窗口位置问题。,https://github.com/deepspeedai/DeepSpeed/issues/5494
DeepSpeed,这个issue类型是更新维护，主要对象是ROCm6，可能是由于DeepSpeed需要适配最新的ROCm6版本而提出的。,https://github.com/deepspeedai/DeepSpeed/issues/5491
DeepSpeed,这是一个关于优化和重构代码的Issue，主要涉及DeepSpeed框架中的Inference OPS，其中涉及到对InferenceBuilder的替换和改进。,https://github.com/deepspeedai/DeepSpeed/issues/5490
DeepSpeed,这是一个用户提出需求的问题，主要涉及如何在Deepspeed中仅微调特定子网络参数。这个问题可能是由于Deepspeed中的优化器与Pytorch中的不同导致LoRA和非LoRA权重在训练期间均未发生变化。,https://github.com/deepspeedai/DeepSpeed/issues/5487
DeepSpeed,这是用户提出的请求(issue)，涉及到如何在Deepspeed中仅微调网络参数的子集，由于Deepspeed中的优化器与Pytorch中的不同，导致LoRA和非LoRA的权重在训练过程中都没有改变。,https://github.com/deepspeedai/DeepSpeed/issues/5486
DeepSpeed,这是关于修复功能的PR，修复了FPQuantizer模块中量化权重数据与原始检查点权重形状不兼容的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5483
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中的Universal checkpoint功能，由于ZeRO Stage 3不支持Pipeline parallelism，因此导致了需要更新相关代码以支持转换Zero checkpoints为Universal checkpoints和加载Universal checkpoints的需求。,https://github.com/deepspeedai/DeepSpeed/issues/5475
DeepSpeed,该issue是一个功能需求报告，主要涉及DeepSpeed下的CompressedBackend和onebit optimizers的添加。由于不同加速器设备对于`compressed_allreduce`的实现差异在于`packbits`和`unpackbits`，因此需要提供通用的CompressedBackend来支持各种加速器设备。,https://github.com/deepspeedai/DeepSpeed/issues/5473
DeepSpeed,这是一个功能请求，主要涉及DeepSpeed构建中的路径问题，由于缺少正确的Python头文件路径导致构建失败。,https://github.com/deepspeedai/DeepSpeed/issues/5471
DeepSpeed,这是一个新功能提议，涉及DeepSpeed中的监控选项和CometML集成。,https://github.com/deepspeedai/DeepSpeed/issues/5466
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed多GPU优化文件如何合并为一个PyTorch优化文件。用户想要了解如何将DeepSpeed多GPU优化文件合并为一个PyTorch优化文件。,https://github.com/deepspeedai/DeepSpeed/issues/5460
DeepSpeed,这是一个更新版本信息的issue，属于维护类问题，主要对象是DeepSpeed软件的版本信息。,https://github.com/deepspeedai/DeepSpeed/issues/5458
DeepSpeed,这是一个用户提出需求的issue，主要涉及如何为构建CPU操作指定CPU架构，用户希望能够指定CPU架构而不使用`march=native`。,https://github.com/deepspeedai/DeepSpeed/issues/5451
DeepSpeed,这个issue是一个用户提出的需求。用户希望在导入deepspeed时能够去除引起恐慌的警告信息。这个问题主要涉及到deepspeed的导入过程中出现的警告信息。,https://github.com/deepspeedai/DeepSpeed/issues/5448
DeepSpeed,该问题属于功能需求提出类型，主要对象是DeepSpeed中的phi2模型，用户希望实现phi2模型的自动投票。,https://github.com/deepspeedai/DeepSpeed/issues/5436
DeepSpeed,"这是一个关于需求咨询的issue，主要涉及DeepSpeed Ulysses对长序列长度的支持。用户质疑在本地attention计算中，Query仍然具有形状`[N, d/P]`，是否会导致内存消耗仍然很大。",https://github.com/deepspeedai/DeepSpeed/issues/5435
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的模型autotp和conv tp的使用。由于模型中的qk共享导致了期望值与实际值不符，需要调整数值和oproj权重以适应这种qk类型。,https://github.com/deepspeedai/DeepSpeed/issues/5428
DeepSpeed,这是一个关于添加更多测试到XPU CI的需求类型的问题单，主要对象是测试环境。由于合并了使用non_daemonic_proc作为XPU设备默认值的更改，使得可以在CI中添加更多测试。,https://github.com/deepspeedai/DeepSpeed/issues/5427
DeepSpeed,这是一个功能需求类型的issue，主要涉及到DeepSpeed项目的XPU支持和op builder的集成。,https://github.com/deepspeedai/DeepSpeed/issues/5425
DeepSpeed,这是一个功能增强提议，主要涉及到DeepSpeed中的溢出检查的统一性问题。,https://github.com/deepspeedai/DeepSpeed/issues/5424
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的加速器（accelerator）。,https://github.com/deepspeedai/DeepSpeed/issues/5423
DeepSpeed,这是一个优化需求，主要涉及DeepSpeed中的zero3 fetch 参数功能。由于使用all_reduce代替all_gather，减少了拼接和切片的开销，从而提高了性能。,https://github.com/deepspeedai/DeepSpeed/issues/5420
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed的engine.py文件。,https://github.com/deepspeedai/DeepSpeed/issues/5414
DeepSpeed,这个issue属于更新型issue，主要对象是DeepSpeed版本信息文本文件。原因是发布了0.14.1版本所致。,https://github.com/deepspeedai/DeepSpeed/issues/5413
DeepSpeed,这个issue是一个功能需求提议，主要涉及的对象是DeepSpeed中的XPU设备。由于默认设置导致需要在XPU设备上手动设置non_daemonic_proc参数为True，用户提议将其默认设置为True以简化操作。,https://github.com/deepspeedai/DeepSpeed/issues/5412
DeepSpeed,这个issue属于功能需求，主要涉及CPUAdam对BF16和FP16的支持。原因是开发者添加了对BF16的支持，根据输入参数的数据类型在运行时选择正确的模板。,https://github.com/deepspeedai/DeepSpeed/issues/5409
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed，由于用户希望支持在finetune extra large model中使用lora和pipeline，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/5404
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中加速器（accelerator）的设备配置环境（device config env）的问题，用户希望能够为加速器实现环境设置以适应不同设备的需求。,https://github.com/deepspeedai/DeepSpeed/issues/5396
DeepSpeed,这是一个功能增强的issue，主要涉及DeepSpeed中的CPU推断性能优化。,https://github.com/deepspeedai/DeepSpeed/issues/5391
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中通用检查点的自定义重塑功能，用户提出了对权重张量重塑的需求以支持更复杂的使用情况。,https://github.com/deepspeedai/DeepSpeed/issues/5390
DeepSpeed,这个issue是关于更新GitHub Actions中的checkout action的问题，涉及的主要对象是运行在Ubuntu 18.04或更早版本的工作流程。这个问题是由于需要使用Node.js 20+版本的GLIBC导致的。,https://github.com/deepspeedai/DeepSpeed/issues/5387
DeepSpeed,这个issue属于用户提出需求类型，涉及的主要对象是DeepSpeed项目中的CI集成，用户希望为Intel XPU/Max1100添加持续集成。,https://github.com/deepspeedai/DeepSpeed/issues/5385
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中的pipeline stages分配功能。由于重启训练时会耗费大量时间，用户希望能将pipeline stages的分配结果缓存到磁盘上，或者通过传递列表方式告知DeepSpeed如何分配pipeline stages。,https://github.com/deepspeedai/DeepSpeed/issues/5379
DeepSpeed,这个issue属于一个需求类型，涉及的主要对象是添加Intel XPU/Max1100的CI。由于缺少相应的测试工作流程，导致了新增测试的延迟。,https://github.com/deepspeedai/DeepSpeed/issues/5376
DeepSpeed,这是一个功能新增的issue，主要涉及Selective dequantization功能。,https://github.com/deepspeedai/DeepSpeed/issues/5375
DeepSpeed,这是一个优化性能的PR，主要关注DeepSpeed中的fp-dequantizer，而不是一个bug报告。,https://github.com/deepspeedai/DeepSpeed/issues/5373
DeepSpeed,这是一个用户提出需求的issue，主要关注的对象是DeepSpeed中的推理模块（deepspeed-inference），用户想了解是否有计划支持Flash Decoding或类似功能。,https://github.com/deepspeedai/DeepSpeed/issues/5371
DeepSpeed,这个issue类型是重构请求，主要涉及DeepSpeed项目中的代码清理工作，旨在移动和更新特定函数的引用和检查，以提高代码的整洁性和可维护性。,https://github.com/deepspeedai/DeepSpeed/issues/5370
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed的性能优化，其中通过配置可以调整吞吐量计时器的行为。,https://github.com/deepspeedai/DeepSpeed/issues/5363
DeepSpeed,这个issue是涉及更新工作流程中使用cu116到cu117的问题，主要对象是DeepSpeed的相关工作流程。这个问题由于使用了已停用的cu116而导致了症状bug产生，需要相应的更新来解决。,https://github.com/deepspeedai/DeepSpeed/issues/5361
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed中的Gaudi2容器版本更新。这个问题的原因是需要将容器版本从1.14.0升级到1.15.0，因此用户请求更新构建以获取最新版本。,https://github.com/deepspeedai/DeepSpeed/issues/5360
DeepSpeed,这是一个需要增加Gaudi2加速器的CI覆盖率的需求，主要对象是HPU加速器。由于之前的CI覆盖率不足，可能导致HPU加速器在使用过程中出现问题，需要增加测试覆盖率以确保其稳定性。,https://github.com/deepspeedai/DeepSpeed/issues/5358
DeepSpeed,"这个issue类型是用户提出需求，涉及的主要对象是DeepSpeed下的OptimizedLinear实现，用户请求了一个优化版本的`nn.Linear`，具有LoRA和FP [6,8,12] quantization功能。",https://github.com/deepspeedai/DeepSpeed/issues/5355
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed-Ulysses和Pure Deepspeed Zero之间的集成问题。由于缺乏基于Deepspeed Zero3的示例或教程，用户需要指导如何使用`get_sequence_parallel_group()`函数和`DistAttn`进行适当的设置。,https://github.com/deepspeedai/DeepSpeed/issues/5348
DeepSpeed,这是一个功能增强的issue，主要涉及DeepSpeed对于pipeline模型支持MoE功能的改进。,https://github.com/deepspeedai/DeepSpeed/issues/5338
DeepSpeed,这是一个新功能需求类型的issue，主要涉及Flexiblebit quantizer/dequantizer库的支持fp6/fp12/fp8，并需要Ampere+架构，由于op最初只专注于`bfloat16`输入类型，由此产生。,https://github.com/deepspeedai/DeepSpeed/issues/5336
DeepSpeed,这是一个关于更新DeepSpeed以使用ROCm 5.7的需求报告，涉及主要对象为DeepSpeed库。由于ROCm版本升级，需要更新DeepSpeed以适配新的ROCm 5.7版本。,https://github.com/deepspeedai/DeepSpeed/issues/5330
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是深度学习模型Mixtral 8X7B。由于当前模型只支持autoTP，用户希望增加支持以使用更多并行化（如EP、DP），导致用户提出了如何增加支持的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5327
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的HPU（High Processing Unit）。原因可能是用户希望增加HPU的功能覆盖范围。,https://github.com/deepspeedai/DeepSpeed/issues/5324
DeepSpeed,这是一个改进性能的issue，涉及DeepSpeed中CPU SHM based inference_all_reduce的优化。unnable to execute code in %s.,https://github.com/deepspeedai/DeepSpeed/issues/5320
DeepSpeed,该issue类型是用户提出需求，请教问题，主要对象是在Hugging Face Transformers中如何集成DeepSpeed-Ulysess，用户想要了解如何将DeepSpeed-Ulysess集成到Transformers中。,https://github.com/deepspeedai/DeepSpeed/issues/5319
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed与Galore方法在使用中存在的兼容性问题。,https://github.com/deepspeedai/DeepSpeed/issues/5309
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed，由于需要真实的cuda设备才能构建evoformer导致用户无法构建预编译的wheels。,https://github.com/deepspeedai/DeepSpeed/issues/5308
DeepSpeed,这是一个功能增强的issue，主要针对了正则表达式在支持Unicode字符和简化代码方面进行了修改。,https://github.com/deepspeedai/DeepSpeed/issues/5306
DeepSpeed,这是用户提出需求类型的issue，主要涉及 LoRA finetune 的内存需求估算问题，由于计算过程不考虑 peft_model with adapters 的情况，导致无法准确估算。,https://github.com/deepspeedai/DeepSpeed/issues/5304
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed项目的README文件。该需求是希望在README中添加Intel Gaudi作为贡献的硬件信息。,https://github.com/deepspeedai/DeepSpeed/issues/5300
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed库中的compile_backend。,https://github.com/deepspeedai/DeepSpeed/issues/5299
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed框架，用户希望其支持自定义后端加速器。,https://github.com/deepspeedai/DeepSpeed/issues/5298
DeepSpeed,这是一个用户提出的功能需求，主要涉及DeepSpeed中的`DistributedDataAnalyzer`，用户希望为`map`操作添加多CPU处理，以提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/5291
DeepSpeed,该issue类型为需求提议，主要涉及到在DeepSpeed项目的README中添加Habana Gaudi2的CI标记。原因是需要为Habana Labs Gaudi2测试运行启用一些单元测试。,https://github.com/deepspeedai/DeepSpeed/issues/5286
DeepSpeed,这是一个关于需求的问题，主要涉及的对象是 DeepSpeed 对 Ada-Lovelace 架构的支持。由于 DeepSpeed 在 Ada-Lovelace 架构的 GPU 上会偶尔产生错误输出，导致需要探讨是否支持 Ada-Lovelace 架构的计划。,https://github.com/deepspeedai/DeepSpeed/issues/5284
DeepSpeed,这个issue是用户提出的需求类型，主要对象是DeepSpeed中的inference.v2，用户想要知道是否有计划支持PagedAttention，并提出了支持PagedAttention的解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/5282
DeepSpeed,这个issue类型是用户提出需求，主要涉及DeepSpeed在fine-tuning大型语言模型上的性能和效率问题。原因是当前offload整个优化器使用32位CPU Adam导致内存使用效率低，用户希望通过改进并增加支持来提高训练速度和降低成本。,https://github.com/deepspeedai/DeepSpeed/issues/5276
DeepSpeed,"这是一个类型为""新增功能""的issue，主要对象是Gaudi2 CI测试。由于需要增加Gaudi2 CI测试，用户提出了这个issue。",https://github.com/deepspeedai/DeepSpeed/issues/5275
DeepSpeed,"这是一个用户提出需求的issue，该问题涉及的主要对象是准备为HPU启用单元测试。
这个问题可能是由于缺乏针对HPU的单元测试导致的，用户希望为HPU准备好单元测试。",https://github.com/deepspeedai/DeepSpeed/issues/5272
DeepSpeed,这个issue属于功能需求的提出，主要对象是HPU accelerator，由于该硬件不支持1bit压缩和sparsegrad测试，用户提出需要跳过这些测试。,https://github.com/deepspeedai/DeepSpeed/issues/5270
DeepSpeed,该issue为功能新增类型，主要涉及DeepSpeed中的torch.compile测试，添加了对HPU backend的支持。原因是为了在torch.compile中使用HPU backend提供更多的支持。,https://github.com/deepspeedai/DeepSpeed/issues/5269
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed FastGen不支持tokens作为输入，可能导致无法使用某些模型的特定功能。,https://github.com/deepspeedai/DeepSpeed/issues/5261
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed库中的`deepspeed.initialize`方法缺乏`distributed_port`参数。,https://github.com/deepspeedai/DeepSpeed/issues/5260
DeepSpeed,这是一个优化建议类型的issue，主要涉及DeepSpeed中的PipelineModule初始化参数计数过程。由于在计算参数数量时禁用torch.nn.init函数调用，可以加快参数划分过程，提高管道并行性能。,https://github.com/deepspeedai/DeepSpeed/issues/5258
DeepSpeed,这是一个功能需求提议，主要涉及的对象是DeepSpeed中的mixtral 8x7b自动优化功能。,https://github.com/deepspeedai/DeepSpeed/issues/5257
DeepSpeed,这是一个优化性能的issue，主要涉及对象为DeepSpeed中的scaled_global_grad_norm计算。,https://github.com/deepspeedai/DeepSpeed/issues/5256
DeepSpeed,这是一个性能优化相关的issue，主要涉及DeepSpeed中的PipelineModule，提出禁用在初始化PipelineModule过程中计算参数时的torch.nn.init函数调用，以加快参数分区速度。,https://github.com/deepspeedai/DeepSpeed/issues/5254
DeepSpeed,这是一个用户提出需求的Issue，主要涉及DeepSpeed Zero中配置自定义的数据迁移机制，由于目前只支持NVMe SSDs，用户希望能够根据特定需求配置数据迁移机制。,https://github.com/deepspeedai/DeepSpeed/issues/5246
DeepSpeed,这是一个功能需求，涉及Habana Labs HPU/Gaudi2的CI测试。问题是由于操作尚未实现，导致无法启用完整的单元测试。,https://github.com/deepspeedai/DeepSpeed/issues/5244
DeepSpeed,这个issue是用户提出需求的类型，主要涉及DeepSpeed支持的硬件验证和说明，用户想要了解他们的硬件是否被DeepSpeed支持以及如何验证硬件。,https://github.com/deepspeedai/DeepSpeed/issues/5240
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及的对象是有关DeepSpeed中关于Fp16的博客。由于未提供具体内容，无法分析具体原因导致了何种症状的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5239
DeepSpeed,这是一个更新版本信息的issue，类型为功能需求，涉及的主要对象是DeepSpeed库，由于需要将版本信息更新为0.14.0版本而创建。,https://github.com/deepspeedai/DeepSpeed/issues/5238
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的变量批处理大小和学习率调度器。因为在多种情况下，特别是在处理LLMs时，需要处理变长输入（句子），而当前系统不支持动态批处理大小（dynamic batch size）和相应的学习率调整，用户希望系统能够支持这一特性。,https://github.com/deepspeedai/DeepSpeed/issues/5237
DeepSpeed,"这是一个用户提出需求的issue，主要涉及关于DeepSpeed中的""LOMO Low Memory Optimization""的实现请求。BUG或问题的症状是缺乏这种方法会导致内存占用过大。",https://github.com/deepspeedai/DeepSpeed/issues/5236
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的FP6 quantization功能。由于用户希望在该功能中实现end-to-end的量化，因此提出了该问题。,https://github.com/deepspeedai/DeepSpeed/issues/5234
DeepSpeed,这是一个更新版本信息的Issue，主要对象是DeepSpeed软件包。,https://github.com/deepspeedai/DeepSpeed/issues/5229
DeepSpeed,这是关于修改工作流程命名的需求，不是bug报告。主要对象是DeepSpeed项目中的一个工作流程。这个问题是由于命名不当导致混淆，用户希望将工作流程命名更改为更准确的名称。,https://github.com/deepspeedai/DeepSpeed/issues/5226
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在使用`deepspeed.comm`替代`torch.distributed`的问题。由于某种原因，用户希望使用`deepspeed.comm`来替代`torch.distributed`。,https://github.com/deepspeedai/DeepSpeed/issues/5225
DeepSpeed,这是一个功能需求提议，主要对象是DeepSpeed中的reduce_scatter_coalesced操作，旨在节省维护扁平缓冲区时所需的时间和内存开销。,https://github.com/deepspeedai/DeepSpeed/issues/5224
DeepSpeed,该issue属于用户提出需求类型，主要对象是MII CI。由于缺乏指定MII分支的功能，用户需要在MII CI中能够指定MII分支。,https://github.com/deepspeedai/DeepSpeed/issues/5208
DeepSpeed,这个issue是一个功能需求，主要涉及DeepSpeed中操作构建器检测适应加速器变化的问题，原因是在DeepSpeed启动时如果加速器发生变化，会导致`installed_ops`和`compatible_ops`列表信息不正确。,https://github.com/deepspeedai/DeepSpeed/issues/5206
DeepSpeed,这个issue类型是自动更新版本信息的操作，不是bug报告，主要涉及的对象是DeepSpeed软件版本信息。,https://github.com/deepspeedai/DeepSpeed/issues/5196
DeepSpeed,这个issue类型为功能需求，涉及DeepSpeed中的编译器后端，用户需要启用调试或实验性后端，并通过这个issue来实现这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/5191
DeepSpeed,这是一个用户提出需求的类型问题，该问题单涉及的主要对象是DeepSpeed项目下的脚本。由于缺少对 `--extra-index-url` 的检查脚本，用户提出需求添加这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/5184
DeepSpeed,该issue是关于更改CPU推理工作流程中的软件包下载源的建议，主要涉及的对象是软件包下载与版本选择机制。这个问题是由于默认下载源不是预期源以及可能选择了错误版本导致的。,https://github.com/deepspeedai/DeepSpeed/issues/5182
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的优化技术，用户希望添加参数分片、梯度分片和优化器状态分片等功能，并解决与不同通信瓶颈相关的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5178
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed如何处理硬件变化导致的加速器选择问题。,https://github.com/deepspeedai/DeepSpeed/issues/5173
DeepSpeed,这是一个功能需求类型的issue，主要涉及的对象是DeepSpeed中的一个叫做`SerialDataAnalyzer`的类。由于需要实现一个新的功能来使用单个节点获取即时课程参数，这个issue被提出。,https://github.com/deepspeedai/DeepSpeed/issues/5168
DeepSpeed,这是一个需求提报类型的 issue，主要对象是将 DeepSpeed 更新至 Pydantic v2，由于 DeepSpeed 一直使用 Pydantic v1 API 导致此问题。,https://github.com/deepspeedai/DeepSpeed/issues/5167
DeepSpeed,这个issue属于功能需求类型，主要涉及DeepSpeed中HPU加速器在单元测试中的支持。此问题涉及在单元测试中启用HPU支持。,https://github.com/deepspeedai/DeepSpeed/issues/5162
DeepSpeed,这个issue属于功能增强类型，主要对象是DeepSpeed中的Inference V2测试。由于硬件配置缺乏对Inference V2的支持，导致需要跳过相关测试，以提高测试套件的可靠性。,https://github.com/deepspeedai/DeepSpeed/issues/5161
DeepSpeed,这是一个需求增强类型的issue，主要涉及的对象是DeepSpeed中的测试功能。由于硬件配置不支持FusedAdam和FusedLamb优化器，导致了测试中相关功能需要被跳过以提高测试的可靠性和稳定性。,https://github.com/deepspeedai/DeepSpeed/issues/5159
DeepSpeed,这是一个功能需求提出的issue，主要涉及DeepSpeed中MoE模型对topk参数取值的限制问题；由于DeepSpeed中对topk参数的限制仅支持1或2，导致用户无法满足实验需求中topk=6的要求。,https://github.com/deepspeedai/DeepSpeed/issues/5155
DeepSpeed,这个issue类型属于功能需求提议，主要对象是DeepSpeed中的加速器（accelerators），提议是为了优化optimizer配置过程中的流程，解决了需要在不同加速器中检查特定配置情况的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5149
DeepSpeed,这个issue是更新和重命名至torch 1.11，涉及DeepSpeed项目中的测试版本更新问题。因为torch 1.10版本较旧，此问题可能是为了保持项目与最新torch版本的兼容性。,https://github.com/deepspeedai/DeepSpeed/issues/5141
DeepSpeed,这是一个用户提出需求的类型，问题涉及的主要对象是DeepSpeed中的TestEmptyParameterGroup。由于要避免使用与cuda相关的optimizer，用户希望替换fusedAdam为torch.optim.AdamW这一一般性optimizer。,https://github.com/deepspeedai/DeepSpeed/issues/5139
DeepSpeed,这是一个性能优化建议。该问题类型属于用户提出需求类型， 主要涉及了在tensorboard logging中使用.item()方法引起的性能问题。,https://github.com/deepspeedai/DeepSpeed/issues/5135
DeepSpeed,这是一个关于提出需求的issue，主要对象是在DeepSpeed下添加了一个名为DistributedDataAnlyzer的类来实现在分布式内存上的map-reduce操作。原因是为了解决在DataAnalyzer中产生大量临时文件导致写入和加载速度慢、内存占用高的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5129
DeepSpeed,这是一个功能需求类型的GitHub issue，主要涉及DeepSpeed中计算全局范数的操作，提议避免通过主机同步而保持数据在设备上。,https://github.com/deepspeedai/DeepSpeed/issues/5125
DeepSpeed,这个issue属于用户提出需求类型，主要涉及到DeepSpeed中的模型权重和Adam优化器的手动转移操作。用户想在RLHF推断阶段将模型权重和Adam优化器从GPU移动到CPU以提高效率，然后在阶段结束后将它们重新加载到GPU。,https://github.com/deepspeedai/DeepSpeed/issues/5123
DeepSpeed,这个issue类型是功能需求，涉及主要对象是DeepSpeed下的`HIP_Accelerator` accelerator。由于需要扩展支持AMD，需要添加新的HIP加速器抽象来提供更大的灵活性，并允许加速器功能覆盖。,https://github.com/deepspeedai/DeepSpeed/issues/5120
DeepSpeed,这个issue类型为版本更新，主要涉及的对象是DeepSpeed软件的版本管理，问题是由于发布了版本0.13.2，需要更新version.txt文件。,https://github.com/deepspeedai/DeepSpeed/issues/5119
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO功能，问题是GBS在使用大量GPU时变得过大，造成训练效率下降。,https://github.com/deepspeedai/DeepSpeed/issues/5114
DeepSpeed,这是一个需求类型的 issue，主要涉及到 DeepSpeed 项目中的文件管理和版本控制。,https://github.com/deepspeedai/DeepSpeed/issues/5111
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的nv-inference功能。由于没有使用`-x`参数在pytest中执行，用户希望更新以解决相关问题。,https://github.com/deepspeedai/DeepSpeed/issues/5103
DeepSpeed,这是一个性能优化的issue，主要涉及DeepSpeed中FastGen allocator的重构和性能提升。,https://github.com/deepspeedai/DeepSpeed/issues/5090
DeepSpeed,这是一个建议性质的issue，主要涉及的对象是DeepSpeed的构建过程。原因是之前禁用了`ninja`，导致构建过程时间较长，建议启用并解决相关路径问题。,https://github.com/deepspeedai/DeepSpeed/issues/5088
DeepSpeed,这是一个更新torch版本的issue，属于技术调整类型，主要涉及DeepSpeed框架下的测试环境。,https://github.com/deepspeedai/DeepSpeed/issues/5086
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed中的CPU加速器，提出需要创建一个新的通用x86 CPU加速器以满足特定需求。,https://github.com/deepspeedai/DeepSpeed/issues/5084
DeepSpeed,这个issue是需要更新导入以适应最新diffusers变更的，属于代码调整类型，主要对象是代码库DeepSpeed中的diffusers模块。原因可能是之前的导入代码已经不再适用于最新的diffusers变更。,https://github.com/deepspeedai/DeepSpeed/issues/5065
DeepSpeed,这是一个用户提出的需求类型的issue，主要对象涉及DeepSpeed中的MoE模块，用户希望能够通过指定expert_group_name或者expert number来分配参数组，以解决参数混合、跟踪和优化相关的问题。,https://github.com/deepspeedai/DeepSpeed/issues/5064
DeepSpeed,这是一个功能改进类型的issue，主要涉及DeepSpeed中的`deepspeed.moe.utils`和`deepspeed.moe.layer`模块，提出了关于使用`dict`和`defaultdict`的优化建议，以及增加类型提示和小型风格更改。,https://github.com/deepspeedai/DeepSpeed/issues/5060
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed的zero2模块和adapter fusion模型的参数权重保存问题。根据用户提出的问题，可能是因为希望在保存checkpoint时控制不保存适配器融合模型的整体参数权重。,https://github.com/deepspeedai/DeepSpeed/issues/5058
DeepSpeed,这是一个需求类的issue，涉及主要对象为DeepSpeed中的NPU加速器，用户希望将日志级别改为debug以与其他加速器保持一致。,https://github.com/deepspeedai/DeepSpeed/issues/5051
DeepSpeed,这个issue是文档更新类型，主要对象是DeepSpeed中关于`mp_size`字段的文档。产生这个问题的原因是`mp_size`字段已经被弃用，需要更新相关文档。,https://github.com/deepspeedai/DeepSpeed/issues/5048
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed项目的landing page。由于MI100 badge不应显示在landing page上，用户请求将其移除。,https://github.com/deepspeedai/DeepSpeed/issues/5036
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed的init_distributed函数，用户提出需要在该函数描述中添加HCCL的信息。,https://github.com/deepspeedai/DeepSpeed/issues/5034
DeepSpeed,这个issue类型是用户提出需求，询问如何使用DeepSpeed MoE API在分布式训练环境中有效迁移预训练模型参数到新的MoE结构模型。,https://github.com/deepspeedai/DeepSpeed/issues/5032
DeepSpeed,这是一个需要更新文档的issue，单涉及的主要对象是推理页面，可能是为了指向FastGen服务。,https://github.com/deepspeedai/DeepSpeed/issues/5029
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed Fast-Gen Multi-LoRA功能的改进。用户希望在使用LoRA适配器训练完成后，能够在Deepspeed FastGen中有效地启用、切换或禁用多个LoRA适配器，希望得到关于如何实现这一功能的指引和方向。,https://github.com/deepspeedai/DeepSpeed/issues/5027
DeepSpeed,这是一个特性需求的issue，主要涉及到DeepSpeed中is_synchronized_device()接口的拆分问题。由于HPU设备的特性与CUDA流的行为不一致，导致需要拆分该接口为多个API。,https://github.com/deepspeedai/DeepSpeed/issues/5026
DeepSpeed,这是一个用户提出需求的issue，主要对象是Corp for menage binancians，由于缺乏清晰的描述，导致用户无法准确表达问题或需求。,https://github.com/deepspeedai/DeepSpeed/issues/5020
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed的workflow配置，提出了希望在所有workflow上启用`workflow_dispatch`功能的需求，使得能够在不创建PR或合并到主分支的情况下，在不同分支上运行CI/scheduled builds并查看结果。,https://github.com/deepspeedai/DeepSpeed/issues/5016
DeepSpeed,这是一个用户提出需求的类型，主要对象是允许每晚触发执行测试。由于缺乏夜间测试，用户正在寻找一种途径来提高测试频率。,https://github.com/deepspeedai/DeepSpeed/issues/5014
DeepSpeed,这个issue是关于功能需求的，主要对象是HPU加速器，用户希望实现针对HPU加速器的图捕获和重放API。,https://github.com/deepspeedai/DeepSpeed/issues/5013
DeepSpeed,这个issue类型是代码质量和可读性改进，主要对象是代码库中的一小部分代码。由于代码质量和可读性较低，需要进行改进。,https://github.com/deepspeedai/DeepSpeed/issues/5011
DeepSpeed,这是一个需求提出的issue，主要涉及DeepSpeed对于`torch.compile`的支持问题，由于ZeRO3支持、Pipeline parallel支持的相关任务完成，未解决的问题包括Activation checkpointing和HuggingFace CI测试。,https://github.com/deepspeedai/DeepSpeed/issues/5009
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中启动多进程时日志不详细的问题，用户希望添加日志以反映每个进程的活动。,https://github.com/deepspeedai/DeepSpeed/issues/5004
DeepSpeed,这个issue是一个版本更新类型的请求，涉及的主要对象是DeepSpeed软件的版本号文件。,https://github.com/deepspeedai/DeepSpeed/issues/5002
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中`save_16bit_model`方法缺乏`exclude_frozen_parameters`参数的支持。,https://github.com/deepspeedai/DeepSpeed/issues/4999
DeepSpeed,这是一个性能优化类型的issue，主要涉及DeepSpeed中通信模块中的`comm.py`文件。由于当前采用`subprocess`方式在获取主机名时性能较低，导致了潜在的性能瓶颈，因此建议替换为更高效的`socket`方式来提升性能。,https://github.com/deepspeedai/DeepSpeed/issues/4991
DeepSpeed,这是一个用户提交需求的issue，主要涉及更新FastGen博客标题，可能由于现有标题不准确或不合适而导致用户请求修改。,https://github.com/deepspeedai/DeepSpeed/issues/4983
DeepSpeed,该issue类型为更新问题（Update issue），主要涉及的对象是DeepSpeed的版本控制（Version control），由于需要在DeepSpeed发布0.13.0版本后自动更新version.txt文件，所以提出该问题。,https://github.com/deepspeedai/DeepSpeed/issues/4982
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是 FastGen 模块。由于日期错误导致无内容，用户希望在博客中能够正确显示2024年1月的内容。,https://github.com/deepspeedai/DeepSpeed/issues/4980
DeepSpeed,这是一个用户提出需求的issue，该问题单涉及的主要对象是`zero_to_fp32.py`脚本。由于用户需要仅合并可训练的参数，提出了需要新增一个参数以支持此功能。,https://github.com/deepspeedai/DeepSpeed/issues/4979
DeepSpeed,这是一个用户需求类型的issue，涉及DeepSpeed中BF16 optimizer的实现改进。由于默认情况下未开启一个新的配置选项，用户想要改进设备利用率并并行化工作负载。,https://github.com/deepspeedai/DeepSpeed/issues/4975
DeepSpeed,这个issue属于性能优化类型，主要涉及到深度学习框架DeepSpeed中梯度范数计算的优化问题，由于在计算过程中存在设备和主机之间的数据转换，希望通过使用PyTorch原生操作在设备上完成所有计算以提高效率。,https://github.com/deepspeedai/DeepSpeed/issues/4974
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeedFastGen的query APIs。该问题的提出是为了改进查询效率并提供跳过调度检查的选项。,https://github.com/deepspeedai/DeepSpeed/issues/4965
DeepSpeed,这个issue是关于需求的，需要添加缺失的op_builder.hpu组件以支持HPU加速器。,https://github.com/deepspeedai/DeepSpeed/issues/4963
DeepSpeed,这是一个用户提出需求的issue，主要涉及自动张量并行性的工作流程，用户希望验证LLM模型的AutoTP功能。,https://github.com/deepspeedai/DeepSpeed/issues/4961
DeepSpeed,这是一个文档更新类型的issue，涉及主要对象是DeepSpeed中的AutoTP支持的模型列表。由于新增了一些模型，需要更新文档中的相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/4960
DeepSpeed,这个issue属于代码优化类型，涉及的主要对象是Qwen positional embedding配置代码。这个问题产生的原因是为了优化Qwen推断代码中的位置嵌入配置代码。,https://github.com/deepspeedai/DeepSpeed/issues/4955
DeepSpeed,这是一个关于优化计算KV缓存需求的提议，不是bug报告。这个提议涉及到DeepSpeed中的KV缓存需求。原因是为了重用已计算的KV缓存需求，以提高频繁检查调度可行性的效率。,https://github.com/deepspeedai/DeepSpeed/issues/4952
DeepSpeed,这是一个功能新增类型的issue，涉及的主要对象是DeepSpeed中的Aim监控器。由于需要增强实验监控功能，使其更透明和用户友好。,https://github.com/deepspeedai/DeepSpeed/issues/4947
DeepSpeed,这是一个优化类的issue，主要涉及DeepSpeed库中ragged batching的预处理效率问题。,https://github.com/deepspeedai/DeepSpeed/issues/4942
DeepSpeed,这个issue是一个需求提出类型，主要涉及的对象是改进DeepSpeed在推理测试中模型列表共享的机制，由于需要在CI运行器之间共享信息，所以需要将模型列表缓存在blob存储中。,https://github.com/deepspeedai/DeepSpeed/issues/4940
DeepSpeed,这个issue类型为功能需求，涉及的主要对象是DeepSpeed项目中的MII (Model Inference Interface) CI (Continuous Integration)。原因可能是为了只针对推理（inference）更改运行 MII CI，以提高效率和精确性。,https://github.com/deepspeedai/DeepSpeed/issues/4939
DeepSpeed,这是一个功能需求类型的 issue，主要涉及 DeepSpeed 对 Triton 2.2+ 的支持。由于 Triton 版本更新，用户希望 DeepSpeed 能够适配 Triton 2.2+，提供更好的兼容性和功能支持。,https://github.com/deepspeedai/DeepSpeed/issues/4937
DeepSpeed,这个issue是一个用户提出需求的类型，主要对象涉及DeepSpeed中的nv-a6000 workflow，用户希望能够允许指定mii分支。,https://github.com/deepspeedai/DeepSpeed/issues/4936
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中关于HPU加速器的API实现。由于缺乏相应的API，用户希望能够增加某些HPU加速器相关功能的接口。,https://github.com/deepspeedai/DeepSpeed/issues/4935
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是triton 2.1.0。由于一些问题导致了nv-inference存在一些固有的问题，该issue提出了将DeepSpeed项目固定到triton 2.1.0以解决这些问题。,https://github.com/deepspeedai/DeepSpeed/issues/4929
DeepSpeed,这是一个用户提出需求的类型，主要涉及到 DeepSpeed 库中的 secrets.GITHUB_TOKEN 权限问题。,https://github.com/deepspeedai/DeepSpeed/issues/4927
DeepSpeed,这是一个用户提出需求的 issue，主要涉及 DeepSpeed 的 num_local_io_workers 参数设置问题，可能由于初始化代码无法设置 dataloader 的 num_workers 参数而导致。,https://github.com/deepspeedai/DeepSpeed/issues/4925
DeepSpeed,这是一个需求提出类型的issue，主要涉及到DeepSpeed中运行SD tests时路径的问题，要求在更新requirements.txt或VAE/UNET时能够运行相应的测试。,https://github.com/deepspeedai/DeepSpeed/issues/4919
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed 0.6.0版本。用户想要获取Deepspeed 0.6.0版中完整的梯度，但由于该版本没有safe_full_get_grad函数，所以用户寻求其它方法来获取完整的梯度。,https://github.com/deepspeedai/DeepSpeed/issues/4917
DeepSpeed,该issue类型为用户提出需求，主要对象是DeepSpeed中的代码文档，用户希望添加WarmupCosineLR调度器到文档中。,https://github.com/deepspeedai/DeepSpeed/issues/4916
DeepSpeed,这是一个优化建议类型的issue，主要涉及的对象是DeepSpeed中的clip_grad_norm_函数。由于调用.item()方法导致设备在主机端等待的时间过长，因此提出优化clip_grad_norm_函数的建议。,https://github.com/deepspeedai/DeepSpeed/issues/4915
DeepSpeed,"这个issue是用户提出需求，并且主要涉及DeepSpeed-FastGen的Qwen模型支持。由于用户希望将Qwen模型（7b, 14b, 72b）添加到DeepSpeed-FastGen中，因此提出了这个问题。",https://github.com/deepspeedai/DeepSpeed/issues/4913
DeepSpeed,这个issue是一个需求请求，关于为DeepSpeed添加对Habana Labs HPU加速器的支持。,https://github.com/deepspeedai/DeepSpeed/issues/4912
DeepSpeed,这个issue属于功能改进类型，主要涉及DeepSpeed中的hpz功能，通过深度学习张量次要存在的情况来启用hpz，解决了潜在的数据不同步问题，并且提高了性能。,https://github.com/deepspeedai/DeepSpeed/issues/4906
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的BF16/fp16训练基准。由于缺乏关于bf16/fp16训练基准的文档，用户在benchmarking时遇到了困惑，探讨了在深度速度配置中手动强制转换的可行性。,https://github.com/deepspeedai/DeepSpeed/issues/4904
DeepSpeed,这是一个功能增强的issue，涉及的主要对象是Qwen模型，由于需要增加自动Tensor Parallelism支持，对应的模块匹配和相关变量进行了调整，并在Qwen1_8B和Qwen72Bchat上进行了验证。,https://github.com/deepspeedai/DeepSpeed/issues/4902
DeepSpeed,这是一个需求更新的issue，主要涉及的是DeepSpeed在ROCm 5.7环境下的兼容性问题。,https://github.com/deepspeedai/DeepSpeed/issues/4898
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及到添加Habana Labs HPU加速器支持。原因可能是用户希望DeepSpeed库能够与Habana Labs HPU加速器进行集成，以提升性能。,https://github.com/deepspeedai/DeepSpeed/issues/4897
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中的starcode(kv_head=1) autotp功能的启用。,https://github.com/deepspeedai/DeepSpeed/issues/4896
DeepSpeed,这个issue属于用户提出需求类型，主要涉及到DeepSpeed中的AutoTP workflow。由于Popular models模型检查点过大、需要测试AutoTP的效果和支持新的DeepSpeed模型，导致需要一个可扩展的workflow来满足这些需求。,https://github.com/deepspeedai/DeepSpeed/issues/4894
DeepSpeed,这是一个用户提出需求的问题单，主要涉及对象是在DeepSpeed项目中使用Ascend NPU进行模型训练。用户因为不清楚如何在Ascend NPU上训练模型，向社区寻求示例和帮助。,https://github.com/deepspeedai/DeepSpeed/issues/4892
DeepSpeed,该issue是关于功能需求的，涉及主要对象为DeepSpeed的`torch.compile`功能。由于缺少对`torch.compile`的合适配置，用户提出了对ZeRO stages 1/2/3的支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4878
DeepSpeed,这是一个用户提出需求的issue，主要涉及如何为自定义层添加分片和并行性，由于文档链接引用错误导致用户无法找到相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/4876
DeepSpeed,这个issue类型为功能改进请求，涉及的主要对象是DeepSpeed库中的参数分区功能。原因是为了简化符号表示和提高可读性。,https://github.com/deepspeedai/DeepSpeed/issues/4868
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的zero inference在两台不同计算机上运行时的问题。由于配置不正确，导致分布式推理时GPU利用率较低且性能没有提升。,https://github.com/deepspeedai/DeepSpeed/issues/4860
DeepSpeed,这个issue是关于功能需求的，主要涉及DeepSpeed中Pipeline模块的评估微批大小配置问题，该问题由于当前的代码假设使用相同的微批大小和全局大小，导致评估微批大小不能独立配置，需要修改代码以支持评估微批大小的独立设置。,https://github.com/deepspeedai/DeepSpeed/issues/4859
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及对象是DeepSpeed中的Sparse Attention模块，用户希望升级Sparse Attention中使用的Triton版本到2.x以解决错误堆栈中的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4857
DeepSpeed,这是一个功能需求提议的issue，主要涉及DeepSpeed中AutoTP模块中安全张量的分片加载问题。,https://github.com/deepspeedai/DeepSpeed/issues/4854
DeepSpeed,这个issue类型是更新请求，涉及的主要对象是DeepSpeed版本信息。,https://github.com/deepspeedai/DeepSpeed/issues/4850
DeepSpeed,这是一个功能需求提议，涉及DeepSpeed中Comm Group的缓存机制，提议在创建新comm groups时共享单个对象，以避免内存泄漏问题。,https://github.com/deepspeedai/DeepSpeed/issues/4849
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed的测试触发器。由于没有提供具体内容，无法准确判断用户需求的具体目的。,https://github.com/deepspeedai/DeepSpeed/issues/4848
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是更新CODEOWNERS文件。由于文件中的内容为空，用户提出了更新的请求。,https://github.com/deepspeedai/DeepSpeed/issues/4838
DeepSpeed,这是一个功能需求的issue，该问题涉及DeepSpeed中的BF16优化器，主要对象是针对设备利用率并行化梯度更新操作，用户提出需求开启一个新的配置标志以实现此功能。,https://github.com/deepspeedai/DeepSpeed/issues/4837
DeepSpeed,这是关于增加NPU支持混合引擎的需求提出的issue，涉及的主要对象是DeepSpeed。,https://github.com/deepspeedai/DeepSpeed/issues/4831
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中多节点训练功能对不同加速器的支持，原因可能是不同硬件设备需要不同的环境变量设置。,https://github.com/deepspeedai/DeepSpeed/issues/4830
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed库中的Mixtral FastGen支持。由于用户希望实现Top2 MoE支持、更好的RoPE thetas支持以及添加Mistral模型实现，因此提出了该需求。,https://github.com/deepspeedai/DeepSpeed/issues/4828
DeepSpeed,这是一个更新版本文本文件的issue，类型为维护类操作，涉及主要对象为DeepSpeed库。,https://github.com/deepspeedai/DeepSpeed/issues/4826
DeepSpeed,这是一个关于CI workflows的小型更新的Issue，类型为工具/流程优化，涉及主要对象为项目的持续集成（CI）工作流。,https://github.com/deepspeedai/DeepSpeed/issues/4823
DeepSpeed,这是一个类型为代码更新的issue，涉及主要对象为DeepSpeed中的TRANSFORMERS_CACHE变量。由于TRANSFORMERS_CACHE被标记为即将移除，用户被建议使用HF_HOME替代，因此产生了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/4816
DeepSpeed,这是一个用户提出需求的issue，主要涉及 DeepSpeed 中针对NPU设备的ZeRO-Infinity功能添加问题。,https://github.com/deepspeedai/DeepSpeed/issues/4809
DeepSpeed,这是一个用户提出需求的类型，主要关注DeepSpeed和Megatron之间的流水线并行性差异。,https://github.com/deepspeedai/DeepSpeed/issues/4801
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是DeepSpeed下的transformers workflow。由于当前无法使用`workflow_dispatch`触发transformers workflow，用户提出需求希望实现这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/4800
DeepSpeed,这个issue是一个功能需求类型，主要涉及的对象是DeepSpeed项目。由于需要更新加速Torch版本，用户提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/4799
DeepSpeed,这是一个关于更新transformers工作流以使用最新torch版本的issue，类型为需求。主要涉及的对象是DeepSpeed下的transformers工作流。由于一些可能的问题，transformers工作流被禁用，需要重新启用。,https://github.com/deepspeedai/DeepSpeed/issues/4798
DeepSpeed,这个issue类型是需求提出，该问题单涉及的主要对象是CI流程。由于未指定使用最新的transformers库，导致CI流程中没有使用最新版本的transformers。,https://github.com/deepspeedai/DeepSpeed/issues/4796
DeepSpeed,这是对DeepSpeed-FastGen添加对Falcon模型支持的一项功能需求，涉及主要对象为不同规模的Falcon模型，通过这个需求，用户希望DeepSpeed-FastGen能够支持Falcon的不同规模模型并生成对应的文本输出。,https://github.com/deepspeedai/DeepSpeed/issues/4790
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的fast-gen模块是否支持int4/int8权重推理，由于int4/int8量化是最佳推理加速策略，用户希望fast-gen能够支持int4/int8权重推理。,https://github.com/deepspeedai/DeepSpeed/issues/4786
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是Dockerfile，用户希望更新其中的版本信息。原因可能是为了修复或更新依赖项，以确保系统的稳定性和可靠性。,https://github.com/deepspeedai/DeepSpeed/issues/4780
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed框架中的工作流程编辑功能。可能是用户希望实现对编辑后的工作流程执行特定操作的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4779
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中的加载机制问题。由于加载检查点到CPU可能导致性能问题，用户建议通过元设备加载权重。,https://github.com/deepspeedai/DeepSpeed/issues/4773
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中BF16训练中缺乏自动类型转换的功能。,https://github.com/deepspeedai/DeepSpeed/issues/4772
DeepSpeed,这是一个代码优化类型的issue，主要涉及到DeepSpeed项目中构建脚本的可读性和易维护性。,https://github.com/deepspeedai/DeepSpeed/issues/4767
DeepSpeed,这是一个功能需求的问题，主要涉及的对象是DeepSpeed中的PipelineModule模块，要求允许在PipelineModule中进行多变量的forward传递。,https://github.com/deepspeedai/DeepSpeed/issues/4766
DeepSpeed,这个issue属于用户提出需求类型，主要对象是Dockerfile。由于基础镜像、Python3、PyTorch等需要更新到最新版本，导致用户提出了更新Dockerfile的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4763
DeepSpeed,该issue类型为用户提出需求/请教问题，主要涉及DeepSpeed和torchrun之间训练结果的显著差异。由于warmup_min_lr和优化器等配置的差异，导致了深度学习模型在两个训练框架下表现差异显著的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4762
DeepSpeed,这是一个用户提出需求的issue，涉及DeepSpeed的fp16优化状态支持与Zero Stage3相关的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4757
DeepSpeed,"这是一个关于""请求""类型的issue，涉及DeepSpeed是否支持基于Torch后端的Keras模型，用户想知道是否可以使用DeepSpeed来训练这种模型。",https://github.com/deepspeedai/DeepSpeed/issues/4754
DeepSpeed,这是一个功能需求报告，主要涉及DeepSpeed中的Universal Checkpoint模块，问题涉及了在训练中支持DS序列并行性的情况。,https://github.com/deepspeedai/DeepSpeed/issues/4752
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed下的推理路径中支持仅权重量化的问题。原因是用户希望使权重量化能在多卡上生效。,https://github.com/deepspeedai/DeepSpeed/issues/4750
DeepSpeed,这是一个关于更新DeepSpeed在Windows上安装指南的issue，主要对象是安装DeepSpeed的Windows用户。原因是为了大幅改进安装指南，因为先前的指南可能存在不清晰或不完整的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4748
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及的对象是DeepSpeed的README文件。ldapSearch.performFilter(caught Exception)可能由于缺少nv-sd徽章而导致错误。,https://github.com/deepspeedai/DeepSpeed/issues/4747
DeepSpeed,这是一个特性改进类型的issue，涉及主要对象为SP的性能优化。原因是为了减少通信开销，特别是在增加SPdegree到>2（4或8）时。,https://github.com/deepspeedai/DeepSpeed/issues/4735
DeepSpeed,该issue属于用户提出需求类型，涉及对象为DeepSpeed的文档。由于缺少具体内容，该问题可能是用户希望DeepSpeed文档中包含有关Intel推理的信息或者相关博客链接。,https://github.com/deepspeedai/DeepSpeed/issues/4734
DeepSpeed,这个issue属于功能需求类型，涉及主要对象是DeepSpeed中的NPU设备支持。这个问题是为了支持NPU设备的特性需求，如1bit Adam，1bit Lamb，0/1 Adam而提出的。,https://github.com/deepspeedai/DeepSpeed/issues/4733
DeepSpeed,这个issue类型是需求提议，主要涉及的对象是为支持特定NPU设备功能而添加的`HcclBackend`。,https://github.com/deepspeedai/DeepSpeed/issues/4732
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及DeepSpeed在Windows上安装的正确指导。由于当前指示不清晰，导致DeepSpeed版本9.x至12.x在Windows上编译失败，用户寻求正确的安装指导。,https://github.com/deepspeedai/DeepSpeed/issues/4729
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的flops profiler更新，针对transformer模型中的`scaled_dot_product_attention`和`__matmul__`表达式进行处理，由于原先的flops profiler没有支持这些操作，用户请求更新以支持这些操作。,https://github.com/deepspeedai/DeepSpeed/issues/4724
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed下的FLOPS Profiler，用户希望支持`scaled_dot_product_attention`和`__matmul__`等表达式。,https://github.com/deepspeedai/DeepSpeed/issues/4723
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeedEngine对模型进行包装后，使用named_parameters()时大部分权重都是空的情况。,https://github.com/deepspeedai/DeepSpeed/issues/4720
DeepSpeed,这个issue是一个用户提出的需求，主要针对DeepSpeed在CPU平台上的部署问题，用户希望实现部分权重的按需加载。导致这个问题的原因是当在CPU上使用AutoTP时，单个rank加载所有权重导致大量内存需求。,https://github.com/deepspeedai/DeepSpeed/issues/4719
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中BF16_Optimizer优化器的支持性需求。,https://github.com/deepspeedai/DeepSpeed/issues/4713
DeepSpeed,"这个issue是关于提交一个有关""Nvme offload checkpoint""的功能提议，主要涉及深度学习模型训练中的checkpointing问题，提出了更稳定的文件命名方案以解决原来基于Python id导致的不稳定性问题，并解决了训练过程中NVMe offload文件不匹配的情况。",https://github.com/deepspeedai/DeepSpeed/issues/4707
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed库中的多维并行性布局功能。,https://github.com/deepspeedai/DeepSpeed/issues/4706
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中支持多节点推理的功能。用户希望在多个节点上运行一个LLM模型，其中一个节点使用张量并行加速，而在多个节点间使用管道并行。用户询问是否支持该功能以及有关文档的信息。,https://github.com/deepspeedai/DeepSpeed/issues/4704
DeepSpeed,这是一个需求提出类型的issue，主要涉及DeepSpeed中的launcher功能，解决了在rank数极高时出现的Linux命令行长度限制问题。,https://github.com/deepspeedai/DeepSpeed/issues/4699
DeepSpeed,该issue属于改进类型，主要涉及DeepSpeed库中的分片大小平衡问题，主要问题是分片大小不均匀可能导致性能下降。,https://github.com/deepspeedai/DeepSpeed/issues/4697
DeepSpeed,这是一个优化需求类型的issue，主要涉及DeepSpeed中的通信优化问题，用户寻求改进大规模训练系统的通信性能。,https://github.com/deepspeedai/DeepSpeed/issues/4695
DeepSpeed,这是一个用户提出需求的问题，主要涉及对于benchmarking studies代码是否在该repo中的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/4691
DeepSpeed,这是一个用户提出需求的issue，涉及到更新requirements.txt文件。原因可能是希望添加或更新依赖的库版本。,https://github.com/deepspeedai/DeepSpeed/issues/4690
DeepSpeed,这是一个用户提出的需求问题，主要涉及DeepSpeed中模型参数的dtype混用。,https://github.com/deepspeedai/DeepSpeed/issues/4689
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed的Transformer核心性能评测，用户想了解Transformer核心的性能加速情况。,https://github.com/deepspeedai/DeepSpeed/issues/4686
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中添加了用于获取和设置ZeRO-3分区参数的API。由于在某些场景下只需要本地状态，因此需要引入新的API来处理局部参数。,https://github.com/deepspeedai/DeepSpeed/issues/4681
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的inference.v2 HuggingFaceCheckpointEngine，用户希望新增对safetensors支持。由于HF Transformers新版本支持safetensors格式的权重文件，并且越来越多新模型也采用该格式。,https://github.com/deepspeedai/DeepSpeed/issues/4680
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed与torch.compile的结合问题，由于tracing hooks的实现问题导致DeepSpeed与torch.compile无法正常运行。,https://github.com/deepspeedai/DeepSpeed/issues/4677
DeepSpeed,这是一个用户提出需求的issue，针对DeepSpeed是否支持使用XLA/TPU进行PyTorch模型训练的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4676
DeepSpeed,这是一个需求请求（REQUEST），主要涉及的对象是LLaMA 30B模型在DeepSpeed中的实现，由于最大可设置的序列并行维度受限于16而不是预期的64，导致性能下降，用户提出了关于处理8K序列长度效率低的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4674
DeepSpeed,这个issue类型是更新请求，涉及主要对象是DeepSpeed版本控制，由于发布了0.12.3版本而需要更新version.txt。,https://github.com/deepspeedai/DeepSpeed/issues/4673
DeepSpeed,这个issue类型是功能改进，主要涉及的对象是DeepSpeed中的KVcache，由于需要改进支持不同类型的KV缓存和适当的统计信息权衡，提出这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/4668
DeepSpeed,这是一个关于用户需求的问题，主要涉及如何监控和优化GPU和CPU资源使用；用户想更好地了解和监控在多GPU环境中的通信、参数和操作符传输，以及GPU内存和CPU内存的使用情况，希望能够获得更详细的分析和优化建议。,https://github.com/deepspeedai/DeepSpeed/issues/4667
DeepSpeed,这个issue类型是功能需求，主要对象是DeepSpeed V2中的Inference Checkpoints。由于需求减少大型模型的加载时间，用户提出了希望增加快照功能以便在引擎中暂停和恢复的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4664
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中添加对`.safetensors`检查点的支持，以解决HF仓库中的性能结果不一致问题。,https://github.com/deepspeedai/DeepSpeed/issues/4659
DeepSpeed,这是关于开发高性能GPU内核的需求探讨，用户询问DeepSpeed团队的CUDA内核测试和性能优化工作流程。,https://github.com/deepspeedai/DeepSpeed/issues/4655
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed-FastGen添加日文博客，并修复原博客中的拼写错误。可能由于用户希望增加DeepSpeed-FastGen的日文支持和纠正原博客的拼写错误而提出。,https://github.com/deepspeedai/DeepSpeed/issues/4651
DeepSpeed,这个issue属于用户提出需求类型，主要对象是DeepSpeed的Pipeline Parallelism功能。导致此需求的原因是DeepSpeed的Sequence Parallelism不能与Tensor Parallelism或Pipeline Parallelism同时使用。,https://github.com/deepspeedai/DeepSpeed/issues/4645
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed的offloadpp组件，该问题可能是因为tutorial链接需要更新而引起的。,https://github.com/deepspeedai/DeepSpeed/issues/4641
DeepSpeed,这是一个关于文档更新的问题，用户提出了需要更新新闻项目的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4640
DeepSpeed,这是一个关于功能增强的issue，主要涉及DeepSpeed的TwinFlow功能的介绍和实现。,https://github.com/deepspeedai/DeepSpeed/issues/4636
DeepSpeed,这个issue类型是用户提出需求，主要对象是Torch latest cpu。由于DeepSpeed软件包在使用Torch最新的CPU版本时出现问题，用户提出了相关需求。,https://github.com/deepspeedai/DeepSpeed/issues/4632
DeepSpeed,这是一个需求提升类的issue，主要涉及的对象是.gitignore文件。由于缺乏描述性的注释和文档，导致人们难以理解规则的用途，从而需要进行改进和解释。,https://github.com/deepspeedai/DeepSpeed/issues/4631
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed-FastGen中缺乏4bit量化支持导致批处理大小限制的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4622
DeepSpeed,这个issue类型是用户提出需求，问题涉及Inference Checkpoints，用户寻求帮助实现推断时的检查点和保存。,https://github.com/deepspeedai/DeepSpeed/issues/4620
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是关于DeepSpeed中快速推理检查点的功能。,https://github.com/deepspeedai/DeepSpeed/issues/4619
DeepSpeed,这个issue类型是更新文档内容，涉及的主要对象是DeepSpeed的版本信息，原因是发布了新的版本0.12.2而需要更新版本信息文档。,https://github.com/deepspeedai/DeepSpeed/issues/4617
DeepSpeed,这个issue属于用户向DeepSpeed提出的需求类型，主要涉及DeepSpeed对PyTorch 2.1的使用情况。用户想了解DeepSpeed是否已经使用了PyTorch 2.1，并希望了解是否有性能方面的改进。,https://github.com/deepspeedai/DeepSpeed/issues/4616
DeepSpeed,这个issue类型为自动更新版本号的任务，主要涉及到DeepSpeed项目的版本控制。这个issue的目的是更新版本号文件，为新发布的版本0.12.1做准备。,https://github.com/deepspeedai/DeepSpeed/issues/4615
DeepSpeed,这是一个功能需求类型的issue，主要涉及添加 latency 比较功能。,https://github.com/deepspeedai/DeepSpeed/issues/4612
DeepSpeed,这是一个更新型issue，主要涉及DeepSpeed的版本更新。,https://github.com/deepspeedai/DeepSpeed/issues/4611
DeepSpeed,这是一个用户提出需求的issue，主要涉及 DeepSpeed-FastGen 的源码和测试代码的隔离，旨在减少持续集成运行次数。,https://github.com/deepspeedai/DeepSpeed/issues/4610
DeepSpeed,这个issue类型为版本更新，涉及主要对象为DeepSpeed项目中的版本文件(version.txt)，由于发布了0.11.2版本导致需要更新版本文件。,https://github.com/deepspeedai/DeepSpeed/issues/4609
DeepSpeed,该issue是一个功能需求，主要涉及DeepSpeed中的CheckpointEngine，提出了为checkpoint engine添加open和close方法的需求。由于缺乏这样的方法，可能导致在重新启动期间无法准确定位和加载检查点。,https://github.com/deepspeedai/DeepSpeed/issues/4608
DeepSpeed,这个issue是关于新功能提议的，主要对象是DeepSpeedFastGen，由于追求更高性能，引入了SplitFuse机制来提高连续批处理和系统吞吐量。,https://github.com/deepspeedai/DeepSpeed/issues/4604
DeepSpeed,这是一个关于更新DeepSpeed中ds-chat工作流以适应新的安装方式的issue，属于用户提出需求的类型。,https://github.com/deepspeedai/DeepSpeed/issues/4598
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed下的支持Ascend NPU的moe功能。由于开始支持moe功能后，在GPU和NPU上运行DeepSpeed CIFAR10模型时，使用SGD优化器在测试图像上的准确率存在少许差异。,https://github.com/deepspeedai/DeepSpeed/issues/4586
DeepSpeed,这个issue是关于代码修改的需求，并涉及到DeepSpeed在ROCm上定义HIP_PLATFORM_AMD的问题，主要是为了在非JIT构建时确保正确性。,https://github.com/deepspeedai/DeepSpeed/issues/4585
DeepSpeed,这是一个文档更新的issue，涉及到DeepSpeed的文档内容。可能是由于原始文档缺少某些信息，需要补充更新。,https://github.com/deepspeedai/DeepSpeed/issues/4584
DeepSpeed,这是一个用户提出的需求类别的issue，问题涉及的主要对象是DeepSpeed。由于没有具体的内容，无法分析导致的具体症状或问题。,https://github.com/deepspeedai/DeepSpeed/issues/4583
DeepSpeed,这个issue类型是用户提出需求，主要涉及DeepSpeed中关于优化器状态和梯度之间通信的改进建议。由于当前的ZeRO $P_{os + g}$优化方案中通信量较大，用户提出通过直接建立梯度和优化器状态之间的对应关系来减少通信量。,https://github.com/deepspeedai/DeepSpeed/issues/4582
DeepSpeed,这是一个特性需求的issue，主要涉及DeepSpeed推断框架中新增加权重量化算法的统一接口。原因是为了支持更多的权重量化算法，以便用户可以根据实际情况选择使用哪种算法。,https://github.com/deepspeedai/DeepSpeed/issues/4577
DeepSpeed,这个issue类型是用户提出需求，请教问题，主要涉及DeepSpeed中的不同框架的速度性能，以及是否有比DeepSpeed更快的框架。由于用户想了解哪种DeepSpeed框架拥有最佳速度性能，以及是否有超过DeepSpeed的更快框架。,https://github.com/deepspeedai/DeepSpeed/issues/4575
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中参数梯度和优化器状态分区的问题，用户希望保持指定参数的梯度和优化器状态不被分区。,https://github.com/deepspeedai/DeepSpeed/issues/4572
DeepSpeed,这是一个用户提出需求的issue，主要涉及支持在Ascend NPU上进行CPU离线优化器（optimizer）使用。,https://github.com/deepspeedai/DeepSpeed/issues/4568
DeepSpeed,该issue类型为功能需求，涉及对象为DeepSpeed下的Full feature支持Ascend NPU。由于已有NPU加速器支持的合并，但要实现完整支持还需要实现更多功能。,https://github.com/deepspeedai/DeepSpeed/issues/4567
DeepSpeed,这是一个用户提出需求的类型，主要对象是Triton在Windows操作系统上的支持。由于Triton在Windows上仍不可用，用户表达了遗憾。,https://github.com/deepspeedai/DeepSpeed/issues/4564
DeepSpeed,这是一个用户提出需求的issue，该问题单涉及的主要对象是DeepSpeed的lr_schedules.py文件。由于缺少余弦退火调度器，用户提出需要添加这一功能以支持更广泛的应用。,https://github.com/deepspeedai/DeepSpeed/issues/4563
DeepSpeed,这是一个关于用户提出需求的issue，主要涉及DeepSpeed中的梯度提取和日志记录问题。原因是DeepSpeed在使用时无法返回梯度，导致用户无法获取梯度范数来跟踪训练状态。,https://github.com/deepspeedai/DeepSpeed/issues/4555
DeepSpeed,该issue属于需求提出类型，主要涉及的对象是对DeepSpeed项目添加对BigCode模型的支持。由于BigCode模型在 https://github.com/microsoft/DeepSpeed/issues/3811中被提及为一个流行的架构，因此提出了该需求。,https://github.com/deepspeedai/DeepSpeed/issues/4549
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed中的sequence parallelism实现是否需要在optimizer.step()之前进行梯度的全局归约，可能是由于在训练网络前将序列分割为子序列的原因，导致用户疑惑是否需要类似数据并行中的梯度全归约操作。,https://github.com/deepspeedai/DeepSpeed/issues/4548
DeepSpeed,这是一个功能增强类型的issue，主要涉及XPU加速器对Intel GPU设备的支持。原因可能是为了增强DeepSpeed对不同XPU设备的兼容性。,https://github.com/deepspeedai/DeepSpeed/issues/4547
DeepSpeed,这是一个文档更新的问题，主要对象是DeepSpeed项目。原因可能是希望更新论文内容或者说明文档的相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/4543
DeepSpeed,该issue类型为文档需求，主要涉及DeepSpeed的ZeRO infinity功能的幻灯片和博客。由于缺乏相关资料，用户请求补充相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/4542
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的Ulysses项目，用户希望在Ulysses博客上添加ColAI评估。可能是用户希望增加ColAI评估功能以提升Ulysses项目的性能。,https://github.com/deepspeedai/DeepSpeed/issues/4517
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的universal checkpoint功能，用户希望在zero stage 1中启用该功能。原因可能是为了提升模型训练的效率和灵活性。,https://github.com/deepspeedai/DeepSpeed/issues/4516
DeepSpeed,这是一个关于功能需求的issue，主要涉及DeepSpeed在推理过程中支持传递`past_key_values`的问题，由于DeepSpeed似乎不支持在input_ids长度`>=2`的情况下输入自定义的`past_key_values`，导致了在使用LLMA技术时产生错误的输出。,https://github.com/deepspeedai/DeepSpeed/issues/4515
DeepSpeed,这是一个类型是需求提出的issue，主要对象是DeepSpeed4Science white paper，用户提出了添加新的论文的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4502
DeepSpeed,这是一个用户提出需求的issue，该问题单涉及的主要对象是DeepSpeed的zero模块。由于需要添加zero35功能，用户请求对该模块进行扩展。,https://github.com/deepspeedai/DeepSpeed/issues/4499
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中关于转换zero sharded checkpoint文件以及包含MoE层的模型到fp32状态字典的功能支持。该问题由于当前zero_to_fp32.py的实现无法处理包含MoE层的模型导致报错。,https://github.com/deepspeedai/DeepSpeed/issues/4497
DeepSpeed,这个issue属于需求类型，涉及的主要对象是DeepSpeed的CI系统。由于需要将AMD团队添加到CI问题创建中，可能是因为需要他们参与问题追踪或相关工作。,https://github.com/deepspeedai/DeepSpeed/issues/4487
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed的ROCm版本更新。可能由于原版本与新版本的不兼容性或者功能缺失导致用户需要ROCm版本更新。,https://github.com/deepspeedai/DeepSpeed/issues/4486
DeepSpeed,这是一个更新版本号的issue，涉及的主要对象是DeepSpeed软件。,https://github.com/deepspeedai/DeepSpeed/issues/4484
DeepSpeed,这是一个用户提出需求的issue，主要涉及如何在所有rank中获取平均损失。该问题出现的原因是每个rank中损失不同，用户希望获取所有rank的平均损失。,https://github.com/deepspeedai/DeepSpeed/issues/4472
DeepSpeed,该issue类型为功能需求，主要涉及到更新发布脚本以及添加可以发布DeepSpeed新版本的yml流程。,https://github.com/deepspeedai/DeepSpeed/issues/4467
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及的对象是DeepSpeed中的Loadams/update release功能。由于缺少具体描述内容，无法确定导致此issue的原因和用户需求的具体内容。,https://github.com/deepspeedai/DeepSpeed/issues/4466
DeepSpeed,这是一个用户提出需求的问题，主要涉及到DeepSpeed中添加Meta-Transformer作为统一多模态学习框架的请求。由于多模态学习中设计统一网络处理不同模态数据的挑战，导致用户提出Meta-Transformer框架的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4463
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed-VisualChat项目，用户希望有关中文博客的内容。,https://github.com/deepspeedai/DeepSpeed/issues/4458
DeepSpeed,这个issue类型是用户提出需求，该问题主要涉及DeepSpeed项目的README-Japanese.md文件更新。由于该文件可能需要添加或修改信息，用户提出了更新的请求。,https://github.com/deepspeedai/DeepSpeed/issues/4457
DeepSpeed,这个issue类型是内容增加请求，涉及主要对象是DSVisualChat博客。由于需要添加DSVisualChat博客的日文翻译，用户提出了这个请求。,https://github.com/deepspeedai/DeepSpeed/issues/4454
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed下的8bit dequantization kernel的添加。,https://github.com/deepspeedai/DeepSpeed/issues/4450
DeepSpeed,该问题类型是缺失功能提出，该问题单涉及的主要对象是DeepSpeed的`load_from_fp32_weights`配置参数。导致此问题的原因是在DeepSpeed文档中缺少对该配置参数的说明。,https://github.com/deepspeedai/DeepSpeed/issues/4449
DeepSpeed,这是一个关于DeepSpeed-VisualChat Blog的类型为用户提出需求的issue，主要涉及的对象是DeepSpeed-VisualChat Blog。,https://github.com/deepspeedai/DeepSpeed/issues/4446
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed的VChat功能。,https://github.com/deepspeedai/DeepSpeed/issues/4445
DeepSpeed,这是一个需求提出的issue，主要对象是DeepSpeed库中的cpu_inference功能。该问题暴露了用户希望在运行时灵活启用cpu_inference功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4444
DeepSpeed,这是一个关于使用DeepSpeed中Sequence Parallel功能时遇到的问题，用户提出了关于通信时间减少与序列长度增加之间的困惑。,https://github.com/deepspeedai/DeepSpeed/issues/4441
DeepSpeed,这是一个优化需求，涉及到DeepSpeed库中的grad_norm计算，在计算过程中频繁调用.item()导致设备与主机之间的同步。,https://github.com/deepspeedai/DeepSpeed/issues/4436
DeepSpeed,这个issue是一个功能请求，请求跳过测试模型任务。这可能是因为测试模型任务耗时长，用户希望能够灵活选择是否执行该任务。,https://github.com/deepspeedai/DeepSpeed/issues/4427
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的ZeRO Hybrid Sharding Strategies。原因是用户在训练规模大的模型时，遇到了ZeRO1内存不足和ZeRO3跨节点传输带宽不足的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4426
DeepSpeed,这是一个需求撤销某次提交的issue，主要涉及DeepSpeed中的CPU autotp功能。这个问题可能是由某次提交引入了意外行为或者功能问题导致的。,https://github.com/deepspeedai/DeepSpeed/issues/4419
DeepSpeed,这个issue是一个功能建议类型，主要涉及到DeepSpeed中使用ZeRO-3训练并进行NVMe offload时的模型checkpointing。由于当前的文件命名方案不稳定，导致文件不断增长在磁盘上占用空间，且无法重新加载checkpoint，因此提出了替代的命名方案和相关实现。,https://github.com/deepspeedai/DeepSpeed/issues/4416
DeepSpeed,这是一个需求更新类的issue，主要涉及的对象是DeepSpeed中的nv-transformers代码库。用户提出更新nv-transformers以使用cuda 11.6的需求，同时需要在Ubuntu 20.04上运行以支持新的async_io特性。,https://github.com/deepspeedai/DeepSpeed/issues/4412
DeepSpeed,这个issue类型是用户提出需求，需要获取Ubuntu版本信息，由于缺少相关信息，用户需要寻求帮助。,https://github.com/deepspeedai/DeepSpeed/issues/4411
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed4Science博客更新问题。由于DeepSpeed相关内容需要更新到博客中，导致用户提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/4408
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及到DeepSpeed在pydantic>=2.0.0版本上的兼容性问题，希望引入一个用于支持pydantic>=2.0.0的兼容性模块。,https://github.com/deepspeedai/DeepSpeed/issues/4407
DeepSpeed,这个issue类型属于功能需求提议，主要涉及DeepSpeed项目中添加了控制超时时间的环境变量，用户希望在某些特定场景下能够更精细地控制超时时间。,https://github.com/deepspeedai/DeepSpeed/issues/4405
DeepSpeed,这是一个性能优化相关的issue，主要涉及的对象是DeepSpeed框架的CCLBackend，提出了并行memcpy以提高推理操作的性能，并为大型消息大小的处理做了改进。这个问题的原因是为了充分利用主机内存带宽并提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/4404
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的checkpoint和rng_state存储问题。根据用户描述，该需求是由于对存储空间的需求过高而提出的。,https://github.com/deepspeedai/DeepSpeed/issues/4403
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed项目中的transformers版本问题，希望对transformers workflow进行解除版本锁定。,https://github.com/deepspeedai/DeepSpeed/issues/4388
DeepSpeed,这个issue类型是需求提出，主要涉及的对象是DeepSpeed库中的ZeRO模块，用户提出了需要添加destroy函数和destructor以释放内存的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4383
DeepSpeed,这是一个用户提出需求的issue，涉及DeepSpeed的CCLBackend接口更新。,https://github.com/deepspeedai/DeepSpeed/issues/4378
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed下的Generic Transformer和Specialized Transformer的代码释出及如何在推理中使用，用户询问是否该repo中包含了这些代码，并如何在推理中使用。,https://github.com/deepspeedai/DeepSpeed/issues/4371
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed社区，用户希望能够有一篇关于DeepSpeed在科学计算中的日文博客。,https://github.com/deepspeedai/DeepSpeed/issues/4369
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed4Science中文博客，可能由于缺乏相关文档或信息导致用户需要额外的资源或指导。,https://github.com/deepspeedai/DeepSpeed/issues/4366
DeepSpeed,这是一个用户提出需求的类型issue，主要涉及的对象是DeepSpeed项目。这个issue由于用户希望添加DeepSpeed4Science博客链接而提出。,https://github.com/deepspeedai/DeepSpeed/issues/4364
DeepSpeed,这是一个需要更新conda环境使pydantic版本达到最大的类型为需求提出的issue，主要涉及DeepSpeed项目中的conda环境配置。由于当前的pydantic版本不是最新的，用户需要更新conda环境以满足项目要求。,https://github.com/deepspeedai/DeepSpeed/issues/4362
DeepSpeed,这个issue类型是在优化CI流程的需求提出，主要涉及DeepSpeed项目中的CI测试。由于每次PR都需要手动修改来运行测试修复，用户希望通过添加workflow_dispatch来简化这一流程。,https://github.com/deepspeedai/DeepSpeed/issues/4361
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed中的PPO算法和Zero优化阶段。用户询问由于使用Zero阶段3和offload导致生成速度太慢，寻求解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/4354
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的内存管理方式，提出了希望通过启用同页合并功能来减少内存占用的建议。,https://github.com/deepspeedai/DeepSpeed/issues/4352
DeepSpeed,这是一个功能提议，主要涉及DeepSpeed中的DS-Inference Quantization功能。由于新特性的添加，可能会出现一些问题需要修复。,https://github.com/deepspeedai/DeepSpeed/issues/4351
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中autotp和metatensor功能与Llama2等所有模型的配合使用，请求支持传递base_dir参数以加载模型文件。,https://github.com/deepspeedai/DeepSpeed/issues/4348
DeepSpeed,这是一个性能优化类型的issue，主要涉及DeepSpeed中的RMSNormalize与Trident结合优化带来的性能提升。由于Trident性能库的应用，导致在Forward和Backward阶段性能提升分别达到了300%和425%。,https://github.com/deepspeedai/DeepSpeed/issues/4347
DeepSpeed,这是一个用户提出需求的问题，主要涉及的对象是DeepSpeed中的分布式训练功能。用户询问如何提高训练效率，原因是在使用forward_microstep和backward_microstep时花费了大量时间。,https://github.com/deepspeedai/DeepSpeed/issues/4346
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是添加NPU FusedAdam支持。,https://github.com/deepspeedai/DeepSpeed/issues/4343
DeepSpeed,这是一个用户提出需求的类型的issue， 主要涉及对象是DeepSpeed中的test_compression功能。,https://github.com/deepspeedai/DeepSpeed/issues/4339
DeepSpeed,这是一个增强功能提议类型的issue，主要涉及DeepSpeed中的triton flash attention2 kernel。由于性能需求，用户提出了添加新的attention2 kernel以进一步优化推理时的延迟问题。,https://github.com/deepspeedai/DeepSpeed/issues/4337
DeepSpeed,这是一个功能需求类型的issue，主要涉及的对象是DeepSpeed中新增的Lion optimizer。,https://github.com/deepspeedai/DeepSpeed/issues/4331
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中使用transformers和torch版本的BERT替代文件，原因是需要更新部分函数签名和替换部分文件。,https://github.com/deepspeedai/DeepSpeed/issues/4329
DeepSpeed,这个issue类型为功能需求，主要对象为DeepSpeed项目。这个issue的原因是需要添加一个脚本来检查下一个DeepSpeed版本是否有效。,https://github.com/deepspeedai/DeepSpeed/issues/4328
DeepSpeed,该issue类型为优化需求，主要涉及 DeepSpeed 中的 _aggregate_total_loss 函数，旨在通过减少设备和主机之间的拷贝依赖来提高运行效率。,https://github.com/deepspeedai/DeepSpeed/issues/4327
DeepSpeed,这是一个功能需求提议，主要对象是DeepSpeed的pipe engine，提议为eval_batch添加一个选项来禁用损失广播，以减少通信开销。,https://github.com/deepspeedai/DeepSpeed/issues/4326
DeepSpeed,这是一个优化建议类型的issue，主要涉及DeepSpeed中Pipe Engine的实现。由于重复的通信和主机同步，建议缓存TP激活和梯度的元数据以避免性能下降。,https://github.com/deepspeedai/DeepSpeed/issues/4324
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed配置中的管道分区选项的添加，用户希望可以禁用管道分区。由于需要在训练过程中禁用激活和梯度的分区，用户提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/4322
DeepSpeed,这个issue类型是用户提出需求，并涉及到如何使用图来解决短内核序列执行的效率问题。由于短内核序列的启动开销远高于执行开销，需要通过图来优化，同时需要在GPU上执行部分操作以解决此问题。,https://github.com/deepspeedai/DeepSpeed/issues/4318
DeepSpeed,这个issue类型是文档更新，主要涉及DeepSpeed的README.md，由于有几个错别字而被指出。,https://github.com/deepspeedai/DeepSpeed/issues/4316
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed框架中如何在指定GPU上训练GAN模型的问题。由于用户希望在共享工作站上只使用特定的GPU，因此寻求设置DeepSpeed以在指定GPU上运行的方法。,https://github.com/deepspeedai/DeepSpeed/issues/4315
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的llama模型的支持问题。由于目前不支持需要kvsharing的llama变种，用户提出了添加对KVshared架构的支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4313
DeepSpeed,这是一个用户提出需求的issue，该问题单涉及的主要对象是DeepSpeed，用户提出希望能够训练模型时使用多个优化器，由于当前不支持该功能，导致在模型选择方面受到限制。,https://github.com/deepspeedai/DeepSpeed/issues/4307
DeepSpeed,这个issue属于更新需求，主要涉及DeepSpeed中的ZeROInference release。可能是由于需要更新ZeROInference的README而提出的。,https://github.com/deepspeedai/DeepSpeed/issues/4303
DeepSpeed,这个issue类型是文档更新，涉及主要对象为DeepSpeed项目的文档。由于存在几处拼写错误，导致了需要修正文档的情况。,https://github.com/deepspeedai/DeepSpeed/issues/4297
DeepSpeed,这是一个用户提出需求的issue，主要针对DeepSpeed中的MoE layer，请求添加每个MoE层可以使用多种类型的网络而不仅限于一种网络。,https://github.com/deepspeedai/DeepSpeed/issues/4291
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的allgather操作。由于使用allgather会导致额外的GPU内存分配和D2D操作，建议替换为all_gather_into_tensor以节省GPU内存和减少额外操作。,https://github.com/deepspeedai/DeepSpeed/issues/4282
DeepSpeed,这是一个需求提出类型的issue，主要涉及DeepSpeed下的pipeline runtime的泛化，目的是为了支持Slapo的调度原语实现每个pipeline阶段的张量并行化。,https://github.com/deepspeedai/DeepSpeed/issues/4280
DeepSpeed,这是一个用户对DeepSpeed推理中生成文本行为不清楚的需求，涉及到DeepSpeed推理时针对生成文本的行为方式问题，主要原因是缺乏文档中的明确说明。,https://github.com/deepspeedai/DeepSpeed/issues/4276
DeepSpeed,这是一个关于如何在DeepSpeed中储存优化器状态以便稍后恢复微调的问题，类型为用户提出需求。主要涉及对象是深度学习模型训练中的优化器状态和学习率调度器状态的储存与加载。问题源于用户尝试进行长时间微调，并需要支持优化器和学习率调度器状态的检查点功能。,https://github.com/deepspeedai/DeepSpeed/issues/4275
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中FlashAttention在LLM推断过程中的支持问题。,https://github.com/deepspeedai/DeepSpeed/issues/4273
DeepSpeed,这个issue是针对功能需求的，涉及主要对象为Llama2 finetuning和HF Accelerate finetuning。,https://github.com/deepspeedai/DeepSpeed/issues/4270
DeepSpeed,这是一个需求提出类型的issue，主要涉及了DeepSpeed库中的注入功能的重构以接受策略参数，可能是为了提高性能或实现特定的功能需求。,https://github.com/deepspeedai/DeepSpeed/issues/4267
DeepSpeed,这是一个特性请求，关于DeepSpeed推理引擎是否支持Code Llama 34B的问题。由于目前不支持该功能，用户正在询问未来是否有计划加入支持。,https://github.com/deepspeedai/DeepSpeed/issues/4261
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的FFN计算，在训练大型上下文LLMs时可能出现内存不足错误。,https://github.com/deepspeedai/DeepSpeed/issues/4260
DeepSpeed,该issue类型是用户提出需求，主要对象是使用DeepSpeed中的pipeline engine。由于用户希望在pipeline的第一个阶段读取数据而不是每个阶段都将数据加载到内存中，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/4255
DeepSpeed,这是一个用户提出需求的类型，主要对象是希望DeepSpeed项目添加发布版本。这可能是由于用户希望获得最新功能和稳定性改进而提出的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4252
DeepSpeed,这是一个需求类型的issue，主要对象是DeepSpeed的landing pages，用户提出需要在landing pages中添加新闻链接。,https://github.com/deepspeedai/DeepSpeed/issues/4243
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed框架下的Mixed Precision ZeRO++教程添加问题。,https://github.com/deepspeedai/DeepSpeed/issues/4241
DeepSpeed,这个issue类型是需求提出，主要对象是DeepSpeed项目中的AMD MI200和H100。由于需要在分支中进行测试而无需修改YML文件来适应测试，用户提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/4238
DeepSpeed,这个issue类型是功能增强需求，涉及的主要对象是DeepSpeed中的ZeRO分布式训练功能。,https://github.com/deepspeedai/DeepSpeed/issues/4232
DeepSpeed,这是一个需求提交类型的issue，主要涉及DeepSpeed中添加NPU支持数据类型的需求。由于缺乏NPU支持数据类型，在特定场景下可能存在限制或性能问题。,https://github.com/deepspeedai/DeepSpeed/issues/4223
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中通信量降低分析的问题。用户想要更好理解DeepSpeed Ulysses中通信量减少的分析。,https://github.com/deepspeedai/DeepSpeed/issues/4217
DeepSpeed,该issue类型为特性需求，主要涉及DeepSpeed中的TiedLayerSpec，用户提出需要支持多个tied weights，以实现整个层的重用。,https://github.com/deepspeedai/DeepSpeed/issues/4216
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是添加 Ulysses 博客索引。,https://github.com/deepspeedai/DeepSpeed/issues/4215
DeepSpeed,该issue类型为功能需求，主要涉及DeepSpeed中的梯度属性名称简化以及在ZeRO阶段1和2中的函数调用。该需求由于需要简化梯度属性名称和函数调用，以提高代码可读性和易用性。,https://github.com/deepspeedai/DeepSpeed/issues/4214
DeepSpeed,该issue属于文档更新类型，主要涉及README.md文件的修改。,https://github.com/deepspeedai/DeepSpeed/issues/4211
DeepSpeed,这是一个用户提出需求的类型的issue，主要对象是DeepSpeed下的DS-Ulysses项目。,https://github.com/deepspeedai/DeepSpeed/issues/4209
DeepSpeed,这个issue是一个用户需求类型的问题，主要涉及如何加载一个130G的模型。用户寻求帮助解决加载大型模型时可能遇到的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4208
DeepSpeed,这是一个需求类型的issue，主要涉及更新DeepSpeed项目中的Ulyssess README文件。由于README内容需要更新，用户提出了更新Ulyssess README的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4205
DeepSpeed,这是一个需求提出类型的issue，主要对象是DeepSpeedUlysses。由于信息匮乏，用户提出了一个新的DeepSpeedUlysses的消息项需求。,https://github.com/deepspeedai/DeepSpeed/issues/4202
DeepSpeed,该issue类型是用户提出需求，请教问题，主要涉及对象是DeepSpeed Ulysses tutorial。由于缺乏详细内容，用户可能在尝试使用DeepSpeed Ulysses tutorial时遇到问题或需要帮助。,https://github.com/deepspeedai/DeepSpeed/issues/4200
DeepSpeed,该issue属于用户提出需求类型，主要对象是DeepSpeed-Ulysses tutorial。,https://github.com/deepspeedai/DeepSpeed/issues/4199
DeepSpeed,这个issue是关于DeepSpeed Ulysses release的需求类型问题，涉及主要对象是DeepSpeed。这个问题是用户在寻求关于DeepSpeed Ulysses release sequence parallelism支持的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/4198
DeepSpeed,这是一个功能优化建议，单涉及的主要对象是DeepSpeed中的ZeRO-Inference功能。由于需要优化权重量化和KV缓存的内存卸载至CPU，用户希望提出这个建议。,https://github.com/deepspeedai/DeepSpeed/issues/4197
DeepSpeed,这是一个用户提出需求的型 issue，主要涉及 DeepSpeed 下的权重修改问题，由于无法修改在深度训练阶段 3 模型中的权重导致的症状。,https://github.com/deepspeedai/DeepSpeed/issues/4192
DeepSpeed,这是一个功能使用问题，涉及主要对象为DeepSpeed。被警告缺少相关依赖的原因导致了用户无法正确执行DeepSpeed命令。,https://github.com/deepspeedai/DeepSpeed/issues/4189
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的autotuning功能。用户希望autotuner在考虑模型参数超出内存时，能够考虑到一些参数是冻结的。,https://github.com/deepspeedai/DeepSpeed/issues/4177
DeepSpeed,这是一个用户提出需求的issue，涉及主要对象为DeepSpeedExamples中的nv-ds-chat workflow。原因是为了添加一个名为`dse_branch`的输入用于指定克隆`DeepSpeedExamples`时要使用的分支，默认设置为`master`。,https://github.com/deepspeedai/DeepSpeed/issues/4173
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed在推理时如何在多个GPU上同时进行推理。用户提出了需要在服务器代码中集成DeepSpeed进行推理的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4166
DeepSpeed,这个issue类型为用户提出需求，主要涉及的对象为DeepSpeed。由于缺乏对 SIGTERM 信号的处理，用户提出了需要处理 SIGTERM 信号的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4160
DeepSpeed,这是关于特性请求的issue，主要涉及DeepSpeed中模型分片的示例代码请求，由于`save_mp_checkpoint_path`关键字在版本v0.10.0中无效导致无法保存被分片的OPT模型。,https://github.com/deepspeedai/DeepSpeed/issues/4159
DeepSpeed,这是一个用户提出需求的 issue，主要涉及减少模型和优化器初始化过程中 CPU 内存占用的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4148
DeepSpeed,这是一个功能请求，用户希望DeepSpeed在零优化模式下支持O1混合精度。,https://github.com/deepspeedai/DeepSpeed/issues/4142
DeepSpeed,这个issue是一个功能需求问题，主要涉及DeepSpeed项目中的冻结权重单元测试，讨论的主要内容是将冻结权重单元测试推广到所有零阶段。,https://github.com/deepspeedai/DeepSpeed/issues/4140
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed中的新模型InternLM，由于需要支持InternLM模型的特性所以提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/4137
DeepSpeed,该issue类型为功能迭代，主要涉及DeepSpeed中DS-Chat CI workflow的添加，用于测试DSChat pipeline的第三步。由于需要验证训练过程的正确性和模型文件的保存情况，可能是为了保证整个训练流程的准确性和可靠性。,https://github.com/deepspeedai/DeepSpeed/issues/4127
DeepSpeed,这个issue类型是用户提出需求，主要对象是DeepSpeed项目。由于缺乏指导，用户想要在DeepSpeed中添加DS_BUILD_*的完整列表。,https://github.com/deepspeedai/DeepSpeed/issues/4119
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的`activation_checkpointing`功能，旨在解决需要不计算梯度的输入导致反向传播错误的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4118
DeepSpeed,该issue为用户提出需求类型，主要涉及DeepSpeed中新增的端口参数功能。原因可能是用户希望能够通过SSH传输数据时指定端口。,https://github.com/deepspeedai/DeepSpeed/issues/4117
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及DeepSpeed的多节点训练/推理功能，请求添加自定义SSH端口参数选项。由于默认端口为22在某些情况下并不适用，导致无法使用多节点功能。,https://github.com/deepspeedai/DeepSpeed/issues/4116
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed在ROCm环境下启用量化器扩展。原因可能是希望在ROCm环境下使用量化器功能，但目前还不支持。,https://github.com/deepspeedai/DeepSpeed/issues/4114
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中量化模型的可用性问题，用户希望有一种方式能正确保存和加载带有正确数据类型的量化模型，同时解决如何在推理过程中使用这些模型的问题。这个问题可能是由于文档缺乏相关指导导致的。,https://github.com/deepspeedai/DeepSpeed/issues/4112
DeepSpeed,这个issue类型是需求更新，涉及主要对象是更新DeepSpeed中的megatron-lm版本。,https://github.com/deepspeedai/DeepSpeed/issues/4111
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed chat arxiv report的内容，用户希望往DeepSpeed项目中添加报告。,https://github.com/deepspeedai/DeepSpeed/issues/4110
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的数据预处理步骤共享问题，用户想要避免在每个GPU上重复运行数据预处理步骤。,https://github.com/deepspeedai/DeepSpeed/issues/4108
DeepSpeed,这是一个用户提出需求的类型issue，主要涉及DeepSpeed在Kubernetes环境下处理SIGTERM的问题。原因是Kubernetes中的kubelet发送SIGTERM给进程，DeepSpeed目前未处理该信号，因此用户希望DeepSpeed能够处理SIGTERM并传递给训练进程。,https://github.com/deepspeedai/DeepSpeed/issues/4098
DeepSpeed,这是一个关于需求的问题，涉及主要对象是DeepSpeed多GPU训练过程中关于额外信息`EXTRA_STATES`的优化存储和内存利用，用户寻求如何手动进行状态分区和全局通信以优化内存利用的解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/4082
DeepSpeed,这是一个向DeepSpeed开发者询问关于内存需求计算公式含义的问题，涉及到代码中的参数计算逻辑，用户对计算过程中不同部分所需内存的疑惑。,https://github.com/deepspeedai/DeepSpeed/issues/4076
DeepSpeed,这是关于代码更新的问题，涉及主要对象为DeepSpeed中的Sparse attention部分。由于最新的triton/tl.dot更新，导致需要更新Sparse attention来适配这些变化。,https://github.com/deepspeedai/DeepSpeed/issues/4071
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed中的triton版本更新问题。由于triton版本较旧，用户请求将其更新到最新版本。,https://github.com/deepspeedai/DeepSpeed/issues/4070
DeepSpeed,这是一个用户提出需求的类型，该问题涉及的主要对象是DeepSpeed项目的持续集成环境。由于缺少运行程序，导致需要禁用H100 CI测试。,https://github.com/deepspeedai/DeepSpeed/issues/4069
DeepSpeed,这个issue是一个用户提出需求的问题，涉及到DeepSpeed中的zero.GatheredParameters功能。用户想知道deepspeed.zero.GatheredParameters和checkpoint_event_prologue以及checkpoint_event_epilogue之间的区别。,https://github.com/deepspeedai/DeepSpeed/issues/4065
DeepSpeed,这是一个关于功能需求的issue，主要对象是DeepSpeed中的mpt模型，用户寻求自动计算TP（throughput）功能在huggingface model hub中的应用。,https://github.com/deepspeedai/DeepSpeed/issues/4062
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed库中的glm/codegen，用户希望在单元测试中添加一个融合qkv权重模型，并更新文档。,https://github.com/deepspeedai/DeepSpeed/issues/4057
DeepSpeed,这是一个性能优化的issue，主要涉及DeepSpeed库中的SHM allreduce功能。由于未经并行化处理的reduce kernel导致通信时间较长，通过添加OpenMP的并行处理来优化性能。,https://github.com/deepspeedai/DeepSpeed/issues/4049
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的安装指引问题。由于缺乏清晰的安装指南，用户遇到了安装错误并寻求能够正常运行的安装指引。,https://github.com/deepspeedai/DeepSpeed/issues/4046
DeepSpeed,"这是一个用户需求探讨类的问题，主要涉及DeepSpeed中选择Adam优化器类型的困惑。由于DeepSpeed不支持名为""cpu_adam""的优化器类型，导致用户无法仅在CPU上使用Adam，希望解决此问题。",https://github.com/deepspeedai/DeepSpeed/issues/4042
DeepSpeed,该issue是一个用户请求实现更改的需求，主要涉及DeepSpeed中的1F1B实现以及关于如何在pipeline并行中实现交错1F1B的问题。由于DeepSpeed目前没有这样的功能设计，用户希望得到帮助或建议。,https://github.com/deepspeedai/DeepSpeed/issues/4035
DeepSpeed,这是一个用户提出需求（feature request）类型的 issue，主要涉及DeepSpeed中的zero_to_fp32.py脚本，用户希望能够指定输出文件的数据类型（dtype），原因是转换DeepSpeed检查点至PyTorch检查点时会使用双倍的磁盘空间。,https://github.com/deepspeedai/DeepSpeed/issues/4032
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中监测损失计算功能的更新。,https://github.com/deepspeedai/DeepSpeed/issues/4030
DeepSpeed,这是一个用户提出需求的类型issue，主要涉及DeepSpeed下的checkpoint文件分离问题，用户希望能够更快地提取fp32权重而不必加载全部多余数据。,https://github.com/deepspeedai/DeepSpeed/issues/4029
DeepSpeed,这是一个关于性能优化的issue，主要涉及DeepSpeed中转换80B参数模型Z3检查点时CPU内存需求过高的问题。,https://github.com/deepspeedai/DeepSpeed/issues/4025
DeepSpeed,这是一个用户需求问题，用户询问如何在不经过训练的情况下使用int8量化推理，并希望能够直接对现有的llm模型进行量化推理。,https://github.com/deepspeedai/DeepSpeed/issues/4023
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的训练管道引擎，需要支持生成任务，可能由于RLHF需要这个功能而提出。,https://github.com/deepspeedai/DeepSpeed/issues/4018
DeepSpeed,这是一个功能需求类型的issue，涉及DeepSpeed中的AutoTP模块，用户希望能够在头数不被设备数整除的情况下仍然正常运行，因为有时用户可能需要让系统中的所有设备都参与工作。,https://github.com/deepspeedai/DeepSpeed/issues/4011
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed环境文件名的自定义，由于默认文件名`.deepspeed_env`会导致无法同时运行多个作业。,https://github.com/deepspeedai/DeepSpeed/issues/4006
DeepSpeed,这是一个优化需求类型的issue，主要对象是DeepSpeed的AMD/ROCm apex安装目录，用户提出通过预编译和重复使用apex install来节省编译时间。,https://github.com/deepspeedai/DeepSpeed/issues/3997
DeepSpeed,该issue属于优化问题，主要涉及DeepSpeed中计时器和日志记录功能的修改建议和疑问，由于命名、冗余性和风格等方面的问题，用户提出了优化建议。,https://github.com/deepspeedai/DeepSpeed/issues/3996
DeepSpeed,这是一个功能需求类型的issue，涉及的主要对象是更新libaio以不需要C++14，原因可能是为了提高代码的兼容性和可移植性。,https://github.com/deepspeedai/DeepSpeed/issues/3976
DeepSpeed,这是一个用户提出需求的issue，要求在文档中添加Zero++ paper的链接。,https://github.com/deepspeedai/DeepSpeed/issues/3974
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的配置文件`.deepspeed_env`，由于DeepSpeed只会在当前工作目录和主目录查找配置文件，导致在并行运行多个具有相同工作目录但需要不同环境变量的情况下出现冲突。,https://github.com/deepspeedai/DeepSpeed/issues/3965
DeepSpeed,这个issue是一个特性需求，主要涉及的对象是DeepSpeed下特定模型的lm_head和embed_out层的张量并行化。,https://github.com/deepspeedai/DeepSpeed/issues/3962
DeepSpeed,该issue类型为用户提出需求，主要涉及如何处理在ZeRO 3训练过程中操纵模型权重的问题，其中涉及到直接访问模型state_dict可能导致大小为0的张量问题。,https://github.com/deepspeedai/DeepSpeed/issues/3959
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是用于HE（Homomorphic Encryption）的autoTP模块。由于本身功能不完善或缺失，导致用户希望增加或改进HE支持的自动张量分片功能。,https://github.com/deepspeedai/DeepSpeed/issues/3957
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是DeepSpeed的例子文件。由于将所有DeepSpeed示例移动到一个独立的文件夹中，所以需要修复Megatron-DeepSpeed的链接。,https://github.com/deepspeedai/DeepSpeed/issues/3956
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的MP ZeRO++功能。,https://github.com/deepspeedai/DeepSpeed/issues/3954
DeepSpeed,该issue类型为用户提出需求，主要对象是DeepSpeed下的checkpoint保存功能。由于用户希望添加一个选项来只保存可训练权重，可能是为了节约存储空间或提高保存效率。,https://github.com/deepspeedai/DeepSpeed/issues/3953
DeepSpeed,这个issue类型是需求提出，涉及主要对象为CI构建流程。导致这个问题的原因是需要在CI构建失败时自动创建GitHub issue来通知相关人员。,https://github.com/deepspeedai/DeepSpeed/issues/3952
DeepSpeed,这是一个用户需求类型的issue，主要涉及的对象是DeepSpeed库的错误代码。由于错误代码不清晰，导致用户无法理解错误含义或解决问题。,https://github.com/deepspeedai/DeepSpeed/issues/3949
DeepSpeed,这个issue类型是用户提出需求，主要涉及DeepSpeed库中的模型检查点保存功能，用户希望添加一个选项只保存可训练权重，原因是在使用类似LoRA的PEFT方法训练LLMs时，只有少量参数被设置为可训练，为了减少存储空间并节省检查点保存时间，用户希望只保存可训练的参数。,https://github.com/deepspeedai/DeepSpeed/issues/3948
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed对ChatGLM模型的推断支持问题。,https://github.com/deepspeedai/DeepSpeed/issues/3945
DeepSpeed,这个issue属于用户提出需求类型，主要对象是DeepSpeed文档。由于缺少具体内容，用户提出了添加 xTrimoPGLM 的需求。,https://github.com/deepspeedai/DeepSpeed/issues/3940
DeepSpeed,这是一个用户提出需求的issue，主要关注DeepSpeed是否支持文中提到的pipeline parallelism特性。由于用户想了解DeepSpeed目前是否支持这些特性，以及未来是否会支持它们。,https://github.com/deepspeedai/DeepSpeed/issues/3935
DeepSpeed,这是一个用户提出需求的类型。该问题涉及的主要对象是DeepSpeed，用户希望添加flash attention作为deepspeed操作，以便开发人员在自己的设备上实现fmha。,https://github.com/deepspeedai/DeepSpeed/issues/3931
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed的部署，用户想要在k8s上部署DeepSpeed，寻求是否有yaml模板可用。,https://github.com/deepspeedai/DeepSpeed/issues/3922
DeepSpeed,这个issue属于多个修复和优化的集合，涉及DeepSpeed的CPU功能，原因是为了支持客户，并解决了多个问题。,https://github.com/deepspeedai/DeepSpeed/issues/3920
DeepSpeed,这个issue类型是功能需求，主要涉及DeepSpeed AutoTP推理工作负载中的allreduce性能优化。由于推理场景下allreduce的低延迟需求，因此用户提出了实现低延迟allreduce的需求。,https://github.com/deepspeedai/DeepSpeed/issues/3919
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed在CPU系统中支持HBM flatmode和fakenuma模式。这个需求来自于需要更好地利用HBM内存带宽和正确地在这些系统上运行DeepSpeed。,https://github.com/deepspeedai/DeepSpeed/issues/3918
DeepSpeed,这是一个需求类型的issue，涉及的主要对象是DeepSpeed库中的多个文件。这个问题主要是为了简化代码结构，提高代码可读性。,https://github.com/deepspeedai/DeepSpeed/issues/3912
DeepSpeed,这个issue类型为功能需求，主要涉及DeepSpeed在苹果芯片GPU加速上的支持，由于无法在M1芯片上安装DeepSpeed和M1 Max支持问题而产生。,https://github.com/deepspeedai/DeepSpeed/issues/3907
DeepSpeed,该issue是一个用户提出的需求，主要涉及DeepSpeed中的超时设置问题，用户希望能够通过命令行参数或环境变量来设置超时时间。,https://github.com/deepspeedai/DeepSpeed/issues/3905
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed项目中的Pydantic v2支持。,https://github.com/deepspeedai/DeepSpeed/issues/3902
DeepSpeed,这个issue类型属于用户提出需求，主要涉及DeepSpeed下的DS inference对Megatron MoE GPT的支持问题，用户询问是否有特定分支支持这一功能导致的问题。,https://github.com/deepspeedai/DeepSpeed/issues/3897
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed项目的工作流更新问题，希望将CI触发器添加到工作流程中。,https://github.com/deepspeedai/DeepSpeed/issues/3892
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed，用户希望知道如何在一个节点上选择使用特定的GPU。,https://github.com/deepspeedai/DeepSpeed/issues/3891
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中HE-Lora测试的扩展和Z3支持。,https://github.com/deepspeedai/DeepSpeed/issues/3883
DeepSpeed,该issue类型为更新依赖库版本，主要涉及的对象为CI系统。由于之前固定了`pytorchlightning`的版本导致CI问题，现需要解除固定以便测试最新版本。,https://github.com/deepspeedai/DeepSpeed/issues/3882
DeepSpeed,这个issue类型是文档更新，主要对象是MMEngine，由于要展示MMEngine已经集成了DeepSpeed，需要更新文档。,https://github.com/deepspeedai/DeepSpeed/issues/3879
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed和Triton的兼容性问题，由于DeepSpeed 0.10.0不兼容Triton 1.0.0导致需求兼容性解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/3863
DeepSpeed,这个issue属于需求提出类型，主要涉及DeepSpeed中的 MPT 模块，用户希望为 MPT 启用 autoTP。,https://github.com/deepspeedai/DeepSpeed/issues/3861
DeepSpeed,这是一个关于优化单元测试时间的issue，主要涉及DeepSpeed项目中的测试环境和参数设置，通过优化共享环境变量以减少重复代码和实现一致的测试环境。,https://github.com/deepspeedai/DeepSpeed/issues/3850
DeepSpeed,该issue类型为需求报告，涉及到DeepSpeed中的测试流程在YML文件中禁用AMD相关测试流程。,https://github.com/deepspeedai/DeepSpeed/issues/3847
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中GPTBigCode模型的`injection_policy`应用问题，由于`self.kv_dim=128`没有被正确划分，导致了RuntimeError错误。,https://github.com/deepspeedai/DeepSpeed/issues/3845
DeepSpeed,这是一个用户提出需求的 issue，主要涉及 DeepSpeed 的 pretrain_zeropp_gpt.py 文件，并询问其具体位置。用户可能因找不到此文件而请求帮助。,https://github.com/deepspeedai/DeepSpeed/issues/3840
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在确定给定计算预算情况下可训练的最大模型规模以及如何有效减少GPU内存使用的问题。原因是用户发现当前使用DeepSpeed的方法是基于试错，导致效率低下。,https://github.com/deepspeedai/DeepSpeed/issues/3839
DeepSpeed,这个issue是一个优化需求，主要涉及DeepSpeed下的单元测试优化。原因是为了减少单元测试的执行时间以提高开发效率。,https://github.com/deepspeedai/DeepSpeed/issues/3838
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed Zero模型的权重保存格式转换问题。用户询问是否有一种更快捷的方式将模型权重保存为用于DeepSpeed推理的meta tensor格式。,https://github.com/deepspeedai/DeepSpeed/issues/3835
DeepSpeed,这是一个需求提出的issue，主要涉及的对象是DeepSpeed库中的NPU加速器支持。原因是为了增加对Ascend NPU加速器的支持，提供了新的加速器选项'npu'并对自动检测代码进行了优化。,https://github.com/deepspeedai/DeepSpeed/issues/3831
DeepSpeed,该issue类型是用户提出需求，主要涉及到在DeepSpeed阶段3模型中如何修改权重。用户提出需要在训练过程中每N步修改权重，但希望在优化过程之外进行操作。,https://github.com/deepspeedai/DeepSpeed/issues/3830
DeepSpeed,这个issue类型是关于性能优化的需求，主要涉及减少单元测试的运行时间和资源消耗。,https://github.com/deepspeedai/DeepSpeed/issues/3829
DeepSpeed,该issue类型属于用户提出需求，询问如何在DeepSpeed中实现对HuggingFace模型（例如GPTJ、LLaMA）进行3D并行处理，主要涉及3D并行处理以及DeepSpeed库的使用。,https://github.com/deepspeedai/DeepSpeed/issues/3826
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的模型权重存储问题，由于保存了优化器状态导致了不必要的内存消耗。,https://github.com/deepspeedai/DeepSpeed/issues/3820
DeepSpeed,这是一个用户提出需求的Issue，主要涉及DeepSpeed推理模型的支持情况，请求添加对MPT和Falcon推理的支持。原因是目前不清楚DeepSpeed支持哪些模型，用户希望有清晰的文档或工具列出支持的模型，并希望DeepSpeed能支持当前最优秀的LLM模型MPT和Falcon。,https://github.com/deepspeedai/DeepSpeed/issues/3818
DeepSpeed,这是一个需求提出类型的issue，主要涉及DeepSpeed对huggingface GPT BigCode模型的支持问题。,https://github.com/deepspeedai/DeepSpeed/issues/3811
DeepSpeed,这是一个文档更新提示，涉及主要对象为DeepSpeed中的文档。这个变化是由于chrome://tracing功能被弃用，默认将重定向到https://ui.perfetto.dev，原因是chrome://tracing功能被弃用导致链接重定向。,https://github.com/deepspeedai/DeepSpeed/issues/3805
DeepSpeed,该问题类型为功能需求，主要涉及对象是增加日文博客翻译，由于用户需要增加ZeRO++博客的日文翻译。,https://github.com/deepspeedai/DeepSpeed/issues/3797
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed网站导航栏中添加Zero++选项。原因可能是用户希望便捷访问Zero++相关内容。,https://github.com/deepspeedai/DeepSpeed/issues/3796
DeepSpeed,这是一个用户提出需求的 issue，主要涉及 DeepSpeed 的持续集成中关于移除 staging 触发器的问题。,https://github.com/deepspeedai/DeepSpeed/issues/3792
DeepSpeed,这是一个关于DeepSpeed中的ZeRO++功能发布的issue，主要涉及的对象是系统优化策略。,https://github.com/deepspeedai/DeepSpeed/issues/3784
DeepSpeed,这是一个需求类的issue，主要涉及DeepSpeed中的Zero++ tutorial。原因是需要合并以满足微软研究院（MSR）团队的要求。,https://github.com/deepspeedai/DeepSpeed/issues/3783
DeepSpeed,这个issue是一个用户提出需求的类型，主要涉及DeepSpeed推断性能的自定义配置和基准测试，用户想要实现基于不同模型配置的基准测试。,https://github.com/deepspeedai/DeepSpeed/issues/3781
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed engine中增加一个调整微批次大小的API。这个需求是由用户在训练LLM时希望在不重启训练的情况下能够方便地调整微批次大小所引发的。,https://github.com/deepspeedai/DeepSpeed/issues/3773
DeepSpeed,该issue是关于代码优化的建议，主要涉及DeepSpeed中的一个类`DSVAE`，建议去除不必要的`CUDAGraph`继承，并使用`torch.no_grad()`来优化代码性能。,https://github.com/deepspeedai/DeepSpeed/issues/3761
DeepSpeed,这是一个关于需求的问题，主要对象是DeepSpeed中的`PipeModelDataParallelTopology` API，用户想了解该API是否支持训练过程中的模型并行。,https://github.com/deepspeedai/DeepSpeed/issues/3757
DeepSpeed,这个issue类型是用户提出需求，添加中国知乎社交账户，但内容为空。,https://github.com/deepspeedai/DeepSpeed/issues/3755
DeepSpeed,这个issue类型是一个需求提议，主要涉及DeepSpeed项目中的unit tests和status badge。,https://github.com/deepspeedai/DeepSpeed/issues/3754
DeepSpeed,该issue属于功能需求提议，主要涉及将Triton集成到DeepSpeed中以提升BERT-like模型的推理速度，提供了1.14~1.68倍不同模型和GPU的速度提升。,https://github.com/deepspeedai/DeepSpeed/issues/3748
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是安装`apex`，导致症状是无法使用PEP517版本的`apex`。,https://github.com/deepspeedai/DeepSpeed/issues/3745
DeepSpeed,这个issue是一个用户需求类型，主要涉及DeepSpeed下的inference启动脚本问题，用户想要得到65B MP=8的启动脚本。,https://github.com/deepspeedai/DeepSpeed/issues/3744
DeepSpeed,这是一个用户提出需求的issue，涉及到DeepSpeed中Pipeline Parallelism的多输入处理，用户希望支持字典形式的输入输出，提高灵活性。,https://github.com/deepspeedai/DeepSpeed/issues/3738
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed Zero3阶段的梯度操作，用户希望有一种类似于GatheredParameters上下文的方法来聚合梯度、更新梯度并重新分区，然后再将其移动到优化器步骤函数中。,https://github.com/deepspeedai/DeepSpeed/issues/3732
DeepSpeed,这是一个用户提出需求的issue，该问题涉及的主要对象是DeepSpeed库，用户提出了希望DeepSpeed支持自动选择不同类型网络卡的功能，以便在异构GPU集群之间进行分布式训练。由于无法建立高速IB网络，用户需要通过TCP/IP（Socket）使用Ethernet进行跨集群通信，并希望通过修改DeepSpeed来实现自动卡检测和配置。,https://github.com/deepspeedai/DeepSpeed/issues/3729
DeepSpeed,这是一个优化建议，旨在移除不必要的工具加载并使用torch的(un)flatten操作。,https://github.com/deepspeedai/DeepSpeed/issues/3728
DeepSpeed,这是一个用户提出需求的问题，主要对象是 DeepSpeed，用户想知道是否支持类似于 PyTorch FSDP 中的混合分片配置。,https://github.com/deepspeedai/DeepSpeed/issues/3721
DeepSpeed,这是一个关于功能支持的问题，主要涉及AutoTP对GPT2模型的支持问题，用户询问为何AutoTP不支持GPT2以及可否添加支持。,https://github.com/deepspeedai/DeepSpeed/issues/3711
DeepSpeed,这是一个用户提出需求的类型，主要关注的对象是将DeepSpeed概述翻译成日语。由于用户需要在日语上获得DeepSpeed的概述信息，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/3709
DeepSpeed,这是一个用户提出需求的类型，主要涉及MoE/ZeRO的支持问题，可能是用户希望在非CG zero-1中添加MoE支持。,https://github.com/deepspeedai/DeepSpeed/issues/3707
DeepSpeed,这是一条关于更新Dockerfile的issue，类型为用户提出需求，主要涉及的对象是DeepSpeed的Dockerfile。由于用户可能发现Dockerfile有需要改进的地方，因此提出了这个issue。,https://github.com/deepspeedai/DeepSpeed/issues/3705
DeepSpeed,这是一个用户提出需求的 issue，主要涉及到在 DeepSpeed 中实现 Mixture of Experts (MoE) 在 Segmentation 任务中的应用。由于现有的 DeepSpeedMoE 只支持 MLP Linear Gate Network，用户想要在 DeepSpeed 中支持 Segmentation 任务。,https://github.com/deepspeedai/DeepSpeed/issues/3701
DeepSpeed,这是一个功能更新类型的issue，主要涉及的对象是DeepSpeed中的asymmetric quant算法。由于用户希望减少最大误差，即牺牲略高平均误差来更新asymmetric quant算法。,https://github.com/deepspeedai/DeepSpeed/issues/3696
DeepSpeed,这个issue是用户提出需求或寻求帮助，主要对象为DeepSpeed。原因可能是缺乏具体的信息导致标题和内容为空。,https://github.com/deepspeedai/DeepSpeed/issues/3692
DeepSpeed,这是一个用户提出需求的issue，主要涉及到在个人PC上训练DeepSpeed模型的问题。由于个人PC资源有限，用户寻求使用OpenMPI或Rocks cluster运行GPU集群，并希望得到有关在OpenMPI上运行DeepSpeed的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/3691
DeepSpeed,这个issue类型为更新请求，主要涉及的对象是DeepSpeed的README文档。原因可能是作者想更新README来添加有关Tensor Parallel MoEs的ICS'23论文。,https://github.com/deepspeedai/DeepSpeed/issues/3687
DeepSpeed,这是一个用户提出的需求，主要对象是DeepSpeed中的模型状态和优化器状态，在加载检查点时希望可以分别加载模型权重和优化器状态。,https://github.com/deepspeedai/DeepSpeed/issues/3661
DeepSpeed,这是一个性能优化和功能添加类型的issue，主要涉及DeepSpeed在推理环境中支持FALCON-40B模型。这个issue是由于DeepSpeed希望为FALCON-40B模型添加推理内核支持，以提高模型推理性能。,https://github.com/deepspeedai/DeepSpeed/issues/3656
DeepSpeed,这个issue类型是改进提案，涉及的主要对象是代码审查流程。由于缺少忽略路径，文档更新不会触发测试，导致CI流程中未对文档修改进行测试。,https://github.com/deepspeedai/DeepSpeed/issues/3651
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的FALCON Auto-TP支持。由于对多个A100 GPU上运行Falcon40B模型的需求，用户请求在DeepSpeedExample仓库中添加支持。,https://github.com/deepspeedai/DeepSpeed/issues/3640
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中的Elastic Training功能，用户询问如何在训练过程中动态添加或移除节点。,https://github.com/deepspeedai/DeepSpeed/issues/3632
DeepSpeed,这个issue类型是用户提出需求，涉及主要对象是DeepSpeed。由于用户需要描述问题和期望的解决方案，说明该需求可能是为了改进DeepSpeed的某些功能或性能问题。,https://github.com/deepspeedai/DeepSpeed/issues/3631
DeepSpeed,这个issue属于用户提出需求类型，主要对象是在CPU内存有限的机器上运行DeepSpeed的用户。这个问题是因为在同一节点的所有rank同时加载优化状态可能导致OOM，所以用户提出希望能够通过管道方式进行加载检查点。,https://github.com/deepspeedai/DeepSpeed/issues/3629
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的quantize_training文档更新问题，由于DeepSpeed版本从v0.7.0开始将量化配置移动至compression_training，导致MoQ教程过时需要更新。,https://github.com/deepspeedai/DeepSpeed/issues/3626
DeepSpeed,该issue类型为用户提出需求，并涉及DeepSpeed在多节点环境下特定GPU分配训练问题，由于hostfile配置与启动脚本参数不匹配导致报错。,https://github.com/deepspeedai/DeepSpeed/issues/3625
DeepSpeed,这是一个用户需求问题，涉及DeepSpeed框架在多节点训练中指定GPU的困难。,https://github.com/deepspeedai/DeepSpeed/issues/3624
DeepSpeed,该issue类型为性能优化报告，主要涉及DeepSpeed下的ZeRO3模块在不同硬件配置下的性能表现。根据结果显示，不同GPU配置和微批量大小对性能表现产生了影响。,https://github.com/deepspeedai/DeepSpeed/issues/3622
DeepSpeed,这是一个用户提出需求的Issue，主要涉及DeepSpeed与Quantized model（例如qlora）训练时的内存占用问题。,https://github.com/deepspeedai/DeepSpeed/issues/3620
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是DeepSpeed的Batch scheduler功能。,https://github.com/deepspeedai/DeepSpeed/issues/3617
DeepSpeed,该issue是一个用户提出需求的类型，主要涉及DeepSpeed下vit model的代码。由于vit model中缺少PP/TP相关内容，用户请求得到一些建议。,https://github.com/deepspeedai/DeepSpeed/issues/3612
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的可调用优化器，问题出现的原因是需要将ds_config中的优化器参数传递给客户端提供的可调用优化器。,https://github.com/deepspeedai/DeepSpeed/issues/3597
DeepSpeed,这是一个需求提出类型的issue，主要涉及的对象是DeepSpeed中的Ascend NPU加速器支持。,https://github.com/deepspeedai/DeepSpeed/issues/3595
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed下的性能分析和优化，用户提出了需要在communication logging中添加显示straggler effect的功能。,https://github.com/deepspeedai/DeepSpeed/issues/3579
DeepSpeed,这是一个需求提出的issue，主要涉及DeepSpeed中的通信日志功能，用户提出了增加straggler effect的功能需求。由于Tensor parallel推断中存在straggler效应，影响了扩展效率。,https://github.com/deepspeedai/DeepSpeed/issues/3578
DeepSpeed,这个issue类型是需求提议，主要涉及DeepSpeed中Profiler模块的性能分析功能增强。原因是为了提供更详细的内存和数据类型分析，以便优化性能。,https://github.com/deepspeedai/DeepSpeed/issues/3576
DeepSpeed,这个issue类型属于用户提出需求，该问题单涉及的主要对象是DeepSpeed，用户需要将特定的pull request添加到DeeperSpeed直到其被合并到上游。,https://github.com/deepspeedai/DeepSpeed/issues/3573
DeepSpeed,这是一个用户提出改进需求的issue，涉及DeepSpeed中的Profiler功能，主要讨论了添加内存详细信息和dtype分解以及修复细小问题。,https://github.com/deepspeedai/DeepSpeed/issues/3572
DeepSpeed,这个issue类型是用户提出需求，请教问题，主要涉及DeepSpeed下的参数和梯度自动分区，可能由于Zero3版本问题导致无法实现预期的参数和梯度分区功能。,https://github.com/deepspeedai/DeepSpeed/issues/3566
DeepSpeed,这是一个用户提出的需求，主要涉及DeepSpeed在使用多GPU进行张量并行推理时可能会出现内存耗尽的问题。提出需要允许使用单个进程对模型进行预分片，以避免重复加载模型到内存中。,https://github.com/deepspeedai/DeepSpeed/issues/3562
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中使用HF进行文本生成时无法直接在训练过程中使用```generate()```方法。原因是在使用```pipeline_engine```进行训练时，当前需要使用```engine.eval_batch()```方法进行评估。,https://github.com/deepspeedai/DeepSpeed/issues/3557
DeepSpeed,这是一个用户提出需求的 issue，主要涉及 DeepSpeed 中的动态损失缩放功能的连续滞后参数。由于在训练中周期性遇到不稳定性或坏样本，用户希望通过将连续滞后功能暴露给用户来改善这一问题。,https://github.com/deepspeedai/DeepSpeed/issues/3553
DeepSpeed,这是一个用户提出需求的类型，主要涉及 DeepSpeed Zero3 训练万亿参数模型的示例或演示的问题。,https://github.com/deepspeedai/DeepSpeed/issues/3538
DeepSpeed,该issue属于功能需求类型，主要涉及DeepSpeed下的SD和SparseAttn更新到最新的triton环境。,https://github.com/deepspeedai/DeepSpeed/issues/3533
DeepSpeed,这个issue类型为用户提出需求，涉及的主要对象是修改GitHub Actions中的`actions/checkout`版本。原因是希望将`actions/checkout`升级到v3版本。,https://github.com/deepspeedai/DeepSpeed/issues/3526
DeepSpeed,这个issue是关于更换GitHub Actions中Node.js版本的需求，涉及到的主要对象是GitHub Actions中的actions/checkout，由于GitHub Actions中的Node.js 12版本已被弃用，需要更新到Node.js 16版本。,https://github.com/deepspeedai/DeepSpeed/issues/3525
DeepSpeed,这是一个文档更新的issue，涉及主要对象是DeepSpeed项目。,https://github.com/deepspeedai/DeepSpeed/issues/3520
DeepSpeed,这是一个需求提出类型的issue，主要涉及DeepSpeed inference initialization API的初始化参数扩展。这个需求是由于希望能够在推理期间控制模型权重的量化组数，以提供更灵活的模型量化选项。,https://github.com/deepspeedai/DeepSpeed/issues/3519
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及DeepSpeed和Megatron的结合问题。该问题源于用户想要了解如何将DeepSpeed与Megatron结合使用。,https://github.com/deepspeedai/DeepSpeed/issues/3502
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中Inference Engine类的checkpoint加载功能。,https://github.com/deepspeedai/DeepSpeed/issues/3498
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed中的商业使用问题。用户询问是否能在商业环境下使用默认步骤构建自己的模型，以及如何训练自己的模型。,https://github.com/deepspeedai/DeepSpeed/issues/3496
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed，由于主管不打算在服务器上安装Linux，用户希望添加对Windows的支持。,https://github.com/deepspeedai/DeepSpeed/issues/3493
DeepSpeed,这是一个用户提出需求的问题，用户希望DeepSpeed能够支持CosineAnnealingLR scheduler，由于当前DeepSpeed不支持该调度器，用户希望得到关于如何开发自定义调度器的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/3492
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的分布式训练模式问题。由于缺乏关于分区数量的配置，用户在大规模集群上使用单个zero3时性能下降，可能是由于额外的网络开销导致。,https://github.com/deepspeedai/DeepSpeed/issues/3485
DeepSpeed,该issue是一个用户提出需求的类型，主要对象是DeepSpeed中的tensor parallelism和pipeline parallelism的结合使用问题。用户希望能够得到关于如何结合这两种并行方式的示例，以帮助理解和应用。,https://github.com/deepspeedai/DeepSpeed/issues/3468
DeepSpeed,这是一个用户需求提问类型的issue，主要涉及DeepSpeed中的RLHF训练模型。用户因为不清楚训练步骤和模型参数的确定方式，所以询问如何确认Step3 RLHF训练是否正常。,https://github.com/deepspeedai/DeepSpeed/issues/3465
DeepSpeed,这是一个用户提出需求的类型。该问题涉及的主要对象是DeepSpeed。,https://github.com/deepspeedai/DeepSpeed/issues/3455
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及DeepSpeed的支持是否能够添加自定义操作符，询问是否有现有示例。这可能是由用户希望通过添加在CUDA中编写的自定义操作符来加速训练的需求所导致的。,https://github.com/deepspeedai/DeepSpeed/issues/3450
DeepSpeed,这是一个用户提出需求的issue，主要关注是否需要在DeepSpeedPPOTrainer中使用critic模型，可能是由于模型中的critic_model在训练过程中没有被使用而引发的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/3446
DeepSpeed,这个issue属于用户提出需求，涉及到Dockerfile的构建过程。该问题原因是缺少相应的GPG密钥导致构建过程出现了错误。,https://github.com/deepspeedai/DeepSpeed/issues/3439
DeepSpeed,这是一个用户提出的需求，请求在 `zero_to_fp32.py` 中添加一个命令行参数来仅合并可训练参数。由于使用 LoRA 训练会使合并模型变得与完整模型一样大，因此提出了这个解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/3437
DeepSpeed,这是一个用户提出需求的 issue，主要涉及的对象是 DeepSpeed 的 FusedAdam 模块。由于 Apex 添加了 bf16 功能，要求将 DeepSpeed 的 FusedAdam 同步支持 bf16。,https://github.com/deepspeedai/DeepSpeed/issues/3434
DeepSpeed,这是一个需求用户需要帮助的issue，主要涉及到在使用DeepSpeed进行多节点训练时缺乏相关文档的问题。,https://github.com/deepspeedai/DeepSpeed/issues/3419
DeepSpeed,这是一个文档更新（Documentation Update）类型的issue，主要涉及DeepSpeed文档的更新。,https://github.com/deepspeedai/DeepSpeed/issues/3415
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的ZeRO优化器系统。,https://github.com/deepspeedai/DeepSpeed/issues/3401
DeepSpeed,这个issue类型为功能需求提议，主要涉及对象是DeepSpeed中的FunnelTransformer和TransformerXL模型，用户提出需要正确的attention output/dense Gemm实现。,https://github.com/deepspeedai/DeepSpeed/issues/3398
DeepSpeed,这个issue是用户提出需求。该问题单涉及的主要对象是DeepSpeed项目。原因是用户想要回退到之前通过git安装的较旧版本的megatron，并寻求帮助进行这个操作。,https://github.com/deepspeedai/DeepSpeed/issues/3397
DeepSpeed,这是一个关于用户需求的issue，主要涉及DeepSpeed是否使用cuda unified memory技术以及其API位置的问题。用户想了解DeepSpeed是否使用此技术以及如何调用其API。,https://github.com/deepspeedai/DeepSpeed/issues/3393
DeepSpeed,"这个issue属于用户提出需求类型，主要对象是DeepSpeedChat的""Inference""功能。用户提出了关于""Inference""在DeepSpeedChat中的理解疑问，希望澄清其在训练过程中的作用和效率提升范围。",https://github.com/deepspeedai/DeepSpeed/issues/3392
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed项目下的Megatron-lm安装方式问题，请求将安装方式由pip改为git。,https://github.com/deepspeedai/DeepSpeed/issues/3390
DeepSpeed,这个issue是与功能变更相关的问题，涉及到DeepSpeed项目中的Megatron-LM模块，通过移除该模块来更改依赖项和安装方式。,https://github.com/deepspeedai/DeepSpeed/issues/3389
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的参数传递问题，原因是参数应该在`main.py`之后添加导致Launcher无法识别。,https://github.com/deepspeedai/DeepSpeed/issues/3381
DeepSpeed,这是一个用户提出需求的问题，关于如何在DeepSpeed中使用torch.compile功能引发的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/3375
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed项目。造成这个问题的原因可能是用户希望添加DeepSpeed的聊天博客链接以及标签。,https://github.com/deepspeedai/DeepSpeed/issues/3369
DeepSpeed,这个issue类型是用户提需求，需更新DS-Chat的问题模板，涉及主要对象是DS-Chat。,https://github.com/deepspeedai/DeepSpeed/issues/3368
DeepSpeed,该issue类型为用户提出需求，主要对象是预训练模型的中文能力。由于用户尝试了opt1.3b的训练过程，但发现英语能力不如意，出现理解困难的症状，希望能够增加中文预训练模型。,https://github.com/deepspeedai/DeepSpeed/issues/3365
DeepSpeed,这是一个用户需求问题，主要涉及的对象是DeepSpeed中的flops_profiler模块。这个问题是由于激活检查点启用时，大部分前向计算会被重新计算，导致FLOPS计算需要更新而提出的需求。,https://github.com/deepspeedai/DeepSpeed/issues/3362
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的推断支持PTQ的问题，用户询问是否有针对PTQ模型的推断功能。,https://github.com/deepspeedai/DeepSpeed/issues/3359
DeepSpeed,这是一个用户提出需求的问题，主要涉及的对象是使用DeepSpeed中的单节点脚本，用户想知道该脚本默认使用多少个GPU来调整训练批量大小。,https://github.com/deepspeedai/DeepSpeed/issues/3351
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed工具的帮助菜单中缺少默认启动器数值的问题。,https://github.com/deepspeedai/DeepSpeed/issues/3347
DeepSpeed,这是一个需求变更类型的issue，涉及主要对象为DeepSpeed的预编译测试，由于需要将测试更名并加入支持AMD/ROCm，可能是为了增加对非NVIDIA GPU的支持。,https://github.com/deepspeedai/DeepSpeed/issues/3346
DeepSpeed,这是一个用户提出需求的类型issue，主要对象是更新DeepSpeed与MosaicML集成文档链接。,https://github.com/deepspeedai/DeepSpeed/issues/3344
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的zero stage2 all_gather功能。由于bucketing导致all_gather变慢且消耗更多GPU内存，用户请求禁用这一特性。,https://github.com/deepspeedai/DeepSpeed/issues/3338
DeepSpeed,这个issue是用户提出需求类型的，主要涉及到DeepSpeedChat中如何微调和推理自定义数据的问题。由于缺乏相关文档指导，用户提出了关于数据存放、模型加载和推理的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/3331
DeepSpeed,这是一个功能需求，要求在帮助菜单中提及默认启动器值。由于用户建议缺少启动器默认值的信息，因此提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/3326
DeepSpeed,这是一个功能需求类型的issue，主要涉及的对象是 ROCm，问题是为了在 ROCm 上启用 cooperative groups 函数而提出的。,https://github.com/deepspeedai/DeepSpeed/issues/3323
DeepSpeed,该issue为用户提出需求类型，主要涉及DeepSpeed和数据库的整合以实现数据统计功能，由于用户拥有有限的计算资源，询问需要准备的语料和预训练模型规格。,https://github.com/deepspeedai/DeepSpeed/issues/3322
DeepSpeed,该问题是用户提出需求，希望加入精准识图功能，主要对象是DeepSpeed项目。由于目前缺乏AI理解设计图中批注和加入自己批注的功能，用户认为这功能在工程设计领域非常重要，并期望实现这一特性。,https://github.com/deepspeedai/DeepSpeed/issues/3317
DeepSpeed,这是一个用户提出需求的issue，主要涉及GitHub上的图片缓存机制，因为最后修改链接后未添加随机数，导致贡献者的图片无法实时刷新。,https://github.com/deepspeedai/DeepSpeed/issues/3315
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中如何在模型训练过程中访问和操作梯度的问题。,https://github.com/deepspeedai/DeepSpeed/issues/3310
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed compression以实现`snip_momentum` structured pruning，并测试结果显示性能有所提升。,https://github.com/deepspeedai/DeepSpeed/issues/3300
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中PipelineModule加载预训练权重的方法，用户疑惑是否采用特定方法加载会导致信息丢失。,https://github.com/deepspeedai/DeepSpeed/issues/3297
DeepSpeed,这是一个更新类型的issue，涉及主要的对象是DeepSpeed库中的PyDantic模块。目前出现需要处理PyDantic升级到v2版本的变动，需要对应相应的更新来支持这些破坏性变化。,https://github.com/deepspeedai/DeepSpeed/issues/3289
DeepSpeed,这个issue类型是用户提出需求，希望在DeepSpeed的README.md文件中加入贡献者的头像，以表彰他们的贡献。,https://github.com/deepspeedai/DeepSpeed/issues/3282
DeepSpeed,这是一个用户提出需求的问题，用户想要了解如何在没有网络连接的情况下从huggingface本地加载预训练权重模型。,https://github.com/deepspeedai/DeepSpeed/issues/3281
DeepSpeed,该issue类型为测试需求，主要涉及DeepSpeed ops的验证。由于未来的更改可能导致功能故障，需要添加一个测试来确保DeepSpeed ops的构建功能正常。,https://github.com/deepspeedai/DeepSpeed/issues/3277
DeepSpeed,这个issue类型是用户提出需求，主要对象是DeepSpeed训练过程中的缺乏进度条显示。用户提出希望在训练阶段添加进度条，以便清楚了解训练何时完成。,https://github.com/deepspeedai/DeepSpeed/issues/3273
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及的对象是DeepSpeed推理功能，用户在询问是否未来的版本中DeepSpeed推理会支持管道并行化，因为目前仅支持张量并行化。,https://github.com/deepspeedai/DeepSpeed/issues/3271
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed或DeepSpeedchat是否添加一个名为RRHF的功能。这是因为提出者希望减少GPU内存消耗以支持更大的LLMs。,https://github.com/deepspeedai/DeepSpeed/issues/3267
DeepSpeed,这个issue类型是用户提出需求，主要涉及到如何在位置好、商业业态成熟、人流量少的楼盘进行销售。这个问题提出者可能在寻求有关营销策略或销售技巧方面的建议或帮助。,https://github.com/deepspeedai/DeepSpeed/issues/3265
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中对vicuna模型进行微调或嵌入的步骤指南。原因是用户需要更详细的文档来指导如何使用DeepSpeed进行此任务。,https://github.com/deepspeedai/DeepSpeed/issues/3264
DeepSpeed,这个issue为用户提出需求，主要涉及DeepSpeed Chat框架的安装和使用困难。,https://github.com/deepspeedai/DeepSpeed/issues/3252
DeepSpeed,这是一个用户提出需求的issue，主要涉及在DeepSpeed项目中的中文README.md文件的格式和样式问题。原因可能是当前的中文README.md内容排版不符合Markdown标准，希望改进为有序列表、无序列表与代码块的格式。,https://github.com/deepspeedai/DeepSpeed/issues/3243
DeepSpeed,该issue是一个文档更新请求，涉及的主要对象是DeepSpeed项目的PL（Process Language）文档链接。,https://github.com/deepspeedai/DeepSpeed/issues/3237
DeepSpeed,这个issue属于用户提出需求的类型，主要对象是DeepSpeed项目的文档。由于文档结构混乱，缺乏逻辑性，用户认为发布的内容不完善，希望开发者花更多时间提高项目的可用性。,https://github.com/deepspeedai/DeepSpeed/issues/3220
DeepSpeed,这是一个涉及文档更新的issue，主要涉及DeepSpeed-Chat脚本的修改。,https://github.com/deepspeedai/DeepSpeed/issues/3219
DeepSpeed,该issue属于文档更新类型，涉及到DeepSpeed的DS-Chat文档，用户提出了更新此文档至v0.9.0版本的需求。这可能是因为DeepSpeed进行了更新和改进，用户希望文档也能及时更新以反映这些变化。,https://github.com/deepspeedai/DeepSpeed/issues/3216
DeepSpeed,这是一个文档更新类型的issue，涉及到DeepSpeedExamples中的CC（更新bert images）。这个问题是由于DeepSpeedExamples中的变更导致cifar10.md文档引用需要更新而产生的。,https://github.com/deepspeedai/DeepSpeed/issues/3212
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed这个项目。由于标题的描述不清晰，可能导致用户寻求帮助或者提出了关于病句的问题。,https://github.com/deepspeedai/DeepSpeed/issues/3206
DeepSpeed,这是一个功能需求的issue，主要涉及对象是DeepSpeed库。由于MI200的特性需要额外的改动来支持，用户提出了对DeepSpeed进行额外修改的建议。,https://github.com/deepspeedai/DeepSpeed/issues/3204
DeepSpeed,该issue类型为功能建议，主要涉及DeepSpeed中不支持的模型，并寻求移除这些模型的建议。,https://github.com/deepspeedai/DeepSpeed/issues/3198
DeepSpeed,这是一个用户提出需求的类型，主要涉及添加日语版本的ChatGPT-like pipeline博客。由于目前仅有英文版本，用户提出添加日语版本的需求。,https://github.com/deepspeedai/DeepSpeed/issues/3194
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及Chatgpt中文博客，用户希望增加中文博客的功能或内容。,https://github.com/deepspeedai/DeepSpeed/issues/3193
DeepSpeed,这个issue类型是用户提出需求，主要涉及对象是DeepSpeed中的model containers，提出了需要为GPTJ、GPTNeo和GPTNeox添加HE支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/3191
DeepSpeed,这是一个需求性质的问题，主要涉及int4 asymmetric quantization的精度改进。,https://github.com/deepspeedai/DeepSpeed/issues/3190
DeepSpeed,这是一个需求类型的 issue，主要涉及到文档的更新。由于缺少具体的描述内容，导致无法准确推断用户需求的具体内容。,https://github.com/deepspeedai/DeepSpeed/issues/3187
DeepSpeed,这是一个需求类型的issue，主要对象是DeepSpeed Chat，用户提出了关于RLHF训练ChatGPT模型的需求。,https://github.com/deepspeedai/DeepSpeed/issues/3186
DeepSpeed,这是一个用户需求类型的issue，涉及主要对象是向DeepSpeed项目中添加发布博客文章。,https://github.com/deepspeedai/DeepSpeed/issues/3185
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是DeepSpeed下的AMD工作流。由于需要增加对AMD MI200的单元测试流程，因此提出更新AMD工作流程的需求。,https://github.com/deepspeedai/DeepSpeed/issues/3179
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的多头注意力机制（multi-head attention），用户询问是否DeepSpeed推理支持多头注意力机制的操作融合（op fusion）。,https://github.com/deepspeedai/DeepSpeed/issues/3172
DeepSpeed,这个issue类型是功能需求提议，主要涉及到DeepSpeed下的llama模型，由于auto TP policy未被正确设置导致模型输出存在问题。,https://github.com/deepspeedai/DeepSpeed/issues/3170
DeepSpeed,这是一个功能改进（Enhancement）类型的issue，主要涉及DeepSpeed中的数据分区方法`partition_uniform`，用户希望让其更加均匀。,https://github.com/deepspeedai/DeepSpeed/issues/3159
DeepSpeed,这个issue是一个需求提出，请求在DeepSpeed的Monitor模块中添加Aim，主要对象是Aim实验追踪工具。,https://github.com/deepspeedai/DeepSpeed/issues/3158
DeepSpeed,这是一个需求变更的issue，涉及的主要对象是DeepSpeed benchmarks代码。原因可能是将benchmark代码移动到DeepSpeedExample repo，需要移除已经复制过去的部分并添加相应的README文件。,https://github.com/deepspeedai/DeepSpeed/issues/3157
DeepSpeed,这个issue是一个请求类型的issue，涉及DeepSpeed中的collective操作在使用torchdynamo时无法被trace，提出了一种短期解决方案使得graph breaks更加干净和可预测。,https://github.com/deepspeedai/DeepSpeed/issues/3150
DeepSpeed,这是一个用户提出需求的类型，主要涉及的对象是DeepSpeed下的megatron-lm模块。该问题由于torch 2版本更新导致功能不兼容问题，用户希望更新megatron-lm以支持新版本的torch 2。,https://github.com/deepspeedai/DeepSpeed/issues/3148
DeepSpeed,这个issue类型为功能增强，主要涉及DeepSpeed中Diffusers 0.13.0和0.14.0的支持以及一些函数的更新和配置属性的添加。,https://github.com/deepspeedai/DeepSpeed/issues/3142
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed中推理不支持BF16，用户想知道是否可以通过重用所有FP16 kernels来支持BF16，以及是否有相关计划。,https://github.com/deepspeedai/DeepSpeed/issues/3139
DeepSpeed,这个issue类型是更新需求，主要涉及Stable Diffusion Triton版本更新问题，由于之前版本不再托管在pypi.org导致的bug。,https://github.com/deepspeedai/DeepSpeed/issues/3135
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed项目中的功能请求。这个问题由于用户遇到某些问题或需求某种功能而提出。,https://github.com/deepspeedai/DeepSpeed/issues/3134
DeepSpeed,这是一个请求更新AMD工作流程中使用的ROCm版本的问题，属于需求更改类型，主要涉及DeepSpeed项目中与AMD协作相关的部分。,https://github.com/deepspeedai/DeepSpeed/issues/3133
DeepSpeed,这是一个用户提出需求的issue，主要涉及创建一个用于Tensor Map的cpp PYBIND模块，包括针对性能测试的额外更改。,https://github.com/deepspeedai/DeepSpeed/issues/3129
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中使用pipeline方法时如何启用fp16功能。由于设置了fp16参数，但在运行`engine.train_batch()`时出现了错误。,https://github.com/deepspeedai/DeepSpeed/issues/3128
DeepSpeed,这是一个需求反馈类型的issue，主要涉及的对象是VL MoE系统。由于用户希望在DeepSpeed中实现VL MoE功能，因此提出了相关需求。,https://github.com/deepspeedai/DeepSpeed/issues/3120
DeepSpeed,这是一个用户需求提出的issue，主要涉及DeepSpeed集成Smoothquant这一优化操作，由于Smoothquant能够更快地进行整数量化推理，用户希望DeepSpeed能够实现该优化并实现更快的推理速度。,https://github.com/deepspeedai/DeepSpeed/issues/3118
DeepSpeed,这是一个性能优化相关的issue，主要涉及到DeepSpeed中Softmax Kernel的优化问题。,https://github.com/deepspeedai/DeepSpeed/issues/3112
DeepSpeed,这个issue是关于更新DeepSpeed版权许可证到Apache 2.0，而不是Bug报告。,https://github.com/deepspeedai/DeepSpeed/issues/3111
DeepSpeed,这是一个用户提出需求的问题，主要涉及的对象是DeepSpeed中的Tensor Parallelism模块。用户希望了解为什么AutoTP不支持CodeGen模型，并希望找到解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/3106
DeepSpeed,这是一个需求类型的issue， 主要对象是DeepSpeed中的AutoTP路径。原因是为了在初始化阶段降低内存峰值，需要添加分片的checkpoint加载功能。,https://github.com/deepspeedai/DeepSpeed/issues/3102
DeepSpeed,这是一个用户提出需求的类型，主要涉及的对象是 DeepSpeed 下的 yapf 版本和格式设置。由于使用的 yapf 镜像不再维护，希望更新到官方版本并修改格式设置以改善代码可读性。,https://github.com/deepspeedai/DeepSpeed/issues/3098
DeepSpeed,这是一个需求提出的issue，主要涉及到DeepSpeed下的nv-transformers-v100和HF Transformers间使用不同版本的PyTorch导致的持续性问题。,https://github.com/deepspeedai/DeepSpeed/issues/3096
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed无法同时运行多个模型的问题。因为DeepSpeed似乎无法很好地支持同时运行多个模型，导致CUDA内存不足错误，用户希望了解是否有功能能够支持这种情况，如能否指定每个模型使用的GPU来避免GPU内存分配问题。,https://github.com/deepspeedai/DeepSpeed/issues/3093
DeepSpeed,这是一个功能需求类型的issue，主要涉及添加bf16 cuda kernel支持，由于需要进行cuda和cpu之间的比较，以及将现有的fp16实现与fp32版本结合，故提出此问题。,https://github.com/deepspeedai/DeepSpeed/issues/3092
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及的对象是DeepSpeed中的Trainer，用户想要了解如何在该Trainer中对compute_loss函数进行自定义行为的重写。,https://github.com/deepspeedai/DeepSpeed/issues/3079
DeepSpeed,该问题类型是用户提出需求，希望如何将DeepSpeed生成的信息重定向到本地文件，用户希望将DeepSpeed的输出保存到`log.txt`文件中。,https://github.com/deepspeedai/DeepSpeed/issues/3075
DeepSpeed,这是一个用户需求类型的issue，主要对象是DeepSpeed库中的cuda检查，由于现有skip检查分布在代码中的多个地方，用户建议将这些检查移动到utils工具函数中。,https://github.com/deepspeedai/DeepSpeed/issues/3074
DeepSpeed,这是一个功能改进的issue，主要涉及DeepSpeed中的CI流程和稀疏梯度测试。,https://github.com/deepspeedai/DeepSpeed/issues/3072
DeepSpeed,这是一个需求类型的issue，主要涉及要将MCR-DL论文添加到DeepSpeed的README和文档中。可能由于想要让用户了解最新的研究进展，所以提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/3066
DeepSpeed,这是一个用户提出需求的类型。该问题单涉及的主要对象为DeepSpeed，并询问是否可以在C++中运行推理。由于用户可能想要在C++中集成DeepSpeed并进行推理操作，因此向社区寻求关于在C++中运行推理的方法。,https://github.com/deepspeedai/DeepSpeed/issues/3065
DeepSpeed,这是一个关于功能需求或技术问题的issue，主要涉及DeepSpeed是否使用GPUDirect技术实现NVMe的offload。询问如何验证DeepSpeed是否使用GPUDirect以及是否支持NVMe offload功能。,https://github.com/deepspeedai/DeepSpeed/issues/3063
DeepSpeed,这是一个用户提出需求的issue，主要关注在将PowerSGD集成到DeepSpeed中，原因是在网络带宽有限时，PowerSGD能够提供帮助。,https://github.com/deepspeedai/DeepSpeed/issues/3061
DeepSpeed,该issue是一则关于技术需求的问题，涉及主要对象为使用DeepSpeed训练6B模型时的资源配置，用户寻求如何在4个A40（49G）和100G CPU内存下进行训练。,https://github.com/deepspeedai/DeepSpeed/issues/3059
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的accelerator模块。由于用户无法理解accelerator模块设计意图，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/3057
DeepSpeed,这个issue类型为优化提案，主要涉及到DeepSpeed的tp_parser代码，原因是存在冗余代码需要清理。,https://github.com/deepspeedai/DeepSpeed/issues/3049
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的PipelineModule和eval_batch功能。由于eval dataloader的batch_size与train_micro_batch_size_per_gpu设置不一致，导致输出tensor在不同阶段形状变化引发不确定结果。,https://github.com/deepspeedai/DeepSpeed/issues/3045
DeepSpeed,这是一个与软件库更新相关的需求问题，涉及的主要对象是GitHub workflows。更新是由于Torch升级至2.0.0导致的，需要更新以允许测试运行。,https://github.com/deepspeedai/DeepSpeed/issues/3039
DeepSpeed,该问题类型为功能新增，主要涉及对象是DeepSpeed中的参数分片机制。原因是用户想要添加对`NamedTuple`输出的支持，并扩展测试以覆盖未经测试的输出类型。,https://github.com/deepspeedai/DeepSpeed/issues/3037
DeepSpeed,这个issue是关于更新文档内容的问题，涉及主要对象是curriculum-learning.md。,https://github.com/deepspeedai/DeepSpeed/issues/3031
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的`logger`模块，用户提出希望实现`logger.warning_once`功能，以替代现有的一些不规范做法。,https://github.com/deepspeedai/DeepSpeed/issues/3021
DeepSpeed,这是一个需求类型的issue，主要涉及文档的更新。,https://github.com/deepspeedai/DeepSpeed/issues/3018
DeepSpeed,这个issue类型为需求提出，主要涉及对象是DeepSpeed中的模型参数。由于需要将生成器模型的参数转换为列表，可能是为了实现特定功能或满足特定需求。,https://github.com/deepspeedai/DeepSpeed/issues/3017
DeepSpeed,这个issue类型是需求提出，主要对象是DeepSpeed的checkpointing code。由于GPTNeoX模型的高级特性需要与DeepSpeed的checkpointing代码兼容，因此提出了对代码进行调整以支持Universal Checkpoints的功能需求。,https://github.com/deepspeedai/DeepSpeed/issues/3015
DeepSpeed,这是一个功能需求报告，涉及DeepSpeed库中的CheckpointEngine，添加了open()和close()调用用于在读取checkpoint时标记起始和结束位置，以帮助checkpoint engine准备和释放资源。,https://github.com/deepspeedai/DeepSpeed/issues/3009
DeepSpeed,这个issue类型是需求，主要涉及的对象是需要将DeepSpeed中的FusedAdam与NVIDIA/apex中的版本进行同步，由于DeepSpeed在fork后未同步最新修复和BF16支持，导致需要对代码进行更新。,https://github.com/deepspeedai/DeepSpeed/issues/3006
DeepSpeed,这个issue是一个用户提出需求类型的反馈，涉及的主要对象是DeepSpeed下的Fairseq Transformer LM模型。由于缺乏对Fairseq Transformer LM模型使用DeepSpeed推理引擎的支持，导致用户在加载DeepSpeed训练的检查点时遇到错误。,https://github.com/deepspeedai/DeepSpeed/issues/3001
DeepSpeed,这是一个关于减少内存需求的请求，该用户正在尝试在8x48GB GPU机器上训练一个13B模型，但遇到OOM错误。,https://github.com/deepspeedai/DeepSpeed/issues/2996
DeepSpeed,这是一个功能改进的issue，主要涉及深度学习库DeepSpeed中通信函数的替换操作。造成这个问题的原因可能是为了使用新的通信函数以取代已弃用的函数。,https://github.com/deepspeedai/DeepSpeed/issues/2995
DeepSpeed,此issue属于用户提出需求类型，主要对象是DeepSpeed的checkpoint engine。创建checkpoint目录的任务委托到checkpoint engine，以满足不同创建目录需求。,https://github.com/deepspeedai/DeepSpeed/issues/2988
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的InferenceEngine，原因是希望将local CUDA图搜索泛化到更通用的情形。,https://github.com/deepspeedai/DeepSpeed/issues/2987
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中CUDA特定代码的代码检查，目的是避免在新代码中意外使用CUDA代码。,https://github.com/deepspeedai/DeepSpeed/issues/2981
DeepSpeed,这是一个用户提出需求的Issue，涉及DeepSpeed中的CUDA_VISIBLE_DEVICES在单节点训练时被重置，导致用户无法指定GPU ID训练模型。,https://github.com/deepspeedai/DeepSpeed/issues/2980
DeepSpeed,该issue类型为功能需求，主要涉及对象是DeepSpeed的训练基准脚本。,https://github.com/deepspeedai/DeepSpeed/issues/2975
DeepSpeed,这个issue是用户发出的需求请求，主要是针对DeepSpeed中PipelineModule的自定义分区选项的问题。这个问题主要涉及到对层分区控制的限制，导致当前的分区方式对于典型的语言模型来说并不理想，因为无法实现GPU内存和利用率的平衡。,https://github.com/deepspeedai/DeepSpeed/issues/2974
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的SCR checkpoint engine，用户提出由于某些情况下需要延迟创建目录，希望将目录创建委托给CheckpointEngine。,https://github.com/deepspeedai/DeepSpeed/issues/2972
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中MiCS的实现，可能是由于MiCS功能还在初步实现阶段而导致用户需求尝试和测试。,https://github.com/deepspeedai/DeepSpeed/issues/2964
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中Auto Tensor Parallelism教程的更新，涉及到在T5示例中集成OPT以展示模型不具有kernel注入选项。,https://github.com/deepspeedai/DeepSpeed/issues/2962
DeepSpeed,这是一个用户提出需求的 issue，主要涉及DeepSpeed中如何在多节点训练中不使用 hostfile 部署的问题，由于在任务开始时分配 GPU，因此希望能像 torch 一样只通过传递 master_addr 和 master_port 来进行多节点训练。,https://github.com/deepspeedai/DeepSpeed/issues/2958
DeepSpeed,这个issue是一个用户提出的需求，涉及DeepSpeed框架下的推理过程中关于使用bfloat16的问题，主要是用户想要在推理过程中支持bfloat16，并关注将bfloat16转换为float16可能引起的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2955
DeepSpeed,这个issue属于用户提出的需求类型，主要涉及DeepSpeed中如何正确存储和移动预编译文件的问题。用户询问如何在不重新编译的情况下将编译文件从一台机器移动到另一台机器，尝试使用相同路径时仍然进行重新编译，希望DeepSpeed能够指定查找cpu_adam.so和utils.so的位置。,https://github.com/deepspeedai/DeepSpeed/issues/2947
DeepSpeed,这个issue类型是功能需求，主要涉及的对象是DeepSpeed中的AutoTP功能。由于一些模型不被AutoTP支持，导致用户无法成功运行，因此需要提供对核注入的支持以解决此问题。,https://github.com/deepspeedai/DeepSpeed/issues/2939
DeepSpeed,这个issue是用户提出的需求，主要涉及DeepSpeed中添加对L-BFGS优化器的支持。用户希望通过DeepSpeed来优化LBFGS方法的内存使用，以提高用户对该方法的访问性。,https://github.com/deepspeedai/DeepSpeed/issues/2935
DeepSpeed,该issue是一个关于需求提出的问题，主要讨论了在处理ZeRO-3流量时所需的网络吞吐量。由于需要确定网络连接是否足够以避免过度支付空闲GPU，并且需要计算所需的吞吐量以训练模型和推理。,https://github.com/deepspeedai/DeepSpeed/issues/2928
DeepSpeed,这是一个用户提出的需求类型的issue，主要对象是DeepSpeed中的`dist.init_process_group`函数的超时参数问题。用户希望能够在使用`zero.Init`时覆盖默认的超时时间。由于DeepSpeed使用了自己的通信模块，用户发现很难正确地传递参数来覆盖默认值，导致可能出现的GPU闪退和超时问题。,https://github.com/deepspeedai/DeepSpeed/issues/2927
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO模型，用户希望实现universal checkpoint以支持在不同GPU数量下恢复训练。,https://github.com/deepspeedai/DeepSpeed/issues/2921
DeepSpeed,这是一个需求类型的issue，涉及到重构社区文件的位置。,https://github.com/deepspeedai/DeepSpeed/issues/2919
DeepSpeed,这是一个功能新增的issue，主要涉及到DeepSpeed中的CodeGen模型的推断支持，用户提出了关于在A10040G上运行`Salesforce/codegen16bmulti`模型推断时遇到OOM的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2916
DeepSpeed,这是一个用户提出需求的问题，主要涉及AutoTP对BLOOM模型的支持问题，用户希望了解AutoTP为何不支持BLOOM以及是否有计划在未来支持这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/2915
DeepSpeed,这是一个关于功能需求的问题，主要涉及DeepSpeed在生成sharded checkpoints时类型不一致的情况。用户询问是否会在将来引入每个模型的新类型。,https://github.com/deepspeedai/DeepSpeed/issues/2910
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed与Kubernetes环境冲突的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2908
DeepSpeed,这是一个功能改进提案，涉及到针对验证或测试数据集使用DistributedSampler来避免在GPU上重复加载数据集所导致的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2907
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed是否支持Hybrid Parallelism以及如何组合不同的并行方式。用户想要了解如何同时使用数据并行、流水线并行和张量并行，并请求提供示例。,https://github.com/deepspeedai/DeepSpeed/issues/2906
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的设备无关压缩算法添加请求。原因是DeepSpeed目前缺乏主流支持的一些受欢迎的压缩算法，用户希望将类似于后训练静态量化和结构稀疏性这样的算法纳入DeepSpeed。,https://github.com/deepspeedai/DeepSpeed/issues/2894
DeepSpeed,该issue类型为改进/增强需求，主要对象是DeepSpeed项目中的AutoTP tutorial web formatting和news bit，用户提出了希望将表格格式与网页兼容，并在网站首页添加最新新闻的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2883
DeepSpeed,这是一个用户提出需求的issue，涉及 DeepSpeed 框架中的 CPU 支持，主要希望通过绑定不同CPU核心来支持CPU作为虚拟加速器，提供AVX2/AVX512/AMX指令集的向量或张量计算。,https://github.com/deepspeedai/DeepSpeed/issues/2881
DeepSpeed,该issue类型为用户提出需求，主要涉及DeepSpeed中的模型quantization和presharding操作，用户请求提供用于生成presharded quantization model的代码，同时询问是否可为Bloomz模型提供相同的presharded quantization model。,https://github.com/deepspeedai/DeepSpeed/issues/2878
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的网站目录和GitHub仓库主页的更新，以及教程表格的兼容性修改和不支持模型列表的添加。,https://github.com/deepspeedai/DeepSpeed/issues/2877
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed的加载推断检查点时使用容器。由于需要在加载推断检查点时访问容器中的变量，因此更新了相关函数以实现这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/2875
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的ZeRO3支持Mixture-of-Experts（MoE）层的问题。由于ZeRO3目前不支持MoE模型，用户在使用MoE模型时遇到了瓶颈。,https://github.com/deepspeedai/DeepSpeed/issues/2870
DeepSpeed,这是一个用户提出需求的问题，主要对象是DeepSpeed框架，用户希望DeepSpeed也支持除PyTorch之外的其他框架，由于DeepSpeed目前仅支持PyTorch，用户提出了希望支持其他框架的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2857
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO模块。由于缺乏BF16混合精度到fp32梯度累积的配置选项，用户请求添加可配置的梯度累积数据类型到所有ZeRO阶段。,https://github.com/deepspeedai/DeepSpeed/issues/2847
DeepSpeed,这是一个添加新功能的issue，涉及主要对象是DeepSpeed中的T5支持。由于目前仍在开发中，所以还未完全实现。,https://github.com/deepspeedai/DeepSpeed/issues/2843
DeepSpeed,这是一个用户提交的需求。该问题单涉及的主要对象是DeepSpeed的运行器。,https://github.com/deepspeedai/DeepSpeed/issues/2839
DeepSpeed,这是一个需求报告类型的 issue，主要涉及 DeepSpeed 下的 pre/post forward 方法及 generate 方法的新增，修改及文档支持，用户可能在实际使用中遇到了问题或需要相关的支持和指导。,https://github.com/deepspeedai/DeepSpeed/issues/2832
DeepSpeed,这个issue类型是用户提出需求，主要涉及DeepSpeed的推理API重构，由于要简化推理API的使用方式和代码结构，支持三种情况，并解决了用户相关问题。,https://github.com/deepspeedai/DeepSpeed/issues/2831
DeepSpeed,这个issue属于用户提出需求类问题，主要涉及到DeepSpeed中的MPU模块。用户询问关于MPU的API是否存在以及如何使用这些方法，是否需要手动逐个进行实验。,https://github.com/deepspeedai/DeepSpeed/issues/2827
DeepSpeed,该issue属于代码质量优化，涉及的主要对象是代码中存在不必要的`pass`调用。这个问题由于代码冗余导致了代码质量降低。,https://github.com/deepspeedai/DeepSpeed/issues/2826
DeepSpeed,这个issue是一个用户需求问题，该问题单涉及的主要对象是添加自动生成的PR工作流程。原因是用户希望能够自动生成PR工作流程，以简化工作流程，提高效率。,https://github.com/deepspeedai/DeepSpeed/issues/2822
DeepSpeed,这是一个关于DeepSpeed中ZeRO-Offload功能的问题咨询，用户尝试在使用GPT3-like模型时遇到了OOM错误。,https://github.com/deepspeedai/DeepSpeed/issues/2817
DeepSpeed,这是一个功能需求的问题，主要涉及DeepSpeed中的ZeRO stages 1和2，用户提出需要添加可配置的梯度累积数据类型。,https://github.com/deepspeedai/DeepSpeed/issues/2805
DeepSpeed,这个issue是用户需求类型，主要涉及的对象是PDSH launcher。用户希望添加支持用户定义参数以包含在PDSH launcher中。,https://github.com/deepspeedai/DeepSpeed/issues/2804
DeepSpeed,这个issue类型是用户提出需求，主要对象是对DeepSpeed进行修改以提升模型训练性能，希望将MiCS的实现上游到DeepSpeed代码库。原因是他们通过使用子组分片和分层通信改进了分片数据并行解决方案，提高了训练表现。,https://github.com/deepspeedai/DeepSpeed/issues/2801
DeepSpeed,这个issue是一个用户提出的需求，请求在DeepSpeed的ZeRO模块中添加支持动态梯度累积/批大小逐步增加的功能。,https://github.com/deepspeedai/DeepSpeed/issues/2798
DeepSpeed,这是一个更新请求，针对DeepSpeed中的ROCm版本更新，主要涉及的对象是AMD，请求将ROCm版本从5.1.1更新至5.3，在amd.yml文件中修改。,https://github.com/deepspeedai/DeepSpeed/issues/2796
DeepSpeed,这个issue是关于代码重构和新增功能的，主要涉及DeepSpeed中的MetaTensorContainer，因为重构了meta tensor checkpoint loading的组织结构并添加了错误报告功能。,https://github.com/deepspeedai/DeepSpeed/issues/2792
DeepSpeed,这个issue类型是功能需求，主要涉及DeepSpeed用户配置支持hjson格式，为了验证DeepSpeed配置能够从dict、json和hjson格式正确加载。,https://github.com/deepspeedai/DeepSpeed/issues/2783
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed软件。,https://github.com/deepspeedai/DeepSpeed/issues/2779
DeepSpeed,该issue类型为功能需求，主要涉及的对象是DeepSpeed中的CPU pagelocked tensors。由于目前只能在CUDA环境下使用 pagelocked tensors，用户希望在非CUDA环境下也能够使用这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/2775
DeepSpeed,这是一个用户提出需求的类型，主要对象是在DeepSpeed中需要增加一个关于使用flops profiler的T5示例的文档，问题可能是关于文档缺失或需要进一步补充示例。,https://github.com/deepspeedai/DeepSpeed/issues/2774
DeepSpeed,这是一个功能优化类型的issue，主要涉及DeepSpeed中的自动调优配置的重构。原因可能是为了更容易审查和合并相关工作。,https://github.com/deepspeedai/DeepSpeed/issues/2769
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的BF16训练中的梯度累积问题，导致训练偶尔发散。,https://github.com/deepspeedai/DeepSpeed/issues/2768
DeepSpeed,这是一个用户提出需求的类型，主要对象是创建 tensor parallelism 的博客或教程。,https://github.com/deepspeedai/DeepSpeed/issues/2766
DeepSpeed,该issue是一个功能需求，主要对象是DeepSpeed中的CUDA操作构建过程。由于需要在开发环境下更详细地输出CUDA操作的编译信息，因此引入了`DS_DEBUG_CUDA_BUILD`环境变量。,https://github.com/deepspeedai/DeepSpeed/issues/2759
DeepSpeed,这是一个需求合并类型的issue，涉及主要对象为DeepSpeed的配置文件。由于需要在主DeepSpeed配置文件中合并多项更改之前的内容，出现了这个issue。,https://github.com/deepspeedai/DeepSpeed/issues/2757
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed项目。由于缺少关于AzureML示例的链接，用户请求添加新的链接。,https://github.com/deepspeedai/DeepSpeed/issues/2756
DeepSpeed,这是一个用户提出的功能需求，主要涉及DeepSpeed在单个机器上同时运行多个任务时可能出现的端口冲突问题，希望提供一个选择端口范围的功能来解决该问题。,https://github.com/deepspeedai/DeepSpeed/issues/2752
DeepSpeed,该issue类型为用户提出需求，主要涉及DeepSpeed下的BF16 Kernel Support。由于需求在开发过程中需要添加或修改多个模块，包括数据类型支持、函数库修改等，因此用户提出了这些任务以完善BF16的支持。,https://github.com/deepspeedai/DeepSpeed/issues/2744
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed模型的最大tokens限制。用户希望增加输入tokens的限制，因为当前限制1024对于他们的模型表现不足够。,https://github.com/deepspeedai/DeepSpeed/issues/2740
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中zero 2 & 3阶段的tensor fragmentation功能。由于缺乏individual tensor `.grad` getter/setter功能而导致需求提出。,https://github.com/deepspeedai/DeepSpeed/issues/2727
DeepSpeed,这是一个功能改进类型的issue，主要涉及DeepSpeed中一个函数参数的命名调整，导致与PyTorch的命名规范不一致。,https://github.com/deepspeedai/DeepSpeed/issues/2726
DeepSpeed,该issue类型为用户请求，主要涉及对象是希望构建DeepSpeed中的特定功能模块，即cpu_adam_op.cython310x8664_linuxgnu.so文件。原因是用户需要进行一些修改以适应其应用程序，但由于不熟悉构建过程而导致反复安装整个DeepSpeed包而显得效率低下。,https://github.com/deepspeedai/DeepSpeed/issues/2719
DeepSpeed,这是一个改进建议，主要对象是CI触发器，由于重复运行单元测试浪费资源和时间。,https://github.com/deepspeedai/DeepSpeed/issues/2712
DeepSpeed,这是一个功能需求报告问题，涉及主要对象为DeepSpeed引擎，用户希望引擎在启用BF16和ZeRO stage 1时能够使用BF16优化器。,https://github.com/deepspeedai/DeepSpeed/issues/2706
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在Kubernetes集群中无法将检查点保存到远程云存储桶的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2701
DeepSpeed,这是一个用户提出需求的问题，主要涉及硬件资源预估相关。用户询问是否有方法可以估算推理和托管所需的硬件要求。,https://github.com/deepspeedai/DeepSpeed/issues/2692
DeepSpeed,这是一个功能需求类型的 issue，涉及主要对象是 DeepSpeed 的 Quantization 库。通过引入 dequantization 功能和弃用 IntegerSymmetric kernels，该问题旨在扩展量化工具的功能。,https://github.com/deepspeedai/DeepSpeed/issues/2683
DeepSpeed,这是一个关于功能需求的问题，主要涉及DeepSpeed在Kubernetes上的连接问题。这个问题的症结在于如何在Kubernetes上运行DeepSpeed而避免使用SSH连接。,https://github.com/deepspeedai/DeepSpeed/issues/2679
DeepSpeed,这是一个用户提出的需求，涉及DeepSpeed库中的一个命名一致性问题。由于参数命名与PyTorch不一致，用户建议将参数`set_grads_to_None`重命名为`set_to_none`。,https://github.com/deepspeedai/DeepSpeed/issues/2678
DeepSpeed,这个issue类型是合并请求（pull request），主要涉及DeepSpeed的加速器（accelerator）接口集成。,https://github.com/deepspeedai/DeepSpeed/issues/2677
DeepSpeed,这是一个功能更新的issue，主要涉及DeepSpeed中自动张量并行性的改进。,https://github.com/deepspeedai/DeepSpeed/issues/2670
DeepSpeed,这是一个用户提出需求的issue，主要涉及如何在使用zero (fp16或bf16)时将归一化层如LayerNorm包装为FP32。造成这个问题的原因是在使用不同精度时需要确保正确的数据处理。,https://github.com/deepspeedai/DeepSpeed/issues/2667
DeepSpeed,这是一个用户提出需求的issue，主要涉及修改DeepSpeed库中的默认参数`initial_scale_power`，由于目前默认值32导致潜在的性能问题，因此建议将其调整为较小的值16。,https://github.com/deepspeedai/DeepSpeed/issues/2663
DeepSpeed,这个issue类型是功能增强请求，主要对象是DeepSpeed工具中的INT8量化功能，用户提出了增强INT8量化支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2662
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed对Intel GPU的支持问题。展开阅读,https://github.com/deepspeedai/DeepSpeed/issues/2656
DeepSpeed,这是一个关于用户提出需求的issue，主要涉及DeepSpeed下的`zero.Init`功能，用户请求增加对自定义`_init_weights`函数的支持。该问题的症状是初始化权重不生效。,https://github.com/deepspeedai/DeepSpeed/issues/2650
DeepSpeed,这是一篇用户提出需求的issue，主要涉及DeepSpeed中的ds-attn、distilbert policy和mup模块，用户可能在这些模块的优化或调整方面遇到了问题或需要帮助。,https://github.com/deepspeedai/DeepSpeed/issues/2649
DeepSpeed,这个issue是关于用户提出需求，要求改进DeepSpeed项目的requirements文档，主要涉及的对象为项目的安装和构建需求，用户提出这个问题是因为当前的安装设置过于复杂，缺乏足够的文档说明，需要更清晰的指导。,https://github.com/deepspeedai/DeepSpeed/issues/2648
DeepSpeed,这个issue类型是需求类型，主要对象是对DeepSpeed中的nebula模块进行单元测试。由于缺乏对nebula模块save和load功能的单元测试，用户提出需要添加这方面的测试。,https://github.com/deepspeedai/DeepSpeed/issues/2646
DeepSpeed,这是一个优化issue，主要涉及的对象是DeepSpeed中的Megatron单元测试，问题是关于安装Nvidia Apex时耗时过长的情况。,https://github.com/deepspeedai/DeepSpeed/issues/2625
DeepSpeed,这是一则用户提出的关于DeepSpeed在模型推断方面的重构需求。,https://github.com/deepspeedai/DeepSpeed/issues/2623
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中推理过程中动态切换权重的功能。用户希望能够在不重新启动deepspeed的情况下快速更换模型权重。,https://github.com/deepspeedai/DeepSpeed/issues/2619
DeepSpeed,这是一个用户提出需求的问题单，主要涉及DeepSpeed中处理冻结权重的效率问题。用户希望能够在训练中有效地处理冻结的权重，避免浪费内存和计算资源。,https://github.com/deepspeedai/DeepSpeed/issues/2615
DeepSpeed,这是一个用户提出需求的issue，该问题单涉及DeepSpeed下的Megatron CI workflow的添加。,https://github.com/deepspeedai/DeepSpeed/issues/2614
DeepSpeed,这是一个用户提出需求的类型，主要涉及的对象是DeepSpeed下的ROCm Dockerfile。由于当前使用的是过时的PyTorch版本，用户希望更新为最新版本以解决依赖或性能问题。,https://github.com/deepspeedai/DeepSpeed/issues/2613
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed的模块配置，用户希望将训练标志传递给模块的前向调用，可能导致在模块配置中无法正确处理训练标志的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2604
DeepSpeed,这是一个优化性质的issue，主要涉及DeepSpeed中的ops测试，分析发现顺序执行测试可以降低`nvinference`工作流的延迟。,https://github.com/deepspeedai/DeepSpeed/issues/2599
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed下的benchmark功能。由于BERT和RoBERTa使用不同的mask tokens，用户希望能够更轻松地在两者之间切换。,https://github.com/deepspeedai/DeepSpeed/issues/2592
DeepSpeed,该issue类型是用户提出需求，主要涉及对象为DeepSpeed框架中的LayerNorm模块调度的更新。由于需要提高性能并与即将推出的量化模块调度相匹配而引发。,https://github.com/deepspeedai/DeepSpeed/issues/2590
DeepSpeed,这是一个文档更新类型的issue，主要涉及DeepSpeed的API文档。由于API更新，需要更新README文档。,https://github.com/deepspeedai/DeepSpeed/issues/2587
DeepSpeed,这是一个关于更新API文档的类型为需求提出的issue，涉及主要对象为PipeModule和MoE模块。,https://github.com/deepspeedai/DeepSpeed/issues/2586
DeepSpeed,这是一个功能改进类型的issue，主要涉及W8A16推断的迁移和去量化处理。原因可能是为了更新和改进去量化功能，以提高性能和准确性。,https://github.com/deepspeedai/DeepSpeed/issues/2580
DeepSpeed,这个issue类型是用户提出需求，请求数，涉及的主要对象是代码中使用了被弃用的PyTorch函数，由于使用了不推荐的代码导致需要修复并更新相关部分。,https://github.com/deepspeedai/DeepSpeed/issues/2567
DeepSpeed,这是一个关于文档更新的问题，主要涉及DeepSpeed下的MegatronLM tutorial locator的变更。,https://github.com/deepspeedai/DeepSpeed/issues/2564
DeepSpeed,这是一个用户提出需求的issue，要求添加一个基本的moe container骨架用于头脑风暴。,https://github.com/deepspeedai/DeepSpeed/issues/2562
DeepSpeed,这是一个需求类型的issue， 主要涉及添加checkpoint sharding单元测试，其中涉及创建和加载分片checkpoint的测试。这个issue要求Fix quantizedinference，同时增加对BLOOM、OPT、GPTJ和GPTNEO模型的支持，并测试是否支持metatensor和fp16以及int8。,https://github.com/deepspeedai/DeepSpeed/issues/2561
DeepSpeed,这个issue类型是功能改进，主要涉及的对象是DeepSpeed推理代码的重构。由于需要改进DeepSpeed推理代码的组织结构，以支持特征分离、并行工作流、模型特定代码等。,https://github.com/deepspeedai/DeepSpeed/issues/2554
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中关于在梯度累积边界报告进度的功能改进。,https://github.com/deepspeedai/DeepSpeed/issues/2553
DeepSpeed,这是一个用户提出需求的issue，主要对象是将flash attention集成到DeepSpeed中，用户希望优化内存数据传输以提升性能。,https://github.com/deepspeedai/DeepSpeed/issues/2552
DeepSpeed,这个issue是关于命名规范的建议，主要涉及DeepSpeed中的PostBackwardFunction类。由于命名不够清晰，导致用户提出需要更清晰区分PostBackwardFunction类和PreBackwardFunction类的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2548
DeepSpeed,这是一个功能增强型的问题单，主要涉及DeepSpeed中自动张量并行性研究，通过解析模块并生成注入策略字典以实现对较多Hugging Face模型的支持。,https://github.com/deepspeedai/DeepSpeed/issues/2545
DeepSpeed,这个issue是关于代码更新的需求，主要涉及DeepSpeed下的新DS Inference配置传递。由于需要将新配置从init传递给inference engine和replace_transformer_layer等，开发者请求审查并反馈。,https://github.com/deepspeedai/DeepSpeed/issues/2539
DeepSpeed,这是一个功能需求提案，涉及深度学习推断（Inference）配置的可读性问题。,https://github.com/deepspeedai/DeepSpeed/issues/2537
DeepSpeed,这是一个关于优化性能和提高输出质量的issue，主要涉及DeepSpeed中的Triton/DS kernels和diffusers attention，修复了ClipEncoder与latest `diffusers` main的兼容性问题。,https://github.com/deepspeedai/DeepSpeed/issues/2536
DeepSpeed,该issue类型为功能需求，主要涉及内容为DeepSpeed的AML环境下解析AZ_ML主机列表并构建资源池的功能。原因是AML v2不包含hostfile，而是使用逗号分隔的主机列表，需要通过特定的环境变量实现此功能。,https://github.com/deepspeedai/DeepSpeed/issues/2535
DeepSpeed,这个issue类型是用户提出需求，主要对象是DeepSpeed推理配置，用户寻求将新的DeepSpeed推理配置改为接受json文件设置。,https://github.com/deepspeedai/DeepSpeed/issues/2532
DeepSpeed,这是一个关于功能需求的issue，主要涉及到DeepSpeed中的Pipelined Quantization实现。,https://github.com/deepspeedai/DeepSpeed/issues/2529
DeepSpeed,这是一个用户提出需求的类型为功能请求的issue，主要涉及DeepSpeed代码中使用了已弃用的PyTorch代码，导致某些库在测试时出现问题。,https://github.com/deepspeedai/DeepSpeed/issues/2527
DeepSpeed,该issue类型为需求提出，主要涉及在DeepSpeed中添加4位量化推断以在2个A100 GPU上运行BLOOM-176B。由于需求降低GPU数量并减少推理成本，用户提出了关于4位量化推断性能评估及运行的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2526
DeepSpeed,该issue是一个用户提出需求的类型，主要涉及更新代码所有者以更有针对性通知团队成员针对感兴趣的PR，因为当前通知制度导致所有成员收到所有PR的通知而不仅仅是他们关注的。,https://github.com/deepspeedai/DeepSpeed/issues/2525
DeepSpeed,该issue是一个功能需求的提出，涉及到Dequantization Utils Library的实现。,https://github.com/deepspeedai/DeepSpeed/issues/2521
DeepSpeed,这个issue类型是文档改进，涉及DeepSpeed框架需要nvcc/hipcc的问题。由于缺乏明确说明，用户提出了关于DeepSpeed需要nvcc/hipcc的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/2519
DeepSpeed,这个issue是一个用户提出需求类型的问题，主要涉及的对象是MoE（Mixture of Experts）模型，用户希望添加专家选择路由选项以改进tokenexpert routing。,https://github.com/deepspeedai/DeepSpeed/issues/2517
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中使用`Flops Profiler`来测试`model.generate()`的功能问题。由于目前`Flops Profiler`只能测试transformers如`bert`的`forward`过程，而对于`model.generate()`如`t5`的需要，用户询问如何使用`Flops Profiler`来测试该过程。,https://github.com/deepspeedai/DeepSpeed/issues/2515
DeepSpeed,这是一个用户提出需求的Issue，主要涉及如何使用`Flops Profiler`测试`model.generate()`，由于在针对`t5`时使用了`model.generate()`，希望能够测试其时间成本。,https://github.com/deepspeedai/DeepSpeed/issues/2514
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed的配置。因为用户希望通过DeepSpeed配置更明确地指定训练中使用哪种数据类型，部分解决了添加明确的梯度累积数据类型配置的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2512
DeepSpeed,这是一个功能增强的issue，涉及的对象是DeepSpeed的配置文档生成，用户希望通过pydantic自动生成配置模型文档，提高开发效率。,https://github.com/deepspeedai/DeepSpeed/issues/2509
DeepSpeed,这个issue是一个功能改进类型的问题，涉及到DeepSpeed库中的`init_inference` API，主要解决了配置重构后对旧版本的兼容性问题。,https://github.com/deepspeedai/DeepSpeed/issues/2508
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的CUDA optional deepspeed ops，问题提出者正在寻求在CPUAdam中添加编译选项以实现参数从CPU到GPU的拷贝。,https://github.com/deepspeedai/DeepSpeed/issues/2507
DeepSpeed,该issue是用户提出需求类型，主要涉及DeepSpeed中的吞吐量计算，由于没有考虑梯度累积步骤，导致计算结果不准确。,https://github.com/deepspeedai/DeepSpeed/issues/2505
DeepSpeed,该issue为一个特性请求，主要涉及DeepSpeed中的抽象加速器支持。由于CC([RFC] add device abstraction to allow other device than CUDA be used)对许多文件进行了触及和可能引起冲突，因此将该PR拆分为多个步骤以实现平滑的合并过程。,https://github.com/deepspeedai/DeepSpeed/issues/2504
DeepSpeed,这个issue是需求类型，主要涉及DeepSpeed中添加稳定扩散的单元测试，由于需要不使用需要HF token的stablediffusion模型来创建测试，主要原因是为了验证Midjourney模型的结构与稳定扩散模型的相似性。,https://github.com/deepspeedai/DeepSpeed/issues/2496
DeepSpeed,该issue类型为用户提出需求，并涉及在DeepSpeed的Automated Machine Learning（AML）功能中添加mlflow logging的功能。由于缺乏metrics日志记录，用户希望能够将指标记录到自动化机器学习过程中，以便更好地分析和监控实验结果。,https://github.com/deepspeedai/DeepSpeed/issues/2495
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的DS-inference模块，用户请求添加`scale_attn_by_inverse_layer_idx`功能。导致这个需求的原因是Hugging Face GPT2实现中存在该功能，但在DSinference中缺少该功能。,https://github.com/deepspeedai/DeepSpeed/issues/2487
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的dsinference模块缺少`scale_attn_by_inverse_layer_idx`功能，原因是缺乏类似Hugging Face GPT2实现中的对应功能。,https://github.com/deepspeedai/DeepSpeed/issues/2486
DeepSpeed,该问题类型为功能增强需求，涉及主要对象为DeepSpeed的训练脚本。由于用户希望通过设置MLFLOW环境变量来在训练中记录指标，可能是为了更方便的监控和分析训练过程中的指标数据。,https://github.com/deepspeedai/DeepSpeed/issues/2477
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed在训练Imagenet数据时遇到问题。该问题可能是由于需要删除一些多进程参数才能成功运行。,https://github.com/deepspeedai/DeepSpeed/issues/2475
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及DeepSpeed中支持加速器抽象的问题。由于DeepSpeed目前硬编码了CUDA，因此用户提出需求添加设备抽象，以支持不同类型的设备使用。,https://github.com/deepspeedai/DeepSpeed/issues/2471
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的bf16_optimizer。由于需求是关于BF16混合精度梯度积累在FP32上的问题，所以用户提出了请求需要改进bf16_optimizer以支持非流水线并行ism。,https://github.com/deepspeedai/DeepSpeed/issues/2470
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed是否支持HuggingFace BERT/RoBERTa模型转换为int8精度并导出到ONNX格式，以及在Triton Inference Server中是否能够正确加载导出的ONNX模型。,https://github.com/deepspeedai/DeepSpeed/issues/2465
DeepSpeed,这个issue类型是功能改进提议，主要涉及DeepSpeed中的模型特定代码的文件结构调整。,https://github.com/deepspeedai/DeepSpeed/issues/2464
DeepSpeed,这是一个用户需求提出的issue，主要涉及DeepSpeed中的自动调优指标的日志记录问题，用户希望能够通过mlflow将autotune指标记录到automl作业中。,https://github.com/deepspeedai/DeepSpeed/issues/2460
DeepSpeed,这是一个用户提出的功能需求，涉及DeepSpeed下的DS inference config，用户希望通过该配置来提高代码的可读性和类型检查，同时修复一些代码错误和问题。,https://github.com/deepspeedai/DeepSpeed/issues/2459
DeepSpeed,这是一个用户需求的问题，涉及主要对象为DeepSpeed中的CUDA图，由于CUDA图使不同批次大小出现问题，用户反馈是否应该限制CUDA图的创建数量。,https://github.com/deepspeedai/DeepSpeed/issues/2458
DeepSpeed,这个issue类型为用户提出需求，主要对象是DeepSpeed中的encoder-decoder架构。因为用户期望在Inference阶段支持encoder-decoder架构，所以提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/2451
DeepSpeed,这是一个新功能添加的issue，涉及DeepSpeed的量化库，主要对象是量化库的组件和抽象概念。,https://github.com/deepspeedai/DeepSpeed/issues/2450
DeepSpeed,这个issue是关于优化建议，主要对象是DeepSpeed中的MoE模块，用户提出了关于为什么exp_counts需要移动到CPU的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/2444
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中激活检查点同步的用途和必要性，询问在什么场景下需要启用同步。,https://github.com/deepspeedai/DeepSpeed/issues/2439
DeepSpeed,这是一个功能改进请求，主要涉及创建GPU库抽象层，旨在消除DeepSpeed C++代码中的条件编译模式，并使代码更具硬件无关性。,https://github.com/deepspeedai/DeepSpeed/issues/2437
DeepSpeed,这是一个功能需求类型的issue，涉及的主要对象是DeepSpeed下的一个kernel utility。这个issue提出了关于blockscope reductions的改进，通过引入一个kernel utility来提供高性能、易于使用的接口，以优化常见的reduction patterns。,https://github.com/deepspeedai/DeepSpeed/issues/2436
DeepSpeed,"这是一个用户提出需求的issue，主要涉及DeepSpeed中Bloom int8模型的权重尺度形状问题。用户想了解每个权重的尺度形状为什么是[32, 3]。",https://github.com/deepspeedai/DeepSpeed/issues/2434
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的memory estimators功能，用户希望更新文档以展示当前配置参数的示例输出。,https://github.com/deepspeedai/DeepSpeed/issues/2431
DeepSpeed,这是一个用户提出需求的 issue，主要涉及对象为DeepSpeed在Windows系统上的支持。用户反馈了DeepSpeed在Windows上编译和依赖库安装的困难，以及软件在该系统上的不稳定性问题。,https://github.com/deepspeedai/DeepSpeed/issues/2427
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及的对象是DeepSpeed下的模型参数操作，用户尝试在训练过程中调整模型参数，但发现调整后的参数在optimizer步进时被重置，寻求如何正确传递调整后的参数给optimizer的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/2424
DeepSpeed,此issue为文档更新类型，主要涉及DeepSpeed项目下的mii blog标题更新。原因可能是需要修正或更新mii博客的相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/2423
DeepSpeed,"这是一个用户提出需求的类型的issue，主要涉及""MII blog post""。由于未提供具体内容，导致用户无法继续进行相关操作或获取信息。",https://github.com/deepspeedai/DeepSpeed/issues/2418
DeepSpeed,这是一个功能需求的issue，该问题主要涉及到DeepSpeed中的启动脚本`launch.py`。由于所有的rank将日志输出到控制台，导致不易查看输出日志，用户提出需要将每个rank的输出日志分别存储在不同的日志文件中。,https://github.com/deepspeedai/DeepSpeed/issues/2409
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed-inference的checkpoint加载功能。由于缺乏通用的checkpoint加载功能，用户提出了希望实现在DeepSpeed-inference中通用加载检查点的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2405
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed下添加SLURM多节点运行器，用户希望能在SLURM集群上使用DeepSpeed运行，并添加了`SlurmRunner`类来实现。,https://github.com/deepspeedai/DeepSpeed/issues/2404
DeepSpeed,这是一个功能需求类型的 issue，主要涉及 DeepSpeed 中的自动调试器（autotuner），用户提出了通过命令行参数传递 base64 编码的配置文件以及在远程工作者上启动任务的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2403
DeepSpeed,这是一个用户提出的需求类型的问题，主要关注于DeepSpeed中是否能够临时禁用自动混合精度（autocast）功能，以在全精度模式下运行代码。,https://github.com/deepspeedai/DeepSpeed/issues/2400
DeepSpeed,这个issue类型是功能需求，主要对象是merge activation binding，用户提出了对merge activation functions 的样本需求。,https://github.com/deepspeedai/DeepSpeed/issues/2393
DeepSpeed,这是一个功能需求的issue，主要涉及到DeepSpeed中的Activations Utilities Library。由于需要统一激活函数的实现、类型转换以及避免重复代码，所以新增了一个激活函数库，并提供了不同激活函数的支持。,https://github.com/deepspeedai/DeepSpeed/issues/2391
DeepSpeed,该issue为需求报告，主要涉及DeepSpeed对Triton的依赖性管理。由于Triton尚未正式发布2.0版本，需要将特定版本作为可选依赖项固定在开发版本，以解决依赖管理问题。,https://github.com/deepspeedai/DeepSpeed/issues/2381
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中如何对模型进行预分片，用户试图通过预分片来加快模型加载速度。,https://github.com/deepspeedai/DeepSpeed/issues/2379
DeepSpeed,这是一个需求提出类的issue，主要涉及DeepSpeed下的benchmark scripts的更新和统一，由于需要简化更新测量/报告基准结果的过程。,https://github.com/deepspeedai/DeepSpeed/issues/2374
DeepSpeed,这个issue属于需求提出类型，主要涉及DeepSpeed中的`mem_access`工具新增的功能，提出了关于添加谓词全局加载的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2373
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed模型推断性能分析。原因是用户希望使用CUDA事件来进行模型推断性能分析。,https://github.com/deepspeedai/DeepSpeed/issues/2371
DeepSpeed,这是一个模板更新的issue，不涉及具体的bug或需求，主要对象是GitHub上的DeepSpeed项目。,https://github.com/deepspeedai/DeepSpeed/issues/2363
DeepSpeed,这是一个优化类型的issue，主要涉及DeepSpeed中`fused_bias_residual` kernels的重构工作。这次重构并没有改变算法或计算核心，而主要是为了改进代码的可读性和安全性。,https://github.com/deepspeedai/DeepSpeed/issues/2356
DeepSpeed,这个issue类型是用户提出需求，主要涉及的对象是提供 JSON schema 文件以支持用户在编辑 ds_config.json 文件时获得更好的支持，原因是用户发现编辑配置文件时很难了解所有不同配置选项，希望能获得更好的代码编辑器支持。,https://github.com/deepspeedai/DeepSpeed/issues/2355
DeepSpeed,这个issue是一个功能需求类型的问题，主要涉及到DeepSpeed中的`residual_add_bias` kernel tests扩展到覆盖在attention层之前/之后应用层归一化的情况。,https://github.com/deepspeedai/DeepSpeed/issues/2354
DeepSpeed,"这是一个用户提出需求的issue，主要对象是DeepSpeed中的ZeRO模块。用户提出需要将BF16Optimizer中的fp32梯度累积功能回溯到ZeRO1,2,3中。",https://github.com/deepspeedai/DeepSpeed/issues/2352
DeepSpeed,这个issue是一个功能增强建议，涉及的主要对象是DeepSpeed的推理引擎。由于需要改进DS推理代码、添加BERT推理基准测试以及支持对BERT和GPT模型的前向传递进行性能分析，由此提出了此功能增强建议。,https://github.com/deepspeedai/DeepSpeed/issues/2349
DeepSpeed,这个issue类型为功能增强请求，主要涉及DeepSpeed下的Inference Engine的性能优化。,https://github.com/deepspeedai/DeepSpeed/issues/2348
DeepSpeed,该issue类型为改进建议，提出需要增加最低预提交版本，以解决版本不一致导致的格式不一致问题。,https://github.com/deepspeedai/DeepSpeed/issues/2346
DeepSpeed,这是一个功能需求问题，主要涉及的对象是unit tests，由于依赖数据集文件存储的问题，导致需要在unit tests中使用blob storage。,https://github.com/deepspeedai/DeepSpeed/issues/2342
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的Onebit Optimizers。,https://github.com/deepspeedai/DeepSpeed/issues/2340
DeepSpeed,这是一个用户提出需求的issue，主要涉及文本生成测试中生成长度的调整，以及测试输出信息的简化。,https://github.com/deepspeedai/DeepSpeed/issues/2338
DeepSpeed,这是一个需求更改类型的 issue，主要涉及 DeepSpeed 中的 MOE matmult 核心代码，原因是为了优化内存访问。,https://github.com/deepspeedai/DeepSpeed/issues/2336
DeepSpeed,该issue是一个功能优化的PR，主要涉及重构residual add kernels，目的是提供更灵活的模板函数，并在Python级别暴露fp16/fp32 residual add kernels，同时计划在将来的PR中增加类型/形状匹配的负面测试。,https://github.com/deepspeedai/DeepSpeed/issues/2333
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中的NCCL后端。根据内容可以看出用户希望通过引入专用的NCCL后端，使得通信调用能够直接通过NCCL进行，而无需通过PyTorch的分布式模块。,https://github.com/deepspeedai/DeepSpeed/issues/2332
DeepSpeed,这个issue是一个特性需求，主要涉及Kernel Data Conversion Utility，由于需要统一宏和常量定义以及引入新的数据类型转换实用程序，以便在不同模块之间保持一致性、提供硬件warp大小并在编译时获得可用。,https://github.com/deepspeedai/DeepSpeed/issues/2327
DeepSpeed,这是一个用户提出需求的类型，主要涉及ZeroQuant inference（ZeroQuant推断引擎）和GPT模型，问题出现的原因是ZeroQuant推断引擎尚未发布。,https://github.com/deepspeedai/DeepSpeed/issues/2326
DeepSpeed,该issue类型为功能增强请求，涉及添加更多选项到推断基准测试中。,https://github.com/deepspeedai/DeepSpeed/issues/2325
DeepSpeed,这是一个需求类型的issue，针对DeepSpeed中的ZeRO-Inference功能，请求在README中添加链接。这可能是因为用户想要更方便地查阅相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/2322
DeepSpeed,这是一个功能需求，涉及DeepSpeed加速器抽象层的开发，主要目的是简化DeepSpeed中支持深度学习加速器的编写和维护。,https://github.com/deepspeedai/DeepSpeed/issues/2320
DeepSpeed,这是一个用户向DeepSpeed提交的需求类型的Issue，主要涉及添加张量并行推理单元测试。由于需要保证张量并行推理功能的正确性，因此需要添加相应的单元测试来验证其功能正常性。,https://github.com/deepspeedai/DeepSpeed/issues/2311
DeepSpeed,这是一个优化更新的issue，主要涉及DeepSpeed项目下的ReLU计算核心更新操作。,https://github.com/deepspeedai/DeepSpeed/issues/2306
DeepSpeed,这是一个用户提出需求的issue，主要涉及 DeepSpeed 下希望添加一个选项以保存模型 state_dict 并进行手动保存和加载 model state_dict 的问题，由于使用 ZERO3 时无法直接保存模型 state_dict，用户希望了解如何手动请求所有GPU保存其分区 state_dict。,https://github.com/deepspeedai/DeepSpeed/issues/2304
DeepSpeed,这是一个用户提出需求的issue，主要涉及 DeepSpeed 下的推理评估脚本，旨在帮助评估 DeepSpeed 推理 + HF transformer 模型。,https://github.com/deepspeedai/DeepSpeed/issues/2303
DeepSpeed,这是一个用户提出需求（如何推断使用ZeroQuant技术支持的Int8模型）的问题，涉及DeepSpeed ZeroQuant技术及GPT模型。用户提问的主要问题是如何使用DeepSpeed进行推断。,https://github.com/deepspeedai/DeepSpeed/issues/2301
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的individual tensor `.grad` getter/setter，用户希望通过重写访问器来访问被扁平化的tensor数据以获取对重要数据的正常访问权限。,https://github.com/deepspeedai/DeepSpeed/issues/2290
DeepSpeed,这是一个用户提出需求的issue，主要涉及如何使用DeepSpeed中的`get_fp32_state_dict_from_zero_checkpoint`方法来加载模型checkpoint并实现在不恢复最后的优化器状态下训练模型的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2288
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是GPT模型。,https://github.com/deepspeedai/DeepSpeed/issues/2280
DeepSpeed,这是一个用户提出需求的类型，主要涉及到代码所有者文件。用户希望将自己添加到代码所有者文件中。,https://github.com/deepspeedai/DeepSpeed/issues/2279
DeepSpeed,这个issue是关于提供Memory Access Utility的功能增强，主要对象是DeepSpeed中的CUDA kernels，是用户提出了需求，希望简化Loads操作以及支持async操作。,https://github.com/deepspeedai/DeepSpeed/issues/2276
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed库中的ZeRO-Inference功能，用户可能正在寻求关于该功能的信息或者希望了解相关的Blog内容。,https://github.com/deepspeedai/DeepSpeed/issues/2271
DeepSpeed,这是一个关于需求的issue，主要涉及 DeepSpeed 中如何记录所有排名的 `wall_clock_breakdown` 结果，导致只有排名0报告时间分解的症状。,https://github.com/deepspeedai/DeepSpeed/issues/2264
DeepSpeed,这是一个用户提出需求的类型，主要涉及的对象是CI runners，用户希望将 blob storage 添加到CI运行程序中。,https://github.com/deepspeedai/DeepSpeed/issues/2260
DeepSpeed,这个issue类型是用户提出需求。该问题单涉及的主要对象是DeepSpeed项目中的nv-transformers-v100.yml文件。由于某些变化或者新需求，用户需要对该文件进行更新。,https://github.com/deepspeedai/DeepSpeed/issues/2259
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及的对象是测试pydantic v1.10，由于需要测试新版本的pydantic，所以会引发相关的测试问题或需求。,https://github.com/deepspeedai/DeepSpeed/issues/2256
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中的universal checkpointing和tensor fragments的重构。,https://github.com/deepspeedai/DeepSpeed/issues/2253
DeepSpeed,这是一个关于更新社区视频的内容问题，不属于bug报告。,https://github.com/deepspeedai/DeepSpeed/issues/2249
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在CPU系统上的支持问题。由于部分功能无法在CPU上运行，因此用户希望在没有GPU的系统上进行开发和训练时能够节省计算资源。,https://github.com/deepspeedai/DeepSpeed/issues/2245
DeepSpeed,该issue类型为功能开发（Feature Development），针对Moe pipelining的收敛问题。,https://github.com/deepspeedai/DeepSpeed/issues/2238
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed项目。由于当前批量大小的内存重新分配问题，用户希望实现更大的批量大小。,https://github.com/deepspeedai/DeepSpeed/issues/2236
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中Flops Profiler无法独立包装支持对后向过程进行性能分析的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2234
DeepSpeed,这个issue是一个需求提出类型的问题，主要对象是DeepSpeed中的tensor parallel inference模块。根据标题内容推测，用户希望为这个模块添加单元测试。,https://github.com/deepspeedai/DeepSpeed/issues/2232
DeepSpeed,这是用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed的新bert示例文档。由于缺乏文档，用户请求添加一个新的bert示例文档。,https://github.com/deepspeedai/DeepSpeed/issues/2224
DeepSpeed,这是一个需求提出的issue，主要涉及DeepSpeed需要添加设备抽象以允许使用除CUDA以外的其他设备。原因是DeepSpeed目前硬编码了CUDA，限制了其仅针对CUDA设备的可用性。,https://github.com/deepspeedai/DeepSpeed/issues/2221
DeepSpeed,这是一个功能增强的issue，主要涉及DeepSpeed中的Int8支持，通过ZeroQuant技术实现。,https://github.com/deepspeedai/DeepSpeed/issues/2217
DeepSpeed,这个issue类型为需求提出，主要对象是在DeepSpeed加速测试中使用torch 1.9。由于DeepSpeed目前测试中使用的是torch 1.8，用户提交了这个issue请求将torch升级至1.9。,https://github.com/deepspeedai/DeepSpeed/issues/2215
DeepSpeed,这是一个功能需求类型的issue，涉及主要对象是DeepSpeed Config中的subconfigs，需重构为使用pydantic库。,https://github.com/deepspeedai/DeepSpeed/issues/2210
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在Azure VM上进行多节点训练时关于H5数据加载的示例。,https://github.com/deepspeedai/DeepSpeed/issues/2209
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中ZeroQuant quantization的使用问题，用户提出了关于压缩模型后推理性能没有提升、指导如何运行推理、压缩模块错误以及其他相关问题。,https://github.com/deepspeedai/DeepSpeed/issues/2207
DeepSpeed,这是一个需求提问类型的issue，主要涉及DeepSpeed中BERT Pre-training的详细信息，用户询问关于训练时间、训练轮数、数据集和样本量等问题，以及如何计算数据集大小的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/2206
DeepSpeed,这个issue是对DeepSpeed中支持OPT模型的新增功能的需求，主要涉及OPT模型的优化策略、激活函数选择和层归一化设置等方面的改进。,https://github.com/deepspeedai/DeepSpeed/issues/2205
DeepSpeed,这是一个用户提出需求的issue，主要涉及GPT distillation and quantization的教程添加和评估结果更新。,https://github.com/deepspeedai/DeepSpeed/issues/2197
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在配置中使用多个NVMe SSD的问题，用户希望了解如何添加其他NVMe SSD至配置中。,https://github.com/deepspeedai/DeepSpeed/issues/2195
DeepSpeed,这个issue属于用户提出需求类型，主要对象是ZeroQuant的发布进度，用户想知道关于ZeroQuant发布日期的具体信息。,https://github.com/deepspeedai/DeepSpeed/issues/2193
DeepSpeed,该issue类型为用户提出需求，涉及主要对象为DeepSpeed文档。由于需要添加更多模型供用户采用，用户提出了增加更多模型的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2189
DeepSpeed,这是一个功能需求问题，涉及DeepSpeed中梯度平均标志的支持。原因是用户希望针对稀疏梯度禁用梯度平均化功能。,https://github.com/deepspeedai/DeepSpeed/issues/2188
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中推理API的简化，可能由于现有API过于复杂或使用不便，用户希望简化推理操作以提升用户体验。,https://github.com/deepspeedai/DeepSpeed/issues/2181
DeepSpeed,这是一个需求类型的issue，涉及的主要对象是DeepSpeed下的分布式测试。原因是为了重构所有分布式测试以适应最新的torch版本，以解决与pytorch>=1.12版本兼容的问题。,https://github.com/deepspeedai/DeepSpeed/issues/2180
DeepSpeed,这是一个文档更新类型的issue，涉及DeepSpeed中的zero stage 1 + offload功能，由于文档不清晰导致出现问题。,https://github.com/deepspeedai/DeepSpeed/issues/2178
DeepSpeed,这个issue是关于更新DeepSpeed README文件的，类型为用户提出需求。这个问题涉及到DeepSpeed的Composer library和文档链接。由于README中链接未总是指向最新版本的文档，用户提出了更新建议。,https://github.com/deepspeedai/DeepSpeed/issues/2177
DeepSpeed,这个issue属于文档更新类型，主要涉及DeepSpeed项目。由于采用新的措施或更新，用户可能需要更新或适应这些变化。,https://github.com/deepspeedai/DeepSpeed/issues/2173
DeepSpeed,这个issue是关于更新AMD CI工作流程的，主要涉及的对象是DeepSpeed项目中的AMD CI runner。这个issue由于CUDA初始化错误，导致需要更新AMD CI runner安装自己的pytorch / ROCm以及对测试版本和单元测试的调整。,https://github.com/deepspeedai/DeepSpeed/issues/2172
DeepSpeed,这是一个用户提出需求的类型，主要涉及的对象是DeepSpeed配置。由于用户希望将DeepSpeed配置转换为Pydantic模型，因此提出了该问题。,https://github.com/deepspeedai/DeepSpeed/issues/2170
DeepSpeed,这是一个功能需求问题，主要涉及的对象是DeepSpeed库中的梯度量化功能。这个问题由于需要减少梯度通信开销而引起。,https://github.com/deepspeedai/DeepSpeed/issues/2164
DeepSpeed,这是一个关于需求的issue，主要对象是DeepSpeed的Elastic Training功能。由于需要支持elastic training，因此在master deepspeed branch中添加相关功能。,https://github.com/deepspeedai/DeepSpeed/issues/2156
DeepSpeed,这个issue类型是用户提出需求，主要对象是DeepSpeed软件的master分支。原因是用户希望添加弹性训练支持，以实现更灵活的训练方式。,https://github.com/deepspeedai/DeepSpeed/issues/2155
DeepSpeed,"这个issue是关于DeepSpeed中的""Elastic Training support""功能的需求反馈，并涉及到对深度学习模型的弹性训练支持。由于用户希望使用Elastic Agent在PyTorch中实现弹性训练，因此对DeepSpeed进行了修改和测试。",https://github.com/deepspeedai/DeepSpeed/issues/2153
DeepSpeed,这是一个用户提交功能需求类型的issue，主要涉及对象为DeepSpeed在ROCm上启用fused_lamb_cuda_kernel。由于ROCm平台上某些HIP cooperative groups的可用性，提出了支持 fused_lamb_cuda_kernel 的需求。,https://github.com/deepspeedai/DeepSpeed/issues/2148
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的文档安装要求，可能由于现有文档缺乏必要的安装信息导致用户需要更详细的指导。,https://github.com/deepspeedai/DeepSpeed/issues/2143
DeepSpeed,该issue类型为功能需求，主要涉及DeepSpeed中的梯度伸缩调整及吞吐率计算。这个需求可能是由于用户希望在梯度累积启用时能够控制梯度伸缩操作，并希望吞吐率计算包含当前和平均值。,https://github.com/deepspeedai/DeepSpeed/issues/2140
DeepSpeed,这个issue类型是文档更新请求，涉及主要对象是DeepSpeed在Azure环境下的使用。,https://github.com/deepspeedai/DeepSpeed/issues/2138
DeepSpeed,这是一个用户提出需求的类型，该问题涉及了Azure blog news item主题。可能由于信息缺失或不清晰导致用户希望能得到相关Azure博客新闻的内容或更新。,https://github.com/deepspeedai/DeepSpeed/issues/2135
DeepSpeed,这个issue类型是功能需求，主要对象是针对DeepSpeed项目中的HF Accelerate + DS测试流程进行改进。,https://github.com/deepspeedai/DeepSpeed/issues/2134
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed在Azure上的使用问题，用户可能因为想了解DeepSpeed在Azure环境下的更多信息或者遇到了与Azure相关的问题而提出。,https://github.com/deepspeedai/DeepSpeed/issues/2133
DeepSpeed,这个issue是关于文档的改进，主要对象是Gemfile.lock文件。由于文档可能需要更新或者错误，用户希望删除Gemfile.lock文件。,https://github.com/deepspeedai/DeepSpeed/issues/2130
DeepSpeed,这个issue是一个用户提出的需求类型，主要涉及DeepSpeed推理在C++环境下运行的问题，用户想要了解如何在C++中运行DeepSpeed推理，并提出了可能使用TorchScript和自定义操作的解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/2129
DeepSpeed,该issue类型是用户提出需求，单涉及的主要对象是DeepSpeed的官方网站。由于需要更新网站内容，用户提出希望进行网站刷新。,https://github.com/deepspeedai/DeepSpeed/issues/2123
DeepSpeed,这个issue类型为文档更新，主要涉及更新链接和推荐方法，原因可能是为了更新信息和指导用户使用。,https://github.com/deepspeedai/DeepSpeed/issues/2122
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的benchmark工具的质量改进。原因可能是为了提高安装、命名、集体功能等方面的用户体验。,https://github.com/deepspeedai/DeepSpeed/issues/2120
DeepSpeed,这是一个用户提出的需求。该问题单涉及的主要对象是减少DeepSpeed中推理日志的冗余信息。原因可能是用户希望在推理过程中降低日志输出量，以便更轻松地查看重要信息。,https://github.com/deepspeedai/DeepSpeed/issues/2111
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed中的模型压缩相关文档。由于需要修改模型压缩文档，用户提出了相应的修改建议。,https://github.com/deepspeedai/DeepSpeed/issues/2108
DeepSpeed,这个issue是一个特性需求，主要涉及DeepSpeed库中的ds-inference功能，用户提出了改进tqdm进度条以减少噪音并提供更多信息的建议。,https://github.com/deepspeedai/DeepSpeed/issues/2107
DeepSpeed,这个issue主要是用户提出需求。该问题涉及DeepSpeed Compression Composer的添加，用户提出了需要软件支持、教程、配置说明等内容。,https://github.com/deepspeedai/DeepSpeed/issues/2105
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的推理模块和布隆过滤器的支持，可能是由于某些功能缺失或者改进建议而引起。,https://github.com/deepspeedai/DeepSpeed/issues/2104
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是在训练FP16模型时能否将部分层设置为FP32。,https://github.com/deepspeedai/DeepSpeed/issues/2100
DeepSpeed,这个issue属于文档更新类型，主要涉及到模型并行训练的教程文档。由于moe支持已经合并到megatrondeepspeed中，需要更新相关教程指向主分支。,https://github.com/deepspeedai/DeepSpeed/issues/2098
DeepSpeed,这是一个用户提出需求的问题，涉及主要对象为DeepSpeed中的MoE层。由于MoE要求输入维度必须与输出维度相同，用户认为这增加了复杂性且不易理解，希望能够自由设定输出维度。,https://github.com/deepspeedai/DeepSpeed/issues/2096
DeepSpeed,这个issue属于功能需求，主要对象是DeepSpeed中持久化参数。导致这个问题的原因是参数被固定在GPU内存中，导致浪费交换缓冲区。,https://github.com/deepspeedai/DeepSpeed/issues/2089
DeepSpeed,这是一个用户提出需求的类型，该问题涉及的主要对象是DeepSpeed中的HuggingFace NeoX injection策略支持。,https://github.com/deepspeedai/DeepSpeed/issues/2087
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed的inference API，用户想知道如何设置批量大小参数。,https://github.com/deepspeedai/DeepSpeed/issues/2081
DeepSpeed,这个issue类型是优化建议，主要涉及的对象是DeepSpeed中的Z2+MoE模型内存利用率问题，是由于高expert到GPU比例时梯度放大步骤消耗大量内存导致。,https://github.com/deepspeedai/DeepSpeed/issues/2079
DeepSpeed,这是一个功能需求的issue，涉及深度学习框架DeepSpeed中的Tensor parallelism for Mixture of Experts。出现该需求是因为在当前实现中，非专家令牌在到达专家之前会出现重复，可能导致收敛问题，因此需要实现去重功能。,https://github.com/deepspeedai/DeepSpeed/issues/2074
DeepSpeed,这个issue属于用户提出需求类型，主要对象是添加DeepSpeed推理论文。原因是用户希望增加与DeepSpeed推理相关的论文内容。,https://github.com/deepspeedai/DeepSpeed/issues/2072
DeepSpeed,这是一个功能需求类型的issue，主要涉及的对象是DeepSpeed库中的参数数量统计功能。由于一些用户更倾向于移除在参数计数时的require_grad检查，因此提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/2065
DeepSpeed,该问题类型为用户提出需求，涉及主要对象为DeepSpeed框架，由于用户没有GPU，询问是否可以使用DeepSpeed进行训练。,https://github.com/deepspeedai/DeepSpeed/issues/2052
DeepSpeed,这是一个需求提议类型的issue，主要涉及将flake8添加到pre-commit检查中。这个问题的原因是作者希望通过pre-commit检查来捕获一些潜在的问题，但由于本地测试基础设施的限制，作者决定通过定期推送至PR来解决问题，并指出自己尝试修改代码时遇到了一些困难。,https://github.com/deepspeedai/DeepSpeed/issues/2051
DeepSpeed,"这是一个用户提出需求的issue，主要对象是DeepSpeed框架中的""nebula fast checkpointing""功能。由于用户希望启用该功能，因此提出了这个需求。",https://github.com/deepspeedai/DeepSpeed/issues/2045
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed中的gas loss scaling功能。由于用户希望有开关选项来关闭gas loss scaling，导致了这个issue的提出。,https://github.com/deepspeedai/DeepSpeed/issues/2044
DeepSpeed,这是一个特性需求，要求将MuP项目的优化器集成到DeepSpeed中。,https://github.com/deepspeedai/DeepSpeed/issues/2043
DeepSpeed,这是一个用户提出需求的类型，主要对象是向DeepSpeed添加压缩论文。,https://github.com/deepspeedai/DeepSpeed/issues/2042
DeepSpeed,这是一个用户提出需求的issue，涉及DeepSpeed的通信性能基准，主要是添加不同通信操作的基准测试。,https://github.com/deepspeedai/DeepSpeed/issues/2040
DeepSpeed,这是一个关于功能实现问题的issue，主要涉及DeepSpeed的Hierarchical All-to-All通信算法实现情况，用户提出该算法在当前版本中（0.6.6）是否已发布。,https://github.com/deepspeedai/DeepSpeed/issues/2039
DeepSpeed,这是一个用户提出需求的issue，主要对象是TiedLayerSpec API。由于用户希望在TiedLayerSpec API中分享一系列权重属性而不是单个权重属性，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/2035
DeepSpeed,这个issue类型是用户提出需求，单涉及的主要对象是CI（持续集成）。该问题可能是用户请求在CI输出Python环境和强制更新Hugging Face依赖项。,https://github.com/deepspeedai/DeepSpeed/issues/2014
DeepSpeed,这个issue是一个用户提出需求的类型，主要涉及DeepSpeed Monitor Module (Master)。由于缺乏一般性Monitor工具，用户需求定制化日志记录以及使DeepSpeed能够轻松应用日志记录引起了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/2013
DeepSpeed,这个issue类型是功能增强请求，主要涉及DeepSpeed通信调用的日志记录，提供了新的Logging功能。,https://github.com/deepspeedai/DeepSpeed/issues/2012
DeepSpeed,这是一个用户提出需求的类型，主要涉及的对象是DeepSpeed中的参数分离功能。由于此功能想要与阶段3优化器分开，用户提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/2009
DeepSpeed,该问题是一个用户提出需求，请求减少DeepSpeed配置信息打印，可能导致打印内容过长问题。,https://github.com/deepspeedai/DeepSpeed/issues/2005
DeepSpeed,这个issue类型为用户提出需求，涉及的主要对象为DeepSpeed项目的文档页面。由于缺少新的构建徽章，用户提出需要在首页添加新的构建徽章。,https://github.com/deepspeedai/DeepSpeed/issues/1998
DeepSpeed,这是一个用户提出需求的issue，涉及DeepSpeed的checkpoint保存优化，由于保存checkpoint所需时间较长，用户希望优化这一过程。,https://github.com/deepspeedai/DeepSpeed/issues/1997
DeepSpeed,这个issue类型为功能增强请求，主要对象是DeepSpeed监控模块。原因是为了引入一个通用的`Monitor`工具，用于管理日志记录，并使DeepSpeed能够轻松地应用于引擎外部。,https://github.com/deepspeedai/DeepSpeed/issues/1996
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的PDSH launcher，用户希望通过增加参数来将最大错误代码传递给deepspeed。,https://github.com/deepspeedai/DeepSpeed/issues/1994
DeepSpeed,这个issue类型为测试需求，主要涉及DeepSpeed中ds-kernels对于BERT、RoBERTa、GPT-2、GPT-Neo、GPT-J的测试。,https://github.com/deepspeedai/DeepSpeed/issues/1992
DeepSpeed,这是一个功能需求的issue，主要对象是DeepSpeed Comm. Backend模块，由于当前高级通信方案依赖于混合Python级别通信包，为简化通信原型，需要在DeepSpeed中添加对定制通信后端的支持。,https://github.com/deepspeedai/DeepSpeed/issues/1985
DeepSpeed,这个issue是用户提出需求类型的，主要对象是DeepSpeed文档。因为没有具体的内容描述，无法确定用户具体是想要添加什么530b paper，需要进一步沟通明确。,https://github.com/deepspeedai/DeepSpeed/issues/1979
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中关于OPT的推理支持，因为当前OPT只能通过自定义内核注入策略来支持，用户希望官方能提供正式支持。,https://github.com/deepspeedai/DeepSpeed/issues/1978
DeepSpeed,该问题类型是用户提出需求，请教问题，主要涉及DeepSpeed在推断模式下如何将模型分割成子部分。用户想了解DeepSpeed是如何将模型分割成子部分的，特别是在推断模式下。,https://github.com/deepspeedai/DeepSpeed/issues/1976
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中GAN模型对于不同学习率和调度器在同一个配置文件中的设置。用户想要了解如何在config文件中指定两个独立的scheduler。,https://github.com/deepspeedai/DeepSpeed/issues/1975
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed框架对CUDA/XPU设备运行时的抽象，由于需要使DeepSpeed独立于PyTorch CUDA运行时，并提供了抽象层和查询设备信息函数。,https://github.com/deepspeedai/DeepSpeed/issues/1972
DeepSpeed,这是一个更新（通常是安全更新）的issue，主要涉及的对象是DeepSpeed中的/docs文档，由于CVE2022-29181引起了SAX解析器中对非受信输入的处理不当，导致了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1965
DeepSpeed,这是一个需求新增类型的 issue，主要涉及 DeepSpeed 项目的 sliding window sparse attention 配置添加。这个需求可能是为了引入某种新功能或改进现有功能。,https://github.com/deepspeedai/DeepSpeed/issues/1962
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中添加Unidirectional Sparse Attention Type到BigBird和BSLongformer，讨论了如何将EleutherAI的DeeperSpeed中的更新合并到DeepSpeed中。,https://github.com/deepspeedai/DeepSpeed/issues/1959
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed中的launcher模块。这个需求是由于用户在多节点设置中可能有不同的SSH参数需求，希望可以绕过SSH检查。,https://github.com/deepspeedai/DeepSpeed/issues/1957
DeepSpeed,这是一个功能需求类型的issue，主要涉及Checkpoint reshaping utilities。这个问题是关于通用检查点的。,https://github.com/deepspeedai/DeepSpeed/issues/1953
DeepSpeed,这是一个功能需求报告，涉及的主要对象是DeepSpeed下的bert-type模型。由于需要优化性能或提高效率，用户提出了希望在DeepSpeed推断过程中启用CUDA图功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1952
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在ROCm设备上推理的功能。由于缺少必要的代码，用户需要启用DeepSpeed推理示例。,https://github.com/deepspeedai/DeepSpeed/issues/1922
DeepSpeed,该issue为用户提出的功能需求。主要涉及对象为DeepSpeed中的b16 sharded optimizer，用户希望实现bf16推理功能。,https://github.com/deepspeedai/DeepSpeed/issues/1917
DeepSpeed,这个issue是关于功能改进，主要涉及Z3追踪管理。原因可能是追踪缓存失效，需要分离NVMe预取和全收预取，以及处理子模块的连续执行。,https://github.com/deepspeedai/DeepSpeed/issues/1916
DeepSpeed,这是一个特性需求的 issue，涉及 DeepSpeed 对 Fairseq 模型的支持。,https://github.com/deepspeedai/DeepSpeed/issues/1915
DeepSpeed,该问题是关于优化ds_report在HIP/ROCm环境下的输出，主要涉及的对象是DeepSpeed软件的用户，由于之前错误显示了与HIP/ROCm环境不相关的`nvcc`错误信息，以及显示了无关的依赖信息，导致了用户体验不佳。,https://github.com/deepspeedai/DeepSpeed/issues/1906
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中缺少torch等效的model.no_sync() API。用户希望找到类似功能以避免不必要的同步操作。,https://github.com/deepspeedai/DeepSpeed/issues/1902
DeepSpeed,这是一个建议类型的issue，主要涉及到DeepSpeed项目中的代码质量问题。由于未提供具体内容，无法进一步分析导致此建议的原因。,https://github.com/deepspeedai/DeepSpeed/issues/1900
DeepSpeed,这个issue是一个功能需求提出，主要针对DeepSpeed中用于Inference的PP（Persistent Partition）在neox项目中的改动。,https://github.com/deepspeedai/DeepSpeed/issues/1899
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中SSG GPU支持。由于用户认为SSG GPU在处理大规模语言模型方面具有优势，希望DeepSpeed能够支持这种类型的GPU。,https://github.com/deepspeedai/DeepSpeed/issues/1898
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的CPU offload功能，用户提出需要添加最大CPU内存配置来避免桌面崩溃的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1891
DeepSpeed,这是一个建议性的issue，主要涉及的对象是DeepSpeed项目的构建过程。由于conda提供了`libaio`二进制包，建议将其作为构建过程的替代方案，并探讨如何在预构建时自动检测conda环境变量，以简化Linux发行版的支持。,https://github.com/deepspeedai/DeepSpeed/issues/1890
DeepSpeed,这是一个更新依赖项的issue，涉及到将nokogiri从1.13.3升级到1.13.4版本的文档。原因是为了解决安全漏洞CVE202224836，CVE201825032和CVE202223437。,https://github.com/deepspeedai/DeepSpeed/issues/1889
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的ZeRO3支持问题，其原因是需要移除所有层总是同步执行的要求。,https://github.com/deepspeedai/DeepSpeed/issues/1888
DeepSpeed,这个issue类型是用户提出需求，针对的主要对象是DeepSpeed下的配置选项，用户希望添加一个配置选项以启用融合的fp16优化器。,https://github.com/deepspeedai/DeepSpeed/issues/1882
DeepSpeed,这是一个用户提出需求的类型，主要涉及文档添加MOE论文，原因可能是为了完善DeepSpeed文档。,https://github.com/deepspeedai/DeepSpeed/issues/1875
DeepSpeed,这是一个用户提出需求类型的issue，该问题单涉及的主要对象是DeepSpeed文档网站。由于用户希望将AMD博客添加到网站，可能是为了丰富文档内容或提供更多相关资源。,https://github.com/deepspeedai/DeepSpeed/issues/1874
DeepSpeed,这个issue类型是功能需求，主要涉及DeepSpeed中支持多个模块在具有相同架构时应用单一策略的情况。,https://github.com/deepspeedai/DeepSpeed/issues/1869
DeepSpeed,这个issue属于用户提出需求类型，主要涉及的对象是AWS SageMaker环境，由于缺乏DeepSpeed在AWS SageMaker环境下的设置帮助程序，用户提出了需要支持AWS SageMaker的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1868
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中的模型推理操作，问题是在使用DeepSpeed进行分布式训练时，如何执行模型推理操作。,https://github.com/deepspeedai/DeepSpeed/issues/1863
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及如何在DeepSpeed训练过程中进行推断操作，并询问如何在DeepSpeed中调用特定模型函数的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1862
DeepSpeed,这个issue是用户提出的需求。主要对象是DeepSpeed，用户希望能够使用预训练权重来初始化模型，而不是从头训练模型。,https://github.com/deepspeedai/DeepSpeed/issues/1860
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed文档更新。,https://github.com/deepspeedai/DeepSpeed/issues/1848
DeepSpeed,这是一个用户提出需求的issue，主要涉及GitHub Actions Workflow中的并发策略。由于当前CI工作流程可能导致队列中和运行中的作业无法被取消，用户希望添加并发标签以提供这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/1844
DeepSpeed,这是一个关于代码优化的需求报告，主要涉及DeepSpeed中全局规范计算逻辑的整合。因为存在多个重复实现及处理模型并行性的不足，导致需要对其中的实现进行优化。,https://github.com/deepspeedai/DeepSpeed/issues/1839
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed库中的expert和data parallel groups的创建。由于ep_size必须小于world_size，导致了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/1838
DeepSpeed,这个issue是对DeepSpeed项目中添加明确的gradient_accumulation_dtype配置的建议。,https://github.com/deepspeedai/DeepSpeed/issues/1835
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是网站页面布局。由于网页宽度需要调整，用户希望进行相应的调整。,https://github.com/deepspeedai/DeepSpeed/issues/1829
DeepSpeed,这个issue类型是功能增强，主要对象是DeepSpeed的github action unit tests。由于不同平台上的单元测试运行失败时只能重新运行整个测试套件，故提出希望能够独立按平台运行测试的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1828
DeepSpeed,这是一个提供更新信息的issue，主要涉及DeepSpeed的docker环境配置。,https://github.com/deepspeedai/DeepSpeed/issues/1819
DeepSpeed,这是一个更新文档内容的issue，涉及主要对象是Gemfile。这个issue是由于依赖库`commonmarker`版本存在安全问题导致的。,https://github.com/deepspeedai/DeepSpeed/issues/1817
DeepSpeed,这个issue是一个用户提出的需求，关于DeepSpeed在初始化过程中CPU内存使用率过高的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1814
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed的PyPi页面。原因是需要将GitHub URL添加到PyPi页面来显示相关链接和帮助自动化工具找到Requests项目的源代码。,https://github.com/deepspeedai/DeepSpeed/issues/1812
DeepSpeed,这个issue类型为改进提议，主要涉及到DeepSpeed引擎中的梯度归约代码的简化。这个提议的原因是为了改进代码结构和提高代码效率。,https://github.com/deepspeedai/DeepSpeed/issues/1811
DeepSpeed,该issue是一个用户提出的需求，涉及主要对象是DeepSpeedConfig。由于用户在配置文件中拼写错误导致配置不生效，希望DeepSpeed能够在创建DeepSpeedConfig时验证参数，提高配置准确性。,https://github.com/deepspeedai/DeepSpeed/issues/1809
DeepSpeed,这是一个用户需求问题，主要涉及的对象是DeepSpeed中的op builder和JIT extension builds。,https://github.com/deepspeedai/DeepSpeed/issues/1807
DeepSpeed,这个issue属于用户提出需求，主要涉及DeepSpeed的Minjiaz/mos文档。用户可能在寻求有关文档的相关帮助或者提供反馈意见。,https://github.com/deepspeedai/DeepSpeed/issues/1802
DeepSpeed,这个issue属于功能需求类，主要涉及bf16_optimizer的集成问题，因为需要实现optimizer state sharding与pipeline parallelism的整合。,https://github.com/deepspeedai/DeepSpeed/issues/1801
DeepSpeed,这是一个用户提出需求的issue，主要涉及网站文章和教程的改进。,https://github.com/deepspeedai/DeepSpeed/issues/1799
DeepSpeed,该问题类型为用户提出需求，主要涉及到如何在DeepSpeed中手动更改全局批大小以实现动态批大小效果。原因是DeepSpeed目前不支持动态批大小，但用户急需使用此功能。,https://github.com/deepspeedai/DeepSpeed/issues/1796
DeepSpeed,这是用户提出的新特性需求问题，主要对象是DeepSpeed中的Adam优化器。由于作者提到的最大化通信效率的新方法，因此用户可能希望在Largescale训练中实现更有效的通信。,https://github.com/deepspeedai/DeepSpeed/issues/1790
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed的zero3配置以及对345m GPT模型性能的优化问题，由于用户希望能够获得经典的配置模型来方便体验DeepSpeed的算法效果。,https://github.com/deepspeedai/DeepSpeed/issues/1782
DeepSpeed,这是一个用户提出的需求问题，主要涉及对象是DeepSpeed中的优化器状态。原因可能是用户希望仅维护可训练参数的优化器状态。,https://github.com/deepspeedai/DeepSpeed/issues/1780
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的checkpoint机制，用户希望通过添加新的参数来控制加载/保存checkpoint时是否跳过某些通信操作，以减少CPU内存使用。,https://github.com/deepspeedai/DeepSpeed/issues/1778
DeepSpeed,这是一个关于软件功能需求的问题，主要涉及DeepSpeed中的模型初始化和权重加载，用户希望能够在预训练过程中使用非DeepSpeed训练得到的权重进行初始化，但目前在尝试时遇到了类型错误的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1777
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed中的Bfloat zero1支持问题。由于zero1尚未实现Bfloat支持，用户请求实现这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/1771
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中的Flops Profiler API，由于之前的实现方式限制了对模型输入格式的灵活性，导致用户无法正确使用。,https://github.com/deepspeedai/DeepSpeed/issues/1768
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed是否支持CUDA Graphs的功能。用户想知道是否可以在DeepSpeed中使用CUDA Graphs来进一步优化训练过程。,https://github.com/deepspeedai/DeepSpeed/issues/1746
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的ZeRO Inference，用户请求添加一个`initialize_inference`包装程序。,https://github.com/deepspeedai/DeepSpeed/issues/1744
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中ZeRO推断配置不直观的问题。原因是对于可能不再关心训练的用户来说，仍有与训练相关的配置变量，导致使用不直观，用户希望提出针对训练和推断的非特定配置名称，以及废弃与训练相关的配置变量。,https://github.com/deepspeedai/DeepSpeed/issues/1738
DeepSpeed,这是一个特性需求提出的issue，主要涉及DeepSpeed下的模型文件，要求将param_shapes从zero_*的checkpoint文件中移出到mp_rank_*文件中，并使用符号常量替换硬编码值。,https://github.com/deepspeedai/DeepSpeed/issues/1732
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed对于backward propagation profiling的支持。由于当前DeepSpeed只支持前向传播的分析，用户希望添加对于反向传播的分析以实现参数优化和计算培训中的理论峰值FLOPs百分比。,https://github.com/deepspeedai/DeepSpeed/issues/1731
DeepSpeed,这是一个功能需求提议，主要涉及DeepSpeed中的Multi-node保存进程ID支持和稀疏注意力额外参数。由于需要手动安装Triton来支持稀疏注意力，需要进行相关的后续工作。,https://github.com/deepspeedai/DeepSpeed/issues/1728
DeepSpeed,这是一个用户提出需求的issue，主要对象是为DeepSpeed添加一个简单的PyTorch Lightning测试。由于需要确保DeepSpeed与Lightning兼容性，用户希望添加简单的测试以验证插件功能。,https://github.com/deepspeedai/DeepSpeed/issues/1726
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的优化器状态整合功能缺失问题，可能因为在训练过程中切换不同精度或资源数量导致无法恢复优化器状态。,https://github.com/deepspeedai/DeepSpeed/issues/1723
DeepSpeed,这个issue是用户提出的需求类型，主要涉及DeepSpeed中PDSH launcher的返回值问题，由于默认行为是即使有节点返回错误也会返回0，导致需要查看节点日志来确定DeepSpeed运行是否成功。,https://github.com/deepspeedai/DeepSpeed/issues/1722
DeepSpeed,这个issue属于功能需求类型，主要对象是DeepSpeed项目的pre-commit检查。由于缺乏有效的拼写检查工具，导致过去存在拼写错误并且难以定期进行修复。,https://github.com/deepspeedai/DeepSpeed/issues/1717
DeepSpeed,这是一个用户提出需求的issue，主要对象是请求DeepSpeed团队提供BERT预训练数据设置说明。由于缺乏预处理数据的指导，用户寻求关于数据预处理阶段的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/1716
DeepSpeed,这是一个文档更新类型的issue，主要涉及到bfloat16文档的更新。原因是需要将文档与最新更改同步。,https://github.com/deepspeedai/DeepSpeed/issues/1715
DeepSpeed,该issue类型是用户提出需求，主要对象是DeepSpeed中MoE相关的新闻和教程整理。,https://github.com/deepspeedai/DeepSpeed/issues/1708
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是MoE Inference tutorial。由于缺乏足够的上下文，用户请求添加更多关于MoE Inference tutorial的相关内容。,https://github.com/deepspeedai/DeepSpeed/issues/1707
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed项目中的moe-inference模块。由于缺少相关教程，用户希望添加一个moe-inference的教程。,https://github.com/deepspeedai/DeepSpeed/issues/1706
DeepSpeed,这是一个用户提出需求的issue，涉及MoE推理支持和PR-MoE模型的支持。用户希望DeepSpeed添加MoE推理支持和PRMoE模型的特性，并提出了相关的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1705
DeepSpeed,这是一个需求类型的issue，主要涉及的对象是PR MoE教程的创建。原因可能是用户希望添加PR MoE教程以提高使用体验。,https://github.com/deepspeedai/DeepSpeed/issues/1704
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed推理模型在生产环境中无法方便地不使用命令行运行的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1702
DeepSpeed,该issue类型为功能需求，主要对象是对DeepSpeed中的top-2 gating添加了RTS和token masking功能，并提供了可配置的抖动epsilon。这个需求的提出是为了增强模型的性能和灵活性。,https://github.com/deepspeedai/DeepSpeed/issues/1700
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中初始化stage 1和stage 2引擎时无法将optimizer设置为None的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1699
DeepSpeed,这是一个关于用户提出需求的问题，主要涉及DeepSpeed Inference中的初始化推理引擎功能。用户发现在生产中使用DeepSpeed推理时，初始化推理引擎的时间过长，希望能够减少这种延迟，可能通过保存或导出已初始化的推理引擎来实现。,https://github.com/deepspeedai/DeepSpeed/issues/1694
DeepSpeed,这个issue类型为需求提出，主要对象是DeepSpeed的launcher程序，用户提出需求是为了支持更灵活地调用用户脚本。,https://github.com/deepspeedai/DeepSpeed/issues/1690
DeepSpeed,该issue类型为用户提出需求，主要对象为DeepSpeed文档。原因是用户希望在文档中添加logo。,https://github.com/deepspeedai/DeepSpeed/issues/1676
DeepSpeed,这是一个用户提出需求类的issue，主要涉及Public Pile数据集更新情况。由于数据集信息有更新，用户可能希望了解最新的公共Pile数据集结果。,https://github.com/deepspeedai/DeepSpeed/issues/1675
DeepSpeed,这个issue是一个用户提出需求类型的问题，该问题单涉及的主要对象是PyTorch Lightning（PL）和DeepSpeed。用户提出了关于建立Lightning生态系统CI的需求，以防止版本不兼容导致的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1674
DeepSpeed,这是一个用户提出需求类型的issue， 主要涉及的对象是关于DeepSpeed上如何集成DL模型到其pipeline中。 由于用户想要了解如何将指定的深度学习模型集成到DeepSpeed框架中并进行整合，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1671
DeepSpeed,这是一个功能需求的issue，主要涉及的对象是GPT-J模型的推理支持。,https://github.com/deepspeedai/DeepSpeed/issues/1670
DeepSpeed,该issue类型为文档改进，主要涉及的对象是DeepSpeed项目的文档。由于存在拼写错误、语法问题以及表达不清晰，用户提出了需要改进文档内容的要求。,https://github.com/deepspeedai/DeepSpeed/issues/1665
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中的`save_fp16_model`函数，用户提出要添加返回状态功能以供调用者判断模型是否成功保存。,https://github.com/deepspeedai/DeepSpeed/issues/1663
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的分布式训练端口号灵活性，可能由于当前端口限制导致用户无法实现自定义端口号。,https://github.com/deepspeedai/DeepSpeed/issues/1656
DeepSpeed,这是一个用户提出需求的issue。该问题主要涉及DeepSpeed库中的大型类，用户认为代码中的大类会导致阅读和维护困难，建议进行大规模重构以提高代码质量。,https://github.com/deepspeedai/DeepSpeed/issues/1650
DeepSpeed,这个issue类型为新功能贡献指南，涉及的主要对象为DeepSpeed项目。由于用户想要为项目贡献新功能，因此提出了关于新特性贡献的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1646
DeepSpeed,这个issue是用户提出的需求，要求在当有效批量大小发生变化时添加调整超参数的建议。,https://github.com/deepspeedai/DeepSpeed/issues/1641
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed中稀疏注意力模块的FP32支持，用户希望了解如何实现FP32支持以提升模型的多功能性。,https://github.com/deepspeedai/DeepSpeed/issues/1640
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中部署Mixture of Expert模型，并寻求帮助如何强制仅对专家层进行并行化的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1638
DeepSpeed,这是一个关于性能优化和并行测试的issue，涉及到DeepSpeed项目中的单元测试。原因是为了提高测试的运行效率和支持并行化测试，但有时会导致共享GPU导致内存溢出错误，需要解决。,https://github.com/deepspeedai/DeepSpeed/issues/1636
DeepSpeed,这是一个功能需求提出的issue，主要涉及 DeepSpeed 在 Windows 上支持 CPU Adam 和 Adagrad，原因是为了确保这些功能在 MSVC++ 编译下能够正常构建并通过单元测试。,https://github.com/deepspeedai/DeepSpeed/issues/1634
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed的MoE for NLG功能。由于用户想要获得关于MoE for NLG的教程，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1633
DeepSpeed,这是一个文档更新类的issue，主要对象是DeepSpeed的README文件。可能是由于文档内容需要更新或修正导致用户提出此issue。,https://github.com/deepspeedai/DeepSpeed/issues/1632
DeepSpeed,这个issue属于功能需求提出，涉及对象为DeepSpeed中的Pipeline parallelism和ZeRO-2和ZeRO-3，用户询问为什么Pipeline parallelism与ZeRO-2和ZeRO-3不兼容，以及如何能够运行带有ZeRO-2/3的示例。,https://github.com/deepspeedai/DeepSpeed/issues/1629
DeepSpeed,这是一个新功能公告类的issue，主要涉及到模型的改进与更新。,https://github.com/deepspeedai/DeepSpeed/issues/1628
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed中的ds_report输出控制，提出了新增两个flag以及调整函数入口的建议，目的是修复启动时出现的asyncio相关警告问题。,https://github.com/deepspeedai/DeepSpeed/issues/1622
DeepSpeed,这是一个如何在使用多个节点进行训练时找到低速节点的问题。这个问题类型属于用户提出需求或请教问题的类型，主要涉及的对象是深度学习训练中使用多个节点的情况。由于用户想要找到训练过程中速度较慢的节点，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1617
DeepSpeed,这是一个功能需求类型的 issue，主要对象是 DeepSpeed 下的 SIMD_WIDTH 参数检测，AVX2 和 AVX512 在 Windows 平台的检测。,https://github.com/deepspeedai/DeepSpeed/issues/1616
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中top2gating功能的支持问题。原因可能是目前只有top1gating支持tutel，用户希望也能够支持top2gating。,https://github.com/deepspeedai/DeepSpeed/issues/1614
DeepSpeed,这是一个需求更改的issue，主要涉及DeepSpeed下的ZeRO模块命名的重构。原因可能是当前命名存在混淆，需要更清晰明了的命名结构。,https://github.com/deepspeedai/DeepSpeed/issues/1607
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed在slurm环境下无法实现ssh连接，导致无法进行多节点分布式训练。,https://github.com/deepspeedai/DeepSpeed/issues/1603
DeepSpeed,这是一个改进建议的Issue，涉及到DeepSpeed项目中的预提交钩子的更新和改进。,https://github.com/deepspeedai/DeepSpeed/issues/1602
DeepSpeed,这是一个用户提出需求的issue， 主要涉及对象是在分布式DeepSpeed训练中如何重新执行前向传递。用户希望能够在单个rank中重置模型的梯度、激活检查点等，并重新开始前向传递，以便根据输出条件性地更改模型的输入。,https://github.com/deepspeedai/DeepSpeed/issues/1600
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的sparse attention模块，由于句子长度不同导致需要支持不同的attention masks。,https://github.com/deepspeedai/DeepSpeed/issues/1597
DeepSpeed,该issue属于功能增强类，主要涉及DeepSpeed中flopps计算中增加tensor方法的支持，分离MACs和flops计数，以及改进FLOPS的计算方法。,https://github.com/deepspeedai/DeepSpeed/issues/1591
DeepSpeed,这个issue是一个功能需求，涉及的主要对象是DeepSpeed库中的gradient accumulation boundary控制。这个功能需求是为了让客户端代码可以手动控制梯度积累边界状态，以便更轻松地集成不同功能。,https://github.com/deepspeedai/DeepSpeed/issues/1588
DeepSpeed,这个issue是用户提出需求类型，主要涉及DeepSpeed库是否支持Macbook Pro M1 Max，由于用户想在M1 Max上进行GPTJ的微调，希望DeepSpeed库能够支持这一硬件平台。,https://github.com/deepspeedai/DeepSpeed/issues/1580
DeepSpeed,该issue属于用户提出需求，并涉及对DeepSpeed中TensorBoard日志记录的补充文档。由于需要填写`Train/Samples/elapsed_time_ms_backward_inner`和`Train/Samples/elapsed_time_ms_step`的oneliner，用户寻求帮助以完善文档描述。,https://github.com/deepspeedai/DeepSpeed/issues/1577
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及DeepSpeed的NUMA亲和性设置和内存拷贝速度优化。该问题由于不同系统配置下内存拷贝速度的差异导致用户希望DeepSpeed提供NUMA亲和性设置的指导和支持。,https://github.com/deepspeedai/DeepSpeed/issues/1576
DeepSpeed,这是一个用户提出需求的issue，主要涉及Activation Checkpoint Prefetch功能的请求。由于在中等规模模型中（100B〜1T），将激活预取特性用于扩大批次大小。,https://github.com/deepspeedai/DeepSpeed/issues/1575
DeepSpeed,这个issue类型是技术改进/优化，主要考虑了PyTorch 1.2+对tensorboardX的替代性支持，旨在减少对特定版本的硬性依赖。,https://github.com/deepspeedai/DeepSpeed/issues/1571
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中添加共享库的功能。这个需求可能是为了提高代码复用性或者简化项目结构。,https://github.com/deepspeedai/DeepSpeed/issues/1567
DeepSpeed,这个issue属于文档改进类型，主要涉及到DeepSpeed的autotuning功能文档，问题症状是存在一些拼写错误和需要补充新闻文章。,https://github.com/deepspeedai/DeepSpeed/issues/1565
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中的autotuning功能的添加。由于手动调整系统参数可能耗时耗力，因此用户提出了添加Autotuner功能以提高系统效率的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1554
DeepSpeed,这是一个功能需求类型的issue，主要对象是DeepSpeed库中的inference communication功能。由于没有具体内容，无法分析具体原因。,https://github.com/deepspeedai/DeepSpeed/issues/1552
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的量化器（quantizer），用户希望对其进行优化以支持更大的隐藏层维度和分组大小。 ,https://github.com/deepspeedai/DeepSpeed/issues/1544
DeepSpeed,"这是一个用户提出需求的issue，主要涉及贡献Curriculum Learning方法到DeepSpeed框架。用户提出了要实现""minibatch trimming""的方法，希望了解如何实现和参与贡献。",https://github.com/deepspeedai/DeepSpeed/issues/1539
DeepSpeed,这是一个需求提出型的issue，主要涉及更新参数名称以及优化器配置名称的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1536
DeepSpeed,这是一个功能更新的issue，涉及DeepSpeed中的`multinode_runner.py`文件，并对`PDSHRunner`做了一些修改。原因是为了使用`fstr`，添加了`%n`的引用链接，并在构造函数中移动了验证逻辑。,https://github.com/deepspeedai/DeepSpeed/issues/1532
DeepSpeed,这是一个功能需求类型的issue，主要对象是DeepSpeed库中的WarmupLR和WarmupDecayLR类，用户提出了需要添加`warmup_type`参数来控制学习率在预热阶段的增长方式。原因是当前预热调度器不是线性预热，导致对大型模型而言学习率上升过快，用户希望能够选择使用线性增长方式。,https://github.com/deepspeedai/DeepSpeed/issues/1530
DeepSpeed,这是一个功能改进的issue，主要涉及DeepSpeed中的pipeline module、engine和assertion，用户提出了关于改进错误消息、参数移动及重命名、添加断言和提高可用性等方面的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1529
DeepSpeed,该issue是一个功能需求提案，主要涉及DeepSpeed中MoE layer的优化添加。,https://github.com/deepspeedai/DeepSpeed/issues/1528
DeepSpeed,这是一个关于性能优化的issue，涉及DeepSpeed中的ZeRO功能，主要问题是减少在checkpoint加载过程中的CPU内存开销。,https://github.com/deepspeedai/DeepSpeed/issues/1525
DeepSpeed,这是一个关于优化DeepSpeed中quantize.py文件的issue，类型为功能改进，主要涉及代码效率的优化，并且包括了更新计算方法、修改随机舍入逻辑、处理数据类型溢出问题等方面的改进。,https://github.com/deepspeedai/DeepSpeed/issues/1519
DeepSpeed,这个issue类型是文档需求，主要涉及DeepSpeed中bfloat16文档的缺失，可能由于开发者忘记或遗漏更新对应的文档。,https://github.com/deepspeedai/DeepSpeed/issues/1516
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中ZeRO-3推断支持的问题。由于需要支持ZeRO-3的推断功能，因此要求没有指定优化器。,https://github.com/deepspeedai/DeepSpeed/issues/1514
DeepSpeed,这是一个需求提报，主要涉及Tensor-Parallelism在不同模型结构上的支持和验证。,https://github.com/deepspeedai/DeepSpeed/issues/1512
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed项目下的`runner.py`文件中关于字符串格式化的统一问题。由于存在不一致的字符串风格和无关的`shutil`导入，作者希望统一使用`format()`方法并移除无用的导入，增加代码的可读性。,https://github.com/deepspeedai/DeepSpeed/issues/1511
DeepSpeed,这个issue是一个用户提出的需求，主要涉及模型服务过程中的并行化问题。用户希望能够在DeepSpeed中以模型并行的方式运行模型服务代码，但目前存在端口冲突等问题。,https://github.com/deepspeedai/DeepSpeed/issues/1508
DeepSpeed,这个issue类型是文档更新请求，主要涉及DeepSpeed中CL的实现和调优策略。,https://github.com/deepspeedai/DeepSpeed/issues/1506
DeepSpeed,该issue为功能需求，用户希望能够将自定义的hostfile路径传递给`ds_ssh`命令，旨在实现定制化的主机文件路径设置。,https://github.com/deepspeedai/DeepSpeed/issues/1504
DeepSpeed,这是一个文档更新类型的issue，主要涉及的对象是DeepSpeed的DSE Megatron链接。这个问题是由于需要更新链接指向ZeRO3版本引起的。,https://github.com/deepspeedai/DeepSpeed/issues/1500
DeepSpeed,这是一个用户提出需求的问题，主要涉及到模型训练过程中的NaN loss问题，用户希望能够跳过对这些导致NaN loss的样本的反向传播。,https://github.com/deepspeedai/DeepSpeed/issues/1496
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中MOE（Mixture of Experts）模块的token dropping功能的启用和禁用。,https://github.com/deepspeedai/DeepSpeed/issues/1492
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中关于数据类型和稀疏数据处理的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1487
DeepSpeed,这个issue类型为功能建议，主要涉及DeepSpeed下的Embedding和EmbeddingBag模块的梯度计算优化。由于rebase操作出现错误，导致了这个问题的提出。,https://github.com/deepspeedai/DeepSpeed/issues/1484
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed在使用pytorch multiprocess启动时的问题。由于用户想知道如何使用pytorch multiprocess启动deepspeed，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1468
DeepSpeed,这是一个关于需求探讨的问题，主要涉及DeepSpeed中推理时融合内核的支持问题。用户询问有关推理适用融合内核的支持是否被移除，存在的原因可能是代码中推理函数未融合以及缺少构建使用融合内核的专门推理构建器。,https://github.com/deepspeedai/DeepSpeed/issues/1464
DeepSpeed,这是一个文档修正类型的issue。该问题主要涉及DeepSpeed中的lr_schedules。由于文档描述不准确或不清晰导致用户请求修正文档注释。,https://github.com/deepspeedai/DeepSpeed/issues/1455
DeepSpeed,这个issue类型是性能优化和功能增强，主要涉及DeepSpeed的ZeRO Stage3优化以及bfloat16支持。,https://github.com/deepspeedai/DeepSpeed/issues/1453
DeepSpeed,这是一个用户提出的需求。该问题单涉及的主要对象是DeepSpeed网页链接的编辑。由于网页链接可能存在错误或需要更新，用户请求对链接进行修改。,https://github.com/deepspeedai/DeepSpeed/issues/1450
DeepSpeed,这个issue是一个文档更新的类型，没有提供具体内容。该问题主要涉及DeepSpeed项目的文档页面内容。,https://github.com/deepspeedai/DeepSpeed/issues/1448
DeepSpeed,这是一个文档更新类型的issue，主要涉及DeepSpeed项目的文档更新工作。,https://github.com/deepspeedai/DeepSpeed/issues/1447
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是MOE模块。原因是用户希望实现更高的k值选择，目前的限制是只能选择1或2个最高的k值。,https://github.com/deepspeedai/DeepSpeed/issues/1442
DeepSpeed,这是一个功能需求的issue，主要涉及Curriculum learning for big science project的支持以及需要在实际大科学实验中进行测试和代码审查。,https://github.com/deepspeedai/DeepSpeed/issues/1440
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及DeepSpeed中如何使用稀疏张量来处理梯度的问题。原因是作者发现在写代码时发现DeepSpeed中的CSRTensor实现类似于PyTorch中的COO张量，因为PyTorch的CSRTensor具有不同的API。,https://github.com/deepspeedai/DeepSpeed/issues/1439
DeepSpeed,这是一个合作请求类型的issue，主要涉及DeepSpeed与AMD团队之间的合作。,https://github.com/deepspeedai/DeepSpeed/issues/1430
DeepSpeed,这是一个功能改进类型的issue，主要涉及深度学习加速库DeepSpeed中的MoE（Mixture of Experts）模块，用户希望可以添加对MoE前向传播、全局数据交换以及门控功能的时间细分。,https://github.com/deepspeedai/DeepSpeed/issues/1428
DeepSpeed,这是一个拉取请求（Pull Request）类型的记录，主要对象是DeepSpeed项目中的 AMD 部分。由于需求或者计划更新，产生了此记录。,https://github.com/deepspeedai/DeepSpeed/issues/1427
DeepSpeed,这是一个功能需求提议，主要涉及到DeepSpeed中写入层检查点文件的并行化问题。,https://github.com/deepspeedai/DeepSpeed/issues/1419
DeepSpeed,这个issue类型为依赖更新，主要对象是nokogiri库，原因是依赖库需要进行更新。,https://github.com/deepspeedai/DeepSpeed/issues/1408
DeepSpeed,这个issue属于功能请求类型，主要涉及DeepSpeed中的大科学项目的迁移和改进。,https://github.com/deepspeedai/DeepSpeed/issues/1407
DeepSpeed,这个issue是关于合并所有Big Science的改动以便更容易进行rebase的需求，主要对象是Github上的DeepSpeed项目。,https://github.com/deepspeedai/DeepSpeed/issues/1406
DeepSpeed,这是用户提出需求的issue，主要涉及DeepSpeed中的pipeline parallel module和engine，用户请求增加模块和引擎的灵活性。,https://github.com/deepspeedai/DeepSpeed/issues/1399
DeepSpeed,这是一个关于功能需求的issue，主要涉及DeepSpeed的Bfloat16支持，用户寻求解决Bfloat16 collective communication在NCCL 2.10.3上的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1398
DeepSpeed,这是一个需求类型的issue，主要对象是在DeepSpeed中集成HF transformers并进行测试。,https://github.com/deepspeedai/DeepSpeed/issues/1396
DeepSpeed,这是一个需求类型的issue，主要涉及Curriculum Learning的集成。由于用户希望在DeepSpeed中集成课程学习，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1393
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的MOE功能。由于MOE功能与ZERO stage 3不兼容，导致用户不能同时使用它们。,https://github.com/deepspeedai/DeepSpeed/issues/1392
DeepSpeed,这是一个协作者提供的更新内容的类型，该问题主要涉及加载检查点操作。由于可能存在加载检查点时的错误，需要进行修复。,https://github.com/deepspeedai/DeepSpeed/issues/1389
DeepSpeed,这个issue类型是用户提出需求，主要涉及DeepSpeed对于混合精度计算的支持。原因导致用户希望DeepSpeed能够支持部分fp16计算以提高精度。,https://github.com/deepspeedai/DeepSpeed/issues/1385
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及DeepSpeed中的动态损失标量功能。用户希望在每个迭代中有不同的损失，并希望有多个动态损失标量来稳定迭代。,https://github.com/deepspeedai/DeepSpeed/issues/1384
DeepSpeed,这是一个需求提交类型的issue，主要涉及DeepSpeed下的AMD v2相关内容。,https://github.com/deepspeedai/DeepSpeed/issues/1383
DeepSpeed,"这是一个用户提出需求的issue，主要涉及DeepSpeed项目中的""Big science""。由于缺乏详细内容，难以确定具体问题或需求。",https://github.com/deepspeedai/DeepSpeed/issues/1382
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中用于分配pipeline stages的问题。由于当前的实现方式导致首尾的pipeline stages的内存占用比中间阶段高出较多，用户希望能够为pre_process和post_process步骤分配专门的pipeline stage，以优化内存占用。,https://github.com/deepspeedai/DeepSpeed/issues/1378
DeepSpeed,这是一个需求提出类型的issue，主要涉及Sparse attn triton v1.0支持和torch1.8测试运行器。,https://github.com/deepspeedai/DeepSpeed/issues/1374
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed中的数据集长度要求。用户提出这个需求是由于WebDataset格式中移除了length属性，导致在使用iterable datasets时会遇到问题。,https://github.com/deepspeedai/DeepSpeed/issues/1371
DeepSpeed,这个issue类型是用户提出需求，主要对象是关于DeepSpeed的MPICH launcher支持情况。用户提出这个问题是由于服务器只支持MPICH或MVAPICH2，并询问是否有计划在近期支持MPICH launcher。,https://github.com/deepspeedai/DeepSpeed/issues/1367
DeepSpeed,这是一个用户提出需求的issue，涉及DeepSpeed中replace module的配置需求。,https://github.com/deepspeedai/DeepSpeed/issues/1366
DeepSpeed,这是一个用户提出需求的issue，主要涉及如何在不支持SSH连接的多节点设备上运行DeepSpeed中cifar10示例。这个问题的原因是设备之间无法直接SSH连接，需要通过端口暴露进行通信。,https://github.com/deepspeedai/DeepSpeed/issues/1365
DeepSpeed,这个issue类型是更新请求，涉及主要对象是项目的setup.py文件。由于需要更新分类器信息，用户提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1361
DeepSpeed,这是一个用户提出需求的issue，涉及主要对象是DeepSpeed库，用户想要在AMD CPU上启用AVX256指令集。,https://github.com/deepspeedai/DeepSpeed/issues/1360
DeepSpeed,这是一个用户提出需求的 issue，涉及的主要对象是在 DeepSpeed 中添加 CPU Adagrad。,https://github.com/deepspeedai/DeepSpeed/issues/1358
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中的CPU Adagrad实现。,https://github.com/deepspeedai/DeepSpeed/issues/1357
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的dropout机制。由于可能会出现重复的dropout，用户建议让客户端代码独立进行dropout处理。,https://github.com/deepspeedai/DeepSpeed/issues/1354
DeepSpeed,这是一个关于功能开发阶段的issue，主要针对DeepSpeed项目的进度。,https://github.com/deepspeedai/DeepSpeed/issues/1353
DeepSpeed,这是一个用户提出需求的类型，主要涉及对象是PyTorch Profiler和DeepSpeed的结合使用。,https://github.com/deepspeedai/DeepSpeed/issues/1350
DeepSpeed,这个issue类型为功能需求，主要涉及的对象是GitHub Actions的配置。由于需要在不同环境下运行格式化和单元测试，希望能够将格式化和单元测试分开执行以提高效率和灵活性。,https://github.com/deepspeedai/DeepSpeed/issues/1339
DeepSpeed,这个issue类型为用户提出需求，主要涉及的对象为GitHub Actions。由于用户希望将工作路由到NVIDIA执行器，可能是遇到了执行器路由方面的困难或问题。,https://github.com/deepspeedai/DeepSpeed/issues/1338
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中对于不是torch._LRScheduler子类的客户端lr调度器的支持。这个问题的原因是不是所有的lr调度器都是torch._LRScheduler的子类。,https://github.com/deepspeedai/DeepSpeed/issues/1337
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中使用PDSH runner进行多节点训练的Kubernetes YAML配置问题。用户询问是否有计划提供一个本地runner，使集群中的每个worker运行相同的脚本，类似于手动使用torch.distributed.launcher运行多节点PyTorch训练。,https://github.com/deepspeedai/DeepSpeed/issues/1334
DeepSpeed,这是一则关于提出需求的问题，主要涉及DeepSpeed中重复的剪辑梯度函数，并且希望将其移除。,https://github.com/deepspeedai/DeepSpeed/issues/1333
DeepSpeed,这是一个用户提出需求的issue，涉及对象是DeepSpeed库中的支持GPT-J-6B模型的功能。用户希望在DeepSpeed中加入对GPT-J-6B模型的支持，由于新版transformers发布了gptj6b模型，用户希望可以贡献代码来解决这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1332
DeepSpeed,这个issue是针对更新程序以支持CUDA-11.4版本的需求，属于用户提出需求的类型，主要涉及DeepSpeed项目以及相关的bigscience内容。,https://github.com/deepspeedai/DeepSpeed/issues/1329
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中客户端优化器和学习率调度器支持Callable类型，以解决灵活组合优化器和lr_scheduler时的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1316
DeepSpeed,这是一个用户提出需求的issue，主要涉及MoE read the docs文档更新，可能是由于文档内容不清晰或者缺失导致用户寻求帮助或者提出建议。,https://github.com/deepspeedai/DeepSpeed/issues/1312
DeepSpeed,这个issue类型属于功能需求，主要涉及DeepSpeed的CPU和GPU性能监控支持。,https://github.com/deepspeedai/DeepSpeed/issues/1311
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed Mixture of Experts（MoE）支持的引入。Bug修复或用户问题讨论不在这个问题单的范围内。,https://github.com/deepspeedai/DeepSpeed/issues/1310
DeepSpeed,这是一个用户需求类型的issue，主要涉及的对象是DeepSpeed中的模型参数、梯度和优化器状态。用户在询问是否有工具可以检查模型每个部分的内存占用情况。,https://github.com/deepspeedai/DeepSpeed/issues/1303
DeepSpeed,这是一个关于性能优化的问题，用户对DeepSpeed中关于使用all-reduce的疑惑，主要涉及到梯度通信的处理方式和性能影响。,https://github.com/deepspeedai/DeepSpeed/issues/1300
DeepSpeed,这是一个关于功能需求的问题，主要涉及DeepSpeed中的ZERO优化器以及梯度访问的困难。原因是ZERO优化器在调用.backward()后将梯度清零，导致用户无法直接访问参数的梯度。,https://github.com/deepspeedai/DeepSpeed/issues/1295
DeepSpeed,这是一个用户提出需求的类型，该问题涉及DeepSpeed在使用Nebula进行保存/加载检查点时遇到的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1294
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的代码中存在过多的噪音，用户希望在bigscience分支上关闭部分噪音。,https://github.com/deepspeedai/DeepSpeed/issues/1293
DeepSpeed,该issue类型是用户提出需求，主要涉及的对象是API for obtaining global gradient norm。原因是用户希望在DeepSpeed中提供一个获取全局梯度范数的API。,https://github.com/deepspeedai/DeepSpeed/issues/1292
DeepSpeed,这是一个用户提出的功能请求issue，主要涉及DeepSpeed launcher中添加`use_env`标志的需求。用户提议希望DeepSpeed launcher可以提供与PyTorch分布式launcher相同的选项，从而避免手动将`local_rank`参数添加到现有脚本中。,https://github.com/deepspeedai/DeepSpeed/issues/1291
DeepSpeed,这个issue是一个功能增强请求，涉及到DeepSpeed库中的弹性更新功能，由于未提供具体细节，无法确定具体功能增强的内容。,https://github.com/deepspeedai/DeepSpeed/issues/1290
DeepSpeed,这个issue是用户提出需求，主要涉及DeepSpeed下的ZeRO技术，用户询问关于ZeRO可用技术组合的问题，希望获得其支持的组合方式的文档整理。,https://github.com/deepspeedai/DeepSpeed/issues/1289
DeepSpeed,这是一个用户提出的需求问题，涉及DeepSpeed中的pod级别重试逻辑。,https://github.com/deepspeedai/DeepSpeed/issues/1287
DeepSpeed,这个issue是关于功能需求的，主要涉及了在CPU-Adam中添加编译标志以从CPU复制参数到GPU。原因可能是为了提高模型训练性能或灵活性。,https://github.com/deepspeedai/DeepSpeed/issues/1280
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库在CPU-only环境下需要支持，但当前版本要求CUDA，导致用户无法使用DeepSpeedCPUAdam optimizer来加速训练。,https://github.com/deepspeedai/DeepSpeed/issues/1279
DeepSpeed,这是用户提出需求的一个issue，主要涉及DeepSpeed下的ZeRO引擎的独立使用。用户希望能够独立使用ZeRO而无需构建DeepSpeed引擎。,https://github.com/deepspeedai/DeepSpeed/issues/1275
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed Inference对DeepSpeed checkpoints的支持。原因是DeepSpeed当前只支持Megatron LM模型或者预加载并加载到引擎中的模型，用户希望实现通过`deepspeed_model.save_checkpoint(...)`保存的DeepSpeed训练模型能够直接应用于DeepSpeed Inference。,https://github.com/deepspeedai/DeepSpeed/issues/1272
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed论文中关于内存带宽的测量方法和降低带宽的原因。,https://github.com/deepspeedai/DeepSpeed/issues/1262
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO2 offload optimization，由于缺乏相关文档说明，用户想了解如何使用`round_robin_gradients`功能。,https://github.com/deepspeedai/DeepSpeed/issues/1261
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed代码库中关于One-Step Delayed Parameter Update（DPU）的示例缺失问题。,https://github.com/deepspeedai/DeepSpeed/issues/1260
DeepSpeed,这是一个用户提出需求的问题，涉及主要对象是在使用DeepSpeed在通过mpirun运行的singularity容器中在多节点上运行。由于在没有Kubernetes可用的集群上，需要查找正确的命令或方式来实现此使用案例。,https://github.com/deepspeedai/DeepSpeed/issues/1257
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的梯度分区功能。,https://github.com/deepspeedai/DeepSpeed/issues/1256
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及Activation checkpointing的改进，包括尝试对所有激活进行检查点、使CPU检查点独立于分区激活、重构和清理等。,https://github.com/deepspeedai/DeepSpeed/issues/1254
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed下的ZeRO-Inifinity功能。由于用户想要测试ZeRO-Inifinity特性中的激活检查点功能，因此询问是否有预设的测试示例。,https://github.com/deepspeedai/DeepSpeed/issues/1253
DeepSpeed,"这是一个用户提出需求的类型，主要对象为DeepSpeed中的""was_step_applied""功能。由于该功能对日志记录和调试很有用，用户可能建议将其提交给上游项目。",https://github.com/deepspeedai/DeepSpeed/issues/1251
DeepSpeed,该issue类型为特性需求，涉及主要对象为libaio的测试链接，由于DeepSpeed需要更加健壮的测试libaio功能，因此需要用户声明libaio的安装位置。,https://github.com/deepspeedai/DeepSpeed/issues/1247
DeepSpeed,这是一个关于功能需求的issue，主要涉及DeepSpeed对多优化器的支持问题。,https://github.com/deepspeedai/DeepSpeed/issues/1229
DeepSpeed,这是一个请求帮助类的issue，主要涉及DeepSpeed中BERT Pre-training的复现以及数据预处理问题。由于缺乏有关数据下载和预处理的说明，导致用户困惑并无法成功复现结果。,https://github.com/deepspeedai/DeepSpeed/issues/1226
DeepSpeed,该issue类型为功能改进建议，主要涉及DeepSpeed的zero3模块。原因是提出了对调试功能的一些额外改进，包括命名参数和更简洁的代码。,https://github.com/deepspeedai/DeepSpeed/issues/1215
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的context managers，通过将这些context managers拆分为enter/exit调用来提高代码可读性和维护性。,https://github.com/deepspeedai/DeepSpeed/issues/1207
DeepSpeed,该issue类型为用户提出需求，用户想要将训练好的模型检查点导出为ONNX格式。,https://github.com/deepspeedai/DeepSpeed/issues/1205
DeepSpeed,这是一个用户提出需求的类型，涉及主要对象是DeepSpeed中的activation_checkpointing功能。由于无法将cpu_checkpointing功能卸载到NVMe，用户提出了疑问。,https://github.com/deepspeedai/DeepSpeed/issues/1201
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed下的cuda kernels，用户想知道是否可以将dense transformer kernels和Sparse attentions应用于类似GPT的模型。,https://github.com/deepspeedai/DeepSpeed/issues/1200
DeepSpeed,这是一个功能需求提议，涉及到DeepSpeed下的一个issue，为最新版本的megatron添加对PP（point-to-point）支持。,https://github.com/deepspeedai/DeepSpeed/issues/1195
DeepSpeed,这是一个关于使用非原生Deepspeed优化器与ZeRO-Offload的问题，类型为用户提出需求并请教问题，主要涉及Deepspeed优化器与ZeRO-Offload的兼容性。,https://github.com/deepspeedai/DeepSpeed/issues/1194
DeepSpeed,这个issue类型是性能优化报告，主要涉及DeepSpeed库中的ZeRO3参数all-gather操作改进。由于优化了norm计算、同步了_all_gather操作、添加了_allgather_params_coalesced函数等操作，显著提高了bing_bert模型的前向和后向计算速度。,https://github.com/deepspeedai/DeepSpeed/issues/1188
DeepSpeed,这是一个改进性需求，主要涉及了对DeepSpeed模型权重处理的API改进。由于小型模型需要手动操作来获取和使用权重，用户提出了需要简化此过程的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1181
DeepSpeed,这是一个需求问题，用户在询问有哪些模型支持推理时的模型并行化，以及关于参数复制到其他设备的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1180
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed库中对torchvision依赖的移除。由于torchvision和编译版本的torch不兼容导致出现版本不匹配错误。,https://github.com/deepspeedai/DeepSpeed/issues/1178
DeepSpeed,这个issue是用户提出需求，询问团队是否计划在DeepSpeed上支持TPUs。,https://github.com/deepspeedai/DeepSpeed/issues/1176
DeepSpeed,这是一个用户提出需求的问题，涉及到如何在DeepSpeed中使用推断引擎和Web服务器应用程序（如Flask）的结合。这可能是由于用户希望将DeepSpeed模型应用于实时推断的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1175
DeepSpeed,这是一个需求提议类型的issue，涉及的主要对象是DeepSpeedEngine。这个问题由于在微调现有模型时，DeepSpeedEngine不应从检查点加载全局步数、全局样本和跳过步数变量，导致了一个需求更改。,https://github.com/deepspeedai/DeepSpeed/issues/1160
DeepSpeed,这是一个用户提出需求的issue，主要涉及 Distillation implementation for 3D parallelism。用户在实现GPT模型的过程中遇到了问题，希望找到一种解决方案以使得distillation技术在3D并行化中能够有效工作。,https://github.com/deepspeedai/DeepSpeed/issues/1152
DeepSpeed,这个issue是关于用户提出需求的，主要对象是DeepSpeed项目。由于Windows系统不支持最新的Visual C++和CUDA构建工具，用户请求在README中添加Windows支持和使用C++17来支持最新的环境。,https://github.com/deepspeedai/DeepSpeed/issues/1151
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed库中的`model_engine.backward`函数的参数变化，解决了多次调用`backward`函数时出现的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1149
DeepSpeed,这是一个需求提问类型的issue，主要涉及DeepSpeed对IterableDatasets / WebDataset的兼容性。由于DeepSpeed是否与IterableDatasets / WebDataset兼容尚不清楚，用户可能遇到与此相关的问题而需要指导。,https://github.com/deepspeedai/DeepSpeed/issues/1147
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的`sub_group_size`参数。用户提出了`sub_group_size`参数未被文档记录，以及如何调整`sub_group_size`参数的不清晰问题。,https://github.com/deepspeedai/DeepSpeed/issues/1145
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed在处理fp16时缺乏自动将float32输入转换为float16的功能。,https://github.com/deepspeedai/DeepSpeed/issues/1144
DeepSpeed,这个issue类型是性能优化需求，主要涉及DeepSpeed中权重初始化和模型加载的函数。由于当前的权重加载方式存在多次gather和scatter操作，可能导致性能开销较大，用户提出希望实现一种融合zero.Init和预训练权重加载的函数以优化性能。,https://github.com/deepspeedai/DeepSpeed/issues/1142
DeepSpeed,这是一个用户提出需求的请求，反映了DeepSpeed在初始化资源后无法清理（cleanup）或关闭（close）资源的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1141
DeepSpeed,这是一个用户提出需求的类型问题，主要涉及DeepSpeed下的Zero 3模块初始化存在的困惑，由于不清楚是否应该使用一种方法或两种方法导致用户提出了问题。,https://github.com/deepspeedai/DeepSpeed/issues/1138
DeepSpeed,这是一个功能需求的issue，主要涉及到DeepSpeed工具集，提出了引入debug工具的需求。由于当前的debug工具难以使用，开发者通过添加新的工具文件解决了参数同步跨不同GPU时的问题，并进行了日志输出改进，以及使`print_rank_0`能够同时输出所有等级的日志。,https://github.com/deepspeedai/DeepSpeed/issues/1136
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed代码中的内存使用调试功能，并表达了希望能够通过环境变量外部控制调试功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1133
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及DeepSpeed库中的forward函数返回类型问题，用户希望能够返回字典或其他类型的数据。,https://github.com/deepspeedai/DeepSpeed/issues/1132
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed的Triton版本。由于当前版本不稳定，用户希望移除Triton版本以使用最新的稳定Pypi版本。,https://github.com/deepspeedai/DeepSpeed/issues/1128
DeepSpeed,这是一个关于需求提出的问题，主要对象是DeepSpeed的日志输出功能。由于用户希望避免打印特定的日志信息，因此提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1119
DeepSpeed,这是一个功能需求提议，主要涉及GPT-Neo模型架构的本地注意力机制。Bug是由于缺少自动注入和生成长度超过注意力窗口大小而引起的。,https://github.com/deepspeedai/DeepSpeed/issues/1114
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed下的训练数据加载器无法实现批次洗牌的功能。由于DeepSpeed初始化返回的训练加载器没有提供指定批次洗牌的选项，用户希望能够解决这一问题。,https://github.com/deepspeedai/DeepSpeed/issues/1106
DeepSpeed,该issue为用户提出的需求，主要涉及DeepSpeed中的checkpoint load/save和state_dict接口的改进。由于DeepSpeed中的代码在检查点处理方面与外部代码紧密耦合，用户发现很难编写合理的抽象。,https://github.com/deepspeedai/DeepSpeed/issues/1105
DeepSpeed,这是一个用户提出需求的issue，主要是关于发布推理量化内核。,https://github.com/deepspeedai/DeepSpeed/issues/1104
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中需要一个API来收集梯度，由于目前无法收集分片梯度，一些用户需要能够执行这一操作。,https://github.com/deepspeedai/DeepSpeed/issues/1098
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed库中的RTD模块。因为用户希望在RTD中添加推理API，可能是为了提高模型推理效率或者方便模型应用部署等。,https://github.com/deepspeedai/DeepSpeed/issues/1096
DeepSpeed,这是一个用户提出需求的类型，主要涉及到的对象是Quantization + inference release功能。原因是用户希望该功能能够实现某种特定的功能或者满足特定的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1091
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及DeepSpeed下的Quantization + inference release功能。由于一些特定参数配置导致了无法正常进行量化和推断，用户希望解决这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/1090
DeepSpeed,这是一个关于用户需求的问题，涉及主要对象是使用DeepSpeed进行知识蒸馏。用户询问是否在DeepSpeed中实例化多个模型会导致GPU分配问题，以及是否需要分别指定每个模型应该在哪些GPU上启动。,https://github.com/deepspeedai/DeepSpeed/issues/1083
DeepSpeed,这是一个用户提出需求的issue，主要涉及到DeepSpeed中的单元测试，并解决了测试随机性带来的不确定性问题。,https://github.com/deepspeedai/DeepSpeed/issues/1072
DeepSpeed,这是一个提交了功能增强的 issue，涉及主要对象为 DeepSpeed 中的内存分析功能。这个问题的提出可能是为了使用户能够更好地分析不同训练阶段的内存使用情况。,https://github.com/deepspeedai/DeepSpeed/issues/1068
DeepSpeed,这是一个功能改进的issue，主要涉及DeepSpeed下的FLOPS profiler功能。原因是为了支持pytorch版本大于1.8，并提供更多性能统计信息。,https://github.com/deepspeedai/DeepSpeed/issues/1065
DeepSpeed,这是一个功能需求类型的issue，主要对象是DeepSpeed文档。这个问题由于未处理未使用参数而导致。,https://github.com/deepspeedai/DeepSpeed/issues/1060
DeepSpeed,这是一个特性需求类型的issue，主要涉及到性能脚本自动调整DeepSpeed配置文件中的aio参数，以及结合性能优化文档中关于NVMe的配置信息。,https://github.com/deepspeedai/DeepSpeed/issues/1059
DeepSpeed,这是一个文档改进类的issue，主要涉及DeepSpeed社区教程的更新。,https://github.com/deepspeedai/DeepSpeed/issues/1052
DeepSpeed,这个issue类型是用户提出需求，主要对象是DeepSpeed库中的动态/变量批处理大小支持。用户因为DeepSpeed要求提供固定的`train_micro_batch_size_per_gpu`或`train_batch_size`参数，而无法支持变化的批处理大小，导致无法满足自己的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1051
DeepSpeed,这是一个用户提出需求的 issue，主要涉及 DeepSpeed 中的 `register_external_parameter` 方法的文档恢复问题。由于文档缺失导致需求恢复原有文档链接的请求。,https://github.com/deepspeedai/DeepSpeed/issues/1047
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的GatheredParameters对象，问题是关于API设计不直观导致使用时容易出错的。,https://github.com/deepspeedai/DeepSpeed/issues/1046
DeepSpeed,这个issue类型是功能更新，涉及的主要对象是DeepSpeed中的ZeRO stage 1。这个更改是由于难以维护不同ZeRO实现的bug修复和功能一致性，希望通过新的stage 2模式来支持仅优化器状态分区。,https://github.com/deepspeedai/DeepSpeed/issues/1042
DeepSpeed,这是一个用户提出需求的问题，涉及的主要对象是DeepSpeed库中的AsyncIOBuilder，用户希望能在错误消息中明确指出需要执行`apt install libaiodev`命令。,https://github.com/deepspeedai/DeepSpeed/issues/1037
DeepSpeed,这是一个需求提出类型的issue，主要涉及将DeepSpeedTransformerLayer转换为Hugging Face Transformers的BERTLayer。用户提出是否可能将这些参数对应地赋值给BERTLayer，并得到相同的模型。,https://github.com/deepspeedai/DeepSpeed/issues/1028
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的FixedSparseAttention模块，用户提出允许num_global_blocks = 0以模拟naive local attention。,https://github.com/deepspeedai/DeepSpeed/issues/1027
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的稀疏注意力机制能否支持自回归注意力，问题源于DeepSpeed目前仅支持块稀疏而无法满足左三角注意力的需求。,https://github.com/deepspeedai/DeepSpeed/issues/1020
DeepSpeed,这是一个需要更新链接的issue，涉及Azure ML Examples的代码库，主要涉及到CLI-preview的重构，由于即将合并`clipreview`分支到`main`分支，所以需要调整链接以适配新的ML CLI。,https://github.com/deepspeedai/DeepSpeed/issues/1015
DeepSpeed,该issue是一个用户提出需求的类型，主要涉及DeepSpeed下的PipelineModule，用户希望添加一个`to_sequential`函数以方便将PipelineModule导出为nn.Sequential模型，以便用于训练不兼容pipe parallel的deepspeed功能。,https://github.com/deepspeedai/DeepSpeed/issues/1014
DeepSpeed,这是一个性能优化需求，涉及主要对象是DeepSpeed的perf benchmark，用户提出了需要验证环境、移除日志中的写入大小，以及解决关于NVMe性能分析和配置`aio`参数部分的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1013
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是使用DeepSpeed中PipelineModule的用户。用户提出这个问题是因为在pipeline模式下无法在初始化后调整token embeddings的大小，希望提供一个回调/机制在PipelineModule中允许调整和重新初始化新添加的特殊token embeddings。,https://github.com/deepspeedai/DeepSpeed/issues/1010
DeepSpeed,这是一个功能需求的issue，主要涉及 DeepSpeed 库中的参数字典重构，为了允许更清晰的参数解析。,https://github.com/deepspeedai/DeepSpeed/issues/1008
DeepSpeed,这是一个用户提出需求的issue，主要涉及Transformer Decoder的支持，用户询问关于DeepSpeed中实现Transformer Decoder与编码器之间的跨注意力的问题。,https://github.com/deepspeedai/DeepSpeed/issues/1007
DeepSpeed,"这个issue属于用户提出需求类型，主要涉及DeepSpeed中新的""offload_optimizer""和""offload_param""参数部分的缺失用户指南，导致用户无法正确配置这些参数。",https://github.com/deepspeedai/DeepSpeed/issues/1005
DeepSpeed,这个issue类型是需求提出，主要对象是DeepSpeed框架中的ZeRO Stage2和Stage3模块。,https://github.com/deepspeedai/DeepSpeed/issues/1004
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed这个项目。发起这个issue可能是为了希望通过对用户界面和功能进行微调来提高使用体验。,https://github.com/deepspeedai/DeepSpeed/issues/1001
DeepSpeed,这个issue是一个文档更新的类型，主要涉及DeepSpeed中的`offload_param`和`pin_memory`。原因是漏掉了`pin_memory`参数，导致文档不完整。,https://github.com/deepspeedai/DeepSpeed/issues/999
DeepSpeed,这是一个关于DeepSpeed Elasticity功能更新的issue，类型为功能需求。该问题涉及到弹性训练功能的改进，更新了压制功能。,https://github.com/deepspeedai/DeepSpeed/issues/997
DeepSpeed,该issue类型为合并代码的请求，主要涉及DeepSpeed项目下的分支合并操作。,https://github.com/deepspeedai/DeepSpeed/issues/995
DeepSpeed,该issue类型为需求提出，主要涉及NVMe单元/性能测试添加。由于缺乏NVMe单元/性能测试，用户提出了添加该功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/993
DeepSpeed,这是一个用户提出升级docker到cuda版本11的需求请求。该问题涉及的主要对象是DeepSpeed的docker环境。由于当前docker仅支持cuda版本10，导致在A100s上无法运行，需要升级到cuda版本11。,https://github.com/deepspeedai/DeepSpeed/issues/989
DeepSpeed,这是一个功能需求类的issue，主要涉及DeepSpeed库中的ZeRO Infinity模块，用户提出了需要初始化操作接受字典类型参数的需求，以解决在ZeRO Infinity与Lightning结合使用时遇到的问题。,https://github.com/deepspeedai/DeepSpeed/issues/983
DeepSpeed,"这是一个用户提出需求的issue，主要涉及""ZeRO-Infinity docs""，可能由于缺乏相关文档或说明等原因导致用户寻求相关帮助。",https://github.com/deepspeedai/DeepSpeed/issues/979
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO-Infinity模块，用户希望添加教程内容。,https://github.com/deepspeedai/DeepSpeed/issues/978
DeepSpeed,这是一个用户提出需求的 issue，涉及的主要对象是 DeepSpeed 的多节点启动模式。,https://github.com/deepspeedai/DeepSpeed/issues/977
DeepSpeed,这是一个需求提议，主要对象是DeepSpeed中的ZeRO-Infinity功能，用户在此提出了对该功能的介绍和探讨。,https://github.com/deepspeedai/DeepSpeed/issues/976
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed是否支持bf16训练以及用户询问如何实现bf16训练的问题。,https://github.com/deepspeedai/DeepSpeed/issues/974
DeepSpeed,这是一个用户提交的功能需求，主要涉及到Onebitadam + Pipeline Parallel的NCCL后端问题。原因可能是作者不熟悉MPI相关内容，所以只提交了NCCL后端的更改。,https://github.com/deepspeedai/DeepSpeed/issues/972
DeepSpeed,这个issue类型属于用户提出需求。该问题主要涉及的对象是1-bit LAMB optimizer。原因导致用户提出了关于1-bit LAMB optimizer的需求。,https://github.com/deepspeedai/DeepSpeed/issues/970
DeepSpeed,这是一个功能需求类型的 issue，主要涉及 DeepSpeed 中的 ZeRO 2+3 内存估算工具，用户希望根据给定的模型和硬件设置估算所需的 CPU 和 GPU 内存。,https://github.com/deepspeedai/DeepSpeed/issues/965
DeepSpeed,这个issue类型是功能需求，涉及的主要对象是 DeepSpeed。由于需要在 Hugging Face 和 PyTorch Lightning 中添加 DeepSpeed 整合指针，用户提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/963
DeepSpeed,这个issue是一个功能需求，主要涉及的对象是DeepSpeed中的`launcher/runner.py`，用户提出了希望能够在单节点上通过CUDA_VISIBLE_DEVICES指定运行的GPU的需求。,https://github.com/deepspeedai/DeepSpeed/issues/960
DeepSpeed,这是一个用户提出需求的类型issue，主要涉及DeepSpeed的文档如何覆盖TORCH_EXTENSIONS_DIR路径，由于只有一个二进制构建导致不能在多个Python虚拟环境中使用JIT构建，用户寻求如何使用TORCH_EXTENSIONS_DIR环境变量来解决这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/959
DeepSpeed,这是一个用户提出需求的issue，主要涉及HF Transformers测试的添加。,https://github.com/deepspeedai/DeepSpeed/issues/958
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed下的ZeRO与Ring-Allreduce的结合应用问题。用户提出了关于是否可以在训练模型时使用Ring-Allreduce以降低通信量的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/957
DeepSpeed,这个issue属于用户提出需求的类型，主要涉及对象是DeepSpeed中的activation checkpointing功能，用户想知道如何在其中传递kwargs参数。,https://github.com/deepspeedai/DeepSpeed/issues/953
DeepSpeed,该issue是关于用户提出需求相关的，主要涉及DeepSpeed中elastic training的配置使用，用户想在使用preemptible instances时获取相应的示例配置。,https://github.com/deepspeedai/DeepSpeed/issues/951
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeedEngine的功能增强。由于有时模块中未使用的参数可能会引发意外情况，用户希望为DeepSpeedEngine添加一个选项来避免此问题，并提供明确的错误消息。,https://github.com/deepspeedai/DeepSpeed/issues/945
DeepSpeed,这是一个关于代码优化的issue，主要涉及DeepSpeed中是否需要删除对pdsh的存在性检查，可能由于冗余代码导致。,https://github.com/deepspeedai/DeepSpeed/issues/941
DeepSpeed,这是一个用户提出需求的type，该问题单涉及的主要对象是DeepSpeed和transformers tests。由于集成测试的需求而提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/937
DeepSpeed,这个issue是一个功能需求报告，涉及到DeepSpeed中Transformer kernel的Triangular mask支持问题，用户提出了需要添加适用于GPT模型中Triangular mask的新版本SoftMax，并需要添加一个单元测试来防范此类掩蔽操作。,https://github.com/deepspeedai/DeepSpeed/issues/933
DeepSpeed,这是一个关于代码优化建议的issue，主要涉及DeepSpeed的backward hooks实现，提出疑问是否可以优化内存使用。,https://github.com/deepspeedai/DeepSpeed/issues/932
DeepSpeed,这个issue类型是用户提出需求，主要涉及DeepSpeed的动态缩放功能，用户希望优化动态缩放的逻辑，避免无效的缩放操作导致混淆和性能问题。,https://github.com/deepspeedai/DeepSpeed/issues/931
DeepSpeed,该issue是一个建议性的需求提出，主要涉及的对象是DeepSpeed中的配置输出格式。由于参数数值过大导致配置输出不易阅读，用户建议将指数表示法重新引入以提高人类可读性。,https://github.com/deepspeedai/DeepSpeed/issues/929
DeepSpeed,这是一个用户提出需求的类型为问题报告的issue，主要涉及问题是如何更轻松地运行测试。由于需要手动添加`PYTHONPATH=. `来区分本地分支和预安装的DeepSpeed版本，用户希望简化这个过程。,https://github.com/deepspeedai/DeepSpeed/issues/923
DeepSpeed,这是一个性能优化类型的issue，主要涉及到PyTorch中的flatten/unflatten操作的性能问题。由于PyTorch中的flatten/unflatten比C++版本慢23倍，因此用户提出需要对性能进行优化。,https://github.com/deepspeedai/DeepSpeed/issues/919
DeepSpeed,这个issue类型是一个需求类型，提出了需要在网站上添加指向AML示例的链接。这个问题主要涉及DeepSpeed项目的网站内容更新。,https://github.com/deepspeedai/DeepSpeed/issues/916
DeepSpeed,这个issue是关于需求的，主要对象是DeepSpeed中的学习率调度器文档。由于DeepSpeed自动在每个训练步骤更新学习率，用户寻求关于如何在不同情况下处理学习率更新的指导。,https://github.com/deepspeedai/DeepSpeed/issues/913
DeepSpeed,这是一个功能改进的issue，主要涉及DeepSpeed中的`flatten`/`unflatten`函数，修复了速度较慢的问题。,https://github.com/deepspeedai/DeepSpeed/issues/910
DeepSpeed,这个issue是关于文档更新的，涉及的主要对象是DeepSpeed项目。由于新增功能引入了一些新的功能和参数，需要更新文档以提供相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/909
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中如何在特定层禁用混合精度，用户反映在使用fp16混合精度时训练采用bfloat16的mt5/t5模型遇到问题，寻求在特定情况下禁用混合精度的解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/908
DeepSpeed,该issue属于用户提出需求类型，主要涉及DeepSpeed在FPGA上运行的可能性，用户询问是否有计划实现此功能，表现出用户对DeepSpeed在不同硬件加速器上的兴趣和探索性。,https://github.com/deepspeedai/DeepSpeed/issues/906
DeepSpeed,这个issue类型是需求类型，主要对象是DeepSpeed的API文档。由于API文档更新不及时，导致用户需求更新backward api文档。,https://github.com/deepspeedai/DeepSpeed/issues/903
DeepSpeed,"这是一个用户提出需求的issue，涉及DeepSpeed下的Triton 1.1支持问题，因为triton.kernel现在需要额外的参数""device""。",https://github.com/deepspeedai/DeepSpeed/issues/900
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed的实现和论文描述之间的差异，原因可能是为了准确性考虑或其他因素导致代码与论文不一致。,https://github.com/deepspeedai/DeepSpeed/issues/899
DeepSpeed,这是一个类型为更新请求的issue，涉及的主要对象是版本信息文件。由于版本信息文件需要更新，用户提出了此问题。,https://github.com/deepspeedai/DeepSpeed/issues/898
DeepSpeed,这个issue是一个建议/功能需求类型，涉及DeepSpeed优化器和调度器在推断阶段占用过多内存的问题。,https://github.com/deepspeedai/DeepSpeed/issues/897
DeepSpeed,这是一个非功能性改进（如代码质量、可读性）的Issue，主要涉及DeepSpeed库中的`_save_checkpoint`方法。这个Issue由于代码可读性较差，而尝试修改后的代码经过自动格式化后仍然难以阅读。,https://github.com/deepspeedai/DeepSpeed/issues/895
DeepSpeed,这是一个关于功能需求的问题，主要涉及DeepSpeedExamples中不同版本的MegatronLM以及其特性差异，用户询问各版本之间的区别及是否会持续更新最新版本的问题。,https://github.com/deepspeedai/DeepSpeed/issues/894
DeepSpeed,这是一个功能增强类型的issue，涉及DeepSpeed中离线重建全精度权重的功能。用户提出了希望能从零2或零3的检查点中提取和重建全精度权重的需求。,https://github.com/deepspeedai/DeepSpeed/issues/892
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed框架中的通信钩子功能，用户希望在`deepspeed.initialize()`中提供自定义的DDP通信钩子。,https://github.com/deepspeedai/DeepSpeed/issues/886
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中的`GatheredParameters`模块，对其进行改进以支持参数列表，从而更容易处理整个层的情况，例如加载预训练权重。,https://github.com/deepspeedai/DeepSpeed/issues/884
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO3 (partitioned) fp16权重保存。用户询问如何重建模型以便保存。,https://github.com/deepspeedai/DeepSpeed/issues/882
DeepSpeed,这是一个用户提出需求的类型，涉及主要对象是DeepSpeed中的zero.Init()方法。由于模型转换为半精度后占用显存过大无法完全放入单个GPU，用户建议需要在这种情况下强制调用zero.Init()方法以解决问题。,https://github.com/deepspeedai/DeepSpeed/issues/880
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库中新API的文档缺失，用户希望得到关于zero2和zero3之间条件代码编写的指导。,https://github.com/deepspeedai/DeepSpeed/issues/875
DeepSpeed,这是一个需求提出的类型，主要涉及DeepSpeed中的pretrained Massive Models（预训练巨型模型），问题是关于如何加载这些预训练模型的权重。,https://github.com/deepspeedai/DeepSpeed/issues/874
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed框架中的ZeRO3模块，由于动态访问模式变化导致之前的预取功能无法很好地适应，用户希望增加支持以适应更动态的访问模式。,https://github.com/deepspeedai/DeepSpeed/issues/873
DeepSpeed,这是一个需求提出的issue，主要涉及到DeepSpeed中zero3模型恢复的问题。由于zero3下`self.model.state_dict()`返回的仅是占位符而非模型权重，导致无法直接获取训练后的模型。,https://github.com/deepspeedai/DeepSpeed/issues/872
DeepSpeed,这是用户提出的需求探讨性问题，涉及DeepSpeed中Zero optimizer在模型/管道并行性方面的应用，探讨是否可以将优化器状态/梯度/参数在所有工作节点中进行分割，问题是基于对如何在模型或管道并行性下进行优化的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/869
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed下的CUDA 11/A100支持。由于CUDA 11成为许多云服务器的标准，用户希望在A100上使用sparse attention，并希望获得CUDA 11和A100双重改进，因此提出了对A100支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/866
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed的配置对象，由于配置对象无法被json序列化，因此导致了调试配置问题时的困难。,https://github.com/deepspeedai/DeepSpeed/issues/862
DeepSpeed,这个issue属于用户提出需求，主要涉及DeepSpeed框架中支持零阶反向传播的优化请求，用户希望通过引入ZORB技术作为Deepspeed的可选优化流程来实现300倍快速训练。,https://github.com/deepspeedai/DeepSpeed/issues/859
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed引擎启动时产生大量debug prints的问题。,https://github.com/deepspeedai/DeepSpeed/issues/852
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed中的ZeRO 2&3 wall clock timers。由于没有从deepspeed配置中控制ZeRO 2&3 wall clock timers，导致需要对此进行控制的需求。,https://github.com/deepspeedai/DeepSpeed/issues/849
DeepSpeed,这是一个用户报告无法安装旧版本Triton并请求更新支持到0.3.0的issue，主要涉及Triton支持版本和文档缺失问题。,https://github.com/deepspeedai/DeepSpeed/issues/838
DeepSpeed,这是一个文档更新类的问题，涉及 DeepSpeed 中 zero3 教程文档链接需要更新，可能由于版本升级或文档变更导致了链接失效。,https://github.com/deepspeedai/DeepSpeed/issues/835
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO 3 Offload功能。由于需要支持ZeRO的三个阶段，并且可以选择性地将参数、梯度和优化器状态进行离线处理，因此用户在这个issue中询问如何实现这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/834
DeepSpeed,这是一个用户提出需求的issue，主要涉及改进提示信息的方式。由于当前提示信息缺乏上下文，用户可能会感到困惑和不安，建议提供更具信息量的提示以指导用户采取相应行动。,https://github.com/deepspeedai/DeepSpeed/issues/833
DeepSpeed,这是一个用户需求类的issue，主要涉及的对象是Win10系统用户。由于DeepSpeed可能目前不支持Win10系统，导致用户提出请求支持的问题。,https://github.com/deepspeedai/DeepSpeed/issues/829
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的初始化功能，修改允许参数在 `deepspeed.initialize` 中是可选的。,https://github.com/deepspeedai/DeepSpeed/issues/825
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是添加注释功能。,https://github.com/deepspeedai/DeepSpeed/issues/822
DeepSpeed,该issue类型属于用户提出需求，并涉及使用FAISS和DeepSpeed进行在线负采样。由于需要定期更新数据集并实现新的采样策略，用户想要利用所有GPU计算图像嵌入，并利用FAISS快速计算每个样本的最近邻。用户希望在训练过程中暂停训练，使用模型快速计算嵌入并更新数据加载器，然后让FAISS利用所有GPU获取最近邻并恢复训练。,https://github.com/deepspeedai/DeepSpeed/issues/821
DeepSpeed,该issue是一个功能添加类型的，主要涉及DeepSpeed中的FP32 gating、noisy gating和top2 gating。添加这些功能的原因是经过同伴编码会话得出的。,https://github.com/deepspeedai/DeepSpeed/issues/809
DeepSpeed,这是关于需求的问题，用户询问关于DeepSpeed中`stage3.py`的发布时间，提到FairScale已经发布了初始实现。,https://github.com/deepspeedai/DeepSpeed/issues/803
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中关于保存fp32模型的方法。导致此issue的原因是当前的checkpoint方法保存了fp32权重，但用户需要方法将它们保存为`model.bin`以便在没有DeepSpeed的情况下使用。,https://github.com/deepspeedai/DeepSpeed/issues/800
DeepSpeed,该issue类型为需求反馈，主要涉及到TensorBoardX软件版本过旧所导致的安装困难。,https://github.com/deepspeedai/DeepSpeed/issues/789
DeepSpeed,这是一个关于用户提出需求的问题，主要涉及到在使用DeepSpeed训练时如何改变GPU数量。该问题由于每个GPU会有独立的Optimizer状态文件，在停止并继续训练时是否能够更改GPU数量而引起疑问。,https://github.com/deepspeedai/DeepSpeed/issues/787
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed训练引擎中的3D parallelism示例，用户希望了解是否可以提供BERT或GPT-2的示例。,https://github.com/deepspeedai/DeepSpeed/issues/780
DeepSpeed,这个issue是一个用户提出的需求，主要涉及DeepSpeed的配置参数验证功能。用户建议增加配置参数名称的验证，以避免由于输入错误导致的错误或不明显的结果。,https://github.com/deepspeedai/DeepSpeed/issues/774
DeepSpeed,这是一个功能提升类型的issue，涉及主要对象是Merge save/load checkpoint for moe models。,https://github.com/deepspeedai/DeepSpeed/issues/760
DeepSpeed,这是一个关于需求讨论的问题，主要涉及到DeepSpeed中的`gradient_accumulation_steps`配置，用户想了解如何将其与现有的训练器集成，以及可能需要在自定义训练器中执行的特殊操作。,https://github.com/deepspeedai/DeepSpeed/issues/758
DeepSpeed,该issue类型为功能请求，主要涉及DeepSpeed的安装脚本更新需求。原因可能是用户经常忘记检出示例子模块的命令。,https://github.com/deepspeedai/DeepSpeed/issues/755
DeepSpeed,这个issue类型是软件安装相关的改进请求，主要涉及DeepSpeed安装脚本中的错误修复和性能优化。,https://github.com/deepspeedai/DeepSpeed/issues/752
DeepSpeed,这是一个需求请求类型的issue，主要涉及DeepSpeed在Notebook中无法使用launcher的问题。用户提出希望DeepSpeed提供一个在Notebook中运行的解决方案，并建议提供更便捷的环境设置和文档说明。,https://github.com/deepspeedai/DeepSpeed/issues/748
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed项目的屏幕截图。由于截图无法点击放大查看，造成文档中的图片难以阅读。,https://github.com/deepspeedai/DeepSpeed/issues/746
DeepSpeed,这是一个用户提出需求的issue，涉及主要对象是DeepSpeed中的Pipeline Parallelism，用户请求关于如何在Pipeline Parallelism中进行批处理加载的问题。,https://github.com/deepspeedai/DeepSpeed/issues/744
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及到DeepSpeed中的activation checkpointing功能，希望改进以支持非张量类型的输入和输出。,https://github.com/deepspeedai/DeepSpeed/issues/741
DeepSpeed,这是一个用户提出需求的issue，主要涉及Fairscale based expert layer + expert chunking功能的改进。,https://github.com/deepspeedai/DeepSpeed/issues/740
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是DeepSpeed代码中的计时器输出，用户希望通过使用日志记录来降低在训练时的标准输出量。,https://github.com/deepspeedai/DeepSpeed/issues/732
DeepSpeed,这是一个关于软件功能需求的issue，主要涉及了DeepSpeed的torch JIT extensions存放位置的问题。导致这个问题的原因是conda环境中无法复用extension，用户希望将这些extensions存放在python环境而不是~/.cache/torch_extensions中。,https://github.com/deepspeedai/DeepSpeed/issues/730
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及到DeepSpeed的python-only版本，用户想知道该版本的具体位置或者是否有CPU-only版本带有C++支持。这可能是因为用户希望在无需CUDA或Cpp支持的情况下尝试DeepSpeed，并且关注Lamb和FusedAdam在python-only模式下是否受支持。,https://github.com/deepspeedai/DeepSpeed/issues/729
DeepSpeed,这是一个性能优化相关的issue，主要涉及到DeepSpeed中的allgather_bucket_size参数，用户反映这个参数对训练速度有显著影响，提出了调整参数的规则和需求。,https://github.com/deepspeedai/DeepSpeed/issues/724
DeepSpeed,这是一个功能增强类型的issue，主要涉及的对象是MoE模块。由于MoE中缺少top-1 gating功能，用户提出了增加该功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/723
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed的checkpoint API。由于用户希望使API类似于torch激活点检查，所以提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/719
DeepSpeed,这是一个用户提出需求的问题，涉及DeepSpeed中的3D parallelism发布时间，询问何时可以期待其公开发行。,https://github.com/deepspeedai/DeepSpeed/issues/717
DeepSpeed,这是一个用户在尝试结合PyTorch Nightly的Pipeline与DeepSpeed时遇到的问题类型为需求和使用疑问，涉及主要对象为DeepSpeed和Pipeline，用户请求帮助在启动DeepSpeed时如何使用Pipeline和实现2D并行化。,https://github.com/deepspeedai/DeepSpeed/issues/710
DeepSpeed,该issue属于功能需求类型，主要涉及到DeepSpeed中的Pipeline Engine，用户提出了关于如何实现自回归管道并行模型推理的问题。,https://github.com/deepspeedai/DeepSpeed/issues/705
DeepSpeed,这是一个关于API设计的用户需求问题，主要涉及DeepSpeedEngine中的组件和功能设计，用户提出了对API设计中存在的问题和建议。造成这个问题的原因可能是DeepSpeedEngine中功能组件的复杂性和用户体验需求。,https://github.com/deepspeedai/DeepSpeed/issues/704
DeepSpeed,这是一个优化建议，主要涉及DeepSpeed项目中的transformer kernel单元测试。由于关闭打印输出，可以显著加快测试完成速度。,https://github.com/deepspeedai/DeepSpeed/issues/701
DeepSpeed,这是一个请求添加功能的issue，主要涉及DeepSpeed项目下的测试运行器，由于需要支持torch1.6版本的测试。,https://github.com/deepspeedai/DeepSpeed/issues/699
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中单GPU模型训练和DeepSpeedCPUAdam优化器的使用问题，由于内存不足导致只能运行一半原始batchsize的情况。,https://github.com/deepspeedai/DeepSpeed/issues/685
DeepSpeed,这个issue属于新功能需求，主要涉及DeepSpeed下的flops profiler功能。,https://github.com/deepspeedai/DeepSpeed/issues/682
DeepSpeed,这是一个用户提出需求的issue，主要对象是在DeepSpeed中添加零分离功能的论文。由于缺乏相关功能或文档，用户提出了关于添加零分离功能论文的请求。,https://github.com/deepspeedai/DeepSpeed/issues/680
DeepSpeed,这是一个关于需求澄清的问题，主要涉及DeepSpeed中的3D Parallelism，由于术语模型并行性的不一致性导致困惑。,https://github.com/deepspeedai/DeepSpeed/issues/673
DeepSpeed,该issue类型为功能增强请求，主要对象是DeepSpeed中的优化器（optimizers）。这个问题或需求源于对AdamW优化器的原生支持，可能是为了增强训练效果或提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/672
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed的AdamW优化器类型和ZeRO支持问题。,https://github.com/deepspeedai/DeepSpeed/issues/670
DeepSpeed,这是一个用户提出需求的issue，主要涉及优化器配置问题，用户希望能够更简单地指定使用的优化器及参数设置。,https://github.com/deepspeedai/DeepSpeed/issues/668
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeed框架中的模型模块（module），用户希望添加能够测量模型每个模块在时间、估计的FLOPS数量和参数数量上的性能分析工具。,https://github.com/deepspeedai/DeepSpeed/issues/664
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及DeepSpeed框架中的workspace内存分配功能。,https://github.com/deepspeedai/DeepSpeed/issues/661
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed配置日志的显示方式问题，用户希望在启动时记录填充了默认值的最终配置以便用户了解配置细节。,https://github.com/deepspeedai/DeepSpeed/issues/657
DeepSpeed,这个issue是关于系统需求的提问，主要对象是DeepSpeed中的t5-11b模型，用户询问在GPU上运行t5-11b模型的内存需求和是否能在具有24 GB内存的GPU（例如RTX 3090）上运行。,https://github.com/deepspeedai/DeepSpeed/issues/655
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed中13b参数模型的系统要求。根据情况推测用户可能想了解在运行13b参数模型时所需的系统配置和环境。,https://github.com/deepspeedai/DeepSpeed/issues/654
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的网站配置菜单项缺失问题，需要对网站进行重新生成以解决。,https://github.com/deepspeedai/DeepSpeed/issues/652
DeepSpeed,该issue类型为请求新版本发布和文档审核，涉及主要对象为DeepSpeed集成到HF trainer的过程。该请求由于当前DeepSpeed包缺少最新修复导致集成无法正常工作，同时需要对文档进行审查以提供更好的使用体验。,https://github.com/deepspeedai/DeepSpeed/issues/648
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed下的model checkpointing功能。原因是用户对如何最佳集成DeepSpeed到已有的模型checkpointing代码存在疑惑。,https://github.com/deepspeedai/DeepSpeed/issues/647
DeepSpeed,这个issue是用户提出需求，希望将deepspeed.init_distributed添加到RTD页面。,https://github.com/deepspeedai/DeepSpeed/issues/645
DeepSpeed,该问题属于文档更新类型，主要涉及DeepSpeed库中的deepseed.initialize()方法，提出了关于参数CC(WarmupDecayLR.params.total_num_steps)用法的疑问。,https://github.com/deepspeedai/DeepSpeed/issues/644
DeepSpeed,这是一个用户提出需求的issue，主要涉及模型并行性，用户想要获得关于设置MPU的示例，以进行单机多GPU训练。,https://github.com/deepspeedai/DeepSpeed/issues/639
DeepSpeed,这是一个需求提议，主要涉及DeepSpeed中的`deepspeed.init_distributed`函数，建议添加一个可选的超时参数。,https://github.com/deepspeedai/DeepSpeed/issues/637
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中如何从保存的checkpoint中恢复客户端状态，由于文档不清晰导致用户不明白如何重新获取客户端状态。,https://github.com/deepspeedai/DeepSpeed/issues/636
DeepSpeed,该issue属于用户提出需求，并主要涉及DeepSpeed中的fp16配置问题。用户询问如何在DeepSpeed配置文件中映射HF trainer中的fp16参数，以及关于DeepSpeed是否支持原生pytorch amp，以及如何获取apex库等问题。,https://github.com/deepspeedai/DeepSpeed/issues/634
DeepSpeed,这个issue类型是需求用户需求，主要涉及对象是`WarmupDecayLR`模块的参数`total_num_steps`，问题出现的原因是在初始化时无法将`total_num_steps`传递给`WarmupDecayLR`，可能导致配置不准确。,https://github.com/deepspeedai/DeepSpeed/issues/633
DeepSpeed,这个issue类型为功能需求，主要对象是DeepSpeed引擎的初始化过程，用户希望能够通过python字典来初始化DeepSpeed引擎，以解决HF trainer集成问题，并避免zero_optimization.cpu_offload参数导致的静默崩溃。,https://github.com/deepspeedai/DeepSpeed/issues/632
DeepSpeed,这是一个更新依赖库版本的Issue，涉及的主要对象是DeepSpeed项目中的/docs目录。原因是为了改进Nokogiri库的安装速度和可靠性，特别是在Linux和OSX/Darwin平台上加入了预编译的native gems，以提高安装效率和可靠性。,https://github.com/deepspeedai/DeepSpeed/issues/630
DeepSpeed,这是一个建议类型的issue，主要涉及网站链接的问题，由于缺乏定期检查链接，导致网站的部分链接失效。,https://github.com/deepspeedai/DeepSpeed/issues/626
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed是否支持Megatron-LM的3D并行性，用户想知道DeepSpeed当前是否能够支持使用3D并行性训练GPT2以及是否需要进行额外的系统级实现。,https://github.com/deepspeedai/DeepSpeed/issues/621
DeepSpeed,这是一个关于性能优化的问题，主要涉及DeepSpeed中torch.cuda.synchronize()函数的性能瓶颈和如何通过去除该函数或采用多个CUDA流来优化通信过程。,https://github.com/deepspeedai/DeepSpeed/issues/620
DeepSpeed,这是一个用户提出需求的问题，涉及主要对象是DeepSpeed下的模型并行模型，用户在询问如何将模型保存为单一检查点以在单个GPU上进行推理。,https://github.com/deepspeedai/DeepSpeed/issues/619
DeepSpeed,这是一个关于需求讨论的issue，涉及DeepSpeed在transformers中的集成，讨论了是否将两个命令行参数合并为一个的建议。,https://github.com/deepspeedai/DeepSpeed/issues/616
DeepSpeed,这是一个用户提出需求的issue，主要涉及 DeepSpeed 的 1-bit Adam 安装依赖问题。用户想要了解如何在自定义的 Docker image 中添加这个特性所需的安装步骤。,https://github.com/deepspeedai/DeepSpeed/issues/614
DeepSpeed,"该issue类型是用户提出需求，主要涉及的对象是DeepSpeed中的pip pre-built binary。用户提出需求的原因是希望获得一个self-contained, prebuilt binary来方便DeepSpeed的采用。",https://github.com/deepspeedai/DeepSpeed/issues/613
DeepSpeed,这是一个需求反馈类型的issue，涉及主要对象是DeepSpeed中的`clip_grad_norm`方法。由于`clip_grad_norm`方法与`torch.nn.utils.clip_grad_norm_`重复且没有特殊功能，用户提出是否可以考虑移除该方法或统一不同引擎中同名方法的签名。,https://github.com/deepspeedai/DeepSpeed/issues/611
DeepSpeed,这是一个需求报告，主要涉及在DeepSpeed下初始化分布式后端以支持构建需要torch分布式的模型，其中的问题是在使用mpi启动器或在AML上运行时，运行类似megatron的模型会出现问题。,https://github.com/deepspeedai/DeepSpeed/issues/608
DeepSpeed,该issue类型为用户提出需求，主要涉及DeepSpeed中的elastic training support功能，用户提出需要支持根据GPU数量动态调整训练规模，以便于使用不同数量GPU进行训练。,https://github.com/deepspeedai/DeepSpeed/issues/602
DeepSpeed,该issue是关于用户提出需求的，主要涉及DeepSpeed Transformer Kernel对于Transformer模型的扩展计划。用户询问了是否有将Transformer内核扩展到支持更多模型，比如GPT2，或者整合模型并行性来训练大模型的计划。,https://github.com/deepspeedai/DeepSpeed/issues/600
DeepSpeed,这是一个代码优化和功能改进类的issue，涉及主要对象是DeepSpeed下的1-bit Adam实现，用户请求评审并提供反馈。,https://github.com/deepspeedai/DeepSpeed/issues/594
DeepSpeed,这是一个功能增强型的问题，涉及的主要对象是DeepSpeed中的通信后端和1-bit adam优化实现，用户提出了添加NCCL通信后端和1-bit Adam实现的重构代码的需求。,https://github.com/deepspeedai/DeepSpeed/issues/593
DeepSpeed,这是一个用户提出需求的 issue，主要对象是DeepSpeed项目。由于缺少相关论文和视频，用户请求在项目的 readme 或网站上添加这些内容。,https://github.com/deepspeedai/DeepSpeed/issues/592
DeepSpeed,这个issue属于用户提需求类型， 主要对象是Transformer-kernel， 用户提出了希望支持任意长度序列的需求。,https://github.com/deepspeedai/DeepSpeed/issues/587
DeepSpeed,这是一个用户提出需求的 issue，主要涉及 DeepSpeed 的模块替换支持。,https://github.com/deepspeedai/DeepSpeed/issues/586
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed中的lr_scheduler。由于某些常见的调度器需要额外的信息来告知调度器当前的度量指标，通过一些微小的修复支持这一需求，而不改变任何默认行为。,https://github.com/deepspeedai/DeepSpeed/issues/584
DeepSpeed,这是一个类型为需求或请教问题的issue，主要涉及对象是如何在多节点中使用DeepSpeed。由于PDSH无法建立连接，导致用户无法进行多节点训练。,https://github.com/deepspeedai/DeepSpeed/issues/579
DeepSpeed,这个issue属于功能需求类型，主要对象是DeepSpeed项目中的构建配置，用户提出了关于添加对RTX 30系列GPU compute_86算力支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/577
DeepSpeed,这是一个功能需求类型的issue，涉及到DeepSpeed中Transformer kernel的注入问题，用户希望能够通过运行脚本来自动添加DeepSpeed操作，而不是直接修改建模文件。,https://github.com/deepspeedai/DeepSpeed/issues/576
DeepSpeed,这是一个用户提出需求的issue，主要涉及到提升DeepSpeed在多核编译时的速度和添加清理过程的功能。,https://github.com/deepspeedai/DeepSpeed/issues/573
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed库的开发者。由于当前版本没有支持CUDA 11+的8.0计算能力，用户请求添加这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/572
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的`train`方法，在调用`train()`时对DDP进行包装会失败，用户希望`deepspeed`的`train`方法接受一个可选的`mode`参数。,https://github.com/deepspeedai/DeepSpeed/issues/571
DeepSpeed,这是一个用户提出的功能需求，主要涉及DeepSpeed中新增了最新检查点保存/加载支持的功能。,https://github.com/deepspeedai/DeepSpeed/issues/569
DeepSpeed,这是一个更新issue，涉及到维护和更新项目的依赖版本问题。,https://github.com/deepspeedai/DeepSpeed/issues/561
DeepSpeed,这是一个用户提出需求的issue，主要涉及于DeepSpeed的setup.py文件添加long_description信息，可能是为了提供更详细的项目描述信息而提交的。,https://github.com/deepspeedai/DeepSpeed/issues/560
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中transformer layer的hidden dimension问题。用户希望移除先前对隐藏维度的限制，实现任何大小的隐藏维度模型训练，并修改CUDA内核以支持此功能。,https://github.com/deepspeedai/DeepSpeed/issues/559
DeepSpeed,该issue类型是关于项目文档和持续集成的更新，主要对象是项目仓库中的徽章和CI命名。原因可能是为了保持项目信息的准确性和可视化效果。,https://github.com/deepspeedai/DeepSpeed/issues/557
DeepSpeed,这是一个用户提出的改进需求类型的issue，主要对象是将DeepSpeed切换至使用GitHub Actions作为持续集成系统。,https://github.com/deepspeedai/DeepSpeed/issues/556
DeepSpeed,这是一个用户提出需求的类型，询问为什么ZeRO只支持fp16训练，是否有办法启用/修复支持fp32的问题。,https://github.com/deepspeedai/DeepSpeed/issues/555
DeepSpeed,这是一个功能建议类型的issue，主要提议部分功能的废弃，涉及DeepSpeed的客户端对梯度规约的禁用能力。,https://github.com/deepspeedai/DeepSpeed/issues/552
DeepSpeed,这个issue类型是功能需求提出，主要涉及DeepSpeed中ZeRO-1模块的checkpoint功能，用户希望支持非张量状态的保存和加载。,https://github.com/deepspeedai/DeepSpeed/issues/548
DeepSpeed,这是一个用户提出需求的issue，涉及主要对象是DeepSpeed库中的unfused optimizer。此问题提出在unfused optimizer中添加static_loss_scale参数的需求。,https://github.com/deepspeedai/DeepSpeed/issues/546
DeepSpeed,这是一个提出需求的issue，主要对象是DeepSpeed框架。其提出了关于添加模块flops分析器的需求。,https://github.com/deepspeedai/DeepSpeed/issues/544
DeepSpeed,这是一个功能需求报告，涉及到在AzureML上发现必要的标志以初始化PyTorch中的NCCL后端，避免依赖mpi4py所带来的问题。,https://github.com/deepspeedai/DeepSpeed/issues/542
DeepSpeed,这是一个需求讨论类型的问题，主要涉及对DeepSpeed中CPU Checkpointing与partitioned activations的关系问题进行探讨。,https://github.com/deepspeedai/DeepSpeed/issues/541
DeepSpeed,这是一个用户提出需求的类型，涉及的主要对象是 DeepSpeed 下的 elastic checkpointing 功能。,https://github.com/deepspeedai/DeepSpeed/issues/526
DeepSpeed,这是关于DeepSpeed安装文档更新的问题，属于用户提出需求类型，主要涉及安装文档的更新。,https://github.com/deepspeedai/DeepSpeed/issues/525
DeepSpeed,这是一个用户需求类型的issue，主要涉及DeepSpeed中如何在ZeRO优化器中使用float BatchNorm，用户提出寻求关于在mixed precision情况下使用float BatchNorm的示例或文档。,https://github.com/deepspeedai/DeepSpeed/issues/520
DeepSpeed,这是一个请求类型的issue，主要涉及DeepSpeed中PLD tutorial的更新。这个问题可能是因为DeepSeed实现导致了教程内容需要更新。,https://github.com/deepspeedai/DeepSpeed/issues/512
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed和pytorch-lightning之间如何集成checkpointing的问题，用户不确定如何处理checkpointing，并寻求关于在pytorch-lightning项目中集成DeepSpeed时如何正确处理checkpointing的帮助。,https://github.com/deepspeedai/DeepSpeed/issues/509
DeepSpeed,这个issue类型是功能需求，主要涉及的对象是DeepSpeed中PLD的实现，用户提出需要实现PLD以及编写单元测试的需求。,https://github.com/deepspeedai/DeepSpeed/issues/508
DeepSpeed,这个issue类型是用户提出需求，问题单涉及的主要对象是如何在使用DeepSpeed训练过的Megatron-LM模型上生成文本。由于缺乏关于生成文本的指导，用户无法加载DeepSpeed包装生成的检查点，希望了解MegatronLM和DeepSpeed包装的检查点是否兼容以及是否需要使用deepspeed模块包装文本生成代码。,https://github.com/deepspeedai/DeepSpeed/issues/507
DeepSpeed,这是一个用户提出需求的 issue，主要涉及如何将 ZeRO checkpoint 导出为通用的 PyTorch .pth 文件，由于目前 ZeRO 只提供了用于保存和加载 checkpoint 的 API，导致用户无法直接导出通用的模型文件。,https://github.com/deepspeedai/DeepSpeed/issues/506
DeepSpeed,这个issue类型是用户提出需求，讨论了DeepSpeed中的Pipeline Parallelism和Parameter Partitioning的区别和优缺点。由于用户对DeepSpeed新功能的相关概念不清楚而提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/505
DeepSpeed,这是一个关于需求的问题，涉及主要对象是DeepSpeed的使用者，由于SyncBatchNorm只在torch.nn.parallel.DistributedDataParallel环境下支持，导致无法直接结合DeepSpeed使用。,https://github.com/deepspeedai/DeepSpeed/issues/502
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed模型加载的兼容性问题，由于使用不同的kernel参数训练和加载模型导致了加载时的困难。,https://github.com/deepspeedai/DeepSpeed/issues/498
DeepSpeed,该issue属于功能需求类型，涉及DeepSpeed的JIT op和PyPI支持，主要为了支持JIT、预安装操作，简化新操作添加流程，并提供环境报告功能。,https://github.com/deepspeedai/DeepSpeed/issues/496
DeepSpeed,这是一个需求更新的类型issue， 主要涉及DeepSpeed中的ZeRO教程内容。这个问题是由于需要更改教程以指定激活检查点的使用而提出的。,https://github.com/deepspeedai/DeepSpeed/issues/495
DeepSpeed,这个issue属于功能需求提出，主要涉及到DeepSpeed中参数融合在优化器分区中导致LAMB表现不同的问题。由于参数融合会使得分片数量较少，在使用LAMB等优化器时，无法复用在其他框架中调试的超参数。,https://github.com/deepspeedai/DeepSpeed/issues/490
DeepSpeed,这是一个用户提出需求的类型。该问题单涉及的主要对象是预训练的GPT-2模型。由于DeepSpeed未提供预训练模型的下载端口，用户想知道在哪里下载模型相关的内容。,https://github.com/deepspeedai/DeepSpeed/issues/486
DeepSpeed,这是一个需求类型的issue，主要对象是DeepSpeed引擎，用户提出需求添加CPUAdam优化器用于zero-offload。,https://github.com/deepspeedai/DeepSpeed/issues/484
DeepSpeed,这是一个功能增强的类型，主要对象是DeepSpeed CPUAdam实现，用户提出需要添加AdamW选项和解决收敛问题。,https://github.com/deepspeedai/DeepSpeed/issues/482
DeepSpeed,该issue类型为功能需求，主要涉及DeepSpeed的op JIT支持。由于需要简化新op添加、改进环境报告和减少依赖性，因此提出了此需求。,https://github.com/deepspeedai/DeepSpeed/issues/476
DeepSpeed,这个issue类型是更新需求，主要涉及网站的依赖更新和修复错误链接。原因是网站依赖需要最新版本，并且存在一个损坏的链接需要修复。,https://github.com/deepspeedai/DeepSpeed/issues/475
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的Dockerfile使用较旧版本的PyTorch，用户希望更新至较新版本。,https://github.com/deepspeedai/DeepSpeed/issues/473
DeepSpeed,这个issue是一个需求类型，主要涉及的对象是DeepSpeed下的transformer kernels。由于缺乏对计算能力6.0（例如P100）的支持，用户提出了需要添加这一支持的需求。,https://github.com/deepspeedai/DeepSpeed/issues/470
DeepSpeed,"这是一个用户提出需求的类型，请求新增DeepSpeed_Adam optimizer。由于DeepSpeed当前版本尚未包含Adam optimizer, 用户建议添加该优化器以丰富DeepSpeed的功能。",https://github.com/deepspeedai/DeepSpeed/issues/468
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中有关数据预取功能的寻找困难。由于未能在DeepSpeed存储库中找到相关代码，用户请求帮助这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/466
DeepSpeed,这是一个用户提出需求的类型，主要对象是gan tutorial。由于缺乏相关指导，用户希望获取关于GAN在DeepSpeed下的教程。,https://github.com/deepspeedai/DeepSpeed/issues/462
DeepSpeed,"这个issue类型是用户提出需求，主要对象是DeepSpeed下的Tutorials docs。由于内容仅为""!image""，看起来用户可能希望在文档中添加图片，或者是图片未正确显示导致的问题。",https://github.com/deepspeedai/DeepSpeed/issues/457
DeepSpeed,这是一个功能需求提出的issue，主要涉及的对象是DeepSpeed中的optimizer engine设计。由于原有的optimizer engine设计可能存在不足，用户提出了关于通用optimizer engine设计的需求和建议。,https://github.com/deepspeedai/DeepSpeed/issues/453
DeepSpeed,该issue类型为功能增强（feature enhancement），主要涉及到Docker测试文件的添加。由于需求是在测试节点上使用Dockerfile含有最低要求，额外的依赖通过conda安装在这个Docker镜像之上，用户可能提出了需要在测试环境中方便地运行DeepSpeed的需求。,https://github.com/deepspeedai/DeepSpeed/issues/451
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed软件包。由于默认安装了CPU Adam，用户希望能够禁用这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/450
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed在model-parallel&multi-gpu训练和推断环境中的内存使用以及推断功能，用户希望了解关于GPU内存使用和推断API的资料和技术支持。,https://github.com/deepspeedai/DeepSpeed/issues/448
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及DeepSpeed优化器以全半精度训练内存优化为对象。用户询问是否有完全半精度优化器的实现，以减少大模型训练所需的内存。,https://github.com/deepspeedai/DeepSpeed/issues/447
DeepSpeed,这是一个用户需求问题，涉及的主要对象是DeepSpeedExamples中使用最新版本的Megatron-ML的相关指导。用户提出该问题可能是因为希望了解如何集成最新版本的Megatron-ML与DeepSpeed，而当前提供的示例中的Megatron-ML版本已经过时。,https://github.com/deepspeedai/DeepSpeed/issues/445
DeepSpeed,这个issue主要为用户提出需求，询问如何在DeepSpeed和MegatronLM中结合使用，希望获得相关示例或教程。,https://github.com/deepspeedai/DeepSpeed/issues/444
DeepSpeed,这个issue是关于修改单元测试命名的更正请求，属于代码质量优化类型，主要涉及的对象是测试文件和测试用例名称。,https://github.com/deepspeedai/DeepSpeed/issues/442
DeepSpeed,该issue类型为用户提出需求，主要对象是DeepSpeed中的PipelineModule。用户想了解是否PipelineModule支持跨机器的分段传输，以及若不支持可能存在的注意事项。,https://github.com/deepspeedai/DeepSpeed/issues/434
DeepSpeed,这是一个功能改进的issue，主要对象是DeepSpeed库。由于移除了cpufeature并默认使用AVX-2指令集，可能是为了提升性能或兼容性而做出的改动。,https://github.com/deepspeedai/DeepSpeed/issues/433
DeepSpeed,这是一个功能需求，主要涉及LR scheduler的单元测试。原因可能是为了增加LR Range Test和1Cycle Closing CC的单元测试覆盖。,https://github.com/deepspeedai/DeepSpeed/issues/429
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed框架的支持动态序列长度功能。,https://github.com/deepspeedai/DeepSpeed/issues/424
DeepSpeed,这是一个功能需求的issue，涉及DeepSpeed中Transformer模型核心代码的修改。可能是用户提出了需要增加中间层大小的功能需求。,https://github.com/deepspeedai/DeepSpeed/issues/423
DeepSpeed,这个issue类型是代码回退（Revert）的操作请求，主要涉及DeepSpeed中的Activation checkpointing bugfix和unit tests，可能是由于之前的bug修复引入了新的问题或导致了其他不可预料的情况。,https://github.com/deepspeedai/DeepSpeed/issues/422
DeepSpeed,这是一个需求类型的issue，涉及到DeepSpeed下的lr schedule功能。由于用户希望添加Linear warmup+decay lr schedule，并更新lr schedule unit tests，因此提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/414
DeepSpeed,这个issue属于用户提出需求的类型，主要涉及DeepSpeed安装测试，可能是由于缺少CPU-Adam、重新格式化或需要添加颜色等功能而提出。,https://github.com/deepspeedai/DeepSpeed/issues/413
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed测试的运行，由于用户可能不安装某些ops导致pytest跳过特定测试。,https://github.com/deepspeedai/DeepSpeed/issues/411
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是配置文件。由于缺少readthedocs yaml配置，用户希望能够添加该功能。,https://github.com/deepspeedai/DeepSpeed/issues/410
DeepSpeed,这是用户提出需求的类型的issue，主要涉及DeepSpeed下的模型训练过程中的中间层大小设置。用户可能由于训练需求或者模型结构的特殊要求，希望支持不同于默认设置的中间层大小选项，以满足特定的训练需求。,https://github.com/deepspeedai/DeepSpeed/issues/409
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed项目。 ,https://github.com/deepspeedai/DeepSpeed/issues/405
DeepSpeed,这是一个类型为需求提出的issue，主要涉及对象为readthedocs，由于readthedocs的升级导致相关问题。,https://github.com/deepspeedai/DeepSpeed/issues/402
DeepSpeed,这是一个文档更新类的issue，涉及对象为DeepSpeed的ZeRO-Offload功能，由于链接需要更新导致此issue。,https://github.com/deepspeedai/DeepSpeed/issues/401
DeepSpeed,这是一个用户提出需求的类型，涉及主要对象是DeepSpeed的landing page。由于landing page需要更新，用户提出了更新页面的请求。,https://github.com/deepspeedai/DeepSpeed/issues/395
DeepSpeed,这是关于文档问题的需求问题，主要涉及更新代码标签。可能是由于文档中代码标签需要从Python更改为JSON或者Bash而导致的问题。,https://github.com/deepspeedai/DeepSpeed/issues/394
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中的Pipeline parallel training engine，用户在这个issue中可能会寻求关于该引擎的帮助或者提出改进建议。,https://github.com/deepspeedai/DeepSpeed/issues/392
DeepSpeed,这个issue类型是功能需求，涉及到DeepSpeed中的ZeRO-Offload功能。由于用户希望获得关于ZeRO-Offload发布的相关信息。,https://github.com/deepspeedai/DeepSpeed/issues/391
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的intermediate size设置。原因是先前默认设定为4倍hidden_dimension，现在将其改为可配置，并默认设置为与以前相同的intermediate_size。,https://github.com/deepspeedai/DeepSpeed/issues/389
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed文档更新的问题，可能是为了增强1-bit Adam的功能和信息准确性。,https://github.com/deepspeedai/DeepSpeed/issues/388
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed中ZeRO相关的教程。,https://github.com/deepspeedai/DeepSpeed/issues/384
DeepSpeed,这是一个用户提出改进建议的类型，主要涉及DeepSpeed的AVX2支持。产生这个问题的原因可能是要更新DeepSpeed以支持AVX2指令集。,https://github.com/deepspeedai/DeepSpeed/issues/383
DeepSpeed,这是一个关于文档更新的问题，用户询问为什么新增的功能页面没有自动显示在主页上。,https://github.com/deepspeedai/DeepSpeed/issues/382
DeepSpeed,这个issue类型是需求。该问题单涉及的主要对象是DeepSpeed。由于需要在DeepSpeed中添加1-bit Adam支持，用户提出了这个需求。,https://github.com/deepspeedai/DeepSpeed/issues/380
DeepSpeed,这是一个用户提出需求的类型，问题主要涉及DeepSpeed的ZeRO phase 3实现计划，用户询问该功能何时可用。,https://github.com/deepspeedai/DeepSpeed/issues/379
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中添加稀疏注意力功能的问题，用户可能提出了关于如何在feature index页面中添加稀疏注意力的建议或需求。,https://github.com/deepspeedai/DeepSpeed/issues/377
DeepSpeed,这个issue是一个功能需求，并且涉及的主要对象是深度学习库DeepSpeed。由于需要增加稀疏注意力索引项目，用户提出了这个问题。,https://github.com/deepspeedai/DeepSpeed/issues/376
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中的launcher工具，由于mvapich缺失而导致不能支持。,https://github.com/deepspeedai/DeepSpeed/issues/375
DeepSpeed,这是一个功能需求提出的类型，主要涉及DeepSpeed中的ZeRO stage 2对象。由于ZeRO stage 2不支持梯度累积，导致用户提出希望支持梯度累积的需求。,https://github.com/deepspeedai/DeepSpeed/issues/373
DeepSpeed,这个issue类型是功能需求，主要涉及的对象是添加fp32-fp16转换内核。,https://github.com/deepspeedai/DeepSpeed/issues/369
DeepSpeed,该issue类型为需求提出，主要对象是DeepSpeed项目中的代码质量测试，用户提出希望将代码质量测试迁移到Azure托管的代理上。,https://github.com/deepspeedai/DeepSpeed/issues/368
DeepSpeed,这是一个关于功能测试通过的issue，主要涉及到DeepSpeed中的ZeRO-Offload功能。原因可能是之前未经集成导致的问题。,https://github.com/deepspeedai/DeepSpeed/issues/366
DeepSpeed,该issue类型为文档更新请求，涉及主要对象为DeepSpeed安装说明。原因是当前说明不清晰或过时，需要更新。,https://github.com/deepspeedai/DeepSpeed/issues/362
DeepSpeed,这个issue类型是用户提出需求， 主要涉及的对象是DeepSpeed中Adam优化器。 由于用户想要手动调用DeepSpeed Adam优化器传递fp16参数，因此提出这个问题寻求帮助。,https://github.com/deepspeedai/DeepSpeed/issues/361
DeepSpeed,该issue类型为用户提出需求，请教问题，主要涉及的对象是DeepSpeed。由于需要在CPU上直接分配CPU内存，而无需从GPU上进行内存传输，用户提出了相关问题并寻求解决方案。,https://github.com/deepspeedai/DeepSpeed/issues/360
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中稀疏注意力教程的更新问题。,https://github.com/deepspeedai/DeepSpeed/issues/357
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed库，用户请求添加一个CPU版本的Adam优化器。,https://github.com/deepspeedai/DeepSpeed/issues/356
DeepSpeed,这是一个用户提出需求的类型issue，主要涉及DeepSpeed中的导航页面，用户希望添加对稀疏注意力的链接。,https://github.com/deepspeedai/DeepSpeed/issues/355
DeepSpeed,这是一个功能性修复的问题，主要涉及代码合并。,https://github.com/deepspeedai/DeepSpeed/issues/354
DeepSpeed,这是一个用户提出需求的类型，主要涉及的对象是DeepSpeed库中的1-bit adam算法。这个issue提出了关于1-bit adam算法的需求或者建议。,https://github.com/deepspeedai/DeepSpeed/issues/353
DeepSpeed,这是一个功能需求类型的issue，主要涉及1bit adam的优化器实现。,https://github.com/deepspeedai/DeepSpeed/issues/352
DeepSpeed,这是一个用户提出需求的 issue，主要涉及 Jekyll 的安装指导，原因可能是为了优化 Jekyll 的设置流程。,https://github.com/deepspeedai/DeepSpeed/issues/351
DeepSpeed,这是一个需求优化类的issue，主要涉及DeepSpeed框架中优化器的类型检测和冗余配置键的优化。原因可能是检测优化器类型不够精准，导致存在冗余配置键或相关问题。,https://github.com/deepspeedai/DeepSpeed/issues/349
DeepSpeed,该issue类型为功能增强请求，主要涉及的是将1bit Adam引入DeepSpeed。由于目前DeepSpeed尚未支持1bit Adam，因此用户提出了增加此功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/348
DeepSpeed,这个issue类型是需求类，涉及的主要对象是ZeRO-Offload功能。由于需要帮助进行重新基础，因此提出了将当前ZeROOffload的更改压缩的需求。,https://github.com/deepspeedai/DeepSpeed/issues/345
DeepSpeed,这个issue属于功能改进类型，主要涉及Sparse attention、Ops/runtime refactor和版本更新，由于新增了条件操作/核构建支持，可能是为了优化计算性能或增加灵活性。,https://github.com/deepspeedai/DeepSpeed/issues/343
DeepSpeed,该issue类型为功能需求，涉及Sparse attention功能、代码重构、版本标记以及条件构建。,https://github.com/deepspeedai/DeepSpeed/issues/342
DeepSpeed,该issue类型为需求变更，涉及主要对象是DeepSpeed中的BBS example。原因是需要调整minibatch size和gradient accumulation steps以适应内存限制。,https://github.com/deepspeedai/DeepSpeed/issues/341
DeepSpeed,该issue类型为测试矩阵改进，涉及到测试基础设施和CUDA版本的功能。该问题的产生是为了快速构建新的测试节点以应对排队时间过长的情况，同时需要优化硬件和驱动的约束条件。,https://github.com/deepspeedai/DeepSpeed/issues/340
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed项目中实现WarmupLinearSchedule scheduler的建议。由于自适应学习率与衰减已被证明优于固定学习率，用户建议实现此调度程序来提高性能。,https://github.com/deepspeedai/DeepSpeed/issues/339
DeepSpeed,这个issue是用户提出的需求，涉及的主要对象是DeepSpeed中的Stage 2 Feature Tests。由于需要支持Gradient Accumulation，用户提出了相关功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/338
DeepSpeed,这是一个请求添加代码所有者的issue，涉及DeepSpeed团队，旨在明确团队中代码文件的所有权。,https://github.com/deepspeedai/DeepSpeed/issues/335
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed的setup.py文件。由于可能需要更新或修改setup.py文件来解决某些问题或改进功能。,https://github.com/deepspeedai/DeepSpeed/issues/333
DeepSpeed,这个issue属于功能测试报告类型，主要涉及到DeepSpeed库中从tensorboardX切换到torch.utils.tensorboard的功能。,https://github.com/deepspeedai/DeepSpeed/issues/330
DeepSpeed,该问题是关于用户提出需求，主要对象是DeepSpeed中的Tensorboard功能。由于DeepSpeed默认提供的Tensorboard功能仅支持有限的日志记录，用户希望能够自定义Tensorboard以查看损失列表中的每个组件的图形。,https://github.com/deepspeedai/DeepSpeed/issues/327
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed项目添加ROCm的hip扩展。原因是为了支持hipification并构建hip扩展，以提高项目的功能性。,https://github.com/deepspeedai/DeepSpeed/issues/323
DeepSpeed,该issue类型为增加功能/改进，主要涉及对象为DeepSpeed在ROCm上的支持。,https://github.com/deepspeedai/DeepSpeed/issues/321
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是 transformer kernel code。这个问题由于原始代码不支持动态序列长度，用户进行了相关更改以支持动态序列长度。,https://github.com/deepspeedai/DeepSpeed/issues/320
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed，用户想了解DeepSpeed是否支持TensorFlow。,https://github.com/deepspeedai/DeepSpeed/issues/319
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeed项目中的提交（commit）更新。由于用户希望更新DSE（DeepSpeedEngine）的提交，可能是为了修复bug或添加新功能。,https://github.com/deepspeedai/DeepSpeed/issues/317
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的ZeRO-2模块启用CPU卸载功能的问题。,https://github.com/deepspeedai/DeepSpeed/issues/313
DeepSpeed,这是一个更新提交以修复Pillow安全问题的问题，属于代码更新类型，主要涉及DeepSpeed库。,https://github.com/deepspeedai/DeepSpeed/issues/312
DeepSpeed,这是一个需要更新网站gems的issue，类型是其他类型，涉及的主要对象是GitHub仓库的内容。由于kramdown警告导致需要更新网站gems以解决问题。,https://github.com/deepspeedai/DeepSpeed/issues/311
DeepSpeed,该issue属于提出需求类型，涉及的主要对象是添加webinar链接。,https://github.com/deepspeedai/DeepSpeed/issues/309
DeepSpeed,这个issue类型为用户的请求，主要对象是关闭错误创建的拉取请求。这个问题产生的原因是用户不小心创建了一个拉取请求，希望得到帮助关闭它。,https://github.com/deepspeedai/DeepSpeed/issues/306
DeepSpeed,"这是一个用户提出需求的issue，主要对象是添加""Docker pull badge""。根据标题来看，用户希望在项目中添加Docker拉取徽章，以提供更方便的访问方式。",https://github.com/deepspeedai/DeepSpeed/issues/302
DeepSpeed,这是一个用户提出需求的类型，该问题单主要涉及的对象是DSExamples。这个问题可能是用户想要更新或升级 DSExamples 的版本。,https://github.com/deepspeedai/DeepSpeed/issues/300
DeepSpeed,"这是一个需求更新的issue，主要涉及更新DeepSpeed文档中有关自动混合精度（automatic mixed precision, AMP）的内容。",https://github.com/deepspeedai/DeepSpeed/issues/287
DeepSpeed,这个issue类型是用户提出需求，主要涉及的对象是DeepSpeed框架。由于用户希望支持amp deepspeed backend，因此提出了该需求。,https://github.com/deepspeedai/DeepSpeed/issues/286
DeepSpeed,这是一个用户提出需求的问题，主要对象是DeepSpeed，由于需要处理大规模的数据集，用户想知道DeepSpeed是否支持IterableDataset。,https://github.com/deepspeedai/DeepSpeed/issues/285
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed使用中关于训练规模和性能验证的问题，用户想要查看验证了6B和13B参数训练的相关代码和参数设置。,https://github.com/deepspeedai/DeepSpeed/issues/284
DeepSpeed,这是一个需求类型的issue，主要涉及到DeepSpeed项目中的依赖管理，用户提出删除需求中的TensorFlow依赖。,https://github.com/deepspeedai/DeepSpeed/issues/282
DeepSpeed,这是一个用户提出需求的issue，主要是关于DeepSpeed库中的tensorflow依赖是否可以变为可选项，以减少安装时不必要的依赖。,https://github.com/deepspeedai/DeepSpeed/issues/279
DeepSpeed,这个issue类型是需求类，主要对象是DeepSpeed代码库，可能是用户想要更新项目中的DeepSpeed External（DSE）的提交。,https://github.com/deepspeedai/DeepSpeed/issues/278
DeepSpeed,这是一个关于功能需求的issue，涉及的主要对象是DeepSpeed中的模型操作，用户提出需要自动化使用不同操作的需求。,https://github.com/deepspeedai/DeepSpeed/issues/277
DeepSpeed,这是一个用户提出的需求，主要涉及DeepSpeed库中支持分布式初始化的问题。由于当前需求需要在DeepSpeed初始化之外初始化Torch分布式，特别是用于MPI发现（例如AML）和其他模型的场景，以及自动尝试发现MPI world信息而不需要deepseed_mpi标志，需要更新文档、移除标志并添加单元测试。,https://github.com/deepspeedai/DeepSpeed/issues/276
DeepSpeed,该issue类型为用户提出需求，主要涉及DeepSpeed在Kubernetes上进行基于多节点训练时的操作方式问题，用户询问如何在Kubernetes下实现DeepSpeed基于多节点训练。,https://github.com/deepspeedai/DeepSpeed/issues/274
DeepSpeed,这个issue类型为功能需求，涉及的主要对象是DeepSpeed中的参数组。这个问题是因为需要支持参数组小于DP的情况，简化参数填充代码，并暴露API以从fp16副本中刷新主参数。,https://github.com/deepspeedai/DeepSpeed/issues/273
DeepSpeed,"这是一个用户提出需求的issue，主要涉及到DeepSpeed项目中的""Ai scale""。由于未提供具体内容，无法分析导致的问题或需求。",https://github.com/deepspeedai/DeepSpeed/issues/271
DeepSpeed,这是一个用户提出需求的issue类型，主要涉及DeepSpeed官方网站的改进。这可能是由于用户希望改善DeepSpeed网站的用户体验，或者添加新的功能或内容。,https://github.com/deepspeedai/DeepSpeed/issues/269
DeepSpeed,这是一个用户提出需求的 issue，主要涉及的对象是 DeepSpeed 的新 transformer pre-ln image。由于缺少详细描述，无法确定具体问题或者需求。,https://github.com/deepspeedai/DeepSpeed/issues/268
DeepSpeed,这个issue类型为功能增强（feature enhancement），涉及到DeepSpeed的网站分析功能。由于用户想要为网站添加gtag进行网站分析，希望DeepSpeed能够支持这一功能。,https://github.com/deepspeedai/DeepSpeed/issues/266
DeepSpeed,这是一个功能需求的issue，主要涉及的对象是DeepSpeed库中的transformer模块。,https://github.com/deepspeedai/DeepSpeed/issues/263
DeepSpeed,这是一个用户对DeepSpeed项目提出的需求更改（Feature Request），主要涉及DeepSpeed的参数管理和重复代码清理。原因或导致的问题可能是在执行DeepSpeed相关任务时，存在需要更好管理参数和清理代码重复性的需要。,https://github.com/deepspeedai/DeepSpeed/issues/262
DeepSpeed,这是一个功能优化类型的issue，主要涉及loss scaler的重构，目的是添加一个基类以去除重复代码。,https://github.com/deepspeedai/DeepSpeed/issues/261
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是希望添加微调结果。由于缺乏相关信息，用户希望能够将微调结果记录在其中。,https://github.com/deepspeedai/DeepSpeed/issues/260
DeepSpeed,这个issue类型是希望建议，涉及的主要对象是DeepSpeed的安装更新操作。由于需要在无需sudo权限的情况下进行干净的构建文件，用户希望获得相关的帮助和支持。,https://github.com/deepspeedai/DeepSpeed/issues/258
DeepSpeed,这是一个建议性质的issue，提出了减少全局变量使用量及优化代码结构的建议。该问题主要涉及DeepSpeed模块中存在过多的全局变量使用。,https://github.com/deepspeedai/DeepSpeed/issues/256
DeepSpeed,这个issue是一个用户提出需求的类型，主要对象是DeepSpeed的更新提交。由于缺少具体内容，无法确定用户具体在请求什么样的更新提交。,https://github.com/deepspeedai/DeepSpeed/issues/254
DeepSpeed,"这个issue类型是用户提出需求，寻求关于""Features links""的帮助。",https://github.com/deepspeedai/DeepSpeed/issues/252
DeepSpeed,该问题单属于用户提出需求类型，主要对象是更新路径以及包含有关子模块的信息，根据内容推测用户请求审查并合并这个请求。,https://github.com/deepspeedai/DeepSpeed/issues/250
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中的优化器支持迁移至FP16。很可能由于初始化参数和避免重复梯度范数计算等原因导致此问题的提出。,https://github.com/deepspeedai/DeepSpeed/issues/249
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed的数据传输操作，用户希望减少主机和设备之间的内存拷贝操作。,https://github.com/deepspeedai/DeepSpeed/issues/248
DeepSpeed,这是一个用户提出需求的类型的issue，该问题单涉及的主要对象是DeepSpeed中的overlap_comm功能。这个问题的提出可能是因为用户希望添加有关overlap_comm的文档说明，以便更好地了解和使用该功能。,https://github.com/deepspeedai/DeepSpeed/issues/247
DeepSpeed,这个issue类型属于用户提出需求，该问题主要涉及 DeepSpeed 下的 Point BERT pretraining 教程的性能优化指导。,https://github.com/deepspeedai/DeepSpeed/issues/246
DeepSpeed,这是一个用户提出需求的问题，涉及的主要对象是更新DeepSpeed中的BERT图片。,https://github.com/deepspeedai/DeepSpeed/issues/245
DeepSpeed,"这是一个用户提出需求的issue类型，主要涉及对象是""center images""。这个问题可能是由于图像在处理过程中位置未居中导致用户需要对图像进行中心对齐的要求。",https://github.com/deepspeedai/DeepSpeed/issues/244
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及DeepSpeed中支持在改变数据并行度度和GPU数量的情况下加载和保存ZeRO检查点。,https://github.com/deepspeedai/DeepSpeed/issues/240
DeepSpeed,这是一个建议类型的issue，主要涉及对象是DeepSpeed库中的Contiguous Gradients参数。由于默认设置为True可能导致性能损失，用户建议将其默认设置为False。,https://github.com/deepspeedai/DeepSpeed/issues/239
DeepSpeed,这个issue类型是用户提出需求，主要涉及 DeepSpeed 中配置选项支持 predivide 的添加。,https://github.com/deepspeedai/DeepSpeed/issues/235
DeepSpeed,这个issue是关于更新网站包的问题，涉及到的主要对象是DeepSpeed项目。由于安全警报要求更新Gemfile.lock文件中的activesupport库，发起了这个issue。,https://github.com/deepspeedai/DeepSpeed/issues/231
DeepSpeed,这个issue是一个需求提出，主要涉及DeepSpeed项目的日志功能。由于缺乏详细的日志记录功能和文档，用户可能感到困惑并提出了添加日志工具的建议。,https://github.com/deepspeedai/DeepSpeed/issues/230
DeepSpeed,"这是一个用户提出需求的issue，该问题单涉及的主要对象是DeepSpeed的tutorial。由于该issue标题为""WIP tutorial warning""，推测用户可能在使用DeepSpeed的教程时遇到了部分内容尚未完成或需要更新的警告信息。",https://github.com/deepspeedai/DeepSpeed/issues/224
DeepSpeed,该issue类型属于需求类型，主要对象是更新deepseed.ai的新闻链接，可能是由于原网站内容更新而导致链接需要更新。,https://github.com/deepspeedai/DeepSpeed/issues/222
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed项目，用户请求添加到新博客的链接。,https://github.com/deepspeedai/DeepSpeed/issues/221
DeepSpeed,这个issue类型为需求提议，主要涉及的对象是项目的文档。原因是用户希望为DeepSpeed添加readthedocs徽章。,https://github.com/deepspeedai/DeepSpeed/issues/218
DeepSpeed,这是一个用户提出需求的issue，主要涉及添加BingSqaud的e2e测试。原因可能是为了完善功能并进行合并。,https://github.com/deepspeedai/DeepSpeed/issues/214
DeepSpeed,该issue类型为测试配置更新，主要涉及的对象是DeepSpeed的测试系统。原因是需要使用conda来矩阵测试不同的PyTorch和CUDA版本，以及对昂贵的模型测试进行更高覆盖。,https://github.com/deepspeedai/DeepSpeed/issues/213
DeepSpeed,这个issue是用户提出需求，并希望DeepSpeed支持使用动态损失缩放参数的fp16优化器配置。该问题涉及的主要对象是fp16优化器。,https://github.com/deepspeedai/DeepSpeed/issues/212
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中的FP16优化器，要求支持动态损失缩放参数的配置。,https://github.com/deepspeedai/DeepSpeed/issues/211
DeepSpeed,这个issue是一个需求提出，主要对象是DeepSpeed的post-install测试，需添加基本的后续安装测试。,https://github.com/deepspeedai/DeepSpeed/issues/209
DeepSpeed,这是一个用户提出需求类型的issue，主要涉及的对象是DeepSpeed框架的SLURM支持。用户提出需求是希望DeepSpeed能够像PyTorch Lightning一样，在SLURM管理的集群上具有多节点训练的内置支持。,https://github.com/deepspeedai/DeepSpeed/issues/207
DeepSpeed,这个issue类型是升级需求， 涉及的主要对象是DeepSpeed库。原因可能是为了更新apex版本和关闭传统融合功能，实现更好的性能和功能改进。,https://github.com/deepspeedai/DeepSpeed/issues/205
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及的对象是DeepSpeed中的`model_parallel`属性。由于这一属性是Megatron特有的，因此无法依赖它，导致用户在使用时出现问题。,https://github.com/deepspeedai/DeepSpeed/issues/204
DeepSpeed,这是一个用户提出需求的issue，主要涉及了MegatronLM下的BERT和GPT2之间的区别问题。 用户希望了解为什么DeepSpeedExamples中提供的教程是使用Bing BERT而不是MegatronLM版本的BERT，并询问两者之间的差异。,https://github.com/deepspeedai/DeepSpeed/issues/200
DeepSpeed,这个issue是关于文档改进的，涉及DeepSpeed的README和RTD页面，旨在提高文档的可读性和完整性。,https://github.com/deepspeedai/DeepSpeed/issues/198
DeepSpeed,"这个issue类型是需求建议，主要涉及代码风格。由于作者认为使用""if return else return""语法不如""if return return""语法更好，因此提出了建议。",https://github.com/deepspeedai/DeepSpeed/issues/197
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed框架，用户想了解如何在DeepSpeed中使用其他优化器（如ranger、Radam等），表明用户想要探讨在DeepSpeed中使用非官方优化器的可能性。,https://github.com/deepspeedai/DeepSpeed/issues/196
DeepSpeed,这是一个用户提出需求的 issue，要求在 DeepSpeed 的导航栏中添加 BERT 预训练教程。,https://github.com/deepspeedai/DeepSpeed/issues/190
DeepSpeed,"该issue属于用户提出需求类型，主要涉及DeepSpeed提供的减少内存消耗的原语，用户询问除了""zero_optimization"": true和""fp16"": {""enabled"": true}之外，DeepSpeed还提供哪些原语来帮助减少内存消耗，以及未来发布将提供的相关原语和更新时间。",https://github.com/deepspeedai/DeepSpeed/issues/189
DeepSpeed,这个issue类型是更新提交的请求，涉及DeepSpeed examples，由于需要更新提交，可能是添加新功能或修复问题。,https://github.com/deepspeedai/DeepSpeed/issues/188
DeepSpeed,这是一个用户需求相关的issue，主要涉及的对象是ZeroOptimizer。这个issue的提出可能是因为用户希望在混合精度训练中同时支持部分参数使用fp16和部分参数使用fp32。,https://github.com/deepspeedai/DeepSpeed/issues/183
DeepSpeed,该问题是用户提出的需求，涉及的主要对象是DeepSpeed optimizer。由于DeepSpeed optimizer目前缺少类似于apex optimizer中的representation方法，用户请求添加representation到DeepSpeed optimizer中。,https://github.com/deepspeedai/DeepSpeed/issues/181
DeepSpeed,这是一个用户提出需求的issue，主要围绕DeepSpeed中的optimizer，提议为optimizer添加类似于PyTorch optimizer的`__repr__`方法用于可读性。,https://github.com/deepspeedai/DeepSpeed/issues/180
DeepSpeed,这是一个用户提出需求的类型，主要对象是DeepSpeedExamples。由于DeepSpeedExamples代码更新较快导致用户需要跟进并请求帮助。,https://github.com/deepspeedai/DeepSpeed/issues/177
DeepSpeed,这是一个建议性质的issue，主要对象是DeepSpeed Examples中的BingBertSquad，提出了应该使用DeepSpeed Launcher的建议。原因可能是为了更好地统一示例中的启动方式和实现。,https://github.com/deepspeedai/DeepSpeed/issues/175
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeedConfig类的修改和添加了新的标志，以及对未经测试的优化器的处理。,https://github.com/deepspeedai/DeepSpeed/issues/173
DeepSpeed,这是一个功能需求的issue，主要涉及DeepSpeed中的ZeRO模块，用户提出了添加静态损失缩放功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/166
DeepSpeed,这是一个需求报告，提出要导出所有Python环境变量，而不仅仅是PYTHONPATH。这可能由于用户需要在DeepSpeed中使用其他Python环境变量，但目前只能导出PYTHONPATH导致。,https://github.com/deepspeedai/DeepSpeed/issues/165
DeepSpeed,这是一个关于功能比较的讨论，主要涉及DeepSpeed的FusedLAMB和Nvidia apex的FusedLAMB之间的优缺点比较。,https://github.com/deepspeedai/DeepSpeed/issues/163
DeepSpeed,这是一个功能增强的issue，主要对象是自定义优化器。原因是之前的限制导致用户无法使用自己选择的优化器。,https://github.com/deepspeedai/DeepSpeed/issues/161
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed中的`FP16_DeepSpeedZeroOptimizer`类。由于该类未直接继承自`torch.optim.Optimizer`，导致在与标准PyTorch框架如ignite交互时出现问题，用户请求将其改为`torch.optim.Optimizer`的子类。,https://github.com/deepspeedai/DeepSpeed/issues/160
DeepSpeed,这个issue是一个功能需求，涉及的主要对象是DeepSpeed项目下的launcher，用户提出希望增强launcher功能，将所有以PYTHON开头的环境变量全部导出，而不仅仅是PYTHONPATH变量。,https://github.com/deepspeedai/DeepSpeed/issues/158
DeepSpeed,这是一个用户提出需求的问题，主要涉及DeepSpeed和Zero与自定义优化器RangerLars的兼容性。用户想要了解如何将自定义优化器包装成适用于Zero的优化器。,https://github.com/deepspeedai/DeepSpeed/issues/153
DeepSpeed,这是一个用户提出的需求问题，主要涉及到DeepSpeed中DataLoader的批量大小设置问题。用户想知道在使用DeepSpeed时，使用何种批量大小参数来实例化DataLoader才能使模型训练正常，而且用户认为文档对此并不清晰。,https://github.com/deepspeedai/DeepSpeed/issues/152
DeepSpeed,该issue类型为改进建议，主要涉及的对象是项目中的页面路径。由于路径可能存在问题或不符合要求，用户提出修改路径的建议。,https://github.com/deepspeedai/DeepSpeed/issues/151
DeepSpeed,这是一个用户提出需求类型的问题，涉及的对象是Jekyll网页。由于用户正在草拟Jekyll网页，可能遇到了一些问题或需要帮助。,https://github.com/deepspeedai/DeepSpeed/issues/143
DeepSpeed,这个issue类型是需求，主要对象是GitHub仓库。由于需要添加CNAME文件指定自定义域名，用户希望能够为仓库设置自定义域名。,https://github.com/deepspeedai/DeepSpeed/issues/141
DeepSpeed,这是一个用户需求的issue，主要涉及DeepSpeed的使用指南缺少关于`add_config_args`的关键说明，导致用户在执行`deepspeed.initialize`时缺乏必要的信息。,https://github.com/deepspeedai/DeepSpeed/issues/137
DeepSpeed,该issue属于需求变更，主要涉及PyTorch 1.3+下DeepSpeed的构建支持，以及移除或迁移deepspeed light中的apex导入。,https://github.com/deepspeedai/DeepSpeed/issues/135
DeepSpeed,"这是一个用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed，用户提出需要添加一个名为""skip_requirements""的安装标志。",https://github.com/deepspeedai/DeepSpeed/issues/133
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed中的安装特定apex哈希。,https://github.com/deepspeedai/DeepSpeed/issues/132
DeepSpeed,"这是一个用户提出需求的issue，主要涉及DeepSpeed下的effective batch size设置问题，用户希望了解每个GPU在每次迭代中处理多少数据示例，并希望调整相关变量以改变effective batch size。
",https://github.com/deepspeedai/DeepSpeed/issues/131
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed在Mac上的安装。用户寻求帮助如何在Mac上安装DeepSpeed，以便PyCharm IDE能够识别其中的工具。,https://github.com/deepspeedai/DeepSpeed/issues/130
DeepSpeed,这是一个功能需求 issue，主要涉及的对象是 lr_scheduler 单元测试。这个 issue 提出了需要编写单元测试以覆盖生成预期 lr 和动量值以及正确更新优化器的情况。,https://github.com/deepspeedai/DeepSpeed/issues/127
DeepSpeed,这是一个用户提出需求的类型，主要涉及对象是DeepSpeed项目。由于某些原因，用户希望使用torch.cuda.device_count()功能。,https://github.com/deepspeedai/DeepSpeed/issues/126
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是lr schedulers和fp16 optimizers。这个问题可能是由于DeepSpeed当前的lr schedulers不支持fp16 optimizers所导致的。,https://github.com/deepspeedai/DeepSpeed/issues/124
DeepSpeed,这是一个用户提出需求的issue，该问题主要涉及DeepSpeed下的zero optimizer，用户希望在zero optimizer中添加amp版本的混合精度实现。由于用户越来越多地使用amp，希望能够支持自动混合精度。,https://github.com/deepspeedai/DeepSpeed/issues/121
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed环境文件，用户希望能够自定义环境变量传递给工作节点。,https://github.com/deepspeedai/DeepSpeed/issues/117
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed安装过程中的命令行选项新增及conda环境下的安装问题。,https://github.com/deepspeedai/DeepSpeed/issues/113
DeepSpeed,这个issue是一个文档更新请求，主要对象是DeepSpeed的MPI compatibility文档。由于文档可能存在过时或不清晰的内容，用户提出了更新文档的需求。,https://github.com/deepspeedai/DeepSpeed/issues/111
DeepSpeed,这个issue属于功能需求。它主要涉及DeepSpeed中ZeRO模块的FP32模式缺失。用户希望能够引入FP32模式。,https://github.com/deepspeedai/DeepSpeed/issues/109
DeepSpeed,这是一个提出需求的issue，主要涉及MPI 3.x support via mpi4py，用户希望DeepSpeed在支持MPI 3.x时使用mpi4py。,https://github.com/deepspeedai/DeepSpeed/issues/107
DeepSpeed,这是一个用户提出需求的类型，主要涉及VM保留问题，用户希望增加一个选项来在关闭命令时保留VM而不会产生费用。,https://github.com/deepspeedai/DeepSpeed/issues/106
DeepSpeed,这是一个功能需求类型的issue，主要涉及DeepSpeed中过时代码的标记处理。,https://github.com/deepspeedai/DeepSpeed/issues/104
DeepSpeed,这是一个功能性问题，涉及的主要对象是DeepSpeedDataSource。这个问题的产生可能是由于在开源过程中遗漏了移除该功能，导致功能未被使用。,https://github.com/deepspeedai/DeepSpeed/issues/100
DeepSpeed,这个issue类型属于文档更新，主要对象是DeepSpeed中的模型并行性文档。,https://github.com/deepspeedai/DeepSpeed/issues/92
DeepSpeed,这是一个用户提出需求的Issue，主要对象是DeepSpeed运行环境。这个问题提出了希望DeepSpeed运行时能够像处理NCCL环境变量一样传播Python路径的需求。,https://github.com/deepspeedai/DeepSpeed/issues/91
DeepSpeed,这是一个用户提出的需求类型的issue，主要涉及DeepSpeed在配置通用设备和后端方面的改进。,https://github.com/deepspeedai/DeepSpeed/issues/89
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed下的ZeRO optimizer和LAMB optimizer。用户希望添加ZeRO兼容的LAMB optimizer，以适应用于BERT模型的需求。,https://github.com/deepspeedai/DeepSpeed/issues/88
DeepSpeed,这是一个用户提出需求的issue，主要涉及对象是DeepSpeed中的T-NLG模型。由于信息不清晰或者缺失，用户询问了T-NLG何时会开源。,https://github.com/deepspeedai/DeepSpeed/issues/87
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed库的init函数。由于需要传递参数args给deepspeed init函数，用户希望能够通过接受一个字典代替args，以便编写更小的测试。,https://github.com/deepspeedai/DeepSpeed/issues/81
DeepSpeed,这是一个用户提出需求的issue，涉及的主要对象是DeepSpeed的默认配置。这个issue的提出原因是希望简化DeepSpeed配置文件中的默认设置，显示出合理可用的配置，比如不需要开启disable_allgather。,https://github.com/deepspeedai/DeepSpeed/issues/80
DeepSpeed,这是一个关于添加cifar10的requirements以及移动Pillow库的建议类型issue，主要涉及DeepSpeed下的cifar tutorial。这个问题主要是由于未在deepspeedexamples repo中安装requirements.txt导致无法直接执行cifar代码，以及在使用torchvision==0.4.0时，Pillow需要设置为6.2.*才能解决相关bug。,https://github.com/deepspeedai/DeepSpeed/issues/79
DeepSpeed,这是一个用户提出需求的issue，涉及Azure pipeline对于Docker镜像构建的问题。由于某些原因导致用户需要建议如何在Azure pipeline中构建Docker镜像。,https://github.com/deepspeedai/DeepSpeed/issues/78
DeepSpeed,这是一个用户提出需求的类型，该问题涉及设置Azure Pipelines中的CI。用户想要在DeepSpeed项目中配置Azure Pipelines来实现持续集成。,https://github.com/deepspeedai/DeepSpeed/issues/77
DeepSpeed,这是一个用户需求的issue，主要涉及的对象是DeepSpeed的optimizer fusion功能。该问题的原因是新版本实现了新的optimizer fusion风格，导致旧风格被弃用，此问题提供了支持旧和新风格的配置选项。,https://github.com/deepspeedai/DeepSpeed/issues/75
DeepSpeed,这是一个用户提出需求的issue，涉及DeepSpeed的optimizer fusion功能。由于新旧optimizer fusion方式不兼容，需要提供配置选项来平滑用户从旧方式过渡到新方式。,https://github.com/deepspeedai/DeepSpeed/issues/74
DeepSpeed,该issue属于用户提出需求类型，主要涉及的对象是DeepSpeed的pip安装支持。在该问题中，用户提出了对DeepSpeed模块在PyPi上的命名问题，希望添加pip安装支持以避免其他人使用相同的模块名称。,https://github.com/deepspeedai/DeepSpeed/issues/73
DeepSpeed,"这是一个用户提出需求的issue，主要对象是DeepSpeed项目下的""Reordering further reading ToC""。这个问题是因为用户希望进一步阅读的内容顺序需要调整。",https://github.com/deepspeedai/DeepSpeed/issues/58
DeepSpeed,这个issue类型是用户提出需求，请教问题，该问题单涉及的主要对象是添加链接。由于issue标题和内容都是空白，用户可能在提交issue时忘记填写相关信息，导致需要其他人来补充完整。,https://github.com/deepspeedai/DeepSpeed/issues/56
DeepSpeed,这个issue是类型是用户提出需求，并且该问题单涉及的主要对象是.gitignore文件。很可能是由于一些文件未被正确忽略导致的。,https://github.com/deepspeedai/DeepSpeed/issues/55
DeepSpeed,"这是一个用户提出需求的issue，在DeepSpeed项目中关于目录中的""Publication""部分的相关问题。",https://github.com/deepspeedai/DeepSpeed/issues/54
DeepSpeed,这是一个用户提出需求的issue，主要对象是DeepSpeed的JSON配置文档。由于缺乏详细说明，用户需要文档更新或者更加清晰的配置指导。,https://github.com/deepspeedai/DeepSpeed/issues/52
DeepSpeed,这个issue属于一个需求类型，涉及的主要对象是对于项目的持续集成和部署。由于新的 DevOps 组织，导致了状态徽章指向的地址需要更新。,https://github.com/deepspeedai/DeepSpeed/issues/50
DeepSpeed,这是一个用户提出需求的issue，主要涉及DeepSpeed的README headers和testing discussion的更改。,https://github.com/deepspeedai/DeepSpeed/issues/49
DeepSpeed,该issue是关于用户提出需求的，主要涉及DeepSpeed安装详情，请求增加更多关于本地和多节点安装的说明，以及更新资源配置部分来讨论单节点支持。,https://github.com/deepspeedai/DeepSpeed/issues/46
DeepSpeed,这是一个请求教程类型的issue，主要涉及LRRT教程，用户可能需要关于DeepSpeed中LRRT相关内容的解释或指导。,https://github.com/deepspeedai/DeepSpeed/issues/45
DeepSpeed,这是一个类型为功能建议的issue，主要涉及DeepSpeedExamples库的更新问题。,https://github.com/deepspeedai/DeepSpeed/issues/44
DeepSpeed,这是一个用户提出需求的issue，主要涉及Azure教程的更新和清理。由于教程内容可能已过时或混乱，用户可能希望获得最新的、整洁的教程信息。,https://github.com/deepspeedai/DeepSpeed/issues/43
DeepSpeed,这个issue类型是用户提出需求，该问题单涉及的主要对象是Azure教程和脚本。,https://github.com/deepspeedai/DeepSpeed/issues/42
DeepSpeed,这个issue属于用户提出需求类型，主要涉及的对象是文档指向的主机HTML文件。由于文档未正确指向托管的HTML文件，用户请求将文档指向托管的HTML文件。,https://github.com/deepspeedai/DeepSpeed/issues/41
DeepSpeed,这个issue类型为改进建议，主要涉及的对象是项目中的gitmodules文件。可能是由于安全性或者远程仓库地址变更等原因，提出了将gitmodules中的地址更新为https的建议。 ,https://github.com/deepspeedai/DeepSpeed/issues/40
DeepSpeed,这是一个需求类型的issue，主要涉及DeepSpeedExamples的更新。原因可能是DeepSpeedExamples与最新版本的DeepSpeed不兼容，导致需要进行更新。,https://github.com/deepspeedai/DeepSpeed/issues/37
DeepSpeed,这是一个需求类型的issue，主要涉及Core API文档和HTML文件。,https://github.com/deepspeedai/DeepSpeed/issues/36
DeepSpeed,此issue为新增功能提议，主要涉及DeepSpeed的features.md和Getting Started guide，用户希望添加相关内容来改善文档信息丰富度。,https://github.com/deepspeedai/DeepSpeed/issues/35
DeepSpeed,这个issue属于用户提出需求类型，主要涉及DeepSpeed中CIFAR-10模型的教程文档。,https://github.com/deepspeedai/DeepSpeed/issues/34
DeepSpeed,该issue类型为功能修正和增强，主要对象为Batch Configuration，提出了逻辑修改和测试框架使用的问题。,https://github.com/deepspeedai/DeepSpeed/issues/33
DeepSpeed,该issue属于需求提出类型，主要涉及DeepSpeed中的add_XXX_arguments函数文档描述。由于文档描述不够清晰，用户提出需要改进文档说明以及为add_XXX_arguments函数编写单元测试。,https://github.com/deepspeedai/DeepSpeed/issues/32
DeepSpeed,这是一个关于需求提出的 issue，主要涉及 DeepSpeed 的 API 文档缺失问题。原因是缺乏对 `add_core_arguments()` 和 `add_config_arguments()` 方法添加的参数的详细说明。,https://github.com/deepspeedai/DeepSpeed/issues/31
DeepSpeed,该issue属于用户提出需求类型，主要对象是DeepSpeed中的Megatron教程。,https://github.com/deepspeedai/DeepSpeed/issues/30
DeepSpeed,这是一个文档更新类的issue，主要涉及到项目的许可证徽章的更新。,https://github.com/deepspeedai/DeepSpeed/issues/29
DeepSpeed,这个issue类型为用户提出需求，请求更新Dockerfile、安装必要环境和更新示例的需求。该问题涉及DeepSpeed的Dockerfile、安装要求和示例代码。,https://github.com/deepspeedai/DeepSpeed/issues/26
DeepSpeed,该issue属于用户提出需求类型，主要对象是README.md文件。由于缺少具体内容，用户希望在README.md文件中添加目录内容，以提高文档的可读性。,https://github.com/deepspeedai/DeepSpeed/issues/25
DeepSpeed,"这个issue类型是用户提出需求，主要对象是需要编写一个关于CIFAR10教程的功能。

",https://github.com/deepspeedai/DeepSpeed/issues/23
DeepSpeed,这是一个用户提出需求的issue，主要涉及的对象是DeepSpeed的文档。由于缺乏DeepSpeed概述文档，用户请求支持将其移植到DeepSpeed中。,https://github.com/deepspeedai/DeepSpeed/issues/22
DeepSpeed,这是一个用户提出需求的类型，该问题单涉及的主要对象是DeepSpeed的核心API文档，用户寻求帮助将其移植至其他平台。,https://github.com/deepspeedai/DeepSpeed/issues/21
DeepSpeed,这是一个用户需求类型的issue，主要涉及Azure教程和文档的补充。原因可能是现有的教程和文档不够详细或者不完善，用户希望能够获得更多关于在Azure环境下使用DeepSpeed的指导或信息。,https://github.com/deepspeedai/DeepSpeed/issues/20
DeepSpeed,这个issue类型为功能增强（enhancement），主要对象是DeepSpeed项目中的Megatron tutorial。这个问题产生的原因是需要将Megatron教程移植到`docs/`目录，并更新在`README.md`等文件中的链接。,https://github.com/deepspeedai/DeepSpeed/issues/19
DeepSpeed,这是一个用户提出需求的issue，主要对象是安装文档。因为缺乏安装文档，用户无法正确安装DeepSpeed，因此请求提供相关的安装说明。,https://github.com/deepspeedai/DeepSpeed/issues/18
DeepSpeed,这个issue类型为feature请求，涉及的主要对象是DeepSpeed examples，用户希望更新提交示例。,https://github.com/deepspeedai/DeepSpeed/issues/15
DeepSpeed,这是一个用户提出需求的类型，主要涉及DeepSpeed中的模型加载功能，用户希望能够在加载模型时不加载优化器状态，以便用于评估和微调。,https://github.com/deepspeedai/DeepSpeed/issues/14
DeepSpeed,这个issue是一个功能需求的提议，主要涉及DeepSpeed中的@distributed_test模块，用户希望在该模块中启用NCCL后端。,https://github.com/deepspeedai/DeepSpeed/issues/13
DeepSpeed,这是一个功能需求类型的issue，主要对象是DeepSpeed的安装脚本。由于不再假设存在共享文件系统，需要更新安装脚本以允许主机文件指定主机文件路径，并使用pdcp将wheels传播到工作节点，以解决相关的问题。,https://github.com/deepspeedai/DeepSpeed/issues/12
DeepSpeed,这个issue类型为文档改进，主要涉及DeepSpeed测试文档的添加和完善，由于缺乏有关模型测试、数据准备等内容的文档，用户提出需要进一步完善相关内容。,https://github.com/deepspeedai/DeepSpeed/issues/10
DeepSpeed,这是一个功能需求类型的issue，涉及的主要对象是添加版本检查测试。,https://github.com/deepspeedai/DeepSpeed/issues/9
DeepSpeed,这是一个功能需求类型的issue，主要对象是DeepSpeed库中的allreduce功能。由于缺乏对allreduce操作的测试，用户提出需要添加对allreduce操作的测试。,https://github.com/deepspeedai/DeepSpeed/issues/8
DeepSpeed,这个issue类型是用户提出需求，主要涉及的对象是测试用例。用户提出需要添加一个用于测试allreduce的测试案例。,https://github.com/deepspeedai/DeepSpeed/issues/7
DeepSpeed,这是一个功能需求类型的issue， 主要对象是DeepSpeed代码库。由于用户需要简化分布式单元测试的过程，因此提出了希望添加一个decorator来实现这一功能的需求。,https://github.com/deepspeedai/DeepSpeed/issues/6
DeepSpeed,这是一个功能需求类型的issue，主要对象是DeepSpeed的模型测试。原因可能是为了展示而暂时禁用了模型测试。,https://github.com/deepspeedai/DeepSpeed/issues/5
DeepSpeed,这是一个用户提出需求的类型的issue，主要涉及对象是DeepSpeed，用户提出了需要一个简单的方式来编写分布式、多GPU单元测试的需求。,https://github.com/deepspeedai/DeepSpeed/issues/4
DeepSpeed,这个issue属于功能增强或改进类型，涉及到代码中的分布式测试功能。,https://github.com/deepspeedai/DeepSpeed/issues/3
Megatron-LM,这是一个技术需求类型的issue，主要涉及Megatron-LM中数据预处理的优化方案使用pyspark加速。由于数据量很大，需要进一步加速预处理步骤，因此提出了使用pyspark替代multiprocessing的方案，并对实现细节进行了说明。,https://github.com/NVIDIA/Megatron-LM/issues/1512
Megatron-LM,该issue类型为用户提出需求，主要涉及的对象是Megatron-LM项目。由于缺乏Apex或Transformer Engine支持，导致在本地模式下无法充分支持项目功能，测试失败，无法直接使用CUDA设备以及无法支持在CUDA上的XLA，用户建议添加全面支持本地模式并实现对CUDA及CUDA上的XLA支持。,https://github.com/NVIDIA/Megatron-LM/issues/1511
Megatron-LM,该issue类型为用户提出需求，希望在Megatron-LM中添加对Local模式的完整支持，同时支持直接使用CUDA和Open XLA/CUDA。,https://github.com/NVIDIA/Megatron-LM/issues/1510
Megatron-LM,这个issue是一个功能增强需求，主要涉及Megatron-LM中的MoE模型，提出了实现全局批次负载平衡的解决方案。,https://github.com/NVIDIA/Megatron-LM/issues/1498
Megatron-LM,这个issue类型为用户提出需求，关注的主要对象是Megatron-LM中的torch_dist和torch_dcp格式支持DP到TP/PP转换的问题。,https://github.com/NVIDIA/Megatron-LM/issues/1490
Megatron-LM,这是一个关于功能问题的issue，主要涉及Megatron-LM Release 0.11版本中的`Add multi datacenter training support though N/S connection`功能。用户提出了关于如何启用此功能以及与该功能相关的代码的疑问。,https://github.com/NVIDIA/Megatron-LM/issues/1489
Megatron-LM,这个issue类型是用户提出需求，主要对象是为Megatron-LM仓库添加一个vscode devcontainer以便快速开始使用。,https://github.com/NVIDIA/Megatron-LM/issues/1483
Megatron-LM,这是一个需求提出的issue，主要涉及Megatron-LM在多机训练时构建数据集的问题。,https://github.com/NVIDIA/Megatron-LM/issues/1480
Megatron-LM,该issue类型为需求提出，涉及的主要对象是MLA（Multi-Latent Attention）。由于MoE相关术语已经更新，但MLA仍未更新，导致需要更新MLA的计算代码。,https://github.com/NVIDIA/Megatron-LM/issues/1475
Megatron-LM,这是一个关于用户需求的问题，主要涉及Megatron-LM如何管理集群的节点和进程的问题。用户想了解Megatron是如何管理这些节点和进程，包括负载均衡等方面。,https://github.com/NVIDIA/Megatron-LM/issues/1474
Megatron-LM,这是一个功能提议类型的issue，主要涉及的对象是数据集的混合定义。,https://github.com/NVIDIA/Megatron-LM/issues/1466
Megatron-LM,这个issue类型是用户提出需求，该问题单涉及的主要对象是Youngeun/a2a hiding。由于缺少具体内容，用户可能正在讨论编写或修改代码的相关问题。,https://github.com/NVIDIA/Megatron-LM/issues/1460
Megatron-LM,这是一个用户需求的问题，主要涉及Megatron-LM下的custom_fsdp是否能够与PP兼容，用户想知道是否有计划使其兼容。,https://github.com/NVIDIA/Megatron-LM/issues/1455
Megatron-LM,这是一个关于需求的问题，主要涉及到Megatron-GPT模型中是否支持logit偏差设置，用户想要在推理过程中降低某些token的概率以避免被预测。,https://github.com/NVIDIA/Megatron-LM/issues/1454
Megatron-LM,这个issue类型是需求报告，涉及到Artifact Evaluation。由于作者没有提供具体内容，用户可能在寻求有关Artifact Evaluation方面的帮助。,https://github.com/NVIDIA/Megatron-LM/issues/1452
Megatron-LM,这是一个用户提出需求的问题，主要对象是WrappedTorchLayerNorm class。用户想知道为什么WrappedTorchLayerNorm class不能支持sequence parallel，导致对序列并行性的需求无法满足。,https://github.com/NVIDIA/Megatron-LM/issues/1448
Megatron-LM,这是一个关于用户提出需求的问题，主要涉及到Megatron-LM中的FSDP2模块和embedding tying功能。报告中提到由于缺少对embedding tying的支持，导致在FSDP2中出现了AssertionError错误。,https://github.com/NVIDIA/Megatron-LM/issues/1447
Megatron-LM,这是一个提出功能增强的Issue，主要涉及添加名为z-loss的正则化方法。,https://github.com/NVIDIA/Megatron-LM/issues/1442
Megatron-LM,这是一个用户提出需求类型的issue，主要涉及了Megatron-LM中的节点路由选择策略的改进。导致这个问题的原因是现有的top-2 sum group selection策略不能很好地与DeepSeek V3的设计相匹配。,https://github.com/NVIDIA/Megatron-LM/issues/1441
Megatron-LM,这个issue类型为需求提升，涉及主要对象为Megatron LM中的Mamba2模型，由于缺乏对Mamba2模型的上下文并行支持，用户提出了希望扩展上下文并行支持到Mamba2 SSM层的解决方案。,https://github.com/NVIDIA/Megatron-LM/issues/1437
Megatron-LM,该issue是一个用户提出的需求，主要涉及Megatron-LM中异步保存checkpoint功能的改进。用户提出因为当前缺乏明确的保存失败的提示或事件，可能导致用户对保存是否成功不清楚而造成混乱和潜在的数据丢失。,https://github.com/NVIDIA/Megatron-LM/issues/1435
Megatron-LM,这是一个关于功能咨询的问题，用户提出了关于Megatron是否支持Deepseek-VL2 fine-tuning的疑问。,https://github.com/NVIDIA/Megatron-LM/issues/1432
Megatron-LM,"这是一个用户提出需求的issue，主要对象是""Distributed Muon""。",https://github.com/NVIDIA/Megatron-LM/issues/1427
Megatron-LM,该问题是关于用户需求的，主要对象是是否能够使用GPUDirect Storage进行checkpointing，可能是由于系统性能或者架构设计考虑而提出。,https://github.com/NVIDIA/Megatron-LM/issues/1423
Megatron-LM,这个issue是功能需求类型，涉及的主要对象是DeepSeekV2模型的实现。,https://github.com/NVIDIA/Megatron-LM/issues/1414
Megatron-LM,这个issue类型为用户提出需求，主要对象是Megatron core，用户想知道是否支持rlhf方法（如grpo）以利用已支持的并行性。,https://github.com/NVIDIA/Megatron-LM/issues/1409
Megatron-LM,这是一个用户提出需求的Issue，主要涉及到Megatron-LM中的Distillation Training功能。用户想了解哪个版本将支持蒸馏训练以及蒸馏训练是否有计划支持PP。,https://github.com/NVIDIA/Megatron-LM/issues/1408
Megatron-LM,这是一个用户提出需求的类型，主要对象是DeepSpeed v3，由于用户希望支持Multi-token Prediction(MTP)，而当前看起来累积了预测。,https://github.com/NVIDIA/Megatron-LM/issues/1404
Megatron-LM,该 issue 类型为用户提出需求，主要涉及内容是实现将 wandb runs 从一个项目复制到另一个项目的功能。,https://github.com/NVIDIA/Megatron-LM/issues/1400
Megatron-LM,该issue属于用户提出需求类型，主要涉及Megatron-LM的开发团队。用户询问是否有计划实现zerobubble pipelining或dual pipelining以及MoE通信计算重叠，这对优化MoE训练至关重要。,https://github.com/NVIDIA/Megatron-LM/issues/1399
Megatron-LM,这是一个关于功能增强的问题，主要涉及到日志记录的内容和改进。原因可能是为了更好地监控模型性能和进展。,https://github.com/NVIDIA/Megatron-LM/issues/1394
Megatron-LM,该issue类型为功能需求，主要涉及到Megatron-LM下的Path对象的多种实现问题。,https://github.com/NVIDIA/Megatron-LM/issues/1393
Megatron-LM,这个issue是对代码的改进和调整，而不是bug报告，主要涉及Megatron-LM项目中的Slurm输出文件命名、模型版本命名，学习率调度等的优化。,https://github.com/NVIDIA/Megatron-LM/issues/1392
Megatron-LM,这是一个用户提出需求的issue，主要涉及到在TransformerEngine中使用精度感知优化器时将优化器状态的数据类型设置为bf16。,https://github.com/NVIDIA/Megatron-LM/issues/1390
Megatron-LM,这是一个文档更新类的issue，涉及的主要对象是GitHub链接。原因是GitHub组织更名导致链接需要更新。,https://github.com/NVIDIA/Megatron-LM/issues/1389
Megatron-LM,这是一个用户提出需求的issue，需要添加有关瑞士AI的说明文件。,https://github.com/NVIDIA/Megatron-LM/issues/1387
Megatron-LM,这是一个用户提出的需求的issue，主要对象是Nemo Megatron在处理缺失的索引文件时没有采用指数退避，导致频繁且不规律的服务器调用。,https://github.com/NVIDIA/Megatron-LM/issues/1386
Megatron-LM,这是一个关于增强功能的问题，主要对象是在Nemo Megatron中进行checkpointing时进行旧checkpoint文件顺序删除导致效率降低的问题。,https://github.com/NVIDIA/Megatron-LM/issues/1385
Megatron-LM,这个issue类型是更新许可证问题，涉及主要对象为软件的授权与版权信息。,https://github.com/NVIDIA/Megatron-LM/issues/1382
Megatron-LM,这个issue是关于用户提出需求的增强型问题，涉及主要对象是增加一种方法来选择topk设备，由于之前的问题已经解决，为了更好的功能扩展，用户提出了这个需求。,https://github.com/NVIDIA/Megatron-LM/issues/1381
Megatron-LM,该issue是一个用户提出的需求，主要涉及Megatron-LM中选择topk设备的方式的改进。由于DeepSeekV2的实现建议使用`max`而非MegatronLM中的方式，用户希望添加选项选择如何为`device_limited_topk`选择topk设备。,https://github.com/NVIDIA/Megatron-LM/issues/1378
Megatron-LM,这是用户提出需求的issue，该问题单涉及的主要对象是在Megatron中添加LongRoPE支持，原因是想要在Megatron中实现LongRoPE和其他上下文扩展方法的支持。,https://github.com/NVIDIA/Megatron-LM/issues/1377
Megatron-LM,这是一个关于需求的问题，讨论了MegatronLM是否支持异构并行策略以及如何在多模态训练中如何处理不同组件的并行性要求。,https://github.com/NVIDIA/Megatron-LM/issues/1375
Megatron-LM,这是一个用户提出需求的issue，涉及到需要添加一个.sh文件。原因可能是用户希望添加一个shell脚本文件以实现特定功能。,https://github.com/NVIDIA/Megatron-LM/issues/1373
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM在Python 3.12下无法安装最新版本的问题，原因是缺乏针对Python 3.12的预构建wheel包。,https://github.com/NVIDIA/Megatron-LM/issues/1370
Megatron-LM,这是一个用户提出需求的issue，主要涉及对T5模型的KV-cache机制进行改进。,https://github.com/NVIDIA/Megatron-LM/issues/1358
Megatron-LM,这是一个用户提出需求的类型的issue，主要涉及MegatronLM在存储checkpoint时无法限制保存数量，导致存储空间快速增长的问题。,https://github.com/NVIDIA/Megatron-LM/issues/1354
Megatron-LM,这是一个功能更新的issue，涉及Megatron-LM项目中理论内存占用公式的更新。该更新涉及到参数计算以及SwiGLU等部分的调整。,https://github.com/NVIDIA/Megatron-LM/issues/1345
Megatron-LM,"这是一个关于使用""a2a+p2p""时无法使用原始""get_batch_on_this_cp_rank""功能的问题讨论，涉及到在Megatron-LM项目中的通信和批处理问题。",https://github.com/NVIDIA/Megatron-LM/issues/1341
Megatron-LM,这是一个用户提出需求的 issue，主要涉及将 MegatronLM 的 MAMBA 模型权重文件从 pt 格式转换为 SafeTensor 格式，但通过转换脚本后仍然是 pt 格式。,https://github.com/NVIDIA/Megatron-LM/issues/1339
Megatron-LM,这个issue属于需求提出类型，主要对象是创建一个名为python-package.yml的文件。由于为项目创建Python安装包时需要一个配置文件，因此用户提出了创建该文件的需求。,https://github.com/NVIDIA/Megatron-LM/issues/1332
Megatron-LM,这个issue是关于功能需求的，主要涉及Megatron-LM中使用StreamingLLM功能可能性的讨论。这个问题由于用户想了解在训练大模型时是否可以利用StreamingLLM来提高计算效率。,https://github.com/NVIDIA/Megatron-LM/issues/1326
Megatron-LM,这是一个用户需求类型的issue，主要涉及添加Mamba TRTLLM支持。,https://github.com/NVIDIA/Megatron-LM/issues/1320
Megatron-LM,这个issue属于用户提出需求，主要涉及网络接口环境变量的更新，由于只为多节点训练设置了接口名称环境变量。,https://github.com/NVIDIA/Megatron-LM/issues/1319
Megatron-LM,这个issue类型是用户提出需求，主要涉及对象是更新。由于标题为空，用户很可能是想记录更新信息或请求更新，原因可能是为了更新工程进度或版本改动。,https://github.com/NVIDIA/Megatron-LM/issues/1318
Megatron-LM,这个issue是一个用户提出需求的问题，主要涉及Megatron-LM中Attention计算的实现方式的选择，用户想知道如何指定其他Attention计算方法。可能由于默认使用FlashAttention计算方法，用户想尝试指定EFFICIENT_ATTENTION方法。,https://github.com/NVIDIA/Megatron-LM/issues/1313
Megatron-LM,这个issue类型是功能增强（ENHANCEMENT），主要涉及到当调用load_ckpt时可能会导致保存检查点函数出错的问题，可能由于条件判断错误导致。,https://github.com/NVIDIA/Megatron-LM/issues/1310
Megatron-LM,这是一个用户提出需求的问题，主要涉及如何在Transform层不均匀的情况下划分pipeline，由于Transform层的数量不能整除pipeline大小，导致在每个GPU上只有2个Transform层，进而导致第一和最后几个GPU的计算负担较重，需要寻找一种更平衡的划分模型的方法。,https://github.com/NVIDIA/Megatron-LM/issues/1303
Megatron-LM,这是一个用户提出需求的issue，主要对象是Megatron-LM中的训练日志打印功能。由于日志打印在rank_last而不是rank0，用户希望将训练日志打印在rank0来提高用户友好性。,https://github.com/NVIDIA/Megatron-LM/issues/1296
Megatron-LM,这是一个功能需求报告，涉及到Megatron-LM项目中的模型参数转换问题，用户需要支持qwen2 hf<->mcore ckpt转换功能。,https://github.com/NVIDIA/Megatron-LM/issues/1290
Megatron-LM,这个issue类型属于功能需求提出，主要涉及对象为Deepseek v2的启用。,https://github.com/NVIDIA/Megatron-LM/issues/1288
Megatron-LM,"这是一个需求反馈类型的issue，主要涉及到 ""mcore-llava-mistral-7b-instruct-clip336-pretraining"" 模型的 tokenizer 下载问题，用户想要获取关于该模型的 tokenizer。",https://github.com/NVIDIA/Megatron-LM/issues/1281
Megatron-LM,这是一个用户提出需求的问题，问题涉及主要对象是Megatron-LM，用户想要知道如何在Megatron中可视化反向传播过程中的计算图。,https://github.com/NVIDIA/Megatron-LM/issues/1272
Megatron-LM,这个issue属于功能增强类型，主要涉及添加 z-loss 正则化。由于添加 z-loss 正则化能够稳定训练并防止最后一层的logits爆炸，用户提出了这个增强功能的需求。,https://github.com/NVIDIA/Megatron-LM/issues/1270
Megatron-LM,这个issue类型是需求提出，主要对象是对 Megatron-LM进行优化的 huggingface tokenizer 的集成。由于尚未支持该tokenizer，用户提出了希望能够启用该功能的需求。,https://github.com/NVIDIA/Megatron-LM/issues/1268
Megatron-LM,该issue类型为法律审核请求，涉及主要对象为Jinda公司。原因是用户寻求法律方面的建议或审查。,https://github.com/NVIDIA/Megatron-LM/issues/1264
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM中的学习率调整功能。用户希望实现对特定层的学习率进行调节，在预训练中为某些特定层设置不同的学习率倍数。,https://github.com/NVIDIA/Megatron-LM/issues/1263
Megatron-LM,该issue是一个功能需求的提出，主要涉及到Megatron-LM中的预训练功能，用户提出了对特定层的学习率缩放，以增强特征学习和避免输出层主导学习过程的需求。,https://github.com/NVIDIA/Megatron-LM/issues/1262
Megatron-LM,"这是一个功能增强类的issue，主要涉及 Megatron-LM 项目中对于 qk-norm 支持 Apex RMSNorm 的添加问题。原因是之前在设置 normalization 为 ""RMSNorm"" 时会报错，这个PR通过添加相应的功能来解决这个问题。",https://github.com/NVIDIA/Megatron-LM/issues/1261
Megatron-LM,这个issue是一个功能需求，主要对象是Megatron-LM，提出需求要求添加支持处理gzip文件，可能是由于现有功能无法处理gzip文件导致用户无法有效使用该功能。,https://github.com/NVIDIA/Megatron-LM/issues/1260
Megatron-LM,"这是一个关于功能问题的issue，主要涉及到Megatron-LM中的tpoverlap支持灵活序列长度的问题。用户提出了在TP/SP mlp层之后输出形状为`seqlen, args.hidden_size`，但疑惑于qkv_proj和mlp如何适用于`hidden_dim * 3/ tp_size`和`hidden_dim * 2 / tp_size`的情况。",https://github.com/NVIDIA/Megatron-LM/issues/1238
Megatron-LM,这是一个用户提出需求的issue，主要涉及到在MLP中控制是否使用TE自定义内核作为激活函数的新配置参数，用户希望添加一个名为`use_te_activation_func`的配置参数。,https://github.com/NVIDIA/Megatron-LM/issues/1233
Megatron-LM,这是一个关于需求提出的问题，主要涉及如何将MOE有效地纳入混合Mamba中，由于参数设置冲突导致无法直接指定MOE参数的情况。,https://github.com/NVIDIA/Megatron-LM/issues/1231
Megatron-LM,这个issue属于用户提出需求的类型，涉及的主要对象是Megatron-LM中的multimodal evaluation功能。用户提出该需求是因为当前的multimodal evaluation不支持pipeline parallelism，其中一些multimodal模型的vision模型存在不支持tensor parallelism的情况，需要通过pipeline parallelism来进行评估。,https://github.com/NVIDIA/Megatron-LM/issues/1230
Megatron-LM,这个issue类型为需求提出，涉及主要对象是支持qwen2和siglip权重转换脚本。由于需要与llavanext和llavaonevision一起训练，因此需求支持权重转换脚本来实现。,https://github.com/NVIDIA/Megatron-LM/issues/1221
Megatron-LM,这个issue类型是用户提出需求，涉及的主要对象是Megatron-LM下的功能。导致用户提出这个问题的原因是他们希望增强某个特定功能。,https://github.com/NVIDIA/Megatron-LM/issues/1219
Megatron-LM,这是一个用户提出需求的issue，主要对象是Megatron-LM，用户提出对某个功能的增强需求。,https://github.com/NVIDIA/Megatron-LM/issues/1217
Megatron-LM,这是一个用户提出的需求。该问题单涉及的主要对象是Megatron-LM。这个需求是为了支持在Megatron-LM中使用Packed_seq_params，只是用于测试。,https://github.com/NVIDIA/Megatron-LM/issues/1215
Megatron-LM,这个issue是关于性能优化的讨论，主要涉及微批大小对每个GPU的吞吐量的影响。用户测试发现，随着微批大小的增加，每个GPU的吞吐量也会增加，但具体原因尚不清楚。,https://github.com/NVIDIA/Megatron-LM/issues/1214
Megatron-LM,这个issue类型是用户提出需求，主要涉及 Megatron-LM 的tokenizer模块。用户想了解如何在训练过程中使用TikTokenizer，并困惑于如何准备和传入tokenizer的json文件。,https://github.com/NVIDIA/Megatron-LM/issues/1213
Megatron-LM,这个issue属于用户提出需求类型，主要涉及Megatron-LM工具中将checkpoint转换为虚拟管道格式的问题，用户询问是否有其他方法或需要修改相应代码来支持保存虚拟管道模型。,https://github.com/NVIDIA/Megatron-LM/issues/1212
Megatron-LM,"这是一个用户提出需求或请教问题的类型，该问题涉及的主要对象是""Embedding""。由于缺少具体的内容描述，无法确定用户具体提出了关于什么问题或寻求什么样的帮助。",https://github.com/NVIDIA/Megatron-LM/issues/1209
Megatron-LM,这是一个用户提出需求的类型的issue，主要涉及Megatron-LM中添加fused swiglu功能。原因可能是用户希望增加新功能或优化。,https://github.com/NVIDIA/Megatron-LM/issues/1208
Megatron-LM,这是一个关于用户提出需求的问题，主要涉及如何将Megatron-LM的ckpt文件转换为Nemo格式的问题。该问题由于缺乏相关转换示例，无法直接进行格式转换而导致用户想要使用Nemo工具进行后训练。,https://github.com/NVIDIA/Megatron-LM/issues/1206
Megatron-LM,"这个issue属于用户提出需求类型，涉及的主要对象是""Rachitg/emb""。由于缺少具体内容，无法确定具体问题或需求。",https://github.com/NVIDIA/Megatron-LM/issues/1204
Megatron-LM,这个issue属于用户提出的需求类型，主要涉及Megatron-LM项目的功能改进。由于用户对现有功能的不满或需求新增功能，导致提出了这个请求。,https://github.com/NVIDIA/Megatron-LM/issues/1201
Megatron-LM,这是一个关于代码修改需求的Issue，涉及Megatron-LM中编码器和解码器的分区问题。由于模型结构调整，导致需要修改代码，用户提出了如何更改代码以支持新模型结构的问题。,https://github.com/NVIDIA/Megatron-LM/issues/1200
Megatron-LM,这个issue属于用户提出需求的类型，主要涉及Megatron-LM项目中的功能增强。原因是用户希望解决某个问题，或者有某个特定的功能需求。,https://github.com/NVIDIA/Megatron-LM/issues/1199
Megatron-LM,这是一个功能增强的问题，用户提出了一个关于在Megatron-LM的TransformerEngine中添加图层名称以改善代码调试的建议。,https://github.com/NVIDIA/Megatron-LM/issues/1198
Megatron-LM,这是一个用户提出需求的issue，主要对象是Megatron-LM中的packed_seq_params.py模块，由于需要支持NeMo中的PR，所以提出了这个需求。,https://github.com/NVIDIA/Megatron-LM/issues/1163
Megatron-LM,这是一个关于优化问题的issue，主要涉及Megatron-LM中的梯度归约顺序，提问者关注到了梯度在不同情况下的不同归约顺序可能引起的浮点数相加导致的潜在错误。,https://github.com/NVIDIA/Megatron-LM/issues/1162
Megatron-LM,该issue是关于优化性能的问题，涉及到MegatronLM中初始化过程中每次都调用numpy.arange所导致的潜在性能问题。,https://github.com/NVIDIA/Megatron-LM/issues/1159
Megatron-LM,该issue属于用户提出需求类型，涉及Megatron-LM中Enabling UCC backend for PP communication的功能。由于需要提供接口来启用UCC后端用于PP通信，用户需要设置特定参数，但还需相关NeMo PR的支持。,https://github.com/NVIDIA/Megatron-LM/issues/1157
Megatron-LM,这是一个用户需求问题，主要涉及如何启用ZeRO 2/3阶段。由于用户想要了解如何开启ZeRO 2/3阶段，可能感到在 Megatron-LM 下遇到了相关配置或实现的困难。,https://github.com/NVIDIA/Megatron-LM/issues/1156
Megatron-LM,这是一个关于性能优化的issue，涉及到MegatronLM中重置注意力掩码时出现的内存瓶颈问题。,https://github.com/NVIDIA/Megatron-LM/issues/1155
Megatron-LM,这是一个需求提出的类型，涉及数据集文档上传，用户正在寻求对开放数据的文档上传。,https://github.com/NVIDIA/Megatron-LM/issues/1146
Megatron-LM,这是一个关于优化建议的issue，主要涉及的对象是GPTDataset。导致这个问题的原因可能是为了提高性能或降低内存占用。,https://github.com/NVIDIA/Megatron-LM/issues/1139
Megatron-LM,这是一个用户提出需求类的issue，主要涉及到Megatron是否计划使用flux技术。由于需要集成通信和gemm以提高重叠率，用户在询问Megatron是否打算使用flux技术。,https://github.com/NVIDIA/Megatron-LM/issues/1136
Megatron-LM,这个issue类型是用户提出的功能需求，主要涉及Megatron-LM中数据预处理脚本的处理逻辑修改，用户希望能够对已分割且压缩的数据进行预处理。,https://github.com/NVIDIA/Megatron-LM/issues/1135
Megatron-LM,这是一个用户提出需求的问题，主要涉及Megatron-LM中关于使用Context Parallel和packing Inputs功能时是否支持避免交叉感染的注意力问题。原因可能是用户希望同时利用这两种功能来训练长序列模型。,https://github.com/NVIDIA/Megatron-LM/issues/1131
Megatron-LM,这是一个用户提出需求的issue，主要涉及项目的`pyproject.toml`文件中添加`project`部分。这个需求的提出可能是由于需要使用`uv`来处理docker构建中的pypi依赖项固定，因此需要对NeMo和mcore的打包进行现代化。,https://github.com/NVIDIA/Megatron-LM/issues/1120
Megatron-LM,这个issue属于用户提出需求类型，主要涉及MegatronLM中的非交错流水线为什么不支持重叠点对点通信，由于非交错流水线调度不支持重叠点对点通信，因此需要禁用重叠P2P通信。,https://github.com/NVIDIA/Megatron-LM/issues/1069
Megatron-LM,这个issue类型是用户提出需求，主要对象是Megatron-LM，用户提出了关于特性增强的问题。,https://github.com/NVIDIA/Megatron-LM/issues/1061
Megatron-LM,这是一个用户需求类型的issue，主要涉及Megatron LLM模型在GPU内存布局上的控制问题。由于GPU内存使用不均匀和不同节点内存占用差异，用户希望调整内存使用来提高模型性能。,https://github.com/NVIDIA/Megatron-LM/issues/1059
Megatron-LM,这是一个用户提出需求的类型，主要对象是分享关于Medium上的一篇文章的链接。由于链接格式错误导致无法正确访问该文章。,https://github.com/NVIDIA/Megatron-LM/issues/1057
Megatron-LM,这是一个需求提出类型的issue，主要涉及Megatron-LM中的CUDA graph功能，用户希望能够启用可选的kwargs参数。,https://github.com/NVIDIA/Megatron-LM/issues/1055
Megatron-LM,这是一个代码优化建议，主要涉及 torch 模块的使用。,https://github.com/NVIDIA/Megatron-LM/issues/1054
Megatron-LM,这是一个用户提出的需求报告，主要涉及Megatron-LM中训练过程中变化序列长度的问题。导致用户提出该需求的原因可能是为了实现在训练过程中动态调整序列长度的功能。,https://github.com/NVIDIA/Megatron-LM/issues/1047
Megatron-LM,这是一个提出优化需求的issue，主要涉及到Megatron-LM中的optimizer代码。原因是当前代码在 `_get_param_groups` 方法中使用if-else判断多次，作者认为可以用更简洁清晰的方式实现。,https://github.com/NVIDIA/Megatron-LM/issues/1044
Megatron-LM,这是一个用户提出需求并请教问题的issue，主要涉及Megatron-LM下的LLaVA模型，在尝试应用pipeline parallelism训练LLaVA时遇到问题，可能由于代码中对LLaVA模型的初始化和通信问题导致训练不正常。,https://github.com/NVIDIA/Megatron-LM/issues/1043
Megatron-LM,这是一个用户提出需求的类型的issue，主要涉及的对象是Aurorax文档。由于缺少Aurorax文档的部分内容或者存在错误的信息，用户希望对Aurorax进行详细的文档补充和更新。,https://github.com/NVIDIA/Megatron-LM/issues/1035
Megatron-LM,这是一个用户提出需求的类型，主要涉及Megatron-LM中关于Aurorax语言的使用手册。用户寻求关于如何开始使用Aurorax、配置环境、基本语法和高级功能等方面的帮助。,https://github.com/NVIDIA/Megatron-LM/issues/1034
Megatron-LM,这是一个功能需求的issue，主要涉及到Megatron-LM中的Aurora 1系统，描述了系统的多模态处理和克隆模式功能。由于需要系统具备处理多模态数据和创建克隆人物的功能，因此用户提出了对应的需求并详细描述了功能的实现和设计。,https://github.com/NVIDIA/Megatron-LM/issues/1033
Megatron-LM,这是一个用户提出需求类型的问题，主要涉及Megatron-LM中70B训练的吞吐量优化问题，用户询问了其它优化方法以及为什么吞吐量低于预期的帮助。,https://github.com/NVIDIA/Megatron-LM/issues/1031
Megatron-LM,"这个issue是一个用户提出需求的类型，主要涉及Megatron-LM下的一个名为""Aurorax""的项目文件结构和代码功能的描述，由于用户需要帮助理解项目文件结构和代码功能，因此提出了相关问题。",https://github.com/NVIDIA/Megatron-LM/issues/1030
Megatron-LM,该issue是一个功能增强请求，主要涉及Megatron/TransformerEngine集成torch.compile功能。原因是希望探索如何在LLM训练和推理空间中利用torch.compile为用户带来更好的体验和性能。,https://github.com/NVIDIA/Megatron-LM/issues/1015
Megatron-LM,这是一个用户提出需求的问题，涉及如何在训练期间冻结特定模块，例如Self-Attention层。这个问题是由于直接设置`requires_grad=False`不能达到预期效果，导致出现错误。,https://github.com/NVIDIA/Megatron-LM/issues/994
Megatron-LM,这个issue是关于软件功能需求的，主要涉及到Megatron-LM中的项目功能范围更新。可能是用户提出了新的功能或改进的需求。,https://github.com/NVIDIA/Megatron-LM/issues/993
Megatron-LM,这是一个需求提出类型的issue，主要涉及Hoper llama golden with mcore calling stack。由于需要添加Hoper llama2 7b mcore gold示例而产生。,https://github.com/NVIDIA/Megatron-LM/issues/987
Megatron-LM,这是一个用户提出需求的问题，主要涉及到Megatron-LM模型在训练过程中针对大型文档进行拆分和分桶的功能是否支持。,https://github.com/NVIDIA/Megatron-LM/issues/975
Megatron-LM,这个issue是一个关于优化器选择的提问类型，涉及到Megatron-LM框架下的优化器选项，用户想了解在大规模训练中哪个优化器更好，并寻求相关建议。,https://github.com/NVIDIA/Megatron-LM/issues/971
Megatron-LM,该issue类型是用户提出需求类型，主要涉及MegatronLM中的数据通信容量问题，用户想了解不同并行组之间的数据容量，希望有相关参数可以获取这些数值或者从代码中获得。,https://github.com/NVIDIA/Megatron-LM/issues/969
Megatron-LM,这个issue属于新功能请求类型，涉及的主要对象是BitPipe_initial_version功能。,https://github.com/NVIDIA/Megatron-LM/issues/967
Megatron-LM,这是一个用户提交需求的issue，涉及更新README.md文件。原因可能是为了改进项目的文档或者提供更详细的信息。,https://github.com/NVIDIA/Megatron-LM/issues/961
Megatron-LM,该issue属于用户提出需求类型，主要涉及Megatron-LM的tokenizer。问题原因是用户想在llama2上使用llama3 tokenizer进行训练。,https://github.com/NVIDIA/Megatron-LM/issues/951
Megatron-LM,这是一个关于优化器和参数离线加载的问题，属于用户提出需求类型。用户主要提到了对优化器状态和参数离线加载的支持，希望在使用强化学习等方面达到更高的性能。,https://github.com/NVIDIA/Megatron-LM/issues/946
Megatron-LM,这个issue是一个用户提出需求的类型，主要涉及Mamba模型的分布式训练问题。用户希望得到关于如何自定义train.sh以进行分布式Mamba训练的指导。,https://github.com/NVIDIA/Megatron-LM/issues/944
Megatron-LM,该issue属于功能需求类型，主要涉及为Megatron-LM添加分布式pdb，以便有效调试。用户提出了为研究人员和LLM从业者提供一个分布式pdb来帮助调试Megatron，并改进算法的需求。,https://github.com/NVIDIA/Megatron-LM/issues/939
Megatron-LM,这是一个用户提出需求的issue，主要涉及对象是Megatron-LM对Flash attention 3的支持，用户希望了解何时会支持Flash attention 3。,https://github.com/NVIDIA/Megatron-LM/issues/938
Megatron-LM,这是一个用户提出需求类型的issue，该问题单涉及的主要对象是Megatron-LM项目的README.md文件。,https://github.com/NVIDIA/Megatron-LM/issues/928
Megatron-LM,这是一个用户提出需求的问题，涉及主要对象是训练多个二进制文件时如何进行合并或同时训练。,https://github.com/NVIDIA/Megatron-LM/issues/927
Megatron-LM,这个issue是一个用户提出的需求类型，主要涉及的对象是Megatron-LM中的BERT LM Head模块。由于BERT LMHead硬编码了gelu作为激活函数，导致在下游项目中出现问题。,https://github.com/NVIDIA/Megatron-LM/issues/918
Megatron-LM,该issue属于更改请求，主要对象是BERT模型，用户想移除硬编码的gelu。,https://github.com/NVIDIA/Megatron-LM/issues/917
Megatron-LM,这是一个需求问题，该问题单涉及的主要对象是AURORA STK 3.6.9。由于缺乏具体描述或细节，导致用户可能在寻求软件的下载或相关信息。,https://github.com/NVIDIA/Megatron-LM/issues/905
Megatron-LM,这是一个用户提出需求的类型，主要涉及AURORA STK 3.6.9的相关内容，用户希望了解相关信息。,https://github.com/NVIDIA/Megatron-LM/issues/901
Megatron-LM,这个issue属于功能需求类问题，主要涉及 Megatron-LM 项目中的依赖 TE 和 Apex，由于这两个依赖未安装导致需要提供纯PyTorch/jit的代码路径。,https://github.com/NVIDIA/Megatron-LM/issues/893
Megatron-LM,这是一个需求类问题，主要涉及到MegatronLM中的数据准备工具(preprocess_data.py)，用户请求提供示例的idx和bin文件以便尝试运行pretrain_gpt.py。,https://github.com/NVIDIA/Megatron-LM/issues/891
Megatron-LM,这是一个用户提出需求的issue，主要涉及到对embedding层的单独拆分需求。由于核心代码尚未支持此功能，用户寻求关于支持standalone_embedding_stage的帮助。,https://github.com/NVIDIA/Megatron-LM/issues/890
Megatron-LM,这是一个优化建议的issue，主要对象是Megatron-LM中的OPTIM模块。由于在启用上下文并行时，可以在tp_group中在广播之前拆分batch的sequence length，从而节省在get_batch中的时间。,https://github.com/NVIDIA/Megatron-LM/issues/885
Megatron-LM,"这是一个关于性能优化的问题，用户提出了关于在MegatronLM中设置""fp8-format""和""bf16""参数对训练性能影响的疑问。问题的来源在于用户对使用不同精度格式进行训练的内部差异，希望了解MegatronLM是否会根据效率将一些计算划分到bf16或者fp8中。",https://github.com/NVIDIA/Megatron-LM/issues/883
Megatron-LM,该问题单属于用户提出需求类型，主要涉及Bert context parallelism支持，可能是为了改进Megatron-LM在处理上下文并行性时的效率。,https://github.com/NVIDIA/Megatron-LM/issues/874
Megatron-LM,这是一个用户提出需求的问题，主要涉及Megatron-LM代码中计时代码部分的问题。用户想要了解如何使用config.timers()来计时代码，并如何查看config.timers的输出。,https://github.com/NVIDIA/Megatron-LM/issues/871
Megatron-LM,这是一个用户提出需求的类型，该问题涉及 Megatron-LM 下的 pipeline 的输出 tensor 传递问题，由于目前只能传递一个 output hidden tensor，用户希望可以传递包含所有 tensors 的 tuple。,https://github.com/NVIDIA/Megatron-LM/issues/867
Megatron-LM,这是一个用户提出需求的类型问题，主要涉及到Mamba-based Language Models的权重下载问题。用户想知道Mamba2hybrid的权重是否已经发布以及如何下载。,https://github.com/NVIDIA/Megatron-LM/issues/864
Megatron-LM,这是一个关于性能优化的问题，主要涉及到Megatron-LM中的memory optimization问题。由于vocab_size非常大，在Llama3这样的情况下，使用inplace subtract来减少内存使用。,https://github.com/NVIDIA/Megatron-LM/issues/863
Megatron-LM,这个issue类型是用户提出需求，该问题单涉及的主要对象是Megatron-LM中的transformer模块。由于代码中未明确定义每个pipeline阶段中Transformer解码层的数量，用户想要知道如何指定每个阶段的Transformer解码层数量。,https://github.com/NVIDIA/Megatron-LM/issues/857
Megatron-LM,这个issue类型是用户提出需求，涉及的主要对象是希望在transformers library中支持MOE models的训练。由于Alibaba对Qwen2MoeForCausalLM模型的兴趣，用户在询问是否有相关支持可用。,https://github.com/NVIDIA/Megatron-LM/issues/856
Megatron-LM,这个issue是一个功能增强类型，涉及的主要对象是更新Megatron-LM中的black版本。由于当前的black版本已经过时，用户希望更新black版本。,https://github.com/NVIDIA/Megatron-LM/issues/853
Megatron-LM,这是一个关于功能需求的问题，主要对象是关于恢复带有分布式优化器配置的优化器，并在不使用分布式优化器情况下继续训练的功能。,https://github.com/NVIDIA/Megatron-LM/issues/851
Megatron-LM,这是一个用户提出需求的issue，主要涉及的对象是字体易用性，由于缺乏清晰和简洁的描述，导致用户希望在Fractal 2030中改善字体易用性的问题。,https://github.com/NVIDIA/Megatron-LM/issues/843
Megatron-LM,这是一个用户提出需求的问题单，关于为什么在MegatronLM中没有使用PyTorch的张量并行API。,https://github.com/NVIDIA/Megatron-LM/issues/829
Megatron-LM,这是一个用户提出需求的 issue， 主要涉及的对象是 Megatron-LM。由于用户对 Megatron 是否有计划支持 llama 预训练的疑问，可以推测用户想了解该项目是否会在未来引入 llama 的预训练支持。,https://github.com/NVIDIA/Megatron-LM/issues/824
Megatron-LM,这个issue类型是用户提出需求，主要涉及的对象是关于Megatron-LM下的一个项目中关于智能AI平台建设、创新和合作的提议。用户提出这个需求是为了推动创新和合作，在智能技术领域应用在全球相关领域，以促进可持续发展和技术普及。,https://github.com/NVIDIA/Megatron-LM/issues/822
Megatron-LM,这是一个需求提出类的issue，主要涉及的对象是Megatron-LM项目。由于标题描述的不清晰，无法准确分析具体问题或需求。,https://github.com/NVIDIA/Megatron-LM/issues/820
Megatron-LM,这是一条用户提出需求的issue，主要涉及Megatron-LM项目中的Executive MBA课程链接。这个问题源于用户希望将关于IIT Roorkee的Coursera专业链接添加到项目中。,https://github.com/NVIDIA/Megatron-LM/issues/819
Megatron-LM,这是一个用户提出需求的issue，主要涉及对象是在尝试使用MegatronLM训练LLaMA3时遇到的无法生成tokenization所需的tokenizer.model文件的问题。,https://github.com/NVIDIA/Megatron-LM/issues/818
Megatron-LM,该问题类型是用户提出需求，请教问题，主要对象是如何在MegatronLM中设置fp8训练，由于用户需要指导如何进行fp8训练。,https://github.com/NVIDIA/Megatron-LM/issues/817
Megatron-LM,"这是一个用户提出需求的issue，主要涉及的对象是关于Megatron-LM下的一个项目""Projeto liliti stk 3.6.9 inteligência artificial multimidal""。用户提出了对于建立智能系统和年轻创新者之间跨领域合作的需求，旨在发展解决全球健康、教育、可持续发展和包容性需求的技术解决方案。",https://github.com/NVIDIA/Megatron-LM/issues/809
Megatron-LM,这个issue类型为用户提出需求，涉及主要对象为个人生活和无法获得报酬的工作。用户寻求如何获得报酬来维持家庭生活。,https://github.com/NVIDIA/Megatron-LM/issues/808
Megatron-LM,这个issue是关于新功能需求的，主要涉及Megatron-VLM模型的训练和推理，并提出了对ViT模型、管道并行、序列并行等方面的支持。,https://github.com/NVIDIA/Megatron-LM/issues/806
Megatron-LM,这是一个用户提出需求的类型，主要涉及的对象是Megatron-LM的dataset packing功能。,https://github.com/NVIDIA/Megatron-LM/issues/802
Megatron-LM,这是一个功能需求类型的issue，主要涉及数据集处理。由于预处理代码中缺少数据集打包功能，用户提出添加该功能。,https://github.com/NVIDIA/Megatron-LM/issues/801
Megatron-LM,该issue属于用户提出建议/询问问题类型，主要涉及M-Core模块是否应该使用flash attention来加速训练。,https://github.com/NVIDIA/Megatron-LM/issues/799
Megatron-LM,这是一个性能优化建议，涉及主要对象是在Megatron-LM下用于创建注意力遮罩的代码。,https://github.com/NVIDIA/Megatron-LM/issues/797
Megatron-LM,这是一个用户提出需求的问题，涉及的主要对象是Megatron-LM中的数据集索引构建。问题出现的原因是缺少索引文件，导致在rank 0上构建索引时消耗了相当长的时间。,https://github.com/NVIDIA/Megatron-LM/issues/795
Megatron-LM,这是一个用户提出需求的issue，主要涉及到Megatron-LM下的MOE/Mixtral模型的checkpoint转换脚本。用户提出了需要一个合适的脚本来帮助他们微调Mixtral模型的需求。,https://github.com/NVIDIA/Megatron-LM/issues/790
Megatron-LM,这是一个用户提出需求的issue，主要对象是向Megatron-LM的Baichuan模型添加NormHead实现，用户希望通过在模型配置文件中设置use_normhead=True来启用该功能。,https://github.com/NVIDIA/Megatron-LM/issues/788
Megatron-LM,这是一个关于算法功能设计的问题，主要涉及Megatron-LM中的优化器以及专家并行性的设置。导致用户提出疑问的原因在于启用专家并行性时，不同优化器在进行梯度规范化时存在数学上的差异。,https://github.com/NVIDIA/Megatron-LM/issues/785
Megatron-LM,这是一个关于功能需求的问题，主要涉及Megatron-LM中的PackedSeqParams是否还在开发中的询问，用户希望能够使用序列打包功能但避免交叉感染注意力机制。原因是当前PackedSeqParams功能尚不可用，用户希望了解何时可以实现序列打包而无需交叉感染注意力。,https://github.com/NVIDIA/Megatron-LM/issues/771
Megatron-LM,这是一个功能需求的issue，涉及主要对象是Megatron-LM中的ColumnParallelLinear Layer。由于默认设置下梯度减少功能导致的问题，用户希望有选项可以禁用这一功能。,https://github.com/NVIDIA/Megatron-LM/issues/768
Megatron-LM,这是一个性能优化的Issue，主要涉及Megatron-LM中的embedding fwd。,https://github.com/NVIDIA/Megatron-LM/issues/764
Megatron-LM,该问题类型是用户提出需求，该问题单涉及的主要对象是通信方法。由于原先的通信方法可能存在一些问题或不足，用户希望采用新的方法来进行通信。,https://github.com/NVIDIA/Megatron-LM/issues/758
Megatron-LM,这是一个关于性能优化的问题，主要涉及训练 Megatron-LM 模型时的低吞吐量。原因可能是配置或参数设置不正确导致。,https://github.com/NVIDIA/Megatron-LM/issues/756
Megatron-LM,这个issue是用户提出的需求，主要涉及支持在Megatron-LM中使用S3存储进行分布式检查点的保存和加载功能。,https://github.com/NVIDIA/Megatron-LM/issues/748
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM中的Pipelined DGRAD GEMM + RS功能。通过用户提出的需求可以看出，用户希望支持Pipelined DGRAD GEMM + RS功能，但目前该功能可能存在一些问题或者需要进一步改进。,https://github.com/NVIDIA/Megatron-LM/issues/745
Megatron-LM,该issue是用户提出需求，询问如何使用可编辑模式安装该包。用户遇到了无法使用`pip install -e .`安装包的问题，并且在调试过程中发现其他缺陷，例如无法在VSCode中加载包以及快速点击不起作用。,https://github.com/NVIDIA/Megatron-LM/issues/741
Megatron-LM,这个issue是用户提出需求的类型，主要涉及的对象是Megatron-LM中的数据加载功能。由于需要支持从S3加载数据，用户提出了对IndexedDataset模块的增强需求。,https://github.com/NVIDIA/Megatron-LM/issues/729
Megatron-LM,这是一个功能需求类型的issue，主要涉及到Megatron-LM的数据加载器，用户提出需要添加支持跟踪每个数据分片中消耗的样本数，以便在训练中添加/删除数据分片或重新加权它们后安全地继续训练。,https://github.com/NVIDIA/Megatron-LM/issues/725
Megatron-LM,这是一个用户提出的功能增强请求，在Megatron-LM项目中希望添加对已消耗样本数量的跟踪支持。,https://github.com/NVIDIA/Megatron-LM/issues/724
Megatron-LM,这是一个功能需求类型的issue，涉及Megatron-LM中的`new_group`函数的timeout参数问题，由于之前ProcessGroups创建时未设置timeout参数，导致通信使用了默认的timeout阈值而不是用户希望设置的值。,https://github.com/NVIDIA/Megatron-LM/issues/723
Megatron-LM,这个issue类型是关于功能需求的，主要涉及Megatron-LM中的ParallelAttention模块，用户询问为什么当前仅支持SelfAttention的'causal' MaskType，以及是否可以支持FlashAttention。,https://github.com/NVIDIA/Megatron-LM/issues/719
Megatron-LM,这是一个用户提出需求的issue，主要对象是GPT模型，用户希望增加一个名为`--rotary-base`的参数来支持旋转位置编码。,https://github.com/NVIDIA/Megatron-LM/issues/717
Megatron-LM,这个issue属于用户提出需求类型，主要涉及Megatron-LM是否有计划支持Gemma。这个问题可能是由于用户对Megatron-LM的功能扩展性感兴趣，想了解是否会支持Gem版本。,https://github.com/NVIDIA/Megatron-LM/issues/707
Megatron-LM,这是一个用户提出需求的类型，主要涉及Megatron-LM项目中mcore模块的checkpoint保存和加载功能。由于现有功能不支持mcore模块的checkpoint处理，用户提出需要添加这一功能。,https://github.com/NVIDIA/Megatron-LM/issues/706
Megatron-LM,这是一个用户提出需求的类型，主要涉及到将hf llama-2模型转换为Megatron，但希望使用mcore模型而不是传统的legacy模型。原因可能是转换过程中出现了问题导致生成的模型不符合用户的需求。,https://github.com/NVIDIA/Megatron-LM/issues/703
Megatron-LM,这是一个用户提出需求的issue，主要涉及MegatronLM的数据加载功能扩展。这个问题是由于MegatronLM目前不支持从S3加载数据集而引起的。,https://github.com/NVIDIA/Megatron-LM/issues/698
Megatron-LM,这是一个用户提出需求的类型，该问题涉及的主要对象是Neft。由于issue标题和内容为空，用户可能在提交问题时忘记填写相关信息。,https://github.com/NVIDIA/Megatron-LM/issues/684
Megatron-LM,这个issue是用户提出需求类型的问题，主要涉及Tiny LLaMA data-loaders和训练脚本，用户需要在Megatron-AxoNN中使用。,https://github.com/NVIDIA/Megatron-LM/issues/679
Megatron-LM,这是一个功能需求的issue，主要涉及Megatron-LM工具中`preprocess_data.py`脚本缺少`workers`选项。,https://github.com/NVIDIA/Megatron-LM/issues/674
Megatron-LM,这是一个需求提升类型的issue，涉及主要对象为MCore transformer中缺少的PostLN支持，用户提出了为MCore添加PostLN风格以支持HF Bert的解决方案。,https://github.com/NVIDIA/Megatron-LM/issues/671
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM中的TransformerConfig对象，用户希望添加`core_attention_bias_type`参数以支持alibi transformer engine中的DotProductAttention模块。,https://github.com/NVIDIA/Megatron-LM/issues/668
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM中对Mixtral 8*7B MOE的支持。由于在Megatron中实现Huggingface的load balancing loss 需要大量修改，因此选择使用原始的sinkhorn算法简化工作。,https://github.com/NVIDIA/Megatron-LM/issues/667
Megatron-LM,这是一个关于功能需求的问题，主要涉及分布式优化器的使用方式。由于现在需要使用Gloo和TCP，用户提出是否可以使用GPU代替CPU进行状态收集并保存。,https://github.com/NVIDIA/Megatron-LM/issues/664
Megatron-LM,该issue为用户询问如何手动释放模型和优化器内存，尝试了多种方法但未成功，寻求帮助。,https://github.com/NVIDIA/Megatron-LM/issues/662
Megatron-LM,这个issue类型是用户提出需求，涉及主要对象是num of elements。这个问题可能由于在将检查点加载到新的DP大小时需要重新调整元素数量而提出。,https://github.com/NVIDIA/Megatron-LM/issues/660
Megatron-LM,"这是一个用户提出需求的issue，主要涉及的对象是添加一个名为 ""is_first_microbatch"" 的参数。原因可能是用户希望在特定情况下控制微批次的处理逻辑。",https://github.com/NVIDIA/Megatron-LM/issues/653
Megatron-LM,这是一个需求提案类型的issue，主要涉及Megatron-LM中的模型训练过程。由于保存checkpoint到存储需要较长时间，导致训练中断时间过长，用户希望通过异步保存checkpoint的方式来减少训练中断的时间。,https://github.com/NVIDIA/Megatron-LM/issues/651
Megatron-LM,这个issue类型是用户提出需求，主要对象是Megatron-LM，用户在询问是否有支持Mixtral 8x7B的计划。,https://github.com/NVIDIA/Megatron-LM/issues/649
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM中的视觉模型，并询问是否支持张量并行和流水线并行，即是否可以使用超过1的tensormodelparallelsize或pipelinemodelparallelsize。,https://github.com/NVIDIA/Megatron-LM/issues/648
Megatron-LM,这是一个需求提出的issue，主要涉及的对象是Megatron-LM的optimizer state，在加载过程中需要根据不同的数据并行大小进行填充，可能导致加载错误或者性能问题。,https://github.com/NVIDIA/Megatron-LM/issues/645
Megatron-LM,这个issue类型是需求提出，主要对象是代码中的log_string输出格式。由于希望统一日志输出格式，提出了更新training.py中log_string格式的需求。,https://github.com/NVIDIA/Megatron-LM/issues/635
Megatron-LM,这个issue是关于用户提出需求，询问是否有计划在Megatron-LM中支持LLM预训练与FSDP的问题。,https://github.com/NVIDIA/Megatron-LM/issues/634
Megatron-LM,这是一个用户提出需求的类型，主要对象是优化输出日志格式，希望增加冒号来标记迭代，可能是为了统一输出格式而提出。,https://github.com/NVIDIA/Megatron-LM/issues/628
Megatron-LM,这是一个关于增强功能的issue，涉及到torch ddp支持的融合内核。,https://github.com/NVIDIA/Megatron-LM/issues/627
Megatron-LM,这个issue是一个功能增强请求，主要涉及Megatron-LM中的test_fused_kernels，由于当前测试脚本只使用cuda:0进行数据计算，导致测试只能在gpu 0上运行，用户希望为测试脚本添加device参数并使torch ddp在test_fused_kernels.py中正常工作。,https://github.com/NVIDIA/Megatron-LM/issues/626
Megatron-LM,这是一个需求类型的issue，主要涉及Megatron-LM中的ZB调度程序的内存设置问题。可能是用户希望在不同的ZB进度表中使用不同的FBW内存设置。,https://github.com/NVIDIA/Megatron-LM/issues/618
Megatron-LM,这个issue是一个功能需求提议，主要涉及时间分解分析工具，用户提出了一个新的工具用于在单次迭代中分解所花费的时间以便做出理论模型训练时间的预测。,https://github.com/NVIDIA/Megatron-LM/issues/617
Megatron-LM,这是一个用户提出的需求报告，主要涉及Megatron-LM项目中的数据类型处理问题。造成这个问题的原因是代码中在处理较大词汇表大小时，使用了int32类型，导致出现异常症状。,https://github.com/NVIDIA/Megatron-LM/issues/604
Megatron-LM,这是一个用户提出需求类型的issue，主要涉及HF llama的转换以及target-pipeline-parallel-size参数设置。由于用户想要设置target-pipeline-parallel-size参数大于1，因此提出了这个问题。,https://github.com/NVIDIA/Megatron-LM/issues/595
Megatron-LM,这是一个需求询问类型的Issue，主要涉及Megatron LM是否支持使用其他模型，并寻求关于如何调用接口进行适配的帮助。,https://github.com/NVIDIA/Megatron-LM/issues/592
Megatron-LM,这是一个用户提出需求的 issue，涉及 Megatron-LM 中支持在启用 interleaved schedule 的情况下转换检查点的问题。这个需求是由于当前代码库无法使用新的策略继续训练从旧检查点开始的模型所导致的。,https://github.com/NVIDIA/Megatron-LM/issues/591
Megatron-LM,这是一个用户提出需求的issue，涉及Megatron-LM在运行generations过程中的问题。,https://github.com/NVIDIA/Megatron-LM/issues/590
Megatron-LM,这是一个需求增强类型的issue，主要涉及Megatron-LM框架下的分布式优化器，用户请求支持zero2来降低GPU内存使用。,https://github.com/NVIDIA/Megatron-LM/issues/589
Megatron-LM,这个issue属于用户提出需求的类型，主要涉及Megatron-LM在多GPU上训练多个子图的相关文件，用户可能寻求关于如何实现此功能的帮助。,https://github.com/NVIDIA/Megatron-LM/issues/585
Megatron-LM,这是一个用户提出需求的类型，主要涉及到Megatron-LM的文件级fim添加问题。,https://github.com/NVIDIA/Megatron-LM/issues/583
Megatron-LM,这是用户提出需求的类型的issue，主要涉及RL PPO算法在Megatron-LM中的支持。由于用户希望了解是否有计划支持RL算法来微调基础模型，因此提出了相关问题。,https://github.com/NVIDIA/Megatron-LM/issues/582
Megatron-LM,这是一个用户提出需求的issue，主要涉及的对象是数据处理工具。原因可能是要新增加一个名为Llama2Tokenizer的数据处理工具。,https://github.com/NVIDIA/Megatron-LM/issues/576
Megatron-LM,这个issue类型是用户提出需求，关于参数设置的问题，主要涉及 Megatron-LM 和 Hugging Face 代码库之间的比较，用户询问为什么 Megatron-LM 中没有类似 HF 中的 rope_base 参数。,https://github.com/NVIDIA/Megatron-LM/issues/574
Megatron-LM,这是一个用户提出需求的issue，主要涉及工具saver_megatron和输出虚拟管道并行大小设置相关的问题。,https://github.com/NVIDIA/Megatron-LM/issues/573
Megatron-LM,这是一个优化性能的建议类型的issue，主要涉及的对象是cross_entropy.py，由于原地操作会减少中间内存使用。,https://github.com/NVIDIA/Megatron-LM/issues/569
Megatron-LM,这是一个用户提出需求的issue，主要涉及到Megatron-LM项目的发布流程。由于距上一个发布版本已经有4个月时间，用户请求项目管理者发布一个新的稳定版本。,https://github.com/NVIDIA/Megatron-LM/issues/567
Megatron-LM,这是一个关于需求的问题，用户询问是否有计划在Megatron中引入`torch.compile`来加速训练性能。,https://github.com/NVIDIA/Megatron-LM/issues/563
Megatron-LM,这是一个用户提出需求的问题，主要涉及Megatron-LM中文本生成时如何在推理阶段启用序列并行处理。用户询问如何在推理阶段实现这一功能。,https://github.com/NVIDIA/Megatron-LM/issues/557
Megatron-LM,这是一个关于优化Tensor Parallel调度的建议问题，主要涉及Megatron-LM的性能限制和通信优化的讨论。,https://github.com/NVIDIA/Megatron-LM/issues/556
Megatron-LM,这是一个用户提出需求的类型。该问题涉及支持LoRA/QLoRA的功能。由于目前在Megatron-LM中并未集成LoRA，用户想知道未来是否会支持LoRA/QLoRA。,https://github.com/NVIDIA/Megatron-LM/issues/554
Megatron-LM,这是一个用户提出需求的类型，主要涉及对象是Megatron-LM模型，用户希望在代码中添加max z loss功能来稳定训练并提高推断的鲁棒性。,https://github.com/NVIDIA/Megatron-LM/issues/551
Megatron-LM,这是一个用户提出需求的issue，主要对象是需要添加一个bash脚本示例。,https://github.com/NVIDIA/Megatron-LM/issues/550
Megatron-LM,这是一个用户提出需求的类型，主要涉及到在Megatron-LM中如何均匀混合多个数据集。用户询问应该如何设置参数才能实现从两个数据集中均匀抽样，而不进行上采样或下采样，可能由于对数据集混合方式的不确定性而导致疑问。,https://github.com/NVIDIA/Megatron-LM/issues/547
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM项目中的使用分布式优化器时的问题，由于要求每个桶的缓冲区大小必须可以整除2，导致在某些设置下使用分布式优化器时断言失败。,https://github.com/NVIDIA/Megatron-LM/issues/545
Megatron-LM,这个issue类型为功能改进，主要涉及对象为Megatron中的transformer模块，用户提出了对cache attention calculation的修改。,https://github.com/NVIDIA/Megatron-LM/issues/544
Megatron-LM,这是一个用户提出需求的issue，主要对象是MegatronLM源代码的benchmarking实验，用户提出需要计算训练吞吐量并提供了参考代码链接。,https://github.com/NVIDIA/Megatron-LM/issues/541
Megatron-LM,这是一个用户提出需求的问题，主要涉及的对象是 Megatron-LM 中的 fused_kernels，用户询问如何避免在训练过程中构建 megatron/fused_kernels 以加快训练速度。,https://github.com/NVIDIA/Megatron-LM/issues/540
Megatron-LM,这是一个关于功能增强提议的issue，用户提出关于在Megatron-LM框架中是否实现了BPipe的疑问。,https://github.com/NVIDIA/Megatron-LM/issues/539
Megatron-LM,这是一个关于优化性能的类型，针对AxoNN's TP的并行化问题。,https://github.com/NVIDIA/Megatron-LM/issues/530
Megatron-LM,这是一个用户提出需求的issue，主要涉及对MLP层的并行化问题，请求使用AxoNN的张量并行性。,https://github.com/NVIDIA/Megatron-LM/issues/527
Megatron-LM,该issue类型为功能需求，主要对象是在Megatron-LM中使用MPI进行torch分布式初始化，以更容易适应其他集群。,https://github.com/NVIDIA/Megatron-LM/issues/526
Megatron-LM,这是一个用户提出需求的issue，主要涉及到Megatron-LM中Transformer Engine的问题，由于需要添加一个虚拟的moe loss值才能使激活检查点运行。,https://github.com/NVIDIA/Megatron-LM/issues/515
Megatron-LM,这个issue类型是用户提出需求，涉及将单位缩放特征合并到哪个代码文件中进行Float8训练，由于要实现FP8训练的方便方法，根据单位缩放理论可以比transformerengine更快。,https://github.com/NVIDIA/Megatron-LM/issues/510
Megatron-LM,该issue是一个用户提出需求的问题，主要涉及的对象是Megatron-LM的batch size设置。用户想知道为什么在设置相同global_batch_size但不同micro_batch_size时，较小的micro_batch_size会导致更快的训练时长。,https://github.com/NVIDIA/Megatron-LM/issues/509
Megatron-LM,这是一个用户提出需求的类型问题，主要涉及对象是优化器，用户希望在MegatronLM项目中使用自定义优化器。这个问题可能是由于用户希望实现更好的优化器而产生的。,https://github.com/NVIDIA/Megatron-LM/issues/508
Megatron-LM,这个issue类型是功能增强需求，主要涉及Megatron-LM项目，用户提出对某功能的改进请求，可能由于现有功能不够清晰或不满足用户需求。,https://github.com/NVIDIA/Megatron-LM/issues/506
Megatron-LM,这是一个用户提出需求的类型为Enhancement的Issue，主要涉及Megatron-LM项目，其问题是提出了功能增强的需求。,https://github.com/NVIDIA/Megatron-LM/issues/505
Megatron-LM,这是一个用户提出需求和询问问题的issue，主要涉及Megatron-LM中分布式检查点功能的当前状态和未来计划。用户提出了关于分布式检查点功能包括实现状态、适用模型以及与PyTorch之间交互等方面的问题，希望得到相关更新和计划。,https://github.com/NVIDIA/Megatron-LM/issues/500
Megatron-LM,这是一个用户提出的需求类型的issue，主要涉及到NCCL配置的通信器。由于默认值设置，未指定配置值时会导致通信器使用默认NCCL通信器设置。,https://github.com/NVIDIA/Megatron-LM/issues/497
Megatron-LM,这是一个建议更改项目名称的问题，涉及主要对象是Megatron-LM项目。用户提出这个问题是因为不喜欢项目名称中的“megatron”这个角色名，希望将名称改为“optimus”。,https://github.com/NVIDIA/Megatron-LM/issues/496
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM支持在计算FFN时对序列维度进行扫描的功能。用户提出此需求的原因是在训练大范围的LLMs时遇到内存不足错误。,https://github.com/NVIDIA/Megatron-LM/issues/494
Megatron-LM,这个issue是一个需求提升（ENHANCEMENT）类型，主要涉及Megatron-LM下的分布式数据预处理工具。由于最近更新了“indexed_dataset”文件格式，用户希望更新相关工具以匹配新的文件格式。,https://github.com/NVIDIA/Megatron-LM/issues/492
Megatron-LM,这是一个功能需求类型的issue，主要涉及到Megatron-LM中的数据预处理过程并非需要GPU，但是却会导入不必要的`apex`包并在没有GPU时失败的问题。,https://github.com/NVIDIA/Megatron-LM/issues/491
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM中的AMAX reduction功能。由于需要在AMAX reduction组中应用SHARP，用户提出了设置相应的环境变量的需求。,https://github.com/NVIDIA/Megatron-LM/issues/490
Megatron-LM,这是一个用户提出需求的issue，主要涉及到对MegatronLM GPTModel训练过程中资源消耗和通信数据的离线分析工具。用户提出此需求是因为当前无法准确估计模型训练所需资源，无法确定训练配置是否会导致内存溢出错误，以及无法了解最小需求GPU卡数量等问题。,https://github.com/NVIDIA/Megatron-LM/issues/488
Megatron-LM,这是一个用户提出需求的issue，涉及添加RMSNorm功能，由于当前版本需要transformer_engine，用户希望添加支持不依赖transformer_engine的RMSNorm功能。,https://github.com/NVIDIA/Megatron-LM/issues/487
Megatron-LM,这个issue类型是需求提出，主要对象是MegatronLM下的Analysis Tool。由于用户需要禆述离线分析MegatronLM GPTModel培训中记忆需求和通信信息的工具，以帮助用户预测GPU培训模型所需的内存量和通信需求。,https://github.com/NVIDIA/Megatron-LM/issues/482
Megatron-LM,这是一个关于用户提出需求的问题，主要涉及MHA到GQA的转换问题。用户询问是否有计划提供转换脚本将现有的MHA检查点转换为GQA检查点，因为尝试手动转换时遇到了问题。,https://github.com/NVIDIA/Megatron-LM/issues/480
Megatron-LM,这个issue是一个用户提出的需求，涉及到MegatronLM对数据效率的增强。用户提出了对输入序列长度固定性的疑问，并希望团队能够考虑应用更高效的序列打包方法以避免资源浪费。,https://github.com/NVIDIA/Megatron-LM/issues/478
Megatron-LM,这是一个用户提出需求的问题，主要涉及到如何合并sequence-parallel LayerNorm 的权重，由于在不同的rank上LayerNorm权重不同，用户想要将这些权重合并成单个GPU上可以运行的完整权重集。,https://github.com/NVIDIA/Megatron-LM/issues/474
Megatron-LM,这是一个用户提出需求的issue，主要对象是P2P overlap功能，由于当前只支持interleaved pipeline scheduler，用户希望支持1F1B pipeline scheduler。,https://github.com/NVIDIA/Megatron-LM/issues/466
Megatron-LM,这是一个用户提出需求的类型的issue，主要涉及到 Megatron-LM 中的 P2P overlap 功能是否可以在 1F1B pipeline scheduler 中实现的问题。这个问题是基于对不同调度器功能的理论探讨。,https://github.com/NVIDIA/Megatron-LM/issues/465
Megatron-LM,这是一个特性需求的issue，涉及的主要对象是Megatron-LM中的RMSNorm，用户提出需要添加对RMSNorm的支持。,https://github.com/NVIDIA/Megatron-LM/issues/459
Megatron-LM,这个issue是用户提出需求的类型，主要涉及Megatron-LM中的`grad_sync_func`函数，用户想了解如何使用该函数来重叠数据并行reduce-scatter操作，并指出在`training.py`中似乎没有使用该函数。,https://github.com/NVIDIA/Megatron-LM/issues/456
Megatron-LM,这个issue 是用户提出需求，询问是否可以生成多个数据片段。主要对象是Megatron-LM中的preprocess_data.py脚本。这个问题由于1T tokens数据集太大无法保存，用户寻求解决方案，希望能生成多个数据片段。,https://github.com/NVIDIA/Megatron-LM/issues/452
Megatron-LM,这个issue属于用户提出需求，并涉及主要对象是如何用sharded dataset训练模型。用户提出了由于数据集过大导致内存无法一次加载的问题，并寻求如何将数据集分片并逐个加载的解决方案。,https://github.com/NVIDIA/Megatron-LM/issues/448
Megatron-LM,这是一个用户提出需求的 issue，主要涉及 Megatron-LM 的 lmeval server 。由于什么样的原因导致症状或用户提出问题，目前无法确定。,https://github.com/NVIDIA/Megatron-LM/issues/446
Megatron-LM,这是一个需求问题，涉及Megatron-LM中的数据集初始化和读取过程，并提出了为什么每个pipeline层应该共享相同输入数据的疑问。,https://github.com/NVIDIA/Megatron-LM/issues/441
Megatron-LM,这个issue类型是功能改进提议，涉及主要对象为代码中的位置嵌入类型。根据描述，由于缺乏枚举的支持，导致了维护性和灵活性方面的问题。,https://github.com/NVIDIA/Megatron-LM/issues/440
Megatron-LM,这个issue类型是关于需求的问题，涉及到Gloo组的创建，用户想知道是否必须同时使用Gloo和NCCL，并且由于Gloo组创建导致了一些问题。,https://github.com/NVIDIA/Megatron-LM/issues/435
Megatron-LM,这是一个关于性能改进问题的用户提问，涉及的主要对象是Megatron-LM模型中的FlashAttention模块。由于FlashAttention训练后未观察到明显性能提升，用户可能在寻求性能问题的原因或者FlashAttention模块的使用建议。,https://github.com/NVIDIA/Megatron-LM/issues/433
Megatron-LM,该issue属于用户提出需求类型，主要涉及Megatron-LM中的FlashAttention2模块，主要原因是想要在GPT预训练中允许使用非变长的FlashAttention。,https://github.com/NVIDIA/Megatron-LM/issues/432
Megatron-LM,该问题类型属于功能需求，用户关注的主要对象是是否在Megatron-LM中添加了torch DDP与管道并行的功能。 由于当前本地DDP的通信方式不具有与反向传播重叠的效率，所以用户提出了这个需求。,https://github.com/NVIDIA/Megatron-LM/issues/428
Megatron-LM,这是一个用户提出需求的类型，关于增强对LLaMA和LLaMA-2的支持。,https://github.com/NVIDIA/Megatron-LM/issues/423
Megatron-LM,这是一个用户提出需求的issue，主要涉及的对象是缓存输入激活，该需求可能是为了提高模型训练效率。,https://github.com/NVIDIA/Megatron-LM/issues/417
Megatron-LM,该issue属于用户提出需求类型，主要涉及Megatron-LM中使用code parrot数据集预训练GPT模型的演示代码。由于需要在两个A10 GPU上利用序列并行性，提供了基于torchrun的运行脚本，并介绍了如何在NVIDIA Docker容器中进行训练。,https://github.com/NVIDIA/Megatron-LM/issues/416
Megatron-LM,"这是一个关于功能需求的问题，涉及主要对象是Megatron-LM中的flash attention模块。由于Megatron在使用""reset-position-ids""时未传递attention mask给flash attention，导致不同输入具有不同的attention masks，用户提出了关于此情况的疑问。",https://github.com/NVIDIA/Megatron-LM/issues/415
Megatron-LM,这个issue是一个功能增强请求，主要涉及Megatron-LM中的位置嵌入功能。主要原因是为了改进位置信息管理和动态性能。,https://github.com/NVIDIA/Megatron-LM/issues/401
Megatron-LM,"这是一个用户提出需求的issue，主要涉及创建一个名为""stale.yml""的文件。",https://github.com/NVIDIA/Megatron-LM/issues/398
Megatron-LM,这是一个关于更新问题模板的issue，类型为需求反馈，主要涉及到项目的问题模板。,https://github.com/NVIDIA/Megatron-LM/issues/397
Megatron-LM,该issue属于用户提出需求类型，主要涉及的对象是Megatron-LM中的`DistributedOptimizer`。这个问题的提出是因为用户希望在训练过程中能够将collective communication与backward computation重叠，以提高效率。,https://github.com/NVIDIA/Megatron-LM/issues/391
Megatron-LM,这是一个代码优化类型的issue，主要涉及到了Megatron-LM代码中的冗余的`save_for_backward`函数调用。由于多余的`save_for_backward`导致代码冗余，需要进行精简优化。,https://github.com/NVIDIA/Megatron-LM/issues/389
Megatron-LM,这个issue类型是用户提出需求，主要涉及的对象是优化器和学习率调度器的应用，由于MegatronLM目前仅支持一个优化器用于整个模型，用户希望能够针对GPT模型的不同部分应用不同的优化器和学习率调度器。,https://github.com/NVIDIA/Megatron-LM/issues/385
Megatron-LM,该issue属于功能需求类型，主要涉及到设置初始热身学习率参数的问题，用户希望可以从非零值开始进行线性热身。,https://github.com/NVIDIA/Megatron-LM/issues/384
Megatron-LM,这是一个用户提出需求的类型，该问题涉及分布式训练功能。由于外部集群存在2个节点，用户希望在内部集群上增加2个节点的分布式训练支持。,https://github.com/NVIDIA/Megatron-LM/issues/382
Megatron-LM,该issue属于用户提出需求类型，涉及主要对象为如何在使用pipeline并行时不均匀地拆分层数；由于MegatronLM要求num_layers必须被pipeline_model_parallel_size整除，因此用户想知道如何在不均匀拆分的情况下设定num_layers。,https://github.com/NVIDIA/Megatron-LM/issues/381
Megatron-LM,这是一个用户提出需求或寻求帮助的问题，主要涉及Megatron-LM模型中的enums模块，由于无法安装megatron.enums，用户询问如何获取该模块及安装的相关信息。,https://github.com/NVIDIA/Megatron-LM/issues/376
Megatron-LM,这是一个优化性质的issue，涉及主要对象是代码中的_get_epoch函数，由于原先的代码在tokens_per_epoch较小时存在效率问题。,https://github.com/NVIDIA/Megatron-LM/issues/373
Megatron-LM,这是一个关于需求的问题，主要涉及Megatron-LM对于自动选择不同类型网络卡的支持问题。由于无法建立高速IB网络连接，用户希望实现在异构集群间进行分布式训练，因此提出了关于网络卡自动检测和配置的疑问。,https://github.com/NVIDIA/Megatron-LM/issues/369
Megatron-LM,这是一个用户请求寻求帮助的issue，主要涉及到在Megatron-LM项目中使用PP/TP训练VIT模型的问题，用户希望得到关于这方面代码的建议。,https://github.com/NVIDIA/Megatron-LM/issues/365
Megatron-LM,这个issue类型为改进建议，主要涉及的对象为`_build_doc_idx`函数实现。这个问题被提出是因为原有的递归实现复杂，希望简化代码以提高可读性。,https://github.com/NVIDIA/Megatron-LM/issues/364
Megatron-LM,该issue类型为用户提出需求，主要涉及Megatron-LM训练中如何在恢复训练时扩展数据并行的规模。由于在64个节点上使用Zero1时，优化器状态在dp=2之间分裂，需在128个节点上重新分配dp2优化器状态到dp4。,https://github.com/NVIDIA/Megatron-LM/issues/361
Megatron-LM,这个issue属于特性请求类型，主要涉及代码库的分支合并操作，由于需要将指定commit之前的代码合并至主分支。,https://github.com/NVIDIA/Megatron-LM/issues/359
Megatron-LM,该issue类型为代码优化建议，涉及的主要对象是ensemble_classifier.py文件中的get_threshold和calc_threshold函数，由于代码中存在冗余和性能问题，导致了优化建议的提出。,https://github.com/NVIDIA/Megatron-LM/issues/354
Megatron-LM,这是一个建议性质的issue，主要对象是代码中的argument parsing部分，原因是提出使用argparse模块代替自定义函数并提供更标准灵活的命令行参数处理。,https://github.com/NVIDIA/Megatron-LM/issues/353
Megatron-LM,这是一个用户提出需求的issue，主要涉及distrib_optimizer.py 文件，用户试图添加缺失的_copy_model_params_to_main_params函数。原因可能是为了改进分布式优化器的功能。,https://github.com/NVIDIA/Megatron-LM/issues/352
Megatron-LM,这是一个用户提出需求的问题，主要涉及GPT模型的参数冻结操作。用户想要解决如何正确冻结模型参数的问题。,https://github.com/NVIDIA/Megatron-LM/issues/345
Megatron-LM,"该issue是关于提出需求的问题，主要涉及到""NVIDIA/Megatron-LM""项目，由于需要在混合IB/Socket设置中操控NCCL_NET而导致此需求。",https://github.com/NVIDIA/Megatron-LM/issues/341
Megatron-LM,这是一个用户提出需求的issue，该问题涉及的主要对象是Megatron-LM。,https://github.com/NVIDIA/Megatron-LM/issues/337
Megatron-LM,这是一个文档更新的issue，主要涉及README.md文件，用户希望在文件中添加与参数有效学习有关的论文。,https://github.com/NVIDIA/Megatron-LM/issues/335
Megatron-LM,这个issue属于README.md文件的更新，不是bug报告。该问题主要涉及的对象是与Megatron-LM相关的文档。,https://github.com/NVIDIA/Megatron-LM/issues/334
Megatron-LM,这是一个优化通信的问题，涉及Megatron-LM下的NeMoMegatron支持的恢复，主要针对pipeline并行性能进行优化，解决GPU空闲导致性能下降的问题。,https://github.com/NVIDIA/Megatron-LM/issues/331
Megatron-LM,这是一个需求优化类型的issue，主要涉及1f1b interleaved schedule的调度逻辑，该问题是关于代码难以理解。,https://github.com/NVIDIA/Megatron-LM/issues/325
Megatron-LM,这是一个用户提出需求的类型，主要涉及的对象是Megatron-LM中复现RETRO的权重。由于用户想要获取已复现的RETRO权重，因此提出此问题。,https://github.com/NVIDIA/Megatron-LM/issues/313
Megatron-LM,这是一个用户提出需求的issue，主要涉及到Megatron-LM代码中的tokens_per_epoch参数。由于缺少打印tokens_per_epoch的功能，用户提出了希望在输出中打印tokens_per_epoch参数的需求。,https://github.com/NVIDIA/Megatron-LM/issues/309
Megatron-LM,这是一个用户提出需求的issue，涉及到Megatron-LM中的VocabParallelEmbedding，用户询问为什么要使用分布式词汇嵌入。,https://github.com/NVIDIA/Megatron-LM/issues/300
Megatron-LM,这是一个用户提出需求的问题，主要涉及如何在SQuAD上对BERT进行微调。用户寻求关于如何在SQuAD上对BERT进行微调的帮助。,https://github.com/NVIDIA/Megatron-LM/issues/292
Megatron-LM,这是一个用户提交需求的issue，主要涉及到在Megatron中添加对MegaBlocks MoEs的支持。由于用户希望在Megatron中使用MegaBlocks的dMoE和MoE层，因此提出了这项修改请求。,https://github.com/NVIDIA/Megatron-LM/issues/288
Megatron-LM,这个issue类型是功能增强请求，主要涉及添加MegaBlocks MoEs支持至Megatron-LM中，由于需要扩展支持到其他预训练脚本，且计划在未来的PR中扩展MegaBlocks支持的tensor模型并行化。,https://github.com/NVIDIA/Megatron-LM/issues/287
Megatron-LM,"这是一个关于Megatron-LM中""Megablocks migration""的类型为需求提出的issue，主要涉及的对象是模块迁移。原因可能是用户寻求关于迁移Megablocks的帮助。",https://github.com/NVIDIA/Megatron-LM/issues/286
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM下的GPT模型如何自定义参数数量。用户想知道是否有设置可以自定义GPT模型的参数数量，例如6.7B参数。,https://github.com/NVIDIA/Megatron-LM/issues/282
Megatron-LM,这个issue是用户提出需求，询问是否有其他的layer norm函数可用，如RMSNorm或DeepNorm。,https://github.com/NVIDIA/Megatron-LM/issues/281
Megatron-LM,该issue属于用户提出需求类型，主要涉及Megatron-LM项目是否支持OpenAI的Whisper模型。用户询问关于项目支持更多模型以及如何将Whisper模型集成到该项目的建议或示例。,https://github.com/NVIDIA/Megatron-LM/issues/280
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron-LM下的pipeline stages自定义功能。由于当前支持的功能是一个要么虚拟pipeline，要么自定义pipeline的情况，用户希望支持同时使用虚拟pipeline和自定义pipeline。,https://github.com/NVIDIA/Megatron-LM/issues/274
Megatron-LM,这是一个需求提出，主要对象是为Megatron-LM添加支持HF tokenizer的功能。,https://github.com/NVIDIA/Megatron-LM/issues/272
Megatron-LM,这是一个用户提出需求的issue，主要对象是Megatron-LM模型。由于长时间没有发布新版本，用户在询问是否会发布包含更多高性能操作和适配PyTorch 2.0等功能的新版本。,https://github.com/NVIDIA/Megatron-LM/issues/269
Megatron-LM,这是一个功能增强的issue，涉及Megatron-LM下的UL2数据采样和预训练功能的添加。,https://github.com/NVIDIA/Megatron-LM/issues/268
Megatron-LM,这是一个关于代码格式的需求提出问题，主要涉及MegatronLM代码库的代码格式。由于缺乏统一的代码格式规范，用户试图提出PR来改善代码格式，以便与上游同步。,https://github.com/NVIDIA/Megatron-LM/issues/265
Megatron-LM,这是一个用户提出需求的类型，主要涉及到Megatron-LM下的GPT模型。这个问题是由于需要在使用张量并行性进行微调GPT模型时，仅有合并的检查点而没有支持管道并行性，因此用户编写了一个脚本来解决这个问题。,https://github.com/NVIDIA/Megatron-LM/issues/263
Megatron-LM,这是一个用户提出需求的问题，关于如何在自定义改动PyTorch的情况下达到与NGC PyTorch相同性能的问题。,https://github.com/NVIDIA/Megatron-LM/issues/254
Megatron-LM,这是一个功能需求的issue，主要涉及到添加`setup.py`文件，目的是为了对项目进行设置和安装。,https://github.com/NVIDIA/Megatron-LM/issues/252
Megatron-LM,这是一个关于功能需求的问题，涉及Megatron-LM对于变长序列长度的unpad策略支持。由于MLPerf Bert训练中使用unpad策略可以提高性能，用户想知道Megatron是否支持或将支持这一策略。,https://github.com/NVIDIA/Megatron-LM/issues/247
Megatron-LM,该issue类型为功能需求，涉及到在多节点训练作业中不使用分布式文件系统时的支持。,https://github.com/NVIDIA/Megatron-LM/issues/246
Megatron-LM,这个issue类型是需求修改，主要涉及的对象是修改Megatron-LM中的最大序列长度参数。原因是用户想要使用比2048更长的序列长度，所以需要将最大序列长度从2048修改为8192。,https://github.com/NVIDIA/Megatron-LM/issues/243
Megatron-LM,这是一个用户提出需求的issue，主要关注的对象是Megatron库。由于缺乏类型支持，用户感到使用时容易出错，且很难找到特定函数的输出。,https://github.com/NVIDIA/Megatron-LM/issues/238
Megatron-LM,这是一个文档更新类的问题，主要涉及到代码配置项的修改。原因可能是为了统一命名规范或者代码更易理解。,https://github.com/NVIDIA/Megatron-LM/issues/237
Megatron-LM,这个issue是用户提出需求类型的，主要涉及对象是Megatron-LM的checkpointing脚本和模型代码，用户提出了需要添加用于合并和拆分checkpoint的脚本以及修复运行脚本时的一些问题。,https://github.com/NVIDIA/Megatron-LM/issues/233
Megatron-LM,这是一个用户提出需求的issue，主要涉及到Megatron-LM中的model parallel partitions。导致问题出现的原因是当前脚本只提供了一部分功能，用户希望能够实现更多功能。,https://github.com/NVIDIA/Megatron-LM/issues/232
Megatron-LM,这是一个关于需求的问题，主要涉及Megatron-LM下的MSDP模型缺少提到的检查点。这可能是由于部分内容未能包含在代码中导致的。,https://github.com/NVIDIA/Megatron-LM/issues/230
Megatron-LM,这是一个用户提出需求的类型，主要对象是获取用于合并管道并行模型分区的脚本。用户希望提供这些脚本以便更好地进行相关操作。,https://github.com/NVIDIA/Megatron-LM/issues/227
Megatron-LM,这是一个需求提出的issue，主要涉及对象是Megatron-LM项目。由于用户想要使用类似于Oxford Union辩论的方式进行辩论，所以提出了这个问题询问是否有相关功能实现。,https://github.com/NVIDIA/Megatron-LM/issues/223
Megatron-LM,这个issue类型是用户提出需求，请教问题，主要涉及的对象是Megatron LM模型。由于用户希望能够保存计算结果并在中间点后继续计算，可能会导致操作性不足或功能不完整。,https://github.com/NVIDIA/Megatron-LM/issues/222
Megatron-LM,这是一个文档改进类型的issue，其中提到的主要对象是Megatron-LM文档。原因可能是文档可读性较差，需要对其进行改进。,https://github.com/NVIDIA/Megatron-LM/issues/218
Megatron-LM,这是一个用户提出需求的issue，主要对象是在多GPU上实现推理。用户想要使用预训练模型在多GPU上进行推理，希望能够实现模型并行化。,https://github.com/NVIDIA/Megatron-LM/issues/215
Megatron-LM,这是一个用户提出需求的issue，主要关注模型参数的唯一数量，问题可能源于用户想要获得特定模型的详细信息。,https://github.com/NVIDIA/Megatron-LM/issues/213
Megatron-LM,该issue类型是代码合并（merge）请求，涉及的主要对象是F1STAT分支。,https://github.com/NVIDIA/Megatron-LM/issues/204
Megatron-LM,这是用户提出的需求，询问关于Megatron-LM中 visual transformer 的训练示例或说明的问题。,https://github.com/NVIDIA/Megatron-LM/issues/202
Megatron-LM,这是一个需求讨论类型的issue，主要涉及Megatron-LM中的fused softmax操作范围问题，由于操作范围过于紧凑导致实验结果不理想，用户提议是否可以改变操作范围。,https://github.com/NVIDIA/Megatron-LM/issues/191
Megatron-LM,这是一个用户提出需求的issue，主要涉及到如何获取论文中提到的指标，并询问软件是否提供相关接口。该问题的原因是用户想了解如何获得Megatron-LM软件的性能指标，如吞吐量和浮点运算次数。,https://github.com/NVIDIA/Megatron-LM/issues/190
Megatron-LM,这个issue类型是功能需求，主要涉及支持MixtureofStudents（MoS），旨在减少MixtureofExperts模型的大小。,https://github.com/NVIDIA/Megatron-LM/issues/189
Megatron-LM,这个issue是关于需求的，主要对象是README中的report tables。由于缺乏明确的GPU和节点尺寸信息以及模型并行大小列的解释，导致用户难以理解报告表格内容。,https://github.com/NVIDIA/Megatron-LM/issues/183
Megatron-LM,这是一个关于需求的问题，涉及到Megatron-LM中继续使用BERT-base权重进行进一步预训练的情况。用户希望能够利用公开的bertbaseuncased模型作为同样结构的Megatron模型的检查点，但由于资源限制无法使用默认大小的Megatron进行预训练。,https://github.com/NVIDIA/Megatron-LM/issues/182
Megatron-LM,这是一个用户提出需求的issue，主要涉及对象是如何将MegatronT5模型转换为HuggingFace T5模型。用户提出这个问题可能是由于希望能够使用官方脚本来实现这一转换过程。,https://github.com/NVIDIA/Megatron-LM/issues/178
Megatron-LM,这是一个用户提出需求的issue，主要涉及如何使用Megatron对T5进行微调。用户想要在下游任务中微调T5模型，希望示例代码能够帮助解决这个问题。,https://github.com/NVIDIA/Megatron-LM/issues/177
Megatron-LM,这是一个用户提出需求的类型。该问题涉及到Megatron-LM框架不再支持lazy dataloader。用户提出需求是希望能够支持lazy dataloader，因为预训练语料库可能庞大且无法被存储为单个文件。,https://github.com/NVIDIA/Megatron-LM/issues/170
Megatron-LM,这是一个用户提出需求的 issue，主要对象是为当前的 Megatron-LM 添加 BigBird 支持。,https://github.com/NVIDIA/Megatron-LM/issues/165
Megatron-LM,这是一个用户提出需求的问题，主要涉及如何在线上进行推理的问题，由于无法在服务器中使用特定的PyTorch镜像而导致提问。,https://github.com/NVIDIA/Megatron-LM/issues/158
Megatron-LM,这是一个关于性能问题的用户需求问题，主要涉及数据集构建过程，由于数据集过大导致进程中止。,https://github.com/NVIDIA/Megatron-LM/issues/154
Megatron-LM,该issue类型是用户提出需求，该问题单涉及的主要对象是softmax kernel的实现。由于使用scale upper triangular mask softmax时未应用pad mask，用户询问是否有计划将pad mask添加到该kernel中。,https://github.com/NVIDIA/Megatron-LM/issues/152
Megatron-LM,这是一个建议性issue，讨论是否使用fused layernorm较好，主要涉及对象是Megatron-LM中的layernorm实现。由于对比torch layernorm，提出不再需要使用fused layernorm，希望得到其他人的看法。,https://github.com/NVIDIA/Megatron-LM/issues/150
Megatron-LM,这个issue类型是用户提出需求，主要涉及的对象是加载T5预训练模型时遇到的困难，由于加载预训练模型时需要处理模型结构和手动分割检查点的繁琐过程，用户提出了如何更好地处理这些问题的需求。,https://github.com/NVIDIA/Megatron-LM/issues/139
Megatron-LM,这是一个关于功能需求的问题，涉及Megatron-LM和BigScience Megatron-LM fork的新功能导入。由于代码库开始分化，可能导致难以轻松导入新功能，需要协作者的额外资源进行处理。,https://github.com/NVIDIA/Megatron-LM/issues/137
Megatron-LM,这是一个需求问题，主要涉及Megatron-LM项目中T5实现缺少`relative_attention_bias`功能的情况。,https://github.com/NVIDIA/Megatron-LM/issues/128
Megatron-LM,这个issue类型为用户提出需求，主要涉及到ORQA任务中关于SQuAD格式数据集的细节。用户想了解是否可以针对SQuAD格式数据集进行监督微调和评估，并询问如何实现。,https://github.com/NVIDIA/Megatron-LM/issues/124
Megatron-LM,这是一个用户提出需求的issue，主要对象是Megatron-LM的example scripts。由于当前需要手动编辑环境变量才能运行脚本，用户希望能够设置环境变量并传递给脚本，以便更轻松地在不同节点上运行脚本。,https://github.com/NVIDIA/Megatron-LM/issues/121
Megatron-LM,这是一个关于需求的问题，主要涉及到Megatron-LM/BERT的数据处理和训练过程。由于预处理生成了大量的.idx和.bin文件，导致在BERT预训练中无法直接使用多个文件的问题。,https://github.com/NVIDIA/Megatron-LM/issues/117
Megatron-LM,该issue类型是用户提出需求，主要涉及的对象是对于Megatron-LM支持替代fine-tuning的算法。由于用户认为fine-tuning对于大型模型造成负担，因此提出支持类似Ptuning和LoRA这样的替代算法的建议。,https://github.com/NVIDIA/Megatron-LM/issues/114
Megatron-LM,这个issue是一个修改建议，主要涉及Megatron-LM中的t5预处理，并涉及环境变量和命令行参数的修复。,https://github.com/NVIDIA/Megatron-LM/issues/105
Megatron-LM,该issue是一个用户提出需求的类型，主要涉及Megatron-LM下的GPT2(1.5B)在使用openWebText数据集训练时表现不如预期，用户询问为何相同参数大小的MegatronLM训练在一个更大的数据集下（包括Wikipedia、CCStories、RealNews、和OpenWebText）的准确率低于OpenAI GPT2在仅使用OpenWebText数据集训练时的准确率。,https://github.com/NVIDIA/Megatron-LM/issues/102
Megatron-LM,这个issue属于用户提出需求的类型，主要涉及PyTorch中的in-place操作，用户提出在Megatron-LM中添加mark_dirty的建议，维护原始张量数据的一致性。,https://github.com/NVIDIA/Megatron-LM/issues/97
Megatron-LM,"该issue类型为用户提出需求，请教问题，涉及主要对象是Megatron-LM的数据预处理步骤，由于用户不清楚在使用""/create_doc_index.py""之前是否需要先使用preprocess.py脚本，所以寻求了相关信息。",https://github.com/NVIDIA/Megatron-LM/issues/94
Megatron-LM,这是一个需求提出的issue， 主要涉及到支持LAMB optimizer的问题。由于当前MegatronLM仅支持Adam optimizer，用户希望添加对LAMB optimizer的支持，这将使得可以实现更大的batch size和更短的训练时间，从而可以更好地处理大型数据集。,https://github.com/NVIDIA/Megatron-LM/issues/87
Megatron-LM,这是一个用户提出需求的issue，主要涉及GLUE任务的缺失问题。由于GLUE任务中只包含了MNLI和QQP，用户想知道是否将来会添加其他任务的评估。,https://github.com/NVIDIA/Megatron-LM/issues/74
Megatron-LM,这个issue是关于提出需求的类型问题，主要涉及Megatron-LM中的position embedding使用可训练embedding而非绝对正弦编码的原因。,https://github.com/NVIDIA/Megatron-LM/issues/72
Megatron-LM,这是一个关于功能需求的问题，主要涉及Megatron-LM是否支持在“分片”数据集上训练，用户想要按顺序在多个小数据集上训练模型，但不知道如何实现此操作，可能是因为预处理大数据集耗时导致。,https://github.com/NVIDIA/Megatron-LM/issues/69
Megatron-LM,这是一个用户提出需求的issue，涉及到Megatron-LM项目的依赖问题。由于requirements.txt中没有包含apex相关信息，导致用户在运行Megatron时需要先安装具有cpp扩展的Apex，因此希望更新readme以包含apex安装/要求信息。,https://github.com/NVIDIA/Megatron-LM/issues/64
Megatron-LM,这个issue属于功能需求类型，主要涉及到Megatron-LM模型训练过程中的超参数保存问题，用户提出了需要保存模型结构相关的超参数，以确保后续的finetuning或文本生成过程中能够保持参数一致，避免出现任务失败的情况。,https://github.com/NVIDIA/Megatron-LM/issues/56
Megatron-LM,这是一个用户提出需求的issue，主要涉及到Megatron-LM模型在使用现有模型微调或生成文本时需要保存词汇和合并文件，原因是为了方便继续使用相同的词汇和合并文件。,https://github.com/NVIDIA/Megatron-LM/issues/55
Megatron-LM,这是一个关于需求的问题，涉及Megatron-LM中的优化器使用半精度训练的主题。用户提出了想要实现完全半精度优化器的需求。,https://github.com/NVIDIA/Megatron-LM/issues/45
Megatron-LM,这是一个用户提出需求的issue，主要涉及的对象是MegatronLM和DeepSpeed项目。该问题由于DeepSpeedExamples项目中使用的MegatronLM版本较旧，缺乏最新更新，用户询问是否考虑支持集成最新版本的MegatronLM和DeepSpeed项目。,https://github.com/NVIDIA/Megatron-LM/issues/44
Megatron-LM,这是一个关于系统扩展性的需求提出。这个问题单涉及的主要对象是Megatron LM，用户想知道是否容易将其扩展到175B规模。,https://github.com/NVIDIA/Megatron-LM/issues/40
Megatron-LM,这是用户提出需求的类型issue，主要涉及到获取8.3B参数模型的问题。原因可能是用户想使用更大参数的模型，但当前链接指向的是较小参数的模型。,https://github.com/NVIDIA/Megatron-LM/issues/39
Megatron-LM,这是一个用户提出需求的问题，主要对象是 Megatron 的 BERT 模型，在询问如何评估在所有 GLUE 任务上的性能。,https://github.com/NVIDIA/Megatron-LM/issues/31
Megatron-LM,这是一个用户提出需求的issue，主要涉及Megatron modelparallel模型在V100 GPU上运行时如何从每次迭代的耗时计算TFLOPS性能，用户希望了解如何根据模型配置计算TFLOPS/PFLOPS，可能由于缺乏直接获取TFLOPS性能数据的方法而导致这个问题的提出。,https://github.com/NVIDIA/Megatron-LM/issues/27
Megatron-LM,该issue为用户提出需求，主要涉及GPT2模型在不同任务上进行评估参数设置的问题，用户想要模仿GPT2论文中报告的结果。,https://github.com/NVIDIA/Megatron-LM/issues/24
Megatron-LM,该issue类型为用户提出需求，主要涉及的对象是`torch.multiprocessing.spawn`下的数据加载部分，用户提出了关于并行训练中数据加载的问题。,https://github.com/NVIDIA/Megatron-LM/issues/21
Megatron-LM,这是一个请求更名的issue，主要对象是项目中的命令，原因是为了与readme文件中的命令名称保持一致。,https://github.com/NVIDIA/Megatron-LM/issues/16
Megatron-LM,这个issue是关于用户提出需求的类型，主要对象是是否发布预训练模型。这是因为用户想知道是否有预训练模型可供使用。,https://github.com/NVIDIA/Megatron-LM/issues/11
Megatron-LM,这个issue类型是用户提出需求，主要对象是希望获取Megatron LM工具的输出示例。由于缺少输出样本，用户想要了解工具的实际效果而不是必须运行才能确认。,https://github.com/NVIDIA/Megatron-LM/issues/10
Megatron-LM,这个issue是关于支持最新PyTorch RNG state API的需求，主要对象是Megatron-LM项目。原因是需要修复对PyTorch 1.2的支持问题。,https://github.com/NVIDIA/Megatron-LM/issues/8
Megatron-LM,这个issue属于用户提出需求类型，主要涉及Megatron-LM下的GPT2模型和tokenizer与pytorch-transformers包的兼容性问题，用户想了解如何将训练好的模型和tokenizer导入pytorch-transformers包以进行进一步的微调。,https://github.com/NVIDIA/Megatron-LM/issues/3
Megatron-LM,这是一个用户提出需求的issue，主要对象是Megatron-LM的dataloader merge功能。由于dataloader merge操作速度较慢，用户希望提升其性能。,https://github.com/NVIDIA/Megatron-LM/issues/1
vllm,这个issue类型是用户提出需求，主要对象是如何在同一个docker实例中运行多个模型，从而提供多个功能而无需多个docker实例或监听不同端口的api服务器。,https://github.com/vllm-project/vllm/issues/16065
vllm,这个issue类型是用户提出需求，寻求关于获取最新开发版docker镜像的帮助。,https://github.com/vllm-project/vllm/issues/16064
vllm,这个issue类型属于性能优化建议，主要涉及CUDA kernel的优化，提出了关于性能提升和性能回归的讨论。,https://github.com/vllm-project/vllm/issues/16060
vllm,这是一个用户需求的issue，主要对象是v1 engine在CPU上的支持问题。用户想要了解为什么v1 engine暂时不支持CPU，并寻求如何在特定模型上运行推理的帮助。,https://github.com/vllm-project/vllm/issues/16056
vllm,该问题类型为性能优化建议，针对vllm的离线LLM推理过程中随着时间推移速度逐渐变慢的问题。,https://github.com/vllm-project/vllm/issues/16050
vllm,这个issue类型为更新需求，涉及主要对象是hpuextension，由于最新更新移除了Index Reduce fp32 casts，需要更新requirementshpu.txt文件。,https://github.com/vllm-project/vllm/issues/16047
vllm,这个issue类型是需求改进，涉及chat_with_tools示例的改进。由于第一次使用需权限设置，处理以特定前缀开头的JSON输出，增强JSON解析处理和添加调试信息等问题。,https://github.com/vllm-project/vllm/issues/16044
vllm,这是一个功能需求类型的issue，主要涉及NeuronX的分布式推理支持、推测解码和动态设备上的抽样。导致此需求产生的原因可能是为了增强Neuron的功能性和性能。,https://github.com/vllm-project/vllm/issues/16043
vllm,这是一个需求类型的issue，涉及的主要对象是在TPUs上建立性能基准测试。由于当前还没有TPU版本的持续性能基准测试，用户提出了添加Llama 3.1 8B和Llama 3.1 70B在不同版本上的性能测试的需求。,https://github.com/vllm-project/vllm/issues/16042
vllm,这是一个需求报告，主要涉及模型中使用压缩张量的问题。原因可能是为了改进模型性能或减少资源消耗。,https://github.com/vllm-project/vllm/issues/16038
vllm,这是一个需求讨论的Issue，主要涉及Data Parallel Attention和Expert Parallel MoEs的设计和实现。问题的根源在于为了解决MLA模型中的内存占用问题，需要重新设计Attention机制并支持专家并行化。,https://github.com/vllm-project/vllm/issues/16037
vllm,这个issue类型是功能需求，主要涉及的对象是EAGLE模型在V1版本中的加载问题。由于目前存在一些限制，例如只支持单GPU、需要在eager模式下运行等，导致用户需要进行特定的操作才能正确运行这个PR。,https://github.com/vllm-project/vllm/issues/16035
vllm,这个issue类型是功能需求，主要目标是将gfx950添加到支持的架构中。,https://github.com/vllm-project/vllm/issues/16034
vllm,这是一个用户提出需求的issue，主要涉及支持Cutlass MLA for Blackwell GPUs，由于最新的Cutlass支持Blackwell GPUs的MLA，用户希望在下一个版本（v3.9）中提供集成此核心的功能。,https://github.com/vllm-project/vllm/issues/16032
vllm,这是一个需求提出的issue，主要涉及 lmformatenforcer 工具在与 pydantic 结合使用时出现的问题。,https://github.com/vllm-project/vllm/issues/16029
vllm,这个issue是一个功能改进类型的问题，主要涉及自动检测处理bitsandbytes预量化模型，解决了需要显式传递quantization engine参数的问题。,https://github.com/vllm-project/vllm/issues/16027
vllm,"该issue属于用户提出需求类型，主要涉及vllm的服务benchmark功能，用户希望在在线benchmark中能够指定采样参数(topk, topp等)，并通过在客户端请求的""extra_body""字段中添加采样参数来实现，同时在benchmark_serving.py中新增了命令行标志（`topp`、`topk`和`temperature`）。",https://github.com/vllm-project/vllm/issues/16026
vllm,"这个issue类型为需求提出，主要涉及的对象是 ""Advance tpu.txt"" 和 ""torch_xla""。这个需求是为了将 ""tpu.txt"" 和 ""torch_xla"" 进行更新到最新的 nightly torch 版本。",https://github.com/vllm-project/vllm/issues/16024
vllm,这个issue类型为功能需求提案，主要涉及添加在在线基准测试中指定采样参数的功能。用户提出需要在基准测试中指定采样参数（如topk、topp等），并展示了在TPU上设置topk和topp采样参数后的性能优化结果。,https://github.com/vllm-project/vllm/issues/16022
vllm,这是一个功能需求提出的issue，涉及的主要对象是模型aware kv操作的帮助函数。,https://github.com/vllm-project/vllm/issues/16020
vllm,这个issue类型为用户提出需求，需要为VLLM添加支持SmolVLM。该问题单涉及的主要对象是模型扩展。由于HuggingFaceTB/SmolVLM22.2BInstruct的推出，用户希望VLLM能够支持该模型。,https://github.com/vllm-project/vllm/issues/16017
vllm,这是一个功能请求（RFC - Request for Comments），主要涉及vLLM的缓存技术中的安全性问题。原因是缓存在面临时序侧信道攻击时存在漏洞，需要一种能够跨组织分段缓存复用的技术改进。,https://github.com/vllm-project/vllm/issues/16016
vllm,这个issue是关于需求提议的，主要涉及`huggingface-cli[hf_xet]`的移动问题。由于疑问`huggingface-cli[hf_xet]`是否需要在`requirements/common.txt`中，提议将其移动到`requirements/rocm.txt`或`requirements/rocmtest.txt`，并寻求反馈。,https://github.com/vllm-project/vllm/issues/16012
vllm,这是一个功能改进类型的issue，主要涉及的对象是vllm中的SupportsMultiModal接口。导致这个问题的原因是命名标准不一致，需要添加一个getter来抽象命名。,https://github.com/vllm-project/vllm/issues/16007
vllm,这个issue属于性能优化类型，涉及的主要对象是CPU backend。这个问题由于默认的block_size导致性能较差，因此需要将其修改为128来提升性能。,https://github.com/vllm-project/vllm/issues/16002
vllm,该issue属于功能改进类，主要涉及到项目中的脚本组织和管理。根据描述，问题是由于原有文件目录中脚本过多，需要将硬件CI相关的脚本移动到`scripts/hardware_ci`目录下，以及将其他脚本移到`scripts/`目录下，以更好地组织文件目录结构。,https://github.com/vllm-project/vllm/issues/16001
vllm,该issue类型是用户提出需求，主要涉及的对象是针对vLLM中离线推断的性能问题。由于在同步模式下运行离线推断时用户体验性能问题，需手动管理分批处理，故提出引入异步模式来自动处理内部分批，提高效率。,https://github.com/vllm-project/vllm/issues/16000
vllm,这个issue类型是功能更新，并涉及更新benchmark README和添加AIMODataset功能，可能由于最新变更而导致需要更新benchmark文档。,https://github.com/vllm-project/vllm/issues/15998
vllm,这是一个需求提交类型的issue，主要对象是改进错误信息提示。由于用户在推断设备类型时遇到困难，希望在错误消息中添加指导说明以帮助用户。,https://github.com/vllm-project/vllm/issues/15994
vllm,这个issue是关于文档更新的请求，主要涉及到基准测试（Benchmark）的内容。由于最新更改（添加 AIMO 数据集到基准测试中）导致文档需要更新。,https://github.com/vllm-project/vllm/issues/15993
vllm,"这是一个用户提出需求的类型的issue, 主要涉及的对象是Core下的EncoderDecoderModelRunner，由于该对象缺少LoRA功能，用户提出需要为其添加LoRA。",https://github.com/vllm-project/vllm/issues/15990
vllm,这是一个功能需求的issue，涉及V1版本的DP scale-out实现，主要涉及到引擎进程管理和通信的解耦。原因是为了支持本地和/或远程引擎的混合运行，以及在不同节点上运行的远程引擎。,https://github.com/vllm-project/vllm/issues/15977
vllm,该issue类型是功能更新，涉及对象为V0和V1的集成与性能优化。,https://github.com/vllm-project/vllm/issues/15975
vllm,这是一个用户提出需求的issue，主要涉及对象是LlamaForCausalLM模型，用户希望能够对LlamaMLP设置reduce_results，并且设置层类型。,https://github.com/vllm-project/vllm/issues/15962
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm的Mixtral模型。由于现有的vllm不支持对Mixtral8x7b模型进行fp8量化，用户希望添加这一支持。,https://github.com/vllm-project/vllm/issues/15961
vllm,这个issue是一个功能需求。它主要涉及到v1版本中的KV Connector API以支持分解预填充（disaggregated prefill）功能。,https://github.com/vllm-project/vllm/issues/15960
vllm,这是一个功能需求类型的issue，主要涉及DeepGemm MoE expert map的支持问题。由于当前支持方式无法与cudagraphs兼容，导致无法正常工作。,https://github.com/vllm-project/vllm/issues/15957
vllm,这是一个用户提出需求的issue，主要涉及模块化融合专家并集成PPLX内核，旨在使MoE内核更模块化，以便能够轻松地支持多种通信机制。,https://github.com/vllm-project/vllm/issues/15956
vllm,这个issue类型是用户提出需求，涉及的主要对象是在benchmark中添加AIMO Dataset。由于需要增加AIMO数据集以及传递随机种子以实现可重现性，因此可能导致用户需要在benchmark测试中进行验证。,https://github.com/vllm-project/vllm/issues/15955
vllm,这是一个用户提出需求的issue，主要涉及Chameleon、Chatglm和Commandr模型的支持量化功能，原因是为了统一支持所有模型的量化，并确保正确更新嵌套模型的`packed_modules_mapping`和`ignored modules`属性。,https://github.com/vllm-project/vllm/issues/15952
vllm,这是一个需求提出的issue，主要涉及V1版本中对Mistral3的支持。,https://github.com/vllm-project/vllm/issues/15950
vllm,该issue类型属于功能需求提出，涉及的主要对象是`guidance` guided decoding backend。由于`guidance`在处理JSON object schemas时遵循JSON Schema规范，而`outlines`和`xgrammar`默认不允许对象具有额外的属性，因此用户提出了实现一个选项来更好地对齐这些行为的需求。,https://github.com/vllm-project/vllm/issues/15949
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加最大输出长度限制功能。这个需求是由于当前系统在请求时只检查输入token长度和max_tokens总和是否超过模型最大长度，导致可能出现资源紧缺和系统延迟增加的问题。,https://github.com/vllm-project/vllm/issues/15948
vllm,这是一个功能需求的提交，主要对象是代码库中的模型（BaiChuanBaseForCausalLM，GPTNeoXForCausalLM，MPTForCausalLM），目的是修复使用`AutoWeightsLoader`加载复合模型时的问题。,https://github.com/vllm-project/vllm/issues/15939
vllm,这是一个需求问题单，主要对象是Kernel中的CUTLASS MoE kernels，用户提出了需要实现BF16和FP16权重支持的需求。,https://github.com/vllm-project/vllm/issues/15932
vllm,该issue属于用户提出需求类型，涉及主要对象是在使用vLLM进行视频问答任务中如何在在线模式下加载并使用预先计算的视觉标记。用户提出了如何修改代码以便在在线服务中直接加载embedding，以及是否有更好的方法来实现这一目标。,https://github.com/vllm-project/vllm/issues/15925
vllm,这个issue属于用户提出需求和寻求帮助类型，主要涉及如何确定参数以加快处理高请求吞吐量的速度。这是因为用户想要在部署vLLM服务器时确定`max_num_seqs`和`max_model_len`参数。,https://github.com/vllm-project/vllm/issues/15924
vllm,这是一个用户提出需求的issue，涉及项目文档的更新请求。用户希望能够直接点击目标文档链接，而无需先打开`CONTRIBUTING.md`，再点击目标链接。,https://github.com/vllm-project/vllm/issues/15922
vllm,这是一个用户提出需求的issue，主要涉及CLI命令行工具中缺少描述属性，导致用户在使用时无法清晰了解每个选项的作用。,https://github.com/vllm-project/vllm/issues/15921
vllm,这是一个功能需求的issue，主要涉及到支持deepseek-v3-0324工具调用，由于尚未添加该功能，用户提出了相关需求。,https://github.com/vllm-project/vllm/issues/15916
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是文档内容。,https://github.com/vllm-project/vllm/issues/15913
vllm,这个issue类型是优化建议，主要对象是Neuron模块中的kv缓存。由于将KV缓存融合成单个张量可以消除KV缓存张量上的不必要切片操作，因此提出了这个优化建议。,https://github.com/vllm-project/vllm/issues/15911
vllm,这个issue类型属于需求提出，主要涉及的对象是Translation API。,https://github.com/vllm-project/vllm/issues/15910
vllm,这是一个功能需求提议，主要涉及数据处理规模和多节点支持，通过改变端口绑定和连接方式以解决输入信号传递和连接过程中可能发生的竞态条件问题。,https://github.com/vllm-project/vllm/issues/15906
vllm,这个issue是用户提出需求，希望添加Ollama meetup slides的文档到项目中。,https://github.com/vllm-project/vllm/issues/15905
vllm,这是一个增强功能的Issue，主要涉及到V1版本中的SpecDecoding Metrics 日志的修改。原因可能是为了使日志样式更符合其他日志的风格。,https://github.com/vllm-project/vllm/issues/15902
vllm,这个issue类型是需求报告，涉及VLLM项目中对EAGLE模型的支持。原因可能是EAGLE模型在V1版本中需要进行一些初始化、缓存管理、边界情况处理等方面的改进。,https://github.com/vllm-project/vllm/issues/15901
vllm,这是一个文档更新类型的issue，涉及主要对象为项目的使用统计语言。这个issue由于可能存在意外数据泄露和关于模型架构信息的潜在识别性而需要澄清语言。,https://github.com/vllm-project/vllm/issues/15898
vllm,这是一个用户请求添加配置和故障排除指南的类型为文档问题的issue，主要涉及到NCCL & GPUDirect RDMA的配置和故障排除。由于用户可能遇到特定错误，需要更好的指导以解决问题。,https://github.com/vllm-project/vllm/issues/15897
vllm,该issue类型为功能需求，主要对象是使能CUDA图形而无需启用torch.compile，用户希望简化模型的融合过程以提高性能。,https://github.com/vllm-project/vllm/issues/15896
vllm,这是一个功能特性新增的issue，涉及到支持 AITER MLA 的集成。,https://github.com/vllm-project/vllm/issues/15893
vllm,这是一个优化问题，主要涉及的对象是TPU-optimized top-k implementation。导致这个问题的原因是使用torch.topk导致了性能问题，而torch.scatter是原始topk实现中的瓶颈。,https://github.com/vllm-project/vllm/issues/15891
vllm,这是一个用户提出需求的issue，主要涉及到项目中的`format.sh`和`pre-commit`安装流程的简化问题。,https://github.com/vllm-project/vllm/issues/15890
vllm,这是一个需求提出的issue，涉及的主要对象是gemm3模块名称前缀。该需求要求在gemm3中添加模块名称前缀，以匹配权重中的张量名称。,https://github.com/vllm-project/vllm/issues/15889
vllm,这是一个请求移除不再支持的文件的问题，主要涉及到项目中的混乱和冗余文件。,https://github.com/vllm-project/vllm/issues/15885
vllm,这是一个需求类型的issue，主要涉及的对象是项目中的`format.sh`文件。由于`format.sh`文件在超过70天以上没有得到支持，导致了用户提出需要移除该文件的需求。,https://github.com/vllm-project/vllm/issues/15884
vllm,这是关于性能提升的建议，用户希望了解0.8.1版本相比0.7.4dev122为何提升了14%性能，并提供了性能测试的结果。,https://github.com/vllm-project/vllm/issues/15881
vllm,这是一个用户提出需求的issue，主要涉及对象是“Fused MoE config for Nvidia RTX 3090”。用户询问如何构建该配置或从何处开始，可能由于脚本中dtype无法设置为w4a16而导致了这个问题。,https://github.com/vllm-project/vllm/issues/15880
vllm,该issue属于功能需求提出类型，主要涉及GGUF dequantization的dtype支持的增加。,https://github.com/vllm-project/vllm/issues/15879
vllm,这是一个用户提出需求的issue，主要涉及的对象是`huggingface_hub`的版本。这个问题是因为当前版本无法通过Xet下载模型造成下载速度慢，用户希望添加对`huggingface_hub`的最低版本要求以启用Xet下载。,https://github.com/vllm-project/vllm/issues/15873
vllm,该issue属于性能优化建议，主要涉及到大图像下Qwen2.5VL预处理速度过慢导致GPU利用率低的问题。,https://github.com/vllm-project/vllm/issues/15869
vllm,这个issue类型为优化性修改，主要涉及的对象是LoRA测试。由于需要减少测试时间并确保覆盖所有LoRA功能，在此作出了相应修改。,https://github.com/vllm-project/vllm/issues/15867
vllm,这个issue属于功能需求提议，涉及的主要对象是DPAsyncMPClient类，用户提出了支持多节点数据并行的需求。,https://github.com/vllm-project/vllm/issues/15863
vllm,这是一个用户提出需求的类型，主要涉及到VLLM在支持结构化剪枝后，每层注意力头数目变化的问题。用户想要在VLLM上部署剪枝后的模型，可能出现此问题的原因是用户希望了解VLLM是否支持这种情况下的部署操作。,https://github.com/vllm-project/vllm/issues/15854
vllm,这是一个用户提出需求的issue，主要涉及的对象是两个新的embedding模型。由于缺乏对这两个模型的支持，用户向vllm团队询问是否考虑支持它们。,https://github.com/vllm-project/vllm/issues/15849
vllm,这个issue类型是用户提出需求。该问题单涉及的主要对象是V1 LoRA支持CPU offload。由于什么样的原因导致了用户需要这个功能的需求？,https://github.com/vllm-project/vllm/issues/15843
vllm,这是一个性能优化类的issue，主要关注的对象是dynamo guard，通过优化后提升了系统的吞吐量。,https://github.com/vllm-project/vllm/issues/15840
vllm,该issue类型为用户提出需求，主要涉及到vllm中的hacked classifier free guidance方法。这个问题可能是由于对vllm中CFG的实现不尽如人意而提出，希望能够有更好的实现方法。,https://github.com/vllm-project/vllm/issues/15839
vllm,这个issue类型是功能改进请求，主要涉及 SchedulerConfig 中的新增参数 `disable_chunked_mm_input`，用户提出了需要在多模态项目内部禁用分块传输的需求。,https://github.com/vllm-project/vllm/issues/15837
vllm,这是一个功能需求的issue，涉及到修改环境变量以控制Compiled Graph使用的通道类型。由于Ray 2.42的变化，需要更改默认使用的通道类型，以便支持调试和未来可能的拓展。,https://github.com/vllm-project/vllm/issues/15831
vllm,这个issue类型是性能优化提升，主要对象是在ROCm上进行未量化线性操作时的性能问题，由于批处理大小小于等于2时性能不佳，用户希望通过添加skinny gemms来改善性能。,https://github.com/vllm-project/vllm/issues/15830
vllm,这是一个需求变更类型的issue，主要涉及重命名fallback模型、整理支持的模型部分以及更新实用函数以反映新的命名约定。原因可能是为了提高代码可读性和易用性。,https://github.com/vllm-project/vllm/issues/15829
vllm,这个issue属于用户提出需求类型，并主要涉及到在VLLM添加Ovis2模型。由于想要在VLLM中添加ovis架构并继续在AIDCAI/Ovis上进行讨论，因此用户提交了这个问题。,https://github.com/vllm-project/vllm/issues/15826
vllm,这是关于功能增强的issue，主要涉及到Intel Gaudi硬件上的自动前缀缓存支持。,https://github.com/vllm-project/vllm/issues/15821
vllm,这是一个用户提出需求的类型，主要对象是开发者指南。因为用户希望在开发指南中推荐使用 Python 3.12，可能是因为他们希望充分利用最新版本的 Python 的特性或者功能。,https://github.com/vllm-project/vllm/issues/15811
vllm,这是一个需求类型的issue，主要涉及实现XpYd基于点对点通信的动态扩展功能，其中涉及动态扩展和收缩实例的PD分离。,https://github.com/vllm-project/vllm/issues/15806
vllm,该issue为用户提出需求，主要涉及的对象是在Jupiter Jülich HPC上使用的GH200机器。由于缺少ARM机器，用户希望有人能够构建一个包含最新vLLM版本的GH200容器，以解决使用Gemma 3时出现的问题。,https://github.com/vllm-project/vllm/issues/15803
vllm,这个issue属于特性需求类型，涉及的主要对象是VLLM代码库。由于需要支持零开销调度，需要修改代码以处理同步操作，引入占位符令牌以返回虚假结果并处理实际令牌，以及修改序列类来标记有效令牌长度。,https://github.com/vllm-project/vllm/issues/15801
vllm,这个issue类型是功能增强，讨论了V1版本的多模态输入默认启用，涉及主要对象是V1版本的多模态输入功能。,https://github.com/vllm-project/vllm/issues/15799
vllm,这是一个功能需求issue，主要涉及VLLM项目中模型指定的问题。该issue主要针对在配置文件中指定模型的功能进行了测试，并指出了不同情况下的预期行为和测试用例。,https://github.com/vllm-project/vllm/issues/15798
vllm,这个issue是关于更新actions/setup-python版本的操作，属于功能增强类型，主要涉及到版本控制和相关工作流程的调整。,https://github.com/vllm-project/vllm/issues/15789
vllm,这是一个关于性能优化的issue，主要对象是llama model，提出通过并行化计算和通信来减少prefill latency的问题。,https://github.com/vllm-project/vllm/issues/15787
vllm,该issue属于功能需求类型，主要涉及Zero Overhead Scheduling的实现，通过实施该功能来提高性能。,https://github.com/vllm-project/vllm/issues/15781
vllm,这个issue是一个优化建议，主要对象是调度器（Scheduler），由于每次请求都调用了`_try_schedule_encoder_inputs`函数，在没有包含编码器输入的请求中也被调用，导致了性能损失。,https://github.com/vllm-project/vllm/issues/15778
vllm,这个issue类型是特性需求提案，主要涉及添加推理模式（测试时间计算）功能。这个需求是由于优化后的推理算法使得vLLM非常适合应用于测试时间算法。,https://github.com/vllm-project/vllm/issues/15774
vllm,这是一个功能优化的issue，主要涉及到vLLM中的cascade attention机制。由于原始cascade机制在处理并行采样时存在问题，导致需要对attention逻辑进行修改和优化。,https://github.com/vllm-project/vllm/issues/15772
vllm,这个issue类型是需求，主要对象是评论的英文翻译。由于评论未使用英文，开发者体验不佳，需要对评论进行英文翻译的修复。,https://github.com/vllm-project/vllm/issues/15768
vllm,这是一个用户提出需求的 issue，主要涉及更新 Conda 的用法链接，由于缺少此信息，用户难以获得文档。,https://github.com/vllm-project/vllm/issues/15761
vllm,这是一个关于功能需求的issue，主要涉及vllm中AWQ量化不支持专家并行性的问题，用户想在当前架构下实现专家并行性，但遇到了无法实现的错误。,https://github.com/vllm-project/vllm/issues/15760
vllm,该issue类型是用户提出需求，主要对象是新增kv connector的自定义实现，问题涉及的原因是为了更方便地使用第三方的kv connector，同时让vllm不必为每个自定义的kv connector实现负责。,https://github.com/vllm-project/vllm/issues/15756
vllm,这是一个用户提出需求的类型的issue，主要涉及N-gram Proposer Interface的更新。,https://github.com/vllm-project/vllm/issues/15750
vllm,该issue类型为用户提出需求，主要涉及的对象是项目的构建方式。由于需要摆脱 requirements/build.txt 并利用 pyproject.toml，用户提出了使用 pip wheel 构建 wheels 的建议。,https://github.com/vllm-project/vllm/issues/15749
vllm,这是一个需求更新的issue，主要涉及更新Vision Arena Dataset和HuggingFaceDataset设置，由于需要更新数据集和设置，所以提出这个问题。,https://github.com/vllm-project/vllm/issues/15748
vllm,这是一个用户提出需求的issue，主要涉及到vllm命令行工具，用户建议添加一个新的CLI PR类型，因为vllm命令有不同的参数和许多选项需要维护。,https://github.com/vllm-project/vllm/issues/15747
vllm,这是一个用户提出需求的类型，涉及的主要对象是Docker base image。由于需要使用Ubuntu 24.04作为基础镜像，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/15745
vllm,这是一个需求类型的issue，主要对象是尝试在项目中使用Python 3.13版本。,https://github.com/vllm-project/vllm/issues/15743
vllm,这是一个优化建议，主要涉及到简化拒绝采样器的输出解析过程，避免不必要的集合操作。,https://github.com/vllm-project/vllm/issues/15741
vllm,该issue属于文档更新类型，主要涉及文档构建及查看。,https://github.com/vllm-project/vllm/issues/15740
vllm,这是一个需求文档类型的issue，主要涉及到v0 engine在推理输出中支持结构化输出问题，用户反馈由于只有v0 engine支持结构化输出而造成部分用户困惑。,https://github.com/vllm-project/vllm/issues/15739
vllm,这个issue是关于性能优化的，主要针对TPU上的topk和topp算法进行优化，避免使用`torch.scatter`导致性能低下。,https://github.com/vllm-project/vllm/issues/15736
vllm,这是一个特性需求的issue，主要涉及vLLM项目的路线图规划。该需求主要是为了指出在Q2 2025期间vLLM项目计划的新功能和改进方向。,https://github.com/vllm-project/vllm/issues/15735
vllm,这个issue是一个功能需求，主要涉及的对象是在VLLM下添加对FP8模型的量化支持。由于需要在FA输出后保留FP8量化以及执行FA循环时对softmax(QK^T)量化为FP8，而现有的支持仅包括Llama，因此提出了这一功能需求。,https://github.com/vllm-project/vllm/issues/15734
vllm,这个issue属于功能需求提出类型，涉及解决LoRA加载时可能产生的并发性问题，主要解决对象是`resolve_lora`函数中的`lora_name`。,https://github.com/vllm-project/vllm/issues/15733
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM，用户希望在vLLM层面增加对滑动窗口和logit softcapping的支持，以便支持gemma模型。,https://github.com/vllm-project/vllm/issues/15732
vllm,这个issue是关于实现Eagle Proposer的需求，涉及到PR中使用虚拟模型和忽略抽样参数。原因是为了降低实现的复杂度并提高最终性能。,https://github.com/vllm-project/vllm/issues/15729
vllm,这是一个需求提出类型的issue，主要涉及的对象是VLLM中的Gem 3后端，由于需要增加Ultravox支持以及修改tokenizer来使用新的`` token。,https://github.com/vllm-project/vllm/issues/15728
vllm,这是一个功能需求提出的issue，主要涉及到vllm的默认输出后端设置。原因是之前默认设置为`xgrammar`，现改为`auto`，以满足更多请求，并允许用户手动选择后端。,https://github.com/vllm-project/vllm/issues/15724
vllm,这是一个代码优化类型的issue，涉及对代码的简化和效率调整。,https://github.com/vllm-project/vllm/issues/15723
vllm,这是一个优化性质的issue，主要对象是V1 structured output tests，由于每个测试案例都创建了一个新的LLM实例，导致测试速度缓慢。,https://github.com/vllm-project/vllm/issues/15718
vllm,这是一个文档优化的问题，涉及修正绘图和更新注释，在V1版本中。,https://github.com/vllm-project/vllm/issues/15716
vllm,这个issue是一个特性更新，主要涉及到V1版本中的Scatter and gather placeholders功能，并且由于需要避免干扰TPU图编译。,https://github.com/vllm-project/vllm/issues/15712
vllm,这是一个改进建议类型的issue，主要涉及到持续集成测试流程中的pytest命令配置。由于需要在CI日志中查看最慢的测试，因此建议将--durations=100参数添加到pytest命令中。,https://github.com/vllm-project/vllm/issues/15711
vllm,这个issue属于用户提出需求类型，主要涉及vLLM项目中的attention模块，由于当前不支持cascade attention for sliding window attention，用户希望扩展这一功能。,https://github.com/vllm-project/vllm/issues/15710
vllm,该issue类型为代码优化，主要涉及的对象是未使用的utils和import。,https://github.com/vllm-project/vllm/issues/15708
vllm,这是一个用户提出需求的类型的issue，主要对象是代码中的`mm_counts`。由于V1版本只会使用单个项目进行分析，用户希望重写`mm_counts`以使用提供的值而不是`limit_mm_per_prompt`，以为后续可配置的多模态数据做准备。,https://github.com/vllm-project/vllm/issues/15703
vllm,这是一个用户提出需求的类型，主要涉及文档（Documentation）。由于用户请求在故障排除中添加“生成质量变化”部分，涉及到关于生成质量变化的问题或者帮助。,https://github.com/vllm-project/vllm/issues/15701
vllm,这是一个用户提出需求的类型，主要涉及到vLLM中复合模型加载使用`AutoWeightsLoader`的问题，提出在所有语言backbones中应用该方法的建议。原因是目前仅有少数语言backbones实现了`load_weights`函数，希望该方法能被标准化并应用于所有语言backbones。,https://github.com/vllm-project/vllm/issues/15697
vllm,这是一个用户提出需求的issue，涉及前端开发，主要是添加聊天/完成模式的退出功能。,https://github.com/vllm-project/vllm/issues/15693
vllm,这是一个用户提出需求的issue，主要涉及的对象是使用vllm的用户，由于Python 3.10+版本需要支持numpy>=2，需要更新numba到0.61来满足这一需求。,https://github.com/vllm-project/vllm/issues/15692
vllm,这是一个功能改进的issue，主要涉及的对象是 CPU Dockerfile。根据描述，这个改进主要是为了减少构建时间和镜像大小。,https://github.com/vllm-project/vllm/issues/15690
vllm,这个issue是一个特性需求。它涉及到vllm中的legacy input mapper/processor的移除。由于融合input processor和input mapper的提案，需要对Phi4multimodal进行重构来支持V1版本，为了避免破坏旧的模型运行器，InputRegistry将在这个PR中保留以便它们有时间进行更新。,https://github.com/vllm-project/vllm/issues/15686
vllm,这是一个用户提出需求的issue，主要涉及的对象是开发环境容器（DevContainer）。这个需求源于提高开发者体验，希望添加一个开发环境容器以提供更好的开发体验。,https://github.com/vllm-project/vllm/issues/15684
vllm,这个issue类型为改进需求，主要涉及移动docker文件至特定目录，由于主目录混乱，导致需要进行清理。,https://github.com/vllm-project/vllm/issues/15683
vllm,该issue属于一个功能需求类型，主要涉及在Docker中运行两个模型的问题。由于CUDA资源不足，导致无法同时运行两个模型，用户正在寻求解决方案。,https://github.com/vllm-project/vllm/issues/15682
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM项目。由于论文中提到的Slim attention功能可以减少近一半的内存使用，用户认为这将更适合个人电脑使用。,https://github.com/vllm-project/vllm/issues/15679
vllm,这是一个需求提出类型的issue，主要涉及到移除遗留的输入注册表，由于输入映射器已经从V1中移除，所以需要进一步删除输入注册表。,https://github.com/vllm-project/vllm/issues/15673
vllm,这是一个功能更新类型的issue，涉及到需要更新到flashinfer 0.2.3版本的集成。,https://github.com/vllm-project/vllm/issues/15666
vllm,这个issue属于功能需求，涉及对象为环境变量的设置功能。由于需要提供一个环境变量来禁用采样器，可能是为了解决Trtion延迟导入时的问题。,https://github.com/vllm-project/vllm/issues/15662
vllm,这是一个关于新功能开发的issue，主要对象是在macos上分发软件包。由于vLLM现在可以在macos上直接用`pip install .`来构建，用户希望能够构建wheel并发布到pypi，以便用户能够直接安装。,https://github.com/vllm-project/vllm/issues/15661
vllm,这是一个关于用户需求的问题，主要涉及如何在 vLLM 中设置加载指定模型的内容。用户希望只在用户选择特定模型时加载模型，而不是在 Docker 容器启动时就加载所有模型。,https://github.com/vllm-project/vllm/issues/15660
vllm,这个issue类型是代码更新，主要涉及对象是vLLM的TPU内核，由于最近内核的更新，需要更新代码。,https://github.com/vllm-project/vllm/issues/15659
vllm,这个issue类型是功能需求，主要涉及tool parser manager和reasoning parser manager的逻辑重构，原因可能是需要优化相关实现。,https://github.com/vllm-project/vllm/issues/15658
vllm,这个issue类型为功能需求，涉及主要对象是实现http服务性能指标监控。由于当前无法跟踪http层面的指标，用户希望通过prometheus fastapi-instrumentor来实现这一功能。,https://github.com/vllm-project/vllm/issues/15657
vllm,这是一个优化代码性能的issue，主要涉及硬件TPU的V1版本，用户提出了几处优化的地方，并反馈了一些待解决的问题。,https://github.com/vllm-project/vllm/issues/15655
vllm,"该issue类型为用户提出需求，该问题单涉及的主要对象是对""Draft changes""的修改。由于没有提供具体内容，导致用户在编辑草稿时遇到困惑或者希望获得反馈和建议。",https://github.com/vllm-project/vllm/issues/15654
vllm,这个issue是针对优化的需求，主要涉及了哈希KV块处理。,https://github.com/vllm-project/vllm/issues/15652
vllm,这个issue类型是功能需求，涉及到了测试基础功能的转换和并发预加载设计。,https://github.com/vllm-project/vllm/issues/15651
vllm,这是一个性能优化的issue，涉及更新Cascade Attention启发式算法以适配FA3，由于原先的启发式算法针对FA2优化，所以导致现在在FA3上使用不准确。,https://github.com/vllm-project/vllm/issues/15647
vllm,这是一个关于功能改进的issue，涉及到FastAPI进行Bearer认证的替换，由于当前的API key身份验证实现无法被OpenAPI规范发现和包含，需要进一步测试。,https://github.com/vllm-project/vllm/issues/15641
vllm,"该issue为一个用户提出需求的问题单，主要涉及的对象是名为""Kv""的项目。",https://github.com/vllm-project/vllm/issues/15638
vllm,这是一个功能需求类型的issue，主要涉及支持从s3桶直接加载LoRA适配器。原因是当前加载LoRA适配器时从s3获取数据会导致验证错误。,https://github.com/vllm-project/vllm/issues/15633
vllm,这个issue属于功能更新类型，主要涉及AMD支持的架构列表更新。这个问题的来源可能是为了保持最新的GPU架构支持情况，以便在构建时正确识别和配置支持的硬件。,https://github.com/vllm-project/vllm/issues/15632
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是项目的文档维护和贡献者。由于当前贡献者找不到入门任务，需要在 Contributing 部分直接链接到入门任务。,https://github.com/vllm-project/vllm/issues/15629
vllm,这个issue类型是用户提出需求，主要涉及的对象是网页上的“Ask AI”按钮的布局位置。原因可能是与文档版本选择器的布局冲突，导致按钮位置需要调整。,https://github.com/vllm-project/vllm/issues/15628
vllm,这个issue是关于代码优化的建议，主要涉及对象是全局`mm_registry`直接访问，旨在更好地跟踪`compute_encoder_budget`的依赖关系。,https://github.com/vllm-project/vllm/issues/15621
vllm,这是一个文档更新类的问题，主要涉及到安装 transformers 时所需的系统要求，由于缺少相关信息导致用户安装失败。,https://github.com/vllm-project/vllm/issues/15616
vllm,该issue属于用户提出需求类型，主要涉及的对象是要求支持Babel系列模型，由于目前不支持该系列模型，用户提出了相关支持请求。,https://github.com/vllm-project/vllm/issues/15612
vllm,这是一个功能需求的issue，主要涉及 BitsAndBytes 的 V1 支持。由于缺少 V1 的支持，用户无法使用该功能。,https://github.com/vllm-project/vllm/issues/15611
vllm,该issue类型为功能需求，主要涉及到在多模态推断中对文本内容进行缓存以优化推断效率的问题。,https://github.com/vllm-project/vllm/issues/15608
vllm,这是一个用户提出需求的issue，主要涉及支持交错的模态项目，问题出现的原因是当前解决方法在处理大量交错项目时可能效率低下，需要更多工作来支持混合模态项目/交错嵌入。,https://github.com/vllm-project/vllm/issues/15605
vllm,这个issue类型为性能改进提议，涉及的主要对象是vLLM安装和使用过程中遇到的多个大语言模型的服务问题。由于不同模型需要不同版本的transformers库，安装特定版本可能导致与其他模型的兼容性问题，可能产生性能回退或错误。,https://github.com/vllm-project/vllm/issues/15602
vllm,这是一个关于优化和合并缓存功能的issue。,https://github.com/vllm-project/vllm/issues/15595
vllm,这个issue类型是用户提出需求，主要涉及的对象是API服务器的响应日志记录功能，用户希望增加一个中间件来记录API服务器的响应信息。,https://github.com/vllm-project/vllm/issues/15593
vllm,这是一个功能需求的issue，涉及的主要对象是vllm中的Quantization模块。这个issue提出了关于支持通道级动态token组GroupedGEMM的功能需求。,https://github.com/vllm-project/vllm/issues/15587
vllm,这个issue是一个需求提交，主要请求重新启用通过测试的AMD测试。,https://github.com/vllm-project/vllm/issues/15586
vllm,这是一个文档更新的issue，涉及的主要对象是V1用户指南。原因是根据CC启用了V1 Fp8缓存，需要更新相应文档。,https://github.com/vllm-project/vllm/issues/15585
vllm,该issue类型为需求提出，涉及的主要对象是guidance backend，由于出现了无限多的换行导致测试偶尔失败。,https://github.com/vllm-project/vllm/issues/15584
vllm,这是一个需求提出的issue，主要是关于CLI中自动显示默认值的问题。,https://github.com/vllm-project/vllm/issues/15582
vllm,这是一个关于优化代码结构和功能改进的issue，主要涉及的对象是vllm项目中的server_load追踪逻辑。由于load_aware_call装饰器存在冗余，并且需要与http_middleware装饰器进行融合，因此需要对代码进行重构来实现prometheus中对server_load的跟踪。,https://github.com/vllm-project/vllm/issues/15580
vllm,这个issue是关于功能增强的，主要涉及Bert、Blip、Blip2和Bloom模型，目的是统一支持所有模型的量化操作。,https://github.com/vllm-project/vllm/issues/15573
vllm,这是一个用户提出需求的issue，主要对象是VLLM的日志输出功能。由于当前设置为DEBUG模式时无法看到JSON响应负载，导致开发者难以调试VLLM请求和响应，需要改进日志输出以提供更详细的信息。,https://github.com/vllm-project/vllm/issues/15571
vllm,这是一个功能需求的issue，主要涉及在vLLM中实现Ring Attention以处理RL应用中的长序列上下文。,https://github.com/vllm-project/vllm/issues/15566
vllm,该issue类型为用户提出需求，主要对象是支持Qwen/Qwen2.5Omni7B模型。由于该模型尚未得到vllm的支持，用户请求添加对该模型的支持。,https://github.com/vllm-project/vllm/issues/15563
vllm,这个issue类型是功能需求，主要涉及LMCache支持到vLLM的CPU版本。原因是要实现预填充和KV缓存共享。,https://github.com/vllm-project/vllm/issues/15562
vllm,这是一个用户提出需求的issue，主要涉及到对于mergify.yml文件添加自动tpu标签的需求。,https://github.com/vllm-project/vllm/issues/15560
vllm,这是一个功能改进类型的issue，主要涉及`scatter_patch_features`函数的优化，以解决参数维度不一致导致需要使用hacky方法的问题。,https://github.com/vllm-project/vllm/issues/15559
vllm,该issue类型为功能需求，主要涉及的对象是ROCm环境下的自定义PA功能，发起该需求的原因是在内核上游过程中丢失了用于触发自定义PA功能的环境变量，希望补充一个环境变量来触发自定义PA，以便强制禁用自定义PA内核并回退到默认实现。,https://github.com/vllm-project/vllm/issues/15557
vllm,这是一个用户提出的需求，主要涉及增加端点负载指标，并在响应头中报告引擎指标的格式化选择。,https://github.com/vllm-project/vllm/issues/15555
vllm,这是一个功能改进建议的issue，主要涉及Transformers backend中对TP验证的改进。,https://github.com/vllm-project/vllm/issues/15540
vllm,"这是一个用户提出需求的issue，主要涉及对象是V1版本中如何获取""num_gpu_blocks""，由于LLM和EngineCore处于不同进程中，导致用户无法通过之前的方式获取该参数。",https://github.com/vllm-project/vllm/issues/15538
vllm,该issue类型为用户提出需求，用户需要关于如何使用vllm来运行特定模型的帮助。,https://github.com/vllm-project/vllm/issues/15529
vllm,这是一个建议改进类型的issue，主要涉及到改进示例脚本的输出。可能是为了让查看过程更加简单明了。,https://github.com/vllm-project/vllm/issues/15528
vllm,这是一个用户提出需求的类型，主要涉及的对象是关于改进推理模型处理效率的功能。由于推理模型处理时间过长，用户希望能够提供一种新特性，以改进模型中的推理过程，使之更加高效。,https://github.com/vllm-project/vllm/issues/15524
vllm,该issue属于需求提出类型，主要涉及追踪http服务错误计数，并注册到prometheus指标中，主要原因为需要监控应用状态中的错误计数。,https://github.com/vllm-project/vllm/issues/15523
vllm,这个issue是一个功能更新请求，涉及的主要对象是Dockerfile。原因是用户希望更新Dockerfile以添加tzdata，以便在所有环境中设置时区。,https://github.com/vllm-project/vllm/issues/15522
vllm,这个issue类型是需求提出，涉及的主要对象是Dockerfile。由于缺少tzdate包，用户希望更新Dockerfile以添加该包以允许设置环境变量TZ。,https://github.com/vllm-project/vllm/issues/15521
vllm,这是一个需求更新的issue，主要涉及的对象是Dockerfile，用户提出了添加tzdate包的需求。,https://github.com/vllm-project/vllm/issues/15520
vllm,这是一个改进性质的issue，主要涉及expert parallelism placement的优化问题，原因是当前的逻辑导致GPU利用率不佳。,https://github.com/vllm-project/vllm/issues/15517
vllm,这是一个功能改进类型的issue，主要涉及到权重（weight）名称的情况，导致某些量化工具无法兼容。,https://github.com/vllm-project/vllm/issues/15515
vllm,这个issue类型为用户提出需求，主要对象是使用vllm进行分布式运行，并询问如何获取worker的运行时错误日志。由于用户想要在特定模型上运行推理，但不清楚如何与vllm集成，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/15514
vllm,这是一个需求相关的issue，主要涉及改进自动化中更好的JSON输出。原因可能是当前JSON输出不够满足用户需求或者提高使用体验。,https://github.com/vllm-project/vllm/issues/15512
vllm,这个issue类型为特性请求，涉及的主要对象是fused_moe kernel。由于与另一个pull request的兼容性变更，用户希望使用缓存提示改进性能，主要是在延迟指标方面有轻微改善。,https://github.com/vllm-project/vllm/issues/15511
vllm,这是一个与性能优化相关的问题，涉及到使用Cache Hinting来改进fused_moe kernel，这个问题并非bug报告。,https://github.com/vllm-project/vllm/issues/15510
vllm,这个issue类型是用户提出需求，主要对象是添加一段推断性能基准脚本。,https://github.com/vllm-project/vllm/issues/15504
vllm,这是一个需求改进类型的issue，涉及的主要对象是LoRA长上下文测试。由于缺乏活跃用户使用该功能，因此决定删除长上下文测试。,https://github.com/vllm-project/vllm/issues/15503
vllm,该issue类型是功能增强请求，其主要涉及对象为DeepSeek V2/3模型的MoE模块。由于启用了共享专家融合功能，导致性能提升并具有降低ITL和TTLT损失的效果。,https://github.com/vllm-project/vllm/issues/15502
vllm,这是一个功能添加的issue，主要涉及DeepSeek V2/3模型的特征融合。由于特征融合功能的启用，导致性能有所提升。,https://github.com/vllm-project/vllm/issues/15501
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm项目中的wake_up()函数。这个需求是为了给wake_up()函数添加一个tags参数，以支持更好的权重更新。,https://github.com/vllm-project/vllm/issues/15500
vllm,这是一个用户提出需求的类型，主要对象涉及到引擎调度程序字段。原因是为了让IDE知道方法是什么。,https://github.com/vllm-project/vllm/issues/15499
vllm,这个issue类型是需求提出，主要涉及的对象是`wake_up()`函数。由于需要改进在睡眠模式唤醒时的权重更新支持，用户提出了添加tags参数的需求。,https://github.com/vllm-project/vllm/issues/15497
vllm,这个issue类型为功能改进，并涉及到vLLM V0版本和paged attention kernels的移除。,https://github.com/vllm-project/vllm/issues/15495
vllm,这是一个功能提议，主要涉及到添加新的API支持jumpforward解码。,https://github.com/vllm-project/vllm/issues/15490
vllm,这个issue属于性能优化类问题，主要对象是simple_connector.py文件中的GPU内存使用情况。导致这个问题的原因是使用`torch.cat`在子张量上会占用双倍GPU内存。,https://github.com/vllm-project/vllm/issues/15485
vllm,这是一个用户提出需求的issue，主要涉及的对象是multi-node offline DP+EP的功能。由于用户想要方便地运行multi-node DP+EP，所以提出了这个需求。,https://github.com/vllm-project/vllm/issues/15484
vllm,这是一个用户提出需求的issue，主要涉及的对象是支持不同设备的CG以提高性能，由于需要支持Ascend NPU，需要与其他两个PR一起使用。,https://github.com/vllm-project/vllm/issues/15482
vllm,这个issue是一个功能修复的问题，涉及的主要对象是LRUCache implementations。原因是为了统一不同LRUCache实现的功能，提供了新的LRUCache实现，并进行了相关的补充和重写。,https://github.com/vllm-project/vllm/issues/15481
vllm,这是一个功能需求的issue，主要涉及到添加量化到VLLM的自定义allreduce中，用户提出了关于不同量化制度选项的需求。,https://github.com/vllm-project/vllm/issues/15479
vllm,这是一个功能改进（Feature Improvement）类型的issue，主要涉及Sampler组件。由于增加了针对128k词汇、1024批量大小的情况下的top-k优化实现，使得处理速度显著提升。,https://github.com/vllm-project/vllm/issues/15478
vllm,这是一个需求提出类型的issue，主要对象是模型Phi-4-multimodal，用户需要重构代码以支持V1版本，并处理图像和音频输入。,https://github.com/vllm-project/vllm/issues/15477
vllm,这个issue类型为需求提出，主要对象可能为Richard。原因可能是用户需要与Richard进行某种互动或交流。,https://github.com/vllm-project/vllm/issues/15474
vllm,这是一个功能需求的issue，主要涉及的对象是logits processor。由于当前解决方案不尽如人意，用户希望通过添加可选参数的方式来增强logits processor，以便更好地处理数据。,https://github.com/vllm-project/vllm/issues/15473
vllm,这是一个需求提出的issue，主要涉及到分离`TransformersModel`成基础类`TransformersModel`和LM类`TransformersForCausalLM`。这个需求的提出可能是为了提高代码的可重用性和可维护性。,https://github.com/vllm-project/vllm/issues/15467
vllm,这是一个功能优化类的issue，主要涉及到spec decode配置参数的移除和优化。,https://github.com/vllm-project/vllm/issues/15466
vllm,这是一个功能需求报告，主要针对vllm下的Embedding API dimensions功能目前不支持的问题。这个问题的根本原因是请求无法完全匹配OpenAI的格式。,https://github.com/vllm-project/vllm/issues/15465
vllm,这是一个用户提出需求的issue，主要对象是Transformers backend。由于Transformers backend已经支持V1，需要将``添加到`TransformersModel`以启用`torch.compile`。,https://github.com/vllm-project/vllm/issues/15463
vllm,该issue类型为文档更新，主要涉及V1版本用户指南的多模态功能介绍缺失问题。可能由于V1版本中缺少对多模态功能当前状态的解释而导致用户提出补充。,https://github.com/vllm-project/vllm/issues/15460
vllm,这个issue类型为用户提出需求，主要对象是vLLM的权重预处理。由于硬件需要对预压缩的权重数据进行解压缩，当前的权重分区和压缩方法导致了性能开销较大，因此提议将权重分区和用户定义的压缩整合到一个单独的预处理工具中。,https://github.com/vllm-project/vllm/issues/15459
vllm,该issue类型是需求更改，涉及的主要对象是vllm的metrics展示。由于vllm在0.8版本中已经废弃了特定的metrics，需要隐藏这些被废弃的metrics。,https://github.com/vllm-project/vllm/issues/15458
vllm,这是一个用户提出需求的issue，主要涉及vLLM中Whisper在处理长音频时是否支持Sequential算法的问题，用户询问了关于长音频处理的支持情况。,https://github.com/vllm-project/vllm/issues/15454
vllm,这是一个用户提出需求/请教问题的issue，主要涉及vllm服务的启动问题。该问题产生的原因可能是用户需要运行特定模型的推断，但不清楚如何与vllm集成。,https://github.com/vllm-project/vllm/issues/15451
vllm,这个issue类型为功能需求，主要涉及对象是XpYd系统。这是用户提出的关于点对点通信功能的需求。,https://github.com/vllm-project/vllm/issues/15448
vllm,这个issue类型是用户提出需求，请教问题，主要涉及的对象是vllm。用户不清楚如何与vllm集成以进行特定模型的推理，可能由于缺乏相关文档或指导而导致。,https://github.com/vllm-project/vllm/issues/15447
vllm,该issue类型为代码优化，主要涉及的对象是`scatter_patch_features`函数。这个改动是为了去除`num_embeds`参数，并通过`embeds_is_patch`的形状来推断该参数，从而简化函数实现。,https://github.com/vllm-project/vllm/issues/15443
vllm,这是一个功能需求的issue，主要涉及的对象是Molmo模型。由于目前Molmo模型不支持多图像，用户提出需要为Molmo模型添加多图像支持。,https://github.com/vllm-project/vllm/issues/15438
vllm,这个issue属于功能需求类型，主要涉及在返回隐藏状态的选项，由于新增了一些文件来支持隐藏状态的返回设置，需要测试入口点才能确定功能是否正常。,https://github.com/vllm-project/vllm/issues/15434
vllm,这是一个功能需求的issue，主要关注于在VLLM中集成int8 scaled gemm内核并优化性能，需要在使用llmcompressor生成的压缩张量格式的模型中使用这一功能。,https://github.com/vllm-project/vllm/issues/15433
vllm,这是一个用户提出需求的issue，主要涉及V1版本不支持Collective RPC功能，用户希望有人实现这一功能。,https://github.com/vllm-project/vllm/issues/15430
vllm,这是一个用户提出需求的issue，主要涉及Baichuan-Audio模型的支持，请求VLLM协助。,https://github.com/vllm-project/vllm/issues/15425
vllm,这是一个功能需求报告，主要涉及了支持新模型 glm-4-voice-9b。用户希望vllm能够支持这个新的端到端大型音频模型。,https://github.com/vllm-project/vllm/issues/15424
vllm,这是一个优化类型的issue，主要涉及到mamba2模块中冗余计算的优化，原因是观察到在不同层之间存在冗余计算。,https://github.com/vllm-project/vllm/issues/15423
vllm,这个issue是关于[Core] LoRA的调度优化，主要是关于bug报告。涉及的主要对象是LoRA Scheduler。由于当前调度逻辑无法继续扫描等待队列，导致某些请求被阻塞无法进行调度。,https://github.com/vllm-project/vllm/issues/15422
vllm,这是一个用户提出需求的类型，主要对象是v1调度器，由于需要支持并发部分预填充，需要定义long_prefill_token_threshold，并控制令牌阈值。,https://github.com/vllm-project/vllm/issues/15419
vllm,这是一个用户提出需求的 issue，主要对象是关于限制思考标记在推理模型中的使用。,https://github.com/vllm-project/vllm/issues/15418
vllm,这是一个功能优化类型的issue，涉及主要对象是torchcompiled softmax。由于torchcompiled softmax并未提供显著的加速效果，所以选择移除它。,https://github.com/vllm-project/vllm/issues/15416
vllm,该issue属于需求类型，主要涉及性能回归测试对象。由于当前性能校准是手动进行的，用户提出了添加性能回归测试来捕捉提交中的异常性能回归。,https://github.com/vllm-project/vllm/issues/15414
vllm,这个issue类型是需求更改，主要涉及工作流程和记录器。导致这个问题的原因可能是当前工作流程的顺序不当，以及记录器输出干扰。,https://github.com/vllm-project/vllm/issues/15412
vllm,这是一个用户提出需求的issue，主要涉及用户定义的聊天模板，由于缺乏警告信息导致了用户无法准确判断聊天模板的匹配情况。,https://github.com/vllm-project/vllm/issues/15408
vllm,这是一个功能需求提案，主要涉及实现嵌入模型在V1版本中。由于需要支持新的嵌入模型，并创建一个新的调度器，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/15406
vllm,这是一个需求提出的issue，主要对象是添加一个多模态开发示例，由于缺乏相关内容而需要补充。,https://github.com/vllm-project/vllm/issues/15405
vllm,这是一个用户提出需求的issue，主要涉及Gemma 3模型的JSON工具调用。由于用户喜欢Qwen 2.5和Mistral模型使用JSON模板的一致性和工作效果，因此请求为Gemma 3模型引入基于JSON的工具调用。,https://github.com/vllm-project/vllm/issues/15403
vllm,该issue类型为功能更新，主要涉及Dockerfile.ppc64le的更新和基于UBI9的依赖项构建。根据描述，问题主要是关于Docker容器中OpenAI端点的验证。,https://github.com/vllm-project/vllm/issues/15402
vllm,该问题属于功能需求，主要涉及的对象是在TPU上实现引导解码的问题。可能由于无法直接在TPU上进行位掩码操作，导致需将logits移动到CPU上再返回到TPU进行抽样。,https://github.com/vllm-project/vllm/issues/15401
vllm,该issue类型为用户提出需求，主要涉及添加一个警告功能，以便在用户提供与特定模型官方模板不符的聊天模板时发出警告。导致该问题的原因是当前vLLM没有在这种情况下提供警告，可能导致性能问题。,https://github.com/vllm-project/vllm/issues/15395
vllm,这是一个用户提出需求的issue，主要涉及使用vllm加载大型语言模型时如何利用不同显卡组合的问题。用户询问如何在具有不同显存的多个显卡上运行特定模型的推断。,https://github.com/vllm-project/vllm/issues/15390
vllm,这个issue是一个代码优化性质的需求，主要涉及到LoRA日志的删除和对PunicaWrapperGPU代码注释的改进。,https://github.com/vllm-project/vllm/issues/15388
vllm,这是一个用户提出需求的issue，主要对象是请求VLLM团队为bgem3嵌入模型添加对稠密和稀疏特征的支持。,https://github.com/vllm-project/vllm/issues/15384
vllm,这个issue是一个功能需求，主要涉及的对象是vllm，用户提出需要支持Top-nσ sampling的功能，并指出了希望vllm也能支持mirostat、tail free sampling等功能，原因是目前的动态截断采样(SOTA performance)相对于min_p表现更好，而当前可用的sglang或transformers等方式较慢。,https://github.com/vllm-project/vllm/issues/15379
vllm,这是一个用户提出需求的issue，主要涉及在VLLM的benchmark中添加CoT数据集。由于最近推理LLMs变得更受欢迎，生成CoT导致解码阶段更长，旨在更好地评估和捕捉推理LLMs性能特点。,https://github.com/vllm-project/vllm/issues/15378
vllm,这个issue属于功能需求类，并涉及LRUCache的统一功能实现。由于硬件限制，仅对Qwen2VL2BInstruct模型做了测试，尚未验证与其他模型的兼容性。,https://github.com/vllm-project/vllm/issues/15375
vllm,该issue类型为需求提出，主要对象是vllm中的whisper模块的LoRA适配器，用户提出需要vllm支持whisper的LoRA适配器。,https://github.com/vllm-project/vllm/issues/15370
vllm,该issue类型属于功能改进，主要涉及vLLM的测试效率和速度问题，由于前端代码耗时较长，需要通过mocking和最小化e2e测试来加快测试速度。,https://github.com/vllm-project/vllm/issues/15369
vllm,该issue属于用户提出需求类型，主要涉及问题是关于vllm异步引擎批量请求的用法。由于当前环境下无法正确处理批量请求，导致用户无法如预期地进行请求队列处理。,https://github.com/vllm-project/vllm/issues/15363
vllm,这是一个用户提出需求的issue，主要对象是希望添加对SFREmbeddingCode2B_R embedding model的支持。可能由于vllm模型尚未支持该特定模型，导致用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/15362
vllm,这是一个功能改进的issue，主要涉及的对象是V1版本的Spec Decode，由于默认的N-gram参数设置不合适，导致出现了过多匹配的问题。,https://github.com/vllm-project/vllm/issues/15358
vllm,这个issue是文档更新类型，涉及到VLLM的OOM处理，用户提出了对设置`maxnumseqs`和其他选项进行更新以避免内存溢出问题。,https://github.com/vllm-project/vllm/issues/15357
vllm,这是一则关于功能需求的issue，主要涉及到vLLM V1中对CPU offloading的实现问题。由于当前实现不兼容`torch.compile`，无法支持某些功能，导致用户提出了基于UVA技术重写CPU offloading的全透明方案。,https://github.com/vllm-project/vllm/issues/15354
vllm,这是一个性能改进请求，涉及到 Kernel 中的 paged attention 模块。原因可能是为了提升该模块的性能。,https://github.com/vllm-project/vllm/issues/15353
vllm,这是一个性能改进的RFC（请求评论）类型的issue，主要涉及优化paged attention kernel的性能，提出对于GQA的改进。由于当前kernel设计导致对数据的多次重复加载，浪费了大量内存，希望通过引入新的kernel来优化性能。,https://github.com/vllm-project/vllm/issues/15351
vllm,这是一个用户提出需求的issue，主要涉及Vllm v1 eagle proposer模块。存在的问题可能是用户想就该模块的功能性或设计方面提出建议或需求。,https://github.com/vllm-project/vllm/issues/15346
vllm,这是一个特性需求的issue，主要涉及到 `PDController` 和 `PDWorker` 的原型实现。这个需求由于想要实现 `PDController` 和 `PDWorker` 之间的消息传递和协同工作而提出。,https://github.com/vllm-project/vllm/issues/15343
vllm,这个issue是一个用户提出需求的类型，主要涉及如何在数据集上快速获取Deepseek V3生成，问题可能由于需要在大量示例上进行离线推理，所以寻求最快的方法。,https://github.com/vllm-project/vllm/issues/15342
vllm,这是一个技术改进建议类型的issue，主要涉及的对象是vllm项目中的OpenVINO支持。该问题单由于要将OpenVINO支持移至外部插件存储库而引发。,https://github.com/vllm-project/vllm/issues/15339
vllm,该issue是一个功能增强请求，主要涉及的对象是spec decode接口，通过修改接口以更好地适配eagle，并为eagle头部分配slots。这个修改还没有添加完整的eagle所需参数，作者期待在CC([V1][Usage] Refactor speculative decoding configuration and tests)合并后进行重排。,https://github.com/vllm-project/vllm/issues/15334
vllm,这是一个用户提出需求的issue，主要涉及到在README中保留以前的新闻以便用户查找以往的meetup幻灯片，因为之前的新闻在PR#15261中被删除以缩短README的长度，导致用户反馈需求仍需要保留以前的信息。,https://github.com/vllm-project/vllm/issues/15331
vllm,这个issue是针对功能改进的请求，主要涉及的对象是FullAttentionSpec以及相关的类，用户提出需要添加针对`use_mla`参数的单元测试以验证内存管理和缓存大小计算，同时指出一些相关类缺乏单元测试，但由于测试对功能的影响有限，暂时跳过。,https://github.com/vllm-project/vllm/issues/15325
vllm,这是一个需求提交的issue，主要对象是优化DeepSeekR1 671B在NVIDIA L20设备上的推断性能。,https://github.com/vllm-project/vllm/issues/15322
vllm,这是一个功能需求提议，主要涉及的对象是V1 LoRA。这个提议的原因可能是为了提升V1版本的性能表现。,https://github.com/vllm-project/vllm/issues/15320
vllm,该issue类型为用户提出需求，主要涉及添加生成算法功能，用户希望实现一篇论文中描述的算法以用于贪婪生成操作中。,https://github.com/vllm-project/vllm/issues/15315
vllm,这是一个用户提出需求的issue，主要涉及将vllm集成到异步引擎中批量请求推理的功能。原因可能是用户想要通过队列处理一批请求，并希望能够维护队列以避免请求接收问题。,https://github.com/vllm-project/vllm/issues/15314
vllm,该issue属于需求提出类型，主要涉及V1版本的num_computed_tokens逻辑重构，并解决了token采样和请求处理过程中的问题。,https://github.com/vllm-project/vllm/issues/15307
vllm,这个issue类型是用户提出需求，主要涉及VLLM库中的生成多个完成问题，用户请求如何为每个输入查询生成多个完成。错误可能是由于参数设置或使用方法不正确导致无法生成多个完成。,https://github.com/vllm-project/vllm/issues/15304
vllm,这是一个技术需求的issue，主要涉及通过添加InstructCoder来进行推断解码基准测试，其中通过添加了ngram proposer和benchmark_serving作为性能评估的指标。,https://github.com/vllm-project/vllm/issues/15303
vllm,这是一个关于改进功能的issue，主要涉及的对象是VLLM项目中的前缀缓存功能。原因是当前的哈希函数可能导致不相关请求的块被访问，因为存在哈希冲突的情况，提出了使用SHA256替换哈希函数的改进建议。,https://github.com/vllm-project/vllm/issues/15297
vllm,这是一个需求提出类型的issue，主要涉及的对象是向vLLM贡献新模型Qwen3和Qwen3MoE，由于作者在GitHub上已经提交了包含这些模型实现的pull request，现在希望将这些新模型添加到vLLM中。,https://github.com/vllm-project/vllm/issues/15289
vllm,这个issue类型是性能优化请求，涉及到torch sdpa F.scaled_dot_product_attention在TPU上表现低效的问题。,https://github.com/vllm-project/vllm/issues/15288
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM。由于需要进行稀疏实验，用户提出了希望实现GPU空闲时自动释放内存并在新实验开始时重新分配内存的功能需求。,https://github.com/vllm-project/vllm/issues/15287
vllm,这是一个用户提出需求的issue，涉及对象是vllm的docker image，由于缺少ray命令，用户寻求关于在CPU模式下进行分布式推理和serving的帮助。,https://github.com/vllm-project/vllm/issues/15284
vllm,该issue类型为功能优化，主要涉及的对象是调度器（Scheduler）。这个改动是为了引入一个公共的状态类 `CommonSchedulerStates`，其中包含了两个状态变量，目的是将这些状态移动到一个单独的文件中，以提高代码整洁度，但不会影响功能或性能。,https://github.com/vllm-project/vllm/issues/15271
vllm,这是一个用户提出需求的类型，主要涉及到知识库检索。由于用户想要在知识库中基于用户输入搜索相关信息，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/15268
vllm,这是一个用户提出需求的issue，主要涉及对象是vllm在Ray cluster上支持CPU推理，由于当前环境下无法正确利用集群资源导致无法完成CPU推理任务。,https://github.com/vllm-project/vllm/issues/15266
vllm,该issue类型为用户提出需求，问题涉及的主要对象为VLLM 0.8，用户提出需要一个示例来确保实验的可复现性。,https://github.com/vllm-project/vllm/issues/15262
vllm,这个issue属于需求提出类型，主要涉及RLHF中使用sleep mode和wake_up()时遇到的内存OOM问题，由于新旧模型权重、kv缓存等都存在于同一个GPU上导致。,https://github.com/vllm-project/vllm/issues/15254
vllm,这是一个功能需求的issue，主要涉及VLLM的解码引擎不支持指定的配置参数，导致无法使用特定配置参数导致的问题。,https://github.com/vllm-project/vllm/issues/15252
vllm,该issue类型为功能拓展，主要涉及对象为V1 scheduler。由于代码重构需要，引入了SchedulerInterface类，没有预期功能或性能改变。,https://github.com/vllm-project/vllm/issues/15250
vllm,这个issue属于升级需求类型，涉及主要对象为torch和ROCm环境。原因由于需要在ROCm上升级torch至2.6版本，导致相关更新的需求提出。,https://github.com/vllm-project/vllm/issues/15244
vllm,这个issue类型为功能增强，涉及主要对象为V1模型，因为用户需要添加一个禁用级联注意力的标志，以避免潜在的数值问题。,https://github.com/vllm-project/vllm/issues/15243
vllm,这是一个性能优化的issue，主要涉及的对象是通过在TPU上使用`torch.topk`来加速top-k操作。由于在TPU上使用离散的torch操作实现topk效率低下，使用`torch.topk`能够显著提升性能。,https://github.com/vllm-project/vllm/issues/15242
vllm,该问题类型为功能需求，主要对象是通过OpenAI客户端传递vLLM专用参数。由于需要更好的默认值以匹配Hugging Face，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/15240
vllm,这是一个需求类型的issue，主要涉及vLLM对OpenAI Response API的支持。由于OpenAI推出了新的Response API，用户提出希望vLLM也能添加对该API的支持。,https://github.com/vllm-project/vllm/issues/15237
vllm,该issue类型为改进提议，主要涉及将`misc`问题替换为指向论坛的链接。由于问题数量和推广论坛的原因导致了这样的提议。,https://github.com/vllm-project/vllm/issues/15226
vllm,这是一个功能需求类型的issue，主要对象是vLLM实例。由于vLLM实例冷启动时间过长，用户提出了能够预热vLLM实例以减少启动时间的功能需求。,https://github.com/vllm-project/vllm/issues/15225
vllm,这是一个用户提出的需求，主要涉及将tracing依赖项包含在vllm容器镜像中。原因是为了避免在测试流程中手动安装这些依赖项。,https://github.com/vllm-project/vllm/issues/15224
vllm,该issue是一个功能需求，主要涉及支持多卡的Disaggregated Prefill/Decode以及提供自动化基准测试。,https://github.com/vllm-project/vllm/issues/15221
vllm,这个issue是一个用户提出的需求类型，涉及主要对象是README文件。原因是缺少用户论坛链接和联系方式部分的链接，需要完善以提供更全面的信息。,https://github.com/vllm-project/vllm/issues/15220
vllm,这个issue类型属于需求提出，主要涉及的对象是vllm项目和一个自定义模型，用户请求帮助修改VLLM以支持自定义模型中的外部MoE路由。,https://github.com/vllm-project/vllm/issues/15214
vllm,这是一个功能需求提出的issue，主要涉及Mistral Small 3.1 HF格式的支持。由于目前仅支持Mistral格式，用户希望增加对HF格式的支持。,https://github.com/vllm-project/vllm/issues/15212
vllm,这是一个用户提出需求的issue，主要对象是vLLM containers，由于切换到虚拟Python环境导致无法在OpenShift中运行，提出寻求rootless container解决方案。,https://github.com/vllm-project/vllm/issues/15206
vllm,这是一个需求提出的Issue，主要涉及到对于请求延迟指标的存储不足，导致长时间推断请求的指标无法准确记录。,https://github.com/vllm-project/vllm/issues/15202
vllm,这个issue是一个功能需求类型，主要对象是vllm，用户在寻求关于对话前缀延续功能的支持。,https://github.com/vllm-project/vllm/issues/15198
vllm,这个issue类型是功能需求，涉及的主要对象是在实现注释后端方面的扩展性。,https://github.com/vllm-project/vllm/issues/15195
vllm,这个issue是一个功能需求，主要涉及到实现FA3 Fp8 cache支持的功能更新。,https://github.com/vllm-project/vllm/issues/15191
vllm,这个issue是文档更新类型的问题。主要涉及的对象是项目的README.md文件。,https://github.com/vllm-project/vllm/issues/15187
vllm,这个issue属于用户提出需求的类型，主要涉及的对象是增加对指定模型的支持。由于vllm已经支持了一个最接近的模型，用户提出了希望支持另一个模型的需求。,https://github.com/vllm-project/vllm/issues/15186
vllm,这是一个需求更新类型的issue，涉及主要对象为模型和示例的配置文件和权重。用户提出了vLLM目前只支持mistral格式而非HF格式，因此需要更新示例以指向mistral格式，询问是否支持MistralSmall3.1，并寻求此方面的修复。,https://github.com/vllm-project/vllm/issues/15184
vllm,此issue是关于新增功能的请求，涉及到使预测解码与流水线并行性兼容。由于draft pipeline_parallel_size与target pipeline_parallel_size不同，会导致错误，因此需要引入num_virtual_engine config以解决这个问题。,https://github.com/vllm-project/vllm/issues/15173
vllm,该issue为功能特性提议，涉及到测试样例在分布式环境下的运行。这个问题主要是为了确保SPMD基于runner的运行在存在外部数据并行时不会出现故障。,https://github.com/vllm-project/vllm/issues/15172
vllm,这是用户提出的需求，主要目标是允许在基准数据集中请求过采样选项，因为小数据集（如visionarena）通常需要这个功能。,https://github.com/vllm-project/vllm/issues/15170
vllm,"这个issue是一个需求变更，涉及的主要对象是代码文件""ray_distributed_executor.py""。",https://github.com/vllm-project/vllm/issues/15165
vllm,这是一个用户提出的需求类型的issue，主要涉及的对象是LWS文档。由于文档不完整，用户要求更新LWS用户指南以更全面。,https://github.com/vllm-project/vllm/issues/15163
vllm,这个issue属于一个功能需求报告，主要涉及的对象是vllm中的online rotations功能。由于之前vLLM中没有支持online rotations的接口，导致用户提出了这个需求来支持QuaRot quantized模型中的在线旋转功能。,https://github.com/vllm-project/vllm/issues/15162
vllm,该issue是性能优化类型问题，主要涉及的对象是请求输出队列。由于队列操作在高qps时进行性能剖析时出现，因为我们合并了`RequestOutput`对象，所以实际上不需要使用一个真正的队列，在添加时合并输出而不是在移除时，从而导致了此问题。,https://github.com/vllm-project/vllm/issues/15156
vllm,这是一个功能需求的issue，主要涉及到XGrammar的正则表达式支持。由于之前的VLLM版本中的正则表达式回退到了轮廓，用户提交了这个PR来移除VLLM v0用户的这种回退。,https://github.com/vllm-project/vllm/issues/15155
vllm,这是一个类型为功能添加的issue，涉及到V1版本中的speculative decoding metrics。由于需求添加了新的度量标准支持，导致需要进行代码修复和功能增强。,https://github.com/vllm-project/vllm/issues/15151
vllm,这是一个优化性能的issue，主要涉及前端，目的是避免在大多数常见情况下的合并开销。,https://github.com/vllm-project/vllm/issues/15150
vllm,这个issue类型为功能增强或新功能请求，主要涉及的对象是支持mrope模型（Qwen2VL），用户希望继续支持相关工作。,https://github.com/vllm-project/vllm/issues/15149
vllm,这个issue属于软件需求类型，主要涉及到CI/CD构建中缺少latest标签，导致用户需要手动更改镜像版本的问题。,https://github.com/vllm-project/vllm/issues/15142
vllm,这是一个需求提议issue，主要涉及vLLM如何导出指标数据的问题，用户希望支持OpenTelemetry格式以及delta temporality。,https://github.com/vllm-project/vllm/issues/15141
vllm,这个issue类型为需求提议，主要涉及的对象是OpenTelemetry API。由于可能需要更精确的度量，用户提出了与OpenTelemetry API相关的度量提案。,https://github.com/vllm-project/vllm/issues/15138
vllm,这个issue类型是性能优化建议，主要对象是`tokenizers` library，提出了利用`DecodeStream`功能实现更快的增量detokenization，但无法帮助mistral tokenizers。,https://github.com/vllm-project/vllm/issues/15137
vllm,该issue类型为更新需求，涉及主要对象为torch nightly版本，由于旧版本wheels已过期导致该问题产生。,https://github.com/vllm-project/vllm/issues/15135
vllm,该issue类型是功能请求，主要涉及vllm中的torch_compile_cache功能。由于每个rank在v1中重定向缓存到不同的目录，因此不再需要附加pid导致的问题。,https://github.com/vllm-project/vllm/issues/15133
vllm,该issue类型为用户提出需求，主要对象是vllm模型及其多轮问答功能。由于用户有多个问题针对同一图像，但目前的方式需要模型每次都重新处理图像，用户希望找到更好的方法来实现多轮问答。,https://github.com/vllm-project/vllm/issues/15132
vllm,这是一个用户提出需求的issue，主要关注embedding size和vocab_size之间的关系，以及如何通过嵌入向量检索特定的提示标记。,https://github.com/vllm-project/vllm/issues/15131
vllm,"该issue为一个新功能需求，主要涉及到在vLLM上添加对Qwen2.5Omni模型的支持，该需求只涉及到""thinker""部分的实现。",https://github.com/vllm-project/vllm/issues/15130
vllm,这是一个性能优化类型的issue ，主要涉及优化`vllm`的import时间。由于`import torch`和`from openai import xxx`导致了import时间较长，需要优化。,https://github.com/vllm-project/vllm/issues/15128
vllm,这是一个用户提出需求的类型，主要对象是添加了一个用于 w8a8 块 fp8 调整的脚本，可能由于需要进行参数调整或优化而提出帮助或需求。,https://github.com/vllm-project/vllm/issues/15126
vllm,这是一个需求提议类型的issue，主要涉及kv cache的优化和扩充批处理能力。由于kv cache传输成本过高，导致机器学习模型训练中无法充分利用GPU计算资源，提议将kv cache管理转移到CPU并优化批处理以实现更高效的计算。,https://github.com/vllm-project/vllm/issues/15123
vllm,这是一个功能需求的讨论，主要涉及添加一个w8a8块fp8调整脚本，原因可能是为了优化模型的量化过程。,https://github.com/vllm-project/vllm/issues/15118
vllm,这个issue是性能优化相关的需求。用户提出了优化Fused MoE在NVIDIA_L20上的性能问题。,https://github.com/vllm-project/vllm/issues/15117
vllm,这是一个用户提出需求的issue，主要涉及的对象是GPTQ算法的实现。由于缺乏GPTQ算法中类似AWQ算法的**modules_to_not_convert**属性，用户提出了希望增加这一功能的需求。,https://github.com/vllm-project/vllm/issues/15116
vllm,这是一条关于更新XPU Dockerfile和CI脚本的issue，主要涉及Intel GPU，并由于Intel apt仓库问题导致的bug报告。,https://github.com/vllm-project/vllm/issues/15109
vllm,这是一则关于使用VLLM添加功能的疑问，而非bug报告，主要涉及到了在一个单独环境中使用torch的问题。可能由于用户在特定环境中无法成功使用VLLM添加功能而提出疑问。,https://github.com/vllm-project/vllm/issues/15108
vllm,这是一个关于代码优化的issue，主要涉及VLLM中的HF `do_rescale`警告问题，用户提出了设置`do_rescale=False`可以避免不必要的重新缩放的建议。,https://github.com/vllm-project/vllm/issues/15107
vllm,"这是一个关于功能未完善的问题，主要涉及 Aya vision 32b 模型的支持情况。由于功能尚未完全实现，导致 ""dont review "" 指令无法生效。",https://github.com/vllm-project/vllm/issues/15103
vllm,这是关于用户需求/寻求帮助的类型，主要涉及vllm的特定模型推理运行集成问题。由于用户不知道如何与vllm集成，可能导致无法进行特定模型推理的问题。,https://github.com/vllm-project/vllm/issues/15092
vllm,这是一个关于代码逻辑优化的建议类型的issue，主要涉及调度器（scheduler）。由于等待队列（waiting queue）在弹出（popleft）前并未进行排序，可能导致逻辑执行顺序存在疑问。,https://github.com/vllm-project/vllm/issues/15091
vllm,"这是一个需求类型的issue，主要对象是请求在v0.8.0 Docker Image中包含vllm[""audio,video""] package。造成这个问题的原因是在调用/v1/audio/transcriptions端点时出现ImportError，因为缺少vllm[audio] package。",https://github.com/vllm-project/vllm/issues/15087
vllm,这是一个用户提出需求的issue，主要涉及vllm是否能够支持在H20服务器上natively运行特定模型的推断，缺乏集成方法导致用户不清楚如何使用vllm。,https://github.com/vllm-project/vllm/issues/15084
vllm,该issue类型为用户提出需求，主要涉及的对象是对vLLM的新attention layer的实现及kv cache的兼容性。由于vLLM目前仅支持基本的多头注意力机制，用户希望了解如何实现新的attention layer，并提出了kv cache应如何处理以兼容vLLM的问题。,https://github.com/vllm-project/vllm/issues/15077
vllm,这是一个功能增强需求，主要涉及到为vLLM Python代码添加cProfile辅助工具。,https://github.com/vllm-project/vllm/issues/15074
vllm,这个issue类型是用户提出需求，主要涉及的对象是新增模型支持。因为vllm尚未支持用户想要的模型，用户提出了希望添加新模型支持的需求。,https://github.com/vllm-project/vllm/issues/15068
vllm,这是一个功能集成请求，涉及与Pallas内核的写入缓存集成。,https://github.com/vllm-project/vllm/issues/15067
vllm,这个issue类型是文档更新，主要涉及的对象是v1版本的用户指南。,https://github.com/vllm-project/vllm/issues/15064
vllm,这是一个功能需求类型的issue，主要涉及对top-p和top-k采样的解码功能。根据描述，用户希望通过实现`apply_top_k_top_p`来为包含topp和topk采样的请求启用特定解码。,https://github.com/vllm-project/vllm/issues/15063
vllm,这是一个功能增强提议，主要涉及的对象是V1版本中的Tensor parallel模块。,https://github.com/vllm-project/vllm/issues/15059
vllm,这是一个用户提出需求的issue，主要涉及的对象是支持更多视频加载器的功能。由于OpenCV在aarch64平台上可能无法读取视频流，用户试图使用OpenCV作为视频IO的备选方案。,https://github.com/vllm-project/vllm/issues/15055
vllm,这是一个需求报告，主要涉及的对象是模型选择。,https://github.com/vllm-project/vllm/issues/15054
vllm,该issue类型为用户需求，主要对象是用户操作文档页面时可能遇到的问题。由于用户未在文档页面底部的聊天机器人中搜索相关问题，导致用户提出了寻求常见问题答案的需求。,https://github.com/vllm-project/vllm/issues/15052
vllm,这个issue是关于代码优化的讨论，主要涉及到在模型编译期间对多模态编码器进行优化。由于在处理设备张量时可能导致重新编译，用户提出了一些相关问题。,https://github.com/vllm-project/vllm/issues/15051
vllm,这是一个需求提交的issue，主要涉及的对象是StableLMAlphaForCausalLM模型，用户提出了对该模型支持的困难性。,https://github.com/vllm-project/vllm/issues/15046
vllm,这是一个需求类型的issue，主要涉及到如何计算和修正最大和最小像素值的问题。原因可能是用户在使用qwen2vl7b进行图像数据提取时遇到了困惑。,https://github.com/vllm-project/vllm/issues/15034
vllm,这是一个关于性能优化的issue，主要涉及到大批量推断下使用spec decoder时的性能问题，用户关注数据处理开销是否会影响MTP模型执行效率。,https://github.com/vllm-project/vllm/issues/15029
vllm,这是一个用户提出需求的类型，主要对象涉及是Mistral-Small-3.1。由于缺乏详细信息和反馈，用户询问此功能是否支持。,https://github.com/vllm-project/vllm/issues/15027
vllm,这是一个关于如何在多节点上评估DeepSeek-R1-671B性能的需求问题，涉及到在多个节点和GPU上设置benchmark_throughput.py的命令行参数。,https://github.com/vllm-project/vllm/issues/15024
vllm,这是一个用户提出需求的issue，主要涉及的对象是增加对TeleFLM模型的支持，在此基础上验证了相关功能操作正常。,https://github.com/vllm-project/vllm/issues/15023
vllm,这是一个用户提出需求的issue，涉及的主要对象是`torchcodec`。由于`torchcodec`目前不提供适用于Linux的ARM64 wheel，导致用户提出希望支持更多视频加载器的需求。,https://github.com/vllm-project/vllm/issues/15022
vllm,这是一个功能增强请求，主要涉及到CPU后端的支持。由于之前缺乏这项支持，导致当前无法在CPU后端编译torch，需要增加相关功能以解决这一问题。 ,https://github.com/vllm-project/vllm/issues/15020
vllm,这是一个功能需求的issue，主要涉及的对象是视频加载器。由于`decord`库存在长时间未维护以及适配性问题，导致需要支持更多视频加载器，以满足不同用户的需求。,https://github.com/vllm-project/vllm/issues/15011
vllm,这个issue是一个功能需求，该问题涉及到vllm项目中关于s3支持的自定义实现，问题主要是由于S3Model的初始化高度依赖于AWS配置文件导致。,https://github.com/vllm-project/vllm/issues/15005
vllm,这是一个用户提出需求的 issue，主要对象是修改代码库中的某个功能。原因是用户希望能够通过指定的设备重置设备前缀缓存，既可以重置 CPU 的设备前缀缓存也可以重置 GPU 的设备前缀缓存。,https://github.com/vllm-project/vllm/issues/15003
vllm,这是一个功能增强的issue，涉及ROCm集成Paged Attention Kernel，由于模块目前不支持某些`kv_cache_dtype`导致了问题。,https://github.com/vllm-project/vllm/issues/15001
vllm,这是一个用户提出需求的issue，主要涉及vllm中的multimodal generate功能，用户希望能够直接传递预处理的图像数据来避免在vllm内部进行图像预处理。,https://github.com/vllm-project/vllm/issues/14998
vllm,这是一个用户提出需求的issue，主要对象是tpu_model_runner，用户提出了改进token_num padding逻辑的建议。,https://github.com/vllm-project/vllm/issues/14995
vllm,这个issue类型为功能增强需求，涉及主要对象为EAGLE Architecture。造成这个问题的原因是EAGLE实现中的近似KV缓存bug引起性能下降，而用户提出了使用正确的RMS规范来暂时缓解这个bug的需求。,https://github.com/vllm-project/vllm/issues/14990
vllm,这是一个性能优化的Issue，涉及V1版本的改进，主要针对杂项简化和性能优化。,https://github.com/vllm-project/vllm/issues/14989
vllm,这个issue是关于优化代码的提出需求，并涉及主要对象为V1版本的并行抽样实现。,https://github.com/vllm-project/vllm/issues/14985
vllm,该issue属于代码优化类型，主要涉及AITER kernel在Fused MoE的集成，通过将AITER kernel选择逻辑从triton kernels wrapper类移动到UnquantizedFusedMoEMethod类来优化代码结构。,https://github.com/vllm-project/vllm/issues/14982
vllm,这是一个用户提出需求的issue，主要涉及到vLLM在Windows平台上添加CUDA支持。由于当前开源AI模型服务存在碎片化和复杂性，用户希望通过整合vLLM作为标准服务器来简化开发流程，提高用户体验。,https://github.com/vllm-project/vllm/issues/14981
vllm,这个issue属于用户提出需求类型，涉及对象是如何在torch 2.5.1版本中使用最新的main分支。这个问题可能是由于用户想要在现有环境中结合最新的代码功能，但需要一些指导来实现。,https://github.com/vllm-project/vllm/issues/14973
vllm,这个issue类型是功能需求，主要涉及添加XFormers + FP8 KV Cache支持，可能涉及到关于该功能的意义和是否值得实现的讨论。,https://github.com/vllm-project/vllm/issues/14970
vllm,这是一个特性需求的issue，涉及的主要对象是向vLLM集成AITER的Block-Scaled GEMM功能。,https://github.com/vllm-project/vllm/issues/14968
vllm,这是一个特性需求的issue，主要涉及集成AITER提供的融合MoE核，对于未量化模型权重和动态per-tensor量化模型权重以及块fp8量化方法等情况提供支持。,https://github.com/vllm-project/vllm/issues/14967
vllm,该issue属于用户提出需求类型，主要涉及AITER Kernel Integration，并且由于一系列的PRs将kernels拆分，需要进行审查。,https://github.com/vllm-project/vllm/issues/14964
vllm,这是一个改进功能性能的issue，主要涉及到在使用guide decoding时的sample性能问题。由于某些语法下的guide解码吞吐量过慢，于是提出了新的SampleV2算法来优化这一性能问题。,https://github.com/vllm-project/vllm/issues/14962
vllm,该issue是一个功能特性请求，主要涉及的对象是将AITER的RMS Norm层功能整合到vLLM中。由于需要在ROCm平台上改进性能，因此进行了这项功能请求。,https://github.com/vllm-project/vllm/issues/14959
vllm,这个issue类型是功能请求，该问题单涉及的主要对象是pixtral。由于缺乏patch merger功能，用户希望为pixtral添加这一功能。,https://github.com/vllm-project/vllm/issues/14957
vllm,这个issue类型是改进建议，主要涉及的对象是模型加载方式。由于之前使用的`AutoModelForVision2Seq`加载图像模型方式不准确，故提议改用`AutoModelForImageTextToText`以正确加载imagetotext模型。,https://github.com/vllm-project/vllm/issues/14945
vllm,该issue属于需求类型，主要涉及vllm在moe专家并行实现中使用all_reduce而不是all_to_all通信，以及寻找'torch.ops.vllm.moe_forward'函数的问题。这可能是因为用户想了解vllm的使用方式和特定功能的调用位置造成的。,https://github.com/vllm-project/vllm/issues/14944
vllm,这是一个用户提出需求的issue，主要涉及如何在当前环境中使用vllm进行特定模型的推理。用户不清楚如何将其集成到vllm中。,https://github.com/vllm-project/vllm/issues/14943
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm中的attention adaptation模块，用户想知道如何修改kv cache。这个问题产生的原因是用户想在vllm中应用新的attention模块但需要进一步指导。,https://github.com/vllm-project/vllm/issues/14940
vllm,这是一个用户提出需求的问题单，主要对象是要将vLLM北京见面会的幻灯片添加到文档中。,https://github.com/vllm-project/vllm/issues/14938
vllm,这个issue类型是用户提出需求，主要对象是关于VLLM模型的KV缓存大小，用户希望实现Slim Attention特性来减少KV缓存大小。,https://github.com/vllm-project/vllm/issues/14937
vllm,这是一个用户需求提出的issue，主要涉及添加`--seed`选项到离线多模式示例脚本中。由于示例脚本是用户可见的，开发者决定将默认种子保留为`None`，用户可以通过在命令中追加`seed 0`来检查模型实现。,https://github.com/vllm-project/vllm/issues/14934
vllm,这是一个优化性质的问题，主要涉及VLLM中的rejection sampler，由于使用了custom Triton kernels而带来的性能提升和其他优势。,https://github.com/vllm-project/vllm/issues/14930
vllm,这是一个需求类型的issue，主要涉及LRUCache的实现合并问题，由于现有代码中存在两种不同的LRUCache实现，可能会导致混淆。,https://github.com/vllm-project/vllm/issues/14927
vllm,这是一个代码优化类型的Issue，主要涉及AMD构建中的未使用变量清理，通过强制使用Wnounusedvariable来实现。,https://github.com/vllm-project/vllm/issues/14926
vllm,这是一个需求提出的issue，主要涉及vLLM库import时间过长的问题，可能是因为初始化CUDA上下文导致。,https://github.com/vllm-project/vllm/issues/14924
vllm,这个issue是一个功能需求，主要涉及vLLM的benchmark serving脚本，需要确保它不会导入vLLM并满足最低依赖要求。,https://github.com/vllm-project/vllm/issues/14923
vllm,该issue属于功能需求，主要涉及EAGLE模型的chunked prefill支持。由于EAGLE模型需要保存和传递非终端块的最后一个token的隐藏状态，需要对vllm进行扩展以支持此功能。,https://github.com/vllm-project/vllm/issues/14922
vllm,这是一个功能需求类型的issue，主要涉及llama 3.2 vision和CUDA graph支持。由于当前缺乏CUDA graph支持，需要对mllama进行相应的修改以实现此功能。,https://github.com/vllm-project/vllm/issues/14917
vllm,这是一个功能特性需求的issue，主要涉及了AITER Linear kernel的支持在vLLM框架中的集成。这个issue主要讨论了如何启用和性能比较，以及由于使用了AITER Linear导致的性能提升。,https://github.com/vllm-project/vllm/issues/14916
vllm,这是一个用户提出需求的issue，涉及到支持序列并行的功能。由于模型如llama需要支持TP的序列并行，因此需要修改相关层以支持SP并实现一些TODOs。,https://github.com/vllm-project/vllm/issues/14908
vllm,这是一个需求提出类型的issue，主要涉及对象是ConstantList类，用户提出了需要为ConstantList类添加__repr__方法的需求。,https://github.com/vllm-project/vllm/issues/14907
vllm,这是一个功能需求，主要涉及到增强与高级负载均衡/网关的集成，实现更好的负载/成本报告和 LoRA 管理。,https://github.com/vllm-project/vllm/issues/14906
vllm,"这个issue类型为性能优化，主要对象是""overhead of rewinding""。",https://github.com/vllm-project/vllm/issues/14905
vllm,这是一个关于功能需求的issue，主要涉及到启用`v1/entrypoints`，由于之前的skip操作导致该需求未被执行。,https://github.com/vllm-project/vllm/issues/14903
vllm,这是一个性能优化类的issue，主要涉及到VLLM engine的性能问题。导致这个问题的原因是高QPS下性能较差，用户提出希望在engine core初始化后冻结gc以提升性能。,https://github.com/vllm-project/vllm/issues/14902
vllm,这个issue类型为质量改进（Quality Improvement），涉及主要对象为测试设置（test settings），原因是为了提高测试的可重复性。,https://github.com/vllm-project/vllm/issues/14893
vllm,这是一个用户提出需求的特性建议。主要对象是使用guided_decoding来实现Function Calling。该建议起因于希望确保任何模型都按要求的schema进行输出。,https://github.com/vllm-project/vllm/issues/14890
vllm,这是一个改进建议类型的issue，主要涉及benchmark脚本的优化。原因是为了提高性能，避免默认情况下保存详细信息到json中。,https://github.com/vllm-project/vllm/issues/14879
vllm,这是一个用户提出需求的issue，涉及的主要对象是添加更多调谐配置。由于vLLM缺少某些NVIDIA GPU的Triton配置，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/14877
vllm,这个issue属于需求提出类型，主要对象是 请求多模型模型。由于项目变更，导致链接更新至新的项目板。,https://github.com/vllm-project/vllm/issues/14876
vllm,这个issue是关于优化代码性能的建议，主要涉及模型运行器中聚合分块提示日志概率的操作。,https://github.com/vllm-project/vllm/issues/14875
vllm,这是一个用户提出需求的issue，主要涉及的对象是 benchmark README。这个问题由于需要修改默认的文本到文本示例以使用 `vllm` 后端，以便更好地与离线情况保持一致。,https://github.com/vllm-project/vllm/issues/14874
vllm,这是一个需求更改的issue，主要涉及的对象是 mistral-tokenizer 模块。,https://github.com/vllm-project/vllm/issues/14873
vllm,这是一个需求提出类型的 issue，主要涉及到添加一个用于测试 `--load-format sharded_state` 功能的示例脚本。,https://github.com/vllm-project/vllm/issues/14872
vllm,这是一个用户提出需求的类型，主要涉及的对象是VLLM库中的sequence parallel功能。由于还未支持与PP的组合和更多模型，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/14871
vllm,这是一个用户提出需求的issue，主要涉及的对象是要求vllm添加对CohereForAI/c4aicommanda032025模型的支持。由于需要Properly支持tokenizer和templates，以及对模型进行调用，用户提出了这个问题。,https://github.com/vllm-project/vllm/issues/14866
vllm,这是一个用户提出需求的类型，主要涉及将vLLM部署在仅使用CPU的Kubernetes环境中。这是因为之前的部署要求GPU作为先决条件，用户提出希望提供只使用CPU的部署指南。,https://github.com/vllm-project/vllm/issues/14865
vllm,这个issue类型是功能改进，涉及主要对象为V1版本的多模态模型。,https://github.com/vllm-project/vllm/issues/14864
vllm,这是一个关于用户提出需求的问题，主要涉及VLLM引擎如何跳过新请求的预填充步骤。这个问题可能由于用户想要在添加新请求到引擎时跳过预填充步骤，直接开始解码阶段而导致。,https://github.com/vllm-project/vllm/issues/14863
vllm,这是一个用户提出需求的issue，主要涉及DP MLA + EP/TP MoE算法在在线服务中的使用问题。由于缺乏相应文档，用户无法找到如何使用该功能的方法。,https://github.com/vllm-project/vllm/issues/14862
vllm,这个issue类型是功能需求提出，涉及的主要对象是Llama模型。这个提议是为了通过hf-overrides调整Llama模型中的隐藏层数，以帮助在开发自定义内核、后端和其他组件时减少测试时间。,https://github.com/vllm-project/vllm/issues/14861
vllm,该issue为优化类型，主要涉及Intel GPU CI docker构建流程，由于需要在Intel GPU CI队列中添加多个代理，因此需要重新定义docker镜像名称和容器名称。,https://github.com/vllm-project/vllm/issues/14860
vllm,这是一个功能需求的issue，主要涉及VLLM工具中在config.yaml中指定模型的功能。,https://github.com/vllm-project/vllm/issues/14855
vllm,该issue类型为需求提出，主要涉及的对象是CI/Build。由于需要将dockerfile移动，用户可能需要重新配置持续集成构建流程。,https://github.com/vllm-project/vllm/issues/14853
vllm,这是一个用户提出需求的类型，该问题单涉及添加新的 meetups 到 README 和 meetups.md 页面。由于缺少East Coast vLLM Meetup slides的链接，用户请求将其添加到文档中。,https://github.com/vllm-project/vllm/issues/14852
vllm,这是一个功能需求类型的issue，主要涉及V1 tpu_model_runner的测试添加。由于当前测试不足，需要增加更多测试以提高代码质量。,https://github.com/vllm-project/vllm/issues/14843
vllm,这个issue类型是功能改进请求，主要涉及的对象是TPU作业功能。由于日志清理命令引起了混淆和干扰，需要将其从TPU作业中移除。,https://github.com/vllm-project/vllm/issues/14838
vllm,这个issue为功能增强类型，主要涉及的对象是vllm项目中的ROCm平台下的注意力机制，其中涉及添加了AITER PagedAttention类、移动了AITER环境变量检查、在非8位数据类型上禁用AITER paged attention以及添加了回退机制。,https://github.com/vllm-project/vllm/issues/14836
vllm,这个issue属于需求类型，涉及的主要对象是构建/持续集成（Build/CI）系统。这个问题提出了将ninja移动到通用依赖项的要求，从而保证符合xgrammar的要求，提交者Russell Bryant签名支持。,https://github.com/vllm-project/vllm/issues/14835
vllm,这是一个需求类型的issue，主要涉及的对象是添加TPU V1的CI测试，原因可能是为了确保系统对TPU V1的兼容性以及持续集成测试的完整性。,https://github.com/vllm-project/vllm/issues/14834
vllm,该issue类型是功能需求提交，主要对象是Entrypoints Test。由于原因导致了该需求未被启用。,https://github.com/vllm-project/vllm/issues/14832
vllm,这是一个需求提出类型的issue，主要对象是vllm的CI功能。因为需要在vllm CI中添加一个运行TPU V1测试的脚本，说明用户希望在CI流程中测试TPU V1功能，以确保代码的兼容性和稳定性。,https://github.com/vllm-project/vllm/issues/14831
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是添加TPU V1测试。,https://github.com/vllm-project/vllm/issues/14830
vllm,这个issue属于用户提出需求类型，主要涉及LoRA的float32支持问题，用户因为当前环境不支持float32而提出了问题。,https://github.com/vllm-project/vllm/issues/14827
vllm,这是一个升级请求。它涉及的主要对象是vllm库中的bitsandbytes。这个问题由于与triton==3.2不兼容性而导致，需要升级来解决此问题。,https://github.com/vllm-project/vllm/issues/14825
vllm,这是一个用户提出需求的issue，涉及对象是指定在`config.yaml`文件中指定`model`的功能。用户提出希望不仅可以通过CLI指定`model_tag`，也可以通过配置文件指定。,https://github.com/vllm-project/vllm/issues/14819
vllm,这是一个需求提出类型的issue，涉及主要对象是VLM（Vision Language Model），由于代码中存在重复的navit实现，并需要处理跳过的层次，希望进行整理和优化。,https://github.com/vllm-project/vllm/issues/14812
vllm,这是一个需求提出类型的 issue，主要涉及 VLLM（Very Large Language Model）的多模态输入缓存限制。原因是基于内存使用而不是多模态项目数量来限制输入缓存，以解决存储问题和性能优化。,https://github.com/vllm-project/vllm/issues/14805
vllm,这个issue属于文档需求类型，主要涉及load_format项的补充，由于在文档中缺少load_format的相关内容，导致用户可能无法正确使用该功能。,https://github.com/vllm-project/vllm/issues/14804
vllm,这是一个功能增强类型的issue，主要涉及的对象是ROCm Flash Attention后端，问题是关于启用ROCm Flash Attention用于编码-解码模型。,https://github.com/vllm-project/vllm/issues/14803
vllm,这个issue类型为用户提出需求，涉及主要对象为使用vLLM时多个不同品牌和频率的4090显卡。用户询问vLLM是否能在这种设置下正常工作，以及如何集成vLLM进行特定模型的推断。,https://github.com/vllm-project/vllm/issues/14802
vllm,这是一个需求提出类型的issue，涉及的主要对象是`SupportsMultiModal`接口。,https://github.com/vllm-project/vllm/issues/14794
vllm,"这是一个需求类型的issue，主要涉及的对象是""fastcheck""。由于测试数量过多，用户希望减少其数量以提高效率。",https://github.com/vllm-project/vllm/issues/14782
vllm,这是一个提出需求的issue，主要涉及调度器（Scheduler）模块，旨在讨论使用字典（dict）作为运行队列的潜在优势。,https://github.com/vllm-project/vllm/issues/14781
vllm,这是一个功能需求类型的issue，涉及V1版本的guidance作为后端来支持结构化输出，同时添加了一个名为`auto`的新后端模式。,https://github.com/vllm-project/vllm/issues/14779
vllm,这是一个优化性能的issue，主要涉及到vLLM中的Mamba2 Prefill功能。造成这个问题的原因是频繁的内存复制导致性能下降。,https://github.com/vllm-project/vllm/issues/14778
vllm,这是一个用户提出需求的issue，涉及到Truncation control for embedding models。由于缺乏截断控制功能，导致需要对模型输入的提示进行裁剪。,https://github.com/vllm-project/vllm/issues/14776
vllm,这是一个用户提出需求的issue，主要涉及如何在benchmark_serving.py中添加Sampling Parameters（n和best_of），用户希望能够通过使用Sampling Parameters来运行推理，但不知道如何将其整合到vllm serving benchmark中。,https://github.com/vllm-project/vllm/issues/14775
vllm,这是一个用户提出需求的issue，主要涉及V1 TPU，用户希望默认启用前缀缓存。,https://github.com/vllm-project/vllm/issues/14773
vllm,这个issue类型是功能优化，主要涉及的对象是处理器测试代码。原因是为了让`build_model_context`自动使用来自`HF_EXAMPLE_MODELS`的配置。,https://github.com/vllm-project/vllm/issues/14771
vllm,该issue属于提出需求类型，主要涉及到为多模态模型检查输入形状定义Schema。由于当前对多模态输入的验证仅进行了最小限度的检查，导致一些模型仅检查输入的类型，实际输入形状可能与文档中所述不符。,https://github.com/vllm-project/vllm/issues/14764
vllm,这个issue类型是功能改进，涉及V1引擎在启动过程中错误处理的优化，主要对象是启动时的错误提示信息。原因是要提高启动过程中的错误信息提示和可扩展性。,https://github.com/vllm-project/vllm/issues/14758
vllm,这是一个用户提出需求的issue，主要涉及到文档缺失的问题，可能是由于未更新或遗漏导致的。,https://github.com/vllm-project/vllm/issues/14757
vllm,这个issue是用户需求报告，主要涉及用户寻找公开的vllmcpu镜像，但无法找到官方提供的镜像。,https://github.com/vllm-project/vllm/issues/14756
vllm,这个issue是关于用户提出需求的类型，主要涉及支持custom all reduce for rocm。由于缺少该功能，用户提出了希望添加支持custom all reduce for rocm的需求。,https://github.com/vllm-project/vllm/issues/14755
vllm,这是一个功能建议类型的 issue，主要涉及修改多模态输入时出现当前计数大于允许计数引发 ValueError 的情况。由于当前实现导致客户端无法获取错误信息，建议修改为响应最后的允许计数项。,https://github.com/vllm-project/vllm/issues/14752
vllm,这是一个用户提出需求的issue，主要对象是扩展CI范围以使用两个卡来测试hpu设备，防止多卡并行导致的问题。,https://github.com/vllm-project/vllm/issues/14751
vllm,这是一个用户提出需求的类型，主要涉及MoE模型在VLLM中的专家并行化应用问题。用户想要定制MoE模型专家层的放置方式，例如将不同的专家放置在不同的GPU上。,https://github.com/vllm-project/vllm/issues/14749
vllm,这个issue类型是用户提出需求，涉及主要对象是改进启动时的错误处理；用户希望实现的功能是改进启动时的错误处理。,https://github.com/vllm-project/vllm/issues/14748
vllm,这是一个功能更新提议的issue，主要涉及的对象是代码中的`need_kv_parallel_group`函数。由于代码更新替换了`need_kv_parallel_group`函数，在初始化KVTransferAgent时使用`is_kv_transfer_instance`，导致`need_kv_parallel_group`函数不再需要，因此建议将其删除。,https://github.com/vllm-project/vllm/issues/14746
vllm,这是一个需求提出类型的issue，主要涉及的对象是DeepSeek模型的工具调用功能。用户怀疑DeepSeek模型是否支持工具调用，并提出了相关疑问。,https://github.com/vllm-project/vllm/issues/14745
vllm,这个issue类型是功能需求，涉及的主要对象是CPU后端。由于需要在CPU后端中添加对FP8E5M2 KV缓存的支持并更新测试，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/14741
vllm,这是一个用户提出需求的issue，主要涉及VLA series large models的支持问题。由于当前vllm文档不支持该系列模型，用户希望能够添加对VLA series模型的支持。,https://github.com/vllm-project/vllm/issues/14739
vllm,这是一个用户需求报告的issue，主要涉及的对象是支持加载InternVideo2.5模型作为原始InternVLChatModel。由于Hierarchical token compression (HiCo)技术尚未支持，导致输出结果与HF当前结果不同，用户可能在此寻求支持或解决方案。,https://github.com/vllm-project/vllm/issues/14738
vllm,这是一个用户提出需求的issue，主要涉及如何禁用HTTP请求日志，可能是因为用户想要在特定模型上运行推理但不清楚如何集成。,https://github.com/vllm-project/vllm/issues/14736
vllm,这是一个需求提出的issue，主要涉及对象是vLLM和Gemma3，由于无法导入`__version__`导致的错误。,https://github.com/vllm-project/vllm/issues/14734
vllm,这是一个需求改进类型的issue， 主要涉及VLLM V1的日志记录在初始化过程中常见错误的情况。导致此问题的原因是初始化过程中可能出现内存不足或KV缓存空间不足的情况。,https://github.com/vllm-project/vllm/issues/14733
vllm,"这是一个建议性质的issue，主要涉及scheduler。由于内容描述只有""A superminor improvement on the scheduler.""，无法确定具体是哪方面的改进或问题。",https://github.com/vllm-project/vllm/issues/14732
vllm,这个issue是关于删除ultravox LoRA测试的建议，主要涉及的对象是CI/Build过程。原因是LoRA测试存在问题，需要下载2个模型且未能有效训练LoRA模型，导致CI测试压力增加。,https://github.com/vllm-project/vllm/issues/14730
vllm,这个issue属于需求提出类型，主要涉及V1版本性能优化中的Cascade Kernel，由于目前V1只支持单个树时的级联注意力，需要扩展到支持多个树的情况。,https://github.com/vllm-project/vllm/issues/14729
vllm,这是一个用户提出需求的问题单，主要涉及的对象是vllm中的xgrammar实现，可能是由于缺乏对新功能支持计划而导致用户希望获取关于实现这一功能的建议或替代方案。,https://github.com/vllm-project/vllm/issues/14727
vllm,这是一个功能提议的issue，主要涉及VLLM的矩阵计算优化，提议将`VLLM_MLA_PERFORM_MATRIX_ABSORPTION=0`设置为默认选项，以减少内存开销。,https://github.com/vllm-project/vllm/issues/14725
vllm,这是一个需求提案（RFC），主要涉及的对象是对 vLLM 项目中的 KV Cache 进行扩展，为了支持更高效的 KV Cache 卸载和跨引擎的 KV 复用。由于当前 KV transfer connector 框架在跨引擎 KV 复用方面存在缺失，需要增加功能以确保一致性视图。,https://github.com/vllm-project/vllm/issues/14724
vllm,这是一个用户提出需求的issue，主要对象是vLLM项目。由于OpenAI发布了新的Responses API，需要vLLM添加对该API的兼容，以保持与OpenAI更新的同步。,https://github.com/vllm-project/vllm/issues/14721
vllm,这个issue类型是请求反馈，并涉及V1 Spec Decode Eagle支持，用户寻求社区关于设计的反馈。,https://github.com/vllm-project/vllm/issues/14719
vllm,这是一个用户提出需求的issue，主要涉及vllm中AsyncLLMEngine是否支持batch inference需求。原因是用户需要实现同时支持批处理推断和流式返回的引擎，但当前发现vllm中的`AsyncLLMEngine`支持流式返回，但不支持批处理推断，因此想了解是否有其他引擎能够同时支持这两种需求。,https://github.com/vllm-project/vllm/issues/14717
vllm,这是一个功能改进的issue，主要涉及的对象是神经元（neuron）的注意力机制内核（attention kernels），由于优化测试参数，在重新设计测试脚本的过程中简化了测试用例的数量，并加强了覆盖范围，以减少持续集成时间，同时保证了不同规模和精度模式下的正确性。,https://github.com/vllm-project/vllm/issues/14712
vllm,"这是一个包含""Differential Revision: D71078962""内容的类型为代码审查/合并请求的issue，涉及的主要对象是代码修改的审查和合并。",https://github.com/vllm-project/vllm/issues/14708
vllm,这是一个关于优化功能升级的RFC（Request for Comments），涉及自动功能化V2在PyTorch 2.7+中的使用，主要问题是某些自定义融合通过依赖模式匹配，目前无法与auto_functionalized_v2兼容，以及另一个问题导致当前在PyTorch 2.6+中禁用V2。,https://github.com/vllm-project/vllm/issues/14703
vllm,这是一个功能优化的issue，主要涉及到了V1版本中的结构化输出。原因是为了改进推断解码时的结果验证和GPU运行效率，以确保结果符合语法约束。,https://github.com/vllm-project/vllm/issues/14702
vllm,该issue类型为用户提出需求，涉及的主要对象是BartModel。由于缺少了SupportsQuant和packed_modules_mapping，用户提出了需要为所有模型添加这两个功能的要求。,https://github.com/vllm-project/vllm/issues/14699
vllm,这是一个用户提出需求的issue，主要对象是Bamba模型。由于缺少对SupportsQuant和packed_modules_mapping的支持，需要将其添加到所有模型中。,https://github.com/vllm-project/vllm/issues/14698
vllm,这是一个功能需求提出的issue，主要涉及的对象是TPU日志。,https://github.com/vllm-project/vllm/issues/14697
vllm,这是一个特性需求的issue，主要涉及的对象是vLLM和Gemma 3架构。由于缺乏对Gemma 3的支持，导致在加载google/gemma312bit时出现错误，用户希望vLLM能够支持Gemma 3。,https://github.com/vllm-project/vllm/issues/14696
vllm,这个issue是关于更新v0.8中废弃指标列表的，涉及的主要对象是已弃用指标，由于指标已在代码中标记为废弃，但没有在文档中列出，导致了这个问题。,https://github.com/vllm-project/vllm/issues/14695
vllm,该issue属于需求提出类型，涉及主要对象为添加ray[data]作为tpu依赖。由于在编译图中需要使用ray[data]，因此用户提出添加该依赖的需求。,https://github.com/vllm-project/vllm/issues/14691
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是内存交错（Memory interleaving）。由于系统具有多个内存节点，通过控制内存策略来提高性能，但由于节点可能依赖于允许的CPU，在导入torch后才能确定交错节点，这导致了用户需求对内存交错进行控制。,https://github.com/vllm-project/vllm/issues/14690
vllm,这是一个用户提出需求的issue，主要涉及的对象是InternVL2.5和InternVideo2.5模型。由于当前版本只支持特定的InternVideo2_5_Chat_8B模型，用户提出需要为InternVL2.5模型添加视频输入支持并处理交错图像视频输入的需求。,https://github.com/vllm-project/vllm/issues/14688
vllm,这个issue是关于性能优化的建议，涉及的主要对象是vLLM v1引擎，提出了减少重复预加载标记对性能影响的问题。,https://github.com/vllm-project/vllm/issues/14686
vllm,这是一个请求移除功能或做法类型的issue，主要涉及LoRA中的SGMV和BGMV kernels，需删除以便使用V1 kernels。,https://github.com/vllm-project/vllm/issues/14685
vllm,该issue类型是性能优化，主要涉及 Qwen2VL 的 LM `mrope_positions` 和 ViT 的 `rot_pos_ids` 的优化。,https://github.com/vllm-project/vllm/issues/14684
vllm,这个issue是关于功能需求的，主要涉及到数据并行推理的实现。由于单个GPU内存能力有限，需要实现在离线模式下利用多个GPU进行数据并行推理以提高效率。,https://github.com/vllm-project/vllm/issues/14683
vllm,这是一个用户提出需求的issue，主要涉及改进内存交错以提高CPU和系统内存之间的内存带宽，其背景是在Linux系统上，多个内存节点和物理内存控制器可能导致内存带宽受限，因为操作系统通常会从最接近运行分配进程的CPU的节点中分配内存。,https://github.com/vllm-project/vllm/issues/14680
vllm,这是一个用户提出的需求，主要涉及加入一个Level 3睡眠模式，可以将模型权重存储到磁盘并且舍弃kv缓存。原因可能是为了在唤醒时最大限度地利用CPU内存并有效地从磁盘加载。,https://github.com/vllm-project/vllm/issues/14678
vllm,这是一个用户提出需求的类型issue，主要涉及 VLLM 推理是否支持混合精度模型，用户想要了解如何与 VLLM 集成以运行特定模型推理。这个问题的根本原因在于用户需要更多关于模型精度配置和集成的信息。,https://github.com/vllm-project/vllm/issues/14674
vllm,这是一个特性需求的issue，主要涉及Ray的Compiled Graph功能对其他设备的支持。,https://github.com/vllm-project/vllm/issues/14668
vllm,这个issue是关于用户需求，主要涉及如何设置特定文件以用于vllm运行推理，并表明用户不清楚如何与vllm集成，可能是由于缺乏相关集成教程或文档引起的。,https://github.com/vllm-project/vllm/issues/14665
vllm,这是一个用户提出需求的issue，主要对象是新模型Gemma 3，原因是想让vllm支持该模型。,https://github.com/vllm-project/vllm/issues/14663
vllm,这是一个功能需求类型的issue，主要涉及 V1 AsyncLLM 的日志记录定制问题，因为现在的日志记录功能不支持自定义 logger 扩展 StatLoggerBase，用户希望能够实现与 V0 版本相同的日志记录定制支持。,https://github.com/vllm-project/vllm/issues/14661
vllm,这个issue是一个功能需求类型的问题，主要涉及的对象是VLLM模型的Gemna 3版本。其原因是由于目前的注意力机制后端无法高效支持Gemna 3模型对于图片token的双向注意力，因此必须临时使用PyTorch SDPA进行处理。,https://github.com/vllm-project/vllm/issues/14660
vllm,这是一个用户提出需求类型的issue，主要涉及hub.docker.com缺少arm架构的docker镜像文件。,https://github.com/vllm-project/vllm/issues/14656
vllm,这是一个新功能需求的issue， 主要涉及visionarena offline support for benchmark_throughput功能的添加。,https://github.com/vllm-project/vllm/issues/14654
vllm,这是一个用户提出需求的问题，主要涉及如何在内存中缓存Lora适配器，由于找不到实现代码而导致需求未满足。,https://github.com/vllm-project/vllm/issues/14651
vllm,这是一个文档更新类的issue，主要涉及性能基准数据集的更新。,https://github.com/vllm-project/vllm/issues/14646
vllm,这是一个功能需求，涉及到替换`current_platform.has_device_capability()`为特定功能API，主要是为了提高代码的可读性和易维护性。,https://github.com/vllm-project/vllm/issues/14637
vllm,这是一个需求变更的issue，主要涉及支持量化（Quantization）功能中的模块映射问题，由于未正确处理`ignored_modules`导致脚本执行失败。,https://github.com/vllm-project/vllm/issues/14635
vllm,这是一个功能需求的issue，涉及VLLM的LoRA适配器动态加载至缓存目录，因为需要实现在没有外部基础设施的情况下，根据指定目录加载并使用特定模型的问题。,https://github.com/vllm-project/vllm/issues/14634
vllm,这是一个功能需求类型的issue，主要涉及VLLM项目中的模型添加关于量化支持的功能。由于某些模型具有子模型关系，导致`packed_modules_mapping`只能在初始化时确定，需要引入`SupportsQuant`以支持这一需求。,https://github.com/vllm-project/vllm/issues/14631
vllm,这是一个功能需求的issue，针对V1 LoRA启用CUDAGraphs支持。,https://github.com/vllm-project/vllm/issues/14626
vllm,这个issue类型是功能需求提出，主要对象是VLLM项目中的Core模块，用户提出了支持MistralTokenizer用于结构化输出的需求。,https://github.com/vllm-project/vllm/issues/14625
vllm,该issue属于代码优化类型，涉及主要对象为StructuredOutputManager，由于xgrammar已经实现了缓存功能，因此不再需要在StructuredOutputManager中额外实现缓存，从而简化代码结构。,https://github.com/vllm-project/vllm/issues/14622
vllm,这是一个用户提出需求的issue，主要对象是关于在vllm中如何使用embeddings作为输入而不是token_ids。由于用户想要在推理过程中使用DeepSeekR1DistillQwen1.5B模型，但不知道如何与vllm集成，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/14621
vllm,这是一个需求类型的issue，主要涉及的对象是进行CI测试的`v1/entrypoints`。,https://github.com/vllm-project/vllm/issues/14619
vllm,这是一个功能改进类型的issue，主要涉及Kernel中的GGUF MoE kernel，通过提高速度并启用图缓存，可在8xH100上将DeepSeek GGUF的速度从10提升到50 tok/s，对Q4_K quants。,https://github.com/vllm-project/vllm/issues/14613
vllm,这是一则涉及软件功能增强的issue，主要对象是增加Intel GPU V1引擎支持。,https://github.com/vllm-project/vllm/issues/14612
vllm,这个issue类型是性能优化，主要涉及的对象是DeepSeek GGUF，由于添加了 GGUF MoE kernel，导致速度从10提升到50 tok/s。,https://github.com/vllm-project/vllm/issues/14611
vllm,这个issue类型是用户提出需求，涉及主要对象是vllm软件安装。由于nvidia驱动和cuda版本不兼容，导致无法安装vllm==0.7.3。,https://github.com/vllm-project/vllm/issues/14608
vllm,这是一个用户提出需求的issue，主要涉及对象是获取VLLM模型的原始隐藏状态。这个问题的根本原因是用户想要通过VLLM获取与transformers输出相同形状和数值的隐藏状态，但目前VLLM提供的方法无法达到这一目标。,https://github.com/vllm-project/vllm/issues/14607
vllm,该issue类型为用户提出需求类型，主要涉及到对模型stepfunai/GOTOCR2.0hf的支持。由于没有收到响应，用户提出了对所需模型支持的困难，但并未详细说明导致问题的原因。,https://github.com/vllm-project/vllm/issues/14606
vllm,这个issue类型为需求探讨，主要涉及的对象是vllm中构建物理KV缓存块时选择的维度布局。由于布局顺序的调整，用户提出是否能改善内存访问，是否无差异或性能会下降。,https://github.com/vllm-project/vllm/issues/14604
vllm,这个issue类型是用户提出需求，询问关于vllm CPU backend是否支持Intel AMX的问题。,https://github.com/vllm-project/vllm/issues/14603
vllm,这是一个用户提出需求的类型为使用问题，主要涉及多模态数据集的离线吞吐量/延迟基准测试。由于目前只能在在线模式下使用 benchmark_serving 实现多模态数据集支持，用户想知道如何在离线模式下获取多模态输入的吞吐量和延迟结果。,https://github.com/vllm-project/vllm/issues/14600
vllm,这个issue属于用户提出需求类型，主要对象是Varun/v1 lora kernels tuner。原因是用户希望运行自动调谐程序。,https://github.com/vllm-project/vllm/issues/14594
vllm,这是一个需求提出类型的 issue，主要涉及 xgrammar 的升级和添加正则表达式结构化输出支持。,https://github.com/vllm-project/vllm/issues/14590
vllm,这是一个用户提出需求的 issue，主要是关于给一个测试命名。原因可能是为了更清晰地区分不同的测试用例。,https://github.com/vllm-project/vllm/issues/14584
vllm,该issue为需求提议，主要涉及硬件优化和TPU加速，旨在减少编译时间。,https://github.com/vllm-project/vllm/issues/14582
vllm,该issue类型为需求提出，主要对象涉及token_num的padding逻辑。由于当前策略会导致计算资源浪费，需要改进。,https://github.com/vllm-project/vllm/issues/14581
vllm,这是一个功能需求类型的Issue，主要涉及vLLM在TPU上添加重新编译检查的功能。由于PyTorch/XLA的隐式编译导致LLM提供服务期间出现过多的重新编译，影响性能。,https://github.com/vllm-project/vllm/issues/14580
vllm,这个issue类型为功能需求，涉及支持FP8 GEMM层中输入是FP8数据类型的情况，以便实现与融合FP8转换的关注准备，可能由于当前版本未实现此功能而导致用户需要此功能。,https://github.com/vllm-project/vllm/issues/14578
vllm,这个issue类型是用户提出需求，主要对象是SmolLM2系列中的 135M 和 360M 参数变体。由于用户需要更轻量级的解决方案，并且希望利用 vLLM 的高性能推理引擎，因此请求在 vLLM 中支持这些较小的模型。,https://github.com/vllm-project/vllm/issues/14576
vllm,这个issue是关于功能增强（feature enhancement），主要涉及FlashAttention3中的FP8 KV缓存支持，提供了性能更好的FlashAttention。导致提出该需求的主要原因是FlashAttention对所有的Q、K和V都进行FP8计算，与FlashInfer不同，希望提升性能。,https://github.com/vllm-project/vllm/issues/14570
vllm,"这个issue类型为用户提出需求，主要对象是一个新的模型模块""phi-4-multimodal-instruct""，用户寻求支持该新模型所遇到的困难。",https://github.com/vllm-project/vllm/issues/14569
vllm,该issue是关于优化moe优化中的permute/unpermute kernel实现的。,https://github.com/vllm-project/vllm/issues/14568
vllm,这个issue属于文档更新类型，涉及PaliGemma笔记，主要目的是将笔记更新为警告内容。,https://github.com/vllm-project/vllm/issues/14565
vllm,这个issue类型是功能升级，涉及的主要对象是vllm中的Intel GPU，并由于IPEXxpu版本升级引发。,https://github.com/vllm-project/vllm/issues/14564
vllm,这个issue类型是功能更新，并涉及到XGrammar版本升级到0.1.15。根据描述，用户寻求升级支持aarch64架构以及解决之前版本不兼容的问题。,https://github.com/vllm-project/vllm/issues/14563
vllm,这是一个功能需求建议，主要涉及最大输出长度限制，由于大量 max_tokens 可能导致延迟或请求饥饿。,https://github.com/vllm-project/vllm/issues/14553
vllm,这个issue属于用户需求类，主要涉及的对象是Transformer模型的文档。用户提出这个问题是希望文档中提及`model_impl`参数，以帮助用户了解如何手动使用该参数。,https://github.com/vllm-project/vllm/issues/14552
vllm,这是一个需求提出类型的issue，主要涉及将dockerfiles移动到它们自己的目录中。,https://github.com/vllm-project/vllm/issues/14549
vllm,这个issue类型是功能改进，主要涉及对象是 `QKVCrossParallelLinear`。由于需要支持BNB 4位量化，导致之前的CC被撤销，并进行相关功能的修改。,https://github.com/vllm-project/vllm/issues/14545
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm中的深度模型。,https://github.com/vllm-project/vllm/issues/14544
vllm,这是一个需求类型的issue，主要对象是编译测试的CI环境。,https://github.com/vllm-project/vllm/issues/14543
vllm,这个issue是性能改进类型的，主要涉及到V1版本中MLA的性能优化。原因是之前的`rotary_emb`专用性问题导致了一些CPU开销，另外`build`函数中的操作顺序问题也导致了一些额外开销。,https://github.com/vllm-project/vllm/issues/14540
vllm,这个issue是用户询问关于vllm是否支持inflight batch功能的问题，主要涉及vllm的功能使用。用户提出问题是由于在vllm文档中找不到相关信息。,https://github.com/vllm-project/vllm/issues/14536
vllm,这是一个用户提出需求的issue，主要涉及vllm启动bge时如何支持不同返回类型的问题。可能由于使用指定模型的推理过程中不清楚如何与vllm集成，导致用户需要寻求相关帮助。,https://github.com/vllm-project/vllm/issues/14533
vllm,这是一个功能需求类型的issue，主要涉及到VLLM库中的例子代码。由于内部结构变化，需要更新例子代码以支持最新版本的VLLM。,https://github.com/vllm-project/vllm/issues/14530
vllm,"这是一个功能请求 issue，主要涉及的对象是 bgem3 embedding model。这个issue描述了如何通过传入""additional_data"": {""sparse_embeddings"": true}来请求稀疏嵌入向量。原因是目前只能返回单个张量类型的结果，导致了这个问题。",https://github.com/vllm-project/vllm/issues/14526
vllm,这是一个需求提出类型的issue，主要涉及到Vllm项目中的Prompt Validation功能更新问题。这个问题是由于可能存在用户输入异常数据导致的需求调整。,https://github.com/vllm-project/vllm/issues/14525
vllm,这个issue属于改进建议类型，主要对象是测试套件中的os.environ函数调用，由于需要将所有os.environ(xxx)替换为monkeypatch.setenv导致需要进行改进。,https://github.com/vllm-project/vllm/issues/14515
vllm,这是一个用户提出需求的类型的issue，主要涉及vLLM的并发处理能力问题。由于当前环境下vLLM版本的限制，导致用户在200个并发用户时只能处理最多100个请求。,https://github.com/vllm-project/vllm/issues/14513
vllm,这是一个用户提出需求（feature request）的issue，主要涉及前端工具调用和推理解析器，并解决了工具调用只从内容而不是推理内容中解析的问题。,https://github.com/vllm-project/vllm/issues/14511
vllm,这是一个功能需求类型的issue，主要涉及对象是SpeculativeConfig，原因是为了使得推理模型选择更加清晰和易于推断所需的推理模型。,https://github.com/vllm-project/vllm/issues/14509
vllm,这是一个用户提出需求的issue，主要涉及使用 microsoft/Phi-4-multimodal-instruct audio，用户希望在vllm中运行具体模型的推断，但不清楚如何集成音频部分。,https://github.com/vllm-project/vllm/issues/14507
vllm,这是一个功能需求的issue，主要涉及LMCache连接器的支持chunked prefill功能。由于LMCache连接器目前不支持chunked prefill，用户提出了这一需求。,https://github.com/vllm-project/vllm/issues/14505
vllm,这个issue是一个功能需求，涉及主要对象为测试套件中的`os.environ(xxx)`转换为`monkeypatch.setenv`，由于需要将环境变量的设置方式统一，可能是为了增强测试套件的可重复性和稳定性。,https://github.com/vllm-project/vllm/issues/14499
vllm,该issue是一个功能需求，主要涉及LoRa槽位数量不足导致的错误。,https://github.com/vllm-project/vllm/issues/14495
vllm,该issue类型为用户提出需求，主要涉及的对象是vLLM中的 reasoning model。由于目前vLLM无法同时支持工具调用和推理，用户提出希望在推理步骤之后支持工具调用，以提高性能。,https://github.com/vllm-project/vllm/issues/14490
vllm,这是一个性能优化提案，涉及Eagle实现效率不高的问题。提议改用BatchPrefillAttention而不是BatchDecodeAttention来提高性能。,https://github.com/vllm-project/vllm/issues/14486
vllm,这是一个功能优化类型的issue，涉及到代码库中逐步引入ruffformat并逐渐废弃yapf和isort的过程，主要涉及到使用了特定目录选项的相关代码。,https://github.com/vllm-project/vllm/issues/14485
vllm,这是一个用户提出需求的issue，主要对象是新增支持多语言模型。,https://github.com/vllm-project/vllm/issues/14484
vllm,这个issue类型为功能需求，主要对象是RLHF文档，由于缺乏相关文档，用户提出需求添加RLHF文档以便后续完善。,https://github.com/vllm-project/vllm/issues/14482
vllm,这个issue类型是一个功能需求，主要对象是benchmark性能测试，用户提出了对性能分析的需求。,https://github.com/vllm-project/vllm/issues/14481
vllm,这是一个文档更新类型的issue，主要涉及到支持的模型列表的修改。这个问题是由于新增模型QwQ-32B导致需要更新文档，以确保在推理输出文档中正确列出已支持的模型。,https://github.com/vllm-project/vllm/issues/14479
vllm,该issue类型为文档更新，主要对象是关于Qwen模型工具调用的文档。,https://github.com/vllm-project/vllm/issues/14478
vllm,这是一个功能需求，该问题单主要涉及的对象是当前使用对象不包括推理令牌，导致用户希望在响应中包含推理令牌。,https://github.com/vllm-project/vllm/issues/14472
vllm,这是一个功能需求提出的issue，主要涉及的对象是V1引擎的可插拔调度器，由于当前无法在运行时覆盖调度器类，开发者希望能够让调度器支持插拔功能以便添加额外的约束。,https://github.com/vllm-project/vllm/issues/14466
vllm,这个issue是用户提出需求，希望最小化格式更改，以减少对文档的干扰。,https://github.com/vllm-project/vllm/issues/14461
vllm,该issue类型属于功能需求，涉及的主要对象是benchmark相关的数据集选项。这个需求是为了测试在不利情况下的性能，即每个请求使用唯一的jsonschema，避免使用基于模式缓存的优势。,https://github.com/vllm-project/vllm/issues/14457
vllm,这是一个优化问题，涉及到MoE权重填充，主要针对半精度类型进行优化，为提高性能而进行。,https://github.com/vllm-project/vllm/issues/14454
vllm,这是一个功能请求，主要涉及添加TP支持和移除一些未使用的训练功能。,https://github.com/vllm-project/vllm/issues/14453
vllm,这个issue类型是需求提出类型，涉及的主要对象是MLA（Machine Learning Accelerator），用户提出了将FlashMLA后端设置为默认选项的需求。,https://github.com/vllm-project/vllm/issues/14451
vllm,这个issue类型是关于性能改进的提议，主要涉及的对象是LLMengine中关于LoRA内存占用的计算方式。这可能是由于未考虑LoRA内存占用导致的性能问题而引发的讨论。,https://github.com/vllm-project/vllm/issues/14450
vllm,这是一个用户提出需求的issue，主要涉及的对象是针对VLLM项目中引入moewna16 Marlin核心进行优化。这个问题是由于已有的moe + marlin核心无法充分利用Marlin核心的性能优势，尤其在专家数量较多时，希望引入新的moewna16 Marlin核心来处理所有moe块并支持并行处理。,https://github.com/vllm-project/vllm/issues/14447
vllm,这是一个用户提出需求的类型。该问题涉及的主要对象是LoRA。由于当前未提供忽略层功能，用户希望添加一个忽略层来解决特定的问题。,https://github.com/vllm-project/vllm/issues/14445
vllm,该issue类型为用户提出需求，主要涉及对象是在PyCharm中运行/调试vllm。由于需要更友好的开发体验，用户希望能够在现代IDE中编辑/调试vllm。,https://github.com/vllm-project/vllm/issues/14444
vllm,这是一个用户提出需求的issue，主要涉及到PD分离支持前缀缓存功能。由于前缀缓存为True时，导致恢复seq_lens到实际的seq_lens值，避免因前一个文本命中时导致当前seq_lens值不准确的问题。,https://github.com/vllm-project/vllm/issues/14440
vllm,这个issue类型为提出需求，主要涉及对象为 vLLM 的文档提供。由于缺乏关于在训练场景中使用 vLLM 的文档，用户提出了添加培训文档标识到 TRL。,https://github.com/vllm-project/vllm/issues/14439
vllm,这是一个用户提出需求的issue，主要涉及了在多模态数据分析中控制数据的配置问题。由于目前配置选项不足，导致无法限制模型接受的输入类型或减少多模态数据的内存消耗。,https://github.com/vllm-project/vllm/issues/14438
vllm,这是一个用户提出需求的类型，主要涉及`torch.compile`的使用解释。原因可能是v1版本发布给公众之后需要说明如何使用这个功能。,https://github.com/vllm-project/vllm/issues/14437
vllm,这是一个关于性能优化的问题，并涉及VLLM在使用LoRA时推理速度变慢的Bug报告。由于LoRA的rank=256导致推理速度减慢4倍，用户寻求优化建议。,https://github.com/vllm-project/vllm/issues/14435
vllm,该issue为功能需求，主要涉及到了对speculative decoding配置和测试的重构。原因是为了使配置更具层次性和清晰性。,https://github.com/vllm-project/vllm/issues/14434
vllm,这个issue是一个性能优化的PR，主要涉及的对象是V1版本的ROCm（Triton）后端，由于对`chunked_prefill_paged_decode` op进行了一些优化，从而改善了对具有GQA的模型的处理，从而实现了更好的吞吐量表现。,https://github.com/vllm-project/vllm/issues/14431
vllm,该issue类型为功能需求提出，主要涉及工具选择和推理功能的同时使用问题，由于`enableautotoolchoice`和`enablereasoning`无法同时启用导致。,https://github.com/vllm-project/vllm/issues/14429
vllm,这个issue是关于用户提出需求，主要对象是如何支持通过PCW或其他prefill技术处理100万个prompt tokens输入，可能因为开发自动化任务编排的自主代理系统需要这样的功能。,https://github.com/vllm-project/vllm/issues/14425
vllm,这个issue是关于文档更新的需求，主要涉及了代码库中的prefix_caching.md文件，由于当前描述与示例图不符可能导致误解。,https://github.com/vllm-project/vllm/issues/14420
vllm,这是一个关于性能提升的建议，涉及 vllm 后端使用中的 GPU kv cache 利用率问题。,https://github.com/vllm-project/vllm/issues/14412
vllm,这是一个功能需求的issue，主要涉及到`Platform`类的`__getitem__`方法的添加，用户提出需要根据不同设备获取特定的流和事件，因此不再需要对`Stream`或`Event`进行抽象。,https://github.com/vllm-project/vllm/issues/14411
vllm,这个issue类型为需求提出，主要涉及的对象是Engine Args & Documentation。原因可能是出于对参数顺序和文档整洁性的改进需求。,https://github.com/vllm-project/vllm/issues/14409
vllm,这个issue是关于需求变更，主要涉及到移除引擎v1中所有与PA有关的内容。原因可能是决定不再支持prompt adapter，而导致需要进行相应修改。,https://github.com/vllm-project/vllm/issues/14408
vllm,这是一个功能需求类型的 Issue，主要对象是 VLLM 库。原因是用户希望通过设置环境变量来禁用 TQDM 日志，以减少日志输出量。,https://github.com/vllm-project/vllm/issues/14407
vllm,这是一个功能需求类型的issue，主要涉及的对象是GitHub仓库中的标签和自动化操作。,https://github.com/vllm-project/vllm/issues/14402
vllm,这是一个类型为更新请求的issue，主要涉及的对象是内核模块cutlass FP8 blockwise。由于cutlass内核需要更新到最新版本，因此提出了这个issue。,https://github.com/vllm-project/vllm/issues/14395
vllm,这个issue是用户提出需求类型的，主要对象是链接至 https://github.com/pytorch/ao/pull/1848/。由于用户需要具体的资源链接，可能是为了查看或参考相关内容。,https://github.com/vllm-project/vllm/issues/14393
vllm,这是一个用户提出需求的issue，主要涉及Neuron的KV cache更新，由于输入张量与缓存张量的布局不匹配，需要添加一个函数来处理这种布局不匹配的情况。,https://github.com/vllm-project/vllm/issues/14391
vllm,这是一个由用户提出的需求。该问题涉及的主要对象是VllmConfig。由于VllmConfig需要在`__init__`方法中被访问，因此需要将`apply_fp8_linear`和`apply_fp8_linear_generic`重构为对象。,https://github.com/vllm-project/vllm/issues/14390
vllm,这个issue是关于功能新增，涉及主要对象为LoRA类。由于需要新增DoRA支持，所以需要对现有代码做相应的修改和添加。,https://github.com/vllm-project/vllm/issues/14389
vllm,这个issue类型是改进建议，主要涉及V1版本的VLLM项目中如何及时移除已完成的请求。由于目前处理方式会导致额外的等待时间，需要提前处理已完成的请求以避免影响后续流程。,https://github.com/vllm-project/vllm/issues/14388
vllm,这个issue是一个撤销特定提交的请求，主要涉及VXE ISA for s390x架构的CPU推断功能的问题。,https://github.com/vllm-project/vllm/issues/14387
vllm,该issue属于用户提出需求类型，主要对象是vLLM引擎的engine arguments及其文档。原因是由于引擎参数列表过于混乱，随着功能增多，导致难以维护和使用，需要重新整理和分类。,https://github.com/vllm-project/vllm/issues/14386
vllm,该问题单属于性能优化类，主要涉及的对象是MLA（Machine Learning Accelerator），由于rotary embeddings导致了CPU overheads增加，需要减少。,https://github.com/vllm-project/vllm/issues/14384
vllm,这个issue属于功能需求类型，涉及的主要对象是为cutlass添加blackwell fp8 blockwise gemm支持。,https://github.com/vllm-project/vllm/issues/14383
vllm,该issue类型为用户提出需求，主要对象为vLLM，提出了关于是否可以集成QFilters来改进长文本支持和在受限硬件上使用更大模型的问题。,https://github.com/vllm-project/vllm/issues/14381
vllm,这是一个性能优化类的issue，主要涉及到qwen2-vl模块中的逻辑优化问题，导致了GPU和CPU数据传输造成的时间消耗增加。,https://github.com/vllm-project/vllm/issues/14377
vllm,这是一个优化性质的问题，相关内容主要涉及到Qwen2-vl中的逻辑优化，由于GPU张量被多次复制到CPU，导致CUDAMemcpyAsync时间消耗增加。,https://github.com/vllm-project/vllm/issues/14375
vllm,这是一个用户提出需求的类型，主要对象是如何从图片中获取嵌入结果，询问Qwen2.5-vl-7b能否实现此功能。,https://github.com/vllm-project/vllm/issues/14348
vllm,该issue类型为用户提出需求，主要涉及的对象是Ovis2 VLM series，用户寻求关于该系列性能表现的帮助。,https://github.com/vllm-project/vllm/issues/14346
vllm,这是一个需求类型的issue，主要涉及对象是添加Phi4-MM示例。,https://github.com/vllm-project/vllm/issues/14343
vllm,这个issue类型是用户提出需求，主要涉及的对象是eagle投机采样。用户希望增加对多模态模型的eagle投机采样。,https://github.com/vllm-project/vllm/issues/14337
vllm,这个issue类型属于功能需求提出，在Chat Completion Response中缺少`reasoning_tokens`的元数据。,https://github.com/vllm-project/vllm/issues/14335
vllm,这个issue类型是需求提出，主要涉及的对象是为vllm添加在单一节点上运行评估脚本。由于当前的文档需要更新，且针对两个节点的情况还未解决，用户提出此需求。,https://github.com/vllm-project/vllm/issues/14332
vllm,这是一个关于优化内核后使用优化的块大小的问题，类型为需求提出。问题涉及主要对象为代码中的块大小参数。,https://github.com/vllm-project/vllm/issues/14329
vllm,这是一个用户提出需求的issue，主要涉及对out-of-tree quantization method在cli args中无法加载的问题，由于加载插件后未进行quantization methods的验证导致新注册的quantization methods无法在验证时生效。,https://github.com/vllm-project/vllm/issues/14328
vllm,该issue属于用户提出需求类型，主要涉及vllm库中设置输入图像大小的问题。由于用户想要运行特定模型的推断，但不知道如何将其集成到vllm中，故向社区寻求帮助。,https://github.com/vllm-project/vllm/issues/14325
vllm,这是一个用户提出需求的issue，主要涉及输入构造方式是否会影响服务器性能，原因可能是单次请求仅发送一个图片和一个提示导致。,https://github.com/vllm-project/vllm/issues/14322
vllm,这个issue类型是用户提出需求，主要涉及对象是新模型的支持。用户提出了关于新模型QwQ-32B的支持问题。,https://github.com/vllm-project/vllm/issues/14321
vllm,这是一个功能需求的issue，主要涉及到vllm中关于CUDA的注意力后端以及错误消息显示的改进。原因是希望在出现错误时提供更具体的提示信息，改进错误类型和错误消息的表达方式。,https://github.com/vllm-project/vllm/issues/14320
vllm,该issue类型为功能需求反馈，主要对象为ray部署用户，提出了需要更多指导以解决ray部署问题的需求。,https://github.com/vllm-project/vllm/issues/14318
vllm,这是一个用户提出需求的 issue， 主要涉及 V1 engine 的测试内容，并希望新增一些测试用例。,https://github.com/vllm-project/vllm/issues/14315
vllm,这是一个功能需求的issue，主要对象是API端点`/is_sleeping`。原因是用户希望通过只读API检查引擎是否正在休眠。,https://github.com/vllm-project/vllm/issues/14312
vllm,该issue类型为用户提出需求，主要涉及的对象是vLLM引擎。由于缺乏能够检查引擎是否处于睡眠状态的只读API，用户提出了对引擎睡眠状态进行监测的需求。,https://github.com/vllm-project/vllm/issues/14311
vllm,该issue类型为代码审查和修改请求，主要涉及的对象是tpu_model_runner.py文件。由于https://github.com/vllmproject/vllm/pull/14098需要进行干净的合并，因此需要移除self.kv_caches。,https://github.com/vllm-project/vllm/issues/14309
vllm,这是一个功能需求类型的issue，该问题涉及到代码中的环境变量`VLLM_TEST_ENABLE_EP`的替换。原因是为了提供一个新的参数`enable_expert_parallel`来替代原有的环境变量设置。,https://github.com/vllm-project/vllm/issues/14305
vllm,这个issue类型是需求提出，主要涉及文档的结构和内容，用户提议将profiling移到性能部分考虑。,https://github.com/vllm-project/vllm/issues/14298
vllm,这是一个用户提出需求的issue，涉及到文档更新。由于缺少具体内容，用户提出了需要添加元幻灯片的需求。,https://github.com/vllm-project/vllm/issues/14297
vllm,该issue类型是功能增强，涉及的主要对象是vLLM中的ColQwen2VL模型。由于用户需要在vLLM中添加对ColQwen2VL模型的支持，以增加文档检索的能力。,https://github.com/vllm-project/vllm/issues/14291
vllm,这是一个用户提出需求的issue，主要涉及的对象是带有`AsyncLLMEngine`的`LLM`类，用户希望能够在`AsyncLLMEngine`中实现类似于`chat`方法的功能，或者将`generate`方法的`PromptType`扩展以支持更多的提示变体，例如聊天对话。,https://github.com/vllm-project/vllm/issues/14289
vllm,这是一个用户提出需求或建议的类型的issue，主要涉及的对象是external_launcher backend的文档。这个问题的原因是用户需要设置LOCAL_RANK来完成相关的操作。,https://github.com/vllm-project/vllm/issues/14288
vllm,该issue类型是功能需求，主要涉及对象是代码中的`interval`变量，用户提出了将`interval`重命名为`max_recent_requests`来增强代码可读性的建议。,https://github.com/vllm-project/vllm/issues/14285
vllm,这是一个功能需求的issue，主要涉及的对象是vllm中的benchmark功能。由于benchmark_serving.py只支持文本和图像输入，并且只使用openai chat作为后端，因此用户想了解是否支持音频输入到多模态并尝试用vllm的benchmark来测试多模态Qwen2Audio的吞吐量和延迟。,https://github.com/vllm-project/vllm/issues/14284
vllm,这个issue类型是需求提出，主要涉及的对象是文档内容。由于缺少对encoderdecoder的示例，需要为开发多模式处理器提供示例代码。,https://github.com/vllm-project/vllm/issues/14278
vllm,这是一个用户提出需求的issue，主要涉及的对象是在vLLM OpenAI服务器上运行时在Ray集群上部署指定节点的控制，由于当前Ray集成似乎受限于指定tp和pp，用户提出是否支持自定义放置组是可行的。,https://github.com/vllm-project/vllm/issues/14277
vllm,这是一个需求提出类型的issue，主要涉及的对象是Qwen2MoeForCausalLM模型。提出该需求的原因可能是希望添加对Qwen2MoeForCausalLM模型的moe调整支持。,https://github.com/vllm-project/vllm/issues/14276
vllm,这个issue是关于文档更新的，涉及修改`uv venv`指令的Python环境设置说明。这次修改是由于之前指南中使用了已存在的目录名`vllm`，导致在设置Python虚拟环境时出现错误。,https://github.com/vllm-project/vllm/issues/14273
vllm,这是一个关于优化并发请求处理的需求类issue，涉及主要对象为Qwen2VL模型在单个24GB GPU上的并发推理问题。主要问题是如何在单个实例上有效处理并发请求以及为何第二个请求的响应时间较长，以及如何进行优化。,https://github.com/vllm-project/vllm/issues/14226
vllm,这是一个关于用户提出需求的issue，主要涉及获取vllm中间输出的问题，用户希望了解如何在vllm中获取中间输出并保存在磁盘上。这可能由于缺乏文档或相关功能尚未明确支持而导致。,https://github.com/vllm-project/vllm/issues/14222
vllm,这是一个功能需求类型的issue，主要涉及到V1版本中的MultiprocExecutor以支持PP。,https://github.com/vllm-project/vllm/issues/14219
vllm,这个issue类型为文档更新，主要涉及到运行vLLM容器时应避免使用特权标志并指定目标GPU ID，以避免OOM问题。,https://github.com/vllm-project/vllm/issues/14217
vllm,该issue类型为用户提出需求，该问题涉及的主要对象为新增支持aya 32b vision模型。由于新增发布的aya vision 32b模型具有仅32b参数并达到qwen vl 2.5 72B的能力，用户提出希望支持该模型的需求。,https://github.com/vllm-project/vllm/issues/14216
vllm,这是一个需更新文档的问题，主要涉及到 Dockerfile 依赖镜像的更新。,https://github.com/vllm-project/vllm/issues/14215
vllm,这是一个关于新增模型支持的需求类型的issue，主要涉及的对象是PFN的plamo28b模型。用户提出希望vllm添加对该模型的支持，但未得到响应，并未说明难点所在，可能由于缺乏响应和未列明具体困难而引起。,https://github.com/vllm-project/vllm/issues/14214
vllm,这是一个用户提出需求的issue，主要涉及更新`compressed-tensors`来支持zero-points，可能由于当前功能不支持zero-points导致用户需要更新。,https://github.com/vllm-project/vllm/issues/14211
vllm,该issue类型为用户提出需求类型，主要涉及了vllm平台支持新模型nicolinho/QRM-Llama3.1-8B-v2。原因可能是希望vllm能够支持特定的模型，需求未得到响应。,https://github.com/vllm-project/vllm/issues/14208
vllm,这个issue类型是需求类型，主要对象是Kernel中的MoE tuning功能。这个问题被提出是因为在warmup运行时是否跳过慢速配置，可以将运行速度提高至少50%。,https://github.com/vllm-project/vllm/issues/14207
vllm,这是一个用户提出需求的issue，主要涉及如何记录请求令牌长度，可能是由于缺乏相关文档或信息导致用户产生的疑问。,https://github.com/vllm-project/vllm/issues/14206
vllm,这是一个用户提出需求的类型issue，涉及对象为qwen2修改的_sample函数在transformers中如何快速与vllm兼容。由于qwen2中的_sample函数被修改，用户需要寻求方法将其与vllm兼容，可能是由于vllm的接口或功能的差异导致兼容性问题。,https://github.com/vllm-project/vllm/issues/14204
vllm,"这是一个用户需求类型的issue，主要涉及到对如何在vllm中强制实现文本输出以""\n"" 开头进行讨论。可能出现的原因是用户想要在运行推理时与特定模型集成，但不清楚如何在vllm中实现。",https://github.com/vllm-project/vllm/issues/14201
vllm,这是一个类型为优化/改进的issue，涉及主要对象为软件依赖管理。由于numba不应该包含在cpu版本的vllm中，因此将其从通用依赖转移到cuda/rocm特定依赖中，以避免在cpu版本中引入不必要的依赖。,https://github.com/vllm-project/vllm/issues/14199
vllm,这个issue是一个文档更新类型的请求，涉及到vLLM项目中s390x架构的CPU实现。这个请求是由于需要更全面的文档来描述如何构建这一特定架构的CPU实现。,https://github.com/vllm-project/vllm/issues/14198
vllm,这个issue是用户提出需求类型的反馈，主要涉及的对象是vllm模型。用户希望支持新的模型deepseek-vl2，但未收到相应的回复。,https://github.com/vllm-project/vllm/issues/14192
vllm,这是一个功能需求报告，涉及vLLM中使用LLaMA 3.1指令模型时在ChatML格式下的提示格式问题。由于`tokenizer.chat_template`似乎未正确强制执行带有特殊标记`{role}\n\n{content}`的格式，导致多轮对话中提示格式未正确维护。,https://github.com/vllm-project/vllm/issues/14190
vllm,这是一个用户提出需求的issue，涉及主要对象是运行vllm进行推断的功能。用户想要输出的结果与输入顺序不一致，可能由于程序实现逻辑错误导致。,https://github.com/vllm-project/vllm/issues/14187
vllm,这是一个需求提出的issue，主要涉及RLHF用户和vLLM的worker类，由于即将迁移至V1版本，需要更改worker类继承方式。,https://github.com/vllm-project/vllm/issues/14185
vllm,该issue属于用户提出需求的类型，主要涉及的对象是vllm项目中的模型支持情况。由于InternVideo2.5是建立在InternVL2.5之上，用户希望vllm能够支持或实现OpenGVLab/InternVideo2_5_Chat_8模型。,https://github.com/vllm-project/vllm/issues/14180
vllm,该issue类型为用户提出需求，主要涉及DeepSeekR1W8A8模型的quantization方法不支持vLLM，用户希望得知是否有计划支持该模型。,https://github.com/vllm-project/vllm/issues/14176
vllm,这是一个用户提出需求的类型的issue，主要涉及Whisper语音识别模型是否会添加语言检测功能。由于当前代码只能执行转录任务且无法检测语言，用户请求添加语言检测功能。,https://github.com/vllm-project/vllm/issues/14174
vllm,这是一个性能优化的issue，主要涉及的对象是`IncrementalDetokenizer.update()`方法。原因是当前实现中逐个处理token会在处理多个token时效率变低。,https://github.com/vllm-project/vllm/issues/14173
vllm,该issue类型是优化提案，主要涉及token ID到token的转换；由于需要优化批处理操作，以提高效率。,https://github.com/vllm-project/vllm/issues/14172
vllm,这是一个特性请求，主要涉及修改torch版本至2.6.0。,https://github.com/vllm-project/vllm/issues/14171
vllm,这是一个功能需求的问题单，涉及到vllm项目中的`max_num_generation_tokens`参数，用户提出了要将其弃用的需求。,https://github.com/vllm-project/vllm/issues/14168
vllm,该issue是一个功能需求，主要涉及SLora中的LoRa适配器的热加载问题。,https://github.com/vllm-project/vllm/issues/14166
vllm,这个issue是一个Feature需求报告，涉及主要对象为VLLM的prompt adapter加载功能。由于目前VLLM只支持从固定路径读取prompt adapters，用户提出动态加载prompt adapters的需求。,https://github.com/vllm-project/vllm/issues/14163
vllm,这是一个功能需求的issue，主要涉及DeepSeek MTP的多步草稿支持，存在兼容性问题。,https://github.com/vllm-project/vllm/issues/14160
vllm,该问题属于用户需求类型，主要对象是前端的BatchRequestInput。原因是为了提供更便利的使用体验，增加了默认数值。,https://github.com/vllm-project/vllm/issues/14153
vllm,这个issue类型是性能优化，主要涉及V1 Triton（ROCm）后端的性能问题，由于Triton后端性能较差导致总吞吐量比FlashAttention基准低5倍，用户寻求帮助实现性能改进。,https://github.com/vllm-project/vllm/issues/14152
vllm,这个issue是一个功能需求提案，主要涉及vLLM进程与NUMA区域的关联，由于未指定CPU亲和性导致性能问题。,https://github.com/vllm-project/vllm/issues/14149
vllm,这个issue为需求提出，主要对象是V1版本的Metrics功能。由于缺少特定的Metrics（Tokens），用户提出了需要添加额外指标的要求。,https://github.com/vllm-project/vllm/issues/14148
vllm,这是一个用户提出需求的类型，该问题涉及的主要对象是`TransformersModel`。由于当前文档不够清晰，用户希望改进文档以提高该模块的可见性，并更清楚地解释其支持的内容以及远程模型编写者需要包含的内容。,https://github.com/vllm-project/vllm/issues/14147
vllm,这是一个特性请求(issue)，主要涉及到代码中使用`np.prod`导致的额外imports和计算开销较高问题。,https://github.com/vllm-project/vllm/issues/14144
vllm,这个issue是关于特性需求的，主要涉及到代码库中的某些操作，提出使用更轻量级的math.prod替代np.prod来减少导入和开销。原因是为了降低轻量级操作时的资源消耗。,https://github.com/vllm-project/vllm/issues/14142
vllm,这是一个功能需求类型的issue，主要涉及的对象是在vllm模型中添加了用于fp8稠密模型的DeepGEMM。由于缺少输出路径导致无法保存评估结果，用户希望获得建议。,https://github.com/vllm-project/vllm/issues/14140
vllm,这个issue类型是功能改进，主要涉及到对一些KV/prefix缓存指标的废弃，由于不再实现KV缓存卸载，导致需要废弃一些相关指标并引入新的计数器来取代。,https://github.com/vllm-project/vllm/issues/14136
vllm,这是一个关于优化度量指标的issue，主要涉及到vllm中重复的请求时间度量指标。原因是存在重复且数值较为无意义的度量指标，需要进行优化。,https://github.com/vllm-project/vllm/issues/14135
vllm,这个issue是一个功能改进，在vLLM项目中涉及使用`HFCompatibleLoRA`替代实例方法替换，修复了原来打印模型或检查层时返回HFCompatibleX而非原始层类型的问题。,https://github.com/vllm-project/vllm/issues/14131
vllm,这个issue是关于功能需求的，主要涉及的对象是vLLM在ARM CPU上的INT8量化支持。由于vLLM默认情况下采用通道级量化策略，而ACL库不支持通道级量化，导致ACL内核无法在INT8模型上运行。,https://github.com/vllm-project/vllm/issues/14129
vllm,这个issue类型属于用户提出需求/问题类型，主要涉及vllm版本升级时精度损失的问题，用户想了解如何确定这些损失是否在可接受范围内。,https://github.com/vllm-project/vllm/issues/14128
vllm,该issue类型为用户提出需求，请教问题，主要涉及如何在vllm中运行特定模型的推断集成问题。原因是用户不清楚如何将模型与vllm集成。,https://github.com/vllm-project/vllm/issues/14127
vllm,这是一个功能需求的issue，主要涉及的对象是具有AMD GPU（MI300X）的用户。,https://github.com/vllm-project/vllm/issues/14125
vllm,这个issue属于功能需求提出类型，主要涉及到如何使用model_redirect来将模型名称重定向到本地文件夹，用户希望能够通过模型名称代替硬编码的模型路径进行本地模型的引用。,https://github.com/vllm-project/vllm/issues/14116
vllm,这是一个用户提出需求的issue，主要涉及vLLM中的Qwen2.5-VL模型是否支持批处理多个视频的问题。用户想要了解Qwen2.5VL是否已经支持批处理多个视频，以及如何启用这一功能，或者是否有计划在未来扩展vLLM以支持批量视频输入。,https://github.com/vllm-project/vllm/issues/14112
vllm,这是一个用户提出需求的类型的issue，涉及的主要对象是在使用speculative decoding时指定设备的参数。由于部署在特定设备上的speculative模型可能会减少interGPU通信并减少对主模型的影响，因此提出了这一需求。,https://github.com/vllm-project/vllm/issues/14107
vllm,这是一个用户提出需求的issue，主要对象是baichuaninc/BaichuanOmni1.5模型的支持，原因是请求该模型的集成。,https://github.com/vllm-project/vllm/issues/14104
vllm,这个issue是一个功能增强的类型，主要涉及到前端代码中的KV缓存数据统计，由于未提供与`lmcache`指标的集成，导致无法获取完整的统计数据。,https://github.com/vllm-project/vllm/issues/14101
vllm,这是一个需求类的issue，涉及主要对象是model runner。由于kv_caches在model中不再需要，导致需要对代码进行调整以删除不必要的代码。,https://github.com/vllm-project/vllm/issues/14098
vllm,这是一个实现功能的issue，主要涉及kv_cache_manager中的sliding window attention功能。原因是要支持真正的滑动窗口以及前缀缓存需求。,https://github.com/vllm-project/vllm/issues/14097
vllm,这是一个需求提出类型的issue，主要涉及的对象是`tests/distributed/test_comm_ops.py`文件中的测试用例。由于目前测试用例只支持CUDA设备，未能兼容其他设备，用户提出需要使测试用例兼容更多设备。,https://github.com/vllm-project/vllm/issues/14091
vllm,这是一个用户提出需求的类型的issue，主要涉及到vllm中的Speculative decoding与Pipeline Parallelism的兼容性问题，用户想要在R1上同时运行PP和MTP，可能由于某些原因导致无法实现。,https://github.com/vllm-project/vllm/issues/14089
vllm,这个issue是关于功能（Feature）的，主要对象是前端（Frontend），提出了是否要废弃`--enable-reasoning`选项的讨论。,https://github.com/vllm-project/vllm/issues/14088
vllm,这是一个用户提出需求的issue，主要涉及对象是如何在仅有CPU的环境下运行DeepSeek-R1-Distill-Qwen-32B模型作为离线批处理推理，并需要提供Hugging Face cache directory、Hugging Face token以及max tokens等参数。这个问题的症状是用户不清楚如何在LLM类或其初始化函数中设置这些参数。,https://github.com/vllm-project/vllm/issues/14087
vllm,这个issue类型是文档改进，主要涉及源代码构建的克隆步骤，用户提出了对构建步骤的补充。,https://github.com/vllm-project/vllm/issues/14086
vllm,该issue类型为功能增强，主要涉及添加Neuron设备通信功能。,https://github.com/vllm-project/vllm/issues/14085
vllm,该issue类型为文档更新，主要涉及到Kubernetes部署指南的修改。由于前文提到的问题，导致了一些具体项目的引用被移除，以及一些观点性内容的删除，需要更新为更中立和官方的指南。,https://github.com/vllm-project/vllm/issues/14084
vllm,这是一个用户提出需求的issue，主要涉及改善VLLM V1在初始化过程中出现常见错误的日志记录，由于当前异常的日志记录不够清晰，希望能够更明确地捕获这些问题并返回更清晰的错误消息。,https://github.com/vllm-project/vllm/issues/14083
vllm,这是一个关于提出需求（feature request）的issue，主要涉及StatsLogger接口的更新和日志统计任务的优化。,https://github.com/vllm-project/vllm/issues/14082
vllm,这是一个用户提出需求的类型，主要涉及VLLM系统中的预填响应功能。用户希望能够通过预填响应来控制生成的输出。,https://github.com/vllm-project/vllm/issues/14080
vllm,这是一个需求变更的issue，涉及主要对象为KVCacheConfig的重构，由于需要改变KVCacheSpec的含义以及调整KVCacheConfig的保存逻辑，导致需要对KVCacheConfig进行重新设计和调整。,https://github.com/vllm-project/vllm/issues/14079
vllm,这是一个需求提出类型的issue，主要对象是vllm库中的LogitsProcessor，由于无法获取每个序列的唯一标识，导致无法在自定义LogitsProcessor中维护每个序列的状态。,https://github.com/vllm-project/vllm/issues/14078
vllm,这是一个关于文档更新的issue，涉及主要对象是使用 reasoning model with stream 的用户，提出了关于如何在OpenAI库中使用的问题。,https://github.com/vllm-project/vllm/issues/14077
vllm,这个issue类型为用户提出需求，并询问vLLM MLA使用的缓存类型是KV Cache还是C Cache；主要对象是vLLM MLA的缓存系统；用户问题可能是由于缓存系统不清楚或文档不明确而引发的。,https://github.com/vllm-project/vllm/issues/14072
vllm,这是一个用户提出需求的issue，主要涉及V1版本的TritonAttention在Nvidia GPU上的支持。由于V1 ROCm attention后端已实现为Triton，在Nvidia GPU上也可以用于triton内核开发。,https://github.com/vllm-project/vllm/issues/14071
vllm,这个issue是关于文档更新的，主要涉及对象是论据输出流示例。由于DeltaMessage不支持自定义字段，之前使用requests而非OpenAI客户端，现有贡献者建议使用OpenAI客户端来解决这个问题。,https://github.com/vllm-project/vllm/issues/14070
vllm,这是一个功能需求的issue，主要涉及的对象是模型调优过程中的fp8块量化支持。由于当前实现的fp8调优不考虑块量化，导致在调优DeepSeekV3模型时未考虑重量尺度，这个问题提出了添加对块量化支持的需求。,https://github.com/vllm-project/vllm/issues/14068
vllm,这是一个用户提出需求的issue，主要涉及前端（Frontend）功能的改进。由于需求不同，希望能够通过请求参数来控制token显示类型，以适应不同情况的服务需求。,https://github.com/vllm-project/vllm/issues/14066
vllm,这是一个功能需求的issue，主要涉及的对象是ROCmAttentionBackend类。原因是V1 GPUModelRunner支持其他元数据结构，需要添加get_builder_cls方法。,https://github.com/vllm-project/vllm/issues/14065
vllm,这个issue类型为功能需求提出，涉及主要对象为vllm的release tag，用户提出了通过RELEASE.md来触发带有``vX.Y.Zrc``版本号的release构建。,https://github.com/vllm-project/vllm/issues/14064
vllm,这是一个功能需求类型的issue，主要涉及模型加载权重的时间准确记录，因为生成器的惰性特性导致下载阶段与加载权重阶段的时间精确统计存在困难。,https://github.com/vllm-project/vllm/issues/14063
vllm,这个issue是关于缺少某个参数的文档内容更新，属于文档改进类型，主要涉及的对象是优化文档。这个问题可能由于缺乏`pipeline_parallel_size`参数的文档说明，导致用户无法了解该参数的使用和作用。,https://github.com/vllm-project/vllm/issues/14059
vllm,这是一个用户提出需求的issue，主要涉及的对象是DeviceCommunicatorBase。由于需要将reduce_scatter功能从一个特定的组件中分离出来，用户认为这是一个很好的改进建议。,https://github.com/vllm-project/vllm/issues/14057
vllm,该issue属于需求提出类型，主要对象是为GPTQModel添加文档。这可能是因为用户希望将GPTQModel作为ModelCloud.AI用户完全支持的另一种GPTQ模型量化选项。,https://github.com/vllm-project/vllm/issues/14056
vllm,这个issue类型是用户提出需求，请求功能添加，主要涉及的对象是文档整理和示例代码。,https://github.com/vllm-project/vllm/issues/14050
vllm,这个issue是关于代码修改建议的，主要涉及到`InputPreprocessor`和`MultimodalProcessor`，可能是为了优化代码结构而提出的问题。,https://github.com/vllm-project/vllm/issues/14049
vllm,这是一个关于改进功能的issue，主要涉及到对多模态内容格式的全面交错支持。由于当前方法导致了在'string'聊天模板内容格式中所有图像标记都堆叠在提示的开头，可能会对某些模型的性能产生负面影响，因此用户提出了需要开启全面交错支持的选项。,https://github.com/vllm-project/vllm/issues/14047
vllm,这是一个功能需求类型的issue，主要对象是在VLLM项目中添加 likaixin/InstructCoder 作为 spec decode 基准数据集选项。这个需求由于希望添加新的数据集以用于性能基准测试，并优化 Triton Kernels 上的 rejection sampler。,https://github.com/vllm-project/vllm/issues/14045
vllm,这个issue类型是用户提出需求，主要涉及的对象是Deepseek的部署配置。由于block_size无法被32整除，导致在32个GPU上启用PP后不支持speculative decoding。,https://github.com/vllm-project/vllm/issues/14044
vllm,"这是一个文档更新类型的issue，主要涉及""AutoAWQ""文档的更新。由于之前的修改未能完全覆盖到指定的更改内容，因此需要进一步更新文档。",https://github.com/vllm-project/vllm/issues/14042
vllm,该issue类型为需求提出，主要涉及性能基准数据集的整合，并说明了如何更新和优化数据集以及相关的代码文件。,https://github.com/vllm-project/vllm/issues/14036
vllm,这是一个用户提出需求的类型，该问题涉及的主要对象是使用vllm部署DeepSeek模型的输出缺失<think>标签问题。可能由于集成vllm时的问题导致这种症状的bug。,https://github.com/vllm-project/vllm/issues/14028
vllm,这是一个功能需求报告，主要涉及benchmark_serving.py中的随机输入和sharegpt数据集。由于随机输入在VLLM客户端和SGLang客户端之间存在差异，导致某些模型表现不佳，用户提出了添加新功能以解决这一差距。,https://github.com/vllm-project/vllm/issues/14026
vllm,该issue是关于性能优化的需求，主要涉及vLLM下Qwen-0.5B模型在Nvidia H20 GPU上生成吞吐量不佳的问题。,https://github.com/vllm-project/vllm/issues/14023
vllm,这个issue类型是功能需求，主要对象是benchmark_serving.py文件。由于在比较在线服务性能时观察到随机输入的差异，用户提出了添加来自sharegpt数据集的随机输入以尽量缩小这种差距的需求。,https://github.com/vllm-project/vllm/issues/14020
vllm,这个issue属于用户提出需求，主要涉及的对象是移动多模态嵌入API示例到在线服务页面。原因是为了消除当前在多模态输入页面中列出的与嵌入输入相关的混乱。,https://github.com/vllm-project/vllm/issues/14017
vllm,这是一个用户提出需求的issue，主要对象是需要在基准测试中添加一个与代码相关的数据集，以展示ngram speculative decoding的主要用例和优势。,https://github.com/vllm-project/vllm/issues/14013
vllm,该issue类型为用户提出需求，主要涉及对象为将KVCache存储在3FS中。由于没有收到额外的上下文信息，用户提出了将KVCache存储在3FS中的新功能需求。,https://github.com/vllm-project/vllm/issues/14012
vllm,该issue是关于用户需求的，主要对象是DeepSeek v3/r1 MTP支持PP功能，用户询问是否有计划在MTP上支持PP，尚未得到回应。,https://github.com/vllm-project/vllm/issues/14005
vllm,这是一个用户需求类型的issue，主要涉及到vllm是否支持在多节点环境下使用前缀缓存的功能。原因可能是用户想要在多节点中推理DeepSeekR1671B时如何共享kv缓存。,https://github.com/vllm-project/vllm/issues/14004
vllm,这个issue类型是用户提出需求，主要涉及的对象是V1 Engine，用户提出了关于实现并发部分预填的需求。,https://github.com/vllm-project/vllm/issues/14003
vllm,该issue属于需求提出类型，主要涉及实现V1引擎中的优先级调度功能，用户希望在V1版本中支持请求优先级，由于当前版本（V0）尚未支持此功能，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/14002
vllm,这个issue类型是功能需求，涉及主要对象是在ROCm环境下启用 AITER ASM MoE。,https://github.com/vllm-project/vllm/issues/14001
vllm,这是一个关于优化建议的类型，涉及的主要对象是代码审查工具 codespell 缓存未正确生成，导致某些拼写错误未被预设的代码检查挂钩捕获。,https://github.com/vllm-project/vllm/issues/14000
vllm,这个issue类型是用户提出需求，主要对象是vllm0.7.3版本内置的pytorch不支持nvidia5080，导致无法在本地部署deepseek14b。,https://github.com/vllm-project/vllm/issues/13999
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是使Deepseek在ROCm平台上启用AITER ASM MoE功能。,https://github.com/vllm-project/vllm/issues/13998
vllm,这个issue属于功能变更请求，主要涉及的对象是vLLM中的`best_of`抽样参数。由于用户和行业对此功能的使用逐渐减少，导致该功能在即将发布的vLLM V1版本中被取消支持。,https://github.com/vllm-project/vllm/issues/13997
vllm,这是一个用户提出需求的issue，主要涉及添加`vllm bench` CLI命令。由于目前仅支持`vllm bench serve`，用户希望在后续的PR中添加其他benchmark模式。,https://github.com/vllm-project/vllm/issues/13993
vllm,这是一个用户提出需求的issue，主要涉及的对象是V1用户指南。,https://github.com/vllm-project/vllm/issues/13991
vllm,"这是一个需求讨论的issue，主要涉及V1 rejection sampler中的acceptance rate, accepted tokens等指标的定义和跟踪。",https://github.com/vllm-project/vllm/issues/13990
vllm,该issue是关于需求提出类型，主要涉及的对象是TPU worker。由于TPU profiler在长时间的 profiling 过程中会丢失部分数据，用户提出了需要实现`start_profile`和`stop_profile`方法来控制 profiling 会话以及在benchmark_serving.py中启用 profiling 的需求。,https://github.com/vllm-project/vllm/issues/13988
vllm,这是一个用户提出需求的issue，主要涉及更新XGrammar至0.1.14版本。原因是用户想在生产环境中使用更加功能丰富的CFG语法。,https://github.com/vllm-project/vllm/issues/13986
vllm,这是一个用户提出需求的类型的issue，涉及主要对象是添加AMD Quark文档。由于目前缺少AMD Quark文档，用户希望能够补充相关内容。,https://github.com/vllm-project/vllm/issues/13984
vllm,该issue类型为代码优化建议，主要涉及的对象是GPU Model Runner的代码。由于存在冗余的类型注解，用户建议进行清理以提升代码质量和可读性。,https://github.com/vllm-project/vllm/issues/13983
vllm,这是一个功能需求的issue，主要涉及支持TPU中的标准V1 Sampler。由于topk/topp在TPU上运行缓慢，需要找到更快的方法来执行抽样操作。,https://github.com/vllm-project/vllm/issues/13982
vllm,这是一个需求提出的issue，主要涉及的对象是VLM（Visual Language Modeling）模型的legacy输入映射器。,https://github.com/vllm-project/vllm/issues/13979
vllm,这是一个用户提出需求的issue，涉及的主要对象是BlockTable类。由于需要准备混合内存分配器以支持混合模型，希望通过移除特定参数和调整参数顺序来优化BlockTable类的操作。,https://github.com/vllm-project/vllm/issues/13977
vllm,该issue属于需求提出类型，涉及的主要对象是ROCm的AITER kernels。,https://github.com/vllm-project/vllm/issues/13975
vllm,这是一个API优化类型的issue，涉及主要对象为`KVCacheManager`，旨在减少代码重复，为新的KVCacheManager引入混合模型准备。,https://github.com/vllm-project/vllm/issues/13973
vllm,这是一个提出新功能需求的issue，主要对象是 CUTLASS grouped gemm fp8 MoE kernel实现。由于用户需要在Benchmark中测试该功能的性能表现，于是提出了这个issue。,https://github.com/vllm-project/vllm/issues/13972
vllm,这个issue类型为技术改进，涉及主要对象为更新Python 3.8的typing使用。,https://github.com/vllm-project/vllm/issues/13971
vllm,这个issue是一项改进提议，主要涉及到`.pre-commit-config.yaml`文件中的重复项处理。原因是替换每个`repo`内的`exclude`为顶层的`exclude`。,https://github.com/vllm-project/vllm/issues/13967
vllm,这个issue是一个功能提升类的问题，主要涉及VLM（Visual Language Model）项目中的PromptUpdate功能更新。,https://github.com/vllm-project/vllm/issues/13964
vllm,这是一个功能需求的issue，主要涉及支持Triton中的非注意力路径操作，包括自定义操作和非自定义操作。,https://github.com/vllm-project/vllm/issues/13963
vllm,这是一个关于用户需求的issue，主要涉及使用DeepSeek MTP针对Distil模型进行推理的问题，用户不清楚如何将其与vllm集成。,https://github.com/vllm-project/vllm/issues/13960
vllm,该issue属于功能需求的提出，涉及对象为vLLM项目中的模型定义。由于部分模型不兼容vLLM V1版本，需要添加`SupportsV0Only`协议以便可以程序化检查兼容性。,https://github.com/vllm-project/vllm/issues/13959
vllm,这个issue类型为改进建议，主要对象是项目的快速入门文档，由于缺少对`uv run`的使用说明，可能导致用户误解创建环境的操作方式。,https://github.com/vllm-project/vllm/issues/13958
vllm,这是一个用户提出需求的issue，主要涉及前端相关内容。由于尚未支持图片嵌入，用户提出希望在OpenAI API中支持图片嵌入的功能。,https://github.com/vllm-project/vllm/issues/13955
vllm,该issue为需求类型，主要涉及添加Jump-Forward支持到Guided Decoding，由于当前使用FSM在进行引导解码时只能一次转换一个标记状态，导致解码速度慢，作者希望引入JumpForward解码以提高速度。,https://github.com/vllm-project/vllm/issues/13953
vllm,这是一个功能需求的issue，主要涉及前端实时请求并发跟踪，用户提出了关于跟踪服务负载的需求。,https://github.com/vllm-project/vllm/issues/13950
vllm,这是一个功能需求类型的issue，主要涉及VLLM_TEST_ENABLE_EP环境变量的处理方式。,https://github.com/vllm-project/vllm/issues/13947
vllm,这是一个需求提议，主要涉及的对象是CI/Build系统。由于需要在触及examples目录的PR上添加标签，开发者提议将该目录标注为可通过`mergify`进行标记，以增强PR的可管理性。,https://github.com/vllm-project/vllm/issues/13944
vllm,这个issue类型是用户提出需求，涉及的主要对象是vLLM模型定义。由于需要检查模型与vLLM V1的兼容性，因此提出了添加`SupportsV1`协议的需求。,https://github.com/vllm-project/vllm/issues/13943
vllm,这个issue属于功能需求提出，主要涉及的对象是模型的量化支持，用户提出了需要支持 deepseek v3/r1 w8a8 int8 block-wise quantization 的需求。,https://github.com/vllm-project/vllm/issues/13942
vllm,这是一个用户提出需求的 issue，主要是希望为vllm创建一个特定的Docker镜像，包括`audio`和`video`功能，以便用户可以直接从Docker Hub拉取使用。,https://github.com/vllm-project/vllm/issues/13940
vllm,这是一个功能需求的issue，主要对象是vllm命令行工具，需添加一个帮助命令以列出可用的CLI命令及其简要说明。,https://github.com/vllm-project/vllm/issues/13938
vllm,这是一个用户提出需求的issue，主要涉及到对新模型Phi-4 Multimodal Instruct的支持。由于开发者没有回应，用户想知道为什么无法支持他们想要的模型。,https://github.com/vllm-project/vllm/issues/13936
vllm,该issue属于一个功能性需求，主要涉及在使用不均匀分配GPU的情况下支持张量并行计算，存在精度问题可能由于attention heads数量不能被GPU数量整除。,https://github.com/vllm-project/vllm/issues/13934
vllm,这是一个功能需求提出的issue，主要涉及V1版本的vllm中的spec decode功能。由于V1只支持ngram，用户提出希望支持随机采样的需求，但目前spec decode功能与top_p、top_k采样不兼容。,https://github.com/vllm-project/vllm/issues/13933
vllm,这是一个功能需求提议，主要涉及使用DeepGemm的grouped gemm kernel来进行fused MoE操作。,https://github.com/vllm-project/vllm/issues/13932
vllm,这是一个关于代码改进的issue，主要涉及V1版本中的EP/TP MoE + DP Attention功能的实现。,https://github.com/vllm-project/vllm/issues/13931
vllm,这个issue类型为需求提出，主要对象是添加RELEASE.md文档。,https://github.com/vllm-project/vllm/issues/13926
vllm,这个issue是一个功能需求提案，主要涉及的对象是V1版本的AsyncLLM，提出了针对数据并行性能优化的新特性需求。因为目前需要在多节点环境下对请求进行负载均衡以提高性能，所以提出了一些关于多节点支持和请求调度优化的建议。,https://github.com/vllm-project/vllm/issues/13923
vllm,这个issue类型是功能增强（Feature Enhancement），主要对象是日志输出系统。原因是为了通过loguru库实现输出JSON格式的日志，同时更新相关文档。,https://github.com/vllm-project/vllm/issues/13920
vllm,这是一个需求类型的issue，主要涉及的对象是要为DeepGEMM和vLLM Block FP8 Dense GEMM添加基准测试。,https://github.com/vllm-project/vllm/issues/13917
vllm,这是一个用户提出需求的issue，主要涉及要添加对Magma模型的支持，其原因是为了优化Magma推断性能和实现快速执行。,https://github.com/vllm-project/vllm/issues/13913
vllm,这是一个用户提出需求的issue，主要涉及的对象是whisper endpoint，用户希望在whisper endpoint中添加更多的抽样参数来提升功能。,https://github.com/vllm-project/vllm/issues/13910
vllm,这个issue类型是用户提出需求，针对的主要对象是vLLM的OpenAI兼容服务器，其提出了支持在Unix域套接字上进行绑定的功能请求。,https://github.com/vllm-project/vllm/issues/13907
vllm,这个issue是一个特性需求的提出，主要涉及的对象是T5Model，用户希望使用vLLM来进行T5 reranker。,https://github.com/vllm-project/vllm/issues/13903
vllm,该issue属于功能讨论类型，涉及主要对象为vllm中的paged attention机制。由于分块计算的paged attention不是精确的attention机制，存在attention scores和不为1的情况，用户讨论是否可以将paged attention称为精确的attention。,https://github.com/vllm-project/vllm/issues/13900
vllm,这个issue类型是用户提出需求，涉及主要对象是VLLM与External Launcher TorchRUN的集成，导致由于无法设置CUDA_VISIBLE_DEVICES而需要解决不同world sizes下VLLM与trainer的协作问题。,https://github.com/vllm-project/vllm/issues/13896
vllm,这个issue类型是功能特性提出，主要涉及支持cutlass 3.8。由于特征、动机和pitch的改变，需要支持blackwell和新版本的cuda，因此需要修改flash attention。,https://github.com/vllm-project/vllm/issues/13893
vllm,这个issue类型是优化建议，主要涉及的对象是模型的嵌入（embedding model）。由于模型参数过多，导致性能较低，因此提议使用更小的嵌入模型以提高性能。,https://github.com/vllm-project/vllm/issues/13891
vllm,这是一个用户提出需求的类型，该问题涉及到访问`kv_cache`和`attn_metadata`，由于缺少相关注释，用户希望在代码中添加注释以便更好地理解相关功能。,https://github.com/vllm-project/vllm/issues/13887
vllm,这个issue类型是需求提出，主要涉及对象是在Ampere架构GPU上运行DeepSeek R1 FP8模型，由于Marlin不支持块状的fp8导致无法部署DeepSeekR1 fp8模型。,https://github.com/vllm-project/vllm/issues/13885
vllm,这个issue是用户提出需求的类型，主要涉及支持Deepseek的DeepGemm MoE，用户想了解MoE操作符是否会采用Deepseek开源的DeepGemm操作符。,https://github.com/vllm-project/vllm/issues/13879
vllm,这个issue类型为性能优化，涉及的主要对象是KVCacheManager._get_cached_block。由于性能优化优化KVCacheManager cached block dict的检索方式，以提高性能。,https://github.com/vllm-project/vllm/issues/13878
vllm,该issue属于用户提出需求类型，涉及到在部署模型时设置最大输入字符串长度、最大输入标记数以及最大输出长度的问题。用户表达了对如何使用vllm进行推理的不了解，并希望获得相关集成指导。,https://github.com/vllm-project/vllm/issues/13874
vllm,这是一个用户提出需求的类型，主要涉及支持Microsoft/MAGMA-8B模型的问题。由于未得到回应，用户关注该模型何时能被支持。,https://github.com/vllm-project/vllm/issues/13870
vllm,这个issue属于用户提出需求类型，主要涉及的对象是新增模型RapidOCR。由于未得到回应，用户希望了解为何当前支持的最接近的模型无法满足需求。,https://github.com/vllm-project/vllm/issues/13868
vllm,这个issue属于用户需求，涉及VLLM的Attention模块，用户希望使用Flash MLA来获得特定结果。,https://github.com/vllm-project/vllm/issues/13867
vllm,"这个issue类型是用户提出需求，主要对象是引入新模型""silero-vad""。通过没有得到回应，可能是由于一些原因导致无法支持该模型。",https://github.com/vllm-project/vllm/issues/13866
vllm,该issue类型为新增模型需求，主要对象是要在项目中加入一个OCR模型。由于需求添加OCR模型，可能是为了实现文字识别的功能。,https://github.com/vllm-project/vllm/issues/13862
vllm,该issue属于功能需求类型，主要对象是vllm中的代理负载平衡功能。用户提出了希望vllm支持类似lmdeploy库中代理命令的功能，以实现更高的吞吐量和更方便的模型实例管理。,https://github.com/vllm-project/vllm/issues/13861
vllm,这是一个用户提出需求的类型，主要涉及的对象是在使用TPU时希望能够使用torch2.6 whl包。原因可能是当前环境无法正常运行torch2.6版本或者缺少相关支持，导致用户寻求解决方案。,https://github.com/vllm-project/vllm/issues/13860
vllm,这是一个代码优化类型的issue，主要涉及对象是代码中的变量赋值，由于重复赋值导致代码冗余。,https://github.com/vllm-project/vllm/issues/13859
vllm,这是一个用户提出需求的问题，主要涉及的对象是vLLM，用户在询问是否有计划或可能的途径与DeepGEMM集成/合作，以优化GEMM操作。,https://github.com/vllm-project/vllm/issues/13857
vllm,这个issue类型是特性需求，主要涉及到vLLM进程在DGX机器上固定到正确的NUMA区域。原因是希望提升GPU的H2D和D2H性能的可预测性。,https://github.com/vllm-project/vllm/issues/13855
vllm,这是一个需求请求，涉及主要对象为HPU，请求启用AutoGPTQ/AutoAWQ量化模型推断功能。,https://github.com/vllm-project/vllm/issues/13853
vllm,这个issue类型是改进提案，主要涉及对象是CMakeLists.txt文件，用户提出需要统一CUTLASS版本以避免重复。,https://github.com/vllm-project/vllm/issues/13846
vllm,这是一个特性需求的issue，主要涉及的对象是为AMD Navi 3x/4x GPU添加自定义分页注意力内核。由于Navi架构与MI架构的差异，需要针对每种架构添加新的内核。,https://github.com/vllm-project/vllm/issues/13843
vllm,这是一个用户提出需求的issue，主要涉及的对象是关于 Kubernetes 部署文档。,https://github.com/vllm-project/vllm/issues/13841
vllm,这是一个用户提出需求的 issue，主要对象是为 vllm 添加 CLI 命令进行基准测试，由于目前的 vllm 命令只支持 `servecomplete`，用户希望添加类似基准测试的命令以提高功能。,https://github.com/vllm-project/vllm/issues/13840
vllm,这是一个需求提出的issue，主要涉及pipeline分区优化，由于层数不均匀分布导致的计算平衡问题。,https://github.com/vllm-project/vllm/issues/13839
vllm,这个issue类型是性能优化，涉及主要对象是XGrammarLogitsProcessor数据结构，由于深拷贝操作引起添加请求时的性能开销过大。,https://github.com/vllm-project/vllm/issues/13837
vllm,该issue类型为需求提出，主要涉及的对象是集成层，用户提出需要支持使用生成的block FP8矩阵乘法内核的w8a8_block_fp8_matmul支持，并且介绍了性能比较以及问题的具体原因。,https://github.com/vllm-project/vllm/issues/13835
vllm,这是一个功能需求提出类型的issue，主要涉及对象是DeepSeek V2/V3/R1中的`lm_head`，用户提出的需求是为了在pipeline ranks之间平衡内存使用，仅在最后一个rank上放置`lm_head`。,https://github.com/vllm-project/vllm/issues/13833
vllm,这是一个功能增强的issue，涉及主要对象是支持cuda 12.8.0和SBSA wheels，由于需要增强兼容性和安装过程，对操作系统和架构进行了更新，并改进了CUDA安装脚本。,https://github.com/vllm-project/vllm/issues/13830
vllm,这是一个提出需求的 issue，涉及的主要对象是 triton 3.3.0，用户请求支持新的 GPUs（显卡）。,https://github.com/vllm-project/vllm/issues/13829
vllm,这个issue类型是用户提出需求，主要涉及对象为vLLM，请求添加对ColBERT family of models的支持。,https://github.com/vllm-project/vllm/issues/13827
vllm,这是一个关于功能使用的 issue，涉及主要对象为 MLA（Machine Learning Assisted）模型。原因是在特定环境下禁用了前缀缓存，导致了该问题。,https://github.com/vllm-project/vllm/issues/13822
vllm,该issue类型为性能优化建议，主要涉及解码性能问题，提出了两种优化建议以提高吞吐量。原因是由于接收键、值和隐藏状态导致阻塞计算，从而影响执行模型的效率。,https://github.com/vllm-project/vllm/issues/13816
vllm,这是一个功能需求的issue，涉及主要对象是模型加载器（Model Loader）。原因可能是为了解决在加载庞大模型时从本地下载耗时的问题，希望支持直接从远程KV存储中加载模型。,https://github.com/vllm-project/vllm/issues/13809
vllm,这是一个用户提出需求的issue，主要涉及的对象是Model中的Speculative Decoding功能。由于当前MTP仅支持k=1，用户希望扩展支持k > 1，以实现更灵活的解码选项。,https://github.com/vllm-project/vllm/issues/13805
vllm,这是一个用户提出需求的issue，主要涉及DeepEP功能是否能被整合到vllm，由于缺乏反馈，用户正在寻求相关帮助。,https://github.com/vllm-project/vllm/issues/13804
vllm,这个issue是关于用户提出需求的，主要涉及调度器状态监控中的metric添加，用于追踪并组合运行中和等待中的请求数量，以提供更全面的调度器状态视图。,https://github.com/vllm-project/vllm/issues/13799
vllm,这是一个功能增强的请求，主要涉及VLLM中添加Cutlass支持到Blackwell FP8 Gemm。,https://github.com/vllm-project/vllm/issues/13798
vllm,这个issue类型是功能增强请求，涉及到VLLM项目中的模型支持问题，用户提出了对Grok1模型的一些功能改进的需求。,https://github.com/vllm-project/vllm/issues/13795
vllm,这是一个优化性质的问题单，涉及到mrope计算的性能优化。原因是为了改善mrope计算的速度，采用了预先计算torch.arange以及矢量化的方法。,https://github.com/vllm-project/vllm/issues/13793
vllm,这是一个用户提出需求的issue，主要涉及V1 engine的parallel sampling功能，由于并行抽样请求输出不包含指标，导致用户希望并行抽样请求输出包含指标。,https://github.com/vllm-project/vllm/issues/13792
vllm,"这个issue类型是需求提出， 主要涉及对象是项目中的""custom_cache_manager""。由于使用的torch 2.6.0版本集成了triton 3.2.0，因此需要移除掉""custom_cache_manager""。",https://github.com/vllm-project/vllm/issues/13791
vllm,这个issue是关于功能需求的，主要涉及Zero-copy tensor/ndarray的序列化和传输问题，由于msgpack无法原生处理递归类型的数据，在处理Image data时需要自定义序列化逻辑，从而实现直接传输数据而不需要复制。,https://github.com/vllm-project/vllm/issues/13790
vllm,这个issue类型是用户提出需求。该问题涉及的主要对象是为MLA添加支持加载压缩张量格式权重的功能。,https://github.com/vllm-project/vllm/issues/13787
vllm,这是一个用户提出需求的类型，主要涉及的对象是cache guided decoding logits processor。由于缺乏缓存引导解码的logits处理器，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/13781
vllm,这是一个优化性质的issue，主要涉及的对象是关于并行采样的实现方式。该问题由于在高层次抽象中实现并行采样导致代码复杂且需要大量的后处理工作，建议在层次创建`RequestOutput`对象的层次来实现并行采样以减少开销和复杂性。,https://github.com/vllm-project/vllm/issues/13774
vllm,这个issue类型是功能需求，主要涉及的对象是为TransformersModel添加LoRA支持，用户提出需求并表示需要添加单元测试和更新文档。,https://github.com/vllm-project/vllm/issues/13770
vllm,这是一个功能需求类型的issue，主要涉及的对象是`TransformersModel`。增加LoRA支持是为了添加对LoRA相关的功能支持，可能是为了实现更高级的功能或优化模型性能。,https://github.com/vllm-project/vllm/issues/13768
vllm,这是一个用户提出需求的issue，主要涉及DeepSeek-V3工具解析器的支持，由于用户期望DeepSeek工具解析器的支持，因此提出此需求。,https://github.com/vllm-project/vllm/issues/13764
vllm,该issue是一个功能改进提议，主要涉及前端代码，用于优化vLLM的BatchLLM性能。原因是当前的隐式共享前缀识别方法在离线场景下无法充分利用共享前缀，导致性能下降。,https://github.com/vllm-project/vllm/issues/13762
vllm,该issue是一个功能需求问题，主要涉及对象是vllm支持在多类型GPU上部署模型。这个问题的提出可能是因为用户想要利用不同类型的GPU来提高模型的性能或灵活性。,https://github.com/vllm-project/vllm/issues/13760
vllm,这是一个feature请求类型的issue，主要涉及对Whisper中beam search的实现。缺少了beam procedure的退出条件，速度优化和编码部分的重计算问题。,https://github.com/vllm-project/vllm/issues/13758
vllm,这是一个用户提出需求的issue，涉及主要对象是VLMs与transformers backend的支持。这个问题由于需要对transformers进行一些更改，以支持VLMs在Transformers后端运行。,https://github.com/vllm-project/vllm/issues/13754
vllm,这是一个功能改进类型的issue，主要对象是vllm中的spec_decode部分。该改进是为了消除spec_decode中对CUDA的硬编码，以便更容易地让其他设备使用该功能。,https://github.com/vllm-project/vllm/issues/13746
vllm,这个issue是关于用户提出需求，主要涉及vllm是否会支持FlashMLA，提出希望未来版本的vllm能够支持DeepSeek开源的FlashMLA技术，以优化大型transformer模型的推断任务效率。,https://github.com/vllm-project/vllm/issues/13744
vllm,这个issue属于功能更新类型，主要涉及到azure/setuphelm工具的版本升级。原因是为了添加新功能、更新依赖关系和改进文档。,https://github.com/vllm-project/vllm/issues/13742
vllm,这是一个功能优化类型的issue，涉及前端代码，针对的主要对象是vLLM中的请求排序。由于在离线情况下有许多具有不同共享前缀的请求，并且现有方法不能充分利用共享前缀，因此提出了添加前缀排序作为BatchLLM优化的先导步骤的需求。,https://github.com/vllm-project/vllm/issues/13740
vllm,这个issue是文档更新类型，主要涉及将`pythonjsonlogger`的存储库从已存档的`madzak/pythonjsonlogger`更新为维护良好的`nhairs/pythonjsonlogger`，由于原存储库已归档，需要更新以确保项目信息的准确性。,https://github.com/vllm-project/vllm/issues/13739
vllm,这是一个需求提出类的issue，主要对象是vllm下的spec_decode功能。此问题由于CUDA硬编码存在，导致无法在其他设备上轻松使用。,https://github.com/vllm-project/vllm/issues/13737
vllm,这个issue是关于代码改进的，主要涉及到Benchmarking工具的功能改进和修复。原先存在的问题是TPOT计算累积平均值而不是每个请求的数值，这可能导致性能指标计算不准确，因此需要进行修复。,https://github.com/vllm-project/vllm/issues/13736
vllm,这个issue是用户提出需求的类型，主要涉及支持FlashMLA功能，由于需要该功能的原因而提出。,https://github.com/vllm-project/vllm/issues/13735
vllm,该issue类型是代码优化，主要涉及的对象是`EngineArgs.create_engine_config`方法。由于原方法代码冗余导致可读性较差，需要将其内的杂项方法移动至辅助方法以提高代码可读性。,https://github.com/vllm-project/vllm/issues/13734
vllm,这是一个功能需求提议，主要涉及到模型权重加载过程中的量化方法的初始化属性。由于属性的位置需要提前，以便直接被量化方法使用。,https://github.com/vllm-project/vllm/issues/13733
vllm,这是一个需求修改类型的issue，主要涉及Speculative Decoding中拒绝采样API的更改。由于需要支持随机采样，需要修改拒绝采样的API。,https://github.com/vllm-project/vllm/issues/13729
vllm,"这个issue是关于如何使用""AsyncLLMEngine""来处理单个实例上的多个（并发）请求，而不是加载""qwen2-vl-7b""。这个问题涉及到如何处理多个请求并发的情况，可能是为了提高处理效率或避免资源浪费。",https://github.com/vllm-project/vllm/issues/13728
vllm,该issue类型为功能需求，主要涉及CI环境变量设置和模型路径逻辑，用户提出需要在S3 CI环境变量开启时将模型路径更改为S3路径。,https://github.com/vllm-project/vllm/issues/13727
vllm,这是一个需求问题，主要涉及 V1 功能启用，用户提出了关于 V1 特性启用的需求。,https://github.com/vllm-project/vllm/issues/13726
vllm,这个issue类型是用户提出需求，主要涉及VLLM支持增量KV缓存的功能。用户提出了希望在ASR模型提供部分转录时减少重新计算整个输入序列以降低生成延迟的需求。,https://github.com/vllm-project/vllm/issues/13723
vllm,这是一个功能需求的issue，主要涉及的对象是在启用MLA时无法使用前缀缓存。因为MLA深度搜索模型可能与前缀缓存存在兼容性问题，用户提出是否可以解决这个问题。,https://github.com/vllm-project/vllm/issues/13720
vllm,这个issue类型是需求提出，主要涉及的对象是改进DSv3在AMD GPU上的性能。,https://github.com/vllm-project/vllm/issues/13718
vllm,这是一个需求提出类型的issue，主要涉及到V1 Workers的WorkerBase类定义。这个issue的提出源于当前代码中存在的重复部分，需要统一处理以减少重复代码。,https://github.com/vllm-project/vllm/issues/13711
vllm,这是一条用户需求类型的issue，涉及到项目中的Quant功能。这个问题是为了添加`SupportsQuant`接口到`BaiChuanBaseForCausalLM`，以及解决阻塞CI的linting问题。,https://github.com/vllm-project/vllm/issues/13710
vllm,这是一个改进类型的issue，主要涉及到benchmark_serving.py文件中被废弃的`--dataset`参数。,https://github.com/vllm-project/vllm/issues/13708
vllm,该issue属于用户需求，主要涉及V1 LLMEngine和AsyncLLM类的功能增加。,https://github.com/vllm-project/vllm/issues/13705
vllm,该issue类型为用户提出需求，涉及的主要对象是添加对DeepSeek-R1-Distill-Qwen-32B工具调用支持，由于目前工具调用和推理功能无法同时启用，用户希望添加对两者的支持。,https://github.com/vllm-project/vllm/issues/13700
vllm,这个issue属于功能增强类型，主要涉及到了对`FakeAttentionMetdata`的引入，用于dummy run操作。,https://github.com/vllm-project/vllm/issues/13689
vllm,这是一个性能优化类型的issue，主要涉及使用IPC（域套接字）ZMQ套接字来进行本地通信。,https://github.com/vllm-project/vllm/issues/13688
vllm,这是一个用户提出需求的issue，主要涉及的对象是MLA后端。这个问题的原因是用户希望能够选择Triton FA作为MLA后端。,https://github.com/vllm-project/vllm/issues/13685
vllm,这个issue属于功能需求类型，主要对象是添加新的模型GPTBigCodeForEmbedding支持token span分类。,https://github.com/vllm-project/vllm/issues/13684
vllm,这是一个新增功能需求的issue，主要关注于添加一个新模型 GPTBigCodeForEmbedding 以支持 token span 分类。其中涉及的主要对象是模型和相关代码文件。,https://github.com/vllm-project/vllm/issues/13681
vllm,这是一个用户提出需求的issue，主要涉及的对象是如何在LLM中使用自定义模型，可能由于环境配置或版本兼容性问题导致用户遇到困难。,https://github.com/vllm-project/vllm/issues/13680
vllm,该issue属于用户提出需求类型，主要涉及支持LoRA for Pooling Models，由于当前vLLM不支持LoRA for Pooling models，用户想了解如何添加支持以及需要注意哪些问题。,https://github.com/vllm-project/vllm/issues/13679
vllm,"这是一个用户提出需求的issue，主要涉及的对象是weights loading功能。这个issue的提出是为了增加对权重加载时间的记录，以提高透明度。
",https://github.com/vllm-project/vllm/issues/13666
vllm,这个issue类型是用户提出需求，主要对象是支持Google SigLip 2模型。由于没有得到回应，用户在寻求如何支持所需的模型。,https://github.com/vllm-project/vllm/issues/13663
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm项目的CPU推断功能。由于用户的机器是Xeon E5-2670v2，只支持AVX指令集，因此希望修改代码以支持AVX。,https://github.com/vllm-project/vllm/issues/13662
vllm,这是一个功能改进类的Issue，涉及的主要对象是models和layers。这个问题的原因是由于代码中硬编码了CUDA，导致其他设备无法轻松使用。,https://github.com/vllm-project/vllm/issues/13658
vllm,这是一个用户提出需求的类型issue，主要涉及到对LLM.embed函数中应用的归一化以及如何修改的问题。用户想要获取最后一个隐藏状态而不进行归一化。,https://github.com/vllm-project/vllm/issues/13652
vllm,这是一个关于文档贡献的建议类型的issue，主要涉及项目贡献指南中完整的CI运行说明，可能由于在过去的PR中的观察，希望增加特定信息。,https://github.com/vllm-project/vllm/issues/13646
vllm,这个issue类型为功能需求提议，涉及主要对象是vllm项目。,https://github.com/vllm-project/vllm/issues/13643
vllm,这个issue是关于提出需求的，主要涉及到torchrun的兼容性问题。最终导致这个问题的原因是需要在同一进程中运行engine和LLM类以满足RLHF框架的需求，同时降低调度不确定性。,https://github.com/vllm-project/vllm/issues/13642
vllm,这是一个用户提出需求的issue，主要涉及的对象是生成过程中是否能够插入用户想要的token。由于用户希望在生成多个token时插入特定的token，可能是出于对模型功能增强的需求。,https://github.com/vllm-project/vllm/issues/13639
vllm,这是一个需求类型的issue，主要涉及到V1版本的PP（Pipeline chunked prefill）功能。,https://github.com/vllm-project/vllm/issues/13638
vllm,这个issue是关于技术改进的，主要涉及到V1版本的VLLM项目中关于预先填充数据块调度的问题，由于更新`num_computed_tokens`的位置不当导致无法保守地调度请求，因此需要调整位置并优化实现。,https://github.com/vllm-project/vllm/issues/13637
vllm,这个issue属于用户提出需求类型，该问题涉及的主要对象是VLLM项目的支持多个模型在同一GPU上运行。由于VLLM目前不支持在同一GPU上同时运行多个模型，用户提出了关于如何支持多个模型在同一GPU上运行的需求。,https://github.com/vllm-project/vllm/issues/13633
vllm,这是一个功能需求类型的issue，主要涉及的对象是增强Ultravox模型以接受超过30秒的音频输入。由于目前模型无法处理长于30秒的音频并且无法批量处理音频样本，用户提出了更新模型以支持长音频、批处理音频样本，并需要相应更新HF上的处理器以验证变化。,https://github.com/vllm-project/vllm/issues/13631
vllm,这是一个用户提出需求的 issue，涉及主要对象是DeepSeek MTP 实现，由于原始实现无法处理 `k > 1`，所以提出了扩展实现以支持更多 speculative tokens 的功能。,https://github.com/vllm-project/vllm/issues/13626
vllm,这是一个优化类型的issue，主要涉及到了深度神经网络模型中的内存优化问题，由于长序列长度导致OOM问题。,https://github.com/vllm-project/vllm/issues/13625
vllm,这是一个需求提出的issue，该问题单涉及的主要对象是项目中的compressed-tensors模块。,https://github.com/vllm-project/vllm/issues/13619
vllm,这个issue类型是功能需求提出，主要涉及的对象是在V1 TPU上添加通过Ray实现的tensor parallel支持。由于需求需要在不改变SMPD和Ray编译标志的情况下实现，这可能导致了一些潜在的执行或性能方面的问题。,https://github.com/vllm-project/vllm/issues/13618
vllm,这是一个关于优化建议（feature request）的issue，主要涉及使用pre-commit来更新`requirements-test.txt`文件，目的是自动运行`pip compile`。,https://github.com/vllm-project/vllm/issues/13617
vllm,该issue类型为用户提出需求，主要对象是vllm模型支持。原因是用户在提出新模型支持的请求时未收到响应。,https://github.com/vllm-project/vllm/issues/13616
vllm,这是一个性能优化需求，主要涉及到测试用例的并行执行问题，原因是当前的测试用例执行时间长且消耗资源过多。,https://github.com/vllm-project/vllm/issues/13613
vllm,这是一个用户提出需求的issue，主要涉及编译过程中的并行任务限制，用户希望去除对NVCC_THREADS和MAX_JOBS的限制，以提高编译加速度。,https://github.com/vllm-project/vllm/issues/13606
vllm,这是一个代码优化类的Issue，主要涉及宏参数重命名和一致性调整。原因为确保代码库中参数命名的一致性，提高代码可读性和可维护性。,https://github.com/vllm-project/vllm/issues/13605
vllm,这个issue是用户提出需求类型的，主要涉及vllm的使用和集成问题。用户想要了解在特定模型推断过程中如何集成vllm，可能由于缺乏相关知识或指导而希望得到帮助。,https://github.com/vllm-project/vllm/issues/13604
vllm,"这个issue类型是需求提出，该问题单涉及的主要对象是V1版本的vllm模块，由于需求让 ""speculative decoding configuration"" 变得像编译配置一样具有层次结构。",https://github.com/vllm-project/vllm/issues/13601
vllm,这个issue是一个提出需求的类型，主要涉及的对象是`memory_profiling`模块的内存管理函数。由于需要将内存管理函数重构至Platform并在每个XxxPlatform子类中实现，以使`memory_profiling`具备跨平台性。,https://github.com/vllm-project/vllm/issues/13599
vllm,这是一个用户需求类型的issue，主要涉及如何在Docker环境下使用benchmark测试vllm性能的问题。这个问题可能由于缺乏官方docker脚本导致无法使用benchmark而产生。,https://github.com/vllm-project/vllm/issues/13598
vllm,"这是一个性能优化的issue，涉及的主要对象是""MROPE计算""，问题在于试图通过创建更高效的向量化代码来优化计算速度时，但是由于创建张量/np.array的开销大于优化带来的性能提升。",https://github.com/vllm-project/vllm/issues/13595
vllm,这是一个版本更新的类型，用户要求将 transformers 版本升级到 v4.49.0。,https://github.com/vllm-project/vllm/issues/13592
vllm,这是一个功能需求问题，涉及数据并行通信的设置。由于需要探索数据并行在深度学习和moe模型中的应用，因此在这个PR中创建了必要的通信频道，但还未设计最终的用户界面。,https://github.com/vllm-project/vllm/issues/13591
vllm,这是一个功能需求的issue，主要涉及集成 torchao 作为一个量化选项到 vllm 中。这个需求由于需要新增支持torchao作为一种量化选项，对性能进行测试以及与现有量化方法进行对比而产生。,https://github.com/vllm-project/vllm/issues/13588
vllm,该issue属于性能优化类型，涉及的主要对象是ROCm下的Mi300混合处理器。由于配置设置不佳，可能导致性能表现不佳，用户提出了更新更好的配置设置以优化性能。,https://github.com/vllm-project/vllm/issues/13577
vllm,这是一个功能需求报告，主要涉及改进使用mo_wna16核作为CompressedTensorsWNA16MoEMethod的后端。由于当前Marlin MoE核对于存在许多小专家的情况不够高效，因此希望将mo_wna16核作为默认版本应用于特定情况。,https://github.com/vllm-project/vllm/issues/13575
vllm,这是一个需求类型的issue，主要涉及在CI中为AMD添加AWS凭证。,https://github.com/vllm-project/vllm/issues/13572
vllm,这个issue是关于软件功能的改进类型的，涉及主要对象为实现nvfp4 cutlass gemm的功能。由于Cutlass 3.8尚未正式发布，导致该功能的实现需要使用占位函数进行构建。,https://github.com/vllm-project/vllm/issues/13571
vllm,该issue是关于增加Quantization测试模型到S3列表的改进提议，属于新功能需求类型，涉及的主要对象是测试用的模型。,https://github.com/vllm-project/vllm/issues/13570
vllm,该问题单属于技术优化，主要涉及使用uv python替代ppa:deadsnakes/ppa来安装Python，因为部分Python apt命令在某些PR上出现超时现象。,https://github.com/vllm-project/vllm/issues/13569
vllm,这是一个功能需求，该问题主要涉及 HTTP 服务器的模型参数设置。由于每次请求都需要指定模型名称，导致服务端与客户端之间的耦合性过高，因此希望将模型参数设置为可选项，当未提供参数时，默认使用服务器上的第一个可用模型。,https://github.com/vllm-project/vllm/issues/13568
vllm,这是一个用户提出需求的类型的issue，主要涉及VLLM使用在线服务器的分类任务支持。由于目前仅支持离线推断，希望能够在Kubernetes中托管VLLM在线推断服务器。,https://github.com/vllm-project/vllm/issues/13567
vllm,这是一个改进建议类型的issue，主要涉及的对象是Dockerfile。由于使用uv库，构建时间从33分钟改进到了28.5分钟。,https://github.com/vllm-project/vllm/issues/13566
vllm,这是一个功能改进类型的issue，主要涉及的对象是 ROCm 的 MI300A 编译目标。原因是移除了 gfx940 和 gfx941 目标，推荐使用 MI300X 的 gfx942 目标。,https://github.com/vllm-project/vllm/issues/13560
vllm,这是一个与代码优化相关的issue，主要涉及到模型定义中未使用的参数的清理，原因是为了简化模型代码并避免进一步的复杂性。,https://github.com/vllm-project/vllm/issues/13555
vllm,这是一个文档改进类的issue，主要涉及到vLLM的用户使用文档。由于文档误导，导致用户在使用性能分析时出现错误。,https://github.com/vllm-project/vllm/issues/13554
vllm,这是用户提出需求类型的issue，主要涉及Facebook账户密码丢失或遭到盗用问题，可能由于虚假信息或网络安全问题导致。,https://github.com/vllm-project/vllm/issues/13541
vllm,这个issue类型为用户需求提出，主要涉及对象是openai api。由于用户需求开发团队支持在openai协议api中添加 image_embeds 并通过提出的方案支持prefix caching，以提高传递效率。,https://github.com/vllm-project/vllm/issues/13540
vllm,这个issue属于功能需求类型，主要涉及Qwen2.5VL的在线推断中无法通过OpenAI接口传递`mm_processor_kwargs`参数，由此导致无法准确计算`second_pre_grid_t`值的问题。,https://github.com/vllm-project/vllm/issues/13533
vllm,该issue为用户提出需求，并请求vllm支持facebook/contriever模型，主要涉及对象为新增模型支持。由于该模型在社区中较为流行，用户希望vllm能够支持该模型，以增加功能和使用体验。,https://github.com/vllm-project/vllm/issues/13525
vllm,这个issue是关于用户需求的，主要涉及到vllm是否支持在GPU和CPU上混合部署，并提出了希望在GPU和CPU之间分配权重的功能需求。,https://github.com/vllm-project/vllm/issues/13517
vllm,这是一个需求提出类的issue，主要涉及性能基准测试的前置准备问题。,https://github.com/vllm-project/vllm/issues/13509
vllm,该issue类型是需求反馈，主要涉及部署优化建议，由于存在低GPU KV缓存使用率、低运行请求和高挂起请求，可能由于超参数设置不合理导致。,https://github.com/vllm-project/vllm/issues/13508
vllm,这是一个功能增强类型的问题，主要涉及API Server中端口号范围验证，可能是为了防止用户输入错误的端口号导致程序出错。,https://github.com/vllm-project/vllm/issues/13506
vllm,这是一个新功能需求类型的 issue，主要涉及添加了ROCm下的两种不同的 Mixture of Experts (MoE) configurations。,https://github.com/vllm-project/vllm/issues/13503
vllm,这是一个用户提出需求的类型，主要涉及DeepSeek MTP在V1架构中的支持。由于新V1架构的变动，用户希望该功能能够迁移过来。,https://github.com/vllm-project/vllm/issues/13500
vllm,这是一个用户提出需求的issue，主要涉及V1 rejection sampler的扩展支持随机采样功能。由于目前只支持贪婪采样，用户希望能够实现随机采样。,https://github.com/vllm-project/vllm/issues/13499
vllm,这是一个优化建议的issue，主要涉及到V1版本的拒绝采样器的性能问题。,https://github.com/vllm-project/vllm/issues/13498
vllm,这是一个用户提出需求的issue，主要涉及vLLM在工具调用方面的支持问题，用户想知道是否可以根据工具调用的输出来实现自动完成功能。,https://github.com/vllm-project/vllm/issues/13497
vllm,这是一个需求性的issue，主要涉及的对象是HTTP服务器的SSL Key Rotation。由于某些生产环境需要定期轮换TLS密钥/证书，因此实现了使用watchfiles异步监视ssl密钥、证书和CA文件的更新，并在检测到更改时更新SSLContext的功能。,https://github.com/vllm-project/vllm/issues/13495
vllm,这个issue类型是用户提出需求，询问问题，主要涉及到在部署到Azure托管机器学习实时端点时如何编写评分脚本，以及如何与vllm集成。造成这个问题的原因是用户不清楚如何正确编写评分脚本以接收OpenAI API兼容调用。,https://github.com/vllm-project/vllm/issues/13491
vllm,这个issue类型是需求提出，主要涉及的对象是改进关于截断在嵌入和得分任务中的使用体验。原因是为了控制嵌入和得分函数中的截断参数，并增进开发者体验。,https://github.com/vllm-project/vllm/issues/13489
vllm,这是一个关于性能优化的issue，主要涉及实现专家缓存策略。由于复杂的FusedMOE行为和底层内核的问题，导致实现上遇到困难。,https://github.com/vllm-project/vllm/issues/13486
vllm,这个issue类型是需求提出，主要对象是添加Qwen2.5模型的工具解析器，用户提出需要支持特定的Qwen2.5模型。,https://github.com/vllm-project/vllm/issues/13484
vllm,该issue是关于需求提出的，主要涉及前端实现中的Tool Calling API，用户提出支持`tool_choice='required'`的功能。,https://github.com/vllm-project/vllm/issues/13483
vllm,这是一个关于增加功能的issue，主要涉及V1版本的sampler，由于缺少allowed_token_id支持而需要添加这一功能。,https://github.com/vllm-project/vllm/issues/13481
vllm,这个issue属于改进优化类型，主要涉及构建和依赖关系的定制化处理。由于当前构建设置混乱，使用了不同方法和途径处理构建依赖，导致需求不清晰且过时方法使用，因此需要通过定制构建后端和动态构建依赖解决这些问题。,https://github.com/vllm-project/vllm/issues/13480
vllm,这个issue类型是功能需求，主要涉及的对象是kv cache int8，用户提出了关于在线量化kv cache int8计算的需求。,https://github.com/vllm-project/vllm/issues/13478
vllm,这个issue属于用户需求类型，主要涉及的对象是TPU后端支持的模型列表，用户想要了解vllm中支持的模型。,https://github.com/vllm-project/vllm/issues/13476
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是实现真正的PP（ParameterServer）与Ray执行程序之间的结合。,https://github.com/vllm-project/vllm/issues/13472
vllm,这个issue类型是用户提出需求，并涉及到如何评估模型和加速卡性能以及如何使用vllm进行推理。由于环境不同，用户想了解如何在benchmark性能测试中区分不同方法所带来的影响。,https://github.com/vllm-project/vllm/issues/13467
vllm,这是一个用户提出需求的issue，主要涉及到前端支持转录API并具有语言检测功能。由于之前的代码对于未知语言的音频进行识别时限制为英文，因此用户提出希望支持未知语言的音频识别，这个issue意在引入语言检测功能来解决这个问题。,https://github.com/vllm-project/vllm/issues/13465
vllm,这是一个用户提出需求的类型的issue，该问题涉及的主要对象是设置服务中不同请求的最大像素限制。由于Qwen2VL/Qwen2.5VL需要动态处理像素，用户希望能够设置不同请求的最大像素限制。,https://github.com/vllm-project/vllm/issues/13463
vllm,这是一个功能需求的issue，主要涉及Whisper transcription API endpoint的更新，涉及语言识别功能的临时解决方案。,https://github.com/vllm-project/vllm/issues/13461
vllm,这是一个用户提出需求的类型issue，主要涉及支持xTTSv2，可能是由于当前不支持tts模型（encoderdecoder v1 model）导致用户寻求相应的帮助。,https://github.com/vllm-project/vllm/issues/13457
vllm,该issue是关于代码重构和测试覆盖率改进的，不是bug报告。主要涉及Neuron内核测试的模块化和测试逻辑的改进。,https://github.com/vllm-project/vllm/issues/13455
vllm,这个issue属于功能需求类型，主要涉及的对象是模型推理支持问题。由于MultiBatch推理时`finished_requests_ids`问题导致的bug。,https://github.com/vllm-project/vllm/issues/13454
vllm,该issue为用户提出需求，询问如何在离线推理中使用vllm的pipeline并提供具体示例。,https://github.com/vllm-project/vllm/issues/13453
vllm,这个issue是用户提出需求类型，主要涉及JSON schema中的enum属性。由于缺乏对enum属性的处理，导致需要添加轮廓的回退方案。,https://github.com/vllm-project/vllm/issues/13449
vllm,这是一个用户提出需求类的issue，涉及对象为DeepSeek-R1-UD-IQ1_S(1.58bit-guff)，由于gpu设备不够，导致无法支持该需求。,https://github.com/vllm-project/vllm/issues/13447
vllm,这是一个用户提出需求的问题，主要涉及vLLM的GPU内存使用情况，用户想了解在拥有足够GPU内存的情况下，是启动单个vLLM实例更好还是启动多个实例进行负载平衡更好。,https://github.com/vllm-project/vllm/issues/13442
vllm,这是一个用户提出需求的issue，主要涉及到添加对Ovis VLM系列模型的支持。这个问题的原因是现有系统不兼容vLLM实现，导致用户无法加载模型。,https://github.com/vllm-project/vllm/issues/13441
vllm,这是一个关于性能优化和参数调整的issue，涉及主要对象是MLA核心功能在AMD GPUs上的性能改进与MI300X上的MoE配置调整。,https://github.com/vllm-project/vllm/issues/13439
vllm,这是一个用户提出需求的 issue，主要涉及的对象是 vllm 库和 torch 包。导致这个 issue 的原因是 vllm v0.7 版本似乎只支持 torch==2.5.1，用户希望能够兼容 torch==2.6。,https://github.com/vllm-project/vllm/issues/13434
vllm,这个issue是一个功能需求的提出，主要涉及的对象是Jamba项目中的Scores和Weights，用户提出了需要为Scores和Weights添加显式的fp32精度的需求。,https://github.com/vllm-project/vllm/issues/13432
vllm,这是一个功能需求，主要涉及对象是JambaForSequenceClassification模块，用户提出希望在load_weights函数中将权重明确转换为float32的需求。,https://github.com/vllm-project/vllm/issues/13430
vllm,这是一个用户提出需求的issue，主要涉及vllm的安装环境和安装方式。用户询问如何防止重复提交issue以及如何在线获取答案。,https://github.com/vllm-project/vllm/issues/13426
vllm,这个issue类型是功能需求，涉及主要对象是v1 LLMEngine，由于需求是实现v1 LLMEngine的并行采样支持。,https://github.com/vllm-project/vllm/issues/13421
vllm,这个issue是关于需求提出的，主要涉及V1中LLMEngine不支持并行采样，可能由于此功能缺失导致了用户的需求。,https://github.com/vllm-project/vllm/issues/13420
vllm,这是一个用户提出需求的类型，主要对象是vLLM v1 async engine，用户希望在AsyncLLM中实现并行采样的支持。,https://github.com/vllm-project/vllm/issues/13419
vllm,这个issue属于功能需求类型，主要涉及的对象是AriaVisionTransformer和AriaTextModel，用户提出了添加SupportsQuant接口的需求。,https://github.com/vllm-project/vllm/issues/13416
vllm,这是一个关于功能需求的issue，涉及到在VLLM v1中添加prompt logprobs支持及与APC兼容性的问题。由于prompt logprobs和APC之间的兼容性挑战，导致了需要重新设计实现选择。,https://github.com/vllm-project/vllm/issues/13414
vllm,这是一个需求类型的issue，主要涉及的对象是github项目vllm下的xgrammar库。由于需要将xgrammar更新至0.1.13版本以支持Python 3.13，这是为了适配https://github.com/vllmproject/vllm/pull/13164，因此提出了这个issue。,https://github.com/vllm-project/vllm/issues/13411
vllm,这个issue是一个功能需求，主要涉及prompt logprobs的支持问题，由于APC缓存机制与prompt logprobs的兼容性挑战，导致了请求失败的问题。,https://github.com/vllm-project/vllm/issues/13409
vllm,这是一个功能提案的issue，主要涉及vllm引擎在崩溃时输入元数据转储功能的添加。导致此功能提案的原因是之前实现的功能被移除，现在希望通过不同的方法来提供更多信息以帮助调试生产环境中的崩溃。,https://github.com/vllm-project/vllm/issues/13407
vllm,这是一个关于功能需求的issue，主要涉及的对象是vLLM项目。由于当前版本尚不支持token级别的时间戳，用户提出了希望在whisper模型中支持token级别时间戳的功能需求。,https://github.com/vllm-project/vllm/issues/13400
vllm,这是一个关于功能需求的issue，主要涉及vllm模型加载时是否需要显式传递解析器参数的讨论。用户建议是否可以提供一个默认解析器，在自动选择工具时能够自动识别json格式的标记。,https://github.com/vllm-project/vllm/issues/13399
vllm,这个issue类型是需求提出，主要涉及的对象是vllm服务器。用户想要拒绝请求当vllm服务器繁忙时，可能由于未找到相关指导或问题而寻求帮助。,https://github.com/vllm-project/vllm/issues/13395
vllm,该issue为用户需求类型，主要涉及多任务支持的实现。用户提出了希望在vllm中实现Whisper的多任务功能，例如语言检测和转录，希望能够为转录功能添加语言检测或额外参数。,https://github.com/vllm-project/vllm/issues/13390
vllm,该issue类型是用户提出需求，主要涉及的对象是关于vLLM下新推出的推理优化方案，用户询问关于引入新的分布式系统LLM推理优化方案的计划和方法。,https://github.com/vllm-project/vllm/issues/13386
vllm,这是一个功能需求的issue，主要涉及Connector API的迁移工作，用户希望将V0的连接器API迁移到V1，同时介绍了最新更改并提供了运行示例的方式。,https://github.com/vllm-project/vllm/issues/13385
vllm,这是一个升级请求，主要涉及 CPU backend 到 torch-2.6，等待代码审查。原因是为了升级到 torch-2.6.0 版本。,https://github.com/vllm-project/vllm/issues/13381
vllm,这是一个需求提出类型的issue，主要涉及的对象是在TPU上集成新的ragged paged attention kernel到vLLM v1。,https://github.com/vllm-project/vllm/issues/13379
vllm,这个issue是一个功能需求，主要涉及到V1版本的核心功能，旨在通过在GPU缓存驱逐时将KV缓存块转移到CPU并在缓存命中时将其切换回GPU，旨在最小化不必要的数据交换所导致的性能开销。,https://github.com/vllm-project/vllm/issues/13377
vllm,这个issue类型是功能需求，涉及的主要对象是将新的ragged paged attention kernel与vLLM v1整合在TPU上。,https://github.com/vllm-project/vllm/issues/13372
vllm,这个issue类型是功能需求，涉及的主要对象是ArcticForCausalLM，用户提出了添加`SupportsQuant`接口的需求。,https://github.com/vllm-project/vllm/issues/13366
vllm,这是一个优化性质的issue，主要涉及N-gram匹配算法的性能优化，由于通过Numba进行JIT编译，导致了算法速度显著提升。,https://github.com/vllm-project/vllm/issues/13365
vllm,这是一个关于安装问题的帮助请求，涉及vLLM的源码构建和flashattention组件，可能由于环境隔离或构建配置问题导致了缺失目标文件的bug。,https://github.com/vllm-project/vllm/issues/13364
vllm,这个issue是一个功能需求提议，涉及主要对象为VLLM的speculative decoding drafter，并由于功能设计需要将其移动到model runner中。,https://github.com/vllm-project/vllm/issues/13363
vllm,这是一个用户提出需求的类型的issue，主要是针对`LogitsProcessor`接口的设计提出了一些建议和设想。可能是由于需要更好地处理采样参数与持久批处理相关的功能，以及提供自定义logits处理器的扩展点。,https://github.com/vllm-project/vllm/issues/13360
vllm,"该issue类型是用户提出需求，主要涉及到支持多GPU规格的功能，用户希望vllm在初始化时能够支持类似cuda:0,1这样的多GPU规格。",https://github.com/vllm-project/vllm/issues/13357
vllm,这是一个用户提出需求的issue，主要对象是日志统计间隔。用户希望实现对日志统计间隔的配置，以便更灵活地记录统计数据。,https://github.com/vllm-project/vllm/issues/13356
vllm,该issue是关于特性需求，提出了要整合性能基准数据集，并因为数据集采样函数定义在各自脚本中导致维护困难及灵活性较差。,https://github.com/vllm-project/vllm/issues/13351
vllm,这是一个需求提出的issue，主要涉及的对象是benchmark_serving.py文件，由于需要添加LongBench数据集以支持具有更长输入长度的数据，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/13350
vllm,该问题类型为需求提出，主要涉及对象为H2O-VL文档和示例。原因是需要更新文档以反映H2OVL在V1上的支持情况，并进行比较说明不同版本之间的下载量和喜爱程度。,https://github.com/vllm-project/vllm/issues/13349
vllm,这个issue属于用户提出需求的类型，主要涉及的对象是vllm项目的代码风格设置。由于Black默认使用88字符作为行长限制，用户建议使用相同的设置以保持兼容性。,https://github.com/vllm-project/vllm/issues/13347
vllm,该问题类型为用户提出需求，询问关于是否支持FP8 attention的问题，涉及主要的对象是vllm。由于用户想了解vllm是否支持FP8 attention，希望得到相关信息。,https://github.com/vllm-project/vllm/issues/13345
vllm,这是一个用户提出需求的类型的issue， 主要涉及的对象是Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit和Qwen2.5-VL-72B-Instruct-bnb-4bit系列模型。由于缺乏应用支持，导致用户提出希望为这些模型添加应用支持的需求。,https://github.com/vllm-project/vllm/issues/13344
vllm,这是一个用户提出需求的issue，主要涉及到的对象是应用程序支持InternVL2.5-78B系列模型，由于缺少相应的支持，用户需要添加对这些模型的支持。,https://github.com/vllm-project/vllm/issues/13343
vllm,这是一个功能需求的issue，主要涉及到vllm下的S1-32B推理解析器的支持问题。由于S1在识别法律文本中的逻辑结构时，推理相对不够结构化，因此需要增加对S1-32B推理解析器的支持。,https://github.com/vllm-project/vllm/issues/13342
vllm,这个issue是关于功能需求的，主要涉及Scheduler和Model Runner，因为需要修改使Model Runner从Scheduler获取输入令牌ID，该需求特别在令牌ID不由Model Runner生成时非常有用。,https://github.com/vllm-project/vllm/issues/13339
vllm,这是一个用户提出需求的issue，主要涉及vllm的chat template配置问题，导致程序输出无意义的结果。,https://github.com/vllm-project/vllm/issues/13337
vllm,这个issue类型是用户提出需求，涉及的主要对象是Molo models，由于configure_quant_config假定所有packed_modules_mapping在初始化前已声明，但实际上某些子模型的映射只能在初始化时确定，因此需要添加SupportsQuant接口以解决此问题。,https://github.com/vllm-project/vllm/issues/13336
vllm,这是一个性能优化类的issue，主要涉及到vllm库中的logit_bias_logits_processor处理过程，由于目前使用的Python操作，导致处理时间较长，用户希望通过转换为张量操作来提升性能。,https://github.com/vllm-project/vllm/issues/13334
vllm,这是一个用户提出需求的类型，主要涉及到Fuyu E2E的示例代码，用户需要避免硬编码`30`作为patch大小的数值。,https://github.com/vllm-project/vllm/issues/13331
vllm,该issue属于用户提出需求类型，主要涉及AsyncLLMEngine是否适用于多模态模型以及是否应该使用MQLLMEngine。由于缺乏文档支持，用户在尝试使用AsyncLLMEngine处理带有图像或视频URL的多模态模型时遇到了困惑。,https://github.com/vllm-project/vllm/issues/13324
vllm,这个issue类型为功能需求，主要涉及flashinfer和vllm之间的合作更新。,https://github.com/vllm-project/vllm/issues/13323
vllm,这是一个性能优化的issue，主要涉及moe wna16 cuda kernel，由于m is small时triton无法发挥最佳性能，因此提出了添加moe wna16 cuda kernel以提升生成速度。,https://github.com/vllm-project/vllm/issues/13321
vllm,这个issue属于用户提出需求类型，主要对象是VLM下的Florence-2模型，由于需要支持多模态输入，用户希望修复这个问题。,https://github.com/vllm-project/vllm/issues/13320
vllm,这个issue是一个需求提出类型的问题单，主要涉及的对象是vLLM中的Transformer执行路径。由于目前vLLM推理服务缺乏完全基于Triton的执行路径，因此用户提出了引入一个完全不涉及CUDA而仅涉及Triton的Transformer执行路径的建议。,https://github.com/vllm-project/vllm/issues/13319
vllm,这个issue类型是用户提出需求，主要涉及的对象是请求支持新的模型AIDC-AI/Ovis2-1B。这个问题的根本原因是vllm还没有支持该新模型，用户希望得到支持。,https://github.com/vllm-project/vllm/issues/13317
vllm,这是一个用户提出需求的issue，主要涉及的对象是模型支持。主要原因是为了添加对`GraniteMoeShared`模型的支持，并且提醒需要使用`transformers >= v4.49.0`。,https://github.com/vllm-project/vllm/issues/13313
vllm,这是一个建议性问题，主要涉及到VLLM模型中的Sampler功能，提出避免在某种情况下冗余操作的建议。,https://github.com/vllm-project/vllm/issues/13311
vllm,这个issue类型是测试需求，涉及的主要对象是V1 multimodal模型。根据内容，用户提出需要为V1 multimodal模型添加测试，包括abort和load测试。,https://github.com/vllm-project/vllm/issues/13308
vllm,这是一个用户需求类型的issue，主要涉及VLLM与RTX 5090（CUDA 12.8）的兼容性问题，导致安装VLLM失败。,https://github.com/vllm-project/vllm/issues/13306
vllm,这个issue是关于优化的，主要针对vLLM V1在AMG GPUs上的性能优化。,https://github.com/vllm-project/vllm/issues/13305
vllm,这个issue属于代码优化类型，主要涉及的对象是QuantizationConfig类。由于类变量值的潜在共享可能会导致不经意的问题，因此需要将`packed_modules_mapping`从类变量改为实例变量。,https://github.com/vllm-project/vllm/issues/13304
vllm,这是一个功能需求的issue，主要涉及到Transcription API streaming相关功能的添加，提出了关于音频长度度量、Dockerfile更新以及优化分割音频的建议。,https://github.com/vllm-project/vllm/issues/13301
vllm,这个issue属于用户提出需求类型，主要对象是SamplingParams。由于需要定制采样实现，因此添加了一个额外参数的字段。,https://github.com/vllm-project/vllm/issues/13300
vllm,这是一个需求类型的issue，主要涉及的对象是V1版本的Hybrid allocator，由于需要实现interleaved full attention & sliding window attention模型，需要调整block的分配方式。,https://github.com/vllm-project/vllm/issues/13296
vllm,这是一个用户提出需求的issue，主要涉及到添加CLI参数以帮助用户更好地处理已弃用的指标。,https://github.com/vllm-project/vllm/issues/13295
vllm,这是一个建议更正文档的issue，该问题涉及主要对象是代码中的工具调用函数。由于代码中硬编码了函数名称而没有使用之前定义的变量，导致用户建议可以从预定义的字典中获取函数名称。,https://github.com/vllm-project/vllm/issues/13291
vllm,该issue属于功能需求，涉及监控指标的添加，主要对象是V1版本的Metrics模块。这个问题产生的原因是引入了基于cudagraph捕获大小的bucket尺寸，导致需要添加包括iteration tokens总数的直方图。,https://github.com/vllm-project/vllm/issues/13288
vllm,这个issue类型属于功能需求，主要涉及的对象是Whisper waveform输入。由于缺少具体内容，用户提出了有关Whisper waveform输入的需求或问题。,https://github.com/vllm-project/vllm/issues/13277
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是logging stats功能。由于当前无法配置日志统计信息的时间间隔，用户提出希望添加可配置的间隔选项以满足需求。,https://github.com/vllm-project/vllm/issues/13275
vllm,这个issue是用户提出需求类型的，主要涉及的对象是VLLM serve的集成和推断运行。这个问题是关于如何尽可能避免VLLM由于CUDA内存不足而导致崩溃的问题。,https://github.com/vllm-project/vllm/issues/13267
vllm,这是一个用户提出需求的类型，主要涉及对象是VL模块，该需求是为了支持LoRA。,https://github.com/vllm-project/vllm/issues/13261
vllm,这是一个功能需求类型的issue，主要涉及设置OpenAI兼容服务器的模型ID。由于当前模型ID的取值方式不够合理，用户希望能够通过指定模型ID来调用API。,https://github.com/vllm-project/vllm/issues/13257
vllm,这个issue类型是用户提出需求，主要涉及模型添加支持的问题。提出原因可能是希望支持一个新的模型Ovis2，但目前提供的最接近的模型是Qwen2.5VL。,https://github.com/vllm-project/vllm/issues/13251
vllm,这个issue类型属于功能增强需求，主要对象是Neuron中的NKI Flash PagedAttention模块，通过开发BlockSparse版本来减少计算浪费。,https://github.com/vllm-project/vllm/issues/13249
vllm,该issue类型为功能需求，主要涉及的对象是RPCProcessRequest和MQLLMEngineClient，用户提出了对添加`extra_args`到这两个类的请求。,https://github.com/vllm-project/vllm/issues/13248
vllm,这个issue是一个功能需求提出，主要涉及的对象是为neuron backend添加自定义操作。由于vLLM正在支持vLLM V1架构为neuron backend做努力，因此需要添加激活函数、layernorm、rotary_embedding和logits_processor等自定义操作。,https://github.com/vllm-project/vllm/issues/13246
vllm,这是一个性能优化类的issue，主要对象是NKI flash attention kernel。由于未对KV缓存加载进行矢量化处理，导致内核受制于从HBM提取分页的KV缓存，因此引发了性能瓶颈。,https://github.com/vllm-project/vllm/issues/13245
vllm,这是一个用户提出的需求类型的issue，主要涉及到了SamplingMetadata对象和input_batch.req_ids，由于原始构建的方式导致了在迭代过程中需要对列表进行切片操作，用户希望进行优化以提高效率。,https://github.com/vllm-project/vllm/issues/13244
vllm,这个issue类型是功能需求提议，主要对象是设置在`pre-commit-passed`标签的成功结果。 ,https://github.com/vllm-project/vllm/issues/13243
vllm,该issue是一个撤销之前添加的功能变更，属于代码修改类型，主要涉及到vllm项目中关于pre-commit功能的处理。,https://github.com/vllm-project/vllm/issues/13242
vllm,这个issue是一个功能需求，主要涉及MM(cache)的大小配置，为了保证两个cache大小相同，需要将硬编码的部分配置为环境变量。,https://github.com/vllm-project/vllm/issues/13239
vllm,这是一个性能优化类型的issue，主要涉及到使用moes和专家数量多时的性能问题，用户在多个模型中测试后发现moes（Mixture of Experts）中默认使用moe_wna16 kernel对于拥有多个专家的情况下是更快的。,https://github.com/vllm-project/vllm/issues/13236
vllm,这个issue类型是用户提交签名测试CI系统的请求，涉及的主要对象是CI系统。由于开发者Alexei V. Ivanov提交了未经认证的签名，可能导致CI系统无法正常运行测试。,https://github.com/vllm-project/vllm/issues/13233
vllm,该issue类型属于性能优化，主要涉及ROCm平台上的矩阵乘法操作。由于硬件限制导致在GEMM中存在严重的Tagram通道热点问题，需要通过应用stride padding来避免这一问题，以提高性能。,https://github.com/vllm-project/vllm/issues/13231
vllm,这是一个关于功能需求的issue，主要涉及了 xgrammar 的正则表达式支持问题，由于旧版本的 xgrammar 存在问题，导致无法开启正则表达式支持。,https://github.com/vllm-project/vllm/issues/13228
vllm,这是一个用户提出的需求，主要涉及日志记录过程中敏感信息过滤的功能添加。,https://github.com/vllm-project/vllm/issues/13225
vllm,该issue是关于文档改进的，主要对象是项目的功能表格展示，原因是为了使表格更易读。,https://github.com/vllm-project/vllm/issues/13224
vllm,该issue是一个功能需求提议，涉及对象为在设置了`VLLM_USE_MODELSCOPE=true`环境下下载lora模型的行为。这个问题的提出原因是为了避免用户在不同库下载模型和lora的不一致性问题。,https://github.com/vllm-project/vllm/issues/13220
vllm,该issue属于用户提出需求，主要是针对新增hidet后端支持的请求。,https://github.com/vllm-project/vllm/issues/13217
vllm,这个issue是关于功能需求的，主要涉及到Hardware上的Intel-Gaudi，通过添加对HPU的区域编译支持来提高性能。,https://github.com/vllm-project/vllm/issues/13213
vllm,这个issue类型是用户提出需求，主要涉及到V1接口的文档清晰度问题。由于缺乏文档逻辑清晰度，导致新开发者在理解input processing和multimodal feature caching logic方面存在困难。,https://github.com/vllm-project/vllm/issues/13211
vllm,该issue类型为功能改进，涉及对象为vllm中的Sampler模块，用户提出了增加allowed_token_ids支持的需求。,https://github.com/vllm-project/vllm/issues/13210
vllm,这是一个用户提出需求的issue，主要涉及的对象是平台通信模块。,https://github.com/vllm-project/vllm/issues/13208
vllm,这个issue类型是功能增强（feature enhancement），主要涉及到将模型加载到CI中，其目的是从S3路径而不是HF加载模型，可能由于HF导致的路径问题导致了bug或者需要用户对加载模型的时候的路径进行改进。,https://github.com/vllm-project/vllm/issues/13205
vllm,这个issue类型为性能优化，主要涉及AMD的ROCm性能优化问题，并由于添加某些调整以改善DeepSeek的表现。,https://github.com/vllm-project/vllm/issues/13199
vllm,这个issue属于用户提出需求类型，主要涉及Qwen2_5_VLForConditionalGeneration模型支持enablelora功能的问题。由于该功能尚未被支持，用户询问何时可实现此功能。,https://github.com/vllm-project/vllm/issues/13194
vllm,这是一个用户提出需求的issue，主要涉及的对象是v1 Sampler。由于缺少对min_p参数的支持，用户请求在v1 Sampler中引入min_p支持。,https://github.com/vllm-project/vllm/issues/13191
vllm,该issue类型为用户提出需求，主要涉及需要实现新的LLaVAVideo7BQwen2模型。用户认为该模型在视频理解方面表现更优，因此请求实现此模型。,https://github.com/vllm-project/vllm/issues/13190
vllm,该issue类型为用户提出需求，询问如何在chat completion API中使用DeepSeekCoderV2LiteInstruct（https://huggingface.co/deepseekai/DeepSeekCoderV2LiteInstruct），询问如何集成到vllm中。,https://github.com/vllm-project/vllm/issues/13189
vllm,这是一个提出需求的issue，主要涉及Zamba2模型集成的支持。由于要添加对Zamba2模型的支持，需要修改代码以确保其与HuggingFace transformers库的集成，并对模型的性能进行评估和比较。,https://github.com/vllm-project/vllm/issues/13185
vllm,这是一个用户提出需求的issue，主要涉及在线推理API中如何设置图像分辨率的问题。用户想要运行特定模型的推理，但不清楚如何集成到vllm中。,https://github.com/vllm-project/vllm/issues/13184
vllm,这是一个功能需求的issue，主要涉及MLA进行量化时需要支持更多类型的量化，原因是需要访问未量化的权重以用于解码。,https://github.com/vllm-project/vllm/issues/13181
vllm,这个issue类型是功能需求，主要对象是CI/Build，用户想要针对ruff开启自动修复功能以简化代码维护。,https://github.com/vllm-project/vllm/issues/13180
vllm,这是一个功能需求提出的issue，主要对象是构建VLLM时的Python-only安装过程。由于当前命令总是使用每晚构建的wheel，导致使用开发分支时可能不兼容，因此提出了基于当前分支基准提交的wheel安装改进。,https://github.com/vllm-project/vllm/issues/13178
vllm,这是一个优化性质的issue，主要涉及硬件TPU在更新kv缓存性能方面的改进，由于TPU不擅长scatter操作导致kv缓存更新性能低下。,https://github.com/vllm-project/vllm/issues/13176
vllm,这是一个功能需求的issue，涉及到V1版本的度量标准功能。由于需要添加核心引擎`PREEMPTED`事件，并且在发生抢占时重新设置计划和第一个令牌的时间戳。,https://github.com/vllm-project/vllm/issues/13169
vllm,这是一个用户提出需求的issue，主要对象涉及VLLM中关于Deepseek GGUF支持的功能。这个issue涉及到由于Huggingface未支持Deepseek，导致加载模型速度慢以及GGUF MoE实现简单且速度缓慢的问题。,https://github.com/vllm-project/vllm/issues/13167
vllm,这个issue类型属于功能需求提出类型，主要涉及的对象是LoRA相关的静态变量`supported_lora_modules`。由于存在该静态变量导致模型实现不够清晰，需求是删除并进行单元测试以提升LoRA支持效果。,https://github.com/vllm-project/vllm/issues/13166
vllm,该issue类型为功能改进，主要对象是Llava系列模型中的clip和siglip。,https://github.com/vllm-project/vllm/issues/13165
vllm,这是一个功能需求的issue，涉及的主要对象是在Huggingface中添加对Deepseek GGUF模型的支持。 由于Huggingface当前不支持Deepseek，所以用户提出了需要添加覆盖路径以读取正确配置的需求。,https://github.com/vllm-project/vllm/issues/13163
vllm,这个issue是一个功能改进（feature enhancement），主要涉及到CI/Build过程中的mypy输出解析，意图是恢复在PR上更清晰地展示mypy错误。这个问题可能由于在转换到pre-commit时配置未完全迁移导致。,https://github.com/vllm-project/vllm/issues/13162
vllm,这个issue是一个功能需求类型的问题，涉及的主要对象是vLLM版本V0。由于当前版本V0未实现可插拔的平台特定调度器功能，用户提出了在V1版本中支持此功能的需求。,https://github.com/vllm-project/vllm/issues/13161
vllm,这是一个用户需求提出的issue，主要涉及到多节点服务部署的问题。由于部署deepseekr1非常复杂，参数繁多，希望提供docker-compose.yml示例以简化多节点服务的部署过程。,https://github.com/vllm-project/vllm/issues/13158
vllm,这是一个优化性质的Issue，主要涉及到VLLM代码库中的一些模块和函数。原因是为了提高速度和减少内存使用。,https://github.com/vllm-project/vllm/issues/13155
vllm,这个issue属于用户需求类型问题，涉及主要对象是vllm长文本输入优化。由于历史信息积累导致首个token生成时间增加。,https://github.com/vllm-project/vllm/issues/13154
vllm,这是一个建议类的issue，主要涉及删除未使用的 LoRA 模块，以减少内存消耗。,https://github.com/vllm-project/vllm/issues/13151
vllm,这是一个用户提出需求的issue，主要涉及如何在vllm中运行特定模型，可能由于当前环境仅支持weightonly而未支持gptq量化，导致用户不清楚如何集成模型并进行推理。,https://github.com/vllm-project/vllm/issues/13149
vllm,这个issue类型属于功能咨询，涉及主要对象为vLLM编译器。用户提出了关于vLLM是否编译草稿模型以及对编译配置的疑问。,https://github.com/vllm-project/vllm/issues/13144
vllm,这个issue类型是关于功能性问题，主要涉及的对象是Tensor Parallelism在vLLM中的实现方式，提出了关于在进行决策时是否考虑GPU内存信息的疑问。,https://github.com/vllm-project/vllm/issues/13141
vllm,这个issue属于用户提出需求类型，主要涉及FastAPI中如何访问请求、响应和令牌数的问题。原因是用户希望在服务器端访问这些数据以便存储。,https://github.com/vllm-project/vllm/issues/13137
vllm,这是一个用户提出需求的类型issue，主要涉及使用VLLM调试时如何设置断点的问题。用户想要禁用编译并像正常python脚本一样设置断点，因为当前执行的vllm是通过一个编译引擎管理的，不易直接在IDE中使用断点调试。,https://github.com/vllm-project/vllm/issues/13120
vllm,该issue类型为需求提出，主要涉及的对象是Qwen和Qwen2模型。由于需要将Qwen和Qwen2模型转换为Qwen2.5并减小模型尺寸，可能是为了优化性能或资源利用效率。,https://github.com/vllm-project/vllm/issues/13118
vllm,这是一个功能需求类型的issue，主要涉及的对象是vllm软件中的睡眠模式功能。,https://github.com/vllm-project/vllm/issues/13115
vllm,这是一个关于改进功能的issue，主要涉及precommit配置文件的位置问题。由于在中间位置的precommit建议影响文件编辑和阅读，用户提出将其移动到最后位置以提高可读性。,https://github.com/vllm-project/vllm/issues/13114
vllm,这是一个关于技术改进及功能优化的类型问题，主要涉及uvicorn在macOS环境下传递预创建的socket，原因是为了解决SO_REUSEADDR和SO_REUSEPORT相关设置的问题。,https://github.com/vllm-project/vllm/issues/13113
vllm,这是一个用户提出需求的issue，主要涉及到如何减少使用python-slim作为基础的vllm Docker镜像的大小。用户希望减小镜像大小，因为当前镜像大小过大，主要原因是vllm的依赖导致。,https://github.com/vllm-project/vllm/issues/13112
vllm,这是一个用户提出需求的类型，该问题涉及的主要对象是支持AWQMarlin与MLA。,https://github.com/vllm-project/vllm/issues/13109
vllm,这是一个优化请求，主要涉及减少与huggingface.co的 HTTP 调用，原因是为了改善性能和效率。,https://github.com/vllm-project/vllm/issues/13107
vllm,这是一个功能需求类型的issue，主要涉及添加`SupportsQuant`到phi3和clip模型，通过引入这个mixin来标记模型支持量化推断，并解决`configure_quant_config`函数假设所有`packed_modules_mapping`都在初始化之前声明的问题。,https://github.com/vllm-project/vllm/issues/13104
vllm,"这是一个文档改进类型的issue，主要涉及OpenVINO的安装文档，由于缺少git clone步骤在""Build wheel from source""的非GPU安装文档中，导致用户反馈缺失。",https://github.com/vllm-project/vllm/issues/13102
vllm,这是一个用户提出需求的issue，主要涉及V1 LoRA中添加新的triton kernels，因为V1不再根据LoRA ID对请求进行分组，需要新的kernels来加载适当的输入tokens。,https://github.com/vllm-project/vllm/issues/13096
vllm,这是一个用户提出需求的issue，主要涉及的对象是V1版本中的prefix_prefill kernel。原因是因为在V1版本中不跟踪上下文长度张量的情况下，需要改进kernel以根据查询长度和序列长度计算上下文长度。,https://github.com/vllm-project/vllm/issues/13095
vllm,这是一个用户提出需求的issue，主要涉及Llama模型在测试中的使用，由于模型更新和依赖关系变化，需要将部分模型统一使用较新的版本。,https://github.com/vllm-project/vllm/issues/13094
vllm,这个issue类型是需求提出，涉及主要对象为GroupCoordinator，由于当前设备类型未正确初始化，导致其他设备无法正确初始化。,https://github.com/vllm-project/vllm/issues/13086
vllm,这是一个需求提出的issue，主要涉及的对象是benchmark script和disaggregation proxy。由于没有特定模型名称提供，脚本使用了模型列表API中的第一个模型，为了与基准服务脚本兼容，需要在disaggregation proxy中添加模型列表API。,https://github.com/vllm-project/vllm/issues/13083
vllm,这是一个功能需求的issue，主要涉及到Pixtral模型的定制化需求。,https://github.com/vllm-project/vllm/issues/13080
vllm,这是一个用户提出需求的issue，涉及的主要对象是v1 Sampler。由于缺少对logit_bias功能的支持，用户提出需要在v1 Sampler中引入该功能。,https://github.com/vllm-project/vllm/issues/13079
vllm,该issue属于用户需求类，主要涉及用户想要在vllm中运行qwen2.5_7B_Instruct大模型的推理操作，但不清楚如何与vllm集成，导致用户提出了如何使用特定模型的问题。,https://github.com/vllm-project/vllm/issues/13070
vllm,这是一个功能需求的issue，涉及主要对象为vLLM的benchmark功能集成到PyTorch OSS benchmark数据库的问题。,https://github.com/vllm-project/vllm/issues/13068
vllm,这是一个用户提出需求的issue，主要涉及的对象是关于Meta Meetup的文档公告。,https://github.com/vllm-project/vllm/issues/13065
vllm,这是一个用户提出需求的issue，主要涉及到V1版本的PP功能，由于需要解耦批处理调度和模型执行，提出了一种新的引擎管理策略。,https://github.com/vllm-project/vllm/issues/13064
vllm,这是一个需求类型的issue，主要涉及的对象是对Benchmark的服务进行添加BurstGPT数据集。这个需求主要是为了提供包含更长prompt和outputs的BurstGPT数据集，而且目前的数据集并不包含请求的内容。,https://github.com/vllm-project/vllm/issues/13063
vllm,这是一个用户提出需求的问题，主要涉及V1版本中处理引擎实用方法的通用机制。这个问题出现的原因是为了集中处理需要在引擎上调用的一系列实用/“控制”操作，并且需要以同步方式调用以知道操作何时完成以及是否成功，因为未来的一些操作可能需要返回结果。,https://github.com/vllm-project/vllm/issues/13060
vllm,这个issue类型是功能需求，主要涉及到V1 ModelRunner的设备无关抽象。由于需要支持多个设备后端，导致需要对现有模型运行器进行转换。,https://github.com/vllm-project/vllm/issues/13059
vllm,该issue是一个需求提出类型的问题，主要涉及到vllm项目中缺少一些采样参数的支持，需要协助将这些参数在V1版本中进行完善。,https://github.com/vllm-project/vllm/issues/13058
vllm,该issue是一个需求提议，涉及到服务器配置信息的获取，由于部分参数值没有默认值和动态选择规则，希望提供一个端点返回服务器配置状态。,https://github.com/vllm-project/vllm/issues/13056
vllm,这个issue是关于特性增加的需求，主要涉及的对象是DeepSeek-V3模型，由于缺少DeepSeek-V3模型配置文件而导致需要添加并注册模型配置。,https://github.com/vllm-project/vllm/issues/13055
vllm,该issue类型为功能增强（Feature enhancement），主要涉及对象是CLIPVisionModel，由于需要支持加载量化的clip模型（phi3v），因此添加了`packed_modules_mapping`属性到CLIPVisionModel。,https://github.com/vllm-project/vllm/issues/13051
vllm,这个issue属于用户提出需求类型，主要涉及VLLM下的Qwen2.5-VL-7B-Instruct模型是否支持视频模式，由于目前看起来该模型不支持视频模式，用户希望了解是否有计划进行添加。,https://github.com/vllm-project/vllm/issues/13050
vllm,这是一个功能需求的issue，主要涉及模型加载和分发过程中的性能优化问题。,https://github.com/vllm-project/vllm/issues/13046
vllm,这是一个Feature请求，主要涉及vllm中提供用于张量化权重的密钥生成的方法。请求的主要原因是希望能够使用给定的密钥而不是随机生成的密钥。,https://github.com/vllm-project/vllm/issues/13042
vllm,这是一个用户提出需求的issue，主要涉及vLLM中的Tensor encryption功能，用户希望能对adapter的权重进行加密。,https://github.com/vllm-project/vllm/issues/13041
vllm,这是一个功能需求的issue， 主要涉及到 vllm 项目中的 GPUModelRunnerBase 模块，提出需要重构 load_model 方法来包含设备参数。,https://github.com/vllm-project/vllm/issues/13037
vllm,这是一个用户提出需求的issue，主要涉及vllm中的多Lora推断支持问题。用户询问在llava v1.6上是否支持多Lora推断，或未来是否会有计划支持更多mllm，以及如何在llavanext上获得支持。这可能是由于当前环境下无法运行多Lora推断导致对应问题的提出。,https://github.com/vllm-project/vllm/issues/13034
vllm,这是一个用户提出需求的类型，主要对象是指定在启动docker镜像时指定进程名称的功能。,https://github.com/vllm-project/vllm/issues/13033
vllm,这个issue类型是性能提升建议，涉及的主要对象是cutlass_scaled_sparse_mm函数的参数设置问题，由于b_scales参数现在是标量而不是按通道设置，导致性能降级。,https://github.com/vllm-project/vllm/issues/13032
vllm,这是一个用户提出需求的问题，主要涉及对象为在ARM架构服务器上使用NVIDIA GPU运行VLLM的功能。由于环境需要在特定架构的服务器上运行VLLM并利用GPU加速，因此用户想知道是否支持这种配置。,https://github.com/vllm-project/vllm/issues/13030
vllm,这是一个用户提出需求的issue，主要涉及如何在VLLM中实现类似Ultravox模型的记忆和流式推理功能。,https://github.com/vllm-project/vllm/issues/13028
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是关于vllm的max_num_seqs参数。由于用户想要运行特定模型的推理，但不清楚如何与vllm集成，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/13026
vllm,这是一个需求提议（Request for Comments）的类型，涉及的主要对象是异步KV缓存传输，用户提出了关于优化解码执行过程中KV缓存传输的方案。,https://github.com/vllm-project/vllm/issues/13020
vllm,这是一个用户提出需求的issue，主要涉及的对象是Client，用户希望添加sleep和wake_up方法来进行模型状态管理。,https://github.com/vllm-project/vllm/issues/13016
vllm,这个issue是关于用户提出需求的，主要涉及vllm的服务在DDP模式下的使用方式。用户想要在单节点上使用多个GPU来启动多个vllm后端，并希望了解是否有相应功能支持或者需要自行实现，以便在这些后端间分配请求进行处理。,https://github.com/vllm-project/vllm/issues/13015
vllm,该issue类型为功能需求，涉及主要对象是Chat Prefix Completion。由于文中提到现有的pr需要对引擎端或结构化输出引擎进行更改，导致用户提出了在前端实现该功能的建议。,https://github.com/vllm-project/vllm/issues/13005
vllm,"该issue是一个功能需求，涉及到在多节点和多GPU上实现""Disaggregated Prefill""功能，用户希望了解在这方面的路线规划。这个问题是由于当前仅支持1P1D，因此无法使用所需功能引起的。",https://github.com/vllm-project/vllm/issues/13004
vllm,"这是一个用户提出需求的issue，主要涉及tool_choice选项中""required""选项的支持情况。该问题由于""required""选项暂未在工具调用中支持，用户提出需要追踪该选项的支持情况。",https://github.com/vllm-project/vllm/issues/13002
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM_CUDART_SO_PATH环境变量。用户提出了一个需求，希望可以通过指定VLLM_CUDART_SO_PATH环境变量解决find_loaded_library失败的问题。,https://github.com/vllm-project/vllm/issues/12998
vllm,这是一个用户提出需求的问题单，主要涉及VLLM双机部署Deepseek-R1，由于用户不清楚如何在两台服务器上设置通信信息而寻求帮助。,https://github.com/vllm-project/vllm/issues/12997
vllm,这个issue是用户提出需求，涉及主要对象为实现pipeline parallel on Ray的功能。由于目前尚未实现调度和流水线操作，只支持在不同pipeline parallel工作者上运行分离的模型层，因此用户提出了这一功能需求。,https://github.com/vllm-project/vllm/issues/12996
vllm,这个issue是一个用户提出的需求类型，主要涉及功能支持中用户指定“触发”标记在开始结构化解码前。导致这个需求是因为用户想要能够在执行结构化解码之前指定一个特定的“触发”标记。,https://github.com/vllm-project/vllm/issues/12995
vllm,"这是一个用户提出需求的issue，主要对象是""Whisper proto""。由于未提供具体内容，无法分析具体导致何种症状的问题。",https://github.com/vllm-project/vllm/issues/12993
vllm,这个issue是一个需求类型，主要涉及core功能，需要添加`/sleep`和`/wake_up`端点并在v1版本中支持。,https://github.com/vllm-project/vllm/issues/12987
vllm,这是一个用户提出需求的类型，该问题涉及到在RLHF中使用cudaipc同步训练actors的权重到vllm workers时的示例问题。,https://github.com/vllm-project/vllm/issues/12984
vllm,"该issue类型是需求，主要对象是vllm包中的_version.py文件，由于需要定制化逻辑并确保""version""库始终是vllm包中首先导入的库，因此提出该需求。",https://github.com/vllm-project/vllm/issues/12979
vllm,这是一个优化需求类型的issue，主要涉及到 CUTLASS kernels。由于K值较大时性能较慢，提出了使用`StreamK`策略优化GEMM计算的方法。,https://github.com/vllm-project/vllm/issues/12978
vllm,这是一个关于需求的RFC（Request For Comments）类型的issue，主要涉及到`pynvml`模块的使用问题，由于版本冲突导致了安装错误。,https://github.com/vllm-project/vllm/issues/12977
vllm,这是一个功能需求类型的issue，主要涉及到vLLM模型在跳过特定层的4比特量化时的问题。由于需要跳过特定的模块并保留16比特的量化，用户提出了此功能需求。,https://github.com/vllm-project/vllm/issues/12974
vllm,这是一个关于性能优化的issue，主要涉及对象是`GPUModelRunner`中的缓存`uses_mrope`。导致这个问题的原因是`model_config.uses_mrope` 每次调用时都会读取HF配置，导致不必要的开销。,https://github.com/vllm-project/vllm/issues/12969
vllm,这是一个需求提出的issue，主要涉及的对象是VLM中的merged multi-modal processor。由于目前模型实现可以处理多图像输入，但需要重构提示替换逻辑才能支持在vLLM中的端到端支持，因为图像令牌是在字符串开头插入而不替换任何部分。,https://github.com/vllm-project/vllm/issues/12966
vllm,这是一个需求提出类型的issue，该问题单涉及的主要对象是`transformers` backend，由于缺乏BNB支持和关于量化的更新文档，用户希望在该backend上启用量化支持。,https://github.com/vllm-project/vllm/issues/12960
vllm,这个issue属于功能需求，主要涉及到支持XpYd的分散预填充与MooncakeStore。由于需要实现XpYd功能，并且需要协调元数据信息，所以提出了这个功能需求。,https://github.com/vllm-project/vllm/issues/12957
vllm,这是一个用户提出需求的issue，主要涉及Guided/Structured grammar和lmformatenforcer之间的支持问题。,https://github.com/vllm-project/vllm/issues/12955
vllm,这是一个用户提出的需求类型的issue，主要涉及LMCache连接器，由于需要支持KV缓存卸载和disagg预填充。,https://github.com/vllm-project/vllm/issues/12953
vllm,这是一个用户提出需求的类型，主要涉及多节点上进行离线推理操作。用户想知道如何在分布式机器上运行模型并将参数放入多节点的不同GPU进行离线推理。,https://github.com/vllm-project/vllm/issues/12950
vllm,这是一个用户提出需求的issue，主要涉及的对象是Qwen2.5VL模型，用户希望在该模型中添加tp兼容BNB支持。,https://github.com/vllm-project/vllm/issues/12944
vllm,这是一个需求提出的issue，主要涉及到CI/Build的自动修复Markdown文件，由于需要更新precommit hook并修改lint规则，以自动修复错误并应用lint规则到所有Markdown文件。,https://github.com/vllm-project/vllm/issues/12941
vllm,该issue是关于功能需求的，主要涉及PD separation支持prefix caching，由于用户在prefix_caching开启时希望恢复真实的seq_lens，以避免当prefill_caching为True时出现的长度问题。,https://github.com/vllm-project/vllm/issues/12939
vllm,这是一个性能优化的问题，主要涉及到Ampere架构A16W8的量化优化，主要是针对GPTQ量化模型。,https://github.com/vllm-project/vllm/issues/12931
vllm,这个issue类型是用户提出需求，主要涉及的对象是vLLM的VRAM预分配调整，由于当前vLLM预分配的VRAM过多，导致用户在单个显卡上无法运行模型。,https://github.com/vllm-project/vllm/issues/12930
vllm,这个issue类型是需求补充，涉及的主要对象是代码注释。这个问题由于原来代码中的注释不清晰，需要移除后补充更准确的注释。,https://github.com/vllm-project/vllm/issues/12928
vllm,这个issue类型是功能改进，主要涉及的对象是日志记录时间消耗用于从HF下载权重，用户希望通过记录下载时间和模型权重加载时间来量化地了解本地缓存的性能提升，以评估（和优化）引擎的性能。,https://github.com/vllm-project/vllm/issues/12926
vllm,这是一个用户提出的需求类型的issue，主要涉及到对于Meta内部AMD构建的支持。由于内部无法直接使用sys.executable，导致需要修改子进程参数。,https://github.com/vllm-project/vllm/issues/12923
vllm,这个issue类型是功能改进，主要涉及对象是`Request`类和`KVCacheManager`类，由于`kv_block_hashes`属性仅被KV缓存管理器使用，将其从`Request`类移至`KVCacheManager`类以简化`Request`类。,https://github.com/vllm-project/vllm/issues/12922
vllm,这是一个需求提出的issue，主要是关于优化CUDA图捕获在特定批处理大小下性能波动的问题。,https://github.com/vllm-project/vllm/issues/12920
vllm,这是一个用户提出需求的issue，主要涉及vLLM的自动基准测试，扩展了性能基准测试以包括v0和v1，确保性能仪表板不受影响。,https://github.com/vllm-project/vllm/issues/12919
vllm,这个issue是关于优化请求序列化以及处理不同请求类型的改进，涉及对象是消息传递过程中的序列化方式及请求处理逻辑。由于msgpack不原生支持张量，而多模请求需要张量，因此现阶段在处理多模请求时还是直接使用pickle进行序列化，导致部分请求效率低下。,https://github.com/vllm-project/vllm/issues/12918
vllm,这是一个功能需求的issue，主要对象是vLLM中的日志记录方式。用户提出需要在一个日志消息中记录模型权重加载时间以简化分析过程。,https://github.com/vllm-project/vllm/issues/12917
vllm,这是一个用户提出需求的类型的issue，主要涉及到改进vLLM的日志记录功能。用户想要区分下载模型和加载模型权重两个步骤所消耗的时间。,https://github.com/vllm-project/vllm/issues/12916
vllm,这是一个技术需求问题，主要涉及DeepSeek-R1模型中的EAGLE-style MTP模块的实现。由于模型代码的变动和对模型输出的处理方式改变，导致需要注册新的实现来提高预测精度。,https://github.com/vllm-project/vllm/issues/12915
vllm,该issue类型是功能添加请求，主要涉及的对象是添加脚本以为多节点vllm部署设置ray。bug或者用户提出问题的原因是需要通过脚本启动ray集群并等待所有ray worker启动完成，而不会启动vllm服务器本身。,https://github.com/vllm-project/vllm/issues/12913
vllm,该issue是一个功能需求，主要对象是Ultravox音频/文本到文本模型。由于对架构进行了一些微小调整，需要支持v0.5版本发布，需要添加一个测试来验证新版本。,https://github.com/vllm-project/vllm/issues/12912
vllm,这是一个用户提出需求的issue，涉及支持Ray CG(ADAG) for NPU的功能需求，主要原因是为了加速推理过程。,https://github.com/vllm-project/vllm/issues/12911
vllm,这是一个功能需求的issue，主要涉及单分类任务中缺少logits导致判断错误的问题，用户希望添加logits以便更好地进行分类。,https://github.com/vllm-project/vllm/issues/12910
vllm,这是一个功能需求类的issue，主要涉及在vllm项目中添加一个名为`/v1/audio/transcriptions`的OpenAI API端点。,https://github.com/vllm-project/vllm/issues/12909
vllm,这是一个改进建议，旨在提升LLM模型在速度估计中的表现，主要涉及性能指标的调整。,https://github.com/vllm-project/vllm/issues/12908
vllm,这个issue类型是文档更新，主要涉及Benchmark模块的README.md文件，用户提出帮助新手入门的connectthedots指引。,https://github.com/vllm-project/vllm/issues/12903
vllm,这是一个关于需求提出的issue，主要涉及对象是VLLM V1，用户提出了对Speculative decoding支持的需求。,https://github.com/vllm-project/vllm/issues/12901
vllm,这是一个功能需求的Issue，涉及主要对象是 benchmark_serving.py，用户寻求添加 LoRA 模型到性能测试中。,https://github.com/vllm-project/vllm/issues/12898
vllm,这是一个用户提出需求的 issue，主要涉及 vLLM 的版本定制问题，用户想要构建指定版本的 wheel 文件而非默认版本。,https://github.com/vllm-project/vllm/issues/12894
vllm,这是一个关于优化构建的issue，涉及主要对象是CI/Build系统。由于容器的时区设置不明确，导致日志记录混乱，因此用户提出将容器时区设置为UTC的需求。,https://github.com/vllm-project/vllm/issues/12888
vllm,这是一个用户提出需求的类型的issue，主要涉及VLM库中对多模态提示的支持需求。由于当前代码中的图像token被堆叠在文本提示的开头，可能会影响教师模型的表现，用户希望实现完全交错的多模态提示支持。,https://github.com/vllm-project/vllm/issues/12885
vllm,这是一个用户提出需求的issue，主要涉及对象是LoRA功能，由于尚未实现其他LoRA具体的serving功能，用户在此提出将`add_lora`函数与`lora_model_runner_mixin.py`连接的需求。,https://github.com/vllm-project/vllm/issues/12883
vllm,这是一个用户提出需求的问题，主要涉及如何使用vllm进行推断，讨论了关于max_num_seqs参数含义的问题。可能的原因是用户不清楚如何将特定模型集成到vllm中。,https://github.com/vllm-project/vllm/issues/12881
vllm,这个issue类型是功能改进，主要对象是pre-commit hooks，由于需要将命令拆分为entry和args，以提升功能的灵活性。,https://github.com/vllm-project/vllm/issues/12880
vllm,这个issue类型属于用户提出需求，主要涉及到数据结构的选择。由于每个请求需要独立的隐藏状态存储，用户想探讨是否使用IntermediateTensors最合适，以及是否有更简化的存储结构可供使用。,https://github.com/vllm-project/vllm/issues/12878
vllm,这是一个性能优化的建议，该问题主要涉及支持DP attention for Deepseek models。原因是为了提高性能和吞吐量。,https://github.com/vllm-project/vllm/issues/12871
vllm,这是一个用户提出需求类型的issue，主要涉及到为vllm添加控制向量支持的功能。原因是用户希望通过添加控制向量来改变模型行为而不需要额外提示，以及可以通过设置不同的大小来改变模型行为的级别。,https://github.com/vllm-project/vllm/issues/12870
vllm,这个issue类型是技术改进，涉及到多模态处理。原因可能是之前存在不必要的解标记化操作，导致额外的解码调用。,https://github.com/vllm-project/vllm/issues/12868
vllm,这个issue类型是功能需求，涉及主要对象为vllm项目中的报告功能。由于需要实现在完成响应头中报告指标，因此用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/12854
vllm,这是一个功能改进类型的issue，涉及主要对象为`EngineCoreOutputs`。由于过多创建小对象在调度器中，导致需要将`EngineCoreOutput`字段直接折叠到`EngineCoreOutputs`中。,https://github.com/vllm-project/vllm/issues/12853
vllm,这是一个优化问题，涉及主要对象为`moe_align_block_size`。这个问题由于需要优化深度搜索V3时的对齐块大小，原因可能是当前的对齐块大小在评估中表现良好但仍有优化空间。,https://github.com/vllm-project/vllm/issues/12850
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是embedding models。由于需要离线/在线评分功能，用户提出了启用/embedding模型的/score端点的需求。,https://github.com/vllm-project/vllm/issues/12846
vllm,这是一个需求提出的issue，主要涉及添加pythonjsonlogger到requirements-common，带来了logging的简化使用以及容器大小影响的问题。,https://github.com/vllm-project/vllm/issues/12842
vllm,这是一个用户提出需求的issue，主要对象是vLLM工具功能的使用和OpenAI模型的集成。用户希望了解如何通过payload中的`tools`和`tool_choice`参数来使用不同的OpenAI兼容模型生成响应。,https://github.com/vllm-project/vllm/issues/12839
vllm,"这是一个用户提出需求的issue，主要对象是vLLM工具的`tool_choice`参数。由于当前请求中`enableautotoolchoice`参数需要必须带有`tool_choice`参数，导致开发者体验不佳，用户提出让`tool_choice`参数在`enableautotoolchoice`情况下成为可选项并默认为""auto""的建议。",https://github.com/vllm-project/vllm/issues/12834
vllm,这是一个功能需求的issue，主要涉及的对象是`TransformersModel`，用户提出了添加pipeline parallel支持的需求。,https://github.com/vllm-project/vllm/issues/12832
vllm,这个issue属于功能需求类型，涉及的主要对象是将IBM/NASA Prithvi Geospatial模型集成到vLLM中。这个需求是为了实现非语言模型与vLLM的集成。,https://github.com/vllm-project/vllm/issues/12830
vllm,这个issue属于用户提出需求，主要涉及的对象是vLLM，由于vLLM目前不支持多Lora分类模型，用户想了解是否有计划在近期内添加支持，并寻求如何实现这个功能的指导。,https://github.com/vllm-project/vllm/issues/12829
vllm,这是一个用户需求类型的issue，主要对象是对Qwen2.5-VL-7B Instruct功能的支持。该需求可能是由于用户需要这一功能或者应用场景的需求而提出。,https://github.com/vllm-project/vllm/issues/12825
vllm,这是一个功能需求类型的issue，主要涉及的对象是vllm框架中的Worker相关类。,https://github.com/vllm-project/vllm/issues/12816
vllm,该issue属于用户提出需求类型，涉及主要对象为文档内容。,https://github.com/vllm-project/vllm/issues/12814
vllm,这个issue类型是功能改进请求，主要对象是Intel Gaudi加速器，用户希望使其支持长上下文和LoRA功能。,https://github.com/vllm-project/vllm/issues/12812
vllm,这是一个需求类型的issue，主要涉及MLA在Hopper上使用FA3的需求。造成这个issue的原因可能是之前没有明确使用FA3导致MLA prefill出错。,https://github.com/vllm-project/vllm/issues/12807
vllm,这个issue类型是测试需求，主要对象是CI系统。由于新增的pre-commit hook可能无法触发CI检查，导致用户无法确认CI是否正常工作。,https://github.com/vllm-project/vllm/issues/12805
vllm,这是一个建议类型的issue，主要涉及到移除DeepSeek V3模型定义的重复内容。原因是DeepSeekV3可被简单地表示在DeepSeekV2模型定义中，因为某些配置已经存在于DeepSeekV2配置中。,https://github.com/vllm-project/vllm/issues/12793
vllm,这是一个用户需求类型的issue，主要涉及在AMD系统上添加对V1的初始支持，由于使用了`vllm/attention/ops/prefix_prefill.py`内核而不是flashattn，可能导致需要更新的安装步骤和示例命令的提出。,https://github.com/vllm-project/vllm/issues/12790
vllm,这个issue类型是用户提出需求。该问题单涉及的主要对象是核心功能的pipeline parallel。由于需要使用线程同时执行多个任务，用户希望实现pipeline并行化。,https://github.com/vllm-project/vllm/issues/12787
vllm,这是一个建议改进类型的issue，涉及主要对象为`TransformersModel`。由于`Linear` layer 无法被tensor并行化，用户体验较差，希望改为警告而非错误信息。,https://github.com/vllm-project/vllm/issues/12785
vllm,该issue类型为功能需求提出，主要对象是支持nvfp4量化，分离fp4量化和fp4 gemm为两个PR。,https://github.com/vllm-project/vllm/issues/12784
vllm,这个issue属于需求提出类型，主要涉及的对象是硬件中的Intel Gaudi以及HPU，可能是用户提出了关于多步调度实现的需求或问题。,https://github.com/vllm-project/vllm/issues/12779
vllm,这是一个性能优化类的issue，主要对象是`TransformersModel`模块，用户提议使用vLLM的`RMSNorm`类来改进性能并改进用户体验。,https://github.com/vllm-project/vllm/issues/12776
vllm,这个issue类型是用户提出需求，询问问题，主要涉及vllm的使用。由于用户不知道如何与vllm集成以运行特定模型的推理，导致提出了这个问题。,https://github.com/vllm-project/vllm/issues/12775
vllm,这是一个功能需求类型的 issue， 主要涉及的对象是 vllm endpoint， 用户提出了希望在 vllm endpoint 中暴露从磁盘加载新模型权重的选项。,https://github.com/vllm-project/vllm/issues/12774
vllm,这是一个用户提出需求的issue，主要对象是想要访问`vllm.v1.core.scheduler.Scheduler`，由于`Scheduler`是`EngineCore`的成员且在后台进程中运行，用户无法直接访问该对象。,https://github.com/vllm-project/vllm/issues/12772
vllm,该issue类型为用户提出需求，该问题单涉及的主要对象为vllm的文档。由于用户想要了解vllm是否支持qwen1的audio大模型，希望寻求潜在的替代方案或修复方法。,https://github.com/vllm-project/vllm/issues/12768
vllm,这个issue是关于功能改进的，主要涉及VLM（Visual Language Modeling）模型使用共享字段传递令牌ID，由于需要改进字段元素的构建方式和增加文档说明，以及为每种字段类型添加示例。,https://github.com/vllm-project/vllm/issues/12767
vllm,这个issue是关于用户需求的问题，主要涉及的对象是vllm多机部署。由于用户当前环境的CUDA版本与PyTorch版本不匹配，导致可能无法成功进行多机部署。,https://github.com/vllm-project/vllm/issues/12765
vllm,该issue是用户提出的需求类型问题，主要涉及使用vllm进行多机部署。由于环境信息中显示CUDA版本与cuDNN版本不匹配，可能导致无法成功进行多机部署。,https://github.com/vllm-project/vllm/issues/12763
vllm,该issue属于一个功能请求（RFC），主要涉及支持V1架构中使用交叉注意力的多模型模型。由于需要支持跨注意力的多模型，导致需要对代码中的多个部分进行修改。,https://github.com/vllm-project/vllm/issues/12761
vllm,这是一个用户提出需求的issue，主要涉及到`FinishReason` enum的短小和使用常量字符串。,https://github.com/vllm-project/vllm/issues/12760
vllm,该issue类型为用户提出需求，询问如何在vllm中集成deepseek-r1-4bit模型，可能是因为用户想要运行特定模型的推断，但不清楚如何在vllm中实现。,https://github.com/vllm-project/vllm/issues/12758
vllm,这个issue类型是功能更新，涉及的主要对象是代码中的w2 weight scales，在更新中添加了条件以根据act order对其进行分区。,https://github.com/vllm-project/vllm/issues/12757
vllm,该issue属于功能需求，主要涉及DeepSeek MTP的实现和性能优化。原因是为了支持DeepSeek MTP层进行下一个n的预测。,https://github.com/vllm-project/vllm/issues/12755
vllm,"这是一个用户提出需求的issue，主要涉及的对象是vLLM项目的前端，涉及的内容是添加""User Defined Custom Tool Calling""解析器。",https://github.com/vllm-project/vllm/issues/12752
vllm,这是一个文档更新类型的issue，主要涉及开发者对贡献内容进行审核时需要引导到Slack交流，原因可能是需要更多人关注和审阅。,https://github.com/vllm-project/vllm/issues/12748
vllm,这个issue属于需求提出类型，涉及的主要对象是添加metrics支持，用户提出了关于v1设计文档的需求。,https://github.com/vllm-project/vllm/issues/12745
vllm,这个issue属于优化建议类型，主要涉及构建依赖项的管理，以避免不必要的依赖。,https://github.com/vllm-project/vllm/issues/12739
vllm,这是一个用户提出需求的issue，主要对象为VLLM的音频转录功能。用户询问为什么只能转录音频的前部分，是否正常或者是否有其他替代方案。,https://github.com/vllm-project/vllm/issues/12738
vllm,这是一个功能需求的issue，主要涉及的对象是`offline_inference`中的多个示例脚本。由于这些示例脚本在逻辑和功能上几乎相同，为了减少混乱，优化体验，增加可配置性和提供更好的使用指引，需要将它们合并为单个`basic`示例。,https://github.com/vllm-project/vllm/issues/12737
vllm,这是一个更新请求，涉及的主要对象是版本控制。,https://github.com/vllm-project/vllm/issues/12736
vllm,这是一个用户提出需求的类型issue，主要涉及VLLM Server的功能定制问题，用户想要实现在使用VLLM服务器时应用自定义评分脚本的需求。,https://github.com/vllm-project/vllm/issues/12733
vllm,这个issue是一个用户提出的需求类型的问题，主要涉及vLLM workers的精细化控制与Ray actor共享GPU的功能需求。,https://github.com/vllm-project/vllm/issues/12732
vllm,这是一个用户提出需求的类型，主要涉及DeepSeek项目支持多标记预测的计划，用户希望通过推测性解码实现。,https://github.com/vllm-project/vllm/issues/12730
vllm,这个issue属于需求提出类型，提及需要将MLA与纯RoPE支持添加到Deepseek-VL2模型中。,https://github.com/vllm-project/vllm/issues/12729
vllm,这个issue是关于代码重构和类型注解的改进，不属于bug报告类型，主要涉及到`TransformersModel`中`Linear`处理的优化。原因可能是为了代码可读性和维护性的考虑。,https://github.com/vllm-project/vllm/issues/12727
vllm,这个issue类型是功能需求提出，主要涉及的对象是vLLM Server。由于需要更多指标，用户提出了更新和改进的版本供讨论。,https://github.com/vllm-project/vllm/issues/12726
vllm,这是一个feature请求，主要涉及V1引擎不支持2080ti GPU的FA版本，用户请求V1引擎支持Xformers。,https://github.com/vllm-project/vllm/issues/12724
vllm,这是一个针对代码优化的issue，主要涉及多模态输入数据在特定情况下的复制操作优化。,https://github.com/vllm-project/vllm/issues/12722
vllm,这个issue类型是升级请求，涉及的主要对象是vllm项目下的torch库。由于升级中CPU被IPEX阻塞，导致需要单独升级其他硬件后端。,https://github.com/vllm-project/vllm/issues/12721
vllm,这是一个功能需求或者功能开发的issue，涉及vllm的核心功能中关于pipeline parallel async execution loop的开发。,https://github.com/vllm-project/vllm/issues/12720
vllm,这个issue属于用户提出需求，主要是添加对Phi3模型中部分旋转嵌入的支持。由于需要使用部分旋转嵌入，在Llama模型中添加了相应的更新，保证了现有配置的向后兼容性。,https://github.com/vllm-project/vllm/issues/12718
vllm,该问题属于用户提出需求类型，主要涉及如何在`task=classify`中获取类标签的问题，用户想知道在使用`LLM.classify`时如何获得类标签或者是否需要自行维护一个基于模型的`class_labels`列表。,https://github.com/vllm-project/vllm/issues/12715
vllm,这是一个功能需求的issue，主要涉及的对象是DeepSeek MoE模型在CPU上的支持。由于Torch未编译启用CUDA，导致用户无法在CPU模式下运行DeepSeekV2LiteInstruct模型。,https://github.com/vllm-project/vllm/issues/12714
vllm,这是一个性能优化类型的issue，涉及到ROCM、AMD、TRITON以及vllm项目。由于减少warps数量用于预加载以减少溢出导致的性能提升，同时产生了一个较小的问题：内核时间降低了对总执行时间的影响。,https://github.com/vllm-project/vllm/issues/12713
vllm,这是一个文档更新类的issue，涉及的主要对象是将`ibmfms`改为`ibmaiplatform`。这个问题由于命名变更可能导致旧名称解析的持续时间而引起相关测试失败的情况。,https://github.com/vllm-project/vllm/issues/12709
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加Triton配置以支持DeepSeekV3在B200设备上的使用。,https://github.com/vllm-project/vllm/issues/12707
vllm,这是一个需求提出的RFC类型的issue，涉及的主要对象是API server的性能优化。由于API server目前在单个进程中运行，利用一个CPU进行处理，在GPU变得更快的情况下，需要将API server扩展到多个CPU以确保处理请求速度足够快，以保持GPU资源的充分利用。,https://github.com/vllm-project/vllm/issues/12705
vllm,这是一个功能改进类型的issue，主要涉及 P3L.py & P3L_mling.py 的测试批量查询功能改进。原因是为了使测试过程更加高效且减少测量噪声。,https://github.com/vllm-project/vllm/issues/12701
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是AMD GPUs。由于需要支持完全透明的睡眠模式，用户提出了关于API，清除GPU内存中所有KV缓存的问题。,https://github.com/vllm-project/vllm/issues/12695
vllm,该issue类型为用户提出需求，主要对象是关于在vllm中使用torch.compile优化推理性能的问题。由于用户对于Qwen2VL模型的torch.compile支持情况不清楚，导致提出了相关问题并寻求帮助。,https://github.com/vllm-project/vllm/issues/12693
vllm,这是一个用户提出需求的issue，主要涉及的对象是V1版本的代码。由于缺乏`SamplingParams.logits_processors`支持，用户希望添加这一功能来完善V1版本。,https://github.com/vllm-project/vllm/issues/12688
vllm,该issue类型为功能需求，主要对象是支持在DeepseekV2模型中添加密集MLP和RoPE功能。可能出现此功能需求是为了与Huggingface建模代码保持兼容。,https://github.com/vllm-project/vllm/issues/12686
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是为vllm项目添加Helm chart release workflow。,https://github.com/vllm-project/vllm/issues/12685
vllm,这个issue类型是需求提议，主要对象是tokenize api。原因是用户希望在tokenize api中添加工具定义。,https://github.com/vllm-project/vllm/issues/12684
vllm,这是一个性能优化的issue，主要涉及到MLA（Multi-layer attention）模型，通过调整KV缓存的对齐方式来提高性能。,https://github.com/vllm-project/vllm/issues/12676
vllm,这个issue类型属于功能改进，涉及主要对象为scheduler（调度器）。由于移除了对部分请求处理的限制，可能导致频繁对调度请求进行更改影响性能，特别是对持久批处理的效果和输入准备开销产生影响。,https://github.com/vllm-project/vllm/issues/12674
vllm,这是一个功能增强和依赖更新的issue，主要涉及到升级actions/setup-python插件版本。,https://github.com/vllm-project/vllm/issues/12672
vllm,该issue类型为用户提出需求，涉及的主要对象是vLLM的用户和开发者。由于slack变得更加成熟，导致提议废弃Discord并鼓励使用vLLM Slack。,https://github.com/vllm-project/vllm/issues/12668
vllm,这个issue是用户需求问题，主要涉及VLLM_HOST_IP参数的应用，由于多节点推理的使用频繁导致需求更清晰的消息给用户。,https://github.com/vllm-project/vllm/issues/12667
vllm,"该issue类型是功能添加，涉及主要对象是在ROCm上启用DeepSeek模型。这个功能添加是由于需要使用Triton MLA后端在ROCm上启用DeepSeek模型，同时需要确保有""trust_remote_code""标志位。",https://github.com/vllm-project/vllm/issues/12662
vllm,这个issue类型是用户提出需求。主要涉及的对象是添加新模型YuE。由于文本生成和音频模型输入输出的差异，用户在问如何支持他们所希望的模型。,https://github.com/vllm-project/vllm/issues/12658
vllm,这个issue属于功能更新类型，主要涉及的对象是Github上的VLLM项目中与混合内存分配器相关的代码。由于需要支持混合模型的内存分配并优化不同类型的注意力层，这个功能更新提出了新的抽象概念和配置来实现多层共享KV缓存内存池的目的。,https://github.com/vllm-project/vllm/issues/12655
vllm,这是一个用户提出需求的issue，主要对象是针对MLA（Machine Learning Accelerator）模块的KV Splits heuristic。,https://github.com/vllm-project/vllm/issues/12654
vllm,这是一个优化建议，主要涉及的对象是`ConstantList`的创建过于频繁，导致性能可能受到一定影响。,https://github.com/vllm-project/vllm/issues/12653
vllm,这是一个需求提出的issue，主要涉及对象是在AWS SageMaker上运行vLLM时需要考虑HTTP头部信息来跟踪推断请求。,https://github.com/vllm-project/vllm/issues/12652
vllm,该issue属于需求提出类型，涉及添加多个请求定时直方图，主要对象是系统中的度量指标。由于需要捕获引擎核心中特定事件之间的精确先请求时机间隔，需要前端进程记录和计算时间戳事件之间的时间间隔。,https://github.com/vllm-project/vllm/issues/12644
vllm,该issue类型为需求提出，主要涉及的对象是改进批量LLM以在离线场景中更好地利用共享前缀，但是目前内容的描述较为简单，需要参考RFC Batchllm以获取更多详细信息。,https://github.com/vllm-project/vllm/issues/12641
vllm,这个issue属于功能增强请求，涉及的主要对象是vllm项目中的Apple Metal支持。由于尝试使用PyTorch MPS回退模式实现Metal支持，导致目前生成的文本不合理，需要进一步调试和改进。,https://github.com/vllm-project/vllm/issues/12640
vllm,这个issue类型为提出需求，涉及的主要对象是MLA with chunked prefill算法，由于需要进一步的基准测试以确定是否将其默认设置为V0版本，同时为V1版本的实现奠定基础。,https://github.com/vllm-project/vllm/issues/12639
vllm,这个issue类型是需求关联，涉及的主要对象是MLA功能。由于需要先完成该文档https://github.com/vllmproject/vllm/pull/12601，导致需要禁用块预填充和/或前缀缓存。,https://github.com/vllm-project/vllm/issues/12638
vllm,这是一个性能优化类的issue，主要涉及到在DeepSeek模型中应用`torch.compile`来提高生成速度，原因是模型中的大量专家导致了生成延迟的问题。,https://github.com/vllm-project/vllm/issues/12637
vllm,这个issue是关于功能改进的，主要涉及模型中的量化和融合模块映射的优化。由于融合模块的处理逻辑需要改进，并且量化配置需要从模型中获取映射信息，导致目前需要依赖硬编码的方式来确定模块映射关系，代码逻辑复杂，需要进行优化。,https://github.com/vllm-project/vllm/issues/12634
vllm,该issue属于用户需求类型，主要涉及提出添加一个输入步骤以请求发布版本的功能。这是因为用户不想再需要将发布版本放入环境变量中创建新的构建。,https://github.com/vllm-project/vllm/issues/12631
vllm,这是一个建议类型的issue，主要对象是python源文件。由于Linux Foundation建议项目中添加SPDX许可证头部信息，并且工具扫描代码时判断缺少这些信息会导致问题，因此提出了这个issue。,https://github.com/vllm-project/vllm/issues/12628
vllm,这个issue类型是用户提出需求，涉及的主要对象是在vLLM Server中添加额外的指标。,https://github.com/vllm-project/vllm/issues/12627
vllm,这是一个功能需求的issue，主要涉及对象是TPU backend中的MultiLoRA实现。,https://github.com/vllm-project/vllm/issues/12623
vllm,这个issue是一个用户提出需求的类型，主要涉及的对象是Reasoning models，需要在回答中只应用Guided/Structured grammar。这个需求的原因是希望让Reasoning models在输出结构化内容时更有效。,https://github.com/vllm-project/vllm/issues/12619
vllm,这是一个用户提出需求的issue，主要涉及的对象是在s390x架构上添加CPU推理支持。,https://github.com/vllm-project/vllm/issues/12613
vllm,这是一个用户提出需求的类型issue，主要涉及文档页面的美化。,https://github.com/vllm-project/vllm/issues/12611
vllm,这个issue类型为功能需求，主要对象是LoRA适配器。用户提出了需求，希望允许LoRA适配器通过内存中的张量字典进行指定。,https://github.com/vllm-project/vllm/issues/12609
vllm,这是一个用户提出需求的类型的issue，该问题单涉及的主要对象是创建docker镜像文件。,https://github.com/vllm-project/vllm/issues/12606
vllm,这是一个特性需求类型的github issue，主要关注的对象是vllm服务，用户希望实现将推理和答案分开的功能。可能是为了避免手动处理API响应所带来的错误和繁琐。,https://github.com/vllm-project/vllm/issues/12602
vllm,该问题为功能需求，要求支持MLA算法与FP8计算，其中提到需要先实现另一个相关的pull request。,https://github.com/vllm-project/vllm/issues/12601
vllm,这是一个更新依赖关系的issue，涉及的对象是CI dependencies。,https://github.com/vllm-project/vllm/issues/12599
vllm,该issue类型为需求提议，主要对象是为V1版本创建设计文档中的特定部分，并添加前缀缓存设计文档。由于项目需要V1版本的设计文档，并希望加入前缀缓存设计，故提出该需求。,https://github.com/vllm-project/vllm/issues/12598
vllm,这是一个用户提出需求的issue，主要涉及MLA在deepseek v3/r1的实现，可能因代码合并导致问题。,https://github.com/vllm-project/vllm/issues/12597
vllm,这个issue类型是需求提出，主要对象是Qwen2.5-VL。由于完全缺少具体内容，用户可能正在草拟关于Qwen2.5-VL的提案或要求。,https://github.com/vllm-project/vllm/issues/12596
vllm,这是一则功能改进的issue，主要涉及ROCm中的使用HIP FP8 header，用于抽象硬件指令或软件模拟的fp8转换过程。,https://github.com/vllm-project/vllm/issues/12593
vllm,这是一个功能需求类型的issue，涉及添加GPU prefix cache hit rate % gauge。原因可能是为了添加更精确的GPU缓存命中率指标来监控系统性能。,https://github.com/vllm-project/vllm/issues/12592
vllm,这是一个关于在Triton FAv2 kernel中添加fp8和可变长度序列支持的特性需求，主要涉及的主要对象是Triton FAv2 kernel。由于性能需求和新特性增加，开发人员需要在内核中实现此功能，从而改善模型的运行性能。,https://github.com/vllm-project/vllm/issues/12591
vllm,这是一个用户提出需求的issue，主要对象是Triton FAv2 kernel，用户希望添加对变长序列的支持。,https://github.com/vllm-project/vllm/issues/12589
vllm,这个issue属于功能增强类型，主要涉及到CUDA图支持，旨在为Lucas的PR实现triton MLA attention kernel添加CUDA图支持。,https://github.com/vllm-project/vllm/issues/12588
vllm,这是一个需求类型的issue，主要涉及的对象是集成了blockquantized CUTLASS kernels的线性层（linear layers）。,https://github.com/vllm-project/vllm/issues/12587
vllm,这是一个文档更新的issue，主要涉及int4 w4a16量化范例的添加，用户提出了添加选择正确量化方案和音频校准数据集等改进建议。,https://github.com/vllm-project/vllm/issues/12585
vllm,这是一个功能需求类型的issue，主要涉及了vLLM下的Expert Parallelism（EP）支持问题。由于目前的vLLM执行仅在运行MoE模型时支持TP，因此用户提出了添加EP支持的需求。,https://github.com/vllm-project/vllm/issues/12583
vllm,这是一个功能需求的issue，涉及到添加一个新的计数器和枚举类型，并解决了一些TODO。,https://github.com/vllm-project/vllm/issues/12579
vllm,这是一个用户提出需求的类型，主要涉及性能回归检查。原因是为了快速检查PR，以排除可能被忽视的重大性能回归。,https://github.com/vllm-project/vllm/issues/12576
vllm,这是一个关于文档改进的issue，主要涉及设备标签和安装指引的优化。,https://github.com/vllm-project/vllm/issues/12575
vllm,这个issue是一个用户需求类型的问题，主要涉及到V1版本的日志记录功能，由于需要将V0版本的最大请求并发日志迁移到V1版本。,https://github.com/vllm-project/vllm/issues/12569
vllm,这是一个需求提出的issue，主要涉及的对象是添加GPU缓存使用率的仪表盘。由于在V1版本中默认启用了前缀缓存，因此对CPU缓存使用率并不相关。,https://github.com/vllm-project/vllm/issues/12561
vllm,这是一个用户提出需求的issue，主要涉及VLLM在Mac上的静态分发问题。用户反映由于Python解释器和Torch版本的限制，无法使用VLLM，希望类似于静态Golang或Rust应用程序进行分发。,https://github.com/vllm-project/vllm/issues/12556
vllm,这是一个更新依赖项的问题，主要涉及torch、torchvision和torchaudio的ppc64le平台要求。,https://github.com/vllm-project/vllm/issues/12555
vllm,这是一个功能改进类型的issue，主要涉及到移动`requirements.txt`文件至`requirements/x.txt`目录并更新所有对需求文件的引用。,https://github.com/vllm-project/vllm/issues/12547
vllm,这是一个需求提出类型的issue，主要对象是V1 scheduler interface，由于尚未填写具体内容，无法分析导致的具体问题。,https://github.com/vllm-project/vllm/issues/12544
vllm,这个issue是一个请求添加新模型支持的类型，主要对象是Janus Pro 7B模型。由于目前VLLM不支持该模型，用户提出了需要该模型的支持的需求。,https://github.com/vllm-project/vllm/issues/12538
vllm,这个issue类型是改进请求，主要对象是V1版本中的不支持的配置，用户希望改进错误消息提供更清晰的信息。,https://github.com/vllm-project/vllm/issues/12535
vllm,这个issue属于功能需求类型，该问题单涉及的主要对象是Triton FAv2 kernel，用户正在寻求添加fp8和int8支持的帮助。,https://github.com/vllm-project/vllm/issues/12534
vllm,这是一个用户提出需求的issue，涉及的主要对象是针对新模型架构的支持。用户希望添加一个新的模型架构至vllm中，导致该需求的提出。,https://github.com/vllm-project/vllm/issues/12532
vllm,这是一个功能需求，主要涉及TPU推理效率的性能优化，用户希望添加一个针对TPU推理的性能分析示例。,https://github.com/vllm-project/vllm/issues/12531
vllm,这是一个关于添加TTFT和TPOT直方图的需求问题，涉及到代码中Metrics模块的修改。,https://github.com/vllm-project/vllm/issues/12530
vllm,这个issue类型是优化提案，主要涉及MLA解码优化，由于原始的MHA计算方式较慢，故提议使用潜变量来计算MQA以提高效率。,https://github.com/vllm-project/vllm/issues/12528
vllm,这是一个用户提出的需求类型的issue，主要涉及Buildkite系统中CI的优化。该需求是为了节省CI成本，使得CI只有在precommit通过时才运行。,https://github.com/vllm-project/vllm/issues/12527
vllm,这个issue是一个功能增强类型的问题。该问题单涉及的主要对象是VLLM PDDisagg scenario。由于需要在长上下文场景下提高性能，因此提出了layerwise KV transfer解决方案。,https://github.com/vllm-project/vllm/issues/12523
vllm,这是一个需求类型的issue，主要涉及添加 ModelOpt FP4 Checkpoint 支持，需要修复 Modelopt 模型加载关于 kvscales 的问题。,https://github.com/vllm-project/vllm/issues/12520
vllm,这是一个新功能需求的issue，涉及到Kernel的支持问题，由于需要为NVFP4数据类型和量化内核添加gemms，其主要目的是为了支持NVFP4类型。,https://github.com/vllm-project/vllm/issues/12519
vllm,该issue是一个功能需求提案，主要涉及的对象是vLLM的tokenizer组件。,https://github.com/vllm-project/vllm/issues/12518
vllm,这个issue是一个需求提出类型的问题，主要涉及的对象是指标(metrics)系统，由于需要跟踪每个请求的生成token数量，需要添加请求级别的prompt/generation_tokens直方图，以处理流式增量更新。,https://github.com/vllm-project/vllm/issues/12516
vllm,该issue类型是用户提出需求，主要涉及的对象是添加janus pro support for multi model。原因是用户希望增加这一功能，以支持多模型的需求。,https://github.com/vllm-project/vllm/issues/12512
vllm,这个issue是一个用户需求类型的问题，主要涉及的对象是支持torch.distributed作为多节点推断的运行时，由于目前仅支持Raybased分布式推断，用户希望增加对torch.distributed的支持。,https://github.com/vllm-project/vllm/issues/12511
vllm,这个issue是一个改进提案，涉及的主要对象是`OpenAIServingCompletion`，旨在使`raw_request`参数在`ServingCompletion`中变为可选项。,https://github.com/vllm-project/vllm/issues/12503
vllm,这个issue是一个功能特性请求，主要涉及支持ROCm 6.3及更高版本和GPU Arch MI300及更高版本的PerTokenActivation PerChannelWeight FP8量化推理，其中主要解决了模型量化和性能提升的问题。,https://github.com/vllm-project/vllm/issues/12501
vllm,这是一个功能需求的issue，主要涉及到支持PerTokenActivation PerChannelWeight FP8 Quantization Inferencing。原因是为了增强模型在ROCm 6.3及以后版本以及GPU Arch MI300及以后版本的推断功能。,https://github.com/vllm-project/vllm/issues/12499
vllm,该issue类型为用户提出需求，主要对象是引擎日志。导致此需求的原因可能是用户反馈希望在日志中明确展示当前使用的版本号。,https://github.com/vllm-project/vllm/issues/12496
vllm,该issue类型为更新请求，涉及主要对象为项目的README.md文件。原因是项目发布了V1 alpha版本，需要更新README.md。,https://github.com/vllm-project/vllm/issues/12495
vllm,这个issue是一个需求提出类型，主要对象是针对新的模型Qwen2.5-VL，由于没有回应导致用户寻求支持。,https://github.com/vllm-project/vllm/issues/12486
vllm,这是一个需求类型的issue，主要涉及的对象是AMD的ROCm配置。原因可能是需要更新默认的闪回注意力至CK FA。,https://github.com/vllm-project/vllm/issues/12485
vllm,这是一个请求反馈（RFC）的类型，涉及主要对象是vLLM项目中添加Google TPU支持。原因是首次将另一个硬件后端添加到V1中并进行代码重构，需要讨论和反馈。,https://github.com/vllm-project/vllm/issues/12480
vllm,这个issue是用户提出需求类型，主要涉及Janus-Series中的Unified Multimodal Understanding and Generation Models。由于缺乏响应和额外的背景信息，用户提出了请求。,https://github.com/vllm-project/vllm/issues/12479
vllm,这是一个需求提出的issue，主要涉及的对象是将`IterationStats`传递给统计记录器，并在日志和Prometheus记录器中记录相关信息。产生该问题的原因是需要在日志统计记录器中基于特定日志间隔中的令牌数量计算吞吐量，在Prometheus记录器中仅需要记录提示和生成的令牌数量。,https://github.com/vllm-project/vllm/issues/12478
vllm,这个issue属于更新需求类型，主要涉及`pre-commit` hooks的linting packages，由于这些linting packages较旧，所以需要更新以符合最新版本要求。,https://github.com/vllm-project/vllm/issues/12475
vllm,这是一个用户提出需求的类型的issue，主要涉及vllm在使用systemd安全功能时的兼容性问题，可能由于相应的systemd安全设置与vllm运行所需设置不兼容而导致。,https://github.com/vllm-project/vllm/issues/12474
vllm,这个issue类型是文档修改需求，涉及将文档中的代码块标记由反引号 fences 转换为冒号 fences，目的是根据特定要求对文档格式进行统一处理。,https://github.com/vllm-project/vllm/issues/12471
vllm,该issue属于用户提出需求类型，主要涉及VLLM在GPU节点上的配置及性能优化。用户想要在拥有4块GPU的节点上配置VLLM，以获得最小延迟或最大吞吐量。导致此问题的原因是用户想要最优化地配置VLLM以提高性能。,https://github.com/vllm-project/vllm/issues/12470
vllm,这个issue是用户提出需求类型的，主要对象是API for reasoning models like DeepSeekR1，用户希望添加`reasoning_content`参数来支持推理模型，以便用户可以查看推理过程中的步骤。,https://github.com/vllm-project/vllm/issues/12468
vllm,这是一个发布更新的技术要求类issue，涉及到vllm项目的软件版本更新及相关功能支持。,https://github.com/vllm-project/vllm/issues/12465
vllm,这是一个更新请求类型的issue，主要涉及的对象是helm/chart-testing-action。由于需要更新ct至v3.12.0等组件版本，并新增e2e测试，导致更新至2.7.0的需求。,https://github.com/vllm-project/vllm/issues/12463
vllm,这个issue是关于更新依赖版本的问题，主要涉及到GitHub仓库中的actions/stale。原因可能是为了获取最新的功能或修复已知的问题。,https://github.com/vllm-project/vllm/issues/12462
vllm,这个issue属于需求提出类型，主要涉及的对象是vLLM的插件后端，由于需要在插件后端的集成测试中重用vLLM现有的本地单元测试，需要将vLLM现有的UTs中与cuda相关的函数泛化，以便支持外部设备的计数和环境变量的更改。,https://github.com/vllm-project/vllm/issues/12461
vllm,这是一个用户提出需求的类型，主要涉及到vLLM项目的文档内容。由于缺少vLLM博客的链接，用户请求在文档中添加以增加博客的可见性。,https://github.com/vllm-project/vllm/issues/12460
vllm,这是一个需求类型的issue，涉及的主要对象是前端（FrontEnd），由于未明确指出具体问题或需求细节，用户可能提出了关于支持Whisper Transcription Protocol的需求或问题。,https://github.com/vllm-project/vllm/issues/12458
vllm,这是一个优化建议，主要涉及输入准备过程中避免使用Python列表操作。,https://github.com/vllm-project/vllm/issues/12457
vllm,这是一个性能优化类型的issue，涉及到V1版本的功能改进，主要目标是提高输出IO性能以确保与GPU执行的重叠。,https://github.com/vllm-project/vllm/issues/12456
vllm,这是一个用户提出需求类型的issue，涉及的主要对象是update_from_output函数。由于需要对update_from_output函数做一些微小的优化，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/12454
vllm,这是一个功能需求提案，主要涉及到VLM数据集采样函数在benchmark_serving.py中的复杂性问题。,https://github.com/vllm-project/vllm/issues/12447
vllm,这个issue属于用户提出需求类型，主要涉及对新模型“IDEA-Research/ChatRex-7B”的支持。原因在于vllm当前不支持具有特定体系结构和输入类型要求的模型，导致用户无法使用他们想要的模型。,https://github.com/vllm-project/vllm/issues/12444
vllm,这个issue是用户提出需求，请求支持深度搜索R1 GGUF 4位(Q4KM)。由于目前系统显示不支持该配置，用户希望获得相关支持。,https://github.com/vllm-project/vllm/issues/12436
vllm,这是一个用户提出需求的issue，主要对象是为AMD MI25/50/60添加支持。由于原先没有对这些旧的AMD GCN5架构显卡提供支持，用户通过添加两行代码实现了vllm对AMD MI60显卡的正常工作。,https://github.com/vllm-project/vllm/issues/12431
vllm,该issue是关于功能需求的，主要涉及前端的run_batch功能，添加了对score请求的支持，之前只支持chat completions和embeddings requests。,https://github.com/vllm-project/vllm/issues/12430
vllm,这个issue类型为功能添加需求，主要涉及vLLM在Hopper GPUs上Flash Attention 3（FA3）的支持，需要实现对SM 8.9和8.6的优化以及硬件支持。原因是在8.6和8.9上由于共享内存不足导致当前实现被完全禁用，需要进一步开展相关工作。,https://github.com/vllm-project/vllm/issues/12429
vllm,这个issue类型是用户提出需求，询问关于在vllm中如何使用聊天模板的问题。,https://github.com/vllm-project/vllm/issues/12423
vllm,这是一个需求提出类型的issue，主要涉及ROCm和AMD的模型llama 3.2的支持问题。,https://github.com/vllm-project/vllm/issues/12421
vllm,这是一个用户提出需求的issue，主要涉及ROCM llama 3.2的支持向上游推进。由于需要支持ROCM架构上的多模式llama 3.2，用户提出将相关代码合并到上游的请求。,https://github.com/vllm-project/vllm/issues/12419
vllm,该issue属于功能增强，主要涉及到对disaggregated prefill的离线测试。由于需要验证disaggregated prefill使用情况，因此增加了该离线测试。,https://github.com/vllm-project/vllm/issues/12418
vllm,这是一个实现功能的issue，主要涉及到日志记录器在V1版本中的初步实现。由于之前采用了多进程模式，导致一些指标数据丢失，现在需要恢复这些指标数据。,https://github.com/vllm-project/vllm/issues/12416
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM项目中的使用统计功能。,https://github.com/vllm-project/vllm/issues/12414
vllm,这是一个用户提出需求的 issue，主要涉及到如何在vLLM中使用`meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8`模型。用户希望能够将该模型用于较大模型的推理，但由于可能不支持该量化方案，导致运行时出现错误。,https://github.com/vllm-project/vllm/issues/12411
vllm,这是一个特性需求，涉及前端支持在参数中重写生成配置的功能。,https://github.com/vllm-project/vllm/issues/12409
vllm,这是一个功能需求类型的issue，主要涉及的对象是AMD MI300，用户提出了添加针对该硬件的tuned moe配置。,https://github.com/vllm-project/vllm/issues/12408
vllm,这是一个关于需求咨询的问题单，主要涉及到使用vLLM在服务Llama 3.370b模型给多个用户时的配置和性能问题。原因可能是用户在文档中未找到关于该模型使用的详细信息。,https://github.com/vllm-project/vllm/issues/12400
vllm,该issue属于用户提出需求类型，主要涉及集成SVDquant（W4A4 quantization）quantization方法到vLLM。用户提出这个需求是因为希望在LLM serving场景中通过该quantization方法实现模型优化和提高内存效率。,https://github.com/vllm-project/vllm/issues/12399
vllm,这是一个用户提出的需求。该问题涉及到vllm在Ascend NPU backend中对变长输入的支持。,https://github.com/vllm-project/vllm/issues/12397
vllm,这是一个功能需求类型的issue，主要涉及对象是Intel GPU。由于需要添加XPU bf16支持，用户提出了对该功能的需求。,https://github.com/vllm-project/vllm/issues/12392
vllm,这是一个用户提出需求的问题，涉及的主要对象是使用vllm来服务gguf模型只使用CPU时出现的错误。由于用户不清楚如何将特定模型集成到vllm中，导致需要帮助解决。,https://github.com/vllm-project/vllm/issues/12391
vllm,这个issue是用户提出需求。主要对象是VLM模型在VisionArena Dataset上的性能测试。,https://github.com/vllm-project/vllm/issues/12389
vllm,"该issue是关于""Support for Structured Outputs""的需求提出，主要对象是V1版本的engine。原因是目前V1版本的功能虽然部分实现了结构化输出支持，但与V0版本的支持还不完整，在speculative decoding方面存在一些问题，可能需要进一步的工作和优化。",https://github.com/vllm-project/vllm/issues/12388
vllm,这是一个用户提出需求的issue，主要涉及mistralai/Ministral-8B-Instruct-2410模型配置的context length限制问题。造成这个问题的原因是当前的模型配置只允许最大长度为32768，而用户需要增加到128k context length。,https://github.com/vllm-project/vllm/issues/12385
vllm,这个issue属于需求变更类型，主要涉及到移除与两个metrics相关的过时代码，原因是这些metrics已被标记为过时，并且已经在之前的变更中提到。,https://github.com/vllm-project/vllm/issues/12383
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM下的Whisper，用户希望添加BNB量化。,https://github.com/vllm-project/vllm/issues/12381
vllm,这个issue类型是用户提出需求，主要涉及如何在vllm中使用回归任务，用户询问如何使用vllm进行回归任务推断。,https://github.com/vllm-project/vllm/issues/12379
vllm,这是一个用户提出需求的issue，主要对象是V1版本的API server以及H100和H200模型，由于2K token budget限制导致了对H100和H200模型性能的影响。,https://github.com/vllm-project/vllm/issues/12369
vllm,该issue属于功能需求提案，主要涉及对象是VLLM模型的日志处理线程设置，并提出了一个新的环境变量`VLLM_LOGITS_PROCESSOR_THREADS`，旨在通过多线程方式优化VLLM模型在大批量数据情况下的性能以增加GPU利用率和降低ITL，特别对于那些需要额外启动CUDA核心或在不持有Python全局解释器锁时执行大量CPU绑定工作的情况。,https://github.com/vllm-project/vllm/issues/12368
vllm,这是一个需求类型的issue，涉及更新compressed-tensors版本，用户希望更新到v0.9.0版本。,https://github.com/vllm-project/vllm/issues/12367
vllm,这个issue是关于提出需求的，主要涉及到vLLM项目中的配置格式和加载格式。由于存在一些不常见的使用情况没有被当前代码覆盖，因此提出了RFC以实现自定义配置和权重格式以及从自定义存储后端加载配置和权重的功能。,https://github.com/vllm-project/vllm/issues/12363
vllm,这是一个需求文档的issue，主要涉及的对象是Phi-4支持文档。,https://github.com/vllm-project/vllm/issues/12362
vllm,这个issue类型属于用户提出需求，并涉及支持Microsoft/Phi-4 Model。由于不太明白如何添加新模型，用户请求支持该模型。,https://github.com/vllm-project/vllm/issues/12358
vllm,这个issue属于功能需求类型，主要涉及的对象是benchmark script，用户提出了关于启用代理支持的需求。,https://github.com/vllm-project/vllm/issues/12356
vllm,这是一个用户提出需求的issue，涉及ViT MHA层添加FA2支持的功能更新。,https://github.com/vllm-project/vllm/issues/12355
vllm,这是一个优化性质的issue，主要涉及简化`mrope_positions`构建，原因为发现了一种简化方法。,https://github.com/vllm-project/vllm/issues/12352
vllm,这是一个用户提出的关于使用工具调用功能的问题，主要涉及到如何在auto模式下使用vllm工具。用户遇到的问题是当未指定工具时，无法正常使用。,https://github.com/vllm-project/vllm/issues/12349
vllm,这是一个功能优化的Issue，主要涉及到ROCm/vllm中关于Custom Paged Attention的性能优化。,https://github.com/vllm-project/vllm/issues/12348
vllm,该issue类型为需求提交，主要对象是文档。由于用户希望添加meetup幻灯片到文档中，因此发起了该需求。,https://github.com/vllm-project/vllm/issues/12345
vllm,这是一个用户提出需求并请教问题的类型，主要涉及到vllm中的fp8 sparse gemm实现文件。用户询问了有关实现公式和数据排列的问题，可能是由于理解不清晰或需要进一步指导而提出的。,https://github.com/vllm-project/vllm/issues/12344
vllm,这个issue是一个功能增强（feature enhancement），主要涉及到FLOP计数的功能增加，因为用户在LLM推断过程中需要统计每个操作的FLOPS以及GPU上的性能限制。,https://github.com/vllm-project/vllm/issues/12341
vllm,这是一个关于如何在vllm服务中记录传入请求（输入和输出）的问题，类型是用户提出需求。该问题涉及到vllm的服务日志记录功能。由于当前使用`vllm serve`命令无法直接记录请求，用户寻求帮助如何实现记录请求的功能。,https://github.com/vllm-project/vllm/issues/12336
vllm,这是一个关于功能使用的issue，涉及主要对象为SPMD和Prefix Cache，用户可能遇到由于模型大小与版本兼容性问题而导致的困惑。,https://github.com/vllm-project/vllm/issues/12335
vllm,这是一个需求更新的issue，主要涉及更新TPU CI脚本和使用torchxla nightly版本。,https://github.com/vllm-project/vllm/issues/12334
vllm,该issue是一个功能请求，主要涉及kv cache manager中的`.uncache_blocks`添加，目的是在给定请求的`num_computed_tokens`减少时取消缓存块。,https://github.com/vllm-project/vllm/issues/12333
vllm,这个issue类型为功能需求，主要涉及对象是添加interleave sliding window功能。这个需求可能是由于提高数据处理性能或实现特定功能要求而提出的。,https://github.com/vllm-project/vllm/issues/12331
vllm,这是一个关于模型配置与RoPE（Positional Encoding）和Sliding Windows处理文本长度的比较与优化的issue。,https://github.com/vllm-project/vllm/issues/12328
vllm,这个issue类型是需求类问题，涉及主要对象是文档的漏洞披露流程。由于缺乏明确的操作顺序，导致漏洞修复和公告发布流程不清晰。,https://github.com/vllm-project/vllm/issues/12326
vllm,这是一个优化Cross-Attention QKVParallelLinear计算的Issue，提出了针对当前suboptimal的QKV投影进行优化的建议。,https://github.com/vllm-project/vllm/issues/12325
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm项目下的pre-commit安装。由于安装过程速度较慢，用户希望能够默认使用uv来提升速度。,https://github.com/vllm-project/vllm/issues/12324
vllm,这个issue属于用户需求类问题，主要涉及对象是VLLM生成速度是否能通过添加另一块视频卡来提升。用户询问是因为在使用两块视频卡进行模型推理时，并没有提升生成速度，想寻求加速生成速度的方法。,https://github.com/vllm-project/vllm/issues/12322
vllm,这个issue属于性能优化提议，主要关注vLLM在A100设备上无法达到宣称的吞吐量和延迟结果，可能由于性能回归导致问题。,https://github.com/vllm-project/vllm/issues/12315
vllm,这是一个优化建议的issue，主要对象是VLM中的tokenization过程。由于重复对`mm_token`进行tokenization导致运行测试耗时较长，提出了优化建议。,https://github.com/vllm-project/vllm/issues/12310
vllm,这个issue是关于功能需求的，主要涉及Eagle Spec Decode的简化使用，原因可能是现有功能较为复杂，用户希望简化使用流程。,https://github.com/vllm-project/vllm/issues/12304
vllm,该issue类型为特性请求，主要涉及的对象是启用Mixtral的动态MoE功能。由于用户想要测试精度，因此提出了启用Dynamic MoE的特性请求。,https://github.com/vllm-project/vllm/issues/12303
vllm,这是一个需求提议类型的issue，主要涉及Disaggregated prefill与pipeline parallelism的兼容性问题。该问题源于当前逻辑只支持假设中间隐藏状态为张量，而在pipeline parallelism中可能为字典。,https://github.com/vllm-project/vllm/issues/12301
vllm,该issue类型为功能改进，涉及主要对象为vLLM的安装配置。由于安装vLLM时使用了`requirementscuda.txt`导致下载了许多与cuda相关的包，因此需要更新为使用`requirementscpu.txt`来避免这种情况。,https://github.com/vllm-project/vllm/issues/12299
vllm,该issue属于优化类型，主要涉及前端代码中的请求输出处理，问题是在高负载情况下，请求数据可能会累积导致处理速度变慢，通过合并输出来提高性能。,https://github.com/vllm-project/vllm/issues/12298
vllm,这是一个需求提交类型的issue，主要涉及DeepSeek-R1工具选择和功能调用，用户希望支持工具选择和chattemplate以获得更好的结果。,https://github.com/vllm-project/vllm/issues/12297
vllm,这是一个功能改进的issue，主要涉及Kernel和paged_attention，并由于需要解决一些模型的采纳问题而提出。,https://github.com/vllm-project/vllm/issues/12294
vllm,这是一个用户提出需求的issue，主要对象是vLLM-Llama-2-7b-hf，由于环境信息显示的PyTorch版本与CUDA信息不匹配，可能导致用户想了解vLLM-Llama-2-7b-hf中使用了哪些操作符。,https://github.com/vllm-project/vllm/issues/12293
vllm,这个issue类型是用户提出需求，询问是否支持从Google Cloud Storage bucket加载模型，主要涉及对象是vllm中的Model Streamer功能。由于文档中只提到支持从AWS S3存储加载模型，用户想知道是否支持从GCS加载模型。,https://github.com/vllm-project/vllm/issues/12290
vllm,这是一个用户提出需求的问题，主要涉及并发实现，由于同时调用openai api导致无法同时回答问题。,https://github.com/vllm-project/vllm/issues/12289
vllm,这个issue是关于性能优化的需求，主要涉及前端服务的在线性能改进，通过对输出处理和GC优化来提升性能。,https://github.com/vllm-project/vllm/issues/12287
vllm,这个issue属于需求反馈类型，主要涉及核心功能中的指标统计。导致该需求提出的原因是为了实现自动扩展策略时能够正确计算等待队列中的令牌数。,https://github.com/vllm-project/vllm/issues/12286
vllm,这是一个关于优化的问题，主要涉及了Disaggregated Prefilling和KV Cache Transfer，由于不是所有批处理请求中的KV缓存都能收到，导致之前decode节点针对所有请求进行了预填充，现在希望只对那些缺少KV缓存的请求进行预填充。,https://github.com/vllm-project/vllm/issues/12285
vllm,这是一个功能需求类型的issue， 主要涉及的对象是vllm代码库中的prefix cache，问题是关于支持`reset_prefix_cache`功能。,https://github.com/vllm-project/vllm/issues/12284
vllm,这是一个用户提出需求的issue，主要涉及添加KV Cache Metrics到Usage Object，用户希望能更好地了解工作负载中缓存的令牌情况。,https://github.com/vllm-project/vllm/issues/12283
vllm,该issue类型为文档更新，涉及的主要对象是为了添加关于预构建的ROCm vLLM docker用于性能验证目的的信息。,https://github.com/vllm-project/vllm/issues/12281
vllm,这是一个需求类型的issue，主要涉及自动化添加标签功能。这个问题的原因是希望根据PR涉及的文件自动应用相关标签。,https://github.com/vllm-project/vllm/issues/12280
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM，用户想知道vLLM是否支持MoE模型的投机性解码功能。,https://github.com/vllm-project/vllm/issues/12278
vllm,这是一个用户提出需求的issue，主要涉及文档中缺乏关于如何使用Quark来准备模型以及vLLM目前支持哪种类型的Quark量化方案的内容。,https://github.com/vllm-project/vllm/issues/12272
vllm,这是一个用户提出需求的类型，涉及的主要对象是NVIDIA Blackwell codegen，用户提出关于Blackwell B100/B200和Blackwell RTX 50 codegen版本的问题。,https://github.com/vllm-project/vllm/issues/12271
vllm,这是一个需求变更的issue，涉及的主要对象是更新 nightly torch 版本。其原因是由于 pytorch cuda 12.4 不再发布 arm64 版本的 wheels，因此需要更新到 cuda 12.6 版本。,https://github.com/vllm-project/vllm/issues/12270
vllm,这是一个用户提出需求的issue，主要涉及到VLM（Vision-Language Model），其内容表明用户希望简化替换信息的后处理过程。,https://github.com/vllm-project/vllm/issues/12269
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是encoder-decoder模型。该issue的提出主要是为了支持encoder-decoder模型的多步调度。,https://github.com/vllm-project/vllm/issues/12265
vllm,这是一个用户需求问题，主要涉及对使用OLMo模型进行工具调用的需求。由于缺乏所需的工具解析器，用户询问是否有可用的OLMo模型的工具解析器。,https://github.com/vllm-project/vllm/issues/12263
vllm,这个issue属于用户提出需求类型，主要涉及问题是在L20 GPUs上是否可以运行2:4 sparse fp8 quantized model，原因在于CUDA Compute Capability不符合要求。,https://github.com/vllm-project/vllm/issues/12262
vllm,该issue类型为Feature需求，主要涉及到torch dynamo的编译功能。由于设置编译级别为PIECEWISE(3)时无法使用用户指定的后端，用户希望实现支持传入用户指定后端以进行编译。,https://github.com/vllm-project/vllm/issues/12261
vllm,这是一个针对功能改进的问题，主要涉及的对象是PD分离中的前缀缓存支持。问题由于未能支持前缀缓存，导致了计算方式的错误。,https://github.com/vllm-project/vllm/issues/12257
vllm,这是一个功能需求（RFC）的issue，主要涉及了关于Sparse KV cache management framework的设计提议。由于需要在长上下文中清除低利用率的token，提出了一种灵活和高性能的KV缓存管理框架。,https://github.com/vllm-project/vllm/issues/12254
vllm,该issue类型是功能改进，涉及对象为构建器实例及注意力元数据构建器实例，由于当前每个批次都创建新的构建器实例，导致注意力元数据构建器无法访问全局信息，需要在每个批次准备时保存全局配置信息，解决在升级到flashinfer 0.2版本时，注意力元数据构建器无法访问全局信息的问题。,https://github.com/vllm-project/vllm/issues/12253
vllm,这是一个用户提出需求的issue，主要涉及到vLLM框架对BaichuanM1模型推理支持的新增。由于新增模型BaichuanM1在医疗领域表现出色，用户希望vLLM能够无缝处理该模型的推理，以优化在自然语言处理任务中的性能。,https://github.com/vllm-project/vllm/issues/12251
vllm,该issue类型是一个关于提出需求的特性请求，主要涉及到通过远程KV存储（如Redis）加载模型的功能。由于目前加载模型所花费的时间较长且涉及到多个步骤，用户提出了使用RemoteModelLoader类来直接从远程数据库加载模型，以更快地加载模型并避免涉及本地磁盘的方式。,https://github.com/vllm-project/vllm/issues/12250
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM模型的隐藏状态处理。由于目前使用Pooler提取隐藏状态的方法仍不够用户友好，并且缺乏对隐藏状态的自定义处理支持，用户提出了希望添加`HiddenStatesProcessor`来自定义处理隐藏状态的建议。,https://github.com/vllm-project/vllm/issues/12249
vllm,这个issue属于代码优化类，主要涉及的对象是 base model 中的 TypeVar，由于不再使用 `VllmConfig`，所以移除了与 HF config 相关的冗余 TypeVar。,https://github.com/vllm-project/vllm/issues/12248
vllm,这是一个特性需求的issue，主要涉及的对象是`tortch.compile`功能，反馈了添加更多日志信息的需求。,https://github.com/vllm-project/vllm/issues/12246
vllm,这是一个关于更名问题的需求类型的 issue，主要涉及名称为`MultiModalInputsV2`的对象。原因可能是在两个月前就已经提出更名请求，所以现在认为可以替换旧名称。,https://github.com/vllm-project/vllm/issues/12244
vllm,这是一个功能改进的issue，主要涉及到torch.compile中的compile sizes和cudagraph sizes的解耦。,https://github.com/vllm-project/vllm/issues/12243
vllm,这个issue是关于增加功能需求方面的，主要涉及到前端和服务器之间的最大生成token数设置。导致这个需求提出的原因可能是用户希望能够更灵活地控制生成token的数量。,https://github.com/vllm-project/vllm/issues/12242
vllm,这是一个文档更新类型的issue，涉及到FP8 KV Cache的文档细节更新。,https://github.com/vllm-project/vllm/issues/12238
vllm,这是一个用户提出需求的issue，主要涉及到Xformers库中的注意力机制，默认将后端设置为SDPA以适用于非GPU平台。,https://github.com/vllm-project/vllm/issues/12235
vllm,这个issue类型是用户提出需求，该问题涉及的主要对象是如何输入多条消息作为一批而不只是一条。由于无法成功输入一批大于1的消息，用户希望知道如何批量输入多条消息。,https://github.com/vllm-project/vllm/issues/12234
vllm,这是一个功能改进类型的issue，涉及主要对象是代码中的find_loaded_library函数。这个问题的原因是为不同平台创建自定义版本的platform_aware_utils.py来覆盖特定平台相关的工具函数。,https://github.com/vllm-project/vllm/issues/12231
vllm,这是一个更新代码所有者和GitHub账户信息的问题，主要涉及GitHub代码库的管理和团队协作。由于团队成员更改GitHub账户和团队架构变动的原因，需要更新代码所有者信息。,https://github.com/vllm-project/vllm/issues/12229
vllm,这个issue类型为用户提出需求，关注的主要对象是代码中的quantization和guided decoding模块，用户请求添加这两者的CODEOWNERS。,https://github.com/vllm-project/vllm/issues/12228
vllm,这个issue类型是新模型添加请求，主要涉及的对象是DeepSeek R1模型。原因是用户想要vllm添加对DeepSeek R1模型的支持。,https://github.com/vllm-project/vllm/issues/12226
vllm,这是一个用户提出需求的issue，主要涉及的对象是moe模型中的`moe_align_block_size`参数。由于当前`moe_align_block_size`在cuda图中不兼容并且在`num_experts`较大时效率较低，用户希望通过优化此参数来解决这些问题。,https://github.com/vllm-project/vllm/issues/12222
vllm,这个issue是一个用户提出需求的类型，主要涉及的对象是VLLM库的功能扩展。由于SwiftKV cache compression技术带来的性能优势，用户建议将其合并到VLLM中。,https://github.com/vllm-project/vllm/issues/12220
vllm,该issue类型为功能改进，主要涉及到attention layer的自定义实现。可能是由于quantization attention方法无法直接使用参数，因此需要将attention对象传递给attention backend来解决这个问题。,https://github.com/vllm-project/vllm/issues/12218
vllm,这个issue是关于提出需求的，主要对象是开发人员。问题是由于开发人员在临时提交时需要通过linters可能会感到烦恼，不是每个人都知道如何使用`noverify`标志，所以添加了一个虚拟的hook来提示用户如何绕过钩子。,https://github.com/vllm-project/vllm/issues/12217
vllm,这是一个用户提出需求的issue，主要涉及FP8 KV Cache功能支持的模型数量问题。用户疑问仅支持 Llama 2 是否可信，或许是因为文档未明确说明支持的模型范围所致。,https://github.com/vllm-project/vllm/issues/12215
vllm,这是一个类型为改进（enhancement）的issue，主要涉及到代码中的函数`_get_cache_block_size`，由于其被替换为`get_kv_cache_config`，因此需要将该函数进行移除操作。,https://github.com/vllm-project/vllm/issues/12214
vllm,这是一个用户提出需求的issue，主要涉及设置默认温度参数的问题。由于无法从服务器端修改默认值，用户希望了解如何设置默认温度以及其他参数。,https://github.com/vllm-project/vllm/issues/11861
vllm,这是一个用户需求类型的issue，主要涉及到vllm的Online Inference使用上的问题，用户想知道如何进行多模态数据的在线推理操作。,https://github.com/vllm-project/vllm/issues/11859
vllm,这是一个用户提出需求的类型，主要涉及到支持Microsoft phi-4模型。用户提出问题的原因可能是想要使用该模型来训练或应用。,https://github.com/vllm-project/vllm/issues/11856
vllm,这是一个用户提出需求的issue，主要涉及支持使用Jinja模板来展示Llama3.3模型的信息。用户希望获取Llama3.3的聊天模板示例，并问询最佳推荐，可能由于缺乏相应的模板而导致了这个问题。,https://github.com/vllm-project/vllm/issues/11854
vllm,这个issue类型是功能需求，主要对象是对多模式API参考文档的扩展。这是因为即将发布的教程需要引用这些API参考文档。,https://github.com/vllm-project/vllm/issues/11852
vllm,该issue类型为代码优化，主要涉及的对象是移动模型工具到`vision.py`文件中。,https://github.com/vllm-project/vllm/issues/11848
vllm,这个issue类型是文档或注释的改进，涉及的主要对象是BlockHashType，用户提出需求补充更多关于该类型的解释。,https://github.com/vllm-project/vllm/issues/11847
vllm,这是一个用户提出需求和讨论功能实现的issue，涉及主要对象为实现了dualchunk flash attention和sparse attention support的PR。由于需要实现Qwen models的新功能和支持，才导致了这个issue的提出。,https://github.com/vllm-project/vllm/issues/11844
vllm,该issue类型为文档更新，涉及主要对象为API Reference页面的组织结构。,https://github.com/vllm-project/vllm/issues/11843
vllm,这个issue类型是用户提出需求，主要对象是比较两个分支。,https://github.com/vllm-project/vllm/issues/11841
vllm,这个issue类型是用户提出需求，询问关于构建Triton使用特定提交的原因，主要涉及VLLM Docker和Triton。,https://github.com/vllm-project/vllm/issues/11838
vllm,这是一个用户需求类型的issue，主要涉及vllm中使用'tqdm'特性进行beam_search方法时的限制。用户想要讨论如何在beam_search方法中使用'use_tqdm'特性。,https://github.com/vllm-project/vllm/issues/11835
vllm,这是一个文档更新类的issue，主要涉及赞助商名称的更改，由于赞助商名称需要更新为'Novita AI'。,https://github.com/vllm-project/vllm/issues/11833
vllm,这个issue是用户提出的一个需求，主要涉及硬件和CPU，请求支持MOE模型在x86 CPU上运行。,https://github.com/vllm-project/vllm/issues/11831
vllm,这个issue类型是用户提出需求，涉及的主要对象是更新wheels的URL。原因可能是为了简化URL并提高用户体验。,https://github.com/vllm-project/vllm/issues/11830
vllm,这是一个功能需求报告，主要涉及的对象是vllm api_server。由于存在可能导致服务异常的跨域设置冲突，用户提出需要添加一个设置来禁用跨域中间件。,https://github.com/vllm-project/vllm/issues/11827
vllm,这个issue类型是功能需求提出，主要对象是Intel Gaudi，由于需求为启用INC FP8支持，意在实现基于Intel® Neural Compressor (INC)工具包的FP8推理。,https://github.com/vllm-project/vllm/issues/11826
vllm,这个issue属于代码优化类问题，主要涉及到代码中重复导入模块的修复。,https://github.com/vllm-project/vllm/issues/11824
vllm,该issue类型是用户提出需求，询问问题，该问题单涉及的主要对象是vLLM模型的集成与推断。由于用户不清楚如何将特定模型集成到vLLM中，因此提出需要关于集成的指导。,https://github.com/vllm-project/vllm/issues/11822
vllm,这是一个用户提出需求的类型的问题单，主要对象是在DeepSeek-V3中添加fused_moe配置。这个需求可能由于DeepSeek-V3模型需要具备更高的性能和灵活性而提出。,https://github.com/vllm-project/vllm/issues/11820
vllm,这个issue类型是对功能的改进，主要涉及 torch profiler 的打印表排序问题。,https://github.com/vllm-project/vllm/issues/11813
vllm,这是一个功能优化的issue，主要涉及到VLM（Visual Language Model）中与多模态分析和处理相关的代码重组。由于优化了处理相关代码的结构和逻辑，使得代码更清晰、易维护。,https://github.com/vllm-project/vllm/issues/11812
vllm,这是一个文档更新类型的issue，主要涉及的对象是pip包管理工具。原因可能是用户需要了解如何使用pip安装nightly版本的软件包。,https://github.com/vllm-project/vllm/issues/11806
vllm,这个issue类型为改进建议，主要对象是scheduler.py文件中的重复导入的类型提示。原因是导致代码冗余且不符合最佳实践。,https://github.com/vllm-project/vllm/issues/11804
vllm,这个issue类型为功能需求提出，主要涉及到llama3.3工具调用支持的问题，用户希望实现一个通用的、可扩展的llama工具调用支持。,https://github.com/vllm-project/vllm/issues/11799
vllm,该issue是一个文档更新类型的问题，涉及到V1版本支持`LLaVa-NeXT-Video`，由于CC([V1] Extend beyond image modality and support mixedmodality inference with LlavaOneVision)的合并，这个更新属于一次无缝升级。,https://github.com/vllm-project/vllm/issues/11798
vllm,这个issue类型是需求提出，主要涉及的对象是支持非可分割注意力头，由于模型的奇异形状导致无法在特定GPU节点上运行。,https://github.com/vllm-project/vllm/issues/11797
vllm,这是一个性能优化的issue，主要涉及到前端的预填解码功能，由于使用ZMQ替代HTTP通信和实现持久ZMQ连接，导致预填解码分解性能显著提升。,https://github.com/vllm-project/vllm/issues/11791
vllm,该issue类型为功能需求提议，主要涉及对象为VLLM Connect服务，用户希望通过使用zmq来改善预填充和解码阶段的性能分离。,https://github.com/vllm-project/vllm/issues/11789
vllm,该issue类型为功能需求，涉及的主要对象是docker image构建过程。由于并发构建docker images所需的构建号未被添加到docker image名称中，导致无法在同一服务器上并行构建docker images。,https://github.com/vllm-project/vllm/issues/11788
vllm,这是一个功能需求的issue，主要涉及支持TPU上的W8A8模型，原因是提出了关于TPU上压缩张量W8A8模型的使用支持和相关测试的需求。,https://github.com/vllm-project/vllm/issues/11785
vllm,这是一个用户提出需求的类型，主要涉及文档结构的调整和分类优化。这个issue主要是为了优化文档结构和提升用户体验。,https://github.com/vllm-project/vllm/issues/11782
vllm,该issue属于功能增强类型，涉及主要对象为在vLLM仓库中添加Google T5模型支持。,https://github.com/vllm-project/vllm/issues/11780
vllm,该issue属于功能需求类型，主要涉及vLLM构建流程的优化。造成此需求的原因是为了加快构建速度和减小基础镜像的体积。,https://github.com/vllm-project/vllm/issues/11777
vllm,这是一个与代码更新相关的问题，涉及对象为 Qwen2-Audio multi-modal processor。原因是为了预防在发布时被 https://github.com/huggingface/transformers/pull/35534 所影响，作者进行了本地测试并更新了处理器版本。,https://github.com/vllm-project/vllm/issues/11776
vllm,这个issue类型是文档补充请求，涉及主要对象是vllm下的uv模块。由于缺乏详细的使用说明文档，用户提出需要添加关于如何使用uv模块的文档说明。,https://github.com/vllm-project/vllm/issues/11773
vllm,这是一个需求类型的issue，主要涉及添加interleaving sliding window支持，避免类似之前发生的问题。,https://github.com/vllm-project/vllm/issues/11771
vllm,这是一个文档更新类的issue，主要涉及vLLM文档中Serving部分的重新组织。,https://github.com/vllm-project/vllm/issues/11766
vllm,这是一个跟踪issue类型的提出需求，主要涉及get_executor_cls的重构，该问题可能是由于之前的Pull Request引发的问题而导致新的修改需求。,https://github.com/vllm-project/vllm/issues/11754
vllm,这是一个优化类型的issue，主要涉及到Python函数调用的优化，旨在降低额外开销。,https://github.com/vllm-project/vllm/issues/11750
vllm,这是一个优化代码的issue，主要对象为PyNcclCommunicator，通过消除change_state函数的冗余来提高代码性能。,https://github.com/vllm-project/vllm/issues/11749
vllm,这是一个功能增强（Enhancement）类型的Issue，主要涉及VLM库中的多模态处理器类（multimodal processor class）。通过将与性能分析相关的逻辑和通用功能分离为新的类和函数，以提高代码结构和可维护性。,https://github.com/vllm-project/vllm/issues/11746
vllm,这是一个关于性能问题的建议报告，用户提出了VLLM在上下文长度方面存在问题，导致内存不足错误。,https://github.com/vllm-project/vllm/issues/11745
vllm,这是一个优化代码的issue，涉及PyNcclCommunicator对象。由于冗余的流初始化和赋值操作被移除，导致了代码的简化和提升性能。,https://github.com/vllm-project/vllm/issues/11744
vllm,这是一个功能需求提出的issue，主要涉及vllm项目中的sleep mode功能。由于未释放cudagraph内存池以及其他成本，导致sleep后未达到预期的内存释放量。,https://github.com/vllm-project/vllm/issues/11743
vllm,这个issue类型是需求提出，主要涉及的对象是在PyTorch的子类中为forward函数添加mypy类型提示。此需求是为了在mypy类型检查中能够正确地识别forward函数的返回类型。,https://github.com/vllm-project/vllm/issues/11740
vllm,这个issue属于优化建议类别，主要涉及模型的权重初始化逻辑。原因是权重将被模型的检查点所覆盖，所以不需要自定义初始化。,https://github.com/vllm-project/vllm/issues/11736
vllm,这是一个功能需求提出的issue，主要涉及到V1版本的音频语言模型支持，但由于缺少预处理步骤，需要在另一个相关的PR合并后再进行合并。,https://github.com/vllm-project/vllm/issues/11733
vllm,这是一个用户提出需求的issue，主要涉及的对象是关于服务名为'LLaVA-Next-Video-7B-Qwen2'的服务。 由于给出的文本内容不清晰，无法明确问题的具体原因。,https://github.com/vllm-project/vllm/issues/11731
vllm,这是一个建议性质的issue，提议替换PyTorch中使用的c10::optional为std::optional。这个提议的目的是为了消除c10::optional的使用，推荐使用std::optional。,https://github.com/vllm-project/vllm/issues/11730
vllm,这个issue类型是用户提出需求，主要对象是vLLM是否支持在同一服务器上托管多个llm base模型。这个问题是由于用户希望从成本节约的角度考虑，了解vLLM是否有计划支持此功能而提出的。,https://github.com/vllm-project/vllm/issues/11729
vllm,这个issue属于用户提出需求类型，主要涉及vLLM为GH200发布Arm镜像的问题。用户提出该需求是因为目前用户需要自行构建和发布镜像，对用户体验不友好。,https://github.com/vllm-project/vllm/issues/11728
vllm,这是一个用户提出需求的类型的issue，主要涉及到对新模型unsloth/Llama-3.3-70B-Instruct-bnb-4bit的支持。原因可能是用户希望vllm支持该模型以提高推理速度。,https://github.com/vllm-project/vllm/issues/11725
vllm,这个issue属于用户需求提出类型，主要涉及到vLLM中的CPU绑定问题，用户希望vLLM可以将内存绑定到与CPU列表相关的所有NUMA节点而不仅仅是与列表中第一个CPU相关的节点。,https://github.com/vllm-project/vllm/issues/11720
vllm,这个issue类型为功能改进，涉及的主要对象是更新文档中的例子。,https://github.com/vllm-project/vllm/issues/11718
vllm,这是一个用户提出需求的issue，主要涉及VLM下的多模态处理器的合并实现。这个问题是由于需要将固定图像大小的测试从e2e移动到处理器测试，并在处理视频输入时对其进行正确的分析而做出的修改。,https://github.com/vllm-project/vllm/issues/11717
vllm,这是一个用户提出需求的issue，主要对象是前端功能。由于未指定prefix和suffix，可能导致生成的tokens不符合预期。,https://github.com/vllm-project/vllm/issues/11713
vllm,这个issue类型为用户提出需求，主要涉及的对象是为`AsyncLLM`（api server）添加`RayExecutor`支持。,https://github.com/vllm-project/vllm/issues/11712
vllm,这是一个用户提出功能需求的issue，主要涉及tool_choice字段，请求支持required类型，导致症状为目前不支持该类型。,https://github.com/vllm-project/vllm/issues/11700
vllm,这是一个功能改进和性能优化的issue，主要涉及到PD Disagg Performance enhance & benchmark tool的更新，主要关注提高预填充延迟和改进基准测试工具。,https://github.com/vllm-project/vllm/issues/11699
vllm,这是一个优化建议类型的issue，主要涉及到对Triton中使用的块大小启发式的调整，旨在提高性能。,https://github.com/vllm-project/vllm/issues/11698
vllm,这是一个功能需求的问题，主要对象是benchmark scripts。该问题由于detokenization耗时较长，且并非始终需要，因此用户希望在benchmark scripts中增加选择性地包含或排除detokenization的功能。,https://github.com/vllm-project/vllm/issues/11697
vllm,这是一个特性需求的issue，涉及的主要对象是在Apple芯片的Mac设备上构建和运行VLLM的问题。由于VLLM需要进行一些ARM/CPU构建脚本的调整，才能在macOS上的Apple芯片设备上正常运行。,https://github.com/vllm-project/vllm/issues/11696
vllm,这是一个需求更新类的issue，主要涉及更新支持Python 3.9和3.11的requirements-tpu.txt文件，同时建议添加一个阻止表格尺寸警告的功能。,https://github.com/vllm-project/vllm/issues/11695
vllm,这是一个文档更新类型的issue，主要涉及的对象是用于chunked prefill的默认 max_num_batch_tokens 参数。原因可能是为了准确记录新的默认值以提高性能。,https://github.com/vllm-project/vllm/issues/11694
vllm,这是一个用户提出需求的issue，主要涉及的对象是新增的`BlockTable`类。在此之前，为了降低输入准备逻辑的复杂性，需要优化从CPU到GPU的块表复制。,https://github.com/vllm-project/vllm/issues/11693
vllm,这是一个用户提出需求的issue，主要涉及对象是Tokenize Endpoint。这个问题由于用户想要更好地控制tokenization以避免提示注入。,https://github.com/vllm-project/vllm/issues/11691
vllm,这个issue是关于代码优化和功能改进的， 主要涉及对象是Attention类。由于需要从Attention类获取attn_type参数，所以移动了attn_type到Attention.__init__()中。,https://github.com/vllm-project/vllm/issues/11690
vllm,这是一个关于功能需求的issue，主要涉及Qwen2VL的模型对BitsAndBytes量化的支持问题。原因可能是目前的模型暂不支持BitsAndBytes量化。,https://github.com/vllm-project/vllm/issues/11687
vllm,该issue属于用户提出需求类型，主要涉及V1模块的扩展，支持多模态推断并添加LlavaOneVision支持。这个问题的提出可能是为了扩大模型的支持范围，使其能够处理融合输入处理器的模型以及混合模态推理。,https://github.com/vllm-project/vllm/issues/11685
vllm,这个issue类型是功能更新，涉及主要对象是VLM下的LLaVA-NeXT项目。,https://github.com/vllm-project/vllm/issues/11682
vllm,该issue类型是功能改进建议，主要涉及k8s-config中的secret对象，并建议使用`stringData`替代`data`来简化文档说明。,https://github.com/vllm-project/vllm/issues/11679
vllm,这是一个技术改进的issue，主要涉及的对象是torch.compile中的KV cache。导致这个问题的原因是为了支持隐藏持续批处理复杂性和混合内存分配器。,https://github.com/vllm-project/vllm/issues/11677
vllm,这个issue属于技术需求类型，主要涉及到Mypy worker的支持问题。这个问题可能是由于需求增加或者功能改进而提出。,https://github.com/vllm-project/vllm/issues/11675
vllm,这个issue类型是功能改进，该问题单涉及的主要对象是VLM（Visual Language Model），由于需要确保最大占位符数量和最大项目数之间的一致性，因此将相关代码移到合并的多模处理器中，同时对Aria模型文件进行了类型注释和删除了一些冗余代码。,https://github.com/vllm-project/vllm/issues/11669
vllm,这个issue是一个功能需求，主要涉及到Qwen2-VL模型的V1支持，其中涉及到动态维度处理、dummy数据获取、MRoPE支持等内容。,https://github.com/vllm-project/vllm/issues/11668
vllm,该issue属于用户提出需求类型，主要涉及QWenVL LoRA模型的优化和对视觉模块及投影模块的LoRA支持。,https://github.com/vllm-project/vllm/issues/11663
vllm,这个issue属于用户提出需求类型，主要涉及到设备与rank的映射，由于不同系统的设备之间的通信路由不同，因此需要引入`VLLM_LOCAL_RANK_DEV_MAP`来更灵活地分配设备与rank的关系。,https://github.com/vllm-project/vllm/issues/11662
vllm,这个issue是关于优化程序关闭流程，非bug报告。涉及的主要对象是实现后台进程管理或进程间通信的对象。原因是为了解决当LLM被清除时产生奇怪日志的问题，并且简化LLM、LLMEngine或AsyncLLM等高级类在关闭时无需特殊处理。,https://github.com/vllm-project/vllm/issues/11659
vllm,这个issue是用户提出的需求，主要对象是VLLM的支持情况，用户提出需要支持8位的Inflight quantization以提高速度。,https://github.com/vllm-project/vllm/issues/11655
vllm,这个issue为用户提出需求类型，主要涉及对象是vllm下的qwen2vl-7b服务。导致出现问题的原因是用户不知道如何在vllm中集成特定模型进行推理。,https://github.com/vllm-project/vllm/issues/11652
vllm,这个issue类型为性能优化提议，主要涉及V1版本与V0版本之间的性能对比。由于多步骤处理的原因，V0在单个请求中运行更快。,https://github.com/vllm-project/vllm/issues/11649
vllm,这个issue属于优化建议，主要涉及视觉模块哈希的简化，由于之前的hash格式可能导致混淆，提出了更简洁的方案。,https://github.com/vllm-project/vllm/issues/11646
vllm,该issue属于用户提出需求类型，主要涉及对赞助页面的重新组织。原因可能是要添加赞助商级别和推荐，并突出Slack赞助 Anyscale 的部分。,https://github.com/vllm-project/vllm/issues/11639
vllm,这是一个用户提出需求的issue，主要涉及到RL后训练工作负载，提出了避免KV缓存和转移模型权重的功能需求。,https://github.com/vllm-project/vllm/issues/11638
vllm,这个issue类型属于功能需求，主要涉及的对象是Cascade Attention的实现。原因是为了节省HBM带宽，当请求共享相同前缀时，使用Cascade Attention。,https://github.com/vllm-project/vllm/issues/11635
vllm,这是一个用户提出需求的issue，主要涉及V1 engine中如何获取logprobs的问题。由于V1 engine可能不支持logprobs参数，用户希望了解是否有其他方法可以获取logprobs。,https://github.com/vllm-project/vllm/issues/11634
vllm,这是一个功能新增的issue，主要涉及V1支持的单图模型。由于代码更改较大，需要进行审查。,https://github.com/vllm-project/vllm/issues/11632
vllm,"该issue为用户需求提出类型，主要涉及的对象是在prefill阶段的cpu负担过重，由于此原因导致用户希望在prefill阶段能够使用""cudagraph""。",https://github.com/vllm-project/vllm/issues/11628
vllm,"这是一个用户提出需求的issue，主要涉及的对象是vllm中的Kernel，用户想要添加一个支持MulAndSilu的Kernel来改进jais, ultravox 和 molmo 模型的性能。",https://github.com/vllm-project/vllm/issues/11624
vllm,这是一个用户提出需求的issue，主要涉及AsyncEngineArgs加载多个lora模块的问题，用户想了解如何同时加载多个lora模块。,https://github.com/vllm-project/vllm/issues/11621
vllm,这个issue类型是需求提出，涉及到VLM（Visual Language Model）的multi-modal data parsing抽象化，开发者需要修改数据解析器来支持模型的额外模态。,https://github.com/vllm-project/vllm/issues/11620
vllm,这个Issue类型为建议/需求，主要对象是torch.compile模块；用户提出需要考虑相关代码在编译缓存中的问题，可能是为了更好的性能和代码执行效果。,https://github.com/vllm-project/vllm/issues/11614
vllm,这是一个升级请求，涉及将 helm/kind-action 从 1.10.0 升级到 1.12.0，其中包含了更新和新功能。,https://github.com/vllm-project/vllm/issues/11612
vllm,这是一个用户提出需求的issue，主要对象是需要允许平台指定注意力后端。由于当前的限制，需要新增功能支持不同平台实现自定义的注意力后端。,https://github.com/vllm-project/vllm/issues/11609
vllm,这是一个更新请求类型的issue，涉及主要对象是kernel。由于CUTLASS 3.6.0版本不再支持MixedInput kernel schedule tags，需要相应更新Machete kernels，导致该issue的提出。,https://github.com/vllm-project/vllm/issues/11607
vllm,这个issue类型是技术需求，主要涉及到模型Qwen/Qwen2VL2BInstruct输出结果的置信度问题。由于logprobs返回结果为空列表导致正确答案的置信度为0，用户希望找到解码置信度的方法。,https://github.com/vllm-project/vllm/issues/11606
vllm,这个issue属于功能需求，主要涉及平台插件的启用，由于需要重构代码并启用未注册的平台插件。,https://github.com/vllm-project/vllm/issues/11602
vllm,该issue属于用户提出需求类型，主要对象是deepseek v3的性能问题。由于环境中存在一些参数配置不佳，导致了性能吞吐量低下的问题。,https://github.com/vllm-project/vllm/issues/11600
vllm,这个issue是更新Neuron SDK版本以支持PyTorch 2.5的请求类型，涉及的主要对象是Neuron SDK。,https://github.com/vllm-project/vllm/issues/11593
vllm,这个issue类型是用户提出需求，涉及主要对象是vllm中的parallel tool calls功能，用户询问关于该功能在llama3.1模型中的使用方式。,https://github.com/vllm-project/vllm/issues/11592
vllm,这是一个用户提出的需求类型的issue，主要涉及的对象是LM Eval With Streaming Integration Tests。,https://github.com/vllm-project/vllm/issues/11590
vllm,该问题类型为功能需求，主要涉及 Triton 配置用于 Fp8 Block 量化的问题。由于需要使用 DeepSeekV3，并且基于某些更新，因此需要适配相关配置。,https://github.com/vllm-project/vllm/issues/11589
vllm,这是一个功能改进类的issue，涉及的主要对象是Pixtral模型。这个问题是由于之前在模型文件中硬编码的token IDs，现在可以从模型配置文件中获取，通过这个PR进行了更新，以解决问题。,https://github.com/vllm-project/vllm/issues/11582
vllm,这是一个优化性质的issue，主要对象是针对`token_ids_cpu` Tensor的内存使用优化。,https://github.com/vllm-project/vllm/issues/11581
vllm,这是一个用户提出需求的issue，主要涉及的对象是LoRA kernel micro benchmarks。用户希望增加LoRA内核微基准测试以优化LoRA内核，以帮助调整/优化LoRA内核，并避免基准测试期间的缓存效应。,https://github.com/vllm-project/vllm/issues/11579
vllm,这是关于模型初始化的需求提出。该问题单主要涉及支持Deepseek-VL2模型。这个问题的原因是DeepseekAI的DeepSeekVL2Tiny模型由于未使用MLA attention，目前并不受支持。,https://github.com/vllm-project/vllm/issues/11578
vllm,这是一个技术改进型的Issue，主要涉及SageMaker服务的兼容性问题，通过实现`/ping`和`/invocations`路径以及做出相关Dockerfile调整来解决，并探讨了一些路由和功能上的变化。,https://github.com/vllm-project/vllm/issues/11575
vllm,该issue类型为用户提出需求，该问题单涉及的主要对象是模型支持 InternLM2 奖励模型。,https://github.com/vllm-project/vllm/issues/11571
vllm,这是一个需求类型的issue，主要涉及前端（Frontend）的错误处理改进。原因是需要更新V0请求处理以匹配V1的处理方式。,https://github.com/vllm-project/vllm/issues/11570
vllm,这是一个文档更新类型的issue，涉及将mllama示例代码根据官方文档进行更新，主要涉及到使用chat模板的问题。,https://github.com/vllm-project/vllm/issues/11567
vllm,该issue类型是用户提出需求，讨论关于在vllm中支持稀疏性问题，提到了支持2:4稀疏性以及未来可能提供无结构稀疏性支持的问题。,https://github.com/vllm-project/vllm/issues/11563
vllm,这是一个用户提出需求的issue，主要涉及的对象是 LoRA 模型。由于用户希望训练完全的 lm_head 和 embed_tokens，提出了这个需求。,https://github.com/vllm-project/vllm/issues/11558
vllm,这是一个功能需求类型的issue，涉及vLLM项目对SageMaker-required endpoints的支持。由于AWS需要管理镜像，此功能之前未被支持，但用户提出通过将必需的SageMaker端点路由整合到vLLM源码中来提供支持。,https://github.com/vllm-project/vllm/issues/11557
vllm,"这个issue是一个需求提出，主要对象是模型""LoRA with lm_head and embed_tokens fully trained""。由于该模型需要进一步训练，用户提出了相关需求。",https://github.com/vllm-project/vllm/issues/11556
vllm,这个issue类型为代码优化建议，主要对象是关于Worker的代码实现，由于重复执行`torch.cuda.synchronize()`导致不必要的性能开销。,https://github.com/vllm-project/vllm/issues/11555
vllm,"这个issue是关于新功能需求的，主要涉及的对象是针对vllm下的speculative decoding模块。这个需求是为了添加一项名为""mean accept length metric""的指标来评估模型的性能。",https://github.com/vllm-project/vllm/issues/11552
vllm,这是一个用户提出需求的issue，主要涉及到MolmoForCausalLM模型的BNB量化支持。用户提出了需要在BNB加载器中添加权重映射器，并进一步修改MolmoForCausalLM的MLP以避免BNB生成错误结果。,https://github.com/vllm-project/vllm/issues/11551
vllm,这是一个需求类型的issue，主要涉及到文档（doc）。可能由于当前文档中缺少对xgrammar的描述，用户提出需求添加相关内容。,https://github.com/vllm-project/vllm/issues/11549
vllm,该issue类型为文档更新请求，涉及的主要对象是文档。由于文档缺少xgrammar的描述，用户请求添加相关内容。,https://github.com/vllm-project/vllm/issues/11548
vllm,这是一个用户需求的issue，主要涉及的对象是API Server中的`Detokenizer`和`EngineCore`模块的输入统一问题，由于需要准备3个处理架构，提出了此问题。,https://github.com/vllm-project/vllm/issues/11545
vllm,该issue属于需求提出类型，主要涉及Deepseek V3模型的改进，可能由于对模型功能和性能的迭代需求而产生。,https://github.com/vllm-project/vllm/issues/11539
vllm,这是一个需求更新的类型，涉及的主要对象是文档。通过原因导致的提交issue，以更新文档的内容。,https://github.com/vllm-project/vllm/issues/11536
vllm,该issue类型为用户提出需求，涉及主要对象为Deepseek V3模型，用户希望在文档和README中添加模型名称。由于缺乏该信息，用户无法准确了解和使用Deepseek V3模型，因此提出了需要更新文档的需求。,https://github.com/vllm-project/vllm/issues/11535
vllm,这个issue是性能优化类型，主要涉及API服务器，由于高QPS时减少任务切换导致性能提升和支持正确处理取消操作的问题。,https://github.com/vllm-project/vllm/issues/11534
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加CPU offloading的基准测试脚本。,https://github.com/vllm-project/vllm/issues/11533
vllm,这个issue类型是功能增强（Feature Enhancement），主要涉及到vLLM项目中的Block Allocator以支持KV缓存CPU卸载，由于CPU offloading在基准测试中表现更好，提出了相关实现和性能优化的变化。,https://github.com/vllm-project/vllm/issues/11532
vllm,这是一个性能优化类型的issue，涉及到CUDA kernel实现的优化。,https://github.com/vllm-project/vllm/issues/11531
vllm,该issue类型为功能更新（Breaking Change），主要涉及API Server的修改和性能优化。原因是在这次变更中，移除了默认的代理功能，导致性能提升，但可能会影响到之前依赖代理功能的用户或系统。,https://github.com/vllm-project/vllm/issues/11529
vllm,这是一个功能改进的issue，主要涉及到对媒体内容读取和写入逻辑的抽象，以及在`BaseMultiModalItemTracker.add`中去除重复的字典键。,https://github.com/vllm-project/vllm/issues/11527
vllm,这个issue是对模型量化支持的需求提出，涉及主要对象是 deepseek_v3 模型。,https://github.com/vllm-project/vllm/issues/11523
vllm,这个issue属于需求提出类型，主要涉及工具解析器的重构，因为当前工具解析器存在错误且代码混乱，导致性能瓶颈和功能问题。,https://github.com/vllm-project/vllm/issues/11522
vllm,该issue类型为需求提出，主要涉及对象是`openai_chat_completion_client_for_multimodal.py`文件缺失视频示例。,https://github.com/vllm-project/vllm/issues/11521
vllm,这个issue类型为需求提出，主要对象是DeepseekV3模型，由于不支持该模型架构导致数值错误。,https://github.com/vllm-project/vllm/issues/11520
vllm,该issue属于功能改进（Feature Improvement），涉及的主要对象是`MolmoForCausalLM`模型。原因是当前版本中`image_projector`和LLM共用相同的MLP导致了权重形式不一致，这造成了混淆，并且增加了对BNB和LoRA的支持困难。,https://github.com/vllm-project/vllm/issues/11510
vllm,这是一个用户提出需求的issue，涉及的主要对象是vLLM模型列表文档。用户提出需求是因为vLLM模型名称不反映其架构，建议将`QVQ`和`QwQ`明确列入已支持的模型列表。,https://github.com/vllm-project/vllm/issues/11509
vllm,这是一个用户提出需求类型的 issue，主要涉及LLama3.2 Vision Instruct模型的推理问题，用户询问为什么实际效果与理论不一致。,https://github.com/vllm-project/vllm/issues/11508
vllm,这个issue类型是用户提出需求，请教问题，主要涉及VLLM是否支持基于其他AI框架（如MindSpore）的模型，用户希望将MindSpore集成到VLLM中以扩展MindSpore生态系统。,https://github.com/vllm-project/vllm/issues/11507
vllm,这是一个用户提出需求的问题，主要对象是vllm，问题是关于是否支持基于其他人工智能框架的模型的请求。,https://github.com/vllm-project/vllm/issues/11505
vllm,这个issue类型是需求提出，涉及的主要对象是平台相关功能。原因是为了在硬件可插拔功能的基础上移动模型架构检查功能到平台。,https://github.com/vllm-project/vllm/issues/11503
vllm,这是一个用户提出需求的issue，主要对象是在vllm项目中添加一个占位符模块来简化导入可选模块。,https://github.com/vllm-project/vllm/issues/11501
vllm,该issue属于增强功能请求类型，主要涉及到vllm/compilation组件。由于缺乏类型检查，可能导致`compiled_graph`和`wrap_inductor`方法返回不正确的计算图，因此用户建议加入Mypy类型检查来确保正确性。,https://github.com/vllm-project/vllm/issues/11496
vllm,这是一个功能需求提出的issue，主要涉及前端的视频加载功能，由于当前仅支持基于图像帧的base64 URL，用户提出使用decord来加载视频以支持其他类型的视频，并提及了相应的 GitHub issue。,https://github.com/vllm-project/vllm/issues/11492
vllm,这是一个功能改进类型的issue，主要涉及的对象是GitHub链接格式。原因是为了减少复制和粘贴完整URL的操作。,https://github.com/vllm-project/vllm/issues/11491
vllm,该issue属于用户需求类型，主要涉及加速函数self.model()的问题，用户希望使用VLLM来提高函数运行速度，但不清楚如何操作。,https://github.com/vllm-project/vllm/issues/11483
vllm,这是一个需求提出类型的issue，主要涉及的对象为KV cache transfer connector。由于当前使用ifelse语句进行初始化，用户希望能够通过注册表进行初始化，以便更轻松地添加第三方连接器。,https://github.com/vllm-project/vllm/issues/11481
vllm,这是一个特性需求问题，关于允许在`kv_parallel_size==1`时初始化KV缓存传输代理。,https://github.com/vllm-project/vllm/issues/11480
vllm,这个issue是关于用户提出需求的，主要涉及的对象是新增模型支持。这个问题由于新模型目前仅支持单轮对话和图像，不支持视频输入，用户希望最新版本至少可以支持对该模型的推断。,https://github.com/vllm-project/vllm/issues/11479
vllm,这是一个功能需求，主要对象是vLLM的load balancer，用户需要解决prefix cache和load balancer之间的不兼容性问题。,https://github.com/vllm-project/vllm/issues/11477
vllm,这是一个需求的提出，涉及主要对象是 AMD 的 ROCm 安装文档。这个需求是基于 ROCm 6.3 版本的更新。,https://github.com/vllm-project/vllm/issues/11470
vllm,该issue类型为用户提出需求，涉及主要对象为vLLM模型的自动转换功能。由于缺少如何推断nvidia/Llama3.1Nemotron70BRewardHF的奖励模型的使用说明，用户寻求了关于推断奖励模型的帮助。,https://github.com/vllm-project/vllm/issues/11469
vllm,这是一个关于性能优化的问题，涉及到Molmo模型在多GPU环境下推理速度较慢以及使用中的错误提示。,https://github.com/vllm-project/vllm/issues/11468
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是在使用AMD ROCm GPU时的部署示例。由于缺乏关于在 deploying_with_k8s.md 中使用AMD ROCm GPU 的示例，用户提出了需求添加这方面的内容。,https://github.com/vllm-project/vllm/issues/11465
vllm,这是一个需求提出的类型，该问题单涉及的主要对象是更新部署文档，并添加针对 AMD ROCm GPU 的示例。原因可能是用户希望了解如何在K8S上部署使用 AMD ROCm GPU 的Pod。,https://github.com/vllm-project/vllm/issues/11464
vllm,这是一个用户提出需求的特性请求，主要涉及了将logits-processor-zoo中的logits处理器集成到vLLM框架中。原因是希望使用这些处理器提升文本生成的控制和输出效果。,https://github.com/vllm-project/vllm/issues/11461
vllm,这是一个用户提出需求的issue，主要涉及如何推断nvidia/Llama-3.1-Nemotron-70B-Reward-HF的奖励模型，可能是由于用户对如何集成vllm以及在推断特定模型方面的不了解而导致的。,https://github.com/vllm-project/vllm/issues/11459
vllm,这是一个建议改进的issue，主要涉及前端代码中的hermes_tool_parser.py文件。Bug症状可能是解析JSON输出时的健壮性不足。,https://github.com/vllm-project/vllm/issues/11453
vllm,这是一个改进建议类型的issue，主要涉及Frontend中的hermes_tool_parser.py文件，建议使用json_repair来提高解析JSON输出的健壮性。,https://github.com/vllm-project/vllm/issues/11444
vllm,这是一个用户提出需求的issue，主要涉及支持fairseq2 Llama模型，并添加了`Fairseq2LlamaForCausalLM`到模型注册表，支持sharded weights的加载。,https://github.com/vllm-project/vllm/issues/11442
vllm,这个issue是关于性能优化的建议和提问，主要涉及vllm项目中的Prefill功能的性能问题，导致了执行时的慢速表现。,https://github.com/vllm-project/vllm/issues/11436
vllm,该issue类型为用户提出需求和请教问题，主要涉及企业实施和数据收集细节。由于企业需要确认vLLM在商业环境下是否符合许可要求，以及是否有内置数据收集功能，因此用户提出了这些问题寻求帮助。,https://github.com/vllm-project/vllm/issues/11433
vllm,这是一个用户需求问题，用户想要了解如何在vllm中手动加载权重而不是使用huggingface_hub的权重。,https://github.com/vllm-project/vllm/issues/11429
vllm,该issue类型为功能需求，主要对象是offline_inference_with_prefix.py文件。由于缺少TTFT这一重要度量标准，导致prefix缓存功能不完整。,https://github.com/vllm-project/vllm/issues/11428
vllm,这个issue类型为功能需求提议，主要涉及的对象是VLM中的merged multimodal processor实现，用户寻求帮助来初始化encoder-decoder LMMs的merged multimodal processor。,https://github.com/vllm-project/vllm/issues/11427
vllm,这是一个需求合并(issue)类型，主要对象为Zhn/fish项目的e2e测试。,https://github.com/vllm-project/vllm/issues/11426
vllm,这个issue类型是软件更新，涉及的主要对象是helm/kind-action项目。由于更新导致的问题可能是功能缺陷，需要修复或新增功能。,https://github.com/vllm-project/vllm/issues/11424
vllm,该issue类型为技术改进类型，主要对象是vllm/worker中的mypy类型提示缺失问题。原因是缺少了对vllm/worker代码中的mypy类型提示，导致需要增加这些提示以进行类型检查。,https://github.com/vllm-project/vllm/issues/11418
vllm,这是一个用户提出需求的issue，主要涉及VLLM中的QTIP Quantization特性。用户希望将QTIP算法实现到VLLM中，因为该算法在2位时几乎不损失性能，并且相对于其他2位量化算法表现更好。,https://github.com/vllm-project/vllm/issues/11416
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm，用户希望vllm支持两个新的功能并提出了具体的改进建议。,https://github.com/vllm-project/vllm/issues/11410
vllm,这是一个用户提出需求的类型为特性请求（Feature Request）的issue，主要涉及对象是V1版本的Pixtral-HF模型，用户希望支持在V1上使用与Transformers兼容的Pixtral检查点。,https://github.com/vllm-project/vllm/issues/11409
vllm,这是一个用户提出需求的issue，主要涉及vllm库在使用过程中可能导致GPU内存占用问题，用户希望在使用Torch进行反向传播时，vllm能释放其KV缓存内存，以避免Torch内存不足的情况。,https://github.com/vllm-project/vllm/issues/11408
vllm,"这个issue是用户提出需求，主要对象是""tool choice support""。由于用户需要支持此功能，导致提出了该问题。",https://github.com/vllm-project/vllm/issues/11405
vllm,这是一个性能优化类型的issue，主要涉及到block table在CPU和GPU之间的数据传输问题。由于每次都发送整个block table导致开销过大，因此提出了只传输差异数据的优化方案。,https://github.com/vllm-project/vllm/issues/11401
vllm,这是一个提出新功能的请求（RFC），主要涉及vLLM中的完全SPMD执行，原因是为了改善离线推理的吞吐量。,https://github.com/vllm-project/vllm/issues/11400
vllm,这是一个关于需求提案的issue， 主要涉及到vLLM工作者的灵活权重同步。由于参数服务器的集中化导致了高权重同步开销和灵活性上的不足。,https://github.com/vllm-project/vllm/issues/11399
vllm,这个issue属于用户提出需求类型，主要对象是想要直接获取logits，该问题可能由于缺乏对该功能的理解或者相关功能尚未实现而导致。,https://github.com/vllm-project/vllm/issues/11397
vllm,这是一个功能改进的issue，涉及VLM项目中的缓存功能优化。原因是为了提高处理效率并重新定义输出的schema。,https://github.com/vllm-project/vllm/issues/11396
vllm,这是一个优化需求，主要涉及到V1 sampler中的topp和topk采样功能。,https://github.com/vllm-project/vllm/issues/11394
vllm,这是一个用户提出需求的issue，主要涉及到KV cache的大小限制问题，导致无法设置较大的maxmodellen参数。,https://github.com/vllm-project/vllm/issues/11391
vllm,这个issue类型是更新请求，主要涉及的对象是Dockerfile.rocm文件。,https://github.com/vllm-project/vllm/issues/11387
vllm,该issue类型为功能需求，主要对象是扩展APC以支持全局前缀缓存。由于APC缓存被清除时无法命中，需要在预填阶段中重用全局KV缓存池中的KV缓存，以及允许其他vllm实例直接使用全局KV缓存池中的KV缓存。,https://github.com/vllm-project/vllm/issues/11385
vllm,这是一个功能需求的issue，主要涉及到API Key的新增功能。原因可能是为了访问受保护的VLLM服务器而需要提供API Key。,https://github.com/vllm-project/vllm/issues/11384
vllm,这是一个需求报告，主要涉及AsyncLLMEngine的V1/V0行为切换方式问题，用户需要修改代码以使用V1。,https://github.com/vllm-project/vllm/issues/11383
vllm,这是一个用户提出需求的类型的 issue，主要涉及增强 vLLM 模型中的内存分配器来支持混合型模型，其中不同类型的层需要不同的KV缓存大小。,https://github.com/vllm-project/vllm/issues/11382
vllm,这个issue属于用户提出需求类型，主要涉及如何在多节点（通过SLURM）下运行LLama 405B BF16进行离线推断。用户因无法直接在节点上运行命令，希望得到关于使用多个节点进行离线推断的参考脚本或资源。,https://github.com/vllm-project/vllm/issues/11379
vllm,这是一个关于需求的issue，主要涉及vLLM是否会支持flash-attention 3，用户想了解未来是否会支持该功能。,https://github.com/vllm-project/vllm/issues/11372
vllm,这是一个功能改进的issue，主要涉及到支持拥有合并输入处理器的模型，而不是分开的`input_processor`和`input_mapper`，导致需要对应支持该特性的模型进行修改和测试。,https://github.com/vllm-project/vllm/issues/11370
vllm,这个issue是一个功能增强类型的问题，涉及的主要对象是`DeviceMemoryProfiler`模块。原因是为了使`DeviceMemoryProfiler`在不同平台上的可用性更好，所以对`current_memory_usage()`函数进行了重构。,https://github.com/vllm-project/vllm/issues/11369
vllm,这是一个请求新增功能的issue，主要涉及到前端开发中的DRY Sampling功能。由于使用DRY可能导致高达25%的性能损失，因此需要对其进行优化。,https://github.com/vllm-project/vllm/issues/11368
vllm,这个issue是用户提出的需求，主要对象是vLLM模型，用户希望添加对attention score输出的支持，因为当前的模型在推断过程中没有提供访问注意力得分的功能，这对于模型分析和可解释性研究至关重要。,https://github.com/vllm-project/vllm/issues/11365
vllm,这个issue类型属于用户提出需求类型，涉及的主要对象是vllm中的_IntermediateTensor_类。由于使用Pipeline Parallelism时，在_IntermediateTensor_对象中传递的张量数据中包含了名为'residual'的体积信息，用户想了解此信息的用途和推理计算中何时会被消耗。,https://github.com/vllm-project/vllm/issues/11364
vllm,这是一个用户提出需求的issue，主要对象是meta-llama/Prompt-Guard-86M模型，由于DebertaV2ForSequenceClassification架构暂不支持而导致使用该模型时发生数值错误。,https://github.com/vllm-project/vllm/issues/11360
vllm,这是一个合并请求（pull request）类型的issue，主要涉及Dockerfile的清理，由于需要在上游仓库的另一个分支中测试才能合并。,https://github.com/vllm-project/vllm/issues/11351
vllm,这个issue类型是需求提出，主要对象是要求添加GHA来检查失效的超链接。原因可能是希望确保项目中的超链接都能够正常访问，避免死链影响用户体验。,https://github.com/vllm-project/vllm/issues/11350
vllm,这是一个建议性质的问题，主要对象是在图像捕获期间添加tqdm进度条，由于缺乏视觉进度反馈而导致用户提出建议。,https://github.com/vllm-project/vllm/issues/11349
vllm,这是一个功能追加类型的issue，涉及Kernel中添加ExLlamaV2权重量化支持。由于还在进行中，具体原因和症状需要进一步的补充信息来分析。,https://github.com/vllm-project/vllm/issues/11348
vllm,这个issue是用户提出需求，想要在vllm中支持answerdotai/ModernBERT-large模型。,https://github.com/vllm-project/vllm/issues/11347
vllm,这是一个需求类型的issue，主要涉及VLLM执行器中关于GPU ID和工作器排名的类型检查。由于缺少对变量类型的明确定义，可能导致混淆，用户提出加强Mypy类型检查以澄清GPU ID可以是整数或字符串。,https://github.com/vllm-project/vllm/issues/11344
vllm,这是一个代码优化（code optimization）类型的issue，主要涉及LRUCache相关的类合并及类型注解的改进。原因可能是LRUDictCache和LRUCache本质上相同，合并后提高代码精准性。,https://github.com/vllm-project/vllm/issues/11339
vllm,这是一个关于需求的issue，主要涉及多模态模型的推理环境扩展问题，由于内存溢出导致无法将推理上下文长度扩展到128k。,https://github.com/vllm-project/vllm/issues/11337
vllm,这个issue属于项目优化类型，涉及到移除项目中未被引用的模块文件。原因是该文件已不再被项目引用，可以安全地删除。,https://github.com/vllm-project/vllm/issues/11336
vllm,这是一个用户提出的需求，主要涉及对T5模型添加自定义attention偏置支持。源自于以前的工作，提供了更通用的新增功能，允许在PagedAttention内使用任意自定义的attention偏置。,https://github.com/vllm-project/vllm/issues/11334
vllm,该issue类型为需求提出，涉及对象为在vllm中添加`transformers`作为后端支持。因为需求是为了支持更多的模型并增加tensor parallel支持。,https://github.com/vllm-project/vllm/issues/11330
vllm,这个issue是一个用户提出的需求，主要涉及到支持pytorch自定义操作可插拔性。可能的原因是用户希望能够注册特定平台的`forward`功能。,https://github.com/vllm-project/vllm/issues/11328
vllm,这是一个功能需求的issue，涉及主要对象是为设备特定的通信器添加基类，通过避免在每个通信器操作器中进行繁琐的调度来简化`GroupCoordinator`的实现。,https://github.com/vllm-project/vllm/issues/11324
vllm,这是一个用户提出需求的issue，主要涉及到在使用vllm中同时启用guided-decoding、chunked-prefill和prefix-caching时出现的问题。,https://github.com/vllm-project/vllm/issues/11322
vllm,该issue类型是文档更新，主要涉及到修改默认的max_num_batch_tokens参数。原因是文档中的数值需要与代码保持一致。,https://github.com/vllm-project/vllm/issues/11319
vllm,这是一个用户提出需求的 issue，主要涉及支持自定义torch.compile后端键，旨在使常见模块更平台无关。,https://github.com/vllm-project/vllm/issues/11318
vllm,这是一个优化性质的issue，主要涉及V1版本的VLLM项目中的prefix caching逻辑，由于在`self._touch`中删除了`evictable_computed_blocks`，在`allocate_slots`中不再需要对它们进行特殊处理，导致`num_evictable_computed_blocks`常为零，而此问题不会对性能产生影响。,https://github.com/vllm-project/vllm/issues/11310
vllm,这个issue属于用户提出需求类型，主要涉及模型支持问题，用户希望添加一个projection layer到模型中。,https://github.com/vllm-project/vllm/issues/11306
vllm,这是一个功能增强请求，主要涉及V1版本的VLM。由于VLM前缀缓存表现较佳，因此这个问题提议默认启用VLM处理器缓存以获得最佳性能。,https://github.com/vllm-project/vllm/issues/11305
vllm,这是一个关于需求的提议，涉及VLM下的Qwen2-Audio模块中多模态处理器的合并与优化。,https://github.com/vllm-project/vllm/issues/11303
vllm,这是一个代码优化类型的issue，主要涉及的对象是`vllm.utils`文件。导致这个问题的原因是文件变得混乱无序，需要进一步整理优化。,https://github.com/vllm-project/vllm/issues/11298
vllm,这是一个关于功能使用的问题，主要涉及到vllm中关于离线beam搜索推理的参数设置。由于最近beam搜索API发生了变化，导致用户不清楚如何在离线推理中触发beam搜索。,https://github.com/vllm-project/vllm/issues/11297
vllm,这是一个用户提出需求类型的issue，主要涉及发布arm64镜像到DockerHub。这个需求是由于构建arm64镜像耗时较长，希望提供arm64镜像来促进在低端arm64平台上的更多使用。,https://github.com/vllm-project/vllm/issues/11296
vllm,这个issue类型是优化建议，主要对象是Mllama模型中的encoder部分，提出了避免中间状态拷贝以提高性能的建议。,https://github.com/vllm-project/vllm/issues/11295
vllm,这是一个性能优化建议类型的 issue，主要对象是cache_engine.py文件，由于torch.zeros操作耗时较长，可以使用直接复制方法来提高速度。,https://github.com/vllm-project/vllm/issues/11294
vllm,这是一个优化建议，旨在改进代码执行速度。,https://github.com/vllm-project/vllm/issues/11293
vllm,这个issue是关于性能优化的提议，主要涉及到cache_engine.py文件。原因是使用torch.zeros函数导致运行时间较长，建议改用直接复制的方式以提高速度。,https://github.com/vllm-project/vllm/issues/11292
vllm,这是一个性能优化类型的issue，主要涉及到cache_engine.py文件。由于torch.zeros函数消耗大量时间，建议使用直接复制方法来改进速度。,https://github.com/vllm-project/vllm/issues/11291
vllm,这是一个优化建议，主要涉及到缓存引擎的更新。这个问题由于torch.zeros在消耗大量时间，建议使用直接拷贝方法来提高速度。,https://github.com/vllm-project/vllm/issues/11288
vllm,这是一个需求或功能补充类型的issue，涉及主要对象为OpenAI版本更新，由于缺少`ChatCompletionContentPartInputAudioParam`导致未能支持输入音频功能。,https://github.com/vllm-project/vllm/issues/11287
vllm,这是一个用户提出需求的类型，主要涉及vllm在不同网络环境下是否支持pipeline并行性的问题。用户想了解是否可以在不同网络的机器上使用vllm的pipeline并行功能。,https://github.com/vllm-project/vllm/issues/11285
vllm,该issue类型为功能新增，主要涉及的对象是vLLM Neuron后端，由于需要支持chunked prefill with flash attention，开发引入了一个NKI-based kernel。,https://github.com/vllm-project/vllm/issues/11277
vllm,这是一个用户提出需求的issue，主要涉及如何设置在生成标记时动态调整温度系数的功能。该问题可能由于当前的`sampling_params`无法实现此特性而产生。,https://github.com/vllm-project/vllm/issues/11276
vllm,"这是一个类型为""功能移除""的issue，主要对象是Github Action Release Workflow。",https://github.com/vllm-project/vllm/issues/11274
vllm,这是一个用户提出需求类型的issue，主要涉及对象是更新rubra vllm。由于原因未清楚描述，用户可能遇到了问题或者需要更新rubra vllm。,https://github.com/vllm-project/vllm/issues/11273
vllm,这个issue属于文档更新类型，主要涉及vllm TPU镜像在部署文档中的更新，可能由于现有文档未包含vllm TPU镜像信息而导致用户寻找困难。,https://github.com/vllm-project/vllm/issues/11270
vllm,这是一个功能需求的issue，涉及在benchmark_throughput.py中添加对LoRA基准测试的支持。,https://github.com/vllm-project/vllm/issues/11267
vllm,这是一个用户提出需求的issue，主要涉及对象是为了在vllm中加入ray[default]以便实现分布式推理功能。,https://github.com/vllm-project/vllm/issues/11265
vllm,这是一个功能需求的issue，涉及主要对象是为vllm项目添加CPU docker image构建流水线。,https://github.com/vllm-project/vllm/issues/11261
vllm,这是一个用户提出需求的issue，主要涉及模型的重构问题；用户希望将Qwen2-VL改造为使用合并的多模态处理器。,https://github.com/vllm-project/vllm/issues/11258
vllm,这个issue类型是功能改进，主要涉及Executor的变化，用户提出了使用`get_node_and_accelerator_ids`替换`get_node_and_gpu_ids`的需求。,https://github.com/vllm-project/vllm/issues/11257
vllm,这个issue类型是功能需求提出，主要涉及的对象是VLLM项目的平台无关执行器。,https://github.com/vllm-project/vllm/issues/11256
vllm,这是一个用户提出需求的issue，主要涉及vLLM模型中LoRA支持的问题。用户反馈使用LoRA适配器后推理结果明显较差，希望能够支持多模态模型的LoRA，同时询问是否有临时解决方案和预计支持时间。,https://github.com/vllm-project/vllm/issues/11255
vllm,该issue类型为提出需求，主要涉及LoRA支持与Mistral模型的兼容性。原因可能是LoRA支持在Ultravox模型中的应用也可适用于使用LlamaForCasualLM架构的Mistral模型。,https://github.com/vllm-project/vllm/issues/11253
vllm,这个issue类型是需求提出，主要对象是xformers backend，Gemme2团队观察到在推理过程中去除softcapping会导致非常轻微的差异。,https://github.com/vllm-project/vllm/issues/11252
vllm,该问题单为用户提出需求类型，主要涉及的对象为VLLM的核心功能。,https://github.com/vllm-project/vllm/issues/11243
vllm,这个issue类型是功能改进，主要涉及的对象是代码中注册的`shutdown`函数，由于使用`weakref.finalize`替代`atexit`导致了更好的实现方式。,https://github.com/vllm-project/vllm/issues/11242
vllm,这个issue属于性能优化类别，问题主要涉及了RMSNorm内核的比较和性能测试。这个issue是为了比较vLLM自定义算子和Flashinfer之间内核差异的性能测试。,https://github.com/vllm-project/vllm/issues/11241
vllm,这个issue是一个功能需求提议，涉及到性能优化和CI/Build过程中的GPU资源管理，由于需要减少GPU资源的消耗并允许并发作业运行，故引入了新的CPU-based runners和Modal client来动态分配GPU资源。,https://github.com/vllm-project/vllm/issues/11239
vllm,这是一个功能改进的issue，主要涉及到API server和AsyncLLM的性能优化和错误处理。其中存在的问题是需要提高API server和AsyncLLM的性能，并改进错误处理，以及减少zmq sockets的使用。,https://github.com/vllm-project/vllm/issues/11237
vllm,该issue属于用户提出需求类型，主要涉及的对象是DeepSeek-VL2模型的支持问题，用户想要了解为何还未支持该模型。,https://github.com/vllm-project/vllm/issues/11236
vllm,这是一个文档更新类型的issue，主要涉及多节点分布式服务环境下使用GPU性能计数器的问题。由于启动脚本在没有管理员权限下启动Docker容器，导致GPU性能分析和追踪功能受限，用户希望文档能提供解决办法。,https://github.com/vllm-project/vllm/issues/11235
vllm,这个issue类型是改进请求，主要涉及LoRA内核的优化和性能改进。由于对sgmv的水平融合操作，带来了性能提升和单位测试的更新。,https://github.com/vllm-project/vllm/issues/11234
vllm,这是一个需求性问题，主要涉及文档的重新排序。由于乱序的示例使得在IDE开发过程中难以找到需要修改/调试的功能。,https://github.com/vllm-project/vllm/issues/11228
vllm,该issue类型为用户提出需求，请教问题，主要涉及离线部署文本语音模型和知识库作为个性化问答模型的指导。原因可能是用户对如何实现该功能及可用性有疑问。,https://github.com/vllm-project/vllm/issues/11223
vllm,这是一个功能需求类型的issue，涉及平台插件框架的添加。由于需要添加新的平台插件框架，故引起了这个issue。,https://github.com/vllm-project/vllm/issues/11222
vllm,这个issue是关于安装问题的帮助请求，涉及主要对象为vllm软件用户。该问题可能由于缺少CUDA库或链接错误导致无法正确安装vllm。,https://github.com/vllm-project/vllm/issues/11215
vllm,这是一个优化建议类型的issue，主要涉及到提高输入准备的性能。,https://github.com/vllm-project/vllm/issues/11214
vllm,该issue是关于优化Dockerfile构建过程，属于性能提升类型，涉及的主要对象是ARM64系统。通过简化构建流程和优化要求处理，以确保在ARM64+CUDA兼容性下正确安装torch和bitsandbytes，以解决构建过程繁琐的问题。,https://github.com/vllm-project/vllm/issues/11212
vllm,这个issue类型是功能改进，涉及主要对象为在ARM64系统上构建Dockerfile，通过简化构建流程并优化要求处理来确保torch和bitsandbytes在ARM64+CUDA兼容性上正确安装。,https://github.com/vllm-project/vllm/issues/11211
vllm,这个issue类型是功能需求，涉及的主要对象是LoRA模块和Jamba、Mamba模型；用户提出这个需求是因为希望添加LoRA支持，包括对建模和权重加载的变化。,https://github.com/vllm-project/vllm/issues/11209
vllm,这是一个用户提出需求的issue，主要对象是Cohere2ForCausalLM模型，用户希望为其添加类似HuggingFace Transformers中支持的滑动窗口功能。,https://github.com/vllm-project/vllm/issues/11203
vllm,这是一个用户提出需求的issue，主要涉及如何在私人仓库中运行模型，可能由于缺乏明确的使用说明而导致问题。,https://github.com/vllm-project/vllm/issues/11202
vllm,这是一个升级维护请求，主要涉及bitsandbytes库，用户请求将其升级到最新版本0.45.0。,https://github.com/vllm-project/vllm/issues/11201
vllm,这是一个用户提出需求的issue，主要涉及文档的更新。该问题由于缺乏相关文档而导致用户需求补充文档。,https://github.com/vllm-project/vllm/issues/11197
vllm,该问题类型为功能增强（feature enhancement），主要对象是 VLM 的离线批量推理基准测试。由于希望添加支持其他框架进行比较，因此被提出。,https://github.com/vllm-project/vllm/issues/11196
vllm,这个issue类型为功能需求，主要对象是进行离线批量推断基准测试的VLM模型。这个需求可能是为了在MMMPro视觉方面添加其他框架支持来进行比较。,https://github.com/vllm-project/vllm/issues/11195
vllm,这是一个升级请求类的issue，涉及的主要对象是FlashInfer attention backend。该issue由于需要升级到v0.2.0版本而被创建。,https://github.com/vllm-project/vllm/issues/11194
vllm,这是一个功能增强的issue，主要涉及VLM（multimodal language model），旨在为其添加前缀缓存以提高性能。,https://github.com/vllm-project/vllm/issues/11187
vllm,这个issue属于更新需求，涉及到更新compressedtensor到最新版本0.8.1。由于当前使用的版本可能存在功能改进或者bug修复等需求，导致用户希望将库更新到最新版本以获得这些改进。,https://github.com/vllm-project/vllm/issues/11183
vllm,这是一个用户提出需求的issue，主要对象是希望支持新模型Cohere2 (Command R7B)，但可能会遇到难点，因为模型中的部分特征和架构不符合当前支持的模型。,https://github.com/vllm-project/vllm/issues/11181
vllm,这是一个功能需求类型的issue，主要涉及的对象是benchmark_serving.py脚本。由于需要测试内存缓存（MM cache）的HIT/MISS情况，开发者提出了增加重复图片选项的需求。,https://github.com/vllm-project/vllm/issues/11177
vllm,这是一个用户提出需求的issue，主要涉及的对象是benchmark_serving.py脚本，由于缺少tokenizer_mode参数导致用户需要增加该功能。,https://github.com/vllm-project/vllm/issues/11174
vllm,这个issue类型是用户提出需求或寻求帮助，主要涉及XPU依赖缺失的问题。用户可能遇到XPU相关功能无法正常运行或安装失败的情况。,https://github.com/vllm-project/vllm/issues/11173
vllm,这个issue类型是需求修改，主要对象是VLLM项目中的软件实现。由于软件实现中缺少logits的软上限处理，导致在CPU后端上运行Gemna2和PaliGemma 2时产生了微小差异，需要修改错误警告以允许这些模型正常运行。,https://github.com/vllm-project/vllm/issues/11170
vllm,这个issue是关于需求添加，主要涉及的对象是模型。由于需要添加rwkv6 linear attention模型，涉及的代码从https://github.com/sustcsonglin/flashlinearattention进行了适配。,https://github.com/vllm-project/vllm/issues/11166
vllm,这是一个需求提出的issue，主要涉及到在VLLM模型中添加从模型加载生成配置的功能。原因是为了允许加载和覆盖默认的抽样参数，以及为了保留现有行为而默认设置该标志为None。,https://github.com/vllm-project/vllm/issues/11164
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM的硬件后端插件化功能。这个问题的原因是vLLM支持的硬件后端越来越多，导致代码复杂且维护困难，为了解决这个问题，提出了支持硬件可插拔的解决方案。,https://github.com/vllm-project/vllm/issues/11162
vllm,这是一个关于用户提出需求的issue，主要涉及如何通过CLI来指定额外参数进行vLLM终端命令，导致用户在配置guided decoding时遇到挑战。,https://github.com/vllm-project/vllm/issues/11153
vllm,这是一个提出需求的RFC类型的issue，主要涉及vLLM的Neuron Backend与V1架构的集成方法。由于当前的neuron backend受限于支持feature的组合，提出了一些变化以支持更多类型的模型变体。,https://github.com/vllm-project/vllm/issues/11152
vllm,这是一个用户提出需求的 issue，主要对象是前端代码。,https://github.com/vllm-project/vllm/issues/11150
vllm,这个issue属于功能请求类型，主要涉及到`ChatCompletionRequest`和`CompletionRequest`对象，用户提出了希望新增`logits_processors`参数的需求来直接指导抽样过程。,https://github.com/vllm-project/vllm/issues/11149
vllm,这个issue属于功能需求。主要涉及的对象是PaliGemma 2。原因可能是要对现有功能进行一些改进以支持PaliGemma 2。,https://github.com/vllm-project/vllm/issues/11142
vllm,这是一个关于性能优化的Issue，主要涉及到软件Outlines的更新和线程池大小调整。由于之前线程池大小被硬编码为2，而更新后根据CPU核数动态调整，虽然效果不是很显著，但仍有可测量的改进。,https://github.com/vllm-project/vllm/issues/11140
vllm,这个issue类型是需求提出，主要涉及的对象是在vLLM中提升Ray支持以改善资源管理、弹性和性能。原因是由于当前与Ray集成存在的挑战导致了资源分配延迟、版本兼容性以及部署效率等方面的问题。,https://github.com/vllm-project/vllm/issues/11137
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM的pipeline parallelism功能。这个问题是由于Ray应用作为调度器需要更详细的信息来有效管理资源分配和增强性能。,https://github.com/vllm-project/vllm/issues/11136
vllm,这是一个功能需求类型的issue，涉及的主要对象是PG_WAIT_TIMEOUT的设置。由于PG_WAIT_TIMEOUT目前是硬编码的，需要将其配置化以适应不同环境。,https://github.com/vllm-project/vllm/issues/11135
vllm,这是一个功能需求（Feature Request）类型的issue，主要涉及Ray placement group在资源准备时间方面的需求。由于集群资源准备时间较长，用户希望允许Ray placement group等待更多时间以确保资源就绪。,https://github.com/vllm-project/vllm/issues/11134
vllm,这个issue是一个功能需求提案，主要涉及的对象是V1版本的异步调度功能。由于异步调度的实现，导致了GPU利用率持续为100%，但在使用cudagraph时出现了精度不一致的问题，需要讨论解决。,https://github.com/vllm-project/vllm/issues/11133
vllm,这是一个用户提出需求的 Issue，主要涉及使用不带 GPU 的节点进行 ray 功能的请求，原因是当前的分布式功能只支持 GPU。,https://github.com/vllm-project/vllm/issues/11131
vllm,这是一个用户提出需求的issue，涉及到frontend中分离离线推理中的池化API。由于模型的任务需要，对`LLM.encode()` API进行了拆分，以避免访问池化输出时的数据格式混乱。,https://github.com/vllm-project/vllm/issues/11129
vllm,这个issue类型是用户提出需求，主要对象是EAGLE在vLLM上的使用问题。由于无法成功运行EAGLE，用户寻求帮助解决问题。,https://github.com/vllm-project/vllm/issues/11126
vllm,这个issue是用户提出需求类型的，主要对象是vLLM的文档。用户提出了希望添加媒体包的链接以获取vLLM的logo。,https://github.com/vllm-project/vllm/issues/11121
vllm,这个issue是一个功能需求，主要涉及了支持Lora Prometheus指标的功能。原因是要增强vLLM中LoRA管理功能的生产环境。,https://github.com/vllm-project/vllm/issues/11117
vllm,这个issue类型是文档更新，主要涉及到的对象是LlamaStack远程vLLM指南的链接。由于链接需要更新并发布到LlamaStack网站，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/11112
vllm,该issue类型为优化提升，主要涉及输入准备过程中的性能优化。原因是为了减少小批量输入过程中创建新张量所引入的延迟。,https://github.com/vllm-project/vllm/issues/11111
vllm,该issue属于功能改进类型，主要涉及VLLM V1启动时的错误处理。由于当前的错误处理流程导致用户不容易看到错误的根本原因，因此希望能够改进错误信息的展示方式。,https://github.com/vllm-project/vllm/issues/11109
vllm,这是一个需求提出类型的issue，主要涉及的对象是torch.compile中的fast inductor，在提出这个需求的背后可能是为了提高模型训练的效率与性能。,https://github.com/vllm-project/vllm/issues/11108
vllm,这个issue类型是功能需求提出类型，主要对象是关于返回输入token的logits，而非生成答案。由于用户希望获取输入token的logits，而非生成答案。,https://github.com/vllm-project/vllm/issues/11106
vllm,这个issue是关于优化代码质量，主要涉及到使用mypy检查代码。由于一些具体情况需要进一步审查，导致了提出这些需要改进的问题。,https://github.com/vllm-project/vllm/issues/11105
vllm,这个issue类型为用户提出需求，主要涉及对象是TPUModelRunner，用户希望在TPU上实现有指导的解码功能以获取结构化输出。,https://github.com/vllm-project/vllm/issues/11104
vllm,这是一个用户提出需求的issue，主要询问新发布的多模态模型internVL2_5何时会被vllm支持。,https://github.com/vllm-project/vllm/issues/11101
vllm,这个issue类型是功能添加，涉及的主要对象是CPU backend，用户提出了增加MultiLoRA在CPU上的实现，并进行了相关测试以及更新操作。,https://github.com/vllm-project/vllm/issues/11100
vllm,这个issue属于功能需求类型，涉及主要对象为CI/Build系统，由于需要在AMD环境中启用前缀缓存测试步骤。,https://github.com/vllm-project/vllm/issues/11098
vllm,该issue类型为功能更新反馈，涉及主要对象为InternVL模型。Bug修复导致RTX3090上可成功运行InternVL2.526B。,https://github.com/vllm-project/vllm/issues/11095
vllm,"该issue属于文档更新类型，主要涉及到关于汇集模型和嵌入模型的文档更新。原因是希望更新更多文档来指向更通用的""汇集模型""而不是""嵌入模型""。",https://github.com/vllm-project/vllm/issues/11093
vllm,该issue属于用户提出需求，主要对象是对LoRA模型和基础模型的度量指标进行区分。由于当前度量指标都统一汇总在基础模型的度量指标下，导致无法准确监测和分析LoRA模型的使用情况和性能。,https://github.com/vllm-project/vllm/issues/11091
vllm,这是一个优化性质的issue，主要涉及到平台的代码结构调整。,https://github.com/vllm-project/vllm/issues/11085
vllm,这个issue类型是优化提升，主要涉及VLM测试的分组调整。,https://github.com/vllm-project/vllm/issues/11083
vllm,该issue属于用户提出需求类型，主要涉及的对象是`torch.compile`的性能问题，用户通过追踪forward time来测试`torch.compile`的性能，以便更好地衡量优化效果。,https://github.com/vllm-project/vllm/issues/11081
vllm,该issue类型为需求提出，主要涉及的对象是改进N-gram speculation功能，用户提出希望能够针对除了prompt之外的任意token序列进行ngram推测。因为当前功能限制了只能在prompt中进行ngram推测，用户希望能够更灵活地使用ngram功能。,https://github.com/vllm-project/vllm/issues/11079
vllm,这是一个用户提出需求的类型，主要对象是torch.compile的使用指南。这个需求可能是因为现有文档不够清晰或详细，用户需要更多关于torch.compile的使用信息。,https://github.com/vllm-project/vllm/issues/11078
vllm,这是一个特征需求的issue，主要涉及添加W8A8整型不均匀/对称模型。,https://github.com/vllm-project/vllm/issues/11075
vllm,这个issue类型是功能改进提议，涉及到对V1版本使用多进程方法的默认设置。由于之前的代码强制使用`spawn`多进程方法导致在某些配置中出现问题，所以作者将其更改为默认开启，并尝试解决已有代码中的相关问题。,https://github.com/vllm-project/vllm/issues/11074
vllm,该issue为用户提出需求，主要对象是关于vLLM的logo和品牌指南。用户希望获取vLLM logo的SVG文件，但无法找到透明背景下的浅色和深色变体。,https://github.com/vllm-project/vllm/issues/11066
vllm,该issue类型为需求提议，主要涉及支持地理空间模型的添加。由于现代模型不仅针对文本的生成，还涉及从文本或图像输入生成图像，因此该RFC旨在支持生成图像的模型，并展示相关模型实例的示例。,https://github.com/vllm-project/vllm/issues/11065
vllm,这是一个用户提出需求的类型，该问题涉及主要的对象是VLLM代码库中的硬件和CPU支持。,https://github.com/vllm-project/vllm/issues/11063
vllm,这是一个用户提出需求的issue，主要涉及torch.compile模块，用户希望添加一个标志来跟踪 batchsize 统计信息。,https://github.com/vllm-project/vllm/issues/11059
vllm,这是一个建议性能优化的issue，主要涉及到了`fused_moe_kernel`的优化处理。原因是`EM`和`num_valid_tokens`参数不需要特殊化，但由于其频繁变化导致了性能影响。,https://github.com/vllm-project/vllm/issues/11056
vllm,这是一个功能需求类的issue，涉及vLLM项目中日志记录功能的改进，主要对象是请求ID。由于当前实现中仍在使用随机ID而非xrequestid，用户建议考虑使用xrequestid以提升请求追踪效果。,https://github.com/vllm-project/vllm/issues/11050
vllm,这是一个关于功能增强的issue，主要涉及了测试代码。原因可能是为了方便启用guided decoding测试而进行的更新。,https://github.com/vllm-project/vllm/issues/11048
vllm,该issue为功能需求类型，主要涉及TPU性能分析功能。由于用户希望能够使用TPU进行性能分析，因此提出了需要通过profile traces实现TPU性能分析的需求。,https://github.com/vllm-project/vllm/issues/11041
vllm,这是一个用户提出需求的类型issue，主要对象是项目的README.md文件。由于vLLM加入pytorch生态系统，用户提出需要更新README.md文件以添加相关新闻。,https://github.com/vllm-project/vllm/issues/11034
vllm,这是一个功能需求的提出。这个问题单涉及的主要对象是 Triton Paged Attn Decode Kernel。,https://github.com/vllm-project/vllm/issues/11033
vllm,这个issue是一个性能优化问题，涉及到V1版本中模型运行器在文本模型中使用`input_ids`而不是`input_embeds`，由于这种改变未在CUDA图中包含嵌入层，导致了性能略微下降的情况。,https://github.com/vllm-project/vllm/issues/11032
vllm,这个issue类型是功能需求，主要涉及的对象是关于监控tokens per step的metrics，因为需要用于`torch.compile`。,https://github.com/vllm-project/vllm/issues/11031
vllm,这是一个功能需求提出的issue，主要涉及的对象是针对添加多进程HPU执行器的问题。,https://github.com/vllm-project/vllm/issues/11030
vllm,这是一个功能需求的issue，主要涉及到在VLLM中添加OpenAI API支持input_audio。原因是由于当前版本还没有输出音频支持，所以需要针对输入音频的格式进行适配。,https://github.com/vllm-project/vllm/issues/11027
vllm,这是一个用户提出需求的issue，主要涉及在vLLM中增加了SwiftKV模型。原因是需要在模型中实现提前退出一些标记而不是其他标记的功能，目的是最小化对vLLM核心代码的更改。,https://github.com/vllm-project/vllm/issues/11023
vllm,这个issue类型是需求提出，主要对象是xgrammar库，由于xgrammar需要升级到0.1.6版本来解决使用JSON Schema时的问题。,https://github.com/vllm-project/vllm/issues/11021
vllm,这是一个功能增强类的issue，主要涉及到VLM预处理器的哈希功能和多模态预处理器/映射器的缓存问题。由于性能分析尚未完成，多模态预处理器缓存功能默认处于禁用状态，用户可能在请求中需要关于性能分析、编码器缓存和前缀缓存的帮助。,https://github.com/vllm-project/vllm/issues/11020
vllm,这个issue是用于提出功能需求的，主要对象是vllm模型。用户提出在同一个模型上进行多次推断时需要重复加载权重，导致时间消耗过多，希望能够通过vllm只加载一次权重来解决这个问题。,https://github.com/vllm-project/vllm/issues/11014
vllm,这个issue类型属于技术改进，涉及修改CI依赖列表的Python版本；用户为了基于Python 3.12重新编译，解决前一次使用Python 3.9编译时可能出现的问题。,https://github.com/vllm-project/vllm/issues/11013
vllm,这是一个用户提出需求的issue，主要涉及对象是关于vllm下的Llama-3-Groq-8B-Tool-Use工具使用支持。用户无法使用Groq/Llama3Groq8BToolUse中的工具使用功能，希望有一个支持此功能的tool_chat_template。,https://github.com/vllm-project/vllm/issues/11009
vllm,这是一个用户提出需求的issue，主要对象是新增支持Emu3模型。用户想知道何时会支持Emu3模型，可能由于当前模型vllm不支持Emu3模型引发。,https://github.com/vllm-project/vllm/issues/11008
vllm,这是一个用户提出需求的类型，主要涉及vllm在多轮图片对话中的支持。由于缺乏参数设置支持，用户在上传第二张图片进行对话时遇到错误。,https://github.com/vllm-project/vllm/issues/11006
vllm,这是一个关于性能优化的issue，主要涉及torch.compile的动态时间追踪，用户提出了关于编译时间分析的需求。,https://github.com/vllm-project/vllm/issues/11005
vllm,这是一个需求提出类型的issue，主要涉及的对象是LoRA功能管理。由于缺乏对不支持特性的验证和错误处理，用户提出需要添加PEFTHelper类来优化LoRA特性管理。,https://github.com/vllm-project/vllm/issues/11003
vllm,这是一个需求类型的issue，主要涉及到Qwen2-VL模型在AWS Neuron上的支持问题。由于目前Neuron仅支持部分模型架构，用户提出希望将Qwen2-VL也纳入支持范围。,https://github.com/vllm-project/vllm/issues/11002
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM的Block Manager。由于当前block-based架构在I/O效率上存在限制，需要更好地利用I/O带宽，因此提出了使用Block Group Manager替换现有的Block Manager来改善问题。,https://github.com/vllm-project/vllm/issues/11001
vllm,这是一个需求提出类型的issue，涉及主要对象是代码中的日志输出。由于之前使用的`logger.info`方式难以在中间插入字段，需要修改为使用fstring以便更容易插入字段。,https://github.com/vllm-project/vllm/issues/10999
vllm,该issue类型为文档更新，主要涉及V1支持列的添加至多模态模型文档。由于V1对多模态模型的支持将逐步推出，因此该PR旨在文档中添加一列，以指示V1中可用的内容。,https://github.com/vllm-project/vllm/issues/10998
vllm,这是一个用户提出需求的issue，主要涉及到cudagraph的batchsize padding逻辑。用户希望统一这个逻辑并允许自定义cudagraph的捕捉大小，以加快启动时间和只捕捉用户关心的大小的cudagraph。,https://github.com/vllm-project/vllm/issues/10996
vllm,该issue类型为用户提出需求，涉及的主要对象是Cutlass库中的稀疏量化和非量化内核，由于需要支持对2of4稀疏模型的压缩张量操作，导致用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/10995
vllm,这个issue是关于用户提出需求的问题，用户想知道如何通过curl命令调用特定模型的endpoint。由于用户尝试调用时遇到困难，希望得到相应的支持和解决方案。,https://github.com/vllm-project/vllm/issues/10994
vllm,"这是一个功能需求类的issue，主要涉及到为Mamba-like模型（Jamba, Mamba, MambaFalcon等）添加PP支持。",https://github.com/vllm-project/vllm/issues/10992
vllm,这是一个优化性质的Issue，主要涉及 V1 flash-attn 模块中 CPU 的性能开销。,https://github.com/vllm-project/vllm/issues/10989
vllm,这是一个功能需求提出类型的issue，主要涉及的对象是V1-rearch模型的图像推断支持。原因是为了实现MRoPE功能，最终用户希望修复图像嵌入作为输入测试的问题。,https://github.com/vllm-project/vllm/issues/10988
vllm,这个issue类型是需求更新，主要对象是README.md文件。由于最新消息部分的更新需要以可折叠列表的形式呈现，因此提出了这一需求。,https://github.com/vllm-project/vllm/issues/10985
vllm,该issue属于用户提出需求类型，主要涉及torch.compile中允许候选编译尺寸的设置。由于该功能扩展了编译尺寸的选择范围，并提供了更灵活的指定方式，用户可以更方便地控制编译尺寸，从而优化模型性能。,https://github.com/vllm-project/vllm/issues/10984
vllm,这是一个用户提出需求的issue，主要涉及改进benchmarking代码，更新了NVIDIA CUTLASS dense FP8 matmul kernels的选择启发式。,https://github.com/vllm-project/vllm/issues/10982
vllm,该issue类型是功能需求，主要对象是V1 engine中的parallel sampling功能。这个需求是由于添加了对于异步LLM和LLMEngine的并行采样支持。,https://github.com/vllm-project/vllm/issues/10980
vllm,这是一个需求提出型的issue，主要涉及的对象是Phi3Vision模型的输入处理器。,https://github.com/vllm-project/vllm/issues/10977
vllm,这个issue是关于功能需求的，主要涉及到VLLM的核心功能executor，问题是instance id将附加到vllm_config对象而不是通过环境变量传递，导致了这个改变的原因是为了简化实例化id的实现。,https://github.com/vllm-project/vllm/issues/10976
vllm,这是一个用户提出需求的issue，主要对象是使用vllm在docker中运行本地模型，并寻求相关集成方法的帮助。由于用户不知道如何整合本地模型和vllm，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/10974
vllm,该issue属于用户提出需求类型，主要对象是在torch.compile中使用depyf库来dump内部信息。原因是用户希望能够通过运行特定命令获取包括Dynamo编译的字节码、Inductor生成的内核、maxautotune配置、各种转换阶段的图模块等信息。,https://github.com/vllm-project/vllm/issues/10972
vllm,这是一个功能建议，涉及到前端，旨在通过使用`XRequestId`标头中的请求ID来填充引擎中的请求ID，以改善追踪性能，使用户能够将vLLM日志与生成它们的请求相关联。,https://github.com/vllm-project/vllm/issues/10968
vllm,这是一个关于功能需求的issue，主要涉及vllm模型加载速度的问题，用户提出需要在保存分片状态时也包含GPU P2P访问缓存。,https://github.com/vllm-project/vllm/issues/10967
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM在支持加载已进行TP分片的权重时的功能。,https://github.com/vllm-project/vllm/issues/10965
vllm,"这是一个用户提出需求的issue，主要涉及的对象是新增模型""LLama3.3""的规划。由于该模型是新添加的，用户请求vllm添加对该模型的支持。",https://github.com/vllm-project/vllm/issues/10964
vllm,这是一个关于代码优化的issue，主要涉及到日志消息的清理，其中涉及到多个不太有趣的日志消息被修改。由于部分日志消息不够具有普遍性，需要进行清理操作。,https://github.com/vllm-project/vllm/issues/10961
vllm,这是一个用户提出需求的issue，主要对象是在该项目中运行mypy检查。原因可能是为了改进代码质量和类型检查。,https://github.com/vllm-project/vllm/issues/10959
vllm,这是一个需求提出的issue，主要涉及了`MergedQKVParallelLinearWithLora`的重构和清理。,https://github.com/vllm-project/vllm/issues/10958
vllm,这是一个功能增强的issue，主要涉及LoRA支持的添加，通过V1版本运行LoRA请求，并且由于CUDA图表的存在导致性能差距。,https://github.com/vllm-project/vllm/issues/10957
vllm,这个issue是用户提出需求和请教问题的类型，主要对象是如何使用Python脚本来启动一个FastAPI服务来进行internvl2-8b的推理，而不是使用终端命令vllm serve。用户不清楚如何将vllm集成到他们的项目中。,https://github.com/vllm-project/vllm/issues/10953
vllm,这是一个用户提出需求的issue，主要涉及如何通过vllm获取token级别的概率分数。由于在使用vllm时无法得到与huggingface库相同的结果，用户寻求如何在vllm中实现相同操作。,https://github.com/vllm-project/vllm/issues/10951
vllm,这是一个需求类型的issue，主要涉及支持torchao量化模型。由于vllm不支持torchao量化方法，导致用户无法成功部署该模型。,https://github.com/vllm-project/vllm/issues/10947
vllm,"该问题类型为用户提出需求，主要涉及的对象是新增""optimum-neuron""功能。由于vllm with aws neuron目前只支持llama和mistral，用户希望添加optimum-neuron功能来支持Qwen2.5等模型。",https://github.com/vllm-project/vllm/issues/10946
vllm,这个issue类型为性能优化提案，涉及主要对象为vllm中的版本V1，用户寻求帮助来优化性能提升。,https://github.com/vllm-project/vllm/issues/10945
vllm,这是一个用户提出需求的类型，主要对象应该是某个功能或某个特性。由于未提供具体内容，无法分析出导致的原因和问题类型。,https://github.com/vllm-project/vllm/issues/10943
vllm,这个issue属于需求提出类型，主要涉及的对象是动态KV缓存压缩，用户提出了在vLLM框架上基于KV稀疏性进行动态KV缓存压缩的需求。,https://github.com/vllm-project/vllm/issues/10942
vllm,这是一个关于代码编译时间记录的功能需求，主要对象是torch.compile模块，用户希望能够添加日志记录编译时间的功能。,https://github.com/vllm-project/vllm/issues/10941
vllm,这是一个用户提出需求的issue，主要对象是vLLM模型，用户请求添加对BERT和DistilBERT模型进行文本分类任务支持。这是因为这些模型在多语言支持、参数数量和推理速度等方面比Qwen2模型更具优势。,https://github.com/vllm-project/vllm/issues/10939
vllm,这是一个用户提出需求的issue，主要涉及对象是AsyncLLMEngine的generate方法以及多模态输入支持，用户询问了如何在多模态模型中输入图像的问题。,https://github.com/vllm-project/vllm/issues/10937
vllm,这个issue属于用户提出需求的类型，主要对象是vLLM的release pipeline。造成这个问题的原因是目前缺乏针对构建和推送vLLM TPU镜像的发布管道任务。,https://github.com/vllm-project/vllm/issues/10936
vllm,这是一个功能改进类型的issue，涉及到torch.compile中的size tuning。由于之前编译特定尺寸的模型存在性能问题，这个issue解决了这一瓶颈，显著提高了编译性能。,https://github.com/vllm-project/vllm/issues/10933
vllm,该issue类型为用户提出需求，用户希望通过Python脚本实现vllm的功能定制化，主要涉及的对象是vllm服务的使用和定制化需求。由于用户想要自定义日志输出，因此提出了如何通过Python脚本实现vllm功能的问题。,https://github.com/vllm-project/vllm/issues/10929
vllm,这是一个关于正在进行中的功能开发的issue，主要涉及到深度学习模型加速相关的内容。原因是项目还很混乱，设计不足，导致部分功能未完成或存在问题。,https://github.com/vllm-project/vllm/issues/10927
vllm,这是一个用户提出需求的issue，主要对象是如何在vllm中运行特定模型推断。用户提出这个问题可能是因为不知道如何将特定模型集成到vllm中。,https://github.com/vllm-project/vllm/issues/10926
vllm,这个issue类型是需求提出，主要涉及的对象是VLLM中的核心功能。由于希望统一`_run_workers`函数，以实现设备无关的执行器，因此需要移除`use_dummy`驱动。,https://github.com/vllm-project/vllm/issues/10920
vllm,这个issue属于用户提出需求类型，主要涉及提供预构建的CPU Docker镜像，由于当前需要自行构建Docker镜像，希望能够提供预构建的镜像以便用户使用。,https://github.com/vllm-project/vllm/issues/10919
vllm,这是一个功能改进类型的issue，主要涉及Punica中LoRA函数接口的重组整理。导致该问题的原因是当前接口较为复杂和组织混乱，需要进行简化和标准化以方便在其他平台上支持LoRA。,https://github.com/vllm-project/vllm/issues/10917
vllm,这个issue类型是用户提出需求，主要涉及到vllm中processor的输出问题，用户在代码中发现一个例子让他感到困惑，希望能够得到解释。,https://github.com/vllm-project/vllm/issues/10910
vllm,这是一个需求添加新模型的issue，主要涉及到Bamba模型。由于新增模型的实现过程中存在部分问题，导致无法支持chunked prefill，需要完善相关功能。,https://github.com/vllm-project/vllm/issues/10909
vllm,这是一个功能需求的issue，涉及的主要对象是请求统计更新和请求统计类型。,https://github.com/vllm-project/vllm/issues/10907
vllm,这个issue类型为功能增强，主要对象是torch.compile中的RMSNorm + (fp8) quant fusion功能。,https://github.com/vllm-project/vllm/issues/10906
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm的v1模型添加logits processor的支持。由于引入了v1的SamplingMetadata，需要决定如何处理v0版本的LogitsProcessor，导致需要讨论是否应支持同时处理v0和v1引擎的问题。,https://github.com/vllm-project/vllm/issues/10905
vllm,这个issue是关于更新llama 3.2模板以支持系统提示与图像，并不是bug报告。问题涉及到更新聊天模板以支持使用系统提示与图像。,https://github.com/vllm-project/vllm/issues/10901
vllm,这个issue类型是功能改进，主要涉及的对象是PT HPU lazy backend。这个改进针对的是目前需要显式启用torch.compile并且性能最佳实践是使用HPUGraphs，通过禁用torch.compile并默认启用lazy collectives，解决了多HPU推理过程中的报错问题。,https://github.com/vllm-project/vllm/issues/10897
vllm,该issue属于功能需求类型，主要涉及到AQLM支持矩阵向量乘法的FP8/INT8情况，用户因推理大规模LLaMA 3 70B模型速度较慢而提出是否AQLM支持该功能。,https://github.com/vllm-project/vllm/issues/10894
vllm,这个issue类型为功能改进，主要涉及的对象是ViTs的注意力机制实现。通过添加`MultiHeadAttention`层来整合现有的重复注意力前向实现，以改进ViTs的性能。,https://github.com/vllm-project/vllm/issues/10893
vllm,这是一个用户提出需求的类型，主要涉及的对象是Vllm项目，用户希望在下一个版本中添加一些新功能，并询问了新版本发布的时间。,https://github.com/vllm-project/vllm/issues/10892
vllm,该issue属于用户提出需求，主要对象是serving VLM VILA。由于缺乏响应，导致用户提出了关于serving VLM VILA的需求。,https://github.com/vllm-project/vllm/issues/10889
vllm,这个issue类型为功能改进，涉及的主要对象是VLLM的CPU prefix caching机制，由于希望优化预填充步骤的效率而提出了这个改进建议。,https://github.com/vllm-project/vllm/issues/10888
vllm,这是一个更新维护类的issue，涉及将postmerge镜像从`vllmcitestrepo`更改为`vllmcipostmergerepo`，旨在修正相关实体引用问题。,https://github.com/vllm-project/vllm/issues/10887
vllm,这是一个用户提出需求的issue，涉及主要对象是如何进行多节点推理。由于文档未涵盖多节点推理的部分，用户希望了解如何实现多节点推理。,https://github.com/vllm-project/vllm/issues/10886
vllm,这是一个用户提出需求的issue，主要涉及对象是LogitsProcessor，用户想要访问当前生成的token。由于用户想要获取LogitsProcessor生成的token，但不清楚如何操作，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/10885
vllm,这是一个功能增强的issue，涉及到vLLM的Mooncake Transfer Engine支持的问题。,https://github.com/vllm-project/vllm/issues/10884
vllm,这是一个用户提出需求的issue，涉及到更新test-pipeline.yaml文件。原因可能是需要更新测试流水线的配置或内容。,https://github.com/vllm-project/vllm/issues/10882
vllm,这是一个功能需求，主要对象是vllm-tpu镜像用户，请求发布vllm-tpu镜像到DockerHub，因为目前用户需要手动构建镜像。,https://github.com/vllm-project/vllm/issues/10878
vllm,该issue属于需求类型，主要涉及Release pipeline中构建和推送`vllmopenai`镜像到Release ECR的任务。由于缺少这个功能，用户提出了关于构建和推送镜像的需求。,https://github.com/vllm-project/vllm/issues/10877
vllm,这是一个改进操作，目的是使`pynccl`测试更加健壮。,https://github.com/vllm-project/vllm/issues/10876
vllm,这个issue类型是需求变更，涉及到Release pipeline的队列命名变更。,https://github.com/vllm-project/vllm/issues/10875
vllm,这是一个建议性质的issue，涉及的主要对象是支持在CPU上卸载KV缓存。原因可能是之前的实现存在性能瓶颈，需要通过CPU卸载KV缓存来提升性能。,https://github.com/vllm-project/vllm/issues/10874
vllm,这个issue类型为文档更新，主要涉及的对象是安装vllm_test_utils包，由于缺少该包导致运行单元测试时出现问题。,https://github.com/vllm-project/vllm/issues/10872
vllm,这是一个技术改进的提议，主要涉及到在引擎中异步初始化xgrammar，以提高性能和并行处理。,https://github.com/vllm-project/vllm/issues/10871
vllm,这是一个用户提出需求的issue，主要涉及核心功能的改进，需要扩展XGrammar以支持所有类型的语法，并移除回退功能。,https://github.com/vllm-project/vllm/issues/10870
vllm,这个issue是一个功能需求，涉及VLM图片的哈希和映射缓存，用户希望实现跳过编码器执行和缓存结果。,https://github.com/vllm-project/vllm/issues/10868
vllm,这是一个用户提出需求的issue，主要涉及torch编译器中添加torch感应器传递以用于融合silu_and_mul与后续scaled_fp8_quant操作。此需求的原因可能是为了提高FP8 Llama和TTFT的性能。,https://github.com/vllm-project/vllm/issues/10867
vllm,这是一个用户提出需求的issue，主要涉及每个请求包含不同的上下文无关文法或正则表达式，询问vLLM是否支持此功能。,https://github.com/vllm-project/vllm/issues/10862
vllm,这是一个用户提出需求的类型，该问题单涉及主要对象为VLLM项目中的模型JambaForSequenceClassification。由于缺乏对应的模型支持，用户提出了需要添加JambaForSequenceClassification模型的需求。,https://github.com/vllm-project/vllm/issues/10860
vllm,"这是一个用户提出需求的类型。问题单涉及的主要对象是""Model""。由于缺乏对嵌入模型JambaClassification的支持，用户提出了添加该模型支持的需求。",https://github.com/vllm-project/vllm/issues/10858
vllm,这是一个用户提出需求的issue，涉及主要对象是vllm项目的Regional compilation support功能。,https://github.com/vllm-project/vllm/issues/10851
vllm,这是一个优化建议的issue，涉及到减少在`_filter_model_output`中多次调用`cudaStreamSynchronize`。原因是避免不必要的性能开销。,https://github.com/vllm-project/vllm/issues/10850
vllm,该issue类型为用户提出需求，主要涉及对添加DoRA支持的功能需求。由于DoRA在各种任务和backbones上表现出色，用户提出是否可以添加对其支持。,https://github.com/vllm-project/vllm/issues/10849
vllm,这个issue类型是功能需求提出，主要涉及分布式功能。由于缺少具体描述，无法准确分析具体原因。,https://github.com/vllm-project/vllm/issues/10843
vllm,这个issue是用户提出需求类型，主要涉及Model支持bitsandbytes quantization with minicpm model。这个问题由于缺乏bitsandbytes quantization支持而导致用户希望添加该功能。,https://github.com/vllm-project/vllm/issues/10842
vllm,这个issue类型是请求修改，主要对象是test-pipeline.yaml文件。由于可能需要更新测试管道的设定或配置，用户提出了更新test-pipeline.yaml的需求。,https://github.com/vllm-project/vllm/issues/10839
vllm,该issue类型为需求提出，主要涉及对象是torch.compile模块。这个问题由于编译上下文存在混淆，需要简化代码并在配置初始化期间确定cudagraph批处理大小。,https://github.com/vllm-project/vllm/issues/10838
vllm,这是一个文档更新的类型，涉及主要对象为KubeAI，更新原因可能是添加KubeAI到服务集成文档中。,https://github.com/vllm-project/vllm/issues/10837
vllm,这是一个用户提出需求的issue，主要涉及对象是LlavaMultiModalProjector。由于尚未支持TP和BNB量化，用户提出需要向llava multimodal projector添加TP和BNB量化支持的问题。,https://github.com/vllm-project/vllm/issues/10834
vllm,该issue为用户提出需求，主要涉及的对象是支持自定义CORS设置。由于缺乏对CORS中间件的支持，用户提出了添加自定义CORS设置的功能需求。,https://github.com/vllm-project/vllm/issues/10832
vllm,这是一个功能需求问题，涉及将lora偏置计算代码从layers.py和fully_sharded_layers.py移动到punica.py，以支持不同硬件平台上的lora功能。,https://github.com/vllm-project/vllm/issues/10829
vllm,这是一个用户提出需求的类型，主要对象是文档结构。原因是部分页面与其父级章节不相关，创建了一个新的“Usage”章节以容纳这些页面。,https://github.com/vllm-project/vllm/issues/10827
vllm,该issue属于代码改进类型，涉及主要对象是mistral_common版本。由于维护一致性导致的更新问题。,https://github.com/vllm-project/vllm/issues/10825
vllm,这是一个用户提出需求的issue，涉及的主要对象是LoRA support for LLama 3.2 Vision Models。用户因为需要在LLama 3.2 11B vision instruct中使用多个LoRAs，并且目前无法合并LoRA适配器到原始模型权重中，希望获得LoRA支持以及时间表和工作方法的帮助。,https://github.com/vllm-project/vllm/issues/10824
vllm,这是一个开发者提出的需求问题，主要涉及修改Attention机制的参数以满足类型检查需求。,https://github.com/vllm-project/vllm/issues/10822
vllm,该issue类型为功能需求，涉及的主要对象是vLLM中的Pooling任务划分。由于新增了关于不同类型Pooling模型的任务划分需求，希望实现更灵活的模型功能。,https://github.com/vllm-project/vllm/issues/10820
vllm,这是一个功能需求的issue，主要对象是在vLLM中实现分散预填和KV缓存传输。原因是为了改善预填和解码之间的连接方式和性能。,https://github.com/vllm-project/vllm/issues/10818
vllm,这个issue属于代码优化类型，主要涉及到移除已废弃的标识符并引入新的`deprecated`装饰器，以消除警告。,https://github.com/vllm-project/vllm/issues/10817
vllm,这个issue属于增加新功能类型，主要涉及的对象是模型GritLM。由于需要让vLLM更易于采用，所以进行了对原始存储库的GritLM/GritLM7B的架构名称修改。,https://github.com/vllm-project/vllm/issues/10816
vllm,这是一个用户提出需求的问题，主要涉及了如何在vllm中使用bitsandbytes量化的llava模型，由于vllm不支持llava，导致用户无法成功加载模型。,https://github.com/vllm-project/vllm/issues/10813
vllm,这是一个功能改进类的issue，主要涉及FlashAttention库中的`flash_attn_varlen_func`函数，使用`out`参数来避免冗余复制。由于这个改变，可能减少了不必要的内存复制操作，提升了性能。,https://github.com/vllm-project/vllm/issues/10811
vllm,这是一个功能需求的issue，主要涉及的对象是vllm库下的LLM核心功能。原因是需要支持在运行时销毁所有KV缓存，以提供更灵活的内存管理。,https://github.com/vllm-project/vllm/issues/10810
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加xgrammar作为引导生成提供者。原因是由于outlines相对较慢且难以集成，而新版本不注重可pickle性，这对于使用多处理引擎的我们来说是一个关键特性。,https://github.com/vllm-project/vllm/issues/10803
vllm,这是一个用户提出需求的类型的issue，主要对象是vLLM中的模型。由于需要更高的速度而非质量，用户提出可以直接访问draft模型的建议。,https://github.com/vllm-project/vllm/issues/10790
vllm,这是一个改进优化类型的issue，涉及主要对象为CPU vector types。由于ISA specifics被放置在不适合的位置，导致了代码可读性和可维护性下降。,https://github.com/vllm-project/vllm/issues/10787
vllm,这是一个用户提出需求的类型，主要涉及的对象是支持的模型。导致该问题的原因是当前版本的模型启动后得到的答案都是符号，用户希望尽快支持这个模型。,https://github.com/vllm-project/vllm/issues/10786
vllm,这是一个用户需求类型的issue，主要对象是LoRa模型加载在/models端点无法显示，可能是由于动态加载LoRa导致的。,https://github.com/vllm-project/vllm/issues/10784
vllm,"该issue类型为用户提出需求，主要涉及希望添加新模型""nvidia/Hymba-1.5B-Base""支持，并讨论了对于""Mamba""和注意力头的混合架构支持以及滑动窗口和元（内存）token的注意力。",https://github.com/vllm-project/vllm/issues/10783
vllm,这是一个性能优化改进的issue，主要涉及到vllm模型中的sin/cos buffers准备工作。,https://github.com/vllm-project/vllm/issues/10776
vllm,这是一个功能需求的issue，主要对象是vllm引擎。由于长序列预填充导致引擎变得不稳定，需要解决多用户同时请求时资源分配的问题。,https://github.com/vllm-project/vllm/issues/10774
vllm,这是一个功能优化的issue，涉及主要对象为`Molmo`模型权重加载，希望通过重构使用`WeightsMapper`和`AutoWeightsLoader`解决问题。,https://github.com/vllm-project/vllm/issues/10771
vllm,这是一个功能需求类型的issue，主要涉及的对象是vLLM模型中的embedding models。,https://github.com/vllm-project/vllm/issues/10769
vllm,这是一个功能改进类型的issue，主要涉及到平台端的设备硬编码问题。,https://github.com/vllm-project/vllm/issues/10768
vllm,这个issue是用户提出需求类型的问题单，主要涉及对象是vllm的日志打印功能。用户提出寻求在服务端查看模型输出的需求。,https://github.com/vllm-project/vllm/issues/10760
vllm,该issue类型为用户提出需求，主要涉及ChatCompletionRequest的默认值取自generation_config.json文件，由于用户未设置这些值导致的问题。,https://github.com/vllm-project/vllm/issues/10758
vllm,这是一个用户提出需求的issue，主要涉及到添加新的功能和优化代码结构，以区分设备名称。这个issue的原因可能是为了避免使用过多的if else语句并且需要更好地区分不同的设备名称。,https://github.com/vllm-project/vllm/issues/10757
vllm,该issue属于用户提出需求类型，主要涉及vllm的vision ID支持，用户寻求如何为processor分配vision ID。,https://github.com/vllm-project/vllm/issues/10756
vllm,这是一个关于用户提出需求的问题，涉及的主要对象是vllm项目的beam search功能。由于新方法对beam search功能进行了限制，用户请求在新方法中加入对生成控制（top_p等）和受限制beam search的支持。,https://github.com/vllm-project/vllm/issues/10754
vllm,这是一个用户提出需求的issue，主要涉及的对象是embedding models。原因是目前只有crossencoder models支持`/score`端点，用户想要将其扩展到使用biencoding的embedding models上，以计算嵌入向量之间的余弦相似度得分。,https://github.com/vllm-project/vllm/issues/10752
vllm,这是一个优化代码的类型，主要涉及的对象是MiniCPMV，原因是为了简化权重加载代码和消除对LLMWrapper的使用。,https://github.com/vllm-project/vllm/issues/10751
vllm,该issue类型为更新需求，主要涉及vllm-hpu-extension的更新。由于需要引入PipelinedPA，导致需要将vllm-hpu-extension更新为50e10ea版本。,https://github.com/vllm-project/vllm/issues/10750
vllm,这个issue类型是需求反馈，主要涉及的对象是vllm项目的模型并行配置，由于配置与张量并行要求不兼容，导致存在无法生成张量并行组的问题。,https://github.com/vllm-project/vllm/issues/10749
vllm,这是一个用户提出需求的issue，主要涉及需求是关于个别请求的能耗消耗。,https://github.com/vllm-project/vllm/issues/10748
vllm,这个issue属于功能更新类型，涉及的主要对象是 vllm-flash-attn 内核版本。由于当前版本存在较大的CPU开销，在启动内核时会导致性能下降，因此需要升级版本来减少CPU开销。,https://github.com/vllm-project/vllm/issues/10742
vllm,这是一个关于性能优化的issue，主要涉及logit bias实现方式的问题，提议用`scatter_add`替代慢速for循环，以提高生成速度。,https://github.com/vllm-project/vllm/issues/10741
vllm,这个issue是一个功能改进的请求，主要涉及的对象是RMSNorm和Mamba模型。该问题由于未能正确初始化RMSNorm层的权重，导致需要添加`elementwise_affine`参数来解决无法学习的RMSNorm层问题，同时也需要重新启用Mamba模型的权重加载跟踪功能。,https://github.com/vllm-project/vllm/issues/10739
vllm,"这是一个需求提交的issue，主要涉及支持新模型的问题，由于当前配置无法处理超过32,768 tokens的长文本输入。",https://github.com/vllm-project/vllm/issues/10737
vllm,这个issue是与性能优化相关的，主要涉及的对象是vllm-flash-attn。原因是为了提高`flash_attn_varlen_func`的运行速度，避免了两次冗余的复制操作。,https://github.com/vllm-project/vllm/issues/10736
vllm,这是一个优化性能的issue，主要涉及的对象是FlashAttention custom op。由于 `FlashAttnFunc` 继承 `torch.autograd.Function` 导致不必要的CPU开销，需要进一步减少FlashAttention op内的不必要CPU操作。,https://github.com/vllm-project/vllm/issues/10733
vllm,这是一个文档更新类的issue，主要涉及配置文件的文档更新，由于缺少参数描述而需要更新。,https://github.com/vllm-project/vllm/issues/10732
vllm,这个issue类型为功能需求，主要涉及的对象是对模型权重文件锁定功能的改进。,https://github.com/vllm-project/vllm/issues/10729
vllm,这个issue类型是功能改进，主要涉及Mooncake Transfer Engine在LLM服务中的应用。这个改进是为了使用Mooncake的Transfer Engine来实现KVCache的传输，取代原先的NCCL。,https://github.com/vllm-project/vllm/issues/10728
vllm,这个issue属于需求提出类型，主要涉及Mooncake框架下的vLLM集成，原因是为了实现更好的LLM推断性能。,https://github.com/vllm-project/vllm/issues/10727
vllm,这是一个需求类型的issue，主要涉及的对象是更新依赖项到torch_xla[tpu]的版本至20241126。,https://github.com/vllm-project/vllm/issues/10726
vllm,该issue属于新功能需求，主要涉及Ray executor的支持。由于之前出现的性能问题，该PR计划只支持Ray编译图版本的后端，解决了性能问题，需要进行清理测试、性能基准测试，并统一接口。,https://github.com/vllm-project/vllm/issues/10725
vllm,这是一个功能需求的issue，主要涉及vLLM的GPU缓存管理，用户提出了希望实现GPU内存中的KV缓存清理功能（或`睡眠模式`），以便在推理请求空闲时允许GPU执行其他计算任务。,https://github.com/vllm-project/vllm/issues/10714
vllm,这个issue类型是功能增强请求，主要涉及的对象是更新多模态处理器以支持Mantis模型。,https://github.com/vllm-project/vllm/issues/10711
vllm,这是一个性能优化的提案，提出将预填和解码处理统一在一个前向调用中，但在尝试后并未发现性能提升，希望通过进一步研究解释这种意外的减速原因。,https://github.com/vllm-project/vllm/issues/10707
vllm,这是一个用户提出需求的issue，主要涉及的对象是将genai_perf纳入夜间基准测试，由于需要的Python版本高于3.10，还未启用且需定义更多测试用例。,https://github.com/vllm-project/vllm/issues/10704
vllm,这个issue类型是功能更新，主要涉及mistral-format Pixtral接口的更新。,https://github.com/vllm-project/vllm/issues/10703
vllm,这是一个文档更新请求，针对 VLLM 模型架构的更新，主要对象是代码文档，可能由于描述与实际代码不符导致不一致。,https://github.com/vllm-project/vllm/issues/10701
vllm,这是一个功能需求的issue，主要涉及到V1模型的支持，因为缺少Qwen2VL的MRope实现而导致需要通过单独的PR来解决。,https://github.com/vllm-project/vllm/issues/10699
vllm,这个issue属于用户提出需求，主要涉及V1版本中是否支持frequency_penalties，由于该功能在V1中似乎被移除导致用户发现重复率严重增加。,https://github.com/vllm-project/vllm/issues/10696
vllm,该issue属于性能优化类型，主要对象为多核CPU利用率不足。这个问题由于推断过程中CPU负载过高，提出需求将工作负载分布到更多进程以充分利用多核CPU能力。,https://github.com/vllm-project/vllm/issues/10690
vllm,这是一个用户提出需求的问题，用户希望得到VLLM模型中输出每一个token的logits而不是经过softmax处理后的logprob，由于VLLM目前不提供logits的输出，用户询问如何实现此功能。,https://github.com/vllm-project/vllm/issues/10688
vllm,这个issue类型为功能需求，涉及的主要对象是模型支持的量化方式，用户希望在 minicpm3 模型中添加 bitsandbytes 量化支持。,https://github.com/vllm-project/vllm/issues/10682
vllm,该issue类型是一个功能提议，主要涉及到V1版本的采样器功能的增强。原因是为了支持最小标记数量、重复惩罚、存在惩罚和频率惩罚功能的采样。,https://github.com/vllm-project/vllm/issues/10681
vllm,这个issue类型为功能更新，主要对象是`Idefics3ForConditionalGeneration`类。由于CC([V1] Refactor model executable interface for multimodal models)未更新其接口，导致该类需要更新。,https://github.com/vllm-project/vllm/issues/10680
vllm,这是一个需求类型的issue，涉及到多模型的重新基础。由于需要重新基础多个模型，用户提出了这个问题寻求帮助。,https://github.com/vllm-project/vllm/issues/10679
vllm,这个issue类型是功能增强（Feature Enhancement），主要涉及的对象是LLaVA模型的输入处理器（Input Processor）。原因是为了支持多模态模型，更新了`MultiModalProcessor`及相关组件。,https://github.com/vllm-project/vllm/issues/10676
vllm,这是一个用户提出需求的RFC（Request for Comments）类型的issue，主要对象是vLLM中的文本生成模型，原因是目前需要针对每个现有架构单独开展新的PR来添加池化功能，希望通过自动化来消除这种重复的工作。,https://github.com/vllm-project/vllm/issues/10674
vllm,这是一个用户提出需求的issue，主要涉及对象是模型Llama-2-7b-chat-hf，由于修改模型为Llama27bchathf后运行代码时遇到错误导致无法成功进行推断并提取嵌入向量。,https://github.com/vllm-project/vllm/issues/10673
vllm,这个issue类型是增加文档的功能，主要对象是代码引用，原因是为了给代码引用添加github链接。,https://github.com/vllm-project/vllm/issues/10672
vllm,该issue类型为用户提出需求，主要涉及如何在vllm中获取每个输出token的得分，由于用户希望了解在vllm中如何实现类似transformers中output_scores的功能。,https://github.com/vllm-project/vllm/issues/10670
vllm,这是一个提出需求的Issue，主要涉及的对象是VllmConfig以及VllmState。由于VllmConfig中的一些参数对用户来说应该是不可变的，因此提出创建VllmState来管理这些参数。,https://github.com/vllm-project/vllm/issues/10666
vllm,这个issue是一个功能需求问题，主要涉及的对象是`LLMEngine`。由于缺少对`AsyncLLMEngine`的profile支持，用户提出了需要启用此功能的需求。,https://github.com/vllm-project/vllm/issues/10665
vllm,这个issue类型是用户提出需求，请求解决Llama 3.1 405B-FP8模型与AMD Mi250的兼容性问题。,https://github.com/vllm-project/vllm/issues/10663
vllm,这个issue是一个功能需求提案，主要涉及与XGrammar集成以在LLM推理中实现零额外负担的结构化生成。原因为希望实现更高效灵活的结构化生成。,https://github.com/vllm-project/vllm/issues/10660
vllm,这是一个需求提议，主要涉及对于在macOS上安装vllm的脚本添加的问题。由于之前的更新，需要清理dockerfile并找到安装vllm的方法。,https://github.com/vllm-project/vllm/issues/10658
vllm,该issue类型为代码改进，涉及的主要对象是vllm的optional protocols，由于每个模型现在应该接受vllm_config参数，因此需要移除旧的__init__规范。,https://github.com/vllm-project/vllm/issues/10655
vllm,这是一个用户提出需求的issue，涉及主要对象是Qwen2-VL在AWS Inf2上的运行支持情况。由于无人回复，用户在询问Qwen2VL是否支持在AWS Inf2上运行。,https://github.com/vllm-project/vllm/issues/10654
vllm,这个issue是关于需求和设计提案，主要涉及到指标和统计数据的原型功能。,https://github.com/vllm-project/vllm/issues/10651
vllm,该issue属于用户提出需求类型，主要涉及huggingface/transformers中Mixtral model class的manual `head_dim`功能。这个问题的原因是用户需要此功能，以支持更灵活的模型操作。,https://github.com/vllm-project/vllm/issues/10649
vllm,这是一个功能增强的issue，主要涉及的对象是在加载模型权重时提升速度。由于采用了fastsafetensor库直接将权重从存储加载到GPU内存，相比于逐个参数从文件中读取，能够提升加载速度。,https://github.com/vllm-project/vllm/issues/10647
vllm,这个issue类型是需求提议，主要涉及支持KV缓存压缩，并提出了支持不同压缩方法以及相关功能变更的建议。,https://github.com/vllm-project/vllm/issues/10646
vllm,这是一个用户提出需求的类型的issue，涉及的主要对象是要在文档中添加关于Snowflake幻灯片的内容。,https://github.com/vllm-project/vllm/issues/10641
vllm,这个issue是关于性能优化和功能改进的，主要涉及的对象是VLM（Vision-Language Model）。由于前端处理进程中运行了多模态映射器/预处理器，执行时间得到了1.7倍的改进。,https://github.com/vllm-project/vllm/issues/10640
vllm,这是一个功能需求的issue，涉及的主要对象是vLLM中的embedding模型。由于不同的embedding模型使用了不同的checkpoint格式，导致在加载模型时可能出现架构名称与期望权重不匹配的问题，用户提出希望能够提高灵活性并允许使用不同格式的权重来加载embedding模型。,https://github.com/vllm-project/vllm/issues/10639
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm模型。由于未能显式指定GPU设备导致了模型加载时的资源冲突和冻结问题。,https://github.com/vllm-project/vllm/issues/10638
vllm,这是一个关于API一致性问题的用户需求提问，主要涉及到vllm库中的`LLM.generate`和`chat.completions.create`方法的响应是否一致的问题。,https://github.com/vllm-project/vllm/issues/10629
vllm,这是一个代码改进类型的issue，主要涉及将函数移动到config.py文件中。根据讨论链接，用户希望将特定函数放到相关的配置文件中，以提高代码的清晰度和可维护性。,https://github.com/vllm-project/vllm/issues/10624
vllm,这个issue是关于功能增强的需求，主要对象是LoRA适配器，用户提出了需要支持自适应增加rank的功能。,https://github.com/vllm-project/vllm/issues/10623
vllm,该issue类型是需求提出，主要涉及的对象是torch.compile中的模型。这个需求是想要为不支持的模型添加警告信息。,https://github.com/vllm-project/vllm/issues/10622
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm的Supported Models页面。由于还没有提供encoder-based models的示例以及关于Score API的offline inference功能的说明，用户建议在另一个PR中添加这一功能，并询问是否应将其重命名为Scoring API以与其他在线API的命名形式保持一致。,https://github.com/vllm-project/vllm/issues/10616
vllm,这是一个用户提出需求的issue，主要涉及对象是支持`torch.compile`编译encoder based models。因为之前的pull request（https://github.com/vllmproject/vllm/pull/10558）已经为其提供基础，现在应该比较容易实现支持。,https://github.com/vllm-project/vllm/issues/10613
vllm,这个issue属于需求提出类型，主要涉及kv cache在磁盘上的加载和保存操作，用户希望通过将kv cache缓存到磁盘上来提高命中率。,https://github.com/vllm-project/vllm/issues/10611
vllm,这个issue属于文档更新类型，主要涉及更新README.md文件中关于Ray Summit talk链接的内容。由于发现了包含所有ray summit vLLM Track视频的youtube链接，要求更新README文件以反映这一新链接。,https://github.com/vllm-project/vllm/issues/10610
vllm,这是一个用户提出需求的issue，主要涉及的对象是CPU offloading implementation。原因可能是用户希望在使用`torch.compile`时更加友好。,https://github.com/vllm-project/vllm/issues/10609
vllm,这是一个用户提出需求的issue，主要涉及vllm.SamplingParams中`use_beam_search`被删除的问题，用户想了解如何从v0.6.3开始控制beam search的使用。,https://github.com/vllm-project/vllm/issues/10605
vllm,这是一个文档更新的类型，涉及到项目的代码模型，由于文档过时引发了问题。,https://github.com/vllm-project/vllm/issues/10603
vllm,这是一个用户提出需求的issue，主要涉及VLLM后端日志记录模型响应信息的功能。问题出现的原因是VLLM目前只记录请求日志，未记录LLM的响应日志，给程序调试带来不便。,https://github.com/vllm-project/vllm/issues/10602
vllm,该issue类型为功能需求提议，主要涉及的对象为模型和工具函数，并提出了添加提取层索引功能的需求。,https://github.com/vllm-project/vllm/issues/10599
vllm,这是一个代码优化提议，旨在进一步减少BNB静态变量，称该问题单主要涉及代码性能优化。,https://github.com/vllm-project/vllm/issues/10597
vllm,这是一个建议性issue，主要涉及CI（持续集成）日志可读性问题。,https://github.com/vllm-project/vllm/issues/10594
vllm,这是一个针对Ministral-8B-Instruct-2410的功能需求。该需求是为了在Interleaving sliding window测试模型。,https://github.com/vllm-project/vllm/issues/10591
vllm,这是一个改进请求（PR），主要涉及移除CUDA在推测解码中的硬依赖。,https://github.com/vllm-project/vllm/issues/10587
vllm,这是一个需求提出的issue，主要涉及的对象是vllm项目的metrics支持。由于现有统计信息不齐全，用户提出添加对系统和请求统计日志的支持、将指标导出到Prometheus等需求。,https://github.com/vllm-project/vllm/issues/10582
vllm,这是一个用户提出需求的issue，主要涉及V1 LoRA的支持，由于V1 LoRA运行较慢，需要进行清理、单元测试、性能优化等操作。,https://github.com/vllm-project/vllm/issues/10579
vllm,这是一个关于用户需求的问题，主要涉及使用vllm进行批量推理时如何为每个样本设置不同的SamplingParams，由于OpenAI API中无法找到设置不同top_p和top_k的方式，用户寻求帮助解决这个问题。,https://github.com/vllm-project/vllm/issues/10578
vllm,这是一个优化代码质量的Issue，主要涉及 vllm 下的 LLMEngine 类。删除了一些不必要的临时变量，并替换为实例变量来提高代码质量。,https://github.com/vllm-project/vllm/issues/10577
vllm,这个issue是一个更新请求，涉及vllm项目和outlines库，由于outlines 0.1.x版本存在序列化问题，导致了vllm集成受到影响。,https://github.com/vllm-project/vllm/issues/10576
vllm,这是一个需求提出的issue，涉及主要对象是Kernels和AMD，用户提出需要添加fused moe triton configs for mixtral。,https://github.com/vllm-project/vllm/issues/10574
vllm,这是一个技术改进的issue，主要涉及到V1 VLM的多模态语言模型接口重构，要求对指定的多模态语言模型实现进行改进以满足新的接口要求。,https://github.com/vllm-project/vllm/issues/10570
vllm,该issue类型是功能需求提议，涉及主要对象为Intel Gaudi硬件上的LoRA支持。,https://github.com/vllm-project/vllm/issues/10565
vllm,这是一个功能需求提出的issue，主要涉及对象是EngineCore。由于GPU空闲时间显著减少，可能导致性能改进或者功能优化。,https://github.com/vllm-project/vllm/issues/10564
vllm,该issue为用户提出需求类型，主要涉及如何在运行假设模型时使用张量并行ism所引发的疑问。,https://github.com/vllm-project/vllm/issues/10562
vllm,这是一个功能添加的issue，主要涉及Model添加GLM-4系列hf格式模型支持。,https://github.com/vllm-project/vllm/issues/10561
vllm,这个issue是一个用户提出需求类型的问题，主要涉及VLLM项目下的多内存支持。由于现有代码中缺少对多内存的支持，用户提出添加多内存支持的需求。,https://github.com/vllm-project/vllm/issues/10560
vllm,这是一个用户提出需求的issue，主要涉及到torch.compile中attention backends的支持。原因是之前attention ops分别注册，现在希望统一注册，使得不需要一个一个注册不同的attention backends。,https://github.com/vllm-project/vllm/issues/10558
vllm,这是一个需求提出的issue，主要涉及到VLLM项目中的结构化输出基准测试添加及相应功能改进。,https://github.com/vllm-project/vllm/issues/10557
vllm,这是一个功能需求类型的issue，主要涉及的对象是不同平台的工作人员类。原因是每个平台应该在自己的代码中指定工作人员类，并且默认情况下为`auto`，允许用户为扩展性指定自定义类。,https://github.com/vllm-project/vllm/issues/10555
vllm,这个issue类型是用户提出需求，涉及的主要对象是文档页面的更新。,https://github.com/vllm-project/vllm/issues/10554
vllm,这是一个用户提出需求的issue，主要对象是在metrics.rst文档中添加一个小的示例。,https://github.com/vllm-project/vllm/issues/10550
vllm,这个issue属于用户提出需求的类型，主要对象是支持以qwen模型实现bitsandbytes量化。,https://github.com/vllm-project/vllm/issues/10549
vllm,这是一个用户需求问题，涉及到需要扩展gemma29b和llama3.18b等模型的上下文长度，可能是因为需要处理更多token导致的。,https://github.com/vllm-project/vllm/issues/10548
vllm,该issue类型为功能需求提议，主要涉及vLLM动态加载LoRA文件的功能扩展。因为当前实现仅支持本地存储，用户需要能够从远程服务器动态加载LoRA文件，故需实现LoRAResolver以支持此功能。,https://github.com/vllm-project/vllm/issues/10546
vllm,这是一个需求变更类型的issue，涉及主要对象是离线推理示例（offline_inference.py）。由于希望保持offline_inference.py示例尽可能简单，因此将之前的更改移至新文件offline_inference_cli.py。,https://github.com/vllm-project/vllm/issues/10545
vllm,这是一个功能改进类型的issue，涉及主要对象为vllm中默认最大批次标记数的设定。由于之前设定的512值过于保守，因此提出将其增加到2048以优化大型prefills处理或吞吐量的建议。,https://github.com/vllm-project/vllm/issues/10544
vllm,这是一个需求提出类型的issue，主要涉及实现分布式的张量平行RMS Norm功能，可能是为了新的OLMo模型中的Q和K的RMS Norms。,https://github.com/vllm-project/vllm/issues/10542
vllm,这是一个功能需求的issue，主要对象是支持注册模型特定默认采样参数。原因是为了避免手动设置采样参数带来的不便和可能导致的输出不一致。,https://github.com/vllm-project/vllm/issues/10539
vllm,这个issue类型为用户提出需求，请求添加Sageattention backend。,https://github.com/vllm-project/vllm/issues/10532
vllm,这个issue类型是代码改进，主要涉及日志问题，由于重复记录了关于遗留多模式输入处理管道弃用的消息，可能导致日志重复。,https://github.com/vllm-project/vllm/issues/10530
vllm,这个issue类型属于优化需求，主要涉及的对象是Qwen2.572B模型的部署和性能优化。由于处理长度大于6000个token的序列导致高TTFT，用户希望优化配置以降低TTFT并提高服务的响应性。,https://github.com/vllm-project/vllm/issues/10527
vllm,这个issue类型是用户提出需求，主要涉及OpenAI的API中关于`tool_choice`参数设置的功能需求。原因是用户需要LLM在调用外部工具时具有更多选择性。,https://github.com/vllm-project/vllm/issues/10526
vllm,这个issue类型是用户提出需求，主要涉及的对象是punica ops注册方式。由于当前punica自定义op在eager模式下存在性能开销，用户提出直接注册punica ops以减少开销。,https://github.com/vllm-project/vllm/issues/10522
vllm,这是一个功能需求的issue，涉及主要对象是vLLM的LoRA加载机制。由于当前实现仅支持从本地存储加载LoRA，用户需要能够从远程服务器动态加载LoRA，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/10519
vllm,这是一个功能需求的issue，涉及到在VLLM中手动注入前缀KV Cache，由于需要手动将KVCache压缩为特殊令牌，因此需要一个方法将其馈送到生成函数中。,https://github.com/vllm-project/vllm/issues/10515
vllm,该issue类型为用户提出需求，该问题单涉及的主要对象是在VLLM中添加对Aria模型的支持。主要原因是用户想要使用rhymesai/Aria，一个多模态MoE模型，并请求添加对其模型的支持。,https://github.com/vllm-project/vllm/issues/10514
vllm,这是一个关于功能需求的issue，主要涉及到Multimodel prefix-caching features的支持问题，用户询问何时会支持这个特性。,https://github.com/vllm-project/vllm/issues/10510
vllm,这个issue类型是用户提出需求， 主要对象是使用vllm进行多个任务的推理及获取kvcache分配信息。可能由于当前没有提供该功能或者用户不清楚如何实现，导致用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/10509
vllm,这是一个功能需求类型的issue，主要涉及到`Platform`，由于需要在`Platform`中添加`device_type`属性，以满足系统的需求或者提高用户体验。,https://github.com/vllm-project/vllm/issues/10508
vllm,这个issue属于改进类型，主要涉及KVCacheManager组件的重构，目的是更容易传递更多元数据进行哈希计算，消除在块内跟踪令牌ID的需要，并因此不再需要在块中跟踪令牌ID来计算哈希。,https://github.com/vllm-project/vllm/issues/10507
vllm,这是一个请求打开V1功能的issue，主要对象是H200构建。,https://github.com/vllm-project/vllm/issues/10505
vllm,这是一个功能添加的issue，主要涉及的对象是vLLM模型。由于需要添加新的OLMo November 2024模型并进行一些架构改变，因此需要在vLLM中实现这一模型。,https://github.com/vllm-project/vllm/issues/10503
vllm,这是一个功能需求类型的issue，主要涉及EmbeddingChatRequest.add_generation_prompt这个参数，默认值应设置为False，原因是在embedding/reward modeling中，模型不需要生成文本响应，而是输出嵌入向量。,https://github.com/vllm-project/vllm/issues/10501
vllm,这是一个功能需求提议，涉及ROCm Flash Attention模块，添加了对softcap的支持。,https://github.com/vllm-project/vllm/issues/10500
vllm,这是一个功能增强类型的issue，主要涉及VLM前缀缓存中对图像进行哈希处理，由于需要支持图像哈希计算，因此提交了相关逻辑来实现该功能。,https://github.com/vllm-project/vllm/issues/10497
vllm,这是一个用户提出需求的issue，主要涉及的对象是在vLLM项目中添加Chat模板。由于用户希望贡献Chat模板并且已经准备好相关内容，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/10496
vllm,这是一个改进请求，涉及的主要对象是视频获取超时时间，由于测试入口点可能不稳定，因此需要增加默认视频获取超时时间。,https://github.com/vllm-project/vllm/issues/10495
vllm,这个issue类型是功能需求，主要对象是KVPress，由于长序列LLM部署成本高，需要大量内存。,https://github.com/vllm-project/vllm/issues/10491
vllm,这是一个功能优化（Feature）的issue，主要涉及到更新或放宽对outlines依赖的支持，目的是提高速度。,https://github.com/vllm-project/vllm/issues/10490
vllm,该issue类型为功能需求，主要涉及vllm对于outlines版本的支持。原因是outlines版本升级改善了内部功能并提升了性能，用户寻求支持vllm使用最新版本。,https://github.com/vllm-project/vllm/issues/10489
vllm,这是一个功能需求的issue，主要涉及对于在完成响应头部添加指标报告支持。由于需要增强与高级负载均衡/网关的集成，提出了对负载/成本报告和LoRA管理的加强。,https://github.com/vllm-project/vllm/issues/10484
vllm,这是一个用户提出需求的issue，主要涉及到与高级LB/gateways的集成，具体需求为改进加载/成本报告和LoRA管理。,https://github.com/vllm-project/vllm/issues/10476
vllm,这个issue类型为功能需求提出，主要涉及的对象是为了提升与先进负载均衡/网关集成的性能和成本报告以及LoRA管理。这个需求由于缺乏对高级负载均衡/网关的集成以及成本报告和LoRA管理的功能，希望通过实现orca响应头以proto格式来解决。,https://github.com/vllm-project/vllm/issues/10475
vllm,这个issue是关于性能优化讨论的，主要涉及到Python/PyTorch中的内存管理问题，由于执行模型的前向传播过程中使用了多余的内存，导致在某些情况下支持的并发性能有显著差异。,https://github.com/vllm-project/vllm/issues/10473
vllm,这是一个功能需求，涉及的主要对象是LazyDict，用户提出需要为LazyDict添加__setitem__方法以支持item赋值。,https://github.com/vllm-project/vllm/issues/10469
vllm,这个issue是关于功能改进的，主要涉及的对象是xpu代码，用户提出了将特定代码重构为更通用方法的建议。,https://github.com/vllm-project/vllm/issues/10468
vllm,这是一个用户提出需求的issue，主要是关于为FlashInfer添加多步骤分段预加载支持。,https://github.com/vllm-project/vllm/issues/10467
vllm,这个issue是关于修改CI构建配置的需求，主要涉及到将nightly测试改为optional，删除nightly关键字。,https://github.com/vllm-project/vllm/issues/10465
vllm,这个issue类型是需求反馈，主要涉及添加滑动窗口支持至 flashinfer的问题，并需要测试来验证。,https://github.com/vllm-project/vllm/issues/10462
vllm,这是一则用户提出需求的issue，主要涉及Llama32在ROCM FA支持上游的问题。,https://github.com/vllm-project/vllm/issues/10461
vllm,这是一个关于优化编译时间的问题，主要涉及到torch.compile功能。原因可能是为了减少编译时间而进行优化。,https://github.com/vllm-project/vllm/issues/10460
vllm,这是一个优化需求，主要涉及Medusa模型中权重大小的优化。该需求是由于在实际部署中，只有ResidualBlock通常被训练，而不是lm_head所致。,https://github.com/vllm-project/vllm/issues/10454
vllm,这是一个用户提出需求的issue，主要涉及Marlin kernel在vllm中为什么不需要evict_first hint的问题，原因可能是优化处理或者特定的实现方式。,https://github.com/vllm-project/vllm/issues/10448
vllm,这个issue是用户提出需求，请求支持LoRA fine-tuning model for DeepSeek V2，由于需要加速LLM inference。,https://github.com/vllm-project/vllm/issues/10446
vllm,这个issue是一个功能需求提议，主要涉及的对象是CI（持续集成）以及CPU。由于需要在容器名后添加NUMA节点编号，可能是为了更好地管理和区分容器资源，提高系统性能。,https://github.com/vllm-project/vllm/issues/10441
vllm,这个issue是关于用户提出需求的，主要涉及的对象是vllm.LLM模块，由于需要用户界面改进，用户现在需要通过命令行或直接构建CompilationConfig对象来进行编译配置，但目前只能控制3个级别，稍后会提供更精细的控制，并计划后续提供如何使用以及设计文档。,https://github.com/vllm-project/vllm/issues/10437
vllm,这个issue类型属于需求提出，主要对象是dependabot。由于过多的依赖项导致所有补丁更新造成干扰，用户希望让dependabot忽略所有的补丁版本更新。,https://github.com/vllm-project/vllm/issues/10436
vllm,这个issue类型是用户提出需求，主要对象是在TPU上支持`w8a8`，由于之前的pull request被取代所导致。,https://github.com/vllm-project/vllm/issues/10435
vllm,"这是一个用户提出需求的issue，涉及对象是添加""openai.beta.chat.completions.parse""的示例到文档中。由于缺少该示例，用户可能希望文档能提供更全面的结构化输出信息。",https://github.com/vllm-project/vllm/issues/10433
vllm,这个issue类型是需求提出，主要涉及文档格式和易用性，由于ReStructuredText相对不易上手，用户提出将文档迁移至Markdown，以降低门槛并提高贡献参与度。,https://github.com/vllm-project/vllm/issues/10427
vllm,该issue类型是文档更新请求，主要涉及的对象是GLM-4V模块的LoRA支持文档。这个问题由于新增了LoRA支持，需要更新相关文档内容。,https://github.com/vllm-project/vllm/issues/10425
vllm,这个issue类型是用户提出需求类型，涉及的主要对象是支持用本地 cutlass 路径编译，原因可能是需要改进项目的构建流程。,https://github.com/vllm-project/vllm/issues/10424
vllm,这个issue类型是提出需求，涉及的主要对象是为vllm添加对通过环境变量指定本地 CUTLASS 源目录的支持。用户提出了在一些网络条件差的环境下，或者用户希望使用自定义版本的 CUTLASS 代码编译 vllm 时存在不便的问题。,https://github.com/vllm-project/vllm/issues/10423
vllm,这是一个关于优化代码质量的Issue，主要涉及到Medusa模型的权重优化问题。原因可能是为了减少显存占用并提高模型性能。,https://github.com/vllm-project/vllm/issues/10422
vllm,该issue属于需求类型，主要涉及 vLLM 和 LMDeploy 的 Triton kernels 比较，用户想了解哪种 Triton kernels 更有效，并尝试使用 vLLM 来提高性能。,https://github.com/vllm-project/vllm/issues/10420
vllm,这是一个关于软件更新与自动化操作的issue，主要对象是软件的依赖库更新。由于依赖库更新可能导致冲突或自动化操作问题，用户在此提出了相关操作指南并寻求针对此类问题的帮助。,https://github.com/vllm-project/vllm/issues/10413
vllm,这个issue属于用户提出需求类型，主要涉及为希望使用ROCm GPU的AMD用户添加ROCm设备插件链接到“使用Kubernetes部署”页面。,https://github.com/vllm-project/vllm/issues/10412
vllm,这是一个功能需求类型的issue，主要涉及VLM模型输出中的多模态占位符报告，目的是帮助检查VLM模型的性能，以区分文本标记和多模态嵌入。,https://github.com/vllm-project/vllm/issues/10407
vllm,这是一个需求报告issue，主要涉及fishaudio/fish-speech-1.4模型。由于该模型难以支持现有的VLLM层，用户在寻求支持该模型所遇到的困难。,https://github.com/vllm-project/vllm/issues/10404
vllm,这是一个关于代码重构的类型。主要涉及的对象是平台（platforms）。这个问题的原因可能是为了优化代码结构或提升性能。,https://github.com/vllm-project/vllm/issues/10402
vllm,这个issue类型是功能改进，主要涉及的对象是torch.compile中的编译配置。由于之前全局变量过多，现在通过调整编译配置，可以更好地存储信息，提升代码整洁性和可维护性。,https://github.com/vllm-project/vllm/issues/10401
vllm,该issue是关于功能增强的请求，涉及支持Cross Encoder模型的问题。用户提出了增加对RoBERTA和BERT SequenceClassification模型的支持，以及相应的API端点和方法。,https://github.com/vllm-project/vllm/issues/10400
vllm,这个issue类型是改进需求，主要涉及到对自定义操作日志的控制和统计。,https://github.com/vllm-project/vllm/issues/10399
vllm,这是一个请求类型的 issue， 主要对象是`vllm/v1`，由于需要明确代码负责人，所以添加了代码负责人。,https://github.com/vllm-project/vllm/issues/10397
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是增强offline_inference.py的可配置参数以增加灵活性。由于当前功能较为受限，用户希望通过这个提议来增加程序的配置参数，提高灵活性。,https://github.com/vllm-project/vllm/issues/10392
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是要增强 offline_inference.py 的功能性。原因是当前的功能相对基础，开发者经常需要手动修改脚本以满足特定需求，提出了增加可配置参数以提升脚本的灵活性和适用性。,https://github.com/vllm-project/vllm/issues/10391
vllm,这个issue属于功能需求类型，主要涉及的对象是vllm下的API接口。原因是用户提出了关于添加ngram猜测功能到API的需求。,https://github.com/vllm-project/vllm/issues/10390
vllm,"这是一个用户提出需求的问题，该问题涉及到对于异步输出处理支持的道路，并表达了对于支持异步输出处理的急需。
",https://github.com/vllm-project/vllm/issues/10387
vllm,这是一个用户提出需求的类型为改进日志消息的issue，主要涉及的对象是插件(plugin)。这个问题的原因是日志信息不清晰，容易让用户感到困惑。,https://github.com/vllm-project/vllm/issues/10386
vllm,这是一个需求提出类的issue，主要涉及将编译配置整合到vllm的配置文件中，由于目前core代码中仍有读取环境变量的情况，需要将环境变量转移到命令行参数中，并在初始化阶段初始化所有配置字段，从而解决这个问题。,https://github.com/vllm-project/vllm/issues/10383
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vllm的引擎配置（EngineArgs），由于需要实现方便地在v0和v1版本之间切换的功能，以及实现更容易的迁移，所以提出了相关改动。,https://github.com/vllm-project/vllm/issues/10382
vllm,该issue类型为提出需求，并主要涉及NVIDIA Triton GenAI Perf Benchmark的集成与性能测试。该需求是为了比较vLLM的性能和准确性，需要实现新的基准测试工具进行负载测试和比较结果指标。,https://github.com/vllm-project/vllm/issues/10377
vllm,这是一个特征改进的 issue，主要涉及到 vLLM 中的模型执行接口的重构。由于要支持 V1 VLM 重构和 `torch.compile`，需要满足一定的模型实现要求。,https://github.com/vllm-project/vllm/issues/10374
vllm,这是一个用户提出需求的 issue，主要涉及前端添加 `--version` flag 到 CLI 工具中。,https://github.com/vllm-project/vllm/issues/10369
vllm,这个issue类型是文档增加需求，主要涉及vLLM的架构概述页面，作者打算逐层介绍主要组件，并因此制作了带有图示的文档。,https://github.com/vllm-project/vllm/issues/10368
vllm,这是一个需求提出类型的issue，主要涉及到mistral_common版本的更新。原因是为了更好地工具使用。,https://github.com/vllm-project/vllm/issues/10367
vllm,该issue类型是一个功能需求提议，涉及主要对象是CI/Build。由于缺乏文档检查工具，用户提议添加sphinxlint来清理文档中的风格和格式错误，以提高文档质量。,https://github.com/vllm-project/vllm/issues/10366
vllm,这个issue类型属于功能需求提出，主要对象是Medusa服务。这个问题是由于在生产环境中，一些服务提供的权重具有偏差，而一些权重则没有偏差，因此需要支持自定义偏差的灵活配置。,https://github.com/vllm-project/vllm/issues/10361
vllm,这是一个用户提出需求的 issue， 主要涉及的对象为旧版本的类。造成这个问题的原因可能是新版本的更改会对用户造成困扰，建议保留兼容性并先发出警告。,https://github.com/vllm-project/vllm/issues/10356
vllm,这是一个用户提出需求的issue，主要涉及的对象是为VLLM添加KV-Cache int8量化支持。原因可能是为了支持int8量化，并且提供了关于如何实现和使用KVCache int8的详细说明。,https://github.com/vllm-project/vllm/issues/10354
vllm,这个issue属于用户提出的需求。主要对象是VllmRunner中的模型访问接口。由于用户需要在每个工作节点中应用函数到模型，但又不想直接访问底层的模型实例，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/10353
vllm,这是一个用户提出需求的issue，主要涉及VLLM在TPU上使用Pallas后端时的head_size限制问题。,https://github.com/vllm-project/vllm/issues/10343
vllm,这个issue类型是功能增强，主要涉及添加了Cutlass稀疏支持和相应的操作，包括实现2:4结构稀疏支持、权重矩阵转换以及模型加载管道修改等。,https://github.com/vllm-project/vllm/issues/10340
vllm,这是一个性能优化类的 issue，主要涉及 llama 在峰值内存使用上的问题，由于维护多个名称会导致引用计数的增加，从而增加了峰值内存使用，表现为内存分析中更多叠加在一起的块。,https://github.com/vllm-project/vllm/issues/10339
vllm,这个issue类型是功能增强请求，主要涉及的对象是为VLLM添加了Cutlass稀疏支持。,https://github.com/vllm-project/vllm/issues/10335
vllm,这是一个功能改进的issue，涉及主要对象是Mistral工具解析，由于需要更加稳健的函数调用解析，作者提出了改善和修正解析逻辑。,https://github.com/vllm-project/vllm/issues/10333
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm中的代码。,https://github.com/vllm-project/vllm/issues/10332
vllm,这是一个用户提出需求的类型，该问题单涉及向发布 meetup 演示文稿。,https://github.com/vllm-project/vllm/issues/10331
vllm,这是一个功能需求的issue，主要涉及VLLM引擎中的keyvalue缓存，并提出了在特定条件下将缓存迁移到主机内存的优化方案。,https://github.com/vllm-project/vllm/issues/10330
vllm,这个issue类型是用户提出需求，主要涉及对模型功能的修改。用户想直接传入embedding以适应TTS任务，而不是通过token_id。,https://github.com/vllm-project/vllm/issues/10323
vllm,该issue是关于用户需求的，主要涉及使用vLLM作为后端来运行推理时出现问题。可能是由于配置问题或API集成错误导致该问题。,https://github.com/vllm-project/vllm/issues/10322
vllm,这是一个用户提出需求的issue，主要涉及vllm==0.6.2版本在cuda 11.8环境下的支持问题，用户希望能够得到关于添加`vllm==0.6.2+cu118`支持的帮助。,https://github.com/vllm-project/vllm/issues/10319
vllm,这个issue属于用户提出需求类型，主要涉及OpenAI Chat Completion Client For Multimodal中使用视频作为输入的问题。由于文档只展示了使用多个图像，用户提出了如何使用视频作为输入的问题。,https://github.com/vllm-project/vllm/issues/10316
vllm,这是一个需求提出的issue，主要涉及添加Cambricon MLU推理后端到vLLM项目中。,https://github.com/vllm-project/vllm/issues/10315
vllm,这个issue类型属于功能需求，主要涉及的对象是模型 BNB quantization support。由于现有的Idefics3模型尚未支持 BNB quantization，用户提出需要添加支持的功能需求。,https://github.com/vllm-project/vllm/issues/10310
vllm,这是一个用户提出需求的issue，主要涉及如何使用`vllm`输出代码而不包括额外解释，用户询问是否`vllm`支持此功能并寻求帮助。,https://github.com/vllm-project/vllm/issues/10309
vllm,这个issue属于功能需求提出，主要对象是TPU后端，用户提出需要实现prefix caching支持。,https://github.com/vllm-project/vllm/issues/10307
vllm,这是一个优化建议，建议简化脚本代码，主要对象是format.sh。由于第3个参数可以由第一个参数推导出来，因此建议删除第3个参数。,https://github.com/vllm-project/vllm/issues/10305
vllm,这个issue类型是功能请求，主要涉及的对象是vllm引擎下的kv缓存，用户提出了开启主机内存作为kv缓存的需求，因为GPU内存不足导致缓存命中率下降。,https://github.com/vllm-project/vllm/issues/10302
vllm,这是一个功能需求类型的issue，主要对象是将Quark量化格式上游到VLLM，由于目前Quark导出的量化模型格式与VLLM支持的格式不同，需要贡献代码到VLLM以添加对Quark格式的支持。,https://github.com/vllm-project/vllm/issues/10294
vllm,这是一个PR请求，主要是为了添加对多模态Granite模型的支持。,https://github.com/vllm-project/vllm/issues/10291
vllm,该issue类型为用户提出需求，主要涉及的对象是在vLLM中添加2D tensor parallelism和expert parallelism功能。由于缺乏这些功能，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/10289
vllm,这是一个需求更新类型的issue，主要涉及benchmark_serving.py文件中设置image_url部分的更新，用于测试ShareGPT4V数据集。由于需要在client端使用image_url，所以更新了部分设置。,https://github.com/vllm-project/vllm/issues/10287
vllm,这个issue是一个改进请求，主要涉及的对象是CI/Build系统和相关的脚本。这个问题是由于在本地运行`format.sh`时产生了INFO级别的问题，需要更新`shellcheck.sh`以在CI过程中标记此类问题。,https://github.com/vllm-project/vllm/issues/10285
vllm,这是一个需求更新类型的 issue，涉及主要对象为 compressedtensors 版本的更新。,https://github.com/vllm-project/vllm/issues/10279
vllm,这是一个类型为需求更新的issue，主要涉及到更新`compressed-tensors`库版本至v0.8.0。原因可能是为了使用最新的压缩张量功能以提升性能或解决已知问题。,https://github.com/vllm-project/vllm/issues/10278
vllm,这个issue是用户提出需求，主要涉及的对象是在VLLM实现中添加fasteroutlines作为一种导向解码后端。,https://github.com/vllm-project/vllm/issues/10277
vllm,这是一个用户提出的需求类型的issue，主要涉及到使用TCPStore替代内部API的建议。原因可能是为了简化代码和降低依赖性。,https://github.com/vllm-project/vllm/issues/10275
vllm,这是一则用户提出需求的issue，主要涉及添加或替换QwenModel的模型测试。用户提出此问题可能是为了更全面地测试QwenModel模型。,https://github.com/vllm-project/vllm/issues/10274
vllm,这是一个功能需求的issue，主要涉及了torch.compile中的PassManager和PassConfig的重构，其中包括对后处理passes的提取、pass配置的提取、自定义inductor passes的注册等内容。,https://github.com/vllm-project/vllm/issues/10273
vllm,这是一个关于文档改进的类型的问题，涉及主要对象为调试文档。这个问题可能源于之前的文档不清晰或有错误，导致使用者难以理解如何调试或解决问题。,https://github.com/vllm-project/vllm/issues/10270
vllm,该issue为用户提出需求，主要涉及VLLM的支持NVIDIA Unified memory，用户想了解是否有计划支持统一内存。,https://github.com/vllm-project/vllm/issues/10267
vllm,这个issue是关于用户提出需求的类型，主要涉及VLLM中特定功能的支持，具体是关于2:4 sparsity + w4a16 support的。,https://github.com/vllm-project/vllm/issues/10260
vllm,这是一个用户提出需求的issue，该问题涉及VLLM是否支持具有动态激活稀疏性的推理。原因是由于语言模型在推理过程中表现出激活稀疏性，用户希望VLLM能够支持这种特性。,https://github.com/vllm-project/vllm/issues/10259
vllm,这个issue类型为用户提出需求，并涉及vLLM模型在超级计算集群上远程访问问题，由于`base_url`连接问题导致无法正确访问部署在集群上的vLLM服务。,https://github.com/vllm-project/vllm/issues/10257
vllm,该issue类型为功能增强，主要涉及AMD硬件以及ROCm的支持，并解决了在ROCm上运行GGUF模型的问题。,https://github.com/vllm-project/vllm/issues/10254
vllm,该issue类型为用户提出需求，主要涉及安装vllm时的离线聊天功能，用户提出了如何在没有网络的情况下使用聊天功能的问题。,https://github.com/vllm-project/vllm/issues/10251
vllm,这是一个需求讨论类型的issue，主要涉及EngineCoreRequest对象的序列化和添加多模态输入功能。这个问题产生的原因是为了支持VLMs的细粒度调度并解决多模态输入的序列化问题。,https://github.com/vllm-project/vllm/issues/10245
vllm,这个issue类型为需求提出，涉及主要对象为Layerwise profile更新。由于工具需要添加新功能来分析处理不同输出长度请求的场景，需要更改现有的参数和添加新的子命令来更清晰地表达用户意图。,https://github.com/vllm-project/vllm/issues/10242
vllm,这是一个用户提出需求的issue，主要涉及的对象是“HPU”，用户提出在每个模型解码层后添加`mark_step`以提高性能。,https://github.com/vllm-project/vllm/issues/10239
vllm,这是一个用户提出需求的issue，主要涉及torch.compile用户界面设计。原因是在导入模型之前需要设置环境变量，这会给用户带来困惑，现在的解决方案是在初始化模型时读取环境变量，并计划将它们移至与模型的'vllm_config'捆绑的编译配置中。,https://github.com/vllm-project/vllm/issues/10237
vllm,这是一个用户提出需求的issue，主要涉及vLLM中核心功能的优化，是关于减少TTFT的问题。,https://github.com/vllm-project/vllm/issues/10235
vllm,这是一个功能需求，要求在python_only_dev中使用shutil重命名以支持在Linux系统中不同设备之间的重命名。,https://github.com/vllm-project/vllm/issues/10233
vllm,这个issue类型是需求提出，涉及主要对象是vLLM模型，用户提出需要将vLLM与Mistral fp8权重兼容。,https://github.com/vllm-project/vllm/issues/10229
vllm,这是一个用户提交需求的类型，主要涉及的对象是在CUDA图中启用自定义操作。由于之前的版本中处理CUDA图的自定义操作出现问题，导致用户提出了需要修正这一问题的需求。,https://github.com/vllm-project/vllm/issues/10228
vllm,这个issue是关于需求变更的，涉及主要对象为piecewise CUDA graphs，由于修改遗漏导致需要再次进行修改。,https://github.com/vllm-project/vllm/issues/10227
vllm,这是一个关于优化编译时间的修改类型的issue，主要涉及到使用TorchInductor的问题，因为编译时间过长且存在问题导致的。,https://github.com/vllm-project/vllm/issues/10225
vllm,这个issue类型为功能增强需求，主要涉及的对象是CPU测试，用户想要通过配置来绑定不同的核心。,https://github.com/vllm-project/vllm/issues/10222
vllm,这是一个需求提出的 issue，主要涉及 vLLM 中 Qwen2VL 模型对多个图像嵌入输入支持不同分辨率的问题。原因是当前 vLLM 实现要求所有输入图像具有相同分辨率，而 Qwen2VL 模型支持变化的图像分辨率。,https://github.com/vllm-project/vllm/issues/10221
vllm,这是一个用户提出需求的issue，主要涉及修改python -m vllm.entrypoints.openai.api_server的提示词。用户希望在代码、模型文件或命令行中改变系统提示词。,https://github.com/vllm-project/vllm/issues/10220
vllm,这是一个功能需求的issue，涉及的主要对象是添加Guidance后端以支持引导解码，由于需要扩展引导解码功能，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/10217
vllm,这个issue属于用户提出需求类型，主要对象是core中的分布式部分，用户希望新增无状态进程组功能。,https://github.com/vllm-project/vllm/issues/10216
vllm,这是一个请求移除未使用的模块的issue，涉及对象为vLLM的Kernel模块。,https://github.com/vllm-project/vllm/issues/10214
vllm,"该issue类型为更新请求，涉及的主要对象是3个软件包（`awscli`, `boto3`, `botocore`），用户请求更新了这3个软件包的版本号。",https://github.com/vllm-project/vllm/issues/10210
vllm,这是一个功能需求的issue，主要涉及的对象是添加指导解码的指导标签处理器。由于需要扩展指导解码功能，因此添加了指导后端并支持不同类型的指导处理，包括JSON、选择和正则表达式生成。,https://github.com/vllm-project/vllm/issues/10208
vllm,这个issue类型是用户提出需求，主要涉及到vllm下的torch.compile模块的 cudagraph buffer 管理。用户提出通过支持管理 cudagraph buffer 在简化用户代码的同时增加了额外输入张量和拷贝核的成本。,https://github.com/vllm-project/vllm/issues/10203
vllm,这是一个需求改进类型的issue，涉及主要对象是cloudpickle registration and tests。,https://github.com/vllm-project/vllm/issues/10202
vllm,这是一个功能需求的issue，主要涉及的对象是新模型的支持。由于未得到响应，用户提出了有关新模型支持的困难。,https://github.com/vllm-project/vllm/issues/10197
vllm,这是一个用户提出的需求类型的issue，主要涉及到AriaForConditionalGeneration模型不支持LoRA的问题，用户希望在未来增加支持。,https://github.com/vllm-project/vllm/issues/10194
vllm,这个issue类型为功能需求，涉及主要对象为加载模型时使用RunAI Model Streamer作为可选加载程序。由于需求增加了新的加载选项，用户想要能够从S3中加载模型并使用RunAI Model Streamer作为加载器。,https://github.com/vllm-project/vllm/issues/10192
vllm,这是一个用户提出需求的issue，主要涉及的对象是在vLLM中添加了Runai Model Streamer作为加载选项。,https://github.com/vllm-project/vllm/issues/10191
vllm,这个issue类型是代码修改建议，主要涉及日志记录的添加。原因可能是为了更好的调试和信息记录。,https://github.com/vllm-project/vllm/issues/10186
vllm,这是一个用户提出需求的类型，主要关于添加关于使用Llama Stack 进行服务的文档。,https://github.com/vllm-project/vllm/issues/10183
vllm,该issue是一个需求提出，主要涉及的对象是支持Duo-Attention功能。这个需求是因为DuoAttention方法可以显著减少长序列推理内存，提高推理速度，加速预填充，并可与量化结合在单个GPU上进行大规模推理。,https://github.com/vllm-project/vllm/issues/10181
vllm,这是一个关于功能需求的issue，主要涉及的对象是Qwen2VL模型。用户询问了关于Qwen2VL是否会支持LoRA推理的问题，因为当前的环境下报错指出该模型尚不支持LoRA，但用户希望进行LoRA推理实验。,https://github.com/vllm-project/vllm/issues/10178
vllm,该issue类型为需求提出，主要涉及的对象是VLLM服务。由于需要在chat completions中添加base path环境变量以减少对反向代理的需求，用户提出了希望能够通过设置BASE_URL环境变量来访问服务的需求。,https://github.com/vllm-project/vllm/issues/10172
vllm,该issue类型为功能需求提出，主要涉及的对象是CI/Build过程，用户提出了添加runhputest.sh脚本来触发基本的CICD流程。,https://github.com/vllm-project/vllm/issues/10167
vllm,这个issue类型是文档更新，主要涉及更新TPU安装指南，涉及的主要对象是安装和配置TPU的用户。由于原先的安装指南中存在一些错误链接、缺少对miniconda安装的说明以及需要替换参数值为小写，因此用户需要更新和修正这些问题。,https://github.com/vllm-project/vllm/issues/10165
vllm,这是一个功能需求类的issue，主要对象是CI/Build。由于不希望在Git忽略的文件上运行shellcheck，因此提出了这一需求。,https://github.com/vllm-project/vllm/issues/10162
vllm,该issue是文档更新类型，涉及主要对象是项目的PR模板内容。由于需要将PR模板内容移至文档部分，导致需要进行相关的调整。,https://github.com/vllm-project/vllm/issues/10159
vllm,这是一个用户提出需求的issue，主要涉及如何获取VLLM模型中令牌的logits分数。用户希望在执行判别任务时直接使用最后一层的logits分数，但由于设计限制，无法直接输出两个令牌的logits分数。,https://github.com/vllm-project/vllm/issues/10149
vllm,这是一个用户提出需求的issue，主要涉及的对象是`Idefics3Processor`/`Idefics3ImageProcessor`。由于缺乏对`size`参数的暴露，用户无法通过控制图片的大小来减少内存使用。,https://github.com/vllm-project/vllm/issues/10146
vllm,这个issue属于项目需求提出类型，主要涉及的对象是对EncoderDecoderModelRunner中LoRA支持状态的更新。由于LoRA支持在该项目中处于早期阶段，导致用户提出了需要补充LoRA相关代码、完善mllama的LoRA支持以及添加单元测试的需求。,https://github.com/vllm-project/vllm/issues/10143
vllm,该issue类型为用户需求，主要涉及改进首帧响应速度，可能由于当前环境下首帧响应速度较慢而导致用户提出该需求。,https://github.com/vllm-project/vllm/issues/10140
vllm,这是一个用户提出需求的issue，主要涉及支持预测输出功能，由于对提高潜在输出性能的需求和动机驱动。,https://github.com/vllm-project/vllm/issues/10137
vllm,这是一个用户提出需求的类型，主要对象是`Request`类。导致这个需求的原因是希望能够更高效地处理token ids而避免每次都需要进行O(n)的计算。,https://github.com/vllm-project/vllm/issues/10135
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM模块中的logging模块。原因是为了避免与内置的logging模块重名而导致的问题。,https://github.com/vllm-project/vllm/issues/10134
vllm,这是一个修改建议类型的issue，涉及到vllm项目中的logging模块名称重名问题，建议将其重命名为logging_utils以避免与Python内置模块冲突。,https://github.com/vllm-project/vllm/issues/10133
vllm,这个issue是一个功能提议，涉及到MLPSpeculator/Medusa和`prompt_logprobs`的启用，并且出现了关于输出格式变化导致的问题。,https://github.com/vllm-project/vllm/issues/10132
vllm,这个issue属于提出需求，主要对象是为Mistral模型添加FP8格式。由于需要运行一些测试，需要确认添加FP8格式是否可行。,https://github.com/vllm-project/vllm/issues/10130
vllm,这是一个功能改进的issue，主要涉及V0版本中的调度问题，并且针对高缓存命中率时的性能提升进行了优化。,https://github.com/vllm-project/vllm/issues/10128
vllm,这个issue类型是功能需求，主要涉及的对象是vLLM的2D prefills功能。由于2D prefills在计算token预填充时未考虑padding，导致了计算资源浪费和内存错误风险，用户提出需要添加padding-aware scheduling功能来解决这一问题。,https://github.com/vllm-project/vllm/issues/10125
vllm,这个issue类型为需求提出，涉及的主要对象是CI/Build。由于GitHub配置不支持在条件性运行的工作流程上阻止自动合并，所以需要合并此PR，以便使mypy工作流程在所有PR上运行，不管更改了哪些文件。,https://github.com/vllm-project/vllm/issues/10122
vllm,这是一个类型为功能更改的issue，涉及的主要对象是pynccl的all reduce功能，由于这次更改将pynccl的all reduce变为out of place操作并移除了对torch distributed all reduce的支持。,https://github.com/vllm-project/vllm/issues/10121
vllm,这个issue类型是用户提出需求，涉及主要对象是用户希望支持一个新的模型。由于模型需要加载一个额外的线性层，可能导致无法直接在现有框架下支持。,https://github.com/vllm-project/vllm/issues/10119
vllm,这是一个功能需求类型的issue，主要涉及了vLLM中多模态模型的输入处理，问题是由于当前设计中的input processor和input mapper导致的一些实现问题和性能问题。,https://github.com/vllm-project/vllm/issues/10114
vllm,这个issue是一个需求提案，主要对象是为了为serving benchmark添加基于gamma分布的请求生成支持，以更好地将基准测试过程与实际场景和性能特征对齐。原因是现有的benchmarking process与真实场景有一定差距，需要增加gamma分布请求生成支持。,https://github.com/vllm-project/vllm/issues/10105
vllm,这是一个在项目代码中进行功能优化的issue，涉及的主要对象是ModelConfig类。由于之前的命名不清晰，导致在模型运行时无法直接引用一些属性，需要进行代码整合优化。,https://github.com/vllm-project/vllm/issues/10104
vllm,这个issue类型是功能需求，主要涉及支持优先级预抢占和分块预填充功能。由于chunkprefill启用时，优先级调度不会触发预抢占，用户提出了这个功能需求。,https://github.com/vllm-project/vllm/issues/10101
vllm,该issue为用户提出需求，主要对象是代码中的Python 3.8 ABI。由于之前被删除而导致的缺失，用户请求重新添加Python 3.8 ABI。,https://github.com/vllm-project/vllm/issues/10100
vllm,该issue类型是用户提出需求，主要涉及vLLM生成多个相同提示的回答，并询问是否会存储缓存以提升速度。,https://github.com/vllm-project/vllm/issues/10099
vllm,这是一个需求提出类型的issue，主要对象是在Dockerfile中将hf_transfer包安装到测试镜像中，以便CUDA构建测试能够更快地下载模型。,https://github.com/vllm-project/vllm/issues/10096
vllm,这是一个关于用户提出需求的issue，主要涉及如何禁用pydantic的请求验证以通过自定义角色，可能是因为需要在vllm中使用自定义角色而提出的这个问题。,https://github.com/vllm-project/vllm/issues/10093
vllm,这是一个需求报告，主要涉及持续集成/build流程，由于Github配置不够灵活，要求在自动合并PR之前必须运行通过特定的workflow，但无法精确控制workflow何时运行，因此提出需要始终运行特定workflow的问题。,https://github.com/vllm-project/vllm/issues/10092
vllm,这是一个用户需求类型的 issue，该问题单涉及的主要对象是 vllm 下的 attention kernel 文件。由于模板实例化过多，编译速度变慢，因此需要将 attention kernel 文件分成两个文件以提高编译速度。,https://github.com/vllm-project/vllm/issues/10091
vllm,"这是一个UI改进类型的Issue，主要涉及网站界面上""ask AI""和文档版本选择重叠的问题，需要将文档版本选择移到左侧以提升用户体验。",https://github.com/vllm-project/vllm/issues/10089
vllm,该issue属于功能增强类型，主要涉及改进与先进负载均衡/网关的集成，提供更好的负载/成本报告和LoRA管理。由于AI推理具有独特特征，需要更高级的负载均衡策略，作者希望提高vLLM与先进LB/gateways的集成，以支持更有效的load balancing。,https://github.com/vllm-project/vllm/issues/10086
vllm,这个issue类型属于功能需求，主要涉及的对象是Github上的CI/Build流程。原因是为了确保PR合并时PR正文内容适合包含在合并后的压缩提交中。,https://github.com/vllm-project/vllm/issues/10082
vllm,这是一个需求类型的issue，主要对象是为Serving Benchmark添加对基于Gamma分布的请求支持。,https://github.com/vllm-project/vllm/issues/10079
vllm,这是一个用户提出需求的issue，主要关注Gamma分布请求在服务基准测试中的支持。,https://github.com/vllm-project/vllm/issues/10077
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是分布式功能的stateless_init_process_group功能。,https://github.com/vllm-project/vllm/issues/10072
vllm,这是一个功能需求类型的issue，主要涉及到LlamaEmbeddingModel。由于用户希望验证文本嵌入模型是否支持LoRA，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/10071
vllm,该issue类型为改进/优化提案，涉及主要对象为VLLM模型测试；通过将模型测试拆分为组来减少由于不稳定性（例如连接失败）导致的影响，并且修复了由两个commit引入的VLM测试失败问题。,https://github.com/vllm-project/vllm/issues/10069
vllm,这个issue是关于用户提出需求的，主要涉及对象是vllm模型架构。由于当前版本不支持LlavaNextForConditionalGeneration模型架构，用户想要使用LLAVA1.6hfmistral7B，导致了无法完成所需功能。,https://github.com/vllm-project/vllm/issues/10065
vllm,这个issue类型为功能需求提议，主要对象是分布式IPC缓冲区的创建。由于PyTorch的IPC句柄格式可能改变，使用PyTorch进行CUDA IPC会受到PyTorch变化的影响。,https://github.com/vllm-project/vllm/issues/10064
vllm,这个issue是关于代码质量的改进，主要涉及到了解决Wswitch-bool编译器警告问题。原因是代码中存在switch on boolean的写法，导致编译器报警。,https://github.com/vllm-project/vllm/issues/10060
vllm,这是一个性能优化类型的issue，主要对象是vllm库中的piecewise cudagraph。导致该问题的原因可能是图形捕获时间过长，已经进行了优化，降低了从5秒到0.5秒的时间。,https://github.com/vllm-project/vllm/issues/10059
vllm,这是一个功能需求的issue，主要涉及V1模型运行器中集成分段CUDA图的功能。由于目前分段CUDA图与自定义操作不兼容，因此依赖于Torch Inductor来优化模型，导致无法支持FP8或其他量化模式。,https://github.com/vllm-project/vllm/issues/10058
vllm,这是一条关于代码优化的issue，主要涉及前端API实现。由于对try/except块进行了调整，提出建议将它们调整为单个块以提高代码清晰性和简洁性。,https://github.com/vllm-project/vllm/issues/10056
vllm,这是一个文档更新类的issue，涉及主要对象为该项目的TPU安装指南。由于安装指南中未明确指出需要等待资源请求分配、未提供检查请求状态的命令、未提供安装Miniconda的命令，导致用户可能遇到问题或需要更新的信息。,https://github.com/vllm-project/vllm/issues/10055
vllm,这个issue是优化建议，关注的主要对象是`entrypoints/openai/test_accuracy.py`和`entrypoints/openai/test_run_batch.py`。这个优化建议是因为这两个测试占据了大部分entrypoint时间。,https://github.com/vllm-project/vllm/issues/10052
vllm,这是一个用户提出需求的issue，涉及的主要对象是将新的Navi架构添加到支持的架构列表中。,https://github.com/vllm-project/vllm/issues/10050
vllm,这个issue类型属于功能性需求，主要涉及的对象为Cutlass c3x kernels的重构。原因是为了提高代码的可维护性并方便进行实验。,https://github.com/vllm-project/vllm/issues/10049
vllm,这是一个功能需求的issue，主要涉及的对象是多模态处理器。用户提出该问题是为了减少由后续PR引起的合并冲突风险，目前还没有使用新多模态处理器的模型。,https://github.com/vllm-project/vllm/issues/10044
vllm,这是一个用户提出需求的issue，主要涉及的对象是新增模型支持。由于Tencent发布了一个性能优越的389B MoE模型，用户希望在vllm中添加对该模型的支持。,https://github.com/vllm-project/vllm/issues/10043
vllm,这个issue类型是优化建议，涉及的主要对象是项目的CI/Build过程。改动主要是为了改善本地使用`format.sh`时对Python版本兼容性的检测以及CI作业对不同Python版本的兼容性检查。,https://github.com/vllm-project/vllm/issues/10041
vllm,这个issue属于修改需求，主要涉及对象是`MultiModalInputs`。由于命名不符导致混淆，需要将其重命名为`MultiModalKwargs`。,https://github.com/vllm-project/vllm/issues/10040
vllm,该Issue属于代码优化类型，主要涉及的对象是嵌入模型列表的排序和去除不必要的合并功能。,https://github.com/vllm-project/vllm/issues/10037
vllm,这个issue是用户提出需求，请求vllm项目支持OpenAI的/v1/models端点，由于当前vllm项目不提供对此端点的支持，导致无法查看可用模型。,https://github.com/vllm-project/vllm/issues/10029
vllm,这是一个需求报告，用户询问关于vllm的A100 CI Benchmark结果中使用的A100硬件配置的问题。,https://github.com/vllm-project/vllm/issues/10026
vllm,这个issue是功能新增类型，主要对象是Qwen2VL模型的LoRA支持。由于用户希望在Qwen2VL中实现LoRA支持，因此添加了这一功能。,https://github.com/vllm-project/vllm/issues/10022
vllm,这个issue属于一个功能需求提议，提议为VLMs添加在线视频支持。,https://github.com/vllm-project/vllm/issues/10020
vllm,该issue类型为用户提出需求，涉及主要对象是关于vllm中如何组织大量请求的调用。用户询问如何在调度级别做最佳实践以降低延迟，并寻求减少潜在影响推理性能的解决方案。,https://github.com/vllm-project/vllm/issues/10018
vllm,该issue属于用户提出需求，主要针对vllm CLI flags排序问题。原因可能是当前CLI flags无序导致用户阅读困难。,https://github.com/vllm-project/vllm/issues/10017
vllm,这是一个功能需求的issue，主要涉及vllm CLI flags的显示顺序问题，用户希望能够提升CLI参数的易读性。,https://github.com/vllm-project/vllm/issues/10016
vllm,这是一个需求类型的issue，涉及主要对象是vLLM模型。这个问题提出了添加级联推断功能以加速推断过程的需求。,https://github.com/vllm-project/vllm/issues/10011
vllm,这是一个与代码优化和构建依赖项相关的issue，主要涉及TPU要求文件的重构和构建依赖项的确定。,https://github.com/vllm-project/vllm/issues/10008
vllm,这是一个文档更新的issue，主要涉及vLLM文档中关于从本地文件加载的内容。由于markdown渲染存在问题，因此采用原始的HTML格式。可能是由于markdown渲染问题导致了这个issue。,https://github.com/vllm-project/vllm/issues/9999
vllm,这个issue是一个性能优化的建议，主要涉及到ShmRingBuffer中的时间延迟问题，提出使用os.sched_yield替代time.sleep，以提高性能。,https://github.com/vllm-project/vllm/issues/9994
vllm,这是一个特性改进请求，主要涉及的对象是优化 BNB 静态变量，以简化 BNB 量化支持过程。,https://github.com/vllm-project/vllm/issues/9987
vllm,这个issue类型为用户提出需求，主要涉及的对象是vllm的批量离线推理。用户询问关于离线批处理推理是否有需求对提示列表长度有要求。,https://github.com/vllm-project/vllm/issues/9985
vllm,这是一个功能更新类型的 issue，主要对象是该项目中的 Encoder Decoder 模块。由于当前版本只能运行在 xFormers 后端，用户希望更新 Mllama，使其能够在 xFormers 和 FlashAttention 两个后端上运行。,https://github.com/vllm-project/vllm/issues/9982
vllm,这个issue是一个功能需求报告，主要涉及VLLM的prefix benchmarking功能。,https://github.com/vllm-project/vllm/issues/9929
vllm,这个issue属于用户提出需求类型，主要涉及vLLM的性能文档改进。由于当前性能相关信息分散在多个地方，缺乏整体性的概述，用户提出了希望改进性能文档的建议。,https://github.com/vllm-project/vllm/issues/9927
vllm,该issue属于文档更新类型，主要涉及文档中有关TPU更多详细信息的更新。由于原文档中缺少关于TPUs的详细信息和链接，导致用户提出需要更多关于TPUs的信息。,https://github.com/vllm-project/vllm/issues/9926
vllm,这个issue是关于提出需求的，主要涉及的对象是为项目创建一个漏洞管理团队。导致提出这个需求的原因是项目需要负责及时响应漏洞报告的团队。,https://github.com/vllm-project/vllm/issues/9925
vllm,这个issue是关于新增功能的需求，主要涉及支持PixtralHFTransformer的量化。原因是之前只能量化语言模型，导致无法量化视觉编码器。,https://github.com/vllm-project/vllm/issues/9921
vllm,这个issue属于用户提出需求类型， 主要对象是对SparseLLM/prosparse-llama-2-7b模型的支持。由于当前vllm无法正确运行SparseLLM/prosparse-llama-2-7b模型的架构，导致用户寻求对该新模型的支持。,https://github.com/vllm-project/vllm/issues/9916
vllm,这是一个关于需求提出的issue，主要涉及前端多模态支持加载本地图片文件。由于缺乏多模态支持，用户无法加载本地图片文件，故提出需求。,https://github.com/vllm-project/vllm/issues/9915
vllm,这是一个需求提出的issue，主要涉及对象是VLM2Vec模型，用户在此提出需求是因为现有的chat模板无法很好地适配VLM2Vec模型，且VLM2Vec并不是用于聊天的模型。,https://github.com/vllm-project/vllm/issues/9912
vllm,该issue是一个需求报告，涉及的主要对象是更新torch至2.5版本。由于markdown渲染问题，无法正常显示所以使用了原始的html代码表示。,https://github.com/vllm-project/vllm/issues/9911
vllm,这个issue是关于更新文档，涉及VLLM项目中多输入支持的更新。,https://github.com/vllm-project/vllm/issues/9906
vllm,这是一个用户提出需求的类型，主要涉及对象是想在vllm中使用Mixtral MoE模型时获取`router_logits`，用户询问如何实现此功能。,https://github.com/vllm-project/vllm/issues/9905
vllm,这个issue是一个需求提出类型，主要涉及增加Llama 3和CommandR Chat Templates至chat_templates仓库，可能是为了丰富Chat Templates的功能。,https://github.com/vllm-project/vllm/issues/9904
vllm,这个issue类型是需求提出，主要涉及对象是vllm开发中自动释放显卡内存的功能。用户希望通过添加代码实现vllm在不使用模型推理时自动释放显卡内存，以避免未来可能出现的OOM问题。,https://github.com/vllm-project/vllm/issues/9903
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm模型中的kv-cache。由于设置了max_tokens=1，导致模型仅在prefill阶段中运行，从而提出了关于kvcache计算和优化的问题。,https://github.com/vllm-project/vllm/issues/9902
vllm,这是一个用户请求详细配置的类型的Issue，主要对象是VLLM的配置和硬件环境，用户想要获得关于在A100硬件上运行特定模型的最佳配置命令。,https://github.com/vllm-project/vllm/issues/9899
vllm,这是一个关于性能优化的讨论，不是bug报告或用户提出需求。,https://github.com/vllm-project/vllm/issues/9896
vllm,这是一个用户提出需求的issue，主要涉及到支持引导解码与多步解码的结合。原因可能是希望在使用多步解码的同时，仍然能够使用引导解码，以提高速度。,https://github.com/vllm-project/vllm/issues/9893
vllm,这是一个需求类型的issue，主要涉及的对象是torch.compile模块，用户希望能够使用PyTorch中稳定API的解释器，以避免过多地使用PyTorch的内部API。,https://github.com/vllm-project/vllm/issues/9889
vllm,这是一个关于优化v1版本可测试性的issue。,https://github.com/vllm-project/vllm/issues/9888
vllm,这个issue属于用户提出需求类型，主要涉及项目中需要添加Github Action来构建和推送cpu-inference镜像，可能由于当前尚未为此功能添加相应的自动化操作导致用户发起该需求。,https://github.com/vllm-project/vllm/issues/9884
vllm,这是一个功能需求类型的issue，主要涉及的对象是前端组件，用户提出需要添加`max_tokens`作为prometheus指标的功能。,https://github.com/vllm-project/vllm/issues/9881
vllm,这个issue属于用户提出需求类型，主要涉及vLLM v1中的logprobs和prompt logprobs支持，用户希望增加此功能来获得token的logprobs值、token ranks等信息。,https://github.com/vllm-project/vllm/issues/9880
vllm,这是一个功能增强的issue，涉及到torch compile模块的注释和测试。,https://github.com/vllm-project/vllm/issues/9876
vllm,此issue类型为用户提出需求，询问vllm是否有计划支持类似GPT-SoVITS语音生成大模型，未获得明确回复。,https://github.com/vllm-project/vllm/issues/9873
vllm,这是一个关于需求提出的issue，主要涉及了VLMs支持细粒度调度的问题。由于V0版本未考虑到复杂依赖关系，无法灵活调度，因此提出了对V1版本进行改进的建议。,https://github.com/vllm-project/vllm/issues/9871
vllm,这个issue属于文档更新类型，涉及更新Qwen文档，由于PR描述部分未填写，需要完善文档并按照要求填写PR信息。,https://github.com/vllm-project/vllm/issues/9869
vllm,这是一个用户提出需求的issue，主要涉及支持一个新模型NV-Embed-v2。用户提出需求得不到响应，可能导致症状为功能不完善或无法使用新模型。,https://github.com/vllm-project/vllm/issues/9868
vllm,这是一个功能改进（feature enhancement）类型的issue，涉及到torch.compile相关的测试计划的重构。,https://github.com/vllm-project/vllm/issues/9866
vllm,这是一个性能优化提案，涉及主要对象是Qwen2-VL-7B AWQ模型的性能。出现该问题可能是由于性能回归导致推断时间未能得到有效改善。,https://github.com/vllm-project/vllm/issues/9863
vllm,该issue类型为功能需求，主要涉及V1版本中的多进程张量并行支持，因为需要实现新特性并进行性能优化。,https://github.com/vllm-project/vllm/issues/9856
vllm,这是一个功能性需求的issue，主要涉及Machete核心支持W4A8量化和重构。原因可能是需要增强其性能和功能。,https://github.com/vllm-project/vllm/issues/9855
vllm,这个issue是用户提出需求，要求为flashinfer添加支持滑动窗口特性。,https://github.com/vllm-project/vllm/issues/9854
vllm,这是一个功能更新的issue，涉及更新benchmark_throughput.py以支持图像输入。,https://github.com/vllm-project/vllm/issues/9851
vllm,"这个issue类型是用户提出需求，主要对象是要求支持新模型""BAAI/bgem3""。缺乏响应可能是由于该模型与最接近的已支持模型之间存在较大差异导致难以支持。",https://github.com/vllm-project/vllm/issues/9847
vllm,该issue是一个新功能添加类型的PR，主要涉及对Qwen2-VL模型进行测试的增加。由于长上下文问题，之前对qwen2vl模型测试遇到了问题，现在将`runner_mm_key`从测试`VLMTestInfo`中移到`CustomTestOptions`，并对VLM测试进行标准和扩展子集分割，以解决这些问题。,https://github.com/vllm-project/vllm/issues/9846
vllm,这是一个用户提出需求的issue，主要涉及的对象是为IBM Granite模型添加Pipeline Parallelism支持。因为目前vllm仅支持一些特定的架构，未包括Granite模型，导致无法使用Pipeline Parallelism来扩展工作负载和保持性能。,https://github.com/vllm-project/vllm/issues/9844
vllm,该issue类型是用户提出需求，主要对象是支持在线视频的VLMs。由于现有的VLMs只支持图像输入，用户提出了增加对视频输入的支持。,https://github.com/vllm-project/vllm/issues/9842
vllm,这是一个用户提出需求类型的issue，主要涉及 Jenkins CI 中添加多步调度场景测试功能。由于当前 Jenkins CI 尚未包含多步调度场景测试功能，用户希望添加该功能以进一步完善产品功能。,https://github.com/vllm-project/vllm/issues/9836
vllm,该issue类型为提出需求，主要涉及vllm wheels的安装方式和发布流程优化。导致该需求的原因是当前安装vllm wheels的方式不够简洁和方便，用户希望通过更直接的方式来安装vllm wheels。,https://github.com/vllm-project/vllm/issues/9831
vllm,这个issue属于用户提出需求类型，主要对象为支持torch 2.5.1的公共二进制发布。由于torch发布了新版本，用户需求支持这一版本的二进制发布。,https://github.com/vllm-project/vllm/issues/9830
vllm,这是一个关于性能优化的issue，主要涉及到两处优化方案和其效果评估。原因是通过优化代码实现中的不必要操作，提高性能。,https://github.com/vllm-project/vllm/issues/9829
vllm,这个issue类型是需求提出，涉及的主要对象是软件依赖库pynvml。可能是由于缺少对pynvml的最低版本要求导致的需求提出。,https://github.com/vllm-project/vllm/issues/9827
vllm,这个issue类型为功能改进，涉及的主要对象是`AsyncLLM`实现。原因导致需要改进是为了更好地利用GPU和CPU的重叠进行任务处理。,https://github.com/vllm-project/vllm/issues/9826
vllm,这是一个用户提出需求的issue，主要涉及的对象是Qwen2VisionTransformer for Qwen2-VL，由于之前只能对language model进行量化，用户希望能够支持对vision encoder的量化。,https://github.com/vllm-project/vllm/issues/9817
vllm,这个issue类型为功能增强，主要涉及了PixtralHF模型的模型测试。原因是开发人员希望为PixtralHF模型添加一个简单的vLLM vs HF测试。,https://github.com/vllm-project/vllm/issues/9813
vllm,这是一个需求类型的issue，主要涉及依赖管理工具的配置。,https://github.com/vllm-project/vllm/issues/9811
vllm,"这是一个需求提出类型的issue，主要涉及的对象是vllm，由于需要将""Writing in the Margins""算法集成到系统中，因此提出了这个问题。",https://github.com/vllm-project/vllm/issues/9807
vllm,此issue为新增功能请求，涉及的主要对象是为vLLM添加`LlamaEmbeddingModel`作为`LlamaModel`的嵌入实现。由于公司政策原因，该功能的细节暂时无法公开，但希望能够在vLLM中使用该模型，并最终公开使用。,https://github.com/vllm-project/vllm/issues/9806
vllm,这个issue是一个功能需求，主要涉及的对象是vLLM软件中的limitmmperprompt功能。由于现有的逻辑过于保守，导致无法同时支持10个小图像和更大的图像。,https://github.com/vllm-project/vllm/issues/9805
vllm,这个issue是在请求文档更新，主要对象是项目贡献者；由于需要在PR中添加`Signedoffby`头部作为DCO的确认，且文档需加入DCO信息和许可证链接。,https://github.com/vllm-project/vllm/issues/9803
vllm,该issue属于用户提出需求类型，主要涉及的对象是在Intel x86 MacBook Pro 上运行Phi3.5。由于用户希望在特定环境中运行Phi3.5，可能由于硬件或软件配置不匹配而导致无法顺利运行，需要寻求相关支持或解决方案。,https://github.com/vllm-project/vllm/issues/9795
vllm,这个issue是关于需求的，主要涉及vllm中对于多模态模型的前缀缓存支持问题。原因是当前不支持启用前缀缓存功能，用户在尝试进行离线批量推理时遇到了问题。,https://github.com/vllm-project/vllm/issues/9790
vllm,这个issue类型是用户提出需求，问题或寻求帮助，主要对象是关于Qwen2.5工具选择auto支持的问题。由于没有收到响应，用户提出了与该功能相关的疑问。,https://github.com/vllm-project/vllm/issues/9789
vllm,这是一个改进代码质量的issue，主要涉及到api server测试中异常跟踪的简化。,https://github.com/vllm-project/vllm/issues/9787
vllm,这是一个请求帮助的issue，涉及主要对象是vllm中返回logits的问题。由于连接丢失导致无法返回输出值给RequestOutput打印。,https://github.com/vllm-project/vllm/issues/9784
vllm,这是一个关于提议改进`LLMEngineCore`的issue，涉及到代码重复、多进程处理和API设计方面的讨论。,https://github.com/vllm-project/vllm/issues/9782
vllm,该issue属于功能增强类型，涉及的主要对象是FlashInfer实现。原因是为了在cascade inference稳定之前在当前的FlashInfer实现中添加chunked-prefill支持。,https://github.com/vllm-project/vllm/issues/9781
vllm,这个issue是一个用户需求提出，主要对象是VLLM前端，用户建议添加用户队列功能来进行离线推理。,https://github.com/vllm-project/vllm/issues/9780
vllm,这个issue类型是用户提出需求，主要涉及的对象是实现imagemodal模型的吞吐量基准测试，由于该功能较大，需要跨多个PR进行实现。,https://github.com/vllm-project/vllm/issues/9778
vllm,这个issue类型为功能需求提议，涉及主要对象为metrics polling，由于需要更有效地追踪等待适配器和活跃适配器的最近历史，从而避免过于频繁的指标轮询。,https://github.com/vllm-project/vllm/issues/9777
vllm,"该issue为需求添加新功能，主要对象为torch.compile。原因是用户希望添加名为""deepseek v2""的编译功能并进行本地测试。",https://github.com/vllm-project/vllm/issues/9775
vllm,这是一个功能新增的issue，主要涉及的对象是添加对Idefics3的支持，原因是根据讨论需要实现这一功能。,https://github.com/vllm-project/vllm/issues/9767
vllm,这是一个功能需求的issue，主要涉及的对象是模型量化和Marlin内核扩展。,https://github.com/vllm-project/vllm/issues/9766
vllm,这是一个用户提出需求的issue，主要涉及支持HQ support以及float16 kernels，可能由于某种原因导致需要对float zero points进行调整。,https://github.com/vllm-project/vllm/issues/9764
vllm,这个issue是一个PR请求，类型为模型（Model），主要涉及添加工具解析器用于openbmb/MiniCPM34B。这个问题出现的原因可能是缺少对应的工具解析器，需要对vLLM进行改进。,https://github.com/vllm-project/vllm/issues/9762
vllm,这是一个用户提出需求的 issue，主要涉及使用 vLLM 进行最大批量处理的情况。用户希望了解如何设置参数以实现最大吞吐量。,https://github.com/vllm-project/vllm/issues/9760
vllm,该issue为功能需求提案，涉及将 chat 对话作为输入进行处理。,https://github.com/vllm-project/vllm/issues/9759
vllm,"这个issue属于功能增强类型，涉及主要对象为torch编译系统，旨在为一些moe模型添加""torch compile""注释并测试其编译是否通过。",https://github.com/vllm-project/vllm/issues/9758
vllm,这是一个用户提出需求的issue，主要涉及如何获取`vLLM`模型的交叉熵损失，由于用户认为目前的方法不够便捷，希望能够像`transformers`一样直接获取交叉熵。,https://github.com/vllm-project/vllm/issues/9750
vllm,这是一个用户提出需求的类型，主要涉及的对象是XPU设备，由于使用gather操作产生了额外的cat操作，通过使用allreduce来替换gather操作，可以提升性能。,https://github.com/vllm-project/vllm/issues/9748
vllm,这是一个关于更新操作/checkout版本的issue，类型为功能更新，涉及的主要对象是vllm。由于更新导致的bug或问题不明确。,https://github.com/vllm-project/vllm/issues/9746
vllm,这是一个功能需求类型的issue，主要涉及到支持在`load_weights`后清空kvcache的API功能。由于当前vllm实现中，加载新权重后未更新缓存，导致新模型使用旧缓存前缀。,https://github.com/vllm-project/vllm/issues/9744
vllm,这是一个功能需求的issue，主要涉及的对象是vLLM的vision API endpoint，用户提出需要通过提供绝对路径来供应图片，以降低图片处理时的延迟。,https://github.com/vllm-project/vllm/issues/9742
vllm,这是一个关于代码改进的issue，涉及到Refactor LLMEngine To Use Multiprocessing。由于引入Multiprocessing，需要添加`AsyncLLMEngine`和更新`LLM`以在这种上下文中更好地工作。,https://github.com/vllm-project/vllm/issues/9741
vllm,这个issue类型是改进提议，主要对象是flash attention，由于之前的工作需要进行重构。,https://github.com/vllm-project/vllm/issues/9740
vllm,这个issue是关于提出需求的，并涉及到了`Detokenizer`的完全异步化。这个问题由于当前的实现方式导致了`Detokenizer`可能会滞后，需要维护请求滞后步数，并存在消息传递复杂性和性能开销的问题。,https://github.com/vllm-project/vllm/issues/9725
vllm,这是一个功能需求提问，主要对象是`cudagraph`。由于无法在普通的PyTorch中共享输出缓冲区，因此用户提出了使用`tensor weak reference`来实现共享输出缓冲区的功能需求。,https://github.com/vllm-project/vllm/issues/9724
vllm,这是一个性能改进的议题，提出在并发情况下如何提高性能。,https://github.com/vllm-project/vllm/issues/9722
vllm,这个issue类型是对模型的改进，主要涉及对象是为Mllama添加BNB量化支持，由于当前MllamaForConditionalGeneration模型尚不支持BitsAndBytes量化，故用户提出这一需求。,https://github.com/vllm-project/vllm/issues/9720
vllm,这个issue类型是功能需求提议，主要对象是SpecDecodeWorker类。原因是为了支持性能分析而改进了SpecDecodeWorker类中的功能。,https://github.com/vllm-project/vllm/issues/9719
vllm,该issue属于用户需求类型，主要涉及VLLM的编译控制以及自定义操作的细粒度控制。由于需要重新设计编译控制，用户提出通过`VLLM_CUSTOM_OPS` 环境变量进行对自定义操作的精细控制。,https://github.com/vllm-project/vllm/issues/9715
vllm,该issue类型为用户提出需求，主要涉及对象是新增模型Tarsier，并希望支持该模型。原因可能是希望扩展现有模型支持范围，以提高模型可用性。,https://github.com/vllm-project/vllm/issues/9707
vllm,这是一个关于更新README.md的issue，主要类型是文档更新，涉及的主要对象是代码仓库vLLM。原因是markdown渲染不起作用，导致需要使用原始HTML代码。,https://github.com/vllm-project/vllm/issues/9705
vllm,这是一个关于需求的问题，用户想讨论关于vllm模型在推断和嵌入中的使用。由于用户想要了解是否可以同时在同一VLLM实例中使用同一个模型进行推断和嵌入，可能是由于对该功能实现的兴趣或需求导致。,https://github.com/vllm-project/vllm/issues/9702
vllm,这是一篇关于性能优化的issue，讨论了NVLink Sharp对系统性能的影响。,https://github.com/vllm-project/vllm/issues/9699
vllm,这是一个用户提出需求的issue，主要对象是vllm api server的功能，用户希望能够使用多个秘钥来监控和管理多个用户的访问权限。,https://github.com/vllm-project/vllm/issues/9698
vllm,这个issue类型是功能需求，主要涉及模型的支持扩展，用户请求支持新的嵌入模型。,https://github.com/vllm-project/vllm/issues/9697
vllm,这是关于功能使用问题的用户提出的需求。该问题涉及到vLLM模型的使用和数据处理流程，可能是由于代码中导入模块错误而导致的问题。,https://github.com/vllm-project/vllm/issues/9695
vllm,这个issue类型为用户提出需求，主要涉及使用vllm的langchain工具调用问题，用户寻求关于如何使用langchain进行vllm serve工具调用的帮助。,https://github.com/vllm-project/vllm/issues/9692
vllm,这是一个增加功能需求的issue，主要涉及vLLM在初始化过程中启动时间较长的问题，用户提出了添加一个max_batch_size_to_capture标志来减少初始化时间的建议。,https://github.com/vllm-project/vllm/issues/9686
vllm,这个issue类型是用户提出需求，询问问题，该问题单涉及的主要对象是如何提高多卡推断的吞吐量，由于增加显卡资源没有显著改善Vincent模型推断的吞吐量，用户想知道如何改进这个指标。,https://github.com/vllm-project/vllm/issues/9684
vllm,这个issue是用户提出需求，主要涉及LLM模型性能优化，用户寻求高吞吐量的benchmark和指引。,https://github.com/vllm-project/vllm/issues/9680
vllm,这是一个用户提出需求的issue，涉及的主要对象是V1模型。这个问题是由于需要支持在flash attention backend上使用滑动窗口attention而提出的需求。,https://github.com/vllm-project/vllm/issues/9679
vllm,这是一个用户提出需求的类型，主要涉及的对象是Navi4x平台的Llama模型系列，用户要求为Llama模型系列添加FP8支持，可能是为了解决兼容性或功能性方面的问题。,https://github.com/vllm-project/vllm/issues/9674
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加XQA kernel用于masked multihead attention。,https://github.com/vllm-project/vllm/issues/9672
vllm,这是一个功能需求的issue，主要涉及V1版本中的prefix caching功能的实现。原因可能是为了提高V1引擎的性能和效率。,https://github.com/vllm-project/vllm/issues/9668
vllm,这是一个功能需求的issue，涉及重构日志部分和改进嵌入模型的日志记录。原因是为了支持不同模型的日志记录，通过重构Stats类和引入Double Dispatch模式来简化扩展，并提供清晰可扩展的日志架构。,https://github.com/vllm-project/vllm/issues/9667
vllm,这是一个文档更新的任务，目的是更新spec_decode.rst中的FAQ链接。,https://github.com/vllm-project/vllm/issues/9662
vllm,这个issue类型是用户提出需求，涉及的主要对象是GitHub上的一个workflow。增加`operationsperrun`的限制是为了确保workflow的更改能更快地传播到所有的issues和PRs，而之前的限制导致了操作限制不足的问题。,https://github.com/vllm-project/vllm/issues/9661
vllm,这是一个用户提出需求的issue，涉及的主要对象是添加指标以获取请求排队时间、转发时间和执行时间的信息。由于`otlptracesendpoint`标志导致对`collect_model_forward_time`和`collect_model_execute_time`的限制，用户要求移除该限制以便收集有关`model_forward_time`和`model_execute_time`的信息。,https://github.com/vllm-project/vllm/issues/9659
vllm,这是一个需求求助类型的issue，主要涉及在kubernetes环境下运行Llama-3.1-70B-Instruct模型推理，用户寻求针对多用户同时发送大量请求以获取最佳吞吐量的最佳参数和建议。,https://github.com/vllm-project/vllm/issues/9658
vllm,这是一个用户提出需求的issue，主要对象是传递多个LoRA模块，并出现解析错误，原因是无法正确传递多个LoRA模块。,https://github.com/vllm-project/vllm/issues/9655
vllm,这是一个用户提出需求的issue，主要涉及支持SageAttention功能。由于期望实现Quantized Attention来提高速度，并且在各种模型中不损失终端指标，导致提出了这个需求。,https://github.com/vllm-project/vllm/issues/9654
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM框架和IBM Spyre AI accelerator，由于IBM Spyre暂时只支持执行IBM开源Foundation Model Stack (`fms`)中的建模代码，并且不支持分页注意力或连续批处理，因此用户希望在vLLM框架中添加对IBM Spyre的支持。,https://github.com/vllm-project/vllm/issues/9652
vllm,这个issue是一个模型改进的类型，主要涉及LLavanext中计算最大令牌数和虚拟数据的问题。原因是之前使用了固定的高度和宽度来计算，现在需要根据不同的配置计算图像特征并保留最大值来纠正模型计算的问题。,https://github.com/vllm-project/vllm/issues/9650
vllm,该issue类型是用户提出的需求。主要涉及对象是vLLM项目和Cambricon MLU。由于Cambricon团队已经在内部支持了vLLM在Cambricon MLU上的使用，提出希望贡献MLU适配代码到vLLM项目中，并计划在11月份提交pull request。,https://github.com/vllm-project/vllm/issues/9649
vllm,这是一个需求类型的issue，涉及主要对象是mm_input_mapper，用户希望将其移动到一个独立的进程中。,https://github.com/vllm-project/vllm/issues/9646
vllm,这是一个关于缓存重用功能的问题，涉及主要对象是vllm。由于用户想了解vllm中的前缀缓存是否支持在请求计算之前自动重用缓存，以及新批次请求是否会自动重用共享前缀，以及与TensorRTLLM的缓存重用机制的比较。,https://github.com/vllm-project/vllm/issues/9643
vllm,这个issue类型是代码改进，涉及的主要对象是在vllm项目中使用current_platform.is_rocm。原因是代码中需要更新变量`is_hip`为`current_platform.is_rocm`。,https://github.com/vllm-project/vllm/issues/9642
vllm,这是一个更新issue，主要涉及torch compile annotations添加到某些模型中。,https://github.com/vllm-project/vllm/issues/9641
vllm,这个issue类型为功能需求，主要涉及的对象是用户对于添加新模型的请求。该问题没有得到回应，用户希望支持一个叫做Ovis1.6-Gemma2-9B的新模型。,https://github.com/vllm-project/vllm/issues/9638
vllm,这个issue是一个功能增强类型，主要涉及到torch compile的支持和修复allgather编译。由于缺乏torch compile注释，需要自动推断dynamic_arg_dims以修复问题。,https://github.com/vllm-project/vllm/issues/9637
vllm,这个issue是关于用户需求的问题，主要涉及如何在vllm中进行多模态内容的测试，原因是用户不清楚如何使用benchmark_serving.py来测试多模态功能而导致问题出现。,https://github.com/vllm-project/vllm/issues/9633
vllm,这是一个用户提出需求的issue，主要涉及的对象是moe models。原因是moe models在forward过程中读取配置文件导致与torch.compile不兼容。,https://github.com/vllm-project/vllm/issues/9632
vllm,该issue类型为功能性需求提议，主要涉及Qwen模型的LoRA支持添加，由于需要更好地支持LoRA，故提出对Qwen LLM和VL进行区分。,https://github.com/vllm-project/vllm/issues/9622
vllm,该issue属于需求提出类，主要涉及改进python-only开发环境设置。由于当前的python-only开发和构建过程较为繁琐，提出使用`VLLM_USE_PRECOMPILED`环境变量来跳过扩展编译，以简化流程。,https://github.com/vllm-project/vllm/issues/9621
vllm,这个issue类型是功能改进，主要涉及的对象是torch compile模块。这次改进的原因是为了增强模型的注释和测试。,https://github.com/vllm-project/vllm/issues/9614
vllm,这是一个模型改进类型的issue，主要涉及Qwen2VL模型新增min_pixels / max_pixels参数，用户提交了该需求并已提供相应代码更新。,https://github.com/vllm-project/vllm/issues/9612
vllm,这是一个用户提出需求的issue，主要涉及的对象是Kernel中新增一个针对FATReLU的kernel。由于缺少具体描述和细节，可能是用户请求新增相关内容。,https://github.com/vllm-project/vllm/issues/9610
vllm,这是一个关于新增模型支持的需求提出的issue，主要涉及的对象是vllm项目和Pangea-7B模型。由于该模型与已支持的LlavaNext具有相同的架构，提出者希望了解支持该模型会有什么困难。,https://github.com/vllm-project/vllm/issues/9607
vllm,"这是一个用户提出需求的issue，主要涉及支持新模型""stepfun-ai/GOT-OCR2_0""，因为当前版本不支持该架构。",https://github.com/vllm-project/vllm/issues/9606
vllm,这个issue属于用户提出的需求类型，主要涉及到了在encoder-decoder inputs的结构转变中的问题。这个问题的提出是为了更方便地支持decoder-only模型的输入嵌入。,https://github.com/vllm-project/vllm/issues/9604
vllm,这是一个用户需求类型的issue，涉及主要对象是如何向vLLM处理过程环境中添加插件处理进程。原因是用户需要在一个包含k+1个进程的同一world中实现特定的应用场景。,https://github.com/vllm-project/vllm/issues/9603
vllm,这是一个建议更改类型的issue，主要涉及openai api中的响应格式，用户想要禁用响应中的额外部分。,https://github.com/vllm-project/vllm/issues/9601
vllm,"这是一个用户提出需求的issue，主要涉及的对象是MiniCPM3模型的infer速度问题，可能由于""forceeager""参数设置导致infer速度过慢，用户在寻找解决此问题的方法。",https://github.com/vllm-project/vllm/issues/9600
vllm,这是一个功能特性请求，涉及支持 `x-request-id` 头，在在线服务中有关请求标识的需求。,https://github.com/vllm-project/vllm/issues/9593
vllm,这是一个用户提出需求类型的issue，主要涉及支持新的IBM Granite 3.0模型。由于没有对应的支持，用户请求添加对该模型的支持。,https://github.com/vllm-project/vllm/issues/9591
vllm,这是一个用户提出需求的issue，主要对象是torch.compile。由于当前torch.compile的使用不够方便，用户希望支持更简单的方法来进行类型注释的动态尺寸推断。,https://github.com/vllm-project/vllm/issues/9589
vllm,这是一个关于功能缺失的问题，主要涉及到vLLM中的FP8 KV Cache功能，用户询问是否存在逆量化操作，以及如何寻找实现逆量化操作的代码。,https://github.com/vllm-project/vllm/issues/9585
vllm,该issue类型为用户提出需求，主要对象是QWenLMHeadModel模型，由于LoRA功能未支持该模型，用户希望未来的版本中添加对该功能的支持。,https://github.com/vllm-project/vllm/issues/9584
vllm,这是一个用户提出需求的issue，主要涉及vLLM是否支持生成结构化输出的问题，用户想要运行特定模型的推断，并询问如何集成vLLM。,https://github.com/vllm-project/vllm/issues/9582
vllm,该issue类型为功能增强，主要涉及对象是模型VLLM以支持E5-V，并添加对图像输入的测试。,https://github.com/vllm-project/vllm/issues/9576
vllm,这是一个用户提出需求的issue，主要涉及vllm是否支持在线批量推理功能。原因是用户使用Qwen2VL并部署了在线服务器，想了解是否支持在线批量推理。,https://github.com/vllm-project/vllm/issues/9575
vllm,这是一个关于功能支持的issue，主要涉及的对象是Qwen2 bnb，由于需要支持TP，用户进行了相应的PR操作。,https://github.com/vllm-project/vllm/issues/9574
vllm,这个issue类型为优化建议，主要涉及代码中的ifelse条件，提出简化代码逻辑的建议。,https://github.com/vllm-project/vllm/issues/9569
vllm,该issue类型为Feature请求，主要涉及VLLM软件支持1.58-bit模型的需求。,https://github.com/vllm-project/vllm/issues/9566
vllm,该issue类型为功能增强请求，涉及的主要对象是Encoder-Decoder模型。由于模型实现依赖于PagedAttention，所以在dtype=bfloat16的情况下仍然使用Xformers后端，而不是默认的FlashAttention。,https://github.com/vllm-project/vllm/issues/9559
vllm,这个issue是关于添加新模型或改进现有模型的问题，提议将Florence-2语言骨干支持添加到vLLM中。,https://github.com/vllm-project/vllm/issues/9555
vllm,这是一个用户提出需求的 issue，主要涉及 vllm 中实现自定义生成方法时遇到的问题，由于在访问和存储 attention 输出时出现错误，导致无法成功实现目标。,https://github.com/vllm-project/vllm/issues/9551
vllm,"这是一个需求反馈类型的issue，主要涉及支持模型""bert-base-chinese""，用户可能遇到的困难是支持BertForMaskedLM模型。",https://github.com/vllm-project/vllm/issues/9548
vllm,这是一个用户提出需求的issue，主要涉及对象是在LLM.generate中使用Qwen2-VL进行离线推断时设置max_pixels参数。原因可能是由于当前环境中的PyTorch版本与CUDA版本之间的不匹配导致。,https://github.com/vllm-project/vllm/issues/9545
vllm,这个issue是关于提出需求的，主要涉及的对象是函数追踪文件路径。由于读取追踪日志困难，用户建议通过可视化来帮助理解，这样有助于性能调整和调试。,https://github.com/vllm-project/vllm/issues/9543
vllm,这是一个特性需求的issue，主要对象是实现对控制解码的支持。由于作者希望实现对比解码，并为此进行了相关的工作和讨论，希望社区能够做出贡献。,https://github.com/vllm-project/vllm/issues/9541
vllm,该issue类型为提出需求，请教问题，主要对象是关于vllm服务器并发处理的。由于用户在尝试构建自己的llm api_server时，发现在测试代码时`vllm serve`表现出很强的推理效率，询问应该如何改进fastapi中的代码或者如何查看`vllm serve`的处理方式。,https://github.com/vllm-project/vllm/issues/9540
vllm,这是一个用户提出需求类的Issue，主要涉及vllm v0.6.3版本中的`System efficiency`和`Number of emitted tokens`的含义。由于用户想了解如何理解日志（当进行猜测性解码）而提出此问题。,https://github.com/vllm-project/vllm/issues/9539
vllm,这是一个需求提出的issue，主要涉及到Disaggregated prefilling功能的实现。由于需要实现X prefill + Y decode形式的功能，作者计划引入kv数据库，并设计了新组件PDEngine和RabbitMQ来实现此功能。,https://github.com/vllm-project/vllm/issues/9537
vllm,这是一个需求更改类的issue，主要涉及的对象是benchmark脚本。由于CLI参数重复导致维护困难和部分特性无法暴露在脚本中，用户提出了更新benchmark脚本以直接使用EngineArgs提供的CLI参数，避免重复和增强可维护性。,https://github.com/vllm-project/vllm/issues/9529
vllm,这是一个用户提出需求的 issue，主要涉及的对象是代码模型，其需求是支持在自动补全API中处理针对特定模型的FIM编码规则。,https://github.com/vllm-project/vllm/issues/9522
vllm,这个issue是关于性能优化的，主要涉及到PixtralHFVision模型，在使用Mistral的Pixtral和HF的apply_rotary_pos_emb时导致内存使用量过高。,https://github.com/vllm-project/vllm/issues/9520
vllm,这个issue是优化类型的，主要涉及Pixtral模型的输入处理函数，目的是提高性能。,https://github.com/vllm-project/vllm/issues/9514
vllm,这是一个用户提出需求的issue，主要涉及VLLM是否能与finetuned unsloth模型一起使用，用户想要在finetuned unsloth模型上运行推断。这个问题可能是由于用户想要使用VLLM进行推断时遇到了困难，希望能获得相关支持或指导。,https://github.com/vllm-project/vllm/issues/9509
vllm,这个issue是关于文档更新的，主要对象是关于GPU内存利用率的flag，描述了这个flag是对内存利用的全局限制。,https://github.com/vllm-project/vllm/issues/9507
vllm,这是一个[功能添加]类型的issue，主要涉及加载句子转换模型中的汇聚（pooling）配置文件，用户希望通过这个方法读取sentencetransformers模型的相关内容。,https://github.com/vllm-project/vllm/issues/9506
vllm,这是一个模型更新的issue，与MPT模型相关。,https://github.com/vllm-project/vllm/issues/9500
vllm,这个issue属于功能改进提案，主要对象是Flashinfer后端，用户提出需要添加环境变量来强制启用tensor cores。,https://github.com/vllm-project/vllm/issues/9497
vllm,这是一个关于使用vllm时建议使用flashinfer作为默认后端的问题，属于用户需求类型。用户提出了对flashinfer作为后端的疑惑，并希望了解在什么情况下使用flashinfer会比FA2更好的官方基准测试。,https://github.com/vllm-project/vllm/issues/9491
vllm,这是一个改进建议类型的issue，主要涉及到测试LM的decoder-only模块，旨在避免难以复现的问题。,https://github.com/vllm-project/vllm/issues/9488
vllm,这是一个用户提出需求的issue，主要涉及到前端部分的代码。由于缺乏控制执行顺序和惩罚范围的功能，导致了解决特定问题时的限制。,https://github.com/vllm-project/vllm/issues/9485
vllm,这是一个功能需求的issue，主要涉及的对象是vLLM下的Diff-Transformer，由于研究人员引入了Differential Transformer提出了相关功能需求。,https://github.com/vllm-project/vllm/issues/9480
vllm,这是一个用户提出需求的issue，主要涉及的对象是MllamaForCausalLM模型。由于该用户想要仅使用MllamaForCausalLM模型的文本层部分而无需使用视觉层，因此请求支持这一功能。,https://github.com/vllm-project/vllm/issues/9479
vllm,这个issue属于需求提出类型，主要涉及前端代码的修改以支持更简单的图像输入格式，这个需求是由于部分用户希望使用一种更简单的图像输入格式，目前系统无法处理这种格式所致。,https://github.com/vllm-project/vllm/issues/9478
vllm,这是一个功能请求类的issue，涉及主要对象是日志记录功能。原因是为了在LoRa启用时记录日志指标，并用于后续路由决策。,https://github.com/vllm-project/vllm/issues/9477
vllm,这是一个用户需求相关的issue，主要涉及多GPU环境下指定不同显存利用率的问题。,https://github.com/vllm-project/vllm/issues/9465
vllm,这是一个关于功能需求的issue，主要涉及到关于在VLLM中实现交替的局部-全局注意力层，其目的是降低KV缓存的大小并提高内存的利用效率。,https://github.com/vllm-project/vllm/issues/9464
vllm,这是一个用户提出需求的issue；该问题涉及如何在vllm serve命令中传递模型路径；由于环境中PyTorch版本为2.4.0，CUDA版本为12.6，用户希望了解如何正确传递模型路径。,https://github.com/vllm-project/vllm/issues/9459
vllm,该issue类型为功能需求提出，主要涉及到API中的`parallel_tool_calls`参数。由于目前该参数并未被实际使用，导致调用模型返回多个工具调用而不是单独返回，进而导致与某些框架的兼容性问题。,https://github.com/vllm-project/vllm/issues/9451
vllm,这是一个关于代码优化的Issue，涉及vLLM中计算查询起点位置/序列起点位置的问题。通过避免创建中间张量query_lens_tensor并在CPU上计算query_start_loc，实现异步处理。,https://github.com/vllm-project/vllm/issues/9447
vllm,该issue类型为用户提出需求，主要涉及对象是在使用Slack时需要添加注释。由于缺乏关于如何使用Slack的说明，用户提出了需要添加关于Slack使用的注释的需求。,https://github.com/vllm-project/vllm/issues/9442
vllm,这是一个关于改进/优化的issue，主要涉及到XPU模块。原因是XPU模块不作为依赖项，所以需要避免导入triton。,https://github.com/vllm-project/vllm/issues/9440
vllm,这是一个关于技术性更新和改进的issue，涉及主要对象为PyTorch XLA和TPU，需要提升内存使用峰值检测的准确性。,https://github.com/vllm-project/vllm/issues/9438
vllm,这个issue是一个功能需求类型，涉及自动关闭过期的问题和PR，主要对象是CI/Build系统。,https://github.com/vllm-project/vllm/issues/9436
vllm,这个issue类型是建议（RFC），主要涉及的对象是启用过期机器人用于对issue和PR进行处理，由于项目中有许多未活动的issue和PR，需要关闭不活跃的项。,https://github.com/vllm-project/vllm/issues/9435
vllm,该issue是关于用户提出需求的，主要涉及pynccl库中添加all_gather和reduce_scatter的wrapper，以便在CUDA图中使用，特别是Molmo和OLMo需要使用all_gather功能。,https://github.com/vllm-project/vllm/issues/9432
vllm,这个issue是关于CI/Build方面的改进提案，涉及到GitHub actions的使用方式。原因是为了提高安全性，避免恶意修改标签指向的问题。,https://github.com/vllm-project/vllm/issues/9430
vllm,这是一个关于功能需求的问题，主要对象是vLLM模型加载和实例契约。用户提出了是否可以支持在同一vLLM实例上运行多个模型的问题，原因是他们希望在性能强大的GPU上服务多个小模型而不是为每个模型管理单独的vLLM实例。,https://github.com/vllm-project/vllm/issues/9429
vllm,"该issue类型是一个功能需求，主要对象是vllm下的tool_choice参数设置问题，由于vllm还不支持设置tool_choice为""none""，导致用户无法阻止函数调用。",https://github.com/vllm-project/vllm/issues/9426
vllm,这个issue类型是功能需求，主要涉及对象是VLLM中的模型配置。由于新加入的`task`选项会带来对VLM2Vec用户的破坏性变化，需要用户显式指定任务模式，以保持长期可维护性。,https://github.com/vllm-project/vllm/issues/9424
vllm,这是一个用户提出需求的issue，主要涉及到如何在Metrics中获取time_to_first_token_seconds信息，可能是由于缺乏相关文档或指导而导致用户不清楚如何使用该功能。,https://github.com/vllm-project/vllm/issues/9416
vllm,该issue是一个功能需求，主要涉及支持 mistral 模型的 interleaved attention patterns。由于目前无法清晰地支持这种 attention 模式，需要将最大模型长度限制为允许的 attention 模式的最小长度。,https://github.com/vllm-project/vllm/issues/9414
vllm,"这是用户提出的需求。

这个问题单主要涉及vLLM中的功能设置以及针对Cohere的c4aicommandrplus082024模型不支持的情况。",https://github.com/vllm-project/vllm/issues/9405
vllm,这是一个用户提出需求的类型issue，主要涉及到对VLLM模型中特定prompt log probability的抽样需求。由于当前的抽样参数返回所有log probability，但用户希望能够有选择地抽样特定的log probability，以解决CUDA OOM的问题。,https://github.com/vllm-project/vllm/issues/9404
vllm,这个issue类型是关于用户需求和使用问题的，主要涉及到对于vllm分支选择以及优化的疑问。用户想了解应该使用哪个分支来测试推测解码以及哪个分支拥有最佳的推测解码优化。这可能是由于用户想要进行特定功能测试或者寻求性能优化建议引起的。,https://github.com/vllm-project/vllm/issues/9400
vllm,这个issue类型是用户提出需求，主要对象是为VLLM添加Exllama作为压缩张量后端。由于测试中出现了一些问题，用户可能在寻求针对这些特定模型配置的支持或解决方案。,https://github.com/vllm-project/vllm/issues/9395
vllm,这是一个功能改进的issue，主要涉及到代码重构executor_base ABC，由于不同的后端结构不同且存在重复功能实现，导致难以添加类型检查和代码可读性。,https://github.com/vllm-project/vllm/issues/9392
vllm,这是一个用户需求提出的issue，主要涉及benchmarking script的并发限制问题，用户希望添加一个新的flag来设定最大并发请求数。,https://github.com/vllm-project/vllm/issues/9390
vllm,这是一个用户提出需求的issue，涉及的主要对象是对于支持sentence-transformers配置文件的需求。由于新添加的模型需要额外的配置文件来设置汇总方法和是否需要对嵌入进行归一化，用户提出了希望支持这些配置文件的需求。,https://github.com/vllm-project/vllm/issues/9388
vllm,这个issue类型是功能需求提出，涉及主要对象是支持Roberta embedding models，由于需要在项目中添加对Roberta embedding models的支持。,https://github.com/vllm-project/vllm/issues/9387
vllm,这是一个性能改进类型的issue，主要涉及到vllm中的fused_moe层的性能优化，涉及到增加线程数、替换torch.sum函数、移动源文件等操作。,https://github.com/vllm-project/vllm/issues/9384
vllm,这个issue类型为需求提出，主要涉及的对象是支持Zyphra/Zamba2-7B模型的集成。由于尚未支持Mamba2，导致无法轻松支持Zamba2-7B模型，尤其是在使用`use_shared_attention_lora`的情况下可能更加复杂。,https://github.com/vllm-project/vllm/issues/9382
vllm,这个issue类型是优化改进，主要涉及VLLM的测试整合。由于VLLM的多模态测试存在冗余，需要对测试进行整合以提高效率。,https://github.com/vllm-project/vllm/issues/9372
vllm,这是一个用户提出需求的类型的issue，主要对象是关于支持prompt缓存的功能。用户希望能够在模型的多次推理过程中固定常用指令和特定部分的提示，以提高效率和性能。,https://github.com/vllm-project/vllm/issues/9368
vllm,这是关于需求的问题，涉及主要对象是vllm下的cu118版本的WHL包，由于v0.6.3和v0.6.2版本缺少对应的cu118 WHL包，导致用户无法通过WHL包安装VLLM。,https://github.com/vllm-project/vllm/issues/9353
vllm,这是一个用户提出需求的类型的问题，涉及到获取成功率/错误率的度量指标的问题。用户希望了解如何通过分母来计算成功率百分比指标，并询问了是否存在错误计数度量以便计算成功率。,https://github.com/vllm-project/vllm/issues/9346
vllm,这个issue属于建议类型，涉及到在容器映像中添加opentelemetry软件包。由于当前环境下缺乏opentelemetry软件包，用户提出建议将其包含在容器映像中。,https://github.com/vllm-project/vllm/issues/9340
vllm,这是一个功能需求的issue，涉及vLLM中的前端部分，主要是添加支持Goodput metric用于benchmarking serving system。,https://github.com/vllm-project/vllm/issues/9338
vllm,这是一个用户提出需求的类型，主要对象是vllm模型，用户提出了允许`max_tokens=0`的需求。,https://github.com/vllm-project/vllm/issues/9336
vllm,这是一个性能优化的问题，主要涉及到vllm中ngram查找性能的提升，原因是CPU GPU通信和ngram构建中的小内核导致了较长的提取时间。,https://github.com/vllm-project/vllm/issues/9333
vllm,该issue类型为功能需求，涉及的主要对象是LLaVA OneVision模型的quantization支持。由于应用需要在资源有限的硬件上运行，需要对模型进行量化处理，但目前的模型尚不支持BitsAndBytes量化。,https://github.com/vllm-project/vllm/issues/9324
vllm,该issue类型为用户提出需求，主要涉及的对象是vllm的支持扩展到rhymes-ai/Aria模型。这是由于用户希望vllm能够支持rhymes-ai/Aria模型，提出了这个需求。,https://github.com/vllm-project/vllm/issues/9323
vllm,这个issue属于代码优化类型，主要对象是vllm/attention/ops/triton_flash_attention.py文件。其中提出了删除与dropout相关内容以简化代码的建议，可能是为了提高代码清晰度和简洁性。,https://github.com/vllm-project/vllm/issues/9322
vllm,这个issue是关于测试的需求，主要涉及到vllm的Python-only开发。原因是需要在pythononly开发中编写一些测试，可能由于缺乏测试而导致潜在的问题。,https://github.com/vllm-project/vllm/issues/9315
vllm,该issue属于建议性反馈（RFC），主要涉及到了vllm中关于让每个模型都能成为奖励模型/嵌入模型的讨论。由于对奖励模型的兴趣日益增长，该RFC旨在通过让所有生成模型都能作为嵌入模型提供，从而支持处理监督奖励模型（PRM）。,https://github.com/vllm-project/vllm/issues/9314
vllm,这是一个关于改进安装脚本和相关文档的请求，而不是一个bug报告。,https://github.com/vllm-project/vllm/issues/9309
vllm,这是一个功能需求，主要涉及支持VLM2Vec多模态嵌入模型。原因是希望添加CLI选项以指定生成或嵌入模型，增加多模态嵌入API以及支持更多多模态嵌入模型。,https://github.com/vllm-project/vllm/issues/9303
vllm,这个issue类型为需求提出，主要涉及对象是vllm的核心功能。这个需求由于更好地隐藏seq group而导致于处理并行采样在llm引擎中。,https://github.com/vllm-project/vllm/issues/9302
vllm,这个issue属于功能需求报告，主要涉及的对象是TPU集成`compressed-tensors`以支持w8a8和w8a16压缩。原因是需要为TPU集成这些新的压缩支持。,https://github.com/vllm-project/vllm/issues/9301
vllm,这个issue是一个特性需求，主要涉及到vLLM中fine-grained CustomOp启用机制的实现。原因可能是为了提高代码质量和改进代码审查流程。,https://github.com/vllm-project/vllm/issues/9300
vllm,该issue类型为新特性需求，主要涉及的对象是支持Mamba2模型。这个需求是由于要添加对Mamba2的支持，需要进行集成测试、支持分块填充、修复张量并行性等工作。,https://github.com/vllm-project/vllm/issues/9292
vllm,这是一个关于提出需求的issue，主要涉及PrefillDecode与Speculative Decoder的整合，由于需要支持chunk prefill和spec decoding。,https://github.com/vllm-project/vllm/issues/9291
vllm,这个issue类型是需求提出，主要对象是实现vLLM V1功能。这个需求由于项目开发需要，要求实现vLLM V1功能。,https://github.com/vllm-project/vllm/issues/9289
vllm,这是一个文档更新类型的issue，涉及的主要对象是软件的功能模块。导致此issue的原因是过时的注释可能导致误解。,https://github.com/vllm-project/vllm/issues/9287
vllm,这是一个功能需求报告，涉及到添加HPU手动种子设置到`seed_everything`方法中。,https://github.com/vllm-project/vllm/issues/9281
vllm,这个issue是一个用户提出需求的类型，主要对象是vllm中LoRA模型的权重加载功能。由于LoRA权重保存格式不支持正则表达式形式，导致无法加载`target_modules`。,https://github.com/vllm-project/vllm/issues/9275
vllm,这是一个用户提出需求的issue，主要涉及VLLM推理引擎的人工增加推理时间的问题。用户想通过手动方式延长Qwen2.5在服务器上的推理时间，并受到Noam Brown关于推理时间缩放的启发。,https://github.com/vllm-project/vllm/issues/9274
vllm,这个issue类型是用户提出需求，主要对象是VLLM 0.6.2版本的vllm-flash-attn模块，可能是因为用户想确认是否需要单独安装flash-attn。,https://github.com/vllm-project/vllm/issues/9273
vllm,这是一个用户提出需求的issue，主要涉及的对象是benchark_throughput脚本，用户希望暴露max_num_seqs参数给该脚本。,https://github.com/vllm-project/vllm/issues/9271
vllm,该issue属于用户提出需求类型，主要涉及实现设备无关性以支持多样化硬件的问题，原因是当前针对不同硬件设备需要实现独立的Worker/Executor/Model Runner框架，导致代码冗余和维护困难。,https://github.com/vllm-project/vllm/issues/9268
vllm,这个issue类型是代码优化，主要涉及mypy类型检查，由于代码扩展导致部分错误的问题。,https://github.com/vllm-project/vllm/issues/9267
vllm,这是一个用户提出需求的issue，主要涉及改进logging for embedding models，由于当前的logging metrics不够native导致缺少KV cache和generation tokens的问题。,https://github.com/vllm-project/vllm/issues/9265
vllm,该issue属于用户提出需求类型，主要涉及的对象是持续集成 (CI) 构建流程。这个问题是因为测试在开发期间意外运行了几次，需要在代码中添加注释以明确只用于测试个别模型，不应合并到主分支测试。,https://github.com/vllm-project/vllm/issues/9262
vllm,"这是一个用户提交需求的问题单，主要涉及到隐藏""best_of""功能，用户提出希望代表提交的需求。",https://github.com/vllm-project/vllm/issues/9261
vllm,该issue属于需求提出类型，涉及主要对象为Mergify自动标记PRs的配置。由于目前需要更多路径以实现更全面的标记，故用户提出了此需求。,https://github.com/vllm-project/vllm/issues/9259
vllm,这是一个关于编译器通用修饰器的建议类型的issue，主要涉及的对象是torch.compile模块。由于缺乏通用修饰器功能，可能导致用户在编译器的使用中遇到一些限制或不便。,https://github.com/vllm-project/vllm/issues/9258
vllm,这个issue属于用户提出需求，并主要涉及改进快速入门文档。原因可能是现有文档在系统要求、安装步骤、术语解释等方面不够清晰，导致新用户在快速生成项目时遇到困难。,https://github.com/vllm-project/vllm/issues/9256
vllm,这是一个用户提出需求的类型。该问题单涉及的主要对象是FlashAttentionBackend和paged_attention_kernel。原因是用户不清楚FlashAttentionBackend如何与paged attention相关联，导致提出了关于两者之间关系的问题。,https://github.com/vllm-project/vllm/issues/9247
vllm,这个issue是一个feature请求，涉及的主要对象是为vllm添加GLM-4v支持。由于GLM4V部署报告KeyError: 'transformer.vision.transformer.layers.45.mlp.fc2.weight'，需要修复此问题。,https://github.com/vllm-project/vllm/issues/9242
vllm,这个issue类型是改进型，主要对象是模型支持信息的收集过程。由于之前的改进未统一在一个进程中收集模型支持信息，导致需要进行后续跟进来优化这一过程。,https://github.com/vllm-project/vllm/issues/9233
vllm,这是一个用户提出的需求，主要对象是希望在AARCH64架构上启用vLLM，以支持ARM处理器，旨在拓展vLLM在不同应用场景中的可用性。,https://github.com/vllm-project/vllm/issues/9228
vllm,这是一个需求提议类型的issue，主要涉及构建自定义操作（custom ops）同时支持CPU和CUDA，用户提出了需要根据张量设备进行分发的建议。,https://github.com/vllm-project/vllm/issues/9224
vllm,该issue属于功能改进类型，主要涉及的对象是VLM的vision encoder。原因是为了维护一致性，在传递`QuantizationConfig`和关联的`prefix`参数给vision towers时，添加了注释忽略了尚未通过现有方法进行量化的vision tower。,https://github.com/vllm-project/vllm/issues/9217
vllm,这个issue类型是代码贡献（pull request）的请求，主要对象是核心组件（core components），由于代码的重要性需要仔细审查，因此提出了给关键部分指定codeowners的建议。,https://github.com/vllm-project/vllm/issues/9210
vllm,这是一个关于功能需求的issue，主要涉及vLLM中简单的数据并行性，由于用户希望在单台机器上部署多个vLLM实例，导致需要实现数据并行处理。,https://github.com/vllm-project/vllm/issues/9206
vllm,该issue是关于文档改进的，主要涉及调试文档的提升。由于现有文档可读性较差，可能影响用户理解，因此需要对文档进行优化。,https://github.com/vllm-project/vllm/issues/9204
vllm,这是一个关于提出需求的RFC（Request for Comments）类型的issue，主要涉及到Shared Mixture of Experts。由于GPU内存需求的原因，提议添加一个新的Mixtraltype模型，并实现专家共享，以便在不同客户端的多个模型实例之间共享专家，以减少内存需求。,https://github.com/vllm-project/vllm/issues/9203
vllm,该issue是一个用户提出的需求，主要涉及添加一个helm chart示例，用于在Kubernetes集群上部署vllm。用户希望通过此示例实现一个自主部署的配置，包括在启动、可读性和健康检查方面的k8s探针，以确保模型完全加载并在健康检查返回200后将pod标记为运行状态。,https://github.com/vllm-project/vllm/issues/9199
vllm,这个issue类型是功能更新，该问题单涉及的主要对象是vllm的actions/setup-python工具。由于更新版本时涉及到node版本的更新和依赖项的更新，可能在使用中会遇到一些改动所带来的问题。,https://github.com/vllm-project/vllm/issues/9195
vllm,该issue类型为任务需求，涉及到mypy类型检查相关的优化。原因是需要解决`vllm/entrypoints`中的mypy警告，并在`format.sh`和CI中开启`mypy`检查。,https://github.com/vllm-project/vllm/issues/9194
vllm,这是一个需求提案类型的issue，主要涉及的对象是vLLM项目的PR自动标记。这个提案是由于项目活跃度高，PR数量庞大，希望自动标记PR以便根据个人兴趣和PR状态快速识别。,https://github.com/vllm-project/vllm/issues/9192
vllm,这个issue类型为功能改进，涉及主要对象为Mamba和Jamba kernels以及相关模型，用户提出了改进持续批处理的需求。,https://github.com/vllm-project/vllm/issues/9189
vllm,这是一个关于特性需求的issue，主要涉及到VLLM中视觉编码器的输出状态配置。这个需求是由于在使用siglip或clip作为多模态视觉编码器时，可能会出现不同情况需要处理输出状态的需求。,https://github.com/vllm-project/vllm/issues/9186
vllm,这个issue类型是需求增强，涉及支持prefill only模型的调度器的改进。原因是为了更高效地支持只填充（仅编码器）模型。,https://github.com/vllm-project/vllm/issues/9181
vllm,这是一个用户提出需求的Issue，涉及主要对象为将KServe部署指南的链接更新。,https://github.com/vllm-project/vllm/issues/9173
vllm,这个issue类型为用户提出需求，主要涉及到VLLM软件包，用户提出了为setup.py添加分类器的需求，以增加软件包在PyPI上的可发现性。,https://github.com/vllm-project/vllm/issues/9171
vllm,这是一个用户提出需求的issue，主要涉及的对象是支持自定义`max_mm_tokens`。由于当前环境信息的收集可能由于某些原因导致了该需求未被满足。,https://github.com/vllm-project/vllm/issues/9169
vllm,该issue为改进建议类型，主要涉及持续集成和测试程序，目的是通过使用虚拟加载数据来跳过模型下载和加载时间。,https://github.com/vllm-project/vllm/issues/9165
vllm,这是一个用户提出的功能需求，涉及优化支持更大批次大小的序列。由于advance_step.cu中对序列数量的限制导致无法支持更大批次，希望改进以提高性能。,https://github.com/vllm-project/vllm/issues/9164
vllm,这是一个更新类型的问题，主要涉及到Github仓库中的测试环境依赖的管理。这个问题的根本原因是在CI测试过程中未指定`lmeval`的确切版本，导致需要将其移到全局测试依赖项并锁定版本为`lmeval==0.4.4`。,https://github.com/vllm-project/vllm/issues/9161
vllm,这是一个功能需求的issue，主要讨论了vllm中新增的numschedulersteps参数的设置和使用方式，用户希望了解该参数的具体含义和设置指南，以及针对70B模型在2x A100上的特定情况下如何选择合适的数值范围。,https://github.com/vllm-project/vllm/issues/9158
vllm,这是一个文档更新类型的issue，主要涉及的对象是vllm项目下的vllm.rst文档。由于缺少视频示例，用户提出需要更新文档以包含有关视频的示例。,https://github.com/vllm-project/vllm/issues/9155
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加`JambaToolParser`以支持对ai21labs/AI21Jamba1.5Mini和ai21labs/AI21Jamba1.5Large的工具调用。,https://github.com/vllm-project/vllm/issues/9154
vllm,该issue属于用户提出需求类型，主要对象是vllm项目中的PR（Pull Request）8797。由于未得到审阅导致用户感到困扰，希望得到他人关注和审阅。,https://github.com/vllm-project/vllm/issues/9150
vllm,这个issue类型是需求提出，涉及主要对象是BlockSpaceManagerV1，由于该功能正在被BlockSpaceManagerV2取代，需要引入一个环境变量来明确设置以允许使用BlockSpaceManagerV1。,https://github.com/vllm-project/vllm/issues/9149
vllm,这是一个用户提出需求的issue，问题涉及到支持更多模型的bitsandbytes量化，在测试中出现了一些头尺寸不被整除导致的不支持情况。,https://github.com/vllm-project/vllm/issues/9148
vllm,这是一个需求类型的 issue，主要涉及改进用户体验。原因是用户希望能够点击链接。,https://github.com/vllm-project/vllm/issues/9147
vllm,这是一个用户提出需求的issue，主要涉及如何在vllm中保存日志到本地路径和控制日志速率的问题。由于缺乏相关参数，用户无法找到解决办法。,https://github.com/vllm-project/vllm/issues/9146
vllm,这是一个用户提出需求的issue，主要涉及项目的README文件。由于缺少具体内容，用户想在README文件中添加对Slack的说明。,https://github.com/vllm-project/vllm/issues/9137
vllm,这个issue类型为文档改进，并涉及到在README中包含性能基准测试。原因是缺少性能基准信息可能导致用户无法准确了解该项目的性能表现。,https://github.com/vllm-project/vllm/issues/9135
vllm,这是一个用户提出需求的issue，主要涉及在Vllm上运行多GPU和多节点的本地hugging face模型推理的问题，可能是由于集成模型和运行配置方面的问题导致。,https://github.com/vllm-project/vllm/issues/9134
vllm,该issue属于文档改进类型，主要涉及贡献者和安装文档，旨在改善文档组织和易读性，指向安装文档关键部分以便新开发者找到信息。,https://github.com/vllm-project/vllm/issues/9132
vllm,这是一个需求报告类型的issue，主要涉及到在推断时添加`mm_processor_kwargs`的支持。原因可能是为了在推断过程中能够提供定制的处理参数。,https://github.com/vllm-project/vllm/issues/9131
vllm,这个issue类型是用户提出需求，主要涉及docker image的更新，由于安全和包版本等原因，用户希望将vLLM docker image升级到更现代的操作系统版本。,https://github.com/vllm-project/vllm/issues/9130
vllm,这个issue类型是维护和改进任务，主要涉及CMake配置的清理和重构，目的是提高可读性和稳定性。,https://github.com/vllm-project/vllm/issues/9129
vllm,这是一个用户提出需求的issue，主要对象是获取推理指标并希望将其包含在API响应或日志中。由于当前环境未提供所需的推理度量指标，用户希望了解如何获取这些指标。,https://github.com/vllm-project/vllm/issues/9126
vllm,这个issue是关于提出需求的，主要对象是添加一个新的阿拉伯语模型，可能会出现处理token的方式不同的困难。,https://github.com/vllm-project/vllm/issues/9125
vllm,该issue是关于需求的，主要涉及的对象是支持prefill only models的attention实现。由于需要简化attention实现并支持单独启用双向模式，导致了提出这个问题。,https://github.com/vllm-project/vllm/issues/9124
vllm,这个issue类型是需求提出，主要对象是OpenVINO，用户需要明确说明使用2.4.0版本的torch，并使用最新的`optimum`和`optimumintel`来支持最新的transformer版本。,https://github.com/vllm-project/vllm/issues/9121
vllm,这是一个需求提议，主要涉及到在MQLLMEngine上添加对beam search API支持的功能。,https://github.com/vllm-project/vllm/issues/9117
vllm,这个issue是关于功能需求（Feature Request），主要涉及对象是vLLM是否支持ONNX模型。,https://github.com/vllm-project/vllm/issues/9112
vllm,这是一个功能需求的issue，涉及主要对象为VLLM引擎和ModelConfig，用户提出在本地运行VLLM服务器时缺少InferenceClient类的问题。,https://github.com/vllm-project/vllm/issues/9110
vllm,这个issue类型是需求支持，主要对象是要求支持Jetson AGX Orin。由于Jetson AGX Orin的硬件和软件特征与现有支持列表不匹配，用户请求添加对该设备的支持。,https://github.com/vllm-project/vllm/issues/9109
vllm,这个issue属于功能改进类型，主要涉及vLLM模型接口的明确化和支持OOT嵌入模型。原因是之前缺乏明确的接口规范，可能导致开发和重构过程中的混淆，现在通过新增`interface_base.py`来明确定义所需的方法，同时将嵌入模型自动检测放入测试中。,https://github.com/vllm-project/vllm/issues/9108
vllm,这是一个功能需求的issue，主要对象是vllm core中的beam search功能。由于beam search功能需要在vllm core之外重新实现，因此需要将其从vllm core中移除。,https://github.com/vllm-project/vllm/issues/9105
vllm,这个issue类型是功能需求提出，主要对象是torch.compile中的blocksparse attention，由于custom op只接受Python常量和张量，需要调整实现以支持此限制。,https://github.com/vllm-project/vllm/issues/9102
vllm,这个issue类型是在提出需求，主要涉及的对象是优化模型的forward computation逻辑。由于连续数据批处理和异构模型的复杂性，导致了attention层的复杂度难以处理，因此提出通过前向上下文来隐藏连续批处理的复杂性。,https://github.com/vllm-project/vllm/issues/9098
vllm,这是一个功能需求提出的issue，主要对象是VLLM项目中的Flash Infer模块，问题涉及到与torch.compile singlegraph capture的兼容性。,https://github.com/vllm-project/vllm/issues/9097
vllm,这是一个文档更新的issue，涉及更新README.md文件以包含有关Ray峰会幻灯片的信息。可能是由于markdown渲染问题，所以需要使用原始的HTML来呈现信息。,https://github.com/vllm-project/vllm/issues/9088
vllm,这个issue是关于功能需求的，主要涉及到前端API支持beam search。原因是为了让API服务器在不使用前端多进程的情况下运行更高层次的beam search。,https://github.com/vllm-project/vllm/issues/9087
vllm,这个issue是关于功能需求的，主要涉及Prefill-Decode separation 在vllm中的实现以及新增的InfiniteStore功能。导致该需求的原因是为了优化数据传输效率和管理KV缓存。,https://github.com/vllm-project/vllm/issues/9079
vllm,这是一个请求移除特定内容的issue，类型为用户提出需求，主要对象涉及到AMD Ray Summit Banner。,https://github.com/vllm-project/vllm/issues/9075
vllm,这是一个用户提出需求的issue，主要涉及需要理解在Q4路线图中对torch.compile的支持，可能由于缺乏对该功能具体支持需求的理解而提出帮助请求。,https://github.com/vllm-project/vllm/issues/9072
vllm,这是一个用户提出需求的issue，主要涉及到如何在多模态语言模型中截断输入时避免图像特征与相应的图像标记不对齐的问题。,https://github.com/vllm-project/vllm/issues/9067
vllm,这是一个用户提出需求的类型，主要对象是vllm的中文文档翻译。由于缺乏回应，用户提出如何贡献中文文档翻译的建议。,https://github.com/vllm-project/vllm/issues/9065
vllm,这是一个代码优化类型的issue，主要涉及到 `vllm.model_executor.models` 的命名空间问题，由于需要避免污染该命名空间，因此将模型注册表等相关代码移动到了单独的 `registry.py` 文件中。,https://github.com/vllm-project/vllm/issues/9064
vllm,这是一个功能需求类型的issue，主要涉及torch.compile和自定义allreduce的集成问题，由于pytorch目前不支持在动态图中使用自定义操作，导致了无法隐藏运行时大小检查和内存复制或者计算等方面的问题。,https://github.com/vllm-project/vllm/issues/9061
vllm,这个issue是一个[Misc]类型的需求报告，主要涉及LoRA模块以及请求排序逻辑的修改，导致需要支持chunked prefill，并保证prefill序列始终在解码序列之前。,https://github.com/vllm-project/vllm/issues/9057
vllm,这个issue是关于功能需求的，主要涉及到VLLM项目中的多步输出流。,https://github.com/vllm-project/vllm/issues/9047
vllm,这是一个用户提出需求类型的issue，主要涉及到vLLM v0.6.2缺少CUDA 11.8二进制文件的问题。由于缺少相应的CUDA 11.8 wheels，用户希望开发者发布与最新版本对应的CUDA 11.8二进制文件。,https://github.com/vllm-project/vllm/issues/9043
vllm,"这是一个用户提出需求的issue，主要对象是希望支持的新模型""Nvidia/NVLM-D-72B""。原因可能是该模型目前尚未得到支持，用户希望得到支持和解决问题。",https://github.com/vllm-project/vllm/issues/9041
vllm,这个issue类型为需求提出，主要涉及的对象是支持新模型NVLM 1.0，由于该模型具有新架构，需要时间来在vLLM中实现支持。,https://github.com/vllm-project/vllm/issues/9040
vllm,这个issue类型是用户提出需求，涉及的主要对象是增加对llama32在rocm vllm中的支持，原因可能是用户需要在rocm vllm中使用llama32相关的命令。,https://github.com/vllm-project/vllm/issues/9035
vllm,这是一个用户提出需求的类型。该问题单涉及的主要对象是在 vllm 项目中的注意力机制。由于缺少前向上下文，导致在该项目中无法正确实现某个功能。,https://github.com/vllm-project/vllm/issues/9029
vllm,"这是一个功能请求，该问题单涉及的主要对象是 ""ibmgranite/granite3.08binstruct""。",https://github.com/vllm-project/vllm/issues/9027
vllm,这是一个文档更新类型的issue，主要涉及的对象是Granite模型文档。,https://github.com/vllm-project/vllm/issues/9025
vllm,这是一个需求类型的issue，主要涉及ShareGPT数据集的输入和输出序列长度默认值问题。由于benchmark_serving.py未为ShareGPT数据集提供inputlen和outputlen值，导致用户想了解该数据集的默认数值及如何在vllm中使用该数据集。,https://github.com/vllm-project/vllm/issues/9009
vllm,这个issue是一个需求报告，主要涉及vLLM Roadmap Q4 2024的规划内容。由于缺乏对`transformers`后端支持等功能，用户提出了一些功能增强和硬件支持方面的需求。,https://github.com/vllm-project/vllm/issues/9006
vllm,这是一个功能需求提议，主要涉及 Torch 的编译过程，并需要添加日志记录峰值内存使用情况。,https://github.com/vllm-project/vllm/issues/9003
vllm,这个issue是关于功能改进的，主要对象是VLLM模型的接口和自动检测功能。因为更新了模型注册表，现在不需要将几乎每个模型硬编码到支持PP列表中，这个问题主要解决了反复添加模型的繁琐工作。,https://github.com/vllm-project/vllm/issues/9000
vllm,这个issue属于用户提出需求类型，主要涉及vllm的RISC-V支持，可能由于用户当前环境下无法安装vllm而提出该问题。,https://github.com/vllm-project/vllm/issues/8996
vllm,这是一个用户提出的需求类型的issue，主要涉及到从Jamba中分离出Mamba以提供更广泛的支持，并为将来将Mamba缓存机制整合到vLLM引擎中奠定基础。,https://github.com/vllm-project/vllm/issues/8993
vllm,"这个issue是一个文档更新类型的问题，涉及的主要对象是vllm中的支持模型列表。由于缺少部分模型信息，导致用户提出了需要添加DeepSeek, DeepSeekV2和Granite模型的需求。",https://github.com/vllm-project/vllm/issues/8987
vllm,这是一个需求类issue，主要对象是Vllm中的规范解码功能。由于单个GPU的内存和计算限制，需要在多个GPU上对草稿模型进行分片，以便处理更大的模型。,https://github.com/vllm-project/vllm/issues/8981
vllm,这是一个功能需求，主要涉及VLLM在多个GPU上划分draft模型的问题，用户提出需要支持draft模型在多个GPU上分片，以解决单个GPU内存和计算限制导致模型大小受限的问题。,https://github.com/vllm-project/vllm/issues/8980
vllm,这个issue是一个功能增强提议，涉及到Zero point support的添加和AWQ Fused MoE的支持，可能由于需要在MarlinMoE kernel上实现zero point support以及对AWQ的支持而产生。,https://github.com/vllm-project/vllm/issues/8973
vllm,这是一个用户提出需求的issue，主要涉及DummyModelLoader，提出了添加process_weights_after_loading功能的需求。,https://github.com/vllm-project/vllm/issues/8969
vllm,该issue是一个功能请求，主要涉及的对象是Reward Modelling。由于当前端点无法满足奖励模型的需求，导致需要通过一种不太理想的方式来与OpenAI兼容服务器一起使用。,https://github.com/vllm-project/vllm/issues/8967
vllm,这个issue是关于文档中的建议和意见，主要涉及的对象是示例代码中的数据分批处理方法。用户提出了由于需要连续的批处理而使用ray.remote()替代ds.map_batches()的建议。,https://github.com/vllm-project/vllm/issues/8966
vllm,这是一个需求提出的issue，主要涉及支持BERT模型的代码组织和prefill only models相关的功能开发。原因是为了支持prefill only models的特定需求，需要对模块和功能进行相应的调整和添加。,https://github.com/vllm-project/vllm/issues/8964
vllm,这是一个用户提出的需求类型的issue，主要涉及的对象是硬件相关的代码。导致这个需求的原因是为了使设备在不同硬件上都能通用。,https://github.com/vllm-project/vllm/issues/8961
vllm,这是一个用户提出需求的类型，主要涉及max_position_embeddings参数的调整，原因是为了LoRA的兼容性。,https://github.com/vllm-project/vllm/issues/8957
vllm,这个issue类型为功能建议，主要涉及 torch.compile 对象支持哪些模型和功能，以及需要的性能测试和优化。,https://github.com/vllm-project/vllm/issues/8949
vllm,这是一个关于用户提出需求的问题，主要涉及到vLLM能否实现使用不同批次进行前缀缓存，可能是由于用户想要在不同批次下使用vLLM进行前缀缓存而提出的。,https://github.com/vllm-project/vllm/issues/8939
vllm,这是一个优化建议，主要涉及到提高hash_of_block函数的计算速度，由于当前时间复杂度为O(n^2)，希望通过优化到O(n)来提高效率。,https://github.com/vllm-project/vllm/issues/8935
vllm,这是一个关于硬件后端退役策略的需求报告，主要涉及到vLLM对不同硬件后端支持的问题。由于vLLM团队要求硬件供应商跟进PyTorch更新，未能支持最新PyTorch版本的硬件后端将被删除。,https://github.com/vllm-project/vllm/issues/8932
vllm,这个issue是一个需求提出类型的问题，主要涉及到安装文档的组织和提交记录的docker。原因可能是用户希望更清晰地了解安装过程和每个提交的docker信息。,https://github.com/vllm-project/vllm/issues/8931
vllm,这是一个用户提出需求的issue，主要涉及获取logits而不是logprobs用于教师模型离线蒸馏，可能是出于加快速度的考虑。,https://github.com/vllm-project/vllm/issues/8926
vllm,这是一个需求更改的issue，主要涉及benchmark_throughput.py脚本的修改，用户添加了`maxnumseqs`参数作为可选参数。原因是需要将输出标记和总标记分开。,https://github.com/vllm-project/vllm/issues/8914
vllm,该issue属于用户提出需求，主要涉及QuantizationConfig和QuantizeMethodBase的重构，旨在简化内核集成，由于硬件和内核多样性增加导致紧密耦合的问题。,https://github.com/vllm-project/vllm/issues/8913
vllm,这是一个用户提出需要的需求问题，涉及到需要跳过P2P检查的功能。用户希望能够通过设置环境变量来直接信任驱动程序，从而减少检查的时间。,https://github.com/vllm-project/vllm/issues/8911
vllm,这是一个功能需求类型的issue，主要涉及的对象是FlashAttention backend，由于之前限制了特定的大小，而现在原生的FA内核支持任意大小的头尺寸，导致需要支持所有头尺寸最多到256的问题。,https://github.com/vllm-project/vllm/issues/8910
vllm,这个issue类型是功能需求提议，主要涉及对象是对于checkpoint定义中直接使用压缩张量。原因可能是希望重复使用压缩和量化配置实现。,https://github.com/vllm-project/vllm/issues/8909
vllm,这个issue是用户需求类型，主要涉及使用vllm中的guided_regex功能在离线模式下的问题，需求是在本地环境中使用guided_regex而非通过服务器。,https://github.com/vllm-project/vllm/issues/8907
vllm,该issue类型为用户提出需求，主要涉及的对象是在VLLM中生成FSM的过程。由于生成复杂JSON模式的FSM过程耗时长，用户希望添加一个特性来保存生成的模式，并在重新部署时重新加载，以节省时间。,https://github.com/vllm-project/vllm/issues/8902
vllm,这是一个性能优化类型的 Issue，主要涉及到LLMEngine中的SamplingParams对象。通过移除对SamplingParams的防御性拷贝，在某些场景下可以提高性能，问题提出者想要了解是否理解准确。,https://github.com/vllm-project/vllm/issues/8901
vllm,"这个issue是关于构建流程的优化，主要涉及到Dockerfile、pyproject.toml和构建脚本，并因为使用过程中出现了""dirty""repo问题而提出优化需求。",https://github.com/vllm-project/vllm/issues/8900
vllm,这是一个性能优化类型的issue，主要涉及到在前端中利用`recv_multipart`的非阻塞和异常特性以减少每个请求中的`poll`操作。原因是在优化前每个`poll`操作最少耗时10微秒。,https://github.com/vllm-project/vllm/issues/8882
vllm,"这个issue是一个[增强功能]类型的报告，涉及到vllm中pallas.py文件的更新以支持trillium，用户提出此需求是因为trillium不支持""lite""加速类型但也不使用megacore，需要对代码进行调整以支持这一新的硬件加速器。",https://github.com/vllm-project/vllm/issues/8871
vllm,该issue类型为需求提出，主要涉及对聊天模板添加模型上下文信息的功能。由于某些模型对给定提示非常敏感，用户希望能够检测聊天模板中使用的模型，并相应调整输入。,https://github.com/vllm-project/vllm/issues/8869
vllm,"这是一个功能请求的issue，涉及主要对象是""qwen2vl""模块。由于无法仅仅依赖图像嵌入来生成新的prompt_token_ids，导致了难以实现此功能的困难。",https://github.com/vllm-project/vllm/issues/8857
vllm,这是一个关于添加新模型或改进现有模型的问题，主要涉及支持将图像嵌入作为qwen2vl输入。这个问题可能由于markdown格式错误导致描述内容未正确显示。,https://github.com/vllm-project/vllm/issues/8856
vllm,这是一个关于需求的问题，主要对象是需要支持多模态模型系统消息。这个问题的原因是用户希望在文本查询和/或图像中使用多模态模型，但当前系统消息不兼容此操作。,https://github.com/vllm-project/vllm/issues/8854
vllm,这是一个优化和性能改进的issue，主要涉及编译和构建过程，目的是通过在文件级别设置所需的计算能力来减小构建时间和减小wheel文件大小。,https://github.com/vllm-project/vllm/issues/8845
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM模型在TPUs上编译模型图的支持。该问题想知道是否有办法获取或下载已编译好的LLM模型图，以避免在TPU VMs上耗费时间进行图编译。,https://github.com/vllm-project/vllm/issues/8843
vllm,这是一个用户提出需求的 issue，主要涉及到希望控制抽样器执行顺序的功能请求。,https://github.com/vllm-project/vllm/issues/8835
vllm,该issue类型为需求提出，主要涉及到Llama3.2 Vision Model的功能支持。由于性能尚待优化，用户正在寻求关于如何更好支持该模型的帮助。,https://github.com/vllm-project/vllm/issues/8826
vllm,这是一个功能改进（Feature Improvement）类型的issue，主要涉及Python多进程方法的选择。由于历史遗留代码可能存在问题，需要改善现有选择机制以避免潜在的错误。,https://github.com/vllm-project/vllm/issues/8823
vllm,这是一个用户提出需求的类型的issue，主要涉及vllm与dynamo的兼容性问题。由于需要在pytorch的编译过程中对整数进行特殊处理，用户寻求帮助以解决每次运行模型都会触发重新编译的问题。,https://github.com/vllm-project/vllm/issues/8821
vllm,这是一个用户提出需求的类型，主要涉及的对象是该项目的安装流程。这个需求提出了让那些只想更改Python文件的用户更容易构建项目的建议。,https://github.com/vllm-project/vllm/issues/8818
vllm,这是一个文档更新的Issue，涉及更新Transformers 4.45版本的文档内容。由于Markdown渲染出现问题，需要使用原始的HTML代码。,https://github.com/vllm-project/vllm/issues/8817
vllm,该issue属于用户提出需求类型，主要涉及如何在vllm中使用BitsAndBytesConfig以及其他quantization参数的配置问题。用户想要运行特定模型的推理，但不清楚如何与vllm集成。,https://github.com/vllm-project/vllm/issues/8813
vllm,这是一个用户提出需求的issue，主要涉及升级版本至Llama 3.2的支持。原因是Llama 3.2是一种不同于以前的多模型Llama模型的架构。,https://github.com/vllm-project/vllm/issues/8812
vllm,这个issue属于添加新功能的类型，主要涉及的对象是Llama 3.2模型。原因是受阻于HuggingFace Transformers库的发布，导致需要等待相关PR合并才能进行新版本发布。,https://github.com/vllm-project/vllm/issues/8811
vllm,这是一个用户提出需求的issue，主要涉及的对象是希望支持新模型allenai/Molmo-7B-0-0924 VisionLM。其原因是该模型包含视觉信息，而之前的模型不包含视觉信息。,https://github.com/vllm-project/vllm/issues/8808
vllm,这是一个用户需求问题，主要涉及如何在vllm中训练已冻结的模型，由于vllm不支持使用已加载的模型进行训练，用户希望了解是否可以反过来使用vllm已加载的模型来传递给训练器。,https://github.com/vllm-project/vllm/issues/8806
vllm,这个issue类型是一个改进提案，主要涉及到vLLM的Core模块。由于前述功能组合的支持尚未实现，需要进一步完善。,https://github.com/vllm-project/vllm/issues/8804
vllm,这个issue类型为用户提出需求，主要涉及的对象是Pixtral模型。由于用户需要在Pixtral模型中添加LoRA适配器，以支持其在HF版本上进行finetuning，并请求在未来版本中添加该功能。,https://github.com/vllm-project/vllm/issues/8802
vllm,这是一个用户提出需求的类型，主要涉及Marlin MoE内核中动态组块的问题，由于需要性能调查，提议将组块数量作为参数传递给内核，以减小文件大小和编译时间。,https://github.com/vllm-project/vllm/issues/8801
vllm,这个issue类型是一个功能改进请求，主要涉及的对象是Marlin MoE内核，由于当前无法直接使用BFloat16输入类型，需要在GPTQ代码路径中执行类型转换。,https://github.com/vllm-project/vllm/issues/8800
vllm,这是一个需求提出类型的issue，主要涉及到CI（持续集成）的配置，请求添加CODEOWNERS以及将prefill拆分成单独的测试。,https://github.com/vllm-project/vllm/issues/8795
vllm,这是一个用户提出需求的 issue，主要涉及 vLLM 项目中使用本地存储的模型路径时遇到的错误。用户在尝试加载本地存储的 finetuned model 时遇到 wrong_path 错误，尽管路径是正确的。,https://github.com/vllm-project/vllm/issues/8794
vllm,这是一个功能需求的issue，主要涉及VLLM项目中添加RWKV v5 (Eagle)支持的任务。,https://github.com/vllm-project/vllm/issues/8787
vllm,这是一个用户需求类型的issue，主要涉及到FlashInfer backend的改进。原因是为了集成新的FlashInfer内核，并在FlashInfer后端中实现更多vLLM功能。,https://github.com/vllm-project/vllm/issues/8786
vllm,"这个issue是一个功能请求，主要涉及到添加一个名为""Goodput""的用户定义指标，用户提出了从GenAI服务用户角度衡量服务的需求。",https://github.com/vllm-project/vllm/issues/8782
vllm,这是一个特性需求，涉及到将静态项目元数据从`setup.py`迁移到`pyproject.toml`，旨在提高静态元数据管理的效率。,https://github.com/vllm-project/vllm/issues/8772
vllm,这个issue类型为升级请求，主要涉及的对象是最新版本的软件库`bitsandbytes`，由于旧版本的4bit模型不支持CUDA图，导致用户提出升级请求。,https://github.com/vllm-project/vllm/issues/8768
vllm,这个issue类型是用户提出需求，用户需要获取vllm中的logits信息。这是因为用户想要在运行benchmark_throughput.py时执行softmax优化，但发现vllm的输出中并没有logits信息。,https://github.com/vllm-project/vllm/issues/8762
vllm,这是关于功能增强或新功能的 issue ，主要对象是 MQLLMEngine。,https://github.com/vllm-project/vllm/issues/8761
vllm,该issue类型是一个功能修改，主要涉及对象是`PromptInputs`、`PromptType`、`inputs`和`prompt`，发起修改的原因是为了重命名这些对象，同时保留了对之前命名的向后兼容性。,https://github.com/vllm-project/vllm/issues/8760
vllm,该issue属于用户提出需求并请教问题的类型，主要涉及到vllm的dequantization功能。用户不理解为什么在去量化过程中需要减去1024或64然后再加回去。,https://github.com/vllm-project/vllm/issues/8759
vllm,这是一个feature请求，涉及vLLM中的自定义浮点运行时量化。,https://github.com/vllm-project/vllm/issues/8751
vllm,这是一个特性需求的issue，主要涉及对vllm中的`mm_processor_kwargs`进行推理覆盖支持的功能。,https://github.com/vllm-project/vllm/issues/8742
vllm,这是一个需求类型的issue，主要涉及添加 LlamaForSequenceClassification 模型到 vLLM。由于使用`last_hidden_state`在`compute_logits`中导致了形状不匹配的问题。,https://github.com/vllm-project/vllm/issues/8740
vllm,这是一个增加模型功能的issue，主要涉及的对象是vLLM框架下的LlamaForSequenceClassification模型。由于返回`last_hidden_state`引起了形状不匹配问题，需要解决这个bug并补充文档、添加测试。,https://github.com/vllm-project/vllm/issues/8739
vllm,这是关于使用 FusedMoE 模型的一个需求指引类型的 issue，主要涉及到 deepseek 模型使用问题。,https://github.com/vllm-project/vllm/issues/8737
vllm,这是一个需求提出类型的issue，主要涉及到依赖管理工具dependabot。由于尚未配置所有依赖，导致目前仅与GitHub Actions有关，用户希望启用dependabot管理已知漏洞。,https://github.com/vllm-project/vllm/issues/8734
vllm,这是一个用户提出需求的issue，主要涉及的对象是为VLLM添加支持加载本地文件（图片和视频）的多模态功能。由于当前环境中PyTorch版本为2.4.0，CUDA版本为12.0，但在加载本地文件时遇到了问题。,https://github.com/vllm-project/vllm/issues/8730
vllm,这个issue是关于[Hardware][CPU] Refactor CPU model runner的类型为改进代码结构，主要涉及到CPU模型运行程序，由于需要使用其他模块来重构`cpu_model_runner.py`，所以提交了该问题。,https://github.com/vllm-project/vllm/issues/8729
vllm,这是一个用户提交的需求类型的issue，主要涉及到重新实现vllm核心上的beam search。由于缺少具体信息，无法确定提交者或具体需求。,https://github.com/vllm-project/vllm/issues/8726
vllm,这是一个用户提出需求的issue，主要涉及VLLM支持本地LORA路径而非Huggingface的LORA路径，可能出现的原因是相关文档信息不清晰导致用户困惑。,https://github.com/vllm-project/vllm/issues/8725
vllm,这是一个用户需求问题，主要涉及的对象是如何加载使用8位二进制量化的模型。用户想知道如何改变代码以将模型加载为8位而不是4位。,https://github.com/vllm-project/vllm/issues/8720
vllm,这个issue是关于提出需求的类型，主要涉及VLLM（Very Large Language Model）的量化和性能优化。它提出了对ALPINDALE模型实现量化的改进方案，并寻求关于设计方面的反馈。,https://github.com/vllm-project/vllm/issues/8716
vllm,这是一个用户需求类型的issue，需求是针对构建设置的调整。,https://github.com/vllm-project/vllm/issues/8713
vllm,这个issue类型是用户提出需求，主要对象是优化vllm中的Prefix Caching triton kernels编译过程，避免在首次运行时导致e2e性能下降。,https://github.com/vllm-project/vllm/issues/8712
vllm,这是一个关于功能使用的问题，涉及到max_tokens和max_model_len参数的差异，用户想知道它们之间是否有区别。,https://github.com/vllm-project/vllm/issues/8706
vllm,这个issue属于需求提出类型，主要对象是项目中的`CudaMemoryProfiler`，由于命名不够准确可能导致混淆，用户提出应将其重命名为`DeviceMemoryProfiler`。,https://github.com/vllm-project/vllm/issues/8703
vllm,这个issue类型为代码优化类型，主要涉及到的对象是bonus token逻辑。由于之前的bonus token逻辑已经过时，PR的目的是清理并移除这部分逻辑。,https://github.com/vllm-project/vllm/issues/8701
vllm,这个issue类型是用户提出需求，主要涉及添加对序列分类模型的支持，由于需要建立高质量的合成数据流水线、验证模型推理和多代理系统等需求，所以用户提出了新增对Skywork/SkyworkRewardLlama3.18B等序列分类模型的支持。,https://github.com/vllm-project/vllm/issues/8700
vllm,这是一个功能需求的issue，主要涉及vLLM的Memory Tiering功能。由于需求增加了Context Caching with TTL支持、Blockwise swapping for DRAM and Disk、Layered transfer between DRAM and HBM等功能，需要进行相关功能的开发和实现。,https://github.com/vllm-project/vllm/issues/8694
vllm,这个issue类型为功能需求变更，主要涉及的对象是输入数据类型的重命名。由于代码中存在一些命名不规范或者过时的输入数据类型，需要进行相应的更名以提高代码的可读性和维护性。,https://github.com/vllm-project/vllm/issues/8688
vllm,这是一个功能优化的issue，主要涉及VLM的更新和优化，包括使用新的构造函数和处理BLIP dummy sequence data。,https://github.com/vllm-project/vllm/issues/8687
vllm,这个issue属于功能需求反馈类型，主要涉及了vLLM模型对HF版本的Pixtral模型提供支持的问题。由于HF的Pixtral模型使用了不同的格式，导致需要调整vLLM的支持来满足新模型，以便实现模型的量化功能。,https://github.com/vllm-project/vllm/issues/8685
vllm,这是一个需求反馈类型的issue，主要对象是beam search功能，用户提出需要添加输出以手动检查正确性。,https://github.com/vllm-project/vllm/issues/8684
vllm,这个issue是一个功能需求，主要对象是改进分布式后端选择。出现这个问题的原因是在初始化CUDA上下文时，使用基于`fork`的多进程方法无法正常工作。,https://github.com/vllm-project/vllm/issues/8683
vllm,这是一个用户提出需求的类型，涉及主要对象为系统路径指定。原因可能是安装过程强制要求git克隆CUTLASS，而用户已经在系统中编译了CUTLASS，希望能够在安装时指定CUTLASS路径。,https://github.com/vllm-project/vllm/issues/8679
vllm,这是一个用户提出需求的issue，主要涉及将BlockSpaceManagerV2设置为默认的BlockManager。由于要逐步废弃BlockManager V1，因此用户提出希望在此过程中将BlockSpaceManagerV2设为默认选项。,https://github.com/vllm-project/vllm/issues/8678
vllm,这是一个功能需求类型的issue，主要涉及的对象是`SequenceData`和`Sequence`类。由于代码重复和缓存处理不当，导致需要添加新的构造函数以及更改属性的缓存机制。,https://github.com/vllm-project/vllm/issues/8675
vllm,这是一个类型为功能需求变更的issue，主要涉及对象是代码中的`PromptInputs`和`inputs`，由于与`LLMInputs`的混淆导致需要对命名进行调整。,https://github.com/vllm-project/vllm/issues/8673
vllm,该issue类型是一个为提出需求，主要涉及的对象是OpenAI API server的FastAPI middleware层，用户寻求了关于在FastAPI middleware层中传播使用计算的支持，以便跟踪请求活动。,https://github.com/vllm-project/vllm/issues/8672
vllm,这是一个文档更新类型的issue，主要涉及到neuron的文档内容。由于文档不够清晰或者有小的错漏，导致需要进行一些澄清性的修改。,https://github.com/vllm-project/vllm/issues/8671
vllm,该issue类型为功能需求，主要涉及到vllm不支持外部树形多模态模型的问题，由于vllm版本>=0.6不支持此功能。,https://github.com/vllm-project/vllm/issues/8667
vllm,这是一个用户提出需求的issue，主要涉及vLLM如何使用generation_config.json作为默认生成配置的问题。原因是当前配置与generation_config.json不一致，用户想了解是否可以让vLLM默认使用generation_config.json。,https://github.com/vllm-project/vllm/issues/8664
vllm,这个issue属于优化类型，主要对象是Marlin MoE kernels的编译过程。由于将Marlin MoE kernels拆分为多个文件，以实现更快的并行编译，从而显著减少了编译时间。,https://github.com/vllm-project/vllm/issues/8661
vllm,这是一个用户提出需求的issue，主要涉及多模态处理器kwargs参数的支持。问题是为了能够在初始化时传递processors_kwargs来覆盖数值，特别是在多模态模型中提供有效的processor_kwargs，并提供简洁的实现方式。,https://github.com/vllm-project/vllm/issues/8657
vllm,这个issue是性能优化建议，主要对象是在使用cutlass的情况下默认采用per_token量化为fp8，以提高性能。,https://github.com/vllm-project/vllm/issues/8651
vllm,这是一个功能需求提案，主要涉及对象是`collect_env.py`，用户想要显示AMD GPU的拓扑结构。,https://github.com/vllm-project/vllm/issues/8649
vllm,这个issue是关于[Frontend]批量推断功能的改进。,https://github.com/vllm-project/vllm/issues/8648
vllm,这是一个需求提出的issue，主要涉及前端的改进，即为`llm.chat()` API添加批量推断功能。因为目前API仅支持单个对话进行推断。,https://github.com/vllm-project/vllm/issues/8647
vllm,这是一个性能优化相关的issue，主要涉及到代码中的tl.atomic_add导致MI300性能严重下降的问题。,https://github.com/vllm-project/vllm/issues/8646
vllm,这是一个关于优化性能的issue，涉及vLLM中CUDA图的使用问题。原因是多步骤和分块预取在`main`上的使用不够充分，导致性能不佳。,https://github.com/vllm-project/vllm/issues/8645
vllm,"该issue类型为功能需求，主要对象是项目的安全性。由于项目缺乏安全指引，用户建议创建一个名为""SECURITY.md""的文件以提高项目的安全性。",https://github.com/vllm-project/vllm/issues/8642
vllm,这是一个用户提出需求的issue，主要涉及vLLM与Ray结合使用的问题，用户想要利用Ray进行离线批量推断。,https://github.com/vllm-project/vllm/issues/8636
vllm,这是一个用户提出需求的issue，主要对象是针对OpenAI o1模型的Chain-of-thought（CoT）推理工作流的功能和改进提案。,https://github.com/vllm-project/vllm/issues/8633
vllm,这是一个提出需求的issue，主要对象是希望在本地模型上使用OpenAI Python SDK进行在线推理，可能由于现有实现不兼容导致相关错误。,https://github.com/vllm-project/vllm/issues/8631
vllm,这是一个功能需求类型的issue，主要涉及如何获得给定输入和输出的logps。用户提出了关于如何获取给定输入和输出logps的问题。,https://github.com/vllm-project/vllm/issues/8622
vllm,这是一个关于代码优化的issue，主要涉及的对象是VLLM的logits resort操作，由于原代码实现复杂，用户提出优化简化其过程。,https://github.com/vllm-project/vllm/issues/8619
vllm,这个issue类型是文档修复和改进，主要涉及GGUF量化文档的增加。用户提出了关于GGUF模型使用docker、如何使用openai兼容api运行GGUF模型以及运行GGUF模型需要模板的问题。,https://github.com/vllm-project/vllm/issues/8618
vllm,这是一个用户提出需求的issue，主要涉及到vllm的在线推理服务。导致这个问题的原因是用户想要获取当前队列中的请求数量或正在处理中的请求数量。,https://github.com/vllm-project/vllm/issues/8617
vllm,这个issue属于用户需求类型，主要涉及vllm的性能测试，用户想了解官方网站是否提供性能测试信息。可能由于用户对vllm的性能表现感兴趣，希望了解官方是否有相关数据可供参考。,https://github.com/vllm-project/vllm/issues/8610
vllm,这是一个用户提出需求的issue，主要涉及前端部分的代码。,https://github.com/vllm-project/vllm/issues/8584
vllm,这是一个关于需求的问题，主要涉及VLLM在多个TPU主机上的运行问题。由于VLLM未能正确检测和利用所有4个主机，导致用户无法在更大模型上运行。,https://github.com/vllm-project/vllm/issues/8582
vllm,这个issue是一个关于提出需求的类型，主要涉及到改进sampling功能。由于现有的samplers在小模型中存在重复问题，需要一种能够完全避免重复的新的sampler，以提高结果的一致性。,https://github.com/vllm-project/vllm/issues/8581
vllm,这个issue是关于功能改进的提议， 主要涉及到多进程方法的默认设置。由于`fork`方法已知存在问题，因此提议将默认设置更改为`spawn`。,https://github.com/vllm-project/vllm/issues/8576
vllm,这个issue是一个优化类型的问题，主要涉及Marlin MoE中关于删除`thread_m_blocks`模板参数的优化。,https://github.com/vllm-project/vllm/issues/8573
vllm,这是一个功能需求类型的issue，主要涉及对象是vllm库，用户提出希望实现离线FP8量化功能。,https://github.com/vllm-project/vllm/issues/8566
vllm,这个issue是关于功能建议类型的，主要涉及到为vllm创建ProfileConfig用于个性化配置性能分析。由于用户在使用过程中关注内存使用情况，提出了希望提供灵活性的配置选项的建议。,https://github.com/vllm-project/vllm/issues/8561
vllm,这个issue是一个需求提报，主要涉及的对象是FastAPI。由于公司不允许在生产环境中使用某些端点，提出了增加一个新参数选项来禁用FastAPI文档的功能。,https://github.com/vllm-project/vllm/issues/8554
vllm,这是一个用户提出需求的issue，主要涉及vllm的请求处理机制。由于用户想控制在一个批次中运行请求的数量，需要调整引擎参数来实现。,https://github.com/vllm-project/vllm/issues/8552
vllm,这个issue类型是文档改进，主要涉及安装文档的改进。原因是增加了从另一个issue中获得的经验教训。,https://github.com/vllm-project/vllm/issues/8550
vllm,这个issue属于Feature请求类型，主要涉及的对象是Quantisation Support with CPU Backend功能。由于当前项目不支持在CPU后端上运行量化模型，用户询问是否有计划在未来支持，并寻求相关帮助或解决方案。,https://github.com/vllm-project/vllm/issues/8547
vllm,这个issue属于文档更新类型，主要涉及到调试文档中关于`gpu_memory_utilization`和CUDA OOM问题的解释补充，可能是由于当前文档不够详细或清晰而导致用户在理解和解决相关问题时遇到困难。,https://github.com/vllm-project/vllm/issues/8541
vllm,这个issue类型是用户提出需求，主要对象是关于vllm的安装过程。由于用户在安装vllm时遇到了问题，需要寻求帮助或解决方案。,https://github.com/vllm-project/vllm/issues/8535
vllm,这是一个关于性能优化的issue，主要涉及到CUDA初始化相关的函数调用。由于多次运行测试时出现CUDA重新初始化错误，因此需要减少部分CUDA相关函数的调用来避免这种问题。,https://github.com/vllm-project/vllm/issues/8534
vllm,这是一个功能更新的issue，涉及到Varlen prefill和Prefill chunking支持的添加，主要涉及到mamba kernels和Jamba model。,https://github.com/vllm-project/vllm/issues/8533
vllm,这个issue属于功能需求提出类型，主要涉及了torch.compile中的allreduce操作。这个问题提出了改进现有代码的方式，以便更好地支持未来类似操作。,https://github.com/vllm-project/vllm/issues/8526
vllm,这是一个关于提出需求的issue，主要涉及vllm的APC introspection接口，用户希望通过添加函数使vllm实例能够处理给定的块，以提高吞吐量。,https://github.com/vllm-project/vllm/issues/8523
vllm,这是一个功能需求类型的issue，主要涉及的对象是外部缓存服务。由于缺乏对外部缓存服务的度量衡，用户提出了需要将度量指标添加到外部缓存服务的需求。,https://github.com/vllm-project/vllm/issues/8522
vllm,这个issue类型是关于改进发布流程的请求，主要涉及到版本控制和发布过程。,https://github.com/vllm-project/vllm/issues/8517
vllm,该issue类型为用户提出需求，主要涉及的对象是针对大规模批量推理的优化参数。原因是用户希望找到最佳的参数设置，以提高405B FP8输出的生成效率。,https://github.com/vllm-project/vllm/issues/8513
vllm,这个issue类型为用户提出需求，主要涉及的对象是向vLLM文档添加互斥特性的兼容性矩阵，用户希望为用户提供快速查阅以便规划实施或学习。,https://github.com/vllm-project/vllm/issues/8512
vllm,这是一个用户提出需求的issue，主要涉及vLLM中的TPU支持问题，由于logprobs在TPU上不受支持，导致无法运行`lmevalharness`对某些评估进行处理。,https://github.com/vllm-project/vllm/issues/8499
vllm,这是一个关于提出新增功能需求的类型的issue，主要涉及到LoRa服务实现，由于用户对Microsoft提出的有关在生产中提供LoRa服务的论文感兴趣。,https://github.com/vllm-project/vllm/issues/8497
vllm,这是一个用户提出的需求类型的issue，主要涉及对象是TPU（Tensor Processing Unit）。,https://github.com/vllm-project/vllm/issues/8489
vllm,这是一个需求提出类的issue，主要涉及到torch.compile中添加一个flag来禁用自定义操作。由于当前torch.compile会消耗更多内存，用户希望测试一个新的flag来控制其行为。,https://github.com/vllm-project/vllm/issues/8488
vllm,这个issue类型是需求提出，主要对象是VLM模型的添加和支持。由于LLaVAOneVision模型在Huggingface中支持更多configs，但Huggingface在发布版本中使用默认值，导致出现了配置相关的问题。,https://github.com/vllm-project/vllm/issues/8486
vllm,这个issue类型是用户提出需求，主要对象是vLLM，用户想要了解vLLM是否支持多模态LLM的嵌入API，可能出现的原因是用户需要在vLLM中获取minicpmv 2.6的嵌入。,https://github.com/vllm-project/vllm/issues/8483
vllm,这是一个用户提出需求类型的issue，主要涉及`llm.chat()` API在批量推理方面的功能不足。可能由于目前API仅支持一对话进行推理，导致用户无法充分利用vLLM进行高效的离线处理。,https://github.com/vllm-project/vllm/issues/8481
vllm,"这个issue是关于添加""awq fused moe method""的功能请求，不涉及具体bug报告。",https://github.com/vllm-project/vllm/issues/8478
vllm,该issue类型为版本更新请求，涉及的主要对象是软件版本。由于需要修正或更新特定版本中存在的问题或添加新功能，因此请求将软件升级至v0.6.1.post2。,https://github.com/vllm-project/vllm/issues/8473
vllm,这是一个用户提出需求的issue，主要对象是获取首个token的延迟数据。由于benchmark_latency.py只提供了处理单个请求批次的延迟数据，用户希望能够获得首个token的延迟数据。,https://github.com/vllm-project/vllm/issues/8471
vllm,这是一个性能改进的提议，该问题主要涉及的对象是vllm库中的_fwd_kernel()函数。由于移动v变量的初始化，导致性能出现一致的提升。,https://github.com/vllm-project/vllm/issues/8466
vllm,这个issue属于用户提出需求类型，主要对象是vllm，用户想在离线服务环境中收集性能指标，可能是由于用户需要评估vllm在离线服务场景下的性能表现。,https://github.com/vllm-project/vllm/issues/8465
vllm,这是一个用户提出需求的issue，主要涉及要在int4量化下运行phi-3.5视觉指导模型，但目前不清楚如何支持quantization。,https://github.com/vllm-project/vllm/issues/8463
vllm,这个issue类型是一个开发需求，主要涉及vLLM的核心逻辑修改，旨在支持仅编码模型（如xlm-roberta、bge-m3...），用户提出了这个改进来增强vLLM的功能性。,https://github.com/vllm-project/vllm/issues/8462
vllm,该issue类型为用户提出需求，主要涉及AutoModelForSequenceClassification的正确使用，用户寻求关于如何与vllm集成以运行推理的帮助。,https://github.com/vllm-project/vllm/issues/8459
vllm,这个issue是关于用户提出需求的，主要涉及如何在vllm中部署lora模型和如何实现lora模块的可插拔性。用户想要通过在传递标志的方式控制是否使用lora模块。,https://github.com/vllm-project/vllm/issues/8454
vllm,这是一个用户提出需求的issue，主要涉及vllm对于支持encode only models的讨论。原因是vllm在支持更多模型和功能时出现了复杂性增加，需要不同的模块和处理器，导致某些新特性牺牲以实现兼容性，最终导致结果不佳。,https://github.com/vllm-project/vllm/issues/8453
vllm,这个issue属于需求提出类型，主要涉及的对象是VLLM中的支持编码模型。由于新模型需要增加对编码模型的支持，需要调整注意力机制实现以及支持新模型的特性。,https://github.com/vllm-project/vllm/issues/8452
vllm,该issue属于用户提出需求类型，主要对象是针对torch.compile插件，用户希望添加自定义编译后端。,https://github.com/vllm-project/vllm/issues/8445
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM的在线推理批处理功能。用户希望了解如何在指定的批大小情况下并行处理vLLM的请求，因为当前无法确定vLLM内部处理提示的批大小。,https://github.com/vllm-project/vllm/issues/8441
vllm,这个issue类型是版本更新请求，涉及主要对象是项目中的软件版本。,https://github.com/vllm-project/vllm/issues/8440
vllm,这个issue类型是需求提出，主要涉及对象是vLLM在Kubernetes (K8s)环境中的部署文档缺失。用户提出由于当前文档只提供Docker的部署指导，而不包括K8s的部署信息，导致K8s用户无法顺利部署vLLM。,https://github.com/vllm-project/vllm/issues/8438
vllm,这是一个功能提议，提出了关于在 bitsandbytes 量化中应用张量并行性的问题。,https://github.com/vllm-project/vllm/issues/8434
vllm,这个issue是一个文档更新类型的问题，主要涉及的对象是Pixtral示例更新。由于chunked prefill被默认关闭导致旧的命令不再适用，需要更新示例以适应此变化。,https://github.com/vllm-project/vllm/issues/8431
vllm,这是一个用户提出需求的类型，主要涉及的对象是BLIP/BLIP-2模型。由于需要支持加载复合模型以及提取中间状态，导致了需要对BLIP vision encoder进行重构和对BLIP2进行更新的问题。,https://github.com/vllm-project/vllm/issues/8407
vllm,这是一个特性需求的issue，主要涉及前端工具调用internlm/internlm2_57bchat模型的支持。原因是为了增加支持工具调用解析器的管理，以及为那些不支持并行工具调用的模型添加并行测试跳过配置。,https://github.com/vllm-project/vllm/issues/8405
vllm,该issue类型为用户请求帮助，主要涉及如何使用openai兼容的API运行GGUF模型。用户提出了关于如何集成该模型到vllm中以及如何运行特定模型推断的问题。,https://github.com/vllm-project/vllm/issues/8401
vllm,这是一个技术需求提出的issue，主要涉及到torch.compile中递归编译加载模型的解决方案。原因是为了解决直接应用torch.compile到模型时的一些限制。,https://github.com/vllm-project/vllm/issues/8398
vllm,该issue类型为功能增强，主要涉及的对象是MRotaryEmbedding模型中使用_ROPE_DICT提高内存利用率的功能。由于现有模型在不同层之间不能共享RoPE，导致额外的GPU内存占用和初始化时间增加的问题。,https://github.com/vllm-project/vllm/issues/8396
vllm,该issue类型为用户提出需求，并主要涉及支持语音模型qwenaudio。由于缺乏计划支持此类声音模型，用户提出相应问题。,https://github.com/vllm-project/vllm/issues/8394
vllm,这个issue类型是需求提出；主要涉及的对象是使用TPU的分布式后端；用户提出了使用Ray作为默认分布式后端的需求。,https://github.com/vllm-project/vllm/issues/8389
vllm,这个issue是一个用户提出需求的类型，主要涉及的对象是增加一个多模态功能用于评估离线延迟、吞吐量和在线服务对Pixtral多模态模型的表现评估。,https://github.com/vllm-project/vllm/issues/8385
vllm,这是一个建议改善性能的问题，涉及JSONLogitsProcessor，在每个请求中重复实例化对象并重新编译正则表达式，可能导致性能下降。,https://github.com/vllm-project/vllm/issues/8383
vllm,这是一个版本更新类型的Issue，涉及的主要对象是软件版本。,https://github.com/vllm-project/vllm/issues/8379
vllm,这是一个功能需求的issue，主要涉及MultiStep和Chunked Prefill的支持。,https://github.com/vllm-project/vllm/issues/8378
vllm,这是一个功能需求的issue，主要涉及到生成函数中条件性包含提示的功能实现。该需求的动机是为了提高在流式场景下的效率。,https://github.com/vllm-project/vllm/issues/8360
vllm,这是一个功能提议的issue，主要涉及的对象是在`vllm/entrypoints/api_server.py`中的`generate`函数。原因是当前实现中，无论是否启用了流式传输，`generate`函数始终在其响应中包含提示，这在流式传输模式下会导致效率低下。,https://github.com/vllm-project/vllm/issues/8359
vllm,这个issue类型为用户提出需求，主要对象是vllm服务器的关闭操作。作者想知道如何使用命令关闭vllm服务器，表现为缺乏关于关闭vllm服务器的指令的使用说明。,https://github.com/vllm-project/vllm/issues/8356
vllm,这个issue是关于安全性的需求提出，要求在CI工作流中添加Bandit安全检查。,https://github.com/vllm-project/vllm/issues/8353
vllm,这是一个功能增强的issue，主要涉及VLM核心模块，修改支持多模态模型的前缀缓存。,https://github.com/vllm-project/vllm/issues/8348
vllm,该issue是一个需求报告，涉及到VLM核心模块，主要是为了增加精确的多模态占位符追踪功能。导致此需求的原因是当前多模态提示占位符仅与多模态嵌入索引相关联，需要引入一种精确跟踪多模态占位符范围的机制。,https://github.com/vllm-project/vllm/issues/8346
vllm,这个issue类型是功能需求，主要涉及的对象是为Llama 3.1和3.2工具添加支持。原因是Llama模型与其他模型在工具调用方面有一些显著差异，因此需要对现有单元测试进行调整以满足Llama的需求。,https://github.com/vllm-project/vllm/issues/8343
vllm,这是一个关于性能优化的issue，涉及到chunked prefill和prefix caching的设置。,https://github.com/vllm-project/vllm/issues/8342
vllm,这个issue类型为需求提出，涉及主要对象为模型工具调用解析支持。这个问题是由于需要增加对`ibm-granite/granite-20b-functioncalling`的工具调用解析支持，同时添加一个基于granite fc论文的示例聊天模板而被提出。,https://github.com/vllm-project/vllm/issues/8339
vllm,该issue类型为功能需求反馈，主要对象是CUDA Time Layerwise Profiler。由于用户需要通过分析CUDA（GPU核心）每个模块/层所花费的时间来了解性能表现，因此提出了开发类似Layerwise profiler来实现这一功能的需求。,https://github.com/vllm-project/vllm/issues/8337
vllm,这是一个用户提出需求的issue，主要对象是改进vLLM在ROCm设备上的安装体验。由于ROCm Docker镜像体积庞大和从源代码构建vLLM耗时较长，导致安装时间较长，用户提出需要官方预构建二进制文件以加速安装过程。,https://github.com/vllm-project/vllm/issues/8336
vllm,这是一个特性需求的issue，主要涉及到输出流支持的添加，针对于 multistep + async 的实现。 由于需求对性能有一定要求，缓存输出对象以降低Python对象分配，同时通过最近的delta outputs改进来维持好的性能。,https://github.com/vllm-project/vllm/issues/8335
vllm,这是一个关于需求提出的issue，主要涉及到vLLM中提供固定缓存时效功能的问题。最后，用户提出了需要改进现有缓存管理逻辑来支持不同的缓存策略的需求。,https://github.com/vllm-project/vllm/issues/8333
vllm,这是一个用户需求报告，主要对象是关于如何停止VLLM生成过程的问题。用户想要通过`curl`触发停止当前生成的需求。,https://github.com/vllm-project/vllm/issues/8332
vllm,这是一个针对代码修改建议的issue，主要涉及Qwen2-MOE GPTQ模型的加载优化。,https://github.com/vllm-project/vllm/issues/8329
vllm,这个issue是一个需求提出，涉及对象是CI/Build中的HIP builds。缺乏对`ccache`和`sscache`的使用导致了HIP builds的性能不佳。,https://github.com/vllm-project/vllm/issues/8327
vllm,这是用户提出需求的类型，主要涉及vLLM模块的使用问题，用户想了解是否可以直接将input_embeds传递给generate函数。,https://github.com/vllm-project/vllm/issues/8323
vllm,该问题类型为需求提出，主要涉及添加NVIDIA Meetup幻灯片、公布AMD Meetup、添加联系信息等内容。由于缺少特定的幻灯片、会议信息和联系方式，导致用户需要补充这些内容以完善会议相关信息。,https://github.com/vllm-project/vllm/issues/8319
vllm,这是一个需求类型的issue，涉及到Speculative Decoding测试的重构。,https://github.com/vllm-project/vllm/issues/8317
vllm,该issue类型为代码优化，涉及主要对象为mistral tokenizer的类型注释问题。这个问题由于mistral tokenizer的输入输出与HF的tokenizer不同，导致需要将它们拆分为不同的函数以避免引入过多的联合类型。,https://github.com/vllm-project/vllm/issues/8314
vllm,这是一个需求提出的issue，主要涉及的对象是需要支持Qwen2工具调用模板。该需求由于要求支持Qwen2工具调用模板，可能由于缺乏相应功能或特性而导致。,https://github.com/vllm-project/vllm/issues/8312
vllm,这是一个建议性质的问题，主要涉及到重构和分离vLLM核心上的波束搜索功能。由于波束搜索是一个搜索算法，而不是采样算法，与其他采样算法有冲突，因此导致了代码维护和性能优化方面的挑战。,https://github.com/vllm-project/vllm/issues/8306
vllm,该issue是一个功能需求，主要涉及的对象是vllm容器，用户提出了关于在多主机推理中使用ray集群启动逻辑的请求。,https://github.com/vllm-project/vllm/issues/8302
vllm,这个issue是一个模型支持的请求，主要关注的对象是MiniCPM34B模型，由于markdown渲染问题导致需要使用原始HTML标签。,https://github.com/vllm-project/vllm/issues/8297
vllm,该issue属于需求提出类型，主要对象是将vLLM中的单一处理过程拆分为异步标记化、模型推理和去标记化步骤，以提高GPU利用率。,https://github.com/vllm-project/vllm/issues/8295
vllm,这是一个功能增强类型的issue，涉及的主要对象是指标系统。由于需要在日志中实时显示指标，因此新增了一个名为num_cumulative_preemption的指标。,https://github.com/vllm-project/vllm/issues/8294
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm的serving profiler工具。,https://github.com/vllm-project/vllm/issues/8241
vllm,这是一个功能需求的issue，主要涉及的对象是Block Manager v2的内容哈希生成逻辑。由于Block Manager v2在prefix caching模式下不支持LoRA和prompt adapter，导致用户提出了需要支持这两个功能的需求。,https://github.com/vllm-project/vllm/issues/8240
vllm,这是一个用户提出需求的问题，涉及到针对vLLM进行批量离线推理时如何确定批处理大小的问题。用户提到不清楚当前GPU利用率、适宜的批处理大小以及如何实现自动设置动态批处理大小的困惑。,https://github.com/vllm-project/vllm/issues/8239
vllm,这是一个功能需求类型的issue，主要对象是vllm中的LLaVA离线推断功能，用户提出了希望添加支持多图像输入的功能。,https://github.com/vllm-project/vllm/issues/8236
vllm,这是一个关于新模型支持的需求提出的issue，主要涉及支持MiniCPM3ForCausalLM MiniCPM3-4B模型，由于未得到回应，用户可能在寻求增加新模型支持的帮助。,https://github.com/vllm-project/vllm/issues/8232
vllm,这是一个用户提出需求和问题的类型的issue，涉及对象为vllm模型和Quantization功能。由于在使用FP8和INT8数据类型时出现了警告和OOM错误，用户想了解是否影响以及是否可以在不同的环境中运行。,https://github.com/vllm-project/vllm/issues/8225
vllm,"这是一个用户提出需求的类型，主要对象是CUDA 12，并询问是否还有基于CUDA 12的"".whl""安装包。原因可能是用户希望找到适用于CUDA 12的安装包以进行相应的操作。",https://github.com/vllm-project/vllm/issues/8222
vllm,这个issue类型是需求问题，主要涉及的对象是`SqueezeLLM`模块的移除。,https://github.com/vllm-project/vllm/issues/8220
vllm,"这是一个功能需求报告，主要涉及""Supporting Guided Decoding via AsyncLLMEngine""。由于无法通过generate方法传递GuidedDecodingRequest对象，用户提出希望能够支持通过AsyncLLMEngine.generate()接口实现Guided Decoding的功能。",https://github.com/vllm-project/vllm/issues/8218
vllm,这是一个用户提出需求的issue，主要涉及将GPTQ的Marlin MoE支持添加到项目中，由于当前Marlin MoE内核仅支持int4，因此需要更新或添加大型MoE模型的可选测试。,https://github.com/vllm-project/vllm/issues/8217
vllm,这是一个用户提出需求的issue，主要涉及如何在MiniCPM-V-2_6模型或任何视觉语言模型中执行多图像推论的问题。由于用户已经展示了对vllm的使用方式，但可能遇到了某些问题。,https://github.com/vllm-project/vllm/issues/8215
vllm,这是一个用户提出需求的类型的issue，主要涉及调整终端显示日志间隔时间的问题。可能由于用户希望更改日志显示频率或者根据特定需求调整终端日志显示间隔，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/8210
vllm,这个issue类型是功能需求，涉及的主要对象是在vLLM中添加Granite MoE。,https://github.com/vllm-project/vllm/issues/8206
vllm,这是一个用户提出需求的issue，主要对象是vllm中的benchmark_serving.py文件。用户希望了解是否可以在Multimodal LLM（llava）中使用benchmark_serving.py，并支持图像输入。,https://github.com/vllm-project/vllm/issues/8205
vllm,这是一个用户提出需求的issue，主要涉及需要对缓存的内核进行重新设计以支持不同的布局。由于许多快速注意力内核只支持HND布局，该问题是针对调整布局以支持不同内核需求的。,https://github.com/vllm-project/vllm/issues/8200
vllm,这是一个功能需求类型的issue，主要涉及到解决使用大型检查点训练的QLoRA适配器无法使用TP>1进行模型分片的问题。,https://github.com/vllm-project/vllm/issues/8197
vllm,该issue属于用户提出需求类型，主要涉及使用vllm调用qwen2模型的问题。用户不清楚如何编写聊天模板以运行特定模型的推理。,https://github.com/vllm-project/vllm/issues/8196
vllm,"这个issue是用户提出需求类型的问题，主要涉及vllm模型支持DiT模型的输出为Tensor (N, C, H, W)布局。用户提出了希望支持DiT模型的多模态输入和输出需求。",https://github.com/vllm-project/vllm/issues/8195
vllm,这是一个特性需求的issue，主要涉及到vLLM的benchmarking脚本`benchmark_serving.py`，由于当前脚本不支持配置`logprobs`参数，导致无法实现在benchmark过程中对不同`logprobs`设置对vLLM性能影响的评估。,https://github.com/vllm-project/vllm/issues/8193
vllm,这个issue类型是功能需求，主要涉及的对象是OpenVINO vLLM backend的GPU支持。由于目前的OpenVINO vLLM backend不支持GPU设备，用户提出了希望添加GPU支持的需求。,https://github.com/vllm-project/vllm/issues/8192
vllm,这个Issue属于用户提出需求，并主要涉及VLLM OpenAI Docker镜像。用户想要了解如何使用VLLM进行SQL编码器的推理，并寻求启动OpenAI服务器的Python脚本方法。,https://github.com/vllm-project/vllm/issues/8183
vllm,这个issue类型是文档更新，主要涉及的对象是VLM的文档。,https://github.com/vllm-project/vllm/issues/8181
vllm,这个issue是一个关于增加benchmark_throughput.py中block_size选项的功能需求，不是一个bug报告。,https://github.com/vllm-project/vllm/issues/8175
vllm,这个issue属于用户需求类型，主要涉及vllm是否支持allenai/OLMoE-1B-7B-0924模型，由于未得到响应，用户来询问如何支持他们想要的模型。,https://github.com/vllm-project/vllm/issues/8170
vllm,这个issue是关于新的Model加载格式的改进。,https://github.com/vllm-project/vllm/issues/8168
vllm,该issue属于版本更新类型，涉及主要对象为项目版本控制工具。,https://github.com/vllm-project/vllm/issues/8166
vllm,这个issue属于用户提出需求类型，主要涉及vLLM模型中部分上下文支持的功能，由于目标模型的上下文大于草稿模型，用户希望支持部分上下文以提供更灵活的模型训练。,https://github.com/vllm-project/vllm/issues/8159
vllm,这是一个关于需求的issue，涉及主要对象是vllm中的LLMEngine和AsyncLLMEngine，用户提出了关于如何使用response_format和guided output的问题。,https://github.com/vllm-project/vllm/issues/8156
vllm,这个issue类型是功能需求，主要涉及Cherry pick of delayed sampling功能，用户提出需要在vllm server传递额外的标志来使用该功能。,https://github.com/vllm-project/vllm/issues/8154
vllm,该issue类型为用户提出需求，主要涉及对象为vllm下的VisionEncoderDecoderModel，用户寻求关于是否有该模型的问题。,https://github.com/vllm-project/vllm/issues/8153
vllm,这是一个用户提出需求的issue，主要对象是为vLLM添加对`GPTNeoXForSequenceClassification`架构的支持。这个问题是由于目前vLLM不支持该架构，导致训练和评估RLHF方法变得缓慢。,https://github.com/vllm-project/vllm/issues/8152
vllm,这是一个用户提出需求的issue，主要涉及vllm模型与Internvl2 8b视频推理的集成问题。由于缺乏集成指导，用户不知道如何使用vllm进行视频推理。,https://github.com/vllm-project/vllm/issues/8151
vllm,这是一个特性需求的issue，主要涉及到对性能进行优化。,https://github.com/vllm-project/vllm/issues/8150
vllm,该issue类型是用户提出需求，主要对象是实现混合专家模型的专家并行功能，希望在推理过程中减少通信成本。,https://github.com/vllm-project/vllm/issues/8148
vllm,这是一个需求报告的issue，主要关注是支持MultiModal inputs using Llama3.1的功能。由于当前版本的vLLM不支持Llama3.1的MultiModal输入，用户希望了解是否可以通过启用llama3.1作为VLM或者其他方式来支持。,https://github.com/vllm-project/vllm/issues/8146
vllm,这是一个功能增强的issue，主要涉及添加一个新的指标(metric)来跟踪每次迭代的token数量，以帮助了解import GEMM的形状。,https://github.com/vllm-project/vllm/issues/8140
vllm,这个issue类型是用户提出需求，主要对象是要支持一个新的模型Qwen2-VL。原因是该模型目前还未被vllm支持，用户在询问此模型是否可以被加入支持。,https://github.com/vllm-project/vllm/issues/8139
vllm,这是一个功能更新（Feature Update）类型的issue，主要涉及到GPTQ激活排序功能的更新。由于需要支持`actorder`参数和`weight_g_idx`参数的加载以及相应的测试，导致了这个issue的提出。,https://github.com/vllm-project/vllm/issues/8135
vllm,这是一个用户提出需求的类型，该问题单主要涉及CI相关功能。由于at-mentions引起的spam，需要修改PR remainder以减少垃圾信息。,https://github.com/vllm-project/vllm/issues/8134
vllm,这是一个功能需求的issue，主要涉及到在cuda镜像中使用Python 3.12版本。由于Python 3.11在vLLM图像中表现更快，而Python 3.12带来了更多性能修复，因此建议更新Python版本以提高性能。,https://github.com/vllm-project/vllm/issues/8133
vllm,这是一个关于在vllm中添加级联推理（cascade inference）功能的PR。,https://github.com/vllm-project/vllm/issues/8132
vllm,该issue为关于CI/Build改进的问题，主要涉及Enabling kernels tests for AMD，并忽略了一些失败的测试。这可能是因为AMD内核测试存在一些问题导致部分测试无法通过。,https://github.com/vllm-project/vllm/issues/8130
vllm,该issue类型是用户提出需求，主要涉及vLLM模型的quantization支持，用户希望实现int8量化支持，以减少内存使用、提高性能和降低运营成本。,https://github.com/vllm-project/vllm/issues/8127
vllm,该issue类型为需求更改，涉及的主要对象为`engine_use_ray`。由于mo无法分配足够的时间来处理这个问题，需要对之前的提交进行重新处理。,https://github.com/vllm-project/vllm/issues/8126
vllm,这是一个用户提出需求的issue，主要涉及的对象是CI系统。这个问题是由于需要限制只有PR的审阅者和提交者可以触发CI，而现有的工作流无法实现该需求。,https://github.com/vllm-project/vllm/issues/8124
vllm,该issue类型为用户提出需求或请教问题，涉及主要对象为`max_num_batched_tokens`。由于用户困惑为什么较小的`max_num_batched_tokens`值会获得更好的ITL，并且询问关于模型并行运行速度，序列总长度超过`max_num_batched_tokens`会受限制等问题，寻求关于如何设置该参数的帮助。,https://github.com/vllm-project/vllm/issues/8121
vllm,这个issue类型为代码优化，涉及主要对象为缓存预取和前缀缓存的功能。由于缺少正确的合著者，导致了合作者列表的错误，需要添加正确的合著者信息。,https://github.com/vllm-project/vllm/issues/8120
vllm,这是一个用户提出需求的issue，主要涉及VLLM在分类或确定任务中如何输出每个可能标记的logprob，用户希望获得关于不同类别或真假判断的概率输出。可能由于代码逻辑或参数设置的问题导致了logprob输出异常。,https://github.com/vllm-project/vllm/issues/8111
vllm,这是一个用户提出需求的issue，主要涉及vLLM是否可以同时处理多轮对话和多用户身份的情况。用户询问如何实现多轮和多用户实例同时处理的方式，以及如何让模型记住每位用户说过的话。由于模型遗忘了用户说过的内容，用户希望能实现这样的功能。,https://github.com/vllm-project/vllm/issues/8105
vllm,这个issue是关于性能提升建议，主要涉及TTFT随着批量tokens数目线性增加的问题，作者希望了解为什么TTFT与prompt长度呈这样明显的增长关系，以及寻求解决方案。,https://github.com/vllm-project/vllm/issues/8086
vllm,"这个issue类型为用户提出需求，主要涉及的对象是新模型""quantized Qwen2 MoE models""。该问题暂未得到回应，用户可能寻求支持新模型的集成或解决相关问题。",https://github.com/vllm-project/vllm/issues/8078
vllm,该issue属于用户提出的需求类型，主要涉及改进vLLM中关于快速引导解码的功能。由于预定义输出的确定性特性未完全被充分利用，用户提出了关于并行解码以提高解码效率的建议。,https://github.com/vllm-project/vllm/issues/8075
vllm,这是一个功能需求的issue，该问题涉及主要对象是vLLM在Kubernetes上支持多节点服务。由于缺乏集成到vLLM代码库中的解决方案，用户提出了希望官方团队开发此功能的需求。,https://github.com/vllm-project/vllm/issues/8074
vllm,该issue属于用户提出需求类型，主要涉及的对象是在vllm中添加smoothquant支持。由于用户认为smoothquant功能很棒，并已经在其他项目中添加了该功能，因此希望在vllm中也能集成smoothquant。,https://github.com/vllm-project/vllm/issues/8072
vllm,这是一个用户提出需求的issue，主要涉及到VLLM是否支持使用mpirun启动多个卡，并绑定不同的CPU到每个卡上。可能是由于性能测试需要，用户想要通过mpirun将不同的CPU绑定到不同的卡上。,https://github.com/vllm-project/vllm/issues/8069
vllm,这个issue是一个特性需求，主要对象是Huggingface generation interface，用户提出需求希望实现beam search with nonzero temperature来增加生成的多样性。,https://github.com/vllm-project/vllm/issues/8067
vllm,这是一个用户提出需求的issue，主要涉及如何在TPU环境下使用InternVL2模型。用户因不知道如何将InternVL2与vllm集成而提出了这个问题。,https://github.com/vllm-project/vllm/issues/8065
vllm,"这个issue是关于""Neuron""模块下添加或覆盖神经元配置和量化配置的需求。",https://github.com/vllm-project/vllm/issues/8062
vllm,这是一个用户提出需求报告的Github上的issue，目的是为`run_batch.py`添加一个tqdm进度指示器。,https://github.com/vllm-project/vllm/issues/8060
vllm,这是一个技术更新的issue，主要对象是vLLM项目中的Ascend NPU后端。由于该PR是vLLM昇腾的POC版本且不再维护，用户可能遇到无法获取最新支持和功能的问题。,https://github.com/vllm-project/vllm/issues/8054
vllm,这个issue是一个优化性质的需求提出，主要涉及Async + Multi-step的性能优化，由于之前的异步操作仅在解码步骤中执行，现在扩展到了解码的所有步骤和前一个提示执行，最终导致性能改进了约28%。,https://github.com/vllm-project/vllm/issues/8050
vllm,这是一个优化工程的Issue，主要涉及到Dockerfile的构建过程。,https://github.com/vllm-project/vllm/issues/8044
vllm,这个issue是关于内核更改接口以适应连续批处理的需求。,https://github.com/vllm-project/vllm/issues/8039
vllm,这是一个用户提出需求的 issue，目标是添加一个类似于 https://docs.ray.io/en/latest/serve/tutorials/vllmexample.html 的 RayServe 示例。,https://github.com/vllm-project/vllm/issues/8038
vllm,该问题类型为用户提出需求，主要对象是将T5模型贡献给vLLM项目。由于该用户希望将T5模型贡献给项目，并表示自己是第一次贡献，希望从团队中学习，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/8036
vllm,这是一个功能改进的issue，主要涉及的对象是 Fused Marlin MoE 模块。,https://github.com/vllm-project/vllm/issues/8032
vllm,这个issue是技术需求类型的，主要涉及vllm模型在GPU上支持的最大并发请求数的问题，用户疑惑如何确定单个V100 GPU支持的最大并发请求数量以及处理过程中可能出现的问题。,https://github.com/vllm-project/vllm/issues/8031
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是希望支持BAAI/bgererankerv2m3模型。由于没有收到任何回应，用户希望vllm项目能够支持这个模型。,https://github.com/vllm-project/vllm/issues/8022
vllm,这是一个用户提出需求的issue，主要对象是在多节点多GPU上使用torchrun启动分布式推理服务器，用户遇到无法正常工作的问题，可能是由于配置或操作步骤不正确导致。,https://github.com/vllm-project/vllm/issues/8021
vllm,这是一个功能需求类型的issue，主要涉及到vllm核心内核的支持外部交换器，由于GPU内存不足，需要将kv缓存交换到CPU内存。,https://github.com/vllm-project/vllm/issues/8018
vllm,这是一个功能需求报告，主要涉及的对象是打印请求指标到标准输出，以避免需要使用 opentelemetry 进行性能分析。,https://github.com/vllm-project/vllm/issues/8014
vllm,这是一个用户提出需求的 issue，主要涉及的对象是 vLLM 中的 causal_conv1d_update 接口。由于连续批处理导致当前批处理中的状态来自不同位置，需要改变接口以允许传递索引列表进行更新。,https://github.com/vllm-project/vllm/issues/8012
vllm,这个issue是用户提出需求，请求关于TPU进行异步输出处理的帮助。,https://github.com/vllm-project/vllm/issues/8011
vllm,"这是关于文档更新的问题，主要涉及NVIDIA AMMO 工具更名为 ""TensorRT model optimizer""，导致相关文档过时。",https://github.com/vllm-project/vllm/issues/8010
vllm,这个issue类型是特性需求，涉及到Proof of Work价值的提案。此需求主要涉及到运行LLM的矿工和验证者，由于LLM的非确定性特性和其结果的难以比较导致矿工有动机使用轻量模型，需要提出解决这一问题的方案。,https://github.com/vllm-project/vllm/issues/8003
vllm,这是一个功能改进类的issue，主要涉及的对象是vllm中的linear layers。由于未正确处理没有偏置的线性层情况，在调用`extra_repr`时可能会出现问题。,https://github.com/vllm-project/vllm/issues/8000
vllm,这是一个关于性能优化的issue，主要对象是VLLM的内核，经原因为降低内存消耗而修改默认值。,https://github.com/vllm-project/vllm/issues/7995
vllm,这是一个功能需求的issue，主要涉及vllm支持外部交换器的功能。由于GPU内存不足时，需要将kv缓存交换到CPU内存，因此添加了外部交换器接口以及相应的实现，来扩展token的存储空间。,https://github.com/vllm-project/vllm/issues/7994
vllm,这是一个功能需求问题，主要涉及使用vllm部署多个lora时切换问题。由于用户希望同时使用两个lora设备而不进行切换导致症状。,https://github.com/vllm-project/vllm/issues/7990
vllm,这是一个关于用户提出需求的issue，主要涉及到HuatuGPTVision 7B模型的架构 LlavaQwen2ForCausalLM 在vLLM中不被支持的问题。,https://github.com/vllm-project/vllm/issues/7984
vllm,这是一个用户提出需求的问题，主要涉及如何使用vllm生成logits而不是文本。用户提出该问题的原因可能是无法找到logits生成的文件路径。,https://github.com/vllm-project/vllm/issues/7981
vllm,此issue属于需求更改类型，涉及主要对象为`GPTQ`项目，由于新需求，需要更新代码以使用`vLLMParameters`。,https://github.com/vllm-project/vllm/issues/7976
vllm,这是一个功能改进类型的issue，主要涉及到torch.compile中的代码。由于在https://github.com/vllmproject/vllm/pull/7898中跳过了dynamo runtime的开销，不再需要在分析后重置代码了。,https://github.com/vllm-project/vllm/issues/7975
vllm,这是一个用户提交需求的issue，主要涉及更新TPU Int8以使用新的vLLM参数。原因可能是为了提高模型的性能或准确度。,https://github.com/vllm-project/vllm/issues/7973
vllm,这是一个更新类型的issue，涉及的主要对象是`FBGEMMFp8`。由于需要更新使用`vLLMParameters`，才能满足最新的要求。,https://github.com/vllm-project/vllm/issues/7972
vllm,这是一个关于需求升级的问题，主要涉及PyTorch XLA nightly，可能是由于当前版本存在不足或者需要新增功能导致的。,https://github.com/vllm-project/vllm/issues/7967
vllm,这个issue为功能改进提议，涉及benchmark_throughput.py脚本中添加`--async-engine`选项，主要因为使用AsyncLLMEngine接口替代LLM，并根据`disablefrontendmultiprocessing`的指定来确定是否使用独立的前端。,https://github.com/vllm-project/vllm/issues/7964
vllm,这是一个关于文档请求的issue，主要涉及Speculative Decoding在vLLM中的lossless guarantees，用户提出需要关于此功能的文档信息。,https://github.com/vllm-project/vllm/issues/7962
vllm,该issue属于性能优化类问题，主要涉及前端与API服务器之间的客户端取消连接检查频率限制，由于当前逻辑导致每次生成令牌时都会进行检查，导致成本过高，可以通过限制每秒最多一次的方式来优化。,https://github.com/vllm-project/vllm/issues/7959
vllm,这是一个关于性能优化的issue，主要涉及前端代码。原因是通过一些优化措施，预计可以获得接近100%的吞吐量提升。,https://github.com/vllm-project/vllm/issues/7957
vllm,"这是一个功能需求的issue，主要涉及到""Context Caching""的功能。由于长提示导致TTFT速度较慢，用户提出了保存kv缓存以提高速度的需求。",https://github.com/vllm-project/vllm/issues/7952
vllm,这是一个特性请求，涉及主要对象是Gemmma 2模型在VLLM TPU上运行时出现的错误。原因是pallas backend不支持logit softcapping，用户希望在VLLM中实现此功能。,https://github.com/vllm-project/vllm/issues/7950
vllm,这个issue属于需求提出类型，主要涉及的对象是vllm中的speculative decoding功能。由于工作负载的不同，用户希望增加多个提议者的支持，以实现不同的调度策略。,https://github.com/vllm-project/vllm/issues/7947
vllm,这个issue属于功能需求类型，涉及LLM的前端，是关于添加Torch profiler支持的功能需求。,https://github.com/vllm-project/vllm/issues/7943
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM模型和EXAONE 3.0模型。由于EXAONE 3.0模型是作为开放模型发布，因此用户希望将其集成到vLLM中。,https://github.com/vllm-project/vllm/issues/7942
vllm,这个issue是用户提出需求，希望支持使用SLURM启动集群而不仅仅是Ray。原因是大多数集群只支持SLURM而不是Ray。,https://github.com/vllm-project/vllm/issues/7933
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM模型在支持google/madlad400-3b-mt翻译模型上的问题。导致问题的原因是vLLM当前不支持所需的T5ForConditionalGeneration架构和T5Tokenizer标记器。,https://github.com/vllm-project/vllm/issues/7930
vllm,这是一个关于代码质量优化的issue，涉及主要对象为shell脚本。由于shell脚本中存在一些潜在的问题，导致需要通过添加shellcheck来进行shell脚本代码检查和优化。,https://github.com/vllm-project/vllm/issues/7925
vllm,这个issue类型为功能改进，主要对象是VLLM的async postprocessor和multi-step功能。原因是为了解决运行多步执行时被阻塞的问题，提高性能。,https://github.com/vllm-project/vllm/issues/7921
vllm,这是一个用户提出需求的issue， 主要对象是VLLM模型，用户尝试强制修改embedding_mode为True时出现了错误。,https://github.com/vllm-project/vllm/issues/7915
vllm,这是一个关于功能支持问题的issue，主要涉及vllm的工具调用功能。用户提出根据文档指示无法执行特定函数调用，希望确认这一是否持续，同时提到本地llm响应存在问题。,https://github.com/vllm-project/vllm/issues/7912
vllm,这是一个用户提出需求的issue，主要涉及的对象是异步后处理器的虚拟引擎支持。,https://github.com/vllm-project/vllm/issues/7911
vllm,这是一个关于启用vLLM的内存分层功能的问题，主要涉及了不同BlockManager版本之间的兼容性和功能支持问题。,https://github.com/vllm-project/vllm/issues/7910
vllm,这是一个用户提出需求的issue，主要涉及的对象是在加载Lora时出现的错误。由于minicpmvd2_6不支持LoRA，导致用户无法在vllm端点中加载相应的finetuning脚本。,https://github.com/vllm-project/vllm/issues/7909
vllm,这是一个需求类型的issue，主要涉及的对象是关于使用vllm中的Async Mistral 7B进行离线推断时如何传递JSON content-type的问题。由于缺乏Async引擎的包装类，导致不能使用`AsyncLLMEngine`类传递模式schema。,https://github.com/vllm-project/vllm/issues/7908
vllm,这是一个特性需求，涉及添加控制向量支持到 vllm 的问题。,https://github.com/vllm-project/vllm/issues/7906
vllm,这是一个功能需求的issue，主要涉及的对象是vLLM的多模态输入处理。由于需要支持每个提示的多个{图像，音频}输入，因此需要对输入进行堆叠，以便为每个提示内的多个图像创建维度。,https://github.com/vllm-project/vllm/issues/7902
vllm,这是一个集成新模型请求的issue，涉及的主要对象是在vllm中集成EXAONE 3.0模型。由于EXAONE 3.0是一个开放模型，所以需要进行集成，以解决相关任务的繁琐性。,https://github.com/vllm-project/vllm/issues/7901
vllm,这是一个优化性能的issue，主要涉及到减少TPU上的运行时开销。该问题表明需要避免Dynamo guard评估开销，以提高解码步骤的执行效率。,https://github.com/vllm-project/vllm/issues/7898
vllm,这是一个关于功能需求的issue，主要涉及vLLM是否支持前缀解码器及如何实现非因果自注意力的问题。,https://github.com/vllm-project/vllm/issues/7895
vllm,这是一个用户提出需求的issue，主要对象是前端（Frontend）的LLMEngine，用户提出增加一个`return_hidden_states`选项以返回模型的隐藏状态。,https://github.com/vllm-project/vllm/issues/7892
vllm,这是一个关于文档更新的Issue，涉及到VLLM包中的tensorizer部分，用户提出需要在文档中添加安装`tensorizer`扩展的说明。,https://github.com/vllm-project/vllm/issues/7889
vllm,这个issue是一个需求提议，主要涉及Neuron中添加对上下文长度和token生成桶(bucket)的支持。原因是为了通过环境变量控制上下文长度和token生成桶，以优化Neuron设备上的延迟性能。,https://github.com/vllm-project/vllm/issues/7885
vllm,这个issue类型是功能改进，主要对象是将自定义执行器移动到插件系统中，以减少主代码库的复杂性。,https://github.com/vllm-project/vllm/issues/7879
vllm,这是一个增强功能型的issue，涉及到github actions workflows的linting，主要对象是github actions workflows的配置。原因是为了避免未来意外引入错误，已经通过本地运行linting和CI自动运行linter的方式实现linting。,https://github.com/vllm-project/vllm/issues/7876
vllm,该issue类型为性能优化，涉及主要对象是vllm的speculative decoding过程。由于cuda memcpy的异常出现导致内存拷贝开销较大，最终影响了性能表现。,https://github.com/vllm-project/vllm/issues/7875
vllm,这是一个用户提出需求的issue，主要涉及对象是希望将Chexagent Multimodel整合到vLLM中。这个问题的原因是Chexagent模型的架构未被vLLM支持，用户希望得到帮助解决这一集成问题。,https://github.com/vllm-project/vllm/issues/7863
vllm,这是一则关于优化特征解码逻辑的需求报告，主要涉及批处理的张量操作和逻辑处理，旨在提高处理各种批处理场景时的效率。,https://github.com/vllm-project/vllm/issues/7851
vllm,这个issue属于需求提出类型，主要涉及到对模型推理过程中发生无限循环的问题，提出了需要添加`no_repeat_n_gram`参数来避免此类情况的需求。,https://github.com/vllm-project/vllm/issues/7842
vllm,这是一个功能改进的issue，主要涉及到更新RemoteOpenAIServer以提高效率，减少下载的模型文件数量。,https://github.com/vllm-project/vllm/issues/7839
vllm,这是一个用户提出需求的issue，主要涉及到使用slurm cluster而不使用guided decoding的用户。,https://github.com/vllm-project/vllm/issues/7831
vllm,这个issue属于代码优化类型，涉及更新压缩张量的生命周期以消除在`create_weights`中使用`prefix`，由于选择方案时需要消除`prefix`，导致需要更新压缩张量以匹配其他量化方法的生命周期。,https://github.com/vllm-project/vllm/issues/7825
vllm,这是一个版本更新的issue，主要涉及的对象是软件的版本号。,https://github.com/vllm-project/vllm/issues/7823
vllm,这是一个优化建议类型的issue，主要涉及模型测试的重新组织。由于超时错误，根据给出的指导重新分裂模型测试，同时将分布式基本正确性和模型测试移动到相应的文件中，以避免测试逻辑的碎片化。,https://github.com/vllm-project/vllm/issues/7820
vllm,这个issue类型是对Core部分的修改请求，主要涉及到Multi Step Scheduling中的Chunked Prefill支持，用户提出了添加Force Single Step和Ignore Prefill策略的需求。,https://github.com/vllm-project/vllm/issues/7814
vllm,这是一个用户提出需求的类型，主要涉及vllm中的seed参数使用方式。由于用户注意到在vllm中存在两个不同的seed参数，分别在vllm.LLM和vllm.SamplingParams中，因此想了解这两者之间的区别。,https://github.com/vllm-project/vllm/issues/7812
vllm,这是一个用户提出需求的issue，主要对象是希望LLM先回答FAQs，可能是因为用户希望首先通过LLM解决问题或获取答案。,https://github.com/vllm-project/vllm/issues/7809
vllm,这是一个更新请求，涉及到更新`qqq`以使用`vLLMParameters`。这个更新请求可能是为了优化算法或者适应新的参数配置方式。,https://github.com/vllm-project/vllm/issues/7805
vllm,这是一则需求提出的issue，主要涉及的对象是更新`marlin`以使用`vLLMParameters`，添加了运行Marlin通道分组量化模型的权重加载TP测试。,https://github.com/vllm-project/vllm/issues/7803
vllm,这是一个用户提出需求的issue，主要涉及请求支持deepseek-gptq版本。由于缺乏对deepseek-gptq版本的支持，用户提出需要该版本的支持。,https://github.com/vllm-project/vllm/issues/7799
vllm,这是一个功能需求提议，涉及主要对象为优化内核性能，通过启用FlashInfer后端以提高FP8 KV Cache的性能。,https://github.com/vllm-project/vllm/issues/7798
vllm,这是一个用户提出需求的issue，主要涉及vllm下的vision-language model，用户想要在模型forward完成后hook一些特征，但不清楚如何获取这些特征。,https://github.com/vllm-project/vllm/issues/7795
vllm,这是一个用户提出需求的issue，主要涉及VLLM不支持Snowflake Arctic Embed家族的嵌入模型。原因是目前只有Mistral嵌入模型得到支持。,https://github.com/vllm-project/vllm/issues/7792
vllm,这个issue类型是功能需求，该问题单涉及的主要对象是神经魔术预量化W8A8/16模型的TPU后端。该需求可能是为了实现在TPU后端上启用神经魔术预量化W8A8/16模型的检查点而提交。,https://github.com/vllm-project/vllm/issues/7790
vllm,这是一个改进提案，主要对象是LLM引擎，旨在为LLM引擎添加多步骤支持。,https://github.com/vllm-project/vllm/issues/7789
vllm,这是一个功能需求的issue，涉及的主要对象是为Phi-3-vision模型添加支持多图片输入的功能。,https://github.com/vllm-project/vllm/issues/7783
vllm,这是一个关于功能需求的issue，主要涉及LLM在解码阶段使用json格式输出时高吞吐量的问题，可能是由于guided_json参数引起，导致了生成吞吐量低的症状。,https://github.com/vllm-project/vllm/issues/7778
vllm,这个issue类型是用户提出需求，主要涉及的对象是`JetMoE`模型支持。用户提出了关于增加`JetMoE`模型支持的需求。,https://github.com/vllm-project/vllm/issues/7771
vllm,这个issue类型是用户提出需求，主要对象是在vLLM文档中增加更新日志和常见问题解答。这个需求是由于用户希望了解vLLM不同版本之间的变化以及获取常见问题的解答。,https://github.com/vllm-project/vllm/issues/7769
vllm,这是一个特性更新的issue，主要涉及了MoE权重加载和Marlin Fused MoE Kernel的添加。由于扩展了权重加载支持和添加了新的Kernel，可能是为了改进模型性能或功能。,https://github.com/vllm-project/vllm/issues/7766
vllm,这是一个特性更新类的issue，涉及的主要对象是`gptq_marlin_24`，由于需要更新quantization来使用`vLLMParameters`。,https://github.com/vllm-project/vllm/issues/7762
vllm,这个issue类型是建议增加功能，主要涉及将Llama405B模型包含在夜间基准测试中。原因是成本，由于需要1624个H100s，可能导致Akash.Network考虑提供基础设施。,https://github.com/vllm-project/vllm/issues/7761
vllm,这是一个功能需求的issue，主要涉及添加更多的百分位数和延迟信息，以便更好地评估吞吐量性能。,https://github.com/vllm-project/vllm/issues/7759
vllm,该issue类型为功能开发/问题解决，涉及到使用NATS消费任务并将结果返回的功能开发。,https://github.com/vllm-project/vllm/issues/7758
vllm,这是一个功能需求的issue，主要涉及到为torch添加自定义操作以进行all_reduce。导致问题的原因是当前实现中混合了inplace和outofplace内核，导致无法在torch编译中正常运行。,https://github.com/vllm-project/vllm/issues/7755
vllm,这是一个性能优化类的issue，主要涉及chunked prefill和prefix caching的同时启用。原因是为了简化处理部分块时的逻辑。,https://github.com/vllm-project/vllm/issues/7753
vllm,该issue为用户提出需求，主要涉及VLLM与`Formatron`的集成，并由于Python解释器开销过大而希望通过集成Rust后端的`Formatron`来改善性能。,https://github.com/vllm-project/vllm/issues/7752
vllm,这是一个性能优化的提案，但用户在比较llama38binstruct与mlpspeculator版本的性能时未看到任何提升，询问原因并询问关于GPU内存释放的问题。,https://github.com/vllm-project/vllm/issues/7748
vllm,这是一个关于功能改进的issue，主要涉及到了异步后处理和多步执行的结合。,https://github.com/vllm-project/vllm/issues/7743
vllm,这是一个用户提出需求的 issue，主要涉及使用vllm时需要等待每个预测响应，可能是由于需求使用先前的预测结果来补充后续预测而导致。,https://github.com/vllm-project/vllm/issues/7741
vllm,这个issue是一个特性请求，主要涉及vllm不支持phi3.5模型的多图像功能，导致用户提出需要改进的需求。,https://github.com/vllm-project/vllm/issues/7740
vllm,这是一个feature请求issue，主要涉及的对象是将Mistral Tokenization集成到模型中，以提高鲁棒性和聊天编码，在使用官方Mistral模型时出现的编码/解码问题和聊天编码问题可能是由于缺乏测试和兼容性所致。,https://github.com/vllm-project/vllm/issues/7739
vllm,这个issue属于用户提出需求类型，主要对象是vllm的cli（命令行界面），用户提出为什么vllm cli没有提供config参数的问题。,https://github.com/vllm-project/vllm/issues/7737
vllm,这是一个用户提出需求的类型的issue，主要涉及vLLM模型中如何在推理过程中访问注意力矩阵或者KVCache，用户想了解是否有类似transformers包中output_attentions=True参数的选项。,https://github.com/vllm-project/vllm/issues/7736
vllm,这是一个用户提出需求的issue，主要涉及的对象是BitNet模型支持。由于开源BitNet实现提供了独特的tokenizer和模型架构，需要引入一个新的BitNet模型来解决问题。,https://github.com/vllm-project/vllm/issues/7725
vllm,这是一个用户提出需求的类型issue，主要涉及使用FP8或其他量化算法对Minicpmv2_6进行推断，询问如何在vllm中实现，并寻求哪种量化算法具有最佳的吞吐量提升。,https://github.com/vllm-project/vllm/issues/7724
vllm,这是一个测试请求类型的issue，主要对象是代码。由于没有具体内容提及，无法确定具体原因导致了什么样症状的问题。,https://github.com/vllm-project/vllm/issues/7720
vllm,这是一个功能改进类的issue，主要涉及规范解码草案模型的默认最大长度设置问题。,https://github.com/vllm-project/vllm/issues/7706
vllm,这个issue类型是需求提出，主要涉及的对象是Dockerfile的优化和参数传递，用户提出了希望清理整理Dockerfile、优化依赖安装和传递Python版本等需求。,https://github.com/vllm-project/vllm/issues/7705
vllm,这是一个用户提出需求的issue，主要涉及vllm和tr的应用对齐问题，用户询问在使用过程中是否需要对prompt进行特殊处理，可能是由于文档和实际应用中的格式要求不一致引起的。,https://github.com/vllm-project/vllm/issues/7702
vllm,这个issue类型是关于功能增强的需求，主要涉及对象是Machete kernels和GPTQMarlin、CompressedTensorsWNA16，用户提出了添加Machete kernels作为后端内核的请求以及性能优化的相关需求。,https://github.com/vllm-project/vllm/issues/7701
vllm,这是一个用户需求类型的问题，主要涉及对象是vllm推断请求的终止操作。该问题由于无法停止推断请求而导致了症状。,https://github.com/vllm-project/vllm/issues/7700
vllm,这个issue是有关于Kernel的改进，主要涉及支持在并行中支持预填和解码阶段的注意力核心，以及使用CUDA多流来启用分块预填时支持并行操作。原因是为了在启用分块预填标志时更好地利用GPU资源。,https://github.com/vllm-project/vllm/issues/7696
vllm,这是一个关于添加`jinja2`作为显式构建要求的需求，涉及到Machete核心的构建过程中产生的问题。,https://github.com/vllm-project/vllm/issues/7695
vllm,这是一个用户提出需求的issue，主要涉及vllm在spot instances上进行批量推断时如何定期备份处理结果，提出了如何避免由于instance被驱逐而导致进度丢失的问题。,https://github.com/vllm-project/vllm/issues/7694
vllm,这是一个建议（RFC）类型的issue，主要涉及将Ascend NPU作为一个新的后端添加到VLLM中。由于许多用户希望在Ascend NPU上使用VLLM，因此提出了这个建议。,https://github.com/vllm-project/vllm/issues/7692
vllm,该issue类型为功能需求提议，涉及主要对象为vllm模型的加载和预填充。由于LLM推理请求每秒不固定，需要按需启动vllm引擎，因此需要重叠模型加载和预填充，尤其是对于加载时间较长的大型模型。,https://github.com/vllm-project/vllm/issues/7690
vllm,该issue是关于更新文档的，主要对象是OpenVINO。问题类型是文档修改。原因是更新了文档内容且修复了markdown渲染不起作用的问题。,https://github.com/vllm-project/vllm/issues/7687
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm下的Vision Language Models。由于当前vllm仅支持语言模型而不支持视觉模型，导致出现了Pipeline parallelism的NotImplementedError错误，用户希望增加对视觉语言模型的支持以实现更高效的工作负载扩展和模型性能维护。,https://github.com/vllm-project/vllm/issues/7684
vllm,这是一个关于测试的需求提出类型的issue，主要涉及vllm项目的后端配置和CI服务相关，用户提出了如何添加OpenVINO后端测试的问题。,https://github.com/vllm-project/vllm/issues/7683
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是添加nvidia相关的库到collect env。可能由于当前的环境中缺少nvidia相关的库，用户需要将其添加进来。,https://github.com/vllm-project/vllm/issues/7674
vllm,这是一个关于持续集成的功能需求。,https://github.com/vllm-project/vllm/issues/7667
vllm,这是一个提出需求的issue，主要涉及vLLM中前端和核心部分的logits processor构建，要求通过相关请求参数在`LLMEngine`中构建标准的logits processors，以提高性能并简化用户体验。,https://github.com/vllm-project/vllm/issues/7666
vllm,这个issue是一个用户提出需求的类型，主要涉及的对象是vllm的GGUF quantization with tensor parallelism功能。由于当前版本不支持GGUF quantization与tensor parallelism的结合，导致用户遇到数值错误的问题并请求增加该功能。,https://github.com/vllm-project/vllm/issues/7662
vllm,这个issue属于功能新增请求，主要涉及的对象是ScalarType。由于需要支持无符号零的NAN表示，以及对`float8_e4m3fnuz`的支持，可能是由于当前版本无法满足特定需求而导致的。,https://github.com/vllm-project/vllm/issues/7661
vllm,这是一个用户提出的需求。该问题单涉及的主要对象是vLLM。由于OpenAI现在支持JSONSchema，用户希望vLLM也能支持`response_format.type == 'json_schema'`，以便在两种格式之间轻松切换。,https://github.com/vllm-project/vllm/issues/7656
vllm,这个issue类型是需求提出，涉及主要对象是将mamba_ssm和causal_conv1d kernels迁移到vLLM，由于多次尝试将改进推送到相关仓库失败，导致需要迁移代码以加快内核的开发/优化进度。,https://github.com/vllm-project/vllm/issues/7651
vllm,这是一个功能需求的issue，主要涉及的对象是`LLM`类，由于需要支持流式处理，所以对`.generate()`方法进行了修改。,https://github.com/vllm-project/vllm/issues/7648
vllm,这是一个用户提出需求的issue，主要对象是LLAVA，用户提出需要支持chunked_prefill以提高RLHF中生成步骤的吞吐量。,https://github.com/vllm-project/vllm/issues/7646
vllm,这个issue类型是技术改进，主要对象是Gemmar模型的加速。,https://github.com/vllm-project/vllm/issues/7642
vllm,这是一个功能性需求的issue，涉及对象是在run_batch API中发布Prometheus指标，问题是由于run_batch不在api_server中创建Prometheus客户端，导致无法发布Prometheus指标。,https://github.com/vllm-project/vllm/issues/7641
vllm,这是一个更改请求类型的issue，涉及主要对象是Gemini RoPE模块。由于Gemini RoPE实际上与普通的RotaryEmbedding相同，导致出现无效重复代码的情况。,https://github.com/vllm-project/vllm/issues/7638
vllm,该issue类型为性能优化，主要涉及RotaryEmbedding中的`forward_native2`方法的优化和重构。原因是为了提高在TPU上的运行效率，减少冗余的数据拷贝操作，从而降低延迟。,https://github.com/vllm-project/vllm/issues/7636
vllm,这个issue是关于用户提出的需求，主要对象是vllm服务器/引擎。由于服务器崩溃/失败时，无法轻松判断服务器状态，用户提出希望在失败时能够退出程序以便重新启动。,https://github.com/vllm-project/vllm/issues/7633
vllm,这个issue类型是关于CI/Build的改进，涉及到性能基准文件的重组。由于需要将所有文档、workflow yaml文件和脚本放置在特定的文件夹内，可能是为了更好地组织和管理这些文件。,https://github.com/vllm-project/vllm/issues/7616
vllm,这是一个用户提出需求的issue，主要涉及vllm的安装和容器镜像大小的问题，用户希望获得CPU版本的镜像并减小GPU镜像的大小。,https://github.com/vllm-project/vllm/issues/7609
vllm,这个issue是一个功能提议，主要涉及的对象是将Prefix caching kernel 应用于 Pallas 的TPU后端，由于需要启用基于CUDA和Triton的内核实现。,https://github.com/vllm-project/vllm/issues/7607
vllm,这个issue属于功能改进类型，主要涉及prefix cache hit rate 和evictor v2 的优化。由于更新last access time后不必`.move_to_end`会导致性能提升、但v1和v2性能差异仍需进一步调查。,https://github.com/vllm-project/vllm/issues/7606
vllm,这个issue属于用户提出需求类型，主要涉及Kernel的tuned triton配置。原因可能是用户需要为ExpertsInt8添加经过优化的triton配置文件。,https://github.com/vllm-project/vllm/issues/7601
vllm,这个issue类型为文档更新，主要涉及更新支持量化的硬件表格，由于LaTex不支持某些Unicode字符，导致文档变更可能会破坏PDF构建。,https://github.com/vllm-project/vllm/issues/7595
vllm,这是一个用户需求提出的类型的issue，主要涉及到对vllm项目中的benchmarking脚本扩展的需求。由于当前的benchmark脚本无法测试针对speculative decode性能，用户提出了增加一个benchmark_spec_decode.py脚本的建议。,https://github.com/vllm-project/vllm/issues/7586
vllm,这是一个用户提出需求的issue，主要涉及的对象是如何在vllm中开启并发推理。原因是当前环境下的PyTorch版本不支持并发推理，导致用户无法进行此操作。,https://github.com/vllm-project/vllm/issues/7578
vllm,这是一个优化类的issue，主要对象是将quantization cpu offloading tests从fastcheck移出，解决了Fastcheck运行时间过长的问题。,https://github.com/vllm-project/vllm/issues/7574
vllm,这是一个用户提出需求的issue，主要涉及于使用uvloop与zmq解耦前端。原因是uvloop比原生Python asyncio事件循环实现更快，需要在启动基于zmq的RPC服务器的引擎时显式启用它。,https://github.com/vllm-project/vllm/issues/7570
vllm,该issue属于功能需求提出，主要涉及API对推测解码和前缀缓存的控制。原因是用户希望在运行时能够控制推测解码，而当前只能在启动时进行设置。,https://github.com/vllm-project/vllm/issues/7569
vllm,这是一个用户提出需求的Issue，主要涉及 Automatic Prefix Caching 和 Truncating 的问题。由于上下文长度限制导致的输入截断，使得 Automatic Prefix Caching 失效，提出了可能实现 Context Shifting 来解决此问题的需求。,https://github.com/vllm-project/vllm/issues/7560
vllm,这是一个需求提出类型的issue，主要涉及支持LlavaNextVideo模型的相关功能和API。该issue的产生可能是由于缺乏支持特定视频输入类型和其他功能导致的。,https://github.com/vllm-project/vllm/issues/7559
vllm,这是一个提出需求的类型，主要涉及 vLLM 对视频输入的支持。该需求由于目前模型对图像和视频输入的处理方式不同而提出，希望提供新的 API 和推理支持来处理视频输入。,https://github.com/vllm-project/vllm/issues/7558
vllm,这个issue类型是用户提出需求，主要涉及对象是vllm中的constrained decoding参数设置，由于用户在SamplingParams中没有找到相关参数导致无法应用constrained decoding进行离线推断。,https://github.com/vllm-project/vllm/issues/7551
vllm,这是一个新功能提议的issue，涉及到对 speculative decode worker 进行扩展，以实现新的 decode 策略。,https://github.com/vllm-project/vllm/issues/7549
vllm,该issue属于用户提出需求，主要涉及API KEY authentication功能，用户希望能够为模型生成API KEY并追踪用户的使用情况。,https://github.com/vllm-project/vllm/issues/7547
vllm,这是一个用户需求问题，主要涉及到使用vllm模型时在分布式设置下无法完全访问模型权重的问题。这可能是由于tp=2参数限制导致的。,https://github.com/vllm-project/vllm/issues/7543
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是vllm的异常处理功能。由于vllm启动的server在遇到错误时会直接挂断，用户提出了希望支持异常处理的需求。,https://github.com/vllm-project/vllm/issues/7542
vllm,该issue类型为功能需求，涉及主要对象是自定义操作注册和torch操作。由于需要注册自定义操作和将其从torch操作中使用，用户可能遇到相关操作在使用过程中的问题或需要支持和指导。,https://github.com/vllm-project/vllm/issues/7536
vllm,这是关于代码重构和逻辑优化的issue，主要涉及到`MultiModalConfig`的初始化和性能优化。由于之前对所有模型都初始化`MultiModalConfig`，导致开发者容易混淆，所以需要进行重构并将`MultiModalConfig`作为`ModelConfig`的属性进行处理。,https://github.com/vllm-project/vllm/issues/7530
vllm,这是一个功能需求的issue，涉及到vLLM中的Multi-step scheduling功能。原因是Multi-step scheduling与Chunked Prefill功能相互作用复杂，需要制定适当的调度策略以解决问题。,https://github.com/vllm-project/vllm/issues/7528
vllm,这个issue属于功能增强类型，主要涉及的对象是VLLM中的MoE权重加载和新增Marlin MoE内核。产生这个问题的原因是需要扩展权重加载以支持分组和每通道权重量化，同时更新MoE模型以使用更新的权重加载。,https://github.com/vllm-project/vllm/issues/7527
vllm,这是一个需求报告，主要涉及到构建系统的定制化和动态依赖关系。这个问题是由于当前的构建设置在不同的Dockerfile中使用不同的方法和依赖关系管理方式，导致过时的构建方法仍在使用，希望通过更新构建方式和统一依赖管理来解决这个问题。,https://github.com/vllm-project/vllm/issues/7525
vllm,该issue类型为用户提出需求，主要涉及的对象是针对vllm测试套件中的GPU内存利用问题。由于默认情况下vllm会占用整个GPU的内存，导致无法在pytestxdist中并行运行多个测试，可能会出现OOM错误；用户提出需要一个基于GB的替代方案来解决GPU内存利用问题。,https://github.com/vllm-project/vllm/issues/7524
vllm,这是一个关于提出新功能的issue，主要涉及的对象是vLLM的Context Parallelism。,https://github.com/vllm-project/vllm/issues/7519
vllm,这个issue类型是关于性能优化建议，主要涉及到prefix caching的性能提升的讨论。原因可能是对prefix caching在模型执行中的作用机理理解不清导致的。,https://github.com/vllm-project/vllm/issues/7518
vllm,该issue类型为用户提出需求，并主要涉及vllm中的嵌入模型。这个问题的出现可能是由于用户在集成大型嵌入模型时遇到了困难，需要指导如何在vllm中进行推理操作。,https://github.com/vllm-project/vllm/issues/7506
vllm,此issue属于需求提出，主要对象是测试脚本。由于未覆盖到cudagraph，在测试中出现了问题或者需要进一步测试。,https://github.com/vllm-project/vllm/issues/7501
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm库，用户想知道是否有选项可以减少GPU内存使用量，因为当前使用默认设置时，GPU内存占用较大可能会导致OOM错误。,https://github.com/vllm-project/vllm/issues/7495
vllm,这是一个用户提出需求的类型的issue，主要涉及vllm的小型模型加载问题。由于PagedAttention的存在，导致小模型无法被正常加载，用户希望能够在测试时使用小型模型。,https://github.com/vllm-project/vllm/issues/7490
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是要发布NVIDIA Meetup公告。由于issue内容为空，用户可能正在寻求发布NVIDIA Meetup的方式或者寻求相关帮助。,https://github.com/vllm-project/vllm/issues/7483
vllm,该issue是关于提出需求的，主要涉及添加一个类似于FastChat的路由服务器来为多个模型提供中心端点。由于需要实现一种可用于设置多个模型并连接到统一路由器服务器的方案，以解决FAQ中的一个问题。,https://github.com/vllm-project/vllm/issues/7480
vllm,这个issue是关于功能需求的，主要涉及VLLM是否支持Falcon Mamba模型，并询问何时将支持。这可能是由于用户尝试使用Falcon Mamba模型时遇到问题，希望VLLM能够添加对其的支持。,https://github.com/vllm-project/vllm/issues/7478
vllm,这是一个用户提出需求的类型，该问题涉及的主要对象是vllm的部署方式。由于当前无法使用.env文件部署vllm，用户提出希望vllm能够支持使用.env文件来配置。,https://github.com/vllm-project/vllm/issues/7465
vllm,这是一个用户提出需求的类型，主要涉及vllm实现early-stopping功能。这个问题的产生可能是用户希望在计算/内存资源不足时能主动中止请求，而不需要用户手动退出请求。,https://github.com/vllm-project/vllm/issues/7461
vllm,这是一个关于需求和最佳实践的问题，主要涉及版本发布说明和模型部署方法。用户希望了解如何发布新版本的说明以及如何指导用户部署具有更高性能的模型。,https://github.com/vllm-project/vllm/issues/7460
vllm,这是一个功能需求类型的issue，主要涉及的对象是TPU支持多主机推断。这个问题的原因是全局排名和世界大小需要转换为本地排名和本地世界大小。,https://github.com/vllm-project/vllm/issues/7457
vllm,这是一个功能需求的issue，主要涉及的对象是OpenAI服务器的chat completions API。这个需求是为了添加对prompt_logprobs选项的支持，并通过详细说明了实现细节。,https://github.com/vllm-project/vllm/issues/7453
vllm,这是一个功能增强类型的issue，主要涉及的对象是vllm worker、openai client和benchmark_serving.py，用户希望添加Torch profiler支持来帮助性能调优。,https://github.com/vllm-project/vllm/issues/7451
vllm,这个issue属于功能需求提议，涉及vllm项目集成flash-infer FP8 KV Cache Chunked-Prefill功能。原因可能是为了提升模型性能或者改进模型训练过程。,https://github.com/vllm-project/vllm/issues/7450
vllm,这是一个特性需求的issue，涉及的主要对象是VLLM中的跨注意力并行QKV计算，由于跨注意力层需要从前一解码器层次的隐藏状态计算Q，并从编码器输出隐藏状态计算KV，目前的解决方法效率低下，需要构建一个新的QKV并行线性层。,https://github.com/vllm-project/vllm/issues/7448
vllm,该issue属于用户提出需求类型，主要对象是encoder-decoder模型，由于目前在decode阶段不支持cuda graph，用户希望增加这一支持以提高decode阶段速度。,https://github.com/vllm-project/vllm/issues/7447
vllm,这是一个功能需求issue，主要涉及在vLLM中添加支持音频语言模型的基础设施。,https://github.com/vllm-project/vllm/issues/7446
vllm,该issue类型为文档补充需求，主要对象是LLM Compressor的FP8和INT8 W8A8量化。,https://github.com/vllm-project/vllm/issues/7444
vllm,该issue属于用户提出需求类型，主要涉及Gemina 2模型的sliding window attention支持。用户询问是否有计划支持奇数层的sliding window attention，以及在这些层不使用sliding windows会对性能造成什么样的影响。,https://github.com/vllm-project/vllm/issues/7442
vllm,该issue类型属于功能需求提议，主要涉及到CI流程中的Models Test和Vision Language Models Test的拆分。由于Models Test执行时间长导致CI过程变慢，需要进行拆分以提升效率。,https://github.com/vllm-project/vllm/issues/7439
vllm,该issue属于用户提出需求类型，涉及主要对象为vLLM的模型架构插件，由于vLLM需要支持外部模型架构插件，使得用户可以通过创建独立包并安装在与vLLM相同的环境中来实现对模型架构的定制。,https://github.com/vllm-project/vllm/issues/7438
vllm,这个issue类型属于代码优化类，涉及主要对象为项目fp8，由于需要更新以使用`vLLMParameters`，这可能由于代码结构调整或性能优化需求所致。,https://github.com/vllm-project/vllm/issues/7437
vllm,这是一个新功能需求类型的issue，主要涉及的对象是vLLM模型。,https://github.com/vllm-project/vllm/issues/7436
vllm,该issue类型为需求提出，主要涉及的对象是CI（持续集成）。,https://github.com/vllm-project/vllm/issues/7433
vllm,这是一个功能需求类型的 issue，主要涉及持续集成 (CI) 测试流程的优化，目的是避免对相同提交运行相同的测试。,https://github.com/vllm-project/vllm/issues/7427
vllm,这是一个关于添加插件系统实现的功能需求，该问题涉及的主要对象为插件系统。,https://github.com/vllm-project/vllm/issues/7426
vllm,这是一个更新issue，主要涉及的对象是`awq`和`awq_marlin` quant方法，需要更新为使用`vLLMParameters`进行权重加载。,https://github.com/vllm-project/vllm/issues/7422
vllm,这个issue类型是功能需求，主要涉及支持新模型LLaVA-OneVision的问题，原因是希望将新模型与现有模型进行混合以实现更多功能。,https://github.com/vllm-project/vllm/issues/7420
vllm,这是一个用户提出需求的类型，询问如何在OpenVINO中使用Intel GPU。,https://github.com/vllm-project/vllm/issues/7418
vllm,这是一个功能增强的issue，主要涉及`GB`常量的合并和允许浮点数`GB`参数。,https://github.com/vllm-project/vllm/issues/7416
vllm,该issue是一个用户提出需求的类型，主要涉及到`fused_moe` triton kernel的优化，请求支持W8A16 with Int8，并在Ampere/Ada lovelace/Hopper上运行。,https://github.com/vllm-project/vllm/issues/7415
vllm,这是一个用户提出需求的issue，主要涉及到GPU利用率和推理执行的优化。由于GPU kernel启动仅需要很短的时间，提出了在等待GPU完成推理时进行其他操作以提高性能的建议。,https://github.com/vllm-project/vllm/issues/7408
vllm,这个issue是关于CI/Build的改进，主要涉及到vLLM的资源管理。原因是之前使用的存储桶不属于vLLM组管理，导致需要迁移图像资源到新的由vLLM组管理的存储桶。,https://github.com/vllm-project/vllm/issues/7407
vllm,这是一个更新请求，涉及的主要对象是`awq`和`awq_marlin`的量化方法，用户寻求的帮助是更新这两种方法以使用`vLLMParameters`进行权重加载。,https://github.com/vllm-project/vllm/issues/7406
vllm,这是一个功能需求类型的issue，主要涉及的对象是collect env。用户提出了添加commit id的需求，以便更好地识别用户所使用的代码版本。,https://github.com/vllm-project/vllm/issues/7405
vllm,这个issue类型是需求提出，主要涉及vLLM的文档内容，用户希望添加构建vLLM时使用VLLM_TARGET_DEVICE=empty的指南。,https://github.com/vllm-project/vllm/issues/7403
vllm,这是一个关于技术优化的issue，主要涉及VLLM项目中的detokenization处理。这个问题针对的是优化处理效率，通过将detokenization移出关键模型路径，转移到前端处理逻辑。,https://github.com/vllm-project/vllm/issues/7402
vllm,这个issue类型是性能优化，主要涉及的对象是crossattention QKV计算的效率问题。由于crossattention需要从两个输入中计算QKV，当前方法存在效率问题，导致计算过程中存在冗余的GEMM操作。,https://github.com/vllm-project/vllm/issues/7397
vllm,这是一个优化测试过程的issue，主要涉及LoRA测试的时间消耗过大。,https://github.com/vllm-project/vllm/issues/7396
vllm,这是一个关于增加交叉注意力并行QKV计算效率的功能需求，涉及主要对象为VLLM库中的QKVParallelLinear模块。由于跨注意力层需要在解码阶段只计算Q以及在填充阶段同时计算Q和KV，目前的不足之处是需要多次应用QKVParallelLinear层来实现，因此提出了引入交叉注意力QKV并行线性层以优化计算效率。,https://github.com/vllm-project/vllm/issues/7395
vllm,这个issue是关于在vLLM中添加Triton实现以支持AWQ模型的改进，不是bug报告。,https://github.com/vllm-project/vllm/issues/7386
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是RequestMetrics，在vllm中添加了与preempt相关的新指标。,https://github.com/vllm-project/vllm/issues/7384
vllm,这个issue是提出一个新功能需求，主要涉及的对象是`LLMEngine`和`AsyncLLMEngine` API。由于当前API在每个步骤返回所有序列的累积输出和相关数据，而实际上对于`LLM.generate`或OpenAI服务器API，只需要最终输出或增量数据，因此提出增加一个`output_kind`参数来控制返回内容类型。,https://github.com/vllm-project/vllm/issues/7381
vllm,这是一个功能需求的问题单，主要对象是block manager v2，由于chunked prefill无法与block manager v2一起使用，导致无法正常工作。,https://github.com/vllm-project/vllm/issues/7371
vllm,这个issue是关于性能优化和功能改进的讨论，涉及到VLLM系统中KVCACHE的数据传输效率问题。由于非连续的GPU内存分布导致循环传输效率低下，用户提出使用CUDA操作并行化处理以提高性能。,https://github.com/vllm-project/vllm/issues/7370
vllm,这是一个用户提出需求的issue，主要涉及的对象是使用vllm进行推断时出现的错误。该问题由于未能正确使用AsyncLLMEngine代替LLMEngine导致。,https://github.com/vllm-project/vllm/issues/7367
vllm,这是一个用户需求类型的RFC（Request for Comments）issue，主要涉及vLLM对encoder/decoder模型的支持。这个issue提出了希望社区能够贡献PR以提高vLLM的encoder/decoder功能与decoder-only功能的成熟度，并分析了现有支持及未来需要新增支持的模型和特性。,https://github.com/vllm-project/vllm/issues/7366
vllm,这是一个需求提出的issue，主要涉及的对象是vLLM的RPC服务器，由于要在不同的机器上部署API服务器和vLLM引擎服务器，因此需要保持RPC服务器的TCP协议连接。,https://github.com/vllm-project/vllm/issues/7365
vllm,这个issue类型为代码优化，主要涉及的对象是项目中的性能问题，由于先前代码提交未完全解决审查意见和一些小的改进，导致需要进一步完善及优化。,https://github.com/vllm-project/vllm/issues/7364
vllm,这个issue类型为功能需求，涉及主要对象是新添加的解码策略参数，用户提出需要使用这个新参数来改进动态并行策略。,https://github.com/vllm-project/vllm/issues/7363
vllm,这个issue属于用户提出需求类型，主要对象是实现并行评分器功能。由于需要获取下一个token的特征并在单个前向请求中运行目标模型并返回下一个token的得分，因此用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/7362
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是模型执行器和模型运行器。由于需要实现一个并行解码算法的未来特性请求，需要修改当前的实现以便在需要时返回一系列下一个token而不仅仅是最后一个token。,https://github.com/vllm-project/vllm/issues/7361
vllm,这是一个用户提出的需求类型的 issue，主要涉及实现从提示中随机抽取的令牌创建草案，原因是为了根据先前的草案信息和已接受的令牌数量，将新的令牌附加到动态草案中。,https://github.com/vllm-project/vllm/issues/7359
vllm,这是一个用户提出需求的issue，主要涉及的对象是保存推测解码状态。由于需要调试和训练自定义的推测模型，用户希望能够接收到推测解码算法的内部状态信息。,https://github.com/vllm-project/vllm/issues/7358
vllm,该issue类型为文档改进，涉及主要对象是InternVL示例。由于markdown渲染问题，导致需要在PR描述中添加`stop_token_ids`。,https://github.com/vllm-project/vllm/issues/7354
vllm,这个issue属于用户需求提出类型，主要涉及vllm.LLM引擎，由于vllm engines无法使用连续的批处理，用户提出了希望为vllm.LLM引擎启用连续批处理或允许在线更新模型参数的需求。,https://github.com/vllm-project/vllm/issues/7353
vllm,该issue类型为需求提案，主要涉及实现新的 speculative decode dynamic parallel strategy。原因是团队需要实现该功能来提高模型的性能和效率。,https://github.com/vllm-project/vllm/issues/7351
vllm,该issue类型为用户提出需求，主要对象是vllm的命令行接口。由于用户想要通过配置文件来启动vllm，但当前的CLI并不提供这样的功能，用户希望通过传入配置文件路径的方式进行配置。,https://github.com/vllm-project/vllm/issues/7350
vllm,这个issue类型为功能需求，涉及对象为RequestMetrics，用户要求在vllm中添加预抢占指标，以监视资源不足时的预抢占情况。,https://github.com/vllm-project/vllm/issues/7346
vllm,这个issue是一个功能需求提出，主要对象是为vllm添加对于draft model的量化配置支持。 由于当前无法指定量化方法，导致在设置INT4 gptqbased草稿模型时可能会出现使用`gptq_marlin`配置量化并引发错误的问题。,https://github.com/vllm-project/vllm/issues/7343
vllm,这是一个关于优化模型前向传播中GPU内存重复利用的Issue，主要对象是vLLM模型。原因是在模型前向传播期间，存在大量的临时GPU内存分配给激活张量，导致资源浪费，缩减了可用缓存空间，并降低了能够同时运行的批次和上下文长度数量。,https://github.com/vllm-project/vllm/issues/7341
vllm,这个issue是关于性能优化的需求类型，主要涉及的对象是TPU编译过程。由于编译时间较长，用户提出使用mark_dynamic来减少编译时间的问题。,https://github.com/vllm-project/vllm/issues/7340
vllm,此issue为功能需求，主要涉及到vllm模型在小型模型推理效率方面存在瓶颈，用户希望了解为什么vllm在这方面不如SGLang和TensorRT，并询问是否会改进。,https://github.com/vllm-project/vllm/issues/7339
vllm,这个issue是一个功能需求类型，用户提出需求，希望支持声音模型像cosyVoice一样。,https://github.com/vllm-project/vllm/issues/7335
vllm,这是一个特性更新的issue，主要涉及Fused MoE权重加载的更新。由于AWQ模型具有转置权重，需要对权重加载逻辑进行复杂处理，而旧版的expert_params_mapping需要针对fp16和fp8进行重构，这导致了调用fused_moe时出现问题。,https://github.com/vllm-project/vllm/issues/7334
vllm,这是一个用户在询问需求方面的问题，主要涉及vllm是否支持动态量化以及相关功能的现状和未来计划。,https://github.com/vllm-project/vllm/issues/7333
vllm,这个issue类型是用户提出需求的问题，主要对象是vllm模型的流式输出。产生这个问题的原因可能是用户对流式输出的设置不理解，希望直接输出模型生成的内容。,https://github.com/vllm-project/vllm/issues/7330
vllm,这个issue类型是在进行功能增强或优化的讨论，并且涉及到输入预处理的问题。,https://github.com/vllm-project/vllm/issues/7329
vllm,该issue类型为用户提出需求，主要涉及对象为在8-GPU A800（80G）服务器部署qwen2-7b模型作为API时，如何配置参数以支持更高并发性能。该问题的根本原因在于如何设置参数以实现更好的并发性能利用8个GPU。,https://github.com/vllm-project/vllm/issues/7325
vllm,这个issue类型是代码改进建议，主要涉及的对象是使用`torch.testing.assert_close`来改进测试。由于目前当`allclose`测试失败时，无法准确了解错误的性质，因此建议使用更易于调试和诊断的方法。,https://github.com/vllm-project/vllm/issues/7324
vllm,这个issue属于技术优化，主要涉及的对象是gptq_marlin kernel，问题由于需要使用 ScalarType 来代替 num_bits 来进行 dispatch，以支持C++17标准下将标量类型作为模板参数传递。,https://github.com/vllm-project/vllm/issues/7323
vllm,这个issue属于用户提出需求类型，主要涉及对象是VLLM在8xA100上运行DeepSeekCoderV2InstructFP8的支持问题。原因是VLLM使用的Triton不支持FP8 Marlin mixedprecision，导致用户在寻求解决方案。,https://github.com/vllm-project/vllm/issues/7322
vllm,这是一个功能需求问题，主要涉及bitsandbytes库的支持，用户提出需要添加对特定模型的支持。,https://github.com/vllm-project/vllm/issues/7320
vllm,这是一个用户提出需求的issue，主要对象是OpenAI API和VLLM，由于添加了Enable_prefix_caching参数，导致用户不知道如何计算当前处理的token数量。,https://github.com/vllm-project/vllm/issues/7318
vllm,这是一个功能改进（Feature Improvement）类型的issue，主要涉及到GPTQ Marlin核心代码的编译时间过长。原因是`gptq_marlin.cu`文件过于庞大和使用了大量模板，导致开发者编译所需时间过长。,https://github.com/vllm-project/vllm/issues/7317
vllm,这是一个文档更新类的issue，涉及到更新项目的readme文件。,https://github.com/vllm-project/vllm/issues/7316
vllm,这是一个用户提出需求的issue，主要涉及支持attention backend with FlexAttention。由于PyTorch版本需要升级到2.5.0，目前无法使用该功能。,https://github.com/vllm-project/vllm/issues/7315
vllm,这个issue类型是请求赞助，涉及主要对象是Skywork AI。,https://github.com/vllm-project/vllm/issues/7314
vllm,"这个issue是一个需求提出类型，主要对象是为了在prompt adapter中支持安装 ""peft""。由于缺乏此功能，用户提出了需要支持 ""peft"" 安装的需求。",https://github.com/vllm-project/vllm/issues/7311
vllm,该issue类型为用户需求，涉及主要对象为issue模板的展现形式，由于collect_env.py输出较大，需要将其放置在一个可以折叠的detail块中以提高可读性。,https://github.com/vllm-project/vllm/issues/7310
vllm,这个issue是一个功能需求类型的问题，主要涉及的对象是在PyTorch中的测试模块。由于使用`torch.allclose`不够好诊断测试失败，用户建议使用`torch.testing.assert_close`作为建议实践。,https://github.com/vllm-project/vllm/issues/7307
vllm,这是一个用户提出需求的类型，主要涉及保存分片状态的问题。可能由于脚本执行时出现卡住的情况，用户想要使用VLLM保存分片的需求。,https://github.com/vllm-project/vllm/issues/7304
vllm,该issue属于改进建议类型，主要对象是Dockerfile.cpu的构建优化，通过使用新的安装方式和缓存绑定挂载等措施来提高构建效率。,https://github.com/vllm-project/vllm/issues/7298
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm项目的构建系统。原因是NVIDIA Jetson Orin设备使用CUDA计算能力8.7，需要将其添加到白名单中。,https://github.com/vllm-project/vllm/issues/7293
vllm,该issue类型为用户提出需求，涉及到Beam Search及其缺乏控制多样性的参数。,https://github.com/vllm-project/vllm/issues/7289
vllm,这个issue是关于[Frontend]的需求提出，希望移除lora的max_num_batched_tokens限制。,https://github.com/vllm-project/vllm/issues/7288
vllm,"这个issue属于用户提出需求类型，主要对象是""HammingDiversityLogitsProcessor""，用户提出了关于增加新功能的需求。",https://github.com/vllm-project/vllm/issues/7287
vllm,这个issue类型是需求提出，主要涉及的对象是`merge_async_iterators`工具函数。问题是由于新增的`is_cancelled`参数导致不兼容性，因此需要将其变为可选参数以保持向后兼容性。,https://github.com/vllm-project/vllm/issues/7282
vllm,这是一个更新请求，主要涉及的对象是`gptq_marlin`参数。这个更新是为了使用`vLLMParameters`简化线性层权重加载，同时添加`PackedColumnParameter`以支持不带行并行性的紧凑参数。,https://github.com/vllm-project/vllm/issues/7281
vllm,这是一个功能需求问题，主要涉及构建时是否需要生成每次提交的wheel包，由于每次需要大约810分钟来构建CUDA 12.1 wheel，因此提出不需要每次提交时都生成wheel的建议。,https://github.com/vllm-project/vllm/issues/7278
vllm,这是一个功能需求的issue，主要涉及到VLLM项目代码重用compressedtensors中的压缩和量化配置。造成这个需求的原因可能是为了提高代码复用性和减少重复开发工作。,https://github.com/vllm-project/vllm/issues/7277
vllm,该issue属于文档更新类型，主要涉及更新支持量化硬件目标的表格。可能是为了维护最新的硬件支持信息而提出。,https://github.com/vllm-project/vllm/issues/7276
vllm,这是一个功能需求问题，主要涉及将MiniCPMVQwen2重命名为MiniCPMV2.6，涉及到对逻辑的轻微修改和添加新的支持模型。,https://github.com/vllm-project/vllm/issues/7273
vllm,该issue类型是针对代码质量的PR需求，主要涉及对CUDA kernels或其他compute kernels的变更，由于markdown渲染无法正常工作，需要使用原始的html进行标记。这是一个代码贡献相关的问题。,https://github.com/vllm-project/vllm/issues/7270
vllm,这是用户提出需求的issue类型，主要涉及的对象是对MiniCPM-V2_6模型的支持。由于目前vllm还未支持该模型，用户想知道当前支持模型最接近的是哪一个，并询问支持该模型的难度。,https://github.com/vllm-project/vllm/issues/7267
vllm,这是一个用户提出需求的issue，主要涉及对象为VLLM中的Chroma和Quadrant。由于无法直接在VLLM的openai包装器中使用自定义嵌入函数，导致出现内部函数参数错误或内存错误的问题。,https://github.com/vllm-project/vllm/issues/7265
vllm,该issue类型为用户提出需求，该问题单涉及的主要对象是某个功能的增加。由于用户需要实现某项特定功能，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/7263
vllm,该issue类型为用户提出需求，主要对象是支持更长的max_num_batched_tokens for lora。由于许多模型需要长上下文能力，导致需要支持更长的max_num_batched_tokens。,https://github.com/vllm-project/vllm/issues/7259
vllm,这是一个性能优化类型的issue，主要涉及vLLM的speculative decoding功能。由于当前的评分机制效率低下，导致在speculative decoding中没有或很少性能改进。,https://github.com/vllm-project/vllm/issues/7255
vllm,这是一个需求提议的issue，主要涉及vLLM在 speculative decoding 过程中因为不同的vocab_size而出现的错误。,https://github.com/vllm-project/vllm/issues/7252
vllm,这是一个关于软件依赖更新的问题，涉及到OpenVINO和optimumintel版本更新，由于安装过程中的markdown渲染问题导致的bug。,https://github.com/vllm-project/vllm/issues/7251
vllm,这个issue类型是功能需求，涉及的主要对象是在cuda平台上的分布式系统；该需求是为了避免重复代码，根据用户的请求添加专门的方法。,https://github.com/vllm-project/vllm/issues/7249
vllm,这是一个功能需求的issue，主要对象是`vllm.entrypoints`，由于需要更改类型注解和启用`followimports=silent`，部分解决了Enable mypy type checking引起的问题。,https://github.com/vllm-project/vllm/issues/7248
vllm,该issue类型为功能请求，主要涉及目标是为vLLM引入对RBLN NPU的初始支持。原因是要开发RBLN后端，使其能够与Atom和Rebel等NPU兼容，并支持大型语言模型。,https://github.com/vllm-project/vllm/issues/7247
vllm,这个issue是关于文档的修复和改进请求。原因是markdown渲染在该部分不起作用，需要使用raw html来进行渲染。,https://github.com/vllm-project/vllm/issues/7245
vllm,这是一个需求类型的issue，提出了添加在线推测解码示例的需求。,https://github.com/vllm-project/vllm/issues/7243
vllm,这是一个需求类型的issue，主要涉及到vllm中最大并发请求数量限制的问题，用户希望提高当前默认设置的100。,https://github.com/vllm-project/vllm/issues/7241
vllm,这个issue是一个用户提出需求的类型，主要对象是请求vllm添加对LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct模型的支持。由于这个模型是一个新的架构，用户请求vllm团队添加对该模型的支持。,https://github.com/vllm-project/vllm/issues/7236
vllm,"这个issue类型属于用户提出需求。该问题单涉及的主要对象是注册自定义操作flash attention。由于当前工作正在进行中（""[wip]""），用户可能正在尝试注册自定义操作并遇到一些困难或需要指导。",https://github.com/vllm-project/vllm/issues/7235
vllm,这是一个关于代码优化的issue，涉及到Kernel模块。由于替换了自定义的blockReduce函数，导致性能有微小的变化。,https://github.com/vllm-project/vllm/issues/7233
vllm,该issue类型为技术改进需求，主要涉及的对象是`vllm/core`模块；用户希望通过启用mypy类型检查部分解决关于启用mypy类型检查的问题。,https://github.com/vllm-project/vllm/issues/7229
vllm,这个issue是一个功能需求，主要涉及支持FLUTE量化，由于FLUTE支持各种新的量化技术和模型，希望为vLLM带来新的研究和改进。,https://github.com/vllm-project/vllm/issues/7221
vllm,这是一个需求提出的issue，主要涉及将API与OAI的结构化输出对齐，因为OpenAI API引入了支持结构化输出的功能。,https://github.com/vllm-project/vllm/issues/7220
vllm,该issue是关于更新TensorRT-LLM版本以进行性能比较的文档问题，属于需求类型，主要涉及的对象是TensorRT-LLM版本号。,https://github.com/vllm-project/vllm/issues/7215
vllm,这是一个用户提出需求的类型issue，主要对象为vllm API server和Flask app。由于API server以JSON对象序列形式返回响应，用户想在Flask应用程序中以流的方式显示最终文本，并希望获取关于如何正确切割文本以及实现实时更新屏幕响应的建议。,https://github.com/vllm-project/vllm/issues/7214
vllm,这是一个需求类型的issue，主要涉及的对象是为 Rocm 添加 fp8 Linear Layer，由于 Rocm 使用了 torch 2.5 版本，需要进行相应的数据类型和权重调整。,https://github.com/vllm-project/vllm/issues/7210
vllm,这是一个关于性能优化的issue，主要涉及到Evictor V1和V2中使用prefix caching模式时的性能问题，导致了时间到第一个标记（TTFT）增加和系统变慢。,https://github.com/vllm-project/vllm/issues/7209
vllm,这个issue类型是功能需求，涉及的主要对象是为vLLM项目中的LoRA功能增加MiniCPMV2.5的支持。,https://github.com/vllm-project/vllm/issues/7199
vllm,该issue类型是用户提出需求，主要涉及调整`self.model_config.max_model_len`size来解决GPU内存不足导致的错误。,https://github.com/vllm-project/vllm/issues/7195
vllm,这个issue是用户提出需求，请求支持Qwen-VL模型。,https://github.com/vllm-project/vllm/issues/7192
vllm,这是一个需求提交的issue，主要涉及的对象是VLM中各种ViTs模型。由于更新了ViT版本，导致需要更新对应模型的加载权重部分，以确保正确加载模型。,https://github.com/vllm-project/vllm/issues/7186
vllm,这是一个关于更新 Dockerfile 的 issue，主要涉及到 CPU 版本的 protobuf 安装问题。用户提出此 issue 是因为 LlamaTokenizer 的新行为需要安装 protobuf，并对 .buildkite/runcputest.sh 中的冗余安装进行了优化。,https://github.com/vllm-project/vllm/issues/7182
vllm,这是一个功能需求类型的issue，该问题涉及VLLM模型库中是否支持添加mistral-large模型。原因可能是该模型与最接近的模型之间存在一些技术障碍导致的。,https://github.com/vllm-project/vllm/issues/7177
vllm,这是一个用户提出需求的issue，主要涉及优化Hopper架构的线性核心Kernel。由于Hopper Nvidia发布了新的`wgmma`指令，Marlin (v1)无法达到最高FLOPs，因此引入了这个新的Kernel。,https://github.com/vllm-project/vllm/issues/7174
vllm,这是一个改进性质的issue，主要针对代码优化。原因是为了提高数据传输效率，采用了非阻塞数据传输功能。,https://github.com/vllm-project/vllm/issues/7172
vllm,这是一个需求修改类型的issue，涉及到更新ROCM基础镜像和添加OpenAI服务器入口点。,https://github.com/vllm-project/vllm/issues/7165
vllm,这是一个性能优化类的issue，主要涉及到减少Python对象分配，包括针对特定对象类型的内存优化，以及调度器针对不同情况的优化。,https://github.com/vllm-project/vllm/issues/7162
vllm,该issue类型为性能优化提案，主要涉及最大模型长度参数对LLM类GPU利用的限制问题，提议修改GPU内存分配方式以改善资源利用效率。,https://github.com/vllm-project/vllm/issues/7155
vllm,这是一个关于性能优化的Issue，主要涉及Chunked-Prefill功能在高QPS下对ITL/e2e延迟的性能提升问题。由于VLLM 0.5.3版本和A100 GPU测试结果与预期相差较大，用户希望了解更多测试信息以及相关设置指导。,https://github.com/vllm-project/vllm/issues/7147
vllm,这是一个功能需求提交，涉及LoRA的支持扩展至64以上的Ranks功能。,https://github.com/vllm-project/vllm/issues/7146
vllm,该issue类型为用户提出需求，主要涉及VLLM是否支持将siglip模型作为视觉模型在多模态模型中使用。原因可能是用户需要在特定模型上运行推断，但不清楚如何将其集成到VLLM中。,https://github.com/vllm-project/vllm/issues/7144
vllm,"该issue为用户提出需求类型，主要涉及支持新模型""Mantis""。由于需要支持新的Siglip模型，用户提出了对该模型的支持需求。",https://github.com/vllm-project/vllm/issues/7143
vllm,该issue类型为功能需求，涉及对象为版本更新以及与llama3.1 405b fp8 checkpoint的兼容性确认。由于需要确认特定版本的checkpoint是否与vLLM兼容，故用户目前等待确认。,https://github.com/vllm-project/vllm/issues/7139
vllm,这是一个关于如何构建类似AI-Ask功能的AI知识库的文档问题，提出者想了解如何使用vLLM实现，可能是由于缺乏相关指导文档导致提出帮助请求。,https://github.com/vllm-project/vllm/issues/7133
vllm,该issue类型为功能需求，涉及主要对象为 vllm 中的 run_batch API。由于目前的 run_batch API 只支持对话完成，导致用户需求添加对嵌入的支持及支持输入文件中空行的功能。,https://github.com/vllm-project/vllm/issues/7132
vllm,这是一个用户提出需求类型的问题，主要涉及到需要添加一个插件。由于缺乏详细内容，无法确定具体原因导致的需求。,https://github.com/vllm-project/vllm/issues/7130
vllm,这是一个功能需求的issue，主要涉及VLM核心模块在处理多模态输入时的计算问题。,https://github.com/vllm-project/vllm/issues/7126
vllm,这是一个功能需求类型的issue，主要涉及如何运行gemm2-27b模型的int4量化版本。,https://github.com/vllm-project/vllm/issues/7125
vllm,该issue是用户提出了需求，主要涉及模型架构插件，由于之前的合并请求被拒绝，导致用户需要通过自己的分支来支持特定的场景。,https://github.com/vllm-project/vllm/issues/7124
vllm,这是一个用户提出需求的issue，主要涉及替换调度程序。原因是一些使用案例需要优先考虑其他指标而不是最大吞吐量，例如在不同用户请求之间维持公平性。,https://github.com/vllm-project/vllm/issues/7123
vllm,这是一个功能增强的issue，主要涉及Minicpmv模型在离线推理中添加多图支持。,https://github.com/vllm-project/vllm/issues/7122
vllm,这是一个需求类型的issue，主要涉及collect env的功能。,https://github.com/vllm-project/vllm/issues/7119
vllm,这是一个用户提出需求的issue，主要涉及VLLM模型的代码输出处理。由于存在非并行抽样和非束搜索的情况，用户希望简化输出处理流程。,https://github.com/vllm-project/vllm/issues/7117
vllm,这是一个性能改进的提议，涉及到vLLM中的SequenceGroup和Sequence的概念转换效率问题，由于在函数操作时使用的是SequenceGroup级别，而实际逻辑更适合使用Sequence级别操作，导致代码效率低下且难以维护。,https://github.com/vllm-project/vllm/issues/7116
vllm,这个issue是关于技术改进，主要对象是CI/测试系统的优化，通过新进程方式来执行测试。,https://github.com/vllm-project/vllm/issues/7114
vllm,这个issue主要是一个功能需求提出，主要涉及vLLM下的一些基本自定义操作的优化。,https://github.com/vllm-project/vllm/issues/7110
vllm,这个issue属于性能优化类型，主要涉及SPMD架构和序列化性能优化，用户提出了优化性能、集成功能和运行基准测试等建议。,https://github.com/vllm-project/vllm/issues/7109
vllm,该issue为代码优化建议，主要涉及硬件平台上的is_tpu使用问题，用户建议统一使用current_platform.is_tpu()。,https://github.com/vllm-project/vllm/issues/7102
vllm,这是一个关于测试优化的issue，主要涉及CI和前端，通过使用parametrized fixture进行测试。,https://github.com/vllm-project/vllm/issues/7101
vllm,这是一个文档更新类型的issue，主要涉及MLPSpeculator相关文档内容的更新。由于需要添加MLPSpeculator的相关文档和链接，可能是为了使项目更易于理解和使用。,https://github.com/vllm-project/vllm/issues/7100
vllm,这是一个用户提出的需求性issue，主要涉及了ci以及distributed方面的测试命令合并。由于项目需要优化测试流程，用户提出了合并分布式测试命令的需求。,https://github.com/vllm-project/vllm/issues/7097
vllm,这个issue类型为功能需求，主要对象是代码库中的编译功能。原因是为了让测试变得更加方便。,https://github.com/vllm-project/vllm/issues/7092
vllm,这是一个关于性能优化的issue，涉及VLLM模型的forward时间统计，由于未能在没有同步时正常工作，可能导致了一些潜在的bug或性能问题。,https://github.com/vllm-project/vllm/issues/7089
vllm,这是一个功能需求的issue，涉及到GPTQModel的动态量化功能的支持，主要是为了实现对特定层和模块的量化配置进行覆盖控制。,https://github.com/vllm-project/vllm/issues/7086
vllm,这是一个关于更新CUTLASS到3.5.1版本的问题，涉及的主要对象是代码库中的CUDA核心。原因是CUTLASS 3.5.1 中包含了修复上游`RowBroadcast`的问题，需要更新至最新的main分支以解决相关问题。,https://github.com/vllm-project/vllm/issues/7085
vllm,这个issue是一个[Frontend]类型的需求报告，主要涉及的对象是OpenAI API server。因为缺乏readiness和liveness endpoints导致Kubernetes无法确定vLLM是否准备好接收请求或者是否仍处于运行状态。,https://github.com/vllm-project/vllm/issues/7078
vllm,这是一个关于增加安全层的建议，属于用户提出需求类型的issue，主要涉及的对象是系统安全性。,https://github.com/vllm-project/vllm/issues/7076
vllm,这是一个需求类型的issue，主要涉及的对象是vLLM前端（例如OpenAI API server），用户提出了添加一个abort_request端点的需求。,https://github.com/vllm-project/vllm/issues/7071
vllm,"这个issue是一个不再需要的功能移除请求，涉及的主要对象是一个名为""error_on_invalid_device_count_status""的功能。",https://github.com/vllm-project/vllm/issues/7069
vllm,这个issue是一个关于优化文件大小的杂项类型的PR。,https://github.com/vllm-project/vllm/issues/7066
vllm,这是一个用户提出需求的issue，主要涉及VLLM中的BitAndBytes量化模型无法直接在多GPU上使用，可能由于VLLM当前不支持BNB量化模型在多GPU上运行导致。,https://github.com/vllm-project/vllm/issues/7063
vllm,这个issue是关于对服务基准测试中p90和p95进行更多百分位数的需求，不是关于bug报告。,https://github.com/vllm-project/vllm/issues/7062
vllm,这是一个关于支持新模型的用户提出需求问题，涉及的主要对象是模型`gemma-2-2b-it`，用户希望能够获得更快的推断速度。,https://github.com/vllm-project/vllm/issues/7060
vllm,这个issue属于性能优化类型，主要涉及的对象是mypy的检查过程。这个问题是因为mypy在本地运行时速度过慢，提出了一种优化方案来加快检查速度。,https://github.com/vllm-project/vllm/issues/7056
vllm,这是一个改进需求（Feature Enhancement），主要涉及到前端（Frontend）的聊天消息解析（chat message parsing）。,https://github.com/vllm-project/vllm/issues/7055
vllm,这是一个需求提出类型的issue，主要涉及vllm项目下的core模块。由于需要在core内部获取kv_cache而不经过worker，因此实现了get_kv_from_block方法。,https://github.com/vllm-project/vllm/issues/7052
vllm,这是一个性能优化类型的issue，主要涉及到了`seq_group.get_seqs()`方法的优化。原因是在方法中引入了`seqs: List[Sequence]`来提供性能提升。,https://github.com/vllm-project/vllm/issues/7051
vllm,该issue是关于新功能的实现，主要涉及修改VLLM中的异步输出处理器。由于GPU总是在CPU之前运行，通过在每个解码步骤中完全重叠的cuda图前向传递，使得吞吐量提高了11%。,https://github.com/vllm-project/vllm/issues/7049
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm和amazon/FalconLite2模型。用户想要在vllm中使用amazon/FalconLite2模型进行性能基准测试，但是发现该模型不受支持。,https://github.com/vllm-project/vllm/issues/7040
vllm,这是一个用户提出需求的issue，主要涉及的对象是使用Falconlite2进行vllm benchmarking。用户提出了想要同时使用falconlite2和vllm进行基准测试，但由于vllm尚不支持falconlite2，无法实现该需求。,https://github.com/vllm-project/vllm/issues/7038
vllm,这个issue类型是优化建议，涉及对象是CI/Build流程。由于已经有了`lmevalharness`对compressedtensors检查点的覆盖率，因此没有必要保留这个测试和依赖关系。,https://github.com/vllm-project/vllm/issues/7037
vllm,这是一个用户需求问题，主要涉及使用vllm进行Mistral 7B的离线推理时如何传递JSON内容类型。用户希望通过`generate`方法使用JSON模式，但仅使用提示似乎无法产生所需的JSON输出。,https://github.com/vllm-project/vllm/issues/7030
vllm,这是一个请求清理代码的Issue，涉及主要对象为Punica C信息。由于最近合并了一个相关PR并进行了重构，因此需要清理剩余的Punica C信息。,https://github.com/vllm-project/vllm/issues/7027
vllm,这个issue类型是功能增强，主要涉及的对象是FlashAttention后端。这个问题由于attention logits softcapping的支持不完整导致Gem2模型运行时出现多个问题，用户提出了此需求以解决这些问题。,https://github.com/vllm-project/vllm/issues/7022
vllm,这是一个需求调整（Feature Request）类型的issue，涉及主要对象是MiniCPMV，由于未来功能（LoRA 和 BNB）的支持需求，导致需要将MiniCPMV的不同版本分离以方便支持。,https://github.com/vllm-project/vllm/issues/7020
vllm,这是一个用户提出需求的issue，涉及的主要对象是Qwen-VL-Chat模型，由于该模型在vllm API中不受支持导致无法执行相关代码。,https://github.com/vllm-project/vllm/issues/7017
vllm,这是一个用户提出需求的issue，主要涉及到在vLLM中实现无分类器指导的功能。用户之前在issue中提到了由于论文作者提出的一个问题，现在已经根据另一个用户的建议实现了一个版本，并希望能够得到一些建议。,https://github.com/vllm-project/vllm/issues/7016
vllm,这个issue是性能优化类型，主要涉及的对象是代码中的`prepare_input`函数，是由于性能考虑而提出修改建议。,https://github.com/vllm-project/vllm/issues/7014
vllm,这是一个用户提出需求的issue，主要涉及的对象是在优化的量化列表中添加压缩张量，其原因是目前列表中只包含了下划线版本而未包含短横线版本导致。,https://github.com/vllm-project/vllm/issues/7006
vllm,这是一个功能需求类型的issue，主要涉及TPU后端加载时W8A16量化的添加。这个问题是由于需要为TPU后端添加新的加载时int8权重量化功能，以优化模型的性能。,https://github.com/vllm-project/vllm/issues/7005
vllm,这个issue类型是CI/Build改进，该问题单涉及的主要对象是vLLM代码库的构建和持续集成（Continuous Integration），由于缺乏对除零和缺少返回语句警告的处理，导致了编译警告的出现。,https://github.com/vllm-project/vllm/issues/7001
vllm,这是一个功能需求类型的issue，主要涉及到vLLM项目中的Multi Step Scheduling。原因是为了增加对多步调度的支持。,https://github.com/vllm-project/vllm/issues/7000
vllm,"这是一个文档更新issue，主要涉及的对象是更新""run-amd-test.sh""脚本，原因是为了解决docker pull限制问题。",https://github.com/vllm-project/vllm/issues/6997
vllm,这个issue属于需求提出类型，主要涉及到vLLM编译器的警告标准的更严格设置。由于过多警告会掩盖真正问题并影响视觉效果，开发人员在构建源代码时遇到了大量可能是除零错误的警告导致了这一需求。,https://github.com/vllm-project/vllm/issues/6994
vllm,这是一个用户提出需求的issue，主要对象是控制在进行请求时是否使用预测解码，用户提出这个需求的原因是需要在某些输入情况下禁用预测解码。,https://github.com/vllm-project/vllm/issues/6993
vllm,这个issue是一个用户需求报告，主要对象是vllm引擎，用户提出需要添加对Python 3.12的支持。,https://github.com/vllm-project/vllm/issues/6990
vllm,这是一个功能需求的issue，主要涉及的对象是为VLLM项目添加Fused MoE W8A8(Int8)支持，由于当前Int8 MoE比使用bf16与Fused MoE组合的速度更慢，需要进行更多优化以提升性能。,https://github.com/vllm-project/vllm/issues/6978
vllm,这是一个用户提出需求的issue，主要涉及如何在vllm中主动终止请求和停止推理过程，可能出现由于推理时间过长或生成的token重复导致无法停止推理的情况。,https://github.com/vllm-project/vllm/issues/6975
vllm,这是一个用户提出需求的类型。该问题涉及的主要对象是在VLLM下添加对QWen 1模型的支持。,https://github.com/vllm-project/vllm/issues/6974
vllm,这是一个关于性能优化的issue，针对vLLM中多步推理过程中的一个潜在问题。,https://github.com/vllm-project/vllm/issues/6971
vllm,这是一个用户提出需求的issue，主要涉及在服务器上添加安全方案，由于当前身份验证方法没有在自动生成的openapi.json文档中记录，导致Swagger页面无法提供登录或授权头的功能。,https://github.com/vllm-project/vllm/issues/6970
vllm,这个issue类型为性能优化，主要涉及LLAMA3 70b + speculative decoding，在使用`turboderp/Qwama0.5BInstruct`模型时QPS为2。,https://github.com/vllm-project/vllm/issues/6964
vllm,这是一个用户提出需求的issue，主要对象是在vllm下的Speculative decoding模块。由于缺乏时间记录，用户希望添加周期性记录，以便测量提议每个令牌所花费的平均时间、评分所花的时间以及验证所花费的时间。,https://github.com/vllm-project/vllm/issues/6963
vllm,这个issue是一个功能需求，主要涉及到punica kernel和IBM granite 20b模型的适配。,https://github.com/vllm-project/vllm/issues/6962
vllm,这是一个性能优化建议的issue，问题主要涉及VLLM在使用TensorParallel时处理token性能问题，原因可能是处理性能在tensor-parallel上缩放不佳。,https://github.com/vllm-project/vllm/issues/6955
vllm,这是一个更新请求类型的issue，涉及主要对象是项目中的PyTorch依赖。,https://github.com/vllm-project/vllm/issues/6951
vllm,这是一个关于性能优化的issue，主要涉及Ada Lovelace架构的性能问题。由于Cutlass内核在某些情况下表现略逊于scaled_mm内核，但在一些benchmark测试中表现更好，但在输出令牌吞吐量方面略逊于scaled_mm，需要进一步调查和比较。,https://github.com/vllm-project/vllm/issues/6950
vllm,这是用户提出的需求类型issue，主要涉及对象是Llama模型。由于找不到相关参数来启用embeddings API，用户无法实现这一功能，因此请求帮助启用embeddings API。,https://github.com/vllm-project/vllm/issues/6947
vllm,这是一个性能优化提议类型的issue，主要涉及vllm的性能问题，用户提出希望通过增加吞吐量来改进性能。,https://github.com/vllm-project/vllm/issues/6945
vllm,该issue属于用户提出需求类型，主要涉及的对象是该项目是否支持在torch 2.4下开箱即用，原因可能是用户希望该项目能够兼容最新版本的torch。,https://github.com/vllm-project/vllm/issues/6944
vllm,这个issue是用户提出需求，希望支持在vLLM中使用SiglipVisionModel，类似于支持CLIPVisionModel。,https://github.com/vllm-project/vllm/issues/6941
vllm,这是一个用户需求类型的issue，主要涉及如何实现在使用LLMs时实现实时流式响应，由于当前代码的generate方法会等待整个响应生成完毕才展示，用户希望能够实现流式展示并增强用户交互性。,https://github.com/vllm-project/vllm/issues/6940
vllm,这个issue是一个功能补充类型的问题，涉及的主要对象是LLM类。由于需要增加应用聊天模板的功能，因此提出了添加apply_chat_template方法和更新generate方法的需求。,https://github.com/vllm-project/vllm/issues/6936
vllm,这是一个特性请求，涉及到将 Jamba 中的 MoE 层替换为标准 vLLM FusedMoE 层。,https://github.com/vllm-project/vllm/issues/6935
vllm,这是一个功能需求，用户提出需要支持INT4类型数据对MiniCPM-Llama3-V-2_5进行操作。,https://github.com/vllm-project/vllm/issues/6932
vllm,该问题类型为性能改进提议，主要涉及到与sglang项目性能进行比较。提出原因是希望改进性能，但未收到任何响应。,https://github.com/vllm-project/vllm/issues/6929
vllm,该issue类型为用户提出需求，主要涉及的对象是支持rerank模型。由于Rerank模型对于RAG工作流程至关重要，用户询问是否有计划支持这个功能，以及实现这一功能的主要步骤。,https://github.com/vllm-project/vllm/issues/6928
vllm,这是一个关于在vllm项目中添加对Qwen2的Pipeline parallel支持的需求。,https://github.com/vllm-project/vllm/issues/6924
vllm,这个issue类型为功能增强需求，涉及Github Actions工作流程以及标签管理。,https://github.com/vllm-project/vllm/issues/6921
vllm,这个issue类型是功能增强提案，主要涉及的对象是vLLM的pipeline parallelism分区策略。由于当前PP kvcache容量受限于所有PP阶段中最小的一个，通常是第一个或最后一个阶段，导致整体吞吐量受限，因此需要引入新的分区算法。,https://github.com/vllm-project/vllm/issues/6920
vllm,这是一个关于代码修改和功能添加的issue，主要涉及到在vLLM中支持`torch.compile`时添加元函数以防止torch.compile图形断裂的问题。,https://github.com/vllm-project/vllm/issues/6917
vllm,这是一个性能优化提议，涉及到移动推测解码的准备输入到GPU，以提高性能。,https://github.com/vllm-project/vllm/issues/6915
vllm,该issue属于代码质量改进类型，涉及到的主要对象是在vLLM中的一些内核警告。这个问题的出现是因为开发者在多个内核中遇到了一些次要警告，需要进行修复。,https://github.com/vllm-project/vllm/issues/6914
vllm,该issue类型为需求提出，主要涉及LLMEngine中输出处理导致GPU性能瓶颈的问题。,https://github.com/vllm-project/vllm/issues/6913
vllm,这是一个功能需求的issue，主要涉及到减少LoRA延迟的技术实现。,https://github.com/vllm-project/vllm/issues/6912
vllm,这是一个需求提议，涉及的主要对象是将pipeline parallelism与speculative decoding结合起来，以降低延迟。,https://github.com/vllm-project/vllm/issues/6911
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM模型的功能。由于用户希望提出一种新的特性，让模型支持在给定上下文中生成多个模型内容，以加快内容生成速度。,https://github.com/vllm-project/vllm/issues/6906
vllm,这个issue是关于优化序列化过程的，涉及到将数据发送给workers时减少payload大小，并且需要修改现有数据类以支持msgspec兼容的数据结构。原因是通过发送增量以减少要发送给workers的负载大小，以及使用msgspec.msgpack可以提高端到端吞吐量。,https://github.com/vllm-project/vllm/issues/6903
vllm,这是一个增强功能的issue，涉及主要对象是VLLM中的处理器。这个PR的目的是为了避免重复的维护工作，删除了VLLM中重复的处理器。,https://github.com/vllm-project/vllm/issues/6900
vllm,这是一个关于功能需求的issue，主要涉及到FlashInfer后端的Sliding Window支持。由于vLLM的FlashAttention封装不支持Sliding Window，用户询问了对于Sliding Window和分页KV缓存之间的冲突，以及这是否意味着无法在FlashInfer中使用。,https://github.com/vllm-project/vllm/issues/6899
vllm,这是一个性能优化建议的issue，主要涉及到提高ray dag和spmd worker的性能问题。原因可能是目前spmd worker速度较慢，提议使其与ray dag速度相当。,https://github.com/vllm-project/vllm/issues/6888
vllm,这个issue属于一个功能需求的讨论，涉及到通过使用多进程以及`zeromq`来提高OpenAI服务器的性能。,https://github.com/vllm-project/vllm/issues/6883
vllm,该issue是一份要求将代码合并到主分支的请求，涉及vLLM项目的代码贡献规范问题。,https://github.com/vllm-project/vllm/issues/6882
vllm,这是一个性能改进的提案，主要涉及在vLLM中使用Python数组以替换Python列表来进行零拷贝张量创建。这个提案主要是为了减少在将数据从CPU内存复制到GPU内存时发生的拷贝操作。,https://github.com/vllm-project/vllm/issues/6879
vllm,这是一个用户提出需求的issue，主要涉及的对象是支持Python 3.12版本。由于ray目前不支持Python 3.12，用户提出希望该库未来能够支持该版本的需求。,https://github.com/vllm-project/vllm/issues/6877
vllm,这是一个用户提出需求的issue，主要涉及的对象是`generate`方法。用户希望`LLM.generate`方法支持传递`prompt_embeds`作为参数，以便在只微调嵌入层时能够使用，这样能够支持同一模型后端支持多个自定义微调嵌入层。,https://github.com/vllm-project/vllm/issues/6869
vllm,这个issue是关于优化调度程序的改进，不是bug报告。,https://github.com/vllm-project/vllm/issues/6867
vllm,这是一个性能优化类型的issue，主要涉及到`free_finished_seq_groups`函数。原因是在处理大规模离线批量推理时，该函数在每个模型步骤之后调用，导致性能下降。,https://github.com/vllm-project/vllm/issues/6865
vllm,该issue类型是文档整理与更新，涉及主要对象为VLM（Vision Language Models），由于之前每个VLM需要单独创建示例文件，现在经过最近的重构后不再需要，因此此PR将这些示例文件整合到一个文件中以清理文档和示例文件夹。,https://github.com/vllm-project/vllm/issues/6858
vllm,这是一个用户提出需求的issue，主要涉及在vLLM文档站点上添加RunLLM聊天小部件。由于缺少该小部件，用户可能需要更方便地与RunLLM进行交流和咨询。,https://github.com/vllm-project/vllm/issues/6857
vllm,这是一个性能优化相关的问题，主要对象是PyTorch XLA版本。这个问题是因为旧版本XLA编译时间较长，通过升级版本并使用新的动态形状支持来减少编译时间。,https://github.com/vllm-project/vllm/issues/6856
vllm,这是一个功能请求(issue)。主要对象是vLLM中的多步调度(Multi-Step Scheduling)。该问题涉及到GPU空闲等待CPU操作，导致GPU泡泡(513ms的GPU泡泡)。,https://github.com/vllm-project/vllm/issues/6854
vllm,这是一个关于在vLLM中为Ada Lovelace添加优化int8内核的Kernel类别issue，主要涉及对Gemm形状进行优化配置。由于无法使用markdown渲染，内容展示时采用了原始的html格式。,https://github.com/vllm-project/vllm/issues/6848
vllm,这是一个文档更新类型的issue，主要涉及到更新README.md文件中的参数名称。原因是由于之前的PR修改导致markdown渲染不起作用，需要使用原始HTML语法。,https://github.com/vllm-project/vllm/issues/6847
vllm,这是一个关于文档更新的issue，主要涉及的对象是neuron构建相关的文档。由于markdown渲染不起作用，所以在这里使用了原始html。,https://github.com/vllm-project/vllm/issues/6844
vllm,这是一个用户需求类型的issue，主要涉及的对象为VLLM模型文档，用户提出添加Nemotron支持的需求。,https://github.com/vllm-project/vllm/issues/6843
vllm,这是一个需求提出类型的issue，主要涉及Dockerfile中的CUDA版本恢复到12.1，由于12.4版本要求主机保持最新导致的问题。,https://github.com/vllm-project/vllm/issues/6840
vllm,"这是一个用户提出需求的issue，主要涉及grpc openai server prototypes。由于标题中含有""[ DO NOT MERGE ]""，说明该issue可能是一些原型性工作或者未经验证的代码，需要保持不合并到主分支。",https://github.com/vllm-project/vllm/issues/6839
vllm,该issue属于功能请求类型，涉及到EAGLE draft model的支持。,https://github.com/vllm-project/vllm/issues/6830
vllm,该issue类型为用户提出需求，主要涉及在多节点上部署vLLM并使用qwen2模型以加快推理速度的问题，由于大的TP可能导致注意力头的划分错误并出现错误，而且qwen2上未实现PP功能，用户希望能够实现在每个节点上独立并行运行多个模型并在一个端口上合并的效果。,https://github.com/vllm-project/vllm/issues/6825
vllm,该issue类型为用户提出需求，并涉及到如何在具有不同内存的两个GPU上部署模型，用户询问如何最大化利用总共的80GB内存。,https://github.com/vllm-project/vllm/issues/6824
vllm,这是一个特性需求的issue，主要涉及到TeleFLM模型的支持。用户提出希望添加对TeleFLM模型的支持，并说明已经进行了相关测试并得到合理的模型响应。,https://github.com/vllm-project/vllm/issues/6822
vllm,这是一个需求提出的issue，主要涉及 MoE 模块的更新，用户希望能在单个 GPU 上加载 Mixtral。,https://github.com/vllm-project/vllm/issues/6817
vllm,这是一个用户提出的需求类型的issue，主要涉及支持TPU设备上的集体通信。这个issue可能由于XLA设备中缺少集体通信支持而产生。,https://github.com/vllm-project/vllm/issues/6813
vllm,这个issue类型是功能需求，主要涉及的对象是支持在初始化Ray集群中添加对TPU的支持，以及将在PR中添加对HPU的支持。,https://github.com/vllm-project/vllm/issues/6812
vllm,这是一个改进优化类的issue，主要涉及Dockerfile.rocm文件的调整。由于vLLM项目未使用torchaudio和Numpy的热修复，因此进行了相应的简化调整。,https://github.com/vllm-project/vllm/issues/6811
vllm,这个issue是一个功能需求报告，涉及的主要对象是扩展代码以支持SSD offloading，并实现从block中获取KV缓存和将KV缓存添加到block中的功能。由于工作尚未完成，导致目前功能不可用。,https://github.com/vllm-project/vllm/issues/6808
vllm,这是一个用户提出需求的issue，主要涉及的对象是在vllm框架中集成MiniGPT4_video模型。原因是用户表示在阅读教程后仍然无法成功集成该模型到vllm中，希望得到帮助解决这个问题。,https://github.com/vllm-project/vllm/issues/6805
vllm,这个issue类型是技术文档改进，关注于改进多节点服务文档，解决了与多节点服务相关的问题。,https://github.com/vllm-project/vllm/issues/6804
vllm,这个issue类型是关于性能优化的讨论，主要涉及vllm项目的性能问题和优化计划。,https://github.com/vllm-project/vllm/issues/6801
vllm,该问题类型为需求发布幻灯片，主要涉及文档发布。,https://github.com/vllm-project/vllm/issues/6799
vllm,这是一个关于改进OpenAI Server架构的请求，涉及到将API服务器和AsyncLLMEngine分离为两个进程通信。由于目前它们共享相同的asyncio事件循环，导致CPU组件和API服务器争夺相同资源。,https://github.com/vllm-project/vllm/issues/6797
vllm,这是一个技术改进的issue，主要涉及到Marlin内核的精度增加问题。,https://github.com/vllm-project/vllm/issues/6795
vllm,这是一个用户提出需求的issue，主要涉及对象是vllm模型。用户想了解是否可以在classification model中使用vllm，由于不清楚vllm是否支持该模型，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/6789
vllm,该issue类型为用户提出需求，主要对象是ngramspecdecode阶段的功能。用户提出了关于在speculative decoding中评估多个ngram猜测的建议。,https://github.com/vllm-project/vllm/issues/6785
vllm,这个issue是关于[Core]部分的改进提议，主要涉及到SequenceData和Sampler，通过使用`array.array`来加速填充操作，提升了性能。,https://github.com/vllm-project/vllm/issues/6779
vllm,这是一个功能需求提出的issue，主要涉及到支持`Mistral-Large-Instruct-2407`函数调用。源于对`MistralLargeInstruct2407`性能优越的需求，以及可能类似于前一个issue的功能需求。,https://github.com/vllm-project/vllm/issues/6778
vllm,这是一个用户提出需求的issue，主要涉及使用Quantized模型在分布式环境下运行推理时遇到的问题。,https://github.com/vllm-project/vllm/issues/6773
vllm,这是一个用户提出需求的类型的issue，主要涉及chat API assistant 的预填内容功能。用户期望能够预填assistant的回应，但系统似乎不支持此功能。,https://github.com/vllm-project/vllm/issues/6772
vllm,该issue为文档更新类型，涉及vLLM项目的PR任务和代码质量要求。,https://github.com/vllm-project/vllm/issues/6771
vllm,这是一个用户提出需求的issue，主要涉及的对象是如何使用medusa speculative sampling 推断模型。由于显示的硬件信息中GPU型号为NVIDIA H100，但PyTorch版本为2.3.1+cu121，一些限制了2.3版本下的medusa功能，用户可能遇到了一些推断模型时的问题。,https://github.com/vllm-project/vllm/issues/6768
vllm,这是一个用户提出需求的issue，主要涉及的对象是前端（Frontend），他们提出添加一个新的`allowed_token_ids`解码请求参数来限制解码令牌。,https://github.com/vllm-project/vllm/issues/6753
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm项目中的前端部分。由于需要将`run_server`拆分为`build_server`和`run_server`两部分，以实现更灵活的服务器配置。,https://github.com/vllm-project/vllm/issues/6740
vllm,这是一个优化型的问题，涉及到了Mamba缓存单一缓冲器的分配优化。,https://github.com/vllm-project/vllm/issues/6739
vllm,这是一个用户提出需求的类型的issue，主要涉及对象是Llama-3.1，用户寻求为Llama-3.1 添加支持的帮助。,https://github.com/vllm-project/vllm/issues/6731
vllm,该issue类型为用户提出需求，主要对象是vLLM在Ascend NPU上的支持。由于Ascend NPU具有强大的计算能力，用户希望vLLM可以在Ascend NPU上运行，以服务更多用户。,https://github.com/vllm-project/vllm/issues/6728
vllm,这个issue是用户提出需求类型，主要对象是GLM4 function call的支持。由于缺乏对GLM4 function call的支持，用户希望了解是否可以添加此功能。,https://github.com/vllm-project/vllm/issues/6721
vllm,该issue属于功能需求问题，主要涉及logits processor的tensor缓存，并提出了缓存机制的更新。,https://github.com/vllm-project/vllm/issues/6715
vllm,这是一个功能需求类型的issue，主要涉及到MLP Speculator CI测试的优化，需要将模型从当前的3B换成更小的llama160maccelerator模型，以提高速度和节省内存。,https://github.com/vllm-project/vllm/issues/6714
vllm,该issue是关于对model runner/input builder developer APIs进行调整的改进请求。,https://github.com/vllm-project/vllm/issues/6712
vllm,该issue是关于构建/持续集成（CI）的改进，主要涉及修复从Docker Hub仓库中拉取容器的访问限制问题。可能原因是之前存在访问限制导致无法成功拉取容器。,https://github.com/vllm-project/vllm/issues/6711
vllm,这是一个关于功能需求的issue，主要涉及的对象是Prompt adapters，由于缺乏对 *.pt 类型的支持，导致用户提出需要支持该类型的prompt adapters。,https://github.com/vllm-project/vllm/issues/6709
vllm,这是一个优化建议类型的issue，主要涉及到持续集成（CI）和夜间基准测试（nightly benchmark）的流程。,https://github.com/vllm-project/vllm/issues/6706
vllm,这个issue类型是需求修正，主要涉及的对象是构建过程中的 wheel 大小限制，因为可能会导致过大的wheel文件无法上传到pypi。,https://github.com/vllm-project/vllm/issues/6704
vllm,这是一个用户提出需求的issue，主要对象是vllm中的Meta Llama 3.1版本。用户提出了关于修改配置文件以加载特定数值的需求。,https://github.com/vllm-project/vllm/issues/6692
vllm,这个issue类型为文档更新，主要涉及 llama3.1 版本的支持。,https://github.com/vllm-project/vllm/issues/6688
vllm,该issue类型为用户提出需求，涉及主要对象为vTensor - Flexible Virtual Tensor Management for Efficient LLM Serving。由于该功能看起来很棒，用户可能在探讨如何利用vTensor来提高LLM服务的效率。,https://github.com/vllm-project/vllm/issues/6687
vllm,这是一个性能优化讨论的issue，涉及优化_prepare_model_input_tensors函数，作者提出了关于加速循环并实现cuda kernel的问题，希望了解优化计划和时间表。,https://github.com/vllm-project/vllm/issues/6684
vllm,这是一个关于优化性能和类型检查的issue，涉及的主要对象是一些Python文件和目录。原因是为了实施更严格的类型检查，更新相关设置以解决问题。,https://github.com/vllm-project/vllm/issues/6681
vllm,这是一个用户提出需求的issue，主要涉及到需要在无法使用docker的环境下从源代码构建vLLM，由于无法使用docker，用户需要提供详细的构建指令。,https://github.com/vllm-project/vllm/issues/6680
vllm,这个issue类型是用户提出需求，请教问题等，主要涉及将容器镜像发布到额外的镜像注册表（qhcr或quay.io），由于需要增加冗余备份，用户提出了这个建议。,https://github.com/vllm-project/vllm/issues/6678
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是Prefix Caching功能，用户询问是否支持将其卸载到CPU以及是否有计划支持此功能。,https://github.com/vllm-project/vllm/issues/6676
vllm,这是一个版本更新（feature request）类型的issue，主要涉及的对象是软件版本号。原因可能是需要发布新功能或修复现有功能中的问题。,https://github.com/vllm-project/vllm/issues/6674
vllm,这是一个用户提出需求的问题，主要对象是模型加载器。由于缺少支持忽略模式的功能，导致用户体验不佳。,https://github.com/vllm-project/vllm/issues/6673
vllm,这是一个功能改进类的issue，主要对象是用户界面，导致这个问题的原因是缺少加载模型的日志信息。,https://github.com/vllm-project/vllm/issues/6670
vllm,这个issue是关于将fp8支持添加到`reshape_and_cache_flash`中的特性改进，不是bug报告。,https://github.com/vllm-project/vllm/issues/6667
vllm,这是一个功能增强类的issue，主要涉及长上下文模型的chunked prefill操作。由于长上下文模型可能出现OOM错误，因此需要默认启用chunked prefill，避免初始内存分配过大导致的问题。,https://github.com/vllm-project/vllm/issues/6666
vllm,该issue类型为需求提出，涉及主要对象是SPMD worker，由于描述内容待填写，用户可能在尝试使用SPMD worker来减少控制平面通信时遇到了困难，需要相关方面提供帮助。,https://github.com/vllm-project/vllm/issues/6664
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM模型。增加默认chunk大小到2048可能是为了提高模型性能或者解决特定应用场景下的问题。,https://github.com/vllm-project/vllm/issues/6662
vllm,这个issue类型是功能需求提议，主要涉及的对象是在Docker容器中添加vim编辑器。由于需要在Docker容器内部进行代码调试和修改，用户提出了需要在Docker中添加vim编辑器的需求。,https://github.com/vllm-project/vllm/issues/6661
vllm,该issue类型为用户提出需求，主要涉及对象为如何在vllm中禁用日志记录。可能由于用户需要在使用vllm时禁用LLM类的日志记录，所以提出了这个问题。,https://github.com/vllm-project/vllm/issues/6660
vllm,这是一个需求类型的issue，主要涉及的对象是VLLM模型中的beam search功能。由于代码中存在过时警告，需要移除以优化代码。,https://github.com/vllm-project/vllm/issues/6659
vllm,这个issue类型是性能优化，主要涉及对Llama3推断优化的相关工作。原因为提供一系列的优化措施，以使推断栈在更少的设备上运行大规模模型或在使用更多GPU时获得最高吞吐量。,https://github.com/vllm-project/vllm/issues/6658
vllm,该issue属于用户提出需求类型，主要涉及fp8.py中添加忽略层的功能。由于在llama.py中进行逻辑处理会导致难以维护，因此提出了对Fp8Config.get_quant_method(layer)方法进行扩展，以实现忽略特定层的功能需求。,https://github.com/vllm-project/vllm/issues/6657
vllm,这是一个用户提出需求的issue，主要涉及的对象是在构建CUDA 11.8 wheel时使用不同的sccache存储桶。,https://github.com/vllm-project/vllm/issues/6656
vllm,该issue类型为文档更新，涉及主要对象为vLLM文档。原因是文档中的警告需要更新以反映最新工作和版本发布计划。,https://github.com/vllm-project/vllm/issues/6654
vllm,这个issue类型是文档更新，主要涉及的对象是AutoAWQ文档。导致这个问题的原因是AutoAWQ内容已过时，需要更新以反映vLLM的新版本。,https://github.com/vllm-project/vllm/issues/6653
vllm,这是一个用户提出需求的issue，主要涉及的对象是ChatCompletion模块。,https://github.com/vllm-project/vllm/issues/6652
vllm,这个issue是一个[Frontend]类型的需求提出，主要涉及的对象是 vLLM 的前端。由于缺少 `add_special_tokens` 参数，用户无法在 OpenAI API Completions Endpoint 中指定该参数。,https://github.com/vllm-project/vllm/issues/6637
vllm,这是一个用户提出需求的issue，主要涉及Chat Templates based on glm4的创建，可能是因为缺乏关于如何基于glm4制作Chat模板的相关信息。,https://github.com/vllm-project/vllm/issues/6634
vllm,这是一个用户提出需求的issue，涉及的主要对象是VLLM，用户问题是关于支持奖励模型API的功能是否可用。,https://github.com/vllm-project/vllm/issues/6620
vllm,该issue是一个功能增强请求，主要涉及的对象是torch.inference_mode。由于一些硬件后端（如TPU）不支持torch.inference_mode，导致需要引入一个包装类来为不支持的后端回退到torch.no_grad。,https://github.com/vllm-project/vllm/issues/6618
vllm,这是一个性能改进的建议，主要涉及到FlashInfer和FlashAttention后端的比较。由于FlashInfer并没有像博客中描述的那样带来较大的性能提升，用户提出了对性能回归的报告，并讨论了可能的原因。,https://github.com/vllm-project/vllm/issues/6617
vllm,这是一个用户提出需求的issue，主要对象是vllm中的4D attention mask功能。由于Huggingface提供了这一接口，用户想了解vllm是否有计划集成这一功能。,https://github.com/vllm-project/vllm/issues/6615
vllm,该issue属于用户提出需求类型，主要对象是支持多模态对话交互的VLM模型。用户提出了需要在嵌入式设备上部署图像编码器和投影仪，并在请求中发送编码向量以减少数据传输量和降低延迟的需求。,https://github.com/vllm-project/vllm/issues/6604
vllm,这是一个功能需求类型的Issue，主要涉及到前端的chat utils模块移动，用户希望通过此操作更容易地为离线LLM添加多模态支持。,https://github.com/vllm-project/vllm/issues/6602
vllm,这是一个用户需求提出的issue，主要涉及到如何在vllm中使用类似transformers中device_map方法来指定每个层的部署方式。问题的根本原因是用户所需的模型需要大于可用GPU的总内存，且无法满足并行部署GPU数量必须为32的倍数的要求。,https://github.com/vllm-project/vllm/issues/6601
vllm,这是一个需求提出的issue，主要涉及的对象是实现多节点支持。由于目前不支持InfiniBand，用户提出需要添加相应功能以支持该特性。,https://github.com/vllm-project/vllm/issues/6599
vllm,这是一个文档更新的问题，用户提出了关于VLLM中PP文档的需求。,https://github.com/vllm-project/vllm/issues/6598
vllm,该issue是一个提出需求的类型，主要涉及到模型输入构建和注意力元数据构建的优化。,https://github.com/vllm-project/vllm/issues/6596
vllm,该issue类型为用户提出需求，主要涉及对象为vllm下的Gemba29b语言模型。由于Gemba29b目前受到4K滑动窗口的限制，用户提出希望支持8K token context，以加速开发完全本地代理系统的进展。,https://github.com/vllm-project/vllm/issues/6595
vllm,"这是一个需求提出的issue，主要涉及vLLM项目中对于FP8量化中""ignored_layers""支持的缺失。原因是vLLM目前没有遵循""ignored_layers""字段，而是对所有层中的所有模块应用统一的量化。",https://github.com/vllm-project/vllm/issues/6592
vllm,这是一个用户提出需求的issue，主要涉及LLM2Vec（Fine-Tuned Embeddings）的支持，由于LlamaBiModel目前不受支持，用户希望能够解决这个问题。,https://github.com/vllm-project/vllm/issues/6584
vllm,这个issue属于文档更新类型，主要涉及wheel位置相关内容。由于切换到版本不可知的wheels，所以需要更新文档中nightly wheel的链接。,https://github.com/vllm-project/vllm/issues/6580
vllm,这个issue类型是用户提出需求，关于fp8 quantization在ROCm中的支持问题。,https://github.com/vllm-project/vllm/issues/6576
vllm,该issue类型为用户提出需求，涉及主要对象为VLLM项目中的Lora Adapter功能。由于新版mistral模型生成的safetensors与VLLM期望的权重键名不同，以及包含VLLM难以处理的新键名，用户希望能实现一个更新，使得从mistral-finetune生成的Lora safetensors文件可以直接加载为Lora适配器，而无需手动映射权重键名或处理陌生的键名。,https://github.com/vllm-project/vllm/issues/6573
vllm,这是一个用户提出需求的类型 issue，主要涉及的对象是 Llava-Next-Video 模型。由于需要实现视频处理和文本嵌入的合并，用户正在寻求对该模型的支持。,https://github.com/vllm-project/vllm/issues/6571
vllm,这个issue类型是用户提出需求，请求升级vLLM的numpy依赖版本。,https://github.com/vllm-project/vllm/issues/6570
vllm,这是一个提出新功能的issue，涉及到在api server中支持加载和卸载LoRA适配器的功能。,https://github.com/vllm-project/vllm/issues/6566
vllm,"该issue是一个功能需求类型的问题，主要涉及vllm项目对于支持新模型""Mistral-Nemo""的困难。",https://github.com/vllm-project/vllm/issues/6563
vllm,这个issue是用户提出修改vLLM核心逻辑的请求，允许自定义Executor和TokenizerGroup类，为了进行易于实验和定制化部署。,https://github.com/vllm-project/vllm/issues/6557
vllm,这个issue类型是提出需求，主要涉及的对象是关于SPMD式控制平面的工作控制。由于原来的架构存在一些缺陷，如与NCCL广播操作干扰驱动功能、难以支持推导解码等，因此提出了改进控制平面架构的需求。,https://github.com/vllm-project/vllm/issues/6556
vllm,这个issue是用户提出需求类型，涉及的主要对象是支持rope扩展方法。,https://github.com/vllm-project/vllm/issues/6553
vllm,该issue类型为用户提出需求，用户想要了解如何一键安装和调用vLLM中所有的测试用例，但在文档中未找到相关说明，因此寻求其他人的建议。,https://github.com/vllm-project/vllm/issues/6550
vllm,这个issue类型为测试需求，涉及主要对象是vLLM的cpu offloading功能。由于目前的测试基础设施只针对没有cpu offloading的vLLM进行测试，可能是为了验证该功能的正确性。,https://github.com/vllm-project/vllm/issues/6549
vllm,这是一个用户提出需求的类型的issue，主要涉及将嵌入模型（如bge、e5等）部署到vllm中，并寻求相关资源指引。,https://github.com/vllm-project/vllm/issues/6498
vllm,这是一个用户提出的需求类型的issue，主要涉及的对象是vllm中的模型。由于GPU内存限制，用户提出了对于CPU的卸载实现，以扩展GPU可用内存的需求。,https://github.com/vllm-project/vllm/issues/6496
vllm,这个issue类型是CI/Build更新，涉及的主要对象是flashinfer库，由于需要使用最新版本的flashinfer并包含重要更新和修复，所以进行了从0.0.8版本到0.0.9版本的更新。,https://github.com/vllm-project/vllm/issues/6490
vllm,这个issue属于更新通知类型，主要涉及到vllm中的测试脚本，更新flashinfer版本至v0.0.9引起。,https://github.com/vllm-project/vllm/issues/6489
vllm,"这是一个用户提出需求的issue，主要涉及支持新模型""Mamba Codestral""，由于该模型是非transformer架构，但已经有类似的模型""Mamba""得到支持，因此用户不确定如何支持新模型。",https://github.com/vllm-project/vllm/issues/6479
vllm,这是一个文档更新的Issue，主要涉及README的内容更新以突出激活量化。由于Markdown渲染的问题，使用了原始HTML。,https://github.com/vllm-project/vllm/issues/6476
vllm,该issue是关于Feature请求，主要涉及Pipeline parallelism支持的qwen模型，由于只支持特定的架构，导致该功能无法在其他架构上使用。,https://github.com/vllm-project/vllm/issues/6471
vllm,这是一个关于提出需求的issue，主要涉及到vLLM中 Speculative Decoding 算法的性能优化提议。由于缺乏有关成本系数的信息，导致用户无法获取每个模型的成本系数数值。,https://github.com/vllm-project/vllm/issues/6468
vllm,这个issue属于功能需求类型，主要涉及的对象是TPU硬件支持，由于需要支持MoE模型使用GMM Pallas内核，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/6457
vllm,这个issue类型是性能优化请求，主要涉及到Pipeline Parallel scenarios中Llama模型初始化内存占用优化。,https://github.com/vllm-project/vllm/issues/6455
vllm,这个issue类型为功能增强，涉及主要对象为性能优化和度量指标。原因是为了添加一种对引擎度量指标进行推测解码的功能，并测试相关度量是否正确记录。,https://github.com/vllm-project/vllm/issues/6454
vllm,该问题单属于用户提出需求类型，涉及的主要对象是网站vllm，用户寻求将Google Cloud添加到赞助商列表。,https://github.com/vllm-project/vllm/issues/6450
vllm,这是一个功能增强类的issue，主要涉及的对象是vLLM缓存目录。由于缺乏缓存目录，导致需要使用`s3`或`curl`下载VLM示例和测试中使用的图片，该issue旨在为vLLM添加缓存目录以解决这一问题。,https://github.com/vllm-project/vllm/issues/6444
vllm,这是一个提速优化issue，主要涉及vLLM中的token处理，由于list comprehension被NumPy操作替代，造成sampler处理大批量数据时速度提升了50%。,https://github.com/vllm-project/vllm/issues/6442
vllm,这是一个提出建议的Issue，讨论了如何集成Quant支持以优化Intel CPU和GPU性能。,https://github.com/vllm-project/vllm/issues/6440
vllm,这个issue类型是文档更新，主要对象是希望更新关于成功性检查的文档。由于信息不清晰，用户提出需要给出一个清晰的成功性检查消息。,https://github.com/vllm-project/vllm/issues/6439
vllm,这个issue是针对vLLM项目中的一个优化提升，不属于bug报告。该问题涉及的主要对象是ClipVisionModel模型。原因是为了节省GPU空间而进行了代码优化。,https://github.com/vllm-project/vllm/issues/6436
vllm,这是一个用户提出需求的issue，主要对象涉及到GitHub repository。由于项目获得GitHub赞助商批准并能够链接到OpenCollective，因此需要添加FUNDING.yml文件。,https://github.com/vllm-project/vllm/issues/6435
vllm,这是一个版本升级请求的issue，涉及的主要对象是软件版本。,https://github.com/vllm-project/vllm/issues/6433
vllm,这个issue主要是关于文档和测试更新，涉及到vLLM CLI的新功能需求，而非bug报告。,https://github.com/vllm-project/vllm/issues/6431
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm下的attention layer，用户希望添加返回softmax的功能。,https://github.com/vllm-project/vllm/issues/6424
vllm,这个issue类型为功能更新，涉及主要对象为AWQ MoE模型。由于重构后的代码逻辑复杂，导致新增功能的实现需要处理更多的索引逻辑，以及重新映射参数，同时还存在Deepseek功能不受支持的问题。,https://github.com/vllm-project/vllm/issues/6422
vllm,这是一个用户提出需求的issue，涉及主要对象为分布式推理。由于现有文档不包含关于分布式推理的建议，用户提出需要添加这方面的建议。,https://github.com/vllm-project/vllm/issues/6418
vllm,这个issue是一个[ Misc ]类型的PR请求，涉及主要对象是将MoE Refactor应用到DeepSeekv2和Qwen2Moe以支持`fp8`，用户请求支持fused AWQ，原因是markdown渲染无法正常工作，因此使用原始html。,https://github.com/vllm-project/vllm/issues/6417
vllm,这是一个用户提出需求的issue，主要对象是`LLM`类，由于无法通过`LLM`类应用聊天模板，用户提出希望添加此功能的请求。,https://github.com/vllm-project/vllm/issues/6416
vllm,这是一个需求类型的issue，主要涉及vllm项目中关于CI和分布式的功能测试，提出了为pipeline并行性增加正确性测试的需求。,https://github.com/vllm-project/vllm/issues/6410
vllm,这是一个关于优化代码以支持管道并行的问题，主要涉及模型的权重加载和层构造部分，旨在减少修改代码量并简化模型支持管道并行的实现。,https://github.com/vllm-project/vllm/issues/6406
vllm,这是一个用户提出需求的issue，主要对象是beam search功能。原因是为了在完全删除时能够及时通知。,https://github.com/vllm-project/vllm/issues/6404
vllm,这个issue类型是功能需求，涉及的主要对象是Mixtral模型。由于需要添加pipeline支持，用户希望在Mixtral模型上运行pipeline并进行测试。,https://github.com/vllm-project/vllm/issues/6403
vllm,这个issue是一个功能需求，主要涉及的对象是修复Quantized类型使用中的歧义问题，由于新增的fp8 Marlin使得参数`num_bits`不能准确区分权重是8位整数还是4位整数，导致需要复制和粘贴Marlin代码并创建额外的Python入口点来解决这种歧义。,https://github.com/vllm-project/vllm/issues/6396
vllm,这个issue是关于Github上vllm项目中的一个需求提案，主要涉及在安装库时将生成的git提交哈希嵌入到库中，目的是为了避免意外提交提交ID并保持`setup.py`不会影响开发者的git状态。,https://github.com/vllm-project/vllm/issues/6386
vllm,这个issue类型是功能增强要求，涉及主要对象为GHA workflows。导致这个需求的原因可能是为了提高CI/CD流程的效率和自动化程度。,https://github.com/vllm-project/vllm/issues/6381
vllm,这是一个需求提出类型的issue，主要涉及将构建一组默认的nightly wheels，通过此操作可以帮助测试。,https://github.com/vllm-project/vllm/issues/6380
vllm,这是一个提出新特性需求的issue，主要涉及的对象是在vLLM中使用torch.compile进行图优化系统。,https://github.com/vllm-project/vllm/issues/6378
vllm,这个issue类型是一个功能需求，主要涉及vLLM项目中的优化器实现，由于需要实现新的优化器，用户提交了对应的PR。,https://github.com/vllm-project/vllm/issues/6377
vllm,这个issue是关于优化的需求，主要涉及限制预加载批处理大小以避免重新编译。,https://github.com/vllm-project/vllm/issues/6374
vllm,这个issue是一个功能请求，其涉及的主要对象是添加Ascend NPU的支持。由于目前项目只支持GPU等硬件加速器，缺乏对NPU的支持，用户希望通过该功能请求实现更快速、更高效的计算。,https://github.com/vllm-project/vllm/issues/6368
vllm,这是一个需求提出类型的issue，主要涉及测试用例的组织和默认运行设置。,https://github.com/vllm-project/vllm/issues/6365
vllm,这是一个关于功能讨论的issue，主要涉及到VLLM中的多模态数据处理，用户询问关于连续批处理中如何匹配多模态数据和序列的问题。原因是可能存在无法确定多模态数据属于哪个输入的问题。,https://github.com/vllm-project/vllm/issues/6362
vllm,这是一个功能需求类型的issue，主要涉及支持压缩张量模型的激活序列量化，由于需要支持激活序列的组内量化，需要添加相应的参数和层参数。,https://github.com/vllm-project/vllm/issues/6358
vllm,这个issue是关于对`compressed-tensors`集成中支持具有偏差的模型的需求。,https://github.com/vllm-project/vllm/issues/6356
vllm,这是一个与软件功能相关的需求提出issue，主要涉及的对象是flashinfer功能。由于需要flashinfer新版本才能通过CI测试，旧版本会导致CI失败。,https://github.com/vllm-project/vllm/issues/6351
vllm,这是一个用户提出需求的 issue，主要涉及 FlashAttention 3 的支持，由于该功能有望带来 1.5 倍的性能改进。,https://github.com/vllm-project/vllm/issues/6348
vllm,这是一个文档更新类的issue， 主要对象是vllm项目中的pipeline parallel功能的说明。,https://github.com/vllm-project/vllm/issues/6347
vllm,这个issue类型是改进建议，主要涉及到优化vllm项目中的CSR矩阵计算方法。由于用户困惑需要安装scipy，提出了优化建议。,https://github.com/vllm-project/vllm/issues/6343
vllm,这是一个功能改进型的issue，涉及主要对象为vllm项目下的draft_model_runner模块。由于在advance_step中处理prepare_inputs的操作尚未在GPU上执行，导致了性能表现不佳，用户提出将其转移到GPU上以提升性能。,https://github.com/vllm-project/vllm/issues/6338
vllm,这是一个性能优化的建议类型的issue，主要涉及NVIDIA Nsight Compute在Linux上的使用问题，提出了关于性能降级的报告和未得到回应的问题。,https://github.com/vllm-project/vllm/issues/6336
vllm,这个issue属于需求提出类型，主要对象是VLLM模型中的RowParallelLinear模块，作者提出了一个关于优化算法的建议。,https://github.com/vllm-project/vllm/issues/6327
vllm,这是一个功能需求的issue，主要涉及到在vllm模型中实现混合注意力（hybrid attention）的问题。发起这个issue的原因是由于目前vllm模型只使用全局注意力而忽略了局部注意力，用户希望在vllm/flashattn中通过设置窗口大小来实现局部注意力，以加速预填充阶段并优化内存消耗。,https://github.com/vllm-project/vllm/issues/6323
vllm,"这是一个用户提出需求的类型的 issue，主要涉及到对模型 ""InternVL2"" 的支持。由于InternVL2是一个功能强大的开源多模态大型语言模型，用户希望将其集成到vLLM框架中以获得更大的受益。",https://github.com/vllm-project/vllm/issues/6321
vllm,这个issue是关于代码优化和文档规范的改进，主要对象是项目的代码库。原因是发现部分代码可以进行优化以提高可读性。,https://github.com/vllm-project/vllm/issues/6320
vllm,这是一个用户提出需求的issue，主要涉及vLLM模型的条件性CPU权重卸载功能。由于GPU资源有限，导致大型模型的性能可能出现高延迟的问题。,https://github.com/vllm-project/vllm/issues/6317
vllm,这是一个关于核心功能改进的issue，主要涉及支持Lora适配器谱系和基础模型元数据管理。由于Lora命令支持json格式且用户可以显式指定基础模型，需要利用ModelCard中的`root`和`parent`字段展示lora适配器谱系，并引入`BaseModelPath`来使用基础模型信息代替`served_model_names`。,https://github.com/vllm-project/vllm/issues/6315
vllm,这是一个用户提出需求的issue，该问题涉及VLLM对多个GPU的控制问题，用户想要指定VLLM使用其中一个GPU而不是默认占用所有GPU。,https://github.com/vllm-project/vllm/issues/6312
vllm,这个issue属于用户提出需求类型，主要涉及对象是在使用vllm加载模型后运行自定义的post_init函数。用户之所以提出这个问题是因为想要在transformers模型加载后执行特定的自定义操作。,https://github.com/vllm-project/vllm/issues/6311
vllm,这个issue属于用户提出需求类型，主要对象是vLLM，用户提出了关于是否支持CrossLayer Attention的问题。,https://github.com/vllm-project/vllm/issues/6307
vllm,这是一个特性需求的issue，主要涉及到支持 speculative decoding 中的多个提议者，其中提到了 ngram proposer 和 DraftModelBased Proposers 的性能问题。,https://github.com/vllm-project/vllm/issues/6300
vllm,该issue为需求类型，针对CI中测试TPU的提案。 ,https://github.com/vllm-project/vllm/issues/6293
vllm,这是一个用户需求类型的问题，涉及到vllm在k8s集群中产生大量日志文件的问题，用户希望了解如何禁用这些日志文件的生成。,https://github.com/vllm-project/vllm/issues/6290
vllm,这是一个特性需求的提出，主要对象是 `UnquantizedFusedMoEMethod`，用户提出需要添加 `CustomOp` 接口以便支持多种硬件后端，因为目前直接导入 Triton fused MoE kernel 限制了其他硬件后端支持 MoE 模型。,https://github.com/vllm-project/vllm/issues/6289
vllm,这个issue类型是用户提出需求，涉及的主要对象是如何修改WeMM使其兼容vllm。由于WeMM中使用了自定义的Plora模块替代了基本的线性层，需要调整代码以支持该模型。,https://github.com/vllm-project/vllm/issues/6281
vllm,这是一个用户提出需求的issue，主要涉及的对象是CI（持续集成）。,https://github.com/vllm-project/vllm/issues/6280
vllm,这是一个功能需求类型的issue，主要对象是为TPU backend添加一个CI测试。,https://github.com/vllm-project/vllm/issues/6277
vllm,这是一个功能增强的请求（RFC），主要涉及vLLM中LoRA管理的改进，以使其更适合生产环境。原因是LoRA集成在生产环境中面临几个挑战，需要解决以确保平稳高效的部署和管理。,https://github.com/vllm-project/vllm/issues/6275
vllm,这是一个需求提出类型的issue，主要涉及到/v1/models接口的响应内容。由于无法在响应中展示Lora与基础模型之间的血缘关系，用户提出应在root字段表示模型路径，parent字段表示Lora适配器的基础模型，以解决这一问题。,https://github.com/vllm-project/vllm/issues/6274
vllm,这是一个功能需求的issue，主要涉及对象是LLMEngine中的SamplingController接口。原因是为了改进guided decoding的表现和API。,https://github.com/vllm-project/vllm/issues/6273
vllm,这个issue类型是需求，主要对象是模型部署推理配置优化，用户提出了关于优化模型部署推理配置以满足延迟和吞吐量两种主要需求的问题。,https://github.com/vllm-project/vllm/issues/6272
vllm,这是一个功能需求的报告，主要涉及CogVlm2模型的支持问题。此问题的原因是模型使用了自定义的int4量化方法，需要进行支持。,https://github.com/vllm-project/vllm/issues/6265
vllm,这个issue是一个feature需求，主要涉及的对象是对新模型XLMRobertaForSequenceClassification的集成和使用。出现这个需求是因为想要实现vllm对‘任何’模型的集成，并替换模型层以提升性能。,https://github.com/vllm-project/vllm/issues/6260
vllm,这是一个用户提出需求的issue，主要涉及如何在langchain_community中使用VLLM对图像进行推断。用户在使用microsoft/Phi3vision128kinstruct模型时遇到集成问题，需要帮助。,https://github.com/vllm-project/vllm/issues/6257
vllm,这个issue是关于用户提出需求的，主要对象是vllm模型。由于启用前缀缓存需要禁用滑动窗口，导致在设置max model len时受到限制，需要增加max model len值超过默认的滑动窗口值，用户询问是否可能实现这一要求。,https://github.com/vllm-project/vllm/issues/6253
vllm,这是一个用户提出需求的issue，主要涉及的对象是为vllm添加固定输入长度和输出长度的基准测试。这个需求可能是为了更全面地评估vllm性能，或者用于验证模型的准确性和稳定性。,https://github.com/vllm-project/vllm/issues/6251
vllm,这是一个用户提出需求类型的issue，主要涉及配置vLLM模型在GCP上使用所有可用CUDA设备的功能。原因是在GCP部署时没有设置环境变量的选项，导致需要为每种可能的设置构建一个容器或注册一个模型。,https://github.com/vllm-project/vllm/issues/6248
vllm,这是一个提出需求的issue，主要涉及VLLM中关于fp8数据类型的支持问题，用户提出希望在attention计算中支持fp8类型，而当前存在需要先将fp8转换为fp16/bf16的问题。,https://github.com/vllm-project/vllm/issues/6246
vllm,该issue类型为性能优化建议，主要涉及的对象是vLLM中的`LLMEngine`，问题是由于`LLMEngine`与tensor并行的rank 0进程共处一个进程中，导致了难以使用不同GPU创建多个vLLM实例，需要测量对象序列化的开销。,https://github.com/vllm-project/vllm/issues/6241
vllm,这个issue是一个需求，主要涉及的对象是支持从HuggingFace加载lora适配器。由于用户在启动引擎时必须指定本地的lora路径，导致操作繁琐，用户希望能够在运行时加载远程的lora模型。,https://github.com/vllm-project/vllm/issues/6233
vllm,这是一个文档更新类型的issue，涉及主要对象为Pipeline Parallel。由于缺乏相关文档，用户提出需要更新Pipeline Parallel的文档。,https://github.com/vllm-project/vllm/issues/6222
vllm,这是关于功能需求的问题，主要涉及FlashInfer和Gemma 2在AMD GPU上的兼容性。用户想知道如何在AMD GPU上运行Gemma 2模型，并提到遇到了环境变量设置问题以及导入错误。,https://github.com/vllm-project/vllm/issues/6218
vllm,这个issue是关于文档更新的类型为文档修复和改进，主要涉及vLLM与CPU后端的部署方面的内容。由于CPU后端与GPU后端不同，需要更新相关考虑事项的文档。,https://github.com/vllm-project/vllm/issues/6212
vllm,这个issue类型是优化建议，涉及的主要对象是Kernel中的fused_moe配置，由于最后一个chunk大小较小，建议重新加载kernel配置以提高性能。,https://github.com/vllm-project/vllm/issues/6210
vllm,这是一个用户提出需求的类型，主要涉及的对象是vllm是否支持多模态的API调用以及是否支持minicpm-v2.5，用户询问如何与vllm集成以运行特定模型的推理。,https://github.com/vllm-project/vllm/issues/6209
vllm,这是一个关于性能优化的issue，主要涉及GPU KV缓存的利用率问题。由于GPU KV缓存使用率较低，用户想要了解如何通过Radix前缀缓存来提高KV缓存利用率。,https://github.com/vllm-project/vllm/issues/6206
vllm,这个issue属于文档需求类型，主要涉及的对象是为开发自定义多模态插件提供指南。,https://github.com/vllm-project/vllm/issues/6205
vllm,这是一个关于改进建议的RFC，主要涉及OpenAI Entrypoint中的`stop_reason`字段被弃用，建议使用`finish_reason`替代。原因是`stop_reason`并非OpenAI规范中的一部分，可能会导致对现有用户的不兼容性。,https://github.com/vllm-project/vllm/issues/6202
vllm,该Issue类型为用户提出需求，主要涉及VLLM模型中如何使两次不同输入调用的结果保持一致，可能由于用户希望在不同输入情况下确保结果的一致性，可能需要使用种子值等方法进行控制。,https://github.com/vllm-project/vllm/issues/6197
vllm,这是一个用户需求提出类型的issue，主要涉及到在离线推理中使用类似OpenAI API的功能。用户提出希望能够通过代码中的提示来运行不同的llm推理，而不是使用API调用的方式。,https://github.com/vllm-project/vllm/issues/6191
vllm,这是一个用户提出需求的issue，主要涉及vLLM模型在GPU上设备放置，由于当前不支持精确的模型设备放置导致用户提出了此问题。,https://github.com/vllm-project/vllm/issues/6189
vllm,这是一个需求提出的issue，主要涉及vLLM中lazy import功能的添加。由于没有使用VLM功能，但在推理时出现了错误，提出希望实现VLM相关模块的延迟导入，以减少对VLM组件的强依赖。,https://github.com/vllm-project/vllm/issues/6187
vllm,这是一个关于性能优化的issue，主要涉及到vLLM的Speculative Decoding功能。原因可能是当前的draftmodel speculative decoding效率不高。,https://github.com/vllm-project/vllm/issues/6185
vllm,这是一个功能需求的issue，涉及到在处理大型对象广播时的改进措施。,https://github.com/vllm-project/vllm/issues/6183
vllm,该issue类型为新功能需求；主要涉及的对象是项目的测试流程；用户提出了添加新的测试内容的需求。,https://github.com/vllm-project/vllm/issues/6182
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是支持AVX2指令集的CPU。由于AVX512指令集要求，导致了部分CPU无法进行推断，用户要求扩展CPU支持到AVX2以解决此问题。,https://github.com/vllm-project/vllm/issues/6178
vllm,这个issue属于用户提出需求类型，涉及的主要对象是vLLM gemma2，用户希望增加对ROPE scaling的支持。,https://github.com/vllm-project/vllm/issues/6175
vllm,这个issue类型是一个[RFC]（请求讨论）的类型，主要涉及vLLM项目中关于实现通过KV缓存传输进行解聚预加载的功能。用户提出了一些实现上的目标和步骤，以及需要的功能支持。,https://github.com/vllm-project/vllm/issues/6170
vllm,这个issue属于文档改进类型，主要涉及到移动指南内容以及改进输入处理和多模态文档。,https://github.com/vllm-project/vllm/issues/6168
vllm,这个issue是文档改进类型的需求，主要涉及vLLM支持的模型按类型重新组织，可能由于当前文档的排版格式问题而导致markdown渲染不正常。,https://github.com/vllm-project/vllm/issues/6167
vllm,这是一个关于功能请求的issue，主要涉及到返回隐藏状态的问题，由于新代码已经支持`return_hidden_states`，但在LLM引擎中尚未支持，用户想知道开发进展和时间表。,https://github.com/vllm-project/vllm/issues/6165
vllm,该issue属于功能优化类型，主要涉及对`_prepare_model_input_tensors`方法的重构，并引入了`ModelRunnerInputBuilder`来进行逻辑隔离和模块化。由于存在逻辑耦合和复杂性，需要通过重构和优化来提升代码的可维护性和性能。,https://github.com/vllm-project/vllm/issues/6164
vllm,这是一个功能需求，主要对象是wheel构建过程中的debug信息，用户要求将构建过程中的debug信息剥离以减小最终wheel文件的大小。,https://github.com/vllm-project/vllm/issues/6161
vllm,该issue类型为版本更新请求，主要对象是软件版本。这个问题是由于需要更新软件版本而导致的。,https://github.com/vllm-project/vllm/issues/6157
vllm,这是一个用户提出需求的issue，涉及到在Vllm中如何使用多实例，并询问其功能是否独立支持，原因在于用户想要利用类似TensorRTLLM后端的多模型支持。,https://github.com/vllm-project/vllm/issues/6155
vllm,这是一个用户提出需求的issue，主要涉及vLLM在aws batch上无法查看作业进度，希望能够暴露tqdm进度条以实现进度记录。,https://github.com/vllm-project/vllm/issues/6154
vllm,该issue类型为功能需求，主要涉及vLLM在新backend上的集成。原因导致用户提出新设备集成需求。,https://github.com/vllm-project/vllm/issues/6151
vllm,这是一个功能需求问题，主要涉及的对象是 ChatGLMForCausalLM 模型。由于 LoRA 功能未被支持但已启用，导致了这个问题的提出。,https://github.com/vllm-project/vllm/issues/6148
vllm,这是一个文档改进的issue，涉及改进最大图像特征大小计算和虚拟数据生成的一致性。可能由于markdown渲染问题，采用了HTML格式叙述。,https://github.com/vllm-project/vllm/issues/6146
vllm,这是一个增加Intel Gaudi（HPU）推断后端的需求类型的issue，主要涉及到对应硬件和软件环境的支持，由于缺少对Beam search等功能的支持导致了部分功能无法实现。,https://github.com/vllm-project/vllm/issues/6143
vllm,这是一个关于功能需求的issue，主要涉及到deepseek-v2的AWQ支持问题，用户询问是否已经支持了。导致问题的原因可能是尚未支持该功能或者存在其他配置问题。,https://github.com/vllm-project/vllm/issues/6142
vllm,该issue是一个功能需求报告，主要涉及到在Qwen2模型中实现Dual Chunk Attention方法。由于需要扩展模型上下文长度，并且需要处理1M tokens输入，因此用户提出了对Qwen2模型集成Dual Chunk Attention方法的需求。,https://github.com/vllm-project/vllm/issues/6139
vllm,该issue类型为用户提出需求，主要涉及使用vLLM跨多个GPU进行推断及管理多个LoRA，可能是因为用户需要在该环境下进行模型推断并进行LoRA管理。,https://github.com/vllm-project/vllm/issues/6133
vllm,这是一个需求类型的issue，主要涉及到分布式单节点PP（Parallel Processing）多进程支持。由于缺乏PP多进程支持，用户想要在分布式环境中实现并行处理。,https://github.com/vllm-project/vllm/issues/6132
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm的server模式和GPU利用率。问题是用户想要充分利用GPU资源，但目前GPU利用率始终低于50%。,https://github.com/vllm-project/vllm/issues/6131
vllm,这是一个用户需求类型的issue，主要涉及了VLLM的核心功能中的多进程Pipeline Parallel支持，用户希望能够扩展支持多节点操作，而当前只适用于单节点操作。,https://github.com/vllm-project/vllm/issues/6130
vllm,这是一个用户提出需求的 issue，主要涉及 vllm 模型是否支持 embedding 输入，用户希望运行特定模型的推理，但不清楚如何将其集成到 vllm 中。,https://github.com/vllm-project/vllm/issues/6128
vllm,这是一个用户提出需求的issue，主要涉及vLLM在旧nvidia gpu上支持LoRA适配器的问题，由于vLLM依赖的upstream项目punicaai/punica不支持旧gpu，导致部分用户无法在Kaggle等环境中运行。,https://github.com/vllm-project/vllm/issues/6123
vllm,这是一个功能需求的issue，主要涉及到VLM（Vision and Language Multimodal）模块中计算模型最大多模态标记数量的功能添加。,https://github.com/vllm-project/vllm/issues/6121
vllm,这是一个需求提升类的issue，涉及的主要对象是VLLM项目的安装文档。用户建议提升建议的Python版本至3.10以与CI环境对齐，并为用户提供更好的用户体验。,https://github.com/vllm-project/vllm/issues/6119
vllm,这是一个用户提出的需求类型的issue，主要对象是VLLM的core组件和分布式系统。该需求是希望在管道并行大小大于1时，仍然能够在张量并行组中使用自定义的allreduce。,https://github.com/vllm-project/vllm/issues/6117
vllm,这是一个特性需求的issue，主要涉及pipeline parallel inference，由于不支持层大小不能被pp大小整除，导致需求提出。,https://github.com/vllm-project/vllm/issues/6114
vllm,这是一个代码质量改进的issue，涉及到vLLM下的`CompressedTensorsW8A8`。这个issue由于发现不需要在`act_scale`部分做分支，做了软件工程上的简化。,https://github.com/vllm-project/vllm/issues/6113
vllm,这个issue属于[Hardware][NV] Add support for ModelOpt static scaling checkpoints. 主要对象是为vLLM添加支持ModelOpt静态缩放检查点的功能。原因是需要通过HF加载ModelOpt检查点并且需要hf_quant_config.json文件来加载正确的量化格式。,https://github.com/vllm-project/vllm/issues/6112
vllm,这个issue是一个功能需求，主要涉及到在vllm项目中实现模拟的fp8推理。,https://github.com/vllm-project/vllm/issues/6111
vllm,这是一个关于支持Fp8的功能改进类 issue，主要涉及到vLLM项目中的`compressedtensors` Fp8模型的支持。由于新增了相关功能和重构了一些工具，可能是为了改进模型压缩和性能问题。,https://github.com/vllm-project/vllm/issues/6110
vllm,这个issue是关于添加额外模型的需求，而不是一个bug报告。,https://github.com/vllm-project/vllm/issues/6108
vllm,该issue属于功能增强请求类型，主要涉及修改vllm的基础镜像。,https://github.com/vllm-project/vllm/issues/6104
vllm,这个issue属于新功能添加类型，主要涉及的对象是Nvidia GPU。由于现有的vLLM方法预先分配了大量的缓存张量块，导致了复杂的手动块表管理、性能开销以及刚性的GPU内存利用，因此用户提出了增加vmm（虚拟内存管理）kv缓存的功能需求。,https://github.com/vllm-project/vllm/issues/6102
vllm,该issue是一个用户提出需求的类型，涉及的主要对象是为BloomForCausalLM添加LoRA支持。由于当前vLLM不支持LoRA适配器，用户提出希望添加LoRA适配器支持。,https://github.com/vllm-project/vllm/issues/6100
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是GLM4v VLM模型。用户想了解是否有计划支持这个模型，但未得到回复。,https://github.com/vllm-project/vllm/issues/6097
vllm,这个issue类型为文档改进，主要涉及vLLM Frontend的图片插入问题，由于markdown渲染不起作用，需要使用原始HTML标记。,https://github.com/vllm-project/vllm/issues/6091
vllm,这是一个用户提出需求的issue，主要涉及如何在openai api server中部署多个模型并为每个模型指定不同的GPU，用户想知道如何加载多个模型，并且允许用户通过指定不同的模型调用ChatCompletionRequest.model，同时为每个模型分配不同的GPU。,https://github.com/vllm-project/vllm/issues/6085
vllm,这是一个用户提出需求的类型，主要涉及到加速和部署使用vLLM的Llava系列和Phi3-Vision的最佳实践。问题是关于讨论vllm，用户在此寻求加速和部署LLM和多模态LLM的最佳实践。,https://github.com/vllm-project/vllm/issues/6084
vllm,这是一个用户请求帮助的issue，主要对象是vllm的使用方式。用户询问如何在vllm中集成特定模型以进行推理。,https://github.com/vllm-project/vllm/issues/6083
vllm,这个issue是关于[ Misc ] Refactor Marlin Python Utilities的一个需求，主要涉及更新compressedtensors和gptq_marlin以使用process_weights_after_loading来重新打包，以及将marlin实用程序提取出来供gptq_marlin和compressedtensors共享。,https://github.com/vllm-project/vllm/issues/6082
vllm,这是一个用户提出需求的类型，主要涉及Attention机制中`kv_scale`的拆分为`k_scale`和`v_scale`。由于目前的实现没有分开处理`key`和`value`的缩放，导致了在FP8模型中存在的精度损失。,https://github.com/vllm-project/vllm/issues/6081
vllm,该issue属于功能需求类型，主要涉及的对象是平台抽象化。原因是为了消除代码中的`ifelse`模式，将平台识别集中管理，从而实现平台抽象化的目标。,https://github.com/vllm-project/vllm/issues/6080
vllm,这个issue属于用户提出需求，主要涉及的对象是在VLLM中添加对可交换基数注意力的支持。原因是用户希望通过调整基数注意力功能来提高特定设置下的计算效率。,https://github.com/vllm-project/vllm/issues/6078
vllm,该issue属于功能请求类型，主要涉及的对象是vLLM的调度机制。由于当前vLLM仅支持基于请求到达时间的先来先服务调度，用户提出增加优先级调度机制以实现请求的优先排序，以实现细粒度调度和公平性维护的需求。,https://github.com/vllm-project/vllm/issues/6077
vllm,这个issue类型是用户提出需求，涉及的主要对象是vLLM在Kubernetes上的运行。由于vLLM的 `/health` endpoint 在服务启动后才返回 HTTP 200，导致难以正确配置`liveness check`，需要添加 `/ready` endpoint 来提前返回状态。,https://github.com/vllm-project/vllm/issues/6073
vllm,这是一个用户请求使用说明的类型，主要涉及gemm2-27b的初始化问题，用户感到困惑并寻求如何使用4位量化的帮助。,https://github.com/vllm-project/vllm/issues/6068
vllm,这是一个用户提出需求的issue，主要对象是是否支持Ascend 910B，由于对该硬件的支持尚未确定，用户想了解未来是否会支持。,https://github.com/vllm-project/vllm/issues/6066
vllm,这是一个用户提出需求的issue，主要涉及如何在请求OpenAI Completions API时使用beam search，可能出现问题是由于缺乏正确的使用方法或参数传递导致的。,https://github.com/vllm-project/vllm/issues/6057
vllm,这是一个用户提出需求的issue，主要涉及对象是vllm模型的快速注意力机制。导致此问题的原因可能是部署模型时耗时过长。,https://github.com/vllm-project/vllm/issues/6053
vllm,这是一个用户提出需求的issue，主要涉及VLLM中attention backend的内核统一问题。这个问题由于目前使用不同内核导致了attention backend逻辑复杂化和性能子优。,https://github.com/vllm-project/vllm/issues/6052
vllm,这个issue类型是功能需求提出，涉及到vllm项目中Kernel和Model的调整，主要是为了满足Gemma2模型使用flashinfer需要添加logits_soft_cap功能。由于Gemma2模型需要logits_soft_cap支持，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/6051
vllm,这是一个特性需求的issue，涉及MLPSpeculator的Tensor Parallel支持，主要是为了实现对draft worker和target worker不同Tensor Parallel大小的支持。,https://github.com/vllm-project/vllm/issues/6050
vllm,这是一个关于在VLLM中创建带有引导解码功能的示例文件的问题，提出了关于正则表达式和引导输入方面的困惑，可能由于markdown渲染问题，导致展示不完整。,https://github.com/vllm-project/vllm/issues/6045
vllm,这个issue是一个功能增强请求，主要涉及的对象是支持Microsoft Runtime Kernel Library来增强低精度计算能力。原因是为了优化混合精度DNN模型部署，特别是在大型语言模型中的量化计算。,https://github.com/vllm-project/vllm/issues/6036
vllm,这是一个正在进行中的功能添加issue，主要涉及向模型添加辅助头并在serving时获取分数。,https://github.com/vllm-project/vllm/issues/6035
vllm,这是一个功能改进的Issue，涉及VLLM的SPMD工作执行和Ray加速DAG，通过引入SPMD执行模式来改善性能。,https://github.com/vllm-project/vllm/issues/6032
vllm,这是一个用户提出需求的类型，主要涉及对支持量化kv缓存（压缩张量）的改进。由于用户需要加载量化模型并使用额外的kv缓存压缩，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/6028
vllm,这是用户提出需求的类型，主要对象是使用Offline Batched Inference来运行多个对话，由于用户想要实现这个功能但不清楚如何操作。,https://github.com/vllm-project/vllm/issues/6022
vllm,这个issue是一个文档更新类型的问题，主要涉及到更新benchmark backend for scalellm。由于markdown渲染不起作用，所以使用了原始的html代码。,https://github.com/vllm-project/vllm/issues/6018
vllm,"这个issue类型是需求提出，主要涉及对象是新增模型""facebook/seamless-m4t-v2-large""。用户提出了关于支持未被vllm项目现有模型所支持的SeamlessM4Tv2Model的需求，并指出了相关架构以及需要解决的问题。",https://github.com/vllm-project/vllm/issues/6017
vllm,这个issue是关于用户需求的问题，涉及的主要对象是vllm模型加载。由于缺少`config.json`文件，用户无法加载本地路径上的模型，询问是否支持加载huggingface本地pytorch pt模型或onnx模型以及如何在离线Python代码和OpenAI Completions API中加载这些模型。,https://github.com/vllm-project/vllm/issues/6012
vllm,该issue是一个Feature请求，主要涉及的对象是在ROCm上启用了Scaled FP8 GEMM，因为新AMD硬件（MI30x及更高版本）的加速计算。,https://github.com/vllm-project/vllm/issues/6006
vllm,这是一个文档更新类的issue，涉及主要对象是vLLM的CPU支持描述，可能是由于描述过时导致用户发现与实际情况不符，希望修正。,https://github.com/vllm-project/vllm/issues/6003
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm。由于vllm不支持特定的格式，导致用户无法使用该工具进行推断。,https://github.com/vllm-project/vllm/issues/5992
vllm,这是一个“功能增强”类型的issue，主要涉及构建CI/Build流程中代码重用且目的是简化新端到端测试的添加。,https://github.com/vllm-project/vllm/issues/5988
vllm,这是一个用户需求反馈问题，主要涉及支持BartForSequenceClassification模型的请求。由于该模型目前不被支持，导致用户无法成功运行该模型。,https://github.com/vllm-project/vllm/issues/5985
vllm,这是一个用户提出需求的issue，主要涉及的对象是RotaryEmbedding。由于未提供具体内容，无法分析导致的症状或问题具体是关于什么的。,https://github.com/vllm-project/vllm/issues/5984
vllm,这是一个用户提出需求的issue，主要涉及SPMD worker执行模式的引入，目的是减少系统性能开销并提升任务执行效率。,https://github.com/vllm-project/vllm/issues/5980
vllm,这是一个用户提出需求的类型，主要涉及的对象是将最新的html设为默认选项。,https://github.com/vllm-project/vllm/issues/5979
vllm,这个issue是关于功能需求的，主要涉及vLLM中支持Ampere GPU上FP8的扩展，问题提出了关于如何改进性能和内存利用的讨论。,https://github.com/vllm-project/vllm/issues/5975
vllm,这是一个性能优化的issue，主要涉及的对象是`SequenceStatus.is_finished`方法。由于每次调用该方法时都会创建一个列表，导致性能较慢，因此通过切换到`IntEnum`来进行简单的比较可以加快速度。,https://github.com/vllm-project/vllm/issues/5974
vllm,"这个issue类型为用户提出需求，主要涉及的对象是模块""yum install""。由于未提供具体内容，无法分析导致的症状或问题。",https://github.com/vllm-project/vllm/issues/5973
vllm,这个issue类型是功能改进。主要对象是`get_min_capability`方法。由于旧方法不够灵活，需要更新为类方法以支持所有压缩张量方案。,https://github.com/vllm-project/vllm/issues/5971
vllm,这是一个优化类型的issue，主要涉及数据结构在分配器中的优化，通过更改数据结构可以提高性能。,https://github.com/vllm-project/vllm/issues/5968
vllm,这个issue属于功能需求，主要涉及的对象是MLPSpeculator模型的新特性。这个需求是由于需要减小模型的大小并在vLLM中支持这两个新功能。,https://github.com/vllm-project/vllm/issues/5965
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM的API Server。由于没有提供`/info` endpoint，用户提出需要获取模型配置信息的需求。,https://github.com/vllm-project/vllm/issues/5959
vllm,这是一个关于添加优先调度功能的GitHub issue，涉及vLLM的核心功能。,https://github.com/vllm-project/vllm/issues/5958
vllm,这是一个需求类型的issue，主要涉及支持在CI上运行多节点测试的脚本。,https://github.com/vllm-project/vllm/issues/5955
vllm,这个issue类型是用户提出需求，主要对象是希望支持从Google获取的Gemma2模型。由于vllm已经支持最接近的模型gemma，用户在询问如何支持他们想要的模型，但没有得到回应。,https://github.com/vllm-project/vllm/issues/5953
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm项目中的LLM类和其相关方法。用户提出了希望vllm支持使用LLM的最后隐藏状态嵌入向量的功能。,https://github.com/vllm-project/vllm/issues/5950
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm，用户想要在多节点多CPU环境下运行推理。,https://github.com/vllm-project/vllm/issues/5948
vllm,这个issue类型为用户提出需求，主要对象是关于添加FAQ文档到'serving'模块。由于用户提到了在短期内解决常见问题和在长期内建立一个常见问题的知识库，表明用户希望解决项目中一些常见问题并提供更好的支持和文档。,https://github.com/vllm-project/vllm/issues/5946
vllm,这是一个用户提出需求的issue，主要涉及对象是如何将vllm保存日志到文件。原因是用户希望能够将日志保存到文件而不是仅在屏幕上显示。,https://github.com/vllm-project/vllm/issues/5945
vllm,这个issue是关于硬件优化的需求，涉及对象为Intel CPU，用户希望使用ipex varlen attention来计算prompts以获得更好的性能。根据描述，这个需求是为了提高计算效率和代码质量。,https://github.com/vllm-project/vllm/issues/5943
vllm,这是一个优化类型的Issue，主要对象是vllm项目中的代码。由于使用了一个numpy ndarray对象池来保存序列数据，导致了提升了请求速率和tokens速率。,https://github.com/vllm-project/vllm/issues/5942
vllm,这个issue是关于Kernel部分增加per-tensor和per-token AZP epilogues的需求，属于其他类型的贡献请求。,https://github.com/vllm-project/vllm/issues/5941
vllm,该issue类型是需求提出，主要对象是前端部分，由于当前版本存在一些障碍使得base64 embedding无法启用。,https://github.com/vllm-project/vllm/issues/5935
vllm,"该issue类型为用户提出需求，主要涉及的对象是添加支持新模型""microsoft/Florence2base""。由于尚未收到任何响应，用户希望知道支持他们想要的模型存在何种困难。",https://github.com/vllm-project/vllm/issues/5934
vllm,该issue类型为性能优化（Enhancement）请求，主要对象是针对mi300X的fused_moe Triton内核，由于调整后的配置文件带来了约10%的性能提升。,https://github.com/vllm-project/vllm/issues/5932
vllm,这是一个功能需求问题，涉及前端代码，要求在tokenize端点中支持聊天完成输入。,https://github.com/vllm-project/vllm/issues/5923
vllm,这是一个需求报告，主要涉及的对象是Blip2模型支持，由于需要实现BLIP2模型以及BLIP2支持问题而产生。,https://github.com/vllm-project/vllm/issues/5920
vllm,这是一个用户提出需求的 issue，主要涉及提高模型贪心解码效率的方法讨论，由于模型在 confidence 低时不必要的推测导致速度下降。,https://github.com/vllm-project/vllm/issues/5914
vllm,这个issue类型是用户提出需求，主要涉及的对象是为VLLM模型增加对新的Gemma 2模型的支持，由于当前尚未支持这些新模型，用户希望能够为其添加支持。,https://github.com/vllm-project/vllm/issues/5912
vllm,这个issue类型是改进提案，主要涉及前端代码，处理用户设定的`max_model_len`超出模型参数的错误条件，改为警告，以适应特定需求。原因是先前用户超出最大长度可能触发错误，现在改为警告，提高灵活性。,https://github.com/vllm-project/vllm/issues/5911
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加对Lora适配器的分布式推理支持。由于Lora适配器在加载70B模型时遇到问题，导致生成吞吐量和提示吞吐量为0。,https://github.com/vllm-project/vllm/issues/5891
vllm,这是一个需求提出类型的issue，主要涉及vLLM中的SmartSpec方法是否会被实现到主代码库中。,https://github.com/vllm-project/vllm/issues/5886
vllm,这个issue是一个性能优化类型的issue，主要对象是逻辑块。由于不需要逻辑块，导致了性能提升的bug。,https://github.com/vllm-project/vllm/issues/5882
vllm,这是一个优化类型的issue，主要涉及到使用numpy ndarray池来存储序列数据。原因是为了提高性能并确保在用户请求中只使用Python列表。,https://github.com/vllm-project/vllm/issues/5877
vllm,这是一个功能优化的issue，主要涉及重构线性层权重加载的逻辑和引入新的参数类型。,https://github.com/vllm-project/vllm/issues/5874
vllm,这个issue类型是文档更新类，涉及主要对象是环境变量的使用，由于之前环境变量的使用方式可能引发了冲突，需要更新文档避免此类问题。,https://github.com/vllm-project/vllm/issues/5873
vllm,这是一个用户提出需求的类型。该问题单涉及的主要对象是在TPU上实现张量并行。由于需求是在TPU上实现分布式推理支持，从而为该需求提供基于Ray的TPU执行器。,https://github.com/vllm-project/vllm/issues/5871
vllm,这是一个用户提出需求的 issue，主要涉及 vllm 在线推理时无法找到 'stream' 开关所导致的问题。,https://github.com/vllm-project/vllm/issues/5862
vllm,这是一个用户提出需求的类型的issue，主要涉及支持在vllm中记录模型的输入和输出至文件的功能。,https://github.com/vllm-project/vllm/issues/5859
vllm,这是一个用户提出需求的issue，主要涉及的对象是Optimumquanto quantized models，用户想知道是否可能通过vllm来加速这些模型。,https://github.com/vllm-project/vllm/issues/5858
vllm,这是一个关于需求提出的问题单，主要涉及VLLM模型的benchmark测试，因为目前只支持Sonnet和ShareGPT数据集，但输入和输出长度不固定，提出需求实现固定输入和输出长度的benchmark。,https://github.com/vllm-project/vllm/issues/5857
vllm,这是一个特性改进的issue，涉及vllm项目中的speculative decoding功能，主要对象是支持draft model在不同的tensor-parallel大小上运行。最后的问题可能是关于性能改进或功能扩展的需求。,https://github.com/vllm-project/vllm/issues/5856
vllm,这是一个关于提升TPU后端并行采样和交换功能的功能需求，涉及的主要对象是TPU后端。,https://github.com/vllm-project/vllm/issues/5855
vllm,这是一个用户提出需求的issue，主要涉及自动合并测试PR的功能。,https://github.com/vllm-project/vllm/issues/5837
vllm,这是一个功能请求，涉及修改HF（HuggingFace）模型的`config.json`文件。由于需要修改HF模型的配置文件以及与OpenAI兼容的服务器测试，所以提出了相关问题。,https://github.com/vllm-project/vllm/issues/5836
vllm,这个issue是一个功能需求（Feature request），主要涉及的对象是对vLLM实现是否支持分布式推理（distributed speculative inference）的问题。,https://github.com/vllm-project/vllm/issues/5835
vllm,该issue属于需求提出类型，涉及到对gloo和nccl通信测试的添加。原因是需要测试gloo通信，并相应地设置`GLOO_SOCKET_IFNAME`/`NCCL_SOCKET_IFNAME`。,https://github.com/vllm-project/vllm/issues/5834
vllm,这个issue类型是功能改进，主要对象是TPU backend。,https://github.com/vllm-project/vllm/issues/5831
vllm,这是一个用户提出需求的issue，主要涉及vLLM中多LoRA问题实现的具体细节。该问题探讨了adapter切换的机制，包括基于队列的权重因素和GPU内存中adapter数控制等，用户希望了解如何管理adapter以优化模型服务器的性能。,https://github.com/vllm-project/vllm/issues/5830
vllm,该issue属于代码优化类型，主要涉及到cpu_worker文件中的无用代码。原因可能是需要简化代码结构或者提高代码执行效率。,https://github.com/vllm-project/vllm/issues/5824
vllm,该issue属于功能增强类型，主要涉及到测试图像的使用方式。这个改动是为了提高使用测试图像的便利性，通过将相关方法整理到新的类中（ImageAsset）。,https://github.com/vllm-project/vllm/issues/5821
vllm,这是一个功能需求，主要涉及对象是针对vllm项目中的Phi-3视觉模型，用户希望能够在该模型中支持多个图像输入，而不仅仅限于单个图像。,https://github.com/vllm-project/vllm/issues/5820
vllm,该issue属于文档更新类型，主要对象是VLMs的用户。这个问题是由于计划进行几项对VLMs的重大更改，需要提前通知用户。,https://github.com/vllm-project/vllm/issues/5818
vllm,这是一个需求提出类型的issue，主要涉及的对象是为vllm项目添加对DeepSeek VL的支持。由于缺乏对DeepSeek VL的支持，用户希望在该模型中进行测试并修复相关问题。,https://github.com/vllm-project/vllm/issues/5817
vllm,这个issue类型是提出需求，涉及的主要对象是CI workflow for PRs，由于当前CI成本过高，需要采取措施降低CI成本，因此提出了更改CI工作流程的建议。,https://github.com/vllm-project/vllm/issues/5816
vllm,这是一个功能增强类型的issue，主要涉及vLLM中的bitsandbytes量化模块，并提供了对张量并行性的支持。,https://github.com/vllm-project/vllm/issues/5813
vllm,这个issue类型是需求提出，该问题单涉及的主要对象是将分布式推理中的模型注册支持外部扩展，原因是当前系统在分布式推理环境中缺乏对模型的外部注册支持。,https://github.com/vllm-project/vllm/issues/5810
vllm,这是一个功能需求类型的issue，主要涉及到MLPSpeculator Tensor Parallel支持的问题。,https://github.com/vllm-project/vllm/issues/5809
vllm,这是一个用户提出需求的issue，主要对象是支持MiniCPM-Llama3-V-2_5模型。由于目前vllm 0.5.0.post1版本不支持该模型，用户希望添加对其的支持。,https://github.com/vllm-project/vllm/issues/5808
vllm,这个issue属于需求提出类型，主要对象是vLLM项目的开发和规划。用户提出了vLLM在Q3 2024路线图中要实现的主要特性和目标计划，并邀请社区参与讨论和贡献。,https://github.com/vllm-project/vllm/issues/5805
vllm,这是一个用户提出需求的Issue，主要涉及到vLLM logger在默认情况下禁用其他已存在的loggers，可能导致其他代码无法输出日志的问题。,https://github.com/vllm-project/vllm/issues/5803
vllm,这是一个功能需求类型的issue，主要涉及集成测试和模型质量测试的需求。由于vLLM新增了更多适用于CI的节点，需要添加模型质量测试以确保内核和调度器的更改不会降低模型的准确性表现，并且保证不会破坏lmevalharness的集成。,https://github.com/vllm-project/vllm/issues/5800
vllm,这个issue类型是功能提议，涉及的主要对象是DraftModelRunner，由于需要在模型执行过程中实现多个功能并行，导致需要引入DraftModelRunner来优化性能。,https://github.com/vllm-project/vllm/issues/5799
vllm,这个issue属于用户提出需求类型，主要涉及vllm的推理请求缓存设置。用户询问是否需要启用缓存以及缓存保存位置，原因可能是希望了解如何利用缓存来处理类似的推理请求。,https://github.com/vllm-project/vllm/issues/5796
vllm,这是一个特性需求类的issue，主要涉及到对`compressed-tensors`支持`w8a16`模型进行扩展。由于需要支持`w8a16`模型并允许使用相同的内核，所以需要将支持的方案从`CompressedTensorsW4A16`更改为`CompressedTensorsWNA16`。,https://github.com/vllm-project/vllm/issues/5794
vllm,"这是一个用户提出需求的issue，主要涉及文档页面的编辑功能，用户希望增加一个""建议编辑""按钮便于直接修改文档。",https://github.com/vllm-project/vllm/issues/5789
vllm,这是一个用户提出需求的issue，主要涉及到vllm的benchmark_latency.py脚本，用户在运行该脚本后发现生成的JSON文件过大，希望减小文件大小或获得分析工具推荐。,https://github.com/vllm-project/vllm/issues/5787
vllm,这是一个建议类型的issue， 主要涉及到文档问题。由于缺少下载模型的警告，用户可能会遇到下载模型的问题或困惑。,https://github.com/vllm-project/vllm/issues/5783
vllm,这是一个需求类型的issue，主要涉及的对象是vllm项目中的cuda kernels，由于使用32位整数作为索引可能导致溢出问题，建议使用64位整数以提高安全性。,https://github.com/vllm-project/vllm/issues/5781
vllm,这个issue属于用户提出需求类型，主要涉及问题是启动后缺少特定方法，可能是由于配置不正确导致的。,https://github.com/vllm-project/vllm/issues/5780
vllm,这个issue类型是提出需求，主要涉及vLLM架构的分布式推断，由于当前架构不够灵活，导致添加推测解码和协作进程时出现困难。,https://github.com/vllm-project/vllm/issues/5775
vllm,这个issue属于需求提出类型，主要对象是支持Qwen2系列模型，由于当前版本不支持Qwen2系列模型的大小，导致无法运行该系列模型。,https://github.com/vllm-project/vllm/issues/5773
vllm,这是一个用户提出需求的issue，主要对象是vLLM的shm broadcast功能。由于固定chunk size导致共享内存空间浪费，需要支持变长对象以提高灵活性。,https://github.com/vllm-project/vllm/issues/5768
vllm,这是一个关于功能需求的issue，主要涉及的对象是CI（持续集成），提出的问题是关于启用分布式测试的功能。,https://github.com/vllm-project/vllm/issues/5766
vllm,这个issue是关于性能优化的需求，主要涉及的对象是在KV缓存模型中实现了特定优化功能的MultiStepWorker。由于未清除序列 ID 列表中的序列 ID，导致在序列结束时无法将其移除，需要找到一种方法来在序列结束时通知并移除这些序列 ID。,https://github.com/vllm-project/vllm/issues/5765
vllm,该issue属于向开发团队提出新模型支持的请求，涉及的主要对象是DeepSeekCoderV2模型。用户提出该请求的原因是他们在将模型端口到vLLM时缺乏经验。,https://github.com/vllm-project/vllm/issues/5763
vllm,这是一个用户提出需求的issue，主要涉及的对象为设置vllm中--max-logprobs参数。由于用户在使用不同的LLM时需要手动设置max-logprobs值为词汇表的大小，希望能有简便的方式将max-logprobs默认设置为词汇表的大小。,https://github.com/vllm-project/vllm/issues/5762
vllm,这个issue是一个需求问题，主要涉及安装vllm时内存不足导致OOM错误。,https://github.com/vllm-project/vllm/issues/5758
vllm,这个issue类型是用户提出需求，主要涉及到在跨节点广播时添加消息队列，驱动进程移出tp 0 worker以提高效率，并需要添加多节点测试。,https://github.com/vllm-project/vllm/issues/5755
vllm,这个issue是用户提出需求，请求支持对bnb预量化模型的读取。该问题涉及的主要对象是vllm。,https://github.com/vllm-project/vllm/issues/5753
vllm,这是一个需求提出类型的issue，主要涉及了支持稀疏KV缓存框架的问题。原因是当前大型模型推断中，KV缓存占用了重要的GPU内存，减小KV缓存的大小是一项重要的改进方向。,https://github.com/vllm-project/vllm/issues/5752
vllm,这个issue类型是需求提出，主要对象是支持稀疏KV缓存框架。由于当前大型模型推断中KV缓存占据了大部分GPU内存，因此减小KV缓存的大小是一个重要的改进方向。,https://github.com/vllm-project/vllm/issues/5751
vllm,这个issue是一个功能需求，主要涉及的对象是对比sparseml和vllm模型在压缩张量上的准确性，用户希望通过对比验证sparseml生成的token ids是否位于vllm生成的top_n logprobs之中。,https://github.com/vllm-project/vllm/issues/5750
vllm,该issue属于用户提出需求类型，主要涉及文档中对支持硬件的描述，用户希望添加一个表格以详细说明各内核支持的硬件情况。,https://github.com/vllm-project/vllm/issues/5745
vllm,这个issue是一个功能需求的提出，主要涉及到前端的持续使用统计功能，由于需要实现`Continuous streaming of UsageInfo`，导致了需要添加这一功能。,https://github.com/vllm-project/vllm/issues/5742
vllm,这个issue是关于功能需求的，主要对象是vLLM项目不支持非x86架构的CPU推理，由于需求支持PowerPC架构，提出了需要在项目中考虑非x86平台的问题。,https://github.com/vllm-project/vllm/issues/5741
vllm,这是一个用户提出需求的类型，涉及主要对象为Langchain，由于Langchain无法访问OpenAIEmbeddings导致用户请求支持或寻求帮助。,https://github.com/vllm-project/vllm/issues/5734
vllm,该issue属于功能需求类型，主要涉及支持LoRA中的偏置设置，并且由于需要支持调整偏置来使adapters与vLLM兼容。,https://github.com/vllm-project/vllm/issues/5733
vllm,这个issue是一个提出需求的请求，主要涉及到vLLM在在线RL训练过程中的权重更新问题，用户希望实现在运行时同步权重，以便加速训练过程。,https://github.com/vllm-project/vllm/issues/5723
vllm,该issue属于用户提出需求类型，主要涉及对象为新模型Nemotron-4-340B，用户询问是否计划支持该模型以及是否已经在进行相关工作。,https://github.com/vllm-project/vllm/issues/5722
vllm,该issue类型为需求提出，主要涉及的对象是支持Chameleon模型的集成。这个问题的原因是用户希望vllm项目能够支持Chameleon模型，但由于Chameleon模型与现有的vllm框架存在一些区别，因此需要进行调整和改进。,https://github.com/vllm-project/vllm/issues/5721
vllm,这是一个用户提出需求类型的issue，主要对象是分布式功能。,https://github.com/vllm-project/vllm/issues/5719
vllm,这个issue是一个功能需求，主要涉及的对象是vllm项目的前端。这个需求由于用户希望支持在参数命名中同时使用下划线和短横线，因此提出了添加FlexibleArgumentParser来处理这一要求。,https://github.com/vllm-project/vllm/issues/5718
vllm,这是一个用户提出需求的RFC（Request For Comments），主要涉及LLM类在资源清理方面的改进。原因是当前项目中存在资源未释放的问题，用户需求是能够在上下文管理器中自动释放资源。,https://github.com/vllm-project/vllm/issues/5716
vllm,这是一个功能需求的issue，涉及到对HuggingFace的`ChatCompletionRequest`类添加模板相关参数，由于需要控制`apply_chat_template`的行为和模板，因此引发了这个问题。,https://github.com/vllm-project/vllm/issues/5709
vllm,这是一个功能需求类型的问题，主要涉及的对象是vLLM的`UsageInfo`，用户提出持续流式传输`UsageInfo`的需求，以解决在性能评估过程中遇到的“账务”问题。,https://github.com/vllm-project/vllm/issues/5708
vllm,"这是一个[改进建议]类型的issue，主要涉及""top1_proposal output tensor initialization""。由于使用`torch.tensor().expand()`操作进行初始化导致效率低下，用户建议采用`torch.full()`和`torch.zero()`操作来改善初始化效率和代码可读性。",https://github.com/vllm-project/vllm/issues/5706
vllm,这是一个用户提出需求的issue，主要对象是支持torch==2.3.1版本。由于当前代码不支持该版本，导致用户需要请求相关支持。,https://github.com/vllm-project/vllm/issues/5705
vllm,这是一个用户提出需求的问题，主要涉及到在NVIDIA多型号的GPU上实现vllm在多卡上的并行运行的功能。这个问题的原因是目前vllm本地部署模型加载时只使用了一块RTX3060显卡，无法成功运行9B的模型，用户希望能够在多个卡上实现并行运行。,https://github.com/vllm-project/vllm/issues/5704
vllm,这是一个关于性能改进的问题。主要讨论对象是在vllm中使用flash attention算法的选择及其性能考量。该问题探讨选用flash_attn_varlen_func()而非其他实现版本的理由，以及是否带来更优的速度表现。,https://github.com/vllm-project/vllm/issues/5702
vllm,这个issue是一个优化建议，主要涉及sampler的优化，由于top_p=1且top_k>0时，排序不是必要的且不如topk高效。,https://github.com/vllm-project/vllm/issues/5698
vllm,这是一个需求类型的issue，主要涉及使用MQA kernel进行目标模型验证，但由于添加cuda图支持和其他困难导致了一些实现上的困难。,https://github.com/vllm-project/vllm/issues/5691
vllm,这个issue是属于用户提出需求类型，主要对象是在ci和distributed方面加入关于自定义allreduce的测试。,https://github.com/vllm-project/vllm/issues/5689
vllm,这是一个关于改进GPU分布式执行器的issue，主要涉及到对xpu设备的重构和改进。,https://github.com/vllm-project/vllm/issues/5685
vllm,"这是一个用户提出需求的类型，主要对象是要添加一个名为""facebook/multi-token-prediction""的新模型，原因是为了加速模型推理过程。",https://github.com/vllm-project/vllm/issues/5683
vllm,"这个issue是关于代码质量的改进，提出了对""launch_tgi_server.sh""脚本进行参数化。",https://github.com/vllm-project/vllm/issues/5676
vllm,这是一个功能需求类型的issue，主要涉及设置文件目录的创建，但目前尚未实现。,https://github.com/vllm-project/vllm/issues/5673
vllm,该issue类型为需求报告，主要涉及对象为vllm类LLM的CPU推理支持。由于目前的class LLM不支持在CPU上运行推理，用户在尝试在CPU上运行时遇到了问题，希望了解是否有替代方案或解决方法。,https://github.com/vllm-project/vllm/issues/5664
vllm,该issue属于功能需求添加类型，主要涉及Model中添加FP8 KV cache for Qwen2，用户提出了扩展FP8 KV缓存加载功能的需求。,https://github.com/vllm-project/vllm/issues/5656
vllm,该issue类型为功能需求，涉及主要对象为支持CPU inference with VSX PowerPC ISA。原因是需要提供基本的CPU支持供PowerPC（Power9和POWER10）使用。,https://github.com/vllm-project/vllm/issues/5652
vllm,这个issue是功能增强类型，涉及主要对象是`CompressedTensorsW8A8StaticTensor` scheme。由于之前的scheme只支持per-tensor weight quantization，用户提出扩展为支持channelwise quantization的需求。,https://github.com/vllm-project/vllm/issues/5650
vllm,这是一个特性需求，主要涉及到OpenAI-Compatible Tools API + Streaming for Hermes & Mistral models。由于OpenAI-style tool calling的支持尚未添加，用户提出了对应的功能需求。,https://github.com/vllm-project/vllm/issues/5649
vllm,这个issue类型是用户提出需求，涉及主要对象是在AWS CI模板中为A100 GPU添加一个队列，导致用户希望能够使用A100 GPU来运行在`testpipeline.yaml`中定义为`gpu: a100`的步骤。,https://github.com/vllm-project/vllm/issues/5648
vllm,此issue类型为技术改进，主要涉及移除nvidia runtime docker基础镜像，由于不希望依赖该镜像中的任何内容，试图修复与nvidia runtime镜像相关的问题。,https://github.com/vllm-project/vllm/issues/5646
vllm,"这个issue类型是用户提出需求，主要涉及的对象是vLLM的RESTful API server。由于设定参数""--max-num-seqs""值为默认值256或大于6时无法生效，可能是由于参数设定方式或API server限制导致。",https://github.com/vllm-project/vllm/issues/5634
vllm,这是一个功能需求类型的issue，主要涉及benchmark_latency.py脚本，由于长上下文可能导致模型初始化失败，用户提出添加max-model-len参数限制GPU内存使用。,https://github.com/vllm-project/vllm/issues/5629
vllm,这是一个关于性能问题的用户需求类型的Issue，涉及主要对象是GPU KV cache的利用率。用户怀疑GPU KV缓存使用率过低导致生成吞吐速度慢，希望了解如何增加GPU KV缓存使用率以提高生成吞吐速度。,https://github.com/vllm-project/vllm/issues/5626
vllm,这是一个功能改进类的issue，主要涉及的对象是CI模板。由于现在默认模板应该使用`testtemplateaws.j2`，因此需要移除原先的模板`testtemplate.j2`。,https://github.com/vllm-project/vllm/issues/5624
vllm,这个issue类型是文档更新，涉及的主要对象是Dockerfile相关的文档内容。由于当前文档不易于新用户查找相关信息，需要进行小的导航改进，清理部分内容冗余和嵌套的项目符号。,https://github.com/vllm-project/vllm/issues/5614
vllm,这是一个用户提出需求的类型，主要涉及到Release pipeline的设置和构建发布wheels的优化，由于Dockerfile的调整，引发了需要重新设置Release pipeline的问题。,https://github.com/vllm-project/vllm/issues/5610
vllm,该issue属于功能需求类型，主要涉及的对象是VLLM模型的生成过程。由于用户希望能够在生成过程中控制初始token，以减少模型输出中的随机性，因此提出了这个功能需求。,https://github.com/vllm-project/vllm/issues/5609
vllm,该issue类型为功能新增，主要涉及对象为scheduler。导致这个问题的原因是需要在scheduler方法add_seq_group中添加用户信息作为输入参数，并在OpenAI Completions API中使用时支持用户信息传入。,https://github.com/vllm-project/vllm/issues/5608
vllm,这是一个用户提出需求的issue，主要涉及的是如何在调度程序中获取用户信息。用户希望能够基于用户创建调度策略，但目前调度程序无法获取用户标识符，希望能够找到其他方法来获取这些信息。,https://github.com/vllm-project/vllm/issues/5605
vllm,这是一个功能需求类型的issue，主要涉及LoRA适配器在LRU缓存中的固定支持。,https://github.com/vllm-project/vllm/issues/5603
vllm,这个issue类型是优化性质的，主要涉及block_manager_v2和block_manager_v1的性能优化对比。原因是为了使block_manager_v2成为默认选项，并优化了在批处理中的Block对象分配/释放问题。,https://github.com/vllm-project/vllm/issues/5602
vllm,这是一个需求提出的issue，主要对象是支持Qwen2 embedding，问题由于当前最佳嵌入模型AlibabaNLP/gteQwen27Binstruct在使用嵌入端点时返回错误，用户希望能支持除E5mistral外的其他嵌入模型。,https://github.com/vllm-project/vllm/issues/5600
vllm,这是一个用户提出需求的issue，主要涉及VLLM项目中的配置参数设置问题。造成该问题的原因是默认参数限制导致无法满足用户对并行VLLM作业提交和集成的需求。,https://github.com/vllm-project/vllm/issues/5598
vllm,这个issue属于模型改进类型，涉及到Phi3 rope scaling type的命名问题。原因在于之前的命名方式混乱，需要进行更正。,https://github.com/vllm-project/vllm/issues/5595
vllm,这个issue是一个Feature请求，主要涉及vLLM的benchmark script和组件的解耦。用户提出需求将`get_tokenizer`函数提取到`backend_request_func`中，避免依赖vLLM组件的问题。,https://github.com/vllm-project/vllm/issues/5586
vllm,该issue类型为用户提出需求，并询问如何在使用vllm时指定GPU。该问题涉及主要对象为vllm框架及其API服务器。用户提出问题的原因是不知道如何在vllm中集成特定模型以在GPU上运行推理。,https://github.com/vllm-project/vllm/issues/5585
vllm,这个issue类型是优化建议，主要涉及的对象是通过重复使用LogicalTokenBlock.token_ids来提高性能。,https://github.com/vllm-project/vllm/issues/5584
vllm,这是一个关于文档更新的需求，主要涉及多节点调试和崩溃调试方面的帮助提示。,https://github.com/vllm-project/vllm/issues/5581
vllm,这是一个用户提出需求的issue，主要对象是VLLM前端。由于目前加载权重时无法显示模型的峰值内存使用，用户希望能添加这一功能，以便更好地监控模型的内存使用情况。,https://github.com/vllm-project/vllm/issues/5580
vllm,这是一个用户提出需求的issue，主要涉及支持非AVX512的vLLM构建和测试。原因是为了使vLLM能够用AVX2构建并进行简单测试。,https://github.com/vllm-project/vllm/issues/5574
vllm,这是一个改进性的issue，主要涉及的对象是OpenAI Completions API。由于之前仅测试了批处理文本数据，现在需要验证是否逻辑处理批处理文本和批处理token IDs。,https://github.com/vllm-project/vllm/issues/5568
vllm,该issue属于用户提出需求类型，主要涉及到vLLM版本控制和开发版本管理的问题。由于当前的版本控制方式导致开发版本与发布版本等同，在热修复和补丁发布等方面存在困难，用户希望提出新的版本控制方案进行讨论和改进。,https://github.com/vllm-project/vllm/issues/5565
vllm,这个issue类型是性能优化建议，涉及主要对象是vLLM的autoregressive proposal方法，由于sampler在执行时浪费了时间和计算资源导致性能不佳。,https://github.com/vllm-project/vllm/issues/5561
vllm,这个issue是关于添加bias epilogue支持到`cutlass_scaled_mm`的Kernel功能修改。,https://github.com/vllm-project/vllm/issues/5560
vllm,这是一个用户提出需求的类型的issue，主要涉及了如何实现通过KV缓存传输实现分理预填（disaggregated prefilling）。这个问题涉及了如何在不同vLLM实例之间转移KV缓存的需求。,https://github.com/vllm-project/vllm/issues/5557
vllm,这是一个需求提出类型的issue，主要涉及的对象是amd，用户提出需要将ccache添加到项目中以减少每次运行重新编译导致长达40分钟的构建时间。,https://github.com/vllm-project/vllm/issues/5555
vllm,该issue为测试请求，不是bug报告。主要涉及的对象为CI的功能。由于需要对CI进行测试，用户提交了这个PR来请求CI的功能验证，但请求不要合并此PR。,https://github.com/vllm-project/vllm/issues/5551
vllm,这个issue类型是一个优化建议，主要涉及持续集成（CI）构建过程中的资源利用问题。,https://github.com/vllm-project/vllm/issues/5549
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是文档。由于缺少具体内容，用户请求将ZhenFund添加为赞助商。,https://github.com/vllm-project/vllm/issues/5548
vllm,这个issue类型为功能需求，主要涉及的对象是`CompressedTensorsW8A8DynamicToken` scheme，用户请求对其添加通道级量化支持以支持模型。,https://github.com/vllm-project/vllm/issues/5542
vllm,这是用户提出的一个关于“Feature”的需求，主要涉及到vllm模型在处理asymmetric tensor parallel时出现的问题以及可能的改进。,https://github.com/vllm-project/vllm/issues/5541
vllm,这是一个功能建议类型的issue，主要涉及的对象是在Mixtral模型中添加LoRA支持。用户提出了增加对GPTQ和AWQ量化Mixtral模型支持的需求。,https://github.com/vllm-project/vllm/issues/5540
vllm,这是一个性能提升的建议，不涉及bug报告，主要涉及使用vllm.attention.ops.triton_flash_attention替换flash_attn package。用户提出此建议是因为他的GPU过旧，无法安装flash_attn package。,https://github.com/vllm-project/vllm/issues/5534
vllm,这是一个用户需求相关的issue，主要涉及vllm中`enablechunkedprefill`参数的使用问题。用户想了解在使用`enablechunkedprefill`时与`maxnumseqs`、`maxnumbatchedtokens`和`maxmodellen`参数之间的限制，并询问是否可以同时使用`enablechunkedprefill`和`enableprefixcaching`。,https://github.com/vllm-project/vllm/issues/5533
vllm,这是一个用户提出需求的类型，主要对象是关于文档管理的。由于缺少第四次会议的幻灯片，用户请求添加。,https://github.com/vllm-project/vllm/issues/5509
vllm,这是一个用户提出需求的问题，涉及使用vLLM运行RAG系统，用户想通过vLLM运行llm模型并获取答案，但无法定义上下文和问题的提示模板。,https://github.com/vllm-project/vllm/issues/5502
vllm,这个issue属于需求提案类型，主要涉及vLLM中加载和卸载多个LLM的API功能的添加。原因是为了提高资源利用率、扩展性和成本效益。,https://github.com/vllm-project/vllm/issues/5491
vllm,这个issue是一个用户提出需求的类型，主要对象是让vllm支持tensorrt编译之后的engine。原因可能是用户希望通过tensorrt加速但并发性没有vllm做得好。,https://github.com/vllm-project/vllm/issues/5489
vllm,这是一个用户需求类型的issue，主要涉及 VLLM 项目中的 latency benchmarking 功能，由于固定随机种子会导致输出结果随机性受到影响，用户提出启用随机种子选项的建议以增加配置灵活性。,https://github.com/vllm-project/vllm/issues/5488
vllm,这是一个功能需求类型的issue，主要涉及的对象是vLLM的前端代码。由于现有的实现方式无法支持使用非预编译的新中间尺寸的linear模块，用户提出了在注意力模块中仅支持使用linear模块的需求。,https://github.com/vllm-project/vllm/issues/5483
vllm,这是一个用户提出需求的issue，主要涉及的对象是 vllm 项目。,https://github.com/vllm-project/vllm/issues/5482
vllm,这是一个用户提出需求的issue，主要涉及vllm库中的模型推理过程中使用bitsandbytes功能的集成问题。用户询问如何将bitsandbytes功能与vllm库集成以运行一个混合模型qlora的推理过程。,https://github.com/vllm-project/vllm/issues/5480
vllm,这个issue类型是一个建议，提出了将dev requirements分为lint和test两部分的建议。原因是为了让安装lint requirements更容易。,https://github.com/vllm-project/vllm/issues/5474
vllm,这个issue是文档更新类型，涉及的主要对象是Tensorizer。原因是需要更新vLLM中关于Tensorizer的文档介绍和链接，以及引导用户查看示例脚本和使用指南。,https://github.com/vllm-project/vllm/issues/5471
vllm,这是一个用户提出需求的issue，涉及主要对象为上传wheel文件到Buildkite和S3 bucket。,https://github.com/vllm-project/vllm/issues/5469
vllm,这个issue类型是用户提出需求，该问题涉及的主要对象是vLLM的OpenAI兼容服务器。由于当前系统不支持用户定义的额外请求参数被记录在日志中，用户提出希望能够将自定义的额外参数传递给服务器以便记录在日志中。,https://github.com/vllm-project/vllm/issues/5467
vllm,这是一个功能改进的issue，主要涉及到代码库中的量化测试配置及CUDA计算能力检查，通过优化代码以避免重复的检查操作。,https://github.com/vllm-project/vllm/issues/5466
vllm,这是一个需求提出的issue，主要涉及CI/CD环境配置。由于需要新增AMD、Neuron和Intel测试，并将默认软失败关闭，可能是为了扩展CI测试范围和提高测试准确性。,https://github.com/vllm-project/vllm/issues/5464
vllm,这是一个用户提出需求的issue，主要涉及到PagedAttention模块的head维度限制问题，用户想了解如何实现head维度也可以是8的倍数。,https://github.com/vllm-project/vllm/issues/5459
vllm,这是一个用户提出需求的issue，主要对象是支持CPU推断的AVX2 ISA。由于目前只支持AVX512 CPUs，用户提出需要支持AVX2 CPUs以扩大vLLM的应用范围和吸引更多开发者的参与。,https://github.com/vllm-project/vllm/issues/5452
vllm,这个issue类型为用户提出需求，主要涉及的对象是vllm，在询问如何同时使用embedding model和LLM时提出了问题。,https://github.com/vllm-project/vllm/issues/5449
vllm,这个issue类型为硬件相关的功能改进，主要涉及生成自定义激活操作，由于需要避免重新跟踪导致的重新编译，可能出现了markdown渲染问题。,https://github.com/vllm-project/vllm/issues/5446
vllm,这是一个文档更新类型的issue，涉及更新调试文档的可读性，旨在改进调试提示的清晰度。由于当前的调试文档可读性较差，导致用户在调试过程中可能遇到困难，因此提出了这个问题以寻求改进。,https://github.com/vllm-project/vllm/issues/5438
vllm,这是一个需求类型的issue，主要涉及到LLaVA文档的更新。,https://github.com/vllm-project/vllm/issues/5437
vllm,这个issue是一个功能请求，主要涉及到在支持`24`稀疏度的`w4a16`模型中添加`CompressedTensors24`支持，由于需要支持稀疏量化并通过compressedtensors保存模型，导致需要调整相应的内核。,https://github.com/vllm-project/vllm/issues/5435
vllm,这是一个需求提出类型的issue，主要涉及的对象是vLLM模型库。用户在资源受限的环境中提出是否可以实现PagedAttention功能，由于模型参数无法容纳在CPU内存中，用户希望探讨如何将其拓展至更适合的环境。,https://github.com/vllm-project/vllm/issues/5434
vllm,该issue属于用户提出需求类型，主要涉及vllm中的sampling parameters，由于当前sampling parameters不支持guided grammar参数，导致用户无法在LLM Engine中使用guided grammar。,https://github.com/vllm-project/vllm/issues/5433
vllm,这是一个用户提出需求的issue，问题涉及到vLLM中对于RecurrentGemmaForCausalLM模型的支持问题，因为当前不支持该模型架构导致取值错误。,https://github.com/vllm-project/vllm/issues/5431
vllm,这个issue类型是用户提出需求，讨论vGPU在CI测试中的可行性和成本效益，以及测试软件支持范围的需求。,https://github.com/vllm-project/vllm/issues/5426
vllm,"此issue属于需求提出类型，主要对象为vLLM的前端界面。由于缺乏""Prefill Speed""（输入速率）信息导致用户反馈性能评估不够全面。",https://github.com/vllm-project/vllm/issues/5425
vllm,这是一个用户提出需求的类型，主要涉及到guided decoding & logit processor API的改进。由于API不完整且存在多个问题，导致了需要支持batch/async logit processing以及改善logit processing performance的问题。,https://github.com/vllm-project/vllm/issues/5423
vllm,这个issue是关于进行更新到ROCm 6.1、改进Dockerfile、修复测试的问题，主要涉及到硬件、AMD以及构建文档。由于升级后一些配置不正确或代码逻辑缺陷，导致了一些bug和持续集成构建相关的问题。,https://github.com/vllm-project/vllm/issues/5422
vllm,这是一个需求提出的issue，主要涉及到持续集成（CI）流程中对使用sccache来构建Docker镜像的需求。,https://github.com/vllm-project/vllm/issues/5419
vllm,这个issue是一个功能需求，涉及主要对象是针对vllm下的draft model和target model，用户提出了支持不同尺寸的张量并行度的需求。,https://github.com/vllm-project/vllm/issues/5414
vllm,这是一个用户提出需求/请教问题的issue，主要涉及MoE层在Jamba block中的疑惑。用户疑惑MoE层的具体定义及相关的数学或图表，希望能够指导或分享Jamba所采用的准确论文，并寻求潜在的替代/修复方法。,https://github.com/vllm-project/vllm/issues/5413
vllm,这个issue类型是用户提出需求，涉及的主要对象是添加调试提示。由于缺乏调试信息，用户提出了关于调试hang/crash的帮助请求。,https://github.com/vllm-project/vllm/issues/5409
vllm,这是一个需求类型的issue，涉及主要对象是 Worker 和 ModelRunner 类。由于目前的代码结构使得控制平面通信与模型执行代码混杂，导致难以替换控制平面机制，同时也难以提高多GPU设置的性能。,https://github.com/vllm-project/vllm/issues/5408
vllm,这是一个用户提出需求的issue， 主要对象涉及vllm模型的hidden states，由于提取hidden states的功能不支持，用户希望增加类似于transformers output_hidden_states的功能。,https://github.com/vllm-project/vllm/issues/5406
vllm,这是一个用户提出需求的issue，主要涉及到如何使用最新版本的预测解码。原因可能是用户需要了解如何使用预测解码功能或者文件中缺少相关文档。,https://github.com/vllm-project/vllm/issues/5400
vllm,这是一个性能优化的issue，主要涉及到使用共享内存广播Python对象的协议。,https://github.com/vllm-project/vllm/issues/5399
vllm,这是一个关于优化Kernel的issue，主要涉及对FP8 quantize kernel的改进，通过优化来提高速度。,https://github.com/vllm-project/vllm/issues/5396
vllm,这个issue是一个特性请求，主要涉及在`cutlass_scaled_mm_dq`内核中添加融合偏置加操作。原因是为了优化W8A8线性层与偏置的性能，并且目前仅支持fp8路径，而不支持int8路径。,https://github.com/vllm-project/vllm/issues/5390
vllm,这个issue属于文档更新类型，主要涉及FP8 W8A8量化特性的文档补充。,https://github.com/vllm-project/vllm/issues/5388
vllm,该issue属于功能增强类别，主要涉及ModelOpt FP8检查点的支持，由于需要将ModelOpt键的名称转换为vLLM在FP8量化模式下识别的键名称。,https://github.com/vllm-project/vllm/issues/5387
vllm,这是一个需求提出的Issue，主要涉及添加对`w4a16`模型的支持。,https://github.com/vllm-project/vllm/issues/5385
vllm,这个issue类型是版本更新，主要对象是项目的软件版本号。,https://github.com/vllm-project/vllm/issues/5384
vllm,这是一个新增功能需求的 issue，主要涉及到为 `compressed_tensors` 模型添加对 `w4a16` 支持。这个需求是由于需要支持 quantized `w4a16` 模型，并通过 compressedtensors 进行保存。,https://github.com/vllm-project/vllm/issues/5378
vllm,这个issue类型为功能需求，涉及主要对象是OpenVINO vLLM backend。由于OpenVINO提供更好的性能以及支持现代vLLM特性，用户提出需要在vLLM中集成OpenVINO backend以提高效率和功能性。,https://github.com/vllm-project/vllm/issues/5377
vllm,这是一个用户提出的需求类型的issue，主要涉及到同一节点检测功能的添加。这个需求是因为CI只能在同一节点运行，为了在多节点上测试正确性而提出。,https://github.com/vllm-project/vllm/issues/5369
vllm,这是一个提出需求的issue，主要涉及到代码改进。原因是为了使代码更清晰，并将其分离成一个独立的PR。,https://github.com/vllm-project/vllm/issues/5368
vllm,这是一个功能需求提交的issue，涉及的主要对象是支持在tensor parallel模式下处理attention heads的分配问题。由于用户需要将一个70B的模型运行在3个GPU上，但当前vLLM不支持这种情况，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/5367
vllm,这是一个特性请求 issue，主要涉及添加每晚基准测试，对比多个模型的性能。,https://github.com/vllm-project/vllm/issues/5362
vllm,"这是一个功能需求类型的issue，主要涉及到""broadcast_tensor_dict""的合并，原因是两处调用需要合并为一处调用。",https://github.com/vllm-project/vllm/issues/5354
vllm,这个issue类型是更新需求，涉及的主要对象是`compressed-tensors` quantization方法，由于新的配置文件结构，需要更新以符合新的配置文件结构。,https://github.com/vllm-project/vllm/issues/5350
vllm,这是一个功能需求的issue，主要涉及Speculative Decoding Worker的集成TypicalAcceptanceSampler，并通过配置参数来提高接受率。,https://github.com/vllm-project/vllm/issues/5348
vllm,该issue是一个需求提出，主要涉及MoE（Mixture of Experts）相关功能的重构。由于现有功能被嵌入到特定模型定义中，导致每个模型文件都需要重新实现大部分功能。,https://github.com/vllm-project/vllm/issues/5346
vllm,这是一个代码优化类型的issue，主要涉及到VLLM项目中的CPU后端。由于`cuda_utils.h`被错误地包含在CPU后端中，而未被使用，导致了需要进行移除的问题。,https://github.com/vllm-project/vllm/issues/5345
vllm,这个issue类型是用户需求报告，主要涉及对象是如何通过CLI启动时传递开关来减少vllm在终端输出的信息量。由于终端输出过多Info信息，用户希望找到方法将其减少。,https://github.com/vllm-project/vllm/issues/5338
vllm,这是一个用户提出需求的类型，并且主要涉及的对象是benchmark_throughput.py和benchmark_latency.py脚本。这个问题由于缺少选择分布式执行器的参数，导致用户需要手动修改代码以选择执行器，希望提供更方便的方式选择分布式执行器。,https://github.com/vllm-project/vllm/issues/5335
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM的`get_open_port`函数。由于限制了一次只能返回一个端口导致需要使用多个端口时出现问题。,https://github.com/vllm-project/vllm/issues/5333
vllm,这是一个优化建议类型的issue，主要涉及文档构建过程中使用不必要的重型CPU实例，提出使用较小的CPU实例以节约成本和空间。,https://github.com/vllm-project/vllm/issues/5331
vllm,这是一个用户提出需求的issue，主要涉及到vllm对于mistral v0.3的函数调用功能，用户想要运行mistral instruct 0.3的推理。,https://github.com/vllm-project/vllm/issues/5325
vllm,该issue类型为文档改进，涉及主要对象为vLLM文档。由于缺少一个关于自动前缀缓存（APC）的部分，用户提出了补充该内容的需求。,https://github.com/vllm-project/vllm/issues/5324
vllm,这是一个用户提出需求的issue，主要涉及支持新模型 mistralai/Codestral22Bv0.1，由于无法通过 OpenAI server API 加载该模型所致。,https://github.com/vllm-project/vllm/issues/5318
vllm,"这是一个用户提出需求的issue，主要涉及Docker镜像的版本问题，用户希望能够通过创建一个标记为""latest""的不稳定版本来解决。",https://github.com/vllm-project/vllm/issues/5315
vllm,该issue属于需求提出类型，主要涉及kv cache在vllm上的实现及并发推理的共享，提出了对性能优化的建议。,https://github.com/vllm-project/vllm/issues/5314
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm中的chat template功能，由于需求是如何在VLLM中实现选择聊天模板的功能，需要讨论如何实现。,https://github.com/vllm-project/vllm/issues/5309
vllm,这个issue属于用户提出需求类型，主要对象是vllm项目的kv cache模块。提问者想讨论是否可以通过添加新的GPU资源来增加kv缓存块的总数，而不涉及张量并行或其他并行方法。,https://github.com/vllm-project/vllm/issues/5308
vllm,这是一个与代码改进相关的issue，涉及的主要对象是LLaVA模型测试代码。,https://github.com/vllm-project/vllm/issues/5307
vllm,这是一个用户提出需求的issue，主要涉及的对象是对话生成模型GLM-4-9B-Chat，用户因暂无响应而询问为何无法支持这一模型。,https://github.com/vllm-project/vllm/issues/5306
vllm,这是一个技术需求类型的issue，涉及的主要对象是前端vLLM。由于需要优化批处理和吞吐量，希望能够通过一次性传递多个LoRA适配器到`generate()`来实现。,https://github.com/vllm-project/vllm/issues/5300
vllm,这个issue类型为文档更新请求，主要对象是 Ray Summit CFP。由于缺少具体内容，用户提出了需要添加 Ray Summit CFP 的请求。,https://github.com/vllm-project/vllm/issues/5295
vllm,该issue属于功能需求类型，主要涉及将协调员添加到tp和pp中以减少代码重复。该问题主要解决了分布式编程中的重复代码问题。,https://github.com/vllm-project/vllm/issues/5293
vllm,这个issue是一个特性请求，主要涉及的对象是硬件TPU集成。由于性能问题，暂时禁用了（Fast）topp抽样。,https://github.com/vllm-project/vllm/issues/5292
vllm,这个issue类型为优化建议，主要涉及主对象是模型的logits_scale参数。由于其值为1.0时的微小优化，提出了这个建议。,https://github.com/vllm-project/vllm/issues/5291
vllm,这是一个关于文档更新的issue，涉及的主要对象是新增赞助商Sequoia。,https://github.com/vllm-project/vllm/issues/5287
vllm,这个issue是一个特性请求，主要涉及动态图像大小支持，由于要充分利用LLaVANeXT的多分辨率功能，故需要插入动态数量的图像token。,https://github.com/vllm-project/vllm/issues/5279
vllm,这是一个功能需求类型的issue，涉及前端组件中OpenAI API服务器的修改，主要讨论了在生成对话提示时是否添加BOS特殊标记的问题。,https://github.com/vllm-project/vllm/issues/5278
vllm,该issue属于功能需求类型，涉及主要对象为VLM（Variable-length language model）。由于需要动态支持图片尺寸，用户提出了对VLM模型的输入处理进行更新的需求。,https://github.com/vllm-project/vllm/issues/5276
vllm,该issue是一个[Kernel]类型的更新，涉及到Cutlass int8 kernel configs for SM80，主要是添加6个int8 Cutlass Kernel配置来适应不同的Gemm形状。,https://github.com/vllm-project/vllm/issues/5275
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm模型加载过程。由于32GBCPU内存不足以加载qwen1.5 32b bf16模型，用户提出如何在较少的CPU内存下运行vllm的问题。,https://github.com/vllm-project/vllm/issues/5269
vllm,这是一个[特性改进]的issue，主要涉及benchmark_serving.py脚本的改进，通过添加ITL结果和调整TPOT结果来改善性能分析和统计数据的可用性。,https://github.com/vllm-project/vllm/issues/5263
vllm,这是一个用户提出需求的issue，主要对象是CI（持续集成），用户希望添加夜间基准测试。,https://github.com/vllm-project/vllm/issues/5260
vllm,这是一个需求类型的issue，涉及主要对象是transformers版本信息的收集。原因是缺乏transformers版本信息导致无法确定与具体模型支持相关的问题。,https://github.com/vllm-project/vllm/issues/5259
vllm,该issue类型为功能需求，主要对象是自定义层（custom layers）。导致该需求的原因是目前自定义层存在两个问题：1.对于TPU和Gaudi等设备不支持._custom_ops的直接导入；2.假设自定义操作在所有设备上以相同方式实现。,https://github.com/vllm-project/vllm/issues/5255
vllm,这是一个功能改进请求的issue，主要涉及到代码中的CUDA compute capability检查优化。原因是为了避免重复的代码和更好地管理测试配置。,https://github.com/vllm-project/vllm/issues/5253
vllm,这是一个功能增强提议，主要涉及`HfRunner`模型加载简化，用户不再需要手动注册新模型来加载HuggingFace模型，而是通过传递`is_embedding_model`或`is_vision_model`来指示如何加载模型。,https://github.com/vllm-project/vllm/issues/5251
vllm,这个issue类型为用户提出需求，涉及vLLM中支持Classifier-Free Guidance Logits processor的功能。该问题源于需要在vLLM中执行Model Forward Process与unconditional_ids相结合以计算logits。,https://github.com/vllm-project/vllm/issues/5245
vllm,这个issue是一个关于CI/Build优化的类型为[CI/Build]的PR，主要涉及vLLM的CPU CI执行时间。原因是通过将CPU核心绑定到本地内存节点，从而减少执行时间。,https://github.com/vllm-project/vllm/issues/5241
vllm,该issue类型为功能需求。主要涉及对象为前端开发，用户提出在VLLM中增加对OpenAI Vision API的支持。由于需要在VLLM中增加GPT4V兼容推理的支持，用户提交了此功能需求。,https://github.com/vllm-project/vllm/issues/5237
vllm,这个issue类型是功能需求提出，主要涉及的对象是为vLLM添加一个高效的接口来评估固定提示-完成对的概率。由于当前接口效率低下或不完全支持该使用案例，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/5234
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm-flash-attn，用户请求支持cu118版本，由于当前版本似乎不支持cu118，导致用户提出该问题。,https://github.com/vllm-project/vllm/issues/5232
vllm,这个issue属于需求提出类型，主要涉及的对象是VLLM项目中的分布式执行器。,https://github.com/vllm-project/vllm/issues/5230
vllm,这是一个用户提出需求的issue，主要涉及在vLLM中实现自定义注意力掩码的功能。由于用户想要从解码器 Causal LLM 到双向上下文 LLM 的转换，需要使用自定义的注意力掩码来实现该功能。,https://github.com/vllm-project/vllm/issues/5228
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是如何通过`LLM`对象启动推理服务。用户想要在项目中通过编程方式启动和停止vllm OpenAI兼容的推理服务器，以替代使用命令行操作。,https://github.com/vllm-project/vllm/issues/5227
vllm,该issue为文档更新类别，涉及vLLM的分布式推断和Serving功能，用户提出了关于更新文档以反映使用MultiprocessingGPUExecutor作为Ray的替代方案的需求。,https://github.com/vllm-project/vllm/issues/5221
vllm,这是一个关于提出需求的issue，主要涉及的对象是vllm提供的W4A8 quantization功能。由于新集成的QQQ quantization方法可能会减少一些推理性能，用户可能需要帮助来理解如何在其模型中使用该功能。,https://github.com/vllm-project/vllm/issues/5218
vllm,这个issue是一个功能增强请求，涉及的主要对象是CI/Build，由于开发者在之前的工作中忘记为`test_input.py`和`multimodal`目录添加新的CI步骤`inputstests`。,https://github.com/vllm-project/vllm/issues/5215
vllm,这个issue属于新功能需求，并涉及到处理模型输入时注册的相关对象，由于更改了`MultiModalRegistry`接口导致了症状性的问题。,https://github.com/vllm-project/vllm/issues/5214
vllm,这是一个用户提出需求的issue，主要涉及对模型功能的更新要求，包括增加对Mirostat、Dynamic Temperature和Quadratic Sampling的支持。,https://github.com/vllm-project/vllm/issues/5209
vllm,这个issue是关于功能需求的，主要涉及到HuggingFace的配置文件问题，用户提出希望能在vLLM CLI中提供覆盖配置文件的选项以解决加载模型时可能出现的问题。,https://github.com/vllm-project/vllm/issues/5205
vllm,这是一个特性需求的issue，主要涉及的对象是vllm的Speculative Decoding功能。由于draft模型和target模型的vocab大小不一致，导致推断流程无法运行，提出是否可以实现不一致的vocab大小支持。,https://github.com/vllm-project/vllm/issues/5203
vllm,这是一个用户提出需求的issue，主要涉及VLLM模型不支持量化版本，用户希望添加支持以提高成本效益。,https://github.com/vllm-project/vllm/issues/5202
vllm,"该issue类型属于功能需求，主要涉及到vLLM的新特性""Speculative edits""。该需求由于现有框架对前缀的不可变性做出了假设，导致用户提出了对于新的speculative方法的需求。",https://github.com/vllm-project/vllm/issues/5201
vllm,该issue类型为功能需求提议，主要涉及RoPE theta的定制化，原因是改变RoPE theta有助于提高规模模型的性能。,https://github.com/vllm-project/vllm/issues/5197
vllm,这是一个用户提出需求的issue，主要涉及的对象是在k8s HPA中使用gpu_cache_usage_perc作为自定义指标。由于gpu_cache_usage_perc是一个小数值（0到1之间），而不是计数器，导致用户无法找到实现HPA策略的方法。,https://github.com/vllm-project/vllm/issues/5195
vllm,这个issue属于用户提出需求类型，主要涉及到在部署llama370b模型时遇到的内存限制以及希望了解启用和禁用lora时CUDA图加速的内存消耗差异。,https://github.com/vllm-project/vllm/issues/5193
vllm,这个issue是关于支持加载GGUF模型的功能需求。,https://github.com/vllm-project/vllm/issues/5191
vllm,这是一个用户提出需求的类型issue，主要对象是添加PaliGemma Google的CuttingEdge Open Vision Language Model。,https://github.com/vllm-project/vllm/issues/5189
vllm,这是一个用户提出需求的类型的issue，该问题单涉及的主要对象是支持BERT模型。,https://github.com/vllm-project/vllm/issues/5179
vllm,这是一个关于VLLM的功能需求类型的issue，主要涉及到prefix caching功能在VLLM服务器中的表现问题，包括跨多个请求时缓存无法持续、询问是否可以实现LFU替代LRU方式、以及想要在服务器启动时配置KV缓存的前缀列表并防止其在服务器停止前被转储的问题。,https://github.com/vllm-project/vllm/issues/5176
vllm,这个issue是一个文档修复请求，主要涉及vLLM项目的PR提交检查列表在Markdown渲染上的显示问题。,https://github.com/vllm-project/vllm/issues/5175
vllm,"这个issue属于文档更新类型，涉及主要对象为""vllm""项目的GPTBigCodeForCausalLM LoRA支持，问题是由于更新导致文档遗漏了相关信息。",https://github.com/vllm-project/vllm/issues/5171
vllm,这是一个用户提出需求的issue，主要涉及vLLM项目的性能优化方面。由于OctoAI展示了如何通过一些优化技术提高速度，用户想要知道如何达到相同的性能水平。,https://github.com/vllm-project/vllm/issues/5167
vllm,这个issue类型是改进建议，主要涉及的对象是vLLM中的GPU内存利用参数。由于GPU内存利用参数应用方式不同导致vLLM未充分利用GPU资源的结果。,https://github.com/vllm-project/vllm/issues/5158
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM模型的函数调用支持。由于VLLM在推理速度上表现出色，但无法找到如何使用函数调用，用户提出了关于VLLM支持函数调用的需求。,https://github.com/vllm-project/vllm/issues/5156
vllm,该issue属于用户提出需求类型，主要涉及的对象是vLLM中的Linear adapter支持。由于当前vLLM仅支持特定层的LoRA适配器运行推断，用户希望扩展至线性层以实现推断训练支持一致性。,https://github.com/vllm-project/vllm/issues/5155
vllm,该issue属于优化和重构类型，主要涉及的对象是int8量化内核。原因是根据作者的描述，通过重构和优化内核，旨在提高性能并解决指令依赖链导致的问题。,https://github.com/vllm-project/vllm/issues/5146
vllm,该issue类型为Kernel更新问题，涉及到Cutlass fp8配置。由于Markdown格式无法正常呈现，所以使用了原始HTML。由此导致在PR标题和分类中囊括了不适用的分类。,https://github.com/vllm-project/vllm/issues/5144
vllm,这是一个用户提出需求的类型，主要涉及到数据并行化的问题。由于数据量大，用户希望在多节点上对模型进行离线推理，并使用数据并行化和张量并行化。,https://github.com/vllm-project/vllm/issues/5143
vllm,这是一个关于在 `ChatCompletionRequest` 类中添加对 `stream_options` 支持的特性请求。,https://github.com/vllm-project/vllm/issues/5135
vllm,这是关于用户需求的问题，涉及使用vllm与tensorrt-LLM的教程。,https://github.com/vllm-project/vllm/issues/5134
vllm,该issue属于特性需求类型，主要涉及实现在Speculative Decoding verifier中使用典型接受采样作为一种替代采样技术。原因可能是为了通过Medusa的典型接受来提高接受率。,https://github.com/vllm-project/vllm/issues/5131
vllm,这个issue类型是需求类型，主要对象是持续集成（CI）/构建（Build）系统。这个问题是由于文件大小限制导致提交构建出错。,https://github.com/vllm-project/vllm/issues/5130
vllm,该issue是一个用户提出需求的类型，主要涉及的对象是VLLM和PreTrainedModel对象。用户希望能够使VLLM支持加载PreTrainedModel对象，以加速其在MOE-LoRA架构中的应用。,https://github.com/vllm-project/vllm/issues/5128
vllm,这是一个用户提出需求的issue，主要对象是 Triton GPTQ 实现，由于实现的差异性导致性能问题。,https://github.com/vllm-project/vllm/issues/5127
vllm,这是一个用户提出需求的issue，主要涉及VLLM在抽取式问答方面的应用。由于用户不清楚如何在每个提示中仅从上下文中选择令牌，才提出了这个问题。,https://github.com/vllm-project/vllm/issues/5126
vllm,这个issue类型是文档更新，主要对象是API reference和LLM entrypoints的文档。,https://github.com/vllm-project/vllm/issues/5125
vllm,这是用户提出需求的类型，主要对象是关于支持LLaVANeXT-Video项目的新模型。由于目前Hugging Face尚未支持该模型，用户希望知道是否会有支持该项目的计划。,https://github.com/vllm-project/vllm/issues/5124
vllm,这是一个需求提出的issue，主要涉及要简化代码和修复类型注释，在合并后将启用`tests/`目录下的类型检查。,https://github.com/vllm-project/vllm/issues/5118
vllm,这是一个添加新功能的issue，主要涉及添加对`w4a16`模型的支持，包括更新`CompressedTensorsConfig`以及使用kerne等操作。,https://github.com/vllm-project/vllm/issues/5116
vllm,这是一个关于代码重构和功能扩展的 issue，主要涉及 CUTLASS kernels 中的 epilogue 定义，修复了与 CUDA Graphs 相关的一些问题，并且扩展了功能。,https://github.com/vllm-project/vllm/issues/5114
vllm,这是一个用户提出需求的issue，涉及到CI测试模板在AWS平台上的更新。,https://github.com/vllm-project/vllm/issues/5110
vllm,这是一个文档更新类型的issue，涉及主要对象为 vllmnccl 模块。由于移除了 vllmnccl 模块，需要更新一些旧注释和命令，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/5103
vllm,这是一个需求提出的issue，主要涉及到对ModelRunner进行更改以支持具有不同输入签名的模型。由于需要支持模型的不同输入签名，所以需要对ModelRunner进行相应的修改和优化。,https://github.com/vllm-project/vllm/issues/5101
vllm,这个issue是关于提升OpenAI服务器测试设置的优化建议，不是bug报告。此问题涉及主要对象是对vLLM测试环境的设置逻辑和测试文件的重新组织。,https://github.com/vllm-project/vllm/issues/5100
vllm,这是一个功能改进类型的issue，主要对象是LLM引擎的Sequence模块。由于引入CC导致需要传递`prompt`和`multi_modal_data`参数，无论是否实际使用，因此需要简化构建测试中的dummy Sequence，解决相关症状。,https://github.com/vllm-project/vllm/issues/5099
vllm,这是一个功能改进类型的 issue，主要涉及到 vLLM 库中的工具函数和版本获取。,https://github.com/vllm-project/vllm/issues/5098
vllm,这个issue类型是优化建议，主要涉及对象是vllm-nccl，并由于NCCL 2.19默认开启虚拟内存导致内存消耗过大，提示需要添加新环境变量解决问题。,https://github.com/vllm-project/vllm/issues/5091
vllm,这个issue是用户提出的一个需求，主要对象是添加vLLM CLI用于服务和查询OpenAI兼容服务器。由于PR描述部分需要选择适当的前缀以指示更改类型，但markdown渲染不起作用，因此需要使用原始html。,https://github.com/vllm-project/vllm/issues/5090
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是在文档中添加Dropbox作为赞助商。,https://github.com/vllm-project/vllm/issues/5089
vllm,"这是一个用户提出需求的类型，该问题单涉及的主要对象是""skywork moe""。由于缺乏具体内容，无法确定具体问题是关于bug还是其他，并且无法分析导致的原因和具体问题。",https://github.com/vllm-project/vllm/issues/5087
vllm,这是一个用户提出需求的issue，主要涉及在vLLM中采纳Colossal Inference 特性，希望实现性能提升，由于batched prefilling的性能问题导致速度不如预期。,https://github.com/vllm-project/vllm/issues/5085
vllm,这是一个特性请求（Feature Request）issue，主要涉及OpenAI Triton后端的硬件兼容性问题，用户探讨在不同平台上运行Triton代码的可能性。,https://github.com/vllm-project/vllm/issues/5083
vllm,这个issue类型为用户提出需求，涉及的主要对象是VLLM的量化选项。由于VLLM有多种不同的量化选项，用户在询问这些选项的区别，特别是对于Marlin选项的使用。,https://github.com/vllm-project/vllm/issues/5080
vllm,这是一个用户提出需求的issue，主要涉及vllm的离线批量推理以及LLama3-8b-Instruct模型使用Lora Adapter的情况。,https://github.com/vllm-project/vllm/issues/5078
vllm,这是一个用户提出需求的issue，主要对象是CUDA Graph，由于当前内存占用较高，用户建议增加输出缓冲区以减少内存占用。,https://github.com/vllm-project/vllm/issues/5074
vllm,该issue类型为功能需求提案，主要对象是为vllm添加性能基准测试的CI，由于需要对vllm的性能进行基准测试以及与其他替代方案进行比较，由此提出该需求。,https://github.com/vllm-project/vllm/issues/5073
vllm,这个issue类型是功能需求，主要涉及的对象是实现异构推理解码的功能。这个需求由于需要对CPU操作进行编译以及实现自定义操作的tensordriven分发，以实现异构推理解码。,https://github.com/vllm-project/vllm/issues/5065
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm中的guided decoding功能和正则表达式的使用。用户希望实现对指定key的初始字符串的强制限制，询问是否支持以及如何实现。,https://github.com/vllm-project/vllm/issues/5063
vllm,该issue为需求提出类型，主要涉及改进GPUExecutorAsync中的execute_model_async功能，旨在减少调度成本以提高小模型的性能。,https://github.com/vllm-project/vllm/issues/5055
vllm,这是一个功能需求类型的issue，主要涉及到前端开发。由于OpenAI只有一个分词器可供所有模型使用，对于其他用户来说很难确定要使用哪个分词器，因此这个问题提出了添加分词/去分词端点的需求，以解决在终端应用程序中对令牌计数或其他令牌使用的问题。,https://github.com/vllm-project/vllm/issues/5054
vllm,这是一个关于性能改进的issue，不是bug报告。主要涉及的是优化外轮廓功能。,https://github.com/vllm-project/vllm/issues/5053
vllm,这个issue属于功能需求类型，主要涉及的对象是添加一个名为`num_requests_preempted`的新指标。这个需求是为了解决现有指标无法提供关于GPU利用率的完整信息，以及提供高级调度器避免将新请求添加到低效GPU上的问题。,https://github.com/vllm-project/vllm/issues/5051
vllm,这是一个需求类型的issue，涉及到为offline LLM类应用chat模板，新加入了generate_chat()方法。,https://github.com/vllm-project/vllm/issues/5049
vllm,这是一个技术更新类型的issue，涉及到使用`TORCH_LIBRARY`替代`PYBIND11_MODULE`来定义自定义PyTorch运算符。由于需要支持`torch.compile`在vllm模型上的使用，以及通过使用Python的稳定API来构建和链接，因此进行了相关修改和更新。,https://github.com/vllm-project/vllm/issues/5047
vllm,这个issue是关于版本更新，属于其他类型，主要涉及的对象是代码库的版本更新。,https://github.com/vllm-project/vllm/issues/5046
vllm,该issue类型是用户提出需求，涉及主要对象为日志记录系统。由于用户认为调试信息只能通过配置文件来开启不便（他从未编写过配置文件），因此提出添加一个环境变量来更改用户可见的默认日志记录级别。,https://github.com/vllm-project/vllm/issues/5045
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM服务器在Kubernetes中实现更好的自动扩展和负载均衡，由于现有的metrics缺少一些关键指标，导致无法有效支持自动扩展vLLM服务器或在多个vLLM服务器之间更有效地分配负载。,https://github.com/vllm-project/vllm/issues/5041
vllm,这个issue类型为用户提出需求，涉及的主要对象是CI（持续集成）流程。由于该issue仅包含“draft”一词，推测用户可能想要草拟或起草一些内容，可能需要相关支持或指导。,https://github.com/vllm-project/vllm/issues/5040
vllm,这是一个功能增强的issue，主要涉及的对象是vllm库中的动态逐标记激活量化功能。这个issue的提出是为了支持w8a8模型的动态逐标记激活量化，通过向CompressedTensorsScheme中添加CompressedTensorsW8A8DynamicToken方案来实现。,https://github.com/vllm-project/vllm/issues/5037
vllm,这是一个功能需求的issue，涉及到vLLM社区中的punica kernel以及与LoRA相关的问题，由于punica kernel无法支持特定维度导致问题。,https://github.com/vllm-project/vllm/issues/5036
vllm,这是一个用户提出的需求问题，主要涉及代码构建过程，其原因是为了加快构建速度。,https://github.com/vllm-project/vllm/issues/5034
vllm,这个issue类型是潜在的功能增强请求，主要涉及SGMV Triton Kernels对多个Lora进行计算，旨在提高速度和处理能力。,https://github.com/vllm-project/vllm/issues/5025
vllm,这是一个与代码质量提升相关的需求，该问题主要涉及Core代码中的Helpers for PP功能，可能是为了提升代码的可维护性或性能方面的需求。,https://github.com/vllm-project/vllm/issues/5021
vllm,这个issue类型是需求提出，主要涉及对象是LoRA支持的模型。由于没有定义LoRA支持模型的统一接口，需要添加一个基类来实现统一接口。,https://github.com/vllm-project/vllm/issues/5018
vllm,这是一个用户提出需求的issue，主要涉及到mypy类型检查在tests/目录下的应用问题。原因可能是为了提高代码质量和可靠性。,https://github.com/vllm-project/vllm/issues/5017
vllm,这是一个功能需求类型的 issue，涉及到 vLLM 中的 speculative decoding 和 chunked prefill 的结合，旨在平衡延迟和吞吐量优化。,https://github.com/vllm-project/vllm/issues/5016
vllm,这是一个功能需求类型的issue，主要涉及增加vLLM的接受率，因为当前的拒绝采样机制过于严格，可能会导致拒绝掉一些合理的潜在预测token。,https://github.com/vllm-project/vllm/issues/5015
vllm,这个issue类型为用户提出需求，主要对象是文档。用户希望在vllm的文档中加入有关如何使用`ccache`的指导。,https://github.com/vllm-project/vllm/issues/5012
vllm,这是一个需求提出类型的 issue，主要对象是添加新模型 tiiuae/falcon-11B 到 vllm 模型支持列表。由于新模型的架构细节和功能上的差异，用户希望支持这个新模型以提供更多多语言处理的潜力。,https://github.com/vllm-project/vllm/issues/5010
vllm,这是一个改进代码组织的issue，主要涉及代码中的工具和测试代码。,https://github.com/vllm-project/vllm/issues/5004
vllm,该issue属于功能需求，主要涉及的对象是vLLM中的Tensor Parallelism功能。由于attention heads数量无法被3整除而导致无法运行模型，用户提出需要支持不整除的张量分布以解决这一问题。,https://github.com/vllm-project/vllm/issues/5003
vllm,该issue属于用户提出需求类型，主要对象是请求支持InternLM2ForCausalLM lora loading，可能由于缺乏该功能导致用户无法加载相关lora文件。,https://github.com/vllm-project/vllm/issues/5002
vllm,这个issue是关于性能改进的提议，主要涉及到使用具有不同vRAM的多个GPU来分割模型的问题。由于不同GPU具有不同的vRAM，加载大型模型时可能导致OOM错误。,https://github.com/vllm-project/vllm/issues/4998
vllm,该issue是一个特性需求，主要涉及的对象是lora与chunked prefill的集成。由于lora index逻辑中未考虑到不需要采样的情况，导致了lora无法与chunked prefill正常工作。,https://github.com/vllm-project/vllm/issues/4995
vllm,该issue类型为功能需求，主要对象是支持多个PP组，以帮助支持CUDAGraph。,https://github.com/vllm-project/vllm/issues/4988
vllm,这是一个性能优化的issue，主要涉及Marlin 24的预填充功能，由于旧版本的代码导致内存和计算之间的重叠不足，降低了计算性能。,https://github.com/vllm-project/vllm/issues/4983
vllm,这是一个用户提出需求的issue，主要涉及的对象是新增模型DeepSeek VL。用户想要支持这个模型，询问了当前支持模型Llava的难度。,https://github.com/vllm-project/vllm/issues/4982
vllm,这是一个用户需求问题，主要涉及vLLM在特定GPU上启动的方法。由于无法成功启动第二个vLLM实例，用户询问如何在空闲GPU上启动。,https://github.com/vllm-project/vllm/issues/4981
vllm,这个issue类型是功能需求提出，涉及的主要对象是vLLM的Medusa实现。用户提出了关于vLLM是否支持Medusa头部以及对支持新模型的需求。,https://github.com/vllm-project/vllm/issues/4978
vllm,这是一个功能需求的issue，主要对象是关于logit processor和prompt token ids的使用。这个需求是为了增强logit processor的功能，使其能够根据不同的参数来处理prompt tokens ids和model-generated tokens。,https://github.com/vllm-project/vllm/issues/4976
vllm,这个issue是用户提出需求，主要对象是关于输出打印信息的时间统计优化。由于时间统计信息不清晰，用户无法计算第一个token时间和下一个token时间，需要修改时间单位以提高可读性。,https://github.com/vllm-project/vllm/issues/4968
vllm,这是一个功能需求提案，主要对象是关于支持`stream_options`选项的功能。,https://github.com/vllm-project/vllm/issues/4967
vllm,这是一个用户需求类型的issue，主要涉及到vllm的推理速度较慢。用户询问如何优化gemm7B在vllm中的推理速度，并寻求帮助。,https://github.com/vllm-project/vllm/issues/4964
vllm,这是一个关于改进代码逻辑的Issue，主要涉及改进用户偏好设置和修复FlashInfer的问题。原因是当前逻辑在NVIDIA GPU上存在一些问题，导致用户无法正确设置偏好选项。,https://github.com/vllm-project/vllm/issues/4960
vllm,这个issue类型为用户提出需求，主要涉及的对象是microsoft/Phi-3-vision-128k-instruct模型。由于vllm在视觉支持方面落后，用户询问关于microsoft/Phi-3-vision-128k-instruct模型支持的情况。,https://github.com/vllm-project/vllm/issues/4958
vllm,这是一个功能需求相关的issue，主要对象是vLLM序列化模型和Tensorizer，用户提出了需要支持加载分片vLLM序列化模型至多个GPU的需求，由于目前的限制导致无法实现此功能。,https://github.com/vllm-project/vllm/issues/4957
vllm,这是一个用户提出需求的issue，主要涉及分布式推理后端的自动选择。由于当前针对单GPU和多节点推理的选择不够灵活，用户希望增加一个自动选择后端的功能以提升性能。,https://github.com/vllm-project/vllm/issues/4955
vllm,这个issue类型是用户提出需求，主要对象是软件在构建速度上的优化。由于频繁合并的pull requests导致本地构建耗时长，用户提出希望能够获取预发布版本或夜间构建版本以加快构建过程。,https://github.com/vllm-project/vllm/issues/4949
vllm,这个issue属于功能需求提升，主要涉及MLPSpeculator的引入和实现。,https://github.com/vllm-project/vllm/issues/4947
vllm,该issue属于提出需求的类型，主要涉及的对象是针对vLLM的ModelRunner子类，通过支持跨注意力和编码器序列来进一步支持编码器/解码器模型，由于需要对vLLM核心进行更改以适应交叉注意力机制来支持编码器/解码器模型的推理过程。,https://github.com/vllm-project/vllm/issues/4942
vllm,这个issue是针对添加idefics2模型的需求，属于[Feature]类型。,https://github.com/vllm-project/vllm/issues/4937
vllm,该issue类型为文档需求，涉及前端vLLM中添加提示标记id到logit处理器的问题。出现这个问题是因为markdown渲染不起作用，所以使用了原始的html标记。,https://github.com/vllm-project/vllm/issues/4932
vllm,这是一个性能优化类的issue，主要涉及到的对象是FP8模块，由于删除了一个额外调用的核函数，导致了吞吐量提升了约2%。,https://github.com/vllm-project/vllm/issues/4931
vllm,这个issue属于用户提出需求，主要涉及的对象是Qwen2模型，由于缺少rope_scaling支持导致无法正确输出结果。,https://github.com/vllm-project/vllm/issues/4930
vllm,这是一个用户提出需求的issue，涉及主要对象是vllm的logit processor。用户提出希望能够在vllm的logit processor中获得prompt和生成token的token ID信息。,https://github.com/vllm-project/vllm/issues/4928
vllm,这个issue类型为增加功能请求，主要对象是给vLLM项目添加感谢赞助商的部分。,https://github.com/vllm-project/vllm/issues/4925
vllm,"这个issue是关于更新""setup.py""文件，属于文档修复类型，涉及到PR描述填写问题。由于markdown渲染失败，导致需要使用原始html代码。",https://github.com/vllm-project/vllm/issues/4924
vllm,这是一个功能增强类型的issue，主要涉及MoE（Mixture of Experts）的调优和基准测试脚本改进。,https://github.com/vllm-project/vllm/issues/4921
vllm,这个issue属于文档需求类型，主要涉及benchmarking脚本和TGI docker的环境变量设置问题，用户提出需要添加文档说明和环境变量HF_TOKEN的设置。,https://github.com/vllm-project/vllm/issues/4920
vllm,这是一个功能需求类问题，主要涉及到Marlin警告提示问题。由于Marlin在较小的GPU上表现良好，因此需要移除关于m_block尺寸减小的警告。,https://github.com/vllm-project/vllm/issues/4918
vllm,该issue属于用户提出需求类型，主要对象是vllm和其支持的模型。由于用户想要讨论是否vllm能够支持推断出一个支持800k长内容的70B模型，表明用户希望了解vllm对长内容推断的支持情况。,https://github.com/vllm-project/vllm/issues/4909
vllm,该问题类型是需求提出，主要对象是支持Falcon-11B模型，用户提出需要增加Falcon-11B模型的支持，包括在fp16下的工作推断。,https://github.com/vllm-project/vllm/issues/4902
vllm,这是一个用户需求，针对VLLM性能测试中需要分别对Prefill和Decode两个阶段进行独立性能测量的问题。,https://github.com/vllm-project/vllm/issues/4900
vllm,这是一个用户提出需求的issue，主要涉及到vllm在离线推断中通过轮廓强制生成json，并寻求相关文档和示例的帮助。,https://github.com/vllm-project/vllm/issues/4899
vllm,这是一个用户提出需求的issue，主要涉及测试用例的更新和添加新模型，可能因前期测试内容不足或模型升级而引起。,https://github.com/vllm-project/vllm/issues/4898
vllm,这是一个用户提出需求的类型。该问题单涉及的主要对象为vllm serving engine。原因是用户需要在vllm serving engine中添加类似OpenAI风格的批处理API。,https://github.com/vllm-project/vllm/issues/4896
vllm,这是一个技术改进的PR（Pull Request），旨在优化并简化并行工作任务调度的逻辑。,https://github.com/vllm-project/vllm/issues/4894
vllm,这是一个需求类型的issue，主要涉及FP8 kv-cache scaling factors的加载，由于目前无法在相应的模型上测试，需要更新指定模型以包含kvcache scaling factors。,https://github.com/vllm-project/vllm/issues/4893
vllm,这是一个用户提出需求的类型，主要对象是vllm项目的发布计划，用户想了解下一个版本计划发布的日期。,https://github.com/vllm-project/vllm/issues/4892
vllm,这是一个关于改进`ShardedStateLoader`功能的PR，属于[Core]类型，主要涉及到从HF下载checkpoint的功能修改。该改进是为了解决在本地文件系统中没有检查点时无法下载的问题。,https://github.com/vllm-project/vllm/issues/4889
vllm,该issue是关于功能需求的，主要涉及到vLLM中支持encoder/decoder模型的工作。这个问题的根本原因是要修改xFormers后端以支持交叉注意力操作。,https://github.com/vllm-project/vllm/issues/4888
vllm,这个issue是一个新模型添加请求，主要涉及到Phi-2 LoRA支持。原因是该功能尚未添加，用户想要为vLLM新增Phi2 LoRA支持。,https://github.com/vllm-project/vllm/issues/4886
vllm,这个issue属于用户反馈需求类型问题，主要涉及vllm中的gpu使用设置，由于可能存在设置问题导致无法正确配置gpu个数。,https://github.com/vllm-project/vllm/issues/4882
vllm,这是一个用户提出需求的issue，主要涉及vLLM的pip包安装方法更新问题。用户询问关于CPU后端的pip安装方式是否会更新，并表达了想要通过`pip install vllm=0.4.2+cpu`安装的意愿。,https://github.com/vllm-project/vllm/issues/4881
vllm,这个issue是针对性能优化的PR，主要对象是vLLM模型的测试运行，由于模型测试运行时间过长而被中断，需要通过共享HuggingFace缓存来减少运行时间。,https://github.com/vllm-project/vllm/issues/4874
vllm,这个issue是一个需求提出类型的问题，涉及主要对象是为vLLM添加控制面板支持，由于Fastchat目前不支持最新的vLLM特性，导致了这个问题的提出。,https://github.com/vllm-project/vllm/issues/4873
vllm,这个issue属于文档更新类型，主要涉及到更新Ray Data分布式离线推断示例。这个issue是由于需要使用新的`ray_remote_args_fn` API来实现张量并行性，以进行批量推断操作，同时也涉及到Markdown渲染问题的原因所导致的。,https://github.com/vllm-project/vllm/issues/4871
vllm,这是一个关于优化vllm设置模式并使安装模式成为默认的issue，旨在提高最终用户友好性。,https://github.com/vllm-project/vllm/issues/4870
vllm,这个issue类型是功能需求，该问题涉及的主要对象是CI/Build。由于缺乏健康检查功能，用户提出了添加健康检查以解决相关问题。,https://github.com/vllm-project/vllm/issues/4868
vllm,这个issue类型是功能需求报告，主要涉及的对象是vLLM的Dockerfile，用户提出了为了增加可靠性而添加健康检查的建议。,https://github.com/vllm-project/vllm/issues/4867
vllm,这是一个代码优化类的Issue，主要涉及到代码中的过期注释和`pynccl_backend`变量的使用问题。,https://github.com/vllm-project/vllm/issues/4866
vllm,这是一个用户提出需求的issue，主要涉及的对象是在环境中使用KubeRay进行多节点分布式推理的实现。该问题是关于如何在当前环境下实现多节点之间的RDMA。,https://github.com/vllm-project/vllm/issues/4865
vllm,这是一个关于性能优化和代码逻辑的问题，主要涉及vllm的flash-attn后端。原因是用户想了解`forward_decode`是否等待`forward_prefix`执行完毕后再运行，以及如何利用chunked-prefill实现更好的性能。,https://github.com/vllm-project/vllm/issues/4863
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加一个控制面板来管理多个vllm实例。由于vllm openai兼容服务器的快速发展，用户希望将fastchat中的控制面板功能移植到vllm中，以享受fastchat带来的最新特性。,https://github.com/vllm-project/vllm/issues/4861
vllm,这是一个关于代码优化的问题，用户询问为什么在打开前缀缓存后，在解码阶段优化PA内核时间成本，导致PA的时间消耗减少。,https://github.com/vllm-project/vllm/issues/4860
vllm,这个issue是用户（用户提出需求）提出希望添加`local_files_only`参数以在本地路径加载模型的功能。导致这个问题的原因是当前的代码需要连接到huggingface.co，用户希望能够避免这种远程连接。,https://github.com/vllm-project/vllm/issues/4859
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是将自行训练的模型适配到vllm上。原因是用户想要将他们训练的模型适配到vllm这一框架上。,https://github.com/vllm-project/vllm/issues/4854
vllm,该issue类型是用户提出需求，其中涉及的主要对象是如何确定在可接受时间段内支持多少并发请求，可能由于硬件环境信息的收集和系统配置导致用户希望了解如何利用演示API服务器来评估并发请求的支持能力。,https://github.com/vllm-project/vllm/issues/4853
vllm,这个issue是用户提出需求，主要对象是Kernel中的Qwen1.532B模型。问题由于Qwen1.532B模型需要支持LoRA，因此需要在Punica中增加额外的尺寸。,https://github.com/vllm-project/vllm/issues/4850
vllm,该issue是一个功能需求，主要涉及benchmark_latency和benchmark_throughput的JSON输出支持。由于原功能缺乏JSON输出功能，为了更方便解析和聚合benchmark数据，用户提出了添加JSON输出支持的请求。,https://github.com/vllm-project/vllm/issues/4848
vllm,这是一个用户提出需求的issue，主要涉及benchmark_throughput和benchmark_latency两个指标，需要增加写入JSON文件的功能。原因可能是为了方便对结果进行聚合分析，避免解析日志数据。,https://github.com/vllm-project/vllm/issues/4847
vllm,这是一个需求提出类型的issue，主要涉及到添加一个新的kernel来融合fused-moe gemm中的去量化过程，由于该PR在DeepSpeed项目中的另一个PR的依赖关系，需要更新。,https://github.com/vllm-project/vllm/issues/4841
vllm,这是一个需求特性提议，主要对象是Neuron docker image的构建和发布。由于当前的docker images不支持Neuron (Inferentia)，导致用户提出需要测试的、管理的Neuron docker image，并希望增加关于在容器中运行vLLm Neuron的文档的需求。,https://github.com/vllm-project/vllm/issues/4838
vllm,这个issue类型为用户提出需求，主要对象是新增模型Google's Paligemma family of models。由于尚未得到回复，用户正在询问vllm团队是否有困难支持他们想要的模型。,https://github.com/vllm-project/vllm/issues/4833
vllm,这是一个用户提出需求的类型，主要涉及VLLM模型在混合模式CPU/GPU上运行的问题，因为用户希望能够在GPU内存有限的情况下运行LLM模型。,https://github.com/vllm-project/vllm/issues/4832
vllm,该issue类型为硬件特定的变更需求，主要涉及到对Intel平台的LoRA适配器在CPU后端的支持。由于设备不满足`compute capacity >= 8.0`的要求，导致需要添加新的函数实现以支持`punica`内核的启动。,https://github.com/vllm-project/vllm/issues/4830
vllm,这是用户提出的需求类型的 issue，主要对象是支持在 Kubernetes 上使用 LWS 服务 vLLM，在多节点分布式推理中的部署。由于 LWS 社区有相关的示例，因此用户正在寻求关于如何使用 LWS 部署 vLLM 的帮助。,https://github.com/vllm-project/vllm/issues/4829
vllm,该issue类型为功能需求提出，主要涉及对象是 qwen2（例如 Qwen2Attention），请求支持 rope_scaling 功能，以便使用 yarn/ntk 特性。,https://github.com/vllm-project/vllm/issues/4824
vllm,这个issue是关于优化的需求，主要涉及到了graph mode函数的合并。由于在图形捕获过程中需要手动打开图形模式，在捕获外不需要，因此建议将这两个函数合并为一个。,https://github.com/vllm-project/vllm/issues/4818
vllm,这是一个用户提出需求的issue，主要涉及的对象是项目的README文件。由于添加第四次聚会的公告缺失，用户请求将其添加到README中。,https://github.com/vllm-project/vllm/issues/4817
vllm,这个issue属于功能需求，主要涉及的对象是marlin unit tests和benchmarking code，用户提出要将这些代码从magic_wand中移植到marlin中。,https://github.com/vllm-project/vllm/issues/4815
vllm,这是一个用户提出需求的issue，涉及主要对象是将已有模型转换为另一种格式，原因是当前模型无法被VLLM框架支持。,https://github.com/vllm-project/vllm/issues/4811
vllm,这是一个特性需求的issue，主要涉及vLLM中的speculative decoding功能。,https://github.com/vllm-project/vllm/issues/4808
vllm,这是一个性能优化的问题，主要涉及到Qwen 7b chat model在高并发环境下CPU利用率达到100%，而GPU利用率不高的情况。可能由于Python协程实现的调度和计算逻辑限制，导致CPU成为计算瓶颈，影响GPU的性能表现。,https://github.com/vllm-project/vllm/issues/4806
vllm,这是一个性能优化的问题，涉及到如何正确测试tensorrt-llm serving，由于TTFT性能严重不佳，提出升级性能的建议。,https://github.com/vllm-project/vllm/issues/4803
vllm,这是一个需求类型的issue，主要涉及的对象是文档`PoolingParams`。由于新的类`PoolingParams`被引入，需要添加与之相关的页面以完善文档信息。,https://github.com/vllm-project/vllm/issues/4800
vllm,这是一个需求提出的issue，主要涉及了Microsoft Phi3Small8K和Phi3Small128K模型以及blocksparse flash attention功能。,https://github.com/vllm-project/vllm/issues/4799
vllm,该issue属于文档更新类型，涉及添加以前的meetups到文档中。原因是文档渲染不工作，因此在此处使用原始HTML。,https://github.com/vllm-project/vllm/issues/4798
vllm,这是一个需求提出类型的issue，主要涉及扩展测试集包括Regression、Basic Correctness、Distributed、Engine 和 Llava Tests，目的是启用这些测试。,https://github.com/vllm-project/vllm/issues/4797
vllm,该issue类型为功能需求报告，主要涉及到在vllm中添加对GPTQ Marlin 2:4稀疏结构支持的功能更新。原因是为了能够运行2:4稀疏模型，并且目前已支持的配置包括group_size为128或1以及4位或8位。,https://github.com/vllm-project/vllm/issues/4790
vllm,这是一个需求类型的issue，主要涉及的对象是为gptq marlin kernel添加bfloat16支持。由于一些模型使用fp16推理时会发生溢出，因此需要为量化内核添加bfloat16支持。,https://github.com/vllm-project/vllm/issues/4788
vllm,该issue是关于对vLLM的支持长上下文LoRA的请求，需要增加批量化的rotary embedding kernel以提高多个长上下文长度LoRA的服务效率。,https://github.com/vllm-project/vllm/issues/4787
vllm,这是用户提出需求的类型的issue，主要涉及如何在使用VLLM时修改批处理大小。用户在使用VLLM时想要运行特定模型的推理，但不清楚如何集成。,https://github.com/vllm-project/vllm/issues/4783
vllm,这个issue属于功能需求，主要涉及bfloat16支持，由于某些模型在使用fp16推理时会溢出，需要为quantization kernel添加bfloat16支持。,https://github.com/vllm-project/vllm/issues/4781
vllm,这是一个用户提出需求的issue，主要对象是支持OpenAI Batch Chat Completions文件格式。,https://github.com/vllm-project/vllm/issues/4777
vllm,该issue是 feature 请求类型，主要涉及的对象是 vLLM 的支持 bitsandbytes 量化和 QLoRA。原因是为了支持 QLoRA 在 vLLM 中的应用，实现对基础模型进行 bitsandbytes 4位量化，并配合低秩但高精度的 LowRank 权重矩阵生成输出。,https://github.com/vllm-project/vllm/issues/4776
vllm,这是一个功能需求，涉及到SequenceController在SamplingParams中的引入，用于在模型前向计算过程中处理logit token masks等功能。,https://github.com/vllm-project/vllm/issues/4775
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM在Mac环境下的构建和安装。由于macOS环境下暂不支持运行vLLM，用户希望能够在MacOS上导入vLLM用于开发，需要构建一个平台无关的wheel以提升开发体验。,https://github.com/vllm-project/vllm/issues/4773
vllm,这是一个用户提出功能需求的issue，主要对象是为vllm提供CPU支持的Docker镜像，由于现有的Docker镜像只支持GPU而排除了部分用户，因此用户提出希望添加CPU支持的镜像来简化用户使用流程。,https://github.com/vllm-project/vllm/issues/4771
vllm,这是一个功能需求的issue，主要涉及vllm项目中的CI测试在NVLink启用的机器上进行测试。,https://github.com/vllm-project/vllm/issues/4770
vllm,这是一个功能需求提议，主要涉及Logits processor plugins，旨在提供支持自定义Logits处理器，以消除直接更改vLLM源代码的需要。,https://github.com/vllm-project/vllm/issues/4769
vllm,这个issue是关于提出需求，主要对象是对vllmnccl库的访问权限。由于路径限制，用户希望将libnccl.so文件迁移到conda环境的库目录中以便更便于在生产环境中使用。,https://github.com/vllm-project/vllm/issues/4767
vllm,这个issue是一个功能需求报告，主要涉及的对象是paged_attention_v1模型，用户希望该模型能够支持参数'attn_bias'。,https://github.com/vllm-project/vllm/issues/4766
vllm,这个issue属于用户提出需求类型，主要涉及QServe库支持W4A8KV4 Quantization(QoQ)，由于QServe系统引入了一系列创新，包括W4A8KV4 Quantization技术，以提高LLM serving的效率和吞吐量。,https://github.com/vllm-project/vllm/issues/4763
vllm,这是一个性能优化的issue，主要涉及的对象是实现快速广播功能的张量字典，并且因为减少广播次数从而提升了传输速度。,https://github.com/vllm-project/vllm/issues/4757
vllm,这是一个用户提出需求的issue，主要涉及vLLM在支持定制角色方面的问题，原因是升级到v0.4.2后发现定制角色不再被支持。,https://github.com/vllm-project/vllm/issues/4755
vllm,该issue属于功能需求类型，主要涉及到自定义allreduce功能的重构。原因是之前该功能仅绑定在world group上，通过此PR实现正确绑定到tp group，要求除去函数内的import并移除默认组None选项。,https://github.com/vllm-project/vllm/issues/4754
vllm,这个issue类型是功能增强，主要涉及Attention Selector的改进；原因是为了提供更多信息并移动kv_cache_dtype字段，为后续的解码过程使用flashattn做准备。,https://github.com/vllm-project/vllm/issues/4751
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM的使用方法。用户询问如何在特定任务上评估vLLM模型的logprobs，以及如何进行特定模型的推理。,https://github.com/vllm-project/vllm/issues/4747
vllm,这是一个关于需求改进的issue，主要涉及的对象是vLLM中关于模型量化的功能。这个问题是由于默认的量化设置限制用户无法轻松自定义量化设置，需要修改配置文件来实现。,https://github.com/vllm-project/vllm/issues/4743
vllm,这是一个功能需求提出的issue，主要涉及到vllm模型对Blip2模型的支持问题。由于Blip2模型具有不同的架构，目前vllm还不支持该模型。,https://github.com/vllm-project/vllm/issues/4739
vllm,这个issue类型是功能改进，主要涉及的对象是项目的构建和版本控制，用户想要通过使用setuptools-scm从git直接获取版本信息，以便正确构建Dockerfiles。,https://github.com/vllm-project/vllm/issues/4738
vllm,这是一个用户提出需求的issue，涉及主要对象为GPTQ/AWQ模型的量化优化问题，可能由于未完全优化导致速度比非量化模型慢。,https://github.com/vllm-project/vllm/issues/4359
vllm,这是一个功能需求的issue，主要涉及到添加用于基于多进程worker的工具的功能。这个需求可能是为了增加多进程支持以提高性能。,https://github.com/vllm-project/vllm/issues/4357
vllm,这个issue是一个需求提出，涉及对象是vllm的多GPU支持，由于未知原因需要强制启用eager模式。,https://github.com/vllm-project/vllm/issues/4354
vllm,该issue为改进建议类，主要涉及到代码逻辑的重构。,https://github.com/vllm-project/vllm/issues/4352
vllm,这是一个功能需求提出的issue，主要涉及的对象是`ExecutorBase`。这个需求的提出是为了解决`ExecutorBase`无法被垃圾回收的问题，以便引入`MultiprocessingGPUExecutor`。,https://github.com/vllm-project/vllm/issues/4349
vllm,这是一个功能需求添加的issue，主要涉及对象是`DistributedGPUExecutor` abstract class，原因是为了引入`sibling MultiprocessingGPUExecutor`做准备。,https://github.com/vllm-project/vllm/issues/4348
vllm,这个issue类型是用户提出需求，主要涉及对象是vllm的`max_model_len`和`max_position_embeddings`参数。由于用户误解了参数限制的作用，导致提出了关于预填序列长度限制问题和如何集成特定模型进行推理的疑问。,https://github.com/vllm-project/vllm/issues/4346
vllm,这是一个功能需求的issue，主要涉及的对象是在单节点多GPU环境下使用多进程执行器。这个issue由于需要引入`MultiProcGPUExecutor`作为tensor并行的替代方案而产生。,https://github.com/vllm-project/vllm/issues/4345
vllm,这是一个优化性能的issue，涉及主要对象是MoE kernel / Mixtral，用户提出了关于优化FP8支持的问题。,https://github.com/vllm-project/vllm/issues/4343
vllm,这是一个用户提出需求的 issue，主要涉及于将 vLLM 中的 `linear_method` 通用化为 `quant_method`，以实现各模块自定义量化逻辑。,https://github.com/vllm-project/vllm/issues/4342
vllm,这是一个用户提出需求的类型，主要涉及到Docker用户和NCCL库访问权限的问题，由于`/root/.config/vllm/nccl/cu12/libnccl.so.2.18.1`文件对其他用户不可访问，用户建议在文档中说明如何更改权限。,https://github.com/vllm-project/vllm/issues/4340
vllm,这是一个关于需求提议的issue，主要涉及vLLM中动态设置RoPE缩放参数的问题，用户在自己的代码中无法动态设置这些参数。,https://github.com/vllm-project/vllm/issues/4334
vllm,这是一个需求提议的issue，主要涉及于支持在 vllm 中加载序列化的 fp8 模型以及使用静态和动态权重和激活尺度。,https://github.com/vllm-project/vllm/issues/4332
vllm,这是一个功能需求问题，涉及的主要对象是logger功能。原因可能是用户希望在logger中增加额外的信息。,https://github.com/vllm-project/vllm/issues/4329
vllm,该issue是一个功能需求，涉及实现tree attention用于加速并行解码，主要对象是解码过程中的注意力机制。,https://github.com/vllm-project/vllm/issues/4325
vllm,这是一个提出需求的issue，主要对象是CI（持续集成），导致这个问题的原因是为了及早发现调整CC([Misc] Reduce supported Punica dtypes)的问题。,https://github.com/vllm-project/vllm/issues/4319
vllm,这是一个用户提出需求的issue，主要涉及到代码优化，避免重复造轮子。,https://github.com/vllm-project/vllm/issues/4318
vllm,这是一个用户提出需求的类型的 issue，该问题主要涉及 vllm 的 offline Engine 和 batched streaming inference 功能。由于当前的 AsyncEngine 不支持这个功能，用户希望能够实现对模型性能的离线批处理流式评估。,https://github.com/vllm-project/vllm/issues/4314
vllm,这是一个关于代码重构和功能改进的issue，主要涉及vLLM中采样器和支持提示日志概率的改动，希望解决chunked prefill情况下的问题。,https://github.com/vllm-project/vllm/issues/4309
vllm,这是一个功能需求提出的issue，涉及主要对象是vLLM中的Outlines模块，用户要求能够定义whitespace pattern。,https://github.com/vllm-project/vllm/issues/4305
vllm,这是一个用户提出需求的issue，主要涉及的对象是实现并支持批量并行解码。由于需要在vllm上提高推理效率，在高请求率情况下，使用草案模型/基于树的验证可能会带来额外的开销，影响服务延迟。,https://github.com/vllm-project/vllm/issues/4303
vllm,这是一个建议性质的issue，主要涉及使用库切换的建议。,https://github.com/vllm-project/vllm/issues/4302
vllm,这个issue类型是验证功能实现（speculative decoding）的正确性，主要对象是CUDA graphs的支持情况。,https://github.com/vllm-project/vllm/issues/4295
vllm,这是一个功能需求提议，目标对象是通过启用支持Pascal GPU来实现更好的性能。,https://github.com/vllm-project/vllm/issues/4290
vllm,这个issue是用户提出需求类型的，主要涉及的对象是在使用vllm-0.2.1.post1版本时遇到了无法安装最新版本vllm的问题，希望了解如何在旧版本中添加llama3或修改代码以解决。,https://github.com/vllm-project/vllm/issues/4288
vllm,这是一个关于文档更新的issue，主要涉及SkyPilot部署文档中的缩进问题和自动扩展功能，可能由于Markdown渲染问题导致指示错误。,https://github.com/vllm-project/vllm/issues/4283
vllm,这个issue是一个功能增强请求，主要对象是CI（持续集成）。原因是为了通过增加ccache来提高wheel构建作业的响应速度。,https://github.com/vllm-project/vllm/issues/4281
vllm,这是一个关于性能优化的issue，主要涉及到代码中的`get_max_num_running_seqs`函数，通过对其进行优化来提高性能。,https://github.com/vllm-project/vllm/issues/4280
vllm,该issue是一个用户提出需求的类型，涉及到Rework logger以便提供pythonic自定义日志配置的能力。原因是为了让用户能够以更符合Python规范的方式通过logging.config.dictConfig注入自定义日志配置，以扩展vLLM的功能。,https://github.com/vllm-project/vllm/issues/4273
vllm,这个issue是文档更新类型，主要涉及到修复`autodoc`指令的问题。由于使用了不正确的指令和选项以及未更新的导入路径，导致了类的文档未能正确显示和函数缺失。,https://github.com/vllm-project/vllm/issues/4272
vllm,这个issue类型是功能需求，主要涉及的对象是项目中的 standalone_api_server。,https://github.com/vllm-project/vllm/issues/4269
vllm,这是一个用户提出需求的issue，主要对象是AMD CI pipeline，旨在扩展在AMD硬件上运行的测试集。,https://github.com/vllm-project/vllm/issues/4267
vllm,该issue属于用户提出需求类型，主要涉及vllm中支持speculative model的功能。用户对vllm中关于speculation的支持提出了疑问，并希望了解如何使用此功能。,https://github.com/vllm-project/vllm/issues/4266
vllm,这是一个关于使用特定GPU的需求问题，用户在vllm中想要选择特定的GPU运行模型而遇到了困难。,https://github.com/vllm-project/vllm/issues/4265
vllm,这个issue属于用户提出需求，主要涉及的对象是AMD CI pipeline，用户希望在AMD硬件上运行更多的测试。,https://github.com/vllm-project/vllm/issues/4264
vllm,"该issue属于用户提出需求类型，需要在serving models api_server.py中添加""eos_token_id""参数终结符。由于新的LLama3模型使用不同的终结符，需要在API中明确定义这一终结符以解决角色解析不准确的问题。",https://github.com/vllm-project/vllm/issues/4260
vllm,"这个issue类型是需求提出，主要对象是关于""chunked prefill""的支持问题。由于缺乏文档支持，用户询问是否现在已支持这一功能。",https://github.com/vllm-project/vllm/issues/4255
vllm,这是一个用户提出需求的issue，主要涉及vllm的HTTP请求处理，用户想要在请求中包含SamplingParams参数。这可能导致当前用户无法通过aiohttp发送请求获取所需的信息。,https://github.com/vllm-project/vllm/issues/4254
vllm,这是一个用户提出需求的issue，主要涉及vllm无法支持diverse beam search功能，用户希望vllm能够实现这一功能。,https://github.com/vllm-project/vllm/issues/4250
vllm,这个issue类型是功能优化，主要涉及到nccl初始化过程中关于unique id的处理。,https://github.com/vllm-project/vllm/issues/4248
vllm,这是一个用户提出需求的类型，主要涉及的对象是FlashAttention后端。由于FlashAttention v2不支持Turing GPU，用户请求支持FlashAttention v1.0.9以减少vram的使用。,https://github.com/vllm-project/vllm/issues/4246
vllm,该issue类型为需求或者工作管理，主要涉及到Upstream sync操作。原因可能是需要将某个项目同步至上游仓库，但具体内容并未提供。,https://github.com/vllm-project/vllm/issues/4245
vllm,这个issue类型是用户提出需求，主要对象是如何在使用 llama-3 时应用特殊模板，由于直接使用提示语句导致异常响应，用户想要了解是否需要特殊模板以及如何支持所需的模型。,https://github.com/vllm-project/vllm/issues/4239
vllm,这个issue类型是用户提出需求，主要涉及的对象是添加ngram prompt查找解码功能。由于直接从提示获取草稿，因此不需要另一个模型或修改后的模型来获取提案，这是最便利的享受推测加速的方式。,https://github.com/vllm-project/vllm/issues/4237
vllm,这个issue类型是文档更新需求，涉及更新添加新模型页面的内容；这个问题由于代码变更导致文档需要更新注册模型部分的内容。,https://github.com/vllm-project/vllm/issues/4235
vllm,这个issue是关于功能需求的，主要涉及VLLM中的LORA适配器加载问题，用户想知道是否能在不为每个新训练的LORA重新启动基础模型的情况下根据需要加载LORA适配器。,https://github.com/vllm-project/vllm/issues/4234
vllm,这个issue类型是用户提出需求，主要对象是pytest。由于缺少importlib选项，pytest可能会继续使用本地路径进行模块导入，而不是使用已安装的路径。,https://github.com/vllm-project/vllm/issues/4231
vllm,这个issue是关于需求提出的，主要对象是beam search功能，用户希望添加温度参数以增强抽样过程的可选性。,https://github.com/vllm-project/vllm/issues/4230
vllm,该issue属于用户提出需求类型，主要对象是vllm下的集成模型控制器面板支持。由于vllm不断增加LLM支持特性，导致用户提出需要一个类似fastchat控制器功能的解决方案来与模型工作 looselycoupled，并动态加入和注册到控制器的服务器后端。,https://github.com/vllm-project/vllm/issues/4226
vllm,这个issue属于功能需求提出，主要涉及的对象是文档生成，由于当前文档中未包含嵌套/多文件示例，并提出将示例脚本改写为笔记本形式以实现更好的展示。,https://github.com/vllm-project/vllm/issues/4225
vllm,这个issue类型属于功能改进，涉及更新lmformatenforcer版本和在文档中添加解码库链接。这个改进主要是为了提高性能和用户理解参数含义。,https://github.com/vllm-project/vllm/issues/4222
vllm,这是一个用户提出需求的issue，主要对象是VLLM模型的部署情况。由于一些较大的模型（如grok）无法在单台机器上部署，用户提出了支持高效多节点部署的需求。,https://github.com/vllm-project/vllm/issues/4221
vllm,这是一个用户提出需求的issue，主要是针对vllm中beam search模式不足的问题。由于目前实现的beam search版本过于简化，导致生成文本输出的创造性受到束缚，用户希望能够在beam search中设置更多选项来提升生成输出的创造性。,https://github.com/vllm-project/vllm/issues/4215
vllm,"该issue类型为功能需求，主要涉及设置重置节点GPU功能，用于解决""HIP outofmemory""情况，保证测试之间独立性，避免前一作业影响当前作业性能。",https://github.com/vllm-project/vllm/issues/4213
vllm,这是一个性能优化建议，主要涉及到vllm项目中的bonus tokens的重新启用。,https://github.com/vllm-project/vllm/issues/4212
vllm,这是一个需求类型的Issue，主要涉及的对象是与nccl相关的包信息。由于项目转移到vllm管理的nccl，需要收集`nccl`相关的包信息。,https://github.com/vllm-project/vllm/issues/4211
vllm,这是一个类型为性能优化的github issue，主要涉及到前端的vLLM模块和`tensorizer`的更新，用户提出了自动检测vLLM-tensorized模型和更新`tensorizer`版本的需求。,https://github.com/vllm-project/vllm/issues/4208
vllm,这是一个用户需求报道，主要涉及 vllm 库在 Pypi 上版本发布的时间问题。由于用户在 VPN 受限的环境下无法通过克隆源码安装，只能通过 Pypi 安装，但目前版本不可用，用户询问新版本 v0.4.1 何时会发布到 Pypi。,https://github.com/vllm-project/vllm/issues/4206
vllm,这个issue是用户提出需求，寻求关于如何在AsyncLLMEngine中使用LoRARequest的帮助。,https://github.com/vllm-project/vllm/issues/4203
vllm,这是一个用户提出需求的issue，主要涉及支持GPT-4V的图像输入功能。由于需要对OpenAI Chat Completions API进行相应调整以支持该功能，导致该issue的提出。,https://github.com/vllm-project/vllm/issues/4200
vllm,这是一个功能增强的issue，主要涉及LLaVANeXT模型的初始支持，由于输入图像大小受固定配置限制，可能导致无法充分利用额外分辨率优势。,https://github.com/vllm-project/vllm/issues/4199
vllm,这是一个性能优化的issue，主要对象是eager mode的运行效率，由于某个函数导致主机时间延迟，影响了运行性能。,https://github.com/vllm-project/vllm/issues/4196
vllm,这个issue类型为需求提出，主要涉及vLLM项目中的多模态支持。根据内容，用户提出了对vLLM V1引擎重构中多模态模型性能优化的需求。,https://github.com/vllm-project/vllm/issues/4194
vllm,该issue类型为功能需求，主要对象为更新docker以支持llama3模型。此问题可能是由于llama3模型与vllm模型最接近，但vllm模型目前尚未提供支持导致的。,https://github.com/vllm-project/vllm/issues/4192
vllm,这个issue是一个功能需求，涉及的主要对象是EngineArgs和SchedulerConfig对象。原因是需要添加一个控制最大排队时间的参数，以避免超出最大排队长度导致错误或者返回错误503的情况。,https://github.com/vllm-project/vllm/issues/4190
vllm,这个issue是关于用户提出需求，请求添加vLLM openai server中的状态监控API。,https://github.com/vllm-project/vllm/issues/4185
vllm,这是一个优化代码的issue，主要涉及WorkerWrapper的改动。原因是为了修复类型错误、减少冗余和提高代码执行效率。,https://github.com/vllm-project/vllm/issues/4183
vllm,这个issue是关于文档添加多阶段dockerfile可视化的需求。,https://github.com/vllm-project/vllm/issues/4179
vllm,这是一个版本更新的issue，涉及的主要对象是软件的版本号。,https://github.com/vllm-project/vllm/issues/4177
vllm,这是一个关于软件版本更新的类型为维护性需求的issue，主要对象是transformers库和llama 3 titkoken tokenizer，由于需要支持转换后的tokenizer导致需要对tokenizers库进行小的更改。,https://github.com/vllm-project/vllm/issues/4176
vllm,这个issue类型是功能需求，主要对象涉及到vllm工具中的命令行功能。由于用户想要更加方便地使用命令行功能，提出了新增`vllm serve`来包装`vllm.entrypoints.openai.api_server`的需求。,https://github.com/vllm-project/vllm/issues/4167
vllm,该issue属于功能需求类型，涉及VLLM项目下的InternLM2模型的LoRA加载支持问题，由于修改后进行Lora加载后推理结果不正确，可能是由于修改的代码部分未正确处理数据导致。,https://github.com/vllm-project/vllm/issues/4160
vllm,这个issue属于用户提出需求类型，主要涉及到对于vllm中KV cache的直接访问需求，原因是用户想要进行一个涉及到在节点之间复制KV cache内容的实验。,https://github.com/vllm-project/vllm/issues/4156
vllm,这是一个需求类型的issue，主要涉及到vllm和outlines之间的版本依赖关系。由于版本更新导致的错误未能正常更新vllm以使用outlines的最新版本功能，用户在寻求解决此问题的方案。,https://github.com/vllm-project/vllm/issues/4153
vllm,这个issue类型是用户提出需求，主要对象是AMD ROCm 6.1支持，用户询问何时可以期待vLLM支持ROCm 6.1。,https://github.com/vllm-project/vllm/issues/4152
vllm,此issue属于功能开发类型，涉及到vllm引擎的端到端测试以验证基本正确性，旨在比较vllm OpenAI服务器输出的tokens与由`AutoModelForCausalLM.from_pretrained()`创建的HuggingFace模型生成的tokens，存在的问题是不同输出中`logprobs`的排列方式不同导致需要寻找解决方案。,https://github.com/vllm-project/vllm/issues/4148
vllm,这个issue属于用户提出需求类型，主要涉及使用多GPU运行34B模型，用户想要在A100 40G上进行特定模型的推理，但不清楚如何与vllm集成。,https://github.com/vllm-project/vllm/issues/4147
vllm,这是一个功能需求的issue，主要涉及对象是PrefixCachingBlockAllocator。因为该issue描述了为PrefixCachingBlockAllocator添加自动前缀缓存支持的功能，主要讨论了实现此功能的LRU缓存策略，而非在现有代码中存在的bug。,https://github.com/vllm-project/vllm/issues/4146
vllm,这是一个功能需求类型的issue，主要涉及的对象是在启用块管理器V2时启用前缀缓存。原因是为了通过APC功能在块管理器V2上维护内容哈希和物理块之间的映射，并改善数据块的分配和回收处理。,https://github.com/vllm-project/vllm/issues/4142
vllm,这个issue是关于用户需求的，主要对象是PhiForCausalLM模型，用户要求为其添加LoRA支持，但目前该模型并不支持LoRA。,https://github.com/vllm-project/vllm/issues/4141
vllm,这是一个用户提出需求的issue，主要涉及的对象是为Swallow-MS-7B LoRA添加punica dimension。由于该模型扩大了词汇量，为了支持LoRA，需要在punica中添加相应的大小。,https://github.com/vllm-project/vllm/issues/4134
vllm,这是一个需求类型的issue，主要涉及在离线接口中增加对指导解码的支持。由于markdown渲染不起作用，所以使用了原始的HTML。,https://github.com/vllm-project/vllm/issues/4130
vllm,这是一个关于代码修复和改进的 issue，主要涉及 AMD 硬件上的 xformer 清理和改进，解决了 CI 失败的问题。,https://github.com/vllm-project/vllm/issues/4129
vllm,这个issue是一个用户提出需求的类型，主要涉及对象为w2f功能。由于未提供具体内容，无法分析导致的症状或问题。,https://github.com/vllm-project/vllm/issues/4125
vllm,这个issue是一个用户提出的需求，主要涉及的对象是关于支持HuggingFaceM4/idefics2-8b作为视觉模型的功能添加。原因是该模型具有更好的基准表现，但至今尚未有相应的PR提交，用户在寻求对该功能的支持。,https://github.com/vllm-project/vllm/issues/4124
vllm,这是一个功能需求的issue，涉及vLLM中支持FP8计算的初始支持，由于需要在加载模型权重后计算权重的per-tensor scaling factor并相应地量化权重。,https://github.com/vllm-project/vllm/issues/4118
vllm,这是一个用户提出需求的issue，主要涉及到向vLLM添加Jamba支持。用户希望将Jamba模型整合到vLLM中，并实现请求ID的传递和缓存管理，以及在请求完成时清理缓存。,https://github.com/vllm-project/vllm/issues/4115
vllm,这个issue类型为功能增强，涉及主要对象为vllm代码库中的Ray CPU Executor功能。这个变更是为了增强vllm对分布式执行的支持，使其能够结合CPU和Ray进行执行。,https://github.com/vllm-project/vllm/issues/4110
vllm,这个issue是关于更新Outlines Integration功能从`FSM`到`Guide`的。,https://github.com/vllm-project/vllm/issues/4109
vllm,该issue属于用户提出需求类型，涉及到vllm的 `enable_prefix_caching` 标志是否对 prompts 和生成的 kv 缓存进行重用，由于可能存在未明确描述的缓存重用问题，用户寻求关于 RadixAttention 的具体细节和潜在修复方案的帮助。,https://github.com/vllm-project/vllm/issues/4104
vllm,这是一个用户提出需求的issue，主要涉及如何在非HuggingFace模型中使用vLLM，用户希望能够在不转换整个代码库为HuggingFace的情况下插入vLLM。,https://github.com/vllm-project/vllm/issues/4094
vllm,这个issue是文档改进类型的问题，主要涉及的对象是关于tensorizer使用的解释，可能是由于当前文档表达不清晰导致用户无法准确了解如何使用。,https://github.com/vllm-project/vllm/issues/4090
vllm,这是一个用户提出需求的类型issue，主要涉及到vllm是否支持FacebookAI/roberta-large模型的推理。缺乏响应可能是由于缺乏对该模型支持的明确说明或者文档。,https://github.com/vllm-project/vllm/issues/4086
vllm,这是一个用户需求类型的issue，主要涉及的对象是vllm模型。由于用户在使用模型钩子时只能捕获第一个token的信息而不是所有token的信息，所以提出了如何更好地在库中使用模型钩子的问题。,https://github.com/vllm-project/vllm/issues/4084
vllm,该issue是用户提出需求，希望为ChatGLM3和llama2模板添加chat模板来支持多轮对话，以解决在多轮对话中请求需要在流模式下添加eos令牌的问题。,https://github.com/vllm-project/vllm/issues/4082
vllm,这是一个用户提出需求的issue，涉及主要对象是vllm工具中的tools/tool_choice功能。用户提出这个问题是因为想要在使用open api端点进行推断时调用功能/tools功能，但目前无法得到关于如何实现的详细信息。,https://github.com/vllm-project/vllm/issues/4080
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是360Zhinao模型系列。由于需求新增支持360Zhinao模型，原因可能是为了将其集成到相应的系统或平台中。,https://github.com/vllm-project/vllm/issues/4078
vllm,这个issue类型是功能需求提议，主要涉及的对象是对Mixtral 8x22Bv0.1版本的支持。由于未明确记录支持该版本所导致的用户提出对应的功能需求。,https://github.com/vllm-project/vllm/issues/4073
vllm,这是一个关于功能需求的issue，主要涉及到LoRA适配器的更新和优化。,https://github.com/vllm-project/vllm/issues/4068
vllm,这是一个用户提出需求的类型，主要对象是对支持Mixtral-8x22B-v0.1的请求，由于缺乏该支持而导致用户提出了关于支持时间的问题。,https://github.com/vllm-project/vllm/issues/4064
vllm,这是一个特性需求，主要涉及到Punica和ChineseMixtral两个对象。这个需求是由于ChinesMixtral的词表大小为57000，需要将额外的57088大小的内容集成到Punica中。,https://github.com/vllm-project/vllm/issues/4063
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是添加 Radix_tree.py 作为数据结构来识别通用前缀。,https://github.com/vllm-project/vllm/issues/4060
vllm,该issue属于用户提出需求类型，主要对象是vllm在T4平台上无法支持Automatic Prefix Caching功能。由于T4平台的计算能力不符合要求，导致该功能无法被实现。,https://github.com/vllm-project/vllm/issues/4059
vllm,这是一个特性请求，涉及功能的问题，主要对象是sliding window attention，导致chunked prefill和prefix caching不能与sliding window attention配合使用。,https://github.com/vllm-project/vllm/issues/4057
vllm,该issue类型是功能需求，主要涉及对象是vLLM模型中的cuda graph功能。由于当前设置导致了当prefill和decodes一起批处理时cuda graph未启用，用户希望通过启用cuda graph来尝试提高性能。,https://github.com/vllm-project/vllm/issues/4056
vllm,这是一个需求提出的issue，主要对象是Baichuan-13B模型。由于Baichuan-13B的hidden_size不满足支持LoRA的需求，需要在Punica中加入额外的大小以支持该功能。,https://github.com/vllm-project/vllm/issues/4053
vllm,这是一个用户提需求，关于如何在vLLM上使用OpenAI客户端进行批量请求后者进行基准测试的问题。,https://github.com/vllm-project/vllm/issues/4052
vllm,这个issue是关于用户提出需求，询问本地测试部分代码修改的策略，主要涉及到VLLM代码库的局部修改的测试方法，用户想知道是否有避免每次都需要从源代码重新构建的方法来更快更有效地测试修改。,https://github.com/vllm-project/vllm/issues/4048
vllm,这是一个功能需求类型的issue，主要涉及到 `max_num_batched_tokens` 参数的计算问题，用户在设定该参数时缺乏指导，可能导致性能不佳。,https://github.com/vllm-project/vllm/issues/4044
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM项目中的logger.py文件。这个issue提出了重构logger以支持用户以Python风格提供自定义日志配置的需求。,https://github.com/vllm-project/vllm/issues/4038
vllm,这是一个用户提出需求的issue，主要涉及到如何在使用现有的Ray集群时初始化vLLM引擎，问题出现的原因是可能存在在vLLM调用`ray.init`之后再次尝试初始化Ray集群导致的错误提示。,https://github.com/vllm-project/vllm/issues/4034
vllm,这是一个关于需求的issue，涉及到对于VLLM模型添加bitsandbytes 4bit quantization的支持。原因可能是现有的实现对于特定用途有较大优势，但尚未被添加。,https://github.com/vllm-project/vllm/issues/4033
vllm,这个issue是一个功能需求，主要涉及的对象是支持Int8 dtype用于存储权重，目前使用FP16浪费50%的VRAM。原因是当使用Int8模型时，权重以FP16存储，导致消耗大量VRAM。,https://github.com/vllm-project/vllm/issues/4031
vllm,这个issue类型是用户提出需求，主要对象是在使用vllm时希望能够将两个大模型分布在四个GPU上，并且希望这两个模型能够在同一个进程中运行。由于vRAM不足导致无法正确配置两个LLM实例在不同GPU上运行。,https://github.com/vllm-project/vllm/issues/4029
vllm,该issue是功能改进类型，涉及的主要对象是代码文件中的`merge_async_iterators`函数。由于`merge_async_iterators`函数的功能不仅限于OpenAI server，为了改善代码组织，提议将其移动到常用工具中。,https://github.com/vllm-project/vllm/issues/4026
vllm,这个issue属于用户提出需求类型，主要涉及VLLM模型中支持4位KV Cache，需求源于希望减少在处理长文本时GPU内存使用量的问题。,https://github.com/vllm-project/vllm/issues/4025
vllm,这是一个用户提出需求的issue，主要涉及对象是vllm下的一个特定功能WorkerWrapper，问题是因为特定函数`set_cuda_visible_devices`过于具体且使用范围狭窄，同时初始化过程中的部分信息在初始化后丢失，导致难以获取这些信息。,https://github.com/vllm-project/vllm/issues/4024
vllm,这是一个功能需求报告，主要涉及模型和内核，由于缺少16和32 kernel sizes，导致部署SanjiWatsuki/TinyMixtral32x248M和BEEspokedata/smol_llama220M等小型模型时与vllm不兼容。,https://github.com/vllm-project/vllm/issues/4020
vllm,该issue类型为用户提出需求，请教问题，主要涉及对象为vLLM的安装程序。由于用户使用AMD GPU，希望vLLM能提供预编译版本以便在该硬件上使用。,https://github.com/vllm-project/vllm/issues/4017
vllm,这个issue是一个特性需求，主要涉及的对象是vLLM项目的CUDA核心代码，由于需要支持更大的词汇表，以便于在更多模型上进行lm head的修改。,https://github.com/vllm-project/vllm/issues/4015
vllm,"这是一个用户提出需求的issue，主要对象是vllm下的""Distributed""模块。由于当前的`init_distributed_environment`需要`world_size`和`rank`参数，用户希望将其变为可选参数，并设置默认的`distributed_init_method`为`env://`，以便与`torchrun`更方便地使用。",https://github.com/vllm-project/vllm/issues/4014
vllm,这是一个功能需求的 issue，主要涉及添加 LoRA 支持到 quantized 模型，并引入了对张量并行的支持。,https://github.com/vllm-project/vllm/issues/4012
vllm,这是一个用户提出需求的issue，主要涉及对象是vllm模型，问题是用户需要调整GPU计算能力以支持40k长度的提示信息计算，由于softmax和其他计算超出了单个GPU的容量，用户寻求如何调整解决这一问题。,https://github.com/vllm-project/vllm/issues/4005
vllm,这是一个功能增强类型的issue，主要涉及的对象是针对Mixtral 8x22在A10080G和H100设备上的fused MoE配置，旨在提高高批量大小下的性能。,https://github.com/vllm-project/vllm/issues/4002
vllm,这是一个用户提出需求的issue，主要涉及vLLM服务器无法处理异常终止请求导致的排队问题，用户希望找到一种方式清除队列并重置系统。,https://github.com/vllm-project/vllm/issues/4000
vllm,该issue类型为用户提出需求，主要涉及对象是添加chatglm6b模型支持，由于chatglm6b是一个非常受欢迎的模型，用户希望在项目中恢复其支持。,https://github.com/vllm-project/vllm/issues/3999
vllm,这是一个用户提出需求的issue，主要涉及到对VLLM添加对JetMoE模型的支持。,https://github.com/vllm-project/vllm/issues/3995
vllm,这个issue类型是关于代码改进的需求，主要涉及到改进vLLM的前端以支持CPU后端。原因可能是目前前端没有对CPU后端的支持，需要对代码进行相应修改。,https://github.com/vllm-project/vllm/issues/3993
vllm,这个issue类型是硬件相关的需求，主要对象是vLLM的CPU后端。导致这个需求的原因是要启用CPU后端的异步引擎代码路径。,https://github.com/vllm-project/vllm/issues/3992
vllm,这是一个关于升级PyTorch和Xformers版本的请求，涉及到Xformers依赖的PyTorch版本问题。这个issue的主要原因是Xformers需要使用至少PyTorch 2.1.2，但用户需要使用至少PyTorch 2.2.1，因此提出请求升级Xformers至v0.0.25.post1，以支持PyTorch 2.2.2。,https://github.com/vllm-project/vllm/issues/3988
vllm,这是一个功能请求的issue，主要涉及Xformers升级到`v0.0.25.post1`和PyTorch 2.2.2，但由于环境问题导致本地测试无法运行。,https://github.com/vllm-project/vllm/issues/3987
vllm,该issue类型是性能优化建议，主要涉及layernorm加速优化。由于每次`self`发生变化时重新编译，导致性能表现不佳，提出提取计算到独立函数以减少编译次数。,https://github.com/vllm-project/vllm/issues/3985
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是Mixtral8x22Bv0.1模型。用户提出了关于是否支持该模型运行的问题，由于编译时间过长导致了用户的疑问。,https://github.com/vllm-project/vllm/issues/3983
vllm,这是一个需求报告，该问题单涉及的主要对象是Int8 Activation Quantization。由于需要加速Prefill的计算速度，工程团队提出了一种增量式的量化方法，为了快速支持量化模型，他们打算首先替换线性和注意力模块，并在运算前动态量化激活函数。,https://github.com/vllm-project/vllm/issues/3975
vllm,该issue是关于提出需求的，主要涉及前端工具和RAG（Retrieval-Augmented Generation）。由于用户对工具调用参数、不必要的工具调用返回、文档参数的用途、以及未来功能的建议进行了讨论。,https://github.com/vllm-project/vllm/issues/3971
vllm,该issue类型为功能需求，主要涉及合并对计算能力 6.x 的支持。由于代码排除了某些GPU，尽管它们可以工作，用户提出了合并支持的代码以解决这个问题。,https://github.com/vllm-project/vllm/issues/3969
vllm,这个issue属于性能优化建议类型，主要涉及了项目中的up_proj部分。由于使用两个shrink和expand kernels计算adapter导致性能降低，提议合并它们以提高性能。,https://github.com/vllm-project/vllm/issues/3966
vllm,这个Issue是关于功能需求的，主要涉及实现Tree Attention功能。由于计划替换已有的kernel，提出了疑问如何实现新功能。,https://github.com/vllm-project/vllm/issues/3960
vllm,该issue为用户提出需求，并询问是否可以启用前缀缓存以提高在使用多个LoRa时的性能。,https://github.com/vllm-project/vllm/issues/3958
vllm,该issue是关于优化MoE实现性能的进一步改进。,https://github.com/vllm-project/vllm/issues/3954
vllm,"这个issue属于功能需求提出类型，主要涉及到""LoRA gptbigcode""模型的实现。",https://github.com/vllm-project/vllm/issues/3949
vllm,"这是一个需求提出类型的issue，主要对象涉及""magic wand""版本，由于缺乏具体规范导致需要制定测试计划来运行远程推送。",https://github.com/vllm-project/vllm/issues/3943
vllm,这是一个用户提出需求的issue，主要涉及vllm模型在使用过程中存在的问题，用户希望实现token级别的重用以提高效率。,https://github.com/vllm-project/vllm/issues/3942
vllm,这是一个用户提出需求的issue，主要涉及vllm支持序列并行的功能。原因可能是用户希望进行序列并行操作，但目前vllm尚不支持该功能。,https://github.com/vllm-project/vllm/issues/3940
vllm,这是一个用户提出需求的issue，主要涉及对象是LoRa的支持。由于当前代码不支持高于64的LoRa ranks，导致用户无法使用r=128和r=256，从而出现数值错误的bug。,https://github.com/vllm-project/vllm/issues/3934
vllm,这是一个关于需求的问题，涉及对象是如何使用VLLM加载模型。由于用户只有1个RTX4090（24G），希望VLLM能够将某些层迁移到CPU，某些层保留在GPU上。,https://github.com/vllm-project/vllm/issues/3931
vllm,这个issue类型为用户提出需求，涉及主要对象为VLLM。用户在询问VLLM的Page Attention V2是否包含了Flash Decoding的实现。,https://github.com/vllm-project/vllm/issues/3929
vllm,这个issue类型为用户提出需求，主要涉及对象为vllm的paged attention功能。由于用户不清楚如何在特定模型上打开paged attention功能，因此提出了如何使用该功能的问题。,https://github.com/vllm-project/vllm/issues/3927
vllm,该issue属于用户提出需求类型，主要对象是vllm下的openai entrypoint，由于现有的entrypoints只能在GPU上工作，用户希望添加对CPU的支持。,https://github.com/vllm-project/vllm/issues/3923
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm项目。用户询问关于“ROCm + Triton Backend”是否已经存在于代码中，表明用户想了解该功能在哪里可以找到。,https://github.com/vllm-project/vllm/issues/3921
vllm,这个issue属于功能需求提议类型，主要涉及到HPU利用时间的测量功能。由于用户需要对工作量片段进行时间测量，并添加相应的代码调用。,https://github.com/vllm-project/vllm/issues/3917
vllm,这个issue是一个需求反馈，主要涉及benchmark脚本的cpu选项添加。原因是在进行性能测试时需要指定cpu选项。,https://github.com/vllm-project/vllm/issues/3915
vllm,这是一个需求类型的issue，主要涉及的对象是distributed_init和worker。由于distributed_init当前与worker耦合，导致其只适用于gpu backend，需要refactor使其变得更通用。,https://github.com/vllm-project/vllm/issues/3904
vllm,这是一个用户提出需求的issue，主要涉及vLLM在多节点分布式推断上的支持。该问题由于部分团队可能缺乏Ray知识而提出希望在Kubernetes上提供简单的协同功能。,https://github.com/vllm-project/vllm/issues/3902
vllm,这是一个关于如何确定vllm引擎是否已满的问题。用户想知道如何检查引擎是否接受更多请求以避免排队，并询问是否可以获取引擎利用率的问题。,https://github.com/vllm-project/vllm/issues/3897
vllm,该issue类型为功能性开发，涉及对象为LLMEngine和speculative decoding组件的集成。由于尚未实现完整的e2e正确性实现，导致输出为垃圾。,https://github.com/vllm-project/vllm/issues/3894
vllm,这个issue是一个关于在vLLM项目中添加新的模型（minicpm和其变体）的类型为[Model]的请求。,https://github.com/vllm-project/vllm/issues/3893
vllm,这是一个优化需求，主要对象是代码中的一些低效部分，由于代码效率不高导致性能延迟较高。,https://github.com/vllm-project/vllm/issues/3890
vllm,这个issue类型为性能优化需求，主要涉及的对象是对Latency benchmark的稳定性要求。该问题由于之前使用平均延迟计算不稳定，提出了使用更多迭代次数、采用中值而非平均值来增加稳定性的解决方案。,https://github.com/vllm-project/vllm/issues/3889
vllm,"这个issue类型是需求用户提出需求，主要对象是关于如何最大化Typical GPUs（如A100，V100）的吞吐量表现。由于设置的参数（max_num_batched_tokens=10240, max_num_seqs=1024）不符合预期，导致GPU利用率仅为20%。",https://github.com/vllm-project/vllm/issues/3885
vllm,该问题是关于核心功能的一个issue，涉及到完全可工作的分块预填充终端到终端功能。,https://github.com/vllm-project/vllm/issues/3884
vllm,这个issue是一个用户提出需求的类型，主要涉及到缺乏llava 1.5在OpenAI兼容服务器上使用的文档和示例。用户询问是否有参考示例来使用llava通过OpenAI SDK，并提出了潜在的问题，但未收到回应。,https://github.com/vllm-project/vllm/issues/3873
vllm,"这个issue类型是用户需求。该问题单主要涉及的对象是""Core""。由于未明确说明如何添加三行代码以启用""out-of-tree model register""，用户提出了需要相关帮助的需求。",https://github.com/vllm-project/vllm/issues/3871
vllm,这个issue类型是用户提出需求，主要涉及的对象是实现基于当前聊天和聊天历史的AI对话功能，请求使用vllm和llama-2-13B模型帮助实现。,https://github.com/vllm-project/vllm/issues/3870
vllm,这是一个功能需求类型的issue，涉及主要对象为LM Format Enforcer Guided Decoding Support。由于用户simonmo提出需要添加LMFE解码支持到OpenAI服务器，故提出了此问题。,https://github.com/vllm-project/vllm/issues/3868
vllm,这是一个功能需求，用户提出了希望能够通过pytest标记来避免全局测试清理，以加快CPUonly测试的速度，即使在同一目录/文件中使用GPU的其他测试。,https://github.com/vllm-project/vllm/issues/3863
vllm,这个issue类型为需求提出，主要涉及的对象是vLLM的项目路线图。由于用户希望在Q2 2024看到vLLM的新特性和发展方向，提出了对性能优化、硬件覆盖、模型支持等方面的需求和期望。,https://github.com/vllm-project/vllm/issues/3861
vllm,这个issue类型是功能改进，涉及 Chunked Prefill 的调度优化。由于之前一个 PR 导致了一个回归问题，需要更新调度算法来支持 chunked prefill。,https://github.com/vllm-project/vllm/issues/3853
vllm,这个issue是关于建议性质的，主要对象是项目中的一个文件`vllm/entrypoints/api_server.py`。原因是用户在项目中的很多问题中使用了`m vllm.entrypoints.api_server`，显示用户可能没有注意到该文件顶部的说明，建议删除该文件以避免将来的混淆。,https://github.com/vllm-project/vllm/issues/3852
vllm,"这个issue是一个有关""[Frontend] openAI entrypoint dynamic adapter load""的需求提出，并涉及为vLLM添加动态adapter加载功能。",https://github.com/vllm-project/vllm/issues/3850
vllm,这个issue属于文档更新类型，涉及到VLLM引擎的参数文档更新。,https://github.com/vllm-project/vllm/issues/3845
vllm,这是一个关于代码改进的issue，主要对象是项目中的公共依赖项。由于需要更好地管理不同后端的依赖关系，因此将公共依赖项转移到`requirementscommon.txt`中。,https://github.com/vllm-project/vllm/issues/3841
vllm,该issue类型为用户提出需求，主要涉及VLLM中MoE（例如Mixtral）在运行时动态选择专家数量的功能。由于用户想要同时保证性能和速度，希望能够通过单独触发Mixtral来使用单个7B专家，并在效率上与单个7B相媲美。,https://github.com/vllm-project/vllm/issues/3838
vllm,这是一个用户提出需求的类型的issue，主要涉及要发布第三次见面会的幻灯片。,https://github.com/vllm-project/vllm/issues/3835
vllm,这是一个用户提出需求的issue，主要涉及到VLLM工程中的分布式初始化方法，用户希望添加一个简化集群上Slurm多节点设置的选项。,https://github.com/vllm-project/vllm/issues/3833
vllm,该issue属于用户提出需求类型，主要对象是vllm工具在不同硬件环境下的性能分析。用户在CPU系统上使用perf top时遇到了问题，希望找到更高效的工具监控性能，并寻求关于如何在使用torch作为后端框架时启用分析器的帮助。,https://github.com/vllm-project/vllm/issues/3825
vllm,这个issue是关于[Core] Dynamic model class的需求提出，主要对象是允许动态指定模型实现的方式，导致这个需求的原因是为了可以通过导入路径来动态指定模型的实现。,https://github.com/vllm-project/vllm/issues/3820
vllm,这个issue是文档更新类型的，未指明有具体的相关问题，用raw html标签来解决markdown渲染问题。,https://github.com/vllm-project/vllm/issues/3819
vllm,这是一个关于向vLLM项目贡献代码的用户提出需求的Issue，主要涉及启用`hf_transfer`后端的默认问题。由于环境变量的方式启用`hf_transfer`会错过一些用户，并导致即使在此Python环境中只有可用，也被强制全局启用`hf_transfer`。,https://github.com/vllm-project/vllm/issues/3817
vllm,这是一个关于安装问题的类型为用户提出需求的issue，主要涉及对象为Tesla V100和cuda11.4。由于没有权限安装更高版本的cuda驱动程序，导致无法成功安装vllm，用户需要帮助解决安装问题。,https://github.com/vllm-project/vllm/issues/3812
vllm,这个issue是文档更新类型的，涉及主要对象是vLLM的README.md文件。由于markdown渲染出现问题，需要使用原始的html标签，用户可能提出需要更新文档的帮助。,https://github.com/vllm-project/vllm/issues/3806
vllm,这是一个需求提出类的 issue，主要对象是尝试管理 nccl 版本，可能由于系统无法有效管理 nccl 版本导致的问题。,https://github.com/vllm-project/vllm/issues/3802
vllm,这是一个功能需求提议，该问题涉及的主要对象是`vllm`的`outlines`依赖，用户希望`vllm`可以将其结构生成依赖设置为可选，因为用户需要使用`outlines`包的新版本中的修复。,https://github.com/vllm-project/vllm/issues/3794
vllm,这是一个特性需求类型的issue，主要涉及在vLLM中添加OpenTelemetry分布式追踪功能。由于缺乏分布式追踪功能，用户无法将追踪数据导出到可视化工具，可能导致缺乏系统的观察性和故障排查能力。,https://github.com/vllm-project/vllm/issues/3789
vllm,这是一个功能请求issue，主要涉及的对象是vllm 0.4.0版本。由于vllm从0.3.3版本升级到0.4.0版本后，在基于`nvcr.io/nvidia/pytorch:23.10py3`的Python 3.10，cuda 12.2.2，torch 2.1.0a0+32f93b1的docker镜像上出现错误，而0.3.3版本没有这个问题。,https://github.com/vllm-project/vllm/issues/3786
vllm,这是一个关于改善CI/Build速度的建议，主要涉及到block manager CPU-only unit tests。原因是全局清理torch缓存对于CPUonly测试是不必要的，并且导致了CPUonly测试的严重减慢。,https://github.com/vllm-project/vllm/issues/3783
vllm,该issue是关于代码维护的建议，主要涉及到分离CPUModelRunner和ModelRunner，提高维护性。,https://github.com/vllm-project/vllm/issues/3776
vllm,该issue属于性能优化类型，主要涉及到并行工作者在每个步骤中调度的开销。,https://github.com/vllm-project/vllm/issues/3763
vllm,这是一个用户提出需求的issue，主要涉及vllm模型是否支持特定的quantization类型，用户希望能够在弱设备上运行该模型。,https://github.com/vllm-project/vllm/issues/3756
vllm,这个issue是用户提出需求，希望有更真实对话来进行基准测试。,https://github.com/vllm-project/vllm/issues/3755
vllm,这是一个需求提出类型的issue，主要涉及vllm中如何将LLM对象固定到特定的CUDA设备上。原因是用户希望在同一Python脚本中使用多个vllm实例，每个实例在不同的CUDA设备上运行。,https://github.com/vllm-project/vllm/issues/3750
vllm,这是一个功能需求的issue，主要涉及对象是VLLM项目中的detokenization功能。由于用户需求或者实际场景需要，在生成文本时可选择是否进行token还原。,https://github.com/vllm-project/vllm/issues/3749
vllm,这是一个功能需求，该功能需求主要涉及代码中的初始化过程。由于需要在初始化时禁用分词器，用户提出需要支持在不进行分词的情况下生成文本，因此提出了该需求。,https://github.com/vllm-project/vllm/issues/3748
vllm,这是一个文档更新的Issue，涉及到构建源码和torch/cuda版本依赖的说明。该Issue的主要原因是用户安装时遇到困难，尤其在使用Cmake构建系统时出现了更多问题。,https://github.com/vllm-project/vllm/issues/3746
vllm,这是一个文档更新的Issue，涉及更新 Transformers 到 v4.39.2 版本的内容。原因是为了修复向后兼容性和功能性问题。,https://github.com/vllm-project/vllm/issues/3745
vllm,这是一个用户提出需求的issue， 主要对象是Pytorch 2.2 版本。用户希望Pytorch 2.2 版本能够实现更快的flash attention功能，目前的Pytorch 2.1版本速度较慢。,https://github.com/vllm-project/vllm/issues/3742
vllm,这是一个用户提出需求的 issue，主要涉及到如何从`AsyncLLMEngine`中提取`stat_logger`的信息，但存在访问困难，可能是由于异步运行导致的。,https://github.com/vllm-project/vllm/issues/3736
vllm,"这是用户提出的需求类型的问题，涉及主要对象为`vllm`，由于用户需要另一种获取库版本的途径来符合他们的docker安装, 导致提出了这个问题。",https://github.com/vllm-project/vllm/issues/3735
vllm,这是一个需求提报的issue，主要涉及的对象是e5-mistral-7b-instruct模型和Embedding API。由于需要在embedding模式下关闭KV缓存，并实现嵌入向量生成功能，所以用户提出了关于功能集成和后端计算的问题。,https://github.com/vllm-project/vllm/issues/3734
vllm,这是一个需求提出类型的issue，主要涉及模型加载过程中的性能优化，提出了使用分布式加载方法来加速模型加载的需求。,https://github.com/vllm-project/vllm/issues/3729
vllm,这是一个用户提出需求的issue，主要涉及的对象是为offline LLM课程提供聊天式指导/交流方法。该需求是为了利用和激活指导调整功能。,https://github.com/vllm-project/vllm/issues/3718
vllm,这是一个功能需求类型的issue，主要涉及到为没有提供默认聊天模板的模型分发一组默认聊天模板。造成此问题可能是原始模型的`tokenizer_config.json`中未提供与模型调优相关的聊天/指令模板。,https://github.com/vllm-project/vllm/issues/3717
vllm,这是一个功能需求类的issue，涉及更新vLLM中Outlines集成的方式从FSM到Guide，用户希望新增加速/快进功能以提高模型性能，并可能辅助其他框架集成。,https://github.com/vllm-project/vllm/issues/3715
vllm,这是一个功能需求的issue，主要涉及的对象是将AICI集成到现有的guided decoding stack中。,https://github.com/vllm-project/vllm/issues/3714
vllm,这是一个需求提出的issue，主要涉及的对象是与lm-format-enforcer集成，并由于需要在运行模型服务时与多个不同schema结构接口的客户进行交互导致此需求。,https://github.com/vllm-project/vllm/issues/3713
vllm,这是一个用户提出需求的issue，主要涉及的对象是模型Qwen2ForCausalLM。由于该模型不支持LoRA，但LoRA被启用，因此用户无法运行特定模型进行推理。,https://github.com/vllm-project/vllm/issues/3709
vllm,这是一个更新版本号的issue，不涉及具体的bug报告或需求提出。,https://github.com/vllm-project/vllm/issues/3705
vllm,这是一个开发需求类型的issue，主要涉及到在docker构建镜像中添加ccache，旨在通过CMake的改动减少CI构建时间。,https://github.com/vllm-project/vllm/issues/3704
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是MoE Triton kernel configs。这个问题的原因是AWS请求为A100 40GB GPUs添加MoE Triton kernel configs，以支持Mixtral和DBRX。,https://github.com/vllm-project/vllm/issues/3700
vllm,这是一个关于用户提出需求的 issue，主要涉及支持新模型Jamba。这可能是因为Jamba是一个新型号，具有新颖的混合架构MoE Mamba模型，看起来非常有前景。,https://github.com/vllm-project/vllm/issues/3690
vllm,这是一个需求变更的issue，涉及的主要对象是GPTQ marlin quants加载兼容性，原因是由于Autogptq更新导致了需要支持新的`checkpoint_format`属性来存储`marlin`和未来的格式，以确保与最新Autogptq版本的兼容性。,https://github.com/vllm-project/vllm/issues/3689
vllm,这个issue类型是用户提出需求，主要对象是vllm代码库。由于未启用类型检查，用户希望讨论如何启用mypy类型检查。,https://github.com/vllm-project/vllm/issues/3680
vllm,这是一个提出需求的类型，主要涉及的对象是Triton MoE kernel configs。由于A100 GPUs上缺少适用于DBRX的最佳配置，导致用户提交此需求。,https://github.com/vllm-project/vllm/issues/3679
vllm,这是一个需求报告的issue，主要涉及使用Skypilot在A100机器上运行模型测试，因为无法在GCP中获取足够的资源。,https://github.com/vllm-project/vllm/issues/3674
vllm,这是一个关于代码优化的issue，主要涉及detokenization逻辑的简化。,https://github.com/vllm-project/vllm/issues/3670
vllm,这是一个用户提出需求的issue，主要对象是关于VLLM模型。用户希望在文档中添加支持Command-R的相关信息。,https://github.com/vllm-project/vllm/issues/3669
vllm,这个issue类型是需求提出，主要涉及Prefix caching在BlockManagerV2中的支持缺失，造成需要实现eviction policies、tracking computed bit、E2E correctness test等功能。,https://github.com/vllm-project/vllm/issues/3667
vllm,该issue类型是功能需求，涉及的主要对象是BlockManagerV2，用户提出了关于实现CPU/GPU交换的功能需求。,https://github.com/vllm-project/vllm/issues/3666
vllm,这是一个特性需求的issue，主要涉及到V2版本的block manager中实现滑动窗口支持，由于目前尚未提供滑动窗口功能，需要添加 `SlidingWindowBlockTable` 来支持这一特性。,https://github.com/vllm-project/vllm/issues/3665
vllm,这是一个功能增强类的issue，主要涉及的对象是在Rocm上为向量查询添加自定义内核。根据描述，用户提出了需求并提交了相关的代码修改，旨在通过添加自定义内核来提高Linear Layer中向量输入的性能。,https://github.com/vllm-project/vllm/issues/3664
vllm,这是一个关于代码性能优化的Kernel issue，主要涉及Layernorm性能问题。原因可能是当前实现的性能不足，需要进行优化。,https://github.com/vllm-project/vllm/issues/3662
vllm,这个issue属于需求开发类型，主要对象是对`DBRX`的支持。,https://github.com/vllm-project/vllm/issues/3659
vllm,这是一个用户提出需求的issue，主要涉及支持Databricks发布的DBRX模型，可能需要定制化实现。,https://github.com/vllm-project/vllm/issues/3658
vllm,这是一则关于功能需求的issue，主要涉及支持CPU的执行器以及相关特性的支持。,https://github.com/vllm-project/vllm/issues/3654
vllm,这是一个用户提出需求的Issue，主要对象是希望在VLLM中指定每个请求生成的令牌数。由于用户需要在每个请求中分别指定生成的令牌数，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/3650
vllm,该issue是关于功能需求的，主要涉及VLLM的LLM引擎，由于LLM引擎在初始化时必须调用_init_tokenizer方法，导致无法在不提供tokenizer的情况下初始化引擎，用户希望能支持在不需要tokenizer的情况下初始化LLM引擎。,https://github.com/vllm-project/vllm/issues/3647
vllm,这个issue是一个[功能改进]类型的问题，主要涉及到针对ROCm的默认FA使用Triton Kernel的相关调整。这个改动是为了添加一个新的Triton FA后端并移除一些AMD版本中不支持的参数，以及允许在多个后端中选择而不仅仅是两个后端之间的选择。,https://github.com/vllm-project/vllm/issues/3643
vllm,这个issue是一个功能需求，主要涉及到在集成gptq_marlin时支持act_order和所有group_sizes。由于新增功能需要验证多GPU模式是否正常工作，所以用户可能需要帮助验证多GPU模式。,https://github.com/vllm-project/vllm/issues/3642
vllm,这是一个需求类型的issue，主要涉及CI系统中添加测试用例来运行示例脚本。这个问题由于缺乏在CI中测试示例代码而导致的。,https://github.com/vllm-project/vllm/issues/3638
vllm,这是一个关于用户提出需求的issue，主要涉及到改进vllm在生成文本时跳过detokenization操作的功能。由于当前环境下CPU利用率较高，GPU利用率较低，用户希望优化算法以提高性能。,https://github.com/vllm-project/vllm/issues/3635
vllm,这个issue是关于新增CPU推断后端的需求，涉及到vLLM项目的硬件和Intel处理器。由于需要扩展vLLM的支持到CPU推断，并且添加了新的功能但还未完全实现，因此提出了是否符合标准的RFC以及相关的检查清单。,https://github.com/vllm-project/vllm/issues/3634
vllm,这个issue属于用户提出需求类型，主要对象是如何安全地将经过训练的LLM模型交付给客户以进行演示。由于模型敏感且可能被滥用，需要加密确保数据安全。,https://github.com/vllm-project/vllm/issues/3624
vllm,这是一个关于新增功能提案的issue，主要对象是为支持Cloud TPUs而对 vLLM 进行的工作。,https://github.com/vllm-project/vllm/issues/3620
vllm,这是一个关于需求提议的issue，主要涉及到vllm项目中持续集成环境的优化。,https://github.com/vllm-project/vllm/issues/3619
vllm,这个issue类型是用户提出需求，主要涉及增强系统的可观测性，用户希望能够获得更多的操作指标以监控系统状态。由于用户需要对生产监控获得更多运行指标，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/3616
vllm,这是一个功能需求提出的issue，主要涉及的对象是针对EETQ量化方法的实现。由于无法在线量化，用户请求帮助解决这个问题。,https://github.com/vllm-project/vllm/issues/3614
vllm,这是一个关于优化代码性能的issue，主要涉及benchmark_throughput.py中sample_requests函数执行时间长的问题，可能导致吞吐量差异。,https://github.com/vllm-project/vllm/issues/3613
vllm,这是一个用户提出需求的issue，主要对象是为vllm添加对xverse models的支持。用户请求通过pip安装支持xverse models的最新版本。,https://github.com/vllm-project/vllm/issues/3610
vllm,该issue类型是关于提出需求，主要对象是关于LLM Serving的高吞吐GPU-Efficient方案，问题是由GPU效率低下在顺序生成令牌时导致成本高昂的瓶颈，以及KVCache占用过多内存无法同时容纳更多序列所导致的挑战。,https://github.com/vllm-project/vllm/issues/3606
vllm,这是一个用户提出需求的issue，主要是关于使用vllm从特定层提取隐藏状态的问题。由于提取隐藏状态的方法速度较慢，用户希望找到更快的方法，而且想获得所有层的隐藏状态嵌入。,https://github.com/vllm-project/vllm/issues/3594
vllm,这是一个更新请求，主要涉及的对象是`transformers`版本。由于`4.39.1`中修复了对LLaVA模型的一些破坏性更改，导致 https://github.com/vllmproject/vllm/pull/3042 无法通过。,https://github.com/vllm-project/vllm/issues/3592
vllm,这是一个需求提出的RFC（请求提案变更）类型的issue，主要涉及分布式推断环境的接口和抽象。问题的根本原因是当前代码库中的分布式推断存在问题导致头绪混乱，并且出现了死锁和挂起的情况，尤其在升级到pytorch 2.2.0版本时问题更为突出。,https://github.com/vllm-project/vllm/issues/3587
vllm,这是一个用户提出需求的类型的issue，主要涉及支持RWKV模型，由于准备2T的检查点，提议使用线性注意力来实现更长的上下文和更快的推理。,https://github.com/vllm-project/vllm/issues/3583
vllm,这是一个用户提出需求的问题，用户想了解在运行OPT模型推断时，停止生成新输出token的条件在哪里查找。,https://github.com/vllm-project/vllm/issues/3576
vllm,这是一个关于需求提议的issue，主要对象是扩展LLM上下文窗口至2048k tokens。原因是希望增强预训练语言模型的能力。,https://github.com/vllm-project/vllm/issues/3575
vllm,该问题类型为用户提出需求，主要涉及对象是新增功能，用户询问是否希望实现长上下文窗口功能。,https://github.com/vllm-project/vllm/issues/3573
vllm,这个issue是关于CI/CD的改进，主要涉及vLLM项目中的neuron backend支持。原因可能是为了提高代码质量和审查效率。,https://github.com/vllm-project/vllm/issues/3571
vllm,这是一个关于在vLLM前端支持多个采样参数的需求提出的issue。,https://github.com/vllm-project/vllm/issues/3570
vllm,该issue属于用户提出需求类型，主要涉及的对象是vLLM模型权重的CPU分担。由于AI模型规模快速扩张，需要更大内存容量的GPU，用户缺乏强大GPU资源，难以运行vLLM在多个模型上。因此用户希望通过cpu_offload_weight功能在vLLM中实现将模型权重分担到CPU，以解决这一挑战。,https://github.com/vllm-project/vllm/issues/3563
vllm,这是一个用户提出需求的issue，主要对象是在vllm上支持Phi-2与LoRA模型的情况。由于未收到任何响应，用户询问是否有困难支持所需的模型。,https://github.com/vllm-project/vllm/issues/3562
vllm,这个issue类型是用户提出需求，主要涉及的对象是vllm，用户提出需要测试PyTorch Nightly和其他依赖库以获取早期信息。,https://github.com/vllm-project/vllm/issues/3560
vllm,这是一个用户提出需求的类型的issue，主要涉及VLLM加速框架是否兼容TensorRTLLM的问题。原因可能是用户希望了解两者是否可以一起使用。,https://github.com/vllm-project/vllm/issues/3555
vllm,这个issue属于升级需求，主要对象是更新了transformers版本到v4.39.0以及删除StarCoder2Config，原因是为了更好地升级transformers版本并移除不再需要的配置文件。,https://github.com/vllm-project/vllm/issues/3551
vllm,这是一个用户提出需求的issue，主要关注的对象是关于动态内存压缩和LLM用于加速推理的特性。,https://github.com/vllm-project/vllm/issues/3549
vllm,这个issue类型为文档改进， 主要涉及的对象是项目的示例脚本，用户提出了建议将示例脚本同步至文档仓库中使其更易发现和可搜索。,https://github.com/vllm-project/vllm/issues/3547
vllm,这是一个用户提出需求的issue， 主要对象是项目vllm。 由于项目需要每夜生成镜像和wheels，用户希望能够简单地构建并上传至Google容器仓库和存储桶。,https://github.com/vllm-project/vllm/issues/3545
vllm,这是一个用户提出需求的issue，主要对象是增加对于Guided Decoding的支持。原因是用户希望在offline接口中与OpenAI推理服务器同样支持Guided Decoding功能。,https://github.com/vllm-project/vllm/issues/3536
vllm,这是一个功能需求（Feature Request）issue，主要涉及的对象是vLLM模型加载使用CoreWeave的`tensorizer`功能。由于模型张量存储在容器映像中导致映像变得庞大，用户提出了使用`tensorizer`加载模型以减小容器映像大小和加载模型时间的需求。,https://github.com/vllm-project/vllm/issues/3533
vllm,这个issue是关于一个新功能提议，涉及的主要对象是提高大型语言模型推理效率的技术。,https://github.com/vllm-project/vllm/issues/3531
vllm,这是一个功能增强的需求，主要对象是用于基准测试的请求发送程序。,https://github.com/vllm-project/vllm/issues/3530
vllm,该issue类型是需求提出，主要涉及到在vLLM中实现全张量并行化的LoRA层，由于当前的multiLoRA层只能使用张量并行化运行其中一个LoRA，在更高的序列长度、rank以及张量并行化尺寸下具有更好的性能，但在较小规模下性能类似。,https://github.com/vllm-project/vllm/issues/3524
vllm,该issue类型为用户提出需求，主要涉及对象是vLLM和KubeRay。由于vLLM在启动时急切检查可用的GPU资源并失败快速，导致无法利用KubeRay的自动缩放功能。,https://github.com/vllm-project/vllm/issues/3522
vllm,这个issue属于用户提出需求类型，主要涉及到支持新模型CogVLM。由于需要修改vLLM引擎以支持视觉模块，导致了用户提出的问题。,https://github.com/vllm-project/vllm/issues/3519
vllm,这是一个用户提出需求的issue，用户希望能够在vllm中指定生成的token数量。,https://github.com/vllm-project/vllm/issues/3518
vllm,这是一个功能需求类型的issue，主要涉及Token Rank在Logprobs对象中的添加以及与IBM的相关功能对齐。,https://github.com/vllm-project/vllm/issues/3516
vllm,这是一个需求类型的issue，主要涉及到在VLLM模型中添加attention sinks功能。用户提出了支持Attention Sink以解决模型关注到序列中前几个token导致生成无关输出的问题。,https://github.com/vllm-project/vllm/issues/3515
vllm,这是一个功能需求类型的issue，主要涉及对象是`LRUCache`和`BaseTokenizerGroup`。由于缺乏泛型类型和非必需关键字参数反映在`BaseTokenizerGroup`抽象方法中，导致需要添加对泛型类型的支持来反映这些非必需参数。,https://github.com/vllm-project/vllm/issues/3511
vllm,这是一个关于使用lru_cache进行一些环境检测工具的需求问题，针对Python 3.8兼容性问题。,https://github.com/vllm-project/vllm/issues/3508
vllm,这是一个提出拉取请求的issue，涉及集成FP8 attention kernel。由于markdown渲染不起作用，需要使用原始html进行描述。,https://github.com/vllm-project/vllm/issues/3502
vllm,这是一个用户提出需求的 issue，主要对象是为了实现动态下载 LoRA 适配器，目前需要先将适配器下载到服务器上。,https://github.com/vllm-project/vllm/issues/3501
vllm,这是一个用户提出需求的issue，主要涉及了通过LoRA对vllm进行扩展的功能。用户想要实现在初始化时附加LoRA，而不是在每个请求中都进行附加的操作。,https://github.com/vllm-project/vllm/issues/3499
vllm,这是一个用户提出需求的issue，主要涉及于vLLM模型的功能增强，请求计算并记录serving FLOPs，旨在帮助调试性能并检查GPU利用率。,https://github.com/vllm-project/vllm/issues/3490
vllm,这是一个用户提出需求的类型的issue，主要涉及对象是vllm的GPU内存使用控制，用户想了解为什么增加maxnumseqs会减少内存使用。,https://github.com/vllm-project/vllm/issues/3489
vllm,这个issue类型是文档更新，涉及更新vLLM第三次Meetup的README。由于markdown渲染不起作用，导致需要使用HTML来更新内容。,https://github.com/vllm-project/vllm/issues/3479
vllm,这是一个功能需求问题，涉及到在vllm中添加使用`tensorizer`进行模型加载的功能。,https://github.com/vllm-project/vllm/issues/3476
vllm,这个issue是用户提出需求类型的，主要涉及的对象是对vLLM支持xai-org/grok-1模型的请求。由于模型代码基于JAX且需要int8量化，用户希望有人将其移植到PyTorch以便与vLLM集成并进行MOE架构的额外优化。,https://github.com/vllm-project/vllm/issues/3472
vllm,这是一个用户提出需求的issue，主要对象是关于Microsoft提出的DeepSpeed-FP6优化方法，询问vLLM团队是否研究过该方法，可能是因为想了解该方法是否对模型速度和精度有所优化。,https://github.com/vllm-project/vllm/issues/3468
vllm,这个issue属于功能需求类型，主要对象是针对vllm项目中的单节点多GPU部署。由于对ray依赖的需求不符合实时同步推理的特定要求，并且希望在非ray集群环境下使用“轻量级”选项来提高生产安全合规性，因此提出了对ray可选性的改进。,https://github.com/vllm-project/vllm/issues/3466
vllm,这是一个关于如何增加vLLM端点请求处理数量的问题。主要涉及vLLM服务在处理组请求时性能问题，表现为请求处理数量限制。,https://github.com/vllm-project/vllm/issues/3464
vllm,这个issue是一个功能改进的提议，主要涉及Attention模块的重构。原因是为了隐藏特定于后端的Attention实现细节，以便更方便地引入新的后端。,https://github.com/vllm-project/vllm/issues/3462
vllm,该issue属于代码改进类型，涉及到vLLM项目中的cache_stream和cache_events对象。这个问题主要由于vLLM引入CUDA graphs后使得原本的finegrained overlapping功能被禁用，导致这两个对象成为了无用的对象。,https://github.com/vllm-project/vllm/issues/3461
vllm,这是一个用户提出的需求类型的issue，主要对象是Popular chinese LLMs baichuan/baichuan2/qwen/chatlgm， 用户希望提供对这些LLMs的支持。,https://github.com/vllm-project/vllm/issues/3458
vllm,这是一个特性需求类型的issue，主要涉及的对象是 `InputMetadata` 类。这个需求是为了让模型运行器中的代码变得更清晰。,https://github.com/vllm-project/vllm/issues/3452
vllm,这是一个功能需求类型的issue，主要涉及添加对控制向量的支持，相关链接可参考提供的github页面。,https://github.com/vllm-project/vllm/issues/3451
vllm,这是一个功能改进的issue，主要涉及的对象是异步tokenization的线程池支持，由于需要使用线程池代替ray来处理tokenization，且考虑HF tokenizers的线程安全性问题，每个线程都会使用单独的tokenizer实例。,https://github.com/vllm-project/vllm/issues/3449
vllm,这是一个功能需求提交，主要涉及前端支持新的lora模块添加到OpenAI Entrypoints的问题。由于先前版本的OpenAI entrypoints不支持将lora适配器添加到实时服务器，因此用户需要更新版本以解决该问题。,https://github.com/vllm-project/vllm/issues/3446
vllm,这是一个特性请求，主要涉及AQLM量化方法的介绍和性能分析。,https://github.com/vllm-project/vllm/issues/3439
vllm,这是一个用户提出需求类型的issue，主要涉及的对象是为vllm添加对Cohere的Command-R模型的支持。,https://github.com/vllm-project/vllm/issues/3433
vllm,这是一个关于性能优化的issue，主要涉及到evictor的改进，由于使用`OrderedDict`替代`Dict`，提升了缓存处理速度。,https://github.com/vllm-project/vllm/issues/3431
vllm,该问题类型为功能需求，主要涉及更新Dockerfile以实现ModelScope支持，用户提出了是否需要修改文档或其他文件的疑问。,https://github.com/vllm-project/vllm/issues/3429
vllm,这是一个用户提出需求的issue，主要涉及vllm和Modelscope，用户希望添加Docker支持以运行vllm。,https://github.com/vllm-project/vllm/issues/3428
vllm,这是一个用户提出需求的issue，主要对象是vllm项目。用户想了解如何在pytorch/xla上运行vllm。,https://github.com/vllm-project/vllm/issues/3424
vllm,这个issue是一个用户提出需求的类型， 主要对象是vllm，用户询问是否有计划支持pytorch 2.2.0。,https://github.com/vllm-project/vllm/issues/3423
vllm,这个issue类型是需求提出，涉及的主要对象是Falcon软件不同版本的聊天格式。,https://github.com/vllm-project/vllm/issues/3420
vllm,该问题为用户提出需求，主要涉及的对象是设置主机IP环境变量。由于主机有多个IP地址且用户需要指定使用哪一个IP地址，需要在分布式集群中让用户指定使用的IP地址。,https://github.com/vllm-project/vllm/issues/3419
vllm,这是一个需求提出类型的issue，主要涉及的对象是ChatGLM/ChatGLM2。,https://github.com/vllm-project/vllm/issues/3418
vllm,这个issue类型是用户提出需求，主要涉及的对象是vllm（版本0.3.3）是否支持 wizardcoder 和 deepseek 模型的 lora 推理，原因是项目需要使用这些模型的 loraadapters 进行推理。,https://github.com/vllm-project/vllm/issues/3417
vllm,这个issue属于功能改进类型，涉及主要对象为vLLM中的Gemma实现，原因是要改进实现方式以使用近似的GELU函数。,https://github.com/vllm-project/vllm/issues/3411
vllm,这个issue属于用户提出需求类型，主要涉及的对象是uvicorn，用户提出了在mTLS支持方面参数不足的问题。,https://github.com/vllm-project/vllm/issues/3410
vllm,该issue类型为功能需求提议，主要对象是LLM模型的entrypoint。由于现有设计导致用户在等待所有请求完成之后才能获得输出结果，用户提出了需要增加选项来实现每个引擎步骤输出结果的流式传输。,https://github.com/vllm-project/vllm/issues/3409
vllm,这是一个用户提出需求的类型为Feature请求的issue，主要涉及的对象是VLLM。由于用户需要支持Cohere Command-R模型在VLLM中的使用，请求对该模型提供支持。,https://github.com/vllm-project/vllm/issues/3403
vllm,这是一个需求优化的issue，主要对象是vLLM中的prefill with prefix cache功能。问题出现的原因是当前实现中批处理大小较小，导致效率低下和重复的incremental_detokenize操作。,https://github.com/vllm-project/vllm/issues/3402
vllm,这是一个用户提出需求类型的issue，主要涉及到VLLM的批处理功能，用户想要指定最大批处理大小以在连续批处理中找到最佳的最大批处理大小。,https://github.com/vllm-project/vllm/issues/3395
vllm,这是一个用户提出需求的类型的 issue，主要涉及的对象是将 FastV 推断加速方法与 vLLM 合并，需要实现在某个层之后删除一些 token 以达到推断加速和降低 GPU 使用的效果。,https://github.com/vllm-project/vllm/issues/3394
vllm,这是一个用户提出需求的issue，主要涉及解决当多个vllm实例部署在kubernetes中时，grafana显示多个vllm实例指标时混乱的问题。请求添加一个model_name作为变量，允许用户选择显示哪个vllm实例的指标。,https://github.com/vllm-project/vllm/issues/3393
vllm,这是一个关于需求的问题，涉及AsyncLLMEngine.generate方法返回内容格式的调整。用户希望方法调用只返回新生成的文本而不包含每次增量文本。,https://github.com/vllm-project/vllm/issues/3391
vllm,这是一个功能需求类型的issue，主要涉及的对象是kernel benchmark script。由于需要调整moe kernel在A100/H100上的性能参数，需要修改benchmark脚本以便直接使用结果。,https://github.com/vllm-project/vllm/issues/3389
vllm,这是一个用户提出需求的issue，主要涉及到对ChatGLM3和BaiChuan7B等模型进行LoRA支持的实现。导致这个需求的原因是这些模型中的特定层已经被合并，因此需要实现新的线性层来满足LoRA集成的需求。,https://github.com/vllm-project/vllm/issues/3382
vllm,该issue属于用户需求，用户希望VLLM能够选择使用哪个GPU进行推断计算。,https://github.com/vllm-project/vllm/issues/3379
vllm,这是一个需求提出类型的issue，主要涉及的对象是 GPTBigCode 项目中的 Lora Adapter 模块。,https://github.com/vllm-project/vllm/issues/3362
vllm,这是一个用户提出需求的issue，主要涉及到在文本生成过程中如何排除不良语言的问题。,https://github.com/vllm-project/vllm/issues/3361
vllm,这是一个需求类的issue，主要涉及到issue模板的添加。由于开发人员负担过重，需要先对问题进行分类。,https://github.com/vllm-project/vllm/issues/3360
vllm,这是一个用户提出需求的issue，主要涉及到`FlashAttentionBackend`在大小限制方面的问题。用户想要了解为什么只支持特定大小的头部，是否可以扩展支持所有尺寸的头部。,https://github.com/vllm-project/vllm/issues/3359
vllm,该issue类型为用户提出需求，主要涉及的对象是DeepSeek VL。这个问题是用户希望DeepSeek VL能够支持所有服务功能所致，由于当前DeepSeek VL不支持所有服务功能，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/3356
vllm,这是一个用户提出需求的issue，主要涉及的对象是要为vLLM添加对Qwen2MoeModel的支持。,https://github.com/vllm-project/vllm/issues/3346
vllm,这个issue为“用户提出需求”，主要对象是vllm的MLFQ实现。,https://github.com/vllm-project/vllm/issues/3342
vllm,这是一个用户提出需求的类型的issue，主要对象是GPTQ量化内核的集成问题，用户希望添加4位NormalFloat（NF4）的支持。,https://github.com/vllm-project/vllm/issues/3339
vllm,该issue类型为功能需求，涉及对象为vLLM的GeGLU内核，通过添加近似GELU内核为将来更改模型定义做准备，可能由于Hugging Face尚未修复其模型而导致需求添加近似GELU内核。,https://github.com/vllm-project/vllm/issues/3337
vllm,"这是一个用户提出需求的类型的issue，主要涉及BentoML文档在""Serving""章节的部分更新。",https://github.com/vllm-project/vllm/issues/3336
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是要求支持CohereForCausalLM模型。用户提出需求的原因是希望在项目中添加对CohereForCausalLM模型的支持，因为该模型支持多种语言，包括一些其他模型不支持的语言，如越南语和波斯语，用户认为该模型会被非英语AI社区广泛使用。,https://github.com/vllm-project/vllm/issues/3335
vllm,这是一个关于软件功能使用问题的issue，主要涉及到使用vllm时关于随机种子设置的疑惑。用户想了解在LLM引擎和SamplingParams中设置随机种子的最佳实践，以及这两者之间的优先级关系。,https://github.com/vllm-project/vllm/issues/3333
vllm,该issue是用户提出需求，主要涉及的对象是CohereForAI/c4aicommandrv01。由于用户希望获得对CohereForAI/c4aicommandrv01的支持，以及该模型在多种语言生成和高性能RAG功能上的表现。,https://github.com/vllm-project/vllm/issues/3330
vllm,这是一个功能需求的issue，主要涉及的对象是实现了一个JSON grammar解析引擎。由于启动时间过长，该功能默认禁用，用户需要使用`enablejsonmode`命令行参数来启用。,https://github.com/vllm-project/vllm/issues/3328
vllm,该issue是关于用户需求的，主要涉及的对象是vLLM模型。由于用户想要将模型加载到CPU内存而不是GPU内存，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/3327
vllm,该issue属于用户提出需求类型，主要涉及在vLLM中是否能够同时服务多个模型的问题。发起问题的原因是想要选择不同的GPU加载不同模型，但可能受限于GPU内存保留给单个模型的kv缓存。,https://github.com/vllm-project/vllm/issues/3326
vllm,这个issue是用户提出需求，请求添加aya-101模型到vllm。,https://github.com/vllm-project/vllm/issues/3318
vllm,这是一个用户提出需求的issue，主要涉及的对象是提交第二个拉取请求。可能是由于项目需要进一步修改或添加内容，用户需要再次提交请求。,https://github.com/vllm-project/vllm/issues/3317
vllm,这个issue是关于性能问题的。主要对象是vllm中的lora功能，用户在测试中发现动态调用lora的速度比合并lora权重后的速度要慢，询问是否正常。,https://github.com/vllm-project/vllm/issues/3316
vllm,这是用户提出需求的类型，主要涉及Qwen1.5模型以及参考llama的Lora，可能是由于Qwen1.5模型目前尚未实现Lora功能，希望参考llama的实现来完成此功能。,https://github.com/vllm-project/vllm/issues/3315
vllm,这是一个关于用户提出需求的issue，主要涉及的对象是vllm团队。用户询问了关于pipeline parallelism未在新路线图中的原因，可能是出于实现挑战或者当前计划中没有合适安排。,https://github.com/vllm-project/vllm/issues/3314
vllm,这是一个特性需求，用户希望在`LLM` API中支持多个不同的采样参数。,https://github.com/vllm-project/vllm/issues/3313
vllm,这是一个功能需求的issue，主要涉及到添加和删除finetuned weights的API支持。这个问题产生的原因是在生产系统中需要一个API动态地添加或移除finetuned weights，并且推理调用者不需要在每次调用中指定LoRA位置。,https://github.com/vllm-project/vllm/issues/3308
vllm,这是一个用户提出需求的issue，主要对象是关于在nm-vllm中使用稀疏推断和仅权重为int8量化是否可以同时用于进一步提高推断速度的问题。用户询问是否有相关计划。,https://github.com/vllm-project/vllm/issues/3307
vllm,这个issue属于功能需求类，主要对象是linting工具，由于当前linter不检查行宽度，导致需要重新启用这个检查的功能。,https://github.com/vllm-project/vllm/issues/3305
vllm,这是一个文档更新类的Issue，主要涉及的对象是关于LoRA支持信息的模型。由于LoRA文档中存在小错误，导致用户需要更新相关信息。,https://github.com/vllm-project/vllm/issues/3299
vllm,这个issue是用户提出需求类型的问题，主要涉及到在使用vllm和ray serve进行多副本的张量并行时可能遇到的问题或需要解决的方案。,https://github.com/vllm-project/vllm/issues/3294
vllm,此issue属于功能改进，请汇报了关于在ROCm上启用基于 FP8（e4m3fn）KV 缓存的建议，主要涉及的对象是在AMD GPU上使用 FP8 缓存。原因是为了在推理过程中减少量化损失，并展示了如何通过AMD Quantizer和nVIDIA AMMO实现FP8量化及KV缓存。,https://github.com/vllm-project/vllm/issues/3290
vllm,这是一个功能改进的issue，主要涉及新的AWQ内核的实现。,https://github.com/vllm-project/vllm/issues/3289
vllm,该issue属于文档更新类型，主要涉及的对象是OpenAI兼容服务器，原因是为了提供关于启动服务器、调用OpenAI客户端、兼容性、参数覆盖、额外采样参数支持和命令行参数等信息。,https://github.com/vllm-project/vllm/issues/3288
vllm,这是一个功能需求的issue，主要涉及的对象是AQLM CUDA支持。由于新增的AQLM压缩推断在运行时格式为1x16和2x8时性能较好，而其他格式在性能上可能受到影响，用户寻求支持这两种格式的CUDA kernels以提高性能。,https://github.com/vllm-project/vllm/issues/3287
vllm,这是一个用户提出需求的issue，主要涉及对象是使用json模板与mixtral 7x8b模型。用户想要强制生成的json遵循初始模式而非按字母顺序排列，因为按字母顺序排列对生成质量产生了重大影响。,https://github.com/vllm-project/vllm/issues/3283
vllm,该issue为用户提出需求类型的问题，主要涉及的对象是vLLM推理服务器的调度器。由于调度器过早安排提示，导致了非常小的提示批次，进而导致解码阶段饥饿，用户通过添加`schedulerusedelay`特性来动态增加调度延迟以改善ITL性能。,https://github.com/vllm-project/vllm/issues/3279
vllm,该issue类型为用户提出需求，主要涉及对象是vLLM是否支持在cuda图模式下与cupy进行多节点连接，用户想知道vLLM是否有计划支持此功能。,https://github.com/vllm-project/vllm/issues/3278
vllm,该issue是关于特性需求的，主要涉及对Serving Benchmark的改进，包括增加前缀缓存基准测试，并添加保存每个请求的单个令牌延迟到结果json以进行进一步调试的功能。,https://github.com/vllm-project/vllm/issues/3277
vllm,这个issue类型为需求，主要对象是合并他人 fork 的代码，问题由于需要整合另一个用户fork的代码。,https://github.com/vllm-project/vllm/issues/3274
vllm,这是一个功能新增的issue，主要涉及FlashInfer、Flash Attention后端和缓存引擎，提供了新增功能和改进的实现详情。,https://github.com/vllm-project/vllm/issues/3273
vllm,这是一个需求提问类型的issue，主要涉及对象是vLLM模型的8位量化支持问题，用户寻求关于在大上下文窗口下使用vLLM时需使用8位量化的帮助。,https://github.com/vllm-project/vllm/issues/3261
vllm,该issue属于用户提出需求类型，主要涉及将引擎健康检查连接到OpenAI服务器，以便在Kubernetes健康探针中使用。这个问题的提出是为了使异步引擎更加健壮。,https://github.com/vllm-project/vllm/issues/3260
vllm,这是一个关于性能优化的issue，主要涉及到GPU利用率和吞吐量的问题。原因可能是当前配置下无法最大化GPU利用率，需要优化配置以提高吞吐量。,https://github.com/vllm-project/vllm/issues/3257
vllm,"这个issue属于功能需求，主要涉及了""Lookahead scheduling""的概念的引入。这个issue由于需要提前为每个序列分配KV插槽，以便用于推测解码，带来了新功能和修改的需求。",https://github.com/vllm-project/vllm/issues/3250
vllm,这个issue属于用户提出需求的类型，主要涉及模型与LoRA适配器的支持关系。原因是该模型不支持LoRA，导致数值错误，并询问哪些模型受支持以及关于模型适配的计划。,https://github.com/vllm-project/vllm/issues/3245
vllm,这是一个用户提出需求的issue，主要目的是增强lora测试中的层次和排名变化。,https://github.com/vllm-project/vllm/issues/3243
vllm,这是一个需求提出的issue，主要涉及对象是Vim-Like-LaTeX-Markup (vllm)插件。由于tmux窗口大小可能会改变，建议添加`dynamic_ncols=True`选项以提高使用体验。,https://github.com/vllm-project/vllm/issues/3242
vllm,该issue类型为性能优化，针对的主要对象是针对A100 TP2的fused moe kernel配置。,https://github.com/vllm-project/vllm/issues/3240
vllm,这是一个用户提出需求的issue，涉及的主要对象是OpenAI Tools的函数调用功能。这个问题是由于需要更新使用新的guided generation实现，用户希望在查询时可以根据需要选择使用模板系统或者引导生成参数。,https://github.com/vllm-project/vllm/issues/3237
vllm,这是一个需求修改类型的issue，主要涉及代码实现和查询格式的调整，因为当前的查询格式不适用于chunked prefill，导致在启用chunked prefill后无法有效批处理填充和解码请求。,https://github.com/vllm-project/vllm/issues/3236
vllm,这是一个需要重构查询形状以支持特定功能的Issue，主要涉及到查询形状和填充配置的调整。这个问题是因为当前查询格式不适合chunked prefill的情况而引起的。,https://github.com/vllm-project/vllm/issues/3235
vllm,这是一个功能需求的issue，主要涉及的对象是模型运行器（model_runner），产生的问题是为了使Sampler更简单和清晰而将`logits`计算和收集迁移到`model_runner`中。,https://github.com/vllm-project/vllm/issues/3233
vllm,该issue属于代码优化类型，主要涉及的对象是`tensor_model_parallel_all_gather`函数。这个问题是由于CC(Use NCCL instead of ray for controlplane communication)造成的，导致`tensor_model_parallel_all_gather`函数目前似乎没有被使用，而被`tensor_model_parallel_gather`函数替代。,https://github.com/vllm-project/vllm/issues/3231
vllm,这个issue是用户提出需求的类型，主要对象是对QLORA/QA-QLORA权重的支持，原因是目前仅支持原始LORA作为未融合的适配器，用户希望能够在不与基础模型融合的情况下添加对QLORA/QA-QLORA支持。,https://github.com/vllm-project/vllm/issues/3225
vllm,这个issue类型为功能更新，主要涉及将FlashInfer集成到vLLM中，但目前尚未实际使用。,https://github.com/vllm-project/vllm/issues/3221
vllm,这个issue是关于需求的，用户询问了何时将Qwen Lora模型整合并加载。,https://github.com/vllm-project/vllm/issues/3216
vllm,该问题属于需求询问类型，主要涉及的对象是Qwen Lora模型集成。由于用户想了解何时集成Qwen Lora模型，可能是因为希望在模型中使用该功能或者了解相关集成进度。,https://github.com/vllm-project/vllm/issues/3215
vllm,这个issue是用户提出的需求类型，需求涉及支持在OpenAI和Context Free Grammar中使用任意的json_object，可能由于现有功能限制导致无法实现该需求。,https://github.com/vllm-project/vllm/issues/3211
vllm,这是一个用户提出需求的issue，主要涉及对象是npcache load format，由于未能提供比safetensors更快的加载速度，反而更慢，用户在询问是否有其特定使用场景。,https://github.com/vllm-project/vllm/issues/3210
vllm,这是一个关于性能优化和开发需求的issue，主要涉及Mixtral前向传播中的计算优化问题，由于时间大部分花费在全连接层/融合moe内核，需要使用fp8进行计算以提升性能。,https://github.com/vllm-project/vllm/issues/3208
vllm,该issue为一个功能增强类型的内容，主要涉及异步tokenization和线程池的使用。在该issue中，用户提出使用线程池而非ray来进行tokenization，主要是由于HF tokenizers在官方声明中不是线程安全的，尽管实际上除非涉及填充/截断这种操作，否则它们在实践中可能是线程安全的。,https://github.com/vllm-project/vllm/issues/3206
vllm,这是一个需求提出类型的issue，主要涉及日志记录相关的度量指标，原因可能是为了满足交付物1的要求。,https://github.com/vllm-project/vllm/issues/3205
vllm,这个issue类型为功能开发中的工作进行中（Work in Progress），主要涉及到chunked prefill部分的实现。由于目前chunked prefill功能尚未实现，导致用户无法使用相关功能。,https://github.com/vllm-project/vllm/issues/3204
vllm,这个issue是用户提出需求。用户希望能支持ExLlamaV2作为一个快速而优秀的库来运行LLM。,https://github.com/vllm-project/vllm/issues/3203
vllm,这是一个用户提出需求的问题，主要对象是 OpenAI 服务器，用户提出了关于添加授权机制以确保端点安全的需求。,https://github.com/vllm-project/vllm/issues/3202
vllm,这是一个用户提出需求的issue，主要涉及对象是vLLM模型。用户想要获取在使用vLLM时的注意力分数。,https://github.com/vllm-project/vllm/issues/3192
vllm,这是一个功能需求提议，主要涉及的对象是`LLMEngine`中的分布式工作管理器部分。由于需要在不使用ray的情况下支持单盒分布式执行，并且将代码分离为不同的硬件后端，因此需要添加分布式模型执行器抽象。,https://github.com/vllm-project/vllm/issues/3191
vllm,这个issue是新增功能请求，涉及的主要对象是支持嵌入模型的集成。,https://github.com/vllm-project/vllm/issues/3187
vllm,这个issue类型是性能优化建议，主要涉及到模型加载时的性能问题，由于默认设备为'cpu'导致了加载速度严重变慢。,https://github.com/vllm-project/vllm/issues/3185
vllm,这是一个新功能需求，主要涉及Jais模型的支持，而不是bug报告。,https://github.com/vllm-project/vllm/issues/3183
vllm,这是一个需求提出类型的issue，主要涉及的对象是requirements-dev.txt文件。由于缺少aiohttp==3.9.3包，在benchmarking脚本中无法正常运行，需要添加该依赖包。,https://github.com/vllm-project/vllm/issues/3181
vllm,这是一个用户提出新增功能需求的issue，主要涉及的对象是Qwen2设备。,https://github.com/vllm-project/vllm/issues/3177
vllm,这是一个需求提出类型的issue，主要涉及的对象是vllm库中GPU的指定，由于版本更新导致以前可指定GPU的功能在0.2.6之后的版本中不再受支持，用户提出了需要指定GPU编号的功能以提高资源利用和吞吐量。,https://github.com/vllm-project/vllm/issues/3172
vllm,这是一个功能需求的issue，主要涉及的对象是vLLM。由于缺少Quadratic和Cubic Sampling方法导致用户提出了添加这些功能的需求。,https://github.com/vllm-project/vllm/issues/3167
vllm,这是一个功能需求的issue，主要对象涉及到Sequence类，提出将`eos_token_id`存储在Sequence中以方便频繁访问。,https://github.com/vllm-project/vllm/issues/3166
vllm,这个issue类型为用户提出需求，涉及的主要对象是Model support中的starcoder2，用户提出需要支持bigcode/starcoder215b。,https://github.com/vllm-project/vllm/issues/3165
vllm,这是一个关于工具功能改进的issue，主要涉及到对vLLM仓库中过期或无关的问题进行自动清理。原因可能是问题过多导致组织混乱，需要更好的管理和维护。,https://github.com/vllm-project/vllm/issues/3164
vllm,这是一个功能需求类型的issue，主要涉及的对象是用于高张量并行性模型分析的Ray框架。,https://github.com/vllm-project/vllm/issues/3162
vllm,这个issue是一个功能需求，主要涉及到vLLM版本信息的记录和访问。原因是为了方便运维和版本一致性验证。,https://github.com/vllm-project/vllm/issues/3161
vllm,该issue属于用户提出需求的类型，主要对象是关于使用API端点进行微调的功能。用户提出这个需求是因为希望更易于进行微调操作。,https://github.com/vllm-project/vllm/issues/3159
vllm,这是一个功能需求提议，主要对象是支持Mistral模型推断。,https://github.com/vllm-project/vllm/issues/3153
vllm,这是一个用户提出需求的issue，主要涉及的对象是OpenAI server。这个issue是关于支持response_format为json_object的功能需求，目的是让vLLM能够处理任意的JSON格式数据。,https://github.com/vllm-project/vllm/issues/3148
vllm,这个issue属于用户提出需求类型，主要对象是OpenAI completion API，用户提出了添加选项以截断提示标记的功能请求。,https://github.com/vllm-project/vllm/issues/3144
vllm,这个issue是一个需求提出类型，主要涉及vllm中v100对int4的支持情况，用户希望确认这一功能是否真正可行。,https://github.com/vllm-project/vllm/issues/3141
vllm,这个issue是一个功能增强请求，涉及的主要对象是为AMD GPU添加对Punica内核的支持，主要原因是为了在AMD GPU上启用multiLoRA，并解决不同于Nvidia GPU的warp size差异引起的问题。,https://github.com/vllm-project/vllm/issues/3140
vllm,这是一个用户提出需求的类型，该问题单主要涉及到vllm中如何禁用cuda_graph。由于用户可能希望在使用过程中禁用cuda_graph以解决特定问题或者优化性能，因此提出了如何在vllm中禁用cuda_graph的问题。,https://github.com/vllm-project/vllm/issues/3137
vllm,"这是一个功能需求的issue，主要涉及的对象是triton=""2.0.0""版本的prefix_prefill.py，用户提出是否计划实现该版本的prefix_prefill.py。",https://github.com/vllm-project/vllm/issues/3135
vllm,这是一个特性请求（Feature Request）的issue，主要涉及Chunked Prefill功能。由于内存绑定的解码请求与计算绑定的预填充请求之间存在重叠，可以极大地提高系统效率。,https://github.com/vllm-project/vllm/issues/3130
vllm,这是一个更新请求，涉及到版本升级调整，关闭了一个版本发布跟踪任务。,https://github.com/vllm-project/vllm/issues/3129
vllm,这是一个用户提出需求的issue，主要涉及到使用vLLM支持Hugging Face下载的Mixtral-8x7B-Instruct-v0.1模型的4-bit量化版本，用户希望得到关于如何在vLLM中使用该4-bit版本的帮助。,https://github.com/vllm-project/vllm/issues/3128
vllm,这个issue是用户提出需求，希望将vllm发布到condaforge，并询问是否有计划这样做。,https://github.com/vllm-project/vllm/issues/3126
vllm,这是一个功能需求类型的issue，主要涉及到在离线环境中使用预缓存文件加载模型的功能。问题提出需要增加`localfilesonly`标志支持加载本地缓存文件。,https://github.com/vllm-project/vllm/issues/3125
vllm,这个issue属于一个新功能需求，主要涉及实现`min_tokens`抽样参数，解决了相关PR无法继续的问题。,https://github.com/vllm-project/vllm/issues/3124
vllm,这个issue属于需求提出类型，主要涉及cupy在ROCm backend中的启用问题，由于需要在cudagraph模式下成功运行吞吐量基准测试脚本，因此需要启用cupy。,https://github.com/vllm-project/vllm/issues/3123
vllm,这是一个功能需求的issue，主要涉及vLLM中添加Sarathi-Serve支持，以提高吞吐量和降低延迟。,https://github.com/vllm-project/vllm/issues/3121
vllm,这是一个功能需求类型的issue，主要对象是VLLM模型。由于缺乏直接测量模型权重内存使用的功能，用户提出了添加这一功能的建议。,https://github.com/vllm-project/vllm/issues/3120
vllm,这个issue是用户提出需求，并询问使用vllm进行离线推理时是否应该使用tokenizer模板，由于文档不明确，用户需要澄清在离线推理时是否需要应用tokenizer模板。,https://github.com/vllm-project/vllm/issues/3119
vllm,这个issue类型是需求提出，主要涉及的对象是vllm这个项目。源自用户对ORCA批量优化功能的需求。,https://github.com/vllm-project/vllm/issues/3110
vllm,这是一个用户提出需求的类型，涉及日志级别设置功能，用户希望能够通过命令行参数选择日志级别而不是固定的'info'，问题可能源于用户希望在启动API服务器时能够更灵活地控制日志输出级别。,https://github.com/vllm-project/vllm/issues/3109
vllm,"这是一个提出需求的issue，涉及的主要对象是""Chunked Prefill""功能。由于还在进行中（Work in Progress），用户可能正在尝试实现或者讨论关于这个功能的改进，具体表现为为了实现这个功能而需要完成的工作或线索等。",https://github.com/vllm-project/vllm/issues/3106
vllm,这是一个功能需求的issue，主要涉及到kv缓存模块的重构，使得开发者可以更容易地实验和自定义kv缓存变化。,https://github.com/vllm-project/vllm/issues/3105
vllm,这是一个需求类型的issue，主要涉及的对象是GPU运行相关的代码文件。由于之前硬编码了关于cuda的API调用，导致无法支持更多设备，因此提出提供接口以隐藏cuda API调用来更友好地支持新设备。,https://github.com/vllm-project/vllm/issues/3102
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是缓存配置信息的展示。由于用户希望能通过Prometheus指标获取缓存配置信息并在Grafana中展示，所以提出了这个需求。,https://github.com/vllm-project/vllm/issues/3100
vllm,这个issue是一个需求提出类型的问题，涉及主要对象是软件中的新功能和性能优化，用户请求添加了新的功能支持和性能优化，并要求在特定日期发布。,https://github.com/vllm-project/vllm/issues/3097
vllm,这是一个功能需求类型的issue，主要涉及添加批处理的RoPE内核，由于目前需要为每个LoRA请求调用旋转嵌入内核，导致无法有效地为多个具有不同上下文长度的LoRA提供服务。,https://github.com/vllm-project/vllm/issues/3095
vllm,该issue属于特性请求类型，主要涉及到许可证更新。由于原始仓库的LICENSE标头被继承到了分支中，需要帮助将metadata从Apache更新为Neural Magic Community License。,https://github.com/vllm-project/vllm/issues/3093
vllm,这是一个用户提出需求的issue，主要涉及对VLLM添加AQLM支持。原因可能是用户希望在项目中使用AQLM，并希望VLLM能够支持该功能。,https://github.com/vllm-project/vllm/issues/3091
vllm,这是一个用户提出需求的issue，主要涉及对象是VLLM库的模型加载功能。这个issue由于用户希望扩展VLLM库支持从S3位置加载模型而非本地路径而产生。,https://github.com/vllm-project/vllm/issues/3090
vllm,这是一个用户提出需求的issue，主要涉及支持starcoder2架构，用户希望检查代码是否存在bug。,https://github.com/vllm-project/vllm/issues/3089
vllm,这个issue是一个用户提出需求的类型，主要涉及的对象是支持新架构StarCoder2，发起者询问是否有专家支持该新架构，因为他们想要为vLLM实现这个功能。,https://github.com/vllm-project/vllm/issues/3075
vllm,这是一个用户提出需求的类型，此问题涉及添加dstack上的另一个服务选项。,https://github.com/vllm-project/vllm/issues/3074
vllm,这是一个功能改进的issue，主要涉及的对象是vllm下的LLMEngine，通过将logprob detokenization逻辑迁移到LLMEngine来实现目的。,https://github.com/vllm-project/vllm/issues/3065
vllm,这是一个关于减少Docker镜像中漏洞的建议类型的问题单，主要对象是关于vLLM项目的Docker镜像，问题源于依赖完整的Ubuntu基础镜像导致镜像中存在大量漏洞，用户希望找到/创建一个不依赖Ubuntu且可支持CUDA的镜像或通过其他方式减少漏洞。,https://github.com/vllm-project/vllm/issues/3062
vllm,这个issue类型是用户提出需求，主要涉及添加llmperf benchmark到现有的benchmarks中，原因是llmperf benchmark在blogs中越来越受欢迎，提出者认为应该将其加入。,https://github.com/vllm-project/vllm/issues/3060
vllm,这是一个用户提出需求的问题，主要涉及的对象是使用vllm部署的openai服务器。用户想知道是否可以在服务推理结束之前发送终止信号，以节省GPU资源。,https://github.com/vllm-project/vllm/issues/3059
vllm,这个issue属于用户提出需求类型，主要涉及请求优先级机制的缺失。这个问题可能是由于没有找到优先级队列来处理请求导致用户无法获得对一些请求更快的响应。,https://github.com/vllm-project/vllm/issues/3058
vllm,该issue是一个用户提出需求类型的问题，主要涉及的对象是qwen2。由于缺乏对Lora的支持而导致用户提交该需求。,https://github.com/vllm-project/vllm/issues/3054
vllm,这个issue类型是询问需求或者功能支持，涉及主要对象是VLLM framework是否可以支持华为的910B芯片，用户可能由于希望使用华为的910B芯片而询问VLLM framework是否能够支持。,https://github.com/vllm-project/vllm/issues/3052
vllm,这是一个用户提出需求的issue，主要涉及对象为Gemma，由于目前不支持LoRA，用户希望添加LoRA支持。,https://github.com/vllm-project/vllm/issues/3050
vllm,这是一个建议性issue，主要涉及的对象是`gather_cached_kv` kernel，由于该kernel目前未被使用，提议移除。,https://github.com/vllm-project/vllm/issues/3043
vllm,这个issue类型是用户提出需求，主要对象是vLLM的视觉语言模型支持。由于需求增加了视觉语言支持，需要进行API更改以支持视觉输入。,https://github.com/vllm-project/vllm/issues/3042
vllm,这个issue属于技术改进类型，主要涉及到移除未使用的配置文件，并使用`transformers.PretrainedConfig`代替。可能由于需要优化代码结构或者减少不必要的文件而提出。,https://github.com/vllm-project/vllm/issues/3039
vllm,这是一个用户提出需求的issue，主要涉及vllm在k8s pod中加载模型时无法指定最大缓存内存使用，导致内存占用问题。,https://github.com/vllm-project/vllm/issues/3035
vllm,这是一个用户提出需求的类型。该问题单主要涉及的对象是将hf_transfer添加到requirements.txt中。用户提出需求是为了能够通过设置'HF_HUB_ENABLE_HF_TRANSFER=True'来大幅提高从HF下载权重文件的速度。,https://github.com/vllm-project/vllm/issues/3031
vllm,该issue类型为需求提出，主要涉及的对象是在vLLM中引入了一种名为speculative decoding的新特性。由于开发者希望实现该特性并对现有的尝试进行比较，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/3029
vllm,该issue类型为功能需求，主要涉及对象是OpenAI API的completions/chat completions，用户提出了关于实现logit bias的需求。,https://github.com/vllm-project/vllm/issues/3027
vllm,这是一个用户请求帮助的issue， 主要涉及multi round decoding问题。由于需要在连续的多个步骤中保留之前的信息，用户询问如何在代码中实现使用所有之前的信息。,https://github.com/vllm-project/vllm/issues/3021
vllm,这是一个用户提出需求的问题，主要涉及vllm下的alltoall通信集合的实现方式，由于用户可能需要在vllm中实现alltoall通信集合，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/3020
vllm,这个issue是一个改进建议，主要涉及到输出响应类的优化。,https://github.com/vllm-project/vllm/issues/3017
vllm,这是一个需求类型的issue，涉及的主要对象为Engine。原因是为生产用例添加健康检查以检测引擎故障，并使异步引擎更加健壮。,https://github.com/vllm-project/vllm/issues/3015
vllm,这是一个功能需求类型的issue，主要涉及vLLM中的torch.compile()支持，作者希望通过此功能对模型进行编译以提高性能。,https://github.com/vllm-project/vllm/issues/3014
vllm,这个issue是一个用户需求报告，主要涉及到GPU使用的指定问题。用户在VLLM代码中无法明确指定不同模型使用的GPU，导致无法实现不同模型在不同GPU上的运行。,https://github.com/vllm-project/vllm/issues/3012
vllm,这个issue是一个用户提出需求的类型，主要对象是GPTBigCodeForCausalLM，用户希望添加LoRA支持，因为他们已经为每种语言微调了适配器。,https://github.com/vllm-project/vllm/issues/3011
vllm,这是一个关于性能优化的issue，主要涉及到flashattention在v2.5.0版本中引入的新特性及与xformers相比的性能表现差异。原因是flashattention的kernel在某些情况下表现优于vllm的paged attention kernels。,https://github.com/vllm-project/vllm/issues/3010
vllm,该issue属于功能改进类型，主要涉及到对激活层进行性能优化，使用者提出了关于激活函数的比较和优化建议。,https://github.com/vllm-project/vllm/issues/3009
vllm,这是一个用户提出的需求类型的issue，主要涉及的对象是Docker openai模板。用户提出了添加pip安装hf_transfer到Docker模板的建议，以提高权重下载速度的需求。,https://github.com/vllm-project/vllm/issues/3008
vllm,这个issue类型为功能需求，主要涉及的对象是VLLM项目中的prefix prefill kernels模块。由于用户需要支持GQA数据集，因此提出了使prefix prefill kernels模块支持GQA的需求。,https://github.com/vllm-project/vllm/issues/3007
vllm,这是一个功能需求的issue，主要涉及的对象是添加代码用于令牌长度测试和延迟测试。,https://github.com/vllm-project/vllm/issues/3006
vllm,这个issue类型为功能增强提案，主要涉及attention层的重构和性能改进，通过分离处理Ampere或更新的NVIDIA GPU与其他GPU的代码路径，简化了前者的代码，同时提升ALiBi模型的性能。,https://github.com/vllm-project/vllm/issues/3005
vllm,这个issue类型为用户提出需求，主要涉及对象是如何在离线情况下使用lora进行批量推理，用户可能因为需要在无网络环境下对数据进行批量推理而提出了这个问题。,https://github.com/vllm-project/vllm/issues/3001
vllm,该issue类型为用户提出需求，主要涉及对象是OpenAI api的文档。这导致了用户感觉文档中关于如何批量处理数据的信息不足。,https://github.com/vllm-project/vllm/issues/2999
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是TRT-LLM backend。原因可能是用户希望在TRTLLM backend中增加benchmark支持。,https://github.com/vllm-project/vllm/issues/2998
vllm,这是一个版本更新请求的issue，涉及到主要对象的更新至0.3.2版本。,https://github.com/vllm-project/vllm/issues/2990
vllm,这个issue是一个功能需求，涉及到修改metrics名称中的model_name字段，以方便grafana管理员管理。原因是model_name包含目录字符，不便于管理。,https://github.com/vllm-project/vllm/issues/2987
vllm,这是一个优化型issue，主要涉及替代优化了fused MoE Kernel，由于更积极的参数搜索，实现了胜过TensorRT kernels的表现，用户提出了性能优化的需求。,https://github.com/vllm-project/vllm/issues/2979
vllm,这是一个功能需求，主要涉及到在文本生成过程中识别停止条件的问题。由于当前无法确定文本生成停止的具体原因，用户提出添加`stop_reason`字段以指示匹配的停止标识符。,https://github.com/vllm-project/vllm/issues/2976
vllm,该issue是一个优化性质的问题，涉及主要对象是Gemini中的GeGLU层。,https://github.com/vllm-project/vllm/issues/2975
vllm,这是一个优化建议的issue，主要涉及Gemam的RMSNorm优化。,https://github.com/vllm-project/vllm/issues/2974
vllm,这个issue类型为用户提出需求，关注点是vLLM是否利用了transformers中关于静态缓存和torch编译的新速度优化。,https://github.com/vllm-project/vllm/issues/2969
vllm,这是一个需求类型的issue，主要涉及版本更新和新增模型支持的功能性改进。,https://github.com/vllm-project/vllm/issues/2968
vllm,这个issue类型为需求添加，主要涉及的对象是Gemini模型文档。该需求是为了将Gemma模型添加到文档中，因此是为了完善文档内容而提出的。,https://github.com/vllm-project/vllm/issues/2966
vllm,该issue为升级transformers到v4.38.0版本的操作请求，涉及的主要对象是transformers库。,https://github.com/vllm-project/vllm/issues/2965
vllm,这是一个关于需求的issue，主要涉及追踪生产环境的分支，用户可能由于需要跟踪生产环境的变化而提出这个问题。,https://github.com/vllm-project/vllm/issues/2962
vllm,该issue属于用户提出需求的类型，主要涉及的对象是VLLM项目中的模型支持扩展，由于Gem模型架构的不同而需要对其进行适配。,https://github.com/vllm-project/vllm/issues/2960
vllm,这是一个关于功能需求的issue，主要涉及VLLM中的mistral模型，用户提出了关于添加embedding支持的建议。,https://github.com/vllm-project/vllm/issues/2936
vllm,该issue为用户提出需求，问题涉及到如何在离线推理时获取指标（Metrics）信息，由于目前无法通过公开API直接获取这些指标信息，用户尝试使用内部方法来获取，但并没有成功。,https://github.com/vllm-project/vllm/issues/2935
vllm,这个issue是一个优化需求，涉及的主要对象是vllm的部署效果与原始数据之间的一致性。由于vllm部署的结果与原始数据不一致，用户希望尽快优化qwen14B的集成效果。,https://github.com/vllm-project/vllm/issues/2927
vllm,这个issue是用户提出的需求，主要涉及到VLLM模型中的stop words处理方式。由于当前只支持单个stop words，用户希望支持多个stop words，并提出了具体的更新请求。,https://github.com/vllm-project/vllm/issues/2926
vllm,这是一个关于功能需求的issue，主要涉及到VLLM模型在处理多次输入时内存残留的问题，用户希望了解如何清除模型的缓存或历史记录。,https://github.com/vllm-project/vllm/issues/2925
vllm,这个issue类型是用户提出需求，主要涉及的对象是如何指定本地模型，由于用户不清楚如何使用命令下载本地模型而询问如何指定本地模型。,https://github.com/vllm-project/vllm/issues/2924
vllm,这个issue是用户提出的需求，主要涉及VLLM引擎的初始化方法，通过添加`tokenizer_init_kwargs`参数来改进。,https://github.com/vllm-project/vllm/issues/2923
vllm,这个issue是关于功能请求的，主要对象是VLLM，用户提出了集成QUICK核心进行AWQ量化的需求。,https://github.com/vllm-project/vllm/issues/2920
vllm,这是一个用户提出需求的issue，主要涉及支持在vLLM上使用Smaug-72B-v0.1模型。,https://github.com/vllm-project/vllm/issues/2917
vllm,这是一个用户提出需求的issue，主要涉及如何在VLLM中动态调用/添加新的Lora模块到live server。用户想知道是否可以在不重启服务器的情况下实时调用最近添加的Lora模块。,https://github.com/vllm-project/vllm/issues/2916
vllm,这个issue是请求将标签0.3.1添加到docker镜像vllm/vllmopenai的需求类型。,https://github.com/vllm-project/vllm/issues/2914
vllm,这是一个优化型问题，涉及的主要对象是MoE（Mixture of Experts）内核。原因是由于TensorRT MoE内核在小批量大小范围内表现良好，而融合 MoE 内核在大批量大小范围内表现更好，因此希望能够创建一个统一的内核以覆盖所有情况。,https://github.com/vllm-project/vllm/issues/2913
vllm,这是一个用户提出的功能请求，主要涉及LangChain集成LoRA支持。用户希望在GPU内存不足的情况下利用vLLM，并提到了QLoRA的集成。,https://github.com/vllm-project/vllm/issues/2911
vllm,这是一个用户提出需求的issue，主要涉及vLLM模型生成结果的确定性选项。用户想要知道是否有一个选项或标志可以确保在相同的提示下，不同运行的结果是相同的，而不是随机的。,https://github.com/vllm-project/vllm/issues/2910
vllm,这是一个功能需求类型的Issue，主要涉及测试新增功能的需求。由于缺乏基础正确性测试，需要添加一个简单的测试以确保主干通过基本正确性测试。,https://github.com/vllm-project/vllm/issues/2908
vllm,这是一个用户提出需求的issue，主要涉及的对象是增强`from huggingface_hub import snapshot_download`的功能。,https://github.com/vllm-project/vllm/issues/2907
vllm,这个issue类型是代码优化类型，涉及vllm下的worker初始化逻辑，用户提出需要让分布式初始化逻辑看起来更清晰。,https://github.com/vllm-project/vllm/issues/2905
vllm,这是一个用户提出需求的类型，主要涉及的对象是VLLM项目。由于用户需要支持千问的图片对话模型，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/2903
vllm,这是一个关于需求的问题，用户想了解vl模型是否支持lora后的模型以及是否可以支持部署。,https://github.com/vllm-project/vllm/issues/2902
vllm,这是一个用户提出需求的问题，主要涉及vLLM对请求进行排队时的控制机制，用户想要了解如何控制当队列过长时vLLM是否会拒绝请求。,https://github.com/vllm-project/vllm/issues/2901
vllm,这是一个用户提出需求的issue，主要涉及到将ray作为单节点部署的可选项，提出了对于实时同步推理需求的轻量级选项需求。,https://github.com/vllm-project/vllm/issues/2898
vllm,这是一个关于功能需求的问题，主要涉及VLLM模型中迭代prompt tokens的过程。用户不清楚模型是如何从输入序列生成tokens，可能是因为现有函数主要处理已生成的输出导致。,https://github.com/vllm-project/vllm/issues/2897
vllm,这是一个用户提出需求的issue，主要涉及到对于稀疏层添加偏置支持的改进请求。,https://github.com/vllm-project/vllm/issues/2896
vllm,这个issue类型是功能需求，主要涉及添加docker-compose.yml文件和对应的.env文件。由于vLLM缺少Composefile，用户提供了一个可用的基础版本以供使用和进一步开发。,https://github.com/vllm-project/vllm/issues/2895
vllm,这个issue属于用户提出需求类型，主要涉及模型在多个名称下提供服务，用户希望能够方便地更新模型名称而不影响用户配置。,https://github.com/vllm-project/vllm/issues/2894
vllm,这是一个用户提出需求的issue，主要涉及Vllm Lora是否支持SGMV，用户希望社区是否有计划支持SGMV以提升灵活性。,https://github.com/vllm-project/vllm/issues/2893
vllm,该issue属于用户提出需求类型，主要涉及Hugging Face Hub中的模型代码下载功能，用户希望新增`coderevision`参数以指定特定版本的模型代码，在之前默认下载最新版本可能导致不一致的情况。,https://github.com/vllm-project/vllm/issues/2892
vllm,该issue属于用户提出需求类型，主要涉及的对象是对vLLM集成XLoRA的需求。用户提出了需要在vLLM中实现XLoRA所需的2 KV缓存的功能，由于XLoRA采用双向前向传播方法，因此需要增加此功能。,https://github.com/vllm-project/vllm/issues/2890
vllm,该issue是一个功能需求的报告，主要涉及AI Controller Interface (AICI)的集成，提出了一些功能实现的支持，并就部分功能的实现方式提出了疑问。,https://github.com/vllm-project/vllm/issues/2888
vllm,这是一个关于软件版本更新的issue，主要对象是v0.3.1版本，由于需要发行新版本或修复问题导致的更新。,https://github.com/vllm-project/vllm/issues/2887
vllm,这是一个用户提出需求的issue，主要涉及到vLLM中异步标记化的问题。 由于同步标记化导致长提示的标记化阻塞事件循环，造成在高QPS场景下标记生成和请求处理变慢。,https://github.com/vllm-project/vllm/issues/2879
vllm,这是一个用户提出的需求，关于让vLLM日志格式化变更可选的问题，主要涉及日志记录功能。这个问题的原因是为了更好地与现有系统集成，允许用户选择是否要应用vLLM的日志格式化更改。,https://github.com/vllm-project/vllm/issues/2877
vllm,这是一个功能需求的issue，主要涉及的对象是RequestOutput类。由于缺乏额外的指标，用户希望为RequestOutput添加一些用于第三方日志/指标记录的额外指标。,https://github.com/vllm-project/vllm/issues/2876
vllm,这是一个性能优化的issue，主要涉及AWQ gemm kernel的更新和速度提升，用户可能在寻求提升模型性能和验证长序列长度的解决方案。,https://github.com/vllm-project/vllm/issues/2874
vllm,这是一个用户提出需求的issue，主要涉及的对象是Marlin downstream PR。由于用户希望提交Marlin代码的变更，导致了这个issue的产生。,https://github.com/vllm-project/vllm/issues/2872
vllm,这是一个用户提出的需求问题，主要涉及VLLM是否支持HalfQuadratic Quantization (HQQ)。由于部分模型已使用HQQ，因此用户希望VLLM也能支持该功能。,https://github.com/vllm-project/vllm/issues/2871
vllm,这是一个用户的需求报告，主要涉及如何基于huggingface模型类RagTokenForGeneration添加一个模型执行列表中的模型。用户想了解如何在这种情况下添加新模型。,https://github.com/vllm-project/vllm/issues/2866
vllm,这是一个需求类型的issue，涉及Sparse fused gemm integration功能。,https://github.com/vllm-project/vllm/issues/2862
vllm,这是一个功能改进类的issue，涉及的主要对象是将InternLMForCausalLM迁移到LlamaForCausalLM。,https://github.com/vllm-project/vllm/issues/2860
vllm,这是一个功能改进的issue，涉及的主要对象是替换Yi模型定义为使用`LlamaForCausalLM`，原因是为了避免代码重复和确保Yi模型继承所有为llama做的修复，如LoRA支持。,https://github.com/vllm-project/vllm/issues/2854
vllm,该issue类型为功能需求，主要涉及GPU内存达到容量时数据未能自动转移至CPU内存，用户询问了CPU KV cache的触发条件及如何调用CPU KV cache进行数据交换。,https://github.com/vllm-project/vllm/issues/2853
vllm,"这是一个功能提议的issue，主要涉及到vllm项目中的""Usage Stats Collection""。",https://github.com/vllm-project/vllm/issues/2852
vllm,这是一个用户提出需求的类型的issue，主要涉及到实现Telemetry Support功能。由于需要收集信息并发送至服务器，可能是为了跟踪使用数据或优化性能。,https://github.com/vllm-project/vllm/issues/2846
vllm,这是一个关于改进性能的问题，涉及主要对象是CI系统，由于需要确保模型输出与Huggingface版本完全匹配，因此对测试进行了快照基础的实现。,https://github.com/vllm-project/vllm/issues/2844
vllm,这是一个功能需求类型的issue，涉及主要对象是模型参数的打包和解包。,https://github.com/vllm-project/vllm/issues/2843
vllm,这个issue类型是功能增强，主要涉及文档构建功能，由于需要确保文档能够在CI中成功构建，因此这个功能被新增。,https://github.com/vllm-project/vllm/issues/2842
vllm,这是一个升级请求，涉及主要对象是PyTorch库，由于升级导致可能会破坏AMD GPU的支持，需要更新docker文件和xformers patch以修复这一问题。,https://github.com/vllm-project/vllm/issues/2838
vllm,这是一个用户提出需求的issue，主要涉及如何使用模型和tokenizer创建vllm.LLM()对象。问题出现的原因是vllm当前不支持bitsandbytes量化方法。,https://github.com/vllm-project/vllm/issues/2836
vllm,该issue属于用户提出需求类型，主要涉及支持Google的TPU硬件。原因可能是用户对VLLM软件在2024Q1的路线图中添加对TPU硬件的支持感兴趣并想了解该问题的最新进展。,https://github.com/vllm-project/vllm/issues/2835
vllm,这个issue是用户提出需求的类型，涉及主要对象是文档和LoRA适配器。用户指出LoRA适配器目前不在文档中，因此他们撰写了关于LoRA的文档部分。,https://github.com/vllm-project/vllm/issues/2834
vllm,这是一个功能增强类的issue，主要涉及添加对OLMo模型的支持。,https://github.com/vllm-project/vllm/issues/2832
vllm,这是用户提出的需求。该问题单涉及的主要对象是为Mixtral添加LoRA支持。原因是当前的软件版本没有LoRA配置，导致用户无法在Mixtral上使用LoRA功能。,https://github.com/vllm-project/vllm/issues/2831
vllm,这是用户提出的功能需求，主要涉及向 vLLM CLI 中添加 `vllm serve modelname` 命令，以改进接口提供更好的访问 OpenAI API 服务。,https://github.com/vllm-project/vllm/issues/2822
vllm,这是一个用户提出需求的issue，主要涉及LoRA在量化模型中的支持问题。由于无法与4/8位、awq等格式兼容，用户希望能够在此情况下使用LoRA。,https://github.com/vllm-project/vllm/issues/2821
vllm,这个issue是关于代码拼写检查的改进，属于代码优化类别，主要对象是vllm项目。,https://github.com/vllm-project/vllm/issues/2820
vllm,这是一个提出需求的issue，主要对象是为OpenAI API server添加引导解码（guided decoding）功能。,https://github.com/vllm-project/vllm/issues/2819
vllm,这个issue是一个功能需求，涉及对象是添加对gunicorn多进程处理的支持，用户寻求帮助解决相关问题。,https://github.com/vllm-project/vllm/issues/2818
vllm,这是一个用户提出需求并寻求帮助的issue，主要涉及如何在VLLM Multi-Lora中处理lm_head和embed_tokens模块，可能由于adapter_weights中的lora设置问题而导致。,https://github.com/vllm-project/vllm/issues/2816
vllm,这个issue属于功能需求类型，主要涉及的对象是vLLM的API服务。原因是用户提出了对指导解码（guided decoding）功能的支持，以解决由于支持约束解码而引起的问题。,https://github.com/vllm-project/vllm/issues/2815
vllm,这是一个需求添加的Issue，主要涉及vLLM项目中的Splitwise实现。由于需要实现Splitwise中的并行化处理逻辑，引入了MSCCL++通信库以提高KVcache传输速度。,https://github.com/vllm-project/vllm/issues/2809
vllm,这个issue类型是更新请求，涉及主要对象是升级torch版本。原因是希望更新到`torch==2.2.1`，但由于还没有发布`torch==2.2`容器，导致无法更新ROCM。,https://github.com/vllm-project/vllm/issues/2804
vllm,这是一个用户需求问题，主要涉及AutoQuant quantization model for INT8/INT4 inference，由于INT4 runs faster than AWQ and FP16，用户可能希望了解更多关于AutoQuant的性能和特性。,https://github.com/vllm-project/vllm/issues/2801
vllm,这是一个性能优化相关的issue，提出了关于如何进行增量构建的文档需求，旨在减少编译时间，提高开发迭代速度。,https://github.com/vllm-project/vllm/issues/2796
vllm,这是一个用户提出需求的问题，主要涉及 VLLM 下的改进Speculative decoding。用户提出了关于添加Eagle、Medusa、Look Ahead decoding等新的改进以提高VLLM性能的建议。,https://github.com/vllm-project/vllm/issues/2791
vllm,这是一个关于功能特性确认的问题，主要涉及到需要确认multilora的工作情况。用户提出了slora功能是否正常工作以及如何跟踪lora推断的疑问。,https://github.com/vllm-project/vllm/issues/2787
vllm,这是一个用户提出需求的issue，主要涉及的对象是支持将logprobs设置为模型词汇大小。由于模型tokenizer和vocab_size的使用方式不一致，导致无法轻松设置logprobs到模型词汇大小。,https://github.com/vllm-project/vllm/issues/2782
vllm,这是一个用户提出需求的issue，主要涉及到vLLM的Splitwise实现。原因可能是用户想将Splitwise集成到vLLM中以实现特定功能。,https://github.com/vllm-project/vllm/issues/2779
vllm,这是一个用户提出需求的issue，主要涉及vllm项目中支持neuron后端的设置文档，提供了关于为推理准备trn1/inf2实例的指导，需要进行与PR https://github.com/vllmproject/vllm/pull/2569中的更改相关的步骤。,https://github.com/vllm-project/vllm/issues/2777
vllm,该issue为性能优化建议，主要涉及到layernorm kernels的调优。通过使用共享内存来提高性能，特别针对隐藏大小小于4090的情况，可以获得更好的性能表现。,https://github.com/vllm-project/vllm/issues/2776
vllm,这个issue是一个用户提出需求的问题，主要涉及将multiLoRA功能集成到OpenAI服务器中。由于尚未对客户端权限进行划分到特定模型的工作，用户可能无法正确区分多个LoRA模块返回的数值。,https://github.com/vllm-project/vllm/issues/2775
vllm,这是一个用户提出需求的issue，主要涉及到设置本地日志级别的功能。由于默认始终使用`logging.DEBUG`，用户想要通过环境变量`LOG_LEVEL`手动设置日志级别。,https://github.com/vllm-project/vllm/issues/2774
vllm,这是一个性能优化的issue，主要涉及FlashInfer的实现和GQA PagedAttention的改进，由于提高了处理速度，导致了性能指标的提升。,https://github.com/vllm-project/vllm/issues/2772
vllm,这是一个需求类型的Issue，涉及主要对象是为MoE（Mixture of Experts）添加融合的Top-K softmax kernel。由于需要更多MoE相关的kernel和优化性能，开发者提出了两个待办事项。,https://github.com/vllm-project/vllm/issues/2769
vllm,这个issue类型是用户提出需求，主要涉及的对象是vllm对AMD Radeon™ 7900系列GPU（gfx1100）的支持。由于flashattention不完全支持gfx1100，需要使用vllm参考实现，需要通过特定的docker build命令来构建镜像。,https://github.com/vllm-project/vllm/issues/2768
vllm,这是一个用户需求类型的issue，主要涉及vLLM导入FlashInfer中的PagedAttention kernels以支持GQA。原因是FlashInfer在批处理GQA解码注意力方面比vLLM的PagedAttention快3倍。,https://github.com/vllm-project/vllm/issues/2767
vllm,这个issue类型是用户提出需求，涉及的主要对象是关于context (conversation history)在multi-turn conversation中的维护。由于用户希望了解在vLLM中多轮对话时支持的最大context长度以及如何处理此问题，表明用户关注在多轮对话中如何有效地维护和利用对话历史。,https://github.com/vllm-project/vllm/issues/2766
vllm,这是一个用户提出需求的类型，主要对象是添加更多的Prometheus metrics。,https://github.com/vllm-project/vllm/issues/2764
vllm,这个issue类型是用户提出需求，该问题单涉及主要对象是支持新的OLMo模型。由于当前系统尚未支持新的OLMo模型，用户在此提出希望系统能够支持这些新模型的需求。,https://github.com/vllm-project/vllm/issues/2763
vllm,这个issue是关于功能需求的，主要对象是BlockAllocator类，由于需要实现自动前缀缓存功能，导致用户提出了关于实现自动前缀缓存的需求。,https://github.com/vllm-project/vllm/issues/2762
vllm,该issue属于用户提出需求类型，主要涉及的对象是在GPTQ & AWQ Fused MOE中支持增加对quantized fusedMoE的支持以及添加GPTQ group gemm kernels。由于当前代码需要优化以支持这些新特性，因此用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/2761
vllm,这是一个用户提出需求的issue，主要对象是vllm，用户提出了关于是否vllm支持多模态LLM的疑问。,https://github.com/vllm-project/vllm/issues/2757
vllm,这个issue属于功能需求提出类型，主要涉及了vLLM在Intel CPU和GPU设备上的集成。由于需要逐步将代码从CUDA设备转移到非CUDA设备，用户希望对vLLM进行改动以支持Intel GPU/CPU设备的运行。,https://github.com/vllm-project/vllm/issues/2753
vllm,这是一个用户提出的需求问题，主要涉及的对象是openai server，用户希望支持early_stopping sampling参数。,https://github.com/vllm-project/vllm/issues/2751
vllm,这是一个性能改进的issue，主要涉及支持MQA/GQA在解码阶段的优化，并且主要针对小批量大小的情况。,https://github.com/vllm-project/vllm/issues/2744
vllm,这是一个需求类型的issue，主要涉及VLLM项目中解码阶段使用flash attention的需求。,https://github.com/vllm-project/vllm/issues/2743
vllm,这个issue属于功能需求类型，涉及主要对象为LLMEngine内部的原始分词器。这个问题是由于LLMEngine使用TokenizerGroup，导致在LLM中的get_tokenizer和set_tokenizer方法需要调整，以操作LLMEngine中的原始分词器而提出的。,https://github.com/vllm-project/vllm/issues/2741
vllm,这是关于软件版本更新的问题，主要涉及的对象是pytorch库。这个问题由于v2.1.2版本中存在的内存溢出bug导致，用户在询问是否会更新到解决了这一bug的新版本v2.2.0。,https://github.com/vllm-project/vllm/issues/2738
vllm,这是一个用户提出需求的issue，主要涉及vllm scheduler中的prompt数量限制。由于prompt限制，用户无法提供足够的提示来进行对话。,https://github.com/vllm-project/vllm/issues/2737
vllm,这是一个文档更新类型的issue，主要涉及到VLLM的langchain服务说明文档可能需要更新。可能是由于最新版本的变化或者错误导致用户需要更新相关说明文档。,https://github.com/vllm-project/vllm/issues/2736
vllm,这个issue属于功能需求类型，主要涉及将`aioprometheus`的指标迁移到`prometheus_client`，由于`prometheus_client`更受关注且更活跃维护，因此需要进行更迁换。,https://github.com/vllm-project/vllm/issues/2730
vllm,这是一个需求报告，涉及到vllm中的Choronz patch，用户希望更新xformers到版本0.0.24。,https://github.com/vllm-project/vllm/issues/2726
vllm,这是一个关于用户提出需求的类型，主要涉及的对象是支持vision/audio模型的计划。由于缺乏支持，用户询问是否有计划支持vision/audio模型像QwenVLChat和QwenAudioChat这样的模型。,https://github.com/vllm-project/vllm/issues/2724
vllm,这是一个需求提出类型的issue，主要涉及gemm kernels的重构操作。由于代码重复导致功能冗余，需要合并两个函数并添加参数以提高效率。,https://github.com/vllm-project/vllm/issues/2723
vllm,该issue类型为用户提出需求，请求支持。主要对象是 Torch2.2 。由于用户需要在该版本中获得支持，因此提出了此问题。,https://github.com/vllm-project/vllm/issues/2722
vllm,这是一个关于更新README的issue，类型为用户提出需求。主要对象是meetup slides。,https://github.com/vllm-project/vllm/issues/2718
vllm,这是一个用户提出需求的类型，主要对象是在 vllm 中运行 openai 服务器时如何在 Detach 模式下运行。,https://github.com/vllm-project/vllm/issues/2715
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm，可能是由于功能缺失或者技术限制导致无法加载lora模块，用户希望得到关于支持lora加载的帮助。,https://github.com/vllm-project/vllm/issues/2710
vllm,这是一个用户提出需求的issue，主要涉及如何获取句子概率而不需要生成句子，存在速度较慢的问题。,https://github.com/vllm-project/vllm/issues/2701
vllm,这是一个用户提出的需求，涉及主要对象是代码库中的SamplingParams类。原因是当前代码仅支持单词作为停止词，用户提出支持多词停止词的功能。,https://github.com/vllm-project/vllm/issues/2698
vllm,这是一个用户提出需求的issue，主要涉及vLLM的batch inference在Ray分布式环境下的运行示例。,https://github.com/vllm-project/vllm/issues/2696
vllm,这是一个需求提出的issue，主要涉及vLLM在Q1 2024的路线图，用户提出了相关特性的讨论和贡献。,https://github.com/vllm-project/vllm/issues/2681
vllm,这个issue属于用户提出需求类型，主要对象是vllm所实现的kernel。由于目前vllm仅支持在cuda设备上运行kernel，而用户希望添加设备调度以支持其他类型设备，如cpu或xpu。,https://github.com/vllm-project/vllm/issues/2678
vllm,这个issue是一个需求类型的问题，主要涉及的对象是在vLLM中添加Mixtral MoE层的单元测试。这个问题的提出是为了增加对Mixtral MoE层的单元测试覆盖。,https://github.com/vllm-project/vllm/issues/2677
vllm,这是一个用户提出需求的issue，主要涉及支持Web Dashboard的功能需求。用户希望能够同时部署Web Dashboard作为vLLM推理引擎，以便更轻松地观察、诊断和管理推理系统。,https://github.com/vllm-project/vllm/issues/2662
vllm,这是一个用户提出需求的issue，主要涉及vLLM的代码合并问题。由于用户需要将qwen-72b-chat-int4的支持代码合并到主分支，进而使用QwenLM大型语言模型的模型推理。,https://github.com/vllm-project/vllm/issues/2660
vllm,这是一个关于功能询问的issue，涉及VLLM对LLM输出是否产生影响，症结在于加速框架是否会影响最终结果的准确度。,https://github.com/vllm-project/vllm/issues/2657
vllm,该issue类型为版本更新，涉及更新新功能和文档。,https://github.com/vllm-project/vllm/issues/2656
vllm,这个issue类型是需求提出，主要涉及编译构建系统，用户寻求帮助寻找更适合的构建工具优化当前系统的编译过程。,https://github.com/vllm-project/vllm/issues/2654
vllm,这是一个用户提出需求的issue，主要涉及对vllm的Prometheus指标添加相关提案。由于已有的计数器类型指标不够具有意义，用户希望增加Histogram类型的指标。,https://github.com/vllm-project/vllm/issues/2650
vllm,这个issue是关于用户提出需求的，主要涉及到如何从Mistral / Mixtral获取`logprobs`和softmax概率的问题，用户想要获得模型输出的原始分数和概率。,https://github.com/vllm-project/vllm/issues/2649
vllm,这个issue是用户提出需求，主要对象是项目的基准测试，希望能够支持对填充和解码阶段的数据进行单独测试。这个需求可能由于项目需要更精确地评估不同阶段的性能表现而提出。,https://github.com/vllm-project/vllm/issues/2647
vllm,这是一个关于功能需求的Issue，主要涉及的对象是PR（Pull Request），用户提出了是否接受将ExLlamaV2 kernels与AWQ集成的请求。由于ExLlamaV2 kernels的性能优化，用户希望在解码过程中获得更高的运行速度。,https://github.com/vllm-project/vllm/issues/2645
vllm,这个issue类型为用户提出需求，主要涉及对象是在集成vLLM中的API服务器以便lm-evaluation-harness更轻松。这个问题主要是由于需要在不同网络环境下使用vLLM的tokenizer功能，但目前该功能尚未支持，因此用户提出了添加`/get_tokenizer`端点到`api_server`的建议。,https://github.com/vllm-project/vllm/issues/2643
vllm,这个issue类型是用户提出需求，涉及到更新runbenchmark.sh文件，主要是为了添加--kv-cache-dtype选项支持fp8_e5m2数据类型。,https://github.com/vllm-project/vllm/issues/2639
vllm,这是一个关于需求的问题，主要涉及的对象是vLLM模型是否支持卷积层。用户可能想了解vLLM模型是否支持卷积层以及如何在模型中使用该功能。,https://github.com/vllm-project/vllm/issues/2638
vllm,该issue属于需求提出类型，主要涉及优化llama家族模型的重构。由于存在重复的代码逻辑，导致需要对llama家族模型进行优化以减少冗余代码。,https://github.com/vllm-project/vllm/issues/2637
vllm,这个issue属于用户提出需求类型，主要涉及添加对Pascal GPU的计算能力6.x的支持。原因是由于vLLM的限制，用户无法使用旧架构的GPU进行计算。,https://github.com/vllm-project/vllm/issues/2635
vllm,这是一个用户提出需求的issue，主要涉及Dockerfile中添加`buildarg punica_kernels=0`选项，希望在构建docker镜像时避免构建Punica内核。,https://github.com/vllm-project/vllm/issues/2633
vllm,该问题类型是性能优化，主要对象是Punica编译过程。由于将Punica内核拆分为单独文件，编译时间从30分钟缩短到4分钟，且默认开启Punica编译，可能是为了提高编译效率。,https://github.com/vllm-project/vllm/issues/2632
vllm,"这是一个用户提出的需求，主要涉及""swap_blocks""的单元测试。由于可能测试覆盖不完整，用户请求批准执行剩余的工作。",https://github.com/vllm-project/vllm/issues/2616
vllm,这是一个用户提出需求的类型，主要涉及VLLM构建过程中torch版本设置的问题。由于当前的构建流程会自动下载并安装指定版本的torch，用户希望了解如何更改所需的torch版本并探究为何VLLM在源代码构建中总是下载torch等包而非使用已安装的版本。,https://github.com/vllm-project/vllm/issues/2615
vllm,这个issue类型是提出需求，主要对象是vLLM中的KV cache management。由于需要实现自动前缀缓存，提出了管理KV块的新方案，以及与KV块Trie相比的设计优势。,https://github.com/vllm-project/vllm/issues/2614
vllm,这是一个用户提出需求的问题，主要涉及如何获取生成的文本中第一个token对应的logit值。由于用户希望获取特定token的logit值而不是采样token，所以寻求方法实现这一需求。,https://github.com/vllm-project/vllm/issues/2613
vllm,该issue属于功能需求类型，主要涉及到为OpenAI入口点添加Lora接口。,https://github.com/vllm-project/vllm/issues/2610
vllm,"这是一个功能需求报告，主要涉及了""Speculative Decoding""功能。由于该功能还在实验阶段，并不支持一些特定功能，因此用户可能会遇到一些限制性的问题。",https://github.com/vllm-project/vllm/issues/2607
vllm,这是一个需求类型的issue，主要涉及构建速度较慢的punica kernels，并建议默认情况下不构建它们。,https://github.com/vllm-project/vllm/issues/2605
vllm,该issue类型是需求报告，主要涉及的对象是多LoRA功能的文档内容。,https://github.com/vllm-project/vllm/issues/2603
vllm,这是一个用户提出需求的类型，主要涉及的对象是对多LoRA支持扩展到更多体系结构。原因是目前仅支持Llama和Mistral体系结构，用户希望扩展到Yi、Qwen、Phi和Mixtral等架构。,https://github.com/vllm-project/vllm/issues/2602
vllm,这个issue是一个功能需求提议，主要涉及如何将多个LoRA功能与OpenAI服务器集成。由于目前无法通过vLLM OpenAI服务器查询LoRAs，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/2600
vllm,这是关于修改vllm项目中的vocab_size参数的建议。,https://github.com/vllm-project/vllm/issues/2599
vllm,这是一个用户需求，要求在建模代码中添加来自Stable LM 2的config.qkv_bias。这个需求主要涉及建模代码的修改。,https://github.com/vllm-project/vllm/issues/2598
vllm,这是一个用户提出需求的 issue，主要涉及的对象是 vLLM 的 OpenAI api 服务器，用户希望减少主句的长度以便更容易扩展服务器功能。,https://github.com/vllm-project/vllm/issues/2597
vllm,这个issue类型为用户提出需求，该问题主要涉及AWS Sagemaker Docker，用户提出需求提供一个Docker用于在AWS Sagemaker中部署vLLM，以增加vLLM的使用率。,https://github.com/vllm-project/vllm/issues/2592
vllm,"这是一个用户提出需求的类型，主要对象是""Jais""。由于内容为空，用户可能在提交issue时遗漏了关键信息或者出现了数据丢失，导致无法准确定义具体问题。",https://github.com/vllm-project/vllm/issues/2591
vllm,这是一个关于功能需求的issue，主要涉及vllm的`max_concurrent_workers`参数被禁用了的问题，用户想了解为什么这个参数被禁用了。,https://github.com/vllm-project/vllm/issues/2588
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm模型的内存管理。由于vllm模型的内存是预分配的，用户想知道如何测量在特定序列长度下的最大GPU内存消耗。,https://github.com/vllm-project/vllm/issues/2587
vllm,这是一个用户提出需求的类型的issue，主要涉及内容是如何部署vLLM模型与KServe，可能由于缺乏相关教程或指导而提出。,https://github.com/vllm-project/vllm/issues/2586
vllm,该issue为用户提出需求类型，涉及主要对象为VLLM项目的支持。,https://github.com/vllm-project/vllm/issues/2585
vllm,这是一个功能需求，主要涉及到ROCm项目的编译目标设置。原因是用户希望能够在没有GPU的机器上为不同目标编译，以解决无法在没有GPU的机器上构建docker镜像的问题。,https://github.com/vllm-project/vllm/issues/2581
vllm,这是一个用户提出需求的 issue，主要涉及的对象是vllm加速的chatglm3模型，在输出时不带上prompt只输出结论这一功能。,https://github.com/vllm-project/vllm/issues/2578
vllm,这是一个用户需求类型的issue，主要涉及的对象是internlm2-20b模型。,https://github.com/vllm-project/vllm/issues/2576
vllm,这是一个功能需求类型的issue，主要涉及支持使用transformers-neuronx在Inferentia上进行模型推理。,https://github.com/vllm-project/vllm/issues/2569
vllm,这是一个需求类型的issue，主要涉及lint工具格式化python文件的问题，可能由于部分文件没有使用yapf格式化而导致。,https://github.com/vllm-project/vllm/issues/2567
vllm,该issue类型为性能优化，主要涉及到vLLM中的模型推理速度问题，原因是使用cublas代替`torch.matmul`可能提高推理效率。,https://github.com/vllm-project/vllm/issues/2566
vllm,这是一个功能需求的issue，主要涉及到需要将CC（支持从输入嵌入生成）与主分支的最新更改合并的问题。,https://github.com/vllm-project/vllm/issues/2563
vllm,这是一个用户提出需求的issue，主要涉及的对象是对OpenAI API参数的更新。,https://github.com/vllm-project/vllm/issues/2562
vllm,这是关于新功能请求的issue，主要涉及RadixAttention技术实现的KV缓存重用，以及与持续批处理和分页注意力等现有技术的兼容性。,https://github.com/vllm-project/vllm/issues/2560
vllm,这个issue是关于代码注释的需求，主要涉及调用context_attention_fwd函数的频率问题，由于缺乏文档说明导致用户对代码行为感到困惑。,https://github.com/vllm-project/vllm/issues/2553
vllm,这个issue是关于优化性能，而不是bug报告，主要涉及到AWQ的上下文处理。,https://github.com/vllm-project/vllm/issues/2551
vllm,这是一个用户提出需求的issue，主要涉及vLLM库的KV Cache和获取LogProbs功能。由于用户需要在大量序列上运行推断，并获取它们的log probabilities，他们问及库是否能自动优化KV Cache以重复利用共同前缀的计算。,https://github.com/vllm-project/vllm/issues/2549
vllm,这是一个用户提出需求的issue，主要涉及的对象是针对vLLM的Grammar from Llamacpppython，用户希望能够使用该Grammar与vLLM结合，以便获取所需的响应。,https://github.com/vllm-project/vllm/issues/2548
vllm,该issue是一个用户需求的提出，用户想要在通过openai服务器动态运行模型时通过传递args来覆盖HF配置类中的现有参数，但目前不支持这一功能。,https://github.com/vllm-project/vllm/issues/2547
vllm,该issue为用户提出需求，请求寻找更高性能的服务器来运行DistilWhisper模型。,https://github.com/vllm-project/vllm/issues/2546
vllm,这是一个涉及性能改进的 issue，类型不是 bug 报告。,https://github.com/vllm-project/vllm/issues/2542
vllm,这是一个用户提出需求的issue，主要涉及支持Orion模型的请求。在本次问题中，用户请求支持Orion模型：https://huggingface.co/OrionStarAI/Orion14BBase。,https://github.com/vllm-project/vllm/issues/2539
vllm,这是一个用户提出需求的issue，主要涉及支持Orion模型。由于用户希望支持Orion模型，因此提出了该需求。,https://github.com/vllm-project/vllm/issues/2537
vllm,这是一个用户提出需求的类型的issue，主要涉及添加Orion模型到项目中。由于用户需要在项目中增加Orion模型，所以提出了这个需求。,https://github.com/vllm-project/vllm/issues/2536
vllm,这是一个用户提出需求的类型，主要对象是为 vllm 添加一个 Orion 模型。这个需求可能是为了增加模型选择，并提高模型的多样性。,https://github.com/vllm-project/vllm/issues/2535
vllm,这是一个用户提出需求的issue，主要涉及的对象是希望支持DeepSeekMoE项目。由于该项目目前不支持DeepSeek MoE，用户希望看到该支持的功能。,https://github.com/vllm-project/vllm/issues/2534
vllm,这是一个需求修改的 issue，主要涉及改进基准测试服务。可能是因为服务性能不佳或者需要调整功能实现。,https://github.com/vllm-project/vllm/issues/2532
vllm,这个issue类型为功能需求提议，涉及的主要对象是OpenAI completion protocol。由于缺乏对`prompt`输入参数多样性的支持，用户无法按照需求提供不同形式的输入数据导致此需求被提出。,https://github.com/vllm-project/vllm/issues/2529
vllm,这是一个关于提出需求的RFC（Request For Comments）类型的issue，主要涉及的对象是vLLM上的推理速度，由于现有的技术环境中，大型语言模型（LLMs）在生成人工智能（GenAI）工作负载和模型中越来越受到关注和普及，作者提出了在Intel平台上通过Intel® Extension for PyTorch*来加速vLLM推理的需求。,https://github.com/vllm-project/vllm/issues/2526
vllm,该issue类型为用户提出需求，主要涉及的对象是InternLM2PreTrainedModel。这个问题是用户希望vllm和tp能支持InternLM2PreTrainedModel，因为它需要支持200k的输入和输出来构建一个模型服务。,https://github.com/vllm-project/vllm/issues/2525
vllm,这是一个用户提出需求的issue，主要涉及VLLM中的通信操作的改进，需要添加一个新的参数`group`来指定通信的目标组。 ,https://github.com/vllm-project/vllm/issues/2522
vllm,这是一个用户提出需求的问题，主要对象是Prefix Cache support的使用。用户想要了解如何衡量生成带有和不带有前缀响应之间的性能提升，希望有人提供一个示例来演示性能提升。,https://github.com/vllm-project/vllm/issues/2519
vllm,这是一个用户提出需求的issue，主要涉及服务器的api server，用户期望支持prefix caching功能以处理特定的系统提示字符串。,https://github.com/vllm-project/vllm/issues/2515
vllm,这个issue是关于功能需求的，涉及支持每个请求的种子，主要对象是请求的唯一性标识`request_id`。由于`request_id`唯一性可能存在问题，可能需要分配一个保证唯一的内部id，以解决如何在生成之前设置种子的问题。,https://github.com/vllm-project/vllm/issues/2514
vllm,"这是一个功能改进的issue，主要涉及到""Prefix caching and deallocation mechanism""。由于block.ref_count不适合表示多少个序列共享一个前缀，因此引入了新的seq_ref_counter来处理此问题。",https://github.com/vllm-project/vllm/issues/2511
vllm,这是一个优化建议类型的issue，主要涉及到优化采样算法中的累加运算。该问题由于性能考量，提出了通过使用上三角矩阵相乘加速累加运算的方法。,https://github.com/vllm-project/vllm/issues/2510
vllm,这是一个提出需求的类型，该问题涉及的主要对象是vllm。由于用户对于vllm是否能应用其解码方法感到困惑。,https://github.com/vllm-project/vllm/issues/2508
vllm,这是一个需求提出的issue，主要涉及的对象是benchmark serving。由于下载了完整的sharegpt数据集，但仅发送了20个提示，因此建议考虑使用更小的数据集或生成虚拟输入。,https://github.com/vllm-project/vllm/issues/2505
vllm,"这是一个用户提出需求的issue，涉及主要对象是代码中硬编码的`device=""cuda""`，导致不易支持其他设备的问题。",https://github.com/vllm-project/vllm/issues/2503
vllm,该issue类型为代码优化，主要涉及控制消息的广播逻辑简化。这是因为之前的广播逻辑复杂且混乱，需要进行统一和简化。,https://github.com/vllm-project/vllm/issues/2501
vllm,这是一个需求类型的issue，主要对象是completion api代码。开发者对代码的可读性和维护性提出了改进建议，以便更好地支持批量完成功能。,https://github.com/vllm-project/vllm/issues/2499
vllm,这个issue类型是功能需求，涉及主要对象是集成Marlin内核用于Int4 GPTQ推理，用户提出了关于GPTQ模型运行、性能优化和支持特定功能的需求。,https://github.com/vllm-project/vllm/issues/2497
vllm,该issue属于用户提出需求类型，主要涉及的对象是向vLLM贡献新模型Qwen2。,https://github.com/vllm-project/vllm/issues/2495
vllm,这是一个功能需求类型的issue，主要涉及的对象是vllm代码库，用户提出需要添加对最新transformer更新中引入的prompt_lookup_num_tokens的支持，以提高推断速度。,https://github.com/vllm-project/vllm/issues/2490
vllm,这是一个功能需求的issue，涉及的主要对象是VLLM库中的Sampler类。由于用户希望在文本生成过程中动态限制token，根据每个生成步骤确定允许的token集合，因此提出了实现`allowed_tokens_fn`特性的需求。,https://github.com/vllm-project/vllm/issues/2434
vllm,该issue类型是需求提议，主要涉及vLLM的online serving benchmark脚本的重构和功能增强。原因是为了使脚本更易于使用和贡献，同时增加更多特性和支持。,https://github.com/vllm-project/vllm/issues/2433
vllm,这是一个用户提出需求的issue，涉及主要对象是日志记录功能，由于需要在项目中添加JSON格式的日志记录支持，并希望使用loguru来输出JSON格式的日志。,https://github.com/vllm-project/vllm/issues/2432
vllm,这是一个用户提出需求的类型，主要涉及VLLM项目集成Tree Speculate，并询问是否可能进行集成。由于用户认为Tree Speculate能在加速推理方面提供痛点缓解，因此提出了此功能请求。,https://github.com/vllm-project/vllm/issues/2426
vllm,这个issue类型是功能需求，主要涉及到实现多步骤工作，由于需要多次调用底层模型来进行推断，但需要未来的PR来支持在调度迭代中安排多于一个令牌。,https://github.com/vllm-project/vllm/issues/2424
vllm,这个issue属于用户提出需求类型，主要对象是获取原始的logprobs而不是经过softmax处理的logprobs。由于用户希望获取原始的logprobs数据而不是经过softmax处理后的数据，可能是为了进行个性化的后续处理或分析。,https://github.com/vllm-project/vllm/issues/2421
vllm,这是一个关于构建vllm源码时遇到的问题的用户需求类型的issue，涉及主要对象为在没有GPU的机器上构建vllm并在拥有GPU的机器上运行，原因是因为缺少GPU导致的构建错误。,https://github.com/vllm-project/vllm/issues/2411
vllm,该issue属于用户提出需求类型，主要涉及的对象是MoE architectures中的expert parallel strategy。由于当前实现未能充分利用稀疏激活特性而使用了与密集模型相同的计算量，用户提出了在MoE中实现expert parallel策略以优化计算效率的需求。,https://github.com/vllm-project/vllm/issues/2405
vllm,这个issue类型是用户提出需求，主要对象是Openai接口的Chatglm模型。由于用户希望Openai接口能够自动适配或者指定模型类型而不需要手动编写jinja模板chattemplate，这导致了用户提出了关于增加主流大模型chat template功能的需求。,https://github.com/vllm-project/vllm/issues/2403
vllm,这是一个性能优化相关的issue，主要涉及到在prefill阶段支持MQA/GQA的问题。原因是xformers目前不支持MQA/GQA，所以需要对`key`和`value`进行扩展，才能正确计算结果。,https://github.com/vllm-project/vllm/issues/2401
vllm,这是一个用户提出需求的Issue，主要涉及请求参数控制。,https://github.com/vllm-project/vllm/issues/2400
vllm,这是一个用户提出需求的 issue，主要对象涉及服务器端的指标收集，用户提出希望通过 `/metrics` 端点支持“平均首个令牌延迟”的功能。,https://github.com/vllm-project/vllm/issues/2399
vllm,这是一个用户提出的需求类型的issue，主要对象是日志输出格式。用户需求将日志输出配置为JSON格式，以便更好地解析日志。,https://github.com/vllm-project/vllm/issues/2397
vllm,这是一个关于功能需求的issue，主要涉及LLMEngine模块下的batching和streaming操作，用户提出了如何在处理多个请求，同时实现流式处理的问题。,https://github.com/vllm-project/vllm/issues/2396
vllm,这是一个功能请求，涉及Mixtral的混合任务卸载技术。,https://github.com/vllm-project/vllm/issues/2394
vllm,这个issue属于改进需求类型，主要对象是vllm-openai docker镜像，因为当前镜像使用的Cuda版本为12.1，导致在不同GPU驱动程序下出现版本不一致的问题，希望更新镜像以支持从11.8版本及更高的动态cuda版本。,https://github.com/vllm-project/vllm/issues/2393
vllm,该issue类型为用户提出需求，并涉及到设置vllm中的CUDA_VISIBLE_DEVICES环境变量；由于用户想要在另一块GPU上运行vllm，因此需要指定CUDA_VISIBLE_DEVICES环境变量来实现这一需求。,https://github.com/vllm-project/vllm/issues/2387
vllm,该issue属于重命名需求，主要涉及到“phi_1_5”变量命名。这个需求是由于vLLM同时支持phi和phi2模型，因此需要统一变量命名。,https://github.com/vllm-project/vllm/issues/2385
vllm,该issue类型为代码优化，主要对象是attention模块中的未使用代码。,https://github.com/vllm-project/vllm/issues/2384
vllm,这是一个功能需求的Issue，主要涉及将vLLM在Intel GPU设备上通过SYCL执行的原型开发。,https://github.com/vllm-project/vllm/issues/2378
vllm,这是一个用户提出需求的类型issue，主要涉及到VLLM模型中的文本生成算法，用户提出了关于改进文本生成质量的需求。,https://github.com/vllm-project/vllm/issues/2375
vllm,这个issue类型是建议改进，主要针对V100 GPU用户提示错误信息改进。原因是建议增加对于初学者的友好指导，以提供更周到的建议。,https://github.com/vllm-project/vllm/issues/2374
vllm,这是一个关于性能优化的问题，主要涉及到VLLM和TRTLLM两个框架对AWQ量化推断的比较，用户想知道VLLM中AWQ GEMM内核性能差的原因以及是否可以将TRTLLM中的内核计算移植到VLLM中以提升性能。,https://github.com/vllm-project/vllm/issues/2373
vllm,这是一个需求新增类型的issue，主要涉及的对象是为vllm增加新模型StableLM3B，原因可能是支持稳定性增强和功能扩展。,https://github.com/vllm-project/vllm/issues/2372
vllm,这是一个用户提出需求的issue， 主要涉及的对象是vllm， 用户询问如何在vllm中使用Splitwise插件，由于关键字搜索不到指引，发帖寻求帮助。,https://github.com/vllm-project/vllm/issues/2370
vllm,"这个issue属于优化建议类型，主要涉及了在注意力机制内核中在FP32中计算logits*V来提高数值精度。原因是在之前的做法中，将logits从FP32转换为BF16/FP16，然后计算dot(logits, V)，而现在的做法是将V从BF16/FP16转换为FP32，然后计算dot(logits, V)。",https://github.com/vllm-project/vllm/issues/2368
vllm,这是一个用户提出需求的issue，主要涉及vllm项目中的Phi2模型，用户希望通过API计算prompt在整个数据集上的perplexity。,https://github.com/vllm-project/vllm/issues/2364
vllm,这是一个功能需求的issue，主要涉及到OpenAI API的重构和函数调用的实现。原因是为了使代码更清晰易修改，并将完成部分视为传统功能。,https://github.com/vllm-project/vllm/issues/2360
vllm,这是一个建议提案类型的issue，主要涉及调度策略以提高吞吐量，因为通过长度排序序列会减少填充标记的百分比，而引入了一个新的调度策略来实现这一点。,https://github.com/vllm-project/vllm/issues/2357
vllm,这是一则关于功能需求的问题，涉及vllm的paged attention以及prompt cache的应用。用户询问是否可以通过缓存特定前缀的kvcache状态来优化模型性能。,https://github.com/vllm-project/vllm/issues/2354
vllm,这是一个用户提出需求的 issue，主要涉及 VLLM 使用快照下载私有模型的问题。,https://github.com/vllm-project/vllm/issues/2353
vllm,这是一个用户提出需求的issue，该问题涉及支持SelfExtend式上下文扩展的功能。由于论文中描述了一种方法来扩展任何基于rope的模型的上下文窗口，而不需要在推断时进行微调，因此用户提出了如何在vllm中添加对此的支持。,https://github.com/vllm-project/vllm/issues/2349
vllm,这是一个关于用户需求的issue，主要涉及到FastAPI在运行时设置root_path参数的问题。其原因是在路径代理后，Swagger无法正确展示页面导致404错误。,https://github.com/vllm-project/vllm/issues/2341
vllm,这是一个版本升级的请求，主要涉及软件的更新和发布跟踪。,https://github.com/vllm-project/vllm/issues/2337
vllm,这是一个用户提出需求的issue，主要涉及vllm是否支持从huggingface进行私有模型服务。,https://github.com/vllm-project/vllm/issues/2334
vllm,这是一个功能需求的issue，主要涉及到软件版本发布的跟踪和更新细节。,https://github.com/vllm-project/vllm/issues/2332
vllm,"这是一个用户提出需求的issue，主要涉及到block_size参数。有人问及为什么block_size受到了8, 16, 32的限制，是否可以使用更大的block_size处理更长的序列。",https://github.com/vllm-project/vllm/issues/2331
vllm,这个issue属于功能需求提出，主要涉及支持2/3/8位GPTQ量化模型，提出了关于比特量化模型性能和适用性的讨论。,https://github.com/vllm-project/vllm/issues/2330
vllm,这是一个关于性能优化和潜在改进的问题，涉及主要对象为vllm项目。用户提出了关于如何使vllm运行更快以及是否可以应用类似于GPTFast等优化方法的问题。,https://github.com/vllm-project/vllm/issues/2327
vllm,这个issue属于用户提出需求类型，主要涉及的对象是加载awq量化模型到CPU内存而不是GPU内存。这个问题可能由于用户希望在CPU上运行模型而不是GPU上而产生。,https://github.com/vllm-project/vllm/issues/2326
vllm,"这是一个更新依赖关系的问题，涉及主要对象是pydantic库版本。由于OpenAI更新了对pydantic的版本要求，导致需要将pydantic版本从1.10.13提升到>= 1.9.0, < 3。",https://github.com/vllm-project/vllm/issues/2322
vllm,这是一个关于增加请求级别指标的需求提出的issue，涉及到优化和重构Prometheus及添加性能监控指标的功能。,https://github.com/vllm-project/vllm/issues/2316
vllm,这个issue是一个用户提出需求，询问如何在OpenShift或Kubernetes上部署vLLM的问题。,https://github.com/vllm-project/vllm/issues/2314
vllm,这是一个关于升级Pydantic版本的需求问题，涉及主要对象是vLLM项目。该问题由于OpenAI要求使用Pydantic版本大于等于1.9.0且小于3，导致需要将vLLM中的Pydantic版本也进行对应升级。,https://github.com/vllm-project/vllm/issues/2313
vllm,这是一个用户提出需求的issue，主要涉及在为OpenAI网页服务器添加gradio chatbot功能时遇到的问题。,https://github.com/vllm-project/vllm/issues/2307
vllm,这是一个用户提出需求的issue，主要涉及的对象是为vllm下的openai webserver添加chatbot功能。,https://github.com/vllm-project/vllm/issues/2306
vllm,该issue属于用户提出需求/请教问题类型，主要涉及Python和Rust两种语言针对HTTP服务器并发性能的比较。由于Python的http server对于用户自定义调度逻辑比较困难，用户想知道是否使用Rust语言的http server能提供更好的并发性能。,https://github.com/vllm-project/vllm/issues/2303
vllm,这个issue属于用户提出需求类型，主要涉及的对象是Prometheus指标。用户提出添加更多指标，以便监控服务器的性能和生成速度。,https://github.com/vllm-project/vllm/issues/2302
vllm,这是一个需求提出类型的issue，主要涉及的对象是关于在vLLM中添加对不具有注意力偏置的GPT-NeoX模型的支持。由于Hugging Face Transformers的更新导致存在版本差异问题，需要修改代码以正确初始化这些模型。,https://github.com/vllm-project/vllm/issues/2301
vllm,这是一个性能优化的issue，主要涉及GEMM kernel的优化，通过实现自定义的GEMM kernel提高小批量大小下的性能。,https://github.com/vllm-project/vllm/issues/2300
vllm,这是一个用户提出的需求问题，主要涉及到VLLM中的prefix prompt cache功能。用户希望添加接口能够生成和存储特定prefix内容的kv cache，并在请求阶段根据传入参数使用相应的cache。,https://github.com/vllm-project/vllm/issues/2296
vllm,这个issue类型是性能优化，主要涉及的对象是在Github上的vllm项目中实现的tensor parallel MOE。由于对模型权重进行适当地分片并在不同rank间运行MLP操作，该issue旨在提高模型训练时的效率和速度。,https://github.com/vllm-project/vllm/issues/2293
vllm,这是一个用户提出需求的issue，主要对象是Vllm，由于缺少对chat completion api的logprobs支持，用户提出了增加该功能的需求。,https://github.com/vllm-project/vllm/issues/2292
vllm,这是一个功能改进的issue，涉及到Scheduler的数据结构改动，目的是提高调度器的性能。,https://github.com/vllm-project/vllm/issues/2290
vllm,该issue类型为文档更新需求，涉及主要对象为vLLM和Haystack集成。,https://github.com/vllm-project/vllm/issues/2288
vllm,这个issue类型为用户提出需求，主要涉及对象是如何测试前缀共享性能，在论文中复现前缀共享部分的实验，用户寻求关于如何进行实验以及缺乏关于前缀设置代码的帮助。,https://github.com/vllm-project/vllm/issues/2286
vllm,这个issue类型是用户提出需求，询问如何检索支持的模型，主要对象是vllm库。用户提出这个问题可能是因为想了解如何查询库中支持的模型列表。,https://github.com/vllm-project/vllm/issues/2282
vllm,这是一个用户提出需求的issue，主要涉及支持FP8-E5M2 KV Cache，问题源于需要优化内存使用以提高吞吐量。,https://github.com/vllm-project/vllm/issues/2279
vllm,这是一个用户提出需求的issue，主要涉及到VLLM项目中的prompt evaluation和token generation操作。由于当前调度策略在接受新请求时需要暂停解码操作，用户提出了是否可以并行执行prompt evaluation和token generation的问题。,https://github.com/vllm-project/vllm/issues/2278
vllm,这是一个用户提出的需求问题，主要涉及prompt evaluation和token generation的并行执行，由于当前调度策略需要在接受新请求时暂停解码。,https://github.com/vllm-project/vllm/issues/2277
vllm,这是一个关于用户提出需求的issue，主要涉及的对象是`ChatCompletionRequest`。根据标题可以推测用户希望在openai api server中添加对`logprobs`的支持。,https://github.com/vllm-project/vllm/issues/2276
vllm,这个issue类型为功能增强请求，涉及主要对象为ROCm 6.0和MI300。因为当前系统缺乏对ROCm 6.0和MI300的支持，所以用户提交了这个请求以增加对它们的支持。,https://github.com/vllm-project/vllm/issues/2274
vllm,这是一个用户提出需求的issue，涉及到对vllm的缓存淘汰功能的支持，用户希望在持久会话中能够管理缓存。,https://github.com/vllm-project/vllm/issues/2265
vllm,这是一个用户提出需求的issue，主要涉及vllm是否能支持Fuyu-8B这个multimodel llm模型，由于当前vllm只支持纯文本，用户想知道如何处理包含图片的multimodel模型。,https://github.com/vllm-project/vllm/issues/2262
vllm,"这个issue类型是用户提出需求，请求添加一个""About""标题到README.md文件中，以清晰区分""Latest News""部分和基本信息部分。",https://github.com/vllm-project/vllm/issues/2260
vllm,这个issue是用户提出需求，询问是否vLLM支持Selective batching，希望实现同一批次中不同输入序列生成时输出长度不同的功能。,https://github.com/vllm-project/vllm/issues/2252
vllm,这是一个更新安装说明的 issue，涉及到 vLLM 安装时 CUDA 11.8 xFormers 兼容性错误的问题。,https://github.com/vllm-project/vllm/issues/2246
vllm,这个issue是一个功能需求，主要涉及的对象是AutoAWQ模块的支持Mixtral，并需要实现新的`modules_to_not_convert`参数以防止quantize模型中的`gate`，原因是要在加载这个4bit模型时避免将其量化为线性层导致内存消耗较大。,https://github.com/vllm-project/vllm/issues/2243
vllm,这是一个关于用户提出需求的类型，主要涉及的对象是构建windows wheels with cuda 12.1，由于cuda 12.1，用户希望项目能提供对应的预编译二进制包。,https://github.com/vllm-project/vllm/issues/2242
vllm,这是一个用户提出需求的issue，主要涉及如何对回答中重复部分进行惩罚。由于重复部分导致了一些回答不准确的问题。,https://github.com/vllm-project/vllm/issues/2238
vllm,该issue类型为用户提出需求，涉及到在/vllm/entrypoints/api_server.py中如何添加历史记录。用户想要知道如何将历史记录添加到请求中的提示中，并询问是否应该使用模板，以及每个模型是否有不同的模板。,https://github.com/vllm-project/vllm/issues/2228
vllm,这是一个关于用户提出需求的issue，主要涉及 Pipeline Parallelism 的支持问题。,https://github.com/vllm-project/vllm/issues/2226
vllm,这是一个关于需求的问题，涉及主要对象是torch requirement，由于可能出现的原因是用户希望降低torch的要求。,https://github.com/vllm-project/vllm/issues/2225
vllm,这是一个用户提出需求的issue，主要涉及的对象是在GPTQ quantization过程中添加新的quantization kernels。由于需要支持不同量化位数的使用情况，因此需要增加额外的选项和相应的代码实现。,https://github.com/vllm-project/vllm/issues/2223
vllm,这是一个特性改进的issue，主要涉及vLLM项目中的控制节点通信方式的优化，旨在减少ray通信的序列化开销。,https://github.com/vllm-project/vllm/issues/2221
vllm,该issue类型为用户提出需求，主要涉及对象是vllm，用户提出希望将PowerInfer整合到vllm中，以利用CPU和GPU结合进行更快的推理。,https://github.com/vllm-project/vllm/issues/2212
vllm,"这是一个功能需求的issue，主要涉及的对象是OpenAI API的重构和函数调用处理。由于需要实现函数调用处理，对OpenAI API进行了重构，同时增加了类似于OpenAI API中的""tools""功能。",https://github.com/vllm-project/vllm/issues/2210
vllm,这是一个功能改进类型的issue，主要涉及到vllm项目中的sampler功能模块。,https://github.com/vllm-project/vllm/issues/2209
vllm,这是一个性能优化的issue，涉及主要对象是_prepare_sample函数，由于减少_prepare_sample函数的延迟，不影响模型执行。,https://github.com/vllm-project/vllm/issues/2207
vllm,这个issue是用户提出的需求，请求增加对json格式和正则表达式格式的支持，主要涉及到的对象是项目中的功能CC，用户提出此需求可能是为了优化数据的处理和展示方式。,https://github.com/vllm-project/vllm/issues/2205
vllm,这是一个用户提出需求的 issue， 主要对象是 vllm 项目的 openai 入口点，请求增加一个 json 格式选项以便更好地格式化输出。,https://github.com/vllm-project/vllm/issues/2204
vllm,"该issue类型为协作需求, 主要涉及对象为Ray Integration以及vllm的experimental accelerated DAG API, 由于当前nightly implementation存在一些限制，例如actor出现异常或worker死机导致hang，DAG初始化后无法运行其他actor任务，用户提出了关于这些限制以及改进方向的讨论。",https://github.com/vllm-project/vllm/issues/2201
vllm,这个issue是关于需求或问题的提问，主要涉及的对象是内部和外部碎片的计算，以及Hugging Face Transformers库中是否存在碎片问题。由于用户想了解如何测量内部和外部碎片，以及是否存在标准Hugging Face Transformers中的碎片问题。,https://github.com/vllm-project/vllm/issues/2200
vllm,这是一个用户提出需求的类型，主要对象是vLLM模型的GPU内存使用。由于vLLM只支持单一模型运行并占用固定大小的GPU内存，导致难以在同一GPU上共享多个模型。,https://github.com/vllm-project/vllm/issues/2193
vllm,这是一个用户提出需求的issue，主要涉及为vLLM添加对批量完成（离线）的支持。原因是为了提高吞吐量，支持一次传递一批提示文本。,https://github.com/vllm-project/vllm/issues/2191
vllm,这个issue属于对项目的增强需求，主要涉及到vLLM中的speculative decoding功能的实现和优化。,https://github.com/vllm-project/vllm/issues/2188
vllm,该issue类型是文档更新，主要涉及更新`gpumemoryutilization`参数的帮助文本以与vllm文档保持一致。,https://github.com/vllm-project/vllm/issues/2183
vllm,这是一个功能需求类型的issue，主要对象是`benchmark_serving.py`文件。由于用户希望能够在OpenAI API server中支持`benchmark_serving.py`文件，需要添加`endpoint`和`model`参数以便用户评估他们的OpenAI兼容vLLM服务器。,https://github.com/vllm-project/vllm/issues/2172
vllm,这是一个特性请求的issue，主要涉及Faster and memory efficient top-p top-k kernel。这个问题的提出是为了向vllm添加一个更快速和更节省内存的cuda kernel，以支持float16/bfloat16并限制topk<=1024。,https://github.com/vllm-project/vllm/issues/2169
vllm,这是一个关于禁用CUDA图表功能的问题，类型为用户提出需求。该问题涉及主要对象为SqueezeLLM。由于某些原因，用户想要禁用CUDA图表功能。,https://github.com/vllm-project/vllm/issues/2161
vllm,这是一个需求类型的issue，主要涉及的对象是vLLM的模型Phi 2。这个问题是由于需要将Phi 2添加到支持的模型列表中，因为作者发现vLLM的Phi 2输出与HF（Hugging Face）的匹配。,https://github.com/vllm-project/vllm/issues/2159
vllm,这是一个功能需求的issue，涉及到LLava模型的支持添加。由于LLava模型的一些基本实现问题，用户向开发团队提出了关于支持LLava模型加载、多模态推理和API问题的需求。,https://github.com/vllm-project/vllm/issues/2153
vllm,这个issue类型是功能改进建议，主要涉及的对象是减少对CuPy的依赖性。原因是考虑到`TORCH_NCCL_AVOID_RECORD_STREAMS=1`可以启用CUDA图来使用`torch.distributed.all_reduce`，从而避免对CuPy的依赖。,https://github.com/vllm-project/vllm/issues/2152
vllm,该issue类型为用户提出需求，在文档中添加关于CUDA图支持的详细信息。,https://github.com/vllm-project/vllm/issues/2148
vllm,这是一个建议删除 Llama 分词器警告的 issue，涉及主要对象为 Llama V1 分词器。其原因是警告有可能误导用户使用不需要的 HF 内部分词器。,https://github.com/vllm-project/vllm/issues/2146
vllm,这是一个需求提出类型的issue，主要涉及的对象是`quantization`参数。,https://github.com/vllm-project/vllm/issues/2145
vllm,这是一个用户提出的需求类型的 issue，主要涉及优化内存使用的建议，提出了使用LRU缓存来保存CUDA图数据的方法。,https://github.com/vllm-project/vllm/issues/2143
vllm,这是一个功能需求类型的issue，主要涉及logit processors在处理文本生成时缺少额外上下文信息，用户提出希望能够传递额外的参数以提供更详细的信息。,https://github.com/vllm-project/vllm/issues/2142
vllm,这是一个用户提出需求的 issue，主要涉及对vLLM添加 LogitProcessors 的功能。用户提出该需求的原因是为了支持实现数字水印算法，并希望将更复杂的 LogitProcessor 集成到 vLLM 项目中。,https://github.com/vllm-project/vllm/issues/2141
vllm,这是一个用户提出需求的issue，主要涉及该项目的文档，用户希望添加量化支持的说明。,https://github.com/vllm-project/vllm/issues/2135
vllm,这个issue类型是用户需求，涉及主要对象是服务Mixtral 8x7B with vllm OpenAI Server。由于文档不清晰，用户不清楚如何设置`tensor_parallel_size = 2`以在两个A100 GPU上分割Mixtral，希望获得帮助。,https://github.com/vllm-project/vllm/issues/2134
vllm,这是一个用户提出需求的类型，该问题单涉及的主要对象是Llama ILQL训练模型。,https://github.com/vllm-project/vllm/issues/2128
vllm,这是一个用户提出需求的issue，主要涉及对象是`AsyncLLMEngine`，用户希望找到同时支持流式推断和批处理推断的解决方案。,https://github.com/vllm-project/vllm/issues/2125
vllm,这是一个用户提出需求的类型的issue，主要涉及到关于vLLM模型中集成packing inference的想法。由于用户希望通过在连续批处理过程中实现packing来充分利用GPU，提出了以上问题。,https://github.com/vllm-project/vllm/issues/2121
vllm,这是一个需求提议类型的issue，主要对象是VLLM模型的单步生成多个token功能。,https://github.com/vllm-project/vllm/issues/2120
vllm,该issue类型为用户提出需求，主要涉及对象是如何配置使用的GPU；由于缺乏对GPU选择进行配置的参数，导致无法更改使用的GPU编号，即使尝试设置CUDA_VISIBLE_DEVICES仍未生效。,https://github.com/vllm-project/vllm/issues/2118
vllm,该issue类型是用户提出需求，用户想要支持使用tiktoken作为tokenizer选项。,https://github.com/vllm-project/vllm/issues/2117
vllm,这个issue是关于功能增强的，主要对象是api_server，由于该用户做了一些更新导致功能存在一些问题，他希望能将这些更新合并到vLLM中。,https://github.com/vllm-project/vllm/issues/2110
vllm,这个issue属于用户提出需求，主要涉及API服务器，用户希望能够通过传递SSL证书和密钥文件路径来启用HTTPS。,https://github.com/vllm-project/vllm/issues/2109
vllm,该issue为性能优化问题，主要涉及VLLM中H100的性能优化需求。可能由于当前设置未能使H100在速度上有显著提升，用户希望寻求优化性能的方式。,https://github.com/vllm-project/vllm/issues/2107
vllm,该issue类型为功能增强（feature enhancement），主要涉及对象是AsyncLLMEngine函数的generate功能，原因是需要将AsyncIterator类型添加到函数中，同时解决了yapf对toml未安装时的警告问题。,https://github.com/vllm-project/vllm/issues/2100
vllm,这个issue为用户提出需求类型，主要涉及到vllm模型的推理性能。用户提出了关于Mixtral-8x7B推理速度较慢的问题，并希望vllm团队能够关注并改进相关工作。,https://github.com/vllm-project/vllm/issues/2098
vllm,该issue类型为软件版本升级请求，涉及到代码库vllm，并由于需要合并CC(Mixtral expert parallelism)分支而产生。,https://github.com/vllm-project/vllm/issues/2095
vllm,该issue类型为用户提出需求，主要对象是多模型部署。询问如何在两个GPU上分别加载完整模型而非使用张量并行。,https://github.com/vllm-project/vllm/issues/2094
vllm,这是一个关于需求的问题，涉及的主要对象是在vllm中如何指定特定的GPU，可能由于用户需要精确控制训练过程中GPU的分配。,https://github.com/vllm-project/vllm/issues/2092
vllm,该issue是关于特性改进（Feature Enhancement），涉及到Mixtral模型的并行计算方案优化。这次改进主要是为了提高Mixtral模型的效率和性能。,https://github.com/vllm-project/vllm/issues/2090
vllm,这是一个文档相关的需求问题，用户希望在 ROCm 支持的模型上添加说明。,https://github.com/vllm-project/vllm/issues/2087
vllm,该issue属于用户提出需求类型，主要涉及对象是vllm模型。由于在示例中没有找到关于vllm支持离线推理中流式聊天的说明，用户想要知道vllm是否支持离线推理中的流式聊天。,https://github.com/vllm-project/vllm/issues/2083
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM支持的AMD GPU模型。用户提出这个问题是想了解目前vLLM支持哪些型号的AMD GPU。,https://github.com/vllm-project/vllm/issues/2082
vllm,这个issue属于用户提出需求类型，主要对象是vLLM框架的维护者。用户想咨询关于在vLLM框架中集成Mac Metal API支持的可能性，因为在Mac设备中使用Metal API能够带来GPU加速计算的性能优势。,https://github.com/vllm-project/vllm/issues/2081
vllm,这个issue类型是更新需求，涉及的主要对象是xformers版本依赖关系。由于ROCm版本与CUDA使用的xformers版本不匹配，导致需要更新xformers版本以解决兼容性问题。,https://github.com/vllm-project/vllm/issues/2079
vllm,该issue是用户提出需求，询问是否可以在vllm中使用类似llama-2-70b-chat-gptq这样的自动压缩模型。,https://github.com/vllm-project/vllm/issues/2077
vllm,这是一个用户提出需求的issue，主要涉及的对象是安装vllm时需要使用CUDA 11.8而非12.1版本。,https://github.com/vllm-project/vllm/issues/2072
vllm,这是一个用户需求类型的issue，主要涉及的对象是添加策略。由于用户需要添加特定的策略，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/2071
vllm,这是一个功能需求的issue，主要涉及的对象是实现Triton-based AWQ kernel。问题可能由于性能模型不正确和一些功能缺失导致。,https://github.com/vllm-project/vllm/issues/2068
vllm,这个issue是一个用户需求，主要涉及的对象是允许用户选择不同的文件夹来存储Ray日志和临时文件，由于Ray日志在Sagemaker上填满了/tmp文件夹，需要提供选项让用户选择存储位置。,https://github.com/vllm-project/vllm/issues/2066
vllm,这个issue是用户提出需求类型，主要涉及的对象是为vLLM添加与Neuron工具链兼容的构建选项。,https://github.com/vllm-project/vllm/issues/2065
vllm,这个issue类型为用户提出需求，主要涉及的对象是VLM模型和GPT4V API。,https://github.com/vllm-project/vllm/issues/2058
vllm,这是一个用户提出需求的类型，针对实现添加RR策略。,https://github.com/vllm-project/vllm/issues/2055
vllm,这是一个用户提出需求的issue，主要涉及的对象是添加政策。可能是由于现有政策不完善或者未列明清楚导致用户提出需求希望添加新的政策。,https://github.com/vllm-project/vllm/issues/2054
vllm,这个issue是关于移除依赖项einops的请求，主要涉及Phi模型未使用该软件包。,https://github.com/vllm-project/vllm/issues/2049
vllm,这是一个需求类型的issue，主要涉及的对象是v0.2.4版本的VLLM软件。由于缺少cuda11.8 wheel发布，用户提出了需要该版本的需求。,https://github.com/vllm-project/vllm/issues/2047
vllm,这个issue类型是用户提出需求，需要升级transformers版本至4.36.0来支持Mixtral功能。,https://github.com/vllm-project/vllm/issues/2046
vllm,这是一个类型为需求报告的issue，主要涉及对象是PyTorch版本。由于xformers (v0.0.23)要求PyTorch v2.1.1，因此需要将PyTorch版本升级为v2.1.1。,https://github.com/vllm-project/vllm/issues/2045
vllm,该issue属于功能需求类型，主要涉及到模型加载的延迟实现。,https://github.com/vllm-project/vllm/issues/2044
vllm,这是一个用户提出需求的 issue，主要涉及到更新 Dockerfile 以构建 Megablocks。由于需要为多种架构构建 Megablocks，因此需要更新 Dockerfile 并构建 wheels 以便在服务器上安装。,https://github.com/vllm-project/vllm/issues/2042
vllm,这是一个用户提出需求的issue，主要涉及改进支持的权重格式，由于目前仅支持 Mistral 的 pt 格式权重，在未来可能会有 HF 修改 pt 权重文件的可能性。,https://github.com/vllm-project/vllm/issues/2041
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是关于提高LLM推理吞吐量的一篇论文。,https://github.com/vllm-project/vllm/issues/2039
vllm,这个issue是用户提出需求，主要涉及的对象是在使用Retriever与Question Answer Chain时想要在vllm中实现Retrieval QA支持。,https://github.com/vllm-project/vllm/issues/2037
vllm,这个issue类型是更新请求，涉及的主要对象是vllm项目。由于可能需要提升版本以获取新功能或修复bug，触发了此更新请求。,https://github.com/vllm-project/vllm/issues/2034
vllm,这是一个更新说明文档的issue，涉及的主要对象是megablocks requirement for mixtral。这个issue是由于需要在mixtral项目的README.md中添加关于megablocks的要求。,https://github.com/vllm-project/vllm/issues/2033
vllm,这个issue类型是需求提出，主要对象是重构Mixtral以重用MegaBlocks中代码，由于MoE实现中存在从MegaBlocks直接复制代码的情况，导致代码重复且易出错。,https://github.com/vllm-project/vllm/issues/2032
vllm,这个issue类型为需求更新，涉及的主要对象是mixtral项目的requirements.txt文件。由于缺少对megablocks的依赖声明，导致用户提出需要添加该依赖项的需求。,https://github.com/vllm-project/vllm/issues/2029
vllm,用户提出需求，要求添加对`mistralai/Mixtral8x7Bv0.1`和`mistralai/Mixtral8x7BInstructv0.1`模型的支持。,https://github.com/vllm-project/vllm/issues/2011
vllm,这是一个关于性能优化的问题，主要涉及AWQ Triton kernel中的量化零点解压缩以及相关性能提升，提出了一些疑问和困惑，包括当前Triton kernel如何运行、为什么比CUDA kernel慢等问题。,https://github.com/vllm-project/vllm/issues/2009
vllm,这个issue是用户提出需求，要求添加mamba_chat (2.8b)模型，并询问vllm团队是否能够帮助添加。,https://github.com/vllm-project/vllm/issues/2001
vllm,该issue类型为用户提出需求，主要涉及QuIP# 2-bit Quantization Support。由于用户希望获得该功能的支持，提出了相关需求。,https://github.com/vllm-project/vllm/issues/1998
vllm,这是一个功能需求提出的issue，主要涉及的对象是attention kernel。由于head_mapping参数从全局内存加载，需要将其替换为num_kv_heads来避免这种加载。,https://github.com/vllm-project/vllm/issues/1997
vllm,这是一个用户提出需求的类型的 issue，主要涉及如何在8个 A100 GPU 上部署 vllm 模型，并且实现高并发支持。,https://github.com/vllm-project/vllm/issues/1995
vllm,这是一个功能需求提出的issue，主要涉及的对象是代码中的`head_mapping`变量。这个问题提出了使用`num_queries_per_kv`替代`head_mapping`，以避免全局内存访问。,https://github.com/vllm-project/vllm/issues/1994
vllm,这个issue属于用户提出需求类型，主要涉及的对象是vLLM OpenAI server模式下的 `model` 键。用户认为当重新训练并部署模型时，希望请求中的 `model` 被忽略，以便后端能够接受当前活动的任何模型。,https://github.com/vllm-project/vllm/issues/1988
vllm,这个issue类型是用户提出需求，涉及的主要对象是prefix cache功能。由于实现了rudimentary版本的前缀缓存功能在特定场景下表现出显著的性能提升，因此用户提出是否需要此功能，并描述了性能测试结果和需求。,https://github.com/vllm-project/vllm/issues/1983
vllm,该issue类型为用户提出需求，用户想知道VLLM是否支持流式输出以及如何解决没有stream_chat接口的问题。,https://github.com/vllm-project/vllm/issues/1982
vllm,这是一个特性需求，主要对象是vLLM项目中的文本生成引擎。由于当前vLLM在生成文本时会去除终止字符串，但用户希望保留终止字符串以便了解生成停止的原因。,https://github.com/vllm-project/vllm/issues/1973
vllm,这个issue是关于建议性提案，提出了使用pre-commit替代format.sh的建议，主要涉及项目的开发者和贡献者。原因可能是为了简化代码格式化工具的调用，提高项目代码风格的一致性。,https://github.com/vllm-project/vllm/issues/1969
vllm,这是一个关于功能需求的issue，主要涉及的对象是v100显卡，用户提出了关于该显卡是否被支持的问题。,https://github.com/vllm-project/vllm/issues/1955
vllm,这是一个用户提出需求的类型issue，主要涉及对象是希望了解有关该项目性能方面的详细信息。具体原因是用户希望了解这个项目对延迟和吞吐量的影响。,https://github.com/vllm-project/vllm/issues/1951
vllm,这是一个用户需求问题，主要涉及的对象是vLLM代码库。该问题的原因是vLLM目前在代码中大量使用`cuda`，导致只能兼容NVIDAI/AMD/Intel GPU，无法支持其他加速器，因此需要使用更通用的抽象（如DeepSpeed的xpu）来支持更多种硬件。,https://github.com/vllm-project/vllm/issues/1948
vllm,这是一个功能需求提报，主要对象是 SamplingParams 类。这个问题由于生成的 tokens 过短而导致，用户提出需要添加 min_tokens 参数来避免这种情况。,https://github.com/vllm-project/vllm/issues/1945
vllm,这是一个用户提出需求的issue，主要涉及Qwen模型的chat_stream替代generate功能，原因是之前的generation参数存在问题。,https://github.com/vllm-project/vllm/issues/1943
vllm,这是一个用户提出需求的issue，主要涉及如何在多个gpu上启用数据并行ism，用户希望实现数据并行ism来提高推理性能，但尝试使用多线程进行并行请求导致运行时问题。,https://github.com/vllm-project/vllm/issues/1940
vllm,"这是一个功能增强类的issue，主要涉及对象是""MPTAttention""模块，原因是需要修改""mpt.py""来支持GQA。",https://github.com/vllm-project/vllm/issues/1938
vllm,这是一个用户需求问题，涉及模型加载器的兼容性问题，导致不能支持ChatGLMForConditionalGeneration。,https://github.com/vllm-project/vllm/issues/1932
vllm,该issue类型为优化建议，主要涉及的对象是模型执行时的CUDA图，提出了使用CUDA图优化CPU开销的建议。,https://github.com/vllm-project/vllm/issues/1926
vllm,这个issue类型是一个需求提出，主要涉及对象是vllm软件在macOS上是否支持M2芯片，用户想知道软件能否在此平台上正常运行。,https://github.com/vllm-project/vllm/issues/1921
vllm,这是一个用户提出需求的issue，主要对象是支持 Yi-34B-Chat-8bits（gptq），用户想知道何时可以支持该语言模型。,https://github.com/vllm-project/vllm/issues/1916
vllm,该issue类型为用户提出需求，请教问题，主要涉及对象是Tensor Core和CUDA Core。用户询问了关于在A100上默认使用Tensor Core进行推理，以及针对int4量化模型推理是否需要反量化计算的问题。,https://github.com/vllm-project/vllm/issues/1915
vllm,这是一个用户提出需求的issue，主要对象是vllm模型，用户想要在使用`.generate`生成文本后能够反向传播梯度到加载的模型。,https://github.com/vllm-project/vllm/issues/1909
vllm,该issue类型为用户提出需求，关注主要对象为vllm.LLM模型。用户提出了如何终止vllm.LLM并释放GPU内存的问题。,https://github.com/vllm-project/vllm/issues/1908
vllm,这个issue类型是用户请求帮助，主要对象是无法解决特定问题。由于无法解决问题而导致用户寻求帮助。,https://github.com/vllm-project/vllm/issues/1906
vllm,这是一个功能更新类型的issue，涉及主要对象为Yi模型和配置文件。这个问题由于Yi作者改变了他们的代码/配置文件以兼容LLaMA，导致用户需要慢慢移除该模型和配置代码。,https://github.com/vllm-project/vllm/issues/1899
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm库中的custom layers，用户希望添加PyTorch原生实现的自定义层，并列出了该变动带来的多方面益处。,https://github.com/vllm-project/vllm/issues/1898
vllm,这个issue是关于用户提出需求的，主要涉及如何在OpenRLHF中加载本地LLM并重新调整LLM引擎的权重。用户提出这个问题可能是因为需要在训练期间重新调整LLM模型的权重，并释放vLLM GPU内存以为训练留更多内存。,https://github.com/vllm-project/vllm/issues/1897
vllm,这是一个用户提出需求类型的issue，主要涉及到添加一个选项来在docker构建过程中使用nvcc_threads来防止OOM，这个需求的提出是因为在构建过程中出现了内存耗尽的问题。,https://github.com/vllm-project/vllm/issues/1893
vllm,这是一个用户提出需求的类型，主要涉及VLLM模型中的presence_penalty或frequency_penalty带来的问题。用户发现在生成长文本时，使用这两个超参数会导致标点符号在文本中逐渐消失，推测是因为这两个参数使标点符号被过度惩罚。,https://github.com/vllm-project/vllm/issues/1892
vllm,这是一个需求提交类型的issue，主要涉及为VLLM项目添加以Prometheus格式的生产指标。,https://github.com/vllm-project/vllm/issues/1890
vllm,这是一个优化性质的issue，主要涉及sampler的优化，由于以前的操作导致GPU无法连续运行，现在通过重排操作来减少采样开销以提高性能。,https://github.com/vllm-project/vllm/issues/1889
vllm,该issue类型属于用户提出需求，要求添加repetition_penalty支持到OpenAI API。,https://github.com/vllm-project/vllm/issues/1887
vllm,这是一个用户提出需求的问题，主要涉及如何注册用户修改的模型和分词器，原因是当前使用受限于特定目录中的模型和分词器，希望能够在不将其纳入vllm或transformers的情况下进行注册。,https://github.com/vllm-project/vllm/issues/1884
vllm,这个issue类型是一个功能需求，主要涉及 AWQ Quantization 的内核更新和优化。原因是当前AWQ内核未经优化，用户希望更新内核并测试新内核性能。,https://github.com/vllm-project/vllm/issues/1882
vllm,这个issue类型是用户提出需求，主要涉及对象是Qwen-14B-Chat-Int4(gptq)，用户寻求帮助使用vllm解决问题。,https://github.com/vllm-project/vllm/issues/1881
vllm,这是一个性能优化类型的issue，主要涉及到vLLM中的MQA/GQA模块的优化问题。由于vLLM当前没有使用优化的内核，导致无法发挥MQA/GQA的优势，可能导致性能下降。,https://github.com/vllm-project/vllm/issues/1880
vllm,这个issue属于代码改进类型，主要涉及Baichuan 2的权重归一化处理。原因是缺少权重归一化导致Baichuan 2支持出现问题。,https://github.com/vllm-project/vllm/issues/1876
vllm,该issue为用户提出需求类型，主要对象是保存pytorch profiler输出用于延迟基准测试，因为用户希望能够保存跟踪文件以便在网站如https://ui.perfetto.dev/或tensorboard中查看。,https://github.com/vllm-project/vllm/issues/1871
vllm,这是一个特性请求，主要关注在项目中添加延迟指标。,https://github.com/vllm-project/vllm/issues/1870
vllm,这个issue属于功能需求提议，主要涉及到OpenAI兼容服务的参数更新。,https://github.com/vllm-project/vllm/issues/1869
vllm,这是一个用户提出需求的issue，主要涉及VLLM模型的自定义服务器参数，原因是用户认为添加`min_p`和`repetition_penalty`这些非官方参数对扩展功能有用。,https://github.com/vllm-project/vllm/issues/1868
vllm,这个issue类型为用户提出需求，涉及的主要对象是API server。原因可能是用户想了解API server是否支持动态批处理发送请求。,https://github.com/vllm-project/vllm/issues/1864
vllm,这是一个用户提出需求的issue，主要对象是要求支持INT8量化的llama模型。用户希望能够直接使用vllm推理已量化为INT8的模型，但目前使用smoothquant和torchint存在兼容性问题。,https://github.com/vllm-project/vllm/issues/1863
vllm,这是一个用户提出需求的issue，主要涉及如何获取LLM隐藏状态，由于用户需要使用最终输入token的隐藏状态来表示整个句子，但在源代码中并没有找到直接满足该需求的解决方案。,https://github.com/vllm-project/vllm/issues/1857
vllm,这是一个需求相关的issue，主要涉及的对象是软件版本[v0.2.3] Release Tracker。原因可能是为了添加新功能、修复问题或进行改进。,https://github.com/vllm-project/vllm/issues/1856
vllm,这是一个用户提出需求的issue，主要对象涉及的是程序的性能优化。导致该需求的原因是为了保存性能分析结果方便后续分析。,https://github.com/vllm-project/vllm/issues/1854
vllm,这是一个关于性能优化的问题，涉及到GPTQ W4A16和AWQ W4A16的实现比较，用户想了解它们谁更快，是否可以共享相同的CUDA函数，由于两者的数学计算类似，希望得到回复解答。,https://github.com/vllm-project/vllm/issues/1853
vllm,这个issue类型是需求提出，主要对象是优化调度中的列表操作，导致需求是为了通过使用Numpy来减少列表操作。,https://github.com/vllm-project/vllm/issues/1850
vllm,这是一个用户提出需求的issue，主要对象是添加获取内存统计数据的API接口。原因是当前没有可用的接口来监控内存使用情况，因此用户请求添加这样的接口来进行监控。,https://github.com/vllm-project/vllm/issues/1849
vllm,这是一个用户提出需求的类型，主要涉及的对象是vllm cli，用户希望创建一个更方便的入口点用于运行openai兼容服务器，并解决`cuda_utils`部分导入的常见问题。,https://github.com/vllm-project/vllm/issues/1844
vllm,这是一个关于需求讨论的issue，主要涉及Cache Events是否必要的讨论。原因在于操作都在同一流中进行，可能导致一些同步问题。,https://github.com/vllm-project/vllm/issues/1842
vllm,该issue属于功能需求的类型，主要涉及的对象是Latency benchmark。这个需求是希望在性能测试中添加profile选项以便使用PyTorch分析GPU活动。,https://github.com/vllm-project/vllm/issues/1839
vllm,这是一个用户提出需求的类型的issue，该问题单涉及的主要对象是vLLM的文档。,https://github.com/vllm-project/vllm/issues/1837
vllm,这个issue是关于合并EmbeddedLLM/vllm-rocm到vLLM主分支的需求，主要涉及添加ROCm支持。原因是之前的PR中有太多更改，所以需要新的pull request来继续这项工作。,https://github.com/vllm-project/vllm/issues/1836
vllm,这是一个用户提出需求的类型，主要涉及的对象是VLLM。由于用户希望VLLM能够支持DeepSeek模型，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/1834
vllm,这是一个用户提出需求的issue，主要涉及到的对象是模型训练参数。这个问题提出了关于early stopping 参数设置以及beam search的使用关系的疑问。,https://github.com/vllm-project/vllm/issues/1829
vllm,这是一个用户的需求问题，主要涉及到如何指定使用的GPU编号。由于默认使用的GPU编号是cuda:0，用户想知道如何指定其他GPU编号来使用。,https://github.com/vllm-project/vllm/issues/1827
vllm,该issue类型为需求提出，涉及主要对象为Vllm与Ray Serve的集成。由于Vllm向Ray请求了所有资源，导致无法与其他模型同时使用，用户提出需要更好地与Ray Serve整合以避免此限制。,https://github.com/vllm-project/vllm/issues/1821
vllm,这是一个用户提出需求的类型的issue，主要涉及到vLLM模型在使用`best_of`采样时对累积对数概率进行标准化的选项。,https://github.com/vllm-project/vllm/issues/1820
vllm,这是一个关于代码改进的issue，主要涉及的对象是ops.h头文件。由于没有添加头文件保护宏导致了重定义错误。,https://github.com/vllm-project/vllm/issues/1817
vllm,该issue类型为用户提出需求，询问Vllm是否兼容推理自定义微调模型，涉及主要对象是Vllm模型。,https://github.com/vllm-project/vllm/issues/1811
vllm,这是一个用户提出需求的issue，该问题涉及的主要对象是请求在CUDA 11.8、Torch 2.0.1和Python 3.9环境下发布一个适用的vllm.whl文件。这个问题由于当前vllm版本与CUDA 11.8不兼容，用户希望得到一个兼容的解决方案。,https://github.com/vllm-project/vllm/issues/1807
vllm,这是一个功能增强的issue，主要涉及到为VLLM添加对多个LoRA适配器的支持。原因是为了实现在单个批处理中运行多个LoRA适配器，以类似于SLoRA和punica项目的方式。,https://github.com/vllm-project/vllm/issues/1804
vllm,这个issue是一个用户提出需求的特性请求，主要涉及的对象是VLLM模型。由于当前的输入导向任务（比如总结和代码修改）的性能需求，用户希望实现Prompt lookup decoding (PLD)来提高模型的吞吐量。,https://github.com/vllm-project/vllm/issues/1802
vllm,这个issue是用户提出的需求。主要对象是将fastchat Conversation中的特殊tokens作为默认的停用词加入到OpenAI API服务器中。,https://github.com/vllm-project/vllm/issues/1800
vllm,这是一个功能需求问题，主要涉及对象是模型初始化在GPU上以减少CPU内存占用。这个问题可能是由于模型在CPU上初始化导致内存占用过高而提出的。,https://github.com/vllm-project/vllm/issues/1796
vllm,这是一个优化性能的issue，主要涉及的对象是模型执行过程。问题产生的原因是为了减少CPU开销，通过使用`torch.compile`与CUDA图形来实现优化。,https://github.com/vllm-project/vllm/issues/1795
vllm,这个issue属于技术改进需求，主要涉及对象为`BlockTable`和`BlockAllocator`，由于`BlockTable`的定义位置不当而导致`BlockAllocator`无法使用它。,https://github.com/vllm-project/vllm/issues/1791
vllm,这是一个用户提出需求的issue，主要涉及Internlm动态网络，并询问如何支持使上下文达到16k的问题。可能由于当前Internlm不支持这一功能，用户希望了解如何实现。,https://github.com/vllm-project/vllm/issues/1790
vllm,这是一个需求更改类型的issue，主要涉及到命名规范问题。由于实际功能与命名不符，用户提出应将部分类名更改，以更准确地反映其功能。,https://github.com/vllm-project/vllm/issues/1787
vllm,这是一个用户提出需求的类型。主要对象是模型加载到多个GPU。由于原因是需要将模型的一部分加载到一个GPU，另一部分加载到另一个GPU，导致用户寻求帮助。,https://github.com/vllm-project/vllm/issues/1778
vllm,这是一个用户提出需求的类型，主要涉及的对象是batch中长度不一的序列。由于序列长度的不均匀性可能影响GPU计算效率，用户想知道序列长度的均匀性对GPU计算的影响，希望获得相关建议。,https://github.com/vllm-project/vllm/issues/1773
vllm,"该issue是一个关于功能需求的问题，主要涉及VLLM中的量化方法问题。用户提出了关于未知量化方法""gptq""的问题，并询问该如何使用""gptq""模型。由于最新代码中未对""gptq""进行支持，导致用户遇到无法使用该模型的问题。",https://github.com/vllm-project/vllm/issues/1769
vllm,这是一个用户提出需求的issue，主要涉及的对象是支持长上下文长度模型。由于GPU VRAM大小限制，用户无法部署需要更长上下文长度的模型。,https://github.com/vllm-project/vllm/issues/1767
vllm,这个issue是一个功能增强请求，主要涉及到vLLM对于chat API的支持，由于作者替换了分支导致问题重现。,https://github.com/vllm-project/vllm/issues/1756
vllm,这是一个用户提出需求的issue，主要涉及支持Lookahead Decoding的建议。用户想知道在vLLM中是否可以实现Lookahead Decoding，并希望得到相关支持。,https://github.com/vllm-project/vllm/issues/1754
vllm,这是一个用户提出需求的issue，主要涉及添加worker registry service来托管多个vllm模型，并通过单个api网关进行访问。由于现有的vllm集成不能完全满足其需求，用户希望扩展功能，例如支持beam search等。,https://github.com/vllm-project/vllm/issues/1753
vllm,这是一个关于用户提出需求的issue，主要对象是关于加速视觉语言模型像LLaVa这样的多模态LLM，用户询问如何在给定示例中使用。,https://github.com/vllm-project/vllm/issues/1751
vllm,这个issue属于用户提出需求类型，主要涉及对象为支持 PP for LLaMA 以及支持 num_layers % pp_size != 0。原因可能是当前模型不支持PP以及优化工程和调度程序以实现完全流水线。,https://github.com/vllm-project/vllm/issues/1750
vllm,这个issue属于功能需求类型，主要涉及的对象是vLLM项目的主分支合并EmbeddedLLM/vllm-rocm分支，用户提出了需要动态选择CUDA或ROCm的功能，但尚需通过所有单元测试。问题的原因可能是为了增强PyTorch在CUDA或ROCm环境下的灵活性与适应性。,https://github.com/vllm-project/vllm/issues/1749
vllm,这个issue类型是需求提出，主要对象是vllm模型的greedy sampling功能。用户提出需要将greedy sampling中的top_p设置为0并将top_k设置为-1。,https://github.com/vllm-project/vllm/issues/1748
vllm,这是一个用户提出的需求类型的 issue，主要涉及的对象是 vllm 模型。由于 vllm 模型目前不支持设置随机种子，导致用户无法实现每次请求的推理都可复现。,https://github.com/vllm-project/vllm/issues/1744
vllm,这个issue是关于需求的，涉及到是否可以将lookahead decoding集成到vllm中。,https://github.com/vllm-project/vllm/issues/1742
vllm,这是一个需求提出类的issue，主要涉及到文档内容的补充需求。原因是引擎参数文档缺失，导致用户难以理解参数含义，需要浏览源代码来获取信息。,https://github.com/vllm-project/vllm/issues/1741
vllm,这个issue类型是文档更新，主要涉及的对象是README.md文件。由于文档链接需更新，导致出现修复需求。,https://github.com/vllm-project/vllm/issues/1730
vllm,这是一个关于需求的问题，主要涉及到从Google Cloud Storage存储桶而不是Hugging Face存储库下载模型的方式。这个问题可能是由用户希望通过指定GCP存储桶位置来下载模型而引起的。,https://github.com/vllm-project/vllm/issues/1729
vllm,这是一个用户需求报告，主要涉及的对象是解决vllm+cu118无法安装的问题，导致的主要症状是无法找到 libcudart.so.12 共享对象文件。,https://github.com/vllm-project/vllm/issues/1717
vllm,这个issue类型是功能改进（Feature Enhancement），主要涉及的对象是为所有模型添加AWQ支持，通过添加`ScaledActivation`。由于在quantized GPTBigCodeForCausalLM中出现了KeyError导致CC(load_weights)错误，需要修复这个问题。,https://github.com/vllm-project/vllm/issues/1714
vllm,这是一个用户提出需求的issue，涉及发布官方docker镜像，由于缺乏相关文档，用户希望推送docker镜像并给出操作指南。,https://github.com/vllm-project/vllm/issues/1709
vllm,该issue类型为需求提出，主要对象涉及vllm的分布式推断功能。这个问题是关于是否支持使用多台机器进行推断，用户想要了解是否vllm支持将一个实例在两台拥有单个GPU的机器上运行。,https://github.com/vllm-project/vllm/issues/1702
vllm,这是一个用户提出需求的issue，主要涉及的对象是Intel GPU，原因是希望开始在Intel GPU上上游SYCL内核支持。,https://github.com/vllm-project/vllm/issues/1699
vllm,这是一个功能需求提议，主要涉及的对象是VLLM项目中的HF模型配置。由于需要支持HF官方的量化方法（如AWQ和GPTQ），该提议表示需要读取HF模型配置中的量化配置。,https://github.com/vllm-project/vllm/issues/1695
vllm,这是一个用户请求删除issue的类型。该问题单涉及的主要对象是OOM llama 2。由于用户误操作或其他原因，导致用户在此issue中请求删除。,https://github.com/vllm-project/vllm/issues/1693
vllm,这是一个文档更新类的issue，主要涉及的对象是添加模型文档。原因是对代码重构后的文档需要进行更新。,https://github.com/vllm-project/vllm/issues/1692
vllm,这个issue类型是版本升级请求，主要涉及对象是项目的版本号。,https://github.com/vllm-project/vllm/issues/1689
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是支持Nemotron，由于缺乏Nemotron的支持，用户希望能够在VLLM中添加对Nemotron的支持。,https://github.com/vllm-project/vllm/issues/1686
vllm,这是一个用户提出需求的issue，主要涉及安装程序在Windows系统下无法在没有WSL的情况下运行的问题，原因是公司策略对GPU机器不能启用WSL，导致了CUDA runtime未找到的错误。,https://github.com/vllm-project/vllm/issues/1685
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm支持torch==2.1.0版本cu11.8，导致问题的原因可能是在指定的URL中找不到对应的vllm包。,https://github.com/vllm-project/vllm/issues/1680
vllm,这个issue是关于开发一个新功能的提案，涉及 VLLM 模型中的 cached multi-query attention。,https://github.com/vllm-project/vllm/issues/1679
vllm,这是一个关于功能需求的问题，主要涉及vllm是否支持 quantized INT4 和 INT8 模型的问题。可能由于用户需要在项目中使用此类模型，或者对未来是否会有相应支持有所关注。,https://github.com/vllm-project/vllm/issues/1678
vllm,这是一个用户提出需求的问题单，主要涉及对象是vllm模型。原因是用户希望在同一进程中运行多个vllm模型，以获取不同模型的对话输出。,https://github.com/vllm-project/vllm/issues/1676
vllm,这是一个用户提出需求的issue，主要涉及的对象是模型的token生成时间，由于用户想要打印每个生成token的时间戳，未完成的需求可能导致打印时间信息的bug。,https://github.com/vllm-project/vllm/issues/1675
vllm,该issue为用户提出需求类型，主要涉及Vllm模型如何与Streamlit UI连接的问题。由于用户希望通过Streamlit UI展示Vllm模型的输出，因此寻求代码示例来实现这一功能。,https://github.com/vllm-project/vllm/issues/1674
vllm,该issue类型为功能需求提案，在实验性功能中新增了前缀缓存支持，主要涉及VLLM模型的前缀缓存功能。,https://github.com/vllm-project/vllm/issues/1669
vllm,这个issue属于优化性质，主要涉及LLaMA项目的性能提升，减少CPU负担。,https://github.com/vllm-project/vllm/issues/1667
vllm,这是一个功能优化的issue，主要涉及迁移代码检查工具从`pylint`到`ruff`，开发者提到`ruff`实现了更多实用的规则，并且具有更高的检查速度和自动修复功能。,https://github.com/vllm-project/vllm/issues/1665
vllm,这是一个用户提出需求的issue，主要涉及的对象是openai requests，用户希望OpenAI可以在流式请求中包含输入、输出和总标记数的信息，因为当前的OpenAI后端在流式请求中并不包含这些信息，而这是一个用户非常要求的功能。,https://github.com/vllm-project/vllm/issues/1663
vllm,这是一个用户提出的需求。该问题单涉及的主要对象是VLLM项目的功能性。由于缺乏对Prometheus指标的支持，用户需要增加此功能以便更好地进行监控，特别是在运行多个实例时。,https://github.com/vllm-project/vllm/issues/1662
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm库的支持版本。由于vllm目前不支持Torch 2.1版本，用户希望能够添加对该版本的支持。,https://github.com/vllm-project/vllm/issues/1661
vllm,这是一个需求类型的issue，主要涉及Cached multi-query attention实现的修改。原因是为了使用当前的单个查询注意力内核轻微修改以实现缓存的多查询注意力。,https://github.com/vllm-project/vllm/issues/1659
vllm,这是一个关于优化性能的问题，涉及到如何使用 vllm 快速地提取嵌入，可能由于每次调用 generate 函数导致速度过慢。,https://github.com/vllm-project/vllm/issues/1654
vllm,这是一个用户提出的需求问题，主要涉及的对象是将 DeepSpeedMII 后端添加到 benchmark 脚本中。,https://github.com/vllm-project/vllm/issues/1649
vllm,这是一个功能需求提出的issue，主要对象是增加使用情况报告功能。,https://github.com/vllm-project/vllm/issues/1648
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM模型的 Sampler功能。由于用户尝试使用 min_p 参数进行推理时获得更好的结果，因此提出了支持 Min P Sampler 的需求。,https://github.com/vllm-project/vllm/issues/1642
vllm,这是一个功能增强类型的issue，涉及的主要对象是配置文件解析器。这个issue产生的原因是需要支持解析torch.dtype，以便更好地处理相关数据类型。,https://github.com/vllm-project/vllm/issues/1641
vllm,这个issue是关于用户提出需求的，涉及主要对象是vLLM和jais模型。造成这个问题是用户想了解vLLM是否支持jais模型。,https://github.com/vllm-project/vllm/issues/1638
vllm,这是一个关于功能使用问题的issue。主要涉及对象是vLLMOpenAI，用户提出无法在Langchain网站上实现批处理和异步批处理操作。,https://github.com/vllm-project/vllm/issues/1636
vllm,这个issue是用户提出的一个功能需求，涉及主要对象为prompt caching。由于长fewshot examples的重复编码导致效率低下，用户建议实现缓存重叠提示以便重复使用。,https://github.com/vllm-project/vllm/issues/1635
vllm,这是一个需求类型的issue，主要涉及对象是RoPE selection logic的移动操作。原因可能是为了优化代码结构或提高代码的可维护性。,https://github.com/vllm-project/vllm/issues/1633
vllm,这是一个用户提出需求的issue，主要对象是vLLM团队。用户希望默认支持添加01ai/Yi34B200K模型，因为当前需要手动添加，希望团队能够在默认配置中包含该模型。,https://github.com/vllm-project/vllm/issues/1632
vllm,该issue类型为性能优化，主要针对构建过程进行优化，减少构建时间。,https://github.com/vllm-project/vllm/issues/1624
vllm,这个issue类型是功能需求报告，主要涉及的对象是vllm项目中的quantization、weight loading 和 tensor parallelism 代码，用户提出了希望重构quantized linear logic，并扩展量化支持到所有模型的功能。,https://github.com/vllm-project/vllm/issues/1622
vllm,这是一个关于性能优化的问题，主要涉及对象为sampler CPU overhead，用户寻求减少CPU开销的帮助。,https://github.com/vllm-project/vllm/issues/1621
vllm,这是一个功能需求提出的issue，主要涉及的对象是config parser和ChatGLM2模块，用户希望在`_get_and_verify_max_len`方法中添加seq_length参数。,https://github.com/vllm-project/vllm/issues/1617
vllm,这是一个用户提出的需求问题单，主要涉及Llama模型定义与torch.compile的兼容性，导致用户寻求2倍速度提升。,https://github.com/vllm-project/vllm/issues/1615
vllm,这是一个功能需求，主要对象是API服务器，由于CC（输入被附加到输出）而导致需要更精细调控输出的问题。,https://github.com/vllm-project/vllm/issues/1613
vllm,这个issue是关于用户提出需求。主要对象是将S-Lora整合到vLLM中。根据用户的描述，用户希望能够在不需要将模型合并的情况下进行推理，这对于为Mixture of Expert模型提供服务或需要基于相同基础模型进行多个不同的finetuned lora适配器的服务非常有用。,https://github.com/vllm-project/vllm/issues/1610
vllm,该issue类型为用户提出需求，主要对象是请求在项目中支持Ascend系列显卡。由于最近的地缘政治情况导致中国开发者无法访问 NVIDIA 显卡，因此希望项目能够扩展对Ascend系列显卡的支持。,https://github.com/vllm-project/vllm/issues/1606
vllm,这个issue是关于用户提出需求的，主要涉及将Yi模型添加到量化支持中，源于Yi模型被添加到AutoAWQ和Yi的相关Pull Requests。,https://github.com/vllm-project/vllm/issues/1600
vllm,这是用户提出需求的类型的issue，主要涉及的对象是预加载过程中的分页注意力操作。用户提出了希望支持针对预加载的分页注意力，以便在每批中并行计算多个标记。,https://github.com/vllm-project/vllm/issues/1598
vllm,这是一个需求提出类型的issue，主要涉及构建与发布vLLM的CUDA11.8版本wheels，以及与CUDA 12.1版本wheels文件名的命名规则。,https://github.com/vllm-project/vllm/issues/1596
vllm,该issue类型是关于功能需求的，主要涉及如何在每次生成之前设置特定的种子。由于用户想要在每次生成文本前设置特定种子，而不必每次重新加载模型，因此提出了这个问题。,https://github.com/vllm-project/vllm/issues/1595
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM项目。由于CUDA版本兼容性的问题，用户建议支持旧版本的torch和CUDA，以便更多人可以安装vLLM。,https://github.com/vllm-project/vllm/issues/1589
vllm,这是一个功能需求提出的issue，主要涉及的对象是从 www.modelscope.cn 下载模型，由于原始版本不兼容，导致用户需要支持该功能。,https://github.com/vllm-project/vllm/issues/1588
vllm,这是一个请求帮助的issue，主要涉及vllm中key cache和value cache的形状问题，提问者想了解其中x的作用、为什么key和value形状不同以及形状中的34652代表什么。,https://github.com/vllm-project/vllm/issues/1586
vllm,这是一个用户提出需求的issue，主要涉及vllm框架中的qwenchat7b模型，用户想要改造生成输出为指令形式的数据。原因是用户希望获得chat方法来输出具有指令形式的数据。,https://github.com/vllm-project/vllm/issues/1583
vllm,该issue类型是用户提出需求，该问题单涉及的主要对象是vllm模型。由于用户想要使用vllm的chat方法而不仅仅是generate方法，因此提出了该问题。,https://github.com/vllm-project/vllm/issues/1582
vllm,该issue类型为功能需求，涉及主要对象为新增GPTQ的支持。由于希望最小化代码变更并避免与正在进行的重构工作可能发生的冲突，导致了此需求。,https://github.com/vllm-project/vllm/issues/1580
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM模型的批处理大小设置。用户想要测试不同批处理大小下vLLM的端到端延迟。,https://github.com/vllm-project/vllm/issues/1576
vllm,这是一个用户提出需求的类型的issue，主要涉及对VLLM支持稀疏性的问题。用户希望了解是否可以实现半结构稀疏性以降低推断延迟。,https://github.com/vllm-project/vllm/issues/1574
vllm,这是一个建议性的改进 issue，主要涉及的对象是LLAMA模型中的GPU内存缓存，提出在`LlamaDecoderLayer.forward()`方法末尾添加`torch.cuda.empty_cache()`以减轻模型前向传播过程中的GPU缓存增加问题。,https://github.com/vllm-project/vllm/issues/1571
vllm,该issue类型为用户提出需求，主要涉及vLLM是否支持Dynamic SplitFuse方法，原因是研究人员认为这是DeepSpeedFastGen优于vLLM的主要原因之一。,https://github.com/vllm-project/vllm/issues/1569
vllm,这是一个用户提出需求的类型的issue，主要对象是添加新功能的实现。用户提出了关于引入flash decoding++的请求。,https://github.com/vllm-project/vllm/issues/1568
vllm,这个issue是一个feature请求，主要涉及支持Yi34B模型。由于其模型架构不完全一致，用户在询问vLLM是否能够支持该模型。,https://github.com/vllm-project/vllm/issues/1567
vllm,该issue为用户提出需求，希望在vLLM中集成SwitchTransformer / NLLB-MoE，询问是否已经有相关工作并询问主要挑战是什么。,https://github.com/vllm-project/vllm/issues/1565
vllm,这是一个用户提出需求的issue，主要涉及的对象是在VLLM下支持加载MPT模型的功能缺失。用户询问关于在AWQ格式中加载MPT模型的支持时间以及是否有临时解决方法。,https://github.com/vllm-project/vllm/issues/1564
vllm,这是一个需求提出的issue，主要涉及到vLLM项目维护者和DeepSpeed框架，用户提出了关于实现Dynamic SplitFuse的需求。,https://github.com/vllm-project/vllm/issues/1562
vllm,这是一个用户提出需求的issue，主要涉及对象是增加对Baichuan2模型和多个LoRA适配器的支持。这个问题由于用户需要同时使用Baichuan213B和多个LoRA适配器，需要在批量推理中支持这些功能，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/1560
vllm,这是一个用户提出需求的issue，主要涉及的对象是 vllm 模型。用户询问是否有可能支持 LongLora 模型中使用的 short shift attention。,https://github.com/vllm-project/vllm/issues/1557
vllm,该issue类型属于用户需求提出，主要涉及对象是vllm_engine的请求处理机制。由于缺乏关于llm_engine中请求处理过程的详细说明和文档，用户提出了关于如何并行处理更多请求的问题。,https://github.com/vllm-project/vllm/issues/1555
vllm,这个issue属于用户需求类型，主要涉及vLLM是否支持Yi34B模型，并询问其支持情况及链接。,https://github.com/vllm-project/vllm/issues/1554
vllm,该issue属于用户提出需求类型，主要对象是为vLLM项目添加对ChatGLM3模型的支持。这个问题产生的原因是团队目前受到其他更高优先级工作的压倒，导致存在五个无法合并的过时PR，并需要社区志愿者的帮助将ChatGLM3模型推动完成。,https://github.com/vllm-project/vllm/issues/1552
vllm,这个issue类型是性能优化报告，主要涉及vLLM框架在2023年11月2日的性能瓶颈问题。原因可能是某些函数调用时消耗过多时间，需要进一步优化。,https://github.com/vllm-project/vllm/issues/1550
vllm,这是一个用户提出需求的特性请求，主要对象是控制HuggingFace模型中config.json文件的内容。由于用户希望在加载模型时能够自定义config.json文件中的数值，以实现对默认参数的修改。,https://github.com/vllm-project/vllm/issues/1542
vllm,这是一个用户提出需求的类型，主要涉及的对象是部署在Kubernetes上的应用程序。这个需求是由于在部署过程中，Ingress需要进行健康检查以确保Pod正常运行。,https://github.com/vllm-project/vllm/issues/1540
vllm,这是一个用户提出需求的issue，主要涉及 VLLM 下的 prompt_logprobs 功能，用户希望避免推理以节省运行时间。因此，该问题可能是关于优化运行时性能的建议或需求。,https://github.com/vllm-project/vllm/issues/1535
vllm,这个issue是关于用户提出需求的，主要涉及支持baichuan2模型和多lora适配器在单个批次中的使用。原因可能是为了提高效率和支持更多的硬件配置。,https://github.com/vllm-project/vllm/issues/1533
vllm,这是一个用户提出需求的issue，主要涉及系统提示模板的更改。这个问题由于用户希望能够调整默认系统提示以更好地适应任务要求。,https://github.com/vllm-project/vllm/issues/1530
vllm,这是一个变更请求类型的issue，主要涉及的对象是项目中的`MPTConfig`配置。由于MPT已经是HF Transformers中的一个常规模型，因此需要移除自定义的`MPTConfig`配置。,https://github.com/vllm-project/vllm/issues/1529
vllm,此issue属于用户提出需求类型，主要涉及对象是在VLLM模型中支持LoRA finetuned models。用户提出这个问题是因为需要在同一时间内使用多个LoRA适配器，并尝试自行实现这些功能。,https://github.com/vllm-project/vllm/issues/1523
vllm,这个issue类型是用户请求指导，主要涉及的对象是vllm工具，用户想要了解如何使用vllm计算标记的数量。,https://github.com/vllm-project/vllm/issues/1521
vllm,这个issue是关于用户提出需求的问题，主要涉及对象是vllm。原因是用户想要指定vllm在特定的GPU上运行。,https://github.com/vllm-project/vllm/issues/1517
vllm,这是一个用户提出需求的issue，涉及vllm能否同时处理两个提示，用户想了解是否可以同时处理多个问题。,https://github.com/vllm-project/vllm/issues/1516
vllm,这是一个用户提出需求的类型issue，主要涉及API服务器是否会自动批量处理用户请求，导致使用者无法设置批量大小而感到困惑。,https://github.com/vllm-project/vllm/issues/1515
vllm,这是一个类型为功能需求提议的issue，主要涉及的对象是代码质量检测工具和代码格式化工具的更替。用户提出替换pylint和yapf为ruff的原因是ruff速度快且被广泛采用，同时指出yapf存在格式化限制和不确定性的问题。,https://github.com/vllm-project/vllm/issues/1511
vllm,这是一个功能需求提议，针对VLLM中的长上下文问题。,https://github.com/vllm-project/vllm/issues/1510
vllm,这是一个用户需求类型的issue，主要针对vLLM类型的Python包，问题是关于添加 py.typed 文件以便消费者可以进行类型检查。,https://github.com/vllm-project/vllm/issues/1509
vllm,这是一个特性需求报告，涉及vLLM模型中的W8A8推理支持。其原因是为了实现更好的模型推理性能。,https://github.com/vllm-project/vllm/issues/1508
vllm,这是一个功能增强的issue，主要涉及支持int8 KVCache Quant在Vllm中的实现。原因是为了提高吞吐量而引入了此功能。,https://github.com/vllm-project/vllm/issues/1507
vllm,该问题属于用户提出需求类型，主要涉及VLLM与ChatGLM3的兼容性。这可能是因为用户希望在VLLM中添加对ChatGLM3的兼容性支持。,https://github.com/vllm-project/vllm/issues/1505
vllm,这是一个关于优化的issue，主要涉及到如何实现磁盘缓存功能，用户正在寻求针对大量相似prompt prefixes的优化方案。,https://github.com/vllm-project/vllm/issues/1495
vllm,这是一个用户提出需求的issue，主要是要求在vLLM中添加聊天模板支持。由于现有的FastChat无法处理模板，因此希望引入一种更标准化的模板处理方式，并增强对OpenAI Chat API的兼容性。,https://github.com/vllm-project/vllm/issues/1493
vllm,该问题类型是用户提出了需求，主要涉及的对象是GGUF支持，用户询问是否有计划或时间表支持GGUF。这个问题可能是由于用户希望了解软件是否计划支持GGUF，并想了解相关计划或时间表。,https://github.com/vllm-project/vllm/issues/1491
vllm,这是一个用户提出需求的issue，主要涉及的对象是是否可以根据某些条件将请求路由到不同的模型工作者。,https://github.com/vllm-project/vllm/issues/1484
vllm,这是一个关于用户需求的问题，主要涉及到在NVIDIA Launchpad上运行vllm，并询问是否支持cuda版本12.x。,https://github.com/vllm-project/vllm/issues/1482
vllm,这是一个关于功能支持的问题，主要涉及vLLM的CPU推理以及量化模型支持的问题。用户想了解vLLM在CPU上是否支持LLM推理以及量化模型，并询问vLLM和GGML之间的区别。,https://github.com/vllm-project/vllm/issues/1481
vllm,这是一个需求提出类型的issue，主要对象是OpenAI API中的model_check功能。由于检查模型名称的必要性存疑，用户建议删除或提供关闭选项。,https://github.com/vllm-project/vllm/issues/1478
vllm,这是一个用户提出需求的issue，主要对象是支持chatglm2。由于缺乏对chatglm2的支持，用户提出了这个问题。,https://github.com/vllm-project/vllm/issues/1477
vllm,这是一个关于需求探讨的issue，主要讨论了VLLM团队是否在考虑使用TorchDynamo和XLA进行优化，以及可能带来的冲突。用户主要关注VLLM是否能获得类似的性能提升。,https://github.com/vllm-project/vllm/issues/1475
vllm,这个issue是用户提出需求类型的问题，主要涉及的对象是部署模型到多个GPU，并且由于一些指导不清导致用户陷入困境。,https://github.com/vllm-project/vllm/issues/1473
vllm,这是一个用户提出需求的issue，主要涉及到VLLM生成请求中请求ID连续导致的问题，用户希望能够获取非连续的请求ID来区分请求状态。,https://github.com/vllm-project/vllm/issues/1471
vllm,这个issue类型是用户提出需求，主要涉及llm库中的模型引擎，用户询问如何触发处理待处理请求、获取已处理提示的输出、获取等待请求或中止所有请求的机制，并报告了处理过程中出现的错误。,https://github.com/vllm-project/vllm/issues/1470
vllm,这是一个用户提出的需求类型的issue，主要涉及的对象是vLLM的SamplingParams。由于缺乏API，用户提出了添加logits processor以允许自定义代码在模型生成logits后对其进行修改，以实现特定格式输出等需求。,https://github.com/vllm-project/vllm/issues/1469
vllm,这是一个用户提出需求的类型，该问题涉及的主要对象是支持同时处理提示和生成的功能。,https://github.com/vllm-project/vllm/issues/1468
vllm,这是一个需求提出的issue，该问题涉及到添加模型注册以支持私有模型。原因是用户目前需要在本地维护vLLM的内部版本，以便与专有模型一起运行。,https://github.com/vllm-project/vllm/issues/1467
vllm,这个issue类型是用户提出需求，主要涉及对象是vllm，用户询问如何在使用vllm时添加最大窗口大小。这可能是因为用户想要控制vllm窗口的大小，但目前无法实现。,https://github.com/vllm-project/vllm/issues/1466
vllm,这是一个功能需求类型的issue，主要涉及的对象是支持内存共享的查询提示。由于目前仅共享了前缀的内存，而针对每个查询都进行了计算，用户提出需要共享内存以提高吞吐量。,https://github.com/vllm-project/vllm/issues/1461
vllm,这是一个用户在Github上提出的需求问题，主要涉及到模板格式的更改。由于用户在使用mistral模型时，发现默认模板格式不适用，希望了解如何修改格式。,https://github.com/vllm-project/vllm/issues/1460
vllm,这个issue类型属于需求提出，主要对象是Aquila模型中缺少rope_scaling参数，导致某个特定模型无法正常运行。,https://github.com/vllm-project/vllm/issues/1457
vllm,这是一个需求更改类型的问题，主要涉及VLLM中的Tensor Parallel功能，用户提出需要取消此功能，以便按顺序处理请求。,https://github.com/vllm-project/vllm/issues/1456
vllm,这是一个用户提出需求和请教问题的issue，主要涉及到vllm支持高吞吐量的参数设置和性能优化，用户想获取关于处理大量用户负载、优化设置以及处理更多并发请求的指导和建议。,https://github.com/vllm-project/vllm/issues/1452
vllm,这是一个用户提出需求的issue，涉及对象是使用vLLM加载模型时内存不足的问题，用户询问增加tensor_parallel_size参数是否可以解决该问题，并咨询是否有其他方法可以处理。,https://github.com/vllm-project/vllm/issues/1449
vllm,这个issue类型是用户提出需求，主要涉及对象是添加Locally Typical Sampling功能，用户希望在vLLM中加入typical_p参数，并表达了希望通过这个功能实现更好的文本生成效果的需求。,https://github.com/vllm-project/vllm/issues/1444
vllm,这是一个用户提出需求的issue，主要涉及的对象是支持pipeline parallel，原因是因为tensor parallel需要偶数个GPU，所以需要支持任意数量的GPU。,https://github.com/vllm-project/vllm/issues/1443
vllm,该issue属于用户提出需求类型，主要对象是项目中的pre-commit hooks。由于用户认为将format.sh加入pre-commit hooks会比直接运行format.sh更好，因此提出了这个需求。,https://github.com/vllm-project/vllm/issues/1442
vllm,这是一个性能优化相关的issue，主要涉及到CUDA graph在Llama 2模型前向传播过程中的应用。原因是CUDA graph在低批次大小下可以略微提高前向传播速度，但在较大批次大小下则没有明显的收益。,https://github.com/vllm-project/vllm/issues/1440
vllm,这是一个用户提出需求的issue，主要涉及的对象是将vLLM与PyCharm IDE集成，用户希望寻找一个Jetbrain Plugin兼容vLLM并且类似Copilot的功能。,https://github.com/vllm-project/vllm/issues/1439
vllm,这是一个用户提出需求的类型，主要涉及到vLLM项目的Docker镜像的构建和自动构建问题。原因可能是用户希望在类似runpod这样的服务上更轻松地进行推断。,https://github.com/vllm-project/vllm/issues/1438
vllm,该issue类型为功能需求提议，主要涉及添加基于tgi的vllm端点。,https://github.com/vllm-project/vllm/issues/1434
vllm,这是一个用户需求提出的issue，主要涉及的对象是为vLLM添加对StableLM-3B的支持。,https://github.com/vllm-project/vllm/issues/1426
vllm,这是一个功能需求类型的问题，主要涉及对象为VLLM模型。这个需求是关于支持重复惩罚(repetition_penalty)，用户希望系统支持此功能。,https://github.com/vllm-project/vllm/issues/1424
vllm,这是一个需求讨论类的issue，主要涉及vLLM调度器无法对整个序列组进行调度，而只能调度其中的一部分序列。导致该问题的原因是GPU内存不足以容纳整个序列组。,https://github.com/vllm-project/vllm/issues/1423
vllm,这是一个优化建议类型的issue，涉及的主要对象是vllm中的decoder模块。因为在prompt阶段，建议使用最后一个token对应的隐藏状态作为查询计算注意力，从而避免不必要的计算和函数调用。,https://github.com/vllm-project/vllm/issues/1418
vllm,这个issue类型是功能改进，主要涉及的对象是优化CUDA kernel执行top-k和top-p操作，导致原功能较慢并且内存占用较大。,https://github.com/vllm-project/vllm/issues/1416
vllm,该issue类型为功能需求，主要涉及添加Docker文件到项目中以及优化构建速度。,https://github.com/vllm-project/vllm/issues/1415
vllm,这是一个用户提出需求的issue，主要涉及vllm不支持跨不同查询缓存前缀，用户询问是否能够修改vllm实现以支持此功能，问题根源在于vllm未能利用不同查询间的共享前缀进行缓存。,https://github.com/vllm-project/vllm/issues/1414
vllm,这是一个改进建议类型的issue，主要涉及LLMEngine demo脚本的重构，旨在提升代码的清晰度和模块化。,https://github.com/vllm-project/vllm/issues/1413
vllm,这是一个用户提出需求的issue， 主要关注对象是 Falcon180BAWQ 的支持问题，用户想知道是否有计划在不久的将来支持此功能。,https://github.com/vllm-project/vllm/issues/1410
vllm,这个issue类型是用户提出需求，主要对象是vllm模型。由于该模型目前不支持repetition_penalty参数，用户希望在未来的版本中能够添加该参数以提高生成文本的多样性。,https://github.com/vllm-project/vllm/issues/1406
vllm,这是一个关于软件兼容性的建议类型的issue，主要涉及的对象是要升级的pydantic库版本。原因是由于Ray的更新，现在兼容pydantic v2，因此用户提出升级pydantic版本的需求。,https://github.com/vllm-project/vllm/issues/1396
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM项目的重复惩罚功能。由于主要开发者未合并相关代码，并有关于认证支持的疑问，用户决定自行将代码合并到自己的分支。,https://github.com/vllm-project/vllm/issues/1392
vllm,这是一个用户提出需求的类型，主要对象是vllm模型的参数设置。用户可能由于缺乏对vllm模型参数的了解而提出如何设置参数以最小化模型性能下降的问题。,https://github.com/vllm-project/vllm/issues/1390
vllm,这是一个关于技术需求的issue，主要涉及到如何实现对多个用户进行批量推理服务的问题。用户提出这个问题是因为想了解如何实现类似于chatGPT这样的流式批量推理服务。,https://github.com/vllm-project/vllm/issues/1387
vllm,这是一个用户提出需求的issue，主要涉及VLLM模型为何不使用Apex进行推断加速，可能是用户想了解为什么VLLM没有集成Apex来提升推断速度。,https://github.com/vllm-project/vllm/issues/1384
vllm,这是一个需求更改类型的issue，主要涉及调度器和输入张量形状的更改。导致这个更改的原因是为了能够使用更广泛的库和硬件，并为未来的优化提供基础。,https://github.com/vllm-project/vllm/issues/1381
vllm,这是一个需求类型的issue，主要涉及的对象是在构建项目wheel时使用的PyTorch版本。由于已将PyTorch版本固定为2.0.1，因此需要在工作流程中进行相应修复。,https://github.com/vllm-project/vllm/issues/1377
vllm,这是一个功能需求类型的issue，主要涉及的对象是vLLM项目中的数据收集和项目路线规划。原因是缺乏对硬件类型、性能计数器、云服务提供商和用户架构使用情况的可见性和信号，影响了项目优先级的确定。,https://github.com/vllm-project/vllm/issues/1376
vllm,这是一个关于优化利用率的问题，不是bug报告，主要涉及的对象是VLLM模型以及利用率优化。用户询问了如何针对较小的模型提高利用率的问题。,https://github.com/vllm-project/vllm/issues/1375
vllm,该issue类型为用户的问题/需求，主要对象是Nvidia Tesla P40 GPU。用户询问关于在7B模型上使用Nvidia Tesla P40 GPU生成tokens的速度。,https://github.com/vllm-project/vllm/issues/1374
vllm,这是一个用户提出需求的issue，主要涉及VLLM是否支持多轮对话中的KV-cache功能，用户询问了关于多轮对话中是否共用缓存的问题。,https://github.com/vllm-project/vllm/issues/1371
vllm,这个issue类型为功能增强（Feature Enhancement），涉及的主要对象是将Mistral 7B模型添加到`test_models`中。,https://github.com/vllm-project/vllm/issues/1366
vllm,这是一个用户提出的需求类型的issue，主要涉及的对象是在VLLM项目中添加对HuggingFace Chat Templates的支持。,https://github.com/vllm-project/vllm/issues/1365
vllm,这是一个需求类型的issue，主要涉及对象是conversation template的来源问题。由于fastchat只是一种替代方式，导致用户希望使用更准确的huggingface tokenizer template。,https://github.com/vllm-project/vllm/issues/1361
vllm,"这是一个用户提出需求的issue，主要涉及项目的readme.md文件。由于readme.md文件中缺少""hi""，用户请求在文件中添加该内容。",https://github.com/vllm-project/vllm/issues/1358
vllm,这是一个用户提出需求的issue，主要涉及vLLM支持添加Token Filtering API功能。,https://github.com/vllm-project/vllm/issues/1357
vllm,这个issue类型为发布新版本的需求，涉及主要对象为版本控制。原因是需要升级版本到v0.2.1，并发布相应的Release Tracker。,https://github.com/vllm-project/vllm/issues/1355
vllm,这是一个用户提出需求的issue，主要对象是在vLLM中添加dockerfile。用户关注的问题是docker镜像的大小过大，是否需要进一步减小镜像大小。,https://github.com/vllm-project/vllm/issues/1350
vllm,这是一个用户提出需求的issue，主要涉及的对象是实现PagedAttention V2 kernel。通过引入sequence-level parallelism，V2 kernel在小批量大小时能够获得巨大的加速，这说明用户希望通过实现V2版本的kernel来优化算法的性能。,https://github.com/vllm-project/vllm/issues/1348
vllm,这是一个需求跟踪的issue，针对的主要对象是软件项目的发布跟踪。,https://github.com/vllm-project/vllm/issues/1346
vllm,这是一个用户提出需求的issue，主要涉及对象是V100，用户询问是否有支持V100的量化计划，并提出了一些需要修改的地方。,https://github.com/vllm-project/vllm/issues/1345
vllm,该问题单属于用户提出需求类型，主要对象是vllm在k8s上进行健康检查。由于当前健康检查方式相对简单且请求资源消耗较大，用户提出添加简单计算以提供更有效的检查。,https://github.com/vllm-project/vllm/issues/1343
vllm,这个issue是一个用户提出需求，询问AWQ是否支持Qwen baichuan model。原因可能是用户想要了解该模型的支持情况或寻求相关帮助。,https://github.com/vllm-project/vllm/issues/1341
vllm,这是一个性能优化类的issue，主要涉及detokenization功能的优化。这个问题的目的是通过两个主要更改提高detokenization速度，由于快速tokenizer不需要使用慢速的`_convert_tokens_to_string_with_added_encoders`循环，并使用缓存属性来提高速度。,https://github.com/vllm-project/vllm/issues/1338
vllm,这是一个性能优化类型的issue，主要涉及到GPU与CPU同步时机的调整。原因是为了解决在之前PR中引入的轻微性能回退问题。,https://github.com/vllm-project/vllm/issues/1337
vllm,这是一个需求类型的issue，主要涉及 Lightweight (C++ / Native) Runtime，由于Roadmap中未提及轻量级运行时的跟踪问题，因此用户创建了这个问题以便让感兴趣的人能够订阅更新。,https://github.com/vllm-project/vllm/issues/1336
vllm,这是一个用户需求提出的类型的issue，主要涉及的对象是更新README.md文件以包含有关Aquila2模型的信息。由于发布了Aquila2模型并将其开源在HuggingFace上，用户希望更新README.md文件，以便包含有关Aquila2模型的信息。,https://github.com/vllm-project/vllm/issues/1331
vllm,这是一个功能需求的issue，涉及主要对象是vLLM模型。通过添加`prompt_logprobs`支持返回提示标记的对数概率，以支持OpenAI服务器中的`echo`功能，同时优化`logprobs`逻辑以进行批处理查询，可能会导致合并冲突并需要性能测试。,https://github.com/vllm-project/vllm/issues/1328
vllm,"这个issue是一则关于""特性添加""的请求，主要涉及的对象是为VLLM添加对SqueezeLLM量化方法的支持。由于用户希望在项目中运行4位稠密非均匀量化方案，因此请求添加相应的内核和量化配置文件。",https://github.com/vllm-project/vllm/issues/1326
vllm,这是一个用户提出需求的issue，主要涉及VLLM对Qwen长聊天的支持问题，可能是由于VLLM目前还不支持Qwen的ntk_alpha方法，导致用户无法在VLLM中对Qwen的长聊天进行支持。,https://github.com/vllm-project/vllm/issues/1323
vllm,这个issue类型是性能优化，涉及的主要对象是尝试用RPyC替代Ray实现，由于数据传输效率低导致了tokens生成速度较慢的问题。,https://github.com/vllm-project/vllm/issues/1318
vllm,这是一个用户提出需求的issue，主要涉及的对象是AsyncLLMEngine，用户想确认在1x1流量的情况下，AsyncLLMEngine是否会自动形成更大的批次来最大化GPU内存利用率。,https://github.com/vllm-project/vllm/issues/1317
vllm,这是一个关于需求和技术细节方面的问题，涉及主要对象是关于continuous batching的实现，由于连续批处理可能会导致prompt阶段和生成阶段混合的情况，用户询问如何处理这种情况。,https://github.com/vllm-project/vllm/issues/1316
vllm,这个issue类型为用户提出需求，涉及的主要对象是vllm下的`generate()`方法。用户提出疑问是关于`generate()`方法最大能接受多少个prompts，可能由于对vllm的使用需求而需要获取更多关于方法的详细信息。,https://github.com/vllm-project/vllm/issues/1315
vllm,这是一个建议性质的issue，涉及主要对象为`entrypoints.openai.api_server`实现。由于需要使用`monitoring`功能并考虑到`prometheusfastapiinstrumentator`的依赖关系，用户提出了是否将其加入代码库的建议。,https://github.com/vllm-project/vllm/issues/1310
vllm,这个issue类型是用户提出需求，该问题单涉及的主要对象是添加qwen-int4支持。由于原先未提供qwen-int4支持，用户提出了这个需求。,https://github.com/vllm-project/vllm/issues/1307
vllm,这是一个用户提出需求的issue，主要涉及支持Attention Sink，由于需要实现Efficient Streaming Language Models，用户请求支持此功能。,https://github.com/vllm-project/vllm/issues/1304
vllm,该issue类型为用户提出需求，主要对象是Python API。由于用户无法通过Python API获得可以迭代的token流，因此请求提供示例代码以展示如何直接在Python中实现这一功能。,https://github.com/vllm-project/vllm/issues/1302
vllm,该issue是用户提出需求类型，主要涉及VLLM缺乏支持Embedding功能将导致无法构建知识库的问题。,https://github.com/vllm-project/vllm/issues/1296
vllm,这是一个功能需求的issue，主要涉及的对象是OpenAI端点的支持情况。这个问题是由于vLLM API缺少对conversationtemplate参数的支持，导致用户提出了在启动API时指定conversationtemplate路径的需求。,https://github.com/vllm-project/vllm/issues/1294
vllm,这是一个关于需求类型的问题，主要涉及Docker镜像。用户想知道是否有任何Dockerfile或者官方Docker镜像。,https://github.com/vllm-project/vllm/issues/1293
vllm,这是一个内容更新的类型，主要涉及更新README.md文件，用户提出了添加meetup slides到最新动态部分以及删除与TGI相关的过时性能比较的建议。,https://github.com/vllm-project/vllm/issues/1292
vllm,这是一个功能请求的issue，主要涉及模型实例在GPU内存中的利用，用户提出希望vLLM支持在单个GPU上同时运行多个模型实例的需求。Bug报告,https://github.com/vllm-project/vllm/issues/1287
vllm,这是一个用户提出需求的issue，主要涉及对象是LLaVA项目。由于用户想了解如何支持LLaVA，导致提出了这个问题。,https://github.com/vllm-project/vllm/issues/1286
vllm,这是一个功能需求提议的issue，主要涉及的对象是vllm项目的OpenAI API功能。由于无法使所有cuda版本一致导致本地构建失败，用户尝试暴露`logit_bias`功能，并希望其他用户尝试解决问题。,https://github.com/vllm-project/vllm/issues/1279
vllm,这个issue类型是功能需求，主要涉及对象是更新模型加载器(model_loader.py)，原因是为了添加MistralForCausalLM awq量化解决方案。,https://github.com/vllm-project/vllm/issues/1278
vllm,该issue类型是用户提出需求，涉及主要对象为使用正则表达式进行后缀匹配。由于处理长的stop words列表时可能会过于昂贵，用户提出了一种改进建议。,https://github.com/vllm-project/vllm/issues/1275
vllm,这是一个用户提出需求的issue，主要涉及vLLM项目的多提示批量支持问题，用户想要了解是否计划支持这一功能。,https://github.com/vllm-project/vllm/issues/1270
vllm,这是一个用户提出需求的类型的issue， 主要对象是希望vLLM添加对PolyLM模型的支持。用户提出这个需求是因为PolyLM是一个明确支持多语言的语言模型，并在基准测试中表现出色， 对于想要使用非英语或中文语言模型的用户来说，PolyLM的支持将会非常有帮助。,https://github.com/vllm-project/vllm/issues/1266
vllm,这是一个用户需求的issue，主要涉及生成文本的功能，提出了关于实现从embedding输入生成文本的需求。,https://github.com/vllm-project/vllm/issues/1265
vllm,这是一个用户提出需求类型的issue，涉及主要对象为实现YaRN模型支持，由于之前的基础工作，现在需要实现对YaRN模型的支持。,https://github.com/vllm-project/vllm/issues/1264
vllm,这个issue是一个功能增强需求，涉及到OpenAI API server中模板选择的问题，请求支持在请求中指定模板名称。,https://github.com/vllm-project/vllm/issues/1263
vllm,这是一个用户提出需求的类型，主要涉及ChatGLM 2的实现。原因可能是要改进vLLM的功能并为模型推断提供更好的支持。,https://github.com/vllm-project/vllm/issues/1261
vllm,这是一个用户提出需求的issue，主要涉及的对象是vllm中的频率惩罚功能。用户提出的问题是频率惩罚对某些特定token或字符串不起作用，希望添加一个功能来处理这种情况。,https://github.com/vllm-project/vllm/issues/1257
vllm,这是一个用户提出需求的类型，用户在询问是否有计划支持Intel ARC。,https://github.com/vllm-project/vllm/issues/1256
vllm,这是一则用户提出需求的issue，主要涉及了StreamingLLM在VLLM下的支持问题，用户提出了关于是否可以实现该功能的疑问。,https://github.com/vllm-project/vllm/issues/1253
vllm,这个issue是关于代码优化和功能改进的，涉及到调度器（Scheduler）和策略（policy）的模块化重构。,https://github.com/vllm-project/vllm/issues/1251
vllm,这是一个功能需求的issue，主要涉及的对象是benchmarking脚本，用户提出了添加Triton作为后端选项的需求。,https://github.com/vllm-project/vllm/issues/1250
vllm,这是一个建议性质的issue，涉及主要对象是计算时间持续的方式，由于wall clock时间会倒退，建议使用单调时间来计算。,https://github.com/vllm-project/vllm/issues/1249
vllm,这个issue为用户提出需求，要求为`mistral7binstructv0.1`模型提供量化支持，由于目前无法对该模型进行量化，导致无法进行推理操作。,https://github.com/vllm-project/vllm/issues/1247
vllm,这个issue是一个功能需求反馈，主要涉及到在vLLM的api_server.py中缺少`readinessProbe`和`livenessProbe`的功能，导致在Kubernetes上部署LLM模型时无法指示apiserver的健康状态。,https://github.com/vllm-project/vllm/issues/1245
vllm,该issue类型为用户提出需求，涉及到更新`api_server`以包含`livez`和`readyz`端点，用于Kubernetes部署，因为这对健康检查非常有帮助。,https://github.com/vllm-project/vllm/issues/1244
vllm,这个issue属于功能增强类型，主要涉及了对于受限解码的改进。原因是为了让用户能够生成特定的标记。,https://github.com/vllm-project/vllm/issues/1243
vllm,这是一个功能需求类的issue，主要涉及日志记录功能，要求默认情况下禁用详细请求日志记录，默认情况下不再记录详细请求信息并提供了新的开关控制功能。,https://github.com/vllm-project/vllm/issues/1240
vllm,这是一个关于需求的issue，主要涉及的对象是关于模型的并行推断方法。用户想知道是否存在更干净的方法来实现数据并行推理，而不是通过修改CUDA_VISIBLE_DEVICES的方式。,https://github.com/vllm-project/vllm/issues/1237
vllm,这个issue类型是用户提出需求，主要涉及的对象是添加关于如何在vLLM中使用AutoAWQ模型的文档。由于用户希望了解如何结合使用AutoAWQ模型和vLLM，所以提出了更新文档的请求。,https://github.com/vllm-project/vllm/issues/1235
vllm,这个issue类型为用户提出需求，主要涉及的对象是`api_server`入口点。由于vLLM会返回带有结果的提示，用户提出引入`return_prompt`请求选项来解决这个问题。,https://github.com/vllm-project/vllm/issues/1232
vllm,这个issue是一个用户提出的需求。用户希望创建一个示例，演示如何使用Ray Data和vLLM进行分布式离线推断。,https://github.com/vllm-project/vllm/issues/1230
vllm,这是一个用户提出需求的issue，主要涉及的对象是该库的功能扩展，用户希望该库能够支持Grammar和GBNF文件。,https://github.com/vllm-project/vllm/issues/1229
vllm,这个issue属于功能增强类型，主要涉及benchmarking脚本，添加了`dtype`参数以增加与某些特定量化模型的兼容性，比如`TheBloke/CodeLlama13BInstructAWQ`。,https://github.com/vllm-project/vllm/issues/1228
vllm,这个issue类型是功能需求，涉及的主要对象是FastAPI服务器。这个问题是由于部署LLMs在AWS ALB上使用路径前缀时，ALB不支持路径重写，导致请求中包含了不正确的路径，从而破坏了API的正常使用，用户希望能够设置自定义的服务器基本路径来解决这个问题。,https://github.com/vllm-project/vllm/issues/1227
vllm,这个issue类型是提出需求，涉及的主要对象是部分模型，由于部分模型缺少一些属性，导致需要为其提供默认值以避免错误和提醒用户。,https://github.com/vllm-project/vllm/issues/1224
vllm,这是一个用户提出需求的类型的issue，主要对象是使Mistral成为支持的模型，可能由于现有模型列表不包含Mistral，用户希望将其添加到支持的模型列表中。,https://github.com/vllm-project/vllm/issues/1221
vllm,这是一个用户提出需求的issue，主要对象是vllm模型。用户想要添加对对比搜索的支持，因为研究表明这可以明显改善模型质量，并且目前在transformers HF库中已经支持，希望vllm也可以具备这一功能。,https://github.com/vllm-project/vllm/issues/1219
vllm,这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM，用户希望该工具能够全面支持Python 3.12，并包括在CI中进行测试以及将wheels上传至PyPI。,https://github.com/vllm-project/vllm/issues/1218
vllm,这个issue是一个需求提出类型，主要对象是GitHub Actions更新的依赖配置。原因导致用户寻求帮助是为了主动保持GitHub Actions工作流程的更新和安全性，并减少维护工作的负担。,https://github.com/vllm-project/vllm/issues/1217
vllm,该issue属于用户提出需求类型，主要涉及到VLLM库是否支持来自META官方实现，或者只支持Hugging Face风格。此问题可能由用户对库的支持范围和兼容性产生疑问。,https://github.com/vllm-project/vllm/issues/1216
vllm,这是一个用户需求提出的issue，主要对象是希望为OpenAI API服务器指定聊天提示模板。,https://github.com/vllm-project/vllm/issues/1215
vllm,这是一个用户需求类型的issue，主要涉及到带有per-request seed支持的需求。,https://github.com/vllm-project/vllm/issues/1211
vllm,"这是一个用户提出需求的类型，主要涉及的对象是“qwen model”；用户可能需要添加一个名为""rope_scaling""的功能或特性到模型中。",https://github.com/vllm-project/vllm/issues/1210
vllm,这是一个用户提出需求的issue，主要涉及支持在超过8个GPU上进行sharding llama270b，由于现有代码不支持这一功能，用户寻求帮助解决这一问题。,https://github.com/vllm-project/vllm/issues/1209
vllm,这是一个用户提出需求的类型，用户想知道是否可以使用MLCAI库来编译LLM模型。,https://github.com/vllm-project/vllm/issues/1204
vllm,这是一个用户提出需求的类型的issue，涉及的主要对象是vLLM模型。用户询问vLLM是否需要额外更改才能支持Mistral 7B并指出了使用滑动窗口注意力可能需要在vLLM端进行小的修改。,https://github.com/vllm-project/vllm/issues/1199
vllm,这个issue属于功能改进类，主要对象涉及`max_num_batched_tokens`参数；由于默认值2560不适用于所有模型，导致需要根据模型的最大长度来动态设置`max_num_batched_tokens`参数。,https://github.com/vllm-project/vllm/issues/1198
vllm,这是一个用户提出需求的issue，主要对象是Mistral-7B-v0.1支持。,https://github.com/vllm-project/vllm/issues/1196
vllm,这是一个用户提出需求的issue，主要对象是pip package vllm，用户请求发布新版本0.1.8以支持量化（AWQ），由于当前版本0.1.7不具备该功能。,https://github.com/vllm-project/vllm/issues/1194
vllm,这是一个关于需求的问题，主要涉及对象是vllm的大型模型，用户提出如何将fp16转换为int8。这可能是由于用户在模型部署或性能优化过程中遇到了类型转换的问题而提出的。,https://github.com/vllm-project/vllm/issues/1190
vllm,这是一个用户提出需求的issue，涉及的主要对象是VLLMs的配置参数`max_num_batched_tokens`，用户希望将其默认值设置为4096或8192来更好地支持当前4096/8192个prompt的模型。,https://github.com/vllm-project/vllm/issues/1189
vllm,"该issue类型为用户提出需求，主要对象是BartForSequenceClassification模块，用户希望实现类似""facebook/bartlargemnli""的速度需求。",https://github.com/vllm-project/vllm/issues/1187
vllm,这是一个用户提出需求的issue，主要涉及的对象是VLLM模型。原因是目前模型无法生成特殊意义的特殊标记，用户希望通过添加可选参数解决这个问题。,https://github.com/vllm-project/vllm/issues/1186
vllm,这是一个修改请求（PR），涉及到`tensor parallel/quantization/weight loading refactor`，主要涉及`vLLM`的并行线性逻辑简化。,https://github.com/vllm-project/vllm/issues/1181
vllm,这是一个文档改进类型的issue，涉及主要对象是RoPE初始化，可能是因为原始代码缺乏详细注释导致用户希望增加更多解释性注释。,https://github.com/vllm-project/vllm/issues/1176
vllm,该issue属于用户提出需求类型，主要涉及的对象是如何结合vllm与其他库/技术以进一步加速推理，可能由于推理的速度较慢导致用户希望探讨是否有其他库/技术可用于在vllm基础上实现次秒级性能。,https://github.com/vllm-project/vllm/issues/1175
vllm,这是一个用户提出需求的issue，主要关注在如何在超过单个GPU容量的模型上进行推断。造成这个问题的原因是在4090 GPU上选择13b模型时，模型没有被跨越两个GPU进行切分。,https://github.com/vllm-project/vllm/issues/1174
vllm,这是一个功能需求类型的issue，主要涉及的对象是qwen-14b模型。由于缺乏对qwen7b和qwen14b的支持，用户提出了需要同时支持这两个模型的需求。 ,https://github.com/vllm-project/vllm/issues/1173
vllm,这是一个用户提出需求的issue，主要涉及到对于prompt注意力机制的建议。,https://github.com/vllm-project/vllm/issues/1170
vllm,这是一个用户提出需求的issue，主要涉及LLM Engine的更详细文档的需求。,https://github.com/vllm-project/vllm/issues/1168
vllm,该issue类型为用户提出需求，主要涉及对象为VLLM模型Phi 1.5。由于Phi 1.5是Microsoft推出的新模型，用户希望支持该模型并指出了一些特殊功能和需求，如MixFormerSequentialConfig和4位支持。,https://github.com/vllm-project/vllm/issues/1167
vllm,这个issue属于功能需求类型，主要涉及的对象是`uvicorn`模块。这个问题是为了添加标准的额外依赖项，以便在安装和使用`uvicorn`时推荐使用一些推荐的额外依赖项，其中包括`uvloop`，它是asyncio的高性能替代品。 Windows系统上不支持`uvloop`，`uvicorn`会在Windows系统上回退到使用asyncio。,https://github.com/vllm-project/vllm/issues/1166
vllm,这是一个用户提出需求的类型，主要涉及的对象是`VLLM_VERBOSITY`环境变量。,https://github.com/vllm-project/vllm/issues/1165
vllm,这是一个用户提出需求的issue，主要涉及对象是tokenizer revision，在原因可能是tokenizer版本不受支持导致用户无法指定版本的问题。,https://github.com/vllm-project/vllm/issues/1163
vllm,这是一个用户提出需求的issue，主要涉及的对象是vLLM的文档更新。由于Langchain对vLLM的支持需要更新文档并添加使用说明，因此用户提交了这个issue。,https://github.com/vllm-project/vllm/issues/1162
vllm,这是一个需求提出类型的issue， 主要涉及YaRN模型在vLLM下的实现和测试，用户提出了将YaRN实现在vLLM中并进行验证的需求。,https://github.com/vllm-project/vllm/issues/1161
vllm,这是一个用户提出需求的 issue，主要对象是将vllm-client仓库转移到vllm-project组织。由于需要将Python客户端库更改为vllm的组织，用户请求转移该仓库或者允许其执行转移操作。,https://github.com/vllm-project/vllm/issues/1159
vllm,这是一个关于需求的问题，主要涉及对象是VLLM模型加载，用户询问是否支持加载量化模型以便在其GPU上运行推断。,https://github.com/vllm-project/vllm/issues/1155
vllm,这是一个需求提出的issue，主要涉及的对象是在attention kernel中分配更多共享内存，导致的bug或问题是支持更长的上下文长度。,https://github.com/vllm-project/vllm/issues/1154
vllm,这是一个用户提出需求的问题，关注对象是是否支持 Whisper，可能是因为用户想要了解如何在项目中使用 Whisper 或者优化支持 Whisper 的对话模型。,https://github.com/vllm-project/vllm/issues/1152
vllm,这是一个用户发布需求的issue，主要对象是vLLM社区，由于需要组织第一次vLLM见面会而发布这个问题。,https://github.com/vllm-project/vllm/issues/1148
vllm,这个issue类型属于用户提出需求，主要涉及修改benchmark_throughput.py以使用多轮提示来进行吞吐量基准测试。导致用户提出此需求的原因是希望使用多轮提示更贴近实际情况。,https://github.com/vllm-project/vllm/issues/1139
vllm,这是一个关于增加对 GPT2LMHeadModel 的量化支持的需求提出的issue，不是bug报告。,https://github.com/vllm-project/vllm/issues/1138
vllm,这是一个用户提出需求的类型，主要对象是将vLLM部署在容器中，以节省时间。,https://github.com/vllm-project/vllm/issues/1135
vllm,这是一个用户提出需求的issue，主要对象是vLLM engine。原因是用户希望vLLM engine添加一个可选的回调函数，用于提供特定的运行统计信息，以便将这些信息导出到Prometheus指标中，以便在生产环境中可以根据这些信息将请求路由到不同机器上运行的vLLM engine实例。,https://github.com/vllm-project/vllm/issues/1132
vllm,"这是一个用户提出需求的issue，主要涉及""vLLM""在将本地训练的模型添加到Hugging Face时的需求。由于预先条件要求上传到Hugging Face，用户提出是否可以直接服务于本地目录中保存的预训练Transformer模型。",https://github.com/vllm-project/vllm/issues/1131
vllm,这是一个功能需求提交的Issue，主要涉及对象是支持在更多于8个GPU上进行分片的llama270b，该需求的原因是目前Llama2仅有8个KV头，无法在超过8个GPU上运行Llama270b。,https://github.com/vllm-project/vllm/issues/1122
vllm,这是一个用户提出需求的issue，主要涉及实验结果的展示是否考虑到了延迟和吞吐量在不同并发情况下的问题。,https://github.com/vllm-project/vllm/issues/1117
vllm,这是一个用户提出需求的issue，主要涉及的对象是engine的`dtype`参数。用户提出需要将`float16`和`float32`添加到dtype选择中。,https://github.com/vllm-project/vllm/issues/1115
vllm,这个issue是关于功能需求的，主要涉及的对象是vllm下的AWQ模型。由于AWQ模型目前只支持float16而不支持bfloat16，用户提出希望增加对bfloat16的支持或者添加一个--dtype float16选项的建议。,https://github.com/vllm-project/vllm/issues/1114
vllm,该issue属于功能增强类型，主要涉及到int8 KVCacheQuant和W8A8 inference在vLLM中的实现，原因是为了提高吞吐量和减少首个token延迟。,https://github.com/vllm-project/vllm/issues/1112
vllm,这是一个用户提出需求的类型，涉及的主要对象是API server。,https://github.com/vllm-project/vllm/issues/1107
vllm,该issue是关于新增功能（Feature），主要涉及到简单API令牌认证的添加。这个改动可能是为了方便用户安装fschat和accelerate，以及对vllm的openai服务器和vanilla服务器添加了FastAPI本地访问令牌认证。,https://github.com/vllm-project/vllm/issues/1106
vllm,这个issue类型为用户提出需求，涉及的主要对象是Orca implementations。由于用户对于Orca实现的基线感到好奇，希望能够查看实现以更好地理解Orca存在的问题。,https://github.com/vllm-project/vllm/issues/1103
vllm,这是一个性能优化的issue，主要涉及到`detokenize_incrementally`函数，用户提出将`convert_tokens_to_string`只转换新计算的token以提升性能。,https://github.com/vllm-project/vllm/issues/1099
vllm,这是一个功能改进建议的issue，主要涉及vLLM在处理特殊token时生成无法停止的问题。原因可能是默认情况下跳过了特殊token，且提出者认为应该支持`stop_token_ids`参数来解决问题。,https://github.com/vllm-project/vllm/issues/1097
vllm,该issue为用户提出需求，请求增加新的接口功能。该问题主要涉及到llm.generate。用户希望能够在llm.generate中结合其他API使用。,https://github.com/vllm-project/vllm/issues/1093
vllm,这是一个用户提出需求的issue，主要涉及docker镜像和vllm项目。用户希望有一个可用的Dockerfile和相关requirements文件，以节省时间。,https://github.com/vllm-project/vllm/issues/1091
vllm,这是一个用户提出需求的issue，涉及的主要对象是LLM类。由于频繁使用和要求，用户希望将`gpu_memory_utilization`和`swap_space`参数明确添加到LLM类中。,https://github.com/vllm-project/vllm/issues/1090
vllm,这是一个关于改进建议和沟通方式的类型，涉及到vLLM的用户和开发者社区，由于目前的沟通方式效率不高而导致创建了vLLM Discord服务器。,https://github.com/vllm-project/vllm/issues/1088
vllm,这个issue类型为社区建议，主要对象是vLLM Discord server。由于社区用户希望添加 Discord 服务器以增进社区交流。,https://github.com/vllm-project/vllm/issues/1086
vllm,这是一个用户提出需求的issue，主要涉及的对象是为vllm添加Prometheus导出器。这个需求源于用户希望通过Prometheus客户端为API添加`/metrics`路由以帮助vllm成为更完整的产品。,https://github.com/vllm-project/vllm/issues/1083
vllm,这个issue属于功能需求提议，主要涉及LLM模块的量化支持问题。,https://github.com/vllm-project/vllm/issues/1080
vllm,该issue类型是用户提出需求，主要涉及API client package的开发和移动问题。由于需要一个简单且只有最小依赖的API客户端包，以便于标准化客户端并为后续兼容性升级做准备。,https://github.com/vllm-project/vllm/issues/1078
vllm,这个issue是一个功能需求报告，主要涉及vLLM中`diversity_penalty`参数的使用问题，用户希望能够实现输出的多样性。,https://github.com/vllm-project/vllm/issues/1076
vllm,这个issue类型是用户提出需求，询问是否vLLM api server有类似TGI提供的健康和指标端点。,https://github.com/vllm-project/vllm/issues/1075
vllm,这个issue是关于功能需求的，主要涉及实现非自回归语言模型M2M在vLLM中的可能性，发起者询问如何在vLLM中实现M2M，因为目前支持的模型都是因果语言模型，而M2M是非自回归且具有编码解码器结构。,https://github.com/vllm-project/vllm/issues/1072
vllm,这是一个需求类型的issue，主要涉及的对象是AWQ。由于AWQ不支持Turing GPUs，需要添加最低功能要求。,https://github.com/vllm-project/vllm/issues/1064
TensorRT-LLM,这个issue类型是功能需求，涉及的主要对象是TensorRT-LLM，提出了新增对Phi-4-MM的支持的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3296
TensorRT-LLM,这是一个特性增加的请求，主要涉及Blackwell和Hopper MoE Gemm2+Finalize fusion实现，由于需要更新internal_cutlass_kernels，因此暂时不合并。,https://github.com/NVIDIA/TensorRT-LLM/issues/3294
TensorRT-LLM,这个issue属于用户提出需求，主要涉及的对象是更新 speculative-decoding.md 文件。由于原文表述不清，用户希望对文档进行更清晰的表达。,https://github.com/NVIDIA/TensorRT-LLM/issues/3293
TensorRT-LLM,这是一个需求提出的issue，主要涉及的对象是TensorRT-LLM下的lm_eval_tensorrt_llm.py脚本，主要原因是需要支持自动部署、提高Torch编译缓存大小、启用聊天模板以及升级lm_eval到最新版本。,https://github.com/NVIDIA/TensorRT-LLM/issues/3292
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及将 ENABLE_MULTI_DEVICE 注册为 CMake 选项，由于该选项目前在 cpp/CMakeLists.txt 中是隐藏的，用户想了解是否有将其注册为 CMake 选项的潜在影响。,https://github.com/NVIDIA/TensorRT-LLM/issues/3289
TensorRT-LLM,这个issue类型是提出需求，涉及的主要对象是用于TensorRT-LLM的ZMQ socket IPC通信。创建该issue的原因可能是用户希望改进目前的通信方式，使用pydantic json序列化来实现通信。,https://github.com/NVIDIA/TensorRT-LLM/issues/3284
TensorRT-LLM,这是一个基础设施改进的issue，主要涉及将 nvrtc_wrapper 迁移到 Conan 包。因缺少 Rocky8 容器镜像中的 sqlite3 python 模块，导致无法通过 pip 安装 Conan，故采用了官方发布的独立 Conan 可执行文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/3282
TensorRT-LLM,这个issue属于用户提出需求，主要涉及支持DeepseekV3在AutoDeploy中的使用。原因是需要对DeepseekV3进行模型补丁以及其他支持，以实现自动部署功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3281
TensorRT-LLM,"这个issue类型是更新维护（chore），涉及的主要对象是更新内部的""internal_cutlass""版本号。",https://github.com/NVIDIA/TensorRT-LLM/issues/3279
TensorRT-LLM,这个issue类型是用户提出需求，请教问题，主要涉及的对象是attention backend interface，用户寻求对其进行完善。,https://github.com/NVIDIA/TensorRT-LLM/issues/3271
TensorRT-LLM,这是一个功能增强（feature enhancement）类型的issue，主要涉及的对象是Hopper XQA模块。由于之前只有Ampere XQA支持推测解码，而Hopper XQA未支持，因此在Hopper GPU上性能存在改进空间。,https://github.com/NVIDIA/TensorRT-LLM/issues/3269
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及后端类型过滤支持。原因可能是用户希望能够根据后端类型来过滤和筛选相关功能或模块。,https://github.com/NVIDIA/TensorRT-LLM/issues/3267
TensorRT-LLM,这是一个功能增强类型的issue，涉及的对象是TensorRT-LLM中的`tensorrt_llm` package。由于需要增加工具来支持json_schema对LLM中的函数调用进行检测和验证，从而帮助提高代码的可读性和可维护性。,https://github.com/NVIDIA/TensorRT-LLM/issues/3266
TensorRT-LLM,该issue类型是功能增强（feature enhancement），主要涉及的对象是`tensorrt_llm/serve`模块。由于用户希望增强`tensorrt_llm/serve`模块中的功能和对`tool_call`的OpenAI协议处理。,https://github.com/NVIDIA/TensorRT-LLM/issues/3265
TensorRT-LLM,"这是一个用户提出需求的issue，主要对象是TensorRT-LLM中的DS V3文档。由于缺少""Serving""部分，用户希望文档中添加该部分内容。",https://github.com/NVIDIA/TensorRT-LLM/issues/3262
TensorRT-LLM,这个issue是关于功能改进的建议，主要涉及到迁移DeepSeek测试，可能由于测试准确性不佳导致的需改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/3260
TensorRT-LLM,这个issue属于用户提出需求，并且主要涉及的对象是支持 AWQ 模型优化检查点。,https://github.com/NVIDIA/TensorRT-LLM/issues/3258
TensorRT-LLM,这是一个功能特性新增的issue，主要涉及的对象是TensorRT-LLM下的pytorch flow。原因是为了更方便地管理UserBuffers，并优化UB的分配和使用方式。,https://github.com/NVIDIA/TensorRT-LLM/issues/3257
TensorRT-LLM,这个issue类型是用户提出需求，主要对象是TensorRT-LLM下的层级融合功能。由于最后一层PP等级无法使用融合，导致用户提出需要为非边界PP层启用后MLP/MOE融合的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3251
TensorRT-LLM,这是一个功能特性增强（Feature Enhancement）类型的issue，主要涉及的是在TensorRT-LLM中添加压力测试（stress test）功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3250
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及测试超时阈值设置。由于测试运行时间长，用户希望将检测时间限制提高到5400秒。,https://github.com/NVIDIA/TensorRT-LLM/issues/3249
TensorRT-LLM,这是一个功能增强类型的issue，主要对象是新增对SM 120的FP8支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/3248
TensorRT-LLM,"这个issue类型是用户提出需求，涉及的主要对象是""feat: Gemma""。由于缺少具体的描述内容，无法确定具体问题或需求。",https://github.com/NVIDIA/TensorRT-LLM/issues/3247
TensorRT-LLM,该issue类型为功能增强请求，主要对象是AutoTuner缓存的序列化和反序列化功能。这个问题的根本原因是需要实现一个简单的dump选项来实现确定性调优缓存。,https://github.com/NVIDIA/TensorRT-LLM/issues/3245
TensorRT-LLM,这是一个功能需求，单涉及的主要对象是TensorRT-LLM，用户提出了添加一个选项来实现无上下文服务器的分解服务运行，并提到设置环境变量`TRTLLM_DISAGG_BENCHMARK_GEN_ONLY`为1来实现该功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3243
TensorRT-LLM,这是一个文档更新的issue，主要对象是GitHub仓库的gh-pages分支。可能由于文档内容需要更新或修复，导致用户提交了这个issue。,https://github.com/NVIDIA/TensorRT-LLM/issues/3242
TensorRT-LLM,这个issue类型是功能增强请求，主要涉及使用NVRTC来进行DeepGEMM JIT编译。,https://github.com/NVIDIA/TensorRT-LLM/issues/3239
TensorRT-LLM,这个issue为需求类型，主要涉及文档内容。由于缺少具体描述，用户提出需要添加ds1性能文档。,https://github.com/NVIDIA/TensorRT-LLM/issues/3232
TensorRT-LLM,这个issue类型是用户提出需求，请教问题， 主要对象是在TensorRT-LLM下的DSR1，由于缺少最佳性能实践，用户可能遇到性能方面的问题或需求更好的指导。,https://github.com/NVIDIA/TensorRT-LLM/issues/3230
TensorRT-LLM,这是一个用户提出需求的issue，该问题单涉及的主要对象是TensorRT-LLM，导致了这个问题的原因可能是用户希望进行测试或者草稿编写。,https://github.com/NVIDIA/TensorRT-LLM/issues/3227
TensorRT-LLM,这是一个需求类型的 issue，主要涉及到贡献者的代码合并。由于缺乏贡献者指导文件，需要添加一个目录来帮助贡献者更方便地将他们的代码合并到项目中。,https://github.com/NVIDIA/TensorRT-LLM/issues/3224
TensorRT-LLM,这是一个功能需求的issue，主要对象是Qwen2.5-vl vit模型在转换成TensorRT过程中出现了精度问题，并且通过重写乘法操作可以解决精度问题，但会降低运行速度。,https://github.com/NVIDIA/TensorRT-LLM/issues/3223
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是PyTorch flow。原因是用户希望在PyTorch flow中返回logits。,https://github.com/NVIDIA/TensorRT-LLM/issues/3221
TensorRT-LLM,这个issue属于需求类型，主要涉及在测试中移动Qwen测试，可能由于测试精度问题或者性能优化需求而提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/3219
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到在TensorRT-LLM中添加纯PyTorch实现的MLA以进行准确性调试。 由于当前实现不能满足用户的需求，因此用户希望添加这个功能来提高模型的准确性 debugging。,https://github.com/NVIDIA/TensorRT-LLM/issues/3218
TensorRT-LLM,这个issue类型是更新需求，涉及的主要对象是Docker镜像。由于依赖更新导致的版本不匹配，需要更新到新的基础Docker镜像。,https://github.com/NVIDIA/TensorRT-LLM/issues/3216
TensorRT-LLM,这个issue为用户提出需求类型，涉及到fetch new requests方法的优化。由于当前方法不够精细，用户需要对其进行改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/3213
TensorRT-LLM,这是一个用户提出的需求。该问题单主要涉及TensorRT-LLM下的性能指标需要新增总token吞吐量的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3212
TensorRT-LLM,这是一个功能提议，主要涉及GPU直接存储，需要在构建容器中包含cuFile库。,https://github.com/NVIDIA/TensorRT-LLM/issues/3209
TensorRT-LLM,这个issue类型是升级请求，并涉及的主要对象是cmake版本。原因是为了正确支持最新的架构，可能导致在较低版本的cmake上出现问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3208
TensorRT-LLM,这个issue属于用户提出需求类型，涉及主要对象是EAGLE模型的量化支持。由于`smoothquant`参数在转换模型时出错，`use_weight_only`参数会导致部分查询错误，因此用户在寻找一个可行的量化解决方案并寻求帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/3207
TensorRT-LLM,这是一个性能优化类型的issue，主要涉及到使用Llama 3.2 1B模型来加速Llama C++测试套件。可能由于测试套件运行速度较慢，提出采用更小的模型来提高性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3206
TensorRT-LLM,这个issue是一个功能增强的变更，主要涉及的对象是TensorRT-LLM模型的生成过程，由于输出维度未匹配导致生成结果不正确。,https://github.com/NVIDIA/TensorRT-LLM/issues/3205
TensorRT-LLM,这个issue是一个功能测试的反馈报告，主要涉及的对象是LoRa模块。由于某些原因导致了测试失败，需要进一步调查和修复。,https://github.com/NVIDIA/TensorRT-LLM/issues/3201
TensorRT-LLM,这个issue属于用户提出需求类型，主要对象是优化广播新请求方法，可能是为了改善性能或功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3198
TensorRT-LLM,该issue为功能请求类型，提出了为参与者提供框架的目录，便于他们将代码合并到主分支。,https://github.com/NVIDIA/TensorRT-LLM/issues/3197
TensorRT-LLM,该issue类型为文档需求，主要对象是开发人员，由于缺少关于在云端开发或者运行pod上的文档，用户提出了需要相关信息的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3194
TensorRT-LLM,"这个issue类型是用户提出需求，该问题单涉及的主要对象是""update user list""。",https://github.com/NVIDIA/TensorRT-LLM/issues/3193
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是为TensorRT-LLM项目添加Bot帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/3192
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是增加机器人命令帮助和检查机器人命令功能。预计用户希望在TensorRT-LLM项目中添加机器人命令的帮助功能以及检查机器人命令的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3191
TensorRT-LLM,该issue属于功能增强类型，主要涉及的对象是在Hopper和Blackwell上添加对FP8 MLA的支持，包括实现pertensor FP8 e4m3量化以及Q和latent KV的quantization。此问题的产生是为了实现更广泛的硬件支持以及提高模型性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3190
TensorRT-LLM,这是一则用户提出需求的issue，主要涉及的对象是测试。原因可能是需要修改测试脚本来适配新的功能或场景。,https://github.com/NVIDIA/TensorRT-LLM/issues/3189
TensorRT-LLM,这个issue属于代码优化或者项目管理类型，主要涉及的对象是提交记录。原因可能是为了合并、整理或调整提交历史。,https://github.com/NVIDIA/TensorRT-LLM/issues/3188
TensorRT-LLM,这个issue属于用户提出需求类型，主要对象是TensorRT-LLM下的trtllm-serve，用户想要增加新的API示例和文档来增强功能和使用说明。,https://github.com/NVIDIA/TensorRT-LLM/issues/3187
TensorRT-LLM,这是一个功能需求的issue，主要对象是支持PeftCacheManager在Torch中的使用。,https://github.com/NVIDIA/TensorRT-LLM/issues/3186
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及的对象是项目的基础设施。由于需要自动同步分叉与上游的代码同步，用户提出了添加同步分叉操作的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3180
TensorRT-LLM,这个issue属于需求提出类型，主要涉及TensorRT-LLM库在PyTorch示例中添加支持的模型，可能是用户希望了解当前支持的模型列表并了解如何在Torch工作流程中使用。,https://github.com/NVIDIA/TensorRT-LLM/issues/3179
TensorRT-LLM,"这个issue类型是需求提出，主要涉及的对象是Phi4MM。由于名称中包含""Draft""，推测用户正在起草关于Phi4MM的变更，可能寻求相关讨论或意见。",https://github.com/NVIDIA/TensorRT-LLM/issues/3177
TensorRT-LLM,这是一个 feature 请求。该问题单涉及的主要对象是支持 CUDA graphs for EAGLE3。由于需要支持CUDA graphs for EAGLE3，用户请求特性增加。,https://github.com/NVIDIA/TensorRT-LLM/issues/3176
TensorRT-LLM,这是一个用户提出需求的类型。该问题单涉及的主要对象是TensorRT-LLM中的modernBERT模型支持情况。由于缺乏关于modernBERT在TensorRTLLM中的支持情况的清晰说明，用户提出了关于该模型支持情况的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3174
TensorRT-LLM,该issue是一个用户提出需求的问题，主要涉及TensorRT-LLM下的auto-regressive prefill-phase，用户希望得到所有token的hidden_states。由于现有实现仅返回最后一个token的hidden_states而非所有token，用户想要了解是否只能逐一生成token。,https://github.com/NVIDIA/TensorRT-LLM/issues/3170
TensorRT-LLM,这是一个功能改进的issue，主要涉及TensorRT-LLM的测试准确性的扩展以及新的API trtllmeval的引入。,https://github.com/NVIDIA/TensorRT-LLM/issues/3167
TensorRT-LLM,这是一个用户提出的需求类型的issue，涉及主要对象为TensorRT-LLM下的scaffolding worker和openai api worker，原因是需要对Worker进行重构以支持openai api，并实现相关功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3166
TensorRT-LLM,这个issue是一个功能需求，主要涉及到GitHub上的TensorRT-LLM项目，用户提出需要添加一个同步fork操作的工作流程。,https://github.com/NVIDIA/TensorRT-LLM/issues/3164
TensorRT-LLM,该issue类型为文档更新请求，涉及主要对象为TensorRT-LLM的主README文档。由于文档需要更新，用户请求审核并直接更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/3162
TensorRT-LLM,这个issue是一个用户提出需求的类型，主要涉及的对象是TensorRT-LLM中的Scaffolding模块。这个需求是为了支持基于奖励模型的best_of_n方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/3160
TensorRT-LLM,这是一个需求类型的问题单，主要涉及StreamGenerationTask的支持在scaffolding上。原因是在推理时的计算方法中，scaffolding控制器可能需要检查已经生成的标记而不中断正在进行的工作请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3158
TensorRT-LLM,这是一个功能增加的issue，主要涉及TensorRT-LLM的Qwen2.5VL和Qwen2VL。由于Qwen2.5VL需要transformers版本 >= 4.49.0，因此存在等待版本升级的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3156
TensorRT-LLM,这是一个改进代码结构的issue，主要对象是`test_disaggregated.py`脚本。,https://github.com/NVIDIA/TensorRT-LLM/issues/3154
TensorRT-LLM,该issue类型为文档改进，主要对象是项目文档。这个问题可能是由于文档中格式不规范，导致部分内容没有正确使用alert formatting。,https://github.com/NVIDIA/TensorRT-LLM/issues/3153
TensorRT-LLM,这是一个版本更新类型的issue，主要对象是库中的版本号。,https://github.com/NVIDIA/TensorRT-LLM/issues/3152
TensorRT-LLM,这个issue是关于需求反馈的，主要对象是TensorRT-LLM 1.0 Release Planning and API Compatibility Commitment。用户提出了关于1.0版本API兼容性的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3148
TensorRT-LLM,这是一个优化性能的 issue，主要涉及到使用 pinned H2D 以减少 CPU 端的阻塞。,https://github.com/NVIDIA/TensorRT-LLM/issues/3147
TensorRT-LLM,这个issue类型是功能新增提议，相关对象是PyTorch backend的LogitsProcessor。由于项目需要完善测试和更新公共示例，以及未更新相关内容，导致需要继续进行工作。,https://github.com/NVIDIA/TensorRT-LLM/issues/3145
TensorRT-LLM,这是一个用户提出需求的类型，该问题涉及Gemmas 3支持的情况。由于Gemmas 3尚未得到支持，用户提出询问何时将其支持的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3143
TensorRT-LLM,这是一个用户提出需求的issue， 主要对象是Executor API，由于缺乏获取吞吐量的相关信息，用户无法通过get_latest_iteration_stats()方法获得正确的数据。,https://github.com/NVIDIA/TensorRT-LLM/issues/3142
TensorRT-LLM,这个issue类型为“功能需求”，主要涉及对象是针对TensorRT-LLM中DecoderState的重构。由于需要通过绑定暴露DecoderState并集成到TRTLLMDecoder中，用户可能提出了对DecoderState在代码中的结构和功能进行优化的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3139
TensorRT-LLM,这个issue属于功能需求类型，涉及的主要对象是TensorRT-LLM的API。由于目前LLM API尚未支持prompt lookup speculative decoding，用户提出了关于此功能缺失的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3138
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是为TensorRT-LLM添加新文档，由于缺乏这些文档，用户希望添加关于使用`trtllmllmapilaunch`在Slurm或其他MPI系统上的示例文档。,https://github.com/NVIDIA/TensorRT-LLM/issues/3135
TensorRT-LLM,这是一个优化性能的issue，主要涉及TensorRT-LLM中的PP和attention DP，由于同步请求完成点和移除MPI world broadcast导致的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3134
TensorRT-LLM,这个issue是一个功能特性提议，主要涉及Variable-Beam-Width-Search（VBWS），目的是实现VBWS支持的第二部分。,https://github.com/NVIDIA/TensorRT-LLM/issues/3133
TensorRT-LLM,这是一个关于需求提出的issue，主要对象是TensorRT-LLM下的KV Cache Offload功能。由于用户想要强制TensorRT仅使用CPU进行KV Cache，以便比较在GPU和CPU上KV Cache对吞吐量/延迟的影响。,https://github.com/NVIDIA/TensorRT-LLM/issues/3130
TensorRT-LLM,这是一个功能增强（feature enhancement）类型的issue，主要涉及的对象是在TensorRT-LLM中添加对Cohere2ForCausalLM架构的支持。造成这个issue的原因可能是某些功能尚未完成，如无法从模型配置加载正确的滑动窗口配置。,https://github.com/NVIDIA/TensorRT-LLM/issues/3128
TensorRT-LLM,这个issue属于更新类型，涉及主要对象为libcutlass library。由于NVIDIA在内部仓库上进行的更改，在TensorRT-LLM中需要同步更新libcutlass库的内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/3126
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是TensorRT-LLM团队。由于团队希望组织在线见面会话，他们正在寻求社区成员提供讨论话题和反馈意见。,https://github.com/NVIDIA/TensorRT-LLM/issues/3124
TensorRT-LLM,该issue是一个基础设施更新类型的问题单，涉及到并发控制，可能是由于多个操作同时访问导致的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3120
TensorRT-LLM,该issue类型为技术改进（chore），主要涉及的对象是内部核心库的ABI边界稳定性，问题由于需要对内部核心库的ABI边界进行稳定性优化。,https://github.com/NVIDIA/TensorRT-LLM/issues/3117
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到AutoDeploy中RoPE支持的增强。由于transformers.models.llama.modeling_llama.apply_rotary_pos_emb存在两种变体，目前仅支持其中一种，用户可能提出了对第二种变体的支持需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3115
TensorRT-LLM,该问题属于Feature请求，主要涉及Rocky容器。导致这个问题的原因是需要安装sqlite才能在Conan中使用pip。,https://github.com/NVIDIA/TensorRT-LLM/issues/3114
TensorRT-LLM,这个issue是关于提出需求的类型，主要对象是增加机器人帮助功能。这个问题的原因是缺乏机器人帮助功能，用户需求更方便和快速的交互方式。,https://github.com/NVIDIA/TensorRT-LLM/issues/3107
TensorRT-LLM,这个issue类型是功能增强（feature enhancement），涉及的主要对象是TensorRT-LLM中的MoE（Mixture of Experts）模块，由于MoE workspace size很大导致GPU内存不足，故提出将MoE输入拆分为多个块来减少GPU内存使用。,https://github.com/NVIDIA/TensorRT-LLM/issues/3104
TensorRT-LLM,这个issue类型是重构请求，主要涉及DecoderState对象的属性调整，由于需要简化disableLookahead和改进numDecodingEngineTokens处理。,https://github.com/NVIDIA/TensorRT-LLM/issues/3103
TensorRT-LLM,这是一个性能优化的问题，涉及TensorRT-LLM API，由于当前性能不佳，用户希望对API进行优化。,https://github.com/NVIDIA/TensorRT-LLM/issues/3102
TensorRT-LLM,这个issue是一个技术改进性质的提交，涉及到移除TensorRT-LLM中与MPI相关的依赖，原因是为了减少对MPI的依赖性。,https://github.com/NVIDIA/TensorRT-LLM/issues/3101
TensorRT-LLM,这是一个关于GitHub PR文件更改支持的功能需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3098
TensorRT-LLM,这是一个改进性质的issue，主要涉及到测试列表名称检查的添加。原因可能是为了增强代码质量和测试覆盖率。,https://github.com/NVIDIA/TensorRT-LLM/issues/3097
TensorRT-LLM,这是一个功能需求类型的issue，主要对象是TensorRT-LLM中的MTP，用户提出需要添加对MTP+cuda_graph_padding=True的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/3096
TensorRT-LLM,这是一个用户要求性能优化的问题，主要涉及TensorRT-LLM中的DeepSeek功能。由于目前的最小延迟模式下DeepSeek表现不佳，用户希望增加一些优化来提升性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/3093
TensorRT-LLM,这是一个需求类型的issue，主要涉及Blossom debug hook功能。由于开发者可能需要在调试过程中添加某些特定的调试挂钩，因此提出了该需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3091
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是代码中的一些修改。这个issue可能由于某些重要的改动或功能需求导致。,https://github.com/NVIDIA/TensorRT-LLM/issues/3089
TensorRT-LLM,该issue类型为需求提出，主要对象是CI测试。这个问题的原因可能是开发者希望将一些重要的变化合并进项目中。,https://github.com/NVIDIA/TensorRT-LLM/issues/3088
TensorRT-LLM,这是一个用户提出需求类型的issue，涉及的主要对象为CI demo。由于可能有重要且优秀的更改，用户建议将这些更改合并到项目中。,https://github.com/NVIDIA/TensorRT-LLM/issues/3086
TensorRT-LLM,这是一个特性需求，用户提出需要在PyTorch工作流中加入不带缓存的注意力机制。,https://github.com/NVIDIA/TensorRT-LLM/issues/3085
TensorRT-LLM,这是一个功能特性新增（feat）类型的issue，主要涉及TensorRT-LLM中引入Variable-Beam-Width-Search（VBWS），用户提出了对于beam search进行批处理时对beam width进行动态调整的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3082
TensorRT-LLM,这个issue类型是需求提升，涉及主要对象为decoder finalize函数的重构。由于未提供具体的变更细节，用户可能需要查看提交详情来了解改动的具体内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/3077
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及 GptDecoderBatched 的前向方法的简化。由于代码复杂性或不便使用等原因，用户想要简化这些前向方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/3076
TensorRT-LLM,这是一个用户提交需求改进的issue，主要涉及的对象是项目的开发容器（devcontainer），提出的问题是关于提高开发环境的生产力和便利性。,https://github.com/NVIDIA/TensorRT-LLM/issues/3075
TensorRT-LLM,这个issue类型是更新CI allowlist，主要涉及的对象是持续集成系统，用户可能遇到无法通过CI检查的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3074
TensorRT-LLM,该issue属于改进需求类型，涉及到TensorRT-LLM下的disaggregated serving脚本的重构。此问题主要是为了简化disaggregated serving部署并减少重复代码，提出了启动disaggregated workers和server的新方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/3073
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及修改README.md文件来更新支持矩阵和添加切换列表。,https://github.com/NVIDIA/TensorRT-LLM/issues/3072
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是开源项目TensorRT-LLM。由于该需求提出了开源fp8_blockscale_gemm功能的请求，说明用户想要对该功能进行开源。,https://github.com/NVIDIA/TensorRT-LLM/issues/3071
TensorRT-LLM,这是一个用户提出的需求。该问题涉及要添加请求带宽测量功能。这个需求的提出可能是为了更好地了解系统中每个请求的带宽使用情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/3070
TensorRT-LLM,这是一个用户提出需求的issue，该问题涉及的主要对象是API stability references，由于缺乏1.0 criteria scope的创建，用户提出了相关需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3069
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是为TensorRT-LLM添加LoRA支持的gemma。,https://github.com/NVIDIA/TensorRT-LLM/issues/3068
TensorRT-LLM,这是一个功能需求，主要涉及到对UCX支持的缓存传输器进行功能性扩展。,https://github.com/NVIDIA/TensorRT-LLM/issues/3065
TensorRT-LLM,这个issue类型是需求提出，主要涉及的对象是DeepSeek模型。用户提出了添加gpqa准确性测试脚本和测试的需求，并更新了相应文档和测试清单。,https://github.com/NVIDIA/TensorRT-LLM/issues/3063
TensorRT-LLM,这是一个需求更改类型的issue，主要涉及将GPTJ特征测试移至LLaMA模型。由于可能需要整合或调整测试框架，所以作出了这个改动。,https://github.com/NVIDIA/TensorRT-LLM/issues/3061
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及文档中如何在本地运行CI阶段，用户希望了解如何在本地运行CI脚本。,https://github.com/NVIDIA/TensorRT-LLM/issues/3060
TensorRT-LLM,这是一个功能增强的issue，涉及主要对象为TensorRT-LLM中的模型管理。原因是为了增加新发布的EXAONE-Deep模型，并更改默认目录以匹配新模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/3054
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及更新DeepSeekV3文档，因为需要包含flashMLA和DeepGEMM的信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/3052
TensorRT-LLM,"这是一个需求类型的issue， 主要对象是""doc""。由于缺少README.md文件，用户希望添加用于生成模板的说明文档。",https://github.com/NVIDIA/TensorRT-LLM/issues/3048
TensorRT-LLM,这是一个功能增强类型的 issue，涉及主要对象是支持从 ModelOpt 导入 prequantized FP8 ckpt 到 nemotron-mini-4b-instruct 模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/3046
TensorRT-LLM,这是一个功能请求，涉及支持 FP4 量化中的线性块模式布局。,https://github.com/NVIDIA/TensorRT-LLM/issues/3045
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及Pytorch PP + attention DP支持。由于缺乏此功能，用户请求添加对应支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/3044
TensorRT-LLM,这是一个需求修改类型的issue，主要涉及TensorRT-LLM下的llava模块，用户可能希望添加第二个可能的输出。,https://github.com/NVIDIA/TensorRT-LLM/issues/3043
TensorRT-LLM,这是一个功能增强类型的issue，涉及主要对象为TensorRT-LLM中的AutoDeploy功能。 ,https://github.com/NVIDIA/TensorRT-LLM/issues/3041
TensorRT-LLM,这是一个优化建议，主要涉及到使用TensorRT-LLM中的通信方式，建议在rank 0节点上使用`gather`而不是`allgather`。,https://github.com/NVIDIA/TensorRT-LLM/issues/3040
TensorRT-LLM,这个issue类型属于功能增强（feature enhancement），主要涉及到TensorRT-LLM下的trtllm-bench工具的迭代日志记录。这个功能增强的需求可能是为了更详细地统计每次迭代的性能数据。,https://github.com/NVIDIA/TensorRT-LLM/issues/3039
TensorRT-LLM,该issue属于用户提出需求类型，主要涉及TensorRT-LLM中关于NVFP4量化的权重和激活范围校准的问题。导致该问题的原因是用户对于NVFP4量化中的激活范围校准和全局尺度的理解不清晰。,https://github.com/NVIDIA/TensorRT-LLM/issues/3037
TensorRT-LLM,这是一个功能更新的issue，主要涉及TensorRT-LLM中EAGLE-3的初始实现。由于CUDA graphs未与EAGLE3兼容，导致特定性能差异，用户需要进行比较与benchmark测试。,https://github.com/NVIDIA/TensorRT-LLM/issues/3035
TensorRT-LLM,这是一个用户提出需求的issue，涉及的主要对象是LlmRequest、KVCacheManager和Scheduler。由于pybind调用比纯Python调用慢，所以决定重新实现LlmRequest和Scheduler。,https://github.com/NVIDIA/TensorRT-LLM/issues/3034
TensorRT-LLM,这是一个文档更新类的issue，主要涉及到项目的贡献者。由于可能相关文档需要更新或补充，导致用户提出这个issue。,https://github.com/NVIDIA/TensorRT-LLM/issues/3033
TensorRT-LLM,这是一个类型为功能改进（Feature Improvement）的issue，涉及主要对象为TensorRT-LLM中的AllReduce custom op。原因可能是为了统一Module和custom op级别的两个版本的AllReduce操作。,https://github.com/NVIDIA/TensorRT-LLM/issues/3032
TensorRT-LLM,这个issue是关于文档更新的请求，主要对象是TensorRT-LLM下的internal cutlass library和nvrtc_wrapper。,https://github.com/NVIDIA/TensorRT-LLM/issues/3030
TensorRT-LLM,这个issue类型是优化建议，针对的主要对象是CI pipeline执行过程中多次检出Git源代码的重复操作。,https://github.com/NVIDIA/TensorRT-LLM/issues/3029
TensorRT-LLM,这个issue类型属于功能增强，主要涉及TensorRT-LLM中的内存分配问题，由于需要为每个窗口大小分配最小的内存块，可能出现了内存管理方面的症状或者用户提出了此需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3028
TensorRT-LLM,这个issue是关于改进多GPU测试的问题，涉及CI测试的优化。,https://github.com/NVIDIA/TensorRT-LLM/issues/3027
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及的对象是 LoRa 模块。,https://github.com/NVIDIA/TensorRT-LLM/issues/3026
TensorRT-LLM,这是一个功能改进的issue，主要涉及到TensorRT-LLM下的stateful decoders的重构。通过简化构造函数和设置方法签名，去除了一些冗余参数，以提高代码的简洁性和易用性。,https://github.com/NVIDIA/TensorRT-LLM/issues/3024
TensorRT-LLM,这个issue属于改进类型，主要涉及构建脚本中的git lfs操作，导致在未执行`git lfs pull`时报错。,https://github.com/NVIDIA/TensorRT-LLM/issues/3022
TensorRT-LLM,这个issue类型是功能增强（feature enhancement），主要涉及TensorRT-LLM下的cos_sin_cache支持更新。由于需要优化rope实现以及改善用户体验，需要添加新的功能并做性能优化。,https://github.com/NVIDIA/TensorRT-LLM/issues/3020
TensorRT-LLM,这是一个版本更新的问题，涉及的主要对象是TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/3019
TensorRT-LLM,这是一个优化类型的issue，主要涉及TensorRT-LLM项目的代码导入方式。由于当前使用的绝对导入方式，用户建议替换为相对导入方式。,https://github.com/NVIDIA/TensorRT-LLM/issues/3016
TensorRT-LLM,这个issue是一个功能改进类型，主要涉及TensorRT-LLM项目中的trt engine构建时间问题。这个问题是由于autotune的默认最大tactic数量设置导致测试时间过长引起的。,https://github.com/NVIDIA/TensorRT-LLM/issues/3014
TensorRT-LLM,这个issue为功能新增（feature addition），主要针对条件分解测试（conditional disaggregation test），用户需要添加该功能以便进行相关测试。,https://github.com/NVIDIA/TensorRT-LLM/issues/3012
TensorRT-LLM,这个issue是一个性能优化类型的问题，主要对象是CUDA graphs以及在不同GPU上活动请求不均匀。由于活动请求不均匀会导致无法使用CUDA图形，并且引起性能问题，所以提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/3010
TensorRT-LLM,这个issue属于feature功能更新，主要涉及TensorRT-LLM下logits bitmask kernel的更新，由于采用了新的kernel版本(v3)，在不同场景下带来了不同的性能影响。,https://github.com/NVIDIA/TensorRT-LLM/issues/3009
TensorRT-LLM,这是一个特性更新（feature update）的issue，主要涉及到TensorRT-LLM中的allreduce benchmark。这个更新是为了将当前的allreduce benchmark从TRT流程改为PyTorch流程，同时支持cuda图形和norm融合。,https://github.com/NVIDIA/TensorRT-LLM/issues/3005
TensorRT-LLM,这是一个用户提出需求的类型，主要对象涉及到MLA FP8 KV Cache on Blackwell。由于目前不支持fp8 kv cache on blackwell，用户提出了添加该支持的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/3004
TensorRT-LLM,这是一个增加流量限制功能的任务类型的issue，主要对象是GitHub上的TensorRT-LLM项目。,https://github.com/NVIDIA/TensorRT-LLM/issues/3001
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是PyTorch flow，可能由于现有快速启动流程过于复杂而希望简化的原因。,https://github.com/NVIDIA/TensorRT-LLM/issues/3000
TensorRT-LLM,这是一个功能增强（feature enhancement）类型的issue，涉及的主要对象是LlmArgs。由于用户希望添加PeftCacheConfig和SchedulerConfig等纯Python配置，因此提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2997
TensorRT-LLM,这个issue是一个功能请求，主要对象是TensorRT-LLM的UB AR NORM FP16/BF16模块，用户提出了添加一次性版本的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2995
TensorRT-LLM,这是一个更新审批人列表的类型。该问题单涉及的主要对象是项目的审批人列表。原因可能是需要更新或调整项目的审批流程。,https://github.com/NVIDIA/TensorRT-LLM/issues/2994
TensorRT-LLM,这个issue属于功能新增类型，主要涉及的对象是Phi-4-mini模型。由于用户希望在TensorRT-LLM中添加对Phi-4-mini模型的支持，因此提出了这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2990
TensorRT-LLM,该issue类型为功能性改进，涉及删除旧模型的测试用例。,https://github.com/NVIDIA/TensorRT-LLM/issues/2987
TensorRT-LLM,这是一个优化改进类型的issue，涉及主要对象为PD（可能为某个API或模块），旨在优化生成的token的返回逻辑。,https://github.com/NVIDIA/TensorRT-LLM/issues/2986
TensorRT-LLM,该issue类型为功能增强（feature enhancement），主要对象为TensorRT-LLM中的chat completion功能。原因可能是缺乏此功能导致用户无法进行对话完成。,https://github.com/NVIDIA/TensorRT-LLM/issues/2985
TensorRT-LLM,这是一个功能增强类型的issue，主要涉及TensorRT-LLM中的KV缓存传输度量，用户添加了一种精细化的KV缓存传输度量，并提供了设置环境变量来输出指标的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2984
TensorRT-LLM,这是一个功能改进类的issue，主要针对Accuracy test suite的改进。原因可能是为了提高测试的准确性和可靠性。,https://github.com/NVIDIA/TensorRT-LLM/issues/2982
TensorRT-LLM,这个issue类型是功能更新，主要涉及的对象是cutlass库，由于需要更新cutlass库来改进TensorRT-LLM的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2981
TensorRT-LLM,这个issue属于用户提出需求类型，主要对象是贡献指南(CONTRIBUTING.md)，由于缺少具体描述，用户可能需要更新TensorRT-LLM项目的贡献指南，为第一个PR做准备。,https://github.com/NVIDIA/TensorRT-LLM/issues/2980
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是文档更新。由于内容为空，用户可能希望更新文档内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/2979
TensorRT-LLM,这是一个用户提出需求的问题，主要关注TensorRT-LLM的支持情况，希望实现模型在多个GPU上的分布式部署来提高推断速度。,https://github.com/NVIDIA/TensorRT-LLM/issues/2967
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是软件仓库中的 requirements.txt 文件。可能是由于软件版本更新或者新增依赖导致的需求更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2966
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及到请求复制DeepSeek-R1在H200和B200上的配置，用户希望获得在特定硬件上复现详细配置以达到博客中展示的结果。,https://github.com/NVIDIA/TensorRT-LLM/issues/2964
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及的对象是项目的路线图（roadmap）。由于路线图可能过时或不清晰，用户请求更新以获取最新信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/2960
TensorRT-LLM,这是一个类型为文档/注释改进的 issue，主要关注于TensorRT-LLM项目中的 license comment 中嵌入的 reference URL 问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2949
TensorRT-LLM,这是一个功能需求提出的issue，主要涉及NeMo的conformer encoder-transformer decoder模型（canary1b和canary1b flash）的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2945
TensorRT-LLM,这个issue属于更新工作流程的需求类型，主要涉及TensorRT-LLM项目的工作流程控制。,https://github.com/NVIDIA/TensorRT-LLM/issues/2943
TensorRT-LLM,这是一个文档更新的需求，涉及主要对象是项目的文档，可能是由于文档内容过时或者不准确导致用户需要更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2940
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到测试机器人和持续集成。这个问题可能是由于测试机器人和持续集成系统在项目中尚未完全集成或配置正确而引起的。,https://github.com/NVIDIA/TensorRT-LLM/issues/2939
TensorRT-LLM,这个issue是关于更新TensorRT-LLM的，类型为功能需求提升。主要涉及的对象是TensorRT-LLM的功能和性能优化。由于新增功能支持和内存优化，带来了新增功能和性能优化方面的更新和改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/2936
TensorRT-LLM,这个issue属于文档更新类型，主要对象是TensorRT-LLM项目的文档。由于文档内容过时或不完整，用户提出了需要更新文档的帮助请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2935
TensorRT-LLM,这个issue是关于用户提出需求的类型，主要对象是TensorRT-LLM下的trtllm-serve，用户询问是否trtllmserve能自动启用前缀缓存功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2932
TensorRT-LLM,这是一个用户请求更新GitHub工作流程的issue，主要对象是TensorRT-LLM。由于工作流程可能需要改进或更新，用户请求更新以提高工作效率或符合最新规范。,https://github.com/NVIDIA/TensorRT-LLM/issues/2931
TensorRT-LLM,这个issue类型是merge请求，涉及主要对象为TensorRT-LLM项目。由于作者提出了一个合并请求测试，想要将更改合并到主分支中。,https://github.com/NVIDIA/TensorRT-LLM/issues/2930
TensorRT-LLM,这个issue类型是用户提出需求，该问题涉及的主要对象是R1 671B，由于未提供定量，用户想了解在bs=1情况下，R1 671B使用h200模型的吞吐量是多少。,https://github.com/NVIDIA/TensorRT-LLM/issues/2928
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是TensorRT-LLM模型支持，用户关注的是是否会有对phi4和Gemma3模型的支持，以及对于AI领域进展速度慢的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2913
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是支持 Cohere Command-A 模型。这是由于该模型采用了独特的架构，导致用户需要帮助来添加该模型到代码库中。,https://github.com/NVIDIA/TensorRT-LLM/issues/2912
TensorRT-LLM,该issue类型为用户提出需求，主要对象是机器人。由于用户想要触发机器人运行，因此提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2892
TensorRT-LLM,这是一个用户请求运行bot的类型的问题，该问题涉及的主要对象是项目中的CI流程。由于用户想要触发bot的运行，可能是希望自动执行一些操作或检查，或者触发一些特定的流程。,https://github.com/NVIDIA/TensorRT-LLM/issues/2888
TensorRT-LLM,这个issue类型为用户提出需求，该问题涉及的主要对象是添加gemma 3 architecture。这个需求是由于用户希望TensorRT-LLM项目中加入对gemma 3 architecture的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2880
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM和MiniCPM-V2.6模型。用户提出是否有计划适配minicpmv系列模型，可能是因为希望TensorRT-LLM能够支持MiniCPM-V2.6模型，从而获得更好的性能或功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2878
TensorRT-LLM,这个issue是关于功能需求的，主要对象是TensorRT-LLM系统中QwQ 32B的支持更新，由于需要增加QwQ 32B的支持，用户提出了更新readme的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2877
TensorRT-LLM,这个issue是关于更新`setup.py`中脚本路径的更新维护工作，不是bug报告。主要涉及的对象是项目文件`setup.py`。,https://github.com/NVIDIA/TensorRT-LLM/issues/2876
TensorRT-LLM,这个issue是关于更新TensorRT-LLM的，不是bug报告，主要涉及的对象是TensorRT-LLM的功能更新和改进。该更新主要是为了增加了一些新特性和修复了一些问题，旨在提升模型支持和用户体验。,https://github.com/NVIDIA/TensorRT-LLM/issues/2873
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及对象为executor API。用户提出自定义采样器以加快生成速度的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2871
TensorRT-LLM,这是一个用户需求问题单，关于DeepSeek R1在TensorRT中的支持问题。关闭问题未提供有效答复可能是由于相关团队尚未提供支持或解决方案。,https://github.com/NVIDIA/TensorRT-LLM/issues/2870
TensorRT-LLM,这个issue类型为用户提出需求，询问是否支持internvl2.5，主要涉及对象为TensorRT-LLM。由于缺少internvl2.5支持，用户可能遇到功能无法实现或性能问题，因此提出了询问。,https://github.com/NVIDIA/TensorRT-LLM/issues/2867
TensorRT-LLM,这个issue类型为需求提出，主要对象是TensorRT-LLM中的技术优化，用户询问关于是否有计划采纳类似DeepEP、EPLB、DeepGEMM、FlashMLA等优化技术，以及未来版本可能考虑的优化方向。,https://github.com/NVIDIA/TensorRT-LLM/issues/2861
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM for Qwen2.5-VL模型，用户想要发布更新版本以支持Qwen2.5VL模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2859
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是测试用例的使能。用户想要启用名为l0-test.yml的测试用例。,https://github.com/NVIDIA/TensorRT-LLM/issues/2858
TensorRT-LLM,这是一个关于软件版本发布计划和支持不同功能的询问类型的issue，主要涉及到TensorRT-LLM的各个分支之间的差异和支持的TRT引擎功能，由于分支间的差异和信息不清晰，用户寻求关于版本发布计划和当前分支是否支持所需功能的帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/2844
TensorRT-LLM,这是一个需求类型的issue，主要涉及文档所有者添加，目的是使文档组成为/docs/的所有者。,https://github.com/NVIDIA/TensorRT-LLM/issues/2839
TensorRT-LLM,这是一个功能需求的讨论。它涉及到对团队成员资格进行检查的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2835
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及的对象是xgrammar库的更新。由于当前版本的xgrammar在Executor中不支持StructuralTagItem功能，导致无法提供利用该特性的EBNF语法。,https://github.com/NVIDIA/TensorRT-LLM/issues/2832
TensorRT-LLM,这个issue是关于用户提出需求，询问是否计划通过旧的（非pytorch）工作流程支持DeepSeek v3，以及询问旧工作流是否将被废弃，pytorch工作流是否将成为新模型的主要方式。,https://github.com/NVIDIA/TensorRT-LLM/issues/2831
TensorRT-LLM,该issue是关于需求的，主要涉及使用NVIDIA-gha runners来收集测试结果在CI中。这个问题的产生可能是为了优化CI流程以便更有效地收集和分析测试结果。,https://github.com/NVIDIA/TensorRT-LLM/issues/2830
TensorRT-LLM,这是一个新功能需求的issue，涉及到添加CODEOWNERs文件进行规则测试，由于缺乏该文件可能导致团队在规则测试中的管理和决策方面存在一些不便。,https://github.com/NVIDIA/TensorRT-LLM/issues/2828
TensorRT-LLM,这是一个用户建议需求的issue，主要涉及的对象是在TensorRT-LLM中添加R1性能数据到最新消息页面。可能由于用户希望更方便地查看最新的性能数据而提出此需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2823
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下Executor API的使用问题，请求添加关于External Draft model speculative decoding的Python示例和询问encoder-decoder模型支持的Speculative decoding模式。,https://github.com/NVIDIA/TensorRT-LLM/issues/2816
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Executor API，用户希望实现数据并行性以避免由于负载均衡而导致的性能下降问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2812
TensorRT-LLM,这是一个功能需求，主要涉及TensorRT-LLM下的Per-Request Logits Post-Processor Registration。由于当前logits后处理器必须在初始化时注册，无法在每个请求上进行运行时注册，导致应用与模型部署之间紧密耦合，需要已知并命名完整的验证模式集合，并且应用逻辑更改需要模型重新部署。,https://github.com/NVIDIA/TensorRT-LLM/issues/2809
TensorRT-LLM,这是一个功能需求类的issue，主要涉及到TensorRT-LLM项目中的workflow comment trigger。由于原始实现检查整个注释而不是注释的开头，导致无法添加任意参数到触发注释的末尾。,https://github.com/NVIDIA/TensorRT-LLM/issues/2808
TensorRT-LLM,这个issue类型为功能优化，主要涉及TensorRT-LLM下的模型量化参数设置和自动推断，用户希望改进模型优化流程中的批处理大小设置。,https://github.com/NVIDIA/TensorRT-LLM/issues/2806
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM框架中GptManager接口被Executor接口替代的变更。用户询问为何替换了GptManager接口以及Executor接口作为替代的优势，表达了对此变化的困惑并寻求帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/2800
TensorRT-LLM,这是一个功能需求类型的Issue，主要涉及的对象是TensorRT-LLM库。由于更新后的TensorRT-LLM库在aarch64平台下使用的库要求GLIBC_2.38，导致在旧版本系统上无法使用，希望更新构建过程以支持旧版本的GLIBC，以便在旧版系统上运行。,https://github.com/NVIDIA/TensorRT-LLM/issues/2795
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下的Qwen2.5-VL架构。这个问题出现的原因是在TensorRTLLM中的MODEL_MAP中尚未包含Qwen2.5VL，导致无法支持该架构的模型转换和引擎构建。,https://github.com/NVIDIA/TensorRT-LLM/issues/2794
TensorRT-LLM,这是一个需求更新的issue，主要涉及TensorRT-LLM的更新以及添加新功能，如DeepSeek V3/R1支持、批处理logits处理支持等。,https://github.com/NVIDIA/TensorRT-LLM/issues/2783
TensorRT-LLM,这是一个用户提出需求的issue，主要问题是用户想要控制TensorRT使用的最大GPU内存量，但当前的参数不够直接控制，希望能找到更直接的方式，或者能够估计给定参数下TensorRT引擎会使用多少内存。,https://github.com/NVIDIA/TensorRT-LLM/issues/2773
TensorRT-LLM,这个issue属于性能改进类型，主要涉及FP8的性能改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/2766
TensorRT-LLM,这个issue类型为用户提出需求，主要涉及的对象是对于TensorRT-LLM是否计划实现DeepSeek中介绍的DualPipe并行机制。,https://github.com/NVIDIA/TensorRT-LLM/issues/2765
TensorRT-LLM,这是一个需要更新说明文档的问题，主要涉及DeepSeek V3的README。可能由于README文档过时或不完整导致用户需要更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2762
TensorRT-LLM,这是一个功能更新的issue，涉及主要对象为TensorRT-LLM。由于新功能加入、支持和一些限制，用户可能会关注特定平台的pip安装支持问题、新硬件支持、PyTorch工作流程实验特性等内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/2755
TensorRT-LLM,这个issue为更新说明文档的问题，涉及主要对象为TensorRT-LLM项目的README.md文件。原因可能是0.17版本的更新引起了部分内容需要补充或修改。,https://github.com/NVIDIA/TensorRT-LLM/issues/2751
TensorRT-LLM,这是一个用户询问关于新的pytorch workflow的类型为用户提出需求的issue，主要涉及对象是TensorRT-LLM。用户想了解关于使用TRTLLM的推荐方式以及性能表现问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2750
TensorRT-LLM,这是一个优化建议，针对代码中重复读取JSON文件的问题，提出只在必要时读取一次。,https://github.com/NVIDIA/TensorRT-LLM/issues/2747
TensorRT-LLM,这个issue是关于文档页面更新的请求，属于用户提出需求类型，主要涉及到TensorRT-LLM项目的文档页面。由于可能的原因可能是文档内容过时或有错误，导致用户请求更新文档页面。,https://github.com/NVIDIA/TensorRT-LLM/issues/2746
TensorRT-LLM,这是一个关于更新Github Pages的issue，类型为用户提出需求，相关对象为TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/2743
TensorRT-LLM,这是一个用户提出需求并添加备注的类型，主要涉及对象为黑韦尔（Blackwell）用户。由于缺乏具体内容，很难判断用户需求背后的原因或具体问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2742
TensorRT-LLM,这是一个请求更新GitHub页面的issue，属于其他类型（非bug报告）。主要涉及项目的GitHub页面显示。,https://github.com/NVIDIA/TensorRT-LLM/issues/2741
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到fp8 kv cache 和 FP8_PER_CHANNEL_PER_TOKEN quantization。用户询问是否能让llamalike模型使用fp8 kv cache进行FP8_PER_CHANNEL_PER_TOKEN量化，或者是否有计划支持此功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2740
TensorRT-LLM,这是一个关于功能实现的问题，主要涉及TensorRT-LLM下的KV cache。用户询问TensorRTLLM在KV缓存中存储的内容是何种形式。,https://github.com/NVIDIA/TensorRT-LLM/issues/2738
TensorRT-LLM,这是一个关于需求更新的问题，主要涉及到更新cutlass kernel库。由于更新不及时或者缺少某些功能，用户希望对cutlass kernel库进行更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2734
TensorRT-LLM,这是一个用户提出更新文档的需求，可能是因为当前的文档不够清晰或者需要添加新的信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/2732
TensorRT-LLM,该issue类型是功能增强需求，主要涉及的对象是trtllm-serve。由于新功能需求，用户提出了需要向trtllm-serve添加分块上下文/预填充运行时选项的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2731
TensorRT-LLM,这是一个用户需求的类型issue，该问题单涉及到在TensorRT-LLM中添加deepseek模型。因为用户想要向项目中添加deepseek模型，所以提出了这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2728
TensorRT-LLM,这是一个用户提出需求的类型的issue，主要对象是TensorRT-LLM中的性能问题。由于性能没有按预期增加，用户想要确定是什么导致了计算或内存带宽等方面的瓶颈。,https://github.com/NVIDIA/TensorRT-LLM/issues/2722
TensorRT-LLM,这个issue是关于需求问题，主要涉及的对象是TensorRT-LLM项目。用户提出了关于更新频率的疑问，可能由于长时间没有更新导致用户关注并期待新版本的发布。,https://github.com/NVIDIA/TensorRT-LLM/issues/2720
TensorRT-LLM,这是一个功能需求的issue，关于为DeepSeek V3添加FP8支持的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2719
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及的对象是如何在使用自定义模型时在TensorRT-LLM上对模型进行量化。这个问题的根本原因是用户想了解在TensorRT-LLM上使用自定义模型时如何进行模型量化，是否需要编写C++代码以及是否有相关示例。,https://github.com/NVIDIA/TensorRT-LLM/issues/2718
TensorRT-LLM,这是一个关于功能需求的问题，主要涉及TensorRT-LLM下的trtllmserve是否支持部署多模态模型。造成这个问题的原因可能是trtllmserve目前不支持部署多模态模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2714
TensorRT-LLM,这是一个关于需求的issue，用户提出了关于Blackwell架构的支持问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2710
TensorRT-LLM,这个issue类型为功能需求，主要涉及NVILA模型的支持。由于当前示例中展示了VILA + Llama（而NVILA是基于Qwen2的），因此用户询问何时会支持新的NVILA模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2707
TensorRT-LLM,这是一个关于用户提出需求的issue，主要涉及到TensorRT-LLM中对 INT2/INT3 量化的支持，用户想了解是否计划在未来支持INT2/INT3量化，以及了解支持INT2/INT3量化所需的工作量。,https://github.com/NVIDIA/TensorRT-LLM/issues/2704
TensorRT-LLM,"这是一个性能优化建议，主要涉及TensorRT-LLM中的自定义allreduce功能。原因是作者发现当使用自定义allreduce内核进行性能测试时，特别是在使用cuda图形并且批量大小较小时，观察到了可观的延迟。提议通过优化索引计算来改善性能，并认为为一次性allreduce添加__launch_bounds__(512, 1)也能提高性能。",https://github.com/NVIDIA/TensorRT-LLM/issues/2696
TensorRT-LLM,"这个issue属于用户提出需求，涉及主要对象是在TensorRT-LLM中操作动态形状的输入张量并替换为动态形状的zerolike张量。原因在于在运行时需要替换具有动态形状（1,1）的输入张量，而使用Numpy会出现问题。",https://github.com/NVIDIA/TensorRT-LLM/issues/2695
TensorRT-LLM,这是一个关于需求的问题，主要涉及的对象是internvl 2.5版本，用户询问是否支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2686
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及TensorRT-LLM中的量化实现区别，用户询问如何通过不同命令实现量化及是否可以手动修改生成的配置文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/2681
TensorRT-LLM,这是一个关于用户提出需求的问题，主要涉及TensorRT-LLM项目在Jetson平台上的版本兼容性。用户想要了解是否会有与v0.15.0标签对应的Jetson版本，或者如何将v0.15.0转换为适用于Jetson的版本。,https://github.com/NVIDIA/TensorRT-LLM/issues/2676
TensorRT-LLM,这是一个关于需求的问题，涉及到TensorRT-LLM中不同版本的InternVL2模型的问题。由于使用了不同的LLM架构，导致在使用multimodal_model_runner()时可能会出现低精度的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2672
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及TensorRT-LLM下的speculative decoding实现，由于缺乏清晰的文档或示例，用户无法配置后端服务以实现预测解码，且trtllmserve不支持该功能，导致用户请求指导和帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/2671
TensorRT-LLM,这是一个用户提出需求的问题，涉及TensorRT-LLM中构建带有权重稀疏性的模型时加速计算的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2670
TensorRT-LLM,这是一个需求更新的issue，主要对象是项目的README.md文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/2668
TensorRT-LLM,这是一个关于功能需求的问题，主要涉及在TensorRT-LLM中的MLP和Attention层中支持的低比特（int8/fp8/int4）数据类型。用户提出了关于各层中支持的数据类型和内部运行方式的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/2664
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的量化支持。由于新的24位quant格式提供更好的结果，用户建议整合该格式至TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/2663
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM中的cpp executor。用户询问如何将生成的token ids转换为可读的单词。这可能是由于用户需要更详细的指导来完成此操作。,https://github.com/NVIDIA/TensorRT-LLM/issues/2662
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM的Dockerfile中关于PyTorch Nightly支持的问题。原因是当前版本中稳定的PyTorch版本滞后于CUDA、cuDNN、NCCL等组件，用户希望增加对Nightly版本的支持以获得更多bug修复和功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2657
TensorRT-LLM,这个issue属于更新网站文档的类型，主要对象是项目的文档页面。由于可能需要更新项目网站上的内容或修复页面显示的错误。,https://github.com/NVIDIA/TensorRT-LLM/issues/2654
TensorRT-LLM,这个issue类型为用户提出需求，主要涉及对象为项目中的文档。由于文档需要更新或补充相关信息，用户希望对linux.md进行更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2653
TensorRT-LLM,"这个issue属于用户提出需求类型，主要涉及更新""disaggregated-service.md""文档。由于文档可能需要添加、修改或者更新内容，用户建议对该文档进行更新。",https://github.com/NVIDIA/TensorRT-LLM/issues/2650
TensorRT-LLM,这个issue类型是更新请求，主要涉及的对象是github页面。由于需要更新gh-pages，用户提出了这个issue。,https://github.com/NVIDIA/TensorRT-LLM/issues/2646
TensorRT-LLM,这是一个需求更新的issue，主要涉及到服务文档的更新。产生这个问题是因为服务文档需要添加或修改内容以满足新的需求或者更新信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/2645
TensorRT-LLM,这是一个功能需求相关的issue，涉及TensorRT-LLM中的采样层实现，主要问题是无法从前端控制另一个min_p参数，需求包括实现Min-P采样和后续温度调整功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2643
TensorRT-LLM,这是一个关于需求的提问，主要涉及对象是TensorRT-LLM中的attention机制。由于TensorRTLLM代码复杂且一般化程度高，导致用户困惑并希望了解如何在部署新模型时选择合适的attention类型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2639
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是希望在TensorRT-LLM中支持DeepSeek-V3模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2629
TensorRT-LLM,这是一个类型为文档更新的缺陷报告，该问题单涉及的主要对象是更新TensorRT-LLM的gh-pages分支。,https://github.com/NVIDIA/TensorRT-LLM/issues/2625
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM的trtllm-serve是否支持toolparser和guided-decoding，可能是因为用户想了解是否计划支持这些功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2624
TensorRT-LLM,这是一个更新文档的issue，主要对象是Windows版本，原因可能是发布了新版本需要更新相应的文档。,https://github.com/NVIDIA/TensorRT-LLM/issues/2623
TensorRT-LLM,这个issue属于更新网站页面的类型，主要涉及的对象是项目的网站页面。由于更新网站时出现问题，需要更新 gh-pages 分支。,https://github.com/NVIDIA/TensorRT-LLM/issues/2618
TensorRT-LLM,这是一个用户提出需求类的issue，主要涉及TensorRT-LLM模型编译的性能问题，反映出模型编译可能未能提高性能，需要帮助优化配置以减少响应时间。,https://github.com/NVIDIA/TensorRT-LLM/issues/2617
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是Phi4支持，用户想知道是否TensorRT-LLM支持Phi4。,https://github.com/NVIDIA/TensorRT-LLM/issues/2616
TensorRT-LLM,这是一个用户提出需求/请教问题的issue，主要涉及的对象是TensorRT-LLM，用户询问关于是否支持VLM模型的前缀缓存及如何启用该功能的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2608
TensorRT-LLM,这是一个关于现有功能限制的需求，涉及TensorRT-LLM中的Redrafter fp8支持问题，用户希望能够同时转换fp8的基础模型和fp32 Redrafter。原因是转换脚本目前仅接受fp16/fp32/bf16格式的基础模型，无法处理fp8格式的基础模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2607
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是w4a8 quantization支持，用户提出了希望在trtllm中增加更好的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2605
TensorRT-LLM,这是一个需求类别的issue，主要涉及的对象是创建一个c-cpp.yml文件。可能由于缺乏该文件导致项目无法正常运行或配置问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2601
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及如何传递原始图像张量数据给自定义模型的forward()函数，该问题可能是由于需要将原始图像张量数据作为参数传递给模型的forward()函数而产生。,https://github.com/NVIDIA/TensorRT-LLM/issues/2596
TensorRT-LLM,这是一个用户提出需求的类型问题，主要对象是关于DiT或Flux.1-dev相关模型的输入padding功能。这个问题可能是由于用户想要支持多种不同分辨率请求而产生的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2592
TensorRT-LLM,该issue类型为用户提出需求，请教问题，主要涉及对象是转换qwen2vl的可视部分为TensorRT模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2590
TensorRT-LLM,这是一个用户提出需求的类型issue，涉及在自定义encdec架构中切片具有动态形状的张量，可能是由于需要将编码器输出传递给解码器，并需要对动态形状的张量进行切片而导致的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2584
TensorRT-LLM,这个issue类型为更新反映，涉及主要对象为TensorRT-LLM。由于Windows构建出现问题，导致该bug需要修复或者用户寻求相关帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/2582
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及的对象是OpenAIServer。由于OpenAIServer目前只支持LLM而不支持VLM，因此用户提出了无法传递图片信息的问题，并希望后期能够解决。,https://github.com/NVIDIA/TensorRT-LLM/issues/2581
TensorRT-LLM,这是一个关于需求的问题，主要涉及TensorRT-LLM版本1.5是否支持InternVL 2.0模型，可能是由于图片预处理的版本差异引起的。,https://github.com/NVIDIA/TensorRT-LLM/issues/2578
TensorRT-LLM,这是一个关于性能优化的问题，主要涉及到GPU内存的使用情况。用户想要通过配置参数来减少TensorRT-LLM在运行Qwen2.5 7B qint4模型时的GPU内存占用。,https://github.com/NVIDIA/TensorRT-LLM/issues/2576
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是关于获取存储代码的位置，可能是由于缺少相关代码而无法使用或查看。,https://github.com/NVIDIA/TensorRT-LLM/issues/2569
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的tensorrtllm_backend，用户正在寻求针对InternVL2的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2568
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及支持LLaMa3.3模型在TensorRT-LLM中的部署。用户希望更新现有示例以整合新模型，并确认其与现有设置的兼容性。,https://github.com/NVIDIA/TensorRT-LLM/issues/2567
TensorRT-LLM,这是一个关于优化TensorRT-LLM的问题，用户提出了“Add issue triage workflows”的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2566
TensorRT-LLM,这是一个功能更新的Issue，主要涉及TensorRT-LLM的功能更新和改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/2562
TensorRT-LLM,这是一个关于软件功能理解的问题，主要涉及到TensorRT-LLM下的awq_w4a8模型，用户询问了关于safetensor结果中'scaling_factor'各项的含义和在w4a8 gemm中的使用。,https://github.com/NVIDIA/TensorRT-LLM/issues/2561
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及的对象是`infer_cluster_key()`方法。由于当前方法处理的设备列表不包括H200，用户希望添加H200（可能还有GB200）到列表中。,https://github.com/NVIDIA/TensorRT-LLM/issues/2552
TensorRT-LLM,这是一个用户提出需求（feature request）的issue，主要涉及lm_head的量化问题。由于lm_head的权重大小超过了10GB，在一些LLMs中，而当前无法对lm_head进行量化处理。,https://github.com/NVIDIA/TensorRT-LLM/issues/2550
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下trtllm-serve不支持类似tritonserver动态批处理的功能。用户希望通过使用tritonserver（tensorrtbackend）来实现OpenAI API，以获得动态批处理的优势。,https://github.com/NVIDIA/TensorRT-LLM/issues/2549
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT中Qwen2-VL FP8/INT8量化支持的实现。,https://github.com/NVIDIA/TensorRT-LLM/issues/2546
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持PaliGemma模型。由于PaliGemma模型已在transformers中得到支持，用户想了解TensorRT-LLM是否会添加对其的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2538
TensorRT-LLM,这是一个需求更新类型的issue，主要对象是TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/2532
TensorRT-LLM,这是一个类型是更新请求的issue，主要涉及的对象是Github上的TensorRT-LLM。由于网页需要更新，用户提出了更新Github页面的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2530
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是如何在使用executor API时通过每个请求设置eagle_choices或medusa_choices，由于无法在运行时更改它们，用户寻求关于如何在每个请求中设置这些选项的指导。,https://github.com/NVIDIA/TensorRT-LLM/issues/2522
TensorRT-LLM,这是一个功能需求报告，主要涉及的对象是TensorRT-LLM中的模型定义。由于用户希望实现通过onnx等跟踪方式生成模型定义，以便支持那些对trtllm结构不太熟悉或者非通用模型结构的用户，从而提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2519
TensorRT-LLM,"这个issue是用户提交的类型为需求，要求添加一个名为""blossom-ci.yml""的文件。",https://github.com/NVIDIA/TensorRT-LLM/issues/2512
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中是否支持GLM4Voice。用户希望知道是否有计划支持GLM-4-Voice，原因是希望在TensorRT-LLM中能够使用GLM4Voice这个端到端语音模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2510
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到是否存在用于低延迟的基准数据，以及关于TensorRT-LLM是否会开源内部的PDL gemm的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/2504
TensorRT-LLM,这是一个用户需求报告，涉及的主要对象是如何可视化TensorRT-LLM网络。由于TREx不支持LLM并且`trtllmbuil visualize_network`命令无效，用户想知道是否有工具或选项可以可视化LLMs的trt引擎。,https://github.com/NVIDIA/TensorRT-LLM/issues/2501
TensorRT-LLM,这个issue类型为用户提出需求，主要涉及的对象是在使用C++运行时时如何获取准确的TTFT和TPOT值。原因是由于使用C++运行时，难以准确获取所需的预填充延迟和时间复杂度信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/2500
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM模型中提取hidden_states的问题，由于TensorRTLLM没有提供接口访问hidden_states导致用户无法获取想要的参数。,https://github.com/NVIDIA/TensorRT-LLM/issues/2499
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到对MoE模型中自定义路由分布进行运行时配置的需求。由于当前方法需要重新构建引擎以支持不同的路由分布，用户希望找到一种动态配置路由分布而无需重建引擎的方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/2497
TensorRT-LLM,这个issue是用户提出的需求，主要涉及的对象是Qwen convert_checkpoint.py，由于数据集加载问题，用户需要在模型转换过程中添加trust_remote_code参数以避免交互提示。,https://github.com/NVIDIA/TensorRT-LLM/issues/2493
TensorRT-LLM,该issue为用户提出需求类型，主要涉及修改Encoder Decoder模型架构以支持新的配置，寻求关于如何修改代码以支持新架构的帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/2491
TensorRT-LLM,"该issue是关于用户需求的，涉及到如何在TensorRT-LLM中设置""max_num_tokens""和""max_batch_size""作为运行时参数。用户询问如何在Triton上定义这些值，或者在Sagemaker上传递这些值。这个问题实质上是在问如何在不同环境中配置TensorRT-LLM的运行时参数。",https://github.com/NVIDIA/TensorRT-LLM/issues/2490
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中attention的使用。由于用户想要单独使用attention类，但受到了默认网络设置的影响，导致需要指导如何解决。,https://github.com/NVIDIA/TensorRT-LLM/issues/2477
TensorRT-LLM,这是一个关于需求的issue，主要涉及TensorRT-LLM引擎中的确定性相关问题。用户提出了关于如何实现高吞吐量确定性生成的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2474
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下是否支持部署4bit quantised unsloth Llama模型，用户想要知道是否可以使用tensorRTLLM后端来部署这个模型，是否有相关文档可供参考。,https://github.com/NVIDIA/TensorRT-LLM/issues/2472
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及vLLM和TensorRTLLM在`transformers`版本不一致导致无法共同使用的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2465
TensorRT-LLM,"这是一个用户提出需求的issue，主要涉及添加一个名为""blossom-ci.yml""的文件。",https://github.com/NVIDIA/TensorRT-LLM/issues/2464
TensorRT-LLM,这个issue属于更新提议，主要涉及了TensorRT-LLM的模型支持和API更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2460
TensorRT-LLM,这是一个关于功能需求的问题，涉及的主要对象是TensorRT-LLM中的MultiModalRunner.py，用户尝试在其中使用enableBlockReuse参数但遇到错误，询问该功能在多模态情况下是否可用。,https://github.com/NVIDIA/TensorRT-LLM/issues/2459
TensorRT-LLM,这是一个用户提出需求的问题，涉及主要对象是TensorRT-LLM下的模型加载和量化转换，用户希望能够直接从huggingface加载量化模型并转换为TensorRTLLM checkpoint或engine，而不需要经过校准过程。,https://github.com/NVIDIA/TensorRT-LLM/issues/2458
TensorRT-LLM,这是一个关于功能需求的问题，主要涉及TensorRT-LLM中的在线微调和In-flight batching功能如何同时启用的问题。由于TensorRT的Executor不兼容在线微调功能，而CPG Runtime不支持In-flight batching功能，用户想知道是否可以在当前版本中同时启用这两项功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2453
TensorRT-LLM,这个issue是一个功能请求，主要涉及Qserve和int8_kv_cache的集成问题。由于int8_kv_cache与Qserve的集成尚未实现，用户希望团队能够检查并实现这一功能，以便在TensorRTLLM上使用。,https://github.com/NVIDIA/TensorRT-LLM/issues/2444
TensorRT-LLM,这是一个需求报告类型的 issue，主要涉及的对象是将 RoBERTa 模型部署到 Triton 服务器并进行推理。问题产生的原因是缺乏关于如何在 TensorRT-LLM 下进行 BERT 模型的推理的详细指南。,https://github.com/NVIDIA/TensorRT-LLM/issues/2440
TensorRT-LLM,这是一个用户提出需求的issue，涉及的主要对象是TensorRT-LLM中的FA V2（Flash Attention Version 2），用户想了解为什么在预生成阶段使用FA V2，但在生成阶段却不使用，是否是因为Flash attention在解码阶段没有显著的性能提升。,https://github.com/NVIDIA/TensorRT-LLM/issues/2438
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及需要将structured text decoding library outlines集成为TensorRTLLM的一个dependency。问题的原因是希望通过 outlines 的 LogitsProcessor 解决 TensorRTLLM 在大批量大小下的吞吐问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2432
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是为量化模型添加支持LORA推理的功能。提出需求的原因是用户希望在AWQ 4bit量化模型的推理时使用LORA，但目前该功能尚不支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2431
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是是否支持FLUX。由于当前的TensorRT-LLM不支持多GPU并行，用户询问是否有计划在未来支持FLUX。,https://github.com/NVIDIA/TensorRT-LLM/issues/2421
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是要在TensorRT-LLM中加入ColBERT模型。由于用户认为ColBERT的密集检索方法可能是一个有价值的补充，他想评估在现有框架中集成它的可行性。,https://github.com/NVIDIA/TensorRT-LLM/issues/2415
TensorRT-LLM,这是用户提出需求的 issue，主要涉及将Finetuned Llama模型导出为TensorRT格式，但由于相关资源和信息有限，用户寻求相关实现方式。,https://github.com/NVIDIA/TensorRT-LLM/issues/2412
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是关于更新LLM API参考页面。原因可能是现有API参考页面内容需要更新或完善。,https://github.com/NVIDIA/TensorRT-LLM/issues/2410
TensorRT-LLM,这个issue类型是文档更新请求，涉及主要对象为项目的Github Pages。由于项目文档或网页内容需要更新，所以提出这个请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2404
TensorRT-LLM,这是一个更新请求，涉及TensorRT-LLM的API改动和功能增强，主要对象是TensorRT-LLM的v0.14.0版本。,https://github.com/NVIDIA/TensorRT-LLM/issues/2401
TensorRT-LLM,这是一个用户需求问题，涉及主要对象为PyTorch，提出了停止在其命名空间内导出optional的要求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2397
TensorRT-LLM,这个issue是关于需求的，主要涉及的对象是kernel的重写，用户寻求在不参考cutlass实现的情况下如何重写这个kernel。,https://github.com/NVIDIA/TensorRT-LLM/issues/2396
TensorRT-LLM,这是一个需求报告，主要对象是在TensorRT-LLM中添加对interlvl2的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2394
TensorRT-LLM,该问题单属于更新需求，主要涉及的对象是TensorRT-LLM项目。由于缺少具体的更新内容，用户希望得到最新消息的更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2391
TensorRT-LLM,这是一个更新（feature update）类型的issue，涉及主要对象是TensorRT-LLM。由于TensorRT 10.5 中构建 MLLaMa model 存在已知问题，导致用户需要将 `tensorrt` 包版本降级到 10.4.0 作为解决方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/2389
TensorRT-LLM,这是一个功能需求提出的issue，主要对象是TensorRT-LLM中的attention mechanism。这个需求是为了让用户在加载模型时可以选择是否使用注意力机制，以适应旧版GPU的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2384
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及TensorRT-LLM的多LoRA设置集成问题。造成问题可能是由于引擎构建时的参数配置或文件路径设置不正确。,https://github.com/NVIDIA/TensorRT-LLM/issues/2371
TensorRT-LLM,该issue类型为功能需求，主要涉及TensorRT-LLM中的fast-forward tokens in logits post processor功能。由于目前无法在输出阶段追加多个token到序列中，用户提出需要增加这一功能以提高性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2365
TensorRT-LLM,这是一个需求类型的issue，主要涉及对象为c++ inference example。由于缺乏具体的C++示例，用户询问是否可以提供一个包含输入和输出单词的示例。,https://github.com/NVIDIA/TensorRT-LLM/issues/2361
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM库中的C++ API。用户想要了解如何使用C++ API设置ptuning提示嵌入表。由于C++ API中缺少设置提示嵌入表和词汇表大小的功能，用户希望找到解决方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/2359
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及对象是在TensorRT-LLM中寻找enqueue_requests函数实现以及qwen模型的输入头文件代码，以及trtllmbuild执行的入口函数。用户提出问题的原因可能是希望找到相应的代码实现并缺少相关文档信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/2353
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及到TensorRT-LLM中的in-flight batch strategy功能。问题可能由于缺乏关于该策略实现的教程而导致用户难以理解这一技术。,https://github.com/NVIDIA/TensorRT-LLM/issues/2349
TensorRT-LLM,这是一个关于功能需求的 issue，主要涉及到TensorRT-LLM中的Eagle实现状态。这个问题由于目前在最新版本中尚未看到与Eagle Speculative decoding 相关的运行时代码，导致用户提出了关于这种模式是否会很快得到支持的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/2345
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及LLM下的readme文档，原因是需要指定LLama 3.x信息并更新VLM支持情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/2343
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是支持qwen2.5模型，由于目前不支持该模型，用户希望在TensorRT-LLM中添加对qwen2.5模型的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2336
TensorRT-LLM,这是一个关于性能比较和可能优化的issue，主要涉及Expert Parallelism (EP)和Tensor Parallelism (TP)的比较。原因是由于现有实现导致在某些情况下TP总是优于EP，但提出了可能通过专用alltoall EP实现来提高性能的猜想。,https://github.com/NVIDIA/TensorRT-LLM/issues/2331
TensorRT-LLM,这是一个用户需求问题，用户想知道在TensorRT-LLM中是否有办法从attention中获取QK分数。,https://github.com/NVIDIA/TensorRT-LLM/issues/2326
TensorRT-LLM,这个issue类型为用户提出需求，请教问题，主要涉及对象为C++ Executor Leader Mode。用户询问是否可能在一个GPU设备上有一个进程内多个处于Leader模式的Executor。,https://github.com/NVIDIA/TensorRT-LLM/issues/2322
TensorRT-LLM,这是一个功能需求请求类型的issue，涉及主要对象为executor::Response。由于prepopulatedPromptLen字段为私有对象，导致用户无法正常访问，希望能在createResponse函数中添加mPrepopulatedPromptLen字段。,https://github.com/NVIDIA/TensorRT-LLM/issues/2318
TensorRT-LLM,这是一个关于需求的问题，主要涉及TensorRT-LLM中使用fused multi-head attention时数据精度下降的问题。由于需求是希望保持数据精度以提高准确性，用户提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2312
TensorRT-LLM,该issue类型为用户提出需求，主要涉及对象为TensorRT-LLM，用户想了解是否能仅使用CPU进行推理。,https://github.com/NVIDIA/TensorRT-LLM/issues/2308
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到如何在TensorRT-LLM中实现最大GPU利用率以及如何使用多个LLM模型和嵌入模型。用户询问如何通过TensorRT-LLM来管理GPU资源以及是否有计划实现支持Triton托管模型的API。,https://github.com/NVIDIA/TensorRT-LLM/issues/2307
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及Python Executor API的实现，需要添加request interruption功能，可能由于需要在程序执行过程中进行中断操作而提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/2306
TensorRT-LLM,这是一个用户提出需求的类型的issue，主要涉及的对象是如何在TensorRT-LLM中实现和使用自定义的核心，或者如何有效地在两个GPU之间传输kv_cache。导致该问题的原因可能是用户想要了解如何实现类似于https://github.com/microsoft/mscclpp的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2304
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及对象是在deepstream pipeline中集成TensorRT-LLM。用户询问如何在deepstream中使用TensorRT-LLM引擎的具体方法及遇到的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2292
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是在TensorRT-LLM项目中创建特定架构（sm87）的cubin.cpp文件。用户提问是由于缺少sm87架构的cubin.cpp文件，不知道如何创建这些文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/2291
TensorRT-LLM,该issue类型是文档更新，主要对象是高级文档。由于文档中存在一个小错误，导致需要进行修正。,https://github.com/NVIDIA/TensorRT-LLM/issues/2290
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM网站。由于缺少复制代码小部件，用户在网站上手动复制粘贴代码时出现了问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2288
TensorRT-LLM,这个issue类型是用户提出需求，主要对象是文档。由于BF16缺失，用户建议添加，以完善文档内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/2285
TensorRT-LLM,这是一个关于需求提问的issue，主要涉及的对象是TensorRT-LLM，用户想知道是否支持在RISCV上运行。,https://github.com/NVIDIA/TensorRT-LLM/issues/2279
TensorRT-LLM,这是一个请求更新 gh-pages 页面的 issue，涉及的对象是项目的文档。原因可能是需要更新或改进项目的文档信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/2276
TensorRT-LLM,这个issue类型是关于TensorRT-LLM v0.13版本更新的，主要涉及到新功能和增强。,https://github.com/NVIDIA/TensorRT-LLM/issues/2269
TensorRT-LLM,该issue类型为版本更新需求，涉及主要对象是代码库中的版本号。由于代码库需要升级至新版本，因此提出了更新版本号的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2258
TensorRT-LLM,该issue类型为用户提出需求，请教问题，主要涉及对象为如何向TensorRT-LLM中添加新的多模态模型。该问题由于缺乏相关文档或经验指导，导致用户寻求帮助以添加名为Qwen2的LLM模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2257
TensorRT-LLM,这是一个功能更新的issue，主要涉及TensorRT-LLM模型的更新和改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/2253
TensorRT-LLM,这个issue属于用户提出需求的类型，主要涉及C++运行时如何支持新的多模态模型llava-one-vision，用户询问是否有参考文档，并提到了在执行指令时遇到的一些问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2250
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的encoder_repetition_penalty，用户询问是否在TensorRT-LLM的路线图中实现了该功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2249
TensorRT-LLM,这是一个用户在询问功能支持的类型问题，主要涉及TensorRT-LLM是否支持input_embeds作为输入。原因可能是用户想要确认该功能在TensorRT-LLM中是否可用。,https://github.com/NVIDIA/TensorRT-LLM/issues/2245
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是在TensorRT-LLM中添加一份第三方推理速度仪表板。用户提出需求的原因是想要分享有关推理速度仪表板的社区链接资源，其中包括综合性的基准测试和各种优化技术。,https://github.com/NVIDIA/TensorRT-LLM/issues/2244
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及对象为是否TensorRT-LLM支持llama3.1的序列分类。用户询问是否可以在TensorRT-LLM中使用llama3.1的序列分类，并询问在修改代码以添加此功能时是否会遇到障碍。,https://github.com/NVIDIA/TensorRT-LLM/issues/2242
TensorRT-LLM,该问题类型为用户提出需求，主要对象是在使用TensorRT-LLM时，用户发现与VLLM相比，使用TensorRT更加繁琐。原因是在使用TensorRT时需要进行多个步骤才能实现类似VLLM的简单功能，导致用户感受不便。,https://github.com/NVIDIA/TensorRT-LLM/issues/2237
TensorRT-LLM,这个issue是用户提出的需求。主要涉及的对象是TensorRT-LLM。用户希望在不同的GPU或不同的节点上自行配置预加载阶段和解码阶段，以支持自身应用需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2235
TensorRT-LLM,这是一个更新版本的issue，主要对象是TensorRT-LLM。由于开发人员想要将版本提升到`0.14.0.dev2024091700`。,https://github.com/NVIDIA/TensorRT-LLM/issues/2234
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的模型ChatGLM的多Lora支持，用户试图添加多Lora支持但遇到了问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2223
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是支持florence2。可能是用户想要在TensorRT-LLM中使用florence2，但目前还不支持，所以提出了这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2221
TensorRT-LLM,这是一个关于使用nccl ops插件的需求问题，主要涉及到TensorRT-LLM的插件加载机制。用户想要了解如何在其项目中使用由TensorRT-LLM提供的nccl ops插件。,https://github.com/NVIDIA/TensorRT-LLM/issues/2220
TensorRT-LLM,这个issue是关于用户提出需求，并询问如何处理在TensorRT-LLM中不支持的操作，主要涉及到TensorRT-LLM中无法直接支持的复杂数据类型和FFT操作的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2218
TensorRT-LLM,这个issue是一个功能更新的类型，涉及的主要对象是TensorRT-LLM。原因是更新添加了新功能并改变了默认设置，用户可能需要了解这些更改并进行相应的调整。,https://github.com/NVIDIA/TensorRT-LLM/issues/2215
TensorRT-LLM,这是一个优化提案，主要涉及TensorRT-LLM中的small-batched weight only quantization，通过使用shared memory和memcpy_async来减少全局内存负载延迟。,https://github.com/NVIDIA/TensorRT-LLM/issues/2213
TensorRT-LLM,这是一个用户提出需求的问题，主要对象是在TensorRT-LLM生成函数中是否能够直接传递input_embeds参数。,https://github.com/NVIDIA/TensorRT-LLM/issues/2211
TensorRT-LLM,这是一个需求更改类型的用户提出的问题，涉及到模型输入格式的更改，由于用户希望改变输入数据的格式，导致无法正常使用模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2193
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM版本对应的CUDA、TensorRT、Python版本信息，并询问最低要求版本，原因可能是为了确保兼容性和最佳性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2189
TensorRT-LLM,这是一个功能更新的issue，主要涉及TensorRT-LLM的更新和API变更。用户提出了关于新增功能和API变更的需求或问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2184
TensorRT-LLM,这个issue类型是需求报告，涉及主要对象是支持Qwen2-VL。由于Qwen2-VL添加了新功能MROPE，而TensorRT-LLM目前不支持，因此用户提出希望支持该功能的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2183
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是Executor TP在多个GPU上是否可以在同一台主机上运行而不需要MPI。询问这个问题可能是由于用户想要在同一主机上利用多个GPU运行Executor TP而不依赖于MPI。,https://github.com/NVIDIA/TensorRT-LLM/issues/2178
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是有关TensorRT-LLM中的batch_manager和executor的CPP代码。用户询问何时开源这两部分代码，可能是希望获得相关的开源支持或进一步研究。,https://github.com/NVIDIA/TensorRT-LLM/issues/2174
TensorRT-LLM,这是一个用户提出需求并请教问题的类型，主要涉及TensorRT-LLM中的PromptTuningEmbedding参数。由于用户对任务和标记的含义以及特定标记的取值范围不清楚，导致了提出了问题并寻求帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/2171
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM的优化服务问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2169
TensorRT-LLM,这是一个需要更新文档页面的类型为“其它”的issue，主要涉及到项目的网页文档 更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/2168
TensorRT-LLM,这是一个特性请求，主要涉及TensorRT-LLM中的KV缓存复用和Int8 KV 缓存的兼容性问题。由于当前的页上下文FMHA不支持Int8 KV缓存，导致用户无法同时使用KV缓存复用和Int8 KV缓存，限制了在大规模部署中优化性能和效率的能力。,https://github.com/NVIDIA/TensorRT-LLM/issues/2166
TensorRT-LLM,这个issue类型是一个功能需求，主要对象是在TensorRT-LLM下添加Windows库到版本0.12中。,https://github.com/NVIDIA/TensorRT-LLM/issues/2165
TensorRT-LLM,这是一个描述TensorRT-LLM v0.12 更新的issue，主要涉及的对象是更新的主要功能和增强，提供了对LLA和MoE等模型的支持，以及一系列功能和模型的增强。,https://github.com/NVIDIA/TensorRT-LLM/issues/2164
TensorRT-LLM,这是一个关于性能优化的问题，用户希望在GPU上获取包括context logit tensors的输出。,https://github.com/NVIDIA/TensorRT-LLM/issues/2163
TensorRT-LLM,这个issue是关于需求提出的，主要涉及TensorRT-LLM模型的更新和改进，其中用户提出了对Model Support、API、Bug fixes、Benchmark等方面的具体需求和改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/2156
TensorRT-LLM,该issue类型为功能需求，主要涉及到创建名为sync.yml的文件。原因可能是用户需要与指定的同步配置进行交互。,https://github.com/NVIDIA/TensorRT-LLM/issues/2154
TensorRT-LLM,这是一个需求更改类的issue，主要对象是更新快速入门指南文档。,https://github.com/NVIDIA/TensorRT-LLM/issues/2151
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM模型是否支持接受两个图像作为输入的功能。原因可能是用户想了解当前的TensorRT-LLM是否支持这种模型结构。,https://github.com/NVIDIA/TensorRT-LLM/issues/2144
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的GPT模型推断。用户希望得到一个C++示例代码，而当前官方例子只提供Python版本。,https://github.com/NVIDIA/TensorRT-LLM/issues/2142
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的Whisper models，用户希望实现对Whisper模型的多 Lora 支持。这个需求可能是由于用户有多个 Lora 设备需要支持，希望能够在 Whisper 模型中实现多个 Lora 的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/2136
TensorRT-LLM,这个issue是一个功能需求类型，主要涉及TensorRT-LLM下的Max Forward Passes支持，用户提出希望引入maxForwardPasses支持来缓解高延迟问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2127
TensorRT-LLM,这是一个需求报告，主要对象是TensorRT-LLM下无法支持32k context输入，根本原因是内存限制导致OOM错误。,https://github.com/NVIDIA/TensorRT-LLM/issues/2122
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的C++ batch manager API的移除及替换问题，用户关注是否C++ executor API会与batch manager功能保持一致。,https://github.com/NVIDIA/TensorRT-LLM/issues/2114
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM，用户询问是否会支持KOSMOS-2.5版本。,https://github.com/NVIDIA/TensorRT-LLM/issues/2106
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持直接传递final input_embeds，并由于组合不同的视觉特征和input_embeds导致无法满足需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2104
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM项目中添加新的量化方法，用户希望了解如何在代码中实现INT8 KV cache量化以及如何集成CUDA核函数和Python绑定。,https://github.com/NVIDIA/TensorRT-LLM/issues/2102
TensorRT-LLM,这个issue是一个功能需求报告，涉及的主要对象是TensorRT-LLM，用户希望添加对Nougat模型的支持，但当前系统不支持，可能是由于TensorRT-LLM尚未集成Nougat相关功能导致。,https://github.com/NVIDIA/TensorRT-LLM/issues/2098
TensorRT-LLM,这是一个关于功能优化的问题，主要涉及TensorRT-LLM中的streaming推理过程，用户提出了生成的logits格式不够高效的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2090
TensorRT-LLM,这是一个优化性能的issue，涉及到TensorRT-LLM下的decoder MMHA kernel支持INT8 SCALE_Q_INSTEAD_OF_K and SCALE_P_INSTEAD_OF_V，用户提出了性能优化的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2085
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到TensorRT-LLM中的InternVL2支持问题。由于未收到响应，用户正在寻求帮助验证InternVL2的支持情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/2083
TensorRT-LLM,这是一个关于支持cuda12.5和tritonserver 24.07-py3的需求问题，讨论了支持的版本信息和相关环境配置。,https://github.com/NVIDIA/TensorRT-LLM/issues/2082
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是模型加载代码，用户想要修改加载模型的逻辑。,https://github.com/NVIDIA/TensorRT-LLM/issues/2068
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是Propagate QuantConfig.exclude_modules to weight only quantization。,https://github.com/NVIDIA/TensorRT-LLM/issues/2056
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及对象是在TensorRT-LLM中添加图像嵌入到模型输入中的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/2055
TensorRT-LLM,这个issue类型是更新通报，涉及TensorRT-LLM的功能更新和基础设施升级，而非bug报告或用户提出需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2053
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是通过给定数据集来进行吞吐量基准测试。原因可能是用户想要比较不同框架的吞吐量并找到与 benchmarks/benchmark_throughput.py 相似的基准测试脚本。,https://github.com/NVIDIA/TensorRT-LLM/issues/2046
TensorRT-LLM,这是一个用户提出的特性需求，主要涉及Executor API中缺少灵活的logit后处理器API。这个问题存在的原因是当前的logit后处理器实现要求事先在一个映射表中注册后处理器，然后在请求时按名称引用后处理器，无法传递额外的每个请求参数给后处理器。,https://github.com/NVIDIA/TensorRT-LLM/issues/2044
TensorRT-LLM,这是一个关于功能支持的问题，涉及主要对象是TensorRT-LLM和NVIDIA Jetson Orin Nano开发套件。用户想了解Jetson Orin Nano开发套件是否支持在上运行TensorRTLLM，可能由于缺乏相关信息而产生疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/2041
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中关于使用多个Lora的问题，用户希望能够同时使用多个Lora进行推理。,https://github.com/NVIDIA/TensorRT-LLM/issues/2038
TensorRT-LLM,这是一个需求更新的issue，主要对象是TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/2033
TensorRT-LLM,这是一个优化建议类型的issue，主要涉及TensorRT-LLM中的交叉注意力计算，用户认为当前计算过程中存在冗余，建议优化以提高性能和量化效果。,https://github.com/NVIDIA/TensorRT-LLM/issues/2026
TensorRT-LLM,这是一个用户提出需求的类型的issue，请求TensorRT-LLM支持Mistral Large 2模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/2024
TensorRT-LLM,这是一个类型是需求更新的issue，主要涉及更新TensorRT-LLM。由于未提供具体的描述内容，无法分析具体的问题或需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2016
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及的对象是TensorRT-LLM下的支持Llama 3.1的功能。由于Llama 3.1中前向模型部分发生了小改动，导致当前版本的TensorRTLLM无法运行此版本，用户希望更新TensorRTLLM以支持Llama 3.1。,https://github.com/NVIDIA/TensorRT-LLM/issues/2015
TensorRT-LLM,这是一个关于性能优化的咨询问题，主要涉及到TensorRT-LLM中fp8和fp16内核在H100平台上性能比较的情况。由于fp8内核表现不佳，导致在解码阶段的性能比fp16差。,https://github.com/NVIDIA/TensorRT-LLM/issues/2013
TensorRT-LLM,这是一个功能需求报告类型的 issue，涉及支持加载 fp8 hf 权重。,https://github.com/NVIDIA/TensorRT-LLM/issues/2010
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及 Prefix Caching 是否支持CPU卸载，用户希望了解是否有计划支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/2003
TensorRT-LLM,这是一个用户提出需求的问题单，主要涉及TensorRT-LLM下的Context FMHA是否能够用来实现Transformer在视觉编码器中的应用，用户关注为什么示例中的多模态模型都直接使用TensorRT来部署视觉编码器，而不使用TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/2001
TensorRT-LLM,这是一个需求类型的issue，主要涉及TensorRT-LLM中缺乏支持interleaved moe模型架构的功能，用户提出需要添加对interleaved moe的支持以及调整权重命名方式的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/2000
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及FMHA kernels中是否支持自定义alibi slopes，可能由于功能限制或设计缺陷导致用户无法使用自定义alibi slopes。,https://github.com/NVIDIA/TensorRT-LLM/issues/1997
TensorRT-LLM,这是一个关于需求咨询的issue，主要涉及对象是构建TensorRT-LLM Backend for Triton server时关于GPU要求的问题。该问题的产生是由于缺少GPU驱动而导致构建时出现错误。,https://github.com/NVIDIA/TensorRT-LLM/issues/1989
TensorRT-LLM,该issue属于用户提出需求类型，主要涉及支持Mistral Nemo模型在TensorRT-LLM中的问题，用户想了解在不久的将来是否会支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1985
TensorRT-LLM,这是用户提出的需求类型的issue，主要涉及到TensorRT-LLM下的模型支持问题，用户希望添加对GEMMA2的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1984
TensorRT-LLM,这是一个需要更新Windows部分文档的issue，主要涉及TensorRT-LLM项目的文档维护，可能是由于文档不完整或有过时内容导致用户反馈需更新文档。,https://github.com/NVIDIA/TensorRT-LLM/issues/1979
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及更新Windows部分文档。原因可能是旧文档不准确或缺失某些信息，用户希望更新以提供更准确的指导。,https://github.com/NVIDIA/TensorRT-LLM/issues/1977
TensorRT-LLM,这是一个文档更新的issue，主要涉及更新GitHub页面中图片的超链接。原因可能是当前页面中的图片链接需要更新或修复。,https://github.com/NVIDIA/TensorRT-LLM/issues/1973
TensorRT-LLM,这是一个用户需求类型的issue，该问题单涉及的主要对象是更新GitHub页面。,https://github.com/NVIDIA/TensorRT-LLM/issues/1972
TensorRT-LLM,这是一个升级发布版本的需求类型的issue，主要涉及的对象是Github页面（gh-pages）。这个issue出现的原因可能是为了更新项目发布版本0.11在Github页面上的信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/1971
TensorRT-LLM,这是一个更新通知类型的issue，主要涉及TensorRT-LLM v0.11版本的一些关键功能和增强，包括支持长上下文、低延迟优化和LoRA增强等新特性。,https://github.com/NVIDIA/TensorRT-LLM/issues/1969
TensorRT-LLM,这是一个关于更新最新消息的问题，属于需求提出类型，主要对象是TensorRT-LLM。由于缺乏详细信息，用户需要更新最新消息，可能是为了获取最新的功能、改进或者其他重要信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/1966
TensorRT-LLM,这个issue类型是用户提出需求，该问题单涉及的主要对象是添加对minicpm模型的支持，因为minicpm模型在chatRTX中具有良好效果且参数较小，希望能够结合使用。,https://github.com/NVIDIA/TensorRT-LLM/issues/1962
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM下如何执行跨注意力机制与FMHA核的问题。由于两种模态可能具有不同的序列长度，用户想知道如何在这种情况下执行FMHA以及是否可以修改插件以确保使用FMHA。,https://github.com/NVIDIA/TensorRT-LLM/issues/1956
TensorRT-LLM,该issue是关于更新TensorRT-LLM的新特性和API变更，涉及到了软件功能和接口的更新和改动。,https://github.com/NVIDIA/TensorRT-LLM/issues/1954
TensorRT-LLM,这是一个特性需求相关的问题，主要涉及TikToken的集成。原因可能是用户希望在TensorRT-LLM中集成TikToken功能以实现某种特定功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1952
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM中关于在V100上通过双缓冲加速weightOnlyGemm操作的相关讨论。原因是TRTLLM不支持在V100上进行组相关的weightOnlyGemm，导致用户在尝试加速该操作时遇到了问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1951
TensorRT-LLM,这是一个用户提出需求的问题，询问是否TensorRT-LLM支持blip2与fp8量化。,https://github.com/NVIDIA/TensorRT-LLM/issues/1949
TensorRT-LLM,这是一个用户提出需求的类型，该问题涉及的主要对象是支持FlashAttention 3，用户询问是否有支持的计划。,https://github.com/NVIDIA/TensorRT-LLM/issues/1947
TensorRT-LLM,这是一个关于用户需求的问题，主要涉及如何在TensorRT-LLM中量化自定义模型，作者想知道是否可以定义自己的模型和校准过程，然后简单地使用modelopt.torch.quantization.quantize()。,https://github.com/NVIDIA/TensorRT-LLM/issues/1945
TensorRT-LLM,这个issue是一个功能请求，请求支持vAttention风格的分页功能以实现动态内存管理。,https://github.com/NVIDIA/TensorRT-LLM/issues/1944
TensorRT-LLM,这个Issue属于用户提出需求类型，主要对象是为TensorRT项目创建一个Discord频道。这可能是因为缺少Slack、Discord或IRC频道而导致用户希望在特定频道中讨论有关NVIDIA TensorRT、TensorRTLLM和Triton推理服务器的问题，并希望在频道中广告以吸引更多用户参与讨论。,https://github.com/NVIDIA/TensorRT-LLM/issues/1943
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及的对象是InternVL2.0和InternVL1.5模型。由于这两个模型具有相同的架构，用户认为下载统计可能需要重新考虑。,https://github.com/NVIDIA/TensorRT-LLM/issues/1934
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是支持谷歌的Paligemma多模态模型。由于用户希望了解是否有计划支持该模型，因此提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1931
TensorRT-LLM,这是一个用户提出需求的issue， 主要对象是TensorRT-LLM。这个需求提出要添加对自定义分词器和批处理大小的支持，可能是为了增加模型的灵活性和适用性。,https://github.com/NVIDIA/TensorRT-LLM/issues/1927
TensorRT-LLM,这是一个用户提出需求的类型issue，主要对象是TensorRT-LLM项目，用户要求增加对falcon2的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1926
TensorRT-LLM,这是一个用户提出需求的issue，涉及主要对象是TensorRT-LLM中的`masked_multihead_attention_kernel`。用户询问关于为何在该函数中的注意力查询（Q）没有使用FP8量化，以及是否有考虑到精度等方面的原因。,https://github.com/NVIDIA/TensorRT-LLM/issues/1924
TensorRT-LLM,这是一个用户提出需求的issue，涉及主要对象为TensorRT-LLM下的weight-only GEMM。由于weight_only_groupwise_quant_matmul只支持fp16类型的zeropoints作为输入，导致一些模型中保存的zeropoints为int4类型时需要进行数据类型转换，用户希望增加对int类型zeropoints的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1922
TensorRT-LLM,这是一条更新通知，不是bug报告，主要涉及TensorRT-LLM模型支持的增加以及功能更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/1918
TensorRT-LLM,这是一个关于性能优化需求的问题，主要涉及TensorRT-LLM在RTX4090上使用Sparsity fp8 Llama38b时未能获得性能提升，可能是由于稀疏优化策略未被选择，导致用户提出了关于为何在RTX4090上无法获得性能提升以及关于4090支持计划的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/1913
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及TensorRTLLM对fp32 LoRA支持的问题，用户希望TensorRTLLM能够接受fp32 LoRA并在fp32下对LoRA低秩矩阵进行乘法运算，然后将乘积矩阵量化为与基础模型的fp16相匹配，以减少量化损失。,https://github.com/NVIDIA/TensorRT-LLM/issues/1910
TensorRT-LLM,这是一个关于功能支持问题的issue，主要涉及的对象是TensorRT-LLM中的RecurrentGemma模块。原因在于是否RecurrentGemma支持ModelRunnerCpp，导致用户提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1905
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM，用户希望知道是否计划支持Llavanextimage和Llavanextvideo模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1900
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM中的批量推理结果的差异，用户希望了解如何在llavanextvideo模型中设置参数以达到类似于Hugging Face transformers模型generate方法中do_sample=False参数设置的效果。,https://github.com/NVIDIA/TensorRT-LLM/issues/1899
TensorRT-LLM,这是一个用户提出需求的类型问题，主要涉及TensorRT-LLM是否计划支持Dual Chunk Attention (DCA)功能，用户关注是否能支持运行长输入上下文推断。,https://github.com/NVIDIA/TensorRT-LLM/issues/1898
TensorRT-LLM,这是用户提出的需求类型的issue，主要涉及TensorRT-LLM的激活函数支持问题，用户希望添加对`gelu_pytorch_tanh`激活函数的识别。,https://github.com/NVIDIA/TensorRT-LLM/issues/1897
TensorRT-LLM,这是一个用户需求类型的issue，涉及主要对象是TensorRT-LLM下的MInference功能。用户提出根据论文指出可以通过不同的attention机制实现更快10倍的预填充速率，希望实现更快速的预填充。,https://github.com/NVIDIA/TensorRT-LLM/issues/1896
TensorRT-LLM,这是一个关于建议和未来维护的问题，主要涉及到了GPTSession在C++运行时的推理应用。用户提出了executor的灵活性限制的问题，例如无法进一步开发splitwise等方法进行测试。,https://github.com/NVIDIA/TensorRT-LLM/issues/1894
TensorRT-LLM,这个issue属于更新需求类型，主要涉及TensorRT-LLM在特定功能方面的更新。原因是添加了新功能，支持了更多的模型和参数配置选项。,https://github.com/NVIDIA/TensorRT-LLM/issues/1891
TensorRT-LLM,这是用户提出需求的类型，用户在询问关于TensorRT-LLM支持glm4的问题。这个问题可能是由于目前TensorRT-LLM版本尚不支持glm4，用户希望了解是否计划在未来版本中添加此支持所导致的。,https://github.com/NVIDIA/TensorRT-LLM/issues/1882
TensorRT-LLM,这是一个需求类型的issue，主要涉及的对象是Jeston Orin MLPerf 4.1，由于开发需要而提交。,https://github.com/NVIDIA/TensorRT-LLM/issues/1880
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及了TensorRT-LLM是否支持类似huggingface transformer past_key_values的参数。用户希望能够提前计算kv缓存并传递给ModelRunner.generate()来加快解码速度。,https://github.com/NVIDIA/TensorRT-LLM/issues/1867
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及到如何在TensorRT-LLM中选择性地在推理过程中仅对模型的单个层或头运行前向传递的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1866
TensorRT-LLM,这个issue是关于提出需求的，主要涉及到的对象是代码中的参数命名问题，导致可能无法信任远程代码。,https://github.com/NVIDIA/TensorRT-LLM/issues/1863
TensorRT-LLM,这是一个关于功能需求的问题，主要涉及LLAMA 3 70B或LLama 3 8B是否支持XQA，用户询问原因是文档上没有明确提到LLAMA 3是否支持这个功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1858
TensorRT-LLM,这个issue类型为用户提出需求，涉及主要对象为TensorRT-LLM。由于argument parser的设计不够灵活和可重用，用户提出了关于重构argument parser来实现更好的可读性和重用性的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1848
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的BertForSequenceClassification模型。由于模型输入发生变化，需要支持`remove_input_padding`，并对`build.py`脚本进行改进，添加一个新的`run_remove_input_padding.py`演示脚本来展示如何构建模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1834
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及IFB支持添加到更多模型的时间表。由于目前只有少数模型支持IFB，缺乏对其他模型的支持严重影响了TRTLLM框架在生产环境中的实用性。,https://github.com/NVIDIA/TensorRT-LLM/issues/1832
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及在TensorRT-LLM上新增GLM-4系列模型的支持。原因是新的GLM4系列模型具有更好的性能和准确性，用户期望TensorRT能够官方支持这些模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1828
TensorRT-LLM,这个issue是一个功能需求提问，涉及到TensorRT-LLM中的top_p和top_k采样机制的关系。其提问的核心是为什么在已设定top_k条件下会跳过top_p采样。,https://github.com/NVIDIA/TensorRT-LLM/issues/1820
TensorRT-LLM,该问题类型为用户提出需求，涉及主要对象为`GenerationSession`。由于用户需要在VAD标记音频片段为静音时，结束正在进行的解码，因此提出了如何在`GenerationSession`中取消解码的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1818
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中无法在Triton Server上部署多模型模型的问题。由于tensorrtllm_backend不支持多模型导致这一问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1813
TensorRT-LLM,这个issue类型是用户提出需求，涉及到`batch_manager::GenericLlmRequest`的logitsPostProcessor处理批量请求的问题，需要解决如何处理一批请求的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/1801
TensorRT-LLM,这是一个关于需求提出的issue，主要涉及的对象是TensorRT-LLM中的Medusa with Mixtral 8x7B。导致这个问题的原因是当前的Medusa convert_checkpoint.py不支持Mixtral，用户希望获得关于支持Mixtral的相关提示和帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/1798
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及如何自定义模型中的 position_ids，原因是模型中的一些 position_ids 相同，用户希望能够自行设置 position_ids。,https://github.com/NVIDIA/TensorRT-LLM/issues/1797
TensorRT-LLM,这是一个功能需求的问题，主要涉及到CogVLM模型在处理多个图片输入位置时的限制。由于CogVLM固定了输入图片的数量及位置，导致用户无法灵活添加多个图片输入位置。,https://github.com/NVIDIA/TensorRT-LLM/issues/1790
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及Quantizing Mixtral模型到fp8过程中的设备选择问题，用户寻求对通过CPU进行模型量化的结果以及选择适当方法的建议。,https://github.com/NVIDIA/TensorRT-LLM/issues/1777
TensorRT-LLM,这是用户提出的关于功能需求的issue，主要涉及TensorRT-LLM输出约束为JSON或调用函数工具，由于用户希望能够在使用TensorRTLLM与Modal时实现类似于OpenAI API方式的输出和调用，以提高效率。,https://github.com/NVIDIA/TensorRT-LLM/issues/1773
TensorRT-LLM,"这是一个关于更新TensorRT-LLM的issue，涉及的主要对象是Model Support, Features, API 和Bug fixes。由于引入了一系列破坏性更改和修复，可能会导致无法正确调用`convert_hf_mpt_legacy`函数以及其他API变更或修复问题。",https://github.com/NVIDIA/TensorRT-LLM/issues/1763
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及在TensorRT-LLM中支持自定义标定数据集，用户希望能够提供一个带有`text`列的`json`或`jsonl`文件用于标定。,https://github.com/NVIDIA/TensorRT-LLM/issues/1762
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是添加对DeepSeek MoE的支持。这个问题的原因是在TensorRT-LLM中添加了对DeepSeek MoE模型的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1758
TensorRT-LLM,这个issue是一个关于需求的问题，主要涉及的对象是TensorRT-LLM中针对BERT模型是否支持`--remove_input_padding`特性，用户询问是否有计划支持该功能，因为当前配置中没有找到相关参数设置。,https://github.com/NVIDIA/TensorRT-LLM/issues/1755
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及对象是在GCP上配置包含NVIDIA软件安装的虚拟机实例。由于缺少必要的NVIDIA安装，用户无法成功运行特定的docker命令。,https://github.com/NVIDIA/TensorRT-LLM/issues/1752
TensorRT-LLM,这是一个更新issue，涉及的主要对象是将gradio从4.19.2更新到4.36.0版本，原因是添加了监控仪表板和修复了一些问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1751
TensorRT-LLM,这是一个特性请求问题，主要涉及TensorRT-LLM中加载模型部分时只使用一个GPU的需求。因为用户想要测量模型的性能指标，但当前在TensorRT-LLM中必须使用所有GPU运行模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1743
TensorRT-LLM,这是一个更新网页页面的Issue，主要对象是项目的Github Pages。可能由于需要更新网页内容或修复已有页面的问题而提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/1737
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM下的ModelRunner类，用户请求如何使用`enable_block_reuse`参数。可能由于缺乏文档指导，导致用户难以使用该参数。,https://github.com/NVIDIA/TensorRT-LLM/issues/1736
TensorRT-LLM,这是一个发布新版本更新的issue，主要涉及的对象是TensorRT-LLM。通过添加新功能和改进，解决了一些先前版本中的问题，以提高性能和功能性。,https://github.com/NVIDIA/TensorRT-LLM/issues/1734
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及的对象是TensorRT-LLM中的Whisper。由于特定词汇未能正确定位，用户在使用initial prompt时遇到了问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1727
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM。用户提出了无法追踪每个请求的输入和输出标记计数以及无法添加新的响应变量的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1718
TensorRT-LLM,这是一个用户请求寻找Dockerfile以在TensorRT-LLM编译文件上运行推断的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1710
TensorRT-LLM,这个issue是关于功能需求的，主要涉及到TensorRT-LLM中的Grouped Diverse Beam Search功能。由于无法更改分组和大小以及设置beam_diversity_rate后输出没有明显变化，用户寻求如何实现更多样化的输出。,https://github.com/NVIDIA/TensorRT-LLM/issues/1707
TensorRT-LLM,这是一则用户需求报告，主要涉及对象是TensorRT-LLM中的convert_checkpoint.py工具。用户想要在本地文件夹中使用已下载的模型，并希望让convert_checkpoint.py工作于这个本地文件夹。,https://github.com/NVIDIA/TensorRT-LLM/issues/1698
TensorRT-LLM,这是一个关于如何使用`use_paged_context_fmha`功能的问题，用户询问为什么在使用该功能构建引擎并启用`enable_kv_cache_reuse`后，随着QPS的增加，延迟并没有显著增加。,https://github.com/NVIDIA/TensorRT-LLM/issues/1691
TensorRT-LLM,这是一个需求汇报issue，主要涉及的对象是在TensorRT-LLM下的/examples/multimodal中更新transformers版本至4.38.0。因为更新了transformers版本，可能出现了新特性或优化，需要验证新版本在项目中的适用性。,https://github.com/NVIDIA/TensorRT-LLM/issues/1689
TensorRT-LLM,这是一个更新内容的issue，主要涉及TensorRT-LLM的模型支持、特性更新和API变动。,https://github.com/NVIDIA/TensorRT-LLM/issues/1688
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及控制ngram重复问题的问题。由于使用自定义Python代码控制重复效率太低，用户希望知道如何使用何种接口控制ngram重复。,https://github.com/NVIDIA/TensorRT-LLM/issues/1684
TensorRT-LLM,该issue属于用户提出需求的类型，主要涉及TensorRT-LLM项目是否考虑采用min_p采样策略。根据用户提供的一些研究，min_p采样被认为优于其他采样策略，因此用户建议在项目中采用min_p采样方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/1683
TensorRT-LLM,这是一个特性请求，主要涉及到TensorRT-LLM中的logits processor，由于每个请求都会独立调用logits processor导致性能下降约10%。,https://github.com/NVIDIA/TensorRT-LLM/issues/1681
TensorRT-LLM,这个issue是一个“用户提出需求”的类型，主要涉及的对象是TensorRT-LLM中的C++ Executor API。由于C++ Executor API当前只支持函数而不支持类的方式来控制语言模型生成，因此用户提出希望添加LogitsProcessor类支持的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1680
TensorRT-LLM,这个issue是关于用户提出需求，涉及的主要对象是为TensorRT-LLM添加Huggingface model zoo。原因是用户希望讨论如何确定适当的配置以上传模型到Model Zoo。,https://github.com/NVIDIA/TensorRT-LLM/issues/1674
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及到TensorRT-LLM中的kv-reuse功能的使用情况。原因可能是文档缺失导致用户对功能的有效性和可用性产生疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/1671
TensorRT-LLM,这个issue是关于需求的，主要涉及到TensorRT-LLM的版本兼容性问题，用户希望得到支持TRT10.1版本的帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/1663
TensorRT-LLM,这个issue类型是用户提出需求，并涉及了关于自定义内核实现与跨注意力相关的问题，用户寻求创建用于跨注意力的自定义内核的指南或参考。,https://github.com/NVIDIA/TensorRT-LLM/issues/1658
TensorRT-LLM,这是一个用户提出需求的类型的issue，主要涉及的对象是TensorRT-LLM 下的 Cohere family of models。用户请求支持这一系列的模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1657
TensorRT-LLM,"这个issue是关于用户提出一个功能需求的类型，主要涉及的对象是TensorRT-LLM中的kv cache。原因是用户想通过增加""priority""来实现特定查询不留在缓存中的需求。",https://github.com/NVIDIA/TensorRT-LLM/issues/1654
TensorRT-LLM,这个issue是关于优化python基准测试日志记录的，主要包括对logging机制的更新和统一，以及新增的print_report_dict方法和优化后对空csv文件的检查，旨在避免重复写入重复的表头信息到同一个csv文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/1646
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是为TensorRT-LLM添加对CogVLM2模型的支持。这个需求出现的原因是CogVLM2被认为是一款优秀的开源多模态模型，社区希望为其提供TensorRT-LLM引擎以进一步提升模型应用的效率。,https://github.com/NVIDIA/TensorRT-LLM/issues/1644
TensorRT-LLM,该issue属于用户提出需求类型，主要涉及的对象是在TensorRT-LLM中进行fp8量化过程中如何切换量化形式（E4M3或E5M2）；用户提出了关于如何切换量化形式的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1631
TensorRT-LLM,这个issue是关于用户需求的，主要涉及到在流处理过程中增加每个chunk中发送的token数量。问题来源于用户希望了解如何实现增加chunk大小以及如何应用于tritoninferenceserver。,https://github.com/NVIDIA/TensorRT-LLM/issues/1623
TensorRT-LLM,这是一个关于用户需求和困惑的问题，主要涉及TensorRT中Prefix Caching的工作原理以及与普通kv缓存之间的区别。由于描述中存在一些模糊不清的逻辑，导致用户难以理解如何优化性能和进行prompt相关的调整。,https://github.com/NVIDIA/TensorRT-LLM/issues/1619
TensorRT-LLM,该问题属于用户提出需求，主要对象为Medusa int8权重量化。由于用户希望支持仅对权重进行int8量化，目前的实现可能无法满足该需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1615
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及对象是TensorRT-LLM中的默认参数`opt_num_tokens`，原因是用户认为默认设置为`max_batch_size * max_beam_width`不符合批处理中token数量的比例。,https://github.com/NVIDIA/TensorRT-LLM/issues/1613
TensorRT-LLM,这是一个关于功能疑问的问题，主要涉及TensorRT-LLM中的Executor API中Leader和Orchestrator两种模式的实现方式以及性能比较。,https://github.com/NVIDIA/TensorRT-LLM/issues/1608
TensorRT-LLM,这个issue类型是功能更新，涉及的主要对象是TensorRT-LLM。由于功能更新添加了一些新特性和改变原有API，可能需要用户注意新的API用法或迁移旧的代码。,https://github.com/NVIDIA/TensorRT-LLM/issues/1598
TensorRT-LLM,该问题属于用户提出需求类型，主要涉及TensorRT-LLM下的benchmark.py脚本，用户询问如何在不在allowed_configs.py中的情况下测试不同模型的性能。这可能是由于缺乏指导或配置文件的更新而引起的。,https://github.com/NVIDIA/TensorRT-LLM/issues/1597
TensorRT-LLM,这是一个需求类型的issue，主要涉及了在TensorRT-LLM中使用LoRA模型进行分类任务时遇到的问题。由于模型运行脚本`run.py`只支持解码而不支持分类，用户希望能够得到一个示例，展示如何使用LoRA和LLM进行分类任务。,https://github.com/NVIDIA/TensorRT-LLM/issues/1592
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM中的量化问题，用户希望建立一个“模型动物园”来存储量化模型，以解决硬件资源不足的情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/1591
TensorRT-LLM,这是一个用户提出需求类型的issue，主要涉及到nvidia-ammo在Windows上的安装问题。由于nvidia-ammo并没有针对Windows系统的版本，导致用户在quantize功能时遇到了安装nvidia-ammo的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1579
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下enc-dec模型支持inflight batching的问题，疑问是是否可以使用动态batching等方法来提高性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1573
TensorRT-LLM,这个issue是关于性能优化的问题，主要涉及TensorRT-LLM下H200和H100在Mistral-7B上表现的比较差异，提出了为什么H200相较H100显示仅有少量改进的疑问。原因可能是H200在内存受限情况下并未实现预期的显著改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/1570
TensorRT-LLM,这是一个功能支持请求，涉及主要对象为TensorRT-LLM下的gemm操作，由于需支持2bit计算，需要依赖cutlass仓库中两个pull requests的合并。,https://github.com/NVIDIA/TensorRT-LLM/issues/1568
TensorRT-LLM,这个issue属于用户提出需求，并希望添加InternVL-Chat-V1.5支持，意图是认为该模型功能强大且超越了许多专有的多模态模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1567
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM模型更新和支持列表的更新，涉及的对象是TensorRT-LLM工具。,https://github.com/NVIDIA/TensorRT-LLM/issues/1554
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是 TensorRT-LLM 中的模型量化过程。原因可能是在量化模型时出现了OOM错误，用户寻求帮助解决该问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1546
TensorRT-LLM,该问题是关于需求的提出，主要涉及的对象是TensorRT-LLM中的张量操作，用户出于性能优化考虑，希望在不进行数据拷贝的情况下获取子张量。,https://github.com/NVIDIA/TensorRT-LLM/issues/1540
TensorRT-LLM,这个issue是关于功能请求，主要涉及TensorRT-LLM中的SamplingConfig类，用户试图实现MinP采样层，因该类的构造函数不是开源的且存在与.a文件中导致无法修改而导致问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1536
TensorRT-LLM,这个issue类型是用户提出需求。主要对象是正在开发的Medusa Safetensors Lm heads。由于AWQ Conversion存在错误，用户希望能够加载safetensors Medusa Lm heads，需要进行AWQ Conversion的修正。,https://github.com/NVIDIA/TensorRT-LLM/issues/1535
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是高级API。由于用户想要了解何时高级API将支持QWEN模型，因此提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1525
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及改进构建Llama v3时的README内容，原因是需要增加词汇量大小。,https://github.com/NVIDIA/TensorRT-LLM/issues/1522
TensorRT-LLM,这是一个用户提出需求的类型。主要对象是TensorRT-LLM下的BERT模型。用户想要了解是否可以使BERT支持可变长度的输入序列。,https://github.com/NVIDIA/TensorRT-LLM/issues/1518
TensorRT-LLM,这个issue属于用户提出需求类型，围绕着BERT-like模型是否可以应用类似GPT-like模型的特性展开讨论。由于用户对于BERT模型可以借鉴GPT模型的哪些特性存在疑问导致提出这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1517
TensorRT-LLM,这是一个需求提出的issue，主要涉及SDXL框架，用户提出希望支持SDXL和其分布式推理，原因是为了实现更好的性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1514
TensorRT-LLM,这是一个用户需求问题，主要涉及TensorRT-LLM中如何在C++后端控制生成过程中分页kv缓存中的最大令牌数。用户想要指定`max_tokens_in_paged_kv_cache`属性，但不清楚如何实现。,https://github.com/NVIDIA/TensorRT-LLM/issues/1506
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是CogVLM模型支持计划。用户询问是否有对该模型的支持计划。,https://github.com/NVIDIA/TensorRT-LLM/issues/1497
TensorRT-LLM,"这是一个用户提出需求的问题，主要涉及在使用inflight batching时如何将hidden_states直接传递给LLM。由于GenerationRequest类中缺少""prompt_table""属性，导致用户无法直接传递图像特征的hidden_states给LLM。",https://github.com/NVIDIA/TensorRT-LLM/issues/1488
TensorRT-LLM,这是一个用户需求相关的问题，主要涉及对象为TensorRT-LLM中的functional.Tensor，由于版本升级导致获取的数据类型变化，需要转换Tensor为List或numpy以满足原有接口。,https://github.com/NVIDIA/TensorRT-LLM/issues/1484
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及的对象是对TensorRT-LLM的Yi-VL模型支持。这个需求是因为Yi Vision Language (YiVL)模型的优秀性能以及在最新基准测试中排名第一，用户希望能够在TensorRT-LLM中集成Yi-VL模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1473
TensorRT-LLM,这个issue属于用户提出需求类，主要涉及TensorRT-LLM中的auto_parallel功能，用户询问该功能的工作原理以及适用范围。,https://github.com/NVIDIA/TensorRT-LLM/issues/1471
TensorRT-LLM,这是一个特性请求，主要对象是支持 llama v3，用户请求nvidia公司提供帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/1470
TensorRT-LLM,该issue为文档更新请求，涉及的主要对象是GitHub Pages，由于页面内容需要更新或修复，用户提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1464
TensorRT-LLM,这是一个文档更新类型的issue，涉及的主要对象是发布0.9版本的文档。由于发布版本的内容需要更新，用户提出了更新文档的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1461
TensorRT-LLM,该issue类型属于用户提出需求类型，主要涉及TensorRT-LLM下的embedding_bias特性。用户询问如何使用embedding_bias以及它的作用，特别是在解码过程中添加偏置的用例。,https://github.com/NVIDIA/TensorRT-LLM/issues/1457
TensorRT-LLM,这是一个关于TensorRT-LLM更新的issue，主要涉及Bug修复，其中涉及到Pipeline Parallelism和`gather_all_token_logits`功能引起的Segmentation fault问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1455
TensorRT-LLM,这是一个用户提出需求的issue，用户询问何时支持Qwen1.5版本。,https://github.com/NVIDIA/TensorRT-LLM/issues/1454
TensorRT-LLM,这个issue属于需求提出类型，主要涉及TensorRT-LLM的更新和新增功能，其中包括模型支持、功能新增、特性改进等内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/1445
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及如何控制最大并发性。用户希望了解如何限制在构建最大批处理大小为24的引擎时，最多同时运行16个请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1438
TensorRT-LLM,这个issue是关于功能需求的，主要对象是TensorRT-LLM下的model_runner_cpp.py文件，用户建议将gather generation logits和gather context logits进行分开设置。,https://github.com/NVIDIA/TensorRT-LLM/issues/1435
TensorRT-LLM,这个issue类型是用户提出需求，该问题单涉及的主要对象是添加LoRa支持到Qwen或Qwen2模块。由于目前缺乏LoRa支持，用户提出了添加LoRa支持的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1432
TensorRT-LLM,这是关于用户提出需求的问题，主要涉及TensorRT-LLM中的multimoda模块支持GroundingDINO的相关内容。由于用户想了解TensorRT-LLM multimoda是否支持GroundingDINO以及如何支持，因此提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1430
TensorRT-LLM,这是一个功能更新的issue，主要涉及TensorRT-LLM模型的更新和改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/1427
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及到如何在版本0.8.0中使用System Prompt Caching来加速处理超过3000+ tokens的系统提示时的输出延迟问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1426
TensorRT-LLM,这是一个关于用户提出需求的问题，在GptManager中修改topK/topP值的可行性。这个问题主要涉及到在生成过程中动态更改topK/topP值的操作是否安全，用户想要根据特定条件调整这些数值，且疑问修改后会否被正确应用。,https://github.com/NVIDIA/TensorRT-LLM/issues/1425
TensorRT-LLM,该issue类型为用户提出需求，主要涉及TensorRT-LLM下benchmark相关的问题，用户想要了解如何进行基准测试以及如何使用lora进行性能基准测试。,https://github.com/NVIDIA/TensorRT-LLM/issues/1421
TensorRT-LLM,这是一个功能请求，请求提供在TensorRT-LLM中收集稀疏logprobs的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1419
TensorRT-LLM,这是一个关于需求和帮助的问题，涉及TensorRT-LLM引擎的构建参数选择。用户对构建llama引擎的最佳配置和选项含义不清楚，导致不确定如何选择最佳参数配置。,https://github.com/NVIDIA/TensorRT-LLM/issues/1417
TensorRT-LLM,这是一个功能需求的issue，主要涉及TensorRT-LLM中高级API中的SamplingConfig。用户希望通过高级API能够访问完整的SamplingParams集合，以便更容易迁移自vLLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/1405
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中如何同时整合自定义的Mistral LLM和ViT图像编码器模型，并希望得到如何结合和运行这两个组件的指导。,https://github.com/NVIDIA/TensorRT-LLM/issues/1401
TensorRT-LLM,这是一个关于需求问题的issue，主要涉及对象是GptManager InferenceRequest中的Tensor数据类型，用户提出关于这些张量需要的内存类型的问题，可能是由于缺乏关于内存需求的信息而导致的。,https://github.com/NVIDIA/TensorRT-LLM/issues/1399
TensorRT-LLM,这是一个功能需求请求，主要涉及TensorRT-LLM下的internlm2模型的转换支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1392
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的batch_manager和executor，用户寻求是否有开源计划。,https://github.com/NVIDIA/TensorRT-LLM/issues/1391
TensorRT-LLM,这是一个用户提出需求的类型问题。该问题主要涉及TensorRT-LLM对异步allreduce的支持。用户想要实现异步操作并提升速度，可能需要涉及到使用cudagraphs或jit编译。,https://github.com/NVIDIA/TensorRT-LLM/issues/1389
TensorRT-LLM,这个issue类型是需求更新，涉及的主要对象是TensorRT-LLM。由于性能问题，用户提出了减少TensorRT引擎之间`enqueue`调用之间的开销以及一些小的更新和修复。,https://github.com/NVIDIA/TensorRT-LLM/issues/1387
TensorRT-LLM,这个issue是用户提出需求，针对如何向TensorRT-LLM添加自定义简单网络并生成引擎进行推断问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1376
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及的对象是TensorRT-LLM，用户询问是否支持特定的模型，并询问是否有计划支持这个模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1372
TensorRT-LLM,"这个issue属于用户提出需求类型，主要关注对象是generation.py脚本。由于缺乏时间戳打印功能，用户希望在generation.py中添加该功能。

",https://github.com/NVIDIA/TensorRT-LLM/issues/1371
TensorRT-LLM,该issue属于用户提出需求类型，主要涉及TensorRT-LLM中支持HF的`early_stopping`启发式方法的问题，用户怀疑不同的early stopping行为可能导致HF和TRTLLM输出结果的巨大差异。,https://github.com/NVIDIA/TensorRT-LLM/issues/1370
TensorRT-LLM,这是一个关于需求讨论的issue，主要涉及AttentionMaskType中两个选项的区别，用户想了解这两者之间的差异。,https://github.com/NVIDIA/TensorRT-LLM/issues/1369
TensorRT-LLM,这是一个用户需求类型的issue，主要涉及NVIDIA AMMO工具箱的文档相关问题，用户希望了解文档中关于实现功能、支持的模型量化技术以及版本之间的更新内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/1368
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及支持`DBRX`模型的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1363
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及TensorRT-LLM下对Cohere Command-R模型的支持。由于Cohere发布了CommandR模型并取得良好的评估结果，用户希望TensorRT-LLM能够支持该模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1360
TensorRT-LLM,这是用户提出需求，寻找某个功能源码的问题。该问题涉及的主要对象是`KVCacheManager`。用户无法找到 `KVCacheManager` 的cpp实现代码，只找到了头文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/1357
TensorRT-LLM,这是一个用户提出需求的类型的issue，主要涉及到TensorRT-LLM中的量化功能。用户提出需求是希望增加一个选项用于指定量化校准数据集的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1351
TensorRT-LLM,这是一个关于功能需求的问题，主要涉及的对象是TensorRT-LLM中对BERT模型是否支持int8量化。用户在询问是否支持int8量化（权重量化或平滑量化），表达了对该功能的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1350
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是对TensorRT-LLM的Lookahead Decoding支持。由于用户希望添加这一特性，并提供了相关文档链接。,https://github.com/NVIDIA/TensorRT-LLM/issues/1349
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及对象是TensorRT-LLM中支持Qwen2模型的问题。由于当前版本可能尚未支持Qwen2模型，用户请求添加该模型的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1347
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM项目。由于严格固定Python依赖版本导致集成TensorRTLLM变得困难。,https://github.com/NVIDIA/TensorRT-LLM/issues/1346
TensorRT-LLM,这是一个功能需求类型的issue，主要涉及的对象是对新发布的distilwhisper/distillargev3的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1337
TensorRT-LLM,该issue类型为用户提出需求，并涉及到使用SmoothQuant和int8 kv cache。这个问题涉及到如何同时使用这两个功能以及在kv缓存存储精度方面的疑惑。,https://github.com/NVIDIA/TensorRT-LLM/issues/1330
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及自动下载模型转换至TensorRT-LLM模型并保存的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1327
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及TensorRT-LLM中使用FP8权重和FP16激活量化的方法，用户希望权重可以使用FP8格式而保留激活值为FP16格式，但目前只能将权重和激活值一同量化至FP8。,https://github.com/NVIDIA/TensorRT-LLM/issues/1319
TensorRT-LLM,该issue类型为用户提出需求，主要涉及W8A8模型在TensorRT-LLM下是否支持混合精度推理，并由于精度和差异率问题导致单独使用FP16和INT8量化存在困难。,https://github.com/NVIDIA/TensorRT-LLM/issues/1317
TensorRT-LLM,该issue类型为功能更新，主要涉及TensorRT-LLM的更新内容及相关API。由于代码结构重构和参数变更，可能导致了BREAKING CHANGES，并且用户可能面临与之相关的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1315
TensorRT-LLM,这个issue是一个关于功能请求的问题，主要涉及TensorRT-LLM中支持INT4权重和FP8激活的需求。由于当前无法实现该功能，用户提出了这一需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1312
TensorRT-LLM,这是一个用户需求问题，涉及TensorRT-LLM模型中如何在模型生成过程中为每个token添加用户指定的embedding。因用户希望在生成模型时控制token的embedding。,https://github.com/NVIDIA/TensorRT-LLM/issues/1310
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM的性能优化和吞吐量问题。由于当前的基准测试方法有限，并不符合实际情况，导致无法展示出模型优势在吞吐量场景下的表现。,https://github.com/NVIDIA/TensorRT-LLM/issues/1292
TensorRT-LLM,这个issue是关于用户提出需求的问题，主要涉及TensorRT-LLM下的MPT模型，由于缺乏FP8支持，用户寻求关于该特性的帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/1291
TensorRT-LLM,这个issue类型为文档更新（Chore），主要对象是项目的README文件，由于链接错误导致需要修复。,https://github.com/NVIDIA/TensorRT-LLM/issues/1278
TensorRT-LLM,"这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持指定模型""deepseekai/deepseekmoe16bbase""，并询问是否有计划支持。",https://github.com/NVIDIA/TensorRT-LLM/issues/1277
TensorRT-LLM,该问题类型为用户需求，主要涉及对象是TensorRT-LLM中的LORA功能。用户提问是否LORA支持除LLAMA之外的其他LLM，并是否需要自己进行适配。,https://github.com/NVIDIA/TensorRT-LLM/issues/1276
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是更新链接到Nitro下载和LlamaCorn模型引擎，可能是因为链接已过时或指向错误导致用户希望更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/1275
TensorRT-LLM,这个issue属于更新issue，主要涉及TensorRT-LLM的功能更新和改进，包括模型支持、特性添加、API修改、Bug修复、性能优化和基础设施更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/1274
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到如何在0.8.0版本的LLaMA模型推理引擎中开启in-flight batching功能。这个问题可能是由于用户想要优化推理性能而提出的。,https://github.com/NVIDIA/TensorRT-LLM/issues/1269
TensorRT-LLM,这是一个关于用户需求的问题，主要涉及到自定义加载模型参数逻辑。用户希望通过定制加载逻辑来加载每个张量而非使用默认的加载机制，因为他们正在使用tensorrtllm库。,https://github.com/NVIDIA/TensorRT-LLM/issues/1266
TensorRT-LLM,这个issue类型为用户提出需求，主要对象是TensorRT-LLM，用户询问是否有计划支持Yi。可能是由于Yi目前不被支持，用户希望了解是否会在未来添加对Yi的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1265
TensorRT-LLM,这是一个文档改进类的issue，主要涉及TensorRT-LLM下的inflight batching支持文档。由于文档可能缺乏相关信息或者未清晰表达相关概念，用户可能需要更多关于inflight batching的指导或者说明。,https://github.com/NVIDIA/TensorRT-LLM/issues/1263
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是是否支持Qwen Lora，可能是因为用户想要了解TensorRT-LLM是否计划支持Qwen Lora设备。,https://github.com/NVIDIA/TensorRT-LLM/issues/1262
TensorRT-LLM,这是一个需求类型的issue，主要涉及的对象是TensorRT-LLM下的`nitro-tensorrt-llm`项目。由于缺乏README文件中一些关键信息，用户需要说明该项目的目标、与`nitro`的区别，以及如何快速开始、如何编译/导出以及如何使用可执行/编译二进制文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/1257
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的SmoothQuant支持MoE模型（如Mixtral），用户询问其是否在开发路线图中并希望得到明确答复。,https://github.com/NVIDIA/TensorRT-LLM/issues/1241
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下对于Jais模型的支持。由于Jais模型是一种基于transformer-based解码器的模型并使用了SwiGLU非线性，要求将其添加到TensorRT-LLM中以提供更好的上下文处理和模型精度。,https://github.com/NVIDIA/TensorRT-LLM/issues/1237
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM下的Mixtral（MoE）模型支持FP8量化的时间线问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1229
TensorRT-LLM,这是一个用户提出需求的类型的issue，主要涉及对象是TensorRT-LLM，用户询问是否TensorRT-LLM将会支持VIT模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1226
TensorRT-LLM,这是一个功能需求类型的issue，主要涉及TensorRT-LLM集成QUICK内核以实现AWQ量化。,https://github.com/NVIDIA/TensorRT-LLM/issues/1225
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及的对象是多个LoRa设备的支持。由于功能需求，用户询问是否支持SLoRA机制，并寻求关于示例或未来计划的信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/1224
TensorRT-LLM,这是一个用户提出需求的问题，主要对象是如何在两个GPU上初始化两个不同的模型，由于提供的命令导致两个模型都在cuda:0上初始化。,https://github.com/NVIDIA/TensorRT-LLM/issues/1223
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及如何估算TensorRT-LLM模型在特定硬件、模型和量化设置下的最大批次大小，以及推荐的最大token数量。 导致这个问题的原因是用户需要更多关于如何根据硬件和设置来设置合理的最大批次大小相关信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/1222
TensorRT-LLM,这是一个用户提需求的问题，主要需求是如何将Medusa解码添加到服务器中，并提出了在服务器示例中需要的选项不足的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1219
TensorRT-LLM,该issue属于用户提出需求类型，主要涉及如何在TensorRT-LLM中添加新模块，如alltoall或自定义模块。这可能是因为用户想要扩展TensorRT-LLM的功能，但不清楚如何实现。,https://github.com/NVIDIA/TensorRT-LLM/issues/1218
TensorRT-LLM,这是用户提出的需求。该问题单涉及的主要对象是TensorRT-LLM，用户希望支持文本嵌入模型以及来自句子转换器的重新排序模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1213
TensorRT-LLM,这是一个用户请求帮助的issue，主要涉及如何为finetuned bert模型构建TensorRT引擎，用户寻求关于序列分类的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1210
TensorRT-LLM,这个issue属于用户提出的需求类型，主要涉及TensorRT-LLM中的Chunked context支持FP8的问题，由于FP8与paged_context_fmha不兼容导致无法使用该功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1204
TensorRT-LLM,这是一个需求类型的 issue，主要涉及到为 TensorRT-LLM 添加 0.8 batch manager 的静态库。,https://github.com/NVIDIA/TensorRT-LLM/issues/1202
TensorRT-LLM,这个issue是关于更新gh-pages的，属于一般性维护问题，主要对象为项目的网页分支。,https://github.com/NVIDIA/TensorRT-LLM/issues/1196
TensorRT-LLM,这个issue类型是请求更新发布分支，主要涉及TensorRT-LLM。由于发布分支需要更新，用户希望进行相关操作。,https://github.com/NVIDIA/TensorRT-LLM/issues/1192
TensorRT-LLM,这个issue是关于用户提出需求的问题，主要涉及的对象是TensorRT-LLM，用户询问是否支持w4a8 (int4 * int8)，因为一些GPU不支持fp8。,https://github.com/NVIDIA/TensorRT-LLM/issues/1189
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及到TensorRT-LLM中的多模态示例。由于自定义分词器为embedding分词生成了不同的假prompt id，用户想知道这是否符合设计，以及如何通过调整参数来实现所需的效果。,https://github.com/NVIDIA/TensorRT-LLM/issues/1185
TensorRT-LLM,这是一个用户提出需求的类型。该问题涉及TensorRT-LLM中的C++接口使用，并请求提供类似llama.cpp的示例代码。这可能是由于用户想要了解如何使用C++接口来初始化和运行LLM模型，而不需要关注分词器的原因。,https://github.com/NVIDIA/TensorRT-LLM/issues/1179
TensorRT-LLM,这是一个关于技术问题的请求帮助类型的issue，主要涉及将大型世界模型转换为TensorRT LLM的问题，由于权重是使用JAX进行处理，用户不熟悉JAX和TensorRT LLM，导致他们不清楚如何继续进行模型转换。,https://github.com/NVIDIA/TensorRT-LLM/issues/1174
TensorRT-LLM,"这是一个可能与命令行工具 trtllm-build 不支持 ""qwen model"" 的功能或模型类型相关的功能需求问题。",https://github.com/NVIDIA/TensorRT-LLM/issues/1167
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及到如何处理批量预测中变长的decoder_input_ids，由于模型Nougat家族中的模型不支持此功能，用户寻求如何在donutbasefinetuneddocvqa模型中应用。,https://github.com/NVIDIA/TensorRT-LLM/issues/1166
TensorRT-LLM,这是一个功能请求，用户想要在nvidia orin平台上支持TensorRT-LLM，并尝试在该平台上编译TensorRT-LLM时遇到了编译失败的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1160
TensorRT-LLM,这个issue是关于用户需求的，涉及的主要对象是TensorRT-LLM中的`system prompt caching`功能。用户想了解如何使用这个功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1159
TensorRT-LLM,该issue是一个用户提出的需求，主要对象是在TensorRT-LLM中添加Min-P采样层。由于Min-P采样在本地LLM中变得更受欢迎，并提供更有用的结果或感觉如此，因此用户希望这个库能够支持Min-P采样方式。,https://github.com/NVIDIA/TensorRT-LLM/issues/1154
TensorRT-LLM,这是用户提出的一个功能需求，主要对象是对输出字典中的Option fields进行追加。原因是用户希望能够获得每个输入序列位置的TokenIDs的Topk对数概率。,https://github.com/NVIDIA/TensorRT-LLM/issues/1149
TensorRT-LLM,这是一个功能需求的issue，主要涉及的对象是TensorRT-LLM下的Gemmi模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/1147
TensorRT-LLM,这是一个用户提出需求类型的issue，主要涉及TensorRT-LLM中的RelayAttention。,https://github.com/NVIDIA/TensorRT-LLM/issues/1140
TensorRT-LLM,这是一个关于开源代码缺失问题的类型为需求问题的issue，主要涉及的对象是TensorRT-LLM中的batch_manager和executor模块。由于缺失源代码，用户提出了对应开源库中编译二进制文件的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/1135
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM模型中不同概率之间的相关性问题，用户希望找到详细的解释。,https://github.com/NVIDIA/TensorRT-LLM/issues/1132
TensorRT-LLM,这个issue是一则需求更新，涉及到gemma项目的README文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/1128
TensorRT-LLM,这是一个用户需求提出的问题，主要涉及了如何从生成会话中返回logprobs。用户希望能够获取whisper解码器的logprobs，但尝试过程中未能成功，导致所需的logprobs无法获取。,https://github.com/NVIDIA/TensorRT-LLM/issues/1127
TensorRT-LLM,这是一个需求更新类型的issue，涉及的主要对象是README.md文件。用户提出需要将ammo分支版本从0.7.0更新至0.7.3，可能是为了更新项目依赖或修复之前版本存在的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1126
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下moe gemm对w8a8的支持问题，用户询问是否有计划支持w8a8的moe。,https://github.com/NVIDIA/TensorRT-LLM/issues/1124
TensorRT-LLM,这是一个用户提出需求的issue， 主要对象是项目中的某个特定修改，由于没有具体描述内容，无法分析导致问题的原因。,https://github.com/NVIDIA/TensorRT-LLM/issues/1115
TensorRT-LLM,这是一个关于需求探讨的问题，主要涉及的对象是AMMO，问题想要了解是否AMMO支持QAT训练流程。,https://github.com/NVIDIA/TensorRT-LLM/issues/1114
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM的支持Constrained Decoding的功能请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1111
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM中的pipeline parallelism模式。由于需求是指定每个pp层的gpu id，用户可能遇到了无法指定gpu id的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/1108
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM，用户在请求关于int4权重的运行支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1107
TensorRT-LLM,这个issue类型是用户提出需求， 主要对象是关于AWQ配置参数的调整。用户询问如何通过AMMO来配置AWQ的权重分数比。,https://github.com/NVIDIA/TensorRT-LLM/issues/1106
TensorRT-LLM,这个issue是用户提出需求，请教问题类型，主要对象是创建对话的template。,https://github.com/NVIDIA/TensorRT-LLM/issues/1105
TensorRT-LLM,这是一个关于功能询问的问题，主要涉及Quantization methods的支持情况。由于文档信息不一致，用户正在请求了解当前multi-modal (decoder only) pipeline是否支持smoothquant和awq方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/1101
TensorRT-LLM,这是一个功能需求问题，用户提出了需要更新TensorRT-LLM的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1098
TensorRT-LLM,这是一个性能优化的issue，主要涉及TensorRT-LLM中的moe router，通过移除TP实现加速，导致在Mixtral8x7Bv0.1中出现418%的解码速度提升。,https://github.com/NVIDIA/TensorRT-LLM/issues/1091
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到为TensorRT-LLM添加对CogVLM模型的支持。由于用户认为CogVLM是描述图像的最佳模型之一，希望能够在4位上运行该模型以加快图像的字幕生成速度。,https://github.com/NVIDIA/TensorRT-LLM/issues/1087
TensorRT-LLM,这是一个功能请求，主要涉及TensorRT-LLM Python package中的类定义问题。用户希望可以通过pip安装后直接导入类，以提高TensorRTLLM的易用性。,https://github.com/NVIDIA/TensorRT-LLM/issues/1082
TensorRT-LLM,这是一个用户提出需求的issue，该问题单涉及的主要对象是更新MPT requirements脚本。由于一些MPTfamily模型需要einops库，用户希望将此库添加到requirements.txt中。,https://github.com/NVIDIA/TensorRT-LLM/issues/1075
TensorRT-LLM,这是一个功能增强的issue，主要涉及将huggingface的distil-whisper模型权重转换为兼容的pytorch `.pt`文件，以便构建TensorRTLLM引擎。,https://github.com/NVIDIA/TensorRT-LLM/issues/1061
TensorRT-LLM,该issue属于用户提出需求类型，主要对象为TensorRT-LLM。导致这个问题的原因是用户对于TensorRT在TensorRT-LLM中的作用和参与方式产生了疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/1058
TensorRT-LLM,该issue属于功能更新类型，主要涉及TensorRT-LLM模型支持和功能增强。由于新版本增加了更多头部尺寸支持和修复了一些小bug，以及增加了文档内容和博客，用户可以更方便地使用新的功能和优化技术来提升推断速度。,https://github.com/NVIDIA/TensorRT-LLM/issues/1055
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及对象是TensorRT LLM下的bark模型。由于用户需要支持bark模型，因此提出了这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1053
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到TensorRTLLM下的ModelRunner类，用户希望能够在ModelRunner类中提供自定义特征遮罩的功能，但目前未看到相关功能。用户认为缺少这一功能会影响他们使用唯一特征遮罩进行操作。,https://github.com/NVIDIA/TensorRT-LLM/issues/1052
TensorRT-LLM,这个issue类型为用户提出需求，针对的主要对象是TensorRT-LLM GitHub仓库。由于缺乏关于如何克隆指定分支的说明，用户提出了添加更具体的命令，以便克隆特定的`rel`分支而非默认的`main`分支。,https://github.com/NVIDIA/TensorRT-LLM/issues/1050
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的Smoothquant功能。用户询问是否有计划在未来的版本中支持volta GPU上的int8 smoothquant，表明用户希望在将来能够在volta GPU上使用这个功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1044
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的GenerationSession，用户希望能够收集额外的模型输出来满足特定需求，如tokenwise分类和特定头部的注意力分数。,https://github.com/NVIDIA/TensorRT-LLM/issues/1040
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的modified QWenAttention。由于修改后的QWenAttention与原版在数值精度上存在不一致，用户希望了解如何让TensorRT-LLM支持这一修改。,https://github.com/NVIDIA/TensorRT-LLM/issues/1037
TensorRT-LLM,这是一个关于用户需求的问题，用户询问是否有计划添加对xverse模型的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/1036
TensorRT-LLM,这是一个需求报告，涉及主要对象是自动化cuDNN设置。可能由于用户需要简化TensorRT-LLM的cuDNN设置流程导致提出这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1032
TensorRT-LLM,这个issue类型是用户提出需求，请教问题，主要涉及的对象是使用TensorRT-LLM进行简单模型推理的用户。原因是用户想要使用简单输入和输出进行模型前向推理，但目前的ModelRunner实现中缺乏清晰的model.forward()函数。,https://github.com/NVIDIA/TensorRT-LLM/issues/1031
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是QWen是否支持MLM（Masked Language Model）功能。 ,https://github.com/NVIDIA/TensorRT-LLM/issues/1030
TensorRT-LLM,这是一个关于需求的问题，用户希望在TensorRT-LLM中支持pipeline parallelism和tensor parallelism的组合。,https://github.com/NVIDIA/TensorRT-LLM/issues/1027
TensorRT-LLM,这是一个关于兼容性和性能问题的用户需求报告，主要涉及到T4 GPU在运行LLava时遇到内存不足错误。,https://github.com/NVIDIA/TensorRT-LLM/issues/1024
TensorRT-LLM,这是一个关于功能差异的问题，主要涉及TensorRT-LLM的In-flight Batching功能缺失和具体版本的支持情况，用户怀疑在不同版本中的功能差异是否正常工作。,https://github.com/NVIDIA/TensorRT-LLM/issues/1023
TensorRT-LLM,这个issue是关于更新TensorRT-LLM，并涉及bug修复、功能更新以及API迁移，主要涉及的对象是TensorRT-LLM这个项目。,https://github.com/NVIDIA/TensorRT-LLM/issues/1019
TensorRT-LLM,这个issue是关于用户请求指南或优化提示，涉及优化KV缓存使用与inflight batcher，由于缺乏关于处理inflight批次设置的文档，导致KV缓存块数量停止增加，用户寻求模型服务器优化建议。,https://github.com/NVIDIA/TensorRT-LLM/issues/1012
TensorRT-LLM,这是一个文档更新的issue，主要对象是TensorRT-LLM，由于没有具体的内容说明，无法分析具体原因或者提出的问题或需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/1009
TensorRT-LLM,这个issue类型是功能请求，主要涉及的对象是Mixtral支持。由于TensorRT-LLM目前不支持AWQ with AMMO，导致用户无法使用该功能，用户希望能够跟踪该功能的支持情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/1007
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及如何获取模型的hidden_state值。由于TensorRT-LLM在构建后是静态的，用户想要获取模型的中间结果，但启用debug输出会导致推断变得非常缓慢。,https://github.com/NVIDIA/TensorRT-LLM/issues/1005
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM的Windows设置脚本，用户指出运行脚本需要先导航到克隆的存储库。因此，用户在寻求如何运行Windows设置脚本的帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/1001
TensorRT-LLM,这是一个功能需求，主要涉及添加T5模型的权重量化功能。导致问题的原因是现有的HF权重加载机制可能不适用，需要修复以使权重量化功能能够在生产中发挥作用。,https://github.com/NVIDIA/TensorRT-LLM/issues/985
TensorRT-LLM,这个issue是关于用户提出需求的类型，主要涉及TensorRT-LLM下的两个文件中关于优化配置的问题。由于用户对于如何通过设置`mRuntime.addContext`中的值来优化性能提出了疑问，可能是因为用户想要了解在上下文和生成阶段中FLOPs百分比受到什么控制。,https://github.com/NVIDIA/TensorRT-LLM/issues/980
TensorRT-LLM,这是一个关于TensorRT-LLM中max_batch_size和max_input_len参数关系的问题。用户提出了对于请求长度大于设定值直接返回错误的情况下，需求处理超出长度请求而不截断的方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/977
TensorRT-LLM,该issue属于功能增强类型，主要针对TensorRT-LLM中引擎缓存的效率进行优化，主要涉及到引擎编译缓存机制的改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/976
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及TensorRT-LLM下的优化问题，用户希望在Context阶段缓存预先确定的提示词组K和V，以提高产品关联性检测效率。,https://github.com/NVIDIA/TensorRT-LLM/issues/975
TensorRT-LLM,该issue是一个需求提出类型的问题，主要涉及到如何在TensorRT-LLM中获取隐藏状态（hidden_states）。由于作者只简单提到了标题内容，故无法确定具体的问题原因或帮助需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/971
TensorRT-LLM,这个issue属于用户提出需求，询问TensorRT-LLM是否支持facebook/nlgb-200-3.3B，问题涉及主要对象是TensorRT-LLM。由于缺乏详细信息，用户提出了关于TensorRT-LLM是否支持特定模型的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/970
TensorRT-LLM,这是一个功能需求类型的issue，主要涉及对象是TensorRT-LLM的模块（Module），用户提出需要添加打印模块的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/968
TensorRT-LLM,这个issue类型是用户提出需求，主要对象是TensorRT-LLM的smoothquant功能。由于`int8_sq_per_tensor`模式表现异常，用户在测试中发现了准确性比较问题，并询问llama系列模型是否支持在TensorRT-LLM中的pertensor smooth quant。,https://github.com/NVIDIA/TensorRT-LLM/issues/945
TensorRT-LLM,该issue属于功能更新，涉及TensorRT-LLM模型支持、特性、API、bug修复、性能和文档更新，原因是需要增加新的功能和改进现有功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/941
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT对JAIS模型的转换，用户询问如何处理SwiGlu权重的相应偏置。,https://github.com/NVIDIA/TensorRT-LLM/issues/939
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是是否计划支持Yuan2.0模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/937
TensorRT-LLM,这个issue类型是用户提出需求，请教问题，主要涉及对象是TensorRT和TensorRT-LLM，由于用户想了解原始TensorRT API和TensorRT-LLM之间的区别及其性能表现。,https://github.com/NVIDIA/TensorRT-LLM/issues/930
TensorRT-LLM,这是一个用户需求类型的issue，主要对象是GptSession，用户关注是否多个线程能够共享GptSession并同时调用GptSession::generate()。可能由于服务需要并发处理请求，用户在询问多线程下的使用情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/926
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下的pypi包，用户希望预构建的包可以支持所有GPU架构。这个问题的原因可能是预构建包不包括T4 GPU的架构导致无法使用。,https://github.com/NVIDIA/TensorRT-LLM/issues/920
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到TensorRT-LLM的内存需求信息缺失。,https://github.com/NVIDIA/TensorRT-LLM/issues/915
TensorRT-LLM,该issue属于用户提出需求类型，主要涉及TensorRT-LLM的请求推理API以及是否支持在单个上下文中获取多个结果，用户询问如何实现获取多个结果的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/911
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是Nvidia GeForce RTX 4070 GPU以及TensorRT-LLM中的Llama 7b模型。由于GPU VRAM（显存）仅有8GB，导致加载Llama 7b模型时出现错误，用户在寻求建议使用哪个模型来解决问题，或者是否有其他方式可以在RTX 4070 GPU上加载Llama 7b模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/910
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及到将 qwen model 支持 use_paged_context_fmha 特性，并使 enable_kv_cache 在 triton server 中可用。,https://github.com/NVIDIA/TensorRT-LLM/issues/909
TensorRT-LLM,这是一个关于需求的问题，主要对象是TensorRT-LLM中的inflight_batching功能。询问直接从TensorRTLLM中是否可以进行inflight_batching，而不是使用triton，并提及了相关的测试脚本test_gpt_manager.py。,https://github.com/NVIDIA/TensorRT-LLM/issues/907
TensorRT-LLM,这是一个功能需求提问，用户关注的主要对象是TensorRT-LLM下的gptSessionBenchmark benchmark工具。由于缺少关于prompt text作为输入的支持说明，用户想知道gptSessionBenchmark benchmark工具是否支持这样的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/906
TensorRT-LLM,这是一个用户提出需求的问题单，主要涉及TensorRT-LLM模型加载的不同精度。用户想要了解在相同精度下是否可以加载不同精度的模型或者是否需要为不同精度编译不同的引擎文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/903
TensorRT-LLM,这是一个更新TensorRT-LLM的issue，主要涉及到特征更新、性能优化和文档更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/891
TensorRT-LLM,这个issue是用户提出需求，主要对象是在TensorRT-LLM中支持文本和图像嵌入模型，用户询问是否有计划未来支持类似CLIP或sentencetransformers等模型，并询问如何能够实现对这些模型的支持，以加速嵌入模型应用在类似ChromaDB这样的场景中。,https://github.com/NVIDIA/TensorRT-LLM/issues/880
TensorRT-LLM,这是一个关于功能疑问的issue，主要对象是TensorRT-LLM中`generation_logits`的输出行为，问题出现的原因可能是在使用`top_p`参数时，`generation_logits`输出的值是否应该存储概率值而不是对数概率值。,https://github.com/NVIDIA/TensorRT-LLM/issues/879
TensorRT-LLM,这个issue是关于用户提出需求的问题，主要涉及的对象是`batch_manager`源代码。由于源代码未开源，用户提出了请求将`batch_manager`源代码推送到该repo的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/878
TensorRT-LLM,这是一个请求指南或演示的issue，涉及到TensorRT-LLM在Mistral 7b上的并行性和部署。,https://github.com/NVIDIA/TensorRT-LLM/issues/876
TensorRT-LLM,这是一个用户提出需求的issue，涉及主要对象是如何在llama模型中使用嵌入作为输入。由于用户想要根据trtllm自定义输入和输出，导致需要更好的方法来实现此功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/870
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及的对象是 GptManager。由于 GptManager 不够灵活，无法解决一些问题，用户希望使其更加可配置和灵活。,https://github.com/NVIDIA/TensorRT-LLM/issues/868
TensorRT-LLM,这是一个关于用户提出需求的类型的问题，主要涉及支持 DeciLM7Binstruct 的可能性。用户可能由于想了解是否将来会支持 DeciLM7Binstruct 而提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/853
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是深度学习模型优化工具TensorRT-LLM下的Mixtral Offloading功能。用户希望该工具可以支持一种新的缓存技术，以便在较小的GPU上运行时提高性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/849
TensorRT-LLM,这是一个用户提出需求的类型为feature request的issue，该问题单涉及的主要对象是TensorRT-LLM的HLAPI模块。由于用户希望在HLAPI的示例中添加构建命令，可能是为了方便使用和测试，因此提出了这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/848
TensorRT-LLM,这个issue类型是功能需求提出，主要涉及到对TensorRT-LLM模型更新和改进。由于功能要求涉及到添加新的模态模型支持、API增强、性能优化等，用户希望获得更多功能的支持和改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/846
TensorRT-LLM,这是一个功能需求类型的issue，主要涉及的对象是Qwen模型中的logn-scaling attention功能。由于TensorRT-LLM实现的Qwen模型目前不支持logn-scaling，导致输出质量较低，用户提出需要在实现中加入这一功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/836
TensorRT-LLM,这个issue类型属于用户提出需求，主要涉及对象是在TensorRT-LLM下构建和在不同型号GPU上运行引擎的需求。提问者想知道是否能够在一个GPU上构建引擎，然后在另一个不同型号的GPU上加载和运行该引擎。,https://github.com/NVIDIA/TensorRT-LLM/issues/835
TensorRT-LLM,这是一个需求类型的issue，主要涉及添加适用于Windows的批处理管理器静态库。原因可能是为了在Windows平台上提供更好的批处理管理功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/814
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到对句子重新表述的功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/813
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的GptSession或GptManager。用户希望能够直接从pytorch.bin或safetensors模型进行推断，而不必先通过build.py构建并存储引擎到磁盘，从而提高加载模型的效率。,https://github.com/NVIDIA/TensorRT-LLM/issues/809
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是希望在TensorRT-LLM中的examples/stablediffusion文件夹下实现转换和构建代码。由于非LLM transformer模型已经开始在框架中得到支持，用户需要更多的转换和构建代码用于重新定义SD模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/804
TensorRT-LLM,这是一个用户提出需求的Issue，主要涉及TensorRT-LLM下的TRTLLM Triton backend对encdec模型（例如T5）的支持更新。用户想了解是否有计划支持这一功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/800
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Whisper模型添加支持weight-only功能。该需求的目的是通过简化构建命令来减少GPU内存使用、提高推理速度并保持准确性。,https://github.com/NVIDIA/TensorRT-LLM/issues/794
TensorRT-LLM,这个issue属于功能请求类型，主要对象是针对TensorRT-LLM的支持YaRN请求。由于YaRN模型具有更长的上下文长度，因此用户请求该模型的支持以便在涉及广泛上下文的任务中发挥作用。,https://github.com/NVIDIA/TensorRT-LLM/issues/792
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及到TensorRT-LLM下的解码逻辑，用户希望能够改变解码逻辑以支持一些新的功能，例如推测性采样。,https://github.com/NVIDIA/TensorRT-LLM/issues/791
TensorRT-LLM,这个issue类型是用户提出需求，问题单涉及的主要对象是TensorRT-LLM，由于用户想要了解如何使用LLAMA 7b来运行示例而不需要构建引擎。,https://github.com/NVIDIA/TensorRT-LLM/issues/790
TensorRT-LLM,这是一个功能性问题，主要涉及TensorRT-LLM中的Bert和Roberta模型的支持。造成这个问题的原因是Bert模型在处理attention_mask时存在一些问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/778
TensorRT-LLM,这是一个用户需求类型的问题，主要涉及的对象是在TensorRT-LLM中应用平滑量化和管道并行的可能性。用户由于模型过大无法在单个GPU上运行导致量化失败，通过询问是否可以使用管道并行来解决这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/765
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及到如何在TensorRT-LLM中创建int4_awq类型的权重矩阵。用户提出了在quantize.py中无法直接支持int4_awq参数的问题，希望得到关于使用int4_awq的解决方案。,https://github.com/NVIDIA/TensorRT-LLM/issues/761
TensorRT-LLM,这个issue是关于更新TensorRT-LLM主分支的，涉及的主要对象是TensorRT-LLM软件。由于编译错误和文档更新等原因导致了多个bug修复和功能更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/754
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是支持Swin Transformer。原因可能是用户希望在TensorRT-LLM中添加对Swin Transformer的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/751
TensorRT-LLM,这是一个需求更新的issue。该问题单主要涉及TensorRT-LLM项目。由于版本更新或功能改进的需求，用户提出了更新TensorRT-LLM的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/750
TensorRT-LLM,这是一个关于更新TensorRT-LLM Release分支的问题，类型属于需求提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/745
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM，用户需要支持多个LoRA同时进行推理，类似于s-lora，以用于并发推理服务器。,https://github.com/NVIDIA/TensorRT-LLM/issues/738
TensorRT-LLM,这是用户提出的需求问题，主要对象是想在不构建引擎的情况下运行LLAMA 7b示例，可能由于无法使用Docker，希望能直接通过build_wheel.py运行基准测试。,https://github.com/NVIDIA/TensorRT-LLM/issues/731
TensorRT-LLM,这是一个关于性能优化和功能改进的Issue，主要涉及到TensorRT-LLM中重复惩罚内核的重写。该问题由于使用了全局内存分配的信号量进行跨块同步，导致新内核的执行时间增加了18%。,https://github.com/NVIDIA/TensorRT-LLM/issues/727
TensorRT-LLM,这是一个需求类型的issue，主要涉及TensorRT-LLM项目中的容器发布问题，用户由于网络问题无法构建Docker环境。,https://github.com/NVIDIA/TensorRT-LLM/issues/725
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM下的支持的模型版本。由于该repo只支持qwen7b和qwen13b，用户询问是否支持qwen1.8b，可能是出于想要使用这个模型版本的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/723
TensorRT-LLM,这是一个更新issue，涉及的主要对象是在/examples/qwen下的gradio版本更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/721
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及到TensorRT-LLM是否支持Lora/Qlora模型的问题，用户因无法将Lora或Qlora模型与基础checkpoint合并而提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/720
TensorRT-LLM,该issue类型为用户请求教程，主要对象是如何在TensorRT-LLM中加载和运行一个小模型。这个问题是因为用户想测试TensorRT-LLM在小模型上的运行速度，但不清楚如何在其中加载和运行非语言模型的小模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/717
TensorRT-LLM,这是一个需求更新的issue，主要对象是TensorRT-LLM更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/708
TensorRT-LLM,该issue类型是用户提出需求，主要关注的对象是TensorRT-LLM项目，用户在询问是否TensorRT-LLM计划集成页面注意力功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/702
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及自定义采样层和Inflight批处理。由于文档中提到了自定义采样层，用户想知道是否可以为Inflight批处理实现自定义生成循环。,https://github.com/NVIDIA/TensorRT-LLM/issues/692
TensorRT-LLM,这个issue类型为用户提出需求，主要涉及对象为支持llava，具体原因可能是用户希望TensorRT-LLM能够支持llava这个功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/687
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是'TensorRT-LLM'中的 'tensorrt_llm/kernels/kvCacheUtils.h' 文件。问题是用户想将'int32_t'变量改为其他类型（例如'int64'），以支持更大的batch_size/seq_length。这个问题是由于现有数据类型限制导致的，用户希望解决MMHA kernel中的错误。,https://github.com/NVIDIA/TensorRT-LLM/issues/686
TensorRT-LLM,这是一个关于需求提出的issue，主要涉及的对象是是否支持INT8 GEMM。由于TensorRT-LLM目前主要支持浮点数类型，因此用户提出了关于是否支持INT8/INT4 GEMM和W4Q4量化算法的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/682
TensorRT-LLM,"这是一个用户提出需求类型的issue，主要涉及到TensorRT-LLM中的""Speculative decoding""功能的使用问题，用户想了解如何使用此功能以及是否有相关文档可供参考。原因可能是用户想了解最新更新中提供的支持功能的具体细节。",https://github.com/NVIDIA/TensorRT-LLM/issues/671
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中如何连续地主机llama 7b模型的问题。用户希望能够持续加载模型，而不仅仅在推断时加载，可能由于当前内存使用量只在推断时增加而导致此问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/668
TensorRT-LLM,该问题类型为功能需求提议，主要涉及TensorRT-LLM下新增的TensorLLM插件是否能进一步优化ViT和Q-Former模型的运行速度。,https://github.com/NVIDIA/TensorRT-LLM/issues/664
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及Whisper中缺少时间戳特性的问题。用户询问是否有计划支持Whisper的时间戳特性。,https://github.com/NVIDIA/TensorRT-LLM/issues/647
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM中的模型。用户想知道是否可以在模型中传递多个输入层的数据。,https://github.com/NVIDIA/TensorRT-LLM/issues/641
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持SmoothQuant W8A8 for Mistral models的问题，用户想了解是否有预期的更新时间。,https://github.com/NVIDIA/TensorRT-LLM/issues/638
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及是否计划在TensorRT-LLM中支持Stable Diffusion模型，用户询问出于什么原因可以在TRTLLM中支持该模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/634
TensorRT-LLM,这个issue是关于功能需求的，主要涉及TensorRT-LLM中的pipeline parallelism功能，并由于持续批处理模式下未观察到预期性能差异而提出帮助请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/633
TensorRT-LLM,这是一个功能请求（Feature Request）issue，主要涉及了对TensorRT-LLM支持的模型进行增加或改进。由于用户请求支持不同的语言模型模型，包括Decoder Only、Encoder / EncoderDecoder和MultiModal类型的模型，以及一些相关功能的需求，并且此issue追踪了用户的请求和项目正在进行的工作。,https://github.com/NVIDIA/TensorRT-LLM/issues/632
TensorRT-LLM,这个issue类型为用户提出需求，主要对象是TensorRT-LLM中的图重写模块；用户想要了解图重写与计算图优化之间的关系，询问是否会在未来加入其他模式。,https://github.com/NVIDIA/TensorRT-LLM/issues/629
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的生成过程中共享key value cache blocks的优化问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/620
TensorRT-LLM,这是一个关于TensorRT-LLM中安装问题的技术支持请求，用户询问在构建引擎时是否需要设置正确的LD_LIBRARY_PATH参数，以及在不同机器上出现错误的原因可能是与TensorRT安装版本不匹配导致的。,https://github.com/NVIDIA/TensorRT-LLM/issues/619
TensorRT-LLM,这个issue是关于用户需求的，主要对象是在Windows 11上构建TensorRT-LLM时缺少批处理管理器的静态库文件，导致无法完成构建。,https://github.com/NVIDIA/TensorRT-LLM/issues/618
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM下如何支持自定义解码器模型的部署。用户希望在Tritonserver中使用TensorRTLLM后端部署经过训练的1B/6B模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/611
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中CUDA代码是否会在未来开源，问题涉及到实现的CUDA代码是否会对外公开。,https://github.com/NVIDIA/TensorRT-LLM/issues/609
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是StoppingCriteria和LogitsProcessor，在最新版本的Python API中支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/608
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM在Windows Visual Studio下使用cmake构建时遇到的问题。原因是目前的代码主要针对Linux，同时缺少对Windows下的NCCL库依赖处理方案。,https://github.com/NVIDIA/TensorRT-LLM/issues/606
TensorRT-LLM,这是一个关于需求问题，主要涉及TensorRT-LLM中加载多个Lora权重和多个文本输入以进行推理的问题。这可能由于当前只支持单个Lora权重和输入标记，无法支持多个导致。,https://github.com/NVIDIA/TensorRT-LLM/issues/599
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及运行trtllm而无需triton，以及测试/基准inflightbatching。,https://github.com/NVIDIA/TensorRT-LLM/issues/598
TensorRT-LLM,这个issue类型为功能增强，主要涉及的对象是将OpenAI Whisper集成进TensorRT-LLM模型。由于TensorRT引擎优化问题，用户提出了加速推断性能、输出时间戳等建议和需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/596
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及获取上下文隐藏状态的问题，可能导致用户无法按需访问所需的张量信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/595
TensorRT-LLM,这个issue类型是用户提出需求，请教问题，主要涉及的对象是如何设置mpirun的进程数量，以及是否可以使用两个GPU来运行特定任务。由于用户需要了解如何设置mpirun的进程数量以及是否可以使用多个GPU来运行任务，因此提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/593
TensorRT-LLM,这是一个用户提出需求的类型问题，主要涉及TensorRT-LLM engine是否支持在运行时使用if语句，用户困惑于模型中的if语句如何在构建和服务阶段保持一致性的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/589
TensorRT-LLM,这是一个功能需求提出问题，主要涉及TensorRT-LLM下的模型转换流程，用户希望能够直接从HF转换MPT模型到TRT引擎而不必经过FT的中间步骤。,https://github.com/NVIDIA/TensorRT-LLM/issues/586
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的attention kernels，用户关注的问题是在generation phase中，kernel是否支持query length大于1。,https://github.com/NVIDIA/TensorRT-LLM/issues/584
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及TensorRT-LLM的部署和测试，用户遇到安装TensorRTLLM时失败的问题，寻求关于如何使用TensorRTLLM进行部署以及如何构建自定义Docker镜像的帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/581
TensorRT-LLM,该issue类型为用户提出需求类型，主要涉及的对象是在Python中如何运行LLM模型的批量推断。这个问题可能是由于用户想要批量运行LLM模型进行推断而不知道如何操作或者缺乏相关文档或示例的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/570
TensorRT-LLM,这是一个功能需求类型的issue，主要对象是为Windows添加批处理管理器静态库。原因可能是当前缺少这个静态库，导致用户无法在Windows平台上有效地管理批处理任务。,https://github.com/NVIDIA/TensorRT-LLM/issues/569
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT是否支持`sequence_bias`和`renormalize_logits`功能。用户询问是否能够通过TensorRT实现类似的效果，以及提到了相关的GitHub链接。,https://github.com/NVIDIA/TensorRT-LLM/issues/568
TensorRT-LLM,这是一个需求类型的issue，主要涉及yi-34B llamafied model，用户想知道是否可以使用example/llama code。这可能是因为用户想利用llama code来应用在yi-34B llamafied model上，但不确定是否支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/567
TensorRT-LLM,这个issue是关于改进CUDA cast函数实现的，不属于bug报告类型，主要涉及到float到int8_t的转换方法优化。,https://github.com/NVIDIA/TensorRT-LLM/issues/559
TensorRT-LLM,这是用户提出需求的类型，主要对象是Mojo Integration。用户询问关于TensorRT-LLM是否有未来与Mojo集成的计划。,https://github.com/NVIDIA/TensorRT-LLM/issues/556
TensorRT-LLM,这是用户提出的需求类型的问题，主要涉及TensorRT-LLM中支持int4-awq/gptq for Qwen的功能。由于该需求可能与新技术或新功能相关，导致用户提出了关于支持int4-awq/gptq for Qwen的问题或需求帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/554
TensorRT-LLM,这是一个功能需求类型的issue，涉及到GPTj TensorRT-LLM模型的精度选项限制。由于构建脚本中限制了只能选择'float16'和'float32'这两种精度选项，用户想了解是否可能在GPTj TensorRT-LLM引擎中使用bfloat16精度选项。,https://github.com/NVIDIA/TensorRT-LLM/issues/553
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是项目的badge。由于badge信息需要更新，用户希望进行相关修改。,https://github.com/NVIDIA/TensorRT-LLM/issues/551
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到文档更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/550
TensorRT-LLM,这是一个寻求更新最新信息的issue，属于用户提出需求类型，主要涉及TensorRT-LLM项目。可能由于项目信息落后或者缺失导致用户需要更新最新内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/549
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是需要更新GitHub页面。由于GitHub页面可能过时或需要更新，用户提出了这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/544
TensorRT-LLM,这是一个关于功能问题的issue，主要涉及TensorRT-LLM下的sliding window attention是否可用，问题在于代码中发现其似乎被禁用了。,https://github.com/NVIDIA/TensorRT-LLM/issues/543
TensorRT-LLM,这是一个需求更新GitHub页面的类型为其他类型的issue，主要对象是项目的GitHub页面。导致此需求的原因可能是项目页面信息不全或需要更新特定内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/540
TensorRT-LLM,这个issue属于需求提出，主要对象是更新TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/539
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及LLama 70B模型构建引擎，由于模型大小过大导致内存不足问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/538
TensorRT-LLM,这是一个用户需求问题，涉及TensorRT-LLM中如何使用embedding作为输入的主要对象是LLaVa模型。由于目前的模型仅支持input_ids作为输入，用户希望得到关于如何实现使用embedding作为输入的指导。,https://github.com/NVIDIA/TensorRT-LLM/issues/534
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是Blip2在FlanT5xxl上的支持。用户提出了关于在FlanT5xxl上使用Blip2的需求，并寻求帮助解决相关技术问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/531
TensorRT-LLM,这个issue类型是需求更新，涉及主要对象为aarch64 batch manager libraries，用户希望进行更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/525
TensorRT-LLM,这是一个用户需求更新的类型，主要涉及TensorRT-LLM模型支持、特性添加、内存优化和性能提升，用户可能是提出需求或建议进行更新和改进。,https://github.com/NVIDIA/TensorRT-LLM/issues/524
TensorRT-LLM,这个issue属于用户提出需求的类型，主要涉及TensorRT LLM中如何找到第一个标记的延迟问题。用户询问如何在Jetson ORIN上报告不同上下文长度和生成步骤的第一个标记延迟，以供基准测试。,https://github.com/NVIDIA/TensorRT-LLM/issues/517
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是A10图形卡。由于A10图形卡在fp32模式下产生异常结果，在fp16模式下结果几乎正确，用户想了解A10图形卡是否与TensorRTLLM兼容，并希望将其添加到未来版本的兼容图形卡列表中。,https://github.com/NVIDIA/TensorRT-LLM/issues/516
TensorRT-LLM,这是一个用户提出需求的问题，主要对象是TensorRT-LLM。用户想知道是否在TensorRT-LLM中支持使用`stopping_criteria`功能，并请求相关示例。,https://github.com/NVIDIA/TensorRT-LLM/issues/513
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的paged kv cache使用情况展示，用户希望能够了解该缓存的利用情况以确定支持的批处理大小。,https://github.com/NVIDIA/TensorRT-LLM/issues/512
TensorRT-LLM,该issue类型是用户提出需求，主要涉及对象是TensorRT-LLM中的参数设置。由于在Volta架构平台上不支持enable_context_fmha，用户希望修改max_num_tokens以生成更长的序列，寻求如何设置的帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/511
TensorRT-LLM,这是一个需求更新的issue，主要对象是TensorRT-LLM。该问题可能是用户要求更新TensorRT-LLM项目的内容。,https://github.com/NVIDIA/TensorRT-LLM/issues/506
TensorRT-LLM,这是一个用户提出的需求类型的issue，主要涉及到TensorRT-LLM中如何控制GPU内存用量以限制KV缓存分配，用户希望能够为每个trtllm实例限制内存使用，由于在运行多个实例或模型时遇到内存使用问题而提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/486
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及运行Mistral-7B模型时不同tag的差异情况。用户询问在release/v0.5.0标签下运行Mistral-7B是否只有FasterTransformer加速而不需要KV缓存，寻求相关帮助。,https://github.com/NVIDIA/TensorRT-LLM/issues/482
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及到如何将多个prompt传递给run.py llama这个脚本。这个问题可能由于用户想同时处理多个prompt而导致。,https://github.com/NVIDIA/TensorRT-LLM/issues/481
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及的对象是TensorRT-LLM下的llama2模型，用户想要设置系统提示来生成输出。由于系统未提供相关设置选项，用户无法设置系统提示。,https://github.com/NVIDIA/TensorRT-LLM/issues/480
TensorRT-LLM,这个issue类型为用户提出需求，关联的主要对象是统计TensorRT-LLM模型中第一个token和后续token的延迟，原因可能是用户想要了解模型在不同情况下的延迟统计情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/473
TensorRT-LLM,这个issue类型为用户提出需求，请教问题，主要涉及的对象为TensorRT-LLM下的`text_output`功能，用户希望能够让返回的值不再包含提示信息。,https://github.com/NVIDIA/TensorRT-LLM/issues/467
TensorRT-LLM,这是一个关于软件版本发布时间的用户需求类问题，用户想了解0.6.0版本的发布时间。,https://github.com/NVIDIA/TensorRT-LLM/issues/463
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及如何实现在TensorRT-LLM中使用INT4 conv插件，其中问题包括INT4数据类型生成和张量格式转换。,https://github.com/NVIDIA/TensorRT-LLM/issues/462
TensorRT-LLM,这是一个关于功能问题的issue，主要涉及对象是TensorRT-LLM下的context_fmha设置和cross_attention。由于设置条件限制导致在特定情况下内存未正确释放，可能影响Gem操作的执行或功能性问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/461
TensorRT-LLM,这个issue类型是用户提出需求，请添加类似huggingface的StoppingCriteria和LogitsProcessor功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/459
TensorRT-LLM,这是一个关于功能运作机制的问题，涉及主要对象是TensorRT-LLM下的Attention kernels，由于文档描述和CUDA kernel源代码结构之间的混淆，用户想了解context和generation阶段是否调用不同的attention kernel实现。,https://github.com/NVIDIA/TensorRT-LLM/issues/457
TensorRT-LLM,这是关于功能比较的问题，主要涉及到TensorRT和vLLM的批量处理技术差异。用户询问在什么场景下应该使用TensorRT，以及在什么场景下应该使用vLLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/451
TensorRT-LLM,这是一个开发者提出需求的issue，涉及主要对象是TensorRT-LLM中的新插件实现和应用，开发者询问是否有更快的方法来验证新插件是否正常工作。,https://github.com/NVIDIA/TensorRT-LLM/issues/446
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是在docker中构建TensorRT-LLM的过程中遇到的问题。由于没有传递cuDNN相关的环境参数给cmake，导致了构建时的错误。,https://github.com/NVIDIA/TensorRT-LLM/issues/437
TensorRT-LLM,这是一个用户提出需求的问题，主要关注了支持分布式推理在多个节点上运行的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/433
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及添加一个说明TensorRT-LLM和TensorRT之间关系的图表。原因可能是为了提供更直观的文档帮助理解两者之间的关系。,https://github.com/NVIDIA/TensorRT-LLM/issues/428
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM是否支持数据并行或多进程推理，其问题在于无法正确映射TensorRT实例到不同GPU导致无法进行多GPU推理。,https://github.com/NVIDIA/TensorRT-LLM/issues/420
TensorRT-LLM,这个issue属于对代码优化的需求，主要对象是CUDA kernel performance相关的问题，提问者想了解是否有计划优化加载kvcache的次数。,https://github.com/NVIDIA/TensorRT-LLM/issues/407
TensorRT-LLM,这是一个关于功能问题的issue，主要涉及到TensorRT-LLM的batch manager，用户想了解该功能是否支持cuda graph，并表达了针对高延迟的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/402
TensorRT-LLM,这是一个用户请求代码位置相关的问题，主要对象是TensorRT-LLM项目。用户可能因为想要查看代码而提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/397
TensorRT-LLM,这是一个用户提出需求的问题，涉及的主要对象是TensorRT-LLM和transformers库。由于requirements文件中指定了固定的transformers版本，用户询问是否有计划使用最新版本或者是否存在必须固定特定版本的限制。,https://github.com/NVIDIA/TensorRT-LLM/issues/385
TensorRT-LLM,这个issue类型是用户提出需求类型，主要对象涉及TensorRT-LLM下的添加rwkv模型，用户提出需要添加rwkv模型功能并进行相关工作分解，目前尚未完成部分功能的开发。,https://github.com/NVIDIA/TensorRT-LLM/issues/384
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是更新最新新闻，此问题可能由于信息更新不及时或者缺失导致用户需要更新最新的消息。,https://github.com/NVIDIA/TensorRT-LLM/issues/379
TensorRT-LLM,这个issue类型为用户提出需求，该问题单涉及的主要对象是项目的最新消息更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/378
TensorRT-LLM,这是用户提出需求的类型，主要涉及对象为构建HF格式模型。用户询问是否可以直接使用AWQ作为TensorRT引擎来构建已通过AWQ量化的模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/377
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及添加最新消息部分的功能要求。,https://github.com/NVIDIA/TensorRT-LLM/issues/368
TensorRT-LLM,这个issue类型为用户提出需求，希望添加最新消息部分，主要涉及到TensorRT-LLM项目。,https://github.com/NVIDIA/TensorRT-LLM/issues/367
TensorRT-LLM,这个issue类型是用户提出需求，该问题单涉及的主要对象是添加Latest News section。由于当前项目缺少最新动态信息展示，用户在此提出需求添加Latest News section。,https://github.com/NVIDIA/TensorRT-LLM/issues/366
TensorRT-LLM,这是一个需求类型的issue，主要对象是添加最新消息模块。,https://github.com/NVIDIA/TensorRT-LLM/issues/365
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及到TensorRT-LLM中decode函数的参数设置。用户希望添加attention_mask参数来指示padding位置并进行padding token的mask处理，以适应不同长度句子的输入。,https://github.com/NVIDIA/TensorRT-LLM/issues/364
TensorRT-LLM,该issue类型为用户提出需求，主要涉及对象为如何将一个Tensor类型对象转换为torch.Tensor，用户提出了关于如何在TensorRTLLM中实现特定功能的问题，并寻求相应的API或编写代码的方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/363
TensorRT-LLM,"这是一个用户提出需求的问题，主要对象是在TensorRT-LLM项目中添加一个""Latest News""部分。",https://github.com/NVIDIA/TensorRT-LLM/issues/362
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到在TensorRT-LLM项目中添加最新消息部分。可能是希望能够及时了解项目的新动态。,https://github.com/NVIDIA/TensorRT-LLM/issues/361
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及 MBartForCausalLM 的支持。用户请求提供 MBartForCausalLM 的示例。,https://github.com/NVIDIA/TensorRT-LLM/issues/360
TensorRT-LLM,这是一个用户提出需求和寻求帮助的类型，主要涉及TensorRT-LLM中的inflight_batching功能。由于用户不清楚批处理管理器和inflight_batching之间的区别，以及源代码的开放情况，因此提出了这些问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/354
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及添加对Flan-T5模型的支持。这个需求的提出是为了适配FlanT5模型的特定结构和激活函数。,https://github.com/NVIDIA/TensorRT-LLM/issues/351
TensorRT-LLM,这是一个类型为更新请求的issue，主要涉及的对象是TensorRT-LLM。可能由于项目需要更新或者改进功能而提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/349
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及的对象是AutoAWQ support。,https://github.com/NVIDIA/TensorRT-LLM/issues/345
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT 9和PyTorch NGC。由于PyTorch NGC容器23.10不支持TensorRT 9，用户询问是否23.11容器会支持TensorRT 9。,https://github.com/NVIDIA/TensorRT-LLM/issues/337
TensorRT-LLM,这是用户提出的需求，希望提供与OpenAI API类似的简单接口。由于用户希望使用类似OpenAI API的接口，可能是为了方便使用TensorRT-LLM模型进行开发和部署。,https://github.com/NVIDIA/TensorRT-LLM/issues/334
TensorRT-LLM,这是一个用户提出需求的问题，涉及的主要对象是TensorRT-LLM模型加载过程。用户想要在Python中释放GPU内存，而不需要终止Python进程。,https://github.com/NVIDIA/TensorRT-LLM/issues/327
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及到TensorRT-LLM中gemm_config.in文件的生成问题。用户询问是否在TensorRT-LLM中仍然需要手动生成gemm_config.in文件，并希望了解是否TensorRTLLM能够自动选择最佳的GEMM算法。,https://github.com/NVIDIA/TensorRT-LLM/issues/325
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及到TensorRT-LLM下StarCoder的SmoothQuant变体支持问题，用户询问当前构建StarCoder GPT变体在应用smooth quant时失败，并询问是否有计划支持该功能以及如何解决此问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/324
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及在TensorRT-LLM中支持非LLM transformer网络的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/323
TensorRT-LLM,这个issue是一个用户提出需求类型的问题，主要涉及的对象是TensorRT-LLM与Deepspeed之间的通过动态拆分融合实现2倍吞吐量的比较。通过问题描述可以看出用户想要实现类似于Deepspeed的动态拆分融合功能，但不确定是否能在TensorRT中实现，可能由于两者的批处理机制不同导致了这个问题的提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/317
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是在TensorRT-LLM中添加一个最新消息（Latest News）部分。,https://github.com/NVIDIA/TensorRT-LLM/issues/315
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是项目的Latest News部分。 由于该部分还未添加相关内容，用户希望在其中加入最新消息。,https://github.com/NVIDIA/TensorRT-LLM/issues/314
TensorRT-LLM,这个issue是用户提出需求。主要对象是将W8A8引擎转换为ChatGLM2-6b，但没有找到将huggface转换为fastertransformer的方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/312
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是LLM模型。由于LLM作为多模态模型的一部分时，需要支持soft_prompt或inputs_embeds，因此用户提出了这个功能需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/310
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Batch Manager开源问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/308
TensorRT-LLM,这是一个关于需求的问题，主要涉及TensorRT-LLM中的python benchmark，用户询问是否支持pipeline parallel和在运行benchmark时是否使用了tensor parallel和rank是8。,https://github.com/NVIDIA/TensorRT-LLM/issues/303
TensorRT-LLM,这个issue属于更新请求类型，主要涉及TensorRT-LLM项目。由于项目版本可能需要更新或修复某些功能，用户提出了更新请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/302
TensorRT-LLM,这个issue类型是用户提出需求，请添加新的数据集支持。该问题单涉及的主要对象是在TensorRT-LLM下运行MMLU、TruthfulQA和ARC数据集。由于缺少后端支持，用户无法在LMevaluationharness中运行这些数据集。,https://github.com/NVIDIA/TensorRT-LLM/issues/300
TensorRT-LLM,这个issue是用户提出的一个需求，主要涉及到是否能够支持Ptuning功能，其中提到了Ptuning V2 for Chatglm，用户希望在TensorRT-LLM中实现这一功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/295
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是交互式生成功能。由于用户希望了解有关交互式生成功能的计划，因此提出了这个需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/292
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到TensorRT-LLM的BertModel示例构建脚本，用户提出了添加对加载预训练模型的支持的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/291
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM下的Python runtime，问题是关于如何查看在Python运行时中添加的日志打印，可能是由于日志打印未生效或未正确配置导致。,https://github.com/NVIDIA/TensorRT-LLM/issues/289
TensorRT-LLM,这是一个用户提出需求的 issue，主要涉及到安装 TensorRT-LLM 时无法使用 Docker 的问题。由于某些原因，用户无法在系统上使用 Docker，因此寻求不使用 Docker 的安装教程。,https://github.com/NVIDIA/TensorRT-LLM/issues/286
TensorRT-LLM,这是一个关于性能需求的问题，主要涉及TensorRT-LLM中的KV缓存以及在特定硬件上运行时内存消耗的情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/282
TensorRT-LLM,这是一个用户提出需求类似的issue，主要涉及的对象是支持`frequency_penalty`，用户希望增加对此参数的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/275
TensorRT-LLM,这是一个用户提出的需求问题，主要涉及TensorRT-LLM中支持`repetition_penalty`和`presence_penalty`参数互相排斥的情况，导致无法创建符合OpenAI需求的API。,https://github.com/NVIDIA/TensorRT-LLM/issues/274
TensorRT-LLM,这是一个需求类型的issue，主要涉及支持`gather_all_token_logits`标志以构建Llama模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/273
TensorRT-LLM,这个issue是用户提出需求，主要对象是关于TensorRT-LLM中生成token数量的设置或查询，用户想要知道如何获取每个提示生成的输出token数量，并询问是否有方法来设置这个变量。,https://github.com/NVIDIA/TensorRT-LLM/issues/271
TensorRT-LLM,该issue属于用户提出需求类型，主要涉及的对象是TensorRT-LLM项目中的ChatGLM3模型。由于用户想要测试使用chatglm26b来测试ChatGLM3或者了解是否有计划支持ChatGLM3。,https://github.com/NVIDIA/TensorRT-LLM/issues/270
TensorRT-LLM,这是一个用户需求类型的issue，主要涉及benchmarks/cpp/prepare_dataset.py脚本的使用。导致该问题的原因是用户在运行benchmarks/gptManagerBenchmark时需要生成input_ids文件，但不清楚如何准备这个文件的示例。,https://github.com/NVIDIA/TensorRT-LLM/issues/269
TensorRT-LLM,这是一个需求提出的issue，主要涉及支持InternLM模型，由于要实现的功能是支持不同类型的权重和缓存，因此这可能涉及到模型优化和性能改进方面的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/266
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及的对象是TensorRT-LLM，用户询问是否支持arrch64。,https://github.com/NVIDIA/TensorRT-LLM/issues/262
TensorRT-LLM,这是一个用户请求帮助的issue，主要涉及使用flan-t5-base遇到的问题。由于无法加载和使用google/flant5base，用户寻求帮助解决问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/251
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的logits manipulators，用户想知道是否有计划添加类似transformers支持的typical_p功能，以便更容易地过渡到TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/241
TensorRT-LLM,这是一个用户提出需求的类型，该问题涉及request streaming功能，用户提出需要一个标志来指示生成结束。,https://github.com/NVIDIA/TensorRT-LLM/issues/240
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是希望从模型返回token的对数概率。,https://github.com/NVIDIA/TensorRT-LLM/issues/238
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及对象是构建TensorRT引擎。由于内存不足导致Falcon的`build.py`脚本在g5.2xlarge实例上崩溃，用户想知道需要多少内存来构建引擎。,https://github.com/NVIDIA/TensorRT-LLM/issues/236
TensorRT-LLM,这是一个关于用户提出需求的问题，主要涉及的对象是TensorRT-LLM下的模型后处理。用户希望获取仅包含新生成 token 的功能，但由于后处理模型尝试接收来自 decouple 模型 tensorrt_llm 和非 decoupled 模型预处理的输入，导致出现错误，用户寻求解决方案。,https://github.com/NVIDIA/TensorRT-LLM/issues/227
TensorRT-LLM,这是一个关于用户提出需求的问题，主要涉及TensorRT-LLM中是否能够在单个前向传递中处理多个标记的问题。由于用户希望能够同时处理多个输入标记并获得相应的注意力分数，因此提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/223
TensorRT-LLM,这个issue是一个功能需求请求，主要涉及的对象是TensorRT-LLM中的批处理管理库。这个需求是由于缺乏批处理管理库导致用户希望增加该功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/221
TensorRT-LLM,这是一个用户提出需求的issue，主要关注的对象是支持FlashDecoding来加快长序列推理速度。,https://github.com/NVIDIA/TensorRT-LLM/issues/209
TensorRT-LLM,这是一个需求提议的issue，涉及将GQA支持添加到MPT和GPT模型中。原因是当前TensorRTLLM尚不支持GQA模型，但某些MPT模型可能会使用GQA，用户希望增加对GQA的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/205
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM中的max capacity for paged KV cache models的指定方式。原因是希望能够通过其他方式指定最大内存容量，而不是当前的方式。,https://github.com/NVIDIA/TensorRT-LLM/issues/204
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是GptManager。由于C++实现中没有提供设置sampling配置的接口，用户询问如何使用GptManager进行文本采样。,https://github.com/NVIDIA/TensorRT-LLM/issues/193
TensorRT-LLM,这个issue属于用户提出需求。该问题涉及的主要对象是TensorRT-LLM项目。用户在询问是否有计划在本地Linux环境下构建TensorRT-LLM而不是使用基于Docker的构建方法。,https://github.com/NVIDIA/TensorRT-LLM/issues/190
TensorRT-LLM,该issue类型为更新请求，主要涉及的对象是TensorRT-LLM项目。,https://github.com/NVIDIA/TensorRT-LLM/issues/188
TensorRT-LLM,该issue属于用户提出需求类型，主要对象是TensorRT-LLM下的支持ChatGLM3。因发布了ChatGLM3模型成为参数少于10b的最佳中文模型，用户请求添加对ChatGLM3的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/180
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及到项目的文档更新。由于README.md文件可能需要更新或修正，用户提出更新的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/177
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及的对象是TensorRT-LLM项目。由于DeBerta模型目前不受支持，用户请求添加对其的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/174
TensorRT-LLM,这个issue属于功能需求提议类型，主要涉及Activation Functions的实现。原因是用户提出了需要实现包括tanhshrink、logsoftmax、softmin、dimwise tensor sum、selu、logsigmoid以及relu6等激活函数的需求。,https://github.com/NVIDIA/TensorRT-LLM/issues/173
TensorRT-LLM,这是一个关于功能需求的issue，主要涉及的对象是Activation Operators。原因是用户请求跟踪所有激活函数操作符的实现来源。,https://github.com/NVIDIA/TensorRT-LLM/issues/172
TensorRT-LLM,这个issue类型是提出需求，涉及的主要对象是在TensorRT-LLM下实现 speculative sampling / assisted generation功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/169
TensorRT-LLM,这个issue主要是功能需求的提出，涉及到Conv操作在Pytorch API中的实现问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/163
TensorRT-LLM,该issue类型为用户提出需求，涉及主要对象是模型在量化前后在一系列常见基准测试中的性能比较。用户询问如何比较量化前后模型在常见基准测试中的性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/161
TensorRT-LLM,这是一个用户提出需求的类型的issue，主要对象是支持Zephyr 7B模型，由于当前没有该模型的支持，用户请求添加对https://huggingface.co/HuggingFaceH4/zephyr7balpha的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/157
TensorRT-LLM,这是一个更新需求，主要对象是batch manager，用户希望在release/0.5.0版本中对其进行更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/152
TensorRT-LLM,这是一个用户提出需求的类型的issue，主要涉及的对象是TensorRT-LLM中的batch manager组件。由于实现细节被封装在C++静态库中，导致用户难以基于batch manager实现一些优化技术，因此用户询问batch manager是临时还是永久闭源的。,https://github.com/NVIDIA/TensorRT-LLM/issues/150
TensorRT-LLM,这个issue类型是用户提出需求，主要涉及的对象是如何在多个节点之间构建模型。由于两台机器之间没有NVLink连接，用户想知道如何在两台机器之间通信以利用4个GPU构建和运行模型。,https://github.com/NVIDIA/TensorRT-LLM/issues/147
TensorRT-LLM,这是一个用户提出需求的问题，主要涉及TensorRT-LLM对哪些GeForce GPU的支持，以及是否计划扩展对其他NVIDIA GPU的支持。该问题由于不清楚TensorRT-LLM具体支持哪些单个NVIDIA GPU以及其相应的VRAM数量而引起。,https://github.com/NVIDIA/TensorRT-LLM/issues/146
TensorRT-LLM,这个issue类型是需求问题，主要涉及定义模型中的任意大小的Tensor，可能是由于模型定义中出现了'`default_net`'相关错误而导致问题的提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/145
TensorRT-LLM,这个issue是用户提出需求类型的问题，主要涉及的对象是GptManager，用户想要实现从RAM中加载数据而非文件系统导致的问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/144
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及TensorRT-LLM生成不同输出长度句子的问题。这个问题可能是由于TensorRT-LLM在同一批次生成的句子长度相同，需要用户手动修改终止符来实现不同长度的句子生成。,https://github.com/NVIDIA/TensorRT-LLM/issues/140
TensorRT-LLM,这是一个用户需求类型的issue，主要涉及Docker容器中Flask应用与宿主机具有相同IP地址的问题。由于Docker网络隔离性导致，用户希望了解如何使Flask应用与宿主机拥有相同IP地址。,https://github.com/NVIDIA/TensorRT-LLM/issues/131
TensorRT-LLM,这是一个用户提出需求的问题，涉及到TensorRT-LLM，用户询问是否可以运行调整来获得更好的性能。,https://github.com/NVIDIA/TensorRT-LLM/issues/129
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM中的文本生成模型。用户提出需要添加对抽样（sampling）功能的支持，以获得更好的文本生成效果（类似于T5模型）。,https://github.com/NVIDIA/TensorRT-LLM/issues/128
TensorRT-LLM,这个issue是关于用户提出需求的类型，主要涉及到T5模型类型支持。用户想要在TensorRT-LLM中使用T5large模型时遇到了权重定义的问题，希望得到关于T5模型支持的改进和优化建议。,https://github.com/NVIDIA/TensorRT-LLM/issues/127
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是AWQ quantization功能。 由于特定torch版本依赖性限制，导致用户维护自己的镜像时需要构建特定torch版本，提出希望增加灵活性的要求。,https://github.com/NVIDIA/TensorRT-LLM/issues/126
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及支持Robert模型，用户请求添加对Hugging Face Roberta模型的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/124
TensorRT-LLM,这是一个功能需求的issue，主要涉及支持InternLM模型。这个问题尚未完全开发和测试并行计算和量化功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/123
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是TensorRT-LLM下的生成引擎，用户希望支持多个停止标识符。,https://github.com/NVIDIA/TensorRT-LLM/issues/120
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是通过Python运行时实现的stream_decode生成器功能。由于实现非阻塞流处理需要修改源代码，用户在询问是否有其他方法实现非阻塞流处理。,https://github.com/NVIDIA/TensorRT-LLM/issues/111
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是Attention sink。用户询问有没有计划在未来几周或几个月内对Attention sink进行工作，表达了期待看到未来发展的想法。,https://github.com/NVIDIA/TensorRT-LLM/issues/104
TensorRT-LLM,这是一个关于用户提出需求的类型，主要是在询问是否支持Qwen-7B或Qwen-14B等其他模型。原因可能是用户希望了解更多可支持的模型的情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/87
TensorRT-LLM,这是一个用户提出需求的类型，主要涉及到支持InternLM模型。这个问题是由于用户希望TensorRT-LLM支持InternLM模型造成的。,https://github.com/NVIDIA/TensorRT-LLM/issues/86
TensorRT-LLM,该问题属于用户提出需求，并涉及到如何在TRTLLM中输出模型的中间结果，以便定位精度错误所在。,https://github.com/NVIDIA/TensorRT-LLM/issues/80
TensorRT-LLM,"这是一个用户提出需求的类型，主要关注于支持""group beam search""和""diverse beam search""方法的功能新增请求。",https://github.com/NVIDIA/TensorRT-LLM/issues/79
TensorRT-LLM,这是一个增强功能的类型issue，主要涉及的对象是`GptManager`，由于需要手动构建`pygptmanager`模块以及与`BatchManager`符号存在链接问题，导致了相关症状。,https://github.com/NVIDIA/TensorRT-LLM/issues/76
TensorRT-LLM,这是一个用户提出需求的issue，涉及的主要对象是batch_manager。这个问题由于未知原因导致用户想知道未来是否会开源batch_manager。,https://github.com/NVIDIA/TensorRT-LLM/issues/74
TensorRT-LLM,这是一个功能需求的issue， 主要涉及TensorRT-LLM的离线吞吐量benchmark，用户在询问如何使用`use_inflight_batching`和`use_gpt_attention_plugin float16`构建选项的兼容性问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/73
TensorRT-LLM,这个issue类型是用户提出需求， 主要对象是如何在不使用Docker的情况下构建TensorRT-LLM，可能是用户对构建过程或环境有特定需求或限制。,https://github.com/NVIDIA/TensorRT-LLM/issues/70
TensorRT-LLM,这是一个用户提出需求的issue，用户想要了解如何将使用LoRA微调的Llama 2模型转换为TensorRTLLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/68
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及TensorRT-LLM中的max_batch_size和max_num_sequences的区别问题。最可能是由于缺乏文档解释导致用户需要明确了解两者之间的差异。,https://github.com/NVIDIA/TensorRT-LLM/issues/65
TensorRT-LLM,这个issue属于用户请求类，主要对象是文档的可读性。造成这个问题的原因可能是贡献者希望改善Readme.md文件的可读性，并询问所需时间来合并他们的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/63
TensorRT-LLM,该issue是用户请求支持，主要对象是在Nvidia AGX Orin开发套件上运行TensorRTLLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/62
TensorRT-LLM,这是一个用户提出需求类型的issue，主要涉及的对象是TensorRT-LLM项目的Windows相关文档。可能由于过时或不完整的文档，用户提出需要更新Windows相关文档到主分支。,https://github.com/NVIDIA/TensorRT-LLM/issues/60
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM在Windows平台上的相关文档。由于文档需要更新，用户提出了更新Windows相关文档的请求。,https://github.com/NVIDIA/TensorRT-LLM/issues/59
TensorRT-LLM,该issue属于用户提出需求类型，主要对象是Docker image的发布进度。原因可能是用户想了解关于发布时间的情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/52
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到如何处理kv-cache在多模态GPT中的问题。由于模型行为在初始和后续传递之间存在差异，导致了挑战。,https://github.com/NVIDIA/TensorRT-LLM/issues/51
TensorRT-LLM,这是一个用户提出需求的issue，主要对象是添加对Mistral 7B模型的支持。由于Mistral模型比Llama2模型表现更好，用户希望在TensorRT-LLM中添加对Mistral模型的支持。,https://github.com/NVIDIA/TensorRT-LLM/issues/49
TensorRT-LLM,这个issue是用户提出需求，主要涉及TensorRT-LLM对于RWKV项目的支持问题。用户询问是否TensorRT-LLM支持RWKV项目，描述了RWKV项目的特性以及希望得到对应支持的情况。,https://github.com/NVIDIA/TensorRT-LLM/issues/47
TensorRT-LLM,这是一个文档更新的issue，主要对象是batch_manager.md文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/40
TensorRT-LLM,该问题类型是用户提出需求，请教问题，涉及的主要对象是Triton Inference Server。由于当前发布的版本不支持 flashattention，用户询问是否有计划将其集成到 tritoninferenceserver 中，以及是否会在未来支持该功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/33
TensorRT-LLM,这个issue类型为用户提出需求，主要涉及的对象是项目团队。用户想了解项目团队是否计划支持MiniGPT4。,https://github.com/NVIDIA/TensorRT-LLM/issues/31
TensorRT-LLM,这是一个用户提出需求的类型。用户想知道他们能否在GPT-2上使用TensorRT-LLM。,https://github.com/NVIDIA/TensorRT-LLM/issues/27
TensorRT-LLM,这个issue属于用户提出需求类型，主要涉及的对象是支持的GPU类型。用户询问为什么A10 GPU没有列入支持列表，以及是否在将来会支持这个GPU。,https://github.com/NVIDIA/TensorRT-LLM/issues/26
TensorRT-LLM,这是一个用户提出需求的issue，主要关注支持Medusa Sampling。原因在于需求扩展。,https://github.com/NVIDIA/TensorRT-LLM/issues/25
TensorRT-LLM,这是一个关于安装问题的类型为用户提出需求的issue，主要涉及对象为在docker容器中安装TensorRT-LLM，由于用户已经在docker容器中，因此提出如何在此环境下安装TensorRT-LLM的疑问。,https://github.com/NVIDIA/TensorRT-LLM/issues/22
TensorRT-LLM,这个issue类型是需求提出，用户提出了需要在Windows上发布可用的wheel链接。,https://github.com/NVIDIA/TensorRT-LLM/issues/18
TensorRT-LLM,这是一个用户提出需求类型的issue，主要涉及的对象是TensorRT-LLM的项目主页，可能由于当前首页内容需要更新或修正而提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/14
TensorRT-LLM,这是一个用户提出需求的类型，主要对象是项目的首页。由于可能需要对项目首页进行修订或更新，用户提出了这个问题。,https://github.com/NVIDIA/TensorRT-LLM/issues/13
TensorRT-LLM,这是一个关于需求提出的issue，主要涉及的对象是为TensorRT-LLM添加git-lfs依赖的二进制文件。,https://github.com/NVIDIA/TensorRT-LLM/issues/12
TensorRT-LLM,这个issue类型是用户提出需求，请求为TensorRT-LLM添加git-lfs依赖以处理二进制文件的发布。,https://github.com/NVIDIA/TensorRT-LLM/issues/11
TensorRT-LLM,这是一个更新请求类型的issue，主要涉及更新aarch64 batch manager libraries到release/0.5.0版本，可能由于需要更新功能或修复bug而被提出。,https://github.com/NVIDIA/TensorRT-LLM/issues/10
TensorRT-LLM,这是一个更新操作系统相关的功能库至主要分支的issue，主要涉及aarch64 batch manager libraries，原因可能是需要修复bug或者添加新功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/9
TensorRT-LLM,这个issue属于用户提出需求，要求更新aarch64 libraries到release/0.5.0分支。,https://github.com/NVIDIA/TensorRT-LLM/issues/7
TensorRT-LLM,这是一个需求更新的issue，涉及到项目主分支的更新。这个问题可能是由于需要引入新的功能、修复bug或其他一些更新而发起的。,https://github.com/NVIDIA/TensorRT-LLM/issues/5
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及的对象是TensorRT-LLM代码更新。,https://github.com/NVIDIA/TensorRT-LLM/issues/3
TensorRT-LLM,这是一个用户提出需求的issue，主要涉及到添加静态库来管理批处理。根据issue标题和内容推测，用户希望将静态库添加到批处理管理器中。,https://github.com/NVIDIA/TensorRT-LLM/issues/2
TensorRT-LLM,这个issue类型为更新请求，主要涉及的对象是将TensorRT-LLM中的onnx版本从1.12.0升级到1.13.0。这个更新请求是由于新发布的ONNX版本1.13.0带来了一系列新特性和功能。,https://github.com/NVIDIA/TensorRT-LLM/issues/1
ColossalAI,这个issue是一个用户提交的关于更新CI的请求，主要涉及到ColossalAI项目的持续集成（CI）流程。用户提交这个问题是为了更新CI配置以确保代码的质量和可靠性。,https://github.com/hpcaitech/ColossalAI/issues/6254
ColossalAI,该issue是一个功能需求，主要涉及ColossalAI中的支持pipeline并行性的问题，用户寻求了关于如何实现pipeline并行性的帮助。,https://github.com/hpcaitech/ColossalAI/issues/6252
ColossalAI,这个issue是一个功能优化提案，主要涉及Vllm算法的修复，增加过滤、温度退火和学习率下降功能。原因可能是为了提升算法的性能和稳定性。,https://github.com/hpcaitech/ColossalAI/issues/6250
ColossalAI,这是一个关于功能需求的issue，主要涉及的对象是ColossalAI的FP8计算支持情况，用户提出问题是关于ColossalAI FP8是否支持在MoE模型中的`group_gemm`操作。,https://github.com/hpcaitech/ColossalAI/issues/6249
ColossalAI,这个issue是一个功能特性请求，主要涉及到ColossalAI中的分布式log概率支持问题，由于需要进行GRPO训练，用户提出了相关需求。,https://github.com/hpcaitech/ColossalAI/issues/6247
ColossalAI,这个issue是一个需求提出类型的问题，主要涉及ColossalAI中的grpo latest支持npu，可能是为了解决性能或功能方面的需求。,https://github.com/hpcaitech/ColossalAI/issues/6242
ColossalAI,这是一个需求类型的issue，主要涉及AI Agent中文技术交流群。由于内容只为“!Image”，用户可能在寻求关于添加或查看图片的帮助或者提出了关于图片显示的bug。,https://github.com/hpcaitech/ColossalAI/issues/6241
ColossalAI,这个issue是关于更新ColossalAI中load lora model的Readme文档，并非bug报告。,https://github.com/hpcaitech/ColossalAI/issues/6240
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的使用，请求提供使用鲁莽AI训练自定义模型的文档和示例代码。用户希望了解如何使用TP+PP训练方法以及如何编写Policy。,https://github.com/hpcaitech/ColossalAI/issues/6238
ColossalAI,这个issue属于特性需求，主要涉及到ColossalAI中的分布式实现。由于要创建包含各种检查点和指导的分布式GRPO，用户可能会遇到实现问题或需要协作。,https://github.com/hpcaitech/ColossalAI/issues/6230
ColossalAI,这是一个用户向ColossalAI提交的需求类型的Issue，主要涉及实现支持基于整数编程的双管道调度系统。,https://github.com/hpcaitech/ColossalAI/issues/6229
ColossalAI,这个issue属于特性需求类型，主要涉及ColossalAI中添加分布式实现的PPO。原因是为了提供更强大的模型分布式训练支持。,https://github.com/hpcaitech/ColossalAI/issues/6228
ColossalAI,该issue类型为用户提出需求，主要涉及ColossalAI下的特性开发。由于涉及到非张量广播，可能是用户希望扩展该功能以支持更灵活的数据处理需求。,https://github.com/hpcaitech/ColossalAI/issues/6218
ColossalAI,这个issue类型为特性需求，主要涉及的对象是将GRPO集成到RAY中。由于需要实现对GRPO的集成，因此提出了此需求。,https://github.com/hpcaitech/ColossalAI/issues/6214
ColossalAI,这个issue类型为功能需求，主要涉及RL风格生成，由于旧体验生成代码需要适应RL风格生成，因此提出这个功能需求。,https://github.com/hpcaitech/ColossalAI/issues/6213
ColossalAI,这个issue是用户在ColossalAI中提交的一个关于更新torch版本的PR请求。,https://github.com/hpcaitech/ColossalAI/issues/6206
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是GeminiPlugin和DeepSeek V3。用户想知道如何使用GeminiPlugin来finetune或继续预训练DeepSeek V3。,https://github.com/hpcaitech/ColossalAI/issues/6200
ColossalAI,这个issue类型是文档更新，涉及主要对象为DeepSeek V3/R1，用户提出了关于PR创建前的标准步骤和自查清单的问题。,https://github.com/hpcaitech/ColossalAI/issues/6199
ColossalAI,这是一个关于向ColossalAI添加LoRA SFT示例数据的类型为功能需求的issue，涉及的主要对象是ColossalAI应用程序。这个issue的存在是为了让用户能够更轻松地使用ColossalAI，并为用户提供LoRA SFT示例数据。,https://github.com/hpcaitech/ColossalAI/issues/6198
ColossalAI,这个issue类型是文档更新，涉及主要对象为ColossalAI的README。由于缺少相关检查和步骤未完整完成，导致需要更新README以符合规范要求。,https://github.com/hpcaitech/ColossalAI/issues/6196
ColossalAI,这个issue类型是关于代码更新的请求，涉及主要对象是ColossalAI项目。由于缺少必要的步骤和标准操作，这可能导致PR的创建和审查过程存在一些问题。,https://github.com/hpcaitech/ColossalAI/issues/6195
ColossalAI,这个issue是一个用户提出需求的类型，主要涉及ColossalAI中添加lora sft示例，用户希望为该项目添加lora sft示例。这个需求可能是为了展示 lora sft 的使用方法或示例。,https://github.com/hpcaitech/ColossalAI/issues/6192
ColossalAI,这个issue类型是需求提出，主要对象涉及到ColossalAI中的PPO模块，用户希望在其中添加GRPO和支持RLVR。导致用户提出这个需求的原因可能是为了增强PPO模块的功能和灵活性。,https://github.com/hpcaitech/ColossalAI/issues/6186
ColossalAI,该issue类型为用户提出需求，主要涉及ColossalAI的deepseek v3功能支持，可能是由于目前系统缺乏对ep（epoch）的支持而导致用户提出这一需求。,https://github.com/hpcaitech/ColossalAI/issues/6185
ColossalAI,该issue是一个新功能需求，涉及ColossalAI的分布式checkpointio，由于需要创建一个分布式的checkpointio功能，提高ColossalAI的性能和效率。,https://github.com/hpcaitech/ColossalAI/issues/6183
ColossalAI,这个issue类型为需求提出，主要对象是ColossalAI中的checkpointio模块，用户提出了支持模型保存时的分布式检查点IO的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/6181
ColossalAI,该issue类型为用户提出需求，主要涉及对象是MoE模型的expert parallel代码实现，用户询问是否有类似于qwen和deepseek的专家并行代码实现。,https://github.com/hpcaitech/ColossalAI/issues/6180
ColossalAI,这是一个更新通知类型的issue，更新了pre-commit中使用的一些工具的版本。,https://github.com/hpcaitech/ColossalAI/issues/6179
ColossalAI,这个issue是一个功能改进的PR，主要关注ColossalAI的[checkpointio]模块，目的是支持load-pin overlap。,https://github.com/hpcaitech/ColossalAI/issues/6177
ColossalAI,这是一个用户提出需求的 issue，主要涉及到 ColossalAI 中的 ProcessGroupMesh 功能。这个问题是关于使用 ProcessGroupMesh 初始化分布式环境时无法在多个节点上工作的。,https://github.com/hpcaitech/ColossalAI/issues/6176
ColossalAI,该issue属于一个贡献者在发布PR前的更新操作验证类型，主要涉及ColossalAI的版本更新。用户通过该issue提出疑问并寻求团队的指导。,https://github.com/hpcaitech/ColossalAI/issues/6174
ColossalAI,这个issue是一个功能需求类型，涉及的主要对象是ColossalAI中的checkpointio模块。导致这个问题的原因是需要支持非阻塞的pin加载。,https://github.com/hpcaitech/ColossalAI/issues/6172
ColossalAI,这是一个用户提出需求的 issue，主要关注支持大参数模型，如Llama405B，未来版本是否会提供支持。,https://github.com/hpcaitech/ColossalAI/issues/6171
ColossalAI,这是一个需求提出的issue，主要涉及ColossalAI中llama3-405B模型的预训练集成问题，用户想了解如何在llama3预训练脚本中继续基于llama405B进行预训练。,https://github.com/hpcaitech/ColossalAI/issues/6170
ColossalAI,这个issue类型为需求报告，涉及ColossalAI下的Sora项目。由于未按规范进行提交导致标签、格式等问题。,https://github.com/hpcaitech/ColossalAI/issues/6166
ColossalAI,该issue是一个贡献者提交文档更新的类型，主要对象是ColossalAI项目的PR流程，问题产生可能是由于缺乏标准化的PR流程导致。,https://github.com/hpcaitech/ColossalAI/issues/6164
ColossalAI,这个issue是一个提出需求的类型，主要涉及的对象是ColossalAI项目中的数据类型输入（input datatype），由于目前无法进行allgather操作，需要增强该功能。,https://github.com/hpcaitech/ColossalAI/issues/6162
ColossalAI,这是一个用户提出需求的issue，涉及的主要对象是在ColossalAI中添加从openMind_Hub下载模型的功能。该问题出现的原因可能是用户希望能够更方便地获取模型而提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/6158
ColossalAI,这是一个需求类型的issue，主要涉及的对象是ColossalAI下的Shardformer模块，用户提出更新Shardformer以适配transformers的最新版本可能是因为他们需要使用新版本的功能或者解决旧版本的问题。,https://github.com/hpcaitech/ColossalAI/issues/6157
ColossalAI,该issue属于用户提出需求类型，主要涉及的对象为ColossalAI的[checkpointio]模块，用户希望支持debug日志记录。造成这种需求的原因可能是用户需要在调试过程中获得更多的信息以便定位问题。,https://github.com/hpcaitech/ColossalAI/issues/6153
ColossalAI,这个issue是一个需求问题，主要涉及到ColossalAI中的checkpointio模块，由于缺乏对所有模型的asyncio支持，需要进行相应的改进。,https://github.com/hpcaitech/ColossalAI/issues/6152
ColossalAI,该issue类型为文档更新，主要涉及ColossalAI项目中的readme文件，可能由于需要更新项目文档内容而创建。,https://github.com/hpcaitech/ColossalAI/issues/6148
ColossalAI,该issue是一个贡献者在ColossalAI中的一个优化问题，主要涉及优化Adam加载，并包含了PR前的详细检查清单。问题可能由于Adam加载过程中的问题导致的bug或者优化需求而发起。,https://github.com/hpcaitech/ColossalAI/issues/6146
ColossalAI,这是一个需求类型的issue，涉及的主要对象是ColossalAI下的一个插件，用户提出了需要为其他插件添加异步checkpoint API的需求。,https://github.com/hpcaitech/ColossalAI/issues/6145
ColossalAI,这个issue是用户提交的功能需求，主要涉及到ColossalAI中的checkpointio，用户请求支持asyncio用于3d。因此由于用户对asyncio在3d方面的需求，导致了这个issue的提出。,https://github.com/hpcaitech/ColossalAI/issues/6144
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中的checkpointio模块，要求支持异步保存功能。这可能是由于目前的保存方式无法满足用户对异步保存的需求。,https://github.com/hpcaitech/ColossalAI/issues/6141
ColossalAI,这是一个用户提出需求的 issue，涉及的主要对象是添加异步的checkpoint API 给其他插件。,https://github.com/hpcaitech/ColossalAI/issues/6140
ColossalAI,这是一个用户提出需求的类型。该问题单涉及的主要对象是支持将LoRA/QLoRA集成到Gemini或TorchFSDP插件中。这个需求的提出可能是因为用户希望在ColossalAI中能够支持LoRA/QLoRA技术，以便更好地应用于相关项目中。,https://github.com/hpcaitech/ColossalAI/issues/6138
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI框架中的async io支持。由于缺乏async io功能，用户提出希望添加异步IO支持的需求。,https://github.com/hpcaitech/ColossalAI/issues/6137
ColossalAI,"这是一个用户提出需求的issue，该问题单涉及的主要对象是ColossalAI的checkpoint模块。这个需求是为了添加异步ckpt api，用于所有插件，比如save_model, save_optimizer。",https://github.com/hpcaitech/ColossalAI/issues/6136
ColossalAI,"这个issue是一个用户提出的需求，主要涉及的对象是ColossalAI的命令行工具（CLI），用户希望支持""run as module""选项。导致用户提出这个需求的原因可能是希望更灵活地运行ColossalAI作为模块而不是独立应用。",https://github.com/hpcaitech/ColossalAI/issues/6135
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中checkpointio模块，用户希望增加支持异步模型保存的功能。,https://github.com/hpcaitech/ColossalAI/issues/6131
ColossalAI,这个issue是一个功能改进提案，主要对象是ColossalAI中的checkpointio模块。由于异步IO支持的缺失，导致用户寻求在PR中添加async io功能。,https://github.com/hpcaitech/ColossalAI/issues/6130
ColossalAI,该issue是一个需求报告，主要涉及ColossalAI中checkpointio模块的异步IO支持需求。由于当前的同步IO机制可能导致性能瓶颈或者无法满足异步操作的需求，因此用户提出希望修改该模块以支持异步IO。,https://github.com/hpcaitech/ColossalAI/issues/6129
ColossalAI,这个issue属于PR请求，涉及ColossalAI中的utils模块，主要是同步save_state_dict功能。原因可能是为了保持代码的一致性和协作。,https://github.com/hpcaitech/ColossalAI/issues/6128
ColossalAI,这是一个功能需求类型的issue，涉及ColossalAI下的插件，主要问题是添加异步的模型和优化器保存接口。,https://github.com/hpcaitech/ColossalAI/issues/6126
ColossalAI,这是一个功能优化的Issue，主要涉及到ColossalAI的代码维护和规范性问题。原因是存在多余的函数导致维护困难，因此需要进行清理。,https://github.com/hpcaitech/ColossalAI/issues/6125
ColossalAI,这个issue是一个feature请求，请求支持Gemma2Model用于张量并行训练，出现的bug是无法成功运行llama模型。,https://github.com/hpcaitech/ColossalAI/issues/6122
ColossalAI,这是一个功能需求的issue，主要涉及Gemini模型的Tensor Parallelism训练支持，解决了运行llama模型时遇到的小bug。,https://github.com/hpcaitech/ColossalAI/issues/6121
ColossalAI,这个issue是一个用户需求的提出，主要涉及的对象是ColossalAI中的gemm2模型，用户希望支持gemm2在Tensor Parallelism中的应用，由于还未实现相应的Auto policy处理，导致了NotImplementedError异常。,https://github.com/hpcaitech/ColossalAI/issues/6120
ColossalAI,这是一个特性优化的issue，主要涉及到ColossalAI中Coati模块的prompt优化，用户在该PR中对prompt进行了细化，旨在提高推理效果。,https://github.com/hpcaitech/ColossalAI/issues/6117
ColossalAI,这个issue属于一个文档更新的类型，涉及的主要对象是ColossalAI的Coati README，可能是由于需要更新文档内容或者标准化操作而引起的。,https://github.com/hpcaitech/ColossalAI/issues/6116
ColossalAI,这个issue是一个特性需求。主要涉及插件的开发，其中用户希望支持获取梯度范数信息。,https://github.com/hpcaitech/ColossalAI/issues/6115
ColossalAI,这个issue是一个用户提出需求的类型，主要涉及ColossalAI下的Zerobubble支持LinearWithAsyncCommunication for sharderformer policy。由于用户需要在sharderformer llama & mixtral中支持LinearWithAsyncCommunication，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/6114
ColossalAI,这个issue类型是更新通知，主要涉及的对象是ColossalAI项目使用的第三方工具mirrorsclangformat，由于更新从v19.1.2到v19.1.5导致。,https://github.com/hpcaitech/ColossalAI/issues/6113
ColossalAI,这是一个关于GitHub上ColossalAI中某个功能模块[Zerobubble]合并主分支的issue。,https://github.com/hpcaitech/ColossalAI/issues/6107
ColossalAI,这个issue类型是用户提出需求，主要对象是PR的创建和审查流程。用户希望在PR创建和审查过程中增加一些具体的检查项，以减少维护者的工作负担。,https://github.com/hpcaitech/ColossalAI/issues/6104
ColossalAI,这是一个用户提出需求的issue，涉及ColossalAI的Windows用户无法使用部分AI工具的问题。,https://github.com/hpcaitech/ColossalAI/issues/6103
ColossalAI,这是一个需求和贡献文档类型的issue，主要涉及ColossalAI的贡献和问题处理流程，旨在规范和指导贡献者在提交PR前需要做的检查和准备工作。,https://github.com/hpcaitech/ColossalAI/issues/6100
ColossalAI,这个issue类型是功能改进，涉及的主要对象是ColossalAI的PR流程。由于缺少标准化的流程指导，导致了PR过程中可能出现的问题及需进一步改进的需求。,https://github.com/hpcaitech/ColossalAI/issues/6099
ColossalAI,该issue属于功能需求类型，主要涉及实现自我优化的MCTS算法。原因是需要为ColossalAI添加self-refined MCTS功能。,https://github.com/hpcaitech/ColossalAI/issues/6098
ColossalAI,这个issue类型是技术改进的PR提交，涉及ColossalAI中的torch API升级和移除旧版导入，并由于维护代码规范和保持项目健康而提出。,https://github.com/hpcaitech/ColossalAI/issues/6093
ColossalAI,这个issue类型是一个功能更新，主要对象是ColossalAI的代码库。这个问题由于添加了新的safetensors utils功能模块而提出。,https://github.com/hpcaitech/ColossalAI/issues/6088
ColossalAI,这个issue类型是文档更新，主要涉及更新ColossalAI的README.md文件，可能是由于新增了HPCAI.COM活动而引发的。,https://github.com/hpcaitech/ColossalAI/issues/6087
ColossalAI,这个issue是一个优化性质的PR，涉及到ColossalAI中的shardformer模块，主要目的是优化序列并行性能问题。,https://github.com/hpcaitech/ColossalAI/issues/6086
ColossalAI,这是一个关于软件代码改进的issue，主要涉及文档注释和方法参数命名的改进。原因可能是为了提高代码可读性和维护性。,https://github.com/hpcaitech/ColossalAI/issues/6085
ColossalAI,该issue属于功能增强类型，主要涉及ColossalAI的mixtral benchmark，提出了对支持zbv的需求。由于缺少对zbv的支持，导致mixtral benchmark存在功能上的限制。,https://github.com/hpcaitech/ColossalAI/issues/6083
ColossalAI,这是一个feature请求类型的issue，该问题单涉及的主要对象是支持zbv在mixtral基准测试中。由于需要添加对Zerobubble pipeline的支持，导致这个issue被创建。,https://github.com/hpcaitech/ColossalAI/issues/6082
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的zero模块，由于缺乏zero支持导致的问题。,https://github.com/hpcaitech/ColossalAI/issues/6080
ColossalAI,这是一个软件更新类型的issue，涉及的主要对象是ColossalAI的代码库。原因是通过pre-commit.ci进行自动更新导致相关pre-commit插件版本升级。,https://github.com/hpcaitech/ColossalAI/issues/6078
ColossalAI,这个issue是一个特性需求，涉及ColossalAI项目中的ZeroBubble插件，用户提出了支持MoeHybridplugin的需求。,https://github.com/hpcaitech/ColossalAI/issues/6077
ColossalAI,这是一个关于用户需求的问题，用户希望实现FasterMoE的影子专家功能。,https://github.com/hpcaitech/ColossalAI/issues/6076
ColossalAI,这个issue类型是提出需求，主要涉及ColossalAI的开发流程以及贡献者要求，并且由于缺少相应的准备工作（例如创建问题、安装precommit等），要求产生的PR满足一系列的准备条件。,https://github.com/hpcaitech/ColossalAI/issues/6075
ColossalAI,这个issue是一份需求报告，涉及ColossalAI的贡献指引，旨在添加资金动态消息内容。造成这一问题的原因可能是为了提高项目的透明度和可追踪性。,https://github.com/hpcaitech/ColossalAI/issues/6072
ColossalAI,该issue是一个特性请求（feature request），主要涉及ColossalAI的PR发布流程和质量管理。原因可能是为了提高代码质量和效率。,https://github.com/hpcaitech/ColossalAI/issues/6068
ColossalAI,这个issue类型是功能需求，主要涉及ColossalAI中的Zero Bubble功能，用户在此提出了关于支持shardformer输入的需求。,https://github.com/hpcaitech/ColossalAI/issues/6065
ColossalAI,这个issue属于代码贡献请求，涉及ColossalAI的文档更新，由于PR作者未按要求检查标准提交导致。,https://github.com/hpcaitech/ColossalAI/issues/6062
ColossalAI,这是一个功能增强的issue，主要涉及支持vllm推理，可能由于缺乏这一功能导致需要相关支持。,https://github.com/hpcaitech/ColossalAI/issues/6056
ColossalAI,这是一个关于文档更新的issue，主要涉及到ColossalAI中的shardformer。由于创建者要求PR前需要遵循一系列准则和流程，可能是为了提高贡献者的工作效率和质量。,https://github.com/hpcaitech/ColossalAI/issues/6055
ColossalAI,这个issue类型是一则关于提交PR的需求，主要涉及的对象是ColossalAI项目中的文档/代码。由于指定的PR标准格式和流程未完全符合要求，导致用户在提交PR时遇到了问题。,https://github.com/hpcaitech/ColossalAI/issues/6054
ColossalAI,这个issue是一个需求提出，主要涉及DPO训练支持，由于缺少相应支持导致该需求提出。,https://github.com/hpcaitech/ColossalAI/issues/6052
ColossalAI,这是一个文档更新的issue，涉及ColossalAI的FP8训练和沟通文档。这个问题可能是由于需要更新项目文档和改进协作流程而产生的。,https://github.com/hpcaitech/ColossalAI/issues/6050
ColossalAI,该issue属于文档需求类型，主要对象是关于FP8训练和通信的文档。,https://github.com/hpcaitech/ColossalAI/issues/6049
ColossalAI,这是一个用户提出需求的类型为Feature请求，主要涉及的对象是ColossalAI项目中是否能集成Liger-Kernel。用户希望能够将Liger Kernel集成到ColossalAI中，以提高多GPU训练吞吐量并降低内存使用率。,https://github.com/hpcaitech/ColossalAI/issues/6047
ColossalAI,这个issue是一个技术改进的PR，涉及的主要对象是ColossalAI的兼容性测试。由于Triton缓存目前的问题导致兼容性测试失败，需要移除该缓存来解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/6044
ColossalAI,这个issue类型是关于代码贡献的请求，主要涉及ColossalAI框架的版本更新。这个问题的提出可能是为了确保代码贡献符合标准流程和规范。,https://github.com/hpcaitech/ColossalAI/issues/6041
ColossalAI,这是一个用户提出需求的issue，主要涉及支持Zerobubble pipeline功能，请求在ColossalAI中实现。由于缺乏Zerobubble pipeline的支持，用户无法使用相关功能，因此提出这个需求。,https://github.com/hpcaitech/ColossalAI/issues/6037
ColossalAI,这个issue属于请求合并，涉及的主要对象是支持ZeroBubble Pipeline。原因导致的bug或问题可能是需要对ZeroBubble Pipeline进行功能增强。,https://github.com/hpcaitech/ColossalAI/issues/6034
ColossalAI,这个issue是一个feature更新，主要涉及ColossalAI的代码重构和更新。,https://github.com/hpcaitech/ColossalAI/issues/6030
ColossalAI,该issue是一个用户提交的代码贡献请求，主要涉及的对象是train_dpo.py文件，用户可能由于遵循特定的贡献指南和流程而创建该PR。,https://github.com/hpcaitech/ColossalAI/issues/6029
ColossalAI,这个issue类型为用户提出需求，主要对象是如何同时训练两个模型，原因可能是官方文档中只提供了单个模型训练的例子，没有涉及同时训练多个模型的方法。,https://github.com/hpcaitech/ColossalAI/issues/6028
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI库是否支持qwen。由于用户可能需要qwen来完成特定任务，但ColossalAI当前可能尚未提供相关支持，所以提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/6027
ColossalAI,这是一个合并请求（Merge Request），涉及的主要对象是将特性分支（feature/fp8_comm）合并到ColossalAI的主分支上。问题产生的原因可能是为了将功能修改或新增内容合并到主代码库中。,https://github.com/hpcaitech/ColossalAI/issues/6024
ColossalAI,这是一个类型为合并请求的issue，涉及将feature/fp8_comm合并到ColossalAI的主分支。由于原分支的内容需要合并到主分支，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/6016
ColossalAI,这是一个建议性issue，主要涉及的对象是源代码中使用`np.array`导致不必要数据副本的问题。,https://github.com/hpcaitech/ColossalAI/issues/6015
ColossalAI,这是一个合并请求，涉及合并feature/fp8_comm到ColossalAI的主分支，其类型是功能合并。,https://github.com/hpcaitech/ColossalAI/issues/6013
ColossalAI,这是用户提出的一个功能需求类型的issue，主要涉及支持ColossalAI中的FP8通信和训练，涉及的主要对象是相关插件和模型训练功能。,https://github.com/hpcaitech/ColossalAI/issues/6012
ColossalAI,该issue为一个贡献者提交的关于优化插件中日志记录使用的问题，涉及主要对象为ColossalAI库。造成此问题的原因可能是为了提高代码质量和可追溯性。,https://github.com/hpcaitech/ColossalAI/issues/6011
ColossalAI,该issue是一个feature请求，主要涉及ColossalAI的插件使用dist logger，由于缺少此功能导致需要更好的调试和跟踪日志。,https://github.com/hpcaitech/ColossalAI/issues/6010
ColossalAI,"这是一个功能需求的issue，主要涉及的对象是""MoeHybridParallelPlugin""插件，用户提出需要添加一个""use_fp8""选项。",https://github.com/hpcaitech/ColossalAI/issues/6009
ColossalAI,该issue类型为Pull Request请求，主要涉及ColossalChat中Support PP in SFT的功能增强。由于未安装precommit导致的PR创建步骤问题。,https://github.com/hpcaitech/ColossalAI/issues/6007
ColossalAI,这个issue是一个用户提出需求的问题单，主要对象是ColossalAI的reduce-scatter测试，由于引入了fp8数据类型导致的需要更新测试。,https://github.com/hpcaitech/ColossalAI/issues/6002
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalChat，用户添加了PP插件并修复了一些小问题。,https://github.com/hpcaitech/ColossalAI/issues/6001
ColossalAI,这个issue属于功能请求类型，主要涉及ColossalAI的异步通信支持。由于当前不支持异步通信，用户提出需要在ColossalAI中添加对异步通信的支持。,https://github.com/hpcaitech/ColossalAI/issues/5999
ColossalAI,这个issue是一个优化代码的PR，涉及到ColossalAI中的fp8线性部分，原因可能是为了提高性能。,https://github.com/hpcaitech/ColossalAI/issues/5998
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中支持异步FP8通信的问题。原因是系统需要增加对异步FP8通信的支持。,https://github.com/hpcaitech/ColossalAI/issues/5997
ColossalAI,这是一个需求类型的issue，主要涉及的对象是ColossalAI中的pre-commit.ci工具。由于psf/blackprecommitmirror更新导致的版本变更，用户发布了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/5995
ColossalAI,这个issue是一个功能需求（Feature Request），主要涉及ColossalChat的Hybrid PP Training支持。原因是要为SFT训练添加PP。,https://github.com/hpcaitech/ColossalAI/issues/5994
ColossalAI,这个issue是一个特性请求，主要涉及优化FP8通信操作符，来源于对ColossalAI框架的贡献。,https://github.com/hpcaitech/ColossalAI/issues/5984
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中如何跳过自定义节点生成策略的问题，由于无法处理自定义操作符导致症状为无法成功处理。,https://github.com/hpcaitech/ColossalAI/issues/5983
ColossalAI,此issue是一个功能需求提议，主要涉及ColossalAI中的支持混合并行插件和Shardformer的问题，用户寻求对于支持fp8通信的改进。,https://github.com/hpcaitech/ColossalAI/issues/5982
ColossalAI,这个issue属于用户提出需求类型，主要涉及的对象是PR创建检查清单和请求审查前的检查清单，其中用户提到必须要链接到issue以及添加了相关的标签作为PR的区分。,https://github.com/hpcaitech/ColossalAI/issues/5981
ColossalAI,这个issue是一个PR请求，涉及使用`torch compile`来解决问题，目的是在ColossalAI中使用torch >= 2.3.0。,https://github.com/hpcaitech/ColossalAI/issues/5979
ColossalAI,该issue类型是一个功能请求，针对Gemini插件的支持。用户提出了相关的需求并准备对ColossalAI做出贡献。,https://github.com/hpcaitech/ColossalAI/issues/5978
ColossalAI,该issue类型是功能需求，并且涉及Moe对fp8通信的支持。导致这个问题的原因可能是当前系统尚未实现这一功能。,https://github.com/hpcaitech/ColossalAI/issues/5977
ColossalAI,该issue类型为功能开发，涉及到ColossalAI下的fp8支持amp的功能。由于缺少函数/方法的docstrings，用户希望添加此功能至Hybrid Parallel Plugin。,https://github.com/hpcaitech/ColossalAI/issues/5975
ColossalAI,这是一个特性请求，要求支持AMP：FP8（O1）+BF16（O2），使用当前的缩放。,https://github.com/hpcaitech/ColossalAI/issues/5974
ColossalAI,这是一个功能需求的issue，涉及ColossalAI库中的支持混合并行插件的功能。原因是用户希望实现混合并行插件和Shardformer支持fp8通信，包括zero、dp、pp、tp。,https://github.com/hpcaitech/ColossalAI/issues/5973
ColossalAI,这个issue是一个功能需求类型的问题，主要涉及的对象是ColossalAI框架中的fp8（fast parallel）模块，用户提出了支持混合并行插件的需求。,https://github.com/hpcaitech/ColossalAI/issues/5972
ColossalAI,该issue类型为文档编辑请求，涉及ColossalAI的文档更新，希望明确“launch port”的内容。由于缺乏明确的文档指引，导致需要对启动端口进行更清晰的说明。,https://github.com/hpcaitech/ColossalAI/issues/5970
ColossalAI,该issue类型是一个PR请求，针对ColossalAI中的fp8 benchmark功能，主要对象是添加llama fp8 benchmark with transformer_engine；由于作者需要将此功能添加到项目中，因此提出了这个PR。,https://github.com/hpcaitech/ColossalAI/issues/5968
ColossalAI,这个issue属于用户提交需求类型的问题单，涉及的主要对象是ColossalAI中的线性层（linear）。用户提出了在添加fp8线性层时需要遵循的PR创建和审查步骤。,https://github.com/hpcaitech/ColossalAI/issues/5967
ColossalAI,这个issue类型是用户提出需求，该问题单涉及的主要对象是关于distributed training startup instructions中创建hostfile文件时缺乏说明，导致用户不清楚每行每列的参数配置。,https://github.com/hpcaitech/ColossalAI/issues/5965
ColossalAI,这是一个用户提出需求的类型，主要对象是Lora training for Llama3。由于缺乏Lora相关参数，用户无法在脚本中找到对应内容。,https://github.com/hpcaitech/ColossalAI/issues/5964
ColossalAI,这个issue是一个feature需求，主要涉及到ColossalAI的logging功能改进，提出了支持overall loss和更新KTO logging的需求。根据内容可推测用户希望在PR中添加对overall loss的支持以及更新KTO logging相关内容。,https://github.com/hpcaitech/ColossalAI/issues/5962
ColossalAI,这个issue类型是需求提出，该问题单涉及的主要对象是ColossalAI库中的fp8实现，原因可能是用户需要fp8支持低级别的零值。,https://github.com/hpcaitech/ColossalAI/issues/5960
ColossalAI,这是一个特性需求的Issue，主要涉及ColossalAI中的Split cross-entropy计算，由于需要避免在交叉熵计算之前收集隐藏状态以节省通信和计算，同时支持GPT的SP，并合并PP模型fwd和GPT2以及Llama的model fwd逻辑，以方便未来的维护。,https://github.com/hpcaitech/ColossalAI/issues/5959
ColossalAI,这个issue为文档更新类型，涉及的主要对象是ColossalChat的README.md文件。由于缺乏标准的格式和必要的步骤，用户提出了更新README.md以满足标准的要求。,https://github.com/hpcaitech/ColossalAI/issues/5958
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是lora插件。由于当前lora不支持混合并行插件，用户提出了需求使lora支持混合并行插件。,https://github.com/hpcaitech/ColossalAI/issues/5956
ColossalAI,这是一个需求类型的issue，主要涉及到对MoE（Mixture of Experts）的支持。原因可能是用户希望在ColossalAI中实现MoE模型。,https://github.com/hpcaitech/ColossalAI/issues/5954
ColossalAI,这个issue是一个功能优化请求，涉及主要对象为支持FP8格式的all2all操作。原因可能是为了提高ColossalAI在特定场景下的性能表现。,https://github.com/hpcaitech/ColossalAI/issues/5953
ColossalAI,这个issue类型为需求提交，主要涉及到ColossalAI的版本更新。由于没有按照标准格式和要求进行更新，缺少必要内容和步骤，导致需要PR的修订和完善。,https://github.com/hpcaitech/ColossalAI/issues/5952
ColossalAI,这是一个特性更新的issue，主要涉及ColossalAI中数据加载的优化和bug修复。,https://github.com/hpcaitech/ColossalAI/issues/5950
ColossalAI,这个issue是一个需求报告，主要涉及ColossalAI的shardformer模块中的attn mask问题。由于缺少正确的条件限制，可能导致attn mask的错误，需要进行修复。,https://github.com/hpcaitech/ColossalAI/issues/5945
ColossalAI,这是一个用户提出需求的类型，主要涉及Gemini中的参数allgather，请求支持FP8压缩通信。,https://github.com/hpcaitech/ColossalAI/issues/5943
ColossalAI,这是一个特性需求的issue，主要涉及Colossal-Eval的分布式加载支持。由于缺乏此功能，用户希望实现分布式加载，以提高系统的性能和效率。,https://github.com/hpcaitech/ColossalAI/issues/5942
ColossalAI,这是一个功能需求类型的issue，涉及主要对象是ColossalAI模型训练过程中是否保存每个epoch的模型checkpoint。由于保存每个epoch可能消耗大量时间和磁盘空间，用户提出需求添加一个参数供用户选择是否保存每个epoch的模型checkpoint。,https://github.com/hpcaitech/ColossalAI/issues/5941
ColossalAI,该issue类型属于用户提出需求，主要涉及的对象是benchmarking一个LLM。,https://github.com/hpcaitech/ColossalAI/issues/5939
ColossalAI,这是一个功能（Feature）请求类的issue，主要涉及到ColossalAI中的llama shardformer，用户请求在pretraining任务中添加FP8支持以实现损失平衡。造成该需求的原因可能是为了提高模型的性能和功能。,https://github.com/hpcaitech/ColossalAI/issues/5938
ColossalAI,这是一个功能需求类型的issue，主要涉及到ColossalAI中的fp8计算支持，用户提出了支持all-gather flat tensor的需求。,https://github.com/hpcaitech/ColossalAI/issues/5932
ColossalAI,该issue类型是用户提出需求，主要涉及的对象是支持FP8通信的功能实现。由于需要实现FP8压缩通信钩子用于梯度全局归约、梯度散射归约和参数全聚类的功能，用户希望对torch DDP和FSDP的功能进行扩展，以支持FP8通信。,https://github.com/hpcaitech/ColossalAI/issues/5928
ColossalAI,这个issue类型是用户提出需求，主要对象是ColossalAI中的FSDP模块，用户请求支持FP8压缩通信以实现gradient communication和parameter communication。,https://github.com/hpcaitech/ColossalAI/issues/5927
ColossalAI,这个issue是用户提出的需求，主要涉及到ColossalAI下的DDP模块，请求支持FP8压缩通信来进行梯度通信。,https://github.com/hpcaitech/ColossalAI/issues/5926
ColossalAI,这个issue属于需求类型，主要涉及ColossalAI中OPT和GPT示例中添加lazy init功能。由于未提供具体原因，推测可能是为了改善模型的初始化效率或性能。,https://github.com/hpcaitech/ColossalAI/issues/5924
ColossalAI,这个issue是一个特性请求，涉及主要对象是ColossalAI的插件支持hybrid parallel的all-gather overlap功能。这个请求由于需要增加这个功能的支持，以提升协同并行计算性能。,https://github.com/hpcaitech/ColossalAI/issues/5919
ColossalAI,该issue是一个功能需求类型的任务，主要涉及到MoE Ulysses的支持。由于需要添加MoE Ulysses的支持，故涉及到提供相对应的功能和文档。,https://github.com/hpcaitech/ColossalAI/issues/5918
ColossalAI,这是一个用户提交的需求类型的issue，涉及ColossalAI项目的版本更新工作。由于程序版本未及时更新导致出现了相关问题。,https://github.com/hpcaitech/ColossalAI/issues/5917
ColossalAI,这是一个用户提出需求的类型。主要对象是ColossalAI中的Shardformer（SP和TP）。由于带宽受限的情况下，将通信中的数据类型更改为fp8是有用的，因此用户希望支持Shardformer中的fp8通信。,https://github.com/hpcaitech/ColossalAI/issues/5916
ColossalAI,该issue是一个特性请求，请求支持FP8通信在pipeline并行中的应用。,https://github.com/hpcaitech/ColossalAI/issues/5915
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI下的ShardFormer模块，用户希望添加对fp8通信的支持。,https://github.com/hpcaitech/ColossalAI/issues/5914
ColossalAI,这是一个用户提出需求的类型，主要涉及的对象是ShardFormer模块。由于带宽受限的场景，用户提出需求支持在通信中使用fp8数据类型。,https://github.com/hpcaitech/ColossalAI/issues/5913
ColossalAI,该issue类型为功能需求，主要涉及ColossalAI中支持自动分布式数据加载器的问题，用户寻求帮助优化数据加载过程。,https://github.com/hpcaitech/ColossalAI/issues/5911
ColossalAI,该issue类型为用户提出需求，主要涉及LowLevelZero Plugin是否支持Lora，由于代码令人困惑导致用户提出疑问和希望自行解决一部分工作。,https://github.com/hpcaitech/ColossalAI/issues/5908
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中的Zigzag Ring attention的实现。产生这个issue的原因是要支持Llama，并对批量数据进行loadbalancing处理。,https://github.com/hpcaitech/ColossalAI/issues/5905
ColossalAI,这个issue是一个feature请求，提出了支持Torch 2.2/2.3的问题。,https://github.com/hpcaitech/ColossalAI/issues/5902
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及的对象是StableDiffusion3算法模型。用户关注加速训练这一特性，可能是因为希望提高训练效率或者加快模型收敛速度。,https://github.com/hpcaitech/ColossalAI/issues/5900
ColossalAI,这是一个特性需求的issue，主要涉及到ColossalAI项目中的FP8通信模块，提出了关于GPT2 finetune准确率提高的问题，可能是由于FP8引入的噪音作为一个好的正则化项。,https://github.com/hpcaitech/ColossalAI/issues/5899
ColossalAI,这个issue是一个需求提出类型的问题，主要涉及ColossalAI中的ShardFormer模块，提出要为CommandR、Qwen2和ChatGLM添加Ulysses Sequence Parallelism支持。由于缺乏此功能，用户请求增加该功能来支持相应模块。,https://github.com/hpcaitech/ColossalAI/issues/5897
ColossalAI,"这个issue是一个用户提出需求的类型，主要对象涉及ColossalAI下的ShardFormer模块，用户提出了需要为Command-R, Qwen2和ChatGLM添加Ulysses Sequence Parallelism支持的需求。",https://github.com/hpcaitech/ColossalAI/issues/5896
ColossalAI,这个issue属于功能新特性提交，涉及的主要对象是ColossalAI库的开发与贡献者。由于需要支持Distrifusion加速，添加基准测试和FID测试。,https://github.com/hpcaitech/ColossalAI/issues/5895
ColossalAI,这是一个用户提交需求的issue，主要对象是ColossalAI库的支持Torch 2.3版本的问题。,https://github.com/hpcaitech/ColossalAI/issues/5893
ColossalAI,该issue类型为提出需求，主要涉及ColossalAI中的特性开发，由于缺乏标准化的PR创建流程和文档规范，导致需要澄清并规范相应操作。,https://github.com/hpcaitech/ColossalAI/issues/5889
ColossalAI,该issue类型为技术优化，涉及主要对象为Deepseek代码库中的冗余代码；由于存在冗余代码导致了PR创建前的必要检查项未能满足。,https://github.com/hpcaitech/ColossalAI/issues/5888
ColossalAI,这个issue属于提出警示和建议，主要涉及机场运营商和用户，原因是机场跑路现象导致用户损失和隐私泄露的风险问题。,https://github.com/hpcaitech/ColossalAI/issues/5887
ColossalAI,这是一个功能需求的issue，主要涉及到PyTorch的FP8 all-reduce功能，用户提出了使用all-to-all和all-gather来实现该功能。,https://github.com/hpcaitech/ColossalAI/issues/5886
ColossalAI,这个issue是一个feature需求，主要对象是支持FP8通信在pipeline并行ism中。由于需要实现perchannel scaling (in PyTorch) for FP8 quantization和支持PyTorch native FP8 formats，所以提出了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/5885
ColossalAI,这个issue类型为功能需求，主要涉及ColossalAI的模型文件移除以及使用自动配置功能的更改。触发该需求可能是为了简化和优化项目文件结构和配置管理。,https://github.com/hpcaitech/ColossalAI/issues/5884
ColossalAI,这是一个关于软件更新的issue，涉及的主要对象是pre-commit.ci。 由于软件版本升级，用户提出了更新信息。,https://github.com/hpcaitech/ColossalAI/issues/5878
ColossalAI,该issue类型是文档更新，涉及主要对象是ColossalAI的AzureML上线，用户提出了关于如何在AzureML上启动ColossalAI的问题。,https://github.com/hpcaitech/ColossalAI/issues/5877
ColossalAI,该issue类型为用户提出需求，主要对象是实现PyTorch中的per-channel FP8量化，因为用户希望支持PyTorch原生FP8格式并实现per-channel缩放。,https://github.com/hpcaitech/ColossalAI/issues/5873
ColossalAI,这个issue类型是feature需求，主要对象是DeepseekMoE支持，用户提出了支持Deepseek MoE模型的需求。,https://github.com/hpcaitech/ColossalAI/issues/5871
ColossalAI,该issue类型为功能特性支持和单元测试，涉及ColossalAI的deepseek功能。这个问题是由于开发人员在创建PR之前需要按照一定的检查清单进行操作，包括创建相应的issue、添加标签、安装precommit等。,https://github.com/hpcaitech/ColossalAI/issues/5870
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI对torch 2.2的兼容性支持问题，导致用户无法使用感叹号Image功能。,https://github.com/hpcaitech/ColossalAI/issues/5869
ColossalAI,这个issue是关于功能特性的修改，主要涉及ColossalAI下的Llama，并且由于PP + SP被错误地禁用而引发了问题。,https://github.com/hpcaitech/ColossalAI/issues/5868
ColossalAI,该issue类型是用户提出需求，主要对象是ColossalAI项目中的模型Llama，该需求是针对当前大多数模型不支持SP和PP同时存在的问题，用户希望添加对此的支持。,https://github.com/hpcaitech/ColossalAI/issues/5866
ColossalAI,这个issue是一个功能修改类型的问题，主要涉及到ColossalAI项目中的代码优化和修改，原因可能是存在冗余代码或者需要改进mesh实现。,https://github.com/hpcaitech/ColossalAI/issues/5863
ColossalAI,这个issue类型是项目贡献者提出需求，涉及主要对象为代码库ColossalAI中的成员初始化操作。原因可能是为了简化代码结构或避免重复初始化的问题。,https://github.com/hpcaitech/ColossalAI/issues/5862
ColossalAI,该issue属于特性请求类型，主要涉及ColossalAI中的开发流程优化，需要开发者在创建PR之前完成一系列标准化操作，以增强代码质量和项目可维护性。,https://github.com/hpcaitech/ColossalAI/issues/5858
ColossalAI,这是一个功能需求的issue，涉及主要对象是加入对Command-R、Qwen2和ChatGLM的Ulysses Sequence Parallelism支持，解决了该功能未被支持的问题。,https://github.com/hpcaitech/ColossalAI/issues/5854
ColossalAI,"该issue是用户提出需求类型，主要涉及ColossalAI下的三个模型（CommandR, Qwen2和ChatGLM），用户希望添加Ulysses Sequence Parallelism支持。",https://github.com/hpcaitech/ColossalAI/issues/5853
ColossalAI,这个issue类型是更新文档和修复优化表，主要涉及ColossalAI的Llama和sp兼容性，以及修复了分布式优化表的问题。可能由于之前的文档不准确或者优化表存在错误，导致需要进行更新和修复。,https://github.com/hpcaitech/ColossalAI/issues/5852
ColossalAI,"该issue是一个用户需求类型的问题，主要涉及在ColossalAI中增加GPU云平台操作预览功能。由于用户希望能在云端进行GPU操作的预览，因此提出了这个需求。
",https://github.com/hpcaitech/ColossalAI/issues/5851
ColossalAI,这个issue属于用户提出需求类型，主要涉及的对象是ColossalAI项目中的Rlhf模块，由于缺少SimPO支持而需要进行代码改进。,https://github.com/hpcaitech/ColossalAI/issues/5850
ColossalAI,这个issue是一个文档更新的类型，主要涉及ColossalAI中一个模型权重链接的修复。这个问题可能是由于之前的链接错误导致权重下载不成功，需要修复链接以解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/5848
ColossalAI,这个issue是一个文档改进的类型，涉及的主要对象是PR创建流程。由于缺少标准化的PR创建流程导致需要在issue中列出的检查事项未被完整执行。,https://github.com/hpcaitech/ColossalAI/issues/5846
ColossalAI,这是一个需求类型的issue，主要涉及更新Qwen2模型到4.39.3版本。,https://github.com/hpcaitech/ColossalAI/issues/5844
ColossalAI,这是一个用户提交需求的issue，主要对象为ColossalAI中的API。用户提交了修改API的请求，标明了需要满足的检查列表、前提条件以及所需的审核步骤。,https://github.com/hpcaitech/ColossalAI/issues/5843
ColossalAI,这个issue是更新llama模型的PR，类型为需求提出。主要涉及的对象是llama模型，用户提出更新到4.39.3版的transformers。,https://github.com/hpcaitech/ColossalAI/issues/5842
ColossalAI,这个issue是关于用户需求的，主要对象是ColossalAI中的Shardformer，用户希望支持新版本4.39.3中添加的T5ForTokenClassification模型。,https://github.com/hpcaitech/ColossalAI/issues/5841
ColossalAI,这个issue是一个特性请求，主要涉及ColossalAI的Engine和Diffusion Model的重构和支持。原因是引擎设计的重构和支持新的Diffusion模型和PixArtAlpha。,https://github.com/hpcaitech/ColossalAI/issues/5838
ColossalAI,这个issue类型为代码贡献请求，涉及ColossalAI软件平台的功能更新，可能由于更新不及时导致用户需要更新版本或者提交代码。,https://github.com/hpcaitech/ColossalAI/issues/5832
ColossalAI,该issue类型为用户提出需求，涉及支持CommandR模型的特性及其相关信息。造成此需求的原因可能是用户希望在ColossalAI中集成并使用CommandR模型。,https://github.com/hpcaitech/ColossalAI/issues/5831
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中LoRA模型在训练大模型时出现OOM问题，希望支持GeminiPlugin或HybridParallelPlugin来解决该问题。,https://github.com/hpcaitech/ColossalAI/issues/5826
ColossalAI,这个issue是一个feature请求，涉及主要对象是ColossalAI框架中的IPv4主机初始化支持。这个需求产生是因为当前只支持IPv6，用户希望添加IPv4支持。,https://github.com/hpcaitech/ColossalAI/issues/5822
ColossalAI,这个issue类型是功能需求，主要涉及的对象是支持ColossalAI的Command-R模型。由于需要支持Command-R模型，所以提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/5818
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalAI项目中的Zero offload功能。由于用户希望支持qwen模型而提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/5817
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中T5ForTokenClassification model的支持。由于缺乏该支持，用户提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/5816
ColossalAI,这个issue是一个用户提出需求问题，涉及Gemini项目中的transformers更新。原因可能是为了提升Gemini项目的功能和性能。,https://github.com/hpcaitech/ColossalAI/issues/5814
ColossalAI,该issue类型为代码改进提案，涉及主要对象为Gemini IO，由于缺少相关precommit配置导致了此问题。,https://github.com/hpcaitech/ColossalAI/issues/5813
ColossalAI,这个issue是一个功能改善的PR，并不是bug报告，主要涉及ColossalAI的API接口。之所以提出这个PR可能是为了使得新的API接口与之前的版本保持一致。,https://github.com/hpcaitech/ColossalAI/issues/5811
ColossalAI,这是一个需求提交类型的issue，主要涉及将transformers升级至最新版本。由于ColossalAI引发了一些问题，用户提交了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/5810
ColossalAI,这个issue是用户提出了需求，并涉及ColossalAI中的Shardformer模块。用户请求升级transformers以适配GPT2、GPTJ和Whisper模型。,https://github.com/hpcaitech/ColossalAI/issues/5807
ColossalAI,这个issue属于需求提出类，主要涉及ColossalAI文档中的侧边栏添加dist optim内容，可能是因为缺少相关信息导致在文档查找时不便利。,https://github.com/hpcaitech/ColossalAI/issues/5806
ColossalAI,该问题类型为功能升级，主要涉及将transformers升级为mistral，旨在提高ColossalAI的性能和功能。,https://github.com/hpcaitech/ColossalAI/issues/5804
ColossalAI,这是关于代码质量和工作流程的贡献需求，主要涉及Gemini模块的异步操作问题。,https://github.com/hpcaitech/ColossalAI/issues/5803
ColossalAI,该issue类型是一个新功能提议，涉及ColossalAI项目中的MoE模块重构，可能由于ZeRO的最新版本更新导致需要相应重构。,https://github.com/hpcaitech/ColossalAI/issues/5801
ColossalAI,这是一个特性请求，涉及ColossalAI中的Shardformer支持4D并行和Flash Attention。由于需要支持4D和Flash Attention，开发人员正在创建此PR。,https://github.com/hpcaitech/ColossalAI/issues/5789
ColossalAI,这个issue是一个质量要求更新的变更请求，涉及ColossalAI的代码质量和贡献流程。,https://github.com/hpcaitech/ColossalAI/issues/5787
ColossalAI,这是一个特性需求的issue，主要涉及到Lazy Init功能的支持，其中提到了Lazy Init可以加快模型加载过程，减少程序因CPU内存限制而冻结的问题。,https://github.com/hpcaitech/ColossalAI/issues/5785
ColossalAI,这个issue类型为技术改进，主要涉及ColossalAI的CI/CD过程，由于缺少必要步骤或设置导致Docker CI出现问题。,https://github.com/hpcaitech/ColossalAI/issues/5780
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的PR创建流程及规范性检查，由于缺少必要的检查步骤导致可能会影响代码质量。,https://github.com/hpcaitech/ColossalAI/issues/5776
ColossalAI,这个issue是一个特性改进（Feature）类型的问题，主要涉及ColossalAI中的MoE（Mixture of Experts）重构。导致该问题可能由于需要对MoE进行重构以改进系统性能或可维护性。,https://github.com/hpcaitech/ColossalAI/issues/5775
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是`LowLevelZeroOptimizer`的重构，由于不同param groups具有不同的并行策略，需要更好地处理它们，以避免相互干扰和实现各自的通信逻辑。,https://github.com/hpcaitech/ColossalAI/issues/5767
ColossalAI,该issue类型为用户提出需求，主要涉及对象为SP（Scaler 模块）和FlashAttention（另一个功能）。该问题由于SP无法与FlashAttention兼容导致用户提出了希望两者兼容的需求。,https://github.com/hpcaitech/ColossalAI/issues/5762
ColossalAI,这是一个用户提出需求的issue，主要涉及Gemini模块，可能由于数据预取和数据传输导致了性能问题。,https://github.com/hpcaitech/ColossalAI/issues/5761
ColossalAI,这个issue类型是优化需求，涉及到Gemini框架中reduce scatter d2h copy的优化。由于之前的实现没有覆盖到d2h copy部分，导致需要重新实现异步reduce以便overlap所有reduce和d2h copy操作与计算。,https://github.com/hpcaitech/ColossalAI/issues/5760
ColossalAI,这个issue类型是PR优化，主要涉及测试用例的简化，旨在减少测试时间。原因是为了优化开发工作流程。,https://github.com/hpcaitech/ColossalAI/issues/5755
ColossalAI,这个issue是用户提交的关于提交Gemini prefetch benchmark的需求。,https://github.com/hpcaitech/ColossalAI/issues/5754
ColossalAI,这个issue类型是改进测试持续集成流程的请求，涉及的主要对象是测试用例。由于CI执行时间过长，需要去除部分测试用例以减少执行时间。,https://github.com/hpcaitech/ColossalAI/issues/5753
ColossalAI,这是一个特性优化类的Issue，主要涉及Gemini中reduce-scatter overlap和chunk prefetch的代码添加，旨在提升llama benchmark性能。,https://github.com/hpcaitech/ColossalAI/issues/5751
ColossalAI,这个issue属于feature需求类型，涉及Gemini模块，主要目的是在每次操作之前预取下一个片段。原因可能是为了提高操作的效率和性能。,https://github.com/hpcaitech/ColossalAI/issues/5749
ColossalAI,该issue类型为代码贡献请求，涉及主要对象为ColossalAI项目。由于缺少相关标准和流程的遵循，导致创作者需要采取一系列操作进行贡献提交。,https://github.com/hpcaitech/ColossalAI/issues/5747
ColossalAI,这是一个关于功能添加的issue，主要涉及的对象是优化器（optimizers），由于需要将优化器自动转换为分布式版本，开发者需要进行相应的代码修改。,https://github.com/hpcaitech/ColossalAI/issues/5746
ColossalAI,这是一个功能添加的issue，主要涉及对ColossalAI推理框架中的Streaming LLM进行集成和优化。,https://github.com/hpcaitech/ColossalAI/issues/5745
ColossalAI,"这是一个关于""Inference""功能合并的issue，主要涉及ColossalAI下的Tensor Parallelism Inference等功能。由于开发者未按照规范创建PR并未安装预提交，导致需要进行相应的检查和修改。",https://github.com/hpcaitech/ColossalAI/issues/5739
ColossalAI,这个issue是一个文档更新类型的问题，主要涉及ColossalAI的推理（inference）文档更新。由于创建PR前的要求没有完全满足，可能由于缺乏标准化流程或者提出者对流程不够熟悉导致。,https://github.com/hpcaitech/ColossalAI/issues/5736
ColossalAI,该issue类型是功能优化，主要涉及优化Pipeline Parallel中重叠通信与计算，由于原接口不清晰，需要进行接口大规模清理和优化。,https://github.com/hpcaitech/ColossalAI/issues/5735
ColossalAI,这个issue是功能改进类型，主要涉及Gemini模块的自动策略预取功能，由于原代码需要修改。,https://github.com/hpcaitech/ColossalAI/issues/5733
ColossalAI,该issue类型为更新请求，涉及主要对象为推理示例生成脚本和文档。原因可能是脚本或文档需要更新以保持最新或修复问题。,https://github.com/hpcaitech/ColossalAI/issues/5725
ColossalAI,这个issue属于更新文档内容的类型，涉及主要对象为ColossalAI中PyTorch版本的更新。原因可能是为了保持文档内容的准确性和及时性。,https://github.com/hpcaitech/ColossalAI/issues/5724
ColossalAI,这个issue是一个功能改进，主要涉及到ColossalAI中的向量操作，问题是要删除重复的`copy_vector`函数。,https://github.com/hpcaitech/ColossalAI/issues/5716
ColossalAI,这个issue类型是代码优化，主要涉及的对象是向量工具，用户请求清理重复向量的实用工具。这个问题可能是由于代码中存在重复逻辑或功能不完整导致的。,https://github.com/hpcaitech/ColossalAI/issues/5715
ColossalAI,这是一个文档更新类的问题，涉及主要对象是ColossalAI中的PyTorch版本更新。原因可能是为了修复文档中版本信息不一致的问题。,https://github.com/hpcaitech/ColossalAI/issues/5711
ColossalAI,这个issue是关于优化推断过程中重复参数和ngram大小的调整，不是bug报告。,https://github.com/hpcaitech/ColossalAI/issues/5708
ColossalAI,这是一个功能需求issue，主要涉及到ColossalAI中的Inference/Kernel模块，由于需要支持序列长度在线程块中分割，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/5707
ColossalAI,这个issue类型是功能增追，主要对象是添加`convert_fp8` op，用户希望为fp8测试准备。,https://github.com/hpcaitech/ColossalAI/issues/5706
ColossalAI,该issue为一个功能增强类型，主要涉及RPC服务器支持，在进行推理时支持RPC。由于需要在推理过程中实现RPC功能，导致了此问题的提出。,https://github.com/hpcaitech/ColossalAI/issues/5705
ColossalAI,这是一个功能开发的issue，涉及对象是bloom和falcon模型。由于需要在bloom和falcon中添加并行交叉熵计算，因此涉及了这个功能的开发工作。,https://github.com/hpcaitech/ColossalAI/issues/5702
ColossalAI,这个issue是一个文档更新的PR，涉及的主要对象是ColossalAI的分布式优化文档。根据内容推测，用户可能由于要提交修改而提交了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/5701
ColossalAI,这个issue属于特性需求，主要涉及添加quant kvcache接口，原因可能是为了优化推理功能。,https://github.com/hpcaitech/ColossalAI/issues/5700
ColossalAI,这是一个用户提出需求的issue，主要涉及支持Qwen2模型，需要ColossalAI中的Shardformer支持transformers版本在4.39.1以上。,https://github.com/hpcaitech/ColossalAI/issues/5699
ColossalAI,这是一个功能修改的issue，主要对象是Gemini模块。由于需要移除已注册的梯度钩子，故此需求产生。,https://github.com/hpcaitech/ColossalAI/issues/5696
ColossalAI,这是一个功能请求issue，主要涉及ColossalAI中的分布式优化器，涉及的主要对象是优化器的支持程度和集成方式。,https://github.com/hpcaitech/ColossalAI/issues/5694
ColossalAI,"这是一个功能需求的issue，涉及""Support the logic related to ignoring EOS token""，由于需要支持忽略EOS token的逻辑。",https://github.com/hpcaitech/ColossalAI/issues/5693
ColossalAI,这个issue是一个特性请求，涉及对象是ColossalAI中的推断功能。由于缺少量化KVCACHE支持，导致了无法执行解码操作。,https://github.com/hpcaitech/ColossalAI/issues/5686
ColossalAI,这个issue类型为代码同步更新，涉及主要对象为ColossalAI项目的inference分支。该问题由于在更新过程中出现了tests timeout导致的bug。,https://github.com/hpcaitech/ColossalAI/issues/5685
ColossalAI,这个issue类型是用户提出需求，该问题单涉及的主要对象是Shardformer模型，用户寻求添加Shardformer模型的并行输出功能。,https://github.com/hpcaitech/ColossalAI/issues/5684
ColossalAI,这个issue是一个Feature请求，主要涉及到ColossalAI中的MoE refractor和Mixtral集成。原因可能是为了功能优化和扩展。,https://github.com/hpcaitech/ColossalAI/issues/5682
ColossalAI,这是一个请求更改的issue，主要涉及到ColossalAI库中的推理/核心代码，主要是因为不必要的命名问题导致的。,https://github.com/hpcaitech/ColossalAI/issues/5679
ColossalAI,这个issue是关于需求的，主要涉及ColossalAI中的flash attention函数。由于需要将alibi_slopes参数传递给flash_attn_varlen_func函数，导致了这个问题的提出。,https://github.com/hpcaitech/ColossalAI/issues/5678
ColossalAI,这个issue类型是功能增强，主要涉及的对象是支持Triton Kernel中的新KCache布局。这个问题由于需要添加对新的kcache布局的支持而引起，涉及到清理内核文件和移除未使用的triton内核。,https://github.com/hpcaitech/ColossalAI/issues/5677
ColossalAI,这个issue是关于Github Actions的改进建议，主要涉及workflow的优化，包括不在fork上执行scheduled workflows，使用'if'语句等。,https://github.com/hpcaitech/ColossalAI/issues/5668
ColossalAI,这是一个关于改进建议的issue，涉及到GitHub Action的使用规范，主要提到避免在fork仓库上执行定时工作流、为uploadartifact动作使用条件语句、以及在使用GitHub Token时使用权限。这些改进建议可能源自于提高GitHub Action的可靠性和安全性。,https://github.com/hpcaitech/ColossalAI/issues/5667
ColossalAI,这个issue类型是技术改进提案，涉及的主要对象是ColossalAI中的推理模块。由于代码重构和新功能添加，导致之前的管理器和操作在性能方面存在问题，因此需要进行修改和优化。,https://github.com/hpcaitech/ColossalAI/issues/5663
ColossalAI,这是一个新增功能的PR，涉及 Colossal AI 中的 Bloom 模型推断支持。,https://github.com/hpcaitech/ColossalAI/issues/5660
ColossalAI,这是一个需求报告issue，涉及主要对象是Baichuan 7B/13B TP的适配完成。,https://github.com/hpcaitech/ColossalAI/issues/5659
ColossalAI,这个issue是一个功能更新，涉及主要对象是ColossalAI中的Triton Kernel，导致的问题是需要支持新的KCache布局以提高CUDA Flash Decoding性能。,https://github.com/hpcaitech/ColossalAI/issues/5658
ColossalAI,该issue是一个特性需求，主要涉及ColossalAI中的lora和qlora，用户请求支持ddp/zero，可能是出于对分布式训练的需求。,https://github.com/hpcaitech/ColossalAI/issues/5657
ColossalAI,该issue属于功能需求类，主要涉及ColossalAI框架中的Inference/Feat模块，用户提出需要为FlashDecoding添加kvcache量化支持。由于需要在推断过程中实现kvcache量化支持，因此用户在这个issue中描述了相关的操作步骤和checklist。,https://github.com/hpcaitech/ColossalAI/issues/5656
ColossalAI,这个issue是一个Feature需求，主要涉及到ColossalAI中的MoE refractor和与Mixtral的集成。这个问题的提出可能是为了对MoE进行重构以及与Mixtral集成的相关工作和功能。,https://github.com/hpcaitech/ColossalAI/issues/5649
ColossalAI,这个issue属于功能需求类型，主要涉及ColossalAI中的模型支持bias_gelu_jit_fused，用户提出了这一功能需求。,https://github.com/hpcaitech/ColossalAI/issues/5647
ColossalAI,这是一个功能优化的issue，涉及ColossalAI中的pipeline grad ckpt配置重构。造成这个问题的主要原因是为了简化相应API。,https://github.com/hpcaitech/ColossalAI/issues/5646
ColossalAI,这个issue是关于技术优化，需要移除无用代码，不涉及bug报告或用户需求。,https://github.com/hpcaitech/ColossalAI/issues/5645
ColossalAI,这是一个用户提出需求的issue，主要涉及如何使用booster包装多个模型的问题。用户想要解决如何训练两个需要使用不同优化器的模型，并且一个模型较大无法在单卡上加载，另一个辅助训练大模型的小模型也需要参数更新的情况。,https://github.com/hpcaitech/ColossalAI/issues/5641
ColossalAI,这个issue是一个功能更新请求，主要涉及ColossalAI下的shardformer模块中的flashattention替换更新。,https://github.com/hpcaitech/ColossalAI/issues/5637
ColossalAI,此issue为功能特性合并的请求，涉及主要对象为ColossalAI项目。由于未完成检查清单以及相关步骤，用户请求将LoRA特性合并至主分支。,https://github.com/hpcaitech/ColossalAI/issues/5632
ColossalAI,这是一个更新Llama推理示例的issue，主要涉及Llama3配置和推理设置的更新，以及添加Llama生成演示脚本、Llama3基准测试的清理版本和修复Llama策略中的一些BUG。,https://github.com/hpcaitech/ColossalAI/issues/5629
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中的coloattention模块，用户希望修改coloattention模块并做相关的PR。由于用户在开发过程中遵循了一系列标准操作，这表明用户可能在优化代码质量和项目开发的可追溯性方面有一定关注。,https://github.com/hpcaitech/ColossalAI/issues/5627
ColossalAI,这个issue是一个功能性质的任务，涉及ColossalAI中LoRA的更新，由于在创建PR之前的检查中有一些标准要求未满足。,https://github.com/hpcaitech/ColossalAI/issues/5622
ColossalAI,这是一个feature请求，涉及ColossalAI中LoRA的重新基于主分支操作。通过此PR，提交者对工作进行了总结，确保了标题、标签、自审查、测试等步骤，最终可能是为了提升系统功能和性能。,https://github.com/hpcaitech/ColossalAI/issues/5621
ColossalAI,这个issue是一个功能需求问题，主要对象是ColossalAI中的LLaMA-3 CPT和ST，用户希望支持LLaMA-3的CPT和ST。由于缺乏该支持，用户想要添加这些功能以提升系统的功能性。,https://github.com/hpcaitech/ColossalAI/issues/5619
ColossalAI,这个issue类型是PR（Pull Request）请求，主要涉及ColossalAI中关于适应baichuan2 13B的推理，并由于一系列的规范性要求导致了相关的问题和解决措施。,https://github.com/hpcaitech/ColossalAI/issues/5614
ColossalAI,这个issue类型为特性需求，涉及ColossalAI的推理/内核功能，主要是关于新增了rmsnorm baichuan2特性。原因可能是为了优化性能或添加新功能。,https://github.com/hpcaitech/ColossalAI/issues/5611
ColossalAI,这个issue类型为特性需求，主要对象是LoRA模块的量化支持。导致此需求的原因是为了增加对LoRA模块的新功能。,https://github.com/hpcaitech/ColossalAI/issues/5604
ColossalAI,这个issue是关于代码优化的，主要涉及Shardformer的embedding resize，由于需要调整embedding大小以支持更大的词汇表。,https://github.com/hpcaitech/ColossalAI/issues/5603
ColossalAI,该issue类型为功能需求，主要涉及的对象是嵌入层的调整，由于需要调整嵌入层的大小，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/5602
ColossalAI,这个issue是一个特性需求报告，涉及ColossalAI代码库中的多次（部分）反向传播支持，用户提出了关于权重自适应损失和梯度参数两次计算的问题，以及如何解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/5601
ColossalAI,这个issue是一个功能特性PR，涉及到ColossalAI中推理模型适配baichuan2-7B模型的工作，用户希望通过此PR解决相应问题并实现相关功能。,https://github.com/hpcaitech/ColossalAI/issues/5591
ColossalAI,这个issue是一个特性请求，涉及到ColossalAI的在线服务功能的开发。由于开发者要求PR前需要先创建一个issue，添加相关标签和文档，并进行自我代码审核以及添加详尽的测试，可以推测这是为了确保质量和跟踪性。,https://github.com/hpcaitech/ColossalAI/issues/5588
ColossalAI,这个issue是一个Feature请求，主要涉及ColossalAI的Online Serving功能。原因可能是为了实现在线服务部署或者相关功能需求。,https://github.com/hpcaitech/ColossalAI/issues/5584
ColossalAI,这个issue属于需求类型，涉及主要对象为代码重构，由于文件命名问题和代码优化，需要添加新参数并重构代码。,https://github.com/hpcaitech/ColossalAI/issues/5582
ColossalAI,这个issue类型是功能需求，主要涉及的对象是对嵌入进行重置大小。由于需要对嵌入进行大小调整，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/5579
ColossalAI,这个issue类型为技术改进提议，涉及主要对象为ColossalAI中的tester precision。由于在vanilla lamb上移除了ZeRO，可能会导致tester实现方面的精度改善。,https://github.com/hpcaitech/ColossalAI/issues/5576
ColossalAI,这个issue是关于改进测试者的精度，属于功能需求性质，主要对象是测试者的精度。问题可能是由于当前使用了ZeRO在vanilla lamb上导致了精度不佳。,https://github.com/hpcaitech/ColossalAI/issues/5575
ColossalAI,这是一个功能需求提交，主要涉及支持Qwen2模型的问题。由于transformers 4.36.0版本不支持Qwen2模型，在此版本下无法通过测试。,https://github.com/hpcaitech/ColossalAI/issues/5574
ColossalAI,这是一个功能需求（Feature）的issue，涉及支持ColossalAI中的qwen2模型。这个问题主要是用户提出了对于新增模型支持的需求。,https://github.com/hpcaitech/ColossalAI/issues/5573
ColossalAI,这是一则关于pre-commit自动更新的issue，类型为自动更新提示，涉及到各个项目的版本更新信息，用户可能寻求更新信息的确认或希望解决与更新相关的问题。,https://github.com/hpcaitech/ColossalAI/issues/5572
ColossalAI,该issue是一个特性添加的请求，主要涉及ColossalAI中Galore的优化器集成和分布式实现，用户向社区提交了关于这方面工作的需求。由于缺乏这些功能，可能导致用户在使用过程中无法有效进行模型优化或分布式训练。,https://github.com/hpcaitech/ColossalAI/issues/5570
ColossalAI,该issue是一个贡献者提交了一个合并请求（PR），在合并特征/colossal-infer到主代码库时需要遵循的一些规范。,https://github.com/hpcaitech/ColossalAI/issues/5567
ColossalAI,这是一个新增功能类型的issue，涉及的主要对象是ColossalAI推理引擎。由于需求提出了Speculative Decoding和GLIDE Spec-Dec功能的实现，以及对Triton Kernels进行了修订，可能导致了相关功能的缺失或性能不佳的问题。,https://github.com/hpcaitech/ColossalAI/issues/5565
ColossalAI,该issue类型为用户提出需求，主要涉及ColossalAI中的Tensor Model Parallel Support For Inference，用户请求新增对Tensor Model Parallel在推理过程中的支持。由于缺乏该功能，用户希望通过此需求来实现Tensor Model Parallel在推理过程中的支持。,https://github.com/hpcaitech/ColossalAI/issues/5563
ColossalAI,这是一个用户需求类型的issue，主要涉及ColossalAI中的自动并行计算示例在使用cuda 12和torch 2.2时出现错误的问题，用户希望得到支持cuda 12和torch 2.2的代码或示例。,https://github.com/hpcaitech/ColossalAI/issues/5562
ColossalAI,这个issue类型是Feature（功能需求），涉及的主要对象是Lora Quantization Support。由于缺少对于Lora Quantization Support的支持，需要开发者添加该功能以解决相关需求。,https://github.com/hpcaitech/ColossalAI/issues/5561
ColossalAI,这个issue类型是功能开发请求，涉及主要对象是使用MoEManager的测试文件test_moe_checkpoint.py，由于要使用MoeHybridParallelPlugin替换MoEManager，以及消除对MoEManager的依赖，可能是出于性能、功能改进或代码优化的考虑。,https://github.com/hpcaitech/ColossalAI/issues/5550
ColossalAI,这是一个用户提出需求的类型，该问题涉及主要对象为ColossalAI。由于内容只包含一个链接，用户可能提出了一个请求回到之前的页面或者环境。,https://github.com/hpcaitech/ColossalAI/issues/5547
ColossalAI,该issue类型为优化提升，涉及的主要对象是ColossalAI框架的散乱优化点。由于框架中存在散乱的优化点，导致性能问题，需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/5544
ColossalAI,这是一个用户提出需求的issue，该问题单涉及的主要对象是预训练数据格式。由于缺乏预训练数据格式的例子，用户提出了需要提供示例的需求。,https://github.com/hpcaitech/ColossalAI/issues/5542
ColossalAI,该issue是一个用户提出需求类型的问题，主要涉及的对象是ColossalAI中的分布式计算功能。用户提出了关于如何实现在多个节点上进行分布式训练和推断的需求，并请求提供示例代码。,https://github.com/hpcaitech/ColossalAI/issues/5538
ColossalAI,"这是一个“需求添加”类型的问题单，主要涉及ColossalAI的""Inference""模块，由于需要增加Reduce Utils相关功能而产生。",https://github.com/hpcaitech/ColossalAI/issues/5537
ColossalAI,这是一个功能需求报告，用户希望能够在H800上使用torch 2.2和Cuda 12.0运行Colossal AI，但由于现有示例代码基于torch 1.12构建且过时无法在torch 2.2上运行，用户现在遇到了兼容性问题。,https://github.com/hpcaitech/ColossalAI/issues/5536
ColossalAI,这个issue类型是功能改进，主要对象是ShardFormer/Sequence Parallelism Optimization，用户提出了关于优化和提交PR的请求。,https://github.com/hpcaitech/ColossalAI/issues/5533
ColossalAI,这个issue是更新Vit模型相关的内容，不是bug报告。,https://github.com/hpcaitech/ColossalAI/issues/5530
ColossalAI,这是一个更新issue，涉及ColossalAI中的Whisper模型，由于需要升级transformers版本以及一些功能的调整。,https://github.com/hpcaitech/ColossalAI/issues/5529
ColossalAI,"该issue是一个特性添加请求，主要涉及ColossalAI中推理/内核功能模块的更新，请求添加一个名为""get_cos_and_sin Kernel""的功能。由于需要更好的数学计算支持，因此提出了这项功能需求。",https://github.com/hpcaitech/ColossalAI/issues/5528
ColossalAI,这是一个需求更新的issue，涉及到更新ColossalAI中的t5模型。由于需要升级transformers版本，包括t5模型中的t5_stack_forward函数的opt模型，可能是为了解决某些问题或提升功能。,https://github.com/hpcaitech/ColossalAI/issues/5524
ColossalAI,这是一个关于优化的issue，涉及主要对象为Zero1/2平台。由于需要进行CPU卸载、大规模等优化，用户提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/5523
ColossalAI,该issue是关于需求提出的，主要涉及Optimizer offload和Zero中的subgroup参数，问题的根本是为了加速optimizer offload并为Zero添加sub dp_group功能。,https://github.com/hpcaitech/ColossalAI/issues/5521
ColossalAI,这个issue是一个用户提交的PR请求，涉及到ColossalAI的代码优化和标准化，主要对象是代码风格和宏定义相关逻辑。,https://github.com/hpcaitech/ColossalAI/issues/5519
ColossalAI,这个issue是一个优化请求处理程序的问题，主要涉及ColossalAI下的llama。由于请求处理程序的效率问题导致的bug或用户需求。,https://github.com/hpcaitech/ColossalAI/issues/5512
ColossalAI,这个issue是关于更新Mistral模型的功能，属于功能改进类型，涉及主要对象为ColossalAI中的Mistral模型。,https://github.com/hpcaitech/ColossalAI/issues/5511
ColossalAI,这个issue是关于更新ColossalAI中的shardformer模块以支持自定义mask的功能需求类型问题，主要涉及到代码更新和功能扩展。,https://github.com/hpcaitech/ColossalAI/issues/5510
ColossalAI,这是一个功能需求提议，主要涉及`gradient_checkpointing_ratio`和异构分片策略，用户希望更精确地控制梯度检查点，提高训练速度并避免内存溢出。,https://github.com/hpcaitech/ColossalAI/issues/5509
ColossalAI,这个issue是一个特性需求，主要涉及ColossalAI中的Shardformer和Pipeline模块，用户提出了关于添加`gradient_checkpointing_ratio`和异构shard策略的需求。,https://github.com/hpcaitech/ColossalAI/issues/5508
ColossalAI,该issue类型为功能需求提升，涉及主要对象为ColossalAI下的Shardformer模型。这是由于用户需求需要将Shardformer从transformers 4.33.0版本升级到4.36.0版本，主要影响到colossalai/shardformer/modeling目录下的模型。,https://github.com/hpcaitech/ColossalAI/issues/5505
ColossalAI,这是一个更新模型的issue，类型为需求提出，涉及的主要对象为gptj模型，用户请求升级transformers版本并更新gptj模型的函数。,https://github.com/hpcaitech/ColossalAI/issues/5503
ColossalAI,这是一个更新issue，涉及ColossalAI中的Shardformer模块 llama2的升级。,https://github.com/hpcaitech/ColossalAI/issues/5499
ColossalAI,这个issue属于需求提出类型，涉及主要对象为Transformers的升级，原因是为实现更好的功能和性能。,https://github.com/hpcaitech/ColossalAI/issues/5497
ColossalAI,这个issue是一个PR请求，涉及更新Grok-1推理，主要问题是缺少PR标准格式、标签和问题描述细节，需要添加详尽的测试和文档。,https://github.com/hpcaitech/ColossalAI/issues/5495
ColossalAI,这个issue类型是用户提出的需求，涉及对象是Shardformer中的dit model，用户提出希望支持在ColossalAI中实现layer Parallel功能。,https://github.com/hpcaitech/ColossalAI/issues/5494
ColossalAI,这个issue类型为功能特性开发，主要对象涉及ColossalAI代码贡献者。由于维护PR质量和可追溯性的需求，该issue提供了PR创建前的详细检查清单，以确保代码质量和可追溯性。,https://github.com/hpcaitech/ColossalAI/issues/5491
ColossalAI,这个issue是一个功能优化类型的issue，主要涉及Shardformer模块中的padding vocabulary size，目的是为了让其选择更快的核心算法。,https://github.com/hpcaitech/ColossalAI/issues/5489
ColossalAI,这是一个需求提出类型的issue，主要涉及到ColossalAI中的shardformer模块，用户希望该模块能够启用填充词汇大小。,https://github.com/hpcaitech/ColossalAI/issues/5488
ColossalAI,这个issue类型是功能改进，涉及的主要对象是正则表达式（regexp），用户提出改进正则表达式以支持Unicode的需求。,https://github.com/hpcaitech/ColossalAI/issues/5486
ColossalAI,这个issue是一个需求提出，主要涉及的对象是分布式Adafactor优化算法。由于缺少对应标准格式的标题、标签以及相关issue链结，可能导致PR跟踪性不强。,https://github.com/hpcaitech/ColossalAI/issues/5484
ColossalAI,这个issue是一个用户提出需求的类型，主要涉及的对象是GitHub仓库中的ColossalAI。这个issue可能由于用户需要再次确认某些信息或者提出了重复问题而产生。,https://github.com/hpcaitech/ColossalAI/issues/5483
ColossalAI,这是一个用户提出需求的类型，主要涉及到ColossalAI库在Windows上安装的问题。用户提出了需要为Windows编译WHL文件以解决当前缺乏Windows支持的问题。,https://github.com/hpcaitech/ColossalAI/issues/5481
ColossalAI,该issue类型为文档更新，主要涉及到更新open-sora demo，可能由于更新需求或观点改进导致。,https://github.com/hpcaitech/ColossalAI/issues/5479
ColossalAI,这个issue类型是新功能需求，主要对象是DistributedAdafactor模块，由于需要在ColossalAI中添加分布式Adafactor功能，导致此需求的提出。,https://github.com/hpcaitech/ColossalAI/issues/5477
ColossalAI,"这个issue类型是功能增强，涉及的主要对象是新增的VecTypeTrait和相关组件，由于需求扩展，开发者新增了这些功能以支持vec ld,st,compute。",https://github.com/hpcaitech/ColossalAI/issues/5473
ColossalAI,这个issue是一个功能提议类型的问题，主要涉及ColossalAI中的VecTypeTrait相关组件，由于缺乏VecTypeTrait的实现，导致用户需要添加这些相关组件。,https://github.com/hpcaitech/ColossalAI/issues/5472
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的在线服务器Chat API的添加。,https://github.com/hpcaitech/ColossalAI/issues/5470
ColossalAI,这个issue类型是新增功能发布，主要涉及ColossalAI中发布Open-Sora 1.0版本的模型权重，用户寻求创建一个包含详细说明和文档的PR。,https://github.com/hpcaitech/ColossalAI/issues/5468
ColossalAI,"这个issue类型为功能优化，主要涉及ColossalAI下的""refactor colo attention""，由于需要重构扩展api，属于功能改进性工作。",https://github.com/hpcaitech/ColossalAI/issues/5462
ColossalAI,这是一个功能更新的issue，主要涉及ColossalAI中的推理功能，支持FP16/BF16 Flash Attention 2以及给Rotary Embedding添加高精度标志。,https://github.com/hpcaitech/ColossalAI/issues/5461
ColossalAI,这是一个功能改进的issue，主要对象是支持FP16/BF16 Flash Attention 2，用户寻求帮助或者功能增强。,https://github.com/hpcaitech/ColossalAI/issues/5460
ColossalAI,这个issue类型属于功能需求，涉及的主要对象是ColossalAI代码库中的GetGPULaunchConfig1D函数的实现，由于当前编译机制不支持.separate运算符，导致无法实现声明和定义分离。,https://github.com/hpcaitech/ColossalAI/issues/5456
ColossalAI,这是一个特性需求的issue，主要涉及ColossalAI项目下的GLIDE Drafter模型的支持。原因是为了增加对Glide Drafter模型的实现和测试。,https://github.com/hpcaitech/ColossalAI/issues/5455
ColossalAI,这个issue是一个改进请求，涉及到ColossalAI中SpecDec的功能。原因是修复了在特定情况下输入错误的问题，并对KV缓存修剪行为进行了修改。,https://github.com/hpcaitech/ColossalAI/issues/5449
ColossalAI,这个issue类型是用户提出需求，涉及主要对象是ColossalAI框架，用户提出了整合GaLore技术到ColossalAI框架的需求。,https://github.com/hpcaitech/ColossalAI/issues/5443
ColossalAI,这个issue是一个优化cuda kernel的PR，涉及主要对象是ColossalAI中的rmsnorm算法。由于性能提升需求，需要对rmsnorm核心进行优化。,https://github.com/hpcaitech/ColossalAI/issues/5441
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI项目与huggingface accelerate的集成。由于使用huggingface accelerate可能会带来一些效率方面的改进，用户希望了解这一集成特性的具体描述。,https://github.com/hpcaitech/ColossalAI/issues/5439
ColossalAI,这是一个提出性能优化改进的issue，主要对象是ColossalAI项目中的`DimSpec`类。原因是每次生成`DimSpec`对象时都会创建一个字典，导致效率较低，需要通过将字典转换为类属性并进行惰性初始化来提升性能。,https://github.com/hpcaitech/ColossalAI/issues/5437
ColossalAI,这是一个特性需求的issue，涉及ColossalAI库中的CUDA图支持和非功能性API重构。这个问题涉及到了添加CUDA图支持和重构API，以便CUDA图能够捕获整个图。,https://github.com/hpcaitech/ColossalAI/issues/5434
ColossalAI,这个issue是一个feature请求，涉及完成Online Serving Test，添加streaming output api，continuous batching test和example。导致该请求的原因是为了完善ColossalAI的服务功能。,https://github.com/hpcaitech/ColossalAI/issues/5432
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中的stream chat模块，用户提交了添加stream chat示例的请求。,https://github.com/hpcaitech/ColossalAI/issues/5428
ColossalAI,这个issue是一个用户提出需求的类型，主要涉及的对象是PR创建前的一系列操作步骤。用户可能提出这个需求是因为希望在创建PR时更加规范和方便追踪相关工作。,https://github.com/hpcaitech/ColossalAI/issues/5424
ColossalAI,这是一个实现Speculative Decoding功能的issue，主要涉及ColossalAI框架下的模型推理。Bug报告。由于没有添加相关标签而导致了需要进行补充。,https://github.com/hpcaitech/ColossalAI/issues/5423
ColossalAI,这个issue是一个功能修改的请求，涉及到ColossalAI的推断内核，主要是添加了Fused Rotary Embedding和KVCache Memcopy的CUDA内核。原因可能是为了优化模型的性能和速度。,https://github.com/hpcaitech/ColossalAI/issues/5418
ColossalAI,这个issue是一个用户提出需求，涉及ColossalAI中的RMSNorm cuda kernel的特征和新增的unittest、benchmark script。原因可能是为了改进性能或功能。,https://github.com/hpcaitech/ColossalAI/issues/5417
ColossalAI,这是一个向ColossalAI贡献者提出的工作流程建议的issue，主要涉及到PR创建前的检查和请求审查前的准备工作。这个issue的目的是为了提高PR的可追溯性和质量。,https://github.com/hpcaitech/ColossalAI/issues/5412
ColossalAI,该issue是关于技术改进的PR，主要涉及添加CUDA KVCache Kernel，由于在PR过程中未添加正确的标签和未完成相关步骤导致需要修改。,https://github.com/hpcaitech/ColossalAI/issues/5406
ColossalAI,这是一个新功能需求的issue，涉及的主要对象是ColossalAI下的SpecDec（Implement speculative decoding）模块，是因为要为SpecDec添加基本版本的Drafter模型容器。,https://github.com/hpcaitech/ColossalAI/issues/5405
ColossalAI,这个issue是一个功能更新的PR，涉及的主要对象是Triton Kernels以支持Spec-Dec。这个更新的目的是修改triton kernels，以便主/大模型能够并行验证`n`个标记，并使得1）kvcachecopy kernel能够为每个序列复制多个标记，以及使2）解码注意力能够接收具有`q_len > 1`的输入。,https://github.com/hpcaitech/ColossalAI/issues/5401
ColossalAI,该issue类型为功能改进，涉及主要对象为添加llama logits的收集开关，用户希望能够通过此功能改进收集llama logits的方式。,https://github.com/hpcaitech/ColossalAI/issues/5398
ColossalAI,这个issue是关于功能需求的，主要涉及的对象是ColossalAI中的数据并行加载和检查点策略。导致该问题的原因是在大规模环境中使用多个数据并行组的策略会导致数据加载吞吐量过大，例如需要加载巨大的数据量，造成了耗时过长的问题。,https://github.com/hpcaitech/ColossalAI/issues/5397
ColossalAI,这个issue是一个需求提案，涉及的主要对象是为ColossalAI添加使用FastAPI实现异步和同步API服务器。由于缺乏这些功能，导致需要为推断添加同步API服务器。,https://github.com/hpcaitech/ColossalAI/issues/5396
ColossalAI,这是一个优化执行间隔时间的issue，涉及到ColossalAI中cuda kernel之间的优化，类型是性能优化问题，主要对象是cuda kernel执行间隔时间。,https://github.com/hpcaitech/ColossalAI/issues/5390
ColossalAI,这是一个文档更新的issue，主要涉及ColossalAI的安装命令的更新，问题可能是由于安装命令变更导致的需要进行更新文档内容。,https://github.com/hpcaitech/ColossalAI/issues/5389
ColossalAI,这是一个更新翻译的问题，涉及主要对象为README-zh-Hans.md文件，用户提出了需要更新部分翻译的需求。,https://github.com/hpcaitech/ColossalAI/issues/5382
ColossalAI,这个issue是一个技术贡献者在提交更新版本的PR时需要完成的checklist和任务清单。,https://github.com/hpcaitech/ColossalAI/issues/5380
ColossalAI,该issue属于功能需求类型，主要涉及支持vllm在基准测试脚本中进行测试。由于开发者需要进行性能测试和优化，因此需要在benchmark脚本中支持vllm测试。,https://github.com/hpcaitech/ColossalAI/issues/5379
ColossalAI,这是一个改进功能提议的issue，主要涉及的对象是ColossalAI中的推理过程。原因是为了支持在基准脚本中进行vllm测试。,https://github.com/hpcaitech/ColossalAI/issues/5378
ColossalAI,这是一个关于支持NPU的功能需求的issue，涉及到Colossal-LLaMA-2。这个问题的提出可能是为了增强系统的性能和功能。,https://github.com/hpcaitech/ColossalAI/issues/5377
ColossalAI,这个issue是一个优化提案，主要涉及到ColossalAI中的内存拷贝操作，由于性能原因提出了改进方案。,https://github.com/hpcaitech/ColossalAI/issues/5374
ColossalAI,这个issue为代码优化和修复问题，主要涉及ColossalAI下的llama训练脚本和优化checkpoint。原因可能是存在训练脚本不完善或者优化checkpoint问题。,https://github.com/hpcaitech/ColossalAI/issues/5368
ColossalAI,这是一个优化和重构推理批处理/调度的问题报告，主要涉及ColossalAI下的推理模块。由于内存分配逻辑的调整和优化，需要减少CPU时间开销。,https://github.com/hpcaitech/ColossalAI/issues/5367
ColossalAI,这是一个代码优化的PR，主要涉及ColossalAI中的eval模块，由于作者提到要更新 llama npu eval，可能是为了优化评估模块的性能。,https://github.com/hpcaitech/ColossalAI/issues/5366
ColossalAI,这个issue是一个优化PR，主要涉及MLP中门控和向上投影的融合以及优化自动求导过程。原因是为了提高ColossalAI在推理时的性能。,https://github.com/hpcaitech/ColossalAI/issues/5365
ColossalAI,这是一个用户提交需求的issue，主要涉及ColossalAI中的llama模块，用户请求为npu添加flash attn patch。这个需求可能是为了提高ColossalAI在处理flash attn相关任务时的性能或效果。,https://github.com/hpcaitech/ColossalAI/issues/5362
ColossalAI,该issue类型为代码更新，该问题单涉及的主要对象为ColossalAI项目中的训练脚本。由于缺少所需的检查步骤和规范格式，用户需要更新训练脚本，并补充相关的信息以便达到标准要求。,https://github.com/hpcaitech/ColossalAI/issues/5360
ColossalAI,这个issue是一个贡献PR的过程中未满足所有要求的问题。它主要涉及ColossalAI的保存和加载模型/优化器功能的实现。导致该问题的原因可能是作者未添加完善的测试或函数/方法的文档字符串。,https://github.com/hpcaitech/ColossalAI/issues/5357
ColossalAI,该issue类型为优化需求，主要涉及ColossalAI下的推断引擎的生成过程。由于生成过程较慢，用户寻求优化以提高效率。,https://github.com/hpcaitech/ColossalAI/issues/5356
ColossalAI,这个issue是一个优化提升类型的问题，主要涉及到ColossalAI中RMS Norm的中间张量优化，原因可能是为了提高性能或减少计算资源消耗。,https://github.com/hpcaitech/ColossalAI/issues/5350
ColossalAI,该issue是一个优化请求，主要涉及ColossalAI中RMS Norm的中间张量。由于对中间张量的优化不足，导致需要进行一些改进。,https://github.com/hpcaitech/ColossalAI/issues/5349
ColossalAI,这个issue是一个贡献者请求审查，主要涉及ColossalAI中推断功能的调整。原因可能是为了优化代码功能或修复相关bug。,https://github.com/hpcaitech/ColossalAI/issues/5348
ColossalAI,这个issue类型是文档更新，主要涉及ColossalAI中推理（inference）相关部分，由于缺少相关标准和检查，导致需要更新相应的文档。,https://github.com/hpcaitech/ColossalAI/issues/5343
ColossalAI,该issue是一个优化类型的问题，主要涉及替换Attention layer和MLP layer以优化权重转置操作，并添加fused_qkv和fused linear_add功能。这可能是为了提高模型训练或推理过程中的性能。,https://github.com/hpcaitech/ColossalAI/issues/5340
ColossalAI,这个issue类型是需求反馈/贡献代码，主要涉及ColossalAI项目中的PR提交流程和规范，用户提出了为了更好管理代码贡献流程所需的功能和要求。,https://github.com/hpcaitech/ColossalAI/issues/5338
ColossalAI,这个issue是用户提出的需求。主要涉及的对象是ColossalAI中推理模块的默认分词器和生成配置。导致这个问题的原因是需要更新生成配置以符合文档要求，并使分词器自动生成。,https://github.com/hpcaitech/ColossalAI/issues/5337
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI库中TorchFSDPPlugin的功能缺失导致的无法保存和加载分片模型和优化器的问题。,https://github.com/hpcaitech/ColossalAI/issues/5328
ColossalAI,这个issue类型是功能性需求提出，涉及到ColossalAI中推理模型（Inference），用户请求添加nopadding llama modeling，可能是为了改进推理模型性能。,https://github.com/hpcaitech/ColossalAI/issues/5326
ColossalAI,这个issue是一个优化请求，主要涉及ColossalAI中Blocked KVCache和相关的Kernels的修改。原因可能是为了改进性能。,https://github.com/hpcaitech/ColossalAI/issues/5325
ColossalAI,这个issue属于性能优化类型，主要涉及的对象是ColossalAI的核心代码片段。由于现有的内核需要更新和改进以提高性能，因此用户提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/5323
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI下的openmoe项目。用户询问是否openmoe项目能够训练mixtral 8x7b模型，同时提到在openmoe.md中找不到与nvidiaapex相关的检查点。,https://github.com/hpcaitech/ColossalAI/issues/5319
ColossalAI,这个issue是一个功能更新，主要涉及Rotary embedding的加速，对vLLM进行优化。,https://github.com/hpcaitech/ColossalAI/issues/5311
ColossalAI,这个issue是一个功能特性提案，主要涉及ColossalAI中的NPU功能，可能是为了增强计算性能或扩展支持的硬件设备范围。,https://github.com/hpcaitech/ColossalAI/issues/5310
ColossalAI,这个issue类型是PR提交需求，主要涉及ColossalAI的功能更新，由于要求提交前的必要检查和PR评审前的必要检查。,https://github.com/hpcaitech/ColossalAI/issues/5309
ColossalAI,这个issue是用户提交的一个优化问题，涉及ColossalAI的中间张量的使用，由于flash attn存在优化问题导致症状的bug。,https://github.com/hpcaitech/ColossalAI/issues/5304
ColossalAI,这是一个需求提出类的issue，主要涉及ColossalAI的推理功能，由于需要添加一种融合的旋转内核和获取cos缓存内核，所以创建了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/5302
ColossalAI,这是一个用户提出的需求，主要对象是ColossalAI项目下的CUDA构建系统，需要支持在没有GPU设备的主机上进行构建。导致该需求的原因是在HPC中心的构建节点可能没有GPU设备，而需要为多种架构构建项目。,https://github.com/hpcaitech/ColossalAI/issues/5300
ColossalAI,该 issue 类型是特性改进，主要涉及 ColossalAI 的扩展模块重构。导致该改进的原因可能是为了提高功能性能或提供更好的用户体验。,https://github.com/hpcaitech/ColossalAI/issues/5298
ColossalAI,这个issue是一个文档更新类型的问题，主要涉及ColossalAI的README文档。由于缺少日文翻译，用户创建了一个日文版的README。,https://github.com/hpcaitech/ColossalAI/issues/5294
ColossalAI,这个issue类型为更新请求，主要涉及的对象为ColossalChat的RLHF V2模块更新，由于开发者需要在创建PR前完成一系列检查与任务，包括添加文档、测试代码和自审查。,https://github.com/hpcaitech/ColossalAI/issues/5286
ColossalAI,这个issue是一个需求类型的issue，主要涉及ColossalAI中关于推断适应Rotary Embedding和RMS Norm的内容。由于需要在PR中添加相关的标签、图片和链接以及描述自己的工作，可能是为了提高代码的可追溯性和阅读性。,https://github.com/hpcaitech/ColossalAI/issues/5283
ColossalAI,这个issue类型是功能改进，主要对象是推断过程中的旋转嵌入和RMS范数。由于需要适应旋转嵌入和RMS范数，用户为ColossalAI提出了这项改进。,https://github.com/hpcaitech/ColossalAI/issues/5282
ColossalAI,这是一个文档更新的issue，涉及主要对象是ColossalAI的inference readme。由于缺少标签和自审查，用户提出了更新文档的请求。,https://github.com/hpcaitech/ColossalAI/issues/5280
ColossalAI,"该issue类型为PR请求，涉及""NPU分支""的同步。这是由于开发者需要同步NPU分支与主分支造成的。",https://github.com/hpcaitech/ColossalAI/issues/5278
ColossalAI,这个issue是一个用户提交的需求。主要对象是在ColossalAI中对rotary embedding性能进行基准测试，并添加一个fetch函数。由于需要优化和添加新功能，用户提交了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/5277
ColossalAI,这是一个性能优化的issue，涉及的主要对象是解码内核，因为解码阶段使用了 block kv cache（页式注意力），由于未经过修订和优化，导致了性能下降。,https://github.com/hpcaitech/ColossalAI/issues/5274
ColossalAI,该issue属于用户提出需求类型，主要涉及更新文档的问题。,https://github.com/hpcaitech/ColossalAI/issues/5271
ColossalAI,这是一个文档更新的issue，主要涉及ColossalAI的文档更新。用户创建该issue是为了按照标准格式更新文档，并提醒需完成的检查项。,https://github.com/hpcaitech/ColossalAI/issues/5269
ColossalAI,这是一个用户提出需求的类型(issue)，主要涉及使用IB 100g卡进行多节点训练，但内容并未清晰表达具体问题，需要进一步沟通和解释。,https://github.com/hpcaitech/ColossalAI/issues/5266
ColossalAI,这个issue是用户请求功能支持，主要对象是shardformer模块下的gpt2，由于需要支持dist cross entropy。,https://github.com/hpcaitech/ColossalAI/issues/5263
ColossalAI,这个issue类型是功能请求，涉及主要对象是在ColossalAI中添加RMSNorm支持的LayerNorm triton kernel。由于之前缺乏RMSNorm支持的triton kernel，导致用户需要这个功能来进行模型训练或推理。,https://github.com/hpcaitech/ColossalAI/issues/5262
ColossalAI,这个issue是一个用户提交需求的类型，主要对象是ColossalAI的PR提交流程。用户希望确保在创建PR之前完成一系列的任务，以确保PR的准确性和规范性。,https://github.com/hpcaitech/ColossalAI/issues/5257
ColossalAI,"这个issue是一个功能更新的PR，涉及ColossalAI下的一项功能""moe""的容量计算更新。原因可能是为了改进功能表现或增加容量计算功能。",https://github.com/hpcaitech/ColossalAI/issues/5253
ColossalAI,这是一个功能添加的issue，主要涉及到添加新的kernel。由于缺少no pad rotary embedding kernel，需要添加该功能来完善。,https://github.com/hpcaitech/ColossalAI/issues/5252
ColossalAI,这个issue是一个功能改进的PR，主要涉及到ColossalAI中的hybridparallelplugin，用户提出了关于支持梯度累积的需求。这可能是为了提高模型训练过程中的效率和灵活性。,https://github.com/hpcaitech/ColossalAI/issues/5246
ColossalAI,该issue是关于需求提出的，主要涉及的对象是ColossalAI中的speculative decoding功能。由于需要加速解码过程，通过小模型生成部分候选token，并由大模型验证，以提高解码速度和准确性。,https://github.com/hpcaitech/ColossalAI/issues/5245
ColossalAI,"这是一个用户提出需求类型的issue，涉及到""Design and implement TP logic""，由于缺乏TP逻辑的设计和实现，用户需要解决相关问题。",https://github.com/hpcaitech/ColossalAI/issues/5244
ColossalAI,这是一个需求类型的issue，主要涉及的对象是测试离线连续批处理功能，可能需要进行基准测试。,https://github.com/hpcaitech/ColossalAI/issues/5243
ColossalAI,这个issue类型是优化请求，主要涉及ShardFormer中的序列并行性优化，原因是为了支持多种序列并行性方式。,https://github.com/hpcaitech/ColossalAI/issues/5241
ColossalAI,这是一个用户提交的需求问题单，涉及NPU SFT训练代码，遵循ColossalAI的贡献要求，但需要进一步完善与测试。,https://github.com/hpcaitech/ColossalAI/issues/5237
ColossalAI,这是一个文档更新的Issue，涉及ColossalAI的PR创建审查和问题解决检查清单。,https://github.com/hpcaitech/ColossalAI/issues/5236
ColossalAI,这个issue是一个文档更新请求，主要涉及Colossal-LLAMA2项目中的README.md文件。由于缺少对PR创建前的检查和自审代码的完成，用户提出了更新 README.md 的请求。,https://github.com/hpcaitech/ColossalAI/issues/5233
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的Leaderboard格式。这个issue是由于Leaderboard格式不够统一和美观导致的。,https://github.com/hpcaitech/ColossalAI/issues/5231
ColossalAI,这个issue类型是代码风格优化，涉及的主要对象是Colossal-LLaMA-2的tokenizer模块的init_tokenizer.py文件。由于代码风格不符合规范，需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/5228
ColossalAI,这个issue类型是PR请求，主要涉及的对象是对代码style的polish。由于缺少遵循标准格式和相关tags，PR需要调整以符合规范要求。,https://github.com/hpcaitech/ColossalAI/issues/5227
ColossalAI,这是一个用户提出需求的 issue，主要涉及的对象是 ColossalAI 中的 shardformer 模块。用户希望在启用 flash attention 时支持使用 abili for shardformer，可能是由于当前功能不足或设计缺陷导致用户希望增加这一功能。,https://github.com/hpcaitech/ColossalAI/issues/5221
ColossalAI,这个Issue类型是需求报告，主要涉及ColossalEval的MOE支持，原因可能是希望对ColossalAI进行多模型处理能力的支持。,https://github.com/hpcaitech/ColossalAI/issues/5216
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI项目中Webtext链接缺失的问题。可能是由于更新或错误导致该链接丢失，用户希望更新该链接。,https://github.com/hpcaitech/ColossalAI/issues/5215
ColossalAI,这是一个文档更新的issue，主要涉及到更新 requirements-test.txt 文件以适配 Python 3.10 版本。,https://github.com/hpcaitech/ColossalAI/issues/5207
ColossalAI,这是一个优化代码性能的issue，涉及主要对象为shardformer模块中的linear operator，由于使用F.linear导致执行速度低下，需要替换为torch.matmul以提高执行效率。,https://github.com/hpcaitech/ColossalAI/issues/5206
ColossalAI,这是一个优化问题，主要涉及到ColossalAI中的shardformer模块。,https://github.com/hpcaitech/ColossalAI/issues/5205
ColossalAI,这个issue是一个特性需求，涉及的主要对象是ColossalAI中的llama模型，用户提出了新增基于padding输入的llama模型的需求。,https://github.com/hpcaitech/ColossalAI/issues/5202
ColossalAI,该issue是一个功能改进的PR，涉及ColossalAI中pipeline的支持任意大小的批处理模式，主要是为了支持前向传播。,https://github.com/hpcaitech/ColossalAI/issues/5201
ColossalAI,该issue类型为功能需求，主要涉及Shardformer支持梯度累积融合，旨在通过单个操作累积梯度，提高运算符带宽，避免多次访问全局内存。,https://github.com/hpcaitech/ColossalAI/issues/5199
ColossalAI,这是一个用户提出的功能需求。主要对象是ColossalAI库中的Shardformer模块。由于缺乏对梯度累积融合的支持，用户请求对Shardformer模块进行改进，以支持梯度累积融合功能。,https://github.com/hpcaitech/ColossalAI/issues/5198
ColossalAI,这个issue是一个功能改进类型的问题，主要涉及到Colossal_AI中评估指标选择逻辑的改进。由于原逻辑选取指标存在问题，导致选择不准确，需要改进。,https://github.com/hpcaitech/ColossalAI/issues/5196
ColossalAI,该issue类型属于实现特征请求，涉及主要对象为ColossalAI中的triton kernels，用户希望实现与相关子模块和KVCache兼容的triton kernels用于推理。,https://github.com/hpcaitech/ColossalAI/issues/5193
ColossalAI,这是一个关于更新推理配置和修复测试的issue，主要涉及ColossalAI下的推理功能结构的重命名。可能是由于不正确的配置文件路径和结构命名导致的bug。,https://github.com/hpcaitech/ColossalAI/issues/5178
ColossalAI,这个issue类型是文档更新，涉及主要对象为ColossalAI的文档。由于PyTorch版本需要更新，因此用户提出了需要更新文档的请求。,https://github.com/hpcaitech/ColossalAI/issues/5177
ColossalAI,这个issue属于功能需求类型，主要涉及ColossalAI中的shardformer模块，用户正在提出关于llama支持DistCrossEntropy的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/5176
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI库下的shardformer模块的llama支持DistCrossEntropy，用户可能希望增加这一功能来改进模型的训练效果。,https://github.com/hpcaitech/ColossalAI/issues/5175
ColossalAI,这是一个功能开发类的issue，涉及对象是针对ColossalAI中推理引擎逻辑的更新。由于缺乏正确的PR提交前检查清单，可能导致代码实现问题或测试不完备。,https://github.com/hpcaitech/ColossalAI/issues/5173
ColossalAI,这是一个功能更新的issue，涉及主要对象为ColossalAI的npu模块。用户通过引入extension for op builder、完成cpuadam和flashattention、替换调用cpuadam和flashattention为新API来改进功能。,https://github.com/hpcaitech/ColossalAI/issues/5171
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI项目中的ColossalEval模块，要求支持GSM、数据泄漏评估和张量并行计算。由于缺乏相关功能，用户提出了这项改进需求。,https://github.com/hpcaitech/ColossalAI/issues/5169
ColossalAI,这是一个用户提出需求的类型的issue，主要对象是要设计和实现一个Llama模型的演示。,https://github.com/hpcaitech/ColossalAI/issues/5168
ColossalAI,这是一个需求类型的issue，主要涉及到设计和实现推理引擎，用户需要添加推理引擎的逻辑。,https://github.com/hpcaitech/ColossalAI/issues/5167
ColossalAI,这个issue是一个功能增加类型的issue，主要涉及到ColossalAI的推断功能，具体是为添加logit processor和request handler而创建的。这个issue涉及到的主要对象是推断过程中的logit处理和请求处理。,https://github.com/hpcaitech/ColossalAI/issues/5166
ColossalAI,"这是一个功能性问题，涉及的主要对象是""NPU""，该问题源于使用扩展来构建操作的需求。",https://github.com/hpcaitech/ColossalAI/issues/5165
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中flash attention的导入设置问题。,https://github.com/hpcaitech/ColossalAI/issues/5157
ColossalAI,这是一个关于新增功能的issue，涉及到ColossalAI中的CacheBlock和KV-Cache Manager。由于缺乏这些结构导致需要管理逻辑缓存块和实际KV缓存，这可能是为实现更高效的缓存管理而提出的需求。,https://github.com/hpcaitech/ColossalAI/issues/5156
ColossalAI,这是一个缺少详细描述的需求提案issue，主要涉及设计和实现新的kv-cache功能。,https://github.com/hpcaitech/ColossalAI/issues/5154
ColossalAI,这个issue类型是功能需求，主要涉及到ColossalAI中低级零支持LoRa。由于缺乏低级零支持LoRa，用户需要此功能来实现特定的通信需求。,https://github.com/hpcaitech/ColossalAI/issues/5153
ColossalAI,这个issue类型是功能需求，主要对象是ColossalAI中的LowLevelZero支持。由于需要减少内存使用，用户提出需要支持LoRa。,https://github.com/hpcaitech/ColossalAI/issues/5152
ColossalAI,这是一个需求提出的issue，主要涉及到ColossalAI的推理功能。由于缺少BatchInferState、Sequence和InferConfig导致需要添加这些功能。,https://github.com/hpcaitech/ColossalAI/issues/5149
ColossalAI,这个issue类型是功能优化，主要涉及ColossalChat中的coati重构，PPO、LORA修复以及DPO添加。由于升级加速方法、支持张量并行等，存在需要协调更改以及测试的情况。,https://github.com/hpcaitech/ColossalAI/issues/5148
ColossalAI,这个issue是一个特性请求，涉及到ColossalAI的推理功能，请求增加readme并完善请求处理程序。造成此特性请求的原因可能是现有功能不够完善，需要添加文档并优化请求处理。,https://github.com/hpcaitech/ColossalAI/issues/5147
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI项目的基本结构和说明文档的设计。,https://github.com/hpcaitech/ColossalAI/issues/5145
ColossalAI,这是一个用户提出需求的类型，主要涉及Colossal-Infer项目，用户提出了一个提案并希望得到自助服务。,https://github.com/hpcaitech/ColossalAI/issues/5144
ColossalAI,这是一个用户提出需求的类型的issue，主要对象是“activation checkpoint offload”，用户询问如何打开该功能。,https://github.com/hpcaitech/ColossalAI/issues/5142
ColossalAI,这是一个关于创建并请求审查的PR的issue，主要涉及ColossalAI代码贡献者针对代码规范和质量的检查，并要求进行自审查、添加测试和文档说明。,https://github.com/hpcaitech/ColossalAI/issues/5141
ColossalAI,这是一个功能改进的issue，主要涉及到ColossalAI库中的npu（Neural Processing Unit）模块，用户提出了将设备更改为加速器API的需求。,https://github.com/hpcaitech/ColossalAI/issues/5140
ColossalAI,该issue类型为feature添加请求，主要涉及 refactor server and webui & add new feature，可能是由于对ColossalAI项目的需求或者改进而引起的。,https://github.com/hpcaitech/ColossalAI/issues/5138
ColossalAI,这个issue属于功能修改，主要涉及到pipeline中P2P通信、元数据缓存和测试，可能是由于通信问题导致的bug或者用户需求修改。,https://github.com/hpcaitech/ColossalAI/issues/5132
ColossalAI,这是一个文档更新的问题，主要涉及更新论文引用的bibtex，可能是因为最新的论文引用信息需要被更新而引起的。,https://github.com/hpcaitech/ColossalAI/issues/5131
ColossalAI,这个issue类型是新增功能需求，主要涉及的对象是ColossalAI的加速器模块。由于需要为不同的硬件平台提供一个抽象层，所以新增了这个功能。,https://github.com/hpcaitech/ColossalAI/issues/5129
ColossalAI,这个issue是关于文档更新，主要对象是ColossalAI的PR流程，可能由于需要提高文档的可读性和标准性而产生。,https://github.com/hpcaitech/ColossalAI/issues/5128
ColossalAI,这是一个需求或者功能改进的issue，主要涉及ColossalAI中的未使用代码和colo_init_context函数的弃用，由于未使用代码和功能弃用的存在，可能导致系统冗余或性能问题。,https://github.com/hpcaitech/ColossalAI/issues/5117
ColossalAI,该issue类型为功能需求，主要涉及向ColossalEval中添加安全性评估数据集。由于需要增加安全性评估数据集到ColossalEval，因此提出了此问题。,https://github.com/hpcaitech/ColossalAI/issues/5115
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中ColossalEval缺少安全评估数据集的问题。导致该问题的原因可能是需要丰富ColossalEval的安全评估数据集。,https://github.com/hpcaitech/ColossalAI/issues/5114
ColossalAI,这个issue类型是代码优化和修正拼写错误，主要涉及的对象是PR（Pull Request）。原因可能是为了提高代码质量和可读性。,https://github.com/hpcaitech/ColossalAI/issues/5112
ColossalAI,这是一个撤销某项功能的issue，主要涉及ColossalAI的Mistral支持，由于特定的原因导致了需要撤销添加Mistral支持这一功能。,https://github.com/hpcaitech/ColossalAI/issues/5107
ColossalAI,这是一个用户提交需求的 issue，主要涉及支持在hybrid plugin中为llama添加spda attention的功能。原因是需要在shardformer中为GPU和NPU选择相应的attention kernel。,https://github.com/hpcaitech/ColossalAI/issues/5106
ColossalAI,该issue为功能改进类，涉及对象为ColossalAI下的shardformer。导致该issue产生的原因是缺少Mistral的测试，并解决了这一问题。,https://github.com/hpcaitech/ColossalAI/issues/5105
ColossalAI,这是一个需求类型的issue，主要对象是项目中的shardformer模块，用户提出需要为mistral添加测试。,https://github.com/hpcaitech/ColossalAI/issues/5104
ColossalAI,该issue类型为需求，主要涉及的对象是ColossalAI中的模型推断（inference），由于缺乏模型前向准确性测试而导致的问题。,https://github.com/hpcaitech/ColossalAI/issues/5102
ColossalAI,这个issue类型是需求提出，主要对象是ColossalEval，用户提出缺乏安全评估数据集的问题。,https://github.com/hpcaitech/ColossalAI/issues/5094
ColossalAI,这是一个关于代码优化和问题跟踪的issue，涉及对象是ColossalAI中的GPTJ和Falcon，由于并行处理问题导致bug需要进行热更新。,https://github.com/hpcaitech/ColossalAI/issues/5093
ColossalAI,这是一个用户提出需求的issue，主要对象是关于ColossalAI中基于all-to-all通讯的序列并行。,https://github.com/hpcaitech/ColossalAI/issues/5091
ColossalAI,这个issue类型为功能增强，主要涉及ColossalAI的NPU支持插件和LLAMA，由于缺乏NPU支持，用户寻求添加混合插件和LLAMA的支持。,https://github.com/hpcaitech/ColossalAI/issues/5090
ColossalAI,这个issue属于需求类，主要涉及更新Colossal-Inference的用户指南和开发文档，由于之前的文档需要进行修订和完善。,https://github.com/hpcaitech/ColossalAI/issues/5086
ColossalAI,该issue类型为贡献新功能，主要涉及ColossalAI的推理部分，用户希望添加gptq和smoothquant性能基准测试。产生这个issue的原因是为了改进ColossalAI的性能评估和提升用户体验。,https://github.com/hpcaitech/ColossalAI/issues/5082
ColossalAI,该issue是一个PR请求，涉及到Docker中增加用于推理的dockerfile。,https://github.com/hpcaitech/ColossalAI/issues/5081
ColossalAI,这个issue是一个质量改进类问题，主要涉及ColossalAI下的推理代码示例重构和修复时间表，由于缺少必要的标准化和检查，导致代码质量和可维护性下降。,https://github.com/hpcaitech/ColossalAI/issues/5077
ColossalAI,这是一个更新issue，主要涉及ColossalAI中推理引擎名称更新和示例整合的问题。原因可能是为了提高命名一致性和示例文件的整理。,https://github.com/hpcaitech/ColossalAI/issues/5073
ColossalAI,这是一个需求整理的issue，主要涉及ColossalAI仓库中的子模块更新。原因可能是需要更新内容或移除过时的子模块。,https://github.com/hpcaitech/ColossalAI/issues/5070
ColossalAI,这是一个需求提出的issue，主要涉及到为Gemini和Zero添加NPU支持。这个需求是基于需要在ColossalAI中添加对Gemini和Zero的NPU支持而提出的。,https://github.com/hpcaitech/ColossalAI/issues/5067
ColossalAI,这个issue是一个功能性问题，涉及主要对象为ColossalAI中的Kernel。原因是需要将基于lightllm的triton flash-decoding添加到当前的llama forward中。,https://github.com/hpcaitech/ColossalAI/issues/5059
ColossalAI,该issue类型为功能需求，主要涉及的对象是ColossalAI中的TensorRT-LLM模型封装。由于缺乏TensorRTLLM引擎的接口，用户提出了为推理模型添加模型封装的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/5058
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中gemini插件中如何使用tensor并行。主要原因是用户想了解在gemini插件中如何使用张量并行的方法。,https://github.com/hpcaitech/ColossalAI/issues/5054
ColossalAI,这个issue是一个需求类型，主要涉及的对象是ColossalAI推断功能。原因可能是要更新推断功能的示例。,https://github.com/hpcaitech/ColossalAI/issues/5053
ColossalAI,这个issue是用户提交的需求报告，主要涉及的对象是ColossalAI的NPU gemini支持，由于缺乏gemini的NPU支持，用户提出需要为其添加相应功能支持。,https://github.com/hpcaitech/ColossalAI/issues/5052
ColossalAI,这个issue属于文档更新类型，主要对象是更新推理模块的Readme，可能是由于之前的文档不完整或过时导致。,https://github.com/hpcaitech/ColossalAI/issues/5051
ColossalAI,这个issue类型是需求报告，主要涉及的对象是triton kernels的更新。由于triton kernels需要升级至2.1.0版本，导致需要重构triton contextattention kernels以兼容2.0.0和2.1.0版本。,https://github.com/hpcaitech/ColossalAI/issues/5046
ColossalAI,这个issue是一个功能需求的提出，主要对象是Gemini模块，用户希望支持额外的dp参数。,https://github.com/hpcaitech/ColossalAI/issues/5042
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中的优化器，用户想要替换Adam优化器为SGD优化器，因为缺少足够的GPU资源但有时间调整超参数。,https://github.com/hpcaitech/ColossalAI/issues/5037
ColossalAI,该issue类型为功能增强，涉及的主要对象是为ColossalAI添加Tensorboard支持，由于缺乏相关功能，开发者正在寻求添加Tensorboard支持的帮助。,https://github.com/hpcaitech/ColossalAI/issues/5036
ColossalAI,该issue类型为技术优化（Refactor），主要涉及到ColossalAI项目中的策略搜索和量化类型控制。由于CI的问题可能导致任务在本地通过但在CI上失败。,https://github.com/hpcaitech/ColossalAI/issues/5035
ColossalAI,这个issue类型为特性添加请求，涉及主要对象为ColossalAI库中的函数create_ep_hierarchical_group。导致这个问题的原因可能是为了增强这个函数的功能或者改进其性能。,https://github.com/hpcaitech/ColossalAI/issues/5032
ColossalAI,这个issue属于功能需求，主要涉及ShardConfig的优化，添加了支持使用extra_kwargs的功能。原因可能是为了提高配置的灵活性和可定制性。,https://github.com/hpcaitech/ColossalAI/issues/5031
ColossalAI,这是一个用户提出需求的问题，主要涉及到在多机多卡训练过程中遇到NCCL timeout超时，并希望能够实现训练自动重启并从已保存的最新模型恢复。,https://github.com/hpcaitech/ColossalAI/issues/5028
ColossalAI,这是一个用户提出需求的issue，主要对象是Colossal-LLaMA-2中的文本预处理功能，用户反映长文本会被截断，需要对长文本进行分段处理。,https://github.com/hpcaitech/ColossalAI/issues/5026
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalEval模块的mtbench功能支持，因为需要支持单一评分判断。,https://github.com/hpcaitech/ColossalAI/issues/5025
ColossalAI,这个issue属于代码优化类型，主要涉及ColossalAI中模块的合并，由于要优化推断过程，提出了合并chatglm2模块与pp和tp模块的需求。,https://github.com/hpcaitech/ColossalAI/issues/5023
ColossalAI,这是一个需求类型的issue， 主要涉及ColossalAI代码库中有关推理代码的重构。其原因可能是为了简化代码结构或提高性能。,https://github.com/hpcaitech/ColossalAI/issues/5022
ColossalAI,该issue属于功能增强类型，主要涉及Optimizer的checkpoint支持，用户请求更新测试脚本。造成此问题的原因可能是用户希望在ColossalAI中实现对Optimizer的checkpoint功能。,https://github.com/hpcaitech/ColossalAI/issues/5015
ColossalAI,该issue类型为用户提出需求，用户在ColossalLLaMA2项目中寻求有关Llama2 continual pretraining性能评估脚本的支持。,https://github.com/hpcaitech/ColossalAI/issues/5013
ColossalAI,该issue类型是贡献者提交代码优化的请求，主要涉及到GPTQ Llama和Smoothquant Llama的重构，可能由于原有代码结构不够清晰或效率不高导致需要进行重构。,https://github.com/hpcaitech/ColossalAI/issues/5012
ColossalAI,这个issue是一个需求任务，主要涉及Gemini插件的支持Lora功能，由于标签未添加导致需求追踪和PR区分不清晰。,https://github.com/hpcaitech/ColossalAI/issues/5001
ColossalAI,该issue是一个功能修正，主要涉及的对象是ColossalAI的模型推理功能，由于支持TP when PP=1。,https://github.com/hpcaitech/ColossalAI/issues/4998
ColossalAI,这是一个关于文档更新的issue，主要对象是ColossalAI的hybrid parallel plugin，由于缺少支持功能图表，需要更新文档以提供更清晰的指导。,https://github.com/hpcaitech/ColossalAI/issues/4996
ColossalAI,这个issue属于发布相关的更新问题，主要涉及到ColossalAI的PR创建和问题追踪。问题可能是为了确保PR与已有issue相关联并满足标准化要求。,https://github.com/hpcaitech/ColossalAI/issues/4995
ColossalAI,这是一个关于合并pp和tp的功能改进的issue，主要涉及ColossalAI的Pipeline Inference，需解决的问题是合并pp和tp，可能是为了优化功能或减少重复代码。,https://github.com/hpcaitech/ColossalAI/issues/4993
ColossalAI,这个issue是一个关于更新ColossalAI文档的类型，涉及的主要对象是项目的文档。用户可能提出这个问题是为了规范PR创建流程和文档更新要求。,https://github.com/hpcaitech/ColossalAI/issues/4989
ColossalAI,这个issue属于文档更新类型，涉及到更新推理文档。由于缺少更新，需要提供更清晰的文档来帮助用户正确使用功能。,https://github.com/hpcaitech/ColossalAI/issues/4988
ColossalAI,这是一个用户提出需求的issue，主要涉及Colossal-LLaMA-2中知识重放阶段的细节及数据集使用问题，由于用户想要继续使用自己的领域数据进行LLaMA-2的预训练，但Meta未发布预训练数据，因此需要了解知识重放步骤的详细信息和使用的数据集。,https://github.com/hpcaitech/ColossalAI/issues/4985
ColossalAI,这个issue类型是特性请求，主要涉及的对象是ColossalAI中的Lora APIs和TorchDDP，由于缺乏Lora特性支持，导致用户无法使用相关功能。,https://github.com/hpcaitech/ColossalAI/issues/4981
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI支持moe和OpenMoE模型的功能。,https://github.com/hpcaitech/ColossalAI/issues/4979
ColossalAI,这是一个用户需求类的issue，主要涉及对象是大型文本数据集构建方法，用户提出了关于在ColossalLlama的27B模型上训练大量长文本数据集的问题。,https://github.com/hpcaitech/ColossalAI/issues/4974
ColossalAI,这是一个用户需求问题，主要对象是希望将一个booster checkpoint转换为huggingface checkpoint，原因是想在另一个repo中测试该checkpoint而不使用booster插件。,https://github.com/hpcaitech/ColossalAI/issues/4969
ColossalAI,这是一个用户提出需求的issue，主要涉及数据截断功能的改进，用户希望提供数据分割而非截断的选项。,https://github.com/hpcaitech/ColossalAI/issues/4968
ColossalAI,该issue是用户提出需求的类型，主要涉及ColossalAI中PyTorch 2.+所需的C++版本问题，用户希望了解Ninja所需的C++版本。Bug或需求缘由可能是PyTorch版本升级导致的相关问题。,https://github.com/hpcaitech/ColossalAI/issues/4967
ColossalAI,这个issue是一个需求类型的提交，涉及到配置CUDA_VISIBLE_DEVICES时设置设备的问题，由于缺少相关设置，导致用户无法正确指定设备。,https://github.com/hpcaitech/ColossalAI/issues/4966
ColossalAI,这是一个更新代码并增加新功能的issue，涉及ColossalAI的内核更新和添加闪存解码功能，用户提出希望将最新版本的Triton内核（2.1.0）添加到推理系统中。,https://github.com/hpcaitech/ColossalAI/issues/4965
ColossalAI,这是一个用户提出需求的 issue，主要涉及如何在较少资源的情况下持续预训练 Llama 2 70B 模型。原因是作者认为当前的硬件资源要求过于昂贵。,https://github.com/hpcaitech/ColossalAI/issues/4964
ColossalAI,该issue类型为用户提出需求，主要涉及的对象是Data Processing Toolkit。由于用户想了解Data Processing Toolkit的发布时间，表明用户希望在ColossalAI中使用数据处理工具。,https://github.com/hpcaitech/ColossalAI/issues/4960
ColossalAI,该issue为需求提出类型，主要对象是moe训练脚本，用户期望改进openmoe参数接口。,https://github.com/hpcaitech/ColossalAI/issues/4959
ColossalAI,该issue为一个功能提案，主要涉及将Python替换为Mojo的支持。,https://github.com/hpcaitech/ColossalAI/issues/4957
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI。由于用户希望在实时中支持wandb和/或显示TFLOPS，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4956
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的训练过程中内存占用过大的问题，并希望支持int8/int4训练以解决这一问题。,https://github.com/hpcaitech/ColossalAI/issues/4955
ColossalAI,该issue类型是一个新功能提交，主要涉及的对象是Shardformer。由于提交者未在PR创建前针对问题创建issue，并且未按照相关要求添加标签和标题，可能导致审核过程中的追踪和理解难度增加。,https://github.com/hpcaitech/ColossalAI/issues/4954
ColossalAI,这是一个需求提出的issue，主要对象是ColossalAI的推理功能。由于需要支持动态批量推理，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4953
ColossalAI,这个issue类型是功能增强，涉及的主要对象是ColossalAI代码库。因为用户提出了为代码进行优化和添加新功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/4952
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中推理模块的动态批处理功能。原因是为了增加推理功能，通过在线和离线管理器实现动态批处理。,https://github.com/hpcaitech/ColossalAI/issues/4949
ColossalAI,这个issue是针对代码合并和更新相关测试的任务，属于非bug报告类型，主要涉及ColossalAI中的模型组件。,https://github.com/hpcaitech/ColossalAI/issues/4945
ColossalAI,该issue类型为对代码的优化或重构，主要涉及集成了一些lightllm内核到推理系统中。由于特定的内核功能需要被集成到推理系统中，因此提交了这个PR。,https://github.com/hpcaitech/ColossalAI/issues/4943
ColossalAI,这个issue是一个特性需求，涉及Gemini框架的支持张量并行计算，可能由于Gemini框架在此之前未能有效支持张量并行计算而导致。,https://github.com/hpcaitech/ColossalAI/issues/4942
ColossalAI,这是一个用户提出的需求类型的issue，主要对象是ColossalAI项目中的gemini模块。通过分析发现，用户提出了对 gemini 模块支持张量并行性的需求。,https://github.com/hpcaitech/ColossalAI/issues/4941
ColossalAI,这是关于功能需求的issue，涉及PipelineP2PCommunication模块为什么使用broadcast_object而不是send/recv API，原因可能是相关功能的设计选择或需求。,https://github.com/hpcaitech/ColossalAI/issues/4939
ColossalAI,这个issue是一个功能性需求，主要涉及到ColossalAI的Pipeline inference的优化问题，用户希望将kvcache与pipeline inference结合起来。这个需求的提出可能是为了提高推理效率或者降低资源占用等方面的问题。,https://github.com/hpcaitech/ColossalAI/issues/4938
ColossalAI,这个issue是一个功能更新的PR，涉及ColossalAI的新的Async engine的测试，主要对象是该异步引擎的功能。,https://github.com/hpcaitech/ColossalAI/issues/4935
ColossalAI,这个issue是一个功能需求，主要涉及ColossalAI中的bloom模型支持动态批处理和is_running函数。原因可能是用户希望能够在模型推断过程中动态调整批次大小，并且添加了一个用于检查模型是否正在运行的函数。,https://github.com/hpcaitech/ColossalAI/issues/4933
ColossalAI,这个issue类型是功能更新，主要涉及的对象是对bloom模型和is_running函数的支持，导致此问题的原因是需要在ColossalAI中支持动态批处理。,https://github.com/hpcaitech/ColossalAI/issues/4932
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及到ColossalAI中llama 2 70b的预训练/微调所需的最小CPU内存和GPU内存。提问者想要了解由于硬件要求的限制，预训练/微调llama 2 70b所需的最低硬件配置是什么。,https://github.com/hpcaitech/ColossalAI/issues/4931
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的llama2模型。由于ColossalAI需要PyTorch的checkpoint才能继续预训练，用户无法加载通过meta获得的llama2权重或者转换后的Hugging Face权重，询问如何解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4930
ColossalAI,这个issue类型为用户提出需求，主要涉及对象为Gemini项目，用户提出了支持LoRa的需求。,https://github.com/hpcaitech/ColossalAI/issues/4929
ColossalAI,这个issue是功能增强类型，主要涉及ColossalAI项目中的moe模块。导致这个问题的原因是需要对moe模块进行功能增强以支持bf16格式以及更新基准测试。,https://github.com/hpcaitech/ColossalAI/issues/4923
ColossalAI,这是一个功能改进的issue，主要涉及到ColossalAI的推断处理引擎的异步批处理功能，旨在优化测试过程。,https://github.com/hpcaitech/ColossalAI/issues/4917
ColossalAI,该issue类型为功能需求，主要涉及ColossalAI中moe模块的load balance功能支持。此需求可能由于当前系统缺乏load balance功能，导致部分资源负载不均衡或性能问题。,https://github.com/hpcaitech/ColossalAI/issues/4914
ColossalAI,这个issue是一个功能请求，主要涉及ColossalAI中的Ray Distributed Environment Init Scripts。用户请求添加Ray分布式环境初始化脚本。,https://github.com/hpcaitech/ColossalAI/issues/4910
ColossalAI,这是一个用户提出需求的issue，主要涉及基于codellama的continual training功能是否可行，由于codellama和llama2在数据预处理和fine tuning方式上的区别，可能会影响到continual training的实现。,https://github.com/hpcaitech/ColossalAI/issues/4906
ColossalAI,这个issue类型是功能性需求提出，主要涉及ColossalAI下的inference模块，用户提出了添加smoothquant功能以支持Llama模型和对量化模型进行保存和加载checkpoint的需求。,https://github.com/hpcaitech/ColossalAI/issues/4904
ColossalAI,这个issue是关于适配ray serve的需求，涉及主要对象是ColossalAI的代码库。原因可能是要将ColossalAI适配到ray serve上。,https://github.com/hpcaitech/ColossalAI/issues/4897
ColossalAI,这个issue是一个功能需求，主要涉及的对象是ColossalAI中的CPU Adam算法，用户寻求支持纯FP16。由于缺乏对CPU Adam纯FP16的支持，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4896
ColossalAI,这个issue类型是功能改进，主要涉及到ColossalAI中smoothquant模块的调整。这个改进是由于对smoothquant功能的添加和删除无用代码而引起的。,https://github.com/hpcaitech/ColossalAI/issues/4895
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及ColossalAI的inference功能，通过将循环改为异步样式以使其可控。,https://github.com/hpcaitech/ColossalAI/issues/4894
ColossalAI,"这是一个功能需求类型的 issue，主要涉及的对象是 ColossalAI 中的模块""moe""。由于未支持稀疏矩阵乘操作，用户提出了需要支持稀疏矩阵乘运算的需求。",https://github.com/hpcaitech/ColossalAI/issues/4893
ColossalAI,该issue类型为文档更新，主要涉及ColossalAI中的混合Adam问题，由于未添加适当的标签，可能导致跟踪和区分不同PR时出现困难。,https://github.com/hpcaitech/ColossalAI/issues/4889
ColossalAI,这个issue类型是功能需求，主要涉及的对象是shardformer模块。用户提出该需求，希望shardformer模块支持falcon。,https://github.com/hpcaitech/ColossalAI/issues/4883
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的项目shardformer支持falcon。由于用户需要shardformer支持falcon，因此提出了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/4882
ColossalAI,这个issue是用户提出需求，添加Colossal-Llama-2微调示例。由于缺少示例，用户提出了需要添加微调Colossal-Llama-2的请求。,https://github.com/hpcaitech/ColossalAI/issues/4878
ColossalAI,这个issue属于功能需求，涉及到ColossalAI中的moe模块，主要目的是支持混合的零策略，原因可能是为了提高模型的性能和灵活性。,https://github.com/hpcaitech/ColossalAI/issues/4877
ColossalAI,这个issue属于文档更新类，主要涉及更新ColossalAI项目中README.md文件中的modelscope链接。原因可能是旧链接失效或者需要更新到最新版本。,https://github.com/hpcaitech/ColossalAI/issues/4876
ColossalAI,这是一个功能需求，并且主要涉及的对象是ColossalAI项目中的Mixture of Experts模块。这个需求是希望支持混合零策略，并且需要进行相应的代码和单元测试，以支持这一新功能。,https://github.com/hpcaitech/ColossalAI/issues/4874
ColossalAI,这个issue是一个需求提议，主要涉及ColossalAI中Gemini模块的支持AMP O3功能，因为需要添加`master_weights`参数，旨在增强Gemini的功能性。,https://github.com/hpcaitech/ColossalAI/issues/4872
ColossalAI,这是一个用户提出需求的issue，主要涉及Gemini模型的AMP支持。这个问题是由于Gemini当前的AMP只支持O2，而不支持O3（没有主要权重），用户请求添加一个`master_weights`参数以支持O3。,https://github.com/hpcaitech/ColossalAI/issues/4871
ColossalAI,这个issue类型是需求提出，主要涉及的对象是hybrid parallel plugin，用户希望实现gradient accumulation来解决相关问题。,https://github.com/hpcaitech/ColossalAI/issues/4870
ColossalAI,这个issue属于功能需求，主要涉及gemini模块的支持梯度累计，由于用户需要在gemini中实现梯度累积的功能。,https://github.com/hpcaitech/ColossalAI/issues/4869
ColossalAI,这个issue属于改进文档（doc）类型，涉及主要对象为更新高级教程、通过混合并行性训练GPT。原因可能是为了改进学习体验或提升用户使用效果。,https://github.com/hpcaitech/ColossalAI/issues/4866
ColossalAI,这个issue属于用户提出需求类型，涉及ColossalAI中高级教程的更新，可能是用户希望更新提供更丰富详尽的教程内容。,https://github.com/hpcaitech/ColossalAI/issues/4865
ColossalAI,这个issue类型是功能开发，涉及主要对象为smoothquant llama，因为用户提出了新增smoothquant llama功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/4861
ColossalAI,这个issue是一个功能需求反馈类型，主要涉及的对象是需要增加Flash Attention 2这一功能，原因可能是为了改进或扩展现有的注意力机制。,https://github.com/hpcaitech/ColossalAI/issues/4859
ColossalAI,这个issue是一个feature改进提案，涉及到ColossalAI中torchrec模块中模型输出转换代码的优化。这个issue的提出可能是为了优化模型输出的处理逻辑，同时更新了Booster Plugins文档以澄清low_level_zero插件支持的当前模型情况。,https://github.com/hpcaitech/ColossalAI/issues/4856
ColossalAI,这个issue属于一个功能新增类型，主要涉及到在smoothquant llama mlp中添加silu和线性融合功能。可能由于要增加新功能或优化模型准确性而引起该问题。,https://github.com/hpcaitech/ColossalAI/issues/4852
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中的MOE（Multi-Objective Evolution）模块，用户提出需要支持专家TP的重叠。,https://github.com/hpcaitech/ColossalAI/issues/4851
ColossalAI,这是一个新增功能的issue，主要涉及ColossalAI中smoothquant模块的llama attention新增。,https://github.com/hpcaitech/ColossalAI/issues/4850
ColossalAI,该issue属于用户提出需求类型，针对ColossalAI中训练过程中的第二和第三步进行详细技术细节的提问。,https://github.com/hpcaitech/ColossalAI/issues/4845
ColossalAI,这个issue是一个功能需求，主要涉及ColossalAI中添加int8 rotary embedding kernel for smoothquant，并且已经添加了相应的测试。原因是为了改进推理能力。,https://github.com/hpcaitech/ColossalAI/issues/4843
ColossalAI,这个issue属于需求提交类型，主要涉及Colossal Inference的Serving示例添加与Ray Serve的多GPU支持。,https://github.com/hpcaitech/ColossalAI/issues/4841
ColossalAI,这个issue是一个功能建议，涉及的主要对象是hybrid_parallel_plugin，提出的问题是缺少clip_grad_norm功能。,https://github.com/hpcaitech/ColossalAI/issues/4837
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI以支持Mistral模型为目的。由于缺少相关测试和文档注释，导致需要更多贡献者参与完善这部分功能。,https://github.com/hpcaitech/ColossalAI/issues/4836
ColossalAI,这是一个用户提出需求的issue，主要涉及支持ColossalAI下的Mistral模型。由于最新发布的Mistral模型表现出色，用户希望在Shareformer中支持这一强大的模型。,https://github.com/hpcaitech/ColossalAI/issues/4835
ColossalAI,这个issue属于功能添加类型，主要涉及ColossalAI中的shardformer，可能由于需要支持interleaved pipeline parallel导致提出。,https://github.com/hpcaitech/ColossalAI/issues/4834
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是ColossalAI中的shardformer功能。这个问题是由于shardformer目前仅支持非交错的1f1b管道并行模式，用户希望支持交错的管道并行模式。,https://github.com/hpcaitech/ColossalAI/issues/4833
ColossalAI,这个issue类型是文档更新，涉及主要对象是ColossalChat README，由于未按照标准格式更新README文档，导致需要更新以符合规范。,https://github.com/hpcaitech/ColossalAI/issues/4832
ColossalAI,这是一个特性请求issue，主要涉及的对象是推理中的动态批处理（Dynamic Batching），用户提出了关于实现单个GPU和多个GPU的动态批处理的需求。,https://github.com/hpcaitech/ColossalAI/issues/4831
ColossalAI,该issue属于功能完善类型，主要对象是ColossalAI的批处理管理器。由于缺少必要的PR创建前检查和请求审查前检查，导致问题在功能完善过程中存在不完整。,https://github.com/hpcaitech/ColossalAI/issues/4827
ColossalAI,这个issue是一个特性需求，主要涉及ColossalAI的模型分片功能，由于要支持GPTJ模型的自动分片，但暂时不支持懒初始化，导致了这个需求的提出。,https://github.com/hpcaitech/ColossalAI/issues/4825
ColossalAI,这是一个关于更新Slack链接的文档问题，内容包括PR准备的清单、问题追踪和请求审核的步骤检查。,https://github.com/hpcaitech/ColossalAI/issues/4823
ColossalAI,这个issue类型是功能增强请求，主要涉及到ColossalAI中的GPTQ内核，用户提出了希望为GPTQ内核添加自动调优功能。,https://github.com/hpcaitech/ColossalAI/issues/4822
ColossalAI,这是一个用户提出需求的issue，涉及更新Colossal Llama和ColossalEval的readme。由于需要添加Qwen7B和internlm20b的结果以及在Colossal Llama中添加原始Qwen7B结果，触发了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4821
ColossalAI,该issue类型为功能添加，主要涉及ColossalAI添加基于RAY的在线推理框架以支持多卡在线推理。由于需要支持多卡在线推理，因此需要添加相应的服务器支持。,https://github.com/hpcaitech/ColossalAI/issues/4819
ColossalAI,此issue类型为提出需求，主要对象是ColossalAI的low level zero plugin，原因是为支持不存储master权重选项。,https://github.com/hpcaitech/ColossalAI/issues/4816
ColossalAI,这个issue类型为代码风格优化，涉及主要对象为测试文件test_gptq_linear.py。由于代码风格不符合标准要求，需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/4813
ColossalAI,这是一个关于代码优化的issue，主要涉及ColossalAI项目的代码规范和质量管理。,https://github.com/hpcaitech/ColossalAI/issues/4812
ColossalAI,这是一个代码质量优化的issue，主要涉及ColossalAI下的tensor_parallel/policies/llama.py文件。由于开发人员在代码风格上的要求提高，需要对这个文件进行进一步的优化。,https://github.com/hpcaitech/ColossalAI/issues/4811
ColossalAI,这个issue为功能开发类型，涉及ColossalAI项目中Pull Request创建前的检查清单和提交前的审查要求，用户提出了关于提交PR前的规范性审核和自查的问题。,https://github.com/hpcaitech/ColossalAI/issues/4809
ColossalAI,这个issue是文档更新类型，涉及主要对象为ColossalAI代码库的lazy init功能。由于缺少文档说明，用户提出了添加lazy init文档的需求。,https://github.com/hpcaitech/ColossalAI/issues/4808
ColossalAI,这个issue类型是用户提出需求，涉及主要对象为gemini plugin，用户在寻求支持gradient accumulation功能的帮助。,https://github.com/hpcaitech/ColossalAI/issues/4806
ColossalAI,这是一个文档更新的issue，主要涉及Colossal-LLaMA-2的README文件。,https://github.com/hpcaitech/ColossalAI/issues/4805
ColossalAI,这是一个用户提出需求的类型，主要对象是Colossal-LLaMA-2数据集。用户询问是否会开源用于Colossal-LLaMA-2的数据集。,https://github.com/hpcaitech/ColossalAI/issues/4803
ColossalAI,这个issue类型是用户提出需求，主要对象是ColossalAI下的LazyTensor模块。由于LazyTensor模块缺乏对`from_pretrained()`方法的支持，导致用户无法使用lazy init context来初始化。,https://github.com/hpcaitech/ColossalAI/issues/4801
ColossalAI,这个issue属于代码风格优化类，并与ColossalAI相关。这个issue主要涉及到PR提交前的规范检查及附加信息，并可能由于代码风格不符合要求导致提交问题。,https://github.com/hpcaitech/ColossalAI/issues/4799
ColossalAI,这个issue是一个贡献者提交的代码风格优化请求，涉及ColossalAI下的CUDA native模块中的线性GPTQ模块，请求符合代码规范要求。,https://github.com/hpcaitech/ColossalAI/issues/4796
ColossalAI,"该issue类型为代码优化，主要涉及ColossalAI中的""gptq""部分，由于可能缺少一些标准化的步骤和内容，需要进行优化。",https://github.com/hpcaitech/ColossalAI/issues/4793
ColossalAI,这个issue类型是文档更新，涉及ColossalAI下的领域特定解决方案新闻的补充。产生这个问题的原因是为了完善文档内容。,https://github.com/hpcaitech/ColossalAI/issues/4789
ColossalAI,这是一个功能特性提议的Issue，主要涉及ColossalAI中的评估管道（Evaluation Pipeline）用于LLMs（Large Language Models）。由于缺少评估管道，为了更好地评估LLMs的性能，该用户提出了此功能需求。,https://github.com/hpcaitech/ColossalAI/issues/4786
ColossalAI,这个issue类型为用户提出需求，主要涉及ColossalAI下的llama2-70b模型的pretraining过程。原因可能是用户想了解如何在预训练中使用梯度累积以及如何启用该功能。,https://github.com/hpcaitech/ColossalAI/issues/4785
ColossalAI,这个issue是一个新功能请求，涉及Colossal-LLaMA-2的持续预训练项目。原因是需要为LLaMA-2添加一个持续预训练项目。,https://github.com/hpcaitech/ColossalAI/issues/4784
ColossalAI,该issue属于关于特性需求的问题，主要涉及P2P通信实现在ColossalAI中的实施方式和性能对比，用户提出了对于GPU利用率下降的疑问与建议使用更先进的`isend`和`irecv`方法。,https://github.com/hpcaitech/ColossalAI/issues/4783
ColossalAI,该问题类型为用户提出需求，主要涉及对象是ColossalAI框架中的dtensor，用户希望该框架支持在分布式环境中直接初始化分布式的 zeros 张量。,https://github.com/hpcaitech/ColossalAI/issues/4780
ColossalAI,这是一个用户提出需求的类型，主要对象是CosineAnnealingWarmupLR类。由于缺少last_epoch参数，用户希望在该类中添加last_epoch参数。,https://github.com/hpcaitech/ColossalAI/issues/4778
ColossalAI,这个issue类型是用户提需求，涉及的主要对象是PR创建前的必要检查事项。用户希望通过这些检查事项确保对PR内容的准确性和规范性。,https://github.com/hpcaitech/ColossalAI/issues/4777
ColossalAI,这个issue类型是关于代码贡献的请求，涉及到ColossalAI的版本更新。由于在贡献代码时未按照规定的步骤和格式进行，导致需要进行版本更新的工作存在问题。,https://github.com/hpcaitech/ColossalAI/issues/4775
ColossalAI,这个issue类型是功能需求提议，涉及主要对象为ColossalAI中的checkpointIO模块，用户提出了对于混合并行支持无分片的checkpoint的需求。,https://github.com/hpcaitech/ColossalAI/issues/4774
ColossalAI,这是一个功能性问题，主要涉及ColossalAI下的Colossal Inference服务与TorchServe的单GPU环境下的示例部署。由于需要支持单GPU推理，用户提出了需要相关文件以及README中的部署步骤。,https://github.com/hpcaitech/ColossalAI/issues/4771
ColossalAI,这是一个需求更新的issue，主要涉及ColossalAI中的moe（Multi-Object Entities）和fsdp的基准更新。,https://github.com/hpcaitech/ColossalAI/issues/4770
ColossalAI,这是一个需求类型的issue，主要涉及到ColossalAI库中的Vit示例更新。通过对该issue的标题内容来看，用户提出了关于更新Vit示例中的混合并行性的需求。,https://github.com/hpcaitech/ColossalAI/issues/4769
ColossalAI,这是一个文档更新的issue，涉及ColossalAI项目中的文档内容更新。,https://github.com/hpcaitech/ColossalAI/issues/4768
ColossalAI,这是一个用户提出需求的issue，主要对象是checkpointio功能，用户希望支持非分片检查点以实现混合并行。,https://github.com/hpcaitech/ColossalAI/issues/4767
ColossalAI,这个issue类型是用户需求报告，涉及的主要对象是ColossalAI项目的GitHub仓库。由于缺少权重合并配置，用户请求添加Lora合并权重的配置项。,https://github.com/hpcaitech/ColossalAI/issues/4766
ColossalAI,这是一个关于需求讨论的issue，涉及的主要对象是4D并行处理。由于zero3和TP在某种程度上存在冲突，导致开发者提出关于如何更好地结合这两种并行策略的想法。,https://github.com/hpcaitech/ColossalAI/issues/4762
ColossalAI,这个issue类型是文档需求，主要涉及ColossalAI中的插件（plugins），用户想要添加说明，以阐明每个插件适合的使用案例。,https://github.com/hpcaitech/ColossalAI/issues/4756
ColossalAI,该issue类型是技术需求提案，涉及的主要对象是ColossalAI中的vllm模块。由于批处理和页面关注性能需要优化，导致需要添加和优化连续批处理和页面关注。,https://github.com/hpcaitech/ColossalAI/issues/4755
ColossalAI,该issue是一个特性需求，涉及 ColossalAI 中的添加 gptq 用于推理功能。分析发现用户提出了结合 gptq 和 kv 缓存的需求。,https://github.com/hpcaitech/ColossalAI/issues/4754
ColossalAI,这个issue类型是特性更新，主要涉及的对象是项目中的代码质量工具和格式化策略。由于需要更新代码静态分析和格式化工具，以提升代码质量和风格一致性。,https://github.com/hpcaitech/ColossalAI/issues/4752
ColossalAI,这是一个增加`coati`支持tensorboard的功能需求，涉及训练指标记录和保存，从而实现将数据保存到本地tensorboard日志目录的功能。,https://github.com/hpcaitech/ColossalAI/issues/4749
ColossalAI,这是一个用户提出需求的 issue，主要涉及 ColossalAI 下的 moe 模块，用户希望支持混合并行及添加自定义 shard 策略、修改初始化设置、更新并行处理组、使用 layernorm kernel 和 jit fuse。,https://github.com/hpcaitech/ColossalAI/issues/4748
ColossalAI,这个issue是一个功能更新请求，主要涉及的对象是ColossalAI中的测试用例。由于需要添加bert示例并满足flash attn要求导致的。,https://github.com/hpcaitech/ColossalAI/issues/4745
ColossalAI,这个issue是关于更新Triton初始化的问题，类型为bug报告，涉及的主要对象是triton kernel init。由于triton kernel init出现错误导致了该bug。,https://github.com/hpcaitech/ColossalAI/issues/4740
ColossalAI,这个issue类型是文档改进，涉及的主要对象是ColossalAI中的Shardformer模块。由于文档中存在错别字和排版问题，用户提出了对文档进行修订的需求。,https://github.com/hpcaitech/ColossalAI/issues/4735
ColossalAI,这个issue属于需求问题类，主要对象为PR创建前的检查清单。由于遗漏了之前合并分支的删除步骤，需要对此进行修复。,https://github.com/hpcaitech/ColossalAI/issues/4733
ColossalAI,该issue类型为文档更新，涉及主要对象为shardformer，由于缺乏更新并行文档的指导而导致用户提出了更新seq parallel document的需求。,https://github.com/hpcaitech/ColossalAI/issues/4730
ColossalAI,这个issue类型是文档更新，主要涉及的对象是ColossalAI中的shardformer文档和tensor parallel文档。用户提出了新增shardformer支持矩阵的文档，并且更新了tensor parallel文档中的过时用法。,https://github.com/hpcaitech/ColossalAI/issues/4728
ColossalAI,这个issue属于文档更新类型，主要涉及到ColossalAI中的pipeline并行处理，由于文档需要更新，开发者提交了相关的PR。,https://github.com/hpcaitech/ColossalAI/issues/4725
ColossalAI,这个issue是一个特性新增类型的需求，主要对象是添加ColossalAI的chatglm2 demo。这个需求的提出可能是为了让ColossalAI在推理时新增chatglm2演示。,https://github.com/hpcaitech/ColossalAI/issues/4724
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI与Deepspeed在不开offload状态下的性能对比。 用户想与开发者确认ColossalAI Gemini和Deepspeed在不开offload情况下的step time和显存占用的对比情况。,https://github.com/hpcaitech/ColossalAI/issues/4719
ColossalAI,这是一个新增功能请求的issue，涉及到ColossalAI中的shardformer插件，用户想要添加自定义策略参数到混合并行插件中。这可能是因为用户希望能够根据自己的需求定制并行计算策略。,https://github.com/hpcaitech/ColossalAI/issues/4718
ColossalAI,这是一个功能请求，主要涉及到ColossalAI的hybrid parallel插件的自定义策略支持。,https://github.com/hpcaitech/ColossalAI/issues/4717
ColossalAI,这是一个关于代码改进的issue，主要涉及ColossalAI中的GPTQ模型推断过程，要求将线性操作替换为Shardformer操作。原因可能是为了优化模型推断性能。,https://github.com/hpcaitech/ColossalAI/issues/4716
ColossalAI,该issue类型是功能更新，主要涉及的对象是ColossalAI中的gptq和kv cache manager。该问题是由于需要将gptq和kv cache manager进行合并，同时还需要将线性模块替换为Shardformer，导致需要对代码进行修改。,https://github.com/hpcaitech/ColossalAI/issues/4706
ColossalAI,这是一个用户提出需求类型的issue，主要涉及添加检索式对话系统功能。原因可能是提高ColossalAI的功能和交互性。,https://github.com/hpcaitech/ColossalAI/issues/4704
ColossalAI,这是一个优化性质的issue，主要涉及ColossalAI中的电子邮件和MLP，其目的是提高性能。,https://github.com/hpcaitech/ColossalAI/issues/4701
ColossalAI,该issue是关于清理ColossalAI中过时的utils到legacy的优化性问题，主要涉及代码维护和整理。由于utils过时导致代码混乱，需要进行清理和优化。,https://github.com/hpcaitech/ColossalAI/issues/4700
ColossalAI,该issue类型为技术文档更新请求，主要涉及ColossalAI项目中的遗留文档迁移工作，由于过时的文档需要整理和更新。,https://github.com/hpcaitech/ColossalAI/issues/4696
ColossalAI,该issue属于贡献者提出需求类型，主要涉及ColossalAI中的PR创建流程。用户寻求帮助和指导以确保他们的PR符合标准并准备好请求审查。,https://github.com/hpcaitech/ColossalAI/issues/4695
ColossalAI,这个issue是一个技术贡献类的需求提交，主要涉及ColossalAI项目中pipeline模块的过期代码清理操作。,https://github.com/hpcaitech/ColossalAI/issues/4692
ColossalAI,这个issue属于文档更新类型的问题，涉及ColossalAI项目中的pipeline parallel文档；此问题可能是因为文档内容需要更新或者补充新的信息而提出的。,https://github.com/hpcaitech/ColossalAI/issues/4691
ColossalAI,这个issue是一个功能修改的类型，主要涉及到ColossalAI中benchmark的CLI和优化器的更新，可能是由于优化器的旧代码结构不再适用而导致的。,https://github.com/hpcaitech/ColossalAI/issues/4690
ColossalAI,这个issue属于更新文档类型，主要对象是ColossalAI的shardformer模块。原因可能是之前的readme文件内容需要更新或修正。,https://github.com/hpcaitech/ColossalAI/issues/4689
ColossalAI,该issue类型为功能需求，主要对象是HybridPlugin，用户询问其是否支持LazyInit加载。由于较大模型加载会占用较大内存，用户希望了解HybridPlugin是否支持LazyInit加载。,https://github.com/hpcaitech/ColossalAI/issues/4687
ColossalAI,这个issue是关于用户提出需求的，针对的主要对象是ColossalAI模型库。原因是用户无法通过pip install获得模型库，导致无法找到所需的模型库。,https://github.com/hpcaitech/ColossalAI/issues/4682
ColossalAI,这是一个建议类型的issue，主要涉及的对象是Selfservice。用户提出了关于自我服务（Selfservice）功能的一个建议，并表示愿意为此提案做一些初步工作。,https://github.com/hpcaitech/ColossalAI/issues/4679
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是ColossalAI中的shardformer功能。,https://github.com/hpcaitech/ColossalAI/issues/4675
ColossalAI,这个issue是一个功能更新请求，该问题单涉及到ColossalAI的推断性能基准测试参数的修改。原因是要统一使用参数而非绝对路径来进行推断基准测试。,https://github.com/hpcaitech/ColossalAI/issues/4674
ColossalAI,这个issue是一个用户提出需求的类型，涉及主要对象是ColossalAI的llama2模块。该用户提出了添加fine-tune示例的请求。,https://github.com/hpcaitech/ColossalAI/issues/4673
ColossalAI,这是一个类型为代码优化的issue，主要涉及到ColossalAI项目中通信、神经网络和日志系统的重构。由于代码结构需要进行调整，因此需要将这些模块移到legacy，并进行相关代码的更新。,https://github.com/hpcaitech/ColossalAI/issues/4671
ColossalAI,这是一个文档更新的Issue，主要涉及ColossalAI的booster用户文档，包括booster_api.md、booster_checkpoint.md和booster_plugins.md的更新。,https://github.com/hpcaitech/ColossalAI/issues/4669
ColossalAI,这个issue类型为功能需求，涉及功能增强。,https://github.com/hpcaitech/ColossalAI/issues/4660
ColossalAI,这个issue是一个功能需求类型的，涉及主要对象是新增支持多个LLMs，由于需要使用LiteLLM简化LLM API调用，并允许任何LLM替代gpt3.5turbo，所以提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4659
ColossalAI,该issue是一个功能性问题类型，主要涉及ColossalAI中的legacy部分，用户打算将nn移动到legacy。由于可能需要重组代码结构或改变模块位置而引起的问题。,https://github.com/hpcaitech/ColossalAI/issues/4656
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的MOE（Mixture of Experts）模块。由于需要对训练设置和损失函数进行进一步优化，用户提出了新增专家路由损失、交叉熵加z损失以及使用AdaFactor优化器的要求。,https://github.com/hpcaitech/ColossalAI/issues/4655
ColossalAI,这个issue属于改进性质，主要涉及ColossalAI中的日志记录器重构以及清理遗留代码。由于日志记录器和遗留代码混乱，可能导致代码维护困难和不易理解代码。,https://github.com/hpcaitech/ColossalAI/issues/4654
ColossalAI,这个issue是用户提交的一个功能需求，主要涉及ColossalAI中的GPT2 HybridParallelPlugin，由于缺乏该功能的示例，用户提交了此问题。,https://github.com/hpcaitech/ColossalAI/issues/4653
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中的GPT示例更新，用户希望添加混合并行性。,https://github.com/hpcaitech/ColossalAI/issues/4652
ColossalAI,这是一个功能需求的issue，主要涉及的对象是ColossalAI项目，用户提出了添加对VEDV的支持的需求。,https://github.com/hpcaitech/ColossalAI/issues/4651
ColossalAI,这个issue属于更新文档类型，主要涉及的对象是Tensor Parallel，由于文档需要更新或补充，用户提出了更新文档的需求。,https://github.com/hpcaitech/ColossalAI/issues/4649
ColossalAI,这是一个文档更新类型的issue，主要涉及到ColossalAI中的ShardFormer模块。由于缺少具体内容，导致需要更新ShardFormer的readme文件。,https://github.com/hpcaitech/ColossalAI/issues/4648
ColossalAI,这个issue类型是用户提出需求，主要对象是pipeline inference，用户希望支持llama pipeline inference。,https://github.com/hpcaitech/ColossalAI/issues/4647
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalAI中的optimization模块。由于目前示例中缺乏混合并行性能的更新，用户希望对示例进行更新以支持混合并行。,https://github.com/hpcaitech/ColossalAI/issues/4644
ColossalAI,这是一个需求提出类型的issue，主要对象是ColossalAI的llama2示例。由于该示例需要更新以支持混合并行性，因此用户提出了更新示例的需求。,https://github.com/hpcaitech/ColossalAI/issues/4643
ColossalAI,这是一个更新 ColossalAI 下的 vit example 的issue，主要涉及到代码示例的更新和文档添加，属于功能更新类型。,https://github.com/hpcaitech/ColossalAI/issues/4641
ColossalAI,这个issue是用户提出需求，主要涉及ColossalAI中的moe模块，用户希望支持openmoe训练和实现相关功能。,https://github.com/hpcaitech/ColossalAI/issues/4637
ColossalAI,这个issue是一个贡献者提出的PR修改请求，主要涉及到对sharder.py文件的更新。由于未完成一些必要的操作，导致该PR还不能请求审核。,https://github.com/hpcaitech/ColossalAI/issues/4629
ColossalAI,这是一则用户提出需求的issue，涉及的主要对象是ColossalAI的文档更新。,https://github.com/hpcaitech/ColossalAI/issues/4628
ColossalAI,这个issue属于功能需求类型，涉及的主要对象是要为测试添加检查Triton和CUDA版本，可能是为了确保测试环境配置的正确性。,https://github.com/hpcaitech/ColossalAI/issues/4627
ColossalAI,这是一个用户提出需求的 issue，主要涉及对象是代码中的/print函数。其中的问题可能是用户请求添加一个“hello”字符串的输出。,https://github.com/hpcaitech/ColossalAI/issues/4626
ColossalAI,这个issue类型是功能需求，主要涉及的对象是支持ColossalAI的openmoe推断功能。这个需求可能是由于现有功能限制或项目需求导致的。,https://github.com/hpcaitech/ColossalAI/issues/4616
ColossalAI,这个issue是一个需求提出，主要涉及对象是HybridParallelPlugin，由于需要增加overlap optional功能。,https://github.com/hpcaitech/ColossalAI/issues/4615
ColossalAI,该issue是一个功能增强的PR，主要涉及到ColossalAI中的HybridParallelPlugin以及LLaMAv2模型，用户提出了改进HybridParallelPlugin的需求。导致该问题的原因是HybridParallelPlugin目前不支持对自定义封装的LLaMAv2模型进行操作。,https://github.com/hpcaitech/ColossalAI/issues/4614
ColossalAI,这是一个功能性更新的issue，主要涉及ColossalAI中的shardformer模块，目的是更新混合并行插件并修复bug。,https://github.com/hpcaitech/ColossalAI/issues/4612
ColossalAI,这个issue是一个代码优化类型的问题，主要涉及到ColossalAI项目中一些未使用和过时的代码移动到legacy文件夹中，包括移动trainer、engine、builder和registry等模块，并更新相关文档和测试文件路径。,https://github.com/hpcaitech/ColossalAI/issues/4611
ColossalAI,这是一个增加支持huggingface的from_pretrained功能的issue，主要涉及的对象是ColossalAI中的插件。由于用户需要支持huggingface的from_pretrained功能，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4606
ColossalAI,这是一个用户提出需求的issue，主要对象是关于ColossalAI中Llama2 70B模型的finetune操作及保存模型的例子缺失。,https://github.com/hpcaitech/ColossalAI/issues/4605
ColossalAI,这个issue是一个代码优化和重构类的问题，涉及主要对象为ColossalAI库中的builder和registry部分。其原因是为了将这些部分移动到legacy中，并更新相关测试、文档和示例。,https://github.com/hpcaitech/ColossalAI/issues/4603
ColossalAI,这个issue是一个修改请求，主要涉及Pipeline inference中的权重关联，原因可能是为了修改模型的权重分配。,https://github.com/hpcaitech/ColossalAI/issues/4599
ColossalAI,此issue是一个贡献者提出的需求问题，涉及ColossalAI中的moe模块，是为了添加一个top k路由器。可能由于当前功能的缺失或不完善导致贡献者认为需要新增此功能。,https://github.com/hpcaitech/ColossalAI/issues/4597
ColossalAI,这是一个用户提出的需求，主要涉及ColossalAI下的不同插件在加载模型时需要支持通过from_pretrained方式进行操作。,https://github.com/hpcaitech/ColossalAI/issues/4596
ColossalAI,这个issue是一个优化checkpoint IO的内容，主要涉及到ColossalAI的zero优化模块，解决了零优化的内存泄漏问题并进行了相关优化。,https://github.com/hpcaitech/ColossalAI/issues/4591
ColossalAI,这是一个用户提出需求的issue，主要关注Gemini是否支持梯度累积，因为无法增大特别大模型的batch size，可能会导致训练不稳定。,https://github.com/hpcaitech/ColossalAI/issues/4590
ColossalAI,这个issue类型是功能改进，主要对象是ColossalAI的1f1b schedule。由于原来schedule只接收num_microbatches，难以设置，现在也接收microbatch_size，以解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4589
ColossalAI,该issue类型为文档更新，涉及主要对象为PR（Pull Request）创建流程。由于缺少文件更新及流程管理，导致了文档不完善的问题。,https://github.com/hpcaitech/ColossalAI/issues/4586
ColossalAI,这个issue类型是文档编辑请求，涉及到ColossalAI项目中推理模块的完善。由于缺乏benchmarking数据，提交者希望添加llama和bloom的性能比对数据。,https://github.com/hpcaitech/ColossalAI/issues/4585
ColossalAI,这个issue类型是用户提出需求，请求更新bert finetune示例，并涉及到ColossalAI中的shardformer模块。最可能是由于需要使用HybridParallelPlugin，但示例还未更新导致用户请求修改。,https://github.com/hpcaitech/ColossalAI/issues/4583
ColossalAI,该issue类型为功能需求，涉及ColossalAI的推理功能，由于提交者希望在创建PR前确保符合一定的标准，含有一系列的需求和检查事项。,https://github.com/hpcaitech/ColossalAI/issues/4580
ColossalAI,这是一个功能特性的issue，主要涉及ColossalAI中添加TP推理引擎、kv缓存管理器以及相关内核的工作。,https://github.com/hpcaitech/ColossalAI/issues/4577
ColossalAI,这个issue是一个功能改进请求，主要涉及到支持HybridParallelPlugin加载模型时的from_pretrained功能，以及添加Huggingface兼容性测试和支持保存sharded模型/optimizer时的文件夹清理。,https://github.com/hpcaitech/ColossalAI/issues/4575
ColossalAI,这个issue是一个PR请求，主要涉及更新推理代码到主分支。原因可能是为了更新功能或修复bug。,https://github.com/hpcaitech/ColossalAI/issues/4571
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中的Shardformer模块，用户希望增加与huggingface的checkpoint IO兼容性，可能导致目前无法直接使用huggingface相应功能的问题。,https://github.com/hpcaitech/ColossalAI/issues/4568
ColossalAI,这个issue是用户提出的需求类型，主要涉及ColossalAI中的ShardFormer模块。由于用户希望支持Bert fine tune示例与混合并行插件，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4567
ColossalAI,这个issue属于用户提出需求类型，主要对象是ColossalAI项目中的ShardFormer模块，在使用hybrid parallel plugin进行Bert fine tune时出现问题。,https://github.com/hpcaitech/ColossalAI/issues/4566
ColossalAI,该issue属于需求类型，主要涉及ColossalAI下的tensor parallel policy，用户希望在inference llama.py中添加rms_norm操作。由于需要加入该操作来优化ColossalAI的功能，以提高性能。,https://github.com/hpcaitech/ColossalAI/issues/4563
ColossalAI,这是一个用户提出需求的issue，主要对象是shardformer库下的swintransformer模块。这个issue的内容为空，可能是用户在寻求关于swintransformer支持tp的帮助。,https://github.com/hpcaitech/ColossalAI/issues/4562
ColossalAI,"这个issue是一个标注为""Infer""的需求报告，主要涉及到ColossalAI的代码导入（import）功能的问题，用户在此提出了整理PR前的相关检查和要求。",https://github.com/hpcaitech/ColossalAI/issues/4559
ColossalAI,该issue类型为对功能的需求，主要涉及新增`rmsnorm`和`rotary_embedding`内核，用户需要添加这两种内核。导致这个需求的原因可能是当前的功能或特性不满足用户的需求或者用户想扩展该项目的功能。,https://github.com/hpcaitech/ColossalAI/issues/4558
ColossalAI,该issue类型为功能增强，主要对象涉及Bloom inference policy和相关方法的更改，由于新增了Bloom inference policy以及相关方法导致的功能改进需求。,https://github.com/hpcaitech/ColossalAI/issues/4553
ColossalAI,这个issue属于功能改进类，涉及到持续集成（CI）系统，主要解决了当前CI系统不会取消之前运行的问题，导致浪费大量时间。,https://github.com/hpcaitech/ColossalAI/issues/4546
ColossalAI,这个issue类型为需求提出，主要对象是在ColossalAI中增加chatglm功能。由于需求提出并未明确说明另外所涉及的原因，因此无法确定导致该需求的具体原因。,https://github.com/hpcaitech/ColossalAI/issues/4542
ColossalAI,该issue类型为文档更新，涉及Coati仓库的持续集成环境更新。可能由于文档不完整或过时导致了CI环境问题需要更新。,https://github.com/hpcaitech/ColossalAI/issues/4541
ColossalAI,该issue是一个功能增强请求，涉及到ColossalAI中的shardformer模块，主要针对HybridParallelPlugin中优化器的分片存储和加载功能进行支持，以及相关测试和代码重构。,https://github.com/hpcaitech/ColossalAI/issues/4540
ColossalAI,这个issue是用户提出需求类型，主要涉及在ColossalAI中添加chatglm模型。这个需求可能由于ColossalAI缺少chatglm模型的支持而产生。,https://github.com/hpcaitech/ColossalAI/issues/4539
ColossalAI,这个issue是用户提出的需求类型，主要涉及ColossalAI中的GPT模型的张量并行。由于缺少张量并行功能，用户请求添加这一功能以优化模型训练。,https://github.com/hpcaitech/ColossalAI/issues/4538
ColossalAI,该issue类型是功能需求，针对ColossalAI中的shardformer，主要涉及到为gpt2添加overlap support功能。原因可能是为了改进gpt2的性能和功能。,https://github.com/hpcaitech/ColossalAI/issues/4535
ColossalAI,这个issue类型为功能需求，主要涉及ColossalAI下的shardformer项目。由于用户希望为GPT-2添加overlap支持，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4534
ColossalAI,这个issue是一个功能增强类型的问题，涉及主要对象为ColossalAI的TP InferEngine，由于文件路径需要重构和引擎及相关测试的添加。,https://github.com/hpcaitech/ColossalAI/issues/4532
ColossalAI,该issue类型为功能需求提出，涉及主要对象为shardformer模块。由于需要支持pp+tp+zero1测试，用户提出了这个功能方面的需求。,https://github.com/hpcaitech/ColossalAI/issues/4531
ColossalAI,这个issue类型为测试请求，主要对象涉及到ColossalAI中的pp+tp+zero1模块。由于需要增加新的测试来覆盖特定功能或场景，因此用户提出了一个添加测试的需求。,https://github.com/hpcaitech/ColossalAI/issues/4530
ColossalAI,该issue属于用户提出需求类型，主要涉及ColossalAI中的示例添加，由于作者已经进行了大部分的必要检查和准备工作。,https://github.com/hpcaitech/ColossalAI/issues/4527
ColossalAI,这个issue是一个功能增强请求，涉及对象是Coati代码库，用户寻求支持在Coati中加入ChatGLM2功能。,https://github.com/hpcaitech/ColossalAI/issues/4519
ColossalAI,这个issue是一个功能性的PR，主要涉及添加zero1 + pp以及相应的测试，为所有支持的模型添加此功能。这个功能的添加可能导致一些问题需要解决。,https://github.com/hpcaitech/ColossalAI/issues/4517
ColossalAI,这个issue是一个功能改进的PR，主要涉及ColossalAI中zero2的梯度累积支持，用户提出了对gradient accumulation的需求。,https://github.com/hpcaitech/ColossalAI/issues/4511
ColossalAI,这是一个用户提出需求的issue，主要涉及支持gradient accumulation with ZeRO2，用户希望在ZeRO2中实现梯度累积以解决GPU内存限制的问题。,https://github.com/hpcaitech/ColossalAI/issues/4510
ColossalAI,这个issue属于贡献者要求提交ColossalAI项目中的一个新功能，主要涉及到添加llama的推断测试。问题的症状是缺少推断测试。,https://github.com/hpcaitech/ColossalAI/issues/4508
ColossalAI,这个issue类型是功能特性添加，主要涉及的对象是ColossalAI中的llama，由于需要为llama添加推理测试。,https://github.com/hpcaitech/ColossalAI/issues/4507
ColossalAI,这是一个功能需求的issue，涉及的主要对象是ColossalAI中的HybridParallelPlugin模型，由于需要支持模型的分片保存和加载，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4506
ColossalAI,这个issue类型是一个需求类型，主要涉及的对象是ColossalAI中的Kernels模块。由于需要增加一个inference token attention kernel，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4505
ColossalAI,这个issue是一个功能提议，主要涉及ColossalAI中的llama infer demo的创建。原因是为了提供一个推断模板。,https://github.com/hpcaitech/ColossalAI/issues/4503
ColossalAI,这个issue是一个功能需求，主要涉及的对象是ColossalAI下的chatglm推断服务。由于现有功能中缺乏chatglm推断服务，因此需要支持该功能。,https://github.com/hpcaitech/ColossalAI/issues/4496
ColossalAI,这是一个功能需求的issue，涉及的主要对象是为llama和bloom模型推断添加KV缓存管理器。,https://github.com/hpcaitech/ColossalAI/issues/4495
ColossalAI,这个issue是一个技术优化类型的问题，主要涉及ColossalAI中关于GPTQ cuda kernel的加速。由于性能需求或者效率优化的目的，开发者希望优化这部分代码。,https://github.com/hpcaitech/ColossalAI/issues/4494
ColossalAI,这个issue类型是功能更新，主要涉及的对象是ColossalAI中支持3D并行的模型，由于缺少相应的测试导致需要添加新的测试。,https://github.com/hpcaitech/ColossalAI/issues/4493
ColossalAI,"这个issue是关于""Inference pipeline""的功能开发，涉及到提交者在创建PR前的一系列准备工作，但缺少了完备的测试。",https://github.com/hpcaitech/ColossalAI/issues/4492
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的shardformer组件，用户希望支持3D混合并行性。,https://github.com/hpcaitech/ColossalAI/issues/4487
ColossalAI,这是一个用户提出需求的issue，主要对象是“shardformer”，用户希望支持“pp+zero”，可能是由于该功能目前尚未实现而提出的需求。,https://github.com/hpcaitech/ColossalAI/issues/4486
ColossalAI,这是一个新功能开发的issue，该问题涉及ColossalAI中的内核（llama和bloom）的添加，用于关注前端和kv-cache管理器。这个需求有可能源于提高系统吞吐量的目的。,https://github.com/hpcaitech/ColossalAI/issues/4485
ColossalAI,这个issue类型是功能增强，主要涉及ColossalAI中的chatglm模块，用户提出支持序列并行性的需求。,https://github.com/hpcaitech/ColossalAI/issues/4482
ColossalAI,这是一个功能改进的issue，主要涉及Gemini库中的静态放置策略和兼容性问题。,https://github.com/hpcaitech/ColossalAI/issues/4479
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中的booster/accelerator.py文件。这个问题的原因是用户希望ColossalAI支持自定义后端和通信后端，以避免需要频繁修改代码中与cuda和nccl相关的字符串。用户希望能够像DeepSpeed一样使用accelerator来抽象不同的后端。,https://github.com/hpcaitech/ColossalAI/issues/4478
ColossalAI,"这是一个功能需求类型的issue，主要涉及ColossalAI中的""shardformer""模块，用户提出需要支持混合并行插件的分片式checkpointing IO功能。",https://github.com/hpcaitech/ColossalAI/issues/4477
ColossalAI,这是一个需要更新Gemini相关示例的issue，主要涉及ColossalAI下的Gemini模块，用户希望更新与Gemini相关的图片和语言示例。,https://github.com/hpcaitech/ColossalAI/issues/4473
ColossalAI,这个issue是一个特性增强请求，主要涉及ColossalAI中的Shardformer模块，请求支持tp+zero功能，并增加相关测试。,https://github.com/hpcaitech/ColossalAI/issues/4472
ColossalAI,这个issue是更新Gemini文档的类型，主要涉及Gemini文档的更新，可能是由于文档需要更新以反映最新的信息。,https://github.com/hpcaitech/ColossalAI/issues/4468
ColossalAI,这是一个关于优化提案的issue，主要涉及到代码重复问题。,https://github.com/hpcaitech/ColossalAI/issues/4467
ColossalAI,这个issue类型是功能需求提出，主要涉及支持chatglm在coati中的实现，出现的问题可能是缺乏该功能带来的限制。,https://github.com/hpcaitech/ColossalAI/issues/4466
ColossalAI,这个issue类型是功能需求，涉及的主要对象是ColossalAI中的shardformer，用户提出了支持序列并行的需求。,https://github.com/hpcaitech/ColossalAI/issues/4465
ColossalAI,这个issue是一个需求提出类型的问题单，主要涉及到ColossalAI的适配llama2，由于要适配llama2而填写的任务checklist。,https://github.com/hpcaitech/ColossalAI/issues/4463
ColossalAI,这个issue类型是功能增强，涉及的主要对象是添加LLAMA推断的CUDA核心。原因是为了添加LLAMA推断的有用的CUDA核心。,https://github.com/hpcaitech/ColossalAI/issues/4462
ColossalAI,这是一个功能需求的issue，涉及到ColossalAI中shardformer模块下支持gpt2序列并行性的问题。,https://github.com/hpcaitech/ColossalAI/issues/4460
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的shardformer模块，请求支持gpt2序列的并行处理。,https://github.com/hpcaitech/ColossalAI/issues/4459
ColossalAI,该issue类型为更新示例结果，主要涉及更新BERT示例以适配Gemini，增加微调结果。导致该issue产生的原因可能是为了优化ColossalAI的示例展示效果和可用性。,https://github.com/hpcaitech/ColossalAI/issues/4458
ColossalAI,这是一个请求更新示例结果的issue，涉及主要对象是更新ResNet示例以适应Gemini插件，并更新训练结果。,https://github.com/hpcaitech/ColossalAI/issues/4457
ColossalAI,这个issue是一个功能修改请求，涉及到ColossalAI的shardformer模块。由于缺少whisper pipeline forward和相关策略和测试，用户提出了这个修改请求。,https://github.com/hpcaitech/ColossalAI/issues/4456
ColossalAI,这个issue类型是技术改进类型，主要对象是shardformer库中的bert模型，由于需要支持序列并行，所以提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4455
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的shardformer模块，用户希望支持序列并行性的其他模型。,https://github.com/hpcaitech/ColossalAI/issues/4454
ColossalAI,这是一个用户提出需求的issue， 主要涉及ColossalAI下的shardformer模块，用户希望支持TP + ZeRO功能。这个需求可能由于用户希望在shardformer中使用TP + ZeRO功能，从而提出了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/4453
ColossalAI,这是一个需求问题，涉及的主要对象是添加大规模分布式测试标记。由于需要更新pytest标记和ci，导致需要添加此功能。,https://github.com/hpcaitech/ColossalAI/issues/4452
ColossalAI,这个issue是一个功能需求，主要涉及到ColossalAI项目中的分布式测试，需要添加标记以指示哪些测试需要在8gpu节点上运行。原因是CI运行在只有4gpu或8gpu的节点上，需要在某些情况下确保测试在8gpu节点上运行。,https://github.com/hpcaitech/ColossalAI/issues/4451
ColossalAI,这个issue类型是用户提出需求，主要对象是代码提交的PR，用户请求将特定提交精选到新分支。原因可能是为了将部分提交独立出来进行进一步处理或测试。,https://github.com/hpcaitech/ColossalAI/issues/4450
ColossalAI,该issue为功能需求，主要涉及ShardFormer模块支持交错计划，并修复了相关问题。,https://github.com/hpcaitech/ColossalAI/issues/4448
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中pipeline模块的interleaved schedule重构问题，由于之前的实现依赖于即将移除的gpc，需要一个新的实现。,https://github.com/hpcaitech/ColossalAI/issues/4447
ColossalAI,这个issue类型为功能增强，主要涉及到ColossalAI中的Shardformer模型支持DDP，用户提出了支持tp+dp并增加相关测试的需求。,https://github.com/hpcaitech/ColossalAI/issues/4446
ColossalAI,该issue类型为功能需求，主要涉及动态推断集群的配置问题。这个问题是关于用户是否可以在不知道集群规模的情况下使用Slurm启动，以实现集群能够自动适应新的集群大小。,https://github.com/hpcaitech/ColossalAI/issues/4444
ColossalAI,这个issue是一个功能需求，涉及的主要对象是ColossalAI库中的gemini模块。由于需要添加静态位置策略，导致需要更新相关的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/4443
ColossalAI,这是一个需求提交类的issue，涉及的主要对象是ColossalAI中的gemini模块。由于当前的cuda和cpu placement policy无法设置分片参数/转移参数的比例，也无法独立设置optimizer offload和参数 offload，因此用户提出添加一个静态放置策略的需求。,https://github.com/hpcaitech/ColossalAI/issues/4442
ColossalAI,这个issue类型是功能需求提案，主要涉及ColossalAI中的Transformer模块及并行性能的优化。用户提出了添加支持pipeline和混合并行性、GPT2的序列并行性、xformers和flashattn的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/4441
ColossalAI,这是一个用户提出需求的issue， 主要涉及ColossalAI中的pipeline branch更新。由于需要支持不同类型的并行操作，并添加相关的测试和文档，用户请求将此更改合并入主代码库。,https://github.com/hpcaitech/ColossalAI/issues/4440
ColossalAI,这是一个用户提出需求的issue，主要涉及请求提供Docker文件用于在docker容器内重现环境。,https://github.com/hpcaitech/ColossalAI/issues/4437
ColossalAI,该issue属于特性需求类型，涉及的主要对象是ColossalAI中的Coati模块，用户提出了添加chatglm支持的请求。,https://github.com/hpcaitech/ColossalAI/issues/4436
ColossalAI,这个issue类型是用户提交一个合并分支的请求，主要涉及到代码贡献流程。由于可能缺少规范性的PR创建步骤，导致了需要提醒和纠正的问题。,https://github.com/hpcaitech/ColossalAI/issues/4435
ColossalAI,这个issue类型是功能需求，主要涉及moe参数不应该存储在working_groups中，需分别存储在param_groups中，避免并行策略冲突导致问题。,https://github.com/hpcaitech/ColossalAI/issues/4429
ColossalAI,这个issue类型是功能需求提案，主要涉及Shardformer模型的TP+DP支持，原因是为了优化使用DDP的DP部分。,https://github.com/hpcaitech/ColossalAI/issues/4428
ColossalAI,"这是一个用户提出需求的类型。该问题单涉及的主要对象是""shardformer""模块。由于缺少对环形自注意力的支持，用户提出了支持环形自注意力的需求。",https://github.com/hpcaitech/ColossalAI/issues/4427
ColossalAI,这个issue类型是开发者请求同步更新分支至主分支，主要涉及ColossalAI代码库的更新操作。由于需要保持各分支同步而导致的请求。,https://github.com/hpcaitech/ColossalAI/issues/4424
ColossalAI,这个issue类型是功能需求，主要涉及ColossalAI中的shardformer，用户提出了关于支持pipeline和混合并行性能的需求。,https://github.com/hpcaitech/ColossalAI/issues/4423
ColossalAI,这是一个需求类型的issue，主要对象是shardformer。由于目前还未提供具体的内容，无法分析原因和症状。,https://github.com/hpcaitech/ColossalAI/issues/4422
ColossalAI,这个issue类型是功能更新，涉及主要对象是更新ColossalAI下的shardformer模块的测试用例。原因可能是为了验证所有优化是否都已应用到相关模型上。,https://github.com/hpcaitech/ColossalAI/issues/4420
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的shardformer模型对RoBERTa支持tp/pp，可能是由于缺少相关功能或者接口而导致用户希望增加该功能。,https://github.com/hpcaitech/ColossalAI/issues/4419
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI库中的示例/llama，用户希望支持流式数据集读取以避免内存错误。,https://github.com/hpcaitech/ColossalAI/issues/4417
ColossalAI,这个issue是一个功能请求，涉及到ColossalAI中的pipeline模块，用户想要添加whisper pipeline和相关测试。这个请求由于缺少相关测试和文档以及未进行自审查，所以需要进一步的完善和审查。,https://github.com/hpcaitech/ColossalAI/issues/4415
ColossalAI,这是一个用户提出需求的类型。该问题单涉及的主要对象是ColossalAI的shardformer模块。由于用户需要支持whisper的pipeline功能，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4414
ColossalAI,该issue为功能改进类型，主要涉及shardformer模块中测试的优化问题，其中可能需要更新测试以使用所有模型的优化功能。,https://github.com/hpcaitech/ColossalAI/issues/4406
ColossalAI,这是一个文档更新的issue，涉及主要对象为Coati README，由于缺少添加 thorough tests 和 docstrings，需要进行补充。,https://github.com/hpcaitech/ColossalAI/issues/4405
ColossalAI,这是一个特性改进类型的issue，主要涉及ColossalAI中的shardformer/sequence parallel模块，解决了在ColossalAI中反向传播过程中的输入收集和梯度计算重叠的问题。,https://github.com/hpcaitech/ColossalAI/issues/4401
ColossalAI,这个issue是用户提出需求。主要对象是shardformer/sequence parallel模块。用户提出需要支持overlap gather输入以及梯度计算。,https://github.com/hpcaitech/ColossalAI/issues/4400
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的shardformer模块，测试GPT2模型的所有优化。,https://github.com/hpcaitech/ColossalAI/issues/4399
ColossalAI,这是一个修改Gemini优化器和Gemini DDP的优化器代码结构的issue，涉及Gemini优化器和Gemini分布式数据并行对象，由于ZeroDDP的命名和继承关系不清晰，需要替换为GeminiDDP。,https://github.com/hpcaitech/ColossalAI/issues/4398
ColossalAI,这是一个功能需求提出的issue，主要涉及Shardformer模块支持Blip2的pipeline。,https://github.com/hpcaitech/ColossalAI/issues/4397
ColossalAI,这是一个需求类型的issue，涉及重写OPT、Bloom、Llama、ViT和ChatGLM的测试，以及为LinearCol类添加填充功能，造成这一需求的原因可能是为了更好地处理最终分类器/得分头部。,https://github.com/hpcaitech/ColossalAI/issues/4395
ColossalAI,这是一个文档更新类的issue，主要涉及ColossalAI项目中的文档内容。由于代码重构，导致文档需要与代码进行更新以保持一致。,https://github.com/hpcaitech/ColossalAI/issues/4394
ColossalAI,这个issue是一个功能更新的PR，主要涉及到ColossalAI中的shardformer模块，更新内容为使用flash attention 2。这个PR的目的是为了改进shardformer的性能和效果。,https://github.com/hpcaitech/ColossalAI/issues/4392
ColossalAI,这个issue类型是功能需求提交，主要对象是Shardformer模块，用户要求更新Shardformer以使用Flash Attention 2。,https://github.com/hpcaitech/ColossalAI/issues/4391
ColossalAI,这个issue属于用户提出需求类型，主要涉及的对象是ColossalAI中的shardformer模块。原因是用户想要测试GPT-2模型在flash attention 2和pipeline parallelism的情况下的表现。,https://github.com/hpcaitech/ColossalAI/issues/4390
ColossalAI,这是一个用户需求类型的issue，主要对象是ColossalAI项目。用户希望获得关于PyTorch和CUDA的版本适配列表，因为软件版本不匹配可能导致第一步操作失败。,https://github.com/hpcaitech/ColossalAI/issues/4385
ColossalAI,这个issue是一个功能需求，主要涉及支持GPT2的序列并行化。由于缺乏支持，用户提出了支持sequence parallel for gpt2的问题。,https://github.com/hpcaitech/ColossalAI/issues/4384
ColossalAI,这个issue类型是用户提出需求，主要涉及的对象是ColossalAI中的shardformer/sequence parallel模块。用户提出在GPT2中支持sequence parallel的需求。,https://github.com/hpcaitech/ColossalAI/issues/4383
ColossalAI,这个issue是关于功能需求的，涉及主要对象是ColoTensor模块。由于tensor parallelism已由shardformer实现，因此不再需要将ColoTensor视为分布式张量。,https://github.com/hpcaitech/ColossalAI/issues/4378
ColossalAI,这个issue是一个文档更新请求，涉及到ColossalAI的PR标准和请求审查前的检查清单。,https://github.com/hpcaitech/ColossalAI/issues/4377
ColossalAI,这是一个用户需求类型的issue，主要对象是在ColossalAI中如何一起训练多个模型。,https://github.com/hpcaitech/ColossalAI/issues/4373
ColossalAI,这是一个优化PR的issue，主要涉及ColossalAI中的Bert测试重写。原因可能是为了提高代码质量或者效率。,https://github.com/hpcaitech/ColossalAI/issues/4372
ColossalAI,这是一个用户提出需求的issue，主要对象是希望能够使用ColossalAI运行DETR时结合Pipeline和Tensor Parallelism来减少内存消耗，但由于代码更新滞后和存在bug而导致无法成功运行。,https://github.com/hpcaitech/ColossalAI/issues/4371
ColossalAI,这个issue是功能增强类型，主要涉及shardformer测试工具函数的添加，由于需要更简洁清晰地编写测试代码而提出。,https://github.com/hpcaitech/ColossalAI/issues/4366
ColossalAI,这是一个用户提出需求的issue，主要涉及GeminiDDP是否支持纯bf16训练，用户关注GeminiPlugin目前仅支持自动混合精度训练，导致与使用fsdp + bf16相比效果降低的问题。,https://github.com/hpcaitech/ColossalAI/issues/4365
ColossalAI,这个issue类型是一个PR请求，涉及的主要对象是ColossalAI中的Shardformer模块，用户请求将flash attention分支合并到pipeline分支。,https://github.com/hpcaitech/ColossalAI/issues/4362
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中shardformer模块的测试合并问题，需要将多个不同模型的测试整合到原有测试中。,https://github.com/hpcaitech/ColossalAI/issues/4361
ColossalAI,"这是一个请求将""Feature/shardformer models""合并到""Feature/pipeline""的合并请求，涉及的主要对象是ColossalAI中的模型和代码库。由于需要将shardformer模型合并到pipeline中，可能涉及模型整合和代码调整，以实现功能的一致性和完整性。",https://github.com/hpcaitech/ColossalAI/issues/4358
ColossalAI,该issue类型为功能改进，主要涉及ColossalAI中低级zero模块的重构，并增加了一些新的功能支持。由于需要提升低级zero的兼容性，支持新的功能以及进一步改进性能，才引发了该需求。,https://github.com/hpcaitech/ColossalAI/issues/4356
ColossalAI,这个issue是一个功能需求，主要涉及到支持fp32优化器和设计合并测试的功能。原因是为了通过减少启动次数来优化测试流程。,https://github.com/hpcaitech/ColossalAI/issues/4354
ColossalAI,这个issue类型是功能增强，主要涉及的对象是ColossalAI的cuda kernels和bloom attention模块。这个功能增强是为了优化推理过程中的前向计算效果。,https://github.com/hpcaitech/ColossalAI/issues/4352
ColossalAI,该issue是一个功能增强请求，涉及到ColossalAI的coloaattention模块，主要针对对Flash Attention 2的支持。由于新的Flash Attention 2只支持Ampere或更高版本的GPU上的fp16/bf16，因此提出了这一改进。,https://github.com/hpcaitech/ColossalAI/issues/4347
ColossalAI,这是一个功能更新的issue，主要涉及ColossalAI中的coloattention模块，用户提出了支持Flash Attention 2的需求。症状是缺乏对Flash Attention 2的支持。,https://github.com/hpcaitech/ColossalAI/issues/4346
ColossalAI,这个issue是关于功能需求的，主要对象是shardformer模块，用户希望其支持序列并行操作。,https://github.com/hpcaitech/ColossalAI/issues/4335
ColossalAI,该issue类型为功能需求，主要涉及ColossalAI中的vit支持flash attention和jit operator，用户因需要这两种功能而提出了该需求。,https://github.com/hpcaitech/ColossalAI/issues/4334
ColossalAI,这个issue是一个更新版本号的issue，涉及到ColossalAI的PR创建流程，主要对象是开发者，可能由于版本号不匹配或者需要遵循PR创建规范而导致的问题。,https://github.com/hpcaitech/ColossalAI/issues/4332
ColossalAI,该issue类型为功能需求，主要涉及对象是ColossalAI中的chatglm模块，用户需求支持flash attention和jit operator。该需求可能是为了提高模型的性能和效率。,https://github.com/hpcaitech/ColossalAI/issues/4330
ColossalAI,这是一个特性请求，涉及Shardformer库中blip2支持flash attention和jit operator的问题。,https://github.com/hpcaitech/ColossalAI/issues/4325
ColossalAI,这是一个技术改进类的issue，涉及到ColossalAI中的测试管线重构和删除无用的工具函数。原因可能是为了优化测试流程和代码质量。,https://github.com/hpcaitech/ColossalAI/issues/4324
ColossalAI,这是一个更新软件版本的issue，主要涉及的对象是ColossalAI下的flash_attention模块。由于要支持新版本的flash_attention v2.0.1，所以需要修改相关文件以应对新版本的变化。,https://github.com/hpcaitech/ColossalAI/issues/4323
ColossalAI,这是一个类型为功能增强的issue，涉及到ColossalAI中的FlashAttention模块。原因是为了支持FlashAttention v2.0.1而修改代码。,https://github.com/hpcaitech/ColossalAI/issues/4322
ColossalAI,这个issue类型为PR创建请求，涉及主要对象为ColossalAI中的shardformer模块，请求合并代码。可能是由于代码整合需要、新功能添加或者bug修复等原因引起。,https://github.com/hpcaitech/ColossalAI/issues/4317
ColossalAI,这个issue是一个功能需求，主要涉及的对象是ColossalAI中的shardformer模块，用户提出了支持flash attention的功能需求。这个需求可能是为了改进模型性能或功能。,https://github.com/hpcaitech/ColossalAI/issues/4316
ColossalAI,这个issue是一个功能提议，讨论的主要对象是ColossalAI中的shardformer模块，该问题涉及支持flash attention功能的提议。,https://github.com/hpcaitech/ColossalAI/issues/4315
ColossalAI,这是一个关于支持会话型训练的需求问题，主要涉及的对象是ColossalAI中的聊天功能。该需求由于需要支持会话型训练，因此用户提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4313
ColossalAI,该issue类型为新增功能请求，主要涉及到ColossalAI的zero模块。由于需要添加针对新zero设备传输的单元测试而产生。,https://github.com/hpcaitech/ColossalAI/issues/4312
ColossalAI,这个issue类型是功能需求报告，涉及主要对象为ColossalAI中的T5模型；由于需要为所有T5模型添加pipeline支持导致该需求产生。,https://github.com/hpcaitech/ColossalAI/issues/4310
ColossalAI,这是一个功能提交相关的issue，主要对象是ColossalAI中的chat功能，由于没有遵循标准的PR创建检查清单导致缺少相关的标签和链接，需要进行相应的修改和添加。,https://github.com/hpcaitech/ColossalAI/issues/4309
ColossalAI,这个issue是一个feature新增请求，主要涉及到添加chatglm pipeline，其中主要原因是为了提供chatglm模块的pipeline forwards。,https://github.com/hpcaitech/ColossalAI/issues/4307
ColossalAI,这个issue是一个功能增强需求，主要涉及ColossalAI中关于chat模块的训练支持，用户希望实现支持tensorboard和optimizer的保存/加载功能。,https://github.com/hpcaitech/ColossalAI/issues/4306
ColossalAI,这个issue类型为PR请求，涉及ColossalAI项目中CI测试相关的修改。导致此问题的原因可能是为了提高项目代码变更检测的效率。,https://github.com/hpcaitech/ColossalAI/issues/4305
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的pipeline中需要添加一个针对1F1B schedule的unit test。由于缺乏这个unit test，可能会导致测试覆盖不完整。,https://github.com/hpcaitech/ColossalAI/issues/4303
ColossalAI,这个issue类型是功能需求，涉及主要对象是Gemini Plugin，由于缺少支持sharded optimizer checkpointing的功能，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4302
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的shardformer模块，用户提交了关于支持flash attention的需求。由于缺乏flash attention的支持，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4301
ColossalAI,这个issue属于需求类型，涉及到在ColossalAI中添加T5Stack和T5EncoderModel的pipeline支持。由于T5结构复杂，需要设计特殊算法分配层到pipeline阶段，修改前向逻辑以实现pipeline并添加相关测试。,https://github.com/hpcaitech/ColossalAI/issues/4300
ColossalAI,这个issue类型是PR请求，主要涉及ColossalAI的flash attn 2 for llama支持，用户提出PR请求来实现该功能。,https://github.com/hpcaitech/ColossalAI/issues/4299
ColossalAI,这是一个feature请求类型的issue，主要涉及的对象是支持chat glm进行条件生成，由于需求是在创建PR之前进行了一系列的检查和准备工作。,https://github.com/hpcaitech/ColossalAI/issues/4297
ColossalAI,这个issue类型是一个代码合并请求，涉及的主要对象是ColossalAI的代码库。由于合并的分支需要统一到一起，所以提出了这个合并的请求。,https://github.com/hpcaitech/ColossalAI/issues/4290
ColossalAI,该issue是一个功能增强请求，主要涉及Gemini框架的支持问题，用户提出了关于支持hierarchical Gemini的需求。,https://github.com/hpcaitech/ColossalAI/issues/4288
ColossalAI,该issue是一个功能更新的PR，涉及到ColossalAI中的GPT2 pipeline的重构。,https://github.com/hpcaitech/ColossalAI/issues/4287
ColossalAI,这是一个功能需求的issue，主要涉及到ColossalAI中的ChatGLM模型，用户要求添加聊天模型的管线策略。,https://github.com/hpcaitech/ColossalAI/issues/4286
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI下的shardformer模块支持vit模型的pipeline功能。由于用户希望添加对vit模型的pipeline支持，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4285
ColossalAI,这个issue是一个功能增强（feature enhancement），主要涉及Shardformer模块对VIT模型的支持，用户为了实现对VIT模型的支持而提交了该问题。,https://github.com/hpcaitech/ColossalAI/issues/4284
ColossalAI,该issue是一个改进请求，涉及Reformat for unified design，通过与标准格式的标题和添加标签等操作来提高PR的可追溯性和可读性。,https://github.com/hpcaitech/ColossalAI/issues/4283
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的shardformer模块，用户希望支持GLM/ChatGLM。 由于用户希望在shardformer模块中添加对GLM/ChatGLM的支持，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4282
ColossalAI,这是一个用户提出需求的 issue，涉及支持 LLaMA-2 功能的请求。可能是由于现有功能不完整或需求增加而提出。,https://github.com/hpcaitech/ColossalAI/issues/4281
ColossalAI,这是一个关于代码优化的issue，主要涉及ColossalAI的代码修改和PR提交流程的指引。,https://github.com/hpcaitech/ColossalAI/issues/4276
ColossalAI,这个issue是一个需求报告，涉及到ColossalAI项目中Chat模块下inference/server.py代码风格的问题，可能是由于代码风格不规范而导致需要优化和改进。,https://github.com/hpcaitech/ColossalAI/issues/4274
ColossalAI,这是一个改进性能和功能的issue，主要涉及ColossalAI的低级zero模块，因为需要支持shard/noshard的checkpoint IO，以及其他一系列的功能改进。,https://github.com/hpcaitech/ColossalAI/issues/4272
ColossalAI,这个issue属于需求类型，主要对象为ColossalAI的贡献者。该问题由格式不符合标准要求所导致。,https://github.com/hpcaitech/ColossalAI/issues/4270
ColossalAI,这个issue类型是用户提出需求，主要涉及ColossalAI的CUDA支持初始化，用户提出了关于特定功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/4269
ColossalAI,这个issue是一个增强用户体验和代码质量的PR请求，涉及主要对象是代码风格的清理。原因是为了保持代码规范和可读性。,https://github.com/hpcaitech/ColossalAI/issues/4264
ColossalAI,该issue类型为用户提出需求，主要对象是ColossalAI库中的 Coloattention 模块， 用户请求添加 attn mask 的 tensor 类型。,https://github.com/hpcaitech/ColossalAI/issues/4262
ColossalAI,这个issue类型是代码风格优化，涉及的主要对象是ColossalAI中Chat模块下的trainer/base.py文件。由于代码风格不符合规范，因此需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/4260
ColossalAI,这个issue类型属于功能需求，主要涉及的对象是ColossalAI中的OPT模型pipeline。由于开发人员新增了OPT模型pipeline和policy，导致该issue的内容是请求review和补充相关信息。,https://github.com/hpcaitech/ColossalAI/issues/4258
ColossalAI,该issue类型为新功能需求，涉及主要对象为ColossalAI中的LLama预训练模型，用户提出了向项目中添加LLama预训练模型的需求。,https://github.com/hpcaitech/ColossalAI/issues/4257
ColossalAI,该issue是一个需求提出类型的问题，主要涉及ColossalAI下的llama benchmark。由于缺乏必要的PR提交前检查和请求审查步骤，可能导致缺乏完整的代码测试和文档，需要相关贡献者进行对应的完善。,https://github.com/hpcaitech/ColossalAI/issues/4252
ColossalAI,这个issue是一个特性请求，涉及主要对象是ColossalAI中的shardformer模块，用户提出需求希望支持inplace sharding，由于目前缺乏这项功能，用户需要更新策略、修复单元测试以及检查模型检查点。,https://github.com/hpcaitech/ColossalAI/issues/4251
ColossalAI,这个issue属于用户提出需求类型，涉及ColossalAI的代码规范性，可能因为缺乏标准化代码风格引起。,https://github.com/hpcaitech/ColossalAI/issues/4250
ColossalAI,这是一个功能需求的Issue，主要涉及的对象是ColossalAI下的shardformer模块。由于当前的参数分片方法不够内存高效，需要重复处理权重绑定和参数组等问题，因此提出了支持原地分片的建议。,https://github.com/hpcaitech/ColossalAI/issues/4249
ColossalAI,这个issue属于需求提出类型，主要涉及ColossalAI下的shardformer项目对T5模型的支持。,https://github.com/hpcaitech/ColossalAI/issues/4247
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI项目中缺少3D并行插件。,https://github.com/hpcaitech/ColossalAI/issues/4294
ColossalAI,这是一个feature请求，主要涉及ColossalAI中的GPT2模型支持shardformer策略以及完善GPT2的pipeline支持。,https://github.com/hpcaitech/ColossalAI/issues/4245
ColossalAI,这个issue是关于新增功能支持，主要对象是支持Blip2模型；由于需要新增支持Blip2模型所导致的需求。,https://github.com/hpcaitech/ColossalAI/issues/4243
ColossalAI,这是一个用户提出需求的类型，问题主要涉及到ColossalAI下的shardformer模块，用户希望该模块能够支持blip2模型。,https://github.com/hpcaitech/ColossalAI/issues/4242
ColossalAI,这个issue是一个需求报告，涉及主要对象为ColossalAI中的管道（pipeline）模块，用户请求添加支持用于变种GPT2模型的pipeline forward功能。,https://github.com/hpcaitech/ColossalAI/issues/4238
ColossalAI,"这个issue为feature请求，主要涉及ColossalAI中的""shardformer""模块，用户提出为shardformer添加jit fused operator的支持。",https://github.com/hpcaitech/ColossalAI/issues/4236
ColossalAI,这个issue类型为需求提出，主要对象是ColossalAI项目中的jit fused operator功能。由于缺乏对bloom support的支持，用户提出了希望jit fused operator能够添加该功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/4235
ColossalAI,这个issue是一个功能性需求，主要涉及更新GPT2模型以使用ColoAttention，因为用户想要为所有现有模型支持ColossalAI上的快闪注意力机制，从而提升模型效果。,https://github.com/hpcaitech/ColossalAI/issues/4234
ColossalAI,这个issue属于功能开发类型，涉及主要对象是ColossalAI中的bert models。由于pipeline构建不完整导致的bug，需要完成pipeline builds for bert models。,https://github.com/hpcaitech/ColossalAI/issues/4233
ColossalAI,这个issue属于功能增强类型，主要涉及支持SAM模型和为nn.Linear添加融合的qkv线性层。原因可能是为了提供对SAM模型的支持并增加额外的功能。,https://github.com/hpcaitech/ColossalAI/issues/4231
ColossalAI,这是用户提出的需求问题，涉及ColossalAI中的shardformer模块，请求添加对SAM模型的支持和为nn.Linear添加融合的QKV线性层。,https://github.com/hpcaitech/ColossalAI/issues/4230
ColossalAI,这个issue属于功能更新类型，主要涉及ColossalAI中的模型优化（例如opt和llama）使用coloattention，可能是为了优化模型的性能和效果。,https://github.com/hpcaitech/ColossalAI/issues/4226
ColossalAI,这个issue是一个需求类型，主要涉及ColossalAI库中的3D并行处理功能。导致这个需求的原因可能是用户想要在ColossalAI中支持3D并行操作。,https://github.com/hpcaitech/ColossalAI/issues/4225
ColossalAI,这个issue属于需求报告类型，主要涉及到ColossalAI中GPT2Model Shardformer的Pipeline Forward添加以及相关测试的支持。导致提出此问题的原因可能是为了优化模型的性能和功能扩展。,https://github.com/hpcaitech/ColossalAI/issues/4224
ColossalAI,这是一个优化功能的Issue，主要涉及到ColossalAI中的零优化器步骤时间。原因可能是为了提高性能或减少训练时间。,https://github.com/hpcaitech/ColossalAI/issues/4221
ColossalAI,这是一个关于性能优化的问题，主要对象涉及ColossalAI库中的zero优化器。此问题由于在`step()`函数中zero使用主参数进行通信，导致了长时间的optimizer_step时间。,https://github.com/hpcaitech/ColossalAI/issues/4220
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中的Shardformer模块，由于缺乏构建管道，需求构建一个用于opt的管道。,https://github.com/hpcaitech/ColossalAI/issues/4219
ColossalAI,这个issue属于功能需求类型，主要涉及的对象是ColossalAI的pipeline测试流程，通过llama模型实现纯净的pipeline测试过程。,https://github.com/hpcaitech/ColossalAI/issues/4218
ColossalAI,这个issue类型属于功能增强需求，主要对象是shardformer模块，用户提出了支持flash attention的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/4216
ColossalAI,这是一个功能增强的issue，主要涉及Coloattention模块，内容是支持'paddedcausal'类型的attention mask。,https://github.com/hpcaitech/ColossalAI/issues/4215
ColossalAI,这是一个更新benchmark和准确度文档的issue，主要涉及到Bert模型。由于需要更新benchmark和微调结果，可能是为了改进模型性能和准确度。,https://github.com/hpcaitech/ColossalAI/issues/4214
ColossalAI,这个issue是一个功能需求，主要涉及支持whisper模型，由于没有提供具体细节，无法确定导致此需求的具体原因。,https://github.com/hpcaitech/ColossalAI/issues/4212
ColossalAI,这个issue类型是功能需求提交，主要对象是ColossalAI的shardformer模块。,https://github.com/hpcaitech/ColossalAI/issues/4211
ColossalAI,这是一个功能需求的issue，涉及在ColossalAI中添加bloom模型pipeline。由于需要新增功能，用户提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/4210
ColossalAI,这个issue是用户提出的需求，主要涉及ColossalAI下的一个叫做shardformer的模块，用户请求支持bloom pipeline。该问题的根源在于用户希望为shardformer模块添加对bloom pipeline的支持。,https://github.com/hpcaitech/ColossalAI/issues/4209
ColossalAI,这个issue是一个功能需求的提出，主要涉及ColossalAI中的Llama模型，希望添加causal lm和sequence classification pipeline，可能是为了增强模型的功能和应用领域。,https://github.com/hpcaitech/ColossalAI/issues/4208
ColossalAI,该issue是一个功能添加请求，涉及的主要对象是ColossalAI下的shardformer，用户提出了对bert支持flash attention的需求。,https://github.com/hpcaitech/ColossalAI/issues/4206
ColossalAI,该issue是一个功能新增的PR，涉及ColossalAI的Llama pipeline，主要对象是新增的llama model pipeline。原因是开发人员添加了llama模型管道和测试，其中forward方法接收一个阶段索引列表以获取起始和结束层。,https://github.com/hpcaitech/ColossalAI/issues/4205
ColossalAI,这个issue属于用户提出需求类型，主要涉及Shardformer模块添加llama forward和policy功能。由于用户希望在Shardformer中支持llama forward和policy，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4204
ColossalAI,这个issue类型为功能增强（Feature Enhancement），主要涉及ColossalAI中的Shardformer模块，用户希望支持lazy init，由于缺乏lazy init功能，用户无法在Shardformer中进行相关操作。,https://github.com/hpcaitech/ColossalAI/issues/4202
ColossalAI,这是一个用户提出需求的类型，主要对象是ColossalAI中的shardformer模块，用户希望支持懒加载初始化来实现材料化的执行。,https://github.com/hpcaitech/ColossalAI/issues/4201
ColossalAI,这个issue属于用户提出需求的类型，主要涉及ColossalAI中的shardformer功能，要支持GPT模型的pipeline，将其策略集成到shardformer中，并添加相关测试。这可能是用户希望改进ColossalAI在处理GPT模型时的效率和功能性。,https://github.com/hpcaitech/ColossalAI/issues/4200
ColossalAI,这是一个功能需求提出的issue， 主要涉及ColossalAI的Transformer Engine中加入FP8 mixed precision training的支持。由于Hopper GPUs上使用8bit floating point precision能够提供更好的性能，因此用户希望在Booster API中加入FP8支持。,https://github.com/hpcaitech/ColossalAI/issues/4199
ColossalAI,这是用户提出需求的类型，主要涉及ColossalAI下的coati模型能否与类似https://huggingface.co/facebook/opt-125m这样的小型模型一起训练。这个问题可能由于用户想要了解coati模型与小型模型集成训练的可行性而产生。,https://github.com/hpcaitech/ColossalAI/issues/4198
ColossalAI,这个issue是一个功能增强类型的问题，涉及的主要对象是ColossalAI库中的Bert pipeline和Shardformer模块。由于缺乏Bert pipeline和相关测试，导致需要添加新功能并进行测试。,https://github.com/hpcaitech/ColossalAI/issues/4197
ColossalAI,这个issue是一个功能需求提交，涉及到Gemini插件的优化器检查点功能。由于Gemini需要支持优化器的检查点保存功能，因此提交了这个功能需求。,https://github.com/hpcaitech/ColossalAI/issues/4196
ColossalAI,这是一个功能增强的issue，涉及docker支持的SSH和RDMA，由于增加了密码无需SSH和RDMA支持的Docker镜像。,https://github.com/hpcaitech/ColossalAI/issues/4192
ColossalAI,这个issue是关于功能需求的，主要涉及ColossalAI中的shardformer模块，用户希望gpt2能够支持flash attention。由于用户希望在ColossalAI中的shardformer模块中实现gpt2支持flash attention，因此提出了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/4191
ColossalAI,这是一个特性需求，主要涉及到ColossalAI中的shardformer模块，用户希望支持BERT和相关模型的pipeline，替换现有的forward函数。,https://github.com/hpcaitech/ColossalAI/issues/4190
ColossalAI,这个issue属于功能需求类型，主要涉及到ShardFormer模块中的Flash Attention功能支持。由于缺乏对Flash Attention的支持，用户希望增加对bloom的支持。,https://github.com/hpcaitech/ColossalAI/issues/4188
ColossalAI,这个issue类型是特性/功能更新，涉及的主要对象是移动BERT相关pipeline组件到Shardformer。原因是为了更好地整合BERT模型到Shardformer中。,https://github.com/hpcaitech/ColossalAI/issues/4187
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalAI库中的`shard state dict`保存和加载功能的支持。由于`state_dict`的形式与`huggingface`不兼容，用户提出需要支持`shard state dict`形式以实现兼容性。,https://github.com/hpcaitech/ColossalAI/issues/4186
ColossalAI,这个issue是一个用户需求提交，主要涉及对象是ColossalAI中的shardformer模块，用户提出了关于支持flash attention的需求。,https://github.com/hpcaitech/ColossalAI/issues/4185
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI库中的模型支持flash attention的功能。用户希望为所有已存在的模型添加flash attention支持。,https://github.com/hpcaitech/ColossalAI/issues/4184
ColossalAI,这个issue是一个需求提出，涉及的主要对象是ColossalAI下的低级zero模块，由于缺少state_dict接口导致用户提出了新增接口的需求。,https://github.com/hpcaitech/ColossalAI/issues/4179
ColossalAI,这个issue是一个功能改进的请求，主要涉及到ColossalAI中的pipeline组件，由于需要更新shardformer以适配pipeline，导致提出这个请求。,https://github.com/hpcaitech/ColossalAI/issues/4176
ColossalAI,这是一个特性更新类型的issue，主要涉及到ColossalAI中Shardformer的性能测试。由于缺乏性能指标，导致需要增加Shardformer的性能基准测试。,https://github.com/hpcaitech/ColossalAI/issues/4174
ColossalAI,该issue为需求提出类型，主要涉及Shardformer性能基准测试，可能是为了验证Shardformer模块的性能表现。,https://github.com/hpcaitech/ColossalAI/issues/4173
ColossalAI,该issue类型为功能性需求，主要涉及ColossalAI中的ShardFormer性能基准测试的添加。由于需要对ShardFormer进行性能基准测试的需求而产生。,https://github.com/hpcaitech/ColossalAI/issues/4169
ColossalAI,这个issue是用户需求类型，主要涉及到ColossalAI的性能基准测试，原因是提出需要添加Shardformer的性能基准测试。,https://github.com/hpcaitech/ColossalAI/issues/4168
ColossalAI,这个issue是更新同步主分支，主要涉及到了文档更新。原因可能是为了保持主分支和分支的一致性。,https://github.com/hpcaitech/ColossalAI/issues/4166
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的shardformer模块的优化支持flash attention，可能由于用户需要改进flash attention功能而提出。,https://github.com/hpcaitech/ColossalAI/issues/4163
ColossalAI,这是一个功能需求的issue，涉及ColossalAI中的测试持续时间显示。造成这个问题可能是为了优化测试过程并找出需要优化的测试案例。,https://github.com/hpcaitech/ColossalAI/issues/4159
ColossalAI,这个issue是一个功能新增的PR，涉及到ColossalAI的zero模块，支持传递dp和tp process_group到low_level_zero，以支持新的processGroupMesh。,https://github.com/hpcaitech/ColossalAI/issues/4153
ColossalAI,这个issue是一个功能需求，涉及到ColossalAI库中low level zero接口的问题，用户需要能够传递process_group以支持新的ProcessGroupMesh，同时要兼容当前代码。,https://github.com/hpcaitech/ColossalAI/issues/4151
ColossalAI,这个issue属于功能开发类型，主要涉及的对象是开发协议的添加，可能是由于需要规范开发过程而导致的需求。,https://github.com/hpcaitech/ColossalAI/issues/4149
ColossalAI,这个issue类型是关于用户提出需求的，主要对象是ColossalAI中的shardformer模块开发者，由于缺乏关于如何参与开发和遵守SOP的文档，用户提出了需要添加开发标准操作程序的建议。,https://github.com/hpcaitech/ColossalAI/issues/4148
ColossalAI,这是一个功能开发的issue，主要涉及到ColossalAI下的Shardformer模块，用户提出了新功能支持Flash Attention的需求。,https://github.com/hpcaitech/ColossalAI/issues/4147
ColossalAI,该issue类型为功能改进，主要涉及ColossalAI下的shardformer模块，通过使张量并行化可配置，解决了小模型无需并行化即可使用其他优化的需求。,https://github.com/hpcaitech/ColossalAI/issues/4144
ColossalAI,这是一个需求提出类型的issue，主要涉及ColossalAI中的shardformer模块，用户提出支持禁用张量并行性的需求。,https://github.com/hpcaitech/ColossalAI/issues/4143
ColossalAI,这个issue属于功能需求类型，主要涉及Gemini Plugin中的Unsharded Optimizer Checkpoint功能的实现，由于需要支持pytorch模型/优化器检查点，所以实现了对ZeroOptimizer中state_dict()/load_state_dict()方法的重载。,https://github.com/hpcaitech/ColossalAI/issues/4141
ColossalAI,该issue类型为更新配置和提示，涉及对象为ColossalAI。由于更新配置和提示文件导致需要解决或更新的问题。,https://github.com/hpcaitech/ColossalAI/issues/4139
ColossalAI,"这个issue是一个功能需求，主要涉及到ColossalAI中的""zero1 plugin""，原因是需要支持低级zero插件在零点1情况下的""no_sync""功能。",https://github.com/hpcaitech/ColossalAI/issues/4138
ColossalAI,这是一个与文档更新相关的问题，主要涉及到ColossalAI中的`shardformer`模块。原因是在将`shardformer`合并到主分支之前需要更新文档。,https://github.com/hpcaitech/ColossalAI/issues/4136
ColossalAI,这个issue是一个用户提出需求的类型，涉及评估训练性能，可能由于缺乏特定的评估指标或方法而引起。,https://github.com/hpcaitech/ColossalAI/issues/4134
ColossalAI,这个issue是一个功能改进类型的问题，主要涉及到ColossalAI中的shardformer模块，用户提出了关于优化模型支持flash attention的需求。,https://github.com/hpcaitech/ColossalAI/issues/4132
ColossalAI,这是一个用户提出需求的issue，主要涉及Shardformer模块的优化支持Flash Attention。,https://github.com/hpcaitech/ColossalAI/issues/4131
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的ZeRO插件的功能需要支持no_sync，并且解释了当前的限制和建议替代方法。,https://github.com/hpcaitech/ColossalAI/issues/4128
ColossalAI,这个issue是一个用户提出需求的类型，主要涉及的对象是ColossalAI中的shardformer模块。由于用户希望编写一个使用bert finetuning的shardformer示例，可能是为了展示该功能或扩展其应用的灵活性。,https://github.com/hpcaitech/ColossalAI/issues/4126
ColossalAI,这是一个需求Merge Pull Request并评估性能的问题，涉及ColossalAI中的chat功能，由于某方面的修复导致RM和MDP的问题需要解决。,https://github.com/hpcaitech/ColossalAI/issues/4125
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是pipeline中的shardformer policy。这个问题可能是因为需要为shardformer policy适配pipeline，但具体原因未详。,https://github.com/hpcaitech/ColossalAI/issues/4119
ColossalAI,这个issue是一个功能更新类型的问题，涉及的主要对象是ColossalAI的pipeline。由于需要实现1f1b pipeline schedule with new components，导致需要对原有的代码进行重构，所以创建了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/4115
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的Shardformer模块，用户希望增加对融合归一化核的支持以提高效率。,https://github.com/hpcaitech/ColossalAI/issues/4113
ColossalAI,这是一个需求开发类的issue，主要涉及到ColossalAI中的shardformer模型的支持问题，原因是为现有模型添加了融合归一化支持。,https://github.com/hpcaitech/ColossalAI/issues/4112
ColossalAI,这个issue属于功能需求类型，主要涉及Shardformer与BERT微调的示例编写，用户提出了添加示例以展示该功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/4111
ColossalAI,这是一个用户提出需求的类型的issue，主要对象是ColossalAI下的shardformer模块，用户希望编写一个带有BERT微调的shardformer示例。,https://github.com/hpcaitech/ColossalAI/issues/4110
ColossalAI,这是一个文档更新和修正一些错别字和错误的问题单，涉及到ColossalAI文档。,https://github.com/hpcaitech/ColossalAI/issues/4107
ColossalAI,这个issue是一个功能优化类型的问题，涉及主要对象为ColossalAI的工作流。由于未添加构建状态检查，导致可能会生成失败的测试覆盖报告。,https://github.com/hpcaitech/ColossalAI/issues/4106
ColossalAI,这个issue类型是功能需求，该问题单涉及的主要对象是ColossalAI中的shardformer模块。,https://github.com/hpcaitech/ColossalAI/issues/4104
ColossalAI,这个issue是一个功能更新，主要涉及Shardformer与PyTorch DDP的集成，通过将张量并行组传递给Shardformer来支持兼容性。,https://github.com/hpcaitech/ColossalAI/issues/4103
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及对象为ColossalAI下的shardformer模块，用户寻求将shardformer与pytorch的DDP进行整合的帮助。,https://github.com/hpcaitech/ColossalAI/issues/4102
ColossalAI,这个issue类型是一个功能改进，涉及的主要对象是ColossalAI库中的shardformer模块。这个改进是为了支持隐式导入，避免用户因为没有安装`transformers`库而导致的import错误。,https://github.com/hpcaitech/ColossalAI/issues/4101
ColossalAI,该issue类型为功能需求，涉及主要对象为pipeline的P2P通信功能，由于这个功能尚未实现，所以用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4099
ColossalAI,这个issue是一个功能更新的PR，主要涉及Shardformer中的bloom model，由于需要支持新的策略和数值测试。,https://github.com/hpcaitech/ColossalAI/issues/4098
ColossalAI,该issue类型为功能特性新增，主要涉及ColossalAI下的VITT功能模块，可能由于开发人员需求添加新功能或改进现有功能而创建。,https://github.com/hpcaitech/ColossalAI/issues/4096
ColossalAI,这是一个用户提出需求的类型，主要对象为Shardformer模块。由于用户希望支持选项开启Shardformer功能，可能是为了更灵活地控制Shardformer在项目中的使用。,https://github.com/hpcaitech/ColossalAI/issues/4095
ColossalAI,这个issue类型是对代码进行改进建议，主要涉及ColossalAI中的聊天模块，需要移除naive策略并将ColossalAI策略拆分为gemini和low level zero策略。,https://github.com/hpcaitech/ColossalAI/issues/4094
ColossalAI,这是一个功能添加的issue，主要涉及到ColossalAI的pipeline阶段管理器。用户提交该 issue 来为软件添加新的功能模块。,https://github.com/hpcaitech/ColossalAI/issues/4093
ColossalAI,这是一个需求类型的issue，涉及到ColossalAI的pipeline功能，用户提出需求添加一个pipeline阶段管理器。由于尚未实现相应功能，用户希望开发团队能够增加这一功能。,https://github.com/hpcaitech/ColossalAI/issues/4092
ColossalAI,这个issue类型为特性需求，主要涉及对象为shardformer模型。由于需要支持opt模型，用户提出这一需求。,https://github.com/hpcaitech/ColossalAI/issues/4091
ColossalAI,这是一个用户提交需求的issue，主要涉及将ResNet示例从新API复制到图像，并添加一些文档说明。原因可能是为了更好地组织代码示例和文档。,https://github.com/hpcaitech/ColossalAI/issues/4090
ColossalAI,这是一个文档更新的issue，涉及ColossalAI中的chat evaluate readme的更新。原因可能是为了保持文档内容的准确性。,https://github.com/hpcaitech/ColossalAI/issues/4089
ColossalAI,这是一个用户提出需求的类issue，主要涉及Gemini插件不支持no_sync，用户想知道如何在使用Gemini插件时累积梯度。,https://github.com/hpcaitech/ColossalAI/issues/4088
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中的策略（strategy）划分问题。由于`Gemini` strategy和 lowlevel `Zero` strategy的不同，需要将它们拆分为两个独立的策略。,https://github.com/hpcaitech/ColossalAI/issues/4085
ColossalAI,这是一个需要更新示例代码以展示如何使用新API的issue，涉及主要对象是ColossalAI的Booster和Plugin API。,https://github.com/hpcaitech/ColossalAI/issues/4081
ColossalAI,这是一个功能需求的issue，主要是关于在ColossalAI的shardformer模块中添加支持bloom模型的功能。,https://github.com/hpcaitech/ColossalAI/issues/4074
ColossalAI,这是一个功能增强的issue，主要涉及增加layernorm功能到所有支持的模型中，可能由于之前缺乏这一功能而导致用户需要这个功能。,https://github.com/hpcaitech/ColossalAI/issues/4072
ColossalAI,这是一个功能增强类型的issue，主要涉及ColossalAI中的shardformer模块，用户希望为其添加layernorm功能。,https://github.com/hpcaitech/ColossalAI/issues/4071
ColossalAI,这是一个贡献者在ColossalAI中提出的一个增加周报功能的issue，涉及的主要对象是周报。原因是要覆盖所有公共存储库。,https://github.com/hpcaitech/ColossalAI/issues/4069
ColossalAI,这个issue类型是PR创建请求，涉及的主要对象是ColossalAI项目。原因是该用户提交了一个要求添加整体性能的PR。,https://github.com/hpcaitech/ColossalAI/issues/4068
ColossalAI,这个issue是一个需求相关的任务，主要涉及到ColossalAI代码中的添加linearconv1d测试。问题是由于缺少这个测试而导致需要增加这个功能以保证代码的准确性。,https://github.com/hpcaitech/ColossalAI/issues/4067
ColossalAI,这是一个功能增强的issue，涉及到ColossalAI中的shardformer模块，主要是为了支持模块的保存和加载。原因是需要为shardformer模块添加对state_dict和load_state_dict的支持。,https://github.com/hpcaitech/ColossalAI/issues/4062
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是shardformer中的分布式层和普通PyTorch层，在加载和保存权重时需要互相转换。,https://github.com/hpcaitech/ColossalAI/issues/4061
ColossalAI,这个issue是优化代码路径的问题，涉及主要对象为ColossalAI下的模块和文件。,https://github.com/hpcaitech/ColossalAI/issues/4060
ColossalAI,这个issue类型是功能增强请求，主要对象是支持使用kit来进行bert/gpt2测试，由于需要增强测试功能以支持特定模型的测试需要。,https://github.com/hpcaitech/ColossalAI/issues/4055
ColossalAI,这个issue类型是需求，请用户提出需求。该问题单涉及的主要对象是ColossalAI库中的测试功能。  原因是为了增加对BERT和GPT-2的测试支持。,https://github.com/hpcaitech/ColossalAI/issues/4054
ColossalAI,这个issue类型是代码优化，涉及主要对象为ColossalAI中的shardformer层结构。这个问题由于代码结构缺乏清晰性导致，需要重构以提高可读性。,https://github.com/hpcaitech/ColossalAI/issues/4053
ColossalAI,这个issue是一个功能修改类型的问题，主要涉及到ColossalAI下的Shardformer模块，用户修改了T5和LLaMA测试以适应新的API。这个问题可能是由于模型测试的集成性问题引起的。,https://github.com/hpcaitech/ColossalAI/issues/4049
ColossalAI,该问题类型属于功能需求提升，主要涉及ColossalAI下的flash attention模块。由于需要支持tensor bias，但目前的实现不兼容dropout，可能会影响模型鲁棒性，因此需要修改CUDA版本的flash attention以支持tensor bias。,https://github.com/hpcaitech/ColossalAI/issues/4048
ColossalAI,这个issue是一个用户提出需求的类型，主要涉及对象是ColossalAI中的shardformer模块。由于项目路线规划需要，用户提议在shardformer中支持T5模型。,https://github.com/hpcaitech/ColossalAI/issues/4044
ColossalAI,该issue属于用户提出需求类型，主要涉及ColossalAI的一个组件Process group mesh，问题是为了解决全局并行上下文的不足以及处理复杂NDparallelism场景而提出的需求。,https://github.com/hpcaitech/ColossalAI/issues/4038
ColossalAI,这是一个用户提出需求的issue，主要涉及支持LLaMA模型在shardformer中的问题，由于用户希望在shardformer中支持LLaMA模型，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/4037
ColossalAI,该issue类型为功能改进，主要涉及ColossalAI中shardformer模块下的llama适应新API的问题。由于新API变化，需要对llama策略进行重构以保持其与新API的一致性。,https://github.com/hpcaitech/ColossalAI/issues/4036
ColossalAI,这是一个用户需求类型的问题单，主要涉及Gemini插件的优化器检查点功能支持。原因可能是用户希望Gemini插件能够支持优化器检查点功能。,https://github.com/hpcaitech/ColossalAI/issues/4035
ColossalAI,"这个issue是一个功能性需求，主要涉及ColossalAI中的""checkpointio""模块和""Low Level Zero""插件。由于需要将sharded optimizer checkpoint功能适配到Low Level Zero插件，因此提出了这个问题。",https://github.com/hpcaitech/ColossalAI/issues/4034
ColossalAI,这个issue是一个功能改进类型的问题，主要涉及到ColossalAI中的`low level zero`模块，由于需要对该模块进行重新平衡，以提高负载均衡。,https://github.com/hpcaitech/ColossalAI/issues/4030
ColossalAI,这个issue是一个功能添加，主要涉及到在ColossalAI中实现非原地张量分片的功能，用户提出了关于添加支持DTensor的非原地张量分片的需求。,https://github.com/hpcaitech/ColossalAI/issues/4018
ColossalAI,该issue类型是用户提出需求，关于ColossalAI库中对非就地张量分片的支持。,https://github.com/hpcaitech/ColossalAI/issues/4017
ColossalAI,这个issue是文档更新类型的请求，涉及到ColossalAI库中的`shardformer`模块，并由用户对文档的准确性或完整性提出更新。,https://github.com/hpcaitech/ColossalAI/issues/4016
ColossalAI,这是一个需求提出类型的issue，主要涉及ColossalAI中的shardformer模块，用户要求支持bert/gpt2以及下游模型，并希望应用新的API和dtensor。,https://github.com/hpcaitech/ColossalAI/issues/4015
ColossalAI,这个issue为功能增强，涉及的主要对象是Shardformer中的vocabembedding layer和支持BERT模型，由于需要增加对BERT模型的支持导致该问题存在。,https://github.com/hpcaitech/ColossalAI/issues/4014
ColossalAI,这个issue属于用户提出的需求类型，主要对象为ColossalAI中的Shardformer模块。因为当前的层在分布式操作时依赖于进程组参数，用户希望移除对gpc的依赖。,https://github.com/hpcaitech/ColossalAI/issues/4012
ColossalAI,这个issue是一个需求提出，主要涉及对象是ColossalAI中的shardformer和dist layer。由于需要将shardformer整合到dist layer上，导致该需求产生。,https://github.com/hpcaitech/ColossalAI/issues/4011
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中的shardformer模块与dist tensor layer的集成问题。可能是用户希望将shardformer模块与dist tensor layer进行整合，以实现某种功能或优化。,https://github.com/hpcaitech/ColossalAI/issues/4010
ColossalAI,这是一个功能需求类型的issue，主要涉及初始化XPU支持。由于尚未初始化XPU支持，导致需要相应的功能补充。,https://github.com/hpcaitech/ColossalAI/issues/4006
ColossalAI,这个issue属于功能需求类型，主要涉及xpu支持的初始化。原因可能是为了使ColossalAI能够支持xpu设备，以提供更全面的硬件适配能力。,https://github.com/hpcaitech/ColossalAI/issues/4004
ColossalAI,该issue类型为功能改进，主要涉及ColossalAI中的sharded optimizers checkpointing功能。由于新开发的功能需要适配到TorchDDPPlugin，因此需要进行适当的调整和整理参数传递。,https://github.com/hpcaitech/ColossalAI/issues/4002
ColossalAI,这是一个用户提出需求的issue，主要涉及实现xpu设备支持的问题，用户提出了初始化PR以支持xpu设备。,https://github.com/hpcaitech/ColossalAI/issues/3999
ColossalAI,这个issue类型为需求提出，主要对象是ColossalAI的Lowlevel Zero插件，用户提出了支持sharded optimizer checkpoint的需求。,https://github.com/hpcaitech/ColossalAI/issues/3998
ColossalAI,这个issue是用户提出的需求，主要对象是在ColossalAI中的shardformer，用户希望将DTensor集成到shardformer中用于参数分片和保存/加载检查点。,https://github.com/hpcaitech/ColossalAI/issues/3997
ColossalAI,这是一个特性改进的issue，主要涉及ColossalAI中的Linear 1D Column模块和DTensor的集成，用户提出了将DTensor整合到Linear 1D Column模块中并进行测试的需求。,https://github.com/hpcaitech/ColossalAI/issues/3996
ColossalAI,该issue属于技术需求提出，主要涉及ColossalAI中shardformer模块对t5模型的支持。产生这个需求可能是因为用户想要在shardformer中使用t5模型。,https://github.com/hpcaitech/ColossalAI/issues/3994
ColossalAI,这个issue类型是PR请求，涉及到ColossalAI的代码库中的optimizer参数的可选性，用户在提交PR时遇到了相关的问题。,https://github.com/hpcaitech/ColossalAI/issues/3993
ColossalAI,这个issue类型是功能需求，主要涉及ColossalAI中的Booster.boost方法，由于当前方法必须同时传入模型和优化器参数，不符合仅推理的情况，导致用户提出调整使得优化器参数为可选的需求。,https://github.com/hpcaitech/ColossalAI/issues/3992
ColossalAI,这个issue是一个功能性问题，涉及的主要对象是设备（device），原因是需要支持从进程组初始化设备网格，这导致了需要对该功能进行代码修改和测试。,https://github.com/hpcaitech/ColossalAI/issues/3990
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中设备的支持，用户希望能够从进程组中初始化设备网格。,https://github.com/hpcaitech/ColossalAI/issues/3989
ColossalAI,这是一个用户提出了需求的issue， 主要涉及对象是 TorchDDPPlugin，原因可能是希望支持Sharded optimizer checkpoint。,https://github.com/hpcaitech/ColossalAI/issues/3986
ColossalAI,这个issue类型是文档更新，涉及主要对象为ColossalAI的文档。由于需要更新文档内容，可能是为了修复现有文档中的错误或者提供更好的指导。,https://github.com/hpcaitech/ColossalAI/issues/3985
ColossalAI,这个issue是一个功能性问题，主要涉及到ColossalAI项目中的checkpointio模块的General Checkpointing of Sharded Optimizers功能。由于需要避免OOM错误，对于sharded optimizers的loading过程需要进行调整。,https://github.com/hpcaitech/ColossalAI/issues/3984
ColossalAI,该issue属于用户提出需求类型，主要涉及的对象是关于如何验证在步骤1生成的模型。由于用户需要指导如何验证该模型，可能是因为缺乏相关文档或指导造成的。,https://github.com/hpcaitech/ColossalAI/issues/3982
ColossalAI,这是一个需求提出的issue，主要涉及的对象是ColossalAI中的`DDPStrategy`和`ColossalAIStrategy`类，由于新booster API的推出，需要对这两个类进行重构。,https://github.com/hpcaitech/ColossalAI/issues/3977
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI下的shardformer模块，用户请求在huggingface的Bert模型中添加down stream model。,https://github.com/hpcaitech/ColossalAI/issues/3974
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中的Shardformer模块，用户需求支持LLaMA sharding，并测试数值数值。,https://github.com/hpcaitech/ColossalAI/issues/3973
ColossalAI,"该issue是针对""evaluate""功能的支持gpt评估与参考。",https://github.com/hpcaitech/ColossalAI/issues/3972
ColossalAI,这个issue是一个功能需求，主要涉及ColossalAI中支持llama模型使用shardformer脚本的问题。这个需求的提出是为了让ColossalAI更好地支持llama模型在shardformer下的应用。,https://github.com/hpcaitech/ColossalAI/issues/3969
ColossalAI,这个issue是一个贡献者提交的同步更新分支的PR，主要对象是ColossalAI项目中的代码分支，由于代码不同步导致的bug或不便。,https://github.com/hpcaitech/ColossalAI/issues/3967
ColossalAI,这个issue类型是增加新功能的请求，主要涉及的对象是ColossalAI的requirements.txt文件，缺少了pytest和decorator这两个包的引用。,https://github.com/hpcaitech/ColossalAI/issues/3963
ColossalAI,这个issue属于功能需求类型，主要涉及到ColossalAI下的CheckpointIO中的Sharded Optimizers，由于优化器的潜在巨大大小，用户需要支持保存和加载功能。,https://github.com/hpcaitech/ColossalAI/issues/3961
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及的对象是ColossalAI下的Elixir模块中的chunk模块。原因是由于代码规范不规范和一些API不易使用而导致模块混乱，用户提出需要对这个模块进行重构。,https://github.com/hpcaitech/ColossalAI/issues/3957
ColossalAI,这是一个改进代码结构的issue，主要涉及到Elixir的chunk模块，通过添加文档注释、重命名变量/属性/类以及重构代码结构来提高代码的可维护性。,https://github.com/hpcaitech/ColossalAI/issues/3956
ColossalAI,这是一个提出需求的issue，主要涉及ColossalAI库中的`Actor`类，由于当前`Actor`类的方法和策略实现复杂，导致开发人员困惑，因此需要对`Actor`类进行重构以使其更清晰。,https://github.com/hpcaitech/ColossalAI/issues/3955
ColossalAI,这是一个用户提出的需求问题，主要涉及ColossalAI中低级zero的兼容性问题，由于当前实现方式导致在初始化过程中会导致每个rank的负载不均衡。,https://github.com/hpcaitech/ColossalAI/issues/3954
ColossalAI,这个issue是关于需求反馈，主要涉及ColossalAI中的低级zero功能，由于当前的实现导致参数分配不均衡，需要改进参数均匀分配的分片方法。,https://github.com/hpcaitech/ColossalAI/issues/3953
ColossalAI,这是一个需求类型的问题，涉及的主要对象是PPO阶段中Actor的lora训练，导致该问题的原因是基于SFT阶段中训练的LM的lora如何在PPO阶段训练Actor的lora。,https://github.com/hpcaitech/ColossalAI/issues/3948
ColossalAI,这个issue类型属于文档优化（Documentation Improvement），主要对象是ColossalAI中的Elixir模块的README文件。由于README文件较为草稿化，需要优化以保持与其他模块README文件的风格一致。,https://github.com/hpcaitech/ColossalAI/issues/3945
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI的Simulator迁移到op_builder的问题。原因是当前Simulator只能在安装时构建，用户希望支持运行时构建以便更容易安装。,https://github.com/hpcaitech/ColossalAI/issues/3938
ColossalAI,这个issue是文档更新（doc）类型，涉及主要对象为示例链接（example links），由于示例链接需要更新，因此创建了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3937
ColossalAI,这是一个用户提出需求的类型，主要涉及Shardformer中添加dropout层的问题；用户为了确保不同的dropout模式而提出这个需求。,https://github.com/hpcaitech/ColossalAI/issues/3935
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是shardformer组件，用户想要增加shardformer的单元测试来验证其准确性。,https://github.com/hpcaitech/ColossalAI/issues/3927
ColossalAI,这个issue类型是特性更新，涉及的主要对象是dtensor/device模块，由于需要将最新的dtensor/device更新添加到develop分支，因此存在这个issue。,https://github.com/hpcaitech/ColossalAI/issues/3926
ColossalAI,这是一个需求类issue，主要对象是文档，用户提出需要添加关于lazy initialization的教程。,https://github.com/hpcaitech/ColossalAI/issues/3922
ColossalAI,这个issue类型是提出需求，涉及的主要对象是Docker部署。由于没有发布到Docker hub所致。,https://github.com/hpcaitech/ColossalAI/issues/3920
ColossalAI,这个issue属于技术改进类型，主要涉及的对象是ColossalAI的开发流程。由于需要更新torch版本以符合兼容性测试，可能由于现有版本的torch在测试中出现了问题，需要进行更新。,https://github.com/hpcaitech/ColossalAI/issues/3919
ColossalAI,该issue是一则有关修改ColossalAI中OPT示例以适配新的booster API的需求报告，涉及主要对象为代码示例和文档更新。由于ColossalAI引入了新的booster API，需要对OPT示例进行修改以适应这一变化。,https://github.com/hpcaitech/ColossalAI/issues/3918
ColossalAI,这个issue属于更新PR类型，涉及对象为ColossalAI的代码版本更新。由于开发分支需要与主分支同步，因此提出了这个更新操作的问题。,https://github.com/hpcaitech/ColossalAI/issues/3915
ColossalAI,"这是一个用户提出需求的issue，主要涉及的对象是""palm example""。原因是用户希望通过使用booster API来修改palm示例。",https://github.com/hpcaitech/ColossalAI/issues/3914
ColossalAI,这个issue是针对一个功能修改的需求，主要涉及到ColossalAI中的palm示例代码，由于新的booster API导致需要对示例代码进行修改。,https://github.com/hpcaitech/ColossalAI/issues/3913
ColossalAI,这个issue类型是需求类型，主要对象是shardformer readme，用户提出希望更新如何定义新策略的文档内容。,https://github.com/hpcaitech/ColossalAI/issues/3909
ColossalAI,这是一个需求类型的 issue，主要对象是 ColossalAI 下的 sharded BERT。由于实现了 sharded BERT，需要确保其数值能与非 sharded 版本对齐，包括输出和梯度。,https://github.com/hpcaitech/ColossalAI/issues/3906
ColossalAI,该issue是一个软件需求提交，主要涉及ColossalAI的新booster API中训练dreambooth的示例添加。由于新增booster API，用户需要示例来帮助他们了解如何在ColossalAI中训练dreambooth。,https://github.com/hpcaitech/ColossalAI/issues/3905
ColossalAI,这个issue类型是用户提出需求。该问题单涉及的主要对象是ColossalAI下的新 booster API，用户提出了希望添加一个关于使用新booster API训练dreambooth的示例的需求。,https://github.com/hpcaitech/ColossalAI/issues/3904
ColossalAI,这个issue是一个开发工具相关的需求报告，主要涉及的对象是针对ColossalAI中的测试缓存的改进。这个需求可能是由于测试缓存功能不够完善或者存在bug而导致的。,https://github.com/hpcaitech/ColossalAI/issues/3902
ColossalAI,这个issue类型是改进优化提取缓存，主要对象是测试监控缓存（testmon cache），由于缓存提取不够高效导致需优化。,https://github.com/hpcaitech/ColossalAI/issues/3901
ColossalAI,这个issue是关于[devops] improving testmon-cache的优化，主要涉及优化testmon缓存，原因可能是为了提高系统性能。,https://github.com/hpcaitech/ColossalAI/issues/3900
ColossalAI,这个issue属于文档更新类型，涉及主要对象为ColossalAI的中文文档，由于需更新MOE的中文文档而创建。,https://github.com/hpcaitech/ColossalAI/issues/3890
ColossalAI,这是一个关于更新bert示例使用booster api的PR类型。用户主要涉及的对象是ColossalAI项目中的代码实现。这个PR的目的是为了使bert示例能够利用booster api进行更新。,https://github.com/hpcaitech/ColossalAI/issues/3885
ColossalAI,该issue类型为新功能添加，涉及的主要对象是在ColossalAI中添加SFT dataset类的测试。由于缺少SFT dataset类的测试，用户对于需要添加此功能提出了一个issue。,https://github.com/hpcaitech/ColossalAI/issues/3884
ColossalAI,这是一个功能需求的issue，涉及到对GPT2的支持以及修改shard和slicer代码以支持multihead attention layer。由于需要支持新的功能并修改现有代码以适应新功能，因此这个issue的出现是为了解决系统无法支持该功能的问题。,https://github.com/hpcaitech/ColossalAI/issues/3883
ColossalAI,这个issue属于功能更新类型，主要涉及到ColossalAI的bf16支持。由于需求更新和优化的需要，团队在该issue中添加了bf16支持相关的功能和优化。,https://github.com/hpcaitech/ColossalAI/issues/3882
ColossalAI,这个issue是一个功能需求的提出，主要涉及ColossalAI框架下的BF16支持。由于用户需求需要在legacy zero中添加BF16支持，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3879
ColossalAI,该issue是一个功能提升类型的问题单，涉及的主要对象是ColossalAI下的某些插件，由于需要在low level zero和gemini插件中添加bf16支持，因此产生了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3877
ColossalAI,此Issue类型为特性需求，主要涉及ColossalAI添加Elixir功能和其插件。由于缺乏Elixir功能和插件，用户提出了此需求。,https://github.com/hpcaitech/ColossalAI/issues/3873
ColossalAI,这个issue类型是新增功能的提交，主要涉及Gemini模块的bf16支持，用户提出了关于支持BF16混合精度训练的需求。,https://github.com/hpcaitech/ColossalAI/issues/3872
ColossalAI,这个issue类型是PR请求，主要对象是ColossalAI中的Elixir模块，通过更新缓冲区大小的计算来进行优化。,https://github.com/hpcaitech/ColossalAI/issues/3871
ColossalAI,该issue是一个特性请求，涉及的主要对象是ColossalAI中的混合精度和bf16支持。这个问题是由于对混合精度mixin类和bf16支持的低层次优化请求而引起的。,https://github.com/hpcaitech/ColossalAI/issues/3869
ColossalAI,这个issue类型为更新Gemini示例文档，涉及主要对象为Gemini示例。由于Gemini示例中使用了booster训练，可能出现了一些问题，用户提出更新以解决此问题。,https://github.com/hpcaitech/ColossalAI/issues/3868
ColossalAI,这是一个用户提出需求的issue，主要对象是Gemini示例。由于Gemini示例需要更新以使用booster训练，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/3867
ColossalAI,这个issue类型是功能增强请求，主要涉及的对象是新增添加Elixir插件及其单元测试，由于缺乏Elixir插件和对应的单元测试而提出的需求。,https://github.com/hpcaitech/ColossalAI/issues/3865
ColossalAI,这个issue类型为新功能需求，涉及的主要对象是为ColossalAI添加Elixir插件和相应的单元测试。原因可能是为了扩展ColossalAI的功能范围以及提高代码覆盖率。,https://github.com/hpcaitech/ColossalAI/issues/3864
ColossalAI,该issue属于特性需求，涉及到ColossalAI中的mixed precision mixin。由于需要支持更多低精度优化器，提出了使用一个新类来控制所有种类低精度优化器行为的建议。,https://github.com/hpcaitech/ColossalAI/issues/3863
ColossalAI,这是一个需求类型的issue，主要涉及到用户询问关于在ColossalAI使用自定义模型训练的问题。,https://github.com/hpcaitech/ColossalAI/issues/3861
ColossalAI,这个issue是一个功能性问题，主要涉及ColossalAI的CPU Adam和混合Adam优化器支持bf16的问题。由于没有对bf16做本地支持，导致了一些功能性缺陷。,https://github.com/hpcaitech/ColossalAI/issues/3860
ColossalAI,这个issue的类型是用户提出需求，建议改进文档书写。该问题单涉及的主要对象是ColossalAI的文档。由于教程写作不详细，用户认为需要按部就班地提供更详细的安装和使用说明。,https://github.com/hpcaitech/ColossalAI/issues/3859
ColossalAI,这是一个功能增强的issue，主要涉及ColossalAI的shardformer模块的Dropout层支持不同的dropout模式。这个问题出现的原因是想要在训练时添加Dropout层以支持不同的dropout模式，并且在Dropout时修改本地随机状态。,https://github.com/hpcaitech/ColossalAI/issues/3856
ColossalAI,这个issue是文档更新，主要对象是项目ColossalAI下的zero with chunk功能。由于标题错误导致混合精度有问题。,https://github.com/hpcaitech/ColossalAI/issues/3855
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是SFT代码。由于缺乏保存和加载模型以及优化器的功能，用户提出希望添加这一功能以提升用户体验。,https://github.com/hpcaitech/ColossalAI/issues/3851
ColossalAI,这是一个文档更新的issue，涉及主要对象为NVMe offload，原因可能是文档内容需要更新。,https://github.com/hpcaitech/ColossalAI/issues/3850
ColossalAI,这个issue是文档更新类型的，涉及的主要对象是nvme offload文档。由于更新文档可能存在内容过时或需要完善，因此需要对nvme offload文档进行更新。,https://github.com/hpcaitech/ColossalAI/issues/3846
ColossalAI,这个issue是关于更新ColossalAI中DTensor的API和文档，不是一个bug报告，主要涉及对象是DTensor。由于API和文档需要更新，所以提交了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/3845
ColossalAI,这个issue是一个功能增强类的问题，主要涉及的对象是ColossalAI中的fused adam优化器。这个问题由于缺乏bf16（bfloat16）支持导致。,https://github.com/hpcaitech/ColossalAI/issues/3844
ColossalAI,这是一个文档更新的Issue，主要涉及Gemini指令的说明更新。原因可能是文档需要更新以提供更准确和清晰的指导。,https://github.com/hpcaitech/ColossalAI/issues/3842
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI库中的BF16精度训练支持，由于AVX不支持直接加载BF16，导致CPU的Adam核心可能无法实现BF16，用户提出了实现BF16混合精度训练的可能解决方案。,https://github.com/hpcaitech/ColossalAI/issues/3839
ColossalAI,该issue属于代码改进类型，主要涉及到ColossalAI中的dtensor模块，由于缺少对功能实现的详细文档和自审导致需要polish sharding spec docstring。,https://github.com/hpcaitech/ColossalAI/issues/3838
ColossalAI,这个issue类型是一个功能需求，主要涉及对象是将Elixir集成到ColossalAI中，由于需要在项目中添加Elixir并编写其单元测试。,https://github.com/hpcaitech/ColossalAI/issues/3835
ColossalAI,这个issue是一个文档更新类型的问题，主要对象是ColossalAI中的shardformer模块，由于缺少实现文档导致需要更新readme文件。,https://github.com/hpcaitech/ColossalAI/issues/3834
ColossalAI,这个issue是文档更新类型，涉及修改placement_policy到placemarkm_policy的拼写错误。,https://github.com/hpcaitech/ColossalAI/issues/3829
ColossalAI,这是一个特性优化类的issue，主要涉及到ColossalAI下的shardformer模块，由于功能需求和代码清理，对用户API进行了重构。,https://github.com/hpcaitech/ColossalAI/issues/3828
ColossalAI,这个issue属于文档更新类型，主要涉及ColossalAI的模块shardformer的README文档更新，用户在完成代码自审后进行了此文档更新。可能的原因是为了确保项目文档的准确性和完整性。,https://github.com/hpcaitech/ColossalAI/issues/3827
ColossalAI,这个issue是一个更改请求，主要涉及ColossalAI的代码文件夹结构，请求移除sample文件夹。原因可能是不再需要这个样本文件夹，或者要对代码结构进行调整。,https://github.com/hpcaitech/ColossalAI/issues/3823
ColossalAI,该issue类型为功能改进，主要涉及移除GPT-4配置，可能由于不再需要或存在问题而导致。,https://github.com/hpcaitech/ColossalAI/issues/3822
ColossalAI,该issue类型为功能添加请求，涉及添加自动评估流水线，用户寻求帮助于添加自动评估流水线。,https://github.com/hpcaitech/ColossalAI/issues/3821
ColossalAI,这个issue类型是添加功能，该问题单涉及的主要对象是添加中文示例，由于需要为config、battle prompt和evaluation prompt文件添加中文示例而创建。,https://github.com/hpcaitech/ColossalAI/issues/3820
ColossalAI,这是一个开发者提交的功能需求，涉及到ColossalAI框架中的评估模块，原因是需要添加一个评估流水线。,https://github.com/hpcaitech/ColossalAI/issues/3817
ColossalAI,该issue类型是功能改进，主要对象是shardformer模块，用户提出了需求添加docstring和readme以及测试用例。,https://github.com/hpcaitech/ColossalAI/issues/3816
ColossalAI,这个issue属于文档更新类别，涉及主要对象是FSDP（Fully Sharded Data Parallelism）插件，由于FSDP插件目前不支持分片模型检查点，用户提出需要在文档中添加警告。,https://github.com/hpcaitech/ColossalAI/issues/3813
ColossalAI,这是一个用户需求问题，涉及主要对象是如何在特定GPU上运行`train_sft.sh`脚本，由于默认使用命令“torchrun standalone –nprocpernode=n train_sft.sh”会从第一个GPU开始训练，用户希望了解如何在指定GPU上运行。,https://github.com/hpcaitech/ColossalAI/issues/3811
ColossalAI,这个issue是文档更新类型，主要涉及ColossalAI下的AMP，由更新文档引起。,https://github.com/hpcaitech/ColossalAI/issues/3810
ColossalAI,这个issue是贡献者更新评估流程的readme.md文件，属于文档更新类型，涉及主要对象是评估流程。该问题由于需要保证PR的规范性和对问题的详细描述，可能是为了确保代码质量和开发效率。,https://github.com/hpcaitech/ColossalAI/issues/3809
ColossalAI,该issue类型是功能需求，并涉及支持gpt评估模块，由于遗留代码未使用precommit进行格式化导致此问题。,https://github.com/hpcaitech/ColossalAI/issues/3807
ColossalAI,这个issue类型是文档更新，涉及的主要对象是ColossalAI下的AMP模块。原因可能是需要更新AMP模块的文档以提供更准确的信息。,https://github.com/hpcaitech/ColossalAI/issues/3805
ColossalAI,这个issue类型为文档更新，主要对象为ColossalAI中的amp模块，由于更新文档导致。,https://github.com/hpcaitech/ColossalAI/issues/3804
ColossalAI,这是一个用户提出需求的类型，涉及的主要对象是ColossalAI的CI workflows。原因是用户希望CI能够在针对develop和feature分支的PR时也运行，以支持开发进程。,https://github.com/hpcaitech/ColossalAI/issues/3802
ColossalAI,这个issue类型是功能需求提议类型，涉及主要对象是ColossalAI项目的开发流程。由于测试工作流程条件尚未添加到develop和feature分支，需要解决此问题以确保代码质量和稳定性。,https://github.com/hpcaitech/ColossalAI/issues/3801
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的GPT模型。该问题由于用户想要启用GPT模型中的flash attention，但在源代码中难以实现，因此请求提供一个示例支持该功能。,https://github.com/hpcaitech/ColossalAI/issues/3798
ColossalAI,这个issue是一个功能需求，主要涉及到ColossalAI的自动评估模块，并为其添加新的功能。原因可能是为了提供更全面的评估功能以提高用户体验。,https://github.com/hpcaitech/ColossalAI/issues/3792
ColossalAI,这个issue是一个代码贡献请求，主要涉及ColossalAI中的torch fsdp模块，由于修复了checkpoint相关的问题。,https://github.com/hpcaitech/ColossalAI/issues/3788
ColossalAI,这个issue属于用户提出需求，主要涉及的对象是ColossalAI的文档交流和贡献流程。用户提出了关于新增性能和教程的请求，希望为项目添加相关内容。,https://github.com/hpcaitech/ColossalAI/issues/3786
ColossalAI,这个issue是文档更新类型的问题，涉及ColossalAI中的Booster模块，主要是添加了有关Booster检查点的教程，以及对插件检查点的注意事项。这个问题可能是由于缺乏关于Booster检查点的详细文档而导致的。,https://github.com/hpcaitech/ColossalAI/issues/3785
ColossalAI,这个issue类型是用户提出需求，涉及的主要对象是ColossalAI中的`Booster`类，由于缺少`Booster`类关于checkpoint的文档描述，用户提出了添加文档描述和撰写教程的需求。,https://github.com/hpcaitech/ColossalAI/issues/3784
ColossalAI,这个issue类型是改进类型，涉及的主要对象是ColossalAI中的apex amp和naive amp模块，用户提出了需要为这两个模块添加文档字符串和初始化的需求。,https://github.com/hpcaitech/ColossalAI/issues/3783
ColossalAI,这个issue类型是功能新增，涉及主要对象为gpt评估功能的添加，由于缺少gpt评估功能而引起用户在ColossalAI中进行评估时遇到问题。,https://github.com/hpcaitech/ColossalAI/issues/3781
ColossalAI,这个issue是文档更新类型，涉及主要对象为梯度剪裁（gradient clipping），用户因为需要更新梯度剪裁文档而提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3778
ColossalAI,这是一个功能增强的issue，主要涉及的对象是ColossalAI中的torch ddp plugin，用户提供了对支持分片模型检查点的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/3775
ColossalAI,这是一个功能新增的issue，主要涉及的对象是ColossalAI，用户提出了添加naive amp的需求。,https://github.com/hpcaitech/ColossalAI/issues/3774
ColossalAI,这是一个功能需求的issue，主要涉及到Torch DDP插件中缺少对`save_sharded_model()`方法的覆写。,https://github.com/hpcaitech/ColossalAI/issues/3773
ColossalAI,这是一个文档更新的issue，涉及到更新梯度积累的文档。原因可能是为了提供最新的文档信息或者可能相关文档内容有误。,https://github.com/hpcaitech/ColossalAI/issues/3771
ColossalAI,这个issue类型是文档更新，涉及的主要对象是混合并行性文档更新，由于缺少测试用例和函数方法的文档字符串，未能满足完整性要求。,https://github.com/hpcaitech/ColossalAI/issues/3770
ColossalAI,这个issue属于文档更新类型，涉及主要对象为booster training。该issue提出是为了哪些不清晰或过时的文档导致的用户使用上的困惑。,https://github.com/hpcaitech/ColossalAI/issues/3769
ColossalAI,这是一个文档更新的issue，涉及的主要对象是混合精度训练教程。由于缺少指示，需要更新混合精度训练教程。,https://github.com/hpcaitech/ColossalAI/issues/3767
ColossalAI,该issue属于功能更新类型，主要涉及到ColossalAI中的示例代码更新，涉及amp、梯度累积和梯度裁剪示例的更新。原因可能是为了改进示例代码的准确性和易用性。,https://github.com/hpcaitech/ColossalAI/issues/3765
ColossalAI,这是一个用户提出需求类型的issue，主要对象是ColossalAI中的文档更新，可能由于混合精度训练教程内容需要更新而创建。,https://github.com/hpcaitech/ColossalAI/issues/3764
ColossalAI,这是一个文档更新的Issue，涉及到添加关于集群工具类`DistCoordinator`的教程。,https://github.com/hpcaitech/ColossalAI/issues/3763
ColossalAI,这是一个需求类型的issue，主要涉及的对象是ColossalAI库的cluster工具包。用户提出需要为`DistCoordinator`添加教程的需求。,https://github.com/hpcaitech/ColossalAI/issues/3762
ColossalAI,此issue属于需求类型，主要涉及到ColossalAI的文档添加和整理工作，由于缺乏相关的booster plugins tutorial，导致用户可能无法准确使用相关功能。,https://github.com/hpcaitech/ColossalAI/issues/3758
ColossalAI,该issue类型为需求提出，主要涉及ColossalAI的booster插件教程缺失。原因是当前booster插件缺乏相应的教程。,https://github.com/hpcaitech/ColossalAI/issues/3757
ColossalAI,这是一个文档更新的issue，主要涉及ColossalAI文档中基础部分的deprecated警告添加。,https://github.com/hpcaitech/ColossalAI/issues/3754
ColossalAI,这个issue是一个改进需求，涉及到CI/CD构建流程自动化，主要对象是开发者。这个问题的根本原因是标签顺序影响构建次数，导致CI流程不稳定。,https://github.com/hpcaitech/ColossalAI/issues/3747
ColossalAI,这个issue是关于功能开发的，主要涉及到ColossalAI下的chat模块和Ray的集成。根据描述，用户提出了需求希望实现基于Ray的分布式PPO，并进行相关ci测试和更新readme。,https://github.com/hpcaitech/ColossalAI/issues/3741
ColossalAI,这个issue是一个功能增强相关的问题，主要涉及的对象是ColossalAI中分布式PPO训练器的添加。最终用户提出这个需求可能是为了提升系统的性能和功能。,https://github.com/hpcaitech/ColossalAI/issues/3740
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及ColossalAI中的ColossalChat模型的训练内存需求。由于资源限制，用户希望收集成功用户案例来探讨实际需求和最佳策略设置。,https://github.com/hpcaitech/ColossalAI/issues/3734
ColossalAI,该issue类型为新功能（Feature）请求，主要涉及的对象是shardformer模块的代码结构初始化。这一需求可能是为了优化代码组织和维护性。,https://github.com/hpcaitech/ColossalAI/issues/3731
ColossalAI,这个issue属于更新文档类型，主要对象是ColossalAI的文档。由于更新教程，用户可能遇到不明确或过时的说明而需要帮助。,https://github.com/hpcaitech/ColossalAI/issues/3730
ColossalAI,这个issue类型是功能新增，主要涉及的对象是关于自动化指标的添加。由于缺少自动度量，用户希望添加此功能。,https://github.com/hpcaitech/ColossalAI/issues/3728
ColossalAI,这是一个用户提出需求的问题，主要涉及如何训练多轮对话系统以及生成数据集。用户询问如何训练对话系统以及如何生成多轮对话数据格式。,https://github.com/hpcaitech/ColossalAI/issues/3726
ColossalAI,这是一个关于更新ColossalAI CI中torch版本的开发运维问题，涉及到解决测试失败、启用lazy init测试和testmon等任务。可能由于torch版本的更迭导致的测试失败等问题而提出。,https://github.com/hpcaitech/ColossalAI/issues/3725
ColossalAI,该issue类型为文档更新请求，涉及ColossalAI文档中基础部分的标记问题，原因是需要将Basics部分标记为已弃用。,https://github.com/hpcaitech/ColossalAI/issues/3722
ColossalAI,这个issue是关于文档更新的，主要涉及到ColossalAI中Booster教程的更新。由于教程需要引导用户初始化并行特性，因此需要更新文档。,https://github.com/hpcaitech/ColossalAI/issues/3718
ColossalAI,这个issue类型是用户提出需求，主要涉及的对象是booster tutorials。由于教程内容需要更新，用户提出了更新的请求。,https://github.com/hpcaitech/ColossalAI/issues/3717
ColossalAI,这是一个更新booster测试的issue，类型为功能需求，涉及的主要对象是ColossalAI中的booster模块。,https://github.com/hpcaitech/ColossalAI/issues/3716
ColossalAI,这是一个用户提出需求的类型的issue，涉及ColossalAI中Evaluation Module的技术设计，由于需要为每个问题记录特定字段的数据格式，因此提出了关于问题记录内容和答案的详细规范。,https://github.com/hpcaitech/ColossalAI/issues/3714
ColossalAI,这个issue是一个需求提出类型的问题，主要涉及ColossalAI中的API重构，其中涉及了更新booster测试，并最终需要对tutorial文档和booster API进行改进。这个问题的产生可能是为了提高API的性能和可靠性。,https://github.com/hpcaitech/ColossalAI/issues/3713
ColossalAI,该问题类型为功能增强需求，主要涉及对象为API Refactoring中的booster tests。由于缺乏相关的教程文档以及booster单元测试的不完善，用户提出了需要更新booster tests的需求。,https://github.com/hpcaitech/ColossalAI/issues/3712
ColossalAI,这个issue属于功能需求类型，主要涉及ColossalAI的PR创建前的检查清单，包括创建issue、标准格式的标题、添加标签等步骤。原因是为了保证贡献者的PR质量和跟踪问题。,https://github.com/hpcaitech/ColossalAI/issues/3711
ColossalAI,这个issue是一个功能需求的提议，主要涉及到ColossalAI中的booster模块。由于方法命名不规范，需要更新准备数据加载器方法以及相应示例。,https://github.com/hpcaitech/ColossalAI/issues/3706
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的DPPluginBase类中方法prepare_train_dataloader的改进。由于方法应该在基类Plugin中通用定义，需要定义新方法prepare_dataloader并更新相关示例。,https://github.com/hpcaitech/ColossalAI/issues/3705
ColossalAI,这个issue是用户提出需求，并询问关于Colossal Chat的预训练模型。用户想要了解该模型的特性，并寻求获取已有的预训练模型以进行测试。,https://github.com/hpcaitech/ColossalAI/issues/3700
ColossalAI,这是一个提出需求的类型，主要涉及Gemini Zero的优化器类型限制的问题。原因是当前Gemini Zero的优化器选择受限于梯度为FP16、主要权重为FP32的设定。,https://github.com/hpcaitech/ColossalAI/issues/3699
ColossalAI,这是一个功能更新的issue，涉及的主要对象是ColossalAI下的一个示例代码，更新内容包括在CIFAR10上添加ViT模型的训练示例以及支持torch DDP和低级zero。,https://github.com/hpcaitech/ColossalAI/issues/3694
ColossalAI,这是一个用户提交需求的issue，主要涉及ColossalAI中添加finetune bert with booster示例，可能由于缺乏该功能而导致需要用户提交该需求。,https://github.com/hpcaitech/ColossalAI/issues/3693
ColossalAI,这是一个用户提出需求的issue，主要涉及Hugging Face是否官方支持LLaMA模型的问题。用户可能由于Hugging Face对LLaMA模型的支持情况存在疑惑而提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3686
ColossalAI,这个issue是一个文档改进类型的问题，主要涉及ColossalAI中的文档更新，并针对一个特定bug进行了说明。由于jsonl文件读取错误导致的问题。,https://github.com/hpcaitech/ColossalAI/issues/3679
ColossalAI,这是一个用户提出的需求类的issue，涉及到ColossalAI中ZeRO3对优化器类型的限制问题。原因可能是用户希望解除限制以使用更多类型的优化器。,https://github.com/hpcaitech/ColossalAI/issues/3678
ColossalAI,这是一个用户提出需求的类型，主要涉及对象是代码示例。用户希望ColossalAI能够提供一个更通用的微调代码示例。,https://github.com/hpcaitech/ColossalAI/issues/3676
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI是否能够使用transformers的from_pretrained方法加载预训练模型。,https://github.com/hpcaitech/ColossalAI/issues/3669
ColossalAI,这个issue是关于修改默认策略以解决OOM错误报告的功能需求，并跟踪问题处理过程。,https://github.com/hpcaitech/ColossalAI/issues/3667
ColossalAI,这是一个用户提出需求的文档问题，主要涉及到EnergonAI的推理加速硬件以及GPU内存显示的问题。,https://github.com/hpcaitech/ColossalAI/issues/3665
ColossalAI,这是一个用户提出需求的issue，主要涉及2D tensor parallelism下计算损失和反向传播的问题，由于输出尺寸和标签尺寸不匹配导致无法计算损失。,https://github.com/hpcaitech/ColossalAI/issues/3664
ColossalAI,这个issue是关于文档的改进，主要涉及到ColossalAI中的问题重新格式化的步骤。由于当前文档中含有不必要的步骤，需要优化以提高操作效率。,https://github.com/hpcaitech/ColossalAI/issues/3657
ColossalAI,这个issue类型为功能更新，涉及到Chat模块，由于需求中存在无关步骤导致的文档不清晰。,https://github.com/hpcaitech/ColossalAI/issues/3656
ColossalAI,这个issue是一个功能性需求，主要涉及的对象是ColossalAI项目中的OPT模型，用户提出了添加xformer attention kernel支持的需求。,https://github.com/hpcaitech/ColossalAI/issues/3655
ColossalAI,这个Issue是关于删除ColossalAI中的LM模型类的改进提案，主要涉及代码设计和优化。由于LM模型类没有引入任何有用的属性或方法，造成了不必要的设计，因此提出了删除该类以提高用户友好性的改进。,https://github.com/hpcaitech/ColossalAI/issues/3653
ColossalAI,这个issue类型为特性需求，主要涉及的对象为ColossalAI中的 `cmap` 函数。由于用户希望引入类似于 `jax.pmap` 的并行映射功能，因此提出了这个特性需求。,https://github.com/hpcaitech/ColossalAI/issues/3652
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中并行映射功能的实现，用户提出了需要实现类似于jax.pmap的并行映射函数封装。,https://github.com/hpcaitech/ColossalAI/issues/3651
ColossalAI,这个issue是一个功能改进请求，主要涉及到ColossalAI的性能评估器的优化和改进。导致提出这个需求的原因可能是目前性能评估器打印信息不够清晰和详细。,https://github.com/hpcaitech/ColossalAI/issues/3647
ColossalAI,这个issue是一个文档增强类型的需求报告，主要涉及ColossalAI的README.md文件中关于聊天示例的相关内容。由于缺少详细的指导说明，用户提出了关于文档内容的更新需求。,https://github.com/hpcaitech/ColossalAI/issues/3646
ColossalAI,这是一篇针对ColossalAI中TensorShard模块的需求提出，主要涉及到对Huggingface模型的加速优化。,https://github.com/hpcaitech/ColossalAI/issues/3640
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的示例代码在多节点上运行的问题。可能是用户想要了解如何在多个节点上运行示例，或者遇到了单节点运行示例时的问题。,https://github.com/hpcaitech/ColossalAI/issues/3634
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中的zero+gemini模块支持gradient accumulation。,https://github.com/hpcaitech/ColossalAI/issues/3632
ColossalAI,该issue属于技术改进类，涉及ColossalAI项目中文件夹结构的重构。由于文件夹结构混乱，导致需要将部分文件夹移动到新的位置以优化项目结构。,https://github.com/hpcaitech/ColossalAI/issues/3627
ColossalAI,这是一个用户提交需求的issue，主要涉及ColossalAI中关于更新CI的功能。由于项目规范要求，用户在此issue中列出了创建PR前需要完成的检查事项。,https://github.com/hpcaitech/ColossalAI/issues/3619
ColossalAI,这是一个用户提出需求的类型，主要涉及对象为DeepSpeed训练模型和数据库。用户因为计算资源有限，想知道如何准备语料和预训练模型以实现在对话方式中访问数据库并获取所需的数据统计形式。,https://github.com/hpcaitech/ColossalAI/issues/3616
ColossalAI,这是一个用户需求问题，涉及主要对象是如何在使用1080ti/P40时运行ColossalAI模型，出现这个问题可能是由于硬件兼容性或版本支持的原因。,https://github.com/hpcaitech/ColossalAI/issues/3615
ColossalAI,这个issue是一个功能请求，涉及Gemini插件支持分片检查点，由于避免了大文件检查点的问题而产生。,https://github.com/hpcaitech/ColossalAI/issues/3610
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是gemini插件。由于没有支持shard checkpoint，导致了大型的checkpoint文件的问题。,https://github.com/hpcaitech/ColossalAI/issues/3609
ColossalAI,这个issue是一个特性需求，涉及Chat应用的评估功能问题，用户希望添加Chat应用的评估功能。,https://github.com/hpcaitech/ColossalAI/issues/3608
ColossalAI,这个issue属于用户需求类型，主要涉及ColossalAI中Ray实现的PPO算法，用户询问未来哪种实现方式会被维护。,https://github.com/hpcaitech/ColossalAI/issues/3607
ColossalAI,这是一个功能需求提案，主要涉及Gemini插件和Shard的checkpoint保存和加载功能。可能是用户希望实现在Gemini插件中添加对Shard的checkpoint保存和加载功能。,https://github.com/hpcaitech/ColossalAI/issues/3600
ColossalAI,这是一个类型为功能需求的issue，主要涉及自动化更新子模块提交的PR。,https://github.com/hpcaitech/ColossalAI/issues/3596
ColossalAI,这是一个功能增强类型的issue，主要涉及到ColossalAI的booster功能，由于需要添加低级别的zero插件和相应的测试。,https://github.com/hpcaitech/ColossalAI/issues/3594
ColossalAI,这个issue类型是优化代码，主要涉及的对象是源码中的拼写错误，由于拼写错误导致了文档不准确。,https://github.com/hpcaitech/ColossalAI/issues/3593
ColossalAI,这是一个需求提出的issue，主要涉及Gemini模块的状态字典（state dict），提出需要支持fp16以节省内存。,https://github.com/hpcaitech/ColossalAI/issues/3590
ColossalAI,这是一个文档更新的issue，涉及的主要对象是ColossalAI的README.md文件。由于风格不一致，用户提出了调整社区示例标题风格的问题。,https://github.com/hpcaitech/ColossalAI/issues/3586
ColossalAI,这是一个关于优化内存使用的issue，涉及主要对象为ColossalAI中的Llama 13B模型。由于内存使用过高导致内存溢出错误，用户寻求优化内存使用的方法。,https://github.com/hpcaitech/ColossalAI/issues/3583
ColossalAI,这是一个功能需求的issue，主要涉及Gemini模块下的save state dict功能。原因是为了减少内存使用量。,https://github.com/hpcaitech/ColossalAI/issues/3581
ColossalAI,该issue类型为特性需求，主要涉及ColossalAI的社区模型支持指南。由于缺乏社区用户对于社区模型的支持指南，因此用户提出了需要添加此指南的需求。,https://github.com/hpcaitech/ColossalAI/issues/3579
ColossalAI,这是一个关于更新README.md的问题单，类型是文档更新，主要涉及到ColossalAI项目文档的优化，问题由额外输入字符导致。,https://github.com/hpcaitech/ColossalAI/issues/3577
ColossalAI,这是一个需要更新展示格式的issue，主要涉及到ColossalAI的1D tensor并行文档。由于原来的版本存在展示格式问题，因此需要进行同时修改和更新。,https://github.com/hpcaitech/ColossalAI/issues/3573
ColossalAI,这是一个文档改进的issue，主要涉及ColossalAI的zero3模块下的chunk搜索工具的文档缺失问题，可能由于之前缺乏适当的函数文档而导致用户难以理解其用途和用法。,https://github.com/hpcaitech/ColossalAI/issues/3572
ColossalAI,"这是一个用户提出需求的issue，主要涉及ColossalAI中关于添加无监督学习部分的功能。用户提出了关于如何训练特定行业领域的""专业语言模型""，并询问是否有一种方法可以用非问答形式的大量文本数据（如论文、书籍、报纸）进行训练。",https://github.com/hpcaitech/ColossalAI/issues/3570
ColossalAI,该issue类型为功能需求，涉及主要对象为ColossalAI中的PPO训练和ChatGLM-6B模型支持。由于需求在单个显卡上为大型模型和ChatGLM-6B模型提供训练支持，以及添加Peft reward模型和数据预处理支持，所以有了这个功能需求。,https://github.com/hpcaitech/ColossalAI/issues/3567
ColossalAI,这是一个用户提出需求的issue，主要涉及PPO training中的内存管理问题，用户请求实现一种显存友好的训练方式。,https://github.com/hpcaitech/ColossalAI/issues/3566
ColossalAI,这是一个功能需求的issue，主要涉及的对象是ChatGLM模型支持。,https://github.com/hpcaitech/ColossalAI/issues/3565
ColossalAI,这是一个用户提出需求的类型，该问题主要涉及ColossalAI不能训练seq2seq模型的功能。用户可能因为当前提供的脚本只适用于causalLM模型而无法训练seq2seq模型而感到困惑。,https://github.com/hpcaitech/ColossalAI/issues/3564
ColossalAI,这个issue是关于文档内容更新的，涉及ColossalAI中的1D_tensor_parallel.md文件的显示格式优化和翻译修改。,https://github.com/hpcaitech/ColossalAI/issues/3563
ColossalAI,"这个issue是一个功能改进类的请求，涉及到Docker镜像推送的更新。由于缺少""latest""标签，可能会导致在使用""docker pull hpcaitech/colossalai""命令时出现错误。",https://github.com/hpcaitech/ColossalAI/issues/3561
ColossalAI,这是一个功能改进的issue，主要涉及ColossalAI中零元素和操作构建器的verbose参数的添加。,https://github.com/hpcaitech/ColossalAI/issues/3552
ColossalAI,这个issue是关于文档教程的改进，主要涉及修订README文档，可能由于主README与示例README相似但有所不同，导致用户误解。,https://github.com/hpcaitech/ColossalAI/issues/3551
ColossalAI,这是一个文档更新类的issue，主要涉及ColossalAI项目的README.md文件的格式优化问题。由于缺少外部方括号导致格式不规范，需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/3549
ColossalAI,这是一个用户提出需求的issue，涉及ColossalAI中的train_prompts脚本，主要问题在于缺少max_len参数和一些样本数据的需求。,https://github.com/hpcaitech/ColossalAI/issues/3539
ColossalAI,这是一个文档更新的issue，主要涉及ColossalAI的Chat模块，用户在此提交了关于在chat readme中添加有关使用有限资源进行训练的示例的需求。,https://github.com/hpcaitech/ColossalAI/issues/3536
ColossalAI,这个issue属于用户提出需求类型，涉及的主要对象是ColossalAI中的BLOOM模型。由于用户想要了解BLOOM模型是否被支持以及如何像medical model with LLaMA33B一样执行任务调整。,https://github.com/hpcaitech/ColossalAI/issues/3530
ColossalAI,这是一个用户提出需求的issue，主要涉及Gemini框架支持懒惰初始化功能，通过Gemini插件来优化ColoInitContext、LazyInitContext和普通初始化。,https://github.com/hpcaitech/ColossalAI/issues/3529
ColossalAI,这个issue类型为功能更新，主要涉及ColossalAI中Diffusion Code的更新。由于更新稳定性扩散代码以提高性能，但仍需进一步测试和反馈。,https://github.com/hpcaitech/ColossalAI/issues/3522
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的CPU策略，希望添加zero2 CPU策略用于sft训练。,https://github.com/hpcaitech/ColossalAI/issues/3520
ColossalAI,该issue属于文档相关的需求，主要涉及ColossalAI中隐藏diffusion在应用路径中的问题。用户可能因为难以找到所支持的Stable Diffusion和Dreambooth而感到困惑。,https://github.com/hpcaitech/ColossalAI/issues/3519
ColossalAI,这是一个优化代码的issue，涉及ColossalAI中的send_message_to_lark.py文件，通过简化代码和直接使用sys.argv获得命令行参数来提高代码简洁性和可读性。,https://github.com/hpcaitech/ColossalAI/issues/3513
ColossalAI,这是一个用户提出需求的issue，主要涉及8-bits NLP model training mode的功能需求。原因是当前FP16支持只能训练小于2B的模型，无法载入6~7B大小的模型，导致8bits模型训练失败。,https://github.com/hpcaitech/ColossalAI/issues/3508
ColossalAI,这个issue是一个需求类型，主要涉及ColossalChat社区的README文件更新，提出开放社区未来可能提出的功能建议。它由于未添加相关标签，导致缺乏PR分类与追踪的完整性。,https://github.com/hpcaitech/ColossalAI/issues/3506
ColossalAI,这个issue是文档更新类，涉及主要对象为ColossalAI下的zero optim模块，由于缺少文档描述导致PR中的相关要求未满足。,https://github.com/hpcaitech/ColossalAI/issues/3504
ColossalAI,这是一个用户提出需求的类型，主要对象是ColossalAI的文档。由于文档较少详细，新手在部署和训练时遇到困难。,https://github.com/hpcaitech/ColossalAI/issues/3500
ColossalAI,这是一个用户提出需求的Issue，主要涉及PPO中设置不同模型的tokenizer的问题。可能由于train_prompts.py中只允许设置一个tokenizer，导致无法为不同模型分别设置tokenizer。,https://github.com/hpcaitech/ColossalAI/issues/3492
ColossalAI,这个issue是关于功能需求的，主要涉及ColossalAI是否支持同时部署多个模型，可能使用Ray框架，用户询问是否当前版本中有相关支持或正在开发，并提及与AlpaServe论文相关的课程/研究项目。,https://github.com/hpcaitech/ColossalAI/issues/3491
ColossalAI,这个issue属于代码风格优化类型，主要对象是ColossalAI的diffusion代码。这个issue由于缺少相关标签，部分任务未完成而需要进一步完善。,https://github.com/hpcaitech/ColossalAI/issues/3488
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI项目中的Community Pipelines。由于项目进行了 major updates，为了让开发者能够贡献特色功能并促进社区合作，引入了Community-driven Pipelines。,https://github.com/hpcaitech/ColossalAI/issues/3487
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI下的GPT demo，用户询问该demo是否支持混合并行。这可能是由于用户想要在该demo中使用PP或Megatron等混合并行策略，但目前只发现DP和ZeRO策略被支持，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3484
ColossalAI,这是一个开发者提交的修改请求，涉及到代码风格的问题。由于代码需要进一步反馈、测试和建议，导致了需要撤销这次修改。,https://github.com/hpcaitech/ColossalAI/issues/3481
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalChat模型的权重发布。,https://github.com/hpcaitech/ColossalAI/issues/3480
ColossalAI,这个issue类型是用户提出需求，主要涉及的对象是ColossalAI项目中的两个solver，由于需要在分布式训练场景中获得更好的性能，用户提出需要将这两个solvers整合在一起。,https://github.com/hpcaitech/ColossalAI/issues/3478
ColossalAI,这是一个用户提交需求的issue，主要涉及ColossalAI中的自动并行化功能，由于整合2阶段求解器而引发的问题。,https://github.com/hpcaitech/ColossalAI/issues/3476
ColossalAI,这是一个文档更新的issue，涉及的主要对象是贡献者列表。这个问题单是由于需要更新贡献者列表而创建的。,https://github.com/hpcaitech/ColossalAI/issues/3474
ColossalAI,这个issue是一个功能增强类型的问题，主要涉及ColossalAI中的tensor对象，用户反映ColoTensor目前不支持inplace add操作。,https://github.com/hpcaitech/ColossalAI/issues/3473
ColossalAI,这个issue是一个特性需求，主要针对的对象是ColossalAI中的模型训练功能。由于大型模型训练需求，用户提出了添加断点续训功能的请求。,https://github.com/hpcaitech/ColossalAI/issues/3467
ColossalAI,这是一个用户提出需求的issue，涉及到为ColossalAI添加社区示例字典，可能是由于缺少这一项导致用户寻求帮助或者提出了相关需求。,https://github.com/hpcaitech/ColossalAI/issues/3465
ColossalAI,这个issue类型是需求提出，主要涉及ColossalAI的支持在checkpoint中保存和加载分布式张量的功能。发起此需求可能是为了更好地支持分布式训练和模型保存加载。,https://github.com/hpcaitech/ColossalAI/issues/3464
ColossalAI,这是一个用户提出需求的类型issue，主要涉及到ColossalAI库对Huggingface模型sharded checkpoint的支持问题。由于一些Huggingface模型保存为sharded checkpoints，希望支持加载和保存为HF风格的sharded checkpoint。,https://github.com/hpcaitech/ColossalAI/issues/3463
ColossalAI,这个issue是一个功能需求报告，主要涉及ColossalAI中的checkpoint功能，需求是支持huggingface风格的sharded checkpoint。,https://github.com/hpcaitech/ColossalAI/issues/3461
ColossalAI,这是一个需求类型的Issue，主要对象是ColossalAI项目中的测试代码，用户提出将重复的代码片段重构到`colossalai.testing`模块中的需求。,https://github.com/hpcaitech/ColossalAI/issues/3458
ColossalAI,这是一个关于需求的issue，主要涉及到ColossalAI中Huggingface Peft模型支持的话题。,https://github.com/hpcaitech/ColossalAI/issues/3454
ColossalAI,该问题为技术改进相关的问题，主要对象为ColossalAI的测试模块。这个改进主要是为了重构测试相关的代码，并优化内存使用。,https://github.com/hpcaitech/ColossalAI/issues/3452
ColossalAI,这个issue类型为用户提出需求，主要对象是支持AMD的GPU。由于ColossalAI目前不支持AMD的GPU，用户希望在未来能够支持此功能。,https://github.com/hpcaitech/ColossalAI/issues/3451
ColossalAI,这个issue是一个改进类型的issue，主要涉及对于zero/gemini测试的重新组织。导致此问题的主要原因是为了优化测试相关文件夹的结构。,https://github.com/hpcaitech/ColossalAI/issues/3445
ColossalAI,该issue属于用户提出的需求类型，涉及的主要对象是ColossalAI下的booster plugins，用户提出了需要将插件与重构后的checkpointIO API进行适配的需求。,https://github.com/hpcaitech/ColossalAI/issues/3441
ColossalAI,这是一个需求类型的issue，涉及对zero/gemini相关测试文件夹结构的重新组织，用户寻求更新测试文件夹结构以确保CI能通过的帮助。,https://github.com/hpcaitech/ColossalAI/issues/3437
ColossalAI,这个issue是一个功能更新的PR，主要涉及的对象是ColossalAI中的示例文件。导致这个PR的主要原因是旧版zero模块的导入问题，以及一些示例和教程的导入错误。,https://github.com/hpcaitech/ColossalAI/issues/3431
ColossalAI,这是一个用户提出需求的issue，涉及的主要对象是zero/gemini的示例代码，由于更新了zero/gemini的文件夹结构，需要更新相关示例以确保CI能够通过。,https://github.com/hpcaitech/ColossalAI/issues/3430
ColossalAI,这是一个功能性需求的issue，涉及主要对象是ColossalAI的Checkpoint API。这个问题提出了添加对Huggingface safetensors的支持以及对Checkpoint IO API进行重构的需求。,https://github.com/hpcaitech/ColossalAI/issues/3427
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的checkpointIO模块的重构和添加对huggingface safetensors的支持。原因可能是为了优化模块结构并增强功能。,https://github.com/hpcaitech/ColossalAI/issues/3426
ColossalAI,该issue类型为提出需求，主要涉及的对象是zero/gemini文件夹结构。原因是当前文件夹结构混乱，需要重新组织以确保所有单元测试通过。,https://github.com/hpcaitech/ColossalAI/issues/3422
ColossalAI,这是一个用户提出需求的issue，主要涉及到训练过程中缺少两个数据集的问题。,https://github.com/hpcaitech/ColossalAI/issues/3415
ColossalAI,该issue类型为需求提出，涉及的主要对象是zero/gemini下的folder structure，用户提出了对该folder structure进行重构的建议。,https://github.com/hpcaitech/ColossalAI/issues/3409
ColossalAI,这个issue是一个功能开发提案，主要涉及ColossalAI中的自动并行功能集成新的追踪器，由于新的功能集成问题，需要进行开发工作。,https://github.com/hpcaitech/ColossalAI/issues/3408
ColossalAI,这是一个用户需求类型的issue，主要涉及对象为ColossalAI与AMD显卡的兼容性。由于用户的AMD显卡无法使用人工智能，因此用户提出了希望ColossalAI能实现对AMD显卡的支持。,https://github.com/hpcaitech/ColossalAI/issues/3401
ColossalAI,该issue属于用户提出需求类型，主要涉及ColossalAI和ChatGPT之间的关系。由于用户对ChatGPT镜像的原理和与ColossalAI的区别不清楚，因此提出了这项需求。,https://github.com/hpcaitech/ColossalAI/issues/3390
ColossalAI,这是一个质量改进类的issue，主要涉及代码风格优化，问题是作者要求在创建PR前完成的一些操作未完成，导致需要进行必要的编辑和完善。,https://github.com/hpcaitech/ColossalAI/issues/3378
ColossalAI,这是一个用户提出需求的类型，主要涉及Gemini DDP的梯度累积功能缺失。,https://github.com/hpcaitech/ColossalAI/issues/3375
ColossalAI,这个issue类型是用户提出需求，主要涉及ColossalAI的数据集训练阶段，用户想知道如何获取指定的四个训练数据以便快速复现。由于缺乏现成的训练数据，用户提出了相关问题。,https://github.com/hpcaitech/ColossalAI/issues/3360
ColossalAI,这是一个用户提出需求的issue，主要对象是网站，用户提出希望添加暗黑模式功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/3358
ColossalAI,这是一个用户提出需求的issue，涉及主要对象为ColossalAI下的Coati模型格式。由于涉及模型加载和保存涉及PyTorch和HF格式，需要添加阶段123和推理的说明，避免用户误解和错误使用。,https://github.com/hpcaitech/ColossalAI/issues/3353
ColossalAI,这是一个用户提出需求的issue，涉及主要对象是为Gemini实现一个插件，由于Gemini目前只能通过转换函数来转换训练组件。,https://github.com/hpcaitech/ColossalAI/issues/3351
ColossalAI,这是一个需求类型的issue，主要对象是希望得到训练好的模型以进行推断，由于机器无法运行训练阶段导致无法获取训练好的模型。,https://github.com/hpcaitech/ColossalAI/issues/3348
ColossalAI,这个issue属于更新示例内容的类型，主要涉及ColossalAI中的Roberta，可能由于ColossalAI更新而导致示例内容需要更新。,https://github.com/hpcaitech/ColossalAI/issues/3343
ColossalAI,该issue类型为文档更新，涉及主要对象为ColossalAI的Intel合作新闻，由于未按照标准格式和要求创建PR，需要添加相关内容和标签。,https://github.com/hpcaitech/ColossalAI/issues/3333
ColossalAI,这是一个用户提出需求的类型的Issue，主要涉及ColossalAI下的supervised instruct fine-tuning方法，用户想了解为什么不仅仅在目标标记上计算损失。,https://github.com/hpcaitech/ColossalAI/issues/3330
ColossalAI,这个issue是一个优化代码风格的PR，涉及到ColossalAI下的engine/gradient_handler/__init__.py文件。造成这个问题的原因可能是为了提升代码可读性和维护性。,https://github.com/hpcaitech/ColossalAI/issues/3329
ColossalAI,这是一个用户需求问题，主要涉及如何启动ColossalAI，由于缺乏启动ColossalAI的命令示例，用户提出了如何启动ColossalAI的疑问。,https://github.com/hpcaitech/ColossalAI/issues/3328
ColossalAI,该问题为用户提出需求，询问关于ColossalChat三阶段训练的相关信息。,https://github.com/hpcaitech/ColossalAI/issues/3325
ColossalAI,这是一个用户提出需求类型的issue，主要对象是Colossal AI中的Reward Model Dataset。这个issue的原因是因为在Open AI的Instruct GPT论文中提出了由于一个prompt会有多个比较项，如果比较项没有按prompt分割，经过一次与比较项数据集的训练可能会导致过拟合的问题。,https://github.com/hpcaitech/ColossalAI/issues/3320
ColossalAI,这是一个用户提出需求/请教问题的类型，主要涉及ColossalAI项目中的coati模块中的logits和sequences的问题。根据用户问题的描述，可能是在代码中对logits和sequences进行某种操作或计算时需要进行一定的偏移(shift)。,https://github.com/hpcaitech/ColossalAI/issues/3319
ColossalAI,这个issue是一个功能需求问题，主要对象是PPOTrainer，由于缺少vf_coef参数导致需要添加此功能。,https://github.com/hpcaitech/ColossalAI/issues/3318
ColossalAI,该issue为需求类型，主要对象是PPOTrainer，用户提出了希望添加vf_coef参数以支持PPO算法中的value function loss coefficient的需求。,https://github.com/hpcaitech/ColossalAI/issues/3317
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及ColossalAI的代码格式修复或优化。,https://github.com/hpcaitech/ColossalAI/issues/3312
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI中的gemini/paramhooks/_param_hookmgr.py文件。,https://github.com/hpcaitech/ColossalAI/issues/3311
ColossalAI,这是一个需求提出类型的issue，主要涉及到ColossalAI最新博客提到的开源数据集获取问题。可能是由于数据集链接或说明缺失导致用户无法找到所需的开源数据集。,https://github.com/hpcaitech/ColossalAI/issues/3310
ColossalAI,这个issue是一个需求报告，针对ColossalAI代码质量的polish。,https://github.com/hpcaitech/ColossalAI/issues/3308
ColossalAI,这个issue类型是文档修改请求，主要涉及ColossalAI中的readme链接精细化。由于readme链接需要进行一些优化，可能是为了提高文档的可读性或者用户体验。,https://github.com/hpcaitech/ColossalAI/issues/3306
ColossalAI,这个issue类型是文档更新，涉及主要对象是ColossalChat新闻，由于未遵循标准的PR创建流程和格式，需要对文档内容进行更新。,https://github.com/hpcaitech/ColossalAI/issues/3304
ColossalAI,该issue类型为文档更新，主要涉及到应用程序README的更新，可能是由于需要更新应用程序的相关信息而导致。,https://github.com/hpcaitech/ColossalAI/issues/3301
ColossalAI,这个issue是一个用户提出的需求，主要涉及到对话模型的训练，原因是为了优化对话模型的训练效果。,https://github.com/hpcaitech/ColossalAI/issues/3300
ColossalAI,这个issue是一个需求提出类型的问题，主要涉及项目ColossalAI的文档添加工作。由于缺少规范的PR创建步骤和标题格式，导致需要对文档进行更新，以提高PR的质量和可跟踪性。,https://github.com/hpcaitech/ColossalAI/issues/3297
ColossalAI,这是一则关于更新Readme的issue，提供了创建PR前的必要检查列表和请求审核前的检查列表。,https://github.com/hpcaitech/ColossalAI/issues/3296
ColossalAI,这是一个需求提出的issue，主要涉及的对象是ColossalAI下的inference服务。由于没有profanity check，用户可能会输入或得到不当内容。,https://github.com/hpcaitech/ColossalAI/issues/3295
ColossalAI,该issue为用户提出需求，需添加一个repetition_penalty用于推理，这可以提升生成的质量。,https://github.com/hpcaitech/ColossalAI/issues/3294
ColossalAI,这个issue类型是需求提出，主要对象是代码的添加功能限制，由于需求在代码中添加限制，用户提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3293
ColossalAI,这是一个用户提出需求的issue，主要涉及为ColossalAI添加数据集引用。用户请求添加数据集引用可能是为了提高文档的准确性和可信度。,https://github.com/hpcaitech/ColossalAI/issues/3292
ColossalAI,这是一个用户提出需求的 issue，主要涉及ColossalAI 下的 ChatGPT 示例在 Ray 上的运行，用户想要展示如何在 Ray 上运行 ChatGPT 示例。,https://github.com/hpcaitech/ColossalAI/issues/3291
ColossalAI,这是一个请求贡献和提交PR的问题，主要涉及图像转移功能。由于缺少一些标准的检查步骤和文档要求，导致需要对代码进行审核和相关文档的完善。,https://github.com/hpcaitech/ColossalAI/issues/3288
ColossalAI,这个issue类型是关于代码风格优化的，涉及主要对象是ColossalAI中的initializer_data.py文件，由于代码风格不符合规范导致需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/3287
ColossalAI,这是一个用户提出需求的 issue，主要涉及ColossalAI中的添加示例操作。由于缺少示例，用户请求添加示例以提供更好的参考。,https://github.com/hpcaitech/ColossalAI/issues/3286
ColossalAI,这个issue属于一般性需求提议，主要涉及ColossalAI的ChatGPT模块的移除，可能是为了引入新的Coati模块。,https://github.com/hpcaitech/ColossalAI/issues/3284
ColossalAI,这个issue类型是新功能添加的请求，主要涉及Coati模型的支持3stage RLHF训练，并发行Coati模型。可能由于增加新功能而产生的需求和改进。,https://github.com/hpcaitech/ColossalAI/issues/3283
ColossalAI,这个issue是一个贡献者提交更新sft_dataset.py文件的PR，主要涉及文档和功能修订。问题可能由代码缺陷或功能改进引起。,https://github.com/hpcaitech/ColossalAI/issues/3282
ColossalAI,这是一个需求整合的issue，主要涉及ColossalAI的automatic parallelism module，由于集成新版本的analyzer，需要对该模块进行适配和更新。,https://github.com/hpcaitech/ColossalAI/issues/3278
ColossalAI,这个issue类型是代码风格优化，涉及主要对象为ColossalAI库的梯度累积模块。因为缺乏代码规范导致风格不统一，需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/3277
ColossalAI,这个issue类型是代码风格优化，涉及的主要对象是ColossalAI中的parallel_context.py文件。由于代码风格不符合规范，需要优化以提高代码质量和可读性。,https://github.com/hpcaitech/ColossalAI/issues/3276
ColossalAI,这个issue是一个代码风格优化类型的问题，主要涉及ColossalAI下的addmm.py文件的代码风格问题，用户提交了相关的风格优化请求。原因可能是为了统一代码风格或提高代码可读性。,https://github.com/hpcaitech/ColossalAI/issues/3274
ColossalAI,这个issue是一个需求报告，主要涉及ColossalAI项目中的代码风格调整，由于缺乏规范性的代码风格，可能会影响代码的可读性和维护性。,https://github.com/hpcaitech/ColossalAI/issues/3273
ColossalAI,该issue是关于代码风格优化，主要涉及ColossalAI下的amp模块的__init__.py文件。这个issue是优化代码风格，希望通过规范化代码格式提升代码质量。,https://github.com/hpcaitech/ColossalAI/issues/3272
ColossalAI,这个issue属于文档改进类型，涉及ColossalAI的AutoParallel readme文档，用户提出了文档细节的完善与规范化的需求。,https://github.com/hpcaitech/ColossalAI/issues/3270
ColossalAI,这个issue类型是代码优化，主要对象是ColossalAI中的tensor_placement_policy.py文件；由于代码风格不规范导致需要进行代码风格的修改。,https://github.com/hpcaitech/ColossalAI/issues/3265
ColossalAI,这个issue类型是code style优化，主要涉及colossalai/fx/passes/split_module.py文件，由于代码风格不符合规范导致的问题。,https://github.com/hpcaitech/ColossalAI/issues/3263
ColossalAI,这是一个关于代码风格优化的issue，涉及主要对象为贡献者的PR流程。问题可能由于缺乏一致的代码风格导致PR质量不佳而提出。,https://github.com/hpcaitech/ColossalAI/issues/3262
ColossalAI,这个issue类型是代码风格优化，主要对象是ColossalAI下的split_module.py文件。由于代码风格不符合规范，需要进行整理使其更易读。,https://github.com/hpcaitech/ColossalAI/issues/3255
ColossalAI,该issue类型为功能改进，涉及ColossalAI中的meta注册兼容性问题，主要是为了解决Pytorch 1.13版本添加的meta核心功能，避免重写Pytorch中的核心功能。,https://github.com/hpcaitech/ColossalAI/issues/3253
ColossalAI,这个issue属于需求提出类型，主要涉及的对象是ColossalAI中的chatgpt模块，用户提出了新增ColossalAI策略的精度选项的需求。,https://github.com/hpcaitech/ColossalAI/issues/3233
ColossalAI,这是一个功能需求的issue，涉及的主要对象是Booster模块。原因是要增加ResNet + CIFAR的训练和评估示例。,https://github.com/hpcaitech/ColossalAI/issues/3232
ColossalAI,这是一个需求类型的issue，涉及的主要对象是Booster模块。由于需要测试Booster API并提供示例，引起了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/3231
ColossalAI,这个issue属于文档更新类，针对的主要对象是ColossalAI的README.md文件内容，由于paper链接需要更新而产生。,https://github.com/hpcaitech/ColossalAI/issues/3229
ColossalAI,这个issue是一个功能增强请求，主要对象是ColoTensor类，用户提出了需要更新`get_dp_world_size`方法以获取张量的分布式数据并行（DP）世界大小，由于当前方法不可用，导致无法获取DP世界大小信息。,https://github.com/hpcaitech/ColossalAI/issues/3227
ColossalAI,这个issue是一个功能需求，涉及添加RoBERTa预训练模型到RLHF stage 2和3，由于缺少部分关联标签导致了相关问题。,https://github.com/hpcaitech/ColossalAI/issues/3223
ColossalAI,该issue类型为功能需求，主要涉及的对象是支持PPO-PTX在ChatGPT阶段3中的应用。这个问题是由于需要增加PTX损失和调整训练过程而提出的。,https://github.com/hpcaitech/ColossalAI/issues/3220
ColossalAI,这个issue类型是一个功能优化建议，主要涉及ColossalAI的chatgpt模块，用户提出通过将初始模型权重转移到CPU来降低内存成本。,https://github.com/hpcaitech/ColossalAI/issues/3219
ColossalAI,这个issue是一个需求提出的类型，主要涉及到统一数据集操作，可能是为了解决不同数据集的统一操作而提出。,https://github.com/hpcaitech/ColossalAI/issues/3218
ColossalAI,此issue类型为需求提交，主要涉及ChatGPT模块的支持训练指导；由于缺乏训练指导导致用户提出需求，以解决训练方面的问题。,https://github.com/hpcaitech/ColossalAI/issues/3216
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的SFT dataset。由于用户想要获取该dataset和相关处理指南，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/3208
ColossalAI,这是一个需求报告，涉及主要对象为ColossalAI中的chatgpt模块。由于需要在RLHF Stage 2 & 3中添加RoBERTa，导致需要在models文件夹下添加roberta文件夹，并对train_dummy.py和train_reward_model.py进行修改。,https://github.com/hpcaitech/ColossalAI/issues/3207
ColossalAI,这个issue是一个用户需求报告，主要对象是ChatGPT模块，用户提交了关于为RLHF Stage 2 & 3添加RoBERTa的请求。,https://github.com/hpcaitech/ColossalAI/issues/3206
ColossalAI,这是一个特性请求（Feature Request）issue，主要涉及到支持BF16优化器用于LLM训练。由于BF16优化器的必要性已经由BLOOM、OPT和MegatronTuring验证，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/3203
ColossalAI,这是一个用户提出需求的issue，主要对象是为Deberta添加奖励模型代码。可能由于缺少奖励模型代码而导致问题或需求得到满足。,https://github.com/hpcaitech/ColossalAI/issues/3199
ColossalAI,"这个issue是一个功能需求，涉及的主要对象是ColossalAI中的PPO训练模块。由于要实现""Detached"" PPO训练方式，即体验生成器和训练器分离到不同节点进行异步训练，需要添加相关类和结构，实现Ray框架支持，并处理相关已知问题。",https://github.com/hpcaitech/ColossalAI/issues/3195
ColossalAI,这个issue类型是代码质量优化，涉及的主要对象是稳定扩散示例。,https://github.com/hpcaitech/ColossalAI/issues/3194
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI中的reward model相关训练脚本和数据，由于更新内容不符合实际情况且缺少评估数据和脚本，用户希望得到正确的引用以及训练评估所需的资源。,https://github.com/hpcaitech/ColossalAI/issues/3189
ColossalAI,这是一个特性改进的issue，主要涉及对象是 `ZeroContextConfig` 类。原因是为了使用 Python 标准库的 `dataclass` 装饰器来重构该类，以简化代码、提高可读性和减少错误可能性。,https://github.com/hpcaitech/ColossalAI/issues/3186
ColossalAI,这个issue类型是一个特性需求，主要对象是为ColossalAI中的ChatGPT模块添加监督学习微调代码。由于缺乏监督学习微调代码，导致用户在InstructGPT（步骤1）中无法进行相应操作。,https://github.com/hpcaitech/ColossalAI/issues/3183
ColossalAI,这是一个功能需求提议，主要涉及ChatGPT模型的代码改进，由于当前项目中的代码不能很好地适应ChatGPT，因此需要添加模型微调的代码。,https://github.com/hpcaitech/ColossalAI/issues/3182
ColossalAI,该issue属于功能需求类型，主要对象是ColossalAI下的Torch DDP插件实现。由于需要对训练组件进行加速/改进转换，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/3179
ColossalAI,这个issue类型是改进提案，主要对象是ColossalAI中的flash attention模块。由于之前的flash attention API混乱难用，所以提出了改进的新API。,https://github.com/hpcaitech/ColossalAI/issues/3178
ColossalAI,这个issue是一个文档更新类型的问题，主要涉及项目结构的调整，由于项目结构混乱导致需要移动文档并更新路径。,https://github.com/hpcaitech/ColossalAI/issues/3174
ColossalAI,这是一个提出改迷惑性变量命名建议的issue，主要对象涉及ZeRO优化器中的变量命名，由于命名不够清晰导致初学者容易混淆。,https://github.com/hpcaitech/ColossalAI/issues/3173
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的Tensor并行作用范围，问题是由于不清晰列出Tensor并行的生效范围。,https://github.com/hpcaitech/ColossalAI/issues/3165
ColossalAI,该issue是一个需求提出类型的问题单，主要涉及集成不同版本的tracer并测试其在不同模型zoos上的功能。,https://github.com/hpcaitech/ColossalAI/issues/3160
ColossalAI,该issue类型为功能增强，主要涉及的对象是ColossalAI的Booster项目，由于需要实现加速器组件而创建。,https://github.com/hpcaitech/ColossalAI/issues/3159
ColossalAI,这个issue是关于代码改进的PR，涉及到ColossalAI项目中的experimental tracer和hf models，由于需要重构tracer并与hf models适配，所以产生了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/3157
ColossalAI,这个issue类型是功能增强，主要涉及到ColossalAI自动并行功能的添加，用户寻求性能测试和求解器测试。由于需求优化和功能增强，用户提出了相关需求。,https://github.com/hpcaitech/ColossalAI/issues/3154
ColossalAI,这个issue是关于用户需求的，主要涉及ColossalAI的[dlazyinit]功能在分布式环境下的正确性验证和跟踪进展。,https://github.com/hpcaitech/ColossalAI/issues/3149
ColossalAI,这是一个用户提出需求的issue，主要涉及Lazy Tensor和DTensor的结合。由于想实现延迟张量的分布式特性，需要结合DTensor来进行支持。,https://github.com/hpcaitech/ColossalAI/issues/3148
ColossalAI,这是一个用户提出需求的issue，涉及主要对象是添加torchrec模型到测试模型库。由于缺少对应的测试集，用户希望为主流模型库添加测试集。,https://github.com/hpcaitech/ColossalAI/issues/3139
ColossalAI,这个issue是一个功能添加（feature add）类型的问题，主要涉及的对象是测试模型。由于需要添加torchaudio模型到测试模型库，因此用户创建了该issue。,https://github.com/hpcaitech/ColossalAI/issues/3138
ColossalAI,这个issue是一个功能添加类型的问题，主要涉及在ColossalAI的model zoo中添加diffuser models。产生此问题的原因是为了增加模型库中的功能。,https://github.com/hpcaitech/ColossalAI/issues/3136
ColossalAI,该issue属于功能需求类型，主要涉及ColossalAI项目中关于增加正确性验证的工作。由于当前实现并未在许多模型上进行测试，因此需要进行大规模的正确性验证。,https://github.com/hpcaitech/ColossalAI/issues/3134
ColossalAI,这个issue属于功能更新，主要涉及到reward model training process的更新。用户提出了对奖励模型训练过程的更新需求。,https://github.com/hpcaitech/ColossalAI/issues/3133
ColossalAI,这个issue类型是贡献代码需求，涉及的主要对象是为ColossalAI的测试模型库添加torchvision模型。由于测试模型集合中缺少torchvision模型，因此此问题描述了将torchvision模型添加到测试模型库的需求。,https://github.com/hpcaitech/ColossalAI/issues/3132
ColossalAI,这个issue类型是实现特性需求，主要涉及的对象是懒初始化（lazyinit）相关的张量和初始化上下文（init ctx），由于需要实现懒初始化的单进程版本，而目前代码已经在本地测试通过，未来会在下一个PR中添加更多测试。,https://github.com/hpcaitech/ColossalAI/issues/3131
ColossalAI,这是一个功能需求，涉及ColossalAI中的LazyInit功能。由于训练大模型时必须延迟模型初始化，需要实现LazyTensor的单进程版本。,https://github.com/hpcaitech/ColossalAI/issues/3130
ColossalAI,这个issue是一个功能添加类型的问题，涉及到ColossalAI项目中的模型测试和文档规范化，由于需要更新tracer测试以使用最新的kit，来自Timm模型的兼容性存在问题。,https://github.com/hpcaitech/ColossalAI/issues/3129
ColossalAI,该issue属于用户提出需求类型，主要对象是测试ChatGPT的CI系统。由于在创建PR之前未完成必要的检查和标准流程，造成了需要在此issue中跟踪和指导整个PR创建的过程。,https://github.com/hpcaitech/ColossalAI/issues/3122
ColossalAI,这个issue属于需求提出类型，主要涉及到ColossalAI中的autochunk模块，由于需要支持完整的benchmark，用户提出了支持alphafold、transformer、vit、unet等模型，并使用cpu追踪模型的需求。,https://github.com/hpcaitech/ColossalAI/issues/3121
ColossalAI,这是一个功能更新需求，主要涉及到ColossalAI库中的benchmark功能，用户希望更新benchmark以支持vit和更大的模型。,https://github.com/hpcaitech/ColossalAI/issues/3120
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中如何在chatGPT中实现监督微调的问题，用户希望了解如何在stage1中实现监督微调。,https://github.com/hpcaitech/ColossalAI/issues/3119
ColossalAI,这个issue类型属于功能需求，主要涉及的对象是submodule references。原因可能是为了自动化更新submodule commits，提高开发效率。,https://github.com/hpcaitech/ColossalAI/issues/3105
ColossalAI,这个问题类型是功能需求提出，主要对象是ColossalAI库中的tensor parallel microbenchmark工具。原因是当前的工具在OOM时无法继续进行tensor parallel策略的微基准测试。,https://github.com/hpcaitech/ColossalAI/issues/3103
ColossalAI,这个issue是一个功能实现的PR，涉及主要对象为ColossalAI的API。导致提出这个PR的原因可能是为了实现一个环境表格功能。,https://github.com/hpcaitech/ColossalAI/issues/3101
ColossalAI,这个issue是一个技术文档更新的类型，涉及主要对象是TransformerEngine，由于缺乏标准格式的标题、PR创建前的检查事项不全，导致需要对文档内容进行更新。,https://github.com/hpcaitech/ColossalAI/issues/3098
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的reward model training。由于某些数据集和自定义特殊标记不被支持，导致特殊标记缺乏通用性，用户在询问是否有计划支持这些数据集以及是否可以提供自定义特殊标记适应不同的分词器。,https://github.com/hpcaitech/ColossalAI/issues/3097
ColossalAI,这个issue属于用户提出需求的类型，主要涉及ColossalAI中load_checkpoint方法存在的问题，由于gather操作先填充GPU内存导致OOM问题。,https://github.com/hpcaitech/ColossalAI/issues/3095
ColossalAI,这是一个发布更新的issue，涉及到ColossalAI的版本升级。由于版本升级造成的症状和用户提出的需求可能包含新功能、修复bug或提升性能。,https://github.com/hpcaitech/ColossalAI/issues/3094
ColossalAI,这个issue是关于更新CI的问题，属于功能需求，主要对象是ColossalAI项目的持续集成系统。由于CI配置需要更新，导致CI工作不正常。,https://github.com/hpcaitech/ColossalAI/issues/3087
ColossalAI,这个issue类型是新增功能需求，涉及的主要对象是ColossalAI的chatgpt模块，用户提出了添加操作屏蔽标志选项的需求。,https://github.com/hpcaitech/ColossalAI/issues/3086
ColossalAI,这个issue属于功能增强类型，涉及主要对象为ColossalAI中的autochunk支持vit，可能是因为原有功能不支持vit，需要新增支持和修复bug。,https://github.com/hpcaitech/ColossalAI/issues/3084
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的autochunk功能支持vit，用户希望该功能能够支持使用vit模型。,https://github.com/hpcaitech/ColossalAI/issues/3083
ColossalAI,这个issue属于用户提出需求类型，主要对象是为ColossalAI增加使用TransformerEngine API在MNIST数据集上进行简单分类任务的FP8示例脚本。,https://github.com/hpcaitech/ColossalAI/issues/3080
ColossalAI,该issue类型为功能需求，主要涉及的对象是实现LayoutConverter功能。由于需要进行布局转换以优化张量操作效率，但还没有相应功能实现，因此用户提交了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/3067
ColossalAI,这是一个功能需求类型的issue，主要涉及自动化同步子模块提交信息，用户提出希望通过自动PR来更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/3062
ColossalAI,这个issue类型是用户提出需求，主要涉及的对象是DTensor。由于需要实现LayoutConverter功能，用户需要帮助来对tensor进行不同布局之间的转换和估算转换成本。,https://github.com/hpcaitech/ColossalAI/issues/3055
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是`Precision`类的实现，用户希望实现一个管理训练过程中(mixed) precision的抽象类。,https://github.com/hpcaitech/ColossalAI/issues/3054
ColossalAI,该issue是一个用户提出的需求，主要涉及实现针对本地PyTorch模型的CheckpointIO功能。,https://github.com/hpcaitech/ColossalAI/issues/3053
ColossalAI,该issue为提出需求类型，涉及的主要对象为ColossalAI中的Accelerator模块实现。由于未来可能对更多与PyTorch兼容的硬件进行扩展，需要实现一个硬件设备抽象化的模块来移动模型到目标设备。,https://github.com/hpcaitech/ColossalAI/issues/3052
ColossalAI,该issue类型为功能需求，涉及主要对象为ColossalAI中的booster模块。,https://github.com/hpcaitech/ColossalAI/issues/3050
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI要添加用于测试的主流模型库。,https://github.com/hpcaitech/ColossalAI/issues/3049
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是ColossalAI下的ChatGPT模型中的评论者模块。这个问题的提出可能是因为用户认为评论者应该为每个时间步生成输出，而不是整个序列平均输出。,https://github.com/hpcaitech/ColossalAI/issues/3047
ColossalAI,这是一个用户提出需求的类型的问题，主要涉及的对象是代码中的参数'max_length=96'。用户提出这个问题是因为想了解该参数代表什么意思，以及是否能够将其修改为256或其他数值。,https://github.com/hpcaitech/ColossalAI/issues/3045
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI库下的CommSpec功能。由于CommSpec中包含了一些不必要的属性，需要对其进行重构。,https://github.com/hpcaitech/ColossalAI/issues/3035
ColossalAI,这是一个关于提出需求的issue，主要涉及到ColossalAI中的DTensor模块。原issue作者提到了在AutoParallel模块中的CommSpec存在一些不必要的属性，提出了重构CommSpec的建议。,https://github.com/hpcaitech/ColossalAI/issues/3034
ColossalAI,这是一个需求提出类型的 issue，主要涉及 ColossalAI 中的 chatgpt 应用 API。用户希望改变 chatgpt.nn 到 chatgpt.models，以使 API 更加用户友好。,https://github.com/hpcaitech/ColossalAI/issues/3033
ColossalAI,这个issue是一个需求类型，主要涉及ColossalAI中chatgpt API的更改。这个需求来源于提高用户友好性，更方便用户查找模型。,https://github.com/hpcaitech/ColossalAI/issues/3032
ColossalAI,这个issue类型为功能增强，主要对象为ColossalAI的文档测试。这个问题由于缺乏对conda包安装的支持导致。,https://github.com/hpcaitech/ColossalAI/issues/3028
ColossalAI,这是一个用户提出的需求，主要对象是ColossalAI的文档测试，由于无法通过pip安装一些库，用户要求支持conda依赖。,https://github.com/hpcaitech/ColossalAI/issues/3027
ColossalAI,这个issue类型是文档更新，涉及主要对象是ChatGPT，由于文档需要更新而创建的问题。,https://github.com/hpcaitech/ColossalAI/issues/3025
ColossalAI,这个issue是一个文档修改的请求，主要涉及到操作系统要求，通过修改说明文件和代码表明目前不支持Windows系统。,https://github.com/hpcaitech/ColossalAI/issues/3019
ColossalAI,该issue是一个文档更新类型的问题，主要对象是NVME offload，由于需要更新NVME offload文档，添加两个简单示例和API参考，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/3014
ColossalAI,这是一个功能需求报告，涉及到ColossalAI中ChatGPT项目的第三阶段训练脚本中加载奖励模型的问题。由于当前代码未正确加载第二阶段创建的奖励模型，用户提出了需要修改train_prompts.py文件以正确加载奖励模型的请求。,https://github.com/hpcaitech/ColossalAI/issues/3011
ColossalAI,这个issue类型是技术改进提案，涉及的主要对象是docker镜像，由于缺少对opencontainers image-spec的添加，导致docker镜像质量不佳。,https://github.com/hpcaitech/ColossalAI/issues/3006
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及Dockerfile中添加图像规范，作者建议添加注释作为元数据。,https://github.com/hpcaitech/ColossalAI/issues/3005
ColossalAI,该issue类型为文档更新提案，主要涉及的对象是ColossalAI的文档。由于缺乏带有清晰标准的提交步骤和格式规范，导致需要添加ISC教程的PR未符合标准格式，需要进一步完善和规范化。,https://github.com/hpcaitech/ColossalAI/issues/2997
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及ColossalAI中的ShardingSpec的重构。由于之前的ShardingSpec包含了与张量分片无关的信息，因此用户建议只保留与分片和张量相关的信息，同时将其他信息放入Layout类中描述分布式张量的布局。,https://github.com/hpcaitech/ColossalAI/issues/2989
ColossalAI,这是一个功能需求：允许分片初始化并显示警告，主要涉及到ColossalAI中的chatgpt模块。用户在启用shard初始化时会遇到无法兼容`model.from_pretrained()`的问题，因此提出了在启用分片初始化时打印警告的需求。,https://github.com/hpcaitech/ColossalAI/issues/2986
ColossalAI,这是一个用户提出需求的issue，主要涉及训练策略，在单机单显卡下无法运行，用户询问是否有相关的文档说明。,https://github.com/hpcaitech/ColossalAI/issues/2985
ColossalAI,该issue类型为文档更新，主要涉及文档更新和代码审查流程。由于未按照标准PR创建流程，可能导致代码审查和追踪问题。,https://github.com/hpcaitech/ColossalAI/issues/2983
ColossalAI,该issue类型为功能需求，涉及的主要对象是项目中的子模块引用，用户可能希望自动化提交以更新子模块的提交记录。,https://github.com/hpcaitech/ColossalAI/issues/2982
ColossalAI,这是一个用户提出需求的issue，主要涉及了ColossalAI中的chatGPT模型的应用文档。用户希望了解如何进行监督微调（SFT），因为在chatGPT示例中只看到了阶段2和3的相关信息，但希望获得使用示例数据集进行微调的指导。,https://github.com/hpcaitech/ColossalAI/issues/2980
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及项目中的配置文件组织方式，提出了将多个配置文件转移到一个`pyproject.toml`文件中的建议。,https://github.com/hpcaitech/ColossalAI/issues/2976
ColossalAI,这是一个提出需求的issue，主要涉及ColossalAI中引擎核心API的重构。由于当前初始化流程难以维护、引擎使用困难且缺乏灵活性，导致需要对引擎进行重构提升。,https://github.com/hpcaitech/ColossalAI/issues/2975
ColossalAI,这是一个用户提出需求的issue，主要涉及Chatgpt making experience的DP支持，由于缺乏详尽的测试，未添加函数/方法的文档字符串。,https://github.com/hpcaitech/ColossalAI/issues/2971
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的chatgpt模型的分布式数据并行训练和推理中的一些设计细节。,https://github.com/hpcaitech/ColossalAI/issues/2970
ColossalAI,这是一个功能提议类型的issue，主要涉及ColossalAI中的DTensor数据结构。,https://github.com/hpcaitech/ColossalAI/issues/2957
ColossalAI,这个issue属于功能需求类型，主要涉及代码库中子模块引用的自动同步更新需求。,https://github.com/hpcaitech/ColossalAI/issues/2951
ColossalAI,这个issue属于用户提出需求类，主要对象是API文档，由于API文档从官网页面移除，用户提出了为什么API文档被移除的问题。,https://github.com/hpcaitech/ColossalAI/issues/2947
ColossalAI,这个issue是关于需求的，主要对象是为ColossalAI的chatgpt模块添加推理示例，因为缺乏推理示例导致用户提出需要在Chatgpt/examples中添加推理演示。,https://github.com/hpcaitech/ColossalAI/issues/2944
ColossalAI,这是一个文档更新类型的issue，涉及ColossalAI的文档系统，由于缺少README文件导致需要更新文档说明。,https://github.com/hpcaitech/ColossalAI/issues/2935
ColossalAI,这个issue属于文档需求类型，涉及到ColossalAI的文档系统。由于新文档系统刚建立，需要添加README来解释文档的构建、更新和测试过程。,https://github.com/hpcaitech/ColossalAI/issues/2934
ColossalAI,该issue属于文档更新类型，主要涉及环境变量范围的修改。原因可能是为了提高文档的清晰度和完整性。,https://github.com/hpcaitech/ColossalAI/issues/2933
ColossalAI,此issue类型为文档更新，涉及对象为ColossalAI项目的文档，由于不再需要readthedocs相关文件而引发。,https://github.com/hpcaitech/ColossalAI/issues/2932
ColossalAI,这是一个关于文档更新的issue，属于需求变更类型，主要涉及文档系统的优化。由于采用新的文档系统，不再需要原先的Readthedocs和sphinx，因此需要移除相关文件。,https://github.com/hpcaitech/ColossalAI/issues/2931
ColossalAI,这个issue是一个需求提出类型的issue，主要涉及的对象是ColossalAI的文档自动测试工作流程。由于缺乏文档测试工作流程，导致可能存在文档不及时更新的问题。,https://github.com/hpcaitech/ColossalAI/issues/2929
ColossalAI,这个issue类型是功能需求，主要涉及的对象是项目中的文档。由于文档不定期更新，导致用户在尝试教程时遇到困难，因此提出了需要自动化测试文档以确保其有效性的需求。,https://github.com/hpcaitech/ColossalAI/issues/2928
ColossalAI,该问题单为功能需求类型，主要涉及Git仓库中子模块引用更新的自动化处理。此需求由于项目中子模块引用与最新提交间的不同步而导致。,https://github.com/hpcaitech/ColossalAI/issues/2927
ColossalAI,这是一则关于文档更新的issue，主要涉及ColossalAI中GPT安装步骤的更新。原因是为了确保PR的规范性和完整性。,https://github.com/hpcaitech/ColossalAI/issues/2922
ColossalAI,这是一个文档需求类型的issue，主要涉及ColossalAI项目中多节点多GPU示例文档不清晰的问题，导致用户无法理解如何在多节点多GPU环境下进行操作。,https://github.com/hpcaitech/ColossalAI/issues/2921
ColossalAI,该issue属于一个文档更新类型的任务，主要涉及ColossalAI的安装和相关提示的调整。导致此次更新的原因可能是为了清晰地指导用户正确安装ColossalAI并提供更多相关提示。,https://github.com/hpcaitech/ColossalAI/issues/2914
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的自动并行求解器在LLM模型下解决时间不可接受的问题。,https://github.com/hpcaitech/ColossalAI/issues/2913
ColossalAI,这个issue属于功能改进类型，主要涉及到自动并行化过程中的优化策略构建和解决空间剪枝问题。造成这个改进的原因是为了减少求解时间。,https://github.com/hpcaitech/ColossalAI/issues/2912
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI库缺乏完善的文档和教程，用户希望获得更全面系统的解释和使用示例。,https://github.com/hpcaitech/ColossalAI/issues/2904
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的文档不足的问题。用户反映文档太少，无法有效使用，希望能够获得更详细和系统的说明。,https://github.com/hpcaitech/ColossalAI/issues/2903
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI库的文档设置。用户在尝试运行ColossalAI库的快速演示时遇到了问题，主要是缺乏清晰的指导和解释，导致无法顺利进行推断。,https://github.com/hpcaitech/ColossalAI/issues/2898
ColossalAI,这个issue是对ColossalAI项目中的工作流进行改进的PR，不是bug报告。该PR主要涉及pre-commit工作流程的重构。,https://github.com/hpcaitech/ColossalAI/issues/2895
ColossalAI,这是一个用户提出的需求。该问题单涉及的主要对象是ColossalAI项目中的代码提交流程。由于开发人员不熟悉precommit导致许多开发人员没有使用代码样式一致性，因此用户建议使用Post-commit代替precommit来降低外部贡献的障碍。,https://github.com/hpcaitech/ColossalAI/issues/2894
ColossalAI,该issue为用户提出需求，询问如何从预训练开始训练一个文本生成的GPT2或GPT3模型，希望能够提供一个文档教程。,https://github.com/hpcaitech/ColossalAI/issues/2893
ColossalAI,这个issue类型属于用户提出需求类型，主要涉及的对象是关于ChatGPT训练文档。由于示例使用了awesomechatgptprompts，用户建议提供如何准备自定义数据集的示例。,https://github.com/hpcaitech/ColossalAI/issues/2891
ColossalAI,这是一个用户提出需求的类型，主要对象是zero+bf16支持。用户提出这个问题可能是因为希望了解ColossalAI是否会在近期支持zero+bf16这一特性。,https://github.com/hpcaitech/ColossalAI/issues/2887
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI库中LowLevelZeroOptimizer类中混淆的变量命名问题。原因是当前变量名暗示了forced_dtype将是FP16，但如果没有提供forced_dtype（默认值），则优化器将作为常规的Zero Optimizer进行操作，而不是使用混合精度。,https://github.com/hpcaitech/ColossalAI/issues/2881
ColossalAI,这是一个功能增强的issue，主要涉及ColossalAI下的chatgpt模块，用于添加一个保存checkpoint的回调函数。,https://github.com/hpcaitech/ColossalAI/issues/2880
ColossalAI,此issue类型为用户提出需求，主要涉及支持使用GPT2和OPT模型进行rm训练。用户提出这个需求是因为目前系统不支持这一功能。,https://github.com/hpcaitech/ColossalAI/issues/2876
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是chatgpt模型的推断或预测操作。用户询问如何在没有相关脚本或依赖项的情况下对经过scripts训练的chatgpt模型进行推断。,https://github.com/hpcaitech/ColossalAI/issues/2868
ColossalAI,这个issue属于功能优化类型，主要涉及到减少autoparallel intraop solver在LLMs中求解时间过长的问题，通过找到最大重复块来解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/2854
ColossalAI,这是一个用户提出需求的 issue，主要涉及ColossalAI中 auto-parallel intra-op solver 的优化。通过寻找重复块和使用别名集合来降低大型语言模型的求解时间。,https://github.com/hpcaitech/ColossalAI/issues/2853
ColossalAI,这是一个社区贡献者提出的功能需求相关的Issue，主要涉及ColossalAI的静态图分析器，原因可能是为了增加系统的分析功能性。,https://github.com/hpcaitech/ColossalAI/issues/2852
ColossalAI,这是一个用户提交需求的issue，主要涉及ColossalAI下的chatgpt模块，用户提出支持在示例中保存ckpt的功能。由于当前无法保存optim_state和weight，用户请求添加此功能。,https://github.com/hpcaitech/ColossalAI/issues/2846
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI下的ModelZoo。,https://github.com/hpcaitech/ColossalAI/issues/2844
ColossalAI,这是一个用户提出需求的类型，主要涉及到ColossalAI中的ZeroInitContext，ColoInitContext和colossalai.initialize。因为用户需要对这三者之间的区别进行澄清，可能是由于现有文档不够清晰或者存在歧义所导致的。,https://github.com/hpcaitech/ColossalAI/issues/2840
ColossalAI,这是一个用户提出需求的提案类型的GitHub issue，主要对象是关于ColossalAI在主流图像和文本任务上的复现结果。用户提出该提案是为了增加社区对ColossalAI的信任度，希望通过在单GPU和多节点GPU上展示多种分布式模式下的复现结果。,https://github.com/hpcaitech/ColossalAI/issues/2831
ColossalAI,这是一个需求补充的类型的issue，该问题单涉及的主要对象是ColossalAI的依赖环境版本范围的清晰性，由于移除了预编译下载页面，用户难以确定ColossalAI需要哪些版本的依赖环境，需要明确支持的范围，例如Python、PyTorch、CUDA等。,https://github.com/hpcaitech/ColossalAI/issues/2830
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI项目中的示例代码库。,https://github.com/hpcaitech/ColossalAI/issues/2828
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI框架的优化问题，用户询问为何没有对Movidius compute stick或Pine64 Quartz64神经网络加速引擎的支持，并探讨是否可以在没有NvidiaCUDA的情况下实现相应优化。,https://github.com/hpcaitech/ColossalAI/issues/2827
ColossalAI,该issue属于用户需求类型，主要涉及在ColossalAI中添加BERT和ALBERT功能。由于作者未完全添加测试和文档注释，可能导致功能不稳定或者难以理解。,https://github.com/hpcaitech/ColossalAI/issues/2824
ColossalAI,这个issue是一个功能更新的请求，主要涉及到ColossalAI中一些操作符的元信息问题，由于这些操作符不会被SPMD求解器处理，用户希望修复这些问题以便为集成求解器在GPT2上的元信息传播铺平道路。,https://github.com/hpcaitech/ColossalAI/issues/2823
ColossalAI,这是一个用户提出需求的issue， 主要涉及 ColossalAI 框架对 LoRA 支持的优化。,https://github.com/hpcaitech/ColossalAI/issues/2821
ColossalAI,这个issue是用户提交的贡献文档的请求，主要涉及ColossalAI自动并行翻译文档问题，用户提出希望通过自动并行来改进中文文档，从而提高文档质量。,https://github.com/hpcaitech/ColossalAI/issues/2820
ColossalAI,这个issue是一个功能需求， 主要涉及GeminiDDP模型训练过程中保存模型检查点时的数据类型问题。原因是GeminiDDP模型只能以FP32格式输出模型权重，导致用户想要以FP16格式保存模型检查点时遇到困难。,https://github.com/hpcaitech/ColossalAI/issues/2819
ColossalAI,这个issue属于功能需求类型，主要对象是`operator.le()`函数的元信息。它是因为需要更新或修复`operator.le()`函数的元信息而被提出。,https://github.com/hpcaitech/ColossalAI/issues/2815
ColossalAI,这是一个用户提出需求类型的issue，主要涉及的对象是`torch.finfo()`函数的元信息。由于用户需要对`torch.finfo()`函数的元信息进行补丁修改，可能是为了更好地适应特定的使用场景或需求。,https://github.com/hpcaitech/ColossalAI/issues/2814
ColossalAI,这个issue类型为文档更新，主要对象是优化服务（OPT serving），由于缺少清晰的PR创建流程和其他必要标准，需进行检查和更新。,https://github.com/hpcaitech/ColossalAI/issues/2804
ColossalAI,这个issue属于功能需求类型，主要涉及的对象是`torch.where()`函数的元信息，用户需要更新该函数的元信息以满足特定需要。,https://github.com/hpcaitech/ColossalAI/issues/2800
ColossalAI,这是一个功能需求类型的issue，主要涉及到`torch.arange()`方法的元信息补丁。由于GPT自动并行运行时的需求，需要对`torch.arange()`方法的元信息进行修补。,https://github.com/hpcaitech/ColossalAI/issues/2799
ColossalAI,这个issue属于一个需求提出类型，主要涉及的对象是为ColossalAI的chatgpt添加测试检查点。根据描述，用户为了确认保存/加载检查点是否正常操作，因此添加了新的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/2797
ColossalAI,这个issue是一个用户提出需求的问题，主要涉及ColossalAI中ChatGPT的训练步骤的理解，用户提出了关于ChatGPT三个训练步骤的疑问，希望了解由RLHF使用预训练语言模型和奖励模型时的第三步是什么，以及关于使用真实提示数据进行训练的步骤。,https://github.com/hpcaitech/ColossalAI/issues/2793
ColossalAI,这个issue类型是功能更新，主要涉及更新ColossalAI中ChatGPT模块的readme内容，帮助用户理解如何保存和加载checkpoint。原因可能是为了提高项目的易用性和可维护性。,https://github.com/hpcaitech/ColossalAI/issues/2792
ColossalAI,这个issue是一个文档更新的类型，涉及主要对象是ColossalAI的README.md文件，由于拼写错误导致需进行修正。,https://github.com/hpcaitech/ColossalAI/issues/2791
ColossalAI,该issue属于用户提出需求类型，主要涉及ColossalAI中与张量相关操作的元信息补丁，可能由于元信息缺失导致功能不完整或不明确的问题。,https://github.com/hpcaitech/ColossalAI/issues/2789
ColossalAI,这个issue是关于功能新增的需求，主要涉及`torch.Tensor.view()`方法的元信息补丁，原因是为了GPT自动并行运行时的元信息补丁。,https://github.com/hpcaitech/ColossalAI/issues/2788
ColossalAI,这个issue类型是功能需求，主要涉及的对象是`torch.Tensor.split()`函数的元信息，由于CC项目需要更多元信息补丁来支持GPT自动并行运行时，因此需要对`torch.Tensor.split()`的元信息进行补丁。,https://github.com/hpcaitech/ColossalAI/issues/2787
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是`torch.Tensor.permute()`方法的元信息。,https://github.com/hpcaitech/ColossalAI/issues/2786
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是`torch.Tensor.transpose()`，问题是要为`torch.Tensor.transpose()`函数补充元信息。,https://github.com/hpcaitech/ColossalAI/issues/2785
ColossalAI,这是一个功能需求的issue，主要涉及`torch.Tensor.contiguous()`方法的元信息补丁，由GPT自动并行运行时的更多元信息补丁相关。,https://github.com/hpcaitech/ColossalAI/issues/2784
ColossalAI,该问题单属于需求提出类型，主要对象是`torch.Tensor.type()`方法的元信息缺失。原因是为了 GPT 自动并行运行时的更多元信息补丁。,https://github.com/hpcaitech/ColossalAI/issues/2783
ColossalAI,这是一个用户提出需求的类型，主要对象是`torch.Tensor.to()`方法。由于CC([FEATURE]: More meta information patch for GPT autoparallel runtime)需要新增对`torch.Tensor.to()`方法的元信息补丁，所以提出了该需求。,https://github.com/hpcaitech/ColossalAI/issues/2782
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是`torch.Tensor.size()`方法。这个问题由于需要在GPT自动并行运行时中的更多元信息修补而产生。,https://github.com/hpcaitech/ColossalAI/issues/2780
ColossalAI,这个issue是一个新功能请求，主要涉及的对象是`torch.tensor()`。原因是为了在GPT自动并行运行时增强`torch.tensor()`的元信息。,https://github.com/hpcaitech/ColossalAI/issues/2779
ColossalAI,这个issue是一个功能性需求，涉及到ColossalAI中的torch.tanh()和torch.nn.Dropout，通过修改meta信息和优化代码来实现更清洁的实现，由于测试不支持torch 1.11.0版本，因此需要进一步调试。,https://github.com/hpcaitech/ColossalAI/issues/2773
ColossalAI,这个issue类型为文档更新，并涉及ColossalAI的PR创建流程。由于缺乏标准化的PR创建流程，可能导致PR的遗漏或不完整。,https://github.com/hpcaitech/ColossalAI/issues/2769
ColossalAI,这个issue是一个功能需求，涉及的主要对象是ColossalAI中的Strategy类，用户提出需要为Strategy类添加一个prepare()方法来帮助用户更轻松地准备他们的模型和优化器。,https://github.com/hpcaitech/ColossalAI/issues/2766
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中的AutoChunk内存估算逻辑不准确，特别是在unet中，需要改进并澄清其逻辑。,https://github.com/hpcaitech/ColossalAI/issues/2757
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的ChatGPT是否支持Tensor Parallelism或Pipeline Parallelism的问题。由于目前代码尚不支持这两种并行特性，用户询问是否有计划添加这些并行特性。,https://github.com/hpcaitech/ColossalAI/issues/2756
ColossalAI,这是一个关于用户需求的issue，主要涉及Colossal generation API返回序列数量控制的问题。用户希望了解如何通过类似于Huggingface API中的`num_return_sequences`参数来控制返回的序列数量。,https://github.com/hpcaitech/ColossalAI/issues/2753
ColossalAI,该issue是一个功能需求，需要重写ColossalAI中的Pipeline Scheduler，以实现更可读和可扩展的设计。,https://github.com/hpcaitech/ColossalAI/issues/2749
ColossalAI,该issue类型是文档更新，涉及ColossalAI的OPT服务文档的补充。由于文档缺失，用户提出了需要将OPT服务文档添加回网站的需求。,https://github.com/hpcaitech/ColossalAI/issues/2747
ColossalAI,该issue属于需求提出类型，主要涉及ColossalAI中的reward model training策略，用户提出了支持ddp、colossalai_zero、colossalai_gemini策略的需求。造成这个需求的原因可能是为了提高reward model训练的效率和灵活性。,https://github.com/hpcaitech/ColossalAI/issues/2742
ColossalAI,这是一个关于自动更新子模块提交的问题，类型为功能需求，涉及主要对象为代码仓库中的子模块引用。,https://github.com/hpcaitech/ColossalAI/issues/2740
ColossalAI,这个issue属于需求类型，主要涉及的对象是代码格式。由于缺少PR创建前的必要检查，导致需要完善相关内容才能顺利进行代码合并。,https://github.com/hpcaitech/ColossalAI/issues/2738
ColossalAI,这是一个功能需求类型的issue，主要涉及对象是`torch.nn.Embedding`。因为要更新`torch.nn.Embedding`的元信息，可能是为了在GPT自动并行运行时使用更多的元信息。,https://github.com/hpcaitech/ColossalAI/issues/2735
ColossalAI,这个issue类型是代码风格优化，涉及的主要对象是ColossalAI的命令行界面(cli.py)，由于需要统一代码风格，导致了代码提交前需要满足一系列的规范要求。,https://github.com/hpcaitech/ColossalAI/issues/2734
ColossalAI,这是一个文档更新的Issue，涉及的主要对象是ColossalAI的版本文档，用户提出了更新版本列表的需求。,https://github.com/hpcaitech/ColossalAI/issues/2730
ColossalAI,这是一个功能需求反馈，主要涉及到ColossalAI库中并行策略的区分，由于需要生成不同类型的策略，用户希望handler能够根据solver_preference参数区分不同的并行策略。,https://github.com/hpcaitech/ColossalAI/issues/2729
ColossalAI,这个issue是一个代码风格问题的改进，主要对象是ColossalAI中的auto_parallel/tensor_shard/deprecated/op_handler/batch_norm_handler.py文件。原因可能是为了统一代码风格或提高代码可读性。,https://github.com/hpcaitech/ColossalAI/issues/2728
ColossalAI,这个issue属于需求类型，主要涉及更新ColossalAI以包括ChatGPT代码。原因可能是想要整合ChatGPT功能到ColossalAI中。,https://github.com/hpcaitech/ColossalAI/issues/2727
ColossalAI,这是一个代码质量优化的Issue，主要涉及ColossalAI的auto_parallel/tensor_shard/deprecated/op_handler/embedding_handler.py文件，用户希望通过代码风格调整来改进代码质量。,https://github.com/hpcaitech/ColossalAI/issues/2725
ColossalAI,这个issue是一个代码审查请求，主要涉及ColossalAI中的自动并行处理模块的代码风格问题。问题由于缺少符合标准格式的标题以及对代码规范性的关注所引起。,https://github.com/hpcaitech/ColossalAI/issues/2723
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI中的ChatGPT文档优化问题。用户提出了一系列关于如何开始训练、数据集准备、模型保存、模型生成等问题，表明新用户在阅读readme后难以开始训练。,https://github.com/hpcaitech/ColossalAI/issues/2722
ColossalAI,这是一个优化生成kwargs的issue，主要涉及ChatGPT生成器优化参数传递的问题。,https://github.com/hpcaitech/ColossalAI/issues/2717
ColossalAI,这个issue类型是文档更新，涉及主要对象为ColossalAI的版本列表，由于文档需要更新显示最新版本信息。,https://github.com/hpcaitech/ColossalAI/issues/2715
ColossalAI,该issue类型为代码风格优化，涉及对象为ColossalAI下的cli/launcher/__init__.py文件。由于未遵循代码规范，导致代码风格不统一，需要进行整理。,https://github.com/hpcaitech/ColossalAI/issues/2709
ColossalAI,这个issue是关于代码风格的优化，属于代码改进类型，主要涉及到ColossalAI项目中的engine/gradient_handler/utils.py文件，需要优化代码风格。原因可能是为了提升代码可读性和维护性。,https://github.com/hpcaitech/ColossalAI/issues/2708
ColossalAI,这是一个特性请求，涉及更新子模块提交的自动化操作。,https://github.com/hpcaitech/ColossalAI/issues/2707
ColossalAI,这个issue是一个文档更新类型的问题，主要涉及ColossalAI的PR创建流程。由于没有按照标准格式进行操作，所以需要对图像进行调整。,https://github.com/hpcaitech/ColossalAI/issues/2705
ColossalAI,该issue类型为文档（documentation）更新，主要涉及到为ColossalAI添加ChatGPT文档。原因可能是为了完善ColossalAI的文档内容。,https://github.com/hpcaitech/ColossalAI/issues/2703
ColossalAI,该issue属于类型为代码风格优化的需求，主要涉及ColossalAI中`embedding_handler.py`文件的代码风格问题，用户提出了优化该文件代码风格的需求。,https://github.com/hpcaitech/ColossalAI/issues/2702
ColossalAI,这个issue属于PR请求的类型，主要涉及ColossalAI的代码风格优化，用户请求对代码文件进行升级。,https://github.com/hpcaitech/ColossalAI/issues/2701
ColossalAI,这是一个功能增强需求，涉及到ColossalAI库中的自动并行策略方案的区分，由于并行策略顺序的改变，需要对单元测试进行调整。,https://github.com/hpcaitech/ColossalAI/issues/2699
ColossalAI,这个issue类型为功能需求，主要涉及ColossalAI中增加chatgpt应用程序。发起这个需求的原因可能是为了扩展ColossalAI的功能和应用范围。,https://github.com/hpcaitech/ColossalAI/issues/2698
ColossalAI,这个issue是一个功能需求类型，主要涉及ColossalAI中自动并行处理的shard选项，用户想要通过该选项指定求解器偏好和设备网格使用的轴数。,https://github.com/hpcaitech/ColossalAI/issues/2697
ColossalAI,"该issue为用户提出需求，主要对象是""autoparallel""功能，用户提出了添加""shard""选项的需求。",https://github.com/hpcaitech/ColossalAI/issues/2696
ColossalAI,这个issue类型是关于代码风格改进的请求，涉及主要对象是ColossalAI中的自动并行工具。原因是为了改进代码风格和可读性。,https://github.com/hpcaitech/ColossalAI/issues/2695
ColossalAI,这个issue是一个代码风格改进类型的贡献请求，涉及到ColossalAI中的moe_context.py文件，用户希望优化代码风格以提高代码质量。,https://github.com/hpcaitech/ColossalAI/issues/2693
ColossalAI,这个issue类型为代码风格优化，涉及主要对象为ColossalAI中的gemini/gemini_context.py文件，最可能是由于代码风格不符合规范导致的。,https://github.com/hpcaitech/ColossalAI/issues/2690
ColossalAI,这是一个功能更新的issue，主要涉及ColossalAI中grad store的更新方法。由于代码中直接引用私有成员变量而不是调用方法修改内容，导致需要添加或更改grad store数值。,https://github.com/hpcaitech/ColossalAI/issues/2687
ColossalAI,这个issue是一个文档更新的请求，主要涉及ColossalAI的自动并行处理功能，用户希望更新相关论文链接。,https://github.com/hpcaitech/ColossalAI/issues/2686
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI框架。该问题由于在训练过程中无法获取梯度范数(grad_norm)，希望增加grad_norm记录功能以便监控训练过程并确保梯度在期望范围内。,https://github.com/hpcaitech/ColossalAI/issues/2684
ColossalAI,这是一个用户提出需求的类型，主要对象是ColossalAI的文档发布流程。由于现有流程中文档发布步骤不够现代化和健壮，用户提出希望在代码发布后一并发布文档的建议。,https://github.com/hpcaitech/ColossalAI/issues/2679
ColossalAI,这是一个需求提出的类型，主要涉及ColossalAI文档构建系统，用户提出需要检查`docs`文件夹更改时的文档构建过程。,https://github.com/hpcaitech/ColossalAI/issues/2676
ColossalAI,这是一个功能改进的issue，主要涉及到`torch.nn.functional.softmax`和`torch.nn.Softmax`，由于缺少正确的meta信息导致问题。,https://github.com/hpcaitech/ColossalAI/issues/2674
ColossalAI,这个issue属于用户提出需求类型，主要涉及T5 example的PR贡献。由于团队人手有限，需要社区贡献者的帮助来提交这个T5 example。,https://github.com/hpcaitech/ColossalAI/issues/2667
ColossalAI,这个issue类型是一个文档添加请求，主要对象是ColossalAI的CVPR教程。这个请求是由于没有CVPR教程而导致的。,https://github.com/hpcaitech/ColossalAI/issues/2666
ColossalAI,这个issue是一个提出需求的类型，主要对象是ColossalAI中的自动并行（autoparallel）模块，由于长时间未维护的弃用代码和缺乏依赖于弃用版本的演示或示例，需要移除这些废弃代码。,https://github.com/hpcaitech/ColossalAI/issues/2664
ColossalAI,该issue是关于功能特性的需求，主要涉及到ColossalAI项目中的autoparallel模块。原因是由于deprecated代码长时间未维护，且没有基于该版本的演示或示例，因此需要将其移除。,https://github.com/hpcaitech/ColossalAI/issues/2663
ColossalAI,这是一个需求提出类型的issue，主要涉及文档集成和版本控制。,https://github.com/hpcaitech/ColossalAI/issues/2655
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI项目中构建现代和强大文档系统的提议。,https://github.com/hpcaitech/ColossalAI/issues/2651
ColossalAI,这是一个类型为功能需求的issue，主要涉及的对象是ColossalAI仓库中的子模块引用同步操作。,https://github.com/hpcaitech/ColossalAI/issues/2648
ColossalAI,这是一个用户提出需求的类型的issue，涉及的主要对象是ColossalAI的autoparallel runtime preparation pass。这个需求是由于代码的冗长和不易维护性导致，需要进行重构以提高可维护性和可读性。,https://github.com/hpcaitech/ColossalAI/issues/2645
ColossalAI,这个issue类型为特性改进，主要涉及到ColossalAI的runtime pass重构，由于代码冗余性导致可维护性和可读性差。,https://github.com/hpcaitech/ColossalAI/issues/2644
ColossalAI,这个issue属于功能需求类型，主要涉及持续集成（CI）的添加，导致症状为稳定扩散跟踪最新ColossalAI和PyTorch Lightning的功能。,https://github.com/hpcaitech/ColossalAI/issues/2641
ColossalAI,这是一个用户提出需求的类型，主要涉及的对象是ColossalAI中的stable diffusion模块。由于缺乏持续集成（CI），导致无法追踪最新的ColossalAI和PyTorch Lightning版本，用户希望添加CI功能以实现这一目的。,https://github.com/hpcaitech/ColossalAI/issues/2640
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的checkpoint功能，用户认为tutorial和代码不清晰，需要更具体的说明。,https://github.com/hpcaitech/ColossalAI/issues/2638
ColossalAI,这个issue是关于文档更新，不属于bug报告，主要涉及ColossalAI项目的安装指南修改，用户需求改变导致相关内容被移除。,https://github.com/hpcaitech/ColossalAI/issues/2637
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的预构建wheel版本的发布，并由于网络问题、统计安装次数的困难、构建和托管的时间和成本效率低而计划移除预构建版本。,https://github.com/hpcaitech/ColossalAI/issues/2636
ColossalAI,这是一个文档更新的issue，主要涉及到ColossalAI的Sphinx主题更新。根据checklist和描述，该问题由于Sphinx主题过时，用户更新了现代和清爽的`book`主题。,https://github.com/hpcaitech/ColossalAI/issues/2635
ColossalAI,这是一个需求提出的issue，主要涉及文档主题的更新问题，用户希望通过使用现代化的主题来提升用户体验。,https://github.com/hpcaitech/ColossalAI/issues/2634
ColossalAI,这是一个功能需求类型的issue，主要涉及到`torch.Tensor.unsqueeze()`方法的元信息补丁。该问题可能是由于GPT自动并行运行时需要更多元信息补丁所致。,https://github.com/hpcaitech/ColossalAI/issues/2633
ColossalAI,这是一个用户提出需求的类型，主要对象是`torch.nn.LayerNorm`，用户希望在GPT autoparallel runtime中修补该模块的元信息。,https://github.com/hpcaitech/ColossalAI/issues/2632
ColossalAI,这个issue类型是用户提出需求，主要对象是模块`torch.nn.Dropout`，由于需要增加更多的元信息补丁以支持GPT自动并行运行时。,https://github.com/hpcaitech/ColossalAI/issues/2631
ColossalAI,这是一个用户提出需求的issue，主要涉及对象为`torch.tanh()`函数的meta信息补丁。,https://github.com/hpcaitech/ColossalAI/issues/2630
ColossalAI,这是一个用户提出需求的issue，主要对象是`torch.nn.functional.softmax()`，用户希望为该函数增加元信息的补丁。,https://github.com/hpcaitech/ColossalAI/issues/2629
ColossalAI,这是一个功能需求的issue，主要涉及到ColossalAI下的GPT自动并行运行时的增强信息补丁。,https://github.com/hpcaitech/ColossalAI/issues/2628
ColossalAI,这是一个需求类型的issue，主要对象是ColossalAI中的auto parallel特性测试文本。由于auto parallel原始测试过于冗长，用户希望能简化测试文本以适应最新的初始化API。,https://github.com/hpcaitech/ColossalAI/issues/2627
ColossalAI,该issue是关于添加新功能到ColossalAI的教程中所需的软件包要求，类型为功能改进。涉及的主要对象是ColossalAI的教程。这个问题是由于需要在示例中添加`energonai`到`requirements.txt`文件，并符合标准格式和流程的PR提交要求。,https://github.com/hpcaitech/ColossalAI/issues/2625
ColossalAI,这个issue属于功能需求类，涉及到ColossalAI库中的autochunk模块，主要是关于支持对diffusion的操作，并优化内存占用。,https://github.com/hpcaitech/ColossalAI/issues/2621
ColossalAI,该 issue 为用户提出需求类型，主要涉及到 ColossalAI 中的 autochunk 功能，用户希望支持扩散算法。,https://github.com/hpcaitech/ColossalAI/issues/2620
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的教程添加视频链接。该需求可能是为了提供更多的学习资源或增强文档的可读性。,https://github.com/hpcaitech/ColossalAI/issues/2619
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中重新设计reshape处理程序的文件结构。,https://github.com/hpcaitech/ColossalAI/issues/2616
ColossalAI,这是一个技术需求的issue，涉及ColossalAI中的autoparallel模块，该PR旨在重构处理重塑输入张量的处理程序，问题可能源于现有处理程序实现方式不够高效。,https://github.com/hpcaitech/ColossalAI/issues/2615
ColossalAI,这个issue是一个功能需求，涉及autoparallel模块，用户提出需要添加overlap选项来改进功能。,https://github.com/hpcaitech/ColossalAI/issues/2613
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及ColossalAI中自动并行功能的改进。,https://github.com/hpcaitech/ColossalAI/issues/2612
ColossalAI,这是一个针对代码协作中的bot功能的类型为需求提出的issue，主要涉及到更新子模块提交记录，可能由于频繁更新子模块导致需要自动化处理的问题。,https://github.com/hpcaitech/ColossalAI/issues/2607
ColossalAI,这个issue类型是更新文档（documentation）的类型，涉及的主要对象是CI/CD流程。由于文档内容的更新，用户可能提出了关于CI/CD流程的问题或者寻求帮助。,https://github.com/hpcaitech/ColossalAI/issues/2600
ColossalAI,这是一个需求反馈issue，主要涉及更新CI/CD文档，由于完成了自动化改进提案的大部分工作。,https://github.com/hpcaitech/ColossalAI/issues/2599
ColossalAI,这是一个功能优化的issue，主要涉及CUDA扩展构建在发布之前的测试。由于重命名`release_bdist.yml`文件，以便在发布之前仅用于检查CUDA扩展能否构建，从而提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/2598
ColossalAI,这个issue是一个功能改进类型的请求，主要涉及到ColossalAI中的pypi发布通知功能，用户寻求hook pypi release与lark的关联。,https://github.com/hpcaitech/ColossalAI/issues/2596
ColossalAI,这是一个功能新增的issue，涉及主要对象为hook docker release with lark，由于需要将docker release状态通知到lark，因此需要进行相关的操作。,https://github.com/hpcaitech/ColossalAI/issues/2594
ColossalAI,此issue属于一个功能增强的提议，主要涉及到DevOps中docker release的自动化以及与lark的集成，并提到了在Docker构建和推送过程中因网络问题导致的失败情况。,https://github.com/hpcaitech/ColossalAI/issues/2593
ColossalAI,这是一个功能性质的issue，主要涉及ColossalAI项目中的workflow更新，目的是在发布之前在testpypi上进行包分发测试。,https://github.com/hpcaitech/ColossalAI/issues/2591
ColossalAI,这是一个功能改进的issue，涉及的主要对象是ColossalAI的工作流程，问题出现的原因是在example check流程中存在重复，并且缺乏lark通知。,https://github.com/hpcaitech/ColossalAI/issues/2585
ColossalAI,这个issue属于需求提出类，主要涉及到自动化测试与消息通知相关的开发运维工作，目的是提高开发体验。,https://github.com/hpcaitech/ColossalAI/issues/2583
ColossalAI,该issue类型为功能特性需求，涉及到 `torch.matmul` 函数的元信息补丁。原因是为了提供策略与自动并行系统，并通过元信息生成良好的策略。,https://github.com/hpcaitech/ColossalAI/issues/2582
ColossalAI,这个issue是一个需求报告，主要涉及的对象是针对每日自动构建和测试工作流程中GPU可用性的检查。由于GPU可能被其他程序占用导致无法使用，因此提出了需要跳过工作流程的建议。,https://github.com/hpcaitech/ColossalAI/issues/2580
ColossalAI,这是一个技术改进提议的issue，主要涉及ColossalAI项目中用户体验的问题。由于安装过程出现奇怪的失败、文档混乱过时和示例错误等原因，用户体验受到影响。,https://github.com/hpcaitech/ColossalAI/issues/2579
ColossalAI,该issue类型为功能需求，主要涉及的对象是ColossalAI的开发团队。这个问题由于缺乏自动化通知机制，导致开发团队无法及时了解8-GPU测试失败的情况。,https://github.com/hpcaitech/ColossalAI/issues/2575
ColossalAI,这个issue是一个功能增强请求，主要涉及到ColossalAI的工作流程，由于构建和测试失败导致通知的需求。,https://github.com/hpcaitech/ColossalAI/issues/2574
ColossalAI,这是一个关于提高自动化以改善开发体验的建议，主要涉及开发者和组织者，由于缺乏自动化测试缓存功能以及组织者需要清晰的项目状态反馈，提出需要改进的建议。,https://github.com/hpcaitech/ColossalAI/issues/2573
ColossalAI,这个issue属于功能增强类型，涉及的主要对象是ColossalAI的用户社区报告，由于加入讨论统计功能，希望更好地反映团队成员对社区问题的处理情况。,https://github.com/hpcaitech/ColossalAI/issues/2572
ColossalAI,这是一个需求类型的issue，主要涉及的对象是开发社区报告生成流程。原因是当前的报告不包含讨论统计信息，用户建议将其添加进报告中。,https://github.com/hpcaitech/ColossalAI/issues/2571
ColossalAI,该issue属于教程优化类型，主要对象是ColossalAI项目的README页面，涉及的原因是为了提高文档质量和可读性。,https://github.com/hpcaitech/ColossalAI/issues/2568
ColossalAI,这个issue类型为用户提出需求，主要涉及的对象是更新FastFold教程。用户提出了需要更新FastFold教程并添加子模块和README文件的要求。,https://github.com/hpcaitech/ColossalAI/issues/2565
ColossalAI,这是一个feature改进类型的issue，主要涉及到贡献者和用户参与度报告的生成及发送，导致的症状是需要生成包括代码贡献和用户参与度在内的报告。,https://github.com/hpcaitech/ColossalAI/issues/2564
ColossalAI,该issue类型为功能需求，涉及主要对象为生成社区报告功能。,https://github.com/hpcaitech/ColossalAI/issues/2563
ColossalAI,这个issue是一个workflow改进类型的问题，涉及的主要对象是ColossalAI的compatibility test workflow，由于原来的workflow命名和文件名不易维护，因此进行了重构。,https://github.com/hpcaitech/ColossalAI/issues/2560
ColossalAI,这是一个改进工作流程的问题，主要涉及到GPU内存阈值的调整。,https://github.com/hpcaitech/ColossalAI/issues/2558
ColossalAI,这是一个需求提出类型的issue，主要涉及的对象是ColossalAI的devops工作流。由于GPU内存使用阈值设置过低，导致工作流有时会停止，需要调整该阈值以确保每天都能顺利运行。,https://github.com/hpcaitech/ColossalAI/issues/2557
ColossalAI,这个issue是一个文档更新类型的问题，涉及主要对象为ColossalAI中的CHANGE_LOG.md文件。由于CHANGE_LOG.md内容过时，用户提出了更新文档以指向github发布页面的需求。,https://github.com/hpcaitech/ColossalAI/issues/2552
ColossalAI,这个issue是关于文档维护的需求，提出了CHANGE_LOG.md需要更新的问题，建议重定向到GitHub的发布页面。,https://github.com/hpcaitech/ColossalAI/issues/2551
ColossalAI,这个issue类型是关于改进开发流程的提议，主要对象是开发流程和PR模板。这个问题是由于缺乏标准化的开发流程而导致的。,https://github.com/hpcaitech/ColossalAI/issues/2550
ColossalAI,该issue为用户提出需求类型，涉及到删除操作。这可能是由于用户想要删除某些内容或信息而提出的问题或需求。,https://github.com/hpcaitech/ColossalAI/issues/2547
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI中unet模块的meta信息和flop支持。原因是precommit代码格式引入了一些改变。,https://github.com/hpcaitech/ColossalAI/issues/2544
ColossalAI,这是一个需求提出类型的Issue，涉及主要对象为ColossalAI中的transformer和alphafold模块，用户提出增加benchmark，并针对内存和速度进行改进。,https://github.com/hpcaitech/ColossalAI/issues/2543
ColossalAI,这个issue属于代码优化类型，主要涉及ColoTensor及其子模块的调整。导致该问题的原因可能是之前代码实现存在一些不够优化的地方，需要进行进一步的改进。,https://github.com/hpcaitech/ColossalAI/issues/2537
ColossalAI,该issue为提出需求类型，主要涉及Gemini Decouples ChunkManager with the Model。由于ChunkManager目前挂在PyTorch模型上，导致无法处理多个模型同时训练的问题，同时也与PyTorch的使用方式有所差异。,https://github.com/hpcaitech/ColossalAI/issues/2536
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI下的Gemini demo，用户提出了添加性能分析工具的需求。,https://github.com/hpcaitech/ColossalAI/issues/2534
ColossalAI,这是一个需求类型的issue，主要对象是CI系统。这个问题的根源在于CI报告了整个库的测试覆盖率，而开发者只关心PR中更改文件的覆盖率。,https://github.com/hpcaitech/ColossalAI/issues/2530
ColossalAI,"这是一个用户提出需求的issue单，主要涉及的对象是添加一个名为""fastfold""的推理示例。",https://github.com/hpcaitech/ColossalAI/issues/2528
ColossalAI,这个issue属于需求更新类型，主要涉及Gemini模块中GPT示例的更新。,https://github.com/hpcaitech/ColossalAI/issues/2527
ColossalAI,这是一个功能需求类型的issue，涉及到测试覆盖率报告的修改，主要目的是仅报告更改的文件的覆盖率。,https://github.com/hpcaitech/ColossalAI/issues/2524
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的ZeRO（Zero Redundancy Optimizer），用户希望添加两个统一当前ZeRO API的包装函数。,https://github.com/hpcaitech/ColossalAI/issues/2523
ColossalAI,这个issue属于需要更新依赖项的类型，主要涉及到ColossalAI中的稳定扩散（diffusion）功能。由于可能是因为Lightning库的更新，导致现有的依赖项需要更新以适配稳定扩散功能所需的新特性。,https://github.com/hpcaitech/ColossalAI/issues/2522
ColossalAI,这是一个文档更新的issue，主要涉及ColossalAI的例子链接更新。,https://github.com/hpcaitech/ColossalAI/issues/2520
ColossalAI,这个issue属于更新型工作，涉及主要对象为hybrid model parallel GPT2 example。,https://github.com/hpcaitech/ColossalAI/issues/2519
ColossalAI,该issue类型为功能优化，主要涉及Gemini库中的DDP strict mode更新，用户提出了希望能够在初始化参数时减少内存消耗的需求。,https://github.com/hpcaitech/ColossalAI/issues/2518
ColossalAI,这是一个类型是更新请求的issue，涉及主要对象是保持分支与主分支同步，发起请求的目的是保持分支更新。,https://github.com/hpcaitech/ColossalAI/issues/2517
ColossalAI,这是一个关于工作流程（workflow）优化的类型为需求提出的issue，主要涉及测试覆盖率（coverage）报告。,https://github.com/hpcaitech/ColossalAI/issues/2516
ColossalAI,这是一个文档更新的issue，主要涉及到ColossalAI的opt和tutorial链接更新问题。,https://github.com/hpcaitech/ColossalAI/issues/2509
ColossalAI,这个issue是一个功能请求，主要对象是ColossalAI下的zero模块。用户提出了需要添加strict ddp模式的需求。,https://github.com/hpcaitech/ColossalAI/issues/2508
ColossalAI,这个issue属于新功能需求，主要涉及到ColossalAI中的Pipeline模块，用户提出了对于分区方法的新增，以简化DP formulation的实现。,https://github.com/hpcaitech/ColossalAI/issues/2507
ColossalAI,这个issue类型是功能需求，主要涉及的对象是ColossalAI中的autochunk功能。,https://github.com/hpcaitech/ColossalAI/issues/2506
ColossalAI,这个issue类型是功能需求，涉及主要对象是ColossalAI下的autochunk模块，用户提出了需要支持extramsa模块的新功能和更新。,https://github.com/hpcaitech/ColossalAI/issues/2504
ColossalAI,该issue是一个用户需求提出，涉及ColossalAI库中的支持extramsa模块的功能更新。,https://github.com/hpcaitech/ColossalAI/issues/2503
ColossalAI,这个issue类型是功能需求提议，主要对象是ColoGraphModule。由于ActivationCheckpointCodeGen在__init__中被自动设置导致其他codegen不能在该过程中设置，提出了允许控制是否在__init__中设置ActivationCheckpointCodeGen的需求。,https://github.com/hpcaitech/ColossalAI/issues/2498
ColossalAI,这是一个功能更新的issue，主要涉及到ColossalAI中evoformer模型的自动分块支持。,https://github.com/hpcaitech/ColossalAI/issues/2497
ColossalAI,这是一个用户提出需求的类型，该问题涉及到ColossalAI中用于加速GPT-2训练的自动并行化（autoparallel）功能。,https://github.com/hpcaitech/ColossalAI/issues/2495
ColossalAI,这个issue类型是功能需求，要求自动化更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/2492
ColossalAI,这是一个用户需求的请求：为ColossalAI中的pipeline parallel添加对U-Net的支持。,https://github.com/hpcaitech/ColossalAI/issues/2490
ColossalAI,这是一个用户提出需求的issue， 主要涉及的对象是ColossalAI中的evoformer tracer模块。由于之前只支持evoformer的简化版本，现在用户希望添加完整的evoformer tracer功能，包括支持op、测试和添加新代码目录。,https://github.com/hpcaitech/ColossalAI/issues/2485
ColossalAI,该issue属于需求提出类型，主要对象是为GPT模型提供titans支持，可能由于用户希望在ColossalAI中集成titans来增强GPT模型的效果。,https://github.com/hpcaitech/ColossalAI/issues/2484
ColossalAI,这是一个用户提出需求的类型，主要涉及Pipeline中添加avg compute partition的操作。 ,https://github.com/hpcaitech/ColossalAI/issues/2483
ColossalAI,这是一个用户提出需求的issue，涉及ColossalAI中的stable diffusion版本，请求添加路线图。,https://github.com/hpcaitech/ColossalAI/issues/2482
ColossalAI,这是一个用户提出需求的issue，主要对象是更新ColossalAI中的GPT Gemini示例脚本和添加测试CI脚本以覆盖18个案例。,https://github.com/hpcaitech/ColossalAI/issues/2477
ColossalAI,这是一个需求类型的issue，涉及到ColossalAI下的Pipeline中的PP Middleware。由于需要添加GPipe Scheduler、修复生命周期、更换稳定的优化器进行测试以及修复offsetbased rpc，可能是为了提高性能或稳定性。,https://github.com/hpcaitech/ColossalAI/issues/2476
ColossalAI,这是一则需求类型的 issue，主要涉及到持续集成（CI），提出增加针对 palm、opt 和 gpt 的测试脚本。,https://github.com/hpcaitech/ColossalAI/issues/2475
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI项目中的zero优化器，可能由于性能不佳或者其他优化方面的问题而需要进行改进。,https://github.com/hpcaitech/ColossalAI/issues/2473
ColossalAI,这是一个功能需求的issue，主要关注Gemini(offload)、ZeRO1、ZeRO2和ZeRO3之间接口的统一化。,https://github.com/hpcaitech/ColossalAI/issues/2472
ColossalAI,这个issue是关于用户提出需求的类型，主要对象是ColossalAI下的zero模块。由于lowleveloptim模块不支持TP，用户寻求支持TP功能的帮助。,https://github.com/hpcaitech/ColossalAI/issues/2471
ColossalAI,该问题类型为功能需求更新，涉及主要对象为ColossalAI中的VIT（Vision Transformer）模型相关的CI脚本和配置，原因是需要重构虚拟数据加载器并添加更新的CI脚本和配置。,https://github.com/hpcaitech/ColossalAI/issues/2469
ColossalAI,该issue属于功能改进类型，主要涉及LowLevelZeroOptimizer的优化支持ProcessGroup，由于gpc是一个全局变量，导致无法与ColoTensor TP一起使用，此优化解决了此限制。,https://github.com/hpcaitech/ColossalAI/issues/2464
ColossalAI,这个issue是用户提出需求的类型，主要涉及了ColossalAI中的seq-parallel tutorial集成问题。可能由于CI集成问题，需要将该tutorial与CI整合。,https://github.com/hpcaitech/ColossalAI/issues/2463
ColossalAI,该issue是一个功能需求报告，主要涉及的对象是ColossalAI项目中的自动构建流程。,https://github.com/hpcaitech/ColossalAI/issues/2459
ColossalAI,这是用户提出的一个功能需求，主要涉及的对象是PyTorch库。,https://github.com/hpcaitech/ColossalAI/issues/2457
ColossalAI,这是一个用户提出需求并请教问题的issue，主要涉及到ColossalAI中的**GeminiDDP**和**colossalai.initialize**两种方式对torch模型进行封装的比较，用户想了解哪种方式更推荐并了解它们之间的差别。,https://github.com/hpcaitech/ColossalAI/issues/2455
ColossalAI,这是一个用户需求报告，主要涉及ColossalAI项目中的自动兼容性测试。,https://github.com/hpcaitech/ColossalAI/issues/2453
ColossalAI,这是一个需求更新类型的issue，主要对象是ColossalAI下的autoparallel tutorial demo。,https://github.com/hpcaitech/ColossalAI/issues/2449
ColossalAI,该issue属于功能更新，主要涉及到ColossalAI中大批量优化器教程的更新。由于希望将示例与CI集成，并仅保留对合成数据的用法，以便用户快速上手，导致了此次更新。,https://github.com/hpcaitech/ColossalAI/issues/2448
ColossalAI,这是一个功能增强的issue，主要涉及ColossalAI库中关于追踪本地检查点和代码生成的功能。由于之前忽略了使用`ColoTracer`追踪本地检查点并将其应用于`autoparallel`的可能性，因此此问题可能是为了解决这个兼容性问题而提出的。,https://github.com/hpcaitech/ColossalAI/issues/2438
ColossalAI,"这是一个关于功能改进的issue， 主要对象是""ddp""模块。由于需要替换代码中的函数调用方式，导致bug或者功能不完善，用户提出了改进的建议。",https://github.com/hpcaitech/ColossalAI/issues/2434
ColossalAI,这个issue类型是文档更改请求，主要涉及的对象是ColossalAI项目下的example README文件，由于文档的结构和格式不清晰，导致了阅读性和清晰度的问题。,https://github.com/hpcaitech/ColossalAI/issues/2427
ColossalAI,该issue为用户提出的需求类型，主要涉及新增ShardOption类的实现。由于用户需要在不同级别下过滤节点的分片策略，故提出此需求。,https://github.com/hpcaitech/ColossalAI/issues/2423
ColossalAI,这是一个文档更新类型的issue，主要涉及CI/CD工作流程的文档更新。,https://github.com/hpcaitech/ColossalAI/issues/2420
ColossalAI,这是一个功能增强类型的issue，主要涉及到自动评论测试覆盖率报告，用户期望每次进行单元测试时都可以在PR中自动展示测试覆盖率报告。,https://github.com/hpcaitech/ColossalAI/issues/2419
ColossalAI,这是一个功能需求的issue，主要涉及的对象是ColossalAI框架中的zero模块。,https://github.com/hpcaitech/ColossalAI/issues/2418
ColossalAI,这个issue是一个功能需求，涉及到自动评论和添加工作流程，主要解决了precommit检查失败时如何提醒贡献者修复格式的问题。,https://github.com/hpcaitech/ColossalAI/issues/2417
ColossalAI,这个issue属于功能需求类型，主要对象是非英语评论/问题，由于一些用户发布非英语评论导致其难于理解，因此需要将非英语评论自动翻译成英语以实现国际化。,https://github.com/hpcaitech/ColossalAI/issues/2414
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI项目的`setup.py`文件，由于Nightly版本在官方发布版本之后，导致GitHub将`colossalainightly`误认为是包名称。,https://github.com/hpcaitech/ColossalAI/issues/2413
ColossalAI,这个issue类型是需求提出，在benchmark中缺少了DISTPAN参数。,https://github.com/hpcaitech/ColossalAI/issues/2412
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的workflow refactoring。由于需要确保workflow正常运行，因此这个issue的目的是重构示例检查workflow。,https://github.com/hpcaitech/ColossalAI/issues/2411
ColossalAI,这个issue属于文档更新类，并且涉及ColossalAI的README更新。导致该问题的原因可能是之前的文档不完整或需要修正。,https://github.com/hpcaitech/ColossalAI/issues/2406
ColossalAI,这是一个代码优化类的issue，涉及ColossalAI中的`get_static_torch_model`函数。由于未提到具体的bug或问题，该issue主要是对代码进行了改进和优化。,https://github.com/hpcaitech/ColossalAI/issues/2405
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及的对象是ColossalAI项目中的nightly release workflow。导致这个问题产生的原因是为了实现每周一次向PyPI发布nightly版本。,https://github.com/hpcaitech/ColossalAI/issues/2403
ColossalAI,这是一个关于工作流程的改进的 issue，主要涉及到为代码一致性添加 precommit 检查。,https://github.com/hpcaitech/ColossalAI/issues/2401
ColossalAI,该issue为工作流程优化类型，主要对象是代码一致性检查，由于可能存在代码风格不一致导致的问题提出了相关需求。,https://github.com/hpcaitech/ColossalAI/issues/2400
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是ColossalAI项目的workflow文件。这个issue是由于需要生成测试覆盖报告，并将报告作为评论发布到Github的PR页面而提出的。,https://github.com/hpcaitech/ColossalAI/issues/2399
ColossalAI,这是一个关于改进工作流程的issue，涉及主要对象是并行化的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/2397
ColossalAI,这是一个功能需求的issue，主要涉及的对象是AlphaBetaProfiler和autoparallelize api。,https://github.com/hpcaitech/ColossalAI/issues/2393
ColossalAI,这是一个文档更新的issue，涉及主要对象是kernel-related optimizers，由于kernel可以在安装过程或运行时构建，因此需要更新其文档描述。,https://github.com/hpcaitech/ColossalAI/issues/2385
ColossalAI,该issue类型为文档更新请求，主要涉及项目ColossalAI下的builder模块的readme内容。,https://github.com/hpcaitech/ColossalAI/issues/2375
ColossalAI,这是一个功能改进的issue，涉及的主要对象是GPT PP示例。原因是不同设备上出现了一些分区问题。,https://github.com/hpcaitech/ColossalAI/issues/2373
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中关于模型分布式初始化的功能。该issue指出了ColossalAI提供了一种内存高效的初始化模型的方式，通过ColoInitContext和tensor_parallelize来实现1/N参数初始化，并减少初始化阶段的内存消耗。,https://github.com/hpcaitech/ColossalAI/issues/2366
ColossalAI,该issue属于用户提出需求类型，主要涉及ColossalAI项目中的PaLM模块。造成该需求提出的原因可能是为了向PaLM模块添加TFLOPS指标。,https://github.com/hpcaitech/ColossalAI/issues/2365
ColossalAI,这个issue类型为需求提出，主要对象是ColossalAI中的AutoChunk功能。由于高内存消耗造成模型性能瓶颈，用户提出了自动分块功能需求。,https://github.com/hpcaitech/ColossalAI/issues/2364
ColossalAI,这个issue类型是代码风格优化，主要涉及的对象是ColossalAI下的batch_norm_handler.py文件。由于代码风格不统一，可能导致阅读困难或者维护困难的情况。,https://github.com/hpcaitech/ColossalAI/issues/2359
ColossalAI,这个issue类型是代码优化（Code Refactor），主要涉及的对象是ColossalAI项目中的`batch_norm_handler.py`文件。,https://github.com/hpcaitech/ColossalAI/issues/2358
ColossalAI,这是一个代码风格优化类的issue，涉及的主要对象是ColossalAI项目中的batch_norm_handler.py文件。,https://github.com/hpcaitech/ColossalAI/issues/2357
ColossalAI,这是一个功能需求类型的issue，主要涉及Gemini模块的开发，新增了获取静态torch模型的功能，更新了对应的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/2356
ColossalAI,这个issue类型是用户提出需求，请求补充添加Google文档，主要对象是benchmark results of GPT。,https://github.com/hpcaitech/ColossalAI/issues/2355
ColossalAI,这个issue类型为功能需求，主要涉及ColossalAI中的auto parallel gpt2 demo上传。具体原因可能是为了展示自动并行训练的功能、提高模型训练效率。,https://github.com/hpcaitech/ColossalAI/issues/2354
ColossalAI,"这是一个关于GitHub上ColossalAI项目中""GPT example directory""目录结构优化的需求提出。",https://github.com/hpcaitech/ColossalAI/issues/2353
ColossalAI,这是一个用户提交的需求类型的 issue，主要涉及到 ColossalAI 中 auto_parallel/tensor_shard 模块下的 utils 代码。由于缺少具体描述，无法确定具体问题类型。,https://github.com/hpcaitech/ColossalAI/issues/2352
ColossalAI,这是一个用户提出需求的类型，涉及的主要对象是github上的ColossalAI项目。由于缺乏自动测试，用户提出引入GitHub Workflow来检查examples文件夹，节省审查者时间并确保示例代码与最新API一致。,https://github.com/hpcaitech/ColossalAI/issues/2351
ColossalAI,这个issue类型是代码优化（code style）类型，涉及的主要对象是ColossalAI下的auto_parallel/tensor_shard/utils/factory.py文件。由于代码风格不符合规范，用户提出优化请求。,https://github.com/hpcaitech/ColossalAI/issues/2349
ColossalAI,这是一个用户提出需求的issue，主要对象是为ColossalAI添加示例要求。,https://github.com/hpcaitech/ColossalAI/issues/2345
ColossalAI,这是一个需求类型的issue，主要涉及到优化功能的简化。原因可能是用户希望能够在短时间内快速运行优化算法进行性能分析，而不需要下载数据或设置复杂的训练参数。,https://github.com/hpcaitech/ColossalAI/issues/2344
ColossalAI,这个issue属于功能增强类型，主要涉及设备组合和逻辑网格算法，由于需要找到最佳逻辑网格以提高设备通信效率。,https://github.com/hpcaitech/ColossalAI/issues/2342
ColossalAI,这是一个用户提出需求类型的issue，主要涉及ColossalAI库的构建方式，用户希望只在执行时构建所需的CUDA内核以提升用户体验和库的完整性。,https://github.com/hpcaitech/ColossalAI/issues/2337
ColossalAI,这个issue属于特性需求，主要对象是ColossalAI库的用户，由于用户在安装ColossalAI时被强制构建CUDA扩展，导致不便。,https://github.com/hpcaitech/ColossalAI/issues/2336
ColossalAI,这是一个用户提出的需求，主要对象是ColossalAI的CUDA扩展安装，默认为可选，用户不必强制构建。,https://github.com/hpcaitech/ColossalAI/issues/2334
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及的对象是ColossalAI项目。该用户询问了关于将ColossalAI集成到diffusers框架的计划进展情况，表达了期待使用该功能的愿望。,https://github.com/hpcaitech/ColossalAI/issues/2331
ColossalAI,该issue类型是功能特性更新，涉及主要对象是ColossalAI的Dreamblooth组件，用户希望更新Dreamblooth的README文件。,https://github.com/hpcaitech/ColossalAI/issues/2329
ColossalAI,"这是一个代码风格优化的issue，主要对象是""NFC""（No Functional Change），用户提出了改进代码风格的建议。",https://github.com/hpcaitech/ColossalAI/issues/2323
ColossalAI,这是一个文档更新的issue，涉及主要对象为ColossalAI项目的stable diffusion链接。,https://github.com/hpcaitech/ColossalAI/issues/2322
ColossalAI,这个issue属于功能需求类型，主要涉及ColossalAI的发布到PYPI的自动化流程。原因是为了让用户能够更方便地通过`pip install colossalai`安装ColossalAI。,https://github.com/hpcaitech/ColossalAI/issues/2320
ColossalAI,该issue为用户提出需求。涉及主要对象为ColossalAI中的PaLM模型。,https://github.com/hpcaitech/ColossalAI/issues/2319
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是ColossalAI的logo。,https://github.com/hpcaitech/ColossalAI/issues/2316
ColossalAI,这是一个功能改进类型的issue，涉及的主要对象是ColossalAI的op_builder。由于存在重复的op_builder，需要对其进行优化，使其适用于设置和运行时构建。,https://github.com/hpcaitech/ColossalAI/issues/2314
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中的autoparallel模块，用户希望改进auto-ckpt求解器的稳定性和用户友好性。,https://github.com/hpcaitech/ColossalAI/issues/2313
ColossalAI,这是一个需求调整（feature request）类型的issue，涉及的主要对象是ColossalAI中的FX（flexible parallelization framework）。原因可能是为了优化代码结构和提升用户友好性而对算法进行了重构和移动。,https://github.com/hpcaitech/ColossalAI/issues/2312
ColossalAI,这是一个用户提出需求的issue，主要涉及AlphaBetaProfiler的性能分析功能，用户希望能够通过该工具对设备列表的alpha和beta数值进行分析。,https://github.com/hpcaitech/ColossalAI/issues/2311
ColossalAI,这是一个格式优化的issue，涉及代码风格的调整。原因可能是为了提升代码的可读性和一致性。,https://github.com/hpcaitech/ColossalAI/issues/2310
ColossalAI,这个issue类型是用户提出需求，主要涉及到ColossalAI中的train_dreambooth_colossalai脚本。用户提出了希望恢复train_text_encoder参数以支持更好结果的要求。,https://github.com/hpcaitech/ColossalAI/issues/2309
ColossalAI,这是一个类型为代码优化的issue，主要涉及到ColossalAI中cli/benchmark模块下的__init__.py文件。 由于代码风格问题而引起的优化需求。,https://github.com/hpcaitech/ColossalAI/issues/2308
ColossalAI,这个issue类型为功能更新，主要涉及Gemini Benchmark脚本，由于需要添加policy选项和更新版本检查功能。,https://github.com/hpcaitech/ColossalAI/issues/2306
ColossalAI,这是一个代码风格优化类的issue，涉及的主要对象是ColossalAI项目中的layer_norm_handler.py文件。,https://github.com/hpcaitech/ColossalAI/issues/2305
ColossalAI,该issue为需求类型，主要涉及更新ColossalAI下的Diffusion readme文档。,https://github.com/hpcaitech/ColossalAI/issues/2304
ColossalAI,这是一个关于代码风格优化的issue，主要涉及的对象是 communication/p2p_v2.py 文件。,https://github.com/hpcaitech/ColossalAI/issues/2303
ColossalAI,"这是一条针对代码样式的修改请求，针对的对象是""layer_norm_handler.py""文件。",https://github.com/hpcaitech/ColossalAI/issues/2302
ColossalAI,这个issue属于代码风格优化（code style）类型，主要对象是ColossalAI项目中的grad_scaler/dynamic_grad_scaler模块。由于代码风格混乱，需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/2299
ColossalAI,这是一个需求变更的类型。该问题单涉及ColossalAI库中的示例文件夹，主要是为了实现自动检查示例文件功能。,https://github.com/hpcaitech/ColossalAI/issues/2298
ColossalAI,这是一个需要改进代码风格的issue，主要涉及ColossalAI项目中的naive_amp.py文件。,https://github.com/hpcaitech/ColossalAI/issues/2297
ColossalAI,这是一个文档更新类的issue，主要对象是Diffusion文档。,https://github.com/hpcaitech/ColossalAI/issues/2296
ColossalAI,这是一个用户提出需求类型的issue，主要涉及ColossalAI文档中新闻部分的更新。,https://github.com/hpcaitech/ColossalAI/issues/2295
ColossalAI,这个issue类型为代码质量优化，主要涉及对象是ColossalAI的auto_parallel/tensor_shard/deprecated/op_handler/reshape_handler.py文件，问题出现的原因可能是代码风格需要调整，导致影响代码质量。,https://github.com/hpcaitech/ColossalAI/issues/2292
ColossalAI,这个issue类型是代码质量优化，主要涉及ColossalAI的auto_parallel模块中的node_handler.py文件，用户希望对其中的代码风格进行优化。,https://github.com/hpcaitech/ColossalAI/issues/2289
ColossalAI,该issue类型为代码风格优化，涉及的主要对象是ColossalAI项目中的benchmark.py文件。,https://github.com/hpcaitech/ColossalAI/issues/2287
ColossalAI,这是一个版本更新的issue，涉及主要对象为ColossalAI项目。,https://github.com/hpcaitech/ColossalAI/issues/2286
ColossalAI,这是一个用户提出需求的类型，主要涉及的对象是PaLM。由于需要使用参数结合两个版本，用户希望能够得到相关示例或指导。,https://github.com/hpcaitech/ColossalAI/issues/2284
ColossalAI,这是一个需求类型的issue，涉及对象是ColossalAI项目中的amp模块，用户提出需要为单元测试添加梯度裁剪功能。,https://github.com/hpcaitech/ColossalAI/issues/2283
ColossalAI,这是一个用户需求提出的issue，涉及ColossalAI中的特征传播v2、泛光和自动并行功能。,https://github.com/hpcaitech/ColossalAI/issues/2282
ColossalAI,这个issue属于功能需求的用户提出，主要对象是ColossalAI中的`CheckpointSolverRotor`类，用户提出需要传递额外的参数来指定优化器的行为，这是由于修改了CheckpointSolverRotor的`free_memory`导致需要从模型参数和优化器梯度中减去预算。,https://github.com/hpcaitech/ColossalAI/issues/2279
ColossalAI,这是一个优化性质的issue，主要涉及到ColossalAI库中的allgather和reducescatter在3D场景下的性能优化。原因是为了提高3D线性算法的效率。,https://github.com/hpcaitech/ColossalAI/issues/2278
ColossalAI,这是一个关于提出需求的类型。该问题单涉及的主要对象是ColossalAI项目中的MOE builder。,https://github.com/hpcaitech/ColossalAI/issues/2277
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI项目的benchmark。,https://github.com/hpcaitech/ColossalAI/issues/2276
ColossalAI,这是一个用户提出需求的类型的issue，主要对象是ColossalAI项目下的GPT polish readme。用户希望对GPT的Polish版本的readme进行修改。,https://github.com/hpcaitech/ColossalAI/issues/2274
ColossalAI,这个issue是用户提出需求。该问题单主要涉及的对象是添加GPT PP示例。,https://github.com/hpcaitech/ColossalAI/issues/2272
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI下的CkptSolver模块，用户提出将shape-consistency节点合并到一个大节点中的需求。,https://github.com/hpcaitech/ColossalAI/issues/2271
ColossalAI,该issue属于需求提出类型，主要涉及ColossalAI的工作流文件和示例文件，用户提出了新增自动检查功能和修复示例代码中的小错误的需求。,https://github.com/hpcaitech/ColossalAI/issues/2268
ColossalAI,"这是关于ColossalAI中autoparallel模块的""GPT-2 autoparallel examples""的功能需求。",https://github.com/hpcaitech/ColossalAI/issues/2267
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的autockpt选项与SPMD solver之间的连接，并针对性地提出了多个改进和新增功能的要求。,https://github.com/hpcaitech/ColossalAI/issues/2258
ColossalAI,这是一个关于自动激活检查点解决方案的功能需求类型的issue，主要涉及ColossalAI中ResNet节点的元信息hook以及通信节点的元信息处理，由于某些操作尚未被修补，未能将某些节点处理程序转换为`MetaInfoNodeHandler`或`MetaInfoModuleHandler`导致的。,https://github.com/hpcaitech/ColossalAI/issues/2248
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是`torch.flatten()`方法的元信息，问题是由于元信息需要与其他方法相匹配，当前情况下需要对元信息进行调整。,https://github.com/hpcaitech/ColossalAI/issues/2247
ColossalAI,这个issue类型是用户提需求，主要涉及的对象是ColossalAI项目中的稳定扩散示例，用户提出希望更新README.md以提供详细的Docker使用说明。,https://github.com/hpcaitech/ColossalAI/issues/2244
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及到ColossalAI项目的diffusion安装。这个问题可能由于Docker安装过程中的配置或执行步骤出现问题，导致用户无法成功安装diffusion，需要寻求帮助。,https://github.com/hpcaitech/ColossalAI/issues/2239
ColossalAI,这是一个功能需求类型的issue，涉及主要对象为autoparallelize初始化API。,https://github.com/hpcaitech/ColossalAI/issues/2238
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI库中的Palm和GeminiDPP的结合问题。原因可能是用户希望使这两个部分能够协同工作。,https://github.com/hpcaitech/ColossalAI/issues/2227
ColossalAI,这是一个用户提出需求类型的issue，涉及的主要对象是为GPT添加benchmark.sh脚本。,https://github.com/hpcaitech/ColossalAI/issues/2226
ColossalAI,这是一个用户提出需求的issue，该问题单涉及的主要对象是ColossalAI下的GPT示例，用户希望更新GPT示例。,https://github.com/hpcaitech/ColossalAI/issues/2225
ColossalAI,这是一个需求类型的issue，主要对象是项目中的一个特性模块。,https://github.com/hpcaitech/ColossalAI/issues/2223
ColossalAI,这个issue是一个功能需求，主要涉及Palm添加Gemini，可能是为了增强功能或者改进用户体验。,https://github.com/hpcaitech/ColossalAI/issues/2221
ColossalAI,这是一个用户提出需求的类型问题，主要对象是更新GPT基准测试。,https://github.com/hpcaitech/ColossalAI/issues/2219
ColossalAI,这是一个功能需求的issue，主要对象是ColossalAI代码中的autoparallel模块。,https://github.com/hpcaitech/ColossalAI/issues/2217
ColossalAI,这个issue类型是需求或者建议，涉及的主要对象是ColossalAI的builder模块。,https://github.com/hpcaitech/ColossalAI/issues/2216
ColossalAI,该issue为更新README文件的类型，单涉及的主要对象是ColossalAI中的diffusion模块。,https://github.com/hpcaitech/ColossalAI/issues/2214
ColossalAI,该issue类型是需求更新，主要涉及对象是GPT示例的基准结果。,https://github.com/hpcaitech/ColossalAI/issues/2212
ColossalAI,这是一个需求更新类型的issue，主要涉及ColossalAI的GPT示例的更新。Bug报告通常会描述具体的错误或异常行为，而此问题主要是针对示例代码的更新需求。,https://github.com/hpcaitech/ColossalAI/issues/2211
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI中的示例代码，用户希望将einsum替换为matmul。,https://github.com/hpcaitech/ColossalAI/issues/2210
ColossalAI,该issue类型为用户提出需求，针对的主要对象是README文件。由于README文件未更新性能信息，用户请求更新。,https://github.com/hpcaitech/ColossalAI/issues/2206
ColossalAI,这是一个用户提出需求的issue，主要涉及构建多头注意力机制运行时。原因可能是需要改进ColossalAI中多头注意力机制的构建过程。,https://github.com/hpcaitech/ColossalAI/issues/2203
ColossalAI,这是一个需求类型的issue，涉及对象为文档格式。原因可能是需要进一步修改、提升文档内容的质量。,https://github.com/hpcaitech/ColossalAI/issues/2201
ColossalAI,这个issue类型是需求提出，主要对象是ColossalAI的训练设置，用户想要更改一些训练设置来适应Diffusion模型。,https://github.com/hpcaitech/ColossalAI/issues/2195
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的auto parallel性能测试，目的是衡量自动并行系统在gpt2模型上的性能。,https://github.com/hpcaitech/ColossalAI/issues/2194
ColossalAI,这个issue属于类型为更新链接的问题，主要涉及到更新新闻链接。由于原始链接失效或需要更新，导致需要修正链接的问题。,https://github.com/hpcaitech/ColossalAI/issues/2191
ColossalAI,这个issue是用户提出的需求类型，涉及的主要对象是ColossalAI库中的cpu_optim和fused_optim功能。原因是用户希望能够自由地使用这些功能而不用担心其是否在运行时构建。,https://github.com/hpcaitech/ColossalAI/issues/2190
ColossalAI,这个issue类型是技术需求，涉及使用runtime builder来优化融合优化器。,https://github.com/hpcaitech/ColossalAI/issues/2189
ColossalAI,"这个issue属于用户提出需求类型，该问题单涉及的主要对象是名为""DreamBlooth""的功能。",https://github.com/hpcaitech/ColossalAI/issues/2188
ColossalAI,这是一个用户提交的需求，涉及ColossalAI的构建过程中针对CPU Adam和融合优化器的建议操作。,https://github.com/hpcaitech/ColossalAI/issues/2187
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中支持稳定扩散的量化推断问题，发起者正在寻求支持量化推断的功能。,https://github.com/hpcaitech/ColossalAI/issues/2186
ColossalAI,这个issue属于功能需求，涉及的主要对象是ColossalAI中的HyrbidAdam优化器构建器。由于现在可以在运行时构建一些内核，因此现在HyrbidAdam可以在运行时构建，无需安装cuda扩展。,https://github.com/hpcaitech/ColossalAI/issues/2184
ColossalAI,这个issue为用户提出需求类型，主要涉及的对象是GPT演示的更高准确性和TFLOPS。由于当前演示的准确性和TFLOPS性能不理想，用户希望改进和提高这两方面的表现。,https://github.com/hpcaitech/ColossalAI/issues/2178
ColossalAI,"这是一个用户提出需求的issue，涉及主要对象是""gpt_performance""模块。由于性能方面的需求或者问题，用户可能正在寻求优化建议或解决方案。",https://github.com/hpcaitech/ColossalAI/issues/2177
ColossalAI,这是一个用户提出需求类型的issue，主要涉及ColossalAI下的代码库的Pytorch版本添加问题。,https://github.com/hpcaitech/ColossalAI/issues/2172
ColossalAI,这是一个用户提出需求的类型，主要对象是Palm PyTorch新版本。,https://github.com/hpcaitech/ColossalAI/issues/2168
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI示例的新增。可能由于用户需要示例中加入PaLM模型而提出。,https://github.com/hpcaitech/ColossalAI/issues/2165
ColossalAI,该issue类型为功能需求，涉及主要对象为MetaInfo类，用户提出了关于在MetaInfo类中附加输入、缓冲区和输出张量的需求。,https://github.com/hpcaitech/ColossalAI/issues/2162
ColossalAI,这个issue类型是用户提出需求，该问题单涉及的主要对象是在ColossalAI中缺失的ViT相关功能。,https://github.com/hpcaitech/ColossalAI/issues/2154
ColossalAI,这是一个需求类型的issue，主要涉及GeminiDPP转换为PyTorch Module，可能是为了增强模型性能或者提供更好的接口给用户。,https://github.com/hpcaitech/ColossalAI/issues/2151
ColossalAI,这个issue属于代码优化类型，涉及主要对象是ColossalAI中的图节点代码。由于存在无用的图节点代码，需要移除以提升代码质量和效率。,https://github.com/hpcaitech/ColossalAI/issues/2150
ColossalAI,这个issue类型是功能需求提出，主要涉及ColossalAI中的LazyInit功能。造成该需求提出的原因可能是之前版本中的LazyInit功能存在问题或者需要改进。,https://github.com/hpcaitech/ColossalAI/issues/2148
ColossalAI,这是一个改进请求，涉及更新ColossalAI中的coloinit_ctx和dreambooth模块。由于需要支持meta_tensor和在dreambooth中使用，需要修改相关函数以实现这些功能。,https://github.com/hpcaitech/ColossalAI/issues/2147
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象为GPT示例。由于缺少具体的描述或内容，无法确定用户想要添加的zero1和zero2示例的具体内容或用途。,https://github.com/hpcaitech/ColossalAI/issues/2146
ColossalAI,该问题类型为功能增强/新功能需求，主要涉及ColossalAI项目中的内存估算和形状一致性管理器。,https://github.com/hpcaitech/ColossalAI/issues/2144
ColossalAI,这是一个请求将版本更新到v0.1.13的问题类型为用户提出需求，主要对象是ColossalAI。,https://github.com/hpcaitech/ColossalAI/issues/2139
ColossalAI,这个issue类型是功能需求，主要涉及的对象是ColossalAI库中的子模块引用同步，用户希望实现自动化更新子模块提交的功能。,https://github.com/hpcaitech/ColossalAI/issues/2136
ColossalAI,这是一个用户需求类型的issue，涉及将所有GPT相关测试整合到一个文件夹中，主要目的是整理和优化测试文件的结构。,https://github.com/hpcaitech/ColossalAI/issues/2134
ColossalAI,该issue类型为新功能提议，涉及主要对象为ColossalAI框架的attention kernels接口。该issue是由于需求增加，希望更新ColossalAI中的attention kernels，以支持来自xformers的新接口。,https://github.com/hpcaitech/ColossalAI/issues/2133
ColossalAI,这个issue类型为功能提升，主要涉及softmax handler的实现，用户提出了关于生成softmax策略的需求。,https://github.com/hpcaitech/ColossalAI/issues/2132
ColossalAI,这是一个类型为特性请求的issue，主要涉及到同步子模块引用，是为了自动化提交请求更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/2131
ColossalAI,这个issue类型是功能需求，涉及的主要对象是auto parallel系统中的tensor处理。由于不同设备上的sharding_spec可能导致tensor大小需转换，并且slice对象涉及节点类型参数的问题，用户希望实现一个运行时准备pass以管理大小值转换。,https://github.com/hpcaitech/ColossalAI/issues/2130
ColossalAI,这个issue类型是用户提出需求，主要涉及Gemini项目中的chunkmemstatscollector API更新。由于之前记录的cpu mem statistics未被使用，而cuda模型数据和总体cuda使用不需要以列表形式存储，只需记录最新值，因为它们用于计算非模型数据。,https://github.com/hpcaitech/ColossalAI/issues/2129
ColossalAI,这是关于对Gemini项目中运行时内存追踪器中非模型数据记录方法的更新的问题，类型为功能需求。该问题涉及主要对象为Gemini项目的运行时内存追踪器。,https://github.com/hpcaitech/ColossalAI/issues/2128
ColossalAI,这是一个需求提出的issue，涉及Gemini模块中非模型数据计算方法的更新。,https://github.com/hpcaitech/ColossalAI/issues/2126
ColossalAI,这是一个功能改进类型的issue，主要涉及Memstats在维护preop timesteps和params之间的映射。这个问题可能由于需要准确记录和管理preop timesteps和params之间的关联而被提出。,https://github.com/hpcaitech/ColossalAI/issues/2124
ColossalAI,该issue属于功能需求，主要涉及代码库中子模块引用的同步问题，用户需要自动生成PR以更新子模块的提交。,https://github.com/hpcaitech/ColossalAI/issues/2123
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的支持稳定性扩散v2，用户希望从https://github.com/Fazziekey/stablediffusion更新到v2版本。,https://github.com/hpcaitech/ColossalAI/issues/2120
ColossalAI,这是一个特性改进的issue，主要涉及ColossalAI库中的Chunk Manager API，用户希望更新部分变量名以及修改某些方法名称。,https://github.com/hpcaitech/ColossalAI/issues/2119
ColossalAI,这是一个用户提出需求的issue，主要涉及优化器的功能调整，新增了`div_scale`参数。,https://github.com/hpcaitech/ColossalAI/issues/2117
ColossalAI,这是一个代码质量优化类的issue，主要涉及到ColossalAI项目中的Chunk类。由于注释不够规范或详细，导致开发者需要进一步完善注释以提高代码可读性和可维护性。,https://github.com/hpcaitech/ColossalAI/issues/2116
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中的ZeRO，用户提出添加L2梯度裁剪功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/2112
ColossalAI,该issue类型为功能需求，主要涉及PP Middleware。由于缺少具体内容描述，用户需要添加bwd和step到PP middleware中。,https://github.com/hpcaitech/ColossalAI/issues/2111
ColossalAI,这是一个功能需求类型的issue，主要涉及Gemini模块中的参数控制和张量映射。,https://github.com/hpcaitech/ColossalAI/issues/2110
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及的对象是gemini项目。由于之前在初始化chunk时使用参数定义顺序，而实际运行时参数访问顺序不同，导致内存碎片化，用户希望记录运行时的参数访问顺序来改进初始化方法。,https://github.com/hpcaitech/ColossalAI/issues/2108
ColossalAI,这个issue类型是功能需求提出，主要涉及 ColossalAI 中的自动并行处理功能，用户提出要支持线性函数偏置添加。,https://github.com/hpcaitech/ColossalAI/issues/2104
ColossalAI,这个issue类型是版本更新，主要对象是ColossalAI。由于软件版本从0.1.11rc5更新到0.1.12，用户可能提出了关于更新内容或版本迁移的问题。,https://github.com/hpcaitech/ColossalAI/issues/2103
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI中的autoparallel模块，该issue提供了对torch.sum和torch.Tensor.sum的sum handler的实现，主要是为了支持torch.addbmm等需要在批处理维度进行求和的计算，并提供了相应的测试用例。,https://github.com/hpcaitech/ColossalAI/issues/2101
ColossalAI,这是一个关于Gemini使用运行时内存追踪器（RMT）的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/2099
ColossalAI,这个issue是一个功能需求，主要涉及ColossalAI中的自动并行计算功能的实现。,https://github.com/hpcaitech/ColossalAI/issues/2098
ColossalAI,这个issue类型是用户提出需求，更新GPT的README，主要对象是ColossalAI项目，用户希望更新GPT项目的README说明。,https://github.com/hpcaitech/ColossalAI/issues/2095
ColossalAI,该issue类型为代码优化，主要涉及Gemini项目中的全局模型数据追踪器。由于该追踪器的作用被移除，可能由于代码结构调整或者性能优化导致。,https://github.com/hpcaitech/ColossalAI/issues/2091
ColossalAI,这是一个功能需求的issue，主要涉及embedding handler的添加，问题出现的原因是需要在运行时准备环节中执行更多操作才能处理num_embeddings这种情况。,https://github.com/hpcaitech/ColossalAI/issues/2089
ColossalAI,这是一个用户提出需求的issue，主要涉及Gemini项目中的Runtime Memory Tracer功能。由于需要将跟踪结果放入MemStats，以便Gemini管理器能够读取，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/2088
ColossalAI,这个issue是一个功能测试需求，主要涉及ColossalAI中的Algebraic Language Model以及其他模型的测试。由于作者希望测试Algebraic Language Model及其他模型在实际应用中的表现，因此提出了这个测试需求。,https://github.com/hpcaitech/ColossalAI/issues/2086
ColossalAI,这是一个功能需求提议，主要涉及到将MemStats与Collector分离，以便使用一致的MemStats实例存储多个不同收集器的跟踪结果。,https://github.com/hpcaitech/ColossalAI/issues/2084
ColossalAI,这是一个功能增强（feature enhancement）类型的issue，主要涉及到ColossalAI中的tensor构造函数处理器（TensorConstructorHandler），由于复制策略只能创建本地的张量而无法创建分布式张量。,https://github.com/hpcaitech/ColossalAI/issues/2082
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是Gemini模型测试以及添加Albert模型。,https://github.com/hpcaitech/ColossalAI/issues/2075
ColossalAI,这是一个技术改进类型的issue，主要对象涉及Gemini工具中未使用的MemtracerWrapper，表明需要移除未使用的部分。,https://github.com/hpcaitech/ColossalAI/issues/2072
ColossalAI,这是一个关于代码改进的issue，主要涉及的对象是`F.conv` metainfo。由于之前的PR，可能出现了一些关于自动并行性和线性metainfo的小bug，所以这个issue主要是修复bug和调整测试。,https://github.com/hpcaitech/ColossalAI/issues/2069
ColossalAI,这是一个用户需求类型的issue，主要对象是ColossalAI是否支持基于图像条件而不是文本条件来训练条件模型。,https://github.com/hpcaitech/ColossalAI/issues/2068
ColossalAI,这是一个用户提出需求的类型issue，主要涉及ColossalAI是否支持t5或mt5模型训练，用户询问是否可使用自己的数据集进行这些模型的训练。,https://github.com/hpcaitech/ColossalAI/issues/2063
ColossalAI,该issue为代码优化类型，涉及的主要对象是ColossalAI中的autoparallel模块。,https://github.com/hpcaitech/ColossalAI/issues/2062
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI项目中的示例更新。,https://github.com/hpcaitech/ColossalAI/issues/2061
ColossalAI,该issue类型为功能需求提出，主要涉及的对象是ColossalAI中的PP middleware。这个需求是为了提高代码的可读性和可维护性而提出的。,https://github.com/hpcaitech/ColossalAI/issues/2059
ColossalAI,这是一个功能改进的issue，主要涉及Auto Parallel模块下的二元元素级操作metainfo的补充，由于目前这个metainfo只是二元元素级操作的粗略估计，可能与实际情况不符，导致了部分内存估算不准确的情况。,https://github.com/hpcaitech/ColossalAI/issues/2058
ColossalAI,这个issue类型是发布更新，主要涉及到更新ColossalAI至版本0.1.11rc5所需要更新的内容。导致此issue的原因可能是为了修复已知问题或添加新功能。,https://github.com/hpcaitech/ColossalAI/issues/2053
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中的参数追踪功能。可能由于功能缺失或用户需要，导致用户提出了关于参数追踪的需求。,https://github.com/hpcaitech/ColossalAI/issues/2051
ColossalAI,该issue类型为用户提出需求，主要对象是Gemini模块。由于Gemini模块缺少参数，用户请求添加参数功能。,https://github.com/hpcaitech/ColossalAI/issues/2046
ColossalAI,这是一个用户提出需求的类型，主要对象是代码中的模型名称。,https://github.com/hpcaitech/ColossalAI/issues/2045
ColossalAI,这是一个用户提出需求的类型问题，该问题涉及如何使用图像作为条件来训练一个基于ldm的有条件模型。,https://github.com/hpcaitech/ColossalAI/issues/2041
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中多个模型，用户提出了需要通过tensor.storage来自由分配和释放cuda内存，并且添加梯度钩子的需求。,https://github.com/hpcaitech/ColossalAI/issues/2040
ColossalAI,这是一个用户提出需求的issue，涉及的主要对象是ColossalAI下的autoparallel模块，旨在更新Solver类以适应最新接口，更新相关的工具函数并上传自注意力测试文件。,https://github.com/hpcaitech/ColossalAI/issues/2037
ColossalAI,这是一个需求类型的issue，主要涉及到ColossalAI中的RPC功能。这个问题可能是由于需要将RPC与DAG进行拆分，以支持用于旧RPC PP的CTS，同时也涉及到ResNet图像和GPT图像。,https://github.com/hpcaitech/ColossalAI/issues/2028
ColossalAI,这个issue类型为功能请求， 主要对象为ColossalAI库中的分区划分方法。由于DAG信息分割不准确，导致需要对分区划分方法进行改进。,https://github.com/hpcaitech/ColossalAI/issues/2025
ColossalAI,这是一个改进建议（feature request）类型的问题，涉及的主要对象是GitHub上的ColossalAI项目。这个问题由于需要在模板中添加关于机构信息的提示，为了提高用户体验，以及更好的组织和协调工作。,https://github.com/hpcaitech/ColossalAI/issues/2023
ColossalAI,这是一个需求类型的issue，主要涉及到PyPI发布工作流程，由于CUDA扩展安装问题，用户不再使用PyPI并移至私有pip源。,https://github.com/hpcaitech/ColossalAI/issues/2022
ColossalAI,这是一个用户提出需求类别的issue，主要涉及Param hook。由于缺乏具体内容，用户可能在使用Param hook时遇到问题或者需要更多指导和支持。,https://github.com/hpcaitech/ColossalAI/issues/2019
ColossalAI,这个issue是一个功能需求类型，主要涉及的对象是ColossalAI中的autoparallel模块。由于强制将输入转换为完全复制状态，导致在重塑操作中进行不必要的额外通信。,https://github.com/hpcaitech/ColossalAI/issues/2011
ColossalAI,这是一个需求类型的issue，主要涉及Gemini项目的测试模块和参数操作的单元测试，请求添加特定功能和完善相关测试。,https://github.com/hpcaitech/ColossalAI/issues/2004
ColossalAI,这是一个功能需求的issue，主要涉及Gemini库支持ColoTensor的orch.add_函数。这个issue提到Gemini目前还不能完全支持resnet18，作者仍在继续努力工作中。,https://github.com/hpcaitech/ColossalAI/issues/2003
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColoTracer工具的新版本功能更新。由于新版本尚未支持torchaudio等模块，用户提出了期待支持这些模块的需求。,https://github.com/hpcaitech/ColossalAI/issues/2002
ColossalAI,这是一个用户提出需求类型的issue，主要涉及到ColoTracer的新版本支持的模型/库列表，针对尚未支持的torchaudio模型的补充。,https://github.com/hpcaitech/ColossalAI/issues/2001
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI中MetaTensor执行过程中的元数据注册项，希望添加更多元素来进行注册。,https://github.com/hpcaitech/ColossalAI/issues/2000
ColossalAI,这是一个关于优化性能和提高系统健壮性的问题，涉及到ColossalAI中的自动并行处理功能。,https://github.com/hpcaitech/ColossalAI/issues/1989
ColossalAI,该issue类型是功能改进，涉及对象为`torch.nn.functional.linear`的metainfo支持。造成这个问题的原因是为了保持与`torch.nn.Linear`相一致的内存估计结果，同时为未来对带偏置线性层的支持做准备。,https://github.com/hpcaitech/ColossalAI/issues/1987
ColossalAI,这个issue类型是用户提出需求，主要涉及ColossalAI的训练过程中如何使用多个节点进行数据和模型训练。,https://github.com/hpcaitech/ColossalAI/issues/1983
ColossalAI,"这个issue是关于新增""Bert for MemtracerWrapper""的功能，类型为需求提出，主要对象为Gemini项目。",https://github.com/hpcaitech/ColossalAI/issues/1982
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是在ColossalAI中添加RoBERTa模型。由于用户希望在ColossalAI中使用RoBERTa模型，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/1980
ColossalAI,这个issue为需求提出类型，主要对象是向ColossalAI项目添加RoBERTa模型。,https://github.com/hpcaitech/ColossalAI/issues/1979
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中的autoparallel模块，提出了一个新的转换操作mixgather。提出此需求是为了降低通信成本，通过新的操作方式来优化计算过程。,https://github.com/hpcaitech/ColossalAI/issues/1977
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI项目。用户希望支持在ColossalAI中进行使用diffusers进行训练dreambooth和文本反演，以便与已有的Stable diffusion脚本相结合。,https://github.com/hpcaitech/ColossalAI/issues/1976
ColossalAI,这是一个需求类型的issue，主要涉及Gemini独立运行时跟踪器。,https://github.com/hpcaitech/ColossalAI/issues/1974
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的自动并行处理（autoparallel）功能，用户想要添加获取$\alpha$和$\beta$的功能。,https://github.com/hpcaitech/ColossalAI/issues/1973
ColossalAI,这个issue看起来是关于代码优化或者问题修复，主要涉及Gemini项目中的未使用的MemTraceOp清理工作。原因可能是由于代码冗余或者性能优化需求导致的。,https://github.com/hpcaitech/ColossalAI/issues/1970
ColossalAI,这个issue类型是技术性改进（类似于优化性能或代码维护），该问题单涉及的主要对象是ColossalAI代码库。原因是移除不必要文件_mem_tracer_hook.py导致的问题。,https://github.com/hpcaitech/ColossalAI/issues/1963
ColossalAI,这是一个需求建议类型的issue，主要涉及的对象是ColossalAI代码库中的MemStatsCollector相关类。由于命名问题导致了混乱，需要将类的命名规范性进行整理。,https://github.com/hpcaitech/ColossalAI/issues/1962
ColossalAI,这是一个功能增强的issue，主要涉及torch.addmm函数的支持情况。导致这个issue的原因是需要支持addmm操作在symbolic tracer中的使用。,https://github.com/hpcaitech/ColossalAI/issues/1961
ColossalAI,这是一个用户提出需求的issue，主要对象是Gemini Optimizer。由于Gemini Optimizer原始版本过于复杂，用户希望添加GeminiAdamOptimizer以简化操作。,https://github.com/hpcaitech/ColossalAI/issues/1960
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及到ColossalAI下的GPT demo。由于原功能的不足或用户需要更多功能/改进，用户想要增强GPT演示。,https://github.com/hpcaitech/ColossalAI/issues/1959
ColossalAI,这是一个用户提出需求的 issue，主要涉及到更新README中关于模型下载的内容。原因可能是现有的模型下载说明有误或者需要更新。,https://github.com/hpcaitech/ColossalAI/issues/1958
ColossalAI,这个issue类型是需求提出，主要涉及的对象是对ColossalAI仓库中子模块引用的同步更新。这个问题是由于需要自动化地通过PR来更新子模块的提交引起的。,https://github.com/hpcaitech/ColossalAI/issues/1957
ColossalAI,这个issue是请求更新ColossalAI中的lightning版本，属于用户提出需求类型，涉及到稳定扩散功能。,https://github.com/hpcaitech/ColossalAI/issues/1954
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的教程增加快速入门指南。原因可能是用户需要更快速入门ColossalAI。,https://github.com/hpcaitech/ColossalAI/issues/1946
ColossalAI,这是一个用户提出需求的类型，主要对象是ColossalAI中的example。由于缺少具体内容，用户在该issue中提出需求添加一个 Vit 相关的示例设置。,https://github.com/hpcaitech/ColossalAI/issues/1942
ColossalAI,这是一个用户提出需求类型的issue，主要对象是ColossalAI中的spmd README文件。由于缺少相关的requirements，用户请求将requirements添加到spmd README中。,https://github.com/hpcaitech/ColossalAI/issues/1941
ColossalAI,这是一个功能需求issue，主要涉及ColossalAI中的kernel模块，新增了MaskedFlashAttention API，用户希望拥有更方便的使用方式。,https://github.com/hpcaitech/ColossalAI/issues/1940
ColossalAI,这个issue类型是功能增强提议，主要对象是ColoTensor的参数初始化过程。,https://github.com/hpcaitech/ColossalAI/issues/1937
ColossalAI,这个issue为需求类型，主要涉及ColossalAI与StableDiffusion Dreambooth的集成，用户希望具有新概念的模型训练示例。,https://github.com/hpcaitech/ColossalAI/issues/1935
ColossalAI,这个issue是一个版本更新的请求，涉及的主要对象是软件项目ColossalAI。原因是要求更新软件版本，可能是为了修复bug、增加新功能或提升性能。,https://github.com/hpcaitech/ColossalAI/issues/1931
ColossalAI,这是一个用户提出需求类型的issue，主要是关于优化README和OPT文件。由于现有文件可能存在不完善或混乱的问题，用户希望对其进行改进和优化。,https://github.com/hpcaitech/ColossalAI/issues/1930
ColossalAI,该issue是关于教程更新的，属于用户提出需求类型，主要涉及ColossalAI项目下的hybrid parallel部分。,https://github.com/hpcaitech/ColossalAI/issues/1928
ColossalAI,这个issue是用户提出需求类型，主要涉及到ColossalAI项目中的教程（tutorial），由于缺少具体内容，用户可能提出了添加关于sequence parallel模型的合成数据的请求。,https://github.com/hpcaitech/ColossalAI/issues/1927
ColossalAI,这个issue类型是用户提出需求，主要对象是教程（tutorial）。由于缺乏合成数据集，用户请求添加用于优化的合成数据集。,https://github.com/hpcaitech/ColossalAI/issues/1924
ColossalAI,这是一个用户提交的关于教程补充的请求，主要对象是ColossalAI，并请求添加关于混合并行的合成数据。,https://github.com/hpcaitech/ColossalAI/issues/1921
ColossalAI,该issue类型是功能改进，涉及主要对象是ColossalAI下的auto activation checkpoint benchmark。由于修改后的benchmark更加简洁，可能是为了提高代码可读性或执行效率。,https://github.com/hpcaitech/ColossalAI/issues/1920
ColossalAI,这个issue类型是文档改进请求，涉及的主要对象是教程（tutorial）。由于可能缺少关于新增合成数据的详细说明，用户请求添加合成数据的部分来帮助更好地了解混合并行任务的实现方式。,https://github.com/hpcaitech/ColossalAI/issues/1919
ColossalAI,该issue属于用户提出需求类型，主要对象是给ColossalAI添加了一个用于自动并行演示的合成数据集。,https://github.com/hpcaitech/ColossalAI/issues/1918
ColossalAI,这是一个更新教程示例以反映最新数据路径的问题，涉及主要对象为ColossalAI。这个问题可能是由于数据路径更改导致示例代码不再有效而产生的。,https://github.com/hpcaitech/ColossalAI/issues/1917
ColossalAI,该issue类型为新功能请求，主要涉及数据脚本和更新README。由于需要新增数据脚本和更新README文档，用户请求将这些内容添加到项目中。,https://github.com/hpcaitech/ColossalAI/issues/1916
ColossalAI,这是一个更新类型的issue，涉及到ColossalAI下的文件路径和标签的调整。,https://github.com/hpcaitech/ColossalAI/issues/1910
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI项目下的tutorial。可能是用户想要在CIFAR-10数据集上应用Diffusion模型，但目前还未有相关教程，希望添加相关内容。,https://github.com/hpcaitech/ColossalAI/issues/1907
ColossalAI,这个issue类型是功能需求提出，主要涉及 ColossalAI 库中的自动并行功能，用户提出了希望支持分布式数据加载器选项的需求。,https://github.com/hpcaitech/ColossalAI/issues/1906
ColossalAI,这是一个代码风格优化的issue，涉及到ColossalAI项目中amp/naive_amp模块的代码。,https://github.com/hpcaitech/ColossalAI/issues/1905
ColossalAI,这是一个归类为代码改进的issue，主要对象是ColossalAI项目中的教程示例，由于之前的更新导致示例目录中出现了重复的例子，需要进行修复。,https://github.com/hpcaitech/ColossalAI/issues/1903
ColossalAI,这是一个功能需求类型的issue，主要涉及到为ColossalAI项目添加CIFAR-10数据集。原因可能是为了丰富模型训练数据集的选择。,https://github.com/hpcaitech/ColossalAI/issues/1902
ColossalAI,这个issue类型为文档更新请求，主要对象是ColossalAI项目。由于内容为空，用户可能希望添加最新的信息或变化到文档中。,https://github.com/hpcaitech/ColossalAI/issues/1901
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的教程更新，用户可能是寻求更多实践例子来学习。,https://github.com/hpcaitech/ColossalAI/issues/1896
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是autoparallel模块。由于需要对给定的一维设备列表进行$\alpha$和$\beta$的估计，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/1895
ColossalAI,该issue类型为功能增强提议，主要涉及ColossalAI中的自动并行处理功能，并包含了去除冗余通信节点和计算重分片成本两方面的改进。,https://github.com/hpcaitech/ColossalAI/issues/1893
ColossalAI,这是一个用户提出需求的issue，主要涉及GPT2测试及脚本重构。由于之前的问题未解决，作者进行了修复并添加了新的测试内容。,https://github.com/hpcaitech/ColossalAI/issues/1889
ColossalAI,这个issue类型是功能需求添加，主要对象是自动检查点功能。由于缺乏GPT示例以及需要完善README，用户提交了功能添加的需求。,https://github.com/hpcaitech/ColossalAI/issues/1880
ColossalAI,这是一个用户需求类型的 issue，主要涉及到 ColossalAI 库中的 CheckpointSolver API 设计。这个问题可能是由于之前的 API 参数不够用户友好，导致用户在使用时可能遇到困难或不方便。,https://github.com/hpcaitech/ColossalAI/issues/1879
ColossalAI,这是一个提出需求的issue，主要涉及的对象是ColossalAI库中的ZeRO1和ZeRO2 optimizer。由于新增的ZeRO1和ZeRO2优化器，需要修正`clip_grad_norm`与模型和管道并行性相匹配，并进行训练效率测试。,https://github.com/hpcaitech/ColossalAI/issues/1878
ColossalAI,这是一个特性需求的issue，主要涉及Linear 1D Row的权重共享和计算方式的优化。,https://github.com/hpcaitech/ColossalAI/issues/1874
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的`metainfo_trace` API。由于无法设计一个好的函数名称，需要讨论添加一个简单的API。,https://github.com/hpcaitech/ColossalAI/issues/1873
ColossalAI,该问题类型为需求提出，涉及主要对象为ColossalAI中的手动示例迁移(handson examples migration)。由于需要将教程的手动示例迁移，以及对相关内容进行修改，故该需求提出。,https://github.com/hpcaitech/ColossalAI/issues/1871
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是`torch.nn.ReLU`模块。原因是为了实现`MetaInfo`功能而进行修改。,https://github.com/hpcaitech/ColossalAI/issues/1868
ColossalAI,这个issue是一则需求类型，涉及的主要对象是ColossalAI的配置文件。由于配置文件中存在冗余信息，需要进行清理操作。,https://github.com/hpcaitech/ColossalAI/issues/1866
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI下的初始化教程。由于缺少具体内容，用户提出了关于初始化教程的问题或寻求相关帮助。,https://github.com/hpcaitech/ColossalAI/issues/1865
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是 ColossalAI 中的 torch amp 模块。这个issue由于需要对比 torch amp 和 apex amp，可能是为了测试性能或功能差异。,https://github.com/hpcaitech/ColossalAI/issues/1860
ColossalAI,这是一个需求反馈类型的issue，主要涉及ColossalAI库中关于在CUDA上测试完整工作流程。由于测试不通过，用户提出了需要调查修复的问题。,https://github.com/hpcaitech/ColossalAI/issues/1859
ColossalAI,这是一个关于代码优化的issue，主要对象是`generate_release_draft.py`脚本。可能是因为代码需要进一步优化，导致了这个issue的提出。,https://github.com/hpcaitech/ColossalAI/issues/1855
ColossalAI,该issue类型为代码风格优化，主要对象是ColossalAI中的workflows模块。,https://github.com/hpcaitech/ColossalAI/issues/1854
ColossalAI,这是一个代码风格优化（code style）类别的issue，涉及到ColossalAI下的amp/apex_amp/__init__.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1853
ColossalAI,这是一个改进代码风格的issue，主要涉及ColossalAI项目中的发布流程文件，需要进行代码风格优化。,https://github.com/hpcaitech/ColossalAI/issues/1851
ColossalAI,"这是一个用户提出需求的issue，主要涉及ColossalAI网站首页的""Get Started""按钮指向错误页面的问题，导致新用户无法顺利开始使用ColossalAI。",https://github.com/hpcaitech/ColossalAI/issues/1850
ColossalAI,这是一个功能增强（enhancement）类型的issue，主要涉及了tensorparallel中的非tp linear操作，添加了skip_bias_add选项，并更新了相关文件格式。,https://github.com/hpcaitech/ColossalAI/issues/1848
ColossalAI,这是一个用户提出需求的issue，主要对象是文档添加版本解释，由于缺少具体内容导致无法确定涉及的主要版本和详细说明。,https://github.com/hpcaitech/ColossalAI/issues/1847
ColossalAI,这个issue属于代码风格优化，涉及到ColossalAI的代码格式需求，用户希望统一代码格式以提高代码可读性。,https://github.com/hpcaitech/ColossalAI/issues/1846
ColossalAI,这个issue类型是代码风格优化（code style）的修复，主要涉及ColossalAI的auto_parallel模块中的operator_handler.py文件。由于代码风格不符合规范，需要进行调整。,https://github.com/hpcaitech/ColossalAI/issues/1845
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及ColossalAI下的diffusion模块的readme文档。由于readme文档的内容需要完善和改进，用户提出了需要对readme文档进行润色的建议。,https://github.com/hpcaitech/ColossalAI/issues/1842
ColossalAI,该issue类型为文档修改需求，主要对象是ColossalAI中Diffusion模块的README文件。,https://github.com/hpcaitech/ColossalAI/issues/1840
ColossalAI,这是一个代码风格优化（Code style）的Issue，主要涉及ColossalAI项目中的github workflow文件，请求整理其代码风格。,https://github.com/hpcaitech/ColossalAI/issues/1837
ColossalAI,这是一个代码风格优化（polish）的issue，主要对象是ColossalAI中的./colossalai/amp/torch_amp/__init__.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1836
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI项目中torch_amp模块的代码风格是否需要改进的讨论。,https://github.com/hpcaitech/ColossalAI/issues/1835
ColossalAI,这是一个版本更新的Issue，涉及的主要对象是ColossalAI库。,https://github.com/hpcaitech/ColossalAI/issues/1832
ColossalAI,这是一个关于更新gitignore文件移除DS_Store的issue，类型为优化建议，主要对象是项目版本控制中需要忽略的文件。可能是为了优化代码仓库的整洁性，避免不必要的文件被提交到版本控制系统中导致的问题。,https://github.com/hpcaitech/ColossalAI/issues/1830
ColossalAI,这个issue类型是代码风格优化，涉及的主要对象是ColossalAI中的auto_parallel/tensor_shard/deprecated/conv_handler.py文件。由于该文件代码风格不符合规范，用户提出了需要对其进行优化的请求。,https://github.com/hpcaitech/ColossalAI/issues/1829
ColossalAI,该问题是关于需求新增的issue，涉及主要对象是ColossalAI项目中的示例（example）。,https://github.com/hpcaitech/ColossalAI/issues/1828
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的README文件。,https://github.com/hpcaitech/ColossalAI/issues/1827
ColossalAI,这个issue类型是改进请求，主要对象是ColossalAI下的GPT2 huggingface example。,https://github.com/hpcaitech/ColossalAI/issues/1826
ColossalAI,这是一个用户提出需求类型的issue，主要涉及的对象是ColossalAI项目。由于缺乏Colossal Diffuser代码，用户希望添加Colossal Diffuser代码到项目中。,https://github.com/hpcaitech/ColossalAI/issues/1825
ColossalAI,这是一个代码风格优化类型的issue，涉及主要对象是ColossalAI下的torch_amp/_grad_scaler.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1823
ColossalAI,该issue类型为用户提出需求，主要涉及Gemini工具的简化使用。原因可能是用户认为当前Gemini使用复杂，希望能够更简单地使用该工具。,https://github.com/hpcaitech/ColossalAI/issues/1821
ColossalAI,这是一条代码风格优化的Issue，主要涉及到ColossalAI库中的_fp16_optimizer.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1819
ColossalAI,这个issue类型为功能请求，涉及ColossalAI项目中ColoDiffusion代码的添加，用户可能希望将特定功能或模块集成到项目中。,https://github.com/hpcaitech/ColossalAI/issues/1817
ColossalAI,这是一个功能更新的issue，涉及的主要对象是autoparallel模块中的batchnorm metainfo。,https://github.com/hpcaitech/ColossalAI/issues/1815
ColossalAI,这个issue类型是代码质量优化请求，主要涉及到ColossalAI的GPU构建流程，由于代码风格不符合规范需要进行修改。,https://github.com/hpcaitech/ColossalAI/issues/1813
ColossalAI,该issue类型为用户提出需求，涉及主要对象为ColossalAI中的symbolic_trace API，用户希望添加一个更加用户友好的API。由于当前API较为复杂，用户希望简化实现以提高易用性。,https://github.com/hpcaitech/ColossalAI/issues/1812
ColossalAI,这个issue类型是用户提出需求，希望添加GPT相关内容到ColossalAI。,https://github.com/hpcaitech/ColossalAI/issues/1810
ColossalAI,这是一个用户提出需求的issue，主要对象是在ColossalAI中添加一个opt model在language中。,https://github.com/hpcaitech/ColossalAI/issues/1809
ColossalAI,"这是一个用户提出需求的issue，主要涉及ColossalAI中添加一个名为""symbolic_trace""的API。由于当前API并不太用户友好，用户期望创建一个更简单易用的API。",https://github.com/hpcaitech/ColossalAI/issues/1807
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI项目中的 strategies_constructor.py 文件。,https://github.com/hpcaitech/ColossalAI/issues/1806
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI的example。由于功能缺失，用户希望在示例中添加扩散。,https://github.com/hpcaitech/ColossalAI/issues/1805
ColossalAI,这是一个功能需求类型的issue，主要涉及到ColossalAI中的flash attention接口。导致用户提出该需求的原因可能是为了增加更加灵活的接口以满足特定的应用需求。,https://github.com/hpcaitech/ColossalAI/issues/1804
ColossalAI,这个issue类型是技术提升建议，主要涉及ColossalAI库中的类型提示，用户可能在代码维护或协作过程中希望进一步提升代码的可读性。,https://github.com/hpcaitech/ColossalAI/issues/1801
ColossalAI,这个issue类型是功能改进（Enhancement），该问题单涉及的主要对象是ColossalAI中的两个模块，即flash_attn和triton， 原因是当这两个模块不可用时，需要跳过相关的测试。,https://github.com/hpcaitech/ColossalAI/issues/1798
ColossalAI,这是一个类型为需求提出的issue，主要对象是ColossalAI项目中的子模块引用，用户提出了自动化更新子模块提交的需求。,https://github.com/hpcaitech/ColossalAI/issues/1797
ColossalAI,这是一则关于技术改进的issue，主要涉及到ColossalAI中的自动并行系统中卷积操作的元信息类。由于与实时测试结果的不匹配，提出了修改测试代码的需求。,https://github.com/hpcaitech/ColossalAI/issues/1796
ColossalAI,这个issue类型是功能改进，主要涉及核心融合（kernelfusion）功能，并导致了需要添加JIT预热和更新1D的layernorm核心。,https://github.com/hpcaitech/ColossalAI/issues/1792
ColossalAI,这是一个功能需求的issue，主要涉及alpha-beta估计，由于框架的问题导致潜在的延迟方差较大。,https://github.com/hpcaitech/ColossalAI/issues/1791
ColossalAI,这是一个需求提出的issue，主要涉及ColossalAI中的`CheckpointSolverRotor().solve()`方法的重新设计和参数调整。,https://github.com/hpcaitech/ColossalAI/issues/1789
ColossalAI,这个issue属于功能需求类型，主要涉及到子模块引用同步，用户需求为自动化PR来更新子模块的提交。,https://github.com/hpcaitech/ColossalAI/issues/1785
ColossalAI,这个issue类型是功能需求报告，涉及的主要对象是Pipeline。由于缺少flatten，导致exec_seq为None时出现问题，需要添加flatten来解决这一问题。,https://github.com/hpcaitech/ColossalAI/issues/1784
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的自动并行系统中添加线性元信息类的功能。由于关于metainfo测量的方式导致内存测试结果不符合预期，用户希望在CI上激活测试，并对代码进行修改以解决这一问题。,https://github.com/hpcaitech/ColossalAI/issues/1783
ColossalAI,这是一个需求报告，涉及到ColossalAI中的Pipeline模块，由于Huggingface将使用自己的数据结构作为层之间的输出，需要移除对torch.Tensor的类型检查。,https://github.com/hpcaitech/ColossalAI/issues/1782
ColossalAI,该issue类型为功能需求，主要涉及自动化同步子模块引用的问题。由于需要自动化更新子模块的提交，可能是因为维护子模块引用与主项目之间的关联时出现了问题。,https://github.com/hpcaitech/ColossalAI/issues/1781
ColossalAI,这个issue是一个功能改进的类型，主要涉及ColossalAI下的autoparallel模块中的tracer模块，由于需要支持在创建代理时添加多个节点，希望优化参数列表的生成过程。,https://github.com/hpcaitech/ColossalAI/issues/1776
ColossalAI,这个issue类型为性能优化，主要涉及的对象是ColossalAI下的[tensorparallel]模块的层（layers）。,https://github.com/hpcaitech/ColossalAI/issues/1775
ColossalAI,这个issue是关于代码更新的，涉及到CommSpec和CommAction对象的更新，由于之前的CommSpec被替换为CommAction，导致需要更新所有使用CommSpec的处理函数。,https://github.com/hpcaitech/ColossalAI/issues/1768
ColossalAI,"这是一个功能需求的issue，涉及到ColossalAI中的""get_attr""操作节点，通过添加策略处理程序来实现。",https://github.com/hpcaitech/ColossalAI/issues/1767
ColossalAI,这是一个用户提出需求的类型的Issue，主要涉及到ColossalAI项目中FastFold的文档更新。,https://github.com/hpcaitech/ColossalAI/issues/1766
ColossalAI,这个issue类型是功能需求，主要对象是MemStatsCollectorStatic组件。这个需求是因为用户需要一个静态的内存统计收集器来提供内存相关的统计信息。,https://github.com/hpcaitech/ColossalAI/issues/1765
ColossalAI,这个issue是关于功能需求的，主要涉及了ColossalAI中的activation checkpoint solvers。原因可能是为了优化代码结构和数据准备。,https://github.com/hpcaitech/ColossalAI/issues/1764
ColossalAI,这是一个功能需求报告，主要涉及ColossalAI下的autoparallel问题，用户希望添加`torch.matmul`操作的节点处理程序来提高自注意力模块的并行性能。,https://github.com/hpcaitech/ColossalAI/issues/1763
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中添加cuda和triton flash attention的功能。,https://github.com/hpcaitech/ColossalAI/issues/1762
ColossalAI,这是一个功能需求的issue，主要涉及的对象是ColossalAI中的节点策略（node strategies）。由于需要对节点策略进行数值测试，因此提出了需要添加数值测试的API。,https://github.com/hpcaitech/ColossalAI/issues/1760
ColossalAI,这是一个功能增强的issue，主要涉及到ColossalAI项目中缺少处理二元元素级操作的节点处理程序，导致代码库无法处理残差连接等情况。,https://github.com/hpcaitech/ColossalAI/issues/1758
ColossalAI,这是一个特性需求，该问题涉及Runtime Pass的重构和文档添加。,https://github.com/hpcaitech/ColossalAI/issues/1757
ColossalAI,这是一个功能改进（feature enhancement）类型的issue，主要涉及到ColossalAI项目中的memory utils和shard utils模块的重构。由于代码重构导致了太多变更，开发者认为需要将这些变更分开，以提升代码清晰度与审阅者的理解。,https://github.com/hpcaitech/ColossalAI/issues/1754
ColossalAI,这个issue是一个功能增强类的PR，主要涉及torch.addbmm和torch.Tensor.addbmm的handler添加，旨在处理broadcast shape到其物理形状的转换问题。,https://github.com/hpcaitech/ColossalAI/issues/1751
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI项目中的测试文件test_3d.py。,https://github.com/hpcaitech/ColossalAI/issues/1740
ColossalAI,这是一个软件更新的需求，主要涉及到ColossalAI的版本升级。,https://github.com/hpcaitech/ColossalAI/issues/1739
ColossalAI,这个issue类型为版本更新，主要对象是ColossalAI项目。由于需要对项目ColossalAI进行更新到v0.1.11版本，故提出此issue。,https://github.com/hpcaitech/ColossalAI/issues/1736
ColossalAI,这是一个功能需求的issue，涉及到ColossalAI库中的通信操作顺序问题。,https://github.com/hpcaitech/ColossalAI/issues/1735
ColossalAI,这是一个代码风格优化的issue，主要涉及的对象是代码文件common.py。,https://github.com/hpcaitech/ColossalAI/issues/1733
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是更新推荐系统目录。,https://github.com/hpcaitech/ColossalAI/issues/1732
ColossalAI,这个issue是一个代码风格改进（Code Style）类型的反馈，涉及的主要对象是ColossalAI项目中的测试文件。由于代码风格问题，用户提出了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/1731
ColossalAI,"这个issue类型是功能需求，主要对象是zero库，由于用户希望添加一个叫做""chunk init""的函数。",https://github.com/hpcaitech/ColossalAI/issues/1729
ColossalAI,这个issue类型为代码风格优化，主要对象是ColossalAI中的`nn/metric/_utils.py`文件。由于代码风格不符合规范，用户希望对其进行优化。,https://github.com/hpcaitech/ColossalAI/issues/1727
ColossalAI,这个issue属于功能增强类型，主要涉及到项目的代码规范和代码质量。,https://github.com/hpcaitech/ColossalAI/issues/1726
ColossalAI,这个issue类型是文档更新类型，主要涉及的对象是推荐系统URLs。由于推荐系统URLs需要更新，用户提出了更新文档的需求。,https://github.com/hpcaitech/ColossalAI/issues/1725
ColossalAI,这是一个代码优化类的issue，主要对象是ColossalAI项目中的代码，用户提出了希望通过添加autoflake和isort工具在precommit中重新格式化所有代码的需求。,https://github.com/hpcaitech/ColossalAI/issues/1724
ColossalAI,这是一个关于代码风格优化的issue，主要涉及ColossalAI中的_checkpoint_hook.py文件。由于代码风格不符合规范，用户提出希望优化代码风格。,https://github.com/hpcaitech/ColossalAI/issues/1722
ColossalAI,这个issue是代码风格优化，涉及到ColossalAI库下的zero模块中sharded_param/__init__.py文件的代码风格问题。,https://github.com/hpcaitech/ColossalAI/issues/1717
ColossalAI,这是一个代码优化类的issue，涉及到ColossalAI的Neural Network模块中的lr_scheduler/linear.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1716
ColossalAI,这是一个代码风格优化的issue，其中涉及的主要对象是ColossalAI代码仓库中的check_operation_2d.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1715
ColossalAI,这个issue类型是代码风格优化，涉及主要对象是ColossalAI下的nn/metric/accuracy_2p5d.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1714
ColossalAI,这是一个改进类型的Issue，主要涉及到ColossalAI中的测试文件的移动操作。,https://github.com/hpcaitech/ColossalAI/issues/1713
ColossalAI,这是一个用户提出需求的类型，主要对象是代码风格。这可能是由于代码风格不规范而导致破坏了整体代码质量，用户希望在代码合并之前进行修改。,https://github.com/hpcaitech/ColossalAI/issues/1712
ColossalAI,这是一个用户需求类型的issue，主要对象是ColossalAI的embeddings模块。由于缺少文档在readme中的描述，用户提出了需求添加文档的问题。,https://github.com/hpcaitech/ColossalAI/issues/1711
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI项目下的优化器模块。,https://github.com/hpcaitech/ColossalAI/issues/1707
ColossalAI,这是一个优化代码结构的Issue，主要涉及ColossalAI项目下的autoparallel模块，旨在清理旧代码与新版本代码混合导致的混乱，重新组织模块结构。,https://github.com/hpcaitech/ColossalAI/issues/1706
ColossalAI,这是一个文档需求类型的Issue，主要涉及到pytest_wrapper.py的文档不完整问题，由于缺乏文档导致使用上的困惑和不便。,https://github.com/hpcaitech/ColossalAI/issues/1704
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI的自动并行处理功能。,https://github.com/hpcaitech/ColossalAI/issues/1702
ColossalAI,这是一个用户提出的功能需求issue，主要涉及到测试环境的条件控制。,https://github.com/hpcaitech/ColossalAI/issues/1701
ColossalAI,这是一个需求更名的问题，主要对象是频率感知嵌入层（FreqAwareEmbedding），原因是要将其更名为CachedEmbedding。,https://github.com/hpcaitech/ColossalAI/issues/1699
ColossalAI,这是一个用户提出需求的issue，主要涉及到支持bf16数据类型在amp和zero中的使用。,https://github.com/hpcaitech/ColossalAI/issues/1697
ColossalAI,这是一个功能需求类的issue，主要涉及的对象是 Solver、CostGraph 和 TrainCycleItem。由于最新 handler 使用了 TrainCycleItem 类型的成本数据，导致 Solver 和 CostGraph 需要进行相应的调整。,https://github.com/hpcaitech/ColossalAI/issues/1695
ColossalAI,这是一个用户提出需求的 issue，主要涉及到 embeddings 模块。该需求是为了添加更详细的计时器。,https://github.com/hpcaitech/ColossalAI/issues/1692
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的autoparallel模块。,https://github.com/hpcaitech/ColossalAI/issues/1690
ColossalAI,这是一个功能需求类型的issue，涉及主要对象是ColossalAI中的CheckpointIO模块，用户提出了统一的检查点输入/输出模块需求。,https://github.com/hpcaitech/ColossalAI/issues/1689
ColossalAI,这是一个功能改进的issue，主要涉及到ColossalAI中的where_handler_v2，因为引入了新的where_handler和重载了函数，以恢复逻辑策略到物理策略，导致一些成本计算可能会出现错误。,https://github.com/hpcaitech/ColossalAI/issues/1688
ColossalAI,这是一个功能增强类型的issue，涉及ColossalAI中自动并行计算功能的线性处理器，解决了在处理多维张量时需要将逻辑分片策略映射回其物理形状的问题。,https://github.com/hpcaitech/ColossalAI/issues/1687
ColossalAI,这是一个新增功能的issue，主要涉及ColossalAI中的 `ConcreteInfoProp` 类。,https://github.com/hpcaitech/ColossalAI/issues/1677
ColossalAI,这个issue是版本更新请求，涉及主要对象是ColossalAI软件包。原因可能是修复issue 175引起的问题。,https://github.com/hpcaitech/ColossalAI/issues/1676
ColossalAI,这个issue是一个功能需求类型，主要涉及ColossalAI项目中的自动并行处理功能。,https://github.com/hpcaitech/ColossalAI/issues/1674
ColossalAI,这是一个功能需求的issue，主要涉及了ColossalAI下的自动并行处理模块中的Node生成器和操作的处理。,https://github.com/hpcaitech/ColossalAI/issues/1673
ColossalAI,"这是一个功能需求的issue，主要涉及对象是""torch.nn.LayerNorm""。这个issue由于需要为torch.nn.LayerNorm添加处理程序而被提出。",https://github.com/hpcaitech/ColossalAI/issues/1671
ColossalAI,这是一个需求提出的issue，主要涉及到ColossalAI中关于CommSpec模块的更新。由于之前CommSpec只在ShapeConsistencyManager类中使用，现在希望将其用于autoparallel模块等其他模块，因此移动CommSpec类到一个独立文件中。,https://github.com/hpcaitech/ColossalAI/issues/1667
ColossalAI,这个issue是一个功能需求，主要涉及到ColossalAI库中的autoparallel模块，用户希望添加批量归一化处理器。,https://github.com/hpcaitech/ColossalAI/issues/1666
ColossalAI,这是一个功能需求类型的issue，涉及到broadcast operation的工具函数的添加。,https://github.com/hpcaitech/ColossalAI/issues/1665
ColossalAI,这个issue属于功能提议类型，主要对象是ColossalAI中的autoparallel模块，问题是由于之前的PR未考虑bias term的通信操作，导致了需要在matmul策略中添加对bias term的通信规格。,https://github.com/hpcaitech/ColossalAI/issues/1664
ColossalAI,这是一个用户提出需求的issue，主要目标是向ColossalAI项目中添加conv handler v2。由于原有功能的不足或者需要改进，用户提出了这个需求来扩展项目的功能。,https://github.com/hpcaitech/ColossalAI/issues/1663
ColossalAI,这是一个功能更新的issue，主要涉及的对象是ColossalAI库中的node handler。由于其他处理程序尚未完成，导致无法测试相关功能。,https://github.com/hpcaitech/ColossalAI/issues/1662
ColossalAI,该issue类型为功能需求，涉及的主要对象为新增的策略构造器模板，由于节点处理程序尚未迁移到新版本，因此尚未实现该功能。,https://github.com/hpcaitech/ColossalAI/issues/1661
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI下的rotor solver模块，由于`MetaInfoProp`消耗时间较长，用户提出添加C版本的动态规划表以优化性能。,https://github.com/hpcaitech/ColossalAI/issues/1658
ColossalAI,这是一个功能增强请求，要求对嵌入进行更详细的分析。,https://github.com/hpcaitech/ColossalAI/issues/1656
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中autoparallel模块的`torch.bmm`和`torch.Tensor.bmm`的处理程序添加。,https://github.com/hpcaitech/ColossalAI/issues/1655
ColossalAI,这个issue是一个功能需求，主要对象是嵌入（embedding）模块。由于需要输出性能分析结果，用户希望添加打印分析结果的功能。,https://github.com/hpcaitech/ColossalAI/issues/1654
ColossalAI,这个issue类型是需求提出，主要涉及对象是ColossalAI下的autoparallel solver，用户希望适应gpt。,https://github.com/hpcaitech/ColossalAI/issues/1653
ColossalAI,这是关于性能优化的问题，主要涉及到CPU和GPU之间的数据传输，提出了非阻塞的CPU-GPU拷贝。,https://github.com/hpcaitech/ColossalAI/issues/1647
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中的ZeRO实现。由于当前ZeRO实现中存在性能问题，在数据传输过程中，不同进程间的不对称分配导致一个进程阻碍其他进程，降低了ZeRO的效率。,https://github.com/hpcaitech/ColossalAI/issues/1644
ColossalAI,这个issue类型是功能需求，主要对象是ColossalAI中的pipeline/pytree模块，由于需要使用`pytree_map`替换args和kwargs的处理函数以及提供`data_process_func`接口来处理args和kwargs的结果。,https://github.com/hpcaitech/ColossalAI/issues/1642
ColossalAI,这是一个功能需求类型的issue，涉及主要对象是ColossalAI中的数据类型bf16，可能是由于当前的版本还未支持bf16数据类型与naive_amp的结合，用户希望添加对bf16的支持。,https://github.com/hpcaitech/ColossalAI/issues/1641
ColossalAI,该issue类型为功能需求，主要对象是ColossalAI中的MoE（Mixture of Experts）组。由于需要在初始化MoE组时使用ProcessGroup，用户提出了这个功能需求。,https://github.com/hpcaitech/ColossalAI/issues/1640
ColossalAI,这个issue是关于功能实现的，主要涉及Linear Projection Strategy Generator，用户提出了一个关于代码实现的需求。由于功能尚未实现，用户希望添加线性操作生成器以满足特定需求。,https://github.com/hpcaitech/ColossalAI/issues/1639
ColossalAI,这是一个功能需求类型的issue，主要对象是ColossalAI中的DotHandler V1和MLP模型，由于需要更新成本计算方法和在MLP模型中使用solver，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/1638
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的pofo sequence annotation模块。由于之前添加了pofo solver，现在完成了graph module的pofo sequence注释，希望完成整个pofo solver的流程。,https://github.com/hpcaitech/ColossalAI/issues/1637
ColossalAI,该issue类型为技术改进，主要涉及到ColossalAI中的自动并行处理逻辑。由于重复策略生成导致冗余检查，需要改进生成逻辑以避免重复，同时保持输入策略的索引用于新策略名称。,https://github.com/hpcaitech/ColossalAI/issues/1636
ColossalAI,这个issue类型是功能需求提出，该问题单涉及的主要对象是嵌入模型的缓存选项功能。,https://github.com/hpcaitech/ColossalAI/issues/1635
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中自动并行处理部分。由于缺少LayerNorm处理程序，用户请求添加该功能。,https://github.com/hpcaitech/ColossalAI/issues/1629
ColossalAI,这是一个用户提出的需求类型的issue，涉及的主要对象是ColossalAI中的ZeRO实现。这个需求源于当前版本ZeRO存在性能问题，由于不均匀分配导致一个进程读取CPU内存中的chunk内容时阻碍其他进程，从而延长了数据传输时间并降低了ZeRO的效率。,https://github.com/hpcaitech/ColossalAI/issues/1623
ColossalAI,这个issue是用户提出的需求。主要对象是autoparallel中的elementwise handler。,https://github.com/hpcaitech/ColossalAI/issues/1622
ColossalAI,这个issue为功能需求，涉及主要对象为ColossalAI编译器。由于图操作后需要重新编译，用户希望添加一个重新编译的步骤。,https://github.com/hpcaitech/ColossalAI/issues/1621
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的autoparallel模块，用户提出了添加embedding handler的需求。,https://github.com/hpcaitech/ColossalAI/issues/1620
ColossalAI,这是一个功能修改的issue，涉及ColossalAI中的offload codegen。由于添加了新的pofo solver，发现旧版本可能不满足需求，因此需要修改offload机制。,https://github.com/hpcaitech/ColossalAI/issues/1618
ColossalAI,这是一个技术需求的issue，主要涉及ColossalAI中关于张量通信的自动微分功能的使用。,https://github.com/hpcaitech/ColossalAI/issues/1617
ColossalAI,这是一个功能增强的问题单，主要涉及ColossalAI中的autoparallel模块，用户对OperatorHandler的替代实现NodeHandler进行了讨论。,https://github.com/hpcaitech/ColossalAI/issues/1612
ColossalAI,这个issue是关于代码优化提升性能的建议，主要涉及嵌入层的操作参数调整。,https://github.com/hpcaitech/ColossalAI/issues/1611
ColossalAI,这是一个功能优化的issue，主要涉及到ColossalAI中用于分片策略的数据结构重构。,https://github.com/hpcaitech/ColossalAI/issues/1610
ColossalAI,这个issue类型是功能需求，涉及主要对象为代码仓库中的子模块引用；用户希望自动生成更新子模块提交的PR。,https://github.com/hpcaitech/ColossalAI/issues/1609
ColossalAI,这是一个功能需求提交的issue，主要涉及ColossalAI中的solver添加。由于需要重设计代码生成并增加图注释，此issue还没有完成。,https://github.com/hpcaitech/ColossalAI/issues/1608
ColossalAI,这个issue是一个功能需求类型，主要对象是ColossalAI中的fx模块。它涉及到对运行时形状一致性应用的Proof of Concept。,https://github.com/hpcaitech/ColossalAI/issues/1607
ColossalAI,这是一个功能需求类型的 issue，主要涉及的对象是ColossalAI 中的 broadcast matmul 策略。由于之前的 PR 没有涵盖 torch.matmul 以及其额外的矩阵维度，导致需要 BcastOpHandler 创建 broadcast matmul 运算符的策略。,https://github.com/hpcaitech/ColossalAI/issues/1605
ColossalAI,该issue类型是功能增强，主要涉及到ColossalAI中的自动并行计算矩阵乘法操作，用户提出的需求是完善所有不需要广播的矩阵乘法策略。,https://github.com/hpcaitech/ColossalAI/issues/1603
ColossalAI,该issue类型为功能需求，主要涉及自动并行化和矩阵乘法（bmm）策略生成器。,https://github.com/hpcaitech/ColossalAI/issues/1602
ColossalAI,这个issue属于用户提出需求类型，主要涉及embedding模块的默认参数设置更新。,https://github.com/hpcaitech/ColossalAI/issues/1601
ColossalAI,这是一个需求类型的issue，主要涉及到ColossalAI中的自动并行处理功能的改进，提出需要添加bcast操作处理器。,https://github.com/hpcaitech/ColossalAI/issues/1600
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI的offload codegen功能。由于尚未实现`ColoGraphModule`的torch11版本，因此跳过了此部分的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/1598
ColossalAI,这个issue类型为功能需求，涉及主要对象为ColossalAI的子模块引用，由于需要自动化更新子模块提交而提出。,https://github.com/hpcaitech/ColossalAI/issues/1596
ColossalAI,该issue类型为需求提出，涉及主要对象为ColossalAI下的PipelineBase和Worker，由于需要重构cuda rpc以支持更易设计和实现新的调度，并测试dispatch的正确性。,https://github.com/hpcaitech/ColossalAI/issues/1595
ColossalAI,这是一个用户提出需求的issue，该问题涉及ColossalAI项目中的autoparallel功能。,https://github.com/hpcaitech/ColossalAI/issues/1594
ColossalAI,这个issue是关于代码重构和性能优化的，主要涉及到ColossalAI中的autoparallel模块的shape consistency manager和resharding操作，原因是之前计算重分片成本的代码重复实现导致了冗余。,https://github.com/hpcaitech/ColossalAI/issues/1591
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的autoparallel模块。用户提出了对resnet autoparallel单元测试的需求，并解释了在某些极端情况下参数较大时通信成本未被计入可能导致求解器无法得到合理解决的问题。,https://github.com/hpcaitech/ColossalAI/issues/1589
ColossalAI,该issue类型为功能改进，主要涉及对象为“SolverOptions”数据类，提出了使用数据类替代字典的改进。,https://github.com/hpcaitech/ColossalAI/issues/1588
ColossalAI,这是一个技术性的改进型issue，主要涉及到ColossalAI中线性化和转子求解器的改进。改动的原因是之前的共同节点检测和求解器机制存在一些问题，导致了一些不符合预期的行为。,https://github.com/hpcaitech/ColossalAI/issues/1586
ColossalAI,这是一个需求提出的issue，主要涉及ColossalAI中激活检查点代码生成中添加嵌套检查点的功能。原因是需要实现嵌套检查点来处理在解析器更新中的特定结构，以及应用到特定的Activation Checkpoint Solver中。,https://github.com/hpcaitech/ColossalAI/issues/1585
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI中的solver类与ResNet模型的适配问题。原因是为了更好地管理内存成本，优化解决方案的搜索策略。,https://github.com/hpcaitech/ColossalAI/issues/1583
ColossalAI,这是一个用户提出需求的类型issue，主要对象是ColossalAI的项目。,https://github.com/hpcaitech/ColossalAI/issues/1581
ColossalAI,这是一个关于代码风格优化的issue，不是bug报告。该问题单涉及的主要对象是ColossalAI库的训练器中的学习率调度器钩子。,https://github.com/hpcaitech/ColossalAI/issues/1576
ColossalAI,这是一个代码风格优化（Code Style）类的Issue，主要涉及到ColossalAI项目中的NN模块中的cache_embedding/parallel_req_aware_embedding.py文件。由于代码风格不符合规范导致需要进行调整。,https://github.com/hpcaitech/ColossalAI/issues/1575
ColossalAI,这是一个发布版本更新的类型，针对ColossalAI的问题。,https://github.com/hpcaitech/ColossalAI/issues/1574
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI中的多步学习率调度器（lr_scheduler/multistep.py）模块。,https://github.com/hpcaitech/ColossalAI/issues/1572
ColossalAI,这是一个代码风格优化（Code Style）类的issue，主要对象是ColossalAI库中的torch.py文件，用户提出了代码风格方面的优化需求。,https://github.com/hpcaitech/ColossalAI/issues/1571
ColossalAI,这个issue类型是代码风格优化，涉及的主要对象是ColossalAI库中的`data_parallel.py`文件，用户提出了代码风格优化的需求。,https://github.com/hpcaitech/ColossalAI/issues/1570
ColossalAI,这是一个用户提出需求的issue，主要对象是VocabParallelClassifier1D模块。,https://github.com/hpcaitech/ColossalAI/issues/1569
ColossalAI,这个issue类型属于代码风格优化，主要对象是ColossalAI中的测试组件gpt.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1567
ColossalAI,这是一则代码风格优化的issue，主要涉及的对象是ColossalAI项目中的tensor_detector.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1566
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI项目中的gemini/update/chunkv2.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1565
ColossalAI,这个issue属于功能改进类型，主要涉及缓存嵌入的小改进，用户希望对嵌入缓存进行优化以提升性能。,https://github.com/hpcaitech/ColossalAI/issues/1564
ColossalAI,这是一个关于代码风格优化的issue，主要涉及ColossalAI项目中pipeline/utils.py文件的代码。,https://github.com/hpcaitech/ColossalAI/issues/1562
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI库中的embedding.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1561
ColossalAI,这个issue类型是代码质量优化（code style）要求，主要对象是ColossalAI代码库中的`colossalai/builder/__init__.py`文件，用户提出了修改此文件以符合代码风格规范的需求。,https://github.com/hpcaitech/ColossalAI/issues/1560
ColossalAI,这是一个代码风格优化的issue，主要涉及的对象是ColossalAI仓库中的multi_tensor_apply.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1559
ColossalAI,这是一个代码风格优化的issue，主要涉及到ColossalAI代码库中的测试模块。  这个问题主要是由于代码风格不符合规范而导致的，需要优化代码风格。,https://github.com/hpcaitech/ColossalAI/issues/1558
ColossalAI,这个issue类型是代码优化（NFC）的提交，主要对象是ColossalAI库中的神经网络模块。,https://github.com/hpcaitech/ColossalAI/issues/1556
ColossalAI,这是一个代码风格优化类的issue，主要涉及ColossalAI中的layernorm.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1555
ColossalAI,这是一个代码风格优化（code style）类型的issue，主要涉及ColossalAI中zero库下的sharded_model/reduce_scatter.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1554
ColossalAI,这是一个代码风格优化（polishing）类型的issue，主要涉及ColossalAI中nn模块下loss_2p5d.py文件的代码风格调整。,https://github.com/hpcaitech/ColossalAI/issues/1553
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI中的embedding_bag.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1552
ColossalAI,这个issue类型是一个质量改进类型，该问题单涉及的主要对象是代码质量。原因可能是代码待优化，有待进一步完善。,https://github.com/hpcaitech/ColossalAI/issues/1551
ColossalAI,这是一个功能需求类型的issue，涉及到对子模块引用进行同步更新。,https://github.com/hpcaitech/ColossalAI/issues/1550
ColossalAI,这个issue类型为功能需求，主要涉及pipeline/converge，用户提出需要添加一个用于测试收敛性的接口。,https://github.com/hpcaitech/ColossalAI/issues/1549
ColossalAI,这是一个需求优化的issue，主要涉及utils模块下的parallel layers checkpoint和bcast model在加载checkpoint时的重构。由于代码结构需要改进，可能导致加载checkpoint时出现错误或性能问题。,https://github.com/hpcaitech/ColossalAI/issues/1548
ColossalAI,这是一个优化代码的issue，主要对象是ColossalAI下的utils模块，用户提出了关于优化分割方式的需求。,https://github.com/hpcaitech/ColossalAI/issues/1546
ColossalAI,这是一个关于优化性能的issue，主要涉及到ColossalAI中的调度性能问题。原因是需要调整主循环以避免忙等待，并将ste集成到主循环中以避免通过线程切换导致额外的cuda内存开销。,https://github.com/hpcaitech/ColossalAI/issues/1544
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的autoparallel模块。,https://github.com/hpcaitech/ColossalAI/issues/1539
ColossalAI,这是一个用户提出需求的issue，主要对象是频率感知嵌入（freq_aware_embedding）。产生这个issue的原因可能是为了为调用方应用程序添加一些小功能。,https://github.com/hpcaitech/ColossalAI/issues/1537
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中支持aten级别计算图的元追踪，为了实现获取任何模型的计算图，提出了将_meta_registration.py移动到根目录的变更。,https://github.com/hpcaitech/ColossalAI/issues/1536
ColossalAI,"这是一个关于需求的issue，涉及""embedding""模块，用户希望实现""tablewise sharding polish""。",https://github.com/hpcaitech/ColossalAI/issues/1535
ColossalAI,这是一个用户需求类型的issue，主要涉及到自动化更新子模块提交的问题。,https://github.com/hpcaitech/ColossalAI/issues/1534
ColossalAI,这个issue属于用户提出需求的类型，涉及的主要对象是MetaTensor和MetaInfoProp，由于新特性的引入，需要进一步完善act_ckpt的研究，但因为缺少集成新特性的测试，用户提出了需要完善的建议。,https://github.com/hpcaitech/ColossalAI/issues/1532
ColossalAI,这是一个技术改进的issue，主要涉及ColossalAI下的activation checkpoint solver和ckpt solver rotor，由于旧的linearize代码不够优雅，导致需要修改。,https://github.com/hpcaitech/ColossalAI/issues/1531
ColossalAI,"这是一个用户提出需求的issue，主要涉及ColossalAI中的autoparallel模块以及operator handler，提出支持""function""的需求。",https://github.com/hpcaitech/ColossalAI/issues/1529
ColossalAI,这是一个需求类型的issue，涉及ColossalAI中的Tablewise cache功能。这个issue的原因是因为缺乏对Tablewise cache功能的测试，用户提出了需要进行测试的需求。,https://github.com/hpcaitech/ColossalAI/issues/1526
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的自动并行计算模块，该issue描述了在卷积策略中添加反向传播信息的改进，包括内存成本、计算成本、通信成本和重新分片成本。,https://github.com/hpcaitech/ColossalAI/issues/1524
ColossalAI,这个issue是关于自动化更新子模块提交的需求类型问题，主要涉及到协调参考子模块的更新。,https://github.com/hpcaitech/ColossalAI/issues/1523
ColossalAI,这个issue是一个功能需求，主要对象是ColossalAI的子模块引用，用户请求自动化更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/1518
ColossalAI,这是一个功能需求的issue， 主要涉及到ColossalAI中的自动并行化功能。这个issue讨论了如何通过计算图添加存活性分析，以确定在每个阶段（节点）中产生或销毁的存活变量。,https://github.com/hpcaitech/ColossalAI/issues/1516
ColossalAI,这是一个关于优化LFU算法在ColossalAI框架中使用cuda内存的问题。,https://github.com/hpcaitech/ColossalAI/issues/1512
ColossalAI,这是一个关于自动更新子模块提交的issue，属于功能需求类型，主要涉及到同步子模块引用。,https://github.com/hpcaitech/ColossalAI/issues/1511
ColossalAI,这个issue类型为功能需求，涉及的主要对象为ColossalAI中的pipeline模块。,https://github.com/hpcaitech/ColossalAI/issues/1508
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalAI中的StrategiesConstructor类。这个问题由于需要添加一个StrategiesConstructor类来分析图形并找到最佳策略组合。,https://github.com/hpcaitech/ColossalAI/issues/1505
ColossalAI,这是一个文档更新类型的issue，涉及的主要对象是GitHub上的ColossalAI项目。,https://github.com/hpcaitech/ColossalAI/issues/1503
ColossalAI,这个issue是一则关于自动化更新子模块提交的请求，类型为需求。主要涉及的对象是ColossalAI仓库中的子模块。,https://github.com/hpcaitech/ColossalAI/issues/1499
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是服务器的GPU分配。由于用户需要指定服务器上每个GPU的使用情况，因此提出了如何指定每台服务器的GPU的问题。,https://github.com/hpcaitech/ColossalAI/issues/1498
ColossalAI,这个issue类型是优化需求，主要涉及的对象是pipeline/rpc模块的outstanding机制和dispatching策略，用户提出了需要更新outstanding机制并优化dispatch策略的需求。,https://github.com/hpcaitech/ColossalAI/issues/1497
ColossalAI,这个issue类型是功能更新，涉及的主要对象是activation checkpoint solver rotor算法。这是由于引入了rotor算法来计算我们需要使用的指标，并为此添加了相关的单元测试，但在CI上会跳过该测试，原因是还没有在torch11中实现`ColoGraphModule`。,https://github.com/hpcaitech/ColossalAI/issues/1496
ColossalAI,这是一个需求提议，主要涉及ColossalAI中的性能优化和错误信息输出。由于Profiler无法准确检测临时内存开销，用户提出了针对unsupported ops的错误信息和指导的需求。,https://github.com/hpcaitech/ColossalAI/issues/1495
ColossalAI,这个issue是一个功能需求提出类型，主要涉及FAW embedding使用LRU作为淘汰策略并使用数据集统计进行初始化，可能由于缓存策略不够高效导致内存占用过多或性能下降。,https://github.com/hpcaitech/ColossalAI/issues/1494
ColossalAI,这个issue是一个功能需求，主要涉及到在1D设备网格中进行通信时的优化。,https://github.com/hpcaitech/ColossalAI/issues/1492
ColossalAI,这是一个用户提出需求类型的Issue，主要关注如何支持将mmdetection作为分布式后端来加速训练。用户想要通过使用分布式工具来减少训练时间和GPU需求。,https://github.com/hpcaitech/ColossalAI/issues/1491
ColossalAI,这是一个功能需求类型的issue，主要对象是ColossalAI库中的`auto_parallel`模块。由于缺乏命名空间标准化，用户提出需要在该模块的文件中添加`__all__`。,https://github.com/hpcaitech/ColossalAI/issues/1490
ColossalAI,该issue类型为功能需求，涉及到ColossalAI中的LFU实现，用户提出了需要为FAW初始化一个LFU实现的需求。,https://github.com/hpcaitech/ColossalAI/issues/1488
ColossalAI,这是一个用户提出需求的issue， 主要涉及到ColossalAI中自动并行计算的卷积操作的分片策略。由于之前的PR遗漏了某些关键的分片策略，导致用户在本次PR中添加了这些策略。,https://github.com/hpcaitech/ColossalAI/issues/1487
ColossalAI,这是一个功能需求的issue，主要涉及的对象是关于实现分布式优化器和使用assert_close进行正确性测试。,https://github.com/hpcaitech/ColossalAI/issues/1486
ColossalAI,该issue类型是功能需求，主要涉及的对象是协助同步子模块引用，由于需要自动化更新子模块提交所导致的对应问题。,https://github.com/hpcaitech/ColossalAI/issues/1484
ColossalAI,这是一个特性需求报告，主要涉及到ColossalAI中的fx节点。由于需要对内存成本和FLOPs进行基准测试，用户正在寻求添加分析器以支持查看模型成本和性能。,https://github.com/hpcaitech/ColossalAI/issues/1480
ColossalAI,这个issue是关于功能改进的，主要涉及ColossalAI下的autoparallel模块的集成问题，由于Integration with torch fx的实现需要进行代码重构和简化，包括使用torch.fx Node自动检索节点、简化API以及OperatorHandler提供module_named_parameters属性等改动。,https://github.com/hpcaitech/ColossalAI/issues/1479
ColossalAI,该issue类型为功能需求，涉及的主要对象是ColossalAI的项目子模块引用同步。,https://github.com/hpcaitech/ColossalAI/issues/1478
ColossalAI,这个issue类型为文档更新，主要对象涉及到ColossalAI下的xTrimoMultimer项目。,https://github.com/hpcaitech/ColossalAI/issues/1477
ColossalAI,这个issue类型是功能改进，涉及到autoparallel模块中的dot handler的添加和重构，主要目的是与另一个功能相关联。,https://github.com/hpcaitech/ColossalAI/issues/1475
ColossalAI,这是一个需求类型的issue，涉及到自动化更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/1472
ColossalAI,这个issue是一个功能改进类型的反馈，主要涉及ColossalAI中的操作符处理程序（op handler），提到对API一致性的改进和减少代码冗余。,https://github.com/hpcaitech/ColossalAI/issues/1471
ColossalAI,该issue类型为用户提出需求，涉及实现一个使用cuda rpc框架的demo，主要目标是支持不同功能特性并测试性能。原因是希望实现特定功能并对其进行测试。,https://github.com/hpcaitech/ColossalAI/issues/1470
ColossalAI,这个issue类型为优化需求，主要涉及到代码结构的规范化。原因是为了在auto parallel格式下统一代码结构。,https://github.com/hpcaitech/ColossalAI/issues/1469
ColossalAI,这是一个文档更新类型的issue，涉及主要对象为ProcessGroup类。原因是需要更新ProcessGroup类的文档字符串。,https://github.com/hpcaitech/ColossalAI/issues/1468
ColossalAI,该issue是用户提出的需求类型，涉及ColossalAI中的自动并行处理功能的Conv handler，目的是生成卷积节点的各种策略并提供相应成本信息。,https://github.com/hpcaitech/ColossalAI/issues/1467
ColossalAI,这个issue类型是功能需求提议，主要涉及的对象是ColossalAI下的子模块引用同步，用户提出了自动化更新子模块提交的需求。,https://github.com/hpcaitech/ColossalAI/issues/1465
ColossalAI,这是一个功能更新类的issue，主要对象是FreqCacheEmbedding接口。由于新增了接口来处理频率统计信息，用户现在可以提前将频率统计信息异步移动到GPU，从而加速初始化过程。,https://github.com/hpcaitech/ColossalAI/issues/1462
ColossalAI,该issue为功能需求报告，涉及ColossalAI中的线性化计算图搜索；由于现有的激活检查点框架假设前向图是线性化的，作者添加了规则来线性化计算图以进行搜索。,https://github.com/hpcaitech/ColossalAI/issues/1461
ColossalAI,这个issue类型为用户提出需求，主要对象是ColoTensor，由于缺乏文档，用户请求添加更多关于ColoTensor的文档。,https://github.com/hpcaitech/ColossalAI/issues/1458
ColossalAI,该issue类型为文档优化，主要涉及ColoTensor文档风格的修改。,https://github.com/hpcaitech/ColossalAI/issues/1457
ColossalAI,这是一个功能需求的issue，主要涉及到在CC PR中支持runtime ShardingSpec apply。由于目前还未支持runtime ShardingSpec apply，导致在运行时无法应用ShardingSpec转换，需要增加相应的功能来支持此操作。,https://github.com/hpcaitech/ColossalAI/issues/1453
ColossalAI,这个issue是关于自动化更新子模块提交的问题，属于功能需求类型，主要涉及子模块的引用同步。,https://github.com/hpcaitech/ColossalAI/issues/1452
ColossalAI,这是一个功能增加的类型，该问题单涉及的主要对象是ColossalAI中的TensorNVMe。,https://github.com/hpcaitech/ColossalAI/issues/1449
ColossalAI,这个issue类型是需求提出，涉及主要对象是FreqCacheEmbedding，由于继承结构混乱导致了冗余代码和接口不兼容的问题。,https://github.com/hpcaitech/ColossalAI/issues/1448
ColossalAI,这是一个功能需求的issue，主要涉及的对象是ColossalAI中的ColoTensor和ZeroOptimizer。由于grads可以处于不同的计算设备和数据类型之间，导致需要在这些情况下实现clip_grad_norm的功能。,https://github.com/hpcaitech/ColossalAI/issues/1442
ColossalAI,这是一个关于新增功能的issue，主要对象是zero中的chunk_managerV2，用户希望为all-gather chunk添加该功能。,https://github.com/hpcaitech/ColossalAI/issues/1441
ColossalAI,这是一个用户提出需求的issue，涉及主要对象是ColossalAI中的代码生成模块。由于未能跟踪offload行为，用户提出了使用ColossalAI的checkpoint替换torch的checkpoint函数，并希望代码生成模块能识别图中检查点区域的激活_offload选项。,https://github.com/hpcaitech/ColossalAI/issues/1439
ColossalAI,这是一个用户提出需求的issue，涉及的主要对象是在ColossalAI中的一项功能（FAW），用户想要在_ops中导出FAW。,https://github.com/hpcaitech/ColossalAI/issues/1438
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中参数组的新分配算法。,https://github.com/hpcaitech/ColossalAI/issues/1436
ColossalAI,这个issue类型是功能需求，主要涉及到ColossalAI中的[tensor] shape consistency功能。由于需要支持自动特定的转换，用户提出了关于自动并行策略搜索和运行时分片特定的需求。,https://github.com/hpcaitech/ColossalAI/issues/1435
ColossalAI,该issue类型为功能增强（feature enhancement），主要对象是 ColossalAI 中的 activation checkpoint 功能。由于用户提出了希望添加对 vanilla activation checkpoint search 的支持，并提供了算法参考和两行代码实现的需求。,https://github.com/hpcaitech/ColossalAI/issues/1433
ColossalAI,这是一则需求提出的issue，涉及的主要对象是Gemini库。,https://github.com/hpcaitech/ColossalAI/issues/1432
ColossalAI,这是一个用户需求类型的issue，主要涉及的对象是ColossalAI库中的clip grad norm功能。由于缺乏详细内容，无法确定具体问题或需求是什么。,https://github.com/hpcaitech/ColossalAI/issues/1430
ColossalAI,这是关于增加并行 FAW 嵌入的需求问题，主要对象是 ColossalAI。由于需要优化模型性能，用户希望实现并行 FAW 嵌入。,https://github.com/hpcaitech/ColossalAI/issues/1428
ColossalAI,这个issue类型是功能改进，主要涉及节点大小计算问题，用户寻求改进计算内存消耗的方法。,https://github.com/hpcaitech/ColossalAI/issues/1425
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的频率感知嵌入（FreqAwareEmbedding），用户寻求并行实现该功能的帮助。,https://github.com/hpcaitech/ColossalAI/issues/1424
ColossalAI,这个issue类型是需求提出，该问题单主要涉及的对象是ColossalAI库中的FreqAwareEmbeddingBag模块。,https://github.com/hpcaitech/ColossalAI/issues/1421
ColossalAI,这个issue类型是用户提出需求，要求在readme中添加代码示例。该问题主要涉及ColossalAI框架的代码文档。由于内存使用减少但速度未提升，用户希望将相应代码示例添加到readme中。,https://github.com/hpcaitech/ColossalAI/issues/1420
ColossalAI,这是一个用户提出的需求。该问题涉及将软件迁移到ColossalAI，并添加缓存管理器。,https://github.com/hpcaitech/ColossalAI/issues/1419
ColossalAI,该issue类型为需求提出，主要涉及到ColossalAI中的tensor模块，用户提出了对shape一致性特性的支持需求。,https://github.com/hpcaitech/ColossalAI/issues/1418
ColossalAI,"这是一个需求提交的类型，主要涉及的对象是添加一个名为""AgChunk""的功能。由于更新计划将会合并单元测试至下一个拉取请求，因此提出了这项需求。",https://github.com/hpcaitech/ColossalAI/issues/1417
ColossalAI,这个issue属于需求类型，主要涉及到同步子模块引用，由于需要自动化PR来更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/1415
ColossalAI,这是一个用户提出需求的issue，主要涉及了MegatronLM和ColossalAI实验中使用的Batchsize，用户希望在benchmark结果中看到这些信息。可能由于缺乏这些信息，用户感到困惑或需要进一步了解实验的具体设置。,https://github.com/hpcaitech/ColossalAI/issues/1414
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI是否支持定制化并行调度的问题，原因是用户希望能够在ColossalAI上测试他们设计的并行调度。,https://github.com/hpcaitech/ColossalAI/issues/1411
ColossalAI,这是一个需求报告类型的issue，主要涉及到engin/schedule中的pipeline_schedule，出现这个问题的原因可能是需要使用p2p_v2来重构pipeline_schedule。,https://github.com/hpcaitech/ColossalAI/issues/1409
ColossalAI,这是一个需求提报，主要涉及的对象是ColossalAI中的p2p_v2和pipeline_schedule。,https://github.com/hpcaitech/ColossalAI/issues/1408
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中的通信模块，在本问题中用户提出了需要支持与List[Any]进行通信的需求。,https://github.com/hpcaitech/ColossalAI/issues/1407
ColossalAI,这个issue类型是功能需求，主要涉及支持在不同节点间使用List[Any]进行通信，其中的Any可以是基本Python对象、CPU上的张量和CUDA上的张量。,https://github.com/hpcaitech/ColossalAI/issues/1406
ColossalAI,这是一个建议类型的issue，该问题单涉及的主要对象是ColossalAI中的tensor模块。由于未提供具体内容，无法得知具体原因或问题类型。,https://github.com/hpcaitech/ColossalAI/issues/1405
ColossalAI,该问题类型为功能需求，涉及的主要对象是子模块引用的同步更新，用户提出了需要自动化更新子模块提交的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/1404
ColossalAI,这是一个功能改进类的issue，主要涉及ColossalAI中clip_grad_norm_fp32()函数的重构。,https://github.com/hpcaitech/ColossalAI/issues/1403
ColossalAI,这是一个用户提出需求的类型，主要对象是ZeroDDP，该问题涉及支持控制输出的数据类型。,https://github.com/hpcaitech/ColossalAI/issues/1399
ColossalAI,这是一个功能更新类的issue，主要涉及到更新ColossalAI项目的README中关于nvme的内容。这个issue可能是由于README文件中的nvme信息过时或有误，需要更新以保持项目文档的准确性。,https://github.com/hpcaitech/ColossalAI/issues/1397
ColossalAI,这个issue是一个用户提出需求的类型，主要对象是ColossalAI仓库中的子模块引用，用户希望自动化更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/1396
ColossalAI,该issue类型为用户提出需求，该问题单涉及的主要对象是添加GPT-2 NVME。这个问题可能是由于需要更新GPT2 NVME结果并更新'Heterogeneous Memory Management'链接而提出的需求。,https://github.com/hpcaitech/ColossalAI/issues/1395
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是设备(Device)。,https://github.com/hpcaitech/ColossalAI/issues/1394
ColossalAI,这个issue类型是功能增强或优化，主要涉及的对象是ColossalAI中关于torch.max操作符和数据移动操作的改进。,https://github.com/hpcaitech/ColossalAI/issues/1391
ColossalAI,这是一个功能需求类型的issue，主要涉及到ColossalAI库中的zero optimizer（zero_optim），提出了只有DP rank 0返回全局状态字典的控制需求（only_rank_0参数）。,https://github.com/hpcaitech/ColossalAI/issues/1384
ColossalAI,这个issue类型是功能需求，涉及主要对象为ColossalAI的子模块参考，用户提出自动化更新子模块提交的需求。,https://github.com/hpcaitech/ColossalAI/issues/1380
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI框架中的checkpoint功能，用户提议添加加载状态字典时的参数选项。,https://github.com/hpcaitech/ColossalAI/issues/1374
ColossalAI,这是一个功能更新的issue，主要涉及ColossalAI中的split module pass和自定义split策略，由于之前的处理方式不符合pipeline parallel的需求，导致需要进行更新。,https://github.com/hpcaitech/ColossalAI/issues/1373
ColossalAI,这个issue类型是功能需求，主要涉及到的对象是 submodule references。这个需求的目的是为了自动化更新子模块的提交内容。,https://github.com/hpcaitech/ColossalAI/issues/1372
ColossalAI,这是一个用户提出需求类型的issue， 主要对象是torchrec.models。由于需要添加colotracer兼容性测试，用户希望确保ColossalAI的模型在torchrec环境中能够正常运行。,https://github.com/hpcaitech/ColossalAI/issues/1370
ColossalAI,这个issue类型是用户提出需求，主要对象是ColossalAI的checkpoint功能，用户提出了在save_checkpoint和load_checkpoint函数中使用args和kwargs参数的建议。,https://github.com/hpcaitech/ColossalAI/issues/1368
ColossalAI,这是一个类型为功能需求的issue，涉及到子模块引用的同步更新。,https://github.com/hpcaitech/ColossalAI/issues/1364
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的NVMe功能。,https://github.com/hpcaitech/ColossalAI/issues/1360
ColossalAI,这是一个功能增强提示，涉及 ColossalAI 项目中的代码生成支持。,https://github.com/hpcaitech/ColossalAI/issues/1359
ColossalAI,这是一个功能增强的issue，主要涉及ColossalAI项目中的模型前向推断代码生成，由于代码生成仅适用于torch 1.12，导致pytest中的测试被跳过。,https://github.com/hpcaitech/ColossalAI/issues/1355
ColossalAI,"这是一个用户提出需求的类型，主要涉及到将""tensornvme""添加到Docker中。由于用户需要在Docker中引入""tensornvme""，可能是为了进行NVMe相关的操作或功能。",https://github.com/hpcaitech/ColossalAI/issues/1354
ColossalAI,这个issue是文档更新类的问题，涉及的主要对象为ColossalAI程序的rst文件和docstring，由于可能文档内容有误或者不全，需要更新以提供更准确的信息。,https://github.com/hpcaitech/ColossalAI/issues/1351
ColossalAI,这个issue属于需求类型，主要涉及ColossalAI的代码库，用户提出了添加激活检查点注释的需求。,https://github.com/hpcaitech/ColossalAI/issues/1349
ColossalAI,该issue类型为功能需求，主要涉及子模块引用的同步更新。由于需要自动化提交，这可能导致存在子模块更新不及时或不完整的问题。,https://github.com/hpcaitech/ColossalAI/issues/1348
ColossalAI,这是一个用户提出需求的 issue，主要涉及到 ColossalAI 下的 ColoTensor 类。由于当前的 ColoTensor 类的初始化方法只设置 distspec 信息而不应用 sharding 到张量本身，导致用户提出需要在 ColoTensor 类的初始化方法中添加 set_dist_spec() 方法来解决这一问题。,https://github.com/hpcaitech/ColossalAI/issues/1347
ColossalAI,这个issue类型是功能需求，主要对象是ColossalAI库中colotensor模块的Tensor类，用户提出了添加Tensor.view操作和相应的单元测试的需求。,https://github.com/hpcaitech/ColossalAI/issues/1346
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI的README。由于缺少描述，用户希望添加关于社区集成的OPT选项的说明。,https://github.com/hpcaitech/ColossalAI/issues/1345
ColossalAI,这个issue属于需求类型，主要涉及的对象是ColossalAI中的colotensor模块。,https://github.com/hpcaitech/ColossalAI/issues/1343
ColossalAI,这个issue是关于需求修改的问题，涉及主要对象是ColossalAI的Docker build workflow。这个问题可能是由于需要使用代理导致，所以用户提出了更新Docker build workflow的需求。,https://github.com/hpcaitech/ColossalAI/issues/1334
ColossalAI,这是一个需求问题，用户希望更新8个GPU测试以使用torch 1.11，可能是由于新的torch版本引入了功能变更或修复bug导致需要更新。,https://github.com/hpcaitech/ColossalAI/issues/1332
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI与PyTorch Lightning集成的问题。,https://github.com/hpcaitech/ColossalAI/issues/1330
ColossalAI,这是一个优化类型的issue，主要涉及到ColossalAI中共享模型返回CPU状态的问题，旨在减少CUDA内存使用量。,https://github.com/hpcaitech/ColossalAI/issues/1328
ColossalAI,这个issue类型为功能更新，主要涉及对象是ColossalAI中的utils模块。这个功能更新的原因可能是为了整合ColoTensor与懒初始化上下文。,https://github.com/hpcaitech/ColossalAI/issues/1324
ColossalAI,这是一个用户提出需求的issue，关于ColossalAI项目添加对Huggingface transformers的支持。Chuck关注项目中Huggingface transformers的脚本并询问何时会有官方发布支持。,https://github.com/hpcaitech/ColossalAI/issues/1322
ColossalAI,该问题单是关于自动化更新子模块提交的操作，属于功能需求类型，主要涉及到对子模块引用的同步操作。,https://github.com/hpcaitech/ColossalAI/issues/1319
ColossalAI,这个issue类型是功能需求列表， 主要涉及的对象是项目的工作流程。由于需要支持任意输入，同时默认构建流程会使用最新的PyTorch版本，不清楚用户是否考虑到了兼容性或其他潜在问题。,https://github.com/hpcaitech/ColossalAI/issues/1318
ColossalAI,该issue类型是功能需求，主要对象是ColossalAI中使用ColoTensor的Module。由于当前的基本功能不足以满足需求，用户提出了引入一些高级特性来优化模型的保存和加载过程。,https://github.com/hpcaitech/ColossalAI/issues/1317
ColossalAI,这个issue属于需求类型，主要涉及ColoOptimizer的checkpointing功能添加，用户需求此功能的实现。,https://github.com/hpcaitech/ColossalAI/issues/1316
ColossalAI,这个issue类型为功能更新，涉及的主要对象是ColossalAI框架中的torch版本更新。,https://github.com/hpcaitech/ColossalAI/issues/1313
ColossalAI,这是一个功能改进类型的issue，主要涉及的对象是ColossalAI中的Optimizer模块。这个问题的提出可能是因为ColoOptimizer的功能被认为是多余的或者存在一定的设计缺陷。,https://github.com/hpcaitech/ColossalAI/issues/1312
ColossalAI,这是一个功能请求类型的issue，主要涉及ColossalAI中的PyTorch兼容性测试模块。由于之前版本设置是硬编码的，用户提出希望支持任意版本输入以提高灵活性。,https://github.com/hpcaitech/ColossalAI/issues/1311
ColossalAI,这个issue是关于代码优化的建议，主要对象是ColoOptimizer的初始化方法。,https://github.com/hpcaitech/ColossalAI/issues/1310
ColossalAI,这是一个代码风格优化的issue，主要对象为ColossalAI代码库的构建脚本，用户提出需要优化代码风格。,https://github.com/hpcaitech/ColossalAI/issues/1306
ColossalAI,这个issue是关于自动化更新子模块提交的问题，属于功能需求类型，主要涉及的对象是ColossalAI下的一个子模块。,https://github.com/hpcaitech/ColossalAI/issues/1305
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中关于Exponential Moving Average (EMA)的实现问题，由于模型分片，用户不清楚如何正确实现EMA。,https://github.com/hpcaitech/ColossalAI/issues/1304
ColossalAI,这个issue类型是代码风格优化，该问题单涉及的主要对象是ColossalAI下的nn/layer/wrapper/pipeline_wrapper.py文件。 ,https://github.com/hpcaitech/ColossalAI/issues/1303
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI下的nn模块中loss_2p5d.py文件的代码风格问题。,https://github.com/hpcaitech/ColossalAI/issues/1296
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI中nn/layer/vanilla/layers.py文件的代码格式问题。,https://github.com/hpcaitech/ColossalAI/issues/1295
ColossalAI,这是一个代码风格优化（code style）类别的issue，主要涉及ColossalAI项目中的神经网络（nn）模块下的vanilla包中的__init__.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1293
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI的nn/init.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1292
ColossalAI,这是一个代码风格优化（NFC）的issue，主要涉及到ColossalAI的CUDA内核代码。原因可能是为了统一代码风格或提高代码可读性。,https://github.com/hpcaitech/ColossalAI/issues/1291
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI中的loss_2p5d.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1288
ColossalAI,这是一个代码风格优化类的issue，主要涉及ColossalAI下的CUDA核心代码风格问题。​ ​​,https://github.com/hpcaitech/ColossalAI/issues/1286
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI代码库中的`__init__.py`文件。,https://github.com/hpcaitech/ColossalAI/issues/1285
ColossalAI,这个issue类型是代码风格优化，涉及的主要对象是ColossalAI中的神经网络模型的并行序列层(layers)代码。,https://github.com/hpcaitech/ColossalAI/issues/1280
ColossalAI,这是一个代码风格优化的Issue，涉及的主要对象是ColossalAI下的nn/loss/loss_2p5d.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1273
ColossalAI,这个issue类型是代码优化，涉及对象是ColossalAI中的测试代码，此问题是为了删除T5测试代码中的跳过装饰器。,https://github.com/hpcaitech/ColossalAI/issues/1271
ColossalAI,"这个issue是关于""代码风格优化""，主要涉及ColossalAI代码库中的`colossalai/nn/lr_scheduler/onecycle.py`文件。",https://github.com/hpcaitech/ColossalAI/issues/1269
ColossalAI,这是一个代码风格优化（polish）类型的issue，主要涉及ColossalAI库中的一个多项式学习率调度器模块，用户提出风格方面的改进需求。,https://github.com/hpcaitech/ColossalAI/issues/1267
ColossalAI,这个issue属于代码风格优化类型，主要涉及的是ColossalAI库中的builder.py文件的代码风格问题。,https://github.com/hpcaitech/ColossalAI/issues/1265
ColossalAI,这是一个需要优化代码风格的类型为其他类型的issue，主要涉及ColossalAI中CUDA相关模块的代码。由于代码风格不符合要求，需要进行优化。,https://github.com/hpcaitech/ColossalAI/issues/1263
ColossalAI,这个issue类型是代码优化，主要对象是ColossalAI中的communication/collective.py模块。,https://github.com/hpcaitech/ColossalAI/issues/1262
ColossalAI,这是一个代码风格优化类的issue，主要涉及ColossalAI中CUDA kernel中的dropout_kernels.cu文件，由Ziyu Huang提出。,https://github.com/hpcaitech/ColossalAI/issues/1261
ColossalAI,这是一个代码风格优化类的issue，主要涉及 ColossalAI 项目中的 CUDA 相关类型转换代码。原因可能是为了统一代码风格，提高代码可读性。,https://github.com/hpcaitech/ColossalAI/issues/1260
ColossalAI,这是一个代码风格优化类的issue，主要对象是ColossalAI下的一个文件。由于代码风格不符合规范，用户提出了优化代码风格的请求。,https://github.com/hpcaitech/ColossalAI/issues/1258
ColossalAI,这是一个技术改进类的issue，主要涉及ColossalAI下的utils模块，原因可能是为了整合colotensor和lazy init context。,https://github.com/hpcaitech/ColossalAI/issues/1257
ColossalAI,这个issue属于代码风格优化，主要对象是ColossalAI下的engine库中的ophooks/utils.py文件。,https://github.com/hpcaitech/ColossalAI/issues/1256
ColossalAI,这是一个类型为代码风格优化（code style）的issue，主要涉及的对象是ColossalAI中的lr_scheduler模块。原因是需要对该模块中的代码风格进行优化。,https://github.com/hpcaitech/ColossalAI/issues/1255
ColossalAI,这是一个用户提出需求的类型，涉及到ColossalAI中GPT-3可视化功能的更新问题。,https://github.com/hpcaitech/ColossalAI/issues/1254
ColossalAI,这个issue类型是需求提出，主要涉及的对象是ColossalAI的fx模块。这个需求是为了新增一个平衡策略的版本2。,https://github.com/hpcaitech/ColossalAI/issues/1251
ColossalAI,这是一个功能需求的issue，主要涉及到ColossalAI项目中的transformer MLP模块。,https://github.com/hpcaitech/ColossalAI/issues/1248
ColossalAI,这个issue是一个需求类型，主要涉及tensor的重新分配问题，用户希望在不同的进程组之间重新分配tensor。,https://github.com/hpcaitech/ColossalAI/issues/1247
ColossalAI,这是一个用户提出需求的类型，主要对象是ColossalAI项目中的fx图属性。用户希望有方法可以获取fx图属性。,https://github.com/hpcaitech/ColossalAI/issues/1246
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI下的[tensor]，该需求可能是要求简化分片和复制规范。,https://github.com/hpcaitech/ColossalAI/issues/1245
ColossalAI,这是一个请求重命名的issue，主要涉及ColossalAI库中的某个功能重命名问题。,https://github.com/hpcaitech/ColossalAI/issues/1243
ColossalAI,该issue类型是功能增强请求，主要对象是ColossalAI项目中的分割模块pass和单元测试。,https://github.com/hpcaitech/ColossalAI/issues/1242
ColossalAI,这是一个功能需求类型的 issue，主要涉及到自动化更新子模块提交的问题。,https://github.com/hpcaitech/ColossalAI/issues/1241
ColossalAI,这个issue是一个功能需求类型的问题，主要涉及的对象是分布式参数checkpointing，用户在这个问题中提出了对参数级别的checkpointing功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/1240
ColossalAI,这个issue类型是功能需求提交，主要涉及的对象是ColossalAI框架中的checkpoint功能。由于该issue是关于保存分片式优化器状态的功能需求，可能是为了提高训练效率或者解决大规模训练时的性能问题。,https://github.com/hpcaitech/ColossalAI/issues/1237
ColossalAI,"这是一个功能需求提议，主要涉及的对象是tensor模块。根据标题来看，这个issue提议增加一个名为""zero_like colo op""的功能，这个功能对优化器非常重要。",https://github.com/hpcaitech/ColossalAI/issues/1236
ColossalAI,这个issue类型为代码优化，主要涉及对象为ColoTensor、DistSpec和ProcessGroup，由于需要改进这些类的__repr__方法的实现方式。,https://github.com/hpcaitech/ColossalAI/issues/1235
ColossalAI,这是一个测试用例请求，主要涉及ColossalAI中的ColoTensor 1DTP的交叉熵函数，用户提出希望添加对该函数的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/1230
ColossalAI,这是一个功能需求issue， 主要对象是ColossalAI的checkpoint功能。由于需要支持通用调度程序，用户提出了这个功能需求。,https://github.com/hpcaitech/ColossalAI/issues/1222
ColossalAI,该issue是一个功能更新，主要涉及timm模型追踪测试。,https://github.com/hpcaitech/ColossalAI/issues/1221
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是[tensor]下的sharded global process group，用户提出了关于添加全局数据分片处理的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/1219
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI库中的checkpoint功能。导致这个问题的原因可能是checkpoint的性能较慢，用户希望优化提升单元测试的速度。,https://github.com/hpcaitech/ColossalAI/issues/1217
ColossalAI,这是一个功能增强的issue，涉及ColossalAI中的ColoTracer测试torchvision模型，解决了PyTorch 1.11中关于元张量支持池化的问题。,https://github.com/hpcaitech/ColossalAI/issues/1216
ColossalAI,这个issue是关于代码优化的需求。主要涉及的对象是1d_linear_pass，可能是因为性能或代码规范要求，暂时使用了1d_linear_pass。,https://github.com/hpcaitech/ColossalAI/issues/1215
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI项目中的张量规范应用在图上的新增。原因可能是为了增强ColossalAI对张量操作的支持和优化。,https://github.com/hpcaitech/ColossalAI/issues/1214
ColossalAI,这个issue是需求提出类型，主要对象是ColossalAI中的colotensor 1d pass功能。,https://github.com/hpcaitech/ColossalAI/issues/1212
ColossalAI,这个issue属于需求类型，主要涉及的对象是权重（weights），原因可能是需要附加张量规范到权重。,https://github.com/hpcaitech/ColossalAI/issues/1209
ColossalAI,这是一个需求类型的issue，该问题单涉及的主要对象是ColossalAI项目中的fx模块。由于缺少统一的政策，用户提出了添加统一政策的需求。,https://github.com/hpcaitech/ColossalAI/issues/1208
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI的子模块引用同步，由于需要自动化提交来更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/1204
ColossalAI,这是一个需求提出的issue，主要涉及对象是ColossalAI下的ColoTensor。由于ColoTensor的process group永远不会改变，需要避免将具有process group的dist spec作为参数设置。,https://github.com/hpcaitech/ColossalAI/issues/1203
ColossalAI,这是一个需求类型的issue，主要涉及到ColossalAI库中的tensor模块，用户提出需要初始化并行卷积操作的需求。,https://github.com/hpcaitech/ColossalAI/issues/1202
ColossalAI,这是一个用户提出需求类型的issue，主要涉及到ColossalAI库中的HuggingFace BERT模型。,https://github.com/hpcaitech/ColossalAI/issues/1201
ColossalAI,这是一个关于需求的问题，主要涉及Tensor的CPU组在分布式数据并行（DDP）中的添加，可能是为了优化和改进在ColossalAI中的模型训练和推理过程。,https://github.com/hpcaitech/ColossalAI/issues/1200
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中的1D linear pass函数的优化。,https://github.com/hpcaitech/ColossalAI/issues/1199
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中新增的materialization context功能。这个问题是用户在开发过程中发现需要添加materialization context功能，并提出在该环境中存在的需求。,https://github.com/hpcaitech/ColossalAI/issues/1198
ColossalAI,这是一个功能增强的类型，涉及的主要对象是ColossalAI中的模块。这个issue是为了添加池化层的模块补丁。,https://github.com/hpcaitech/ColossalAI/issues/1197
ColossalAI,这个issue类型为功能需求提出，主要涉及的对象是ColoTensor Model。由于用户需要在ColossalAI中实现针对ColoTensor Model的checkpoint功能，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/1196
ColossalAI,这个issue类型是功能需求报告，涉及的主要对象是ColossalAI下的子模块引用同步，用户希望能够通过自动化的PR来更新子模块的提交信息。,https://github.com/hpcaitech/ColossalAI/issues/1194
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI库中的模块材料化 functionality。,https://github.com/hpcaitech/ColossalAI/issues/1193
ColossalAI,这个issue类型是用户提出需求，主要涉及Gemini中的AutoPlacementPolicy，用户希望可以配置warmup ratio。,https://github.com/hpcaitech/ColossalAI/issues/1192
ColossalAI,这是一个用户提出需求的issue，主要对象是AutoPlacementPolicy；用户希望能够使AutoPlacementPolicy可配置。,https://github.com/hpcaitech/ColossalAI/issues/1191
ColossalAI,这个issue是一个功能需求类型，主要涉及的对象是模型初始化过程中的元张量，问题是关于如何使用元张量来延迟初始化模型。,https://github.com/hpcaitech/ColossalAI/issues/1187
ColossalAI,这是一个技术改进类型的issue，涉及ColossalAI中对数据依赖的控制流追踪支持。,https://github.com/hpcaitech/ColossalAI/issues/1185
ColossalAI,这个issue是一个用户提出需求的类型，主要涉及到ColoTensor库，请求添加独立进程组功能。,https://github.com/hpcaitech/ColossalAI/issues/1179
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI中TensorSpec的API重命名和视图unittest的优化。,https://github.com/hpcaitech/ColossalAI/issues/1176
ColossalAI,这是一个需求用户提出的issue，主要对象是Tensor模块中的parallel_action函数。,https://github.com/hpcaitech/ColossalAI/issues/1174
ColossalAI,这是一个需求类型的issue，涉及ColossalAI中colotensor API的命名问题。,https://github.com/hpcaitech/ColossalAI/issues/1172
ColossalAI,这个issue是一个需求类型，主要涉及ColossalAI中的Tensor模块，提出了对分布式视图支持多进程混合并行的要求。,https://github.com/hpcaitech/ColossalAI/issues/1169
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColoTensor的API重命名和添加output_replicate到ComputeSpec，可能是为了提高代码的可读性和功能的完善。,https://github.com/hpcaitech/ColossalAI/issues/1168
ColossalAI,这个issue类型是功能需求，主要对象是ColossalAI中的Tensor模块，用户提出希望在使用ColoTensor时能够有一个轻量级的启动器，不需要使用配置文件启动。,https://github.com/hpcaitech/ColossalAI/issues/1167
ColossalAI,这是一个功能改进类的issue，涉及Tensor模块下的ParallelAction的替换操作。,https://github.com/hpcaitech/ColossalAI/issues/1166
ColossalAI,这个issue类型是需求提出，主要对象为项目的README和Dockerfile，用户要求对其进行修正和优化。,https://github.com/hpcaitech/ColossalAI/issues/1165
ColossalAI,这是一个用户提出需求的类型，主要对象是workflow。由于目前无内容，用户可能在计划中自动发布docker镜像时遇到问题或者寻求相关帮助。,https://github.com/hpcaitech/ColossalAI/issues/1164
ColossalAI,这是一个需求类型的issue，涉及的主要对象是Bot。这个问题是由于项目中子模块引用需要同步更新而产生的。,https://github.com/hpcaitech/ColossalAI/issues/1159
ColossalAI,这是一个需求类型的issue， 主要涉及ColossalAI中的图构建。对图构建的改进可能由于当前功能不够强大或者性能不佳而导致用户需求更好的图构建功能。,https://github.com/hpcaitech/ColossalAI/issues/1157
ColossalAI,这个issue是关于需求的，主要对象是针对ColossalAI中的Tensor模块下的embedding bag操作的新增需求。,https://github.com/hpcaitech/ColossalAI/issues/1156
ColossalAI,这是一个用户提出需求的issue， 主要对象是ColossalAI的tensor模块。,https://github.com/hpcaitech/ColossalAI/issues/1155
ColossalAI,这是一个功能需求类型的issue，主要涉及ColoTensor的init函数改进，与ColoTensors基本功能相关。,https://github.com/hpcaitech/ColossalAI/issues/1150
ColossalAI,这个issue属于功能需求提出，主要对象是ColoTensor，用户发现ColoTensor缺少一些基本功能性。,https://github.com/hpcaitech/ColossalAI/issues/1149
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是ColossalAI库中的tensor，用户希望为tensor spec添加`__repr__`方法和并行操作，以改善调试体验。,https://github.com/hpcaitech/ColossalAI/issues/1147
ColossalAI,这是一个功能需求的issue，主要涉及ColoDDP和ZeroDDP的重构。,https://github.com/hpcaitech/ColossalAI/issues/1146
ColossalAI,这是一个涉及工作流的功能需求，主要对象是发布管理流程。,https://github.com/hpcaitech/ColossalAI/issues/1144
ColossalAI,这个issue属于功能改进类型，涉及持续集成（CI）脚本自动生成发布文本内容，原因可能是为了简化发布流程和提高效率。,https://github.com/hpcaitech/ColossalAI/issues/1142
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalAI中的pipeline功能。,https://github.com/hpcaitech/ColossalAI/issues/1139
ColossalAI,这个issue类型是用户提出需求，该问题单涉及的主要对象是ColossalAI的Pipeline功能。由于目前Pipeline功能不够灵活，用户希望支持更多灵活的Pipeline操作，希望在该部分进行改进。,https://github.com/hpcaitech/ColossalAI/issues/1138
ColossalAI,这是一个功能需求类型的issue，涉及到ColossalAI中的优化（optim）模块，主要讨论了fused SGD对ColoDDPV2的支持范围进行了扩展。,https://github.com/hpcaitech/ColossalAI/issues/1134
ColossalAI,这个issue是用户提出的需求类型，主要涉及的对象是ColossalAI的pipeline schedule。由于用户想要添加双流功能到pipeline schedule，可能是为了实现同时处理两个数据流的需求。,https://github.com/hpcaitech/ColossalAI/issues/1130
ColossalAI,这个issue类型是功能需求，主要对象是ColossalAI的子模块引用同步。由于项目需要自动化更新子模块提交，所以提出了这个功能需求。,https://github.com/hpcaitech/ColossalAI/issues/1129
ColossalAI,这是一个用户提出需求的issue，涉及ColoDDP模块，用户提出了添加保存和加载状态字典的功能，以方便使用。,https://github.com/hpcaitech/ColossalAI/issues/1127
ColossalAI,这个issue类型是用户提出需求，主要对象是ColossalAI项目的pipeline模块，用户想要支持List of Dict数据作为输入。,https://github.com/hpcaitech/ColossalAI/issues/1125
ColossalAI,这是一个用户需求提出类型的issue，主要对象是ColoDDPV2，用户提出需要为ColoDDP实现支持_params_and_buffers_to_ignore_for_model方法。,https://github.com/hpcaitech/ColossalAI/issues/1122
ColossalAI,这是一个功能需求类型的issue，主要涉及到ColossalAI中的前端代码。,https://github.com/hpcaitech/ColossalAI/issues/1121
ColossalAI,这是一个功能需求。它要求ColossalAI的gemini mgr支持“cpu”放置策略。,https://github.com/hpcaitech/ColossalAI/issues/1118
ColossalAI,这是一个功能增强类型的issue，主要涉及ColossalAI中新增的coloproxy功能。,https://github.com/hpcaitech/ColossalAI/issues/1115
ColossalAI,该issue类型为用户提出需求，主要涉及到如何设置lr_scheduler的总步数以及在启用梯度累积时如何进行设置。这可能是因为用户在使用ColossalAI时遇到了关于梯度积累和学习率调度器的问题，需要指导和解决。,https://github.com/hpcaitech/ColossalAI/issues/1112
ColossalAI,该issue类型为用户需求，主要对象是ColossalAI中的trainer mode，用户希望定制epoch执行过程。,https://github.com/hpcaitech/ColossalAI/issues/1111
ColossalAI,这个issue类型是文档建议，主要对象是log_to_file函数的使用说明不清晰，导致用户难以学习如何使用。,https://github.com/hpcaitech/ColossalAI/issues/1109
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中的pipeline并行训练模块，用户提出了希望支持更灵活的数据流控制以及更多的数据格式支持的需求。,https://github.com/hpcaitech/ColossalAI/issues/1108
ColossalAI,这是一个用户提出需求的issue，涉及主要对象是ColoDDPV2模块，用户希望在混合并行计算中排除某些参数并应用自定义的并行方法。,https://github.com/hpcaitech/ColossalAI/issues/1107
ColossalAI,这是一个需求类的issue，主要涉及的对象是Submodule References。由于需要自动化更新子模块的提交，可能是为了确保主模块与子模块同步更新，以避免版本冲突或代码错误。,https://github.com/hpcaitech/ColossalAI/issues/1105
ColossalAI,这是一个更新版本号的issue，类型为发布请求，涉及对象为版本号更新。,https://github.com/hpcaitech/ColossalAI/issues/1103
ColossalAI,这是一个改进型issue，涉及的主要对象是ColossalAI项目的CI测试环境。通过新增CI测试来解决由于4GPU机器限制导致部分测试被跳过的问题，以及为了保证夜间实验不受影响而添加的自动执行于午夜的8GPU测试。,https://github.com/hpcaitech/ColossalAI/issues/1099
ColossalAI,这个issue类型是文档改进，主要对象是ColossalAI中的`chunk.py`和chunk manager。缺乏文档可能使代码难以阅读和理解。,https://github.com/hpcaitech/ColossalAI/issues/1094
ColossalAI,"这是一个功能需求报告，该问题涉及主要对象是Gemini模块。这个问题是由于目前只支持`placement_policy=""cuda""`，而不支持""cpu""和""auto""，导致用户期待支持其他两种模式而提出的需求。",https://github.com/hpcaitech/ColossalAI/issues/1093
ColossalAI,这是一个功能需求的issue，主要涉及到测试运行环境中GPU数量的限制，并提出了自动执行测试的解决方案。,https://github.com/hpcaitech/ColossalAI/issues/1090
ColossalAI,该issue为用户提出的需求，主要涉及的对象是ColossalAI中的ColoTensor模块。由于当前ColoTensor模块没有支持懒初始化，用户提出了实现懒初始化的方法，并描述了具体的改进内容和原因。,https://github.com/hpcaitech/ColossalAI/issues/1088
ColossalAI,这是一个用户提出需求的issue，主要涉及到ColossalAI下的一个子模块引用同步问题。,https://github.com/hpcaitech/ColossalAI/issues/1081
ColossalAI,该issue类型为功能改进，主要对象是对内存使用进行监控。该功能改进是为了更容易地监测内存使用情况。,https://github.com/hpcaitech/ColossalAI/issues/1077
ColossalAI,这是一个用户提出需求的issue，主要针对Tensor中1d row embedding的实现。,https://github.com/hpcaitech/ColossalAI/issues/1075
ColossalAI,这是一个用户提出需求的类型，主要涉及到ColossalAI库中的nn.parallel模块。根据标题可知用户希望添加一个名为nn.parallel的模块。,https://github.com/hpcaitech/ColossalAI/issues/1068
ColossalAI,这是一个关于模块组织优化的提案，主要涉及到ColossalAI中的一些模块组织问题，导致循环导入错误和混乱情况。,https://github.com/hpcaitech/ColossalAI/issues/1067
ColossalAI,该issue属于需求提出类型，涉及主要对象为ColossalAI库下的tensor目录结构。由于需要避免可能的循环导入，需要将模块、_ops和optimizer相关代码移动到colossalai.nn目录中重新组织tensor目录结构。,https://github.com/hpcaitech/ColossalAI/issues/1062
ColossalAI,这是一个功能需求类型的issue，涉及主要对象为ColossalAI中的Tensor模块。通过描述中的内容分析，用户提出了需要添加演示、修复bug以及优化colomodule和buffer设备移动的需求。,https://github.com/hpcaitech/ColossalAI/issues/1059
ColossalAI,这个issue类型为功能更新请求，涉及ColossalAI中的collective ops API。由于数据缺失或不统一的原因，用户提出需要对collective ops API进行更新。,https://github.com/hpcaitech/ColossalAI/issues/1054
ColossalAI,这是一个功能需求报告，主要涉及ColossalAI中的chunk manager，需添加对块大小的搜索功能。,https://github.com/hpcaitech/ColossalAI/issues/1052
ColossalAI,这个issue类型属于功能改进，主要涉及pipeline中ppschedule模块的优化，原因是需要支持tensor list。,https://github.com/hpcaitech/ColossalAI/issues/1050
ColossalAI,这个issue类型属于功能需求提案，主要对象是ColossalAI仓库中的子模块引用，希望实现自动化更新子模块提交的PR。,https://github.com/hpcaitech/ColossalAI/issues/1049
ColossalAI,这是一个更新版本号的issue，主要对象是代码库中的版本文件，由于需要更新版本号导致的。,https://github.com/hpcaitech/ColossalAI/issues/1048
ColossalAI,这个issue类型是需求提出，该问题单涉及的主要对象是ColossalAI下的模块。由于缺少推理子模块路径，用户提出需要添加推理子模块。,https://github.com/hpcaitech/ColossalAI/issues/1047
ColossalAI,"这是一个功能需求的issue，主要对象是ColossalAI的ColoTensor。由于缺乏名为""zero""的优化器，用户提出需要为ColoTensor添加该优化器的功能需求。",https://github.com/hpcaitech/ColossalAI/issues/1046
ColossalAI,这个issue类型是需求提出，该问题单涉及的主要对象是ColossalAI项目下的模型推断功能。由于用户需要新增推断功能，所以提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/1044
ColossalAI,这是一个用户提出需求的issue，主要针对ColossalAI的开发者，用户希望添加Colab教程页面以便用户更轻松地尝试ColossalAI的功能。,https://github.com/hpcaitech/ColossalAI/issues/1039
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI中预训练模型的实例化问题，由于ZeroInitContext参数导致无法直接使用预训练checkpoint加载模型。,https://github.com/hpcaitech/ColossalAI/issues/1038
ColossalAI,该issue为用户提出了关于新功能的需求，主要涉及ColossalAI中的Diffusion Model，用户希望描述Latent Diffusion特性。,https://github.com/hpcaitech/ColossalAI/issues/1037
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及ColossalAI中的feature styleganXL。可能是由于最近发布的代码简化以及组合了stylegan3和stylegan2ada的通用训练架构，用户希望在案例中使用该功能。,https://github.com/hpcaitech/ColossalAI/issues/1036
ColossalAI,这是一个功能需求的issue，主要涉及到同步子模块引用，由于需要自动化地更新子模块提交而提出。,https://github.com/hpcaitech/ColossalAI/issues/1034
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的默认参数设置。用户遇到了训练GPT2模型时无法更改最大步数等参数的问题，请求帮助解决。,https://github.com/hpcaitech/ColossalAI/issues/1032
ColossalAI,该issue属于功能新增，主要涉及ColossalAI下的Tensor模块，用户提出了添加ColoEmbedding、Bert测试以及检查不同层共享参数的要求。,https://github.com/hpcaitech/ColossalAI/issues/1031
ColossalAI,"这是一个用户提出需求的 issue，主要对象是""P2P""模块，需要添加对象列表的发送/接收功能。",https://github.com/hpcaitech/ColossalAI/issues/1024
ColossalAI,这是一个特性请求，主要涉及对象为Tensor模块的linear处理器，用户提出了添加模块规范函数、添加ColoLinear作为示例以及添加单元测试的需求。,https://github.com/hpcaitech/ColossalAI/issues/1021
ColossalAI,这是一个文档更新类型的issue，涉及主要对象是Docker指令的使用。更新可能是因为之前的指令过时或不完整，导致用户在使用时遇到困惑或错误。,https://github.com/hpcaitech/ColossalAI/issues/1020
ColossalAI,这是一个用户提交的需求类型的issue，涉及更新docker镜像名称，由于docker镜像现在统一在官方docker hub中集中管理，相关docker文件也放置在此处。,https://github.com/hpcaitech/ColossalAI/issues/1017
ColossalAI,这是一个用户提出需求类型的issue，主要涉及的对象是ColossalAI中的图模块。可能由于缺乏图模块导致用户无法处理相关领域的需求。,https://github.com/hpcaitech/ColossalAI/issues/1016
ColossalAI,这个issue类型是功能需求提出，主要涉及ColoTensor支持ZeRo时参数`chunk_size=None`的用法问题。由于引入了ZeRo支持，用户需要了解如何正确使用参数`chunk_size=None`。,https://github.com/hpcaitech/ColossalAI/issues/1015
ColossalAI,这个issue是关于代码风格的修改请求，类型为代码优化需求，主要涉及ColossalAI中的CUDA前端C代码。,https://github.com/hpcaitech/ColossalAI/issues/1010
ColossalAI,这是一个需求特性实现的issue，主要涉及ColoTensor的ColoDDP实现和`test_gpt`的并行测试重构。,https://github.com/hpcaitech/ColossalAI/issues/1009
ColossalAI,这个issue类型是改进建议，主要对象是ColossalAI中的tensor模块。由于需要重构并行操作的相关代码，以提升效率和可维护性，因此提出了这个建议。,https://github.com/hpcaitech/ColossalAI/issues/1007
ColossalAI,这个issue类型是需求提出，主要涉及的对象是Tensor模块。由于需要添加测试检查点和移除旧ColoTensor的技巧，用户可能寻求测试相关的帮助或反馈。,https://github.com/hpcaitech/ColossalAI/issues/1004
ColossalAI,该issue类型为自动化功能需求，涉及到子模块引用同步，原因是需要自动化更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/1003
ColossalAI,这是关于代码优化的修改，属于bug报告类型，主要涉及的对象是代码中未使用的导入项。这个问题的原因是未使用的导入项可能会导致导入错误。,https://github.com/hpcaitech/ColossalAI/issues/1001
ColossalAI,这个issue类型是代码风格优化，主要对象是ColossalAI的CUDA前端文件 colossal_C_frontend.c。由于代码风格不符合标准，用户提出需要对代码进行优化。,https://github.com/hpcaitech/ColossalAI/issues/1000
ColossalAI,这是一个用户提出需求的类型，主要关注对象是TPU支持。用户询问是否ColossalAI项目支持TPU。,https://github.com/hpcaitech/ColossalAI/issues/989
ColossalAI,这个issue是关于代码风格的改进，类型为代码质量优化，涉及到ColossalAI库中的nn/layer/utils/common.py文件。原因可能是为了提高代码可读性或统一风格。,https://github.com/hpcaitech/ColossalAI/issues/983
ColossalAI,这是一个用户提出版本更新的issue，涉及的主要对象是ColossalAI库。,https://github.com/hpcaitech/ColossalAI/issues/982
ColossalAI,这个issue类型为功能需求，涉及主要对象为ColossalAI的更新结果和安装操作。,https://github.com/hpcaitech/ColossalAI/issues/981
ColossalAI,该issue类型为代码风格优化，单涉及的主要对象是ColossalAI项目中的cuda_native/layer_norm.py文件，用户提出了优化代码风格的需求。,https://github.com/hpcaitech/ColossalAI/issues/980
ColossalAI,这个issue类型是代码风格优化，主要涉及ColossalAI下的CUDA内核代码，用户提出需要对dropout_kernels.cu代码风格进行整理。,https://github.com/hpcaitech/ColossalAI/issues/979
ColossalAI,这是关于代码风格的修改建议问题，涉及的主要对象是ColossalAI的CUDA多张量SGD优化核心代码，用户提出了改进该代码的代码风格的建议。,https://github.com/hpcaitech/ColossalAI/issues/978
ColossalAI,这是一个关于代码风格优化的issue，主要对象是ColossalAI下的一个2D并行层的代码文件(layers.py)，用户提出需要优化代码风格的需求。,https://github.com/hpcaitech/ColossalAI/issues/976
ColossalAI,该issue为代码风格优化（代码修饰）类型，主要涉及ColossalAI中的CUDA原生代码文件`multi_tensor_scale_kernel.cu`，由于代码风格不符合规范导致需要对其进行修改。,https://github.com/hpcaitech/ColossalAI/issues/975
ColossalAI,这个issue类型是代码风格优化，主要涉及的对象是ColossalAI的CUDA模块中的layer_norm_cuda.cpp文件，用户提出了优化代码风格的需求。,https://github.com/hpcaitech/ColossalAI/issues/973
ColossalAI,这是一个代码风格优化的issue，主要涉及到ColossalAI项目中的神经网络模块中的某些代码风格问题。,https://github.com/hpcaitech/ColossalAI/issues/972
ColossalAI,这是一个功能需求类型的issue，主要涉及到计算模式的推导和移除。,https://github.com/hpcaitech/ColossalAI/issues/971
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI库中的nn/layer/parallel_2p5d/layers.py代码。,https://github.com/hpcaitech/ColossalAI/issues/969
ColossalAI,这是一个关于代码风格（code style）的需求问题，涉及 ColossalAI 的 CUDA 核心模块下的 feed_forward.h 文件。,https://github.com/hpcaitech/ColossalAI/issues/968
ColossalAI,这个issue是关于代码风格优化的请求，涉及ColossalAI下的nn/layer/parallel_3d/layers.py文件。,https://github.com/hpcaitech/ColossalAI/issues/966
ColossalAI,这个issue类型是代码优化，该问题单涉及的主要对象是ColossalAI的`__init__.py`文件。原因可能是代码风格不符合规范，需要有人对其进行优化。,https://github.com/hpcaitech/ColossalAI/issues/965
ColossalAI,这个issue类型为代码风格优化，涉及的主要对象是ColossalAI项目中的CUDA原生代码。原因可能是为了代码风格一致性或者更清晰易读的目的。,https://github.com/hpcaitech/ColossalAI/issues/964
ColossalAI,这是一个类型为代码质量优化的issue，主要涉及ColossalAI项目中CUDA内核的代码样式。由于代码样式不符合规范导致需要进行整理优化。,https://github.com/hpcaitech/ColossalAI/issues/963
ColossalAI,这个issue类型是功能需求，主要涉及的对象是同步子模块引用，用户提出了自动化更新子模块提交的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/960
ColossalAI,这个issue是关于代码风格的优化，而不是bug报告或用户需求。,https://github.com/hpcaitech/ColossalAI/issues/957
ColossalAI,这是一个代码风格优化（NFC）类型的issue，主要涉及ColossalAI项目中cuda_native模块下的context.h文件，通过细节调整代码风格以提升可读性和一致性。,https://github.com/hpcaitech/ColossalAI/issues/956
ColossalAI,这是一个代码风格优化（NFC）类型的issue，主要涉及ColossalAI中的cuda_native模块下的scaled_softmax.py文件的代码风格问题。,https://github.com/hpcaitech/ColossalAI/issues/955
ColossalAI,这是一个关于代码风格的修改建议，非bug报告类型。,https://github.com/hpcaitech/ColossalAI/issues/953
ColossalAI,这是一个代码风格优化的issue，涉及到ColossalAI库中的builder/pipeline.py代码文件。,https://github.com/hpcaitech/ColossalAI/issues/951
ColossalAI,这是一个功能需求提出的issue，主要涉及ColossalAI中的Tensor模块，用户需求在loss和test_model中添加DistSpec功能。,https://github.com/hpcaitech/ColossalAI/issues/947
ColossalAI,这是一个代码风格优化（code style）的issue，主要涉及ColossalAI项目中的kernel/jit/bias_gelu.py文件。,https://github.com/hpcaitech/ColossalAI/issues/946
ColossalAI,这是一个代码风格优化类型的issue，主要涉及ColossalAI下的cuda_native模块中的scaled_upper_triang_masked_softmax_cuda.cu文件的代码风格问题。,https://github.com/hpcaitech/ColossalAI/issues/943
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI下的moe_cuda.cpp文件。,https://github.com/hpcaitech/ColossalAI/issues/942
ColossalAI,这是一个需要代码风格优化的issue，主要涉及的对象是ColossalAI中的moe_cuda_kernel.cu文件。原因可能是为了提高代码可读性和维护性。,https://github.com/hpcaitech/ColossalAI/issues/940
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI中cuda_native模块下的cpu_adam.cpp文件。,https://github.com/hpcaitech/ColossalAI/issues/936
ColossalAI,这是一个请求不要批准或合并的类型，涉及的对象是某项测试，原因可能是测试尚未完成或存在问题。,https://github.com/hpcaitech/ColossalAI/issues/935
ColossalAI,这个issue类型是用户提出需求，主要对象是ColoTensor的DistSpec和DistSpecManager。由于需要设计新的DistSpec和DistSpecManager，并更新当前的操作，同时跳过`test_model.py`测试，可能是为了提高ColoTensor的分布式计算性能或功能。,https://github.com/hpcaitech/ColossalAI/issues/934
ColossalAI,这个issue类型为功能需求，主要对象是ColossalAI下的Tensor模块，用户提出了需要向BERT测试添加一个优化器的需求。,https://github.com/hpcaitech/ColossalAI/issues/933
ColossalAI,这是一个特性请求，主要涉及到自动化更新子模块提交的功能。这个问题可能由于项目中子模块引用需要定期同步更新而提出。,https://github.com/hpcaitech/ColossalAI/issues/932
ColossalAI,该issue属于功能需求类，涉及的主要对象是代码仓库中的子模块引用。,https://github.com/hpcaitech/ColossalAI/issues/929
ColossalAI,这个issue是一个功能需求，主要涉及的对象是设置操作的ComputePattern而不是参数。原因是某些模型（如GPT2）在不同层之间共享参数，导致在训练过程中参数可能具有不同的compute pattern。,https://github.com/hpcaitech/ColossalAI/issues/926
ColossalAI,这是一个功能需求的issue，该问题涉及主要对象为ColossalAI的[tensor]模块。,https://github.com/hpcaitech/ColossalAI/issues/923
ColossalAI,这个issue类型是用户提出需求，主要对象是[Tensor]模块。这个需求是针对在ColossalAI中添加对pretrained模型的支持以及进行相关测试。,https://github.com/hpcaitech/ColossalAI/issues/921
ColossalAI,这是一个用户提出需求类型的issue，主要涉及的对象是ColossalAI的CUDA架构支持。,https://github.com/hpcaitech/ColossalAI/issues/920
ColossalAI,这个issue类型是功能需求，涉及的主要对象是ColossalAI库的cuda扩展功能。原因是因为当前版本不支持sm70架构，导致用户无法使用最新的cuda扩展。,https://github.com/hpcaitech/ColossalAI/issues/919
ColossalAI,这是一个功能需求的issue，主要涉及对象是ColossalAI中的Tensor模块，需要添加1d vocab loss功能，并在simple_net中添加测试。这个需求可能是为了改进模型训练和测试的功能性。,https://github.com/hpcaitech/ColossalAI/issues/918
ColossalAI,这是一个建议性的issue，关于ColossalAI中的图计算相关内容，主要是关于如何减少代码冗余以及提出程序设计方面的建议。,https://github.com/hpcaitech/ColossalAI/issues/917
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的Tensor，由于需要区分模型数据和非模型数据张量，故提出了初始化ColoParameter类的建议。,https://github.com/hpcaitech/ColossalAI/issues/914
ColossalAI,这是一个功能需求类型的issue，涉及的主要对象是一个自动化机器人（Bot），原因是用户希望通过自动化PR来更新子模块的提交。,https://github.com/hpcaitech/ColossalAI/issues/912
ColossalAI,该issue属于需求提出，主要对象是在ColossalAI中添加一个基本的Huggingface BERT。由于目前尚未替换为1D ColoTensor，因此需要进一步完善。,https://github.com/hpcaitech/ColossalAI/issues/911
ColossalAI,这是一个关于软件版本本地更新格式的issue，属于功能需求类型，主要涉及ColossalAI库的版本控制机制。,https://github.com/hpcaitech/ColossalAI/issues/909
ColossalAI,该issue为用户提出需求类型，主要涉及ColossalAI是否计划支持OPT模型，可能由于用户希望ColossalAI展示独特优势而提出该需求。,https://github.com/hpcaitech/ColossalAI/issues/908
ColossalAI,这个issue类型是功能需求，涉及主要对象为Github项目中的子模块引用同步，因为需要编写自动化的PR来更新子模块的提交信息。,https://github.com/hpcaitech/ColossalAI/issues/907
ColossalAI,这个issue是一个功能需求类型的问题，主要涉及到ColossalAI项目中的构建脚本。由于需要自动构建ColossalAI与各种torch和python版本，所以开发人员添加了与torch、cuda和python版本相关的构建脚本。,https://github.com/hpcaitech/ColossalAI/issues/905
ColossalAI,这个issue是一个功能需求提案，主要涉及Tensor中添加embedding功能，根据描述可能是为了完善模型简化网络测试。,https://github.com/hpcaitech/ColossalAI/issues/904
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI下的pipelinable功能对GPT模型的支持。,https://github.com/hpcaitech/ColossalAI/issues/903
ColossalAI,这是一个版本更新的请求，主要对象是ColossalAI项目。,https://github.com/hpcaitech/ColossalAI/issues/900
ColossalAI,这是一个功能需求的issue，主要对象是ColossalAI库中的Tensor模块，用户希望增加ColoTensor TP1Dcol Embedding功能和对应的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/899
ColossalAI,这是一个需求类型的issue，该问题涉及的主要对象是Tensor模块，问题在于需要初始化ColoOptimizer。,https://github.com/hpcaitech/ColossalAI/issues/898
ColossalAI,这个issue属于用户提出需求类型，涉及主要对象为ColossalAI项目的配置模式。问题提出是为了添加验证和一致性的配置模式，以确保项目的规范性和正确性。,https://github.com/hpcaitech/ColossalAI/issues/895
ColossalAI,这是一个功能需求的issue，主要涉及对象是ShardedModelV2模型，用户提出需要支持`state_dict()`和`load_state_dict()`方法。,https://github.com/hpcaitech/ColossalAI/issues/894
ColossalAI,这是一个功能需求类的issue，涉及主要对象为ColoTensor。原因可能是需要在ColossalAI的[tensor]模块中添加gather()和ShardPattern，以及对linear代码进行优化，并为layernorm添加gather功能。,https://github.com/hpcaitech/ColossalAI/issues/893
ColossalAI,"这是一个""需求提议""类型的issue，主要涉及项目中的配置管理，并提出了通过引入`schema`模块来规范配置说明，以解决用户配置错误导致的问题，请教问题。",https://github.com/hpcaitech/ColossalAI/issues/892
ColossalAI,这是一个需求提出的issue，主要对象是ColossalAI库中的Tensor模块，用户提出要添加ColoTensor 1Dcol功能，并增加相应的单元测试和将gather_out功能添加到ParallelAction中。由于当前功能不包含此功能，用户请求增加新功能以提升库的灵活性和功能性。,https://github.com/hpcaitech/ColossalAI/issues/888
ColossalAI,这是一个需求类型的issue，主要涉及的对象是在ColossalAI下的torch_tensor。这个issue的提出可能是为了将torch_tensor中的函数包装到ColoTensor中，并添加单元测试，修复__torch_functions__中的返回问题。,https://github.com/hpcaitech/ColossalAI/issues/881
ColossalAI,"这是一个用户提出需求的类型，主要涉及的对象是在ColossalAI下的一个名为""[Tensor] make a simple net works with 1D row TP""的简单网。",https://github.com/hpcaitech/ColossalAI/issues/879
ColossalAI,这是一个功能优化建议类型的issue，主要涉及ColossalAI中gemini模块的`adjust_layout()`函数，通过缓存`move_to_cuda_tensor_list`和`hold_cuda_tensor_list`结果来提高性能。,https://github.com/hpcaitech/ColossalAI/issues/878
ColossalAI,这个issue属于用户提出需求类型，主要涉及的对象是ColoTensor。由于用户需要对ColoTensor添加一些属性，以便对huggingface bert进行一些修改，从而实现特定功能。,https://github.com/hpcaitech/ColossalAI/issues/877
ColossalAI,这是一个用户提出需求类型的 Issue，涉及主要对象是 ColossalAI 库中的自定义操作（Op）。原因是需要将 Op 的输出设置为 ColoTensor，以便更好地表示其特性。,https://github.com/hpcaitech/ColossalAI/issues/875
ColossalAI,这是一则针对文档字符串和断言消息改进的问题，属于贡献代码类型，主要涉及ColossalAI的engine模块，可能是为了提高代码可读性和debugging体验而提出的。,https://github.com/hpcaitech/ColossalAI/issues/871
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是Tensor。这个问题的原因可能是需要在TensorSpec中添加一个新函数，并更新与Tensor有关的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/869
ColossalAI,这个issue类型为功能需求，主要对象是[tensor]模块。由于缺少cross_entropy_loss功能，用户提出希望添加该功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/868
ColossalAI,该issue是需求提交类型的，主要对象是Gemini（ColossalAI的一部分）。,https://github.com/hpcaitech/ColossalAI/issues/867
ColossalAI,这是一个用户提出需求的类型，主要涉及ColossalAI中的tensor功能，由于缺乏tensor规格的初始设计，用户希望能够讨论和补充相关内容。,https://github.com/hpcaitech/ColossalAI/issues/865
ColossalAI,这是一个文档改进类的issue，主要涉及ColossalAI通信模块的文档描述和类型提示改进。,https://github.com/hpcaitech/ColossalAI/issues/863
ColossalAI,该issue属于文档改进类型，主要涉及日志(logging)模块的函数和类的文档说明改进。这是为了使它在ReadTheDocs上显示更加清晰美观。,https://github.com/hpcaitech/ColossalAI/issues/861
ColossalAI,这个issue是关于优化CI运行时，主要涉及到cuda扩展的缓存操作，不属于bug报告。,https://github.com/hpcaitech/ColossalAI/issues/860
ColossalAI,此issue类型为优化建议，主要涉及使用代理缩短CI构建时间。这个问题的原因是Docker默认使用代理，因此无需再使用清华源。,https://github.com/hpcaitech/ColossalAI/issues/859
ColossalAI,这是一个功能改进类型的issue，主要涉及微基准测试CLI的重构和新增指标，由于之前的基准测试提供了有限的指标并存在冗余代码片段，这个问题得以解决。,https://github.com/hpcaitech/ColossalAI/issues/858
ColossalAI,这是一个文档改进类的issue，主要涉及ColossalAI的amp module中相关函数和类的文档改进。原因是为了使文档在ReadTheDocs上显示更加美观。,https://github.com/hpcaitech/ColossalAI/issues/857
ColossalAI,这个issue类型是代码优化，主要涉及Gemini相关的代码。由于代码质量不佳或者效率较低，用户提出了需要对代码进行优化的需求。,https://github.com/hpcaitech/ColossalAI/issues/855
ColossalAI,该issue是一个用户提出需求的类型，主要对象是Tensor，用户希望添加1Drow weight reshard功能并更新对应的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/854
ColossalAI,这个issue属于需求提出，主要涉及ColossalAI中的ColoTensor替换dummy tensor的问题。,https://github.com/hpcaitech/ColossalAI/issues/853
ColossalAI,这个issue类型是用户提出需求，针对的对象是Tensor模块。由于缺少layer norm Op，用户希望添加这个功能到ColossalAI中。,https://github.com/hpcaitech/ColossalAI/issues/852
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI库中的Tensor模块，用户提出了需要更新nn.Linear以支持ColoTensor作为输入和输出，并在设置Spec参数时自动分片权重的需求。,https://github.com/hpcaitech/ColossalAI/issues/849
ColossalAI,这个issue类型是功能需求，主要对象是ColossalAI中的Tensor模块。,https://github.com/hpcaitech/ColossalAI/issues/843
ColossalAI,这是一个功能需求报告，主要涉及ColossalAI库中的Init Context对象，用户提出支持延迟分配模型内存的功能。,https://github.com/hpcaitech/ColossalAI/issues/842
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI下的工具（utils）中的profiler模块。由于需要重构profiler，添加新的功能和移动旧的功能到新的位置，所以提出了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/837
ColossalAI,这是一个功能需求类型的issue，主要对象是ColossalAI项目。这个issue源于ColossalAI无法在Python 3.6上安装的问题，用户提出需要让ColossalAI能够兼容Python 3.6的需求。,https://github.com/hpcaitech/ColossalAI/issues/834
ColossalAI,这是一个关于依赖管理的修改建议，不属于bug报告。该问题涉及到ColossalAI的依赖管理，提出了移除`torchvision`等库的建议。,https://github.com/hpcaitech/ColossalAI/issues/833
ColossalAI,这是一个功能增强的issue，主要涉及GeminiMemoryManager的添加和对StatefulTensor的重构。,https://github.com/hpcaitech/ColossalAI/issues/832
ColossalAI,这个issue类型为功能开发，主要涉及ColossalAI中的ColoTensor在1D行TP上的应用。由于代码实现尚未完整，等待Jzy完成详细实现，目标是让用户在不替换Linear为col.Linear的情况下使用TP，并通过定义Linear权重的shard_spec来实现自动调用并行Linear。,https://github.com/hpcaitech/ColossalAI/issues/831
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中对于1D row linear的ColoTensor应用。由于实现细节由Jzy完成，目的是让用户在不替换Linear的情况下使用TP，只需定义Linear权重的shard_spec，Colossal会自动调用并行Linear。,https://github.com/hpcaitech/ColossalAI/issues/830
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是ColossalAI中的ColoTensor类。这个问题是为了扩展当前的ColoTensor类，并使其更加多功能和适用于更多场景。,https://github.com/hpcaitech/ColossalAI/issues/828
ColossalAI,这是一个功能需求类型的issue，主要涉及到ColossalAI项目中子模块引用的同步更新。,https://github.com/hpcaitech/ColossalAI/issues/827
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI下的InsertPostInitMethodToModuleSubClasses类的重构。,https://github.com/hpcaitech/ColossalAI/issues/824
ColossalAI,这个issue是关于需求提出，主要涉及ColossalAI库中的Tensor对象，通过更新ColoTensor的torch_function来实现。,https://github.com/hpcaitech/ColossalAI/issues/822
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是ColossalAI中的ColoTensor。由于用户希望在Torch函数中应用ColoTensor，因此提出了此问题。,https://github.com/hpcaitech/ColossalAI/issues/821
ColossalAI,这个issue属于需求提出类型，主要涉及的对象是ColossalAI中的tensor模块，原因是经讨论后决定将tensor作为独立模块，并更改为ColoTensor及其相关函数名。,https://github.com/hpcaitech/ColossalAI/issues/820
ColossalAI,这个issue是关于自动化提交以更新子模块提交的，类型为功能需求，主要涉及到代码库中的子模块引用同步。可能是由于需要保持子模块引用与其最新提交同步，开发人员提出了这个功能需求。,https://github.com/hpcaitech/ColossalAI/issues/819
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI项目中的新tensor数据结构。由于需要统一tensor结构，用户提出了相关RFC，希望实现一个新的数据结构。,https://github.com/hpcaitech/ColossalAI/issues/818
ColossalAI,这是一个功能增强类型的issue，主要对象是ColossalAI的CLI工具。导致这个问题产生的原因可能是为了方便用户检查ColossalAI的构建情况而新增了相应的CLI功能。,https://github.com/hpcaitech/ColossalAI/issues/815
ColossalAI,这是一个功能需求类型的issue，涉及Gemini项目中在每次迭代中收集CPU-GPU移动的体积数据。可能由于需要实时监测数据的变化或优化算法效率等原因引起。,https://github.com/hpcaitech/ColossalAI/issues/813
ColossalAI,这个issue类型为功能需求，涉及的主要对象是同步子模块引用，用户在此提出自动化更新子模块提交的需求。,https://github.com/hpcaitech/ColossalAI/issues/810
ColossalAI,这是一个用户提出需求的issue，主要涉及Gemini模块的APIs用于设置CPU内存容量。这个问题很可能是用户想要调整CPU内存容量而缺乏相应的API支持。,https://github.com/hpcaitech/ColossalAI/issues/809
ColossalAI,这是一则用户提出需求的 issue，主要涉及的对象是ColossalAI的CLI模块。由于没有将`click`添加到所需的列表中，用户提出了需求并修改了CLI模块的代码。,https://github.com/hpcaitech/ColossalAI/issues/805
ColossalAI,这是一个重构请求，该问题主要涉及到循环导入的问题。,https://github.com/hpcaitech/ColossalAI/issues/804
ColossalAI,这个issue类型是用户提出需求，主要对象是ColossalAI项目中的pp build，用户因为需要适应ckpt而提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/803
ColossalAI,这个issue是一个功能需求，要求在日志中显示tflops信息。该需求涉及的主要对象是ColossalAI的日志系统。,https://github.com/hpcaitech/ColossalAI/issues/802
ColossalAI,这是一个关于CI流程更新的Issue，主要涉及CI/CD流程。原因可能是需要通过代理更新工作流程，以确保持续集成的顺利运行。,https://github.com/hpcaitech/ColossalAI/issues/797
ColossalAI,这是一个用户提出需求的类型的issue，主要对象是对某个提案的初步工作。由于用户愿意开始这个提案的初始工作，因此可能正在寻求确定下一步的具体步骤或者相关的指导。,https://github.com/hpcaitech/ColossalAI/issues/794
ColossalAI,这是一个功能需求的issue，主要涉及的对象是ColossalAI库中的zero元素张量分片策略。,https://github.com/hpcaitech/ColossalAI/issues/793
ColossalAI,该issue类型为功能需求，涉及ColossalAI中的Throughput hook并添加Tflops metric记录。,https://github.com/hpcaitech/ColossalAI/issues/792
ColossalAI,该issue为功能改进类型，新增了分布式启动器命令。,https://github.com/hpcaitech/ColossalAI/issues/791
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI的命令行接口。,https://github.com/hpcaitech/ColossalAI/issues/789
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是开发。由于内容为空，用户可能正在寻求开发相关的帮助或资源。,https://github.com/hpcaitech/ColossalAI/issues/788
ColossalAI,这是一个需求提出的问题，主要涉及ColossalAI中对不同设备进行张量初始化的功能。由于PyTorch 1.8不支持在GPU上直接初始化张量，目前的实现强制TensorParallel层在GPU上默认初始化。,https://github.com/hpcaitech/ColossalAI/issues/786
ColossalAI,这是一个用户需求类型的issue，主要涉及ColossalAI中的Develop2功能。,https://github.com/hpcaitech/ColossalAI/issues/785
ColossalAI,这是一个需求提出类型的issue，主要涉及到ColossalAI中的parallel layernorm optimization。由于需要优化计算variance的过程，提出了一种并行计算信息聚合的方法。,https://github.com/hpcaitech/ColossalAI/issues/782
ColossalAI,这是一个用于更新版本的issue，类型为需求提出，涉及的主要对象为ColossalAI项目。,https://github.com/hpcaitech/ColossalAI/issues/779
ColossalAI,这个issue是一个需求类型的问题，主要涉及到ColossalAI库中shard和gather操作的重构。,https://github.com/hpcaitech/ColossalAI/issues/773
ColossalAI,这是一个功能需求类型的issue，涉及的主要对象是同步子模块引用，由于需要自动化更新子模块提交。,https://github.com/hpcaitech/ColossalAI/issues/771
ColossalAI,这个issue类型是技术性改进（technical improvement），主要涉及ColossalAI的sharded optim文档以及警告信息。由于文档和警告信息需要进一步完善和细化，该issue可能涉及改进文档的可读性或者警告信息的准确性。,https://github.com/hpcaitech/ColossalAI/issues/770
ColossalAI,这个issue类型是关于文档的需求提出，主要涉及ColossalAI的配置文件和多个并行性的使用。产生这个问题的原因是为了更好地让用户了解ColossalAI可以通过在配置文件中进行少量更改来实现多并行性的使用，从而提高用户对ColossalAI易用性的理解。,https://github.com/hpcaitech/ColossalAI/issues/767
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的中文文档同步问题，可能是由于中文文档与英文文档内容不一致或落后导致的。,https://github.com/hpcaitech/ColossalAI/issues/766
ColossalAI,这个issue属于文档更新类型，涉及主要对象为README.md文件。由于内容为空，提供的信息不足以展示项目的关键信息或者完成特定任务。,https://github.com/hpcaitech/ColossalAI/issues/765
ColossalAI,这个issue类型是对README文件进行优化的建议，主要涉及的对象是ColossalAI项目的文档。,https://github.com/hpcaitech/ColossalAI/issues/764
ColossalAI,这是一则关于代码重构和替换功能的问题，涉及主要对象为ColossalAI中的`rerun_on_exception`和`rerun_if_address_is_in_use`。由于需要保持与PyTorch的向后兼容性，导致此更改。,https://github.com/hpcaitech/ColossalAI/issues/763
ColossalAI,这是一个功能需求问题，主要涉及Gemini模块和ZeRO之间的耦合问题。,https://github.com/hpcaitech/ColossalAI/issues/755
ColossalAI,这个issue类型为需求提出，主要涉及的是子模块引用同步，由于需要自动化更新子模块提交而产生。,https://github.com/hpcaitech/ColossalAI/issues/751
ColossalAI,这是一个用户提出的需求类型的issue，主要涉及LayerNorm模块，希望可以实现没有偏置项的LayerNorm。,https://github.com/hpcaitech/ColossalAI/issues/750
ColossalAI,这是一个用户需求类型的issue，涉及的主要对象是ColossalAI的CLI功能。由于缺乏CLI启动功能，用户提出了希望增加CLI启动功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/747
ColossalAI,这是一个需求类型的issue，主要涉及到将PyTorch项目与Colossal-AI集成的教程问题。,https://github.com/hpcaitech/ColossalAI/issues/745
ColossalAI,该issue类型是功能需求，主要对象是ColossalAI项目中的示例、基准和模型库。由于更新/维护/使用方式不统一以及缺乏描述，同名模型文件可能导致用户混淆，影响了用户体验和开发效率。,https://github.com/hpcaitech/ColossalAI/issues/744
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的stateful tensor manager，用户希望添加tensor的放置策略，并对相关的单元测试进行更新。,https://github.com/hpcaitech/ColossalAI/issues/743
ColossalAI,这个issue类型是功能需求，主要涉及的对象是ColossalAI的utils模块。由于需要增加一个同步的CUDA内存监视器，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/740
ColossalAI,这个issue类型为用户提出需求，涉及的主要对象是在ColossalAI下添加视频功能。,https://github.com/hpcaitech/ColossalAI/issues/732
ColossalAI,这是一条关于文档更新的issue，涉及主要对象为ColossalAI项目的安装指令。由于过时的安装命令导致了需要进行更新，引发了该issue。,https://github.com/hpcaitech/ColossalAI/issues/730
ColossalAI,这是一个功能需求的issue，主要对象是ColossalAI中的unitest，由于需要为moe zero test添加checkpoint。,https://github.com/hpcaitech/ColossalAI/issues/729
ColossalAI,这个issue类型是功能需求，涉及主要对象是ColossalAI项目中的util模块，用户提出了需要支持检测当前节点上进程数量的需求。,https://github.com/hpcaitech/ColossalAI/issues/723
ColossalAI,该issue类型为功能需求，主要涉及的对象是在内存优化中需要检测当前节点上运行的进程数量。因为目前无法准确获取节点上运行的进程数量，需要添加相应功能以解决此问题。,https://github.com/hpcaitech/ColossalAI/issues/722
ColossalAI,这是一个用户提出需求的类型。该问题单涉及的主要对象是ColossalAI项目中的导出性能数据至JSON文件以供Tensorboard插件直接访问。,https://github.com/hpcaitech/ColossalAI/issues/717
ColossalAI,该issue是关于提出需求的，主要对象是ColossalAI项目中的tensorboard功能。由于tensorboard提供的API有限，无法绘制交互式饼状图和迭代级别的内存使用图表，需要开发自己的tensorboard插件管理器来解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/716
ColossalAI,该issue类型为用户提出需求，主要对象是ParallelContext中的进程共享CPU，由于在混合并行计算中，用户想了解有多少个进程在共享同一个CPU。,https://github.com/hpcaitech/ColossalAI/issues/714
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI需要展示CUDA和Torch版本的示例。这个需求可能由于Colossal要求使用torch>=1.8，需要cu111最小版本的CUDA，因此提出希望添加一个展示CUDA和Torch版本的Colab示例，并建议添加CUDA版本检查。,https://github.com/hpcaitech/ColossalAI/issues/713
ColossalAI,这个issue类型是功能需求，涉及主要对象是ColossalAI中的参数处理功能。由于需要提高不共享参数的适应性，导致了需要在后向传播钩子、优化器和梯度处理方面进行适应性改进。,https://github.com/hpcaitech/ColossalAI/issues/708
ColossalAI,这个issue类型是功能需求，主要涉及ColossalAI的MemStatsCollector的重构和新增功能。,https://github.com/hpcaitech/ColossalAI/issues/706
ColossalAI,该issue属于用户提出需求类型，主要涉及的对象是添加PaLM链接。由于原因未明，内容为空，导致用户提出了添加PaLM链接的需求。,https://github.com/hpcaitech/ColossalAI/issues/705
ColossalAI,该issue是用户提出需求，主要对象是ColossalAI项目。由于没有提供具体的信息或者内容，无法确定用户具体需要添加什么样的PaLM链接。,https://github.com/hpcaitech/ColossalAI/issues/704
ColossalAI,这是一个用户提出需求类型的issue，主要对象是ColossalAI库中的梯度checkpoint功能。导致用户提出需求的原因是希望梯度checkpoint函数支持传递一个字典作为参数，以提高用户的使用便利性。,https://github.com/hpcaitech/ColossalAI/issues/703
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalAIExamples里的Swish Transformer 的示例。,https://github.com/hpcaitech/ColossalAI/issues/702
ColossalAI,这是一个用户提出需求的issue，主要涉及添加CPU/Hybrid Adafactor支持的功能。,https://github.com/hpcaitech/ColossalAI/issues/697
ColossalAI,这是一个功能需求提出的issue，主要涉及的对象是Bot，由于需要自动化更新子模块的提交而提出。,https://github.com/hpcaitech/ColossalAI/issues/695
ColossalAI,该issue类型为CI工作流程相关的需求更新，涉及主要对象为CI工作流程和选项列表。由于更新后选项列表仅当PR合并到主分支时才有效，但在`hotfix/cicompatibility`分支手动触发工作流程时仍能正常工作。,https://github.com/hpcaitech/ColossalAI/issues/691
ColossalAI,这是一个用户提出需求的类型，主要对象是ColossalAI项目中与兼容性测试CI相关的功能。由于之前的兼容性测试需要运行所有PyTorch版本，耗时可能超过1小时，所以需要添加一个选项列表，让执行者选择只运行特定版本的测试。,https://github.com/hpcaitech/ColossalAI/issues/690
ColossalAI,这是一个用户提出需求类的issue，主要涉及到RL example特性。原因可能是用户希望添加一个强化学习示例。,https://github.com/hpcaitech/ColossalAI/issues/685
ColossalAI,这个issue类型是需求提出， 主要对象是Stateful OS tensors， 用户提出了关于动态管理CPUGPU空间中的OS tensor的需求。,https://github.com/hpcaitech/ColossalAI/issues/681
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及ColossalAI下的pipeline schedule参数推断，由于用户希望能够根据配置文件推断参数并自动构建调度。,https://github.com/hpcaitech/ColossalAI/issues/680
ColossalAI,这是一个用户提出的需求。该问题涉及的主要对象是ColossalAI的pipeline模块。由于用户需要从用户配置文件中推断管道调度参数，例如 tensor_shape 或 scatter_gather_tensors，因此用户希望对管道进行重构。,https://github.com/hpcaitech/ColossalAI/issues/679
ColossalAI,这是一个类型为更新版本的issue，涉及主要对象为ColossalAI。  由于需要更新版本，导致该问题的需求是更新软件的版本。,https://github.com/hpcaitech/ColossalAI/issues/673
ColossalAI,这是一个功能需求的issue，主要对象是ColossalAI中的DistributedDataParallel（DDP），用户希望在初始化时能够传递关键参数来自定义DDP的参数设置。,https://github.com/hpcaitech/ColossalAI/issues/670
ColossalAI,这个issue属于实验结果更新，是一种非bug报告类型，主要涉及的对象是GPT-2实验结果。可能是由于之前结果的不完善或者需要更新，导致用户希望修复或更新现有结果。,https://github.com/hpcaitech/ColossalAI/issues/666
ColossalAI,这个issue属于用户提出需求的类型，主要涉及的对象是ColossalAI。因为ColossalAI性能有所提升，用户期待看到更新。,https://github.com/hpcaitech/ColossalAI/issues/665
ColossalAI,这是一个对代码风格进行改进的issue，主要涉及ColossalAI代码库中的builder.py文件。,https://github.com/hpcaitech/ColossalAI/issues/662
ColossalAI,这是一个代码风格优化的issue，主要对象是ColossalAI中的cuda_native模块，可能由于代码风格不规范导致阅读困难。,https://github.com/hpcaitech/ColossalAI/issues/661
ColossalAI,这是一个用户提出需求的 issue，主要涉及到 activation checkpoint，问题是关于测试 offload 的正确方式。,https://github.com/hpcaitech/ColossalAI/issues/660
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAIExamples下的一个hybrid_parallel示例。由于大多数用户没有64个GPU进行测试配置，希望提供使用更少资源的便捷示例。,https://github.com/hpcaitech/ColossalAI/issues/658
ColossalAI,这个issue类型是代码风格规范优化，主要涉及的对象是ColossalAI通信模块的utils.py文件，用户提出了优化代码风格的需求。,https://github.com/hpcaitech/ColossalAI/issues/656
ColossalAI,这个issue类型是需求提交，涉及主要对象是ColossalAI文档。这个问题由于文档不完整而导致用户无法定义自己的并行模型。,https://github.com/hpcaitech/ColossalAI/issues/654
ColossalAI,这个issue类型为用户提出需求，该问题单涉及的主要对象是在GitHub上的ColossalAI，用户在提出需求时想给issue模板添加前缀和标签。,https://github.com/hpcaitech/ColossalAI/issues/652
ColossalAI,这是一个请求不要批准或合并代码的issue，类型为请求类，主要对象是代码审查和合并人员。由于可能存在代码问题或其他潜在风险，请求不要批准或合并，可能是为了保障代码质量和稳定性。,https://github.com/hpcaitech/ColossalAI/issues/646
ColossalAI,这是一个代码风格优化的issue，主要涉及ColossalAI库中的process_group_initializer模块。原因可能是为了保持代码风格一致性而提出。,https://github.com/hpcaitech/ColossalAI/issues/639
ColossalAI,这个issue类型是代码风格优化，主要对象是ColossalAI下的builder/pipeline.py文件，用户提出了希望优化代码风格的需求。,https://github.com/hpcaitech/ColossalAI/issues/638
ColossalAI,这个issue是一个文档优化类型的问题，主要涉及ColossalAI库中的checkpoint功能模块。,https://github.com/hpcaitech/ColossalAI/issues/637
ColossalAI,这是关于代码风格的修改请求，主要对象是ColossalAI项目中的cuda_native模块中的cpu_adam.cpp文件。,https://github.com/hpcaitech/ColossalAI/issues/636
ColossalAI,该问题类型为用户提出需求，关注的主要对象是Loki-Promtail监控配置。该问题由于用户需要添加与Loki-Promtail相关的监控配置而提出。,https://github.com/hpcaitech/ColossalAI/issues/632
ColossalAI,这是一个代码风格优化的issue，主要涉及到ColossalAI的引擎基类_base_engine.py文件。这个问题由于代码风格不够规范，需要进行整理和优化。,https://github.com/hpcaitech/ColossalAI/issues/631
ColossalAI,这是一个代码风格优化（Code Style Polish）的Issue，主要涉及ColossalAI中的communication/ring.py文件。,https://github.com/hpcaitech/ColossalAI/issues/630
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI工程中的pipeline和engine模块。,https://github.com/hpcaitech/ColossalAI/issues/627
ColossalAI,这个issue类型为功能需求，涉及的主要对象是ColossalAI框架。由于缺少关于tensor parallel输入的检查，用户提出需要添加这个功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/621
ColossalAI,这是一个文档更新类型的issue，主要涉及ColossalAI的文档内容。由于缺少具体内容，可能是用户提出了文档更新的需求或者报告了文档内容不完整的问题。,https://github.com/hpcaitech/ColossalAI/issues/615
ColossalAI,这是一个用户提出需求的issue，涉及对象是ColossalAI中的zero模块，可能由于需要实现CC中RFC中的heterogenous training相关功能而提出。,https://github.com/hpcaitech/ColossalAI/issues/614
ColossalAI,该issue类型为功能需求类型，涉及主要对象为Checkpoint测试，用户提出了增加单位测试和更新.gitignore文件的需求。,https://github.com/hpcaitech/ColossalAI/issues/599
ColossalAI,这是一个功能更新的issue，主要涉及到ColossalAI中的模型检查点功能。,https://github.com/hpcaitech/ColossalAI/issues/598
ColossalAI,该issue类型是功能需求提出，主要涉及ColossalAI中的3D保存/加载功能，问题提出者对3D图层的存储和加载进行了更新操作。,https://github.com/hpcaitech/ColossalAI/issues/597
ColossalAI,该issue属于功能需求类型，主要涉及ColossalAI的2.5D图层的保存和加载更新。由于原功能可能存在缺陷或需要优化，用户提出了更新该功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/596
ColossalAI,这是一则特性需求，主要涉及ColossalAI库中的2D图层的保存和加载功能更新。由于需要增强2D图层的持久化操作，导致更新了保存和加载功能。,https://github.com/hpcaitech/ColossalAI/issues/595
ColossalAI,这是一个功能需求的issue，主要涉及1D层的保存和加载更新。这个问题可能是为了改进ColossalAI库中1D层的功能性和性能。,https://github.com/hpcaitech/ColossalAI/issues/594
ColossalAI,该issue类型为功能优化，主要涉及ColossalAI中的模型保存与加载功能。该修改是为了简化统一层的保存和加载状态，添加了一个基本的统一模块类，并更新了统一层的初始化过程。,https://github.com/hpcaitech/ColossalAI/issues/593
ColossalAI,这个issue类型是功能需求，主要涉及对象是模型检查点的保存和加载工具，由于原来的工具可能存在不足或需要改进，用户希望更新这些工具。,https://github.com/hpcaitech/ColossalAI/issues/592
ColossalAI,这是一个功能调整类型的issue，主要涉及到ColossalAI库中的Checkpoint utils模块。由于迁移了`ensure_path_exists `功能至`utils.common`模块，导致了此问题。,https://github.com/hpcaitech/ColossalAI/issues/591
ColossalAI,这是一个功能改进的issue，主要涉及CPU通信操作，更新了CPU张量的通信操作并添加了来自torch的scatter_object_list。,https://github.com/hpcaitech/ColossalAI/issues/590
ColossalAI,这是一个功能增强（Feature enhancement）类型的issue，主要涉及ColossalAI中关于checkpoint和gloo的使用。由于需要在传输CPU tensors时添加gloo groups，因此用户提交了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/589
ColossalAI,这个issue是关于提出需求的，主要涉及的对象是编写Prometheus导出器。问题的原因是希望实现应用监控，特别是训练过程的监控。,https://github.com/hpcaitech/ColossalAI/issues/588
ColossalAI,该issue类型为功能需求，涉及主要对象为ColossalAI中的日志监控系统的配置。,https://github.com/hpcaitech/ColossalAI/issues/587
ColossalAI,该issue是一个功能需求提议，主要涉及ColossalAI库中的初始化函数，用户提出要添加混合Adam优化器功能。,https://github.com/hpcaitech/ColossalAI/issues/584
ColossalAI,这是一个功能需求类型的issue，主要对象是ColossalAI模型库中的GPT模型。这个问题是用户在请求添加激活函数卸载功能，可能是为了优化模型的性能和资源利用。,https://github.com/hpcaitech/ColossalAI/issues/582
ColossalAI,这个issue类型是用户提出需求，涉及的主要对象是工具。由于修改clangformat style到google style，用户提出了希望为pre-commit创建一个.clang-format文件。,https://github.com/hpcaitech/ColossalAI/issues/578
ColossalAI,"这是一个需求类型的issue，主要涉及的对象是一个名为""test_tensor_move.py""的测试文件。该问题涉及到在仅使用1个GPU运行时，需要对该测试文件进行适当的更新。",https://github.com/hpcaitech/ColossalAI/issues/576
ColossalAI,这是一个用户需求类型的issue，主要涉及ColossalAI库中关于fp16/32 grad和fp32 param的状态追踪问题。,https://github.com/hpcaitech/ColossalAI/issues/571
ColossalAI,这个issue类型是技术优化（Polish），涉及到MOE gradient handler的问题。,https://github.com/hpcaitech/ColossalAI/issues/569
ColossalAI,这是一个需求类issue，主要对象是一个CPP文件格式化。原因可能是代码风格不规范或者需要符合项目的统一规范。,https://github.com/hpcaitech/ColossalAI/issues/566
ColossalAI,这是一个用户提出需求的 issue，主要涉及 ColossalAI 中异构训练到极端模型规模的问题。由于 GPU 和 CPU 内存限制，导致 ZeRO 操作失败。,https://github.com/hpcaitech/ColossalAI/issues/562
ColossalAI,这是一个需求类型的issue，主要涉及的对象是ColossalAI中的zero模型参数适配问题。,https://github.com/hpcaitech/ColossalAI/issues/561
ColossalAI,该问题类型为测试请求，主要涉及ColossalAI中utils模块中与零张量相关的API。由于新增了一些函数，在这个issue中用户请求测试这些函数。,https://github.com/hpcaitech/ColossalAI/issues/559
ColossalAI,这个issue类型为改进建议，主要涉及的对象是代码中的一个变量名「col_attr」。由于命名不准确导致在代码中难以理解该变量的含义。,https://github.com/hpcaitech/ColossalAI/issues/558
ColossalAI,这是一个用户提出需求类的issue，主要涉及到模块引用的同步更新，问题可能是由于子模块提交的更新需要进行自动化处理。,https://github.com/hpcaitech/ColossalAI/issues/556
ColossalAI,这是一个需求问题，涉及的主要对象是ColossalAI中的`colo tensor moving APIs`。,https://github.com/hpcaitech/ColossalAI/issues/553
ColossalAI,这个issue属于文档更新类型，涉及的主要对象是 `hybrid adam` 和 `cpu adam`。由于文档需要更新，可能是为了提供更准确和清晰的信息。,https://github.com/hpcaitech/ColossalAI/issues/552
ColossalAI,这是一个功能需求的issue，主要对象是实现PatrickStar stateful tensor，以实现动态CPUGPU布局的参数FP16和梯度FP16数据。,https://github.com/hpcaitech/ColossalAI/issues/549
ColossalAI,这是一个功能需求（Feature Request）类型的issue，主要涉及日志系统，用户希望日志记录中可以显示文件名和行号信息。,https://github.com/hpcaitech/ColossalAI/issues/543
ColossalAI,这是一个用户需求提出的issue，主要涉及的对象是Linear模块，用户希望在Linear1D中添加gather_out参数。,https://github.com/hpcaitech/ColossalAI/issues/541
ColossalAI,这是一个需求变更类型的issue，主要涉及ColossalAI中ZeroInitContext相关的参数改动，由于需要去除不再需要的参数并优化代码和测试单元。,https://github.com/hpcaitech/ColossalAI/issues/540
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的模型数据追踪重构。由于旧实现方式的局限性，导致需要重新建立非模型数据的CUDA内存使用曲线，以解决跟踪模型数据移动和内存变化的问题。,https://github.com/hpcaitech/ColossalAI/issues/537
ColossalAI,这是一则用户提出需求类型的issue，主要涉及ColossalAI中的内存使用情况。,https://github.com/hpcaitech/ColossalAI/issues/536
ColossalAI,这个issue类型为版本更新，主要涉及ColossalAI项目的发布版本。,https://github.com/hpcaitech/ColossalAI/issues/533
ColossalAI,"这个issue类型是文档优化，并涉及'amp', 'builder', 'communication', 'context'和'engine'模块。由于文档风格问题，需要重构这些模块的文档。",https://github.com/hpcaitech/ColossalAI/issues/532
ColossalAI,这是一个文档更新的类型问题，主要对象为ColossalAI项目的API文档。,https://github.com/hpcaitech/ColossalAI/issues/530
ColossalAI,这是一个功能更新的issue，该问题单涉及主要对象为ColossalAI中的优化器Adam的实现。由于移除了在Adam中使用loss scale，可能导致与habit使用相违背的情况。,https://github.com/hpcaitech/ColossalAI/issues/527
ColossalAI,该issue类型是需求提出，主要对象是'engine' module，在进行更新和文档修订的过程中需要进行google style的重构。,https://github.com/hpcaitech/ColossalAI/issues/526
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的模型数据张量内联移动API。,https://github.com/hpcaitech/ColossalAI/issues/521
ColossalAI,这是文档更新类的issue，主要涉及ColossalAI项目中的文档内容。因为之前的文档格式不符合Google风格，所以进行了重构。,https://github.com/hpcaitech/ColossalAI/issues/518
ColossalAI,这是一个需求类型的issue，主要涉及的对象是ColossalAI中的旧零代码，由于其令开发人员困惑，需要被移除。,https://github.com/hpcaitech/ColossalAI/issues/517
ColossalAI,这个issue是一个功能需求类型，主要对象是ColossalAI库中的zero模块。据描述，用户想要在zero context初始化后展示模型数据的CUDA内存使用情况，可能是为了优化模型训练过程中的资源管理。,https://github.com/hpcaitech/ColossalAI/issues/515
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是添加MOEGPT模型。,https://github.com/hpcaitech/ColossalAI/issues/510
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的依赖管理工具pip，在该issue中用户要求暂时不要合并关于移除清华源的修改。因此，这个问题可能是因为修改有潜在风险或者需要进一步讨论。,https://github.com/hpcaitech/ColossalAI/issues/507
ColossalAI,这个issue属于用户提出需求类型，主要对象是ColossalAI库，问题是关于设置CUDA内存使用比例的功能需求。,https://github.com/hpcaitech/ColossalAI/issues/506
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI中的内存管理。这个需求是为了添加一个可以在CUDA上移动张量并收集模型数据量变化的API。,https://github.com/hpcaitech/ColossalAI/issues/503
ColossalAI,这是一个特性需求的issue，主要涉及ColossalAI中关于模型状态保存与加载的一系列更新和新增功能。,https://github.com/hpcaitech/ColossalAI/issues/502
ColossalAI,该issue是关于自动化更新子模块提交的功能需求，而非bug报告，涉及到对ColossalAI中子模块引用的同步。,https://github.com/hpcaitech/ColossalAI/issues/501
ColossalAI,这是一个功能改进类型的issue，主要涉及的对象是ColossalAI中的Singleton和全局上下文。由于避免包之间循环引用，导致需要改进单例和全局上下文的定义方法。,https://github.com/hpcaitech/ColossalAI/issues/500
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI下的一个示例代码，用户希望展示如何使用自定义Tensor结构编写并行操作。,https://github.com/hpcaitech/ColossalAI/issues/498
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是ColossalAI中的fused adam。这个问题提出了对应用程序中的fused adam进行修改，以支持fp16和fp32混合精度计算。,https://github.com/hpcaitech/ColossalAI/issues/497
ColossalAI,这个issue类型是用户提交的请求类型，请求的主要对象是ColossalAI的主分支合并(Merge main)，原因可能是想将当前开发分支的代码合并到主分支中。,https://github.com/hpcaitech/ColossalAI/issues/496
ColossalAI,这是一个关于功能优化的issue，主要涉及ColossalAI库中支持fp16模型参数和梯度复用的功能改进。,https://github.com/hpcaitech/ColossalAI/issues/495
ColossalAI,这是一个需求类型的issue，主要涉及到要移除ColossalAI中旧的MoE（Mixture of Experts）遗留代码。,https://github.com/hpcaitech/ColossalAI/issues/493
ColossalAI,这是一个功能需求的提出，主要涉及对象是ColossalAI的`ShardedModel`， 原因是要实现`ShardedModel`管理ophooks的独立功能，即使没有`Engine`也能充分利用ophooks特性。,https://github.com/hpcaitech/ColossalAI/issues/492
ColossalAI,这是一个功能需求的issue，主要涉及对象是ColossalAI中的CPUAdam和CPUAdamW类，由于用户无法通过传递`adamw_mode`参数来定义AdamW，因此添加了CPUAdamW类。,https://github.com/hpcaitech/ColossalAI/issues/491
ColossalAI,这个issue是一个功能改进类型的问题，其中涉及到ColossalAI的zero optimizer，主要是由于优化代码结构而提出的建议。,https://github.com/hpcaitech/ColossalAI/issues/490
ColossalAI,这是一个用户提出需求的issue，该问题单涉及的主要对象是ColossalAI的MOE功能。由于用户需要支持PR-MOE，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/488
ColossalAI,这个issue类型为功能需求，主要针对ColossalAI中的零优化算法(sharded optim)支持混合CPU Adam的需求。,https://github.com/hpcaitech/ColossalAI/issues/486
ColossalAI,这是一个功能需求类型的issue，主要涉及到子模块引用的同步更新问题，用户提出自动化更新子模块提交的 PR。,https://github.com/hpcaitech/ColossalAI/issues/483
ColossalAI,这个issue是一个功能需求的用户提出需求类型，主要涉及到MOE linear gating。由于新功能需要加入MOE线性门控模块，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/482
ColossalAI,这个issue类型为格式优化，涉及到ColossalAI的MOE名称格式。由于名称格式需要优化，用户提出了关于名称格式优化的需求。,https://github.com/hpcaitech/ColossalAI/issues/481
ColossalAI,这是一个功能需求问题，用户想要为NaiveAMP上的MOE添加LinearGate。,https://github.com/hpcaitech/ColossalAI/issues/480
ColossalAI,这个issue类型是实验结果更新，主要涉及ColossalAI与ZeRO和PyTorch的可视化比较。由于用户希望添加ColossalAI与ZeRO vs. PyTorch的可视化结果，可能是为了展示ColossalAI在使用ZeRO时的性能表现。,https://github.com/hpcaitech/ColossalAI/issues/479
ColossalAI,这是一个新功能需求的issue，主要涉及ColossalAI下的ZeRO模块支持pipeline parallel。这个需求可能是为了提高模型训练的效率和速度。,https://github.com/hpcaitech/ColossalAI/issues/477
ColossalAI,这个issue是一个功能改进（feature enhancement），主要涉及的对象是ColossalAI库中的异常处理。由于CI中出现`Address already in use`错误，导致需要增加 `rerun_on_exception` 函数来处理异常情况，并对 `parameterize` 函数的文档字符串进行修复。,https://github.com/hpcaitech/ColossalAI/issues/475
ColossalAI,这个issue类型是需求优化，主要对象是ColossalAI文档的文档字符串和HTML页面。,https://github.com/hpcaitech/ColossalAI/issues/473
ColossalAI,这是一个文档更新类型的issue，主要对象是API文档，用户提出需要更新`RST`文件。,https://github.com/hpcaitech/ColossalAI/issues/470
ColossalAI,这个issue类型是代码优化/测试需求，涉及到MOE（Mixture of Experts）模块的 experts layout、gradient handler和kernel，由于旧的MOE初始化方式需要移除，需要添加单元测试以确保新的实现能够正确运行。,https://github.com/hpcaitech/ColossalAI/issues/469
ColossalAI,该issue类型为功能改进，主要涉及ColossalAI中的MOE模块。由于全局变量到核心模块的环境重定向以及在moe layer.py中使用分布式进程组而非ParallelMode，导致需要为专家构建函数添加注释。,https://github.com/hpcaitech/ColossalAI/issues/467
ColossalAI,这是一个更新版本的issue，不涉及具体的bug报告或用户需求，主要对象是更新版本文件。,https://github.com/hpcaitech/ColossalAI/issues/465
ColossalAI,这个issue类型是功能改进，主要涉及到ColossalAI中的MOE模块，由于迁移parallelmode到dist process group而引起的。,https://github.com/hpcaitech/ColossalAI/issues/460
ColossalAI,这个issue类型是文档更新（Documentation）的请求，主要对象是ColossalAI下的ZeRO模块。,https://github.com/hpcaitech/ColossalAI/issues/459
ColossalAI,这是一个需求更新的issue，主要涉及到ColossalAI中ZeRO的初始化更新，需要更新相关的API和单元测试。,https://github.com/hpcaitech/ColossalAI/issues/458
ColossalAI,这是一个需求提出的issue，主要涉及的对象是ColossalAI库中的moe context和moe utilities模块以及gradient handler模块。,https://github.com/hpcaitech/ColossalAI/issues/455
ColossalAI,这是一个需求更新类型的issue，主要涉及到`ShardedOptimizerV2`和`ShardedModelV2`的优化和初始化问题。由于需要将优化器实例传递给`ShardedOptimizerV2`，以及初始化`ShardedModelV2`需要`ZeroInitContext`实例初始化模块，因此用户提出了这些更新需求。,https://github.com/hpcaitech/ColossalAI/issues/453
ColossalAI,该issue属于性能优化类别，主要对象是针对ColossalAI中的zero相关测试。由于CI时间过长的问题，针对部分测试进行了优化处理，整体将CI时间从20分钟减少到12分钟。,https://github.com/hpcaitech/ColossalAI/issues/452
ColossalAI,这是一个功能需求问题单，涉及主要对象是ColossalAI的子模块引用，由于需要自动化处理子模块提交更新，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/451
ColossalAI,该issue类型为特性改进，主要涉及到MOE中的通信创建和梯度处理器的重构。,https://github.com/hpcaitech/ColossalAI/issues/450
ColossalAI,这是一个优化测试引擎和训练器的issue，类型为性能优化，主要涉及到测试代码，通过减小测试问题规模来缩短测试时间，以解决持续集成过程中耗时较长的问题。,https://github.com/hpcaitech/ColossalAI/issues/448
ColossalAI,这是一个优化性能的issue，主要涉及到单元测试中的`test_context`部分，通过优化该部分代码，将时间消耗从45秒减少到27秒。,https://github.com/hpcaitech/ColossalAI/issues/446
ColossalAI,这是一个功能需求报告，主要涉及ColossalAI库中的CPUADAM优化问题。由于之前的CPUADAM版本无法在CUDA上运行，现在需要使其更加通用以解决这一问题。,https://github.com/hpcaitech/ColossalAI/issues/445
ColossalAI,这是一个用户提出需求的issue，主要涉及支持在CPU上使用ADAMW优化器。这个需求可能由于ColossalAI库目前无法在CPU上使用ADAMW优化器，用户希望实现这个功能而提出。,https://github.com/hpcaitech/ColossalAI/issues/444
ColossalAI,"这是一个关于""DevOps""的问题; 主要涉及到CI的优化; 原因是CI时间过长，需要优化git克隆和单元测试。",https://github.com/hpcaitech/ColossalAI/issues/443
ColossalAI,这个issue是文档更新类型，主要涉及到贡献者指南的更新。可能是由于项目工作流程的更新需要反映到贡献者指南中。,https://github.com/hpcaitech/ColossalAI/issues/440
ColossalAI,这是一个文档更新类型的issue，主要涉及到项目贡献指南。原因可能是贡献流程发生了变化，需要更新文档以反映当前的工作流程。,https://github.com/hpcaitech/ColossalAI/issues/439
ColossalAI,这个issue类型是提出需求，主要对象是unittest的配置。这个问题提出了关于unittest零配置的问题或者寻求相关帮助。,https://github.com/hpcaitech/ColossalAI/issues/438
ColossalAI,这是一个需求提出的issue，主要涉及将DLRM适配到不兼容的API接口，提出了使用tuple输入以减少工作量的建议。,https://github.com/hpcaitech/ColossalAI/issues/437
ColossalAI,这是一个需求更改的问题，主要涉及到代码质量评估工具的选择。由于CodeBeat评估速度慢，与CodeFactor功能重叠，因此需要在README中将CodeBeat替换为CodeFactor。,https://github.com/hpcaitech/ColossalAI/issues/436
ColossalAI,该issue是一个功能需求，新增了一个测试模块。,https://github.com/hpcaitech/ColossalAI/issues/435
ColossalAI,这个issue是用户提出的需求类型，主要涉及ColossalAI中的Multiply Jitter和capacity factor eval的新增功能。,https://github.com/hpcaitech/ColossalAI/issues/434
ColossalAI,这是一个功能改进（feature enhancement）类型的issue，主要涉及ColossalAI库中参数检查点（params checkpointing）。由于当前的检查点功能可能存在一些限制或不完善，用户希望更新和改进这部分功能。,https://github.com/hpcaitech/ColossalAI/issues/429
ColossalAI,这个issue是关于测试的合并请求，类型为代码质量优化。主要对象涉及测试代码。由于测试代码合并可能存在问题，导致需要进行优化处理。,https://github.com/hpcaitech/ColossalAI/issues/428
ColossalAI,这是一个用户提出需求类型的issue，主要涉及改进日志显示和使用`rich`库。由于当前日志显示效果不佳，用户希望通过`rich`库实现更好的日志显示。,https://github.com/hpcaitech/ColossalAI/issues/426
ColossalAI,这个issue类型为功能增强（feature enhancement），主要涉及的对象是ColossalAI的ViLT-MLM模型。,https://github.com/hpcaitech/ColossalAI/issues/423
ColossalAI,这个issue类型是功能需求，主要涉及的对象是ColossalAI库中的梯度处理方式，通过使用双缓冲区处理梯度来支持部分卸载操作，因为需要支持部分卸载，所以使用双缓冲区来处理梯度。,https://github.com/hpcaitech/ColossalAI/issues/422
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI下的MoE模型的模型同步功能。,https://github.com/hpcaitech/ColossalAI/issues/421
ColossalAI,这个issue类型是需求提出，主要涉及的对象是ColossalAI。该问题源于用户希望实现混合Adam优化器，并将部分操作系统迁移到CUDA的需求。,https://github.com/hpcaitech/ColossalAI/issues/420
ColossalAI,这是一个技术性改进类的issue，涉及ColossalAI的全局模型数据追踪器。,https://github.com/hpcaitech/ColossalAI/issues/417
ColossalAI,这是一个需求类型的issue，涉及主要对象为 ColossalAI 库中的 ShardedOptimV2 类，用户希望重构该类的 init 方法。,https://github.com/hpcaitech/ColossalAI/issues/416
ColossalAI,该issue类型为功能需求，主要涉及的对象是ColossalAI的CLI。由于目前系统没有提供CLI功能，用户提出了添加CLI以提供易于使用的实用工具的建议。,https://github.com/hpcaitech/ColossalAI/issues/415
ColossalAI,这是一个功能需求类型的issue，主要涉及到项目中子模块引用的同步更新。,https://github.com/hpcaitech/ColossalAI/issues/414
ColossalAI,这个issue是一个需求提出类型的问题，主要涉及优化ColossalAI库中的ShardedOptimV2初始化方法并添加新功能。,https://github.com/hpcaitech/ColossalAI/issues/412
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI框架中的梯度优化操作。由于当前版本未能充分利用PCIE带宽，用户提出允许在刷新桶时offload梯度的需求。,https://github.com/hpcaitech/ColossalAI/issues/411
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及的对象是项目的徽章。用户可能添加了Hugging Face徽章，希望能够在项目中展示。,https://github.com/hpcaitech/ColossalAI/issues/407
ColossalAI,这是一个用户提出需求的类型，该问题单涉及的主要对象是ColossalAI项目中的ShardedOptimv2模块。由于需要为ShardedOptimv2提供新的接口，用户提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/406
ColossalAI,该issue是关于用户提出需求的，主要对象是在ColossalAI项目下的测试函数。由于需要避免为每个测试用例初始化分布式网络，用户提议将常用的测试函数整合到colossalai.testing模块中。,https://github.com/hpcaitech/ColossalAI/issues/405
ColossalAI,这个issue是用户提出的需求类型问题，涉及的主要对象是ColossalAI中的tensor shard策略。,https://github.com/hpcaitech/ColossalAI/issues/403
ColossalAI,这是一个功能需求，主要涉及多个GPU之间的点对点通信，用户提出需要添加一个功能来检查多个GPU是否支持点对点通信，并测试多个GPU是否支持P2P通信。由于Nvidia不支持通过PCIe进行点对点通信，用户希望通过NvLink解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/399
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI项目的功能增强建议。由于用户提出了对集群日志监控工具和性能监控功能的需求，以及针对程序运行和模型部署的建议，说明用户希望提升系统的可用性和性能监控，以支持更广泛的数据处理和模型部署需求。,https://github.com/hpcaitech/ColossalAI/issues/398
ColossalAI,这是一个功能需求类型的issue，涉及到自动更新子模块提交的功能。,https://github.com/hpcaitech/ColossalAI/issues/397
ColossalAI,该issue属于用户提出需求类型，主要涉及添加ColossalAI组织到Hugging Face这个平台。原因可能是用户希望将ColossalAI的相关模型、数据集、以及网络演示空间整合到Hugging Face中。,https://github.com/hpcaitech/ColossalAI/issues/396
ColossalAI,这是一个功能需求相关的问题，主要涉及的对象是 ColossalAI 库中的 CUDA 内存使用统计功能。这个需求来源于用户对训练过程中内存统计的需求。,https://github.com/hpcaitech/ColossalAI/issues/395
ColossalAI,这是一个功能添加类型的issue， 主要涉及的对象是ColossalAI中的tensor detector， 由于需要新增功能，所以用户提交了这个issue。,https://github.com/hpcaitech/ColossalAI/issues/393
ColossalAI,这是一个用户提出需求的类型。该问题单涉及的主要对象是fp16 optimizer。由于代码需要重构，用户希望对fp16 optimizer进行修改。,https://github.com/hpcaitech/ColossalAI/issues/392
ColossalAI,"这是一个功能新增的issue，涉及主要对象为""tensor detector""，用户提出了添加该功能的需求。",https://github.com/hpcaitech/ColossalAI/issues/386
ColossalAI,该issue是关于更新README和图片路径的，属于文档改进类型，主要涉及ColossalAI的文档路径。由于路径错误导致README和图片无法正常显示。,https://github.com/hpcaitech/ColossalAI/issues/384
ColossalAI,这是一个关于代码优化的问题，涉及的主要对象是调整ColossalAI中的fp16优化器。原因是要解决版权问题所引发的问题。,https://github.com/hpcaitech/ColossalAI/issues/382
ColossalAI,这是一个功能增强类型的issue，主要涉及ColossalAI中的PCIE profiler功能。由于数据传输监测需要，新增了PCIE profiler功能。,https://github.com/hpcaitech/ColossalAI/issues/373
ColossalAI,这个issue类型是文档更新，主要对象是 README-zh-Hans.md。由于负照进行了更新。,https://github.com/hpcaitech/ColossalAI/issues/367
ColossalAI,这是一个需求变更类型的issue，主要涉及ColossalAI模型库中的模型。这个问题可能是由于决策变更或者更新需求而导致的。,https://github.com/hpcaitech/ColossalAI/issues/364
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的全局模型数据内存追踪功能。,https://github.com/hpcaitech/ColossalAI/issues/360
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI项目中的性能分析模块，其实现中存在实时数据监测部分尚未完成。,https://github.com/hpcaitech/ColossalAI/issues/356
ColossalAI,这是一个用户提出需求类型的issue，主要涉及的对象是ColossalAI项目中的Profiler模块。由于尚未正确设置文件位置、未进行离线性能测试以及缺乏实时监控，用户提出了添加MemProfiler的需求。,https://github.com/hpcaitech/ColossalAI/issues/355
ColossalAI,这是一个用户提出需求的issue，主要对象是ProfilerContext。由于需要添加示例以完善功能或提高易用性。,https://github.com/hpcaitech/ColossalAI/issues/349
ColossalAI,这是一个功能需求类型的issue，主要涉及测试`ShardedOptimV2`与`CPUAdam`的情况，需要测试Grad和fp32 master param是否被分配到CPU。,https://github.com/hpcaitech/ColossalAI/issues/347
ColossalAI,该issue是一个需求类型，主要对象是ColossalAI库中的异步内存，可能是为了提高性能或组织结构而提出。,https://github.com/hpcaitech/ColossalAI/issues/345
ColossalAI,这是一个用户提出需求的issue，主要对象是Flake8代码格式化，由于代码格式不符合规范导致需要对其进行重新格式化。,https://github.com/hpcaitech/ColossalAI/issues/344
ColossalAI,这个issue是一个功能需求，涉及的主要对象是Profiler Context。这个需求是为了管理所有的profilers，提高系统的性能监控和调试功能。,https://github.com/hpcaitech/ColossalAI/issues/340
ColossalAI,这是一个功能改进类型的issue，涉及到ColossalAI库中的fp16梯度缩放器的重构。,https://github.com/hpcaitech/ColossalAI/issues/338
ColossalAI,这是一个关于功能新增（feature addition）的issue，涉及的主要对象是正在开发的ColossalAI系统。原因是新增的功能成功通过了单元测试。,https://github.com/hpcaitech/ColossalAI/issues/331
ColossalAI,该issue类型是功能需求提议，主要涉及的对象是Profiler模块。 由于目前的Memory Profiler模块无法自适应采样时间间隔，用户提议添加自适应采样功能，根据热身阶段的统计信息自动调整时间间隔。,https://github.com/hpcaitech/ColossalAI/issues/330
ColossalAI,这是一个用户提交的功能需求，请求添加基础性的性能分析工具。,https://github.com/hpcaitech/ColossalAI/issues/329
ColossalAI,这个issue属于需求提出类型，涉及ColossalAI中zero模块下的ZeroInitContext和test utils的更新，主要问题是需要在non_distributed_component_funcs中添加一个返回模型和优化器函数，并且需要在ZeroInitContext中添加一个不删除torch参数负载的功能。,https://github.com/hpcaitech/ColossalAI/issues/327
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是 ColossalAI 中的 Communication Profiler。由于当前的 Communication Profiler 必须在训练之前进行初始化，因此用户希望提供一个上下文管理器，以支持对程序中特定部分的更精细的性能分析。,https://github.com/hpcaitech/ColossalAI/issues/326
ColossalAI,这个issue是关于功能改进的类型，涉及到ColossalAI项目中的测试组件。由于测试引擎的重构，将节省colossalai.launch时间并能够测试不同的非分布式模型，同时在后续的PR中会对其他测试用例进行重构。,https://github.com/hpcaitech/ColossalAI/issues/324
ColossalAI,这个issue是一个用户提出的需求，主要涉及model初始化的context，用户希望在初始化模型的过程中实现模型切片，减少GPU内存使用，并且将参数转换为fp16格式，以及适应ShardedParamV2。,https://github.com/hpcaitech/ColossalAI/issues/321
ColossalAI,该issue属于功能需求提议，主要涉及ColossalAI中的参数分片功能的优化。,https://github.com/hpcaitech/ColossalAI/issues/320
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及项目的文档展示和贡献者信息。由于缺乏徽章和贡献者信息，用户希望在项目中添加这些内容。,https://github.com/hpcaitech/ColossalAI/issues/316
ColossalAI,这个issue是功能需求类型，主要涉及的对象是ShardedTensor和ShardedModel，用户希望增加对`ShardedTensor` API的支持，以实现`ShardedModel`和普通模型的功能。,https://github.com/hpcaitech/ColossalAI/issues/315
ColossalAI,"这是一个用户提出需求的类型issue, 主要关注的对象是ColossalAI用户。这个问题的原因是用户需要一个集成配置教程以便定制他们的应用。",https://github.com/hpcaitech/ColossalAI/issues/314
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI代码库中的zero hook功能。可能是由于缺乏zero hook功能，用户希望为ColossalAI添加这个功能。,https://github.com/hpcaitech/ColossalAI/issues/313
ColossalAI,这是一个用户提出需求的类型。该问题单涉及的主要对象是ColossalAI中的分片策略。由于需要支持更多的策略，现有的分片策略和聚合策略需返回一个张量列表，并且它们应该互为逆操作。新增加了两种策略：TensorListShardStrategy以贪婪的方式对张量列表进行分片，而保持每个张量完整；FlatShardStrategy则对所有张量进行扁平化处理再进行分片。,https://github.com/hpcaitech/ColossalAI/issues/312
ColossalAI,该issue类型为功能改进，主要涉及到ColossalAI中的zero模块，用户寻求关于改进分片参数的帮助。,https://github.com/hpcaitech/ColossalAI/issues/311
ColossalAI,这个issue是关于提出需求的类型，主要涉及ColossalAI底层的zero库，提出了改进shard策略的需求。,https://github.com/hpcaitech/ColossalAI/issues/310
ColossalAI,这个issue是一个功能需求的提出，主要涉及ColossalAI下的optim v2和unit test。可能是由于旧版本存在一定的问题或者缺陷，需要更新并添加新的单元测试来提高功能的稳定性和可靠性。,https://github.com/hpcaitech/ColossalAI/issues/308
ColossalAI,这个issue是关于需求的，主要涉及对象是ColossalAI中的zero模块。由于需要更精细的张量分片策略，用户提出了一个与张量粒度相关的问题。,https://github.com/hpcaitech/ColossalAI/issues/307
ColossalAI,这个issue属于功能需求类型，主要涉及到ColossalAI框架中的性能分析器（profiler），用户可能提出了需要为性能分析器添加通信操作的功能要求。,https://github.com/hpcaitech/ColossalAI/issues/306
ColossalAI,这是一个用户提出需求的类型，主要对象是ShardedParam；用户希望为ShardedParam添加一个名为set_payload的方法。,https://github.com/hpcaitech/ColossalAI/issues/304
ColossalAI,这个issue是一个用户提出的功能需求，主要涉及github action的重构，用户希望优化DevOps体验。,https://github.com/hpcaitech/ColossalAI/issues/303
ColossalAI,这个issue类型是特性请求，主要对象是主内存追踪器，用户希望提交关于主内存追踪器的PR。,https://github.com/hpcaitech/ColossalAI/issues/298
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是sharded parameter。原因是参数命名需修改，并需要将sharded参数从头开始初始化。,https://github.com/hpcaitech/ColossalAI/issues/297
ColossalAI,该issue类型是用户提出需求，主要涉及到primary memory tracer，用户提出了关于是否提供`warmup`参数以及结果展示方式的问题。,https://github.com/hpcaitech/ColossalAI/issues/296
ColossalAI,该issue类型为功能改进，主要对象是sharded optimizer。由于需要确保fp16 sharded optimizer在不同阶段的操作结果与fp32 torch DDP在同等输入条件下产生的结果在可接受的数值误差范围内，并验证各阶段之间的一致性，该unit test被添加进来。,https://github.com/hpcaitech/ColossalAI/issues/293
ColossalAI,这是一个用户提出需求的类型，主要涉及的对象是参数钩子。这个问题由于需要用户阅读大量文档来学习如何使用钩子，希望提供一个工具简化注册参数钩子的操作。,https://github.com/hpcaitech/ColossalAI/issues/292
ColossalAI,这个issue属于功能需求类型，涉及到ColossalAI项目中的参数梯度和hook机制。,https://github.com/hpcaitech/ColossalAI/issues/287
ColossalAI,这个issue属于用户提出需求类型，主要对象是ColossalAI中的sharded gradient和gradient hooks。原因是可能需要在代码中添加sharded gradient功能和重构梯度hooks。,https://github.com/hpcaitech/ColossalAI/issues/286
ColossalAI,这个issue类型为功能需求，主要涉及到梯度分片和梯度hook的添加和重构。,https://github.com/hpcaitech/ColossalAI/issues/285
ColossalAI,这是一个用户提出需求的类型的issue，主要对象是增加检查点功能。,https://github.com/hpcaitech/ColossalAI/issues/282
ColossalAI,这个issue类型为功能需求提议，主要对象为ColossalAI中的Feature/zero功能模块。,https://github.com/hpcaitech/ColossalAI/issues/279
ColossalAI,这个issue属于需求提出类型，主要对象是CI运行环境，由于文档变更的pull request无需进行测试，因此提出优化CI触发配置以减轻服务器负荷。,https://github.com/hpcaitech/ColossalAI/issues/278
ColossalAI,这个issue类型是需求更新，涉及的主要对象是issue模板。由于需求变更或者使用过程中发现问题，用户提出了更新issue模板的需求。,https://github.com/hpcaitech/ColossalAI/issues/277
ColossalAI,这个issue类型是用户提出需求，涉及的主要对象是GitHub上ColossalAI项目中的模板。由于缺少提案和用户组信息，用户需要在issue模板中增加这些内容。,https://github.com/hpcaitech/ColossalAI/issues/276
ColossalAI,该issue类型为功能需求提出，主要涉及ColossalAI的兼容性和发布选项，用户可能需要此功能以便更方便地进行持续集成和发布。,https://github.com/hpcaitech/ColossalAI/issues/275
ColossalAI,这是一个功能需求的issue， 主要涉及的对象是ColossalAI中的ZeRO模型实现。原因在于提供更优雅的sharded model实现，并解决了一些hook管理、内存管理和模块化方面的问题。,https://github.com/hpcaitech/ColossalAI/issues/274
ColossalAI,这是一个需求提议的issue， 主要涉及到更新README和问题模板。原因是为了完善文档内容和社区建设。,https://github.com/hpcaitech/ColossalAI/issues/272
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及社区群组的添加。,https://github.com/hpcaitech/ColossalAI/issues/271
ColossalAI,这是一个用户提出需求的类型的issue，主要涉及Zero3ParameterManager模块的重新配置，目的是在更细的粒度上对参数进行分片和聚合。,https://github.com/hpcaitech/ColossalAI/issues/270
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI中的clip_grad函数。由于现有代码不支持zero3，并缺少相关的单元测试，用户提出了需要支持zero3并添加相应的单元测试的要求。,https://github.com/hpcaitech/ColossalAI/issues/268
ColossalAI,这个issue类型是测试请求，涉及主要对象是Zero3ParameterManager，用户需要添加该对象的单元测试。,https://github.com/hpcaitech/ColossalAI/issues/267
ColossalAI,该issue类型为功能实现（Feature Implementation），涉及主要对象为Sharded Optimizer和Model，由于PR仍处于草稿状态，需要补充更多内容。,https://github.com/hpcaitech/ColossalAI/issues/265
ColossalAI,这是一个功能需求类型的issue，主要涉及到ColossalAI项目中的TPExpert模块。由于特殊情况的需要，用户提出了添加TPExpert功能的需求。,https://github.com/hpcaitech/ColossalAI/issues/264
ColossalAI,这个issue类型是功能增强，涉及ColossalAI中的ZeRO优化器的阶段1和阶段2的实现与CPU卸载功能，用户提出了对ResNet和CIFAR10数据集的收敛验证。,https://github.com/hpcaitech/ColossalAI/issues/263
ColossalAI,该issue类型是用户提出需求，主要涉及ColossalAI的激活CPU卸载功能。原因可能是为了节省GPU内存而提出这一功能需求。,https://github.com/hpcaitech/ColossalAI/issues/262
ColossalAI,这个issue类型是功能需求提议，主要涉及实现一个用于监控训练过程中内存使用情况的运行时内存跟踪器的功能。由于用户希望能够实时监测GPU内存占用并根据统计数据进行优化决策，因此提出了这一需求。,https://github.com/hpcaitech/ColossalAI/issues/261
ColossalAI,这是一个需求类型的issue，主要涉及对象是添加GPT benchmark代码到DeepSpeed和FairScale中。,https://github.com/hpcaitech/ColossalAI/issues/260
ColossalAI,这个issue类型为功能需求，涉及主要对象是添加GPT基准代码到DeepSpeed和FairScale。,https://github.com/hpcaitech/ColossalAI/issues/259
ColossalAI,这是一个功能需求的issue，涉及主要对象是ColossalAI中的fp16兼容性以及SyncBN。导致这个问题的原因是当前的fp16实现将批量归一化层中的`count`转换为fp16，导致一些内部数据类型检查失败，进而导致同一输入在不同排名上的模型产生不同的结果。,https://github.com/hpcaitech/ColossalAI/issues/258
ColossalAI,这个issue是关于用户提出需求的，主要对象是ColossalAI中的activation CPU offloading功能。用户希望该功能类似于deepspeed中的CPU_checkpointing，并参考deepspeed的实现。,https://github.com/hpcaitech/ColossalAI/issues/257
ColossalAI,该问题类型为用户提出需求，主要涉及对象是模型保存和加载的教程，用户想了解如何在ColossalAI中保存和加载模型的过程。,https://github.com/hpcaitech/ColossalAI/issues/256
ColossalAI,这是一个特性需求的issue，主要涉及文档的中文翻译。由于代码合并导致的问题可能导致翻译未能准确反映更新的内容。,https://github.com/hpcaitech/ColossalAI/issues/253
ColossalAI,这是用户提出的需求，主要涉及ColossalAI训练过程中添加更多运行时钩子的功能。,https://github.com/hpcaitech/ColossalAI/issues/250
ColossalAI,"该issue类型为功能需求类型，主要对象是ColossalAI项目中的""Feature/zero v3""功能模块。",https://github.com/hpcaitech/ColossalAI/issues/249
ColossalAI,这是一个开发优化和功能增强类的issue，涉及主要对象为ColossalAI中的MoE（Mixture of Experts）层和相关代码。原因导致的bug修复和新增功能可能是为了提高模型性能和功能扩展。,https://github.com/hpcaitech/ColossalAI/issues/248
ColossalAI,这个issue类型为功能需求，主要涉及到同步子模块引用的问题，用户希望实现自动化PR以更新子模块的提交。,https://github.com/hpcaitech/ColossalAI/issues/246
ColossalAI,这是一个用户需求类型的issue，主要涉及添加中文README文件。原因可能是希望更多中文用户能够更好地了解项目内容。,https://github.com/hpcaitech/ColossalAI/issues/244
ColossalAI,这个issue是一个需求类型，主要涉及ColossalAI中的gather函数参数的完整性，并提出希望添加一个完整参数上下文的需求。,https://github.com/hpcaitech/ColossalAI/issues/243
ColossalAI,这是一个用户提出需求的issue，涉及的主要对象是`colossalai/zero/zero_stage3_develop.py`和`colossalai/zero/param_manager.py`。由于遗留了一些待办事项，可能导致了相关功能的实现尚未完成。,https://github.com/hpcaitech/ColossalAI/issues/240
ColossalAI,这是一个用户需求提出类型的 issue，主要涉及对象是 VanillaPatchEmbedding，用户想了解其是否具有并行计算功能，以及希望相关文档能添加更多内容。,https://github.com/hpcaitech/ColossalAI/issues/239
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的缺乏实验结果展示，造成用户难以理解和重现的问题。,https://github.com/hpcaitech/ColossalAI/issues/238
ColossalAI,这是一个用户提出的需求问题，涉及到API on collective operations，主要问题是用户在部分实现张量并行模型时遇到困难，无法让单个rank获得完整的张量，希望提供更易于使用的集体操作函数。,https://github.com/hpcaitech/ColossalAI/issues/237
ColossalAI,这是一个用户需求的问题，主要涉及ColossalAI的模型微调功能，并表明用户需要一个微调示例。,https://github.com/hpcaitech/ColossalAI/issues/236
ColossalAI,该issue类型为用户提出需求，涉及到自动发布ColossalAI项目到PyPI的功能设置。这是由于当前发布流程需要手动操作，存在不稳定性和耗时的问题，用户希望实现自动化发布流程来提高效率。,https://github.com/hpcaitech/ColossalAI/issues/235
ColossalAI,这是一个用户提出需求的类型的issue，主要对象是ColossalAI的README和文档。由于缺少中文的说明文档，用户希望提供相关内容。,https://github.com/hpcaitech/ColossalAI/issues/234
ColossalAI,这个issue类型是需求更新，主要涉及到ColossalAI项目的设置信息更新。由于缺少长描述和PYPI的URLs，需要补充这些内容。,https://github.com/hpcaitech/ColossalAI/issues/233
ColossalAI,这是一个用户提出需求的issue，主要对象是项目的设置信息。由于缺少长描述和PYPI的URLs，用户提出需要对设置信息进行更新。,https://github.com/hpcaitech/ColossalAI/issues/232
ColossalAI,这个issue类型是需求提出，主要涉及的对象是ColossalAI在PyPi上的包。这个问题是由于缺少在`setup.py`中添加长描述导致，需要在下一个发布版本中解决。,https://github.com/hpcaitech/ColossalAI/issues/231
ColossalAI,这个issue类型为更新操作，主要对象是ColossalAI的README和change log文件。,https://github.com/hpcaitech/ColossalAI/issues/224
ColossalAI,这是一个用户需求的issue，涉及到ColossalAI的安装设置和工作流更新。原因是用户希望在安装时能够根据环境自动构建CUDA扩展。,https://github.com/hpcaitech/ColossalAI/issues/222
ColossalAI,该issue类型为功能需求，涉及主要对象为安装程序。由于环境变量不正确，导致无法自动构建CUDA扩展，因此用户提出了需要更新设置和工作流的需求。,https://github.com/hpcaitech/ColossalAI/issues/221
ColossalAI,这个issue类型是配置更新，主要对象是flake8配置。由于继承自patrick star，这个问题单添加了新的flake8配置。,https://github.com/hpcaitech/ColossalAI/issues/218
ColossalAI,这是一个用户提出需求的issue，涉及的主要对象是ColossalAI的github action。,https://github.com/hpcaitech/ColossalAI/issues/213
ColossalAI,这是一个需求更新类型的issue，涉及更新零阶段1的开发。原因可能是为了完善功能或改进现有的代码质量。,https://github.com/hpcaitech/ColossalAI/issues/212
ColossalAI,这是一个功能需求类型的issue，涉及的主要对象是代码仓库中的子模块引用。,https://github.com/hpcaitech/ColossalAI/issues/210
ColossalAI,"这是一个用户提出需求的类型，主要涉及添加名称为""zero1""的内容。由于缺少具体描述内容，无法分析具体问题或原因。",https://github.com/hpcaitech/ColossalAI/issues/209
ColossalAI,这是一个需求类型的issue，主要对象是更新github actions。由于可能存在已有的github actions配置需要更新或改进，用户提出了需要对github actions进行更新的需求。,https://github.com/hpcaitech/ColossalAI/issues/204
ColossalAI,这是一个功能需求问题，涉及到ColossalAI中的子模块引用同步更新的自动化问题。,https://github.com/hpcaitech/ColossalAI/issues/203
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是代码质量徽章。由于缺少代码质量徽章，用户希望添加此功能以提高项目的可信度。,https://github.com/hpcaitech/ColossalAI/issues/201
ColossalAI,这是一个需求提交类型的Issue，主要涉及到代码文档和示例的更新，可能由于之前的文档不清晰或缺少示例而导致用户使用时出现困惑。,https://github.com/hpcaitech/ColossalAI/issues/200
ColossalAI,该issue类型为需求提出，主要涉及对象是如何在ColossalAI中基于TensorFlow框架构建新的示例。由于用户已经有一个基于TensorFlow的深度神经网络模型，想了解如何将其作为示例上传，因此提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/199
ColossalAI,这是一个更新GitHub action和pre-commit设置的issue，类型为功能需求，主要涉及的对象是项目的代码质量和自动化工作流程。,https://github.com/hpcaitech/ColossalAI/issues/198
ColossalAI,这个issue是关于更新GitHub action和pre-commit设置的，属于用户提出需求的类型，主要涉及ColossalAI项目的CI/CD环境和代码质量工具设置。,https://github.com/hpcaitech/ColossalAI/issues/196
ColossalAI,这个issue类型为功能需求，涉及的主要对象是代码仓库的分支管理方式。由于开发人员希望能够将新功能合并到develop分支而非master分支，表明对代码管理流程的优化需求。,https://github.com/hpcaitech/ColossalAI/issues/195
ColossalAI,这个issue是关于功能改进类型的，涉及的主要对象是ColossalAI的`benchmark`和`examples`子模块。原因是这两个子模块只指向可能很快就会过时的提交ID，导致需要自动更新子模块提交并创建拉取请求。,https://github.com/hpcaitech/ColossalAI/issues/193
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的新功能(action)，用户可能希望添加某种特定的操作或功能来丰富代码库的功能性。,https://github.com/hpcaitech/ColossalAI/issues/188
ColossalAI,这个issue类型为需求提出，涉及主要对象为ColossalAI下的一些配置文件。由于一些新功能的添加和更新导致需要创建和更新一系列配置文件，以便提高项目的质量和效率。,https://github.com/hpcaitech/ColossalAI/issues/187
ColossalAI,这个issue是关于功能更新，涉及的主要对象是“Action”。由于可能是新功能添加或现有功能的改进，用户可能提出了需求或者问题，或寻求相关帮助。,https://github.com/hpcaitech/ColossalAI/issues/186
ColossalAI,这个issue属于需求更新类型，涉及的主要对象是actions。由于需要对actions进行更新，用户提出了这个问题。,https://github.com/hpcaitech/ColossalAI/issues/184
ColossalAI,这是一个需求更新的issue，主要涉及项目的README.md文件。由于内容为空，用户需要更新README.md文件。,https://github.com/hpcaitech/ColossalAI/issues/183
ColossalAI,这个issue属于功能需求，主要涉及ColossalAI代码库中添加PyTorch hooks的问题。原因可能是为了修复在操作符之前和之后添加钩子时出现的错误。,https://github.com/hpcaitech/ColossalAI/issues/179
ColossalAI,这个issue类型是功能增强（feature enhancement），涉及到词汇并行层（vocab parallel layers）。原因是为了在环境变量与全局变量之间切换，添加了分支上下文，更新了GPT模型。,https://github.com/hpcaitech/ColossalAI/issues/178
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI项目中的hook系统，用户希望在PyTorch子模块执行前后执行一些操作，例如记录内存使用情况。,https://github.com/hpcaitech/ColossalAI/issues/175
ColossalAI,这是一个需求提出类的issue，主要涉及到ColossalAI的环境变量、分支上下文以及GPT模型性能更新。可能由于需要更好的管理环境变量和支持分支特性，以及优化GPT模型的性能而产生了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/174
ColossalAI,这个issue类型是功能需求，主要涉及对象是ColossalAI的logo。原因是用户希望将logo的背景改为透明。,https://github.com/hpcaitech/ColossalAI/issues/173
ColossalAI,这是一个用户提出需求的issue，涉及的主要对象是安装过程中的CUDA扩展构建，用户寻求自动根据环境构建CUDA扩展的支持。,https://github.com/hpcaitech/ColossalAI/issues/172
ColossalAI,这是一个用户提出需求的issue，主要对象是更新ColossalAI的README文件。由于安装方法顺序需要调换，用户提出更新README文件的请求。,https://github.com/hpcaitech/ColossalAI/issues/168
ColossalAI,这是一个更新工作流文件和readme.md的issue，类型为需求提出，涉及的主要对象是ColossalAI项目。,https://github.com/hpcaitech/ColossalAI/issues/166
ColossalAI,这是一个更新文档需求和rtd配置的issue，涉及到ColossalAI项目的文档更新。可能是由于文档需求变更或者配置更新导致的。,https://github.com/hpcaitech/ColossalAI/issues/165
ColossalAI,该issue类型为文档更新，涉及对象为ColossalAI的tutorial markdown文件和rst文件，由于需要保持一致性和更新API文档，才会出现这一问题。,https://github.com/hpcaitech/ColossalAI/issues/164
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的examples文件夹。这个问题的原因是指向错误的ColossalAIExamples文件夹，导致了需要将示例设置为子模块的需求。,https://github.com/hpcaitech/ColossalAI/issues/162
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI网站首页的logo添加和在issue模板中添加论坛的功能。可能是因为用户希望提升网站的可视性和提供更方便的交流参与方式。,https://github.com/hpcaitech/ColossalAI/issues/161
ColossalAI,该issue类型为文档更新，涉及主要对象是AMP模型。这个问题可能由于文档内容需要更新或修正而产生。,https://github.com/hpcaitech/ColossalAI/issues/160
ColossalAI,这个issue类型是更新内容，主要对象是benchmark submodule commit ID。由于需要将benchmark submodule的commit ID更新到最新，可能是为了修复或更新相关的功能或性能。,https://github.com/hpcaitech/ColossalAI/issues/159
ColossalAI,这是一个功能需求类型的issue，主要涉及ColossalAI中的GPT3和GPT2模型示例的整合。,https://github.com/hpcaitech/ColossalAI/issues/157
ColossalAI,这是一个改进建议类型的issue，主要涉及ColossalAI的基准模块的调整。,https://github.com/hpcaitech/ColossalAI/issues/156
ColossalAI,这是一则用户提出需求的issue，主要涉及的对象是ColossalAI库中的transpose_pad_fusion kernel。 由于缺乏此kernel的实现，用户希望添加用于推理的transpose_pad_fusion kernel。,https://github.com/hpcaitech/ColossalAI/issues/153
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI的pipeline parallelism中最后阶段的多输出支持。该问题由于需要支持多个输出和标签，以及记录输出模型的张量形状和数据类型而被提出。,https://github.com/hpcaitech/ColossalAI/issues/151
ColossalAI,该issue属于功能增强，涉及ColossalAI中VisionandLanguage Transformer的ViLT数据并行示例的添加。,https://github.com/hpcaitech/ColossalAI/issues/148
ColossalAI,该issue类型为功能改进，主要涉及的对象是dali dataloader。由于增加了随机数据增强，导致了模型在训练过程中准确率提升了4.1%，但每个epoch的训练时间增加了10秒。,https://github.com/hpcaitech/ColossalAI/issues/147
ColossalAI,这个issue属于用户提出需求类型，主要涉及对象是ColossalAI的GPT-2模型。由于用户希望增加GPT-2的示例，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/146
ColossalAI,这是一个关于需求的issue，主要涉及文档自动更新设置在readthedocs上的问题。,https://github.com/hpcaitech/ColossalAI/issues/145
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的数据处理。由于缺少readme文档，用户希望添加关于GPT数据处理的说明。,https://github.com/hpcaitech/ColossalAI/issues/144
ColossalAI,这是一个需求类型的issue，主要涉及到ColossalAI中CUDA kernel的重构。导致此需求的原因可能是为了优化工程结构和提高编译效率。,https://github.com/hpcaitech/ColossalAI/issues/142
ColossalAI,这个issue类型为功能增强（Feature Enhancement），主要涉及对象是对 deeplabv3+ 模型在 CamVid 数据集上进行训练的例子。,https://github.com/hpcaitech/ColossalAI/issues/141
ColossalAI,该issue类型为功能增强请求，主要涉及ColossalAI在CIFAR-10上的多MOE并行示例。原因可能是为了优化模型性能或提供更多丰富的示例。,https://github.com/hpcaitech/ColossalAI/issues/140
ColossalAI,这个issue类型是需求更新，涉及主要对象为GPT实验，用户可能需要对实验进行更新或改进。,https://github.com/hpcaitech/ColossalAI/issues/138
ColossalAI,该issue类型为用户提出需求，主要对象是在ColossalAI上的deeplabv3+模型在CamVid数据集上进行分割任务训练。由此可能是用户需要关于在CamVid数据集上训练deeplabv3+模型的示例或指导。,https://github.com/hpcaitech/ColossalAI/issues/135
ColossalAI,这是一个用户提出需求的issue，关于向ColossalAI项目中添加文档问题模板。,https://github.com/hpcaitech/ColossalAI/issues/133
ColossalAI,这个issue类型是功能增强（feature enhancement），主要对象是ColossalAI项目中的GPT混合并行模型。,https://github.com/hpcaitech/ColossalAI/issues/132
ColossalAI,这是一个用户提出需求的issue，主要涉及文档部署工作流程。由于`docs`和`colossalai`的更改，导致文档无法自动更新和部署。,https://github.com/hpcaitech/ColossalAI/issues/129
ColossalAI,这是一个功能新增的issue，涉及主要对象为ColossalAI项目的docker镜像构建工作。,https://github.com/hpcaitech/ColossalAI/issues/128
ColossalAI,这是一个功能增强类型的issue，主要涉及到ColossalAI中的MoE并行功能。这个问题可能是由于需要对内存优化进行更新而提出。,https://github.com/hpcaitech/ColossalAI/issues/127
ColossalAI,这是一个功能需求类型的issue，主要涉及的对象是ColossalAI项目。,https://github.com/hpcaitech/ColossalAI/issues/126
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的scatter/gather优化。原因是用户希望为pipeline并行添加scatter/gather优化以减少通信负载。,https://github.com/hpcaitech/ColossalAI/issues/123
ColossalAI,这是一个功能需求报告，主要涉及ColossalAI中关于各种批处理格式的兼容性问题。由于当前实现中只能处理字典值形式的批处理，导致在处理其他格式批次时无法正确确定批次大小，进而引发问题。,https://github.com/hpcaitech/ColossalAI/issues/120
ColossalAI,这个issue类型是用户提出需求，主要涉及ColossalAI项目是否支持梯度检查点技术，原因在于用户认为梯度检查点是一种有效的技术，建议在示例或基准脚本中添加相关内容。,https://github.com/hpcaitech/ColossalAI/issues/117
ColossalAI,这个issue类型为示例请求，主要涉及到GPT2模型。这个问题的提出可能是为了展示如何使用GPT2或如何将其集成到特定项目中。,https://github.com/hpcaitech/ColossalAI/issues/115
ColossalAI,这是一个功能需求的issue，涉及主要对象为ColossalAI中的MoE parallel功能。,https://github.com/hpcaitech/ColossalAI/issues/113
ColossalAI,这是一个需求类型的issue，主要涉及ColossalAI的层集成文档更新。可能由于现有文档不够清晰或者缺失关键信息导致用户需求更新。,https://github.com/hpcaitech/ColossalAI/issues/108
ColossalAI,这个issue属于用户提出需求类型，主要对象是针对ColossalAI中的WebtextDataset，用户询问如何准备数据来运行gpt2示例。,https://github.com/hpcaitech/ColossalAI/issues/106
ColossalAI,这个issue是关于用户提出的一个功能需求，主要涉及的对象是如何在ColossalAI中快速测试一个Hugging Face Transformer模型。用户提出该问题的原因是缺乏明确的文档指导如何将Hugging Face模型移植到ColossalAI，并希望获得相应的解决方案。,https://github.com/hpcaitech/ColossalAI/issues/103
ColossalAI,该issue属于功能需求类型，涉及的主要对象为ColossalAI的默认日志记录器。由于现有的默认日志记录器名称为'root'，需要替换为'colossalai'，同时需要添加`disable_existing_loggers()`函数以在调用`colossalai.launch()`之前禁用现有的日志记录器，以解决存在的问题（例如torch.distributed.distributed_c10d的问题）。,https://github.com/hpcaitech/ColossalAI/issues/101
ColossalAI,该issue类型为功能需求，主要涉及ColossalAI日志系统的默认设置更新。,https://github.com/hpcaitech/ColossalAI/issues/100
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中VIT示例代码的更新。,https://github.com/hpcaitech/ColossalAI/issues/99
ColossalAI,这是一个用户提出需求的 issue，主要对象是该项目中的 VIT 示例。由于新的 API 更新，需要调整 VIT 示例代码以适配新的接口。,https://github.com/hpcaitech/ColossalAI/issues/98
ColossalAI,这个issue是用户提出需求，请求添加randaug功能。这个需求是为了增强ColossalAI的功能性，提供更丰富的数据增强选项。,https://github.com/hpcaitech/ColossalAI/issues/96
ColossalAI,这是一个功能需求的issue，主要涉及到ColossalAI中的pipeline调度优化问题。,https://github.com/hpcaitech/ColossalAI/issues/94
ColossalAI,这个issue是一个用户提出的需求，主要涉及ColossalAI库中的`PipelineSchedule`和`InterleavedPipelineSchedule`，旨在通过添加一个`tensor_shape`参数来进一步减少通信量。,https://github.com/hpcaitech/ColossalAI/issues/93
ColossalAI,这个issue属于功能更新类型，涉及ColossalAI的1D layer apis、nn.layer模块、版本dropout layer以及测试的修改。,https://github.com/hpcaitech/ColossalAI/issues/92
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中pipeline GPU内存优化的问题，由于GPU内存使用过多，用户提出需要优化loss累积和减少输出内存使用的需求。,https://github.com/hpcaitech/ColossalAI/issues/91
ColossalAI,这是一个功能性更改的issue，涉及到ColossalAI中的`PIPELINE_PREV`和`PIPELINE_NEXT`进程组，由于现在使用了P2P通信，所以这两个进程组不再需要。,https://github.com/hpcaitech/ColossalAI/issues/88
ColossalAI,这个issue是关于功能需求的，主要涉及到ColossalAI库中的pipeline shared module wrapper和load batch功能的更新。原因是为了支持GPT/BERT训练中的管道并行处理，需要添加新的功能并更新加载数据批次的规则。,https://github.com/hpcaitech/ColossalAI/issues/87
ColossalAI,这是一个功能增强的issue，主要涉及ColossalAI中的model parallel process group功能。,https://github.com/hpcaitech/ColossalAI/issues/86
ColossalAI,这个issue是一个功能增强类型的需求，主要涉及ColossalAI项目中的模型并行相关内容，由于需要增加模型并行初始化器和单元测试，可能是为了改进模型并行训练过程中的性能和稳定性。,https://github.com/hpcaitech/ColossalAI/issues/84
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中的Layer集成功能。由于对API重命名、指标重构、初始化重构等操作，用户提出了新的需求或对已有功能的改进。,https://github.com/hpcaitech/ColossalAI/issues/83
ColossalAI,这是一个功能增强的issue，主要涉及ColossalAI在MNIST图像分类中的应用示例。,https://github.com/hpcaitech/ColossalAI/issues/81
ColossalAI,这个issue类型是用户提出需求，涉及主要对象是为ColossalAI添加一个使用迁移学习进行二进制图像分类示例。,https://github.com/hpcaitech/ColossalAI/issues/78
ColossalAI,这是一个功能需求的issue，主要涉及ColossalAI中的PipelineModelInitializer，改动了初始化模型的方式。,https://github.com/hpcaitech/ColossalAI/issues/77
ColossalAI,该issue类型是性能优化建议，主要对象是2d操作过程中的计算和通信。,https://github.com/hpcaitech/ColossalAI/issues/75
ColossalAI,这是一个需求提出类的issue，主要涉及性能优化问题，希望通过在2D操作中重叠计算和通信来提高性能。,https://github.com/hpcaitech/ColossalAI/issues/74
ColossalAI,这是一个需求类型的issue，主要涉及到性能优化问题，用户希望通过在2D操作中重叠计算和通信来提高性能。,https://github.com/hpcaitech/ColossalAI/issues/73
ColossalAI,这个issue属于特性请求类型，主要涉及ColossalAI中添加了交错流水线功能。原因可能是用户想要了解如何使用这个新功能。,https://github.com/hpcaitech/ColossalAI/issues/72
ColossalAI,该issue类型为需求提出，主要涉及对象为ColossalAI的pipeline功能。这个问题的原因可能是用户希望在ColossalAI中添加一个新的交错管线功能来提高效率。,https://github.com/hpcaitech/ColossalAI/issues/71
ColossalAI,这个issue是一个需求提出，主要涉及性能优化的问题，要求通过重叠计算和通信来提高2D操作的性能。,https://github.com/hpcaitech/ColossalAI/issues/70
ColossalAI,这个issue类型为功能增强，涉及主要对象为持续集成（CI）系统。由于需要在该项目中添加自动构建和单元测试功能，因此增加了自动化部署运行器，并调整了测试文件夹以适应单元测试。,https://github.com/hpcaitech/ColossalAI/issues/69
ColossalAI,这个issue属于用户提出需求类型，主要涉及对象是ColossalAI库下的GPT模型。由于用户期望添加1D和原始版GPT模型，因此提出了这个需求。,https://github.com/hpcaitech/ColossalAI/issues/67
ColossalAI,这个issue类型是需求更新，涉及的主要对象是issue模板。由于issue模板可能已过时或需要改进，用户提出了更新issue模板的需求。,https://github.com/hpcaitech/ColossalAI/issues/66
ColossalAI,这是一个用户提出需求的issue，主要涉及的对象是ColossalAI项目中的激活检查点。由于需要将激活检查点离线到CPU内存，用户希望实现这一功能。,https://github.com/hpcaitech/ColossalAI/issues/65
ColossalAI,这个issue类型是功能需求，涉及的主要对象是ColossalAI中的MPI示例，用户提出需要关于使用MPI在多个GPU上训练模型自动分割眼底图像中血管的功能。,https://github.com/hpcaitech/ColossalAI/issues/64
ColossalAI,这是一个关于更新API文档的issue，属于用户提出需求类型，主要涉及ColossalAI项目的Sphinx文档。,https://github.com/hpcaitech/ColossalAI/issues/63
ColossalAI,这个issue属于文档更新类型，涉及的主要对象是ColossalAI的API。由于新增了新的API接口，需要更新英文的markdown文档以反映这些更改。,https://github.com/hpcaitech/ColossalAI/issues/60
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的API更新。原因可能是为了提高API的易用性和灵活性。,https://github.com/hpcaitech/ColossalAI/issues/58
ColossalAI,这个issue类型是功能增强，涉及到代码示例。由于缺少详细说明或具体示例，可能需要进一步交流以完善这些示例。,https://github.com/hpcaitech/ColossalAI/issues/57
ColossalAI,这个issue类型为用户提出需求，该问题单涉及的主要对象是ColossalAI中的vit-b16模型训练配置文件。用户提出了需要从头开始使用imagenet数据集训练vit-b16模型的需求。,https://github.com/hpcaitech/ColossalAI/issues/56
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI的kernel模块，是希望添加特定功能的请求。,https://github.com/hpcaitech/ColossalAI/issues/55
ColossalAI,这个issue为功能改进类型，涉及的主要对象是ColossalAI中的2.5d layer code。原因可能是为了在imagenet1k数据集上获得类似的准确性。,https://github.com/hpcaitech/ColossalAI/issues/54
ColossalAI,这是一个用户提出需求的类型，主要涉及的对象是ColossalAI在Google Colab平台上进行高光谱影像处理示例，用户可能提出了关于如何在ColossalAI中应用于高光谱影像处理的问题或者寻求相关帮助。,https://github.com/hpcaitech/ColossalAI/issues/53
ColossalAI,这是一个更新代码的issue，涉及的主要对象是2.5d代码。,https://github.com/hpcaitech/ColossalAI/issues/52
ColossalAI,这是一个优化性能的issue，主要涉及的对象是ColossalAI中的3D实现。 这个问题的原因是3D层运算速度较慢，用户提出了优化方案来解决这个问题。,https://github.com/hpcaitech/ColossalAI/issues/51
ColossalAI,这是一个功能需求类型的issue，主要对象是支持使用torch ddp作为默认的ddp策略。,https://github.com/hpcaitech/ColossalAI/issues/49
ColossalAI,这是一个用户的需求，主要涉及如何构建 TFRecord 数据集，可能由于用户希望从原始Imagenet1K数据集构建TFRecord数据集而提出。,https://github.com/hpcaitech/ColossalAI/issues/48
ColossalAI,这是一个用户需求类型的 issue，主要涉及ColossalAI项目中的配置文件及Python文件的描述和修改。用户希望能够在单台机器上运行，在运行时需要进行一些修改。,https://github.com/hpcaitech/ColossalAI/issues/46
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中vit-b16例子的缺少细节问题。,https://github.com/hpcaitech/ColossalAI/issues/45
ColossalAI,这个issue属于用户提出需求类型，主要涉及ColossalAI中Vit-b16模块的单机运行方式和Python文件描述内容，用户需要更详细的使用说明和文件描述。问题根因可能是当前文档缺乏相关信息，导致用户无法顺利运行或理解代码。,https://github.com/hpcaitech/ColossalAI/issues/44
ColossalAI,这是一个用户提出需求的issue，针对的主要对象是ColossalAI中的vit-b16示例。由于缺乏细节，用户希望在vit-b16示例中添加一些详细信息。,https://github.com/hpcaitech/ColossalAI/issues/43
ColossalAI,这个issue类型是用户提出需求，主要涉及的对象是在ColossalAI中添加自监督SimCLR训练的示例。由于缺乏关于如何进行自监督SimCLR训练的示例，用户请求添加此内容。,https://github.com/hpcaitech/ColossalAI/issues/42
ColossalAI,这是一个用户提出需求的issue，主要对象是在ColossalAI中添加SimCLR自监督学习的示例。,https://github.com/hpcaitech/ColossalAI/issues/41
ColossalAI,这是一个功能需求的issue，主要涉及到优化pipeline的通信方式。,https://github.com/hpcaitech/ColossalAI/issues/40
ColossalAI,这是一个用户提出需求的issue，主要涉及Colossal-AI中集成1d张量并行处理。,https://github.com/hpcaitech/ColossalAI/issues/39
ColossalAI,这是一个关于性能优化的问题，主要涉及的对象是ColossalAI中的2D和1D ViT收敛性，用户寻求关于修复和优化的帮助。,https://github.com/hpcaitech/ColossalAI/issues/38
ColossalAI,这是一个用户提出需求的issue，主要涉及对象是在只有一个多GPU服务器且没有分布式系统的情况下，如何利用所有GPU来训练模型。,https://github.com/hpcaitech/ColossalAI/issues/37
ColossalAI,这个issue类型是用户提出需求，主要对象是ColossalAI中的ViT示例，用户请求添加ViT 32K示例文件的解释。,https://github.com/hpcaitech/ColossalAI/issues/36
ColossalAI,这个issue是用户提出需求类型，主要对象是ViT（Vision Transformer）例子。,https://github.com/hpcaitech/ColossalAI/issues/35
ColossalAI,这个issue类型属于功能需求提出，主要涉及到ColossalAI中的timer功能。由于前几步给出不稳定的读数，需要添加一个ignore steps参数来忽略这些步骤。,https://github.com/hpcaitech/ColossalAI/issues/34
ColossalAI,这个issue是一个功能需求，需要为计时器添加一个忽略步骤参数，以便忽略给出不稳定读数的前几个步骤。,https://github.com/hpcaitech/ColossalAI/issues/33
ColossalAI,这是一个用户提出需求的issue，主要涉及Colossal-AI中的1d tensor并行集成。由于缺乏1d基础层的更新，transformers/ViT层的添加以及相关功能的缺失，用户提出了需要修改和更新的问题。,https://github.com/hpcaitech/ColossalAI/issues/32
ColossalAI,这是一个功能需求类型的issue，主要涉及lr_scheduler中warmup step和epoch混合使用导致使用者困惑，建议优化命名并改善用户体验。,https://github.com/hpcaitech/ColossalAI/issues/31
ColossalAI,这是一个需求提出的issue，主要涉及的对象是ColossalAI中的代码库。由于缺少ViT-B/16的示例并且需要移除LAMB中的w_norm剪裁，需要添加相应示例和调整代码逻辑。,https://github.com/hpcaitech/ColossalAI/issues/29
ColossalAI,该issue属于用户提出需求类型，主要涉及ColossalAI中的ViTB/16及LAMB模型，用户希望在代码中添加一个ViTB/16示例并移除LAMB中的w_norm剪裁。,https://github.com/hpcaitech/ColossalAI/issues/28
ColossalAI,这个issue类型为功能需求，主要涉及对象为ColossalAI项目中的TP-compatible Torch AMP和trainer API。,https://github.com/hpcaitech/ColossalAI/issues/27
ColossalAI,这是一个用户提出需求的issue，主要对象是ColossalAI项目的文档生成功能。这个问题是关于控制文档语言的需求，用户提出了通过设置环境变量来切换语言，但默认语言为英语，需要支持中文，并提供了生成中文文档的命令。,https://github.com/hpcaitech/ColossalAI/issues/25
ColossalAI,该issue为功能需求类型，涉及文档生成过程中的语言切换功能，用户提出了需要通过设置环境变量来控制文档生成语言的需求。,https://github.com/hpcaitech/ColossalAI/issues/24
ColossalAI,这个issue是一个优化性质的需求提出，主要涉及ColossalAI项目中的Trainer、Engine和Schedule模块的一致性以及逻辑性问题。,https://github.com/hpcaitech/ColossalAI/issues/23
ColossalAI,这是一个用户提出需求的issue，主要涉及ColossalAI中的Engine模块，用户提出需要添加梯度累积功能和修复学习率调度器，原因是用户需要更灵活地控制梯度累积和学习率调度器更新的方式。,https://github.com/hpcaitech/ColossalAI/issues/17
ColossalAI,这是一个文档更新的类似需求类型的issue，主要涉及ColossalAI的说明文档。,https://github.com/hpcaitech/ColossalAI/issues/13
ColossalAI,这个issue属于文档优化类型，主要对象是ColossalAI中的文档。由于提交了中文文档和修正了英文文档中的拼写错误，这个issue属于文档内容更新，不属于bug报告或其他类型。,https://github.com/hpcaitech/ColossalAI/issues/9
ColossalAI,这是一个文档更新的issue，涉及ColossalAI项目的中英文文档内容。原因可能是为了完善项目的中文文档和修复英文文档中存在的拼写错误和不一致之处。,https://github.com/hpcaitech/ColossalAI/issues/8
ColossalAI,这个issue类型是文档更新，涉及对象是ColossalAI仓库的文档。原因是添加了中文文档并修复了英文文档中的拼写错误和不一致之处。,https://github.com/hpcaitech/ColossalAI/issues/7
ColossalAI,这是一个需求问题，涉及的主要对象是并行化文档，由于实验性并行化方法被移动到了张量并行化方法之上。,https://github.com/hpcaitech/ColossalAI/issues/4
ColossalAI,这个issue类型为文档改进建议，主要涉及的对象是ColossalAI项目的并行化文档。由于描述张量并行方法的部分在描述流水线并行方法之后，可能导致读者难以快速找到所需信息，需要调整顺序以优化文档组织结构。,https://github.com/hpcaitech/ColossalAI/issues/2
ColossalAI,这个issue类型是用户提出需求，涉及主要对象是Pipeline Parallel功能。用户寻求帮助指定一个自定义调度表。,https://github.com/hpcaitech/ColossalAI/issues/1
mindnlp,这是一个用户提出需求的issue，主要对象是mindnlp库中的qwen2部分。由于需要对qwen2进行修改以支持jit，用户提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/2028
mindnlp,这个issue属于需求提出类型，主要涉及的对象是开源实习中使用GIT模型进行图像描述。该问题提出了需求在自定义数据集上微调GIT模型的笔记本。,https://github.com/mindspore-lab/mindnlp/issues/2027
mindnlp,这是一个用户提出需求的issue，主要涉及bart模型微调，由于未完整描述实现内容，用户提出相关任务I的编号。,https://github.com/mindspore-lab/mindnlp/issues/2026
mindnlp,该问题属于需求修改类，主要涉及mindnlp中的qwen2对象。由于用户希望对qwen2进行修改，导致了此需求提出。,https://github.com/mindspore-lab/mindnlp/issues/2023
mindnlp,"这是一个需求类型的issue，主要涉及的对象是""audio_spectrogram_transformer""模型，用户在进行微调时遇到了问题或者需要帮助。",https://github.com/mindspore-lab/mindnlp/issues/2022
mindnlp,这是一个用户提出需求类型的issue，主要涉及Autoformer模型微调任务，用户寻求关于如何进行Autoformer模型微调的帮助。,https://github.com/mindspore-lab/mindnlp/issues/2019
mindnlp,这是一个特性新增（Feature）类型的需求提出，涉及到新增WhisperFlashAttention2功能。由于缺少详细描述，无法准确判断导致了什么样的问题。,https://github.com/mindspore-lab/mindnlp/issues/2018
mindnlp,这个issue类型属于用户提出需求，涉及的主要对象是mindnlp 0.5.0版本初始化与mindtorch。由于用户希望在mindnlp中集成mindtorch，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/2016
mindnlp,这个issue类型是用户提出需求，主要对象是新增的WhisperFlashAttention2功能。由于软件环境设置问题，用户在Ascend环境中测试时遇到困难。,https://github.com/mindspore-lab/mindnlp/issues/2015
mindnlp,这是一个功能需求报告，主要对象是支持 Whisper + FlashAttention2。由于截断了描述，用户正在提出涉及某项功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/2014
mindnlp,这是一个用户提出需求的类型，涉及到Autoformer模型的微调。这个问题单可能是为了寻求帮助来进行Autoformer模型的微调工作。,https://github.com/mindspore-lab/mindnlp/issues/2013
mindnlp,这是一个用户提出需求的issue，主要对象是对audio_spectrogram_transformer模型的微调报告。,https://github.com/mindspore-lab/mindnlp/issues/2010
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是Mamba2模型迁移。,https://github.com/mindspore-lab/mindnlp/issues/2009
mindnlp,该issue类型为用户提出需求，主要涉及到Albet模型微调任务链接。原因是用户在github上提出了对Albet模型微调的需求或帮助请求。,https://github.com/mindspore-lab/mindnlp/issues/2008
mindnlp,这个issue属于功能增强类型，主要对象是mindnlp的项目。原因可能是为了添加支持Qwen2.5模型而提出的需求。,https://github.com/mindspore-lab/mindnlp/issues/2003
mindnlp,这是用户提交的一个功能需求，涉及到移植glpn模型在mindspore框架下的推理代码。根据问题描述，该需求主要关注在开源实习的应用开发任务中使用这个模型。,https://github.com/mindspore-lab/mindnlp/issues/2002
mindnlp,这是一个用户提出需求的issue，主要涉及MindNLP项目在持续集成中使用官方的MindSpore。由于可能当前CI系统未配置使用官方的MindSpore，导致用户提出这一需求。,https://github.com/mindspore-lab/mindnlp/issues/1999
mindnlp,这个issue类型是功能需求，主要涉及的对象是triton自定义操作。用户提出希望mindnlp支持triton自定义操作，可能是为了增强模型的灵活性和适用性。,https://github.com/mindspore-lab/mindnlp/issues/1990
mindnlp,这个issue属于用户提出需求类型，主要涉及对象是mindnlp软件的NPU调用。由于用户想了解如何指定使用NPU，并且是否类似于使用torch的方式，因此提出了有关指定调用NPU的问题。,https://github.com/mindspore-lab/mindnlp/issues/1976
mindnlp,这是一个需求类型的issue，主要涉及 BEiT 模型的微调，用户提出了实现微调任务的链接。由于该模型可能需要进一步微调以满足特定需求或者提高性能，用户寻求相关支持或资源以完成该任务。,https://github.com/mindspore-lab/mindnlp/issues/1975
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp中是否有类似transformer中FlashAttentionKwargs的实现。,https://github.com/mindspore-lab/mindnlp/issues/1974
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是对bigbird_pegasus模型在databricks/databricksdolly15k数据集上的微调实验。由于用户需要开源实习，希望进行模型微调的任务。,https://github.com/mindspore-lab/mindnlp/issues/1972
mindnlp,这是一个关于开源实习项目中未合并提交请求的问题，涉及的主要对象是LayoutLM模型应用开发。原因是之前提交的PR太久未合并，导致开发者将后续任务的commit传上来。,https://github.com/mindspore-lab/mindnlp/issues/1971
mindnlp,这是一个用户提出开源实习需求的issue，涉及到LayoutXLM模型应用开发。由于该模型应用的需求或者问题，用户寻求相关的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1970
mindnlp,这是一个用户提出需求的issue，主要涉及将DeepSeekR1DistillQwen1.5B的akpt模型转成mindir，可能是由于缺乏相关操作指引导致用户无法完成这一操作。,https://github.com/mindspore-lab/mindnlp/issues/1968
mindnlp,这是一个用户提出需求类型的issue，该问题单涉及的主要对象是OneFormer模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1967
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是blip_2模型微调。由于实现了blip_2在Food500Cap数据集，用户在issue中寻求开源实习机会。,https://github.com/mindspore-lab/mindnlp/issues/1965
mindnlp,"这是一个用户提出需求的issue，主要涉及的对象是""bertweet模型""，问题由于需要实现bertweet在hate_speech任务上的微调。",https://github.com/mindspore-lab/mindnlp/issues/1964
mindnlp,这是一个需求类型的issue，涉及任务Table_transformer应用实现，问题是Table_transformer模型尚未迁移导致无法实现。,https://github.com/mindspore-lab/mindnlp/issues/1959
mindnlp,这个issue是用户提出需求的类型，涉及的主要对象是LayoutLM模型。由于微调后模型在test集上的对比效果，用户希望进行分析并寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1957
mindnlp,这个issue属于用户提出需求类型，主要对象是ViTMAE模型应用开发。由于需要开发ViTMAE模型应用，用户寻求相关帮助或者资源。,https://github.com/mindspore-lab/mindnlp/issues/1956
mindnlp,这是一个提出需求的issue，主要涉及的对象是代码中的数据类型和文件路径。,https://github.com/mindspore-lab/mindnlp/issues/1955
mindnlp,"这是一个需求类型的issue，主要涉及的对象是添加一个名为""janus pro""的功能或组件。",https://github.com/mindspore-lab/mindnlp/issues/1954
mindnlp,这是一个功能需求类型的issue，主要涉及修改图片路径。原因可能是用户希望更改现有图片路径以满足特定需求。,https://github.com/mindspore-lab/mindnlp/issues/1953
mindnlp,这是一个用户提出需求的类型的issue，主要涉及的对象是向mindnlp添加Janus Pro模型。,https://github.com/mindspore-lab/mindnlp/issues/1952
mindnlp,这是一个需求提出类型的issue，主要对象是要对Janus pro model进行adapt。该问题可能由于当前模型功能不完善或用户需求变更导致。,https://github.com/mindspore-lab/mindnlp/issues/1951
mindnlp,这个issue是用户提出的需求类型，主要对象是mindnlp中的Bernoulli概率函数，由于prob参数未作为位置参数公开导致用户希望将其公开作为位置参数。,https://github.com/mindspore-lab/mindnlp/issues/1949
mindnlp,该issue类型为用户提出需求，针对的主要对象是Jukebox模型迁移。,https://github.com/mindspore-lab/mindnlp/issues/1948
mindnlp,这是一个用户提出需求的类型，主要对象是Perceiver IO模型应用开发。由于用户寻求开源实习机会，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1947
mindnlp,这个issue类型是开源项目ConvNeXT模型应用开发的需求提议。,https://github.com/mindspore-lab/mindnlp/issues/1946
mindnlp,这个issue类型是功能需求报告，主要涉及的对象是mimi模型迁移。由于用户需要迁移mimi模型，可能原因是需要将模型应用到不同的环境或任务中。,https://github.com/mindspore-lab/mindnlp/issues/1942
mindnlp,这是一个用户提出需求的Issue，主要涉及Mindnlp中mimi模型的迁移。,https://github.com/mindspore-lab/mindnlp/issues/1940
mindnlp,这是一个用户提出需求的类型的issue，主要涉及SigLIP模型迁移，用户寻求相关实习开源项目的支持。,https://github.com/mindspore-lab/mindnlp/issues/1939
mindnlp,这是一个需求类型的issue，主要涉及的对象是Mindnlp项目中的mimi模型。,https://github.com/mindspore-lab/mindnlp/issues/1937
mindnlp,这是一个迁移模型的功能需求，主要涉及MindNLP套件的更新。,https://github.com/mindspore-lab/mindnlp/issues/1935
mindnlp,这是一个需求提出的issue，主要对象是迁移mimi模型，由于模型迁移的相关需求或问题导致。,https://github.com/mindspore-lab/mindnlp/issues/1934
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是Mindnlp打卡任务中的mimi模型迁移。,https://github.com/mindspore-lab/mindnlp/issues/1933
mindnlp,这是一则需求提出类型的issue，主要讨论了Mindnlp中关于mimi模型迁移的任务。,https://github.com/mindspore-lab/mindnlp/issues/1931
mindnlp,"这是一则需求问题，相关对象为""mimi模型""，提问者请求对""mimi模型""进行迁移。",https://github.com/mindspore-lab/mindnlp/issues/1930
mindnlp,这是一个关于迁移模型的任务需求，主要对象是mimi模型。由于需要将mimi模型进行迁移，可能涉及到模型结构、参数或者训练数据的转移，用户可能需要帮助解决相关问题或获取支持。,https://github.com/mindspore-lab/mindnlp/issues/1929
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是模型迁移。用户可能因为需要迁移某个模型而提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1928
mindnlp,这是一个需求提出类型的 issue，主要对象是 Mimi package。由于用户需要某些功能或改进，因此提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1927
mindnlp,这是一个用户提出需求的 issue，主要涉及到迁移Mimi模型，用户可能是在寻求关于如何迁移该模型的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1925
mindnlp,这是一个用户提出需求的issue，涉及的主要对象是mimi。由于需要迁移mimi，用户提出了相关的需求。,https://github.com/mindspore-lab/mindnlp/issues/1924
mindnlp,该issue属于用户提出的需求类型，主要涉及T5模型应用开发，用户希望使用mindnlp和mindspore对T5模型进行微调文本摘要总结。,https://github.com/mindspore-lab/mindnlp/issues/1920
mindnlp,这是一个用户提出需求类型的issue，主要对象是Mimi模型移植。用户可能遇到了移植Mimi模型时的问题，需要寻求帮助解决。,https://github.com/mindspore-lab/mindnlp/issues/1919
mindnlp,这是一个用户提出的需求类型的issue，涉及的主要对象是Configuration，Model和Unit tests。,https://github.com/mindspore-lab/mindnlp/issues/1916
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是迁移mimi模型。由于需求迁移的需求变化或者系统迁移导致的问题，用户希望能够将mimi模型顺利迁移到另一个环境中。,https://github.com/mindspore-lab/mindnlp/issues/1915
mindnlp,这是一个需求类型的issue，涉及将mimi迁移，并学习他人的代码。原因可能是为了改进现有功能或添加新功能。,https://github.com/mindspore-lab/mindnlp/issues/1914
mindnlp,这是一个开源实习相关的需求提出类的issue，主要涉及对象是bert_generation模型，用户寻求在coco2017数据集上的ZOC任务全参微调。,https://github.com/mindspore-lab/mindnlp/issues/1911
mindnlp,这是一个需求提出的类型，主要对象是将mimi模型迁移到mindnlp，可能由于mindnlp平台的变化或更新，用户需要帮助实现这一迁移过程。,https://github.com/mindspore-lab/mindnlp/issues/1910
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是TrOCR模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1908
mindnlp,这个issue类型是请求更新（update）且涉及主要对象qwen2_moe，由于缺失具体内容，无法具体分析导致的原因或描述问题的症状。,https://github.com/mindspore-lab/mindnlp/issues/1901
mindnlp,这是一个用户提出需求的类型，主要对象是昇腾AI创新大赛。这个问题由于缺少具体内容而被提出。  ,https://github.com/mindspore-lab/mindnlp/issues/1899
mindnlp,这是一个用户提出需求的类型，主要对象是blip模型微调任务。由于用户需要进行blip模型微调，提出了开源实习问题。,https://github.com/mindspore-lab/mindnlp/issues/1896
mindnlp,这个issue属于用户提出需求类型，涉及主要对象是对bert_japanese模型在wrime数据集上的微调。由于未完整提供任务链接，用户可能需要寻求帮助或者反馈微调过程中遇到的问题。,https://github.com/mindspore-lab/mindnlp/issues/1892
mindnlp,这是一个用户提出需求的类型，主要涉及文件下载时的并行操作，由于缺乏文件锁机制导致多线程环境下可能出现文件下载错位的问题。,https://github.com/mindspore-lab/mindnlp/issues/1887
mindnlp,这是一个需求类型的issue，主要涉及MindNLP项目中的core.ops模块更新问题。,https://github.com/mindspore-lab/mindnlp/issues/1884
mindnlp,这是一个用户提出需求的issue，主要涉及minicpm3模型和动态推断演示。通过用户希望添加minicpm3模型和动态推断演示，以增加功能和展示特定功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/1870
mindnlp,这是一个用户提出需求的issue，主要对象是YOLOS模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1867
mindnlp,这是一个需求类型的issue，主要涉及到在MindNLP项目中添加GLUE-QNLI基准测试，包括对10个模型进行推理精度比较。,https://github.com/mindspore-lab/mindnlp/issues/1865
mindnlp,这是一个用户提出需求的类型，主要对象是MaskFormer模型应用开发。由于原因导致用户提出了关于MaskFormer模型应用开发的问题或者寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/1864
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是MindSpore自定义RWKV算子的Python接口实现，用户寻求在该项目中新增RWKV6 Python接口的实现。,https://github.com/mindspore-lab/mindnlp/issues/1862
mindnlp,这个issue类型是功能需求，主要涉及的对象是多卡运行模型；用户提出寻求多个模型在同一进程中多卡运行的功能。,https://github.com/mindspore-lab/mindnlp/issues/1856
mindnlp,这是一个功能需求，用户希望添加关于summarization微调的例子。,https://github.com/mindspore-lab/mindnlp/issues/1854
mindnlp,这个issue是关于功能增强（feature enhancement），主要涉及的对象是mindnlp.Trainer.base类。由于需要决定并行分布类型，所以添加了一个描述加速分布类型的常量。,https://github.com/mindspore-lab/mindnlp/issues/1852
mindnlp,这是一个用户提出需求的issue，主要对象是缺少部分pytorch封装，可能是由于功能缺失导致的。,https://github.com/mindspore-lab/mindnlp/issues/1851
mindnlp,这是一则用户请求进行文本内容解读的issue，主要涉及对象是align论文。,https://github.com/mindspore-lab/mindnlp/issues/1845
mindnlp,这个issue是一个用户提出需求类型的问题，主要涉及mindnlp下的peft_adalora_sep2模型微调。,https://github.com/mindspore-lab/mindnlp/issues/1844
mindnlp,该issue属于用户提出需求类型，主要涉及的对象是TAPAS模型。,https://github.com/mindspore-lab/mindnlp/issues/1839
mindnlp,这是一则关于在value_and_grad函数中支持kwargs的功能需求，涉及主要对象为mindnlp库。由于当前的value_and_grad函数无法处理关键字参数kwargs，因此导致用户提出了这项功能改进请求。,https://github.com/mindspore-lab/mindnlp/issues/1835
mindnlp,该issue为用户提出需求，主要涉及Module的H2D move支持。这个问题可能是由于用户希望在MindNLP中实现模块间的数据传输而提出的。,https://github.com/mindspore-lab/mindnlp/issues/1831
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是image_classification_timm_peft_lora模型微调，用户寻求关于如何微调该模型的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1830
mindnlp,这个issue属于用户提出需求类型，主要涉及对象是将baichuan2模型迁移。原因是用户需要进行迁移操作。,https://github.com/mindspore-lab/mindnlp/issues/1819
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是dpt模型应用开发。由于与代码仓对比时出现错误的字符序列，可能导致需要对dpt模型应用开发的相关内容进行调整或修复。,https://github.com/mindspore-lab/mindnlp/issues/1813
mindnlp,这是一个开发需求类型的issue，该问题涉及的主要对象是dpt模型应用开发。由于缺少必要的代码对比信息，导致无法进行有效的模型应用开发工作。,https://github.com/mindspore-lab/mindnlp/issues/1812
mindnlp,这是一个用户提出需求的类型，主要对象是dpt模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1811
mindnlp,这是一个用户提交需求的issue，主要对象是mindnlp，用户希望增加mindspore推理功能的补丁。,https://github.com/mindspore-lab/mindnlp/issues/1810
mindnlp,这是一个用户提出需求的类型，主要对象是LayoutLMv2模型应用开发。由于用户希望开发LayoutLMv2模型应用，故提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/1805
mindnlp,这是一个需求类型的issue，主要涉及到对项目的初始化操作和移动测试文件夹。可能是为了整理项目结构或者优化开发流程而提出的建议。,https://github.com/mindspore-lab/mindnlp/issues/1804
mindnlp,这是一个用户提出需求的issue，主要涉及到GLM4Voice模型。由于缺少详细描述，无法准确确定用户需求的具体内容。,https://github.com/mindspore-lab/mindnlp/issues/1803
mindnlp,这个issue属于用户提出需求类型，主要涉及的对象是支持fill_mask/image_classification/image_feature_extraction pipelines。这个问题的提出可能是用户需要增加新的功能或支持不同的数据处理流程。,https://github.com/mindspore-lab/mindnlp/issues/1801
mindnlp,这个issue类型是用户提出需求，主要涉及支持深度估计和文档问答功能。,https://github.com/mindspore-lab/mindnlp/issues/1800
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp中LoRA的半精度微调的实现方法，用户希望了解如何在硬件上实现这一功能。,https://github.com/mindspore-lab/mindnlp/issues/1798
mindnlp,"这是一个用户提出需求的issue，主要涉及的对象是""dino应用开发""。由于缺乏具体内容，用户并未描述具体的需求或问题。",https://github.com/mindspore-lab/mindnlp/issues/1797
mindnlp,该issue类型为功能需求添加，主要对象是模型mirror和NLP SIG介绍，可能由于项目原有功能不足以满足相关需求而引起。,https://github.com/mindspore-lab/mindnlp/issues/1787
mindnlp,该issue为用户提出需求类型，主要涉及将MindNLP支持魔乐下载。原因可能是用户希望能够使用魔乐下载等功能，但目前系统尚未提供相关支持。,https://github.com/mindspore-lab/mindnlp/issues/1785
mindnlp,这是一个用户提出需求的issue，涉及的主要对象是项目的README文件。原因是缺少对SIG（Special Interest Group）的简介、使命等信息，用户希望这些内容能够被添加进去。,https://github.com/mindspore-lab/mindnlp/issues/1784
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是BERT模型的应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1781
mindnlp,这个issue类型是用户需求，主要涉及对象是CPU计算中的Gamma操作，由于缺少Gamma功能，用户提出了对CPU计算或更新版本MindSpore的需求。,https://github.com/mindspore-lab/mindnlp/issues/1779
mindnlp,"这是一个需求问题，主要涉及的对象是""wav2vec""。原因可能是用户希望升级""wav2vec""至最新版本以获得新功能或改进。",https://github.com/mindspore-lab/mindnlp/issues/1776
mindnlp,这是一个需求类型的issue，主要涉及静态图加速jit文件。,https://github.com/mindspore-lab/mindnlp/issues/1775
mindnlp,"这是一个功能需求问题，用户需要指导如何在mindnlp中实现类似于""train_dataset.set_format(""torch"")""的格式转换操作。",https://github.com/mindspore-lab/mindnlp/issues/1771
mindnlp,这是一个开源实习项目的招聘需求。,https://github.com/mindspore-lab/mindnlp/issues/1770
mindnlp,该问题类型为用户提出需求，主要对象是实现pipeline支持，由于当前系统无法支持pipeline功能，用户提出了需求并寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/1768
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是Atlas 300I Duo，用户可能想知道是否支持该设备。,https://github.com/mindspore-lab/mindnlp/issues/1767
mindnlp,这个issue属于需求报告类型，主要涉及的对象是Mask2Former模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1764
mindnlp,这个issue是一个需求提出，并且涉及到将`safe_load_file`方法使用`mmap`来加速。原因是希望通过使用`mmap`来提高文件加载的速度。,https://github.com/mindspore-lab/mindnlp/issues/1763
mindnlp,这是一个用户提出需求的issue， 主要对象是支持模型合并功能。 由于描述不完整，导致无法确定具体问题或需求。,https://github.com/mindspore-lab/mindnlp/issues/1753
mindnlp,这是一个用户提出需求的类型，主要对象是项目的文档（readme.md），用户想要增加一个目录来提高文档的导航性。,https://github.com/mindspore-lab/mindnlp/issues/1746
mindnlp,该issue类型为需求提出，主要涉及的对象是GPT-J-6B模型应用开发。由于图片链接无法正常显示，用户可能寻求解决图片加载问题或相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1742
mindnlp,这是一个功能需求提出的issue，主要对象是paddle支持的generate方法，用户希望增加返回序列scores的选项。原因可能是用户希望更多信息用于生成文本序列。,https://github.com/mindspore-lab/mindnlp/issues/1741
mindnlp,这个issue属于需求类型，主要对象为要求添加名为finetune_blip.ipynb的新文件。,https://github.com/mindspore-lab/mindnlp/issues/1739
mindnlp,这是一个请求迁移项目的类型的问题，主要涉及到minimind项目。由于用户希望能够在香橙派aipro上运行大模型以获得更好的体验，因此提出了这个请求。,https://github.com/mindspore-lab/mindnlp/issues/1738
mindnlp,这是一个提出需求类的issue，主要对象是更新readme文件。,https://github.com/mindspore-lab/mindnlp/issues/1736
mindnlp,这是一个需求类型的issue，主要涉及的对象是更新名为prompt_direct.txt的文件。由于未提供具体内容，导致用户需要更新此文件时无法得到准确的指导或提示。,https://github.com/mindspore-lab/mindnlp/issues/1732
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是改进 README.md 文件。原因可能是用户希望更清晰地展示项目的信息，提高文档的质量。,https://github.com/mindspore-lab/mindnlp/issues/1729
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是Speech2Text模型迁移，用户寻求关于该任务的实习机会。,https://github.com/mindspore-lab/mindnlp/issues/1725
mindnlp,这是一个用户提交的需求，要求开发VideoMAE模型应用，涉及MindSpore和PyTorch推理结果对比的问题。,https://github.com/mindspore-lab/mindnlp/issues/1724
mindnlp,该issue类型为软件需求，涉及Unispeech模型迁移，用户寻求帮助将模型迁移到另一个平台。,https://github.com/mindspore-lab/mindnlp/issues/1722
mindnlp,这是一个用户提出需求的issue，主要涉及OrangePi上支持静态图，可能是由于当前系统不支持静态图功能，用户希望在OrangePi上使用此功能。,https://github.com/mindspore-lab/mindnlp/issues/1720
mindnlp,这是一个用户提出需求的issue，主要涉及 llama 支持静态图。原因可能是用户希望 llama 能够对静态图进行支持。,https://github.com/mindspore-lab/mindnlp/issues/1718
mindnlp,"这是一个用户提出需求的issue，主要对象是添加名为""chinese_clip""的功能。由于用户需要该功能来处理中文文本，因此提出了这个需求。",https://github.com/mindspore-lab/mindnlp/issues/1704
mindnlp,这是一个用户提出需求的issue，主要涉及Speech2Text模型的迁移，用户寻求开源实习的相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1702
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是为mindnlp添加dpo trainer和支持dpo训练。,https://github.com/mindspore-lab/mindnlp/issues/1701
mindnlp,这是一个用户提出需求的类型，主要涉及到开源实习的ImageGPT模型应用开发。由于用户寻求关于在华为云AI gallery上开发该模型的帮助而产生。,https://github.com/mindspore-lab/mindnlp/issues/1700
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是Unispeech模型迁移。,https://github.com/mindspore-lab/mindnlp/issues/1699
mindnlp,这个issue类型为需求提出，主要涉及X_CLIP模型应用开发。由于用户想要进行X_CLIP模型应用开发，所以提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1694
mindnlp,这是一个需求类型的issue，主要涉及mllama和yoso的支持和修复。可能由于两者之间的兼容性问题或功能缺失导致用户需要提出这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1692
mindnlp,这是一个开源实习需求的类型为需求提出，主要对象为SAM模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1688
mindnlp,这个issue是用户提出的需求类型，主要涉及Mindnlp支持分布式推理的功能。,https://github.com/mindspore-lab/mindnlp/issues/1687
mindnlp,该问题属于一个功能请求，主要涉及的对象是MindNLP代码库中的cells_to_save和modules_to_save参数。,https://github.com/mindspore-lab/mindnlp/issues/1684
mindnlp,这是一个用户提出需求的issue，主要对象是nn.Module在支持ms.jit时出现的问题。,https://github.com/mindspore-lab/mindnlp/issues/1682
mindnlp,这是一个用户新增功能请求类型的issue，主要涉及到MindPilot项目的README.md文件。由于初步开发完成，用户希望添加MindPilot V0.0.1的信息到README.md中。,https://github.com/mindspore-lab/mindnlp/issues/1681
mindnlp,这是一个用户提出需求的类型。该问题单涉及的主要对象是 BEiT 模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1680
mindnlp,用户提出需求。该问题单涉及的主要对象是在mindnlp\mindnlp\transformers\models\路径下新建模型名称为poly的目录以及__init__.py文件。由于用户需要添加一个名为poly的模型目录，并在其中加入一个__init__.py文件。,https://github.com/mindspore-lab/mindnlp/issues/1679
mindnlp,"这是一个包含需求链接的issue，涉及主要对象是""Accelerate""。原因是用户想要加速操作，但并没有提供具体问题或需求细节。",https://github.com/mindspore-lab/mindnlp/issues/1678
mindnlp,这是一个关于文档完善和实操教程完善建议的问题，涉及到MindNLP官网文档无法切换到中文、直接显示404、API栏目列表一级目录显示为空白、以及缺乏MindNLP实操教程的情况。,https://github.com/mindspore-lab/mindnlp/issues/1675
mindnlp,这是一个更新请求，目标是将Whisper应用程序更新为WhisperV3版本。这可能是由于WhisperV3版本具有新的功能或者修复了之前版本中的bug。,https://github.com/mindspore-lab/mindnlp/issues/1674
mindnlp,这是一个用户提出需求的类型，主要涉及mindnlp支持Whisper Transcribe Audio功能。通过这个问题描述可以看出，用户希望mindnlp能够支持Whisper Transcribe Audio功能。,https://github.com/mindspore-lab/mindnlp/issues/1673
mindnlp,"这是一个用户提出需求的类型，主要涉及mindnlp库中的模型""qwen2_vl""。 由于用户需求新增支持此特定模型，需要开发人员进行相应的支持工作。",https://github.com/mindspore-lab/mindnlp/issues/1672
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp。由于需要新增mindbnb和支持8bit量化，用户希望对mindnlp进行相应改进。,https://github.com/mindspore-lab/mindnlp/issues/1671
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp，用户希望添加mindbnb并支持8位量化。,https://github.com/mindspore-lab/mindnlp/issues/1670
mindnlp,这是一个用户提出需求的类型，主要对象是Speech2Text模型迁移。由于答案中没有提供具体内容，无法分析具体原因导致的问题或需要帮助的内容。,https://github.com/mindspore-lab/mindnlp/issues/1669
mindnlp,这个issue属于用户提出需求类型，主要涉及mindnlp项目和mindbnb，请求添加mindbnb并支持8位量化。,https://github.com/mindspore-lab/mindnlp/issues/1665
mindnlp,这是一个请求查看的类型，涉及的主要对象是SegFormer模型应用开发。可能由于对该模型应用的开发存在疑问或者需要老师审查。,https://github.com/mindspore-lab/mindnlp/issues/1663
mindnlp,这是一个用户提出需求的类型的issue，涉及的主要对象是SegFormer模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1662
mindnlp,这是一个用户提出需求的issue，主要涉及对象是OrangePi，由于需要支持int8量化，用户请求相关功能的支持。,https://github.com/mindspore-lab/mindnlp/issues/1659
mindnlp,这个issue类型为用户提出需求，该问题单涉及的主要对象是mindnlp下的一个模型文件。由于新增的模型文件 qwen1.5-0.b，用户请求将其添加到 orangepi 平台。,https://github.com/mindspore-lab/mindnlp/issues/1658
mindnlp,这是一个关于用户提出需求的问题，主要涉及Whisper模型推理功能的例子，由于用户不清楚如何进行语言类型检测和流式输出，导致提问。,https://github.com/mindspore-lab/mindnlp/issues/1657
mindnlp,这是一个用户提出需求的issue，主要对象是需要qwen2-vl-7b模型。原因可能是当前功能无法满足用户需求。,https://github.com/mindspore-lab/mindnlp/issues/1656
mindnlp,这是一个需求类型的issue，主要涉及到CLIPSeg模型的应用开发。由于原因未详，用户提出了与该模型应用开发相关的问题。,https://github.com/mindspore-lab/mindnlp/issues/1655
mindnlp,该issue类型为需求提出，涉及的主要对象为CLIPSeg模型应用开发。由于需要开发CLIPSeg模型的应用，所以提出了该需求。,https://github.com/mindspore-lab/mindnlp/issues/1654
mindnlp,这是一个用户提出需求的issue，主要涉及对象是将tinyllama添加到OrangePi上。由于未提供具体内容，无法确定导致这个问题的原因。,https://github.com/mindspore-lab/mindnlp/issues/1653
mindnlp,这是一个用户对于支持OrangePi进行BERT推断和训练的需求类型的issue。,https://github.com/mindspore-lab/mindnlp/issues/1651
mindnlp,这个issue是关于需求的，主要对象是模型的微调过程。,https://github.com/mindspore-lab/mindnlp/issues/1650
mindnlp,这是一个用户提出需求的问题单，主要涉及对于`forward`函数参数匹配顺序的支持。用户认为合并时出现了问题。,https://github.com/mindspore-lab/mindnlp/issues/1645
mindnlp,这是一个用户提出需求的issue，该问题单涉及的主要对象是 `dna_lm`，由于缺少 `peft example`，用户请求添加示例。,https://github.com/mindspore-lab/mindnlp/issues/1644
mindnlp,这是一个开源实习相关的任务需求，提及了Vision Transformer模型应用开发，涉及主要对象是Mindnlp项目。,https://github.com/mindspore-lab/mindnlp/issues/1643
mindnlp,这是一个用户提出需求的issue，主要对象是MindNLP的时间记录功能。,https://github.com/mindspore-lab/mindnlp/issues/1635
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp的使用readme以及如何使用ashell1中文数据集重新训练wav2vec2。由于缺乏相关信息，导致用户询问如何操作重新训练过程中的词汇表、分词器以及数据处理。,https://github.com/mindspore-lab/mindnlp/issues/1629
mindnlp,这是一个用户提出需求的类型，主要涉及nllbmoe模型的迁移。用户反映了关于模型pytest slow和py方面的问题。,https://github.com/mindspore-lab/mindnlp/issues/1626
mindnlp,该issue类型是用户提出需求，涉及主要对象是Dora-finetuning模型微调；用户可能需要帮助或者指导如何进行Dora-finetuning模型微调。,https://github.com/mindspore-lab/mindnlp/issues/1620
mindnlp,这个issue是一个性能优化的请求，主要涉及safetensors load功能，请求优化加载速度。,https://github.com/mindspore-lab/mindnlp/issues/1615
mindnlp,这个issue类型为特性功能请求，涉及主要对象是PatchTSMixer模型。由于用户希望进行模型迁移，可能是因为需要在不同环境中使用该模型实现相关任务。,https://github.com/mindspore-lab/mindnlp/issues/1611
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是模型peft_ia3_seq2seq。由于用户希望微调该模型，因此提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/1608
mindnlp,这个issue类型为需求提出，涉及的主要对象是CodeGeeX课程资料更新，用户提出了关于开源实习资料更新的需求。,https://github.com/mindspore-lab/mindnlp/issues/1600
mindnlp,这是一条需求更新的issue，涉及的主要对象是CodeGeeX课程资料。,https://github.com/mindspore-lab/mindnlp/issues/1598
mindnlp,这是一个用户提出需求的类型问题单，主要涉及的对象是mindnlp项目中的comm_func功能。可能由于MS2.2版本缺少与comm_func相关的功能，用户需要添加该功能以满足特定需求。,https://github.com/mindspore-lab/mindnlp/issues/1597
mindnlp,这是一个用户提出需求的issue，主要对象是MindNLP下的CodeGeeX课程资料更新。,https://github.com/mindspore-lab/mindnlp/issues/1596
mindnlp,这是一个需求更新的issue，涉及主要对象为CodeGeeX课程资料。,https://github.com/mindspore-lab/mindnlp/issues/1595
mindnlp,这个issue属于用户提出需求类型，主要对象是模型推理过程中的多进程支持，用户希望能够在多进程环境下进行推理操作。,https://github.com/mindspore-lab/mindnlp/issues/1592
mindnlp,这是一个需求类型的issue，主要涉及MindNLP中添加Roberta_prelayernorm模型，由于用户可能希望在项目中使用这个模型而提出该需求。,https://github.com/mindspore-lab/mindnlp/issues/1587
mindnlp,这是一个用户提出需求的issue，主要涉及MindSpore框架中的BERT/CLIP/MixTral算法优化问题，可能是由于Ascend芯片的兼容性或性能优化方面的需求所致。,https://github.com/mindspore-lab/mindnlp/issues/1586
mindnlp,这是一个用户提出需求的类型的issue，主要涉及MindNLP中的Whisper模块。由于需要更新Whisper功能，用户提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/1580
mindnlp,这是一个需要更新类的issue，涉及的主要对象是代码库中的某个类。,https://github.com/mindspore-lab/mindnlp/issues/1577
mindnlp,这个issue类型为请求更新（update），主要涉及的对象是 qwen2_moe 模块。,https://github.com/mindspore-lab/mindnlp/issues/1573
mindnlp,这是一个需求类型的issue，主要涉及的对象是表格问答pipeline功能。由于模型中的ops和nn替换问题导致了功能无法正常运行。,https://github.com/mindspore-lab/mindnlp/issues/1572
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是flaubert模型迁移。由于需求添加requirement依赖库项并替换特定代码，可能导致相关功能或性能问题，用户提出了对模型迁移的特定需求或建议。,https://github.com/mindspore-lab/mindnlp/issues/1568
mindnlp,这个issue类型是需求提出，主要涉及的对象是mindnlp项目中的模型clvp和conditional_detr，用户在此提出了关于这两个模型的问题或需求。,https://github.com/mindspore-lab/mindnlp/issues/1561
mindnlp,这是一个用户提出需求的issue，在请求支持`nn.AdaptiveLogSoftmaxWithLoss`。,https://github.com/mindspore-lab/mindnlp/issues/1560
mindnlp,这是一个用户提出的需求问题，主要涉及GPTSAN-japanese的迁移，可能是由于预训练模型迁移时的技术难点或需要适配新环境等原因引起。,https://github.com/mindspore-lab/mindnlp/issues/1556
mindnlp,这是一个需求类型的issue，主要涉及的对象是dbrx模型。由于迁移问题，用户寻求关于dbrx模型的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1553
mindnlp,这是一个用户提出需求的issue，主要对象是开源实习项目Pix2struct。,https://github.com/mindspore-lab/mindnlp/issues/1551
mindnlp,这是一个功能需求问题，关于Dbrx模型迁移。 由于某种原因导致希望进行模型迁移，需求相关支持或解决方案。,https://github.com/mindspore-lab/mindnlp/issues/1550
mindnlp,这个issue类型为用户提出需求，涉及对象是更新prefix_tuning_t5，由于需要改进模型的参数调节，用户希望进行相关更新。,https://github.com/mindspore-lab/mindnlp/issues/1543
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp库的vision-encoder-decoder支持。,https://github.com/mindspore-lab/mindnlp/issues/1541
mindnlp,"这个issue类型属于需求提出，主要涉及的对象是更新名为""musicgen""的内容。由于可能需要添加新功能、修复bug或改进性能，用户提出了更新""musicgen""的需求。",https://github.com/mindspore-lab/mindnlp/issues/1535
mindnlp,这是一个提出需求的issue，主要涉及的对象是MindNLP项目中的T5、JetMoe和LayoutLMv3模型更新和添加。,https://github.com/mindspore-lab/mindnlp/issues/1533
mindnlp,该issue属于用户提出需求类型，主要涉及的对象是在MindNLP项目中添加了`vivit`模型和`conv3d`模块。由于用户可能希望扩展项目功能或增加特定模型和模块，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1528
mindnlp,该issue类型为模型迁移需求，主要涉及到depth_anything模型。由于需要开源实习，用户可能遇到了模型迁移相关问题或需求帮助。,https://github.com/mindspore-lab/mindnlp/issues/1525
mindnlp,这个issue类型是用户提出需求，该问题涉及主要对象是更新了GPT IMDb微调示例。,https://github.com/mindspore-lab/mindnlp/issues/1524
mindnlp,这个issue类型是需求提交，主要涉及的对象是florence2模型迁移。,https://github.com/mindspore-lab/mindnlp/issues/1506
mindnlp,这个issue类型为用户提出需求，主要对象是在mindnlp项目中新增一个名为glm4的模块。,https://github.com/mindspore-lab/mindnlp/issues/1504
mindnlp,"这是一个用户提出需求的类型的issue，主要涉及的对象是""florence2模型""。由于需要进行模型迁移，用户可能遇到了一些问题或需要额外的帮助。",https://github.com/mindspore-lab/mindnlp/issues/1500
mindnlp,这是一个用户提出需求的issue，主要对象是开源项目MindNLP中的PegasusX模型。,https://github.com/mindspore-lab/mindnlp/issues/1496
mindnlp,该issue是一个用户提出需求类型的问题，主要涉及的对象是模型应用开发。,https://github.com/mindspore-lab/mindnlp/issues/1495
mindnlp,这是一个用户提出需求的issue，主要对象是新增优化器和更新bloom模型至最新版本。这个需求可能是由于项目需要跟随最新的技术发展或者改进模型性能而提出的。,https://github.com/mindspore-lab/mindnlp/issues/1491
mindnlp,这个issue类型为用户提出需求，主要对象是SuperPoint模型迁移，可能是由于模型迁移过程中遇到了问题或需要帮助。,https://github.com/mindspore-lab/mindnlp/issues/1488
mindnlp,这个issue类型是需要更新到最新版本的需求，主要涉及mindnlp库中的transformers模块，由于huggingface发布了更新版本，用户希望mindnlp库也能及时跟进更新。,https://github.com/mindspore-lab/mindnlp/issues/1484
mindnlp,这个issue类型为用户提出需求，该问题单涉及的主要对象是Mindnlp项目中的RAG模型。,https://github.com/mindspore-lab/mindnlp/issues/1479
mindnlp,这是一个用户提出需求的issue， 主要涉及Mindnlp是否支持Electra模型。,https://github.com/mindspore-lab/mindnlp/issues/1476
mindnlp,这个issue类型是开源实习任务需求，主要涉及的对象是idefics模型，用户提出了关于开源实习任务的需求或寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1475
mindnlp,该issue为用户提出需求，希望添加新的CI Pip，主要对象是MindNLP项目。,https://github.com/mindspore-lab/mindnlp/issues/1474
mindnlp,这是一个需求类型的issue，主要涉及mindnlp库中的ops.unfold功能。此问题可能由于用户需要使用ops.unfold功能，但目前库中尚不支持该功能所致。,https://github.com/mindspore-lab/mindnlp/issues/1473
mindnlp,"这是一个用户提出需求的类型，涉及主要对象是""xlnet_model模型""，用户提出了关于模型迁移的问题或者寻求迁移模型的帮助。",https://github.com/mindspore-lab/mindnlp/issues/1472
mindnlp,这是一个用户提出需求的问题，涉及对象是mindspore库中的.detach()函数。由于mindspore库似乎没有提供.detach()方法，用户疑惑是否可以直接删除此操作。,https://github.com/mindspore-lab/mindnlp/issues/1469
mindnlp,这是一个需求提出类型的issue，涉及主要对象是randperm接口，用户提出了需要使用该接口的需求。,https://github.com/mindspore-lab/mindnlp/issues/1463
mindnlp,这是一个用户提出需求的issue，主要涉及到mindnlp库中的core.ops模块，由于缺少了nansum函数，用户需求添加该功能。,https://github.com/mindspore-lab/mindnlp/issues/1459
mindnlp,这是一个关于功能需求的issue，主要涉及到mindnlp.ops中缺乏one_hot功能的问题，导致用户无法使用该功能。,https://github.com/mindspore-lab/mindnlp/issues/1450
mindnlp,这是一个功能需求问题，用户希望在mindnlp中实现index_select功能，目前尚未实现。,https://github.com/mindspore-lab/mindnlp/issues/1449
mindnlp,这是一个类型为任务需求的issue，主要涉及的对象是SuperPoint模型迁移。可能是因迁移时遇到困难或问题，希望得到支持或帮助。,https://github.com/mindspore-lab/mindnlp/issues/1436
mindnlp,这是一个用户需求类型的问题单，主要涉及对SFT方法的添加和示例的提供。导致这个问题的原因可能是用户需要了解如何使用SFT方法并需要相应配置文件和示例。,https://github.com/mindspore-lab/mindnlp/issues/1434
mindnlp,这是一个用户提出需求的类型。该问题单涉及的主要对象是REALM(refactored)。由于某种重构，用户提出需要更改，或者添加新功能。,https://github.com/mindspore-lab/mindnlp/issues/1432
mindnlp,"这是一个用户提出需求或问题的类型，涉及主要对象为""昇腾AI创新大赛""，用户可能提出了关于Deta的问题或需要相关帮助。",https://github.com/mindspore-lab/mindnlp/issues/1431
mindnlp,这是一个优化性能的issue，主要涉及llama3在Ascend上运行速度的问题。,https://github.com/mindspore-lab/mindnlp/issues/1427
mindnlp,这个issue类型是需求类型，主要涉及NLLB模型迁移。原因可能是用户希望进行NLLB模型迁移并寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1426
mindnlp,这是一个性能优化类型的issue，主要对象是 Ascend 设备，用户提出了希望提升 llama3 在 Ascend 设备上的运行速度的需求。,https://github.com/mindspore-lab/mindnlp/issues/1423
mindnlp,这是一个缺少内容的功能需求提出issue，类型为用户提出需求。主要涉及对象为mindnlp的新模块使用情况。,https://github.com/mindspore-lab/mindnlp/issues/1422
mindnlp,这是一个需求问题，涉及FSMT(refactored)的相关事项，可能是用户提出了关于该功能的问题或需要帮助。,https://github.com/mindspore-lab/mindnlp/issues/1421
mindnlp,这是一个需求报告，涉及到UDop模型的迁移。最可能是用户提出了关于UDop模型迁移的问题或寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1420
mindnlp,这是一个需求类型的issue，主要涉及Udop模型的迁移。原因可能是用户需要帮助或者寻求相关指导。,https://github.com/mindspore-lab/mindnlp/issues/1419
mindnlp,这是一个需求问题，用户提出了Udop模型迁移的需求。这可能是因为用户想要将Udop模型从一个系统迁移到另一个系统，或者对模型的迁移过程有疑问，希望获得相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1418
mindnlp,这个issue类型是需求报告，主要对象是Udop模型迁移。由于用户需要将Udop模型进行迁移，所以提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1416
mindnlp,这是一个功能需求类型的issue，主要涉及的对象是Udop模型和部分layoutlmv3模型。由于需要进行Udop模型迁移和包含layoutlmv3的部分内容，可能是用户希望在模型迁移过程中遇到的问题或请求帮助。,https://github.com/mindspore-lab/mindnlp/issues/1415
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是添加教程使用镜像。由于用户希望使用镜像来进行教程，可能是为了加快速度或者避免网络问题。,https://github.com/mindspore-lab/mindnlp/issues/1412
mindnlp,该issue是用户提出需求或请教问题类型，主要涉及的对象是Perceiver。由于开源实习相关需求或问题，用户提出了关于Perceiver的相关讨论或帮助请求。,https://github.com/mindspore-lab/mindnlp/issues/1411
mindnlp,这是一个用户提出需求的问题单，主要涉及到FUYU(refactored)，可能是基于该模型的重构工作或者相关功能的添加。,https://github.com/mindspore-lab/mindnlp/issues/1410
mindnlp,这是一个性能优化类的issue，主要对象是项目中的chatglm2模块。由于Ascend性能问题导致的速度较慢，用户提出希望能够提升性能。,https://github.com/mindspore-lab/mindnlp/issues/1409
mindnlp,"该issue是一个缺少具体内容的需求提出，该问题单涉及的主要对象是添加名为""model patchtst""的模型。",https://github.com/mindspore-lab/mindnlp/issues/1407
mindnlp,"这是一个用户提出需求的issue，主要对象是向mindnlp库添加一个名为""xglm""的模型。",https://github.com/mindspore-lab/mindnlp/issues/1406
mindnlp,"这个issue是需求提出类型，主要涉及的对象是在Mindnlp项目中添加名为""yolos""的模型。",https://github.com/mindspore-lab/mindnlp/issues/1401
mindnlp,这是一个用户提出需求的类型，主要涉及到本地分支操作。原因可能是用户想要了解关于本地分支的相关问题或者寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1400
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是在mindnlp的项目中的deepseek_v2模块。原因是为了更改transformers/models中的文件结构以支持新功能的添加。,https://github.com/mindspore-lab/mindnlp/issues/1399
mindnlp,这是一个用户提出需求的类型，主要是针对社区实习的XLMProphetNet。,https://github.com/mindspore-lab/mindnlp/issues/1396
mindnlp,这是一个需求类型的issue，主要涉及到xml_ProphetNet。原因是用户在社区实习中寻求关于xml_ProphetNet的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1395
mindnlp,这是一个用户提出需求的issue，主要对象是添加模型Deta到mindnlp中。,https://github.com/mindspore-lab/mindnlp/issues/1391
mindnlp,这个issue是关于如何在Mindnlp.transformers中实现huggingface的VisionEncoderDecoderModel的需求问题，主要涉及对象是模型接口的转换和适配。,https://github.com/mindspore-lab/mindnlp/issues/1390
mindnlp,这是一个用户提出需求的issue，主要对象是Data2vec audio模型迁移。由于用户希望进行模型迁移，所以提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1388
mindnlp,这是一个需求问题，涉及Data2vec audio模型迁移。由于具体内容缺失，用户可能需要帮助解决如何进行Data2vec audio模型的迁移。,https://github.com/mindspore-lab/mindnlp/issues/1387
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是UniSpeechSat项目。由于缺失了完整的Gitee issue链接，导致用户无法直接访问到相关详细信息。,https://github.com/mindspore-lab/mindnlp/issues/1386
mindnlp,这是一个需求类型的issue，主要涉及到Data2vec audio模型的迁移。可能是由于用户需要在该模型上进行相关操作或迁移，向社区寻求帮助或建议。,https://github.com/mindspore-lab/mindnlp/issues/1385
mindnlp,这个issue类型是需求，主要涉及的对象是Data2vec audio模型。由于需要进行模型迁移，用户可能需要帮助或者讨论相关问题。,https://github.com/mindspore-lab/mindnlp/issues/1383
mindnlp,这是一个需求/功能提议类型的issue，主要涉及Data2vec audio模型的迁移。这个问题可能由于用户想要在项目中迁移Data2vec audio模型而产生。,https://github.com/mindspore-lab/mindnlp/issues/1382
mindnlp,这个issue是关于功能需求的，主要涉及Data2vec audio 模型的迁移，用户可能由于迁移UT遇到了问题或疑惑。,https://github.com/mindspore-lab/mindnlp/issues/1379
mindnlp,这是一个需求类型的issue，主要涉及到data2vec audio模型的模型迁移。由于需要进行模型迁移，用户可能遇到了一些问题或者需要帮助。,https://github.com/mindspore-lab/mindnlp/issues/1376
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是MindSpore社区实习任务DINOv2，用户寻求参与该实习任务的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1375
mindnlp,该issue类型为用户提出需求，涉及的主要对象是mgp-str模块。由于缺乏具体内容，用户可能要求实习任务的相关信息或者提出问题寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/1369
mindnlp,"这是一个需求类型的issue， 主要涉及的对象是添加名为""vit_mae""的模型。由于缺乏这个模型，在该issue中用户请求添加这个模型以满足需求。",https://github.com/mindspore-lab/mindnlp/issues/1368
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是Kosmos2。,https://github.com/mindspore-lab/mindnlp/issues/1367
mindnlp,该issue属于用户提出需求类型，主要涉及MindNLP社区实习项目中的Lxmert模块。原因可能是用户希望进行Lxmert模型相关的实习任务，寻求相关帮助或指导。,https://github.com/mindspore-lab/mindnlp/issues/1366
mindnlp,"这个issue类型是用户提出需求，主要对象是""mgp-str""，可能是用户寻求关于该功能的帮助。",https://github.com/mindspore-lab/mindnlp/issues/1364
mindnlp,这是一个需求类型的issue，主要涉及的对象是mindspore教程。由于用户需要更多有关mindspore教程的信息，因此提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1363
mindnlp,该issue为用户提出需求（Feature Request），主要对象是模型Persimmon。,https://github.com/mindspore-lab/mindnlp/issues/1361
mindnlp,这是一个用户提出需求的 issue，主要涉及的对象是 UnivNet 模型。由于社区实习需求或意图不明确，导致用户未填写具体内容。,https://github.com/mindspore-lab/mindnlp/issues/1358
mindnlp,这是一个社区实习相关的任务需求，主要对象是Nougat。由于内容为空，导致了用户需要补充相关信息或继续交流的问题。,https://github.com/mindspore-lab/mindnlp/issues/1357
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是UMTA5。由于未提供具体内容，无法分析导致的原因和具体需求。,https://github.com/mindspore-lab/mindnlp/issues/1354
mindnlp,这个issue属于用户提出需求的类型，涉及的主要对象是添加DonutSwin模型。由于某些原因导致该任务未能通过审核。,https://github.com/mindspore-lab/mindnlp/issues/1347
mindnlp,这是一个用户提交需求的类型的issue，主要涉及的对象是UPerNet，可能由于项目实习而寻求帮助或者合作。,https://github.com/mindspore-lab/mindnlp/issues/1346
mindnlp,这个issue属于用户提出需求类型，主要涉及的对象是添加rag demo。由于用户需要展示rag demo，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1343
mindnlp,这是一个需求提交的issue，主要对象是mluke模型，用户提出了关于修改mluke模型的需求。,https://github.com/mindspore-lab/mindnlp/issues/1342
mindnlp,这个issue是一个用户提出的需求类型，主要对象是safetensor，可能由于safetensor的加载速度较慢导致用户提出了快速加载的需求。,https://github.com/mindspore-lab/mindnlp/issues/1339
mindnlp,这是一个用户提出需求的issue，主要涉及动静态图实现分隔的功能。,https://github.com/mindspore-lab/mindnlp/issues/1338
mindnlp,这是一个用户提出需求的issue，主要对象是Vilt。由于缺少具体内容，无法分析导致的具体症状或问题。,https://github.com/mindspore-lab/mindnlp/issues/1333
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是Vilt模型。,https://github.com/mindspore-lab/mindnlp/issues/1331
mindnlp,这个issue类型是需求提出，涉及大模型任务下的retnet任务，并希望在PR内容中添加相关任务链接。,https://github.com/mindspore-lab/mindnlp/issues/1329
mindnlp,这是一个需求提交类型的issue，主要对象是向mindnlp项目添加M-CTC-T模型。当前issue中的内容为!pass，可能是暂时无需进一步操作或者待处理。,https://github.com/mindspore-lab/mindnlp/issues/1328
mindnlp,这是一个请求特定模型的issue，主要对象是需要使用tapas模型的用户。,https://github.com/mindspore-lab/mindnlp/issues/1327
mindnlp,这个issue类型是需求提出，主要涉及对象是昇腾AI创新大赛中的TAPAS模型。,https://github.com/mindspore-lab/mindnlp/issues/1324
mindnlp,这是一个需求类型的issue，主要涉及mindnlp/trl/models文档下的文件更新操作。由于作者未能通过本地测试，导致未能完成所有文件的更新。,https://github.com/mindspore-lab/mindnlp/issues/1322
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是向mindnlp项目中添加vit_msn模型。,https://github.com/mindspore-lab/mindnlp/issues/1321
mindnlp,"这是一个用户提出需求的issue，主要对象是一个名为""vision_encoder_decoder""的项目。",https://github.com/mindspore-lab/mindnlp/issues/1319
mindnlp,该类型为用户提出需求，主要对象为“qdqbert”模型。由于issue内容为空导致用户提出了需求或问题。,https://github.com/mindspore-lab/mindnlp/issues/1309
mindnlp,"这个issue属于用户提出需求，主要涉及对象是""mvp 注册auto""。由于用户想要在昇腾AI创新大赛中注册mvp时遇到了问题，需要寻求帮助解决。",https://github.com/mindspore-lab/mindnlp/issues/1308
mindnlp,这个issue类型为用户提出需求，主要涉及的对象是增加 Focalnet 算法模型到 mindnlp 中。这是由于用户希望社区实习项目中能够新增加 Focalnet 模型，以丰富模型选择和功能的原因。,https://github.com/mindspore-lab/mindnlp/issues/1306
mindnlp,这是一个用户提出需求类型的issue，主要涉及的对象是owlv2注册auto功能和精度提升。,https://github.com/mindspore-lab/mindnlp/issues/1305
mindnlp,这是一则用户提出需求的issue，主要涉及到增加owlv2模型，原因可能是为了提升精度和注册auto。,https://github.com/mindspore-lab/mindnlp/issues/1304
mindnlp,这是一个用户提出需求的类型的issue，主要涉及要更新Lilt模型，可能是由于目前的模型不满足用户需求或者有新数据需要更新。,https://github.com/mindspore-lab/mindnlp/issues/1302
mindnlp,这个issue属于用户提出需求类型，主要涉及mindnlp的Whisper graph模式功能支持。用户希望能够通过README.MD快速开始使用这个功能。,https://github.com/mindspore-lab/mindnlp/issues/1300
mindnlp,该issue类型为需求提出，主要对象是向MindNLP项目添加Swin2SR模型。,https://github.com/mindspore-lab/mindnlp/issues/1299
mindnlp,"这个issue是用户提出需求或者建议，主要涉及到""昇腾AI创新大赛""的""vision-encoder-decoder""。",https://github.com/mindspore-lab/mindnlp/issues/1298
mindnlp,该issue类型为用户提出需求，主要涉及的对象是需要补充deit的模型代码和测试代码以及vision_text_dual_encoder和deit模型相关的测试代码。由于缺乏相关代码，用户提出了需要补充的需求。,https://github.com/mindspore-lab/mindnlp/issues/1296
mindnlp,"这个issue属于用户提出需求的类型，主要涉及的对象是在github上的mindnlp项目中的""vision_text_dual_encoder""相关代码。原因是用户想要补充deit模型相关的代码和测试代码。",https://github.com/mindspore-lab/mindnlp/issues/1294
mindnlp,这是一个需求类型的Issue，主要涉及对象是splinter模型。,https://github.com/mindspore-lab/mindnlp/issues/1291
mindnlp,这个issue类型是用户提出需求，请求添加一个名为convnextv2的模型。主要对象是mindnlp的项目。,https://github.com/mindspore-lab/mindnlp/issues/1290
mindnlp,"这是一个用户提出需求或咨询问题的类型，主要对象是""【昇腾AI创新大赛】clap""。由于缺乏具体的内容描述，无法确定具体问题或需求。",https://github.com/mindspore-lab/mindnlp/issues/1284
mindnlp,这是一个用户提出需求的类型，主要对象是昇腾AI创新大赛的项目iBert。,https://github.com/mindspore-lab/mindnlp/issues/1282
mindnlp,这是一个用户提出需求的issue，主要涉及要向mindnlp添加模型DPR。用户可能因为希望增加DPR模型的功能而提出这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1279
mindnlp,该issue属于用户提出需求类别，主要涉及的对象是昇腾AI创新大赛owlv2。,https://github.com/mindspore-lab/mindnlp/issues/1278
mindnlp,这个issue类型为用户提出需求，主要涉及的对象是在GitHub上的mindnlp仓库。,https://github.com/mindspore-lab/mindnlp/issues/1276
mindnlp,"这是一个用户提出需求的issue，主要对象是新增一个名为""tokenizer_fast""的功能模块。由于需要加快分词器的处理速度或者增加支持更快速的分词方式，用户提出了这个需求。",https://github.com/mindspore-lab/mindnlp/issues/1275
mindnlp,这是一个用户提出需求的issue，请求在mindnlp中添加一个dpr模型。,https://github.com/mindspore-lab/mindnlp/issues/1274
mindnlp,这是一个用户提出需求的issue，主要对象是在mindnlp中添加模型pegasusx。 由于缺乏该模型，用户希望可以在mindnlp中使用pegasusx模型进行相关任务。,https://github.com/mindspore-lab/mindnlp/issues/1273
mindnlp,这是一个用户提出需求的issue，主要涉及到昇腾AI创新大赛的ibert。,https://github.com/mindspore-lab/mindnlp/issues/1271
mindnlp,这是一个用户提交的需求问题，涉及的主要对象是Detr模型。这个问题可能是用户在使用Detr模型时遇到了一些困难或者希望对该模型进行改进。,https://github.com/mindspore-lab/mindnlp/issues/1269
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是名为Vitdet的开源实习项目。,https://github.com/mindspore-lab/mindnlp/issues/1268
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是昇腾AI创新大赛speecht5。,https://github.com/mindspore-lab/mindnlp/issues/1267
mindnlp,这是一个用户提出需求的类型，主要涉及DETR模型。由于参与昇腾AI创新大赛，用户可能在使用DETR模型时遇到了问题或需要相关支持。,https://github.com/mindspore-lab/mindnlp/issues/1265
mindnlp,这个issue类型属于用户提出需求，主要对象是昇腾AI创新大赛 DETR model，用户寻求关于该模型在比赛中的相关问题的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1264
mindnlp,该issue类型为用户提出需求，主要涉及的对象是文档（docs for transformers），由于任务地址指向错误导致用户无法访问相关页面。,https://github.com/mindspore-lab/mindnlp/issues/1263
mindnlp,这是一个用户提出需求的issue，主要对象是transformers测试文档。由于缺乏详细文档，用户需要进一步了解如何测试transformers功能。,https://github.com/mindspore-lab/mindnlp/issues/1262
mindnlp,这是一个需求类型的issue，主要涉及测试文档transformers，用户可能在使用过程中遇到了一些需求相关的问题。,https://github.com/mindspore-lab/mindnlp/issues/1261
mindnlp,这个issue类型是需求类型，主要涉及的对象是测试文档中的transformers部分。,https://github.com/mindspore-lab/mindnlp/issues/1260
mindnlp,这是一个用户需求类型的issue，涉及到MindNLP下的transformers文档，用户提出了一个地址任务和提交请求的需求。,https://github.com/mindspore-lab/mindnlp/issues/1259
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是名称为Data2VecText的功能模块。,https://github.com/mindspore-lab/mindnlp/issues/1258
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是昇腾AI创新大赛的gptj，具体内容为本地通过。,https://github.com/mindspore-lab/mindnlp/issues/1257
mindnlp,这个issue是用户提出需求类型的问题，涉及的主要对象是MindNLP中的transformers文档，用户想要获取关于transformers的文档内容。,https://github.com/mindspore-lab/mindnlp/issues/1255
mindnlp,"这是一个用户提出需求类型的空content issue，主要涉及的对象是""videomae""。",https://github.com/mindspore-lab/mindnlp/issues/1254
mindnlp,这个issue类型为用户提出需求，主要涉及对象是文档中的transformers部分。由于缺少文档或指导，用户提出了需要补充transformers部分的文档的需求或建议。,https://github.com/mindspore-lab/mindnlp/issues/1252
mindnlp,这个issue是一个用户提出需求的类型，主要对象是文档的更新。由于文档缺失或不清晰，用户希望添加关于transformers测试的相关内容。,https://github.com/mindspore-lab/mindnlp/issues/1247
mindnlp,这是一个用户提出需求的类型的issue，主要涉及文档的缺失。由于文档的内容为空，用户希望得到有关transformers的相关信息。,https://github.com/mindspore-lab/mindnlp/issues/1242
mindnlp,这个issue类型为用户提出需求，涉及主要对象为transformers文档，用户提出问题或寻求帮助，由于缺少transformers文档而导致。,https://github.com/mindspore-lab/mindnlp/issues/1241
mindnlp,"这是一个用户提出需求的类型，主要涉及的对象是""flava""。由于未提供具体内容，无法分析导致症状的原因。",https://github.com/mindspore-lab/mindnlp/issues/1240
mindnlp,这是一个用户提出需求的类型，主要涉及MindNLP中的文档内容，由于缺乏transformers相关文档，用户提出了更新的请求。,https://github.com/mindspore-lab/mindnlp/issues/1239
mindnlp,这是一个用户提出需求类型的issue，主要涉及到mindnlp中的transformers的文档问题。原因可能是用户希望添加或改善transformers的文档内容。,https://github.com/mindspore-lab/mindnlp/issues/1238
mindnlp,这个issue是一个需求报告，主要涉及mindnlp下的transformers文档，用户提出希望添加文档的需求。,https://github.com/mindspore-lab/mindnlp/issues/1237
mindnlp,这个issue类型是用户提出需求，涉及的主要对象是DETR模型。由于昇腾AI创新大赛，用户可能在提出关于DETR模型的问题或寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1236
mindnlp,这个issue类型属于用户提出需求，主要涉及到 transformers 的文档。由于缺乏文档，用户可能需要了解有关 transformers 的相关信息。,https://github.com/mindspore-lab/mindnlp/issues/1235
mindnlp,"这是一个用户提出需求的issue， 主要涉及的对象是昇腾AI创新大赛。用户可能因为想了解""wav2vec2bert""相关信息或者想咨询有关该主题的内容。",https://github.com/mindspore-lab/mindnlp/issues/1234
mindnlp,这是一个关于更新模型以及提交 Pull Request 的类型。该问题涉及到MindNLP 模型库中的 Segformer 模型。原因可能是赛题要求使用的 Segformer 模型与 MindNLP 当前版本中的有所不同，用户试图尝试更新并提交修复。,https://github.com/mindspore-lab/mindnlp/issues/1233
mindnlp,这是一个用户提出需求的issue，主要对象是昇腾AI创新大赛中的wavlm模型。由于缺少具体内容，用户可能是想咨询关于该模型的使用方法或者反馈使用过程中的问题。,https://github.com/mindspore-lab/mindnlp/issues/1230
mindnlp,这是一个用户提出需求的issue，主要对象是向mindnlp这个项目添加GPTJ模型。可能由于用户希望使用GPTJ模型的功能或者增强项目的整体功能性而提出此需求。,https://github.com/mindspore-lab/mindnlp/issues/1229
mindnlp,这个issue类型是改进文档，主要涉及到Transformer库中的文档注释。原因可能是要提高代码可读性或者准确性。,https://github.com/mindspore-lab/mindnlp/issues/1228
mindnlp,这是一个用户提出需求类的issue，主要涉及的对象是昇腾AI创新大赛中的gptj模型。,https://github.com/mindspore-lab/mindnlp/issues/1227
mindnlp,这个issue属于用户提出需求类型，主要涉及昇腾AI创新大赛中LED相关问题。,https://github.com/mindspore-lab/mindnlp/issues/1226
mindnlp,"该问题是一个需求类型的issue，主要涉及的对象是""昇腾AI创新大赛""。",https://github.com/mindspore-lab/mindnlp/issues/1225
mindnlp,"这是一个用户提出需求的issue，主要涉及的对象是""昇腾AI创新大赛""，用户可能提出关于Tapas项目的相关问题或寻求相关帮助。",https://github.com/mindspore-lab/mindnlp/issues/1222
mindnlp,该问题为用户提出需求类型的issue，主要对象是关于昇腾AI创新大赛中的一个项目Tapas。,https://github.com/mindspore-lab/mindnlp/issues/1216
mindnlp,这个issue类型是用户提出需求，针对的主要对象是昇腾AI创新大赛的marian模型。,https://github.com/mindspore-lab/mindnlp/issues/1214
mindnlp,"这是一个用户提出需求的 issue，主要涉及的对象是""昇腾AI创新大赛]marian""项目。由于缺少具体内容，用户可能提出了关于项目在比赛中遇到的问题或寻求相关帮助的请求。",https://github.com/mindspore-lab/mindnlp/issues/1213
mindnlp,这是一个用户提出需求的类型的issue，主要涉及的对象是昇腾AI创新大赛的wav2vec2_conformer模型。由于用户想要参与AI创新大赛，需要相关模型的支持。,https://github.com/mindspore-lab/mindnlp/issues/1212
mindnlp,这是一个用户提出需求的issue，主要对象是昇腾AI创新大赛的git相关问题。,https://github.com/mindspore-lab/mindnlp/issues/1209
mindnlp,这个issue类型为功能需求，主要涉及对象是 GPT_NeoX 样本代码，用户希望基于 LoRa 进行指令微调。,https://github.com/mindspore-lab/mindnlp/issues/1204
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是昇腾AI创新大赛的wav2vec2_conformer模型。,https://github.com/mindspore-lab/mindnlp/issues/1202
mindnlp,"这是一个用户提出需求的类型，主要涉及的对象是""昇腾AI创新大赛""。由于用户希望利用""xlm-roberta-xl""来参与比赛，但具体内容并未给出，可能需要相关支持或反馈。",https://github.com/mindspore-lab/mindnlp/issues/1201
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是昇腾AI创新大赛的wav2vec2_conformer，该需求可能是用户寻求相关帮助或提出bug问题。,https://github.com/mindspore-lab/mindnlp/issues/1200
mindnlp,这是一个用户提出需求的issue，主要对象是添加一个名为PEFT的教程。,https://github.com/mindspore-lab/mindnlp/issues/1199
mindnlp,"这是一个用户提出需求的类型的issue，主要涉及对象为""vision_encoder_decoder""，用户寻求关于参与昇腾AI创新大赛的帮助。",https://github.com/mindspore-lab/mindnlp/issues/1198
mindnlp,这是一个用户提出需求的类型，主要涉及到昇腾AI创新大赛和xlm-roberta-xl。由于用户希望参与昇腾AI创新大赛，可能在使用xlm-roberta-xl时遇到了问题或需要相关支持。,https://github.com/mindspore-lab/mindnlp/issues/1197
mindnlp,这个issue属于用户提出需求类型，主要涉及的对象是在昇腾AI创新大赛中添加犬类模型。,https://github.com/mindspore-lab/mindnlp/issues/1195
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是昇腾AI创新大赛中的videomae项目。由于没有具体内容，用户可能提出了关于该项目的问题或寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1193
mindnlp,这是一则用户提出需求的issue，主要涉及的对象是mindnlp下的switch_transformers模型，其原因可能是用户希望在昇腾AI创新大赛中使用这个模型。,https://github.com/mindspore-lab/mindnlp/issues/1190
mindnlp,这个issue是关于需求的，主要涉及 Speech encoder decoder。用户提出了关于昇腾AI创新大赛的问题。,https://github.com/mindspore-lab/mindnlp/issues/1189
mindnlp,这是一个撤销提交的类型。主要对象是mindnlp下的一个提交（【昇腾AI创新大赛】switch_transformers）导致的问题或需求。,https://github.com/mindspore-lab/mindnlp/issues/1188
mindnlp,这个issue类型是用户提出需求，主要对象是昇腾AI创新大赛的time_series_transformer项目。,https://github.com/mindspore-lab/mindnlp/issues/1187
mindnlp,"这是一个用户提出需求的issue，主要涉及的对象是昇腾AI创新大赛的项目""vision_text_dual_encoder""。",https://github.com/mindspore-lab/mindnlp/issues/1186
mindnlp,这个issue类型为测试反馈、功能性确认，主要涉及对象为switch_transformers模型。可能是由于开发人员进行门禁测试后本地通过，但仍有问题导致用户在issue中进行反馈。,https://github.com/mindspore-lab/mindnlp/issues/1181
mindnlp,这是一条用户提出需求的issue，主要对象是昇腾AI创新大赛的项目videomae。,https://github.com/mindspore-lab/mindnlp/issues/1180
mindnlp,这是一个用户提出需求的issue，主要涉及到MindNLP中的模型cohere。,https://github.com/mindspore-lab/mindnlp/issues/1175
mindnlp,这个issue属于用户提出需求类型，主要涉及Mobilenet_v1模型。由于社区实习的需要，用户可能正在寻求关于Mobilenet_v1的具体问题或帮助。,https://github.com/mindspore-lab/mindnlp/issues/1174
mindnlp,这是一个用户提出需求的类型，主要对象是EfficientFormer模型。,https://github.com/mindspore-lab/mindnlp/issues/1173
mindnlp,"该issue类型为用户提出需求，主要对象是""昇腾AI创新大赛""，用户寻求关于""Cohere""方面的帮助。",https://github.com/mindspore-lab/mindnlp/issues/1172
mindnlp,该issue类型为功能需求，涉及的主要对象是添加昇腾AI控制模型，问题由于需要为昇腾AI创新大赛添加新的控制模型而提出。,https://github.com/mindspore-lab/mindnlp/issues/1171
mindnlp,"这是一个用户提出需求的issue，主要对象是""decision_transformer""模块。",https://github.com/mindspore-lab/mindnlp/issues/1170
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是昇腾AI创新大赛的decision_transformer模块。,https://github.com/mindspore-lab/mindnlp/issues/1169
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是decision_transformer模块。 由于需求参赛者在昇腾AI创新大赛中需要相关支持。,https://github.com/mindspore-lab/mindnlp/issues/1167
mindnlp,"这是一个新功能提议，待实现""。",https://github.com/mindspore-lab/mindnlp/issues/1166
mindnlp,这个issue属于功能需求类型，主要涉及在MindNLP中增加一个用于加载和编码sentence transformer模块的功能。,https://github.com/mindspore-lab/mindnlp/issues/1164
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是GLM4-9b，用户希望提供一个适配GLM4-9b并提供推理代码的请求。,https://github.com/mindspore-lab/mindnlp/issues/1163
mindnlp,"这是一个用户提出需求的issue，主要涉及的对象是""昇腾AI创新大赛""。",https://github.com/mindspore-lab/mindnlp/issues/1162
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp下的时间序列转换器（time_series_transformer）。,https://github.com/mindspore-lab/mindnlp/issues/1160
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是昇腾AI创新大赛中的roc_bert。由于没有提供具体描述或细节，无法确定具体问题或需求的原因。,https://github.com/mindspore-lab/mindnlp/issues/1159
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是昇腾AI创新大赛的poolformer模型。由于在issue内容中未提供具体信息，无法确定具体问题或请求的性质。,https://github.com/mindspore-lab/mindnlp/issues/1158
mindnlp,这个issue属于用户提出需求类型，主要对象是参加昇腾AI创新大赛的swiftformer模型。,https://github.com/mindspore-lab/mindnlp/issues/1157
mindnlp,这个issue属于用户提出需求类型，主要涉及到昇腾AI创新大赛中的与ctrl相关的问题。,https://github.com/mindspore-lab/mindnlp/issues/1156
mindnlp,这是一个用户提交需求的issue，主要对象是DeBERTa-v2模型。,https://github.com/mindspore-lab/mindnlp/issues/1155
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是昇腾AI创新大赛中的nystromformer模型。,https://github.com/mindspore-lab/mindnlp/issues/1153
mindnlp,该issue类型为用户提出需求，主要涉及的对象是昇腾AI创新大赛的Visual bert模型。该问题由于用户希望参与昇腾AI创新大赛，并对Visual bert模型进行相关开发或应用而提出。,https://github.com/mindspore-lab/mindnlp/issues/1151
mindnlp,该问题为用户提出需求。它涉及的主要对象是昇腾AI创新大赛中关于speech_encoder_decoder的内容。,https://github.com/mindspore-lab/mindnlp/issues/1150
mindnlp,这个issue属于用户提出需求的类型，主要涉及到昇腾AI创新大赛中的imagegpt项目。由于缺少具体内容，用户可能是在请教问题、寻求帮助或提出建议。,https://github.com/mindspore-lab/mindnlp/issues/1149
mindnlp,该issue属于需求类型，主要对象为添加transformers的文档说明。原因可能是缺乏有关transformers的文档，需要补充说明来丰富项目文档内容。,https://github.com/mindspore-lab/mindnlp/issues/1148
mindnlp,"这是一个关于需求的issue，涉及对象为""昇腾AI创新大赛""和""chinese_clip模型""。该问题的产生原因是需要调用chinese_clip模型来进行推理测试。",https://github.com/mindspore-lab/mindnlp/issues/1147
mindnlp,"该问题单为需求提出，主要对象是参与“昇腾AI创新大赛”的团队""groupvit""，问题由于需要提交相关比赛作品而提出。",https://github.com/mindspore-lab/mindnlp/issues/1146
mindnlp,"这是一个用户提出需求的issue，主要涉及到mindnlp下的一个模型groupvit。由于用户只提供了简单的命令""/model groupvit""，可能是要求示例代码或查看模型详情的请求。",https://github.com/mindspore-lab/mindnlp/issues/1144
mindnlp,这是一个用户提出需求的类型issue，主要涉及的对象是pynative核心模块。,https://github.com/mindspore-lab/mindnlp/issues/1143
mindnlp,"该问题单为需求提出，主要涉及昇腾AI创新大赛的参赛作品""owlvit""。",https://github.com/mindspore-lab/mindnlp/issues/1142
mindnlp,"该issue类型为用户提出需求，主要对象是""gpt_neox_japanese""模型。由于内容为空，用户可能在寻求关于该模型的帮助或者相关信息。",https://github.com/mindspore-lab/mindnlp/issues/1141
mindnlp,"该issue类型为技术需求，主要对象是""昇腾AI创新大赛""。",https://github.com/mindspore-lab/mindnlp/issues/1138
mindnlp,这个issue是用户提出需求。该问题单涉及的主要对象是MindSpore模型开发挑战赛。原因是用户在寻求有关MindSpore模型开发挑战赛的支持。,https://github.com/mindspore-lab/mindnlp/issues/1137
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是昇思MindSpore模型开发挑战赛。,https://github.com/mindspore-lab/mindnlp/issues/1136
mindnlp,这是一个用户提出需求类型的问题，主要对象是昇腾AI创新大赛。,https://github.com/mindspore-lab/mindnlp/issues/1135
mindnlp,这个issue类型是用户提出需求，针对的主要对象是ln_tuning功能。由于缺少ln_tuning支持和示例，用户提出了希望添加ln_tuning支持并提供一个相应示例的需求。,https://github.com/mindspore-lab/mindnlp/issues/1129
mindnlp,这是一个功能需求类型的issue，主要涉及的对象是核心文件core.py，由于之前版本的core.py存在问题，需要重新提交修复后的core.py文件。,https://github.com/mindspore-lab/mindnlp/issues/1127
mindnlp,这是一个需求提出的issue，主要涉及的对象是ln_tuning功能。由于未提供具体的问题描述或需求细节，导致无法准确了解用户所需的支持或反馈内容。,https://github.com/mindspore-lab/mindnlp/issues/1124
mindnlp,这是一个用户提出的需求类型的issue，主要涉及到支持多任务提示调整。可能由于现有功能限制或用户需求，需要支持在模型训练中进行多任务提示调整。,https://github.com/mindspore-lab/mindnlp/issues/1123
mindnlp,这是一个需求类型的issue，主要涉及的对象是要在mindnlp中添加ViT模型。由于现有ViT模型缺失，用户希望通过添加该模型来提升mindnlp的功能。,https://github.com/mindspore-lab/mindnlp/issues/1122
mindnlp,这是一个用户需求类型的issue，主要涉及的对象是mindnlp/print_trainable_parameters函数。由于用户需要基本支持Poly，并且提到了print_trainable_parameters函数，推测用户可能希望实现对Poly的基本支持功能。,https://github.com/mindspore-lab/mindnlp/issues/1121
mindnlp,这是一个用户提出需求的 issue，主要对象是mindnlp/print_trainable_parameters函数，可能由于功能实现不完整导致用户提出了该需求。,https://github.com/mindspore-lab/mindnlp/issues/1120
mindnlp,这个issue类型是需求提出，涉及的主要对象是代码注释生成功能。由于用户希望自动生成代码注释以提高代码质量和可读性，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1119
mindnlp,这个issue类型属于用户提出需求，请教问题，主要涉及的对象是comment check model。由于用户希望使用评论检查模型，因此提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/1114
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是代码示例库。由于缺乏前缀调整示例，用户希望添加一个示例来补充代码库的功能。,https://github.com/mindspore-lab/mindnlp/issues/1112
mindnlp,"这是一个需求提出类型的issue，主要涉及的对象是""Generate_Comments""功能。由于功能不可用或者效果不理想，用户提出了需要生成评论或者改进评论生成功能的需求。",https://github.com/mindspore-lab/mindnlp/issues/1108
mindnlp,这是一个用户提出需求类型的issue，主要涉及到engine docs，用户寻求相关实习任务的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1105
mindnlp,这个issue类型是功能需求，主要涉及到支持 prefix-tuning、p-tuning 和 multitask-prompt-tuning。由于用户希望增加这些功能以改善模型性能和应用范围。,https://github.com/mindspore-lab/mindnlp/issues/1104
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是dataset module documentation，由于缺少文档，用户需要相关内容的说明。,https://github.com/mindspore-lab/mindnlp/issues/1103
mindnlp,这是一个需求类型的issue，主要对象是为mindnlp添加关于mkdocs的教程markdown文件。原因是可能用户需要更多关于mkdocs的操作指南和数据准备教程。,https://github.com/mindspore-lab/mindnlp/issues/1101
mindnlp,这是一个需求修改类的issue，主要涉及到engine文档的修改，需要更换部分内容。原因可能是为了更新相关内容或者修复问题。,https://github.com/mindspore-lab/mindnlp/issues/1100
mindnlp,这是一个需求类型的issue，主要涉及MindSpore的任务链接替换。由于需要替换带torch nn.Module的任务链接，用户可能遇到此问题并寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/1099
mindnlp,这是一个用户提出需求的issue，主要对象是要在mindnlp中添加seq2seq lora功能。,https://github.com/mindspore-lab/mindnlp/issues/1098
mindnlp,这个issue类型是需求提出，主要对象是Mindnlp项目。由于缺乏数据预处理和使用训练器的教程，用户请求添加相应的教程来指导操作。,https://github.com/mindspore-lab/mindnlp/issues/1095
mindnlp,这是一个需求提出的Issue，主要涉及到支持LoHa，由于用户需要在MindNLP上支持LoHa，可能是因为LoHa是用户感兴趣的工具或框架，希望在MindNLP中实现相关功能。,https://github.com/mindspore-lab/mindnlp/issues/1094
mindnlp,这是一个需求反馈类型的issue，涉及到LoHa的支持；可能由于参数设置不正确导致无法成功训练LoHa模型。,https://github.com/mindspore-lab/mindnlp/issues/1093
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp模型的可训练参数问题。原因可能是用户想了解和比较不同模型的参数情况。,https://github.com/mindspore-lab/mindnlp/issues/1092
mindnlp,这是一个用户提出需求类型的issue，主要涉及适配支持 EMU2多模态模型，可能是由于当前系统不支持该模型导致的需求。,https://github.com/mindspore-lab/mindnlp/issues/1090
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是在mindnlp下添加幻灯片。,https://github.com/mindspore-lab/mindnlp/issues/1089
mindnlp,这是一个用户提出需求的issue，主要涉及到Engine文档的补充完善。由于文档内容不完整，用户提出了需要完善Engine文档的需求。,https://github.com/mindspore-lab/mindnlp/issues/1087
mindnlp,这是一个用户提出需求类别的issue，主要涉及到MindNLP引擎文档的完善。由于文档不完整导致用户需求相关的信息或操作步骤无法获得，需要补充完成文档以解决用户使用中的困扰。,https://github.com/mindspore-lab/mindnlp/issues/1086
mindnlp,这是一个用户提出需求的类型，主要对象是更新文档，可能是为了完善现有的文档内容或者修正错误。,https://github.com/mindspore-lab/mindnlp/issues/1084
mindnlp,这个issue类型为功能需求，主要涉及对象是MindNLP Starcoder vscode demo，用户提出了希望添加相关功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/1079
mindnlp,这是一个用户提出需求的类型的issue，主要涉及peft模块的文档完善。由于缺乏完善的文档，用户提出了需要完善peft模块文档的需求。,https://github.com/mindspore-lab/mindnlp/issues/1078
mindnlp,这是一个需求类型的issue，主要涉及对PEFT集成的建议。,https://github.com/mindspore-lab/mindnlp/issues/1076
mindnlp,该issue类型为功能实现，并涉及到实现cogvlm模型和demo。原因是迁移模型完成后，进行了精度校验并与torch版本进行了对比。,https://github.com/mindspore-lab/mindnlp/issues/1075
mindnlp,这个issue类型是用户提出需求，主要对象是支持prompt_tuning。原因可能是用户希望mindnlp支持prompt_tuning功能。,https://github.com/mindspore-lab/mindnlp/issues/1073
mindnlp,这个issue属于用户提出需求类型，主要涉及使用mkdocs生成文档，可能是由于现有文档不清晰或希望改善文档呈现方式而提出。,https://github.com/mindspore-lab/mindnlp/issues/1070
mindnlp,这是一个用户提出需求的类型，该问题单涉及主要对象是需添加教程快速入门。这个问题由于项目缺乏入门指导，用户希望能有一份快速入门教程来帮助新用户快速上手。,https://github.com/mindspore-lab/mindnlp/issues/1069
mindnlp,这是一个用户提出需求的类型的issue，主要对象是添加音乐生成器到gradio演示。,https://github.com/mindspore-lab/mindnlp/issues/1066
mindnlp,该issue类型为用户需求，主要涉及的对象是支持phi3，用户要求添加对phi3的支持。,https://github.com/mindspore-lab/mindnlp/issues/1064
mindnlp,这个issue属于用户提出需求类别，主要对象是添加gsm8k prompt示例。可能是用户希望在mindnlp中添加gsm8k prompt示例用于文本生成。,https://github.com/mindspore-lab/mindnlp/issues/1063
mindnlp,这是一个用户提出需求的类型，主要涉及添加openELM模型。可能是由于openELM模型尚未集成到项目中，用户希望能添加这个新模型。,https://github.com/mindspore-lab/mindnlp/issues/1061
mindnlp,这是一个功能需求提出的issue，主要对象是Huggingface PEFT中的add_adapter接口。由于MindNLP当前不支持add_adapter接口导致存在功能缺失。,https://github.com/mindspore-lab/mindnlp/issues/1058
mindnlp,该issue类型为用户提出需求，需要加速初始化操作；主要对象是mindnlp项目。由于用户需求在项目初始化阶段操作速度较慢，需要加速处理。,https://github.com/mindspore-lab/mindnlp/issues/1057
mindnlp,这是一个功能需求问题，主要涉及下载功能，用户提出了需求支持断点续传下载。,https://github.com/mindspore-lab/mindnlp/issues/1056
mindnlp,这是一个用户提出需求的issue，主要涉及添加convnext、cvt、resnet和van模型。,https://github.com/mindspore-lab/mindnlp/issues/1055
mindnlp,这是一个用户提出需求类型的issue，主要对象是MindNLP介绍PPT。由于缺少具体内容导致用户无法了解MindNLP的相关信息，请求提供介绍PPT。,https://github.com/mindspore-lab/mindnlp/issues/1054
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是设计新的张量。由于目前的模块不包括设计张量的核心模块，用户希望增加一个核心模块来设计新的张量。,https://github.com/mindspore-lab/mindnlp/issues/1053
mindnlp,这是一个用户提出需求的issue，主要涉及对象是mindnlp库，用户提出了支持类bitsandbytes的量化能力。,https://github.com/mindspore-lab/mindnlp/issues/1052
mindnlp,这个issue类型是需求提出，主要对象是支持huggingface、modelscope、openmind、wisemodel等平台的模型上传。,https://github.com/mindspore-lab/mindnlp/issues/1051
mindnlp,这是一个需求类型的issue，主要涉及到实习任务地址的问题，可能是用户寻求关于实习任务地址的帮助。,https://github.com/mindspore-lab/mindnlp/issues/1050
mindnlp,这是一个需求类型的issue，主要涉及迁移hf的llava_next llava 和vipllava模型。由于用户希望迁移这两个模型，可能是因为需要进行模型更新或者优化。,https://github.com/mindspore-lab/mindnlp/issues/1047
mindnlp,这个issue是关于模型迁移的需求，主要涉及的对象是名为llava-next、llava和vipllava的三个模型。由于原有模型无法满足需求或者有更好的模型可供使用，导致用户提出迁移到新模型的需求。,https://github.com/mindspore-lab/mindnlp/issues/1046
mindnlp,这个issue类型是需求提出，该问题涉及的主要对象是增加modelscope & wisemodel endpoint。,https://github.com/mindspore-lab/mindnlp/issues/1045
mindnlp,这个issue属于需求类型，该问题单涉及的主要对象是mindnlp库中的convbert模型；由于需要在convbert模型中添加数据并行处理，用户提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1044
mindnlp,这个issue类型为功能请求，主要对象是支持HuggingFace镜像。该问题由于缺乏对HuggingFace镜像的支持而导致功能无法实现。,https://github.com/mindspore-lab/mindnlp/issues/1043
mindnlp,这个issue是用户提出需求，涉及主要对象是为MindNLP设计的微调模块Adaption Prompt。这个问题由于缺少model directory导致用户需要为Adaption Prompt微调模块引入可训练的提示（Prompts）时遇到困难。,https://github.com/mindspore-lab/mindnlp/issues/1042
mindnlp,这是一个用户提出需求的issue，主要对象是t5_finetune。由于用户需要进一步细化和确认t5_finetune内容，因此提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/1040
mindnlp,这是一个需求提出类型的issue，主要涉及到olmo支持的问题。原因可能是出于扩展或增强功能的目的，用户提出了对olmo的支持需求。,https://github.com/mindspore-lab/mindnlp/issues/1039
mindnlp,此issue类型为功能需求，主要对象是MPT Model。由于模型可能存在更新或改进的需求，用户提出了更新MPT Model的请求。,https://github.com/mindspore-lab/mindnlp/issues/1038
mindnlp,这是一个需求提出类型的issue，主要涉及MindNLP中添加一个类似torch.amp的新amp模块，使用autocast代替amp level。原因可能是为了提供更方便和灵活的自动混合精度训练方法。,https://github.com/mindspore-lab/mindnlp/issues/1036
mindnlp,这个issue是一个功能需求类型，涉及主要对象为starcoder with llm-vscode。很可能是用户在该项目中缺少示例代码，因此提出添加此功能的请求。,https://github.com/mindspore-lab/mindnlp/issues/1035
mindnlp,这是一个需求类型的issue，主要涉及mindnlp下的Mpt Model。由于用户需要添加Mpt Model，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1034
mindnlp,"这是一个用户提出需求的类型，该问题单涉及的主要对象为""ia3微调模块""，用户寻求开发这个模块的帮助。",https://github.com/mindspore-lab/mindnlp/issues/1033
mindnlp,这个issue类型是功能需求，主要涉及Mindnlp项目中添加对llama3的支持。由于llama3现在还不被支持，用户提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/1032
mindnlp,这是一个用户提出需求的类型，主要涉及到希望提供COT推理的代码样例。询问的原因可能是想要用llama模型基于GSM8K数据集和CommonsenseQA进行推理。,https://github.com/mindspore-lab/mindnlp/issues/1029
mindnlp,这是一个用户需求类型的issue，主要涉及到mindnlp库中关于prompt tuning代码样例的需求。由于当前教程中未提供prompt tuning的代码示例，用户希望能够得到相关示例以便实现该功能。,https://github.com/mindspore-lab/mindnlp/issues/1028
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是Mixtral 8x7B模型。由于尚未进行Mixtral 8x7B模型的适配，导致用户无法在910A上部署该模型，希望能够获得适配模型的代码。,https://github.com/mindspore-lab/mindnlp/issues/1024
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp库不支持stream_generate功能。这可能导致用户无法使用该功能，需要开发人员提供支持。,https://github.com/mindspore-lab/mindnlp/issues/1023
mindnlp,这个issue是用户提出需求，希望Trainer能够支持'map_fn'，并添加bloom finetune示例。,https://github.com/mindspore-lab/mindnlp/issues/1022
mindnlp,这个issue属于用户提出需求类型，主要涉及的对象是Trainer，由于当前的限制导致用户希望Trainer能够支持data_collator操作。,https://github.com/mindspore-lab/mindnlp/issues/1020
mindnlp,这个issue是关于需求变更的，主要涉及到metrics ut（单元测试）的迁移。由于项目需要进行功能迁移或代码重构，导致需要将该部分代码迁移到legacy（遗留代码）中，可能存在一些兼容性或功能实现方面的问题。,https://github.com/mindspore-lab/mindnlp/issues/1017
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是support sam。由于用户需要sam的支持，可能是希望解决一些问题或需要相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/1015
mindnlp,这是一个用户提出需求的issue，主要对象是支持segformer模型。,https://github.com/mindspore-lab/mindnlp/issues/1014
mindnlp,这是一个新增功能请求，要求添加Pegasus模型和单元测试到MindNLP。,https://github.com/mindspore-lab/mindnlp/issues/1012
mindnlp,这是一个用户提出需求的类型，主要涉及到需要将SAM适配到https://github.com/huggingface/transformers的问题。原因可能是用户希望在transformers项目中使用SAM。,https://github.com/mindspore-lab/mindnlp/issues/1009
mindnlp,这是一个用户提出需求的类型。该问题主要涉及MindNLP库中的图模式以及训练脚本的添加。由于现有模型功能的限制或用户需求的改进，导致用户提出了对新增图模式`convbert`以及相关训练脚本的需求。,https://github.com/mindspore-lab/mindnlp/issues/1007
mindnlp,这是一个用户需求类型的issue，主要涉及到项目中的segformer适配问题。原因可能是用户希望对该模型进行适配或相关功能的使用。,https://github.com/mindspore-lab/mindnlp/issues/1005
mindnlp,这是一个用户提出需求的issue，主要对象是添加MPNet模型。 ,https://github.com/mindspore-lab/mindnlp/issues/1001
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是关于添加类似hf-transformers的新Trainer。,https://github.com/mindspore-lab/mindnlp/issues/1000
mindnlp,这是一个需求更新的issue，主要对象是支持列表。用户可能发现当前支持列表不完整或有错误，需要更新以确保准确性和完整性。,https://github.com/mindspore-lab/mindnlp/issues/999
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是针对mindnlp中的convbert模型的微调，用户提出了在SQuAD数据集上进行问答任务时采用PEFT方法的需求。,https://github.com/mindspore-lab/mindnlp/issues/997
mindnlp,这是一个功能增强的问题，主要涉及到增加了hypercomplex Tensor分解功能及其示例的使用。,https://github.com/mindspore-lab/mindnlp/issues/995
mindnlp,这是一个用户提出需求的类型，主要对象是支持 Bridgetower & Bros 模型。,https://github.com/mindspore-lab/mindnlp/issues/994
mindnlp,这个issue类型是功能需求提议，主要涉及的对象是新增名为timesformer的模型及对应的单元测试。,https://github.com/mindspore-lab/mindnlp/issues/992
mindnlp,这是一个需求类型的issue，要求添加`ConvBert`模型并传递。,https://github.com/mindspore-lab/mindnlp/issues/991
mindnlp,这是一个用户提出需求的issue，主要对象是GPT2 inference performance，由于性能需要提升才能与PyTorch竞争。,https://github.com/mindspore-lab/mindnlp/issues/990
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp下embedding模型的调用示例问题，可能是由于调用代码问题导致无法成功运行。,https://github.com/mindspore-lab/mindnlp/issues/989
mindnlp,这是一个需求类型的issue，主要涉及的对象是mindnlp的库，用户提出了添加Xlnet模型的请求。,https://github.com/mindspore-lab/mindnlp/issues/988
mindnlp,这是一个用户提出需求的issue，主要涉及对象是mindnlp中的Blenderbot small模型。其存在的原因可能是用户希望支持Blenderbot small模型的相关功能。,https://github.com/mindspore-lab/mindnlp/issues/985
mindnlp,这是一个用户提出需求的issue，该问题单涉及的主要对象是mindnlp项目，用户希望支持blenderbot。,https://github.com/mindspore-lab/mindnlp/issues/984
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp的text2vec模块。用户希望增加text2vec模块，可能是为了提供更多文本向量化的功能或者提升文本处理的效率。,https://github.com/mindspore-lab/mindnlp/issues/983
mindnlp,"这是一个用户提出需求的类型，涉及主要对象是在mindnlp中添加名为""musicgen_melody""的功能。",https://github.com/mindspore-lab/mindnlp/issues/980
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp库，由于未提供具体内容，无法分析具体问题产生的原因。,https://github.com/mindspore-lab/mindnlp/issues/979
mindnlp,"这是一个需求提出类型的issue，主要对象是在mindnlp中添加名为""jamba""的东西。",https://github.com/mindspore-lab/mindnlp/issues/978
mindnlp,这是一个需求类型的issue，用户提出需要添加table_transformer模型，可能是为了增强系统的模型支持。,https://github.com/mindspore-lab/mindnlp/issues/976
mindnlp,这是一个用户需求类型的issue，用户请求添加RLHF demo。主要对象涉及于mindnlp项目的功能演示。,https://github.com/mindspore-lab/mindnlp/issues/975
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是llama index(sql) demo。可能是用户希望增加一个llama index(sql) demo示例，以展示相关功能或特性。,https://github.com/mindspore-lab/mindnlp/issues/974
mindnlp,这个issue属于功能需求类型，主要涉及对象是ChatPDF demo。由于缺乏ChatPDF demo，用户提出了需要添加的需求。,https://github.com/mindspore-lab/mindnlp/issues/973
mindnlp,这个issue类型是用户提出需求，请教问题等，该问题涉及的主要对象是dbrx。由于缺少具体内容，导致用户提出了关于dbrx支持的问题或需求帮助。,https://github.com/mindspore-lab/mindnlp/issues/971
mindnlp,这是一个需求问题，涉及主要对象为qwen2 moe，由于用户对其提出了支持的要求。,https://github.com/mindspore-lab/mindnlp/issues/970
mindnlp,这个issue属于用户提出需求类型，主要涉及Mindnlp库支持管道并行推理的问题。,https://github.com/mindspore-lab/mindnlp/issues/969
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是支持jamba。由于用户需要新增对jamba的支持，可能是因为jamba是用户使用的一种新的功能或工具，需要在项目中集成。,https://github.com/mindspore-lab/mindnlp/issues/968
mindnlp,这是一个优化建议类型的issue，主要涉及到CPU运行速度较慢的问题。,https://github.com/mindspore-lab/mindnlp/issues/966
mindnlp,这个issue类型是用户提出需求，主要涉及对象是适配 Yi-VL-34B 模型，并由于需要适配该模型到指定地址而产生。,https://github.com/mindspore-lab/mindnlp/issues/964
mindnlp,这是一个用户建议更新FlashAttention核心的issue，涉及的主要对象是MindNLP。由于新版本的FlashAttention核心可能带来更好的性能或功能，用户提出了更新的需求。,https://github.com/mindspore-lab/mindnlp/issues/962
mindnlp,这是一个用户提出需求的issue，主要对象是对mindnlp项目的支持。由于缺乏对internlm的支持，用户提出了相应需求。,https://github.com/mindspore-lab/mindnlp/issues/961
mindnlp,这个issue属于用户提出需求类型，主要对象是向mindnlp仓库请求添加bigbird_pegasus模型。,https://github.com/mindspore-lab/mindnlp/issues/960
mindnlp,这是一个需求类型的issue，主要涉及更新模型并移除特定依赖。最可能由于项目中需要更新模型版本且不再需要某些依赖，所以提出此需求。,https://github.com/mindspore-lab/mindnlp/issues/958
mindnlp,这是一个用户提出需求的类型，需要更新编码模型并添加音乐生成模型。可能是因为现有模型不足以满足用户的需求，所以提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/956
mindnlp,这是一个用户需求报告，主要对象是GPT-SOVITS模型在MindSpore上的适配问题，用户希望提供GPT-SOVITS的适配版本。,https://github.com/mindspore-lab/mindnlp/issues/955
mindnlp,这是一个用户请求支持hf-style flagembedding bge_m3模型的需求，主要对象是MindNLP。,https://github.com/mindspore-lab/mindnlp/issues/954
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是向mindnlp项目添加bert_generation模型。由于项目当前没有该模型，用户希望团队能够考虑添加这个模型以丰富功能。,https://github.com/mindspore-lab/mindnlp/issues/953
mindnlp,这是一个需求提出类型的Issue。主要涉及的对象是添加GPTNeoX模型。由于尚未具体描述问题或提供更多细节，原因导致用户提出了关于添加GPTNeoX模型的需求。,https://github.com/mindspore-lab/mindnlp/issues/952
mindnlp,这是一个用户提出需求的issue，主要对象是在mindnlp项目中添加GPTNeoX模型。,https://github.com/mindspore-lab/mindnlp/issues/951
mindnlp,这是一个用户提出需求的类型的issue，主要涉及的对象是添加beit模型和更新pylint规则。,https://github.com/mindspore-lab/mindnlp/issues/950
mindnlp,这个issue类型是用户提出需求，涉及主要对象是mindnlp下的question answering pipeline，用户寻求帮助或者提出了一个需求。,https://github.com/mindspore-lab/mindnlp/issues/949
mindnlp,这个issue类型是需求更新，涉及的主要对象是bert-crf示例。由于bert-crf示例需要更新，用户提出了更新该示例的需求。,https://github.com/mindspore-lab/mindnlp/issues/948
mindnlp,该issue为用户提出需求，针对主要对象为支持Layoutlm2的大模型任务。由于缺乏对Layoutlm2的支持，用户提出了关于此功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/946
mindnlp,这个issue属于用户提出需求的类型，主要对象是要为mindnlp添加altclip和ast model。,https://github.com/mindspore-lab/mindnlp/issues/944
mindnlp,这是一个用户提出需求的issue，主要涉及Zero_Shot_Classification Pipeline。由于链接指向了gitee而非github，用户可能在寻求与该主题相关的开源实习机会或项目。,https://github.com/mindspore-lab/mindnlp/issues/943
mindnlp,这是一个用户提出需求的issue，主要涉及MindNLP支持对齐模型的功能。,https://github.com/mindspore-lab/mindnlp/issues/942
mindnlp,这是一个用户提出需求的类型，主要对象是在Mindnlp中添加biogpt模型。由于用户希望扩展Mindnlp支持的模型，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/940
mindnlp,这是一个功能需求类型的issue，主要涉及需要添加一个evaluate模块。这个需求可能由于现有模块的不完整或者用户需要更全面的功能而提出。,https://github.com/mindspore-lab/mindnlp/issues/939
mindnlp,这是一个用户提出需求类型的issue，主要涉及对象是mindnlp库的支持，由于缺乏对wav2vec2_with_lm功能的支持，用户提出希望增加相关支持的需求。,https://github.com/mindspore-lab/mindnlp/issues/936
mindnlp,该issue类型为请求功能（feature request），主要涉及对象为添加tokenization_layoutlm功能。由于模糊的issue描述，用户可能正在请求添加新的tokenization_layoutlm功能，但尚未提供具体的内容或信息。,https://github.com/mindspore-lab/mindnlp/issues/935
mindnlp,这个issue属于用户提出需求。主要对象是mindnlp项目，用户提出希望支持自动语音识别流水线。由于目前不支持该功能，用户提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/934
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp的支持融合注意力（fused attention）功能。,https://github.com/mindspore-lab/mindnlp/issues/933
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是新增对Reformer模型的支持。,https://github.com/mindspore-lab/mindnlp/issues/931
mindnlp,这是一个用户提出需求的issue，主要对象是增加一个bce示例，可能是为了更好地展示如何使用该功能。,https://github.com/mindspore-lab/mindnlp/issues/930
mindnlp,这是一个用户提出需求的类型的issue，主要涉及支持DeBERTa模型的问题。由于DeBERTa是一个相对新的模型，用户可能希望MindNLP能够支持该模型以获得更好的性能。,https://github.com/mindspore-lab/mindnlp/issues/928
mindnlp,这个issue类型是需求更新，涉及的主要对象是CI/CD pipeline。可能是由于项目需求变更或优化导致的。,https://github.com/mindspore-lab/mindnlp/issues/927
mindnlp,这是用户提出需求的类型，主要涉及的对象是mindnlp下的question answering功能。由于用户希望开源实习自然语言问答pipeline，因此提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/923
mindnlp,这个issue类型是功能增强，主要涉及的对象是新增的双值Bert模型实现。,https://github.com/mindspore-lab/mindnlp/issues/921
mindnlp,这个issue类型为功能需求反馈，主要涉及对象为mamba软件，用户提出需要支持图模式，并希望相关功能能够得到支持。,https://github.com/mindspore-lab/mindnlp/issues/919
mindnlp,该issue属于用户发出需求类型，主要对象是text generation pipeline，由于issue链接缺失部分内容导致用户寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/918
mindnlp,这是一个用户提出需求的issue，主要对象是text2text generation pipeline，用户希望添加该功能至原有项目中。,https://github.com/mindspore-lab/mindnlp/issues/917
mindnlp,这是一个用户提出需求的issue，主要涉及对象是对于软件支持mamba。可能由于当前软件不兼容mamba导致用户无法使用或者希望更快的包管理。,https://github.com/mindspore-lab/mindnlp/issues/916
mindnlp,这是一个用户提出需求的issue，主要对象是要求适配Bark或者更多的语音合成模型。,https://github.com/mindspore-lab/mindnlp/issues/915
mindnlp,这个issue类型为用户提出需求，主要涉及的对象是支持mixtral，由于没有明确的内容，无法分析导致的问题或需求。,https://github.com/mindspore-lab/mindnlp/issues/912
mindnlp,这是一个用户提出需求的类型，主要对象是郑州智算项目，需把模型迁移到npu上。,https://github.com/mindspore-lab/mindnlp/issues/911
mindnlp,这是一个用户提出需求的issue，涉及对象为如何在指定显存大小情况下同时运行glm3和bgelargezh，原因是想在具有64G显存的环境中同时运行这两个任务。,https://github.com/mindspore-lab/mindnlp/issues/908
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是百川模型。原因可能是加载了baichuan13bchat模型后，却没有chat功能的视线，用户希望添加chat功能以便体验百川模型的chat功能。,https://github.com/mindspore-lab/mindnlp/issues/907
mindnlp,这个issue类型是功能需求提出，主要涉及的对象是支持混合语言（例如中英文）的支持。,https://github.com/mindspore-lab/mindnlp/issues/904
mindnlp,该issue类型为需求提出，主要对象是支持Reformer模型。原因可能是用户希望在项目中集成Reformer模型以改善自然语言处理任务的效果。,https://github.com/mindspore-lab/mindnlp/issues/903
mindnlp,这是一个关于用户提出需求的issue，主要对象是支持unified_transformer模型的实现。原因可能是用户希望在项目中使用这种模型，但目前尚未提供相关支持。,https://github.com/mindspore-lab/mindnlp/issues/902
mindnlp,该issue类型是用户提出需求，主要涉及的对象是对DeBERTa模型的支持。,https://github.com/mindspore-lab/mindnlp/issues/901
mindnlp,这个issue是一个用户提出需求的类型，主要涉及的对象是支持biogpt。这个问题可能是由于Biogpt模型尚未集成到MindNLP中而导致用户希望得到支持。,https://github.com/mindspore-lab/mindnlp/issues/900
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是支持starcoder2。原因可能是用户希望mindnlp能支持starcoder2，但具体内容未提供。,https://github.com/mindspore-lab/mindnlp/issues/898
mindnlp,这个issue类型为用户提出需求，主要涉及的对象是模型迁移至npu，原因是项目需要将模型从CPU迁移到NPU进行使用。,https://github.com/mindspore-lab/mindnlp/issues/897
mindnlp,这是一个用户提出需求的类型的issue，主要涉及的对象是在昇腾上运行fastwhisperlargeV3模型。,https://github.com/mindspore-lab/mindnlp/issues/896
mindnlp,这个issue是用户提出需求类型，讨论新增BGE模型的适配以及提供了适配链接。,https://github.com/mindspore-lab/mindnlp/issues/895
mindnlp,"这个issue类型为需求提出，主要涉及到对""starcoder2""的支持。",https://github.com/mindspore-lab/mindnlp/issues/894
mindnlp,这是一个关于开发对标huggingface的TrainingAuguments类的需求提出。该问题主要涉及的对象是开发者。由于huggingface提供了优秀的TrainingAuguments类，用户想要开发一个类似的功能，以提升模型训练的效率和灵活性。,https://github.com/mindspore-lab/mindnlp/issues/891
mindnlp,这是一个请求更新文档的issue，涉及的主要对象是项目的README文件。,https://github.com/mindspore-lab/mindnlp/issues/889
mindnlp,该issue属于用户提出需求类型，主要涉及对象为模型下载功能。原因可能是用户希望限制只能从Hugging Face下载模型。,https://github.com/mindspore-lab/mindnlp/issues/888
mindnlp,这是一个用户提出需求的类型，主要涉及Mindnlp支持Olmo模型的问题。可能是由于用户想要在Mindnlp中使用Olmo模型，但目前该模型尚未被支持，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/887
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是要在mindnlp中添加qwen2模型。可能由于用户需要使用qwen2模型来进行特定的任务，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/884
mindnlp,这个issue类型是用户提出需求，主要对象是在mindnlp下新增esm模型。原因是用户希望能够使用该模型进行相关的文本处理任务。,https://github.com/mindspore-lab/mindnlp/issues/883
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp是否能够支持qwen、qwen1.5系列的大模型。,https://github.com/mindspore-lab/mindnlp/issues/882
mindnlp,该issue类型是用户提出需求，该问题单涉及的主要对象是更新bark模型。由于bark模型可能存在过时或者需要改进的情况，用户提出了更新bark模型的需求。,https://github.com/mindspore-lab/mindnlp/issues/879
mindnlp,这是一个用户提出需求的issue，涉及到更新clip。可能是由于clip功能不足或者需要改进所致。,https://github.com/mindspore-lab/mindnlp/issues/878
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是在GitHub上的MindNLP仓库中的LayoutLM模型初始化问题。可能原因是用户尝试初始化LayoutLM时遇到了困难，需要帮助或指导。,https://github.com/mindspore-lab/mindnlp/issues/874
mindnlp,这个issue类型是功能需求，主要涉及到CPM模型的更新和添加streamers，可能提出了需要更新模型和添加新功能的请求。,https://github.com/mindspore-lab/mindnlp/issues/873
mindnlp,"这是一个请求支持 ""gemma"" 的问题，类型为用户提出需求，主要对象是 gemma。这个问题可能是因为用户需要关于 gemma 的支持或帮助。",https://github.com/mindspore-lab/mindnlp/issues/871
mindnlp,这是一个需求类型的issue，主要涉及mindnlp的推理速度优化。 ,https://github.com/mindspore-lab/mindnlp/issues/870
mindnlp,这是一个功能需求类型的issue，主要涉及MindNLP项目中的BERT模型在IMDB数据集上微调的示例添加。,https://github.com/mindspore-lab/mindnlp/issues/869
mindnlp,这是一个用户提出需求的类型的issue，主要涉及支持文本分类管道，原因可能是现有功能无法满足用户的需求。,https://github.com/mindspore-lab/mindnlp/issues/867
mindnlp,这是一个需求类型的issue，主要对象是支持minicpm，可能是由于缺乏相关功能导致用户提出需求。,https://github.com/mindspore-lab/mindnlp/issues/866
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是对模型Hubert的添加。,https://github.com/mindspore-lab/mindnlp/issues/865
mindnlp,这是一个用户提出需求的issue，主要涉及MindNLP库中使用jieba替换cjieba的问题。原因可能是jieba更普遍、更稳定、或者包含更多的功能。,https://github.com/mindspore-lab/mindnlp/issues/864
mindnlp,这是一个用户提出需求的issue，主要涉及到添加BERT的自动并行训练，由于需要修改MindSpore的源代码，可能用户希望实现BERT模型在MindSpore中自动并行训练的功能。,https://github.com/mindspore-lab/mindnlp/issues/860
mindnlp,这是一个需求提出类的issue，主要涉及音乐生成模型Pop2piano的添加。原因可能是用户想要在Mindnlp中使用Pop2piano生成钢琴音乐。,https://github.com/mindspore-lab/mindnlp/issues/859
mindnlp,该issue属于功能需求类型，主要涉及的对象是Cell类。用户提出需求支持Cell类中的half()和float()方法。,https://github.com/mindspore-lab/mindnlp/issues/858
mindnlp,该issue类型为用户提出需求，主要对象是mindnlp下的utils工具包。原因是希望为工具包添加python懒加载模块。,https://github.com/mindspore-lab/mindnlp/issues/857
mindnlp,"这个issue属于用户提出需求类型，主要涉及的对象是添加名为""regnet""的模型到mindnlp中。由于缺少regnet模型目录和初始化文件，用户希望将其添加到mindnlp中。",https://github.com/mindspore-lab/mindnlp/issues/853
mindnlp,这是一个请求需求类型的issue，主要涉及的对象是Electra模型初始化。原因可能是开发者之前未实现相关功能，用户提出这个需求来初始化Electra模型。,https://github.com/mindspore-lab/mindnlp/issues/852
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是pangu更新。由于用户想要更新pangu，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/851
mindnlp,这是一个需求更新的issue，主要对象是whisper ut & crf ut。由于需要维护和更新代码库，用户提出了更新的需求。,https://github.com/mindspore-lab/mindnlp/issues/849
mindnlp,"这是一个需求。该问题单涉及的主要对象是更新名为""bloom""的功能。",https://github.com/mindspore-lab/mindnlp/issues/848
mindnlp,这个issue是关于需求更新的，涉及到phi模型。原因可能是需要改进模型功能或性能。,https://github.com/mindspore-lab/mindnlp/issues/847
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是XLMRobertaModel。由于XLMRobertaModel目前不支持AutoModel，用户提出希望增加对AutoModel的支持。,https://github.com/mindspore-lab/mindnlp/issues/845
mindnlp,这是一个用户提出需求的issue，主要涉及对象是为MindNLP添加关于phi2的Streamlit示例。由于缺少此示例，用户请求添加以展示如何在Streamlit中使用phi2。,https://github.com/mindspore-lab/mindnlp/issues/844
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是添加mbart模型。这个问题可能是由于使用mbart模型能够提高多语言处理效果的需求而导致。,https://github.com/mindspore-lab/mindnlp/issues/839
mindnlp,这是一个用户提出需求的 issue，主要对象是更新名为bart的功能。由于需要添加新功能或改进现有功能，用户提出了对bart的更新请求。,https://github.com/mindspore-lab/mindnlp/issues/837
mindnlp,该issue属于用户提出需求，并希望添加mbart模型。,https://github.com/mindspore-lab/mindnlp/issues/835
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是在mindnlp中添加图注意力模型（Graphormer）微调的功能。,https://github.com/mindspore-lab/mindnlp/issues/834
mindnlp,这是一个功能需求提出的issue，主要涉及到添加phi_2模型和加快加载速度；由于速度较慢的模型加载，用户提出需要添加一个新模型并改善加载速度。,https://github.com/mindspore-lab/mindnlp/issues/833
mindnlp,这个issue类型是用户提出需求，主要涉及对象是ERNIE & ERNIE_M模型的支持图模式，用户提出希望支持图模式的功能。,https://github.com/mindspore-lab/mindnlp/issues/832
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp对xlm-robert-base和xlm-roberta-large模型的适配问题，导致无法对bgerank模型进行解析。,https://github.com/mindspore-lab/mindnlp/issues/831
mindnlp,"这是一个用户提出需求的issue，主要对象是要求在mindnlp中添加""add falcon finetune""功能。",https://github.com/mindspore-lab/mindnlp/issues/830
mindnlp,这是一个需求类型的issue，主要涉及对象是mindnlp下的falcon模块，用户提出需添加falcon finetune功能。可能是因为用户希望能够使用finetune功能来对falcon模型进行微调，以满足特定任务需求。,https://github.com/mindspore-lab/mindnlp/issues/829
mindnlp,这个issue属于用户提出需求，主要涉及的对象是Mindnlp。由于Ascend平台上不支持flash attention，用户提出需要在Ascend平台上支持flash attention。,https://github.com/mindspore-lab/mindnlp/issues/827
mindnlp,这个issue是用户提出需求。该问题涉及主要对象是添加 LongStorage 支持。,https://github.com/mindspore-lab/mindnlp/issues/826
mindnlp,这是一个用户提出的需求类型的issue，主要对象是mindnlp的API文档。由于API文档是英文的，用户希望提供中文翻译，以帮助更多后来学习mindnlp的用户。,https://github.com/mindspore-lab/mindnlp/issues/822
mindnlp,这个issue类型为功能需求，主要对象是Tensor.expand函数。由于该函数目前不支持以元组或列表形式作为输入扩展维度，用户提出了增加这一功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/821
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是在mindnlp项目中添加uie推理代码的功能。,https://github.com/mindspore-lab/mindnlp/issues/820
mindnlp,这是一个用户提出需求类型的issue，主要涉及Mindnlp缺少用于Mask LM任务的数据集处理器。造成该需求的原因是用户在预训练模型时需要进行mask等处理，但目前的Mindnlp功能不支持该处理器。,https://github.com/mindspore-lab/mindnlp/issues/817
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是在mindnlp框架下预训练llama模型所需的运行环境。由于无法准确加载或训练llama模型，用户请求提供相关的运行环境，希望得到帮助。,https://github.com/mindspore-lab/mindnlp/issues/816
mindnlp,这个issue是用户提出需求，主要对象是支持uie & uie_m。由于缺少具体描述内容，无法分析具体问题的原因。,https://github.com/mindspore-lab/mindnlp/issues/814
mindnlp,这个issue类型是需求提出，主要涉及的对象是支持UIE和ERNIE模型。,https://github.com/mindspore-lab/mindnlp/issues/810
mindnlp,这个issue是用户提出需求。主要对象是mindnlp下的chatglm2 & chatglm3模块。,https://github.com/mindspore-lab/mindnlp/issues/807
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是新增功能的添加。原因是用户请求添加名为seamless_m4t和seamless_m4t_v2的功能。,https://github.com/mindspore-lab/mindnlp/issues/806
mindnlp,这是一个用户提出需求的 issue， 主要涉及的对象是调整 `gpt_bigcode-santacoder` 模型的性能。,https://github.com/mindspore-lab/mindnlp/issues/805
mindnlp,这是一个用户提出需求的类型，主要对象是增加chatglm图实现和使用ops.dense的功能。原因可能是为了增加模型的功能或提升性能。,https://github.com/mindspore-lab/mindnlp/issues/803
mindnlp,这是一个用户提出需求的issue，主要对象是在mindnlp的README中希望添加代码使用示例。原因可能是用户需要更直观的代码示例来理解如何使用该项目。,https://github.com/mindspore-lab/mindnlp/issues/802
mindnlp,这是一个用户提出需求的issue，主要涉及适配模型facebook/seamless-m4t-v2-large。用户请求添加对该模型的适配支持。,https://github.com/mindspore-lab/mindnlp/issues/800
mindnlp,这是一个功能需求的issue，主要涉及对象是支持pytorch模型的加载和保存。原因可能是现有方法不够方便或者不够灵活。,https://github.com/mindspore-lab/mindnlp/issues/799
mindnlp,这是一个需求类型的issue，主要对象是支持旧版本。由于用户可能在使用过程中遇到兼容性问题或对旧版本的功能有需求，因此提出了支持旧版本的需求。,https://github.com/mindspore-lab/mindnlp/issues/797
mindnlp,这个issue属于用户提出需求类型，涉及的主要对象是支持chatglm2。,https://github.com/mindspore-lab/mindnlp/issues/796
mindnlp,这个issue属于需求提出类型，主要涉及支持混合MOE模型(7b*8)，用户提出了对新模型的需求。,https://github.com/mindspore-lab/mindnlp/issues/795
mindnlp,这是一个需求类型的issue，涉及的主要对象是chatglm模块。,https://github.com/mindspore-lab/mindnlp/issues/794
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp中支持在GPU上进行快速注意力的功能。原因可能是当前版本没有实现此功能，用户希望使用mindspore的aot自定义运算符来解决这个问题。,https://github.com/mindspore-lab/mindnlp/issues/792
mindnlp,这是一个需求类型的issue， 主要涉及的对象是支持Mistral。用户提出了关于支持Mistral的需求。,https://github.com/mindspore-lab/mindnlp/issues/791
mindnlp,"该issue类型为用户提出需求，主要对象是增加一个名为""zero_init""的功能。",https://github.com/mindspore-lab/mindnlp/issues/789
mindnlp,这个issue类型为代码更新需求，主要涉及的对象是longT5模型。可能是由于mindnlp的最新修改，需要对longT5的代码进行相应更新。,https://github.com/mindspore-lab/mindnlp/issues/781
mindnlp,该issue属于用户提出需求类型，主要涉及更新gpt模型和示例。由于使用的模型需要更新或改进，用户希望更新gpt模型及示例。,https://github.com/mindspore-lab/mindnlp/issues/780
mindnlp,这是一个关于Falcon模型迁移的需求提出 issue，主要涉及到Falcon模型和其迁移过程。由于原因未提供，无法确定具体问题症状或用户所需帮助内容。,https://github.com/mindspore-lab/mindnlp/issues/779
mindnlp,这是一个用户提出需求的类型，该问题涉及到Graphormer模型迁移。用户可能遇到了模型迁移的困难或者需要帮助进行相关的迁移操作。,https://github.com/mindspore-lab/mindnlp/issues/778
mindnlp,这个issue为需求更新(chatglm)的类型，主要涉及到Mindnlp的chatglm模块。由于可能有新功能需求或者更新需求导致此issue的创建。,https://github.com/mindspore-lab/mindnlp/issues/776
mindnlp,这个issue类型是需求更新，主要涉及chatglm功能的更新。,https://github.com/mindspore-lab/mindnlp/issues/775
mindnlp,这个issue类型是需求新增，涉及的主要对象是代码库中的UT（unit test）。这个issue由于作者没有填写具体内容，未能说明需要新增什么样的UT。,https://github.com/mindspore-lab/mindnlp/issues/773
mindnlp,"该issue类型为需求提出，主要涉及对象为mindnlp项目中的mt5模型，用户提出将变量名称'gamma, beta, embedding_table'改为'weight, bias'。",https://github.com/mindspore-lab/mindnlp/issues/768
mindnlp,这是一个功能需求类型的issue，主要涉及的对象是MindNLP的BERT模型在图模式下的更新。这个需求是为了支持通过重新计算来节省内存。,https://github.com/mindspore-lab/mindnlp/issues/767
mindnlp,这个issue是一个需求提出类型的问题，主要涉及更新白船支持7b和13b模型。用户需求更新白船，支持更大模型的能力。,https://github.com/mindspore-lab/mindnlp/issues/766
mindnlp,这是一个用户提交需求的issue，主要对象是要更新T5模型。,https://github.com/mindspore-lab/mindnlp/issues/763
mindnlp,该issue类型为需求提交，用户提出了添加pop2piano模型的请求，希望该模型被加入到项目中。,https://github.com/mindspore-lab/mindnlp/issues/761
mindnlp,这是一个需求更新（Feature Request）类的issue，主要涉及到github上的mindnlp项目中GPT示例的更新。可能由于GPT示例的代码或示例文本需要更新或改进，导致用户提出了此需求。,https://github.com/mindspore-lab/mindnlp/issues/758
mindnlp,这个issue属于用户提出需求，主要对象是支持 llama2，可能是由于当前版本不支持 llama2，用户希望功能能够覆盖 llama2 ，所以提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/756
mindnlp,这是一个用户提出需求的issue，该问题单主要涉及的对象是添加百川功能。由于没有提供具体的内容，无法分析出具体的原因或症状。,https://github.com/mindspore-lab/mindnlp/issues/755
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是T5模型的数据并行训练。该需求是因用户希望能够使用T5模型进行单机多卡的数据并行训练，以及未来希望支持多机多卡训练。,https://github.com/mindspore-lab/mindnlp/issues/754
mindnlp,这是一个需求类型的issue，主要涉及对象是bert示例。由于示例代码需要更新，用户提出了更新bert示例的请求。,https://github.com/mindspore-lab/mindnlp/issues/752
mindnlp,这是一个需求类型的 issue， 主要涉及的对象是更新 Ernie 和 GPT tokenizer。原因可能是现有的 tokenizer 不再适用或需要升级到最新版本。,https://github.com/mindspore-lab/mindnlp/issues/751
mindnlp,这个issue属于用户提出需求类型，关注的主要对象是中文embedding模型的适配需求，由于需要在910B上训练和推理指定的中文embedding模型，用户请求进行相关工作。,https://github.com/mindspore-lab/mindnlp/issues/747
mindnlp,这是一个关于需求的 issue，主要涉及的对象是使用自托管的action runner。在这个 issue 中，用户提出了希望使用自托管的action runner的需求。,https://github.com/mindspore-lab/mindnlp/issues/744
mindnlp,这个issue是一个需求提出，主要对象是更新whisper模型。原因可能是需要改进模型的性能或添加新功能至模型。,https://github.com/mindspore-lab/mindnlp/issues/743
mindnlp,这个issue属于用户提出需求类型，主要对象是Dev autoformer模块，向项目中添加time_series_utils.py文件。,https://github.com/mindspore-lab/mindnlp/issues/742
mindnlp,这是一个用户提出需求的issue，涉及主要对象是albert模型。由于albert模型需要进行更新，用户提出了更新请求。,https://github.com/mindspore-lab/mindnlp/issues/741
mindnlp,这是一个功能更新需求，主要涉及的对象是XLM模型。,https://github.com/mindspore-lab/mindnlp/issues/740
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是添加Ascendpatch。,https://github.com/mindspore-lab/mindnlp/issues/739
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp中的tokenization模块。 由于目前缺乏tokenization模块，用户希望添加这一功能以提升文本处理的能力。,https://github.com/mindspore-lab/mindnlp/issues/738
mindnlp,这是一个用户提出需求的issue，主要对象是创建MiniGPT4模型。,https://github.com/mindspore-lab/mindnlp/issues/737
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是minigpt4模块。由于缺少__init__.py文件，用户提交了创建该文件的请求。,https://github.com/mindspore-lab/mindnlp/issues/736
mindnlp,这个issue属于用户提出需求类型，主要涉及的对象是mindnlp项目的README文件。由于README文件缺少支持的模型列表，用户提出希望在README中添加支持的模型的需求。,https://github.com/mindspore-lab/mindnlp/issues/734
mindnlp,这是一个需求提出类型的issue，主要对象是需要为MindNLP添加一个whisper模型。由于目前缺少这个模型，用户希望可以为项目增加这一功能。,https://github.com/mindspore-lab/mindnlp/issues/733
mindnlp,这是一个用户提出需求的issue，主要对象是中文embedding模型，用户希望对m3e-large和bge-large-zh-v1.5进行适配以解决大模型无法处理长记忆的问题。,https://github.com/mindspore-lab/mindnlp/issues/732
mindnlp,该issue为用户提出需求，请求大神适配Whisper large-v3版本。主要对象是Whisper large-v3模型。由于新版本提升巨大，用户希望大神进行适配。,https://github.com/mindspore-lab/mindnlp/issues/731
mindnlp,这个issue是关于更新roberta到hf风格，类型为功能需求提出，主要涉及对象是代码库中的roberta模型。这个问题可能由于当前代码库的roberta模型风格与hf（Hugging Face）风格不匹配，导致需要将其更新为符合hf风格的代码结构。,https://github.com/mindspore-lab/mindnlp/issues/730
mindnlp,这是一个用户提出需求的issue， 主要对象是gpt_bigcode，由于需要增加一个名为`ut`的功能。,https://github.com/mindspore-lab/mindnlp/issues/729
mindnlp,这是一个用户提出需求的issue，主要对象是`gpt_bigcode` ut。由于未提供具体内容，无法确定具体问题或需求。,https://github.com/mindspore-lab/mindnlp/issues/728
mindnlp,这是一个需求更新的类型，主要对象是einsum函数。原因可能是einsum函数功能需要更新或者优化。,https://github.com/mindspore-lab/mindnlp/issues/727
mindnlp,该issue类型为用户提出需求，问题涉及使用自定义einsum。原因可能是用户希望能够使用自定义的einsum函数进行特定的计算。,https://github.com/mindspore-lab/mindnlp/issues/725
mindnlp,这是一个需求类型的issue，主要涉及到porting huggingface函数和ut。原因可能是为了整合huggingface的功能和进行单元测试。,https://github.com/mindspore-lab/mindnlp/issues/724
mindnlp,这是一个用户提出需求的issue，该问题单涉及的主要对象是要为'mindnlp'添加'mbart.py'和'mbart_config.py'。,https://github.com/mindspore-lab/mindnlp/issues/723
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是`gpt_bigcode`模块。由于用户需要为`gpt_bigcode`添加ut，因此提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/722
mindnlp,这是一个用户提出需求的issue，主要涉及物理机上使用MindSpore实现语料向量化的功能。由于目前参照文档介绍运行代码时遇到问题，需要实现与给出代码相同功能的解决方案。,https://github.com/mindspore-lab/mindnlp/issues/721
mindnlp,这个issue是用户提出需求。用户请求添加`gpt_bigcode`的URL信息并修正cell名称，同时支持从预训练载入模型。,https://github.com/mindspore-lab/mindnlp/issues/720
mindnlp,这是一个用户提出需求的issue，主要对象是在mindnlp库中添加xlm_roberta模型。,https://github.com/mindspore-lab/mindnlp/issues/718
mindnlp,这个issue类型是用户提出需求，主要对象是longformer ut插件。由于缺少相关打印信息，用户希望增加打印信息以便更好地调试。,https://github.com/mindspore-lab/mindnlp/issues/717
mindnlp,这是一个用户提出需求的issue，主要涉及持续集成（CI）流水线。由于缺少将ms2.2添加到CI流水线的支持，用户希望对应的功能能够被添加进去。,https://github.com/mindspore-lab/mindnlp/issues/715
mindnlp,这是一个需求更新的issue，主要涉及到修改了modelscope的url和BERT Graph模式。原因可能是需要更新模型的链接和改进BERT Graph模式功能。,https://github.com/mindspore-lab/mindnlp/issues/714
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是更新T5模型的开放链接。 造成这个问题的原因可能是当前的Checkpoint链接已过期或无法正常访问。,https://github.com/mindspore-lab/mindnlp/issues/713
mindnlp,这个issue类型为功能需求，主要涉及对象为gpt_bigcode模型及其相关组件，用户提出了完成gpt_bigcode模型、配置和分词器的需求。,https://github.com/mindspore-lab/mindnlp/issues/712
mindnlp,这是一个用户提出需求的issue，主要涉及ChatYuan模型和分词器的添加。,https://github.com/mindspore-lab/mindnlp/issues/710
mindnlp,这个issue类型是需求更新，主要涉及的对象是CRF模块。该问题可能是由于需要改进CRF模块的性能、功能或者bug修复而提出的。,https://github.com/mindspore-lab/mindnlp/issues/708
mindnlp,这是一个需求类型的issue，用户提出了更新名为bert的功能的请求。,https://github.com/mindspore-lab/mindnlp/issues/707
mindnlp,这个issue类型是功能需求，主要涉及的对象是Graphormer初始化。由于用户希望添加初始化Graphormer的功能，因此提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/706
mindnlp,"这是一个用户提出需求的issue，涉及的主要对象是新增一个名为""graphormer""的功能模块。",https://github.com/mindspore-lab/mindnlp/issues/705
mindnlp,该issue类型为用户需求，涉及的主要对象是在mindnlp项目中集成falcon模块。,https://github.com/mindspore-lab/mindnlp/issues/704
mindnlp,这是一个需求类型的issue，主要涉及创建 __init__.py 文件。原因可能是为了在Python包中正确初始化代码。,https://github.com/mindspore-lab/mindnlp/issues/703
mindnlp,这是一个用户提交需求的issue，涉及的主要对象是mindnlp/transformers/models/autoformer/__init__.py文件。,https://github.com/mindspore-lab/mindnlp/issues/702
mindnlp,这个issue类型为功能/需求提议，主要涉及对象为MindNLP中的Maskformer模型，用户可能提出希望将Maskformer模型进行移动的需求。,https://github.com/mindspore-lab/mindnlp/issues/699
mindnlp,这是一个功能需求的issue，主要涉及要将models重构为transformers以兼容Hugging Face的架构。,https://github.com/mindspore-lab/mindnlp/issues/698
mindnlp,"这个issue类型是用户提出需求，涉及的主要对象是""maskformer""模块。用户可能想要对""maskformer""模块进行初始化或有关初始化的问题。",https://github.com/mindspore-lab/mindnlp/issues/697
mindnlp,这是一个用户提出需求的类型，该问题单涉及主要对象是model implementation of dynamic graphs and static graphs。由于需求功能的区分，用户希望实现动态图和静态图的模型有所区分。,https://github.com/mindspore-lab/mindnlp/issues/695
mindnlp,这个issue类型是新功能请求，主要对象是minigpt4项目。,https://github.com/mindspore-lab/mindnlp/issues/694
mindnlp,这是一个用户提出需求的issue，主要对象是为mindspore添加codellama模型。原因可能是为了扩展mindspore的模型库或优化模型性能。,https://github.com/mindspore-lab/mindnlp/issues/693
mindnlp,这是一个新功能需求的issue，主要涉及Mindnlp中的minigpt4。,https://github.com/mindspore-lab/mindnlp/issues/692
mindnlp,这个issue属于用户提出需求类型，主要对象是支持在 npu 上多机多卡训练的 baichuan 1 和 2。由于用户希望在 npu 上使用多机多卡训练，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/691
mindnlp,这是一个新功能/特性请求的issue，主要涉及MindNLP中的minigpt4初始化。,https://github.com/mindspore-lab/mindnlp/issues/690
mindnlp,这个issue属于用户提出需求，主要对象是minigpt4模型的初始化。原因可能是用户希望获取关于如何初始化minigpt4模型的信息。,https://github.com/mindspore-lab/mindnlp/issues/689
mindnlp,这是一个需求类型的issue，涉及到在mindnlp下创建一个名为__init__.py的空文件。由于可能需要与其他文件进行相关性连接，因此需要此空文件作为初始化的一部分。,https://github.com/mindspore-lab/mindnlp/issues/688
mindnlp,这是一个用户提出需求的issue，主要涉及mT5模型适配的问题，用户希望能够加载mT5架构的模型。原因可能是用户需要在文本生成等任务中使用多国语言版本的mT5模型，但目前系统似乎无法实现该需求。,https://github.com/mindspore-lab/mindnlp/issues/686
mindnlp,这是一个功能增强类的issue，主要涉及的对象是longt5模型。原因可能是为了longt5模型的初始化而创建了__init__.py文件。,https://github.com/mindspore-lab/mindnlp/issues/685
mindnlp,这是一个需求提出类型的issue，该问题单涉及的主要对象是mindnlp/models/gpt_neox模块。由于缺少__init__.py文件，用户需要添加该文件。,https://github.com/mindspore-lab/mindnlp/issues/683
mindnlp,这是一个需求提出的issue，涉及主要对象是要在`mindnlp`中添加xnli数据集，这是由于用户希望扩展文本分类的数据集。,https://github.com/mindspore-lab/mindnlp/issues/678
mindnlp,这是一个需求更新类型的issue，主要涉及到multi30k示例。原因可能是multi30k示例的内容需要更新或改进。,https://github.com/mindspore-lab/mindnlp/issues/677
mindnlp,该issue类型为需求提交，主要对象是在mindnlp项目中添加Longformer模型支持。,https://github.com/mindspore-lab/mindnlp/issues/676
mindnlp,这个issue类型是用户提出需求，主要对象是使用huggingface datasets。由于MindNLP目前没有使用huggingface datasets，用户想请求添加该功能。,https://github.com/mindspore-lab/mindnlp/issues/674
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是加载simcse-chinese-roberta-wwm-ext模型和加载AlbertForMaskedLM架构的模型。由于用户需要使用这两种模型，因此提出了对模型读取的需求。,https://github.com/mindspore-lab/mindnlp/issues/673
mindnlp,这是一个请求添加新数据集的issue，主要对象是MindNLP，用户希望添加hf_dureader_robust数据集。,https://github.com/mindspore-lab/mindnlp/issues/671
mindnlp,"这个issue类型是用户提出需求，主要对象是""funsd dataset""。由于数据集缺失，用户请求添加该数据集。",https://github.com/mindspore-lab/mindnlp/issues/670
mindnlp,这是一个请求功能的issue，主要涉及BertTokenizer库中支持'id_to_token'功能。可能由于用户需要通过'id_to_token'来快速获取单词对应的标识符，从而提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/668
mindnlp,这是一个用户提出需求的issue，该问题单涉及的主要对象是添加hf_squad数据集。,https://github.com/mindspore-lab/mindnlp/issues/667
mindnlp,"这是一个用户提出需求的issue，主要涉及的对象是添加名为""LlamaTokenizer""的tokenizer。可能是用户需要新的tokenizer来处理特定类型的文本数据。",https://github.com/mindspore-lab/mindnlp/issues/666
mindnlp,这是一个关于优化性能的 issue，主要对象是 Llama 模块。,https://github.com/mindspore-lab/mindnlp/issues/665
mindnlp,这是一个功能需求类型的issue，该问题单涉及的主要对象是MindNLP中生成任务的greedy search和T5模型。由于缺乏greedy search算法和需要更新T5模型，用户提出了需求来添加greedy search功能和更新T5模型。,https://github.com/mindspore-lab/mindnlp/issues/664
mindnlp,"这是一个用户提出需求类型的issue，主要对象是在MindNLP中添加名为""llama2""的功能。",https://github.com/mindspore-lab/mindnlp/issues/663
mindnlp,这个issue类型是项目更新需求，主要涉及更新项目的README和支持的任务。,https://github.com/mindspore-lab/mindnlp/issues/660
mindnlp,这个issue类型为用户提出需求，涉及的主要对象是在Mindnlp中增加PEFT:LORA的支持。原因是需要对提交进行重新基于。,https://github.com/mindspore-lab/mindnlp/issues/657
mindnlp,该issue类型是重构请求，主要涉及语料库的示例目录。由于目前示例目录混乱或结构不清晰，用户请求重构例子目录。,https://github.com/mindspore-lab/mindnlp/issues/656
mindnlp,这是一个功能需求类型的issue，主要涉及PEFT模块的支持和修改。原因可能是使用PEFT模块时出现了错误，需要支持CellDict并进行修复，并提出了关于加载和保存PEFT模型的需求。,https://github.com/mindspore-lab/mindnlp/issues/655
mindnlp,这是一个用户请求需求的类型，用户要求添加Llama示例。这可能是因为用户想要在Mindnlp中添加一个新的示例以展示Llama的功能。,https://github.com/mindspore-lab/mindnlp/issues/654
mindnlp,"这个issue类型是用户提出需求，该问题单涉及的主要对象是mindnlp库中的函数。由于缺少具体描述，导致无法明确用户需要添加什么样的功能到""llama2 repeat kv function""中。",https://github.com/mindspore-lab/mindnlp/issues/653
mindnlp,这是一个用户提出需求的类型，该问题单主要涉及添加xfund功能。这个需求是为了提供xfund功能的支持或者相关的帮助。,https://github.com/mindspore-lab/mindnlp/issues/652
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是在mindnlp中添加amp测试。,https://github.com/mindspore-lab/mindnlp/issues/649
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp项目。,https://github.com/mindspore-lab/mindnlp/issues/645
mindnlp,这个issue是关于用户需求的，主要对象是在Ascend上使用amp时遇到的问题，由于Ascend仅支持fp16，用户可能遇到了相关设置或兼容性方面的困难。,https://github.com/mindspore-lab/mindnlp/issues/644
mindnlp,这是一个功能需求类型的issue，主要涉及Mindnlp库中的新增功能。,https://github.com/mindspore-lab/mindnlp/issues/641
mindnlp,这是一个用户提出需求的issue，主要涉及MindNLP中的GPT2预训练模型和LogitsWarper。原因可能是用户希望能够导入GPT2PreTrainedModel并实现LogitsWarper与TopPLogit类的功能。,https://github.com/mindspore-lab/mindnlp/issues/640
mindnlp,这是一个用户提出需求的issue，主要涉及对象是 LukeTokenizer。由于缺少具体内容，无法分析是什么原因导致了问题。,https://github.com/mindspore-lab/mindnlp/issues/637
mindnlp,这是一个需求提出的issue，主要对象是Basic PEFT support，其原因可能是需要添加数据集支持和基本PEFT功能。,https://github.com/mindspore-lab/mindnlp/issues/633
mindnlp,这是一个用户提出需求的issue，主要涉及到支持text2vec。通过用户提供的链接，可以推断用户希望mindnlp项目能够支持text2vec功能。,https://github.com/mindspore-lab/mindnlp/issues/632
mindnlp,这个issue类型属于用户提出需求，请教问题类型，主要涉及对象为mindnlp与mindformers，用户想了解mindnlp是否适用于训练大模型的基础框架以及其背后是否有研发团队支撑。,https://github.com/mindspore-lab/mindnlp/issues/630
mindnlp,这是一个需求提出的issue，主要涉及的对象是在mindnlp中添加opt模型、opt分词器和opt模型测试，用户希望对opt350m进行测试。,https://github.com/mindspore-lab/mindnlp/issues/629
mindnlp,这个issue属于用户提出需求类型，主要对象是trainer模块，由于缺乏保存模型的功能，用户提出希望支持模型保存的需求。,https://github.com/mindspore-lab/mindnlp/issues/628
mindnlp,这是一个用户提出需求的issue，主要涉及对象是MegatronBertTokenizer的完善。此issue由于当前MegatronBertTokenizer并未完全实现功能而产生。,https://github.com/mindspore-lab/mindnlp/issues/623
mindnlp,这是一个用户提出需求的issue，主要涉及到chatglm模块使用图形和动态形状的问题。由于缺少描述具体问题的内容，无法确定具体问题所在。,https://github.com/mindspore-lab/mindnlp/issues/615
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是T5模型生成功能。由于链接未完整导致用户需求无法完整展示。,https://github.com/mindspore-lab/mindnlp/issues/614
mindnlp,这是一个用户提出需求的issue，主要对象是从mindnlp中下载tokenizer.json文件，可能出现在代码中导入NezhaConfig时的问题。,https://github.com/mindspore-lab/mindnlp/issues/613
mindnlp,这个issue类型是用户提出需求，主要对象是xlm tokenizer，由于用户想要在mindnlp中添加对xlm tokenizer的支持。,https://github.com/mindspore-lab/mindnlp/issues/612
mindnlp,这个issue是一个需求类型，涉及的主要对象是在安装 triton 时添加条件判断。由于缺乏条件判断，可能导致安装过程出现问题。,https://github.com/mindspore-lab/mindnlp/issues/611
mindnlp,该issue属于用户提交需求类型，主要对象是在mindnlp项目中正在进行的迁移操作。由于未提供具体的描述，无法分析导致的具体症状。,https://github.com/mindspore-lab/mindnlp/issues/609
mindnlp,这个issue是一个用户提出需求的类型，主要涉及的对象是mindnlp库。由于用户需要支持llama2，所以提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/608
mindnlp,这是一个用户提出需求或寻求帮助的issue，主要涉及HF_Squad。由于某些原因导致出现了bug或用户对HF_Squad功能有疑问或需求帮助。,https://github.com/mindspore-lab/mindnlp/issues/607
mindnlp,这是一个需求类型的issue，主要涉及修改DuConv模块。可能由于功能改进或错误修复的需求而产生。,https://github.com/mindspore-lab/mindnlp/issues/605
mindnlp,这个issue类型为功能需求，主要对象是在mindnlp项目下的hf_duconv模块。由于需要对test_hfduconv和init进行修改，可能是为了添加新的功能或改进现有功能。,https://github.com/mindspore-lab/mindnlp/issues/602
mindnlp,这是一个需求提出类型的issue，主要对象是向mindnlp添加hf_duconv模块，用户提出此需求可能是希望扩展MindNLP的功能或者改进现有功能。,https://github.com/mindspore-lab/mindnlp/issues/601
mindnlp,这个issue类型是需求添加，主要对象是为mindnlp添加'peft'目录并下载文件到当前工作目录，可能是用户希望将文件下载到指定目录以便更方便地使用。,https://github.com/mindspore-lab/mindnlp/issues/600
mindnlp,这是一个增加功能的issue，主要对象是添加hf_cmrc2018。,https://github.com/mindspore-lab/mindnlp/issues/599
mindnlp,这个issue类型是用户提出需求，主要对象是使用bucket功能。由于需要使用bucket功能但没有相关的说明或功能实现，用户提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/598
mindnlp,这是一个需求：提升chatglm推理速度的issue，主要涉及chatglm推理部分。原因可能是现有的推理速度较慢，需要针对性的优化。,https://github.com/mindspore-lab/mindnlp/issues/596
mindnlp,这是一个需求更新的issue，主要涉及更新API文档，问题可能是由于之前文档内容多余或过时而产生。,https://github.com/mindspore-lab/mindnlp/issues/593
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是预训练语言模型中的token embeddings调整问题，由于resize_token_embeddings方法暂未实现，用户提出了如何修改模型的问题。,https://github.com/mindspore-lab/mindnlp/issues/590
mindnlp,这是一个用户提出需求的问题，该问题单涉及的主要对象是模型从预训练加载的处理，用户询问默认保存路径。,https://github.com/mindspore-lab/mindnlp/issues/587
mindnlp,这个issue属于需求提出类型，主要涉及的对象是添加cmrc2018和进行测试。,https://github.com/mindspore-lab/mindnlp/issues/586
mindnlp,这个issue属于需求类型，主要对象是项目mindnlp的添加duconv和测试功能。,https://github.com/mindspore-lab/mindnlp/issues/585
mindnlp,这是一个用户提出需求的issue，主要涉及对象是nezha模型的fine-tuning，可能由于指导不清或者功能不完善导致用户需要相关帮助或解决问题。,https://github.com/mindspore-lab/mindnlp/issues/584
mindnlp,这是一个功能需求类型的issue，主要涉及添加对CMRC2018数据集的支持。这个需求可能由于该数据集在项目中需要使用，但目前尚未提供对应的处理代码而产生。,https://github.com/mindspore-lab/mindnlp/issues/583
mindnlp,这个issue类型为功能需求，主要涉及的对象是添加信息提取（UIEWork）到工作流程中并修复ernie_tokenizer。原因是需要将信息提取功能整合到工作流程中并修复ernie_tokenizer的问题。,https://github.com/mindspore-lab/mindnlp/issues/581
mindnlp,这个issue类型是用户提出需求，用户请求重新提交megatron-bert。,https://github.com/mindspore-lab/mindnlp/issues/579
mindnlp,这是一个升级请求类型的issue，主要涉及的对象是CRF模块，用户提出升级以支持mindspore 1.10版本，可能是由于该模块在mindspore 1.10中存在不兼容或其他问题而导致。,https://github.com/mindspore-lab/mindnlp/issues/578
mindnlp,这是一个更新README的issue，类型属于用户提出需求，针对的主要对象是项目文档。,https://github.com/mindspore-lab/mindnlp/issues/576
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是软件chatglm，用户想知道该软件是否支持在x86 cpu+910 gpu的9010服务器上运行。,https://github.com/mindspore-lab/mindnlp/issues/575
mindnlp,这是一个用户提出需求的issue，主要对象是要求添加名为duconv.py的文件。,https://github.com/mindspore-lab/mindnlp/issues/574
mindnlp,该issue是用户提出需求，希望添加duconv.py文件并修改__init__文件，主要涉及MindNLP项目中的代码结构。,https://github.com/mindspore-lab/mindnlp/issues/573
mindnlp,这是一个用户需求类型的issue，主要涉及的对象是添加MobileBERT tokenizer。由于用户希望在MINDNLP中使用MobileBERT tokenizer功能，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/572
mindnlp,这个issue是用户提出需求，请求添加Duconv数据集。,https://github.com/mindspore-lab/mindnlp/issues/570
mindnlp,这个issue属于用户提出需求类型，主要涉及到DuConv数据集加载器。用户提出修改根路径以适应个人位置。,https://github.com/mindspore-lab/mindnlp/issues/569
mindnlp,该issue类型为用户提出需求，针对的主要对象为CPM_generate功能。由于更新导致CPM_generate功能出现问题，用户提交了该issue请求支持更新。,https://github.com/mindspore-lab/mindnlp/issues/567
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是给mindnlp项目添加BART tokenizer。由于当前尚未支持BART tokenizer，用户希望团队将其添加到项目中。,https://github.com/mindspore-lab/mindnlp/issues/566
mindnlp,这个issue类型是需求提出，主要对象是为mindnlp添加hf_xnli.py和test_hfxnli.py文件。,https://github.com/mindspore-lab/mindnlp/issues/563
mindnlp,这是一个需求提出类型的issue，主要涉及MindNLP中的hf_xnli.py和test_hfxnli.py，由于未提供具体内容，导致无法准确判断问题类型。,https://github.com/mindspore-lab/mindnlp/issues/562
mindnlp,这是一个用户提出需求的issue，主要涉及对象是添加下载链接。原因可能是用户想要方便地下载相关内容。,https://github.com/mindspore-lab/mindnlp/issues/560
mindnlp,这是一个功能请求类型的issue，主要对象是在mindnlp项目中添加hf_squad2.py和test_hfsquad2.py文件。,https://github.com/mindspore-lab/mindnlp/issues/558
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象为mindnlp库中的nezha模型。由于用户想要使用预训练的nezha模型，所以提出了这个问题来寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/556
mindnlp,这个issue是一个用户提出需求的类型，主要涉及的对象是mindnlp库中的bloom功能和finetune方法，用户希望增加相关调用方法和示例。,https://github.com/mindspore-lab/mindnlp/issues/554
mindnlp,这是一个用户提出需求的issue，主要对象是更新`generation_mixin`和`logits_process`。,https://github.com/mindspore-lab/mindnlp/issues/551
mindnlp,这是一个用户提出需求的issue，主要对象是Tinybert tokenizer。由于用户希望在mindnlp中引入Tinybert tokenizer来完成特定任务，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/550
mindnlp,这是一个需求类型的issue，主要涉及的对象是新增一个名为hf_squad2.py的文件和对应的测试文件。,https://github.com/mindspore-lab/mindnlp/issues/548
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是将torch模型转换为mindspore模型。原因可能是用户希望能够在mindnlp中使用mindspore模型替代torch模型。,https://github.com/mindspore-lab/mindnlp/issues/547
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是支持ChatGLM。由于用户希望在MindNLP中实现ChatGLM功能，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/546
mindnlp,这是一个需求类型的issue，主要涉及对象是mindnlp项目中的hf_ptb_text_only.py和test_ptb_test_only.py文件。,https://github.com/mindspore-lab/mindnlp/issues/541
mindnlp,"这个issue类型是用户提出需求，主要对象是""xlm code""。由于缺乏具体内容，造成用户提出了关于""xlm code""的问题或者寻求帮助。",https://github.com/mindspore-lab/mindnlp/issues/540
mindnlp,这是一个用户提出需求的issue，主要对象是上传生成文件。原因可能是用户需要上传生成的文件或者需要相关功能改进。,https://github.com/mindspore-lab/mindnlp/issues/538
mindnlp,这是一个用户提出需求类型的issue，主要对象是为mindnlp添加cpm bee支持。,https://github.com/mindspore-lab/mindnlp/issues/537
mindnlp,这是一个需求变更的issue，主要涉及到重构parallel模块。由于代码结构或性能需要优化，导致需要对parallel模块进行重构。,https://github.com/mindspore-lab/mindnlp/issues/536
mindnlp,这是一个用户提出需求的issue，请求添加cpm & cpm_ant功能。,https://github.com/mindspore-lab/mindnlp/issues/534
mindnlp,这个issue类型是请求更新代码，主要涉及的对象是项目中的XLM代码。这可能是因为需要更新XLM代码以修复bug、增加功能或者提高性能。,https://github.com/mindspore-lab/mindnlp/issues/533
mindnlp,这个issue是一个需求类型，主要涉及的对象是xlm tokenizer。用户提出这个需求可能是希望添加一种支持多语言的分词器。,https://github.com/mindspore-lab/mindnlp/issues/531
mindnlp,这是一个用户提出的需求。该问题涉及的主要对象为mindnlp库，并要求添加tensor parallel功能。,https://github.com/mindspore-lab/mindnlp/issues/530
mindnlp,该issue为需求类型，主要涉及的对象为pylint.conf配置文件。原因可能是需要更新pylint.conf文件以满足特定需求。,https://github.com/mindspore-lab/mindnlp/issues/529
mindnlp,这是一个缺少具体内容的需求提出类型的issue，主要涉及的对象是添加pylint配置，由于缺少具体描述，无法确定具体问题或帮助需求。,https://github.com/mindspore-lab/mindnlp/issues/528
mindnlp,"该issue类型为用户提出需求，请教问题， 主要涉及对象为添加名为""hf_msra_ner.py""的文件到mindnlp库中，由于需要增加新的NER模型的支持，用户提出了这个需求。",https://github.com/mindspore-lab/mindnlp/issues/527
mindnlp,"这是一则用户提出需求的issue，主要涉及的对象是mindnlp库。由于缺少具体内容，用户可能正在请求添加一个名为""add_generation_mixin""的功能模块到该库中。",https://github.com/mindspore-lab/mindnlp/issues/526
mindnlp,"这是一个用户提出需求的类型，该问题单涉及的主要对象是MindNLP项目中的生成模型。由于缺少""add_generation_mixin""功能，用户希望能够添加该功能来增强生成模型的功能性。",https://github.com/mindspore-lab/mindnlp/issues/525
mindnlp,这是一个用户提出需求的issue，主要对象是LLM的stream_chat支持。可能是由于缺少相应功能或接口导致用户请求添加stream_chat支持。,https://github.com/mindspore-lab/mindnlp/issues/524
mindnlp,这是一个用户提出需求的issue，主要涉及在GPU上添加wkv自定义运算符并支持RWKV预训练模型。,https://github.com/mindspore-lab/mindnlp/issues/521
mindnlp,这个issue属于用户提出需求类型，主要对象是添加中文文档问答数据集(docvqa_zh)，用户希望能够获取该数据集的下载链接。,https://github.com/mindspore-lab/mindnlp/issues/518
mindnlp,这是一个需求类型的issue，主要对象是模型的单元测试（models ut），问题是如何降低成本。,https://github.com/mindspore-lab/mindnlp/issues/517
mindnlp,这是一个需求添加类型的issue，主要涉及的对象是添加ERNIE tokenizer（UIE部分）。这个需求可能由于现有的文本处理功能不足而产生。,https://github.com/mindspore-lab/mindnlp/issues/516
mindnlp,这是一个用户提出需求的类型，主要对象是要添加BART模型到mindnlp中。可能是因为目前mindnlp没有包含BART模型的原因，用户提出该需求以扩展模型库。,https://github.com/mindspore-lab/mindnlp/issues/515
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是代码中的Bert LSTM CRF模块，用户希望对其进行简化。 ,https://github.com/mindspore-lab/mindnlp/issues/514
mindnlp,这个issue是用户提出需求，请求添加LoRA模块。主要对象是想要在MindNLP中使用LoRA模块的用户。,https://github.com/mindspore-lab/mindnlp/issues/512
mindnlp,这个issue类型是需求提交，涉及的主要对象是ERNIE模型。由于开发者需要提交ERNIE模型的代码或相关文档，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/510
mindnlp,这个issue类型是用户提出需求，主要对象是支持对象网络与损失函数，用户寻求帮助实现这一功能。,https://github.com/mindspore-lab/mindnlp/issues/509
mindnlp,这个issue是一个功能更新建议，主要涉及到GPT2摘要示例的更新。这个问题可能由于现有示例过时或需要改进而导致用户提出需更新的请求。,https://github.com/mindspore-lab/mindnlp/issues/507
mindnlp,这个issue类型为需求添加，主要对象是BART模型的版权信息，导致用户提出此问题可能是为了明确模型的版权信息。,https://github.com/mindspore-lab/mindnlp/issues/506
mindnlp,这个issue属于需求类型，主要对象是需要在mindnlp中添加BART模型。原因可能是用户希望使用BART模型进行某些特定的任务或者实验。,https://github.com/mindspore-lab/mindnlp/issues/504
mindnlp,"这是一个用户需求类型的issue，主要对象是添加""nezha tokenizer""功能。",https://github.com/mindspore-lab/mindnlp/issues/503
mindnlp,这是一个用户提出的需求类型的issue，主要涉及的对象是添加Longformer tokenizer。,https://github.com/mindspore-lab/mindnlp/issues/502
mindnlp,这是一个用户提出需求的issue，主要涉及GPT1在MS1.10上的支持。由于GPT1当前无法与MS1.10兼容，用户提出希望新增该支持的需求。,https://github.com/mindspore-lab/mindnlp/issues/500
mindnlp,这是一个用户提出需求类型的issue，主要对象是Mindnlp项目下的Longformer Tokenizer。由于当前库中尚未添加Longformer Tokenizer，用户希望对其进行添加。,https://github.com/mindspore-lab/mindnlp/issues/499
mindnlp,这是一个用户提出需求的issue，主要需求是让GPT-2模型支持Microsoft 1.10版本的问题。,https://github.com/mindspore-lab/mindnlp/issues/498
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是添加Glue数据集。由于缺乏Glue数据集，用户希望将其添加到项目中以丰富数据集的内容。,https://github.com/mindspore-lab/mindnlp/issues/496
mindnlp,这是一个需求类型的issue，主要涉及的对象是增加Clip模型到项目中。,https://github.com/mindspore-lab/mindnlp/issues/493
mindnlp,这是一个用户提出需求的issue，该问题涉及mindnlp的支持GPT2的图模式功能。产生这个问题可能是因为用户希望在应用中使用GPT2的图模式，但目前该功能尚未支持。,https://github.com/mindspore-lab/mindnlp/issues/492
mindnlp,这个issue是用户提出需求，并涉及到更新gpt2_summarization。用户想要对项目中的gpt2_summarization进行更新，可能由于模型效果不佳或者需要增加新功能而提出此需求。,https://github.com/mindspore-lab/mindnlp/issues/491
mindnlp,这是一个需求类型的issue，主要涉及的对象是更新tokenizers和添加accumulator功能。可能由于之前的tokenizers存在一些问题或者缺少累加器功能，导致用户提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/490
mindnlp,这个issue类型是需求提出，主要对象是添加一个RoBERTa tokenizer。 原因是当前代码库缺少RoBERTa tokenizer的支持，用户希望能够扩展该功能。,https://github.com/mindspore-lab/mindnlp/issues/488
mindnlp,这是一个用户提出需求的类型，主要对象是albert model的更新和添加保存函数。原因可能是希望更新模型并增加保存功能以提高其性能和灵活性。,https://github.com/mindspore-lab/mindnlp/issues/487
mindnlp,这是一个功能需求的issue，主要涉及的对象是GenerationConfig和ut。问题可能是用户需要添加GenerationConfig并制定测试。,https://github.com/mindspore-lab/mindnlp/issues/484
mindnlp,这个issue属于用户提出需求类型，涉及对象是代码生成器。原因可能是用户希望添加一个名为codegentokenizer的功能。,https://github.com/mindspore-lab/mindnlp/issues/483
mindnlp,"该issue类型为功能需求，涉及的主要对象是代码生成的tokenizer。因为用户希望添加一个名为""add_codegentokenizer""的功能，所以提出了这个issue。",https://github.com/mindspore-lab/mindnlp/issues/482
mindnlp,这是一个用户提出需求的issue，主要涉及情感分析示例的更新。可能是由于现有示例不够具体或者不符合需求，用户希望更新这些示例。,https://github.com/mindspore-lab/mindnlp/issues/481
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是对将torch转换为mindspore的功能需求。,https://github.com/mindspore-lab/mindnlp/issues/478
mindnlp,这是一个需求报告， 主要对象是 T5 模型中添加 from_pretrained 方法的功能。可能由于用户希望能够直接加载预训练的 T5 模型而提出该需求。,https://github.com/mindspore-lab/mindnlp/issues/477
mindnlp,这是一个需求提出类型的issue，主要涉及GPT支持ms1.8。原因可能是用户希望Mindnlp的GPT模型能够支持Microsoft 1.8版本。,https://github.com/mindspore-lab/mindnlp/issues/474
mindnlp,这是一个用户提出需求的issue，主要对象是支持resize embedding，由于当前系统不支持该功能，用户提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/473
mindnlp,这个issue类型是用户提出需求，要求在MindNLP中增加T5模型的torch转MindSpore功能。这涉及的主要对象是T5模型的转换操作。由于用户希望在MindNLP中实现T5模型在torch和MindSpore之间的转换，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/472
mindnlp,这是一个用户提出需求的issue，主要涉及tokenizer支持添加特殊标记，可能是为了提供更灵活的特殊标记功能。,https://github.com/mindspore-lab/mindnlp/issues/471
mindnlp,"这个issue类型是用户提出需求，主要涉及的对象是""Cnn-dailymail""数据集。由于数据集下载链接失效，导致用户无法下载该数据集。",https://github.com/mindspore-lab/mindnlp/issues/470
mindnlp,这个issue类型是更新请求，主要对象是T5 tokenizer和其UT。由于T5 tokenizer及其UT需要更新，导致用户提出了更新请求。,https://github.com/mindspore-lab/mindnlp/issues/469
mindnlp,这是一个用户提出需求的 issue，主要对象是为 mindnlp 增加一个 torch 到 mindspore 的功能转换。,https://github.com/mindspore-lab/mindnlp/issues/468
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是在mindnlp项目中需要添加Clip模型。原因是要将从PyTorch转换到MindSpore/MindNLP的Clip模型添加到项目中。,https://github.com/mindspore-lab/mindnlp/issues/467
mindnlp,这是一个用户提出需求的issue，主要对象是让mindnlp添加GPT2Tokenizer功能。可能是用户需要在mindnlp中使用GPT2Tokenizer功能，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/466
mindnlp,这个issue类型为需求提出，涉及主要对象为GPT模型和GPTTokenizer，用户提出需要为mindnlp库添加对GPT模型的支持以及添加GPTTokenizer。,https://github.com/mindspore-lab/mindnlp/issues/464
mindnlp,这是一个用户提出需求的issue，主要对象是gpt模型支持图模式。由于当前模型不支持图模式，用户希望项目能够增加该功能。,https://github.com/mindspore-lab/mindnlp/issues/463
mindnlp,这个issue属于改进需求类型，主要涉及mindnlp项目中的backbones模块。原因可能是为了优化项目结构或提高代码可维护性。,https://github.com/mindspore-lab/mindnlp/issues/462
mindnlp,这个issue属于需求提出类型，主要涉及的对象是BertTokenizer，用户希望支持`from_pretrained`和修改bert_finetune示例。,https://github.com/mindspore-lab/mindnlp/issues/461
mindnlp,这是一个用户提出需求的issue，主要涉及到在mindnlp中添加bert-lstm-crf示例的功能。由于内容简洁且含糊不清，可能是用户希望增加有关于bert-lstm-crf示例的相关信息。,https://github.com/mindspore-lab/mindnlp/issues/460
mindnlp,这个issue类型为功能需求，主要涉及对象为mindnlp项目，用户提出需增加bertlstmcrf示例的功能。,https://github.com/mindspore-lab/mindnlp/issues/459
mindnlp,类型：需求提出；涉及的主要对象：gpt2_config；原因：需要对gpt2_config进行更新。,https://github.com/mindspore-lab/mindnlp/issues/457
mindnlp,这是一个用户提出需求的issue，主要涉及到的对象是CI（持续集成）系统。,https://github.com/mindspore-lab/mindnlp/issues/456
mindnlp,"这是一个用户提出需求的类型，该问题单涉及的主要对象是""feat: PET""。由于需要新增PET功能，用户提出了该需求。",https://github.com/mindspore-lab/mindnlp/issues/454
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是新增的news_summary功能。由于issue内容为空，用户可能希望添加新闻摘要功能到MindNLP中。,https://github.com/mindspore-lab/mindnlp/issues/452
mindnlp,这是一个用户提出需求的issue，主要涉及Mindnlp软件库支持Microsoft 1.8版本的问题。,https://github.com/mindspore-lab/mindnlp/issues/451
mindnlp,这是一个需求变更类型的issue，主要涉及预训练模型的更新。原因可能是为了改进模型性能或者增加新功能。,https://github.com/mindspore-lab/mindnlp/issues/448
mindnlp,这是一个类型为功能改进的issue，涉及主要对象为将mixin.py文件移动到abc.backbones目录。,https://github.com/mindspore-lab/mindnlp/issues/446
mindnlp,这是一个代码更新的issue，主要对象是代码生成器。由于代码可能过时或有bug，需要更新以解决问题。,https://github.com/mindspore-lab/mindnlp/issues/445
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是在mindnlp库中添加Roberta模型。,https://github.com/mindspore-lab/mindnlp/issues/444
mindnlp,这是一则用户提出需求的 issue，主要对象是为 mindnlp 添加 kaggle GPU 测试。,https://github.com/mindspore-lab/mindnlp/issues/442
mindnlp,这是一个需求提出的issue，主要涉及到需要添加T5Tokenizer及其单元测试。由于当前代码库缺少T5Tokenizer及其相关测试，用户提出了在代码库中添加这些内容的需求。,https://github.com/mindspore-lab/mindnlp/issues/441
mindnlp,这个issue是关于需求的，主要涉及更新预训练模型中的from_json_file，并由于用户需要新功能或改进。,https://github.com/mindspore-lab/mindnlp/issues/440
mindnlp,这个issue属于需求提出类型，主要涉及mindnlp项目中的词向量模型 llama_hf 的完善。,https://github.com/mindspore-lab/mindnlp/issues/439
mindnlp,这是一个需求类型的issue，主要涉及到添加 llama_hf 功能。,https://github.com/mindspore-lab/mindnlp/issues/438
mindnlp,这是一个需求类型的issue，主要涉及Mindnlp项目中添加完整的哪吒模型和单元测试。,https://github.com/mindspore-lab/mindnlp/issues/437
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是模型预训练。由于缺乏详细描述，无法确定具体问题原因。,https://github.com/mindspore-lab/mindnlp/issues/436
mindnlp,这是一个需求类型的issue，主要涉及到修复squad流程和添加bidaf scratch。,https://github.com/mindspore-lab/mindnlp/issues/435
mindnlp,"这是一个用户提出需求的类型 issue，主要对象是添加一个名为""model_codegen""的功能。",https://github.com/mindspore-lab/mindnlp/issues/433
mindnlp,这是一个用户提出需求的issue，主要对象是对MobileBERT fine-tuning的功能增加。,https://github.com/mindspore-lab/mindnlp/issues/431
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是GPT-2模型参数和模块更新。,https://github.com/mindspore-lab/mindnlp/issues/430
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是完善 MegatronBert 模型。,https://github.com/mindspore-lab/mindnlp/issues/427
mindnlp,该问题类型是需求提出，主要对象是添加Attention模型。,https://github.com/mindspore-lab/mindnlp/issues/425
mindnlp,该issue类型为需求提出，主要对象是在mindnlp项目中添加预训练模型。由于用户希望在项目中加入预训练模型，故提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/423
mindnlp,这是一个请求将所有模型添加到Mindnlp中的需求类型的issue。该问题涉及的主要对象是Mindnlp项目。,https://github.com/mindspore-lab/mindnlp/issues/422
mindnlp,这个issue属于用户需求，请求在utils中添加一个功能。主要对象是utils模块。可能是由于现有功能不足或者用户需要更多实用的功能而提出了该需求。,https://github.com/mindspore-lab/mindnlp/issues/421
mindnlp,这个issue类型为功能需求，主要涉及的对象是GPTNeo模块。该问题由于用户需要添加GPTNeo模块而提出。,https://github.com/mindspore-lab/mindnlp/issues/420
mindnlp,这是一个用户提出需求的issue，主要对象是添加所有模型。可能由于当前仓库尚未包含所有模型，用户希望将所有模型添加进去。,https://github.com/mindspore-lab/mindnlp/issues/419
mindnlp,这是一个需求类型的issue，提出了添加所有模型的要求。 ,https://github.com/mindspore-lab/mindnlp/issues/418
mindnlp,这是一个用户提出需求的issue，主要对象是mindnlp库，用户提出需要添加bloom模型以及fp16支持，但不支持bf16。,https://github.com/mindspore-lab/mindnlp/issues/417
mindnlp,这个issue类型是用户提出需求，目的是添加Ernie和UIE模型到项目中。,https://github.com/mindspore-lab/mindnlp/issues/415
mindnlp,"这是一个需求提出类型的issue，主要对象是添加一个名为""erine""的功能模块。",https://github.com/mindspore-lab/mindnlp/issues/412
mindnlp,这是一个缺少详细描述的用户提出需求的issue，主要涉及更新XLM代码，原因可能是为了修复BUG或添加新功能。,https://github.com/mindspore-lab/mindnlp/issues/411
mindnlp,这个issue是用户提出需求，请求添加Llama模型和RMSNorm UT测试。该问题单涉及的主要对象是模型和测试工具。由于用户希望增加新的功能模型和测试，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/410
mindnlp,这个issue属于用户提出需求类型，主要涉及的对象是添加albert模型。这个需求是由于缺乏albert模型而提出的。,https://github.com/mindspore-lab/mindnlp/issues/409
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是在MindNLP中添加Longformer SelfAttention类。由于缺乏Longformer SelfAttention类，用户发现无法满足其需求，因此提出了该需求。,https://github.com/mindspore-lab/mindnlp/issues/407
mindnlp,这是一个功能需求类型的issue，主要涉及的对象是mindnlp库中的半精度操作warper。由于缺乏添加半精度操作warper的功能，用户提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/405
mindnlp,这是一个用户提出需求的Issue，请求新增适配xlm-robert-base，xlm-roberta-large，flan-t5-base，flan-t5-large等四个模型到mindnlp的xlmrobert。,https://github.com/mindspore-lab/mindnlp/issues/404
mindnlp,这个issue类型是用户提出需求，主要对象是新增GPTNeo模块。,https://github.com/mindspore-lab/mindnlp/issues/403
mindnlp,这是一个用户提出需求的issue，主要对象是向MindNLP项目添加GPT模型。,https://github.com/mindspore-lab/mindnlp/issues/402
mindnlp,这是一个用户提出需求的issue，主要对象是在github上的mindnlp项目，用户希望添加GPT模型。,https://github.com/mindspore-lab/mindnlp/issues/401
mindnlp,这是一个功能需求的issue，主要涉及支持旧版mindspore，但不包括预训练模型。可能由于用户需要在旧版mindspore中使用自定义模型，因此提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/400
mindnlp,这是一个用户提出需求的Issue，主要涉及的对象是Luke模型的模块和单元测试。,https://github.com/mindspore-lab/mindnlp/issues/399
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是添加功能。由于未提供具体内容，导致无法确定用户需要添加何种功能。,https://github.com/mindspore-lab/mindnlp/issues/398
mindnlp,这个issue是一个用户提出需求类型的问题单，主要涉及修改文档需求，由于可能当前的文档内容不全或有错误导致用户需要修改。,https://github.com/mindspore-lab/mindnlp/issues/397
mindnlp,"这个issue是用户提出需求的类型，主要对象是在mindnlp仓库中的一个名为""add bidaf_squad_concise.ipynb""的文件。",https://github.com/mindspore-lab/mindnlp/issues/396
mindnlp,这是一个用户提出需求的issue，主要对象是添加XLM模型。由于缺少XLM模型，用户希望在项目中添加该模型。,https://github.com/mindspore-lab/mindnlp/issues/395
mindnlp,这是一个功能需求类的issue，主要涉及到fasttext和glove的词汇表实用性问题，用户提出需要为这两个模型添加vocab ut。,https://github.com/mindspore-lab/mindnlp/issues/394
mindnlp,这个issue类型是需求提出，主要涉及的对象是luke模型的添加和版权声明修改。这个问题由于需要在mindnlp中添加luke模型的代码块，并修改版权声明。,https://github.com/mindspore-lab/mindnlp/issues/393
mindnlp,这个issue类型为功能请求，主要涉及的对象是添加LUKE模型的相关代码块和修改版权声明。这个问题来源于用户的需求，希望增加LUKE模型的支持和修改版权声明。,https://github.com/mindspore-lab/mindnlp/issues/392
mindnlp,这个issue类型是需求类型，提出了修改版权的要求。寻求对版权的修改帮助。,https://github.com/mindspore-lab/mindnlp/issues/391
mindnlp,这是一个功能需求提出的issue，主要对象是modele_utils工具。它由于功能不完善而需要更新，用户提出了相关的问题和需求。,https://github.com/mindspore-lab/mindnlp/issues/389
mindnlp,这个issue是一个功能需求提出类型，主要涉及对象是情感分析模型的优化。,https://github.com/mindspore-lab/mindnlp/issues/388
mindnlp,这是一个用户提出需求的issue，主要对象是向mindnlp仓库添加GPTNeo预训练模型，问题产生的原因是用户希望能够使用GPTNeo模型进行相关的任务。,https://github.com/mindspore-lab/mindnlp/issues/384
mindnlp,这是一个用户需求类型的issue，主要涉及MindNLP库中使用Pythonic词汇和支持MindData管道的功能。原因可能是用户希望改进库的易用性和整合性。,https://github.com/mindspore-lab/mindnlp/issues/383
mindnlp,这是一个需求提交类型的issue，主要涉及MindNLP中添加T5 tokenizer和更新T5注释。原因是用户希望在MindNLP中添加T5 tokenizer以及更新相关的T5注释。,https://github.com/mindspore-lab/mindnlp/issues/382
mindnlp,这是一个用户提出需求的issue，主要涉及MindNLP中支持BERT的RandomMask Transform，目的是用于运行时数据集处理。,https://github.com/mindspore-lab/mindnlp/issues/381
mindnlp,这是一个用户提出需求类型的issue，主要涉及对象为添加bert_senta_ipynb和_check_work_files的内容。可能是用户希望文档中增加这两个内容，以便更好地理解和使用相关功能。,https://github.com/mindspore-lab/mindnlp/issues/380
mindnlp,这个issue类型为功能更新请求，主要对象是modles.utils模块。由于可能发现现有功能不够完善或有待改进，用户提出了对utils模块的更新需求。,https://github.com/mindspore-lab/mindnlp/issues/379
mindnlp,该issue属于用户提出需求类型，主要涉及为mindnlp添加TinyBert模型。由于当前仓库并未包含TinyBert模型，用户提出了添加该模型的请求。,https://github.com/mindspore-lab/mindnlp/issues/378
mindnlp,这是一个用户提出需求的issue，主要对象是增加MobileBERT模块，可能是由于项目需要使用MobileBERT模型进行相关任务而提出的。,https://github.com/mindspore-lab/mindnlp/issues/377
mindnlp,这个issue是用户提出需求，并希望添加XLM模型到MindNLP中。,https://github.com/mindspore-lab/mindnlp/issues/375
mindnlp,这是一个用户提出需求类型的issue，主要涉及的对象是在mindnlp中添加Luke模型的LukeEmbeddings功能。,https://github.com/mindspore-lab/mindnlp/issues/372
mindnlp,该issue类型为功能需求提出，主要对象是希望在mindnlp中添加MobileBERT模块的开发者。,https://github.com/mindspore-lab/mindnlp/issues/371
mindnlp,这个issue类型是用户提出需求，请求获取最新的whl文件，主要涉及的对象是mindspore。由于用户希望获取最新版本的whl文件，因此提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/370
mindnlp,这是一个需求类型的issue，主要涉及的对象是向mindnlp项目添加GPTNeo SelfAttention功能。,https://github.com/mindspore-lab/mindnlp/issues/369
mindnlp,这是一个用户提出需求的issue，主要对象是在mindnlp中添加GPT1模型。,https://github.com/mindspore-lab/mindnlp/issues/367
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是向MindNLP工作流程中添加情感分析工作和模型。,https://github.com/mindspore-lab/mindnlp/issues/366
mindnlp,这是一个用户提出需求的issue，主要对象是mindnlp库下的StaticGRU和StaticLSTM模型。由于缺乏这两种静态循环神经网络模型的实现，用户希望添加它们到mindnlp库中。,https://github.com/mindspore-lab/mindnlp/issues/364
mindnlp,这是一个用户提出需求的issue，主要对象是修改文档结构。,https://github.com/mindspore-lab/mindnlp/issues/363
mindnlp,这是一个功能需求问题，主要涉及对象是mindnlp套件中的Vocab对象，用户需要一个用于处理Vocab对象的类，同时删除word2vec embedding和gensim库相关部分。,https://github.com/mindspore-lab/mindnlp/issues/362
mindnlp,这是一个用户提出需求的issue，主要涉及到MobileBERT添加模块的功能。由于用户希望增加一些新的模块，因此提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/358
mindnlp,这个issue类型是用户提出需求，主要对象是mindnlp的functional和embedding class，用户请求在这两个类中添加cumsum功能和长表征（long former）。,https://github.com/mindspore-lab/mindnlp/issues/357
mindnlp,"这个issue类型是功能需求提议，主要对象是mindnlp中的T5模型相关组件。由于当前代码库缺少T5模型相关的组件（如T5Stack, T5Model, T5ForConditionalGeneration, T5EncoderModel），用户提出了需要添加这些组件的需求。",https://github.com/mindspore-lab/mindnlp/issues/356
mindnlp,"这是一个用户提出需求的issue，主要对象是为mindnlp增加mobilebert中的selfoutput,attention,intermediate和outputbottleneck功能。",https://github.com/mindspore-lab/mindnlp/issues/355
mindnlp,"这是一个用户提出需求的 issue，主要涉及的目标是在 mindnlp 中添加 selfoutput, attention, intermediate 和 outputbottleneck。由于缺乏这些功能，用户希望对模型进行更深入的细节处理。",https://github.com/mindspore-lab/mindnlp/issues/354
mindnlp,这个issue类型为代码优化和功能增强，主要涉及到指标重构和添加机器翻译的scratch笔记本。,https://github.com/mindspore-lab/mindnlp/issues/353
mindnlp,这个issue属于用户提出需求类型，主要涉及的对象是修改BertTokenizer。原因可能是用户希望对BertTokenizer进行定制化以满足特定需求。,https://github.com/mindspore-lab/mindnlp/issues/352
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是添加LUKE模型。,https://github.com/mindspore-lab/mindnlp/issues/351
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是添加T5PreTrainedModel和CellUtilMixin，可能是为了增加模型的功能性或者灵活性。,https://github.com/mindspore-lab/mindnlp/issues/350
mindnlp,这是一个用户提出需求的类型。该问题单涉及的主要对象是向mindnlp仓库添加MobileBERT模型。由于当前MindNLP仓库尚未包含MobileBERT模型，用户希望添加该模型以扩展仓库的功能。,https://github.com/mindspore-lab/mindnlp/issues/349
mindnlp,这个issue是用户提出需求，希望添加MobileBERT embeddings和self-attention。,https://github.com/mindspore-lab/mindnlp/issues/348
mindnlp,"这是一个需求提出类型的issue，主要涉及的对象是""mobilebert""模型。由于用户需要了解有关""mobilebert""模型的信息，所以提出了这个问题。",https://github.com/mindspore-lab/mindnlp/issues/347
mindnlp,"这个issue类型是需求提出，主要对象是添加一个名为""gpt2_mmodule""的功能模块。",https://github.com/mindspore-lab/mindnlp/issues/346
mindnlp,这是一个用户提出需求的issue，主要对象是_legacy/functional和longformer_embedding模型，用户提出了添加'cumsum'功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/344
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是在Mindnlp库中添加Longformer Embedding模型。这可能是因为用户希望在该库中使用Longformer Embedding模型来进行自然语言处理任务。,https://github.com/mindspore-lab/mindnlp/issues/343
mindnlp,这是一个用户提出需求的 issue，需要添加xlmpredlayer。由于缺乏xlmpredlayer，用户无法完成特定操作或功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/342
mindnlp,这个issue是用户提出的需求，主要涉及的对象是添加Longformer embedding模型。用户希望在MindNLP中加入这个预训练模型，以扩展其功能。,https://github.com/mindspore-lab/mindnlp/issues/341
mindnlp,这是一个需求类型的问题，主要涉及 mindnlp 下的 models.models.utils 路径调整的问题。由于路径调整不正确导致了错误或不符合预期的行为。,https://github.com/mindspore-lab/mindnlp/issues/340
mindnlp,"这是一个需求提出类型的issue，主要涉及的对象是添加一个名为""utils_modeling_util""的工具类。原因可能是为了提供更多关于模型的实用工具函数或方法。",https://github.com/mindspore-lab/mindnlp/issues/338
mindnlp,这是一个需求类型的issue，主要对象是mindnlp项目中的models目录。,https://github.com/mindspore-lab/mindnlp/issues/337
mindnlp,这是一个用户提出需求的issue， 主要涉及的对象是mindnlp下的T5模型模块。可能是由于缺少相关模块，用户希望添加T5模型中的几个blocks。,https://github.com/mindspore-lab/mindnlp/issues/336
mindnlp,这是一个用户提出需求的类型，主要涉及MindNLP中的gpt2模块添加activations功能。,https://github.com/mindspore-lab/mindnlp/issues/335
mindnlp,这个issue类型是功能需求，主要对象是添加utils activations功能。,https://github.com/mindspore-lab/mindnlp/issues/334
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是在mindnlp库中添加GPT-2模型声明。,https://github.com/mindspore-lab/mindnlp/issues/333
mindnlp,这是一个用户提出需求的类型，主要对象是创建Longformer模型，由于缺少三个空白的Python文件而无法完成。,https://github.com/mindspore-lab/mindnlp/issues/331
mindnlp,该issue是用户提出需求，希望添加BertTokenizer到mindnlp中。,https://github.com/mindspore-lab/mindnlp/issues/329
mindnlp,这是一个用户提出需求的 issue，主要对象是添加一个名为 Vocab 的类。原因可能是用户想要在项目中使用 Vocab 类来实现特定功能。,https://github.com/mindspore-lab/mindnlp/issues/328
mindnlp,这是一个用户提出需求的类型，主要涉及接入Hugging Face数据集的问题。,https://github.com/mindspore-lab/mindnlp/issues/327
mindnlp,这是一个用户提出需求的issue，该问题涉及的主要对象是GRUV2 operator。这可能是用户希望通过使用GRUV2 operator来实现高性能的GRU模型。,https://github.com/mindspore-lab/mindnlp/issues/326
mindnlp,这是一个关于接入huggingface数据集的demo的需求类型issue，主要涉及项目中的数据集接入功能。,https://github.com/mindspore-lab/mindnlp/issues/325
mindnlp,这是一个用户提出需求的issue，该问题主要涉及mindnlp库中的PadTransform模块。由于缺乏PadTransform模块，用户可能无法对文本进行填充操作，因此请求添加该功能以提升库的功能性。,https://github.com/mindspore-lab/mindnlp/issues/323
mindnlp,这是一个用户提出需求的issue，主要对象是legacy MindSpore。这个问题是因为需要支持AddToken transform而产生的。,https://github.com/mindspore-lab/mindnlp/issues/322
mindnlp,"这是一个用户提出需求的类型，主要对象是""easy pr test""。由于该issue标题和内容为空，用户可能在尝试进行某项操作时遇到了问题或者需要进行简单的测试。",https://github.com/mindspore-lab/mindnlp/issues/321
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是models模块。由于缺乏 megatron_bert 模型，用户请求将其添加到models中。,https://github.com/mindspore-lab/mindnlp/issues/320
mindnlp,这个issue类型是需求提出，涉及主要对象是pagua_alphago_init。这可能是用户提出对于pagua_alphago_init的初始化方面的需求或者建议。,https://github.com/mindspore-lab/mindnlp/issues/319
mindnlp,这是一个用户提出需求的issue，主要对象是在mindnlp中添加transformer模型。,https://github.com/mindspore-lab/mindnlp/issues/318
mindnlp,该issue类型为需求提出，主要对象是在mindnlp项目中增加gpt_neo模型。,https://github.com/mindspore-lab/mindnlp/issues/317
mindnlp,这是一个用户提出需求的类型(issue)。该问题单涉及的主要对象是mindnlp项目。由于缺少具体描述内容，无法分析导致了什么样症状的bug或者用户提出了关于什么的问题或者寻求什么样的帮助。,https://github.com/mindspore-lab/mindnlp/issues/316
mindnlp,这个issue类型是用户提出需求，主要对象是增加一个双向LSTM模型用于IMDB数据集的情感分类，用户提出了对模型实现的需求。,https://github.com/mindspore-lab/mindnlp/issues/315
mindnlp,这是一个用户提出需求的issue，主要对象是添加XLM模型。,https://github.com/mindspore-lab/mindnlp/issues/314
mindnlp,这是一个用户需求类型的issue，主要涉及mindnlp项目中的models目录结构规划。该issue源于用户对模型文件夹opt的需求。,https://github.com/mindspore-lab/mindnlp/issues/312
mindnlp,这个issue是用户提出需求，请求在mindnlp下添加bart模型。,https://github.com/mindspore-lab/mindnlp/issues/311
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是在Mindnlp中添加OpenAI GPT1模型。用户之所以提出该需求可能是希望增加OpenAI GPT1模型在项目中的可用性和功能性。,https://github.com/mindspore-lab/mindnlp/issues/310
mindnlp,"这是一个用户提出需求的类型，主要对象是想要创建名为""roberta""的文件夹。由于还未提供具体内容，可能是用户希望组织Roberta模型相关的文件或数据。",https://github.com/mindspore-lab/mindnlp/issues/309
mindnlp,"这个issue是用户提出需求，主要对象是在github上的mindnlp下的mobilebert项目。由于用户需要关于mobilebert的启动信息，用户提出了""mobilebert start""的问题。",https://github.com/mindspore-lab/mindnlp/issues/307
mindnlp,"这是一个需求提出类型的issue，主要对象是""mobilebert""模型。由于用户需要更多关于""mobilebert""的信息或者功能，因此提出了这个问题。",https://github.com/mindspore-lab/mindnlp/issues/306
mindnlp,该issue是添加Longformer模型的需求提出，涉及主要对象为代码中的文件组织和配置。由于未完成的代代码导致了需要添加Longformer模型时出现问题。,https://github.com/mindspore-lab/mindnlp/issues/305
mindnlp,这是一个需求类型的issue，主要对象是要创建longformer文件夹以及相应文件。这个需求可能是为了添加新的功能或模块以支持longformer模型的集成。,https://github.com/mindspore-lab/mindnlp/issues/304
mindnlp,"该issue类型为需求创建，并主要涉及创建名为__init__, longformer_config和longformer_model的文件。",https://github.com/mindspore-lab/mindnlp/issues/303
mindnlp,该issue类型为用户提出需求，主要对象是给mindnlp添加T5模型。,https://github.com/mindspore-lab/mindnlp/issues/302
mindnlp,这个issue类型为需求提出，主要对象是MindNLP库，用户提出希望支持GPT2模型。,https://github.com/mindspore-lab/mindnlp/issues/301
mindnlp,这个issue属于需求提出类型，主要对象是mindnlp的文档目录结构调整。可能是由于当前文档目录结构不够清晰或者易用，用户提出需要调整以提高文档的可阅读性或者使用性。,https://github.com/mindspore-lab/mindnlp/issues/300
mindnlp,这是一个用户提出需求的issue，主要对象是希望增加一个notebook示例，用户认为当前的文档不足以提供足够的指导。,https://github.com/mindspore-lab/mindnlp/issues/299
mindnlp,"这是一个需求修复类型的issue，主要涉及到""machine_translation""部分代码的修复。原因可能是jit参数导致了错误，同时需要完善数据集的文档翻译。",https://github.com/mindspore-lab/mindnlp/issues/296
mindnlp,这是一个功能需求类型的issue，主要涉及到需要移除self-defined multiheadattention模块，可能是因为该模块不再需要或者存在其他更好的替代方案。,https://github.com/mindspore-lab/mindnlp/issues/293
mindnlp,这个issue类型是用户提出需求，需要添加关于transforms的文档说明。该问题单涉及的主要对象是MindNLP库中的transforms功能。由于缺乏transforms的文档说明，用户提出了需要补充transforms文档的需求。,https://github.com/mindspore-lab/mindnlp/issues/291
mindnlp,这是一个用户提出需求的issue，主要涉及文档示例的编写，导致缺少必要的参数说明。,https://github.com/mindspore-lab/mindnlp/issues/286
mindnlp,该问题类型为用户提出需求，主要对象是为MindNLP增加工作流框架。这个需求可能是为了提升MindNLP的处理效率和功能多样性。,https://github.com/mindspore-lab/mindnlp/issues/285
mindnlp,这是一个用户提出需求的issue，主要涉及到MindNLP的文档中关于模型训练和评估的内容，可能是由于缺少初始值和参数变化导致运行错误和评估报错。,https://github.com/mindspore-lab/mindnlp/issues/281
mindnlp,该issue类型为功能开发需求，主要涉及Attentions模块的开发。,https://github.com/mindspore-lab/mindnlp/issues/274
mindnlp,这是一个功能请求类型的issue，主要涉及到函数式metrics的开发。,https://github.com/mindspore-lab/mindnlp/issues/273
mindnlp,这个issue类型是用户提出需求，主要涉及的对象是mindnlp的基类开发。,https://github.com/mindspore-lab/mindnlp/issues/272
mindnlp,这是一个用户提出需求的issue，主要涉及PretrainedModel基类，可能是在使用过程中发现该基类功能不完善或者需要优化。,https://github.com/mindspore-lab/mindnlp/issues/268
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是CheckpointCallback，用户希望开发相关功能。,https://github.com/mindspore-lab/mindnlp/issues/265
mindnlp,这是一个需求提出类的issue，主要对象是BestModelCallback开发。用户可能因为需要在Mindnlp中引入更高效的模型回调机制而提出这个需求。,https://github.com/mindspore-lab/mindnlp/issues/264
mindnlp,这个issue属于用户提出需求类型，主要涉及条件随机场，可能是由于库中条件随机场功能不完善或者用户对该功能的使用方式不清楚而导致。,https://github.com/mindspore-lab/mindnlp/issues/263
mindnlp,该issue类型为需求提出，主要涉及对象是Trainer接口。由于缺乏Trainer接口的开发，用户提出需要开发该功能的需求。,https://github.com/mindspore-lab/mindnlp/issues/262
mindnlp,这是一个关于需求开发的issue，主要涉及的对象是Evaluator接口。由于需要开发该接口，用户提出了相关需求或问题，寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/261
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是TimerCallback功能。,https://github.com/mindspore-lab/mindnlp/issues/260
mindnlp,这是一个用户提出需求的issue，主要涉及EarlyStopCallback的开发。,https://github.com/mindspore-lab/mindnlp/issues/259
mindnlp,这个issue类型为功能需求提议，主要涉及对象是Trainer和Evaluator，用户提出了希望在训练过程中能够边训练边测试的功能需求。,https://github.com/mindspore-lab/mindnlp/issues/258
mindnlp,这是一个用户请求下载http_get的issue，该问题属于需求类型，主要涉及mindnlp库中的下载功能。,https://github.com/mindspore-lab/mindnlp/issues/256
mindnlp,这是一个需求问题，用户提出希望获取下载文件路径的功能。,https://github.com/mindspore-lab/mindnlp/issues/255
mindnlp,这个issue类型是用户提出需求，主要对象是recall功能。由于某种原因导致用户需要使用具体的recall功能或者有关recall功能的问题。,https://github.com/mindspore-lab/mindnlp/issues/244
mindnlp,这是一个用户提出需求的类型，该问题涉及对象是mcc。由于issue内未提供具体内容，无法分析导致 bug 或问题的具体原因。,https://github.com/mindspore-lab/mindnlp/issues/241
mindnlp,"这个issue属于用户提出需求类型，主要对象是MindNLP中的""spearman""功能。",https://github.com/mindspore-lab/mindnlp/issues/239
mindnlp,"这是一个需求问题，涉及主要对象是""em_score""计算功能。可能出现这个问题的原因是开发者希望添加或优化em_score功能，或用户对现有功能有建议或需求。",https://github.com/mindspore-lab/mindnlp/issues/238
mindnlp,这是一个用户提出需求的issue，主要对象是mindnlp这个项目。由于缺少具体内容，无法确定用户提出了何种需求。,https://github.com/mindspore-lab/mindnlp/issues/237
mindnlp,"这是一个用户提出需求的issue，主要涉及的对象是""consine_attention""。由于需要改进注意力机制的效果，用户提出了关于cosine attention的问题或寻求相关帮助。",https://github.com/mindspore-lab/mindnlp/issues/236
mindnlp,"这个issue类型是功能需求提出，主要涉及对象是""location_attention""功能。由于缺少具体描述或内容，用户可能提出关于如何实现或改进该功能的问题或寻求相关帮助。",https://github.com/mindspore-lab/mindnlp/issues/235
mindnlp,"这是一个用户提出需求类型的issue，主要涉及的对象是""scaled_dot_attention""。需要解决的问题可能是该功能在某些情况下无法正常工作，导致用户希望能够改进或修复这部分功能。",https://github.com/mindspore-lab/mindnlp/issues/233
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是 self attention。由于issue内容为空，用户可能提出了关于 self attention 的问题或寻求关于 self attention 的帮助。,https://github.com/mindspore-lab/mindnlp/issues/232
mindnlp,这是一个用户提出需求的类型，主要涉及多头注意力机制的问题。原因可能是该机制在使用过程中出现了某些不符合预期的行为或效果，用户希望能够解决或改进相关问题。,https://github.com/mindspore-lab/mindnlp/issues/231
mindnlp,这是一个用户提出需求的issue，主要对象是BaseModel基类。,https://github.com/mindspore-lab/mindnlp/issues/230
mindnlp,这个issue属于用户提出需求，主要涉及的对象是Seq2seqModel基类。用户可能对该基类功能或设计有建议或需要额外的功能支持。,https://github.com/mindspore-lab/mindnlp/issues/229
mindnlp,这是一个用户提出需求的类型的issue，主要涉及Seq2vecModel的基类。由于缺少具体内容，用户可能提出了关于Seq2vecModel基类的问题或寻求相关帮助。,https://github.com/mindspore-lab/mindnlp/issues/228
mindnlp,这是一个需求提出类的issue，主要涉及的对象是MindNLP中的EncodeBase基类。由于缺乏具体内容，用户可能提出对该基类的新增或修改需求。,https://github.com/mindspore-lab/mindnlp/issues/227
mindnlp,该issue为功能需求提报，主要涉及的对象是DecoderBase基类。,https://github.com/mindspore-lab/mindnlp/issues/226
mindnlp,这是一个需求类型的issue，主要对象是Seq2seq模型。,https://github.com/mindspore-lab/mindnlp/issues/223
mindnlp,这个issue类型是用户提出需求，主要对象是CNN关系抽取模型。由于用户希望提供关于该模型的功能或改进建议，所以提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/222
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是下载缓存文件。由于缓存文件下载可能出现问题，用户提出了需要从缓存中下载文件的需求。,https://github.com/mindspore-lab/mindnlp/issues/221
mindnlp,这个issue类型为需求提出，主要涉及的对象是Callback基类。由于缺乏Callback基类的具体描述和实现细节，用户提出了关于Callback基类的需求或问题，希望得到更多帮助或解决方案。,https://github.com/mindspore-lab/mindnlp/issues/218
mindnlp,这个issue类型是功能需求提出，涉及主要对象是LSTMEncoder模块。由于该模块在功能上有待完善或缺失导致用户提出改进建议或功能补充。,https://github.com/mindspore-lab/mindnlp/issues/215
mindnlp,这是一个用户提出需求类型的issue，该问题单涉及的主要对象是mindnlp下的一个支持旧版本的问题。用户可能由于需要在旧版本中使用特定功能或者修复一些旧版的bug而提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/208
mindnlp,这是一个用户提出需求的issue，主要涉及mindnlp下的common/nn catalog。由于未提供具体内容，无法分析问题的具体原因。,https://github.com/mindspore-lab/mindnlp/issues/207
mindnlp,这个issue类型是优化建议，主要对象是CRF模块。可能是由于当前实现中的效率较低，用户建议使用while循环来加速CRF算法的执行。,https://github.com/mindspore-lab/mindnlp/issues/206
mindnlp,这是一个用户提出需求类型的issue，主要涉及对象是legacy MindSpore，用户提出添加Transformer API的需求。,https://github.com/mindspore-lab/mindnlp/issues/204
mindnlp,这是一个更新构建版本的issue，主要对象是软件的构建版本。由于需要将构建版本从0.0.1更新到0.1.1，用户可能遇到了一些功能或者性能方面的改进，或者修复了一些bug。,https://github.com/mindspore-lab/mindnlp/issues/202
mindnlp,这是一个关于更新构建版本的issue，类型是需求报告，主要对象是软件的版本号。,https://github.com/mindspore-lab/mindnlp/issues/201
mindnlp,这是一个用户提出需求的类型issue，主要涉及对象是添加中文attention的快速入门指南。原因是用户希望能够更快地了解和使用中文attention相关功能。,https://github.com/mindspore-lab/mindnlp/issues/200
mindnlp,这个issue属于需求提出类型，主要对象是MindNLP库的文档，用户提出了希望增加关于数据集和数据转换的文档内容的需求。,https://github.com/mindspore-lab/mindnlp/issues/199
mindnlp,这是一个文档相关的需求问题，主要涉及的对象是数据集和数据转换。,https://github.com/mindspore-lab/mindnlp/issues/197
mindnlp,这是一个用户提出需求类型的issue，主要涉及的对象是attention模块。由于缺少具体描述或内容，用户可能在期望快速开始使用attention模块时遇到困难。,https://github.com/mindspore-lab/mindnlp/issues/196
mindnlp,这是一个用户提出需求的issue，主要涉及添加回调函数快速入门文档的中文翻译，原因可能是为了帮助中文用户更好地理解回调函数的使用方法。,https://github.com/mindspore-lab/mindnlp/issues/195
mindnlp,"这个issue类型是用户提出需求，涉及主要对象是mindnlp仓库。用户希望在目录树中添加""use_callback""功能，可能是为了增强代码的可维护性或者提升开发效率。",https://github.com/mindspore-lab/mindnlp/issues/193
mindnlp,这是一个需求类型的issue单，主要对象是添加回调函数的文档，并提出需要新增回调函数文档的请求。,https://github.com/mindspore-lab/mindnlp/issues/192
mindnlp,这是一个用户提出需求的类型，主要涉及的对象是项目中的Encoder-Decoder translation功能。该需求可能是由于用户希望增加一种新的翻译功能，以满足特定的需求。,https://github.com/mindspore-lab/mindnlp/issues/190
mindnlp,这是一个需求提出类型的issue，主要涉及embedding module translate功能的开发需求。,https://github.com/mindspore-lab/mindnlp/issues/189
mindnlp,这个issue类型是更新po文件，涉及到主要对象是多语言翻译。,https://github.com/mindspore-lab/mindnlp/issues/187
mindnlp,这是一个用户提出需求的类型，主要涉及情感分类更新问题，用户想要进行情感分类的相关更新。,https://github.com/mindspore-lab/mindnlp/issues/185
mindnlp,这是一个请求更新嵌入模块文档的问题，主要涉及嵌入模块（embedding module）。由于文档可能过时或不完整，导致用户需要更新或添加更多关于嵌入模块的信息。,https://github.com/mindspore-lab/mindnlp/issues/184
mindnlp,这是一个需求类型的issue，主要涉及fasttext的文档更新。,https://github.com/mindspore-lab/mindnlp/issues/183
mindnlp,这个issue类型是文档修改请求，涉及到修改一些 .rst 文件。原因可能是文档内容需要更新或者更正。,https://github.com/mindspore-lab/mindnlp/issues/182
mindnlp,这是一个需求更新类型的issue，涉及主要对象是项目中的中文文档。由于文档需要更新，用户提出了更新关于中文文档的请求。,https://github.com/mindspore-lab/mindnlp/issues/181
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是mindnlp库。由于缺少具体内容，用户提出了添加 QA zh_doc 的需求。,https://github.com/mindspore-lab/mindnlp/issues/180
mindnlp,这个issue类型是文档修改需求，主要涉及的对象是mindnlp的第一个模型文档和encoder-decoder部分。原因可能是前文档存在错误或缺漏，需要修复并添加新的内容。,https://github.com/mindspore-lab/mindnlp/issues/179
mindnlp,"这个issue类型是需求提出，该问题单涉及的主要对象是 ""ut"" 类。由于缺少具体描述，用户可能提出了需要使用 ""ddt"" 来进行 ""ut"" 相关功能的需求问题。",https://github.com/mindspore-lab/mindnlp/issues/178
mindnlp,这个issue是用户提出需求类型的，主要对象是metrics功能。该需求由于缺乏中文语言支持而导致用户提出了关于使用metrics的问题或者寻求中文语言相关的帮助。,https://github.com/mindspore-lab/mindnlp/issues/177
mindnlp,这个issue类型为文档更新请求，涉及主要对象为机器翻译。这个问题可能是由于文档内容过时或者不清晰导致用户需要更新或者补充相关内容。,https://github.com/mindspore-lab/mindnlp/issues/176
mindnlp,这个issue类型是请求添加文档内容，主要对象是mindnlp项目，由于缺少question_answer.rst文档，用户提出了添加该文档的需求。,https://github.com/mindspore-lab/mindnlp/issues/174
mindnlp,该issue类型为用户提出需求或者建议，主要涉及的对象是在MindNLP中添加名为machine_translation.rst的文件。由于可能需要扩充或者补充与机器翻译相关的文档内容，用户提出了添加这个文件的建议。,https://github.com/mindspore-lab/mindnlp/issues/173
mindnlp,这是一个用户提出需求的类型的issue，主要涉及MindNLP第一个模型的文档缺失问题，可能由于项目缺乏相关文档或者尚未完善导致用户提出关于添加第一个模型文档的需求。,https://github.com/mindspore-lab/mindnlp/issues/167
mindnlp,这是一个用户提出需求的类型，主要对象是在mindnlp项目下新增模型文档。原因可能是用户需要更详细的模型文档来了解各个模型的功能和用法。,https://github.com/mindspore-lab/mindnlp/issues/166
mindnlp,这是一个用户提出需求的issue，该问题涉及mindnlp项目中的中文训练和评估功能。原因可能是用户想要训练和评估中文模型，但在实现过程中遇到了困难。,https://github.com/mindspore-lab/mindnlp/issues/165
mindnlp,"这是一个需求报告，主要涉及""sequence_tagging example chinese doc""。由于缺少详细说明，导致用户可能需要关于这个示例文档的更多信息。",https://github.com/mindspore-lab/mindnlp/issues/164
mindnlp,这是一个用户提出需求的issue，主要对象是dataset。由于未提供具体内容，用户可能是在请求添加有关dataset的注册函数。,https://github.com/mindspore-lab/mindnlp/issues/163
mindnlp,这个issue类型为更新po文件，主要对象是国际化语言文件。由于内容为空，用户可能在更新po文件时遇到了问题或者希望进行更新。,https://github.com/mindspore-lab/mindnlp/issues/162
mindnlp,这是一个用户提出需求的issue，主要涉及fasttext示例的使用。,https://github.com/mindspore-lab/mindnlp/issues/160
mindnlp,该issue是关于给出了fasttext示例文档。这是一个用户提出需求的issue，主要涉及fasttext的文档内容。可能是由于文档不全或不清晰导致用户需要更多关于fasttext示例的解释或示例。,https://github.com/mindspore-lab/mindnlp/issues/159
mindnlp,"这是一个用户提出需求的类型，该问题单涉及的主要对象是在mindnlp中增加一个名为""machine_translation.rst""的内容。",https://github.com/mindspore-lab/mindnlp/issues/157
mindnlp,这个issue类型是用户提出需求，请教问题，主要对象是train&eval en功能。用户可能提出这个问题是因为希望了解如何训练和评估英文模型。,https://github.com/mindspore-lab/mindnlp/issues/156
mindnlp,这个issue类型是需求提出，主要对象是训练和评估英文模型，在其中可能由于缺乏具体的训练和评估流程导致用户需要帮助或指导。,https://github.com/mindspore-lab/mindnlp/issues/155
mindnlp,这是一个用户提出需求的类型，在此问题中用户主要关注的对象是文档嵌入（embedding docs），可能是希望了解如何对文档进行嵌入以进行后续的分析或处理。,https://github.com/mindspore-lab/mindnlp/issues/153
mindnlp,这是一个用户提出需求的issue，主要涉及embedding文档，用户可能希望增强文档相关性分析的功能。,https://github.com/mindspore-lab/mindnlp/issues/152
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是embedding文档。原因是用户希望添加关于嵌入文档的信息。,https://github.com/mindspore-lab/mindnlp/issues/150
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是关于下载和CNN编码器相关文档的问题。可能由于缺少相应的文档，用户希望添加下载和CNN编码器的文档。,https://github.com/mindspore-lab/mindnlp/issues/149
mindnlp,该issue类型为用户提出需求，主要涉及对象为下载和CNN编码器文档，原因可能是用户希望获取有关这些文档的信息并请求其添加到项目中。,https://github.com/mindspore-lab/mindnlp/issues/148
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是关于embedding文档的问题。,https://github.com/mindspore-lab/mindnlp/issues/147
mindnlp,这个issue属于文档改进类型，涉及的主要对象是数据集处理（dataset process）。由于文档不清晰或不完整，用户可能无法准确理解如何处理数据集，因此提出了需要改进的建议。,https://github.com/mindspore-lab/mindnlp/issues/146
mindnlp,该issue是关于需求的，主要对象是自动镜像功能；用户因自动镜像功能存在的问题或不符合预期的行为提出了这个issue。,https://github.com/mindspore-lab/mindnlp/issues/143
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是文本分类、序列标注和文本生成，可能是用户希望在MindNLP中增加相关文档内容。,https://github.com/mindspore-lab/mindnlp/issues/142
mindnlp,这是一个用户提需求的issue，主要涉及到嵌入式文档（embedding docs）。通过Issue标题，用户提出了对嵌入式文档的需求。,https://github.com/mindspore-lab/mindnlp/issues/141
mindnlp,这是一个功能增强的issue，该问题单涉及的主要对象是MindNLP的中文注释引擎。,https://github.com/mindspore-lab/mindnlp/issues/140
mindnlp,这个issue是文档更新类型，提及了注意力机制和BERT建模相关内容。,https://github.com/mindspore-lab/mindnlp/issues/139
mindnlp,这个issue是一个关于需求的问题，主要涉及的对象是embedding文档。由于缺少具体内容，用户可能正在寻求关于如何使用或生成嵌入文档的帮助。,https://github.com/mindspore-lab/mindnlp/issues/136
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是embedding中文部分文档，用户希望实现文档的中文部分embedding。,https://github.com/mindspore-lab/mindnlp/issues/135
mindnlp,这是一个需求提出的issue，关注embedding部分的中文文档。由于缺乏embedding部分的中文文档，用户提出希望补充相关内容的需求。,https://github.com/mindspore-lab/mindnlp/issues/133
mindnlp,这是一个需求更改的issue，涉及主要对象是mindnlp项目中的imdb_process和seq2seq模块。由于需要更新imdb_process注释并删除seq2seq模块而产生。,https://github.com/mindspore-lab/mindnlp/issues/132
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是文档和源代码，用户希望为attention和BERT模型添加中文注释。,https://github.com/mindspore-lab/mindnlp/issues/131
mindnlp,该issue类型为文档需求提出，主要对象涉及mindnlp库中的backbones、rnn_encoder、rnn_decoder和about模块，用户希望添加相应的文档说明。,https://github.com/mindspore-lab/mindnlp/issues/130
mindnlp,这是一个用户需求问题，主要涉及到对数据集标注的修改。这个问题可能是由于数据集中存在标注不准确或缺失的情况，用户希望改进数据集的质量。,https://github.com/mindspore-lab/mindnlp/issues/128
mindnlp,这是一个文档相关的需求提出类型的issue，主要涉及的对象是loss文档。由于原文缺少中文注释，用户提出了补充中文注释的需求。,https://github.com/mindspore-lab/mindnlp/issues/124
mindnlp,这是一个用户提出需求的问题，主要涉及Mindnlp的文档修改操作。用户可能提出这个问题是因为希望对文档中的某些内容进行修改或更新。,https://github.com/mindspore-lab/mindnlp/issues/122
mindnlp,这是一个功能需求类型的issue，主要对象是对MindNLP的BERT模型实现。由于缺乏函数式融合编程的BERT模型实现，因此用户提出了对标Hugging Face并添加MindNLP Bert modeling的需求。,https://github.com/mindspore-lab/mindnlp/issues/121
mindnlp,这是一个用户提出需求类型的issue，主要涉及文档翻译。用户希望在项目中添加翻译文档，可能是为了扩大项目的受众群体或提供更好的用户体验。,https://github.com/mindspore-lab/mindnlp/issues/120
mindnlp,该问题类型是新功能需求，主要涉及的对象是README文档。由于缺乏介绍，用户希望添加README的介绍。,https://github.com/mindspore-lab/mindnlp/issues/119
mindnlp,"这是一个用户提出需求的类型，该问题单涉及的主要对象是添加一个名为""make_wheel_releases""的action。由于目前项目中缺少该action，用户提出了需求来增加这个功能。",https://github.com/mindspore-lab/mindnlp/issues/118
mindnlp,"这个issue是关于需求的，主要对象是项目的README文档。原因是缺少""Get Started""部分，导致用户无法快速开始使用该项目。",https://github.com/mindspore-lab/mindnlp/issues/117
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是添加BiLSTM-CRF序列标注模型示例。由于标题和内容不一致，可能是用户在提出需求时出现了错误。 ,https://github.com/mindspore-lab/mindnlp/issues/116
mindnlp,这是一个用户提出需求的issue，主要涉及MindNLP中添加SQuAD1数据集处理和Bidaf示例的需求。,https://github.com/mindspore-lab/mindnlp/issues/114
mindnlp,这个issue类型是功能需求提议，主要涉及对象是添加Conll2000Chunking处理过程。由于缺乏Conll2000Chunking处理步骤，用户提议添加此功能以完善MindNLP。,https://github.com/mindspore-lab/mindnlp/issues/113
mindnlp,这是一个用户提出需求的 issue，主要涉及的对象是 RTE dataset。由于数据可能过时或者质量有待改进，用户建议更新 RTE dataset。,https://github.com/mindspore-lab/mindnlp/issues/112
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是fasttext模型的example函数。由于用户可能希望在agnews数据集上使用fasttext模型，因此提出了对相关示例的修改请求。,https://github.com/mindspore-lab/mindnlp/issues/111
mindnlp,这个issue是用户提出的需求类型的，涉及的主要对象是向MindNLP项目添加机器翻译功能和更新Multi30k数据集处理方法。,https://github.com/mindspore-lab/mindnlp/issues/110
mindnlp,这是一个用户提出的功能需求。主要目的是为了在mindnlp的load函数中添加能够使用npy文件加载embedding的功能。,https://github.com/mindspore-lab/mindnlp/issues/106
mindnlp,这是一种功能需求问题，涉及到嵌入表加载逻辑，导致无法使用特定的npy类型文件。,https://github.com/mindspore-lab/mindnlp/issues/104
mindnlp,这是一个用户提出需求的issue，主要涉及到更新openi仓库的URL链接。,https://github.com/mindspore-lab/mindnlp/issues/102
mindnlp,这是一个需要添加IMDB数据集处理流程的功能需求，涉及主要对象为Mindnlp。由于现有处理流程没有包含IMDB数据集，用户提出了需要新增该处理流程的需求。,https://github.com/mindspore-lab/mindnlp/issues/100
mindnlp,这是一个用户提出需求类型的issue，主要涉及的对象是mindnlp库中的trainer.py文件。由于训练器（trainer）的注释（annotation）需更新，用户提出了这个问题来寻求帮助。,https://github.com/mindspore-lab/mindnlp/issues/99
mindnlp,这是一个需求提出类型的issue，主要对象是在mindnlp中增加CRF功能。可能由于用户希望在项目中使用CRF模型来提升自然语言处理性能而提出了此问题。,https://github.com/mindspore-lab/mindnlp/issues/94
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是添加在Windows下的单元测试和文档部分。,https://github.com/mindspore-lab/mindnlp/issues/92
mindnlp,这是一个用户提出需求的issue，主要对象是要求添加openi sync功能。可能是由于用户需要在该项目中实现实时同步功能，所以提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/91
mindnlp,该issue为用户需求类型，主要对象是测试框架pytest。这个问题由于缺少pytest标签导致用户希望补充该标签。,https://github.com/mindspore-lab/mindnlp/issues/88
mindnlp,该issue为需求升级文档类型，涉及主要对象为文档内容。,https://github.com/mindspore-lab/mindnlp/issues/86
mindnlp,该issue是关于更新下载和cnn_encoder的注释，属于文档更新类型，主要涉及到代码注释补充。,https://github.com/mindspore-lab/mindnlp/issues/80
mindnlp,这是一个功能需求，提出了关于数据集模块需要升级以实现自动删除下载文件的问题。,https://github.com/mindspore-lab/mindnlp/issues/78
mindnlp,该issue类型为用户提出需求，涉及主要对象是添加独立数据集的工作流。原因可能是用户希望能够方便地将独立数据集集成到项目中。,https://github.com/mindspore-lab/mindnlp/issues/77
mindnlp,这个issue是需求性质的，涉及到RNNDecoder的代码修改，用户要求将RNN替换为RNNCell。,https://github.com/mindspore-lab/mindnlp/issues/75
mindnlp,这是一个用户提出需求的issue，主要涉及文档（Docs）部分。,https://github.com/mindspore-lab/mindnlp/issues/68
mindnlp,这是一个功能需求类型的issue，主要涉及的对象是embedding module。可能是用户希望给embedding module新增保存和加载功能，以提高模型的持久化和复用性。,https://github.com/mindspore-lab/mindnlp/issues/67
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是text_classification datasets，用户提出了关于处理文本分类数据集的相关问题。,https://github.com/mindspore-lab/mindnlp/issues/65
mindnlp,这是一个功能需求类型的issue，该问题涉及主要对象是BasicTokenizer，由于缺乏Windows支持，用户提出需要为BasicTokenizer增加Windows支持。,https://github.com/mindspore-lab/mindnlp/issues/63
mindnlp,这个issue类型是用户提出需求，该问题单涉及的主要对象是添加自定义amp，由于可能用户需要对mindnlp进行定制化，导致提出了这个需求。,https://github.com/mindspore-lab/mindnlp/issues/60
mindnlp,这是一个用户提出需求的issue，主要涉及的对象是需要为mindnlp添加对mindspore 1.8.1的支持。可能是由于mindnlp目前不支持mindspore 1.8.1版本导致用户提出该需求。,https://github.com/mindspore-lab/mindnlp/issues/59
mindnlp,这是一个缺少具体信息的功能需求问题，主要涉及工具或软件的缺陷。,https://github.com/mindspore-lab/mindnlp/issues/56
mindnlp,这是一个用户提交需求的issue，主要对象是在机器翻译和文本生成中添加进程。可能是用户希望为这两个功能增加相关的处理流程。,https://github.com/mindspore-lab/mindnlp/issues/55
mindnlp,这个issue类型是用户提出需求，涉及的主要对象是向mindnlp仓库添加一些新的度量类。可能是用户希望在mindnlp中使用这些度量类来评估模型性能。,https://github.com/mindspore-lab/mindnlp/issues/48
mindnlp,"这是一个用户提出需求的issue，主要涉及添加Metric类（Perplexity, RougeN, RougeL, Distinct, Precision, Recall, Conf），用户希望这些Metric类能够被加入到项目中。",https://github.com/mindspore-lab/mindnlp/issues/47
mindnlp,这个issue类型是需求提出，主要涉及的对象是要在mindnlp中添加一些评估指标类，原因可能是为了增强模型评估的功能和可靠性。,https://github.com/mindspore-lab/mindnlp/issues/46
mindnlp,这是一个用户提出需求的issue，主要涉及更新训练器和评估器参数及函数名称，可能是因为需要统一命名或者提高代码的可读性。,https://github.com/mindspore-lab/mindnlp/issues/44
mindnlp,这是一个功能需求提议，主要涉及到添加各种度量指标类，如Perplexity、RougeN、RougeL、Distinct、Precision、Recall和Conf。,https://github.com/mindspore-lab/mindnlp/issues/43
mindnlp,这是一个需求类型的issue，主要涉及参数类型提示和代码注释的修改。可能是由于现有代码中的类型提示不清晰或者注释不准确而导致需要修改。,https://github.com/mindspore-lab/mindnlp/issues/42
mindnlp,这个issue类型是需求提出， 主要对象是trainer，由于使用时需要支持多个返回值的情况。,https://github.com/mindspore-lab/mindnlp/issues/41
mindnlp,这是一个需求提出类型的issue，主要涉及到三个数据集的处理过程。由于缺少具体描述，用户可能在处理数据集时遇到了问题，需要获取帮助或解决方案。,https://github.com/mindspore-lab/mindnlp/issues/39
mindnlp,这是一个用户提交的需求类型的issue，主要对象是项目中的README.md文件。这个问题可能是用户想要修改项目的说明文档，更新其中的内容或者格式。,https://github.com/mindspore-lab/mindnlp/issues/38
mindnlp,这个issue类型是用户提出的需求，主要涉及的对象是代码中的数据处理流程。由于可能需要针对特殊数据集进行不同处理，用户在提出了对通用处理逻辑使用方法的建议。,https://github.com/mindspore-lab/mindnlp/issues/37
mindnlp,这个issue是一个测试相关的需求，主要涉及的对象是数据集的单元测试。,https://github.com/mindspore-lab/mindnlp/issues/33
mindnlp,这个issue类型为功能需求提出，主要对象是Seq2Seq模型。由于旧的初始化格式不符合需求，用户提出升级初始化格式的请求。,https://github.com/mindspore-lab/mindnlp/issues/32
mindnlp,这是一个用户提出需求的issue，主要涉及增加新的数据集到mindnlp中。,https://github.com/mindspore-lab/mindnlp/issues/31
mindnlp,这是一个需求提出类型的Issue，主要涉及了添加YelpReviewPolarity、YelpReviewFull和YahooAnswers。其原因可能是为了扩展数据集或增加模型训练的多样性。,https://github.com/mindspore-lab/mindnlp/issues/30
mindnlp,这是一个用户提出需求的issue，主要涉及数据集的添加，可能是为了增加模型训练的多样性。,https://github.com/mindspore-lab/mindnlp/issues/29
mindnlp,"这个issue是一个类型为用户提出需求的问题，涉及的主要对象是在mindnlp中增加AmazonReviewFull, AmazonReviewPolarity, STSB, DBpedia数据集。",https://github.com/mindspore-lab/mindnlp/issues/27
mindnlp,这个issue是一个需求类型，主要对象是数据集。由于需要将代理添加到所有现有数据集，用户希望为数据集添加代理的功能或支持。,https://github.com/mindspore-lab/mindnlp/issues/26
mindnlp,这是一个需求类型的issue，主要对象是将Mindnlp中的pipeline移动到工作流中。,https://github.com/mindspore-lab/mindnlp/issues/25
mindnlp,这是一个需求提出类型的issue，主要涉及的对象是提取通用工具。由于缺乏通用工具的支持，用户提出了需要提取常用工具的需求。,https://github.com/mindspore-lab/mindnlp/issues/24
mindnlp,这是一个需求类型的issue，主要涉及对象是mindnlp库的运行模式。由于需要将运行模式修改为jit，用户提出了此需求。,https://github.com/mindspore-lab/mindnlp/issues/23
mindnlp,这是一个请求移动脚本的问题，涉及对象是脚本文件。由于未提供具体内容，无法确定具体原因。,https://github.com/mindspore-lab/mindnlp/issues/22
mindnlp,这是一个用户提出需求的类型，该问题单涉及的主要对象是在Mindnlp中添加代理的功能。,https://github.com/mindspore-lab/mindnlp/issues/21
mindnlp,这个issue是一个功能增强需求，主要对象是测试文件。由于缺少Pylint检查，导致测试文件可能存在代码质量问题。,https://github.com/mindspore-lab/mindnlp/issues/20
mindnlp,这是一个用户提出需求的issue，主要涉及下载和AG_NEWS功能，用户希望添加代理支持。,https://github.com/mindspore-lab/mindnlp/issues/18
mindnlp,这是一个功能需求的issue，主要涉及的对象是添加SST2数据集和相关的单元测试。,https://github.com/mindspore-lab/mindnlp/issues/17
mindnlp,该issue是一个功能请求，主要涉及添加四个数据集和修复download.py文件。,https://github.com/mindspore-lab/mindnlp/issues/16
mindnlp,这是一个用户提出需求的issue，主要对象是向项目中添加CoLA数据集和进行单元测试。,https://github.com/mindspore-lab/mindnlp/issues/15
mindnlp,这是一个需求类型的issue，主要涉及的对象是增加CoLA数据集和添加ut test_cola测试。,https://github.com/mindspore-lab/mindnlp/issues/14
mindnlp,这是一个用户提出需求的issue，主要涉及添加ag_news数据集的处理。产生这个需求的原因可能是用户希望在mindnlp中增加对该数据集的支持。,https://github.com/mindspore-lab/mindnlp/issues/13
mindnlp,此issue类型为需求提出，主要对象是添加情感分类功能。这个问题的提出可能是为了增强mindnlp的功能和实用性。,https://github.com/mindspore-lab/mindnlp/issues/12
mindnlp,这个issue是一个功能需求，该问题单涉及的主要对象是Mindnlp项目。由于缺少BleuScore功能，用户提出了需要添加BleuScore的需求。,https://github.com/mindspore-lab/mindnlp/issues/11
mindnlp,这个issue属于用户提出需求，请求添加BleuScore，主要涉及Mindnlp项目的代码。,https://github.com/mindspore-lab/mindnlp/issues/10
mindnlp,这是一个用户提出需求的类型，主要对象是项目中的CNNEncoder。这个需求是为了添加CNNEncoder，可能是为了提升项目的性能或功能。,https://github.com/mindspore-lab/mindnlp/issues/9
mindnlp,"这是一个用户提出需求的issue，主要对象是向mindnlp添加一个""map_rule""用于注册的功能。",https://github.com/mindspore-lab/mindnlp/issues/8
mindnlp,这是一个需求提出的issue， 主要涉及文件的重命名和测试文件的添加，由于需要统一命名规范或者补充测试部分，用户提出了这个问题。,https://github.com/mindspore-lab/mindnlp/issues/7
mindnlp,这个issue类型是需求提出，涉及的主要对象是注册功能。这个问题可能由于缺少注册功能导致用户无法创建账户或体验某些特定功能而提出需求。,https://github.com/mindspore-lab/mindnlp/issues/6
mindnlp,这是一个需求类型的issue，主要对象是对于mindnlp项目的multi30k数据集处理流程的新增需求。,https://github.com/mindspore-lab/mindnlp/issues/5
mindnlp,"该issue类型为用户提出需求，主要对象是名为""use action matrix""的功能。由于在内容中未提及具体的问题或需求，很难确定用户所关心的具体信息。",https://github.com/mindspore-lab/mindnlp/issues/4
mindnlp,这是一个用户提出需求类型的issue，主要对象是添加GitHub工作流。,https://github.com/mindspore-lab/mindnlp/issues/2
mindnlp,这是一个需求提出类型的issue，主要涉及到项目中的数据集结构变更和添加4个新的数据集。可能是由于项目需要新增数据集或者对数据集结构进行调整而提出的需求。,https://github.com/mindspore-lab/mindnlp/issues/1
MindSpeed,这个issue是用户提出的需求。主要涉及对象是MindSpeed下的Qwen2.5VL系列模型。用户希望增加LoRA支持，以便在显存较小时进行训练，同时希望支持全系列昇腾卡实现此功能。,https://gitee.com/ascend/MindSpeed/issues/IBZ414
MindSpeed,这是一个用户提出需求的类型问题，主要涉及MindSpeed下的MegatronCore的cpu offloading功能配置，用户希望了解如何在基于MindSpeed的训练中配置cpu offloading的功能。,https://gitee.com/ascend/MindSpeed/issues/IBY945
MindSpeed,这是一个用户提出需求的issue，主要涉及支持FSDP加速训练的问题。由于FSDP是基于pytorch原生提供的，应该直接支持，但需要配置参数。,https://gitee.com/ascend/MindSpeed/issues/IBX3GM
MindSpeed,这是一个用户需求问题，主要涉及到如何配置deepspeed zero+cpu并行的效果。问题可能是由于显存不足导致训练失败，用户希望通过数据并行策略来加速训练过程。,https://gitee.com/ascend/MindSpeed/issues/IBWXBX
MindSpeed,这是一个关于用户提出需求的问题，主要涉及对象是NPU 910A，用户想知道910A是否能使用该库的flashattention或flashattention_v2。,https://gitee.com/ascend/MindSpeed/issues/IBVEV6
MindSpeed,这是一个需求提出类型的issue，主要涉及的对象是MegatronLM模块。由于最新版本的MegatronLM引入了许多改变，导致用户提出需要将MegatronLM版本升级的需求。,https://gitee.com/ascend/MindSpeed/issues/IBI579
MindSpeed,这是一个用户提出需求和请教问题的类型的issue，主要涉及MindSpeed的MC2通算融合功能尚不支持的情况。,https://gitee.com/ascend/MindSpeed/issues/IBDZ87
MindSpeed,这是一个用户提出需求类型的issue，主要涉及MindSpeed和DeepSpeed两者的对比以及是否可以切换迁移。,https://gitee.com/ascend/MindSpeed/issues/IBD6YU
MindSpeed,这是一个用户提出需求的issue，主要涉及的对象是MindSpeed。由于用户希望知道MindSpeed是否支持zero3➕cpu offload，因此提出了这个问题。,https://gitee.com/ascend/MindSpeed/issues/IBC7ST
MindSpeed,这是一个用户提出需求的issue，主要涉及支持CosyVoice功能。原因是用户希望增加一些新的功能和价值到项目中。,https://gitee.com/ascend/MindSpeed/issues/IBBX7O
MindSpeed,这是一个关于功能需求的问题，主要涉及MindSpeed是否支持fp8训练，由于直接使用Megatron的fp8训练会导致对transformer engine的依赖性报错。,https://gitee.com/ascend/MindSpeed/issues/IB4M1F
MindSpeed,这是一个用户提出需求的issue，主要涉及的对象是PP自动并行训练功能以及MegatronLM中的core模块。由于用户想要使用PP自动并行模块来训练使用core模块构造的模型，所以提出了疑问是否支持这种组合。,https://gitee.com/ascend/MindSpeed/issues/IB16S7
MindSpeed,这是一个用户提出需求的issue，涉及主要对象是MindSpeed平台，因为用户想知道除了MegatronLM外是否可以支持其他训练框架，类似于deepspeed。,https://gitee.com/ascend/MindSpeed/issues/IAWZAK
MindSpeed,这是一个用户提出需求的issue，主要对象是llama factory和MindSpeed，用户询问是否可以使用MindSpeed进行训练，并请求给出一个操作实例。,https://gitee.com/ascend/MindSpeed/issues/IAWBA3
MindSpeed,这是一个用户提出需求的类型，该问题涉及MindSpeed Profiling工具的使用。由于未能在训练脚本中正确插入flag，导致只打开了MegatronLM原生的profiling，用户无法使用MindSpeed Profiling工具对程序进行调试和追踪。,https://gitee.com/ascend/MindSpeed/issues/IAUYKW
MindSpeed,这是一个功能优化类型的issue报告，主要涉及到MindSpeed项目中transformer模块中的dot_product_attention_forward_wrapper()函数，具体表现为attention_mask参数被函数内部生成的attention_mask覆盖，因此出现了冗余的情况，需要进行优化。,https://gitee.com/ascend/MindSpeed/issues/IAOEF8
MindSpeed,这个issue是关于用户提出需求的问题，涉及主要对象是使用MindSpeed进行类似vllm的推理，用户寻求关于如何在长上下文推理任务中使用MindSpeed的帮助。,https://gitee.com/ascend/MindSpeed/issues/IABY28
MindSpeed,这是一个用户提出需求的issue，主要涉及的对象是VocabParallelCrossEntropy算子。由于用户希望了解后续是否会开发融合算子，因此提出了这个问题。,https://gitee.com/ascend/MindSpeed/issues/IAABR2
MindSpeed,这个issue是一个功能需求类型，主要涉及的对象是支持Megatron Core 0.7.0，用户询问何时能够支持该版本。,https://gitee.com/ascend/MindSpeed/issues/IA9V1V
MindSpeed,这是一个用户提出需求的类型的issue，主要涉及的对象是transformer engine依赖。用户提出问题是因为安装transformer engine的依赖较为繁琐，希望寻求比较简单的运行方式。,https://gitee.com/ascend/MindSpeed/issues/IA6J71
MindSpeed,这是一个用户提出需求的issue，主要涉及对AscendSpeed项目加入混合序列并行功能时是否引用了feifeibear/longcontextattention项目的内容。原因可能是作者希望在文档或代码中看到相应的引用和致谢。,https://gitee.com/ascend/MindSpeed/issues/I9S8QK
MindSpeed,该issue类型为用户提出需求，询问如何在昇腾上将大模型转为onnx并进行推理，主要涉及对象是在MindSpeed下寻找相关仓库实现。,https://gitee.com/ascend/MindSpeed/issues/I9EX63
MindSpeed,这是一个用户提出需求的issue，主要涉及MindSpeed的发布时间问题，可能是由于想了解项目发布进度而产生的。,https://gitee.com/ascend/MindSpeed/issues/I8X9ZM
mindformers,这是一个用户提出需求的issue，主要对象是deepseekv3模型门禁用例补充。由于功能需求变更或者初期设计不完善，导致用户提出补充门禁用例的请求。,https://gitee.com/mindspore/mindformers/issues/IC2YD7
mindformers,这是一个用户提出需求的issue，主要涉及的对象是MindIE+MindSpore Transformers模型，用户希望增加函数调用支持以提升模型的实用性和交互性。,https://gitee.com/mindspore/mindformers/issues/IC2QL4
mindformers,这个issue是一个需求类型，涉及的主要对象是moev3模块。由于缺乏版本校验，可能导致程序无法正确识别模块版本信息，需要增加版本校验功能来解决这一问题。,https://gitee.com/mindspore/mindformers/issues/IC20VI
mindformers,这是一个用户提出需求的issue，主要对象涉及到seqpipe。原因可能是当前用例优先级不符合预期，用户希望将其调整为level0。,https://gitee.com/mindspore/mindformers/issues/IC1X7N
mindformers,这是一个用户提出需求的issue，主要对象是deepseekv3文档中的环境搭建篇章。由于篇章冗余，用户担心与其他模型的环境搭建不一致，希望文档能统一安装流程。,https://gitee.com/mindspore/mindformers/issues/IC1PTK
mindformers,这个issue类型为需求提出，主要对象是mindformers仓库中的mindspore门禁版本，由于当前版本0322需要更新到0404版本。,https://gitee.com/mindspore/mindformers/issues/IBZ471
mindformers,这个issue是用户提出需求，询问有关内部接口调用方式的问题，主要涉及mindspore库的内部接口调用，用户由于缺乏对应的使用文档或开放接口，想获取相关信息。,https://gitee.com/mindspore/mindformers/issues/IBWJKE
mindformers,这个issue属于需求提出类型，主要对象是mindformers的代码。该问题出现的原因是缺少用例看护，导致代码覆盖率较低，需要增加用例看护。,https://gitee.com/mindspore/mindformers/issues/IBVKDR
mindformers,这个issue类型是建议完善文档，涉及的主要对象是kv_channels参数的使用。原因是该参数在官方文档中没有解释，导致用户可能会感到困惑和误导。,https://gitee.com/mindspore/mindformers/issues/IBUFFQ
mindformers,这是一个用户提出需求的issue，主要涉及到加载HuggingFace权重时的路径配置问题，导致需要二次修改yaml文件的情况。,https://gitee.com/mindspore/mindformers/issues/IBT6K1
mindformers,这是一个用户提出需求的issue，主要对象是指标监控功能的监控频率参数设置，由于监控频率参数需要手动添加在callbacks配置项中，与统一的指标监控配置项分离，用户使用不便。,https://gitee.com/mindspore/mindformers/issues/IBT0EM
mindformers,这是一个需求更改的类型，该问题单涉及的主要对象是1.5.0版本的模型、代码和文档，由于评审结论的决定，需要废弃这些内容。,https://gitee.com/mindspore/mindformers/issues/IBSU2T
mindformers,这是一个用户提出需求的issue，主要涉及swap策略调整和警告提示功能。原因是当前swap默认策略和优先级设置不够符合需求，同时缺少重复使能的警告提示。,https://gitee.com/mindspore/mindformers/issues/IBS8EU
mindformers,这是一个用户提出需求的issue，主要对象是CogVLM2Image，由于缺乏训练微调的示例指导，客户需要补充这部分信息。,https://gitee.com/mindspore/mindformers/issues/IBS7EO
mindformers,这个issue类型是用户提出需求，主要涉及的对象是CogVLM2Video模型。由于缺少多机多卡的微调指导，客户无法评估CogVLM2Video多机多卡微调需要消耗的电力数据，需要更完整的指导。,https://gitee.com/mindspore/mindformers/issues/IBS785
mindformers,"这是一个用户提需求的issue，主要对象是有关""cogvlm2_image""模型在910B上是否支持训练的问题，用户想知道后续是否会支持训练，并寻求具体的时间节点。",https://gitee.com/mindspore/mindformers/issues/IBR7YU
mindformers,这是一个用户需求提出的类型，主要对象是权重保存及加载功能，由于在推理场景中不需要加载优化器权重，用户希望在合并权重时能够选择性过滤部分权重参数。,https://gitee.com/mindspore/mindformers/issues/IBQIQI
mindformers,这个issue是关于需求的，主要涉及的对象是deepseek2模型，用户提出需要为该模型编写UT用例以保证其可用性。,https://gitee.com/mindspore/mindformers/issues/IBNT2Y
mindformers,这是一个功能需求类型的 issue，涉及到软件文档的更新，主要对象是用户。由于缺乏驱动固件版本的提示，导致用户使用功能遇到依赖版本不匹配的问题。,https://gitee.com/mindspore/mindformers/issues/IBNO08
mindformers,这是一个需求类型的issue，主要涉及到deepseek3的UT用例看护可用性。由于缺乏UT用例看护，可能导致测试覆盖不全面或者无法准确评估功能可用性。,https://gitee.com/mindspore/mindformers/issues/IBMSHY
mindformers,这是一个用户提出需求的Issue，主要涉及大规模模型读取时缺乏读取进度条显示，导致使用体验不佳。,https://gitee.com/mindspore/mindformers/issues/IBMK9H
mindformers,这是一个用户提出需求的issue，主要涉及神经网络模型训练中aux loss计算过程中的输入归一化问题。原因可能是深度神经网络在sigmoid激活函数下的收敛困难导致。,https://gitee.com/mindspore/mindformers/issues/IBMDKC
mindformers,这个issue类型是功能需求，主要涉及到深度学习模型中权重初始化的标准差配置，用户希望增加初始化权重标准差的配置，并修改默认值为0.006，以优化模型表现。,https://gitee.com/mindspore/mindformers/issues/IBM8XW
mindformers,这是一个需求提交，主要对象是deepseek3权重转换脚本。,https://gitee.com/mindspore/mindformers/issues/IBLIR8
mindformers,该issue为需求提出类型，主要对象是deepseek微调相关文档上库，由于文档上库存在问题，需要进行相关文档的更新。,https://gitee.com/mindspore/mindformers/issues/IBLBMC
mindformers,这是一个需求提出类型的issue，主要对象是deepseek3数据集预处理脚本。由于缺少具体的内容描述，无法分析导致的bug或问题原因。,https://gitee.com/mindspore/mindformers/issues/IBLBM2
mindformers,这是一个用户提出需求的类型，主要对象是基于昇腾AI硬件与昇思MindSpore AI框架的DeepSeekV3。由于希望开发者体验和了解DeepSeekV3的预训练和推理能力，发帖者提供了相应的训练和推理手把手教程，并欢迎大家进行交流和探讨。,https://gitee.com/mindspore/mindformers/issues/IBL0X5
mindformers,该问题类型为用户提出需求，询问Altas 300 Duo I 推理卡是否支持Qwen2.5和DeepSeekR1的时间计划。这是因为用户关注设备支持情况，希望获得相关信息。,https://gitee.com/mindspore/mindformers/issues/IBKR91
mindformers,这是一个用户提出需求的issue，主要涉及的对象是safetensor分布式加载策略。用户希望在safetensors分布式加载过程中不需要执行unify操作，以提高效率和用户体验。,https://gitee.com/mindspore/mindformers/issues/IBJTJK
mindformers,"这个issue是用户提出的需求，主要涉及增加统计性能时延的工具，其中记录了各个`batch_size`, `in_seq_length`, `out_seq_length`的prepare/predict/post阶段的时延。",https://gitee.com/mindspore/mindformers/issues/IBJKTT
mindformers,该issue类型为用户提出需求，主要涉及对象是Qwen2 VL模型，用户想了解是否可以使用Qwenvl的训练脚本来训练Qwen2 VL模型。,https://gitee.com/mindspore/mindformers/issues/IBJFCX
mindformers,这是一个关于修改推理profiler等级以获取aclnn算子信息的需求。问题涉及到mindspore的profiler等级变更，原因是需要使用Level1才能获取到aclnn算子信息。,https://gitee.com/mindspore/mindformers/issues/IBIAHT
mindformers,该issue为用户提出需求类型，主要涉及qwen2微调训练如何指定卡的问题，由于启动脚本和参数设置不明确，导致用户无法成功指定多张卡进行微调训练。,https://gitee.com/mindspore/mindformers/issues/IBI99E
mindformers,这是一个用户提出需求的问题，主要涉及mindformers是否支持商用的并发推理，用户想了解现在是否支持并发请求。,https://gitee.com/mindspore/mindformers/issues/IBHVEG
mindformers,这个issue类型为需求提出，涉及主要对象为新增ring attention负载均衡支持，用户提出了对该功能的需要。,https://gitee.com/mindspore/mindformers/issues/IBGX94
mindformers,这个issue是用户提出的需求，要求在Qwen2.5文档中新增对transformer版本描述，导致了转换权重操作无法顺利进行。,https://gitee.com/mindspore/mindformers/issues/IBFBAF
mindformers,这是一个优化需求，主要涉及到配置项`run_mode`的读取方式优化。由于目前`run_mode`配置项通过环境变量进行传递，但实际上已记录在全局变量`Context()`中，因此希望仅支持从`Context()`中获取`run_mode`，不再支持直接通过环境变量控制。,https://gitee.com/mindspore/mindformers/issues/IBENFO
mindformers,这是一个用户提出需求的issue，主要涉及llama3.1预训练文档缺失问题，用户想了解如何进行多机和单机训练，以及与llama3配置文件的关系，希望解决报错问题。,https://gitee.com/mindspore/mindformers/issues/IBDPT3
mindformers,这是一个用户询问支持某项功能的问题，主要对象是针对软件支持知识。,https://gitee.com/mindspore/mindformers/issues/IBD6YS
mindformers,这是一个用户提出需求的issue，主要对象是适配Telechat2系列模型。,https://gitee.com/mindspore/mindformers/issues/IBAHKY
mindformers,这个issue类型是功能需求，主要涉及的对象是AI修图业务交付项目。由于需求变更或技术实现问题，用户希望支持vit+internlm27b推理。,https://gitee.com/mindspore/mindformers/issues/IB8H7Z
mindformers,这是一个用户提出需求的issue，涉及主要对象是mindformers的dev分支下的数据处理脚本。该问题的产生可能是因为在qwen2.5 research下面缺少相应的数据处理脚本。,https://gitee.com/mindspore/mindformers/issues/IB71OI
mindformers,这是一个用户提出需求的issue，涉及主要对象是qwen1.572bchat的get_model和get_tokenizer函数，由于官方目前只提供了baichuan213b而没有qwen系列，用户在封装chat_web时遇到了填写get_model和get_tokenizer函数的问题。,https://gitee.com/mindspore/mindformers/issues/IB70YV
mindformers,这是用户提出的需求。该问题单涉及的主要对象是在mindformers项目中实现的权重保存过滤功能。由于缺乏过滤指定关键词权重不保存的配置选项，用户希望通过yaml配置实现此功能。,https://gitee.com/mindspore/mindformers/issues/IB5F26
mindformers,这个issue类型为功能需求，主要涉及pet中存在的冗余代码不利于类方法扩展。,https://gitee.com/mindspore/mindformers/issues/IB5EJX
mindformers,这是一个用户提出需求的issue，涉及主要对象是加载datasets数据集时未设置环境变量导致报错，希望改进环境变量获取方式。,https://gitee.com/mindspore/mindformers/issues/IB5ED1
mindformers,这是一个关于功能兼容性问题的用户提问，涉及主要对象为mindformers1.2分支中的Qwen1.514b以及910A设备。由于用户想确认在910A设备上是否可以运行mindformers1.2分支的Qwen1.514b版本，可能由于平台兼容性或系统要求等原因导致此疑问。,https://gitee.com/mindspore/mindformers/issues/IB4G4Q
mindformers,这是一个用户提出需求的issue，主要涉及mindformers高版本对Altas800训练服务器的支持问题。由于最近的几个版本都没有写Altas800（9000/9010）的支持，用户想了解是否后续不支持该训练服务器了。,https://gitee.com/mindspore/mindformers/issues/IB3RNT
mindformers,这是一个用户提出需求的问题，涉及对象为LLAMA3中的lora训练配置，由于LLAMA3可能不支持lora训练导致训练报错，用户想知道什么时候会支持。,https://gitee.com/mindspore/mindformers/issues/IB1YJ2
mindformers,这是一个用户提出需求的issue，主要涉及到大模型训练配置中sink size参数需要改成1。用户反馈该需求是因为当sink size为2时，每两步才会打印一次loss，不方便用户比对精度。,https://gitee.com/mindspore/mindformers/issues/IB1WI5
mindformers,这是一个需求提出的issue，主要涉及mindformers1.0在推理时无法动态改变batch数量的问题，用户希望了解如何部署支持动态batch的解决方案。,https://gitee.com/mindspore/mindformers/issues/IAYSIN
mindformers,这个issue属于用户提出需求类型，主要对象是mindspore官方最高版本和依赖要求版本不匹配，可能由于版本更新引起的依赖问题。,https://gitee.com/mindspore/mindformers/issues/IAXG65
mindformers,这个issue属于用户提出需求类型，主要涉及了在Mixtral8x7B模型在alpaca_data数据集后微调后在WikiText2和SQuAD 1.1上的评测结果。,https://gitee.com/mindspore/mindformers/issues/IAX98T
mindformers,"这是一个用户提出需求的issue，主要涉及的对象是名为""glm3lora""的功能。由于该用户想要了解目前是否支持""glm3lora""的微调功能，希望得到相关帮助。",https://gitee.com/mindspore/mindformers/issues/IAUHC8
mindformers,这是一个用户提出需求的issue，主要对象是支持 Qwen2VL 72B 模型。用户希望在已支持 QWEN2 的基础上，额外支持 Qwen2VL 72B 的模型。,https://gitee.com/mindspore/mindformers/issues/IAU5AV
mindformers,这是一个用户提出需求或疑问的类型，该问题涉及docker配置环境中使用不同版本的cann。这可能是由于项目需要不同版本的cann导致的需求或疑问。,https://gitee.com/mindspore/mindformers/issues/IAT1EG
mindformers,这是一个用户提出需求的类型的issue，主要涉及的对象是在atlas800A2硬件上训练自定义数据集，用户想要了解是否可以通过lora方式实现。,https://gitee.com/mindspore/mindformers/issues/IASDMF
mindformers,这是一个用户提出需求的issue，主要涉及的对象是在训练日志中增加TFLOPs、每次迭代的数据加载时长、前反向计算时长、参数更新时长，以及checkpoint保存和加载的时长的打点信息。由于需要根据这些指标进行性能和稳定性测试，希望在Mindformers仓中加入对应的打点信息，以便进行准确的统计和交流。,https://gitee.com/mindspore/mindformers/issues/IAR9R8
mindformers,这个issue是用户提出需求类型的，主要涉及Modellink仓中训练日志中缺少关于TFLOPs、数据加载时长等信息的打点信息。,https://gitee.com/mindspore/mindformers/issues/IAR2BS
mindformers,这是一个关于用户提出需求的issue，主要对象是计划适配mt5模型。,https://gitee.com/mindspore/mindformers/issues/IAOBUN
mindformers,这是一个用户需求咨询类的issue，主要涉及的对象是mamba模型，用户在询问该模型是否已经适配以及寻求对标模型的推荐。,https://gitee.com/mindspore/mindformers/issues/IAMS70
mindformers,这是一个用户提出需求的issue，主要对象是适配llavanext110b或其他千亿级参数的多模态模型。用户提出了需要了解MindFormers是否有计划适配这些模型的问题。,https://gitee.com/mindspore/mindformers/issues/IAM2N2
mindformers,这是一个用户提出需求的issue，主要涉及mindformers如何将cpkt或者bin文件转成safetensors文件。由于缺乏转成safetensors的脚本工具，用户无法完成相应的文件转换操作。,https://gitee.com/mindspore/mindformers/issues/IAIRMF
mindformers,这是一个用户请求获取model与tokenizer的issue，涉及到Qwen2ForCausalLM模型和Qwen2Tokenizer。由于未能正确获取model与tokenizer，用户提出了获取这两个对象的问题。,https://gitee.com/mindspore/mindformers/issues/IAIDMA
mindformers,这个issue类型是用户需求，主要涉及的对象是部署名称为qwen2的72版本，用户想知道是否有其他人已经成功部署了这个版本。,https://gitee.com/mindspore/mindformers/issues/IAG5RN
mindformers,这是一个用户提出需求的issue，主要涉及对象是qwen2，用户在询问qwen2是否需要切分重权以及是否有相关的配置文件。,https://gitee.com/mindspore/mindformers/issues/IAFV17
mindformers,这是一个用户提出需求的issue，主要涉及的对象是适配qwen2。用户提出这个问题是因为希望用中国的计算卡跑qwen2的7b和72b，并表示对1.5B和0.5B的适配没有那么大需求。,https://gitee.com/mindspore/mindformers/issues/IAE5LC
mindformers,这是一个用户提出需求的类型，用户询问是否有计划适配cogvlm这个多模态模型。,https://gitee.com/mindspore/mindformers/issues/IACKM3
mindformers,这个issue类型是用户提出需求，主要对象是单卡推理和分布式推理的主要步骤和区别，用户提出了关于这两种推理方式的问题或寻求帮助。,https://gitee.com/mindspore/mindformers/issues/IA8ULX
mindformers,这是一个用户提出需求的类型issue，主要涉及将huggingface的torch模型转换成mindspore支持的模型。这个问题的原因是用户想要在mindspore中使用由huggingface训练的模型，需要进行模型格式的转换。,https://gitee.com/mindspore/mindformers/issues/IA8UB8
mindformers,这个issue类型是用户提出需求，问题主要涉及mindformers中如何实现RLHF微调，由于用户对于这一功能实现方式不清楚而提出了疑问。,https://gitee.com/mindspore/mindformers/issues/IA8UAJ
mindformers,这是一个用户提出需求的类型，主要对象是mindformers支持的数据集格式，用户想了解mindformers支持哪些数据集格式。,https://gitee.com/mindspore/mindformers/issues/IA8UA6
mindformers,这是一个用户提出需求的类型，用户想要了解如何区分configs中配置的yaml文件。这个问题的根本原因在于缺乏清晰的文件区分指南导致用户难以确定何时应该使用哪个yaml配置文件。,https://gitee.com/mindspore/mindformers/issues/IA8U3A
mindformers,这个issue类型是用户提出需求，询问910A是否能够运行qwen2.0 70b。【注意：根据提供的信息无法进一步分析原因和症状】,https://gitee.com/mindspore/mindformers/issues/IA8I58
mindformers,这是一个用户需求问题，涉及主要对象为在chatweb中添加qwen1.572B进行推理，由于需要在chat_web中进行推理千问1.5_72B，用户提出了如何实现chat_web的predict_process.py中的get_model和get_tokenizer的问题。,https://gitee.com/mindspore/mindformers/issues/IA88YU
mindformers,这个issue类型是性能优化，涉及的主要对象是mindformers中的模型训练过程。原因是修改`gradient_aggregation_group`参数后，训练速度显著提升，说明之前参数设置影响了训练效率。,https://gitee.com/mindspore/mindformers/issues/IA7PHK
mindformers,这是一个关于需求的问题单类型，主要对象为文档设计不合理，需要区分开llama和llama2的模型关联文件。原因可能是现有文档未清晰标明两者的区分导致混淆和困惑。,https://gitee.com/mindspore/mindformers/issues/IA5LI5
mindformers,这是一个用户提出需求的类型，主要涉及多卡推理的问题。原因可能是使用Python脚本调用多卡推理导致部分卡进程异常死亡。,https://gitee.com/mindspore/mindformers/issues/IA5HX9
mindformers,这是一个用户提出需求的问题单，主要对象是GLM4库，用户在询问GLM4适配的时间。,https://gitee.com/mindspore/mindformers/issues/I9W29F
mindformers,这是用户提出需求的类型，主要涉及到对于支持计划和在特定硬件上进行训练、微调、推理的问题。,https://gitee.com/mindspore/mindformers/issues/I9VVUX
mindformers,这个issue类型是用户提出需求，主要对象是ceval评测脚本，用户寻求chatglm36B和llama7B的ceval评测脚本。,https://gitee.com/mindspore/mindformers/issues/I9UHW2
mindformers,这是一个用户提出需求的issue，主要对象是支持多卡交互式推理。原因可能是因为当前无法使用10B以上的模型。,https://gitee.com/mindspore/mindformers/issues/I9RBX8
mindformers,这个issue属于用户提出需求类型，主要涉及mindformers是否有对internlm220B的支持计划。用户提出该问题可能是因为目前使用internlm20B没有继续与训练的方法。,https://gitee.com/mindspore/mindformers/issues/I9OFXW
mindformers,这是一个需求提出类型的issue，主要涉及mindformers模型转换为huggingface模型的功能。由于当前只支持单向转换，导致无法在推理阶段使用基于英伟达卡的huggingface模型做推理。,https://gitee.com/mindspore/mindformers/issues/I9O8JO
mindformers,这个issue类型是用户提出需求，请教问题。问题涉及的主要对象是mindspore以及mindformers仓库。由于较低版本的HDK在昇腾社区已下架无法下载，用户想了解是否能够正常使用23.0.3版本的HDK和8.0以上版本的CANN，以及何时能够配套。,https://gitee.com/mindspore/mindformers/issues/I9N0Y6
mindformers,这是一个用户提出需求的issue，主要涉及mindformers中数据集加载功能的问题，由于只能从OBS加载mindrecord导致用户需要费时转换大数据集成mindrecord，希望能添加直接加载数据集目录的功能。,https://gitee.com/mindspore/mindformers/issues/I9MJJR
mindformers,该issue是关于需求的问题，主要涉及qwen1.514B chat版的适配问题，用户询问何时适配。,https://gitee.com/mindspore/mindformers/issues/I9MD2S
mindformers,这是一个用户需求类型的问题单，主要涉及到mindformers中的logger工具以及如何在模型中添加时间信息。这个问题的原因是尝试使用logger.info导出时间信息时出错，用户想要在模型中添加时间戳来定位代码运行到哪个结构。,https://gitee.com/mindspore/mindformers/issues/I9KEEW
mindformers,这个issue类型是用户提出需求，主要涉及的对象是glm3库，用户提出了关于支持lora微调的需求。,https://gitee.com/mindspore/mindformers/issues/I9J8OP
mindformers,这是一个关于需求的问题，主要对象是mindformers软件中的并发推理功能。用户想了解mindformers是否支持并发推理以及相关组件信息。,https://gitee.com/mindspore/mindformers/issues/I9IXP8
mindformers,这是一个用户提出需求的类型的issue，主要涉及对象是telechat项目中的tokenizer.model文件。用户希望在README.md中提供tokenizer.model的下载链接。,https://gitee.com/mindspore/mindformers/issues/I9HZUP
mindformers,这是一个关于需求讨论的issue，主要涉及到视觉大模型vit和swin在使用自定义数据集标签时需要做怎样的修改。这个问题的提出可能是由于缺乏关于如何自定义数据集标签的文档或指导，导致用户不清楚需要修改哪些文件以及如何做出相应的更改。,https://gitee.com/mindspore/mindformers/issues/I9H6PG
mindformers,这是一个用户提出需求的issue，主要涉及到Mindformers项目的初学者使用问题。由于文档不清晰，导致用户对于模型加载、转换以及环境安装等方面产生困惑。,https://gitee.com/mindspore/mindformers/issues/I9H0PG
mindformers,这是用户提出需求的问题，主要涉及MindSpore模型中缺乏动态组网的相关说明和脚本，导致用户在使用动态组网时缺乏指导和支持。,https://gitee.com/mindspore/mindformers/issues/I9GXJ0
mindformers,这是一个用户提出需求的issue，主要对象是mindformers的gitee仓库，由于当前主页支持的大模型列表中缺少GLM3导致用户提出需求添加GLM3及其文档链接。,https://gitee.com/mindspore/mindformers/issues/I9GHXG
mindformers,这个issue类型为功能需求，主要涉及的对象是LoRa权重合并。由于模型微调需要将已训练的权重与新权重合并后再进行评估，用户在询问如何进行权重合并。,https://gitee.com/mindspore/mindformers/issues/I9G6WV
mindformers,这是一个用户提出需求的问题，主要涉及Dataset、Sampler以及DatasetHelper，由于采样顺序无法控制导致每个进程获得的input_ids不一致。,https://gitee.com/mindspore/mindformers/issues/I9E0P4
mindformers,这是一个需求问题，用户提出GLM2是否支持多轮对话的问题，以及关于文档生成不全的疑问。,https://gitee.com/mindspore/mindformers/issues/I99CCW
mindformers,这个issue是用户提出需求，主要涉及到如何将mindformers的日志输出到k8s的pod准输出打印，使用kubectl logs查看日志。,https://gitee.com/mindspore/mindformers/issues/I98VMF
mindformers,这是一个关于如何查看模型训练日志的需求问题，主要涉及到用户对于大模型训练日志查询的困惑。,https://gitee.com/mindspore/mindformers/issues/I98BAJ
mindformers,这是一个用户提出需求的问题，主要涉及web_chat中推理服务相关的适配问题。由于当前chatglm2不支持glm2_6b_lora和llama213b不支持llama2_13b_lora推理，导致用户无法使用微调后的权重进行推理，因此提出了如何自主适配的疑问。,https://gitee.com/mindspore/mindformers/issues/I97982
mindformers,这是一个用户提出需求的issue，主要涉及mindformers库中的Streamer接口，用户希望该接口支持自定义StoppingCriteria功能。,https://gitee.com/mindspore/mindformers/issues/I961GT
mindformers,这是一个需求询问类型的issue，涉及模型转换的问题，由于评估时参数配置不同于训练时，用户询问如何进行模型转换。,https://gitee.com/mindspore/mindformers/issues/I94WO6
mindformers,该issue属于用户提出需求类型，主要涉及对象为使用glm2作为网页前端和后端模型的桥连接。用户询问是否能通过glm2生成json文件，并希望实现类似告诉小爱设定闹钟的功能，但是以文本形式进行，希望得到大佬们的意见和建议。,https://gitee.com/mindspore/mindformers/issues/I945FY
mindformers,这是一个用户提出需求的issue，主要对象是Mindformer trainer功能，用户想了解其是否支持amp。,https://gitee.com/mindspore/mindformers/issues/I93E8Q
mindformers,这是一个关于软件镜像下载的需求提出类型的issue，主要涉及mindformerr1.0 x86官方镜像问题，由于缺乏x86版本的官方镜像，用户询问如何在x86机器上运行mindformerr1.0 lite推理的代码。,https://gitee.com/mindspore/mindformers/issues/I92PPZ
mindformers,这是一个用户提出需求类型的issue，主要涉及的对象是如何将pytorch版本的GPT-2使用的inputs_embeds转换成mindformers的GPT-2要求的input_ids，并将新的数据放入mindformers的GPT-2进行训练。,https://gitee.com/mindspore/mindformers/issues/I92AMA
mindformers,这是用户提出的关于需求的问题，涉及到MindFormers和Ascend Transformer Boost库之间的联系，询问是否会集成相关的量化推理支持。,https://gitee.com/mindspore/mindformers/issues/I927NI
mindformers,这是一个关于功能需求的issue，主要涉及mindformers对模型推理量化的支持问题，用户想知道这种方法是否支持分布式推理。由于mindformers目前是否支持模型推理的量化尚不明确，用户提出了此问题。,https://gitee.com/mindspore/mindformers/issues/I918CH
mindformers,这个issue属于需求提出类型，主要涉及Qwen72b多卡并行推理任务配置的上库时间，由于之前提及的上库时间不同步而提出希望同步的需求。,https://gitee.com/mindspore/mindformers/issues/I90O0U
mindformers,该issue是用户提出需求的类型，主要涉及多机多卡的分布式训练任务。用户询问如何在不使用万兆组网的情况下实现数据并行功能，希望通过千兆以太网传递数据，主要困扰在于卡间不直连的情况下如何处理传递流程。,https://gitee.com/mindspore/mindformers/issues/I90895
mindformers,这个issue类型是关于需求的问题，主要涉及的对象是atlas 800推理服务器（型号3000），用户想要知道是否支持部署mindformers用于完成微调的大模型部署。,https://gitee.com/mindspore/mindformers/issues/I8ZRU9
mindformers,这是一个用户提出需求的issue，主要涉及GLM3框架缺乏预训练功能。由于青岛AICC头部客户采购算力，需要进行GLM36b模型的增量预训练，因此提出了该需求。,https://gitee.com/mindspore/mindformers/issues/I8Z7EX
mindformers,这是一个用户提出需求的issue，主要涉及到mindformers/pipeline/text_generation_pipeline.py文件，由于可能存在逻辑不统一的问题导致用户提出需要统一写一个逻辑。,https://gitee.com/mindspore/mindformers/issues/I8YLMA
mindformers,该issue类型为用户提出需求，请教问题，主要涉及对象为在gitee上的mindformers下的mindspore serving。由于缺少大模型权重转换和配置文件写法等问题，用户寻求关于如何在mindspore serving中运行llama系、qwen、百川的指导。,https://gitee.com/mindspore/mindformers/issues/I8YKR1
mindformers,这是一个用户提出需求的问题单，涉及主要对象是项目代码中的codellama适配。用户询问是否可以参考llama的文档来进行codellama的训练、推理和部署操作。,https://gitee.com/mindspore/mindformers/issues/I8YGZP
mindformers,这是一个功能需求提出的issue，主要涉及GLM模型支持dp/mp并行推理的功能。可能由于当前版本尚不支持，导致用户提出该需求。,https://gitee.com/mindspore/mindformers/issues/I8Y5JF
mindformers,这是一个用户提出需求的issue，主要涉及单机四卡执行Baichuan27BChat单卡多轮对话推理时，控制chat多轮对话模型返回文本长度的参数。导致用户提出这个问题可能是长回答文本只返回部分结果。,https://gitee.com/mindspore/mindformers/issues/I8Y1IK
mindformers,该issue属于用户提出需求的类型，主要涉及对象是数据集转换方式。用户提出问题是如何将中英文PICO标注数据转换成可以进行baichuan27b全参微调的格式。,https://gitee.com/mindspore/mindformers/issues/I8Y0VU
mindformers,这是一个用户提出需求的issue，主要涉及对象是对华为300i系列DUO 96G卡的支持。造成这个issue的原因是用户想了解Mindformers和Lite是否计划支持该系列卡，以及如何在300i没有HCCS情况下进行分布式多卡推理。,https://gitee.com/mindspore/mindformers/issues/I8XF78
mindformers,这是一个关于技术功能需求的issue，主要涉及到wizardcoder通过chatweb接口只支持聊天模式而不支持FIM模式的问题。,https://gitee.com/mindspore/mindformers/issues/I8WZ1C
mindformers,这是一个用户提出需求和问题的类型，主要涉及如何启动多卡推理并构建服务的内容。原因可能是用户想了解如何使用多卡推理并将其作为服务的推理入口，以及如何处理固定的输入在每次运行时都传入pipeline的情况。,https://gitee.com/mindspore/mindformers/issues/I8WOZQ
mindformers,这是一个用户提出需求类型的问题，主要涉及Mindformers在处理多个MindRecord文件时是否能正确读取并训练的功能。由于用户希望能够让Mindformers正确识别和读取不同数据集分别生成的多个MindRecord文件，因此可能存在对MindRecord文件集成性的需求或者对训练数据的批量处理需求。,https://gitee.com/mindspore/mindformers/issues/I8UEUN
mindformers,这是一个功能需求的issue，主要涉及到模型推理性能问题，希望能够增加stop_words_ids参数以提前结束推理，减少推理时延。,https://gitee.com/mindspore/mindformers/issues/I8TV58
mindformers,该issue是关于用户提出需求的，主要对象是mindformers，用户想要自定义evaluation过程，但在readme部分没有找到相关内容，可能是因为缺乏相关文档或指导导致用户无法实现自定义evaluation的功能。,https://gitee.com/mindspore/mindformers/issues/I8TTD0
mindformers,这是一个需求类型的issue，主要涉及到希望适配大模型的openai API。由于openai_api.py在mindspore上无法运行，影响了昇腾服务器上的大模型私有化部署。,https://gitee.com/mindspore/mindformers/issues/I8TJ81
mindformers,这是一个用户需求问题，用户在寻求关于适配flash attention的docker镜像的帮助。,https://gitee.com/mindspore/mindformers/issues/I8RS5E
mindformers,这是一个用户提出需求的类型的issue，主要对象是流式推理代码，用户希望了解如何在异常时终止流式推理，是否有终止推理的接口。,https://gitee.com/mindspore/mindformers/issues/I8RCY9
mindformers,这是一个关于需求/问题探讨的issue，主要涉及mindformers套件在推理阶段出现重复内容推理的问题。可能是由于推理时的循环回答现象导致，用户希望找到强制禁止推理时重复的方法或特殊配置技巧。,https://gitee.com/mindspore/mindformers/issues/I8R3FR
mindformers,这是一个用户提出需求的问题，主要涉及Baichuan213BChat对话推理功能，用户想了解为什么仅支持单卡多轮对话推理以及是否可以支持多卡及何时支持。,https://gitee.com/mindspore/mindformers/issues/I8QMD9
mindformers,这是用户提出的需求，希望文档中解释配置文件中的参数 warmup_ratio。由于缺乏这一参数的解释，用户无法理解其含义。,https://gitee.com/mindspore/mindformers/issues/I8PVLL
mindformers,这是一个用户询问新功能使用文档的问题，涉及的主要对象是动态seq和动态batch功能。用户询问如何使用新特性llama双动态lite推理和KVCacheMgr增量推理接口，是因为缺乏更新后的使用文档。,https://gitee.com/mindspore/mindformers/issues/I8PB8P
mindformers,这是一个用户提出需求的类型，该问题单涉及的主要对象是baichuan2在训练作业中的训练参数配置。通过用户提问的方式，试图了解如何设置训练参数以进行训练。,https://gitee.com/mindspore/mindformers/issues/I8O0NS
mindformers,这是一个关于需求提出的issue，主要涉及的对象是开源MOE模型Mixtral8x7B，用户希望知道是否有适配该模型的计划。,https://gitee.com/mindspore/mindformers/issues/I8NTBM
mindformers,这是一个关于功能需求的issue，主要对象是mindformer，用户询问是否将来支持GPU环境以及可能的时间表。,https://gitee.com/mindspore/mindformers/issues/I8NDKO
mindformers,这个issue属于用户提出需求类型，主要对象涉及微调和合并llama27b模型的任务。原因是用户想要在run_distribute.sh微调任务结束后自动执行权重合并，需要找到判断微调任务是否结束的方法。,https://gitee.com/mindspore/mindformers/issues/I8NCHE
mindformers,这个issue属于用户提出需求类型，主要涉及对Chat Web中对自定义权重和分词器model的说明不清晰导致设置自定义checkpoint_download目录时无法找到tokenizer.model文件的问题。,https://gitee.com/mindspore/mindformers/issues/I8MS7Y
mindformers,这是一个关于代码优化的issue，主要对象是MSLITE推理。这个问题可能是由于性能优化需要，将gather操作移到最后的词表投影MatMul前。,https://gitee.com/mindspore/mindformers/issues/I8MAA7
mindformers,这个issue属于用户提出需求类型，主要涉及到支持GLIP网络的请求。,https://gitee.com/mindspore/mindformers/issues/I8LY0E
mindformers,这是一个关于性能问题的需求提出，主要涉及到推理加速方案不足，导致无法支持商用并发推理需求。,https://gitee.com/mindspore/mindformers/issues/I8LIGK
mindformers,该issue类型为用户提出需求，涉及主要对象为在modelarts环境中进行全量微调时的并行配置。由于硬件资源不匹配以及多机训练无法顺利运行，用户提出了配置建议和测试需求。,https://gitee.com/mindspore/mindformers/issues/I8LFFY
mindformers,这是一个用户提出需求的issue，主要涉及到mindformers中的yaml文件解释不够详细，导致使用上的困惑。,https://gitee.com/mindspore/mindformers/issues/I8KXLT
mindformers,该issue属于用户提出需求类型，主要涉及generate方法的添加校验flash attention开关，由于需求背景信息不完整导致的。,https://gitee.com/mindspore/mindformers/issues/I8KRM4
mindformers,该issue类型为用户提出需求，主要对象是适配Qwen 7b/14b模型预训练任务。由于缺乏适配计划，用户希望了解近期是否会实现此需求。,https://gitee.com/mindspore/mindformers/issues/I8KOCZ
mindformers,这是一个用户提出需求的issue，主要涉及LLAMA2文档中关于API使用的说明不够详细，需要补充说明在未联网环境下如何下载权重文件以及权重文件的放置位置。,https://gitee.com/mindspore/mindformers/issues/I8KEAL
mindformers,这是一个用户提出需求的issue，主要涉及lite推理在物理机上是否支持多卡，用户想要了解多卡支持是否可行。,https://gitee.com/mindspore/mindformers/issues/I8K7JY
mindformers,该issue为用户提出需求。主要对象是baichuan213B模型在910A服务器上的多卡推理部署问题。该问题由于目前baichuan213B在910A上仅支持多卡推理，但不支持交互，导致无法在910A服务器上部署。,https://gitee.com/mindspore/mindformers/issues/I8JKE2
mindformers,这是一个用户提出需求的 issue，主要涉及到 ChatGLM2 的量化推理配置文件缺失问题，用户寻求获取配置文件的帮助。,https://gitee.com/mindspore/mindformers/issues/I8JG42
mindformers,这是一个用户提出需求的问题，主要对象是Mindformers的模型和套件。用户询问有关Mindformers是否有支持多机多卡推理的模型和套件的相关信息。,https://gitee.com/mindspore/mindformers/issues/I8IQIN
mindformers,这是一个用户提出需求的issue，主要涉及日志打印级别的控制。由于目前日志打印级别无法通过参数控制，用户希望能够实现这一功能。,https://gitee.com/mindspore/mindformers/issues/I8HYH4
mindformers,这个issue属于用户提出需求的类型，主要涉及的对象是BERT模型的数据处理功能。原因可能是用户想了解BERT模型是否支持TFRECORD类型输入以及对于MINDRECORD等其他类型输入的支持情况。,https://gitee.com/mindspore/mindformers/issues/I8HUQI
mindformers,这是一个用户提出需求的类型，该问题涉及的主要对象是ChatGLM36b模型的支持和开源大模型的Serving支持。用户提出了关于支持特定模型和开源大模型Serving计划的问题。,https://gitee.com/mindspore/mindformers/issues/I8H5R4
mindformers,这个issue属于需求类问题，主要涉及对象是权重转换，由于使用baichuan27b进行lora微调后得到了分布式的权重，用户想知道如何将其转换为完整权重进行推理。,https://gitee.com/mindspore/mindformers/issues/I8H1V5
mindformers,这是一个用户提出需求的类型。该问题单涉及主要对象是Trainer高阶接口配置内容打印不一致的问题。原因是在评测和推理阶段打印的配置内容与实际输入不匹配，造成冗长且观察困难。,https://gitee.com/mindspore/mindformers/issues/I8GVCP
mindformers,这是一个用户提出需求的类型，主要涉及对象是训练模型所需的计算资源，用户关注在两台910A能够支持最大训练模型的大小和是否能支持34B模型。,https://gitee.com/mindspore/mindformers/issues/I8GQI9
mindformers,这是一个用户提出需求的issue，主要涉及Baichuan213BChat4bits。由于用户想要支持Baichuan213BChat4bits，因此提出了这个问题。,https://gitee.com/mindspore/mindformers/issues/I8GOSF
mindformers,这是一个用户提出需求的问题单，主要涉及大模型的API部署。用户询问是否支持在Baichuan2微调后提供API接口方式去访问模型，希望实现类似vllm上的web或API接口方式的部署。,https://gitee.com/mindspore/mindformers/issues/I8GMY8
mindformers,这是一个用户提出需求和问题的issue，主要涉及mindformers和昇腾310卡，由于昇腾310卡只支持正向推理，无法进行反向传播训练，导致用户想要将mindformers和chatglm3移植到310卡上时遇到困难。,https://gitee.com/mindspore/mindformers/issues/I8G0TS
mindformers,这是一个关于用户提出需求的issue，主要涉及到边微调边评估的功能。用户提出这个问题是因为开启了do_eval=True，但在任务状态为finetune时无法实时看到评估结果。,https://gitee.com/mindspore/mindformers/issues/I8FDVI
mindformers,这是一个关于功能支持的问题，主要涉及设备型号910ProB和Mindformers平台。由于设备型号和文档中支持的型号不匹配，导致在推理过程中报oom（内存不足）错误，用户想要确认Mindformers是否支持量化推理。,https://gitee.com/mindspore/mindformers/issues/I8FCAC
mindformers,这是一个功能需求类的issue，主要涉及MindFormers的各个功能特性不够解耦，需要保持核心代码稳定以实现不同特性之间的代码界限清晰和新增特性代码更加模块化。,https://gitee.com/mindspore/mindformers/issues/I8F81M
mindformers,这是一个用户提出需求的issue，主要对象是数据集加载功能。由于支持的数据集太少，并且无法在线直接加载，用户希望增强数据集加载功能。,https://gitee.com/mindspore/mindformers/issues/I8F7O8
mindformers,这是一个关于mindformers是否支持海光DCU部署的需求提出类型的issue，主要涉及对象是mindformers平台。由于用户可能需要在海光DCU环境中使用mindformers，因此提出了这个问题以了解是否支持相关部署。,https://gitee.com/mindspore/mindformers/issues/I8F7CS
mindformers,这是一个用户提出需求的issue，主要涉及Mindformers团队对310p推理卡在实现mindspore lite框架推理的适配问题。原因是用户想通过310p推理卡完成该工作，需要了解如何进行适配。,https://gitee.com/mindspore/mindformers/issues/I8E0I6
mindformers,这个issue为用户提出需求类型，主要涉及Embedding模型，由于用户关心在910b上能否运行Embedding模型，可能由于资源限制或者性能要求导致用户提出了这个问题。,https://gitee.com/mindspore/mindformers/issues/I8DMVN
mindformers,该issue类型为需求提出，主要对象是qwen14b，用户提出关于适配时间的问题。可能由于该项目尚未适配qwen14b导致用户询问适配时间。,https://gitee.com/mindspore/mindformers/issues/I8CLTD
mindformers,这个issue属于用户提出需求类型，涉及主要对象为使用mindspore实现语料向量化的功能。用户提出的问题是如何通过阅读mindspore的文档来实现与指定代码相同的功能。,https://gitee.com/mindspore/mindformers/issues/I8CHIW
mindformers,"这是一个用户提出需求的issue，主要对象是""自动设置hccl_connect_time""，导致这个问题可能是由于缺乏自动设置hccl_connect_time的功能而引起的。",https://gitee.com/mindspore/mindformers/issues/I8CFWV
mindformers,这是一个需求提出类型的issue，主要对象为MSLite优化中的Llama后处理gather入图功能。,https://gitee.com/mindspore/mindformers/issues/I8CAPI
mindformers,这是一个关于用户提出需求的issue，主要涉及mindformers中大模型部署方案的问题。用户询问是否有推荐的部署方案，以及如何在mindspore serving中部署类似llama模型的问题。,https://gitee.com/mindspore/mindformers/issues/I8BZCH
mindformers,这个issue类型为用户提出需求，主要涉及的对象是chatglm3，由于用户想知道是否支持chatglm3而提出了这个问题。,https://gitee.com/mindspore/mindformers/issues/I8BUL0
mindformers,这个issue属于优化需求，主要对象是当前注册机制。由于每次导入全量类导致导入速度慢且所有依赖库必须全部安装，导致了臃肿的问题。,https://gitee.com/mindspore/mindformers/issues/I8ATAV
mindformers,这是一个需求讨论类型的issue，主要涉及多卡推理下如何切分ckpt，用户希望了解如何在llama27b模型中进行模型切分。,https://gitee.com/mindspore/mindformers/issues/I8AJR7
mindformers,这是一个关于用户需求的issue，涉及主要对象是大模型多轮对话中的history构建，用户询问如何在模型推理中添加多轮对话的history。,https://gitee.com/mindspore/mindformers/issues/I8AHV4
mindformers,这是一个用户需求问题，涉及主要对象为分布式微调，用户想要在分布式微调的同时完成自动权重转换，但执行时出现报错。,https://gitee.com/mindspore/mindformers/issues/I89XNM
mindformers,该issue类型为用户提出需求，主要涉及对象是数据集格式。由于ChatGLM26B目前仅支持.json格式数据集，用户希望了解平台标准数据集格式对于csv或tsv的要求。,https://gitee.com/mindspore/mindformers/issues/I89N04
mindformers,这是一个文档更新类型的issue，主要涉及mslite推理910b推荐配置，提出了配置更新要求及相关说明。,https://gitee.com/mindspore/mindformers/issues/I89FQ8
mindformers,这是一个用户需求类的issue，主要涉及多卡推理支持交互式接口的实现问题，由于采用的模型需要多卡推理且无法同时拉起server服务，导致提供对外服务时存在问题。,https://gitee.com/mindspore/mindformers/issues/I89D5U
mindformers,这个issue类型是用户提出需求，涉及的主要对象是MindFormers的构建过程。由于无法方便地定位源码位置，测试人员提出了需求希望构建时能够附带commit id信息。,https://gitee.com/mindspore/mindformers/issues/I897WD
mindformers,这是一个需求提出类型的issue，主要涉及Mindformer项目是否会集成Vicuna算法。这个问题由用户对Mindformer在近期是否增加Vicuna算法的集成进行了询问。,https://gitee.com/mindspore/mindformers/issues/I895U5
mindformers,这是一个关于功能使用的问题，主要涉及ChatGLM26B单机多卡评估，由于对命令参数理解有误导致提出了关于单机多卡评估和权重文件合并的问题。,https://gitee.com/mindspore/mindformers/issues/I88YOB
mindformers,该issue类型是关于功能支持的询问，主要涉及对象是在nvidia jetson嵌入式系统上使用mindspore和mindformer。询问是由于用户想知道这两个平台是否支持在nvidia jetson上使用，以及如果不支持的话，是全部功能不支持还是部分功能不支持。,https://gitee.com/mindspore/mindformers/issues/I88S4Z
mindformers,这是一个用户提出需求的类型，主要涉及对象是保存检查点（checkpoint）时缺少hasattr函数。,https://gitee.com/mindspore/mindformers/issues/I88ACJ
mindformers,这是一个关于需求的问题，涉及主要对象是脚本运行和参数设定，用户想要知道如何在运行脚本时加入device_num参数。,https://gitee.com/mindspore/mindformers/issues/I8857D
mindformers,这是一个用户提出需求的issue，主要涉及mindformers在GPU上的支持问题，用户想了解是否可以在GPU上实现llama训练及部署。,https://gitee.com/mindspore/mindformers/issues/I86C3R
mindformers,这是一个用户提出需求的类型issue，主要对象是华为mindspore是否支持英伟达GPU部署llama2_7B，由于目前华为mindspore的教程只提供了Ascend 910A部署llama2的方法，用户希望得知能否使用英伟达GPU进行部署。,https://gitee.com/mindspore/mindformers/issues/I85LHP
mindformers,这个issue类型是需求提出，主要涉及Mindformer对310P卡推理的支持情况。原因可能是用户想了解Mindformer何时开始支持310P卡的推理功能。,https://gitee.com/mindspore/mindformers/issues/I84T9H
mindformers,该issue是一个功能需求提出，涉及的主要对象是程序中的规则校验操作，由于当前流水线并行切分数目大于模型层数时报错不明显，需要增加校验功能来抛出错误。,https://gitee.com/mindspore/mindformers/issues/I84PII
mindformers,这是一个用户提出需求的issue，主要涉及到模型文档缺失分布式权重加载说明，导致用户无法进行模型训练。,https://gitee.com/mindspore/mindformers/issues/I84NO9
mindformers,这是一个用户需求的issue，主要涉及集群支持NFS网络存储模式下因手动生成策略文件、手动切分权重和加载权重微调步骤繁琐导致操作不友好的问题。,https://gitee.com/mindspore/mindformers/issues/I84JRI
mindformers,这是一个用户提出需求的issue，主要对象是model cards，由于需要支持lite推理，所以提出了这个问题。,https://gitee.com/mindspore/mindformers/issues/I84HMQ
mindformers,这个issue类型为用户提出需求，主要涉及对象是模型适配问题。由于用户想了解哪些模型适配了910B，导致提出这个问题。,https://gitee.com/mindspore/mindformers/issues/I84GAM
mindformers,这是一个用户提出需求的类型，主要涉及LLama支持动态shape及动态分档lite推理的问题。由于原因未提及，用户可能提出了关于新功能实现的问题或寻求相关帮助。,https://gitee.com/mindspore/mindformers/issues/I849UX
mindformers,这是一个用户提出需求的issue，主要涉及到Xverse13B推理适配需求。由于客户业务需要卡不够用且希望转向信创用国产化，所以需要进行昇腾适配，目标是拿下客户2000卡空间。,https://gitee.com/mindspore/mindformers/issues/I83QZD
mindformers,这是一个用户提出需求的类型，主要涉及mindformers适配xverse13b推理，由于需求适配FasterTransformer进行推理以及处理xverse模型的参数差异等限制导致。,https://gitee.com/mindspore/mindformers/issues/I83HY9
mindformers,这是一个关于API文档整改规则的问题。类型是用户提出需求，并主要涉及文档内容的统一规范。由于文档内容不规范，可能导致用户阅读困难或者理解不清晰。,https://gitee.com/mindspore/mindformers/issues/I83A16
mindformers,这是一个用户在提出需求的类型的issue，主要涉及支持llama34B的计划或codellama34B。用户可能由于缺乏支持或信息，提出该需求以获取相关帮助。,https://gitee.com/mindspore/mindformers/issues/I830E7
mindformers,这个issue是关于文档规范的需求，主要对象是模型卡片（model cards）。由于缺乏统一的文档规范和模板，需要制定规范并提供模板以便更好地管理模型文档内容。,https://gitee.com/mindspore/mindformers/issues/I830A6
mindformers,这个issue是一个用户提出的需求，主要涉及套件易用性的提升，由于当前参数设置存在限制，导致在不同运行模式下会产生报错，需增加前序校验来解决这个问题。,https://gitee.com/mindspore/mindformers/issues/I82QAD
mindformers,这是一个用户提出需求的问题，主要涉及对象是mindrecord文件。由于数据集在已有mindrecord文件基础上做了新增，用户希望了解如何在不重新生成mindrecord的情况下增量添加新的数据。,https://gitee.com/mindspore/mindformers/issues/I824ZS
mindformers,这个issue类型为需求提出，主要涉及自己数据集格式转换问题，由于数据格式与大模型的输入格式不匹配导致需要微调和评估的困扰。,https://gitee.com/mindspore/mindformers/issues/I81N6G
mindformers,该issue为用户提出需求类型的问题单，涉及主要对象为Llama2和codellama结构，用户想了解Llama2是否支持codellama结构。,https://gitee.com/mindspore/mindformers/issues/I81ERG
mindformers,这个issue是关于需求的，主要对象是在已训练bert模型迁移工作中需要yaml和json配置项的对应。可能由于参数在torch的json文件和mindspore中yaml文件的命名方式不同，导致需要整合为对应的yaml文件或文档。,https://gitee.com/mindspore/mindformers/issues/I8153B
mindformers,这是用户提出关于Mindpet套件中微调算法使用及如何进一步优化模型的问题。,https://gitee.com/mindspore/mindformers/issues/I80WTK
mindformers,这个issue属于用户提出需求，主要对象是模型训练过程中的数据集。导致问题的原因是用户可能未清楚需要手动添加标签，导致微调效果差或浪费资源。,https://gitee.com/mindspore/mindformers/issues/I80WD6
mindformers,这是一个需求提出的issue，主要对象是LoRA微调功能，用户提出的问题是希望优化存储ckpt时仅存可训练参数的功能，以及加载时通过yaml文件配置加载原始权重和lora可训权重，原因可能是为了减少磁盘占用空间和提升使用方便性。,https://gitee.com/mindspore/mindformers/issues/I80R8V
mindformers,这是一个用户需求问题，涉及到GLM26B模型的分布式评估和推理路径选择问题。由于无法选择分布式全量微调后合并的merge.ckpt文件路径和评估数据集路径，用户无法按需执行评估和推理。,https://gitee.com/mindspore/mindformers/issues/I80F1W
mindformers,这个issue属于用户提出需求类型，主要涉及的对象是VideoLLaMA软件的适配计划。用户提出这个问题是想了解VideoLLaMA是否有适配的计划。,https://gitee.com/mindspore/mindformers/issues/I80BJX
mindformers,这是一个关于Baichuan13B支持的需求问题，该问题涉及的主要对象是Baichuan13B。,https://gitee.com/mindspore/mindformers/issues/I7WESV
mindformers,这是一个需求提交类型的 issue，主要涉及 GLM2 适配 lite 推理，用户反馈需要相关适配的功能。,https://gitee.com/mindspore/mindformers/issues/I7WERQ
mindformers,这是一个用户提出需求的类型，主要涉及昇腾训练好的模型能否部署在GPU上以及提供权重转换脚本的问题。该问题由于算力成本高、昇腾模型无法在GPU上部署的困扰，用户希望寻求昇腾模型在GPU平台上部署的解决方案。,https://gitee.com/mindspore/mindformers/issues/I7W5HY
mindformers,这是一个需求提出类型的issue，主要涉及盘古模型处理数据集脚本无法处理非标注文本的问题，可能是由于文档描述与实际脚本需求不一致导致。,https://gitee.com/mindspore/mindformers/issues/I7VPVB
mindformers,这是一个用户提出需求的issue，主要涉及mindformers单卡训练脚本的使用方式是否更合理的讨论，原因是README中介绍的使用方式与仓库中实际脚本的命名不一致，导致用户疑惑。,https://gitee.com/mindspore/mindformers/issues/I7VHHL
mindformers,该issue类型为需求提出，主要涉及对象为BLIP2的二阶段支持。用户询问什么时候会支持BLIP2的二阶段可能是因为用户希望了解项目的发展规划和进度。,https://gitee.com/mindspore/mindformers/issues/I7V28J
mindformers,这是一个用户提出需求的issue，主要涉及pipeline接口加载指定权重的功能。用户希望能够在使用pipeline接口时指定加载特定的权重，由于当前功能不支持导致用户提出需求。,https://gitee.com/mindspore/mindformers/issues/I7UVFS
mindformers,这是一个用户提出需求的类型，主要涉及对象为模型mae，用户想了解mae是否仅支持vitbase，是否可以支持更大参数量的模型（如vitlarge或vithuge），原因可能是用户希望在使用mae时能够选择更大的参数量模型。,https://gitee.com/mindspore/mindformers/issues/I7USPX
mindformers,这个issue类型为用户提出需求，主要对象是权重合并功能。这个问题可能源于用户希望在多线程或小内存设备上进行权重合并，但目前系统可能尚不支持此功能。,https://gitee.com/mindspore/mindformers/issues/I7U5JV
mindformers,这是一个用户提出需求的issue，主要涉及到需要增加flanT5模型的增量适配，由于游戏npc场景需要支持，估计用卡数在几百卡。,https://gitee.com/mindspore/mindformers/issues/I7TQN1
mindformers,这是一个用户提出需求的 issue，主要涉及base_model中load_checkpoint方法的功能改进，由于传入参数限制导致子模型权重未正确加载。,https://gitee.com/mindspore/mindformers/issues/I7TPE0
mindformers,这是一个需求问题，用户在使用ModelArts上预训练MAEBASE模型时遇到需要长时间运行的情况，询问是否还需要额外配置。,https://gitee.com/mindspore/mindformers/issues/I7SA85
mindformers,这个issue是关于版本管理的需求问题，涉及主要对象为mindformers的ms版本管理，由于版本散落在各个代码中，导致难以维护。,https://gitee.com/mindspore/mindformers/issues/I7PUN4
mindformers,这个issue是用户提出的需求类型，主要涉及到在模型文件的construct中打印信息的问题。由于直接print无法打印信息，用户试图添加打印代码进行调试。,https://gitee.com/mindspore/mindformers/issues/I7PIID
mindformers,这是一个用户需求提出类的issue，主要涉及的对象是MindFormers团队，由于需要增加对StarCoder模型的支持而提出。,https://gitee.com/mindspore/mindformers/issues/I7OVVI
mindformers,该issue类型为用户提出需求，关注于昇腾910上自己创建数据集做Lora微调训练所需数据量和训练轮数。,https://gitee.com/mindspore/mindformers/issues/I7ONL2
mindformers,这是一个用户提出需求的issue，主要涉及到Pipeline对外API的功能不完善。由于版权问题，用户无法将第三方模型上传至obs，希望能够支持对本地repo中的权重和模型配置进行加载和推理。,https://gitee.com/mindspore/mindformers/issues/I7OMTY
mindformers,这是一个建议修改类型的issue，涉及主要对象为config文件中的batch size参数。原因是config中存在多个batch size可能导致使用时的混淆。,https://gitee.com/mindspore/mindformers/issues/I7OLJE
mindformers,该issue属于用户提出需求类型，主要涉及并行推理需求，由于需要支持来自不同线程的输入并并发返回结果给相应线程，用户提出该需求。,https://gitee.com/mindspore/mindformers/issues/I7O8ZP
mindformers,该issue属于用户提出需求类型，主要对象是Mindformers项目。由于用户希望Mindformers可以支持cv图像重建大模型，可能是由于当前该功能在项目中尚未被支持所致。,https://gitee.com/mindspore/mindformers/issues/I7NKCH
mindformers,这个issue是一个需求提出类型，主要涉及到缺少RLHF例子的问题。由于缺少相关例子，导致用户可能无法解决reward model两个模型的加载问题。,https://gitee.com/mindspore/mindformers/issues/I7MRDD
mindformers,这是一个用户提出需求的issue，主要涉及数据集的input_columns配置问题，由于按照顺序而非key值匹配导致了模型传参的错误。,https://gitee.com/mindspore/mindformers/issues/I7LLWD
mindformers,这个issue类型是用户提出需求，主要涉及到增加训练时间的预估打印，以及一次迭代中训练和评估时间的打印。用户提出这个需求是为了方便了解整个训练过程中的时间分配情况。,https://gitee.com/mindspore/mindformers/issues/I7L8LL
mindformers,这是一个用户提出需求的issue，涉及的主要对象是ChatGLM26B模型，用户询问该模型是否支持。,https://gitee.com/mindspore/mindformers/issues/I7KZLN
mindformers,这是一个用户提出需求的issue，主要涉及GPT2教程中缺少eval_dataset的制作方法。这个问题之所以出现，是因为无法复现对应的效果截图。,https://gitee.com/mindspore/mindformers/issues/I7KY3A
mindformers,这是一个用户提出需求的issue，主要涉及增量推理支持多batch的功能。可能由于当前模型和流程都不支持增量推理多batch，用户希望在两方面都得到支持。,https://gitee.com/mindspore/mindformers/issues/I7KT9B
mindformers,这是一个用户提出需求的issue，主要涉及的对象是增加cosinelr最后为水平的学习率功能。原因可能是用户希望在模型训练中设置一种类似cosine学习率退火的功能，使得学习率在训练后期保持稳定水平。,https://gitee.com/mindspore/mindformers/issues/I7KJPB
mindformers,这是一个用户提出需求的类型issue，主要涉及bloom7.1b版本在ascend910单机8卡上进行微调后如何合并权重文件的问题。该问题是由于用户需要合并微调后的权重文件且指导文档未提供相关步骤或代码而导致的。,https://gitee.com/mindspore/mindformers/issues/I7K2BR
mindformers,这是一个需求类型的issue，主要涉及到CI测试用例的补齐及其在门禁中的监控，用户提出希望完善边训练边评估功能的需求。,https://gitee.com/mindspore/mindformers/issues/I7K0T8
mindformers,这是一个功能优化建议，主要涉及AICC拉取切分权重文件或超大数据集时的易用性问题。原因是当前界面功能拉取权重只能整量拉取，导致在特定场景下造成大量时间和空间浪费。,https://gitee.com/mindspore/mindformers/issues/I7K096
mindformers,该issue属于需求提出类型，主要涉及日志结构及内容的优化；用户要求增加输出内容、调整输出顺序、增加性能信息呈现、配置化控制日志输出、增加日志说明文档等，旨在改善日志的易用性。,https://gitee.com/mindspore/mindformers/issues/I7JUR3
mindformers,该问题单属于用户提出需求类型，涉及主要对象为mindformer支持的推理卡，用户询问是否支持310的卡。由于用户需要确定mindformer是否支持特定型号的推理卡，因此提出了这个问题。,https://gitee.com/mindspore/mindformers/issues/I7IO06
mindformers,这是一个用户提出需求的类型的issue，主要涉及到mindformers库中的temperature设置问题。用户询问是否有类似huggingface中temperature参数的设置选项。,https://gitee.com/mindspore/mindformers/issues/I7I5NU
mindformers,这是一个用户提出需求的issue，主要涉及logger记录日志时能否将文件和代码行数记录下来，快速定位的功能。由于用户希望能快速定位日志记录的文件和代码行数，因此提出了这个需求。,https://gitee.com/mindspore/mindformers/issues/I7H5R4
mindformers,这个issue属于需求类型，主要涉及到类GPT模型下游任务评测体系方案的设计和开发。由于缺乏完整的下游任务评测体系，导致模型迁移验证和对外展示受到影响。,https://gitee.com/mindspore/mindformers/issues/I7GG54
mindformers,这是一个用户提出需求的issue，主要涉及MindFormers套件的易用性问题。由于缺乏相关功能或文档支持，用户提出了合并权重、修改文档、增加分布式推理能力、文档说明等改进建议。,https://gitee.com/mindspore/mindformers/issues/I7DCE9
mindformers,这是一个用户提出需求的issue，主要对象是Bart这个模型。用户希望尽快更新该模型的转换，并因为当前模型可能存在问题或过时导致功能不符合预期。,https://gitee.com/mindspore/mindformers/issues/I7CVRC
mindformers,这个issue类型是特性需求提议，涉及对象是Trainer模块的功能接口。由于MindPet功能需要新增高阶接口方法，用户提出了增加Trainer.finetune接口以适配该功能。,https://gitee.com/mindspore/mindformers/issues/I7CFG6
mindformers,该issue属于用户提出需求类型，涉及对象为关于新增模型的实现机制。由于模型权重转换、权重下载与转换链接问题以及多模态任务代码耦合的困难，开发者寻求关于Mindformers的回复与解答。,https://gitee.com/mindspore/mindformers/issues/I79WAI
mindformers,这是一个用户需求类型的issue，主要涉及到武大LuojiaNet团队的交流和答疑遗留问题，涉及对MAE模型图像重构结果可视化、冻结ViT的backbone部分权重、提供UperNet模型作为像素级任务参考以及推理输入数据集标签映射等内容。,https://gitee.com/mindspore/mindformers/issues/I79ESP
mindformers,该issue是一个用户提出需求的类型，主要涉及MindBook查询类，由于分表过多导致新增模型需要多处修改子表，降低易用性。,https://gitee.com/mindspore/mindformers/issues/I79BNR
mindformers,这是一个用户提出需求的issue，主要涉及的对象是希望能提供类似modelzoo仓库中的export.py脚本，将.ckpt文件导出为.onnx，用于后续推理。,https://gitee.com/mindspore/mindformers/issues/I76SXM
mindformers,这是一个需求类型的issue，主要涉及如何将训练好的GPT2模型应用到推理应用开发中，用户希望了解如何生成.air或者.onnx文件来部署接口。,https://gitee.com/mindspore/mindformers/issues/I76S6O
mindformers,这是一个用户提出需求的issue，主要对象是配置文件内容的修改，由于需要频繁修改配置文件来调试参数，希望支持命令行修改配置文件内容以方便调试。,https://gitee.com/mindspore/mindformers/issues/I76OW5
mindformers,这个issue类型为需求提出，涉及的主要对象是MindFormers中的PR评审流程。由于需要规范PR评审流程，因此提出了相关的评审要求和人员一览表。,https://gitee.com/mindspore/mindformers/issues/I72ZQO
mindformers,这个issue类型是用户提出需求，主要涉及MindFormers关键模型的大规模分布式能力，由于用户需要了解核心关键模型的分布式并行能力一览表。,https://gitee.com/mindspore/mindformers/issues/I72ZN3
mindformers,这是用户提出的功能需求，主要对象是增加边训练边评估特性。由于用户希望在训练过程中执行评估脚本实时查看模型精度表现，所以提出了这个需求。,https://gitee.com/mindspore/mindformers/issues/I72O1W
mindformers,这是一个用户提出需求的类型issue，主要涉及需求完善自定义开发指南，因为缺乏指导导致用户不清楚如何开发自定义模型。,https://gitee.com/mindspore/mindformers/issues/I7268V
mindformers,这是一个用户提出需求的问题，主要涉及盘古大模型的API调用问题。由于用户对于盘古大模型的API调用有需求，因此提出了这个问题。,https://gitee.com/mindspore/mindformers/issues/I6ZZ9C
mindformers,这是一个用户提出需求的issue，主要涉及的对象是在ModelArts的训练任务中支持多级多卡训练。由于目前该模型未提供对应的ModelArts的训练任务中支持多级多卡训练gpt2的支持，用户希望工程师能提供相应支持以便于使用人工智能计算中心的算力资源进行多机多卡训练。,https://gitee.com/mindspore/mindformers/issues/I6YH0T
mindformers,这是一个用户提出需求的类型issue，主要涉及模型复现以及需要获得相关技术和路线上的帮助。,https://gitee.com/mindspore/mindformers/issues/I6WL8R
mindformers,这是一个用户提出需求的issue，主要涉及的对象是增加T5 Pegasus模型和中文文本摘要的下游任务，用户要求在项目中增加这些内容。,https://gitee.com/mindspore/mindformers/issues/I6WBGZ
mindformers,这个issue属于用户提出需求类型，主要对象是Trainer高阶接口，用户希望该接口支持各类分布式并行能力调用。,https://gitee.com/mindspore/mindformers/issues/I6UFCU
mindformers,这是一个用户提出的功能需求，主要对象是数据集的generator接口。由于部分API注释和测试用例中有generator接口，用户建议将其抽取出来成一个独立的接口。,https://gitee.com/mindspore/mindformers/issues/I6S30W
mindformers,这是一个用户提出需求的issue，请求向mindformer适配体系中新增适配 xlmrobertbase、xlmrobertalarge、flant5base、flant5large四个模型。,https://gitee.com/mindspore/mindformers/issues/I6QLIE
mindformers,这是一个需求提出类的issue，主要涉及对象是代码中的新建和加载操作，由于无法准确判断是否需要加载优化器的权重而导致了问题。,https://gitee.com/mindspore/mindformers/issues/I6PSJN
mindformers,这是用户提出的需求问题，主要涉及分布式启动方式以及相应文档的更新，用户可能希望通过mpirun来统一更新各种分布式启动方式。,https://gitee.com/mindspore/mindformers/issues/I6PMM5
mindformers,这是一个用户提出需求的类型，主要涉及TrainingArguments类，由于配置入参指令不全导致易用性不佳。,https://gitee.com/mindspore/mindformers/issues/I6OFGE
mindformers,这是一个需求类型的issue，主要对象是模型的batch size设置。由于目前模型的batch size与网络construct实时获取时未解耦，导致用户无法动态调整模型的batch size，需求解耦以便更灵活地设置batch size。,https://gitee.com/mindspore/mindformers/issues/I6OFEZ
mindformers,该issue属于用户提出需求，希望添加并行配置的文档指导。,https://gitee.com/mindspore/mindformers/issues/I6LWMQ
mindformers,这是一个用户提出需求的issue，主要对象是支持SOTA预训练模型，由于当前不支持这些大模型导致用户期望能够集成这些模型以提升NLP建模的效果。,https://gitee.com/mindspore/mindformers/issues/I6KTSE
mindformers,这是一个用户提出需求的issue，主要对象是支持昇思羲和生态平台权重自动下载及加载。,https://gitee.com/mindspore/mindformers/issues/I6JYEO
mindformers,这是一个用户提出需求的类型，该问题单涉及的主要对象是README文档及特性文档，用户可能希望更新至r0.3版本以获取最新的信息。,https://gitee.com/mindspore/mindformers/issues/I6JYD4
mindformers,这个issue是用户提出需求类型，主要对象是MindFormers GPT2支持MindSpore全部并行策略，用户提出了需要支持全部并行策略的功能。,https://gitee.com/mindspore/mindformers/issues/I6JYCJ
mindformers,这是一个用户提出需求的问题单，主要涉及generate()接口适配gpt的问题。由于缺少具体的描述内容，无法准确分析用户的具体需求或问题。,https://gitee.com/mindspore/mindformers/issues/I6J5BM
mindformers,这个issue属于用户提出需求类型，主要涉及MindFormers的易用性及通用性完善任务列表，由于缺乏标准的面向任务的数据集类规范和复杂的yaml配置，导致数据集部分规范方案和models部分存在一些缺陷。,https://gitee.com/mindspore/mindformers/issues/I6DNC0
mindformers,这是一个用户提出需求的issue，主要涉及的对象是增加通用的train wrapper实现以及clip grad标准实现。该问题产生的原因是缺乏这些功能导致用户无法高效地进行模型训练。,https://gitee.com/mindspore/mindformers/issues/I6DH2V
mindformers,这是一个功能需求类型的issue，主要涉及的对象是t5.py中的TransformerEncoder模块。由于代码分散在不同地方导致需要整理并放在一起，用户提出需求统一整改这一部分API。,https://gitee.com/mindspore/mindformers/issues/I6DG38
mindformers,该issue属于需求提出类型，主要对象是与huggingface对标的学习策略模块，用户提出了需要增加该模块并保持逻辑对齐的需求。,https://gitee.com/mindspore/mindformers/issues/I6DDV6
mindformers,这个issue属于需求提出类型，主要对象是对标HuggingFace的TrainingArguments类，并适配Trainer高阶接口，由于Trainer args入参行为不一致而导致需要对齐huggingface Trainer args入参行为。,https://gitee.com/mindspore/mindformers/issues/I6D87I
mindformers,这是一个需求提出类型的issue，主要涉及到mindformers项目中的日志功能改进。,https://gitee.com/mindspore/mindformers/issues/I6CQZ9
mindformers,这是一个用户提出需求的类型，主要对象是增加直接pip wheel安装包方式。原因可能是用户希望能够更方便快捷地安装包。,https://gitee.com/mindspore/mindformers/issues/I6AHNI
mindformers,这是一个用户提出需求的issue，主要涉及对象是修改obs云上存储默认文件夹名称为MindFormers。,https://gitee.com/mindspore/mindformers/issues/I6AHNH
mindformers,这是一个关于构建时需要在setuptools打包时去除tests文件夹的需求，主要对象是项目的构建配置。,https://gitee.com/mindspore/mindformers/issues/I6ACMD
mindformers,这个issue是一个需求类型，涉及MindFormers 大模型套件 330 版本的优化升级计划，用户提出了待办项列表。,https://gitee.com/mindspore/mindformers/issues/I6AA73
mindformers,这个issue属于功能需求类型，主要对象是优化器参数预处理和权重预处理功能，用户提出了如何将这些功能作为模块调用到通用的task trainer中的问题。,https://gitee.com/mindspore/mindformers/issues/I6A6EU
mindformers,这是一个需求提出类型的issue，涉及主要对象为Mindformers的models文件夹及Model类。,https://gitee.com/mindspore/mindformers/issues/I6A642
mindformers,这是一个用户提出需求的类型，主要涉及数据集API整改和规范化，兼容2.0版本。,https://gitee.com/mindspore/mindformers/issues/I6A63O
mindformers,这是一个需求更改类型的issue，该问题单涉及的主要对象是变量命名。原因是修改变量名导致代码中使用该变量的地方需要做相应的更改。,https://gitee.com/mindspore/mindformers/issues/I6A4AP
mindformers,该issue类型为用户提出需求，涉及的主要对象是兼容Huggingface接口。由于与Huggingface接口不对齐，导致需要在主要接口上进行调整。,https://gitee.com/mindspore/mindformers/issues/I69KB4
mindformers,这是一个用户提出需求的issue，主要涉及词表的配置文件应该存放在哪里的问题。由于现在需要手动给每个model的config中添加tokenizer的内容，因此用户在寻求一个更合理的存放词表配置信息的方式。,https://gitee.com/mindspore/mindformers/issues/I68UAP
mindformers,这是一个需求提出类型的issue，主要涉及到LR组件重复导致的功能重叠问题，提出合并组件并统一命名的建议。,https://gitee.com/mindspore/mindformers/issues/I68U5R
mindformers,这是一个用户提出的需求。该问题单涉及的主要对象是项目中的run_mindformer功能。用户提出需求是为了给run_mindformer添加finetune功能，可能是为了使模型能够在特定任务上进行微调。,https://gitee.com/mindspore/mindformers/issues/I688B2
mindformers,这个issue类型是需求提出，主要涉及Trainer高阶接口API的注释和TaskTrainer逻辑的修复。,https://gitee.com/mindspore/mindformers/issues/I67W0U
mindformers,这是一个用户提出需求的issue，主要涉及到配置文件加载逻辑的改动。根据描述，用户希望将配置文件加载方式从云端切换为本地离线默认yaml加载逻辑。,https://gitee.com/mindspore/mindformers/issues/I67NVC
mindformers,这是一个用户提出需求的类型，主要涉及到mindformers项目中的opt模块。用户希望尽快完善opt模块与相关文档，以便进行文本摘要生成相关工作。,https://gitee.com/mindspore/mindformers/issues/I67LG1
mindformers,这是一个用户提出需求类型的issue，主要涉及新增Predict模板。由于用户需要使用Predict模板来进行某种预测分析，因此提出了这个问题。,https://gitee.com/mindspore/mindformers/issues/I67JMV
mindformers,该issue类型为需求提出，涉及的主要对象是通过resume_checkpoint_path恢复训练功能支持。,https://gitee.com/mindspore/mindformers/issues/I67JMS
mindformers,这个issue类型为用户提出需求，涉及的主要对象是新增的zeroshotimageclassification Trainer API接口。,https://gitee.com/mindspore/mindformers/issues/I67JMQ
mindformers,这个issue类型是需求提出，涉及的主要对象是MindSpore的数据增强算子接口。,https://gitee.com/mindspore/mindformers/issues/I67JML
mindformers,该issue属于用户提出需求类型，主要涉及Task Trainer规范接口和模板代码。,https://gitee.com/mindspore/mindformers/issues/I67JMI
mindformers,这是一个用户提出需求的issue，涉及 MindFormers 套件的API文档注释。该需求可能是为了改进文档的完整性或准确性。,https://gitee.com/mindspore/mindformers/issues/I67JC4
mindformers,这个issue是用户提出的需求，主要涉及TranslationPipeline需要支持BatchSize输入。由于nlp的generate方法以及pipeline的输入目前不支持batch输入，用户希望该功能能够被支持，以提升处理效率。,https://gitee.com/mindspore/mindformers/issues/I67ESZ
mindformers,这是一个用户提出需求的类型，主要涉及的对象是代码中的T5Porcessor，由于使用了错误的T5Porcessor导致了功能无法正常运行，用户希望将其替换为正确的T5Tokenizer。,https://gitee.com/mindspore/mindformers/issues/I67C4K
mindformers,这是一个用户提出需求的 issue，涉及的主要对象是 pipeline 接口。,https://gitee.com/mindspore/mindformers/issues/I679ZW
mindformers,这个issue属于用户提出需求类型，主要对象是Trainer组件。由于缺乏保存配置文件的功能，用户提出了需要Trainer支持配置文件保存的请求。,https://gitee.com/mindspore/mindformers/issues/I674YK
mindformers,这是一个关于需求的issue，主要对象是支持默认的通用task_name类型，由于某种原因导致用户提出了关于该问题如何引起、重现步骤和报错信息的问题。,https://gitee.com/mindspore/mindformers/issues/I672E7
mindformers,这是一个用户提出需求的类型，该问题单涉及的主要对象是Trainer接口。由于Trainer接口未提供指定Wrapper为DynamicLossScale的方法，用户在使用时无法修改相关参数，导致了无法灵活调整动态损失缩放功能的问题。,https://gitee.com/mindspore/mindformers/issues/I66LPR
mindformers,这个issue属于用户提出需求的类型，主要涉及到的对象是`build_tokenizer`的内部实现。由于Yaml文件中没有清晰指定传入的词表文件路径，导致需要提供一个词表文件路径的问题。,https://gitee.com/mindspore/mindformers/issues/I66KD6
mindformers,这是一个用户提出需求的类型的issue，主要涉及到Trainer高阶接口的predict属性函数。由于缺乏这一功能，用户提出了希望Trainer高阶接口能够支持predict属性函数的需求。,https://gitee.com/mindspore/mindformers/issues/I662I3
mindformers,这是一个需求提出类型的issue，主要涉及支持Swinv2分类骨干网络的功能。由于目前系统不支持该功能，用户提出需求希望能够新增对Swinv2分类骨干网络的支持。,https://gitee.com/mindspore/mindformers/issues/I662HM
mindformers,这是用户提出需求类型的issue，主要涉及的对象是支持simmim预训练模型及swinV1分类网络。由于用户希望增加对simmim预训练模型和swinV1分类网络的支持，因此提出了这个需求。,https://gitee.com/mindspore/mindformers/issues/I662HI
mindformers,这是一个功能需求类型的issue，主要涉及支持MAE预训练模型及Vit分类网络。可能是用户需求增加对这两种模型的支持和集成。,https://gitee.com/mindspore/mindformers/issues/I662HA
mindformers,这个issue属于文档改进类型，主要涉及到规范trainer和common模块的docs注释。,https://gitee.com/mindspore/mindformers/issues/I662H5
mindformers,这是一个用户提出需求的类型，主要涉及MindFormers仓库的Import方式规范和统一。,https://gitee.com/mindspore/mindformers/issues/I662B1
mindformers,这个issue类型是用户提出需求，该问题单涉及的主要对象是BERT模型。由于最新版代码中没有支持BERT模型，用户希望尽快添加该支持。,https://gitee.com/mindspore/mindformers/issues/I65Q2X
mindformers,这是一个需求类型的issue，主要涉及的对象是README文档使用说明，并由于文档使用说明不清晰引起了问题。,https://gitee.com/mindspore/mindformers/issues/I65F27
mindformers,这是一个用户提出需求的issue，主要涉及到Trainer接口的完善和功能改进。,https://gitee.com/mindspore/mindformers/issues/I64H1K
mindformers,这个issue类型是需求提出，涉及的主要对象是MindFormers设计文档。由于文档内容描述不清晰，导致用户提出了关于框架对外接口和网络调用设计的问题。,https://gitee.com/mindspore/mindformers/issues/I63TGU
mindformers,这是一个用户提出的需求类型的issue，涉及的主要对象是各个模块的通用build接口。这个问题的提出可能是为了提升项目的代码可维护性和规范性，增加License用于标识各个模块的知识产权归属。,https://gitee.com/mindspore/mindformers/issues/I630CX
mindformers,这是一个需求类型的issue，主要涉及到打包wheel和测试安装的过程。,https://gitee.com/mindspore/mindformers/issues/I62OB5
mindformers,该issue类型是任务需求，主要涉及的对象是接口测试用例的开发，用户希望编写import、register、build接口的测试用例。,https://gitee.com/mindspore/mindformers/issues/I62O22
mindformers,这是一个关于需求的问题，涉及的主要对象是Trainer接口，由于需要与huggingface transformers接口使用方式进行对齐，因此产生了该问题。,https://gitee.com/mindspore/mindformers/issues/I62O1H
mindformers,"这是一个用户提出需求的issue，主要涉及Tokenzier的分析。由于需要增强易用性并减少用户学习成本，用户希望实现通用的Tokenzier，参考HuggingFace库的实现。
",https://gitee.com/mindspore/mindformers/issues/I61KF7
mindformers,这是一个用户提出需求的类型，主要涉及对象是T5和GPT模型的脚本推理功能。由于现有情况下无法进行T5和GPT模型的脚本推理，用户希望增加这一功能。,https://gitee.com/mindspore/mindformers/issues/I5YYL8
mindformers,这是一个用户提出需求的issue，主要对象是mindformers下的transformer库，由于当前transformer库推理流程不支持BeamSearch，导致用户请求优先适配这一功能。,https://gitee.com/mindspore/mindformers/issues/I5Q5NV
mindformers,该issue属于用户提出需求类型，主要对象为项目仓库中的ReadMe.md文件。用户提出在ReadMe.md中增加介绍如何为transformer仓库做贡献和添加自己的网络，旨在方便开发者为仓库做贡献并扩大其影响力。,https://gitee.com/mindspore/mindformers/issues/I5OBJ3
mindformers,这个issue类型是用户提出需求，该问题单涉及的主要对象是transformer仓库。由于开发者希望通过pip安装whl包的方式来使用transformer仓库，因此提出了这个需求。,https://gitee.com/mindspore/mindformers/issues/I5OBIK
mindformers,这是一个需求反馈，主要对象是Transformer库的整体接口，问题源于接口修改复杂、配置文件臃肿以及文档结构不佳。,https://gitee.com/mindspore/mindformers/issues/I5O5ZO
mindformers,这是一个建议性问题，用户提出了关于MindSpore框架中transformer中_Linear参数has_bias不可选的问题，希望增加可选性，允许设置has_bias为False。,https://gitee.com/mindspore/mindformers/issues/I5KM8W
mindformers,这是一个提出需求的issue，主要涉及项目的代码结构设计。,https://gitee.com/mindspore/mindformers/issues/I4VE50
mindformers,这是一个需求提出的issue，主要涉及T5模型实现中的一些差异和参考链接。由于Megatron中T5模型的实现与官方实现存在一些区别，用户提出了需要增加T5模型以及指出了不同之处，并提供了参考链接。,https://gitee.com/mindspore/mindformers/issues/I4UTG1
