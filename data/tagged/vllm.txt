https://github.com/vllm-project/vllm/issues/16071
这是一个Bug报告，主要涉及的对象是triton fused moe kernels。原因是误将未融合权重应用的版本合并，导致问题。

https://github.com/vllm-project/vllm/issues/16070
这是一个bug报告类型的issue，涉及主要对象为TPU worker，由于未对SlidingWindowSpec进行处理，导致TPU CI测试失败。

https://github.com/vllm-project/vllm/issues/16069
这是一个bug报告，涉及的对象是wheel构建pipeline，导致未能生成正确格式的index.html文件，导致无法从指定地址安装特定版本的包。

https://github.com/vllm-project/vllm/issues/16068
这是一个Bug报告，涉及v0.8.1版本的V1，pipeline-parallel-size为4时出现异常响应。

https://github.com/vllm-project/vllm/issues/16067
这是一个Bug报告，主要涉及Model Qwen2.51.5Binstruct在V1版本下速度较V0版本慢的问题。

https://github.com/vllm-project/vllm/issues/16066
这是一个bug报告，涉及主要对象为vllm项目中的cpu模块，由于gettid()未定义在`vllm_source/csrc/cpu/utils.cpp`中导致问题。

https://github.com/vllm-project/vllm/issues/16065
这个issue类型是用户提出需求，主要对象是如何在同一个docker实例中运行多个模型，从而提供多个功能而无需多个docker实例或监听不同端口的api服务器。

https://github.com/vllm-project/vllm/issues/16064
这个issue类型是用户提出需求，寻求关于获取最新开发版docker镜像的帮助。

https://github.com/vllm-project/vllm/issues/16063
这个issue是一个bug报告，涉及到提供环境变量来禁用采样器的功能。问题出现的原因是持续性地进行与redhat相关的工作。

https://github.com/vllm-project/vllm/issues/16062
这个issue类型是bug报告，主要涉及的对象是VLLM在Fedora 41上以及gtx1100等环境下的构建和运行问题。由于`DeviceMemoryProfiler`传递非索引设备字符串（例如"cuda"）到要求整数索引的PyTorch函数，导致V0 engine初始化失败，并且V1 engine存在多个问题需要修复。

https://github.com/vllm-project/vllm/issues/16061
这是一个bug报告，涉及到在运行gemma3模型时出现KeyError: 'local_attn_masks'的错误。原因可能是模型在cudagraph捕获时出现了内存错误。

https://github.com/vllm-project/vllm/issues/16060
这个issue类型属于性能优化建议，主要涉及CUDA kernel的优化，提出了关于性能提升和性能回归的讨论。

https://github.com/vllm-project/vllm/issues/16059
这是一个bug报告，在V1 TPU worker for sliding window中发现并修复了一个bug，导致由于一个测试失败而引发了这个问题。

https://github.com/vllm-project/vllm/issues/16058
这是一个Bug报告，涉及的主要对象是VLLM的Speculative Decoding功能。由于某些情况下发生数值传播和套接字错误，导致部署稳定性下降，用户寻求关于这两个错误的帮助。

https://github.com/vllm-project/vllm/issues/16057
这是一个bug报告，涉及v1版本中关于pipeline parallelism(PP)的默认行为/回退问题。这个问题是由于默认的"distributed executor"不支持v1版本的PP，导致使用默认参数会报错，而手动指定"ray" executor则能正常工作。

https://github.com/vllm-project/vllm/issues/16056
这是一个用户需求的issue，主要对象是v1 engine在CPU上的支持问题。用户想要了解为什么v1 engine暂时不支持CPU，并寻求如何在特定模型上运行推理的帮助。

https://github.com/vllm-project/vllm/issues/16055
这是一个关于bug报告的issue，主要涉及Jenkins benchmark中的失败报告问题。原因可能是代码中的错误导致了失败报告的不准确或不完整。

https://github.com/vllm-project/vllm/issues/16054
这是一个bug报告，主要涉及vllm项目下的test_async_llm.py中的test_abort功能，可能由于CI flake导致assert has_unfinished_requests()失败。

https://github.com/vllm-project/vllm/issues/16053
这是一个Bug报告，涉及的主要对象是vllm项目中的测试脚本。这个问题可能是由于JSON解析错误导致的CI Flake问题。

https://github.com/vllm-project/vllm/issues/16052
该issue属于一个[RFC]（Request For Comments），主要涉及VLLM引擎在支持非文本生成模型方面的改进。导致这个RFC产生的原因是希望VLLM能更好地支持生成不同模态的输出数据。

https://github.com/vllm-project/vllm/issues/16051
这是一个bug报告类型的issue，涉及主要对象是在Kaggle GPUs上运行两个不同模型并指定GPU的问题。由于未能正确指定每个模型的GPU，导致CUDA内存不足错误和错误输出。

https://github.com/vllm-project/vllm/issues/16050
该问题类型为性能优化建议，针对vllm的离线LLM推理过程中随着时间推移速度逐渐变慢的问题。

https://github.com/vllm-project/vllm/issues/16049
这是一个关于bug报告的issue，主要涉及的对象是Mac上的clang编译器。问题是由于Mac上使用较高版本的clang（17及以上）导致导入符号找不到问题，具体表现为`___kmpc_dispatch_deinit`符号找不到的错误。

https://github.com/vllm-project/vllm/issues/16048
这个issue类型是bug报告，涉及的主要对象是Dockerfile.ppc64le。由于opencpython版本问题以及hf-xet的编译安装，导致需要对Dockerfile做修复。

https://github.com/vllm-project/vllm/issues/16047
这个issue类型为更新需求，涉及主要对象是hpuextension，由于最新更新移除了Index Reduce fp32 casts，需要更新requirementshpu.txt文件。

https://github.com/vllm-project/vllm/issues/16046
这是一个Bug报告，主要涉及多轮对话只有最后一轮进行推理的问题。原因可能是推理引擎的逻辑导致的。

https://github.com/vllm-project/vllm/issues/16045
这是一个bug报告，涉及到Neuron配置的覆盖问题，由于用Python `dict`类型代替覆盖的对象类型，导致后续错误。

https://github.com/vllm-project/vllm/issues/16044
这个issue类型是需求改进，涉及chat_with_tools示例的改进。由于第一次使用需权限设置，处理以特定前缀开头的JSON输出，增强JSON解析处理和添加调试信息等问题。

https://github.com/vllm-project/vllm/issues/16043
这是一个功能需求类型的issue，主要涉及NeuronX的分布式推理支持、推测解码和动态设备上的抽样。导致此需求产生的原因可能是为了增强Neuron的功能性和性能。

https://github.com/vllm-project/vllm/issues/16042
这是一个需求类型的issue，涉及的主要对象是在TPUs上建立性能基准测试。由于当前还没有TPU版本的持续性能基准测试，用户提出了添加Llama 3.1 8B和Llama 3.1 70B在不同版本上的性能测试的需求。

https://github.com/vllm-project/vllm/issues/16041
这是一个bug报告，主要涉及的对象是TPU模块，用户提出了需要移除硬编码的ragged attention kernel参数。

https://github.com/vllm-project/vllm/issues/16040
这是一个bug报告，涉及LoRA Punica Kernel的实现顺序问题，可能导致潜在的bug。

https://github.com/vllm-project/vllm/issues/16039
这个issue是关于集成PPLX-kernels的，类型为功能需求，并涉及到MoE kernels的重构。

https://github.com/vllm-project/vllm/issues/16038
这是一个需求报告，主要涉及模型中使用压缩张量的问题。原因可能是为了改进模型性能或减少资源消耗。

https://github.com/vllm-project/vllm/issues/16037
这是一个需求讨论的Issue，主要涉及Data Parallel Attention和Expert Parallel MoEs的设计和实现。问题的根源在于为了解决MLA模型中的内存占用问题，需要重新设计Attention机制并支持专家并行化。

https://github.com/vllm-project/vllm/issues/16036
这是一个针对"vllm"项目中s390x架构相关的bug修复和CI测试脚本添加的issue。

https://github.com/vllm-project/vllm/issues/16035
这个issue类型是功能需求，主要涉及的对象是EAGLE模型在V1版本中的加载问题。由于目前存在一些限制，例如只支持单GPU、需要在eager模式下运行等，导致用户需要进行特定的操作才能正确运行这个PR。

https://github.com/vllm-project/vllm/issues/16034
这个issue类型是功能需求，主要目标是将gfx950添加到支持的架构中。

https://github.com/vllm-project/vllm/issues/16033
这是一个bug报告，主要涉及函数命名问题，由于函数命名的问题导致被pytest错误识别为测试用例。

https://github.com/vllm-project/vllm/issues/16032
这是一个用户提出需求的issue，主要涉及支持Cutlass MLA for Blackwell GPUs，由于最新的Cutlass支持Blackwell GPUs的MLA，用户希望在下一个版本（v3.9）中提供集成此核心的功能。

https://github.com/vllm-project/vllm/issues/16031
这是一个bug报告，该问题单涉及的主要对象是VLLM项目中的`fp8 quant`模块。由于未指定具体内容，无法分析导致的具体原因。

https://github.com/vllm-project/vllm/issues/16030
这是一个 Bug 报告，主要涉及的对象是 xgrammar 的缺失导致服务器崩溃。由于 xgrammar 的缺失或加载错误，用户发送请求时引发错误并导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/16029
这是一个需求提出的issue，主要涉及 lmformatenforcer 工具在与 pydantic 结合使用时出现的问题。

https://github.com/vllm-project/vllm/issues/16028
这是一个关于bug报告的issue，主要涉及VLLM环境下 Llama-3.2-3B-Instruct 模型中出现两个开始序列标记的问题。

https://github.com/vllm-project/vllm/issues/16027
这个issue是一个功能改进类型的问题，主要涉及自动检测处理bitsandbytes预量化模型，解决了需要显式传递quantization engine参数的问题。

https://github.com/vllm-project/vllm/issues/16026
该issue属于用户提出需求类型，主要涉及vllm的服务benchmark功能，用户希望在在线benchmark中能够指定采样参数(topk, topp等)，并通过在客户端请求的"extra_body"字段中添加采样参数来实现，同时在benchmark_serving.py中新增了命令行标志（`topp`、`topk`和`temperature`）。

https://github.com/vllm-project/vllm/issues/16025
这是一个bug报告，主要涉及内容文档的问题，缺少了一个`EOF`，导致Deployment with CPUs部分的PVC/Secret命令不完整。

https://github.com/vllm-project/vllm/issues/16024
这个issue类型为需求提出，主要涉及的对象是 "Advance tpu.txt" 和 "torch_xla"。这个需求是为了将 "tpu.txt" 和 "torch_xla" 进行更新到最新的 nightly torch 版本。

https://github.com/vllm-project/vllm/issues/16023
这是一个bug报告，主要涉及的对象是Spec Decode模块。可能由于禁用了Spec Decoding，导致Metrics中出现无效的nan数值并被记录。

https://github.com/vllm-project/vllm/issues/16022
这个issue类型为功能需求提案，主要涉及添加在在线基准测试中指定采样参数的功能。用户提出需要在基准测试中指定采样参数（如topk、topp等），并展示了在TPU上设置topk和topp采样参数后的性能优化结果。

https://github.com/vllm-project/vllm/issues/16021
这是一个Bug报告，主要涉及的对象是无法在运行Phi4时设置tensor-parallel-size为4，并且涉及到环境信息收集的问题。

https://github.com/vllm-project/vllm/issues/16020
这是一个功能需求提出的issue，涉及的主要对象是模型aware kv操作的帮助函数。

https://github.com/vllm-project/vllm/issues/16019
这个issue类型是bug报告，涉及主要对象是vllm下的一个新模型"support for fashion-clip"。由于缺少支持该模型的功能，导致用户在评估模型时遇到错误。

https://github.com/vllm-project/vllm/issues/16018
这是一个bug报告，涉及主要对象是benchmark代码中的关键字`request_goodput:`，由于其中包含冒号导致metric name在`result_json`中无法正确映射的问题。

https://github.com/vllm-project/vllm/issues/16017
这个issue类型为用户提出需求，需要为VLLM添加支持SmolVLM。该问题单涉及的主要对象是模型扩展。由于HuggingFaceTB/SmolVLM22.2BInstruct的推出，用户希望VLLM能够支持该模型。

https://github.com/vllm-project/vllm/issues/16016
这是一个功能请求（RFC - Request for Comments），主要涉及vLLM的缓存技术中的安全性问题。原因是缓存在面临时序侧信道攻击时存在漏洞，需要一种能够跨组织分段缓存复用的技术改进。

https://github.com/vllm-project/vllm/issues/16015
这个issue类型是bug报告，涉及主要对象是格式化脚本"format.sh"。这个问题可能是由于格式化脚本"format.sh"没有被设置为可执行文件导致的。

https://github.com/vllm-project/vllm/issues/16014
这是一个Bug报告，涉及主要对象为Mistral3.1，可能是由于环境中vllm版本为0.8.1导致的Null response问题。

https://github.com/vllm-project/vllm/issues/16013
这是一个bug报告类型的issue，涉及的主要对象是vllm FlashAttention-2 backend。由于在构建新的vllm image时未正确添加fa_utils.py导致了缺失文件错误。

https://github.com/vllm-project/vllm/issues/16012
这个issue是关于需求提议的，主要涉及`huggingface-cli[hf_xet]`的移动问题。由于疑问`huggingface-cli[hf_xet]`是否需要在`requirements/common.txt`中，提议将其移动到`requirements/rocm.txt`或`requirements/rocmtest.txt`，并寻求反馈。

https://github.com/vllm-project/vllm/issues/16011
这是一个性能比较类型的issue，主要涉及vLLM在不同GPU配置下的性能表现，用户想了解为什么1x8的配置在最大并发性方面优于2x4的配置。

https://github.com/vllm-project/vllm/issues/16010
这是一个bug报告。该问题涉及到用户在ROCm平台上开启自定义allreduce功能时遇到的问题。原因是ROCm平台的设备能力不是顺序的，导致在MI300上开启自定义allreduce时需要进行更改。

https://github.com/vllm-project/vllm/issues/16009
这是一个关于Bug报告类型的issue，主要涉及的对象是vllm库。这个Bug是由于缺少一个必需的参数‘inner_exception’导致的。

https://github.com/vllm-project/vllm/issues/16008
这是一个bug报告，涉及到工具调用在Qwen模型中无法正常工作的问题。原因可能是在v0.8.2版本中出现了幻觉响应以及没有工具调用的输出。

https://github.com/vllm-project/vllm/issues/16007
这是一个功能改进类型的issue，主要涉及的对象是vllm中的SupportsMultiModal接口。导致这个问题的原因是命名标准不一致，需要添加一个getter来抽象命名。

https://github.com/vllm-project/vllm/issues/16006
这是一个Bug报告类型的issue，主要涉及的对象是vllm项目。由于某些原因，在使用VSCode进行调试时会导致vllm服务器崩溃。

https://github.com/vllm-project/vllm/issues/16005
这是一个bug报告，主要涉及测试用例中的问题，由于环境变量限制导致测试无法通过。

https://github.com/vllm-project/vllm/issues/16004
这是一个Bug报告，主要涉及的对象是`test_sharded_state_loader`，原因导致了该issue是当前环境下出现了运行失败的问题。

https://github.com/vllm-project/vllm/issues/16003
这是一个关于在Jupyter Notebook中运行vLLM的问题，类型是需求提出。问题涉及主要对象是vLLM的使用者。由于无法在Jupyter Notebook中成功运行vLLM的代码，导致用户提出了问题。

https://github.com/vllm-project/vllm/issues/16002
这个issue属于性能优化类型，涉及的主要对象是CPU backend。这个问题由于默认的block_size导致性能较差，因此需要将其修改为128来提升性能。

https://github.com/vllm-project/vllm/issues/16001
该issue属于功能改进类，主要涉及到项目中的脚本组织和管理。根据描述，问题是由于原有文件目录中脚本过多，需要将硬件CI相关的脚本移动到`scripts/hardware_ci`目录下，以及将其他脚本移到`scripts/`目录下，以更好地组织文件目录结构。

https://github.com/vllm-project/vllm/issues/16000
该issue类型是用户提出需求，主要涉及的对象是针对vLLM中离线推断的性能问题。由于在同步模式下运行离线推断时用户体验性能问题，需手动管理分批处理，故提出引入异步模式来自动处理内部分批，提高效率。

https://github.com/vllm-project/vllm/issues/15999
这是一个bug报告，涉及到自定义Reasoning Parser在vllm engine初始化时无法动态应用的问题。

https://github.com/vllm-project/vllm/issues/15998
这个issue类型是功能更新，并涉及更新benchmark README和添加AIMODataset功能，可能由于最新变更而导致需要更新benchmark文档。

https://github.com/vllm-project/vllm/issues/15997
这是一个Bug报告，涉及的主要对象是使用vllm Docker镜像和下载的模型创建Helm图表时出现错误。由于Docker镜像中的ENTRYPOINT指令不正确导致了`raise KeyboardInterrupt("terminated") KeyboardInterrupt: terminated`错误。

https://github.com/vllm-project/vllm/issues/15996
这是一个Bug报告，涉及的主要对象是系统压力测试功能。由于GPU KV Cache被快速占用导致系统在高请求量下卡住，调度器无法正确安排请求执行。

https://github.com/vllm-project/vllm/issues/15995
这是一个bug报告，涉及到修复cu118版本的CI失败的问题，由于half到float转换在gguf kernels中出现错误，导致CI构建失败。

https://github.com/vllm-project/vllm/issues/15994
这是一个需求提交类型的issue，主要对象是改进错误信息提示。由于用户在推断设备类型时遇到困难，希望在错误消息中添加指导说明以帮助用户。

https://github.com/vllm-project/vllm/issues/15993
这个issue是关于文档更新的请求，主要涉及到基准测试（Benchmark）的内容。由于最新更改（添加 AIMO 数据集到基准测试中）导致文档需要更新。

https://github.com/vllm-project/vllm/issues/15992
这是一个Bug报告，涉及LLM在加载时出现错误。原因是编译时出现错误导致加载失败。

https://github.com/vllm-project/vllm/issues/15991
这个issue是关于一个bug报告，涉及的主要对象是vllm项目下的amd gpu，由于多个显卡无法正常工作所导致。

https://github.com/vllm-project/vllm/issues/15990
这是一个用户提出需求的类型的issue, 主要涉及的对象是Core下的EncoderDecoderModelRunner，由于该对象缺少LoRA功能，用户提出需要为其添加LoRA。

https://github.com/vllm-project/vllm/issues/15989
这是一个问题报告，涉及主要对象为vllm的安装过程。由于导入模块时出现错误，可能导致了安装失败。

https://github.com/vllm-project/vllm/issues/15988
这是一个bug报告类型的issue，主要涉及TCPStore的地址设置问题导致EADDRINUSE (98)错误。

https://github.com/vllm-project/vllm/issues/15987
这是一个Bug报告，涉及到通过trl的vllm_serve启动nodelocal推理服务器时出现EADDRINUSE错误的问题。原因是torch的TCPStore在处理连接时使用了0.0.0.0接口，导致监听所有网络接口而不仅仅是与主机地址相关联的接口。

https://github.com/vllm-project/vllm/issues/15986
这是一个bug报告，涉及到无法在vllm mac m1上使用`uv run`或`uv run python`，导致无法运行的问题。

https://github.com/vllm-project/vllm/issues/15985
这是一个bug报告，涉及的主要对象是vllm在Mac系列设备上无法使用uv run或uv run python，可能是由于未正确配置Python环境引起的。

https://github.com/vllm-project/vllm/issues/15984
这是一个Bug报告issue，主要涉及VLLM v0.7.3版本中使用"stream_options"参数导致无法导入版本信息的问题。

https://github.com/vllm-project/vllm/issues/15983
这是一个bug报告，主要涉及VLLM在0.8.0版本中启动速度慢且出现OOM错误的问题。

https://github.com/vllm-project/vllm/issues/15982
这是一个bug报告，主要涉及PA kernel selection算法在没有提供sliding window时恢复正确的问题。

https://github.com/vllm-project/vllm/issues/15981
这个issue是一个bug报告，主要涉及的对象是VLLM模型，由于添加了sliding window导致模型失败。

https://github.com/vllm-project/vllm/issues/15980
这个issue是关于bug的报告，涉及主要对象是`torchrun_example.py`文件，由于更新了`tests/distributed/test_torchrun_example.py`，但未更新`torchrun_example.py`导致的问题。

https://github.com/vllm-project/vllm/issues/15979
这是一个Bug报告，涉及的主要对象是Punica Kernel实现中的代码。由于使用了错误的`torch.unique(sorted=False)`而可能导致数据加载错误，需要更改为`torch.unique(sorted=True)`来确保`lora_ids`的排序正确。

https://github.com/vllm-project/vllm/issues/15978
这是一个Bug报告类型的issue，涉及到了v1 + lora + allow_tokens_ids_mask这个功能模块，由于allowed_token_ids_mask参数的shape与logits参数的shape不匹配，导致了sampling_metadata.allowed_token_ids_mask.shape不匹配logits.shape的问题。

https://github.com/vllm-project/vllm/issues/15977
这是一个功能需求的issue，涉及V1版本的DP scale-out实现，主要涉及到引擎进程管理和通信的解耦。原因是为了支持本地和/或远程引擎的混合运行，以及在不同节点上运行的远程引擎。

https://github.com/vllm-project/vllm/issues/15976
这是一个bug报告，主要涉及vllm==0.8.2中使用SamplingParams的"bad_words"参数时出现CUDA内存不足的问题。

https://github.com/vllm-project/vllm/issues/15975
该issue类型是功能更新，涉及对象为V0和V1的集成与性能优化。

https://github.com/vllm-project/vllm/issues/15974
这是一个bug报告，涉及的主要对象是gguf文件检查功能。这个问题是由于可以轻松模拟一个gguf文件并尝试使用GGUF_MAGIC进行检查导致的。

https://github.com/vllm-project/vllm/issues/15973
这是一个Bug报告，主要涉及vLLM 0.8.2版本中启用calculate_kv_scales时出现异常的问题。原因可能是参数配置错误导致的异常现象。

https://github.com/vllm-project/vllm/issues/15972
这是一个Bug报告，涉及的主要对象是github上的vllm项目下的一个issue。这个问题可能是由于缺少`Xformers`在`rocm/vllm:rocm6.3.1_instinct_vllm0.7.3_20250311`版本中导致了无法运行`pixtral-12b-2409`的错误。

https://github.com/vllm-project/vllm/issues/15971
这个issue是一个BUGFIX类型，主要涉及到vllm v1 engine在prompt长度等于max model长度时导致lm_eval失败的问题。

https://github.com/vllm-project/vllm/issues/15970
这是一个关于RFC（Request for Comments）的issue，主要涉及AWS Neuron的NxD Inference集成到vLLM中的问题。原因是为了优化在Neuron上的推理性能，并添加了一些功能。

https://github.com/vllm-project/vllm/issues/15969
这是一个bug报告，涉及的主要对象是`huggingface-cli[hf-xet]`。这个问题可能导致TPU和ROCm安装时出现问题。

https://github.com/vllm-project/vllm/issues/15968
这个issue是一个Bug报告，主要涉及vLLM服务器在尝试以多模态形式调用时崩溃的问题。这可能是由于服务器无法处理通过curl调用导致的问题所致。

https://github.com/vllm-project/vllm/issues/15967
这个issue是关于bug报告，涉及的主要对象是VLLM中的`SamplingParams`。由于未能确定性复现，用户设置了`min_tokens`但有时生成的tokens少于设定值，可能是由于之前的CC([BugFix][V1] Quick fix for min_tokens with multiple EOS)修复的问题仍存在。

https://github.com/vllm-project/vllm/issues/15965
这是一个bug报告，主要涉及的对象是在Ubuntu 22.04.5 LTS上安装Mistral-Small-3.1时出现的错误。由于环境中Python版本为3.10.12，可能导致了无法正确运行Mistral-Small-3.1的问题。

https://github.com/vllm-project/vllm/issues/15964
这个issue是关于用户无法在vllm serve中查看生成的标记，类型为bug报告，主要涉及对象为vllm serve环境。问题可能由于输出未显示在vllm serve中导致。

https://github.com/vllm-project/vllm/issues/15963
这是一个bug报告，涉及到将huggingface_hub的最低版本添加至Xet下载的问题。由于添加最低版本限制导致一些CI失败。

https://github.com/vllm-project/vllm/issues/15962
这是一个用户提出需求的issue，主要涉及对象是LlamaForCausalLM模型，用户希望能够对LlamaMLP设置reduce_results，并且设置层类型。

https://github.com/vllm-project/vllm/issues/15961
这是一个用户提出需求的issue，主要涉及的对象是vllm的Mixtral模型。由于现有的vllm不支持对Mixtral8x7b模型进行fp8量化，用户希望添加这一支持。

https://github.com/vllm-project/vllm/issues/15960
这个issue是一个功能需求。它主要涉及到v1版本中的KV Connector API以支持分解预填充（disaggregated prefill）功能。

https://github.com/vllm-project/vllm/issues/15959
这是一个Bug报告，主要涉及的对象是vLLM中的一个4bit quantized model。可能由于权重或模型形状不匹配导致的错误。

https://github.com/vllm-project/vllm/issues/15958
这是一个bug报告，主要涉及到vllm的SpecDecoding指标显示问题，可能由于启动llava时未启用spec decoding导致输出日志中出现大量无关信息。

https://github.com/vllm-project/vllm/issues/15957
这是一个功能需求类型的issue，主要涉及DeepGemm MoE expert map的支持问题。由于当前支持方式无法与cudagraphs兼容，导致无法正常工作。

https://github.com/vllm-project/vllm/issues/15956
这是一个用户提出需求的issue，主要涉及模块化融合专家并集成PPLX内核，旨在使MoE内核更模块化，以便能够轻松地支持多种通信机制。

https://github.com/vllm-project/vllm/issues/15955
这个issue类型是用户提出需求，涉及的主要对象是在benchmark中添加AIMO Dataset。由于需要增加AIMO数据集以及传递随机种子以实现可重现性，因此可能导致用户需要在benchmark测试中进行验证。

https://github.com/vllm-project/vllm/issues/15954
这是一个Bug报告，主要对象是vLLM在多节点LLM分布式推理中无法使用MPI作为后端，导致无法正确获取world_size和rank。

https://github.com/vllm-project/vllm/issues/15953
这是一个Bug报告，涉及VLLM库在CPU编译时调用方式错误导致安装失败。

https://github.com/vllm-project/vllm/issues/15952
这是一个用户提出需求的issue，主要涉及Chameleon、Chatglm和Commandr模型的支持量化功能，原因是为了统一支持所有模型的量化，并确保正确更新嵌套模型的`packed_modules_mapping`和`ignored modules`属性。

https://github.com/vllm-project/vllm/issues/15951
这是一个bug报告类型的issue，主要涉及到"torch.cuda.is_available()"方法在初始化cuda runtime时导致的问题。

https://github.com/vllm-project/vllm/issues/15950
这是一个需求提出的issue，主要涉及V1版本中对Mistral3的支持。

https://github.com/vllm-project/vllm/issues/15949
该issue类型属于功能需求提出，涉及的主要对象是`guidance` guided decoding backend。由于`guidance`在处理JSON object schemas时遵循JSON Schema规范，而`outlines`和`xgrammar`默认不允许对象具有额外的属性，因此用户提出了实现一个选项来更好地对齐这些行为的需求。

https://github.com/vllm-project/vllm/issues/15948
这是一个用户提出需求的issue，主要涉及的对象是添加最大输出长度限制功能。这个需求是由于当前系统在请求时只检查输入token长度和max_tokens总和是否超过模型最大长度，导致可能出现资源紧缺和系统延迟增加的问题。

https://github.com/vllm-project/vllm/issues/15947
这是一个Bug报告，主要涉及vllm v1版本0.8.2在A100上使用Qwen2.57BInstruct时性能表现异常。

https://github.com/vllm-project/vllm/issues/15946
这是一个bug报告类型的issue，涉及的主要对象是vllm项目中的marlin kernel。由于在使用v1引擎时引入了新参数，但在条件判断中未考虑输入大小导致的错误。

https://github.com/vllm-project/vllm/issues/15945
这是一个 bug 报告，主要涉及 hardware(Gaudi)。由于未同步的HPU代码在主分支更新后导致参数不匹配，产生问题。

https://github.com/vllm-project/vllm/issues/15944
这是一个Bug报告，涉及主要对象是CI构建中的一个测试脚本。由于CI构建中的测试脚本出现问题，导致了flake现象。

https://github.com/vllm-project/vllm/issues/15943
这是一个关于bug报告的issue，主要涉及的对象是get_config函数，由于ConfigFormat.AUTO在macOS下不支持导致了ValueError异常。

https://github.com/vllm-project/vllm/issues/15942
这是一个Bug报告类型的Issue，涉及到版本0.7.*和0.8.*中的内存溢出问题。原因可能是队列机制工作不正常，导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/15941
这是一个Bug报告，主要涉及 torch.ops._C.silu_and_mul 方法不存在的问题。原因可能是方法在当前环境下无法找到，导致引发了错误。

https://github.com/vllm-project/vllm/issues/15940
这是一个bug报告，涉及到CI/CD流程中的重复执行测试。由于合并操作后出现了重复的入口点测试，导致CI流程中存在多余的重复测试。

https://github.com/vllm-project/vllm/issues/15939
这是一个功能需求的提交，主要对象是代码库中的模型（BaiChuanBaseForCausalLM，GPTNeoXForCausalLM，MPTForCausalLM），目的是修复使用`AutoWeightsLoader`加载复合模型时的问题。

https://github.com/vllm-project/vllm/issues/15938
这个issue是一个bug报告，主要涉及的对象是 vLLM 模型名称。由于模型名称为空字符串，导致 vLLM 无法正确返回模型名称，其症状是无法正常响应模型名称。

https://github.com/vllm-project/vllm/issues/15937
这是一个Bug报告，涉及的主要对象是关于TPU和HPU设备在使用Ray进行分布式推断时的一致性问题。由于实现未严格执行代码注释中指定的规则，导致这个问题的症状是功能与文档描述不一致。

https://github.com/vllm-project/vllm/issues/15936
这个issue类型是bug报告，涉及的主要对象是当前的vllm v1版本。由于当前版本默认使用了嵌入模型，但在v1版本中并不支持嵌入模型，导致这个bug。

https://github.com/vllm-project/vllm/issues/15935
这是一个Bug报告，主要涉及VLLM后端编译过程中出现的错误。导致该问题的原因可能是无法直接从缓存加载形状为None的编译图导致程序Hang On。

https://github.com/vllm-project/vllm/issues/15934
这是一个bug报告类型的issue，涉及的主要对象是vLLM CPU后端。由于当前IPEX allreduce与vLLM CPU线程绑定发生冲突，导致产生了该bug。

https://github.com/vllm-project/vllm/issues/15933
这是一个bug报告，涉及vLLM在高并发下产生错误结果的问题，可能是由于并发级别过高导致的。

https://github.com/vllm-project/vllm/issues/15932
这是一个需求问题单，主要对象是Kernel中的CUTLASS MoE kernels，用户提出了需要实现BF16和FP16权重支持的需求。

https://github.com/vllm-project/vllm/issues/15931
这是一个bug报告，涉及vllm 0.8.2中解析工具名称问题。由于本地部署的vllm无法调用MCP服务，可能是由于返回的工具名称不正确导致的。

https://github.com/vllm-project/vllm/issues/15930
这是一个Bug报告，涉及到VLLM项目的stream设置和客户端传入参数拼接JSON字符串时缺少末尾大括号的问题，可能导致反馈为发散状态。

https://github.com/vllm-project/vllm/issues/15929
这是一个关于bug报告的issue，主要涉及到multilingual-e5-large embedding models应该使用mean pooling而不是last pooling的问题。导致这个问题的原因可能是代码实现上的问题，导致结果不符合预期。

https://github.com/vllm-project/vllm/issues/15928
这是一个Bug报告类型的Issue，涉及的主要对象是在使用vLLM v1中的DeepSeek V3时遇到了错误。这个问题由于在服务DeepSeek V3时遇到错误，可能是由于之前对rotary embedding所做的更改引起的。

https://github.com/vllm-project/vllm/issues/15927
这是一个Bug报告，主要涉及的对象是MoE TPU checking。这个问题是由于代码中的拼写错误导致，错误在Python中方法对象没有被调用时会被评估为`True`，从而导致CI测试失败。

https://github.com/vllm-project/vllm/issues/15926
这个issue是关于bug修复的，问题涉及的主要对象是DeepseedMII benchmarking，由于解析器错误提取了错误的结果导致benchmark失败。

https://github.com/vllm-project/vllm/issues/15925
该issue属于用户提出需求类型，涉及主要对象是在使用vLLM进行视频问答任务中如何在在线模式下加载并使用预先计算的视觉标记。用户提出了如何修改代码以便在在线服务中直接加载embedding，以及是否有更好的方法来实现这一目标。

https://github.com/vllm-project/vllm/issues/15924
这个issue属于用户提出需求和寻求帮助类型，主要涉及如何确定参数以加快处理高请求吞吐量的速度。这是因为用户想要在部署vLLM服务器时确定`max_num_seqs`和`max_model_len`参数。

https://github.com/vllm-project/vllm/issues/15923
这是一个bug报告，涉及的主要对象是代码中的print语句。原因是在代码中使用了print语句而不是logger，导致缺乏日志记录和调试信息。

https://github.com/vllm-project/vllm/issues/15922
这是一个用户提出需求的issue，涉及项目文档的更新请求。用户希望能够直接点击目标文档链接，而无需先打开`CONTRIBUTING.md`，再点击目标链接。

https://github.com/vllm-project/vllm/issues/15921
这是一个用户提出需求的issue，主要涉及CLI命令行工具中缺少描述属性，导致用户在使用时无法清晰了解每个选项的作用。

https://github.com/vllm-project/vllm/issues/15920
这个issue是关于CI/Build的进一步清理 LoRA 测试的工作，属于代码质量相关的任务，主要对象是 LoRA 测试代码。

https://github.com/vllm-project/vllm/issues/15919
这是一个Bug报告，主要涉及VLLM 0.7.4和0.8.2在部署Qwen2.5-VL-72B时出现回复停止的问题。原因可能是版本兼容性或其他相关问题。

https://github.com/vllm-project/vllm/issues/15918
这是一个Bug报告，涉及到vllm 0.8.2和torch 0.2.6版本的启动模型报错。原因是编译过程中出现了gcc无法执行cc1的错误。

https://github.com/vllm-project/vllm/issues/15917
这是一个bug报告，主要涉及的对象是rocm.inc.md文件。原因是pytorch的URL错误，导致需要修正。

https://github.com/vllm-project/vllm/issues/15916
这是一个功能需求的issue，主要涉及到支持deepseek-v3-0324工具调用，由于尚未添加该功能，用户提出了相关需求。

https://github.com/vllm-project/vllm/issues/15915
这是一个bug报告，主要涉及的对象是在使用Docker部署DeepSeekR1DistillQwen32B时出现的启动故障。导致这个问题的原因可能是配置参数设置不正确或系统环境不匹配。

https://github.com/vllm-project/vllm/issues/15914
这个issue是一个代码重构类型的问题，主要涉及到了fused_moe.py文件的重构和维护性问题，需要将工具方法和不同的Mixture of Experts 实现分离以提高后续的重构和维护效率。

https://github.com/vllm-project/vllm/issues/15913
这是一个用户提出需求的类型，该问题单涉及的主要对象是文档内容。

https://github.com/vllm-project/vllm/issues/15912
这是一个“构建/持续集成（Build/CI）”类型的issue，涉及的主要对象是“lm-eval”测试依赖版本更新到0.4.8。这个更新是为了可以使用新功能，如“vllmvlms”后端。

https://github.com/vllm-project/vllm/issues/15911
这个issue类型是优化建议，主要对象是Neuron模块中的kv缓存。由于将KV缓存融合成单个张量可以消除KV缓存张量上的不必要切片操作，因此提出了这个优化建议。

https://github.com/vllm-project/vllm/issues/15910
这个issue类型属于需求提出，主要涉及的对象是Translation API。

https://github.com/vllm-project/vllm/issues/15909
这是一个bug报告类型的issue，主要涉及到triton kernel for eagle的输入问题。可能由于代码错误导致了此问题。

https://github.com/vllm-project/vllm/issues/15908
这是一个Bug报告，主要涉及triton kernel for preparing eagle input in CC中的bug。这个问题是由于triton kernel写入相同值到不同块中导致数值重复出现的bug。

https://github.com/vllm-project/vllm/issues/15907
这是一个bug报告，涉及到vLLM中`sampler`的问题，由于未将`torch.gather`的索引张量强制转换为`torch.int64`，导致dtype不匹配错误。

https://github.com/vllm-project/vllm/issues/15906
这是一个功能需求提议，主要涉及数据处理规模和多节点支持，通过改变端口绑定和连接方式以解决输入信号传递和连接过程中可能发生的竞态条件问题。

https://github.com/vllm-project/vllm/issues/15905
这个issue是用户提出需求，希望添加Ollama meetup slides的文档到项目中。

https://github.com/vllm-project/vllm/issues/15904
这是一个Bug报告，涉及主要对象为vllm在Sagemaker endpoint上部署的问题。这个bug产生的原因是在调用create_chat_completion()函数时缺少必要的关键字参数'raw_request'。

https://github.com/vllm-project/vllm/issues/15903
这是一个bug报告，涉及的主要对象是TPU优化的allreduce性能。原因是XLA编译器错误地应用了2Dring策略，而期望是1Dring，同时无法自动调整环的顺序。

https://github.com/vllm-project/vllm/issues/15902
这是一个增强功能的Issue，主要涉及到V1版本中的SpecDecoding Metrics 日志的修改。原因可能是为了使日志样式更符合其他日志的风格。

https://github.com/vllm-project/vllm/issues/15901
这个issue类型是需求报告，涉及VLLM项目中对EAGLE模型的支持。原因可能是EAGLE模型在V1版本中需要进行一些初始化、缓存管理、边界情况处理等方面的改进。

https://github.com/vllm-project/vllm/issues/15900
这是一个bug修复类型的issue，主要涉及到为部分量化模型中未量化层设置标度，导致某些层未被量化。

https://github.com/vllm-project/vllm/issues/15899
这是一个Bug报告，主要涉及benchmark_serving_structured_output.py文件中的问题，由于在处理json数据集时出现了错误，导致`benchmark_serving_structured_output.py`中提供的示例命令无法正常运行。

https://github.com/vllm-project/vllm/issues/15898
这是一个文档更新类型的issue，涉及主要对象为项目的使用统计语言。这个issue由于可能存在意外数据泄露和关于模型架构信息的潜在识别性而需要澄清语言。

https://github.com/vllm-project/vllm/issues/15897
这是一个用户请求添加配置和故障排除指南的类型为文档问题的issue，主要涉及到NCCL & GPUDirect RDMA的配置和故障排除。由于用户可能遇到特定错误，需要更好的指导以解决问题。

https://github.com/vllm-project/vllm/issues/15896
该issue类型为功能需求，主要对象是使能CUDA图形而无需启用torch.compile，用户希望简化模型的融合过程以提高性能。

https://github.com/vllm-project/vllm/issues/15895
这是一个Bug报告，主要涉及vllm的版本0.8.0及以上的问题。导致这个问题的原因是在运行`torchrun nprocpernode=2 torchrun_example.py`时，不同的rank输出不同的结果。

https://github.com/vllm-project/vllm/issues/15894
这是一个bug报告，涉及的主要对象是http metrics middleware。由于只钩住了`prometheus_fastapi_instrumentator`但仅适用于V0，导致了该issue的产生。

https://github.com/vllm-project/vllm/issues/15893
这是一个功能特性新增的issue，涉及到支持 AITER MLA 的集成。

https://github.com/vllm-project/vllm/issues/15892
这个issue是一个bug报告，主要涉及到CI测试中的flaky structure decoding测试，由于该测试在CI上表现不稳定，导致阻塞了许多PR的合并。

https://github.com/vllm-project/vllm/issues/15891
这是一个优化问题，主要涉及的对象是TPU-optimized top-k implementation。导致这个问题的原因是使用torch.topk导致了性能问题，而torch.scatter是原始topk实现中的瓶颈。

https://github.com/vllm-project/vllm/issues/15890
这是一个用户提出需求的issue，主要涉及到项目中的`format.sh`和`pre-commit`安装流程的简化问题。

https://github.com/vllm-project/vllm/issues/15889
这是一个需求提出的issue，涉及的主要对象是gemm3模块名称前缀。该需求要求在gemm3中添加模块名称前缀，以匹配权重中的张量名称。

https://github.com/vllm-project/vllm/issues/15888
这是一个简短的bug报告，主要涉及V1Scheduler的使用，由于缺少具体描述，无法确定具体问题和原因。

https://github.com/vllm-project/vllm/issues/15887
这是一个简短的bug报告，涉及的主要对象是V1Scheduler。这个问题出现的原因可能是在使用V1Scheduler时没有得到正确的警告提示。

https://github.com/vllm-project/vllm/issues/15886
这个issue类型是bug报告，涉及的主要对象是AsyncMetricsCollector。由于使用tensor parallelism with speculative decoding导致内存消耗过大，建议将pin_memory设置为False以显著减少内存使用。

https://github.com/vllm-project/vllm/issues/15885
这是一个请求移除不再支持的文件的问题，主要涉及到项目中的混乱和冗余文件。

https://github.com/vllm-project/vllm/issues/15884
这是一个需求类型的issue，主要涉及的对象是项目中的`format.sh`文件。由于`format.sh`文件在超过70天以上没有得到支持，导致了用户提出需要移除该文件的需求。

https://github.com/vllm-project/vllm/issues/15883
这是一个bug报告，涉及的主要对象是V1版本的TPU，因为在编译抽样时会产生多余的图形。造成这个问题的原因是在运行时不应遇到需要抽样的图形。

https://github.com/vllm-project/vllm/issues/15882
这是一个关于bug报告的issue，涉及主要对象是代码库中的方法"_set_block_scales"。由于存在未实现的方法_set_block_scales，导致了区域编译时出现问题，需要移除该方法以解决编译错误。

https://github.com/vllm-project/vllm/issues/15881
这是关于性能提升的建议，用户希望了解0.8.1版本相比0.7.4dev122为何提升了14%性能，并提供了性能测试的结果。

https://github.com/vllm-project/vllm/issues/15880
这是一个用户提出需求的issue，主要涉及对象是“Fused MoE config for Nvidia RTX 3090”。用户询问如何构建该配置或从何处开始，可能由于脚本中dtype无法设置为w4a16而导致了这个问题。

https://github.com/vllm-project/vllm/issues/15879
该issue属于功能需求提出类型，主要涉及GGUF dequantization的dtype支持的增加。

https://github.com/vllm-project/vllm/issues/15878
这个issue类型是bug报告，主要涉及对象是XGrammar，在V0版本中缺少对Enums的支持导致这个bug。

https://github.com/vllm-project/vllm/issues/15877
这是一个Bug报告，主要涉及到vllm中CPU offload功能无法正常工作的问题。导致这个问题的原因可能是配置参数设置不当或程序逻辑错误。

https://github.com/vllm-project/vllm/issues/15876
这个issue是一个bug报告，主要涉及jina-reranker-v2-base-multilingual模型中关于XLMRobertaForSequenceClassification的支持问题。导致该问题出现的原因可能是模型架构的命名不一致。

https://github.com/vllm-project/vllm/issues/15875
这是一个bug报告，主要涉及对象是vllm库中的Socket关闭功能。原因是在vllm崩溃后，端口仍然保持打开状态，用户希望通过修复这个问题来确保Socket正常关闭。

https://github.com/vllm-project/vllm/issues/15874
这是一个bug报告，主要涉及vllm和lmcache版本不匹配导致的Segmentation Fault问题。

https://github.com/vllm-project/vllm/issues/15873
这是一个用户提出需求的issue，主要涉及的对象是`huggingface_hub`的版本。这个问题是因为当前版本无法通过Xet下载模型造成下载速度慢，用户希望添加对`huggingface_hub`的最低版本要求以启用Xet下载。

https://github.com/vllm-project/vllm/issues/15872
这是一个bug报告类型的issue，涉及主要对象是构建Docker镜像。由于无法从Dockerfile构建，导致出现构建失败的症状。

https://github.com/vllm-project/vllm/issues/15871
这是一个bug报告，涉及主要对象为DeepSeek-V2-Lite-Chat。由于CPU offload功能无法正常工作，导致了程序出现错误提示并运行异常。

https://github.com/vllm-project/vllm/issues/15870
这是一个Bug报告，涉及VLLM中使用0.8.2版本比0.8.0版本速度变慢的问题，主要是由于需要编译模型导致启动速度变慢以及生成字幕速度下降。

https://github.com/vllm-project/vllm/issues/15869
该issue属于性能优化建议，主要涉及到大图像下Qwen2.5VL预处理速度过慢导致GPU利用率低的问题。

https://github.com/vllm-project/vllm/issues/15868
这个issue类型是文档修正，主要涉及的对象是代码库中的文档内容。由于链接文本错误导致的表述不准确。

https://github.com/vllm-project/vllm/issues/15867
这个issue类型为优化性修改，主要涉及的对象是LoRA测试。由于需要减少测试时间并确保覆盖所有LoRA功能，在此作出了相应修改。

https://github.com/vllm-project/vllm/issues/15866
这是一个bug报告，主要涉及的对象是代码中的错误信息处理逻辑。由于原始逻辑中无法提供有效的错误信息描述，无法进入该逻辑，因此需要在提前引发异常。

https://github.com/vllm-project/vllm/issues/15865
这是一个bug报告，主要涉及FP8精度随着长输入的减小并导致准确度下降的问题。

https://github.com/vllm-project/vllm/issues/15864
这是一个bug报告，涉及主要对象是qwen2.5omni模型，因为该模型在启动时失败了。

https://github.com/vllm-project/vllm/issues/15863
这个issue属于功能需求提议，涉及的主要对象是DPAsyncMPClient类，用户提出了支持多节点数据并行的需求。

https://github.com/vllm-project/vllm/issues/15862
这个issue属于bug报告类型，主要涉及到在Kubernetes环境下使用vLLM镜像安装时，模型检查点分片每次都重新加载的问题。产生这个问题的原因可能是与模型检查点下载及加载相关的程序逻辑或配置设置有关。

https://github.com/vllm-project/vllm/issues/15861
这是一个文档补充类的issue，主要涉及到Quark量子化文档的完善。

https://github.com/vllm-project/vllm/issues/15860
这是一个bug报告，主要涉及到VLLM的配置参数。由于使用`prompt_lookup_max`来识别"ngram"是否被使用已被弃用，而现在有了`method`参数来指示当前使用的推测方法，因此在repr字符串中应包含该信息。

https://github.com/vllm-project/vllm/issues/15859
这是一个bug报告，涉及到使用较新版本的cmake在IBM/Powerpc64le架构上导致pytorch和pyarrow构建失败的问题。

https://github.com/vllm-project/vllm/issues/15857
这是一个bug报告类型的issue，主要涉及到torch版本解析逻辑，由于在fbcode中importlib.metadata.version无法工作，需要切换到另一种兼容的方式来加载版本信息。

https://github.com/vllm-project/vllm/issues/15856
这是一个bug报告，涉及主要对象是代码中的`ssl.py`文件。该问题是由于文件名与Python标准库模块名相同导致的无法正确导入模块的错误。

https://github.com/vllm-project/vllm/issues/15855
这是一个bug报告，主要涉及V1版本下的test_llm_engine.py的一个bug。这个issue的症状是CI测试时出现flake的情况。

https://github.com/vllm-project/vllm/issues/15854
这是一个用户提出需求的类型，主要涉及到VLLM在支持结构化剪枝后，每层注意力头数目变化的问题。用户想要在VLLM上部署剪枝后的模型，可能出现此问题的原因是用户希望了解VLLM是否支持这种情况下的部署操作。

https://github.com/vllm-project/vllm/issues/15853
这是一个Bug报告，涉及的主要对象是Testla T4，问题是由于不支持FA版本2而导致了ERROR 0320。

https://github.com/vllm-project/vllm/issues/15852
这个issue类型为文档补充，涉及主要对象为Quark量子化。原因可能是文档中缺少关于Quark量子化的信息，用户希望获得更多相关文档说明。

https://github.com/vllm-project/vllm/issues/15851
这是一个bug报告，涉及的主要对象是额外的逗号。原因可能是在修复额外逗号之前和之后之间存在问题。

https://github.com/vllm-project/vllm/issues/15850
这个issue属于bug报告类型，主要涉及到在Ray中服务多节点模型时出现的客户端socket超时问题。由于rank分配在工作节点初始化期间被错误地撤销，可能导致跨工作者之间的rank分配冲突，进而导致客户端socket超时。

https://github.com/vllm-project/vllm/issues/15849
这是一个用户提出需求的issue，主要涉及的对象是两个新的embedding模型。由于缺乏对这两个模型的支持，用户向vllm团队询问是否考虑支持它们。

https://github.com/vllm-project/vllm/issues/15848
这个issue类型是bug报告，涉及主要对象是CPU MLA，由于计算错误导致了缓存块大小的问题。

https://github.com/vllm-project/vllm/issues/15847
这是一个bug报告，主要涉及的对象是vllm中的流设置及客户端流接收参数，可能由于参数json字符串连接时缺少大括号导致错误出现。

https://github.com/vllm-project/vllm/issues/15846
这个issue类型是bug报告，主要涉及的对象是Qwen2.5VL的32B inference速度异常缓慢，用户询问正常速度及可能出现的问题。

https://github.com/vllm-project/vllm/issues/15845
这是一个bug报告类型的issue，主要涉及vLLM升级后`servedmodelname`参数在响应中未被返回的问题，可能导致API响应中的模型名称与预期不符。

https://github.com/vllm-project/vllm/issues/15844
这是一个Bug报告，涉及到vLLM中在加载模型时出现ImportError导致服务无法正常启动的问题。

https://github.com/vllm-project/vllm/issues/15843
这个issue类型是用户提出需求。该问题单涉及的主要对象是V1 LoRA支持CPU offload。由于什么样的原因导致了用户需要这个功能的需求？

https://github.com/vllm-project/vllm/issues/15841
这个issue是一个bug报告，主要涉及的对象是MoE模型在CPU上的使用。导致此bug的原因是`fp8_utils`导入`triton`导致MoE模型在CPU上调用失败。

https://github.com/vllm-project/vllm/issues/15840
这是一个性能优化类的issue，主要关注的对象是dynamo guard，通过优化后提升了系统的吞吐量。

https://github.com/vllm-project/vllm/issues/15839
该issue类型为用户提出需求，主要涉及到vllm中的hacked classifier free guidance方法。这个问题可能是由于对vllm中CFG的实现不尽如人意而提出，希望能够有更好的实现方法。

https://github.com/vllm-project/vllm/issues/15838
这是一个bug报告，涉及主要对象为ROCm。由于device_type被设置为'cuda'，导致出现误导性警告。

https://github.com/vllm-project/vllm/issues/15837
这个issue类型是功能改进请求，主要涉及 SchedulerConfig 中的新增参数 `disable_chunked_mm_input`，用户提出了需要在多模态项目内部禁用分块传输的需求。

https://github.com/vllm-project/vllm/issues/15836
这是一个Bug报告，涉及的主要对象是Gemma-3 (27B)模型。由于模型加载保存的checkpoint时出现了维度大小不匹配的错误，导致了AssertionError异常。

https://github.com/vllm-project/vllm/issues/15835
这是一个bug报告，涉及主要对象为一个名为"fused_moe triton config"的配置。由于这个配置可能导致输出错误，需要将其移除。

https://github.com/vllm-project/vllm/issues/15834
这个issue属于bug报告，主要涉及的对象是TPU。由于未能正确运行fused MOE，需要进行小修复。

https://github.com/vllm-project/vllm/issues/15833
这是一个bug报告类型的issue，主要涉及的对象是vllm中的TPU V1模型。这个问题导致了执行器在一段时间后崩溃，并且服务器仍接受连接，导致API失败。

https://github.com/vllm-project/vllm/issues/15832
这是一个bug报告，主要涉及的对象是ROCm下的Kernel。由于使用平台相关的num_threads在分页注意力中，导致出现了某种症状的bug。

https://github.com/vllm-project/vllm/issues/15831
这是一个功能需求的issue，涉及到修改环境变量以控制Compiled Graph使用的通道类型。由于Ray 2.42的变化，需要更改默认使用的通道类型，以便支持调试和未来可能的拓展。

https://github.com/vllm-project/vllm/issues/15830
这个issue类型是性能优化提升，主要对象是在ROCm上进行未量化线性操作时的性能问题，由于批处理大小小于等于2时性能不佳，用户希望通过添加skinny gemms来改善性能。

https://github.com/vllm-project/vllm/issues/15829
这是一个需求变更类型的issue，主要涉及重命名fallback模型、整理支持的模型部分以及更新实用函数以反映新的命名约定。原因可能是为了提高代码可读性和易用性。

https://github.com/vllm-project/vllm/issues/15828
这是一个Bug报告，涉及的主要对象是 `MultiModalDataParser`，此问题可能由于计算 `max_num_seqs` 时引起混淆导致无法处理视频/图像分析的边缘情况。

https://github.com/vllm-project/vllm/issues/15827
这是一个Bug报告类型的issue，涉及主要对象为Docker镜像构建过程。该问题是由于Docker构建过程耗时超过5000秒且持续增加，用户寻求帮助找出原因并解决此问题。

https://github.com/vllm-project/vllm/issues/15826
这个issue属于用户提出需求类型，并主要涉及到在VLLM添加Ovis2模型。由于想要在VLLM中添加ovis架构并继续在AIDCAI/Ovis上进行讨论，因此用户提交了这个问题。

https://github.com/vllm-project/vllm/issues/15825
这是一个bug报告，主要涉及Gemini 3 27B IT模型未能读取图片所导致的问题。

https://github.com/vllm-project/vllm/issues/15824
这是一个bug报告类型的issue，主要涉及VocabParallelEmbedding中的is_embedding_layer条件错误导致的问题。

https://github.com/vllm-project/vllm/issues/15823
这是一个Bug报告，主要涉及的对象是VocabParallelEmbedding layer类的断言问题，原因是引发了一个BUG。

https://github.com/vllm-project/vllm/issues/15822
这是一个bug报告类型的issue，涉及到在x86_64 CPU上无法安装torch+cpu的问题。这个bug出现的原因是缺少对应的额外索引URL，导致用户无法成功安装所需的软件。

https://github.com/vllm-project/vllm/issues/15821
这是关于功能增强的issue，主要涉及到Intel Gaudi硬件上的自动前缀缓存支持。

https://github.com/vllm-project/vllm/issues/15820
这是一则关于构建（Build）的bug报告，涉及的主要对象是ROCm fork的base dockerfile。这个问题是由于不同的改动导致base dockerfile和requirements file不同步，可能会导致PR冲突的情况。

https://github.com/vllm-project/vllm/issues/15819
这是一个Bug报告，主要涉及VocabParallelEmbedding层中is_embedding_layer条件始终为false，导致了源代码中的一个逻辑问题。

https://github.com/vllm-project/vllm/issues/15818
这是一个bug报告，涉及VLLM的核心部分。原因是添加了未使用的speculative config，导致产生了无用的功能。

https://github.com/vllm-project/vllm/issues/15817
这是一个Bug报告，涉及的主要对象是在vLLM中无法运行使用GPTQModel量化模型。这可能是由于软件文档中所述的GPTQModel量化模型在vLLM中的支持出现问题引起的。

https://github.com/vllm-project/vllm/issues/15816
该issue类型为Bug报告，主要涉及的对象是V1模块中的多模态嵌入。在这个问题中，部分模型没有通过`get_multimodal_embeddings`输出符合预期的2D张量序列，导致在L40上OOM问题，需要修复相关模型以解决这个bug。

https://github.com/vllm-project/vllm/issues/15815
这个issue类型是bug报告，主要涉及的对象是vllm模型，在使用特定版本和参数时出现了对数函数计算方面的错误或疑惑。

https://github.com/vllm-project/vllm/issues/15814
这是一个Bug报告，主要涉及vllm Docker中运行CohereForAI/c4aicommanda032025模型时，在使用OpenAI Library请求消息时，返回额外附加的信息。

https://github.com/vllm-project/vllm/issues/15813
这个issue类型是bug报告，主要涉及的对象是vLLM下的`AutoWeightsLoader`。原因是在加载模块时遇到了批归一化统计量导致Crashing。

https://github.com/vllm-project/vllm/issues/15812
该issue属于Bug报告，涉及的主要对象是vLLM软件。由于安装环境不完整导致模块缺失，从而产生了ModuleNotFoundError错误。 

https://github.com/vllm-project/vllm/issues/15811
这是一个用户提出需求的类型，主要对象是开发者指南。因为用户希望在开发指南中推荐使用 Python 3.12，可能是因为他们希望充分利用最新版本的 Python 的特性或者功能。

https://github.com/vllm-project/vllm/issues/15810
这是一个bug报告，涉及到对象缓存中的对象没有被正确使用，导致了奇怪的代码片段。

https://github.com/vllm-project/vllm/issues/15809
这是一个性能问题的issue，主要涉及到AWQ模型在H100和A100上的表现差异，用户想了解为什么AWQ模型在H100上的表现没有比A100好两倍像GPTQ模型一样。

https://github.com/vllm-project/vllm/issues/15808
这是一个关于如何获取每一步llm_engine.step()结果的问题，涉及vllm项目的使用。原因是用户希望能够获取每一步的输出结果而不是等到运行结束才能看到全部结果。

https://github.com/vllm-project/vllm/issues/15807
这是一个bug报告，涉及主要对象为日志记录功能，由于单位错误导致模型权重显示不正确。

https://github.com/vllm-project/vllm/issues/15806
这是一个需求类型的issue，主要涉及实现XpYd基于点对点通信的动态扩展功能，其中涉及动态扩展和收缩实例的PD分离。

https://github.com/vllm-project/vllm/issues/15805
这是一个bug报告，涉及到vLLM版本0.8.1+cu121中的图像类型错误导致上传本地图像失败的问题。

https://github.com/vllm-project/vllm/issues/15804
这是一个性能问题，涉及的主要对象是服务器运行vllm过程中的性能表现，由于配置修改后性能没有提升，用户询问如何改进性能。

https://github.com/vllm-project/vllm/issues/15803
该issue为用户提出需求，主要涉及的对象是在Jupiter Jülich HPC上使用的GH200机器。由于缺少ARM机器，用户希望有人能够构建一个包含最新vLLM版本的GH200容器，以解决使用Gemma 3时出现的问题。

https://github.com/vllm-project/vllm/issues/15802
这个issue是关于bug报告，主要涉及到Qwen2.5-VL-7B模型中`computed max_num_seqs < 1`的疑惑。用户想知道如何减少mm_counts。

https://github.com/vllm-project/vllm/issues/15801
这个issue属于特性需求类型，涉及的主要对象是VLLM代码库。由于需要支持零开销调度，需要修改代码以处理同步操作，引入占位符令牌以返回虚假结果并处理实际令牌，以及修改序列类来标记有效令牌长度。

https://github.com/vllm-project/vllm/issues/15800
这是一个bug报告，主要涉及vLLM的运行时出现ChildProcessError，并且提供了当前环境信息。由于某些原因，导致vLLM在CPU上运行DeepSeek-R1-Distill-Qwen-32B benchmark时出现worker died (-9)的错误。

https://github.com/vllm-project/vllm/issues/15799
这个issue类型是功能增强，讨论了V1版本的多模态输入默认启用，涉及主要对象是V1版本的多模态输入功能。

https://github.com/vllm-project/vllm/issues/15798
这是一个功能需求issue，主要涉及VLLM项目中模型指定的问题。该issue主要针对在配置文件中指定模型的功能进行了测试，并指出了不同情况下的预期行为和测试用例。

https://github.com/vllm-project/vllm/issues/15797
这是一个bug报告，涉及的主要对象是使用minio作为S3后端的用户。由于使用了带有S3前缀的文件路径导致了错误，用户无法成功运行模型。

https://github.com/vllm-project/vllm/issues/15796
这是一个bug报告，主要涉及安装要求方面的问题。由于安装要求在setup.py文件中的表现有问题，导致安装时出现错误。

https://github.com/vllm-project/vllm/issues/15795
这个issue是关于Bug报告，主要涉及benchmark result的修复问题，由于代码中硬编码了p99 metrics导致在使用自定义的metric percentiles时出现 KeyError。

https://github.com/vllm-project/vllm/issues/15794
该issue是关于bug报告，涉及到字段在未指定savedetailed的情况下未被移除。

https://github.com/vllm-project/vllm/issues/15793
这是一个 bug 报告，主要涉及到在使用 benchmark_serving 时指定了自定义的 metric percentiles 导致 KeyError 错误。

https://github.com/vllm-project/vllm/issues/15792
这是一个bug报告，主要涉及的对象是uv环境下的collect_env功能。由于缺少pip导致collect_env无法正常工作。

https://github.com/vllm-project/vllm/issues/15791
这是一个类型为"Chore"的issue，主要涉及的对象是 xgrammar 库。由于 xgrammar 的版本升级，添加了对 minLength 和 maxLength 在 JSON schema 中的支持。

https://github.com/vllm-project/vllm/issues/15790
这是一个Bug报告，涉及的主要对象是xgrammar==0.17模块。由于代码中指定了"guided_decoding_backend": "xgrammar"，但在使用该模块时出现了问题，导致无法正确工作。

https://github.com/vllm-project/vllm/issues/15789
这个issue是关于更新actions/setup-python版本的操作，属于功能增强类型，主要涉及到版本控制和相关工作流程的调整。

https://github.com/vllm-project/vllm/issues/15788
这个issue是bug报告，主要涉及vllm phi-4无法运行的问题，可能是由于环境配置或代码错误导致。

https://github.com/vllm-project/vllm/issues/15787
这是一个关于性能优化的issue，主要对象是llama model，提出通过并行化计算和通信来减少prefill latency的问题。

https://github.com/vllm-project/vllm/issues/15786
这是一个Bug报告，涉及VLLM项目中聊天功能的多视频请求无法接收的问题。造成该问题的原因可能是代码中对多视频输入的处理不正确。

https://github.com/vllm-project/vllm/issues/15785
这个issue是一个bug报告，主要涉及GPU memory持续增长导致GPU内存溢出的问题。

https://github.com/vllm-project/vllm/issues/15784
这是一个bug报告，涉及的主要对象是尝试加载vLLM v0.7.3中quantization=modelopt的nvidia/Llama3.170BInstructFP8模型时遇到的加载失败问题。原因可能是与模型加载设置或配置有关。

https://github.com/vllm-project/vllm/issues/15783
这是一个Bug报告，该问题涉及vllm在计算gpu_memory时导致OOM错误。由于vllm版本0.7.2未能正确考虑模型权重，导致内存分配异常，进而触发OOM错误。

https://github.com/vllm-project/vllm/issues/15782
这是一个bug报告类型的issue，主要涉及torch.compile在XLA后端中的问题。由于XLA后端下，每个模型有两个FX图/字节码，需要对缓存的字节码进行处理以满足当前``实现的假设。

https://github.com/vllm-project/vllm/issues/15781
该issue属于功能需求类型，主要涉及Zero Overhead Scheduling的实现，通过实施该功能来提高性能。

https://github.com/vllm-project/vllm/issues/15780
这个issue是关于文本描述错误的问题，属于修正描述类的bug报告。主要涉及VLLM项目中的评论更新。由于描述错误，导致了用户提出需要纠正描述的问题。

https://github.com/vllm-project/vllm/issues/15779
这是一个bug报告，涉及的主要对象是VLLM和nvidia/Llama3_3NemotronSuper49Bv1模型，由于文档中支持的模型与实际测试存在不一致，导致加载模型时出现错误。

https://github.com/vllm-project/vllm/issues/15778
这个issue是一个优化建议，主要对象是调度器（Scheduler），由于每次请求都调用了`_try_schedule_encoder_inputs`函数，在没有包含编码器输入的请求中也被调用，导致了性能损失。

https://github.com/vllm-project/vllm/issues/15777
这个issue是关于bug修复的，涉及的主要对象是vLLM中调用FlashInfer 0.2.3的sampler API，由于FlashInfer 0.2.3引入了一些破坏性更改，导致该问题的产生。

https://github.com/vllm-project/vllm/issues/15776
这个issue属于bug报告类型，主要涉及更新测试部分的指示。由于错别字导致的需更正测试`requirements/dev.txt`为`requirementsdev.txt`，需要修正指导文档。

https://github.com/vllm-project/vllm/issues/15775
这是一个bug报告，涉及的主要对象是V1版本下的`RequestOutput`类，由于`metrics`变量总是为None，导致用户提出如何通过这个变量提供metrics的问题。

https://github.com/vllm-project/vllm/issues/15774
这个issue类型是特性需求提案，主要涉及添加推理模式（测试时间计算）功能。这个需求是由于优化后的推理算法使得vLLM非常适合应用于测试时间算法。

https://github.com/vllm-project/vllm/issues/15773
这个issue属于bug报告类型，主要涉及V1版本无法支持Pooling模型，可能是由于版本兼容性或功能限制引起的。

https://github.com/vllm-project/vllm/issues/15772
这是一个功能优化的issue，主要涉及到vLLM中的cascade attention机制。由于原始cascade机制在处理并行采样时存在问题，导致需要对attention逻辑进行修改和优化。

https://github.com/vllm-project/vllm/issues/15771
这是一个bug报告，涉及的主要对象是VLLM版本0.8.2，由于取消运行导致模型和GPU内存被清空。

https://github.com/vllm-project/vllm/issues/15770
这是一个bug报告类型的issue，主要对象是vllm项目中的模型加载函数。由于没有使用AutoWeightsLoader，在加载模型权重时出现了问题，导致了一些具体模型（BambaForCausalLM、ExaoneForCausalLM、FalconForCausalLM）无法正常运行。

https://github.com/vllm-project/vllm/issues/15769
这是一个Bug报告类型的Issue，主要涉及vLLM引擎在启用了V1环境变量后崩溃的问题。造成这个状况可能是由于环境配置问题导致的。

https://github.com/vllm-project/vllm/issues/15768
这个issue类型是需求，主要对象是评论的英文翻译。由于评论未使用英文，开发者体验不佳，需要对评论进行英文翻译的修复。

https://github.com/vllm-project/vllm/issues/15767
这是一个Bug报告，主要对象是lint fix a ruff checkout语法错误；这个问题很可能是由于语法错误导致lint无法正确执行的。

https://github.com/vllm-project/vllm/issues/15766
这是一个bug报告，主要涉及vllm中gemm327bit的structured output api，由于使用docker compose进行测试时出现断言错误。

https://github.com/vllm-project/vllm/issues/15765
这是一个特性增强的issue，涉及对CascadeAttention进行优化以实现性能提升。

https://github.com/vllm-project/vllm/issues/15764
这是一个Bug报告，涉及的主要对象是 vllm 下的 qwen2-vl 7b 模块。由于在 vllm 0.8.1 & 0.8.2 中有时出现了"ValueError: Attempted to assign 702 = 702 multimodal tokens to 703 placeholders"的错误，原因尚不确定，可能是数据相关。

https://github.com/vllm-project/vllm/issues/15763
这是一个Bug报告类型的Issue，主要涉及的对象是vllm模型的生成质量受gpu_memory_utilization参数影响。导致这个问题的原因是gpu_memory_utilization参数显著影响生成响应的质量。

https://github.com/vllm-project/vllm/issues/15762
这是一个Bug报告，涉及的主要对象是VLLM（Variable Language Launching Machine）。这个问题是由于xgrammar不支持enums，导致vllm无法自动回退到outlines。

https://github.com/vllm-project/vllm/issues/15761
这是一个用户提出需求的 issue，主要涉及更新 Conda 的用法链接，由于缺少此信息，用户难以获得文档。

https://github.com/vllm-project/vllm/issues/15760
这是一个关于功能需求的issue，主要涉及vllm中AWQ量化不支持专家并行性的问题，用户想在当前架构下实现专家并行性，但遇到了无法实现的错误。

https://github.com/vllm-project/vllm/issues/15759
这个issue类型是bug报告，涉及的主要对象是deepseek reasoning parser，通过修复extract_reasoning_content方法来解决https://github.com/vllmproject/vllm/issues/15758中描述的问题。

https://github.com/vllm-project/vllm/issues/15758
这是一个Bug报告，主要涉及到deepseek reasoning parser的一个bug。导致这个问题的原因是提取推理内容的逻辑错误，导致无法正确截取输出。

https://github.com/vllm-project/vllm/issues/15757
这是一个bug报告，涉及到xgrammar structured output支持Enum的问题，因为引起了CI构建失败。

https://github.com/vllm-project/vllm/issues/15756
该issue类型是用户提出需求，主要对象是新增kv connector的自定义实现，问题涉及的原因是为了更方便地使用第三方的kv connector，同时让vllm不必为每个自定义的kv connector实现负责。

https://github.com/vllm-project/vllm/issues/15755
这是一个针对bug的问题，涉及主要对象为vllm项目中的代码。原因可能是在vllm v1版本中，metric num_gpu_blocks未正确初始化，导致出现bug。

https://github.com/vllm-project/vllm/issues/15754
这是一个Bug报告，主要涉及VLLM项目中的模型类型错误导致的问题，表现为无法预处理用户输入而引发错误。

https://github.com/vllm-project/vllm/issues/15753
这是一个Bug报告类型的Issue，主要对象是在TPU V5e 8芯片上测试PHI4模型时遇到的问题。由于某个错误导致Phi4被引导到Llama并在tensor_parallel_size=8选项上失败加载。

https://github.com/vllm-project/vllm/issues/15752
这是一个bug报告，主要涉及Gemma-3-12B-it模型在文本生成过程中出现无限循环输出的问题。

https://github.com/vllm-project/vllm/issues/15751
这是一个安装问题的issue，主要涉及对象是vllm在M40 GPU上的安装情况。可能由于使用`pip install e .`命令安装时出现了问题，导致安装不成功。

https://github.com/vllm-project/vllm/issues/15750
这是一个用户提出需求的类型的issue，主要涉及N-gram Proposer Interface的更新。

https://github.com/vllm-project/vllm/issues/15749
该issue类型为用户提出需求，主要涉及的对象是项目的构建方式。由于需要摆脱 requirements/build.txt 并利用 pyproject.toml，用户提出了使用 pip wheel 构建 wheels 的建议。

https://github.com/vllm-project/vllm/issues/15748
这是一个需求更新的issue，主要涉及更新Vision Arena Dataset和HuggingFaceDataset设置，由于需要更新数据集和设置，所以提出这个问题。

https://github.com/vllm-project/vllm/issues/15747
这是一个用户提出需求的issue，主要涉及到vllm命令行工具，用户建议添加一个新的CLI PR类型，因为vllm命令有不同的参数和许多选项需要维护。

https://github.com/vllm-project/vllm/issues/15746
这是一个bug报告，主要涉及到未使用的脚本`python_only_dev.py`的移除问题。

https://github.com/vllm-project/vllm/issues/15745
这是一个用户提出需求的类型，涉及的主要对象是Docker base image。由于需要使用Ubuntu 24.04作为基础镜像，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/15744
这是一个Bug报告，涉及到ray以及vllm在部署DeepSeekR1模型时可能出现客户端套接字超时的问题。

https://github.com/vllm-project/vllm/issues/15743
这是一个需求类型的issue，主要对象是尝试在项目中使用Python 3.13版本。

https://github.com/vllm-project/vllm/issues/15742
这个issue是一个Bug报告，涉及主要对象是MLA（Machine Learning Accelerator）。由于一个pull request导致性能下降，解决方法是在处理过程中避免处理无穷大值。

https://github.com/vllm-project/vllm/issues/15741
这是一个优化建议，主要涉及到简化拒绝采样器的输出解析过程，避免不必要的集合操作。

https://github.com/vllm-project/vllm/issues/15740
该issue属于文档更新类型，主要涉及文档构建及查看。

https://github.com/vllm-project/vllm/issues/15739
这是一个需求文档类型的issue，主要涉及到v0 engine在推理输出中支持结构化输出问题，用户反馈由于只有v0 engine支持结构化输出而造成部分用户困惑。

https://github.com/vllm-project/vllm/issues/15738
这个issue属于bug报告类型，主要涉及的对象是Sliding Window Attention。由于未正确应用级联注意力，在所有查询长度下，可能会导致Sliding Window Attention的异常行为。

https://github.com/vllm-project/vllm/issues/15737
这是一个bug报告，该问题涉及的主要对象是使用Qwen2.5-VL-32B-Instruct模型时出现的torch.compile加载问题。导致这个问题的原因是模型Qwen2.5-VL-32B-Instruct不支持torch.compile。

https://github.com/vllm-project/vllm/issues/15736
这个issue是关于性能优化的，主要针对TPU上的topk和topp算法进行优化，避免使用`torch.scatter`导致性能低下。

https://github.com/vllm-project/vllm/issues/15735
这是一个特性需求的issue，主要涉及vLLM项目的路线图规划。该需求主要是为了指出在Q2 2025期间vLLM项目计划的新功能和改进方向。

https://github.com/vllm-project/vllm/issues/15734
这个issue是一个功能需求，主要涉及的对象是在VLLM下添加对FP8模型的量化支持。由于需要在FA输出后保留FP8量化以及执行FA循环时对softmax(QK^T)量化为FP8，而现有的支持仅包括Llama，因此提出了这一功能需求。

https://github.com/vllm-project/vllm/issues/15733
这个issue属于功能需求提出类型，涉及解决LoRA加载时可能产生的并发性问题，主要解决对象是`resolve_lora`函数中的`lora_name`。

https://github.com/vllm-project/vllm/issues/15732
这是一个用户提出需求的issue，主要涉及的对象是vLLM，用户希望在vLLM层面增加对滑动窗口和logit softcapping的支持，以便支持gemma模型。

https://github.com/vllm-project/vllm/issues/15731
这是一个 Bugfix issue，主要涉及 Fuyu 模型的 `get_multimodal_embedding` 函数和 `PlaceholderRange` 之间不匹配的问题，通过添加 `embed_is_patch` mask 解决。

https://github.com/vllm-project/vllm/issues/15730
这是一个Bug报告，涉及vllm库在进行多个并行查询时生成结果出现问题。原因可能是与并行处理相关的一些逻辑错误导致的。

https://github.com/vllm-project/vllm/issues/15729
这个issue是关于实现Eagle Proposer的需求，涉及到PR中使用虚拟模型和忽略抽样参数。原因是为了降低实现的复杂度并提高最终性能。

https://github.com/vllm-project/vllm/issues/15728
这是一个需求提出类型的issue，主要涉及的对象是VLLM中的Gem 3后端，由于需要增加Ultravox支持以及修改tokenizer来使用新的`` token。

https://github.com/vllm-project/vllm/issues/15727
这是一个bug报告，主要涉及的对象是Torch中的TPU模块，由于标记`dummy_hidden`为dynamic导致出现shape mismatch错误。

https://github.com/vllm-project/vllm/issues/15726
该issue类型是一个实验性的更改，不属于bug报告。该问题涉及Enabling all tests on AMD。原因是为了在AMD上启用所有测试。

https://github.com/vllm-project/vllm/issues/15725
这是一个bug报告类型的issue，主要涉及 CUDA_HOME 设置与 CMAKE_CUDA_COMPILER 的正确配置问题，导致编译时未使用指定的版本。

https://github.com/vllm-project/vllm/issues/15724
这是一个功能需求提出的issue，主要涉及到vllm的默认输出后端设置。原因是之前默认设置为`xgrammar`，现改为`auto`，以满足更多请求，并允许用户手动选择后端。

https://github.com/vllm-project/vllm/issues/15723
这是一个代码优化类型的issue，涉及对代码的简化和效率调整。

https://github.com/vllm-project/vllm/issues/15722
这是一个bug报告，涉及的主要对象是vLLM脚本的debug功能。由于PyTorch版本为2.6.0+cu124，可能导致调试脚本时出现torch错误。

https://github.com/vllm-project/vllm/issues/15721
这个issue是关于bug报告，涉及的主要对象是将结构化输出的离线推理示例更新至V1版本。由于V1版本的语法与xgrammar不兼容，导致无法工作，原因是在V0中会自动退回到outlines，而在V1中必须通过将backend设置为`auto`来选择启用后退支持。

https://github.com/vllm-project/vllm/issues/15720
这是一个性能问题的issue，涉及V1 FA的使用和ROCm Paged Attention，导致症状是延迟变化。

https://github.com/vllm-project/vllm/issues/15719
这是一个 Bug 报告类型的 issue，主要涉及 vLLM V1 中的 num_gpu_blocks metric 为 None 的问题，可能是由于 V1 版本的回归引发的。

https://github.com/vllm-project/vllm/issues/15718
这是一个优化性质的issue，主要对象是V1 structured output tests，由于每个测试案例都创建了一个新的LLM实例，导致测试速度缓慢。

https://github.com/vllm-project/vllm/issues/15717
这是一个bug报告类型的issue，主要涉及的对象是ROCm平台。这个问题产生的原因是未使用特定的FP8数据类型，导致了一些问题需要进行修复。

https://github.com/vllm-project/vllm/issues/15716
这是一个文档优化的问题，涉及修正绘图和更新注释，在V1版本中。

https://github.com/vllm-project/vllm/issues/15715
这是一个bug报告，涉及的主要对象是LoRA V1引擎。由于V1引擎将异常类型转换为通用异常导致测试失败，用户提出需要为V0和V1引擎同时运行entrypoints测试。

https://github.com/vllm-project/vllm/issues/15714
这是一个Bug修复类型的Issue，主要涉及对象是在使用w8a8 llama模型时出现的编译问题，由于`torch.ops.xla.quantized_matmul`返回的是fp32，而不是bf16，导致隐藏状态为fp32而不是bf16，需要根据实际情况对其进行处理以更好地利用TPU的能力。

https://github.com/vllm-project/vllm/issues/15713
这个issue是一个bug报告，涉及到TPU模型运行器中的chunked prompt处理bug。原因是需要清除未完全处理完整个prompt的请求中已采样的token。

https://github.com/vllm-project/vllm/issues/15712
这个issue是一个特性更新，主要涉及到V1版本中的Scatter and gather placeholders功能，并且由于需要避免干扰TPU图编译。

https://github.com/vllm-project/vllm/issues/15711
这是一个改进建议类型的issue，主要涉及到持续集成测试流程中的pytest命令配置。由于需要在CI日志中查看最慢的测试，因此建议将--durations=100参数添加到pytest命令中。

https://github.com/vllm-project/vllm/issues/15710
这个issue属于用户提出需求类型，主要涉及vLLM项目中的attention模块，由于当前不支持cascade attention for sliding window attention，用户希望扩展这一功能。

https://github.com/vllm-project/vllm/issues/15709
这是一个bug报告类型的issue，主要涉及的对象是vllm的ROCm构建。由于`cmake==4.0`更新导致构建出现问题，需要将cmake版本固定在4.0以下。

https://github.com/vllm-project/vllm/issues/15708
该issue类型为代码优化，主要涉及的对象是未使用的utils和import。

https://github.com/vllm-project/vllm/issues/15707
这是一个Bug报告类型的Issue，涉及到docker集群无法连接到服务器的问题，可能是由于无法连接而导致的错误。

https://github.com/vllm-project/vllm/issues/15706
这是一个用户报告bug类型的issue，主要涉及vllm的ROCm 6.3 docker镜像构建失败问题。用户尝试构建docker镜像是因为从源代码构建不起作用。

https://github.com/vllm-project/vllm/issues/15705
这是一个Bug报告，涉及到VLLM中使用Sliding window时出现的数值错误（ValueError）问题。

https://github.com/vllm-project/vllm/issues/15704
这是关于bug报告的issue，主要涉及Mistral guided generation using xgrammar的问题，原因是Structured Output not working with MistralTokenizer。

https://github.com/vllm-project/vllm/issues/15703
这是一个用户提出需求的类型的issue，主要对象是代码中的`mm_counts`。由于V1版本只会使用单个项目进行分析，用户希望重写`mm_counts`以使用提供的值而不是`limit_mm_per_prompt`，以为后续可配置的多模态数据做准备。

https://github.com/vllm-project/vllm/issues/15702
这是一个Bug报告，涉及到运行Deepseek V3/R1时Hopper GPUs上的Engine周期性崩溃，由于出现Floating point exception错误导致。

https://github.com/vllm-project/vllm/issues/15701
这是一个用户提出需求的类型，主要涉及文档（Documentation）。由于用户请求在故障排除中添加“生成质量变化”部分，涉及到关于生成质量变化的问题或者帮助。

https://github.com/vllm-project/vllm/issues/15700
这个issue是一个bug报告类型，涉及的主要对象是VLLM的API服务器。这个问题是由于`vllm.entrypoionts.openai.api_server`与`vllm.entrypoints.cli.main`不同步导致的。

https://github.com/vllm-project/vllm/issues/15699
这是一个bug报告类型的issue，涉及的主要对象是缺少必要的导入(import)声明。这个问题很可能是由于代码缺少导入声明而导致编译或运行出错。

https://github.com/vllm-project/vllm/issues/15698
这是一个Bug报告，涉及到VLLM项目中TP16在k8s集群上无法正常工作，可能由于环境设置不正确导致的问题。

https://github.com/vllm-project/vllm/issues/15697
这是一个用户提出需求的类型，主要涉及到vLLM中复合模型加载使用`AutoWeightsLoader`的问题，提出在所有语言backbones中应用该方法的建议。原因是目前仅有少数语言backbones实现了`load_weights`函数，希望该方法能被标准化并应用于所有语言backbones。

https://github.com/vllm-project/vllm/issues/15696
这是一个Bugfix类型的issue，主要涉及的对象是Idefics3模型。由于`get_multimodal_embedding`输出与 `PlaceholderRange`之间存在不匹配，导致出现了CC的问题，需要修复以确保模型的正确性。

https://github.com/vllm-project/vllm/issues/15695
这是一个bug报告，主要涉及llm.chat中关于bos token重复导致生成提示时出现错误的问题。

https://github.com/vllm-project/vllm/issues/15694
这个issue是关于需要增加对beam_search的支持，涉及的主要对象是MiniCPM-O模型。这个问题的原因可能是MiniCPM-O模型当前不支持beam search，导致用户得到了不正确的回应。

https://github.com/vllm-project/vllm/issues/15693
这是一个用户提出需求的issue，涉及前端开发，主要是添加聊天/完成模式的退出功能。

https://github.com/vllm-project/vllm/issues/15692
这是一个用户提出需求的issue，主要涉及的对象是使用vllm的用户，由于Python 3.10+版本需要支持numpy>=2，需要更新numba到0.61来满足这一需求。

https://github.com/vllm-project/vllm/issues/15691
这是一个bug报告类型的issue，涉及对象是"fa3 tile size"。这个问题可能是由于之前的代码中有错误或不完善导致的。

https://github.com/vllm-project/vllm/issues/15690
这是一个功能改进的issue，主要涉及的对象是 CPU Dockerfile。根据描述，这个改进主要是为了减少构建时间和镜像大小。

https://github.com/vllm-project/vllm/issues/15689
这是一个Bug报告，涉及的主要对象是MLA + chunkedprefill的性能下降问题，原因是处理方式可能导致执行效率下降。

https://github.com/vllm-project/vllm/issues/15688
这个issue是一个bug报告，涉及到MLA + chunkedprefill在处理过程中性能下降的问题。原因是一个pull request导致了性能下降，需要在`merge_attn_states_kernel`之外处理以避免影响Triton内核的生成。

https://github.com/vllm-project/vllm/issues/15687
这是一个Bug报告，主要涉及vLLM中的Worker在分布式推理期间意外退出所导致的问题。

https://github.com/vllm-project/vllm/issues/15686
这个issue是一个特性需求。它涉及到vllm中的legacy input mapper/processor的移除。由于融合input processor和input mapper的提案，需要对Phi4multimodal进行重构来支持V1版本，为了避免破坏旧的模型运行器，InputRegistry将在这个PR中保留以便它们有时间进行更新。

https://github.com/vllm-project/vllm/issues/15685
这是一个Bug报告类型的issue，涉及的主要对象是模型加载异常，由于模型类型无法被Transformers识别，导致数值错误。

https://github.com/vllm-project/vllm/issues/15684
这是一个用户提出需求的issue，主要涉及的对象是开发环境容器（DevContainer）。这个需求源于提高开发者体验，希望添加一个开发环境容器以提供更好的开发体验。

https://github.com/vllm-project/vllm/issues/15683
这个issue类型为改进需求，主要涉及移动docker文件至特定目录，由于主目录混乱，导致需要进行清理。

https://github.com/vllm-project/vllm/issues/15682
该issue属于一个功能需求类型，主要涉及在Docker中运行两个模型的问题。由于CUDA资源不足，导致无法同时运行两个模型，用户正在寻求解决方案。

https://github.com/vllm-project/vllm/issues/15681
这是一个关于是否默认使用`VLLM_WORKER_MULTIPROC_METHOD=spawn`的讨论RFC(issue)，主要涉及到PyTorch在多进程情况下重复初始化CUDA可能引发的错误。

https://github.com/vllm-project/vllm/issues/15680
这是一个bug报告，涉及到vllm中关于max_seq_length参数配置的问题，用户怀疑设置max_seq_length超过模型设置可能导致优化失败和相关性能问题。

https://github.com/vllm-project/vllm/issues/15679
这是一个用户提出需求的issue，主要涉及的对象是VLLM项目。由于论文中提到的Slim attention功能可以减少近一半的内存使用，用户认为这将更适合个人电脑使用。

https://github.com/vllm-project/vllm/issues/15678
这是一个bug报告，涉及主要对象为lmcache offload功能。由于缺少kv_transfer_config参数，导致_is_v1_supported_oracle函数在进行v1特定测试时出错。

https://github.com/vllm-project/vllm/issues/15677
这是一个bug报告，该问题涉及v0.8.2版本下Qwen2-VL-7B模型生成未完整序列的问题，可能是由于特定并发范围下导致的。

https://github.com/vllm-project/vllm/issues/15676
这个issue是关于Bug报告，涉及到vllm 0.8.2中使用Gemma 3模型部署时出现的GPU内存不足的问题。

https://github.com/vllm-project/vllm/issues/15675
这是一个Bug报告类型的Issue，主要涉及VLLM服务器无法响应请求，可能由于端口被占用导致连接超时的问题。

https://github.com/vllm-project/vllm/issues/15674
这个issue类型是bug报告，涉及的主要对象是InductorAdaptor对象，由于缺少了cache_dir属性导致错误。

https://github.com/vllm-project/vllm/issues/15673
这是一个需求提出类型的issue，主要涉及到移除遗留的输入注册表，由于输入映射器已经从V1中移除，所以需要进一步删除输入注册表。

https://github.com/vllm-project/vllm/issues/15672
这是一个关于bug报告的issue，涉及主要对象是torchrun工具和数据并行与张量并行的结合。由于设置了不匹配的nprocpernode和tensor_parallel_size参数，以及可能存在的操作超时问题，导致出现了GPU未正确分配和Watchdog超时的问题。

https://github.com/vllm-project/vllm/issues/15671
这是一个bug报告，主要涉及TPU的内存使用估计存在问题，导致性能不佳。

https://github.com/vllm-project/vllm/issues/15669
这个issue是一个Bug报告，涉及主要对象为lmcache offload功能中的`_is_v1_supported_oracle`函数。由于缺少`kv_transfer_config`参数导致功能失效，在启用V1时会复现该bug。

https://github.com/vllm-project/vllm/issues/15668
这是一个bug报告，涉及的主要对象是`mm_hashes`和`Phi4multimodal`。由于`mm_hashes`未被正确传递导致遗忘，以及`Phi4multimodal`在旧版输入处理器中使用了错误的类，导致bug产生。

https://github.com/vllm-project/vllm/issues/15667
这个issue属于bug报告类型，主要对象是TPUModelRunner测试。原因是在输入批次中对抽样元数据进行操纵时，测试做出错误假设，导致了路径错误。

https://github.com/vllm-project/vllm/issues/15666
这是一个功能更新类型的issue，涉及到需要更新到flashinfer 0.2.3版本的集成。

https://github.com/vllm-project/vllm/issues/15665
这是一个Bug报告，涉及主要对象为vLLM中的GPTQModel的动态量化功能。由于加载过程中`bit`数值未正确解析/传播，导致不同的GPTQ量化模块可能具有不同的`bit`和/或`group_size`等参数。

https://github.com/vllm-project/vllm/issues/15664
这是一个bug报告，涉及的主要对象是VLLM 0.8.2版本。由于升级到0.8.2版本后，用户在加载模型时立即出现OOM错误，而在0.7.3版本中没有这个问题。

https://github.com/vllm-project/vllm/issues/15663
这是一个Bug报告，涉及的主要对象是vllm项目的默认参数设置。由于初始性能较低，可能会导致每秒仅能处理9.8个标记，需要进一步解决优化问题。

https://github.com/vllm-project/vllm/issues/15662
这个issue属于功能需求，涉及对象为环境变量的设置功能。由于需要提供一个环境变量来禁用采样器，可能是为了解决Trtion延迟导入时的问题。

https://github.com/vllm-project/vllm/issues/15661
这是一个关于新功能开发的issue，主要对象是在macos上分发软件包。由于vLLM现在可以在macos上直接用`pip install .`来构建，用户希望能够构建wheel并发布到pypi，以便用户能够直接安装。

https://github.com/vllm-project/vllm/issues/15660
这是一个关于用户需求的问题，主要涉及如何在 vLLM 中设置加载指定模型的内容。用户希望只在用户选择特定模型时加载模型，而不是在 Docker 容器启动时就加载所有模型。

https://github.com/vllm-project/vllm/issues/15659
这个issue类型是代码更新，主要涉及对象是vLLM的TPU内核，由于最近内核的更新，需要更新代码。

https://github.com/vllm-project/vllm/issues/15658
这个issue类型是功能需求，主要涉及tool parser manager和reasoning parser manager的逻辑重构，原因可能是需要优化相关实现。

https://github.com/vllm-project/vllm/issues/15657
这个issue类型为功能需求，涉及主要对象是实现http服务性能指标监控。由于当前无法跟踪http层面的指标，用户希望通过prometheus fastapi-instrumentor来实现这一功能。

https://github.com/vllm-project/vllm/issues/15656
这是一个bug报告，主要对象是TPU上的Lazy Import功能。该问题可能是由于AsyncLLM DP的cuda库导入问题和非cuda的lazy导入问题导致的。

https://github.com/vllm-project/vllm/issues/15655
这是一个优化代码性能的issue，主要涉及硬件TPU的V1版本，用户提出了几处优化的地方，并反馈了一些待解决的问题。

https://github.com/vllm-project/vllm/issues/15654
该issue类型为用户提出需求，该问题单涉及的主要对象是对"Draft changes"的修改。由于没有提供具体内容，导致用户在编辑草稿时遇到困惑或者希望获得反馈和建议。

https://github.com/vllm-project/vllm/issues/15653
这是一个Bug报告类型的issue，主要涉及vllm server在使用`kv_transfer_config` with LMCache时出现服务器崩溃的问题，由于只使用了tensor parallel 4导致的。

https://github.com/vllm-project/vllm/issues/15652
这个issue是针对优化的需求，主要涉及了哈希KV块处理。

https://github.com/vllm-project/vllm/issues/15651
这个issue类型是功能需求，涉及到了测试基础功能的转换和并发预加载设计。

https://github.com/vllm-project/vllm/issues/15650
这个issue类型是bug报告，涉及的主要对象是在预处理中处理多个异常的错误处理重构。由于未能捕获多个异常类型（ValueError，TypeError，jinja2.TemplateError）导致了错误处理代码的重复，影响代码可维护性，需要改进。

https://github.com/vllm-project/vllm/issues/15649
这个issue类型是bug报告，涉及的主要对象是TensorFlow模型在TPU上的量化测试。由于重新编译时的问题，导致需要暂时禁用量化测试，并设置enforce_eager=False。

https://github.com/vllm-project/vllm/issues/15648
这是一个 bug 报告，主要涉及到 GPTQ/AWQ/CT 模块，在测试时需要实际的权重数据才能通过。

https://github.com/vllm-project/vllm/issues/15647
这是一个性能优化的issue，涉及更新Cascade Attention启发式算法以适配FA3，由于原先的启发式算法针对FA2优化，所以导致现在在FA3上使用不准确。

https://github.com/vllm-project/vllm/issues/15646
该问题类型为维护型，主要涉及到删除过时和未使用的TGI启动脚本，由于脚本已经过时和未使用，需要将其移除。

https://github.com/vllm-project/vllm/issues/15645
这是一个bug报告， 主要对象是vllm项目中的fused_moe kernel代码，由于使用Cache Hinting导致内部Triton PTX代码生成错误以及其他问题，需要撤销相关提交。

https://github.com/vllm-project/vllm/issues/15643
这是一个bug报告类型的issue，主要涉及VLLM在TPU V1上的正确性问题，由于默认设置下未对长文本进行chunked处理导致结果不正确。

https://github.com/vllm-project/vllm/issues/15641
这是一个关于功能改进的issue，涉及到FastAPI进行Bearer认证的替换，由于当前的API key身份验证实现无法被OpenAPI规范发现和包含，需要进一步测试。

https://github.com/vllm-project/vllm/issues/15640
这是一个bug报告，主要对象是vllm中的profiling.py脚本。用户反映在v0.8.1版本中使用该脚本时出现问题。

https://github.com/vllm-project/vllm/issues/15639
该issue是一个bug报告，主要涉及的对象是vLLM中的torch编译功能。由于在编译静态大小时出现崩溃，导致Dynamo metrics context未正确设置，引发了crash错误。

https://github.com/vllm-project/vllm/issues/15638
该issue为一个用户提出需求的问题单，主要涉及的对象是名为"Kv"的项目。

https://github.com/vllm-project/vllm/issues/15637
这个issue属于bug报告，涉及到修复Job Board中的死链接问题，可能导致用户无法访问相关页面。

https://github.com/vllm-project/vllm/issues/15636
这是一个Bug报告，涉及vLLM 0.8+版本出现的轮廓显示问题。用户在非v1标签中遇到了vLLM v1相关的错误消息。

https://github.com/vllm-project/vllm/issues/15635
该issue属于文档维护类，涉及的主要对象是PowerPC处理器到现代IBM Power处理器的转换。由于PowerPC是Power处理器系列的较旧版本且具有不同的ISA，因此需要将相关文档进行修正。

https://github.com/vllm-project/vllm/issues/15634
这是一个bug报告类型的issue，主要涉及修改规则以应用`tpu`标签。由于之前的规则导致标签被错误地应用到一些额外的PR上，因此需要添加一个额外的规则来移除未涉及任何TPU文件的PR上的标签。

https://github.com/vllm-project/vllm/issues/15633
这是一个功能需求类型的issue，主要涉及支持从s3桶直接加载LoRA适配器。原因是当前加载LoRA适配器时从s3获取数据会导致验证错误。

https://github.com/vllm-project/vllm/issues/15632
这个issue属于功能更新类型，主要涉及AMD支持的架构列表更新。这个问题的来源可能是为了保持最新的GPU架构支持情况，以便在构建时正确识别和配置支持的硬件。

https://github.com/vllm-project/vllm/issues/15631
这是一个bug报告类型的issue，涉及vllm无法加载任何LLM模型版本为v0.8.*的问题，可能是由于环境配置或代码逻辑导致的加载错误。

https://github.com/vllm-project/vllm/issues/15630
这是一个Bug报告，涉及的主要对象是vllm中的v1 flash_attn和triton_attn后端，由于缺少`get_state_cls`导致了此问题。

https://github.com/vllm-project/vllm/issues/15629
这个issue类型是用户提出需求，该问题单涉及的主要对象是项目的文档维护和贡献者。由于当前贡献者找不到入门任务，需要在 Contributing 部分直接链接到入门任务。

https://github.com/vllm-project/vllm/issues/15628
这个issue类型是用户提出需求，主要涉及的对象是网页上的“Ask AI”按钮的布局位置。原因可能是与文档版本选择器的布局冲突，导致按钮位置需要调整。

https://github.com/vllm-project/vllm/issues/15627
这是一个bug报告类型的issue，主要涉及vLLM在处理包含JSON子模式引用时出现错误的问题。原因是Pydantic生成了JSON子模式并在vLLM中引发错误。

https://github.com/vllm-project/vllm/issues/15625
这是一个bug报告，涉及的主要对象是VLLM的DeciLMConfig对象。由于新的Nvidia模型Nemotron 49Bv1在VLLM 0.7.3版本中出现了错误，报告者遇到了"DeciLMConfig object has no attribute ‘num_key_value_heads_per_layer’"的问题，并询问此错误是否与模型支持相关，或者可能出现在其自身环境中的问题。

https://github.com/vllm-project/vllm/issues/15624
这是一个bug报告，主要涉及的对象是vllm中的chatglm模型。这个问题可能是由缺少torch compile annotations导致生成的结果不合理。

https://github.com/vllm-project/vllm/issues/15623
这是一个关于如何在Gaudi上启用torchrun的问题，类型为用户需求。

https://github.com/vllm-project/vllm/issues/15622
这是一个bug报告，主要对象是vllm 0.8.2版本。由于模型质量出现严重问题，包括输出不正确的LaTeX公式以及频繁陷入循环，推测可能是由于vllm 0.8.2版本导致的问题。

https://github.com/vllm-project/vllm/issues/15621
这个issue是关于代码优化的建议，主要涉及对象是全局`mm_registry`直接访问，旨在更好地跟踪`compute_encoder_budget`的依赖关系。

https://github.com/vllm-project/vllm/issues/15620
这是一个技术性任务issue，涉及到代码重构和文档更新，主要是针对输入处理程序的更改引起的类型错误问题。

https://github.com/vllm-project/vllm/issues/15619
这是一个Bug报告，涉及的主要对象是Triton JIT Compile。由于PR 15511引入的编译回归问题，导致Triton JIT Compile出现了问题。

https://github.com/vllm-project/vllm/issues/15618
这是一个关于使用问题的issue，主要涉及vllm中deepseek输出与最终结构化内容输出的问题，可能由于参数设置或代码逻辑导致输出异常。

https://github.com/vllm-project/vllm/issues/15617
这个issue属于Bug报告类型，主要涉及到vllm0.8.2版本升级后GPU内存需求显著增加的问题。原因可能是新版本在模型运行时需要更多的GPU内存，导致OOM错误和推理速度极慢。

https://github.com/vllm-project/vllm/issues/15616
这是一个文档更新类的问题，主要涉及到安装 transformers 时所需的系统要求，由于缺少相关信息导致用户安装失败。

https://github.com/vllm-project/vllm/issues/15615
这是一个bug报告，主要涉及的对象是EngineArgs。这个issue是由于缺少`hf_token`参数导致功能异常，因此需要修复。

https://github.com/vllm-project/vllm/issues/15614
这个issue类型是bug报告，主要涉及的对象是vllm安装过程中出现数值错误。该问题可能由于参数设置不正确导致数值错误。

https://github.com/vllm-project/vllm/issues/15613
这是一个Bug报告，涉及的主要对象是在运行vllm image in k3s helm chart时出现数值格式错误导致的数值转换异常。

https://github.com/vllm-project/vllm/issues/15612
该issue属于用户提出需求类型，主要涉及的对象是要求支持Babel系列模型，由于目前不支持该系列模型，用户提出了相关支持请求。

https://github.com/vllm-project/vllm/issues/15611
这是一个功能需求的issue，主要涉及 BitsAndBytes 的 V1 支持。由于缺少 V1 的支持，用户无法使用该功能。

https://github.com/vllm-project/vllm/issues/15610
这是一个bug报告类型的issue，主要涉及的对象是vllm在T4图形卡上推理请求时出现内容为空的问题，可能是由于配置或命令参数设置不正确导致的。

https://github.com/vllm-project/vllm/issues/15609
这是一个bug报告，主要涉及VLLM调用API时设置最大并发请求数的问题，导致出现了无法识别参数"maxconcurrency 20"的错误。

https://github.com/vllm-project/vllm/issues/15608
该issue类型为功能需求，主要涉及到在多模态推断中对文本内容进行缓存以优化推断效率的问题。

https://github.com/vllm-project/vllm/issues/15607
这个issue是一个bug报告，主要涉及在使用tp size为4时无法运行deepseek v2 lite模型，但在tp size为8时可以正常运行。原因可能是与模型运行环境或代码中的参数设置相关。

https://github.com/vllm-project/vllm/issues/15606
这是一个关于如何在vllm中运行特定模型推理的询问类型的issue，主要涉及对象是dynamo模块在vllm主分支的集成情况。可能是由于用户对如何结合vllm进行推理不熟悉，才提出了这个问题。

https://github.com/vllm-project/vllm/issues/15605
这是一个用户提出需求的issue，主要涉及支持交错的模态项目，问题出现的原因是当前解决方法在处理大量交错项目时可能效率低下，需要更多工作来支持混合模态项目/交错嵌入。

https://github.com/vllm-project/vllm/issues/15604
这是一个bug报告，主要涉及VLLM的DeepSeek v2 lite模型在启用专家并行时无法正常运行的问题。由于设置tp大小为2时出现错误，但tp大小为4时正常工作，可能是由于专家并行启用时出现了错误导致。

https://github.com/vllm-project/vllm/issues/15603
这是一个bug报告，主要涉及的对象是FlashInfer backend中的KV cache tensor dimension处理，问题是由于KV cache的形状不匹配导致使用`swap_blocks`和`copy_blocks`操作时出现输出错误。

https://github.com/vllm-project/vllm/issues/15602
这个issue类型为性能改进提议，涉及的主要对象是vLLM安装和使用过程中遇到的多个大语言模型的服务问题。由于不同模型需要不同版本的transformers库，安装特定版本可能导致与其他模型的兼容性问题，可能产生性能回退或错误。

https://github.com/vllm-project/vllm/issues/15601
这是一个Bug报告，涉及的主要对象是使用Qwen2VL2B模型进行GPTQint8量化后推理速度没有改善的情况。原因可能是量化过程出现问题或者模型本身无法在推理速度上获得改进。

https://github.com/vllm-project/vllm/issues/15600
这是一个性能基准测试的issue，主要涉及对象是V1版本的Speculative Decoding模型。问题是由于性能低下，需要收集不同数据集上的性能指标，并进行性能瓶颈分析。

https://github.com/vllm-project/vllm/issues/15599
这是一个 bug 报告，主要涉及 Gemma3 GPU 在加载 gemma312 时导致 GPU 内存占用过高而持续崩溃。

https://github.com/vllm-project/vllm/issues/15598
这是一个Bug报告，主要涉及的对象是vllm模型的内存分配问题，由于CUDA内存不足导致程序出错。

https://github.com/vllm-project/vllm/issues/15597
这是一个bug报告，主要涉及vllm从0.7.0版本开始部署Qwen2_vl服务时存在内存泄漏问题，导致服务进程内存无法释放。

https://github.com/vllm-project/vllm/issues/15596
这是一个Bug报告，主要涉及模型推理过程中出现的警告，用户想了解相关参数添加位置或设计代码模块所在。可能由于参数设置或代码模块位置不明确导致影响推理速度的问题。

https://github.com/vllm-project/vllm/issues/15595
这是一个关于优化和合并缓存功能的issue。

https://github.com/vllm-project/vllm/issues/15594
这是一个bug报告，主要涉及的对象是vLLM中的structured output支持Enum。这个问题可能由于vLLM版本在guided generation / structured output方面存在严重问题，导致在/examples和structured_outputs.html文档中提供的示例无法正常工作而引起。

https://github.com/vllm-project/vllm/issues/15593
这个issue类型是用户提出需求，主要涉及的对象是API服务器的响应日志记录功能，用户希望增加一个中间件来记录API服务器的响应信息。

https://github.com/vllm-project/vllm/issues/15592
这是一个bug报告，涉及的主要对象是vllm模块。由于缺少名为`vllm._C`的模块导致了`ModuleNotFoundError`，出现了无法找到模块的错误。

https://github.com/vllm-project/vllm/issues/15591
该issue属于Bug报告类型，主要涉及XPU中的_k_scale_float/_v_scale_float在ipex_attn中的修复。由于这两个功能在操作过程中存在问题，导致bug产生。

https://github.com/vllm-project/vllm/issues/15590
这是一个Bug报告类型的issue，涉及对象为在运行DeepSeek R1 671B时遇到的问题。该问题可能由于启动命令中的参数设置不正确导致出现了问题。

https://github.com/vllm-project/vllm/issues/15589
这是一个关于bug报告的issue，主要涉及的对象是TPU（Tensor Processing Unit）。该问题由于延迟Triton导入直到需要时才导入，导致了TPU后台的故障。

https://github.com/vllm-project/vllm/issues/15588
这是一个bug报告，主要涉及的对象是前端功能中的--api-key和VLLM_API_KEY参数。该问题是由于设置了VLLM_API_KEY参数时，apikey参数被忽略导致的。

https://github.com/vllm-project/vllm/issues/15587
这是一个功能需求的issue，涉及的主要对象是vllm中的Quantization模块。这个issue提出了关于支持通道级动态token组GroupedGEMM的功能需求。

https://github.com/vllm-project/vllm/issues/15586
这个issue是一个需求提交，主要请求重新启用通过测试的AMD测试。

https://github.com/vllm-project/vllm/issues/15585
这是一个文档更新的issue，涉及的主要对象是V1用户指南。原因是根据CC启用了V1 Fp8缓存，需要更新相应文档。

https://github.com/vllm-project/vllm/issues/15584
该issue类型为需求提出，涉及的主要对象是guidance backend，由于出现了无限多的换行导致测试偶尔失败。

https://github.com/vllm-project/vllm/issues/15583
这个issue类型为bug报告，涉及主要对象是`examples/offline_inference/tpu.py`文件，由于设置`max_num_reqs`小于`MIN_NUM_SEQS`导致不能生成期望的响应。

https://github.com/vllm-project/vllm/issues/15582
这是一个需求提出的issue，主要是关于CLI中自动显示默认值的问题。

https://github.com/vllm-project/vllm/issues/15581
这个issue是一个Bug报告，涉及的主要对象是VLLM日志记录功能。由于使用VLLM_LOGGING_CONFIG_PATH导致服务器崩溃，报错为"Cannot unpickle PostGradPassManager"。

https://github.com/vllm-project/vllm/issues/15580
这是一个关于优化代码结构和功能改进的issue，主要涉及的对象是vllm项目中的server_load追踪逻辑。由于load_aware_call装饰器存在冗余，并且需要与http_middleware装饰器进行融合，因此需要对代码进行重构来实现prometheus中对server_load的跟踪。

https://github.com/vllm-project/vllm/issues/15578
这是一个bug报告，主要涉及平台检查功能被错误移除导致的问题。

https://github.com/vllm-project/vllm/issues/15577
这是一个Bug报告，涉及的主要对象是guided_json在(quantized) mistral-small模型上工作不正确。由于引导解码过程中出现了异常情况，可能与模型的量化有关导致了问题。

https://github.com/vllm-project/vllm/issues/15576
这是一个关于bug报告的issue，主要涉及的对象是在使用TP4时，与RTX 5090 GPU混合使用时出现问题。由于TP4无法正常处理RTX 5090与其他GPU混合使用，导致了一些错误和问题。

https://github.com/vllm-project/vllm/issues/15575
这是一个bug报告，主要涉及针对加载torchao量化模型时无法工作的问题。

https://github.com/vllm-project/vllm/issues/15574
这个issue属于Bug报告类型，主要涉及到多模态编码器序列虚拟数据的填充问题。由于填充额外的虚拟数据导致编码器输入长度超出预期，在Mllama中出现编码器输入长度异常导致断言错误的情况。

https://github.com/vllm-project/vllm/issues/15573
这个issue是关于功能增强的，主要涉及Bert、Blip、Blip2和Bloom模型，目的是统一支持所有模型的量化操作。

https://github.com/vllm-project/vllm/issues/15572
这个issue是一个bug报告，涉及的主要对象是vLLM的ModelConfig类。由于vLLM ModelConfig没有将hf_overrides传递给get_hf_image_processor_config，可能导致无法正确传递Hugging Face的认证令牌，需要修复这个问题。

https://github.com/vllm-project/vllm/issues/15571
这是一个用户提出需求的issue，主要对象是VLLM的日志输出功能。由于当前设置为DEBUG模式时无法看到JSON响应负载，导致开发者难以调试VLLM请求和响应，需要改进日志输出以提供更详细的信息。

https://github.com/vllm-project/vllm/issues/15570
这个issue类型是bug报告，主要涉及的对象是test_compilation.py文件。由于更新了V1 TPU代码，导致test_compilation.py无法正常工作，需要修复该问题。

https://github.com/vllm-project/vllm/issues/15569
这是一个bug报告，涉及Vllm 0.8.2与Ray 2.44的部署问题，由于环境配置问题导致Vllm回滚到V0引擎。

https://github.com/vllm-project/vllm/issues/15568
这是一个Bug报告，涉及vLLM的环境变量设置与命令行参数优先级导致API Key错误的问题。

https://github.com/vllm-project/vllm/issues/15567
这是一个关于bug的报告，主要涉及到PyTorch/XLA编译缓存的问题。原因是XLA编译缓存的key生成方式可能导致错误的编译结果被使用。

https://github.com/vllm-project/vllm/issues/15566
这是一个功能需求的issue，主要涉及在vLLM中实现Ring Attention以处理RL应用中的长序列上下文。

https://github.com/vllm-project/vllm/issues/15565
这是一个bug报告，涉及的主要对象是TPU的padding设置。这个问题是由于padding_gap设置为64导致了正确性问题，暂时通过将默认padding恢复为指数形式来解决。

https://github.com/vllm-project/vllm/issues/15564
这个issue是关于bug修复的，主要涉及到Mllama中对于交错图像输入支持的修复。导致该bug的原因可能是Mllama在处理交错图像输入时出现了错误，需要重写multiimages示例以解决问题。

https://github.com/vllm-project/vllm/issues/15563
该issue类型为用户提出需求，主要对象是支持Qwen/Qwen2.5Omni7B模型。由于该模型尚未得到vllm的支持，用户请求添加对该模型的支持。

https://github.com/vllm-project/vllm/issues/15562
这个issue类型是功能需求，主要涉及LMCache支持到vLLM的CPU版本。原因是要实现预填充和KV缓存共享。

https://github.com/vllm-project/vllm/issues/15561
这个issue是关于bug报告，主要涉及加载Qwen1.5-MoE-A2.7B模型时出现的问题。原因是用户在加载模型时遇到了错误，寻求相关帮助解决问题。

https://github.com/vllm-project/vllm/issues/15560
这是一个用户提出需求的issue，主要涉及到对于mergify.yml文件添加自动tpu标签的需求。

https://github.com/vllm-project/vllm/issues/15559
这是一个功能改进类型的issue，主要涉及`scatter_patch_features`函数的优化，以解决参数维度不一致导致需要使用hacky方法的问题。

https://github.com/vllm-project/vllm/issues/15558
这个issue是bug报告，主要对象是LoRA长上下文测试数据，可能由于PR操作不当导致数据缺失。

https://github.com/vllm-project/vllm/issues/15557
该issue类型为功能需求，主要涉及的对象是ROCm环境下的自定义PA功能，发起该需求的原因是在内核上游过程中丢失了用于触发自定义PA功能的环境变量，希望补充一个环境变量来触发自定义PA，以便强制禁用自定义PA内核并回退到默认实现。

https://github.com/vllm-project/vllm/issues/15556
这个issue是关于bug报告，主要涉及限制ray版本依赖以及更新V1版本中的PP功能警告。由于ray 2.44版本存在一个回归问题，需要限制ray版本依赖，并更新V1版本中的PP功能警告。

https://github.com/vllm-project/vllm/issues/15555
这是一个用户提出的需求，主要涉及增加端点负载指标，并在响应头中报告引擎指标的格式化选择。

https://github.com/vllm-project/vllm/issues/15554
这是一个bug报告，涉及到ngram + guided decoding的功能。由于未知原因导致出现问题，用户寻求帮助解决。

https://github.com/vllm-project/vllm/issues/15553
这是一个bug报告，涉及的主要对象是TPU，由于misconfiguration导致了recompilation的bug。

https://github.com/vllm-project/vllm/issues/15552
这个issue是关于Bugfix的类型，主要涉及的对象是Mixtral模型。由于PR 12634未进行修复，导致quantized Mixtral的qkv_proj未正确加载，从而导致输出变化的bug。

https://github.com/vllm-project/vllm/issues/15551
这个issue是一个Bug报告，涉及的主要对象是vLLM中的Structured Output功能。问题可能是由于Mistral Tokenizer无法正常支持Structured Outputs功能而导致。

https://github.com/vllm-project/vllm/issues/15550
这个issue类型是bug报告，主要涉及Transformer安装时需要uv venv --system。原因是由于引入的一个breaking change导致安装指令需要更新。

https://github.com/vllm-project/vllm/issues/15549
这是一个bug报告类型的issue，主要涉及对象是Mistral工具解析问题。导致问题的原因可能是工具解析输出与预期不符。

https://github.com/vllm-project/vllm/issues/15548
这个issue是一个Bug报告，主要涉及的对象是profile deadlock的问题，由于ray backend和num-scheduler-steps参数不匹配时出现max_tokens % num_scheduler_steps != 0导致了此bug。

https://github.com/vllm-project/vllm/issues/15547
这个issue是关于安装问题的bug报告，涉及到vllmcpu镜像发布不稳定的问题。原因可能是发布会有时成功有时失败。

https://github.com/vllm-project/vllm/issues/15546
这是一个Bug报告，主要涉及前端代码中的流工具输出丢失两个令牌的问题。原因是在处理参数时导致跳过了两个令牌值，进而在最终输出中缺少这两个值。

https://github.com/vllm-project/vllm/issues/15545
这是一个Bug报告，涉及到VLLM工具中流输出时丢失2个token的数据。原因可能是在请求qwen模型时，流结果工具会丢失2个token的数据。

https://github.com/vllm-project/vllm/issues/15544
这是一个bug报告，主要涉及的对象是Transformers后端。由于在vLLM中，`model`属性始终为`model`，因此需要修改权重加载时的前缀为`model`而不是`base_model_prefix`导致的问题。

https://github.com/vllm-project/vllm/issues/15543
这是一个Bug报告，主要涉及到vllm中的profile deadlock问题，由于max_tokens % num_scheduler_steps不等于0导致profile start/stop方法卡住。

https://github.com/vllm-project/vllm/issues/15542
这个issue类型是bug报告，涉及的主要对象是`adapters.py`中的`load_weights`方法。这个bug导致在某些情况下`loaded_params`为`None`，主要原因是`load_weights`方法未正确返回`weights`。

https://github.com/vllm-project/vllm/issues/15541
这是一个bug报告类型的issue，主要涉及的对象是对HuggingFaceTB/SmolVLM2-2B-Instruct模型的支持。问题可能是由于vllm不支持SmolVLMForConditionalGeneration模型导致的错误。

https://github.com/vllm-project/vllm/issues/15540
这是一个功能改进建议的issue，主要涉及Transformers backend中对TP验证的改进。

https://github.com/vllm-project/vllm/issues/15539
这是一个Bug报告，涉及LLAVA推断过程中出现了错误。原因是LLAVA版本1.634B在推断时出现了多模态占位符和项目长度不一致的错误。

https://github.com/vllm-project/vllm/issues/15538
这是一个用户提出需求的issue，主要涉及对象是V1版本中如何获取"num_gpu_blocks"，由于LLM和EngineCore处于不同进程中，导致用户无法通过之前的方式获取该参数。

https://github.com/vllm-project/vllm/issues/15537
这是一个Bug报告，涉及到Distributed Inference and Serving功能。由于/tmp磁盘空间超过95%导致的bug。

https://github.com/vllm-project/vllm/issues/15536
这个issue是关于bug报告，涉及的主要对象是vllm下的Qwen2.5-VL-32B模型。由于weights未从checkpoint初始化，导致了错误的症状。

https://github.com/vllm-project/vllm/issues/15535
这是一个bug报告，主要涉及前端的functioncalling功能，用户提出了FunctionDefinition缺少可选参数strict的问题。

https://github.com/vllm-project/vllm/issues/15534
该issue类型为bug报告，涉及vllm项目中视频模型类型错误导致的异常情况。

https://github.com/vllm-project/vllm/issues/15533
这是一个bug报告，主要涉及的对象是V1引擎的评估分数低。由于环境中使用的PyTorch版本不匹配，可能导致评估分数低下。

https://github.com/vllm-project/vllm/issues/15532
这是一个bug报告，主要涉及torchfix工具在修复问题时出现的错误。原因可能是torchfix未能解决所有问题导致的。

https://github.com/vllm-project/vllm/issues/15531
这是一个关于安装问题的bug报告，涉及主要对象是vLLM服务和CUDA 12.8，导致的症状是依赖冲突错误和ImportError。

https://github.com/vllm-project/vllm/issues/15530
这是一个bug报告，涉及的主要对象是triton mla kernel。这个bug是由于在batch大于1时，最后的序列有时无法得到正确结果导致的。

https://github.com/vllm-project/vllm/issues/15529
该issue类型为用户提出需求，用户需要关于如何使用vllm来运行特定模型的帮助。

https://github.com/vllm-project/vllm/issues/15528
这是一个建议改进类型的issue，主要涉及到改进示例脚本的输出。可能是为了让查看过程更加简单明了。

https://github.com/vllm-project/vllm/issues/15527
该问题属于Bug报告类型，主要涉及的对象是多模态输入处理。原因可能是缺少/不正确的tokens导致了提示替换不匹配，或是实现中存在问题。

https://github.com/vllm-project/vllm/issues/15526
这是一个Bug报告类型的issue，主要涉及的对象是FunctionDefinition类。由于缺少strict参数并在请求中指定了strict模式，导致VLLM返回了错误。

https://github.com/vllm-project/vllm/issues/15525
这是一个bug报告，涉及到VLLM_NCCL_SO_PATH环境变量对spawn worker没有效果的问题，导致输出的nccl版本不符合预期。

https://github.com/vllm-project/vllm/issues/15524
这是一个用户提出需求的类型，主要涉及的对象是关于改进推理模型处理效率的功能。由于推理模型处理时间过长，用户希望能够提供一种新特性，以改进模型中的推理过程，使之更加高效。

https://github.com/vllm-project/vllm/issues/15523
该issue属于需求提出类型，主要涉及追踪http服务错误计数，并注册到prometheus指标中，主要原因为需要监控应用状态中的错误计数。

https://github.com/vllm-project/vllm/issues/15522
这个issue是一个功能更新请求，涉及的主要对象是Dockerfile。原因是用户希望更新Dockerfile以添加tzdata，以便在所有环境中设置时区。

https://github.com/vllm-project/vllm/issues/15521
这个issue类型是需求提出，涉及的主要对象是Dockerfile。由于缺少tzdate包，用户希望更新Dockerfile以添加该包以允许设置环境变量TZ。

https://github.com/vllm-project/vllm/issues/15520
这是一个需求更新的issue，主要涉及的对象是Dockerfile，用户提出了添加tzdate包的需求。

https://github.com/vllm-project/vllm/issues/15519
这是一个Bug报告，主要涉及使用vllm部署BGE-M3嵌入模型时在Cherry Studio中上传文件时出现'400 Bad Request'错误。可能由于环境参数不匹配或后端设置问题导致此错误。

https://github.com/vllm-project/vllm/issues/15518
这是一个bug报告类型的issue，涉及的主要对象是OpenAI API连接错误。由于API连接错误导致用户无法成功调用OpenAI API，出现`APIConnectionError: Connection error`的错误提示。

https://github.com/vllm-project/vllm/issues/15517
这是一个改进性质的issue，主要涉及expert parallelism placement的优化问题，原因是当前的逻辑导致GPU利用率不佳。

https://github.com/vllm-project/vllm/issues/15516
这是一个bug报告，主要涉及数据序列在编码和解码后变短的问题。原因可能是编码和解码过程中出现了数据丢失或截断。

https://github.com/vllm-project/vllm/issues/15515
这是一个功能改进类型的issue，主要涉及到权重（weight）名称的情况，导致某些量化工具无法兼容。

https://github.com/vllm-project/vllm/issues/15514
这个issue类型为用户提出需求，主要对象是使用vllm进行分布式运行，并询问如何获取worker的运行时错误日志。由于用户想要在特定模型上运行推理，但不清楚如何与vllm集成，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/15513
这是一个Bug报告，主要涉及vllm中的加载LLM的过程中出现的NotImplementedError异常。

https://github.com/vllm-project/vllm/issues/15512
这是一个需求相关的issue，主要涉及改进自动化中更好的JSON输出。原因可能是当前JSON输出不够满足用户需求或者提高使用体验。

https://github.com/vllm-project/vllm/issues/15511
这个issue类型为特性请求，涉及的主要对象是fused_moe kernel。由于与另一个pull request的兼容性变更，用户希望使用缓存提示改进性能，主要是在延迟指标方面有轻微改善。

https://github.com/vllm-project/vllm/issues/15510
这是一个与性能优化相关的问题，涉及到使用Cache Hinting来改进fused_moe kernel，这个问题并非bug报告。

https://github.com/vllm-project/vllm/issues/15509
这是一个Bug报告，涉及的主要对象是模型的嵌入层。这个问题可能是由于模型嵌入层维度不匹配导致的。

https://github.com/vllm-project/vllm/issues/15508
这是一个bug报告，主要涉及vllm在win10 wsl2 ubuntu20.04环境下启动Qwen/Qwen2.5VL7BInstruct报错，导致数值错误导致致命信号和worker进程关闭。

https://github.com/vllm-project/vllm/issues/15507
这是一个bug报告，涉及Bitsandbytes quantized models在离线环境下无法通过huggingface缓存加载权重的问题。

https://github.com/vllm-project/vllm/issues/15506
这是一个 BugFix 类型的 issue，主要涉及的对象是 speculative decoding memory leak，由于未在 _run_no_spec 函数中释放隐藏状态 tensors 导致。

https://github.com/vllm-project/vllm/issues/15505
这是一个bug报告，涉及主要对象为vLLM中的Mistral3模型。由于批处理编码问题，导致了无法成功对vLLM V1中的chartqa进行完整评估的问题。

https://github.com/vllm-project/vllm/issues/15504
这个issue类型是用户提出需求，主要对象是添加一段推断性能基准脚本。

https://github.com/vllm-project/vllm/issues/15503
这是一个需求改进类型的issue，涉及的主要对象是LoRA长上下文测试。由于缺乏活跃用户使用该功能，因此决定删除长上下文测试。

https://github.com/vllm-project/vllm/issues/15502
该issue类型是功能增强请求，其主要涉及对象为DeepSeek V2/3模型的MoE模块。由于启用了共享专家融合功能，导致性能提升并具有降低ITL和TTLT损失的效果。

https://github.com/vllm-project/vllm/issues/15501
这是一个功能添加的issue，主要涉及DeepSeek V2/3模型的特征融合。由于特征融合功能的启用，导致性能有所提升。

https://github.com/vllm-project/vllm/issues/15500
这是一个用户提出需求的issue，主要涉及的对象是vllm项目中的wake_up()函数。这个需求是为了给wake_up()函数添加一个tags参数，以支持更好的权重更新。

https://github.com/vllm-project/vllm/issues/15499
这是一个用户提出需求的类型，主要对象涉及到引擎调度程序字段。原因是为了让IDE知道方法是什么。

https://github.com/vllm-project/vllm/issues/15498
这是一个bug报告，涉及到vllm中的硬件脚本运行失败，由于网络地址无法检索导致了IPv6错误的重复出现。

https://github.com/vllm-project/vllm/issues/15497
这个issue类型是需求提出，主要涉及的对象是`wake_up()`函数。由于需要改进在睡眠模式唤醒时的权重更新支持，用户提出了添加tags参数的需求。

https://github.com/vllm-project/vllm/issues/15496
这是一个Bug报告，涉及的主要对象是Llama-3.2-11B-Vision-Instruct模型。由于CUDA图在mllama中不受支持，导致无法生成嵌入，最终出现了错误。

https://github.com/vllm-project/vllm/issues/15495
这个issue类型为功能改进，并涉及到vLLM V0版本和paged attention kernels的移除。

https://github.com/vllm-project/vllm/issues/15494
这是一个bug报告，涉及的主要对象是vllm_compile_cache.py文件的错误文件名。这个问题是由于文件名显示错误导致的。

https://github.com/vllm-project/vllm/issues/15493
这是一个涉及修复K8s代理在CI中的问题的Issue，类型是Bug报告，主要涉及对象为Kubernetes agents。原因是K8s代理在CI中存在一些问题需要修复。

https://github.com/vllm-project/vllm/issues/15492
这是一个bug报告，涉及的主要对象是MLA chunked prefill，由于不正确处理FA2和FA3的差异导致0长度上下文的bug。

https://github.com/vllm-project/vllm/issues/15490
这是一个功能提议，主要涉及到添加新的API支持jumpforward解码。

https://github.com/vllm-project/vllm/issues/15489
这是一个bug报告，涉及的主要对象是启用了Top K优化的问题。原因是在实施新优化时，出现了预期之外的问题。

https://github.com/vllm-project/vllm/issues/15488
这是一个Bug报告，涉及主要对象是xgrammar。由于xgrammar在处理json_object输出格式时未正确验证为JSON对象而出现问题，需要升级xgrammar版本才能修复。

https://github.com/vllm-project/vllm/issues/15487
这是一个bug报告，涉及的主要对象是Model MiniCPM-V/O，原因是忘记推送Molmo修复，导致在CC中的`scatter_patch_features`功能有问题。

https://github.com/vllm-project/vllm/issues/15486
这个issue是关于bug报告，主要涉及vLLM中用户和助手在对话历史中角色顺序的限制，原因是目前要求严格交替的用户和助手角色限制了一些重要的使用场景。

https://github.com/vllm-project/vllm/issues/15485
这个issue属于性能优化类问题，主要对象是simple_connector.py文件中的GPU内存使用情况。导致这个问题的原因是使用`torch.cat`在子张量上会占用双倍GPU内存。

https://github.com/vllm-project/vllm/issues/15484
这是一个用户提出需求的issue，主要涉及的对象是multi-node offline DP+EP的功能。由于用户想要方便地运行multi-node DP+EP，所以提出了这个需求。

https://github.com/vllm-project/vllm/issues/15483
这是一个Bug报告，涉及vLLM引擎在处理聊天请求时出现崩溃和重启，并在休眠状态下加载模型的问题。

https://github.com/vllm-project/vllm/issues/15482
这是一个用户提出需求的issue，主要涉及的对象是支持不同设备的CG以提高性能，由于需要支持Ascend NPU，需要与其他两个PR一起使用。

https://github.com/vllm-project/vllm/issues/15481
这个issue是一个功能修复的问题，涉及的主要对象是LRUCache implementations。原因是为了统一不同LRUCache实现的功能，提供了新的LRUCache实现，并进行了相关的补充和重写。

https://github.com/vllm-project/vllm/issues/15480
这是一个bug报告，涉及的主要对象是vLLM软件的gguf模型类型"gemm3"未被识别，导致RuntimeError错误。

https://github.com/vllm-project/vllm/issues/15479
这是一个功能需求的issue，主要涉及到添加量化到VLLM的自定义allreduce中，用户提出了关于不同量化制度选项的需求。

https://github.com/vllm-project/vllm/issues/15478
这是一个功能改进（Feature Improvement）类型的issue，主要涉及Sampler组件。由于增加了针对128k词汇、1024批量大小的情况下的top-k优化实现，使得处理速度显著提升。

https://github.com/vllm-project/vllm/issues/15477
这是一个需求提出类型的issue，主要对象是模型Phi-4-multimodal，用户需要重构代码以支持V1版本，并处理图像和音频输入。

https://github.com/vllm-project/vllm/issues/15476
这是一个关于bugfix的issue，涉及到的主要对象是guided decoding backend设置的问题。由于在处理请求时，无法通过flag设置默认的guided decoding backend，导致用户无法成功更改默认设置。

https://github.com/vllm-project/vllm/issues/15474
这个issue类型为需求提出，主要对象可能为Richard。原因可能是用户需要与Richard进行某种互动或交流。

https://github.com/vllm-project/vllm/issues/15473
这是一个功能需求的issue，主要涉及的对象是logits processor。由于当前解决方案不尽如人意，用户希望通过添加可选参数的方式来增强logits processor，以便更好地处理数据。

https://github.com/vllm-project/vllm/issues/15472
这个issue类型是bug报告，主要涉及的对象是vllm格式检查功能。由于CI检查失败但本地通过，可能是环境差异导致的问题。

https://github.com/vllm-project/vllm/issues/15471
这是一个bug报告类型的issue，主要涉及到vLLM在与Triton 3.3.0+git95326d9f以及RTX 5090 GPU兼容性的问题，导致vLLM在与特定版本的环境下出现错误。

https://github.com/vllm-project/vllm/issues/15470
这是一个Bug报告。该问题涉及的主要对象是Qwen 2 MoE在vllm环境下的推理速度异常缓慢，可能由于某些配置设置导致。

https://github.com/vllm-project/vllm/issues/15469
该issue类型为bug报告，主要涉及对象为vllm的使用环境和运行推理任务。该问题可能是由于NCCL错误导致的无效使用，用户想要运行特定模型的推理但不清楚如何集成vllm。

https://github.com/vllm-project/vllm/issues/15468
这是一个bug报告，涉及对象为vllm中的LoRAs加载问题，可能由于读取/root/路径导致Adapter未找到的错误。

https://github.com/vllm-project/vllm/issues/15467
这是一个需求提出的issue，主要涉及到分离`TransformersModel`成基础类`TransformersModel`和LM类`TransformersForCausalLM`。这个需求的提出可能是为了提高代码的可重用性和可维护性。

https://github.com/vllm-project/vllm/issues/15466
这是一个功能优化类的issue，主要涉及到spec decode配置参数的移除和优化。

https://github.com/vllm-project/vllm/issues/15465
这是一个功能需求报告，主要针对vllm下的Embedding API dimensions功能目前不支持的问题。这个问题的根本原因是请求无法完全匹配OpenAI的格式。

https://github.com/vllm-project/vllm/issues/15463
这是一个用户提出需求的issue，主要对象是Transformers backend。由于Transformers backend已经支持V1，需要将``添加到`TransformersModel`以启用`torch.compile`。

https://github.com/vllm-project/vllm/issues/15462
这是一个bug报告，主要涉及pythonic工具解析器无法处理负数，导致解析错误。

https://github.com/vllm-project/vllm/issues/15461
这是一个Bug报告，涉及VLLM的GPU部署，在部署过程中出现容器启动失败的问题。

https://github.com/vllm-project/vllm/issues/15460
该issue类型为文档更新，主要涉及V1版本用户指南的多模态功能介绍缺失问题。可能由于V1版本中缺少对多模态功能当前状态的解释而导致用户提出补充。

https://github.com/vllm-project/vllm/issues/15459
这个issue类型为用户提出需求，主要对象是vLLM的权重预处理。由于硬件需要对预压缩的权重数据进行解压缩，当前的权重分区和压缩方法导致了性能开销较大，因此提议将权重分区和用户定义的压缩整合到一个单独的预处理工具中。

https://github.com/vllm-project/vllm/issues/15458
该issue类型是需求更改，涉及的主要对象是vllm的metrics展示。由于vllm在0.8版本中已经废弃了特定的metrics，需要隐藏这些被废弃的metrics。

https://github.com/vllm-project/vllm/issues/15457
这是一个bug报告，涉及vllm软件版本0.8.3下serve命令的错误。由于计算资源配置不足，导致无法成功运行指定的模型。

https://github.com/vllm-project/vllm/issues/15456
这个issue类型是bug报告，涉及的主要对象是vllm项目中的Macro命名。由于MoE和MMQ的宏名冲突，可能导致未来的bug。

https://github.com/vllm-project/vllm/issues/15455
这个issue是关于改善CLI帮助显示的问题，类型为用户需求反馈，主要涉及对象是CLI帮助显示功能。原因可能是希望在显示帮助时不再包含"the detected info"。

https://github.com/vllm-project/vllm/issues/15454
这是一个用户提出需求的issue，主要涉及vLLM中Whisper在处理长音频时是否支持Sequential算法的问题，用户询问了关于长音频处理的支持情况。

https://github.com/vllm-project/vllm/issues/15453
这是一个bug报告，涉及主要对象是DeepSeekV3的部署过程。由于在使用TP=16部署DeepSeekV3时出现`ValueError: not enough values to unpack (expected 22, got 21)`错误，可能是参数解析不匹配所导致的问题。

https://github.com/vllm-project/vllm/issues/15452
这是一个bug报告，主要涉及软件环境中的编译错误，导致出现了'ptxas' died due to signal 11 (Invalid memory reference)的错误。

https://github.com/vllm-project/vllm/issues/15451
这是一个用户提出需求/请教问题的issue，主要涉及vllm服务的启动问题。该问题产生的原因可能是用户需要运行特定模型的推断，但不清楚如何与vllm集成。

https://github.com/vllm-project/vllm/issues/15450
这是一个bug报告，涉及到vllm的安装问题。由于未知的运行环境导致出现了"RuntimeError: Unknown runtime environment"的错误。

https://github.com/vllm-project/vllm/issues/15449
这是一个关于环境信息收集的问题，用户提出了有关Interleaved Text/Image Format在在线推断中的疑问。原因是由于环境信息收集的过程中展示了当前环境的详细配置信息。

https://github.com/vllm-project/vllm/issues/15448
这个issue类型为功能需求，主要涉及对象是XpYd系统。这是用户提出的关于点对点通信功能的需求。

https://github.com/vllm-project/vllm/issues/15447
这个issue类型是用户提出需求，请教问题，主要涉及的对象是vllm。用户不清楚如何与vllm集成以进行特定模型的推理，可能由于缺乏相关文档或指导而导致。

https://github.com/vllm-project/vllm/issues/15446
这是一个Bug报告，主要涉及Token budget设置为128K时出现OOM错误的问题。

https://github.com/vllm-project/vllm/issues/15445
这是一个bug报告，涉及的主要对象是vLLM的模型加载过程。由于缺少相关的共享对象文件导致了头部反序列化错误，出现了错误提示 "MetadataIncompleteBuffer"，用户需要寻求帮助解决这一加载模型的问题。

https://github.com/vllm-project/vllm/issues/15444
这是一个bug报告，主要涉及LLM.collective_rpc这个功能。由于LLM.collective_rpc在v1默认情况下出现故障，导致需要通过支持LLM直接使用collective_rpc()并使用cloudpickle来确保本地函数仍然可以被序列化。

https://github.com/vllm-project/vllm/issues/15443
该issue类型为代码优化，主要涉及的对象是`scatter_patch_features`函数。这个改动是为了去除`num_embeds`参数，并通过`embeds_is_patch`的形状来推断该参数，从而简化函数实现。

https://github.com/vllm-project/vllm/issues/15442
这是一个性能问题报告，主要涉及到在不同分布式环境中QWQ-32B模型的上下文长度差异。由于部署环境的差异导致了单机环境和多机环境支持的上下文长度存在差别。

https://github.com/vllm-project/vllm/issues/15441
这是一个新模型介绍的issue，主要对象是 Aya Vision 模型，由于支持新模型的引入而被提出。

https://github.com/vllm-project/vllm/issues/15440
这是一个bug报告，主要涉及Phi-4-multimodal模块的使用中出现的问题。由于激活LoRA模块导致文本输出混乱，用户希望了解在使用Phi-4-multimodal模块时，是否应该加载LoRA模块以及如何正确加载LoRA模块。

https://github.com/vllm-project/vllm/issues/15439
这是一个bug报告类型的issue，主要涉及到vllm服务无法从硬盘和文件夹路径中运行模型。由于未正确指定模型标签参数而导致无法使用命令行启动服务。

https://github.com/vllm-project/vllm/issues/15438
这是一个功能需求的issue，主要涉及的对象是Molmo模型。由于目前Molmo模型不支持多图像，用户提出需要为Molmo模型添加多图像支持。

https://github.com/vllm-project/vllm/issues/15437
这是一个Bug报告，涉及Greedy sampling无法按预期生成相同结果的问题，用户希望找到支持Greedy sampling的模型并解决这一问题。

https://github.com/vllm-project/vllm/issues/15436
这是一个bug报告，涉及的主要对象是inductor cache on max_position_embeddings。这个问题发生的原因可能是由于代码中的某些错误导致了缓存的问题。

https://github.com/vllm-project/vllm/issues/15435
这是一个关于Bug报告的issue，涉及主要对象是vLLM在H100上构建过程中出现错误，可能是由于某个拉取请求导致构建失败。

https://github.com/vllm-project/vllm/issues/15434
这个issue属于功能需求类型，主要涉及在返回隐藏状态的选项，由于新增了一些文件来支持隐藏状态的返回设置，需要测试入口点才能确定功能是否正常。

https://github.com/vllm-project/vllm/issues/15433
这是一个功能需求的issue，主要关注于在VLLM中集成int8 scaled gemm内核并优化性能，需要在使用llmcompressor生成的压缩张量格式的模型中使用这一功能。

https://github.com/vllm-project/vllm/issues/15432
这是一个bug报告，涉及问题主要对象是加载量化模型时出现的错误。由于新功能的引入导致原有加载量化模型的方法不再适用，需要回退到旧方法以避免破坏现有模型。

https://github.com/vllm-project/vllm/issues/15431
这是一个bug报告类型的issue，主要涉及分布式测试，并且由于修复的原因导致症状的bug。

https://github.com/vllm-project/vllm/issues/15430
这是一个用户提出需求的issue，主要涉及V1版本不支持Collective RPC功能，用户希望有人实现这一功能。

https://github.com/vllm-project/vllm/issues/15429
这是一个bug报告，涉及的主要对象是DeepSeek-r1-AWQ (W4A16)模型的正常推断过程。导致异常行为的原因可能是在使用BF16时能够正常进行推断，但在使用FP16时出现异常。

https://github.com/vllm-project/vllm/issues/15428
这个issue类型是bug报告，主要涉及的对象是添加输入嵌入。由于长时间未更新相关PR，导致rebase后修复了预提交错误，问题是关于添加输入嵌入以支持在只微调嵌入层时使用的情况。

https://github.com/vllm-project/vllm/issues/15427
这个issue类型是bug报告，主要涉及的对象是在目标概率值上进行拒绝采样时，需要更新目标logits。

https://github.com/vllm-project/vllm/issues/15426
这是一个bug报告，涉及到vllm 0.7.2版本在线服务器请求未返回token使用信息的问题。

https://github.com/vllm-project/vllm/issues/15425
这是一个用户提出需求的issue，主要涉及Baichuan-Audio模型的支持，请求VLLM协助。

https://github.com/vllm-project/vllm/issues/15424
这是一个功能需求报告，主要涉及了支持新模型 glm-4-voice-9b。用户希望vllm能够支持这个新的端到端大型音频模型。

https://github.com/vllm-project/vllm/issues/15423
这是一个优化类型的issue，主要涉及到mamba2模块中冗余计算的优化，原因是观察到在不同层之间存在冗余计算。

https://github.com/vllm-project/vllm/issues/15422
这个issue是关于[Core] LoRA的调度优化，主要是关于bug报告。涉及的主要对象是LoRA Scheduler。由于当前调度逻辑无法继续扫描等待队列，导致某些请求被阻塞无法进行调度。

https://github.com/vllm-project/vllm/issues/15421
这是一个bug报告，主要涉及的对象是硬件（TPU），因为编译测试失败导致了需要跳过该测试以解除TPU CI的阻塞。

https://github.com/vllm-project/vllm/issues/15420
这是一个Bug报告，涉及主要对象是在比较`vllm`和`transformers`时，`logprobs/ranks`不匹配。这个问题可能是由于迁移RL采样到`vllm`时产生的不同`logprob`数值和缺少`BOS`标记引起的。

https://github.com/vllm-project/vllm/issues/15419
这是一个用户提出需求的类型，主要对象是v1调度器，由于需要支持并发部分预填充，需要定义long_prefill_token_threshold，并控制令牌阈值。

https://github.com/vllm-project/vllm/issues/15418
这是一个用户提出需求的 issue，主要对象是关于限制思考标记在推理模型中的使用。

https://github.com/vllm-project/vllm/issues/15417
这个issue是一个bug报告，涉及的主要对象是vllm项目中的platform插件。该bug导致plugin在v0.8.1版本无法启动，通过给platform插件添加对于V1的支持来解决这个问题。

https://github.com/vllm-project/vllm/issues/15416
这是一个功能优化类型的issue，涉及主要对象是torchcompiled softmax。由于torchcompiled softmax并未提供显著的加速效果，所以选择移除它。

https://github.com/vllm-project/vllm/issues/15415
这是一个bug报告，主要涉及的对象是Prometheus spec decode counters，问题由于数值累加导致计数异常增高。

https://github.com/vllm-project/vllm/issues/15414
该issue属于需求类型，主要涉及性能回归测试对象。由于当前性能校准是手动进行的，用户提出了添加性能回归测试来捕捉提交中的异常性能回归。

https://github.com/vllm-project/vllm/issues/15413
这个issue是一个Bug报告，主要涉及的对象是ROCm平台的mllama，在图模式下无法正常工作。这个问题是由于在CC(Enable CUDA graph support for llama 3.2 vision)中移除了某个条件导致的，需要重新添加该条件来支持LLama 3.2。

https://github.com/vllm-project/vllm/issues/15412
这个issue类型是需求更改，主要涉及工作流程和记录器。导致这个问题的原因可能是当前工作流程的顺序不当，以及记录器输出干扰。

https://github.com/vllm-project/vllm/issues/15411
这是一个Bug报告，主要涉及vLLM中Qwen2_5_VLModel模型在进行dynamic quantization时出现错误。造成这个问题的原因可能是模型实现不兼容或者缺乏vLLM实现。

https://github.com/vllm-project/vllm/issues/15409
这是一个bug报告类型的issue，涉及到硬件和TPU，主要问题是TPU Profiling在MP下的问题，由于vLLM V1中MP运行时在TPU Worker之前初始化，导致需要将性能分析器初始化延迟到开始进行性能分析。

https://github.com/vllm-project/vllm/issues/15408
这是一个用户提出需求的issue，主要涉及用户定义的聊天模板，由于缺乏警告信息导致了用户无法准确判断聊天模板的匹配情况。

https://github.com/vllm-project/vllm/issues/15407
这是一个bug报告，主要涉及了`min_tokens`功能以及多个EOS标记的模型配置，导致了`SamplingParams.__post_init__`被误用，进而造成了`eos_token_id`在编码器中丢失的问题。

https://github.com/vllm-project/vllm/issues/15406
这是一个功能需求提案，主要涉及实现嵌入模型在V1版本中。由于需要支持新的嵌入模型，并创建一个新的调度器，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/15405
这是一个需求提出的issue，主要对象是添加一个多模态开发示例，由于缺乏相关内容而需要补充。

https://github.com/vllm-project/vllm/issues/15404
这是一个bug报告，涉及的主要对象是在部署`Phi4multimodalinstruct`时遇到错误。这个问题可能是由于特定音频文件导致编码器输出长度与输入长度不匹配而引起的。

https://github.com/vllm-project/vllm/issues/15403
这是一个用户提出需求的issue，主要涉及Gemma 3模型的JSON工具调用。由于用户喜欢Qwen 2.5和Mistral模型使用JSON模板的一致性和工作效果，因此请求为Gemma 3模型引入基于JSON的工具调用。

https://github.com/vllm-project/vllm/issues/15402
该issue类型为功能更新，主要涉及Dockerfile.ppc64le的更新和基于UBI9的依赖项构建。根据描述，问题主要是关于Docker容器中OpenAI端点的验证。

https://github.com/vllm-project/vllm/issues/15401
该问题属于功能需求，主要涉及的对象是在TPU上实现引导解码的问题。可能由于无法直接在TPU上进行位掩码操作，导致需将logits移动到CPU上再返回到TPU进行抽样。

https://github.com/vllm-project/vllm/issues/15400
这个issue是关于Bug报告，涉及主要对象为LoRA加载错误，发生的原因可能是'GPUModelRunner'对象缺少'lora_manager'属性。

https://github.com/vllm-project/vllm/issues/15399
这是一个Bug报告类型的issue，主要涉及的对象是vllm在分布式环境下出现的错误。由于配置参数不匹配或设置错误导致引擎初始化时出现错误信息。

https://github.com/vllm-project/vllm/issues/15398
这个issue是关于撤销与Marlin内核相关的提交，并主要涉及处理非连续输入问题。

https://github.com/vllm-project/vllm/issues/15397
这是一个bug报告，涉及支持Skywork-R1V的问题，由于当前vllm项目在支持Skywork-R1V方面存在错误，用户提出了需要修复该问题的需求。

https://github.com/vllm-project/vllm/issues/15396
这是一个Bug报告，涉及的主要对象是vllm中的PixtralForConditionalGeneration模型。导致这个Bug的原因是PixtralForConditionalGeneration模型尚不支持BitsAndBytes量化。

https://github.com/vllm-project/vllm/issues/15395
该issue类型为用户提出需求，主要涉及添加一个警告功能，以便在用户提供与特定模型官方模板不符的聊天模板时发出警告。导致该问题的原因是当前vLLM没有在这种情况下提供警告，可能导致性能问题。

https://github.com/vllm-project/vllm/issues/15394
这是一个Bug报告，涉及主要对象为VLLM中的RequestMetrics对象。由于某些情况下无法正常访问RequestMetrics对象，导致用户提出了这个问题。

https://github.com/vllm-project/vllm/issues/15393
这是一个bug报告，主要涉及的对象是vllm下的批量嵌入推断功能。由于版本不一致或参数设置错误，导致批量嵌入的推断结果不一致。

https://github.com/vllm-project/vllm/issues/15392
这是一个修复注释的issue，属于代码质量问题，主要对象是代码注释。这个问题由于漏写或错误写注释导致。

https://github.com/vllm-project/vllm/issues/15391
这是一个Bug报告，主要涉及vllm V1 pipeline parallel与ray==2.44.0不兼容，导致cudaErrorIllegalAddress错误。

https://github.com/vllm-project/vllm/issues/15390
这是一个用户提出需求的issue，主要涉及使用vllm加载大型语言模型时如何利用不同显卡组合的问题。用户询问如何在具有不同显存的多个显卡上运行特定模型的推断。

https://github.com/vllm-project/vllm/issues/15389
这是一个bug报告，涉及的主要对象是flashinfer backend。由于flashinfer启动VLLM时报错，导致出现了"flashinfer backend, not callable NoneType object"这个bug。

https://github.com/vllm-project/vllm/issues/15388
这个issue是一个代码优化性质的需求，主要涉及到LoRA日志的删除和对PunicaWrapperGPU代码注释的改进。

https://github.com/vllm-project/vllm/issues/15387
这是一个bug报告类型的issue，主要涉及的对象是视频图像输入，问题是由于无法同时输入视频和图像导致的。

https://github.com/vllm-project/vllm/issues/15386
这是一个Bug报告，涉及的主要对象是vllm库中的DeepSeek-R1-AWQ模型。原因是在运行时将模型转换为了不正确的类型，导致了错误的使用问题。

https://github.com/vllm-project/vllm/issues/15385
这是一个Bug报告，涉及vllm在启动服务时连接GCS失败的问题。

https://github.com/vllm-project/vllm/issues/15384
这是一个用户提出需求的issue，主要对象是请求VLLM团队为bgem3嵌入模型添加对稠密和稀疏特征的支持。

https://github.com/vllm-project/vllm/issues/15383
这是一个特性相关的issue，主要涉及LRUCache的统一实现。这个问题产生的原因是为了统一不同LRUCache实现的功能，以及确保LRUCache的线程安全性，以提供更好的功能和性能。

https://github.com/vllm-project/vllm/issues/15382
这是一个bug报告，该问题涉及到VLLM代码库中的load_aware_call装饰器，由于在升级版本后引发了v1/rerank接口出现错误的现象。

https://github.com/vllm-project/vllm/issues/15381
这是一个Bug报告，主要涉及VLLM版本0.8.0和0.8.1下logprobs输出行为不同的问题。由于版本更新导致了输出结果的不一致性。

https://github.com/vllm-project/vllm/issues/15380
这是一个环境配置类的issue，主要涉及的对象是CUDA环境，由于PyTorch版本与CUDA版本不匹配导致的bug。

https://github.com/vllm-project/vllm/issues/15379
这个issue是一个功能需求，主要涉及的对象是vllm，用户提出需要支持Top-nσ sampling的功能，并指出了希望vllm也能支持mirostat、tail free sampling等功能，原因是目前的动态截断采样(SOTA performance)相对于min_p表现更好，而当前可用的sglang或transformers等方式较慢。

https://github.com/vllm-project/vllm/issues/15378
这是一个用户提出需求的issue，主要涉及在VLLM的benchmark中添加CoT数据集。由于最近推理LLMs变得更受欢迎，生成CoT导致解码阶段更长，旨在更好地评估和捕捉推理LLMs性能特点。

https://github.com/vllm-project/vllm/issues/15377
这个issue类型是bug报告，主要涉及的对象是vllm的docker镜像。由于使用了错误的依赖源，导致了在docker镜像中缺少Python导致的问题。

https://github.com/vllm-project/vllm/issues/15376
这是一个bug报告，涉及的主要对象是VLLM的Docker部署。由于Docker容器无法正常运行，可能是由于构建的Docker镜像存在错误导致。

https://github.com/vllm-project/vllm/issues/15375
这个issue属于功能需求类，并涉及LRUCache的统一功能实现。由于硬件限制，仅对Qwen2VL2BInstruct模型做了测试，尚未验证与其他模型的兼容性。

https://github.com/vllm-project/vllm/issues/15374
This issue is a bugfix related to the gptq-marlin non-contiguous error, aiming to address the non-contiguous error in the `gptq_marlin_gemm` function that occurred during the development of CC([Bug]:0.74 dev.

https://github.com/vllm-project/vllm/issues/15373
这是一个bug报告，主要涉及对象是`ray.init()`函数中的`ignore_reinit_error`参数。这个问题由于不需要 `ignore_reinit_error` 参数，因为`ray.is_initialized()`已经在最初检查了问题。

https://github.com/vllm-project/vllm/issues/15372
这是一个关于Bug报告的Issue，主要涉及的对象是vLLM在Google Colab上的安装问题。由于安装vLLM后出现RuntimeError导致无法import vllm，用户寻求解决该问题的帮助。

https://github.com/vllm-project/vllm/issues/15371
这个issue是一个bug报告，涉及版本升级后访问接口出现错误的问题。可能是由于版本升级引起的接口兼容性问题导致的错误。

https://github.com/vllm-project/vllm/issues/15370
该issue类型为需求提出，主要对象是vllm中的whisper模块的LoRA适配器，用户提出需要vllm支持whisper的LoRA适配器。

https://github.com/vllm-project/vllm/issues/15369
该issue类型属于功能改进，主要涉及vLLM的测试效率和速度问题，由于前端代码耗时较长，需要通过mocking和最小化e2e测试来加快测试速度。

https://github.com/vllm-project/vllm/issues/15368
这个issue类型是bug报告，主要对象是正则表达式的编译显示格式。由于正则表达式的编译显示格式有问题，导致产生了bug。

https://github.com/vllm-project/vllm/issues/15367
这是一个bug报告类型的issue，涉及的主要对象是V1版本中的llm_engine.model_executor。由于V1将`llm_engine.model_executor`移入后台进程导致了一些bug。

https://github.com/vllm-project/vllm/issues/15366
这是一个Bug报告类型的issue，涉及的主要对象是vllm中的模型引擎。由于一些导入错误，出现了包含torch和vllm的相关报错信息。

https://github.com/vllm-project/vllm/issues/15365
这是一个bug报告类型的issue，主要涉及的对象是vllm包中的model.llm_engine.model_executor.driver_worker.model_runner.model。原因可能是0.8.0和0.8.1版本存在的不可访问性和兼容性问题导致params无法访问以及flash attention崩溃。

https://github.com/vllm-project/vllm/issues/15364
这是一个Bug报告，主要涉及VL mm_processor_kwargs参数未被正确识别导致的问题。

https://github.com/vllm-project/vllm/issues/15363
该issue属于用户提出需求类型，主要涉及问题是关于vllm异步引擎批量请求的用法。由于当前环境下无法正确处理批量请求，导致用户无法如预期地进行请求队列处理。

https://github.com/vllm-project/vllm/issues/15362
这是一个用户提出需求的issue，主要对象是希望添加对SFREmbeddingCode2B_R embedding model的支持。可能由于vllm模型尚未支持该特定模型，导致用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/15361
这是一个bug报告类型的issue，主要涉及V1版本中的Spec Decode模块，用户提出移除N-gram警告的需求。

https://github.com/vllm-project/vllm/issues/15360
这个issue是关于bug报告，主要涉及vLLM在Torch编译过程中出现hang的问题，可能是由于在Databricks实例上运行时出现编译问题，而在本地或Dataproc中可以成功编译。

https://github.com/vllm-project/vllm/issues/15359
这个issue是一个Bug报告类型的问题，主要涉及到vllm/vllm-openai:v0.8.1基础镜像无法创建非root用户的问题。导致这个问题的原因是权限错误，导致出现了`bash: /opt/venv/bin/vllm: /opt/venv/bin/python3: bad interpreter: Permission denied`的错误提示。

https://github.com/vllm-project/vllm/issues/15358
这是一个功能改进的issue，主要涉及的对象是V1版本的Spec Decode，由于默认的N-gram参数设置不合适，导致出现了过多匹配的问题。

https://github.com/vllm-project/vllm/issues/15357
这个issue是文档更新类型，涉及到VLLM的OOM处理，用户提出了对设置`maxnumseqs`和其他选项进行更新以避免内存溢出问题。

https://github.com/vllm-project/vllm/issues/15356
这是一个Bug报告类型的Issue，该问题涉及到vllm下的Executor性能下降。由于vllm在LLMEngine层以上实现了一些缓存或优化，导致使用benchmark_serving.py进行推断时性能下降。

https://github.com/vllm-project/vllm/issues/15355
这是一个bug报告，涉及到修复由特定pull request引发的分布式数据处理组的错误。

https://github.com/vllm-project/vllm/issues/15354
这是一则关于功能需求的issue，主要涉及到vLLM V1中对CPU offloading的实现问题。由于当前实现不兼容`torch.compile`，无法支持某些功能，导致用户提出了基于UVA技术重写CPU offloading的全透明方案。

https://github.com/vllm-project/vllm/issues/15353
这是一个性能改进请求，涉及到 Kernel 中的 paged attention 模块。原因可能是为了提升该模块的性能。

https://github.com/vllm-project/vllm/issues/15352
这是一个bug报告，主要涉及使用vllm时无法找到flash attention的入口点。可能是由于修改了`flash_attn.py`但没有生效，导致无法在特定模型上运行推断。

https://github.com/vllm-project/vllm/issues/15351
这是一个性能改进的RFC（请求评论）类型的issue，主要涉及优化paged attention kernel的性能，提出对于GQA的改进。由于当前kernel设计导致对数据的多次重复加载，浪费了大量内存，希望通过引入新的kernel来优化性能。

https://github.com/vllm-project/vllm/issues/15350
这是一个bug报告，涉及到v1版本中的LLM.collective_rpc功能，测试未能成功运行且已经损坏。

https://github.com/vllm-project/vllm/issues/15349
这是一个bug报告，主要涉及到LLM.collective_rpc的问题，由于self.llm_engine.model_executor处于不同的进程，导致了该bug。

https://github.com/vllm-project/vllm/issues/15348
这是一个缺陷报告(issue)类型的问题，主要涉及V1版本中的Ngram spec解码，具体是忽略`prompt_lookup_max`而只使用`prompt_lookup_min`，导致bug的症状。

https://github.com/vllm-project/vllm/issues/15347
该issue类型为bug报告，涉及主要对象为v0.7.2 release的Wheels binary缺失，导致无法使用 'Set up using Python-only build'。

https://github.com/vllm-project/vllm/issues/15346
这是一个用户提出需求的issue，主要涉及Vllm v1 eagle proposer模块。存在的问题可能是用户想就该模块的功能性或设计方面提出建议或需求。

https://github.com/vllm-project/vllm/issues/15345
这是一个bug报告，该问题涉及vllm下的`--tokenizer-mode mistral`参数导致的OSError。

https://github.com/vllm-project/vllm/issues/15344
这是一个文档问题，主要涉及到关于环境变量中flash attention版本不明确的问题。

https://github.com/vllm-project/vllm/issues/15343
这是一个特性需求的issue，主要涉及到 `PDController` 和 `PDWorker` 的原型实现。这个需求由于想要实现 `PDController` 和 `PDWorker` 之间的消息传递和协同工作而提出。

https://github.com/vllm-project/vllm/issues/15342
这个issue是一个用户提出需求的类型，主要涉及如何在数据集上快速获取Deepseek V3生成，问题可能由于需要在大量示例上进行离线推理，所以寻求最快的方法。

https://github.com/vllm-project/vllm/issues/15341
这个issue是关于bug报告，主要涉及的对象是代码中处理IPv6地址格式的逻辑。该问题导致了构建zmq tcp:// URL时未正确处理IPv6地址格式，应该在IPv6地址周围加上方括号以区分IPv6地址和端口号。

https://github.com/vllm-project/vllm/issues/15340
这是一个Bug报告，主要涉及的对象是使用DeepSeek R1 `Q3_K_M` gguf quant模型产生了错误的输出。原因可能是模型在长时间生成后开始产生无意义的结果。

https://github.com/vllm-project/vllm/issues/15339
这是一个技术改进建议类型的issue，主要涉及的对象是vllm项目中的OpenVINO支持。该问题单由于要将OpenVINO支持移至外部插件存储库而引发。

https://github.com/vllm-project/vllm/issues/15338
这是一个Bug报告，主要对象是关于Vllm模型在多模态输入时出现了问题，可能是由于缺少或不正确的tokens导致的。

https://github.com/vllm-project/vllm/issues/15337
该issue类型为代码清理（Cleanup），主要涉及的对象是MiniCPM-V/O模型。原因可能是为了在V1上支持该模型前，先清理代码。

https://github.com/vllm-project/vllm/issues/15336
这是一个bug报告，涉及DeepSeek R1 671B的部署问题，出现了类型错误导致的数值错误。

https://github.com/vllm-project/vllm/issues/15335
这是一个bug报告，涉及的主要对象是VLLM项目下的分布式多节点环境。此问题可能是由于不兼容的显示驱动程序和CUDA驱动程序组合导致的错误803所引起的。

https://github.com/vllm-project/vllm/issues/15334
该issue是一个功能增强请求，主要涉及的对象是spec decode接口，通过修改接口以更好地适配eagle，并为eagle头部分配slots。这个修改还没有添加完整的eagle所需参数，作者期待在CC([V1][Usage] Refactor speculative decoding configuration and tests)合并后进行重排。

https://github.com/vllm-project/vllm/issues/15333
这是一个bug报告，涉及主要对象为VLLM项目。由于对象无法反序列化，导致出现了"Can't deserialize object: ObjectRef"的错误。

https://github.com/vllm-project/vllm/issues/15332
这是一个bug报告，主要涉及DeepSeek R1 H20x16 pp2, v1 engine，由于tensor大小不匹配导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/15331
这是一个用户提出需求的issue，主要涉及到在README中保留以前的新闻以便用户查找以往的meetup幻灯片，因为之前的新闻在PR#15261中被删除以缩短README的长度，导致用户反馈需求仍需要保留以前的信息。

https://github.com/vllm-project/vllm/issues/15330
这是一个性能问题报告，涉及到VLLM中的pipeline parallelism。由于pipeline parallelism中Decode阶段的性能较差，导致与tensor parallelism相比的吞吐量低。

https://github.com/vllm-project/vllm/issues/15329
这是一个bug报告，主要涉及到vllm的api_server运行环境存在问题，可能是由于optree版本过旧导致C++ pytree支持被禁用。

https://github.com/vllm-project/vllm/issues/15328
这个issue是关于bug报告的，主要涉及到`QKVCrossParallelLinear`，由于未正确处理`process_weights_after_loading`，导致了bug。

https://github.com/vllm-project/vllm/issues/15327
这是一则bug报告，涉及DeepSeeKR1Zero模型在特定环境下出现数值错误的问题。

https://github.com/vllm-project/vllm/issues/15326
这个issue是一个bug报告，涉及到vllm中请求ID重复发送的问题，可能导致EngineCore同时接收重复的请求。

https://github.com/vllm-project/vllm/issues/15325
这个issue是针对功能改进的请求，主要涉及的对象是FullAttentionSpec以及相关的类，用户提出需要添加针对`use_mla`参数的单元测试以验证内存管理和缓存大小计算，同时指出一些相关类缺乏单元测试，但由于测试对功能的影响有限，暂时跳过。

https://github.com/vllm-project/vllm/issues/15324
这是一个bug报告，涉及的主要对象是v1中的oracle插件。这个问题可能导致worker-cls和worker-extension-cls功能无法正常使用。

https://github.com/vllm-project/vllm/issues/15323
这是一个bug报告，主要涉及的对象是DP group creation，由于external_dp阻止了正常DP组的创建。

https://github.com/vllm-project/vllm/issues/15322
这是一个需求提交的issue，主要对象是优化DeepSeekR1 671B在NVIDIA L20设备上的推断性能。

https://github.com/vllm-project/vllm/issues/15321
这是一个Bug报告，主要涉及的对象是`tests/models/embedding/vision_language/test_phi3v.py`文件，由于单元测试失败导致了这个Bug。

https://github.com/vllm-project/vllm/issues/15320
这是一个功能需求提议，主要涉及的对象是V1 LoRA。这个提议的原因可能是为了提升V1版本的性能表现。

https://github.com/vllm-project/vllm/issues/15319
这是一个bug报告，涉及主要对象是Marlin kernel，由于kv/prefill caching传递了非连续的输入x到某些只操作连续内存布局的Marlin核心，导致了Crash。

https://github.com/vllm-project/vllm/issues/15318
这个issue类型为bug报告，涉及的主要对象是vllm中的async agent功能。由于async agent的更改，导致出现了某种特定症状的bug。

https://github.com/vllm-project/vllm/issues/15317
这个issue类型是代码重构，涉及的主要对象是生成语法时的后端处理，原因是已经检查后端不为空，不需要额外的断言。

https://github.com/vllm-project/vllm/issues/15316
这是一个bug报告，主要涉及 xgrammar 中缺失了 `disable-any-whitespace` 选项支持，导致 V1 中无法使用该功能。

https://github.com/vllm-project/vllm/issues/15315
该issue类型为用户提出需求，主要涉及添加生成算法功能，用户希望实现一篇论文中描述的算法以用于贪婪生成操作中。

https://github.com/vllm-project/vllm/issues/15314
这是一个用户提出需求的issue，主要涉及将vllm集成到异步引擎中批量请求推理的功能。原因可能是用户想要通过队列处理一批请求，并希望能够维护队列以避免请求接收问题。

https://github.com/vllm-project/vllm/issues/15313
这个issue类型是bug报告，主要涉及vLLM在GKE环境下声明自身健康状态却仍在编译图形的问题，导致请求被发送到还在编译中的pod并失败。

https://github.com/vllm-project/vllm/issues/15312
这是一个bug报告，主要涉及到vLLM在特定环境下因为不支持的Sampling参数而导致的崩溃问题。

https://github.com/vllm-project/vllm/issues/15311
这是一个bug报告，主要涉及到VLLM项目中的skip sampling功能，问题可能是由于代码中的错误导致hang。

https://github.com/vllm-project/vllm/issues/15310
这是一个bug报告，主要涉及的对象是guided decoding logs的更新。导致此问题的原因是在引擎以`disable_log_requests=True`启动时，敏感用户输入被记录在日志中。

https://github.com/vllm-project/vllm/issues/15309
这是一个bug报告，主要涉及的对象是TPU和V1，可能是由于XLA重编译引起了切片问题和XLA未预编译导致的bug。

https://github.com/vllm-project/vllm/issues/15308
这是一个bug报告，主要涉及V0引擎中的一个问题，由于`max_num_seqs`设置在cuda图捕获时可能被忽略，导致了一个运行时错误。

https://github.com/vllm-project/vllm/issues/15307
该issue属于需求提出类型，主要涉及V1版本的num_computed_tokens逻辑重构，并解决了token采样和请求处理过程中的问题。

https://github.com/vllm-project/vllm/issues/15306
这是一个bug报告，涉及的主要对象是VLLM_LOGGING_LEVEL环境变量。该问题是由于不区分大小写导致设置VLLM_LOGGING_LEVEL环境变量时只能使用大写字母，不能使用小写字母，造成不必要的限制。

https://github.com/vllm-project/vllm/issues/15305
这个issue属于bug报告类型，主要对象是`VLLM_LOGGING_LEVEL`环境变量命名错误。原因是命名错误可能导致日志级别设置异常或无效。

https://github.com/vllm-project/vllm/issues/15304
这个issue类型是用户提出需求，主要涉及VLLM库中的生成多个完成问题，用户请求如何为每个输入查询生成多个完成。错误可能是由于参数设置或使用方法不正确导致无法生成多个完成。

https://github.com/vllm-project/vllm/issues/15303
这是一个技术需求的issue，主要涉及通过添加InstructCoder来进行推断解码基准测试，其中通过添加了ngram proposer和benchmark_serving作为性能评估的指标。

https://github.com/vllm-project/vllm/issues/15302
这个issue是关于bug报告，主要涉及到vllm在Openshift环境中无法运行的权限问题，原因是uv默认在非系统范围内安装Python，导致非root用户无法访问。

https://github.com/vllm-project/vllm/issues/15301
这是一个bug报告，涉及主要对象为RayDistributedExecutor和DeepSeek，由于Execution in DeepSeek可能会花费很长时间，导致无法反序列化对象的问题。

https://github.com/vllm-project/vllm/issues/15300
这是一个bug报告，涉及的主要对象是vLLM中的OPEA/Mistral-Small-3.1-24B-Instruct-2503-int4-AutoRound-awq-sym模型。由于某些原因导致了错误输出以及一些警告信息的问题。

https://github.com/vllm-project/vllm/issues/15299
这是一个关于bug的报告，主要涉及到在FIPS启用的机器上使用MD5 hashing时出现的问题。

https://github.com/vllm-project/vllm/issues/15298
这个issue是一个Bug报告，主要对象是Gemma3软件。由于V1 Engine不支持Gemma3，导致了该Bug产生。

https://github.com/vllm-project/vllm/issues/15297
这是一个关于改进功能的issue，主要涉及的对象是VLLM项目中的前缀缓存功能。原因是当前的哈希函数可能导致不相关请求的块被访问，因为存在哈希冲突的情况，提出了使用SHA256替换哈希函数的改进建议。

https://github.com/vllm-project/vllm/issues/15296
这是一个bug报告的issue，涉及的主要对象是Mamba cache，在请求完成后出现了缓存释放不及时而导致槽位累积不可用的问题。

https://github.com/vllm-project/vllm/issues/15295
这个issue属于Bug报告，涉及的主要对象是Worker VllmWorkerProcess，可能是由于环境中多个NVIDIA相关库版本不兼容导致Worker进程异常退出。

https://github.com/vllm-project/vllm/issues/15294
该issue是一个Bug报告，主要涉及vLLM V1 Engine中发现的内存泄漏问题，导致在图像推断过程中RAM使用超过200GB。

https://github.com/vllm-project/vllm/issues/15293
这是一个回滚操作的issue，主要对象是提交了错误更改的特性模型配置，原因是该更改导致了不可接受的结果。

https://github.com/vllm-project/vllm/issues/15292
这是一个bug报告，主要涉及到vllm项目中的utils模块。由于未考虑到`pid`可能为`None`的情况，导致了在部分情况下出现bug。

https://github.com/vllm-project/vllm/issues/15291
这是一个Bug报告，涉及vllm中的在线服务无法同时输入视频和图像，可能由于服务器运行时请求同时输入视频和图像导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/15290
这是一个bug报告，主要涉及的对象是`transformers`库中的`PreTrainedModel`类，该问题由于`is_backend_compatible`方法在处理flex attention时导致了版本兼容性检查错误。

https://github.com/vllm-project/vllm/issues/15289
这是一个需求提出类型的issue，主要涉及的对象是向vLLM贡献新模型Qwen3和Qwen3MoE，由于作者在GitHub上已经提交了包含这些模型实现的pull request，现在希望将这些新模型添加到vLLM中。

https://github.com/vllm-project/vllm/issues/15288
这个issue类型是性能优化请求，涉及到torch sdpa F.scaled_dot_product_attention在TPU上表现低效的问题。

https://github.com/vllm-project/vllm/issues/15287
这个issue属于用户提出需求类型，主要涉及的对象是vLLM。由于需要进行稀疏实验，用户提出了希望实现GPU空闲时自动释放内存并在新实验开始时重新分配内存的功能需求。

https://github.com/vllm-project/vllm/issues/15286
这个issue是关于安装问题的bug报告，主要涉及vllm模块在Windows环境下安装成功但出现模块未找到的错误。由于可能是环境或安装配置有误导致的。

https://github.com/vllm-project/vllm/issues/15285
这是一个bug报告，主要涉及的对象是VLM中的llava processor，问题导致了TypeError错误。

https://github.com/vllm-project/vllm/issues/15284
这是一个用户提出需求的issue，涉及对象是vllm的docker image，由于缺少ray命令，用户寻求关于在CPU模式下进行分布式推理和serving的帮助。

https://github.com/vllm-project/vllm/issues/15283
这是一个关于修复bug的issue，主要涉及到 VLLM 中 CUDA 文档中缺少了 ray[data] 依赖信息，导致在 0.8.0 版本中 Ray 无法找到模型 pyarrow 和 pandas。

https://github.com/vllm-project/vllm/issues/15282
这是一个bug报告，主要涉及v1 Triton backend的内核测试问题，导致在重新基础时缺少重命名操作。

https://github.com/vllm-project/vllm/issues/15281
这是一个关于bug报告的issue，涉及的主要对象是vLLM中的`generation_config`加载，默认参数被覆盖导致用户困惑以及温度参数被忽略的现象。

https://github.com/vllm-project/vllm/issues/15280
这是一个关于bug报告的issue，主要涉及的对象是vllm库中get_flash_attn_version模块的import路径错误导致MLA启用时抛出错误。

https://github.com/vllm-project/vllm/issues/15279
这是一个bug报告，主要涉及解析顺序错误导致的transformers fallback问题。

https://github.com/vllm-project/vllm/issues/15278
这是一个bug报告，主要涉及到torch.compile功能，由于设置了环境变量VLLM_DISABLE_COMPILE_CACHE为1时，会导致torch.compile抛出FileNotFoundError异常。

https://github.com/vllm-project/vllm/issues/15277
这是一个Bug报告，主要涉及的对象是vllm中的DeepSeek2 GGUF模型，由于版本0.8.1还不支持该模型的架构，导致出现问题。

https://github.com/vllm-project/vllm/issues/15276
这是一个Bug报告，主要涉及vLLM在设置VLLM_DISABLE_COMPILE_CACHE=1时出现FileNotFoundError错误，导致编译错误。

https://github.com/vllm-project/vllm/issues/15275
这是一个Bug报告类型的issue，主要涉及到int8稀疏计算时间比fp8长的问题，可能是由于计算时间不同导致。

https://github.com/vllm-project/vllm/issues/15274
这是一个Bug报告，涉及vllm下的工具调用中参数丢失的问题，可能由于参数表述不正确导致。

https://github.com/vllm-project/vllm/issues/15273
这是一个Bug修复类型的Issue，主要涉及Qwen2.5VL的注意力掩码预计算优化的问题。CC([Bugfix] Fix incorrect qwen2.5vl attention mask precomputation导致了注意力掩码预计算错误。

https://github.com/vllm-project/vllm/issues/15272
这是关于代码输出不一致的bug报告，涉及的主要对象是chat_template参数。原因可能是模板中的条件判断逻辑导致了不一致的输出。

https://github.com/vllm-project/vllm/issues/15271
该issue类型为功能优化，主要涉及的对象是调度器（Scheduler）。这个改动是为了引入一个公共的状态类 `CommonSchedulerStates`，其中包含了两个状态变量，目的是将这些状态移动到一个单独的文件中，以提高代码整洁度，但不会影响功能或性能。

https://github.com/vllm-project/vllm/issues/15270
这是一个空白的issue，类型为待定，无法确定问题的主要对象或具体内容。

https://github.com/vllm-project/vllm/issues/15269
该issue为Bug报告，主要涉及VLLM启动时出现的RuntimeError问题，导致的原因是tensor维度不匹配。

https://github.com/vllm-project/vllm/issues/15268
这是一个用户提出需求的类型，主要涉及到知识库检索。由于用户想要在知识库中基于用户输入搜索相关信息，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/15267
这个issue是关于bug报告，主要涉及对象是`collect_env.py`中的版本解析。由于版本解析方式有误，导致了解析结果不正确。

https://github.com/vllm-project/vllm/issues/15266
这是一个用户提出需求的issue，主要涉及对象是vllm在Ray cluster上支持CPU推理，由于当前环境下无法正确利用集群资源导致无法完成CPU推理任务。

https://github.com/vllm-project/vllm/issues/15265
这是一个Bug报告，涉及的主要对象是VLLM（Very Large Language Model）。由于无法导入模块中的特定函数，导致了报错提示和异常症状。

https://github.com/vllm-project/vllm/issues/15264
这是一个Bug报告，问题涉及的主要对象是vllm库。由于加载权重时出现异常，导致引擎核心无法正常初始化。

https://github.com/vllm-project/vllm/issues/15263
这是一个bug报告类型的issue，主要涉及到Oracle设备检查异常的问题，可能是由于强制性的错误导致的。

https://github.com/vllm-project/vllm/issues/15262
该issue类型为用户提出需求，问题涉及的主要对象为VLLM 0.8，用户提出需要一个示例来确保实验的可复现性。

https://github.com/vllm-project/vllm/issues/15261
这是一个文档问题，主要涉及的对象是README文件。由于最新消息未被截断导致文档过长，用户希望对最新消息进行修剪。

https://github.com/vllm-project/vllm/issues/15260
这是一个bug报告，涉及到VLLM中的KVCache模块，用户提出需要添加int8 torch dtype的需求，因为当前的STR_DTYPE_TO_TORCH_DTYPE字典中缺少int8类型。

https://github.com/vllm-project/vllm/issues/15259
这是一个Bugfix类型的Issue，主要涉及多序列logprobs流处理中的边界情况。原因是当一个序列在其他序列之前完成时，其后续步骤的`output_token_ids`将变成空数组，导致在对logprobs数组进行切片时出现了问题。

https://github.com/vllm-project/vllm/issues/15258
这是一个Bug报告，主要涉及VLLM中的内存分配问题导致OOM错误。

https://github.com/vllm-project/vllm/issues/15257
这是一个Bug报告，涉及的主要对象是vllm项目中的测试代码。原因是`tests/v1/tpu/test_sampler.py`中的`ragged_paged_attention`参数不匹配，导致崩溃的问题。

https://github.com/vllm-project/vllm/issues/15256
这是关于bug报告的issue，主要涉及openai-agents sdk中使用Runner.run_streamed()时发生了函数调用错误。原因可能是环境配置或代码逻辑的问题导致了此bug。

https://github.com/vllm-project/vllm/issues/15255
这个issue是一个Bug报告，涉及的主要对象是关于CUDA设备的配置。由于环境中存在不同的CUDA设备，未设置`CUDA_DEVICE_ORDER=PCI_BUS_ID`可能导致`--tensor-parallel-size`错误。

https://github.com/vllm-project/vllm/issues/15254
这个issue属于需求提出类型，主要涉及RLHF中使用sleep mode和wake_up()时遇到的内存OOM问题，由于新旧模型权重、kv缓存等都存在于同一个GPU上导致。

https://github.com/vllm-project/vllm/issues/15253
该issue为性能问题，主要涉及V0和V1推理吞吐量相同的现象，原因可能是预取掩盖了数据交换的开销，保持了高GPU利用率。

https://github.com/vllm-project/vllm/issues/15252
这是一个功能需求的issue，主要涉及VLLM的解码引擎不支持指定的配置参数，导致无法使用特定配置参数导致的问题。

https://github.com/vllm-project/vllm/issues/15251
这个issue是关于Bug报告，涉及的主要对象是VLLM软件版本0.8.1，可能由于某次更新导致了功能回归。

https://github.com/vllm-project/vllm/issues/15250
该issue类型为功能拓展，主要涉及对象为V1 scheduler。由于代码重构需要，引入了SchedulerInterface类，没有预期功能或性能改变。

https://github.com/vllm-project/vllm/issues/15249
这是一个Bug报告，主要对象是torch.compile中的UUID系统，由于在2.6中移除了用于缓存的UUID串行化导致无法反串行化PostGradPassManager，因此报告出现了此问题。

https://github.com/vllm-project/vllm/issues/15248
这是关于bug报告的issue，主要涉及对象是PyTorch cache的清理问题，用户提出由于内存使用问题导致需要在将实例置于休眠状态后立即清除PyTorch缓存。

https://github.com/vllm-project/vllm/issues/15246
这是一个bug报告，主要涉及的对象是关于ROCm（Radeon Open Compute）和CUDA的设备可见性设置，由于RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES和HIP_VISIBLE_DEVICES设置导致了冲突问题。

https://github.com/vllm-project/vllm/issues/15245
这个issue类型为bug报告，主要对象涉及LoRA测试失败，由于测试失败出现了症状问题。

https://github.com/vllm-project/vllm/issues/15244
这个issue属于升级需求类型，涉及主要对象为torch和ROCm环境。原因由于需要在ROCm上升级torch至2.6版本，导致相关更新的需求提出。

https://github.com/vllm-project/vllm/issues/15243
这个issue类型为功能增强，涉及主要对象为V1模型，因为用户需要添加一个禁用级联注意力的标志，以避免潜在的数值问题。

https://github.com/vllm-project/vllm/issues/15242
这是一个性能优化的issue，主要涉及的对象是通过在TPU上使用`torch.topk`来加速top-k操作。由于在TPU上使用离散的torch操作实现topk效率低下，使用`torch.topk`能够显著提升性能。

https://github.com/vllm-project/vllm/issues/15241
这是一个bug报告，主要涉及vLLM版本0.8.0和0.8.1中温度参数被忽略导致的问题。

https://github.com/vllm-project/vllm/issues/15240
该问题类型为功能需求，主要对象是通过OpenAI客户端传递vLLM专用参数。由于需要更好的默认值以匹配Hugging Face，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/15238
这是一个Bug报告，涉及的主要对象是vllm模型。由于某些模块调用错误导致无法运行vllm模型。

https://github.com/vllm-project/vllm/issues/15237
这是一个需求类型的issue，主要涉及vLLM对OpenAI Response API的支持。由于OpenAI推出了新的Response API，用户提出希望vLLM也能添加对该API的支持。

https://github.com/vllm-project/vllm/issues/15236
这是一个Bug报告类型的Issue，涉及vLLM中的guided generation / structured output功能，用户遇到了运行示例代码时出现的多个错误。由于提供的JSON架构特性不被xgrammer支持，导致了错误消息"BadRequestError: Error code: 400"的问题。

https://github.com/vllm-project/vllm/issues/15235
这是一个Bug报告，主要涉及的对象是使用vllm库进行模型训练的用户。出现这个问题的原因可能是PyTorch版本与vllm库不兼容导致的错误。

https://github.com/vllm-project/vllm/issues/15234
这个issue属于bug报告类型，主要涉及 pip-compile 工具在设置 Python 版本时与 Docker 镜像不匹配，可能导致生成的依赖文件与环境不一致。

https://github.com/vllm-project/vllm/issues/15233
这个issue是一个bug报告, 主要涉及的对象是stream response中的include prompt, 由于使用completions API时设置了stream=true和echo=true导致出现BadRequestError(400)。

https://github.com/vllm-project/vllm/issues/15232
这是一个Bug报告，主要涉及到Whisper（语音转写）功能无法正常工作，可能由于参数配置或代码逻辑导致的错误。

https://github.com/vllm-project/vllm/issues/15231
这是一个Bug报告，涉及的主要对象是FlashAttention模块。由于Flash Attention 3不支持ALiBi编码，导致在尝试使用带有ALiBi的模型时出现错误。

https://github.com/vllm-project/vllm/issues/15230
这是一个bug报告类型的issue，主要涉及vllm中应用QwQ32B Chat Template时出现的TemplateSyntaxError，可能是由于模板中引号引发的语法错误。

https://github.com/vllm-project/vllm/issues/15228
这是一个Bug报告，涉及VLLM中Qwen2.5版本在0.8.0和0.8.1中出现内存错误的问题。

https://github.com/vllm-project/vllm/issues/15226
该issue类型为改进提议，主要涉及将`misc`问题替换为指向论坛的链接。由于问题数量和推广论坛的原因导致了这样的提议。

https://github.com/vllm-project/vllm/issues/15225
这是一个功能需求类型的issue，主要对象是vLLM实例。由于vLLM实例冷启动时间过长，用户提出了能够预热vLLM实例以减少启动时间的功能需求。

https://github.com/vllm-project/vllm/issues/15224
这是一个用户提出的需求，主要涉及将tracing依赖项包含在vllm容器镜像中。原因是为了避免在测试流程中手动安装这些依赖项。

https://github.com/vllm-project/vllm/issues/15223
这是一个bug报告，涉及的主要对象是在尝试从python sdk扩展OpenAI兼容服务器时遇到的无法反序列化PostGradPassManager的问题。

https://github.com/vllm-project/vllm/issues/15222
这个issue是关于bug报告，涉及主要对象是加载jinaai的XLMRoberta模型时遇到的缺失参数问题。由于jinaai的XLMRoberta实现与vllm当前实现不同，导致加载jinarerankerv2basemultilingual模型时出现异常。

https://github.com/vllm-project/vllm/issues/15221
该issue是一个功能需求，主要涉及支持多卡的Disaggregated Prefill/Decode以及提供自动化基准测试。

https://github.com/vllm-project/vllm/issues/15220
这个issue是一个用户提出的需求类型，涉及主要对象是README文件。原因是缺少用户论坛链接和联系方式部分的链接，需要完善以提供更全面的信息。

https://github.com/vllm-project/vllm/issues/15219
这是一个bug报告，主要涉及vllm下的函数调用问题。由于在调用`query_engine.query(prompt)`时出现KeyError: 'text'，可能是由于缺少"text"字段导致的。

https://github.com/vllm-project/vllm/issues/15218
这个issue是BUG报告，主要涉及模型从S3存储加载失败的问题，可能是由于模型路径或存储配置的问题导致。

https://github.com/vllm-project/vllm/issues/15217
这是一个bug报告，用户在运行vllm时遇到了GPU识别问题，可能由于设置的world_size超过了可用的GPU数量所导致。

https://github.com/vllm-project/vllm/issues/15216
这是一个bug报告类型的issue，主要涉及的对象是vllm软件的使用。由于vllm 0.8.0在部署whisper-large-v3-turbo模型时出现了内存溢出错误，导致服务启动后立即遇到问题。

https://github.com/vllm-project/vllm/issues/15214
这个issue类型属于需求提出，主要涉及的对象是vllm项目和一个自定义模型，用户请求帮助修改VLLM以支持自定义模型中的外部MoE路由。

https://github.com/vllm-project/vllm/issues/15212
这是一个功能需求提出的issue，主要涉及Mistral Small 3.1 HF格式的支持。由于目前仅支持Mistral格式，用户希望增加对HF格式的支持。

https://github.com/vllm-project/vllm/issues/15211
这是一个bug报告，涉及的主要对象是Alibi-based模型如MPT，在使用时导致了错误的断言失败。

https://github.com/vllm-project/vllm/issues/15210
这是一个bug报告，涉及问题是关于v0.8.1版本更新后，使用正则表达式出现错误。

https://github.com/vllm-project/vllm/issues/15209
这是一个bug报告，主要涉及图像嵌入输入无法缓存导致的错误，出现错误的原因是在哈希器中对张量进行哈希时，需要先将其移至CPU，然后转换为NumPy数组。

https://github.com/vllm-project/vllm/issues/15208
这是一个简短的bug报告，涉及类型注释不准确导致的问题。

https://github.com/vllm-project/vllm/issues/15207
这是一个bug报告，主要涉及vllm版本0.8.0在Python 3.11环境下使用时出现解码错误的问题。

https://github.com/vllm-project/vllm/issues/15206
这是一个用户提出需求的issue，主要对象是vLLM containers，由于切换到虚拟Python环境导致无法在OpenShift中运行，提出寻求rootless container解决方案。

https://github.com/vllm-project/vllm/issues/15205
这是一个bug报告，涉及VLLM 0.8版本中如何消除随机性并获得固定结果的问题。用户尝试设置temperature=0和seed=0以获取固定结果，但发现多次运行后结果是变化的。

https://github.com/vllm-project/vllm/issues/15204
这是一个关于GPU内存占用的问题，用户提出了对于`3BInstructAWQ`和`7BInstructAWQ`模型占用16G内存的疑问。可能是模型大小和参数配置导致了GPU内存异常占用。

https://github.com/vllm-project/vllm/issues/15203
该issue属于性能问题报告，涉及主要对象为vllm中的模型Qwen2.5-3B-Instruct-AWQ，用户询问为什么该模型会占用16GB的GPU内存。

https://github.com/vllm-project/vllm/issues/15202
这是一个需求提出的Issue，主要涉及到对于请求延迟指标的存储不足，导致长时间推断请求的指标无法准确记录。

https://github.com/vllm-project/vllm/issues/15201
这是一个bug报告，涉及的主要对象是benchmark_throughput.py脚本。这个问题可能由于程序错误导致GPU和CPU KV缓存的使用率相反而引起。

https://github.com/vllm-project/vllm/issues/15200
这是一个Bug修复的Issue，主要涉及Qwen2.5-VL模型的注意力掩码计算错误导致输出不正确的问题。

https://github.com/vllm-project/vllm/issues/15199
这是一个Bug报告，涉及主要对象是程序在尝试反序列化由ray报告的对象时出现问题。造成这个问题的原因可能是与PyTorch版本2.6.0+cu124和环境配置相关的兼容性或配置错误。

https://github.com/vllm-project/vllm/issues/15198
这个issue是一个功能需求类型，主要对象是vllm，用户在寻求关于对话前缀延续功能的支持。

https://github.com/vllm-project/vllm/issues/15197
这是一个bug报告，主要涉及的对象是vllm下的Qwen2.5-VL模块。由于vllm0.8.1版本出现问题，导致bbox输出和对象检测完全无法使用。

https://github.com/vllm-project/vllm/issues/15196
这是一个bug报告，涉及的主要对象是vLLM。由于未能将所有张量放置在相同设备上，导致出现了设备不匹配的错误。

https://github.com/vllm-project/vllm/issues/15195
这个issue类型是功能需求，涉及的主要对象是在实现注释后端方面的扩展性。

https://github.com/vllm-project/vllm/issues/15194
这是一个关于bug报告的issue，主要涉及VLLM在使用tensor并行性时在多GPU情况下输出只有感叹号的问题。可能由于并发请求导致了模型输出异常。

https://github.com/vllm-project/vllm/issues/15193
该issue类型为发布问题，涉及vllm-cpu的最新版本发布标记，解决了与特定issue相关的问题。

https://github.com/vllm-project/vllm/issues/15192
这是一个bug报告，主要涉及的对象是类型注解。由于类型注解不准确导致了bug。

https://github.com/vllm-project/vllm/issues/15191
这个issue是一个功能需求，主要涉及到实现FA3 Fp8 cache支持的功能更新。

https://github.com/vllm-project/vllm/issues/15190
这个issue类型是Bug报告，涉及到加载默认聊天模板时出现TypeError。原因可能是代码逻辑错误或数据类型错误导致的。

https://github.com/vllm-project/vllm/issues/15189
这是一个bug报告，主要涉及的对象是config.py文件。由于出现了拼写错误导致的问题。

https://github.com/vllm-project/vllm/issues/15188
这是一个bug报告，涉及的主要对象是使用`enable_reasoning`参数和`stop`参数时出现了`content`字段包含额外字符的问题。这可能是由于在启用`enable_reasoning`时提供`stop`序列导致的。

https://github.com/vllm-project/vllm/issues/15187
这个issue是文档更新类型的问题。主要涉及的对象是项目的README.md文件。

https://github.com/vllm-project/vllm/issues/15186
这个issue属于用户提出需求的类型，主要涉及的对象是增加对指定模型的支持。由于vllm已经支持了一个最接近的模型，用户提出了希望支持另一个模型的需求。

https://github.com/vllm-project/vllm/issues/15185
这是一个Bug报告，涉及的主要对象是vllm的执行过程。由于尝试为多模态标记分配的总数超过了实际占位符的数量，导致了该数值错误的异常。

https://github.com/vllm-project/vllm/issues/15184
这是一个需求更新类型的issue，涉及主要对象为模型和示例的配置文件和权重。用户提出了vLLM目前只支持mistral格式而非HF格式，因此需要更新示例以指向mistral格式，询问是否支持MistralSmall3.1，并寻求此方面的修复。

https://github.com/vllm-project/vllm/issues/15183
这是一个与升级相关的issue，主要涉及对象是PyTorch和BNB版本。原因是升级PyTorch至2.6后，需要强制升级BNB至0.45.3版本。

https://github.com/vllm-project/vllm/issues/15182
这是一个bug报告，涉及的主要对象是在使用8个A800 80 GPU卡时出现了"cuda out of memory"错误。由于GPU内存不足，导致了这个bug。

https://github.com/vllm-project/vllm/issues/15181
这个issue是关于bug报告类型的，涉及的主要对象是CI/Build中的LoRA模块，由于add_lora_test修改了HF_CACHE导致问题，本次PR使用了不需要修改LoRA文件的模块来解决问题。

https://github.com/vllm-project/vllm/issues/15180
这是一个Bug报告，该问题涉及的主要对象是DeepSeekR1Q4模型的加载过程。由于该模型不支持torch.compile，导致出现错误提示并建议在GitHub上提出问题以获取支持。

https://github.com/vllm-project/vllm/issues/15179
这是一个bug报告，主要涉及的对象是一个名为"_schedule_runnning"的函数。这个issue由于代码中有无效代码导致了一个bug。

https://github.com/vllm-project/vllm/issues/15178
该issue类型为性能问题，涉及主要对象是vLLM模型的在线和离线批处理推断速度差异，提出了为什么在线批处理推断速度更快的疑问。

https://github.com/vllm-project/vllm/issues/15177
这个issue属于Bug报告，主要涉及的对象是当前工具解析器的设计缺陷导致的多步输出解析异常，原因是状态配置在处理增量Delta文本包含完整JSON主体时变得无效。

https://github.com/vllm-project/vllm/issues/15176
这个issue是关于bug报告，主要涉及到vllm项目中的DP（DataParallel）组创建问题，由于config文件中的参数设置错误导致无法正确建立DP组。

https://github.com/vllm-project/vllm/issues/15175
这个issue是一个bug报告，涉及的主要对象是Qwen2.5 72B模型在特定环境下的推理失败。原因可能是TMA描述符700无法初始化导致，具体表现为在TP=1和TP=2时推理失败，但在TP=4时成功。

https://github.com/vllm-project/vllm/issues/15174
这是关于缺少Python容器的一个Bug报告，容器中缺少Python环境导致无法正常运行。

https://github.com/vllm-project/vllm/issues/15173
此issue是关于新增功能的请求，涉及到使预测解码与流水线并行性兼容。由于draft pipeline_parallel_size与target pipeline_parallel_size不同，会导致错误，因此需要引入num_virtual_engine config以解决这个问题。

https://github.com/vllm-project/vllm/issues/15172
该issue为功能特性提议，涉及到测试样例在分布式环境下的运行。这个问题主要是为了确保SPMD基于runner的运行在存在外部数据并行时不会出现故障。

https://github.com/vllm-project/vllm/issues/15171
这个issue是关于BugFix类型，涉及主要对象为XgrammarBackend的导入，由于导入Xgrammar会初始化cuda context，而不希望在frontend进程中执行，导致启动服务器时无法使用fork模式。

https://github.com/vllm-project/vllm/issues/15170
这是用户提出的需求，主要目标是允许在基准数据集中请求过采样选项，因为小数据集（如visionarena）通常需要这个功能。

https://github.com/vllm-project/vllm/issues/15169
这个issue类型是bug报告，涉及主要对象是vLLM在NVIDIA Jetson AGX Orin上安装时出现PyTorch版本冲突，导致与CUDA 12.6不兼容。

https://github.com/vllm-project/vllm/issues/15168
这是一个Bug报告，主要涉及NCCL profiling data在nsight sys中无法显示的问题，可能是由于vLLM 0.8.0中的某些问题导致。

https://github.com/vllm-project/vllm/issues/15167
这是一个Bug报告类型的Issue，主要涉及vLLM模块的请求推断时间分桶问题，由于分桶设置不足导致长时间推断请求无法正确处理。

https://github.com/vllm-project/vllm/issues/15166
这是一个bug报告，问题涉及Ray在GKE上运行时缺少环境变量，导致运行出错。

https://github.com/vllm-project/vllm/issues/15165
这个issue是一个需求变更，涉及的主要对象是代码文件"ray_distributed_executor.py"。

https://github.com/vllm-project/vllm/issues/15164
这是一个Bug报告issue，涉及到OpenAI的400错误问题。问题主要原因可能是发送大量请求时出现的无效HTTP请求导致。

https://github.com/vllm-project/vllm/issues/15163
这是一个用户提出的需求类型的issue，主要涉及的对象是LWS文档。由于文档不完整，用户要求更新LWS用户指南以更全面。

https://github.com/vllm-project/vllm/issues/15162
这个issue属于一个功能需求报告，主要涉及的对象是vllm中的online rotations功能。由于之前vLLM中没有支持online rotations的接口，导致用户提出了这个需求来支持QuaRot quantized模型中的在线旋转功能。

https://github.com/vllm-project/vllm/issues/15161
该issue类型为活动通知，涉及的主要对象为与项目相关的Meetups。

https://github.com/vllm-project/vllm/issues/15160
这个issue是关于bug报告，主要涉及CUDA kernel中的数据类型错误导致可能出现意外结果，在大型数据集下可能出现问题。

https://github.com/vllm-project/vllm/issues/15159
该issue是一个bug报告，涉及CUDA kernel index data type的问题，由于使用了错误的数据类型(uint)，可能导致在处理大型数据集时产生意外结果。

https://github.com/vllm-project/vllm/issues/15158
这是一个bug报告，主要涉及到vllm的安装过程中uv install未再安装FlashInfer，可能是由于更新后的文档中移除了UV导致的。

https://github.com/vllm-project/vllm/issues/15156
该issue是性能优化类型问题，主要涉及的对象是请求输出队列。由于队列操作在高qps时进行性能剖析时出现，因为我们合并了`RequestOutput`对象，所以实际上不需要使用一个真正的队列，在添加时合并输出而不是在移除时，从而导致了此问题。

https://github.com/vllm-project/vllm/issues/15155
这是一个功能需求的issue，主要涉及到XGrammar的正则表达式支持。由于之前的VLLM版本中的正则表达式回退到了轮廓，用户提交了这个PR来移除VLLM v0用户的这种回退。

https://github.com/vllm-project/vllm/issues/15154
这是一个Bug报告，主要涉及到程序运行中出现的数值广播错误，导致了`ValueError: could not broadcast input array from shape (256,) into shape (76,)`。

https://github.com/vllm-project/vllm/issues/15153
这是一个bug报告，主要涉及的对象是Deepseek MTP模型，由于Deepseek MTP当前与chunked prefill不兼容，导致在启用两个功能时会因形状不匹配而导致运行时崩溃。

https://github.com/vllm-project/vllm/issues/15152
这是一个bug报告类型的issue，主要涉及的对象是LoRA kernels。由于PR导致的优化移除，V1架构与V0不同，需要在torch操作中添加标志以避免动态控制流与tracing之间的冲突。

https://github.com/vllm-project/vllm/issues/15151
这是一个类型为功能添加的issue，涉及到V1版本中的speculative decoding metrics。由于需求添加了新的度量标准支持，导致需要进行代码修复和功能增强。

https://github.com/vllm-project/vllm/issues/15150
这是一个优化性能的issue，主要涉及前端，目的是避免在大多数常见情况下的合并开销。

https://github.com/vllm-project/vllm/issues/15149
这个issue类型为功能增强或新功能请求，主要涉及的对象是支持mrope模型（Qwen2VL），用户希望继续支持相关工作。

https://github.com/vllm-project/vllm/issues/15148
这是一个用户提出需要文档更明确的问题，主要涉及到分布式推理场景下仅在单个节点上运行vllm的指导。问题的原因可能是用户误解如何在集群中运行vllm，导致出现错误。

https://github.com/vllm-project/vllm/issues/15147
这是一个bug报告，主要涉及在vllm server deployed in Modal中运行ShieldGemma 2B时出现的错误。这个问题可能是由于`guideline`未定义而导致的。

https://github.com/vllm-project/vllm/issues/15146
这是一个bug报告，主要涉及VLLM中Greedy Search和Beam Search性能问题，可能导致表现不佳。

https://github.com/vllm-project/vllm/issues/15145
这个issue是一个bug报告，主要涉及到V1版本中的TPU功能，由于kv缓存形状的更改导致了ragged paged attention kernel在特定情况下的执行失败。

https://github.com/vllm-project/vllm/issues/15144
这是一个Bug报告，涉及到V1中的`get_multimodal_embedding`输出与`PlaceholderRange`之间的不匹配。导致的问题是在高负荷下进行推理时模型崩溃，无法正确分配多模态标记到占位符。

https://github.com/vllm-project/vllm/issues/15143
这是一个Bug报告，涉及主要对象为`AutoProcessor`和`AutoTokenizer`，问题是由于chat template加载顺序导致HF工具调用模板失败和部分情况下出现类型错误的bug。

https://github.com/vllm-project/vllm/issues/15142
这个issue属于软件需求类型，主要涉及到CI/CD构建中缺少latest标签，导致用户需要手动更改镜像版本的问题。

https://github.com/vllm-project/vllm/issues/15141
这是一个需求提议issue，主要涉及vLLM如何导出指标数据的问题，用户希望支持OpenTelemetry格式以及delta temporality。

https://github.com/vllm-project/vllm/issues/15140
这个issue是有关代码清理的问题，该问题单涉及的主要对象是"BitsAndBytes"，由于参数的混乱和不清晰导致了需要清理整理的情况。

https://github.com/vllm-project/vllm/issues/15139
这是一个bug报告，涉及的主要对象是 stats.py 文件。由于代码更新不及时导致的简单bug。

https://github.com/vllm-project/vllm/issues/15138
这个issue类型为需求提议，主要涉及的对象是OpenTelemetry API。由于可能需要更精确的度量，用户提出了与OpenTelemetry API相关的度量提案。

https://github.com/vllm-project/vllm/issues/15137
这个issue类型是性能优化建议，主要对象是`tokenizers` library，提出了利用`DecodeStream`功能实现更快的增量detokenization，但无法帮助mistral tokenizers。

https://github.com/vllm-project/vllm/issues/15136
这是一个Bug报告，涉及的主要对象是vllm 0.8.0版本。由于GPU未被正确利用，导致在视频处理过程中出现长时间卡住的问题。

https://github.com/vllm-project/vllm/issues/15135
该issue类型为更新需求，涉及主要对象为torch nightly版本，由于旧版本wheels已过期导致该问题产生。

https://github.com/vllm-project/vllm/issues/15134
这个issue是一个文档更新类的bug报告，主要涉及到README.md中的链接指向问题；这个问题可能导致用户在查看"the First vLLM China Meetup"幻灯片时造成误导。

https://github.com/vllm-project/vllm/issues/15133
该issue类型是功能请求，主要涉及vllm中的torch_compile_cache功能。由于每个rank在v1中重定向缓存到不同的目录，因此不再需要附加pid导致的问题。

https://github.com/vllm-project/vllm/issues/15132
该issue类型为用户提出需求，主要对象是vllm模型及其多轮问答功能。由于用户有多个问题针对同一图像，但目前的方式需要模型每次都重新处理图像，用户希望找到更好的方法来实现多轮问答。

https://github.com/vllm-project/vllm/issues/15131
这是一个用户提出需求的issue，主要关注embedding size和vocab_size之间的关系，以及如何通过嵌入向量检索特定的提示标记。

https://github.com/vllm-project/vllm/issues/15130
该issue为一个新功能需求，主要涉及到在vLLM上添加对Qwen2.5Omni模型的支持，该需求只涉及到"thinker"部分的实现。

https://github.com/vllm-project/vllm/issues/15129
这是一个bug报告类型的issue，主要涉及到升级pytorch版本至2.6.0，可能由于旧版本的bug或功能限制导致了一些问题。

https://github.com/vllm-project/vllm/issues/15128
这是一个性能优化类型的issue ，主要涉及优化`vllm`的import时间。由于`import torch`和`from openai import xxx`导致了import时间较长，需要优化。

https://github.com/vllm-project/vllm/issues/15127
这是一个 bug 报告 issue，主要涉及 vLLM 引擎异步运行时出现的 AsyncEngineDeadError 错误。由于当前环境的信息收集过程中发生了错误，导致无法继续运行引擎。

https://github.com/vllm-project/vllm/issues/15126
这是一个用户提出需求的类型，主要对象是添加了一个用于 w8a8 块 fp8 调整的脚本，可能由于需要进行参数调整或优化而提出帮助或需求。

https://github.com/vllm-project/vllm/issues/15125
这是一个 bug 报告，涉及的主要对象是 vllm 服务环境，由于输出顺序错误导致 image_pad 总是出现在用户文本之前。

https://github.com/vllm-project/vllm/issues/15124
这是一个bug报告，涉及的主要对象是vllm库下的AssertionError。由于加载权重形状与原始词汇大小不匹配导致的错误。

https://github.com/vllm-project/vllm/issues/15123
这是一个需求提议类型的issue，主要涉及kv cache的优化和扩充批处理能力。由于kv cache传输成本过高，导致机器学习模型训练中无法充分利用GPU计算资源，提议将kv cache管理转移到CPU并优化批处理以实现更高效的计算。

https://github.com/vllm-project/vllm/issues/15122
这个issue是关于bug报告，主要涉及的对象是Qwen VL 2.5模型，由于某些原因导致模型识别图片出现错误。

https://github.com/vllm-project/vllm/issues/15121
这是一个Bug报告类型的issue，主要涉及V1版本的AWQ和GPTQ启动失败，导致TypeError: an integer is required错误。

https://github.com/vllm-project/vllm/issues/15120
这是一个bug报告，涉及的主要对象是vllm的安装过程。由于在配置文件中设置了model_type字段，但在容器中却出现了"Unrecognized model"错误，可能是因为配置文件中的model_type字段未能正确识别导致的。

https://github.com/vllm-project/vllm/issues/15119
这是一个Bug报告类型的issue，主要涉及VLLM中使用completions API时出现了BadRequestError(400)的问题，可能由于参数设置不正确导致请求失败。

https://github.com/vllm-project/vllm/issues/15118
这是一个功能需求的讨论，主要涉及添加一个w8a8块fp8调整脚本，原因可能是为了优化模型的量化过程。

https://github.com/vllm-project/vllm/issues/15117
这个issue是性能优化相关的需求。用户提出了优化Fused MoE在NVIDIA_L20上的性能问题。

https://github.com/vllm-project/vllm/issues/15116
这是一个用户提出需求的issue，主要涉及的对象是GPTQ算法的实现。由于缺乏GPTQ算法中类似AWQ算法的**modules_to_not_convert**属性，用户提出了希望增加这一功能的需求。

https://github.com/vllm-project/vllm/issues/15115
这个issue是关于bug报告，主要涉及了模型在v1引擎中获取不到`sampling_metadata`参数的问题，导致无法正确检测prefill阶段。

https://github.com/vllm-project/vllm/issues/15114
这个issue是bug报告，主要涉及处理缓存大小计算错误问题，由于未递归处理各个张量导致缓存容量计算错误。

https://github.com/vllm-project/vllm/issues/15113
这是一个Bug报告，涉及的主要对象是flash_attn_with_kvcache kernel，可能是由于非法内存访问导致了flash::copy的问题。

https://github.com/vllm-project/vllm/issues/15112
这是一个 bug 报告，主要涉及的对象是 mllama 模型。原因是 mllama 代码在检查层类时通过名称进行，导致对不一致的模型进行解码出现错误，提出了基于层ID区分层的解决方案。

https://github.com/vllm-project/vllm/issues/15111
这个issue是一个Bugfix类型的报告，主要涉及对象是mllama模型和Gaudi设备。由于模型代码在检查层类时使用了isinstance函数判断，而在对模型进行编译后会导致层类名称发生变化，进而导致了数值错误以及无法识别解码器层类型的问题。

https://github.com/vllm-project/vllm/issues/15110
这是一个bug报告，涉及的主要对象是使用Qwen2-VL-7B模型的vLLM Docker容器。由于可能是模型配置或输入格式问题引起的，导致了使用该模型时出现内部服务器错误。

https://github.com/vllm-project/vllm/issues/15109
这是一条关于更新XPU Dockerfile和CI脚本的issue，主要涉及Intel GPU，并由于Intel apt仓库问题导致的bug报告。

https://github.com/vllm-project/vllm/issues/15108
这是一则关于使用VLLM添加功能的疑问，而非bug报告，主要涉及到了在一个单独环境中使用torch的问题。可能由于用户在特定环境中无法成功使用VLLM添加功能而提出疑问。

https://github.com/vllm-project/vllm/issues/15107
这是一个关于代码优化的issue，主要涉及VLLM中的HF `do_rescale`警告问题，用户提出了设置`do_rescale=False`可以避免不必要的重新缩放的建议。

https://github.com/vllm-project/vllm/issues/15106
这是一个bug报告类型的issue，主要涉及 vllm 的 tpot/itl 实现和 chat 接口的代码结构。由于代码实现中的一些逻辑问题，导致在某些情况下返回的 tpot/itl 时间不符合预期，提出了代码调整的建议。

https://github.com/vllm-project/vllm/issues/15105
这是一个bug报告，针对v0.8.0版本出现的问题，主要涉及到文本生成速度下降和内存消耗增加的情况。

https://github.com/vllm-project/vllm/issues/15104
这是一个Bug修复类的Issue，主要涉及到V1版本中的设备检查逻辑。问题由于未针对除AMD和TPU外的其他设备进行逻辑处理，导致V1版本在使用SmolVLM或Idefics3模型时崩溃。

https://github.com/vllm-project/vllm/issues/15103
这是一个关于功能未完善的问题，主要涉及 Aya vision 32b 模型的支持情况。由于功能尚未完全实现，导致 "dont review " 指令无法生效。

https://github.com/vllm-project/vllm/issues/15102
这是一个Bug报告类型的issue，涉及的主要对象是Vllm中的DeepSeekV3模型。由于RayChannelTimeoutError导致推断过程中出现错误，可能由于通道读取超时或其他通道相关问题所导致。

https://github.com/vllm-project/vllm/issues/15101
这是一个bug报告，涉及的主要对象是在运行DeepSeek-R1-awq模型时遇到了错误。由于quant_method未定义导致了断言错误。

https://github.com/vllm-project/vllm/issues/15100
这是一个Bug报告，主要涉及的对象是RayWorkerWrapper，由于缺少model pyarrow和pandas导致出现错误。

https://github.com/vllm-project/vllm/issues/15099
这是一个bug报告，主要涉及的对象是在某些不支持triton的设备上进行推理时可能出现的ModuleNotFoundError。

https://github.com/vllm-project/vllm/issues/15098
这是一个Bug报告，涉及到使用16个GPU加载MoE模型时在NCCL上崩溃的问题。原因是在执行`ncclAllReduce`函数时发生了段错误，导致程序崩溃。

https://github.com/vllm-project/vllm/issues/15097
这是一个bug报告，主要涉及Gemba3无法生成响应，并导致GPU持续运行的问题。

https://github.com/vllm-project/vllm/issues/15096
这是一个bug报告，主要涉及vllm 0.8.0在使用Docker部署时不支持gemma3，导致出现问题。

https://github.com/vllm-project/vllm/issues/15095
这是一个bug报告，主要涉及vLLM在升级版本后出现的TypeError错误。

https://github.com/vllm-project/vllm/issues/15094
这是一个bug报告，涉及的主要对象是如何避免再次对输出进行重新缩放。原因是当前设置应该设置`do_rescale=False`来避免再次对输出进行重新缩放。

https://github.com/vllm-project/vllm/issues/15093
这是一个关于使用问题的bug报告，主要涉及对象是vllm的torch.compile模块，问题由于模型"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"并不支持该模块而导致推理不起作用。

https://github.com/vllm-project/vllm/issues/15092
这是关于用户需求/寻求帮助的类型，主要涉及vllm的特定模型推理运行集成问题。由于用户不知道如何与vllm集成，可能导致无法进行特定模型推理的问题。

https://github.com/vllm-project/vllm/issues/15091
这是一个关于代码逻辑优化的建议类型的issue，主要涉及调度器（scheduler）。由于等待队列（waiting queue）在弹出（popleft）前并未进行排序，可能导致逻辑执行顺序存在疑问。

https://github.com/vllm-project/vllm/issues/15090
这是一个Bug报告，该问题涉及LoRA与enabel_eager=False时的CUDA图捕获问题。由于代码中的token_lora_indices打印结果异常，用户反馈一个token的LoRA_indices为1似乎不正确。

https://github.com/vllm-project/vllm/issues/15089
这是一个Bug报告，主要涉及的对象是vllm程序，原因是无法反序列化PostGradPassManager对象导致数值错误。

https://github.com/vllm-project/vllm/issues/15088
这是一个bug报告，主要对象是v0.8.0 docker安装过程中的python缺失问题，导致所有二进制文件被放置在不同的路径。

https://github.com/vllm-project/vllm/issues/15087
这是一个需求类型的issue，主要对象是请求在v0.8.0 Docker Image中包含vllm["audio,video"] package。造成这个问题的原因是在调用/v1/audio/transcriptions端点时出现ImportError，因为缺少vllm[audio] package。

https://github.com/vllm-project/vllm/issues/15086
这是一个Bug报告，涉及的主要对象是vllm项目下的InternVL-based模型。这个问题是由于多模态嵌入与`PlaceholderRange`索引之间的不匹配导致的。

https://github.com/vllm-project/vllm/issues/15085
这个issue类型为Bug报告，主要涉及vllm 0.8.0版本的RAM泄漏问题。

https://github.com/vllm-project/vllm/issues/15084
这是一个用户提出需求的issue，主要涉及vllm是否能够支持在H20服务器上natively运行特定模型的推断，缺乏集成方法导致用户不清楚如何使用vllm。

https://github.com/vllm-project/vllm/issues/15083
这个issue是一个bug报告，涉及到加载whisper quantized模型时出现CUDA错误的问题。原因可能是环境配置或模型加载的问题。

https://github.com/vllm-project/vllm/issues/15082
这是一个bug报告，主要涉及多视频推理在LLaVA-Onevision上的问题。由于某种原因导致了相关问题需要修复。

https://github.com/vllm-project/vllm/issues/15081
这是一个安装相关的issue，主要涉及如何安装v0.8.0版本的vllm，用户反映尝试多种方式都未成功安装。

https://github.com/vllm-project/vllm/issues/15080
这是一个bug报告，涉及到在使用Qwen 72B模型时传递26x28像素图像导致500错误的问题。

https://github.com/vllm-project/vllm/issues/15079
这是一个关于如何使用FlashMLA进行DeepSeek-V2的问题，属于用户需求询问类型，主要涉及如何在当前环境设置后端以使用FlashMLA进行DeepSeek-V2。这个问题的产生可能是用户想要运行特定模型的推理，但不清楚如何与vllm集成。

https://github.com/vllm-project/vllm/issues/15078
这是一个bug报告，涉及的主要对象是`llava_onevision.py`文件中的`_parse_and_validate_video_input()`函数。原因是在`CC([Bugfix] Loosen type check to avoid errors in V1)`改动后，导致`pixel_values_videos`变成了错误的数据结构，使得输入多个视频时出现问题。

https://github.com/vllm-project/vllm/issues/15077
该issue类型为用户提出需求，主要涉及的对象是对vLLM的新attention layer的实现及kv cache的兼容性。由于vLLM目前仅支持基本的多头注意力机制，用户希望了解如何实现新的attention layer，并提出了kv cache应如何处理以兼容vLLM的问题。

https://github.com/vllm-project/vllm/issues/15076
这是一个bug报告类型的issue，主要涉及到调试消息的优化问题，可能由先前的代码修改或者缺陷导致了无法使用块的错误消息的问题。

https://github.com/vllm-project/vllm/issues/15075
这个issue类型是代码维护（refactor），主要对象是V1 async engine test代码。由于上次清理时错过了一些细节，导致需要做小的代码重构。

https://github.com/vllm-project/vllm/issues/15074
这是一个功能增强需求，主要涉及到为vLLM Python代码添加cProfile辅助工具。

https://github.com/vllm-project/vllm/issues/15073
这是一个Bug报告类型的issue，主要涉及到vllm库在版本升级后在使用guided_json时出现报错。导致该问题的原因可能是升级到了vllm0.7.2版本后对guided_json调用方式或参数有所改变，导致之前正常的调用现在报错。

https://github.com/vllm-project/vllm/issues/15071
这是一个Bug报告，主要涉及的对象是vLLM在多网络接口环境中的部署，导致Ray head节点可能错误检测本地IP地址，最终影响跨节点通信。

https://github.com/vllm-project/vllm/issues/15070
This issue is a documentation update related to using transformers when creating a custom Dockerfile, it addresses an error caused by using `system` when Python installation is managed by uv.

https://github.com/vllm-project/vllm/issues/15069
这是一个bug报告，主要涉及到Mistral聊天完成请求中的重复消息检查。原因是重复获取最后一条消息、重复检查其角色，以及连续两次设置相同的标志，可能是复制粘贴错误导致。

https://github.com/vllm-project/vllm/issues/15068
这个issue类型是用户提出需求，主要涉及的对象是新增模型支持。因为vllm尚未支持用户想要的模型，用户提出了希望添加新模型支持的需求。

https://github.com/vllm-project/vllm/issues/15067
这是一个功能集成请求，涉及与Pallas内核的写入缓存集成。

https://github.com/vllm-project/vllm/issues/15066
这个issue类型是bug报告，涉及vllm在macOS上从源代码构建时无法加载_C.abi3.so文件，导致的症状。

https://github.com/vllm-project/vllm/issues/15065
这是一个bug报告，主要涉及的对象是数据类型的选择。由于数据类型不当导致的bug产生。

https://github.com/vllm-project/vllm/issues/15064
这个issue类型是文档更新，主要涉及的对象是v1版本的用户指南。

https://github.com/vllm-project/vllm/issues/15063
这是一个功能需求类型的issue，主要涉及对top-p和top-k采样的解码功能。根据描述，用户希望通过实现`apply_top_k_top_p`来为包含topp和topk采样的请求启用特定解码。

https://github.com/vllm-project/vllm/issues/15062
这个issue类型是bug报告，主要涉及VLLM库中的离线推断示例出现问题，由于特定条件下（短的`max_model_len`和较低的`max_num_seqs`），生成的序列长度可能大于最大模型长度，导致推断失败。

https://github.com/vllm-project/vllm/issues/15061
这是一个bug报告，主要对象涉及vLLM在没有可用GPU的CUDA平台上的问题。由于CUDA平台上未分配GPU导致`get_device_capability().major`报错。

https://github.com/vllm-project/vllm/issues/15060
这个issue属于bug报告类型，涉及到--enable-chunked-prefill设置不生效的问题。原因可能是设置没有正确应用导致这一症状。

https://github.com/vllm-project/vllm/issues/15059
这是一个功能增强提议，主要涉及的对象是V1版本中的Tensor parallel模块。

https://github.com/vllm-project/vllm/issues/15058
这是一个bug报告，主要涉及vLLM在特定环境下出现“ValueError: Model architectures ['LlamaForCausalLM'] failed to be inspected”错误导致的 segmentation fault 问题。

https://github.com/vllm-project/vllm/issues/15057
这是一个bug报告，涉及的主要对象是使用vLLM V1的SmolVLM / Idefics3模型。由于某种原因，导致服务无法启动且未提供预期的错误堆栈信息。

https://github.com/vllm-project/vllm/issues/15056
这是一个Bug报告类型的Issue，主要涉及对象是tokenizer的创建，可能导致forking在tokenizer创建之后发生。

https://github.com/vllm-project/vllm/issues/15055
这是一个用户提出需求的issue，主要涉及的对象是支持更多视频加载器的功能。由于OpenCV在aarch64平台上可能无法读取视频流，用户试图使用OpenCV作为视频IO的备选方案。

https://github.com/vllm-project/vllm/issues/15054
这是一个需求报告，主要涉及的对象是模型选择。

https://github.com/vllm-project/vllm/issues/15053
这是一个bug报告，涉及的主要对象是Pixtral模型。由于重复的层实例化导致了该问题。

https://github.com/vllm-project/vllm/issues/15052
该issue类型为用户需求，主要对象是用户操作文档页面时可能遇到的问题。由于用户未在文档页面底部的聊天机器人中搜索相关问题，导致用户提出了寻求常见问题答案的需求。

https://github.com/vllm-project/vllm/issues/15051
这个issue是关于代码优化的讨论，主要涉及到在模型编译期间对多模态编码器进行优化。由于在处理设备张量时可能导致重新编译，用户提出了一些相关问题。

https://github.com/vllm-project/vllm/issues/15050
这个issue是一个bug报告，涉及主要对象为"torch.compile on ROCm"，由于更新的fusion pass未正确处理fp8数据类型和动态情况下的layernorm quant kernel，导致了动态情况下的单元测试失败。

https://github.com/vllm-project/vllm/issues/15049
这是一个bug报告，涉及的主要对象是topk采样功能。这个问题的原因在于topk采样中的gather函数要求索引为long dtype，导致出现bug。

https://github.com/vllm-project/vllm/issues/15048
这是关于一个Bug报告的Issue，涉及对象为EmbeddingChatRequest，由于缺少了'mm_processor_kwargs'属性导致了AttributeError。

https://github.com/vllm-project/vllm/issues/15047
这是一个bug报告，针对LoRA额外词汇大小的修复。

https://github.com/vllm-project/vllm/issues/15046
这是一个需求提交的issue，主要涉及的对象是StableLMAlphaForCausalLM模型，用户提出了对该模型支持的困难性。

https://github.com/vllm-project/vllm/issues/15045
这是一个bug报告，涉及主要对象是使用MoE sum kernel时发生精度损失问题，可能是由于topk为8时导致的bfloat16精度损失。

https://github.com/vllm-project/vllm/issues/15044
这是一个Bug报告类型的issue，主要对象是DeepSeek-R1模型。由于PyTorch版本为2.6.0+cpu，未启用CUDA，可能导致CPU推理无法正常工作。

https://github.com/vllm-project/vllm/issues/15043
这是一个bug报告，主要涉及的对象是V1 Engine，由于model_executor的状态未被正确更新导致引发了V1 Engine在处理具有重复请求ID的请求时崩溃的问题。

https://github.com/vllm-project/vllm/issues/15042
这是一个bug报告，主要涉及vllm在使用gemmamodel时出现了大量GPU内存占用问题。

https://github.com/vllm-project/vllm/issues/15041
这个issue是关于bug报告类型，主要涉及V1 Engine在接收到相同request id的请求时会立即崩溃，由于调用者可以任意设置request_id导致这一问题的发生。

https://github.com/vllm-project/vllm/issues/15040
这个issue是关于bug报告，主要涉及的对象是添加对不同`tokenizer_mode`支持的Benchmark。由于`tokenizer_mode=auto`无法正确检测Mistral tokenizer，故需要显式设置`tokenizer_mode`，可能是一个潜在的bug。

https://github.com/vllm-project/vllm/issues/15039
该issue是一个Bug报告，涉及的主要对象是LoRA请求在输入令牌大于8k时引发CUDA OutOfMemoryError。这个问题是由于输入令牌长度过长导致的。

https://github.com/vllm-project/vllm/issues/15038
这是一个bug报告，主要涉及triton模块导入导致CPU量化功能不正常的问题。

https://github.com/vllm-project/vllm/issues/15037
这是一个bug报告，主要涉及的对象是Tensor Processing Unit（TPU）的一种bug。导致这个bug的原因是在使用CP（chunked prefill）结合padding时，未正确处理最后/终端chunk，导致缓冲区被重复使用，从而导致数值来源于先前迭代。

https://github.com/vllm-project/vllm/issues/15036
这是一个Bug报告，主要涉及模型服务器在特定参数下出现tensor shape mismatch导致无法运行的问题。

https://github.com/vllm-project/vllm/issues/15035
这是一个bug报告，涉及主要对象为PydanticAI客户端在使用vLLM时遇到的HTTP 400错误，可能原因是vLLM不支持请求中的可重用子模式。

https://github.com/vllm-project/vllm/issues/15034
这是一个需求类型的issue，主要涉及到如何计算和修正最大和最小像素值的问题。原因可能是用户在使用qwen2vl7b进行图像数据提取时遇到了困惑。

https://github.com/vllm-project/vllm/issues/15033
这是一个关于bug报告的issue，涉及的主要对象是vllm工具中的pickle错误，可能由于对象不一致导致。

https://github.com/vllm-project/vllm/issues/15032
这是一个bug报告，涉及主要对象为vllm项目下的spec_decode模块，导致bug的原因是hidden_states维度不符合要求和未正确限制提议长度的条件。

https://github.com/vllm-project/vllm/issues/15031
这是一个bug报告，主要涉及对象是vLLM中的模型加载过程。由于合并后的模型结构与vLLM的期望不一致，导致在加载模型时出现了缺失`language_model`模块的错误。

https://github.com/vllm-project/vllm/issues/15030
这个issue类型是bug报告，涉及到Triton Dependency for CPU在Quantization情况下的问题，由于triton库未在CPU环境下安装，导致无法执行相关代码。

https://github.com/vllm-project/vllm/issues/15029
这是一个关于性能优化的issue，主要涉及到大批量推断下使用spec decoder时的性能问题，用户关注数据处理开销是否会影响MTP模型执行效率。

https://github.com/vllm-project/vllm/issues/15027
这是一个用户提出需求的类型，主要对象涉及是Mistral-Small-3.1。由于缺乏详细信息和反馈，用户询问此功能是否支持。

https://github.com/vllm-project/vllm/issues/15026
这是一个bug报告，主要涉及对象是PallasAttentionBackendImpl类初始化方法，由于调用出现了意外的关键字参数'q_lora_rank'导致了该问题。

https://github.com/vllm-project/vllm/issues/15025
这是一个bug报告，主要涉及的对象是vLLM中使用speculative decoding时速度下降的问题，可能是由于使用了小型模型且增加了speculative tokens导致。

https://github.com/vllm-project/vllm/issues/15024
这是一个关于如何在多节点上评估DeepSeek-R1-671B性能的需求问题，涉及到在多个节点和GPU上设置benchmark_throughput.py的命令行参数。

https://github.com/vllm-project/vllm/issues/15023
这是一个用户提出需求的issue，主要涉及的对象是增加对TeleFLM模型的支持，在此基础上验证了相关功能操作正常。

https://github.com/vllm-project/vllm/issues/15022
这是一个用户提出需求的issue，涉及的主要对象是`torchcodec`。由于`torchcodec`目前不提供适用于Linux的ARM64 wheel，导致用户提出希望支持更多视频加载器的需求。

https://github.com/vllm-project/vllm/issues/15021
这是一个bug报告类型的issue，主要涉及到类型检查的锁定问题，可能导致V1版本中出现错误。

https://github.com/vllm-project/vllm/issues/15020
这是一个功能增强请求，主要涉及到CPU后端的支持。由于之前缺乏这项支持，导致当前无法在CPU后端编译torch，需要增加相关功能以解决这一问题。 

https://github.com/vllm-project/vllm/issues/15019
这是一个Bug报告，主要涉及vllm 0.8.0 rc3版本中可能存在的问题，用户在测试中设置了`disable_mm_preprocessor_cache=True`，并怀疑在这种情况下是否仍在使用multimodal cache。

https://github.com/vllm-project/vllm/issues/15018
这是一个性能问题报告，主要涉及的对象是 vLLM 模型在运行多个请求时性能下降的情况。由于未明确设置参数导致 GPU 利用率不足，从而导致每秒令牌数下降的问题。

https://github.com/vllm-project/vllm/issues/15017
这是一个bug报告，涉及vllm在使用多个视频输入时出现像素数值错误导致服务器关闭的问题。

https://github.com/vllm-project/vllm/issues/15016
这是一个bug报告，涉及对象是使用VLLM时出现的错误信息"Prompt cannot be empty"。由于未正确输入提示内容导致发生了400错误码的bug。

https://github.com/vllm-project/vllm/issues/15015
这是一个bug报告，主要涉及的对象是vLLM框架下的一个quantized模型，由于CUDA内存溢出错误导致无法运行。

https://github.com/vllm-project/vllm/issues/15014
这是一个Bug报告，涉及VLLM（Very Large Language Model）项目中的Deepseek组件，由于CUDA参数错误导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/15013
这是一个Bug报告，主要涉及张量并行离线推理报错，原因可能是与GCC版本不兼容导致的命令执行错误。

https://github.com/vllm-project/vllm/issues/15012
这是一个bug报告，涉及对象是VLLM在转录过程中出现了"Maximum clip duration (30s) exceeded"的错误提示。由于当前环境中PyTorch版本为2.5.1+cu124，导致了此问题。

https://github.com/vllm-project/vllm/issues/15011
这是一个功能需求的issue，主要涉及的对象是视频加载器。由于`decord`库存在长时间未维护以及适配性问题，导致需要支持更多视频加载器，以满足不同用户的需求。

https://github.com/vllm-project/vllm/issues/15010
这是一个bug报告类型的issue，主要涉及安装vllm时出现的undefined symbol错误，可能由于python版本不兼容导致。

https://github.com/vllm-project/vllm/issues/15009
这是一个bug报告，涉及的主要对象是V0 MQ Engine。由于V0+TP和trust remote code不兼容，导致V0+TP不能使用自定义配置，产生了一个Bug。

https://github.com/vllm-project/vllm/issues/15008
这是一个bug报告，主要涉及到vllm中的NemotronNAS模型的更新支持问题。原因是旧的DeciLMForCausalLM模型不再被支持且存在缓存过分分配等问题。

https://github.com/vllm-project/vllm/issues/15007
这是一个Bug报告，主要涉及到vllm下的一个issue，用户在执行docker时无法找到可执行路径vllmopenai。这个问题可能是由于容器中缺少特定可执行文件导致的。

https://github.com/vllm-project/vllm/issues/15006
这是一个bug报告，主要涉及对象是Qwen2_5_VLForSequenceClassification模型。由于该模型没有vLLM实现，且Transformers实现与vLLM不兼容，导致数值错误（ValueError），用户寻求可能的替代方案或修复。

https://github.com/vllm-project/vllm/issues/15005
这个issue是一个功能需求，该问题涉及到vllm项目中关于s3支持的自定义实现，问题主要是由于S3Model的初始化高度依赖于AWS配置文件导致。

https://github.com/vllm-project/vllm/issues/15004
这个issue是一个Bug报告类型，涉及的主要对象是无法在RTX 3070 & CPU Offload (14GB)环境下成功运行Qwen2.5-7B模型。原因是加载模型权重时出现内存错误，导致引擎核心异常终止。

https://github.com/vllm-project/vllm/issues/15003
这是一个用户提出需求的 issue，主要对象是修改代码库中的某个功能。原因是用户希望能够通过指定的设备重置设备前缀缓存，既可以重置 CPU 的设备前缀缓存也可以重置 GPU 的设备前缀缓存。

https://github.com/vllm-project/vllm/issues/15002
这是一个bug报告类型的issue，涉及对象是DeepSeek-R1-AWQ模型，在不同版本的vllm中出现了问题。由于vllm版本升级到v0.8.0rc3.dev5+g5eeabc2a后，该模型产生了错误的输出。

https://github.com/vllm-project/vllm/issues/15001
这是一个功能增强的issue，涉及ROCm集成Paged Attention Kernel，由于模块目前不支持某些`kv_cache_dtype`导致了问题。

https://github.com/vllm-project/vllm/issues/15000
这是一个bug报告，主要涉及到在多进程中出现异常时导致VLLM无法终止并停止处理新请求的问题。这种情况可能会使系统在生产环境中变得不可响应。

https://github.com/vllm-project/vllm/issues/14999
这是一个bug报告，涉及主要对象是torch.gather，该问题由于更新torch至2.6.0导致在DeepSeek R1上的crash。

https://github.com/vllm-project/vllm/issues/14998
这是一个用户提出需求的issue，主要涉及vllm中的multimodal generate功能，用户希望能够直接传递预处理的图像数据来避免在vllm内部进行图像预处理。

https://github.com/vllm-project/vllm/issues/14997
这是一个bug报告，主要涉及到vllm在使用并行处理时遇到的GPU资源不足问题。

https://github.com/vllm-project/vllm/issues/14996
这是一个bug报告类型的issue，主要涉及到`MacheteLinearKernel`的选择问题，导致在特定环境下出现错误。

https://github.com/vllm-project/vllm/issues/14995
这是一个用户提出需求的issue，主要对象是tpu_model_runner，用户提出了改进token_num padding逻辑的建议。

https://github.com/vllm-project/vllm/issues/14994
这是一个bug报告，涉及到AsyncLLMEngine引擎的CancelledError和engine_client.dead_error问题。这个问题可能是由于参数配置不正确导致的。

https://github.com/vllm-project/vllm/issues/14993
这个issue类型是Bug报告，主要对象涉及使用Sonatype Nexus Repository下载模型时出现错误。由于config.json的url不正确，导致无法正确下载模型。

https://github.com/vllm-project/vllm/issues/14992
这是一个Bug报告，涉及到缓存块内存不足的问题，由于提交了特定的commit导致该问题产生。

https://github.com/vllm-project/vllm/issues/14991
这是一个bug报告类型的issue，主要涉及到Docker镜像中找不到libpython.so的问题。这可能是由于Python版本无法从目标进程中找到，导致出现了这种症状。

https://github.com/vllm-project/vllm/issues/14990
这个issue类型为功能增强需求，涉及主要对象为EAGLE Architecture。造成这个问题的原因是EAGLE实现中的近似KV缓存bug引起性能下降，而用户提出了使用正确的RMS规范来暂时缓解这个bug的需求。

https://github.com/vllm-project/vllm/issues/14989
这是一个性能优化的Issue，涉及V1版本的改进，主要针对杂项简化和性能优化。

https://github.com/vllm-project/vllm/issues/14988
这是一个bug报告，主要涉及的对象是Neuron后端测试基础架构，问题是由于内存不足导致特定测试用例在trn1.2x实例上出现内存溢出。

https://github.com/vllm-project/vllm/issues/14987
这个issue属于bug报告类型，涉及主要对象是 AMD Instinct MI325/325X，由于 BLOCK_SIZE_K 大于 fp8 缩放组大小时，fused_moe_kernel 返回错误的值，需要对其进行修复。

https://github.com/vllm-project/vllm/issues/14986
这是一个Bug报告，主要涉及vllm 0.7.3-0.8.0rc3中出现的一个错误。由于一些原因导致了IndexError: list index out of range的错误。

https://github.com/vllm-project/vllm/issues/14985
这个issue是关于优化代码的提出需求，并涉及主要对象为V1版本的并行抽样实现。

https://github.com/vllm-project/vllm/issues/14984
这是一个缺少具体内容的issue，类型是其它类型，涉及主要对象为项目的更新。

https://github.com/vllm-project/vllm/issues/14983
这个issue类型是bug报告，涉及的主要对象是AMD的vLLM工具，由于模型架构'Qwen2_5_VLForConditionalGeneration'无法被检查而导致数值错误。

https://github.com/vllm-project/vllm/issues/14982
该issue属于代码优化类型，主要涉及AITER kernel在Fused MoE的集成，通过将AITER kernel选择逻辑从triton kernels wrapper类移动到UnquantizedFusedMoEMethod类来优化代码结构。

https://github.com/vllm-project/vllm/issues/14981
这是一个用户提出需求的issue，主要涉及到vLLM在Windows平台上添加CUDA支持。由于当前开源AI模型服务存在碎片化和复杂性，用户希望通过整合vLLM作为标准服务器来简化开发流程，提高用户体验。

https://github.com/vllm-project/vllm/issues/14980
这个issue是一个bug报告，涉及的主要对象是Gemma3模块。由于mismatch和token匹配问题，导致了prompt replacement和image embeddings不匹配的bug。

https://github.com/vllm-project/vllm/issues/14979
这是一个bug报告，主要涉及到模型权重的大小单位问题。由于PyTorch版本为2.5.1+cu124，导致模型权重以GiB为单位而非MB。

https://github.com/vllm-project/vllm/issues/14978
这个issue是一个bug报告，涉及的主要对象是vLLM加载适配器时出现的错误消息。导致这个问题的原因是vLLM在加载适配器时会出现错误，即使适配器在其他方面也可以完全正常工作。

https://github.com/vllm-project/vllm/issues/14977
这是一个bug报告，涉及主要对象是"Mistral-Small 3.1"，出现该问题的原因可能是新的mistralsmall3.1检查点导致了测试失败。

https://github.com/vllm-project/vllm/issues/14976
这是一个关于bug修复的issue，主要涉及对象为Olmo2接口，由于传入参数的错误导致了unexpected keyword argument 'inputs_embeds'错误。

https://github.com/vllm-project/vllm/issues/14975
这是一个Bug报告，涉及的主要对象是vllm中的一个模块。由于输入时传递了一个意外的关键字参数'inputs_embeds'，导致了此问题的发生。

https://github.com/vllm-project/vllm/issues/14974
这个issue是关于bug报告，涉及主要对象是V1的TPU代码。这个问题由于num_kv_heads参数为1时XLA无法完全平铺引起了测试失败，同时用户提出需求移除V0的测试并介绍V1 TPU代码更加灵活、集成了新的ragged paged kernel且性能更好。

https://github.com/vllm-project/vllm/issues/14973
这个issue属于用户提出需求类型，涉及对象是如何在torch 2.5.1版本中使用最新的main分支。这个问题可能是由于用户想要在现有环境中结合最新的代码功能，但需要一些指导来实现。

https://github.com/vllm-project/vllm/issues/14972
这是一个bug报告，主要涉及的对象是V1版本的Guard Against Main Thread Usage。这个问题可能是由于代码中未进行主线程使用的保护措施导致的，可能会导致程序在非主线程运行时出现问题。

https://github.com/vllm-project/vllm/issues/14971
这是一个bug报告，主要涉及的对象是Gemma3 MM模型。由于Gemmma3 MM在V1版本中还不准备好使用，所以在0.8.0版本中将其标记为`SupportsV0Only`。出现的bug是关于Gemma3离线批量推理和数值错误的问题。

https://github.com/vllm-project/vllm/issues/14970
这个issue类型是功能需求，主要涉及添加XFormers + FP8 KV Cache支持，可能涉及到关于该功能的意义和是否值得实现的讨论。

https://github.com/vllm-project/vllm/issues/14969
这是一个bug报告，涉及的主要对象是vllm模型加载错误。原因可能是模型hidden_size不匹配导致数值错误。

https://github.com/vllm-project/vllm/issues/14968
这是一个特性需求的issue，涉及的主要对象是向vLLM集成AITER的Block-Scaled GEMM功能。

https://github.com/vllm-project/vllm/issues/14967
这是一个特性需求的issue，主要涉及集成AITER提供的融合MoE核，对于未量化模型权重和动态per-tensor量化模型权重以及块fp8量化方法等情况提供支持。

https://github.com/vllm-project/vllm/issues/14966
这是一个Bug报告，主要涉及VLLM引擎中的模型配置错误，由于无法传递对象而导致无法pickle。

https://github.com/vllm-project/vllm/issues/14965
这是一个Bug报告，涉及到vllm（一个语言模型库）的Illegal Memory Access错误。用户在使用DeepSeek-R1模型时遇到了CUDA的illegal memory access错误，可能是参数设置不正确导致的。

https://github.com/vllm-project/vllm/issues/14964
该issue属于用户提出需求类型，主要涉及AITER Kernel Integration，并且由于一系列的PRs将kernels拆分，需要进行审查。

https://github.com/vllm-project/vllm/issues/14963
这是一个Bug报告，涉及主要对象是Gemina3模块，由于尝试分配了超过允许数量的多模态令牌到占位符，导致了数值错误。

https://github.com/vllm-project/vllm/issues/14962
这是一个改进功能性能的issue，主要涉及到在使用guide decoding时的sample性能问题。由于某些语法下的guide解码吞吐量过慢，于是提出了新的SampleV2算法来优化这一性能问题。

https://github.com/vllm-project/vllm/issues/14961
这个issue是关于Bugfix的，主要涉及的对象是Mixtral模型中的一个配置参数。原因是MixtralAttention模块没有正确使用MixtralConfig中定义的head_dim参数导致了这个bug。

https://github.com/vllm-project/vllm/issues/14960
这是一个bug报告，针对的主要对象是代码文件pixtral.py。由于代码行过长导致precommit无法通过，需要修复该问题。

https://github.com/vllm-project/vllm/issues/14959
该issue是一个功能特性请求，主要涉及的对象是将AITER的RMS Norm层功能整合到vLLM中。由于需要在ROCm平台上改进性能，因此进行了这项功能请求。

https://github.com/vllm-project/vllm/issues/14958
这个issue属于bug报告类型，涉及到vllm库的import错误，可能由于代码中的命名不一致导致了这些import错误。

https://github.com/vllm-project/vllm/issues/14957
这个issue类型是功能请求，该问题单涉及的主要对象是pixtral。由于缺乏patch merger功能，用户希望为pixtral添加这一功能。

https://github.com/vllm-project/vllm/issues/14955
这是一个文档问题的Issue，涉及多模态性能分析时存在的误导性日志，主要涉及模型长度和提示序列长度的概念混淆。

https://github.com/vllm-project/vllm/issues/14954
这是一个Bug报告，主要涉及使用vLLM在未指定平台上运行时引发NotImplementedError的问题。

https://github.com/vllm-project/vllm/issues/14953
这是一个bug报告，主要涉及torch.compiled的缓存哈希错误问题。原因是不同数值的VLLM_PP_LAYER_PARTITION导致相同的缓存哈希，引发了致命的启动错误。

https://github.com/vllm-project/vllm/issues/14952
这个issue是一个bug报告，主要涉及"Disaggregated Prefilling"功能，由于代码中使用了不同的TP导致了bug的出现。

https://github.com/vllm-project/vllm/issues/14951
这是一个Bug报告类型的issue，涉及到vLLM中tool_calls返回的数据与OpenAI标准不一致的问题。导致这个问题的原因是vLLM返回的数据格式与OpenAI标准不匹配，导致Adaptors无法按预期工作。

https://github.com/vllm-project/vllm/issues/14950
这是一个bug报告，涉及的主要对象是vLLM模型的权重加载过程。这个问题是由于在加载同时包含HF格式和Mistral格式权重的模型时出现错误，导致量化模型时出现错误。

https://github.com/vllm-project/vllm/issues/14949
这是一个bug报告，涉及vLLM中无法运行Phi-4-multimodal-instruct的问题，可能是由于参数设置错误导致的。

https://github.com/vllm-project/vllm/issues/14948
这是一个bug报告，主要是关于在macOS系统上在Tensor Parallel TP模式下启动失败的问题。由于在macOS上使用`tensorparallelsize 2`启动失败，导致出现这个bug。

https://github.com/vllm-project/vllm/issues/14947
这是一个Bug报告，涉及的主要对象是vllm中在macOS上使用`--tensor-parallel-size 2`启动失败的问题。这个问题的原因是在macOS上共享内存分配的大小和缓冲区大小无法完全相等，导致了AssertionError和macOS安全策略阻塞worker进程的问题。

https://github.com/vllm-project/vllm/issues/14946
这是一个bug报告类型的issue，主要涉及MLA kv_c_normed在gptq_marlin_gemm中的连续性问题，由于不连续导致AWQ在命中前缀缓存时引发错误。

https://github.com/vllm-project/vllm/issues/14945
这个issue类型是改进建议，主要涉及的对象是模型加载方式。由于之前使用的`AutoModelForVision2Seq`加载图像模型方式不准确，故提议改用`AutoModelForImageTextToText`以正确加载imagetotext模型。

https://github.com/vllm-project/vllm/issues/14944
该issue属于需求类型，主要涉及vllm在moe专家并行实现中使用all_reduce而不是all_to_all通信，以及寻找'torch.ops.vllm.moe_forward'函数的问题。这可能是因为用户想了解vllm的使用方式和特定功能的调用位置造成的。

https://github.com/vllm-project/vllm/issues/14943
这是一个用户提出需求的issue，主要涉及如何在当前环境中使用vllm进行特定模型的推理。用户不清楚如何将其集成到vllm中。

https://github.com/vllm-project/vllm/issues/14942
这个issue是一个Bug报告，涉及的主要对象是vllm的安装过程。由于缺少符号导致了ImportError，可能是由于编译或依赖关系错误所致。

https://github.com/vllm-project/vllm/issues/14941
这个issue类型是性能问题报告，主要涉及到模型推理的性能对比，由于启用了推测解码（speculative decoding）导致性能回归，希望解释为什么会出现这种情况。

https://github.com/vllm-project/vllm/issues/14940
这是一个用户提出需求的issue，主要涉及的对象是vllm中的attention adaptation模块，用户想知道如何修改kv cache。这个问题产生的原因是用户想在vllm中应用新的attention模块但需要进一步指导。

https://github.com/vllm-project/vllm/issues/14939
这是一个 Bug 类型的 issue，涉及的主要对象是 vLLM 中的 Speculative Decoding 功能。由于某种原因导致在使用指定的深度学习模型时出现 AssertionError 错误。

https://github.com/vllm-project/vllm/issues/14938
这是一个用户提出需求的问题单，主要对象是要将vLLM北京见面会的幻灯片添加到文档中。

https://github.com/vllm-project/vllm/issues/14937
这个issue类型是用户提出需求，主要对象是关于VLLM模型的KV缓存大小，用户希望实现Slim Attention特性来减少KV缓存大小。

https://github.com/vllm-project/vllm/issues/14936
这是一个Bug报告，涉及的主要对象是vllm模型的输出。由于CUDA版本不匹配可能导致在不同GPU上运行时输出的模型结果是一堆无意义的字符串。

https://github.com/vllm-project/vllm/issues/14935
这是一个关于支持 LoRA 的问题，涉及嵌入模型的 bug 报告和功能需求。

https://github.com/vllm-project/vllm/issues/14934
这是一个用户需求提出的issue，主要涉及添加`--seed`选项到离线多模式示例脚本中。由于示例脚本是用户可见的，开发者决定将默认种子保留为`None`，用户可以通过在命令中追加`seed 0`来检查模型实现。

https://github.com/vllm-project/vllm/issues/14933
该issue属于用户提出问题类型，主要涉及如何在OpenVINO后端上运行推理时利用多个GPU或GPU + CPU，导致用户对VLLM的分布式推理支持产生疑问。

https://github.com/vllm-project/vllm/issues/14932
这是一个bug报告，该问题涉及的主要对象是XPU设备管理。由于未将None作为设备容量，可能导致程序无法正确识别XPU设备的容量信息。

https://github.com/vllm-project/vllm/issues/14931
这是一个bug报告，主要涉及VLLM在运行大模型时CPU使用率无法超过100%的问题，用户希望了解是否受到VLLM的限制只能使用一个CPU。

https://github.com/vllm-project/vllm/issues/14930
这是一个优化性质的问题，主要涉及VLLM中的rejection sampler，由于使用了custom Triton kernels而带来的性能提升和其他优势。

https://github.com/vllm-project/vllm/issues/14929
这是一个bug报告，主要涉及的对象是V1模型。由于更新后的`_process_audio_input`每次返回一个音频块的张量，导致Ultravox模型在V1上出现问题。

https://github.com/vllm-project/vllm/issues/14927
这是一个需求类型的issue，主要涉及LRUCache的实现合并问题，由于现有代码中存在两种不同的LRUCache实现，可能会导致混淆。

https://github.com/vllm-project/vllm/issues/14926
这是一个代码优化类型的Issue，主要涉及AMD构建中的未使用变量清理，通过强制使用Wnounusedvariable来实现。

https://github.com/vllm-project/vllm/issues/14925
这个issue是一个Bug报告，涉及主要对象为V0引擎和Deepseek系列模型的自定义配置文件。由于V0引擎无法正常处理自定义配置文件导致的错误。

https://github.com/vllm-project/vllm/issues/14924
这是一个需求提出的issue，主要涉及vLLM库import时间过长的问题，可能是因为初始化CUDA上下文导致。

https://github.com/vllm-project/vllm/issues/14923
这个issue是一个功能需求，主要涉及vLLM的benchmark serving脚本，需要确保它不会导入vLLM并满足最低依赖要求。

https://github.com/vllm-project/vllm/issues/14922
该issue属于功能需求，主要涉及EAGLE模型的chunked prefill支持。由于EAGLE模型需要保存和传递非终端块的最后一个token的隐藏状态，需要对vllm进行扩展以支持此功能。

https://github.com/vllm-project/vllm/issues/14921
这是一个bug报告，主要涉及的对象是Vllm中的MLA（Meta Learning Architecture）。由于当前版本V0在本地调用时存在问题，导致需要使用V1版本修复。

https://github.com/vllm-project/vllm/issues/14920
这是一个bug报告类型的issue，涉及主要对象是V1 APC文档。由于文档中存在错误，导致需要修复V1 APC文档。

https://github.com/vllm-project/vllm/issues/14919
这是一个bug报告，涉及到vLLM的大规模线下推理启动失败问题，可能是由于端口冲突导致的。

https://github.com/vllm-project/vllm/issues/14917
这是一个功能需求类型的issue，主要涉及llama 3.2 vision和CUDA graph支持。由于当前缺乏CUDA graph支持，需要对mllama进行相应的修改以实现此功能。

https://github.com/vllm-project/vllm/issues/14916
这是一个功能特性需求的issue，主要涉及了AITER Linear kernel的支持在vLLM框架中的集成。这个issue主要讨论了如何启用和性能比较，以及由于使用了AITER Linear导致的性能提升。

https://github.com/vllm-project/vllm/issues/14915
这是一个bug报告，涉及的主要对象是CUDA环境。由于request.num_computed_tokens大于request.num_tokens导致了该assert错误的发生。

https://github.com/vllm-project/vllm/issues/14914
这是关于bug报告的issue，主要涉及的对象是vLLM在ROCm平台上的PyTorch支持，由于PyTorch在构建过程中未使用MAGMA和USE_MAGMA标志，导致无法在CUDA张量上使用torch.linalg.cholesky函数。

https://github.com/vllm-project/vllm/issues/14913
这个issue是一个Bug报告，主要涉及的对象是torch模型中的部分覆盖问题。由于之前版本无法正确覆盖两个部分的模型内容，导致无法使用``进行正确覆盖操作。

https://github.com/vllm-project/vllm/issues/14911
这是一个Bug报告，涉及到vllm工具在运行`vllm serve`命令时出现了UserWarning提示。由于某些原因导致PostGradPassManager的序列化被跳过，用户希望了解这个警告的影响程度以及是否可以忽略。

https://github.com/vllm-project/vllm/issues/14910
这是一个Bug报告，该问题主要涉及于MLA + V1 + TP==1导致了CUDA上下文的重新初始化，原因是在`check_and_update_config`中进行了过多的导入操作，需要进行代码重构以解决该问题。

https://github.com/vllm-project/vllm/issues/14909
这个issue是一个bug报告，涉及的主要对象是XPU。这个bug是由于IPEX API的变更导致的问题。

https://github.com/vllm-project/vllm/issues/14908
这是一个用户提出需求的issue，涉及到支持序列并行的功能。由于模型如llama需要支持TP的序列并行，因此需要修改相关层以支持SP并实现一些TODOs。

https://github.com/vllm-project/vllm/issues/14907
这是一个需求提出类型的issue，主要涉及对象是ConstantList类，用户提出了需要为ConstantList类添加__repr__方法的需求。

https://github.com/vllm-project/vllm/issues/14906
这是一个功能需求，主要涉及到增强与高级负载均衡/网关的集成，实现更好的负载/成本报告和 LoRA 管理。

https://github.com/vllm-project/vllm/issues/14905
这个issue类型为性能优化，主要对象是"overhead of rewinding"。

https://github.com/vllm-project/vllm/issues/14904
这是一个bug报告，涉及的主要对象是获取模型配置中的词汇量大小。由于xgrammar处理额外特殊token的方式，导致一直在修改代码以适应其处理方式，但0.1.16版本应该已修复此问题。

https://github.com/vllm-project/vllm/issues/14903
这是一个关于功能需求的issue，主要涉及到启用`v1/entrypoints`，由于之前的skip操作导致该需求未被执行。

https://github.com/vllm-project/vllm/issues/14902
这是一个性能优化类的issue，主要涉及到VLLM engine的性能问题。导致这个问题的原因是高QPS下性能较差，用户提出希望在engine core初始化后冻结gc以提升性能。

https://github.com/vllm-project/vllm/issues/14901
这是一个bug报告，涉及的主要对象是使用`ccache`结合`pip install -e .`时的问题。由于`pip`为每次构建创建一个带有随机名称的新文件夹，阻止了`ccache`识别正在构建的相同文件，导致`ccache`无法有效工作，用户希望在文档中添加指导来解决这个问题。

https://github.com/vllm-project/vllm/issues/14899
这是一个bug报告，主要涉及vllm在使用torchrun时出现的问题。由于设置`VLLM_DP_SIZE`为4导致进程hang，可能是由于CC([core] set up data parallel communication)与外部启动器不兼容所致。

https://github.com/vllm-project/vllm/issues/14898
这是一个bug报告，主要涉及CI工具调用测试，导致夜间构建失败。

https://github.com/vllm-project/vllm/issues/14897
这个issue类型是bug报告，主要涉及Gemmi3的Offline Batch Inference，由于processor相关错误导致了"Attempted to assign XXX multimodal tokens to YYY placeholders"错误。

https://github.com/vllm-project/vllm/issues/14896
这是一个bug报告，涉及到Gemmas3批量离线推理错误的问题，主要问题是模型运行器不能检测模型在使用交替局部和全局注意力时的滑动窗口使用。

https://github.com/vllm-project/vllm/issues/14894
这是一个bug报告，主要涉及"Sampling metadata"的问题，原因是导致了overhead相关的bug。

https://github.com/vllm-project/vllm/issues/14893
这个issue类型为质量改进（Quality Improvement），涉及主要对象为测试设置（test settings），原因是为了提高测试的可重复性。

https://github.com/vllm-project/vllm/issues/14892
这是一个关于 bug 的报告，涉及的主要对象是 V1 模型。由于该 PR 在 V1 上无效，导致需要撤销操作。

https://github.com/vllm-project/vllm/issues/14891
这是一个关于Windows环境下vLLM CUDA支持安装问题的bug报告，主要涉及到在Windows环境下安装和构建vLLM的相关困难。

https://github.com/vllm-project/vllm/issues/14890
这是一个用户提出需求的特性建议。主要对象是使用guided_decoding来实现Function Calling。该建议起因于希望确保任何模型都按要求的schema进行输出。

https://github.com/vllm-project/vllm/issues/14889
这是一个bug报告，涉及主要对象是V1模型。由于V1现在作为默认选项，需要应用`SupportsV0Only`以禁用Phi-4-multimodal，确保在运行模型时自动选择V0。

https://github.com/vllm-project/vllm/issues/14888
这是一个bug报告类型的issue，主要对象是使用vLLM模型的用户。由于缺少'triton'模块，导致出现了`ModuleNotFoundError: No module named ‘triton’`的错误信息。

https://github.com/vllm-project/vllm/issues/14887
这个issue是一个bug报告，涉及主要对象是gptq_marlin_gemm函数调用。由于PyTorch版本和CUDA版本不匹配，导致了错误的bug。

https://github.com/vllm-project/vllm/issues/14886
这个issue是关于bug报告，涉及的主要对象是Phi4mini函数调用支持，症状为Phi4mini函数调用的问题。

https://github.com/vllm-project/vllm/issues/14885
这是一个Bug报告，主要涉及VLLM（Very Large Language Model）代码库，由于CUDA错误导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/14884
这是一个Bug报告，主要涉及的对象是从磁盘加载chat模板时可能出现的异常情况。产生这个问题是因为在尝试检测chat模板内容格式时会抛出异常，虽然会捕捉异常并默认为"string"内容格式，但在输出中看到异常并不友好。

https://github.com/vllm-project/vllm/issues/14883
这是一个bug报告，涉及到项目中的 Model 模块，主要原因是请求具有不同 num_images 的情况下，Llama3.2 无法继续处理。

https://github.com/vllm-project/vllm/issues/14882
这个issue属于bug报告类型，主要涉及的对象是vLLM的DeepSeek R1模块。由于没有正确配置FlashMLA作为推断后端，导致无法解决32k token限制问题。

https://github.com/vllm-project/vllm/issues/14881
这个issue类型是bug报告，主要涉及Gemma 3 在进行离线批量推理时无法使用滑动窗口注意力机制，导致使用llm.chat()处理多个消息时失败。

https://github.com/vllm-project/vllm/issues/14880
这是一个bug报告，涉及到Phi-4-mini函数调用支持的修复。这个issue主要是由于Phi-4-mini函数调用支持出现问题导致的bug。

https://github.com/vllm-project/vllm/issues/14879
这是一个改进建议类型的issue，主要涉及benchmark脚本的优化。原因是为了提高性能，避免默认情况下保存详细信息到json中。

https://github.com/vllm-project/vllm/issues/14878
这是一个bug报告，主要涉及的对象是ngram kmp tests。该问题很可能是由于测试用例存在错误导致的。

https://github.com/vllm-project/vllm/issues/14877
这是一个用户提出需求的issue，涉及的主要对象是添加更多调谐配置。由于vLLM缺少某些NVIDIA GPU的Triton配置，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/14876
这个issue属于需求提出类型，主要对象是 请求多模型模型。由于项目变更，导致链接更新至新的项目板。

https://github.com/vllm-project/vllm/issues/14875
这个issue是关于优化代码性能的建议，主要涉及模型运行器中聚合分块提示日志概率的操作。

https://github.com/vllm-project/vllm/issues/14874
这是一个用户提出需求的issue，主要涉及的对象是 benchmark README。这个问题由于需要修改默认的文本到文本示例以使用 `vllm` 后端，以便更好地与离线情况保持一致。

https://github.com/vllm-project/vllm/issues/14873
这是一个需求更改的issue，主要涉及的对象是 mistral-tokenizer 模块。

https://github.com/vllm-project/vllm/issues/14872
这是一个需求提出类型的 issue，主要涉及到添加一个用于测试 `--load-format sharded_state` 功能的示例脚本。

https://github.com/vllm-project/vllm/issues/14871
这是一个用户提出需求的类型，主要涉及的对象是VLLM库中的sequence parallel功能。由于还未支持与PP的组合和更多模型，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/14870
这是一个bug报告类型的issue，主要涉及的对象是torch distributed stateless PG backend的初始化过程。这个问题的原因是torch 2.5版本中移除了`ProcessGroup.Options`，导致影响了data parallel功能，并且由于测试脚本即使子进程失败也会成功退出，导致最初未能发现该问题。

https://github.com/vllm-project/vllm/issues/14869
这是一个增加测试内容的issue，主要涉及到构建和持续集成。

https://github.com/vllm-project/vllm/issues/14868
这个issue是一个bug报告，涉及的主要对象是vLLM中关于vocab_size的构建匹配器，由于xgrammar更新了一些core API，导致之前版本依赖的功能无法使用，因此出现了这个bug。

https://github.com/vllm-project/vllm/issues/14867
这个issue是一个bug报告，主要涉及VLLM下的compiled graph生成错误，由于不同的VLLM_PP_LAYER_PARTITION值会导致生成相同的cache hash，进而导致出现致命的启动错误。

https://github.com/vllm-project/vllm/issues/14866
这是一个用户提出需求的issue，主要涉及的对象是要求vllm添加对CohereForAI/c4aicommanda032025模型的支持。由于需要Properly支持tokenizer和templates，以及对模型进行调用，用户提出了这个问题。

https://github.com/vllm-project/vllm/issues/14865
这是一个用户提出需求的类型，主要涉及将vLLM部署在仅使用CPU的Kubernetes环境中。这是因为之前的部署要求GPU作为先决条件，用户提出希望提供只使用CPU的部署指南。

https://github.com/vllm-project/vllm/issues/14864
这个issue类型是功能改进，涉及主要对象为V1版本的多模态模型。

https://github.com/vllm-project/vllm/issues/14863
这是一个关于用户提出需求的问题，主要涉及VLLM引擎如何跳过新请求的预填充步骤。这个问题可能由于用户想要在添加新请求到引擎时跳过预填充步骤，直接开始解码阶段而导致。

https://github.com/vllm-project/vllm/issues/14862
这是一个用户提出需求的issue，主要涉及DP MLA + EP/TP MoE算法在在线服务中的使用问题。由于缺乏相应文档，用户无法找到如何使用该功能的方法。

https://github.com/vllm-project/vllm/issues/14861
这个issue类型是功能需求提出，涉及的主要对象是Llama模型。这个提议是为了通过hf-overrides调整Llama模型中的隐藏层数，以帮助在开发自定义内核、后端和其他组件时减少测试时间。

https://github.com/vllm-project/vllm/issues/14860
该issue为优化类型，主要涉及Intel GPU CI docker构建流程，由于需要在Intel GPU CI队列中添加多个代理，因此需要重新定义docker镜像名称和容器名称。

https://github.com/vllm-project/vllm/issues/14858
这是一个bug报告，主要涉及的对象是dtype检测和默认设置。这个问题由于之前的dtype检测方法不完善，导致在测试使用bfloat16模型时可能产生溢出错误的症状，以及多模态模型中dtype检测不准确的问题。

https://github.com/vllm-project/vllm/issues/14857
这是一个bug报告，主要涉及mamba2模型在预加载性能调整中存在过多的内存拷贝，导致性能下降。

https://github.com/vllm-project/vllm/issues/14856
这是一个关于安装问题的issue，涉及主要对象是vllm。原因可能是由于无法完成离线安装导致的错误。

https://github.com/vllm-project/vllm/issues/14855
这是一个功能需求的issue，主要涉及VLLM工具中在config.yaml中指定模型的功能。

https://github.com/vllm-project/vllm/issues/14854
这是一个bug报告，主要涉及到vLLM ModelConfig中的hf_overrides参数未能传递给get_hf_image_processor_config，导致无法通过auth token从huggingface获取自定义模型。

https://github.com/vllm-project/vllm/issues/14853
该issue类型为需求提出，主要涉及的对象是CI/Build。由于需要将dockerfile移动，用户可能需要重新配置持续集成构建流程。

https://github.com/vllm-project/vllm/issues/14852
这是一个用户提出需求的类型，该问题单涉及添加新的 meetups 到 README 和 meetups.md 页面。由于缺少East Coast vLLM Meetup slides的链接，用户请求将其添加到文档中。

https://github.com/vllm-project/vllm/issues/14851
这是一个bug报告，主要涉及到统计vocab_size问题。导致这个bug的原因可能是之前计算vocab_size时存在错误，需要提前计算。

https://github.com/vllm-project/vllm/issues/14850
这个issue类型是bug报告，涉及的主要对象是gemm2和gemm3库。这个问题出现的原因是存在一个误导性的重量加载检查消息，希望通过该issue将其移除以消除混淆。

https://github.com/vllm-project/vllm/issues/14849
这是一个与Bug报告相关的Issue，主要涉及LoRA bias测试的删除，由于实现存在一些问题，因此临时移除测试以减少CI测试压力。

https://github.com/vllm-project/vllm/issues/14848
这个issue属于Bug报告，涉及到Mamba2 Prefill Performance Tweaks模块，由于某些性能调整导致不必要的内存拷贝，需要撤销相关提交。

https://github.com/vllm-project/vllm/issues/14847
这是一个bug报告，主要涉及的对象是V1版本的Ray Compiled Graph测试，由于V0版本存在不确定性，导致V1版本测试未能及时抓住错误。

https://github.com/vllm-project/vllm/issues/14846
这是一个bug报告，主要涉及的对象是vLLM中的ragged paged attention kernel，由于之前的padding导致了效率低下，现在需要移除padding以提高内核的运行效率。

https://github.com/vllm-project/vllm/issues/14845
这是一个Bug报告，主要涉及vLLM在版本0.7.0中的TTFT性能回归问题，导致TTFT变慢3倍。原因可能是升级版本后的代码变动所致。

https://github.com/vllm-project/vllm/issues/14844
这个issue类型是bug报告，主要涉及的对象是torch_xla库。由于seed值为None时出现问题导致的bug。

https://github.com/vllm-project/vllm/issues/14843
这是一个功能需求类型的issue，主要涉及V1 tpu_model_runner的测试添加。由于当前测试不足，需要增加更多测试以提高代码质量。

https://github.com/vllm-project/vllm/issues/14842
这是一个bug报告，涉及到在FlashMLA默认启用时，MLA缓存对齐带来的内存浪费问题。

https://github.com/vllm-project/vllm/issues/14841
这是一个bug报告类型的issue，主要涉及deepseek_r1_reasoning_parser的更新适配问题，由于新的chat_template更改导致了输出中不再产生\符号。

https://github.com/vllm-project/vllm/issues/14840
这是一个与软件漏洞修复相关的issue，主要涉及升级aiohttp库以解决CVE漏洞的问题。造成这个issue的原因是旧版本的aiohttp存在漏洞可能导致安全问题。

https://github.com/vllm-project/vllm/issues/14839
这是一个安全性漏洞的问题报告，涉及主要对象为jinja2库版本。原因是旧版本中存在三个中等严重的CVE漏洞，因此需要升级到3.1.6以上版本以避免潜在安全漏洞。

https://github.com/vllm-project/vllm/issues/14838
这个issue类型是功能改进请求，主要涉及的对象是TPU作业功能。由于日志清理命令引起了混淆和干扰，需要将其从TPU作业中移除。

https://github.com/vllm-project/vllm/issues/14837
这是一个bug报告，涉及的主要对象是vLLM下的Outlines缓存功能。原因是当恶意用户发送大量短解码请求时，会导致缓存过多，可能引发文件系统空间不足的拒绝服务（DoS）问题。

https://github.com/vllm-project/vllm/issues/14836
这个issue为功能增强类型，主要涉及的对象是vllm项目中的ROCm平台下的注意力机制，其中涉及添加了AITER PagedAttention类、移动了AITER环境变量检查、在非8位数据类型上禁用AITER paged attention以及添加了回退机制。

https://github.com/vllm-project/vllm/issues/14835
这个issue属于需求类型，涉及的主要对象是构建/持续集成（Build/CI）系统。这个问题提出了将ninja移动到通用依赖项的要求，从而保证符合xgrammar的要求，提交者Russell Bryant签名支持。

https://github.com/vllm-project/vllm/issues/14834
这是一个需求类型的issue，主要涉及的对象是添加TPU V1的CI测试，原因可能是为了确保系统对TPU V1的兼容性以及持续集成测试的完整性。

https://github.com/vllm-project/vllm/issues/14833
这个issue类型是bug报告，涉及的主要对象是V1版本的模型参数化问题。由于之前方法中将model_name设置为列表，导致LLM类运行失败，现在修复为将两个模型作为参数传递来进行测试。

https://github.com/vllm-project/vllm/issues/14832
该issue类型是功能需求提交，主要对象是Entrypoints Test。由于原因导致了该需求未被启用。

https://github.com/vllm-project/vllm/issues/14831
这是一个需求提出类型的issue，主要对象是vllm的CI功能。因为需要在vllm CI中添加一个运行TPU V1测试的脚本，说明用户希望在CI流程中测试TPU V1功能，以确保代码的兼容性和稳定性。

https://github.com/vllm-project/vllm/issues/14830
这是一个用户提出需求的类型，该问题单涉及的主要对象是添加TPU V1测试。

https://github.com/vllm-project/vllm/issues/14829
这是一个bug报告，主要涉及Neuron项目中的Docker运行命令的更新。由于需要启用Docker用户命名空间重新映射且调整Neuron设备配置，所以需要进行相应的更改。

https://github.com/vllm-project/vllm/issues/14828
这是一个Bug报告，主要涉及Gemini3模型无法加载某些权重的问题。由于Python版本更新到3.11.11，可能导致Gemini3在加载权重时出现问题。

https://github.com/vllm-project/vllm/issues/14827
这个issue属于用户提出需求类型，主要涉及LoRA的float32支持问题，用户因为当前环境不支持float32而提出了问题。

https://github.com/vllm-project/vllm/issues/14826
这是一个bug报告，主要涉及的对象是V1 structured output + Llama3.18BInstruct。由于计算vocab size时出现错误，导致出现了vocab size与max token ID不匹配的问题。

https://github.com/vllm-project/vllm/issues/14825
这是一个升级请求。它涉及的主要对象是vllm库中的bitsandbytes。这个问题由于与triton==3.2不兼容性而导致，需要升级来解决此问题。

https://github.com/vllm-project/vllm/issues/14824
这个issue是一个Bugfix类型的报告，主要涉及到前端功能支持prefill decode disaggregation on deepseek。该问题是由于在deepseek v2中1P1D情况下没有得到正确的输出而引发的。

https://github.com/vllm-project/vllm/issues/14823
这是一个bug报告，主要涉及到Aria test loading的问题，因为缺少模型下载导致无法本地测试。

https://github.com/vllm-project/vllm/issues/14821
这是一个 Bugfix 类型的 issue，涉及到前端代码中的正则表达式检查问题。由于存在错误的正则表达式解析器，导致了在理由完整生成中的问题。

https://github.com/vllm-project/vllm/issues/14820
这是一个bug报告，问题涉及主要对象为`MambaCacheManager`，由于`seq_group`在达到`max_model_len`时被错误地移到了`scheduler._async_stopped`而不是`scheduler._finished_requests_ids`，导致了IndexError错误。

https://github.com/vllm-project/vllm/issues/14819
这是一个用户提出需求的issue，涉及对象是指定在`config.yaml`文件中指定`model`的功能。用户提出希望不仅可以通过CLI指定`model_tag`，也可以通过配置文件指定。

https://github.com/vllm-project/vllm/issues/14818
这是关于Vllm whisper模型在使用verbose_json参数时出现错误的bug报告。

https://github.com/vllm-project/vllm/issues/14817
这是一个Bug报告，涉及Gemma-3模型输出为空文本的问题。由于使用LLM引擎运行Gemma3模型时，4/12/27B模型输出为空文本，仅Gemma31bit模型正常输出文本内容。

https://github.com/vllm-project/vllm/issues/14816
这是一个关于VLLM使用时遇到的问题，主要涉及到模型长度、多模态嵌入和警告信息。由于未正确设置模型长度、最大序列个数以及多模态嵌入的保留标记，导致出现了警告信息和潜在的推断失败问题。

https://github.com/vllm-project/vllm/issues/14815
这个issue是一个bug报告，主要涉及对象是FlashInfer>=v0.2.3，由于FlashInfer在topp和topk采样中引起错误，导致了需要对其进行修复。

https://github.com/vllm-project/vllm/issues/14814
这是一个Bug报告类型的Issue，涉及的主要对象是Gem-3-27b-it-GPTQ模型在sm75环境下无法运行，可能由于安装最新的vllm和transformers版本后导致。

https://github.com/vllm-project/vllm/issues/14813
这是一个bug报告，问题涉及主要对象是vllm软件在Windows环境下无法启动，原因是缺少uvloop模块。

https://github.com/vllm-project/vllm/issues/14812
这是一个需求提出类型的issue，涉及主要对象是VLM（Vision Language Model），由于代码中存在重复的navit实现，并需要处理跳过的层次，希望进行整理和优化。

https://github.com/vllm-project/vllm/issues/14811
这个issue是关于bug报告，涉及的主要对象是vllm中的sampling API，由于去除了早期设计中的返回值可能会导致不兼容的bug。

https://github.com/vllm-project/vllm/issues/14810
这是一个bug报告类型的issue，主要涉及的对象是在ROCm平台上创建新进程时的兼容性问题。由于现有的`fork_new_process_for_each_test`装饰器在ROCm平台上表现不佳，因此引入了新的函数`spawn_new_process_for_each_test`和`create_new_process_for_each_test`来解决兼容性问题。

https://github.com/vllm-project/vllm/issues/14809
这个issue是一个Bug报告，主要涉及的对象是vllm项目中的deepseek模型。由于未考虑`first_k_dense_replace`值大于1的情况，导致了在deepseek_v2模型上存在的fp16溢出问题。

https://github.com/vllm-project/vllm/issues/14808
这是一个bug报告类型的issue，主要涉及vllm编译工作进程数量过多的问题。由于启动vllm serve后出现了32个compile_worker子进程，用户想了解如何减少进程数量。

https://github.com/vllm-project/vllm/issues/14807
这是一个bug报告，主要涉及的对象是vllm项目下的CUDA_VISIBLE_DEVICES环境变量设置问题。由于executor会固定选择GPU设备，导致用户无法通过设置CUDA_VISIBLE_DEVICES环境变量指定设备，需要修复这一问题。

https://github.com/vllm-project/vllm/issues/14806
这是一个bug报告类型的issue，主要涉及的对象是VLLM中的PixtralHF和Molmo模块。导致这个bug的原因是类型检查问题导致num_crops无法正确加载，以及chat template自动加载bug导致PixtralHF异常。

https://github.com/vllm-project/vllm/issues/14805
这是一个需求提出类型的 issue，主要涉及 VLLM（Very Large Language Model）的多模态输入缓存限制。原因是基于内存使用而不是多模态项目数量来限制输入缓存，以解决存储问题和性能优化。

https://github.com/vllm-project/vllm/issues/14804
这个issue属于文档需求类型，主要涉及load_format项的补充，由于在文档中缺少load_format的相关内容，导致用户可能无法正确使用该功能。

https://github.com/vllm-project/vllm/issues/14803
这是一个功能增强类型的issue，主要涉及的对象是ROCm Flash Attention后端，问题是关于启用ROCm Flash Attention用于编码-解码模型。

https://github.com/vllm-project/vllm/issues/14802
这个issue类型为用户提出需求，涉及主要对象为使用vLLM时多个不同品牌和频率的4090显卡。用户询问vLLM是否能在这种设置下正常工作，以及如何集成vLLM进行特定模型的推断。

https://github.com/vllm-project/vllm/issues/14801
这是一个bug报告类型的issue，主要涉及vllm软件在安装过程中遇到的错误。由于使用pip install -e . 安装vllm源代码时出现的错误导致问题产生。

https://github.com/vllm-project/vllm/issues/14800
这是一个代码清理类型的issue，其主要涉及调度度量，由于建议出现代码混乱而导致了此问题。

https://github.com/vllm-project/vllm/issues/14799
这是一个bug报告，主要涉及vLLM服务的CPU使用率异常高的问题。

https://github.com/vllm-project/vllm/issues/14798
这是一个bug报告，主要涉及到csrc文件中未使用的变量需要注释以满足编译器的要求。

https://github.com/vllm-project/vllm/issues/14797
这个issue是关于bug报告，涉及的主要对象是vllm项目中的Gem3ForConditionalGeneration模块。这个问题产生的原因可能是由于LoRA支持方面的错误或缺失导致的。

https://github.com/vllm-project/vllm/issues/14796
这是一个bug报告，涉及cutlass block fp8 binding，由于与CUDA >= 12.4的兼容性问题导致随机/空输出。

https://github.com/vllm-project/vllm/issues/14795
这是一个Bug报告类型的issue，主要涉及VLLM的benchmark_serving功能。由于未能接收到有效的数据块以计算TTFT，导致产生了错误的响应信息。

https://github.com/vllm-project/vllm/issues/14794
这是一个需求提出类型的issue，涉及的主要对象是`SupportsMultiModal`接口。

https://github.com/vllm-project/vllm/issues/14793
这是一个bug报告，涉及主要对象是Streaming delimiter，原因是由于一个小错别字在示例中引起了混淆。

https://github.com/vllm-project/vllm/issues/14792
这是一个bug报告，主要涉及问题是关于设置timeout未生效导致部分请求超过时间限制。

https://github.com/vllm-project/vllm/issues/14791
这是一个文档错误类型的issue，主要涉及Transformers模块的文档，由于简单的拼写错误导致文档中出现了错误。

https://github.com/vllm-project/vllm/issues/14790
这是一个Bug报告，主要涉及vLLM模型在处理GLM4V模型的位置id时出现问题，导致在TextVQA上得分较低的情况。

https://github.com/vllm-project/vllm/issues/14789
这是一个bug报告，关于vLLM团队的Gemme3ForConditionalGeneration模型是否支持LoRA的问题。由于文档与实际代码的不一致，用户提出了是否需要更新文档的疑问。

https://github.com/vllm-project/vllm/issues/14788
这是一个bug报告，主要涉及的对象是FlashInferbased rejection sampling功能。原因是该功能不稳定，导致需要暂时禁用。

https://github.com/vllm-project/vllm/issues/14787
这是一个bug报告，主要涉及处理器测试的相关问题，由于重构导致无法正确检索模型ID，最终导致了CI出现问题。

https://github.com/vllm-project/vllm/issues/14785
这是一个bug报告，主要涉及的对象是VLLM项目中的Whisper模型，问题是由于特定条件下导致的profiling运行序列长度大于最大模型长度。

https://github.com/vllm-project/vllm/issues/14784
这个issue类型是bug报告，主要涉及的对象是mllama.py，导致了在处理混合文本和图像请求时出现了未对齐的问题。

https://github.com/vllm-project/vllm/issues/14783
这是一个文档修正类的 issue，主要涉及到修正 Markdown 文件中的拼写和语法错误。原因是提高文档的可读性和准确性。

https://github.com/vllm-project/vllm/issues/14782
这是一个需求类型的issue，主要涉及的对象是"fastcheck"。由于测试数量过多，用户希望减少其数量以提高效率。

https://github.com/vllm-project/vllm/issues/14781
这是一个提出需求的issue，主要涉及调度器（Scheduler）模块，旨在讨论使用字典（dict）作为运行队列的潜在优势。

https://github.com/vllm-project/vllm/issues/14780
这个issue类型是缺少内容的bug报告，主要对象为"My"。缺少具体内容可能是由于提交者漏填写信息导致。

https://github.com/vllm-project/vllm/issues/14779
这是一个功能需求类型的issue，涉及V1版本的guidance作为后端来支持结构化输出，同时添加了一个名为`auto`的新后端模式。

https://github.com/vllm-project/vllm/issues/14778
这是一个优化性能的issue，主要涉及到vLLM中的Mamba2 Prefill功能。造成这个问题的原因是频繁的内存复制导致性能下降。

https://github.com/vllm-project/vllm/issues/14777
这是一个Bug报告，涉及到V1 CI tests，由于未结束的生成文本和重复答案导致了CUDA IMA（Intermittent CUDA IMA）问题。

https://github.com/vllm-project/vllm/issues/14776
这是一个用户提出需求的issue，涉及到Truncation control for embedding models。由于缺乏截断控制功能，导致需要对模型输入的提示进行裁剪。

https://github.com/vllm-project/vllm/issues/14775
这是一个用户提出需求的issue，主要涉及如何在benchmark_serving.py中添加Sampling Parameters（n和best_of），用户希望能够通过使用Sampling Parameters来运行推理，但不知道如何将其整合到vllm serving benchmark中。

https://github.com/vllm-project/vllm/issues/14774
这是一个bug报告，涉及前端日志消息中使用http而不是https的问题，导致在使用SSL时显示错误的URL协议。

https://github.com/vllm-project/vllm/issues/14773
这是一个用户提出需求的issue，主要涉及V1 TPU，用户希望默认启用前缀缓存。

https://github.com/vllm-project/vllm/issues/14772
这个issue类型是小修复（Minor），主要对象是`SamplingParams`类的`__post_init__()`方法。由于作者在做其他更改时注意到这个问题，可能是为了简化代码结构或提高代码可读性而提出的。

https://github.com/vllm-project/vllm/issues/14771
这个issue类型是功能优化，主要涉及的对象是处理器测试代码。原因是为了让`build_model_context`自动使用来自`HF_EXAMPLE_MODELS`的配置。

https://github.com/vllm-project/vllm/issues/14770
This issue is a feature request related to ML model performance optimization, specifically regarding a memory usage reduction and computational efficiency improvement in the VLLM project.

https://github.com/vllm-project/vllm/issues/14769
这是一个修复bug的issue，涉及主要对象是MLA模块。这个issue的原因是之前的更改导致了性能问题，需要进行部分撤销。

https://github.com/vllm-project/vllm/issues/14768
这是一个bug报告，主要涉及的对象是vLLM被用作Ray actor，由于默认的`VLLM_WORKER_MULTIPROC_METHOD`是`fork`，导致在Ray actor中使用fork会引起未定义的行为，进而在使用V1的放置组方法时造成挂起。

https://github.com/vllm-project/vllm/issues/14767
这是一个Bug报告，涉及对象是vLLM的api_server.py文件。由于代码中错误地显示了http启动信息而实际上是https，导致了这个Bug的症状。

https://github.com/vllm-project/vllm/issues/14766
这个issue是一个Bug报告，主要涉及Gemini3 GGUF模型的支持问题，由于无法识别gguf模型类型为gemma3而导致。

https://github.com/vllm-project/vllm/issues/14765
这个issue是一个Bug报告，涉及主要对象为V1版本的IMA，出现的症状是在使用ngram spec解码和flashinfer时出现问题。

https://github.com/vllm-project/vllm/issues/14764
该issue属于提出需求类型，主要涉及到为多模态模型检查输入形状定义Schema。由于当前对多模态输入的验证仅进行了最小限度的检查，导致一些模型仅检查输入的类型，实际输入形状可能与文档中所述不符。

https://github.com/vllm-project/vllm/issues/14763
这个issue是一个Bug报告，主要涉及的对象是pipeline parallel deployment中的内存使用不平衡。由于内存使用不平衡导致了该问题的症状。

https://github.com/vllm-project/vllm/issues/14762
这是一个Bug报告，主要涉及输入图片以支持Gemma3，原因可能是提示文本缺少/错误令牌导致找不到对应的更新。

https://github.com/vllm-project/vllm/issues/14761
这是一个Bug报告类型的Issue，主要涉及vllm下的deepseek_r1_reasoning_parser.py模块，由于可能是索引超出范围导致的异常。

https://github.com/vllm-project/vllm/issues/14760
这是一个bug报告，涉及代码中的错误，导致了pp_rank计算混乱的问题。

https://github.com/vllm-project/vllm/issues/14759
这是一个bug报告，涉及vLLM版本0.6.3中调用reset_state_for_recompute()导致的AssertionError错误问题。

https://github.com/vllm-project/vllm/issues/14758
这个issue类型是功能改进，涉及V1引擎在启动过程中错误处理的优化，主要对象是启动时的错误提示信息。原因是要提高启动过程中的错误信息提示和可扩展性。

https://github.com/vllm-project/vllm/issues/14757
这是一个用户提出需求的issue，主要涉及到文档缺失的问题，可能是由于未更新或遗漏导致的。

https://github.com/vllm-project/vllm/issues/14756
这个issue是用户需求报告，主要涉及用户寻找公开的vllmcpu镜像，但无法找到官方提供的镜像。

https://github.com/vllm-project/vllm/issues/14755
这个issue是关于用户提出需求的类型，主要涉及支持custom all reduce for rocm。由于缺少该功能，用户提出了希望添加支持custom all reduce for rocm的需求。

https://github.com/vllm-project/vllm/issues/14754
这是一个bug报告，主要涉及的对象是vllm项目下的uvicorn-access-log参数。该问题由于缺少对--disable-uvicorn-access-log参数的支持，导致http请求日志无法正确禁用。

https://github.com/vllm-project/vllm/issues/14753
这是一个bug报告，涉及对象是vllm中的Gemba3 GGUF支持。由于安装最新版本的transformers后在运行时出现了错误，可能是由于版本不兼容导致的。

https://github.com/vllm-project/vllm/issues/14752
这是一个功能建议类型的 issue，主要涉及修改多模态输入时出现当前计数大于允许计数引发 ValueError 的情况。由于当前实现导致客户端无法获取错误信息，建议修改为响应最后的允许计数项。

https://github.com/vllm-project/vllm/issues/14751
这是一个用户提出需求的issue，主要对象是扩展CI范围以使用两个卡来测试hpu设备，防止多卡并行导致的问题。

https://github.com/vllm-project/vllm/issues/14750
这是一个关于软件包错误的bug报告，主要涉及的对象是CentOS7上的vllm项目。由于软件包错误，导致在CentOS7环境下使用Python 3.10.16时出现问题。

https://github.com/vllm-project/vllm/issues/14749
这是一个用户提出需求的类型，主要涉及MoE模型在VLLM中的专家并行化应用问题。用户想要定制MoE模型专家层的放置方式，例如将不同的专家放置在不同的GPU上。

https://github.com/vllm-project/vllm/issues/14748
这个issue类型是用户提出需求，涉及主要对象是改进启动时的错误处理；用户希望实现的功能是改进启动时的错误处理。

https://github.com/vllm-project/vllm/issues/14747
这是一个bug报告，涉及vLLM在XPU上无法从源代码编译的问题。由于编译vLLM失败，导致无法运行vLLM。

https://github.com/vllm-project/vllm/issues/14746
这是一个功能更新提议的issue，主要涉及的对象是代码中的`need_kv_parallel_group`函数。由于代码更新替换了`need_kv_parallel_group`函数，在初始化KVTransferAgent时使用`is_kv_transfer_instance`，导致`need_kv_parallel_group`函数不再需要，因此建议将其删除。

https://github.com/vllm-project/vllm/issues/14745
这是一个需求提出类型的issue，主要涉及的对象是DeepSeek模型的工具调用功能。用户怀疑DeepSeek模型是否支持工具调用，并提出了相关疑问。

https://github.com/vllm-project/vllm/issues/14744
这是一个技术性的PR（Pull Request），主要涉及到针对CPU上MLA的支持，在优化MLA内核方面遇到了一些困难。

https://github.com/vllm-project/vllm/issues/14742
这是一个bug报告，涉及到v1 speculate decoding NgramProposer在长时间压力测试下出现异常的问题。

https://github.com/vllm-project/vllm/issues/14741
这个issue类型是功能需求，涉及的主要对象是CPU后端。由于需要在CPU后端中添加对FP8E5M2 KV缓存的支持并更新测试，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/14740
这是一个关于使用问题的issue，主要涉及DeepSeek R1在K8s环境下部署的方法，用户提出了想要在vllm中运行推理特定模型的需求，并表示不清楚如何与vllm集成。

https://github.com/vllm-project/vllm/issues/14739
这是一个用户提出需求的issue，主要涉及VLA series large models的支持问题。由于当前vllm文档不支持该系列模型，用户希望能够添加对VLA series模型的支持。

https://github.com/vllm-project/vllm/issues/14738
这是一个用户需求报告的issue，主要涉及的对象是支持加载InternVideo2.5模型作为原始InternVLChatModel。由于Hierarchical token compression (HiCo)技术尚未支持，导致输出结果与HF当前结果不同，用户可能在此寻求支持或解决方案。

https://github.com/vllm-project/vllm/issues/14737
这个issue属于BugFix类型，主要涉及到服务性能基准测试时启用性能分析的问题，原因是修复了启用性能分析时的端点断言。

https://github.com/vllm-project/vllm/issues/14736
这是一个用户提出需求的issue，主要涉及如何禁用HTTP请求日志，可能是因为用户想要在特定模型上运行推理但不清楚如何集成。

https://github.com/vllm-project/vllm/issues/14735
这个issue是一个Bug报告，主要涉及的对象是vllm.LLM.generate方法。这个问题由于total count没有正确考虑到每个prompt生成多个输出导致了进度条显示的不一致。

https://github.com/vllm-project/vllm/issues/14734
这是一个需求提出的issue，主要涉及对象是vLLM和Gemma3，由于无法导入`__version__`导致的错误。

https://github.com/vllm-project/vllm/issues/14733
这是一个需求改进类型的issue， 主要涉及VLLM V1的日志记录在初始化过程中常见错误的情况。导致此问题的原因是初始化过程中可能出现内存不足或KV缓存空间不足的情况。

https://github.com/vllm-project/vllm/issues/14732
这是一个建议性质的issue，主要涉及scheduler。由于内容描述只有"A superminor improvement on the scheduler."，无法确定具体是哪方面的改进或问题。

https://github.com/vllm-project/vllm/issues/14731
这个issue是一个feature开发相关的问题，涉及到实现SimpleScheduler。出现的问题是代码重复，需要进行进一步的重构。

https://github.com/vllm-project/vllm/issues/14730
这个issue是关于删除ultravox LoRA测试的建议，主要涉及的对象是CI/Build过程。原因是LoRA测试存在问题，需要下载2个模型且未能有效训练LoRA模型，导致CI测试压力增加。

https://github.com/vllm-project/vllm/issues/14729
这个issue属于需求提出类型，主要涉及V1版本性能优化中的Cascade Kernel，由于目前V1只支持单个树时的级联注意力，需要扩展到支持多个树的情况。

https://github.com/vllm-project/vllm/issues/14728
这是一个关于性能问题修复的issue，主要涉及对象为V1版本的VLLM模型。原因是OOM检查需要移至`sampler run`，因为OOM问题也可能发生在`profile_run`中，修复该问题会减少V1版本的内存使用。

https://github.com/vllm-project/vllm/issues/14727
这是一个用户提出需求的问题单，主要涉及的对象是vllm中的xgrammar实现，可能是由于缺乏对新功能支持计划而导致用户希望获取关于实现这一功能的建议或替代方案。

https://github.com/vllm-project/vllm/issues/14726
这是一个关于性能问题的issue，用户在尝试使用vllm运行deepseek-V3过程中遇到CUDA内存不足的错误。

https://github.com/vllm-project/vllm/issues/14725
这是一个功能提议的issue，主要涉及VLLM的矩阵计算优化，提议将`VLLM_MLA_PERFORM_MATRIX_ABSORPTION=0`设置为默认选项，以减少内存开销。

https://github.com/vllm-project/vllm/issues/14724
这是一个需求提案（RFC），主要涉及的对象是对 vLLM 项目中的 KV Cache 进行扩展，为了支持更高效的 KV Cache 卸载和跨引擎的 KV 复用。由于当前 KV transfer connector 框架在跨引擎 KV 复用方面存在缺失，需要增加功能以确保一致性视图。

https://github.com/vllm-project/vllm/issues/14723
这是一个bug报告类型的issue，涉及到gemm3ForConditionalGeneration功能的使用错误导致Unrecognized configuration class错误。

https://github.com/vllm-project/vllm/issues/14722
这个issue属于Bug报告，主要涉及vllm中使用reinforce++训练RL模型时，在不同推理模式（eager mode和cuda-graph mode）下进行贪婪解码评估结果不一致的问题。

https://github.com/vllm-project/vllm/issues/14721
这是一个用户提出需求的issue，主要对象是vLLM项目。由于OpenAI发布了新的Responses API，需要vLLM添加对该API的兼容，以保持与OpenAI更新的同步。

https://github.com/vllm-project/vllm/issues/14720
这个issue类型是bug报告，涉及的主要对象是VLLM模型。由于未实现方法 'get_kv_cache_spec'，导致出现NotImplementedError异常。

https://github.com/vllm-project/vllm/issues/14719
这个issue类型是请求反馈，并涉及V1 Spec Decode Eagle支持，用户寻求社区关于设计的反馈。

https://github.com/vllm-project/vllm/issues/14717
这是一个用户提出需求的issue，主要涉及vllm中AsyncLLMEngine是否支持batch inference需求。原因是用户需要实现同时支持批处理推断和流式返回的引擎，但当前发现vllm中的`AsyncLLMEngine`支持流式返回，但不支持批处理推断，因此想了解是否有其他引擎能够同时支持这两种需求。

https://github.com/vllm-project/vllm/issues/14716
这个issue类型是缺乏详细内容的bug报告，主要涉及的对象是lora resolver功能。这可能是由于用户没有提供任何描述或细节信息而导致的。

https://github.com/vllm-project/vllm/issues/14715
这是一个bug报告，涉及的主要对象是vLLM下的Qwen2.5-32B-Instruct-GPTQ-Int4模型。这个问题是由于在使用ROCm生成短提示时出现无限输出“!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!”，但使用长提示时生成正常，而在CUDA下该模型正常工作所导致的。

https://github.com/vllm-project/vllm/issues/14714
这是一个Bug报告，涉及到模块导入错误，导致了'ModuleNotFoundError: No module named 'vllm._C'的问题。

https://github.com/vllm-project/vllm/issues/14713
这是一个bug报告类型的issue，主要涉及到测试用例（tests）的修复，原因是存在一些测试用例执行失败（broken tests）导致的问题。

https://github.com/vllm-project/vllm/issues/14712
这是一个功能改进的issue，主要涉及的对象是神经元（neuron）的注意力机制内核（attention kernels），由于优化测试参数，在重新设计测试脚本的过程中简化了测试用例的数量，并加强了覆盖范围，以减少持续集成时间，同时保证了不同规模和精度模式下的正确性。

https://github.com/vllm-project/vllm/issues/14711
这个issue类型是"其他类型”，该问题单涉及的主要对象是AMD Entrypoints Test。由于原因未知，导致需要重新启用 AMD Entrypoints 测试。

https://github.com/vllm-project/vllm/issues/14710
这是一个bug报告，主要涉及硬件（TPU）相关功能。由于缓存编译图数量未按预期保持，在运行时会出现额外的图编译，需要添加检查以避免这种情况。

https://github.com/vllm-project/vllm/issues/14709
这是一个bug报告，涉及的主要对象是VLLM的PR 14245，由于新加入的__hip_fp8_e4m3在ROCm 6.2中不存在，导致了编译失败的问题。

https://github.com/vllm-project/vllm/issues/14708
这是一个包含"Differential Revision: D71078962"内容的类型为代码审查/合并请求的issue，涉及的主要对象是代码修改的审查和合并。

https://github.com/vllm-project/vllm/issues/14707
这是一个bug报告，主要涉及的对象是vllm库中的multi-step scheduler，可能是因为没有添加对TPU backend中`numschedulersteps`参数的无效性断言而导致问题。

https://github.com/vllm-project/vllm/issues/14705
这是一个bug报告，主要涉及到在Ray Executor使用时出现hang的问题。原因是vLLM默认使用fork创建engine进程，而在Ray actor中fork子进程不是最佳实践导致了未定义的行为。

https://github.com/vllm-project/vllm/issues/14704
这是一个Bug报告，主要涉及PyTorch 2.6+版本中的autofunctionalization V2功能禁用的问题，由于无法直接推送到CC分支而创建了这个PR。

https://github.com/vllm-project/vllm/issues/14703
这是一个关于优化功能升级的RFC（Request for Comments），涉及自动功能化V2在PyTorch 2.7+中的使用，主要问题是某些自定义融合通过依赖模式匹配，目前无法与auto_functionalized_v2兼容，以及另一个问题导致当前在PyTorch 2.6+中禁用V2。

https://github.com/vllm-project/vllm/issues/14702
这是一个功能优化的issue，主要涉及到了V1版本中的结构化输出。原因是为了改进推断解码时的结果验证和GPU运行效率，以确保结果符合语法约束。

https://github.com/vllm-project/vllm/issues/14701
这是一个bug报告，主要对象是vLLM在AVX512-capable CPU上的CPU推理没有使用AVX512，可能是由于环境设置或WSL运行导致的。

https://github.com/vllm-project/vllm/issues/14700
这是一个bug报告，涉及修复plugged调度器打印警告消息的问题，是由于配置v1调度器为字符串而导致的。

https://github.com/vllm-project/vllm/issues/14699
该issue类型为用户提出需求，涉及的主要对象是BartModel。由于缺少了SupportsQuant和packed_modules_mapping，用户提出了需要为所有模型添加这两个功能的要求。

https://github.com/vllm-project/vllm/issues/14698
这是一个用户提出需求的issue，主要对象是Bamba模型。由于缺少对SupportsQuant和packed_modules_mapping的支持，需要将其添加到所有模型中。

https://github.com/vllm-project/vllm/issues/14697
这是一个功能需求提出的issue，主要涉及的对象是TPU日志。

https://github.com/vllm-project/vllm/issues/14696
这是一个特性需求的issue，主要涉及的对象是vLLM和Gemma 3架构。由于缺乏对Gemma 3的支持，导致在加载google/gemma312bit时出现错误，用户希望vLLM能够支持Gemma 3。

https://github.com/vllm-project/vllm/issues/14695
这个issue是关于更新v0.8中废弃指标列表的，涉及的主要对象是已弃用指标，由于指标已在代码中标记为废弃，但没有在文档中列出，导致了这个问题。

https://github.com/vllm-project/vllm/issues/14694
这个issue类型是代码重构，主要涉及的对象是V1结构化输出的后端处理方式。

https://github.com/vllm-project/vllm/issues/14693
这是一个Bug报告，涉及VLLM中使用BGE-reranker-v2-me3模型处理rerank业务时出现的错误。由于模型不支持Rerank（Score）API，导致用户经常遇到错误消息。

https://github.com/vllm-project/vllm/issues/14692
这个issue是关于bug报告，主要涉及的对象是setup.py文件。由于对本地`main`分支的假设导致出现了不必要的逻辑，需要对代码进行修改。

https://github.com/vllm-project/vllm/issues/14691
该issue属于需求提出类型，涉及主要对象为添加ray[data]作为tpu依赖。由于在编译图中需要使用ray[data]，因此用户提出添加该依赖的需求。

https://github.com/vllm-project/vllm/issues/14690
这是一个用户提出需求的类型，该问题单涉及的主要对象是内存交错（Memory interleaving）。由于系统具有多个内存节点，通过控制内存策略来提高性能，但由于节点可能依赖于允许的CPU，在导入torch后才能确定交错节点，这导致了用户需求对内存交错进行控制。

https://github.com/vllm-project/vllm/issues/14689
这是一个与bug报告相关的issue，主要涉及fnuz平台，可能是由于CC(dynamic distpatch of fp8 kernels)导致的问题。

https://github.com/vllm-project/vllm/issues/14688
这是一个用户提出需求的issue，主要涉及的对象是InternVL2.5和InternVideo2.5模型。由于当前版本只支持特定的InternVideo2_5_Chat_8B模型，用户提出需要为InternVL2.5模型添加视频输入支持并处理交错图像视频输入的需求。

https://github.com/vllm-project/vllm/issues/14687
这是一个bug报告，主要涉及的对象是vllm的使用和Gemma3Config对象。造成这个bug的原因可能是由于Gemma3Config对象缺少属性'vocab_size'。

https://github.com/vllm-project/vllm/issues/14686
这个issue是关于性能优化的建议，涉及的主要对象是vLLM v1引擎，提出了减少重复预加载标记对性能影响的问题。

https://github.com/vllm-project/vllm/issues/14685
这是一个请求移除功能或做法类型的issue，主要涉及LoRA中的SGMV和BGMV kernels，需删除以便使用V1 kernels。

https://github.com/vllm-project/vllm/issues/14684
该issue类型是性能优化，主要涉及 Qwen2VL 的 LM `mrope_positions` 和 ViT 的 `rot_pos_ids` 的优化。

https://github.com/vllm-project/vllm/issues/14683
这个issue是关于功能需求的，主要涉及到数据并行推理的实现。由于单个GPU内存能力有限，需要实现在离线模式下利用多个GPU进行数据并行推理以提高效率。

https://github.com/vllm-project/vllm/issues/14682
这是一个bug报告，主要涉及Phi-4-mini函数调用的支持。该问题可能由于设置错误导致生成函数调用失败。

https://github.com/vllm-project/vllm/issues/14681
这是一个bug报告，涉及的主要对象是VLLM中的IPEX模块，由于CPU不支持MoE预打包，用户需要对权重进行预打包的决策受限，导致了需要添加`VLLM_CPU_MOE_PREPACK`选项的问题。

https://github.com/vllm-project/vllm/issues/14680
这是一个用户提出需求的issue，主要涉及改进内存交错以提高CPU和系统内存之间的内存带宽，其背景是在Linux系统上，多个内存节点和物理内存控制器可能导致内存带宽受限，因为操作系统通常会从最接近运行分配进程的CPU的节点中分配内存。

https://github.com/vllm-project/vllm/issues/14678
这是一个用户提出的需求，主要涉及加入一个Level 3睡眠模式，可以将模型权重存储到磁盘并且舍弃kv缓存。原因可能是为了在唤醒时最大限度地利用CPU内存并有效地从磁盘加载。

https://github.com/vllm-project/vllm/issues/14677
这是一个Bug报告类型的Issue，主要涉及的对象是vllm下的一个Unit Test文件`tests/models/embedding/vision_language/test_phi3v.py`。该Issue描述了在运行特定的Unit Test时出现了失败的情况，产生了测试不通过的结果。

https://github.com/vllm-project/vllm/issues/14676
这是一个bug报告，主要涉及Vllm的API调用出现Bad Request 400错误。造成错误的原因是请求缺少必要的字段。

https://github.com/vllm-project/vllm/issues/14675
这是一个Bug报告类型的issue，涉及的主要对象是Vllm与cortecs/phi4FP8Dynamic模型的集成。这个问题导致Vllm在运行多个连续请求时会随机重新启动。

https://github.com/vllm-project/vllm/issues/14674
这是一个用户提出需求的类型issue，主要涉及 VLLM 推理是否支持混合精度模型，用户想要了解如何与 VLLM 集成以运行特定模型推理。这个问题的根本原因在于用户需要更多关于模型精度配置和集成的信息。

https://github.com/vllm-project/vllm/issues/14673
这个issue属于bug报告类型，主要涉及到生成tokens的问题，由于使用了特定的参数组合会导致生成tokens的数量显示为0。

https://github.com/vllm-project/vllm/issues/14672
这是一个bug报告，主要涉及Gemma3 multi-modal processor，由于旧版本transformers在测试环境中的使用导致了pan-and-scan image processing功能无法通过CI启用。

https://github.com/vllm-project/vllm/issues/14671
这是一个bug报告，涉及的主要对象是ROCm编译环境，由于引入了新的CUDAonly kernels导致moewna16.cu编译错误，导致ROCm无法构建的问题。

https://github.com/vllm-project/vllm/issues/14670
这是一个bug报告，主要对象是MLA功能。由于某种原因导致MLA在使用时出现性能下降的问题。

https://github.com/vllm-project/vllm/issues/14669
这是一个bug报告，涉及的主要对象是vLLM在ROCm环境下编译出现错误，可能由于添加了CUDA only kernels导致编译错误。

https://github.com/vllm-project/vllm/issues/14668
这是一个特性需求的issue，主要涉及Ray的Compiled Graph功能对其他设备的支持。

https://github.com/vllm-project/vllm/issues/14667
这个issue是关于bug修复，主要对象是CPU rotary embedding kernel。由于`num_tokens`在CPU kernel中被错误计算，导致了outofbounds memory access，最终引发了segfault。

https://github.com/vllm-project/vllm/issues/14666
该issue属于bug报告类型，涉及的主要对象是chunked prefill for GGUF。由于输入到logits处理器的形状为(0, hidden_size)时可能出现问题，导致需要修复这个bug。

https://github.com/vllm-project/vllm/issues/14665
这个issue是关于用户需求，主要涉及如何设置特定文件以用于vllm运行推理，并表明用户不清楚如何与vllm集成，可能是由于缺乏相关集成教程或文档引起的。

https://github.com/vllm-project/vllm/issues/14664
这是一个特性新增的issue，主要涉及ROCm Flash Attention模块并添加了ENCODER_ONLY模型支持。由于在attention类型为encoderonly时出现了属性错误（AttributeError: Invalid attention type encoderonly），导致了该问题的提出。

https://github.com/vllm-project/vllm/issues/14663
这是一个用户提出需求的issue，主要对象是新模型Gemma 3，原因是想让vllm支持该模型。

https://github.com/vllm-project/vllm/issues/14662
这是一个bug报告，涉及vllm模型在处理包含两张图片的请求时输出第二张图片的bbox不正确的问题，原因可能是模型或者vllm设置的问题。

https://github.com/vllm-project/vllm/issues/14661
这是一个功能需求类型的issue，主要涉及 V1 AsyncLLM 的日志记录定制问题，因为现在的日志记录功能不支持自定义 logger 扩展 StatLoggerBase，用户希望能够实现与 V0 版本相同的日志记录定制支持。

https://github.com/vllm-project/vllm/issues/14660
这个issue是一个功能需求类型的问题，主要涉及的对象是VLLM模型的Gemna 3版本。其原因是由于目前的注意力机制后端无法高效支持Gemna 3模型对于图片token的双向注意力，因此必须临时使用PyTorch SDPA进行处理。

https://github.com/vllm-project/vllm/issues/14659
这是一个bug报告，涉及的主要对象是在AMD MI210上构建docker镜像时出现了`subprocess.CalledProcessError`错误。由于某些错误导致了这个问题的出现。

https://github.com/vllm-project/vllm/issues/14658
这是一个bug报告，该问题涉及vllm项目的Kernel模块，由于MLA chunked prefill需要支持非连续输入，要求quant linear方法能够支持非连续输入，导致了这个issue。

https://github.com/vllm-project/vllm/issues/14657
这是一个Bug报告，涉及的主要对象是模型生成混合采样和单独采样在输出上的一致性问题。导致这个问题的原因是在使用最小概率限制时出现了不一致的现象。

https://github.com/vllm-project/vllm/issues/14656
这是一个用户提出需求类型的issue，主要涉及hub.docker.com缺少arm架构的docker镜像文件。

https://github.com/vllm-project/vllm/issues/14655
这是一个Bug报告，主要涉及VLLM在运行过程中出现了内存不足的问题。

https://github.com/vllm-project/vllm/issues/14654
这是一个新功能需求的issue， 主要涉及visionarena offline support for benchmark_throughput功能的添加。

https://github.com/vllm-project/vllm/issues/14653
这是一个bug报告，主要涉及的对象是"Moe"模块，由于CC引入了FP8块量化调整支持导致了benchmark的问题。

https://github.com/vllm-project/vllm/issues/14652
这个issue属于bug报告类型，主要涉及vLLM 0.7.3在DeepSeek R1 67B模型多节点部署中出现的分段错误问题，导致由于输入token序列超过最大长度限制而引发的模型索引错误和内存访问违规。

https://github.com/vllm-project/vllm/issues/14651
这是一个用户提出需求的问题，主要涉及如何在内存中缓存Lora适配器，由于找不到实现代码而导致需求未满足。

https://github.com/vllm-project/vllm/issues/14650
这是一个bug报告，涉及到TPU Ray Actor在多节点情况下初始化时出现挂起的问题。导致该问题的可能原因是TPU actor在Ray中的通信方式存在问题，导致连接失败。

https://github.com/vllm-project/vllm/issues/14649
这是一个Bug报告类型的Issue，主要涉及EAGLE / MTP在多个MTP模块下无法覆盖近似隐藏状态/KV缓存，导致接受长度下降。

https://github.com/vllm-project/vllm/issues/14648
这是一个关于新增实验性支持神经元后端与V1架构的问题，涉及主要对象是代码库中的Neuron组件。

https://github.com/vllm-project/vllm/issues/14647
这是一个bug报告，涉及到EAGLE和DeepSeek MTP模型实现中的问题。由于设计上的考虑，导致在处理第一个token时出现了问题。

https://github.com/vllm-project/vllm/issues/14646
这是一个文档更新类的issue，主要涉及性能基准数据集的更新。

https://github.com/vllm-project/vllm/issues/14645
这是一个Bug修复类型的Issue，主要涉及对象是V1版本的Speculative decoding代码。这个问题是由于批处理中的请求顺序与推测解码代码中的顺序不一致导致的，导致输出结果出现错误。

https://github.com/vllm-project/vllm/issues/14644
这是一个问题描述issue，主要涉及的对象是vLLM实例，询问GPU内存在vLLM实例休眠和唤醒过程中可能存在的冲突。

https://github.com/vllm-project/vllm/issues/14643
这是一个bug报告，主要涉及的对象是V1版本的PP模块，由于之前修复的一个记忆问题引入了一个bug，导致只有在最后一个PP等级才会进行预热采样。

https://github.com/vllm-project/vllm/issues/14642
这是一个bug报告，涉及的主要对象为TPU release node上的log文件。由于log文件不断增长，导致TPU release作业被阻塞。

https://github.com/vllm-project/vllm/issues/14641
这是一个关于bug的报告，主要涉及的对象是run_tpu_test函数。由于测试不再存在，需要移除此测试。

https://github.com/vllm-project/vllm/issues/14637
这是一个功能需求，涉及到替换`current_platform.has_device_capability()`为特定功能API，主要是为了提高代码的可读性和易维护性。

https://github.com/vllm-project/vllm/issues/14636
这个issue是一个Bug报告，涉及的主要对象是vllm下的CPU KV Cache Offload，发生的症状是AssertionError导致的错误信息。

https://github.com/vllm-project/vllm/issues/14635
这是一个需求变更的issue，主要涉及支持量化（Quantization）功能中的模块映射问题，由于未正确处理`ignored_modules`导致脚本执行失败。

https://github.com/vllm-project/vllm/issues/14634
这是一个功能需求的issue，涉及VLLM的LoRA适配器动态加载至缓存目录，因为需要实现在没有外部基础设施的情况下，根据指定目录加载并使用特定模型的问题。

https://github.com/vllm-project/vllm/issues/14633
这是一个Bug报告，主要涉及NVLM-D处理器上缩略图丢失的问题。由于条件限制导致当num_patches为1时，代码未添加缩略图，从而引发了错误。

https://github.com/vllm-project/vllm/issues/14632
这是一个bug报告类型的issue，主要涉及vllm库在使用CUDA图形时释放GPU内存的问题。用户在尝试使用`torch.cuda.empty_cache()`方法释放内存时遇到OOM错误，可能由于使用了图形内核导致。

https://github.com/vllm-project/vllm/issues/14631
这是一个功能需求类型的issue，主要涉及VLLM项目中的模型添加关于量化支持的功能。由于某些模型具有子模型关系，导致`packed_modules_mapping`只能在初始化时确定，需要引入`SupportsQuant`以支持这一需求。

https://github.com/vllm-project/vllm/issues/14630
这是一个Bug报告，主要涉及Xgrammar中使用缓存的vocab_size问题。由于最近版本的Xgrammar检测到的vocab_size包括特殊token，导致了CC([Bug]: [V1] Molmo/Aria not supported on V1 due to xgrammar)问题的出现。

https://github.com/vllm-project/vllm/issues/14629
这是一个bug报告。该问题单涉及的主要对象是ROCm平台。由于moel_wna16_gemm kernel在ROCm平台上构建，导致出现问题。

https://github.com/vllm-project/vllm/issues/14628
这是一个Bug报告，主要涉及到vLLM在使用两块RTX 5090 GPUs进行tensor parallelism推断时遇到的问题。由于无法成功进行推断，用户尝试通过设置NCCL_P2P_DISABLE=1来解决问题，但仍然未能解决。

https://github.com/vllm-project/vllm/issues/14627
这是一个修正拼写错误的Issue，涉及主要对象是代码中的一个变量或参数。原因可能是手误导致拼写错误，需要修复以保证代码规范性。

https://github.com/vllm-project/vllm/issues/14626
这是一个功能需求的issue，针对V1 LoRA启用CUDAGraphs支持。

https://github.com/vllm-project/vllm/issues/14625
这个issue类型是功能需求提出，主要对象是VLLM项目中的Core模块，用户提出了支持MistralTokenizer用于结构化输出的需求。

https://github.com/vllm-project/vllm/issues/14624
这是一个关于bug报告的issue，涉及到V1版本的Detokenizer。原因是Detokenizer未能正确截断EOS或stoptoken导致的问题。

https://github.com/vllm-project/vllm/issues/14623
这是一个Bug报告，涉及V1引擎中的Detokenizer，问题出现在detokenizer没有正确截断输出文本中的EOS/stop-token导致控制标记污染模型输出。

https://github.com/vllm-project/vllm/issues/14622
该issue属于代码优化类型，涉及主要对象为StructuredOutputManager，由于xgrammar已经实现了缓存功能，因此不再需要在StructuredOutputManager中额外实现缓存，从而简化代码结构。

https://github.com/vllm-project/vllm/issues/14621
这是一个用户提出需求的issue，主要对象是关于在vllm中如何使用embeddings作为输入而不是token_ids。由于用户想要在推理过程中使用DeepSeekR1DistillQwen1.5B模型，但不知道如何与vllm集成，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/14620
这是一个文档错误报告，涉及的主要对象是项目代码库中的`requirements.txt`文件。原因可能是一个简单的拼写错误导致了贡献指南中的错误信息。

https://github.com/vllm-project/vllm/issues/14619
这是一个需求类型的issue，主要涉及的对象是进行CI测试的`v1/entrypoints`。

https://github.com/vllm-project/vllm/issues/14618
这是一个bug报告，主要涉及Vllm项目下的MambaMixer2模块中的量化问题。原因是当启用Tensor Parallel时，MambaMixer2不支持量化。

https://github.com/vllm-project/vllm/issues/14617
这是一个bug报告，主要涉及的对象是Mamba2模型的加载逻辑。由于量化层的参数类型不支持修改权重加载器，会导致加载模型错误，本问题修复了这个bug。

https://github.com/vllm-project/vllm/issues/14616
该issue属于bug报告，主要涉及的对象是xgrammar API。延迟了所有xgrammar使用直到需要，由于之前对TPU系统遇到的问题采取的措施不足以解决，需要进一步延迟xgrammar API的使用，以解决与TPU系统相关的问题。

https://github.com/vllm-project/vllm/issues/14614
这是一个关于部署DeepSeekV3使用vllm的最佳实践的问题，主要涉及到如何在多节点GPU集群上运行DeepSeekV3的推理，用户提出了关于性能优化和速度提升的问题。

https://github.com/vllm-project/vllm/issues/14613
这是一个功能改进类型的issue，主要涉及Kernel中的GGUF MoE kernel，通过提高速度并启用图缓存，可在8xH100上将DeepSeek GGUF的速度从10提升到50 tok/s，对Q4_K quants。

https://github.com/vllm-project/vllm/issues/14612
这是一则涉及软件功能增强的issue，主要对象是增加Intel GPU V1引擎支持。

https://github.com/vllm-project/vllm/issues/14611
这个issue类型是性能优化，主要涉及的对象是DeepSeek GGUF，由于添加了 GGUF MoE kernel，导致速度从10提升到50 tok/s。

https://github.com/vllm-project/vllm/issues/14610
这个issue类型是bug报告，主要涉及的对象是Ray framework和vLLM's OpenAI server。由于vLLM的设计没有充分考虑LLMEngine在placement group中运行的情况，导致了"No CUDA GPUs are available"或"Ray does not allocate any GPUs on the driver node"问题的出现。

https://github.com/vllm-project/vllm/issues/14609
这是一个bug报告，涉及的主要对象是`Alibaba-NLP/gte-Qwen2`模型。由于`is_causal`配置在HF仓库中被设置为`False`，导致需要更新文档和测试。

https://github.com/vllm-project/vllm/issues/14608
这个issue类型是用户提出需求，涉及主要对象是vllm软件安装。由于nvidia驱动和cuda版本不兼容，导致无法安装vllm==0.7.3。

https://github.com/vllm-project/vllm/issues/14607
这是一个用户提出需求的issue，主要涉及对象是获取VLLM模型的原始隐藏状态。这个问题的根本原因是用户想要通过VLLM获取与transformers输出相同形状和数值的隐藏状态，但目前VLLM提供的方法无法达到这一目标。

https://github.com/vllm-project/vllm/issues/14606
该issue类型为用户提出需求类型，主要涉及到对模型stepfunai/GOTOCR2.0hf的支持。由于没有收到响应，用户提出了对所需模型支持的困难，但并未详细说明导致问题的原因。

https://github.com/vllm-project/vllm/issues/14605
这是一个bug报告，涉及的主要对象是vLLM Docker image for ARM64。由于PyTorch CUDA版本不匹配以及缺少triton模块，导致了构建Docker镜像和执行容器时的问题。

https://github.com/vllm-project/vllm/issues/14604
这个issue类型为需求探讨，主要涉及的对象是vllm中构建物理KV缓存块时选择的维度布局。由于布局顺序的调整，用户提出是否能改善内存访问，是否无差异或性能会下降。

https://github.com/vllm-project/vllm/issues/14603
这个issue类型是用户提出需求，询问关于vllm CPU backend是否支持Intel AMX的问题。

https://github.com/vllm-project/vllm/issues/14602
这个issue属于bug报告，主要涉及的对象是VLM（可能是软件或项目名称），问题可能源于siglip遗留代码清理和修复损坏的paligemma多模处理器功能。

https://github.com/vllm-project/vllm/issues/14601
这是一个Bug报告类型的Issue，涉及主要对象是torch库下的cutlass_scaled_mm函数，由于某种原因导致出现了RuntimeError。

https://github.com/vllm-project/vllm/issues/14600
这是一个用户提出需求的类型为使用问题，主要涉及多模态数据集的离线吞吐量/延迟基准测试。由于目前只能在在线模式下使用 benchmark_serving 实现多模态数据集支持，用户想知道如何在离线模式下获取多模态输入的吞吐量和延迟结果。

https://github.com/vllm-project/vllm/issues/14599
这是一个bug报告类型的issue，主要涉及模型加载时量化部署的问题。由于未知情况下导致量化方式可能不正确，用户寻求对模型量化部署方式的确认或帮助。

https://github.com/vllm-project/vllm/issues/14598
这个issue是bug报告，主要涉及的对象是数据并行性代码示例。造成这个bug的原因是`CUDA_VISIBLE_DEVICES`在子进程读取前被设置在目标函数中，导致所有进程使用相同的`CUDA_VISIBLE_DEVICES`。

https://github.com/vllm-project/vllm/issues/14597
这是一个bug报告类型的 issue，主要涉及的对象是 vLLM 中的 ragged paged attention 模块。这个问题是由于 block_table.shape[1]%NUM_KV_PAGES_PER_BLOCK!=0 导致的，可能引起越界问题并导致 kernel 崩溃。

https://github.com/vllm-project/vllm/issues/14596
这是一个 Bug 报告，涉及的主要对象是 vLLM 应用程序。由于无效的头部反序列化导致出现错误信息 "Error while deserializing header: InvalidHeaderDeserialization"。

https://github.com/vllm-project/vllm/issues/14595
这个issue属于bug报告类型，主要涉及的对象是benchmark_throughput.py脚本的计算稳定性，并由于未指定特定GPU导致不同运行结果之间的TPS值波动较大。

https://github.com/vllm-project/vllm/issues/14594
这个issue属于用户提出需求类型，主要对象是Varun/v1 lora kernels tuner。原因是用户希望运行自动调谐程序。

https://github.com/vllm-project/vllm/issues/14593
这是一个Bug报告，涉及主要对象是vllm中的模型加载到VRAM。由于使用多个异步引擎在多GPU上提升时出现无限等待，GPU内存未加载，可能由于程序无法正确加载模型到VRAM导致此问题。

https://github.com/vllm-project/vllm/issues/14592
这是一个bug报告，涉及的主要对象是VLLM中的 use_beam_search 参数设置。这个问题可能是由于设置 use_beam_search 为 false 导致了某些功能的错误链接以及其他相关问题。

https://github.com/vllm-project/vllm/issues/14590
这是一个需求提出类型的 issue，主要涉及 xgrammar 的升级和添加正则表达式结构化输出支持。

https://github.com/vllm-project/vllm/issues/14589
这是一个bug报告，涉及的主要对象是Guidance backend for structured output。由于json schemas编译时间过长，导致structured output requests可能会让服务器hang。

https://github.com/vllm-project/vllm/issues/14588
这个issue属于bug报告类型，主要涉及到使用V1 engine时n samples只返回一个response的问题。原因可能是在最新的vllm镜像中出现了这个bug。

https://github.com/vllm-project/vllm/issues/14587
这是一个Bug报告，主要涉及V1版本的Beam Search Test失败。这个问题可能是由于某种环境问题引起的。

https://github.com/vllm-project/vllm/issues/14586
这是一个bug报告，涉及的主要对象是依赖项的安装与卸载。由于未卸载`torch`，`torch_xla`和`torch_vision`依赖项就直接安装`requirements/tpu.txt`导致的版本冲突。

https://github.com/vllm-project/vllm/issues/14585
这是一个bug报告，主要涉及到V1引擎核在没有请求需要调度时仍然阻塞的问题，导致TTFT不稳定和可能更长。

https://github.com/vllm-project/vllm/issues/14584
这是一个用户提出需求的 issue，主要是关于给一个测试命名。原因可能是为了更清晰地区分不同的测试用例。

https://github.com/vllm-project/vllm/issues/14583
这是一个Bug报告，主要涉及VLLM项目中的`engine.py`文件，由于`AttributeError: Invalid attention type encoder-only`错误导致。

https://github.com/vllm-project/vllm/issues/14582
该issue为需求提议，主要涉及硬件优化和TPU加速，旨在减少编译时间。

https://github.com/vllm-project/vllm/issues/14581
该issue类型为需求提出，主要对象涉及token_num的padding逻辑。由于当前策略会导致计算资源浪费，需要改进。

https://github.com/vllm-project/vllm/issues/14580
这是一个功能需求类型的Issue，主要涉及vLLM在TPU上添加重新编译检查的功能。由于PyTorch/XLA的隐式编译导致LLM提供服务期间出现过多的重新编译，影响性能。

https://github.com/vllm-project/vllm/issues/14579
这是一个bug报告issue，主要对象是vllm项目下的Structured Output模块，用户提出了某些环境下测试用例失败的问题。

https://github.com/vllm-project/vllm/issues/14578
这个issue类型为功能需求，涉及支持FP8 GEMM层中输入是FP8数据类型的情况，以便实现与融合FP8转换的关注准备，可能由于当前版本未实现此功能而导致用户需要此功能。

https://github.com/vllm-project/vllm/issues/14577
这是一个bug报告，涉及的主要对象是V1版本的模型。这个问题可能是由于模型在采样时没有缓存logits buffer导致的。

https://github.com/vllm-project/vllm/issues/14576
这个issue类型是用户提出需求，主要对象是SmolLM2系列中的 135M 和 360M 参数变体。由于用户需要更轻量级的解决方案，并且希望利用 vLLM 的高性能推理引擎，因此请求在 vLLM 中支持这些较小的模型。

https://github.com/vllm-project/vllm/issues/14575
这是一个bug报告，主要涉及到vllm下的xgrammar模块。由于在TPU环境中无法正确工作，需要延迟初始化grammar bitmask以避免导入xgrammar。

https://github.com/vllm-project/vllm/issues/14574
这是一个bug报告，涉及到VLLM项目中的CI测试失败，原因是内存不足导致测试失败。

https://github.com/vllm-project/vllm/issues/14573
这是一个Bug报告，主要涉及对象是V1引擎在TPU上的使用，出现问题是由于`StructuredOutputManager`在TPU上依赖于`triton`，导致服务启动失败。

https://github.com/vllm-project/vllm/issues/14572
这是一个bug报告，该问题单涉及主要对象是sparse kernels的构建。由于pull request #14354 导致在hopper上sparse kernels未能构建，引发了问题。

https://github.com/vllm-project/vllm/issues/14571
这个issue类型是bug报告，主要涉及对象是`tqdm bar for parallel sampling`更新逻辑问题，导致总处理输入token数量计算错误。

https://github.com/vllm-project/vllm/issues/14570
这个issue是关于功能增强（feature enhancement），主要涉及FlashAttention3中的FP8 KV缓存支持，提供了性能更好的FlashAttention。导致提出该需求的主要原因是FlashAttention对所有的Q、K和V都进行FP8计算，与FlashInfer不同，希望提升性能。

https://github.com/vllm-project/vllm/issues/14569
这个issue类型为用户提出需求，主要对象是一个新的模型模块"phi-4-multimodal-instruct"，用户寻求支持该新模型所遇到的困难。

https://github.com/vllm-project/vllm/issues/14568
该issue是关于优化moe优化中的permute/unpermute kernel实现的。

https://github.com/vllm-project/vllm/issues/14567
这个issue是一个bug报告，主要涉及jsonschema的改变导致benchmark运行失败。

https://github.com/vllm-project/vllm/issues/14566
这是一个bug报告，涉及到程序中错误的选项设置，导致了不正确的选择项出现在后端中。

https://github.com/vllm-project/vllm/issues/14565
这个issue属于文档更新类型，涉及PaliGemma笔记，主要目的是将笔记更新为警告内容。

https://github.com/vllm-project/vllm/issues/14564
这个issue类型是功能升级，涉及的主要对象是vllm中的Intel GPU，并由于IPEXxpu版本升级引发。

https://github.com/vllm-project/vllm/issues/14563
这个issue类型是功能更新，并涉及到XGrammar版本升级到0.1.15。根据描述，用户寻求升级支持aarch64架构以及解决之前版本不兼容的问题。

https://github.com/vllm-project/vllm/issues/14562
这是一个bug报告，主要涉及的对象是VLLM这个项目的命名方式。原因可能是用户希望更正项目名称的大写形式。

https://github.com/vllm-project/vllm/issues/14561
该issue是关于文档的bug报告，涉及将"Github"修正为"GitHub"的拼写错误。原因可能是作者在书写文档时使用了错误的拼写，并需要进行修正。

https://github.com/vllm-project/vllm/issues/14560
这是一个Bug报告，用户遇到了CPU Out of Memory（OOM）问题在训练Qwen 1.5B模型时。

https://github.com/vllm-project/vllm/issues/14559
这是一个bug报告，主要涉及使用asyncio方法进行并发请求时批处理不成功的问题，导致每个请求都被单独发送至vllm导致症状。

https://github.com/vllm-project/vllm/issues/14558
这是一个bug报告类型的issue，主要涉及deepseek-vl2的chat template的问题，由于在生成提示时使用了两个lb作为分隔符，并在生成提示的结尾多余了一个lb，导致了DeepSeekVL2的低质量表现。

https://github.com/vllm-project/vllm/issues/14557
这是一个bug报告，涉及对象为模型请求的tokens长度，由于请求的tokens长度超出了模型最大长度限制导致报错。

https://github.com/vllm-project/vllm/issues/14556
这是一个关于文档修改的issue，主要涉及到安装URL的优化。这个问题可能由于URL链接不友好导致的阅读体验不佳而被提出。

https://github.com/vllm-project/vllm/issues/14555
这是一个Bug报告，涉及的主要对象是DeepSeek R1 Unsloth GGUF quantized model。由于`W_Q_UK`在模型加载后没有被正确初始化，导致出现错误消息并无法成功部署DeepSeek GGUF模型。

https://github.com/vllm-project/vllm/issues/14554
这是一个Bugfix类型的issue，主要涉及的对象是V1版本的LLava系统。由于某个Pull Request引入了新的参数未在LlavaMultiModalProcessor中定义，导致了此bug。

https://github.com/vllm-project/vllm/issues/14553
这是一个功能需求建议，主要涉及最大输出长度限制，由于大量 max_tokens 可能导致延迟或请求饥饿。

https://github.com/vllm-project/vllm/issues/14552
这个issue属于用户需求类，主要涉及的对象是Transformer模型的文档。用户提出这个问题是希望文档中提及`model_impl`参数，以帮助用户了解如何手动使用该参数。

https://github.com/vllm-project/vllm/issues/14551
这是一个bug报告，涉及Llama3.2无法处理具有不同num_images的请求。造成这一问题的原因可能是代码中未处理请求差异导致的。

https://github.com/vllm-project/vllm/issues/14550
这是一个Bug报告，主要涉及到在reasoning内容中应用了 stop_sequences，导致了逻辑错误和需要修复。

https://github.com/vllm-project/vllm/issues/14549
这是一个需求提出类型的issue，主要涉及将dockerfiles移动到它们自己的目录中。

https://github.com/vllm-project/vllm/issues/14548
这是一个bug报告，主要涉及的对象是Qwen2-VL和Qwen2.5-VL模型，问题是有关`second_per_grid_ts`处理的bug导致Qwen2-VL针对视频输入出现问题。

https://github.com/vllm-project/vllm/issues/14547
这个issue属于Bug报告类型，涉及的主要对象是VLLM项目中的VLLMOpenAI服务。这个问题是由于使用的设备CUDA Capability版本过低导致无法被Triton GPU编译器支持，进而引发了RuntimeError错误。

https://github.com/vllm-project/vllm/issues/14545
这个issue类型是功能改进，主要涉及对象是 `QKVCrossParallelLinear`。由于需要支持BNB 4位量化，导致之前的CC被撤销，并进行相关功能的修改。

https://github.com/vllm-project/vllm/issues/14544
这是一个用户提出需求的issue，主要涉及的对象是vllm中的深度模型。

https://github.com/vllm-project/vllm/issues/14543
这是一个需求类型的issue，主要对象是编译测试的CI环境。

https://github.com/vllm-project/vllm/issues/14542
这是一个bug报告，涉及的主要对象是模板文件"template_deepseek_vl2.jinja"，由于最后一行的代码中缺少了一个必要的结束符"{% endif %}"导致了问题。

https://github.com/vllm-project/vllm/issues/14540
这个issue是性能改进类型的，主要涉及到V1版本中MLA的性能优化。原因是之前的`rotary_emb`专用性问题导致了一些CPU开销，另外`build`函数中的操作顺序问题也导致了一些额外开销。

https://github.com/vllm-project/vllm/issues/14539
这是一个bug报告，涉及的主要对象是GLM4V模型。由于chat template未指定，导致huggingface.co上的glm4v9b在transformers v4.44+下无法运行。

https://github.com/vllm-project/vllm/issues/14538
该issue为Bug报告，主要涉及vLLM-GPU在使用2个H100 GPU的情况下无法充分利用48个CPU核心，导致CPU密集型任务无法正常运行。

https://github.com/vllm-project/vllm/issues/14537
这是一个Bug报告类型的Issue，主要涉及加载模型到8个GPU时出现NCCL错误。其出现的原因可能是硬件驱动不正确导致。

https://github.com/vllm-project/vllm/issues/14536
这个issue是用户询问关于vllm是否支持inflight batch功能的问题，主要涉及vllm的功能使用。用户提出问题是由于在vllm文档中找不到相关信息。

https://github.com/vllm-project/vllm/issues/14535
这是一个bug报告，涉及vLLM初始化时出现CUDA错误的问题，可能是由于在forked subprocess中重新初始化CUDA导致。

https://github.com/vllm-project/vllm/issues/14534
这是一个bug报告，涉及的主要对象是Vllm中的Molmo/Aria模型，原因是由于tokenizer vocab和model vocab的相对大小导致了无法在V1上使用这些模型。

https://github.com/vllm-project/vllm/issues/14533
这是一个用户提出需求的issue，主要涉及vllm启动bge时如何支持不同返回类型的问题。可能由于使用指定模型的推理过程中不清楚如何与vllm集成，导致用户需要寻求相关帮助。

https://github.com/vllm-project/vllm/issues/14532
这是一个bug报告，涉及的主要对象是在V1版本中load formats无法正常工作，可能出现问题需要修复。

https://github.com/vllm-project/vllm/issues/14531
这是一个bug报告，涉及测试初始化在使用V1时出现问题。原因可能是新版本与Grok 1不兼容导致失败。

https://github.com/vllm-project/vllm/issues/14530
这是一个功能需求类型的issue，主要涉及到VLLM库中的例子代码。由于内部结构变化，需要更新例子代码以支持最新版本的VLLM。

https://github.com/vllm-project/vllm/issues/14529
这是一个bug报告，主要涉及到GLM4V的问题，可能是由于bug导致的GLM4V在V0和V1版本上出现故障。

https://github.com/vllm-project/vllm/issues/14528
这是一个Bug报告，主要涉及的对象是vllm中的qwen2-vl模型，用户提出该模型在处理视频输入时出现问题。

https://github.com/vllm-project/vllm/issues/14527
这是一个bug报告，涉及到程序中定义的requirements路径，因移动requirements文件时出现疏忽导致路径错误。

https://github.com/vllm-project/vllm/issues/14526
这是一个功能请求 issue，主要涉及的对象是 bgem3 embedding model。这个issue描述了如何通过传入"additional_data": {"sparse_embeddings": true}来请求稀疏嵌入向量。原因是目前只能返回单个张量类型的结果，导致了这个问题。

https://github.com/vllm-project/vllm/issues/14525
这是一个需求提出类型的issue，主要涉及到Vllm项目中的Prompt Validation功能更新问题。这个问题是由于可能存在用户输入异常数据导致的需求调整。

https://github.com/vllm-project/vllm/issues/14524
这是一个bug报告，涉及的主要对象是VllM中对于不支持的head_dim的处理。这个问题是由于模型具有不支持的head_dim而引起的，目前的处理方式是引发ValueError异常。

https://github.com/vllm-project/vllm/issues/14523
这个issue是一个Bug报告，涉及主要对象是`llavahf/llava1.57bhf`环境，由于当前环境无法在V1上运行而导致了该bug。

https://github.com/vllm-project/vllm/issues/14522
这是一个Bug报告类型的issue，主要涉及对象是V1版本无法支持`MistralTokenizer`，导致无法运行特定模型推理。

https://github.com/vllm-project/vllm/issues/14521
这是一个bug报告，涉及到VLLM软件中的Ray placement group allocation。原因是当尝试使用分数GPU时，环境变量VLLM_RAY_PER_WORKER_GPUS没有被正确处理，导致资源分配出现问题。

https://github.com/vllm-project/vllm/issues/14520
该issue类型为bug报告，主要对象是torch_xla模块，因为seed参数不能处理None值而导致的问题。

https://github.com/vllm-project/vllm/issues/14519
这是一个Bug报告，涉及 Greedy Rejection Sampler 中的 invalid_token 最后一个插槽问题，原因是 CUDA 错误导致了非法内存访问。

https://github.com/vllm-project/vllm/issues/14518
这是一个Bug报告类型的Issue，主要涉及的对象是使用LLM类时无法使用本地模型。用户遇到的问题是试图在Kaggle环境中使用LLM和本地训练模型时出现错误。

https://github.com/vllm-project/vllm/issues/14517
这是一个bug报告类型的issue，主要涉及无法在多个GPU上运行的问题，可能是由于程序在多GPU环境下出现错误导致。

https://github.com/vllm-project/vllm/issues/14516
这是一个技术性的issue，类型为功能改进建议，涉及主要对象为测试套件。这个问题是由于测试套件中使用了过时的`os.environ`函数，导致需要将其替换为`monkeypatch.setenv`来解决。

https://github.com/vllm-project/vllm/issues/14515
这个issue属于改进建议类型，主要对象是测试套件中的os.environ函数调用，由于需要将所有os.environ(xxx)替换为monkeypatch.setenv导致需要进行改进。

https://github.com/vllm-project/vllm/issues/14514
这是一个bug报告，主要涉及的对象是"设置lora适配器"。由于在所有虚拟运行中未设置lora适配器，导致问题的产生。

https://github.com/vllm-project/vllm/issues/14513
这是一个用户提出需求的类型的issue，主要涉及vLLM的并发处理能力问题。由于当前环境下vLLM版本的限制，导致用户在200个并发用户时只能处理最多100个请求。

https://github.com/vllm-project/vllm/issues/14512
这是一个bug报告，该问题单涉及并行采样结束/中止的修复，主要涉及处理请求无法同时完成、合并不同子请求的输出以及中止多于一个请求等问题。

https://github.com/vllm-project/vllm/issues/14511
这是一个用户提出需求（feature request）的issue，主要涉及前端工具调用和推理解析器，并解决了工具调用只从内容而不是推理内容中解析的问题。

https://github.com/vllm-project/vllm/issues/14510
这是一个bug报告，该问题主要涉及硬件（TPU），由于某种原因导致logits处理器在热身之后出现了重新编译的问题。

https://github.com/vllm-project/vllm/issues/14509
这是一个功能需求类型的issue，主要涉及对象是SpeculativeConfig，原因是为了使得推理模型选择更加清晰和易于推断所需的推理模型。

https://github.com/vllm-project/vllm/issues/14508
这个issue是关于bug报告，主要涉及Logits和采样，由于内存问题导致了bug。

https://github.com/vllm-project/vllm/issues/14507
这是一个用户提出需求的issue，主要涉及使用 microsoft/Phi-4-multimodal-instruct audio，用户希望在vllm中运行具体模型的推断，但不清楚如何集成音频部分。

https://github.com/vllm-project/vllm/issues/14506
这是一个bug报告，主要涉及VLLM API服务器启动时出现的错误。这个问题似乎是由于使用phi4multimodalinstruct时带有VLLM_USE_V1环境变量而导致的。

https://github.com/vllm-project/vllm/issues/14505
这是一个功能需求的issue，主要涉及LMCache连接器的支持chunked prefill功能。由于LMCache连接器目前不支持chunked prefill，用户提出了这一需求。

https://github.com/vllm-project/vllm/issues/14504
这是一个撤销之前修复内存问题的提交的issue，涉及到vllm项目下的LoRA功能，由于修复 memory issue 导致了 LoRA 功能出现问题。

https://github.com/vllm-project/vllm/issues/14503
这是一个bug报告，涉及的主要对象是LoRA，由于deviceprofiler没有包括LoRA内存，导致在V1版本上LoRA功能受到影响。

https://github.com/vllm-project/vllm/issues/14502
这是一个BUG报告，涉及主要对象为LoRA模块，由于恢复了一个修改导致LoRA全部失效。

https://github.com/vllm-project/vllm/issues/14501
这是一个bug报告，主要涉及前端工具（tool）的chat templates，由于一个额外的`r`字符导致了问题。

https://github.com/vllm-project/vllm/issues/14500
这是一个类型为Bug报告的Issue，主要涉及使用vllm在WSL2上运行"microsoft/Phil4miniinstruct"模型时出现的数值错误。

https://github.com/vllm-project/vllm/issues/14499
这个issue是一个功能需求，涉及主要对象为测试套件中的`os.environ(xxx)`转换为`monkeypatch.setenv`，由于需要将环境变量的设置方式统一，可能是为了增强测试套件的可重复性和稳定性。

https://github.com/vllm-project/vllm/issues/14498
这是一个Bug报告，主要涉及的对象是代码中的`QKVCrossParallelLinear`类。原因是该类不支持需要额外权重和权重属性注册的量化，因此导致了问题。

https://github.com/vllm-project/vllm/issues/14497
这是一个Bug报告类型的issue，涉及的主要对象是LRUCacheWorkerLoRAManager中的remove_oldest_adapter函数。由于remove_oldest_adapter可能导致删除仍在使用的LoRA，可能会引起被删除的LoRA仍在使用的问题。

https://github.com/vllm-project/vllm/issues/14496
这是一个bug报告，涉及主要对象是CODEOWNERS文件。由于原先的CODEOWNERS文件中的测试没有按照字母顺序排序，导致需要对其进行更新以确保测试的结构输出正确。

https://github.com/vllm-project/vllm/issues/14495
该issue是一个功能需求，主要涉及LoRa槽位数量不足导致的错误。

https://github.com/vllm-project/vllm/issues/14493
这是一个bug报告，该问题单涉及的主要对象是`ruff`工具。由于现在在运行`ruff`命令时未能忽略第三方库，导致`precommit`和直接在命令行中运行`ruff`时的行为不一致。

https://github.com/vllm-project/vllm/issues/14492
这个issue类型是代码维护，主要涉及到Python 3.9的typing模块升级。由于Python 3.8中的typing已经废弃，需要更新以支持额外的目录。

https://github.com/vllm-project/vllm/issues/14491
这是一个Bug报告，主要涉及vllmopenai中server在高负载情况下产生错误输出的问题。

https://github.com/vllm-project/vllm/issues/14490
该issue类型为用户提出需求，主要涉及的对象是vLLM中的 reasoning model。由于目前vLLM无法同时支持工具调用和推理，用户提出希望在推理步骤之后支持工具调用，以提高性能。

https://github.com/vllm-project/vllm/issues/14488
这是一个关于运行新配置与测试相关的issue，类型为其他类型，主要涉及测试配置和程序运行。

https://github.com/vllm-project/vllm/issues/14487
这是一个bug报告，涉及到vllm项目中的模块导入错误。由于缺少名为'pyarrow'的模块而导致的ModuleNotFoundError。

https://github.com/vllm-project/vllm/issues/14486
这是一个性能优化提案，涉及Eagle实现效率不高的问题。提议改用BatchPrefillAttention而不是BatchDecodeAttention来提高性能。

https://github.com/vllm-project/vllm/issues/14485
这是一个功能优化类型的issue，涉及到代码库中逐步引入ruffformat并逐渐废弃yapf和isort的过程，主要涉及到使用了特定目录选项的相关代码。

https://github.com/vllm-project/vllm/issues/14484
这是一个用户提出需求的issue，主要对象是新增支持多语言模型。

https://github.com/vllm-project/vllm/issues/14483
这是一个bug报告，主要涉及的对象是使用vllm时训练过程无法收敛的问题。原因可能是vllm的使用导致训练过程无法达到收敛。

https://github.com/vllm-project/vllm/issues/14482
这个issue类型为功能需求，主要对象是RLHF文档，由于缺乏相关文档，用户提出需求添加RLHF文档以便后续完善。

https://github.com/vllm-project/vllm/issues/14481
这个issue类型是一个功能需求，主要对象是benchmark性能测试，用户提出了对性能分析的需求。

https://github.com/vllm-project/vllm/issues/14480
这个issue类型是bug报告，涉及的主要对象是CI/Build中的flaky tests。导致这个bug的原因是seed值的变化导致了测试的不稳定性。

https://github.com/vllm-project/vllm/issues/14479
这是一个文档更新类型的issue，主要涉及到支持的模型列表的修改。这个问题是由于新增模型QwQ-32B导致需要更新文档，以确保在推理输出文档中正确列出已支持的模型。

https://github.com/vllm-project/vllm/issues/14478
该issue类型为文档更新，主要对象是关于Qwen模型工具调用的文档。

https://github.com/vllm-project/vllm/issues/14477
这是一个bug报告，主要涉及LLaMa 3.2 11b模型的问题，导致了准确度下降了11%。

https://github.com/vllm-project/vllm/issues/14476
这是一个bug报告，涉及的主要对象是DeepSeek模型，产生问题的原因可能是`q_pe`和`k_pe`在MLA中的形状不同所导致的`forward_native`与`forward_cuda`表现不一致。

https://github.com/vllm-project/vllm/issues/14475
这个issue属于bug报告，涉及的主要对象是模型架构['Qwen2ForCausalLM']的检查失败。该问题可能由于PyTorch版本与环境配置不匹配导致模型架构无法被正确检查而产生。

https://github.com/vllm-project/vllm/issues/14474
这个issue属于Bug报告，涉及的主要对象是Pythonic tool names的灵活性。由于Pythonic tool parser只接受字母名称，用户提出了这个问题，寻求支持以接受underscore前缀和kebabcase命名。

https://github.com/vllm-project/vllm/issues/14473
这是一个bug报告，涉及的主要对象是GuidedDecodingParams类。由于装饰器在backend_name属性上的应用导致数值错误和条件检查失效，可能由观察效应或竞争条件引起。

https://github.com/vllm-project/vllm/issues/14472
这是一个功能需求，该问题单主要涉及的对象是当前使用对象不包括推理令牌，导致用户希望在响应中包含推理令牌。

https://github.com/vllm-project/vllm/issues/14471
这是一个bug报告，问题涉及MLA（Machine Learning Accelerator）CPU overheads。造成此问题的原因可能是对性能优化导致的不良影响。

https://github.com/vllm-project/vllm/issues/14470
这是一个Bug报告类型的issue，主要涉及PythonicToolParser在处理tool names时的限制，导致无法支持snake_case、kebabcase、以及数字命名的工具。

https://github.com/vllm-project/vllm/issues/14469
这是一个bug报告，该问题涉及设备性能分析器(deviceprofiler)未考虑LoRA内存，在确定KV缓存块数量时导致的问题。

https://github.com/vllm-project/vllm/issues/14467
这是一个bug报告，涉及的主要对象是VLLM在TPU上的运行。问题可能是由于不必要的填充导致的。

https://github.com/vllm-project/vllm/issues/14466
这是一个功能需求提出的issue，主要涉及的对象是V1引擎的可插拔调度器，由于当前无法在运行时覆盖调度器类，开发者希望能够让调度器支持插拔功能以便添加额外的约束。

https://github.com/vllm-project/vllm/issues/14465
这是一个Bug报告，问题涉及Mistral-Small-24B-Instruct-2501模型的启动失败，并出现了无法使用Mistral tokenizer的错误。可能是由于V1版本启用了guided decoding导致的问题。

https://github.com/vllm-project/vllm/issues/14464
这是一个bug报告类型的issue，主要涉及vLLM下的EAGLE实现，问题出现的原因是dummy output norm的计算错误导致了低性能表现。

https://github.com/vllm-project/vllm/issues/14463
这是一个 Bug 报告，涉及到 NVLM 的代码在处理 num_patches 等于 1 的情况下导致出错的问题。

https://github.com/vllm-project/vllm/issues/14462
这是一个修复bug的issue，主要涉及kv_cache_interface中处理MLA时出现的问题。

https://github.com/vllm-project/vllm/issues/14461
这个issue是用户提出需求，希望最小化格式更改，以减少对文档的干扰。

https://github.com/vllm-project/vllm/issues/14460
这是一个关于修复格式问题的bug报告，涉及到格式错误。由于格式错误导致了界面显示异常。

https://github.com/vllm-project/vllm/issues/14459
这个issue是关于bug报告，主要涉及torch_xla库无法处理None种子的问题。这个bug可能是由于种子默认值未正确设置导致的。

https://github.com/vllm-project/vllm/issues/14458
这是一个用户报告的bug类型的issue，涉及的主要对象是VLLM模型在Sagemaker端点上使用quantization出现错误。由于模型MllamaForConditionalGeneration尚不支持BitsAndBytes量化，导致出现错误信息。

https://github.com/vllm-project/vllm/issues/14457
该issue类型属于功能需求，涉及的主要对象是benchmark相关的数据集选项。这个需求是为了测试在不利情况下的性能，即每个请求使用唯一的jsonschema，避免使用基于模式缓存的优势。

https://github.com/vllm-project/vllm/issues/14456
该issue类型为bug报告，主要涉及vLLM在Ray上无法找到CUDA GPU的问题，可能由于AWQ模型版本的特定问题导致。

https://github.com/vllm-project/vllm/issues/14455
这是一个关于技术支持的issue，主要涉及Intel Gaudi上Deepseek R1模型的启用，并包含关于在不同配置下运行fp8的选项。原因可能是为了在特定环境中提高模型精度或性能。

https://github.com/vllm-project/vllm/issues/14454
这是一个优化问题，涉及到MoE权重填充，主要针对半精度类型进行优化，为提高性能而进行。

https://github.com/vllm-project/vllm/issues/14453
这是一个功能请求，主要涉及添加TP支持和移除一些未使用的训练功能。

https://github.com/vllm-project/vllm/issues/14452
这个issue类型是文档（Doc）相关，主要涉及到在RTX5080或5090上运行vLLM的步骤。由于vLLM的预编译版本还未适配所需torch和CUDA版本，所以需要从源代码构建，文档提供了详细的构建步骤和注意事项。

https://github.com/vllm-project/vllm/issues/14451
这个issue类型是需求提出类型，涉及的主要对象是MLA（Machine Learning Accelerator），用户提出了将FlashMLA后端设置为默认选项的需求。

https://github.com/vllm-project/vllm/issues/14450
这个issue类型是关于性能改进的提议，主要涉及的对象是LLMengine中关于LoRA内存占用的计算方式。这可能是由于未考虑LoRA内存占用导致的性能问题而引发的讨论。

https://github.com/vllm-project/vllm/issues/14449
这个issue是一个Bug报告，涉及的主要对象是使用`vllm`和`Qwen/QwQ-32B-AWQ`模型的用户。该Bug是由于使用`tensor-parallel-size 2`参数而导致两个RTX A6000 GPU在100%利用率下卡住，且没有提供详细的调试日志。

https://github.com/vllm-project/vllm/issues/14448
这是一个bug报告，主要涉及Triton FA内核缓存丢失问题，由于`MAX_SEQLENS_Q/K`值不固定导致不同的key值和编译，进而降低性能。

https://github.com/vllm-project/vllm/issues/14447
这是一个用户提出需求的issue，主要涉及的对象是针对VLLM项目中引入moewna16 Marlin核心进行优化。这个问题是由于已有的moe + marlin核心无法充分利用Marlin核心的性能优势，尤其在专家数量较多时，希望引入新的moewna16 Marlin核心来处理所有moe块并支持并行处理。

https://github.com/vllm-project/vllm/issues/14446
这是一个bug报告，主要涉及的对象是QwQ-32B模型。由于环境信息显示PyTorch版本与CUDA版本不匹配，导致模型无法正常输出思维标签。

https://github.com/vllm-project/vllm/issues/14445
这是一个用户提出需求的类型。该问题涉及的主要对象是LoRA。由于当前未提供忽略层功能，用户希望添加一个忽略层来解决特定的问题。

https://github.com/vllm-project/vllm/issues/14444
该issue类型为用户提出需求，主要涉及对象是在PyCharm中运行/调试vllm。由于需要更友好的开发体验，用户希望能够在现代IDE中编辑/调试vllm。

https://github.com/vllm-project/vllm/issues/14443
这是一个Bug报告，主要涉及到在collocate一个较大模型时出现NaN输出问题，在使用外部启动器与模型分片框架（如FSDP）同时进行时。原因是当模型规模变大时，观察到TPed VLLM模型产生NaN值。

https://github.com/vllm-project/vllm/issues/14442
这是一个bug报告，涉及的主要对象是模型在TPU上运行时出现OOM（out of memory）问题，可能是由于ViT中使用了torch SDPA导致。

https://github.com/vllm-project/vllm/issues/14441
这是一个关于如何使用vllm的问题，请教类型的issue，涉及到Multimodal token ids on offloaded tokenization问题，用户询问是否应该在TokensPrompt中包含图像token以及API服务器中tokenization的效果和可能的性能提升。

https://github.com/vllm-project/vllm/issues/14440
这是一个用户提出需求的issue，主要涉及到PD分离支持前缀缓存功能。由于前缀缓存为True时，导致恢复seq_lens到实际的seq_lens值，避免因前一个文本命中时导致当前seq_lens值不准确的问题。

https://github.com/vllm-project/vllm/issues/14439
这个issue类型为提出需求，主要涉及对象为 vLLM 的文档提供。由于缺乏关于在训练场景中使用 vLLM 的文档，用户提出了添加培训文档标识到 TRL。

https://github.com/vllm-project/vllm/issues/14438
这是一个用户提出需求的issue，主要涉及了在多模态数据分析中控制数据的配置问题。由于目前配置选项不足，导致无法限制模型接受的输入类型或减少多模态数据的内存消耗。

https://github.com/vllm-project/vllm/issues/14437
这是一个用户提出需求的类型，主要涉及`torch.compile`的使用解释。原因可能是v1版本发布给公众之后需要说明如何使用这个功能。

https://github.com/vllm-project/vllm/issues/14436
这是一个bug报告，该问题涉及的主要对象是model_runner中的need_recv方法。由于代码中need_recv_kv与need_send_kv相似，但实际上只有decode运行应该接收kv，导致了这个bug。

https://github.com/vllm-project/vllm/issues/14435
这是一个关于性能优化的问题，并涉及VLLM在使用LoRA时推理速度变慢的Bug报告。由于LoRA的rank=256导致推理速度减慢4倍，用户寻求优化建议。

https://github.com/vllm-project/vllm/issues/14434
该issue为功能需求，主要涉及到了对speculative decoding配置和测试的重构。原因是为了使配置更具层次性和清晰性。

https://github.com/vllm-project/vllm/issues/14433
这个issue是关于bug报告，在该问题中用户提到了Docker GPU镜像存在冗余的CUDA运行时库拷贝，导致镜像过大。

https://github.com/vllm-project/vllm/issues/14432
这个issue类型是bug报告，该问题涉及的主要对象是Cuda加载量化模型时出现内存不足错误。原因是CUDA内存不足，导致加载模型失败。

https://github.com/vllm-project/vllm/issues/14431
这个issue是一个性能优化的PR，主要涉及的对象是V1版本的ROCm（Triton）后端，由于对`chunked_prefill_paged_decode` op进行了一些优化，从而改善了对具有GQA的模型的处理，从而实现了更好的吞吐量表现。

https://github.com/vllm-project/vllm/issues/14430
这是一个bug报告，涉及到kernel编译错误，主要对象是CUDA版本小于12.0的情况，由于AllSpark kernel编译错误导致无法在CUDA版本小于12.0上运行。

https://github.com/vllm-project/vllm/issues/14429
该issue类型为功能需求提出，主要涉及工具选择和推理功能的同时使用问题，由于`enableautotoolchoice`和`enablereasoning`无法同时启用导致。

https://github.com/vllm-project/vllm/issues/14428
这是一个bug报告，主要涉及到关于reasoning的逻辑重构。由于stop sequences被同时应用于reasoning_content和content，引起了输出被截断或显示不正确的问题。

https://github.com/vllm-project/vllm/issues/14427
这是一个bug报告，主要涉及选择特定工具调用时返回内容不符预期的问题，可能是由于程序代码逻辑错误导致。

https://github.com/vllm-project/vllm/issues/14426
这个issue是关于bug报告，主要涉及LLM.beam_search在vLLM 0.7.3版本比0.5.4版本运行速度慢的问题。导致这个问题的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/14425
这个issue是关于用户提出需求，主要对象是如何支持通过PCW或其他prefill技术处理100万个prompt tokens输入，可能因为开发自动化任务编排的自主代理系统需要这样的功能。

https://github.com/vllm-project/vllm/issues/14424
这是一个Bug报告，涉及的主要对象是VLLM中选择工具时出现的意外内容。这个问题可能是由于代码缺少对消息内容进行调整工具的解释而导致的。

https://github.com/vllm-project/vllm/issues/14423
这是一个Bug报告类型的Issue， 主要涉及VLLM加载MixtralForCausalLM GGUF模型时出现大小不匹配错误。这可能是由于GGUF模型Q2_K量化类型导致的。

https://github.com/vllm-project/vllm/issues/14422
这个issue类型是bug报告，涉及的主要对象是LoRA测试。由于GPTQ的LoRA生成结果不稳定，导致无法确保TP=1和TP=2之间完全对齐，因此用户提出了这个bug报告。

https://github.com/vllm-project/vllm/issues/14421
这是一个bug报告类型的issue，主要涉及使用vllm进行推理时出现模型输出异常的问题，可能是由于chat_template或其他原因导致准确性显著下降。

https://github.com/vllm-project/vllm/issues/14420
这个issue是关于文档更新的需求，主要涉及了代码库中的prefix_caching.md文件，由于当前描述与示例图不符可能导致误解。

https://github.com/vllm-project/vllm/issues/14419
这个问题是一个Bug报告，涉及的主要对象是V1工具。由于现有的非Torch分配方法同时考虑了非PyTorch分配和其他运行进程使用的内存，导致在与其他进程共用HBM的情况下无法正常运行V1。

https://github.com/vllm-project/vllm/issues/14418
这是一个bug报告类型的issue，主要涉及的对象是benchmark_serving工具。由于benchmark_serving无法正确处理图像输入，用户提出了如何使用benchmark_serving来测试处理图像和提示输入的表现的问题。

https://github.com/vllm-project/vllm/issues/14417
这是一个bug报告，涉及到multi-modal processors的重构。通过简化访问多模态处理器的方式来避免关键错误，并修复了缓存禁用的问题。

https://github.com/vllm-project/vllm/issues/14416
这是一个Bug报告，涉及问题主要对象为bash脚本运行环境，由于权限限制导致std::system_error异常。

https://github.com/vllm-project/vllm/issues/14415
这个issue是关于Bug报告，涉及到vLLM在特定环境下出现"No CUDA GPUs are available"错误的情况。

https://github.com/vllm-project/vllm/issues/14414
这个issue是一个bug报告，主要涉及的对象是Markdown语法。这个错误可能是由于错置的Markdown语法而导致语法有误。

https://github.com/vllm-project/vllm/issues/14413
这是一个bug报告，涉及到在使用vllm v0.7时出现"No CUDA GPUs are available"的RuntimeError。

https://github.com/vllm-project/vllm/issues/14412
这是一个关于性能提升的建议，涉及 vllm 后端使用中的 GPU kv cache 利用率问题。

https://github.com/vllm-project/vllm/issues/14411
这是一个功能需求的issue，主要涉及到`Platform`类的`__getitem__`方法的添加，用户提出需要根据不同设备获取特定的流和事件，因此不再需要对`Stream`或`Event`进行抽象。

https://github.com/vllm-project/vllm/issues/14410
这是一个关于bug报告的issue，主要涉及的对象是v1 engine创建时的进程与环境变量传递有关。

https://github.com/vllm-project/vllm/issues/14409
这个issue类型为需求提出，主要涉及的对象是Engine Args & Documentation。原因可能是出于对参数顺序和文档整洁性的改进需求。

https://github.com/vllm-project/vllm/issues/14408
这个issue是关于需求变更，主要涉及到移除引擎v1中所有与PA有关的内容。原因可能是决定不再支持prompt adapter，而导致需要进行相应修改。

https://github.com/vllm-project/vllm/issues/14407
这是一个功能需求类型的 Issue，主要对象是 VLLM 库。原因是用户希望通过设置环境变量来禁用 TQDM 日志，以减少日志输出量。

https://github.com/vllm-project/vllm/issues/14406
这是一个Bug报告，主要涉及的对象是"pipeline-parallel"与"QwQ model on TPU v4"。由于当前环境中使用的PyTorch版本与CUDA不匹配，导致"pipeline-parallel"在TPU v4上无法正常工作。

https://github.com/vllm-project/vllm/issues/14405
该issue类型为其它类型，主要涉及对象为对vllm的祝贺和讨论。原因可能是用户想表达对vllm项目的支持和赞扬。

https://github.com/vllm-project/vllm/issues/14404
这是一个bug报告，涉及到CUDA错误，由于非法内存访问导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/14403
这是一个Bug报告类型的issue，主要涉及的对象是在运行Docker image Vllm v0.7.3时遇到了错误。由于未能从cudaGetDeviceCount()中得到预期的结果，导致出现了异常的错误信息。

https://github.com/vllm-project/vllm/issues/14402
这是一个功能需求类型的issue，主要涉及的对象是GitHub仓库中的标签和自动化操作。

https://github.com/vllm-project/vllm/issues/14401
这是一个bug报告，涉及到使用--guided-decoding-backend lm-format-enforcer时出现了"transformers not installed"错误提示。这个问题可能是由于缺少transformers库导致的。

https://github.com/vllm-project/vllm/issues/14400
这是一个bug报告，主要涉及vllm下的fused_moe代码与不支持triton的硬件不兼容导致的错误。

https://github.com/vllm-project/vllm/issues/14399
这是一个bug报告，涉及vllm中的stop_sequences在truncate时作用于reasoning_content和content导致的问题。

https://github.com/vllm-project/vllm/issues/14398
这是一个用户寻求安装帮助的issue，主要涉及安装vllm时的环境和步骤。用户询问关于安装vllm的建议或指导。

https://github.com/vllm-project/vllm/issues/14397
这个issue属于Bug报告类型，主要涉及的对象是vLLM库中的`triton_scaled_mm`功能。由于将`triton_scaled_mm`放在了错误的位置并且在ROCm平台上使用了Cutlass执行路径导致该功能从未被调用，需要进行代码重构以解决这个问题。

https://github.com/vllm-project/vllm/issues/14396
这个issue类型是bug报告，主要涉及的对象是vllm项目中关于blockwise cutlass fp8 GEMMs的内存访问问题。这个bug的原因可能是谓词和加载值之间的关系不对称，导致出现非法内存访问的问题。

https://github.com/vllm-project/vllm/issues/14395
这是一个类型为更新请求的issue，主要涉及的对象是内核模块cutlass FP8 blockwise。由于cutlass内核需要更新到最新版本，因此提出了这个issue。

https://github.com/vllm-project/vllm/issues/14394
这是一个bug报告，主要涉及的对象是vllm-0.7.3的 gptq-int3 模型。用户报告了在特定环境下运行该模型时出现的错误。

https://github.com/vllm-project/vllm/issues/14393
这个issue是用户提出需求类型的，主要对象是链接至 https://github.com/pytorch/ao/pull/1848/。由于用户需要具体的资源链接，可能是为了查看或参考相关内容。

https://github.com/vllm-project/vllm/issues/14392
这个issue属于bug报告，主要涉及vllm中的Lora功能出现了输出错误的问题。导致这个bug的原因可能是Lora功能在处理完成请求时出现了错误，导致输出的结果包含了垃圾信息。

https://github.com/vllm-project/vllm/issues/14391
这是一个用户提出需求的issue，主要涉及Neuron的KV cache更新，由于输入张量与缓存张量的布局不匹配，需要添加一个函数来处理这种布局不匹配的情况。

https://github.com/vllm-project/vllm/issues/14390
这是一个由用户提出的需求。该问题涉及的主要对象是VllmConfig。由于VllmConfig需要在`__init__`方法中被访问，因此需要将`apply_fp8_linear`和`apply_fp8_linear_generic`重构为对象。

https://github.com/vllm-project/vllm/issues/14389
这个issue是关于功能新增，涉及主要对象为LoRA类。由于需要新增DoRA支持，所以需要对现有代码做相应的修改和添加。

https://github.com/vllm-project/vllm/issues/14388
这个issue类型是改进建议，主要涉及V1版本的VLLM项目中如何及时移除已完成的请求。由于目前处理方式会导致额外的等待时间，需要提前处理已完成的请求以避免影响后续流程。

https://github.com/vllm-project/vllm/issues/14387
这个issue是一个撤销特定提交的请求，主要涉及VXE ISA for s390x架构的CPU推断功能的问题。

https://github.com/vllm-project/vllm/issues/14386
该issue属于用户提出需求类型，主要对象是vLLM引擎的engine arguments及其文档。原因是由于引擎参数列表过于混乱，随着功能增多，导致难以维护和使用，需要重新整理和分类。

https://github.com/vllm-project/vllm/issues/14385
这是一个文档错误修正的issue，主要涉及的对象是代码中的参数。原因是参数名称错误导致了文档缺陷。

https://github.com/vllm-project/vllm/issues/14384
该问题单属于性能优化类，主要涉及的对象是MLA（Machine Learning Accelerator），由于rotary embeddings导致了CPU overheads增加，需要减少。

https://github.com/vllm-project/vllm/issues/14383
这个issue属于功能需求类型，涉及的主要对象是为cutlass添加blackwell fp8 blockwise gemm支持。

https://github.com/vllm-project/vllm/issues/14382
这个issue是一个bug报告，涉及到FusedMoE层的条件设置错误导致了一些操作的错误执行。

https://github.com/vllm-project/vllm/issues/14381
该issue类型为用户提出需求，主要对象为vLLM，提出了关于是否可以集成QFilters来改进长文本支持和在受限硬件上使用更大模型的问题。

https://github.com/vllm-project/vllm/issues/14380
这是一个bug报告，主要涉及到在Ray Serve use cases中调用`signal.signal`时出现的异常。

https://github.com/vllm-project/vllm/issues/14379
这是一个bug报告，主要涉及到vLLM中的arg_utils.py文件中的device查询问题，导致Ray Serve使用vLLM的数据结构时出现异常。

https://github.com/vllm-project/vllm/issues/14377
这是一个性能优化类的issue，主要涉及到qwen2-vl模块中的逻辑优化问题，导致了GPU和CPU数据传输造成的时间消耗增加。

https://github.com/vllm-project/vllm/issues/14376
这是一个Bug报告，涉及主要对象为vllm引擎在加载两个模型到同一GPU时出现内存分配问题，导致第二个模型需要更多的内存。

https://github.com/vllm-project/vllm/issues/14375
这是一个优化性质的问题，相关内容主要涉及到Qwen2-vl中的逻辑优化，由于GPU张量被多次复制到CPU，导致CUDAMemcpyAsync时间消耗增加。

https://github.com/vllm-project/vllm/issues/14374
获取错误，请重新获取

https://github.com/vllm-project/vllm/issues/14373
这是一个bug报告，主要涉及的对象是test_sleep功能。由于使用了错误的参数`data`而导致无法设置其他级别，需要改为使用正确的参数`params`来解决。

https://github.com/vllm-project/vllm/issues/14372
这是一个Bug报告类型的issue，主要涉及VLLM在使用nsight systems进行性能分析时出现的问题。由于使用特定的ray参数可以使其正常工作，但导致推断速度变慢，用户想知道如何在不使用ray作为后端的情况下运行nsight profiling。

https://github.com/vllm-project/vllm/issues/14371
这是一个bug报告，问题涉及的主要对象是项目的许可证头部信息中的作者信息被删除，导致需要再次添加。

https://github.com/vllm-project/vllm/issues/14370
这个issue类型是bug报告，涉及主要对象是JambaForCausalLM中的LoRA实现。由于当前的CI测试被跳过，导致问题未被发现，测试时发现问题包括需要为lora layer添加weight属性以及Jamba LoRA测试脚本中包含了当前不支持的MOE层，因此直接移除了Jamba LoRA测试脚本。

https://github.com/vllm-project/vllm/issues/14369
这个issue是一个Bug报告，涉及的主要对象是 `simple_connector.py` 中的 `send_kv_caches_and_hidden_states` 函数。由于假设 `head_size == hidden_size // num_attention_heads` 不一定成立，可能导致数据损坏或死锁，因此作者需要添加一个检查以确保代码的健壮性。

https://github.com/vllm-project/vllm/issues/14368
这是一个bug报告，涉及主要对象是Benchmark模块。由于初始化参数错误，token过滤问题，排序问题和未使用参数，导致功能异常和性能下降。

https://github.com/vllm-project/vllm/issues/14367
这个issue类型是bug报告，主要涉及的对象是torch.store timeout配置，用户提出了需要增加kv_connector_extra_config配置以解决DistStoreError的问题。

https://github.com/vllm-project/vllm/issues/14366
这个issue是bug报告类型，涉及的主要对象是README.md文件，导致的bug是日期拼写错误。

https://github.com/vllm-project/vllm/issues/14365
这是一个Bug报告，主要涉及对象是API连接错误。由于并发API调用导致的API连接错误。

https://github.com/vllm-project/vllm/issues/14364
这是一个bug报告，涉及vllm下的STREAMING功能无法生成空格的问题。可能是由于STREAMING模式的开启导致这一现象。

https://github.com/vllm-project/vllm/issues/14363
该issue类型为文档相关问题，主要涉及VLLM中的beam_search函数的使用说明不清，导致用户难以理解该函数如何使用。

https://github.com/vllm-project/vllm/issues/14362
这个issue是一个bug报告，涉及的主要对象是Dockerfile。由于拼写错误导致了bug。

https://github.com/vllm-project/vllm/issues/14361
这是一个Bug报告，主要涉及的对象是CC模块的OOM问题，原因是加载模型时发生OOM，导致V0版本出现问题而V1版本没有问题。

https://github.com/vllm-project/vllm/issues/14360
这个issue是关于性能问题的报告，涉及到vllm版本0.6.5的优化建议，主要问题是多模态输入降低了服务吞吐量，可能是由于多媒体数据传输引起的。

https://github.com/vllm-project/vllm/issues/14359
这是一个bug报告，主要涉及自动工具调用缺少tool-call-parser导致的错误。

https://github.com/vllm-project/vllm/issues/14358
这是一个bug报告，主要涉及构建过程中使用最新提交的nightly wheel出现的问题，因为nightly wheel尚未上传导致precompiled build失败。

https://github.com/vllm-project/vllm/issues/14357
这是一个bug报告，主要涉及LLMEngine.seq_id_to_seq_group_size方法导致的内存泄漏。

https://github.com/vllm-project/vllm/issues/14356
这是一个bug报告类型的issue，涉及的主要对象是`best_of`功能和`SamplingParams`。由于作者误解，`best_of`在V0中被错误地移除，现在需要重新恢复。

https://github.com/vllm-project/vllm/issues/14355
这是一个Bug报告，涉及的主要对象是LLMEngine.seq_id_to_seq_group_size。由于LLMEngine.seq_id_to_seq_group_size存在内存泄漏，导致了memory leak的问题。

https://github.com/vllm-project/vllm/issues/14354
这是一个BugFix类型的issue，主要涉及Hopper 12.8版本构建时缺失符号的问题，由于在https://github.com/vllmproject/vllm/pull/13798引入的bug导致。

https://github.com/vllm-project/vllm/issues/14353
这是一个Bug报告类型的Issue，主要涉及到vLLM的LLMEngine.seq_id_to_seq_group内存泄漏问题，导致内存使用过多和内存溢出错误。

https://github.com/vllm-project/vllm/issues/14352
这是一个bug报告类型的issue，主要涉及ChatCompletionRequest类的validation bug导致其拒绝自己的默认值。

https://github.com/vllm-project/vllm/issues/14351
这是一个Bug报告，涉及到VLLM中的ChatCompletionRequest模型，默认值被拒绝的问题，导致实例化并序列化后可能出现问题。

https://github.com/vllm-project/vllm/issues/14350
这是一个性能问题的报告，主要涉及计算并发值方面的问题。实际测量到的并发值是预期计算值的两倍，原因可能是计算错误或者vllm有特殊优化。

https://github.com/vllm-project/vllm/issues/14349
这是一个Bug报告，主要涉及vllm无法连接到外部ray集群的问题。由于环境配置或代码逻辑不正确导致了这个连接问题。

https://github.com/vllm-project/vllm/issues/14348
这是一个用户提出需求的类型，主要对象是如何从图片中获取嵌入结果，询问Qwen2.5-vl-7b能否实现此功能。

https://github.com/vllm-project/vllm/issues/14347
这是一个bug报告，涉及的主要对象是代码中的KV_T和CACHE_T宏。由于评论被错误地交换，在调用特定宏时可能导致混乱。

https://github.com/vllm-project/vllm/issues/14346
该issue类型为用户提出需求，主要涉及的对象是Ovis2 VLM series，用户寻求关于该系列性能表现的帮助。

https://github.com/vllm-project/vllm/issues/14345
这是一个bug报告，主要涉及V1 Test在使用spawn时出现问题，导致monkeypatch未被正确识别。

https://github.com/vllm-project/vllm/issues/14344
这是一个用户请教问题的类型，主要涉及chat templates中如何设置date_string变量。用户想要了解如何设置date_string变量以及相关操作。

https://github.com/vllm-project/vllm/issues/14343
这是一个需求类型的issue，主要涉及对象是添加Phi4-MM示例。

https://github.com/vllm-project/vllm/issues/14342
这是一个Bug报告，主要涉及的对象是vllm项目中的Phi-4-multimodal-instruct音频标记标签。这个问题是由于PyTorch版本2.5.1和其他环境信息不匹配导致的。

https://github.com/vllm-project/vllm/issues/14341
这是一个Bug报告，主要涉及在线采样和离线采样结果不一致的问题。可能由于采样参数设置不当或代码实现问题导致了这一现象。

https://github.com/vllm-project/vllm/issues/14340
这是一个bug报告，涉及到分布式推理的多节点通讯问题，由于NCCL_SOCKET_IFNAME和GLOO_SOCKET_IFNAME设置后运行时出现通讯bug，用户寻求关于此问题的帮助。

https://github.com/vllm-project/vllm/issues/14338
这是一个bug报告，问题涉及 OpenVINO 中的 CPU-like conditions，由于某些原因导致出现问题。

https://github.com/vllm-project/vllm/issues/14337
这个issue类型是用户提出需求，主要涉及的对象是eagle投机采样。用户希望增加对多模态模型的eagle投机采样。

https://github.com/vllm-project/vllm/issues/14336
这个issue属于bug报告类型，主要涉及的对象是在多模式分析时避免使用缓存。由于在分析步骤使用缓存，导致无法立即丢弃占用大量内存的虚假输出，因此用户提出了这个问题。

https://github.com/vllm-project/vllm/issues/14335
这个issue类型属于功能需求提出，在Chat Completion Response中缺少`reasoning_tokens`的元数据。

https://github.com/vllm-project/vllm/issues/14334
这是一个bug报告，主要涉及GPU索引指定失败导致启动模型选择错误显卡的问题。

https://github.com/vllm-project/vllm/issues/14333
这是一个bug报告，涉及vLLM在高负载下返回415状态码的问题，可能是由于负载增加导致的。

https://github.com/vllm-project/vllm/issues/14332
这个issue类型是需求提出，主要涉及的对象是为vllm添加在单一节点上运行评估脚本。由于当前的文档需要更新，且针对两个节点的情况还未解决，用户提出此需求。

https://github.com/vllm-project/vllm/issues/14330
这是一个Bug报告，主要涉及opentelemetry POC vLLM span无法与HTTP spans进行拼接的问题。造成这个Bug的原因可能与环境信息中的Python版本、OS版本、以及CUDA可用性等因素有关。

https://github.com/vllm-project/vllm/issues/14329
这是一个关于优化内核后使用优化的块大小的问题，类型为需求提出。问题涉及主要对象为代码中的块大小参数。

https://github.com/vllm-project/vllm/issues/14328
这是一个用户提出需求的issue，主要涉及对out-of-tree quantization method在cli args中无法加载的问题，由于加载插件后未进行quantization methods的验证导致新注册的quantization methods无法在验证时生效。

https://github.com/vllm-project/vllm/issues/14327
这是一个bug报告，主要涉及 `cutlass_scaled_mm_supports_block_fp8` 方法的错误调用。原因可能是该方法被错误地连接。

https://github.com/vllm-project/vllm/issues/14326
这是一个bug报告，涉及的主要对象是处理n>1请求时的异常表现。

https://github.com/vllm-project/vllm/issues/14325
该issue属于用户提出需求类型，主要涉及vllm库中设置输入图像大小的问题。由于用户想要运行特定模型的推断，但不知道如何将其集成到vllm中，故向社区寻求帮助。

https://github.com/vllm-project/vllm/issues/14324
这是一个关于bug的issue报告，主要涉及到DeepseekV2Model对象中缺少'config'属性的问题。导致这个bug的原因可能是代码中的错误或者缺失了对应的配置信息。

https://github.com/vllm-project/vllm/issues/14323
这是一个非bug报告类的issue，主要涉及的对象是为VLLM模型添加支持PLaMo2，并且由于目前尚未支持某些功能，用户计划在后续的PR中解决这些问题。

https://github.com/vllm-project/vllm/issues/14322
这是一个用户提出需求的issue，主要涉及输入构造方式是否会影响服务器性能，原因可能是单次请求仅发送一个图片和一个提示导致。

https://github.com/vllm-project/vllm/issues/14321
这个issue类型是用户提出需求，主要涉及对象是新模型的支持。用户提出了关于新模型QwQ-32B的支持问题。

https://github.com/vllm-project/vllm/issues/14320
这是一个功能需求的issue，主要涉及到vllm中关于CUDA的注意力后端以及错误消息显示的改进。原因是希望在出现错误时提供更具体的提示信息，改进错误类型和错误消息的表达方式。

https://github.com/vllm-project/vllm/issues/14319
这是一个关于文档内容和代码实现之间矛盾的问题，主要涉及到vLLM项目中的参数`block_size`在不同设备上的限制问题。由于不清楚代码文档中关于CUDA设备只支持最大block size为32的限制是从何而来，以及用户可以在Hopper设备上成功使用block size为128并获得性能提升，因此提出了相关疑问。

https://github.com/vllm-project/vllm/issues/14318
该issue类型为功能需求反馈，主要对象为ray部署用户，提出了需要更多指导以解决ray部署问题的需求。

https://github.com/vllm-project/vllm/issues/14317
这是一个bug报告，主要涉及FP8模型在RoCM运行以及Ada Lovelace GPUs时vLLM构建于CUDA < 12.4 CC时的问题，由于在forward pass中调用`get_current_vllm_config()`但未在worker初始化外设置`_current_vllm_config`导致警告大量输出。

https://github.com/vllm-project/vllm/issues/14316
这是一个bug报告类型的issue，主要涉及的对象是vllm下的MLA模块。由于LSE计算在triton内核中无法正常运行，导致在这种情况下始终会回退到flash attention。

https://github.com/vllm-project/vllm/issues/14315
这是一个用户提出需求的 issue， 主要涉及 V1 engine 的测试内容，并希望新增一些测试用例。

https://github.com/vllm-project/vllm/issues/14313
这个issue类型是bug报告，涉及的主要对象是Attention模块，在quantized模型中被忽略时存在bug，导致quant_method返回错误的方法。

https://github.com/vllm-project/vllm/issues/14312
这是一个功能需求的issue，主要对象是API端点`/is_sleeping`。原因是用户希望通过只读API检查引擎是否正在休眠。

https://github.com/vllm-project/vllm/issues/14311
该issue类型为用户提出需求，主要涉及的对象是vLLM引擎。由于缺乏能够检查引擎是否处于睡眠状态的只读API，用户提出了对引擎睡眠状态进行监测的需求。

https://github.com/vllm-project/vllm/issues/14310
这是一个bug报告，主要涉及Hardware中TPU相关的功能问题，由于参数布局的改变导致需要更新torch xla nightly wheel。

https://github.com/vllm-project/vllm/issues/14309
该issue类型为代码审查和修改请求，主要涉及的对象是tpu_model_runner.py文件。由于https://github.com/vllmproject/vllm/pull/14098需要进行干净的合并，因此需要移除self.kv_caches。

https://github.com/vllm-project/vllm/issues/14308
这是一个bug报告，涉及的主要对象是V1版本中的sampler，原因是如果allowed_token_ids不为None但却为空时会引发错误。

https://github.com/vllm-project/vllm/issues/14307
这是一个bug报告，涉及到vllm对希腊字符处理不准确的问题，用户希望在使用Mistral Small模型进行推理时能够正确处理希腊字符。

https://github.com/vllm-project/vllm/issues/14306
这是一个bug报告，涉及主要对象为GEMM kernels，由于PyTorch 2.6.0默认行为更改导致自定义运算默认要求连续内存，而GEMM kernels中需要对权重进行离线转置，造成冲突。

https://github.com/vllm-project/vllm/issues/14305
这是一个功能需求类型的issue，该问题涉及到代码中的环境变量`VLLM_TEST_ENABLE_EP`的替换。原因是为了提供一个新的参数`enable_expert_parallel`来替代原有的环境变量设置。

https://github.com/vllm-project/vllm/issues/14304
这是一个bug报告类型的issue，涉及的主要对象是源代码中的一个过时的FIXME注释。

https://github.com/vllm-project/vllm/issues/14303
这个issue是关于bug报告，主要对象是在v1版本上运行benchmark时出现的问题，由于参数`best_of`在v1版本中尚不支持，导致benchmark serving脚本失败并导致v1仪表板上无法显示任何serving metrics。

https://github.com/vllm-project/vllm/issues/14302
这是一个bug报告类型的issue，主要涉及的对象是`num_tokens_across_dp`变量。由于在更新过程中意外删除了一行代码，导致`num_tokens_across_dp`变量未被正确更新，最终表现为该变量在当前dp_rank中包含batch_size而在其他情况下为0。

https://github.com/vllm-project/vllm/issues/14301
这是一个Bug报告，主要涉及了处理包含多个指定顶k值和部分指定温度值的请求批次时出现问题。

https://github.com/vllm-project/vllm/issues/14300
这是一个Bug报告，主要涉及logprobs参数导致的性能下降问题，原因可能是logprobs检索操作的复杂度不符预期。

https://github.com/vllm-project/vllm/issues/14299
这个issue是关于bug报告，涉及的主要对象是V1模块，由于最近的更新导致V1拒绝了特定请求，现在的问题是允许指定best_of参数为1但错误地拒绝了该请求。

https://github.com/vllm-project/vllm/issues/14298
这个issue类型是需求提出，主要涉及文档的结构和内容，用户提议将profiling移到性能部分考虑。

https://github.com/vllm-project/vllm/issues/14297
这是一个用户提出需求的issue，涉及到文档更新。由于缺少具体内容，用户提出了需要添加元幻灯片的需求。

https://github.com/vllm-project/vllm/issues/14296
这是一个bug报告类型的issue，主要涉及Cython编译支持的修复，由于类型错误导致了编译错误。

https://github.com/vllm-project/vllm/issues/14295
这是一个bug报告，该问题涉及vLLM针对Intel Core Ultra 7 155H with ARC iGPU的构建和运行问题，可能由于CURL无法返回有效响应导致。

https://github.com/vllm-project/vllm/issues/14294
这是一个bug报告，涉及到文档内容的错误，导致LRU block被误称为"LRU black"。

https://github.com/vllm-project/vllm/issues/14293
这个issue是关于文档修正的类型，涉及的主要对象是LRU缓存。导致该问题的原因是一个笔误导致"C"被误写成了"black"，需要进行修正。

https://github.com/vllm-project/vllm/issues/14292
这个issue是BUG报告，主要涉及的对象是vision language模块，由于CC导致了破坏的vision示例。

https://github.com/vllm-project/vllm/issues/14291
该issue类型是功能增强，涉及的主要对象是vLLM中的ColQwen2VL模型。由于用户需要在vLLM中添加对ColQwen2VL模型的支持，以增加文档检索的能力。

https://github.com/vllm-project/vllm/issues/14290
这个issue类型是bug报告，主要涉及的对象是添加新模型"llava-onevision-qwen2-72b-ov-sft"。问题可能源于模型体系结构不支持，导致报错"ValueError: Model architectures ['TransformersModel'] failed to be inspected."

https://github.com/vllm-project/vllm/issues/14289
这是一个用户提出需求的issue，主要涉及的对象是带有`AsyncLLMEngine`的`LLM`类，用户希望能够在`AsyncLLMEngine`中实现类似于`chat`方法的功能，或者将`generate`方法的`PromptType`扩展以支持更多的提示变体，例如聊天对话。

https://github.com/vllm-project/vllm/issues/14288
这是一个用户提出需求或建议的类型的issue，主要涉及的对象是external_launcher backend的文档。这个问题的原因是用户需要设置LOCAL_RANK来完成相关的操作。

https://github.com/vllm-project/vllm/issues/14287
这个issue是关于bug报告的，主要对象是调试信息的精炼问题，可能由于之前的调试信息不清晰导致症状不明显或者开发者无法轻易定位问题所在。

https://github.com/vllm-project/vllm/issues/14286
这是一个Bug报告，涉及的主要对象是使用vllm加载Llama3.1-8B-INT8模型时出现OOM错误。导致这一状况的原因可能是使用VLLM_V1加载时出现了内存溢出的问题，而切换回VLLM_V0则能够正常加载模型。

https://github.com/vllm-project/vllm/issues/14285
该issue类型是功能需求，主要涉及对象是代码中的`interval`变量，用户提出了将`interval`重命名为`max_recent_requests`来增强代码可读性的建议。

https://github.com/vllm-project/vllm/issues/14284
这是一个功能需求的issue，主要涉及的对象是vllm中的benchmark功能。由于benchmark_serving.py只支持文本和图像输入，并且只使用openai chat作为后端，因此用户想了解是否支持音频输入到多模态并尝试用vllm的benchmark来测试多模态Qwen2Audio的吞吐量和延迟。

https://github.com/vllm-project/vllm/issues/14282
这是一个文档问题，报告了源代码注释中的死链。

https://github.com/vllm-project/vllm/issues/14281
这个issue属于bug报告类型，主要涉及vllm中的多模态处理逻辑，用户提出了如何跳过已经处理过的输入内容的问题。

https://github.com/vllm-project/vllm/issues/14280
这是一个bug报告，涉及V1在设置n>1时仍只返回一个response的问题。原因可能是采样参数设置不正确。

https://github.com/vllm-project/vllm/issues/14279
这是一个bug报告，该问题单涉及的主要对象是debug msg的细节优化，由于debug msg未提供详细信息而引发了问题。

https://github.com/vllm-project/vllm/issues/14278
这个issue类型是需求提出，主要涉及的对象是文档内容。由于缺少对encoderdecoder的示例，需要为开发多模式处理器提供示例代码。

https://github.com/vllm-project/vllm/issues/14277
这是一个用户提出需求的issue，主要涉及的对象是在vLLM OpenAI服务器上运行时在Ray集群上部署指定节点的控制，由于当前Ray集成似乎受限于指定tp和pp，用户提出是否支持自定义放置组是可行的。

https://github.com/vllm-project/vllm/issues/14276
这是一个需求提出类型的issue，主要涉及的对象是Qwen2MoeForCausalLM模型。提出该需求的原因可能是希望添加对Qwen2MoeForCausalLM模型的moe调整支持。

https://github.com/vllm-project/vllm/issues/14275
这是一个bug报告，主要涉及的对象是VLM（Vision and Language Module）。由于V1版本在支持Pixtral-HF时出现问题，导致无法正确生成结果，需要修复此bug。

https://github.com/vllm-project/vllm/issues/14274
这是一个bug报告，涉及的主要对象是vLLM库中的seed参数。原因是在CC(Fix seed parameter behavior in vLLM)中未将seed参数的默认值设置为None，导致需要手动设置seed参数的值。

https://github.com/vllm-project/vllm/issues/14273
这个issue是关于文档更新的，涉及修改`uv venv`指令的Python环境设置说明。这次修改是由于之前指南中使用了已存在的目录名`vllm`，导致在设置Python虚拟环境时出现错误。

https://github.com/vllm-project/vllm/issues/14272
这是一个 bug 报告，主要涉及 Phi4multimod 在使用 LoRA 时观察到的高延迟开销问题。

https://github.com/vllm-project/vllm/issues/14226
这是一个关于优化并发请求处理的需求类issue，涉及主要对象为Qwen2VL模型在单个24GB GPU上的并发推理问题。主要问题是如何在单个实例上有效处理并发请求以及为何第二个请求的响应时间较长，以及如何进行优化。

https://github.com/vllm-project/vllm/issues/14225
这是一个bug报告类型的issue，涉及的主要对象是`vllm`项目的前端部分。该issue由于之前在`serving_chat.py`中未进行`prompt_logprobs`的clamping处理，导致产生了bug，用户提出了在两个文件中都执行clamping的需求。

https://github.com/vllm-project/vllm/issues/14224
这是一个bug报告，主要对象是V1版本的VLLM模型。由于sampling参数detokenize设置为False时，依然进行了detokenize操作，导致了这个bug。

https://github.com/vllm-project/vllm/issues/14223
这是一个bug报告，涉及到性能问题，由于`--generation-config`不是`None`导致了性能差异巨大。

https://github.com/vllm-project/vllm/issues/14222
这是一个关于用户提出需求的issue，主要涉及获取vllm中间输出的问题，用户希望了解如何在vllm中获取中间输出并保存在磁盘上。这可能由于缺乏文档或相关功能尚未明确支持而导致。

https://github.com/vllm-project/vllm/issues/14221
这是一个bug报告，涉及的主要对象是V1版本的attention后端。由于忽略了FP8 KV缓存参数并且对其不支持的后端导致了用户报告KV缓存"工作不正常"的情况。

https://github.com/vllm-project/vllm/issues/14220
这是一个bug报告，主要涉及V1版本中指标计算时的报错问题。由于preemptions和LoRA的配合出现了bug，导致出现了报错。

https://github.com/vllm-project/vllm/issues/14219
这是一个功能需求类型的issue，主要涉及到V1版本中的MultiprocExecutor以支持PP。

https://github.com/vllm-project/vllm/issues/14218
这是一个Bug报告，主要涉及Deepseek R1 671B int8 在TPU上无法工作。原因可能是当前环境下的PyTorch版本不支持。

https://github.com/vllm-project/vllm/issues/14217
这个issue类型为文档更新，主要涉及到运行vLLM容器时应避免使用特权标志并指定目标GPU ID，以避免OOM问题。

https://github.com/vllm-project/vllm/issues/14216
该issue类型为用户提出需求，该问题涉及的主要对象为新增支持aya 32b vision模型。由于新增发布的aya vision 32b模型具有仅32b参数并达到qwen vl 2.5 72B的能力，用户提出希望支持该模型的需求。

https://github.com/vllm-project/vllm/issues/14215
这是一个需更新文档的问题，主要涉及到 Dockerfile 依赖镜像的更新。

https://github.com/vllm-project/vllm/issues/14214
这是一个关于新增模型支持的需求类型的issue，主要涉及的对象是PFN的plamo28b模型。用户提出希望vllm添加对该模型的支持，但未得到响应，并未说明难点所在，可能由于缺乏响应和未列明具体困难而引起。

https://github.com/vllm-project/vllm/issues/14213
这个issue是关于技术疑问，涉及vLLM中的请求分配到多个微批次的逻辑，用户寻找相关源码但未找到，并询问分配请求的具体过程发生在哪个组件中。

https://github.com/vllm-project/vllm/issues/14212
这是一个Bug报告类型的issue，涉及的主要对象是Gemma2ForCausalLM。由于缺少'lm head'导致数值错误(ValueError)。

https://github.com/vllm-project/vllm/issues/14211
这是一个用户提出需求的issue，主要涉及更新`compressed-tensors`来支持zero-points，可能由于当前功能不支持zero-points导致用户需要更新。

https://github.com/vllm-project/vllm/issues/14210
这是一个bug报告，涉及的主要对象是MacOS CPU检测。由于ARM CPU检测可能影响NVIDIA ARM设备，导致需要限制特定情况仅适用于MacOS，这可能导致了问题。

https://github.com/vllm-project/vllm/issues/14209
这是一个Bug报告，主要涉及Ultravox音频在auto tool选择下无法工作的问题。原因可能是auto tool功能导致音频无法正常播放。

https://github.com/vllm-project/vllm/issues/14208
该issue类型为用户提出需求类型，主要涉及了vllm平台支持新模型nicolinho/QRM-Llama3.1-8B-v2。原因可能是希望vllm能够支持特定的模型，需求未得到响应。

https://github.com/vllm-project/vllm/issues/14207
这个issue类型是需求类型，主要对象是Kernel中的MoE tuning功能。这个问题被提出是因为在warmup运行时是否跳过慢速配置，可以将运行速度提高至少50%。

https://github.com/vllm-project/vllm/issues/14206
这是一个用户提出需求的issue，主要涉及如何记录请求令牌长度，可能是由于缺乏相关文档或信息导致用户产生的疑问。

https://github.com/vllm-project/vllm/issues/14205
这是一个bug报告，涉及的主要对象是在使用OpenVINO backend时出现的参数缺失错误，导致无法运行offline inference的示例脚本。

https://github.com/vllm-project/vllm/issues/14204
这是一个用户提出需求的类型issue，涉及对象为qwen2修改的_sample函数在transformers中如何快速与vllm兼容。由于qwen2中的_sample函数被修改，用户需要寻求方法将其与vllm兼容，可能是由于vllm的接口或功能的差异导致兼容性问题。

https://github.com/vllm-project/vllm/issues/14203
这是一个Bug报告，主要涉及到在使用gsm8k时出现不同结果的问题。原因可能是环境设置或代码逻辑导致。

https://github.com/vllm-project/vllm/issues/14202
This is a bug report for adding a reasoning parser for Granite 3.2 models, specifically related to an issue concerning the use of `Here's` instead of `Here is`.

https://github.com/vllm-project/vllm/issues/14201
这是一个用户需求类型的issue，主要涉及到对如何在vllm中强制实现文本输出以"\n" 开头进行讨论。可能出现的原因是用户想要在运行推理时与特定模型集成，但不清楚如何在vllm中实现。

https://github.com/vllm-project/vllm/issues/14200
这是一个文档增加类的 issue，关联到了一个之前的 pull request，主要涉及到解释 "Failed to infer device type" 错误信息，帮助用户理解可能出现的问题来源。

https://github.com/vllm-project/vllm/issues/14199
这是一个类型为优化/改进的issue，涉及主要对象为软件依赖管理。由于numba不应该包含在cpu版本的vllm中，因此将其从通用依赖转移到cuda/rocm特定依赖中，以避免在cpu版本中引入不必要的依赖。

https://github.com/vllm-project/vllm/issues/14198
这个issue是一个文档更新类型的请求，涉及到vLLM项目中s390x架构的CPU实现。这个请求是由于需要更全面的文档来描述如何构建这一特定架构的CPU实现。

https://github.com/vllm-project/vllm/issues/14197
这是一个Bug报告类型的Issue，主要涉及vllm在崩溃后端口仍处于打开状态，导致再次启动相同代码时出现"OSError: [Errno 98] Address already in use"错误。原因可能是缺少正确的端口关闭操作。

https://github.com/vllm-project/vllm/issues/14195
这是一个BUG报告，主要涉及的对象是在推断设备类型时出现的“failed to infer device type”错误，导致用户无法确定问题来源。

https://github.com/vllm-project/vllm/issues/14194
这是一个bug报告，主要对象是项目的README文件。该问题由于链接仍指向一个重定向到首篇博客文章的网站而不是博客文章列表，导致了需要更新链接的情况。

https://github.com/vllm-project/vllm/issues/14193
这是一个Bug报告，主要对象涉及到使用disaggregated_prefill的情况。由于未输入任何内容时，导致KV receiving thread报告超时错误。

https://github.com/vllm-project/vllm/issues/14192
这个issue是用户提出需求类型的反馈，主要涉及的对象是vllm模型。用户希望支持新的模型deepseek-vl2，但未收到相应的回复。

https://github.com/vllm-project/vllm/issues/14191
这是一个bug报告，涉及的主要对象是CUDA_VISIBLE_DEVICES参数配置。由于CUDA显存不足导致CUDA out of memory错误。

https://github.com/vllm-project/vllm/issues/14190
这是一个功能需求报告，涉及vLLM中使用LLaMA 3.1指令模型时在ChatML格式下的提示格式问题。由于`tokenizer.chat_template`似乎未正确强制执行带有特殊标记`{role}\n\n{content}`的格式，导致多轮对话中提示格式未正确维护。

https://github.com/vllm-project/vllm/issues/14189
这个issue是关于bug报告，涉及主要对象是vllm的sleep mode实现。由于sleep mode实现与expandable_segments不兼容，导致了错误的表现，无法在短期内修复。

https://github.com/vllm-project/vllm/issues/14187
这是一个用户提出需求的issue，涉及主要对象是运行vllm进行推断的功能。用户想要输出的结果与输入顺序不一致，可能由于程序实现逻辑错误导致。

https://github.com/vllm-project/vllm/issues/14186
这是一个Bug报告，涉及VLLM多节点部署问题。由于资源请求无法满足导致启动服务时遇到错误。

https://github.com/vllm-project/vllm/issues/14185
这是一个需求提出的issue，主要涉及RLHF用户和vLLM的worker类，由于即将迁移至V1版本，需要更改worker类继承方式。

https://github.com/vllm-project/vllm/issues/14184
这是一个Bug报告，主要涉及VLLM的模型加载问题。由于更新至主git版本后，使用v0版本加载模型时出现OOM错误，但设置环境变量`VLLM_USE_V1=1`后可以正常加载模型。

https://github.com/vllm-project/vllm/issues/14183
这是一个bug报告，主要涉及的对象是benchmark scripts。由于存在`cudaProfilerStart` 被调用两次的错误，以及其他多个问题累积造成的错误的bug报告。

https://github.com/vllm-project/vllm/issues/14182
这是一条缺少具体内容的Bug报告类型的issue，主要涉及Deepseek MTP for V1功能，由于缺少具体信息而无法确定问题的症状。

https://github.com/vllm-project/vllm/issues/14181
这是一个Bug报告，主要涉及vllm下的qwen2.5_vl模型在运行过程中出现cuda kernel crash的问题。

https://github.com/vllm-project/vllm/issues/14180
该issue属于用户提出需求的类型，主要涉及的对象是vllm项目中的模型支持情况。由于InternVideo2.5是建立在InternVL2.5之上，用户希望vllm能够支持或实现OpenGVLab/InternVideo2_5_Chat_8模型。

https://github.com/vllm-project/vllm/issues/14179
这是一个bug报告，该问题单涉及的主要对象是WorkerWrapperBase初始化过程，在此过程中由于访问了model_config和trust_remote_code等内容而导致了问题。

https://github.com/vllm-project/vllm/issues/14178
这是一个Bug报告，涉及的主要对象是在Jetson AGX Orin设备上使用VLLM时出现的运行错误。由于修改了`.deps/flashmlasrc/setup.py`文件中的CUDA参数以及可能涉及的其他配置错误，导致无法成功运行`basic.py`示例文件。

https://github.com/vllm-project/vllm/issues/14177
这是一个技术性issue，涉及到代码维护和功能修改，主要对象是vLLM模型中的`LoRA`功能。该问题由于原始线性层hack导致在外部`transformers`后端实现时增加了维护难度，用户提出了移除这些hack并添加`return_bias`选项以改进代码结构。

https://github.com/vllm-project/vllm/issues/14176
该issue类型为用户提出需求，主要涉及DeepSeekR1W8A8模型的quantization方法不支持vLLM，用户希望得知是否有计划支持该模型。

https://github.com/vllm-project/vllm/issues/14175
这是一个bug报告类型的Issue，主要涉及的对象是"per_token_group_quant_fp8" CUDA kernel。这个问题可能是由于某些形状导致的失败。

https://github.com/vllm-project/vllm/issues/14174
这是一个用户提出需求的类型的issue，主要涉及Whisper语音识别模型是否会添加语言检测功能。由于当前代码只能执行转录任务且无法检测语言，用户请求添加语言检测功能。

https://github.com/vllm-project/vllm/issues/14173
这是一个性能优化的issue，主要涉及的对象是`IncrementalDetokenizer.update()`方法。原因是当前实现中逐个处理token会在处理多个token时效率变低。

https://github.com/vllm-project/vllm/issues/14172
该issue类型是优化提案，主要涉及token ID到token的转换；由于需要优化批处理操作，以提高效率。

https://github.com/vllm-project/vllm/issues/14171
这是一个特性请求，主要涉及修改torch版本至2.6.0。

https://github.com/vllm-project/vllm/issues/14170
这是一个Bug报告，该问题涉及到vllm中的stop参数。由于包含stop参数导致输出被截断，用户在寻求修复这一问题。

https://github.com/vllm-project/vllm/issues/14169
这是一个bug报告，针对v1 Sampler中allowed_token_ids的修复。修复前的问题是因为one和zero调换导致masked_fill_的错误应用。

https://github.com/vllm-project/vllm/issues/14168
这是一个功能需求的问题单，涉及到vllm项目中的`max_num_generation_tokens`参数，用户提出了要将其弃用的需求。

https://github.com/vllm-project/vllm/issues/14167
这个issue是关于bug报告，主要涉及vllm项目中processor.py文件的off-by-one错误导致的对齐问题。

https://github.com/vllm-project/vllm/issues/14166
该issue是一个功能需求，主要涉及SLora中的LoRa适配器的热加载问题。

https://github.com/vllm-project/vllm/issues/14165
这是一个bug报告，涉及的主要对象是benchmarks脚本。该问题是由于重复调用`make_rand_sparse_tensors`导致了性能损耗。

https://github.com/vllm-project/vllm/issues/14164
这是一个bug报告，涉及的主要对象是github上的vllm项目下的benchmark_moe.py文件。由于某次更新导致CUDA设备上的tuning操作失败，无法获取CUDA设备上下文，因此出现了这个bug。

https://github.com/vllm-project/vllm/issues/14163
这个issue是一个Feature需求报告，涉及主要对象为VLLM的prompt adapter加载功能。由于目前VLLM只支持从固定路径读取prompt adapters，用户提出动态加载prompt adapters的需求。

https://github.com/vllm-project/vllm/issues/14162
这是一个bug报告，涉及的主要对象是benchmark_server.py文件。问题是由于async_request_openai_chat_completions检查功能导致'/start_profile'和'/stop_profile'请求无法运行，需要修改请求检查功能以使以'profile'结尾的URL请求有效。

https://github.com/vllm-project/vllm/issues/14161
这是一个类型为bug报告的issue，主要涉及到Github上的V1的Molmo中的`get_multimodal_embeddings()`函数。由于`get_multimodal_embeddings()`返回了一个嵌套列表而不是一个平级列表，当一次请求中包含多个且尾部略有不同的部分时，会触发缓存部分命中的问题。

https://github.com/vllm-project/vllm/issues/14160
这是一个功能需求的issue，主要涉及DeepSeek MTP的多步草稿支持，存在兼容性问题。

https://github.com/vllm-project/vllm/issues/14159
这个issue属于bug报告类型，涉及的主要对象是V1 Runtime Parameters的测试。由于allowed_token_ids采样错误，导致了一些测试失败，需要添加失效性检查和修复。

https://github.com/vllm-project/vllm/issues/14158
这是一个bug报告类型的issue，主要涉及TPU multimodal model的支持，由于缺乏对encoder前向传递的预编译，导致运行时强制编译encoder，可能导致性能问题。

https://github.com/vllm-project/vllm/issues/14157
这是一个bug报告，涉及主要对象是CUDA编译器版本小于12.0的情况，导致CUDA 11.8构建失败并且出现了类型转换不受支持的问题。

https://github.com/vllm-project/vllm/issues/14156
这个issue是一个bug报告，涉及的主要对象是NvmlCudaPlatform中的lru_cache函数。由于lru_cache导致了`get_device_capability`函数的缓存错误，在RayExecutor中引发了错误。

https://github.com/vllm-project/vllm/issues/14155
这是一个代码贡献类的issue，主要对象是对新ragged paged attention Pallas kernel添加注释。

https://github.com/vllm-project/vllm/issues/14154
这是一个Bug报告，涉及主要对象为Vllm（LLL Model），由于GPU当前不支持quantization方法bitsandbytes，导致数值错误导致数值错误。

https://github.com/vllm-project/vllm/issues/14153
该问题属于用户需求类型，主要对象是前端的BatchRequestInput。原因是为了提供更便利的使用体验，增加了默认数值。

https://github.com/vllm-project/vllm/issues/14152
这个issue类型是性能优化，主要涉及V1 Triton（ROCm）后端的性能问题，由于Triton后端性能较差导致总吞吐量比FlashAttention基准低5倍，用户寻求帮助实现性能改进。

https://github.com/vllm-project/vllm/issues/14151
这个issue是一个Bug报告，主要涉及的对象是vLLM的Structured output功能。由于无法取消在构建语法时的相关任务，导致在处理大型json schemas时会出现CPU占用过高、服务器无法处理新请求等问题。

https://github.com/vllm-project/vllm/issues/14150
这个issue是一个Bug报告，涉及的主要对象是qwen2.5vl 3B和qwen2vl 7B模型。由于qwen2.5vl 3B推断时出现OOM，但qwen2vl 7B却没有，可能是由于模型大小和内存消耗之间的关系导致的。

https://github.com/vllm-project/vllm/issues/14149
这个issue是一个功能需求提案，主要涉及vLLM进程与NUMA区域的关联，由于未指定CPU亲和性导致性能问题。

https://github.com/vllm-project/vllm/issues/14148
这个issue为需求提出，主要对象是V1版本的Metrics功能。由于缺少特定的Metrics（Tokens），用户提出了需要添加额外指标的要求。

https://github.com/vllm-project/vllm/issues/14147
这是一个用户提出需求的类型，该问题涉及的主要对象是`TransformersModel`。由于当前文档不够清晰，用户希望改进文档以提高该模块的可见性，并更清楚地解释其支持的内容以及远程模型编写者需要包含的内容。

https://github.com/vllm-project/vllm/issues/14146
这是一个关于软件集成的问题，需要安装FLUTE时需要使用`nodeps`选项。

https://github.com/vllm-project/vllm/issues/14145
这个issue属于bug报告类型，主要涉及了在ROCm上禁用一些内核测试可能导致的问题。

https://github.com/vllm-project/vllm/issues/14144
这是一个特性请求(issue)，主要涉及到代码中使用`np.prod`导致的额外imports和计算开销较高问题。

https://github.com/vllm-project/vllm/issues/14143
该issue是一个bug报告，主要涉及Qwen2VL2B模型在使用`tensorparallelsize=4`时出现的错误，错误原因可能是参数数据的形状不匹配。

https://github.com/vllm-project/vllm/issues/14142
这个issue是关于特性需求的，主要涉及到代码库中的某些操作，提出使用更轻量级的math.prod替代np.prod来减少导入和开销。原因是为了降低轻量级操作时的资源消耗。

https://github.com/vllm-project/vllm/issues/14141
这是一个bug报告，主要涉及对象为`TransformersModel`，由于缺少`head_dim`属性导致该模型在某些情况下运行失败。

https://github.com/vllm-project/vllm/issues/14140
这是一个功能需求类型的issue，主要涉及的对象是在vllm模型中添加了用于fp8稠密模型的DeepGEMM。由于缺少输出路径导致无法保存评估结果，用户希望获得建议。

https://github.com/vllm-project/vllm/issues/14139
这个issue是一个bug报告，主要涉及TransformersModel的使用问题，由于缺少`head_dim`属性导致运行模型时失败。

https://github.com/vllm-project/vllm/issues/14138
这是一个bug报告类型的issue，主要涉及到了gptq/awq marlin kernel的性能优化问题，因为当n较小时，`barrier_acquire`和`barrier_release`开销过大，导致性能下降。

https://github.com/vllm-project/vllm/issues/14137
这是一个Bug报告，涉及到MTP实现的不一致性问题，用户提出了在DeepSeek论文和vllm之间存在的实现矛盾。

https://github.com/vllm-project/vllm/issues/14136
这个issue类型是功能改进，主要涉及到对一些KV/prefix缓存指标的废弃，由于不再实现KV缓存卸载，导致需要废弃一些相关指标并引入新的计数器来取代。

https://github.com/vllm-project/vllm/issues/14135
这是一个关于优化度量指标的issue，主要涉及到vllm中重复的请求时间度量指标。原因是存在重复且数值较为无意义的度量指标，需要进行优化。

https://github.com/vllm-project/vllm/issues/14134
这是一个bug报告，主要涉及的对象是Metrics模块。由于添加`vllm:tokens_total`指标没有正确配置，导致无法在`/metrics`中显示，因此提出了需要移除此指标的问题。

https://github.com/vllm-project/vllm/issues/14132
这是一个Bug报告，主要对象是在GPU上加载14B模型时发生的内核崩溃。可能由于某种原因导致了此问题。

https://github.com/vllm-project/vllm/issues/14131
这个issue是一个功能改进，在vLLM项目中涉及使用`HFCompatibleLoRA`替代实例方法替换，修复了原来打印模型或检查层时返回HFCompatibleX而非原始层类型的问题。

https://github.com/vllm-project/vllm/issues/14130
这是一个bug报告，主要涉及的对象是vllm项目中的处理请求的代码。导致这个bug产生的原因是缺少日志打印导致调试困难。

https://github.com/vllm-project/vllm/issues/14129
这个issue是关于功能需求的，主要涉及的对象是vLLM在ARM CPU上的INT8量化支持。由于vLLM默认情况下采用通道级量化策略，而ACL库不支持通道级量化，导致ACL内核无法在INT8模型上运行。

https://github.com/vllm-project/vllm/issues/14128
这个issue类型属于用户提出需求/问题类型，主要涉及vllm版本升级时精度损失的问题，用户想了解如何确定这些损失是否在可接受范围内。

https://github.com/vllm-project/vllm/issues/14127
该issue类型为用户提出需求，请教问题，主要涉及如何在vllm中运行特定模型的推断集成问题。原因是用户不清楚如何将模型与vllm集成。

https://github.com/vllm-project/vllm/issues/14126
这个issue类型是bug报告，涉及的主要对象是VLLM for Qwen 2.5 72B。由于Quantization和VLLM配置的问题，导致输出始终是特定字符序列，无论输入和提示。

https://github.com/vllm-project/vllm/issues/14125
这是一个功能需求的issue，主要涉及的对象是具有AMD GPU（MI300X）的用户。

https://github.com/vllm-project/vllm/issues/14124
这是一个安装错误的bug报告，主要涉及vllm的安装过程。由于环境或依赖问题导致安装过程中出现错误。

https://github.com/vllm-project/vllm/issues/14123
这是一个bug报告类型的issue，主要涉及"Async run"功能。由于某种原因导致了无法正确执行异步运行的功能。

https://github.com/vllm-project/vllm/issues/14121
这是一个bug报告，主要涉及的对象是运行v1 engine时使用cuda graph时出现错误。由于某些原因导致cuda graph使用错误，从而引发了报错问题。

https://github.com/vllm-project/vllm/issues/14120
这是一个bug报告，涉及主要对象是在使用pipeline parallelism时最后一个GPU始终出现OOM的问题。由于某种原因导致了该GPU内存溢出的bug。

https://github.com/vllm-project/vllm/issues/14119
这是一个新模型支持问题，主要涉及的对象是Phi-4-multimodal-instruct模型。由于未正确设置`maxseqlentocapture=131072`参数，导致vLLM在处理超过8k长度的序列时出现了性能下降问题。

https://github.com/vllm-project/vllm/issues/14118
这是一个bug报告，主要涉及vllm的pipeline_parallel_size和tensor_parallel_size的使用导致prefill功能出现问题。

https://github.com/vllm-project/vllm/issues/14117
这个issue属于bug报告，主要涉及到Pipeline Parallelism参数设置问题，导致了不支持的错误报告。

https://github.com/vllm-project/vllm/issues/14116
这个issue属于功能需求提出类型，主要涉及到如何使用model_redirect来将模型名称重定向到本地文件夹，用户希望能够通过模型名称代替硬编码的模型路径进行本地模型的引用。

https://github.com/vllm-project/vllm/issues/14115
这是一个bug报告，该问题涉及到vLLM中Ovis模型的实现问题，由于Transformers实现不兼容vLLM，导致出现错误信息。

https://github.com/vllm-project/vllm/issues/14114
这个issue属于bug报告，涉及到vllm项目中的outlines engine。导致这个bug的原因可能是structured output实现或者outlines engine本身存在问题，导致生成的tokens不完整。

https://github.com/vllm-project/vllm/issues/14113
这个issue是一个bug报告，主要涉及DeepSeek R1 with outlines structured engine，由于Enum在JSON模式中的存在，在生成token ids后停止生成。

https://github.com/vllm-project/vllm/issues/14112
这是一个用户提出需求的issue，主要涉及vLLM中的Qwen2.5-VL模型是否支持批处理多个视频的问题。用户想要了解Qwen2.5VL是否已经支持批处理多个视频，以及如何启用这一功能，或者是否有计划在未来扩展vLLM以支持批量视频输入。

https://github.com/vllm-project/vllm/issues/14111
这个issue是关于一个bug报告，主要涉及vllm下的分布式推理服务。由于环境配置或操作步骤问题，导致无法成功创建分布式推理服务。

https://github.com/vllm-project/vllm/issues/14110
这是一个Bug报告，涉及主要对象是使用opensouce Qwen2.5 VL72B模型与vllm框架在V100 GPU上部署时出现的持续输出“!”的问题。

https://github.com/vllm-project/vllm/issues/14109
这是一个Bug报告，涉及vLLM引擎启动时GPU资源名称发生变化的问题，可能是由于GPU资源分配错误导致无法启动。

https://github.com/vllm-project/vllm/issues/14108
这是一个Bug报告类型的issue，涉及的主要对象是vllm下的Qwen2.5-vl系列模型部署。由于Engine进程异常终止，导致出现调用卡死的问题和日志报错。

https://github.com/vllm-project/vllm/issues/14107
这是一个用户提出需求的类型的issue，涉及的主要对象是在使用speculative decoding时指定设备的参数。由于部署在特定设备上的speculative模型可能会减少interGPU通信并减少对主模型的影响，因此提出了这一需求。

https://github.com/vllm-project/vllm/issues/14106
这是一个bug报告，涉及的主要对象是deepseek_v2模块。这个问题可能是由于代码中将routed_scaling_factor设置了两次而导致的。

https://github.com/vllm-project/vllm/issues/14105
这个issue是关于无法支持某一模型格式的bug报告，涉及的主要对象是深度搜索AI小型模型。原因可能是缺少支持配置文件或格式不匹配。

https://github.com/vllm-project/vllm/issues/14104
这是一个用户提出需求的issue，主要对象是baichuaninc/BaichuanOmni1.5模型的支持，原因是请求该模型的集成。

https://github.com/vllm-project/vllm/issues/14103
这个issue是关于bug报告，主要涉及的对象是在A100上无法启动deepseek-vl2。由于PyTorch版本与CUDA版本不匹配，导致无法正常启动。

https://github.com/vllm-project/vllm/issues/14102
这是一个bug报告，主要涉及对象是VLLM模型的max_model_len设置失败。由于KV内存的限制导致设置的参数无效，使得模型的max seq len超过了KV缓存中可以存储的最大令牌数。

https://github.com/vllm-project/vllm/issues/14101
这个issue是一个功能增强的类型，主要涉及到前端代码中的KV缓存数据统计，由于未提供与`lmcache`指标的集成，导致无法获取完整的统计数据。

https://github.com/vllm-project/vllm/issues/14100
这个issue是一个bug报告，主要涉及的对象是DeepseekV2MLP模块。导致这个bug的原因是忘记将前缀传递到DeepseekV2MLP用于shared_experts。

https://github.com/vllm-project/vllm/issues/14099
这是一个bug报告，主要涉及驱动程序环境变量未传递给ray工作者造成了错误和混淆的问题。

https://github.com/vllm-project/vllm/issues/14098
这是一个需求类的issue，涉及主要对象是model runner。由于kv_caches在model中不再需要，导致需要对代码进行调整以删除不必要的代码。

https://github.com/vllm-project/vllm/issues/14097
这是一个实现功能的issue，主要涉及kv_cache_manager中的sliding window attention功能。原因是要支持真正的滑动窗口以及前缀缓存需求。

https://github.com/vllm-project/vllm/issues/14096
这是一个Bug报告，涉及到VLLM中的CPU offload功能出现问题，导致了代码运行时的错误。

https://github.com/vllm-project/vllm/issues/14095
这个issue属于bug报告，主要涉及的对象是vllm服务的运行过程。由于PyTorch版本为2.5.1+cu124，导致在处理"determine_num_available_blocks"方法时出现异常。

https://github.com/vllm-project/vllm/issues/14094
这是一个bug报告，涉及到vllm中的MistralTokenizer，由于不支持`skip_special_tokens=False`参数导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/14093
这是一个Bug报告，涉及VLLM中的Ray模块无法在同一主机上进行pipeline并行处理的问题。可能由于在同一机器上存在的两个GPU之间的intergpu通信问题导致了NCCL错误和模型加载失败。

https://github.com/vllm-project/vllm/issues/14092
这个issue是关于bug报告，主要涉及VllmWorkerProcess，在运行DeepSeek-R1:70B时出现异常，可能是由于PyTorch版本与CUDA版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/14091
这是一个需求提出类型的issue，主要涉及的对象是`tests/distributed/test_comm_ops.py`文件中的测试用例。由于目前测试用例只支持CUDA设备，未能兼容其他设备，用户提出需要使测试用例兼容更多设备。

https://github.com/vllm-project/vllm/issues/14090
这是一个关于vLLM安装问题的bug报告，涉及vLLM在特定环境下无法找到支持的配置格式导致的数个模型无法运行的问题。

https://github.com/vllm-project/vllm/issues/14089
这是一个用户提出需求的类型的issue，主要涉及到vllm中的Speculative decoding与Pipeline Parallelism的兼容性问题，用户想要在R1上同时运行PP和MTP，可能由于某些原因导致无法实现。

https://github.com/vllm-project/vllm/issues/14088
这个issue是关于功能（Feature）的，主要对象是前端（Frontend），提出了是否要废弃`--enable-reasoning`选项的讨论。

https://github.com/vllm-project/vllm/issues/14087
这是一个用户提出需求的issue，主要涉及对象是如何在仅有CPU的环境下运行DeepSeek-R1-Distill-Qwen-32B模型作为离线批处理推理，并需要提供Hugging Face cache directory、Hugging Face token以及max tokens等参数。这个问题的症状是用户不清楚如何在LLM类或其初始化函数中设置这些参数。

https://github.com/vllm-project/vllm/issues/14086
这个issue类型是文档改进，主要涉及源代码构建的克隆步骤，用户提出了对构建步骤的补充。

https://github.com/vllm-project/vllm/issues/14085
该issue类型为功能增强，主要涉及添加Neuron设备通信功能。

https://github.com/vllm-project/vllm/issues/14084
该issue类型为文档更新，主要涉及到Kubernetes部署指南的修改。由于前文提到的问题，导致了一些具体项目的引用被移除，以及一些观点性内容的删除，需要更新为更中立和官方的指南。

https://github.com/vllm-project/vllm/issues/14083
这是一个用户提出需求的issue，主要涉及改善VLLM V1在初始化过程中出现常见错误的日志记录，由于当前异常的日志记录不够清晰，希望能够更明确地捕获这些问题并返回更清晰的错误消息。

https://github.com/vllm-project/vllm/issues/14082
这是一个关于提出需求（feature request）的issue，主要涉及StatsLogger接口的更新和日志统计任务的优化。

https://github.com/vllm-project/vllm/issues/14081
这是一个bug报告，涉及的主要对象是KVCacheBlock。问题由于KVCacheBlock作为一个双向链表，在默认打印时会递归打印所有KVCacheBlock对象，可能导致堆栈溢出，建议打印`prev_block_id`和`next_block_id`来避免这一问题。

https://github.com/vllm-project/vllm/issues/14080
这是一个用户提出需求的类型，主要涉及VLLM系统中的预填响应功能。用户希望能够通过预填响应来控制生成的输出。

https://github.com/vllm-project/vllm/issues/14079
这是一个需求变更的issue，涉及主要对象为KVCacheConfig的重构，由于需要改变KVCacheSpec的含义以及调整KVCacheConfig的保存逻辑，导致需要对KVCacheConfig进行重新设计和调整。

https://github.com/vllm-project/vllm/issues/14078
这是一个需求提出类型的issue，主要对象是vllm库中的LogitsProcessor，由于无法获取每个序列的唯一标识，导致无法在自定义LogitsProcessor中维护每个序列的状态。

https://github.com/vllm-project/vllm/issues/14077
这是一个关于文档更新的issue，涉及主要对象是使用 reasoning model with stream 的用户，提出了关于如何在OpenAI库中使用的问题。

https://github.com/vllm-project/vllm/issues/14076
这是一个bug报告，用户在启动过程中遇到了错误。

https://github.com/vllm-project/vllm/issues/14075
这是一个bug报告，涉及到在使用docker环境下运行vllm时出现错误。原因可能是docker环境与conda环境之间的差异导致的。

https://github.com/vllm-project/vllm/issues/14074
这个issue类型是bug报告，主要涉及的对象是使用 Qwen2.5-7B 模型和 OpenVINO 后端的 vLLM。导致中文响应中出现乱码的原因是什么？

https://github.com/vllm-project/vllm/issues/14073
这是一个bug报告，主要涉及对象是`cache_full_blocks`函数，导致该bug的原因是传递给该函数的`num_cached_blocks`参数不正确。

https://github.com/vllm-project/vllm/issues/14072
这个issue类型为用户提出需求，并询问vLLM MLA使用的缓存类型是KV Cache还是C Cache；主要对象是vLLM MLA的缓存系统；用户问题可能是由于缓存系统不清楚或文档不明确而引发的。

https://github.com/vllm-project/vllm/issues/14071
这是一个用户提出需求的issue，主要涉及V1版本的TritonAttention在Nvidia GPU上的支持。由于V1 ROCm attention后端已实现为Triton，在Nvidia GPU上也可以用于triton内核开发。

https://github.com/vllm-project/vllm/issues/14070
这个issue是关于文档更新的，主要涉及对象是论据输出流示例。由于DeltaMessage不支持自定义字段，之前使用requests而非OpenAI客户端，现有贡献者建议使用OpenAI客户端来解决这个问题。

https://github.com/vllm-project/vllm/issues/14069
这是一个Bug报告，涉及MLA模型在运行过程中出现了运行时错误，原因是启用了前缀缓存但禁用了分块预填充功能。

https://github.com/vllm-project/vllm/issues/14068
这是一个功能需求的issue，主要涉及的对象是模型调优过程中的fp8块量化支持。由于当前实现的fp8调优不考虑块量化，导致在调优DeepSeekV3模型时未考虑重量尺度，这个问题提出了添加对块量化支持的需求。

https://github.com/vllm-project/vllm/issues/14067
这是一个bug报告，涉及主要对象是vLLM中的内存分析程序。这个问题由于draft model weights未被计入内存分析导致内存不稳定，可能导致OutOfMemory错误。

https://github.com/vllm-project/vllm/issues/14066
这是一个用户提出需求的issue，主要涉及前端（Frontend）功能的改进。由于需求不同，希望能够通过请求参数来控制token显示类型，以适应不同情况的服务需求。

https://github.com/vllm-project/vllm/issues/14065
这是一个功能需求的issue，主要涉及的对象是ROCmAttentionBackend类。原因是V1 GPUModelRunner支持其他元数据结构，需要添加get_builder_cls方法。

https://github.com/vllm-project/vllm/issues/14064
这个issue类型为功能需求提出，涉及主要对象为vllm的release tag，用户提出了通过RELEASE.md来触发带有``vX.Y.Zrc``版本号的release构建。

https://github.com/vllm-project/vllm/issues/14063
这是一个功能需求类型的issue，主要涉及模型加载权重的时间准确记录，因为生成器的惰性特性导致下载阶段与加载权重阶段的时间精确统计存在困难。

https://github.com/vllm-project/vllm/issues/14062
这是一个bug报告，涉及的主要对象是在AMD MI300X平台上运行的AlibabaNLP/gteQwen27Binstruct模型。原因是在AMD gpu上进行推理时遇到了问题，导致无法成功完成第一次推理。

https://github.com/vllm-project/vllm/issues/14061
这是一个bug报告，主要涉及FlashInfer attention backend的实现细节，包括对Cascade attention逻辑是否有效以及无法设置kv cache dtype的限制。

https://github.com/vllm-project/vllm/issues/14060
这个issue是bug报告，涉及的主要对象是ModelScope下载功能，由于缺乏文件锁导致可能出现的并发下载问题。

https://github.com/vllm-project/vllm/issues/14059
这个issue是关于缺少某个参数的文档内容更新，属于文档改进类型，主要涉及的对象是优化文档。这个问题可能由于缺乏`pipeline_parallel_size`参数的文档说明，导致用户无法了解该参数的使用和作用。

https://github.com/vllm-project/vllm/issues/14058
这是一个bug报告，主要涉及使用vllm==0.7.3进行parallel inference时出现garbage outputs的问题。可能是由于版本不匹配或其他环境配置问题导致的。

https://github.com/vllm-project/vllm/issues/14057
这是一个用户提出需求的issue，主要涉及的对象是DeviceCommunicatorBase。由于需要将reduce_scatter功能从一个特定的组件中分离出来，用户认为这是一个很好的改进建议。

https://github.com/vllm-project/vllm/issues/14056
该issue属于需求提出类型，主要对象是为GPTQModel添加文档。这可能是因为用户希望将GPTQModel作为ModelCloud.AI用户完全支持的另一种GPTQ模型量化选项。

https://github.com/vllm-project/vllm/issues/14055
这个issue是关于增加指标支持，属于功能增强类型，主要涉及到V1版本的metrics实现。该功能增强的原因是为了实现max_num_generation_tokens、request_params_n和request_params_max_tokens这些指标的监控。

https://github.com/vllm-project/vllm/issues/14054
这是一个bug报告，涉及的主要对象是Llama chat模板。该bug的原因是Llama chat模板无法处理`tool_calls`字段设置为空列表的情况，导致程序出现错误。

https://github.com/vllm-project/vllm/issues/14053
这是一个Bug报告，主要涉及V1 MultiprocExecutor中的shutting_down flag检查错误导致终止工作进程时出现问题。

https://github.com/vllm-project/vllm/issues/14052
这是一个bug报告，涉及主要对象为entrypoint tests for embedding models。由于模型更新导致了embedding size和token counts的变化，可能引起了测试失败以及关于GPU并发延迟的疑问。

https://github.com/vllm-project/vllm/issues/14051
这是一个关于bug修复（Bugfix）类型的issue，主要涉及 MacOS 系统环境下安装失败的问题。原因是缺少 "omp.h" 头文件导致 OpenMP 相关功能无法正常使用。

https://github.com/vllm-project/vllm/issues/14050
这个issue类型是用户提出需求，请求功能添加，主要涉及的对象是文档整理和示例代码。

https://github.com/vllm-project/vllm/issues/14049
这个issue是关于代码修改建议的，主要涉及到`InputPreprocessor`和`MultimodalProcessor`，可能是为了优化代码结构而提出的问题。

https://github.com/vllm-project/vllm/issues/14048
这是一个bug报告，主要涉及前端的改进关机和日志功能，由于错误处理不当导致无法正确关闭后台进程、无法清理资源以及不能向客户端返回正确的状态码。

https://github.com/vllm-project/vllm/issues/14047
这是一个关于改进功能的issue，主要涉及到对多模态内容格式的全面交错支持。由于当前方法导致了在'string'聊天模板内容格式中所有图像标记都堆叠在提示的开头，可能会对某些模型的性能产生负面影响，因此用户提出了需要开启全面交错支持的选项。

https://github.com/vllm-project/vllm/issues/14046
这是一个bug报告，主要涉及V1版本中警告未实现方法时出现误报的问题。

https://github.com/vllm-project/vllm/issues/14045
这是一个功能需求类型的issue，主要对象是在VLLM项目中添加 likaixin/InstructCoder 作为 spec decode 基准数据集选项。这个需求由于希望添加新的数据集以用于性能基准测试，并优化 Triton Kernels 上的 rejection sampler。

https://github.com/vllm-project/vllm/issues/14044
这个issue类型是用户提出需求，主要涉及的对象是Deepseek的部署配置。由于block_size无法被32整除，导致在32个GPU上启用PP后不支持speculative decoding。

https://github.com/vllm-project/vllm/issues/14043
这是一个bug报告类型的issue，主要涉及添加更多数据类型支持到GGUF内核，导致现有测试用例中的bfloat16数据类型存在问题。

https://github.com/vllm-project/vllm/issues/14042
这是一个文档更新类型的issue，主要涉及"AutoAWQ"文档的更新。由于之前的修改未能完全覆盖到指定的更改内容，因此需要进一步更新文档。

https://github.com/vllm-project/vllm/issues/14041
这个issue属于bug报告类型，主要涉及文档链接指向已删除的分支，由于这个问题导致链接失效，需要进行修复。

https://github.com/vllm-project/vllm/issues/14040
这是一个bug报告，主要涉及vllm库在Google Colab T4 GPU上出现Quantization scheme不受支持的错误。

https://github.com/vllm-project/vllm/issues/14039
这是一个bug报告，该问题涉及的主要对象是无法在服务器上使用最新的docker镜像运行DeepSeek-R1。由于服务器的操作系统只是一个"Docker runner"，导致无法运行相应的脚本。

https://github.com/vllm-project/vllm/issues/14038
这是一个bug报告，主要涉及VLM中指定提示目标的问题，导致Molmo出现意外结果和替换错误的情况。

https://github.com/vllm-project/vllm/issues/14037
这是一个Bug报告，主要涉及Phi4-mini-instruct在进行连续批处理时产生随机输出的问题，猜测可能是由于使用不同版本的VLLM导致的。

https://github.com/vllm-project/vllm/issues/14036
该issue类型为需求提出，主要涉及性能基准数据集的整合，并说明了如何更新和优化数据集以及相关的代码文件。

https://github.com/vllm-project/vllm/issues/14034
这个issue类型是bug报告，涉及到无法在macOS上找到OpenMP头文件的问题，可能是由于环境配置不正确导致的。

https://github.com/vllm-project/vllm/issues/14033
这是一个bug报告，涉及到vLLM的Dockerfile.cpu安装问题。由于安装过程中出现了错误，可能是由于Dockerfile配置或环境设置问题导致。

https://github.com/vllm-project/vllm/issues/14032
这是一个bug报告类型的issue，涉及VLLM中的EP功能和chunked-prefill功能。由于配置中出现了问题，用户无法确认EP是否正确启用，并发现chunked-prefill-enablement的状态不符合预期。

https://github.com/vllm-project/vllm/issues/14031
这个issue属于bug报告类型，涉及的主要对象是vLLM中的QwenVL Series，由于attention bias没有在与Query/Key/Value张量相同的设备上初始化，导致了cuda设备不匹配的问题。

https://github.com/vllm-project/vllm/issues/14030
这是一个需要修改xgrammar版本的issue，由于需要在aarch64系统上正确运行guided decoding worker与xgrammar，因此需要在合并此PR后修改xgrammar版本。

https://github.com/vllm-project/vllm/issues/14029
这是一个bug报告类型的issue，涉及到Github上的vllm的一个文档，报告的问题是关于链接错误导致的无效链接。

https://github.com/vllm-project/vllm/issues/14028
这是一个用户提出需求的类型，该问题涉及的主要对象是使用vllm部署DeepSeek模型的输出缺失<think>标签问题。可能由于集成vllm时的问题导致这种症状的bug。

https://github.com/vllm-project/vllm/issues/14027
这个issue属于bug报告类型，主要涉及到VLLM下的DeepSeek V2/V3模型中MoE路由器的精度不匹配问题，导致了数值稳定性不佳的bug。

https://github.com/vllm-project/vllm/issues/14026
这是一个功能需求报告，主要涉及benchmark_serving.py中的随机输入和sharegpt数据集。由于随机输入在VLLM客户端和SGLang客户端之间存在差异，导致某些模型表现不佳，用户提出了添加新功能以解决这一差距。

https://github.com/vllm-project/vllm/issues/14025
这是一个Bug报告，涉及的主要对象是MQLLMEngine。由于长时间等待输出而导致程序最终崩溃，可能由于某种原因导致Hangs和Crash。

https://github.com/vllm-project/vllm/issues/14024
这是一个关于bug修复的issue，主要涉及到MoeWNA16Method激活的问题。原因可能是由于激活方法存在bug导致无法正常工作。

https://github.com/vllm-project/vllm/issues/14023
该issue是关于性能优化的需求，主要涉及vLLM下Qwen-0.5B模型在Nvidia H20 GPU上生成吞吐量不佳的问题。

https://github.com/vllm-project/vllm/issues/14022
这是一个关于低GPU利用率的bug报告，用户在运行DeepSeekR1DistillLlama8B时GPU利用率只有20%，希望得到帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/14021
这是一个bug报告类型的issue，主要涉及vllm在推理性能上表现较弱的问题。可能由于不同版本之间的差异导致了这种情况，用户希望通过该issue了解如何集成特定模型进行推理。

https://github.com/vllm-project/vllm/issues/14020
这个issue类型是功能需求，主要对象是benchmark_serving.py文件。由于在比较在线服务性能时观察到随机输入的差异，用户提出了添加来自sharegpt数据集的随机输入以尽量缩小这种差距的需求。

https://github.com/vllm-project/vllm/issues/14018
这是一个Bug报告类型的Issue，主要涉及到vllm 0.7.3版本安装flash-attn或flash-attn时启动容器出现错误的问题，可能是由于安装过程中的某些错误导致的。

https://github.com/vllm-project/vllm/issues/14017
这个issue属于用户提出需求，主要涉及的对象是移动多模态嵌入API示例到在线服务页面。原因是为了消除当前在多模态输入页面中列出的与嵌入输入相关的混乱。

https://github.com/vllm-project/vllm/issues/14016
这是一个bug报告，主要涉及的对象是在2080ti上无法部署DeepseekV3ForCausalLM模型，可能由于环境配置不正确导致。

https://github.com/vllm-project/vllm/issues/14015
这是一个bug报告，涉及到Paligemma的多模态处理，由于HF编码的prompt添加了反斜杠符号，导致不能识别语言，需要修复这个问题。

https://github.com/vllm-project/vllm/issues/14014
这个issue是一个关于Docker release的工作进行中的问题，属于功能需求类别，涉及的主要对象是项目中的Docker部署流程。

https://github.com/vllm-project/vllm/issues/14013
这是一个用户提出需求的issue，主要对象是需要在基准测试中添加一个与代码相关的数据集，以展示ngram speculative decoding的主要用例和优势。

https://github.com/vllm-project/vllm/issues/14012
该issue类型为用户提出需求，主要涉及对象为将KVCache存储在3FS中。由于没有收到额外的上下文信息，用户提出了将KVCache存储在3FS中的新功能需求。

https://github.com/vllm-project/vllm/issues/14011
这是一个bug报告类型的issue，主要涉及的对象是vllm项目中的环境变量。由于内存使用量异常导致45 GiB vs 40 GiB的差异，用户提出是否会导致OOM问题。

https://github.com/vllm-project/vllm/issues/14010
这个issue类型是bug报告，主要涉及对象是代码中的`use_all_gather`和`sampling_metadata.seq_groups`。由于在使用allgather时，导致`_apply_logits_processors`出现错误，需要对`sampling_metadata.seq_groups`进行检查以避免这种情况。

https://github.com/vllm-project/vllm/issues/14009
这个issue是关于bug报告，主要涉及vLLM项目中的一个AssertionError，由于`prefill_metadata.context_chunk_seq_tot`为None导致。

https://github.com/vllm-project/vllm/issues/14008
这是一个bug报告，主要对象是vllm库，由于在主解释器的主线程中才能使用signal，导致出现数值错误（ValueError）。

https://github.com/vllm-project/vllm/issues/14007
这是一个特性增强（FEAT）类型的issue，主要涉及到vllm中的AITER核的启用。

https://github.com/vllm-project/vllm/issues/14006
这是一个bug报告，涉及vllm在docker容器上运行性能不稳定的问题，用户怀疑与Paged Attention的数值缓存有关。

https://github.com/vllm-project/vllm/issues/14005
该issue是关于用户需求的，主要对象是DeepSeek v3/r1 MTP支持PP功能，用户询问是否有计划在MTP上支持PP，尚未得到回应。

https://github.com/vllm-project/vllm/issues/14004
这是一个用户需求类型的issue，主要涉及到vllm是否支持在多节点环境下使用前缀缓存的功能。原因可能是用户想要在多节点中推理DeepSeekR1671B时如何共享kv缓存。

https://github.com/vllm-project/vllm/issues/14003
这个issue类型是用户提出需求，主要涉及的对象是V1 Engine，用户提出了关于实现并发部分预填的需求。

https://github.com/vllm-project/vllm/issues/14002
该issue属于需求提出类型，主要涉及实现V1引擎中的优先级调度功能，用户希望在V1版本中支持请求优先级，由于当前版本（V0）尚未支持此功能，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/14001
这个issue类型是功能需求，涉及主要对象是在ROCm环境下启用 AITER ASM MoE。

https://github.com/vllm-project/vllm/issues/14000
这是一个关于优化建议的类型，涉及的主要对象是代码审查工具 codespell 缓存未正确生成，导致某些拼写错误未被预设的代码检查挂钩捕获。

https://github.com/vllm-project/vllm/issues/13999
这个issue类型是用户提出需求，主要对象是vllm0.7.3版本内置的pytorch不支持nvidia5080，导致无法在本地部署deepseek14b。

https://github.com/vllm-project/vllm/issues/13998
这个issue类型是用户提出需求，该问题单涉及的主要对象是使Deepseek在ROCm平台上启用AITER ASM MoE功能。

https://github.com/vllm-project/vllm/issues/13997
这个issue属于功能变更请求，主要涉及的对象是vLLM中的`best_of`抽样参数。由于用户和行业对此功能的使用逐渐减少，导致该功能在即将发布的vLLM V1版本中被取消支持。

https://github.com/vllm-project/vllm/issues/13996
这是一个关于集成DeepGEMM dense block fp8的问题，主要涉及Kernel集成和性能比较，由于性能不如预期，作者不确定如何分发DeepGEMM。

https://github.com/vllm-project/vllm/issues/13995
这个issue是关于文档说明的，用户提出了一个问题或者寻求帮助。

https://github.com/vllm-project/vllm/issues/13994
这是一个bug报告，主要涉及Ray的版本升级和使用Compiled Graph过程中出现的错误。

https://github.com/vllm-project/vllm/issues/13993
这是一个用户提出需求的issue，主要涉及添加`vllm bench` CLI命令。由于目前仅支持`vllm bench serve`，用户希望在后续的PR中添加其他benchmark模式。

https://github.com/vllm-project/vllm/issues/13992
这个issue属于bug报告，主要涉及了Speculative Decoding Tokens未被包含在Prometheus指标中，可能是由于代码中的错误导致了该问题。

https://github.com/vllm-project/vllm/issues/13991
这是一个用户提出需求的issue，主要涉及的对象是V1用户指南。

https://github.com/vllm-project/vllm/issues/13990
这是一个需求讨论的issue，主要涉及V1 rejection sampler中的acceptance rate, accepted tokens等指标的定义和跟踪。

https://github.com/vllm-project/vllm/issues/13989
这是一个Bug报告，涉及的主要对象是在加载Deepseek R1模型时GPU内存分配停滞的情况。可能由于某些原因导致GPU内存在特定数值时停止增长。

https://github.com/vllm-project/vllm/issues/13988
该issue是关于需求提出类型，主要涉及的对象是TPU worker。由于TPU profiler在长时间的 profiling 过程中会丢失部分数据，用户提出了需要实现`start_profile`和`stop_profile`方法来控制 profiling 会话以及在benchmark_serving.py中启用 profiling 的需求。

https://github.com/vllm-project/vllm/issues/13987
该issue类型是bug报告，主要涉及的对象是vllm中的SimpleConnector中的decode节点，由于inflight batching导致了hangs in SimpleBuffer（nccl based）。

https://github.com/vllm-project/vllm/issues/13986
这是一个用户提出需求的issue，主要涉及更新XGrammar至0.1.14版本。原因是用户想在生产环境中使用更加功能丰富的CFG语法。

https://github.com/vllm-project/vllm/issues/13985
这个issue是一个bug报告，主要涉及到V1版本中Attention后端重复打印的问题，由于CC([Attention] MLA支持V1)的改动导致打印日志重复两次，该问题在PR中被修复。

https://github.com/vllm-project/vllm/issues/13984
这是一个用户提出需求的类型的issue，涉及主要对象是添加AMD Quark文档。由于目前缺少AMD Quark文档，用户希望能够补充相关内容。

https://github.com/vllm-project/vllm/issues/13983
该issue类型为代码优化建议，主要涉及的对象是GPU Model Runner的代码。由于存在冗余的类型注解，用户建议进行清理以提升代码质量和可读性。

https://github.com/vllm-project/vllm/issues/13982
这是一个功能需求的issue，主要涉及支持TPU中的标准V1 Sampler。由于topk/topp在TPU上运行缓慢，需要找到更快的方法来执行抽样操作。

https://github.com/vllm-project/vllm/issues/13981
这是一个RFC（Request For Comments）类型的issue，涉及到需要删除支持prompt adapter功能的代码。原因是为了进行代码清理。

https://github.com/vllm-project/vllm/issues/13980
这是一个Bug报告类型的Issue，主要涉及Vllm在部署过程中出现了数值错误的问题。原因是配置文件中指定的cudagraph大小与模型运行器指定的不一致，导致了无法支持的层使用AWQMarlin而产生错误。

https://github.com/vllm-project/vllm/issues/13979
这是一个需求提出的issue，主要涉及的对象是VLM（Visual Language Modeling）模型的legacy输入映射器。

https://github.com/vllm-project/vllm/issues/13978
这是一个bug报告，涉及Llama chat template无法处理tool_calls为[]的问题。由于chat template要求每个带有tool_calls属性的消息的长度必须为1，当传递空列表时会导致异常。

https://github.com/vllm-project/vllm/issues/13977
这是一个用户提出需求的issue，涉及的主要对象是BlockTable类。由于需要准备混合内存分配器以支持混合模型，希望通过移除特定参数和调整参数顺序来优化BlockTable类的操作。

https://github.com/vllm-project/vllm/issues/13976
这是一个BugFix类型的issue。该问题涉及到vllm项目中的preprocess input步骤。由于只捕获了`ValueError`异常，而未捕获其他异常（如jinja2模板错误、多模态错误），导致在前端openai端点中发生错误时，没有正确返回错误信息，而是返回了`500 Internal Server Error`。

https://github.com/vllm-project/vllm/issues/13975
该issue属于需求提出类型，涉及的主要对象是ROCm的AITER kernels。

https://github.com/vllm-project/vllm/issues/13974
这是一个bug报告，该问题涉及的主要对象是Print FusedMoE detail info功能。由于还未提供具体细节，暂时无法确定导致什么样的bug或用户问题。

https://github.com/vllm-project/vllm/issues/13973
这是一个API优化类型的issue，涉及主要对象为`KVCacheManager`，旨在减少代码重复，为新的KVCacheManager引入混合模型准备。

https://github.com/vllm-project/vllm/issues/13972
这是一个提出新功能需求的issue，主要对象是 CUTLASS grouped gemm fp8 MoE kernel实现。由于用户需要在Benchmark中测试该功能的性能表现，于是提出了这个issue。

https://github.com/vllm-project/vllm/issues/13971
这个issue类型为技术改进，涉及主要对象为更新Python 3.8的typing使用。

https://github.com/vllm-project/vllm/issues/13970
这是一个与AMD CI（Continuous Integration）相关的bug报告类型的issue，主要涉及的对象是针对Kernels、Core和Prefix Caching的测试。该问题由于测试未经AMD平台适配而导致部分测试无法通过，影响AMD CI的完整性。

https://github.com/vllm-project/vllm/issues/13969
这是一个 bug 报告，针对 vllm >= 0.7.2 版本出现的问题。用户遇到与已关闭 issue 相同的 bug，并请求解决。

https://github.com/vllm-project/vllm/issues/13968
这个issue类型是bug报告，主要涉及的对象是Qwen2.5VL3B。由于float16数据类型导致Qwen2.5VL3B在ViT编码器中发生溢出，需要手动转换溢出值来解决此问题。

https://github.com/vllm-project/vllm/issues/13967
这个issue是一项改进提议，主要涉及到`.pre-commit-config.yaml`文件中的重复项处理。原因是替换每个`repo`内的`exclude`为顶层的`exclude`。

https://github.com/vllm-project/vllm/issues/13966
这是一个Bug报告，涉及到VLLM中的Ultravox 0.5在线模式在音频限制大于1时崩溃的问题。

https://github.com/vllm-project/vllm/issues/13965
这个issue类型是bug报告，涉及的主要对象是vllm。由于标题为空，导致了无法准确描述问题或需求。

https://github.com/vllm-project/vllm/issues/13964
这个issue是一个功能提升类的问题，主要涉及VLM（Visual Language Model）项目中的PromptUpdate功能更新。

https://github.com/vllm-project/vllm/issues/13963
这是一个功能需求的issue，主要涉及支持Triton中的非注意力路径操作，包括自定义操作和非自定义操作。

https://github.com/vllm-project/vllm/issues/13962
这是一个bug报告，涉及到VLLM_USE_MODELSCOPE和--tensor-parallel-size > 1时，VLLM下载模型多次导致启动缓慢和失败。

https://github.com/vllm-project/vllm/issues/13961
这是一个Bug报告，涉及的主要对象是程序在初始化TMA描述符700时出现错误，导致无法运行。

https://github.com/vllm-project/vllm/issues/13960
这是一个关于用户需求的issue，主要涉及使用DeepSeek MTP针对Distil模型进行推理的问题，用户不清楚如何将其与vllm集成。

https://github.com/vllm-project/vllm/issues/13959
该issue属于功能需求的提出，涉及对象为vLLM项目中的模型定义。由于部分模型不兼容vLLM V1版本，需要添加`SupportsV0Only`协议以便可以程序化检查兼容性。

https://github.com/vllm-project/vllm/issues/13958
这个issue类型为改进建议，主要对象是项目的快速入门文档，由于缺少对`uv run`的使用说明，可能导致用户误解创建环境的操作方式。

https://github.com/vllm-project/vllm/issues/13957
这个issue是一个bug报告，主要涉及到的对象是Engine iteration，由于Engine iteration timeout，导致了该bug的出现。

https://github.com/vllm-project/vllm/issues/13956
这是一个Bug报告，主要涉及的对象是vllm的multi-card deployment过程。由于ZMQError导致了"Operation not supported"的错误。

https://github.com/vllm-project/vllm/issues/13955
这是一个用户提出需求的issue，主要涉及前端相关内容。由于尚未支持图片嵌入，用户提出希望在OpenAI API中支持图片嵌入的功能。

https://github.com/vllm-project/vllm/issues/13954
这是一个bug报告类型的issue，主要涉及 vllm 在启用前缀缓存时崩溃的问题。由于该问题导致第二个请求返回空，可能是由于前缀缓存功能引起的bug。

https://github.com/vllm-project/vllm/issues/13953
该issue为需求类型，主要涉及添加Jump-Forward支持到Guided Decoding，由于当前使用FSM在进行引导解码时只能一次转换一个标记状态，导致解码速度慢，作者希望引入JumpForward解码以提高速度。

https://github.com/vllm-project/vllm/issues/13952
This issue is a bug report related to the repeated inference output from the vllm library.

https://github.com/vllm-project/vllm/issues/13951
这是一个Bug报告，涉及的主要对象是OpenAI的tokenization endpoint。由于未正确处理输入消息导致出现AttributeError，用户提出需要帮助解决此问题。

https://github.com/vllm-project/vllm/issues/13950
这是一个功能需求的issue，主要涉及前端实时请求并发跟踪，用户提出了关于跟踪服务负载的需求。

https://github.com/vllm-project/vllm/issues/13949
这个issue是有关bug报告类型，主要涉及vLLM v1引擎实例的APC兼容性问题，由于APC必须禁用来支持启用提示logprobs的请求，导致相关问题的产生。

https://github.com/vllm-project/vllm/issues/13948
这是一个关于bug报告的issue，涉及vllm项目中的命令行参数的问题。

https://github.com/vllm-project/vllm/issues/13947
这是一个功能需求类型的issue，主要涉及VLLM_TEST_ENABLE_EP环境变量的处理方式。

https://github.com/vllm-project/vllm/issues/13946
这是一个Bug报告类型的issue，主要涉及LLM在使用Docker部署时出现的错误。由于无法推断设备类型导致的RuntimeError错误。

https://github.com/vllm-project/vllm/issues/13945
这是一个bug报告，主要涉及的对象是RayDistributedExecutor。由于未将环境变量`VLLM_TORCH_PROFILER_DIR`传递给所有的worker，导致即使设置了环境变量也会出现错误，用户提出需要修复这一问题。

https://github.com/vllm-project/vllm/issues/13944
这是一个需求提议，主要涉及的对象是CI/Build系统。由于需要在触及examples目录的PR上添加标签，开发者提议将该目录标注为可通过`mergify`进行标记，以增强PR的可管理性。

https://github.com/vllm-project/vllm/issues/13943
这个issue类型是用户提出需求，涉及的主要对象是vLLM模型定义。由于需要检查模型与vLLM V1的兼容性，因此提出了添加`SupportsV1`协议的需求。

https://github.com/vllm-project/vllm/issues/13942
这个issue属于功能需求提出，主要涉及的对象是模型的量化支持，用户提出了需要支持 deepseek v3/r1 w8a8 int8 block-wise quantization 的需求。

https://github.com/vllm-project/vllm/issues/13941
这个issue是一个Bug报告，涉及的主要对象是vllm中的OOM（Out of Memory）问题。由于问题环境中使用了较大的模型以及GPU内存，导致了OOM错误。

https://github.com/vllm-project/vllm/issues/13940
这是一个用户提出需求的 issue，主要是希望为vllm创建一个特定的Docker镜像，包括`audio`和`video`功能，以便用户可以直接从Docker Hub拉取使用。

https://github.com/vllm-project/vllm/issues/13939
这是一个bug报告，问题涉及到在运行过程中遇到了CUDA错误导致的RuntimeError。

https://github.com/vllm-project/vllm/issues/13938
这是一个功能需求的issue，主要对象是vllm命令行工具，需添加一个帮助命令以列出可用的CLI命令及其简要说明。

https://github.com/vllm-project/vllm/issues/13937
这是一个bug报告，主要涉及vLLM中使用speculative decoding时加载Hugging Face repository出错的问题，可能是由于vLLM将speculative model的路径视为文件而未正确获取导致的。

https://github.com/vllm-project/vllm/issues/13936
这是一个用户提出需求的issue，主要涉及到对新模型Phi-4 Multimodal Instruct的支持。由于开发者没有回应，用户想知道为什么无法支持他们想要的模型。

https://github.com/vllm-project/vllm/issues/13935
这个issue属于bug报告类型，主要涉及的对象是vLLM中的Ray版本和APIs。这个问题是由于Ray 2.42版本的一个bug导致的，影响了PP功能，因此需要等待Ray 2.43版本修复问题。

https://github.com/vllm-project/vllm/issues/13934
该issue属于一个功能性需求，主要涉及在使用不均匀分配GPU的情况下支持张量并行计算，存在精度问题可能由于attention heads数量不能被GPU数量整除。

https://github.com/vllm-project/vllm/issues/13933
这是一个功能需求提出的issue，主要涉及V1版本的vllm中的spec decode功能。由于V1只支持ngram，用户提出希望支持随机采样的需求，但目前spec decode功能与top_p、top_k采样不兼容。

https://github.com/vllm-project/vllm/issues/13932
这是一个功能需求提议，主要涉及使用DeepGemm的grouped gemm kernel来进行fused MoE操作。

https://github.com/vllm-project/vllm/issues/13931
这是一个关于代码改进的issue，主要涉及V1版本中的EP/TP MoE + DP Attention功能的实现。

https://github.com/vllm-project/vllm/issues/13930
这个issue属于bug报告类型，涉及的主要对象是vLLM与transformers library的兼容性问题，由于LMFE版本过低导致无法编译vLLM与transformers 4.49，需要升级LMFE版本来解决兼容性问题。

https://github.com/vllm-project/vllm/issues/13929
这个issue是一个Bug报告，涉及主要对象为vllm下的mllama，由于dummy encoder sequences长度大于实际encoder长度，导致在执行`determine_num_available_blocks()`函数时出现了AssertionError。

https://github.com/vllm-project/vllm/issues/13928
这是一个重命名类型的 issue，涉及主要对象是 Ray ADAG，由于名称调整导致代码需要更新。

https://github.com/vllm-project/vllm/issues/13927
这是一个bug报告类型的issue，主要涉及到通过视频URL传递到/chat/completions时出现的错误。由于环境配置或请求方式可能不正确，导致服务器返回错误响应。

https://github.com/vllm-project/vllm/issues/13926
这个issue类型为需求提出，主要对象是添加RELEASE.md文档。

https://github.com/vllm-project/vllm/issues/13925
这是一个BUG报告类型的issue，主要涉及到pydantic模型类中的字段名共享问题。该问题导致在验证过程中忽略了特定的字段，如 'top_logprobs'，原因是相关的类之间由继承关系连接，导致相同的 `field_names` 被共享，解决方法是重新定义 `field_names` 或者分割继承关系。

https://github.com/vllm-project/vllm/issues/13924
这是一个Bug报告，涉及到VLLM使用过程中出现了总是以finish_reason='length'结束的问题，可能是由于环境信息中使用的PyTorch版本与CUDA版本不兼容所导致的。

https://github.com/vllm-project/vllm/issues/13923
这个issue是一个功能需求提案，主要涉及的对象是V1版本的AsyncLLM，提出了针对数据并行性能优化的新特性需求。因为目前需要在多节点环境下对请求进行负载均衡以提高性能，所以提出了一些关于多节点支持和请求调度优化的建议。

https://github.com/vllm-project/vllm/issues/13922
这是一个bug报告，涉及到对reshape_and_cache的更新来适应CUDA图填充时出现的情况，导致slot_mapping.size(0)不等于key.size(0)的问题。

https://github.com/vllm-project/vllm/issues/13921
这个issue类型是bug报告，涉及的主要对象是vLLM软件的安装过程。由于本地主分支不同步，导致了安装vLLM时无法正确确定prebuilt wheel的提交版本号，因此提出了此问题。

https://github.com/vllm-project/vllm/issues/13920
这个issue类型是功能增强（Feature Enhancement），主要对象是日志输出系统。原因是为了通过loguru库实现输出JSON格式的日志，同时更新相关文档。

https://github.com/vllm-project/vllm/issues/13919
这是一个贡献新增配置文件的issue，涉及主要对象为一系列设备及其优化配置。原因可能是为了提供更多针对特定设备的优化配置文件，以提升性能。

https://github.com/vllm-project/vllm/issues/13918
这个issue是一个bug报告，主要涉及的对象是FP8 Linear模块，由于该模块未被注册到`torch.compile`中，导致用户无法在编译过程中使用该模块。

https://github.com/vllm-project/vllm/issues/13917
这是一个需求类型的issue，主要涉及的对象是要为DeepGEMM和vLLM Block FP8 Dense GEMM添加基准测试。

https://github.com/vllm-project/vllm/issues/13916
这个issue类型是bug报告，涉及的主要对象是Speculative Decoding Tokens未包含在Prometheus指标中。由于某些原因导致这些Tokens未被包含在Prometheus metrics中。

https://github.com/vllm-project/vllm/issues/13915
这是一个bug报告，主要涉及VLLM中的MoE（Mixture of Experts），由于256个专家对于单个gemm操作而言过大，导致出现问题。

https://github.com/vllm-project/vllm/issues/13914
这是一个Bug报告类型的issue，主要涉及DeepSeek-R1 14B模型在使用两块NVIDIA GeForce RTX 5090 D GPU时出现的问题。由于PyTorch版本与CUDA版本不匹配，导致无法正确加载GPU模型。

https://github.com/vllm-project/vllm/issues/13913
这是一个用户提出需求的issue，主要涉及要添加对Magma模型的支持，其原因是为了优化Magma推断性能和实现快速执行。

https://github.com/vllm-project/vllm/issues/13912
这是一个bug报告，涉及到docker容器中缺少librosa的安装，导致无法进行音频转录。

https://github.com/vllm-project/vllm/issues/13911
这是一个bug报告，该问题单涉及的主要对象是在vllm项目中与图像相关的token数量检查。由于重构导致的代码移除和测试更新，导致了图像数量与\<|image|\>标记数量不匹配的bug。

https://github.com/vllm-project/vllm/issues/13910
这是一个用户提出需求的issue，主要涉及的对象是whisper endpoint，用户希望在whisper endpoint中添加更多的抽样参数来提升功能。

https://github.com/vllm-project/vllm/issues/13909
这是一个Bug报告。该问题涉及VLLM中创建placement group时等待时间过长的问题。可能是由于环境信息显示的PyTorch版本与CUDA版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/13908
这是一个bug报告，主要涉及更新TPU测试路径的问题。由于测试文件被移动，导致TPU测试失败。

https://github.com/vllm-project/vllm/issues/13907
这个issue类型是用户提出需求，针对的主要对象是vLLM的OpenAI兼容服务器，其提出了支持在Unix域套接字上进行绑定的功能请求。

https://github.com/vllm-project/vllm/issues/13906
这是一个bug报告，主要涉及的对象是`qwen_vl_utils`库中的`process_vision_info`函数，由于函数参数发生了变化导致症状出现。

https://github.com/vllm-project/vllm/issues/13905
这是一个升级`transformers`到`v4.50.3`版本的issue，涉及的主要对象是库的版本兼容性，需要解决的问题是将模型升级到最新的Transformers版本。

https://github.com/vllm-project/vllm/issues/13903
这个issue是一个特性需求的提出，主要涉及的对象是T5Model，用户希望使用vLLM来进行T5 reranker。

https://github.com/vllm-project/vllm/issues/13901
这是一个bug报告，关于Triton fused MoE配置中大BLOCK_SIZE可能导致的溢出问题。

https://github.com/vllm-project/vllm/issues/13900
该issue属于功能讨论类型，涉及主要对象为vllm中的paged attention机制。由于分块计算的paged attention不是精确的attention机制，存在attention scores和不为1的情况，用户讨论是否可以将paged attention称为精确的attention。

https://github.com/vllm-project/vllm/issues/13899
这是一个Bug报告，涉及的主要对象是vllm0.7.3版本的环境，由于无法正确反序列化头部导致了错误的症状发生。

https://github.com/vllm-project/vllm/issues/13898
这个issue属于bug报告类型，涉及主要对象为TPU CI。由于TPU CI存在问题，导致需要修复以使其正常工作。

https://github.com/vllm-project/vllm/issues/13897
这是一个bug报告，涉及的主要对象是代码中的`kv_c_normed`和`kv_b_proj`。由于对`kv_c_normed`执行了不必要的操作导致了`kv_b_proj`的性能下降。

https://github.com/vllm-project/vllm/issues/13896
这个issue类型是用户提出需求，涉及主要对象是VLLM与External Launcher TorchRUN的集成，导致由于无法设置CUDA_VISIBLE_DEVICES而需要解决不同world sizes下VLLM与trainer的协作问题。

https://github.com/vllm-project/vllm/issues/13895
这是一个bug报告，主要涉及的对象是Ultravox测试。这个问题由于token counts的不匹配导致了bug。

https://github.com/vllm-project/vllm/issues/13894
这是一个bug报告，涉及到vllm的XGRAMMAR在aarch64上的支持。修改包括更新依赖和修改guided decoding过程，旨在简化代码并移除不必要的平台特定处理。

https://github.com/vllm-project/vllm/issues/13893
这个issue类型是功能特性提出，主要涉及支持cutlass 3.8。由于特征、动机和pitch的改变，需要支持blackwell和新版本的cuda，因此需要修改flash attention。

https://github.com/vllm-project/vllm/issues/13891
这个issue类型是优化建议，主要涉及的对象是模型的嵌入（embedding model）。由于模型参数过多，导致性能较低，因此提议使用更小的嵌入模型以提高性能。

https://github.com/vllm-project/vllm/issues/13890
这是一个bug报告，涉及Ultravox v0.5，由于缺少测试示例导致代码中可能存在bug。

https://github.com/vllm-project/vllm/issues/13889
这是一个Bug报告，涉及主要对象是vllm在macOS上的安装和运行问题，由于ConfigFormat.AUTO格式不受支持，导致vllm在macOS启动失败。

https://github.com/vllm-project/vllm/issues/13888
这是一个bug报告类型的issue，主要涉及collect_env脚本无法在uv环境中运行生成相关信息的问题。

https://github.com/vllm-project/vllm/issues/13887
这是一个用户提出需求的类型，该问题涉及到访问`kv_cache`和`attn_metadata`，由于缺少相关注释，用户希望在代码中添加注释以便更好地理解相关功能。

https://github.com/vllm-project/vllm/issues/13886
这是一个Bug报告，主要对象是vllm部署qwen2.5_vl_72b。由于调用频率越来越慢，可能是由于资源占用或性能问题引起的。

https://github.com/vllm-project/vllm/issues/13885
这个issue类型是需求提出，主要涉及对象是在Ampere架构GPU上运行DeepSeek R1 FP8模型，由于Marlin不支持块状的fp8导致无法部署DeepSeekR1 fp8模型。

https://github.com/vllm-project/vllm/issues/13884
这是一个Bug报告，主要涉及到VLLM 0.7.3系统在推理过程中卡住的问题，可能是由于系统环境中的PyTorch版本不兼容导致的。 

https://github.com/vllm-project/vllm/issues/13883
这是一个bug报告，主要涉及的对象是收集环境数据的脚本。由于环境数据的收集存在错误，导致指标结果不正确。

https://github.com/vllm-project/vllm/issues/13882
这是一个bug报告，涉及主要对象为vLLM模型的创建过程。由于PyTorch版本不兼容，导致创建vLLM模型时出现错误。

https://github.com/vllm-project/vllm/issues/13881
这是一个bug报告类型的issue，涉及主要对象为vllm v0.7.3版本和openai 1.59.7版本，可能是由于添加top_logprobs字段后，日志信息消失导致的问题。

https://github.com/vllm-project/vllm/issues/13880
这是一个Bug报告，涉及的主要对象是OpenAI API中的top_logprobs参数。该问题由于在v0.7.1版本中引入的警告信息导致。

https://github.com/vllm-project/vllm/issues/13879
这个issue是用户提出需求的类型，主要涉及支持Deepseek的DeepGemm MoE，用户想了解MoE操作符是否会采用Deepseek开源的DeepGemm操作符。

https://github.com/vllm-project/vllm/issues/13878
这个issue类型为性能优化，涉及的主要对象是KVCacheManager._get_cached_block。由于性能优化优化KVCacheManager cached block dict的检索方式，以提高性能。

https://github.com/vllm-project/vllm/issues/13877
这个issue属于bug报告类型，主要涉及DeepSeek-R1-GGUF模型的不支持问题。这可能是由于该模型架构deepseek2目前尚不被vllm支持所导致。

https://github.com/vllm-project/vllm/issues/13875
这是一个用户提出问题的issue，主要涉及vllm中`max_num_batched_tokens`和`max_model_len`参数之间的关系，用户询问了这两个参数对输入和输出token的限制以及在使用`max_model_len` = 64k时，为何serving model只支持2k tokens的问题。

https://github.com/vllm-project/vllm/issues/13874
该issue属于用户提出需求类型，涉及到在部署模型时设置最大输入字符串长度、最大输入标记数以及最大输出长度的问题。用户表达了对如何使用vllm进行推理的不了解，并希望获得相关集成指导。

https://github.com/vllm-project/vllm/issues/13873
这是一个bug报告，主要涉及到CacheEngine的缓存大小检查问题。由于kv cache在长prompt时被分割成不同的管道，导致长prompt未被阻止即使超出了kv cache容量，需要相同逻辑来检查kv cache大小。

https://github.com/vllm-project/vllm/issues/13872
这个issue是关于Bug报告，主要涉及对象是V1代码库。由于模型不支持torch.compile，导致出现警告并需要在GitHub上提出支持请求。

https://github.com/vllm-project/vllm/issues/13871
这是一个bug报告，主要涉及到Ultravox输入处理的问题，由于最近v0.5更新导致处理器测试出现问题，需要修复。

https://github.com/vllm-project/vllm/issues/13870
这是一个用户提出需求的类型，主要涉及支持Microsoft/MAGMA-8B模型的问题。由于未得到回应，用户关注该模型何时能被支持。

https://github.com/vllm-project/vllm/issues/13869
这是一个Bug报告，主要涉及到V1版本中的同步引擎客户端关闭错误或挂起的问题，导致出现与输出处理线程关闭ZMQ套接字相关的竞争条件。

https://github.com/vllm-project/vllm/issues/13868
这个issue属于用户提出需求类型，主要涉及的对象是新增模型RapidOCR。由于未得到回应，用户希望了解为何当前支持的最接近的模型无法满足需求。

https://github.com/vllm-project/vllm/issues/13867
这个issue属于用户需求，涉及VLLM的Attention模块，用户希望使用Flash MLA来获得特定结果。

https://github.com/vllm-project/vllm/issues/13866
这个issue类型是用户提出需求，主要对象是引入新模型"silero-vad"。通过没有得到回应，可能是由于一些原因导致无法支持该模型。

https://github.com/vllm-project/vllm/issues/13865
这个issue属于bug报告类型，涉及的主要对象是无法推断设备类型，可能由于环境配置不正确导致。

https://github.com/vllm-project/vllm/issues/13864
这是一个Bug报告，涉及vLLM版本升级后使用response_format参数时可能出现的错误。由于在v0.7.3版本中使用了"type": "bool"的参数导致BadRequestError错误。

https://github.com/vllm-project/vllm/issues/13863
这个issue类型是bug报告，主要涉及的对象是"tensorize_vllm_model"模块。这个问题由于模块路径错误引起，导致需要修复。

https://github.com/vllm-project/vllm/issues/13862
该issue类型为新增模型需求，主要对象是要在项目中加入一个OCR模型。由于需求添加OCR模型，可能是为了实现文字识别的功能。

https://github.com/vllm-project/vllm/issues/13861
该issue属于功能需求类型，主要对象是vllm中的代理负载平衡功能。用户提出了希望vllm支持类似lmdeploy库中代理命令的功能，以实现更高的吞吐量和更方便的模型实例管理。

https://github.com/vllm-project/vllm/issues/13860
这是一个用户提出需求的类型，主要涉及的对象是在使用TPU时希望能够使用torch2.6 whl包。原因可能是当前环境无法正常运行torch2.6版本或者缺少相关支持，导致用户寻求解决方案。

https://github.com/vllm-project/vllm/issues/13859
这是一个代码优化类型的issue，主要涉及对象是代码中的变量赋值，由于重复赋值导致代码冗余。

https://github.com/vllm-project/vllm/issues/13858
这是一个Bug报告，该问题涉及的主要对象是Ray分配driver worker时无法分配GPU，导致数值型错误(ValueError)。

https://github.com/vllm-project/vllm/issues/13857
这是一个用户提出需求的问题，主要涉及的对象是vLLM，用户在询问是否有计划或可能的途径与DeepGEMM集成/合作，以优化GEMM操作。

https://github.com/vllm-project/vllm/issues/13856
这是一个类型为"Pull Request"的issue，该问题单涉及的主要对象是项目的上游仓库。

https://github.com/vllm-project/vllm/issues/13855
这个issue类型是特性需求，主要涉及到vLLM进程在DGX机器上固定到正确的NUMA区域。原因是希望提升GPU的H2D和D2H性能的可预测性。

https://github.com/vllm-project/vllm/issues/13854
这个issue类型是文档错误报告，主要涉及安装vLLM on TPU时缺少了git clone指令导致操作失败。

https://github.com/vllm-project/vllm/issues/13853
这是一个需求请求，涉及主要对象为HPU，请求启用AutoGPTQ/AutoAWQ量化模型推断功能。

https://github.com/vllm-project/vllm/issues/13852
这是一个未填写内容的bug报告，涉及的主要对象是项目中的"test"模块。

https://github.com/vllm-project/vllm/issues/13851
这是一个bug报告，该问题涉及到ROCm下的Quantization和Kernel，由于旧版本的头文件中的定义问题，导致代码实现出现错误。

https://github.com/vllm-project/vllm/issues/13850
这是一个bug报告，涉及的主要对象是UV库安装PyTorch时出现的超时问题。

https://github.com/vllm-project/vllm/issues/13849
这是一个关于bug的问题，主要对象是使用Qwen 2.5模型的用户。这个问题是由于Qwen 2.5模型的标记器相同但词汇大小有微小差异导致的，影响了speculative decoding的功能。

https://github.com/vllm-project/vllm/issues/13848
这是一个Bug报告，涉及vLLM 0.7.3版本的API服务器启动时遇到的TypeError问题，可能由于Python 3.11+下的argparse模块不支持`ge`和`le`约束，导致无法正常解析参数而出现。

https://github.com/vllm-project/vllm/issues/13847
这是一个bug报告类型的issue，涉及的主要对象是vLLM中的guided grammar example，由于缺少参数guided_decoding_backend导致生成过程中缺乏引导。

https://github.com/vllm-project/vllm/issues/13846
这个issue类型是改进提案，主要涉及对象是CMakeLists.txt文件，用户提出需要统一CUTLASS版本以避免重复。

https://github.com/vllm-project/vllm/issues/13845
这个issue为用户疑问类型，涉及主要对象为vllm中的KV缓存生成，用户询问了关于如何在进行推测推断时生成kv缓存的问题。

https://github.com/vllm-project/vllm/issues/13844
这是一个bug报告，涉及flash_attn_varlen_func函数在非CUDA平台上不支持return_softmax_lse参数。

https://github.com/vllm-project/vllm/issues/13843
这是一个特性需求的issue，主要涉及的对象是为AMD Navi 3x/4x GPU添加自定义分页注意力内核。由于Navi架构与MI架构的差异，需要针对每种架构添加新的内核。

https://github.com/vllm-project/vllm/issues/13842
这是一个bug报告类型的issue，涉及的主要对象是权重加载测试。由于部分测试用例的设置或代码实现存在问题，导致权重加载测试无法通过。

https://github.com/vllm-project/vllm/issues/13841
这是一个用户提出需求的issue，主要涉及的对象是关于 Kubernetes 部署文档。

https://github.com/vllm-project/vllm/issues/13840
这是一个用户提出需求的 issue，主要对象是为 vllm 添加 CLI 命令进行基准测试，由于目前的 vllm 命令只支持 `servecomplete`，用户希望添加类似基准测试的命令以提高功能。

https://github.com/vllm-project/vllm/issues/13839
这是一个需求提出的issue，主要涉及pipeline分区优化，由于层数不均匀分布导致的计算平衡问题。

https://github.com/vllm-project/vllm/issues/13838
这是一个bug报告，主要涉及VLLM中的数据库磁盘镜像损坏所导致的错误。

https://github.com/vllm-project/vllm/issues/13837
这个issue类型是性能优化，涉及主要对象是XGrammarLogitsProcessor数据结构，由于深拷贝操作引起添加请求时的性能开销过大。

https://github.com/vllm-project/vllm/issues/13836
这是一个bug报告类型的issue，主要涉及到vllm在重启时出现的警告信息以及无法正确执行关闭脚本的问题。

https://github.com/vllm-project/vllm/issues/13835
该issue类型为需求提出，主要涉及的对象是集成层，用户提出需要支持使用生成的block FP8矩阵乘法内核的w8a8_block_fp8_matmul支持，并且介绍了性能比较以及问题的具体原因。

https://github.com/vllm-project/vllm/issues/13833
这是一个功能需求提出类型的issue，主要涉及对象是DeepSeek V2/V3/R1中的`lm_head`，用户提出的需求是为了在pipeline ranks之间平衡内存使用，仅在最后一个rank上放置`lm_head`。

https://github.com/vllm-project/vllm/issues/13832
这是一个Bug报告，涉及的主要对象是VLLM项目中的模型检查代码。这个问题是由于模型检查代码在另一个子进程中加载所有一般插件导致了无限递归。

https://github.com/vllm-project/vllm/issues/13831
这个issue属于拼写错误的问题，涉及主要对象为LoRA，可能由于拼写错误导致了特定文本或代码中的错误。

https://github.com/vllm-project/vllm/issues/13830
这是一个功能增强的issue，涉及主要对象是支持cuda 12.8.0和SBSA wheels，由于需要增强兼容性和安装过程，对操作系统和架构进行了更新，并改进了CUDA安装脚本。

https://github.com/vllm-project/vllm/issues/13829
这是一个提出需求的 issue，涉及的主要对象是 triton 3.3.0，用户请求支持新的 GPUs（显卡）。

https://github.com/vllm-project/vllm/issues/13828
这是一个Bug报告，涉及LLaMA 3.1 8B模型与vLLM结构化输出生成的问题。由于某些情况下模型产生了无意义的输出，导致了输出为无意义字符串的症状。

https://github.com/vllm-project/vllm/issues/13827
这个issue类型是用户提出需求，主要涉及对象为vLLM，请求添加对ColBERT family of models的支持。

https://github.com/vllm-project/vllm/issues/13826
这个issue是一个Bugfix类型的报告，主要涉及Llama3JsonToolParser中JSON编码无法保留非ASCII字符，导致参数重复并转义成Unicode序列的问题。

https://github.com/vllm-project/vllm/issues/13825
这是一个bug报告，涉及的主要对象是代码中的字符串解析错误。由于之前的Pull Request导致了字符串解析错误，从而出现了问题。

https://github.com/vllm-project/vllm/issues/13824
这是一个Bug报告，涉及vllm0.7.3版本出现的非法内存访问错误。原因可能是程序bug引起的。

https://github.com/vllm-project/vllm/issues/13823
这是一个bug报告，主要涉及的对象是vLLM中的structured decoding功能。由于使用xGrammar backend进行speculative decoding时，导致vLLM崩溃，需要引入回滚机制来解决这一问题。

https://github.com/vllm-project/vllm/issues/13822
这是一个关于功能使用的 issue，涉及主要对象为 MLA（Machine Learning Assisted）模型。原因是在特定环境下禁用了前缀缓存，导致了该问题。

https://github.com/vllm-project/vllm/issues/13821
这是一个Bug报告，主要涉及vllm环境中使用JSON schema进行结构生成时产生的问题。导致问题的原因是当输出要求生成一个空列表时，实际产生了一个无效的JSON输出。

https://github.com/vllm-project/vllm/issues/13820
这是一个修复测试失败的bug报告，涉及的主要对象是名为`MyGemma2Embedding`的模型测试。这个问题的原因是在之前的代码更改中删除了模型定义中未使用的参数，导致了测试失败。

https://github.com/vllm-project/vllm/issues/13819
这是一个bug报告类型的issue，主要涉及VLLM模型在更新到0.7.3版本后无法正确加载的问题，可能是由于更新后的torch和triton版本导致的。

https://github.com/vllm-project/vllm/issues/13818
这是一个bug报告，主要涉及的对象是Deepseekvl2模型。由于Deepseekvl2会在num_images大于2时将图像裁剪为(image_size, image_size)，但在处理多模态标记时会引发数值错误。

https://github.com/vllm-project/vllm/issues/13817
这是一个bug报告，主要涉及到vllmopenai docker image中使用特定图片时设置`dtype`为`float16`会出现一系列感叹号的问题。

https://github.com/vllm-project/vllm/issues/13816
该issue类型为性能优化建议，主要涉及解码性能问题，提出了两种优化建议以提高吞吐量。原因是由于接收键、值和隐藏状态导致阻塞计算，从而影响执行模型的效率。

https://github.com/vllm-project/vllm/issues/13815
这个issue属于bug报告类型，主要涉及vLLM中的`device_id_to_physical_device_id`函数，发生`ValueError: invalid literal for int() with base 10`错误，可能是由于传入的参数不符合int()函数要求导致的。

https://github.com/vllm-project/vllm/issues/13814
这是一个bug报告，主要涉及vLLM在启用tensor parallelism时出现加载模型卡顿的问题。可能是由于tensorparallelsize设置大于1时导致的问题。

https://github.com/vllm-project/vllm/issues/13813
这是一个bug报告，主要涉及OOM despite there being enough available GPU memory。这个问题导致torch尝试分配的内存量明显少于可用内存量，用户寻求解释为何会出现这种情况。

https://github.com/vllm-project/vllm/issues/13812
这个issue属于bug报告，涉及到vLLM在4x8*L20集群上部署Deepseek-R1时出现的GPU异常问题。造成这一状况的原因可能是硬件或软件问题。

https://github.com/vllm-project/vllm/issues/13811
这是一个Bug报告类型的Issue，主要涉及到vllm项目的音频转录接口 `/v1/audio/transcriptions` 出现了Bad Request错误。由于使用了FastAPI的Form Models功能，而requirements.txt中指定的FastAPI版本过低，导致服务器在使用旧版本FastAPI时出现错误码400。

https://github.com/vllm-project/vllm/issues/13810
这是一个bug报告，主要涉及vllm v0.7.2版本无法支持baichuan2模型的问题。由于cu_seqlens_k参数传递导致出现错误日志，可能是由于部分参数不兼容引起的。

https://github.com/vllm-project/vllm/issues/13809
这是一个功能需求的issue，涉及主要对象是模型加载器（Model Loader）。原因可能是为了解决在加载庞大模型时从本地下载耗时的问题，希望支持直接从远程KV存储中加载模型。

https://github.com/vllm-project/vllm/issues/13808
这个issue是bug报告，涉及到Model Google SigLip 2。由于与Issue 13663 相关的问题需要修复。

https://github.com/vllm-project/vllm/issues/13807
这是一个bug报告，主要涉及修改了`transformer_utils`中的`modelscope` api的使用方式，导致了原有的`_try_login()`方法被移除。

https://github.com/vllm-project/vllm/issues/13806
这是一个Bug报告，主要涉及的对象是VLLM引擎。该问题由于使用V1的AsyncLLMEngine时在主解释器的主线程中信号(signal)才能工作而出现。

https://github.com/vllm-project/vllm/issues/13805
这是一个用户提出需求的issue，主要涉及的对象是Model中的Speculative Decoding功能。由于当前MTP仅支持k=1，用户希望扩展支持k > 1，以实现更灵活的解码选项。

https://github.com/vllm-project/vllm/issues/13804
这是一个用户提出需求的issue，主要涉及DeepEP功能是否能被整合到vllm，由于缺乏反馈，用户正在寻求相关帮助。

https://github.com/vllm-project/vllm/issues/13802
这是一个bug报告，主要涉及vllm项目的deepseek-v2模块，由于引入了一个变化导致了"missing 1 required positional argument: 'residual'" bug。

https://github.com/vllm-project/vllm/issues/13801
这是一个Bug报告，涉及到VLLM环境中使用不同的tensorparallelsize时出现的准确性不一致问题。原因可能是由于使用不同的参数设置导致的。

https://github.com/vllm-project/vllm/issues/13800
这是一个bug报告，主要涉及页面显示问题，由于设置maxwidth为0导致了兼容性矩阵表头重叠的情况。

https://github.com/vllm-project/vllm/issues/13799
这个issue是关于用户提出需求的，主要涉及调度器状态监控中的metric添加，用于追踪并组合运行中和等待中的请求数量，以提供更全面的调度器状态视图。

https://github.com/vllm-project/vllm/issues/13798
这是一个功能增强的请求，主要涉及VLLM中添加Cutlass支持到Blackwell FP8 Gemm。

https://github.com/vllm-project/vllm/issues/13797
这是一个bug报告，涉及主要对象是 CUTLASS 库。由于 CUTLASS 版本升级至 3.8 引起的代码错误导致了问题的产生。

https://github.com/vllm-project/vllm/issues/13796
这是一个bug报告，主要涉及的对象是运行在RunAI Streamer中的模型路径设置问题，由于模型路径被转换为临时目录，导致无法在RunAI Loader中下载模型权重文件，需要修改使用`model_weights`属性来解决此问题。

https://github.com/vllm-project/vllm/issues/13795
这个issue类型是功能增强请求，涉及到VLLM项目中的模型支持问题，用户提出了对Grok1模型的一些功能改进的需求。

https://github.com/vllm-project/vllm/issues/13794
这是一个bug报告，主要对象是VLLM下的dummy_sampler_run功能。由于未正确处理超过1个PP rank时的情况，导致了异常情况。

https://github.com/vllm-project/vllm/issues/13793
这是一个优化性质的问题单，涉及到mrope计算的性能优化。原因是为了改善mrope计算的速度，采用了预先计算torch.arange以及矢量化的方法。

https://github.com/vllm-project/vllm/issues/13792
这是一个用户提出需求的issue，主要涉及V1 engine的parallel sampling功能，由于并行抽样请求输出不包含指标，导致用户希望并行抽样请求输出包含指标。

https://github.com/vllm-project/vllm/issues/13791
这个issue类型是需求提出， 主要涉及对象是项目中的"custom_cache_manager"。由于使用的torch 2.6.0版本集成了triton 3.2.0，因此需要移除掉"custom_cache_manager"。

https://github.com/vllm-project/vllm/issues/13790
这个issue是关于功能需求的，主要涉及Zero-copy tensor/ndarray的序列化和传输问题，由于msgpack无法原生处理递归类型的数据，在处理Image data时需要自定义序列化逻辑，从而实现直接传输数据而不需要复制。

https://github.com/vllm-project/vllm/issues/13789
这是一个 Pull Request（PR）类型的issue，涉及到 MLA（Machine Learning Accelerator）的 V1 版本支持。由于某些原因，用户提交了这个 PR 来实现相关功能或修复问题。

https://github.com/vllm-project/vllm/issues/13788
这是一个Bug报告，主要涉及使用V100和V1引擎时出现的数值错误。导致该问题的可能原因是不支持的FA版本导致的。

https://github.com/vllm-project/vllm/issues/13787
这个issue类型是用户提出需求。该问题涉及的主要对象是为MLA添加支持加载压缩张量格式权重的功能。

https://github.com/vllm-project/vllm/issues/13786
这个issue类型为bug报告，主要涉及的对象是Mistral function calls。导致这个bug的原因是未处理在客户端传递`None`函数参数的情况。

https://github.com/vllm-project/vllm/issues/13785
这个issue属于文档修复类型，主要涉及arg_utils.py文件，由于此处存在拼写错误导致文档不准确，需要修正。

https://github.com/vllm-project/vllm/issues/13784
该issue是一个bug报告，涉及到`Fp8MoEMethod`类在使用expert parallelism时从`layer`中获取专家数量的问题。这个bug导致了错误提示和程序逻辑上的问题。

https://github.com/vllm-project/vllm/issues/13783
这个issue属于bug报告类型，主要涉及了xgrammar库中的JSON模式转换器，由于缺少对某些属性的验证，导致了未支持的关键字在string类型中被错误地处理。

https://github.com/vllm-project/vllm/issues/13781
这是一个用户提出需求的类型，主要涉及的对象是cache guided decoding logits processor。由于缺乏缓存引导解码的logits处理器，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/13780
这是一个Chore类别的issue，涉及主要对象为`AsyncOutputProcessing`。由于异步输出处理被禁用时仍会生成警告，用户提出清理这些不必要的警告。

https://github.com/vllm-project/vllm/issues/13779
这是一个针对构建过程中的bug报告，主要涉及ROCm环境下使用gcc14构建时出现的失败情况。

https://github.com/vllm-project/vllm/issues/13778
这是一个Bug报告类型的issue，涉及的主要对象是vllm服务。由于某些原因导致vLLM服务在处理一定数量请求后出现hang的症状，具体原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/13777
这是一个bug报告，涉及的主要对象是vllm。由于编译时使用gcc14/hipclang导致std::clamp函数无法正常工作，最终导致了编译错误。

https://github.com/vllm-project/vllm/issues/13776
这是一个bug报告，涉及的主要对象是vLLM中关于logits和sampling的内存问题。这个问题的原因是由于内存碎片化问题导致logits和相关sampling张量的内存使用不断增长，导致了内存问题。

https://github.com/vllm-project/vllm/issues/13775
这是一个bug报告，涉及主要对象是vllm项目下的一个提交，由于cumem和LoRA的问题导致此问题尚未解决。

https://github.com/vllm-project/vllm/issues/13774
这是一个优化性质的issue，主要涉及的对象是关于并行采样的实现方式。该问题由于在高层次抽象中实现并行采样导致代码复杂且需要大量的后处理工作，建议在层次创建`RequestOutput`对象的层次来实现并行采样以减少开销和复杂性。

https://github.com/vllm-project/vllm/issues/13773
这个issue是一个bug报告，涉及主要对象是代码中的一个预提交错误。这个问题产生的原因是由于 #13693 提交引入了一个预提交错误，导致出现了一个签名错误。

https://github.com/vllm-project/vllm/issues/13772
这个issue是关于bug报告，主要涉及到代码提交和precommit检查失败的问题，可能是由于代码合并到主分支后导致precommit检查失败。

https://github.com/vllm-project/vllm/issues/13771
这是一个代码质量提升的issue，需要调整日志输出的格式。

https://github.com/vllm-project/vllm/issues/13770
这个issue类型是功能需求，主要涉及的对象是为TransformersModel添加LoRA支持，用户提出需求并表示需要添加单元测试和更新文档。

https://github.com/vllm-project/vllm/issues/13769
这个issue属于bug报告类型，涉及主要对象为CompressedTensorsWNA16MoE。由于组合尺度的问题导致了bug症状。

https://github.com/vllm-project/vllm/issues/13768
这是一个功能需求类型的issue，主要涉及的对象是`TransformersModel`。增加LoRA支持是为了添加对LoRA相关的功能支持，可能是为了实现更高级的功能或优化模型性能。

https://github.com/vllm-project/vllm/issues/13767
这是一个涉及CI/Build的bug报告，主要涉及V1 LoRA的构建失败问题。

https://github.com/vllm-project/vllm/issues/13766
这是一个bug报告，主要涉及无法在支持计算能力>=8但不包括8.6和8.9的设备上使用FA版本2导致的问题。

https://github.com/vllm-project/vllm/issues/13765
这是一个 bug 报告，主要对象是 "Loading safetensors checkpoint shards" 运行两次时导致的问题。

https://github.com/vllm-project/vllm/issues/13764
这是一个用户提出需求的issue，主要涉及DeepSeek-V3工具解析器的支持，由于用户期望DeepSeek工具解析器的支持，因此提出此需求。

https://github.com/vllm-project/vllm/issues/13763
这是一个bug报告，主要涉及vLLM模型在生成文本时出现的不匹配问题，可能是由于图像prompt引起的。

https://github.com/vllm-project/vllm/issues/13762
该issue是一个功能改进提议，主要涉及前端代码，用于优化vLLM的BatchLLM性能。原因是当前的隐式共享前缀识别方法在离线场景下无法充分利用共享前缀，导致性能下降。

https://github.com/vllm-project/vllm/issues/13761
这个issue是针对bug的修复，主要涉及TPU测试在CI（持续集成）中的问题。由于更新torch和torch_xla、更新nightly镜像、添加torchvision作为要求以及更新测试文件路径，旨在解决因为TPU CI断开而导致的构建失败的问题。

https://github.com/vllm-project/vllm/issues/13760
该issue是一个功能需求问题，主要涉及对象是vllm支持在多类型GPU上部署模型。这个问题的提出可能是因为用户想要利用不同类型的GPU来提高模型的性能或灵活性。

https://github.com/vllm-project/vllm/issues/13759
这是一个Bug报告，主要涉及Speculative Decoding Model Load Error，由于环境中的PyTorch版本与CUDA版本不匹配，导致加载模型错误。

https://github.com/vllm-project/vllm/issues/13758
这是一个feature请求类型的issue，主要涉及对Whisper中beam search的实现。缺少了beam procedure的退出条件，速度优化和编码部分的重计算问题。

https://github.com/vllm-project/vllm/issues/13757
这是一个bug报告，涉及vllm版本0.7.3的transcriptions端点的错误请求问题，由于API请求签名问题导致无法成功创建POST请求。

https://github.com/vllm-project/vllm/issues/13756
这是一个bug报告，涉及vllm在请求API时无法限制推理内容最大长度的问题。

https://github.com/vllm-project/vllm/issues/13754
这是一个用户提出需求的issue，涉及主要对象是VLMs与transformers backend的支持。这个问题由于需要对transformers进行一些更改，以支持VLMs在Transformers后端运行。

https://github.com/vllm-project/vllm/issues/13753
这是一个Bug报告，涉及的主要对象是vllm服务器。由于GPU内存利用率过高导致程序卡住不动。

https://github.com/vllm-project/vllm/issues/13752
这是一个关于vllm和torch.compile之间差异的问题，用户想了解vllm是否支持torch.compile的不同模式，以及vllm的cuda图与torch.compile的reduceoverhead模式之间的区别。

https://github.com/vllm-project/vllm/issues/13751
这是一个用户报告bug类型的issue，主要涉及FP8稀疏矩阵乘法的问题。由于环境问题或代码逻辑错误，导致了稀疏矩阵乘法结果不符合预期。

https://github.com/vllm-project/vllm/issues/13750
这是一个bug报告，主要涉及对象是GPTQ Marlin implementation for DeepSeekV3's FusedMoE。这个问题导致了在通过sglang运行模型时观察到了不正确的结果。

https://github.com/vllm-project/vllm/issues/13749
这是一个bug报告，涉及的主要对象是文档中的header样式。由于`verticaltableheader` css存在问题，导致compatibility_matrix的header重叠。

https://github.com/vllm-project/vllm/issues/13748
这个issue属于bug报告类型，涉及的主要对象是VLLM日志系统。原因是logger对象未正确继承日志级别导致VLLM无法打印INFO级别的日志。

https://github.com/vllm-project/vllm/issues/13747
这是一个关于集成 FlashMLA 的 issue，类型属于需求提出，主要涉及的对象是 Kernel。由于某些原因导致 cudagraphs 功能存在问题，未来的 PR 会修复这个问题并添加 block_size 64 的支持。

https://github.com/vllm-project/vllm/issues/13746
这是一个功能改进类型的issue，主要对象是vllm中的spec_decode部分。该改进是为了消除spec_decode中对CUDA的硬编码，以便更容易地让其他设备使用该功能。

https://github.com/vllm-project/vllm/issues/13745
这个issue是一个安装问题，并提到了无法识别的错误，主要对象是vllm的安装过程。由于信息不完整，用户可能遇到了安装过程中的错误，而请求帮助和解决方案。

https://github.com/vllm-project/vllm/issues/13744
这个issue是关于用户提出需求，主要涉及vllm是否会支持FlashMLA，提出希望未来版本的vllm能够支持DeepSeek开源的FlashMLA技术，以优化大型transformer模型的推断任务效率。

https://github.com/vllm-project/vllm/issues/13743
这是一个 bug 报告，主要涉及模型注册时无法进行检查，导致问题无法解决。

https://github.com/vllm-project/vllm/issues/13742
这个issue属于功能更新类型，主要涉及到azure/setuphelm工具的版本升级。原因是为了添加新功能、更新依赖关系和改进文档。

https://github.com/vllm-project/vllm/issues/13741
这是一个bug报告，涉及对象为MistralTokenizer和vLLM，由于MistralTokenizer不支持truncation配置，导致在输入超过最大模型长度的情况下可能触发AsyncEngineDeadError。

https://github.com/vllm-project/vllm/issues/13740
这是一个功能优化类型的issue，涉及前端代码，针对的主要对象是vLLM中的请求排序。由于在离线情况下有许多具有不同共享前缀的请求，并且现有方法不能充分利用共享前缀，因此提出了添加前缀排序作为BatchLLM优化的先导步骤的需求。

https://github.com/vllm-project/vllm/issues/13739
这个issue是文档更新类型，主要涉及将`pythonjsonlogger`的存储库从已存档的`madzak/pythonjsonlogger`更新为维护良好的`nhairs/pythonjsonlogger`，由于原存储库已归档，需要更新以确保项目信息的准确性。

https://github.com/vllm-project/vllm/issues/13738
这个issue属于bug报告类型，涉及的主要对象是V100 GPU。原因可能是V100 GPU不支持enable-prefix-caching，导致出现问题。

https://github.com/vllm-project/vllm/issues/13737
这是一个需求提出类的issue，主要对象是vllm下的spec_decode功能。此问题由于CUDA硬编码存在，导致无法在其他设备上轻松使用。

https://github.com/vllm-project/vllm/issues/13736
这个issue是关于代码改进的，主要涉及到Benchmarking工具的功能改进和修复。原先存在的问题是TPOT计算累积平均值而不是每个请求的数值，这可能导致性能指标计算不准确，因此需要进行修复。

https://github.com/vllm-project/vllm/issues/13735
这个issue是用户提出需求的类型，主要涉及支持FlashMLA功能，由于需要该功能的原因而提出。

https://github.com/vllm-project/vllm/issues/13734
该issue类型是代码优化，主要涉及的对象是`EngineArgs.create_engine_config`方法。由于原方法代码冗余导致可读性较差，需要将其内的杂项方法移动至辅助方法以提高代码可读性。

https://github.com/vllm-project/vllm/issues/13733
这是一个功能需求提议，主要涉及到模型权重加载过程中的量化方法的初始化属性。由于属性的位置需要提前，以便直接被量化方法使用。

https://github.com/vllm-project/vllm/issues/13732
这是一个bug报告，涉及的主要对象是平台初始化过程中加载失败时的错误信息。由于缺少打印错误信息，用户无法清楚地了解到加载平台失败的原因。

https://github.com/vllm-project/vllm/issues/13731
这是一个bug报告，涉及的主要对象是ray compiled graph，由于类型提示不准确导致了问题。

https://github.com/vllm-project/vllm/issues/13730
这是一个bug报告，主要涉及的对象是VLLM项目中的Attention Backend，该bug可能由于升级到v0.7.3版本导致。

https://github.com/vllm-project/vllm/issues/13729
这是一个需求修改类型的issue，主要涉及Speculative Decoding中拒绝采样API的更改。由于需要支持随机采样，需要修改拒绝采样的API。

https://github.com/vllm-project/vllm/issues/13728
这个issue是关于如何使用"AsyncLLMEngine"来处理单个实例上的多个（并发）请求，而不是加载"qwen2-vl-7b"。这个问题涉及到如何处理多个请求并发的情况，可能是为了提高处理效率或避免资源浪费。

https://github.com/vllm-project/vllm/issues/13727
该issue类型为功能需求，主要涉及CI环境变量设置和模型路径逻辑，用户提出需要在S3 CI环境变量开启时将模型路径更改为S3路径。

https://github.com/vllm-project/vllm/issues/13726
这是一个需求问题，主要涉及 V1 功能启用，用户提出了关于 V1 特性启用的需求。

https://github.com/vllm-project/vllm/issues/13725
这是一个bug报告，主要涉及到vllm下的CompressedTensorsWNA16模型，由于缺少一处改动导致在启用MLA时该模型无法正常运行。

https://github.com/vllm-project/vllm/issues/13724
这是一个bug报告。该issue涉及VLLM中错误处理的问题，主要问题是对于不存在的模型路径和HF仓库ID的处理不清晰。

https://github.com/vllm-project/vllm/issues/13723
这个issue类型是用户提出需求，主要涉及VLLM支持增量KV缓存的功能。用户提出了希望在ASR模型提供部分转录时减少重新计算整个输入序列以降低生成延迟的需求。

https://github.com/vllm-project/vllm/issues/13722
这是一个bug报告，涉及到vllm库中的Speculative Decoding模块在使用CUDA设备时出现的RuntimeError。可能是由于CUDA内存访问问题导致的。

https://github.com/vllm-project/vllm/issues/13721
这是一个bug报告类型的issue，主要涉及vLLM在在线服务中的内存问题，由于logits和相关采样张量的内存碎片问题导致内存使用不断增加。

https://github.com/vllm-project/vllm/issues/13720
这是一个功能需求的issue，主要涉及的对象是在启用MLA时无法使用前缀缓存。因为MLA深度搜索模型可能与前缀缓存存在兼容性问题，用户提出是否可以解决这个问题。

https://github.com/vllm-project/vllm/issues/13719
这个issue是关于bug报告的，主要涉及的对象是VLLM的运行环境和GPU设置。由于不支持超过两个PCIe-only GPU的环境下禁用自定义全局归约，导致出现了警告信息。

https://github.com/vllm-project/vllm/issues/13718
这个issue类型是需求提出，主要涉及的对象是改进DSv3在AMD GPU上的性能。

https://github.com/vllm-project/vllm/issues/13717
这是一个bug报告，主要涉及的对象是测试指标设置问题，在测试指标脚本中出现了一些错误导致测试失败。

https://github.com/vllm-project/vllm/issues/13716
这是一个Bug报告，涉及到测试指标失败的问题。由于可能与当前环境或代码集成有关，导致测试失败。

https://github.com/vllm-project/vllm/issues/13715
这是一个bug报告，主要涉及对象为vllm模型。由于vllm模型目前不支持Qwen2.5-VL架构，导致出现数值错误的bug。

https://github.com/vllm-project/vllm/issues/13714
这是一个 Bug 报告类型的 issue，主要涉及到 vLLM 中使用 `cpu-offload-gb` 未能减少 CUDA 内存使用，用户在尝试使用该设置时没有看到期望的效果。

https://github.com/vllm-project/vllm/issues/13712
这是一个Bug报告，涉及主要对象为v0.7.3版本的升级过程。原因是在升级到v0.7.3后，加载模型时启用引擎1会出现多个问题。

https://github.com/vllm-project/vllm/issues/13711
这是一个需求提出类型的issue，主要涉及到V1 Workers的WorkerBase类定义。这个issue的提出源于当前代码中存在的重复部分，需要统一处理以减少重复代码。

https://github.com/vllm-project/vllm/issues/13710
这是一条用户需求类型的issue，涉及到项目中的Quant功能。这个问题是为了添加`SupportsQuant`接口到`BaiChuanBaseForCausalLM`，以及解决阻塞CI的linting问题。

https://github.com/vllm-project/vllm/issues/13709
这是一个CI/Build的问题，涉及到了预提交错误，并且由于合并代码时没有更新预提交而导致了主分支检查失败的bug。

https://github.com/vllm-project/vllm/issues/13708
这是一个改进类型的issue，主要涉及到benchmark_serving.py文件中被废弃的`--dataset`参数。

https://github.com/vllm-project/vllm/issues/13707
这是一个Bug报告，涉及vLLM加载本地路径模型时出现格式错误警告的问题。

https://github.com/vllm-project/vllm/issues/13706
这是一个关于bug修复的问题，涉及到vllm/attention/backends/utils.py文件中未使用vllm.logger.init_logger中的logger导致的问题。

https://github.com/vllm-project/vllm/issues/13705
该issue属于用户需求，主要涉及V1 LLMEngine和AsyncLLM类的功能增加。

https://github.com/vllm-project/vllm/issues/13704
这个issue是一个Bug报告，主要涉及的对象是DeepSeek-R1-AWQ模块。发生这个bug的原因可能是在启用MTP时，导致DeepSeek模块在生成过程中出现偏差，导致所有token都被拒绝而无法继续生成。

https://github.com/vllm-project/vllm/issues/13703
这个issue类型是bug报告，主要涉及的对象是vLLM中的OpenCV版本兼容性问题，由于OpenCV版本不匹配导致了bug。

https://github.com/vllm-project/vllm/issues/13702
这个issue属于bug报告类型，涉及对象是vllm项目中的`add_cli_args`方法。由于输入参数类型不匹配导致 IDE 提示了类型不匹配的错误。

https://github.com/vllm-project/vllm/issues/13701
这个issue类型是bug报告，涉及到CI的问题。由于precommit修复在合并之前的问题，导致了linter无法正确运行的bug。

https://github.com/vllm-project/vllm/issues/13700
该issue类型为用户提出需求，涉及的主要对象是添加对DeepSeek-R1-Distill-Qwen-32B工具调用支持，由于目前工具调用和推理功能无法同时启用，用户希望添加对两者的支持。

https://github.com/vllm-project/vllm/issues/13699
这是一个bug报告，涉及的主要对象是Dockerfile instructions和optional dependencies和dev transformers。这个问题是由于Docker Container不支持Qwen VL 2.5 Instruct和缺少vllm[audio]所导致的错误。

https://github.com/vllm-project/vllm/issues/13698
这是一个bug报告，主要涉及到`load_adapter`在生成过程中崩溃的问题。导致这个bug的原因可能是在生成进行中调用了`load_adapter`函数。

https://github.com/vllm-project/vllm/issues/13697
这是一个Bug报告，涉及到vllm的安装问题，用户在使用sccache时发现CUDA编译时哈希不同导致无法命中缓存，可能由于特定编译标志变动引起。

https://github.com/vllm-project/vllm/issues/13696
这是一个bug报告，主要涉及pre-commit在主分支上运行时出现的错误。

https://github.com/vllm-project/vllm/issues/13695
这是一个bug报告，涉及的主要对象是 pre-commit 工具。由于 yapf 等工具没有被 pre-commit 触发，导致无法运行。

https://github.com/vllm-project/vllm/issues/13694
这是一个Bug报告类型的Issue，涉及主要对象为软件VLLM的安装过程。由于依赖版本不兼容所导致安装过程中出现了错误信息。

https://github.com/vllm-project/vllm/issues/13693
这是一个bug报告，涉及主要对象为 vLLM 在 MoE On H20 上的部署，由于内存访问越界导致了该bug报告。

https://github.com/vllm-project/vllm/issues/13692
这是一个bug报告，涉及vLLM的安装问题，由于最新版本的wheel命名错误导致下载失败。

https://github.com/vllm-project/vllm/issues/13691
这是一个bug报告，主要涉及vllm后端统计数据不准确的问题，导致max_model_len小于input_len + output_len时出现问题。

https://github.com/vllm-project/vllm/issues/13690
这是一个Bug报告，涉及主要对象是vllm环境中的代码。原因是变量height的类型为NoneType导致了TypeError异常。

https://github.com/vllm-project/vllm/issues/13689
这个issue属于功能增强类型，主要涉及到了对`FakeAttentionMetdata`的引入，用于dummy run操作。

https://github.com/vllm-project/vllm/issues/13688
这是一个性能优化类型的issue，主要涉及使用IPC（域套接字）ZMQ套接字来进行本地通信。

https://github.com/vllm-project/vllm/issues/13687
这个issue是一个Bug修复类型的报告，主要涉及OLMo 2模型在使用GQA或MQA时未正确分割qkv导致的注意力计算错误。

https://github.com/vllm-project/vllm/issues/13686
这是一个bug报告，主要涉及OLMo 2在使用GQA或MQA时未正确分割qkv导致注意力计算错误的问题。

https://github.com/vllm-project/vllm/issues/13685
这是一个用户提出需求的issue，主要涉及的对象是MLA后端。这个问题的原因是用户希望能够选择Triton FA作为MLA后端。

https://github.com/vllm-project/vllm/issues/13684
这个issue属于功能需求类型，主要对象是添加新的模型GPTBigCodeForEmbedding支持token span分类。

https://github.com/vllm-project/vllm/issues/13683
这个issue是关于Bug报告，主要涉及的对象是Guided Decoding功能在生成结构化JSON输出时存在问题。原因可能是导致生成的JSON输出不正确的参数设置或相关代码逻辑的错误。

https://github.com/vllm-project/vllm/issues/13682
这是一个bug报告，涉及的主要对象是DeepSeekR1，用户在尝试使用Ray的编译DAG、SPMD和启用推测解码时遇到错误。

https://github.com/vllm-project/vllm/issues/13681
这是一个新增功能需求的issue，主要关注于添加一个新模型 GPTBigCodeForEmbedding 以支持 token span 分类。其中涉及的主要对象是模型和相关代码文件。

https://github.com/vllm-project/vllm/issues/13680
这是一个用户提出需求的issue，主要涉及的对象是如何在LLM中使用自定义模型，可能由于环境配置或版本兼容性问题导致用户遇到困难。

https://github.com/vllm-project/vllm/issues/13679
该issue属于用户提出需求类型，主要涉及支持LoRA for Pooling Models，由于当前vLLM不支持LoRA for Pooling models，用户想了解如何添加支持以及需要注意哪些问题。

https://github.com/vllm-project/vllm/issues/13678
这个issue是一个Bug报告，涉及主要对象为Mamba2模型（Bamba和Codestral Mamba），由于在RoCM上导入了不支持的vllm_flash_attn模块导致了问题。

https://github.com/vllm-project/vllm/issues/13677
这是一个Bug报告，涉及的主要对象是在运行Qwen/Qwen2.5-VL-72B-AWQ推理时遇到的错误。该问题可能是由于环境配置问题导致的错误信息输出。

https://github.com/vllm-project/vllm/issues/13676
这是一个bug报告类型的issue，主要涉及vllm的运行错误导致整个GPU卡被卡住的问题。

https://github.com/vllm-project/vllm/issues/13675
这是一个关于bug报告的issue，主要涉及到vllm docker image中运行whisper时出现的"ImportError: Please install vllm[audio] for audio support"错误。

https://github.com/vllm-project/vllm/issues/13674
这是一个Bug报告，涉及的主要对象是vLLM中的ROCm支持，由于ROCm支持在特定环境下触发了ERROR_MEMORY_APERTURE_VIOLATION错误。

https://github.com/vllm-project/vllm/issues/13673
这是一个Bug报告类型的Issue，涉及的主要对象是vllm中的Speculative Decoding功能。由于并发请求导致CUDA错误，出现了非法内存访问的RuntimeError。

https://github.com/vllm-project/vllm/issues/13672
这是一个bug报告，涉及主要对象是API Server。由于在端口验证中错误使用了'ge'和'le'，导致出现"TypeError: _StoreAction.__init__() got an unexpected keyword argument 'ge'"错误。

https://github.com/vllm-project/vllm/issues/13671
该issue是关于bug报告，涉及到vLLM 0.6.3在尝试实现工具调用时产生随机挂起的问题。出现这个问题的原因可能是在代码中的逻辑处理或调用过程中出现了错误，导致生成序列的过程在1-2步之后随机挂起。

https://github.com/vllm-project/vllm/issues/13670
这个issue类型是bug报告，涉及的主要对象是vllm项目中的端口号范围验证功能。由于该功能无法在旧版本的Python中使用，因此需要撤销相关更改。

https://github.com/vllm-project/vllm/issues/13669
这个issue是关于bug报告，涉及到vllm命令行中--lora-path参数引起的unrecognized arguments错误。原因可能是参数名称错误或者格式错误导致无法被识别。

https://github.com/vllm-project/vllm/issues/13668
这个issue类型是bug报告，涉及主要对象是grafana dashboard。由于拼写错误导致了数据源不正确的问题。

https://github.com/vllm-project/vllm/issues/13667
这是一个Bug报告的Issue，主要涉及CI failures。原因可能是CI测试出现了问题，导致了失败的结果。

https://github.com/vllm-project/vllm/issues/13666
这是一个用户提出需求的issue，主要涉及的对象是weights loading功能。这个issue的提出是为了增加对权重加载时间的记录，以提高透明度。


https://github.com/vllm-project/vllm/issues/13665
这个issue类型是bug报告，主要涉及的对象是vllm 0.7.3版本的DeepSeekR1UDQ2_K_XL模型推断中的"deepseek2 is not supported yet"错误，可能是由于当前版本不支持gguf导致的。

https://github.com/vllm-project/vllm/issues/13663
这个issue类型是用户提出需求，主要对象是支持Google SigLip 2模型。由于没有得到回应，用户在寻求如何支持所需的模型。

https://github.com/vllm-project/vllm/issues/13662
这是一个用户提出需求的issue，主要涉及的对象是vllm项目的CPU推断功能。由于用户的机器是Xeon E5-2670v2，只支持AVX指令集，因此希望修改代码以支持AVX。

https://github.com/vllm-project/vllm/issues/13661
这是一个bug报告类型的issue，主要涉及使用cuda graph在vllm上的问题，可能是由于环境问题导致无法正确使用cuda graph。

https://github.com/vllm-project/vllm/issues/13660
这是一个关于代码逻辑修改的issue，涉及主要对象是Mamba Mixer 2，在处理当Num Groups不被传输协议大小整除时的情况。原因导致了头部与所属组的映射关系不正确。

https://github.com/vllm-project/vllm/issues/13659
这个issue属于bug报告类型，主要涉及vllm的安装部署问题，由于部署Deepseek-R1 671B with AMD 8xMi300x时出现错误。

https://github.com/vllm-project/vllm/issues/13658
这是一个功能改进类的Issue，涉及的主要对象是models和layers。这个问题的原因是由于代码中硬编码了CUDA，导致其他设备无法轻松使用。

https://github.com/vllm-project/vllm/issues/13657
这个issue类型是bug报告，涉及的主要对象是Qwen2.5-VL-3B服务部署时出现的图片请求错误，可能是由于服务器内部错误导致的。

https://github.com/vllm-project/vllm/issues/13656
这是一个bug报告，涉及的主要对象是vllm v0.7.3版本。由于v0.7.3版本修改了启动信息导致API无法访问，特别在wsl ubuntu镜像网络情况下出现问题。

https://github.com/vllm-project/vllm/issues/13655
这是一个Bug报告类型的Issue，主要涉及的对象是Qwen2.5 VL。该问题可能由于程序运行过程中出现的Internal Server Error导致。

https://github.com/vllm-project/vllm/issues/13654
这个issue类型是bug报告，主要涉及的对象是vllm的安装过程。由于用户在两个仅支持CPU的主机上安装vllm时遇到问题，可能是由于环境配置或安装过程中的特定步骤导致了问题。

https://github.com/vllm-project/vllm/issues/13652
这是一个用户提出需求的类型issue，主要涉及到对LLM.embed函数中应用的归一化以及如何修改的问题。用户想要获取最后一个隐藏状态而不进行归一化。

https://github.com/vllm-project/vllm/issues/13651
这是一个Bug报告，涉及的主要对象是启动qwen2.5-vl系列时卡顿。原因可能是服务启动时卡顿导致。

https://github.com/vllm-project/vllm/issues/13650
这是一个Bug报告，主要涉及的对象是ROCM代码中的native attention function。由于参数数量不匹配导致函数调用错误。

https://github.com/vllm-project/vllm/issues/13649
这是一个bug报告，主要涉及的对象是ROCM中的native attention函数调用，由于参数数量不匹配导致函数调用错误。

https://github.com/vllm-project/vllm/issues/13648
这是一个bug报告，涉及主要对象为ROCM的_sdpa_attention()函数，由于参数数量不匹配导致了报错。

https://github.com/vllm-project/vllm/issues/13646
这是一个关于文档贡献的建议类型的issue，主要涉及项目贡献指南中完整的CI运行说明，可能由于在过去的PR中的观察，希望增加特定信息。

https://github.com/vllm-project/vllm/issues/13645
这是一个Bug报告，主要涉及VLLM中Qwen2.5-VL-72B模型服务启动时出现卡住/挂起的问题，原因可能是启动命令设置不正确。

https://github.com/vllm-project/vllm/issues/13644
这是一个Bug报告类型的Issue，涉及到在Chat相关协议中添加 `mm_processor_kwargs` 字段的问题，由于漏加导致出现了相关问题。

https://github.com/vllm-project/vllm/issues/13643
这个issue类型为功能需求提议，涉及主要对象是vllm项目。

https://github.com/vllm-project/vllm/issues/13642
这个issue是关于提出需求的，主要涉及到torchrun的兼容性问题。最终导致这个问题的原因是需要在同一进程中运行engine和LLM类以满足RLHF框架的需求，同时降低调度不确定性。

https://github.com/vllm-project/vllm/issues/13641
这个issue是关于bug报告，主要涉及benchmark data output的问题，因为JSON parser不支持`inf`值而导致无法存储参数。

https://github.com/vllm-project/vllm/issues/13640
这是一个Bug报告类型的issue，主要涉及了在使用Qwen2.5-VL-72B-AWQ进行推断时出现的错误。问题原因是权重输入尺寸无法被最小线程数整除，建议减少tensor_parallel_size或者运行带有量化gptq的模型。

https://github.com/vllm-project/vllm/issues/13639
这是一个用户提出需求的issue，主要涉及的对象是生成过程中是否能够插入用户想要的token。由于用户希望在生成多个token时插入特定的token，可能是出于对模型功能增强的需求。

https://github.com/vllm-project/vllm/issues/13638
这是一个需求类型的issue，主要涉及到V1版本的PP（Pipeline chunked prefill）功能。

https://github.com/vllm-project/vllm/issues/13637
这个issue是关于技术改进的，主要涉及到V1版本的VLLM项目中关于预先填充数据块调度的问题，由于更新`num_computed_tokens`的位置不当导致无法保守地调度请求，因此需要调整位置并优化实现。

https://github.com/vllm-project/vllm/issues/13635
这是一个bug报告，涉及的主要对象是持续集成的指标测试模型路径问题。原因可能是指标测试模型路径设置错误导致失败。

https://github.com/vllm-project/vllm/issues/13634
这是一个bug报告，主要涉及的对象是一个CI环境中关于是否使用S3桶的控制问题。导致这个bug可能是因为添加了一些无关的更改，对模型名称和加载格式进行了修改，需要将其还原并进行代码优化。

https://github.com/vllm-project/vllm/issues/13633
这个issue属于用户提出需求类型，该问题涉及的主要对象是VLLM项目的支持多个模型在同一GPU上运行。由于VLLM目前不支持在同一GPU上同时运行多个模型，用户提出了关于如何支持多个模型在同一GPU上运行的需求。

https://github.com/vllm-project/vllm/issues/13632
这个issue是关于bug报告，涉及的主要对象是"NVIDIA"。原因是quant操作没有使用当前的stream，导致出现了问题。

https://github.com/vllm-project/vllm/issues/13631
这是一个功能需求类型的issue，主要涉及的对象是增强Ultravox模型以接受超过30秒的音频输入。由于目前模型无法处理长于30秒的音频并且无法批量处理音频样本，用户提出了更新模型以支持长音频、批处理音频样本，并需要相应更新HF上的处理器以验证变化。

https://github.com/vllm-project/vllm/issues/13630
这个issue属于bug报告类型，涉及的主要对象是CUDA graphs，症状表现为CUDA graphs仍然存在问题。

https://github.com/vllm-project/vllm/issues/13629
这是一个Bug报告，主要涉及VLLM软件在不同版本上对GPU运行推理时出现的错误与正确工作的对比。问题可能源自VLLM V1版本的某些改动导致了错误的发生。

https://github.com/vllm-project/vllm/issues/13628
这是一个bug报告，涉及的主要对象是TPU运行器，由于需要更新torch_xla修复TPU请求的问题，导致了无法成功执行TPU Runner。

https://github.com/vllm-project/vllm/issues/13627
这是一个bug报告，涉及的主要对象是更新torch版本，导致无法使用torch==2.6.0.dev20241216+cpu，需要更新版本以解决问题。

https://github.com/vllm-project/vllm/issues/13626
这是一个用户提出需求的 issue，涉及主要对象是DeepSeek MTP 实现，由于原始实现无法处理 `k > 1`，所以提出了扩展实现以支持更多 speculative tokens 的功能。

https://github.com/vllm-project/vllm/issues/13625
这是一个优化类型的issue，主要涉及到了深度神经网络模型中的内存优化问题，由于长序列长度导致OOM问题。

https://github.com/vllm-project/vllm/issues/13624
这是一个bug报告，涉及的主要对象是CI系统。由于S3镜像路径错误导致了问题，需要修复。

https://github.com/vllm-project/vllm/issues/13623
这是一个bug报告，主要涉及到TunableOp在多GPU vLLM工作负载上的问题，由于worker进程在TunableOp析构函数调用之前被销毁，导致了GPU结果写入问题。

https://github.com/vllm-project/vllm/issues/13622
这是一个Bug报告，涉及到Mistral streaming tool parser无法解析整数类型的工具参数，导致在流媒体时丢失某些参数。

https://github.com/vllm-project/vllm/issues/13621
这是一个关于bug报告的issue，主要涉及前端进度条的显示问题，由于生成多个输出时进度条显示不正确。

https://github.com/vllm-project/vllm/issues/13620
这是一个bug报告类型的issue，主要涉及到vllm库中关于MLA（Multi-layer aggregation）功能的问题。原因是在使用deepseek模型时默认的允许提示长度较短，需要手动设置max_num_batched_tokens解决，但希望能自动处理此问题。

https://github.com/vllm-project/vllm/issues/13619
这是一个需求提出的issue，该问题单涉及的主要对象是项目中的compressed-tensors模块。

https://github.com/vllm-project/vllm/issues/13618
这个issue类型是功能需求提出，主要涉及的对象是在V1 TPU上添加通过Ray实现的tensor parallel支持。由于需求需要在不改变SMPD和Ray编译标志的情况下实现，这可能导致了一些潜在的执行或性能方面的问题。

https://github.com/vllm-project/vllm/issues/13617
这是一个关于优化建议（feature request）的issue，主要涉及使用pre-commit来更新`requirements-test.txt`文件，目的是自动运行`pip compile`。

https://github.com/vllm-project/vllm/issues/13616
该issue类型为用户提出需求，主要对象是vllm模型支持。原因是用户在提出新模型支持的请求时未收到响应。

https://github.com/vllm-project/vllm/issues/13615
这个issue类型是bug报告，涉及的主要对象是OpenVINO集成功能。原因是OpenVINO环境变量的布尔转换存在问题，导致误解释用户设置，需要对此进行修复。

https://github.com/vllm-project/vllm/issues/13614
这是一个bug报告，涉及的主要对象是`pre-commit`工具的`isort`版本。这个问题可能是由于`isort`版本较低导致出现警告造成的。

https://github.com/vllm-project/vllm/issues/13613
这是一个性能优化需求，主要涉及到测试用例的并行执行问题，原因是当前的测试用例执行时间长且消耗资源过多。

https://github.com/vllm-project/vllm/issues/13612
这是一个bug报告，主要涉及的对象是vLLM的配置文件缺失导致数值错误。

https://github.com/vllm-project/vllm/issues/13611
这是一个bug报告，主要涉及Dockerfile中uv缓存的修复问题。该问题来源于uv使用自己的缓存路径导致无法使用pip缓存路径。

https://github.com/vllm-project/vllm/issues/13610
这个issue类型是Performance问题，主要涉及的对象是在使用TP8和PP2配置时测试DeepSeekR1模型时出现的TTFT急剧增加问题，可能导致性能下降。

https://github.com/vllm-project/vllm/issues/13609
这是用户询问如何从OpenAI Embedding Client获取稀疏嵌入的问题，属于用户提出需求。用户主要关注的是如何获取稀疏嵌入，可能是因为在运行embedding模型时需要在在线服务器上使用。

https://github.com/vllm-project/vllm/issues/13608
这是一个bug报告，涉及的主要对象是在编译PyTorch和torchvision过程中遇到了导入错误。原因是使用了特定的编译选项导致了undefined symbol错误。

https://github.com/vllm-project/vllm/issues/13607
这个issue是一个bug报告，涉及到使用vLLM在2*8H100环境下出现错误，可能是由于设置了一些环境变量导致了启动vllm serve时出现错误。

https://github.com/vllm-project/vllm/issues/13606
这是一个用户提出需求的issue，主要涉及编译过程中的并行任务限制，用户希望去除对NVCC_THREADS和MAX_JOBS的限制，以提高编译加速度。

https://github.com/vllm-project/vllm/issues/13605
这是一个代码优化类的Issue，主要涉及宏参数重命名和一致性调整。原因为确保代码库中参数命名的一致性，提高代码可读性和可维护性。

https://github.com/vllm-project/vllm/issues/13604
这个issue是用户提出需求类型的，主要涉及vllm的使用和集成问题。用户想要了解在特定模型推断过程中如何集成vllm，可能由于缺乏相关知识或指导而希望得到帮助。

https://github.com/vllm-project/vllm/issues/13603
这是一个关于bug报告的issue，主要涉及的对象是vllm在使用cpu推断时，kv缓存的物理内存空间是否预先分配。这个issue发生的原因可能是vllm在特定环境下出现了内存空间分配不正确的问题。

https://github.com/vllm-project/vllm/issues/13602
这是一个软件升级类型的issue，主要对象是`transformers`库，用户提出了升级至版本4.49以获得对Qwen2.5VL的支持。

https://github.com/vllm-project/vllm/issues/13601
这个issue类型是需求提出，该问题单涉及的主要对象是V1版本的vllm模块，由于需求让 "speculative decoding configuration" 变得像编译配置一样具有层次结构。

https://github.com/vllm-project/vllm/issues/13600
这是关于代码错误的bug报告，涉及对象是在加载模型时发生的格式切换以及需使用safetensors的问题。

https://github.com/vllm-project/vllm/issues/13599
这个issue是一个提出需求的类型，主要涉及的对象是`memory_profiling`模块的内存管理函数。由于需要将内存管理函数重构至Platform并在每个XxxPlatform子类中实现，以使`memory_profiling`具备跨平台性。

https://github.com/vllm-project/vllm/issues/13598
这是一个用户需求类型的issue，主要涉及如何在Docker环境下使用benchmark测试vllm性能的问题。这个问题可能由于缺乏官方docker脚本导致无法使用benchmark而产生。

https://github.com/vllm-project/vllm/issues/13597
这是一个Bug报告，关于在运行vLLM项目RLHF示例脚本时出现的CUDA GPU不可用的问题。

https://github.com/vllm-project/vllm/issues/13596
这是一个bug报告，主要涉及KV cache size的打印格式修改问题。

https://github.com/vllm-project/vllm/issues/13595
这是一个性能优化的issue，涉及的主要对象是"MROPE计算"，问题在于试图通过创建更高效的向量化代码来优化计算速度时，但是由于创建张量/np.array的开销大于优化带来的性能提升。

https://github.com/vllm-project/vllm/issues/13594
这个issue是关于bug修复的，主要涉及V1内存分析工具中V0采样逻辑的集成，但未包括拒绝采样器，问题由于未包含拒绝采样器而导致的症状。

https://github.com/vllm-project/vllm/issues/13593
这是一个bug报告类型的issue，主要涉及vllm在CPU实例上发生的错误。原因是在CPU实例上出现了`AttributeError: '_OpNamespace' '_C' object has no attribute 'silu_and_mul'`的错误。

https://github.com/vllm-project/vllm/issues/13592
这是一个版本更新的类型，用户要求将 transformers 版本升级到 v4.49.0。

https://github.com/vllm-project/vllm/issues/13591
这是一个功能需求问题，涉及数据并行通信的设置。由于需要探索数据并行在深度学习和moe模型中的应用，因此在这个PR中创建了必要的通信频道，但还未设计最终的用户界面。

https://github.com/vllm-project/vllm/issues/13590
这是一个bug报告，主要涉及Marlin kernel在多GPU情况下不能正常工作的问题。

https://github.com/vllm-project/vllm/issues/13589
这是一个bug报告，主要涉及Neuron特定性能问题，导致当并发请求数量大于等于`max_num_seqs`时Neuron性能迅速下降。

https://github.com/vllm-project/vllm/issues/13588
这是一个功能需求的issue，主要涉及集成 torchao 作为一个量化选项到 vllm 中。这个需求由于需要新增支持torchao作为一种量化选项，对性能进行测试以及与现有量化方法进行对比而产生。

https://github.com/vllm-project/vllm/issues/13587
这个issue属于bug报告类型，主要涉及V1版本的Sampler模块，问题是由于温度应用过程中执行了不必要的操作。

https://github.com/vllm-project/vllm/issues/13586
这是一个bug报告，涉及到CPU上的all-reduce实现问题，由于使用torch.distributed库中的all_reduce方法不会返回allreduced张量，导致该问题的产生。

https://github.com/vllm-project/vllm/issues/13585
这是一个bug报告类型的issue，涉及到模块'xformers'缺失的问题。原因可能是代码中引用的'modules'模块拼写错误导致模块无法识别。

https://github.com/vllm-project/vllm/issues/13584
这个issue是关于bug报告，涉及的主要对象是Paligemma的merged multimodal processor。由于缺少newline和paligemma 2支持，需要在processor中进行相应的修改来解决这些问题。

https://github.com/vllm-project/vllm/issues/13582
这是一个关于技术问题的Bug报告，涉及VLLM0.7.2服务器无法在8xNVIDIA-H200上启动DeepSeek-R1模型和CUDA内存不足导致服务关闭的问题。

https://github.com/vllm-project/vllm/issues/13581
这是一个Bug报告，主要涉及的对象是使用Qwen 2.5-VL模型和TP=2时的GPU内存清零问题，可能由于代码逻辑问题导致某一块GPU卡内存清零。

https://github.com/vllm-project/vllm/issues/13580
这是一个bug报告，主要涉及到vllm的docker image以及关于FRP的漏洞扫描问题，用户提出关于docker image安全性的疑虑。

https://github.com/vllm-project/vllm/issues/13579
这是一个Bug报告，涉及的主要对象是vLLM模型在加载检查点时出现了模型架构不被识别的错误。

https://github.com/vllm-project/vllm/issues/13578
这个issue类型是bug报告，主要涉及的对象是spec decode worker，由于在多节点设置下初始化spec_decode_sampler时未正确处理设备序号，导致出现了问题。

https://github.com/vllm-project/vllm/issues/13577
该issue属于性能优化类型，涉及的主要对象是ROCm下的Mi300混合处理器。由于配置设置不佳，可能导致性能表现不佳，用户提出了更新更好的配置设置以优化性能。

https://github.com/vllm-project/vllm/issues/13576
这个issue类型是一个实验性任务调度问题，涉及主要对象是测试作业的队列。原因是为了在amd_mi300队列中切换测试作业，无需合并。

https://github.com/vllm-project/vllm/issues/13575
这是一个功能需求报告，主要涉及改进使用mo_wna16核作为CompressedTensorsWNA16MoEMethod的后端。由于当前Marlin MoE核对于存在许多小专家的情况不够高效，因此希望将mo_wna16核作为默认版本应用于特定情况。

https://github.com/vllm-project/vllm/issues/13574
这是一个关于Docker环境安装问题的bug报告，涉及主要对象为VLLM代码版本2025.2.19，由于CUDA版本冲突导致在Docker环境下安装出错。

https://github.com/vllm-project/vllm/issues/13573
这是一个bug报告，涉及到在HF上原始示例模型名称被更改的问题，可能是由于HF上模型名称变更导致引起的bug。

https://github.com/vllm-project/vllm/issues/13572
这是一个需求类型的issue，主要涉及在CI中为AMD添加AWS凭证。

https://github.com/vllm-project/vllm/issues/13571
这个issue是关于软件功能的改进类型的，涉及主要对象为实现nvfp4 cutlass gemm的功能。由于Cutlass 3.8尚未正式发布，导致该功能的实现需要使用占位函数进行构建。

https://github.com/vllm-project/vllm/issues/13570
该issue是关于增加Quantization测试模型到S3列表的改进提议，属于新功能需求类型，涉及的主要对象是测试用的模型。

https://github.com/vllm-project/vllm/issues/13569
该问题单属于技术优化，主要涉及使用uv python替代ppa:deadsnakes/ppa来安装Python，因为部分Python apt命令在某些PR上出现超时现象。

https://github.com/vllm-project/vllm/issues/13568
这是一个功能需求，该问题主要涉及 HTTP 服务器的模型参数设置。由于每次请求都需要指定模型名称，导致服务端与客户端之间的耦合性过高，因此希望将模型参数设置为可选项，当未提供参数时，默认使用服务器上的第一个可用模型。

https://github.com/vllm-project/vllm/issues/13567
这是一个用户提出需求的类型的issue，主要涉及VLLM使用在线服务器的分类任务支持。由于目前仅支持离线推断，希望能够在Kubernetes中托管VLLM在线推断服务器。

https://github.com/vllm-project/vllm/issues/13566
这是一个改进建议类型的issue，主要涉及的对象是Dockerfile。由于使用uv库，构建时间从33分钟改进到了28.5分钟。

https://github.com/vllm-project/vllm/issues/13565
这个issue是一个BugFix类型的问题，主要涉及V1版本的LLM程序在终止时出现错误回溯的情况。由于PR https://github.com/vllmproject/vllm/pull/13060 引入了该问题。

https://github.com/vllm-project/vllm/issues/13564
这是一个bug报告，该问题涉及S3路径的使用，由于路径格式不正确导致问题。

https://github.com/vllm-project/vllm/issues/13563
这是一个Bug报告，主要涉及的对象是vllm中的xgrammar模块。由于缺少共享对象文件，导致使用xgrammar生成结构化提示时出现了错误。

https://github.com/vllm-project/vllm/issues/13562
这是一个bug报告，主要涉及quantization skip modules在某些模型中失效导致初始化线性方法不正确的问题。

https://github.com/vllm-project/vllm/issues/13561
这是一个bug报告，主要涉及到修复版权年份显示的问题。原因可能是代码中设定的版权年份未自动获取当前年份导致的。

https://github.com/vllm-project/vllm/issues/13560
这是一个功能改进类型的issue，主要涉及的对象是 ROCm 的 MI300A 编译目标。原因是移除了 gfx940 和 gfx941 目标，推荐使用 MI300X 的 gfx942 目标。

https://github.com/vllm-project/vllm/issues/13559
这是一个Bug报告，涉及到vllm库中Pooler中的Index Out of Range Bug，由于尝试使用返回的token IDs来索引hidden_states张量导致错误。

https://github.com/vllm-project/vllm/issues/13558
这个issue是用户反馈问题，主要涉及多节点vllm服务的文档不清晰，导致用户无法在容器内执行必要的命令而无法启动多节点服务。

https://github.com/vllm-project/vllm/issues/13557
这是一个bug报告，涉及的主要对象是在使用vLLM和张量并行ism运行DeepSeek R1模型时遇到worker注册失败的问题。问题可能是由于环境配置或程序逻辑错误导致的。

https://github.com/vllm-project/vllm/issues/13556
这是一个Bug报告，主要涉及LLM模型在增加根卷大小时导致磁盘空间不足的问题。

https://github.com/vllm-project/vllm/issues/13555
这是一个与代码优化相关的issue，主要涉及到模型定义中未使用的参数的清理，原因是为了简化模型代码并避免进一步的复杂性。

https://github.com/vllm-project/vllm/issues/13554
这是一个文档改进类的issue，主要涉及到vLLM的用户使用文档。由于文档误导，导致用户在使用性能分析时出现错误。

https://github.com/vllm-project/vllm/issues/13553
这是一个bug报告，涉及vllm中使用logits processors时出现的并行化生成问题。

https://github.com/vllm-project/vllm/issues/13552
这是一个 bug 报告，主要涉及到 VLLM 中的一个 issue。问题可能由于设置 tp_size=16 导致异常。

https://github.com/vllm-project/vllm/issues/13551
这是一个关于安装环境依赖不匹配的bug报告，涉及主要对象是`xformers`包。用户提出了版本约束不一致的问题，请求放宽`xformers`的版本兼容性。

https://github.com/vllm-project/vllm/issues/13550
这是一个bug报告类型的issue，涉及主要对象是输入处理器，原因是新的输入处理器导致了mllama错误，需要对编码器解码器预热进行调整以解决这个问题。

https://github.com/vllm-project/vllm/issues/13549
这个issue类型为恶意内容/欺诈，主要涉及对象为Facebook账号。由于出现了恶意行为导致本质上违反了平台规定和用户隐私安全，需要防范和解决。

https://github.com/vllm-project/vllm/issues/13548
该issue类型为修复bug，主要涉及XPU中setuptools版本的问题，由于setuptools版本不匹配导致了错误的症状。

https://github.com/vllm-project/vllm/issues/13547
这是一个恶意链接或网络安全问题，用户可能受到诈骗或信息泄露的风险。

https://github.com/vllm-project/vllm/issues/13546
这个issue是钓鱼攻击类型，目标对象是Facebook账户用户。由于不当的行为和欺骗手段，引诱用户点击恶意链接并可能导致账号信息泄露或被盗。

https://github.com/vllm-project/vllm/issues/13545
这是一个bug报告，涉及主要对象为https://wheels.vllm.ai/nightly。由于无法从指定链接中选择版本，导致了无法检查到相关内容的错误。

https://github.com/vllm-project/vllm/issues/13544
这是一个bug报告，主要涉及静态元数据迁移到`pyproject.toml`导致未能在构建时创建`vllm/_version.py`文件。

https://github.com/vllm-project/vllm/issues/13543
这是一个涉及到安全问题和欺诈的issue，用户可能在寻求关于恢复Facebook账号以及获取免费访问的自动化方法。

https://github.com/vllm-project/vllm/issues/13541
这是用户提出需求类型的issue，主要涉及Facebook账户密码丢失或遭到盗用问题，可能由于虚假信息或网络安全问题导致。

https://github.com/vllm-project/vllm/issues/13540
这个issue类型为用户需求提出，主要涉及对象是openai api。由于用户需求开发团队支持在openai协议api中添加 image_embeds 并通过提出的方案支持prefix caching，以提高传递效率。

https://github.com/vllm-project/vllm/issues/13539
这个issue类型是bug报告，涉及的主要对象是chunked prefill scheduler，由于之前的代码在处理preemption/swap时可能会错误地改变请求顺序，导致了chunked prefill scheduler使用过多swap的bug。

https://github.com/vllm-project/vllm/issues/13538
这是一个用户可能正在寻求非法入侵Facebook账户的问题，而不是一个真正的bug报告。

https://github.com/vllm-project/vllm/issues/13537
这个issue是一个bug报告，涉及的主要对象是HACKEáR Insta Gratis软件。由于未明确描述bug内容，导致无法确定具体问题或帮助请求。

https://github.com/vllm-project/vllm/issues/13536
这个issue类型是性能问题报告，涉及的主要对象是关于VLLM模型中enforce_eager参数设置对长文档输入性能指标的影响。导致该问题的原因是在输入超过一定长度（>16k）时，设置enforce_eager=False会导致输出性能下降，而设置为True则可能导致准确性问题。

https://github.com/vllm-project/vllm/issues/13535
这是一个Bug报告，涉及的主要对象是Ray+vllm运行时的软件环境信息。由于环境中的PyTorch、CUDA版本与设备类型等因素，可能导致程序运行后崩溃的问题。

https://github.com/vllm-project/vllm/issues/13534
该issue为bug报告，主要涉及用户在执行代码时出现的程序错误。由于环境配置错误或代码编写问题导致了程序异常，用户需要解决这个问题。

https://github.com/vllm-project/vllm/issues/13533
这个issue属于功能需求类型，主要涉及Qwen2.5VL的在线推断中无法通过OpenAI接口传递`mm_processor_kwargs`参数，由此导致无法准确计算`second_pre_grid_t`值的问题。

https://github.com/vllm-project/vllm/issues/13532
这个issue类型为欺诈，关键对象是Instagram账号，用户可能寻求非法获取Instagram账号的方式。

https://github.com/vllm-project/vllm/issues/13531
这是一个Bug报告，主要涉及Instagram账号被盗。由于用户可能点击了恶意链接导致页面跳转，从而遭受账号盗取的风险。

https://github.com/vllm-project/vllm/issues/13530
这是一个Bug报告，涉及主要对象为调用工具（Tool calling）。该问题由接口调用中传递的参数错误导致程序无法成功调用工具函数而出现bug。

https://github.com/vllm-project/vllm/issues/13529
这个issue类型是垃圾广告或欺诈相关问题，涉及到Instagram账号安全问题。

https://github.com/vllm-project/vllm/issues/13528
这个issue属于安全问题报告，主要涉及到Instagram账号安全。原因可能是用户遭遇了账号被盗或被黑客入侵的问题。

https://github.com/vllm-project/vllm/issues/13526
这个issue属于其他类型，涉及的主要对象是Instagram账户安全。由于该issue提供涉及恶意行为的链接，可能会导致用户隐私泄露或账户被盗。

https://github.com/vllm-project/vllm/issues/13525
该issue为用户提出需求，并请求vllm支持facebook/contriever模型，主要涉及对象为新增模型支持。由于该模型在社区中较为流行，用户希望vllm能够支持该模型，以增加功能和使用体验。

https://github.com/vllm-project/vllm/issues/13523
这个issue类型是骗取用户信息，钓鱼攻击。该问题单涉及的主要对象是Instagram用户。由于存在欺诈链接，导致骗取用户点击并输入个人登录信息等敏感信息。

https://github.com/vllm-project/vllm/issues/13521
这是一个bug报告类型的issue，涉及的主要对象是vLLM API server，用户遇到了无法在ray集群上提供服务的问题，可能由于参数配置或代码逻辑错误导致。

https://github.com/vllm-project/vllm/issues/13519
该问题类型为恶意行为报告，主要对象是Instagram账户安全。由于违规内容引起，导致该账户可能受到黑客攻击或信息泄露风险。

https://github.com/vllm-project/vllm/issues/13518
这是一个安装问题，涉及的主要对象是vllm软件。由于pip安装vllm时出现了失败的情况。

https://github.com/vllm-project/vllm/issues/13517
这个issue是关于用户需求的，主要涉及到vllm是否支持在GPU和CPU上混合部署，并提出了希望在GPU和CPU之间分配权重的功能需求。

https://github.com/vllm-project/vllm/issues/13516
这是一个Bug修复类型的Issue，主要涉及VLM中处理processor_kwargs的问题，导致多模态处理器初始化时无法正确传递kwargs。

https://github.com/vllm-project/vllm/issues/13515
这是一个Bug报告，涉及的主要对象是Colocate示例引发的错误，由于使用了类似方法运行LLM而导致错误的产生。

https://github.com/vllm-project/vllm/issues/13514
这是一个Bug报告，主要涉及vLLM中的正特征层。由于索引不一致导致加载的图层数量出现错误。

https://github.com/vllm-project/vllm/issues/13513
这是一个Bug报告，主要涉及的对象是VLLM中部署qwen2.5-vl模型时无法传递多个图像造成的错误。

https://github.com/vllm-project/vllm/issues/13512
这个issue类型是bug报告，主要涉及的对象是性能基准测试，由于路径问题导致预合并基准测试步骤分割不正确。

https://github.com/vllm-project/vllm/issues/13511
这是一个文档类的issue，主要对象是关于“paligemma”的澄清说明。

https://github.com/vllm-project/vllm/issues/13510
这个issue是关于bug报告，涉及到VLLM启动时出现误导性的错误消息。问题是由于模型路径指定错误导致的。

https://github.com/vllm-project/vllm/issues/13509
这是一个需求提出类的issue，主要涉及性能基准测试的前置准备问题。

https://github.com/vllm-project/vllm/issues/13508
该issue类型是需求反馈，主要涉及部署优化建议，由于存在低GPU KV缓存使用率、低运行请求和高挂起请求，可能由于超参数设置不合理导致。

https://github.com/vllm-project/vllm/issues/13507
这是一个bug报告，主要涉及V1模型运行器在初始内存分析中未考虑或运行采样器的问题，导致内存分析不准确。

https://github.com/vllm-project/vllm/issues/13506
这是一个功能增强类型的问题，主要涉及API Server中端口号范围验证，可能是为了防止用户输入错误的端口号导致程序出错。

https://github.com/vllm-project/vllm/issues/13505
这是一个bug报告，主要涉及的对象是guided decoding backend的backend-specific options。由于未提供正确的支持选项，用户可能会遇到从一个backend到另一个backend的切换问题。

https://github.com/vllm-project/vllm/issues/13504
这个issue是关于要实现vllm:lora_requests_info metric，属于需求提出类型，主要涉及LoRA metrics的兼容性问题，原因是要保留现有用户，并且可能存在更优解决方案。

https://github.com/vllm-project/vllm/issues/13503
这是一个新功能需求类型的 issue，主要涉及添加了ROCm下的两种不同的 Mixture of Experts (MoE) configurations。

https://github.com/vllm-project/vllm/issues/13502
这是一个bug报告，主要涉及的对象是Ray。这个问题是由于TP与Ray配合时无法正常终止导致的。

https://github.com/vllm-project/vllm/issues/13501
这个issue是关于bug报告，主要涉及vLLM包版本检测问题，由于导致平台无法被检测。

https://github.com/vllm-project/vllm/issues/13500
这是一个用户提出需求的类型，主要涉及DeepSeek MTP在V1架构中的支持。由于新V1架构的变动，用户希望该功能能够迁移过来。

https://github.com/vllm-project/vllm/issues/13499
这是一个用户提出需求的issue，主要涉及V1 rejection sampler的扩展支持随机采样功能。由于目前只支持贪婪采样，用户希望能够实现随机采样。

https://github.com/vllm-project/vllm/issues/13498
这是一个优化建议的issue，主要涉及到V1版本的拒绝采样器的性能问题。

https://github.com/vllm-project/vllm/issues/13497
这是一个用户提出需求的issue，主要涉及vLLM在工具调用方面的支持问题，用户想知道是否可以根据工具调用的输出来实现自动完成功能。

https://github.com/vllm-project/vllm/issues/13496
这是一个关于TPU multimodal模型支持的issue，主要涉及LLavastyle multimodal models的可用性和正确性问题。由于模型传入的新形状的图像/视频/音频数据会在运行时强制编译，所以这不涉及预编译编码器前向传递的问题。

https://github.com/vllm-project/vllm/issues/13495
这是一个需求性的issue，主要涉及的对象是HTTP服务器的SSL Key Rotation。由于某些生产环境需要定期轮换TLS密钥/证书，因此实现了使用watchfiles异步监视ssl密钥、证书和CA文件的更新，并在检测到更改时更新SSLContext的功能。

https://github.com/vllm-project/vllm/issues/13494
这是一个bug报告，该问题主要涉及到VLLM在托管Qwen2.5 VL 72B时崩溃并出现异常"Set changed size during iteration"。可能是由于环境配置或代码逻辑错误导致。

https://github.com/vllm-project/vllm/issues/13492
这是一个bug报告，主要涉及到移除关于`--use-v2-block-manager`的悬空引用。由于该参数被删除，导致相关引用出现了问题。

https://github.com/vllm-project/vllm/issues/13491
这个issue类型是用户提出需求，询问问题，主要涉及到在部署到Azure托管机器学习实时端点时如何编写评分脚本，以及如何与vllm集成。造成这个问题的原因是用户不清楚如何正确编写评分脚本以接收OpenAI API兼容调用。

https://github.com/vllm-project/vllm/issues/13490
这是一个bug报告，涉及主要对象是V1中的TP + PP，由于Ray 2.41.0的兼容性问题导致无法正常工作，报错显示无法解析。

https://github.com/vllm-project/vllm/issues/13489
这个issue类型是需求提出，主要涉及的对象是改进关于截断在嵌入和得分任务中的使用体验。原因是为了控制嵌入和得分函数中的截断参数，并增进开发者体验。

https://github.com/vllm-project/vllm/issues/13488
这是一个bug报告，主要涉及的对象是一个通过错误创建的内容。由于该内容是错误创建的，可能导致了一些异常情况或者需要相应的修正措施。

https://github.com/vllm-project/vllm/issues/13487
这是一个bug报告，涉及的主要对象是调试脚本。由于allreduce操作被改为out-of-place方式，但调试脚本没有相应更新，导致bug。

https://github.com/vllm-project/vllm/issues/13486
这是一个关于性能优化的issue，主要涉及实现专家缓存策略。由于复杂的FusedMOE行为和底层内核的问题，导致实现上遇到困难。

https://github.com/vllm-project/vllm/issues/13485
该issue类型为Bug报告，主要涉及vllm在加载模型时发生的错误。由于环境设置问题导致无法从s3兼容存储加载模型，用户正在寻求解决这一问题。

https://github.com/vllm-project/vllm/issues/13484
这个issue类型是需求提出，主要对象是添加Qwen2.5模型的工具解析器，用户提出需要支持特定的Qwen2.5模型。

https://github.com/vllm-project/vllm/issues/13483
该issue是关于需求提出的，主要涉及前端实现中的Tool Calling API，用户提出支持`tool_choice='required'`的功能。

https://github.com/vllm-project/vllm/issues/13482
这是一个bug报告，主要涉及到vllmWorkerProcess异常终止的问题。导致该问题的原因可能是由于在nsight capture nvtx时，使用了多个processing pipeline导致程序异常。

https://github.com/vllm-project/vllm/issues/13481
这是一个关于增加功能的issue，主要涉及V1版本的sampler，由于缺少allowed_token_id支持而需要添加这一功能。

https://github.com/vllm-project/vllm/issues/13480
这个issue属于改进优化类型，主要涉及构建和依赖关系的定制化处理。由于当前构建设置混乱，使用了不同方法和途径处理构建依赖，导致需求不清晰且过时方法使用，因此需要通过定制构建后端和动态构建依赖解决这些问题。

https://github.com/vllm-project/vllm/issues/13479
这是一个关于bug的报告，涉及到的主要对象是vLLM在云TPU上出现XLA错误。由于torch版本不兼容导致无法正常运行，用户寻求解决方案或修复方法。

https://github.com/vllm-project/vllm/issues/13478
这个issue类型是功能需求，主要涉及的对象是kv cache int8，用户提出了关于在线量化kv cache int8计算的需求。

https://github.com/vllm-project/vllm/issues/13477
这是一个Bug报告，涉及的主要对象是在部署两个vllm服务到相同一批GPU上时推理速度变慢的问题。导致推理速度变慢的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/13476
这个issue属于用户需求类型，主要涉及的对象是TPU后端支持的模型列表，用户想要了解vllm中支持的模型。

https://github.com/vllm-project/vllm/issues/13475
这是一个Bug报告类型的issue，涉及主要对象为Model architectures中的一个特定架构MllamaForConditionalGeneration。该问题是因为当前不支持MllamaForConditionalGeneration模型架构，导致数值错误的提示。

https://github.com/vllm-project/vllm/issues/13474
这是一个bug报告，主要涉及的对象是VLLM中的grouped topk计算逻辑。由于逻辑错误导致在特定条件下选择专家存在问题，可能导致选择不正确或次优。

https://github.com/vllm-project/vllm/issues/13473
这是一个Bug报告。该问题涉及到 chat_with_tools.py 文件中函数调用输出中存在额外的 [TOOL_CALLS] 前缀。由于环境信息收集过程中的一些错误或异常，导致函数调用输出中出现了额外的前缀。

https://github.com/vllm-project/vllm/issues/13472
这个issue类型是用户提出需求，该问题单涉及的主要对象是实现真正的PP（ParameterServer）与Ray执行程序之间的结合。

https://github.com/vllm-project/vllm/issues/13471
这是一个bug报告，主要涉及到deepseek r1 + vllm (v0.7.2)中torch.compile错误，可能是由于lru_wrapper和dynamic问题导致的。

https://github.com/vllm-project/vllm/issues/13470
这个issue属于bug报告类型，主要涉及的对象是vllm项目中的fused_moe.py文件。原因是在函数get_config_dtype_str中存在一个拼写错误，导致配置命名不一致，需要修正以解决问题。

https://github.com/vllm-project/vllm/issues/13469
该issue类型是bug报告，主要涉及的对象是代码中的`fused_moe.py`文件中的`get_config_dtype_str`函数，由于一个字符串命名错误导致配置名称不一致，需要修正。

https://github.com/vllm-project/vllm/issues/13468
这是一个bug报告类型的issue，涉及的主要对象是vLLM的attention bias初始化问题。由于attention bias初始化在默认设备上而非Query/Key/Value张量上，导致在特定GPU环境下出现错误。

https://github.com/vllm-project/vllm/issues/13467
这个issue类型是用户提出需求，并涉及到如何评估模型和加速卡性能以及如何使用vllm进行推理。由于环境不同，用户想了解如何在benchmark性能测试中区分不同方法所带来的影响。

https://github.com/vllm-project/vllm/issues/13466
这是一个bug报告，涉及主要对象是vLLM下的Mamba模型。由于vLLM实现中使用了16位的`input_t`而不是通常的fp32，导致SSM states的精度差异，降低了生成文本的质量。

https://github.com/vllm-project/vllm/issues/13465
这是一个用户提出需求的issue，主要涉及到前端支持转录API并具有语言检测功能。由于之前的代码对于未知语言的音频进行识别时限制为英文，因此用户提出希望支持未知语言的音频识别，这个issue意在引入语言检测功能来解决这个问题。

https://github.com/vllm-project/vllm/issues/13464
这是一个Bug报告，主要对象是vllm库中的LLM模块，在设置"max_tokens=1"时导致内存泄漏。

https://github.com/vllm-project/vllm/issues/13463
这是一个用户提出需求的类型的issue，该问题涉及的主要对象是设置服务中不同请求的最大像素限制。由于Qwen2VL/Qwen2.5VL需要动态处理像素，用户希望能够设置不同请求的最大像素限制。

https://github.com/vllm-project/vllm/issues/13462
这是一个bug报告，主要涉及的对象是AWQ模块，在程序环境信息中显示出AWQ不支持4位处理。

https://github.com/vllm-project/vllm/issues/13461
这是一个功能需求的issue，主要涉及Whisper transcription API endpoint的更新，涉及语言识别功能的临时解决方案。

https://github.com/vllm-project/vllm/issues/13460
这是一个bug报告，主要涉及的对象是vllm项目下的ROCm功能。由于缺少amdsmi import，导致CI在其他平台上出现了问题。

https://github.com/vllm-project/vllm/issues/13459
这个issue是一个bug报告，主要涉及的对象是工具调用时可能出现的参数信息丢失问题，可能由于删除参数最后字符导致。

https://github.com/vllm-project/vllm/issues/13458
这是一个bug报告。该问题单涉及的主要对象是本地模型加载过程中产生的噪音错误日志。这个bug是由于噪音错误日志在本地模型加载期间输出导致的。

https://github.com/vllm-project/vllm/issues/13457
这是一个用户提出需求的类型issue，主要涉及支持xTTSv2，可能是由于当前不支持tts模型（encoderdecoder v1 model）导致用户寻求相应的帮助。

https://github.com/vllm-project/vllm/issues/13456
这个issue属于bug报告类型，主要涉及的对象是PyTorch 2.6，由于开启sleep mode导致Basic Correctness测试用例中的segfault，但另一个测试用例通过，用户提到本地测试证实在PyTorch 2.6中修复了这个bug。

https://github.com/vllm-project/vllm/issues/13455
该issue是关于代码重构和测试覆盖率改进的，不是bug报告。主要涉及Neuron内核测试的模块化和测试逻辑的改进。

https://github.com/vllm-project/vllm/issues/13454
这个issue属于功能需求类型，主要涉及的对象是模型推理支持问题。由于MultiBatch推理时`finished_requests_ids`问题导致的bug。

https://github.com/vllm-project/vllm/issues/13453
该issue为用户提出需求，询问如何在离线推理中使用vllm的pipeline并提供具体示例。

https://github.com/vllm-project/vllm/issues/13452
这是一个bug报告，涉及的主要对象是使用VLLM072服务器启动MiniCPM-O-26服务时出现视频和音频输出模型推断错误。原因是缺少vllm[video]模块导致视频支持失败。

https://github.com/vllm-project/vllm/issues/13451
这是一个bug报告，用户在使用vllm/vllmopenai docker镜像时遇到数值错误的问题。

https://github.com/vllm-project/vllm/issues/13450
这是一个bug报告，涉及的主要对象是LoRA路径处理。由于`lora_path`在try块中定义，如果在该变量定义之前发生任何错误，就无法获取所需的错误信息，其中包括LoRA路径和名称信息。

https://github.com/vllm-project/vllm/issues/13449
这个issue是用户提出需求类型，主要涉及JSON schema中的enum属性。由于缺乏对enum属性的处理，导致需要添加轮廓的回退方案。

https://github.com/vllm-project/vllm/issues/13448
这个issue是一个Bug报告，主要涉及了在使用guided decoding进行推断时出现单个字符生成的问题。这个问题的根本原因可能是模型在第一次推断后无法正确处理后续推断数据。

https://github.com/vllm-project/vllm/issues/13447
这是一个用户提出需求类的issue，涉及对象为DeepSeek-R1-UD-IQ1_S(1.58bit-guff)，由于gpu设备不够，导致无法支持该需求。

https://github.com/vllm-project/vllm/issues/13446
这是一个bug报告，涉及VLLM模型在加载检查点时无法识别架构类型导致数值错误的问题。

https://github.com/vllm-project/vllm/issues/13445
这是一个bug报告，主要涉及代码中的排名信息没有正确传递给工作节点，导致工作者无法正确获取排名信息。

https://github.com/vllm-project/vllm/issues/13444
这是一个bug报告类型的issue，该问题单主要涉及程序中打印出`None`的情况。导致该bug的原因是在chat模板未被提供时仍会打印`None`。

https://github.com/vllm-project/vllm/issues/13443
这个issue类型是bug报告，涉及主要对象是代码中的`model_weights`属性和`maybe_pull_model_tokenizer_for_s3`方法。由于`model_weights`属性未在初始化方法中定义，以及`maybe_pull_model_tokenizer_for_s3`方法未使用`model_weights`属性，导致需要对代码进行调整。

https://github.com/vllm-project/vllm/issues/13442
这是一个用户提出需求的问题，主要涉及vLLM的GPU内存使用情况，用户想了解在拥有足够GPU内存的情况下，是启动单个vLLM实例更好还是启动多个实例进行负载平衡更好。

https://github.com/vllm-project/vllm/issues/13441
这是一个用户提出需求的issue，主要涉及到添加对Ovis VLM系列模型的支持。这个问题的原因是现有系统不兼容vLLM实现，导致用户无法加载模型。

https://github.com/vllm-project/vllm/issues/13440
这个issue类型为代码回退，主要涉及到VLLM项目的工作基础构建，由于之前的提交导致了问题，需要进行回退。

https://github.com/vllm-project/vllm/issues/13439
这是一个关于性能优化和参数调整的issue，涉及主要对象是MLA核心功能在AMD GPUs上的性能改进与MI300X上的MoE配置调整。

https://github.com/vllm-project/vllm/issues/13438
这是一个bug报告类型的issue，涉及主要对象为使用ROCm的AMD GPUs。由于硬编码的设备名称问题导致了Trition配置中无法正确区分不同型号的AMD GPU，需要使用amdsmi获取正确的设备名称。

https://github.com/vllm-project/vllm/issues/13437
这个issue是关于bug报告，涉及的主要对象是"TP with Ray"。由于使用Ray作为分布式执行后端时，主进程无法正常终止，导致问题出现。

https://github.com/vllm-project/vllm/issues/13436
这个issue类型是bug报告，涉及的主要对象是V1版本的VLLM项目。由于API改动，需要将Ray的版本固定在2.41.0或2.40.0，并明确指出Pipeline parallelism中需要使用`ray[adag]`而不是`ray[default]`。

https://github.com/vllm-project/vllm/issues/13435
这是一个Bug报告，主要涉及到V1版本的TP功能，报告了在使用torch编译缓存时出现了错误。

https://github.com/vllm-project/vllm/issues/13434
这是一个用户提出需求的 issue，主要涉及的对象是 vllm 库和 torch 包。导致这个 issue 的原因是 vllm v0.7 版本似乎只支持 torch==2.5.1，用户希望能够兼容 torch==2.6。

https://github.com/vllm-project/vllm/issues/13433
这是一个Bug报告，主要涉及到V1版本的Structured decoding中的FSM更新和匹配被请求状态所阻塞。导致这个问题的原因是_is_gramma_ready函数只在请求等待时返回True，而不在运行时返回True，从而导致了Guided decoding被禁用。

https://github.com/vllm-project/vllm/issues/13432
这个issue是一个功能需求的提出，主要涉及的对象是Jamba项目中的Scores和Weights，用户提出了需要为Scores和Weights添加显式的fp32精度的需求。

https://github.com/vllm-project/vllm/issues/13431
这个issue类型为bug报告，涉及修复了一个初始化错误导致NaN值的问题。

https://github.com/vllm-project/vllm/issues/13430
这是一个功能需求，主要涉及对象是JambaForSequenceClassification模块，用户提出希望在load_weights函数中将权重明确转换为float32的需求。

https://github.com/vllm-project/vllm/issues/13429
这是一个bug报告，主要涉及到V1中使用json_object时需要json_schema，而实际上应该是两者应该是分开的。

https://github.com/vllm-project/vllm/issues/13428
此issue类型为用户提出安装问题，主要对象是vllm安装环境和安装方式。由于用户可能遇到安装问题，需要寻求帮助或解决方案。

https://github.com/vllm-project/vllm/issues/13427
这个issue类型是安装问题报告，主要对象是vllm软件安装。用户提出了在安装过程中咨询帮助寻求支持。

https://github.com/vllm-project/vllm/issues/13426
这是一个用户提出需求的issue，主要涉及vllm的安装环境和安装方式。用户询问如何防止重复提交issue以及如何在线获取答案。

https://github.com/vllm-project/vllm/issues/13425
这是一个Bugfix类型的issue，主要涉及vLLM V1在使用Ada Lovelace GPUs和CUDA < 12.4构建时，针对FP8模型的问题，可能由于设置`TORCH_DEVICE_IDENTITY`方式问题导致输出数据异常或报错。

https://github.com/vllm-project/vllm/issues/13424
这是一个bug报告，涉及到vllm安装过程中的flashattention依赖问题。用户指出flashattention的"git submodule update"操作在离线安装时会存在问题，建议改进处理方式。

https://github.com/vllm-project/vllm/issues/13423
这是一个bug报告，涉及主要对象是torch.compile cache，可能由于缓存问题导致运行v1 benchmark时出现异常。

https://github.com/vllm-project/vllm/issues/13422
这个issue类型是bug报告，涉及的主要对象是GPU。由于并发请求处理时，GPU使用率会突然降至0的情况。

https://github.com/vllm-project/vllm/issues/13421
这个issue类型是功能需求，涉及主要对象是v1 LLMEngine，由于需求是实现v1 LLMEngine的并行采样支持。

https://github.com/vllm-project/vllm/issues/13420
这个issue是关于需求提出的，主要涉及V1中LLMEngine不支持并行采样，可能由于此功能缺失导致了用户的需求。

https://github.com/vllm-project/vllm/issues/13419
这是一个用户提出需求的类型，主要对象是vLLM v1 async engine，用户希望在AsyncLLM中实现并行采样的支持。

https://github.com/vllm-project/vllm/issues/13418
这是一个Bug报告，涉及到使用vllm时使用CUDA图时出现卡住的情况，主要原因是在特定情况下，当调度的标记数量不是8的倍数时会出现此问题。

https://github.com/vllm-project/vllm/issues/13417
这个issue是关于bug报告，涉及到中间张量数值未复制到缓存张量的问题，导致CUDA图无法正确工作。

https://github.com/vllm-project/vllm/issues/13416
这个issue属于功能需求类型，主要涉及的对象是AriaVisionTransformer和AriaTextModel，用户提出了添加SupportsQuant接口的需求。

https://github.com/vllm-project/vllm/issues/13415
这是一个关于使用Tokenizer的问题，用户在不使用Tokenizer时遇到了输出错误的情况。

https://github.com/vllm-project/vllm/issues/13414
这是一个关于功能需求的issue，涉及到在VLLM v1中添加prompt logprobs支持及与APC兼容性的问题。由于prompt logprobs和APC之间的兼容性挑战，导致了需要重新设计实现选择。

https://github.com/vllm-project/vllm/issues/13412
这是一个bug报告，涉及问题对象是vLLM中使用max_concurrency参数时，TTFT在输出序列长度增加时显著下降的情况。原因可能与inflight batching、max_concurrency操作方式或其他因素有关。

https://github.com/vllm-project/vllm/issues/13411
这是一个需求类型的issue，主要涉及的对象是github项目vllm下的xgrammar库。由于需要将xgrammar更新至0.1.13版本以支持Python 3.13，这是为了适配https://github.com/vllmproject/vllm/pull/13164，因此提出了这个issue。

https://github.com/vllm-project/vllm/issues/13410
该issue类型属于用户请教问题，涉及主要对象是如何使用vllm来分析多节点上的deepseek-r1，用户提出问题是关于如何使用profiler来进行推断的特定模型。

https://github.com/vllm-project/vllm/issues/13409
这个issue是一个功能需求，主要涉及prompt logprobs的支持问题，由于APC缓存机制与prompt logprobs的兼容性挑战，导致了请求失败的问题。

https://github.com/vllm-project/vllm/issues/13408
这是一个Bug报告，主要涉及vLLM实例在处理并发请求时响应泄漏的问题。

https://github.com/vllm-project/vllm/issues/13407
这是一个功能提案的issue，主要涉及vllm引擎在崩溃时输入元数据转储功能的添加。导致此功能提案的原因是之前实现的功能被移除，现在希望通过不同的方法来提供更多信息以帮助调试生产环境中的崩溃。

https://github.com/vllm-project/vllm/issues/13406
这是一个bug报告，涉及的主要对象是代码中的prompt_logprobs方法。由于未将prompt_logprobs方法从一个地方重构到另一个地方，导致在服务聊天过程中可能出现JSONResponse的错误。

https://github.com/vllm-project/vllm/issues/13405
这是一个Bug报告，涉及到vllm中的is_async_output_supported功能，用户在使用Qwen2.5VL7B模型进行推断时遇到了NotImplementedError异常。通过报错信息分析，可能是由于vllm版本和transformers版本不匹配导致的问题。

https://github.com/vllm-project/vllm/issues/13404
该issue属于bug报告类型，主要涉及Online prompt logprobs serving_chat的问题。原因可能是代码中存在逻辑错误或不符合预期的行为，导致相关功能无法正常工作。

https://github.com/vllm-project/vllm/issues/13403
这是一个bug报告类型的issue，涉及主要对象是`transformers` backend和multiproc executor。由于falsepositive loaded custom module导致在使用VLLM_WORKER_MULTIPROC_METHOD=spawn时，`transformers` backend无法正确加载自定义模块，解决了automap resolving的优化问题。

https://github.com/vllm-project/vllm/issues/13402
这是一个bug报告类型的issue，涉及到 vllm 项目中的 `SamplingType.BEAM` 引用问题，由于该引用被删除导致产生了死代码。

https://github.com/vllm-project/vllm/issues/13401
这是一个bug报告，主要涉及的对象是vllm库中的中文文本处理功能。由于某种原因导致处理批量数据时出现无法识别或者完全错误的情况。

https://github.com/vllm-project/vllm/issues/13400
这是一个关于功能需求的issue，主要涉及的对象是vLLM项目。由于当前版本尚不支持token级别的时间戳，用户提出了希望在whisper模型中支持token级别时间戳的功能需求。

https://github.com/vllm-project/vllm/issues/13399
这是一个关于功能需求的issue，主要涉及vllm模型加载时是否需要显式传递解析器参数的讨论。用户建议是否可以提供一个默认解析器，在自动选择工具时能够自动识别json格式的标记。

https://github.com/vllm-project/vllm/issues/13398
这是一个Bug报告，主要涉及到VLLM V1架构不支持V100显卡的问题，导致调用失败。

https://github.com/vllm-project/vllm/issues/13397
这是一个bug报告，主要涉及Qwen2.5VL在处理图像时出现的错误。错误原因是在一个请求中不支持多个图片提供。

https://github.com/vllm-project/vllm/issues/13396
这个issue是关于bug报告，主要涉及VLLM模型在分配multi-modal tokens到placeholders时出现数值错误的问题。

https://github.com/vllm-project/vllm/issues/13395
这个issue类型是需求提出，主要涉及的对象是vllm服务器。用户想要拒绝请求当vllm服务器繁忙时，可能由于未找到相关指导或问题而寻求帮助。

https://github.com/vllm-project/vllm/issues/13394
这是一个Bug报告，涉及到使用Lora Adapters时出现问题。由于版本0.7.2下num-scheduler-steps与VLLM_USE_V1=0配置不能正常工作，可能导致程序报错。

https://github.com/vllm-project/vllm/issues/13392
这是一个Bug报告类型的issue，涉及主要对象为在多GPU上运行Benchmark v1时出现数值错误导致程序崩溃。

https://github.com/vllm-project/vllm/issues/13391
这是一个Bug报告。主要涉及对象是vllm模型。由于top_p和temperature未能引入随机性，导致文本请求在不同位置时输出结果不一致的问题。

https://github.com/vllm-project/vllm/issues/13390
该issue为用户需求类型，主要涉及多任务支持的实现。用户提出了希望在vllm中实现Whisper的多任务功能，例如语言检测和转录，希望能够为转录功能添加语言检测或额外参数。

https://github.com/vllm-project/vllm/issues/13389
这是一个Bug报告，主要涉及DeepSeek R1 deployment panics when serving requests with cuda memory access，可能是由于CUDA内存访问导致的内存泄漏错误。

https://github.com/vllm-project/vllm/issues/13388
这个issue类型是bug报告，主要涉及GPU Placement Group Creation错误，导致无法创建符合资源请求的放置组。

https://github.com/vllm-project/vllm/issues/13387
这是一个bug报告，涉及主要对象为ppc64le (IBM POWER)，由于Chunk Prefill feature在ppc64le平台上出现故障。

https://github.com/vllm-project/vllm/issues/13386
该issue类型是用户提出需求，主要涉及的对象是关于vLLM下新推出的推理优化方案，用户询问关于引入新的分布式系统LLM推理优化方案的计划和方法。

https://github.com/vllm-project/vllm/issues/13385
这是一个功能需求的issue，主要涉及Connector API的迁移工作，用户希望将V0的连接器API迁移到V1，同时介绍了最新更改并提供了运行示例的方式。

https://github.com/vllm-project/vllm/issues/13384
这是一个Bug报告，涉及到使用VLLM_USE_MODELSCOPE时的问题。这个问题可能由于未正确使用huggingface_hub API导致了无法获取模型文件列表。

https://github.com/vllm-project/vllm/issues/13383
这是一个Bug报告，主要涉及的对象是处理content type的问题。这个问题是由于当前代码未正确处理content type中的多个可选参数导致的。

https://github.com/vllm-project/vllm/issues/13382
这个issue是关于bug报告，涉及主要对象是使用VLLM_USE_MODELSCOPE时访问huggingface_hub API出现连接超时导致无法获取模型文件列表的问题。

https://github.com/vllm-project/vllm/issues/13381
这是一个升级请求，主要涉及 CPU backend 到 torch-2.6，等待代码审查。原因是为了升级到 torch-2.6.0 版本。

https://github.com/vllm-project/vllm/issues/13380
这是一个bug报告，涉及的主要对象是VLM（Very Large Language Model）。由于field config可能依赖于一些必填字段，未先进行所需字段检查可能导致错误信息不够详细。

https://github.com/vllm-project/vllm/issues/13379
这是一个需求提出类型的issue，主要涉及的对象是在TPU上集成新的ragged paged attention kernel到vLLM v1。

https://github.com/vllm-project/vllm/issues/13378
这是一个bug报告，涉及主要对象是代码中的变量名和打印日志功能。由于使用了错误的变量名称和存在拼写错误，导致打印的日志内容不正确。

https://github.com/vllm-project/vllm/issues/13377
这个issue是一个功能需求，主要涉及到V1版本的核心功能，旨在通过在GPU缓存驱逐时将KV缓存块转移到CPU并在缓存命中时将其切换回GPU，旨在最小化不必要的数据交换所导致的性能开销。

https://github.com/vllm-project/vllm/issues/13376
这是一个bug报告，主要涉及在sampler中支持bad_words功能。由于无法将`CachedTokenizer`进行pickle序列化，因此选择了另一种方法来对bad words进行标记化。

https://github.com/vllm-project/vllm/issues/13375
这是一个Bug报告，主要涉及的对象是使用vllm-openai v0.7.2 docker containers的Deepseek resoning功能。由于环境中的Deepseek resoning内容为空，并且思维内容被错误放入内容中，导致了此问题的症状。

https://github.com/vllm-project/vllm/issues/13374
这是一个bug报告，主要对象是MLA attention model。这个问题产生的原因是在vllm/config.py文件中设置了强制禁用chunked prefill和prefix caching，但在用户使用enablechunkedprefill时，max_num_batched_tokens默认设置为2048，导致输入提示过长并超过2048限制。

https://github.com/vllm-project/vllm/issues/13373
这是一个bug报告类型的issue，涉及的主要对象是vllm下的一个模型"Qwen2.5VLForEmbedding"。由于缺少'layers'模块或参数，导致无法使用该模型进行编码embedding。

https://github.com/vllm-project/vllm/issues/13372
这个issue类型是功能需求，涉及的主要对象是将新的ragged paged attention kernel与vLLM v1整合在TPU上。

https://github.com/vllm-project/vllm/issues/13371
这是一个bug报告，主要涉及到vllm中执行'method 'load_model''时出现错误，可能会导致分布式执行中的死锁。原因可能是由于参数配置错误或者代码实现中的bug。

https://github.com/vllm-project/vllm/issues/13370
这是一个bug报告，主要涉及输入提示过长导致参数限制问题。

https://github.com/vllm-project/vllm/issues/13369
这是一个bug报告类型的issue，主要涉及到修正文件中的拼写错误。原因是索引文件中存在拼写错误，需要进行修正。

https://github.com/vllm-project/vllm/issues/13368
这个issue是一个bug报告，主要涉及xpu communicator。由于某些原因导致了xpu communicator出现bug需要修复。

https://github.com/vllm-project/vllm/issues/13367
这是一个bug报告，涉及到vllm环境中worker节点加入head节点后在运行一段时间后自动退出，即使没有发送退出信号。

https://github.com/vllm-project/vllm/issues/13366
这个issue类型是功能需求，涉及的主要对象是ArcticForCausalLM，用户提出了添加`SupportsQuant`接口的需求。

https://github.com/vllm-project/vllm/issues/13365
这是一个优化性质的issue，主要涉及N-gram匹配算法的性能优化，由于通过Numba进行JIT编译，导致了算法速度显著提升。

https://github.com/vllm-project/vllm/issues/13364
这是一个关于安装问题的帮助请求，涉及vLLM的源码构建和flashattention组件，可能由于环境隔离或构建配置问题导致了缺失目标文件的bug。

https://github.com/vllm-project/vllm/issues/13363
这个issue是一个功能需求提议，涉及主要对象为VLLM的speculative decoding drafter，并由于功能设计需要将其移动到model runner中。

https://github.com/vllm-project/vllm/issues/13362
这是一个Bug报告，主要涉及到VLLM的rejection sampler。该问题的原因是问题在于该sampler在每次`forward`调用时打印警告/日志消息，以及未检查`VLLM_USE_FLASHINFER_SAMPLER`环境变量，导致用户无法关闭flashinfer。

https://github.com/vllm-project/vllm/issues/13361
这个issue是关于RFC（Request for Comments），主要对象是Deprecation of the `best_of` Sampling Parameter in vLLM V1。由于低使用率、与行业趋势不符以及对系统简化和性能的要求，导致了此参数的废弃。

https://github.com/vllm-project/vllm/issues/13360
这是一个用户提出需求的类型的issue，主要是针对`LogitsProcessor`接口的设计提出了一些建议和设想。可能是由于需要更好地处理采样参数与持久批处理相关的功能，以及提供自定义logits处理器的扩展点。

https://github.com/vllm-project/vllm/issues/13359
这是一个bug报告类型的issue，主要涉及的对象是v1/spec_decode/，由于缺少__init__.py文件导致的bug。

https://github.com/vllm-project/vllm/issues/13358
这是一个bug报告，涉及到cuda platform在CPU后端的检测问题，导致CPU平台和GPU平台同时激活的情况。

https://github.com/vllm-project/vllm/issues/13357
该issue类型是用户提出需求，主要涉及到支持多GPU规格的功能，用户希望vllm在初始化时能够支持类似cuda:0,1这样的多GPU规格。

https://github.com/vllm-project/vllm/issues/13356
这是一个用户提出需求的issue，主要对象是日志统计间隔。用户希望实现对日志统计间隔的配置，以便更灵活地记录统计数据。

https://github.com/vllm-project/vllm/issues/13355
这是一个Bug报告，涉及到`Sampler`模块中使用的不存在的`SamplingType.BEAM`枚举成员，可能是由于重构时的拼写错误导致。

https://github.com/vllm-project/vllm/issues/13354
这是一个bug报告，涉及的主要对象是在使用3个节点和8个H100 GPU运行Deepseek R1模型时出现了错误"pthread_create failed: Resource temporarily unavailable"。这个问题很可能是由于资源限制导致的。

https://github.com/vllm-project/vllm/issues/13353
这个issue类型是bug报告，主要涉及对象是关于缓存中间张量的功能，由于未正确缓存并重复使用中间张量，可能导致当前PP无法正常工作。

https://github.com/vllm-project/vllm/issues/13352
这是一个bug报告，主要涉及的对象是flashinfer版本更新导致的fp8准确性测试失败，需要暂时跳过该测试。

https://github.com/vllm-project/vllm/issues/13351
该issue是关于特性需求，提出了要整合性能基准数据集，并因为数据集采样函数定义在各自脚本中导致维护困难及灵活性较差。

https://github.com/vllm-project/vllm/issues/13350
这是一个需求提出的issue，主要涉及的对象是benchmark_serving.py文件，由于需要添加LongBench数据集以支持具有更长输入长度的数据，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/13349
该问题类型为需求提出，主要涉及对象为H2O-VL文档和示例。原因是需要更新文档以反映H2OVL在V1上的支持情况，并进行比较说明不同版本之间的下载量和喜爱程度。

https://github.com/vllm-project/vllm/issues/13348
这个issue属于bug报告类型，主要涉及对象是在本地模型路径下出现的错误日志。原因是不必要的`hf_list_repo_files`调用导致了误导性的错误日志。

https://github.com/vllm-project/vllm/issues/13347
这个issue属于用户提出需求的类型，主要涉及的对象是vllm项目的代码风格设置。由于Black默认使用88字符作为行长限制，用户建议使用相同的设置以保持兼容性。

https://github.com/vllm-project/vllm/issues/13346
这是一个Bug报告，涉及VL-LM中针对个体推理返回预期结果和批处理推理返回相同提示的不同结果的问题。原因可能是模型加载或推理过程中的某些问题导致的 bug。

https://github.com/vllm-project/vllm/issues/13345
该问题类型为用户提出需求，询问关于是否支持FP8 attention的问题，涉及主要的对象是vllm。由于用户想了解vllm是否支持FP8 attention，希望得到相关信息。

https://github.com/vllm-project/vllm/issues/13344
这是一个用户提出需求的类型的issue， 主要涉及的对象是Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit和Qwen2.5-VL-72B-Instruct-bnb-4bit系列模型。由于缺乏应用支持，导致用户提出希望为这些模型添加应用支持的需求。

https://github.com/vllm-project/vllm/issues/13343
这是一个用户提出需求的issue，主要涉及到的对象是应用程序支持InternVL2.5-78B系列模型，由于缺少相应的支持，用户需要添加对这些模型的支持。

https://github.com/vllm-project/vllm/issues/13342
这是一个功能需求的issue，主要涉及到vllm下的S1-32B推理解析器的支持问题。由于S1在识别法律文本中的逻辑结构时，推理相对不够结构化，因此需要增加对S1-32B推理解析器的支持。

https://github.com/vllm-project/vllm/issues/13341
这是一个 Bugfix 类型的 issue，主要涉及 Node test 和 Spec Decode test，由于OOM failure和Spec Decode failure引起的。

https://github.com/vllm-project/vllm/issues/13340
这是一个bug报告，问题涉及到vllm server的调用接口出现了400 Bad Request错误，可能是由于接口调用中的base_url参数配置问题导致。

https://github.com/vllm-project/vllm/issues/13339
这个issue是关于功能需求的，主要涉及Scheduler和Model Runner，因为需要修改使Model Runner从Scheduler获取输入令牌ID，该需求特别在令牌ID不由Model Runner生成时非常有用。

https://github.com/vllm-project/vllm/issues/13338
这是一个bug报告issue，主要涉及到了vllm的xgrammar库。由于最新版本0.1.13对apply_token_bitmask_inplace_cuda op的改动导致问题，需要暂时将xgrammar固定在0.1.11版本。

https://github.com/vllm-project/vllm/issues/13337
这是一个用户提出需求的issue，主要涉及vllm的chat template配置问题，导致程序输出无意义的结果。

https://github.com/vllm-project/vllm/issues/13336
这个issue类型是用户提出需求，涉及的主要对象是Molo models，由于configure_quant_config假定所有packed_modules_mapping在初始化前已声明，但实际上某些子模型的映射只能在初始化时确定，因此需要添加SupportsQuant接口以解决此问题。

https://github.com/vllm-project/vllm/issues/13335
这是一个Bug报告，主要涉及的对象是DeepseekR1模型加载过程中出现的weight tied错误。原因是在加载模型时出现了weight tied错误，导致DeepseekR1模型加载失败。

https://github.com/vllm-project/vllm/issues/13334
这是一个性能优化类的issue，主要涉及到vllm库中的logit_bias_logits_processor处理过程，由于目前使用的Python操作，导致处理时间较长，用户希望通过转换为张量操作来提升性能。

https://github.com/vllm-project/vllm/issues/13333
这个issue类型是bug报告，主要涉及vllm中benchmark_serving.py的输出长度只有43个token，可能是由于版本问题导致的。

https://github.com/vllm-project/vllm/issues/13332
这是一个Bug报告，涉及的主要对象是VLLM模型。由于PyTorch版本不兼容，导致出现数值错误(ValueError)，提示模型架构无法检查。

https://github.com/vllm-project/vllm/issues/13331
这是一个用户提出需求的类型，主要涉及到Fuyu E2E的示例代码，用户需要避免硬编码`30`作为patch大小的数值。

https://github.com/vllm-project/vllm/issues/13330
该issue属于bug报告，主要涉及docker image无法拉取安装问题。原因可能是docker指令错误或网络连接问题导致。

https://github.com/vllm-project/vllm/issues/13329
这个issue是bug报告，主要涉及到Nemo gguf模型的量化以及--compilation-config选项的使用问题，用户提出了关于gguf模型不支持compilation-config选项及如何使用该选项的疑问。

https://github.com/vllm-project/vllm/issues/13328
这是一个关于 vLLM 项目的问题讨论，用户询问在异步模式下 vllm 模型如何处理输入请求的问题。

https://github.com/vllm-project/vllm/issues/13327
这是一个寻求帮助的问题，主要涉及vLLM的版本迁移和缓存相关代码问题，由于版本不同导致用户无法找到获取计算kv缓存的代码。

https://github.com/vllm-project/vllm/issues/13326
这是一个bug报告类型的issue，主要涉及vLLM模型对DeepSeek R1模型在大模型推断时是否支持将权重转移到CPU进行处理的问题，用户希望得到相关帮助和解决方案。

https://github.com/vllm-project/vllm/issues/13325
这是一个文档修改类型的issue，主要涉及到更新教程文档。原因可能是为了更新指南内容或修正错误。

https://github.com/vllm-project/vllm/issues/13324
该issue属于用户提出需求类型，主要涉及AsyncLLMEngine是否适用于多模态模型以及是否应该使用MQLLMEngine。由于缺乏文档支持，用户在尝试使用AsyncLLMEngine处理带有图像或视频URL的多模态模型时遇到了困惑。

https://github.com/vllm-project/vllm/issues/13323
这个issue类型为功能需求，主要涉及flashinfer和vllm之间的合作更新。

https://github.com/vllm-project/vllm/issues/13322
这个issue类型是bug报告，主要涉及对象是Qwen2VL2B模型的温度参数（temperature），用户提出由于使用不同温度值时生成结果相同的问题。

https://github.com/vllm-project/vllm/issues/13321
这是一个性能优化的issue，主要涉及moe wna16 cuda kernel，由于m is small时triton无法发挥最佳性能，因此提出了添加moe wna16 cuda kernel以提升生成速度。

https://github.com/vllm-project/vllm/issues/13320
这个issue属于用户提出需求类型，主要对象是VLM下的Florence-2模型，由于需要支持多模态输入，用户希望修复这个问题。

https://github.com/vllm-project/vllm/issues/13319
这个issue是一个需求提出类型的问题单，主要涉及的对象是vLLM中的Transformer执行路径。由于目前vLLM推理服务缺乏完全基于Triton的执行路径，因此用户提出了引入一个完全不涉及CUDA而仅涉及Triton的Transformer执行路径的建议。

https://github.com/vllm-project/vllm/issues/13318
这个issue属于bug报告类型，主要涉及到Whisper的默认`transcription`任务产生的问题。由于默认`transcription`任务导致了多模态测试的出错。

https://github.com/vllm-project/vllm/issues/13317
这个issue类型是用户提出需求，主要涉及的对象是请求支持新的模型AIDC-AI/Ovis2-1B。这个问题的根本原因是vllm还没有支持该新模型，用户希望得到支持。

https://github.com/vllm-project/vllm/issues/13316
这是一个关于Bug报告的issue，主要涉及的对象是CI测试。由于min_p参数的问题导致v1-test测试失败。

https://github.com/vllm-project/vllm/issues/13315
这是一个关于性能问题的issue，涉及到vllm项目中的memory profiling功能。由于目前的实现方式有些不完善，用户提出需要重新编写`profile_run`函数。

https://github.com/vllm-project/vllm/issues/13314
这个issue是一个Bug报告，主要涉及的对象是Test_whisper，由于任务设置问题导致了测试无法通过。

https://github.com/vllm-project/vllm/issues/13313
这是一个用户提出需求的issue，主要涉及的对象是模型支持。主要原因是为了添加对`GraniteMoeShared`模型的支持，并且提醒需要使用`transformers >= v4.49.0`。

https://github.com/vllm-project/vllm/issues/13312
这是一个bug报告，主要涉及vllm中采样器组件的修改问题，由于precisionrelated nondeterminism导致某些问题。

https://github.com/vllm-project/vllm/issues/13311
这是一个建议性问题，主要涉及到VLLM模型中的Sampler功能，提出避免在某种情况下冗余操作的建议。

https://github.com/vllm-project/vllm/issues/13310
这个issue是关于bugfix的，主要对象是vllm下的flash attention接口。由于两个PR引入的接口差异，导致了在RoCM上使用MLA时出现问题。

https://github.com/vllm-project/vllm/issues/13309
这是一个bug报告，涉及主要对象是在多GPU上加载模型时出现的Segmentation fault。原因可能是代码中的某些问题导致了这个bug。

https://github.com/vllm-project/vllm/issues/13308
这个issue类型是测试需求，涉及的主要对象是V1 multimodal模型。根据内容，用户提出需要为V1 multimodal模型添加测试，包括abort和load测试。

https://github.com/vllm-project/vllm/issues/13307
这是一个Bug报告，涉及的主要对象是vllm/vllmopenai:latest docker image。由于引发了KeyError异常，可能是由于未找到'mllama'相关配置而导致的bug。

https://github.com/vllm-project/vllm/issues/13306
这是一个用户需求类型的issue，主要涉及VLLM与RTX 5090（CUDA 12.8）的兼容性问题，导致安装VLLM失败。

https://github.com/vllm-project/vllm/issues/13305
这个issue是关于优化的，主要针对vLLM V1在AMG GPUs上的性能优化。

https://github.com/vllm-project/vllm/issues/13304
这个issue属于代码优化类型，主要涉及的对象是QuantizationConfig类。由于类变量值的潜在共享可能会导致不经意的问题，因此需要将`packed_modules_mapping`从类变量改为实例变量。

https://github.com/vllm-project/vllm/issues/13303
该issue类型是关于重新设计LoRA指标的工作，主要涉及LoRA指标的当前实现存在的问题及需求变更。由于当前的指标实现方式不合理，导致无法有效地统计和区分适用于每个适配器的请求信息。

https://github.com/vllm-project/vllm/issues/13302
这是一个bug报告，主要对象是加载模型时不必扫描整个缓存目录导致性能问题。

https://github.com/vllm-project/vllm/issues/13301
这是一个功能需求的issue，主要涉及到Transcription API streaming相关功能的添加，提出了关于音频长度度量、Dockerfile更新以及优化分割音频的建议。

https://github.com/vllm-project/vllm/issues/13300
这个issue属于用户提出需求类型，主要对象是SamplingParams。由于需要定制采样实现，因此添加了一个额外参数的字段。

https://github.com/vllm-project/vllm/issues/13299
这是一个bug报告类型的issue，该问题涉及到在multiprocessing模式下使用prometheus_client时无法支持Info类型指标，导致需要通过其他方式实现该功能。

https://github.com/vllm-project/vllm/issues/13298
这个issue是一个bug报告，主要涉及的对象是engine core client的关闭功能。由于未显式关闭ZMQ sockets可能导致context.term()方法 hang 问题。

https://github.com/vllm-project/vllm/issues/13297
这是一个Bug报告类型的issue，主要涉及vLLM中多GPU支持加载量化模型的问题，用户提出了如何在多GPU中加载量化模型并请求相关配置指导。

https://github.com/vllm-project/vllm/issues/13296
这是一个需求类型的issue，主要涉及的对象是V1版本的Hybrid allocator，由于需要实现interleaved full attention & sliding window attention模型，需要调整block的分配方式。

https://github.com/vllm-project/vllm/issues/13295
这是一个用户提出需求的issue，主要涉及到添加CLI参数以帮助用户更好地处理已弃用的指标。

https://github.com/vllm-project/vllm/issues/13294
这是一个关于bug报告的issue，主要涉及的对象是vllm，由于未正确执行关闭操作导致了vLLM冻结并出现警告信息。

https://github.com/vllm-project/vllm/issues/13293
这是一个Bug报告，涉及的主要对象是VLLM模型在输出开始时为空的问题。

https://github.com/vllm-project/vllm/issues/13292
这是一个bug报告，主要对象是vLLM模型。问题出现的原因可能是当模型为本地路径时，vLLM仍然尝试在资源库中搜索文件，导致了这个问题。

https://github.com/vllm-project/vllm/issues/13291
这是一个建议更正文档的issue，该问题涉及主要对象是代码中的工具调用函数。由于代码中硬编码了函数名称而没有使用之前定义的变量，导致用户建议可以从预定义的字典中获取函数名称。

https://github.com/vllm-project/vllm/issues/13290
这是一个bug报告类型的issue，主要涉及VDR变量在GGUF内核中缺少解释的问题，导致需要增加相关注释。

https://github.com/vllm-project/vllm/issues/13289
这是一个bug报告，主要涉及的对象是V1版本的llama模型。问题出现的原因是无关的日志输出会干扰用户对单模态模型的测试结果。

https://github.com/vllm-project/vllm/issues/13288
该issue属于功能需求，涉及监控指标的添加，主要对象是V1版本的Metrics模块。这个问题产生的原因是引入了基于cudagraph捕获大小的bucket尺寸，导致需要添加包括iteration tokens总数的直方图。

https://github.com/vllm-project/vllm/issues/13287
这个issue类型是Bug报告，涉及主要对象是OpenVinoWorker对象，由于缺少'cache_engine'属性导致了AttributeError错误的症状。

https://github.com/vllm-project/vllm/issues/13286
这是一个Bug报告类型的issue，主要涉及的对象是Qwen2.5VL图像处理器。由于transformers更新改变了Qwen2.5VL图像处理器的命名，导致Qwen2.5VL无法正常工作，需要等待Qwen团队更新模型库。

https://github.com/vllm-project/vllm/issues/13285
这是一个bug报告，涉及到vllm项目中的Qwen2.5-VL，由于依赖的transformers库发生了变化导致程序功能受影响。

https://github.com/vllm-project/vllm/issues/13284
这是一个bug报告，主要涉及V1版本无法正常提供Qwen模型的问题。由于并发性问题导致性能下降，引发了这个bug。

https://github.com/vllm-project/vllm/issues/13283
这是一个bug报告类型的issue，主要涉及到在部署DeepSeekR1-4bit时出现的文件缺失错误。

https://github.com/vllm-project/vllm/issues/13282
这是一个Bug报告类型的Issue，涉及对象是DeepSeek-R1-Distill-Llama-70B模型，原因是max_model_len参数不能大于8192导致了问题。

https://github.com/vllm-project/vllm/issues/13281
这是一个bug报告类型的issue，主要涉及的对象是vllm的编译和安装过程。由于编译和安装源代码速度过慢，导致用户寻求如何加快vllmflashattn编译的帮助。

https://github.com/vllm-project/vllm/issues/13280
这是一个bug报告，涉及到搜索算法中`start_index`的问题。由于搜索逻辑错误导致了对应场景下`start_index`的计算不准确。

https://github.com/vllm-project/vllm/issues/13279
这是一个bug报告，主要涉及 xformers 在安装时出现的问题，导致启动失败并提示重新安装 xformers。

https://github.com/vllm-project/vllm/issues/13278
这个issue是一个bug报告，主要涉及的对象是Whisper的merged multimodal processor。该问题是由于性能问题导致需要修复，并需要增加处理器测试。

https://github.com/vllm-project/vllm/issues/13277
这个issue类型属于功能需求，主要涉及的对象是Whisper waveform输入。由于缺少具体内容，用户提出了有关Whisper waveform输入的需求或问题。

https://github.com/vllm-project/vllm/issues/13276
这是一个Bug报告，主要涉及VLLM的执行过程中出现的错误导致潜在的死锁问题。

https://github.com/vllm-project/vllm/issues/13275
这是一个用户提出需求的类型的issue，主要涉及的对象是logging stats功能。由于当前无法配置日志统计信息的时间间隔，用户提出希望添加可配置的间隔选项以满足需求。

https://github.com/vllm-project/vllm/issues/13274
这个issue属于bug报告类型，涉及到修复离线Whisper功能，可能由于之前的代码错误导致无法正常工作。

https://github.com/vllm-project/vllm/issues/13273
这是一个bug报告，主要涉及了程序在设置tensor-parallel-size=2时会无法正常运行并卡住的问题。

https://github.com/vllm-project/vllm/issues/13272
这是一个Bug报告类型的Issue，主要涉及Whisper.py文件的测试用例在运行时失败。可能是由于模型运行器的类型导致的。

https://github.com/vllm-project/vllm/issues/13270
这个issue类型是bug报告，主要涉及的对象是finetuning Qwen2.53b，由于缺少`TORCH_USE_CUDA_DSA`编译选项导致CUDA错误，用户在进行finetuning时遇到了这个问题。

https://github.com/vllm-project/vllm/issues/13269
这是一个bug报告，主要对象是修复了在多节点设置下初始化spec_decode_sampler时的设备序号问题。由于原先代码在多节点设置下没有正确处理设备序号，导致了bug。

https://github.com/vllm-project/vllm/issues/13268
这是一个bug报告类型的issue，主要涉及VLLM项目中的Transformer模型在初始化过程中遇到CUDA内存溢出错误的问题。

https://github.com/vllm-project/vllm/issues/13267
这个issue是用户提出需求类型的，主要涉及的对象是VLLM serve的集成和推断运行。这个问题是关于如何尽可能避免VLLM由于CUDA内存不足而导致崩溃的问题。

https://github.com/vllm-project/vllm/issues/13266
这是一个bug报告，涉及的主要对象是加载本地模型时显示误导性日志，实际上并未引发任何错误。导致这个问题的原因是误导性日志给用户造成了困惑。

https://github.com/vllm-project/vllm/issues/13265
这是一个bug报告，主要涉及的对象是GPUModelRunner._update_states方法，由于未正确检查`removed_req_indices`而导致产生了特定症状。

https://github.com/vllm-project/vllm/issues/13264
这是一个bug报告，主要涉及vllm.attention.backend.flashinfer模块在运行speculative decoding时出现的错误行为，导致draft model不正确地拷贝关键参数。

https://github.com/vllm-project/vllm/issues/13263
这是一个bug报告，主要涉及的对象是代码缺少括号。由于缺少括号导致了bug。

https://github.com/vllm-project/vllm/issues/13262
这是一个关于bug报告，涉及vllm的Embedding Model输出精度问题。由于输出decimal length过长且存在与其他在线API不一致的偏差，用户希望了解是否有配置可更改此行为。

https://github.com/vllm-project/vllm/issues/13261
这是一个用户提出需求的类型，主要涉及对象是VL模块，该需求是为了支持LoRA。

https://github.com/vllm-project/vllm/issues/13260
这个issue类型是bug报告，涉及到vllm在structured output generation和json structured generation过程中出现的错误。

https://github.com/vllm-project/vllm/issues/13259
这是一个关于性能问题的bug报告，主要涉及vllm服务的请求处理和GPU利用率，由于请求等待缓存积压导致性能降低。

https://github.com/vllm-project/vllm/issues/13258
这是一个 Bug 报告，主要涉及的对象是 VLLM_ATTENTION_BACKEND。由于设置了 VLLM_ATTENTION_BACKEND=FLASHINFER，导致出现错误，可能是由于环境配置与所需的模块不匹配所致。

https://github.com/vllm-project/vllm/issues/13257
这是一个功能需求类型的issue，主要涉及设置OpenAI兼容服务器的模型ID。由于当前模型ID的取值方式不够合理，用户希望能够通过指定模型ID来调用API。

https://github.com/vllm-project/vllm/issues/13256
这个issue是一个bug报告，主要对象是关于vllm的token计算的不一致性问题。由于使用vllm进行推理服务时，返回的数据与使用transformers中的AutoTokenizer获得的token不一致，导致了此问题的产生。

https://github.com/vllm-project/vllm/issues/13255
这是一个bug报告类型的issue，主要涉及到用户在使用vllm过程中遇到了多卡内存不足的问题，导致出现CUDA内存不足的错误提示。

https://github.com/vllm-project/vllm/issues/13254
这是一个bug报告，主要涉及Gem2ForSequenceClassification在vLLM中没有实现的问题。由于缺少对应的实现，用户在使用SkyworkRewardGemma时遇到了错误。

https://github.com/vllm-project/vllm/issues/13252
这是一个Bug报告，该问题涉及的主要对象是VLLM0.7.2服务器。由于CUDA内存不足导致服务关闭。

https://github.com/vllm-project/vllm/issues/13251
这个issue类型是用户提出需求，主要涉及模型添加支持的问题。提出原因可能是希望支持一个新的模型Ovis2，但目前提供的最接近的模型是Qwen2.5VL。

https://github.com/vllm-project/vllm/issues/13250
这是一个bug报告，涉及测试用例 test_compressed_tensors_2of4_sparse 的状态未正确开启，导致测试被遗漏。

https://github.com/vllm-project/vllm/issues/13249
这个issue类型属于功能增强需求，主要对象是Neuron中的NKI Flash PagedAttention模块，通过开发BlockSparse版本来减少计算浪费。

https://github.com/vllm-project/vllm/issues/13248
该issue类型为功能需求，主要涉及的对象是RPCProcessRequest和MQLLMEngineClient，用户提出了对添加`extra_args`到这两个类的请求。

https://github.com/vllm-project/vllm/issues/13247
这是一个Bug报告，涉及到TPOT在特定条件下变得非常缓慢的问题，可能是由于使用惩罚参数以及批处理大小大于20导致的性能问题。

https://github.com/vllm-project/vllm/issues/13246
这个issue是一个功能需求提出，主要涉及的对象是为neuron backend添加自定义操作。由于vLLM正在支持vLLM V1架构为neuron backend做努力，因此需要添加激活函数、layernorm、rotary_embedding和logits_processor等自定义操作。

https://github.com/vllm-project/vllm/issues/13245
这是一个性能优化类的issue，主要对象是NKI flash attention kernel。由于未对KV缓存加载进行矢量化处理，导致内核受制于从HBM提取分页的KV缓存，因此引发了性能瓶颈。

https://github.com/vllm-project/vllm/issues/13244
这是一个用户提出的需求类型的issue，主要涉及到了SamplingMetadata对象和input_batch.req_ids，由于原始构建的方式导致了在迭代过程中需要对列表进行切片操作，用户希望进行优化以提高效率。

https://github.com/vllm-project/vllm/issues/13243
这个issue类型是功能需求提议，主要对象是设置在`pre-commit-passed`标签的成功结果。 

https://github.com/vllm-project/vllm/issues/13242
该issue是一个撤销之前添加的功能变更，属于代码修改类型，主要涉及到vllm项目中关于pre-commit功能的处理。

https://github.com/vllm-project/vllm/issues/13241
这是一个bug报告，主要涉及的对象是`removeLabels`方法。由于`removeLabels`方法无法正常使用，导致了"removeLabel"的拼写错误。

https://github.com/vllm-project/vllm/issues/13240
这是一个bug报告，主要涉及到多个模型定义中未被使用的`padding_idx`变量清理问题。这可能是由于代码未完全落地导致的。

https://github.com/vllm-project/vllm/issues/13239
这个issue是一个功能需求，主要涉及MM(cache)的大小配置，为了保证两个cache大小相同，需要将硬编码的部分配置为环境变量。

https://github.com/vllm-project/vllm/issues/13238
这是一个bug报告，主要涉及ROCm上使用默认stream导致性能下降的问题。

https://github.com/vllm-project/vllm/issues/13237
这是一个bug报告，主要涉及pre-commit钩子不查看`pyproject.toml`中的`ignorewordslist`，导致了问题。

https://github.com/vllm-project/vllm/issues/13236
这是一个性能优化类型的issue，主要涉及到使用moes和专家数量多时的性能问题，用户在多个模型中测试后发现moes（Mixture of Experts）中默认使用moe_wna16 kernel对于拥有多个专家的情况下是更快的。

https://github.com/vllm-project/vllm/issues/13235
这是一个bug报告，涉及主要对象是 torch_bindings 库。这个问题报告是为了通过更新 torch_bindings，解决在ROCm上构建 scaled_fp4_quant 时出现的问题，以期让AMD CI测试通过。

https://github.com/vllm-project/vllm/issues/13234
这是一个性能问题，主要涉及到在4个A100 GPU 40Gb上运行Llama-70b的性能。用户提出希望减少平均响应时间以提高性能。

https://github.com/vllm-project/vllm/issues/13233
这个issue类型是用户提交签名测试CI系统的请求，涉及的主要对象是CI系统。由于开发者Alexei V. Ivanov提交了未经认证的签名，可能导致CI系统无法正常运行测试。

https://github.com/vllm-project/vllm/issues/13232
这是一个bug报告，主要涉及DeepSeek V2模型在使用fp16数据类型时产生的数值溢出问题。

https://github.com/vllm-project/vllm/issues/13231
该issue类型属于性能优化，主要涉及ROCm平台上的矩阵乘法操作。由于硬件限制导致在GEMM中存在严重的Tagram通道热点问题，需要通过应用stride padding来避免这一问题，以提高性能。

https://github.com/vllm-project/vllm/issues/13230
这个issue是关于bug报告，涉及的主要对象是CUDA GPUs环境。由于当前环境中没有可用的CUDA GPUs，导致了RuntimeError的问题。

https://github.com/vllm-project/vllm/issues/13229
这个issue类型是bug报告，涉及的主要对象是代码中的`scheduler.py`文件。由于`is_prefill`已经被设置，导致了冗余语句的存在。

https://github.com/vllm-project/vllm/issues/13228
这是一个关于功能需求的issue，主要涉及了 xgrammar 的正则表达式支持问题，由于旧版本的 xgrammar 存在问题，导致无法开启正则表达式支持。

https://github.com/vllm-project/vllm/issues/13227
这是一个Bug报告，主要涉及的对象是vllm下的awq kernel。这个问题由于n不是128的倍数，但是是64的倍数，导致awq核函数得出的结果出现错误。

https://github.com/vllm-project/vllm/issues/13226
这是一个Bug报告，涉及的主要对象是Gemma2Config对象。这个Bug是由于vLLM在设置Gemma2模型时寻找`interleaved_sliding_window`属性而导致的。

https://github.com/vllm-project/vllm/issues/13225
这是一个用户提出的需求，主要涉及日志记录过程中敏感信息过滤的功能添加。

https://github.com/vllm-project/vllm/issues/13224
该issue是关于文档改进的，主要对象是项目的功能表格展示，原因是为了使表格更易读。

https://github.com/vllm-project/vllm/issues/13223
这是一个Bug报告issue，涉及的主要对象是`examples/grad_web_server.py`中的`http_bot`函数。由于`json.loads`解析`response.iter_lines`返回的chunk时出错，导致无论输入什么，都会得到错误的结果。

https://github.com/vllm-project/vllm/issues/13222
该issue是关于用户提出问题的类型，主要涉及Groupedquery attention (GQA)的实现是否包含在vLLM中。由于用户对vLLM模型是否使用GQA功能存疑，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/13221
这个issue是一个Bug报告，涉及的主要对象是内存使用和推理解码，导致了Persistent OutOfMemoryError error错误。

https://github.com/vllm-project/vllm/issues/13220
该issue是一个功能需求提议，涉及对象为在设置了`VLLM_USE_MODELSCOPE=true`环境下下载lora模型的行为。这个问题的提出原因是为了避免用户在不同库下载模型和lora的不一致性问题。

https://github.com/vllm-project/vllm/issues/13219
这个issue是一个bug报告，主要涉及VLLM在使用docker部署R1 AWQ时获取到空内容的问题，可能是由于API调用返回的数据为空导致。

https://github.com/vllm-project/vllm/issues/13218
这是一个bug报告，主要涉及到从vllm 0.5.3. post1升级到vllm 0.7.2版本后监控指标avg.generation_throughput_toks_per_2消失的问题。原因可能是升级导致了该监控指标的缺失。

https://github.com/vllm-project/vllm/issues/13217
该issue属于用户提出需求，主要是针对新增hidet后端支持的请求。

https://github.com/vllm-project/vllm/issues/13216
这个issue是关于bug报告，主要涉及的对象是使用deepseekai/DeepSeekR1DistillQwen1.5B模型时出现的数值错误。这个问题可能是由于模型架构`Qwen2ForCausalLM`无法被检查引起的。

https://github.com/vllm-project/vllm/issues/13215
这个issue属于bug报告类型，涉及的主要对象是VLM中的Prompt替换功能。由于处理Prompt替换后没有显式返回替换是否成功导致的问题。

https://github.com/vllm-project/vllm/issues/13214
这是关于Bug的报告，涉及的主要对象是解码实例（decode instance），可能由于提示不一致导致了hang（卡住）问题。

https://github.com/vllm-project/vllm/issues/13213
这个issue是关于功能需求的，主要涉及到Hardware上的Intel-Gaudi，通过添加对HPU的区域编译支持来提高性能。

https://github.com/vllm-project/vllm/issues/13212
这是一个bug报告，主要涉及vLLM V1模型在默认编译配置下在生成输出时出现乱码的问题，可能是由于`compilation_config`的设置问题导致。

https://github.com/vllm-project/vllm/issues/13211
这个issue类型是用户提出需求，主要涉及到V1接口的文档清晰度问题。由于缺乏文档逻辑清晰度，导致新开发者在理解input processing和multimodal feature caching logic方面存在困难。

https://github.com/vllm-project/vllm/issues/13210
该issue类型为功能改进，涉及对象为vllm中的Sampler模块，用户提出了增加allowed_token_ids支持的需求。

https://github.com/vllm-project/vllm/issues/13209
这是一个Bug报告，主要涉及到vllm环境中出现的输入长度限制问题，导致用户无法获得输出。

https://github.com/vllm-project/vllm/issues/13208
这是一个用户提出需求的issue，主要涉及的对象是平台通信模块。

https://github.com/vllm-project/vllm/issues/13207
这是一个bug报告，主要涉及vllm下的Flash Infer backend，由于最近的更新导致使用时会不断出现警告"Current VLLM config is not set"。

https://github.com/vllm-project/vllm/issues/13206
这是一个bug报告，涉及的主要对象是DeepSeek-V2-Lite，由于使用了ngram导致了结果不同。

https://github.com/vllm-project/vllm/issues/13205
这个issue类型是功能增强（feature enhancement），主要涉及到将模型加载到CI中，其目的是从S3路径而不是HF加载模型，可能由于HF导致的路径问题导致了bug或者需要用户对加载模型的时候的路径进行改进。

https://github.com/vllm-project/vllm/issues/13204
这是一个性能问题报告，涉及对象是vllm模型和LoRA适配器，由于使用LoRA适配器导致性能下降，用户提出了性能方面的优化建议。

https://github.com/vllm-project/vllm/issues/13203
这是一个 bug 报告，主要涉及定位 CUDART .so 文件路径的逻辑简化。由于原先计划使用部分路径，而实际上可以使用完整路径，导致了该问题的产生。

https://github.com/vllm-project/vllm/issues/13202
这是一个bug报告，主要涉及到vllm服务器模型在使用中出现了CUDA版本不匹配导致的错误。

https://github.com/vllm-project/vllm/issues/13200
这是一个Bug报告，涉及VLLM多节点服务时出现NCCL错误，用户环境中主要涉及PyTorch版本、CUDA版本以及硬件配置等信息。由于某种原因导致多节点服务时出现NCCL错误。

https://github.com/vllm-project/vllm/issues/13199
这个issue类型为性能优化，主要涉及AMD的ROCm性能优化问题，并由于添加某些调整以改善DeepSeek的表现。

https://github.com/vllm-project/vllm/issues/13198
这是一个bug报告，涉及的主要对象是CUTLASS 2:4 Sparse Kernels，由于一些bug导致了IMA和/或垃圾输出，以及一些其他问题的解决和优化。

https://github.com/vllm-project/vllm/issues/13197
这个issue是一个bug报告，涉及主要对象是vllm中的CompletionChoice对象。问题是用户提出了关于结果中缺少'delta'属性的疑问。

https://github.com/vllm-project/vllm/issues/13196
这是一个bug报告，主要涉及到Pytorch的Speculative Decoding中的Rejection Sampling功能，由于`capped_ratio`在`_create_uniform_samples`中产生了0和1的值而非在0到1之间的实数值，导致了更改种子后输出结果不变的问题。

https://github.com/vllm-project/vllm/issues/13195
这是一个bug报告类型的issue，涉及主要对象是用户数据的收集。由于用户没有选择是否上传数据，导致了缺乏用户选择上传的功能。

https://github.com/vllm-project/vllm/issues/13194
这个issue属于用户提出需求类型，主要涉及Qwen2_5_VLForConditionalGeneration模型支持enablelora功能的问题。由于该功能尚未被支持，用户询问何时可实现此功能。

https://github.com/vllm-project/vllm/issues/13193
这是一个Bug报告类型的Issue，涉及的主要对象是缺少Content Type导致返回500 Internal Server Error而不是415 Unsupported Media Type，该问题是由于缺少Content Type未被正确处理导致的。

https://github.com/vllm-project/vllm/issues/13192
这个issue类型是bug报告，涉及的主要对象是在使用langchain function call时出现function name重复导致调用失败。由于function name重复，导致调用失败。

https://github.com/vllm-project/vllm/issues/13191
这是一个用户提出需求的issue，主要涉及的对象是v1 Sampler。由于缺少对min_p参数的支持，用户请求在v1 Sampler中引入min_p支持。

https://github.com/vllm-project/vllm/issues/13190
该issue类型为用户提出需求，主要涉及需要实现新的LLaVAVideo7BQwen2模型。用户认为该模型在视频理解方面表现更优，因此请求实现此模型。

https://github.com/vllm-project/vllm/issues/13189
该issue类型为用户提出需求，询问如何在chat completion API中使用DeepSeekCoderV2LiteInstruct（https://huggingface.co/deepseekai/DeepSeekCoderV2LiteInstruct），询问如何集成到vllm中。

https://github.com/vllm-project/vllm/issues/13188
这是一个bug报告，主要涉及的对象是"tensor_parallel_size"的使用。由于环境中检测到不同的GPU设备，并建议设置`CUDA_DEVICE_ORDER=PCI_BUS_ID`以避免意外行为，可能导致了这个问题。

https://github.com/vllm-project/vllm/issues/13187
这个issue是关于bug报告，主要涉及test_pos_encoding.py中的测试用例在多设备环境下频繁失败，由于rope未指定设备导致创建时始终在设备0上，从而导致测试用例无法在多设备上通过。

https://github.com/vllm-project/vllm/issues/13186
这是一个Bug报告，涉及vllm server在concurrency下返回混乱的字符问题。

https://github.com/vllm-project/vllm/issues/13185
这是一个提出需求的issue，主要涉及Zamba2模型集成的支持。由于要添加对Zamba2模型的支持，需要修改代码以确保其与HuggingFace transformers库的集成，并对模型的性能进行评估和比较。

https://github.com/vllm-project/vllm/issues/13184
这是一个用户提出需求的issue，主要涉及在线推理API中如何设置图像分辨率的问题。用户想要运行特定模型的推理，但不清楚如何集成到vllm中。

https://github.com/vllm-project/vllm/issues/13183
这是一个关于安装问题的bug报告，涉及对象是vllm的源码安装。导致此问题的原因可能是编译时出现了ninja错误未识别的目标。

https://github.com/vllm-project/vllm/issues/13182
这是一个bug报告，主要涉及项目中的类型注解警告问题。该问题是由于项目需要保持对Python 3.10或更高版本的兼容性，目前仍需继续使用`Optiona[T]`语法而导致的。

https://github.com/vllm-project/vllm/issues/13181
这是一个功能需求的issue，主要涉及MLA进行量化时需要支持更多类型的量化，原因是需要访问未量化的权重以用于解码。

https://github.com/vllm-project/vllm/issues/13180
这个issue类型是功能需求，主要对象是CI/Build，用户想要针对ruff开启自动修复功能以简化代码维护。

https://github.com/vllm-project/vllm/issues/13179
这是一个文档问题报告，主要涉及到VLLM项目中关于QLoRA支持的描述不一致，可能由于文档中信息表达不清导致用户困惑。

https://github.com/vllm-project/vllm/issues/13178
这是一个功能需求提出的issue，主要对象是构建VLLM时的Python-only安装过程。由于当前命令总是使用每晚构建的wheel，导致使用开发分支时可能不兼容，因此提出了基于当前分支基准提交的wheel安装改进。

https://github.com/vllm-project/vllm/issues/13177
这是一个bug报告，涉及的主要对象是DeviceMemoryProfiler。由于未在测量之前将未使用的内存清除，导致DeviceMemoryProfiler无法准确测量峰值内存，可能影响缓存分配的准确性。

https://github.com/vllm-project/vllm/issues/13176
这是一个优化性质的issue，主要涉及硬件TPU在更新kv缓存性能方面的改进，由于TPU不擅长scatter操作导致kv缓存更新性能低下。

https://github.com/vllm-project/vllm/issues/13175
这是一个Bug报告，主要涉及Qwen模型生成的token出现了词汇外的情况，可能导致了异常错误。

https://github.com/vllm-project/vllm/issues/13173
这是一个Bug报告，该问题涉及的主要对象是调用`self.cached[req_id].discard(input_id)`导致的错误，可能由于操作集的大小变化导致VLM请求中止时出现迭代问题。

https://github.com/vllm-project/vllm/issues/13172
这是一个关于新模型迁移的issue，主要涉及到针对Surya OCR模型的技术支持。由于SwinTransformersStyle attention窗口等实现细节，完全将图像编码器移植到vLLM是棘手的。

https://github.com/vllm-project/vllm/issues/13171
这是一个Bug报告，涉及主要对象为InternVL2.5，可能由于最新更新导致multimodal processor功能失效，无法添加图片。

https://github.com/vllm-project/vllm/issues/13170
这是一个bug报告类型的issue，主要涉及的对象是CI（持续集成），由于某种原因导致了FP8 CPU offload测试失败。

https://github.com/vllm-project/vllm/issues/13169
这是一个功能需求的issue，涉及到V1版本的度量标准功能。由于需要添加核心引擎`PREEMPTED`事件，并且在发生抢占时重新设置计划和第一个令牌的时间戳。

https://github.com/vllm-project/vllm/issues/13168
这个issue是一个bug报告，主要涉及的对象是LLMEngine。这个问题的原因是LLMEngine在被pickled时会出现RuntimeError。

https://github.com/vllm-project/vllm/issues/13167
这是一个用户提出需求的issue，主要对象涉及VLLM中关于Deepseek GGUF支持的功能。这个issue涉及到由于Huggingface未支持Deepseek，导致加载模型速度慢以及GGUF MoE实现简单且速度缓慢的问题。

https://github.com/vllm-project/vllm/issues/13166
这个issue类型属于功能需求提出类型，主要涉及的对象是LoRA相关的静态变量`supported_lora_modules`。由于存在该静态变量导致模型实现不够清晰，需求是删除并进行单元测试以提升LoRA支持效果。

https://github.com/vllm-project/vllm/issues/13165
该issue类型为功能改进，主要对象是Llava系列模型中的clip和siglip。

https://github.com/vllm-project/vllm/issues/13164
这个issue类型是bug报告，涉及主要对象为vllm项目的Python 3.13支持，由于部分依赖库不支持Python 3.13，导致了该问题的产生。

https://github.com/vllm-project/vllm/issues/13163
这是一个功能需求的issue，涉及的主要对象是在Huggingface中添加对Deepseek GGUF模型的支持。 由于Huggingface当前不支持Deepseek，所以用户提出了需要添加覆盖路径以读取正确配置的需求。

https://github.com/vllm-project/vllm/issues/13162
这个issue是一个功能改进（feature enhancement），主要涉及到CI/Build过程中的mypy输出解析，意图是恢复在PR上更清晰地展示mypy错误。这个问题可能由于在转换到pre-commit时配置未完全迁移导致。

https://github.com/vllm-project/vllm/issues/13161
这个issue是一个功能需求类型的问题，涉及的主要对象是vLLM版本V0。由于当前版本V0未实现可插拔的平台特定调度器功能，用户提出了在V1版本中支持此功能的需求。

https://github.com/vllm-project/vllm/issues/13160
这是一个bug报告类型的issue，主要涉及到无法推断设备类型导致的问题。

https://github.com/vllm-project/vllm/issues/13159
这是一个Bug报告，主要涉及生成随机数据集的方法导致了重复随机提示，可能导致对启用前缀缓存的服务器进行测试时出现不准确的情况。

https://github.com/vllm-project/vllm/issues/13158
这是一个用户需求提出的issue，主要涉及到多节点服务部署的问题。由于部署deepseekr1非常复杂，参数繁多，希望提供docker-compose.yml示例以简化多节点服务的部署过程。

https://github.com/vllm-project/vllm/issues/13157
这是一个bug报告，主要涉及VLM模型中文本和视觉变体的分离问题。由于这个问题，用户需要明确设置`hfoverrides '{"architectures": []}'`才能使用这些模型，并且需要进行一些其他改变。

https://github.com/vllm-project/vllm/issues/13156
这是一个Bug报告，涉及到vllm064 post1服务提前终止导致无法输出固定数量的标记。原因可能是设置问题导致实际输出标记数不符合预设值。

https://github.com/vllm-project/vllm/issues/13155
这是一个优化性质的Issue，主要涉及到VLLM代码库中的一些模块和函数。原因是为了提高速度和减少内存使用。

https://github.com/vllm-project/vllm/issues/13154
这个issue属于用户需求类型问题，涉及主要对象是vllm长文本输入优化。由于历史信息积累导致首个token生成时间增加。

https://github.com/vllm-project/vllm/issues/13153
这是一个Bug报告，涉及的主要对象是Qwen2.5-VL-72B-Instruct部署问题。导致这个问题的原因是程序输出中提到的`vLLM serve`命令出现了错误。

https://github.com/vllm-project/vllm/issues/13152
这是一个Bug报告类型的Issue，涉及的主要对象是vllm的CUDA内存错误问题，可能由于CUDA内存不足导致benchmark_serving.py脚本崩溃。

https://github.com/vllm-project/vllm/issues/13151
这是一个建议类的issue，主要涉及删除未使用的 LoRA 模块，以减少内存消耗。

https://github.com/vllm-project/vllm/issues/13150
这是一个bug报告，主要涉及vllm serve命令缺少model_tag参数所导致的错误。

https://github.com/vllm-project/vllm/issues/13149
这是一个用户提出需求的issue，主要涉及如何在vllm中运行特定模型，可能由于当前环境仅支持weightonly而未支持gptq量化，导致用户不清楚如何集成模型并进行推理。

https://github.com/vllm-project/vllm/issues/13148
这是一个涉及bug修复的issue，主要对象是Qwen2-VL模型。该问题可能是由于num video tokens计算错误导致的。

https://github.com/vllm-project/vllm/issues/13147
这是一个Bug报告类型的issue，主要涉及vLLM在Radeon（W7900/RDNA3）上启用fp8 KV缓存量化和前缀缓存同时会导致进程崩溃的问题，原因是该架构不支持fp8e4nv类型。

https://github.com/vllm-project/vllm/issues/13146
这是一个Bug报告，涉及的主要对象是github上的vllm项目。此问题可能是由于服务器内部错误导致的，用户寻求帮助解决代码运行错误。

https://github.com/vllm-project/vllm/issues/13145
这是一个bug报告，涉及的主要对象是在Docker多机多卡部署中启动`DeepSeekR1DistillLlama70B`模型时报错，而部署`DeepSeekR1DistillQwen32B`模型则能正常启动和使用。导致此问题的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/13144
这个issue类型属于功能咨询，涉及主要对象为vLLM编译器。用户提出了关于vLLM是否编译草稿模型以及对编译配置的疑问。

https://github.com/vllm-project/vllm/issues/13143
这是一个bug报告，主要涉及的对象是vllm中的Qwen2-VL模块。用户提出了max_pixels参数无效的问题，并询问如何正确配置。

https://github.com/vllm-project/vllm/issues/13142
这个issue是一个关于bug报告的问题单，涉及主要对象为vllm下的deepseekv3引擎。由于某些初始化参数设置不正确导致了深度搜索引擎deepseekv3在运行时引发了RuntimeError。

https://github.com/vllm-project/vllm/issues/13141
这个issue类型是关于功能性问题，主要涉及的对象是Tensor Parallelism在vLLM中的实现方式，提出了关于在进行决策时是否考虑GPU内存信息的疑问。

https://github.com/vllm-project/vllm/issues/13140
这是一个bug报告，主要涉及的对象是DeepSeek R1模型的部署。由于在sgl_moe_align_block_size_kernel中缺乏同步机制导致的，当共享内存包含脏数据时使用atomicAdd可能会导致不正确的结果。

https://github.com/vllm-project/vllm/issues/13139
这是一个Bug报告类型的Issue，主要涉及的对象是vllm 0.7.2版本。由于CUDA_VISIBLE_DEVICES设置不正确，导致调用服务器时出现错误。

https://github.com/vllm-project/vllm/issues/13138
这是一个bug报告，主要涉及vllm在使用minicpmo模型时出现的图像类型错误。这个问题可能是由于集成vllm时出现的问题导致的。

https://github.com/vllm-project/vllm/issues/13137
这个issue属于用户提出需求类型，主要涉及FastAPI中如何访问请求、响应和令牌数的问题。原因是用户希望在服务器端访问这些数据以便存储。

https://github.com/vllm-project/vllm/issues/13136
这是一个Bug报告，主要涉及deepseek-r1模型在使用过程中出现的多节点崩溃现象。可能是由于GPU内存异常下降导致的服务崩溃。

https://github.com/vllm-project/vllm/issues/13135
这是一个 Bug 报告，涉及的主要对象是“qwen2-vl with lora”，由于 PyTorch 版本2.5.1与CUDA版本12.1不兼容导致该问题。

https://github.com/vllm-project/vllm/issues/13134
这个issue类型是bug报告，主要涉及vllm 0.6.4.post1版本中的request hanging问题，由于100%的CPU利用率导致spawn !Image gpu !Image !Image任务无法正常运行。

https://github.com/vllm-project/vllm/issues/13133
这是一个Bug报告类型的issue，主要涉及到vllm软件的KV fp8功能。由于启用V1引擎导致输出错误，使得文本长度变长且内容无法阅读。

https://github.com/vllm-project/vllm/issues/13132
这个issue类型为bug报告，主要涉及的对象是代码中的init_device函数。隐藏或者注释掉init_device导致代码无法正常初始化设备，可能导致了设备无法正确工作或者启动的问题。

https://github.com/vllm-project/vllm/issues/13131
这是一个bug报告，主要涉及到内存使用计算不稳定的问题，由于在具有统一内存的系统上进行内存使用计算时可能会出现其他进程释放GPU内存的情况，导致断言失败。

https://github.com/vllm-project/vllm/issues/13130
这个issue类型是bug报告，主要涉及vllm的安装过程，由于无法检测版本导致构建Docker镜像失败。

https://github.com/vllm-project/vllm/issues/13129
这是一个Bug报告，主要涉及MambaCacheManager._assign_seq_id_to_cache_index的实现问题导致IndexError异常。

https://github.com/vllm-project/vllm/issues/13128
这是一个关于使用问题的issue，主要涉及的对象是vllm模型。可能由于版本不匹配导致的pre-fill和解码过程中是否使用Flash Attention v3实现的问题。

https://github.com/vllm-project/vllm/issues/13127
这是一个bug报告类型的issue，涉及的主要对象是vllm库中的Qwen2Tokenizer，由于版本问题导致调用AutoTokenizer.from_pretrained时出现AttributeError。

https://github.com/vllm-project/vllm/issues/13126
这是一个bug报告，涉及主要对象为GPUModelRunner类。由于GPUModelRunner未在一个序列完成时返回True，导致相关张量变得过时，进而导致了`get_token_bin_counts_and_mask`功能受损并抛出错误。

https://github.com/vllm-project/vllm/issues/13125
这个issue是bug报告，涉及的主要对象是使用 DeepSeekR1DistillQwen32B 模型的输出不完整，无法解析 reasoning_content。原因可能是参数配置或代码逻辑问题导致的。

https://github.com/vllm-project/vllm/issues/13124
这是一个bug报告类型的issue，涉及的主要对象是vllm库。由于tp参数大于1时，导致了vllm无法正常工作的问题。

https://github.com/vllm-project/vllm/issues/13123
这是一个bug报告，主要涉及的对象是CI（Continuous Integration）。由于缺少某些源文件的依赖关系，部分测试无法被正确运行。

https://github.com/vllm-project/vllm/issues/13122
这是一个Bug报告类型的Issue，主要涉及Vllm（一个模型训练工具）出现错误加载模型的问题。

https://github.com/vllm-project/vllm/issues/13121
这是一个bug报告，涉及DeepSeek-R1模型在H100x8*2设备上无法正常运行的问题，用户寻求协助解决。

https://github.com/vllm-project/vllm/issues/13120
这是一个用户提出需求的类型issue，主要涉及使用VLLM调试时如何设置断点的问题。用户想要禁用编译并像正常python脚本一样设置断点，因为当前执行的vllm是通过一个编译引擎管理的，不易直接在IDE中使用断点调试。

https://github.com/vllm-project/vllm/issues/13119
这是一个bug报告，涉及到针对特定模型的图层兼容性问题，导致需要在某些情况下切换到未经优化的内核以支持该图层。

https://github.com/vllm-project/vllm/issues/13118
该issue类型为需求提出，主要涉及的对象是Qwen和Qwen2模型。由于需要将Qwen和Qwen2模型转换为Qwen2.5并减小模型尺寸，可能是为了优化性能或资源利用效率。

https://github.com/vllm-project/vllm/issues/13117
这是一个bug报告，涉及benchmark_serving的初始测试运行失败，原因可能是benchmark参数未正确指定。

https://github.com/vllm-project/vllm/issues/13116
这个issue类型是bug报告，涉及到vllm安装时与xformers版本不一致导致的错误。

https://github.com/vllm-project/vllm/issues/13115
这是一个功能需求类型的issue，主要涉及的对象是vllm软件中的睡眠模式功能。

https://github.com/vllm-project/vllm/issues/13114
这是一个关于改进功能的issue，主要涉及precommit配置文件的位置问题。由于在中间位置的precommit建议影响文件编辑和阅读，用户提出将其移动到最后位置以提高可读性。

https://github.com/vllm-project/vllm/issues/13113
这是一个关于技术改进及功能优化的类型问题，主要涉及uvicorn在macOS环境下传递预创建的socket，原因是为了解决SO_REUSEADDR和SO_REUSEPORT相关设置的问题。

https://github.com/vllm-project/vllm/issues/13112
这是一个用户提出需求的issue，主要涉及到如何减少使用python-slim作为基础的vllm Docker镜像的大小。用户希望减小镜像大小，因为当前镜像大小过大，主要原因是vllm的依赖导致。

https://github.com/vllm-project/vllm/issues/13111
这是一个bug报告，主要涉及到最新FA3代码的更新，用户提到由于动态分割调度程序导致准确性下降。

https://github.com/vllm-project/vllm/issues/13110
该issue是关于代码重构的，并非bug报告，主要涉及的对象是sgmv kernels。由于代码结构不当导致无法有效地重用代码，需要对sgmv_shrink和sgmv_expand kernels进行重构以提高代码重用性。

https://github.com/vllm-project/vllm/issues/13109
这是一个用户提出需求的类型，该问题涉及的主要对象是支持AWQMarlin与MLA。

https://github.com/vllm-project/vllm/issues/13108
这是一个Bug报告，主要涉及对象是代码中的`stop_reason`和`EngineOutput`，问题是由于没有在获取原因时先检查`EngineOutput`中的`stop_reason`导致的。

https://github.com/vllm-project/vllm/issues/13107
这是一个优化请求，主要涉及减少与huggingface.co的 HTTP 调用，原因是为了改善性能和效率。

https://github.com/vllm-project/vllm/issues/13106
这个issue是关于bug报告，涉及到VLLM中的`intial_incremental_detokenization_offset`配置的问题，由于设置不正确导致了问题。

https://github.com/vllm-project/vllm/issues/13105
这是一个关于bug报告类型的issue，主要涉及vLLM引擎在使用prefix缓存时出现断言错误导致推断失败的问题。

https://github.com/vllm-project/vllm/issues/13104
这是一个功能需求类型的issue，主要涉及添加`SupportsQuant`到phi3和clip模型，通过引入这个mixin来标记模型支持量化推断，并解决`configure_quant_config`函数假设所有`packed_modules_mapping`都在初始化之前声明的问题。

https://github.com/vllm-project/vllm/issues/13103
这是一个bug报告，涉及的主要对象是VLLM中的`INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET`参数。原因是没有将此参数设置为可配置导致了与sentencepiece基于的tokenizer不兼容的问题。

https://github.com/vllm-project/vllm/issues/13102
这是一个文档改进类型的issue，主要涉及OpenVINO的安装文档，由于缺少git clone步骤在"Build wheel from source"的非GPU安装文档中，导致用户反馈缺失。

https://github.com/vllm-project/vllm/issues/13101
这个issue类型是bug报告，涉及的主要对象是V0 engine。由于没有异常处理机制，导致处理无效图像数据时会导致整个引擎崩溃，通过增加异常处理功能，解决了这一问题。

https://github.com/vllm-project/vllm/issues/13100
这是一个关于bug报告的issue，主要涉及的对象是Mistral Large模型，由于在调用并发长问题API请求时发生了"CUDA out of memory" crashes。

https://github.com/vllm-project/vllm/issues/13099
这是一个Bug报告，主要涉及Vllm中Qwen2VL传递`max_pixels`和`min_pixels`是否仍然受支持的问题。由于环境中的CUDA运行时版本无法收集，可能导致了此问题的出现。

https://github.com/vllm-project/vllm/issues/13097
这是一个bug报告类型的issue，主要涉及deepseek_r1_reasoning_parser的错误逻辑导致在特定情况下处理reason内容放置错误。

https://github.com/vllm-project/vllm/issues/13096
这是一个用户提出需求的issue，主要涉及V1 LoRA中添加新的triton kernels，因为V1不再根据LoRA ID对请求进行分组，需要新的kernels来加载适当的输入tokens。

https://github.com/vllm-project/vllm/issues/13095
这是一个用户提出需求的issue，主要涉及的对象是V1版本中的prefix_prefill kernel。原因是因为在V1版本中不跟踪上下文长度张量的情况下，需要改进kernel以根据查询长度和序列长度计算上下文长度。

https://github.com/vllm-project/vllm/issues/13094
这是一个用户提出需求的issue，主要涉及Llama模型在测试中的使用，由于模型更新和依赖关系变化，需要将部分模型统一使用较新的版本。

https://github.com/vllm-project/vllm/issues/13093
这是一个Bug报告，涉及到ray distributed inference中出现"Current node has no GPU available"错误的问题。可能是由于vLLM在设置ray placement group时与集群中的默认节点资源字典发生冲突导致。

https://github.com/vllm-project/vllm/issues/13092
这是一个bug报告，涉及的主要对象是DeepSeek-V3 v1中的attn_backend模块。由于模型具有额外的MLA参数，并且当前使用的实现不适用于这些参数，导致了错误的问题。

https://github.com/vllm-project/vllm/issues/13091
这是一个bug报告，涉及到在A100 GPU上在RedHat环境下运行vllm时出现的问题。造成问题的可能原因是xgrammar在该环境下与Triton、CUDA或NVIDIA驱动的兼容性问题。

https://github.com/vllm-project/vllm/issues/13090
这是一个bug报告，涉及的主要对象是V1版本的LLMEngine类。由于添加了检查到AsyncLLMEngine而未添加到LLMEngine，导致V1版本的兼容性受到影响。

https://github.com/vllm-project/vllm/issues/13089
这是一个关于Bug的报告，主要涉及随机数生成的问题。由于相同的种子可能会生成相同的随机数导致测试不稳定。

https://github.com/vllm-project/vllm/issues/13088
这是一个关于bug报告的issue，主要涉及`TransformersModel`中设置`torch_dtype`的问题，由于之前PR未完整进行CI运行，导致遗漏了错误。

https://github.com/vllm-project/vllm/issues/13087
这是一个Bug报告，涉及的主要对象是vllm库。用户设置stop参数后，导致第一个流式块为空。

https://github.com/vllm-project/vllm/issues/13086
这个issue类型是需求提出，涉及主要对象为GroupCoordinator，由于当前设备类型未正确初始化，导致其他设备无法正确初始化。

https://github.com/vllm-project/vllm/issues/13085
这是一个Bug报告issue，涉及对象为"r1gguf format"。由于版本0.7.2不支持gguf quantization导致的MLA错误。

https://github.com/vllm-project/vllm/issues/13084
这个issue是关于性能问题的讨论，涉及到TPOT的变化和影响因素。

https://github.com/vllm-project/vllm/issues/13083
这是一个需求提出的issue，主要涉及的对象是benchmark script和disaggregation proxy。由于没有特定模型名称提供，脚本使用了模型列表API中的第一个模型，为了与基准服务脚本兼容，需要在disaggregation proxy中添加模型列表API。

https://github.com/vllm-project/vllm/issues/13082
这是一个bug报告类型的issue，主要涉及vllm serving与deepseekv3的性能测试中出现请求丢失的问题。

https://github.com/vllm-project/vllm/issues/13081
这是一个bug报告，涉及的主要对象是VLLM（Very Large Language Model），由于代码中出现了'int'和'NoneType'之间的比较导致了这个TypeError。

https://github.com/vllm-project/vllm/issues/13080
这是一个功能需求的issue，主要涉及到Pixtral模型的定制化需求。

https://github.com/vllm-project/vllm/issues/13079
这是一个用户提出需求的issue，涉及的主要对象是v1 Sampler。由于缺少对logit_bias功能的支持，用户提出需要在v1 Sampler中引入该功能。

https://github.com/vllm-project/vllm/issues/13078
这是一个Bug报告类型的issue，主要涉及vllm服务中使用tool_choice指定工具时返回响应错误的问题。原因是模型未正确传递参数导致响应写入了`function.arguments`中。

https://github.com/vllm-project/vllm/issues/13077
这是一个bug报告，涉及VLLM项目中的CPU后端线程数设置问题，导致CI测试变慢。

https://github.com/vllm-project/vllm/issues/13076
这个issue是关于Bug报告，主要涉及vllm下的运行环境和出现的RuntimeError。

https://github.com/vllm-project/vllm/issues/13075
这是一个bug报告，涉及VLLM中使用CUDA backend时出现的运行错误。原因是缺少对应的运算符或者未将其包含在选择性/自定义构建过程中。

https://github.com/vllm-project/vllm/issues/13074
这是一个bug报告，涉及的主要对象是vLLM在使用多节点和GPU时的内存限制问题。这个问题可能是由于CUDA out of memory错误导致的。

https://github.com/vllm-project/vllm/issues/13073
这是一个关于技术细节讨论的issue，主要涉及vllm中关于recomputation和swapping效率比较的细节，用户想要了解为什么recomputation overhead永远不会高于swapping的20%的延迟。

https://github.com/vllm-project/vllm/issues/13071
这是一个Bug报告，主要涉及VLLM（Very Large Language Model）环境设置问题，导致数目不匹配的错误。

https://github.com/vllm-project/vllm/issues/13070
该issue属于用户需求类，主要涉及用户想要在vllm中运行qwen2.5_7B_Instruct大模型的推理操作，但不清楚如何与vllm集成，导致用户提出了如何使用特定模型的问题。

https://github.com/vllm-project/vllm/issues/13069
这是一个bug报告，该问题单涉及的主要对象是V1版本的KV cache manager。由于KV cache manager在V1版本中忽略了滑动窗口，导致用户希望实现的prefix caching与滑动窗口注意力不兼容。

https://github.com/vllm-project/vllm/issues/13068
这是一个功能需求的issue，涉及主要对象为vLLM的benchmark功能集成到PyTorch OSS benchmark数据库的问题。

https://github.com/vllm-project/vllm/issues/13067
这个issue是一个bug报告，主要涉及VLLM服务在长度为12000的提示输入下崩溃的问题。

https://github.com/vllm-project/vllm/issues/13066
这个issue是一个bug报告，主要涉及Deepseek-R1在2*8*H100环境下性能问题，可能由于输入提示较长导致推理速度突然下降至每秒不到1个标记。

https://github.com/vllm-project/vllm/issues/13065
这是一个用户提出需求的issue，主要涉及的对象是关于Meta Meetup的文档公告。

https://github.com/vllm-project/vllm/issues/13064
这是一个用户提出需求的issue，主要涉及到V1版本的PP功能，由于需要解耦批处理调度和模型执行，提出了一种新的引擎管理策略。

https://github.com/vllm-project/vllm/issues/13063
这是一个需求类型的issue，主要涉及的对象是对Benchmark的服务进行添加BurstGPT数据集。这个需求主要是为了提供包含更长prompt和outputs的BurstGPT数据集，而且目前的数据集并不包含请求的内容。

https://github.com/vllm-project/vllm/issues/13062
这是一个代码重组织和增加注释的issue，主要对象是SchedulerOutput类。

https://github.com/vllm-project/vllm/issues/13061
这是一个bug报告，涉及到vllm的KV缓存在两个4090的GPU上未充分利用的问题。主要原因可能是KV缓存的内存分配不准确导致无法充分利用两个GPU。

https://github.com/vllm-project/vllm/issues/13060
这是一个用户提出需求的问题，主要涉及V1版本中处理引擎实用方法的通用机制。这个问题出现的原因是为了集中处理需要在引擎上调用的一系列实用/“控制”操作，并且需要以同步方式调用以知道操作何时完成以及是否成功，因为未来的一些操作可能需要返回结果。

https://github.com/vllm-project/vllm/issues/13059
这个issue类型是功能需求，主要涉及到V1 ModelRunner的设备无关抽象。由于需要支持多个设备后端，导致需要对现有模型运行器进行转换。

https://github.com/vllm-project/vllm/issues/13058
该issue是一个需求提出类型的问题，主要涉及到vllm项目中缺少一些采样参数的支持，需要协助将这些参数在V1版本中进行完善。

https://github.com/vllm-project/vllm/issues/13057
这是一个Bug报告，涉及到VLLM中ROCm构建回退到CPU的问题。由于缺少torch.version.hip检查，导致构建回退到CPU。

https://github.com/vllm-project/vllm/issues/13056
该issue是一个需求提议，涉及到服务器配置信息的获取，由于部分参数值没有默认值和动态选择规则，希望提供一个端点返回服务器配置状态。

https://github.com/vllm-project/vllm/issues/13055
这个issue是关于特性增加的需求，主要涉及的对象是DeepSeek-V3模型，由于缺少DeepSeek-V3模型配置文件而导致需要添加并注册模型配置。

https://github.com/vllm-project/vllm/issues/13054
这是一个bug报告，主要涉及的对象是vLLM在Gaudi2硬件上的运行。由于view size与输入张量的size和stride不兼容，导致出现了RuntimeError。

https://github.com/vllm-project/vllm/issues/13053
这是一个bug报告，主要涉及的对象是vLLM中的HPUAttentionImpl模块。由于使用Gaudi2时，Encoder的self-attention和Encoder/Decoder之间的cross-attention没有实现，导致出现了错误。

https://github.com/vllm-project/vllm/issues/13052
这是一个Bug报告，涉及到GPU节点连接超时问题。原因可能是初始化Ray VLLM多节点Serving时出现了问题。

https://github.com/vllm-project/vllm/issues/13051
该issue类型为功能增强（Feature enhancement），主要涉及对象是CLIPVisionModel，由于需要支持加载量化的clip模型（phi3v），因此添加了`packed_modules_mapping`属性到CLIPVisionModel。

https://github.com/vllm-project/vllm/issues/13050
这个issue属于用户提出需求类型，主要涉及VLLM下的Qwen2.5-VL-7B-Instruct模型是否支持视频模式，由于目前看起来该模型不支持视频模式，用户希望了解是否有计划进行添加。

https://github.com/vllm-project/vllm/issues/13049
这是一个特性拓展类的issue，涉及主要对象为TPU（Tensor Processing Unit）。由于当前TPU attention kernel无法同时支持混合prefills和decodes，在同一个调度迭代中分开处理请求，导致了这个修改和重构的问题。

https://github.com/vllm-project/vllm/issues/13048
这是一个bug报告，主要涉及vllm安装问题。由于环境因素导致vllm从Git源安装出现错误。

https://github.com/vllm-project/vllm/issues/13047
这个issue是关于bug报告的，主要涉及的对象是在A100上运行`0.7.3.dev57+g2ae88905.precompiled`时出现`undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE`错误。可能的原因是在更新版本后出现了依赖问题。

https://github.com/vllm-project/vllm/issues/13046
这是一个功能需求的issue，主要涉及模型加载和分发过程中的性能优化问题。

https://github.com/vllm-project/vllm/issues/13045
这是一个bug报告，主要涉及到在使用Ray和8个A800运行deepseekr1时，一台机器的显存使用率很低，而另一台接近极限，用户想要找到平衡两台机器的方法。

https://github.com/vllm-project/vllm/issues/13044
这是一个Bug报告类型的issue，主要涉及vLLM中codegemma-7b的问题，由于环境配置等原因导致部署过程中出现崩溃。

https://github.com/vllm-project/vllm/issues/13043
这是一个bug报告类型的issue，涉及的主要对象是LoRA triton kernel device，由于cuda设备不等于cuda:0时初始化LLM时出现Triton错误。

https://github.com/vllm-project/vllm/issues/13042
这是一个Feature请求，主要涉及vllm中提供用于张量化权重的密钥生成的方法。请求的主要原因是希望能够使用给定的密钥而不是随机生成的密钥。

https://github.com/vllm-project/vllm/issues/13041
这是一个用户提出需求的issue，主要涉及vLLM中的Tensor encryption功能，用户希望能对adapter的权重进行加密。

https://github.com/vllm-project/vllm/issues/13040
这个issue属于bug报告类型，主要涉及torchvision库中的libcudart.41118559.so.12文件丢失导致无法打开共享对象文件的问题。

https://github.com/vllm-project/vllm/issues/13039
这是一个bug报告，涉及的主要对象是vllm下的openai frontend。这个问题的原因是当前只有`ValueError`异常被捕获，在`preprocess_chat`方法中发生的其他异常没有被捕获，导致无法使用`self.create_error_response`方法处理错误，最终导致出现`500 Internal Server Error` 错误。

https://github.com/vllm-project/vllm/issues/13038
这是一个Bug报告，主要涉及vllm在使用指定的json schema时出现的问题，导致输出结果与预期不符。

https://github.com/vllm-project/vllm/issues/13037
这是一个功能需求的issue， 主要涉及到 vllm 项目中的 GPUModelRunnerBase 模块，提出需要重构 load_model 方法来包含设备参数。

https://github.com/vllm-project/vllm/issues/13036
这是一个Bug报告，主要涉及LLM中的DeviceMemoryProfiler处理设备错误导致内存使用日志显示为0的问题。

https://github.com/vllm-project/vllm/issues/13035
这是一个Bug报告。该问题涉及到MetaLlama3.1405BInstructFP8模型在生成结果时只会产生感叹号的问题。由于使用vllm的指定版本时出现问题，但将版本降级到vLLM v0.6.6版本可以解决该问题。

https://github.com/vllm-project/vllm/issues/13034
这是一个用户提出需求的issue，主要涉及vllm中的多Lora推断支持问题。用户询问在llava v1.6上是否支持多Lora推断，或未来是否会有计划支持更多mllm，以及如何在llavanext上获得支持。这可能是由于当前环境下无法运行多Lora推断导致对应问题的提出。

https://github.com/vllm-project/vllm/issues/13033
这是一个用户提出需求的类型，主要对象是指定在启动docker镜像时指定进程名称的功能。

https://github.com/vllm-project/vllm/issues/13032
这个issue类型是性能提升建议，涉及的主要对象是cutlass_scaled_sparse_mm函数的参数设置问题，由于b_scales参数现在是标量而不是按通道设置，导致性能降级。

https://github.com/vllm-project/vllm/issues/13031
这是一个Bug报告，涉及到vLLM的性能指标与实际测量性能存在差异的问题。可能是由于计算方法或指标不准确导致的。

https://github.com/vllm-project/vllm/issues/13030
这是一个用户提出需求的问题，主要涉及对象为在ARM架构服务器上使用NVIDIA GPU运行VLLM的功能。由于环境需要在特定架构的服务器上运行VLLM并利用GPU加速，因此用户想知道是否支持这种配置。

https://github.com/vllm-project/vllm/issues/13029
这是一个bug报告，涉及的主要对象是Python代码中的资源模块，在Windows系统上不应该导入资源模块导致了模块不存在的错误。

https://github.com/vllm-project/vllm/issues/13028
这是一个用户提出需求的issue，主要涉及如何在VLLM中实现类似Ultravox模型的记忆和流式推理功能。

https://github.com/vllm-project/vllm/issues/13027
这是一个bug报告，主要涉及的对象是executor模块。由于没有正确初始化`local_rank`，导致在指定除card0以外的device时会出现设备冲突。

https://github.com/vllm-project/vllm/issues/13026
这是一个用户提出需求的类型的issue，主要涉及的对象是关于vllm的max_num_seqs参数。由于用户想要运行特定模型的推理，但不清楚如何与vllm集成，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/13025
这个issue是一个bug报告，涉及的主要对象是chat template的Reasoning output实现。这个bug是由于chat template的更改导致了reasoning parser实现的功能上的错误。

https://github.com/vllm-project/vllm/issues/13024
这个issue属于bug报告类型，涉及主要对象是代码中的注释文本。由于存在拼写错误导致了注释内容不准确，需要修正以提高代码文档的准确性。

https://github.com/vllm-project/vllm/issues/13023
这是一个bug报告，涉及主要对象为ColumnParallelLinear 模块，由于在初始化 GGUF 权重时未考虑到 tensor parallel 的情况，导致加载权重时出现尺寸不匹配的问题。

https://github.com/vllm-project/vllm/issues/13022
这是一个bug报告，涉及到AMD构建作业在条件设置中被错误识别为CPU，导致构建失败。

https://github.com/vllm-project/vllm/issues/13021
该issue是关于文档错误，用户在LLM类中找不到max_model_len参数。

https://github.com/vllm-project/vllm/issues/13020
这是一个需求提议（Request for Comments）的类型，涉及的主要对象是异步KV缓存传输，用户提出了关于优化解码执行过程中KV缓存传输的方案。

https://github.com/vllm-project/vllm/issues/13018
这是一个bug报告，主要涉及vllm中不支持MiniCPMo 2.6的finetune lora，导致合并后的模型无法运行的问题。

https://github.com/vllm-project/vllm/issues/13016
这是一个用户提出需求的issue，主要涉及的对象是Client，用户希望添加sleep和wake_up方法来进行模型状态管理。

https://github.com/vllm-project/vllm/issues/13015
这个issue是关于用户提出需求的，主要涉及vllm的服务在DDP模式下的使用方式。用户想要在单节点上使用多个GPU来启动多个vllm后端，并希望了解是否有相应功能支持或者需要自行实现，以便在这些后端间分配请求进行处理。

https://github.com/vllm-project/vllm/issues/13014
这是一个Bug报告，涉及DeepSeek V3的启动问题，由于分布式操作中NCCL通信层的问题导致Python致命错误。

https://github.com/vllm-project/vllm/issues/13013
这是一个bug报告，用户报告CUDA内存不足的问题。

https://github.com/vllm-project/vllm/issues/13012
这是一个bug报告类型的issue，主要涉及到multi-modal processors的清理和修复。这个问题的症状是multi-modal processors在单图像模型中测试时只测试了`limit 1`，而不是`limit`数据项，导致多模态输入完全被忽略。

https://github.com/vllm-project/vllm/issues/13011
这是一个Bug报告类型的issue，主要涉及日志输出格式问题，原因是缺少了一个开方括号导致日志格式错乱。

https://github.com/vllm-project/vllm/issues/13010
该issue属于用户提出问题，主要涉及LLama-2-7b-hf中运算符功能的对应检查。原因可能是环境信息收集时的一些数据未能完整采集导致用户需求无法实施。

https://github.com/vllm-project/vllm/issues/13009
这个issue是一个Bug报告，主要涉及的对象是vLLM 0.7.2版本的multimodal models部署问题，发生了对方法`determine_num_available_blocks`的处理异常。

https://github.com/vllm-project/vllm/issues/13008
这是一个bug报告，涉及到文件存在性检查操作，由于`file_exists`经常在CI上超时导致作业失败，该问题引入了重试机制以期望通过指数退避策略来确保请求能够成功发送至HF。

https://github.com/vllm-project/vllm/issues/13007
这是一个bug报告，涉及到vLLM中的`seed_everything`方法。由于`seed_everything`方法在加载模型时对`seed=None`处理不当，导致随机种子行为混乱。

https://github.com/vllm-project/vllm/issues/13006
这个issue是一个bug报告，主要涉及的对象是florence2模块，由于当前环境中所使用的PyTorch版本和CUDA版本不兼容，导致florence2模块无法支持多模态输入。

https://github.com/vllm-project/vllm/issues/13005
该issue类型为功能需求，涉及主要对象是Chat Prefix Completion。由于文中提到现有的pr需要对引擎端或结构化输出引擎进行更改，导致用户提出了在前端实现该功能的建议。

https://github.com/vllm-project/vllm/issues/13004
该issue是一个功能需求，涉及到在多节点和多GPU上实现"Disaggregated Prefill"功能，用户希望了解在这方面的路线规划。这个问题是由于当前仅支持1P1D，因此无法使用所需功能引起的。

https://github.com/vllm-project/vllm/issues/13003
该问题类型为文档相关，主要涉及工具调用中的链接更新，用户提出需求以便其他人评论和反馈。

https://github.com/vllm-project/vllm/issues/13002
这是一个用户提出需求的issue，主要涉及tool_choice选项中"required"选项的支持情况。该问题由于"required"选项暂未在工具调用中支持，用户提出需要追踪该选项的支持情况。

https://github.com/vllm-project/vllm/issues/13001
这是一个bug报告，涉及主要对象是vLLM中的sleep mode和pytorch checkpoint。由于`torch.cuda.empty_cache()`调用导致使用pytorch checkpoint与sleep mode不兼容。

https://github.com/vllm-project/vllm/issues/13000
这是一个空白的issue，类型未知，无法确定问题单涉及的主要对象。

https://github.com/vllm-project/vllm/issues/12999
这是一个bug报告类型的issue，主要涉及的对象是vllm V0.7.2模型，由于输入输出结果中缺少反斜杠字符'\ '，导致deepseek的reasoning_content始终为None。

https://github.com/vllm-project/vllm/issues/12998
这是一个用户提出需求的issue，主要涉及的对象是VLLM_CUDART_SO_PATH环境变量。用户提出了一个需求，希望可以通过指定VLLM_CUDART_SO_PATH环境变量解决find_loaded_library失败的问题。

https://github.com/vllm-project/vllm/issues/12997
这是一个用户提出需求的问题单，主要涉及VLLM双机部署Deepseek-R1，由于用户不清楚如何在两台服务器上设置通信信息而寻求帮助。

https://github.com/vllm-project/vllm/issues/12996
这个issue是用户提出需求，涉及主要对象为实现pipeline parallel on Ray的功能。由于目前尚未实现调度和流水线操作，只支持在不同pipeline parallel工作者上运行分离的模型层，因此用户提出了这一功能需求。

https://github.com/vllm-project/vllm/issues/12995
这个issue是一个用户提出的需求类型，主要涉及功能支持中用户指定“触发”标记在开始结构化解码前。导致这个需求是因为用户想要能够在执行结构化解码之前指定一个特定的“触发”标记。

https://github.com/vllm-project/vllm/issues/12994
这是一个bug报告，涉及使用ramalama运行vllm时崩溃的问题。

https://github.com/vllm-project/vllm/issues/12993
这是一个用户提出需求的issue，主要对象是"Whisper proto"。由于未提供具体内容，无法分析具体导致何种症状的问题。

https://github.com/vllm-project/vllm/issues/12992
这是一个RFC（Request For Comments），主要涉及vLLM V1架构扩展设备支持，并且由于扩展设备支持时重用现有代码结构所带来的挑战。

https://github.com/vllm-project/vllm/issues/12991
这是一个bug报告，主要涉及vllm模型的运行环境配置问题，导致无法正常运行。

https://github.com/vllm-project/vllm/issues/12990
这是一个bug报告，涉及VLLM安装过程中的编译错误，主要问题是当前环境下不支持非NVIDIA GPU。

https://github.com/vllm-project/vllm/issues/12989
这是一个bug报告，主要涉及vllm在构建时遇到的错误。可能由于依赖库或环境配置不兼容导致。

https://github.com/vllm-project/vllm/issues/12988
这是一个bug报告，主要涉及的对象是Qwen2.5-VL-72B-Instruct-AWQ模型。该问题可能由于TP参数设置为2时带来的性能下降导致了模型运行速度缓慢的状况。

https://github.com/vllm-project/vllm/issues/12987
这个issue是一个需求类型，主要涉及core功能，需要添加`/sleep`和`/wake_up`端点并在v1版本中支持。

https://github.com/vllm-project/vllm/issues/12986
这是一个bug报告，主要涉及的对象是vllm Docker运行命令。由于缺少'='符号，导致无法正确分配NVIDIA运行时，造成bug。

https://github.com/vllm-project/vllm/issues/12985
这是一个Bug报告，涉及GGUF模型在tokenization时出现问题。

https://github.com/vllm-project/vllm/issues/12984
这是一个用户提出需求的类型，该问题涉及到在RLHF中使用cudaipc同步训练actors的权重到vllm workers时的示例问题。

https://github.com/vllm-project/vllm/issues/12983
这是一个Bug报告，主要涉及的对象是VLLM容器。这个问题是由于Ray未在驱动节点上分配任何GPU导致的，建议调整Ray放置组或在GPU节点上运行驱动程序。

https://github.com/vllm-project/vllm/issues/12981
这是一个bug报告类型的issue，主要涉及vLLM在从睡眠模式唤醒时内存不足导致错误处理不当的问题，用户反馈出现了段错误的症状。

https://github.com/vllm-project/vllm/issues/12980
这是一个bug报告，涉及的主要对象是vllm的0.7.2 Docker Container和Qwen 2.5 VL。由于Docker VLLM Container未能支持Qwen 2.5 VL导致的bug。

https://github.com/vllm-project/vllm/issues/12979
该issue类型是需求，主要对象是vllm包中的_version.py文件，由于需要定制化逻辑并确保"version"库始终是vllm包中首先导入的库，因此提出该需求。

https://github.com/vllm-project/vllm/issues/12978
这是一个优化需求类型的issue，主要涉及到 CUTLASS kernels。由于K值较大时性能较慢，提出了使用`StreamK`策略优化GEMM计算的方法。

https://github.com/vllm-project/vllm/issues/12977
这是一个关于需求的RFC（Request For Comments）类型的issue，主要涉及到`pynvml`模块的使用问题，由于版本冲突导致了安装错误。

https://github.com/vllm-project/vllm/issues/12976
这是一个bug报告，主要涉及到xgrammar模块的导入问题，导致guided decoding无法正常使用。

https://github.com/vllm-project/vllm/issues/12975
该issue是一个Bug报告，主要涉及的对象是vllm的get_attn_backend_cls()函数。该问题是由于没有检查选定的后端是否为None导致出现"cannot use None backend"的误导性消息。

https://github.com/vllm-project/vllm/issues/12974
这是一个功能需求类型的issue，主要涉及到vLLM模型在跳过特定层的4比特量化时的问题。由于需要跳过特定的模块并保留16比特的量化，用户提出了此功能需求。

https://github.com/vllm-project/vllm/issues/12973
这是一个Bug报告，主要涉及VLLM框架在加载视觉模型时导致CPU内存溢出的问题。

https://github.com/vllm-project/vllm/issues/12972
这个issue类型是bug报告，涉及的主要对象是CUDA detection，由于pynvml版本过旧导致CUDA检测失败并产生了静默错误。

https://github.com/vllm-project/vllm/issues/12971
这个issue是关于重构和修复CLI代码的，涉及主要对象为vllm的命令行界面（CLI）。由于代码需要重构和一些bug需要修复，导致了一些错误消息和溯源问题的出现。

https://github.com/vllm-project/vllm/issues/12970
该issue类型为代码重构，主要涉及 punica ops 测试，需将两个测试文件合并为一个。

https://github.com/vllm-project/vllm/issues/12969
这是一个关于性能优化的issue，主要涉及对象是`GPUModelRunner`中的缓存`uses_mrope`。导致这个问题的原因是`model_config.uses_mrope` 每次调用时都会读取HF配置，导致不必要的开销。

https://github.com/vllm-project/vllm/issues/12968
这是一个清除过时注释的issue，类型为代码优化/维护，主要涉及代码库中的注释。

https://github.com/vllm-project/vllm/issues/12967
这是一个Bug报告，主要涉及的对象是vllm下的LLM类。由于设备相关的问题导致Triton在多GPU设置中初始化`LLM`类时出错，无法访问某些指针。

https://github.com/vllm-project/vllm/issues/12966
这是一个需求提出的issue，主要涉及的对象是VLM中的merged multi-modal processor。由于目前模型实现可以处理多图像输入，但需要重构提示替换逻辑才能支持在vLLM中的端到端支持，因为图像令牌是在字符串开头插入而不替换任何部分。

https://github.com/vllm-project/vllm/issues/12965
这是一个bug报告类型的issue，主要涉及在Windows机器上通过VSCode安装vllm时出现的安装错误。造成这个bug的原因可能是与PyTorch特定版本安装相关的问题。

https://github.com/vllm-project/vllm/issues/12964
这个issue是关于安装问题，涉及对象为用户尝试从源代码安装软件时遇到长时间卡住的情况。原因可能是环境设置问题或安装过程中的其它异常。

https://github.com/vllm-project/vllm/issues/12963
这个issue是一个任务类型，主要涉及的对象是将pynvml端口到vllm代码库中。这个任务的原因可能是为了修复与pynvml相关的功能问题或提高系统性能。

https://github.com/vllm-project/vllm/issues/12962
这是一个bug报告，涉及的主要对象是CUDA_VISIBLE_DEVICES环境变量。由于`RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES`设置后ray不再操作`CUDA_VISIBLE_DEVICES`，如果节点环境也未设置`CUDA_VISIBLE_DEVICES`，就会导致直接访问 `os.environ["CUDA_VISIBLE_DEVICES"]`时抛出KeyError。

https://github.com/vllm-project/vllm/issues/12961
这是一个bug报告，主要涉及模型中缺少名为'base_model'的模块或参数。可能由于环境配置问题，导致出现这样的错误信息。

https://github.com/vllm-project/vllm/issues/12960
这是一个需求提出类型的issue，该问题单涉及的主要对象是`transformers` backend，由于缺乏BNB支持和关于量化的更新文档，用户希望在该backend上启用量化支持。

https://github.com/vllm-project/vllm/issues/12959
这个issue是关于bugfix的，主要对象是vllm下的rlhf test example，在该issue中由于早期导入了vllm flash attention导致了功能破坏。

https://github.com/vllm-project/vllm/issues/12958
这是一个bug报告类型的issue，涉及VLLM程序在特定环境下出现CUDA初始化错误的问题。

https://github.com/vllm-project/vllm/issues/12957
这个issue属于功能需求，主要涉及到支持XpYd的分散预填充与MooncakeStore。由于需要实现XpYd功能，并且需要协调元数据信息，所以提出了这个功能需求。

https://github.com/vllm-project/vllm/issues/12956
这是一个bug报告，涉及到使用VLLM的问题，由于VLLM_USE_V1=1参数与deepseek-v3组件配合时出现错误，导致命令执行失败。

https://github.com/vllm-project/vllm/issues/12955
这是一个用户提出需求的issue，主要涉及Guided/Structured grammar和lmformatenforcer之间的支持问题。

https://github.com/vllm-project/vllm/issues/12954
这个问题是一个bug报告，主要涉及DeepSeek-R1的输出包含乱码字符的问题，可能是由于代码中的某些原因导致输出中包含了多种语言的字符。

https://github.com/vllm-project/vllm/issues/12953
这是一个用户提出的需求类型的issue，主要涉及LMCache连接器，由于需要支持KV缓存卸载和disagg预填充。

https://github.com/vllm-project/vllm/issues/12952
这是一个bug报告，涉及的主要对象是`vllm`中的`cmake/utils.cmake`，由于没有添加`${Python_EXECUTABLE}`导致bug出现。

https://github.com/vllm-project/vllm/issues/12951
这是一个安装问题的bug报告，主要涉及的对象是vLLM的安装过程。据描述，因为未检测到平台信息，导致了环境信息收集出现异常。

https://github.com/vllm-project/vllm/issues/12950
这是一个用户提出需求的类型，主要涉及多节点上进行离线推理操作。用户想知道如何在分布式机器上运行模型并将参数放入多节点的不同GPU进行离线推理。

https://github.com/vllm-project/vllm/issues/12949
这是一个文档问题（Doc）类型的Issue，主要涉及到HF仓库名称的错误，由于缺少一个符号，导致用户在Hugging Face（HF）的TeleAI仓库中无法正确找到所需的信息。

https://github.com/vllm-project/vllm/issues/12948
这个issue是关于bug报告，主要对象是DeepSeek-R1 671B with vLLM 0.7.2，并由于NCCL Error导致问题。

https://github.com/vllm-project/vllm/issues/12947
这是一个bug报告，主要涉及到使用vllm072启动deepseek-r1-awq时出现RuntimeError导致的错误。

https://github.com/vllm-project/vllm/issues/12946
这是一个bug报告，主要涉及的对象是flash_atten模块。由于环境中缺少所需的GLIBCXX_3.4.26版本，导致出现了ImportError错误，用户在寻求如何关闭flash_atten模块的帮助。

https://github.com/vllm-project/vllm/issues/12945
这是一个 bug 报告，主要涉及 flash_atten 模块，由于缺少 GLIBCXX_3.4.26 版本导致 ImportError 错误。

https://github.com/vllm-project/vllm/issues/12944
这是一个用户提出需求的issue，主要涉及的对象是Qwen2.5VL模型，用户希望在该模型中添加tp兼容BNB支持。

https://github.com/vllm-project/vllm/issues/12943
这是一个非Bug报告类型的issue，主要涉及代码重组，目的是简化`_prepare_inputs`函数。

https://github.com/vllm-project/vllm/issues/12942
这是一个bug报告，涉及到V1不支持MOE的问题，用户在运行mixtral8X7BInstruct模型时遇到了服务器无法正常启动的情况。

https://github.com/vllm-project/vllm/issues/12941
这是一个需求提出的issue，主要涉及到CI/Build的自动修复Markdown文件，由于需要更新precommit hook并修改lint规则，以自动修复错误并应用lint规则到所有Markdown文件。

https://github.com/vllm-project/vllm/issues/12940
这个issue是一个bug报告，涉及的主要对象是vllm下的一个multi-modal model。导致这个bug的原因是和sequence length以及context window limitations相关的三个critical issues。

https://github.com/vllm-project/vllm/issues/12939
该issue是关于功能需求的，主要涉及PD separation支持prefix caching，由于用户在prefix_caching开启时希望恢复真实的seq_lens，以避免当prefill_caching为True时出现的长度问题。

https://github.com/vllm-project/vllm/issues/12938
这是一个bug报告，该问题涉及tool_chat_template_llama，由于tool_calls为空列表，导致无法处理情况的错误。

https://github.com/vllm-project/vllm/issues/12937
该issue属于Bug报告，主要涉及tool_chat_template_llama工具无法处理tool_calls为空列表的情况，导致vLLM抛出错误。

https://github.com/vllm-project/vllm/issues/12936
这是一个Bug报告，主要涉及对象是VLLM中使用TPU时出现的Torch未启用CUDA的AssertionError。原因可能是Torch未正确编译CUDA的问题导致程序崩溃。

https://github.com/vllm-project/vllm/issues/12935
这是一个关于bug报告的issue，主要涉及的对象是ModelInputForGPU类中的seq_group_metadata_list变量。由于seq_group_metadata_list变量未初始化且未被使用，导致在执行execute_model函数时始终为None，造成相关功能异常或性能问题。

https://github.com/vllm-project/vllm/issues/12934
这是一个bug报告，主要涉及的对象是程序中的分布式执行器后端。这个问题是由于即使世界大小为1，也应该尊重用户指定的`distributed_executor_backend`，如果不尊重则会导致问题。

https://github.com/vllm-project/vllm/issues/12933
这是一个bug报告，涉及的主要对象是vllm的平台检测逻辑。由于在导入时进行平台检测导致了日志中出现大量的噪音信息。

https://github.com/vllm-project/vllm/issues/12932
这是一个bug报告，主要涉及的对象是模型架构，可能由于模型架构检查失败导致数值错误。

https://github.com/vllm-project/vllm/issues/12931
这是一个性能优化的问题，主要涉及到Ampere架构A16W8的量化优化，主要是针对GPTQ量化模型。

https://github.com/vllm-project/vllm/issues/12930
这个issue类型是用户提出需求，主要涉及的对象是vLLM的VRAM预分配调整，由于当前vLLM预分配的VRAM过多，导致用户在单个显卡上无法运行模型。

https://github.com/vllm-project/vllm/issues/12929
这是一个bug报告类型的issue，主要涉及VLLM V1 engine 在离线批处理推断代码中无法完成推断的问题。

https://github.com/vllm-project/vllm/issues/12928
这个issue类型是需求补充，涉及的主要对象是代码注释。这个问题由于原来代码中的注释不清晰，需要移除后补充更准确的注释。

https://github.com/vllm-project/vllm/issues/12927
这个issue类型是bug报告，主要涉及前端上传功能的优化，由于上传时可能出现超时等异常导致需要重试。

https://github.com/vllm-project/vllm/issues/12926
这个issue类型是功能改进，主要涉及的对象是日志记录时间消耗用于从HF下载权重，用户希望通过记录下载时间和模型权重加载时间来量化地了解本地缓存的性能提升，以评估（和优化）引擎的性能。

https://github.com/vllm-project/vllm/issues/12925
这是一个bug报告类型的issue，涉及的主要对象是解决transformers-neuronx版本冲突的问题。由于transformers-neuronx==0.13.*与transformers==2.48.2版本冲突，导致出现了权限错误和后端测试问题。

https://github.com/vllm-project/vllm/issues/12924
该issue类型为性能问题报告，主要涉及提高性能和性能回归的报告。

https://github.com/vllm-project/vllm/issues/12923
这是一个用户提出的需求类型的issue，主要涉及到对于Meta内部AMD构建的支持。由于内部无法直接使用sys.executable，导致需要修改子进程参数。

https://github.com/vllm-project/vllm/issues/12922
这个issue类型是功能改进，主要涉及对象是`Request`类和`KVCacheManager`类，由于`kv_block_hashes`属性仅被KV缓存管理器使用，将其从`Request`类移至`KVCacheManager`类以简化`Request`类。

https://github.com/vllm-project/vllm/issues/12921
这个issue是一个bug报告，主要涉及Neuron Kernel的修改，由于之前存在的两个tiling问题和代码质量问题，导致了Flash PagedAttention无法处理较大输入。

https://github.com/vllm-project/vllm/issues/12920
这是一个需求提出的issue，主要是关于优化CUDA图捕获在特定批处理大小下性能波动的问题。

https://github.com/vllm-project/vllm/issues/12919
这是一个用户提出需求的issue，主要涉及vLLM的自动基准测试，扩展了性能基准测试以包括v0和v1，确保性能仪表板不受影响。

https://github.com/vllm-project/vllm/issues/12918
这个issue是关于优化请求序列化以及处理不同请求类型的改进，涉及对象是消息传递过程中的序列化方式及请求处理逻辑。由于msgpack不原生支持张量，而多模请求需要张量，因此现阶段在处理多模请求时还是直接使用pickle进行序列化，导致部分请求效率低下。

https://github.com/vllm-project/vllm/issues/12917
这是一个功能需求的issue，主要对象是vLLM中的日志记录方式。用户提出需要在一个日志消息中记录模型权重加载时间以简化分析过程。

https://github.com/vllm-project/vllm/issues/12916
这是一个用户提出需求的类型的issue，主要涉及到改进vLLM的日志记录功能。用户想要区分下载模型和加载模型权重两个步骤所消耗的时间。

https://github.com/vllm-project/vllm/issues/12915
这是一个技术需求问题，主要涉及DeepSeek-R1模型中的EAGLE-style MTP模块的实现。由于模型代码的变动和对模型输出的处理方式改变，导致需要注册新的实现来提高预测精度。

https://github.com/vllm-project/vllm/issues/12914
这是一个bug报告，涉及的主要对象是vLLM OpenAI-Compatible API与microsoft/Phi-3.5-vision-instruct的集成。由于环境信息中显示的PyTorch版本与CUDA版本不匹配，可能导致请求随机挂起的bug。

https://github.com/vllm-project/vllm/issues/12913
该issue类型是功能添加请求，主要涉及的对象是添加脚本以为多节点vllm部署设置ray。bug或者用户提出问题的原因是需要通过脚本启动ray集群并等待所有ray worker启动完成，而不会启动vllm服务器本身。

https://github.com/vllm-project/vllm/issues/12912
该issue是一个功能需求，主要对象是Ultravox音频/文本到文本模型。由于对架构进行了一些微小调整，需要支持v0.5版本发布，需要添加一个测试来验证新版本。

https://github.com/vllm-project/vllm/issues/12911
这是一个用户提出需求的issue，涉及支持Ray CG(ADAG) for NPU的功能需求，主要原因是为了加速推理过程。

https://github.com/vllm-project/vllm/issues/12910
这是一个功能需求的issue，主要涉及单分类任务中缺少logits导致判断错误的问题，用户希望添加logits以便更好地进行分类。

https://github.com/vllm-project/vllm/issues/12909
这是一个功能需求类的issue，主要涉及在vllm项目中添加一个名为`/v1/audio/transcriptions`的OpenAI API端点。

https://github.com/vllm-project/vllm/issues/12908
这是一个改进建议，旨在提升LLM模型在速度估计中的表现，主要涉及性能指标的调整。

https://github.com/vllm-project/vllm/issues/12906
这是一个bug报告，涉及的主要对象是nvidiamlpy库，由于其动态创建的异常类无法反序列化导致了TypeError的bug。

https://github.com/vllm-project/vllm/issues/12905
这是一个Bugfix类别的issue，涉及到问题对象是Qwen2_5_VLForConditionalGeneration的packed_modules_mapping。由于packed_modules_mapping存在问题，导致与BNB发生了问题。

https://github.com/vllm-project/vllm/issues/12904
这是一个Bug报告，主要涉及对象是vllm库中的LLM模型初始化的过程。由于代码中直接使用字符串赋值给MODEL_PATH，而在初始化LLM模型时需要传入bytes类型的对象，所以导致了报错"TypeError: a bytes-like object is required, not 'str'"。

https://github.com/vllm-project/vllm/issues/12903
这个issue类型是文档更新，主要涉及Benchmark模块的README.md文件，用户提出帮助新手入门的connectthedots指引。

https://github.com/vllm-project/vllm/issues/12902
这是一个Bug报告，涉及的主要对象是Qwen2.5VL72BInstructbnb4bit模型。由于运行环境中出现了错误导致无法成功启动该模型。

https://github.com/vllm-project/vllm/issues/12901
这是一个关于需求提出的issue，主要涉及对象是VLLM V1，用户提出了对Speculative decoding支持的需求。

https://github.com/vllm-project/vllm/issues/12900
这个issue类型是bug报告，主要涉及问题是在使用vllm中启动qwen2.5vl7binstruct时出现了错误，可能是由于数据类型不匹配导致了4bit inflight quantization的失败。

https://github.com/vllm-project/vllm/issues/12899
这是一个bug报告，涉及到vllm中qwen2.5vl7binstruct模型在使用4bit inflight quantization时初始化失败，可能是由于quantization bitsandbytes loadformat bitsandbytes参数添加不正确导致的。

https://github.com/vllm-project/vllm/issues/12898
这是一个功能需求的Issue，涉及主要对象是 benchmark_serving.py，用户寻求添加 LoRA 模型到性能测试中。

https://github.com/vllm-project/vllm/issues/12897
这个issue是一个bug报告，主要涉及的对象是`ErrorResponse`类。这个bug由于错误响应格式不符合OpenAI API要求，导致出现了错误响应消息不匹配的问题。

https://github.com/vllm-project/vllm/issues/12896
这是一个bug报告，涉及文件中的拼写错误问题。由于拼写错误导致了混淆，但不影响代码的运行。

https://github.com/vllm-project/vllm/issues/12895
这是一个bug报告，主要涉及到vllm下的pipeline parallel deployment中存在的内存不平衡问题，可能由于模型层分区不均匀导致最后一个节点消耗GPU内存过多而导致的问题。

https://github.com/vllm-project/vllm/issues/12894
这是一个用户提出需求的 issue，主要涉及 vLLM 的版本定制问题，用户想要构建指定版本的 wheel 文件而非默认版本。

https://github.com/vllm-project/vllm/issues/12893
这是一个 bug 报告，主要涉及 vllm 安装在苹果设备（Mac）上出现的错误。原因是在安装过程中出现了 AttributeError: 'NoneType' object has no attribute 'splitlines' 错误。

https://github.com/vllm-project/vllm/issues/12892
这是一个bug报告类型的issue，主要涉及的对象是DeepSeek-R1 671B模型在8x H200 SXM上产生混乱的输出结果。原因可能是模型在特定环境下无法产生意义的自然语言响应。

https://github.com/vllm-project/vllm/issues/12891
这是一个bug报告，涉及主要对象是DeepSeek R1的部署过程，由于LoRa失败，导致无法在vLLM Engine V1上部署DeepSeek R1实例。

https://github.com/vllm-project/vllm/issues/12890
这个issue类型为性能问题报告，主要涉及到vllm模型在部署时性能不佳的情况。由于部署DeepSeek-V3时通过vllm仅能达到6500的最大模型长度，但通过sglang却可以达到163840，可能导致该性能问题的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/12889
这是一个Bug报告，涉及vllm版本0.7.2的下载和加载问题。由于当前环境的问题，用户无法继续操作。

https://github.com/vllm-project/vllm/issues/12888
这是一个关于优化构建的issue，涉及主要对象是CI/Build系统。由于容器的时区设置不明确，导致日志记录混乱，因此用户提出将容器时区设置为UTC的需求。

https://github.com/vllm-project/vllm/issues/12887
这是一个关于性能问题的报告，主要涉及的对象是vLLM推理速度的批处理大小为1和10时生成相同数量的令牌。由于可能是模型集成或配置方面的问题，导致推理速度出现问题。

https://github.com/vllm-project/vllm/issues/12886
这是一个Bug报告，问题涉及到vllm的错误响应格式不符合OpenAI API标准，导致集成系统无法正确处理错误消息。

https://github.com/vllm-project/vllm/issues/12885
这是一个用户提出需求的类型的issue，主要涉及VLM库中对多模态提示的支持需求。由于当前代码中的图像token被堆叠在文本提示的开头，可能会影响教师模型的表现，用户希望实现完全交错的多模态提示支持。

https://github.com/vllm-project/vllm/issues/12884
这是一个Bug报告，涉及到Mistral tokenizer在处理缺少工具参数时出现问题。由于Mistral schema不接受没有参数字典的函数，导致传递请求时出现错误。

https://github.com/vllm-project/vllm/issues/12883
这是一个用户提出需求的issue，主要涉及对象是LoRA功能，由于尚未实现其他LoRA具体的serving功能，用户在此提出将`add_lora`函数与`lora_model_runner_mixin.py`连接的需求。

https://github.com/vllm-project/vllm/issues/12882
该issue属于bug报告类型，主要涉及greedy search结果不稳定的问题。由于设置了top_k=1和temperature=0后仍无法获得稳定的结果，导致有时运行结果完全一致，有时存在多个不一致之处。

https://github.com/vllm-project/vllm/issues/12881
这是一个用户提出需求的问题，主要涉及如何使用vllm进行推断，讨论了关于max_num_seqs参数含义的问题。可能的原因是用户不清楚如何将特定模型集成到vllm中。

https://github.com/vllm-project/vllm/issues/12880
这个issue类型是功能改进，主要对象是pre-commit hooks，由于需要将命令拆分为entry和args，以提升功能的灵活性。

https://github.com/vllm-project/vllm/issues/12879
这是一个提出如何在多模态输入中使用本地视频的问题，涉及到vllm模型的使用。用户需要解决如何将本地视频引入到该模型中进行聊天生成。

https://github.com/vllm-project/vllm/issues/12878
这个issue类型属于用户提出需求，主要涉及到数据结构的选择。由于每个请求需要独立的隐藏状态存储，用户想探讨是否使用IntermediateTensors最合适，以及是否有更简化的存储结构可供使用。

https://github.com/vllm-project/vllm/issues/12876
这个issue属于bug报告类型，主要涉及到模型调用中的ImportError问题。由于缺少必要的依赖导致程序无法正确导入需要的模块。

https://github.com/vllm-project/vllm/issues/12875
这是一个bug报告，涉及vllm是否支持qwen2.5-vl，用户想了解该支持是否已经实现。

https://github.com/vllm-project/vllm/issues/12874
这是一个bug报告，主要涉及的对象是vllm软件的安装过程。导致问题的原因是安装在CPU平台上无法正常工作。

https://github.com/vllm-project/vllm/issues/12873
这是一个Bug报告类型的Issue，主要涉及VLLM的使用环境和运行推理过程中出现的CUDA driver错误。

https://github.com/vllm-project/vllm/issues/12872
这是一个bug报告，涉及到Lora模型加载失败的问题。由于NCCL操作未被正确销毁导致报错，产生了无法加载Lora模型的症状。

https://github.com/vllm-project/vllm/issues/12871
这是一个性能优化的建议，该问题主要涉及支持DP attention for Deepseek models。原因是为了提高性能和吞吐量。

https://github.com/vllm-project/vllm/issues/12870
这是一个用户提出需求类型的issue，主要涉及到为vllm添加控制向量支持的功能。原因是用户希望通过添加控制向量来改变模型行为而不需要额外提示，以及可以通过设置不同的大小来改变模型行为的级别。

https://github.com/vllm-project/vllm/issues/12869
这是一个bug报告类型的issue，主要涉及到在描述含有中文时会出现bug的情况。

https://github.com/vllm-project/vllm/issues/12868
这个issue类型是技术改进，涉及到多模态处理。原因可能是之前存在不必要的解标记化操作，导致额外的解码调用。

https://github.com/vllm-project/vllm/issues/12867
该问题是一个bug报告，涉及到在使用本地模型时出现错误。由于缺少`repo_type`参数，导致无法正常运行。

https://github.com/vllm-project/vllm/issues/12866
这是一个bug报告，涉及的主要对象是c2x/c3x初始化器问题。由于某些容器内部存在空初始化器警告/错误导致的问题。

https://github.com/vllm-project/vllm/issues/12865
这是一个bug报告，该问题涉及到使用vllm0.7.2运行deepseek-32B时出现了与CUDA相关的错误。

https://github.com/vllm-project/vllm/issues/12864
这是一个Bug报告，涉及的主要对象是vllm项目中的Extra fields。该问题由于输入了不允许的额外参数导致了错误提示消息中提到的"Extra inputs are not permitted"的问题。

https://github.com/vllm-project/vllm/issues/12863
这是一个bug报告，涉及的主要对象是cumem_allocator中CUDA链接目标。由于在CPU环境中，构建系统无法正确查找`lcuda`，导致在CPU构建环境下使用`list(APPEND CUMEM_LIBS cuda)`时`ld`无法找到位置。

https://github.com/vllm-project/vllm/issues/12862
这是一个bug报告类型的issue，主要涉及在CPU容器中使用CUDA库构建vllm wheel时出现的链接错误问题。可能是由于构建系统在CPU容器中无法正确找到lcuda导致的。

https://github.com/vllm-project/vllm/issues/12861
这是一个bug报告，该问题涉及的主要对象是vLLM工具。由于环境配置问题，导致GPU相关库无法成功注册工厂，最终出现了无法识别平台以及相关的错误提示。

https://github.com/vllm-project/vllm/issues/12860
这个issue类型是bug报告，主要涉及的对象是在加载DeepSeek-R1-Distill-Llama-70B模型时遇到的问题，可能是由于环境设置不正确导致无法加载模型。

https://github.com/vllm-project/vllm/issues/12859
这是一个Bug修复类型的Issue，涉及到mistralcommon要求AssistantMessage内容为字符串，在使用mistral tokenizer时会导致OpenAI多轮聊天消息触发错误，解决方法是将消息块列表转换为单个字符串。

https://github.com/vllm-project/vllm/issues/12858
这是一个bug报告类型的issue，涉及主要对象是vllm项目。由于缺少'resource'模块导致了此报错。

https://github.com/vllm-project/vllm/issues/12857
这个issue类型是Bug报告，涉及对象是Entrypoints测试，由于PR #12833的合并导致token数量不匹配。

https://github.com/vllm-project/vllm/issues/12856
这是一个bug报告，关于VLLM中出现的"Spurious warning on dropped args"问题，用户表示传入的参数生效但仍会出现警告。

https://github.com/vllm-project/vllm/issues/12855
这是一个bug报告，主要涉及到vllm 0.7.1版本中BNB模型无法正常工作的问题。可能是由于更新到0.7.1版本后导致MLA不支持bitsandbytes量化，尽管设置了VLLM_MLA_DISABLE环境变量为true，最终导致出现了`got an unexpected keyword argument 'kv_data_type'`错误。

https://github.com/vllm-project/vllm/issues/12854
这个issue类型是功能需求，涉及主要对象为vllm项目中的报告功能。由于需要实现在完成响应头中报告指标，因此用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/12853
这是一个功能改进类型的issue，涉及主要对象为`EngineCoreOutputs`。由于过多创建小对象在调度器中，导致需要将`EngineCoreOutput`字段直接折叠到`EngineCoreOutputs`中。

https://github.com/vllm-project/vllm/issues/12852
这是一个bug报告，涉及vllm库使用过程中出现的错误。用户在进行推断时遇到了一个关于tie_weights设置的错误。

https://github.com/vllm-project/vllm/issues/12851
这是一个Bug报告类型的Issue，主要涉及Neuron Dockerfile的构建失败问题，可能是由于transformers版本冲突导致的。

https://github.com/vllm-project/vllm/issues/12850
这是一个优化问题，涉及主要对象为`moe_align_block_size`。这个问题由于需要优化深度搜索V3时的对齐块大小，原因可能是当前的对齐块大小在评估中表现良好但仍有优化空间。

https://github.com/vllm-project/vllm/issues/12848
这个issue是一个bug报告，主要涉及flashattention repo中的PR，解决了关于FA2 illegal memory access的问题。

https://github.com/vllm-project/vllm/issues/12847
这是一个Bug报告，涉及到vllm工具，在使用主分支commit `467a96a5415dc896170cecc0bb83d9c49c2f3c5e`时出现`RuntimeError: Failed to infer device type`错误。

https://github.com/vllm-project/vllm/issues/12846
这个issue类型是用户提出需求，该问题单涉及的主要对象是embedding models。由于需要离线/在线评分功能，用户提出了启用/embedding模型的/score端点的需求。

https://github.com/vllm-project/vllm/issues/12845
这个issue类型是bug报告，主要涉及了vllm中benchmark结果的解释问题，由于计算方式不正确导致了症状表现出现异常。

https://github.com/vllm-project/vllm/issues/12844
这是一个关于性能测试的issue，主要涉及的对象是单个MI300 8x GPU节点，用户希望进行CI性能测试。

https://github.com/vllm-project/vllm/issues/12843
这是一个bug报告，涉及主要对象为HQQ quantization，由于维度不匹配导致了断言错误。

https://github.com/vllm-project/vllm/issues/12842
这是一个需求提出的issue，主要涉及添加pythonjsonlogger到requirements-common，带来了logging的简化使用以及容器大小影响的问题。

https://github.com/vllm-project/vllm/issues/12841
这是一个Bug报告，涉及VLLM的Speculative decoding在使用分布式推理中报告错误的问题。可能是由于使用了Speculative decoding参数导致的错误。

https://github.com/vllm-project/vllm/issues/12840
这是一个关于文档的问题，类型为文档错误，主要对象是构建脚本。由于没有具体内容，可能是由于缺少相关信息或者格式错误导致的。

https://github.com/vllm-project/vllm/issues/12839
这是一个用户提出需求的issue，主要对象是vLLM工具功能的使用和OpenAI模型的集成。用户希望了解如何通过payload中的`tools`和`tool_choice`参数来使用不同的OpenAI兼容模型生成响应。

https://github.com/vllm-project/vllm/issues/12838
这是一个bug报告，涉及到ROCM下的MOE（Mixture of Experts）模块，该问题是由于搜索空间修剪导致M值错误，使用topk修复。

https://github.com/vllm-project/vllm/issues/12837
这是一个bug报告，涉及到减少对Huggingface Hub的不必要请求的问题。应该避免重复请求以确认文件是否存在。

https://github.com/vllm-project/vllm/issues/12836
这是一个Bug报告，主要涉及的对象是deepseek的embedding layer，由于缺少quant_config导致问题。

https://github.com/vllm-project/vllm/issues/12835
这个issue是一个Bug报告，主要涉及DeepSeek MLA在v0.7.1版本中性能较低。原因是当前环境信息显示PyTorch版本为2.5.1+cu124，而实际情况中CUDA runtime版本为12.6.85，存在版本不匹配导致性能下降的问题。

https://github.com/vllm-project/vllm/issues/12834
这是一个用户提出需求的issue，主要对象是vLLM工具的`tool_choice`参数。由于当前请求中`enableautotoolchoice`参数需要必须带有`tool_choice`参数，导致开发者体验不佳，用户提出让`tool_choice`参数在`enableautotoolchoice`情况下成为可选项并默认为"auto"的建议。

https://github.com/vllm-project/vllm/issues/12833
这个issue类型为bug报告，涉及的主要对象是代码中的decode调用，由于存在不必要的decode调用导致问题。

https://github.com/vllm-project/vllm/issues/12832
这是一个功能需求的issue，主要涉及的对象是`TransformersModel`，用户提出了添加pipeline parallel支持的需求。

https://github.com/vllm-project/vllm/issues/12831
This is a bug report. The main object involved is the weight loading process for the EAGLE model in the vLLM environment. The issue appears to be related to inconsistencies in handling bias parameters during the weight conversion and loading process.

https://github.com/vllm-project/vllm/issues/12830
这个issue属于功能需求类型，涉及的主要对象是将IBM/NASA Prithvi Geospatial模型集成到vLLM中。这个需求是为了实现非语言模型与vLLM的集成。

https://github.com/vllm-project/vllm/issues/12829
这个issue属于用户提出需求，主要涉及的对象是vLLM，由于vLLM目前不支持多Lora分类模型，用户想了解是否有计划在近期内添加支持，并寻求如何实现这个功能的指导。

https://github.com/vllm-project/vllm/issues/12828
这是一个Bug报告，涉及的主要对象是VLLM项目中与Turing GPU相关的问题，由于缺少`AssertionError`异常信息导致Xformers回滚功能出现问题。

https://github.com/vllm-project/vllm/issues/12827
这是一个Bug报告，用户在运行deepseek v3时出现了Runtime error，产生了nccl error 1的异常。

https://github.com/vllm-project/vllm/issues/12826
这是一个bug报告类型的issue，主要涉及Mamba models在使用v1引擎进行profile运行时出现错误。

https://github.com/vllm-project/vllm/issues/12825
这是一个用户需求类型的issue，主要对象是对Qwen2.5-VL-7B Instruct功能的支持。该需求可能是由于用户需要这一功能或者应用场景的需求而提出。

https://github.com/vllm-project/vllm/issues/12824
这是一个Bug报告，主要涉及vllm与verl集成的一些问题。由于这种集成问题，导致了一些bug需要修复。

https://github.com/vllm-project/vllm/issues/12823
这是一个Bug报告，主要涉及的对象是在检查模型架构'MiniCPMO'时出现了错误。错误表现为在环境中使用docker CPU镜像时，检测到平台为cpu，但出现了相应的错误信息。

https://github.com/vllm-project/vllm/issues/12821
这是一个bug报告，涉及到vllm库中的'CUDAGraphBatchDecodeWithPagedKVCacheWrapper'对象缺少'plan'属性并导致问题。

https://github.com/vllm-project/vllm/issues/12820
这是一个bug报告，涉及的主要对象是vLLM API server，导致报错的原因可能是参数配置问题。

https://github.com/vllm-project/vllm/issues/12819
这是一个 bug 报告类型的 issue，主要涉及 vLLM 在 PowerPC 上设置 `tensor-parallel-size` 大于 1 时出现失败的情况。由于设置 `tensor-parallel-size` 为 2 时，在模型推理时出现失败导致无法完成请求。

https://github.com/vllm-project/vllm/issues/12818
这是一个bug报告类型的issue，涉及的主要对象是Dockerfile。由于更新了base链接未正确设置，导致出现了错误的软件依赖问题。

https://github.com/vllm-project/vllm/issues/12817
这是一个用户询问问题的issue，主要涉及的对象是如何使用vllm进行推理特定模型。此问题可能是由于用户不知道如何集成vllm导致的。

https://github.com/vllm-project/vllm/issues/12816
这是一个功能需求类型的issue，主要涉及的对象是vllm框架中的Worker相关类。

https://github.com/vllm-project/vllm/issues/12815
这是一个bug报告，主要涉及v1 engine + flashinfer sampling的使用情况。由于某种原因导致GPU和CPU利用率均保持在100%，出现了死循环的症状。

https://github.com/vllm-project/vllm/issues/12814
该issue属于用户提出需求类型，涉及主要对象为文档内容。

https://github.com/vllm-project/vllm/issues/12813
这是一个Bug报告，涉及的主要对象是MiniCPMOProcessor类。由于MiniCPMOProcessor对象没有get_audio_placeholder属性，导致出现AttributeError: 'MiniCPMOProcessor' object has no attribute 'get_audio_placeholder'的错误。

https://github.com/vllm-project/vllm/issues/12812
这个issue类型是功能改进请求，主要对象是Intel Gaudi加速器，用户希望使其支持长上下文和LoRA功能。

https://github.com/vllm-project/vllm/issues/12811
这是一个关于Bug报告的问题单，主要涉及的对象是VLLM中的Pooling模型，由于LoRA不支持该模型导致embedding模型无法正常工作。

https://github.com/vllm-project/vllm/issues/12810
这是一个bug报告，该问题涉及V1版本的LLM模型在使用过程中出现了无法自动检测平台cuda的问题。

https://github.com/vllm-project/vllm/issues/12809
这是一个bug报告，涉及的主要对象是pynvml库。由于错误的pynvml版本导致运行时错误被抑制，需要改进错误消息以帮助用户解决问题。

https://github.com/vllm-project/vllm/issues/12808
这是一个Bug报告，主要涉及Embedding model API with Lora weights未能按预期工作，可能是由于Lora weights未正确加载导致相似度分数和嵌入的差异。

https://github.com/vllm-project/vllm/issues/12807
这是一个需求类型的issue，主要涉及MLA在Hopper上使用FA3的需求。造成这个issue的原因可能是之前没有明确使用FA3导致MLA prefill出错。

https://github.com/vllm-project/vllm/issues/12806
这个issue是关于Bug报告类型，主要涉及vllm项目中的test_ultravox.py文件，由于https://github.com/vllmproject/vllm/pull/11253/引起的CI构建失败。

https://github.com/vllm-project/vllm/issues/12805
这个issue类型是测试需求，主要对象是CI系统。由于新增的pre-commit hook可能无法触发CI检查，导致用户无法确认CI是否正常工作。

https://github.com/vllm-project/vllm/issues/12804
这个issue属于bug报告类型，主要涉及文件名中存在空格导致在之前的PR中遗漏了部分文件，需要在预提交检查中添加检测空格的功能。

https://github.com/vllm-project/vllm/issues/12803
这是一个Bug报告，涉及到 VLLM 中的 cline 请求出现了错误。原因是引擎后台任务失败，导致任务执行出错。

https://github.com/vllm-project/vllm/issues/12802
这个issue是一个Bugfix类型的问题，主要涉及到VLLM项目中与零温度相关的非确定性问题。 Bug可能由于在计算logits本身时的不一致导致了不稳定的结果。

https://github.com/vllm-project/vllm/issues/12801
这个issue属于Bug报告类型，主要涉及vllm在处理大于8k输入时解码速度降至1.2tok/s的问题。原因可能是算法性能不足或者程序设计上的问题。

https://github.com/vllm-project/vllm/issues/12800
这是一个bug报告，涉及的主要对象是Triton MLA attention backend，由于某个block没有被复制而导致的问题。

https://github.com/vllm-project/vllm/issues/12799
这是一个bug报告，主要涉及文件名中存在空格导致某些代码库无法很好地支持该问题。

https://github.com/vllm-project/vllm/issues/12798
这是一个Bug报告，主要涉及DeepSeekVL2服务的Engine进程在serving时意外终止的问题。

https://github.com/vllm-project/vllm/issues/12797
这是一个bug报告，主要涉及对象是HuggingFace库中的函数`_get_encoder_config`和`get_sentence_transformer_tokenizer_config`。由于每次调用`_get_encoder_config`都会触发不必要的文件检查操作，导致了连接超时或者CI中的问题。

https://github.com/vllm-project/vllm/issues/12796
这是一个bug报告，涉及的主要对象是代码中的blockwise CUTLASS kernels，导致bug的原因是在L40s上调用这些kernels。

https://github.com/vllm-project/vllm/issues/12795
这是一个bug报告，主要涉及vllm项目中使用0 tokens per second in CPU mode导致的endless loop。

https://github.com/vllm-project/vllm/issues/12794
这是一个bug报告，主要涉及vllm中的float16转换为bfloat16在cpu模式下，导致tensorflow在gptq模型加载失败。

https://github.com/vllm-project/vllm/issues/12793
这是一个建议类型的issue，主要涉及到移除DeepSeek V3模型定义的重复内容。原因是DeepSeekV3可被简单地表示在DeepSeekV2模型定义中，因为某些配置已经存在于DeepSeekV2配置中。

https://github.com/vllm-project/vllm/issues/12792
这是一个bug报告，涉及到修复TPU profiling示例的问题，由于profiling服务器无法保持足够长的时间而导致了bug。

https://github.com/vllm-project/vllm/issues/12791
这是一个bug报告，主要涉及vLLM中的Reward model usage功能，因为不支持Sequential classifier type RM导致数值错误。

https://github.com/vllm-project/vllm/issues/12790
这是一个用户需求类型的issue，主要涉及在AMD系统上添加对V1的初始支持，由于使用了`vllm/attention/ops/prefix_prefill.py`内核而不是flashattn，可能导致需要更新的安装步骤和示例命令的提出。

https://github.com/vllm-project/vllm/issues/12789
这是一个bug报告，主要涉及到在ROCm上使用了错误的warp_size数值导致一半数组没有被初始化。

https://github.com/vllm-project/vllm/issues/12788
这是一个Bug报告，涉及的主要对象是chat_utils.py文件。由于`tool_calls`元素的取值为`None`时会导致内部服务器错误，因为`list(None)`是无效的，用户报告了这个问题。

https://github.com/vllm-project/vllm/issues/12787
这个issue类型是用户提出需求。该问题单涉及的主要对象是核心功能的pipeline parallel。由于需要使用线程同时执行多个任务，用户希望实现pipeline并行化。

https://github.com/vllm-project/vllm/issues/12786
这是一个bug报告，主要涉及vLLM的OpenVINO版本在升级到^0.7.0后出现错误。

https://github.com/vllm-project/vllm/issues/12785
这是一个建议改进类型的issue，涉及主要对象为`TransformersModel`。由于`Linear` layer 无法被tensor并行化，用户体验较差，希望改为警告而非错误信息。

https://github.com/vllm-project/vllm/issues/12784
该issue类型为功能需求提出，主要对象是支持nvfp4量化，分离fp4量化和fp4 gemm为两个PR。

https://github.com/vllm-project/vllm/issues/12783
这是一个bug报告，主要涉及使用vllm时出现的错误。问题可能是由于缺少'_orig_mod'模块或参数而导致。

https://github.com/vllm-project/vllm/issues/12782
这是一个bug报告，涉及的主要对象是vllm与verl的集成。由于两处代码需要修改，才能使vllm与verl（fsdp作为后端）无缝集成。

https://github.com/vllm-project/vllm/issues/12781
这个issue类型是Bug报告，涉及的主要对象是VLM（Visual Language Modeling）模型。原因是当前transformers版本更新导致E5V处理器中的`patch_size`需要设置，同时PixtralHF处理器需要更新以保持与huggingface/transformers变化的兼容性。

https://github.com/vllm-project/vllm/issues/12780
这是一个文档问题（Docs），涉及到更新github链接问题，需要移除旧的自定义HTML页面链接。

https://github.com/vllm-project/vllm/issues/12779
这个issue属于需求提出类型，主要涉及的对象是硬件中的Intel Gaudi以及HPU，可能是用户提出了关于多步调度实现的需求或问题。

https://github.com/vllm-project/vllm/issues/12778
这是一个Bug报告，涉及的主要对象是vllm在CPU构建过程中由于intel_extension_for_pytorch导致使用float16或float32时崩溃，只有bfloat16正常工作，从而导致性能极差的问题。

https://github.com/vllm-project/vllm/issues/12777
这是一个bug报告类型的issue，主要涉及到"Kernel"模块下的rotary_embedding ops，用户提出了让这些ops更灵活以处理特定输入形状的需求。

https://github.com/vllm-project/vllm/issues/12776
这是一个性能优化类的issue，主要对象是`TransformersModel`模块，用户提议使用vLLM的`RMSNorm`类来改进性能并改进用户体验。

https://github.com/vllm-project/vllm/issues/12775
这个issue类型是用户提出需求，询问问题，主要涉及vllm的使用。由于用户不知道如何与vllm集成以运行特定模型的推理，导致提出了这个问题。

https://github.com/vllm-project/vllm/issues/12774
这是一个功能需求类型的 issue， 主要涉及的对象是 vllm endpoint， 用户提出了希望在 vllm endpoint 中暴露从磁盘加载新模型权重的选项。

https://github.com/vllm-project/vllm/issues/12773
这是一个Bug报告，涉及vLLM中Deepseek R1在MI300A上出现内存访问故障的问题。可能是由于模型加载后失败，导致无法正常运行。

https://github.com/vllm-project/vllm/issues/12772
这是一个用户提出需求的issue，主要对象是想要访问`vllm.v1.core.scheduler.Scheduler`，由于`Scheduler`是`EngineCore`的成员且在后台进程中运行，用户无法直接访问该对象。

https://github.com/vllm-project/vllm/issues/12771
这是一个Bug报告类型的Issue，涉及vllm在升级V1版本后对于多卡部署模型存在兼容性问题导致的报错。

https://github.com/vllm-project/vllm/issues/12770
该issue属于Bug报告类型，主要涉及的对象是在NVIDIA A40上使用vllm 0.6.4图像部署了qwen2.5-instruct-gptq-int4模型，启用了带有8个解码步骤的多步解码功能，并观察到模型输出中的重复内容。导致这种bug症状的原因可能是多步解码特征的开启引发了输出中的重复内容。

https://github.com/vllm-project/vllm/issues/12769
这是一个Bug报告，涉及的主要对象是收到请求时发生崩溃。原因可能是当前环境中的某些配置或参数不匹配导致程序崩溃。

https://github.com/vllm-project/vllm/issues/12768
该issue类型为用户提出需求，该问题单涉及的主要对象为vllm的文档。由于用户想要了解vllm是否支持qwen1的audio大模型，希望寻求潜在的替代方案或修复方法。

https://github.com/vllm-project/vllm/issues/12767
这个issue是关于功能改进的，主要涉及VLM（Visual Language Modeling）模型使用共享字段传递令牌ID，由于需要改进字段元素的构建方式和增加文档说明，以及为每种字段类型添加示例。

https://github.com/vllm-project/vllm/issues/12766
这是一个用户询问如何在多机环境中部署vllm的问题，涉及主要对象是vllm的多机部署。

https://github.com/vllm-project/vllm/issues/12765
这个issue是关于用户需求的问题，主要涉及的对象是vllm多机部署。由于用户当前环境的CUDA版本与PyTorch版本不匹配，导致可能无法成功进行多机部署。

https://github.com/vllm-project/vllm/issues/12764
这是一个用户询问如何在多机环境下部署vllm的问题，主要对象是vllm多机部署。由于环境信息中展示了PyTorch版本与CUDA信息，用户可能遇到了在多机部署时的问题或困惑。

https://github.com/vllm-project/vllm/issues/12763
该issue是用户提出的需求类型问题，主要涉及使用vllm进行多机部署。由于环境信息中显示CUDA版本与cuDNN版本不匹配，可能导致无法成功进行多机部署。

https://github.com/vllm-project/vllm/issues/12762
这个issue类型是用户请教问题，主要涉及对象是如何在多机环境下部署vllm。由于环境信息显示使用的PyTorch版本不兼容，可能导致出现问题或无法成功完成多机部署。

https://github.com/vllm-project/vllm/issues/12761
该issue属于一个功能请求（RFC），主要涉及支持V1架构中使用交叉注意力的多模型模型。由于需要支持跨注意力的多模型，导致需要对代码中的多个部分进行修改。

https://github.com/vllm-project/vllm/issues/12760
这是一个用户提出需求的issue，主要涉及到`FinishReason` enum的短小和使用常量字符串。

https://github.com/vllm-project/vllm/issues/12759
这是一个bug报告，主要涉及的对象是vllm项目中的numpy版本限制问题。由于限制条件`numpy < 2.0.0`导致的bug，需要运行最新numpy时引发了此问题。

https://github.com/vllm-project/vllm/issues/12758
该issue类型为用户提出需求，询问如何在vllm中集成deepseek-r1-4bit模型，可能是因为用户想要运行特定模型的推断，但不清楚如何在vllm中实现。

https://github.com/vllm-project/vllm/issues/12757
这个issue类型是功能更新，涉及的主要对象是代码中的w2 weight scales，在更新中添加了条件以根据act order对其进行分区。

https://github.com/vllm-project/vllm/issues/12756
这是一个bug报告，涉及的主要对象是在使用vLLM v0.7.1时出现CUDA内存溢出错误。原因可能是模型在serving时占用了过多的显存导致溢出。

https://github.com/vllm-project/vllm/issues/12755
该issue属于功能需求，主要涉及DeepSeek MTP的实现和性能优化。原因是为了支持DeepSeek MTP层进行下一个n的预测。

https://github.com/vllm-project/vllm/issues/12754
这是一个Bug报告类型的Issue，主要涉及的对象是vllm v1，可能是由于vllm v1在tot下异常导致无法正常运行。

https://github.com/vllm-project/vllm/issues/12753
这是一个bug报告，涉及vLLM中的Pooling request失败问题。该问题可能是由于在CPU上无法成功运行`task classify`导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/12752
这是一个用户提出需求的issue，主要涉及的对象是vLLM项目的前端，涉及的内容是添加"User Defined Custom Tool Calling"解析器。

https://github.com/vllm-project/vllm/issues/12751
这是一个Bug报告，涉及到MLP spec decoding在高到达速率下启用`--speculative-disable-by-batch-size`时出现OOM和运行缓慢的问题，可能是由于每次将新返回的hidden states追加到现有hidden states中导致`self.previous_hidden_states`不断增加而引起。

https://github.com/vllm-project/vllm/issues/12750
这个issue是关于bug报告，涉及到OpenVINO模型运行器。由于无法在OpenVINO中为Qwen2.5提供服务以及无法使用OpenVINO后端运行vLLM服务器，导致了这些bug。

https://github.com/vllm-project/vllm/issues/12748
这是一个文档更新类型的issue，主要涉及开发者对贡献内容进行审核时需要引导到Slack交流，原因可能是需要更多人关注和审阅。

https://github.com/vllm-project/vllm/issues/12747
这是一个bug报告，涉及vllm中使用Ray和aDAG时出现的编译图执行次数限制错误。

https://github.com/vllm-project/vllm/issues/12745
这个issue属于需求提出类型，涉及的主要对象是添加metrics支持，用户提出了关于v1设计文档的需求。

https://github.com/vllm-project/vllm/issues/12744
这是一个bug报告，涉及Mistral模型与guided decoding使用json schema生成无限空格的问题。由于xgrammar的更改引入了此bug，建议通过添加一个环境变量来禁用空格以解决这一问题。

https://github.com/vllm-project/vllm/issues/12743
这是一个文档修改类型的issue，涉及的主要对象是"auto_awq.md"文件。

https://github.com/vllm-project/vllm/issues/12742
这是一个bug报告，主要涉及vllm在运行Qwen2-VL-7B时出现了Empty mm_placeholders的错误。

https://github.com/vllm-project/vllm/issues/12741
这是一个关于Bug报告的issue，主要涉及V1 Engine输出不一致的问题。导致这个问题的原因可能是V1 Engine在使用时产生了不一致的输出。

https://github.com/vllm-project/vllm/issues/12740
这是一个 bug 报告，主要涉及的对象是 vllm 中的 chat_template_kwargs 参数。由于 vLLM 在处理缺失图片占位符时会自动添加额外的占位符，导致用户无法在自定义的 chat_template 模板中正确插入额外的图片，引发了这个问题。

https://github.com/vllm-project/vllm/issues/12739
这个issue属于优化建议类型，主要涉及构建依赖项的管理，以避免不必要的依赖。

https://github.com/vllm-project/vllm/issues/12738
这是一个用户提出需求的issue，主要对象为VLLM的音频转录功能。用户询问为什么只能转录音频的前部分，是否正常或者是否有其他替代方案。

https://github.com/vllm-project/vllm/issues/12737
这是一个功能需求的issue，主要涉及的对象是`offline_inference`中的多个示例脚本。由于这些示例脚本在逻辑和功能上几乎相同，为了减少混乱，优化体验，增加可配置性和提供更好的使用指引，需要将它们合并为单个`basic`示例。

https://github.com/vllm-project/vllm/issues/12736
这是一个更新请求，涉及的主要对象是版本控制。

https://github.com/vllm-project/vllm/issues/12735
这个issue属于bug报告类型，主要涉及的对象是Containerfile.arm，由于缺少SELinux relabel导致无法用podman构建arm容器镜像。

https://github.com/vllm-project/vllm/issues/12734
这是一个bug报告类型的issue，涉及主要对象为在使用vllm时无法通过podman构建arm容器映像。这个问题的根本原因是需要在每个bind mount中添加`relabel=private`以确保与SELinux的兼容性。

https://github.com/vllm-project/vllm/issues/12733
这是一个用户提出需求的类型issue，主要涉及VLLM Server的功能定制问题，用户想要实现在使用VLLM服务器时应用自定义评分脚本的需求。

https://github.com/vllm-project/vllm/issues/12732
这个issue是一个用户提出的需求类型的问题，主要涉及vLLM workers的精细化控制与Ray actor共享GPU的功能需求。

https://github.com/vllm-project/vllm/issues/12730
这是一个用户提出需求的类型，主要涉及DeepSeek项目支持多标记预测的计划，用户希望通过推测性解码实现。

https://github.com/vllm-project/vllm/issues/12729
这个issue属于需求提出类型，提及需要将MLA与纯RoPE支持添加到Deepseek-VL2模型中。

https://github.com/vllm-project/vllm/issues/12728
这个issue是一个bug报告，主要涉及到了InternVL和Mantis模型。这个问题是由于过时的processor kwargs测试和新版本Transformers的变化导致的。

https://github.com/vllm-project/vllm/issues/12727
这个issue是关于代码重构和类型注解的改进，不属于bug报告类型，主要涉及到`TransformersModel`中`Linear`处理的优化。原因可能是为了代码可读性和维护性的考虑。

https://github.com/vllm-project/vllm/issues/12726
这个issue类型是功能需求提出，主要涉及的对象是vLLM Server。由于需要更多指标，用户提出了更新和改进的版本供讨论。

https://github.com/vllm-project/vllm/issues/12725
这是一个Bug报告，涉及的主要对象是在使用tensorizer和draft model进行模型加载时出现的问题。原因是`vllm`错误地重用了通过`model_loader_extra_config`字段提供的`TensorizerConfig`，导致加载模型时出错。

https://github.com/vllm-project/vllm/issues/12724
这是一个feature请求，主要涉及V1引擎不支持2080ti GPU的FA版本，用户请求V1引擎支持Xformers。

https://github.com/vllm-project/vllm/issues/12723
这是一个Bug报告，主要涉及"prefill"和"decode"之间的通信问题，由于kv未准备好导致了"prefill"缓冲积累并阻塞其他"prefill"继续。

https://github.com/vllm-project/vllm/issues/12722
这是一个针对代码优化的issue，主要涉及多模态输入数据在特定情况下的复制操作优化。

https://github.com/vllm-project/vllm/issues/12721
这个issue类型是升级请求，涉及的主要对象是vllm项目下的torch库。由于升级中CPU被IPEX阻塞，导致需要单独升级其他硬件后端。

https://github.com/vllm-project/vllm/issues/12720
这是一个功能需求或者功能开发的issue，涉及vllm的核心功能中关于pipeline parallel async execution loop的开发。

https://github.com/vllm-project/vllm/issues/12719
这是一个关于安装问题的bug报告，主要涉及vllm对PyTorch版本的约束。用户希望放宽约束以支持PyTorch 2.6。

https://github.com/vllm-project/vllm/issues/12718
这个issue属于用户提出需求，主要是添加对Phi3模型中部分旋转嵌入的支持。由于需要使用部分旋转嵌入，在Llama模型中添加了相应的更新，保证了现有配置的向后兼容性。

https://github.com/vllm-project/vllm/issues/12717
这个issue类型是配置相关问题，主要涉及到GH200机器，用户在设置Quantization和MoE时遇到了问题。

https://github.com/vllm-project/vllm/issues/12716
这是一个环境信息收集的问题，该问题单涉及的主要对象是加载MoE（如Mixtral）模型的专家，原因是当前环境信息无法准确收集导致信息不完整。

https://github.com/vllm-project/vllm/issues/12715
该问题属于用户提出需求类型，主要涉及如何在`task=classify`中获取类标签的问题，用户想知道在使用`LLM.classify`时如何获得类标签或者是否需要自行维护一个基于模型的`class_labels`列表。

https://github.com/vllm-project/vllm/issues/12714
这是一个功能需求的issue，主要涉及的对象是DeepSeek MoE模型在CPU上的支持。由于Torch未编译启用CUDA，导致用户无法在CPU模式下运行DeepSeekV2LiteInstruct模型。

https://github.com/vllm-project/vllm/issues/12713
这是一个性能优化类型的issue，涉及到ROCM、AMD、TRITON以及vllm项目。由于减少warps数量用于预加载以减少溢出导致的性能提升，同时产生了一个较小的问题：内核时间降低了对总执行时间的影响。

https://github.com/vllm-project/vllm/issues/12712
这个issue是一个bug报告，涉及到vLLM在进行throughput benchmark时无法跳过分词支持的问题，可能是由于回合更新时导致的误操作所致。

https://github.com/vllm-project/vllm/issues/12711
这个issue属于bug报告类型，涉及到的主要对象是Quant化功能的使用。由于`use_mla`存在类型错误，导致带有量化但没有`config_groups`的模型加载失败。

https://github.com/vllm-project/vllm/issues/12710
这是一个bug报告，主要涉及支持加载HF版本的PixtralLarge模型所需的配置更新。

https://github.com/vllm-project/vllm/issues/12709
这是一个文档更新类的issue，涉及的主要对象是将`ibmfms`改为`ibmaiplatform`。这个问题由于命名变更可能导致旧名称解析的持续时间而引起相关测试失败的情况。

https://github.com/vllm-project/vllm/issues/12707
这是一个用户提出需求的issue，主要涉及的对象是添加Triton配置以支持DeepSeekV3在B200设备上的使用。

https://github.com/vllm-project/vllm/issues/12706
这是一个清理未使用的Docker镜像和卷的性能基准测试问题，类型为性能优化。

https://github.com/vllm-project/vllm/issues/12705
这是一个需求提出的RFC类型的issue，涉及的主要对象是API server的性能优化。由于API server目前在单个进程中运行，利用一个CPU进行处理，在GPU变得更快的情况下，需要将API server扩展到多个CPU以确保处理请求速度足够快，以保持GPU资源的充分利用。

https://github.com/vllm-project/vllm/issues/12704
这是一个bug报告，主要涉及到MLA模型加载中的警告问题，由于条件顺序问题导致警告未被正确处理。

https://github.com/vllm-project/vllm/issues/12703
这个issue类型是bug报告，涉及的主要对象是缺少许可证头文件。原因可能是合并错误导致缺失许可证头文件。

https://github.com/vllm-project/vllm/issues/12702
这是一个关于添加初始Blackwell支持的issue，主要涉及到CUTLASS库的提交问题。

https://github.com/vllm-project/vllm/issues/12701
这是一个功能改进类型的issue，主要涉及 P3L.py & P3L_mling.py 的测试批量查询功能改进。原因是为了使测试过程更加高效且减少测量噪声。

https://github.com/vllm-project/vllm/issues/12700
这个issue类型属于发布跟踪，涉及主要对象是vllm项目。由于一系列PR尚未合并，导致需要新的发布以解决相关问题。

https://github.com/vllm-project/vllm/issues/12699
这是一个Bug报告类型的Issue，主要涉及vllm项目中qwen2vl在transformers和vllm环境下运行时得到了显著不同的logprobs。可能原因是环境配置或代码实现存在问题，导致了不一致的结果。

https://github.com/vllm-project/vllm/issues/12698
这是一个bug报告，主要涉及的对象是vllm的Build工具中的AMD功能。导致问题的原因是hipify脚本中哈希符号的问题。

https://github.com/vllm-project/vllm/issues/12697
这是一个Bug报告类型的Issue，主要涉及的对象是vLLM在使用Qwen2.5VL7BInstruct模型时出现的Transformers兼容性问题，导致无法识别模型类型qwen2_5_vl，使得vLLM无法启动。

https://github.com/vllm-project/vllm/issues/12696
这个issue类型是bug报告，主要涉及的对象是Kernel模块，由于传入的线性层在某些情况下会传递转置比例，导致了选择问题，出现了运行错误。

https://github.com/vllm-project/vllm/issues/12695
这是一个用户提出需求类型的issue，主要涉及的对象是AMD GPUs。由于需要支持完全透明的睡眠模式，用户提出了关于API，清除GPU内存中所有KV缓存的问题。

https://github.com/vllm-project/vllm/issues/12694
这是一个bug报告，主要涉及的对象是脚本文件的 SPDX 头部处理逻辑。由于脚本文件以 '!' 开头，导致 SPDX 头部未被正确放置，需要调整逻辑以解决这一问题。

https://github.com/vllm-project/vllm/issues/12693
该issue类型为用户提出需求，主要对象是关于在vllm中使用torch.compile优化推理性能的问题。由于用户对于Qwen2VL模型的torch.compile支持情况不清楚，导致提出了相关问题并寻求帮助。

https://github.com/vllm-project/vllm/issues/12692
这是一个Bug报告，涉及的主要对象是V1引擎。由于V1引擎忽略了引导的JSON，导致用户无法得到正确的响应。

https://github.com/vllm-project/vllm/issues/12691
这个issue类型是bug报告，主要涉及的对象是输入缩放比例。导致这个问题的原因是需要在输入缩放比例中添加一个mul操作，缩放因子为448/240。

https://github.com/vllm-project/vllm/issues/12690
这是一个bug报告，涉及V1与Triton Inference Server Backend之间的兼容性问题，导致模型无法在启动时正常运行。

https://github.com/vllm-project/vllm/issues/12689
这个issue是关于bug修复的，涉及到模型加载的问题，由于加载的细节问题导致报错。

https://github.com/vllm-project/vllm/issues/12688
这是一个用户提出需求的issue，主要涉及的对象是V1版本的代码。由于缺乏`SamplingParams.logits_processors`支持，用户希望添加这一功能来完善V1版本。

https://github.com/vllm-project/vllm/issues/12687
这是一个bug报告，涉及vLLM服务器无法使用OpenVINO后端运行的问题。原因可能是环境配置或代码逻辑问题。

https://github.com/vllm-project/vllm/issues/12686
该issue类型为功能需求，主要对象是支持在DeepseekV2模型中添加密集MLP和RoPE功能。可能出现此功能需求是为了与Huggingface建模代码保持兼容。

https://github.com/vllm-project/vllm/issues/12685
这个issue类型是用户提出需求，该问题单涉及的主要对象是为vllm项目添加Helm chart release workflow。

https://github.com/vllm-project/vllm/issues/12684
这个issue类型是需求提议，主要对象是tokenize api。原因是用户希望在tokenize api中添加工具定义。

https://github.com/vllm-project/vllm/issues/12683
这是一个Bug报告，涉及的主要对象是vLLM模型输出与DeepSeek API输出不匹配，导致应用层处理复杂性。

https://github.com/vllm-project/vllm/issues/12682
这是一个bug报告，涉及主要对象是加载mistral small 2501模型时出现了Assertion error，可能是由于参数形状不匹配导致的。

https://github.com/vllm-project/vllm/issues/12681
这个issue是关于bug报告，主要涉及CI（持续集成）构建系统。可能是由于测试中出现的错误（gh200），需要修复。

https://github.com/vllm-project/vllm/issues/12680
这个issue是关于bug报告，涉及的主要对象是使用v0.7.1版本的vllm框架中使用FP8 KV cache的用户。由于版本更新引入的问题，导致在使用llmcompressor进行量化的模型上出现了大量警告。

https://github.com/vllm-project/vllm/issues/12679
这个issue属于bug报告类型，主要涉及到cuda相关的问题，由于未正确导入pynvml模块，导致类似于之前提到的几个pull requests中出现的问题。

https://github.com/vllm-project/vllm/issues/12678
这是一个bug报告，主要涉及vLLM引擎中的logits processors和min-p sampling功能被忽略的问题。原因是V1引擎没有正确传递sampling metadata，导致这些特性无效。

https://github.com/vllm-project/vllm/issues/12677
这是一个bug报告，涉及到`hipify.py`文件中的许可证头部覆盖了shebang行，导致AMD构建机器无法识别Python语法。

https://github.com/vllm-project/vllm/issues/12676
这是一个性能优化的issue，主要涉及到MLA（Multi-layer attention）模型，通过调整KV缓存的对齐方式来提高性能。

https://github.com/vllm-project/vllm/issues/12675
这个issue类型是内部变更通知，主要对象是Facebook内部员工。原因是Facebook进行了一些内部的变更或调整，可能涉及到系统、流程或政策的改变。

https://github.com/vllm-project/vllm/issues/12674
这个issue类型属于功能改进，涉及主要对象为scheduler（调度器）。由于移除了对部分请求处理的限制，可能导致频繁对调度请求进行更改影响性能，特别是对持久批处理的效果和输入准备开销产生影响。

https://github.com/vllm-project/vllm/issues/12673
这个issue是关于安装问题的bug报告，涉及的主要对象是在运行vllm docker container时遇到的SSL连接错误。这个问题可能是由于SSL连接问题导致无法成功安装和运行docker container。

https://github.com/vllm-project/vllm/issues/12672
这是一个功能增强和依赖更新的issue，主要涉及到升级actions/setup-python插件版本。

https://github.com/vllm-project/vllm/issues/12671
这是一个bug报告类型的issue，涉及到代码库中pyo3、Codemod、FBSourceBlackLinter、vllm等模块的问题。由于代码提交的冲突或错误，导致出现了不正确的依赖管理或配置问题，需要进行修复或回退操作。

https://github.com/vllm-project/vllm/issues/12670
这是一个Bug报告，涉及DeepSeek V3的多节点推理中出现错误，可能是由于资源请求无法被满足导致的。

https://github.com/vllm-project/vllm/issues/12669
这个issue是一个bug报告，主要涉及的对象是Mistral模型。由于cv2版本冲突，导致DeepSeek模型的用户遇到问题。

https://github.com/vllm-project/vllm/issues/12668
该issue类型为用户提出需求，涉及的主要对象是vLLM的用户和开发者。由于slack变得更加成熟，导致提议废弃Discord并鼓励使用vLLM Slack。

https://github.com/vllm-project/vllm/issues/12667
这个issue是用户需求问题，主要涉及VLLM_HOST_IP参数的应用，由于多节点推理的使用频繁导致需求更清晰的消息给用户。

https://github.com/vllm-project/vllm/issues/12666
这是一个bug报告，主要涉及的对象是检查融合层是否在目标列表中。原因是漏掉了对所有融合层进行正确检查。

https://github.com/vllm-project/vllm/issues/12665
这是一个bug报告，主要涉及的对象是AMD平台，由于某个提交导致hipify.py出错，需要修复此问题。

https://github.com/vllm-project/vllm/issues/12664
这是一个Bug报告类型的Issue，主要涉及到vllm在使用过程中GPU利用率达到100%的问题。由于加载模型权重占用大量显存，导致GPU频繁高负载。

https://github.com/vllm-project/vllm/issues/12663
这是一个关于Bug的报告，主要涉及VLLM在重启后GPU占用100%且未返回结果的问题。

https://github.com/vllm-project/vllm/issues/12662
该issue类型是功能添加，涉及主要对象是在ROCm上启用DeepSeek模型。这个功能添加是由于需要使用Triton MLA后端在ROCm上启用DeepSeek模型，同时需要确保有"trust_remote_code"标志位。

https://github.com/vllm-project/vllm/issues/12661
这是一个Bug报告，涉及VLLM使用V1版本时在7.0.1版本中使用RTX 4090或GPU L4 x 4时导致程序崩溃的问题。

https://github.com/vllm-project/vllm/issues/12660
这是一个bug报告类型的issue，主要涉及VLM的merged multimodal processor和V1的支持。由于使用了不正确的命令forcepush导致DCO签署错误。

https://github.com/vllm-project/vllm/issues/12659
这是一个bug报告，涉及的主要对象是Prometheus datasource configuration，由于UID未使用变量，导致无法成功导入grafana配置文件。

https://github.com/vllm-project/vllm/issues/12658
这个issue类型是用户提出需求。主要涉及的对象是添加新模型YuE。由于文本生成和音频模型输入输出的差异，用户在问如何支持他们所希望的模型。

https://github.com/vllm-project/vllm/issues/12656
这是一个bug报告，该问题涉及的主要对象是vllm库中的FlashAttention-2后端，导致无法使用头尺寸为72。Bug的原因可能是与模型参数设置相关。

https://github.com/vllm-project/vllm/issues/12655
这个issue属于功能更新类型，主要涉及的对象是Github上的VLLM项目中与混合内存分配器相关的代码。由于需要支持混合模型的内存分配并优化不同类型的注意力层，这个功能更新提出了新的抽象概念和配置来实现多层共享KV缓存内存池的目的。

https://github.com/vllm-project/vllm/issues/12654
这是一个用户提出需求的issue，主要对象是针对MLA（Machine Learning Accelerator）模块的KV Splits heuristic。

https://github.com/vllm-project/vllm/issues/12653
这是一个优化建议，主要涉及的对象是`ConstantList`的创建过于频繁，导致性能可能受到一定影响。

https://github.com/vllm-project/vllm/issues/12652
这是一个需求提出的issue，主要涉及对象是在AWS SageMaker上运行vLLM时需要考虑HTTP头部信息来跟踪推断请求。

https://github.com/vllm-project/vllm/issues/12651
这是一个bug报告，主要涉及vllm中llm.generate()方法在多次调用时返回结果异常的问题。可能由于代码逻辑或环境配置不当导致了返回的结果不是自然语言。

https://github.com/vllm-project/vllm/issues/12650
这个issue是一个空白的Issue，类型为缺少具体内容的问题报告，涉及主要对象为项目vllm。导致该问题的原因可能是用户未完整描述问题或出现了信息丢失。

https://github.com/vllm-project/vllm/issues/12649
这是一个关于安装问题的bug报告，涉及主要对象是如何为在CPU机器上构建时创建envs.py文件。由于缺少libcudart.so.12文件，导致了无法import vllm._C模块的错误，从而引发了这个问题。

https://github.com/vllm-project/vllm/issues/12648
这个issue属于Bug报告，涉及的主要对象是`moe_wna16`模块。问题是由于`get_quant_method`方法在目标模块是attention层时仍然返回了moe、GPTQbased linear或AWQbased linear方法，导致了不正确的行为。

https://github.com/vllm-project/vllm/issues/12647
这是一个Bug报告，涉及的主要对象是VLLM的使用环境。由于moem_wna16参数引发的程序断言错误，导致出现了Assertion Error。

https://github.com/vllm-project/vllm/issues/12646
该issue属于bug报告，涉及主要对象是benchmark_prioritization.py文件的数据集构造请求函数。由于数据集不是JSON文件，导致构造出来的请求中Tuple元素的数量错误，造成bug。

https://github.com/vllm-project/vllm/issues/12645
这是一个Bug报告，主要涉及 benchmark_prioritization.py 文件中数据集不为JSON文件时构建请求中 Tuple 元素数量错误的问题。

https://github.com/vllm-project/vllm/issues/12644
该issue属于需求提出类型，涉及添加多个请求定时直方图，主要对象是系统中的度量指标。由于需要捕获引擎核心中特定事件之间的精确先请求时机间隔，需要前端进程记录和计算时间戳事件之间的时间间隔。

https://github.com/vllm-project/vllm/issues/12643
这是一个bug报告，涉及的主要对象是项目的文档。由于单词"evolved"被拼错导致的问题。

https://github.com/vllm-project/vllm/issues/12642
这个issue属于bug报告类型，主要涉及的对象是MLA（可能是一个功能或者模块）。由于无法推送到特定分支，需要新建PR以解除对发布的阻塞。

https://github.com/vllm-project/vllm/issues/12641
该issue类型为需求提出，主要涉及的对象是改进批量LLM以在离线场景中更好地利用共享前缀，但是目前内容的描述较为简单，需要参考RFC Batchllm以获取更多详细信息。

https://github.com/vllm-project/vllm/issues/12640
这个issue属于功能增强请求，涉及的主要对象是vllm项目中的Apple Metal支持。由于尝试使用PyTorch MPS回退模式实现Metal支持，导致目前生成的文本不合理，需要进一步调试和改进。

https://github.com/vllm-project/vllm/issues/12639
这个issue类型为提出需求，涉及的主要对象是MLA with chunked prefill算法，由于需要进一步的基准测试以确定是否将其默认设置为V0版本，同时为V1版本的实现奠定基础。

https://github.com/vllm-project/vllm/issues/12638
这个issue类型是需求关联，涉及的主要对象是MLA功能。由于需要先完成该文档https://github.com/vllmproject/vllm/pull/12601，导致需要禁用块预填充和/或前缀缓存。

https://github.com/vllm-project/vllm/issues/12637
这是一个性能优化类的issue，主要涉及到在DeepSeek模型中应用`torch.compile`来提高生成速度，原因是模型中的大量专家导致了生成延迟的问题。

https://github.com/vllm-project/vllm/issues/12636
这是一个bug报告，主要涉及的对象是针对CUDA平台返回字节的get_device_name函数。这个问题的原因是之前两个提交提出的更改引起的。

https://github.com/vllm-project/vllm/issues/12635
这是一个bug报告，涉及的主要对象是vllm工具中的设备返回信息处理逻辑。原因是在判断设备名称中是否包含特定关键词时出现了类型错误，导致无法处理字符串类型数据，进而触发了TypeError异常。

https://github.com/vllm-project/vllm/issues/12634
这个issue是关于功能改进的，主要涉及模型中的量化和融合模块映射的优化。由于融合模块的处理逻辑需要改进，并且量化配置需要从模型中获取映射信息，导致目前需要依赖硬编码的方式来确定模块映射关系，代码逻辑复杂，需要进行优化。

https://github.com/vllm-project/vllm/issues/12633
这是一个Bug报告，主要涉及VLLM项目下的DeepSeek R1模型无法在服务器上使用MI300X GPU运行的问题。这个问题是由于某个具体的提交导致的。

https://github.com/vllm-project/vllm/issues/12632
这个issue是关于bug报告类型，涉及的主要对象是xgrammar库。由于xgrammar版本升级和添加了选择支持，导致用户遇到了在使用xgrammar进行结构化输出时的功能差距。

https://github.com/vllm-project/vllm/issues/12631
该issue属于用户需求类型，主要涉及提出添加一个输入步骤以请求发布版本的功能。这是因为用户不想再需要将发布版本放入环境变量中创建新的构建。

https://github.com/vllm-project/vllm/issues/12630
这是一个关于软件构建（Build）过程中requirements设置的问题，主要涉及到VLLM插件的安装流程。由于设置VLLM_TARGET_DEVICE为特定设备名称会导致错误，建议将其设置为空或者添加"oot"作为另一个可接受值。

https://github.com/vllm-project/vllm/issues/12629
这个issue是bug报告，涉及的主要对象是处理器配置。由于先前的更改导致处理器配置的默认值发生变化，进而影响到L4 MoE模块的运行，导致了该bug。

https://github.com/vllm-project/vllm/issues/12628
这是一个建议类型的issue，主要对象是python源文件。由于Linux Foundation建议项目中添加SPDX许可证头部信息，并且工具扫描代码时判断缺少这些信息会导致问题，因此提出了这个issue。

https://github.com/vllm-project/vllm/issues/12627
这个issue类型是用户提出需求，涉及的主要对象是在vLLM Server中添加额外的指标。

https://github.com/vllm-project/vllm/issues/12626
这是一个bug报告，涉及到测试套件中的一个flaky测试，可能是由于输出token与基准值略有不同导致的 numerical issue。

https://github.com/vllm-project/vllm/issues/12625
这是一个bug报告，涉及到Vllm在执行过程中出现NCCL错误导致的问题。

https://github.com/vllm-project/vllm/issues/12624
这是一个bug报告，主要涉及ROCm下的内存性能分析。该问题源于当前的性能分析未考虑填充内存，导致缓存内存计算不准确并可能导致OOM错误。

https://github.com/vllm-project/vllm/issues/12623
这是一个功能需求的issue，主要涉及对象是TPU backend中的MultiLoRA实现。

https://github.com/vllm-project/vllm/issues/12622
这个issue属于bug报告，涉及vLLM中的generation_config，默认配置的更改导致了Llama3.23BInstruct版本出现的响应数据损坏问题。

https://github.com/vllm-project/vllm/issues/12621
这个issue类型为bug报告，涉及的主要对象是prefix caching。由于Python 3.12更改了`hash(None)`的行为，导致可能发生hash碰撞，进而影响结果准确性。

https://github.com/vllm-project/vllm/issues/12620
这是一个bug报告，主要涉及到检查`local_lora_path`参数时出现的不必要的deprecation警告，原因是始终返回True而不是检查非None值。

https://github.com/vllm-project/vllm/issues/12619
这个issue是一个用户提出需求的类型，主要涉及的对象是Reasoning models，需要在回答中只应用Guided/Structured grammar。这个需求的原因是希望让Reasoning models在输出结构化内容时更有效。

https://github.com/vllm-project/vllm/issues/12617
这是一个bug报告，主要涉及vLLM下的目标匹配修复导致的加载错误问题。

https://github.com/vllm-project/vllm/issues/12616
这是一个性能问题报告，涉及主要对象是使用滑动窗口注意力机制（SWA）时的潜在性能问题。增加批量大小导致显著的延迟增加，可能是由于SWA的支持有限导致的。

https://github.com/vllm-project/vllm/issues/12615
这是一个bug报告类型的issue，主要涉及的对象是cpu image tagging。由于`$RELEASE_VERSION` 变量为空或未设置，导致了cpu image release失败。

https://github.com/vllm-project/vllm/issues/12614
这是一个Bug报告类型的Issue，主要涉及的对象是vLLM应用在Ray集群中加载模型时出现GPU缺失的问题。由于vLLM 0.7.0版本升级后，无法在托管节点找到GPU，导致加载模型失败。

https://github.com/vllm-project/vllm/issues/12613
这是一个用户提出需求的issue，主要涉及的对象是在s390x架构上添加CPU推理支持。

https://github.com/vllm-project/vllm/issues/12612
这是一个bug报告，涉及的主要对象是Quark quantized models加载问题，由于调用顺序错误导致在加载特定类型的模型时权重不一致。

https://github.com/vllm-project/vllm/issues/12611
这是一个用户提出需求的类型issue，主要涉及文档页面的美化。

https://github.com/vllm-project/vllm/issues/12610
这是一个Bug报告类型的Issue，主要涉及的对象是VLLM模型。由于GPU内存逐渐增加导致可能会引发OOM错误。

https://github.com/vllm-project/vllm/issues/12609
这个issue类型为功能需求，主要对象是LoRA适配器。用户提出了需求，希望允许LoRA适配器通过内存中的张量字典进行指定。

https://github.com/vllm-project/vllm/issues/12608
这是一个bug报告，主要涉及KV cache manager中的代码更改。原因是在解码时引发异常以及单元测试用例的相应更改。

https://github.com/vllm-project/vllm/issues/12607
这个issue是关于安装问题的，涉及主要对象是vllm。这个问题可能是由于用户未能正确安装vllm导致环境重复，寻求相关问题帮助。

https://github.com/vllm-project/vllm/issues/12606
这是一个用户提出需求的类型的issue，该问题单涉及的主要对象是创建docker镜像文件。

https://github.com/vllm-project/vllm/issues/12605
这是一个Bug报告，主要涉及到vLLM V1中的TypeError问题，可能是由于函数get_device_name()返回的是bytes而不是字符串，导致调用.lower()时发生TypeError错误。

https://github.com/vllm-project/vllm/issues/12604
这是一个bug报告类型的issue，涉及到VLM的Qwen2.5-VL版本，由于要在transformers 4.49版本发布前运行该模型，需要从源代码安装transformers。

https://github.com/vllm-project/vllm/issues/12603
这是一个Bugfix类型的issue，主要涉及将额外的键添加到块哈希中，以在父块具有不同额外键的两个具有相同令牌字符串的块生成不同的哈希值。

https://github.com/vllm-project/vllm/issues/12602
这是一个特性需求类型的github issue，主要关注的对象是vllm服务，用户希望实现将推理和答案分开的功能。可能是为了避免手动处理API响应所带来的错误和繁琐。

https://github.com/vllm-project/vllm/issues/12601
该问题为功能需求，要求支持MLA算法与FP8计算，其中提到需要先实现另一个相关的pull request。

https://github.com/vllm-project/vllm/issues/12600
这是一个bug报告，主要涉及的对象是V1版本的代码。该问题导致在输入长度超过最大模型长度时引擎崩溃。

https://github.com/vllm-project/vllm/issues/12599
这是一个更新依赖关系的issue，涉及的对象是CI dependencies。

https://github.com/vllm-project/vllm/issues/12598
该issue类型为需求提议，主要对象是为V1版本创建设计文档中的特定部分，并添加前缀缓存设计文档。由于项目需要V1版本的设计文档，并希望加入前缀缓存设计，故提出该需求。

https://github.com/vllm-project/vllm/issues/12597
这是一个用户提出需求的issue，主要涉及MLA在deepseek v3/r1的实现，可能因代码合并导致问题。

https://github.com/vllm-project/vllm/issues/12596
这个issue类型是需求提出，主要对象是Qwen2.5-VL。由于完全缺少具体内容，用户可能正在草拟关于Qwen2.5-VL的提案或要求。

https://github.com/vllm-project/vllm/issues/12595
这个issue类型是bug报告，涉及的主要对象是Git的commit签名（sign-off）功能。这个问题由于缺少`s`导致忘记在`git commit`时进行签名，需要通过繁琐的操作来补救，提出了自动添加签名的解决方案。

https://github.com/vllm-project/vllm/issues/12594
这是一个bug报告，涉及到Torch.Compile对Parameter子类的使用问题，最终原因可能是FP8代码需要重构。

https://github.com/vllm-project/vllm/issues/12593
这是一则功能改进的issue，主要涉及ROCm中的使用HIP FP8 header，用于抽象硬件指令或软件模拟的fp8转换过程。

https://github.com/vllm-project/vllm/issues/12592
这是一个功能需求类型的issue，涉及添加GPU prefix cache hit rate % gauge。原因可能是为了添加更精确的GPU缓存命中率指标来监控系统性能。

https://github.com/vllm-project/vllm/issues/12591
这是一个关于在Triton FAv2 kernel中添加fp8和可变长度序列支持的特性需求，主要涉及的主要对象是Triton FAv2 kernel。由于性能需求和新特性增加，开发人员需要在内核中实现此功能，从而改善模型的运行性能。

https://github.com/vllm-project/vllm/issues/12590
这是一个bug报告，主要涉及的对象是OpenVino安装文档，由于缺少git clone命令，导致文档不完整。

https://github.com/vllm-project/vllm/issues/12589
这是一个用户提出需求的issue，主要对象是Triton FAv2 kernel，用户希望添加对变长序列的支持。

https://github.com/vllm-project/vllm/issues/12588
这个issue属于功能增强类型，主要涉及到CUDA图支持，旨在为Lucas的PR实现triton MLA attention kernel添加CUDA图支持。

https://github.com/vllm-project/vllm/issues/12587
这是一个需求类型的issue，主要涉及的对象是集成了blockquantized CUTLASS kernels的线性层（linear layers）。

https://github.com/vllm-project/vllm/issues/12586
这是一个bug报告，涉及chatglm的quantization修复问题，用户可能遇到量化方面的错误或问题。

https://github.com/vllm-project/vllm/issues/12585
这是一个文档更新的issue，主要涉及int4 w4a16量化范例的添加，用户提出了添加选择正确量化方案和音频校准数据集等改进建议。

https://github.com/vllm-project/vllm/issues/12584
这是关于一个Bug报告，主要涉及V1引擎在使用时无法支持`n>1`，导致`LLM.generate()`仅返回一个`CompletionOutput`的问题。

https://github.com/vllm-project/vllm/issues/12583
这是一个功能需求类型的issue，主要涉及了vLLM下的Expert Parallelism（EP）支持问题。由于目前的vLLM执行仅在运行MoE模型时支持TP，因此用户提出了添加EP支持的需求。

https://github.com/vllm-project/vllm/issues/12582
这是关于bug报告的issue，涉及的主要对象是模型输入处理，由于未能捕获到模型输入导致无法复现和调试非法内存访问错误，只增加了维护工作。

https://github.com/vllm-project/vllm/issues/12581
这是一个Bug报告，涉及的主要对象是vllm中尝试服务DeepSeek R1模型时出现的CUDA错误，导致AssertionError。

https://github.com/vllm-project/vllm/issues/12580
这是一个bug报告，主要涉及vLLM在TPU上出现的非确定性行为。

https://github.com/vllm-project/vllm/issues/12579
这是一个功能需求的issue，涉及到添加一个新的计数器和枚举类型，并解决了一些TODO。

https://github.com/vllm-project/vllm/issues/12578
这是一个bug报告，涉及ROCM指令在Triton安装方面存在错误，可能是由于链接指向错误的github仓库以及不匹配的指令所导致的。

https://github.com/vllm-project/vllm/issues/12577
这是一个bug报告，涉及主要对象为vllm的安装过程。由于环境配置或安装方式问题导致`_C.abi3.so`文件出现了undefined symbol错误。

https://github.com/vllm-project/vllm/issues/12576
这是一个用户提出需求的类型，主要涉及性能回归检查。原因是为了快速检查PR，以排除可能被忽视的重大性能回归。

https://github.com/vllm-project/vllm/issues/12575
这是一个关于文档改进的issue，主要涉及设备标签和安装指引的优化。

https://github.com/vllm-project/vllm/issues/12574
这是一个修复内核相关问题的issue，主要涉及的对象是sgl_moe_align_block_size和moe_align_block_size设置。这个问题可能是由于引入新的代码而导致的功能异常或bug。

https://github.com/vllm-project/vllm/issues/12573
该issue属于bug报告类型，涉及主要对象是DeepSeek-R1 1.58-bit Dynamic Quant模型在VLLM上的应用。由于R1 GGUFs在VLLM上无法正常工作，可能是由于不受支持所引起的错误。

https://github.com/vllm-project/vllm/issues/12572
这是一个Bug报告，主要涉及vLLM在使用过程中遇到的环境变量设置问题。导致问题的原因是环境变量限制了GPU的可见性，导致程序无法正确识别GPU设备。

https://github.com/vllm-project/vllm/issues/12571
这是一个bug报告，主要涉及到由于huggingface hub连接问题导致的启动引擎失败。

https://github.com/vllm-project/vllm/issues/12570
这是一个bug报告，主要涉及的对象是attention layers，在quant化过程中遇到问题。这个问题是由于当前合并破坏了AWQ quant加载新R1模型的过程，导致attention layers无法在16位的情况下保持不被量化。

https://github.com/vllm-project/vllm/issues/12569
这个issue是一个用户需求类型的问题，主要涉及到V1版本的日志记录功能，由于需要将V0版本的最大请求并发日志迁移到V1版本。

https://github.com/vllm-project/vllm/issues/12568
这个issue属于用户反馈类型，主要对象是软件版本V1的用户群体，用户们在此提供关于他们对V1的使用情况以及需求的反馈和意见。

https://github.com/vllm-project/vllm/issues/12567
这是一个Bug报告，涉及V1版本在某些情况下无法 gracefully 处理异常导致出错的问题。

https://github.com/vllm-project/vllm/issues/12566
这是一个bug报告，主要涉及 vllm-flash-attn 在 AMD 平台上构建的问题。原因可能是由于与 AMD 平台相关的兼容性或环境配置的错误。

https://github.com/vllm-project/vllm/issues/12565
这是一个bug报告，主要涉及MoE模型的设备字符串问题。该问题可能导致在Blackwell上运行MoE模型时出现错误。

https://github.com/vllm-project/vllm/issues/12564
这个issue类型是bug报告，主要涉及的对象是LoRA adapter错误消息。由于缺少空格，导致错误消息中出现了拼写错误，需要修复。

https://github.com/vllm-project/vllm/issues/12563
这是一个特性修复的issue，主要涉及的对象是guided decoding bitmask在GPU上的传递和性能优化。原因是原有的blocking bitmask memcpy操作导致CPU无法提前启动采样器内核，需要等待解码完成后再复制bitmask，通过设置`nonblocking=True`实现异步操作解决这个问题。

https://github.com/vllm-project/vllm/issues/12562
这是一个bug报告，主要涉及的对象是benchmark_moe.py文件。由于`is_marlin`没有被传递给`get_default_config`，以及对`tensorparallelsize`的支持不完整，导致了bug的出现。

https://github.com/vllm-project/vllm/issues/12561
这是一个需求提出的issue，主要涉及的对象是添加GPU缓存使用率的仪表盘。由于在V1版本中默认启用了前缀缓存，因此对CPU缓存使用率并不相关。

https://github.com/vllm-project/vllm/issues/12560
这是一个bug报告，主要涉及改变选项卡时设置`?device={device}`，导致了某个具体症状问题。

https://github.com/vllm-project/vllm/issues/12559
这个issue是一个bug报告，主要涉及了vllm容器无法正确设置LD_LIBRARY_PATH的问题，导致了无法找到/usr/local/nvidia目录的症状。

https://github.com/vllm-project/vllm/issues/12558
这是一个bug报告类型的issue，涉及的主要对象是DeepSeekV3模块。由于DeepSeekV3在使用图模式时会遇到错误，需要添加Mixture of Experts（MoE）调优支持，并通过添加`arg.trust_remote_code`参数和调整配置解决使用图模式时的错误。

https://github.com/vllm-project/vllm/issues/12556
这是一个用户提出需求的issue，主要涉及VLLM在Mac上的静态分发问题。用户反映由于Python解释器和Torch版本的限制，无法使用VLLM，希望类似于静态Golang或Rust应用程序进行分发。

https://github.com/vllm-project/vllm/issues/12555
这是一个更新依赖项的问题，主要涉及torch、torchvision和torchaudio的ppc64le平台要求。

https://github.com/vllm-project/vllm/issues/12554
这是一个Bug报告类型的issue，主要涉及的对象是Engine在运行Qwen2.5 Deepseek r1时无法启动。这可能是由于Engine参数配置不当或者代码逻辑错误导致的问题。

https://github.com/vllm-project/vllm/issues/12553
这是一个bug报告类型的issue，主要涉及VLM中的multi-modal processor，由于需要更新InternVL等模型以使用合并的多模态处理器，但在测试H2OVL时由于FlashAttention不支持`head_size=80`导致无法进行测试。

https://github.com/vllm-project/vllm/issues/12552
这是一个bug报告，涉及到vllmproject/vllm CC([Build/CI] Fix libcuda.so linkage)，由于链接问题导致一些用户没有libcuda.so路径出现问题。

https://github.com/vllm-project/vllm/issues/12551
这是一个关于代码优化和特定解码操作不兼容的bug报告，主要涉及piecewise compilation optimization和spec decoding。原因可能是两者目前无法完全集成。

https://github.com/vllm-project/vllm/issues/12550
这是一个bug报告，主要涉及问题出现在Huggingface tokenizer的使用过程中。导致bug的原因是由于在修改tokenizer后，调用断言时出现了错误。

https://github.com/vllm-project/vllm/issues/12549
这是一个bug报告，主要涉及Docker容器构建过程中遇到的构建速度异常缓慢的问题。

https://github.com/vllm-project/vllm/issues/12548
这是一个Bug报告类型的Issue，主要涉及DeepSeek的distilled模型在使用guided JSON输出时生成不完整的响应，可能是由于distilled模型需要更高的`max_tokens`设置才能正确运行的原因。

https://github.com/vllm-project/vllm/issues/12547
这是一个功能改进类型的issue，主要涉及到移动`requirements.txt`文件至`requirements/x.txt`目录并更新所有对需求文件的引用。

https://github.com/vllm-project/vllm/issues/12546
这是一个bug报告，主要涉及到vllm库中关于`intel_extension_for_pytorch`模块在多节点CPU推理过程中的依赖问题。该问题的症状是在CPU架构不支持`ipex`的情况下，当 `tensor-parallel-size` 大于 1 时会出现 `ModuleNotFoundError: No module named 'intel_extension_for_pytorch'` 错误。

https://github.com/vllm-project/vllm/issues/12545
这是一个bug报告，主要涉及的对象是encoder cache。该bug会导致encoder cache在请求中止时没有正确释放，可能会导致内存泄漏问题。

https://github.com/vllm-project/vllm/issues/12544
这是一个需求提出类型的issue，主要对象是V1 scheduler interface，由于尚未填写具体内容，无法分析导致的具体问题。

https://github.com/vllm-project/vllm/issues/12543
这是一个bug报告，涉及的主要对象是vLLM引擎中Flash Attention 3的使用。由于vLLM逻辑不支持指定kv缓存数据类型为fp8时运行Flash Attention后端，导致无法使用指定的模型进行推断。

https://github.com/vllm-project/vllm/issues/12542
这是一个Bug报告，主要涉及Vllm中的V1 Engine在A100上运行时出现问题。原因可能是由于环境配置或命令错误导致无响应。

https://github.com/vllm-project/vllm/issues/12541
这是一个Bug报告，主要涉及DeepseekR1模型加载失败的问题。造成这一问题的原因可能是权重绑定错误。

https://github.com/vllm-project/vllm/issues/12540
这是一个空白的issue，类型未知。

https://github.com/vllm-project/vllm/issues/12539
这是一个关于bug报告的issue，涉及到Deepseek models for L4 GPU无法工作的问题，可能是由于vLLM engine.py文件未能正确使用所有GPU而导致。

https://github.com/vllm-project/vllm/issues/12538
这个issue是一个请求添加新模型支持的类型，主要对象是Janus Pro 7B模型。由于目前VLLM不支持该模型，用户提出了需要该模型的支持的需求。

https://github.com/vllm-project/vllm/issues/12537
这是一个bug报告，主要涉及了MQA scoring中LogitsProcessor的问题，导致Speculative decoding破坏了guided decoding。

https://github.com/vllm-project/vllm/issues/12536
这个issue是一个bug报告，主要涉及Attention模块中使用kv_cache和forward_context.attn_metadata的问题。由于直接访问attn对象和attn_metadata可能导致性能降低和bug出现。

https://github.com/vllm-project/vllm/issues/12535
这个issue类型是改进请求，主要对象是V1版本中的不支持的配置，用户希望改进错误消息提供更清晰的信息。

https://github.com/vllm-project/vllm/issues/12534
这个issue属于功能需求类型，该问题单涉及的主要对象是Triton FAv2 kernel，用户正在寻求添加fp8和int8支持的帮助。

https://github.com/vllm-project/vllm/issues/12533
这是一个Bug报告，主要涉及VLLM和自定义模型，在设置VLLM_USE_V1=1时无法加载自定义模型。

https://github.com/vllm-project/vllm/issues/12532
这是一个用户提出需求的issue，涉及的主要对象是针对新模型架构的支持。用户希望添加一个新的模型架构至vllm中，导致该需求的提出。

https://github.com/vllm-project/vllm/issues/12531
这是一个功能需求，主要涉及TPU推理效率的性能优化，用户希望添加一个针对TPU推理的性能分析示例。

https://github.com/vllm-project/vllm/issues/12530
这是一个关于添加TTFT和TPOT直方图的需求问题，涉及到代码中Metrics模块的修改。

https://github.com/vllm-project/vllm/issues/12529
这是一个性能问题报告，主要涉及V1版本的内存使用情况，导致无法运行高长度上下文。

https://github.com/vllm-project/vllm/issues/12528
这个issue类型是优化提案，主要涉及MLA解码优化，由于原始的MHA计算方式较慢，故提议使用潜变量来计算MQA以提高效率。

https://github.com/vllm-project/vllm/issues/12527
这是一个用户提出的需求类型的issue，主要涉及Buildkite系统中CI的优化。该需求是为了节省CI成本，使得CI只有在precommit通过时才运行。

https://github.com/vllm-project/vllm/issues/12526
这是一个bug报告，涉及的主要对象是在多GPU实例（NC96 - 4xA100s）上运行Vllm时出现的超时错误。这个问题可能是由网络问题导致的。

https://github.com/vllm-project/vllm/issues/12525
这是一个Bug报告，涉及对象是在安装`vllm==0.7.0`时遇到问题。由于使用了不兼容版本的`uv`和`pip`导致安装失败。

https://github.com/vllm-project/vllm/issues/12524
这是一个bug报告，涉及的主要对象是vllm中的whisper quantization功能。这个问题的原因是缺少修复导致quantized whisper运行时出现有关不存在的`bias_scale`的投诉。

https://github.com/vllm-project/vllm/issues/12523
这个issue是一个功能增强类型的问题。该问题单涉及的主要对象是VLLM PDDisagg scenario。由于需要在长上下文场景下提高性能，因此提出了layerwise KV transfer解决方案。

https://github.com/vllm-project/vllm/issues/12522
这是一个Bug报告，主要涉及Vllm 0.7.0版本与DeepSeek Llama 70B引擎结合使用时出现的错误。因环境配置与引擎兼容性问题导致程序无法正常运行。

https://github.com/vllm-project/vllm/issues/12521
这是一个bug报告，处理的主要对象是`vllm`的pre-commit hook。该问题可能是由于每个改变的文件都触发了`suggestion pre-commit` hook导致的，产生了多次打印重复信息的情况。

https://github.com/vllm-project/vllm/issues/12520
这是一个需求类型的issue，主要涉及添加 ModelOpt FP4 Checkpoint 支持，需要修复 Modelopt 模型加载关于 kvscales 的问题。

https://github.com/vllm-project/vllm/issues/12519
这是一个新功能需求的issue，涉及到Kernel的支持问题，由于需要为NVFP4数据类型和量化内核添加gemms，其主要目的是为了支持NVFP4类型。

https://github.com/vllm-project/vllm/issues/12518
该issue是一个功能需求提案，主要涉及的对象是vLLM的tokenizer组件。

https://github.com/vllm-project/vllm/issues/12517
这是一个Bug报告，涉及主要对象是Cutlass integration。该问题是由于`sparsity_config.ignore`列表没有得到正确尊重而导致的。

https://github.com/vllm-project/vllm/issues/12516
这个issue是一个需求提出类型的问题，主要涉及的对象是指标(metrics)系统，由于需要跟踪每个请求的生成token数量，需要添加请求级别的prompt/generation_tokens直方图，以处理流式增量更新。

https://github.com/vllm-project/vllm/issues/12515
这是一个Bug报告，主要涉及的对象是vllm 0.7.0版本。由于模型加载后出现错误，可能是由于版本升级或配置参数等问题导致的。

https://github.com/vllm-project/vllm/issues/12514
这个issue类型是文档错误（typo）修正，涉及的主要对象是x86 CPU的安装文档。原因是文档中有拼写错误，导致阅读者在安装x86 CPU时可能会受到误导。

https://github.com/vllm-project/vllm/issues/12513
这是一个bug报告，主要涉及VLLM中的FlashAttn模块在安装时未解决的问题，导致用户设置`VLLM_ATTENTION_BACKEND`为`flashinfer`但系统未安装`flashinfer`时缺乏错误提示。

https://github.com/vllm-project/vllm/issues/12512
该issue类型是用户提出需求，主要涉及的对象是添加janus pro support for multi model。原因是用户希望增加这一功能，以支持多模型的需求。

https://github.com/vllm-project/vllm/issues/12511
这个issue是一个用户需求类型的问题，主要涉及的对象是支持torch.distributed作为多节点推断的运行时，由于目前仅支持Raybased分布式推断，用户希望增加对torch.distributed的支持。

https://github.com/vllm-project/vllm/issues/12510
这是一个Bug报告，主要涉及vllm版本为0.6.6.post1的环境中Asyncengine在发送请求后无响应的问题。

https://github.com/vllm-project/vllm/issues/12509
这是一个bug报告，涉及的主要对象是VLM（Variable Length Memory）模块。由于某些原因，导致了多模态处理器的`size`暴露存在问题，需要修复，并且需要添加v1支持。同时，需要将idefics3测试迁移到使用较小的模型。

https://github.com/vllm-project/vllm/issues/12507
这是一个Bug报告类型的Issue，主要涉及Metallama/Llama3.23BInstruct模型在使用speculative decoding和所有优化选项时出现的索引越界错误。

https://github.com/vllm-project/vllm/issues/12506
这是一个Bug报告。该问题涉及vLLM版本0.7.0中的token生成问题，由于V1架构可能导致了模型输出随机token。

https://github.com/vllm-project/vllm/issues/12505
这是一个Bug报告，涉及VLLM的Nvidia runtime安装问题，导致`/usr/bin/ld: cannot find lcuda: No such file or directory`错误。

https://github.com/vllm-project/vllm/issues/12504
这是一个bug报告，主要涉及 VLM（Vision-Language Model）中的处理器和 V1 功能支持，用户在测试中发现输出中多了一个额外的 EOS token。

https://github.com/vllm-project/vllm/issues/12503
这个issue是一个改进提案，涉及的主要对象是`OpenAIServingCompletion`，旨在使`raw_request`参数在`ServingCompletion`中变为可选项。

https://github.com/vllm-project/vllm/issues/12502
这个issue属于bug报告类型，涉及的主要对象是Qwen/Qwen2.5-VL-72B-Instruct模型。由于vllm不支持Qwen2_5_VLForConditionalGeneration架构，导致了报错和无法运行的问题。

https://github.com/vllm-project/vllm/issues/12501
这个issue是一个功能特性请求，主要涉及支持ROCm 6.3及更高版本和GPU Arch MI300及更高版本的PerTokenActivation PerChannelWeight FP8量化推理，其中主要解决了模型量化和性能提升的问题。

https://github.com/vllm-project/vllm/issues/12500
这是一个用户提交的文档问题反馈，主要涉及缺少OpenAI Whisper模型在“List of Supported Models”页面上，并未提供任何关于该模型的信息。

https://github.com/vllm-project/vllm/issues/12499
这是一个功能需求的issue，主要涉及到支持PerTokenActivation PerChannelWeight FP8 Quantization Inferencing。原因是为了增强模型在ROCm 6.3及以后版本以及GPU Arch MI300及以后版本的推断功能。

https://github.com/vllm-project/vllm/issues/12498
这是一个bug报告类型的issue，主要涉及vllm项目中的共享内存资源不足导致程序出错的问题。

https://github.com/vllm-project/vllm/issues/12497
这是一个缺失预提交格式文件的bug报告，涉及主要对象为NKI FA，可能由于未正确配置预提交格式文件而导致无法正确进行预提交操作。

https://github.com/vllm-project/vllm/issues/12496
该issue类型为用户提出需求，主要对象是引擎日志。导致此需求的原因可能是用户反馈希望在日志中明确展示当前使用的版本号。

https://github.com/vllm-project/vllm/issues/12495
该issue类型为更新请求，涉及主要对象为项目的README.md文件。原因是项目发布了V1 alpha版本，需要更新README.md。

https://github.com/vllm-project/vllm/issues/12494
这是一个关于修复预提交错误的 bug 报告，主要涉及持续集成系统的问题。

https://github.com/vllm-project/vllm/issues/12493
这是一个用户请教问题的issue，主要涉及对deepseek V3/R1在NVIDIA和AMD硬件上的性能优化及上下文长度选择的问题。

https://github.com/vllm-project/vllm/issues/12492
这是一个bug报告，主要涉及到vllm在使用deepseekai/JanusPro7B模型时出现的错误。原因可能是Transformers无法识别模型类型为`multi_modality`，导致加载模型时出现数值错误。

https://github.com/vllm-project/vllm/issues/12491
这是一个bug报告，主要涉及的对象是Deepseek V3，由于设置max_num_batched_tokens为非常大的数值（例如65536）导致Deepseek V3崩溃。

https://github.com/vllm-project/vllm/issues/12489
这个issue类型是bug报告，主要涉及到vLLM模式在运行时无法跳过分词器导致的问题。

https://github.com/vllm-project/vllm/issues/12488
这是一个关于vLLM中Sliding Window功能的bug报告，用户提到了设置maxmodellen时遇到的问题，疑问vLLM是否实现了滑动窗口机制。

https://github.com/vllm-project/vllm/issues/12487
这个issue类型是bug报告，涉及主要对象是LoRA adapter，由于添加了num_scheduler_steps配置导致adapter在multiGPU设置下无法提供正确响应。

https://github.com/vllm-project/vllm/issues/12486
这个issue是一个需求提出类型，主要对象是针对新的模型Qwen2.5-VL，由于没有回应导致用户寻求支持。

https://github.com/vllm-project/vllm/issues/12485
这是一个需求类型的issue，主要涉及的对象是AMD的ROCm配置。原因可能是需要更新默认的闪回注意力至CK FA。

https://github.com/vllm-project/vllm/issues/12484
这个issue是关于bug报告，主要涉及到vllm在处理冲突特性时崩溃的问题。

https://github.com/vllm-project/vllm/issues/12483
这是一个Bug报告类型的issue，主要涉及到GGUF库版本和Granite 3.0模型的支持。这个问题由于依赖库版本不匹配导致构建失败。

https://github.com/vllm-project/vllm/issues/12482
这个issue是关于代码修改的，主要涉及Pipe attn_logits_soft_cap through paged attention TPU kernels。这个问题可能由于代码结构问题导致的提交错误，需要添加单元测试并修正错误。

https://github.com/vllm-project/vllm/issues/12481
这是一个bug报告，主要涉及的对象是prometheus example中的路径问题。该问题由新的子目录引起，导致路径错误。

https://github.com/vllm-project/vllm/issues/12480
这是一个请求反馈（RFC）的类型，涉及主要对象是vLLM项目中添加Google TPU支持。原因是首次将另一个硬件后端添加到V1中并进行代码重构，需要讨论和反馈。

https://github.com/vllm-project/vllm/issues/12479
这个issue是用户提出需求类型，主要涉及Janus-Series中的Unified Multimodal Understanding and Generation Models。由于缺乏响应和额外的背景信息，用户提出了请求。

https://github.com/vllm-project/vllm/issues/12478
这是一个需求提出的issue，主要涉及的对象是将`IterationStats`传递给统计记录器，并在日志和Prometheus记录器中记录相关信息。产生该问题的原因是需要在日志统计记录器中基于特定日志间隔中的令牌数量计算吞吐量，在Prometheus记录器中仅需要记录提示和生成的令牌数量。

https://github.com/vllm-project/vllm/issues/12477
这是一个bug报告，主要涉及构建过程中内存耗尽导致OOM问题。

https://github.com/vllm-project/vllm/issues/12476
这是一个bug报告，涉及LLM在同一节点上初始化两次导致计算缓存已经被初始化的错误。

https://github.com/vllm-project/vllm/issues/12475
这个issue属于更新需求类型，主要涉及`pre-commit` hooks的linting packages，由于这些linting packages较旧，所以需要更新以符合最新版本要求。

https://github.com/vllm-project/vllm/issues/12474
这是一个用户提出需求的类型的issue，主要涉及vllm在使用systemd安全功能时的兼容性问题，可能由于相应的systemd安全设置与vllm运行所需设置不兼容而导致。

https://github.com/vllm-project/vllm/issues/12473
这是一个bug报告，涉及的主要对象是在API中添加了`reasoning_content`功能来支持推理模型，但在使用流式请求时遇到问题，需要修改openai python包才能正常传递请求。

https://github.com/vllm-project/vllm/issues/12472
这是一个bug报告类型的issue，主要涉及的对象是rerank API。由于缺失了警告功能的一次性提醒，可能导致用户无法及时注意到某些重要信息，需要替换缺失的警告功能。

https://github.com/vllm-project/vllm/issues/12471
这个issue类型是文档修改需求，涉及将文档中的代码块标记由反引号 fences 转换为冒号 fences，目的是根据特定要求对文档格式进行统一处理。

https://github.com/vllm-project/vllm/issues/12470
该issue属于用户提出需求类型，主要涉及VLLM在GPU节点上的配置及性能优化。用户想要在拥有4块GPU的节点上配置VLLM，以获得最小延迟或最大吞吐量。导致此问题的原因是用户想要最优化地配置VLLM以提高性能。

https://github.com/vllm-project/vllm/issues/12469
该issue类型为测试相关，主要涉及的对象是对top-p和top-k采样的基本测试。由于测试并不检查正确性（或准确性），用户提出了需要对这些采样方法进行初步测试的需求。

https://github.com/vllm-project/vllm/issues/12468
这个issue是用户提出需求类型的，主要对象是API for reasoning models like DeepSeekR1，用户希望添加`reasoning_content`参数来支持推理模型，以便用户可以查看推理过程中的步骤。

https://github.com/vllm-project/vllm/issues/12467
这是一个Bug修复类型的issue，涉及到GPT-2 GGUF 推理方法的失败。由于某种原因导致GGUF方法在推理过程中出现问题，需要修复此bug。

https://github.com/vllm-project/vllm/issues/12466
这个issue类型为未填充内容的问题报告，涉及的主要对象是测试。由于信息未填充，无法准确判断用户报告了什么具体问题或需求。

https://github.com/vllm-project/vllm/issues/12465
这是一个发布更新的技术要求类issue，涉及到vllm项目的软件版本更新及相关功能支持。

https://github.com/vllm-project/vllm/issues/12464
这是一个bug报告，主要涉及到xformers attention prefill metadata缺少`seq_start_loc`导致Phi3small with blocksparse attention无法工作的问题。

https://github.com/vllm-project/vllm/issues/12463
这是一个更新请求类型的issue，主要涉及的对象是helm/chart-testing-action。由于需要更新ct至v3.12.0等组件版本，并新增e2e测试，导致更新至2.7.0的需求。

https://github.com/vllm-project/vllm/issues/12462
这个issue是关于更新依赖版本的问题，主要涉及到GitHub仓库中的actions/stale。原因可能是为了获取最新的功能或修复已知的问题。

https://github.com/vllm-project/vllm/issues/12461
这个issue属于需求提出类型，主要涉及的对象是vLLM的插件后端，由于需要在插件后端的集成测试中重用vLLM现有的本地单元测试，需要将vLLM现有的UTs中与cuda相关的函数泛化，以便支持外部设备的计数和环境变量的更改。

https://github.com/vllm-project/vllm/issues/12460
这是一个用户提出需求的类型，主要涉及到vLLM项目的文档内容。由于缺少vLLM博客的链接，用户请求在文档中添加以增加博客的可见性。

https://github.com/vllm-project/vllm/issues/12459
这是一条标题为"updated"的issue，类型为更新通知，与此issue相关的主要对象可能是项目中的文件或功能。由于更新通知内容为空，可能是作者忘记填写具体的更新内容。

https://github.com/vllm-project/vllm/issues/12458
这是一个需求类型的issue，涉及的主要对象是前端（FrontEnd），由于未明确指出具体问题或需求细节，用户可能提出了关于支持Whisper Transcription Protocol的需求或问题。

https://github.com/vllm-project/vllm/issues/12457
这是一个优化建议，主要涉及输入准备过程中避免使用Python列表操作。

https://github.com/vllm-project/vllm/issues/12456
这是一个性能优化类型的issue，涉及到V1版本的功能改进，主要目标是提高输出IO性能以确保与GPU执行的重叠。

https://github.com/vllm-project/vllm/issues/12454
这是一个用户提出需求类型的issue，涉及的主要对象是update_from_output函数。由于需要对update_from_output函数做一些微小的优化，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/12453
这个issue属于bug报告，涉及的主要对象是vLLM在TPU上的运行。由于XLA 2.7与torch 2.7不兼容，在TPU上导致vLLM无法正常运行，需要使用xla 2.7 dev20250124与torch 2.6 dev20241216的组合来解决这个问题。

https://github.com/vllm-project/vllm/issues/12452
这个issue是一个Feature提议，主要涉及Qwen团队发布新模型的支持。

https://github.com/vllm-project/vllm/issues/12451
这个issue类型是bug报告，主要对象是vllm在TPU上运行时出现错误，可能是由于最近xla nightly的更改导致了这一问题。

https://github.com/vllm-project/vllm/issues/12450
这是一个bug报告，涉及的主要对象是`kernels/test_mha_attn.py`文件。出现这个问题的原因是之前的提交未恢复自上一个提交更改的测试。

https://github.com/vllm-project/vllm/issues/12449
这是一个Bug报告，涉及VLM的Merged multi-modal processor for GLM4V，由于无法启用glm4模型的前缀缓存导致的问题。

https://github.com/vllm-project/vllm/issues/12447
这是一个功能需求提案，主要涉及到VLM数据集采样函数在benchmark_serving.py中的复杂性问题。

https://github.com/vllm-project/vllm/issues/12446
这是一个Bug报告，主要涉及Granite 3.0 MoE模型的加载问题，由于未更新quant_config属性导致导致模型加载失败。

https://github.com/vllm-project/vllm/issues/12445
这个issue类型为技术性问题，涉及主要对象为ViT模型的FA3支持，由于FA3支持不可靠导致了bug。

https://github.com/vllm-project/vllm/issues/12444
这个issue属于用户提出需求类型，主要涉及对新模型“IDEA-Research/ChatRex-7B”的支持。原因在于vllm当前不支持具有特定体系结构和输入类型要求的模型，导致用户无法使用他们想要的模型。

https://github.com/vllm-project/vllm/issues/12443
这是一个 Bug 报告，主要涉及 AWS Neuron 环境下的 nrt_tensor_allocate 分配失败问题。

https://github.com/vllm-project/vllm/issues/12442
这是一个bug报告，涉及的主要对象是使用vLLM中的qwen2-vl模型并在ray serve中进行parallel requests时出现的shape mismatch错误。这可能是由于同时进行多个请求时导致的内部数据处理出现了错误。

https://github.com/vllm-project/vllm/issues/12441
这是一个bug报告，主要涉及到无法使用'CUDA'后端运行'_C::rms_norm'。由于环境问题导致出现了错误日志。

https://github.com/vllm-project/vllm/issues/12440
这是一个bug报告类型的issue，主要涉及的对象是vllm中的openai api_server，可能由于环境问题导致了无法获取模型输入数据的bug。

https://github.com/vllm-project/vllm/issues/12439
这是一个Bug修复类型的Issue，主要涉及的对象是mm hashing的功能。原因是希望在用户明确关闭mm hashing的情况下跳过对其的断言检查。

https://github.com/vllm-project/vllm/issues/12438
这是一个 bug 报告，主要涉及vLLM中捕获CUDA图时出现的错误。原因可能是在将代码合并到最新版本后出现的问题。

https://github.com/vllm-project/vllm/issues/12437
这是一个bug报告，涉及VLLM模型从S3加载出现HFValidationError的问题，可能是由环境配置或代码逻辑错误引起的。

https://github.com/vllm-project/vllm/issues/12436
这个issue是用户提出需求，请求支持深度搜索R1 GGUF 4位(Q4KM)。由于目前系统显示不支持该配置，用户希望获得相关支持。

https://github.com/vllm-project/vllm/issues/12435
这个issue是关于bug修复的，涉及到ViT MHA层中支持FA3的问题。原因是修改使用了不推荐使用的`flash_attn_func`而不是`flash_attn_varlen_func`，导致了需要修复的bug。

https://github.com/vllm-project/vllm/issues/12434
该issue类型为bug报告，主要涉及vllm项目中的Kernel，由于PR #12405导致性能退化，进而影响了FA2的使用。

https://github.com/vllm-project/vllm/issues/12433
这是一个bug报告，涉及到ViT MHA层的相关代码。由于使用了已移除的`flash_attn_func` API，导致CI失败，需要等待相关API恢复后再次合并PR。

https://github.com/vllm-project/vllm/issues/12432
这是一个bug报告，涉及的主要对象是平台插件。由于没有地方加载注册方法，导致无法调用`register_quantization_config`，以及新方法和新设备未注册到命令行界面，从而导致了数值错误。

https://github.com/vllm-project/vllm/issues/12431
这是一个用户提出需求的issue，主要对象是为AMD MI25/50/60添加支持。由于原先没有对这些旧的AMD GCN5架构显卡提供支持，用户通过添加两行代码实现了vllm对AMD MI60显卡的正常工作。

https://github.com/vllm-project/vllm/issues/12430
该issue是关于功能需求的，主要涉及前端的run_batch功能，添加了对score请求的支持，之前只支持chat completions和embeddings requests。

https://github.com/vllm-project/vllm/issues/12429
这个issue类型为功能添加需求，主要涉及vLLM在Hopper GPUs上Flash Attention 3（FA3）的支持，需要实现对SM 8.9和8.6的优化以及硬件支持。原因是在8.6和8.9上由于共享内存不足导致当前实现被完全禁用，需要进一步开展相关工作。

https://github.com/vllm-project/vllm/issues/12428
这是一个Bugfix类型的issue，主要涉及vLLM中的进度条显示问题，由于更新进度条步长不正确导致显示错误，需要进一步调整以正确显示进度。

https://github.com/vllm-project/vllm/issues/12427
这是一个Bug报告，涉及macOS系统上使用vllm-cpu v0.6.6版本出现无限感叹号的问题，可能是由于特定模型输入所导致的。

https://github.com/vllm-project/vllm/issues/12426
这是一个bug报告，主要涉及vLLM在M1 Macbook Pro上无法运行的问题，可能是由于环境配置不正确导致的。

https://github.com/vllm-project/vllm/issues/12425
这是一个bug报告，涉及安装问题，用户提到无法找到名为 "resources" 的模块。

https://github.com/vllm-project/vllm/issues/12424
这个issue是关于修复libcuda.so链接的问题，属于构建/持续集成问题，主要涉及到构建过程中的CUDA库链接问题，导致了可能编译错误或链接问题。

https://github.com/vllm-project/vllm/issues/12423
这个issue类型是用户提出需求，询问关于在vllm中如何使用聊天模板的问题。

https://github.com/vllm-project/vllm/issues/12422
该issue类型为技术问题，涉及的主要对象是torch_xla依赖包的更新。由于未更新`requirementstpu.txt`中的torch_xla版本导致CI仍在使用旧的torch_xla nightly版本，可能需要更新稳定的base image或其他方式来避免混淆。

https://github.com/vllm-project/vllm/issues/12421
这是一个需求提出类型的issue，主要涉及ROCm和AMD的模型llama 3.2的支持问题。

https://github.com/vllm-project/vllm/issues/12420
这是一个bug报告，涉及的主要对象是pydantic的日志验证器，由于未防止在对象通过pydantic验证之前记录未使用的额外字段，导致在请求内容的基础上进行路由时出现不正确的日志记录。

https://github.com/vllm-project/vllm/issues/12419
这是一个用户提出需求的issue，主要涉及ROCM llama 3.2的支持向上游推进。由于需要支持ROCM架构上的多模式llama 3.2，用户提出将相关代码合并到上游的请求。

https://github.com/vllm-project/vllm/issues/12418
该issue属于功能增强，主要涉及到对disaggregated prefill的离线测试。由于需要验证disaggregated prefill使用情况，因此增加了该离线测试。

https://github.com/vllm-project/vllm/issues/12417
这是一个bug报告，涉及主要对象是模型`nmtesting/llama27bsparse2of4`，可能由于GEMM kernel或者压缩kernel导致了模型输出乱码的问题。

https://github.com/vllm-project/vllm/issues/12416
这是一个实现功能的issue，主要涉及到日志记录器在V1版本中的初步实现。由于之前采用了多进程模式，导致一些指标数据丢失，现在需要恢复这些指标数据。

https://github.com/vllm-project/vllm/issues/12415
这是一个bug报告，主要问题涉及到`uncache_blocks`的撤销和支持重新缓存完整块，由于发现取消缓存块会导致不安全的情况，因为一个块可能被多个请求使用。

https://github.com/vllm-project/vllm/issues/12414
这是一个用户提出需求的issue，主要涉及的对象是VLLM项目中的使用统计功能。

https://github.com/vllm-project/vllm/issues/12413
这是一个bug报告，涉及主要对象是`moe_align_block_size_kernel`的shared arrays。由于shared arrays的问题，导致Mixtral推断时崩溃。

https://github.com/vllm-project/vllm/issues/12412
这个issue是关于bug修复的，涉及到MultiModal Tests的CI失败问题，可能由于BLIP-2处理错误导致。

https://github.com/vllm-project/vllm/issues/12411
这是一个用户提出需求的 issue，主要涉及到如何在vLLM中使用`meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8`模型。用户希望能够将该模型用于较大模型的推理，但由于可能不支持该量化方案，导致运行时出现错误。

https://github.com/vllm-project/vllm/issues/12410
这是一个bug报告类型的issue，主要涉及到PyTorch regional compilation功能。由于引入的改变导致性能回退和重新编译，用户通过部分回滚改变以减少编译时间。

https://github.com/vllm-project/vllm/issues/12409
这是一个特性需求，涉及前端支持在参数中重写生成配置的功能。

https://github.com/vllm-project/vllm/issues/12408
这是一个功能需求类型的issue，主要涉及的对象是AMD MI300，用户提出了添加针对该硬件的tuned moe配置。

https://github.com/vllm-project/vllm/issues/12407
这是一个bug报告，主要涉及的对象是vllm的benchmark脚本，由于max_model_len小于input_len + output_len导致统计数据不准确。

https://github.com/vllm-project/vllm/issues/12406
这是一个Bug报告类型的Issue，主要涉及Vllm在NeuronX上推理速度缓慢的问题，用户寻求关于如何提高推理速度以使其在一秒内完成的建议。

https://github.com/vllm-project/vllm/issues/12405
这是一个bug报告，主要涉及的对象是Kernel模块。由于选择了错误的设置（packedgqa）导致在进行真实的MHA时出现了错误。

https://github.com/vllm-project/vllm/issues/12404
这个issue是bug报告，涉及到服务启动失败的问题。这可能是由于环境配置问题导致的。

https://github.com/vllm-project/vllm/issues/12403
这是一个bug报告，涉及的主要对象是PyTorch 2.6版本在x86机器上使用cxx abi的问题。造成这个问题可能是因为PyTorch 2.6在处理cxx abi上存在复杂的方案。

https://github.com/vllm-project/vllm/issues/12401
这是一个Bug报告，涉及到VLLM引擎在单个GPU上进行推理时出现AsyncEngineDeadError的问题。这个问题可能是由于异步引擎任务失败导致的。

https://github.com/vllm-project/vllm/issues/12400
这是一个关于需求咨询的问题单，主要涉及到使用vLLM在服务Llama 3.370b模型给多个用户时的配置和性能问题。原因可能是用户在文档中未找到关于该模型使用的详细信息。

https://github.com/vllm-project/vllm/issues/12399
该issue属于用户提出需求类型，主要涉及集成SVDquant（W4A4 quantization）quantization方法到vLLM。用户提出这个需求是因为希望在LLM serving场景中通过该quantization方法实现模型优化和提高内存效率。

https://github.com/vllm-project/vllm/issues/12398
这是一个bug报告类型的issue，涉及到构建系统中的默认值同步问题，由于同步问题导致了构建失败。

https://github.com/vllm-project/vllm/issues/12397
这是一个用户提出的需求。该问题涉及到vllm在Ascend NPU backend中对变长输入的支持。

https://github.com/vllm-project/vllm/issues/12396
这个issue是一个bug报告，涉及的主要对象是CI/CD构建系统。由于wheel文件大小达到290 MiB导致GH200失败。

https://github.com/vllm-project/vllm/issues/12395
这是一个关于性能问题的报告，主要涉及到vLLM Cascade Attention的性能。由于实施了Cascade Attention后性能没有预期的改善，导致性能回归和延迟波动较大。

https://github.com/vllm-project/vllm/issues/12394
这个issue属于bug报告类型，主要涉及到输出 tokens 为0时的问题，原因可能是在使用tgi后端时数据同时包含了"choices"和"usage"。

https://github.com/vllm-project/vllm/issues/12393
该issue属于bug报告，主要涉及的对象是PyTorch 2.6和nightly版本的兼容性，问题可能由于新版本PyTorch与nightly版本之间的变更导致手动测试失败。

https://github.com/vllm-project/vllm/issues/12392
这是一个功能需求类型的issue，主要涉及对象是Intel GPU。由于需要添加XPU bf16支持，用户提出了对该功能的需求。

https://github.com/vllm-project/vllm/issues/12391
这是一个用户提出需求的问题，涉及的主要对象是使用vllm来服务gguf模型只使用CPU时出现的错误。由于用户不清楚如何将特定模型集成到vllm中，导致需要帮助解决。

https://github.com/vllm-project/vllm/issues/12390
这是一个Bug报告，涉及主要对象是在PyTorch环境下发生的UnicodeDecodeError。

https://github.com/vllm-project/vllm/issues/12389
这个issue是用户提出需求。主要对象是VLM模型在VisionArena Dataset上的性能测试。

https://github.com/vllm-project/vllm/issues/12388
该issue是关于"Support for Structured Outputs"的需求提出，主要对象是V1版本的engine。原因是目前V1版本的功能虽然部分实现了结构化输出支持，但与V0版本的支持还不完整，在speculative decoding方面存在一些问题，可能需要进一步的工作和优化。

https://github.com/vllm-project/vllm/issues/12387
该issue类型为性能问题探讨，主要对象是vLLM在推理模型上的性能表现，用户关注vLLM在推理模型上的执行性能及可能存在的性能问题。

https://github.com/vllm-project/vllm/issues/12386
这是一个bug报告，涉及的主要对象是vllm和pynvml之间的不兼容性，导致vLLM版本无法访问cuda兼容性信息。

https://github.com/vllm-project/vllm/issues/12385
这是一个用户提出需求的issue，主要涉及mistralai/Ministral-8B-Instruct-2410模型配置的context length限制问题。造成这个问题的原因是当前的模型配置只允许最大长度为32768，而用户需要增加到128k context length。

https://github.com/vllm-project/vllm/issues/12384
这是一个Bug报告，涉及主要对象为无法在OpenVINO上运行MiniCPMV。由于环境中PyTorch和CUDA版本不匹配，导致无法成功运行MiniCPMV。

https://github.com/vllm-project/vllm/issues/12383
这个issue属于需求变更类型，主要涉及到移除与两个metrics相关的过时代码，原因是这些metrics已被标记为过时，并且已经在之前的变更中提到。

https://github.com/vllm-project/vllm/issues/12382
这个issue类型是bug报告，涉及主要对象是Gaudi硬件，由于缺少安装要求的步骤导致设置过程失败。

https://github.com/vllm-project/vllm/issues/12381
这是一个用户提出需求的issue，主要涉及的对象是VLLM下的Whisper，用户希望添加BNB量化。

https://github.com/vllm-project/vllm/issues/12380
这是一个bug报告，涉及的主要对象是模型的性能。由于误解了`self.decode_only`字段的含义，导致每个批次都使用了上一个批次的`self.decode_only`值，从而导致性能回归。

https://github.com/vllm-project/vllm/issues/12379
这个issue类型是用户提出需求，主要涉及如何在vllm中使用回归任务，用户询问如何使用vllm进行回归任务推断。

https://github.com/vllm-project/vllm/issues/12378
这是一个 bug 报告，主要涉及 vLLM 与本地模型部署相关的问题。原因是无法正确识别模型存储的本地路径导致出现错误。

https://github.com/vllm-project/vllm/issues/12377
这是一个bug报告，涉及到vllm项目中与性能相关的问题，由于合并了初始化和准备操作导致了性能严重下降。

https://github.com/vllm-project/vllm/issues/12376
这是一个关于新增API的issue，主要对象是前端，提出了关于引入rerank API的需求。

https://github.com/vllm-project/vllm/issues/12375
这是一个Bug报告，主要涉及的对象是CUDA版本11.8，由于使用了`__viaddmin_s32`和计算能力为9.0a，导致了在CUDA版本小于12.0时无法构建FA3的问题。

https://github.com/vllm-project/vllm/issues/12373
这个issue是一个bug报告，主要涉及的对象是vllm中的speculative decoding和structured output。这个问题由于speculative decoding与structured output的结合导致xgrammar崩溃。

https://github.com/vllm-project/vllm/issues/12372
这个issue类型是新分支创建，涉及主要对象是版本控制系统。

https://github.com/vllm-project/vllm/issues/12371
这是一个bug报告类型的issue，主要涉及的对象是TPU环境下的模型推理性能问题，由于某些条件下导致性能下降和输出数据损坏。

https://github.com/vllm-project/vllm/issues/12370
该issue类型为内部更改通知，不是bug报告，涉及的主要对象为Facebook的内部代码库。

https://github.com/vllm-project/vllm/issues/12369
这是一个用户提出需求的issue，主要对象是V1版本的API server以及H100和H200模型，由于2K token budget限制导致了对H100和H200模型性能的影响。

https://github.com/vllm-project/vllm/issues/12368
该issue属于功能需求提案，主要涉及对象是VLLM模型的日志处理线程设置，并提出了一个新的环境变量`VLLM_LOGITS_PROCESSOR_THREADS`，旨在通过多线程方式优化VLLM模型在大批量数据情况下的性能以增加GPU利用率和降低ITL，特别对于那些需要额外启动CUDA核心或在不持有Python全局解释器锁时执行大量CPU绑定工作的情况。

https://github.com/vllm-project/vllm/issues/12367
这是一个需求类型的issue，涉及更新compressed-tensors版本，用户希望更新到v0.9.0版本。

https://github.com/vllm-project/vllm/issues/12366
这个issue属于安全漏洞报告，主要涉及torch.load()函数在使用时的安全性问题，可能导致恶意代码执行。

https://github.com/vllm-project/vllm/issues/12365
这个issue类型是发布版本的准备工作，主要涉及到vLLM V1架构的新特性和待办事项。

https://github.com/vllm-project/vllm/issues/12364
这是一个bug报告，主要对象是使用gguf进行推理操作时返回了垃圾数据。问题可能由于使用的模型路径或者参数设置不正确导致。

https://github.com/vllm-project/vllm/issues/12363
这个issue是关于提出需求的，主要涉及到vLLM项目中的配置格式和加载格式。由于存在一些不常见的使用情况没有被当前代码覆盖，因此提出了RFC以实现自定义配置和权重格式以及从自定义存储后端加载配置和权重的功能。

https://github.com/vllm-project/vllm/issues/12362
这是一个需求文档的issue，主要涉及的对象是Phi-4支持文档。

https://github.com/vllm-project/vllm/issues/12361
这是一个bug报告，主要涉及的对象是`wake_up`函数。由于`wake_up`函数缺失在API文档中，以及缺乏正确的调用顺序检查逻辑，导致了可能存在调用错误的bug。

https://github.com/vllm-project/vllm/issues/12360
这是一个Bug报告，主要涉及的对象是InternVL2模块，由于`modality_group_func`导致InternVL2在v1版本上推断出现问题。

https://github.com/vllm-project/vllm/issues/12359
这是一个其他类型的issue，主要涉及的对象是Intel Gaudi(高性能计算单元)，该问题是为了在Intel Gaudi加速器上启用FusedSDPA支持。

https://github.com/vllm-project/vllm/issues/12358
这个issue类型属于用户提出需求，并涉及支持Microsoft/Phi-4 Model。由于不太明白如何添加新模型，用户请求支持该模型。

https://github.com/vllm-project/vllm/issues/12357
这个issue是关于bug报告，主要涉及的对象是修复OpenAI API兼容性问题。由于OpenAI API不支持`ignore_eos`参数，导致需要修改请求payload，以及需要更新响应处理来确保稳健性。

https://github.com/vllm-project/vllm/issues/12356
这个issue属于功能需求类型，主要涉及的对象是benchmark script，用户提出了关于启用代理支持的需求。

https://github.com/vllm-project/vllm/issues/12355
这是一个用户提出需求的issue，涉及ViT MHA层添加FA2支持的功能更新。

https://github.com/vllm-project/vllm/issues/12354
这是一个bug报告，主要涉及的对象是在使用多个GPU运行模型时，工作负载没有被分配的问题。由于环境中PyTorch版本较低以及CUDA版本设置不当，导致工作负载未能正确地分配到多个GPU上。

https://github.com/vllm-project/vllm/issues/12353
这个issue是一个bug报告，主要涉及的对象是 vLLM 中使用 S3 路径时的文件克隆问题，由于路径构建算法错误导致以 "/" 结尾的 S3 路径无法正确克隆文件。

https://github.com/vllm-project/vllm/issues/12352
这是一个优化性质的issue，主要涉及简化`mrope_positions`构建，原因为发现了一种简化方法。

https://github.com/vllm-project/vllm/issues/12351
这个issue是关于bug报告，涉及到vLLM模型检查中的错误，导致无法检查特定模型体系结构，出现报错信息需提供解决方法。

https://github.com/vllm-project/vllm/issues/12350
这是一个bug报告，主要涉及到在 OpenVINO 中无法为 Qwen2.5 提供服务。原因是当前环境中 CUDA 不可用，导致出现该问题。

https://github.com/vllm-project/vllm/issues/12349
这是一个用户提出的关于使用工具调用功能的问题，主要涉及到如何在auto模式下使用vllm工具。用户遇到的问题是当未指定工具时，无法正常使用。

https://github.com/vllm-project/vllm/issues/12348
这是一个功能优化的Issue，主要涉及到ROCm/vllm中关于Custom Paged Attention的性能优化。

https://github.com/vllm-project/vllm/issues/12347
这是一个bug报告，主要涉及到在处理不同类型请求时导致的程序崩溃或`AssertionError`。

https://github.com/vllm-project/vllm/issues/12346
这是一个关于内存消耗过高的bug报告，涉及的主要对象是使用 Jetson Orin NX 16G jetpack 6.2 环境下的 vllm。

https://github.com/vllm-project/vllm/issues/12345
该issue类型为需求提交，主要对象是文档。由于用户希望添加meetup幻灯片到文档中，因此发起了该需求。

https://github.com/vllm-project/vllm/issues/12344
这是一个用户提出需求并请教问题的类型，主要涉及到vllm中的fp8 sparse gemm实现文件。用户询问了有关实现公式和数据排列的问题，可能是由于理解不清晰或需要进一步指导而提出的。

https://github.com/vllm-project/vllm/issues/12343
这是一个Bug报告类型的issue，涉及到vLLM和Hugging Face Transformers推理结果不一致的问题。原因可能是由于环境配置或代码实现的差异引起的。

https://github.com/vllm-project/vllm/issues/12342
这是一个bug报告，涉及主要对象是vllm中的Whisper selfattention模块，问题是因为使用`torch.empty`初始化偏置会导致关键值的溢出，请使用`torch.zeros`替代。

https://github.com/vllm-project/vllm/issues/12341
这个issue是一个功能增强（feature enhancement），主要涉及到FLOP计数的功能增加，因为用户在LLM推断过程中需要统计每个操作的FLOPS以及GPU上的性能限制。

https://github.com/vllm-project/vllm/issues/12340
这个issue类型为文档问题，涉及主要对象是torch.compile，用户在该问题中收集了有关torch.compile的常见错误。

https://github.com/vllm-project/vllm/issues/12339
这是一个bug报告，涉及到构建过程中的一些优化和bug修复，主要对象是构建工具以及CUDA内核版本选择。由于之前构建时会同时构建9.0和9.0a版本的CUDA内核，而实际上只有9.0a版本被使用，因此为了优化构建时间和二进制文件大小，这个issue提出只构建9.0a版本的内核。

https://github.com/vllm-project/vllm/issues/12338
这个issue是一个bug报告，主要涉及的对象是Triton package更新导致的dataclass错误，需要通过固定Triton到v3.1.0版本来解决这个问题。

https://github.com/vllm-project/vllm/issues/12337
这是一个Bug报告，涉及的主要对象是VLLM模型的推理过程。由于在运行多个LLMs的推理时，第二个模型始终处于挂起状态，可能是由于代码中的错误导致输出结果无法接收。

https://github.com/vllm-project/vllm/issues/12336
这是一个关于如何在vllm服务中记录传入请求（输入和输出）的问题，类型是用户提出需求。该问题涉及到vllm的服务日志记录功能。由于当前使用`vllm serve`命令无法直接记录请求，用户寻求帮助如何实现记录请求的功能。

https://github.com/vllm-project/vllm/issues/12335
这是一个关于功能使用的issue，涉及主要对象为SPMD和Prefix Cache，用户可能遇到由于模型大小与版本兼容性问题而导致的困惑。

https://github.com/vllm-project/vllm/issues/12334
这是一个需求更新的issue，主要涉及更新TPU CI脚本和使用torchxla nightly版本。

https://github.com/vllm-project/vllm/issues/12333
该issue是一个功能请求，主要涉及kv cache manager中的`.uncache_blocks`添加，目的是在给定请求的`num_computed_tokens`减少时取消缓存块。

https://github.com/vllm-project/vllm/issues/12332
该issue是关于bug报告，主要涉及前端在使用`tokenizer-mode=mistral`时生成有效的工具调用ID的问题。由于请求中的工具调用ID长度不是9，导致出现错误，并且在某些情况下返回无效的工具ID，需要修复生成有效的9字符工具ID的问题。

https://github.com/vllm-project/vllm/issues/12331
这个issue类型为功能需求，主要涉及对象是添加interleave sliding window功能。这个需求可能是由于提高数据处理性能或实现特定功能要求而提出的。

https://github.com/vllm-project/vllm/issues/12330
这是一个bug报告，主要涉及vllm中的两个指标prompt_tokens_total和generation_tokens_total的总和不符合预期值。

https://github.com/vllm-project/vllm/issues/12329
这是一个bug报告类型的issue，主要涉及到AMD LoRA的CI测试。该问题可能是由于测试代码中的错误导致了CI测试失败。

https://github.com/vllm-project/vllm/issues/12328
这是一个关于模型配置与RoPE（Positional Encoding）和Sliding Windows处理文本长度的比较与优化的issue。

https://github.com/vllm-project/vllm/issues/12327
这是一个bug报告，主要涉及到pre-commit在运行时出现的isort错误。导致此问题的原因是`isort`错误，可能是代码格式问题。

https://github.com/vllm-project/vllm/issues/12326
这个issue类型是需求类问题，涉及主要对象是文档的漏洞披露流程。由于缺乏明确的操作顺序，导致漏洞修复和公告发布流程不清晰。

https://github.com/vllm-project/vllm/issues/12325
这是一个优化Cross-Attention QKVParallelLinear计算的Issue，提出了针对当前suboptimal的QKV投影进行优化的建议。

https://github.com/vllm-project/vllm/issues/12324
这是一个用户提出需求的issue，主要涉及的对象是vllm项目下的pre-commit安装。由于安装过程速度较慢，用户希望能够默认使用uv来提升速度。

https://github.com/vllm-project/vllm/issues/12323
这是一个bug报告，主要涉及vllm的speculative decoding功能，可能存在无法正常工作的问题。

https://github.com/vllm-project/vllm/issues/12322
这个issue属于用户需求类问题，主要涉及对象是VLLM生成速度是否能通过添加另一块视频卡来提升。用户询问是因为在使用两块视频卡进行模型推理时，并没有提升生成速度，想寻求加速生成速度的方法。

https://github.com/vllm-project/vllm/issues/12321
这是一个bug报告，主要涉及的对象是关于通信同步在分立prefilling中的问题。因为通信同步不正确，导致decode阶段无法获取正确的隐藏状态，用户希望解决这个问题。

https://github.com/vllm-project/vllm/issues/12320
这个issue类型是bug报告，单涉及的主要对象是BNB错误消息的可读性。由于错误消息不易理解，用户提出了需要改进错误消息可读性的问题。

https://github.com/vllm-project/vllm/issues/12319
这是一个bug报告，涉及的主要对象是关于`--lora-modules`参数的使用。问题产生的原因是由于`LoRAModulePath`结构体中不包含`local_path`字段，但在使用时却尝试使用了该字段，导致引发了错误。

https://github.com/vllm-project/vllm/issues/12318
该 issue 类型为文档补充，主要涉及的对象是新增关于提示替换的文档。导致该问题的原因是可能之前的文档中未提及有关内容，用户需要了解和参考有关提示替换的信息。

https://github.com/vllm-project/vllm/issues/12317
这是一个bug报告类型的issue，主要涉及的对象是Gaudi硬件。这个问题发生的原因是因为在启用guided decoding时导致的错误。

https://github.com/vllm-project/vllm/issues/12316
这个issue为测试问题，涉及的主要对象是fake-hpu构建。原因可能是为了测试构建流程或验证新功能。

https://github.com/vllm-project/vllm/issues/12315
这个issue属于性能优化提议，主要关注vLLM在A100设备上无法达到宣称的吞吐量和延迟结果，可能由于性能回归导致问题。

https://github.com/vllm-project/vllm/issues/12314
这是一个性能问题，主要涉及日志中请求添加过程中的性能延迟。

https://github.com/vllm-project/vllm/issues/12313
这是一个bug报告，主要涉及VLM下的mixed-modality inference，由于V0和V1基于不同假设进行嵌入合并，导致`get_input_embeddings`在V0下无法正常工作并产生bug。

https://github.com/vllm-project/vllm/issues/12312
这是一个bug报告，该问题涉及 Qwen2VL with lora 在当前环境下无法工作。由于PyTorch版本和CUDA环境等相关信息，可能导致此问题的症状。

https://github.com/vllm-project/vllm/issues/12311
这是一个bug报告，该问题涉及的主要对象是在使用RunAI Model Streamer与S3时遇到文件访问错误，可能是由环境配置问题导致。

https://github.com/vllm-project/vllm/issues/12310
这是一个优化建议的issue，主要对象是VLM中的tokenization过程。由于重复对`mm_token`进行tokenization导致运行测试耗时较长，提出了优化建议。

https://github.com/vllm-project/vllm/issues/12309
该issue类型是bug报告，涉及主要对象为Aria模型输出，由于模型输出不正确导致用户在本地环境下无法获取正确结果。

https://github.com/vllm-project/vllm/issues/12308
这是一个bug报告，涉及的主要对象是vLLM模型在使用docker环境下可能存在GPU内存利用问题。导致该问题可能是由于用户希望在高负载情况下为vLLM提供尽可能多的内存以进行自动扩展。

https://github.com/vllm-project/vllm/issues/12307
这是一个Bug报告类型的Issue，涉及的主要对象是使用vllm版本`v0.6.1.post2`运行在4个A10 GPUs上的应用。由于同时发送20个1k请求导致CUDA异常，可能是由并行用户操作引起的问题。

https://github.com/vllm-project/vllm/issues/12306
这是一个测试性质的问题单，暂时不要合并；主要涉及清理未使用的Docker镜像/容器。

https://github.com/vllm-project/vllm/issues/12305
这个issue类型为用户询问问题，主要涉及LLMEngine的使用和Qwen-VL的集成。用户提出了如何在MLLM中运行推理QwenVL并将其与vllm集成的问题。

https://github.com/vllm-project/vllm/issues/12304
这个issue是关于功能需求的，主要涉及Eagle Spec Decode的简化使用，原因可能是现有功能较为复杂，用户希望简化使用流程。

https://github.com/vllm-project/vllm/issues/12303
该issue类型为特性请求，主要涉及的对象是启用Mixtral的动态MoE功能。由于用户想要测试精度，因此提出了启用Dynamic MoE的特性请求。

https://github.com/vllm-project/vllm/issues/12302
这是一个Bug报告，主要对象是Linting pre-commit hook和yapf工具。导致这个问题的原因是precommit hook没有正确应用yapf修复，导致yapf在静默失败的情况下无法通过。

https://github.com/vllm-project/vllm/issues/12301
这是一个需求提议类型的issue，主要涉及Disaggregated prefill与pipeline parallelism的兼容性问题。该问题源于当前逻辑只支持假设中间隐藏状态为张量，而在pipeline parallelism中可能为字典。

https://github.com/vllm-project/vllm/issues/12300
这个issue是关于bug报告，涉及到构建docker时遇到错误的内容。问题可能是由于代码或环境配置的问题导致的。

https://github.com/vllm-project/vllm/issues/12299
该issue类型为功能改进，涉及主要对象为vLLM的安装配置。由于安装vLLM时使用了`requirementscuda.txt`导致下载了许多与cuda相关的包，因此需要更新为使用`requirementscpu.txt`来避免这种情况。

https://github.com/vllm-project/vllm/issues/12298
该issue属于优化类型，主要涉及前端代码中的请求输出处理，问题是在高负载情况下，请求数据可能会累积导致处理速度变慢，通过合并输出来提高性能。

https://github.com/vllm-project/vllm/issues/12297
这是一个需求提交类型的issue，主要涉及DeepSeek-R1工具选择和功能调用，用户希望支持工具选择和chattemplate以获得更好的结果。

https://github.com/vllm-project/vllm/issues/12296
这是一个bug报告，主要对象是修复mypy错误。这个问题可能是由于代码中存在类型检查错误而导致的。

https://github.com/vllm-project/vllm/issues/12295
这是一个bug报告，主要涉及的对象是CI/lint中的pre-commit功能，由于最近出现的precommit错误导致的问题。

https://github.com/vllm-project/vllm/issues/12294
这是一个功能改进的issue，主要涉及Kernel和paged_attention，并由于需要解决一些模型的采纳问题而提出。

https://github.com/vllm-project/vllm/issues/12293
这是一个用户提出需求的issue，主要对象是vLLM-Llama-2-7b-hf，由于环境信息显示的PyTorch版本与CUDA信息不匹配，可能导致用户想了解vLLM-Llama-2-7b-hf中使用了哪些操作符。

https://github.com/vllm-project/vllm/issues/12292
这是一个Bug报告，主要涉及vLLM在启动时卡在pynccl.py步骤无法加载模型，需要通过设置NCCL_P2P_DISABLE=1参数来解决，可能是由于NCCL相关问题导致的。

https://github.com/vllm-project/vllm/issues/12291
这是一个bug报告类型的issue，主要涉及Neuron CI的稳定性问题，可能导致Neuron后端测试的不稳定性。

https://github.com/vllm-project/vllm/issues/12290
这个issue类型是用户提出需求，询问是否支持从Google Cloud Storage bucket加载模型，主要涉及对象是vllm中的Model Streamer功能。由于文档中只提到支持从AWS S3存储加载模型，用户想知道是否支持从GCS加载模型。

https://github.com/vllm-project/vllm/issues/12289
这是一个用户提出需求的问题，主要涉及并发实现，由于同时调用openai api导致无法同时回答问题。

https://github.com/vllm-project/vllm/issues/12288
这个issue类型为bug报告，主要涉及的对象是TPOT的benchmark_serving.py文件，由于使用tokenizing output text来确定输出标记数量不可靠，特别是在存在特殊标记和/或使用`ignoreeos`时，导致了bug的症状。

https://github.com/vllm-project/vllm/issues/12287
这个issue是关于性能优化的需求，主要涉及前端服务的在线性能改进，通过对输出处理和GC优化来提升性能。

https://github.com/vllm-project/vllm/issues/12286
这个issue属于需求反馈类型，主要涉及核心功能中的指标统计。导致该需求提出的原因是为了实现自动扩展策略时能够正确计算等待队列中的令牌数。

https://github.com/vllm-project/vllm/issues/12285
这是一个关于优化的问题，主要涉及了Disaggregated Prefilling和KV Cache Transfer，由于不是所有批处理请求中的KV缓存都能收到，导致之前decode节点针对所有请求进行了预填充，现在希望只对那些缺少KV缓存的请求进行预填充。

https://github.com/vllm-project/vllm/issues/12284
这是一个功能需求类型的issue， 主要涉及的对象是vllm代码库中的prefix cache，问题是关于支持`reset_prefix_cache`功能。

https://github.com/vllm-project/vllm/issues/12283
这是一个用户提出需求的issue，主要涉及添加KV Cache Metrics到Usage Object，用户希望能更好地了解工作负载中缓存的令牌情况。

https://github.com/vllm-project/vllm/issues/12282
这是一个bug报告，涉及的主要对象是AMD的Quantization功能。由于缺少TritonScaledMMLinearKernel类导致int8模型无法运行。

https://github.com/vllm-project/vllm/issues/12281
该issue类型为文档更新，涉及的主要对象是为了添加关于预构建的ROCm vLLM docker用于性能验证目的的信息。

https://github.com/vllm-project/vllm/issues/12280
这是一个需求类型的issue，主要涉及自动化添加标签功能。这个问题的原因是希望根据PR涉及的文件自动应用相关标签。

https://github.com/vllm-project/vllm/issues/12279
这个issue类型是bug报告，涉及的主要对象是pre-commit中的default arg。由于`extra_args`被更新导致`allfiles`默认参数被重写，需要将默认参数重新添加以及添加新的extra args。

https://github.com/vllm-project/vllm/issues/12278
这是一个用户提出需求的issue，主要涉及的对象是vLLM，用户想知道vLLM是否支持MoE模型的投机性解码功能。

https://github.com/vllm-project/vllm/issues/12277
这是一个关于bug的报告，主要涉及对象是vllm代码中的multistep功能模块。由于在skip_tokenize_init和multistep同时设置时，执行会崩溃，问题是由于multistep缺乏detokenizer的存在性检查所致。

https://github.com/vllm-project/vllm/issues/12275
这个issue属于bug报告，主要涉及到VLLM中的detokenizer，在多步模式下sliptokenizerinit无法正常工作。

https://github.com/vllm-project/vllm/issues/12274
这是一个关于性能问题的Bug报告，主要涉及VLLM版本0.6.1.post2和0.6.6.post1之间的性能差异，用户提出了TTFT方面的巨大性能差异并寻求解决方案。

https://github.com/vllm-project/vllm/issues/12273
这是一个bug报告，主要涉及的对象是benchmark_moe.py文件，由于错误地推送了`is_navi()`检查，导致了问题。

https://github.com/vllm-project/vllm/issues/12272
这是一个用户提出需求的issue，主要涉及文档中缺乏关于如何使用Quark来准备模型以及vLLM目前支持哪种类型的Quark量化方案的内容。

https://github.com/vllm-project/vllm/issues/12271
这是一个用户提出需求的类型，涉及的主要对象是NVIDIA Blackwell codegen，用户提出关于Blackwell B100/B200和Blackwell RTX 50 codegen版本的问题。

https://github.com/vllm-project/vllm/issues/12270
这是一个需求变更的issue，涉及的主要对象是更新 nightly torch 版本。其原因是由于 pytorch cuda 12.4 不再发布 arm64 版本的 wheels，因此需要更新到 cuda 12.6 版本。

https://github.com/vllm-project/vllm/issues/12269
这是一个用户提出需求的issue，主要涉及到VLM（Vision-Language Model），其内容表明用户希望简化替换信息的后处理过程。

https://github.com/vllm-project/vllm/issues/12268
这个issue是bug报告，涉及的主要对象是无法加载W4A16-G128（llmcompressor）量化模型在CPU上。这个问题可能是由于在vllm 0.6.5和ipex 2.5.0上加载模型时出现错误（TypeError: 'NoneType' object is not subscriptable）所致。

https://github.com/vllm-project/vllm/issues/12267
这是一个Bug报告，主要涉及VLLM中的某个函数缺少属性导致 AttributeError 错误。

https://github.com/vllm-project/vllm/issues/12266
这是一个关于性能问题的报告，涉及到VLLM服务中的预填充和解码阶段为同一任务执行两次的现象。

https://github.com/vllm-project/vllm/issues/12265
这是一个用户提出需求类型的issue，主要涉及的对象是encoder-decoder模型。该issue的提出主要是为了支持encoder-decoder模型的多步调度。

https://github.com/vllm-project/vllm/issues/12264
这是一个bug报告，主要涉及的对象是platforms模块的`__getattr__`函数。由于当前实现中返回KeyError而不是AttributeError，因此出现了问题当没有名为`name`的属性时。

https://github.com/vllm-project/vllm/issues/12263
这是一个用户需求问题，主要涉及对使用OLMo模型进行工具调用的需求。由于缺乏所需的工具解析器，用户询问是否有可用的OLMo模型的工具解析器。

https://github.com/vllm-project/vllm/issues/12262
这个issue属于用户提出需求类型，主要涉及问题是在L20 GPUs上是否可以运行2:4 sparse fp8 quantized model，原因在于CUDA Compute Capability不符合要求。

https://github.com/vllm-project/vllm/issues/12261
该issue类型为Feature需求，主要涉及到torch dynamo的编译功能。由于设置编译级别为PIECEWISE(3)时无法使用用户指定的后端，用户希望实现支持传入用户指定后端以进行编译。

https://github.com/vllm-project/vllm/issues/12260
这个issue类型是bug报告，主要涉及的对象是`use_existing_torch.py`脚本。由于存在评论导致两个必需的依赖包被删除，用户请求修复此问题。

https://github.com/vllm-project/vllm/issues/12259
这个issue属于bug报告类型，涉及到了模态感知方面的问题，主要是由于批处理多模态输入时导致嵌入顺序错乱的bug。

https://github.com/vllm-project/vllm/issues/12258
这是一个bug报告，主要涉及的对象是deepseek v3，由于无法在L20上设置`tensor_parallel_size=16`和`pipeline-parallel-size=2`，导致模型加载权重时耗费大量内存并提示性能可能不佳的问题。

https://github.com/vllm-project/vllm/issues/12257
这是一个针对功能改进的问题，主要涉及的对象是PD分离中的前缀缓存支持。问题由于未能支持前缀缓存，导致了计算方式的错误。

https://github.com/vllm-project/vllm/issues/12256
这是一个bug报告，该问题涉及的主要对象是vllm中的deepseek v3模型。由于无法设置`tensor_parallel_size=32`，导致了报错的症状。

https://github.com/vllm-project/vllm/issues/12255
这是一个bug报告，主要涉及到项目中的脚本`use_existing_torch.py`不能正确过滤出指定依赖导致安装错误的问题。

https://github.com/vllm-project/vllm/issues/12254
这是一个功能需求（RFC）的issue，主要涉及了关于Sparse KV cache management framework的设计提议。由于需要在长上下文中清除低利用率的token，提出了一种灵活和高性能的KV缓存管理框架。

https://github.com/vllm-project/vllm/issues/12253
该issue类型是功能改进，涉及对象为构建器实例及注意力元数据构建器实例，由于当前每个批次都创建新的构建器实例，导致注意力元数据构建器无法访问全局信息，需要在每个批次准备时保存全局配置信息，解决在升级到flashinfer 0.2版本时，注意力元数据构建器无法访问全局信息的问题。

https://github.com/vllm-project/vllm/issues/12252
这是一个Bug报告，涉及到VLLM下的一个issue，该问题由于`init_mm_limits_per_prompt`未被调用导致无法正常运行相应命令。

https://github.com/vllm-project/vllm/issues/12251
这是一个用户提出需求的issue，主要涉及到vLLM框架对BaichuanM1模型推理支持的新增。由于新增模型BaichuanM1在医疗领域表现出色，用户希望vLLM能够无缝处理该模型的推理，以优化在自然语言处理任务中的性能。

https://github.com/vllm-project/vllm/issues/12250
该issue类型是一个关于提出需求的特性请求，主要涉及到通过远程KV存储（如Redis）加载模型的功能。由于目前加载模型所花费的时间较长且涉及到多个步骤，用户提出了使用RemoteModelLoader类来直接从远程数据库加载模型，以更快地加载模型并避免涉及本地磁盘的方式。

https://github.com/vllm-project/vllm/issues/12249
这是一个用户提出需求的issue，主要涉及的对象是vLLM模型的隐藏状态处理。由于目前使用Pooler提取隐藏状态的方法仍不够用户友好，并且缺乏对隐藏状态的自定义处理支持，用户提出了希望添加`HiddenStatesProcessor`来自定义处理隐藏状态的建议。

https://github.com/vllm-project/vllm/issues/12248
这个issue属于代码优化类，主要涉及的对象是 base model 中的 TypeVar，由于不再使用 `VllmConfig`，所以移除了与 HF config 相关的冗余 TypeVar。

https://github.com/vllm-project/vllm/issues/12247
这是一个关于vLLM是否支持在CUDA 11.8上运行DeepSeek-V3模型的问题，属于用户需求咨询类型的issue。

https://github.com/vllm-project/vllm/issues/12246
这是一个特性需求的issue，主要涉及的对象是`tortch.compile`功能，反馈了添加更多日志信息的需求。

https://github.com/vllm-project/vllm/issues/12245
这是一个bug报告类型的issue，主要涉及V1 engine在使用Qwen2VL时在单GPU下正常工作，但在TensorSplit（多GPU）下出现问题。由于没有调用init_mm_limits_per_prompt导致了这个bug。

https://github.com/vllm-project/vllm/issues/12244
这是一个关于更名问题的需求类型的 issue，主要涉及名称为`MultiModalInputsV2`的对象。原因可能是在两个月前就已经提出更名请求，所以现在认为可以替换旧名称。

https://github.com/vllm-project/vllm/issues/12243
这是一个功能改进的issue，主要涉及到torch.compile中的compile sizes和cudagraph sizes的解耦。

https://github.com/vllm-project/vllm/issues/12242
这个issue是关于增加功能需求方面的，主要涉及到前端和服务器之间的最大生成token数设置。导致这个需求提出的原因可能是用户希望能够更灵活地控制生成token的数量。

https://github.com/vllm-project/vllm/issues/12241
这是一个bug报告，主要涉及到vllm下的Aria模型输出不正确的问题。原因可能是由于Aria模型与transformers版本升级导致输出变得荒谬或不正确。

https://github.com/vllm-project/vllm/issues/12240
这是一个bug报告类型的issue，主要涉及的对象是两个测试用例。原因是这两个测试用例最近频繁失败，但由于开发人员没有足够的时间来进一步调查，暂时决定禁用它们，直到问题得以解决。

https://github.com/vllm-project/vllm/issues/12239
这是一个bug报告类型的issue，主要涉及了内核代码的修复。由于错误条件判断不正确导致了CUDA错误，用户提出了修改方案来解决这个问题。

https://github.com/vllm-project/vllm/issues/12238
这是一个文档更新类型的issue，涉及到FP8 KV Cache的文档细节更新。

https://github.com/vllm-project/vllm/issues/12237
这是一个bug报告类型的issue，主要涉及到使用`paligemma23bftdocci448`时生成结果仅在本地可以正常，原因可能是与TP参数有关。

https://github.com/vllm-project/vllm/issues/12236
这个issue类型是bug报告，涉及的主要对象是MoE kernel，由于误操作导致的需要删除的提交导致了对shape和tiling信息的错误追加。

https://github.com/vllm-project/vllm/issues/12235
这是一个用户提出需求的issue，主要涉及到Xformers库中的注意力机制，默认将后端设置为SDPA以适用于非GPU平台。

https://github.com/vllm-project/vllm/issues/12234
这个issue类型是用户提出需求，该问题涉及的主要对象是如何输入多条消息作为一批而不只是一条。由于无法成功输入一批大于1的消息，用户希望知道如何批量输入多条消息。

https://github.com/vllm-project/vllm/issues/12233
这是一个Bug报告类型的issue，涉及的主要对象是VLLM模型执行过程中出现的CUDA错误。该问题的出现是由于CUDA发生了非法内存访问而导致的错误。

https://github.com/vllm-project/vllm/issues/12232
这是一个修复CI配置问题的issue，主要对象是内部持续集成系统。原因可能是工作目录设置不正确导致CI测试失败。

https://github.com/vllm-project/vllm/issues/12231
这是一个功能改进类型的issue，涉及主要对象是代码中的find_loaded_library函数。这个问题的原因是为不同平台创建自定义版本的platform_aware_utils.py来覆盖特定平台相关的工具函数。

https://github.com/vllm-project/vllm/issues/12230
这是一个BugFix类型的issue，主要涉及到vLLM中的GGUF模型在多GPU上运行时出现张量大小不匹配的问题。

https://github.com/vllm-project/vllm/issues/12229
这是一个更新代码所有者和GitHub账户信息的问题，主要涉及GitHub代码库的管理和团队协作。由于团队成员更改GitHub账户和团队架构变动的原因，需要更新代码所有者信息。

https://github.com/vllm-project/vllm/issues/12228
这个issue类型为用户提出需求，关注的主要对象是代码中的quantization和guided decoding模块，用户请求添加这两者的CODEOWNERS。

https://github.com/vllm-project/vllm/issues/12227
这是一个bug报告，主要涉及更新平台检测以适应M系列arm架构的MacBook处理器所导致的错误。

https://github.com/vllm-project/vllm/issues/12226
这个issue类型是新模型添加请求，主要涉及的对象是DeepSeek R1模型。原因是用户想要vllm添加对DeepSeek R1模型的支持。

https://github.com/vllm-project/vllm/issues/12225
这个issue是关于bug报告，主要对象是vllm模型在使用`guided_choice`参数时无法按照要求限制输出。导致该情况的可能原因是模型未严格遵循提供的格式。

https://github.com/vllm-project/vllm/issues/12224
这是一个bug报告，涉及pre-commit脚本传递参数导致错误的问题。

https://github.com/vllm-project/vllm/issues/12223
这是一个bug报告，主要涉及修复CI中引入的问题，导致多模态测试无法通过。

https://github.com/vllm-project/vllm/issues/12222
这是一个用户提出需求的issue，主要涉及的对象是moe模型中的`moe_align_block_size`参数。由于当前`moe_align_block_size`在cuda图中不兼容并且在`num_experts`较大时效率较低，用户希望通过优化此参数来解决这些问题。

https://github.com/vllm-project/vllm/issues/12221
这是一个bug报告，主要涉及vllm中的context window导致LLM崩溃的问题。由于context window填满导致LLM崩溃，用户寻求解决这一bug的帮助。

https://github.com/vllm-project/vllm/issues/12220
这个issue是一个用户提出需求的类型，主要涉及的对象是VLLM库的功能扩展。由于SwiftKV cache compression技术带来的性能优势，用户建议将其合并到VLLM中。

https://github.com/vllm-project/vllm/issues/12219
这是一个bug报告类型的issue，主要涉及VLLM中模型架构未能被检查，可能由于模型输入导致数值错误。

https://github.com/vllm-project/vllm/issues/12218
该issue类型为功能改进，主要涉及到attention layer的自定义实现。可能是由于quantization attention方法无法直接使用参数，因此需要将attention对象传递给attention backend来解决这个问题。

https://github.com/vllm-project/vllm/issues/12217
这个issue是关于提出需求的，主要对象是开发人员。问题是由于开发人员在临时提交时需要通过linters可能会感到烦恼，不是每个人都知道如何使用`noverify`标志，所以添加了一个虚拟的hook来提示用户如何绕过钩子。

https://github.com/vllm-project/vllm/issues/12216
这是一个bug报告，涉及到vllm对于Paligemma2模型的BNB量化不支持的问题。原因可能是由于模型与vllm的集成问题导致。

https://github.com/vllm-project/vllm/issues/12215
这是一个用户提出需求的issue，主要涉及FP8 KV Cache功能支持的模型数量问题。用户疑问仅支持 Llama 2 是否可信，或许是因为文档未明确说明支持的模型范围所致。

https://github.com/vllm-project/vllm/issues/12214
这是一个类型为改进（enhancement）的issue，主要涉及到代码中的函数`_get_cache_block_size`，由于其被替换为`get_kv_cache_config`，因此需要将该函数进行移除操作。

https://github.com/vllm-project/vllm/issues/11861
这是一个用户提出需求的issue，主要涉及设置默认温度参数的问题。由于无法从服务器端修改默认值，用户希望了解如何设置默认温度以及其他参数。

https://github.com/vllm-project/vllm/issues/11860
这是一个特定内容为空的issue，类型为用户提出需求。该问题单主要涉及的对象是优化。

https://github.com/vllm-project/vllm/issues/11859
这是一个用户需求类型的issue，主要涉及到vllm的Online Inference使用上的问题，用户想知道如何进行多模态数据的在线推理操作。

https://github.com/vllm-project/vllm/issues/11858
这个issue是关于bug报告，主要涉及到在加载到多个GPU时出现的TypeError错误。原因是之前的Pickling问题已经在一个PR中修复，但加载到多个GPU时出现了新的错误。

https://github.com/vllm-project/vllm/issues/11857
这个issue类型是增加markdown代码检查，主要涉及的对象是CI/Build系统，由于需要排除特定规则而导致linting infractions较多。

https://github.com/vllm-project/vllm/issues/11856
这是一个用户提出需求的类型，主要涉及到支持Microsoft phi-4模型。用户提出问题的原因可能是想要使用该模型来训练或应用。

https://github.com/vllm-project/vllm/issues/11855
这是一个bug报告类型的issue，针对vLLM上运行json guided decoding时输出结果与预期不符的问题。

https://github.com/vllm-project/vllm/issues/11854
这是一个用户提出需求的issue，主要涉及支持使用Jinja模板来展示Llama3.3模型的信息。用户希望获取Llama3.3的聊天模板示例，并问询最佳推荐，可能由于缺乏相应的模板而导致了这个问题。

https://github.com/vllm-project/vllm/issues/11853
这是一个Bug报告类型的Issue，主要涉及到在使用PDM时，在M1/2/3/4芯片的Mac设备上，新的Darwin支持无法正常工作。这个问题可能是由于PDM项目新建后无法正确运行在特定硬件上所导致的。

https://github.com/vllm-project/vllm/issues/11852
这个issue类型是功能需求，主要对象是对多模式API参考文档的扩展。这是因为即将发布的教程需要引用这些API参考文档。

https://github.com/vllm-project/vllm/issues/11851
这是一个bug报告，该问题涉及到slot mapping计算中的start_idx，由于计算时未考虑块大小对齐，导致在下一个解码过程中出现未初始化的缓存数据被使用，需要进行修复。

https://github.com/vllm-project/vllm/issues/11850
这是一个bug报告，涉及的主要对象是CPU操作，由于CPU操作使用了相同的代码路径，导致了bug。

https://github.com/vllm-project/vllm/issues/11849
这是一个文档修复类的Issue， 主要涉及的对象是快速入门指南。原因可能是当前版本中的一些内容需要修复或更新。

https://github.com/vllm-project/vllm/issues/11848
该issue类型为代码优化，主要涉及的对象是移动模型工具到`vision.py`文件中。

https://github.com/vllm-project/vllm/issues/11847
这个issue类型是文档或注释的改进，涉及的主要对象是BlockHashType，用户提出需求补充更多关于该类型的解释。

https://github.com/vllm-project/vllm/issues/11846
这是一个Bug报告，涉及AsyncLLMEngine中的编码错误的问题。原因是出现了'NoneType' object has no attribute 'seed'的属性错误。

https://github.com/vllm-project/vllm/issues/11845
这是一个清理未使用代码的问题，主要对象是IPEx操作，由于更新后相关代码功能不再被使用。

https://github.com/vllm-project/vllm/issues/11844
这是一个用户提出需求和讨论功能实现的issue，涉及主要对象为实现了dualchunk flash attention和sparse attention support的PR。由于需要实现Qwen models的新功能和支持，才导致了这个issue的提出。

https://github.com/vllm-project/vllm/issues/11843
该issue类型为文档更新，涉及主要对象为API Reference页面的组织结构。

https://github.com/vllm-project/vllm/issues/11842
这是一个bug报告，主要涉及vllm中版本升级至0.6.6.post1后设置"limit-mm-per-prompt"时出现问题，导致CUDA错误和OOM错误的情况。

https://github.com/vllm-project/vllm/issues/11841
这个issue类型是用户提出需求，主要对象是比较两个分支。

https://github.com/vllm-project/vllm/issues/11840
该issue类型为文档修改，主要涉及到示例代码的分类整理。

https://github.com/vllm-project/vllm/issues/11839
这是一个bug报告，涉及到vllm中的draft model在GGUF模型中出现数值错误的问题。

https://github.com/vllm-project/vllm/issues/11838
这个issue类型是用户提出需求，询问关于构建Triton使用特定提交的原因，主要涉及VLLM Docker和Triton。

https://github.com/vllm-project/vllm/issues/11837
这是一个Bug报告，涉及对象是vllm库。由于vllm对象缺少'unified_attention'属性，导致了用户在使用时出现500错误并且服务器关闭的情况。

https://github.com/vllm-project/vllm/issues/11836
这个issue类型是bug报告，涉及的主要对象是CI/Build系统。原因是CPU CI镜像清理失败导致CI/Build流程受阻。

https://github.com/vllm-project/vllm/issues/11835
这是一个用户需求类型的issue，主要涉及vllm中使用'tqdm'特性进行beam_search方法时的限制。用户想要讨论如何在beam_search方法中使用'use_tqdm'特性。

https://github.com/vllm-project/vllm/issues/11834
这是一个bug报告，涉及vLLM服务器的聊天界面频繁重启的问题。原因可能是服务器出现故障或软件程序中存在错误。

https://github.com/vllm-project/vllm/issues/11833
这是一个文档更新类的issue，主要涉及赞助商名称的更改，由于赞助商名称需要更新为'Novita AI'。

https://github.com/vllm-project/vllm/issues/11832
这是一个Bug报告类型的Issue，主要涉及 Qwen-VL-Chat 在Docker 部署中持续出现 out of vocabulary (OOV) 错误。这可能是由于部署环境或配置问题导致的。

https://github.com/vllm-project/vllm/issues/11831
这个issue是用户提出的一个需求，主要涉及硬件和CPU，请求支持MOE模型在x86 CPU上运行。

https://github.com/vllm-project/vllm/issues/11830
这个issue类型是用户提出需求，涉及的主要对象是更新wheels的URL。原因可能是为了简化URL并提高用户体验。

https://github.com/vllm-project/vllm/issues/11829
该issue属于bug报告类型，主要涉及vllm中的BlockHashType碰撞问题，由于使用token ID减少了碰撞的可能性，但仍存在潜在的碰撞可能。

https://github.com/vllm-project/vllm/issues/11828
这是一个Bug报告，涉及到在使用`"response_format": { "type": "json_object" }`的情况下，调用`/v1/chat/completions`会导致模型中止的问题。

https://github.com/vllm-project/vllm/issues/11827
这是一个功能需求报告，主要涉及的对象是vllm api_server。由于存在可能导致服务异常的跨域设置冲突，用户提出需要添加一个设置来禁用跨域中间件。

https://github.com/vllm-project/vllm/issues/11826
这个issue类型是功能需求提出，主要对象是Intel Gaudi，由于需求为启用INC FP8支持，意在实现基于Intel® Neural Compressor (INC)工具包的FP8推理。

https://github.com/vllm-project/vllm/issues/11825
这个issue是一个bug报告，主要涉及的对象是RunAI Model Streamer。由于在Streaming过程中Pickling错误导致无法在多个GPU上使用RunAI Model Streamer，用户提出了这个问题。

https://github.com/vllm-project/vllm/issues/11824
这个issue属于代码优化类问题，主要涉及到代码中重复导入模块的修复。

https://github.com/vllm-project/vllm/issues/11823
这是一个bug报告，涉及到的主要对象是XPU模块。由于输入参数的顺序导致了调用错误，需要使用op包装器来解决这个问题。

https://github.com/vllm-project/vllm/issues/11822
该issue类型是用户提出需求，询问问题，该问题单涉及的主要对象是vLLM模型的集成与推断。由于用户不清楚如何将特定模型集成到vLLM中，因此提出需要关于集成的指导。

https://github.com/vllm-project/vllm/issues/11821
这是一个Bug报告，涉及主要对象为RobertaModel embeddings。由于RobertEmbeddingModel未正确处理参数名称匹配，导致load_weights()方法无法正常工作。

https://github.com/vllm-project/vllm/issues/11820
这是一个用户提出需求的类型的问题单，主要对象是在DeepSeek-V3中添加fused_moe配置。这个需求可能由于DeepSeek-V3模型需要具备更高的性能和灵活性而提出。

https://github.com/vllm-project/vllm/issues/11819
这是一个Bug报告，涉及Vllm中运行AI模型流处理器时在多于1个GPU上流式传输时出现Pickling错误的问题。造成这个问题的原因可能是在加载到2个GPU时出现了`Can't pickle : attribute lookup S3 on botocore.client failed`的错误。

https://github.com/vllm-project/vllm/issues/11818
这是一个bug报告类型的issue，涉及对象为使用vllm时出现的属性错误导致无法计算多模型标记数量的问题。原因是在代码中访问了LLMEngine对象中不存在的属性“workers”。

https://github.com/vllm-project/vllm/issues/11817
这是一个文档错误修正的issue，涉及到默认CUDA版本信息，由于12.1被错误地表示为12.4导致文档需要更新。

https://github.com/vllm-project/vllm/issues/11816
这是一个Bug报告，主要涉及PixtralHF准确率在版本更新后下降的问题，可能由于代码更新或环境变化引起。

https://github.com/vllm-project/vllm/issues/11815
这是一个bug报告，涉及的主要对象是代码中的`do_lower_case`参数。由于将配置文件中的`do_lower_case`值传递给tokenizer会导致tokenizer内部的大小写转换被禁用，从而触发一个bug，导致所有大写字符在tokenized后会被识别为`UNK`。

https://github.com/vllm-project/vllm/issues/11814
这是一个Bug报告，涉及的主要对象是在CPU ARM上缺少对BFloat16的检测，导致推理过程中出现崩溃。

https://github.com/vllm-project/vllm/issues/11813
这个issue类型是对功能的改进，主要涉及 torch profiler 的打印表排序问题。

https://github.com/vllm-project/vllm/issues/11812
这是一个功能优化的issue，主要涉及到VLM（Visual Language Model）中与多模态分析和处理相关的代码重组。由于优化了处理相关代码的结构和逻辑，使得代码更清晰、易维护。

https://github.com/vllm-project/vllm/issues/11811
这是一个bug报告类型的issue，涉及vLLM 0.6.6.post1的安装失败在TPU上，由于torch xla版本与Python 3.11不兼容导致。

https://github.com/vllm-project/vllm/issues/11810
这是一个bug报告，主要问题涉及LoRA与CPU offload之间的兼容性检查。原因是当前的LoRA不支持CPU offload，在启用LoRA时会导致CPU offload 失败。

https://github.com/vllm-project/vllm/issues/11809
这是一个bug报告，涉及主要对象是PyTorch内存监控功能。导致这个问题的原因是之前计算内存消耗时的方法不准确，导致无法正确衡量PyTorch的内存占用情况。

https://github.com/vllm-project/vllm/issues/11808
这是一个文档类型的issue，主要涉及`gte-Qwen2`模型，由于缺少注释导致需要修复相关链接问题。

https://github.com/vllm-project/vllm/issues/11807
这是一个bug报告类型的issue，主要涉及vllm的bnb prequantification在cls模型中的不完整支持问题，导致出现了'bn_shard_offsets'属性错误的bug。

https://github.com/vllm-project/vllm/issues/11806
这是一个文档更新类型的issue，主要涉及的对象是pip包管理工具。原因可能是用户需要了解如何使用pip安装nightly版本的软件包。

https://github.com/vllm-project/vllm/issues/11805
这是一个Bug报告类型的Issue，主要涉及到preemptmode和output structure，可能由于prefix caching和preemption之间的冲突导致了输出结构的不稳定性。

https://github.com/vllm-project/vllm/issues/11804
这个issue类型为改进建议，主要对象是scheduler.py文件中的重复导入的类型提示。原因是导致代码冗余且不符合最佳实践。

https://github.com/vllm-project/vllm/issues/11803
这是一个bug报告，涉及到模型无法运行的问题，出现这个问题可能是因为缺少特定的模型参数导致的。

https://github.com/vllm-project/vllm/issues/11802
该issue为bug报告，主要涉及Gem2ForSequenceClassification模型的权重未能从检查点中正确初始化导致的问题。

https://github.com/vllm-project/vllm/issues/11801
这是一个Bug报告。主要涉及的对象是Alibaba-NLP/gte-Qwen2-1.5B-instruct模型在vLLM和huggingface两种方法中嵌入数值差异。原因可能是模型参数或者数据处理方法不一致导致的。

https://github.com/vllm-project/vllm/issues/11800
该issue是一个bug报告，涉及LLaVANeXT feature size calculation的相关问题，由于未完全测试和修复导致症状发生。

https://github.com/vllm-project/vllm/issues/11799
这个issue类型为功能需求提出，主要涉及到llama3.3工具调用支持的问题，用户希望实现一个通用的、可扩展的llama工具调用支持。

https://github.com/vllm-project/vllm/issues/11798
该issue是一个文档更新类型的问题，涉及到V1版本支持`LLaVa-NeXT-Video`，由于CC([V1] Extend beyond image modality and support mixedmodality inference with LlavaOneVision)的合并，这个更新属于一次无缝升级。

https://github.com/vllm-project/vllm/issues/11797
这个issue类型是需求提出，主要涉及的对象是支持非可分割注意力头，由于模型的奇异形状导致无法在特定GPU节点上运行。

https://github.com/vllm-project/vllm/issues/11796
这是一个bug报告，主要涉及到更新run_cluster.sh脚本文件。可能是由于脚本中的某些错误导致了功能异常或者执行失败的问题。

https://github.com/vllm-project/vllm/issues/11795
这是一个bug报告，主要涉及到vllm项目中的qwen2模型中的前缀问题，导致部分量化模型在加载时出现KeyError错误。

https://github.com/vllm-project/vllm/issues/11794
这个issue是bug报告，涉及到CPU性能下降问题，由于未支持multistep调度导致。

https://github.com/vllm-project/vllm/issues/11793
这个issue是关于bug报告，主要涉及Qwen/QwQ-32B-Preview模型中的`tool_calls`功能问题，用户反映`tool_calls`返回为空，疑问是该问题是vllm的问题还是模型能力限制。

https://github.com/vllm-project/vllm/issues/11792
这是一个Bug报告，主要涉及VLLM项目中的模型运行出现的错误，原因是尝试将多个词汇标记分配给不足的位置。

https://github.com/vllm-project/vllm/issues/11791
这是一个性能优化的issue，主要涉及到前端的预填解码功能，由于使用ZMQ替代HTTP通信和实现持久ZMQ连接，导致预填解码分解性能显著提升。

https://github.com/vllm-project/vllm/issues/11790
这是一个bug报告，涉及的主要对象是在加载部分量化模型时出现KeyError 'layers.0.self_attn.qkv_proj.weight'，可能是由于部分quantized模型的MLP层被量化而注意力层未被量化导致的。

https://github.com/vllm-project/vllm/issues/11789
该issue类型为功能需求提议，主要涉及对象为VLLM Connect服务，用户希望通过使用zmq来改善预填充和解码阶段的性能分离。

https://github.com/vllm-project/vllm/issues/11788
该issue类型为功能需求，涉及的主要对象是docker image构建过程。由于并发构建docker images所需的构建号未被添加到docker image名称中，导致无法在同一服务器上并行构建docker images。

https://github.com/vllm-project/vllm/issues/11787
这是一个bug报告，主要涉及了ModelOpt模型加载kv scales的问题，由于NeuralMagic和ModelOpt对kv scales的存储方式不同导致了加载问题。

https://github.com/vllm-project/vllm/issues/11786
这是一个Bug报告，涉及到 llama3.211bvisioninstruct 无法返回工具调用的问题。原因可能是在某些情况下，llama3.211bvisioninstruct 无法返回工具调用，导致 resp.choices[0].message.tool_calls 为null。

https://github.com/vllm-project/vllm/issues/11785
这是一个功能需求的issue，主要涉及支持TPU上的W8A8模型，原因是提出了关于TPU上压缩张量W8A8模型的使用支持和相关测试的需求。

https://github.com/vllm-project/vllm/issues/11784
这是一个bug报告，发生在VLLM中的Whisper模块，由于Attention接口更新导致CI失败。

https://github.com/vllm-project/vllm/issues/11783
该issue是关于Bug报告，主要涉及Continuing a generation不是确定性的问题。这个问题由于确定性方面的问题导致了不同的结果，影响到整个生成的顺序和结果。

https://github.com/vllm-project/vllm/issues/11782
这是一个用户提出需求的类型，主要涉及文档结构的调整和分类优化。这个issue主要是为了优化文档结构和提升用户体验。

https://github.com/vllm-project/vllm/issues/11781
这是一个Bug报告，主要涉及base64字符串在最新的vLLM服务器和pixtral-12b中导致乱码的问题。

https://github.com/vllm-project/vllm/issues/11780
该issue属于功能增强类型，涉及主要对象为在vLLM仓库中添加Google T5模型支持。

https://github.com/vllm-project/vllm/issues/11779
这是一个bug报告，主要涉及Neuron后端的持续集成（CI），由于之前Neuron后端没有被有效测试导致的问题。

https://github.com/vllm-project/vllm/issues/11778
这是一个Bug报告，涉及到VLLM库中的prompt logprobs在batch大小大于1时与batch大小等于1时不一致的问题。原因可能是在处理批处理模式时计算过程中出现了错误。

https://github.com/vllm-project/vllm/issues/11777
该issue属于功能需求类型，主要涉及vLLM构建流程的优化。造成此需求的原因是为了加快构建速度和减小基础镜像的体积。

https://github.com/vllm-project/vllm/issues/11776
这是一个与代码更新相关的问题，涉及对象为 Qwen2-Audio multi-modal processor。原因是为了预防在发布时被 https://github.com/huggingface/transformers/pull/35534 所影响，作者进行了本地测试并更新了处理器版本。

https://github.com/vllm-project/vllm/issues/11775
这是一个Bug报告，主要涉及的对象是vllm的数据并行在lighteval中无法正常工作。原因可能是与模型或GPU相关，导致data_parallel_size=4/2参数无法正确运行。

https://github.com/vllm-project/vllm/issues/11774
这是一个关于使用vllm和OpenAI Swarm框架的问题，类型为Bug报告。用户报告在使用vllm主机模型时遇到了BadRequestError code: 400错误，并询问vllm是否与OpenAI Swarm框架兼容。

https://github.com/vllm-project/vllm/issues/11773
这个issue类型是文档补充请求，涉及主要对象是vllm下的uv模块。由于缺乏详细的使用说明文档，用户提出需要添加关于如何使用uv模块的文档说明。

https://github.com/vllm-project/vllm/issues/11772
这是一个关于bug修复的issue，主要涉及LLaVA-NeXT功能尺寸精度错误，可能导致功能无法正常工作。

https://github.com/vllm-project/vllm/issues/11771
这是一个需求类型的issue，主要涉及添加interleaving sliding window支持，避免类似之前发生的问题。

https://github.com/vllm-project/vllm/issues/11769
该issue是一个bug报告，涉及到LLaVA-Onevision的动态分辨率支持问题。造成该bug的原因是 profiling info 错误地假定分辨率是静态的，导致需要添加一个函数来获取最大可能的ImageSize来进行 profiling。

https://github.com/vllm-project/vllm/issues/11768
这个issue是一个bug报告，涉及主要对象是vllm/distributed/device_communicators/shm_broadcast.py文件。由于`buffer_handle`参数没有在`self.buffer`未初始化的情况下设置为`None`，可能导致初始化错误问题。

https://github.com/vllm-project/vllm/issues/11767
这是一个关于性能问题的bug报告，主要涉及Qwen2-VL-72B-Instruct-GPTQ-Int4模型在A100机器上运行速度慢的问题。这个问题可能是由于从图片提取数据时，即使设置了chuncked_prefill=True和batched_tokens=max_tokens，仍然需要花费190秒来处理单个图片。

https://github.com/vllm-project/vllm/issues/11766
这是一个文档更新类的issue，主要涉及vLLM文档中Serving部分的重新组织。

https://github.com/vllm-project/vllm/issues/11765
这个issue是关于bug报告的，主要涉及的对象是vllm下的finetuned llama3.2 vision instruct model，在finetuning过程中出现了weights shape assertion的失败。

https://github.com/vllm-project/vllm/issues/11764
这是一个bug报告，涉及的主要对象是在TPU上运行MoE模型时出现的问题。原因是在TPU上加载或编译Mixtral8x7BInstructv0.1时遇到了困难，导致无法正常工作。

https://github.com/vllm-project/vllm/issues/11763
这个issue是一个bug报告，主要涉及Cutlass 2:4 Sparsity + FP8/Int8 Quant模型运行时发生了RuntimeError，可能由于环境配置或参数设置错误导致的。

https://github.com/vllm-project/vllm/issues/11762
这个issue是一个Bug报告，涉及主要对象是应用程序无法成功应用Qwen2VLProcessor。由于用户当前环境可能存在问题，导致程序无法正常运行。

https://github.com/vllm-project/vllm/issues/11761
这是一个bug报告，涉及的主要对象是加载LoRA模块后调用v1/models方法未返回该模块的结果。由于加载LoRA模块后v1/models结果不包含该模块，导致该问题的症状。

https://github.com/vllm-project/vllm/issues/11760
这是一个bug报告，主要涉及的对象是vllm在GPU数据传输时出现异常高的性能表现。

https://github.com/vllm-project/vllm/issues/11758
该issue是关于bug报告，涉及的主要对象是NGramWorker类，由于代码中的NGramWorker不支持缓存操作，导致用户提出了需支持缓存操作的问题。

https://github.com/vllm-project/vllm/issues/11756
这是一个Bug报告，主要涉及Cutlass 2:4 Sparsity + FP8/Int8 Quant功能，由于某些原因导致Runtime Error。

https://github.com/vllm-project/vllm/issues/11755
这个issue类型是文档重组，主要涉及到vllm项目中各个文档部分的整理和重命名。

https://github.com/vllm-project/vllm/issues/11754
这是一个跟踪issue类型的提出需求，主要涉及get_executor_cls的重构，该问题可能是由于之前的Pull Request引发的问题而导致新的修改需求。

https://github.com/vllm-project/vllm/issues/11753
这是一个修正拼写错误的issue，主要涉及valid_tool_parses对象。由于拼写错误导致了代码中某些功能无法正常解析，需要修复该错误以保证正常运行。

https://github.com/vllm-project/vllm/issues/11752
这是一个bug报告类型的issue，涉及的主要对象是前端的`StreamingResponse`异常处理，由于只捕获了`ValueError`导致在流式处理中发生`RuntimeErrors`时出现长堆栈跟踪。

https://github.com/vllm-project/vllm/issues/11751
这是一个文档修复类型的issue，主要涉及文档中单词拼写错误的修正。原因是代码注释或文档中包含了拼写错误，导致用户阅读时可能会产生困惑。

https://github.com/vllm-project/vllm/issues/11750
这是一个优化类型的issue，主要涉及到Python函数调用的优化，旨在降低额外开销。

https://github.com/vllm-project/vllm/issues/11749
这是一个优化代码的issue，主要对象为PyNcclCommunicator，通过消除change_state函数的冗余来提高代码性能。

https://github.com/vllm-project/vllm/issues/11748
这是一个Bug报告，涉及的主要对象是CPU Offload功能。这个问题可能由于`enable_lora=True`设置导致CPU Offload失败，进而引发错误。

https://github.com/vllm-project/vllm/issues/11747
这是一个bug报告，主要涉及vllm中cross_device_reduce_1stage中多余的multi_gpu_barrier导致讨论和建议删除其中一个的问题。

https://github.com/vllm-project/vllm/issues/11746
这是一个功能增强（Enhancement）类型的Issue，主要涉及VLM库中的多模态处理器类（multimodal processor class）。通过将与性能分析相关的逻辑和通用功能分离为新的类和函数，以提高代码结构和可维护性。

https://github.com/vllm-project/vllm/issues/11745
这是一个关于性能问题的建议报告，用户提出了VLLM在上下文长度方面存在问题，导致内存不足错误。

https://github.com/vllm-project/vllm/issues/11744
这是一个优化代码的issue，涉及PyNcclCommunicator对象。由于冗余的流初始化和赋值操作被移除，导致了代码的简化和提升性能。

https://github.com/vllm-project/vllm/issues/11743
这是一个功能需求提出的issue，主要涉及vllm项目中的sleep mode功能。由于未释放cudagraph内存池以及其他成本，导致sleep后未达到预期的内存释放量。

https://github.com/vllm-project/vllm/issues/11742
这是一个bug报告，主要涉及了CogAgent model在vLLM中的支持问题，并且由于预期输出与实际输出不一致而导致了问题。

https://github.com/vllm-project/vllm/issues/11741
这是一个bug报告，主要涉及Pixtral-HF的图像输入问题，由于11396版本导致PixtralHF推断功能失效。

https://github.com/vllm-project/vllm/issues/11740
这个issue类型是需求提出，主要涉及的对象是在PyTorch的子类中为forward函数添加mypy类型提示。此需求是为了在mypy类型检查中能够正确地识别forward函数的返回类型。

https://github.com/vllm-project/vllm/issues/11739
这是一个关于使用VLLM时出现的bug报告，主要涉及到平台插件激活问题。由于同时激活了CPU和CUDA平台插件导致出现了RuntimeError错误。

https://github.com/vllm-project/vllm/issues/11738
这是一个关于bugfix的issue，涉及主要对象为test_kv_cache_utils.py文件，由于合并代码未同步导致的问题。

https://github.com/vllm-project/vllm/issues/11737
这是一个bug报告类型的issue，主要涉及到改进后台进程关闭和日志记录问题。问题是由于之前错误处理机制导致在遇到错误时无法正确关闭进程和返回适当的HTTP状态码，特别是在启动过程中遇到错误时的处理不当，以及在使用信号捕获错误时的一些限制导致无法在主线程之外运行。

https://github.com/vllm-project/vllm/issues/11736
这个issue属于优化建议类别，主要涉及模型的权重初始化逻辑。原因是权重将被模型的检查点所覆盖，所以不需要自定义初始化。

https://github.com/vllm-project/vllm/issues/11735
这是一个bug报告，涉及到LLaVA-NeXT的特征大小计算精度问题，可能由于使用Python float64导致精度错误，而使用float32可能解决问题。

https://github.com/vllm-project/vllm/issues/11734
这个issue是关于安装问题，涉及到vllm的XPU依赖未与最新的OneAPI版本兼容。造成这个问题的原因是OneAPI的版本不匹配，导致构建错误。

https://github.com/vllm-project/vllm/issues/11733
这是一个功能需求提出的issue，主要涉及到V1版本的音频语言模型支持，但由于缺少预处理步骤，需要在另一个相关的PR合并后再进行合并。

https://github.com/vllm-project/vllm/issues/11732
这是一个修复Dockerfile.openvino构建的bug报告，涉及到VLLM项目中的OpenVINO模块。由于之前的修复方法更为复杂，用户提出了一种更简单的解决方案。

https://github.com/vllm-project/vllm/issues/11731
这是一个用户提出需求的issue，主要涉及的对象是关于服务名为'LLaVA-Next-Video-7B-Qwen2'的服务。 由于给出的文本内容不清晰，无法明确问题的具体原因。

https://github.com/vllm-project/vllm/issues/11730
这是一个建议性质的issue，提议替换PyTorch中使用的c10::optional为std::optional。这个提议的目的是为了消除c10::optional的使用，推荐使用std::optional。

https://github.com/vllm-project/vllm/issues/11729
这个issue类型是用户提出需求，主要对象是vLLM是否支持在同一服务器上托管多个llm base模型。这个问题是由于用户希望从成本节约的角度考虑，了解vLLM是否有计划支持此功能而提出的。

https://github.com/vllm-project/vllm/issues/11728
这个issue属于用户提出需求类型，主要涉及vLLM为GH200发布Arm镜像的问题。用户提出该需求是因为目前用户需要自行构建和发布镜像，对用户体验不友好。

https://github.com/vllm-project/vllm/issues/11727
这是一个Bug报告，主要涉及Lora适配器的加载验证问题，导致服务器崩溃并关闭。

https://github.com/vllm-project/vllm/issues/11726
这是一个Bug报告类型的Issue，主要对象是PixtralHF模型；由于一个提交引入的回归，导致PixtralHF模型在给定非`[1024x1024]`大小图片或多张图片时出错。

https://github.com/vllm-project/vllm/issues/11725
这是一个用户提出需求的类型的issue，主要涉及到对新模型unsloth/Llama-3.3-70B-Instruct-bnb-4bit的支持。原因可能是用户希望vllm支持该模型以提高推理速度。

https://github.com/vllm-project/vllm/issues/11724
这个issue类型是一个清理无用代码的任务，主要对象是代码库中的冗余代码。

https://github.com/vllm-project/vllm/issues/11723
这个issue属于bug报告类型，主要涉及的对象是vllm中的block size constraint。由于CC引入的block size限制，需要移除该限制，因为Flash Attention等其他注意力模块不受此限制，但Page Attention kernel有这个限制。

https://github.com/vllm-project/vllm/issues/11722
这是一个缺乏内容的 issue，类型是其它，主要对象是测试多模型功能的扩展。

https://github.com/vllm-project/vllm/issues/11721
这个issue属于bug报告类型，主要涉及V1版本中TP>1错误处理和堆栈跟踪改进，主要原因是当前VLLM在处理TP>1时出现启动错误或运行时错误导致程序hang或未正确清理资源。

https://github.com/vllm-project/vllm/issues/11720
这个issue属于用户需求提出类型，主要涉及到vLLM中的CPU绑定问题，用户希望vLLM可以将内存绑定到与CPU列表相关的所有NUMA节点而不仅仅是与列表中第一个CPU相关的节点。

https://github.com/vllm-project/vllm/issues/11719
这是一个修复链接问题的issue，主要涉及BNB量化对Qwen2VL的添加。

https://github.com/vllm-project/vllm/issues/11718
这个issue类型为功能改进，涉及的主要对象是更新文档中的例子。

https://github.com/vllm-project/vllm/issues/11717
这是一个用户提出需求的issue，主要涉及VLM下的多模态处理器的合并实现。这个问题是由于需要将固定图像大小的测试从e2e移动到处理器测试，并在处理视频输入时对其进行正确的分析而做出的修改。

https://github.com/vllm-project/vllm/issues/11716
这是一个bug报告类型的issue，涉及的主要对象是lbx modify功能。由于对lbx modify功能的修改，导致出现了某种症状的bug。

https://github.com/vllm-project/vllm/issues/11715
这是一个bug报告，涉及到vLLM下的一个模型部署问题，由于模型架构无法被检查而导致部署出错。

https://github.com/vllm-project/vllm/issues/11714
这是一个bug报告，涉及的主要对象是VLLM中的LoRA模型。由于Lora base_model.model.lm_head.base_layer.weight不被支持，导致需要支持lm_head和embed_tokens在LoRA中完全训练。

https://github.com/vllm-project/vllm/issues/11713
这是一个用户提出需求的issue，主要对象是前端功能。由于未指定prefix和suffix，可能导致生成的tokens不符合预期。

https://github.com/vllm-project/vllm/issues/11712
这个issue类型为用户提出需求，主要涉及的对象是为`AsyncLLM`（api server）添加`RayExecutor`支持。

https://github.com/vllm-project/vllm/issues/11711
这是一个Bug报告类型的Issue，主要涉及对象是VLLM中的Qwen2VLProcessor，由于升级到VLLM 0.6.6版本后，导致无法支持qwen2vl的Lora适配器，出现了AttributeError错误。

https://github.com/vllm-project/vllm/issues/11710
这是一个bug报告，该问题单涉及的主要对象是性能基准测试（perf-benchmark），由于步骤在基准测试管道中的依赖关系错误，导致各个基准步骤未正确依赖于"Wa博为何总是这样？it for container image step"，可能导致性能基准测试结果不准确。

https://github.com/vllm-project/vllm/issues/11709
这是一个bug报告，主要涉及的对象是vllm中多模态输入的形状不匹配问题，可能是由于数值张量形状不一致导致的运行时错误。

https://github.com/vllm-project/vllm/issues/11708
这是一个Bug报告，涉及的主要对象是代码文件"vllm/lora/layers.py"。这个问题是由于错误的属性引用引起的。

https://github.com/vllm-project/vllm/issues/11707
这是一个Bug报告，涉及到vllm/lora/layers.py文件中的attribute reference self.output_dim应该被更改为self.output_size。这个问题导致了代码中的错误属性引用。

https://github.com/vllm-project/vllm/issues/11705
这是一个bug报告，主要涉及vLLM部署在16个H100 GPU上推理速度极慢的问题，可能是由NCCL启用和部署命令导致的。

https://github.com/vllm-project/vllm/issues/11704
这是一个Bug报告，主要涉及LLava-1.6-Mistral-7B模型在推断多模态数据时出现了占位符不匹配的问题。由于多模态tokens与占位符数量不符导致数值分配错误，进而引发数值赋值错误的异常情况。

https://github.com/vllm-project/vllm/issues/11703
这是一个bug报告，涉及到在marlin_utils.py中出现崩溃，可能是由于CUDA版本不匹配导致的问题。

https://github.com/vllm-project/vllm/issues/11702
这是一个bug报告，主要涉及到vLLM LoRA在使用动态加载时崩溃的问题，可能由于环境设置不当导致。

https://github.com/vllm-project/vllm/issues/11701
这是一个bug报告，主要对象是缺少一个文件tool_chat_template_llama3_json.jinja，导致需要更新为tool_chat_template_llama3.1_json.jinja。

https://github.com/vllm-project/vllm/issues/11700
这是一个用户提出功能需求的issue，主要涉及tool_choice字段，请求支持required类型，导致症状为目前不支持该类型。

https://github.com/vllm-project/vllm/issues/11699
这是一个功能改进和性能优化的issue，主要涉及到PD Disagg Performance enhance & benchmark tool的更新，主要关注提高预填充延迟和改进基准测试工具。

https://github.com/vllm-project/vllm/issues/11698
这是一个优化建议类型的issue，主要涉及到对Triton中使用的块大小启发式的调整，旨在提高性能。

https://github.com/vllm-project/vllm/issues/11697
这是一个功能需求的问题，主要对象是benchmark scripts。该问题由于detokenization耗时较长，且并非始终需要，因此用户希望在benchmark scripts中增加选择性地包含或排除detokenization的功能。

https://github.com/vllm-project/vllm/issues/11696
这是一个特性需求的issue，涉及的主要对象是在Apple芯片的Mac设备上构建和运行VLLM的问题。由于VLLM需要进行一些ARM/CPU构建脚本的调整，才能在macOS上的Apple芯片设备上正常运行。

https://github.com/vllm-project/vllm/issues/11695
这是一个需求更新类的issue，主要涉及更新支持Python 3.9和3.11的requirements-tpu.txt文件，同时建议添加一个阻止表格尺寸警告的功能。

https://github.com/vllm-project/vllm/issues/11694
这是一个文档更新类型的issue，主要涉及的对象是用于chunked prefill的默认 max_num_batch_tokens 参数。原因可能是为了准确记录新的默认值以提高性能。

https://github.com/vllm-project/vllm/issues/11693
这是一个用户提出需求的issue，主要涉及的对象是新增的`BlockTable`类。在此之前，为了降低输入准备逻辑的复杂性，需要优化从CPU到GPU的块表复制。

https://github.com/vllm-project/vllm/issues/11692
这是一个bug报告，主要涉及的对象是`token_ids_cpu`。这个问题是由于没有考虑实际长度在复制`token_ids_cpu`的行时导致的。

https://github.com/vllm-project/vllm/issues/11691
这是一个用户提出需求的issue，主要涉及对象是Tokenize Endpoint。这个问题由于用户想要更好地控制tokenization以避免提示注入。

https://github.com/vllm-project/vllm/issues/11690
这个issue是关于代码优化和功能改进的， 主要涉及对象是Attention类。由于需要从Attention类获取attn_type参数，所以移动了attn_type到Attention.__init__()中。

https://github.com/vllm-project/vllm/issues/11689
这个issue是关于文档错误的bug报告，涉及主要对象为`vllm.EngineArgs`。原因是参数名称错误导致文档不准确。

https://github.com/vllm-project/vllm/issues/11688
这是一个bug报告，涉及到kv缓存的缩放因子参数设置，导致精度问题。

https://github.com/vllm-project/vllm/issues/11687
这是一个关于功能需求的issue，主要涉及Qwen2VL的模型对BitsAndBytes量化的支持问题。原因可能是目前的模型暂不支持BitsAndBytes量化。

https://github.com/vllm-project/vllm/issues/11686
这是一个关于Bug报告的issue，主要涉及vLLM输出信息误发送到错误流导致用户无法正常解析输出的问题。

https://github.com/vllm-project/vllm/issues/11685
该issue属于用户提出需求类型，主要涉及V1模块的扩展，支持多模态推断并添加LlavaOneVision支持。这个问题的提出可能是为了扩大模型的支持范围，使其能够处理融合输入处理器的模型以及混合模态推理。

https://github.com/vllm-project/vllm/issues/11684
这个issue是关于bug修复的，主要涉及到了输入嵌入支持的问题。原因是需要在LLM.generate函数中添加对prompt_embeds的支持，以实现只微调嵌入层的用例。

https://github.com/vllm-project/vllm/issues/11683
这是一个bug报告，主要涉及的对象是vllm库。由于v0.6.6版本存在与cv2相关的问题，导致在import vllm时出现错误。

https://github.com/vllm-project/vllm/issues/11682
这个issue类型是功能更新，涉及主要对象是VLM下的LLaVA-NeXT项目。

https://github.com/vllm-project/vllm/issues/11681
这个issue是关于bug报告，主要涉及添加codeshell 7b模型时出现乱码的问题。原因可能是环境设置导致输出显示乱码。

https://github.com/vllm-project/vllm/issues/11680
这个Issue属于Bug报告，涉及主要对象是在vllm/inputs中的mypy类型检查。原因是`assert_never`行被忽略，导致无法通过mypy类型检查。

https://github.com/vllm-project/vllm/issues/11679
该issue类型是功能改进建议，主要涉及k8s-config中的secret对象，并建议使用`stringData`替代`data`来简化文档说明。

https://github.com/vllm-project/vllm/issues/11678
这是一个Bug报告，涉及对象为Qwen/QVQ-72B-Preview模块，由于shape mismatch导致了crash。

https://github.com/vllm-project/vllm/issues/11677
这是一个技术改进的issue，主要涉及的对象是torch.compile中的KV cache。导致这个问题的原因是为了支持隐藏持续批处理复杂性和混合内存分配器。

https://github.com/vllm-project/vllm/issues/11676
这是一个类型为bug报告的issue，主要涉及到Molmo的文本输入。由于Molmo文本输入存在问题，导致用户无法正确输入文字内容。

https://github.com/vllm-project/vllm/issues/11675
这个issue属于技术需求类型，主要涉及到Mypy worker的支持问题。这个问题可能是由于需求增加或者功能改进而提出。

https://github.com/vllm-project/vllm/issues/11674
这是一个bug报告，主要涉及的对象是V1中使用合并的多模态处理器的模型，由于多模态项目的重命名和引入了新的包装类导致出现了多模态输入失败的问题。

https://github.com/vllm-project/vllm/issues/11673
这个issue类型是bug报告，主要对象涉及到代码中的chain_speculative_sampling。该bug由于没有检查chain_speculative_sampling是否为None导致了错误报告。

https://github.com/vllm-project/vllm/issues/11672
这是一个Bug报告类型的issue，主要涉及vllm下的EAGLE模型性能问题，用户希望解决Eagle性能低于预期的情况。

https://github.com/vllm-project/vllm/issues/11671
这是一个bug报告，涉及的主要对象是通过AWQ和GPTQ方法对模型进行量化后，在vLLM0.6.6中推断时出现的输出大小与量化权重形状不匹配的问题。

https://github.com/vllm-project/vllm/issues/11670
这个issue属于bug报告，主要涉及OpenVINO Dockerfile，由于从Ubuntu 24.04更新导致构建失败。

https://github.com/vllm-project/vllm/issues/11669
这个issue类型是功能改进，该问题单涉及的主要对象是VLM（Visual Language Model），由于需要确保最大占位符数量和最大项目数之间的一致性，因此将相关代码移到合并的多模处理器中，同时对Aria模型文件进行了类型注释和删除了一些冗余代码。

https://github.com/vllm-project/vllm/issues/11668
这个issue是一个功能需求，主要涉及到Qwen2-VL模型的V1支持，其中涉及到动态维度处理、dummy数据获取、MRoPE支持等内容。

https://github.com/vllm-project/vllm/issues/11667
这是一个bug报告，主要涉及文件名中的空格替换。这个问题由于文件名中存在空格导致不符合代码库规范，需要将空格替换为破折号以解决这一问题。

https://github.com/vllm-project/vllm/issues/11666
这是一个文档错误纠正的issue，涉及的主要对象是代码中的`FlashInfer`拼写问题，可能由于拼写错误导致功能无法正常使用。

https://github.com/vllm-project/vllm/issues/11664
这是一个bug报告，主要涉及的对象是关于nvidia GPU上的kv caching scaling factor参数问题。原因是在FP16模式下，kv缓存的scaling factors不能通过`quantizationparampath`加载，导致准确性下降。

https://github.com/vllm-project/vllm/issues/11663
该issue属于用户提出需求类型，主要涉及QWenVL LoRA模型的优化和对视觉模块及投影模块的LoRA支持。

https://github.com/vllm-project/vllm/issues/11662
这个issue属于用户提出需求类型，主要涉及到设备与rank的映射，由于不同系统的设备之间的通信路由不同，因此需要引入`VLLM_LOCAL_RANK_DEV_MAP`来更灵活地分配设备与rank的关系。

https://github.com/vllm-project/vllm/issues/11661
该issue类型是bug报告，涉及到VLM下的数据解析器，主要是为了解决处理过程中可能出现的混乱错误。

https://github.com/vllm-project/vllm/issues/11660
这是一个Bug报告，涉及的主要对象是OpenAI server的模型管理。导致该问题的原因是当前各个独立的`OpenAI*`处理程序类各自管理自己的模型，但在加载或卸载适配器时，所有处理程序都必须保持同步，这会导致拉贝适配器通过`/v1/load_lora_adapter`端点加载后无法显示在`/v1/models`响应中。

https://github.com/vllm-project/vllm/issues/11659
这个issue是关于优化程序关闭流程，非bug报告。涉及的主要对象是实现后台进程管理或进程间通信的对象。原因是为了解决当LLM被清除时产生奇怪日志的问题，并且简化LLM、LLMEngine或AsyncLLM等高级类在关闭时无需特殊处理。

https://github.com/vllm-project/vllm/issues/11658
这个issue类型是bug报告，涉及连续批处理（OpenAI服务器）使用贪婪搜索返回不同结果。可能是由于连续批处理的关键bug导致每次在相同数据上运行推断时结果差异很大。

https://github.com/vllm-project/vllm/issues/11657
这是一个Bug报告，问题涉及到vllm中qwen2vl-7b视频处理服务的推断过程中遇到的错误。原因是提示信息（总长度43698）过长，无法适应模型（上下文长度32768），导致BadRequestError。

https://github.com/vllm-project/vllm/issues/11656
这个issue属于bug报告，主要涉及的对象是MultiprocExecutor，由于未记录GPU blocks数量所致。

https://github.com/vllm-project/vllm/issues/11655
这个issue是用户提出的需求，主要对象是VLLM的支持情况，用户提出需要支持8位的Inflight quantization以提高速度。

https://github.com/vllm-project/vllm/issues/11654
这是一个Bug报告，主要涉及对象是请求`/v1/completions`时发生的错误500。问题可能由于TensorFlow和PyTorch组件之间的不兼容性导致的插件注册问题而引起。

https://github.com/vllm-project/vllm/issues/11653
这是一个bug报告issue，主要涉及vllm==0.6.4版本下的memory_efficient_attention_forward操作未找到导致的NotImplementedError异常。

https://github.com/vllm-project/vllm/issues/11652
这个issue为用户提出需求类型，主要涉及对象是vllm下的qwen2vl-7b服务。导致出现问题的原因是用户不知道如何在vllm中集成特定模型进行推理。

https://github.com/vllm-project/vllm/issues/11651
这是一个bug报告，涉及的主要对象是多节点推理（multinode inference）功能。原因可能是预期16个节点，但实际只有1个节点，导致了"Expected 16, got 1.0"的断言错误。

https://github.com/vllm-project/vllm/issues/11650
这是一个bug报告，涉及主要对象是支持的模型类型，由于不支持所需模型类型导致了数值错误。

https://github.com/vllm-project/vllm/issues/11649
这个issue类型为性能优化提议，主要涉及V1版本与V0版本之间的性能对比。由于多步骤处理的原因，V0在单个请求中运行更快。

https://github.com/vllm-project/vllm/issues/11648
这是一个bug报告，主要涉及XPU设备上的初始化问题，导致了Point-to-point通信无法支持。

https://github.com/vllm-project/vllm/issues/11647
这是一个bug报告，涉及的主要对象是vllm模型架构中的['Gemma2ForSequenceClassification']，由于无法支持该模型架构，用户提出了关于如何使用as_classification_model的问题。

https://github.com/vllm-project/vllm/issues/11646
这个issue属于优化建议，主要涉及视觉模块哈希的简化，由于之前的hash格式可能导致混淆，提出了更简洁的方案。

https://github.com/vllm-project/vllm/issues/11645
该issue是文档重组类型，涉及主要对象是项目的安装指南和文档结构。原因是为了提高导航栏空间利用率，明确硬件平台，并提高FAQ的可见性。

https://github.com/vllm-project/vllm/issues/11644
这是一个Bug报告，涉及到模块导入的问题，由于缺少特定模块 `vllm.transformers_utils.configs.dbrx` 导致了 `ModuleNotFoundError`。

https://github.com/vllm-project/vllm/issues/11643
这是一个Bug报告，涉及主要对象是vllm==0.6.5以及GLM4-9b-chat，由于找不到-lcuda导致了编译错误。

https://github.com/vllm-project/vllm/issues/11641
这是一个Bug报告，问题涉及LLM模型对hessian矩阵取cholesky分解时出现数值不稳定导致ValueError异常。

https://github.com/vllm-project/vllm/issues/11640
这个issue类型是bug报告，涉及的主要对象是vllm版本0.6.3.post1，由于传入video_url参数时出现Unknown part type video_url错误消息。

https://github.com/vllm-project/vllm/issues/11639
该issue属于用户提出需求类型，主要涉及对赞助页面的重新组织。原因可能是要添加赞助商级别和推荐，并突出Slack赞助 Anyscale 的部分。

https://github.com/vllm-project/vllm/issues/11638
这是一个用户提出需求的issue，主要涉及到RL后训练工作负载，提出了避免KV缓存和转移模型权重的功能需求。

https://github.com/vllm-project/vllm/issues/11637
这是一个bug报告，主要涉及到OpenAI的并行采样中使用xgrammar时出现的问题。导致这个bug的原因是xgrammar logit processor具有每个请求的内部状态，因此在并行采样时需要对处理器进行深层复制，但在OpenAI API中未正确处理这一问题，因此需要修复这一问题来确保正确的处理。

https://github.com/vllm-project/vllm/issues/11636
这个issue属于性能问题报告，涉及到API Server中的多进程解标记功能，由于添加第三个进程影响性能，决定采用仅包含两个进程的架构。

https://github.com/vllm-project/vllm/issues/11635
这个issue类型属于功能需求，主要涉及的对象是Cascade Attention的实现。原因是为了节省HBM带宽，当请求共享相同前缀时，使用Cascade Attention。

https://github.com/vllm-project/vllm/issues/11634
这是一个用户提出需求的issue，主要涉及V1 engine中如何获取logprobs的问题。由于V1 engine可能不支持logprobs参数，用户希望了解是否有其他方法可以获取logprobs。

https://github.com/vllm-project/vllm/issues/11633
这个issue属于 bug 报告，主要涉及的对象是 vllm 安装过程。由于 Docker Hub 上没有 v0.6.6.post1 版本的 vllm 镜像，导致无法安装。

https://github.com/vllm-project/vllm/issues/11632
这是一个功能新增的issue，主要涉及V1支持的单图模型。由于代码更改较大，需要进行审查。

https://github.com/vllm-project/vllm/issues/11631
这是一个bug报告，主要对象是MiniCPMVBaseModel中的图像嵌入操作。由于未对图像嵌入进行正确的列表转换，导致在进行批量推理时出现维度不匹配错误。

https://github.com/vllm-project/vllm/issues/11630
这个issue是一个bug报告，涉及主要对象是vllm模型的image embeddings处理，由于维度不匹配导致了错误的出现。

https://github.com/vllm-project/vllm/issues/11629
这是一个Bug报告，涉及VLLM的一个issue，用户在调用`/v1/chat/completions`时遇到了500 Internal Server Error的问题。原因可能是服务器内部问题导致的错误。

https://github.com/vllm-project/vllm/issues/11628
该issue为用户需求提出类型，主要涉及的对象是在prefill阶段的cpu负担过重，由于此原因导致用户希望在prefill阶段能够使用"cudagraph"。

https://github.com/vllm-project/vllm/issues/11627
这是一个关于代码使用问题的issue，涉及到的主要对象是`AsyncLLMEngine`，由于调用`self.model.generate`过多导致出错。

https://github.com/vllm-project/vllm/issues/11626
这是一个Bug报告类型的issue，主要涉及vllm版本0.6.3和启动deepseekai/DeepSeek-V2-Lite-Chat模型推理时出现的错误。原因可能是启动命令中模型名称未能正确识别导致报错。

https://github.com/vllm-project/vllm/issues/11624
这是一个用户提出需求的issue，主要涉及的对象是vllm中的Kernel，用户想要添加一个支持MulAndSilu的Kernel来改进jais, ultravox 和 molmo 模型的性能。

https://github.com/vllm-project/vllm/issues/11623
这是一个bug报告，主要涉及vllm在CUDA 11.8环境下安装时出现错误。由于CUDA driver版本过旧导致cuTensorMapEncodeTiled符号未定义，引发了导入错误。

https://github.com/vllm-project/vllm/issues/11622
这是一个bug报告，涉及主要对象是Parameter subclasses。由于Parameter subclasses 在torch.compile和offloadedTensor中存在兼容性问题，需要将Parameter subclasses替换为raw nn.Parameter，并直接添加必要属性和函数以实现量化参数的所需特性。

https://github.com/vllm-project/vllm/issues/11621
这是一个用户提出需求的issue，主要涉及AsyncEngineArgs加载多个lora模块的问题，用户想了解如何同时加载多个lora模块。

https://github.com/vllm-project/vllm/issues/11620
这个issue类型是需求提出，涉及到VLM（Visual Language Model）的multi-modal data parsing抽象化，开发者需要修改数据解析器来支持模型的额外模态。

https://github.com/vllm-project/vllm/issues/11619
这是一个bug报告，主要涉及到在使用vllm服务过程中出现的错误消息提示以及与chat模板相关的问题。由于transformers v4.44取消了默认的chat模板支持，用户在未提供chat模板的情况下导致出现数值错误和运行警告提示。

https://github.com/vllm-project/vllm/issues/11618
这是一个bug报告类型的issue，涉及主要对象是CI/Build中的CPU。该问题由于triton FP8 kernels未延迟导入导致CPU CI失败。

https://github.com/vllm-project/vllm/issues/11617
这是一个bug报告，涉及的主要对象是端口被占用导致无法启动服务。

https://github.com/vllm-project/vllm/issues/11616
这是一个bug报告，涉及对象是vllm在IBM ppc64le平台上构建失败的问题。由于缺少必要的库文件，导致outlines_core无法定位库文件而出现错误。

https://github.com/vllm-project/vllm/issues/11615
这是一个bug报告，涉及的主要对象是vllm的Dockerfile.rocm，用户遇到了由于缺少特定版本的torch和torchvision wheel文件而无法构建vllm镜像的问题。

https://github.com/vllm-project/vllm/issues/11614
这个Issue类型为建议/需求，主要对象是torch.compile模块；用户提出需要考虑相关代码在编译缓存中的问题，可能是为了更好的性能和代码执行效果。

https://github.com/vllm-project/vllm/issues/11613
这是一个协助拆分多个更小的PR的类型为"[Do Not Merge] - LoRA V1 Reference PR"，主要涉及LoRA功能的实现和优化。这个issue由于引入了许多更改（包括一些不必要的更改），导致需要将更改拆分成较小的PR，以便更好地管理和审查代码变更。

https://github.com/vllm-project/vllm/issues/11612
这是一个升级请求，涉及将 helm/kind-action 从 1.10.0 升级到 1.12.0，其中包含了更新和新功能。

https://github.com/vllm-project/vllm/issues/11611
这是一个Bug报告类型的Issue，涉及主要对象是Nvidia DALI 和 VLLM。由于环境中CUDA版本和PyTorch版本不匹配，导致出现了相关的问题。

https://github.com/vllm-project/vllm/issues/11610
这是一个Bug报告类型的Issue，主要涉及 vllm 库中的'Qwen2ForSequenceClassification'模块，由于新版本 bitsandbytes >= 0.45.0 的改动导致了'NoneType' object has no attribute 'cquantize_blockwise_fp16_nf4'错误。

https://github.com/vllm-project/vllm/issues/11609
这是一个用户提出需求的issue，主要对象是需要允许平台指定注意力后端。由于当前的限制，需要新增功能支持不同平台实现自定义的注意力后端。

https://github.com/vllm-project/vllm/issues/11608
这是一个Bug报告，主要涉及无法加载`Qwen2-VL-72B-Instruct`模型在Vllm中，原因是未能识别问题根源导致加载失败。

https://github.com/vllm-project/vllm/issues/11607
这是一个更新请求类型的issue，涉及主要对象是kernel。由于CUTLASS 3.6.0版本不再支持MixedInput kernel schedule tags，需要相应更新Machete kernels，导致该issue的提出。

https://github.com/vllm-project/vllm/issues/11606
这个issue类型是技术需求，主要涉及到模型Qwen/Qwen2VL2BInstruct输出结果的置信度问题。由于logprobs返回结果为空列表导致正确答案的置信度为0，用户希望找到解码置信度的方法。

https://github.com/vllm-project/vllm/issues/11604
这是一个bug报告，涉及DeepseekScalingRotaryEmbedding中的打印语句问题。在加载deepseek_v3时发现。由于打印语句存在导致不必要的输出，需要移除。

https://github.com/vllm-project/vllm/issues/11603
这是关于Bug报告。问题涉及到VLLM项目中的AsyncEngine后端循环被停止的问题。可能是由于AsyncEngine后端循环停止，导致了一些异常症状或者功能异常。

https://github.com/vllm-project/vllm/issues/11602
这个issue属于功能需求，主要涉及平台插件的启用，由于需要重构代码并启用未注册的平台插件。

https://github.com/vllm-project/vllm/issues/11601
这是一个Bug报告，涉及的主要对象是用户在运行VLLM时遇到问题。由于环境中PyTorch版本不匹配，CUDA版本未能成功收集，导致用户无法使用OpenGVLab/InternVL2_5-78B-MPO-AWQ。

https://github.com/vllm-project/vllm/issues/11600
该issue属于用户提出需求类型，主要对象是deepseek v3的性能问题。由于环境中存在一些参数配置不佳，导致了性能吞吐量低下的问题。

https://github.com/vllm-project/vllm/issues/11598
这是一个bug报告，主要涉及编译缓存的修复问题。由于`self.to_be_compiled_sizes`应该使用`self.compile_sizes.copy()`，而不是`self.compile_sizes.union(self.capture_sizes)`，导致编译缓存无法正常工作。

https://github.com/vllm-project/vllm/issues/11597
这是一个Bug报告，主要涉及VLLM版本问题导致的模块未找到错误。

https://github.com/vllm-project/vllm/issues/11596
这是一个bug报告，主要涉及的对象是vLLM中的cudagraph，由于inplace buffer赋值导致在`microsoft/Phi3.5visioninstruct`中产生了静默错误和垃圾输出。

https://github.com/vllm-project/vllm/issues/11595
这是一个缺乏具体内容的反馈问题，无法确定属于哪种类型。

https://github.com/vllm-project/vllm/issues/11594
这个issue是一个文档相关的修改提议，主要涉及将rST格式的列表表格转换为MyST格式。

https://github.com/vllm-project/vllm/issues/11593
这个issue是更新Neuron SDK版本以支持PyTorch 2.5的请求类型，涉及的主要对象是Neuron SDK。

https://github.com/vllm-project/vllm/issues/11592
这个issue类型是用户提出需求，涉及主要对象是vllm中的parallel tool calls功能，用户询问关于该功能在llama3.1模型中的使用方式。

https://github.com/vllm-project/vllm/issues/11591
这是一个bug报告，涉及到vllm版本v0.6.6.post1的官方Docker镜像缺失的问题。

https://github.com/vllm-project/vllm/issues/11590
这是一个用户提出的需求类型的issue，主要涉及的对象是LM Eval With Streaming Integration Tests。

https://github.com/vllm-project/vllm/issues/11589
该问题类型为功能需求，主要涉及 Triton 配置用于 Fp8 Block 量化的问题。由于需要使用 DeepSeekV3，并且基于某些更新，因此需要适配相关配置。

https://github.com/vllm-project/vllm/issues/11588
这个issue类型是性能问题，涉及到在线推理过程中通过改变参数导致性能瓶颈的情况。

https://github.com/vllm-project/vllm/issues/11587
这是一个bug报告类型的issue，主要涉及的对象是vLLM中的CUDA图捕获错误，由于CUDA图捕获期间出现先前错误导致操作失败的情况。

https://github.com/vllm-project/vllm/issues/11586
这是一个bug报告，主要涉及API Server中处理错误的方式，导致不能清晰返回错误代码给客户端应用程序。

https://github.com/vllm-project/vllm/issues/11585
这是一个bug报告，涉及vllm项目中无法启用glm-4模型的前缀缓存，可能是因为glm4和glm4v模型具有相同的`model_type`值而导致的。

https://github.com/vllm-project/vllm/issues/11584
这是一个bug报告，涉及的主要对象是VLLM模型的运行环境（Pascal架构）。原因是使用64的块大小在NVIDIA Tesla P40上导致错误，此问题可能由于硬件兼容性问题引起。

https://github.com/vllm-project/vllm/issues/11583
这是一个bug报告，主要涉及到vllm项目中cohere2模型的interleaving sliding window支持问题，由于没有正确实现该功能并且依赖于huggingface发布，导致用户提出了这个issue。

https://github.com/vllm-project/vllm/issues/11582
这是一个功能改进类的issue，涉及的主要对象是Pixtral模型。这个问题是由于之前在模型文件中硬编码的token IDs，现在可以从模型配置文件中获取，通过这个PR进行了更新，以解决问题。

https://github.com/vllm-project/vllm/issues/11581
这是一个优化性质的issue，主要对象是针对`token_ids_cpu` Tensor的内存使用优化。

https://github.com/vllm-project/vllm/issues/11580
这个issue类型是文档修复，涉及主要对象为项目文档。

https://github.com/vllm-project/vllm/issues/11579
这是一个用户提出需求的issue，主要涉及的对象是LoRA kernel micro benchmarks。用户希望增加LoRA内核微基准测试以优化LoRA内核，以帮助调整/优化LoRA内核，并避免基准测试期间的缓存效应。

https://github.com/vllm-project/vllm/issues/11578
这是关于模型初始化的需求提出。该问题单主要涉及支持Deepseek-VL2模型。这个问题的原因是DeepseekAI的DeepSeekVL2Tiny模型由于未使用MLA attention，目前并不受支持。

https://github.com/vllm-project/vllm/issues/11577
这是一个关于使用vllm中embeddings API的bug报告，涉及到在任务设置为"generate"时发生的错误，可能是由于数据类型转换不正确导致的问题。

https://github.com/vllm-project/vllm/issues/11576
这是一个关于增加SageMaker兼容性最低要求的issue，主要涉及到Dockerfile的更改以及新增了`/ping`和`/invocations`端点。原因是为了扩展到SageMaker使用情况，但尚未完全测试在SageMaker上的构建和部署。

https://github.com/vllm-project/vllm/issues/11575
这是一个技术改进型的Issue，主要涉及SageMaker服务的兼容性问题，通过实现`/ping`和`/invocations`路径以及做出相关Dockerfile调整来解决，并探讨了一些路由和功能上的变化。

https://github.com/vllm-project/vllm/issues/11574
该issue是关于bug报告，涉及主要对象是更新benchmark_moe.py脚本以使用块内量化功能，并由于某些原因导致调整未能成功，出现不明确的错误。

https://github.com/vllm-project/vllm/issues/11572
这是一个bug报告，该问题涉及到benchmark步骤的依赖性。由于`blockh100`这个依赖项已被删除，导致无法运行benchmark步骤。

https://github.com/vllm-project/vllm/issues/11571
该issue类型为用户提出需求，该问题单涉及的主要对象是模型支持 InternLM2 奖励模型。

https://github.com/vllm-project/vllm/issues/11570
这是一个需求类型的issue，主要涉及前端（Frontend）的错误处理改进。原因是需要更新V0请求处理以匹配V1的处理方式。

https://github.com/vllm-project/vllm/issues/11569
这是关于如何提高vLLM模型生成速度的问题，用户询问当前自定义模型提供服务的方式与预定义方式是否会影响性能。可能的原因是用户想要利用GPU资源最大化来加速生成速度。

https://github.com/vllm-project/vllm/issues/11568
这是一个关于性能问题的bug报告，涉及VLLM模型在VLLM和Huggingface上表现不一致的问题。用户提出了提高性能、性能回归情况和环境信息，并表示不确定是否存在参数错误导致的bug。

https://github.com/vllm-project/vllm/issues/11567
这是一个文档更新类型的issue，涉及将mllama示例代码根据官方文档进行更新，主要涉及到使用chat模板的问题。

https://github.com/vllm-project/vllm/issues/11566
这个issue是关于bug报告的，主要涉及到BNB loader在处理存在相同后缀名称的分片和合并权重时可能加载失败的问题。

https://github.com/vllm-project/vllm/issues/11565
这是一个bug报告，主要涉及的对象是"allocate_slots"方法。由于调用位置错误导致出现了"assert total_num_scheduled_tokens > 0"的bug。

https://github.com/vllm-project/vllm/issues/11564
这是一个Bug报告，主要涉及vllm在与OpenAIcompatible API server集成时出现ZMQError("Operation not supported")异常导致引擎无法启动的问题。

https://github.com/vllm-project/vllm/issues/11563
该issue类型是用户提出需求，讨论关于在vllm中支持稀疏性问题，提到了支持2:4稀疏性以及未来可能提供无结构稀疏性支持的问题。

https://github.com/vllm-project/vllm/issues/11562
这是一个Bug报告，涉及主要对象是kv_cache_manager，由于num_tokens没有适当的判断条件导致了数值为0的情况下发生数值错误的异常。

https://github.com/vllm-project/vllm/issues/11561
这是一个Bugfix类型的issue，涉及的主要对象是ROCM代码路径中的压缩张量支持，由于未检查input_shape层是否存在，导致问题产生。

https://github.com/vllm-project/vllm/issues/11560
这是一个bug报告，主要涉及VLLM在GPU环境下出现CUDA内存不足的问题。

https://github.com/vllm-project/vllm/issues/11559
这个issue属于bug报告类型，主要对象是vllm版本和DeepSeek-V3模型。由于用户不清楚现有的vllm版本是否支持DeepSeek-V3，导致提出这个问题。

https://github.com/vllm-project/vllm/issues/11558
这是一个用户提出需求的issue，主要涉及的对象是 LoRA 模型。由于用户希望训练完全的 lm_head 和 embed_tokens，提出了这个需求。

https://github.com/vllm-project/vllm/issues/11557
这是一个功能需求类型的issue，涉及vLLM项目对SageMaker-required endpoints的支持。由于AWS需要管理镜像，此功能之前未被支持，但用户提出通过将必需的SageMaker端点路由整合到vLLM源码中来提供支持。

https://github.com/vllm-project/vllm/issues/11556
这个issue是一个需求提出，主要对象是模型"LoRA with lm_head and embed_tokens fully trained"。由于该模型需要进一步训练，用户提出了相关需求。

https://github.com/vllm-project/vllm/issues/11555
这个issue类型为代码优化建议，主要对象是关于Worker的代码实现，由于重复执行`torch.cuda.synchronize()`导致不必要的性能开销。

https://github.com/vllm-project/vllm/issues/11554
这是一个bug报告，涉及前端工具解析器的重构和简化工具解析接口。由于流处理JSON解析库可能存在缺陷，并且与非流端的hermes模板版本不一致导致了问题。

https://github.com/vllm-project/vllm/issues/11552
这个issue是关于新功能需求的，主要涉及的对象是针对vllm下的speculative decoding模块。这个需求是为了添加一项名为"mean accept length metric"的指标来评估模型的性能。

https://github.com/vllm-project/vllm/issues/11551
这是一个用户提出需求的issue，主要涉及到MolmoForCausalLM模型的BNB量化支持。用户提出了需要在BNB加载器中添加权重映射器，并进一步修改MolmoForCausalLM的MLP以避免BNB生成错误结果。

https://github.com/vllm-project/vllm/issues/11550
这是一个关于性能提升的问题，主要涉及如何测量实际GPU内存使用情况。问题可能由于缺乏API来测量已分配给现有请求的模型权重和PagedAttention块的GPU内存使用情况而引起。

https://github.com/vllm-project/vllm/issues/11549
这是一个需求类型的issue，主要涉及到文档（doc）。可能由于当前文档中缺少对xgrammar的描述，用户提出需求添加相关内容。

https://github.com/vllm-project/vllm/issues/11548
该issue类型为文档更新请求，涉及的主要对象是文档。由于文档缺少xgrammar的描述，用户请求添加相关内容。

https://github.com/vllm-project/vllm/issues/11547
这是一个bug报告类型的issue，主要涉及到深度模型中MoE（Mixture of Experts）的量化问题。

https://github.com/vllm-project/vllm/issues/11546
这是一个Bug报告，涉及到权重映射模块。由于CC([Misc] Move weights mapper)导致了这个bug。

https://github.com/vllm-project/vllm/issues/11545
这是一个用户需求的issue，主要涉及的对象是API Server中的`Detokenizer`和`EngineCore`模块的输入统一问题，由于需要准备3个处理架构，提出了此问题。

https://github.com/vllm-project/vllm/issues/11544
这是一个bug报告类型的issue，主要涉及的对象是实现了chunked sorting以减少内存使用的PR。原因是Pytorch在logits上进行排序会导致显著的内存占用，在解码时特别是在用户请求获取logits时。

https://github.com/vllm-project/vllm/issues/11543
这是一个 bug 报告 issue，涉及 Continuous Integration（CI） 。由于 CI 出现故障，导致需要修复。

https://github.com/vllm-project/vllm/issues/11542
这是一个bug报告，涉及安装环境问题导致无法找到特定版本的软件包xgrammar。

https://github.com/vllm-project/vllm/issues/11541
这个issue是关于特性增强而不是bug报告，主要涉及API Server中ZMQ/MP Utilities的改进。原因是为了实现更好的多进程架构和错误处理机制。

https://github.com/vllm-project/vllm/issues/11540
这个issue是一个修复类型的issue，主要涉及到代码中的类型错误修复；此问题可能由于代码中的类型错误导致了BUG。

https://github.com/vllm-project/vllm/issues/11539
该issue属于需求提出类型，主要涉及Deepseek V3模型的改进，可能由于对模型功能和性能的迭代需求而产生。

https://github.com/vllm-project/vllm/issues/11538
这个issue是一个Bug报告，主要涉及的对象是vllm项目中的yapf模块。由于原因尚不清楚，导致了需要修复yapf的问题。

https://github.com/vllm-project/vllm/issues/11537
这是一个Bug报告类型的issue，涉及的主要对象是vLLM中的FP8 W8A8 LLM Compressor Models。由于PyTorch版本与CUDA信息不匹配，导致这些模型在vLLM中无法加载。

https://github.com/vllm-project/vllm/issues/11536
这是一个需求更新的类型，涉及的主要对象是文档。通过原因导致的提交issue，以更新文档的内容。

https://github.com/vllm-project/vllm/issues/11535
该issue类型为用户提出需求，涉及主要对象为Deepseek V3模型，用户希望在文档和README中添加模型名称。由于缺乏该信息，用户无法准确了解和使用Deepseek V3模型，因此提出了需要更新文档的需求。

https://github.com/vllm-project/vllm/issues/11534
这个issue是性能优化类型，主要涉及API服务器，由于高QPS时减少任务切换导致性能提升和支持正确处理取消操作的问题。

https://github.com/vllm-project/vllm/issues/11533
这是一个用户提出需求的issue，主要涉及的对象是添加CPU offloading的基准测试脚本。

https://github.com/vllm-project/vllm/issues/11532
这个issue类型是功能增强（Feature Enhancement），主要涉及到vLLM项目中的Block Allocator以支持KV缓存CPU卸载，由于CPU offloading在基准测试中表现更好，提出了相关实现和性能优化的变化。

https://github.com/vllm-project/vllm/issues/11531
这是一个性能优化类型的issue，涉及到CUDA kernel实现的优化。

https://github.com/vllm-project/vllm/issues/11530
这个issue类型是bug报告，主要涉及API Server的ulimit设置，由于没有设置ulimit导致高qps服务时出现文件描述符用尽的问题。

https://github.com/vllm-project/vllm/issues/11529
该issue类型为功能更新（Breaking Change），主要涉及API Server的修改和性能优化。原因是在这次变更中，移除了默认的代理功能，导致性能提升，但可能会影响到之前依赖代理功能的用户或系统。

https://github.com/vllm-project/vllm/issues/11528
这是一个bug报告，主要涉及VLLM中的参数名称和处理权重加载的问题，由于参数命名错误和处理权重加载不当，可能导致权重加载失败或导致模型运行出现错误。

https://github.com/vllm-project/vllm/issues/11527
这是一个功能改进的issue，主要涉及到对媒体内容读取和写入逻辑的抽象，以及在`BaseMultiModalItemTracker.add`中去除重复的字典键。

https://github.com/vllm-project/vllm/issues/11526
这是一个Bug报告，涉及GPU类型不同时导致不同的输出结果。

https://github.com/vllm-project/vllm/issues/11525
这是一个Bug报告，主要涉及的对象是vllm 0.6.5版本以及Qwen2-VL-7B-Instruct模型的运行问题。由于环境信息显示PyTorch版本与CUDA版本不匹配，可能导致Lora LOND成功执行但效果不佳。

https://github.com/vllm-project/vllm/issues/11523
这个issue是对模型量化支持的需求提出，涉及主要对象是 deepseek_v3 模型。

https://github.com/vllm-project/vllm/issues/11522
这个issue属于需求提出类型，主要涉及工具解析器的重构，因为当前工具解析器存在错误且代码混乱，导致性能瓶颈和功能问题。

https://github.com/vllm-project/vllm/issues/11521
该issue类型为需求提出，主要涉及对象是`openai_chat_completion_client_for_multimodal.py`文件缺失视频示例。

https://github.com/vllm-project/vllm/issues/11520
这个issue类型为需求提出，主要对象是DeepseekV3模型，由于不支持该模型架构导致数值错误。

https://github.com/vllm-project/vllm/issues/11519
这是一个Bug报告，涉及的主要对象是tqdm进度条显示的错误。这个问题是由于设定$n$较大时，tqdm进度条显示的提示数字错误导致的。

https://github.com/vllm-project/vllm/issues/11518
这是一个Bug报告类型的Issue，主要涉及的对象是vllm项目在ppc64le平台上的构建问题。原因导致了构建失败的bug。

https://github.com/vllm-project/vllm/issues/11517
这是一个bug报告，涉及的主要对象是vLLM模型在使用tensor parallel size 8时，请求被卡在pending状态，原因可能是GPU资源未能正确分配造成的。

https://github.com/vllm-project/vllm/issues/11516
这个issue是涉及代码重构的PR，主要对象是punica wrapper function。由于需要跨平台兼容性和提升可扩展性，需要将get_punica_wrapper()函数移动到Platform模块中。

https://github.com/vllm-project/vllm/issues/11515
这是一个bug报告，关于在vllm项目中将HIPCC版本替换为更精确的ROCm版本的问题，提出了ROCm版本表示不清晰的情况，导致设置ROCM版本时出现问题。

https://github.com/vllm-project/vllm/issues/11514
这个issue类型是文档问题报告，涉及的主要对象是vllm项目的文档。原因是Python版本在安装和快速入门部分之间不一致，可能导致用户困惑和兼容性问题。

https://github.com/vllm-project/vllm/issues/11513
这是一个添加单元测试的Issue，主要对象是V1的kv缓存相关数据结构，提出了需要新增单元测试的需求。

https://github.com/vllm-project/vllm/issues/11512
这个issue类型是bug报告，涉及主要对象是Qwen2-Audio模型。由于特定范围的音频数据维度与模型输入不匹配，导致在生成输出时出现数值错误。

https://github.com/vllm-project/vllm/issues/11511
这是一个bug报告，涉及主要对象为在ARM CPU上执行推理时CPU使用率非常低的问题。由于执行速度测试时CPU使用率较低，可能导致问题出现。

https://github.com/vllm-project/vllm/issues/11510
该issue属于功能改进（Feature Improvement），涉及的主要对象是`MolmoForCausalLM`模型。原因是当前版本中`image_projector`和LLM共用相同的MLP导致了权重形式不一致，这造成了混淆，并且增加了对BNB和LoRA的支持困难。

https://github.com/vllm-project/vllm/issues/11509
这是一个用户提出需求的issue，涉及的主要对象是vLLM模型列表文档。用户提出需求是因为vLLM模型名称不反映其架构，建议将`QVQ`和`QwQ`明确列入已支持的模型列表。

https://github.com/vllm-project/vllm/issues/11508
这是一个用户提出需求类型的 issue，主要涉及LLama3.2 Vision Instruct模型的推理问题，用户询问为什么实际效果与理论不一致。

https://github.com/vllm-project/vllm/issues/11507
这个issue类型是用户提出需求，请教问题，主要涉及VLLM是否支持基于其他AI框架（如MindSpore）的模型，用户希望将MindSpore集成到VLLM中以扩展MindSpore生态系统。

https://github.com/vllm-project/vllm/issues/11506
这是一个bug报告类型的issue，主要涉及的对象是在A800上推理不成功。由于硬件环境为NVIDIA A800 GPU，可能存在与PyTorch版本和CUDA版本兼容性问题导致推理失败。

https://github.com/vllm-project/vllm/issues/11505
这是一个用户提出需求的问题，主要对象是vllm，问题是关于是否支持基于其他人工智能框架的模型的请求。

https://github.com/vllm-project/vllm/issues/11504
这个issue属于bug报告类型，主要对象是执行失败的用户，问题产生的原因是类型不匹配导致程序执行失败，用户需要获得关于如何处理类型不匹配错误的帮助。

https://github.com/vllm-project/vllm/issues/11503
这个issue类型是需求提出，涉及的主要对象是平台相关功能。原因是为了在硬件可插拔功能的基础上移动模型架构检查功能到平台。

https://github.com/vllm-project/vllm/issues/11502
这是一个bug报告类型的issue，涉及主要对象是Deepseek v3代码。原因是FP8量化不支持CUDA图，导致用户需要关闭CUDA图功能。

https://github.com/vllm-project/vllm/issues/11501
这是一个用户提出需求的issue，主要对象是在vllm项目中添加一个占位符模块来简化导入可选模块。

https://github.com/vllm-project/vllm/issues/11500
这是一个性能问题的报告，主要涉及Pipeline Parallel的性能提升，但未收到响应。由于Pipeline Parallel运行时没有明显的加速效果，可能是由于GPU利用率低导致。

https://github.com/vllm-project/vllm/issues/11499
这是一个用户询问如何将多模态输入传递给LLM.encode()方法的问题，类型为功能需求。用户提出了关于如何在同时传递文本和图像作为输入时调用LLM.encode()的问题。

https://github.com/vllm-project/vllm/issues/11498
这是一个Bug报告，涉及到vLLM模型在使用`response_format` with `outlines`时导致模型服务意外退出的问题。

https://github.com/vllm-project/vllm/issues/11497
这是一个空白内容的Issue，类型无法确定。

https://github.com/vllm-project/vllm/issues/11496
该issue属于增强功能请求类型，主要涉及到vllm/compilation组件。由于缺乏类型检查，可能导致`compiled_graph`和`wrap_inductor`方法返回不正确的计算图，因此用户建议加入Mypy类型检查来确保正确性。

https://github.com/vllm-project/vllm/issues/11495
这是一个Bug报告，涉及到LoRA的权重映射器。原因可能是权重映射器的错误导致了问题。

https://github.com/vllm-project/vllm/issues/11494
这是一个文档重构的Issue，主要涉及到多模态工具文件的清理。

https://github.com/vllm-project/vllm/issues/11493
这是一个bug报告，涉及Marlin kernel中的race conditions。导致了 GPTQ 推理的错误和输出损坏的问题。

https://github.com/vllm-project/vllm/issues/11492
这是一个功能需求提出的issue，主要涉及前端的视频加载功能，由于当前仅支持基于图像帧的base64 URL，用户提出使用decord来加载视频以支持其他类型的视频，并提及了相应的 GitHub issue。

https://github.com/vllm-project/vllm/issues/11491
这是一个功能改进类型的issue，主要涉及的对象是GitHub链接格式。原因是为了减少复制和粘贴完整URL的操作。

https://github.com/vllm-project/vllm/issues/11490
这是一个bug报告类型的issue，主要涉及使用vllm中的prefill-decode disaggregation功能时遇到的问题。问题可能源于命令参数不正确导致模型部署失败，用户寻求关于PDdisaggregation命令的帮助。

https://github.com/vllm-project/vllm/issues/11489
这是一个Bug报告，涉及主要对象为使用了两块显卡的机器。由于单 GPU 无法完全清理缓存导致剩余1 GB 的缓存，用户寻求解决办法。

https://github.com/vllm-project/vllm/issues/11487
这是一个关于在客户端-服务器架构中实现VLLM客户端端错误处理的问题，主要涉及如何在客户端查看详细错误信息以便快速故障排除和诊断，可能是一个功能需求。

https://github.com/vllm-project/vllm/issues/11486
这是一个Bug报告，主要涉及Inference Results在不同数量的GPU上的一致性问题。由于环境设置中出现了CUDA使用不一致的情况，导致了推理结果不一致的bug。

https://github.com/vllm-project/vllm/issues/11485
这是一个bug报告，涉及的主要对象是使用VLLM API server启动时遇到的CUDA错误，原因可能是CUDA设备无法执行所需的内核映像。

https://github.com/vllm-project/vllm/issues/11484
这是一个Bug报告，主要涉及xgrammar在使用speculative decoding时会导致崩溃的问题。

https://github.com/vllm-project/vllm/issues/11483
该issue属于用户需求类型，主要涉及加速函数self.model()的问题，用户希望使用VLLM来提高函数运行速度，但不清楚如何操作。

https://github.com/vllm-project/vllm/issues/11482
这是一个bug报告类型的issue，主要涉及vllm0.6.4的性能提升在qwen2.5上并没有体现。原因可能是性能提升未能有效传递到特定的模型中，导致处理时间未能明显减少。

https://github.com/vllm-project/vllm/issues/11481
这是一个需求提出类型的issue，主要涉及的对象为KV cache transfer connector。由于当前使用ifelse语句进行初始化，用户希望能够通过注册表进行初始化，以便更轻松地添加第三方连接器。

https://github.com/vllm-project/vllm/issues/11480
这是一个特性需求问题，关于允许在`kv_parallel_size==1`时初始化KV缓存传输代理。

https://github.com/vllm-project/vllm/issues/11479
这个issue是关于用户提出需求的，主要涉及的对象是新增模型支持。这个问题由于新模型目前仅支持单轮对话和图像，不支持视频输入，用户希望最新版本至少可以支持对该模型的推断。

https://github.com/vllm-project/vllm/issues/11478
这是一个bug报告，该问题涉及到vllm和transformers推理结果未对齐。由于代码中存在错误，导致两个模型的生成结果不一致。

https://github.com/vllm-project/vllm/issues/11477
这是一个功能需求，主要对象是vLLM的load balancer，用户需要解决prefix cache和load balancer之间的不兼容性问题。

https://github.com/vllm-project/vllm/issues/11476
这是一个bug报告类型的issue，主要涉及Qwen2.5推理长上下文导致回复不完整的问题，使用transformers库可以得到完整结果。可能由于算法实现问题导致这一bug。

https://github.com/vllm-project/vllm/issues/11475
这是一个bug报告类型的issue，主要涉及问题陈述的修复。可能由于问题陈述不清晰或有误导性导致用户提出这个问题。

https://github.com/vllm-project/vllm/issues/11474
这是一个bug报告，用户提出了关于在vllm中运行推理特定模型时遇到的环境配置错误的问题。

https://github.com/vllm-project/vllm/issues/11473
这是一个bug报告类型的issue，主要涉及的对象是vllm。原因可能是vllm在某些情况下没有正确响应结果。

https://github.com/vllm-project/vllm/issues/11472
这个issue是关于bug报告，主要涉及的对象是VLLM中的RayExecutor。由于引擎核心客户端和引擎核心可能在不同进程中，导致需要在EngineCore进程中初始化Ray以解决问题。

https://github.com/vllm-project/vllm/issues/11471
这是一个关于使用问题的issue，主要涉及到vllm模型的模板选择和集成问题。用户需要解决如何为特定模型选择模板以及如何将模型集成到vllm中的问题。

https://github.com/vllm-project/vllm/issues/11470
这是一个需求的提出，涉及主要对象是 AMD 的 ROCm 安装文档。这个需求是基于 ROCm 6.3 版本的更新。

https://github.com/vllm-project/vllm/issues/11469
该issue类型为用户提出需求，涉及主要对象为vLLM模型的自动转换功能。由于缺少如何推断nvidia/Llama3.1Nemotron70BRewardHF的奖励模型的使用说明，用户寻求了关于推断奖励模型的帮助。

https://github.com/vllm-project/vllm/issues/11468
这是一个关于性能优化的问题，涉及到Molmo模型在多GPU环境下推理速度较慢以及使用中的错误提示。

https://github.com/vllm-project/vllm/issues/11467
这是一个Bug报告类型的Issue，主要涉及到VLLM在多GPU环境下启动时出现的错误。导致这个问题的原因可能是由于PyTorch版本与CUDA版本不兼容所致。

https://github.com/vllm-project/vllm/issues/11466
这个issue类型是性能问题提议，主要涉及vLLM中的Speculative Decoding对目标模型输出的影响。产生的原因可能是关于性能回归和对输出准确性的疑问。

https://github.com/vllm-project/vllm/issues/11465
这是一个用户提出需求的类型，该问题单涉及的主要对象是在使用AMD ROCm GPU时的部署示例。由于缺乏关于在 deploying_with_k8s.md 中使用AMD ROCm GPU 的示例，用户提出了需求添加这方面的内容。

https://github.com/vllm-project/vllm/issues/11464
这是一个需求提出的类型，该问题单涉及的主要对象是更新部署文档，并添加针对 AMD ROCm GPU 的示例。原因可能是用户希望了解如何在K8S上部署使用 AMD ROCm GPU 的Pod。

https://github.com/vllm-project/vllm/issues/11463
这是一个bug报告，主要涉及的对象是MultiHeadAttention中的attn backend分支，导致原始逻辑永远无法进入xformer backend分支。

https://github.com/vllm-project/vllm/issues/11462
这是一个用户提出安装问题的类型的issue，主要涉及vllm在v100上部署失败的情况。问题可能是由于缺少相关文档或安装步骤导致。

https://github.com/vllm-project/vllm/issues/11461
这是一个用户提出需求的特性请求，主要涉及了将logits-processor-zoo中的logits处理器集成到vLLM框架中。原因是希望使用这些处理器提升文本生成的控制和输出效果。

https://github.com/vllm-project/vllm/issues/11459
这是一个用户提出需求的issue，主要涉及如何推断nvidia/Llama-3.1-Nemotron-70B-Reward-HF的奖励模型，可能是由于用户对如何集成vllm以及在推断特定模型方面的不了解而导致的。

https://github.com/vllm-project/vllm/issues/11458
这是一个bug报告，涉及OpenVINO的安装冲突问题，由于安装冲突导致了该问题。

https://github.com/vllm-project/vllm/issues/11457
这个issue是关于bug报告，涉及的主要对象是Frontend的Online Pooling API，由于之前的Pooling APIs与Embeddings API不兼容，导致了Embeddings API无法在reward models中使用。

https://github.com/vllm-project/vllm/issues/11456
这个issue类型是bug报告，涉及的主要对象是更新数据的分解基准脚本和测试日志。

https://github.com/vllm-project/vllm/issues/11455
这是一个bug报告，主要涉及vllm的disaggregation test and benchmark tools，在程序运行时出现了多个问题导致程序异常退出。

https://github.com/vllm-project/vllm/issues/11454
这是一个bug报告，涉及的主要对象是InternVL2-40B模型推断结果的JSON格式问题，可能是由于模型推断精度问题导致。

https://github.com/vllm-project/vllm/issues/11453
这是一个建议改进的issue，主要涉及前端代码中的hermes_tool_parser.py文件。Bug症状可能是解析JSON输出时的健壮性不足。

https://github.com/vllm-project/vllm/issues/11452
这是一个性能问题报告，主要涉及VLLM中4090和A30使用GPU内存的差异，可能是由于性能回退导致4090在请求时内存耗尽的情况。

https://github.com/vllm-project/vllm/issues/11451
这是一个bug报告类型的issue，涉及的主要对象是试图添加codeshell 7b模型。由于codeshell.py运行时出现错误，导致无法成功添加模型，用户寻求相关问题解决方案。

https://github.com/vllm-project/vllm/issues/11450
这是一个Bug报告，主要涉及vllm在使用Tensor Parallelism加载模型到两个GPU时出现的错误。原因是在处理参数时出现了'TypeError: 'int' object is not iterable'的异常。

https://github.com/vllm-project/vllm/issues/11449
这是一个Bug报告，主要涉及到模型架构不受支持导致数值错误。

https://github.com/vllm-project/vllm/issues/11448
这是一个bug报告，涉及的主要对象是平台入口点。这个问题由于不一致的入口点导致症状，用户希望在不同平台上统一入口点。

https://github.com/vllm-project/vllm/issues/11447
这是一个bug报告，涉及的主要对象是max-model-len参数，由于该参数的取值影响结果，即使输入长度小于该参数值，导致出现了影响结果的问题。

https://github.com/vllm-project/vllm/issues/11446
这是一个Bug报告，涉及VLLM中的模型推理问题，由于池化API重构导致的错误。

https://github.com/vllm-project/vllm/issues/11445
这个issue是一个Bug报告，主要涉及到VLLM中的Prefill/decode分离导致在多并发场景下出现阻塞和崩溃。导致这个问题的原因是通信组件中存在bug。

https://github.com/vllm-project/vllm/issues/11444
这是一个改进建议类型的issue，主要涉及Frontend中的hermes_tool_parser.py文件，建议使用json_repair来提高解析JSON输出的健壮性。

https://github.com/vllm-project/vllm/issues/11443
这是一个bug报告，涉及的主要对象是权重映射器。由于Qwen2VL LoRA权重加载的错误，导致需要等待手动审查才能合并到主分支。

https://github.com/vllm-project/vllm/issues/11442
这是一个用户提出需求的issue，主要涉及支持fairseq2 Llama模型，并添加了`Fairseq2LlamaForCausalLM`到模型注册表，支持sharded weights的加载。

https://github.com/vllm-project/vllm/issues/11441
这是一个bug报告，涉及的主要对象是v0.6.3版本不支持Qwen2ForCausalLM架构，可能原因是模型输入数据有误。

https://github.com/vllm-project/vllm/issues/11440
这是一个文档修复类型的issue，主要涉及到命令行工具的帮助信息。原因是缺少一个空格导致显示效果不正确。

https://github.com/vllm-project/vllm/issues/11439
这是一个bug报告，主要涉及的对象是MolmoForCausalLM模型，由于LoRA支持在推断中未能正确工作，作者需要帮助验证该功能是否正常。

https://github.com/vllm-project/vllm/issues/11438
这个issue是关于bug报告的，主要对象是修复显示不相关异常信息导致程序启动失败的问题。这个问题的原因是显示的异常信息容易引起混淆。

https://github.com/vllm-project/vllm/issues/11437
这是一个bug报告，涉及的主要对象是vllm中的程序功能。造成这个问题的原因是出现了torch.OutOfMemoryError，导致程序无法启动，而出现的异常消息会误导用户。

https://github.com/vllm-project/vllm/issues/11436
这个issue是关于性能优化的建议和提问，主要涉及vllm项目中的Prefill功能的性能问题，导致了执行时的慢速表现。

https://github.com/vllm-project/vllm/issues/11435
这个issue类型是bug报告，涉及的主要对象是vllm的CPU build Dockerfile。原因可能是setuptoolsscm无法检测版本导致pip安装出错。

https://github.com/vllm-project/vllm/issues/11434
这是一个bug报告，主要涉及vLLM模型服务在使用OpenAIAPI兼容模型时出现崩溃的问题，原因是MRope位置不应该用于创建文本输入的input_positions。

https://github.com/vllm-project/vllm/issues/11433
该issue类型为用户提出需求和请教问题，主要涉及企业实施和数据收集细节。由于企业需要确认vLLM在商业环境下是否符合许可要求，以及是否有内置数据收集功能，因此用户提出了这些问题寻求帮助。

https://github.com/vllm-project/vllm/issues/11432
这是一个bug报告，主要涉及chat template examples，由于缺少`bos_token`导致的问题。

https://github.com/vllm-project/vllm/issues/11431
这是一个bug报告，涉及的主要对象是vLLM中的MolmoForCausalLM模型。由于MolmoForCausalLM目前不支持LoRA适配器，导致用户在使用Lora adapters时出现了AssertionError。

https://github.com/vllm-project/vllm/issues/11430
这个issue是关于bug报告，主要涉及的对象是Qwen2-VL LoRA weight loading，由于Qwen2VL LoRA Adapters的问题导致其不生效。

https://github.com/vllm-project/vllm/issues/11429
这是一个用户需求问题，用户想要了解如何在vllm中手动加载权重而不是使用huggingface_hub的权重。

https://github.com/vllm-project/vllm/issues/11428
该issue类型为功能需求，主要对象是offline_inference_with_prefix.py文件。由于缺少TTFT这一重要度量标准，导致prefix缓存功能不完整。

https://github.com/vllm-project/vllm/issues/11427
这个issue类型为功能需求提议，主要涉及的对象是VLM中的merged multimodal processor实现，用户寻求帮助来初始化encoder-decoder LMMs的merged multimodal processor。

https://github.com/vllm-project/vllm/issues/11426
这是一个需求合并(issue)类型，主要对象为Zhn/fish项目的e2e测试。

https://github.com/vllm-project/vllm/issues/11425
这是一个杂项（misc）类型的issue，主要对象是性能（perf）方面的代码。

https://github.com/vllm-project/vllm/issues/11424
这个issue类型是软件更新，涉及的主要对象是helm/kind-action项目。由于更新导致的问题可能是功能缺陷，需要修复或新增功能。

https://github.com/vllm-project/vllm/issues/11423
这是一个 bug 报告类型的 issue，主要涉及到 ROCm 安装指南中 torch nightly 版本的错误。这个问题的症状是指导链接错误导致无法正常使用指南。

https://github.com/vllm-project/vllm/issues/11422
这个issue类型为测试问题，涉及的主要对象是triton。由于缺少具体描述或内容，无法确定导致了什么样症状的bug或用户提出了关于什么的问题。

https://github.com/vllm-project/vllm/issues/11421
这是一个bug报告，主要涉及v0.6.5版本出现的网络问题导致服务器异常断开连接和请求丢失的情况。

https://github.com/vllm-project/vllm/issues/11420
这是一个bug报告，涉及的主要对象是V1启动过程中的错误处理。原因是缺少错误处理导致启动过程中可能出现异常行为。

https://github.com/vllm-project/vllm/issues/11419
这是一条关于CI（持续集成）的通知，标志着某台名为"Unboock H100"的机器重新连接了。

https://github.com/vllm-project/vllm/issues/11418
该issue类型为技术改进类型，主要对象是vllm/worker中的mypy类型提示缺失问题。原因是缺少了对vllm/worker代码中的mypy类型提示，导致需要增加这些提示以进行类型检查。

https://github.com/vllm-project/vllm/issues/11417
这是一个文档问题，用户提出了关于在vLLM中使用EAGLE的文档需求。

https://github.com/vllm-project/vllm/issues/11416
这是一个用户提出需求的issue，主要涉及VLLM中的QTIP Quantization特性。用户希望将QTIP算法实现到VLLM中，因为该算法在2位时几乎不损失性能，并且相对于其他2位量化算法表现更好。

https://github.com/vllm-project/vllm/issues/11415
这是一个bug报告，涉及到vLLM的AI SDK中的`generateObject`处理可空字符串时出现的问题，导致在0.6.5版本下请求失败。

https://github.com/vllm-project/vllm/issues/11414
这是一个BugFix类型的issue，主要涉及Mamba/Jamba模型在高负载下超出缓存插槽数导致的问题。

https://github.com/vllm-project/vllm/issues/11413
这是一个关于如何同时对EngineCoreClient和EngineCoreProc活动进行profile的问题，涉及主要对象是vllm V1版本。由于EngineCoreClient和EngineCoreProc活动的异步运行导致了仅能记录EngineCoreClient活动而无法捕获EngineCoreProc活动的症状。

https://github.com/vllm-project/vllm/issues/11412
这是一个bug报告类型的issue，主要涉及到无法使用dockerfile安装vllm的问题。导致此问题的原因可能是与使用openvino后端有关。

https://github.com/vllm-project/vllm/issues/11411
这是一个关于无法导入模块的bug报告，主要涉及VLLM和Outlines模块，由于导入错误导致无法运行脚本，用户寻求关于解决ImportError的帮助。

https://github.com/vllm-project/vllm/issues/11410
这是一个用户提出需求的issue，主要涉及的对象是vllm，用户希望vllm支持两个新的功能并提出了具体的改进建议。

https://github.com/vllm-project/vllm/issues/11409
这是一个用户提出需求的类型为特性请求（Feature Request）的issue，主要涉及对象是V1版本的Pixtral-HF模型，用户希望支持在V1上使用与Transformers兼容的Pixtral检查点。

https://github.com/vllm-project/vllm/issues/11408
这是一个用户提出需求的issue，主要涉及vllm库在使用过程中可能导致GPU内存占用问题，用户希望在使用Torch进行反向传播时，vllm能释放其KV缓存内存，以避免Torch内存不足的情况。

https://github.com/vllm-project/vllm/issues/11407
这个issue是一个bug报告，涉及到release pipelines，主要是由于两个release pipelines可能引起race conditions。

https://github.com/vllm-project/vllm/issues/11406
这个issue是关于bug报告，主要涉及到Qwen2-VL LoRA Adapters插件不起作用的问题。导致这个bug的原因可能是插件与当前环境不兼容或者存在配置问题。

https://github.com/vllm-project/vllm/issues/11405
这个issue是用户提出需求，主要对象是"tool choice support"。由于用户需要支持此功能，导致提出了该问题。

https://github.com/vllm-project/vllm/issues/11404
这是一个bug报告类型的issue，主要涉及到VLLM的发布版本和PyPI索引的更新。这个问题的原因是之前的安装命令无法正常工作，用户提出这个issue来报告问题和寻求帮助。

https://github.com/vllm-project/vllm/issues/11403
这是一个bug报告，该问题单涉及的主要对象是OpenAI client tests。由于设置温度参数不为0导致CI出现问题，需要通过将温度设置为0.0来解决flakiness引入的错误。

https://github.com/vllm-project/vllm/issues/11402
这是一个Bug报告，主要涉及IBM Granite 3.1工具解析器失败的问题，可能是由于工具解析器无法正常工作而导致的。

https://github.com/vllm-project/vllm/issues/11401
这是一个性能优化类型的issue，主要涉及到block table在CPU和GPU之间的数据传输问题。由于每次都发送整个block table导致开销过大，因此提出了只传输差异数据的优化方案。

https://github.com/vllm-project/vllm/issues/11400
这是一个提出新功能的请求（RFC），主要涉及vLLM中的完全SPMD执行，原因是为了改善离线推理的吞吐量。

https://github.com/vllm-project/vllm/issues/11399
这是一个关于需求提案的issue， 主要涉及到vLLM工作者的灵活权重同步。由于参数服务器的集中化导致了高权重同步开销和灵活性上的不足。

https://github.com/vllm-project/vllm/issues/11398
这个issue属于bug报告类型，涉及的主要对象是vllm的安装过程。由于用户无法使用openvino backend安装vllm, 导致了这个issue的出现。

https://github.com/vllm-project/vllm/issues/11397
这个issue属于用户提出需求类型，主要对象是想要直接获取logits，该问题可能由于缺乏对该功能的理解或者相关功能尚未实现而导致。

https://github.com/vllm-project/vllm/issues/11396
这是一个功能改进的issue，涉及VLM项目中的缓存功能优化。原因是为了提高处理效率并重新定义输出的schema。

https://github.com/vllm-project/vllm/issues/11395
这是一个bug报告，主要对象涉及到代码中的`available_kv_cache_memory`和`baseline_memory_in_bytes`，可能是由于`baseline_memory_in_bytes`在`memory_for_current_instance`中的使用导致了bug的症状。

https://github.com/vllm-project/vllm/issues/11394
这是一个优化需求，主要涉及到V1 sampler中的topp和topk采样功能。

https://github.com/vllm-project/vllm/issues/11393
这个issue是一个Bugfix类型的报告，涉及主要对象是PixtralLargeInstruct2411模型。由于之前的Pixtral实现假设所有的`PixtralForConditionalGeneration`模型都具有相同的`[IMG_BREAK]`和`[IMG_END]`标记token id，但实际上并非如此，导致了PixtralLargeInstruct2411模型在0.6.5版本上出现了问题。

https://github.com/vllm-project/vllm/issues/11392
这是一个bug报告，主要涉及了v0.6.5版本的vllm中的流式工具调用响应与非流式版本的不一致问题。导致这个问题可能是由于流式特性结合工具调用仍存在bug。

https://github.com/vllm-project/vllm/issues/11391
这是一个用户提出需求的issue，主要涉及到KV cache的大小限制问题，导致无法设置较大的maxmodellen参数。

https://github.com/vllm-project/vllm/issues/11390
这是一个Bugfix类型的Issue，主要涉及到代码中的ReplicatedLinear layer和fully sharded LoRAs的支持问题，由于之前的修改引入了错误，导致使用带有门控目标的适配器时出现assertion错误。

https://github.com/vllm-project/vllm/issues/11389
这是一个Bugfix类型的issue，主要涉及对象是 xgrammar 和 outlines。由于升级到 0.1.11 版本后，outlines后端在将 SQL Lark grammar 转换为 GBNF 格式时出现问题，导致 xgrammar 无法正确地回退到 outlines 后端，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/11388
这是一个bug报告类型的issue，主要涉及marlin24压缩模型，导致用户在运行时报告设置dtype的问题。

https://github.com/vllm-project/vllm/issues/11387
这个issue类型是更新请求，主要涉及的对象是Dockerfile.rocm文件。

https://github.com/vllm-project/vllm/issues/11386
这是一个Bug报告，主要对象是vLLM中的`prompt["multi_modal_data"]`参数。由于在该参数为None或空字典时会导致错误，该问题可能是由于未对这种情况进行处理而导致的。

https://github.com/vllm-project/vllm/issues/11385
该issue类型为功能需求，主要对象是扩展APC以支持全局前缀缓存。由于APC缓存被清除时无法命中，需要在预填阶段中重用全局KV缓存池中的KV缓存，以及允许其他vllm实例直接使用全局KV缓存池中的KV缓存。

https://github.com/vllm-project/vllm/issues/11384
这是一个功能需求的issue，主要涉及到API Key的新增功能。原因可能是为了访问受保护的VLLM服务器而需要提供API Key。

https://github.com/vllm-project/vllm/issues/11383
这是一个需求报告，主要涉及AsyncLLMEngine的V1/V0行为切换方式问题，用户需要修改代码以使用V1。

https://github.com/vllm-project/vllm/issues/11382
这是一个用户提出需求的类型的 issue，主要涉及增强 vLLM 模型中的内存分配器来支持混合型模型，其中不同类型的层需要不同的KV缓存大小。

https://github.com/vllm-project/vllm/issues/11381
这是一个文档相关的问题，主要涉及到在RLHF (Weak learners in the space of non-random functions) 中解释nccl的要求。

https://github.com/vllm-project/vllm/issues/11380
这是一个bug报告，主要涉及到XGrammarLogitsProcessor在并行解码时出现的状态共享导致的错误。

https://github.com/vllm-project/vllm/issues/11379
这个issue属于用户提出需求类型，主要涉及如何在多节点（通过SLURM）下运行LLama 405B BF16进行离线推断。用户因无法直接在节点上运行命令，希望得到关于使用多个节点进行离线推断的参考脚本或资源。

https://github.com/vllm-project/vllm/issues/11378
这个issue是一个bug报告，主要涉及日志记录功能对于别名字段的处理问题，由于日志记录不考虑别名导致错误地报告字段被忽略的情况。

https://github.com/vllm-project/vllm/issues/11377
这是一个Bug报告，涉及到GLM-4模型的Guided decoding功能导致崩溃。

https://github.com/vllm-project/vllm/issues/11376
这是一个bug报告，主要涉及RequestMetrics对象，由于last_token_time被错误地报告为大于first_token_time，可能导致异常行为。

https://github.com/vllm-project/vllm/issues/11375
这是一个bug报告，涉及vLLM在tokenized embedding输入时崩溃的问题。导致这个问题的原因是由于配置错误导致错误的分词器被使用。

https://github.com/vllm-project/vllm/issues/11374
这是一个bug报告，主要对象是`vllm serve`命令，由于传递`--skip-tokenizer-init`参数导致vllm server无法启动。

https://github.com/vllm-project/vllm/issues/11373
这个issue是关于bug的，涉及到vllm项目中的precompiled wheel安装问题，由于precompiled wheel版本格式不符合要求导致安装问题。

https://github.com/vllm-project/vllm/issues/11372
这是一个关于需求的issue，主要涉及vLLM是否会支持flash-attention 3，用户想了解未来是否会支持该功能。

https://github.com/vllm-project/vllm/issues/11371
这是一个Bug报告，涉及的主要对象是LlavaOneVision模块。由于预处理器尝试从字典中获取图像而导致崩溃，且代码无法成功运行。

https://github.com/vllm-project/vllm/issues/11370
这是一个功能改进的issue，主要涉及到支持拥有合并输入处理器的模型，而不是分开的`input_processor`和`input_mapper`，导致需要对应支持该特性的模型进行修改和测试。

https://github.com/vllm-project/vllm/issues/11369
这个issue是一个功能增强类型的问题，涉及的主要对象是`DeviceMemoryProfiler`模块。原因是为了使`DeviceMemoryProfiler`在不同平台上的可用性更好，所以对`current_memory_usage()`函数进行了重构。

https://github.com/vllm-project/vllm/issues/11368
这是一个请求新增功能的issue，主要涉及到前端开发中的DRY Sampling功能。由于使用DRY可能导致高达25%的性能损失，因此需要对其进行优化。

https://github.com/vllm-project/vllm/issues/11367
这是一个Bug修复类型的issue，主要涉及日志记录功能存在的问题。由于当前的实现逻辑，在检测到多个`BaseModel`对象时即使未通过Pydantic验证过程，也会记录检测到的额外字段，这可能会误导用户认为请求有问题。

https://github.com/vllm-project/vllm/issues/11366
这是一个Bug报告，涉及的主要对象是vllm下的一个service操作过程中出现的异常错误导致服务偶发性中断问题。原因是CUDA内存访问错误引起的异常。

https://github.com/vllm-project/vllm/issues/11365
这个issue是用户提出的需求，主要对象是vLLM模型，用户希望添加对attention score输出的支持，因为当前的模型在推断过程中没有提供访问注意力得分的功能，这对于模型分析和可解释性研究至关重要。

https://github.com/vllm-project/vllm/issues/11364
这个issue类型属于用户提出需求类型，涉及的主要对象是vllm中的_IntermediateTensor_类。由于使用Pipeline Parallelism时，在_IntermediateTensor_对象中传递的张量数据中包含了名为'residual'的体积信息，用户想了解此信息的用途和推理计算中何时会被消耗。

https://github.com/vllm-project/vllm/issues/11363
这个issue是一个bug报告，主要涉及到请求中的response_format字段被忽略，可能是由于使用不正确的参数导致。

https://github.com/vllm-project/vllm/issues/11362
这是一个bug报告，主要涉及到平台插件支持功能，由于在全局命名空间中使用`current_platform`存在风险，可能导致未知问题和难以调试的情况。

https://github.com/vllm-project/vllm/issues/11361
这是一个Bug报告类型的Issue，主要涉及问题是关于vllm优先级调度的问题，导致请求的token_per_s设置未按照优先级正确工作。

https://github.com/vllm-project/vllm/issues/11360
这是一个用户提出需求的issue，主要对象是meta-llama/Prompt-Guard-86M模型，由于DebertaV2ForSequenceClassification架构暂不支持而导致使用该模型时发生数值错误。

https://github.com/vllm-project/vllm/issues/11359
这是一个文档相关的issue，涉及到向0.6.4版本的vllm添加向后兼容性，并修复了相关的问题。

https://github.com/vllm-project/vllm/issues/11358
这个issue属于Bugfix类型，涉及主要对象是vllm项目下的cohere2模型。原因是修复滑动窗口的问题导致的bug。

https://github.com/vllm-project/vllm/issues/11357
该issue是关于新增模型的贡献，并涉及到实现cohere2模型在vllm中。

https://github.com/vllm-project/vllm/issues/11356
这是一个bug报告，主要涉及vllm 0.6.3.post1版本在部署qwen2vl 72b时出现崩溃的问题。由于引擎循环异常导致了服务器崩溃。

https://github.com/vllm-project/vllm/issues/11355
这个issue是一个bug报告，涉及到自定义操作的错误消息提示，由于缺少早期错误消息导致问题 。

https://github.com/vllm-project/vllm/issues/11354
这个issue是一个bug报告，主要涉及压缩张量工具中的忽略逻辑错误，导致对于融合层未考虑输入参数提供的忽略列表而自动使用共享项目名称逻辑，修复后考虑忽略列表用于融合层。

https://github.com/vllm-project/vllm/issues/11352
这是一个Bug报告。该问题涉及的主要对象是V100模型。导致此Bug的原因是V100无法使用`-enable-chunked-prefill`方法与`dtype`为`float16`一起使用。

https://github.com/vllm-project/vllm/issues/11351
这是一个合并请求（pull request）类型的issue，主要涉及Dockerfile的清理，由于需要在上游仓库的另一个分支中测试才能合并。

https://github.com/vllm-project/vllm/issues/11350
这个issue类型是需求提出，主要对象是要求添加GHA来检查失效的超链接。原因可能是希望确保项目中的超链接都能够正常访问，避免死链影响用户体验。

https://github.com/vllm-project/vllm/issues/11349
这是一个建议性质的问题，主要对象是在图像捕获期间添加tqdm进度条，由于缺乏视觉进度反馈而导致用户提出建议。

https://github.com/vllm-project/vllm/issues/11348
这是一个功能追加类型的issue，涉及Kernel中添加ExLlamaV2权重量化支持。由于还在进行中，具体原因和症状需要进一步的补充信息来分析。

https://github.com/vllm-project/vllm/issues/11347
这个issue是用户提出需求，想要在vllm中支持answerdotai/ModernBERT-large模型。

https://github.com/vllm-project/vllm/issues/11346
这是一个bug报告类型的issue，主要涉及vllm在启用`VLLM_TORCH_PROFILER_DIR`时无法输出profile的问题。导致这一症状的原因可能是启用了torch profiler但无法正确输出profile文件。

https://github.com/vllm-project/vllm/issues/11345
这是一个性能问题的issue，涉及到分布式计算性能比较，用户尝试复现并对比性能，并提出了使用更高TTFL和ITL时的问题。

https://github.com/vllm-project/vllm/issues/11344
这是一个需求类型的issue，主要涉及VLLM执行器中关于GPU ID和工作器排名的类型检查。由于缺少对变量类型的明确定义，可能导致混淆，用户提出加强Mypy类型检查以澄清GPU ID可以是整数或字符串。

https://github.com/vllm-project/vllm/issues/11343
这是一个Bug报告，主要涉及Paligemma 2模型加载错误，用户提出了由于环境问题导致的Bug。

https://github.com/vllm-project/vllm/issues/11342
这是一个Bug报告，涉及主要对象为VLLM的Multi-Node CPU推断，由于缺少`intel_extension_for_pytorch`模块导致了报错。

https://github.com/vllm-project/vllm/issues/11341
这是一个bug报告类型的issue，涉及的主要对象是VLM（Visual Language Model）。原因是V1 multimodal cache与merged multimodal processor不兼容，导致性能下降，解决方案是在merged multimodal processor内部添加cache。

https://github.com/vllm-project/vllm/issues/11340
这是一个Bug报告，该问题涉及VLLM中的CUDA非法内存访问问题，可能由于特定数值的--max-num-seqs参数导致。

https://github.com/vllm-project/vllm/issues/11339
这是一个代码优化（code optimization）类型的issue，主要涉及LRUCache相关的类合并及类型注解的改进。原因可能是LRUDictCache和LRUCache本质上相同，合并后提高代码精准性。

https://github.com/vllm-project/vllm/issues/11338
这是一个bug报告，涉及到CPU compressed-tensors测试出现问题，可能由于`sparse_cutlass_supported`导致测试失效。

https://github.com/vllm-project/vllm/issues/11337
这是一个关于需求的issue，主要涉及多模态模型的推理环境扩展问题，由于内存溢出导致无法将推理上下文长度扩展到128k。

https://github.com/vllm-project/vllm/issues/11336
这个issue属于项目优化类型，涉及到移除项目中未被引用的模块文件。原因是该文件已不再被项目引用，可以安全地删除。

https://github.com/vllm-project/vllm/issues/11335
这是一个bug报告，主要涉及vllm在处理长内容（9k字）时发生崩溃的问题，可能是由于无法读取提交哈希导致的。

https://github.com/vllm-project/vllm/issues/11334
这是一个用户提出的需求，主要涉及对T5模型添加自定义attention偏置支持。源自于以前的工作，提供了更通用的新增功能，允许在PagedAttention内使用任意自定义的attention偏置。

https://github.com/vllm-project/vllm/issues/11333
这是一个Bug报告，主要涉及Pixtral HF代码的清理和修复。该问题可能导致Pixtral HF的最大图像令牌显示不正确，并清理了已经集成到合并的多模式处理器中的未使用代码。

https://github.com/vllm-project/vllm/issues/11331
这是一个bug报告，主要涉及的对象是关于在POWER10上运行例子时出现的dtype选择错误。由于POWER10架构不支持FP16，导致在选择"auto"类型时失败，因此需要修改选择为BF16来解决问题。

https://github.com/vllm-project/vllm/issues/11330
该issue类型为需求提出，涉及对象为在vllm中添加`transformers`作为后端支持。因为需求是为了支持更多的模型并增加tensor parallel支持。

https://github.com/vllm-project/vllm/issues/11329
这是一个bug报告，涉及的主要对象是vLLM中的FP8 kvCache。由于在使用v1 engine时遇到了与flash attention kernel相关的问题，导致了RuntimeError的bug。

https://github.com/vllm-project/vllm/issues/11328
这个issue是一个用户提出的需求，主要涉及到支持pytorch自定义操作可插拔性。可能的原因是用户希望能够注册特定平台的`forward`功能。

https://github.com/vllm-project/vllm/issues/11327
这个issue是关于bug报告，涉及的主要对象是在POWER10上运行examples/offline_inference.py时出现的reshape_and_cache_cpu_impl操作未对half数据类型进行实现，导致报错。

https://github.com/vllm-project/vllm/issues/11326
这是一个缺少具体描述的问题汇报，涉及项目中的合并操作。

https://github.com/vllm-project/vllm/issues/11325
这是一个bug报告，主要涉及的对象是"Molmo"。原因是输入处理与多模态数据预处理混合，导致需要临时解决方案来修复问题。

https://github.com/vllm-project/vllm/issues/11324
这是一个功能需求的issue，涉及主要对象是为设备特定的通信器添加基类，通过避免在每个通信器操作器中进行繁琐的调度来简化`GroupCoordinator`的实现。

https://github.com/vllm-project/vllm/issues/11323
这个issue是关于如何使用torch_compile的用户提问，主要涉及的对象是torch_compile功能。此问题是由于当前环境PyTorch版本与其他依赖库版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/11322
这是一个用户提出需求的issue，主要涉及到在使用vllm中同时启用guided-decoding、chunked-prefill和prefix-caching时出现的问题。

https://github.com/vllm-project/vllm/issues/11321
这是一个Bug报告，主要涉及vLLM v0.6.5的Python-only构建过程出现错误。原因可能是环境中的Python版本和依赖项不兼容所致。

https://github.com/vllm-project/vllm/issues/11320
这个issue是关于性能问题的讨论，主要涉及到在将Embedding模型提供给GPU时由于CPU瓶颈导致性能下降，用户寻求改进性能的建议。

https://github.com/vllm-project/vllm/issues/11319
该issue类型是文档更新，主要涉及到修改默认的max_num_batch_tokens参数。原因是文档中的数值需要与代码保持一致。

https://github.com/vllm-project/vllm/issues/11318
这是一个用户提出需求的 issue，主要涉及支持自定义torch.compile后端键，旨在使常见模块更平台无关。

https://github.com/vllm-project/vllm/issues/11317
这个issue类型是性能问题报告，主要涉及vllm0.6.5加载GLM4-9B-Chat模型时，动态加载lora模型导致推理性能下降的情况。

https://github.com/vllm-project/vllm/issues/11316
这是一个Bug报告，主要涉及的对象是GPT2模型输出问题。由于输出结果只有"!"，导致用户无法得到正确答案。

https://github.com/vllm-project/vllm/issues/11314
这是一个关于支持0.2.0 flashinfer的更改的问题，属于需求更改类型，涉及主要对象为数据类型和包装器。

https://github.com/vllm-project/vllm/issues/11313
这是一个bug报告，主要涉及测试代码的拓展和反馈错误。问题主要是由于一些后端支持不完善和模型输出质量低导致的测试不稳定。

https://github.com/vllm-project/vllm/issues/11312
这个issue是关于BUG报告的，涉及的主要对象是对话与语法相关的功能。由于未来版本中将删除`TRANSFORMERS_CACHE`，使用`HF_HOME`替代，导致出现了与对话和语法相关的错误。

https://github.com/vllm-project/vllm/issues/11311
这是一个Bugfix类型的Issue，主要涉及的对象是针对CUDA编译的sparse CUTLASS库。由于检查版本错误导致代码编译失败，需要修正为正确的CUDA版本检查并添加新的函数以更好地处理不支持的情况。

https://github.com/vllm-project/vllm/issues/11310
这是一个优化性质的issue，主要涉及V1版本的VLLM项目中的prefix caching逻辑，由于在`self._touch`中删除了`evictable_computed_blocks`，在`allocate_slots`中不再需要对它们进行特殊处理，导致`num_evictable_computed_blocks`常为零，而此问题不会对性能产生影响。

https://github.com/vllm-project/vllm/issues/11309
这是一个bug报告类型的issue，主要涉及更新Dockerfile中的torch_xla版本导致的问题。

https://github.com/vllm-project/vllm/issues/11308
这是一个bug报告，涉及到V1版本中多模态模型的性能分析功能的修复问题。

https://github.com/vllm-project/vllm/issues/11307
这是一个bug报告，涉及IBM Granite 3.1模型的工具调用方式变化导致工具聊天模板无法使用的问题。

https://github.com/vllm-project/vllm/issues/11306
这个issue属于用户提出需求类型，主要涉及模型支持问题，用户希望添加一个projection layer到模型中。

https://github.com/vllm-project/vllm/issues/11305
这是一个功能增强请求，主要涉及V1版本的VLM。由于VLM前缀缓存表现较佳，因此这个问题提议默认启用VLM处理器缓存以获得最佳性能。

https://github.com/vllm-project/vllm/issues/11304
这是一个bug报告类型的issue，该问题涉及到minicpmv测试，因为在性能分析更改后，测试现在占用了过多的内存。

https://github.com/vllm-project/vllm/issues/11303
这是一个关于需求的提议，涉及VLM下的Qwen2-Audio模块中多模态处理器的合并与优化。

https://github.com/vllm-project/vllm/issues/11302
这是一个修改Dockerfile构建用于ARM64和GH200的CI/Build的issue，涉及主要对象是vllm的构建过程。由于ARM64系统构建过程被认为是针对特定PyTorch版本的构建，所以采用了特定的构建方法。

https://github.com/vllm-project/vllm/issues/11301
这是一个关于bug修复的issue，主要涉及xformers中multi_query_kv_attention的测试。其中发现之前未测试的情况会导致测试用例失败。

https://github.com/vllm-project/vllm/issues/11299
这是一个bug报告，涉及的主要对象是vllm库中的模型配置。由于加载的checkpoint模型类型与Transformers库不兼容，导致数值错误(ValueError)产生。

https://github.com/vllm-project/vllm/issues/11298
这是一个代码优化类型的issue，主要涉及的对象是`vllm.utils`文件。导致这个问题的原因是文件变得混乱无序，需要进一步整理优化。

https://github.com/vllm-project/vllm/issues/11297
这是一个关于功能使用的问题，主要涉及到vllm中关于离线beam搜索推理的参数设置。由于最近beam搜索API发生了变化，导致用户不清楚如何在离线推理中触发beam搜索。

https://github.com/vllm-project/vllm/issues/11296
这是一个用户提出需求类型的issue，主要涉及发布arm64镜像到DockerHub。这个需求是由于构建arm64镜像耗时较长，希望提供arm64镜像来促进在低端arm64平台上的更多使用。

https://github.com/vllm-project/vllm/issues/11295
这个issue类型是优化建议，主要对象是Mllama模型中的encoder部分，提出了避免中间状态拷贝以提高性能的建议。

https://github.com/vllm-project/vllm/issues/11294
这是一个性能优化建议类型的 issue，主要对象是cache_engine.py文件，由于torch.zeros操作耗时较长，可以使用直接复制方法来提高速度。

https://github.com/vllm-project/vllm/issues/11293
这是一个优化建议，旨在改进代码执行速度。

https://github.com/vllm-project/vllm/issues/11292
这个issue是关于性能优化的提议，主要涉及到cache_engine.py文件。原因是使用torch.zeros函数导致运行时间较长，建议改用直接复制的方式以提高速度。

https://github.com/vllm-project/vllm/issues/11291
这是一个性能优化类型的issue，主要涉及到cache_engine.py文件。由于torch.zeros函数消耗大量时间，建议使用直接复制方法来改进速度。

https://github.com/vllm-project/vllm/issues/11290
这是一个bug报告，涉及vllm的后端限制的删除，由于原因是某些命令在特定情况下可以正常工作，因此需要移除后端限制。

https://github.com/vllm-project/vllm/issues/11289
这是一个Bug报告类型的Issue，涉及到使用outlines和guided_choice时导致服务hang的问题。

https://github.com/vllm-project/vllm/issues/11288
这是一个优化建议，主要涉及到缓存引擎的更新。这个问题由于torch.zeros在消耗大量时间，建议使用直接拷贝方法来提高速度。

https://github.com/vllm-project/vllm/issues/11287
这是一个需求或功能补充类型的issue，涉及主要对象为OpenAI版本更新，由于缺少`ChatCompletionContentPartInputAudioParam`导致未能支持输入音频功能。

https://github.com/vllm-project/vllm/issues/11286
这是一个性能问题报告，主要涉及vLLM在处理长上下文时解码速度下降，可能是因为随着prompt长度增加，page attention访问频率增加导致。

https://github.com/vllm-project/vllm/issues/11285
这是一个用户提出需求的类型，主要涉及vllm在不同网络环境下是否支持pipeline并行性的问题。用户想了解是否可以在不同网络的机器上使用vllm的pipeline并行功能。

https://github.com/vllm-project/vllm/issues/11284
这是一个Bug报告，涉及的主要对象是vLLM工具的调用功能。由于调用功能异常，可能导致请求出现异常，用户寻求解决此问题的帮助。

https://github.com/vllm-project/vllm/issues/11283
这是一个bug报告，涉及主要对象是vllm-flash-attn以及vllm，原因是版本不兼容导致的问题。

https://github.com/vllm-project/vllm/issues/11282
这是一个bug报告，涉及的主要对象是vllm的模型服务功能。由于代码运行时出现了多处理相关的错误，导致了报错信息的显示。

https://github.com/vllm-project/vllm/issues/11281
这个issue类型是bug报告，主要涉及的对象是int8 w8a8 quantization data set，由于CUDA error导致出现RuntimeError，用户请求帮助解决在生成模型数据集时发生的错误。

https://github.com/vllm-project/vllm/issues/11280
这是一个bug报告，主要涉及Whisper模型的实现。导致问题的原因可能是性能不佳，尤其在较大的批处理大小下。

https://github.com/vllm-project/vllm/issues/11279
这是一个Bug报告，涉及的主要对象是VLLM工具。由于PyTorch版本与CUDA版本不匹配，导致生成的工具参数无效。

https://github.com/vllm-project/vllm/issues/11278
这个issue是一个技术性问题，类型为更新链接以修复CI问题，涉及的主要对象是XPU模块。

https://github.com/vllm-project/vllm/issues/11277
该issue类型为功能新增，主要涉及的对象是vLLM Neuron后端，由于需要支持chunked prefill with flash attention，开发引入了一个NKI-based kernel。

https://github.com/vllm-project/vllm/issues/11276
这是一个用户提出需求的issue，主要涉及如何设置在生成标记时动态调整温度系数的功能。该问题可能由于当前的`sampling_params`无法实现此特性而产生。

https://github.com/vllm-project/vllm/issues/11275
这是一个关于bug的报告，主要对象是ray worker initialization time。由于ray worker初始化时序列化地调用`ray.get(worker.get_node_ip.remote())`导致初始化时间大量增加。

https://github.com/vllm-project/vllm/issues/11274
这是一个类型为"功能移除"的issue，主要对象是Github Action Release Workflow。

https://github.com/vllm-project/vllm/issues/11273
这是一个用户提出需求类型的issue，主要涉及对象是更新rubra vllm。由于原因未清楚描述，用户可能遇到了问题或者需要更新rubra vllm。

https://github.com/vllm-project/vllm/issues/11272
该issue类型为性能测试，主要涉及Ray初始化过程。由于需要基本的benchmarking来评估Ray初始化的性能表现，提升系统性能。

https://github.com/vllm-project/vllm/issues/11271
这是一个bug报告，主要涉及vllm库中的Tokenzier对象，由于属性错误导致了程序无法正常运行。

https://github.com/vllm-project/vllm/issues/11270
这个issue属于文档更新类型，主要涉及vllm TPU镜像在部署文档中的更新，可能由于现有文档未包含vllm TPU镜像信息而导致用户寻找困难。

https://github.com/vllm-project/vllm/issues/11269
这是一个bug报告，主要涉及的对象是加载gemma2模型时的kv cache scales。这个问题由于缺少校准的kv缓存比例尺而导致加载失败。

https://github.com/vllm-project/vllm/issues/11268
这个issue是关于bug报告，涉及到sentence_bert_config.json文件的404 Client Error问题。可能是由于引入了默认加载句子转换器配置，无论实际模型类型是句子转换器还是普通CLMs，导致这个问题的出现。

https://github.com/vllm-project/vllm/issues/11267
这是一个功能需求的issue，涉及在benchmark_throughput.py中添加对LoRA基准测试的支持。

https://github.com/vllm-project/vllm/issues/11266
这是一个Bug报告，涉及vllm-cpu环境在MacOS上无法运行docker的问题，可能是由于模块解析错误导致无法正常运行。

https://github.com/vllm-project/vllm/issues/11265
这是一个用户提出需求的issue，主要涉及对象是为了在vllm中加入ray[default]以便实现分布式推理功能。

https://github.com/vllm-project/vllm/issues/11264
这是一个Bug报告类型的issue，主要涉及的对象是温度设置在测试中的问题。原因是设置温度为0.7解决了entrypoints flaky错误引发的问题。

https://github.com/vllm-project/vllm/issues/11263
这是一个bug报告，涉及到vllm项目中的phi3v模块的mm_processor_kwargs测试。问题是由于`num_crops`未在`get_max_phi3v_image_tokens`中公开导致测试无法通过，并且在CI环境中测试未能触发，原因是测试管道在那时没有更新。

https://github.com/vllm-project/vllm/issues/11262
这个issue类型是空白的不完整的问题报告，该问题单涉及的主要对象是VLLM项目。

https://github.com/vllm-project/vllm/issues/11261
这是一个功能需求的issue，涉及主要对象是为vllm项目添加CPU docker image构建流水线。

https://github.com/vllm-project/vllm/issues/11260
这是一个bug报告类型的issue，涉及vLLM在TPU上与Ray结合使用时pipeline-parallel-size参数不被支持，可能导致服务启动错误。

https://github.com/vllm-project/vllm/issues/11259
这是一个bug报告，主要涉及的对象是vllm项目中的block size设置。导致这个bug的原因是之前的更改不支持较大的block sizes，需要恢复对较大block sizes的支持。

https://github.com/vllm-project/vllm/issues/11258
这是一个用户提出需求的issue，主要涉及模型的重构问题；用户希望将Qwen2-VL改造为使用合并的多模态处理器。

https://github.com/vllm-project/vllm/issues/11257
这个issue类型是功能改进，主要涉及Executor的变化，用户提出了使用`get_node_and_accelerator_ids`替换`get_node_and_gpu_ids`的需求。

https://github.com/vllm-project/vllm/issues/11256
这个issue类型是功能需求提出，主要涉及的对象是VLLM项目的平台无关执行器。

https://github.com/vllm-project/vllm/issues/11255
这是一个用户提出需求的issue，主要涉及vLLM模型中LoRA支持的问题。用户反馈使用LoRA适配器后推理结果明显较差，希望能够支持多模态模型的LoRA，同时询问是否有临时解决方案和预计支持时间。

https://github.com/vllm-project/vllm/issues/11254
这是一个Bug报告，涉及vllm的grounding任务效果不如transformers推理，可能原因是模型在位置预测上不准。

https://github.com/vllm-project/vllm/issues/11253
该issue类型为提出需求，主要涉及LoRA支持与Mistral模型的兼容性。原因可能是LoRA支持在Ultravox模型中的应用也可适用于使用LlamaForCasualLM架构的Mistral模型。

https://github.com/vllm-project/vllm/issues/11252
这个issue类型是需求提出，主要对象是xformers backend，Gemme2团队观察到在推理过程中去除softcapping会导致非常轻微的差异。

https://github.com/vllm-project/vllm/issues/11251
This is a bug report issue related to torch.OutOfMemoryError for vllm 0.6.4.post1, while 0.6.3.post1 is working.

https://github.com/vllm-project/vllm/issues/11250
这是一个Bug报告，涉及的主要对象是Hermes工具选择无法支持格式为'string'的问题。产生这个Bug的原因是auto工具选择已被启用，导致了并行工具调用客户端选项被忽略。

https://github.com/vllm-project/vllm/issues/11249
这是一个bug报告，涉及的主要对象是vllm在ROCm环境下使用AWQ模型时出现的问题，可能由于安装autoawq不成功导致AWQ模型无法正常运行。

https://github.com/vllm-project/vllm/issues/11248
这是一个Bug报告类型的Issue，主要涉及Llama-3.2-90B-Vision-Instruct模型的使用。由于使用lm_format_enforcer或特定schema会导致崩溃。

https://github.com/vllm-project/vllm/issues/11247
这个issue是一个Bug报告，涉及的主要对象是disaggregated prefilling功能。由于TP=2时导致hang的问题，可能是由于程序中的bug或者逻辑错误导致。

https://github.com/vllm-project/vllm/issues/11246
这是一个bug报告，主要涉及vllm工具的test。导致这个问题的原因可能是测试在主分支上出现了问题。

https://github.com/vllm-project/vllm/issues/11245
这个issue类型属于bug报告，涉及主要对象是CI（持续集成）。由于有拼写错误导致需要修复，但未提供具体信息。

https://github.com/vllm-project/vllm/issues/11244
这是一个测试用例相关的issue，旨在添加关于特定功能测试的测试。

https://github.com/vllm-project/vllm/issues/11243
该问题单为用户提出需求类型，主要涉及的对象为VLLM的核心功能。

https://github.com/vllm-project/vllm/issues/11242
这个issue类型是功能改进，主要涉及的对象是代码中注册的`shutdown`函数，由于使用`weakref.finalize`替代`atexit`导致了更好的实现方式。

https://github.com/vllm-project/vllm/issues/11241
这个issue属于性能优化类别，问题主要涉及了RMSNorm内核的比较和性能测试。这个issue是为了比较vLLM自定义算子和Flashinfer之间内核差异的性能测试。

https://github.com/vllm-project/vllm/issues/11240
这个issue类型是新增模型支持，主要涉及的对象是Ovis1.6-Gemma2-9B模型，用户提出了需要添加对该模型的支持。

https://github.com/vllm-project/vllm/issues/11239
这个issue是一个功能需求提议，涉及到性能优化和CI/Build过程中的GPU资源管理，由于需要减少GPU资源的消耗并允许并发作业运行，故引入了新的CPU-based runners和Modal client来动态分配GPU资源。

https://github.com/vllm-project/vllm/issues/11238
这个issue类型是bug报告，主要对象是Phi-4模型，由于当前vllm无法与其兼容而导致问题。

https://github.com/vllm-project/vllm/issues/11237
这是一个功能改进的issue，主要涉及到API server和AsyncLLM的性能优化和错误处理。其中存在的问题是需要提高API server和AsyncLLM的性能，并改进错误处理，以及减少zmq sockets的使用。

https://github.com/vllm-project/vllm/issues/11236
该issue属于用户提出需求类型，主要涉及的对象是DeepSeek-VL2模型的支持问题，用户想要了解为何还未支持该模型。

https://github.com/vllm-project/vllm/issues/11235
这是一个文档更新类型的issue，主要涉及多节点分布式服务环境下使用GPU性能计数器的问题。由于启动脚本在没有管理员权限下启动Docker容器，导致GPU性能分析和追踪功能受限，用户希望文档能提供解决办法。

https://github.com/vllm-project/vllm/issues/11234
这个issue类型是改进请求，主要涉及LoRA内核的优化和性能改进。由于对sgmv的水平融合操作，带来了性能提升和单位测试的更新。

https://github.com/vllm-project/vllm/issues/11233
这是一个Bug报告，涉及的主要对象是在CUDA图捕获过程中使用正确设备初始化GPU数据时出现的问题。

https://github.com/vllm-project/vllm/issues/11232
这是一个Bug报告类型的Issue，主要涉及ROCm和某些需要'trust-remote-code'的HF模型，在这种环境下会出现VLLM_RPC_TIMEOUT和加载失败的问题。

https://github.com/vllm-project/vllm/issues/11231
这是一个Bug报告，涉及的主要对象是VLLM模型。导致这个Bug的原因是模型不支持`reward`任务，而只支持`embedding`任务。

https://github.com/vllm-project/vllm/issues/11230
这是一个bug报告，涉及到vllm的VRAM使用情况显示异常，导致硬件资源几乎无法使用的问题。

https://github.com/vllm-project/vllm/issues/11229
这个issue是一个关于bug报告类型的问题，主要涉及到无法在vllm中注册自定义模型的问题。由于作者想要注册一个稍微修改过的Qwen2.532BInstruct模型，但尝试注册时遇到了错误。

https://github.com/vllm-project/vllm/issues/11228
这是一个需求性问题，主要涉及文档的重新排序。由于乱序的示例使得在IDE开发过程中难以找到需要修改/调试的功能。

https://github.com/vllm-project/vllm/issues/11226
这个issue属于bug报告类型，主要涉及到helm chart中的hpa（Horizontal Pod Autoscaler），因为错误的部署引用导致了问题。

https://github.com/vllm-project/vllm/issues/11225
这是一个bug报告类型的issue，主要涉及到`EngineArgs`的初始化问题，由于在`EngineArgs`初始化阶段调用与平台相关的代码会导致错误，因此需要清理掉在`EngineArgs`初始化阶段的平台指定代码。

https://github.com/vllm-project/vllm/issues/11224
这是一个Bug报告类型的Issue，涉及的主要对象是vLLM对Cerebras GPT模型进行采样时出现的错误。由于在使用Cerebras的模型时，vLLM出现'NoneType' object is not iterable的TypeError错误导致无法执行。

https://github.com/vllm-project/vllm/issues/11223
该issue类型为用户提出需求，请教问题，主要涉及离线部署文本语音模型和知识库作为个性化问答模型的指导。原因可能是用户对如何实现该功能及可用性有疑问。

https://github.com/vllm-project/vllm/issues/11222
这是一个功能需求类型的issue，涉及平台插件框架的添加。由于需要添加新的平台插件框架，故引起了这个issue。

https://github.com/vllm-project/vllm/issues/11221
这是关于如何使用vllm中intern_vit.py来加载intern_vit模型的使用问题，属于用户需求咨询类型。

https://github.com/vllm-project/vllm/issues/11220
这是一个bug报告，主要涉及的对象是vLLM中的`model_execute_time`计算问题。这个问题由于ranks 0到N-2的执行时间异常小，导致与rank N-1相比有12个数量级的差异。

https://github.com/vllm-project/vllm/issues/11219
这个issue是一个Bugfix类型的报告，涉及到ChatCompletionRequest中temperature默认值的修复。导致这个bug的原因是ChatCompletionRequest和SamplingParams中temperature的默认值不一致。

https://github.com/vllm-project/vllm/issues/11218
这是一个关于发布跟踪的issue，主要涉及版本发布的计划和待处理的Pull Requests。

https://github.com/vllm-project/vllm/issues/11217
这个issue是一个bug报告，主要涉及到dynamo overhead measurement的重构，因为之前的PR存在问题，使用了错误的参数导致症状不正确。

https://github.com/vllm-project/vllm/issues/11216
这个issue类型是性能问题报告，主要涉及的对象是dynamo guard evaluation overhead，原因是在运行过程中存在guard evaluation overhead约为0.3ms的情况。

https://github.com/vllm-project/vllm/issues/11215
这个issue是关于安装问题的帮助请求，涉及主要对象为vllm软件用户。该问题可能由于缺少CUDA库或链接错误导致无法正确安装vllm。

https://github.com/vllm-project/vllm/issues/11214
这是一个优化建议类型的issue，主要涉及到提高输入准备的性能。

https://github.com/vllm-project/vllm/issues/11213
这是一个bug报告，涉及到vllm中的sliding_window对象，在else分支中未定义导致错误。

https://github.com/vllm-project/vllm/issues/11212
该issue是关于优化Dockerfile构建过程，属于性能提升类型，涉及的主要对象是ARM64系统。通过简化构建流程和优化要求处理，以确保在ARM64+CUDA兼容性下正确安装torch和bitsandbytes，以解决构建过程繁琐的问题。

https://github.com/vllm-project/vllm/issues/11211
这个issue类型是功能改进，涉及主要对象为在ARM64系统上构建Dockerfile，通过简化构建流程并优化要求处理来确保torch和bitsandbytes在ARM64+CUDA兼容性上正确安装。

https://github.com/vllm-project/vllm/issues/11210
这是一个bug报告，主要涉及的对象是VLLM中的内存分析功能。由于存在CUDA OOM（Out of Memory）的潜在风险，需要正确测量图像语言模型的峰值内存使用量。

https://github.com/vllm-project/vllm/issues/11209
这个issue类型是功能需求，涉及的主要对象是LoRA模块和Jamba、Mamba模型；用户提出这个需求是因为希望添加LoRA支持，包括对建模和权重加载的变化。

https://github.com/vllm-project/vllm/issues/11208
这是一个Bug报告，该问题涉及的主要对象是vllm在ray集群中启动时遇到极慢的启动时间问题，可能是由于重复调用ray.init()导致。

https://github.com/vllm-project/vllm/issues/11207
这是一个代码维护类的issue，主要涉及到multi-modal processor的优化。原因在于不再需要将multi-modal数据传递给替换函数。

https://github.com/vllm-project/vllm/issues/11206
这是一个bug报告类型的issue，涉及主要对象是vllm模型在docker容器中的量化问题。导致这个问题的原因可能是在加载、量化和共享模型时GPU消耗了过多的VRAM。

https://github.com/vllm-project/vllm/issues/11205
这是一个Bug报告，涉及到GPTQ Marlin kernel输出出现损坏的问题，可能是由于两个潜在的竞态条件导致的。

https://github.com/vllm-project/vllm/issues/11204
这是一个Bug报告，主要对象是代码中的Reproducibility问题。由于当tp大于1时，无法保证再现性，可能导致出现不一致的结果。

https://github.com/vllm-project/vllm/issues/11203
这是一个用户提出需求的issue，主要对象是Cohere2ForCausalLM模型，用户希望为其添加类似HuggingFace Transformers中支持的滑动窗口功能。

https://github.com/vllm-project/vllm/issues/11202
这是一个用户提出需求的issue，主要涉及如何在私人仓库中运行模型，可能由于缺乏明确的使用说明而导致问题。

https://github.com/vllm-project/vllm/issues/11201
这是一个升级维护请求，主要涉及bitsandbytes库，用户请求将其升级到最新版本0.45.0。

https://github.com/vllm-project/vllm/issues/11200
这个issue类型是bug报告，涉及的主要对象是PunicaWrapperBase类。由于上一个PR未注意到一个小问题，现在进行修复。

https://github.com/vllm-project/vllm/issues/11199
这是一个bug报告类型的issue，针对VLM中的Fully dynamic prompt replacement功能的问题。导致问题的原因是PixtralHF预处理中的PromptReplacement不正确。

https://github.com/vllm-project/vllm/issues/11198
这个issue属于代码重构类型，主要涉及Ultravox模块，用户提出需要使用合并的输入处理程序来改进代码。

https://github.com/vllm-project/vllm/issues/11197
这是一个用户提出需求的issue，主要涉及文档的更新。该问题由于缺乏相关文档而导致用户需求补充文档。

https://github.com/vllm-project/vllm/issues/11196
该问题类型为功能增强（feature enhancement），主要对象是 VLM 的离线批量推理基准测试。由于希望添加支持其他框架进行比较，因此被提出。

https://github.com/vllm-project/vllm/issues/11195
这个issue类型为功能需求，主要对象是进行离线批量推断基准测试的VLM模型。这个需求可能是为了在MMMPro视觉方面添加其他框架支持来进行比较。

https://github.com/vllm-project/vllm/issues/11194
这是一个升级请求类的issue，涉及的主要对象是FlashInfer attention backend。该issue由于需要升级到v0.2.0版本而被创建。

https://github.com/vllm-project/vllm/issues/11193
该issue类型为Feature Request，主要涉及添加RWKV6 finch linear attention模型。该问题是为了支持RWKV模型，因此用户希望添加RWKV6 finch linear attention模型以解决问题。

https://github.com/vllm-project/vllm/issues/11192
这是一个关于性能问题的问题报告，主要涉及对象是在线服务和Pipeline Parallel。由于数据传输问题导致 GPU 之间的 NVLink 通道未正常连接，从而导致推理吞吐量显著下降。

https://github.com/vllm-project/vllm/issues/11191
这是一个关于安装问题的bug报告，主要涉及vllm在GH200机器上安装时出现了cusparse.h缺失的问题。

https://github.com/vllm-project/vllm/issues/11190
这个issue属于bug报告类型，涉及的主要对象是关于request取消和断开连接检查的修复。导致这个问题的原因是使用`request.is_disconnected()`进行轮询检查会引入更多问题，而基于已有模式的等待断开连接消息来取消工作会更加可靠。

https://github.com/vllm-project/vllm/issues/11189
这是一个Bug报告，主要涉及的对象是Machine Inf2 EC2 Instance，由于`neuronxcc`错误地指向`trn1`而非`inf2`，导致了Serving失败。

https://github.com/vllm-project/vllm/issues/11188
这是一个Bug报告，涉及的主要对象是Speculative decoding draft acceptance rate decreasing over time。由于性能突然出现了问题，用户希望尽快启用该功能。

https://github.com/vllm-project/vllm/issues/11187
这是一个功能增强的issue，主要涉及VLM（multimodal language model），旨在为其添加前缀缓存以提高性能。

https://github.com/vllm-project/vllm/issues/11186
这个issue类型是bug报告，涉及的主要对象是缓存机制（cache）。这个bug是由于`allocate_slots()`计算的推断计算标记数量总是与块大小相乘，导致最后一个块无法正确缓存，破坏了前缀缓存的哈希块链。

https://github.com/vllm-project/vllm/issues/11185
这是一个Bug报告，涉及V1版本中使用VLLM_ENABLE_V1_MULTIPROCESSING和TP size > 1时的引擎核心处理性能问题。

https://github.com/vllm-project/vllm/issues/11184
这是一个Bug报告，涉及到Bert tokenizer将某些标记标记为`UNK`的问题。这个问题可能是由于模型输出不符合预期而引起的。

https://github.com/vllm-project/vllm/issues/11183
这个issue属于更新需求，涉及到更新compressedtensor到最新版本0.8.1。由于当前使用的版本可能存在功能改进或者bug修复等需求，导致用户希望将库更新到最新版本以获得这些改进。

https://github.com/vllm-project/vllm/issues/11182
这是一个bug报告，针对V1版本中关于TP trust-remote-code的问题，修复了一个错误。

https://github.com/vllm-project/vllm/issues/11181
这是一个用户提出需求的issue，主要对象是希望支持新模型Cohere2 (Command R7B)，但可能会遇到难点，因为模型中的部分特征和架构不符合当前支持的模型。

https://github.com/vllm-project/vllm/issues/11180
这个issue类型为bug报告，涉及的主要对象是vLLM中的ARM支持，由于outlines版本问题导致ARM构建失败。

https://github.com/vllm-project/vllm/issues/11179
这个issue类型是bug报告，涉及的主要对象是vLLM的ARM支持，由于outlines v0.1.9在ARM上缺少架构支持，导致ARM vLLM容器构建失败。

https://github.com/vllm-project/vllm/issues/11178
这是一个Bug报告类型的issue，涉及的主要对象是ARM vLLM container build。由于缺少arm64支持导致outlines v0.1.9安装失败，需要更新requirementscommon.txt中的outlines版本。

https://github.com/vllm-project/vllm/issues/11177
这是一个功能需求类型的issue，主要涉及的对象是benchmark_serving.py脚本。由于需要测试内存缓存（MM cache）的HIT/MISS情况，开发者提出了增加重复图片选项的需求。

https://github.com/vllm-project/vllm/issues/11175
这是一个bug报告，主要涉及到INT8量化Qwen2.5-72B模型推理时生成无意义句子的问题。原因可能是模型量化过程中的错误导致了输出结果的异常。

https://github.com/vllm-project/vllm/issues/11174
这是一个用户提出需求的issue，主要涉及的对象是benchmark_serving.py脚本，由于缺少tokenizer_mode参数导致用户需要增加该功能。

https://github.com/vllm-project/vllm/issues/11173
这个issue类型是用户提出需求或寻求帮助，主要涉及XPU依赖缺失的问题。用户可能遇到XPU相关功能无法正常运行或安装失败的情况。

https://github.com/vllm-project/vllm/issues/11172
该issue属于文档相关，并涉及在线汇聚API的重组及接口路径调整。由于接口路径不符合OpenAI规范且汇聚示例存在小问题，导致相关调整。

https://github.com/vllm-project/vllm/issues/11171
这是一个Bug报告，主要涉及缺少Content Type导致返回500 Internal Server Error而非415 Unsupported Media Type的问题。

https://github.com/vllm-project/vllm/issues/11170
这个issue类型是需求修改，主要对象是VLLM项目中的软件实现。由于软件实现中缺少logits的软上限处理，导致在CPU后端上运行Gemna2和PaliGemma 2时产生了微小差异，需要修改错误警告以允许这些模型正常运行。

https://github.com/vllm-project/vllm/issues/11169
这是一个Bug报告，主要涉及的对象是Gemma2模型。由于attn_mask设置错误导致SDPA报错，需要确保is_causal只在mask为None时为True。

https://github.com/vllm-project/vllm/issues/11168
这是一个Bug报告，涉及到vllm.core.block.interfaces.BlockAllocator.NoFreeBlocksError to old Mistral Model，用户提出了关于模型服务中出现问题的疑问。

https://github.com/vllm-project/vllm/issues/11167
这是一个 Bug 报告，主要涉及 vllm 的在线推理基准测试工具在结果中显示的"Successful requests"数量通常低于设置的参数值的问题。这可能是由于程序执行过程中的某种原因导致的。

https://github.com/vllm-project/vllm/issues/11166
这个issue是关于需求添加，主要涉及的对象是模型。由于需要添加rwkv6 linear attention模型，涉及的代码从https://github.com/sustcsonglin/flashlinearattention进行了适配。

https://github.com/vllm-project/vllm/issues/11165
这是一个bug报告，涉及主要对象是VLM（Vision-Language Models），由于没有在批处理中处理仅有一个图像的情况，导致了一个边缘情况的bug。

https://github.com/vllm-project/vllm/issues/11164
这是一个需求提出的issue，主要涉及到在VLLM模型中添加从模型加载生成配置的功能。原因是为了允许加载和覆盖默认的抽样参数，以及为了保留现有行为而默认设置该标志为None。

https://github.com/vllm-project/vllm/issues/11163
这个issue类型是重构（Refactor），主要涉及的对象是设备相关的函数（Device-related functions），用户提出了通过将`is_pin_memory_available`抽象成单独的`Platform`类，减少条件语句的使用，提高可维护性的需求。

https://github.com/vllm-project/vllm/issues/11162
这个issue属于用户提出需求类型，主要涉及的对象是vLLM的硬件后端插件化功能。这个问题的原因是vLLM支持的硬件后端越来越多，导致代码复杂且维护困难，为了解决这个问题，提出了支持硬件可插拔的解决方案。

https://github.com/vllm-project/vllm/issues/11161
这个issue是关于bug报告，主要涉及LoRA请求和num_scheduler_steps配置，出现问题的原因可能是导致lora和num_scheduler_steps=8时输出错误的bug。

https://github.com/vllm-project/vllm/issues/11160
这是一个 Bug 报告类型的 issue，主要对象涉及到 vllm 模型服务。该问题由于生成 API 无法停止并导致输出内容错误，可能是由于服务器日志问题所致。

https://github.com/vllm-project/vllm/issues/11159
这个issue类型属于Bug报告，涉及的主要对象是CI（持续集成），由于缺少datasets package导致CI过程出现问题。

https://github.com/vllm-project/vllm/issues/11158
这是一个Bug报告，该问题涉及Llama-3.2-11B-Vision-Instruct服务器在添加系统提示时崩溃。由于JSON数据构造中存在拼接错误，导致程序中断。

https://github.com/vllm-project/vllm/issues/11157
这是一个Bug报告，主要对象是V1版本中的一个缓存 bug。由于'NoneType'对象没有属性'hash_value'，导致出现了这个问题。

https://github.com/vllm-project/vllm/issues/11156
这是一个bug报告，该问题涉及的主要对象是AllowedTokenIdsLogitsProcessor。由于使用了`len(tokenizer)`而不是`tokenizer.vocab_size`，导致在模型生成输出时出现了错误。

https://github.com/vllm-project/vllm/issues/11155
这是一个bug报告，涉及的主要对象是vllm下的WisdomShell模型。由于传递给引擎的参数或参数配置不正确，导致了初始化引擎时出现了错误。

https://github.com/vllm-project/vllm/issues/11154
这是一个bug报告，涉及的主要对象是vLLM模型服务。由于输入大小不符合预期，导致模型服务崩溃并显示错误信息。

https://github.com/vllm-project/vllm/issues/11153
这是一个关于用户提出需求的issue，主要涉及如何通过CLI来指定额外参数进行vLLM终端命令，导致用户在配置guided decoding时遇到挑战。

https://github.com/vllm-project/vllm/issues/11152
这是一个提出需求的RFC类型的issue，主要涉及vLLM的Neuron Backend与V1架构的集成方法。由于当前的neuron backend受限于支持feature的组合，提出了一些变化以支持更多类型的模型变体。

https://github.com/vllm-project/vllm/issues/11151
这是一个bug报告，主要对象是V1 prefix caching。由于类型错误导致了需要修复的bug。

https://github.com/vllm-project/vllm/issues/11150
这是一个用户提出需求的 issue，主要对象是前端代码。

https://github.com/vllm-project/vllm/issues/11149
这个issue属于功能请求类型，主要涉及到`ChatCompletionRequest`和`CompletionRequest`对象，用户提出了希望新增`logits_processors`参数的需求来直接指导抽样过程。

https://github.com/vllm-project/vllm/issues/11148
这个issue是一个bug报告类型，涉及到starcoder2代码中的k/v scale names，由于缺失更新导致了在kv cache quantization后无法加载。

https://github.com/vllm-project/vllm/issues/11147
这是一个关于如何在vllm中使用6个GPU的问题，用户遇到了无法成功利用6个GPU进行推理计算的困扰，尝试使用pipeline_parallel_size无效且性能极差。

https://github.com/vllm-project/vllm/issues/11146
这是一个bug报告，涉及的主要对象是ROCm平台，由于triton kernel的性能问题导致chunked prefill表现不佳，建议禁用自动启用该功能。

https://github.com/vllm-project/vllm/issues/11145
这个issue属于文档转换类型，涉及到将reStructuredText转换成MyST Markdown格式，升级mystparser版本，更新文件引用和前端代码中的注释等。

https://github.com/vllm-project/vllm/issues/11144
这是一个bug报告类型的issue，主要涉及的对象是GritLM中的pooling logic。由于新加入的任务类型导致了测试失败，因此需要使用`runner_type`替换掉`task`来修复这个问题。

https://github.com/vllm-project/vllm/issues/11143
这是一个bug报告，涉及的主要对象是vLLM的配置信息。由于日志记录方式的更改，导致日志中显示的是python对象引用而非配置信息的实际细节。

https://github.com/vllm-project/vllm/issues/11142
这个issue属于功能需求。主要涉及的对象是PaliGemma 2。原因可能是要对现有功能进行一些改进以支持PaliGemma 2。

https://github.com/vllm-project/vllm/issues/11141
这是一个bug报告，涉及到文档中的一个拼写错误的问题。原因是可能由于拼写错误导致用户无法按照正确的步骤安装helm。

https://github.com/vllm-project/vllm/issues/11140
这是一个关于性能优化的Issue，主要涉及到软件Outlines的更新和线程池大小调整。由于之前线程池大小被硬编码为2，而更新后根据CPU核数动态调整，虽然效果不是很显著，但仍有可测量的改进。

https://github.com/vllm-project/vllm/issues/11139
这是一个Bug报告，主要涉及的对象是vllm在使用ray的过程中遇到hang的问题。由于环境在eks中，使用--pipeline_parallel_size > 1参数时，导致vllm卡在特定日志信息处无响应。

https://github.com/vllm-project/vllm/issues/11138
这是一个bug报告，主要涉及Ray placement group在等待资源就绪时过早拒绝请求的问题。

https://github.com/vllm-project/vllm/issues/11137
这个issue类型是需求提出，主要涉及的对象是在vLLM中提升Ray支持以改善资源管理、弹性和性能。原因是由于当前与Ray集成存在的挑战导致了资源分配延迟、版本兼容性以及部署效率等方面的问题。

https://github.com/vllm-project/vllm/issues/11136
这是一个用户提出需求的issue，主要涉及的对象是vLLM的pipeline parallelism功能。这个问题是由于Ray应用作为调度器需要更详细的信息来有效管理资源分配和增强性能。

https://github.com/vllm-project/vllm/issues/11135
这是一个功能需求类型的issue，涉及的主要对象是PG_WAIT_TIMEOUT的设置。由于PG_WAIT_TIMEOUT目前是硬编码的，需要将其配置化以适应不同环境。

https://github.com/vllm-project/vllm/issues/11134
这是一个功能需求（Feature Request）类型的issue，主要涉及Ray placement group在资源准备时间方面的需求。由于集群资源准备时间较长，用户希望允许Ray placement group等待更多时间以确保资源就绪。

https://github.com/vllm-project/vllm/issues/11133
这个issue是一个功能需求提案，主要涉及的对象是V1版本的异步调度功能。由于异步调度的实现，导致了GPU利用率持续为100%，但在使用cudagraph时出现了精度不一致的问题，需要讨论解决。

https://github.com/vllm-project/vllm/issues/11132
这是一个bug报告，涉及的主要对象是vLLM在TPU上运行时出现异常输出，可能由于TPU上的vLLM无法正常处理包含图片的多模态输入导致。

https://github.com/vllm-project/vllm/issues/11131
这是一个用户提出需求的 Issue，主要涉及使用不带 GPU 的节点进行 ray 功能的请求，原因是当前的分布式功能只支持 GPU。

https://github.com/vllm-project/vllm/issues/11130
这个issue是关于Bug报告，涉及对象是在使用SmolLM2 model进行fp8量化时遇到的错误。原因可能是与A10 GPU上的marlin weight fp8量化相关的问题。

https://github.com/vllm-project/vllm/issues/11129
这是一个用户提出需求的issue，涉及到frontend中分离离线推理中的池化API。由于模型的任务需要，对`LLM.encode()` API进行了拆分，以避免访问池化输出时的数据格式混乱。

https://github.com/vllm-project/vllm/issues/11128
这是一个bug报告，涉及到vllm 0.6.4版本部署MiniCPMV_2_6_awq_int4模型时出现的报错。原因可能是与这个特定模型版本的兼容性问题。

https://github.com/vllm-project/vllm/issues/11127
这是一个bug报告，主要涉及的对象是vllm项目下的rejection_sampler模块。这个问题是由于对包含None部分的种子进行切片时返回了新的张量而不是原始张量的视图，导致这部分原始张量不会被更新。

https://github.com/vllm-project/vllm/issues/11126
这个issue类型是用户提出需求，主要对象是EAGLE在vLLM上的使用问题。由于无法成功运行EAGLE，用户寻求帮助解决问题。

https://github.com/vllm-project/vllm/issues/11125
这个issue是关于bug报告，主要涉及V1版本的torch profiling功能，在多进程环境下出现错误并提供了修复方法。

https://github.com/vllm-project/vllm/issues/11124
这是一个Bug报告，涉及openai api server服务中的流式输出不完整的问题。

https://github.com/vllm-project/vllm/issues/11123
这是一个Bug报告类型的issue，主要涉及N-gram speculative decoding中当一部分seeds为None时输出错误的问题。原因可能是种子值不一致导致的症状。

https://github.com/vllm-project/vllm/issues/11122
这个issue类型是bug报告，主要涉及的对象是vLLM软件。由于某个关键参数的缺失或错误导致了KeyError错误，并影响了程序的运行。

https://github.com/vllm-project/vllm/issues/11121
这个issue是用户提出需求类型的，主要对象是vLLM的文档。用户提出了希望添加媒体包的链接以获取vLLM的logo。

https://github.com/vllm-project/vllm/issues/11120
这个issue是关于bug报告，主要涉及的对象是内存性能分析。导致这个bug的原因是合并两个方法时可能会导致内存利用率行为发生变化。

https://github.com/vllm-project/vllm/issues/11119
这是一个bug报告，主要涉及的对象是xGrammar，由于无效的语法导致在编译时抛出异常，可能导致引擎崩溃和工作进程终止。

https://github.com/vllm-project/vllm/issues/11118
这个issue是一个bug报告，主要涉及到`grammar_is_likely_lark`环境识别EBNF语法错误的问题。

https://github.com/vllm-project/vllm/issues/11117
这个issue是一个功能需求，主要涉及了支持Lora Prometheus指标的功能。原因是要增强vLLM中LoRA管理功能的生产环境。

https://github.com/vllm-project/vllm/issues/11116
这是一个bug报告，主要涉及的对象是V1版本的PP Ray原型。由于某些原因导致出现了bug症状。

https://github.com/vllm-project/vllm/issues/11115
这个issue是一个bug报告，涉及到vllm未清理旧的zmq ipc sockets，导致系统上出现大量孤立文件的问题。

https://github.com/vllm-project/vllm/issues/11114
这是一个关于安装问题的bug报告，涉及的主要对象是vllm的安装过程。原因可能是编译过程中出现了卡住的情况，导致构建长时间无法完成。

https://github.com/vllm-project/vllm/issues/11113
该issue是一个bug报告，主要涉及的对象是vllm下的核心模块。这个问题是由于需要简化核心进程终止流程，去除了`should_shutdown` `multiprocessing.Value`，改为使用一种可以同时处理SIGINT和SIGTERM的异常机制，因此需要测试并验证其正确关闭功能。

https://github.com/vllm-project/vllm/issues/11112
这个issue类型是文档更新，主要涉及到的对象是LlamaStack远程vLLM指南的链接。由于链接需要更新并发布到LlamaStack网站，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/11111
该issue类型为优化提升，主要涉及输入准备过程中的性能优化。原因是为了减少小批量输入过程中创建新张量所引入的延迟。

https://github.com/vllm-project/vllm/issues/11110
这个issue是关于代码优化而不是bug报告，主要涉及的对象是`torch.compile`相关的日志记录功能。由于debug级别的日志记录过多，导致CI输出日志过于冗长，该issue提出移除这两行代码以优化日志输出。

https://github.com/vllm-project/vllm/issues/11109
该issue属于功能改进类型，主要涉及VLLM V1启动时的错误处理。由于当前的错误处理流程导致用户不容易看到错误的根本原因，因此希望能够改进错误信息的展示方式。

https://github.com/vllm-project/vllm/issues/11108
这是一个需求提出类型的issue，主要涉及的对象是torch.compile中的fast inductor，在提出这个需求的背后可能是为了提高模型训练的效率与性能。

https://github.com/vllm-project/vllm/issues/11107
这是一个性能问题报告，主要涉及V1版本的TP Ray executor，由于Ray编译图执行器的性能略慢于预期，导致部分性能指标略有下降。

https://github.com/vllm-project/vllm/issues/11106
这个issue类型是功能需求提出类型，主要对象是关于返回输入token的logits，而非生成答案。由于用户希望获取输入token的logits，而非生成答案。

https://github.com/vllm-project/vllm/issues/11105
这个issue是关于优化代码质量，主要涉及到使用mypy检查代码。由于一些具体情况需要进一步审查，导致了提出这些需要改进的问题。

https://github.com/vllm-project/vllm/issues/11104
这个issue类型为用户提出需求，主要涉及对象是TPUModelRunner，用户希望在TPU上实现有指导的解码功能以获取结构化输出。

https://github.com/vllm-project/vllm/issues/11103
这是一个bug报告，主要涉及到llmcompressor的安装版本问题，由于安装版本不匹配导致了模块导入错误的bug。

https://github.com/vllm-project/vllm/issues/11102
这个issue是一个bug报告，涉及的主要对象是llmcompressor中的`llmcompressor.modifiers.quantization`模块。原因是在安装特定版本0.1.0时，与compressed_tensors中的模块冲突导致ImportError错误。

https://github.com/vllm-project/vllm/issues/11101
这是一个用户提出需求的issue，主要询问新发布的多模态模型internVL2_5何时会被vllm支持。

https://github.com/vllm-project/vllm/issues/11100
这个issue类型是功能添加，涉及的主要对象是CPU backend，用户提出了增加MultiLoRA在CPU上的实现，并进行了相关测试以及更新操作。

https://github.com/vllm-project/vllm/issues/11099
这个issue不是bug报告，而是关于改进性能的一项任务，主要涉及CPU prefix caching的优化。

https://github.com/vllm-project/vllm/issues/11098
这个issue属于功能需求类型，涉及主要对象为CI/Build系统，由于需要在AMD环境中启用前缀缓存测试步骤。

https://github.com/vllm-project/vllm/issues/11097
这个issue是关于bug报告，主要涉及的对象是`mistral_tool_parser`，由于当前实现逻辑不支持解析并行函数调用结果导致了JSON解码错误。

https://github.com/vllm-project/vllm/issues/11096
这是一个Bugfix类型的issue，主要涉及到请求处理的逻辑错误，导致了请求在某些情况下无法正确断开连接。

https://github.com/vllm-project/vllm/issues/11095
该issue类型为功能更新反馈，涉及主要对象为InternVL模型。Bug修复导致RTX3090上可成功运行InternVL2.526B。

https://github.com/vllm-project/vllm/issues/11094
这是一个bug报告，涉及的主要对象是LoRA Adapter请求，具体问题是请求没有返回适配器名称。

https://github.com/vllm-project/vllm/issues/11093
该issue属于文档更新类型，主要涉及到关于汇集模型和嵌入模型的文档更新。原因是希望更新更多文档来指向更通用的"汇集模型"而不是"嵌入模型"。

https://github.com/vllm-project/vllm/issues/11092
这是一个bug报告，涉及到vllm中的动态加载LoRA执行速度过慢的问题，原因是在加载LoRA时执行时间明显增加。

https://github.com/vllm-project/vllm/issues/11091
该issue属于用户提出需求，主要对象是对LoRA模型和基础模型的度量指标进行区分。由于当前度量指标都统一汇总在基础模型的度量指标下，导致无法准确监测和分析LoRA模型的使用情况和性能。

https://github.com/vllm-project/vllm/issues/11090
这是一个Bug报告，涉及的主要对象是lora adapter请求返回了基本模型名称而不是正确的模型名称。这个问题可能是由于lora adapter未正确返回模型名称导致。

https://github.com/vllm-project/vllm/issues/11089
这是一个bug报告，涉及的主要对象是vllm。由于无法导入vllm._C模块和vllm._version模块，导致返回空字符串的bug。

https://github.com/vllm-project/vllm/issues/11088
这是一个bug报告，主要涉及对象是`outlines`库版本管理。导致这个问题的原因是`outlines`版本未固定导致CI失败。

https://github.com/vllm-project/vllm/issues/11087
这是一个bug报告，主要涉及到AMD CI dependencies的问题，由于`outlines`升级到0.1.8后依赖关系发生变化，导致AMD测试失败。

https://github.com/vllm-project/vllm/issues/11086
这是一个Bug报告类型的issue，主要涉及VLLM中使用LoRA模型时设置num-scheduler-steps参数导致输出不符预期的问题。

https://github.com/vllm-project/vllm/issues/11085
这是一个优化性质的issue，主要涉及到平台的代码结构调整。

https://github.com/vllm-project/vllm/issues/11084
这是一个Bug报告，涉及的主要对象是Docker deployment model。这个问题可能是由于版本问题导致的异常。

https://github.com/vllm-project/vllm/issues/11083
这个issue类型是优化提升，主要涉及VLM测试的分组调整。

https://github.com/vllm-project/vllm/issues/11081
该issue属于用户提出需求类型，主要涉及的对象是`torch.compile`的性能问题，用户通过追踪forward time来测试`torch.compile`的性能，以便更好地衡量优化效果。

https://github.com/vllm-project/vllm/issues/11080
这是一个bug报告，主要涉及的对象是Idefics3的多图像推断功能。这个问题导致在每个提示的图像尺寸不同时可能引发错误。

https://github.com/vllm-project/vllm/issues/11079
该issue类型为需求提出，主要涉及的对象是改进N-gram speculation功能，用户提出希望能够针对除了prompt之外的任意token序列进行ngram推测。因为当前功能限制了只能在prompt中进行ngram推测，用户希望能够更灵活地使用ngram功能。

https://github.com/vllm-project/vllm/issues/11078
这是一个用户提出需求的类型，主要对象是torch.compile的使用指南。这个需求可能是因为现有文档不够清晰或详细，用户需要更多关于torch.compile的使用信息。

https://github.com/vllm-project/vllm/issues/11077
这是一个bug报告，该问题涉及到vllm的multilora功能。由于使用async engine时，无论设置了多少个loras，仅有前`maxloras`个loras可被查询，导致了无法正常使用该功能的问题。

https://github.com/vllm-project/vllm/issues/11076
这是一个bug报告，主要涉及V1版本的问题引起的引擎客户端无法正常关闭。原因是关闭代码依赖于垃圾回收来触发，但程序未能执行到触发点。

https://github.com/vllm-project/vllm/issues/11075
这是一个特征需求的issue，主要涉及添加W8A8整型不均匀/对称模型。

https://github.com/vllm-project/vllm/issues/11074
这个issue类型是功能改进提议，涉及到对V1版本使用多进程方法的默认设置。由于之前的代码强制使用`spawn`多进程方法导致在某些配置中出现问题，所以作者将其更改为默认开启，并尝试解决已有代码中的相关问题。

https://github.com/vllm-project/vllm/issues/11073
这是一个bug报告，主要涉及的对象是代码中的logprobs（对数概率值）。该问题是由于logprobs中包含了无穷大值，在某些情况下未被正确限制导致JSON格式不符合要求，同时可能导致服务器内部错误以及与sampling参数相关的问题。

https://github.com/vllm-project/vllm/issues/11072
这是一个Bug报告，主要涉及VLLM服务中KV缓存无法在不同请求之间持久化的问题。原因可能是对缓存查找机制的缺失。

https://github.com/vllm-project/vllm/issues/11071
这个issue是一个bug报告，涉及的主要对象是Mamba和其他attention-free模型。由于缺少`advance_step`方法，导致了在buildkite中出现错误。

https://github.com/vllm-project/vllm/issues/11070
这是一个bug报告类型的issue，主要涉及vLLM的模型结构在Neuron设备上无法运行的问题。由于Neuron不支持该模型结构（'OPTForCausalLM'），导致出现错误并需寻求解决方案。

https://github.com/vllm-project/vllm/issues/11069
这个issue是关于bug报告，涉及主要对象是granite tool call。由于之前的PR的streaming案例存在问题，导致需要修复granite工具调用时的streaming问题。

https://github.com/vllm-project/vllm/issues/11068
这是一个bug报告，涉及的主要对象是代码中的除零潜在问题，在极端情况下可能会导致除零错误。

https://github.com/vllm-project/vllm/issues/11067
这是一个Bug报告，涉及vLLM的问题调度行为导致生成请求被饿死。

https://github.com/vllm-project/vllm/issues/11066
该issue为用户提出需求，主要对象是关于vLLM的logo和品牌指南。用户希望获取vLLM logo的SVG文件，但无法找到透明背景下的浅色和深色变体。

https://github.com/vllm-project/vllm/issues/11065
该issue类型为需求提议，主要涉及支持地理空间模型的添加。由于现代模型不仅针对文本的生成，还涉及从文本或图像输入生成图像，因此该RFC旨在支持生成图像的模型，并展示相关模型实例的示例。

https://github.com/vllm-project/vllm/issues/11064
这是一个Bug报告类型的Issue，主要涉及llmcompressor对llama3 70b模型进行int8-a8w8量化时出现数值不稳定导致无法反转Hessian矩阵的数值错误。

https://github.com/vllm-project/vllm/issues/11063
这是一个用户提出需求的类型，该问题涉及主要的对象是VLLM代码库中的硬件和CPU支持。

https://github.com/vllm-project/vllm/issues/11062
该issue类型为性能问题报告，主要涉及的对象是模型量化过程的执行时间。出现这个问题可能是由于模型量化过程耗时过长，导致用户质疑性能是否符合预期。

https://github.com/vllm-project/vllm/issues/11061
这个issue是一个bug报告，主要涉及的对象是V1模型服务的初始化过程。原因是enable_chunked_prefill参数未正确设置导致在某些模型上出现错误信息并导致服务器失败。

https://github.com/vllm-project/vllm/issues/11060
这是一个bug报告，涉及的主要对象是XGrammar。由于XGrammar在匹配特殊token时会导致引擎崩溃，需要通过修改tokenization过程来避免这种情况。

https://github.com/vllm-project/vllm/issues/11059
这是一个用户提出需求的issue，主要涉及torch.compile模块，用户希望添加一个标志来跟踪 batchsize 统计信息。

https://github.com/vllm-project/vllm/issues/11058
这是一个Bugfix类型的issue， 主要涉及的对象是simple connector for KVCache transfer。由于从`kv_cache[0].shape`获取`num_heads`和`head_size`值会导致数值解压错误，因此需要直接从模型配置获取这些值以避免错误。

https://github.com/vllm-project/vllm/issues/11057
这是一个bug报告，涉及的主要对象是`fused_moe_kernel`的`EM`和`num_valid_tokens`参数。由于最新Triton分支中引入了`do_not_specialize_on_alignment`参数，导致需要修改代码以实现更精确的参数设置。

https://github.com/vllm-project/vllm/issues/11056
这是一个建议性能优化的issue，主要涉及到了`fused_moe_kernel`的优化处理。原因是`EM`和`num_valid_tokens`参数不需要特殊化，但由于其频繁变化导致了性能影响。

https://github.com/vllm-project/vllm/issues/11055
这是一个Bug报告，涉及的主要对象是V1版本的服务启动。由于_LazyConfigMapping.__init__()缺少一个必需的参数'mapping'，导致引发了EngineCoreProc启动失败的异常。

https://github.com/vllm-project/vllm/issues/11054
这是一个Bug报告，涉及到vllm在使用8*GPU时出现的错误。由于环境配置和程序逻辑导致了创建共享内存段时出现错误，进而影响了模型运行。

https://github.com/vllm-project/vllm/issues/11053
这是一个bug报告，主要涉及VLLM_MAX_SIZE_MB参数的限制问题，原因是最新的docker镜像构建超出了LLM Wheel文件的最大大小限制。

https://github.com/vllm-project/vllm/issues/11052
这个issue类型是性能报告，主要涉及TGI在长提示上处理更多token且速度更快，导致性能表现超越vLLM。

https://github.com/vllm-project/vllm/issues/11050
这是一个功能需求类的issue，涉及vLLM项目中日志记录功能的改进，主要对象是请求ID。由于当前实现中仍在使用随机ID而非xrequestid，用户建议考虑使用xrequestid以提升请求追踪效果。

https://github.com/vllm-project/vllm/issues/11049
这是一个关于如何指定本地存储路径的问题，属于用户提出需求类型，主要涉及 VLLM 模型下载过程中无法指定存储路径的困扰。可能是由于环境变量设置不正确导致该问题。

https://github.com/vllm-project/vllm/issues/11048
这是一个关于功能增强的issue，主要涉及了测试代码。原因可能是为了方便启用guided decoding测试而进行的更新。

https://github.com/vllm-project/vllm/issues/11047
这是一个bug报告，主要涉及的对象是vllm项目中的cuda模块。问题是由于调用`.get_device_capability`和`.has_device_capability`可能在cuda环境下抛出错误，导致模块无法成功加载。

https://github.com/vllm-project/vllm/issues/11046
这是一个bug报告，涉及对象为guided decoding with tokenizer mode mistral，由于tokenizer调用被修改导致decode函数获取错误的响应，从而导致了bug。

https://github.com/vllm-project/vllm/issues/11045
这是一个Bug报告，主要涉及Guided decoding当tokenizer_mode设置为mistral时导致crash的问题。定位原因是最近的代码在mistral和设置xgrammar时不起作用。

https://github.com/vllm-project/vllm/issues/11044
这是一个Bug报告，涉及VLLM中使用xgrammar与Pixtral-HF时引起引擎崩溃的问题。问题是由于特殊标记``(空格)被传递而导致引擎崩溃，需求除了引发引擎崩溃外的其他解决方法。

https://github.com/vllm-project/vllm/issues/11043
这是一个bug报告，涉及到在使用更新后的PixtralHF时出现了`AttributeError: 'LlavaConfig' object has no attribute 'vocab_size'`错误，由于transformers库中的`LlavaConfig`类缺少`vocab_size`属性所致。

https://github.com/vllm-project/vllm/issues/11042
这是一个性能问题，涉及主要对象为vLLM和SGLang。用户提出了关于性能比较方式的问题，寻求如何在特定批处理大小下实现准确比较的建议。

https://github.com/vllm-project/vllm/issues/11041
该issue为功能需求类型，主要涉及TPU性能分析功能。由于用户希望能够使用TPU进行性能分析，因此提出了需要通过profile traces实现TPU性能分析的需求。

https://github.com/vllm-project/vllm/issues/11040
这是一个bug报告，主要涉及到Pixtral loading功能。这个bug的原因是使用itertools的`tee`操作符导致内存效率低下，修复后可以提高效率。

https://github.com/vllm-project/vllm/issues/11039
该issue类型为bug报告，主要涉及的对象是granite工具解析器。由于解析器在输出以``开头时无法解析工具调用，用户反馈需要支持该特殊情况下的解析，希望解决这一问题。

https://github.com/vllm-project/vllm/issues/11038
这是一个bug报告，涉及的主要对象是V1模型运行程序。由于存储`None`生成器导致`sampler`的`random_sample`方法在处理请求时顺序执行，导致性能下降。

https://github.com/vllm-project/vllm/issues/11037
这个issue类型为bug报告，涉及对象为vllm安装问题。由于缺少NumPy模块导致安装失败。

https://github.com/vllm-project/vllm/issues/11036
这是一个Bug修复类型的Issue，主要涉及的对象是v0引擎。该问题由于重复请求id导致引擎 hang 且无法正常关闭，需要将v1引擎的请求id唯一性验证功能迁移到v0引擎来解决。

https://github.com/vllm-project/vllm/issues/11035
这个issue属于bug报告类型，主要涉及到ROCm平台上的scpecilative decoding worker class的数值设置问题，导致程序在尝试.rsplot('.')并提取两个值时崩溃。

https://github.com/vllm-project/vllm/issues/11034
这是一个用户提出需求的类型issue，主要对象是项目的README.md文件。由于vLLM加入pytorch生态系统，用户提出需要更新README.md文件以添加相关新闻。

https://github.com/vllm-project/vllm/issues/11033
这是一个功能需求的提出。这个问题单涉及的主要对象是 Triton Paged Attn Decode Kernel。

https://github.com/vllm-project/vllm/issues/11032
这个issue是一个性能优化问题，涉及到V1版本中模型运行器在文本模型中使用`input_ids`而不是`input_embeds`，由于这种改变未在CUDA图中包含嵌入层，导致了性能略微下降的情况。

https://github.com/vllm-project/vllm/issues/11031
这个issue类型是功能需求，主要涉及的对象是关于监控tokens per step的metrics，因为需要用于`torch.compile`。

https://github.com/vllm-project/vllm/issues/11030
这是一个功能需求提出的issue，主要涉及的对象是针对添加多进程HPU执行器的问题。

https://github.com/vllm-project/vllm/issues/11028
这是一个bug报告，涉及的主要对象是vllm_hpu_extension。由于vllm_hpu_extension中移除了`vllm.utils.get_vllm_instance_id`的依赖，导致HPU backend在导入vllm_hpu_extension时崩溃。

https://github.com/vllm-project/vllm/issues/11027
这是一个功能需求的issue，主要涉及到在VLLM中添加OpenAI API支持input_audio。原因是由于当前版本还没有输出音频支持，所以需要针对输入音频的格式进行适配。

https://github.com/vllm-project/vllm/issues/11025
这是一个bug报告，主要涉及的对象是 `LLM.generate(text)` 和 `LLM.encode(text)` 方法，由于`deprecated` decorator 的使用错误，导致这两个方法被错误标记为不推荐使用。

https://github.com/vllm-project/vllm/issues/11024
这个issue是一个Bug报告，涉及的主要对象是Pixtral-HF模型。导致这个Bug的原因是在39e227c7ae之后，PixtralHF模型无法启动，出现了特定的Traceback错误。

https://github.com/vllm-project/vllm/issues/11023
这是一个用户提出需求的issue，主要涉及在vLLM中增加了SwiftKV模型。原因是需要在模型中实现提前退出一些标记而不是其他标记的功能，目的是最小化对vLLM核心代码的更改。

https://github.com/vllm-project/vllm/issues/11022
这个issue属于bug报告，主要涉及的对象是vllm代码库。由于缺少设置HF_TOKEN环境变量，导致出现问题需要修复。

https://github.com/vllm-project/vllm/issues/11021
这个issue类型是需求提出，主要对象是xgrammar库，由于xgrammar需要升级到0.1.6版本来解决使用JSON Schema时的问题。

https://github.com/vllm-project/vllm/issues/11020
这是一个功能增强类的issue，主要涉及到VLM预处理器的哈希功能和多模态预处理器/映射器的缓存问题。由于性能分析尚未完成，多模态预处理器缓存功能默认处于禁用状态，用户可能在请求中需要关于性能分析、编码器缓存和前缀缓存的帮助。

https://github.com/vllm-project/vllm/issues/11019
这是一个关于理解稀疏 Marlin 代码中矩阵转置问题的讨论，探讨了计算中为何不对 meta 进行转置处理的疑问。

https://github.com/vllm-project/vllm/issues/11018
这是一个bug报告，涉及对象为在安装过程中可能会误选择Gaudi/HPU而非XPU。这个问题是由于_is_hpu()在检测/dev/accel/accel0时可能会误判导致，用户希望通过VLLM_TARGET_DEVICE="xpu"来消除误选的情况。

https://github.com/vllm-project/vllm/issues/11017
这是一个用户寻求帮助的issue，涉及主要对象是尝试在vllm上运行Microsoft GraphRAG，出现了API未找到的错误。

https://github.com/vllm-project/vllm/issues/11016
这是一个bug报告，涉及的主要对象是Neuron（神经元），由于旧版本的PyTorch导致了加载模型时出现错误，需要升级Neuron版本来修复。

https://github.com/vllm-project/vllm/issues/11015
这是一个问题提问的issue，涉及主要对象是kuberay，问题由于缺乏对于在多个pod上运行超大模型的支持而导致。

https://github.com/vllm-project/vllm/issues/11014
这个issue是用于提出功能需求的，主要对象是vllm模型。用户提出在同一个模型上进行多次推断时需要重复加载权重，导致时间消耗过多，希望能够通过vllm只加载一次权重来解决这个问题。

https://github.com/vllm-project/vllm/issues/11013
这个issue类型属于技术改进，涉及修改CI依赖列表的Python版本；用户为了基于Python 3.12重新编译，解决前一次使用Python 3.9编译时可能出现的问题。

https://github.com/vllm-project/vllm/issues/11012
这是一个Bug报告，主要涉及的对象是Qwen2ForSequenceClassification模块。由于当前环境中PyTorch版本为2.5.1+cu124，可能导致Qwen2ForSequenceClassification模块无法正常工作，故报告此问题。

https://github.com/vllm-project/vllm/issues/11011
这是一个Bug报告类型的Issue，主要涉及Qwen2ForSequenceClassification模型无法正常工作。

https://github.com/vllm-project/vllm/issues/11010
这是一个bug报告，主要涉及的对象是在使用命令加载Llama3.3模型时遇到了错误导致无法正常工作。可能的原因是模型无法成功加载在GPU上，并请求解决方法。

https://github.com/vllm-project/vllm/issues/11009
这是一个用户提出需求的issue，主要涉及对象是关于vllm下的Llama-3-Groq-8B-Tool-Use工具使用支持。用户无法使用Groq/Llama3Groq8BToolUse中的工具使用功能，希望有一个支持此功能的tool_chat_template。

https://github.com/vllm-project/vllm/issues/11008
这是一个用户提出需求的issue，主要对象是新增支持Emu3模型。用户想知道何时会支持Emu3模型，可能由于当前模型vllm不支持Emu3模型引发。

https://github.com/vllm-project/vllm/issues/11007
该issue属于性能问题，涉及的主要对象是vLLM的sampling过程。由于sampling过程耗时较长，导致每次迭代执行时大部分时间都消耗在了sampling上。

https://github.com/vllm-project/vllm/issues/11006
这是一个用户提出需求的类型，主要涉及vllm在多轮图片对话中的支持。由于缺乏参数设置支持，用户在上传第二张图片进行对话时遇到错误。

https://github.com/vllm-project/vllm/issues/11005
这是一个关于性能优化的issue，主要涉及torch.compile的动态时间追踪，用户提出了关于编译时间分析的需求。

https://github.com/vllm-project/vllm/issues/11004
这是一个关于性能问题的bug报告，涉及主要对象是pipeline parallelism。由于未通过AsyncLLMEngine使用pipeline parallel会导致性能严重下降的问题。

https://github.com/vllm-project/vllm/issues/11003
这是一个需求提出类型的issue，主要涉及的对象是LoRA功能管理。由于缺乏对不支持特性的验证和错误处理，用户提出需要添加PEFTHelper类来优化LoRA特性管理。

https://github.com/vllm-project/vllm/issues/11002
这是一个需求类型的issue，主要涉及到Qwen2-VL模型在AWS Neuron上的支持问题。由于目前Neuron仅支持部分模型架构，用户提出希望将Qwen2-VL也纳入支持范围。

https://github.com/vllm-project/vllm/issues/11001
这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM的Block Manager。由于当前block-based架构在I/O效率上存在限制，需要更好地利用I/O带宽，因此提出了使用Block Group Manager替换现有的Block Manager来改善问题。

https://github.com/vllm-project/vllm/issues/11000
这是一个bug报告，主要涉及v1版本的问题，由于缺少warmup导致cudagraph捕获前需要进行autotuning。

https://github.com/vllm-project/vllm/issues/10999
这是一个需求提出类型的issue，涉及主要对象是代码中的日志输出。由于之前使用的`logger.info`方式难以在中间插入字段，需要修改为使用fstring以便更容易插入字段。

https://github.com/vllm-project/vllm/issues/10998
该issue类型为文档更新，主要涉及V1支持列的添加至多模态模型文档。由于V1对多模态模型的支持将逐步推出，因此该PR旨在文档中添加一列，以指示V1中可用的内容。

https://github.com/vllm-project/vllm/issues/10997
这个issue类型属于bug报告，涉及的主要对象是`AsyncLLM`，由于在加载`Detokenizer`时缺失了一些配置，导致需要修复以支持在线服务的功能。

https://github.com/vllm-project/vllm/issues/10996
这是一个用户提出需求的issue，主要涉及到cudagraph的batchsize padding逻辑。用户希望统一这个逻辑并允许自定义cudagraph的捕捉大小，以加快启动时间和只捕捉用户关心的大小的cudagraph。

https://github.com/vllm-project/vllm/issues/10995
该issue类型为用户提出需求，涉及的主要对象是Cutlass库中的稀疏量化和非量化内核，由于需要支持对2of4稀疏模型的压缩张量操作，导致用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/10994
这个issue是关于用户提出需求的问题，用户想知道如何通过curl命令调用特定模型的endpoint。由于用户尝试调用时遇到困难，希望得到相应的支持和解决方案。

https://github.com/vllm-project/vllm/issues/10993
这个issue属于bug报告类型，主要涉及的对象是torch.compile模块。由于评论存在问题，导致需要修复。

https://github.com/vllm-project/vllm/issues/10992
这是一个功能需求类的issue，主要涉及到为Mamba-like模型（Jamba, Mamba, MambaFalcon等）添加PP支持。

https://github.com/vllm-project/vllm/issues/10991
这是一个CI/Build类型的issue，主要涉及的对象是transformers v4.47版本。这个问题是为了确定是否可以安全地将库更新到transformers v4.47版本。

https://github.com/vllm-project/vllm/issues/10990
这是一个bug报告，涉及模型架构Qwen2ForSequenceClassification不受支持的问题，可能由于文档中未提供相关解决方案导致。

https://github.com/vllm-project/vllm/issues/10989
这是一个优化性质的Issue，主要涉及 V1 flash-attn 模块中 CPU 的性能开销。

https://github.com/vllm-project/vllm/issues/10988
这是一个功能需求提出类型的issue，主要涉及的对象是V1-rearch模型的图像推断支持。原因是为了实现MRoPE功能，最终用户希望修复图像嵌入作为输入测试的问题。

https://github.com/vllm-project/vllm/issues/10987
这是一个bug报告，涉及VLLM中的Qwen2VL在使用TPU后端时出现问题，无法正常运行。

https://github.com/vllm-project/vllm/issues/10986
这是一个bug报告类型的issue，主要涉及到VLLM中的核心功能和分布式模块。由于可能无法从StatelessProcessGroup进行初始化，因此用户提交了这个问题来寻求帮助。

https://github.com/vllm-project/vllm/issues/10985
这个issue类型是需求更新，主要对象是README.md文件。由于最新消息部分的更新需要以可折叠列表的形式呈现，因此提出了这一需求。

https://github.com/vllm-project/vllm/issues/10984
该issue属于用户提出需求类型，主要涉及torch.compile中允许候选编译尺寸的设置。由于该功能扩展了编译尺寸的选择范围，并提供了更灵活的指定方式，用户可以更方便地控制编译尺寸，从而优化模型性能。

https://github.com/vllm-project/vllm/issues/10983
这是一个Bug报告，主要涉及LLama 3.2 vision模型在多图像场景下只关注第一幅图像的问题。原因可能是模型在处理多图像交互时存在错误。

https://github.com/vllm-project/vllm/issues/10982
这是一个用户提出需求的issue，主要涉及改进benchmarking代码，更新了NVIDIA CUTLASS dense FP8 matmul kernels的选择启发式。

https://github.com/vllm-project/vllm/issues/10981
这是一个Bug报告，涉及的主要对象是VLLM的版本0.6.4.post1，可能由于某种原因导致使用结构化输出导致内存溢出，即使在之前版本0.6.3.post1中的解决方法也无法解决。

https://github.com/vllm-project/vllm/issues/10980
该issue类型是功能需求，主要对象是V1 engine中的parallel sampling功能。这个需求是由于添加了对于异步LLM和LLMEngine的并行采样支持。

https://github.com/vllm-project/vllm/issues/10979
这是一个bug报告类型的issue，涉及主要对象为tool streaming with hermes and mistral parsers。由于UTF8 escaping mismatches、final delta未返回等问题导致了argument corruption等症状。

https://github.com/vllm-project/vllm/issues/10978
这是一个文档问题，用户明确声明了InternVL 2.5得到支持，而不需要进一步的更改支持这个模型在vLLM中的工作。

https://github.com/vllm-project/vllm/issues/10977
这是一个需求提出型的issue，主要涉及的对象是Phi3Vision模型的输入处理器。

https://github.com/vllm-project/vllm/issues/10976
这个issue是关于功能需求的，主要涉及到VLLM的核心功能executor，问题是instance id将附加到vllm_config对象而不是通过环境变量传递，导致了这个改变的原因是为了简化实例化id的实现。

https://github.com/vllm-project/vllm/issues/10975
这是一个bug报告类型的issue，主要涉及的对象是 vllm 下的一个model，由于PP和speculative decoding不兼容导致的bug。

https://github.com/vllm-project/vllm/issues/10974
这是一个用户提出需求的issue，主要对象是使用vllm在docker中运行本地模型，并寻求相关集成方法的帮助。由于用户不知道如何整合本地模型和vllm，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/10973
这是一个bug报告，涉及的主要对象是 `ptestpipeline.yaml`，由于某个提交 https://github.com/vllmproject/vllm/pull/10450/filesdiff2fe466060a88bb6a57175df8ca7175849db82a2cf2ba082295d481ab57e58868R233 引入了一个错误，导致了bug。

https://github.com/vllm-project/vllm/issues/10972
该issue属于用户提出需求类型，主要对象是在torch.compile中使用depyf库来dump内部信息。原因是用户希望能够通过运行特定命令获取包括Dynamo编译的字节码、Inductor生成的内核、maxautotune配置、各种转换阶段的图模块等信息。

https://github.com/vllm-project/vllm/issues/10971
这是一个Bug报告，主要涉及Vllm在CPU模式下只使用一个核心，而不是多核心的问题。

https://github.com/vllm-project/vllm/issues/10970
这是一个bug报告，在使用Hugging Face的VLLM库时遇到支持新的旋转嵌入模型的请求。

https://github.com/vllm-project/vllm/issues/10969
这是一个Bug报告类型的issue，涉及到了LlamaForCausalLM模型中ngram Speculation功能的故障，导致了vllm无法正常运行。

https://github.com/vllm-project/vllm/issues/10968
这是一个功能建议，涉及到前端，旨在通过使用`XRequestId`标头中的请求ID来填充引擎中的请求ID，以改善追踪性能，使用户能够将vLLM日志与生成它们的请求相关联。

https://github.com/vllm-project/vllm/issues/10967
这是一个关于功能需求的issue，主要涉及vllm模型加载速度的问题，用户提出需要在保存分片状态时也包含GPU P2P访问缓存。

https://github.com/vllm-project/vllm/issues/10966
这是一个bug报告，涉及用户无法正确使用vllm来提供特定的嵌入模型。由于用户报告的错误信息，可能是因为未正确配置环境或模型文件导致的。

https://github.com/vllm-project/vllm/issues/10965
这是一个用户提出需求的issue，主要涉及的对象是vLLM在支持加载已进行TP分片的权重时的功能。

https://github.com/vllm-project/vllm/issues/10964
这是一个用户提出需求的issue，主要涉及的对象是新增模型"LLama3.3"的规划。由于该模型是新添加的，用户请求vllm添加对该模型的支持。

https://github.com/vllm-project/vllm/issues/10963
这是一个Bug报告，涉及对象为无法在TPU上加载/编译Mixtral-8x7B-Instruct-v0.1模型，可能由于编译阶段失败导致。

https://github.com/vllm-project/vllm/issues/10962
这个issue属于代码重构类型，主要涉及到将对象定义从一个文件移到另一个文件，不涉及功能上的改变。

https://github.com/vllm-project/vllm/issues/10961
这是一个关于代码优化的issue，主要涉及到日志消息的清理，其中涉及到多个不太有趣的日志消息被修改。由于部分日志消息不够具有普遍性，需要进行清理操作。

https://github.com/vllm-project/vllm/issues/10960
这是一个类型为文本错误修正的issue，主要涉及的对象是文档内容。由于有拼写错误或者语法错误导致了这个问题的产生。

https://github.com/vllm-project/vllm/issues/10959
这是一个用户提出需求的issue，主要对象是在该项目中运行mypy检查。原因可能是为了改进代码质量和类型检查。

https://github.com/vllm-project/vllm/issues/10958
这是一个需求提出的issue，主要涉及了`MergedQKVParallelLinearWithLora`的重构和清理。

https://github.com/vllm-project/vllm/issues/10957
这是一个功能增强的issue，主要涉及LoRA支持的添加，通过V1版本运行LoRA请求，并且由于CUDA图表的存在导致性能差距。

https://github.com/vllm-project/vllm/issues/10956
这个issue是一个bug报告，涉及的主要对象是CI测试。这个问题是由于某些错误导致了测试失败。

https://github.com/vllm-project/vllm/issues/10955
这是一个特性新增的issue，主要涉及到`PunicaWrapper`的抽象，允许不同平台支持LoRA，以支持各个平台独立实现自己的`PunicaWrapper`变体。

https://github.com/vllm-project/vllm/issues/10954
这是一个bug报告，该issue涉及vllm下的guided_choice在serving gguf models时无法正常工作。原因可能是与特定模型（.gguf模型）的兼容性问题。

https://github.com/vllm-project/vllm/issues/10953
这个issue是用户提出需求和请教问题的类型，主要对象是如何使用Python脚本来启动一个FastAPI服务来进行internvl2-8b的推理，而不是使用终端命令vllm serve。用户不清楚如何将vllm集成到他们的项目中。

https://github.com/vllm-project/vllm/issues/10952
这是一个Bug报告，涉及到Qwen2.5-Coder模型的函数调用问题。问题可能是由于工具调用解析器的小错误导致函数调用未被正确解析。

https://github.com/vllm-project/vllm/issues/10951
这是一个用户提出需求的issue，主要涉及如何通过vllm获取token级别的概率分数。由于在使用vllm时无法得到与huggingface库相同的结果，用户寻求如何在vllm中实现相同操作。

https://github.com/vllm-project/vllm/issues/10950
这是一个bug报告，主要涉及到CI/Build中的测试失败，由于之前的命令执行失败而导致未能发现问题。

https://github.com/vllm-project/vllm/issues/10949
这是一个bug报告，主要涉及 tqdm 在 n 不等于 1 时显示进度不匹配的问题。原因可能是在处理1000个例子时，设置n为5导致tqdm显示为05000而实际上应显示1000导致混乱。

https://github.com/vllm-project/vllm/issues/10948
这是一个bug报告，涉及到`torch.compile`中的`shallow_copy_dict()`方法即将在未来版本中被弃用的问题。

https://github.com/vllm-project/vllm/issues/10947
这是一个需求类型的issue，主要涉及支持torchao量化模型。由于vllm不支持torchao量化方法，导致用户无法成功部署该模型。

https://github.com/vllm-project/vllm/issues/10946
该问题类型为用户提出需求，主要涉及的对象是新增"optimum-neuron"功能。由于vllm with aws neuron目前只支持llama和mistral，用户希望添加optimum-neuron功能来支持Qwen2.5等模型。

https://github.com/vllm-project/vllm/issues/10945
这个issue类型为性能优化提案，涉及主要对象为vllm中的版本V1，用户寻求帮助来优化性能提升。

https://github.com/vllm-project/vllm/issues/10944
该issue属于一个PR（Pull Request），并非bug报告，主要涉及的对象是Qwen2VL和Qwen2Audio模型的代码。由于存在冗余代码，该PR通过重用子模块中定义的逻辑来进行优化，以提高代码的复用性和简洁性。

https://github.com/vllm-project/vllm/issues/10943
这是一个用户提出需求的类型，主要对象应该是某个功能或某个特性。由于未提供具体内容，无法分析出导致的原因和问题类型。

https://github.com/vllm-project/vllm/issues/10942
这个issue属于需求提出类型，主要涉及的对象是动态KV缓存压缩，用户提出了在vLLM框架上基于KV稀疏性进行动态KV缓存压缩的需求。

https://github.com/vllm-project/vllm/issues/10941
这是一个关于代码编译时间记录的功能需求，主要对象是torch.compile模块，用户希望能够添加日志记录编译时间的功能。

https://github.com/vllm-project/vllm/issues/10939
这是一个用户提出需求的issue，主要对象是vLLM模型，用户请求添加对BERT和DistilBERT模型进行文本分类任务支持。这是因为这些模型在多语言支持、参数数量和推理速度等方面比Qwen2模型更具优势。

https://github.com/vllm-project/vllm/issues/10938
这是一个bug报告，问题涉及的主要对象是vllm模型中的blocksize参数。由于参数的合法取值与实际逻辑不符，导致了运行时错误“Unsupported block size: 128”。

https://github.com/vllm-project/vllm/issues/10937
这是一个用户提出需求的issue，主要涉及对象是AsyncLLMEngine的generate方法以及多模态输入支持，用户询问了如何在多模态模型中输入图像的问题。

https://github.com/vllm-project/vllm/issues/10936
这个issue属于用户提出需求的类型，主要对象是vLLM的release pipeline。造成这个问题的原因是目前缺乏针对构建和推送vLLM TPU镜像的发布管道任务。

https://github.com/vllm-project/vllm/issues/10935
这个issue是一个bug报告，主要涉及的对象是xgrammar作为后端解析时产生空输出的问题。造成这一bug的原因可能是xgrammar没有正确解析schema导致。

https://github.com/vllm-project/vllm/issues/10933
这是一个功能改进类型的issue，涉及到torch.compile中的size tuning。由于之前编译特定尺寸的模型存在性能问题，这个issue解决了这一瓶颈，显著提高了编译性能。

https://github.com/vllm-project/vllm/issues/10932
这是一个Bug报告类型的Issue，主要涉及vllm在Neuron设备上无法成功运行模型等相关问题。可能由于环境配置、安装方式或库版本不兼容导致无法成功运行。

https://github.com/vllm-project/vllm/issues/10931
这个issue是一个bug报告，主要涉及的对象是vLLM的OpenAI兼容HTTP服务器。由于并发请求导致服务器挂起，无法继续进行推理。

https://github.com/vllm-project/vllm/issues/10930
这是一个Bug报告。该问题涉及VLLM项目中SamplingParams和ChatCompletionRequest中默认值不一致导致在线和离线推理结果不一致的问题。

https://github.com/vllm-project/vllm/issues/10929
该issue类型为用户提出需求，用户希望通过Python脚本实现vllm的功能定制化，主要涉及的对象是vllm服务的使用和定制化需求。由于用户想要自定义日志输出，因此提出了如何通过Python脚本实现vllm功能的问题。

https://github.com/vllm-project/vllm/issues/10928
这个issue是一个Bug报告，主要涉及的对象是`causal_conv1d_fn`函数，由于没有提供`conv_states`且特定输入形状下，出现了尝试给`nullptr`赋值的情况，导致H100 GPU在调用该函数时出现非法内存访问错误。

https://github.com/vllm-project/vllm/issues/10927
这是一个关于正在进行中的功能开发的issue，主要涉及到深度学习模型加速相关的内容。原因是项目还很混乱，设计不足，导致部分功能未完成或存在问题。

https://github.com/vllm-project/vllm/issues/10926
这是一个用户提出需求的issue，主要对象是如何在vllm中运行特定模型推断。用户提出这个问题可能是因为不知道如何将特定模型集成到vllm中。

https://github.com/vllm-project/vllm/issues/10925
这是一个Bug报告，涉及vllm中chat/completions功能，由于引擎后台任务失败导致内部服务器错误。

https://github.com/vllm-project/vllm/issues/10924
这是一个Bug报告，主要涉及vllm在Alpine Linux基础镜像上无法安装/编译的问题，可能是由于依赖安装和编译过程中出现错误导致。

https://github.com/vllm-project/vllm/issues/10923
这是一个关于环境配置的bug报告，涉及对象是DeepSeek v2.5和VLLM-0.6.4。这个问题是由于OpenCV安装出现问题，导致出现了错误的属性，需要按照提供的链接修复环境。

https://github.com/vllm-project/vllm/issues/10922
这个issue属于bug报告类别，主要涉及VLLM（Very Low Latency Machine）模型部署的问题，由于模型id没有与deployment.yaml文件中定义的模型匹配而导致问题。

https://github.com/vllm-project/vllm/issues/10921
这是一个bug报告，涉及的主要对象是ModelConfig的初始化过程。这个问题是由于get_sentence_transformer_tokenizer_config方法在执行时无法传递token变量，导致无法提供token来检索配置。

https://github.com/vllm-project/vllm/issues/10920
这个issue类型是需求提出，主要涉及的对象是VLLM中的核心功能。由于希望统一`_run_workers`函数，以实现设备无关的执行器，因此需要移除`use_dummy`驱动。

https://github.com/vllm-project/vllm/issues/10919
这个issue属于用户提出需求类型，主要涉及提供预构建的CPU Docker镜像，由于当前需要自行构建Docker镜像，希望能够提供预构建的镜像以便用户使用。

https://github.com/vllm-project/vllm/issues/10918
这是一个bug报告，涉及主要对象为vllm中的测试文件test_multi_step_worker.py，该问题由于运行pytest时遇到非法内存访问而导致。

https://github.com/vllm-project/vllm/issues/10917
这是一个功能改进类型的issue，主要涉及Punica中LoRA函数接口的重组整理。导致该问题的原因是当前接口较为复杂和组织混乱，需要进行简化和标准化以方便在其他平台上支持LoRA。

https://github.com/vllm-project/vllm/issues/10916
这是一个bug报告，主要涉及vLLM在特定提示下的推理结果不正确。可能是由于PyTorch版本、CUDA版本或其他环境配置问题导致的。

https://github.com/vllm-project/vllm/issues/10915
这是一个关于添加测试的issue，主要对象是Python编译功能。这个问题可能是为了确保Python编译运行的稳定性而提出的。

https://github.com/vllm-project/vllm/issues/10914
这是一个bug报告，涉及到模型生成的token数目显示不正确的问题。

https://github.com/vllm-project/vllm/issues/10913
这是一个bug报告，主要涉及VLLM中Qwen-Coder-32B的speculative decoding对0.5B模型不起作用的问题。原因可能是不同词汇大小引起的。

https://github.com/vllm-project/vllm/issues/10912
这是一个bug报告，涉及的主要对象是vllm 0.6.4.post1版本。由于out of VRAM导致启动顺序不同而导致的问题。

https://github.com/vllm-project/vllm/issues/10910
这个issue类型是用户提出需求，主要涉及到vllm中processor的输出问题，用户在代码中发现一个例子让他感到困惑，希望能够得到解释。

https://github.com/vllm-project/vllm/issues/10909
这是一个需求添加新模型的issue，主要涉及到Bamba模型。由于新增模型的实现过程中存在部分问题，导致无法支持chunked prefill，需要完善相关功能。

https://github.com/vllm-project/vllm/issues/10908
这是一个关于基准测试的问题，需要将H100基准测试设置为可选项。

https://github.com/vllm-project/vllm/issues/10907
这是一个功能需求的issue，涉及的主要对象是请求统计更新和请求统计类型。

https://github.com/vllm-project/vllm/issues/10906
这个issue类型为功能增强，主要对象是torch.compile中的RMSNorm + (fp8) quant fusion功能。

https://github.com/vllm-project/vllm/issues/10905
这是一个用户提出需求的issue，主要涉及的对象是vllm的v1模型添加logits processor的支持。由于引入了v1的SamplingMetadata，需要决定如何处理v0版本的LogitsProcessor，导致需要讨论是否应支持同时处理v0和v1引擎的问题。

https://github.com/vllm-project/vllm/issues/10904
这是一个Bug报告，涉及模型在使用wiki图像URL时出现asyncio.exceptions.CancelledError的错误。

https://github.com/vllm-project/vllm/issues/10903
这个issue是一个bug报告，主要涉及的对象是LlaVa输出，在批量推断过程中产生错误结果，问题源于max_model_len无法被block_size整除。

https://github.com/vllm-project/vllm/issues/10902
这个issue是一个bug报告，主要涉及torch.compile中的RMSNorm + quant fusion在非cutlass-fp8情况下的修复问题，由于branch specialization的不正确工作和融合困难导致了该bug。

https://github.com/vllm-project/vllm/issues/10901
这个issue是关于更新llama 3.2模板以支持系统提示与图像，并不是bug报告。问题涉及到更新聊天模板以支持使用系统提示与图像。

https://github.com/vllm-project/vllm/issues/10900
这是一个Bug报告，涉及的主要对象是模型调用时未正确截断tool_call_id至9位数字，导致用户环境运行时出现了报错信息。

https://github.com/vllm-project/vllm/issues/10899
这个issue是关于bug修复，主要涉及到XGrammar对复杂JSON schemas的支持问题，由于XGrammar无法处理包含正则表达式模式或数字范围等复杂规则的JSON schemas，导致需要回退到outlines来解决该问题。

https://github.com/vllm-project/vllm/issues/10898
这是一个性能问题报告，主要涉及"LoRA adapter"的服务器启动与查询速度差异的问题。由于启用了LoRA adapter导致查询速度变慢，作者想了解这种额外运行时间是从何而来。

https://github.com/vllm-project/vllm/issues/10897
这个issue类型是功能改进，主要涉及的对象是PT HPU lazy backend。这个改进针对的是目前需要显式启用torch.compile并且性能最佳实践是使用HPUGraphs，通过禁用torch.compile并默认启用lazy collectives，解决了多HPU推理过程中的报错问题。

https://github.com/vllm-project/vllm/issues/10896
这个issue类型是bug报告，主要涉及Openai compatible server与HuggingFaceTB/SmolVLM-Instruct产生较小输出的问题，可能由于代码实现不同导致。

https://github.com/vllm-project/vllm/issues/10895
这是一个Bug报告，涉及的主要对象是VLLM库中的`causal_conv1d_fn`函数。由于输入长度为1026时，使用该函数会导致非法内存访问错误。

https://github.com/vllm-project/vllm/issues/10894
该issue属于功能需求类型，主要涉及到AQLM支持矩阵向量乘法的FP8/INT8情况，用户因推理大规模LLaMA 3 70B模型速度较慢而提出是否AQLM支持该功能。

https://github.com/vllm-project/vllm/issues/10893
这个issue类型为功能改进，主要涉及的对象是ViTs的注意力机制实现。通过添加`MultiHeadAttention`层来整合现有的重复注意力前向实现，以改进ViTs的性能。

https://github.com/vllm-project/vllm/issues/10892
这是一个用户提出需求的类型，主要涉及的对象是Vllm项目，用户希望在下一个版本中添加一些新功能，并询问了新版本发布的时间。

https://github.com/vllm-project/vllm/issues/10891
这是一个bug报告，涉及V1代码在批量推断期间输出错误结果的问题，可能由于代码中的某些逻辑错误或参数设置不正确而导致。

https://github.com/vllm-project/vllm/issues/10890
这是一个Bug报告，主要涉及VLLM中多阶段和异步输出处理过程中的问题。由于请求输出达到最大标记时导致剩余步骤未释放，可能导致下一个请求的第一个标记的延迟增加。

https://github.com/vllm-project/vllm/issues/10889
该issue属于用户提出需求，主要对象是serving VLM VILA。由于缺乏响应，导致用户提出了关于serving VLM VILA的需求。

https://github.com/vllm-project/vllm/issues/10888
这个issue类型为功能改进，涉及的主要对象是VLLM的CPU prefix caching机制，由于希望优化预填充步骤的效率而提出了这个改进建议。

https://github.com/vllm-project/vllm/issues/10887
这是一个更新维护类的issue，涉及将postmerge镜像从`vllmcitestrepo`更改为`vllmcipostmergerepo`，旨在修正相关实体引用问题。

https://github.com/vllm-project/vllm/issues/10886
这是一个用户提出需求的issue，涉及主要对象是如何进行多节点推理。由于文档未涵盖多节点推理的部分，用户希望了解如何实现多节点推理。

https://github.com/vllm-project/vllm/issues/10885
这是一个用户提出需求的issue，主要涉及对象是LogitsProcessor，用户想要访问当前生成的token。由于用户想要获取LogitsProcessor生成的token，但不清楚如何操作，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/10884
这是一个功能增强的issue，涉及到vLLM的Mooncake Transfer Engine支持的问题。

https://github.com/vllm-project/vllm/issues/10883
这是一个安装问题报告，主要涉及vLLM的CUDA 11.8安装问题。由于未能正确安装vLLM 0.6.3，并可能由于CUDA版本不匹配导致了该安装问题。

https://github.com/vllm-project/vllm/issues/10882
这是一个用户提出需求的issue，涉及到更新test-pipeline.yaml文件。原因可能是需要更新测试流水线的配置或内容。

https://github.com/vllm-project/vllm/issues/10881
这是一个bug报告，涉及主要对象为AsyncLLMEngine中的所有模型。这个问题是由于提交的请求长度超过max_model_len时，会触发assert失败，需要对所有模型添加长度检查。

https://github.com/vllm-project/vllm/issues/10880
这是一个bug报告类型的issue，主要涉及的对象是Benchmark serving structured output。该问题可能由于测试结果没有被正确输出或保存，导致了bug症状或用户提出了关于结果输出的问题。

https://github.com/vllm-project/vllm/issues/10879
这是一个bug报告，涉及的主要对象是vllm项目中的"benchmark_guided_serving.py"脚本。造成这个问题的原因可能是在使用服务器时指定的参数“guideddecodingratio 0”可能导致了某些意料之外的症状或行为。

https://github.com/vllm-project/vllm/issues/10878
这是一个功能需求，主要对象是vllm-tpu镜像用户，请求发布vllm-tpu镜像到DockerHub，因为目前用户需要手动构建镜像。

https://github.com/vllm-project/vllm/issues/10877
该issue属于需求类型，主要涉及Release pipeline中构建和推送`vllmopenai`镜像到Release ECR的任务。由于缺少这个功能，用户提出了关于构建和推送镜像的需求。

https://github.com/vllm-project/vllm/issues/10876
这是一个改进操作，目的是使`pynccl`测试更加健壮。

https://github.com/vllm-project/vllm/issues/10875
这个issue类型是需求变更，涉及到Release pipeline的队列命名变更。

https://github.com/vllm-project/vllm/issues/10874
这是一个建议性质的issue，涉及的主要对象是支持在CPU上卸载KV缓存。原因可能是之前的实现存在性能瓶颈，需要通过CPU卸载KV缓存来提升性能。

https://github.com/vllm-project/vllm/issues/10873
这是一个不包含任何具体内容的issue，类型为重命名，主要对象为Ibm dev，原因可能是代码库中的文件或者分支需要重命名。

https://github.com/vllm-project/vllm/issues/10872
这个issue类型为文档更新，主要涉及的对象是安装vllm_test_utils包，由于缺少该包导致运行单元测试时出现问题。

https://github.com/vllm-project/vllm/issues/10871
这是一个技术改进的提议，主要涉及到在引擎中异步初始化xgrammar，以提高性能和并行处理。

https://github.com/vllm-project/vllm/issues/10870
这是一个用户提出需求的issue，主要涉及核心功能的改进，需要扩展XGrammar以支持所有类型的语法，并移除回退功能。

https://github.com/vllm-project/vllm/issues/10869
这是一个bug报告类型的issue，涉及的主要对象是vllm模型库。由于vllm尚不支持要求的新模型“OLMo 2 13B”，用户遇到了无法运行所需模型的错误。

https://github.com/vllm-project/vllm/issues/10868
这个issue是一个功能需求，涉及VLM图片的哈希和映射缓存，用户希望实现跳过编码器执行和缓存结果。

https://github.com/vllm-project/vllm/issues/10867
这是一个用户提出需求的issue，主要涉及torch编译器中添加torch感应器传递以用于融合silu_and_mul与后续scaled_fp8_quant操作。此需求的原因可能是为了提高FP8 Llama和TTFT的性能。

https://github.com/vllm-project/vllm/issues/10866
这是一个Bug报告，主要涉及的对象是VLLM的代码。由于类型提示错误，当使用cython编译时会出现关键问题。

https://github.com/vllm-project/vllm/issues/10865
这是一个bug报告，主要涉及到XGrammar在非x86架构下的发布问题，由于XGrammar当前未发布适用于非x86平台的manylinux wheel，因此必须根据CPU限制使用，导致需要修复此问题。

https://github.com/vllm-project/vllm/issues/10864
这是一个与代码完整性测试相关的issue，主要涉及V1版本的Tensor Parallel功能。原因可能是OOM（内存溢出）问题导致测试失败。

https://github.com/vllm-project/vllm/issues/10863
这是一个Bug报告，主要涉及问题是在处理包含空seed的批次时出现错误，导致计算失败。

https://github.com/vllm-project/vllm/issues/10862
这是一个用户提出需求的issue，主要涉及每个请求包含不同的上下文无关文法或正则表达式，询问vLLM是否支持此功能。

https://github.com/vllm-project/vllm/issues/10861
这个issue是一个Bug报告，主要涉及VLLM在Ultra-Long Context下运行时出现错误。这可能是由于错误的环境配置或参数设置所导致的。

https://github.com/vllm-project/vllm/issues/10860
这是一个用户提出需求的类型，该问题单涉及主要对象为VLLM项目中的模型JambaForSequenceClassification。由于缺乏对应的模型支持，用户提出了需要添加JambaForSequenceClassification模型的需求。

https://github.com/vllm-project/vllm/issues/10859
该issue类型为bug报告，主要涉及的对象是XPU CI。这个问题的出现可能是由于XPU CI中存在某种错误导致需要快速修复。

https://github.com/vllm-project/vllm/issues/10858
这是一个用户提出需求的类型。问题单涉及的主要对象是"Model"。由于缺乏对嵌入模型JambaClassification的支持，用户提出了添加该模型支持的需求。

https://github.com/vllm-project/vllm/issues/10856
这是一个bug报告类型的issue，涉及到在使用vllm的docker部署过程中遇到zmq.error.ZMQError: Operation not supported错误，可能是由于docker部署环境配置有误导致。

https://github.com/vllm-project/vllm/issues/10855
这是一个Bug报告，主要涉及vLLM ROCm Image 在 Kubernetes Cluster with AMD GPUs 上的运行问题，由于无法成功初始化vLLM引擎导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/10854
这是一个bug报告，主要涉及的对象是参数设置中的"n"和"bestof"，导致bug的原因是类SamplingParams的clone()函数导致SamplingParams的__post_init__()函数被再次调用。

https://github.com/vllm-project/vllm/issues/10853
这个issue类型为bug报告，涉及的主要对象是vllm的请求预填充时间度量。这个问题由于记录时间度量的错误导致了混淆，需要修复。

https://github.com/vllm-project/vllm/issues/10852
这是一个类型为bug报告的issue，主要涉及vllm的chat completion api在使用sampling参数"N"时无法获得预期的独立样本输出。

https://github.com/vllm-project/vllm/issues/10851
这是一个用户提出需求的issue，涉及主要对象是vllm项目的Regional compilation support功能。

https://github.com/vllm-project/vllm/issues/10850
这是一个优化建议的issue，涉及到减少在`_filter_model_output`中多次调用`cudaStreamSynchronize`。原因是避免不必要的性能开销。

https://github.com/vllm-project/vllm/issues/10849
该issue类型为用户提出需求，主要涉及对添加DoRA支持的功能需求。由于DoRA在各种任务和backbones上表现出色，用户提出是否可以添加对其支持。

https://github.com/vllm-project/vllm/issues/10848
这个issue属于Bug报告类型，主要涉及的对象是GPTQ llama2-7b模型的量化和推理过程。由于某些原因导致了推理过程中遇到问题，可能是模型转换或输入导致的。

https://github.com/vllm-project/vllm/issues/10847
该issue为bug报告，涉及到模型输入参数长度不一致导致的问题。

https://github.com/vllm-project/vllm/issues/10846
这个issue是一个bug报告，涉及对象是自动递增多步骤问题。原因是没有请求没有调度程序导致了此问题。

https://github.com/vllm-project/vllm/issues/10845
该issue类型为其它，该问题单涉及的主要对象为临时whl文件。这个issue可能是用户想要测试临时whl文件的内容。

https://github.com/vllm-project/vllm/issues/10844
这是一个bug报告，涉及的主要对象是 `QKVParallelLinearWithShardedLora` 组件。该问题是由于引入的 https://github.com/vllmproject/vllm/pull/10829 而导致 `QKVParallelLinearWithShardedLora` 的偏差错误。

https://github.com/vllm-project/vllm/issues/10843
这个issue类型是功能需求提出，主要涉及分布式功能。由于缺少具体描述，无法准确分析具体原因。

https://github.com/vllm-project/vllm/issues/10842
这个issue是用户提出需求类型，主要涉及Model支持bitsandbytes quantization with minicpm model。这个问题由于缺乏bitsandbytes quantization支持而导致用户希望添加该功能。

https://github.com/vllm-project/vllm/issues/10840
这是一个缺少具体内容的issue，无法确定类型和主要对象。

https://github.com/vllm-project/vllm/issues/10839
这个issue类型是请求修改，主要对象是test-pipeline.yaml文件。由于可能需要更新测试管道的设定或配置，用户提出了更新test-pipeline.yaml的需求。

https://github.com/vllm-project/vllm/issues/10838
该issue类型为需求提出，主要涉及对象是torch.compile模块。这个问题由于编译上下文存在混淆，需要简化代码并在配置初始化期间确定cudagraph批处理大小。

https://github.com/vllm-project/vllm/issues/10837
这是一个文档更新的类型，涉及主要对象为KubeAI，更新原因可能是添加KubeAI到服务集成文档中。

https://github.com/vllm-project/vllm/issues/10836
这是一个bug报告，主要涉及对象为在rocm上启用 level 3 编译。导致该问题的原因是fusion pass未启用在rocm上，并且在生成的IR中使用了torch.narrow op，从而导致rms+fp8_quant fusion CC无法正常工作。

https://github.com/vllm-project/vllm/issues/10835
这是一个正在进行中的工作（Work in Progress）类型的issue，该问题主要涉及XGrammar的初始化问题。原因是希望通过单个warmup请求来预构建基本的GrammarCompiler，并且目前非常接近TTFT Nonguided的基准。

https://github.com/vllm-project/vllm/issues/10834
这是一个用户提出需求的issue，主要涉及对象是LlavaMultiModalProjector。由于尚未支持TP和BNB量化，用户提出需要向llava multimodal projector添加TP和BNB量化支持的问题。

https://github.com/vllm-project/vllm/issues/10833
这是一个Bug报告，涉及到使用QwenQwQ32BPreview vllm 0.6.4post1和OpenAI API、GGUF Kaggle AWQ Model API时出现的问题。

https://github.com/vllm-project/vllm/issues/10832
该issue为用户提出需求，主要涉及的对象是支持自定义CORS设置。由于缺乏对CORS中间件的支持，用户提出了添加自定义CORS设置的功能需求。

https://github.com/vllm-project/vllm/issues/10831
这是一个bug报告类型的issue，主要涉及的对象是"hermes_tool_parser.py"文件。由于json解析错误导致程序无法正确提取工具调用信息，进而引发了报错信息。

https://github.com/vllm-project/vllm/issues/10830
这个issue是关于bug报告的， 主要涉及的对象是"ValueError: not enough values to unpack (expected 2, got 1)"，可能由于版本不匹配或参数传递错误导致了此bug。

https://github.com/vllm-project/vllm/issues/10829
这是一个功能需求问题，涉及将lora偏置计算代码从layers.py和fully_sharded_layers.py移动到punica.py，以支持不同硬件平台上的lora功能。

https://github.com/vllm-project/vllm/issues/10828
这是一个Bug报告，涉及VLLM模型在运行过程中遇到的错误。BUG可能是由于GPU内存利用率过高导致的。

https://github.com/vllm-project/vllm/issues/10827
这是一个用户提出需求的类型，主要对象是文档结构。原因是部分页面与其父级章节不相关，创建了一个新的“Usage”章节以容纳这些页面。

https://github.com/vllm-project/vllm/issues/10826
这是一个bug报告，涉及问题是在vLLM中从1个GPU切换到2个GPU时出现的问题。该问题可能由于配置nvidia.com/gpu为2而导致模型无法加载并且pod日志无法顺利进行。

https://github.com/vllm-project/vllm/issues/10825
该issue属于代码改进类型，涉及主要对象是mistral_common版本。由于维护一致性导致的更新问题。

https://github.com/vllm-project/vllm/issues/10824
这是一个用户提出需求的issue，涉及的主要对象是LoRA support for LLama 3.2 Vision Models。用户因为需要在LLama 3.2 11B vision instruct中使用多个LoRAs，并且目前无法合并LoRA适配器到原始模型权重中，希望获得LoRA支持以及时间表和工作方法的帮助。

https://github.com/vllm-project/vllm/issues/10823
这是一个 Bug 报告类型的 issue，主要涉及 vllm 的多车推断 bnb 模型，因为支持 Tp 报错导致。

https://github.com/vllm-project/vllm/issues/10822
这是一个开发者提出的需求问题，主要涉及修改Attention机制的参数以满足类型检查需求。

https://github.com/vllm-project/vllm/issues/10821
这是一个Bug报告，涉及到Mistral工具选择错误导致的异常。

https://github.com/vllm-project/vllm/issues/10820
该issue类型为功能需求，涉及的主要对象是vLLM中的Pooling任务划分。由于新增了关于不同类型Pooling模型的任务划分需求，希望实现更灵活的模型功能。

https://github.com/vllm-project/vllm/issues/10819
这是一个bug报告，涉及的主要对象是vllm中的模型压缩和量化过程。由于使用2:4稀疏性时，模型输出异常，可能是由于模型加载和量化过程中出现的问题导致的。

https://github.com/vllm-project/vllm/issues/10818
这是一个功能需求的issue，主要对象是在vLLM中实现分散预填和KV缓存传输。原因是为了改善预填和解码之间的连接方式和性能。

https://github.com/vllm-project/vllm/issues/10817
这个issue属于代码优化类型，主要涉及到移除已废弃的标识符并引入新的`deprecated`装饰器，以消除警告。

https://github.com/vllm-project/vllm/issues/10816
这个issue属于增加新功能类型，主要涉及的对象是模型GritLM。由于需要让vLLM更易于采用，所以进行了对原始存储库的GritLM/GritLM7B的架构名称修改。

https://github.com/vllm-project/vllm/issues/10815
这是一个关于并发处理问题的bug报告，涉及VLLM中的process_model_inputs_async和engine_step两个coroutines未能同时运行导致的现象。

https://github.com/vllm-project/vllm/issues/10814
这是一个移除模型文件的问题，属于代码维护类型，主要涉及VLLM项目中的模型文件。

https://github.com/vllm-project/vllm/issues/10813
这是一个用户提出需求的问题，主要涉及了如何在vllm中使用bitsandbytes量化的llava模型，由于vllm不支持llava，导致用户无法成功加载模型。

https://github.com/vllm-project/vllm/issues/10812
这是一个Bug报告，主要涉及到在使用fp8模型时，出现了Engine进程意外死掉的问题。

https://github.com/vllm-project/vllm/issues/10811
这是一个功能改进类的issue，主要涉及FlashAttention库中的`flash_attn_varlen_func`函数，使用`out`参数来避免冗余复制。由于这个改变，可能减少了不必要的内存复制操作，提升了性能。

https://github.com/vllm-project/vllm/issues/10810
这是一个功能需求的issue，主要涉及的对象是vllm库下的LLM核心功能。原因是需要支持在运行时销毁所有KV缓存，以提供更灵活的内存管理。

https://github.com/vllm-project/vllm/issues/10809
这个issue类型是bug报告，涉及的主要对象是优化metrics log记录时的过滤条件。原因是在idle时存在log noise，但遗漏了speculative decoding和prefix cache metrics的log noise，导致log记录不完整。

https://github.com/vllm-project/vllm/issues/10808
这是一个Bug报告，涉及的主要对象是vllm中的stream generate功能。由于环境中PyTorch版本与CUDA版本不兼容导致的报错。

https://github.com/vllm-project/vllm/issues/10807
这个issue类型是bug报告，主要涉及的对象是vllm版本的性能问题，由于vllm版本在使用h100 GPU时表现缓慢导致的bug。

https://github.com/vllm-project/vllm/issues/10806
这是一个bug报告，主要问题是vllm服务器在终止客户端进程时未能中止请求，可能是由于服务器逻辑的缺陷导致。

https://github.com/vllm-project/vllm/issues/10805
这是一个关于文档修改的类型，主要涉及的对象是比较hf和vllm模型输出，由于在ci中始终使用`do_sample=False`导致出现了这个问题。

https://github.com/vllm-project/vllm/issues/10804
这是一个关于向 serving benchmark 中添加 MMMUPro 视觉数据集的 issue, 主要对象是数据集的集成，用户要求对该数据集进行支持。

https://github.com/vllm-project/vllm/issues/10803
这是一个用户提出需求的issue，主要涉及的对象是添加xgrammar作为引导生成提供者。原因是由于outlines相对较慢且难以集成，而新版本不注重可pickle性，这对于使用多处理引擎的我们来说是一个关键特性。

https://github.com/vllm-project/vllm/issues/10802
这是一个Bug报告，涉及到VLLM项目下的API server启动过程中存在的竞态条件导致输出token顺序错误的问题。

https://github.com/vllm-project/vllm/issues/10801
这个issue是一个API更名的修改类型，涉及vllm项目中的embedding类的重命名。原因可能是为了更清晰地表示其功能或标准化命名规范。

https://github.com/vllm-project/vllm/issues/10800
这是一个关于bug报告的issue，主要涉及到使用CUDA和多进程时出现的错误，导致无法重新初始化CUDA在forked子进程中，需要使用'spawn'启动方法。

https://github.com/vllm-project/vllm/issues/10799
该issue属于bug报告类型，主要涉及TorchSDPAAttentionMetadata对象，在prefill时的seq_lens_tensor为空导致了Robertam模型在prefill过程中无法固定position_ids张量的长度。

https://github.com/vllm-project/vllm/issues/10798
这是一个bug报告类型的issue，涉及vLLM下的LoRa适配器响应与peft/transformers响应不匹配的问题。原因可能是适配器未正确加载属性，导致模型无法正确识别问题内容。

https://github.com/vllm-project/vllm/issues/10797
这是一个bug报告，主要涉及到使用vllm运行推理时内存不足的问题。由于GPU内存不足，无法加载llama 3.2 3b模型。

https://github.com/vllm-project/vllm/issues/10796
这是一个bug报告类型的issue，主要涉及对象是lora_tokenizers。由于lora_tokenizers的capacity设置不当，导致性能下降并出现了未必要的evicted现象。

https://github.com/vllm-project/vllm/issues/10795
This issue is a feature request for adding BNB support to the Llava and Pixtral-HF models, involving fixing the usage of `llavahf/llava1.57bhf` with bitsandbytes quantization in `vllm serve`.

https://github.com/vllm-project/vllm/issues/10794
这是一个Bug报告，涉及到vLLM中错误消息提升问题，由于环境中PyTorch版本不匹配导致。

https://github.com/vllm-project/vllm/issues/10793
这是一个bug报告，主要涉及的对象是openvino工作器。由于最近在openvino工作器中dummy_data_for_profiling签名的更改，导致了需要修复openvino在GPU上的问题。

https://github.com/vllm-project/vllm/issues/10792
这是一个Bug报告类型的issue，主要涉及到beam search中的一个参数限制问题，可能是由于新版本中的更新导致了max_logprobs参数不得高于20的错误。

https://github.com/vllm-project/vllm/issues/10791
这是一个bug报告，主要涉及vllm-flash-attn视觉模块的问题，用户反馈无法运行推理并询问修复计划。

https://github.com/vllm-project/vllm/issues/10790
这是一个用户提出需求的类型的issue，主要对象是vLLM中的模型。由于需要更高的速度而非质量，用户提出可以直接访问draft模型的建议。

https://github.com/vllm-project/vllm/issues/10789
这是一个bug报告，涉及的主要对象是Intel Gaudi文档。由于某些文件标题级别需要更新，导致在索引页上显示了不应暴露的Intel Gaudi文档部分。

https://github.com/vllm-project/vllm/issues/10788
这是一个用户提出问题的issue，主要涉及的对象是关于参数"max_model_len"的影响。用户想确认该参数只影响输入数据和scheduler，而不影响实际模型的max_position_embeddings。

https://github.com/vllm-project/vllm/issues/10787
这是一个改进优化类型的issue，涉及主要对象为CPU vector types。由于ISA specifics被放置在不适合的位置，导致了代码可读性和可维护性下降。

https://github.com/vllm-project/vllm/issues/10786
这是一个用户提出需求的类型，主要涉及的对象是支持的模型。导致该问题的原因是当前版本的模型启动后得到的答案都是符号，用户希望尽快支持这个模型。

https://github.com/vllm-project/vllm/issues/10785
这是一个特性增强类的issue，涉及的主要对象是VLLM的XGrammar支持。原因是为了增加对引导解码的支持并将其设置为默认值。

https://github.com/vllm-project/vllm/issues/10784
这是一个用户需求类型的issue，主要对象是LoRa模型加载在/models端点无法显示，可能是由于动态加载LoRa导致的。

https://github.com/vllm-project/vllm/issues/10783
该issue类型为用户提出需求，主要涉及希望添加新模型"nvidia/Hymba-1.5B-Base"支持，并讨论了对于"Mamba"和注意力头的混合架构支持以及滑动窗口和元（内存）token的注意力。

https://github.com/vllm-project/vllm/issues/10782
这个issue是一个bug报告，主要涉及工具流中使用hermes和mistral解析器时的聊天完成工具参数流的修复。由于工具选择自动时，常常会截断参数流。

https://github.com/vllm-project/vllm/issues/10781
这个issue是一个Bug报告， 主要涉及的对象是在使用"auto"工具选择时，在流式传输参数时出现截断最终增量的问题。这个问题是由于在流式传输中出现了多个问题，包括Delta创建时不会立即发送、工具结束时创建新的Delta而不会提交原始Delta等原因导致的。

https://github.com/vllm-project/vllm/issues/10779
这是一个Bug报告，涉及到OpenVino/Neuron的`driver_worker`初始化问题，导致了 https://github.com/vllmproject/vllm/issues/10775.

https://github.com/vllm-project/vllm/issues/10778
这是一个bug报告，主要涉及到Idefics3模型在OpenAI兼容服务器上与HuggingFaceTB/SmolVLMInstruct集成时出现的问题，导致500 Internal Server错误和流媒体问题。

https://github.com/vllm-project/vllm/issues/10777
这是一个关于如何跳过具有错误的样本并处理剩余样本的使用问题，涉及的主要对象是LLM生成器。这个问题可能是用户在使用LLM生成器时遇到了一些样本处理错误的情况，希望能够找到一种方法来跳过这些错误样本继续处理其余样本。

https://github.com/vllm-project/vllm/issues/10776
这是一个性能优化改进的issue，主要涉及到vllm模型中的sin/cos buffers准备工作。

https://github.com/vllm-project/vllm/issues/10775
这是一个Bug报告，涉及到OpenVINO在CPU上推断时出现了"init_device"错误。这个问题的原因可能是模型转换和部署中的步骤或参数设置有误。

https://github.com/vllm-project/vllm/issues/10774
这是一个功能需求的issue，主要对象是vllm引擎。由于长序列预填充导致引擎变得不稳定，需要解决多用户同时请求时资源分配的问题。

https://github.com/vllm-project/vllm/issues/10773
这是一个bug报告，涉及的主要对象是"Qwen2Model"。原因是在进行awq模型量化时报错，可能是由于属性'rotary_emb'在对象'Qwen2Model'中不存在导致的。

https://github.com/vllm-project/vllm/issues/10772
这是一个Bug报告类的Issue，主要涉及到在尝试运行HuggingFaceTB/SmolVLMInstruct时遇到500 Internal Server Error的问题。

https://github.com/vllm-project/vllm/issues/10771
这是一个功能优化的issue，涉及主要对象为`Molmo`模型权重加载，希望通过重构使用`WeightsMapper`和`AutoWeightsLoader`解决问题。

https://github.com/vllm-project/vllm/issues/10770
这是一个缺陷报告，涉及 vllm 项目中对 aria model 缺乏测试的问题。可能由于添加了 aria model 的更新，但未包含任何模型测试，导致需要此问题单来补充测试。

https://github.com/vllm-project/vllm/issues/10769
这是一个功能需求类型的issue，主要涉及的对象是vLLM模型中的embedding models。

https://github.com/vllm-project/vllm/issues/10768
这是一个功能改进类型的issue，主要涉及到平台端的设备硬编码问题。

https://github.com/vllm-project/vllm/issues/10767
这个issue是关于bug报告类型的，涉及的主要对象是代码中关于ROCm load format检查的部分。导致这个问题的原因是对`rocm_not_supported_load_format`的数值在一年多的时间里一直保持为空值，作者认为这个检查已经不再需要，提出了清理相关检查代码的PR。

https://github.com/vllm-project/vllm/issues/10766
这个issue是关于bug报告，主要涉及vLLM代码库中vllm.openai.rpc文件夹被移除导致引入问题。

https://github.com/vllm-project/vllm/issues/10765
这是一个关于向VLLM上游格式Quark的问题单，涉及的主要对象是格式转换。

https://github.com/vllm-project/vllm/issues/10764
这个issue是关于bug报告，主要涉及Idefics3模型在进行stream操作时出现的问题。这个问题可能是由于模型无法正常stream导致的。

https://github.com/vllm-project/vllm/issues/10763
这是一个bug报告，涉及的主要对象是`support_torch_compile`装饰器。这个bug是由于`support_torch_compile`装饰器在被其他地方使用时擦除了原始类的信息，修复的PR提供了更精确的类型注解以解决这个问题。

https://github.com/vllm-project/vllm/issues/10762
这个issue类型属于bug报告，主要涉及的对象是vllm在加载Lora模型时无法从ModelScope下载的问题，以及私有模型需要手动登录的问题。这个问题的原因是vllm无法自动加载来自ModelScope的Lora模型，并且对于私有模型需要手动设置环境变量才能访问。

https://github.com/vllm-project/vllm/issues/10761
这是一个bug报告，主要涉及对象是vllm中的ChatGLMTokenizer，由于版本不匹配导致了TypeError。

https://github.com/vllm-project/vllm/issues/10760
这个issue是用户提出需求类型的问题单，主要涉及对象是vllm的日志打印功能。用户提出寻求在服务端查看模型输出的需求。

https://github.com/vllm-project/vllm/issues/10759
这是一个bug报告，主要涉及的对象是尝试加载Qwen2.57BInstruct模型时遇到的ValueError。由于模型架构Qwen2ForCausalLM无法被检测，导致出现错误信息提示。

https://github.com/vllm-project/vllm/issues/10758
该issue类型为用户提出需求，主要涉及ChatCompletionRequest的默认值取自generation_config.json文件，由于用户未设置这些值导致的问题。

https://github.com/vllm-project/vllm/issues/10757
这是一个用户提出需求的issue，主要涉及到添加新的功能和优化代码结构，以区分设备名称。这个issue的原因可能是为了避免使用过多的if else语句并且需要更好地区分不同的设备名称。

https://github.com/vllm-project/vllm/issues/10756
该issue属于用户提出需求类型，主要涉及vllm的vision ID支持，用户寻求如何为processor分配vision ID。

https://github.com/vllm-project/vllm/issues/10755
这是一个用户提出使用问题的issue，主要涉及使用`use_image_id`和`max_slice_num`参数时遇到的困惑，导致无法在线推断，可能由于缺乏相关指导或正确参数设置而引发。

https://github.com/vllm-project/vllm/issues/10754
这是一个关于用户提出需求的问题，涉及的主要对象是vllm项目的beam search功能。由于新方法对beam search功能进行了限制，用户请求在新方法中加入对生成控制（top_p等）和受限制beam search的支持。

https://github.com/vllm-project/vllm/issues/10753
这是一个bug报告，主要涉及 VLLM 中 benchmark_throughput.py 脚本的问题。由于默认情况下启用了前缀缓存，并且所有提示都是相同的，导致该脚本误导性地显示了V1的性能过于良好。

https://github.com/vllm-project/vllm/issues/10752
这是一个用户提出需求的issue，主要涉及的对象是embedding models。原因是目前只有crossencoder models支持`/score`端点，用户想要将其扩展到使用biencoding的embedding models上，以计算嵌入向量之间的余弦相似度得分。

https://github.com/vllm-project/vllm/issues/10751
这是一个优化代码的类型，主要涉及的对象是MiniCPMV，原因是为了简化权重加载代码和消除对LLMWrapper的使用。

https://github.com/vllm-project/vllm/issues/10750
该issue类型为更新需求，主要涉及vllm-hpu-extension的更新。由于需要引入PipelinedPA，导致需要将vllm-hpu-extension更新为50e10ea版本。

https://github.com/vllm-project/vllm/issues/10749
这个issue类型是需求反馈，主要涉及的对象是vllm项目的模型并行配置，由于配置与张量并行要求不兼容，导致存在无法生成张量并行组的问题。

https://github.com/vllm-project/vllm/issues/10748
这是一个用户提出需求的issue，主要涉及需求是关于个别请求的能耗消耗。

https://github.com/vllm-project/vllm/issues/10747
这是一个 Bug 报告，主要涉及的对象是 XPU 平台上的 torch.compile() 方法。这个问题的原因是在 XPU 平台进行张量并行时，torch.compile() 会触发 triton 调用，因此需要禁用 torch.compile()。

https://github.com/vllm-project/vllm/issues/10746
这是一个Bug报告，涉及的主要对象是vllm中的模型执行过程。由于gguf quantization存在性能不足的问题，导致了RuntimeError的错误。

https://github.com/vllm-project/vllm/issues/10745
这个issue是关于安装（Installation）的，主要涉及vllm的编译安装及获取wheel文件，用户可能遇到了安装环境问题或操作相关困惑。

https://github.com/vllm-project/vllm/issues/10743
这是一个Bug报告，涉及VLLM中使用OpenAI API时请求错误导致的问题。

https://github.com/vllm-project/vllm/issues/10742
这个issue属于功能更新类型，涉及的主要对象是 vllm-flash-attn 内核版本。由于当前版本存在较大的CPU开销，在启动内核时会导致性能下降，因此需要升级版本来减少CPU开销。

https://github.com/vllm-project/vllm/issues/10741
这是一个关于性能优化的issue，主要涉及logit bias实现方式的问题，提议用`scatter_add`替代慢速for循环，以提高生成速度。

https://github.com/vllm-project/vllm/issues/10740
这是一个bug报告类型的issue，涉及到对sampling_metadata.py文件的修改。导致这个问题的原因是忘记复制Sampling部分，导致代码逻辑过于复杂需要重构。

https://github.com/vllm-project/vllm/issues/10739
这个issue是一个功能改进的请求，主要涉及的对象是RMSNorm和Mamba模型。该问题由于未能正确初始化RMSNorm层的权重，导致需要添加`elementwise_affine`参数来解决无法学习的RMSNorm层问题，同时也需要重新启用Mamba模型的权重加载跟踪功能。

https://github.com/vllm-project/vllm/issues/10738
这个issue属于bug报告类型，涉及对象为vllm项目中的rerank功能。由于使用/v1/score执行Rerank时出现了RuntimeError导致容器退出。

https://github.com/vllm-project/vllm/issues/10737
这是一个需求提交的issue，主要涉及支持新模型的问题，由于当前配置无法处理超过32,768 tokens的长文本输入。

https://github.com/vllm-project/vllm/issues/10736
这个issue是与性能优化相关的，主要涉及的对象是vllm-flash-attn。原因是为了提高`flash_attn_varlen_func`的运行速度，避免了两次冗余的复制操作。

https://github.com/vllm-project/vllm/issues/10735
这是一个描述Bug报告类型的Issue，涉及的主要对象是vllm库中的GPUExecutorAsync类。这个问题源于无法找到`loop._default_executor`属性，导致用户无法正确运行在线模式的推理操作和分析ThreadPoolExecutor中的进程数量。

https://github.com/vllm-project/vllm/issues/10734
这是一个Bug报告类型的issue，主要涉及vLLM中模型文件名过长导致torch.distributed.DistBackendError错误的问题。

https://github.com/vllm-project/vllm/issues/10733
这是一个优化性能的issue，主要涉及的对象是FlashAttention custom op。由于 `FlashAttnFunc` 继承 `torch.autograd.Function` 导致不必要的CPU开销，需要进一步减少FlashAttention op内的不必要CPU操作。

https://github.com/vllm-project/vllm/issues/10732
这是一个文档更新类的issue，主要涉及配置文件的文档更新，由于缺少参数描述而需要更新。

https://github.com/vllm-project/vllm/issues/10731
这是一个升级文件锁版本的类型，主要涉及的对象是第三方库的版本更新。由于版本问题可能导致bug或者需要用户提出升级需求。

https://github.com/vllm-project/vllm/issues/10730
这是一个bug报告，主要涉及V1版本KV缓存管理器在请求中分配了超过`max_model_len // block_size`块的错误。这个问题是因为分配的块超过了block table的固定形状引起的。

https://github.com/vllm-project/vllm/issues/10729
这个issue类型为功能需求，主要涉及的对象是对模型权重文件锁定功能的改进。

https://github.com/vllm-project/vllm/issues/10728
这个issue类型是功能改进，主要涉及Mooncake Transfer Engine在LLM服务中的应用。这个改进是为了使用Mooncake的Transfer Engine来实现KVCache的传输，取代原先的NCCL。

https://github.com/vllm-project/vllm/issues/10727
这个issue属于需求提出类型，主要涉及Mooncake框架下的vLLM集成，原因是为了实现更好的LLM推断性能。

https://github.com/vllm-project/vllm/issues/10726
这是一个需求类型的issue，主要涉及的对象是更新依赖项到torch_xla[tpu]的版本至20241126。

https://github.com/vllm-project/vllm/issues/10725
该issue属于新功能需求，主要涉及Ray executor的支持。由于之前出现的性能问题，该PR计划只支持Ray编译图版本的后端，解决了性能问题，需要进行清理测试、性能基准测试，并统一接口。

https://github.com/vllm-project/vllm/issues/10724
这是一个bug报告，涉及CLI的功能修改，讨论了如何显式禁用前缀缓存。这个问题产生的原因是用户需求改变导致的行为变化。

https://github.com/vllm-project/vllm/issues/10723
这是一个文档相关的问题，用户提出了关于BNB 8位量化文档不完整的需求。

https://github.com/vllm-project/vllm/issues/10722
这是一个bug报告，涉及到vllm库中的shape specialization问题，由于导致了运行时错误或不正确行为。

https://github.com/vllm-project/vllm/issues/10721
这是关于如何从本地路径加载经微调的模型的使用问题，原因是保存的模型格式可能导致与vllm不兼容。

https://github.com/vllm-project/vllm/issues/10720
这是一个bug报告类型的issue，主要涉及到了BNB loader中的`target_modules`参数。由于对`target_modules`参数的误解导致了添加LoRA适配器的错误图层，而不是对应图层的量化。

https://github.com/vllm-project/vllm/issues/10719
这是一个bug报告，涉及的主要对象是加载嵌入模型时忽略`lm_head`。由于代码加载模型时未正确处理`lm_head`导致的问题。

https://github.com/vllm-project/vllm/issues/10718
这是一个Bug报告类型的Issue，涉及的主要对象是VLLM中的XLMRobertaForSequenceClassification模型。由于环境配置或代码实现问题，导致模型无法正确加载并运行，出现了报错信息。

https://github.com/vllm-project/vllm/issues/10717
这个issue是一个bug报告，主要涉及的对象是在docker swarm环境下安装vllm时出现了RuntimeError。原因可能是环境配置不兼容或参数设置有误导致的。

https://github.com/vllm-project/vllm/issues/10714
这是一个功能需求的issue，主要涉及vLLM的GPU缓存管理，用户提出了希望实现GPU内存中的KV缓存清理功能（或`睡眠模式`），以便在推理请求空闲时允许GPU执行其他计算任务。

https://github.com/vllm-project/vllm/issues/10713
这是一个bug报告，主要涉及到vllm的推断功能，用户遇到了输出重复的问题，可能由于GPU类型或cuda驱动程序引起。

https://github.com/vllm-project/vllm/issues/10711
这个issue类型是功能增强请求，主要涉及的对象是更新多模态处理器以支持Mantis模型。

https://github.com/vllm-project/vllm/issues/10710
这是一个bug报告，涉及到vllm下unsloth bitsandbytes quantized model无法运行，主要原因是权重可能需要手动修补。

https://github.com/vllm-project/vllm/issues/10709
这是一个 Bug 报告，主要涉及性能问题，由于增加请求批处理大小导致了明显的性能下降。

https://github.com/vllm-project/vllm/issues/10708
这是一个bug报告，涉及的主要对象是使用nsys profile检测vllm性能时，CUDA HW在部分设备上无法显示。由于未知原因，其余三个进程无法看到包含cuda内核的CUDA HW。

https://github.com/vllm-project/vllm/issues/10707
这是一个性能优化的提案，提出将预填和解码处理统一在一个前向调用中，但在尝试后并未发现性能提升，希望通过进一步研究解释这种意外的减速原因。

https://github.com/vllm-project/vllm/issues/10706
这是一个bug报告，涉及VLLM在ARM cpu上运行非常缓慢的问题，可能是由于VLLM在ARM cpu上的支持存在问题导致的。

https://github.com/vllm-project/vllm/issues/10705
这是一个bug报告，主要涉及对象是Mamba模型，问题由于在Mamba中多步调度器执行过程中未将已完成请求的ids传递至MambaCacheManager，导致内存不足，无法接收新的请求。

https://github.com/vllm-project/vllm/issues/10704
这是一个用户提出需求的issue，主要涉及的对象是将genai_perf纳入夜间基准测试，由于需要的Python版本高于3.10，还未启用且需定义更多测试用例。

https://github.com/vllm-project/vllm/issues/10703
这个issue类型是功能更新，主要涉及mistral-format Pixtral接口的更新。

https://github.com/vllm-project/vllm/issues/10702
这是一个bug报告类型的issue，主要涉及vllm下的LLAMA 70B模型加载时间过长的问题。

https://github.com/vllm-project/vllm/issues/10701
这是一个文档更新请求，针对 VLLM 模型架构的更新，主要对象是代码文档，可能由于描述与实际代码不符导致不一致。

https://github.com/vllm-project/vllm/issues/10700
这个issue是一个bug报告，涉及的主要对象是intel-omp（Intel OpenMP）。这个问题由于最新的intelomp版本会在进程退出时导致段错误，因此提出需将intelomp版本修复以避免问题。

https://github.com/vllm-project/vllm/issues/10699
这是一个功能需求的issue，主要涉及到V1模型的支持，因为缺少Qwen2VL的MRope实现而导致需要通过单独的PR来解决。

https://github.com/vllm-project/vllm/issues/10698
这是一个bug报告，涉及到CI环境性能低下导致测试运行缓慢的问题。

https://github.com/vllm-project/vllm/issues/10697
这是一个bug报告类型的issue，主要涉及了使用vllm部署finetuned Mistral模型后推理结果与期望结果不完全匹配的问题。原因可能是缺少额外的参数或配置导致的。

https://github.com/vllm-project/vllm/issues/10696
这个issue属于用户提出需求，主要涉及V1版本中是否支持frequency_penalties，由于该功能在V1中似乎被移除导致用户发现重复率严重增加。

https://github.com/vllm-project/vllm/issues/10695
这是一个bug报告，涉及主要对象为`get_computed_blocks`函数。这个问题是由于重复计算导致的。

https://github.com/vllm-project/vllm/issues/10693
这是一个Bug报告，涉及主要对象是`MambaCacheManager`。由于`_assign_seq_id_to_cache_index`在某些情况下可能会过度弹出自由索引，导致缓存占用可能会超过`max_batch_size`，从而引发错误。

https://github.com/vllm-project/vllm/issues/10692
这是一个关于bug报告的issue，主要涉及的对象是guided decoding参数在使用/generate时出错。导致这个bug的原因可能是参数错误导致的症状或用户需要修复这部分代码。

https://github.com/vllm-project/vllm/issues/10691
这是一个关于bug报告的issue，主要涉及的对象是guided decode参数在使用`/generate`时出现错误。这个问题可能是由于参数错误或环境配置问题导致的。

https://github.com/vllm-project/vllm/issues/10690
该issue属于性能优化类型，主要对象为多核CPU利用率不足。这个问题由于推断过程中CPU负载过高，提出需求将工作负载分布到更多进程以充分利用多核CPU能力。

https://github.com/vllm-project/vllm/issues/10689
这是一个bug报告，主要涉及的对象是在使用CPU Docker构建时遇到了构建失败的问题。由于Docker构建时出现了某种错误，导致了无法成功构建 vllmcpuenv 的容器。

https://github.com/vllm-project/vllm/issues/10688
这是一个用户提出需求的问题，用户希望得到VLLM模型中输出每一个token的logits而不是经过softmax处理后的logprob，由于VLLM目前不提供logits的输出，用户询问如何实现此功能。

https://github.com/vllm-project/vllm/issues/10687
这个issue是一个bug报告，涉及的主要对象是`marlin_24_cuda_kernel.cu`文件中的if-else结构。由于复制粘贴错误导致存在没有意义的ifelse，需要清理代码。

https://github.com/vllm-project/vllm/issues/10686
这是一个 bug 报告，涉及到 v0.6.4.post1 Qwen2-VL-7B-Instruct-AWQ crash：shape mismatch 的问题。由于 shape mismatch 引起了程序崩溃。

https://github.com/vllm-project/vllm/issues/10685
这是一个Bug报告类型的Issue，主要涉及VLLM在测试性能时出现了AssertError。原因是设置特定环境变量后，在使用swap预占模式时，运行benchmark_serving.py脚本会触发AssertionError。

https://github.com/vllm-project/vllm/issues/10684
这是一个bug报告，主要涉及vLLM的Python进程是否能够捕获api_server，可能导致了无法捕获API服务器的问题。

https://github.com/vllm-project/vllm/issues/10682
这个issue类型为功能需求，涉及的主要对象是模型支持的量化方式，用户希望在 minicpm3 模型中添加 bitsandbytes 量化支持。

https://github.com/vllm-project/vllm/issues/10681
该issue类型是一个功能提议，主要涉及到V1版本的采样器功能的增强。原因是为了支持最小标记数量、重复惩罚、存在惩罚和频率惩罚功能的采样。

https://github.com/vllm-project/vllm/issues/10680
这个issue类型为功能更新，主要对象是`Idefics3ForConditionalGeneration`类。由于CC([V1] Refactor model executable interface for multimodal models)未更新其接口，导致该类需要更新。

https://github.com/vllm-project/vllm/issues/10679
这是一个需求类型的issue，涉及到多模型的重新基础。由于需要重新基础多个模型，用户提出了这个问题寻求帮助。

https://github.com/vllm-project/vllm/issues/10677
这是一个Bug报告，该问题涉及的主要对象是LLMEngine在benchmark_throughput.py中使用`-O[0,3]`时出现的问题。原因是在LLMEngine中的CompilationConfig预处理时，尝试将其转储为json，这是不必要的。

https://github.com/vllm-project/vllm/issues/10676
这个issue类型是功能增强（Feature Enhancement），主要涉及的对象是LLaVA模型的输入处理器（Input Processor）。原因是为了支持多模态模型，更新了`MultiModalProcessor`及相关组件。

https://github.com/vllm-project/vllm/issues/10675
这是一个Bug报告，该问题涉及的主要对象是GGUF模型输出在FP16未量化检查点上重复非意义性的问题，可能由于程序中的错误导致。

https://github.com/vllm-project/vllm/issues/10674
这是一个用户提出需求的RFC（Request for Comments）类型的issue，主要对象是vLLM中的文本生成模型，原因是目前需要针对每个现有架构单独开展新的PR来添加池化功能，希望通过自动化来消除这种重复的工作。

https://github.com/vllm-project/vllm/issues/10673
这是一个用户提出需求的issue，主要涉及对象是模型Llama-2-7b-chat-hf，由于修改模型为Llama27bchathf后运行代码时遇到错误导致无法成功进行推断并提取嵌入向量。

https://github.com/vllm-project/vllm/issues/10672
这个issue类型是增加文档的功能，主要对象是代码引用，原因是为了给代码引用添加github链接。

https://github.com/vllm-project/vllm/issues/10671
这是一个Bug报告，涉及对象是在使用VLLM时无法生成输出。可能是因为嵌入计算发生在CPU而不是GPU上导致的。

https://github.com/vllm-project/vllm/issues/10670
该issue类型为用户提出需求，主要涉及如何在vllm中获取每个输出token的得分，由于用户希望了解在vllm中如何实现类似transformers中output_scores的功能。

https://github.com/vllm-project/vllm/issues/10668
这是一个bug报告，涉及的主要对象是使用Ray作为Qwen2-VL推理后端时出现推理结果截断或重复的问题，导致出现异常情况。

https://github.com/vllm-project/vllm/issues/10667
这是一个bug报告，涉及的主要对象是HPUAttentionBackend类，由于缺少get_name方法，导致在选择后端时抛出错误。

https://github.com/vllm-project/vllm/issues/10666
这是一个提出需求的Issue，主要涉及的对象是VllmConfig以及VllmState。由于VllmConfig中的一些参数对用户来说应该是不可变的，因此提出创建VllmState来管理这些参数。

https://github.com/vllm-project/vllm/issues/10665
这个issue是一个功能需求问题，主要涉及的对象是`LLMEngine`。由于缺少对`AsyncLLMEngine`的profile支持，用户提出了需要启用此功能的需求。

https://github.com/vllm-project/vllm/issues/10664
这个issue属于性能问题报告，主要涉及VLLM中的部署模型性能差异，可能由于某些原因导致了性能回归的症状。

https://github.com/vllm-project/vllm/issues/10663
这个issue类型是用户提出需求，请求解决Llama 3.1 405B-FP8模型与AMD Mi250的兼容性问题。

https://github.com/vllm-project/vllm/issues/10662
这个issue类型是bug报告，主要涉及xformers库无法在老版本GPU上运行，导致出现错误。

https://github.com/vllm-project/vllm/issues/10661
这个issue是关于bug报告，主要涉及vLLM模型中设置task为"embedding"时出现错误的情况。导致该错误可能是由于特定设定下无法找到可用的block所致。

https://github.com/vllm-project/vllm/issues/10660
这个issue是一个功能需求提案，主要涉及与XGrammar集成以在LLM推理中实现零额外负担的结构化生成。原因为希望实现更高效灵活的结构化生成。

https://github.com/vllm-project/vllm/issues/10659
这是一个Bug报告，涉及的主要对象是在测试脚本中无法导入vllm模块，导致需要重复使用测试脚本但只在测试环境中安装该模块。

https://github.com/vllm-project/vllm/issues/10658
这是一个需求提议，主要涉及对于在macOS上安装vllm的脚本添加的问题。由于之前的更新，需要清理dockerfile并找到安装vllm的方法。

https://github.com/vllm-project/vllm/issues/10657
这是一个bug报告，涉及的主要对象是`BitsAndBytesConfig`。导致这个bug的原因是默认值`llm_int8_threshold`在代码中被设置错误。

https://github.com/vllm-project/vllm/issues/10656
这是一个bug报告，主要涉及Qwen2.5-32B-GPTQ-Int4模型在vllm中推理时出现的问题，导致生成结果只包含"!!!!!"。

https://github.com/vllm-project/vllm/issues/10655
该issue类型为代码改进，涉及的主要对象是vllm的optional protocols，由于每个模型现在应该接受vllm_config参数，因此需要移除旧的__init__规范。

https://github.com/vllm-project/vllm/issues/10654
这是一个用户提出需求的issue，涉及主要对象是Qwen2-VL在AWS Inf2上的运行支持情况。由于无人回复，用户在询问Qwen2VL是否支持在AWS Inf2上运行。

https://github.com/vllm-project/vllm/issues/10653
这是一个bug报告，涉及主要对象是AMD GPU RX 7900XT，由于无法推断设备类型导致的问题。

https://github.com/vllm-project/vllm/issues/10652
这是一个Bug报告，主要涉及的对象是在L20 GPU上进行推理时速度异常缓慢。导致这个问题的原因可能是环境配置、PyTorch版本、CUDA版本等问题。

https://github.com/vllm-project/vllm/issues/10651
这个issue是关于需求和设计提案，主要涉及到指标和统计数据的原型功能。

https://github.com/vllm-project/vllm/issues/10650
这个issue属于bug报告类型，主要涉及的对象是vllm中的Qwen2VL72BInstructGPTQInt8模型。这个问题是由于引入"from transformers import Qwen2VLForConditionalGeneration"这一行代码导致的错误，具体表现为使用了过时的`pynvml`包而不是`nvidiamlpy`，引发了警告和错误信息。

https://github.com/vllm-project/vllm/issues/10649
该issue属于用户提出需求类型，主要涉及huggingface/transformers中Mixtral model class的manual `head_dim`功能。这个问题的原因是用户需要此功能，以支持更灵活的模型操作。

https://github.com/vllm-project/vllm/issues/10648
这是一个Bug报告，涉及的主要对象是Llama 3.2 90b软件。由于PyTorch版本与CUDA不匹配导致的软件崩溃。

https://github.com/vllm-project/vllm/issues/10647
这是一个功能增强的issue，主要涉及的对象是在加载模型权重时提升速度。由于采用了fastsafetensor库直接将权重从存储加载到GPU内存，相比于逐个参数从文件中读取，能够提升加载速度。

https://github.com/vllm-project/vllm/issues/10646
这个issue类型是需求提议，主要涉及支持KV缓存压缩，并提出了支持不同压缩方法以及相关功能变更的建议。

https://github.com/vllm-project/vllm/issues/10645
这个issue属于bug报告，主要涉及的对象是aria model和torch.compile，可能由于aria model存在问题导致一些bug，需要修复和添加torch.compile功能。

https://github.com/vllm-project/vllm/issues/10644
这是一个bug报告，涉及对象是关于 HOST_IP。可能由于某种原因，导致读取 HOST_IP 的问题或者用户寻求解决有关 HOST_IP 的帮助。

https://github.com/vllm-project/vllm/issues/10643
这是一个Bug报告，主要涉及vLLM实例在同一GPU上运行时的GPU内存计算问题。导致第二个实例启动失败，原因是第二个实例在计算中将第一个实例的内存使用量计入其中，导致负面的KV缓存大小和初始化失败。

https://github.com/vllm-project/vllm/issues/10642
这是一个bug报告，涉及的主要对象是对`bnb_4bit_quant_storage`参数的配置和支持。由于目前未对此参数进行检查，导致可能出现配置错误的情况。

https://github.com/vllm-project/vllm/issues/10641
这是一个用户提出需求的类型的issue，涉及的主要对象是要在文档中添加关于Snowflake幻灯片的内容。

https://github.com/vllm-project/vllm/issues/10640
这个issue是关于性能优化和功能改进的，主要涉及的对象是VLM（Vision-Language Model）。由于前端处理进程中运行了多模态映射器/预处理器，执行时间得到了1.7倍的改进。

https://github.com/vllm-project/vllm/issues/10639
这是一个功能需求的issue，涉及的主要对象是vLLM中的embedding模型。由于不同的embedding模型使用了不同的checkpoint格式，导致在加载模型时可能出现架构名称与期望权重不匹配的问题，用户提出希望能够提高灵活性并允许使用不同格式的权重来加载embedding模型。

https://github.com/vllm-project/vllm/issues/10638
这是一个用户提出需求的issue，主要涉及的对象是vllm模型。由于未能显式指定GPU设备导致了模型加载时的资源冲突和冻结问题。

https://github.com/vllm-project/vllm/issues/10637
这是一个bug报告类型的issue，涉及到GPU内存利用率参数设置不生效的问题。

https://github.com/vllm-project/vllm/issues/10636
这个issue类型是文档问题，涉及的主要对象是代码注释或者文档中的拼写错误。由于作者在更新代码注释或文档时错过了一些拼写错误，导致需要修正这些错误。

https://github.com/vllm-project/vllm/issues/10635
这是一个bug报告，主要涉及的对象是vllm项目中的OpenAI兼容服务器。导致了服务器在处理长请求和高负载时无响应的症状，原因是同步执行tokenization造成阻塞asyncio事件循环。

https://github.com/vllm-project/vllm/issues/10634
该issue为特性提议（Feature Proposal），主要涉及异步调度的设计和优化。造成该提议的原因是多线程GIL锁问题导致解码之间的间隔较大，作者提出了通过异步化优化来减少解码之间的延迟。

https://github.com/vllm-project/vllm/issues/10633
这是一个文档（Documentation）类型的issue，主要涉及到vllm quantization功能的一个小错别字修正。由于这个错别字可能会导致用户在使用vllm quantization时产生误解，用户提出了需要修改的建议。

https://github.com/vllm-project/vllm/issues/10632
这是一个Bug报告，主要涉及的对象是vllm（Very Large Language Model）工具。由于vllm尝试从本地加载模型权重而不是从Hugging Face下载，导致了PermissionError，最终导致了崩溃。

https://github.com/vllm-project/vllm/issues/10631
这是关于bug报告的 issue，涉及到用户在使用vllm时遇到的无法正确运行chat completion的问题，可能是由于模型集成或chat模板选择不当导致。

https://github.com/vllm-project/vllm/issues/10630
这是一个bug报告类型的issue，涉及的主要对象是使用新添加的bad_words功能时出现了GPU内存泄漏问题，导致了GPU:0上的内存消耗不断增加最终导致内存不足错误。

https://github.com/vllm-project/vllm/issues/10629
这是一个关于API一致性问题的用户需求提问，主要涉及到vllm库中的`LLM.generate`和`chat.completions.create`方法的响应是否一致的问题。

https://github.com/vllm-project/vllm/issues/10628
这个issue是关于性能提升未达预期结果的问题，属于性能优化建议类型，主要涉及性能加速技术bge-m3的使用。

https://github.com/vllm-project/vllm/issues/10627
这是一个Bug报告，主要涉及到vLLM中使用Qwen2-Audio模型进行音频处理时出现崩溃的问题。原因可能是由于模型使用了过时的输入处理流程而引起的。

https://github.com/vllm-project/vllm/issues/10626
这是一个bug报告，涉及的主要对象是NCCL错误，由于tensor_parallel_size设置大于1会导致NCCL错误。

https://github.com/vllm-project/vllm/issues/10625
这个issue类型是bug报告，主要涉及的对象是docker+lmdeploy部署过程中出现的报错。原因是未设置chat_template_config导致无法匹配聊天模板，从而触发了AssertionError。

https://github.com/vllm-project/vllm/issues/10624
这是一个代码改进类型的issue，主要涉及将函数移动到config.py文件中。根据讨论链接，用户希望将特定函数放到相关的配置文件中，以提高代码的清晰度和可维护性。

https://github.com/vllm-project/vllm/issues/10623
这个issue是关于功能增强的需求，主要对象是LoRA适配器，用户提出了需要支持自适应增加rank的功能。

https://github.com/vllm-project/vllm/issues/10622
该issue类型是需求提出，主要涉及的对象是torch.compile中的模型。这个需求是想要为不支持的模型添加警告信息。

https://github.com/vllm-project/vllm/issues/10621
这个issue类型是bug报告，涉及主要对象是Qwen2 model，由于缺少支持`is_causal` HF config field，导致症状为无法正确处理相关功能。

https://github.com/vllm-project/vllm/issues/10620
这个issue是关于bug报告，涉及到torch.compile模块生成多个子进程的问题。

https://github.com/vllm-project/vllm/issues/10619
这个issue属于bug报告类型，主要涉及torch.compile生成了多个子进程的问题。这可能是由于torch.compile的线程数量在导入torch时被确定，因此设置环境变量`TORCHINDUCTOR_COMPILE_THREADS`在导入torch后不会生效。

https://github.com/vllm-project/vllm/issues/10618
这是一个兼容性问题，涉及到torch.compile的兼容性检查。由于不兼容造成了相关症状，需要进行相应的修复。

https://github.com/vllm-project/vllm/issues/10617
这是一个bug报告，主要涉及到了与torch.compile和lora serving的兼容性问题，由于vLLM对多个lora（punica kernel）的支持非常复杂，导致使用`torch.compile`与lora时会失败。

https://github.com/vllm-project/vllm/issues/10616
这是一个用户提出需求的issue，主要涉及的对象是vllm的Supported Models页面。由于还没有提供encoder-based models的示例以及关于Score API的offline inference功能的说明，用户建议在另一个PR中添加这一功能，并询问是否应将其重命名为Scoring API以与其他在线API的命名形式保持一致。

https://github.com/vllm-project/vllm/issues/10615
这是一个关于使用vllm中的speculative decoding支持pipeline parallelism的问题，用户遇到了修改参数时出现的错误。

https://github.com/vllm-project/vllm/issues/10614
这是一个重构（Refactor）类别的issue，主要涉及到代码中的冗余部分。

https://github.com/vllm-project/vllm/issues/10613
这是一个用户提出需求的issue，主要涉及对象是支持`torch.compile`编译encoder based models。因为之前的pull request（https://github.com/vllmproject/vllm/pull/10558）已经为其提供基础，现在应该比较容易实现支持。

https://github.com/vllm-project/vllm/issues/10612
这是一个bug报告，主要涉及torch.compile与cpu offloading兼容性的问题，由于使用了nn.Parameter及其子类，在quantized模型中会导致错误。

https://github.com/vllm-project/vllm/issues/10611
这个issue属于需求提出类型，主要涉及kv cache在磁盘上的加载和保存操作，用户希望通过将kv cache缓存到磁盘上来提高命中率。

https://github.com/vllm-project/vllm/issues/10610
这个issue属于文档更新类型，主要涉及更新README.md文件中关于Ray Summit talk链接的内容。由于发现了包含所有ray summit vLLM Track视频的youtube链接，要求更新README文件以反映这一新链接。

https://github.com/vllm-project/vllm/issues/10609
这是一个用户提出需求的issue，主要涉及的对象是CPU offloading implementation。原因可能是用户希望在使用`torch.compile`时更加友好。

https://github.com/vllm-project/vllm/issues/10608
这是一个Bug报告，主要涉及GPU数据初始化的问题。由于数据在错误的设备上被创建，导致CUDA图捕获失败。

https://github.com/vllm-project/vllm/issues/10607
这是一个bug报告类型的issue，主要涉及到vLLM模型在应用prompt_logprobs时无法显示选择的token，可能是由于JSON dict不保留顺序导致的。

https://github.com/vllm-project/vllm/issues/10606
这是一个bug报告，涉及的主要对象是Authorization机制。由于root_path设置时，授权被忽略，导致了权限控制失效的问题。

https://github.com/vllm-project/vllm/issues/10605
这是一个用户提出需求的issue，主要涉及vllm.SamplingParams中`use_beam_search`被删除的问题，用户想了解如何从v0.6.3开始控制beam search的使用。

https://github.com/vllm-project/vllm/issues/10604
这是一个bug报告，主要涉及的对象是计数`num_accepted_tokens`的逻辑。这个问题出现的原因是未能正确统计`accepted`张量中被接受的令牌数量，导致产生了错误的结果。

https://github.com/vllm-project/vllm/issues/10603
这是一个文档更新的类型，涉及到项目的代码模型，由于文档过时引发了问题。

https://github.com/vllm-project/vllm/issues/10602
这是一个用户提出需求的issue，主要涉及VLLM后端日志记录模型响应信息的功能。问题出现的原因是VLLM目前只记录请求日志，未记录LLM的响应日志，给程序调试带来不便。

https://github.com/vllm-project/vllm/issues/10601
这是一个反馈类型的issue，涉及主要对象为CI/Build过程的日志可读性，由于引入新的方案 https://github.com/vllmproject/buildkiteci/pull/52，因此需要撤销之前的改动。

https://github.com/vllm-project/vllm/issues/10600
这是一个bug报告，主要涉及GGUF Model输出重复的问题，可能是由于环境设置或代码逻辑错误导致的。

https://github.com/vllm-project/vllm/issues/10599
该issue类型为功能需求提议，主要涉及的对象为模型和工具函数，并提出了添加提取层索引功能的需求。

https://github.com/vllm-project/vllm/issues/10598
这个issue是一个bug报告，主要涉及的对象是模型加载过程中出现的错误。由于PyTorch版本与CUDA版本不匹配，导致在加载模型时出现了'layers.0.mlp.down_proj.weight'的错误。

https://github.com/vllm-project/vllm/issues/10597
这是一个代码优化提议，旨在进一步减少BNB静态变量，称该问题单主要涉及代码性能优化。

https://github.com/vllm-project/vllm/issues/10596
这是一个Bug报告，涉及到内存分配问题，导致程序出现了OutOfMemoryError。

https://github.com/vllm-project/vllm/issues/10595
这是一个bug报告，涉及的主要对象是vllm项目中的example/tensorize_vllm_model tests。由于CC([torch.compile] support all attention backends)引起的bug导致该测试用例失败。

https://github.com/vllm-project/vllm/issues/10594
这是一个建议性issue，主要涉及CI（持续集成）日志可读性问题。

https://github.com/vllm-project/vllm/issues/10593
这是一个bug报告，主要涉及mllama库中的CPU backend，并修复了导入错误和测试问题。

https://github.com/vllm-project/vllm/issues/10592
这是一个bug报告，主要涉及的对象是无法在v100 GPU上使用FlashAttention-2后端。原因是由于性能退化导致无法使用FlashAttention。

https://github.com/vllm-project/vllm/issues/10591
这是一个针对Ministral-8B-Instruct-2410的功能需求。该需求是为了在Interleaving sliding window测试模型。

https://github.com/vllm-project/vllm/issues/10590
这是一个Bug报告，主要涉及bitsandbytes 4bit model以torch.bfloat16为quant_storage时无法加载，可能是由于数据类型不匹配导致的问题。

https://github.com/vllm-project/vllm/issues/10589
这是一个Bug报告，主要涉及的对象是vllm项目中的hermes_tool_parser.py工具，用户遇到的问题是关于工具调用时的输出错误。

https://github.com/vllm-project/vllm/issues/10588
这个issue是一个Bug报告，主要涉及到Qwen2-VL-7B模型在MME基准测试中性能下降的问题。导致这个问题的原因需要进一步调查。

https://github.com/vllm-project/vllm/issues/10587
这是一个改进请求（PR），主要涉及移除CUDA在推测解码中的硬依赖。

https://github.com/vllm-project/vllm/issues/10586
这是一条关于性能调优的issue，主要涉及 Kernel Tuning fused moe for qwen2-57b 在 GTX 4090 上的问题，由于未经调优导致benchmark结果不佳。

https://github.com/vllm-project/vllm/issues/10585
这是一个bug报告类型的issue，主要涉及到vllm项目中的CPU测试。这个issue是由于https://github.com/vllmproject/vllm/pull/10558 导致的问题。

https://github.com/vllm-project/vllm/issues/10584
这是一个bug报告，涉及Gemma2模块支持全上下文长度时出现的问题。

https://github.com/vllm-project/vllm/issues/10583
这是一个Bug报告，主要涉及到VLLM引擎中的request_id字段。这个问题起因于request_id字段不再是唯一的导致的bug。

https://github.com/vllm-project/vllm/issues/10582
这是一个需求提出的issue，主要涉及的对象是vllm项目的metrics支持。由于现有统计信息不齐全，用户提出添加对系统和请求统计日志的支持、将指标导出到Prometheus等需求。

https://github.com/vllm-project/vllm/issues/10581
这是一个bug报告，该问题单涉及的主要对象是代码修复。由于提交https://github.com/vllmproject/vllm/pull/10552引起了部分图测试失败。

https://github.com/vllm-project/vllm/issues/10580
这是一个bug报告，主要涉及到`compile/test_full_graph.py`测试时出现的JSON序列化问题，导致`CompilationConfig`对象无法正确转换为JSON。

https://github.com/vllm-project/vllm/issues/10579
这是一个用户提出需求的issue，主要涉及V1 LoRA的支持，由于V1 LoRA运行较慢，需要进行清理、单元测试、性能优化等操作。

https://github.com/vllm-project/vllm/issues/10578
这是一个关于用户需求的问题，主要涉及使用vllm进行批量推理时如何为每个样本设置不同的SamplingParams，由于OpenAI API中无法找到设置不同top_p和top_k的方式，用户寻求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/10577
这是一个优化代码质量的Issue，主要涉及 vllm 下的 LLMEngine 类。删除了一些不必要的临时变量，并替换为实例变量来提高代码质量。

https://github.com/vllm-project/vllm/issues/10576
这个issue是一个更新请求，涉及vllm项目和outlines库，由于outlines 0.1.x版本存在序列化问题，导致了vllm集成受到影响。

https://github.com/vllm-project/vllm/issues/10575
这个issue是关于bug报告，涉及的主要对象是在WSL上构建Docker容器时出现的段错误。这可能是由于环境配置或软件问题导致的。

https://github.com/vllm-project/vllm/issues/10574
这是一个需求提出的issue，涉及主要对象是Kernels和AMD，用户提出需要添加fused moe triton configs for mixtral。

https://github.com/vllm-project/vllm/issues/10573
这是一个平台相关的重构工作的issue，涉及到OpenVINO的代码修改，原因可能是为了提升代码质量或性能。

https://github.com/vllm-project/vllm/issues/10572
这是一个bug报告，涉及到模型修复白川BNB在线量化问题。原因可能是由于白川BNB量化时出现了错误。

https://github.com/vllm-project/vllm/issues/10570
这是一个技术改进的issue，主要涉及到V1 VLM的多模态语言模型接口重构，要求对指定的多模态语言模型实现进行改进以满足新的接口要求。

https://github.com/vllm-project/vllm/issues/10569
这是一个bug报告，主要涉及到在运行vllm==0.6.3时出现OOM错误的问题。由于一些环境配置问题，导致了CUDA和OpenCV相关错误，最终导致了OOM错误。

https://github.com/vllm-project/vllm/issues/10568
这是一个Bug报告，针对Baichuan BNB在线量化的修复问题。原因是在线量化功能存在Bug导致无法正常运行。

https://github.com/vllm-project/vllm/issues/10567
这是一个bug报告类型的issue，涉及主要对象是vllm中的tool_choice参数。由于tool_choice参数输入不正确导致的500 Internal Server Error。

https://github.com/vllm-project/vllm/issues/10566
这是一个bug报告类型的issue，主要涉及Docker、vllm和fastchat的部署问题。导致报错的原因可能是关键字参数'use_beam_search'的意外使用。

https://github.com/vllm-project/vllm/issues/10565
该issue类型是功能需求提议，涉及主要对象为Intel Gaudi硬件上的LoRA支持。

https://github.com/vllm-project/vllm/issues/10564
这是一个功能需求提出的issue，主要涉及对象是EngineCore。由于GPU空闲时间显著减少，可能导致性能改进或者功能优化。

https://github.com/vllm-project/vllm/issues/10563
该issue类型为代码质量问题，主要涉及到无效的`noqa`注释在代码中的使用。

https://github.com/vllm-project/vllm/issues/10562
该issue为用户提出需求类型，主要涉及如何在运行假设模型时使用张量并行ism所引发的疑问。

https://github.com/vllm-project/vllm/issues/10561
这是一个功能添加的issue，主要涉及Model添加GLM-4系列hf格式模型支持。

https://github.com/vllm-project/vllm/issues/10560
这个issue是一个用户提出需求类型的问题，主要涉及VLLM项目下的多内存支持。由于现有代码中缺少对多内存的支持，用户提出添加多内存支持的需求。

https://github.com/vllm-project/vllm/issues/10559
这个issue是一个bug报告，主要涉及Speculative Decoding without enabling eager mode在处理一定数量的tokens后会返回乱码输出的问题。

https://github.com/vllm-project/vllm/issues/10558
这是一个用户提出需求的issue，主要涉及到torch.compile中attention backends的支持。原因是之前attention ops分别注册，现在希望统一注册，使得不需要一个一个注册不同的attention backends。

https://github.com/vllm-project/vllm/issues/10557
这是一个需求提出的issue，主要涉及到VLLM项目中的结构化输出基准测试添加及相应功能改进。

https://github.com/vllm-project/vllm/issues/10556
这是一个关于使用问题的issue，主要涉及到在Docker中定义VLLM_CPU_OMP_THREADS_BIND时遇到失败的问题。导致该问题可能是由于配置环境导致的。

https://github.com/vllm-project/vllm/issues/10555
这是一个功能需求类型的issue，主要涉及的对象是不同平台的工作人员类。原因是每个平台应该在自己的代码中指定工作人员类，并且默认情况下为`auto`，允许用户为扩展性指定自定义类。

https://github.com/vllm-project/vllm/issues/10554
这个issue类型是用户提出需求，涉及的主要对象是文档页面的更新。

https://github.com/vllm-project/vllm/issues/10553
该issue属于bug报告类型，主要涉及改进错误消息，可能是由于之前的issue未添加详细信息导致。

https://github.com/vllm-project/vllm/issues/10552
这个issue是关于bug报告的，主要涉及的对象是torch.compile LLM的使用。由于9个任务中的第9个任务有一些问题，导致了这个bug报告。

https://github.com/vllm-project/vllm/issues/10551
这是一个bug报告，主要涉及到移除嵌入参数中的token添加问题。由于嵌入模型不会在输入文本之上生成额外的标记，因此移除类似“是否应添加assistant turn的token”等参数是合理的。

https://github.com/vllm-project/vllm/issues/10550
这是一个用户提出需求的issue，主要对象是在metrics.rst文档中添加一个小的示例。

https://github.com/vllm-project/vllm/issues/10549
这个issue属于用户提出需求的类型，主要对象是支持以qwen模型实现bitsandbytes量化。

https://github.com/vllm-project/vllm/issues/10548
这是一个用户需求问题，涉及到需要扩展gemma29b和llama3.18b等模型的上下文长度，可能是因为需要处理更多token导致的。

https://github.com/vllm-project/vllm/issues/10547
这是一个关于性能测试的Issue，涉及主要对象为VLLM项目中的H100机器。由于一台机器设置错误，导致H100测试未能成功运行。

https://github.com/vllm-project/vllm/issues/10546
该issue类型为功能需求提议，主要涉及vLLM动态加载LoRA文件的功能扩展。因为当前实现仅支持本地存储，用户需要能够从远程服务器动态加载LoRA文件，故需实现LoRAResolver以支持此功能。

https://github.com/vllm-project/vllm/issues/10545
这是一个需求变更类型的issue，涉及主要对象是离线推理示例（offline_inference.py）。由于希望保持offline_inference.py示例尽可能简单，因此将之前的更改移至新文件offline_inference_cli.py。

https://github.com/vllm-project/vllm/issues/10544
这是一个功能改进类型的issue，涉及主要对象为vllm中默认最大批次标记数的设定。由于之前设定的512值过于保守，因此提出将其增加到2048以优化大型prefills处理或吞吐量的建议。

https://github.com/vllm-project/vllm/issues/10543
这是一个无法确定类型的issue，主要对象是PR。 由于标题中有"[do-not-merge]"可能是为了提醒团队不要合并该PR，内容"PR to trigger image"似乎是为了触发图片的PR，具体原因需要更多上下文来确定。

https://github.com/vllm-project/vllm/issues/10542
这是一个需求提出类型的issue，主要涉及实现分布式的张量平行RMS Norm功能，可能是为了新的OLMo模型中的Q和K的RMS Norms。

https://github.com/vllm-project/vllm/issues/10541
这是一个Bug报告，主要涉及的对象是Github上的vllm项目中的CPU tensor parallel功能。这个问题出现的原因是`multi_modal_kwargs` 在CPU工作线程中没有正确广播，导致CPU tensor parallel推断出现错误。

https://github.com/vllm-project/vllm/issues/10540
这个issue是关于安装问题的bug报告，用户在尝试安装CUDA版本的vllm时遇到困难。

https://github.com/vllm-project/vllm/issues/10539
这是一个功能需求的issue，主要对象是支持注册模型特定默认采样参数。原因是为了避免手动设置采样参数带来的不便和可能导致的输出不一致。

https://github.com/vllm-project/vllm/issues/10538
这个issue属于bug报告，主要涉及对象是CI/Build for ppc64le，由于测试失败导致构建和测试作业失败，并且在CI代理磁盘空间问题的背景下，暂时禁用了测试。

https://github.com/vllm-project/vllm/issues/10537
这是一个寻求使用说明的问题，涉及主要对象是ROPE scaling和ROPE theta参数。由于用户需要将上下文长度扩展到128k tokens，但不清楚如何正确配置ropescaling和ropetheta参数，导致出现错误。

https://github.com/vllm-project/vllm/issues/10536
这是一个Bug报告，主要涉及的对象是Qwen2-Audio模型，问题是由于调用vLLM时使用Qwen/Qwen2Audio7BInstruct模型输入音频导致出现错误。

https://github.com/vllm-project/vllm/issues/10535
这是一个bug报告，主要涉及的对象是CUDA 11.8和CUDA 12.1 wheel的上传过程。原因是由于脚本上传问题导致随机选择CUDA 11.8或12.1版本的bug。

https://github.com/vllm-project/vllm/issues/10534
这是一个bug报告，用户在尝试加载模型参数时遇到问题，可能是由于配置文件错误导致的。

https://github.com/vllm-project/vllm/issues/10533
这是一个Bug报告，涉及VLLM项目中的GPU内存利用问题，导致kv_cache_size为负值。

https://github.com/vllm-project/vllm/issues/10532
这个issue类型为用户提出需求，请求添加Sageattention backend。

https://github.com/vllm-project/vllm/issues/10531
这是一个Bug报告，涉及VLLM项目中授权被忽略的问题，导致即使设置了root_path，API仍然可在默认路径下无需token访问。

https://github.com/vllm-project/vllm/issues/10530
这个issue类型是代码改进，主要涉及日志问题，由于重复记录了关于遗留多模式输入处理管道弃用的消息，可能导致日志重复。

https://github.com/vllm-project/vllm/issues/10529
这个issue是关于bug报告，涉及到CLI（命令行界面）的功能问题。由于无法在CLI标志后紧跟无空格字符，导致无法正常设置参数选项。

https://github.com/vllm-project/vllm/issues/10528
这是一个bug报告，主要涉及编译配置和CUDA图的启用。这个问题由于之前的更改导致性能问题，导致自定义操作无法正常工作，并默认启用了CUDA图。

https://github.com/vllm-project/vllm/issues/10527
这个issue类型属于优化需求，主要涉及的对象是Qwen2.572B模型的部署和性能优化。由于处理长度大于6000个token的序列导致高TTFT，用户希望优化配置以降低TTFT并提高服务的响应性。

https://github.com/vllm-project/vllm/issues/10526
这个issue类型是用户提出需求，主要涉及OpenAI的API中关于`tool_choice`参数设置的功能需求。原因是用户需要LLM在调用外部工具时具有更多选择性。

https://github.com/vllm-project/vllm/issues/10525
这是一个Bug报告，主要涉及对象是Gemma2，由于重复注册了cuFFT、cuDNN和cuBLAS工厂导致了相应的错误提示。

https://github.com/vllm-project/vllm/issues/10524
该issue类型是bug报告，主要涉及的对象是VLLM项目的tokenizer。导致此问题的原因是在处理token时出现错误，导致报错信息类似"`inputs` tokens + `max_new_tokens` must be <= xxx. Given: xxx `inputs` tokens and xxx `max_new_tokens`"。

https://github.com/vllm-project/vllm/issues/10523
这是一个bug报告，涉及torch.distributed中的DistBackendError错误，用户遇到了使用`ipc=host`和`shmsize=10.24gb`标志时无法访问主机共享内存的问题。

https://github.com/vllm-project/vllm/issues/10522
这个issue类型是用户提出需求，主要涉及的对象是punica ops注册方式。由于当前punica自定义op在eager模式下存在性能开销，用户提出直接注册punica ops以减少开销。

https://github.com/vllm-project/vllm/issues/10521
这是一个bug报告类型的issue，主要涉及vllm中设置--tensor-parallel-size参数后无法运行openai server的问题。由于设定参数值为4导致出现新异常，用户希望能够解决这个问题。

https://github.com/vllm-project/vllm/issues/10520
这是一个bug报告，主要涉及平台未指定时错误消息改进。出现这个问题的原因是在特定位置添加了一行代码导致的。

https://github.com/vllm-project/vllm/issues/10519
这是一个功能需求的issue，涉及主要对象是vLLM的LoRA加载机制。由于当前实现仅支持从本地存储加载LoRA，用户需要能够从远程服务器动态加载LoRA，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/10518
这是一个Bug报告，涉及的主要对象是InternVL2模型。由于`max_dynamic_patch`和`add_special_tokens`存在bug，导致`dynamic_image_size=False`和`max_dynamic_patch=1`功能无法正常工作。

https://github.com/vllm-project/vllm/issues/10517
这是一个关于配置问题的bug报告，主要涉及的对象是KV cache和MAX_SEQUENCE_LENGTH。造成该问题可能是由于配置中MAX_SEQUENCE_LENGTH设置不正确导致的。

https://github.com/vllm-project/vllm/issues/10516
这是一个bug报告，主要涉及模型在多个GPU上分配内存时出现错误，导致每个GPU加载的内存超出预期。

https://github.com/vllm-project/vllm/issues/10515
这是一个功能需求的issue，涉及到在VLLM中手动注入前缀KV Cache，由于需要手动将KVCache压缩为特殊令牌，因此需要一个方法将其馈送到生成函数中。

https://github.com/vllm-project/vllm/issues/10514
该issue类型为用户提出需求，该问题单涉及的主要对象是在VLLM中添加对Aria模型的支持。主要原因是用户想要使用rhymesai/Aria，一个多模态MoE模型，并请求添加对其模型的支持。

https://github.com/vllm-project/vllm/issues/10513
这是一个文档问题类型的issue，涉及的主要对象是llama_tool_parser.py文件。由于文档中存在一个小错误导致的问题。

https://github.com/vllm-project/vllm/issues/10512
这个issue属于bug报告，涉及到在运行Pixtral-Large-Instruct-2411模型时出现错误。这可能是由于环境配置问题或者依赖库冲突导致的。

https://github.com/vllm-project/vllm/issues/10511
这是一个bug报告，涉及主要对象为内存分析和兼容性。由于内存分类和程序过程不清晰，导致需修复以及明确解释内存分类的问题。

https://github.com/vllm-project/vllm/issues/10510
这是一个关于功能需求的issue，主要涉及到Multimodel prefix-caching features的支持问题，用户询问何时会支持这个特性。

https://github.com/vllm-project/vllm/issues/10509
这个issue类型是用户提出需求， 主要对象是使用vllm进行多个任务的推理及获取kvcache分配信息。可能由于当前没有提供该功能或者用户不清楚如何实现，导致用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/10508
这是一个功能需求类型的issue，主要涉及到`Platform`，由于需要在`Platform`中添加`device_type`属性，以满足系统的需求或者提高用户体验。

https://github.com/vllm-project/vllm/issues/10507
这个issue属于改进类型，主要涉及KVCacheManager组件的重构，目的是更容易传递更多元数据进行哈希计算，消除在块内跟踪令牌ID的需要，并因此不再需要在块中跟踪令牌ID来计算哈希。

https://github.com/vllm-project/vllm/issues/10506
这是一个bug报告，涉及的主要对象是项目中的jax和jaxlib版本。由于未更新PR，运行`pip install -r requirementstpu.txt`会导致jax和jaxlib版本不匹配的错误。

https://github.com/vllm-project/vllm/issues/10505
这是一个请求打开V1功能的issue，主要对象是H200构建。

https://github.com/vllm-project/vllm/issues/10504
这是一个bug报告，涉及的主要对象是Metrics model name显示不正确，可能导致了用户无法准确获取不同lora模型的指标数据。

https://github.com/vllm-project/vllm/issues/10503
这是一个功能添加的issue，主要涉及的对象是vLLM模型。由于需要添加新的OLMo November 2024模型并进行一些架构改变，因此需要在vLLM中实现这一模型。

https://github.com/vllm-project/vllm/issues/10502
这个issue类型是代码修改提议，主要涉及到实现disaggregated prefill功能。由于DCO问题，导致需要将相关代码迁移到新的issue中。

https://github.com/vllm-project/vllm/issues/10501
这是一个功能需求类型的issue，主要涉及EmbeddingChatRequest.add_generation_prompt这个参数，默认值应设置为False，原因是在embedding/reward modeling中，模型不需要生成文本响应，而是输出嵌入向量。

https://github.com/vllm-project/vllm/issues/10500
这是一个功能需求提议，涉及ROCm Flash Attention模块，添加了对softcap的支持。

https://github.com/vllm-project/vllm/issues/10499
这是一个Bug报告，主要涉及Dockerfile在ARM64系统上的构建和Nvidia GH200平台的测试。由于缺少对ARM64系统和CUDA的支持，ARM aarch64服务器构建失败。

https://github.com/vllm-project/vllm/issues/10498
这个issue是一个Bug报告，涉及的主要对象是GPU memory profiling。原因是最近的更新改变了`gpu_memory_utilization`参数的含义，导致出现了错误的行为。

https://github.com/vllm-project/vllm/issues/10497
这是一个功能增强类型的issue，主要涉及VLM前缀缓存中对图像进行哈希处理，由于需要支持图像哈希计算，因此提交了相关逻辑来实现该功能。

https://github.com/vllm-project/vllm/issues/10496
这是一个用户提出需求的issue，主要涉及的对象是在vLLM项目中添加Chat模板。由于用户希望贡献Chat模板并且已经准备好相关内容，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/10495
这是一个改进请求，涉及的主要对象是视频获取超时时间，由于测试入口点可能不稳定，因此需要增加默认视频获取超时时间。

https://github.com/vllm-project/vllm/issues/10494
这是一个Bug报告，主要涉及的对象是embedding model的pooling_type参数。由于某次提交改变了pooling_type='ALL'的逻辑，导致多个不同长度的输入提示无法正确推断。

https://github.com/vllm-project/vllm/issues/10493
这个issue属于Bug报告类型，涉及的主要对象是使用vLLM进行音频输入时遇到错误。导致该问题的原因可能是环境配置中的Python版本与库的兼容性问题。

https://github.com/vllm-project/vllm/issues/10492
这是一个bug报告，该问题涉及的主要对象是vLLM环境配置。由于自动设置的环境变量`HOST_IP`可能导致连接超时错误，用户提出了需要根据情况设置`VLLM_HOST_IP`环境变量以解决该问题。

https://github.com/vllm-project/vllm/issues/10491
这个issue类型是功能需求，主要对象是KVPress，由于长序列LLM部署成本高，需要大量内存。

https://github.com/vllm-project/vllm/issues/10490
这是一个功能优化（Feature）的issue，主要涉及到更新或放宽对outlines依赖的支持，目的是提高速度。

https://github.com/vllm-project/vllm/issues/10489
该issue类型为功能需求，主要涉及vllm对于outlines版本的支持。原因是outlines版本升级改善了内部功能并提升了性能，用户寻求支持vllm使用最新版本。

https://github.com/vllm-project/vllm/issues/10488
这是一个bug报告，主要涉及到vllm在设置--tensor-parallel-size参数后无法正常工作。具体原因是缺少了'vllm._version'模块导致了TypeError异常。

https://github.com/vllm-project/vllm/issues/10486
这是一个bug报告，主要涉及对象是代码中的`_get_padded_batch_size`函数。这个问题可能是由于当前的遍历搜索方式效率不高，导致函数性能不佳，需要用查找表替换来优化。

https://github.com/vllm-project/vllm/issues/10485
这是一个bug报告类型的issue，主要涉及的对象是处理placeholder tokens的merged multi-modal processor。由于处理prompt时未正确替换目标与placeholder，导致需要更新代码以正确处理包含placeholder tokens的transformers v4.47 processors的issue。

https://github.com/vllm-project/vllm/issues/10484
这是一个功能需求的issue，主要涉及对于在完成响应头部添加指标报告支持。由于需要增强与高级负载均衡/网关的集成，提出了对负载/成本报告和LoRA管理的加强。

https://github.com/vllm-project/vllm/issues/10483
这是一个Bug报告，涉及到VLLM中 speculative_draft_tensor_parallel_size 参数无法设置为除1以外的值问题。由于该参数无法设置为期望的值，导致了错误的行为。

https://github.com/vllm-project/vllm/issues/10482
这个issue是一个bug报告，针对torch模块下的编译问题，主要涉及limit inductor threads和lazy import quant，导致了具体bug或出现问题需要修复。

https://github.com/vllm-project/vllm/issues/10481
这是一个Bug报告，涉及主要对象是在尝试加载被Sentence Transformers最新版本微调的LlaMA3.21B模型时出现数值错误的问题。

https://github.com/vllm-project/vllm/issues/10480
这是一个bug报告类型的issue，涉及vLLM在VSCode debugger中出现hang的问题，可能是由于torch.compile创建的子进程导致hang。

https://github.com/vllm-project/vllm/issues/10479
这个issue是关于bug报告，涉及到平台配置中xpu检查的恢复。这个问题可能由于之前的修复操作而导致了 xpu 检查过程中的一些问题，需要重新调整以修复这一问题。

https://github.com/vllm-project/vllm/issues/10478
这是一个bug报告，主要涉及到vLLM项目中的CPU模式，可能是由于最新的Dockerfile中缺少了一DNN导致的。

https://github.com/vllm-project/vllm/issues/10477
这是一个bug报告，主要涉及vLLM项目中使用pinned memory时内存未被释放的问题，可能导致OOM错误或者内存泄漏。

https://github.com/vllm-project/vllm/issues/10476
这是一个用户提出需求的issue，主要涉及到与高级LB/gateways的集成，具体需求为改进加载/成本报告和LoRA管理。

https://github.com/vllm-project/vllm/issues/10475
这个issue类型为功能需求提出，主要涉及的对象是为了提升与先进负载均衡/网关集成的性能和成本报告以及LoRA管理。这个需求由于缺乏对高级负载均衡/网关的集成以及成本报告和LoRA管理的功能，希望通过实现orca响应头以proto格式来解决。

https://github.com/vllm-project/vllm/issues/10474
这是一个bug报告，主要涉及vllm模型在多GPU节点上无法运行的问题，由于无法在forked subprocess中重新初始化CUDA而导致的报错。

https://github.com/vllm-project/vllm/issues/10473
这个issue是关于性能优化讨论的，主要涉及到Python/PyTorch中的内存管理问题，由于执行模型的前向传播过程中使用了多余的内存，导致在某些情况下支持的并发性能有显著差异。

https://github.com/vllm-project/vllm/issues/10472
这个issue是一个bug报告，涉及的主要对象是Deepak的two kernels模块。这个问题的出现可能是由于两个内核同时运行导致的错误。

https://github.com/vllm-project/vllm/issues/10471
这是一个bug报告类型的issue，涉及的主要对象是"rope_scaling"字段。由于新旧字段命名不一致，导致在使用yarn rope配置时可能存在冲突和错误。

https://github.com/vllm-project/vllm/issues/10470
这是一个关于Bug的报告，涉及的主要对象是嵌入式模型。由于预填不支持分块导致的内部服务器错误。

https://github.com/vllm-project/vllm/issues/10469
这是一个功能需求，涉及的主要对象是LazyDict，用户提出需要为LazyDict添加__setitem__方法以支持item赋值。

https://github.com/vllm-project/vllm/issues/10468
这个issue是关于功能改进的，主要涉及的对象是xpu代码，用户提出了将特定代码重构为更通用方法的建议。

https://github.com/vllm-project/vllm/issues/10467
这是一个用户提出需求的issue，主要是关于为FlashInfer添加多步骤分段预加载支持。

https://github.com/vllm-project/vllm/issues/10466
这是一个关于pip-compile出现冲突问题的bug报告，主要涉及到vllm项目中使用pip-compile命令失败的情况，可能是由于decord库的引入导致的。

https://github.com/vllm-project/vllm/issues/10465
这个issue是关于修改CI构建配置的需求，主要涉及到将nightly测试改为optional，删除nightly关键字。

https://github.com/vllm-project/vllm/issues/10464
这是一个bug报告类型的issue，涉及主要对象为Marlin 2.4，导致的问题是返回错误结果的bug。

https://github.com/vllm-project/vllm/issues/10463
这是一个Bug报告类型的Issue，主要涉及vLLM的OpenAI兼容服务器不接受请求载荷中的额外字段，导致BadRequestError: Error code: 400的错误。

https://github.com/vllm-project/vllm/issues/10462
这个issue类型是需求反馈，主要涉及添加滑动窗口支持至 flashinfer的问题，并需要测试来验证。

https://github.com/vllm-project/vllm/issues/10461
这是一则用户提出需求的issue，主要涉及Llama32在ROCM FA支持上游的问题。

https://github.com/vllm-project/vllm/issues/10460
这是一个关于优化编译时间的问题，主要涉及到torch.compile功能。原因可能是为了减少编译时间而进行优化。

https://github.com/vllm-project/vllm/issues/10459
这个issue属于bug报告类型，主要涉及VLLM在ARM机器上安装时遇到的问题，可能是由于VLLM尝试使用CPU后端而导致的错误。

https://github.com/vllm-project/vllm/issues/10458
这是一个bug报告，主要涉及日志配置文件无效导致的问题。

https://github.com/vllm-project/vllm/issues/10457
这是一个bug报告，涉及的主要对象是vllm中的formatter 'vllm'配置，导致使用最新版本时出现数值错误。

https://github.com/vllm-project/vllm/issues/10456
这是一个bug报告类型的issue，涉及Mamba模型初始化和MLP Speculator权重加载的修复。由于破坏了测试，导致了问题的产生。

https://github.com/vllm-project/vllm/issues/10455
这个issue是文档相关的问题，涉及到页面链接更名，导致了链接错误。

https://github.com/vllm-project/vllm/issues/10454
这是一个优化需求，主要涉及Medusa模型中权重大小的优化。该需求是由于在实际部署中，只有ResidualBlock通常被训练，而不是lm_head所致。

https://github.com/vllm-project/vllm/issues/10453
这个issue类型是bug报告，主要涉及到torch.distributed.DistBackendError和NCCL错误，由于NCCL版本问题导致出现错误信息。

https://github.com/vllm-project/vllm/issues/10452
这是一个bug报告，涉及到修改granite chat模板以保留工具调用的json列表格式。原因是模型读取工具调用输入的方式与其生成方式不一致。

https://github.com/vllm-project/vllm/issues/10451
这是一个Bug报告，主要涉及的对象是vLLM中`gpu_memory_utilization`参数的行为变化。这个问题由于参数行为的改变导致在部署多个模型共享GPU时出现了内存管理方面的挑战。

https://github.com/vllm-project/vllm/issues/10450
这是一个bug报告类型的issue，涉及到vllm项目中的LoRA weight sharding在ColumnParallelLinearWithLoRA中的修复。原因可能是缺少注释和测试，导致了对这部分功能的理解和验证不足。

https://github.com/vllm-project/vllm/issues/10449
这个issue属于bug报告类型，主要涉及PixtralLarge模型的vision-lang适配器。由于初始化bias weights太小导致在推理过程中几乎不产生影响，提出修改方案去除bias以改进。

https://github.com/vllm-project/vllm/issues/10448
这是一个用户提出需求的issue，主要涉及Marlin kernel在vllm中为什么不需要evict_first hint的问题，原因可能是优化处理或者特定的实现方式。

https://github.com/vllm-project/vllm/issues/10447
这是一个bug报告，主要涉及vLLM生成的trace Id重复导致的问题。

https://github.com/vllm-project/vllm/issues/10446
这个issue是用户提出需求，请求支持LoRA fine-tuning model for DeepSeek V2，由于需要加速LLM inference。

https://github.com/vllm-project/vllm/issues/10445
这是一个bug报告，主要涉及vLLM在运行过程中出现的CUDA错误导致的问题。

https://github.com/vllm-project/vllm/issues/10444
这是一个Bug报告，主要涉及VLLM项目中请求奖励模型报告时出现500服务器内部错误的问题。导致这个问题的原因可能是参数传递错误或代码逻辑错误。

https://github.com/vllm-project/vllm/issues/10443
这是一个bug报告，主要涉及插件加载相关的问题。由于插件可以被多次加载，尤其是在启动时会重复加载，导致用户感到困扰。

https://github.com/vllm-project/vllm/issues/10442
这是一个bug报告，涉及到vllm中 Speculative decoding + guided decoding 的功能无法正常工作，导致输出被截断到5个tokens。

https://github.com/vllm-project/vllm/issues/10441
这个issue是一个功能需求提议，主要涉及的对象是CI（持续集成）以及CPU。由于需要在容器名后添加NUMA节点编号，可能是为了更好地管理和区分容器资源，提高系统性能。

https://github.com/vllm-project/vllm/issues/10440
这是一个Bug报告，主要涉及VLLM支持模型中输入提示过长导致的错误。

https://github.com/vllm-project/vllm/issues/10439
这是一个Bug报告，涉及的主要对象是无法在v0.6.4.post1版本中运行Qwen2.5-0.5B-Instruct模型，出现“No available memory for the cache blocks”错误。

https://github.com/vllm-project/vllm/issues/10438
这是一个bug报告，主要涉及到QWenLMHeadModel和ChatGLMForCausalLM这两个模型类。在启用LLM的LoRA时，会输出错误的警告信息，可能是由于代码中的小错误导致。

https://github.com/vllm-project/vllm/issues/10437
这个issue是关于用户提出需求的，主要涉及的对象是vllm.LLM模块，由于需要用户界面改进，用户现在需要通过命令行或直接构建CompilationConfig对象来进行编译配置，但目前只能控制3个级别，稍后会提供更精细的控制，并计划后续提供如何使用以及设计文档。

https://github.com/vllm-project/vllm/issues/10436
这个issue类型属于需求提出，主要对象是dependabot。由于过多的依赖项导致所有补丁更新造成干扰，用户希望让dependabot忽略所有的补丁版本更新。

https://github.com/vllm-project/vllm/issues/10435
这个issue类型是用户提出需求，主要对象是在TPU上支持`w8a8`，由于之前的pull request被取代所导致。

https://github.com/vllm-project/vllm/issues/10434
这个issue属于CI/Build问题，涉及Dockerfile.rocm文件的更新，问题源于pytorch版本标签被删除导致'docker build'功能无法使用。

https://github.com/vllm-project/vllm/issues/10433
这是一个用户提出需求的issue，涉及对象是添加"openai.beta.chat.completions.parse"的示例到文档中。由于缺少该示例，用户可能希望文档能提供更全面的结构化输出信息。

https://github.com/vllm-project/vllm/issues/10432
这是一个bug报告，涉及vllm server在特定条件下崩溃的问题，原因可能是max_token设置为0导致的。

https://github.com/vllm-project/vllm/issues/10431
这是一个bug报告，主要涉及的对象是kernel tests，由于https://github.com/vllmproject/vllm/pull/10383 导致了测试出现问题。

https://github.com/vllm-project/vllm/issues/10430
这是一个bug报告，主要涉及vLLM中负计数指标导致在线模型崩溃的问题，具体原因尚需进一步调查。

https://github.com/vllm-project/vllm/issues/10429
这是一个Bug报告，涉及vllm的ROCM环境下的错误。造成该问题可能是由于运行命令时出现错误而导致的。

https://github.com/vllm-project/vllm/issues/10428
这是一个bug报告，主要涉及文档页面的重定向问题。导致这个问题的原因是页面被移动导致旧的URL返回404错误。

https://github.com/vllm-project/vllm/issues/10427
这个issue类型是需求提出，主要涉及文档格式和易用性，由于ReStructuredText相对不易上手，用户提出将文档迁移至Markdown，以降低门槛并提高贡献参与度。

https://github.com/vllm-project/vllm/issues/10426
这个issue是一个bug报告，涉及的主要对象是FUNDING.yml文件的格式要求。这个问题是由于GitHub只支持在funding.yml文件中的`github`和`custom`值是数组形式，未遵守这个格式规范导致的。

https://github.com/vllm-project/vllm/issues/10425
该issue类型是文档更新请求，主要涉及的对象是GLM-4V模块的LoRA支持文档。这个问题由于新增了LoRA支持，需要更新相关文档内容。

https://github.com/vllm-project/vllm/issues/10424
这个issue类型是用户提出需求类型，涉及的主要对象是支持用本地 cutlass 路径编译，原因可能是需要改进项目的构建流程。

https://github.com/vllm-project/vllm/issues/10423
这个issue类型是提出需求，涉及的主要对象是为vllm添加对通过环境变量指定本地 CUTLASS 源目录的支持。用户提出了在一些网络条件差的环境下，或者用户希望使用自定义版本的 CUTLASS 代码编译 vllm 时存在不便的问题。

https://github.com/vllm-project/vllm/issues/10422
这是一个关于优化代码质量的Issue，主要涉及到Medusa模型的权重优化问题。原因可能是为了减少显存占用并提高模型性能。

https://github.com/vllm-project/vllm/issues/10421
这个issue属于bug报告类型，主要涉及vLLM库在不同架构上构建失败的问题，其中涉及mf16c标志和FP16矢量类型在Power架构上引起的构建错误。

https://github.com/vllm-project/vllm/issues/10420
该issue属于需求类型，主要涉及 vLLM 和 LMDeploy 的 Triton kernels 比较，用户想了解哪种 Triton kernels 更有效，并尝试使用 vLLM 来提高性能。

https://github.com/vllm-project/vllm/issues/10419
这是一个bug报告，主要涉及的对象是在使用两路管道并行性时出现了NCCL错误。由于NCCL相关错误导致无法成功为LLM模型提供服务。

https://github.com/vllm-project/vllm/issues/10418
这是一个bug报告，涉及的主要对象是LoRA模型支持。由于结果在vllm和transformers之间略有差异，用户可能遇到了由于模型分离引起的结果差异问题。

https://github.com/vllm-project/vllm/issues/10417
这个issue是一个bug报告，涉及主要对象是Phi3模型的BNB在线量化问题，由于之前MLP的权重合并在磁盘中，导致当前BNB实现无法对此情况进行处理，引发了Bug。

https://github.com/vllm-project/vllm/issues/10416
这是一个bug报告，涉及Llama-3.2-11B-Vision-Instruct的在线推理部署问题。由于PyTorch版本与CUDA版本不匹配，可能导致部署问题。

https://github.com/vllm-project/vllm/issues/10415
这是一个bug报告，主要涉及的对象是vllm项目中的Pooler模块。造成这个问题的原因是在使用PoolingType.STEP时，没有必要包含冗余的softmax操作。

https://github.com/vllm-project/vllm/issues/10414
这是一个修改模型的issue，涉及的主要对象是ViT模型的注意力机制。原因是为了简化ViT注意力机制的实现并方便后续维护。

https://github.com/vllm-project/vllm/issues/10413
这是一个关于软件更新与自动化操作的issue，主要对象是软件的依赖库更新。由于依赖库更新可能导致冲突或自动化操作问题，用户在此提出了相关操作指南并寻求针对此类问题的帮助。

https://github.com/vllm-project/vllm/issues/10412
这个issue属于用户提出需求类型，主要涉及为希望使用ROCm GPU的AMD用户添加ROCm设备插件链接到“使用Kubernetes部署”页面。

https://github.com/vllm-project/vllm/issues/10411
这是一个Bug报告，涉及的主要对象是KV Cache Quantization with GGUF，由于开启该功能导致模型性能出现异常表现。

https://github.com/vllm-project/vllm/issues/10410
这是一个bug报告类型的issue，主要涉及的对象是VLLM中的Ray库。原因可能是需要更新Ray库以在已编译的图测试中使用_overlap_gpu_communication。

https://github.com/vllm-project/vllm/issues/10409
这是一个Bug报告类型的issue，主要涉及VLLM程序在增加--tensor-parallel-size参数后导致程序崩溃的问题。

https://github.com/vllm-project/vllm/issues/10408
这是一个bug报告，涉及的主要对象是使用vllm和transformer部署Qwen2VL的服务。用户反映微调后权重一致时，同一张图片在transformer和vllm上输出结果不一致，vllm存在叠字和信息提取不到的问题。

https://github.com/vllm-project/vllm/issues/10407
这是一个功能需求类型的issue，主要涉及VLM模型输出中的多模态占位符报告，目的是帮助检查VLM模型的性能，以区分文本标记和多模态嵌入。

https://github.com/vllm-project/vllm/issues/10406
这是一个bug报告，主要对象是torch.jit.script到torch.compile，由于修改修复了一个问题并带来了性能收益。

https://github.com/vllm-project/vllm/issues/10405
这是一个bug报告，主要涉及的对象是 PixtralHF ViT 模型。这个问题是由于在运行 PixtralHF 模型时使用 TP 会导致输入形状错误，尤其在通过 ViT 注意力 profiling 时会出现此问题。

https://github.com/vllm-project/vllm/issues/10404
这是一个需求报告issue，主要涉及fishaudio/fish-speech-1.4模型。由于该模型难以支持现有的VLLM层，用户在寻求支持该模型所遇到的困难。

https://github.com/vllm-project/vllm/issues/10403
这是一个Bug修复类型的Issue，主要涉及vllm中的`mrope_position_delta`常量，在部分完成预填充时被修改导致的bug。

https://github.com/vllm-project/vllm/issues/10402
这是一个关于代码重构的类型。主要涉及的对象是平台（platforms）。这个问题的原因可能是为了优化代码结构或提升性能。

https://github.com/vllm-project/vllm/issues/10401
这个issue类型是功能改进，主要涉及的对象是torch.compile中的编译配置。由于之前全局变量过多，现在通过调整编译配置，可以更好地存储信息，提升代码整洁性和可维护性。

https://github.com/vllm-project/vllm/issues/10400
该issue是关于功能增强的请求，涉及支持Cross Encoder模型的问题。用户提出了增加对RoBERTA和BERT SequenceClassification模型的支持，以及相应的API端点和方法。

https://github.com/vllm-project/vllm/issues/10399
这个issue类型是改进需求，主要涉及到对自定义操作日志的控制和统计。

https://github.com/vllm-project/vllm/issues/10398
这是一个Bug报告，主要涉及的对象是hermes工具的解析器输出错误流参数，在某些情况下产生问题。导致这个Bug的原因是在增量计算参数过程中，partial_json_parser可能会引发错误，因此使用delta_text直接返回给用户以避免这些错误。

https://github.com/vllm-project/vllm/issues/10397
这是一个请求类型的 issue， 主要对象是`vllm/v1`，由于需要明确代码负责人，所以添加了代码负责人。

https://github.com/vllm-project/vllm/issues/10396
这是一个BugFix类型的issue，主要涉及Hermes工具解析器输出的错误流参数在某些情况下的问题，可能由于参数解析错误导致输出异常。

https://github.com/vllm-project/vllm/issues/10395
这个issue是一个bug报告，涉及的主要对象是Hermes工具解析器中的`extract_tool_calls_streaming`组件，由于特定输出内容导致解析流式工具函数输出时生成错误的参数。

https://github.com/vllm-project/vllm/issues/10394
这是一个bug报告，涉及CPU embedding runner的修复以支持tensor parallel推理，可能是由于在使用tensor parallel时CPU embedding runner存在一些不兼容或未处理的情况导致的问题。

https://github.com/vllm-project/vllm/issues/10392
这个issue类型是用户提出需求，该问题单涉及的主要对象是增强offline_inference.py的可配置参数以增加灵活性。由于当前功能较为受限，用户希望通过这个提议来增加程序的配置参数，提高灵活性。

https://github.com/vllm-project/vllm/issues/10391
这是一个用户提出需求的类型的issue，主要涉及的对象是要增强 offline_inference.py 的功能性。原因是当前的功能相对基础，开发者经常需要手动修改脚本以满足特定需求，提出了增加可配置参数以提升脚本的灵活性和适用性。

https://github.com/vllm-project/vllm/issues/10390
这个issue属于功能需求类型，主要涉及的对象是vllm下的API接口。原因是用户提出了关于添加ngram猜测功能到API的需求。

https://github.com/vllm-project/vllm/issues/10389
这是一个bug报告类型的issue，主要涉及vllm模型在执行过程中出现CUDA错误导致崩溃的情况。

https://github.com/vllm-project/vllm/issues/10388
这个issue是一个Bugfix类型的报告，涉及到MRotaryEmbedding的get_input_positions方法，在chunked prefill启用时未正确处理右侧切片，导致了相关错误。

https://github.com/vllm-project/vllm/issues/10387
这是一个用户提出需求的问题，该问题涉及到对于异步输出处理支持的道路，并表达了对于支持异步输出处理的急需。


https://github.com/vllm-project/vllm/issues/10386
这是一个用户提出需求的类型为改进日志消息的issue，主要涉及的对象是插件(plugin)。这个问题的原因是日志信息不清晰，容易让用户感到困惑。

https://github.com/vllm-project/vllm/issues/10385
这是一个BugFix类型的issue，主要涉及Kernel中的GPU SEGV问题，由于可能是由整数溢出导致，需要在`fused_moe.py:fused_moe_kernel`中将offsets改为使用`tl.int64`来修复。

https://github.com/vllm-project/vllm/issues/10384
这是一个Bug报告，涉及主要对象为"IDC hpu"，原因是设备未找到导致的问题。

https://github.com/vllm-project/vllm/issues/10383
这是一个需求提出类的issue，主要涉及将编译配置整合到vllm的配置文件中，由于目前core代码中仍有读取环境变量的情况，需要将环境变量转移到命令行参数中，并在初始化阶段初始化所有配置字段，从而解决这个问题。

https://github.com/vllm-project/vllm/issues/10382
这个issue属于用户提出需求类型，主要涉及的对象是vllm的引擎配置（EngineArgs），由于需要实现方便地在v0和v1版本之间切换的功能，以及实现更容易的迁移，所以提出了相关改动。

https://github.com/vllm-project/vllm/issues/10381
这是一个Bug报告，涉及的主要对象是vllm的GPU SEGV bug，由于offsets使用`tl.int64`导致了GPU SEGV症状。

https://github.com/vllm-project/vllm/issues/10380
这是一个bug报告，主要涉及对象是GPU运行大型模型时遇到整数溢出导致崩溃的问题。

https://github.com/vllm-project/vllm/issues/10379
这是一个Bug报告，主要涉及Granite 3.0的解析器和示例模板之间的断开连接。原因是模板循环遍历工具调用列表，而解析器明确使用`[`符号来确定是否应解析响应，导致需要不同的输入格式和输出格式，使得使用该格式微调模型非常困难。

https://github.com/vllm-project/vllm/issues/10378
这是一个bug报告类型的issue，涉及到测试k8s agent功能。这个问题可能是由于k8s agent功能无法正常工作而导致的。

https://github.com/vllm-project/vllm/issues/10377
该issue类型为提出需求，并主要涉及NVIDIA Triton GenAI Perf Benchmark的集成与性能测试。该需求是为了比较vLLM的性能和准确性，需要实现新的基准测试工具进行负载测试和比较结果指标。

https://github.com/vllm-project/vllm/issues/10376
这是一个bug报告，反映了在streaming模式下引导解码出现问题，导致了测试脚本的执行失败。

https://github.com/vllm-project/vllm/issues/10375
这是一个bug报告，主要涉及对象是ray.init函数调用，问题源于vLLM被另一个程序调用时未正确处理已经初始化的ray实例，导致重复初始化错误。

https://github.com/vllm-project/vllm/issues/10374
这是一个特征改进的 issue，主要涉及到 vLLM 中的模型执行接口的重构。由于要支持 V1 VLM 重构和 `torch.compile`，需要满足一定的模型实现要求。

https://github.com/vllm-project/vllm/issues/10373
这是一个bug报告，主要涉及VLM的benchmark_serving请求，可能由于CUDA版本不匹配导致请求无法工作。

https://github.com/vllm-project/vllm/issues/10372
这是一个文档补充类的issue，涉及的主要对象是插件系统。原因可能是缺少插件系统文档，用户希望补充相关内容。

https://github.com/vllm-project/vllm/issues/10371
这个issue属于感谢类型，主要涉及的对象是Nebius作为赞助商。

https://github.com/vllm-project/vllm/issues/10370
这是一个bug报告类型的issue，涉及到v0.6.3.post2版本。原因可能是版本存在bug导致问题。

https://github.com/vllm-project/vllm/issues/10369
这是一个用户提出需求的 issue，主要涉及前端添加 `--version` flag 到 CLI 工具中。

https://github.com/vllm-project/vllm/issues/10368
这个issue类型是文档增加需求，主要涉及vLLM的架构概述页面，作者打算逐层介绍主要组件，并因此制作了带有图示的文档。

https://github.com/vllm-project/vllm/issues/10367
这是一个需求提出类型的issue，主要涉及到mistral_common版本的更新。原因是为了更好地工具使用。

https://github.com/vllm-project/vllm/issues/10366
该issue类型是一个功能需求提议，涉及主要对象是CI/Build。由于缺乏文档检查工具，用户提议添加sphinxlint来清理文档中的风格和格式错误，以提高文档质量。

https://github.com/vllm-project/vllm/issues/10365
这是一个Bug报告，主要涉及Torch profiling在使用多个worker时无法停止且无法获取所有worker的跟踪。由于使用`distributedexecutorbackend ray`时和多个worker共同使用时出现了这一问题。

https://github.com/vllm-project/vllm/issues/10364
这是一个bug报告，涉及主要对象为测试套件中的 test_fused_moe() 函数。增加绝对公差是由于在大张量上运行时，某些随机生成的输入数据超出了当前的容忍度。

https://github.com/vllm-project/vllm/issues/10363
该issue是关于bug修复的，主要涉及vllm的MistralTokenizer，问题是特殊标记未能正确过滤，导致结构化输出不准确。

https://github.com/vllm-project/vllm/issues/10362
这个issue属于bug报告类型，主要涉及的对象是`arg_utils`工具的帮助信息格式问题。由于文本换行时部分单词连接没有空格，导致了阅读不流畅的问题。

https://github.com/vllm-project/vllm/issues/10361
这个issue类型属于功能需求提出，主要对象是Medusa服务。这个问题是由于在生产环境中，一些服务提供的权重具有偏差，而一些权重则没有偏差，因此需要支持自定义偏差的灵活配置。

https://github.com/vllm-project/vllm/issues/10360
这个issue类型是bug报告，涉及的主要对象是vllm的docker image版本0.6.4。由于docker run 0.6.4需要比0.6.3更多的GPU内存才能触发OOM，导致了该bug的症状。

https://github.com/vllm-project/vllm/issues/10359
这个issue是一个Bug报告，主要涉及的对象是代码中的生成器功能，可能由于某种原因导致输出中断。

https://github.com/vllm-project/vllm/issues/10358
这是一个代码重构的issue，主要涉及到将名为`get_default_attn_backend`的函数提取到`Platform`中，并在不同后端上实现它。

https://github.com/vllm-project/vllm/issues/10357
这是一个Bug报告，涉及Qwen2 VL模型在Huggingface代码和vLLM代码中GPU内存使用量不同的问题。可能是由于代码实现或模型加载方式等原因导致了这种不同的GPU内存使用情况。

https://github.com/vllm-project/vllm/issues/10356
这是一个用户提出需求的 issue， 主要涉及的对象为旧版本的类。造成这个问题的原因可能是新版本的更改会对用户造成困扰，建议保留兼容性并先发出警告。

https://github.com/vllm-project/vllm/issues/10355
这是一个关于Github上vLLM的issue，类型是硬件支持相关，涉及的主要对象是CPU，用户提出需要CPU chunkedprefill和prefixcaching支持，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/10354
这是一个用户提出需求的issue，主要涉及的对象是为VLLM添加KV-Cache int8量化支持。原因可能是为了支持int8量化，并且提供了关于如何实现和使用KVCache int8的详细说明。

https://github.com/vllm-project/vllm/issues/10353
这个issue属于用户提出的需求。主要对象是VllmRunner中的模型访问接口。由于用户需要在每个工作节点中应用函数到模型，但又不想直接访问底层的模型实例，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/10352
这是一个Bugfix类型的issue，主要对象涉及到了LoRA相关的代码。由于代码未能正确处理全部三个`lora_a`都为None的情况，导致出现了无法运行LLama3 LoRA的问题。

https://github.com/vllm-project/vllm/issues/10351
这是一个非bug报告类别的issue，主要涉及到VLLM项目中pooler配置参数的整合和相关代码清理。

https://github.com/vllm-project/vllm/issues/10350
这是一个bug报告，主要涉及的对象是Qwenvl的输出。由于error position_id错误以及新seq_data创建过程中未传递mrope_position_delta信息，导致了在speculative decoding中输出不一致的问题。

https://github.com/vllm-project/vllm/issues/10349
这是一个bug报告类型的issue，主要涉及tensorizer测试模块的import错误，导致测试失败。

https://github.com/vllm-project/vllm/issues/10348
这是一个Bug报告，涉及的主要对象是vllm项目中的--lora-dtype选项。由于float32数据类型实际上不被支持，导致在运行特定命令时出现了AssertionError错误。

https://github.com/vllm-project/vllm/issues/10347
这是一个bug报告，主要涉及Falcon框架，由于`trust_remote_code=True`导致Falcon崩溃。

https://github.com/vllm-project/vllm/issues/10346
该issue类型为草稿（Draft），主要对象是Cutlass 2:4。这个问题可能是用户正在准备提交一个与Cutlass 2:4相关的内容，但尚未完成。

https://github.com/vllm-project/vllm/issues/10345
这个issue属于bug报告类型，涉及的主要对象是vllm库。由于cuda oom导致服务器在同时进行任务生成和嵌入时出现问题，用户希望在同一台服务器上使用GPU进行任务生成，而使用CPU进行嵌入操作。

https://github.com/vllm-project/vllm/issues/10344
这是一个bug报告，涉及Snowflake Arctic在使用TP-8时出现内存错误问题。造成这个问题的原因可能是程序内存管理不当或者数据量过大导致。

https://github.com/vllm-project/vllm/issues/10343
这是一个用户提出需求的issue，主要涉及VLLM在TPU上使用Pallas后端时的head_size限制问题。

https://github.com/vllm-project/vllm/issues/10342
这是一个Bug报告，涉及主要对象是LLama3 LoRA服务。由于输入参数设置错误，导致出现了错误1114以及相关的traceback信息。

https://github.com/vllm-project/vllm/issues/10341
这是一个未提供具体内容的issue，无法判断具体类型和问题对象。

https://github.com/vllm-project/vllm/issues/10340
这个issue类型是功能增强，主要涉及添加了Cutlass稀疏支持和相应的操作，包括实现2:4结构稀疏支持、权重矩阵转换以及模型加载管道修改等。

https://github.com/vllm-project/vllm/issues/10339
这是一个性能优化类的 issue，主要涉及 llama 在峰值内存使用上的问题，由于维护多个名称会导致引用计数的增加，从而增加了峰值内存使用，表现为内存分析中更多叠加在一起的块。

https://github.com/vllm-project/vllm/issues/10337
这是一个bug报告，主要涉及VLLM模型的KV缓存错误问题，原因是在特定参数下引发模型上下文长度丢失的错误。

https://github.com/vllm-project/vllm/issues/10336
这是一个bug报告，涉及VLLM下的单序列推断和并发请求导致不同的输出问题，可能与模型的pad tokens有关。

https://github.com/vllm-project/vllm/issues/10335
这个issue类型是功能增强请求，主要涉及的对象是为VLLM添加了Cutlass稀疏支持。

https://github.com/vllm-project/vllm/issues/10334
这是一个bug报告，主要涉及静态对称量化情况下的修正。由于静态量化情况下，需将零点值（AZP）提前折叠到`azp_adj`中，以修复静态量化中返回零点值的问题。

https://github.com/vllm-project/vllm/issues/10333
这是一个功能改进的issue，涉及主要对象是Mistral工具解析，由于需要更加稳健的函数调用解析，作者提出了改善和修正解析逻辑。

https://github.com/vllm-project/vllm/issues/10332
这是一个用户提出需求的issue，主要涉及的对象是vllm中的代码。

https://github.com/vllm-project/vllm/issues/10331
这是一个用户提出需求的类型，该问题单涉及向发布 meetup 演示文稿。

https://github.com/vllm-project/vllm/issues/10330
这是一个功能需求的issue，主要涉及VLLM引擎中的keyvalue缓存，并提出了在特定条件下将缓存迁移到主机内存的优化方案。

https://github.com/vllm-project/vllm/issues/10329
这个issue类型为草稿，主要对象是Rs 24 sparse。由于尚未完善，用户可能在该草稿中发现错误或需要进一步帮助。

https://github.com/vllm-project/vllm/issues/10327
这是一个bug报告，主要涉及的对象是`AutoWeightsLoader`。由于模型存在未初始化的参数，导致一些由于错误的权重加载或模型初始化而引起的问题很难被发现。

https://github.com/vllm-project/vllm/issues/10325
这是一个bug报告，涉及到使用lm_eval进行MMLU评估时出现的OOM错误。原因是在lm_eval中使用了不同的采样参数，导致内存消耗显著增加。

https://github.com/vllm-project/vllm/issues/10324
这个issue是关于一个Bug报告，涉及到自定义聊天模板发送到模型时出现了问题。原因可能是模板格式或参数设置有误导致输出异常。

https://github.com/vllm-project/vllm/issues/10323
这个issue类型是用户提出需求，主要涉及对模型功能的修改。用户想直接传入embedding以适应TTS任务，而不是通过token_id。

https://github.com/vllm-project/vllm/issues/10322
该issue是关于用户需求的，主要涉及使用vLLM作为后端来运行推理时出现问题。可能是由于配置问题或API集成错误导致该问题。

https://github.com/vllm-project/vllm/issues/10321
这个issue类型为任务分配，涉及主要对象为代码文件model_runner.py，由于缺少callback来覆盖kvcache，导致需要在migration过程中做相应修改。

https://github.com/vllm-project/vllm/issues/10320
这是一个bug报告，涉及vllm的spec model init。由于未提供具体内容，无法确认具体问题。

https://github.com/vllm-project/vllm/issues/10319
这是一个用户提出需求的issue，主要涉及vllm==0.6.2版本在cuda 11.8环境下的支持问题，用户希望能够得到关于添加`vllm==0.6.2+cu118`支持的帮助。

https://github.com/vllm-project/vllm/issues/10318
这个issue类型是性能问题报告，主要涉及vLLM模型在实验中性能无法复现的问题。原因可能是实验设置不明确导致无法达到预期性能水平。

https://github.com/vllm-project/vllm/issues/10317
这是一个bug报告，涉及的主要对象是CI测试环境，由于测试用例可能出现hang导致了CI测试无法结束。

https://github.com/vllm-project/vllm/issues/10316
这个issue属于用户提出需求类型，主要涉及OpenAI Chat Completion Client For Multimodal中使用视频作为输入的问题。由于文档只展示了使用多个图像，用户提出了如何使用视频作为输入的问题。

https://github.com/vllm-project/vllm/issues/10315
这是一个需求提出的issue，主要涉及添加Cambricon MLU推理后端到vLLM项目中。

https://github.com/vllm-project/vllm/issues/10314
这是一个Bug报告，涉及对象是CI/Build系统，由于CPU测试CI无法启动服务器进行在线推理。

https://github.com/vllm-project/vllm/issues/10313
这是一个Bug报告，主要涉及FusedMoE kernel性能与输入提示长度相关性的问题。由于FusedMoE kernel在解码过程中与输入提示长度有关，导致了Mixtral模型解码速度明显下降。

https://github.com/vllm-project/vllm/issues/10312
这是一个bug修复类型的issue，涉及的主要对象是无法加载某些模型，可能由于最近的代码更改导致部分模型无法正确加载。

https://github.com/vllm-project/vllm/issues/10311
这是一个与新增特性相关的问题单，主要涉及了vllm模型的支持telechat2功能。原因是之前关联的模型添加了telechat52b特性，但是telechat2功能尚未支持。

https://github.com/vllm-project/vllm/issues/10310
这个issue类型属于功能需求，主要涉及的对象是模型 BNB quantization support。由于现有的Idefics3模型尚未支持 BNB quantization，用户提出需要添加支持的功能需求。

https://github.com/vllm-project/vllm/issues/10309
这是一个用户提出需求的issue，主要涉及如何使用`vllm`输出代码而不包括额外解释，用户询问是否`vllm`支持此功能并寻求帮助。

https://github.com/vllm-project/vllm/issues/10308
这是一个bug报告，主要涉及到VLLM中的日志输出，由于日志消息过于频繁导致输出信息太多，需要将日志级别从info降级为debug。

https://github.com/vllm-project/vllm/issues/10307
这个issue属于功能需求提出，主要对象是TPU后端，用户提出需要实现prefix caching支持。

https://github.com/vllm-project/vllm/issues/10305
这是一个优化建议，建议简化脚本代码，主要对象是format.sh。由于第3个参数可以由第一个参数推导出来，因此建议删除第3个参数。

https://github.com/vllm-project/vllm/issues/10304
这个issue属于bug报告，主要涉及vllm框架中的旧式类，由于切换到`vllm_config`，导致需要更新outoftree注册的模型，用户需要根据清晰的错误消息进行更新。

https://github.com/vllm-project/vllm/issues/10303
这个issue属于bug报告类型，主要涉及vllm的环境构建错误。原因可能是安装过程中的错误设置或配置导致了无法成功构建环境。

https://github.com/vllm-project/vllm/issues/10302
这个issue类型是功能请求，主要涉及的对象是vllm引擎下的kv缓存，用户提出了开启主机内存作为kv缓存的需求，因为GPU内存不足导致缓存命中率下降。

https://github.com/vllm-project/vllm/issues/10301
这个issue是关于bug报告，主要涉及到vllm-openai的docker image需要更新。由于旧版本的docker image已经过时，导致用户提出需要更新的问题。

https://github.com/vllm-project/vllm/issues/10300
这是一个关于Bug报告的issue，主要涉及到VLLM在安装过程中出现的torch版本冲突导致的错误。

https://github.com/vllm-project/vllm/issues/10299
这是一个bug报告，该问题涉及vllm在使用meta-llama/Llama-3.2-1B时缺少chat_template导致无法使用的情况。

https://github.com/vllm-project/vllm/issues/10298
这是一个bug报告，主要涉及了vllm 0.6.3版本中长文本推理时得到无意义输出的问题，该bug可能是由于0.6.3版本引入的导致的。

https://github.com/vllm-project/vllm/issues/10297
这是一个bug报告，主要涉及的对象是`Qwen2ForSequenceClassification`模型。由于tensor并行处理出现问题导致了bug。

https://github.com/vllm-project/vllm/issues/10296
这是一个Bug报告。该问题涉及的主要对象是VLLLm执行Qwen/Qwen2.5-Coder-32B-Instruct模型时在两个H100 GPU上发生崩溃。问题可能是由于环境设置或代码实现问题导致VLLLm工作过程中出现异常。

https://github.com/vllm-project/vllm/issues/10295
这个issue属于bug报告，主要涉及vllm在使用enforceeager时无法使用async output processor导致的问题。

https://github.com/vllm-project/vllm/issues/10294
这是一个功能需求类型的issue，主要对象是将Quark量化格式上游到VLLM，由于目前Quark导出的量化模型格式与VLLM支持的格式不同，需要贡献代码到VLLM以添加对Quark格式的支持。

https://github.com/vllm-project/vllm/issues/10293
这是一个bug报告类型的issue，涉及的主要对象是在使用vllm中的Qwen2模型时无法正确使用yarn rope配置来处理长上下文。这个问题可能是由于transformers和vllm之间对于"original_max_position_embeddings"参数的不兼容所导致的。

https://github.com/vllm-project/vllm/issues/10292
这是一个[bug修复]类型的issue，主要涉及scaled_int8_quant函数在静态量化中未返回零点导致错误输出。

https://github.com/vllm-project/vllm/issues/10291
这是一个PR请求，主要是为了添加对多模态Granite模型的支持。

https://github.com/vllm-project/vllm/issues/10290
这是一个bug报告，主要涉及到vLLM下的multimodal模型，问题是在使用chunked prefill时出现了错误。

https://github.com/vllm-project/vllm/issues/10289
该issue类型为用户提出需求，主要涉及的对象是在vLLM中添加2D tensor parallelism和expert parallelism功能。由于缺乏这些功能，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/10288
这个issue是关于在添加缺失的分词器选项时出现的问题。

https://github.com/vllm-project/vllm/issues/10287
这是一个需求更新类型的issue，主要涉及benchmark_serving.py文件中设置image_url部分的更新，用于测试ShareGPT4V数据集。由于需要在client端使用image_url，所以更新了部分设置。

https://github.com/vllm-project/vllm/issues/10286
这是一个Bug报告，涉及vllm下的Vision LM模型在使用`vllm serve`时输出异常的问题，可能是由于图像在启动`vllm serve`时处理方式有bug。

https://github.com/vllm-project/vllm/issues/10285
这个issue是一个改进请求，主要涉及的对象是CI/Build系统和相关的脚本。这个问题是由于在本地运行`format.sh`时产生了INFO级别的问题，需要更新`shellcheck.sh`以在CI过程中标记此类问题。

https://github.com/vllm-project/vllm/issues/10284
这是一个bug报告，主要涉及加载和卸载模型时出现数值为None的不一致性问题，导致在使用多个GPU时出现错误。

https://github.com/vllm-project/vllm/issues/10283
这是一个Bug报告，主要涉及VLLM在扩展张量并启用Ray时初始化时间显著增加的问题。

https://github.com/vllm-project/vllm/issues/10282
这个issue属于[Core]类型，涉及主要对象为Flashinfer模块，由于GPU线程无法映射到block_tables导致的错误。

https://github.com/vllm-project/vllm/issues/10281
这是一个bug报告，主要涉及的对象是LoRA在idefics3中的支持。这个bug是由于之前的Idefics3支持引入的错误导致`dummy_data`长度超过了最大模型长度设置，最终修改解决了这个问题。

https://github.com/vllm-project/vllm/issues/10280
这是一个Bug报告，涉及VLLM的版本不同导致启动Qwen2.5服务输出结果错误的问题。

https://github.com/vllm-project/vllm/issues/10279
这是一个需求更新类型的 issue，涉及主要对象为 compressedtensors 版本的更新。

https://github.com/vllm-project/vllm/issues/10278
这是一个类型为需求更新的issue，主要涉及到更新`compressed-tensors`库版本至v0.8.0。原因可能是为了使用最新的压缩张量功能以提升性能或解决已知问题。

https://github.com/vllm-project/vllm/issues/10277
这个issue是用户提出需求，主要涉及的对象是在VLLM实现中添加fasteroutlines作为一种导向解码后端。

https://github.com/vllm-project/vllm/issues/10276
这是一个Bug报告，涉及的主要对象是Speculative Decoding + TP on Spec Worker + Chunked Prefill的功能组合。由于这些功能同时启用时服务器会立即失败，可能是由于代码实现问题导致的。

https://github.com/vllm-project/vllm/issues/10275
这是一个用户提出的需求类型的issue，主要涉及到使用TCPStore替代内部API的建议。原因可能是为了简化代码和降低依赖性。

https://github.com/vllm-project/vllm/issues/10274
这是一则用户提出需求的issue，主要涉及添加或替换QwenModel的模型测试。用户提出此问题可能是为了更全面地测试QwenModel模型。

https://github.com/vllm-project/vllm/issues/10273
这是一个功能需求的issue，主要涉及了torch.compile中的PassManager和PassConfig的重构，其中包括对后处理passes的提取、pass配置的提取、自定义inductor passes的注册等内容。

https://github.com/vllm-project/vllm/issues/10272
这个issue是一个bug报告，主要涉及的对象是V1引擎。原因是由于之前的更改导致CI测试失败。

https://github.com/vllm-project/vllm/issues/10271
这个issue类型是bug报告，涉及的主要对象是CMake版本限制。由于CMake版本限制导致的bug修复问题。

https://github.com/vllm-project/vllm/issues/10270
这是一个关于文档改进的类型的问题，涉及主要对象为调试文档。这个问题可能源于之前的文档不清晰或有错误，导致使用者难以理解如何调试或解决问题。

https://github.com/vllm-project/vllm/issues/10269
这个issue属于bug报告，主要涉及Adaptive Batching和同时发送请求可能导致引擎崩溃的问题。

https://github.com/vllm-project/vllm/issues/10268
这是一个 bug 报告，该问题涉及使用 piecewise CUDA graphs 时出现的输出错误。

https://github.com/vllm-project/vllm/issues/10267
该issue为用户提出需求，主要涉及VLLM的支持NVIDIA Unified memory，用户想了解是否有计划支持统一内存。

https://github.com/vllm-project/vllm/issues/10266
这是一个涉及文档位置的bug报告，用户指出了runllm widget的位置有问题，导致用户在查找时可能会困惑。

https://github.com/vllm-project/vllm/issues/10265
这是一个bug报告，主要涉及的对象是TPU，由于未提供具体内容无法分析导致了什么样症状的bug。

https://github.com/vllm-project/vllm/issues/10264
这是一个文档bug类型的issue，主要涉及在"arg_utils.py"文件中帮助信息消息的拼写错误。

https://github.com/vllm-project/vllm/issues/10263
这是一个bug报告类型的issue，主要涉及vllm对于Qwen系列无法进行量化的问题。原因是模型Qwen2ForCausalLM尚不支持BitsAndBytes量化。

https://github.com/vllm-project/vllm/issues/10262
这是一个bug报告，主要涉及QWen models的参数错误导致无法运行，用户提出了修复补丁并请求审查。

https://github.com/vllm-project/vllm/issues/10261
这是一个Bug报告，主要涉及到vllm项目中metrics.py文件中计算吞吐量的问题。原因是计算中的除数错误导致了性能数据存在显著偏差。

https://github.com/vllm-project/vllm/issues/10260
这个issue是关于用户提出需求的类型，主要涉及VLLM中特定功能的支持，具体是关于2:4 sparsity + w4a16 support的。

https://github.com/vllm-project/vllm/issues/10259
这是一个用户提出需求的issue，该问题涉及VLLM是否支持具有动态激活稀疏性的推理。原因是由于语言模型在推理过程中表现出激活稀疏性，用户希望VLLM能够支持这种特性。

https://github.com/vllm-project/vllm/issues/10258
这个issue属于bug报告类型，主要涉及VLLM（Very Large Language Model）项目中的Qwen2-VL模型不支持Lora的问题。原因是由于模型加载过程中出现了错误，导致无法正确加载模型。

https://github.com/vllm-project/vllm/issues/10257
这个issue类型为用户提出需求，并涉及vLLM模型在超级计算集群上远程访问问题，由于`base_url`连接问题导致无法正确访问部署在集群上的vLLM服务。

https://github.com/vllm-project/vllm/issues/10255
这个issue是关于bug报告，主要涉及的对象是Idefics3Model参数。原因是transformers版本导致了测试被跳过，从而忽略了对该模型参数的修改。

https://github.com/vllm-project/vllm/issues/10254
该issue类型为功能增强，主要涉及AMD硬件以及ROCm的支持，并解决了在ROCm上运行GGUF模型的问题。

https://github.com/vllm-project/vllm/issues/10253
这个issue是关于bug报告，主要涉及的对象是项目中的torch库版本。由于之前的更新导致publish.yml文件中的torch版本未更新，可能会导致构建过程中出现错误或不兼容的情况。

https://github.com/vllm-project/vllm/issues/10252
这是一个bug报告，主要涉及vllm版本0.6.3下的多lora推理问题，并描述了当查询长度超过15,000令牌时，导致了问题的症状。

https://github.com/vllm-project/vllm/issues/10251
该issue类型为用户提出需求，主要涉及安装vllm时的离线聊天功能，用户提出了如何在没有网络的情况下使用聊天功能的问题。

https://github.com/vllm-project/vllm/issues/10245
这是一个需求讨论类型的issue，主要涉及EngineCoreRequest对象的序列化和添加多模态输入功能。这个问题产生的原因是为了支持VLMs的细粒度调度并解决多模态输入的序列化问题。

https://github.com/vllm-project/vllm/issues/10244
这是一个安装问题的bug报告，主要涉及GPU版本的vllm安装，由于缺少名为"triton"的模块导致无法安装成功。

https://github.com/vllm-project/vllm/issues/10243
这是一个bug报告，涉及的主要对象是Deepseek V2 coder 236B awq。由于OpenCV安装出现问题和模块导入失败，导致出现了相关错误提示。

https://github.com/vllm-project/vllm/issues/10242
这个issue类型为需求提出，涉及主要对象为Layerwise profile更新。由于工具需要添加新功能来分析处理不同输出长度请求的场景，需要更改现有的参数和添加新的子命令来更清晰地表达用户意图。

https://github.com/vllm-project/vllm/issues/10241
这是一个特性开发的issue，涉及主要对象是TPU原型开发，包括性能测试和待办事项列表。

https://github.com/vllm-project/vllm/issues/10240
这个issue是关于文档说明vLLM中类层次结构，属于用户需求类型，主要涉及的对象是vLLM代码库。

https://github.com/vllm-project/vllm/issues/10239
这是一个用户提出需求的issue，主要涉及的对象是“HPU”，用户提出在每个模型解码层后添加`mark_step`以提高性能。

https://github.com/vllm-project/vllm/issues/10238
这个issue属于bug报告类型，主要对象是vllm在启用多卡 Ray 模式下注册新模型信息丢失的问题，可能是由于Ray启动后进程切换导致已注册模型列表丢失。

https://github.com/vllm-project/vllm/issues/10237
这是一个用户提出需求的issue，主要涉及torch.compile用户界面设计。原因是在导入模型之前需要设置环境变量，这会给用户带来困惑，现在的解决方案是在初始化模型时读取环境变量，并计划将它们移至与模型的'vllm_config'捆绑的编译配置中。

https://github.com/vllm-project/vllm/issues/10236
这个issue类型是文档更新（非Bug报告），主要对象是调试指南。由于使用时出现问题，用户提出了查看 https://github.com/vllmproject/vllm/issues/5484issuecomment2469199281 的建议。

https://github.com/vllm-project/vllm/issues/10235
这是一个用户提出需求的issue，主要涉及vLLM中核心功能的优化，是关于减少TTFT的问题。

https://github.com/vllm-project/vllm/issues/10234
这是一个Bug报告类型的Issue，主要涉及服务器内部错误以及与logprobs和sampling参数相关的问题。

https://github.com/vllm-project/vllm/issues/10233
这是一个功能需求，要求在python_only_dev中使用shutil重命名以支持在Linux系统中不同设备之间的重命名。

https://github.com/vllm-project/vllm/issues/10232
这是一个Bug报告类型的Issue，主要涉及的对象是vllm项目中的Spec model，由于Chunked prefill + spec decoding + TP on the spec model导致出现`KeyError: 'num_seq_groups'`错误，用户提出请求修复此问题。

https://github.com/vllm-project/vllm/issues/10231
这是一个bug报告，主要涉及的对象是关于`--distributed-executor-backend`参数的帮助文本更新问题。由于当前逻辑与帮助文本描述不一致，导致`ray`与`mp`的默认选择不符预期。

https://github.com/vllm-project/vllm/issues/10230
这个issue类型是feature提议，主要涉及的对象是vllm的CPU backend，用户提出需要支持mixture of experts (MoE)。

https://github.com/vllm-project/vllm/issues/10229
这个issue类型是需求提出，涉及主要对象是vLLM模型，用户提出需要将vLLM与Mistral fp8权重兼容。

https://github.com/vllm-project/vllm/issues/10228
这是一个用户提交需求的类型，主要涉及的对象是在CUDA图中启用自定义操作。由于之前的版本中处理CUDA图的自定义操作出现问题，导致用户提出了需要修正这一问题的需求。

https://github.com/vllm-project/vllm/issues/10227
这个issue是关于需求变更的，涉及主要对象为piecewise CUDA graphs，由于修改遗漏导致需要再次进行修改。

https://github.com/vllm-project/vllm/issues/10226
这个issue是关于bug报告，主要对象是auto port selection和测试禁用，由于端口冲突可能导致CI中出现不稳定性，建议使用auto selection。

https://github.com/vllm-project/vllm/issues/10225
这是一个关于优化编译时间的修改类型的issue，主要涉及到使用TorchInductor的问题，因为编译时间过长且存在问题导致的。

https://github.com/vllm-project/vllm/issues/10224
这是一个bug报告，主要涉及Detokenizer端口不匹配的问题。由于引入CC([V1] Allow `tokenizer_mode` and `trust_remote_code`)导致了端口不匹配的情况。

https://github.com/vllm-project/vllm/issues/10223
这是一个bug报告，主要涉及的对象是LoRA框架中的CUDA设备。由于CUDA设备硬编码导致测试失败，需要将CUDA设备更改为可配置参数。

https://github.com/vllm-project/vllm/issues/10222
这个issue类型为功能增强需求，主要涉及的对象是CPU测试，用户想要通过配置来绑定不同的核心。

https://github.com/vllm-project/vllm/issues/10221
这是一个需求提出的 issue，主要涉及 vLLM 中 Qwen2VL 模型对多个图像嵌入输入支持不同分辨率的问题。原因是当前 vLLM 实现要求所有输入图像具有相同分辨率，而 Qwen2VL 模型支持变化的图像分辨率。

https://github.com/vllm-project/vllm/issues/10220
这是一个用户提出需求的issue，主要涉及修改python -m vllm.entrypoints.openai.api_server的提示词。用户希望在代码、模型文件或命令行中改变系统提示词。

https://github.com/vllm-project/vllm/issues/10219
这是一个关于使用ThreadPoolExecutor可能导致输出质量下降的bug报告，主要涉及OpenAI兼容服务器功能的使用情况。原因可能是多线程执行时导致输出质量不稳定。

https://github.com/vllm-project/vllm/issues/10218
这个issue是bug报告，涉及的主要对象是encoder-decoder CPU runner。原因是在重构CPU encoder-decoder runner时，作者忘记实现了sampling preparation，导致该部分功能出现问题。

https://github.com/vllm-project/vllm/issues/10217
这是一个功能需求的issue，涉及的主要对象是添加Guidance后端以支持引导解码，由于需要扩展引导解码功能，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/10216
这个issue属于用户提出需求类型，主要对象是core中的分布式部分，用户希望新增无状态进程组功能。

https://github.com/vllm-project/vllm/issues/10215
这是一个Bug报告类型的Issue，主要涉及的对象是vllm serve在不同环境下的表现差异。这个问题可能是由于环境参数设置不当导致的异常。

https://github.com/vllm-project/vllm/issues/10214
这是一个请求移除未使用的模块的issue，涉及对象为vLLM的Kernel模块。

https://github.com/vllm-project/vllm/issues/10213
这是一个关于使用OpenTelemetry与fastapi不起作用的bug报告，用户遇到了在设置环境变量时无法正确识别ASGI应用程序而导致的问题。

https://github.com/vllm-project/vllm/issues/10212
这是一个文档修正类型的issue，涉及主要对象是block_manager中的`swap_out`函数。由于doc string中的拼写错误导致文档描述不准确，需要修正。

https://github.com/vllm-project/vllm/issues/10211
这是一个bug报告类型的issue，涉及的主要对象是Detokenizer过程。原因是之前未正确传递`tokenizer_mode`和`trust_remote_code`参数，导致某些需要远程代码的模型在测试时会出现卡住的情况。

https://github.com/vllm-project/vllm/issues/10210
该issue类型为更新请求，涉及的主要对象是3个软件包（`awscli`, `boto3`, `botocore`），用户请求更新了这3个软件包的版本号。

https://github.com/vllm-project/vllm/issues/10209
这个issue是一个bug报告类型的issue，主要涉及到actions/setuppython组件的更新和相关功能的改进。产生这个issue的原因是升级到了版本5.3.0但由于一些bug和功能需求导致了一些问题需要修复或改进。

https://github.com/vllm-project/vllm/issues/10208
这是一个功能需求的issue，主要涉及的对象是添加指导解码的指导标签处理器。由于需要扩展指导解码功能，因此添加了指导后端并支持不同类型的指导处理，包括JSON、选择和正则表达式生成。

https://github.com/vllm-project/vllm/issues/10207
这是一个bug报告，用户在部署使用Qwen2.57BInstructGPTQInt4模型时发现内存使用量异常高，期望量约为原模型的四分之一。

https://github.com/vllm-project/vllm/issues/10206
这是一个bug报告，主要涉及对象是运行debug code时出现的错误。由于GPU数量为1导致出现了AttributeError: 'PyNcclCommunicator' object has no attribute 'device'的错误。

https://github.com/vllm-project/vllm/issues/10205
这是一个关于GitHub上vllm项目下的issue，类型是用户提出需求，并涉及将完整配置传递给内部模型。

https://github.com/vllm-project/vllm/issues/10204
这个issue属于bug报告类型，主要涉及VLLM在多GPU环境下无法加载GGUF模型的问题。由于GPU内存不足导致OOM错误。

https://github.com/vllm-project/vllm/issues/10203
这个issue类型是用户提出需求，主要涉及到vllm下的torch.compile模块的 cudagraph buffer 管理。用户提出通过支持管理 cudagraph buffer 在简化用户代码的同时增加了额外输入张量和拷贝核的成本。

https://github.com/vllm-project/vllm/issues/10202
这是一个需求改进类型的issue，涉及主要对象是cloudpickle registration and tests。

https://github.com/vllm-project/vllm/issues/10200
这个issue类型是bug报告，主要涉及到bitsandbytes模型无法在pipeline parallel设置下运行，并且由于加载模型时出现了`PPMissingLayer`导致设置量化状态时出现错误。

https://github.com/vllm-project/vllm/issues/10199
这是一个 bug 报告类型的 issue，涉及的主要对象是 MistralLarge2407 模型。导致该问题的原因可能是在 v0.6.3post1 版本中使用长上下文时出现了奇怪的结果，而在 v0.6.1post2 版本中一切正常。

https://github.com/vllm-project/vllm/issues/10198
这是一个Bug修复类型的issue，涉及主要对象是关于SpecDecode中的sampling参数，由于采样参数在生成目标概率时不一致，可能导致拒绝采样的准确性问题。

https://github.com/vllm-project/vllm/issues/10197
这是一个功能需求的issue，主要涉及的对象是新模型的支持。由于未得到响应，用户提出了有关新模型支持的困难。

https://github.com/vllm-project/vllm/issues/10196
这是一个Bug报告。该问题涉及vllm/entrypoints/openai/cli_args.py文件中"enableautotoolchoice"参数的帮助消息有拼写错误。

https://github.com/vllm-project/vllm/issues/10195
该issue是关于项目文档完善的反馈，涉及修改与Hugging Face集成文档相关的内容。原因在于提出了处理流程校正、简化和代码参考的建议，旨在改善文档可读性。

https://github.com/vllm-project/vllm/issues/10194
这是一个用户提出的需求类型的issue，主要涉及到AriaForConditionalGeneration模型不支持LoRA的问题，用户希望在未来增加支持。

https://github.com/vllm-project/vllm/issues/10193
这个issue类型是bug报告，涉及的主要对象是vLLM的CPU后端，由于无法嵌入任何文本导致的问题。

https://github.com/vllm-project/vllm/issues/10192
这个issue类型为功能需求，涉及主要对象为加载模型时使用RunAI Model Streamer作为可选加载程序。由于需求增加了新的加载选项，用户想要能够从S3中加载模型并使用RunAI Model Streamer作为加载器。

https://github.com/vllm-project/vllm/issues/10191
这是一个用户提出需求的issue，主要涉及的对象是在vLLM中添加了Runai Model Streamer作为加载选项。

https://github.com/vllm-project/vllm/issues/10190
这个issue属于bug报告类型，主要涉及`CONTRIBUTING.md`文件中链接错误的问题。由于链接错误，导致无法正确指向实际的贡献指南。

https://github.com/vllm-project/vllm/issues/10189
这是一个bug报告类型的issue，主要涉及到vllm项目在使用cmake 3.31版本时出现的构建问题。

https://github.com/vllm-project/vllm/issues/10188
这是一个bug报告类型的issue，主要涉及到CMake版本问题，由于CMake 3.31的发布导致发布流程出现问题，出现了`CMake Error: Could not find CMAKE_ROOT !!! .`的错误提示。

https://github.com/vllm-project/vllm/issues/10186
这个issue类型是代码修改建议，主要涉及日志记录的添加。原因可能是为了更好的调试和信息记录。

https://github.com/vllm-project/vllm/issues/10185
这是一个bug报告，涉及的主要对象是vllm/vllm-openai:v0.6.3.post1 on OpenShift。由于500 Internal Server Error，用户在调用v1/completions和v1/chat/completions时遇到了问题。

https://github.com/vllm-project/vllm/issues/10184
这是一个bug报告，主要涉及模型的支持问题，用户提出了关于Qwen2 embeddings的错误提示。

https://github.com/vllm-project/vllm/issues/10183
这是一个用户提出需求的类型，主要关于添加关于使用Llama Stack 进行服务的文档。

https://github.com/vllm-project/vllm/issues/10182
这是一个bug报告，涉及的主要对象是模型gemm2-27b-it-fp8-dynamic-rope。由于模型具有rope_scaling，禁用滑动窗口功能会导致错误。

https://github.com/vllm-project/vllm/issues/10181
该issue是一个需求提出，主要涉及的对象是支持Duo-Attention功能。这个需求是因为DuoAttention方法可以显著减少长序列推理内存，提高推理速度，加速预填充，并可与量化结合在单个GPU上进行大规模推理。

https://github.com/vllm-project/vllm/issues/10180
这个issue属于Bug报告，主要对象是OpenAI server上的continue_final_message功能，问题是在应用continue_final_message时，"echo":false参数被忽略。

https://github.com/vllm-project/vllm/issues/10179
这是一个bug报告，主要涉及到vllm项目的安装过程。由于环境配置问题导致了编译错误，具体表现为未定义的标识符"__builtin_dynamic_object_size"。

https://github.com/vllm-project/vllm/issues/10178
这是一个关于功能需求的issue，主要涉及的对象是Qwen2VL模型。用户询问了关于Qwen2VL是否会支持LoRA推理的问题，因为当前的环境下报错指出该模型尚不支持LoRA，但用户希望进行LoRA推理实验。

https://github.com/vllm-project/vllm/issues/10177
这个issue属于bug报告，主要对象是mlp speculator模块的测试。由于测试无法通过，导致需要修复来解决问题。

https://github.com/vllm-project/vllm/issues/10176
这是一个bug报告类型的issue，涉及到修复RunLLM的位置。由于链接问题，导致无法正确定位RunLLM的位置。

https://github.com/vllm-project/vllm/issues/10174
这个issue是[Frontend]类型的，主要对象是为vLLM添加每个请求的缓存令牌统计。由于markdown渲染不起作用，采用原始HTML，在这个PR中加了一个新特性，但由于某些原因导致该特性默认为关闭状态。

https://github.com/vllm-project/vllm/issues/10173
这是一个关于文档补充的问题，涉及VLLM与Hugging Face集成说明的内容。

https://github.com/vllm-project/vllm/issues/10172
该issue类型为需求提出，主要涉及的对象是VLLM服务。由于需要在chat completions中添加base path环境变量以减少对反向代理的需求，用户提出了希望能够通过设置BASE_URL环境变量来访问服务的需求。

https://github.com/vllm-project/vllm/issues/10171
该issue是一个bug修复类型的问题，主要涉及将fp8和quantized fullgraph测试启用。导致该问题的原因可能是markdown渲染无法正常工作，因此需要在此处使用原始html。

https://github.com/vllm-project/vllm/issues/10170
这个issue类型是bug报告，涉及到SymIntArrayRef在只包含具体整数上出现错误。这个问题是由于SymIntArrayRef没有只包含具体整数而导致RuntimeError错误。

https://github.com/vllm-project/vllm/issues/10169
这是一个bug报告，涉及到GPTQ配置导致Qwen2-VL视觉模块未正确量化的问题。

https://github.com/vllm-project/vllm/issues/10168
这是一个bug报告，涉及到在使用mistral或phi时出现KeyError的问题。原因是当前环境中缺少vllm._version模块，导致了KeyError异常。

https://github.com/vllm-project/vllm/issues/10167
该issue类型为功能需求提出，主要涉及的对象是CI/Build过程，用户提出了添加runhputest.sh脚本来触发基本的CICD流程。

https://github.com/vllm-project/vllm/issues/10166
这个issue是一个bug报告，主要涉及的对象是V1代码库中的非CUDA图操作名称。这个问题出现的原因是在CC([V1] Make v1 more testable)中修改了attention操作的名称，导致错误地注册了要排除在CUDA图之外的操作名称。

https://github.com/vllm-project/vllm/issues/10165
这个issue类型是文档更新，主要涉及更新TPU安装指南，涉及的主要对象是安装和配置TPU的用户。由于原先的安装指南中存在一些错误链接、缺少对miniconda安装的说明以及需要替换参数值为小写，因此用户需要更新和修正这些问题。

https://github.com/vllm-project/vllm/issues/10164
这是一个Bug报告，主要涉及更新 Llama Chat Templates 以支持非工具使用，导致当前模板无法满足非视觉请求和图片输入的需求。

https://github.com/vllm-project/vllm/issues/10163
这是一个关于bug报告的issue，主要涉及到vllm的成功请求率问题。这个问题可能是由于配置参数不当导致的。

https://github.com/vllm-project/vllm/issues/10162
这是一个功能需求类的issue，主要对象是CI/Build。由于不希望在Git忽略的文件上运行shellcheck，因此提出了这一需求。

https://github.com/vllm-project/vllm/issues/10161
这是一个bug报告，主要涉及的对象是VLM的广播测试，导致问题是当前测试只在GPU:0上运行，GPU:1未被使用。

https://github.com/vllm-project/vllm/issues/10160
这是一个bug报告，主要涉及的对象是VLM broadcast测试，原因是`tensor_parallel_size`没有在测试中传递。

https://github.com/vllm-project/vllm/issues/10159
该issue是文档更新类型，涉及主要对象是项目的PR模板内容。由于需要将PR模板内容移至文档部分，导致需要进行相关的调整。

https://github.com/vllm-project/vllm/issues/10158
这是一个Bug报告，涉及的主要对象是vllm benchmark脚本在使用v1/chat/completions端点时出现错误。这个问题可能是由于benchmark脚本中参数配置不当导致的。

https://github.com/vllm-project/vllm/issues/10157
这是一个用户请教问题的issue，主要涉及如何在离线推断中设置`max_lora_rank`参数。用户提出这个问题可能是由于不了解如何将特定模型集成到vllm中。

https://github.com/vllm-project/vllm/issues/10156
这个issue是一个Bug报告，主要涉及的对象是vllm工具的使用。由于vllm在加载模型时出现了卡住的情况，可能是版本兼容性问题或者配置错误导致的。

https://github.com/vllm-project/vllm/issues/10155
这是一个bug报告，涉及的主要对象是VLLM在使用TPU v5p-16 (Multi-Host) with Ray Cluster安装时出现的问题。由于TPU数量超过了可用的数量，导致出现了"The number of required TPUs exceeds the total number of available TPUs in the placement group"的错误。

https://github.com/vllm-project/vllm/issues/10154
这是一个bug报告，主要涉及 vllm 中的 FlashInfer 功能，由于未设置 `use_tensor_cores=True` 导致出现错误提示。

https://github.com/vllm-project/vllm/issues/10153
这是一个bug报告类型的issue，主要涉及的对象是vllm框架下的Qwen2-VL-72B模型。由于硬件环境中只有4个H10080G，导致了内存不足的问题。

https://github.com/vllm-project/vllm/issues/10152
这是一个bug报告，主要涉及Mistral tokenizer的问题，由于缺少 'id_to_byte_piece' 属性导致了错误的症状。

https://github.com/vllm-project/vllm/issues/10151
这是一个bug报告，涉及的主要对象是使用vllm部署neuralmagic/DeepSeekCoderV2InstructFP8时出现的方法执行错误，可能导致分布式执行中的死锁。这可能是由于参数配置或代码逻辑问题而引起的。

https://github.com/vllm-project/vllm/issues/10150
这是一个bug报告，问题涉及的主要对象是AMD GPU上的tp>1模型。由于GPU卡住且利用率为0，可能是由于模型上线时发生了卡顿的情况。

https://github.com/vllm-project/vllm/issues/10149
这是一个用户提出需求的issue，主要涉及如何获取VLLM模型中令牌的logits分数。用户希望在执行判别任务时直接使用最后一层的logits分数，但由于设计限制，无法直接输出两个令牌的logits分数。

https://github.com/vllm-project/vllm/issues/10148
这个 issue 是关于在 vLLM 中默认仅为文本模型启用 APC 的修复补丁。

https://github.com/vllm-project/vllm/issues/10147
这个issue属于bug报告，涉及主要对象是vllm环境安装。原因是当前环境未在vllm源代码中找到vllm.envs文件，导致无法正确获取环境信息。

https://github.com/vllm-project/vllm/issues/10146
这是一个用户提出需求的issue，主要涉及的对象是`Idefics3Processor`/`Idefics3ImageProcessor`。由于缺乏对`size`参数的暴露，用户无法通过控制图片的大小来减少内存使用。

https://github.com/vllm-project/vllm/issues/10145
这个issue为修复拼写错误的类型，涉及到的主要对象是代码库中的特定Pull Request。修正拼写错误可能是由于在Pull Request评论中的小错误导致的。

https://github.com/vllm-project/vllm/issues/10144
这是一个bug报告，涉及的主要对象是XPU的Tensor parallel，由于升级到Torch 2.5导致XPU的tp出现问题，需要引入专门的XpuCommunicator来修复。

https://github.com/vllm-project/vllm/issues/10143
这个issue属于项目需求提出类型，主要涉及的对象是对EncoderDecoderModelRunner中LoRA支持状态的更新。由于LoRA支持在该项目中处于早期阶段，导致用户提出了需要补充LoRA相关代码、完善mllama的LoRA支持以及添加单元测试的需求。

https://github.com/vllm-project/vllm/issues/10142
这个issue是一个bug报告，主要涉及到GPU不支持FP8计算，导致使用FP8量化可能会降低性能。

https://github.com/vllm-project/vllm/issues/10141
这个issue是关于bug报告，涉及到flashinfer prefill.plan缺少数据类型的问题，可能由于在之前的代码修改中未正确调整引起了该bug。

https://github.com/vllm-project/vllm/issues/10140
该issue类型为用户需求，主要涉及改进首帧响应速度，可能由于当前环境下首帧响应速度较慢而导致用户提出该需求。

https://github.com/vllm-project/vllm/issues/10139
这个issue是一个bug报告，涉及的主要对象是CI/Build过程中的PR cleanup job。由于安全考虑需要使用`pull_request_event`而非`pull_request`，避免恶意PR修改workflow或执行恶意操作。

https://github.com/vllm-project/vllm/issues/10138
这是一个bug报告，涉及的主要对象是MistralTokenizer模型，由于tokenizer使用了`tokenizer.eos_token_id`而非`tokenizer.eos_token`，导致了Outlines与Mistral tokenizer模型的使用失败。

https://github.com/vllm-project/vllm/issues/10137
这是一个用户提出需求的issue，主要涉及支持预测输出功能，由于对提高潜在输出性能的需求和动机驱动。

https://github.com/vllm-project/vllm/issues/10136
这是一个bug报告，主要涉及的对象是启用了张量并行的草稿模型。由于同时启用了specdecode和chunkedprefill会导致在草稿模型中出现错误，需要禁用其中一个来避免问题。

https://github.com/vllm-project/vllm/issues/10135
这是一个用户提出需求的类型，主要对象是`Request`类。导致这个需求的原因是希望能够更高效地处理token ids而避免每次都需要进行O(n)的计算。

https://github.com/vllm-project/vllm/issues/10134
这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM模块中的logging模块。原因是为了避免与内置的logging模块重名而导致的问题。

https://github.com/vllm-project/vllm/issues/10133
这是一个修改建议类型的issue，涉及到vllm项目中的logging模块名称重名问题，建议将其重命名为logging_utils以避免与Python内置模块冲突。

https://github.com/vllm-project/vllm/issues/10132
这个issue是一个功能提议，涉及到MLPSpeculator/Medusa和`prompt_logprobs`的启用，并且出现了关于输出格式变化导致的问题。

https://github.com/vllm-project/vllm/issues/10131
这个issue是一个PR提交问题，涉及到vLLM项目中针对CUDA kernels或其他计算内核的更改。由于markdown渲染问题，内容需要使用原始的HTML。

https://github.com/vllm-project/vllm/issues/10130
这个issue属于提出需求，主要对象是为Mistral模型添加FP8格式。由于需要运行一些测试，需要确认添加FP8格式是否可行。

https://github.com/vllm-project/vllm/issues/10129
这是一个bug报告，涉及vllm中无法加载'microsoft/llavamedv1.5mistral7b'模型的问题，可能由于加载模型的方式不正确导致。

https://github.com/vllm-project/vllm/issues/10128
这是一个功能改进的issue，主要涉及V0版本中的调度问题，并且针对高缓存命中率时的性能提升进行了优化。

https://github.com/vllm-project/vllm/issues/10127
这是一个Bug报告。该问题涉及的主要对象是V1 LLMEngine。由于CC([V1] Make v1 more testable)未正确传递引擎给构造函数，导致启动引擎时崩溃。

https://github.com/vllm-project/vllm/issues/10126
这个issue是关于bug报告，涉及的主要对象是VLLM模型服务。导致该bug的原因是当prompt token长度超过512时，导致无法通过streaming返回响应。

https://github.com/vllm-project/vllm/issues/10125
这个issue类型是功能需求，主要涉及的对象是vLLM的2D prefills功能。由于2D prefills在计算token预填充时未考虑padding，导致了计算资源浪费和内存错误风险，用户提出需要添加padding-aware scheduling功能来解决这一问题。

https://github.com/vllm-project/vllm/issues/10123
这是一个bug报告，主要涉及的对象是vllm中的`qwen2vl7b`模型。由于输入提示过长或传入的图片过多，导致VLLM程序在推理时出现卡顿或错误，并且无法稳定进行推理，最终导致llm_engine崩溃或类似问题。

https://github.com/vllm-project/vllm/issues/10122
这个issue类型为需求提出，涉及的主要对象是CI/Build。由于GitHub配置不支持在条件性运行的工作流程上阻止自动合并，所以需要合并此PR，以便使mypy工作流程在所有PR上运行，不管更改了哪些文件。

https://github.com/vllm-project/vllm/issues/10121
这是一个类型为功能更改的issue，涉及的主要对象是pynccl的all reduce功能，由于这次更改将pynccl的all reduce变为out of place操作并移除了对torch distributed all reduce的支持。

https://github.com/vllm-project/vllm/issues/10120
这是一个bug报告，主要涉及的对象是VL模型中的量化配置，导致无法自动读取配置文件，需要手动指定加载模型。

https://github.com/vllm-project/vllm/issues/10119
这个issue类型是用户提出需求，涉及主要对象是用户希望支持一个新的模型。由于模型需要加载一个额外的线性层，可能导致无法直接在现有框架下支持。

https://github.com/vllm-project/vllm/issues/10116
该issue属于Bug报告，涉及的主要对象是vllm下的7B模型。原因可能是无法连接到本地端口46000，导致7B模型无法使用cmd vllm.entrypoints.openai.api_server。

https://github.com/vllm-project/vllm/issues/10115
这是一个Bug报告类型的Issue，主要涉及的对象是vllm项目的安装和文档指引。导致出现问题的原因可能是安装命令中指定的版本不正确。

https://github.com/vllm-project/vllm/issues/10114
这是一个功能需求类型的issue，主要涉及了vLLM中多模态模型的输入处理，问题是由于当前设计中的input processor和input mapper导致的一些实现问题和性能问题。

https://github.com/vllm-project/vllm/issues/10113
该issue是一个提交PR的类型为Bugfix，并涉及集成torch compile相关的问题。由于markdown渲染问题，导致描述无法显示正确格式。

https://github.com/vllm-project/vllm/issues/10112
这是一个bug报告，涉及到vllm项目中的Qwen2-VL模型处理图像时未正确处理`mm_processor_kwargs`参数造成的错误。

https://github.com/vllm-project/vllm/issues/10111
这是一个bug报告，涉及vLLM中`continue_final_message`参数在与`"echo":false`同时使用时出现的问题，导致新信息总是添加到最后一条消息之前。

https://github.com/vllm-project/vllm/issues/10109
这是一个bug报告，涉及的主要对象是XPU offline_inference.py模块。导致这个问题的原因可能是运行环境不兼容。

https://github.com/vllm-project/vllm/issues/10108
这是一个bug报告，主要涉及硬件相关的bug修复。由于缺少AVX2内核支持，AVX2-only目标上的一半dtype功能无法正常工作。

https://github.com/vllm-project/vllm/issues/10107
这是一个关于向vLLM的XPU后端添加AWQ/GPTQ支持的issue，类型为功能需求。

https://github.com/vllm-project/vllm/issues/10106
这个issue是关于CI/Build的，主要对象是更新transformers版本至4.46.2，解决了Chameleon和Llava问题，以尝试修复破损的测试。

https://github.com/vllm-project/vllm/issues/10105
这个issue是一个需求提案，主要对象是为了为serving benchmark添加基于gamma分布的请求生成支持，以更好地将基准测试过程与实际场景和性能特征对齐。原因是现有的benchmarking process与真实场景有一定差距，需要增加gamma分布请求生成支持。

https://github.com/vllm-project/vllm/issues/10104
这是一个在项目代码中进行功能优化的issue，涉及的主要对象是ModelConfig类。由于之前的命名不清晰，导致在模型运行时无法直接引用一些属性，需要进行代码整合优化。

https://github.com/vllm-project/vllm/issues/10103
这是一个Bug修复类的Issue，主要涉及对象为FlashInfer的API。由于名称变更导致需同步变更，可能会影响系统调用或数据处理。

https://github.com/vllm-project/vllm/issues/10102
这是一个bug报告类型的issue，主要涉及到VLLM引擎在处理多GPU情况（tp=2）下出现的问题，导致请求超时无响应。

https://github.com/vllm-project/vllm/issues/10101
这个issue类型是功能需求，主要涉及支持优先级预抢占和分块预填充功能。由于chunkprefill启用时，优先级调度不会触发预抢占，用户提出了这个功能需求。

https://github.com/vllm-project/vllm/issues/10100
该issue为用户提出需求，主要对象是代码中的Python 3.8 ABI。由于之前被删除而导致的缺失，用户请求重新添加Python 3.8 ABI。

https://github.com/vllm-project/vllm/issues/10099
该issue类型是用户提出需求，主要涉及vLLM生成多个相同提示的回答，并询问是否会存储缓存以提升速度。

https://github.com/vllm-project/vllm/issues/10098
这是一个关于软件bug的报告，主要涉及到使用vllm时在调用API时出现进程崩溃的问题。可能是由于chattemplate或解析器的问题导致了block_table数值为None，进而引发了进程崩溃。

https://github.com/vllm-project/vllm/issues/10097
这是一个Bug报告，主要涉及到VLLM模型中seed参数在temp为0时没有效果的问题。

https://github.com/vllm-project/vllm/issues/10096
这是一个需求提出类型的issue，主要对象是在Dockerfile中将hf_transfer包安装到测试镜像中，以便CUDA构建测试能够更快地下载模型。

https://github.com/vllm-project/vllm/issues/10095
这是一个bug报告，涉及到修复了torch>2.5下CUDA<12.4的FP8 torch._scaled_mm fallback的问题。问题是因为未更新ROCm特例以在CUDA下升级pytorch版本后进行适配导致的。

https://github.com/vllm-project/vllm/issues/10094
这是一个bug报告类型的issue，主要涉及的对象是vllm库中的Speculative Decoding功能。这个问题可能是由于依赖CUDA的硬编码限制导致的，用户提出了需要移除硬编码依赖并为CPU支持Speculative Decoding的需求。

https://github.com/vllm-project/vllm/issues/10093
这是一个关于用户提出需求的issue，主要涉及如何禁用pydantic的请求验证以通过自定义角色，可能是因为需要在vllm中使用自定义角色而提出的这个问题。

https://github.com/vllm-project/vllm/issues/10092
这是一个需求报告，主要涉及持续集成/build流程，由于Github配置不够灵活，要求在自动合并PR之前必须运行通过特定的workflow，但无法精确控制workflow何时运行，因此提出需要始终运行特定workflow的问题。

https://github.com/vllm-project/vllm/issues/10091
这是一个用户需求类型的 issue，该问题单涉及的主要对象是 vllm 下的 attention kernel 文件。由于模板实例化过多，编译速度变慢，因此需要将 attention kernel 文件分成两个文件以提高编译速度。

https://github.com/vllm-project/vllm/issues/10090
这是一个bug报告，涉及Web UI中的布局问题导致"ask AI"按钮与文档版本选择重叠。

https://github.com/vllm-project/vllm/issues/10089
这是一个UI改进类型的Issue，主要涉及网站界面上"ask AI"和文档版本选择重叠的问题，需要将文档版本选择移到左侧以提升用户体验。

https://github.com/vllm-project/vllm/issues/10088
这个issue是一个测试问题，主要涉及到测试脚本，由于缺少TORCH_BLAS_PREFER_HIPBLASLT=1标志，导致了正确性问题。

https://github.com/vllm-project/vllm/issues/10087
这是一个Bug报告，主要涉及的对象是当用户提供中间件时请求无法被中断的问题。问题是由于Starlette行为导致`request.is_disconnected()`无法正确返回断开连接状态，从而导致请求无法被中断。

https://github.com/vllm-project/vllm/issues/10086
该issue属于功能增强类型，主要涉及改进与先进负载均衡/网关的集成，提供更好的负载/成本报告和LoRA管理。由于AI推理具有独特特征，需要更高级的负载均衡策略，作者希望提高vLLM与先进LB/gateways的集成，以支持更有效的load balancing。

https://github.com/vllm-project/vllm/issues/10085
这是一个Bug报告类型的issue，主要涉及到vllm中的ultravox-v0_4模型出现OutOfMemory错误。原因可能是whisper和llama尝试被放置在相同的GPU上导致，用户寻求解决这个问题以将它们调度到不同的GPU。

https://github.com/vllm-project/vllm/issues/10084
这是一个bug报告，主要涉及问题是在推理过程中发送预先标记化的问题似乎没有比原始文本更快。可能因为集成不当导致此现象。

https://github.com/vllm-project/vllm/issues/10083
这个issue类型是CI/Build相关问题，涉及的主要对象是CI的workflow配置文件。原因是代码重构导致`codespell`被误删除，需要重新添加到CI中。

https://github.com/vllm-project/vllm/issues/10082
这个issue类型属于功能需求，主要涉及的对象是Github上的CI/Build流程。原因是为了确保PR合并时PR正文内容适合包含在合并后的压缩提交中。

https://github.com/vllm-project/vllm/issues/10081
这是一个关于Bug报告的问题，主要涉及的对象是vLLM引擎。由于结构化输出推断耗时过长，导致GPU KV缓存使用率达到100%，最终引发超时和引擎崩溃。

https://github.com/vllm-project/vllm/issues/10080
该issue类型为bug报告，涉及的主要对象为CI/Build中的conflict PR comment。原因是PRs opened by dependabot会误将特定评论视作命令来rebase PR，需要更新文本以避免误解。

https://github.com/vllm-project/vllm/issues/10079
这是一个需求类型的issue，主要对象是为Serving Benchmark添加对基于Gamma分布的请求支持。

https://github.com/vllm-project/vllm/issues/10078
这是一个bug报告，主要涉及openai_chat_completion_client_for_multimodal.py的错误。产生这个问题的原因是额外的输入不被允许，导致了BadRequestError异常。

https://github.com/vllm-project/vllm/issues/10077
这是一个用户提出需求的issue，主要关注Gamma分布请求在服务基准测试中的支持。

https://github.com/vllm-project/vllm/issues/10076
这个issue类型是bug报告，主要涉及的对象是前端。由于传递了多个关键字参数导致了错误消息出现多个值。

https://github.com/vllm-project/vllm/issues/10075
这是一个bug报告，主要涉及 vLLM 的 continue_final_message 应用时出现的关键字参数重复错误。这个问题可能由于未清晰的参数传递或版本不匹配引起。

https://github.com/vllm-project/vllm/issues/10074
这是一个Bug报告，涉及的主要对象是vllm项目下的Llama3.1模型。此问题导致在不同运行中生成的输出结果不一致。

https://github.com/vllm-project/vllm/issues/10073
这个issue是一个bug报告，主要涉及到修复了在添加Intel Gaudi (HPU)推断后导致的错误。

https://github.com/vllm-project/vllm/issues/10072
这是一个用户提出需求的类型，该问题单涉及的主要对象是分布式功能的stateless_init_process_group功能。

https://github.com/vllm-project/vllm/issues/10071
这是一个功能需求类型的issue，主要涉及到LlamaEmbeddingModel。由于用户希望验证文本嵌入模型是否支持LoRA，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/10070
这个issue是关于一个CI/Build方面的问题，涉及到了vLLM项目中的`large_gpu_mark`原因修正。原因可能是由于markdown渲染问题导致描述无法正常显示。

https://github.com/vllm-project/vllm/issues/10069
该issue类型为改进/优化提案，涉及主要对象为VLLM模型测试；通过将模型测试拆分为组来减少由于不稳定性（例如连接失败）导致的影响，并且修复了由两个commit引入的VLM测试失败问题。

https://github.com/vllm-project/vllm/issues/10068
这是一个关于技术问题的询问，主要对象是vllm模型的推断过程，用户想了解在使用w8a8量化模型进行推断时是否保留int8上的矩阵运算，寻求具体实现逻辑代码的位置。

https://github.com/vllm-project/vllm/issues/10067
这是一个bug报告类型的issue，主要涉及MiniCPM3-4B模型加载错误，用户表示安装了datamodel_code_generator但仍然无法解决问题。

https://github.com/vllm-project/vllm/issues/10066
这个issue类型是文档问题，主要涉及vllm对函数调用的支持情况。用户提出问题是否支持函数调用，可能由于文档内容不清晰或缺失引起。

https://github.com/vllm-project/vllm/issues/10065
这个issue是关于用户提出需求的，主要涉及对象是vllm模型架构。由于当前版本不支持LlavaNextForConditionalGeneration模型架构，用户想要使用LLAVA1.6hfmistral7B，导致了无法完成所需功能。

https://github.com/vllm-project/vllm/issues/10064
这个issue类型为功能需求提议，主要对象是分布式IPC缓冲区的创建。由于PyTorch的IPC句柄格式可能改变，使用PyTorch进行CUDA IPC会受到PyTorch变化的影响。

https://github.com/vllm-project/vllm/issues/10063
这是一个Bug报告，主要涉及到key由`kv_scale`改变为`k_scale`和`v_scale`导致的加载模型时可能出现KeyError错误的问题。

https://github.com/vllm-project/vllm/issues/10062
这个issue是关于性能问题的，主要涉及到在A100 40GB GPU上使用单个LoRA适配器导致吞吐量和延迟降低的情况。

https://github.com/vllm-project/vllm/issues/10061
这是一个关于在处理chunked prefill时计算动态块大小的一个issue，属于Core类型的贡献。

https://github.com/vllm-project/vllm/issues/10060
这个issue是关于代码质量的改进，主要涉及到了解决Wswitch-bool编译器警告问题。原因是代码中存在switch on boolean的写法，导致编译器报警。

https://github.com/vllm-project/vllm/issues/10059
这是一个性能优化类型的issue，主要对象是vllm库中的piecewise cudagraph。导致该问题的原因可能是图形捕获时间过长，已经进行了优化，降低了从5秒到0.5秒的时间。

https://github.com/vllm-project/vllm/issues/10058
这是一个功能需求的issue，主要涉及V1模型运行器中集成分段CUDA图的功能。由于目前分段CUDA图与自定义操作不兼容，因此依赖于Torch Inductor来优化模型，导致无法支持FP8或其他量化模式。

https://github.com/vllm-project/vllm/issues/10057
这个issue属于bug报告类型，主要涉及的对象是vllm中的AWQ模块，由于存在未实现的`ScaledActivation`导致一些症状问题，并且提出疑问该方法的实际使用场景。

https://github.com/vllm-project/vllm/issues/10056
这是一条关于代码优化的issue，主要涉及前端API实现。由于对try/except块进行了调整，提出建议将它们调整为单个块以提高代码清晰性和简洁性。

https://github.com/vllm-project/vllm/issues/10055
这是一个文档更新类的issue，涉及主要对象为该项目的TPU安装指南。由于安装指南中未明确指出需要等待资源请求分配、未提供检查请求状态的命令、未提供安装Miniconda的命令，导致用户可能遇到问题或需要更新的信息。

https://github.com/vllm-project/vllm/issues/10054
这个issue属于Bugfix类型，主要涉及了CustomChatCompletionContentPartParam multimodal input type的移除，因为该参数未被使用且可能导致请求异常，需要修复以避免触发500 Internal Server Error。

https://github.com/vllm-project/vllm/issues/10053
这个issue是文档修正类型，涉及到添加离线分布式持续批处理示例。由于markdown渲染不起作用，所以使用原始HTML。

https://github.com/vllm-project/vllm/issues/10052
这个issue是优化建议，关注的主要对象是`entrypoints/openai/test_accuracy.py`和`entrypoints/openai/test_run_batch.py`。这个优化建议是因为这两个测试占据了大部分entrypoint时间。

https://github.com/vllm-project/vllm/issues/10051
这是一个bug报告，涉及vLLM下的Mistral Tekken Tokenizer，由于特殊token导致的边缘情况崩溃。

https://github.com/vllm-project/vllm/issues/10050
这是一个用户提出需求的issue，涉及的主要对象是将新的Navi架构添加到支持的架构列表中。

https://github.com/vllm-project/vllm/issues/10049
这个issue类型属于功能性需求，主要涉及的对象为Cutlass c3x kernels的重构。原因是为了提高代码的可维护性并方便进行实验。

https://github.com/vllm-project/vllm/issues/10048
这个issue是一个bug报告，主要涉及的对象是piecewise cudagraph。由于权重都为零，导致了正确性问题，这个问题由于引用弱引用导致。

https://github.com/vllm-project/vllm/issues/10047
这是一个Bug报告，涉及的主要对象是代码中的`trust_remote_code`设置。该问题由于`trust_remote_code`被错误地设置为`True`而导致了bug。

https://github.com/vllm-project/vllm/issues/10046
这是一个关于性能差异的Benchmark issue，涉及VLLM项目中的guided decoding功能，由于使用了 outlines，导致了TPS下降约24%。

https://github.com/vllm-project/vllm/issues/10045
这是一个 bug 报告，主要涉及 vLLM 的 OpenVINO 后端和一些错误及属性问题。原因是缺少 placeholder index maps 和返回 tuple 导致了错误和 AttributeError。

https://github.com/vllm-project/vllm/issues/10044
这是一个功能需求的issue，主要涉及的对象是多模态处理器。用户提出该问题是为了减少由后续PR引起的合并冲突风险，目前还没有使用新多模态处理器的模型。

https://github.com/vllm-project/vllm/issues/10043
这是一个用户提出需求的issue，主要涉及的对象是新增模型支持。由于Tencent发布了一个性能优越的389B MoE模型，用户希望在vllm中添加对该模型的支持。

https://github.com/vllm-project/vllm/issues/10042
这个issue属于bug报告类型，主要涉及的对象是在加载Molmo7B模型时出现了AttributeError错误，可能是由于程序中使用了tuple对象导致的。

https://github.com/vllm-project/vllm/issues/10041
这个issue类型是优化建议，涉及的主要对象是项目的CI/Build过程。改动主要是为了改善本地使用`format.sh`时对Python版本兼容性的检测以及CI作业对不同Python版本的兼容性检查。

https://github.com/vllm-project/vllm/issues/10040
这个issue属于修改需求，主要涉及对象是`MultiModalInputs`。由于命名不符导致混淆，需要将其重命名为`MultiModalKwargs`。

https://github.com/vllm-project/vllm/issues/10039
这是一个Bug报告，涉及到PyTorch 2.5.x版本下vLLM 1.0.0 dev的问题，主要问题是当tensor parallel size设置大于1时引发了ZMQError('Operation not supported')的错误。

https://github.com/vllm-project/vllm/issues/10038
这是一个关于CI/Build的issue，涉及主要对象是Python 3.8支持。由于Pytorch 3.5放弃了对Python 3.8的支持，因此需要删除对Python 3.8的支持。

https://github.com/vllm-project/vllm/issues/10037
该Issue属于代码优化类型，主要涉及的对象是嵌入模型列表的排序和去除不必要的合并功能。

https://github.com/vllm-project/vllm/issues/10036
这是一个bug报告，主要涉及vllm安装环境缺少指定的whl文件，导致用户无法正常安装vllm。

https://github.com/vllm-project/vllm/issues/10035
这个issue是关于Bug报告，主要涉及的对象是mathshepherdmistral7bprm模型。原因是该模型目前不支持'embedding'任务，用户想知道是否有办法在embedding模式下使用该模型。

https://github.com/vllm-project/vllm/issues/10034
这是一个Bug报告，问题涉及到在使用docker环境中无法完全占用GPU内存的情况。由于CUDA内存不足导致无法加载模型权重，产生了CUDA out of memory错误。

https://github.com/vllm-project/vllm/issues/10032
这是一个bug报告，主要涉及的对象是CPU backend，问题可能是由于缺少对CPU backend的射线支持而产生。

https://github.com/vllm-project/vllm/issues/10031
这个issue是关于性能问题的报告，主要涉及Medusa方法的性能比Naive推断方法差的情况，或者作者提出了改进性能的建议。

https://github.com/vllm-project/vllm/issues/10030
这是一个bug修复类型的issue，涉及主要对象是CustomAllreduce的ipc buffer init重构。由于之前使用cuda API来共享张量而不是回复`_share_cuda_`，这会导致在可扩展段或未来的pytorch升级中不会发生中断。

https://github.com/vllm-project/vllm/issues/10029
这个issue是用户提出需求，请求vllm项目支持OpenAI的/v1/models端点，由于当前vllm项目不提供对此端点的支持，导致无法查看可用模型。

https://github.com/vllm-project/vllm/issues/10028
这是一个Bug报告，主要涉及的对象是Molmo模型。由于参数设置错误导致函数dummy_data_for_profiling中的断言始终失败，进而影响了vllm的启动。

https://github.com/vllm-project/vllm/issues/10027
这是一个针对CUDA内存记录的问题，主要涉及到在vLLM中添加CUDA内存日志记录功能，之前的CUDA内存日志记录已过时需要更新。

https://github.com/vllm-project/vllm/issues/10026
这是一个需求报告，用户询问关于vllm的A100 CI Benchmark结果中使用的A100硬件配置的问题。

https://github.com/vllm-project/vllm/issues/10025
这是一个Bug报告，主要涉及vLLM的使用中出现了OpenVINO backend相关的错误。由于OpenVINOAttentionMetadata初始化时出现了`multi_modal_placeholder_index_maps`意外关键字参数的错误。

https://github.com/vllm-project/vllm/issues/10024
这是一个Bug报告，主要涉及VLLM引擎出现了异常导致服务器进程终止的问题。

https://github.com/vllm-project/vllm/issues/10023
这是一个bug报告，涉及到vllm server的配置和使用中的问题，由于缺少必要的chat template文件导致了错误提示。

https://github.com/vllm-project/vllm/issues/10022
这个issue是功能新增类型，主要对象是Qwen2VL模型的LoRA支持。由于用户希望在Qwen2VL中实现LoRA支持，因此添加了这一功能。

https://github.com/vllm-project/vllm/issues/10021
这个issue是一个BugFix类型的问题，主要涉及的对象是Lazy import ray。出现这个问题的原因是为了实现Lazy import ray并在ray不存在时捕获导入错误。

https://github.com/vllm-project/vllm/issues/10020
这个issue属于一个功能需求提议，提议为VLMs添加在线视频支持。

https://github.com/vllm-project/vllm/issues/10019
这是一个bug报告，涉及主要对象是torch.compile中的piecewise CUDA graphs。原因是当前测试不严谨，导致无法准确验证piecewise CUDA图的功能性。

https://github.com/vllm-project/vllm/issues/10018
该issue类型为用户提出需求，涉及主要对象是关于vllm中如何组织大量请求的调用。用户询问如何在调度级别做最佳实践以降低延迟，并寻求减少潜在影响推理性能的解决方案。

https://github.com/vllm-project/vllm/issues/10017
该issue属于用户提出需求，主要针对vllm CLI flags排序问题。原因可能是当前CLI flags无序导致用户阅读困难。

https://github.com/vllm-project/vllm/issues/10016
这是一个功能需求的issue，主要涉及vllm CLI flags的显示顺序问题，用户希望能够提升CLI参数的易读性。

https://github.com/vllm-project/vllm/issues/10015
这是一个Bug报告，针对vllm启动时openai的swarm函数调用出现的错误。原因可能是输入格式不正确导致的错误信息。

https://github.com/vllm-project/vllm/issues/10014
这是一个bug报告类型的issue，主要涉及对象为flash_attn_varlen_func()函数。由于切换了分支导致函数调用时出现了unexpected keyword argument 'window_size'的错误。

https://github.com/vllm-project/vllm/issues/10013
这个issue是一个Bugfix类型的报告，涉及到vLLM的encoderdecoder模型。由于当前实现在重新计算期间没有释放crossattention blocks，导致了"AssertionError: block table already exists"的错误。

https://github.com/vllm-project/vllm/issues/10012
这是一个Bug报告，涉及到vllm的前端相关问题，由于使用了错误的`fd`选项导致"Address already in use"错误。

https://github.com/vllm-project/vllm/issues/10011
这是一个需求类型的issue，涉及主要对象是vLLM模型。这个问题提出了添加级联推断功能以加速推断过程的需求。

https://github.com/vllm-project/vllm/issues/10010
这个issue类型是bug报告，主要涉及的对象是TPU requirements file和build dependencies，其原因可能是旧的URL需要替换导致出现问题。

https://github.com/vllm-project/vllm/issues/10009
这是一个Bug报告类型的issue，主要涉及vLLM多步调度在输入提示过长时导致崩溃的问题。

https://github.com/vllm-project/vllm/issues/10008
这是一个与代码优化和构建依赖项相关的issue，主要涉及TPU要求文件的重构和构建依赖项的确定。

https://github.com/vllm-project/vllm/issues/10007
这个issue是关于bug报告，主要涉及的对象是quant config在speculative decode中出现的问题，导致了构建错误和WSL Ubuntu安装问题。

https://github.com/vllm-project/vllm/issues/10006
这是用户提出的bug报告，涉及到Ray Backend V1，由于目前只适用于TP 1导致bug。

https://github.com/vllm-project/vllm/issues/10005
这是一个文档修复类型的 issue，主要涉及 vLLM 例子中的任务(`task`)应用问题，导致无法运行。

https://github.com/vllm-project/vllm/issues/10004
这是一个Bug报告，涉及到CC模块中MambaMixer的拆分问题。这个bug出现的原因是因为默认情况下没有运行Mamba集成测试。

https://github.com/vllm-project/vllm/issues/10003
这是一个bug报告类型的issue，主要涉及安装vllm CPU版本时出现的错误和环境信息。由于环境中缺少CUDA支持，导致无法成功安装vllm CPU版本。

https://github.com/vllm-project/vllm/issues/10002
这个issue是一个bug报告，主要涉及VLLM引擎出现了"RuntimeError: Engine loop has died with larger context lengths (>32k)"的问题，用户寻求关于如何调试此问题的帮助。

https://github.com/vllm-project/vllm/issues/10001
该issue是一个Bug报告，涉及的主要对象是将vLLM升级到pytorch 2.5.1版本。由于markdown渲染不起作用，所以使用了原始的HTML。

https://github.com/vllm-project/vllm/issues/10000
这是一个BugFix类型的issue，涉及到vLLM中设置`tool_choice`为`none`选项时会引发`ValueError`的问题。由于当前验证检查期望在定义`tool_choice`时也设置`tools`字段，所以设置`tool_choice`为`none`会导致错误。

https://github.com/vllm-project/vllm/issues/9999
这是一个文档更新的issue，主要涉及vLLM文档中关于从本地文件加载的内容。由于markdown渲染存在问题，因此采用原始的HTML格式。可能是由于markdown渲染问题导致了这个issue。

https://github.com/vllm-project/vllm/issues/9998
这是一个bug报告，主要涉及到计算`RequestMetrics`中的时间相关问题，用户提出`last_token_time`等于`arrival_time`是否是一个bug以及关于时间单位和计算tokens/second等指标的问题。

https://github.com/vllm-project/vllm/issues/9997
这是一个bug报告，涉及的主要对象是修改BNB参数名称。

https://github.com/vllm-project/vllm/issues/9996
这是一个bug报告类型的issue，主要涉及的对象是vLLM的内存分析功能。导致这个bug的原因是GPU内存未能在初始化vLLM实例之前正确清理，导致内存分析出现错误并引发问题。

https://github.com/vllm-project/vllm/issues/9995
这是一个Bug报告，涉及到vLLM中的speculative decoding功能，在判断可用块数量时仅考虑了目标模型的内存使用情况，导致可能会发生cuda OOM错误。

https://github.com/vllm-project/vllm/issues/9994
这个issue是一个性能优化的建议，主要涉及到ShmRingBuffer中的时间延迟问题，提出使用os.sched_yield替代time.sleep，以提高性能。

https://github.com/vllm-project/vllm/issues/9993
这是一个bug报告，涉及的主要对象是在导入decord之前导入vllm，在程序引起segmentation fault的现象。

https://github.com/vllm-project/vllm/issues/9992
这是一个性能问题的报告，主要涉及的对象是Qwen2-VL-2B-Instruct模型，由于使用FP8相比于FP16性能表现更差，表现为处理速度下降。

https://github.com/vllm-project/vllm/issues/9991
这是一个Bug报告类型的issue，主要涉及Llama3.2工具调用OpenAI API时出现问题，可能是由于调用方式不正确导致无法获取OpenAI API功能。

https://github.com/vllm-project/vllm/issues/9990
这是一个Bug报告类型的Issue，主要涉及的对象是无法在Tesla T4 GPU上以全精度加载模型进行服务。由于无法加载模型，可能导致无法进行模型的Serving操作。

https://github.com/vllm-project/vllm/issues/9989
这是一个bug报告，涉及到vllm项目中internvl模块的功能异常，用户指出了"max_dynamic_patch"和"add_special_tokens"的问题。

https://github.com/vllm-project/vllm/issues/9988
这是一个Bug报告，主要涉及到vLLM的版本迁移问题，由于不同版本的vLLM在相同输入下输出不同导致服务容易崩溃。

https://github.com/vllm-project/vllm/issues/9987
这是一个特性改进请求，主要涉及的对象是优化 BNB 静态变量，以简化 BNB 量化支持过程。

https://github.com/vllm-project/vllm/issues/9986
这是一个Bug报告，涉及的主要对象是部署vllm时出现的错误信息。导致这个Bug的原因可能是环境配置不正确导致模块缺失和版本不匹配。

https://github.com/vllm-project/vllm/issues/9985
这个issue类型为用户提出需求，主要涉及的对象是vllm的批量离线推理。用户询问关于离线批处理推理是否有需求对提示列表长度有要求。

https://github.com/vllm-project/vllm/issues/9984
这个issue是一个Bug报告，涉及到vllm项目的`benchmark_serving.py`文件中`mean_e2el_ms`和`median_e2el_ms`计算错误导致的问题。

https://github.com/vllm-project/vllm/issues/9983
这是一个需要合并其他内容后才能解决的issue类型，涉及的主要对象是model的配置。由于缺少从另一个代码分支合并的部分，导致无法将完整的配置传递给模型。

https://github.com/vllm-project/vllm/issues/9982
这是一个功能更新类型的 issue，主要对象是该项目中的 Encoder Decoder 模块。由于当前版本只能运行在 xFormers 后端，用户希望更新 Mllama，使其能够在 xFormers 和 FlashAttention 两个后端上运行。

https://github.com/vllm-project/vllm/issues/9981
这是一个bug报告，主要对象是vllm项目中的multi-step模式下的一处代码问题，导致在skip_tokenizer_init=True的情况下出现崩溃。

https://github.com/vllm-project/vllm/issues/9980
这是一个bug报告，主要涉及VLLM的模型架构问题，用户提出了无法启动特定模型的错误。

https://github.com/vllm-project/vllm/issues/9979
这是一个Bug报告，涉及到使用VLLM部署falconmamba7binstruct模型时出现错误的情况。原因可能是模型架构当前不可用导致。

https://github.com/vllm-project/vllm/issues/9929
这个issue是一个功能需求报告，主要涉及VLLM的prefix benchmarking功能。

https://github.com/vllm-project/vllm/issues/9928
这是一个类型是feature enhancement的issue，主要涉及CI/Build相关的工作流程优化。这个问题是为了节省时间和资源，只在相关文件发生变化时运行CI jobs，避免不必要的job运行。

https://github.com/vllm-project/vllm/issues/9927
这个issue属于用户提出需求类型，主要涉及vLLM的性能文档改进。由于当前性能相关信息分散在多个地方，缺乏整体性的概述，用户提出了希望改进性能文档的建议。

https://github.com/vllm-project/vllm/issues/9926
该issue属于文档更新类型，主要涉及文档中有关TPU更多详细信息的更新。由于原文档中缺少关于TPUs的详细信息和链接，导致用户提出需要更多关于TPUs的信息。

https://github.com/vllm-project/vllm/issues/9925
这个issue是关于提出需求的，主要涉及的对象是为项目创建一个漏洞管理团队。导致提出这个需求的原因是项目需要负责及时响应漏洞报告的团队。

https://github.com/vllm-project/vllm/issues/9924
这是一个关于文档相关操作的问题，主要涉及将`CONTRIBUTING.md`文件移到文档站点以建立新的`docs/source/contributing`目录。

https://github.com/vllm-project/vllm/issues/9923
这是一个bug报告，主要涉及Offline engine对比本地服务器在运行批处理时性能没有提升的问题。原因可能是Offline engine的批处理速度与本地服务器的批处理速度相比没有明显提升。

https://github.com/vllm-project/vllm/issues/9922
这是一个Bug报告，涉及的主要对象是vllm中的GPU Quadro RTX 4000。由于不支持该GPU类型的gamma系列（如`google/gemma227bit`），导致在vllm的离线批处理推断中出现数值错误。

https://github.com/vllm-project/vllm/issues/9921
这个issue是关于新增功能的需求，主要涉及支持PixtralHFTransformer的量化。原因是之前只能量化语言模型，导致无法量化视觉编码器。

https://github.com/vllm-project/vllm/issues/9920
这是一个bug报告，该问题涉及vllm在Ada A6000/48GB x2 GPUs上运行时出现的stalls现象。原因可能是无法在重新启动后再次运行代码，导致hangs在llm.generate()。

https://github.com/vllm-project/vllm/issues/9919
这是一个bug报告类型的issue，主要涉及Frontend中chat content format的自动检测，由于之前的命名和逻辑问题导致vllm serve在处理Vision LM models时表现不正确。

https://github.com/vllm-project/vllm/issues/9918
这是关于bug报告的issue，涉及主要对象是vllm项目中的prefix caching功能。由于设置`enable_prefix_caching=True`导致在使用8个H100 GPUs时发生非法内存访问错误。

https://github.com/vllm-project/vllm/issues/9917
该issue类型是bug报告，主要涉及的对象是MiniCPMV和Mllama BNB。原因可能是inflight量化错误导致问题出现。

https://github.com/vllm-project/vllm/issues/9916
这个issue属于用户提出需求类型， 主要对象是对SparseLLM/prosparse-llama-2-7b模型的支持。由于当前vllm无法正确运行SparseLLM/prosparse-llama-2-7b模型的架构，导致用户寻求对该新模型的支持。

https://github.com/vllm-project/vllm/issues/9915
这是一个关于需求提出的issue，主要涉及前端多模态支持加载本地图片文件。由于缺乏多模态支持，用户无法加载本地图片文件，故提出需求。

https://github.com/vllm-project/vllm/issues/9914
这是一个bug报告，主要涉及minicpmv2.6中的BNB in-flight quantization error。由于合并了一个PR后尝试进行in-flight quantization时出现了错误。

https://github.com/vllm-project/vllm/issues/9913
这是一个Bug报告，涉及环境配置和Marlin内核的更改，用户遇到了Deepseek v2 Lite运行时的错误。

https://github.com/vllm-project/vllm/issues/9912
这是一个需求提出的issue，主要涉及对象是VLM2Vec模型，用户在此提出需求是因为现有的chat模板无法很好地适配VLM2Vec模型，且VLM2Vec并不是用于聊天的模型。

https://github.com/vllm-project/vllm/issues/9911
该issue是一个需求报告，涉及的主要对象是更新torch至2.5版本。由于markdown渲染问题，无法正常显示所以使用了原始的html代码表示。

https://github.com/vllm-project/vllm/issues/9910
这个issue类型是bug报告，主要涉及到在版本0.6.3中，同时发出多个请求时，请求之间的token可能会相互影响，导致不同请求结果混乱。

https://github.com/vllm-project/vllm/issues/9909
这个issue类型为bug报告，主要涉及对象是在没有网络的环境中使用VLLM来服务本地模型。该问题可能由于缺少网络连接导致无法正常使用VLLM来提供本地模型服务。

https://github.com/vllm-project/vllm/issues/9908
这是一个Bug报告，涉及到Hermes tool parser出现了值为`None`未处理导致的问题。

https://github.com/vllm-project/vllm/issues/9907
这是一个Bug报告，主要涉及Mistral中的'SentencePieceTokenizer'对象缺少'id_to_byte_piece'属性。由于该属性缺失，导致了报错信息：Error: Before submitting a new issue...

https://github.com/vllm-project/vllm/issues/9906
这个issue是关于更新文档，涉及VLLM项目中多输入支持的更新。

https://github.com/vllm-project/vllm/issues/9905
这是一个用户提出需求的类型，主要涉及对象是想在vllm中使用Mixtral MoE模型时获取`router_logits`，用户询问如何实现此功能。

https://github.com/vllm-project/vllm/issues/9904
这个issue是一个需求提出类型，主要涉及增加Llama 3和CommandR Chat Templates至chat_templates仓库，可能是为了丰富Chat Templates的功能。

https://github.com/vllm-project/vllm/issues/9903
这个issue类型是需求提出，主要涉及对象是vllm开发中自动释放显卡内存的功能。用户希望通过添加代码实现vllm在不使用模型推理时自动释放显卡内存，以避免未来可能出现的OOM问题。

https://github.com/vllm-project/vllm/issues/9902
这是一个用户提出需求的issue，主要涉及的对象是vllm模型中的kv-cache。由于设置了max_tokens=1，导致模型仅在prefill阶段中运行，从而提出了关于kvcache计算和优化的问题。

https://github.com/vllm-project/vllm/issues/9901
这是一个bug报告。该问题单涉及的主要对象是在使用factory_llama工具以qlora的方式训练Qwen/Qwen2.5-1.5B-Instruct模型，然后以vllm加载lora的方式启动时报错。导致报错的原因是模型Qwen2ForCausalLM不支持BitsAndBytes的量化技术。

https://github.com/vllm-project/vllm/issues/9900
这个issue是一个bug报告，涉及的主要对象是vllm软件在0.6.3.post1版本下的`response_format`功能不正常工作。由于最新版本的服务器未编译FSM，而较早版本可以，导致了这个问题。

https://github.com/vllm-project/vllm/issues/9899
这是一个用户请求详细配置的类型的Issue，主要对象是VLLM的配置和硬件环境，用户想要获得关于在A100硬件上运行特定模型的最佳配置命令。

https://github.com/vllm-project/vllm/issues/9898
这是一个关于软件版本依赖问题的bug报告，主要涉及Dockerfile.tpu的依赖版本未固定导致与Trillium不兼容。

https://github.com/vllm-project/vllm/issues/9897
这是一个bug报告，涉及的主要对象是VLM（Very Large Language Model）。由于sentinel字符串错误地被替换为&lt;image&gt;，导致出现了10个更新的问题。

https://github.com/vllm-project/vllm/issues/9896
这是一个关于性能优化的讨论，不是bug报告或用户提出需求。

https://github.com/vllm-project/vllm/issues/9895
这是一个bug报告，涉及的主要对象是AWQ Marlin模块。该问题是由于AWQ Marlin模块未遵守modules_to_not_convert参数而导致的。

https://github.com/vllm-project/vllm/issues/9894
这是一个bug报告，涉及的主要对象是vLLM中的AWQ Marlin backend。由于AWQ Marlin backend未能正确处理`modules_to_not_convert`配置，导致特定模块被忽略导致加载模型时出现错误。

https://github.com/vllm-project/vllm/issues/9893
这是一个用户提出需求的issue，主要涉及到支持引导解码与多步解码的结合。原因可能是希望在使用多步解码的同时，仍然能够使用引导解码，以提高速度。

https://github.com/vllm-project/vllm/issues/9892
这是一个Bug修复类型的issue，主要涉及前端（frontend），解决了在多步骤模式下接收logits processors或guided decoding参数请求时导致服务器崩溃的问题。

https://github.com/vllm-project/vllm/issues/9891
这是一个bug报告，主要涉及到MiniCPMV的支持，由于权重超过了qweight加载，需要添加特定的参数以支持bitsandbytes加载格式。

https://github.com/vllm-project/vllm/issues/9890
这是一个bug报告，涉及到代码格式化导致YAPF检查失败的问题。

https://github.com/vllm-project/vllm/issues/9889
这是一个需求类型的issue，主要涉及的对象是torch.compile模块，用户希望能够使用PyTorch中稳定API的解释器，以避免过多地使用PyTorch的内部API。

https://github.com/vllm-project/vllm/issues/9888
这是一个关于优化v1版本可测试性的issue。

https://github.com/vllm-project/vllm/issues/9887
这个issue属于bug报告类型，主要涉及layer skip logic with bitsandbytes实现错误导致模块匹配错误。

https://github.com/vllm-project/vllm/issues/9886
这是一个特性添加的issue，涉及的主要对象是在gemms中重写和融合集体通信操作。由于新的inductor缓存机制导致死锁，需要修复benchmark serving模式中的死锁问题。

https://github.com/vllm-project/vllm/issues/9885
这是一个Bug报告类的issue，主要涉及到使用cython编译Python代码时出现的类型错误导致程序崩溃的问题。

https://github.com/vllm-project/vllm/issues/9884
这个issue属于用户提出需求类型，主要涉及项目中需要添加Github Action来构建和推送cpu-inference镜像，可能由于当前尚未为此功能添加相应的自动化操作导致用户发起该需求。

https://github.com/vllm-project/vllm/issues/9883
这是一个类型为Kernel且涉及在合作模型中添加一个电感器pass来重写和融合集体通信操作与gemms的issue。

https://github.com/vllm-project/vllm/issues/9882
这是一个缺少PR描述的issue，主要涉及的对象是vLLM代码库中的路径修复问题。这个issue由于markdown渲染问题，导致无法正确展示PR描述，需要填补PR描述信息以解决该问题。

https://github.com/vllm-project/vllm/issues/9881
这是一个功能需求类型的issue，主要涉及的对象是前端组件，用户提出需要添加`max_tokens`作为prometheus指标的功能。

https://github.com/vllm-project/vllm/issues/9880
这个issue属于用户提出需求类型，主要涉及vLLM v1中的logprobs和prompt logprobs支持，用户希望增加此功能来获得token的logprobs值、token ranks等信息。

https://github.com/vllm-project/vllm/issues/9879
这是一个bug报告，涉及到vllm在使用speculative decoding时hidden_states未正常工作的问题，导致用户无法获取正确的hidden_states数据。

https://github.com/vllm-project/vllm/issues/9878
这是一个Bug报告，涉及VllmWorkerProcess的异常。由于网络隔离，无法运行脚本，导致出现了'NoneType' object has no attribute 'get_len'的错误。

https://github.com/vllm-project/vllm/issues/9877
这个issue是关于提交PR前的检查清单（Pull Request Checklist）的，属于准备提交代码变更而非bug报告。原因可能是为了帮助维持代码质量和改进审查流程效率。

https://github.com/vllm-project/vllm/issues/9876
这是一个功能增强的issue，涉及到torch compile模块的注释和测试。

https://github.com/vllm-project/vllm/issues/9875
这是一个Bug报告类型的Issue，涉及vllm项目中在单个带有多个GPU的机器上运行时出现错误的情况。由于代码中的断言错误，导致用户无法成功运行评估脚本。

https://github.com/vllm-project/vllm/issues/9874
这是一个bug报告，主要涉及的对象是使用vLLM中的特定模型和功能。导致这个问题的原因是在使用Streaming功能时出现了错误，可能是由于属性错误导致。

https://github.com/vllm-project/vllm/issues/9873
此issue类型为用户提出需求，询问vllm是否有计划支持类似GPT-SoVITS语音生成大模型，未获得明确回复。

https://github.com/vllm-project/vllm/issues/9872
这是一个bug报告，主要涉及vllm版本0.6.3中使用qwen2_vl模型实例化可能引发警告或错误的问题。

https://github.com/vllm-project/vllm/issues/9871
这是一个关于需求提出的issue，主要涉及了VLMs支持细粒度调度的问题。由于V0版本未考虑到复杂依赖关系，无法灵活调度，因此提出了对V1版本进行改进的建议。

https://github.com/vllm-project/vllm/issues/9870
这是一个bug报告类型的issue，主要涉及LLaVA-v1.5-13B版本与OpenAI兼容API的错误。导致问题的原因可能是运行mmbench样本时出现的问题。

https://github.com/vllm-project/vllm/issues/9869
这个issue属于文档更新类型，涉及更新Qwen文档，由于PR描述部分未填写，需要完善文档并按照要求填写PR信息。

https://github.com/vllm-project/vllm/issues/9868
这是一个用户提出需求的issue，主要涉及支持一个新模型NV-Embed-v2。用户提出需求得不到响应，可能导致症状为功能不完善或无法使用新模型。

https://github.com/vllm-project/vllm/issues/9866
这是一个功能改进（feature enhancement）类型的issue，涉及到torch.compile相关的测试计划的重构。

https://github.com/vllm-project/vllm/issues/9865
这是一个bug报告类的issue，涉及的对象是vllm安装过程中出现的pynvml.NVMLError_InvalidArgument错误。这个问题可能是由于CUDA 12.4不受支持导致的。

https://github.com/vllm-project/vllm/issues/9864
这是一个标记为"[Misc] Remove deprecated arg for cuda graph capture"的issue，主要涉及到需要清理被标记为弃用但仍残留在代码中的`max_context_len_to_capture`参数。

https://github.com/vllm-project/vllm/issues/9863
这是一个性能优化提案，涉及主要对象是Qwen2-VL-7B AWQ模型的性能。出现该问题可能是由于性能回归导致推断时间未能得到有效改善。

https://github.com/vllm-project/vllm/issues/9862
这是一个需要修复的bug报告，涉及到V0版本的Prefix Cache Aware调度。根据内容来看，可能是因为与https://github.com/vllmproject/vllm/issues/7883相关的问题导致了某种bug或需求。

https://github.com/vllm-project/vllm/issues/9861
这个issue是一个bug报告，主要涉及的对象是Flashinfer k_scale和v_scale。由于存在FlashInfer与FP8模型一起使用时出现错误的问题，需要移除对k_scale和v_scale的断言。

https://github.com/vllm-project/vllm/issues/9860
这是一个升级相关的issue，涉及主要对象为PyTorch、Xformers和torchvision，由于构建环境中出现错误，作者尝试升级PyTorch版本到2.5.1 bugfix release来解决问题。

https://github.com/vllm-project/vllm/issues/9859
这是一个bug报告，涉及的主要对象是前端工具解析器。由于 Llama 3.2 和 ToolACE8B 输出的工具格式为 Python 函数调用，导致 Llama3.2 工具调用 OpenAI API 不起作用。

https://github.com/vllm-project/vllm/issues/9858
这是一个升级问题，主要涉及到torch.compile的测试，由于最近将PyTorch升级到2.5版本，现在应该可以完整运行测试。

https://github.com/vllm-project/vllm/issues/9857
这个issue类型为Kernel相关的问题，涉及主要对象为Triton implementation。这个issue是为了解决在运行SmoothQuant模型时只支持对称情况，没有零点调整的问题。

https://github.com/vllm-project/vllm/issues/9856
该issue类型为功能需求，主要涉及V1版本中的多进程张量并行支持，因为需要实现新特性并进行性能优化。

https://github.com/vllm-project/vllm/issues/9855
这是一个功能性需求的issue，主要涉及Machete核心支持W4A8量化和重构。原因可能是需要增强其性能和功能。

https://github.com/vllm-project/vllm/issues/9854
这个issue是用户提出需求，要求为flashinfer添加支持滑动窗口特性。

https://github.com/vllm-project/vllm/issues/9853
这是一个bug报告，该问题单涉及的主要对象是vllm项目中的pp功能（可能是pre-processing的缩写）。由于某种原因导致pp功能出现了问题，需要进行修复。

https://github.com/vllm-project/vllm/issues/9852
这是一个关于bug报告的issue，主要涉及的对象是代码库中的一个特定功能，由于multistep测试失败导致需要撤销之前的bug修复操作。

https://github.com/vllm-project/vllm/issues/9851
这是一个功能更新的issue，涉及更新benchmark_throughput.py以支持图像输入。

https://github.com/vllm-project/vllm/issues/9850
这是一个Bug报告，主要涉及的对象是Turing devices，在Triton上存在一个bug，导致Turing设备无法执行float32的matmul操作。

https://github.com/vllm-project/vllm/issues/9849
这个issue属于CI/Build类型，主要涉及的对象是在CI/Build中添加一个强制的docker system prune以清理空间。原因是为了保持vLLM代码质量并提高代码审查效率。

https://github.com/vllm-project/vllm/issues/9848
这是一个Bug报告，该问题涉及vllm serve在输入shape不匹配时产生数值错误的情况。

https://github.com/vllm-project/vllm/issues/9847
这个issue类型是用户提出需求，主要对象是要求支持新模型"BAAI/bgem3"。缺乏响应可能是由于该模型与最接近的已支持模型之间存在较大差异导致难以支持。

https://github.com/vllm-project/vllm/issues/9846
该issue是一个新功能添加类型的PR，主要涉及对Qwen2-VL模型进行测试的增加。由于长上下文问题，之前对qwen2vl模型测试遇到了问题，现在将`runner_mm_key`从测试`VLMTestInfo`中移到`CustomTestOptions`，并对VLM测试进行标准和扩展子集分割，以解决这些问题。

https://github.com/vllm-project/vllm/issues/9845
这是一个bug报告类型的issue，主要涉及的对象是vLLM项目下的chat completion请求。由于OpenAI客户端不再支持`max_tokens`字段，导致需要更新代码来移除对这一字段的支持。

https://github.com/vllm-project/vllm/issues/9844
这是一个用户提出需求的issue，主要涉及的对象是为IBM Granite模型添加Pipeline Parallelism支持。因为目前vllm仅支持一些特定的架构，未包括Granite模型，导致无法使用Pipeline Parallelism来扩展工作负载和保持性能。

https://github.com/vllm-project/vllm/issues/9843
这个issue类型是文档链接bug报告，涉及的主要对象是markdown渲染功能无法正常工作。原因可能是markdown渲染出现问题导致链接无法正常显示。

https://github.com/vllm-project/vllm/issues/9842
该issue类型是用户提出需求，主要对象是支持在线视频的VLMs。由于现有的VLMs只支持图像输入，用户提出了增加对视频输入的支持。

https://github.com/vllm-project/vllm/issues/9841
这是一个Bug报告，涉及的主要对象是VLLM模型的logprobs在多次运行中表现不一致。这个问题可能由于随机种子设置无效导致。

https://github.com/vllm-project/vllm/issues/9840
这是一个bug报告类型的issue，主要涉及vllm docker image中api_server.py无法识别--task embedding参数导致的错误。

https://github.com/vllm-project/vllm/issues/9839
这是一个 Bug 报告，主要涉及 vllm 引擎在迭代时出现超时导致错误的问题。

https://github.com/vllm-project/vllm/issues/9838
这个issue类型是bug报告，主要涉及的对象是"H100"，由于diff tolerance constraint过高导致bfloat16 test结果匹配失败。

https://github.com/vllm-project/vllm/issues/9837
这是一个Bug报告，涉及到OpenAI在chat completion endpoint中`max_tokens`字段被废弃，影响了使用新`max_completion_tokens`字段的库出现pydantic验证错误。

https://github.com/vllm-project/vllm/issues/9836
这是一个用户提出需求类型的issue，主要涉及 Jenkins CI 中添加多步调度场景测试功能。由于当前 Jenkins CI 尚未包含多步调度场景测试功能，用户希望添加该功能以进一步完善产品功能。

https://github.com/vllm-project/vllm/issues/9835
这是一个bug报告，涉及对象为vllm 0.6.3版本的代码。由于未正确提供SamplingParams或PoolingParams参数，导致在执行特定代码时出现错误信息。

https://github.com/vllm-project/vllm/issues/9834
这是一个Bug报告，涉及Sampling参数在进行推测性采样验证步骤时出现了固定问题。因环境中的PyTorch版本为2.4.0+cu121，可能导致了此Bug的产生。

https://github.com/vllm-project/vllm/issues/9833
这是一个bug报告，用户在使用vllm中的GLM4v multi_modal_data进行多轮对话时遇到了问题。

https://github.com/vllm-project/vllm/issues/9832
这个issue类型是bug报告，主要涉及的对象是使用自定义Qwen2VL GPTQ 4bit模型进行推理时出现数值异常导致ValueError错误。

https://github.com/vllm-project/vllm/issues/9831
该issue类型为提出需求，主要涉及vllm wheels的安装方式和发布流程优化。导致该需求的原因是当前安装vllm wheels的方式不够简洁和方便，用户希望通过更直接的方式来安装vllm wheels。

https://github.com/vllm-project/vllm/issues/9830
这个issue属于用户提出需求类型，主要对象为支持torch 2.5.1的公共二进制发布。由于torch发布了新版本，用户需求支持这一版本的二进制发布。

https://github.com/vllm-project/vllm/issues/9829
这是一个关于性能优化的issue，主要涉及到两处优化方案和其效果评估。原因是通过优化代码实现中的不必要操作，提高性能。

https://github.com/vllm-project/vllm/issues/9828
这是一个 bug 报告，主要涉及 pip 安装问题，由于官方 pip 包 v0.6.3 存在问题，需安装特定的 wheel 包才能解决。

https://github.com/vllm-project/vllm/issues/9827
这个issue类型是需求提出，涉及的主要对象是软件依赖库pynvml。可能是由于缺少对pynvml的最低版本要求导致的需求提出。

https://github.com/vllm-project/vllm/issues/9826
这个issue类型为功能改进，涉及的主要对象是`AsyncLLM`实现。原因导致需要改进是为了更好地利用GPU和CPU的重叠进行任务处理。

https://github.com/vllm-project/vllm/issues/9825
这是一个Bug报告，涉及的主要对象是vLLM中的Qwen系列模型。由于v0.6.3.post1版本中生成长文本时出现乱码，而在先前版本正常工作，用户提出了这一问题。

https://github.com/vllm-project/vllm/issues/9824
这是一个bug报告类型的issue，主要涉及vllm在关闭时出现崩溃导致无法重启的问题。由于torch中的bug导致vllm无法正常关闭从而引发了该问题。

https://github.com/vllm-project/vllm/issues/9823
This issue is a request to upgrade the Pytorch version to 2.5 for the xpu backend in the vLLM project.

https://github.com/vllm-project/vllm/issues/9822
这个issue是关于修复vLLM中Neuron模块构建失败的bug报告，主要涉及Dockerfile.neuron的更新。这个bug可能是由于Dockerfile.neuron的配置不正确导致构建失败。

https://github.com/vllm-project/vllm/issues/9821
这是一个Bug报告，涉及到在加载mistralai/Mixtral-8x22B-Instruct-v0.1时出现的TypeError错误。原因是需要传入一个字节类型的对象而传入了字符串类型。

https://github.com/vllm-project/vllm/issues/9820
这个issue类型是用户需要提供新的fp8 moe配置信息，主要涉及到Kernel中的Mixtral8x7B和8x22B，需求是为MI300X提供TP=1,2,4,8的配置。

https://github.com/vllm-project/vllm/issues/9819
这是一个bug报告，主要涉及的对象是README.md文件。由于拼写错误导致了问题的出现。

https://github.com/vllm-project/vllm/issues/9818
这是一个关于bug修复的issue，主要涉及到VLLM项目中的MQLLMEngine，问题出现在长时间运行后无法发送心跳到客户端导致整个应用程序崩溃。

https://github.com/vllm-project/vllm/issues/9817
这是一个用户提出需求的issue，主要涉及的对象是Qwen2VisionTransformer for Qwen2-VL，由于之前只能对language model进行量化，用户希望能够支持对vision encoder的量化。

https://github.com/vllm-project/vllm/issues/9816
这是一个关于模型checkpoints格式兼容性的问题，主要涉及的对象是Vllm和Eagle模型。由于Vllm的实现要求checkpoint严格遵循特定格式，导致与通用HF（Hugging Face）checkpoints不兼容，需要转换脚本将其转换为Vllm格式。

https://github.com/vllm-project/vllm/issues/9815
这是一个bug报告，主要涉及到pytorch中的自定义allreduce，在处理ipc句柄二进制格式变化时出现了问题。

https://github.com/vllm-project/vllm/issues/9814
该issue属于文档类型，用户提出需求，希望添加关于Snowflake Meetup的相关注释。

https://github.com/vllm-project/vllm/issues/9813
这个issue类型为功能增强，主要涉及了PixtralHF模型的模型测试。原因是开发人员希望为PixtralHF模型添加一个简单的vLLM vs HF测试。

https://github.com/vllm-project/vllm/issues/9812
这是一个Bugfix类型的issue，涉及的主要对象是VLM中的apply_fp8_linear函数。这个问题由于需要与VLM的量化视觉编码器一起使用，需要对apply_fp8_linear函数进行修复以处理大于2D输入。

https://github.com/vllm-project/vllm/issues/9811
这是一个需求类型的issue，主要涉及依赖管理工具的配置。

https://github.com/vllm-project/vllm/issues/9810
这个issue是关于CI构建依赖版本固定的问题，属于优化需求类别，涉及到CI配置和依赖管理。原因是由于最近CI依赖的新版本引起的构建失败，需要将所有依赖版本固定并定期更新。

https://github.com/vllm-project/vllm/issues/9809
这是一个bug报告，主要涉及的对象是Dockerfile.rocm的安装过程。由于torch nightly build版本不再存在，导致无法完成安装。

https://github.com/vllm-project/vllm/issues/9808
这是一个Bug报告，涉及对象是Chameleon测试在Transformers 4.46.1上的失败。由于Chameleon测试仅特别针对4.46.0进行检查，而新发布的4.46.1未解决该问题，导致CI测试失败。

https://github.com/vllm-project/vllm/issues/9807
这是一个需求提出类型的issue，主要涉及的对象是vllm，由于需要将"Writing in the Margins"算法集成到系统中，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/9806
此issue为新增功能请求，涉及的主要对象是为vLLM添加`LlamaEmbeddingModel`作为`LlamaModel`的嵌入实现。由于公司政策原因，该功能的细节暂时无法公开，但希望能够在vLLM中使用该模型，并最终公开使用。

https://github.com/vllm-project/vllm/issues/9805
这个issue是一个功能需求，主要涉及的对象是vLLM软件中的limitmmperprompt功能。由于现有的逻辑过于保守，导致无法同时支持10个小图像和更大的图像。

https://github.com/vllm-project/vllm/issues/9804
这个issue是关于bug报告，涉及的主要对象是CI/Build中的mergify规则，由于规则应用错误导致标签未正确应用。

https://github.com/vllm-project/vllm/issues/9803
这个issue是在请求文档更新，主要对象是项目贡献者；由于需要在PR中添加`Signedoffby`头部作为DCO的确认，且文档需加入DCO信息和许可证链接。

https://github.com/vllm-project/vllm/issues/9802
这是一个bug报告类型的issue，主要涉及Llama 3.1 8b模型在使用多个GPU时并发度不如预期，导致任务无法充分并行执行。

https://github.com/vllm-project/vllm/issues/9801
这是一个关于修正拼写错误的 issue，主要涉及类名的重命名操作，原因是之前的类名拼写有误导致混淆。

https://github.com/vllm-project/vllm/issues/9800
这是一个Bugfix类型的issue，主要涉及到vllm模型中对非量化视觉层的支持问题。由于缺少存在于模型中的缩放因子，导致部分量化的模型无法正常运行。

https://github.com/vllm-project/vllm/issues/9799
这是一个bug报告，主要涉及的对象是vllm下的类名`NeuronCasualLM`和`OpenVINOCasualLM`应该改为`NeuronCausalLM`和`OpenVINOCausalLM`，由于拼写错误导致了问题。

https://github.com/vllm-project/vllm/issues/9798
这是一个bug报告，主要涉及的对象是`host`参数。由于默认值为`0.0.0.0`可能导致严重的服务配置错误，用户建议将默认值更改为`127.0.0.1`。

https://github.com/vllm-project/vllm/issues/9797
这个issue是一个Bug报告，主要涉及vllm的`--host`参数在绑定接口时无效的问题，可能由于代码逻辑错误导致该bug。

https://github.com/vllm-project/vllm/issues/9796
这是一个性能问题报告，涉及对象是使用vLLM引擎时A800 GPU推理速度不如RTX3090的问题。原因可能是与GPU A800性能利用不当有关。

https://github.com/vllm-project/vllm/issues/9795
该issue属于用户提出需求类型，主要涉及的对象是在Intel x86 MacBook Pro 上运行Phi3.5。由于用户希望在特定环境中运行Phi3.5，可能由于硬件或软件配置不匹配而导致无法顺利运行，需要寻求相关支持或解决方案。

https://github.com/vllm-project/vllm/issues/9794
这是一个Bug报告，涉及的主要对象是`DummyLoRAManager().init_random_lora`，由于Lora weight被放在了错误的设备上，导致了测试失败。

https://github.com/vllm-project/vllm/issues/9793
这个issue是一个bug报告，主要涉及修正日志以正确指导用户安装modelscope。原因是Markdown渲染无法工作，因此在此处使用原始HTML。

https://github.com/vllm-project/vllm/issues/9792
这是一个bug报告类型的issue，主要涉及的对象是vLLM中的markdown渲染问题。这个问题出现的原因是markdown无法正常渲染，所以使用了原始的html。

https://github.com/vllm-project/vllm/issues/9791
这是一个Bug报告，问题涉及到启动`vllm serve microsoft/Phi-3-small-8k-instruct`时出现了错误，用户在使用Tesla V100时可以成功启动llama3模型，但无法启动phi3模型，希望得到解决此问题的帮助。

https://github.com/vllm-project/vllm/issues/9790
这个issue是关于需求的，主要涉及vllm中对于多模态模型的前缀缓存支持问题。原因是当前不支持启用前缀缓存功能，用户在尝试进行离线批量推理时遇到了问题。

https://github.com/vllm-project/vllm/issues/9789
这个issue类型是用户提出需求，问题或寻求帮助，主要对象是关于Qwen2.5工具选择auto支持的问题。由于没有收到响应，用户提出了与该功能相关的疑问。

https://github.com/vllm-project/vllm/issues/9788
这是一个性能问题报告，主要涉及到sampler在推断过程中所占的时间比例过高，导致推断时间较长。

https://github.com/vllm-project/vllm/issues/9787
这是一个改进代码质量的issue，主要涉及到api server测试中异常跟踪的简化。

https://github.com/vllm-project/vllm/issues/9786
这是一个Bug报告，涉及的主要对象是vllm模块调用出现的错误。导致这个问题的原因是`import vllm`语句引发了 `ModuleNotFoundError: No module named 'openai.types'` 的错误。

https://github.com/vllm-project/vllm/issues/9785
这是一个关于提交PR的Bug报告，主要涉及硬件相关的改动。原因是在RFC的section中，Markdown渲染无法正常工作，因此需要使用原始HTML代码。

https://github.com/vllm-project/vllm/issues/9784
这是一个请求帮助的issue，涉及主要对象是vllm中返回logits的问题。由于连接丢失导致无法返回输出值给RequestOutput打印。

https://github.com/vllm-project/vllm/issues/9783
这是一个Bug报告类型的issue，主要涉及VLLM在TPU上进行性能分析时出现的错误。原因可能是配置或代码中的问题导致了无法正确生成性能分析数据。

https://github.com/vllm-project/vllm/issues/9782
这是一个关于提议改进`LLMEngineCore`的issue，涉及到代码重复、多进程处理和API设计方面的讨论。

https://github.com/vllm-project/vllm/issues/9781
该issue属于功能增强类型，涉及的主要对象是FlashInfer实现。原因是为了在cascade inference稳定之前在当前的FlashInfer实现中添加chunked-prefill支持。

https://github.com/vllm-project/vllm/issues/9780
这个issue是一个用户需求提出，主要对象是VLLM前端，用户建议添加用户队列功能来进行离线推理。

https://github.com/vllm-project/vllm/issues/9779
这个issue是关于代码重构的，涉及主要对象是benchmark_throughput.py文件，旨在通过使用命名类（而不是元组）来更好地表示数据样本，以提高代码可读性，特别是在添加更多字段到数据样本时。

https://github.com/vllm-project/vllm/issues/9778
这个issue类型是用户提出需求，主要涉及的对象是实现imagemodal模型的吞吐量基准测试，由于该功能较大，需要跨多个PR进行实现。

https://github.com/vllm-project/vllm/issues/9777
这个issue类型为功能需求提议，涉及主要对象为metrics polling，由于需要更有效地追踪等待适配器和活跃适配器的最近历史，从而避免过于频繁的指标轮询。

https://github.com/vllm-project/vllm/issues/9776
这是一个空白内容的issue，无法确定是bug报告还是其它类型，也无法确定主要涉及的对象。

https://github.com/vllm-project/vllm/issues/9775
该issue为需求添加新功能，主要对象为torch.compile。原因是用户希望添加名为"deepseek v2"的编译功能并进行本地测试。

https://github.com/vllm-project/vllm/issues/9774
这个issue是关于bug报告，主要涉及MoE（Mixture of Experts）模型的custom allreduce功能，由于调用`vllm serve allenai/OLMoE1B7B0924 tp 2`命令时出现了Cuda error，导致程序失败。

https://github.com/vllm-project/vllm/issues/9772
这个issue是一个Bug修复类型的 issue，涉及的主要对象是VLMs下的prefix strings构建问题。导致这个bug的原因是由于prefix strings未被正确构建，特别是由于使用了submodels，导致quant_config对于被忽略的modules启用，从而在权重加载期间导致失败。

https://github.com/vllm-project/vllm/issues/9771
这是一个bug报告，主要涉及的对象是使用第三方模型的代码示例，由于没有在`spawn`多进程方法中添加`__main__`保护导致递归运行`runpy.run_module()`，最终导致`OSError: [Errno 98] Address already in use` bug。

https://github.com/vllm-project/vllm/issues/9770
这是一个Bug报告，主要涉及VLLM版本6.3下的模型使用问题，由于规格解码、张量并行和草案并行大小等因素可能导致部分请求输出乱码。

https://github.com/vllm-project/vllm/issues/9769
这是一个bug报告，主要涉及vllm 0.6.3.post1版本在处理长输入时出现无意义输出的问题。

https://github.com/vllm-project/vllm/issues/9768
这是一个关于性能基准测试（benchmark）的问题单，主要对象是针对"H200 development"。因为提供了一个可工作的基准测试链接，可能是用于评估新功能性能或优化瓶颈。

https://github.com/vllm-project/vllm/issues/9767
这是一个功能新增的issue，主要涉及的对象是添加对Idefics3的支持，原因是根据讨论需要实现这一功能。

https://github.com/vllm-project/vllm/issues/9766
这是一个功能需求的issue，主要涉及的对象是模型量化和Marlin内核扩展。

https://github.com/vllm-project/vllm/issues/9765
这是一个bug报告，涉及到循环导入。可能由于模块间相互引用而导致了此问题。

https://github.com/vllm-project/vllm/issues/9764
这是一个用户提出需求的issue，主要涉及支持HQ support以及float16 kernels，可能由于某种原因导致需要对float zero points进行调整。

https://github.com/vllm-project/vllm/issues/9763
这是一个bug报告，涉及环境中vllm版本为0.6.3，由于端口被占用导致出现`OSError: [Errno 98] Address already in use`错误。

https://github.com/vllm-project/vllm/issues/9762
这个issue是一个PR请求，类型为模型（Model），主要涉及添加工具解析器用于openbmb/MiniCPM34B。这个问题出现的原因可能是缺少对应的工具解析器，需要对vLLM进行改进。

https://github.com/vllm-project/vllm/issues/9761
这是一个Bug报告。该问题涉及的主要对象是VLLM项目中的Qwen2.5模型。由于在配置文件中未注册该模型的嵌入任务，导致此问题出现。

https://github.com/vllm-project/vllm/issues/9760
这是一个用户提出需求的 issue，主要涉及使用 vLLM 进行最大批量处理的情况。用户希望了解如何设置参数以实现最大吞吐量。

https://github.com/vllm-project/vllm/issues/9759
该issue为功能需求提案，涉及将 chat 对话作为输入进行处理。

https://github.com/vllm-project/vllm/issues/9758
这个issue属于功能增强类型，涉及主要对象为torch编译系统，旨在为一些moe模型添加"torch compile"注释并测试其编译是否通过。

https://github.com/vllm-project/vllm/issues/9757
这是一个bug报告，涉及对象为在多线程环境下如何正确使用AsyncLLMEngine，由于尝试在多线程环境下使用同一个`self.background_loop`导致第二个请求阻塞。

https://github.com/vllm-project/vllm/issues/9756
这是一个bug报告，涉及的主要对象是qwen2 reward model loading。由于某种原因导致加载该模型时出现问题，需要修复。

https://github.com/vllm-project/vllm/issues/9755
这是一个Bug报告，涉及主要对象是加载qwen2.5-math-rm-72b模型时遇到异常。由于多进程引擎初始化过程中出现了异常，导致加载模型时出现了错误。

https://github.com/vllm-project/vllm/issues/9754
这是一个bug报告，涉及vLLM与MetaLlama3.18BInstruct工具调用交互的问题。由于调用工具后无法正确响应内容，导致输出结果不符合预期。

https://github.com/vllm-project/vllm/issues/9753
这是一个bug报告类型的issue，主要涉及对象是无法在A10 GPU上加载Llama 3.2B模型，导致出现无法mmap字节文件的内存分配错误。

https://github.com/vllm-project/vllm/issues/9752
这是一个Bug报告，主要涉及Llama-3.2-11B-Vision-Instruct模型推理过程中无法提前停止的问题，可能是由于`stop_token_ids`设置不正确导致输出持续到最大token而不提前停止。

https://github.com/vllm-project/vllm/issues/9750
这是一个用户提出需求的issue，主要涉及如何获取`vLLM`模型的交叉熵损失，由于用户认为目前的方法不够便捷，希望能够像`transformers`一样直接获取交叉熵。

https://github.com/vllm-project/vllm/issues/9749
这是一个关于提交PR描述问题的issue，主要涉及vLLM项目中的PR提交流程和相关的问题。

https://github.com/vllm-project/vllm/issues/9748
这是一个用户提出需求的类型，主要涉及的对象是XPU设备，由于使用gather操作产生了额外的cat操作，通过使用allreduce来替换gather操作，可以提升性能。

https://github.com/vllm-project/vllm/issues/9747
这个issue类型为提议新的Model支持，涉及的主要对象是H2OVL-Mississippi系列模型。由于该模型尚未在vllm中得到支持，用户提出了添加对其的支持。

https://github.com/vllm-project/vllm/issues/9746
这是一个关于更新操作/checkout版本的issue，类型为功能更新，涉及的主要对象是vllm。由于更新导致的bug或问题不明确。

https://github.com/vllm-project/vllm/issues/9745
这是一个Bug报告类型的issue，主要涉及的对象是actions/setup-python更新操作。由于升级版本时出现了一些bug，如正则化行尾以确保跨平台一致性等，导致了一些症状的bug。

https://github.com/vllm-project/vllm/issues/9744
这是一个功能需求类型的issue，主要涉及到支持在`load_weights`后清空kvcache的API功能。由于当前vllm实现中，加载新权重后未更新缓存，导致新模型使用旧缓存前缀。

https://github.com/vllm-project/vllm/issues/9743
这是一个用户请教问题的issue，主要涉及vllm的使用和整合，问题来源于用户想要对特定模型进行推断但不清楚如何与vllm集成。

https://github.com/vllm-project/vllm/issues/9742
这是一个功能需求的issue，主要涉及的对象是vLLM的vision API endpoint，用户提出需要通过提供绝对路径来供应图片，以降低图片处理时的延迟。

https://github.com/vllm-project/vllm/issues/9741
这是一个关于代码改进的issue，涉及到Refactor LLMEngine To Use Multiprocessing。由于引入Multiprocessing，需要添加`AsyncLLMEngine`和更新`LLM`以在这种上下文中更好地工作。

https://github.com/vllm-project/vllm/issues/9740
这个issue类型是改进提议，主要对象是flash attention，由于之前的工作需要进行重构。

https://github.com/vllm-project/vllm/issues/9739
这是一个Bug报告，涉及的主要对象是VLLM模型。由于请求中提供了超过1张图片，在应用chat模板时出现数值错误导致报错。

https://github.com/vllm-project/vllm/issues/9738
这是一个Bug报告，主要涉及离线推断失败多节点环境下的问题。原因可能是由于离线推断过程中的某些限制或错误导致此问题。

https://github.com/vllm-project/vllm/issues/9737
这是一个Bug报告类型的issue，主要涉及vllm服务器在崩溃后端口被占用导致无法立即重启的问题。

https://github.com/vllm-project/vllm/issues/9736
这是一个Bug报告，涉及对象为vllm中的MistralModel，由于其不支持mistralai/Mistral7BInstructv0.3模型导致。

https://github.com/vllm-project/vllm/issues/9735
这是一个bug报告，主要涉及到NVIDIA Jetson设备不支持NVML协议。产生这个问题的原因是Jetson设备无法初始化NVML，导致Jetson设备上的CUDA功能无法正常使用。

https://github.com/vllm-project/vllm/issues/9734
这是一个bug报告，涉及的主要对象是github上的vllm项目。因为缓存管理错误导致工作流无法更新其缓存，只能从第一个缓存开始，产生错误403 Resource not accessible by integration。

https://github.com/vllm-project/vllm/issues/9733
这是一个关于使用问题的issue，用户想要在vllm上运行特定模型的推理，但不清楚如何集成，可能由于缺乏相关的指导或文档而导致。

https://github.com/vllm-project/vllm/issues/9732
这是一个Bug报告，涉及到使用OpenAI API推理时出现的输出不一致问题，具体表现为当附加图像时，Qwen2VL产生了不连贯的输出。

https://github.com/vllm-project/vllm/issues/9731
这是一个Bug报告，涉及vllm库中的tensor parallelism 在多节点上的使用问题，用户遇到了无法在新版本中使用16个节点的tensor_parallelism的情况。

https://github.com/vllm-project/vllm/issues/9730
这是一个Bug报告，主要涉及了模型输出中带有额外标记符号时，在处理多个speculative tokens时可能导致KV cache错误，造成token ID不一致的问题。

https://github.com/vllm-project/vllm/issues/9729
这是一个Bug报告，主要涉及的对象是Bfloat16或Half与HF float16/bfloat16结果不兼容。原因可能是代码中的数据类型不匹配导致的。

https://github.com/vllm-project/vllm/issues/9728
这是一个bug报告，主要涉及vLLM在Jetson设备上无法启动engine process的问题，根本原因是新版本的vLLM使用了不支持Jetson设备的NVML来检测CUDA支持。

https://github.com/vllm-project/vllm/issues/9727
这是一个bug报告，主要涉及vllm.LLM库的分布式推断问题，可能由于torch.distributed错误导致无法正确重新初始化多个模型。

https://github.com/vllm-project/vllm/issues/9726
这个issue是一个文档类bug报告，主要涉及到VLLM项目中异步引擎参数的文档规范。由于文档中未明确指定异步引擎参数，用户提出了此issue来指出文档需要更新的问题。

https://github.com/vllm-project/vllm/issues/9725
这个issue是关于提出需求的，并涉及到了`Detokenizer`的完全异步化。这个问题由于当前的实现方式导致了`Detokenizer`可能会滞后，需要维护请求滞后步数，并存在消息传递复杂性和性能开销的问题。

https://github.com/vllm-project/vllm/issues/9724
这是一个功能需求提问，主要对象是`cudagraph`。由于无法在普通的PyTorch中共享输出缓冲区，因此用户提出了使用`tensor weak reference`来实现共享输出缓冲区的功能需求。

https://github.com/vllm-project/vllm/issues/9723
这是一个Bug报告，涉及到vLLM的离线推理单视频相关的问题。用户描述在使用视频进行推理时会产生不连贯的生成输出，而在提供文本或文本+图像时则正常工作，用户寻求关于此问题的帮助。

https://github.com/vllm-project/vllm/issues/9722
这是一个性能改进的议题，提出在并发情况下如何提高性能。

https://github.com/vllm-project/vllm/issues/9721
这个issue类型是bug报告，主要对象是vllm的registry系统。由于没有使用临时目录，导致出现了bug或功能异常。

https://github.com/vllm-project/vllm/issues/9720
这个issue类型是对模型的改进，主要涉及对象是为Mllama添加BNB量化支持，由于当前MllamaForConditionalGeneration模型尚不支持BitsAndBytes量化，故用户提出这一需求。

https://github.com/vllm-project/vllm/issues/9719
这个issue类型是功能需求提议，主要对象是SpecDecodeWorker类。原因是为了支持性能分析而改进了SpecDecodeWorker类中的功能。

https://github.com/vllm-project/vllm/issues/9718
这个issue类型是bug报告，涉及的主要对象是OPTModel架构，由于模型架构不支持从本地加载导致了数值错误。

https://github.com/vllm-project/vllm/issues/9717
这是一个bug报告，涉及的主要对象是Frontend中的SamplingParams对象。由于SamplingParams无法支持bad_words_ids参数，导致生成时出现了问题。

https://github.com/vllm-project/vllm/issues/9716
这是一个Bug报告，涉及到vLLM项目中的Markdown渲染问题，可能是由于Markdown无法正常解析导致无法显示的原因。

https://github.com/vllm-project/vllm/issues/9715
该issue属于用户需求类型，主要涉及VLLM的编译控制以及自定义操作的细粒度控制。由于需要重新设计编译控制，用户提出通过`VLLM_CUSTOM_OPS` 环境变量进行对自定义操作的精细控制。

https://github.com/vllm-project/vllm/issues/9714
这是一个Bug报告，涉及对象是vllm下的Llama3.290BVisionInstructbnb4bit模型，由于该模型不支持BitsAndBytes量化导致了AttributeError错误。

https://github.com/vllm-project/vllm/issues/9713
这是一个bug报告，主要涉及vllm库中的特定模型架构'LlamaForCausalLM'在当前环境中不受支持，导致数值错误。

https://github.com/vllm-project/vllm/issues/9712
这个issue是一个bug报告类型，主要涉及的对象是vllm中的模型加载过程出现了数值错误，导致了模型架构'LLamaForCausalLM'不受支持的问题。

https://github.com/vllm-project/vllm/issues/9711
这个issue是关于获取平均提示标记长度和输出标记长度的用法，涉及主要对象为模型的提示标记和输出标记。由于环境中的错误信息导致部分参数无法正常获取，可能会影响到统计平均长度的准确性。

https://github.com/vllm-project/vllm/issues/9710
这是一个Kernel类型的issue，涉及vllm的Triton部分，主要解决了ScaledQuant模型的支持问题，由于缺少scaled_mm_triton() kernel，导致只有Phi3medium128kinstructquantized.w8a8能正常运行，需要进一步测试和优化支持int8 SmoothQuant。

https://github.com/vllm-project/vllm/issues/9709
这个issue是关于bug报告，涉及到代码中的参数设置问题。导致该bug的原因是stream_options.include_usage未正确设置。

https://github.com/vllm-project/vllm/issues/9708
这是一个Bug报告，主要涉及的对象是`AsyncLLMEngine`的`EngineArgs`参数。这个问题是由于在实例化`AsyncLLMEngine`时，需要`EngineArgs`中不存在的参数`disable_log_requests`导致的。

https://github.com/vllm-project/vllm/issues/9707
该issue类型为用户提出需求，主要涉及对象是新增模型Tarsier，并希望支持该模型。原因可能是希望扩展现有模型支持范围，以提高模型可用性。

https://github.com/vllm-project/vllm/issues/9706
这是一个Bug报告，主要涉及lm_eval_harness在使用vllm作为后端评估Llama 3.1 8B Instruct时，启用/禁用chunked_prefill出现不一致评估结果的问题。由于启用/禁用chunked_prefill导致结果差异，用户怀疑可能存在某些功能异常。

https://github.com/vllm-project/vllm/issues/9705
这是一个关于更新README.md的issue，主要类型是文档更新，涉及的主要对象是代码仓库vLLM。原因是markdown渲染不起作用，导致需要使用原始HTML代码。

https://github.com/vllm-project/vllm/issues/9704
这是一个[Model]类型的issue，涉及添加一个名为Qwen2ForSequenceClassification的分类任务。

https://github.com/vllm-project/vllm/issues/9702
这是一个关于需求的问题，用户想讨论关于vllm模型在推断和嵌入中的使用。由于用户想要了解是否可以同时在同一VLLM实例中使用同一个模型进行推断和嵌入，可能是由于对该功能实现的兴趣或需求导致。

https://github.com/vllm-project/vllm/issues/9701
这是一个关于安装问题的bug报告，用户提到pip安装vllm会强制重新安装CPU版本的torch并替换CUDA torch，导致torch版本发生变化。

https://github.com/vllm-project/vllm/issues/9700
这个issue类型是CI测试，主要对象是持续集成系统。由于可能存在与持续集成相关的问题，导致需要进行测试并确认不合并代码。

https://github.com/vllm-project/vllm/issues/9699
这是一篇关于性能优化的issue，讨论了NVLink Sharp对系统性能的影响。

https://github.com/vllm-project/vllm/issues/9698
这是一个用户提出需求的issue，主要对象是vllm api server的功能，用户希望能够使用多个秘钥来监控和管理多个用户的访问权限。

https://github.com/vllm-project/vllm/issues/9697
这个issue类型是功能需求，主要涉及模型的支持扩展，用户请求支持新的嵌入模型。

https://github.com/vllm-project/vllm/issues/9696
这个issue类型是代码重构，主要涉及对象是Scheduler._preempt函数中的unused argument preemption_mode。这个问题的产生是因为参数preemption_mode被声明但未被使用，需要进行清理以提高代码质量。

https://github.com/vllm-project/vllm/issues/9695
这是关于功能使用问题的用户提出的需求。该问题涉及到vLLM模型的使用和数据处理流程，可能是由于代码中导入模块错误而导致的问题。

https://github.com/vllm-project/vllm/issues/9694
这是一个 bug 报告，涉及 vllm 在 ray 上运行分布式推理时出现 GetTimeoutError 的问题。

https://github.com/vllm-project/vllm/issues/9693
这是一个Bug报告，涉及VLLM库中使用streaming选项时返回的参数为None的问题。

https://github.com/vllm-project/vllm/issues/9692
这个issue类型为用户提出需求，主要涉及使用vllm的langchain工具调用问题，用户寻求关于如何使用langchain进行vllm serve工具调用的帮助。

https://github.com/vllm-project/vllm/issues/9689
这是一个bug报告，涉及的主要对象是使用`lora`和`num_scheduler_steps=8`时，输出结果不符合预期。导致这个问题的原因可能是设置`num_scheduler_steps`时出现异常。

https://github.com/vllm-project/vllm/issues/9688
这是一个Bug报告，涉及Lora模块在使用`num_scheduler_steps=8`参数时加载失败的问题。

https://github.com/vllm-project/vllm/issues/9687
这是一个Bug报告，涉及的主要对象是`benchmark_prefix_caching.py`脚本。这个问题是因为当前脚本生成的输入句子重复了而非生成不同的输入句子。

https://github.com/vllm-project/vllm/issues/9686
这是一个增加功能需求的issue，主要涉及vLLM在初始化过程中启动时间较长的问题，用户提出了添加一个max_batch_size_to_capture标志来减少初始化时间的建议。

https://github.com/vllm-project/vllm/issues/9685
这是一个bug报告类型的Issue，主要涉及GGUF模型新增支持的问题，其症状是部分模型配置提取失败或加载问题。

https://github.com/vllm-project/vllm/issues/9684
这个issue类型是用户提出需求，询问问题，该问题单涉及的主要对象是如何提高多卡推断的吞吐量，由于增加显卡资源没有显著改善Vincent模型推断的吞吐量，用户想知道如何改进这个指标。

https://github.com/vllm-project/vllm/issues/9683
这个issue是关于bug报告，主要涉及的对象是VLLM项目。由于使用了过时的命令`setup.py`和`easy_install`，导致出现了"gettid" was not declared错误。

https://github.com/vllm-project/vllm/issues/9682
这是一个关于在vLLM中支持将KV缓存卸载到CPU的问题，主要包含CPU KV缓存卸载的最小实现，该问题的类型是功能改进，主要涉及vLLM的核心模块，因为GPU空间不足以缓存所有文档时CPU KV缓存更佳。

https://github.com/vllm-project/vllm/issues/9681
这是一个Bug报告issue，主要涉及的对象是在使用TP为1时出现worker超时问题，导致无法执行map_batches。

https://github.com/vllm-project/vllm/issues/9680
这个issue是用户提出需求，主要涉及LLM模型性能优化，用户寻求高吞吐量的benchmark和指引。

https://github.com/vllm-project/vllm/issues/9679
这是一个用户提出需求的issue，涉及的主要对象是V1模型。这个问题是由于需要支持在flash attention backend上使用滑动窗口attention而提出的需求。

https://github.com/vllm-project/vllm/issues/9678
这是一个Bug报告类型的Issue，主要涉及的对象是在导入evaluate之前的vllm。导致此问题的原因是由于插件已被注册过一次，再次注册插件导致异常现象。

https://github.com/vllm-project/vllm/issues/9677
这是一个Bug报告，主要涉及MoE模型的权重加载测试失败，可能导致了程序错误或者异常行为。

https://github.com/vllm-project/vllm/issues/9676
这是一个bug报告，涉及主要对象为Chameleon test和transformers v4.46.0，由于transformers v4.46.0与模型不兼容，导致Chameleon test在`test_broadcast`中暂时被跳过。

https://github.com/vllm-project/vllm/issues/9675
这个issue是一个bug报告，主要涉及的对象是在transformers 4.46.0版本的broadcast测试中跳过了Chameleon功能。由于某种原因，导致Chameleon被跳过，可能会导致测试结果不准确或期望外的行为。

https://github.com/vllm-project/vllm/issues/9674
这是一个用户提出需求的类型，主要涉及的对象是Navi4x平台的Llama模型系列，用户要求为Llama模型系列添加FP8支持，可能是为了解决兼容性或功能性方面的问题。

https://github.com/vllm-project/vllm/issues/9673
这个issue是一个Model类型的问题，主要涉及的对象是添加一个lora模块到granite 3.0 MoE模型，可能是因为缺少一个`layer`模块导致了问题。

https://github.com/vllm-project/vllm/issues/9672
这是一个用户提出需求的issue，主要涉及的对象是添加XQA kernel用于masked multihead attention。

https://github.com/vllm-project/vllm/issues/9671
这是一个bug报告类型的issue，涉及主要对象为发布版本的命名问题。这个问题由于之前的文件重命名行为不正确，导致发布版本的命名错误，需要对脚本进行审查并修复。

https://github.com/vllm-project/vllm/issues/9670
这是一个Bug报告，涉及的主要对象是nvidia/Llama-3.1-Nemotron-70B-Instruct-HF模型。由于版本升级至v0.6.3后，输入超过32K tokens时模型输出异常，而在v0.6.2版本中不存在此问题。

https://github.com/vllm-project/vllm/issues/9669
这是一个关于"[RFC]: Model Deprecation Policy"的提案，提出了当模型无法在最新的`transformers`版本中直接运行且无法得到模型供应商响应时，从vLLM中移除该模型的变更。

https://github.com/vllm-project/vllm/issues/9668
这是一个功能需求的issue，主要涉及V1版本中的prefix caching功能的实现。原因可能是为了提高V1引擎的性能和效率。

https://github.com/vllm-project/vllm/issues/9667
这是一个功能需求的issue，涉及重构日志部分和改进嵌入模型的日志记录。原因是为了支持不同模型的日志记录，通过重构Stats类和引入Double Dispatch模式来简化扩展，并提供清晰可扩展的日志架构。

https://github.com/vllm-project/vllm/issues/9666
这是一个bug报告，主要涉及VLM测试中遇到的问题，其中原因是因为在使用transformers v4.46时，Chameleon模型无法正常工作， `wrap_device` 在嵌套字典上不起作用，以及在PaliGemma测试中遇到dtype不匹配的问题。

https://github.com/vllm-project/vllm/issues/9664
这是一个Bug报告，主要涉及到vllm下的一个issue，用户反馈在启动OpenAI API服务器时，使用多个GPU会导致服务器挂起。造成这一现象的可能原因是程序无法正确处理多GPU环境下的资源分配过程。

https://github.com/vllm-project/vllm/issues/9663
这是一个Bug报告，主要涉及的对象是VLLM项目中的`image_url.detail`默认值检查问题。原因是当前的默认处理不支持`detail == None`，导致产生不必要的警告信息。

https://github.com/vllm-project/vllm/issues/9662
这是一个文档更新的任务，目的是更新spec_decode.rst中的FAQ链接。

https://github.com/vllm-project/vllm/issues/9661
这个issue类型是用户提出需求，涉及的主要对象是GitHub上的一个workflow。增加`operationsperrun`的限制是为了确保workflow的更改能更快地传播到所有的issues和PRs，而之前的限制导致了操作限制不足的问题。

https://github.com/vllm-project/vllm/issues/9660
这是一个关于性能问题的issue，主要涉及GPU KV cache使用和多GPU推理时指标下降的情况，用户需要了解这个问题的原因。

https://github.com/vllm-project/vllm/issues/9659
这是一个用户提出需求的issue，涉及的主要对象是添加指标以获取请求排队时间、转发时间和执行时间的信息。由于`otlptracesendpoint`标志导致对`collect_model_forward_time`和`collect_model_execute_time`的限制，用户要求移除该限制以便收集有关`model_forward_time`和`model_execute_time`的信息。

https://github.com/vllm-project/vllm/issues/9658
这是一个需求求助类型的issue，主要涉及在kubernetes环境下运行Llama-3.1-70B-Instruct模型推理，用户寻求针对多用户同时发送大量请求以获取最佳吞吐量的最佳参数和建议。

https://github.com/vllm-project/vllm/issues/9656
这是一个bug报告类型的issue，涉及对象是使用vllm的用户在使用LLama3.1-405b时遇到了huggingface_hub.errors.HFValidationError错误。用户试图使用下载自`https://huggingface.co/metallama/Llama3.1_405BInstruct`的数据集，但出现了`Repo id must be in the form 'repo_name' or 'namespace/repo_name'`的错误。

https://github.com/vllm-project/vllm/issues/9655
这是一个用户提出需求的issue，主要对象是传递多个LoRA模块，并出现解析错误，原因是无法正确传递多个LoRA模块。

https://github.com/vllm-project/vllm/issues/9654
这是一个用户提出需求的issue，主要涉及支持SageAttention功能。由于期望实现Quantized Attention来提高速度，并且在各种模型中不损失终端指标，导致提出了这个需求。

https://github.com/vllm-project/vllm/issues/9653
这是一个bug报告，涉及到vision encoder的post_norm层在LLaVAOnevision模型上的禁用问题。由于[VLM] Postlayernorm override和vision encoder中的quant配置，导致需要禁用vision encoder的post_norm层。

https://github.com/vllm-project/vllm/issues/9652
这是一个用户提出需求的issue，主要涉及的对象是vLLM框架和IBM Spyre AI accelerator，由于IBM Spyre暂时只支持执行IBM开源Foundation Model Stack (`fms`)中的建模代码，并且不支持分页注意力或连续批处理，因此用户希望在vLLM框架中添加对IBM Spyre的支持。

https://github.com/vllm-project/vllm/issues/9651
这是一个性能问题，用户提出了关于GPU利用率低的问题，希望提升库的性能。

https://github.com/vllm-project/vllm/issues/9650
这个issue是一个模型改进的类型，主要涉及LLavanext中计算最大令牌数和虚拟数据的问题。原因是之前使用了固定的高度和宽度来计算，现在需要根据不同的配置计算图像特征并保留最大值来纠正模型计算的问题。

https://github.com/vllm-project/vllm/issues/9649
该issue类型是用户提出的需求。主要涉及对象是vLLM项目和Cambricon MLU。由于Cambricon团队已经在内部支持了vLLM在Cambricon MLU上的使用，提出希望贡献MLU适配代码到vLLM项目中，并计划在11月份提交pull request。

https://github.com/vllm-project/vllm/issues/9648
这个issue类型是Bug报告，涉及的主要对象是需要支持半数据类型。由于markdown渲染问题，导致PR描述无法显示，导致提交问题时没有填写PR描述。

https://github.com/vllm-project/vllm/issues/9647
这是一个文档问题，主要涉及到GitHub上的vllm仓库中的issue。由于使用模型scope和检查模型支持的说明被埋在了长列表中，导致可见性不佳，提出将其移到页面顶部。

https://github.com/vllm-project/vllm/issues/9646
这是一个需求类型的issue，涉及主要对象是mm_input_mapper，用户希望将其移动到一个独立的进程中。

https://github.com/vllm-project/vllm/issues/9645
这是一个Bug报告，主要涉及VLLM模型在Pipeline Parallel推理中性能出现问题。由于当前环境下的串行请求和并行请求性能不匹配，导致了执行时间异常。

https://github.com/vllm-project/vllm/issues/9643
这是一个关于缓存重用功能的问题，涉及主要对象是vllm。由于用户想了解vllm中的前缀缓存是否支持在请求计算之前自动重用缓存，以及新批次请求是否会自动重用共享前缀，以及与TensorRTLLM的缓存重用机制的比较。

https://github.com/vllm-project/vllm/issues/9642
这个issue类型是代码改进，涉及的主要对象是在vllm项目中使用current_platform.is_rocm。原因是代码中需要更新变量`is_hip`为`current_platform.is_rocm`。

https://github.com/vllm-project/vllm/issues/9641
这是一个更新issue，主要涉及torch compile annotations添加到某些模型中。

https://github.com/vllm-project/vllm/issues/9640
这是一个Bug报告，主要涉及的对象是VLLM模型与glm4-9b-chat-lora-merge模型在并发请求时出现"Aborted request"错误的问题。原因可能是在设置100个并发请求时，当有约30个挂起请求时，推理过程会卡住。

https://github.com/vllm-project/vllm/issues/9639
该issue是一个特性增强类型的问题，涉及的主要对象是torch compile模型。原因可能是为了提高模型的性能和效率。

https://github.com/vllm-project/vllm/issues/9638
这个issue类型为功能需求，主要涉及的对象是用户对于添加新模型的请求。该问题没有得到回应，用户希望支持一个叫做Ovis1.6-Gemma2-9B的新模型。

https://github.com/vllm-project/vllm/issues/9637
这个issue是一个功能增强类型，主要涉及到torch compile的支持和修复allgather编译。由于缺乏torch compile注释，需要自动推断dynamic_arg_dims以修复问题。

https://github.com/vllm-project/vllm/issues/9636
这是一个bug报告类型的issue，主要对象是Fused Moe pytest，由于pytest在大量experts下失败，导致了问题无法得到正确的输出。

https://github.com/vllm-project/vllm/issues/9635
这是一个bug报告，主要涉及的对象是"Mamba 2 inference"。原因可能是代码逻辑错误导致无法正确进行推理。

https://github.com/vllm-project/vllm/issues/9634
这个issue是一个Bugfix类型的报告，涉及主要对象是vllm的前端。由于传入了坏的token id，导致在非mistral模型上的vllm服务器会崩溃。

https://github.com/vllm-project/vllm/issues/9633
这个issue是关于用户需求的问题，主要涉及如何在vllm中进行多模态内容的测试，原因是用户不清楚如何使用benchmark_serving.py来测试多模态功能而导致问题出现。

https://github.com/vllm-project/vllm/issues/9632
这是一个用户提出需求的issue，主要涉及的对象是moe models。原因是moe models在forward过程中读取配置文件导致与torch.compile不兼容。

https://github.com/vllm-project/vllm/issues/9631
这个issue是一个bug报告，涉及的主要对象是Llama 3.2 Vision model，由于发送请求包含guided decoding会导致引擎崩溃，原因是CUDA deviceside错误。

https://github.com/vllm-project/vllm/issues/9630
这是一个bug报告类型的issue，主要涉及vllm的OOM错误问题，可能是由于服务器内存不足导致。

https://github.com/vllm-project/vllm/issues/9629
这个issue是一个bug报告，涉及主要对象是V1版本的请求处理。这个bug产生的原因是请求在被中止时未能正确释放，导致请求对象没有被正确释放。

https://github.com/vllm-project/vllm/issues/9628
这是一个bug报告，主要涉及到MllamaVisionModel实例化时缺少"vision_model"前缀，导致了检查点无法运行的问题。

https://github.com/vllm-project/vllm/issues/9627
这是一个bug报告，主要涉及到了vllm模块中的beam search功能与eos token相关的问题。原因是beam_search在api中包含了eos token导致了输出结果中不应该包含eos token。

https://github.com/vllm-project/vllm/issues/9626
这是一个Bugfix类型的issue，主要涉及Mllama模型中批量多图推理时可能触发CUDA错误的问题。原因是在交叉注意力SDPA期间，Q/K/V状态不是在所有情况下是连续的。

https://github.com/vllm-project/vllm/issues/9625
这是一个bug报告，主要涉及的对象是MistralTokenizer。由于MistralTokenizer是byte-based的，导致在token id和string之间存在冲突，从而引发了两个bug，导致引擎在生成某些特定的token时出错。

https://github.com/vllm-project/vllm/issues/9624
这个issue类型是bug报告，主要涉及的对象是vllm中的模型架构。由于缺少vllm._C模块和vllm._version模块，导致数值计算异常并无法支持'LlamaForCausalLM'模型架构。

https://github.com/vllm-project/vllm/issues/9623
这是一个Bug报告，该问题单涉及的主要对象是vllm库下的Phi-3.5-mini-instruct模型。导致该Bug的原因是数值形状不兼容所致。

https://github.com/vllm-project/vllm/issues/9622
该issue类型为功能性需求提议，主要涉及Qwen模型的LoRA支持添加，由于需要更好地支持LoRA，故提出对Qwen LLM和VL进行区分。

https://github.com/vllm-project/vllm/issues/9621
该issue属于需求提出类，主要涉及改进python-only开发环境设置。由于当前的python-only开发和构建过程较为繁琐，提出使用`VLLM_USE_PRECOMPILED`环境变量来跳过扩展编译，以简化流程。

https://github.com/vllm-project/vllm/issues/9620
这是一个关于修复问题记录的issue，涉及主要对象为Baichuan，由于可能存在混淆，用户提出了需要对Baichuan进行明确说明的帮助。

https://github.com/vllm-project/vllm/issues/9619
这个issue类型是bug报告，涉及的主要对象是vllm项目中的`BaichuanForCausalLM`模型。由于该模型被定义了两次，导致可能会出现覆盖问题，需要确定保留哪一个版本。

https://github.com/vllm-project/vllm/issues/9618
这是一个Bug报告类型的issue，主要涉及vllm的GPTQInt4模型在使用Double Tesla V100加载时输出全部为感叹号的问题，可能是因为GPTQInt4量化存在与vllm的兼容性问题。

https://github.com/vllm-project/vllm/issues/9617
这是一个bug报告，主要涉及VLLM项目中的Mamba模型，在服务该模型时出现的除以零错误，由于`num_total_gpu`在attentionfree模型中为0，导致在llm_engine.py文件中运行时出现除零错误。

https://github.com/vllm-project/vllm/issues/9615
这是一个Bug报告，涉及的主要对象是代码生成模型。这个问题是由于固定了`max_model_len`参数并使用大的上下文长度导致生成结果出现错误。

https://github.com/vllm-project/vllm/issues/9614
这个issue类型是功能改进，主要涉及的对象是torch compile模块。这次改进的原因是为了增强模型的注释和测试。

https://github.com/vllm-project/vllm/issues/9613
这是一个特性新增的类型，该问题单涉及的主要对象是opt模型的管道支持。

https://github.com/vllm-project/vllm/issues/9612
这是一个模型改进类型的issue，主要涉及Qwen2VL模型新增min_pixels / max_pixels参数，用户提交了该需求并已提供相应代码更新。

https://github.com/vllm-project/vllm/issues/9611
这是一个bug报告，该问题单涉及的主要对象是NVLM_D model。由于在相关配置中修改了Vision Encoder导致了NVLM_D model出现问题。

https://github.com/vllm-project/vllm/issues/9610
这是一个用户提出需求的issue，主要涉及的对象是Kernel中新增一个针对FATReLU的kernel。由于缺少具体描述和细节，可能是用户请求新增相关内容。

https://github.com/vllm-project/vllm/issues/9609
这是一个性能问题，关于测试vLLM的预测准确性，是否会受到 speculative decoding 的影响。

https://github.com/vllm-project/vllm/issues/9608
这是一个Bug报告类型的Issue，主要涉及到程序在运行中出现了最大递归深度超出的错误，导致崩溃。

https://github.com/vllm-project/vllm/issues/9607
这是一个关于新增模型支持的需求提出的issue，主要涉及的对象是vllm项目和Pangea-7B模型。由于该模型与已支持的LlavaNext具有相同的架构，提出者希望了解支持该模型会有什么困难。

https://github.com/vllm-project/vllm/issues/9606
这是一个用户提出需求的issue，主要涉及支持新模型"stepfun-ai/GOT-OCR2_0"，因为当前版本不支持该架构。

https://github.com/vllm-project/vllm/issues/9605
这个issue是一个文档问题，主要涉及到vLLM项目中的markdown渲染无法正常工作，导致需要在原始HTML中使用标记。

https://github.com/vllm-project/vllm/issues/9604
这个issue属于用户提出的需求类型，主要涉及到了在encoder-decoder inputs的结构转变中的问题。这个问题的提出是为了更方便地支持decoder-only模型的输入嵌入。

https://github.com/vllm-project/vllm/issues/9603
这是一个用户需求类型的issue，涉及主要对象是如何向vLLM处理过程环境中添加插件处理进程。原因是用户需要在一个包含k+1个进程的同一world中实现特定的应用场景。

https://github.com/vllm-project/vllm/issues/9602
这是一个bug报告，涉及主要对象为vllm中使用pdb导致程序崩溃的问题，原因是无法正确使用最新版本中的pdb功能。

https://github.com/vllm-project/vllm/issues/9601
这是一个建议更改类型的issue，主要涉及openai api中的响应格式，用户想要禁用响应中的额外部分。

https://github.com/vllm-project/vllm/issues/9600
这是一个用户提出需求的issue，主要涉及的对象是MiniCPM3模型的infer速度问题，可能由于"forceeager"参数设置导致infer速度过慢，用户在寻找解决此问题的方法。

https://github.com/vllm-project/vllm/issues/9599
这个issue是关于Bugfix的，主要涉及Hardware中的tpu，处理的主要对象是v5litepod8。由于环境变量设置不正确，导致vllm在tpu v5litepod8上运行失败。

https://github.com/vllm-project/vllm/issues/9598
这个issue类型是测试相关，主要对象是linting problems，原因是为了测试linting问题在另一个PR上的表现。

https://github.com/vllm-project/vllm/issues/9597
这是一个关于bug报告类型的issue，主要涉及的对象是Pixtral和xformers，用户提出需要在非CUDA平台上运行LlavaNextForConditionalGeneration时避免xformers依赖的问题。

https://github.com/vllm-project/vllm/issues/9596
这个issue是关于构建错误的问题，涉及到`FetchContent`多个构建时存在冲突问题，导致二进制目录冲突。

https://github.com/vllm-project/vllm/issues/9595
这是一个bug报告，主要涉及vllm中加载mistral模型时出现KeyError的问题，可能是因为权重加载格式不匹配导致的。

https://github.com/vllm-project/vllm/issues/9594
这是一个关于修复CC([Feature]: support `xrequestid` header)部分的问题报告。由于markdown渲染不起作用，需要使用原始HTML来呈现。

https://github.com/vllm-project/vllm/issues/9593
这是一个功能特性请求，涉及支持 `x-request-id` 头，在在线服务中有关请求标识的需求。

https://github.com/vllm-project/vllm/issues/9592
这是一个Bug修复类型的issue，涉及到vLLM项目中的`benchmark_throughput.py`文件。这个问题是由于生成的序列不符合tokenizer的要求，导致无法生成精确的`args.input_len`个token_ids。

https://github.com/vllm-project/vllm/issues/9591
这是一个用户提出需求类型的issue，主要涉及支持新的IBM Granite 3.0模型。由于没有对应的支持，用户请求添加对该模型的支持。

https://github.com/vllm-project/vllm/issues/9590
这个issue属于一个[杂项]类型，主要涉及添加一个环境变量VLLM_LOGGING_PREFIX，用户可能遇到日志消息前缀不清晰的问题。

https://github.com/vllm-project/vllm/issues/9589
这是一个用户提出需求的issue，主要对象是torch.compile。由于当前torch.compile的使用不够方便，用户希望支持更简单的方法来进行类型注释的动态尺寸推断。

https://github.com/vllm-project/vllm/issues/9588
这是一个升级pytorch版本导致的issue，并不是bug报告。

https://github.com/vllm-project/vllm/issues/9587
这是一个Bug报告，主要涉及的对象是llama-3.1-8b-instruct工具在运行时发生崩溃。这个问题的症状是在收集环境信息时出现了一系列无法收集到版本信息的警告。

https://github.com/vllm-project/vllm/issues/9586
这个issue类型是bug报告，涉及主要对象为internvl2-8b-vllm-server。由于配置或者框架支持问题导致输出质量不佳、内容混杂、长度偏长的问题。

https://github.com/vllm-project/vllm/issues/9585
这是一个关于功能缺失的问题，主要涉及到vLLM中的FP8 KV Cache功能，用户询问是否存在逆量化操作，以及如何寻找实现逆量化操作的代码。

https://github.com/vllm-project/vllm/issues/9584
该issue类型为用户提出需求，主要对象是QWenLMHeadModel模型，由于LoRA功能未支持该模型，用户希望未来的版本中添加对该功能的支持。

https://github.com/vllm-project/vllm/issues/9583
这是一个bug报告，涉及主要对象是OpenAI Compatible Server的socket创建。由于使用`socket.AF_INET`造成在只有IPv6地址的机器上无法从外部访问，所以需修改为`socket.AF_INET6`支持IPv4和IPv6地址。

https://github.com/vllm-project/vllm/issues/9582
这是一个用户提出需求的issue，主要涉及vLLM是否支持生成结构化输出的问题，用户想要运行特定模型的推断，并询问如何集成vLLM。

https://github.com/vllm-project/vllm/issues/9581
这个issue属于性能问题报告，主要涉及到版本0.6.3的性能下降与版本0.6.2的比较，可能是由于两个版本之间的某些变化导致了性能差异。

https://github.com/vllm-project/vllm/issues/9580
这是一个bug报告类型的issue，涉及的主要对象是项目中的配置参数。由于config中的参数命名冲突，导致在decoder层没有bias的情况下，加载模型时出现了问题，需要修改参数名以解决冲突。

https://github.com/vllm-project/vllm/issues/9579
这是一个bug报告类型的issue，涉及的主要对象是项目中的某个测试用例。该bug是由于新增的参数导致了测试用例失败，并掩盖了其他PR引入的问题。

https://github.com/vllm-project/vllm/issues/9578
这个issue属于bug报告，涉及到vllm启动internvl2-8b模型服务时内存溢出的问题。

https://github.com/vllm-project/vllm/issues/9577
这是一个bug报告类型的issue，主要涉及vllm无法处理多模态输入数据导致的TypeError错误。

https://github.com/vllm-project/vllm/issues/9576
该issue类型为功能增强，主要涉及对象是模型VLLM以支持E5-V，并添加对图像输入的测试。

https://github.com/vllm-project/vllm/issues/9575
这是一个用户提出需求的issue，主要涉及vllm是否支持在线批量推理功能。原因是用户使用Qwen2VL并部署了在线服务器，想了解是否支持在线批量推理。

https://github.com/vllm-project/vllm/issues/9574
这是一个关于功能支持的issue，主要涉及的对象是Qwen2 bnb，由于需要支持TP，用户进行了相应的PR操作。

https://github.com/vllm-project/vllm/issues/9573
这是一个bug报告，涉及主要对象是使用Medusa和TP时driver_worker被卡住的问题，可能是由于GPU未能计算lm_head.linear_method.apply()中的logits导致第二个worker进程报错而引起。

https://github.com/vllm-project/vllm/issues/9572
这是一个移除无用文件的issue，主要涉及的对象是vLLM的Evictor_v1模块。该问题由于block manager v1被移除导致Evictor_v1变得无用，需要被安全移除。

https://github.com/vllm-project/vllm/issues/9571
这是一个Bug报告，主要涉及当使用temperature=0时，某些提示会导致模型返回不完整的XML格式输出。导致这种症状的原因可能是模型在处理特定提示时出现了问题。

https://github.com/vllm-project/vllm/issues/9570
这个issue是关于[CI/Build]的，主要涉及的对象是测试用的模型，原因是为了通过替换一些模型为更小的模型来加快测试速度。

https://github.com/vllm-project/vllm/issues/9569
这个issue类型为优化建议，主要涉及代码中的ifelse条件，提出简化代码逻辑的建议。

https://github.com/vllm-project/vllm/issues/9568
这是一个缺失PR描述的issue，涉及到添加固定接受率的问题，可能是由于Markdown渲染问题而使用了原始HTML，导致PR描述内容不完整。

https://github.com/vllm-project/vllm/issues/9567
这是一个bug报告，涉及模型在不同的batch大小下产生不同输出的问题，可能由于请求的调度方式导致。

https://github.com/vllm-project/vllm/issues/9566
该issue类型为Feature请求，主要涉及VLLM软件支持1.58-bit模型的需求。

https://github.com/vllm-project/vllm/issues/9565
这是一个性能问题报告，涉及vllm Eagle性能不如预期，可能原因包括缺失树验证核心、系统开销过高以及数据集与头部微调不匹配等。

https://github.com/vllm-project/vllm/issues/9564
这个issue是一个bug报告，主要涉及的对象是vllm的CI工具。由于格式检查器错误消息不够用户友好，导致需要更改。

https://github.com/vllm-project/vllm/issues/9563
这是一个关于修复内存分析导致的测试失败的bug报告，涉及到测试用例的调整和模型返回结果不稳定的问题。

https://github.com/vllm-project/vllm/issues/9562
这个issue属于bug报告类型，主要涉及的对象是文件格式。导致这个问题的原因可能是格式错误，导致显示出现问题。

https://github.com/vllm-project/vllm/issues/9561
这是一个Bug报告，涉及的主要对象是benchmark serving的功能。由于best_of选项设置大于1时导致benchmark serving测试失败，用户提出了这个问题寻求帮助。

https://github.com/vllm-project/vllm/issues/9560
这是一个bug报告，主要涉及的对象是vLLM的Custom Paged Attention功能。由于当前Custom Paged Attention仅支持MI250和MI300，导致在MI100上出现程序运行失败的问题。

https://github.com/vllm-project/vllm/issues/9559
该issue类型为功能增强请求，涉及的主要对象是Encoder-Decoder模型。由于模型实现依赖于PagedAttention，所以在dtype=bfloat16的情况下仍然使用Xformers后端，而不是默认的FlashAttention。

https://github.com/vllm-project/vllm/issues/9558
这个issue是一个bug报告，涉及的主要对象是使用模型Llama3.1Nemotron70BInstructHFGGUF时出现数值错误，可能是由于使用了错误的参数导致无法reshape数组的大小。

https://github.com/vllm-project/vllm/issues/9557
这个issue是一个Bug报告，涉及到MistralTokenizer Detokenization的问题，用户反映在重现特定错误案例时出现了问题。

https://github.com/vllm-project/vllm/issues/9556
这是一个Bug报告，涉及到在部署pipeline parallel时出现"address already in use"错误。导致此问题的原因可能是端口被占用导致服务部署失败。

https://github.com/vllm-project/vllm/issues/9555
这个issue是关于添加新模型或改进现有模型的问题，提议将Florence-2语言骨干支持添加到vLLM中。

https://github.com/vllm-project/vllm/issues/9554
这是一个 bug 报告，涉及 vllm 不支持 torch 2.5 的安装问题，导致无法正常运行。

https://github.com/vllm-project/vllm/issues/9553
这个issue是一个[Neuron] bug报告，主要涉及到异步输出处理。出现这个bug的原因是无法确认性能提升，即使在3B openllama模型上，使用或不使用异步输出都没有性能提升。

https://github.com/vllm-project/vllm/issues/9552
这个issue是一个Bug报告，主要涉及修复OpenVINO的Dockerfile文件，在运行"docker build"命令时出现无法复制文件的问题。

https://github.com/vllm-project/vllm/issues/9551
这是一个用户提出需求的 issue，主要涉及 vllm 中实现自定义生成方法时遇到的问题，由于在访问和存储 attention 输出时出现错误，导致无法成功实现目标。

https://github.com/vllm-project/vllm/issues/9550
这是一个[Frontend]类型的issue，主要涉及自定义请求id的支持。由于当前的request_id是在create_chat_completion函数中生成的，无法被高级用户控制。在某些情况下，我们希望将自定义的request_id传递给`create_chat_completion`。

https://github.com/vllm-project/vllm/issues/9549
这是一个bug报告类型的issue，主要涉及decoder组件，由于之前一次修复引入了错误，导致了graph capture功能不能正常工作。

https://github.com/vllm-project/vllm/issues/9548
这是一个需求反馈类型的issue，主要涉及支持模型"bert-base-chinese"，用户可能遇到的困难是支持BertForMaskedLM模型。

https://github.com/vllm-project/vllm/issues/9547
这是一个bug报告，涉及的主要对象是vllm环境中的Llama-3.1-8B-Instruct模型，由于字符串尾部包含特殊字符导致了问题。

https://github.com/vllm-project/vllm/issues/9546
这是一个关于技术问题的issue，主要针对vllm中qwen2vl模型在cuda graph实现过程中的问题。由于cuda graph处理textonly输入的位置信息形状为(3, seqlen)，而实际输入为(seqlen)，导致使用cuda graph和eager mode时没有最终结果的差异，用户想了解原因。

https://github.com/vllm-project/vllm/issues/9545
这是一个用户提出需求的issue，主要涉及对象是在LLM.generate中使用Qwen2-VL进行离线推断时设置max_pixels参数。原因可能是由于当前环境中的PyTorch版本与CUDA版本之间的不匹配导致。

https://github.com/vllm-project/vllm/issues/9543
这个issue是关于提出需求的，主要涉及的对象是函数追踪文件路径。由于读取追踪日志困难，用户建议通过可视化来帮助理解，这样有助于性能调整和调试。

https://github.com/vllm-project/vllm/issues/9542
这是一个bug报告，涉及的主要对象是在使用四张4500 ada卡启动四个lora实例时出现OOM错误。由于使用四个4500 ada卡同时启动lora时出现错误，但仅在单个A100卡上启动四个lora实例时没有错误，推测可能是显存不足导致的问题。

https://github.com/vllm-project/vllm/issues/9541
这是一个特性需求的issue，主要对象是实现对控制解码的支持。由于作者希望实现对比解码，并为此进行了相关的工作和讨论，希望社区能够做出贡献。

https://github.com/vllm-project/vllm/issues/9540
该issue类型为提出需求，请教问题，主要对象是关于vllm服务器并发处理的。由于用户在尝试构建自己的llm api_server时，发现在测试代码时`vllm serve`表现出很强的推理效率，询问应该如何改进fastapi中的代码或者如何查看`vllm serve`的处理方式。

https://github.com/vllm-project/vllm/issues/9539
这是一个用户提出需求类的Issue，主要涉及vllm v0.6.3版本中的`System efficiency`和`Number of emitted tokens`的含义。由于用户想了解如何理解日志（当进行猜测性解码）而提出此问题。

https://github.com/vllm-project/vllm/issues/9538
该issue类型为bug报告，涉及主要对象为vllm在TPU上运行时的错误。由于当前环境中collect_env.py未能报告TPU节点上的GPU信息，导致出现了错误。

https://github.com/vllm-project/vllm/issues/9537
这是一个需求提出的issue，主要涉及到Disaggregated prefilling功能的实现。由于需要实现X prefill + Y decode形式的功能，作者计划引入kv数据库，并设计了新组件PDEngine和RabbitMQ来实现此功能。

https://github.com/vllm-project/vllm/issues/9536
这个issue属于bug报告，主要涉及的对象是硬件，由于代码中`is_cpu`被改成了`current_platform.is_cpu`导致了markdown渲染错误。

https://github.com/vllm-project/vllm/issues/9535
这是一个性能问题的报告，主要涉及bitsandbytes量化推理速度的改进。由于bnb量化导致推理速度极慢，可能是由于目前尚未对其进行优化的原因。

https://github.com/vllm-project/vllm/issues/9534
这是一个bug报告，涉及的主要对象是EAGLE模型。由于EAGLE模型可能不支持fp8导致的输出异常问题。

https://github.com/vllm-project/vllm/issues/9533
这是一个Bug修复类型的issue，主要涉及代码加载配置时使用布尔值出现的问题。这个bug可能是由于配置文件解析逻辑不正确导致的。

https://github.com/vllm-project/vllm/issues/9532
这是一个bug报告，主要涉及vLLM中涉及的`Chunked prefill`、`Prefix caching`、`Block manager V2`和`XFormers`这四个功能，在特定请求序列下导致`CUDA error: an illegal memory access was encountered`错误。

https://github.com/vllm-project/vllm/issues/9531
这是一个bug报告，主要涉及vllm中interpl2-8b模型启动服务时出现的错误，原因是没有在本地执行配置文件导致数值错误。

https://github.com/vllm-project/vllm/issues/9530
这是一个Bug报告，涉及的主要对象是vLLM中的json-schema功能。由于在当前vLLM中，`response_format={"type": "json_schema", ...}`无效，导致无法按预期使用json-schema来进行解码，需要修复这个问题。

https://github.com/vllm-project/vllm/issues/9529
这是一个需求更改类的issue，主要涉及的对象是benchmark脚本。由于CLI参数重复导致维护困难和部分特性无法暴露在脚本中，用户提出了更新benchmark脚本以直接使用EngineArgs提供的CLI参数，避免重复和增强可维护性。

https://github.com/vllm-project/vllm/issues/9528
这是一个关于在vLLM中添加对MonoInternVL系列模型支持的Model类型的issue。

https://github.com/vllm-project/vllm/issues/9527
这是一个关于性能问题的bug报告，主要涉及attention模块计算速度下降的问题。由于升级到vllm版本0.6.3，导致attention计算时间增加了1.7倍。

https://github.com/vllm-project/vllm/issues/9524
这是一个bug报告，涉及的主要对象是VLLM中的模型任务。这个bug是由之前的改动引入的，导致破坏了在VLLM中进行speculative decoding的功能。

https://github.com/vllm-project/vllm/issues/9523
这是一个bug报告类型的issue，涉及到加载8位量化模型失败的问题，可能是由于环境配置问题导致无法正确加载模型。

https://github.com/vllm-project/vllm/issues/9522
这是一个用户提出需求的 issue，主要涉及的对象是代码模型，其需求是支持在自动补全API中处理针对特定模型的FIM编码规则。

https://github.com/vllm-project/vllm/issues/9521
这个issue是关于Bug报告，涉及的主要对象是Frontend代码。由于代码中不正确的逻辑导致不必要地创建了`LogitsProcessor`，造成对所有请求都存在缺陷。

https://github.com/vllm-project/vllm/issues/9520
这个issue是关于性能优化的，主要涉及到PixtralHFVision模型，在使用Mistral的Pixtral和HF的apply_rotary_pos_emb时导致内存使用量过高。

https://github.com/vllm-project/vllm/issues/9519
这是一个关于bug报告的issue，主要涉及vLLM在处理BOS注入和复制方面的不一致性问题，导致用户无法有效地控制BOS的添加和生成。

https://github.com/vllm-project/vllm/issues/9518
这个issue是一个Bug修复类型的问题，主要涉及的对象是PixtralHF模型。由于输入数据的多样性导致了在处理多图像批处理时出现了错误，需要对输入数据进行归一化处理，以避免复杂性问题。

https://github.com/vllm-project/vllm/issues/9517
这是一个bug报告，涉及的主要对象是vllm环境，由于用户在加载google/gemma2b模型时设置的max_model_len超过了vllm支持的最大长度，导致出现数值错误的bug。

https://github.com/vllm-project/vllm/issues/9516
这是一个bug报告issue，主要涉及修复torch内存分析的问题。由于之前的假设错误导致了对GPU上内存分配的错误计算。

https://github.com/vllm-project/vllm/issues/9515
这是一个bug报告，主要涉及vLLM的支持模型架构问题，导致数值错误。

https://github.com/vllm-project/vllm/issues/9514
这个issue是优化类型的，主要涉及Pixtral模型的输入处理函数，目的是提高性能。

https://github.com/vllm-project/vllm/issues/9513
这是一个bug报告，主要涉及的对象是CI/Build系统。问题是由于缺少 ruff 生成 GitHub 格式的错误输出导致用户在查看错误时需要打开CI日志。

https://github.com/vllm-project/vllm/issues/9512
这是一个bug报告类型的issue，主要涉及的对象是CI/Build中的mypy任务。由于缺少错误匹配配置，导致CI工作失败时无法正确显示mypy的错误信息，需要添加相应的配置来处理mypy输出中的错误。

https://github.com/vllm-project/vllm/issues/9511
这是一个缺陷报告，主要涉及CI/Build配置的matcher问题，未能正确加载配置导致github无法正常解释作业输出和在diff查看器中突出显示出错。

https://github.com/vllm-project/vllm/issues/9510
这个issue是关于bug修复的，主要涉及的对象是offline_inference_with_prefix.py文件，由于多次实现了`cleanup()`方法，导致代码冗余，需要进行整合。

https://github.com/vllm-project/vllm/issues/9509
这是一个用户提出需求的issue，主要涉及VLLM是否能与finetuned unsloth模型一起使用，用户想要在finetuned unsloth模型上运行推断。这个问题可能是由于用户想要使用VLLM进行推断时遇到了困难，希望能获得相关支持或指导。

https://github.com/vllm-project/vllm/issues/9508
这个issue是文档相关问题，主要涉及代码块的格式和标题的修复。导致这个问题的原因是之前的相关变更和代码块格式不一致。

https://github.com/vllm-project/vllm/issues/9507
这个issue是关于文档更新的，主要对象是关于GPU内存利用率的flag，描述了这个flag是对内存利用的全局限制。

https://github.com/vllm-project/vllm/issues/9506
这是一个[功能添加]类型的issue，主要涉及加载句子转换模型中的汇聚（pooling）配置文件，用户希望通过这个方法读取sentencetransformers模型的相关内容。

https://github.com/vllm-project/vllm/issues/9505
这是一个bug报告，主要涉及代码中的内存使用问题，由于估算不准确导致OOM错误。

https://github.com/vllm-project/vllm/issues/9504
这是一个修复lint错误的CI/Build类型的issue，涉及主要对象是mistral tokenizer，由于引入一个bugfix导致了mypy错误。

https://github.com/vllm-project/vllm/issues/9503
这个issue类型是bug报告，涉及到Phi2上下文扩展功能无法正常工作，由于找不到配置文件中的rope_theta和max_position_embeddings数值导致。

https://github.com/vllm-project/vllm/issues/9502
这是一个bug报告类型的issue，主要涉及vllm的Phi-2模型在context扩展方面存在问题。由于代码中一个小细节错误导致输入长度超过2k时产生垃圾输出。

https://github.com/vllm-project/vllm/issues/9501
这是一个BugFix类型的issue，主要涉及到缓存前缀的问题。由于相同前缀的提示会同时更新缓存，导致了缓存数据不一致的bug。

https://github.com/vllm-project/vllm/issues/9500
这是一个模型更新的issue，与MPT模型相关。

https://github.com/vllm-project/vllm/issues/9499
这是一个Bug报告，涉及的主要对象是vllm工具中的配置文件读取参数，由于代码未考虑参数类型导致了读取配置文件时出现错误。

https://github.com/vllm-project/vllm/issues/9498
这是一个bug报告类型的issue，涉及到命名方式不一致的问题，导致用户尝试启用不支持的功能时可能会感到困惑。

https://github.com/vllm-project/vllm/issues/9497
这个issue属于功能改进提案，主要对象是Flashinfer后端，用户提出需要添加环境变量来强制启用tensor cores。

https://github.com/vllm-project/vllm/issues/9496
这是一个Bug报告类型的issue，主要涉及的对象是vllm下的qwenchat模型，用户提出了关于性能差异的问题。

https://github.com/vllm-project/vllm/issues/9495
这是一个关于Bug报告的Issue，主要涉及到LoRA在InternVLChatModel中的支持。由于当前环境vllm版本为0.6.1，出现了Model Input Dumps无响应的问题。

https://github.com/vllm-project/vllm/issues/9494
这是一个bug报告，涉及vllm 0.6.3更新后出现的问题，由于更新后出现了NCCL错误和core dumped问题。

https://github.com/vllm-project/vllm/issues/9493
这是一个关于修复由triton导致的ImportError的问题，涉及到代码中导入`triton`模块时未检查是否已安装`triton`的情况。

https://github.com/vllm-project/vllm/issues/9492
这是一个BugFix类型的issue，主要涉及使用错误的Python 3二进制文件在Docker.ppc64le入口点中导致的问题。

https://github.com/vllm-project/vllm/issues/9491
这是一个关于使用vllm时建议使用flashinfer作为默认后端的问题，属于用户需求类型。用户提出了对flashinfer作为后端的疑惑，并希望了解在什么情况下使用flashinfer会比FA2更好的官方基准测试。

https://github.com/vllm-project/vllm/issues/9490
这是一个Bug报告，主要涉及vLLLM中的rope_theta选项未能被应用的问题，可能由于配置文件中设置的rope_theta参数未正确生效所致。

https://github.com/vllm-project/vllm/issues/9489
这是一个与代码质量控制和类型检查工具（mypy）相关的Issue，属于改进代码质量的类型。

https://github.com/vllm-project/vllm/issues/9488
这是一个改进建议类型的issue，主要涉及到测试LM的decoder-only模块，旨在避免难以复现的问题。

https://github.com/vllm-project/vllm/issues/9487
这是一个bug报告，涉及到修复在Turing系统上针对W8A8的编译错误问题。由于缺失早期返回导致在运行内核后出错，并修复了在构建Turing系统时一些消息间距的问题。

https://github.com/vllm-project/vllm/issues/9485
这是一个用户提出需求的issue，主要涉及到前端部分的代码。由于缺乏控制执行顺序和惩罚范围的功能，导致了解决特定问题时的限制。

https://github.com/vllm-project/vllm/issues/9484
这个issue属于bug报告类型，主要涉及到的对象是CI/Build功能。由于复制了不必要的函数`fork_new_process_for_each_test`到`large_gpu_test`，导致出现了与GPU内存相关的问题。

https://github.com/vllm-project/vllm/issues/9483
这是一个bug报告，主要涉及InternVL在处理多图像时速度变慢的问题。

https://github.com/vllm-project/vllm/issues/9482
这个issue是关于VLLM rate limit的问题，涉及主要对象是Qwen2.5 32B模型，可能因为VLLM rate limit导致部分查询无法处理。

https://github.com/vllm-project/vllm/issues/9481
这个issue是一个Bug报告，涉及的主要对象是Llama 3.1 70B模型加载时出现的KeyError异常。导致这个问题的原因可能是rope_scaling字典中缺少'type'键引起的。

https://github.com/vllm-project/vllm/issues/9480
这是一个功能需求的issue，主要涉及的对象是vLLM下的Diff-Transformer，由于研究人员引入了Differential Transformer提出了相关功能需求。

https://github.com/vllm-project/vllm/issues/9479
这是一个用户提出需求的issue，主要涉及的对象是MllamaForCausalLM模型。由于该用户想要仅使用MllamaForCausalLM模型的文本层部分而无需使用视觉层，因此请求支持这一功能。

https://github.com/vllm-project/vllm/issues/9478
这个issue属于需求提出类型，主要涉及前端代码的修改以支持更简单的图像输入格式，这个需求是由于部分用户希望使用一种更简单的图像输入格式，目前系统无法处理这种格式所致。

https://github.com/vllm-project/vllm/issues/9477
这是一个功能请求类的issue，涉及主要对象是日志记录功能。原因是为了在LoRa启用时记录日志指标，并用于后续路由决策。

https://github.com/vllm-project/vllm/issues/9476
这是一个性能问题报告，主要涉及的对象是vllm版本 0.6.2 和 0.6.3.post1，报告指出0.6.3是否导致性能退化。

https://github.com/vllm-project/vllm/issues/9475
这个issue是一个bug报告，主要涉及completion API的使用中存在的问题，导致最终选择数据不包含汇总计数，并且API实现的一些逻辑需要简化。

https://github.com/vllm-project/vllm/issues/9474
这是一个性能问题报告，涉及VLLM在请求数量过多时变得缓慢。原因可能是模型部署环境和请求过多导致性能下降。

https://github.com/vllm-project/vllm/issues/9473
这个issue类型为bug报告，主要涉及对象是vllm项目中的RequestOutput.prompt和beam search实现，出现问题是由于最近一个变更导致了RequestOutput.prompt类型的更改，导致接口类型不匹配，从而引起一系列的typing breakages。

https://github.com/vllm-project/vllm/issues/9472
这是一个bug报告，主要涉及的对象是软件发布的.wheel文件。由于在版本v0.6.2中缺少.wheel文件，用户提出了是否这将是永久状态的疑问。

https://github.com/vllm-project/vllm/issues/9471
这是一个性能问题报告，主要涉及vllm库中FLASHINFER后端在H100上的性能问题。由于使用FLASHINFER后端时FP8吞吐量显著较低，希望能够解决这一问题。

https://github.com/vllm-project/vllm/issues/9470
这个issue类型是bug报告，涉及主要对象是代码库中的一个意外引入的文件`commit_id.py`，由于Core进行了版本迁移操作，导致了该文件的错误引入。

https://github.com/vllm-project/vllm/issues/9469
这是一个bug报告，主要涉及vllm与LLaMA-Factory集成时遇到的两个错误。原因是调用了未初始化vllm模型的函数导致bug，用户寻求如何解决这个问题。

https://github.com/vllm-project/vllm/issues/9468
这个issue是关于bug修复的，主要涉及mistral_common tokenizer，由于重复打印警告信息导致日志混乱。

https://github.com/vllm-project/vllm/issues/9467
这是一个PR(issue)，涉及的主要对象是支持`Qwen2ForCausalLM` for `BitsAndBytes` quantization。由于之前的PR并未解决问题，导致再次遇到相同问题，因此提交了这个PR。

https://github.com/vllm-project/vllm/issues/9466
这是一个bug报告，主要涉及FastChat vllm_worker在升级vLLM版本后无法加载模型权重的问题。可能是因为FastChat 0.2.36没有及时更新导致与vLLM 0.6不兼容。

https://github.com/vllm-project/vllm/issues/9465
这是一个用户需求相关的issue，主要涉及多GPU环境下指定不同显存利用率的问题。

https://github.com/vllm-project/vllm/issues/9464
这是一个关于功能需求的issue，主要涉及到关于在VLLM中实现交替的局部-全局注意力层，其目的是降低KV缓存的大小并提高内存的利用效率。

https://github.com/vllm-project/vllm/issues/9463
这是一个bug报告，主要涉及了模型输入中存在空字符串和空字节的问题，以及`top_logprobs`无法识别End-of-Text（EOT）标记。造成这个问题的原因可能是多个token对应单个字符时导致的。

https://github.com/vllm-project/vllm/issues/9462
这个issue是bug报告类型，主要涉及VLLM项目中的structured output inference功能，可能由于升级 0.6.2 到 0.6.3 版本导致了错误。

https://github.com/vllm-project/vllm/issues/9461
这个issue是一个bug报告，主要涉及到代码中的日志打印功能，由于调用错误导致问题的产生。

https://github.com/vllm-project/vllm/issues/9459
这是一个用户提出需求的issue；该问题涉及如何在vllm serve命令中传递模型路径；由于环境中PyTorch版本为2.4.0，CUDA版本为12.6，用户希望了解如何正确传递模型路径。

https://github.com/vllm-project/vllm/issues/9458
这是一个bug报告，主要涉及到vllm安装时出现的模块导入错误，可能是由于安装过程中缺少特定模块导致的。

https://github.com/vllm-project/vllm/issues/9457
这个issue是一个bug报告，涉及主要对象是`mistral_common`模块，用户反映在使用该模块时离线模式出现问题。

https://github.com/vllm-project/vllm/issues/9456
这是一个关于如何使用vllm结构化输出的问题，用户询问关于`response_format`和`guided_json`参数的优先级和作用。

https://github.com/vllm-project/vllm/issues/9454
这个issue是关于bug报告，涉及主要对象是langchain qwen2.5的function calling error，由于调用函数get_current_month时出现错误导致了该bug。

https://github.com/vllm-project/vllm/issues/9453
这是一个Bug报告，涉及到vLLM模型的logprob值受采样参数影响导致与OpenAI API不兼容的问题。

https://github.com/vllm-project/vllm/issues/9452
这是一个bug报告，主要涉及vLLM的性能在使用多个LoRA推理时明显下降，可能是由于新版本（0.6.2）导致的。

https://github.com/vllm-project/vllm/issues/9451
该issue类型为功能需求提出，主要涉及到API中的`parallel_tool_calls`参数。由于目前该参数并未被实际使用，导致调用模型返回多个工具调用而不是单独返回，进而导致与某些框架的兼容性问题。

https://github.com/vllm-project/vllm/issues/9450
这是一个bug报告，涉及到VLLM中的离线推理一致性问题。在输入数据列表长度为1时生成结果正确，长度为100时生成的结果却不正确。

https://github.com/vllm-project/vllm/issues/9449
这是一个bug报告，涉及的主要对象是vllm模块。导致该bug的原因是由于prompt过长导致模型出错。

https://github.com/vllm-project/vllm/issues/9448
这是一个bug报告，主要涉及的对象是vllm 0.6.3模型。问题可能由于输入超过8k令牌的长文本导致输出被截断或者生成不完整/重复答案。

https://github.com/vllm-project/vllm/issues/9447
这是一个关于代码优化的Issue，涉及vLLM中计算查询起点位置/序列起点位置的问题。通过避免创建中间张量query_lens_tensor并在CPU上计算query_start_loc，实现异步处理。

https://github.com/vllm-project/vllm/issues/9446
这是一个关于bug修复的issue，主要涉及的对象是 `mistral_common` tokenizer。由于`continue_final_message`参数与`prefix=True`冲突，导致出现错误提示，并有建议改进OpenAI的行为。

https://github.com/vllm-project/vllm/issues/9445
这个issue类型是bug报告，主要对象是vllm版本0.6.3及Llama-3.2-11B-Vision，由于OOM导致无法成功服务Llama3.211BVision。

https://github.com/vllm-project/vllm/issues/9444
这是一个bug报告，主要涉及Qwen/Qwen2-VL-2B-Instruct模型在处理多条消息时只产生单一输出。原因可能是代码的运行逻辑或调用问题。

https://github.com/vllm-project/vllm/issues/9443
这是一个Bug报告，主要涉及对象为在vllm中执行示例时遇到的dtype设置问题。该问题由于GPU的计算能力不符合要求，导致无法正确运行示例。

https://github.com/vllm-project/vllm/issues/9442
该issue类型为用户提出需求，主要涉及对象是在使用Slack时需要添加注释。由于缺乏关于如何使用Slack的说明，用户提出了需要添加关于Slack使用的注释的需求。

https://github.com/vllm-project/vllm/issues/9441
这是一个bug报告，涉及了vllm模型中的speculative decoding功能，在处理并行请求时产生了乱码的现象。

https://github.com/vllm-project/vllm/issues/9440
这是一个关于改进/优化的issue，主要涉及到XPU模块。原因是XPU模块不作为依赖项，所以需要避免导入triton。

https://github.com/vllm-project/vllm/issues/9439
这个issue属于Bug报告，涉及的主要对象是ray实例检测的流程。由于当前代码未优先尝试连接到最新启动的实例，可能导致无法正确检测实例或创建新实例时未设置正确的GPU数量。

https://github.com/vllm-project/vllm/issues/9438
这是一个关于技术性更新和改进的issue，涉及主要对象为PyTorch XLA和TPU，需要提升内存使用峰值检测的准确性。

https://github.com/vllm-project/vllm/issues/9437
这是一个bug报告，涉及的主要对象是在TPU上加载权重时可能引发的内存使用问题。Bug的症状是在模型加载过程中可能导致OOM错误。

https://github.com/vllm-project/vllm/issues/9436
这个issue是一个功能需求类型，涉及自动关闭过期的问题和PR，主要对象是CI/Build系统。

https://github.com/vllm-project/vllm/issues/9435
这个issue类型是建议（RFC），主要涉及的对象是启用过期机器人用于对issue和PR进行处理，由于项目中有许多未活动的issue和PR，需要关闭不活跃的项。

https://github.com/vllm-project/vllm/issues/9434
这是一个bug报告，主要涉及支持量化模型的张量化模型权重的功能。由于使用张量化加载模型在vLLM上是不稳定的，可能导致错误。

https://github.com/vllm-project/vllm/issues/9433
这是一个bug报告，涉及到在Mixtral模型上guided_grammer功能出现异常。该问题可能由环境配置或代码逻辑错误导致。

https://github.com/vllm-project/vllm/issues/9432
该issue是关于用户提出需求的，主要涉及pynccl库中添加all_gather和reduce_scatter的wrapper，以便在CUDA图中使用，特别是Molmo和OLMo需要使用all_gather功能。

https://github.com/vllm-project/vllm/issues/9430
这个issue是关于CI/Build方面的改进提案，涉及到GitHub actions的使用方式。原因是为了提高安全性，避免恶意修改标签指向的问题。

https://github.com/vllm-project/vllm/issues/9429
这是一个关于功能需求的问题，主要对象是vLLM模型加载和实例契约。用户提出了是否可以支持在同一vLLM实例上运行多个模型的问题，原因是他们希望在性能强大的GPU上服务多个小模型而不是为每个模型管理单独的vLLM实例。

https://github.com/vllm-project/vllm/issues/9428
这是一个Bug报告，主要涉及VLLM无法正确中止请求的问题。导致该问题的原因可能是实现中止请求的方式不正确。

https://github.com/vllm-project/vllm/issues/9427
这是一个Bug报告，涉及的主要对象是前端功能。导致该问题的原因是前端的beam search在处理多模态输入时存在问题。

https://github.com/vllm-project/vllm/issues/9426
该issue类型是一个功能需求，主要对象是vllm下的tool_choice参数设置问题，由于vllm还不支持设置tool_choice为"none"，导致用户无法阻止函数调用。

https://github.com/vllm-project/vllm/issues/9425
这是一个bug报告，主要涉及的对象是fp8 dynamic quantize kernel。由于使用int32可能导致整数溢出，因此可能会发生GPU SEGV，这个问题是由于未处理整数溢出导致的。

https://github.com/vllm-project/vllm/issues/9424
这个issue类型是功能需求，主要涉及对象是VLLM中的模型配置。由于新加入的`task`选项会带来对VLM2Vec用户的破坏性变化，需要用户显式指定任务模式，以保持长期可维护性。

https://github.com/vllm-project/vllm/issues/9423
这是一个Bug报告类型的issue，涉及vLLM中的Speculative decoding功能，导致了Guided decoding无法正常工作。

https://github.com/vllm-project/vllm/issues/9422
该issue是一个bug报告，主要涉及的对象是ChatGLM、Molmo和Qwen2.5-Math-RM模型。由于添加GLM4v支持时意外移除了ChatGLM的PP支持，导致了这个bug。

https://github.com/vllm-project/vllm/issues/9421
这是一个Bug报告，主要涉及VLLM库版本更新到0.6.3后出现模块命名错误导致的问题。

https://github.com/vllm-project/vllm/issues/9420
这是一个关于安装问题的bug报告，用户在使用docker设置时遇到了模块导入错误。

https://github.com/vllm-project/vllm/issues/9419
这是一个bug报告，涉及到INT8(W8A8)格式在0.6.2升级到0.6.3后无法加载的问题，可能的原因是缺少了针对CUDA设备能力小于75的编译cutlass_scaled_mm。

https://github.com/vllm-project/vllm/issues/9417
这是一个Bug报告，涉及的主要对象是v0.6.3版本的vllm项目，用户在使用CUDA图时遇到了Regression问题，可能是由于更新的`awq_marlin` kernels中的动态变化导致的。

https://github.com/vllm-project/vllm/issues/9416
这是一个用户提出需求的issue，主要涉及到如何在Metrics中获取time_to_first_token_seconds信息，可能是由于缺乏相关文档或指导而导致用户不清楚如何使用该功能。

https://github.com/vllm-project/vllm/issues/9415
这是一个Bug报告类型的issue，主要涉及vllm中的vision encoder模型，可能由于在模型执行过程中出现了维度错误导致了该Bug。

https://github.com/vllm-project/vllm/issues/9414
该issue是一个功能需求，主要涉及支持 mistral 模型的 interleaved attention patterns。由于目前无法清晰地支持这种 attention 模式，需要将最大模型长度限制为允许的 attention 模式的最小长度。

https://github.com/vllm-project/vllm/issues/9413
这是一个bug报告，涉及vLLM 0.6.3版本中使用qwen2.5进行推理时感觉速度较慢的问题。

https://github.com/vllm-project/vllm/issues/9412
这个issue是一个[杂项]类型的问题，涉及主要对象是整合OpenAI客户端用于多模态模型的示例用法。由于渲染markdown的问题，内容中包含了原始的HTML标记。

https://github.com/vllm-project/vllm/issues/9411
这是一个bug报告，涉及 v0.6.3 版本与 qwen2.5-72b 不兼容的情况。由于 speculative decoding 模式下没有错误，但 Model Input Dumps 没有响应，可能是由于相关环境配置或参数设置不正确导致的问题。

https://github.com/vllm-project/vllm/issues/9410
这个issue属于[Model]类型，主要涉及的对象是添加SDPA attention支持到Molmo视觉主干。由于代码需要添加SDPA实现到Molmo ViT attention，以及清理ViT attention后端选择的逻辑，导致了这个issue的提出。

https://github.com/vllm-project/vllm/issues/9409
这是一个Bug报告，涉及vllm v0.6.2/v0.6.3版本在prompt中存在大量符号（非单词）时容易产生随机输出的问题，表现为生成的结果随机性增加。

https://github.com/vllm-project/vllm/issues/9408
这是一个bug报告类型的issue，主要涉及VLLM 0.6.3版本下glm-4v-9b模型无法使用默认模板的问题，可能是由于服务端无法正确处理请求导致的。

https://github.com/vllm-project/vllm/issues/9406
这是一个CI/Build相关的Issue报告，涉及VLM embeddings的测试。原因可能是测试未通过或者构建过程出现问题。

https://github.com/vllm-project/vllm/issues/9405
这是用户提出的需求。

这个问题单主要涉及vLLM中的功能设置以及针对Cohere的c4aicommandrplus082024模型不支持的情况。

https://github.com/vllm-project/vllm/issues/9404
这是一个用户提出需求的类型issue，主要涉及到对VLLM模型中特定prompt log probability的抽样需求。由于当前的抽样参数返回所有log probability，但用户希望能够有选择地抽样特定的log probability，以解决CUDA OOM的问题。

https://github.com/vllm-project/vllm/issues/9403
这个issue是一个[Kernel]类型的PR，主要涉及Flash attention backend提供了sliding window支持。这个问题主要是为了改进并提高代码质量，以及优化审查过程的效率。

https://github.com/vllm-project/vllm/issues/9402
这是一个bug报告类型的issue，主要涉及的对象是vLLM Docker。导致这个bug的原因可能是无法推断QLoRA适配器。

https://github.com/vllm-project/vllm/issues/9401
这个issue属于bug报告类型，涉及对象为vllm 0.6.3版本，由于无法正确导入模块'get_vllm_engine'导致了问题。

https://github.com/vllm-project/vllm/issues/9400
这个issue类型是关于用户需求和使用问题的，主要涉及到对于vllm分支选择以及优化的疑问。用户想了解应该使用哪个分支来测试推测解码以及哪个分支拥有最佳的推测解码优化。这可能是由于用户想要进行特定功能测试或者寻求性能优化建议引起的。

https://github.com/vllm-project/vllm/issues/9399
这是一个Bug报告，主要涉及的对象是vllm下的一个异步调用引擎。由于使用speculative decoding时出现了AsyncEngineDeadError，可能是由于程序逻辑或异步处理问题导致。

https://github.com/vllm-project/vllm/issues/9398
这是一个Bug报告，涉及的主要对象是vllm中的EngineArgs类，由于传入了一个未预期的关键字参数'num_scheduler_steps'导致了TypeError。

https://github.com/vllm-project/vllm/issues/9397
这是一个bug报告，主要涉及的对象是vllm项目下的molmo模型。由于查询textonly示例时，molmo会抛出`AttributeError: 'NoneType' object has no attribute 'get'`错误，导致bug的发生。

https://github.com/vllm-project/vllm/issues/9396
这是一个关于模型更新和bug修复的issue，涉及vLLM中添加FATReLU激活函数和支持openbmb/MiniCPM-S-1B-sft的问题。原因是为了修复问题并增强模型功能。

https://github.com/vllm-project/vllm/issues/9395
这个issue类型是用户提出需求，主要对象是为VLLM添加Exllama作为压缩张量后端。由于测试中出现了一些问题，用户可能在寻求针对这些特定模型配置的支持或解决方案。

https://github.com/vllm-project/vllm/issues/9394
这是一个bug报告，主要涉及的对象是draft spec decoding model TP size验证逻辑。该bug的原因是当前检查逻辑限制了draft model TP size与目标model TP size的匹配，导致无法支持指定其他尺寸。

https://github.com/vllm-project/vllm/issues/9393
这是一个bug报告，主要涉及前端代码中关于mllama的图片占位符处理问题，导致在添加多张图片时仅显示一个占位符。

https://github.com/vllm-project/vllm/issues/9392
这是一个功能改进的issue，主要涉及到代码重构executor_base ABC，由于不同的后端结构不同且存在重复功能实现，导致难以添加类型检查和代码可读性。

https://github.com/vllm-project/vllm/issues/9391
这个issue是一个BugFix类型的报告，涉及的主要对象是vLLM中的GPU SEGV错误。由于整数溢出导致GPU SEGV错误。

https://github.com/vllm-project/vllm/issues/9390
这是一个用户需求提出的issue，主要涉及benchmarking script的并发限制问题，用户希望添加一个新的flag来设定最大并发请求数。

https://github.com/vllm-project/vllm/issues/9389
这个issue是关于安装问题的bug报告，主要涉及的对象是无法构建aiohttp导致安装pyproject.toml-based项目失败。

https://github.com/vllm-project/vllm/issues/9388
这是一个用户提出需求的issue，涉及的主要对象是对于支持sentence-transformers配置文件的需求。由于新添加的模型需要额外的配置文件来设置汇总方法和是否需要对嵌入进行归一化，用户提出了希望支持这些配置文件的需求。

https://github.com/vllm-project/vllm/issues/9387
这个issue类型是功能需求提出，涉及主要对象是支持Roberta embedding models，由于需要在项目中添加对Roberta embedding models的支持。

https://github.com/vllm-project/vllm/issues/9386
这是一个Bug修复类型的issue，主要涉及CUDA 11.8构建的问题。由于在构建CUDA 11.8时，针对9.0构建的代码不应该包含在scaled_mm_c2x中，但却错误地尝试构建了，导致了问题的产生。

https://github.com/vllm-project/vllm/issues/9385
这个issue类型是bug报告，主要涉及的对象是vllm安装的依赖项Triton，用户提出希望能更新依赖到新版本3.1以简化AMD支持指令。

https://github.com/vllm-project/vllm/issues/9384
这是一个性能改进类型的issue，主要涉及到vllm中的fused_moe层的性能优化，涉及到增加线程数、替换torch.sum函数、移动源文件等操作。

https://github.com/vllm-project/vllm/issues/9383
这是一个性能问题的报告，用户在尝试对大型模型进行批量推断时遇到了问题。

https://github.com/vllm-project/vllm/issues/9382
这个issue类型为需求提出，主要涉及的对象是支持Zyphra/Zamba2-7B模型的集成。由于尚未支持Mamba2，导致无法轻松支持Zamba2-7B模型，尤其是在使用`use_shared_attention_lora`的情况下可能更加复杂。

https://github.com/vllm-project/vllm/issues/9381
这是一个Bug报告，涉及到VLLM的compressed-tensors解析过程中的静默失败问题，导致`input_activations`设置为None而引发的下游错误。

https://github.com/vllm-project/vllm/issues/9380
这是一个关于修改transformers-neuronx库中简化模型加载的issue，类型为代码优化，主要涉及模型加载的流程简化问题。

https://github.com/vllm-project/vllm/issues/9379
这是一个bug报告，涉及的主要对象是vLLM下的CPU服务器。由于无法正确嵌入任何文本，导致了Embeddings端点抛出异常。

https://github.com/vllm-project/vllm/issues/9378
这个issue是一个Bug报告，涉及主要对象为模型加载过程中使用了enable_lora和8个GPU。由于CUDA图形捕获导致内存占用过高，可能导致运行时崩溃或内存不足的问题。

https://github.com/vllm-project/vllm/issues/9377
这是一个Bug报告，该问题涉及到vllm库中模型架构的支持问题，由于模型架构'LlavaForConditionalGeneration'未得到支持导致数值错误。

https://github.com/vllm-project/vllm/issues/9376
这个issue是一个Bug报告，主要涉及vllm项目中的一个模块在使用中出现了KeyError错误。

https://github.com/vllm-project/vllm/issues/9375
这个issue类型是bug报告，主要涉及的对象是构建Docker镜像时的文件忽略规则。这个问题的原因是在`.dockerignore`中包含了`.github`导致源代码拷贝时产生脏状态，导致版本号不匹配标签，同时还添加了`tools/check_repo.sh`用于检查仓库状态。

https://github.com/vllm-project/vllm/issues/9374
这个issue类型是bug报告，涉及的主要对象是Neuron的启动问题，由于Neuron使用了旧版本的torch (2.1)和transitively triton，导致了Markdown渲染不起作用的问题。

https://github.com/vllm-project/vllm/issues/9373
这是一个bug报告，问题主要涉及Tensor Parallelism执行性能较差，可能是由于硬件环境和代码执行引起的。

https://github.com/vllm-project/vllm/issues/9372
这个issue类型是优化改进，主要涉及VLLM的测试整合。由于VLLM的多模态测试存在冗余，需要对测试进行整合以提高效率。

https://github.com/vllm-project/vllm/issues/9371
这是一个Bug报告，主要涉及的对象是运行benchmark_throughput.py时设置tp>1导致进程终止，原因可能是CUDA相关配置的问题。

https://github.com/vllm-project/vllm/issues/9370
这个issue是一个[CI][Misc]类型的issue，涉及添加针对python-only development的测试。由于markdown渲染不起作用，所以使用了原始的html。

https://github.com/vllm-project/vllm/issues/9369
这是一个bug报告，涉及到vLLM模型无法在TP大于1时运行，可能是由于NCCL初始化错误导致的问题。

https://github.com/vllm-project/vllm/issues/9368
这是一个用户提出需求的类型的issue，主要对象是关于支持prompt缓存的功能。用户希望能够在模型的多次推理过程中固定常用指令和特定部分的提示，以提高效率和性能。

https://github.com/vllm-project/vllm/issues/9367
这个issue类型是bug报告，涉及的主要对象是vllm/vllm-openai容器。导致该bug的原因是CUDA内存不足，触发了torch.OutOfMemoryError。

https://github.com/vllm-project/vllm/issues/9366
这是一个bug报告，主要涉及使用vllm加载lora适配器进行模型推理时需要耗费过长时间且效果降低的问题。

https://github.com/vllm-project/vllm/issues/9365
这是一个bug报告，主要涉及到vllm中调用函数时输出中文出现了反斜杠字符的问题。原因可能是字符编码或者转义字符处理不正确。

https://github.com/vllm-project/vllm/issues/9364
这是一个bug报告，涉及的主要对象是vLLM和Cling。由于400 Bad Request错误，用户在使用Cling与vLLM时遇到问题，希望知道如何修改vLLM或Cling以解决此问题。

https://github.com/vllm-project/vllm/issues/9363
这是一个Bug报告，涉及的主要对象是在多个GPU上运行Qwen2-VL-72B推理的过程。这个问题由于内存问题导致了错误。

https://github.com/vllm-project/vllm/issues/9361
这是一个用户寻求帮助的issue，主要涉及到如何在Vllm中托管用户fine-tuned Llama -3-8b instruct模型。该用户在集成部分遇到困难，需要关于使用Vllm的指导以及学习链接建议。

https://github.com/vllm-project/vllm/issues/9360
这是一个Bug报告，主要涉及的对象是运行Molmo模型时出现了错误。原因可能是模型无法正确运行导致的错误。

https://github.com/vllm-project/vllm/issues/9359
这是一个关于bug报告的issue，主要涉及到使用guided_json参数时在Pixtral上使用OpenAI API时出现的问题。导致问题的原因可能是guided_json参数对特定VLMs如Pixtral不起作用。

https://github.com/vllm-project/vllm/issues/9358
这是一个关于GitHub上vLLM项目下的一个bug报告，主要对象是关于markdown渲染问题导致的无法正确显示内容的bug。

https://github.com/vllm-project/vllm/issues/9357
这是一个bug报告，涉及chat API continuous usage stats，由于completion_tokens统计错误导致结果不累积，需要修复。

https://github.com/vllm-project/vllm/issues/9356
这是一个Bug报告，主要涉及的对象是 llama3.2-11B-Vision-Instruct 模型。原因是模型读取版本号时出现了错误。

https://github.com/vllm-project/vllm/issues/9355
这个issue是一个Bug报告，涉及的主要对象是vLLM中的draft_tp值。由于设置了draft_tp值但target_tp>1时出现逻辑错误，导致了bug。

https://github.com/vllm-project/vllm/issues/9354
这是一个Bug报告，主要涉及到v0.6.3版本的vllm安装问题。由于环境中缺少vllm._C模块导致安装失败。

https://github.com/vllm-project/vllm/issues/9353
这是关于需求的问题，涉及主要对象是vllm下的cu118版本的WHL包，由于v0.6.3和v0.6.2版本缺少对应的cu118 WHL包，导致用户无法通过WHL包安装VLLM。

https://github.com/vllm-project/vllm/issues/9352
这是一个bug报告，主要涉及的对象是关于peak memory profiling在multimodal mllama模型中的使用。由于当前的测量peak memory使用的策略受到垃圾回收的影响，因此可能出现过高或者过低估计peak memory使用，导致产生了bug症状。

https://github.com/vllm-project/vllm/issues/9351
这是一个bug报告，主要涉及对象是InternVL的输入映射器。由于原来的输入映射器只输出像素值，导致无法识别图像嵌入输入，需要更新以支持图像嵌入输入。

https://github.com/vllm-project/vllm/issues/9350
这是一个Bug报告，主要涉及的对象是TPU SMEM OOM，由于Pallas paged attention kernel在`max_model_len`较大时导致SMEM OOM，通过拆分批处理维度为子批次来减少SMEM使用量，暂时解决了SMEM OOM问题。

https://github.com/vllm-project/vllm/issues/9349
这是一个关于功能缺陷的bug报告，主要涉及Benchmark Serving功能模块的参数传递问题。导致这个bug的原因是ignore_eos参数没有在所有的benchmark_serving调用中正确传递，导致不同后端的输出长度可能会不一致。

https://github.com/vllm-project/vllm/issues/9348
这个issue类型是bug报告，主要涉及的对象是代码格式错误。原因是出现了一个无法显示的错误"!image"。

https://github.com/vllm-project/vllm/issues/9347
这是一个用于解决构建问题的类似bug报告的issue，涉及主要对象为Readthedocs的PDF构建，由于PDF构建导致最新版本未更新的问题。

https://github.com/vllm-project/vllm/issues/9346
这是一个用户提出需求的类型的问题，涉及到获取成功率/错误率的度量指标的问题。用户希望了解如何通过分母来计算成功率百分比指标，并询问了是否存在错误计数度量以便计算成功率。

https://github.com/vllm-project/vllm/issues/9345
这是一个bug报告，涉及前端的模型类型错误信息不清晰，导致在聊天模式下，多模态模型不匹配时无法明确匹配类型。

https://github.com/vllm-project/vllm/issues/9344
这个issue类型是[Hardware][CPU]支持压缩张量INT8 W8A8 AZP，涉及主要对象是vLLM。由于markdown渲染问题，无法正确显示内容导致需要使用raw html格式，这可能是由于渲染引擎或者插件的问题。

https://github.com/vllm-project/vllm/issues/9343
这是一个清理代码的issue，不是bug报告，涉及到mamba.py文件。原因可能是代码中存在一些无用的冗余代码，需要进行清理。

https://github.com/vllm-project/vllm/issues/9342
这是一个bug报告，主要涉及的对象是vllm项目中的优先级调度问题。由于优先级调度时发生抢占导致vllm崩溃，用户提出了关于此bug的问题并寻求帮助。

https://github.com/vllm-project/vllm/issues/9341
这是一个Bug报告类型的issue，主要涉及LLAMA 3.2 11B Vision Instruct Model在VLLM 0.6.2版本上无法运行的问题。造成这个问题的原因可能是环境配置或软件版本不匹配所导致的错误。

https://github.com/vllm-project/vllm/issues/9340
这个issue属于建议类型，涉及到在容器映像中添加opentelemetry软件包。由于当前环境下缺乏opentelemetry软件包，用户提出建议将其包含在容器映像中。

https://github.com/vllm-project/vllm/issues/9339
这是一个Bug报告，涉及CPU offload参数无效的问题，用户在使用vllm时设置cpuoffloadgb参数却无效。

https://github.com/vllm-project/vllm/issues/9338
这是一个功能需求的issue，涉及vLLM中的前端部分，主要是添加支持Goodput metric用于benchmarking serving system。

https://github.com/vllm-project/vllm/issues/9337
这个issue是关于向vLLM的Intel GPU添加多步调度器的一个硬件特定更改。

https://github.com/vllm-project/vllm/issues/9336
这是一个用户提出需求的类型，主要对象是vllm模型，用户提出了允许`max_tokens=0`的需求。

https://github.com/vllm-project/vllm/issues/9335
这是一个bug报告，涉及问题单中的主要对象是multiprocessing engine。这个问题可能是由于某种原因导致'Finished request xxxx' log在日志中缺失所引起的。

https://github.com/vllm-project/vllm/issues/9334
这是一个[CI/Build]类型的issue，主要关注构建或持续集成方面的改进。这个问题主要是关于构建vllm wheel package时，根据环境变量设置cuda extra archs(FATBIN)，以支持在多个CUDA设备上运行。在之前的情况下，构建在A10设备上的包无法在H20设备上运行，导致出现`no kernel image is available for execution on the device`错误。因此，需要为不同的CUDA设备构建不同的wheels，不够方便。

https://github.com/vllm-project/vllm/issues/9333
这是一个性能优化的问题，主要涉及到vllm中ngram查找性能的提升，原因是CPU GPU通信和ngram构建中的小内核导致了较长的提取时间。

https://github.com/vllm-project/vllm/issues/9332
这个issue是一个bug报告，主要涉及vllm和transformer之间的权重不一致，可能是由于不同模型导致的。

https://github.com/vllm-project/vllm/issues/9331
这个issue类型是bug报告，主要涉及TPU单主机v5e-8上的HBM OOM问题，并由于加载模型权重时发生内存溢出。

https://github.com/vllm-project/vllm/issues/9330
这是一个bug报告，问题涉及QWEN2-VL模型推理环境下模型速度的异常，用户询问为什么2b模型只比7b模型略快。

https://github.com/vllm-project/vllm/issues/9329
这是一个Bug报告，涉及的主要对象是VLLM（Very Large Language Model）。由于NCCL错误导致的CUDA错误，出现了异常。

https://github.com/vllm-project/vllm/issues/9328
这个issue是一份未提交的PR描述，涉及的主要对象是vLLM项目。原因可能是markdown渲染不起作用，需要使用原始的HTML代码。

https://github.com/vllm-project/vllm/issues/9327
这个issue是一个Bug报告，主要对象是CC（Model）Support Mamba，由于调用`get_attn_backend`导致了除以零错误。

https://github.com/vllm-project/vllm/issues/9326
这个issue是关于bug报告，涉及的主要对象是Gemma 27B模型，可能由于某种原因导致该模型没有产生输出。

https://github.com/vllm-project/vllm/issues/9325
这个issue属于[Model]，主要涉及FalconMamba模型的支持。用户提交了一个PR来增加对FalconMamba模型的支持，该模型是不带前馈块且具有B_C_dt_rms LayerNorm的Mamba类，该问题是为了在vllm中支持FalconMamba模型。

https://github.com/vllm-project/vllm/issues/9324
该issue类型为功能需求，涉及的主要对象是LLaVA OneVision模型的quantization支持。由于应用需要在资源有限的硬件上运行，需要对模型进行量化处理，但目前的模型尚不支持BitsAndBytes量化。

https://github.com/vllm-project/vllm/issues/9323
该issue类型为用户提出需求，主要涉及的对象是vllm的支持扩展到rhymes-ai/Aria模型。这是由于用户希望vllm能够支持rhymes-ai/Aria模型，提出了这个需求。

https://github.com/vllm-project/vllm/issues/9322
这个issue属于代码优化类型，主要对象是vllm/attention/ops/triton_flash_attention.py文件。其中提出了删除与dropout相关内容以简化代码的建议，可能是为了提高代码清晰度和简洁性。

https://github.com/vllm-project/vllm/issues/9321
这是一个Bug报告，涉及的主要对象是vLLM模型。由于频繁使用后突然出现错误，在特定请求时停止工作并无法返回任何输出，并且出现了日志截断的情况。

https://github.com/vllm-project/vllm/issues/9320
这是一个bug报告，主要涉及vLLM部署实现OpenAI API时生成模型使用llama 3 8b instruct执行RAG任务时，出现模型生成不停的问题，可能是由于llama执行api调用时发生了问题导致。

https://github.com/vllm-project/vllm/issues/9319
这是一个bug报告，该问题主要涉及vllm在AMD MI60环境下的部署问题，用户反馈推断过程中出现错误，可能是由于缺少paged_attention导致的。

https://github.com/vllm-project/vllm/issues/9318
这是一个bug报告，主要涉及的对象是VLLM中的LLMEngine。由于代码中引用了LLMEngine对象的一个不存在的属性'driver_worker'，导致出现了AttributeError。

https://github.com/vllm-project/vllm/issues/9317
这是一个bug报告，涉及的主要对象是CI（持续集成），由于合并冲突导致CI失败，需要修复`max_decode_query_len`字段的合并冲突。

https://github.com/vllm-project/vllm/issues/9316
这是一个bug报告，涉及的主要对象是Mixtral 8x22B模型，由于quantizing后出现KeyError导致shard loading错误。

https://github.com/vllm-project/vllm/issues/9315
这个issue是关于测试的需求，主要涉及到vllm的Python-only开发。原因是需要在pythononly开发中编写一些测试，可能由于缺乏测试而导致潜在的问题。

https://github.com/vllm-project/vllm/issues/9314
该issue属于建议性反馈（RFC），主要涉及到了vllm中关于让每个模型都能成为奖励模型/嵌入模型的讨论。由于对奖励模型的兴趣日益增长，该RFC旨在通过让所有生成模型都能作为嵌入模型提供，从而支持处理监督奖励模型（PRM）。

https://github.com/vllm-project/vllm/issues/9313
这是一个bug报告类型的issue，关于vllm模型中使用`copy_`和`=`方法改变参数导致输出结果不一致的问题。

https://github.com/vllm-project/vllm/issues/9312
这是一个bug报告，涉及到VLLM项目中的api_server.py在选择工具调用解析器时出现错误。这可能是由于未更新的文档或者程序bug导致的。

https://github.com/vllm-project/vllm/issues/9311
这是一个Bug报告类型的Issue，涉及更新grafana仪表盘时的问题，主要原因是模板数据源在查询FIX CC中没有正确使用。

https://github.com/vllm-project/vllm/issues/9310
这是一个关于bug报告的issue，主要涉及vllm中调用mistralaiCodestral22Bv0.1时消息总是被截断，可能是由于未知原因导致的。

https://github.com/vllm-project/vllm/issues/9309
这是一个关于改进安装脚本和相关文档的请求，而不是一个bug报告。

https://github.com/vllm-project/vllm/issues/9308
这是一个bug报告类型的issue，主要涉及CUDA错误导致的异常退出问题。

https://github.com/vllm-project/vllm/issues/9307
这是一个bug报告类型的issue，主要涉及VLLM的docker构建问题，可能由于VLLM_MAX_SIZE_MB参数设置错误导致建构错误。

https://github.com/vllm-project/vllm/issues/9306
这是一个bug报告，涉及到CUDA错误导致内存访问违规。

https://github.com/vllm-project/vllm/issues/9305
该issue是关于安装错误的bug报告，主要涉及vllm的安装问题。由于pip安装构建依赖失败，导致了安装vllm时出现错误。

https://github.com/vllm-project/vllm/issues/9304
这是一个bug报告，涉及到Hermes 2 Pro Tool parser无法定位工具调用起始/结束标记的问题。这一问题是由于glm49bchat, MiniCPM34B, llama3.1工具调用解析器设置的错误导致的。

https://github.com/vllm-project/vllm/issues/9303
这是一个功能需求，主要涉及支持VLM2Vec多模态嵌入模型。原因是希望添加CLI选项以指定生成或嵌入模型，增加多模态嵌入API以及支持更多多模态嵌入模型。

https://github.com/vllm-project/vllm/issues/9302
这个issue类型为需求提出，主要涉及对象是vllm的核心功能。这个需求由于更好地隐藏seq group而导致于处理并行采样在llm引擎中。

https://github.com/vllm-project/vllm/issues/9301
这个issue属于功能需求报告，主要涉及的对象是TPU集成`compressed-tensors`以支持w8a8和w8a16压缩。原因是需要为TPU集成这些新的压缩支持。

https://github.com/vllm-project/vllm/issues/9300
这个issue是一个特性需求，主要涉及到vLLM中fine-grained CustomOp启用机制的实现。原因可能是为了提高代码质量和改进代码审查流程。

https://github.com/vllm-project/vllm/issues/9299
这个issue是一个Bugfix类型的问题，涉及的主要对象是custom ops。由于部分custom ops需要支持动态维度，但当前的实现不支持传递自定义的C++类给custom ops，因此做出了相关修改以支持动态维度。

https://github.com/vllm-project/vllm/issues/9298
这是一个非bug类issue，主要涉及的对象是`flash_attn_varlen_func`和MQA scorer，用户在请求中支持不同长度的不同提议，可能是为了支持ngram和动态推理解码。

https://github.com/vllm-project/vllm/issues/9297
这是一个关于CI/Build的类型为[CI/Build]的issue，主要涉及到多步骤测试与TPUs的问题。由于该PR作者提交时未按照规范填写标题和描述，导致需要添加测试来规范代码质量和提高审查效率。

https://github.com/vllm-project/vllm/issues/9296
这是一个合并前端beam search实现的issue，属于功能改进类型，主要涉及到前端代码实现。

https://github.com/vllm-project/vllm/issues/9295
这是一个bug报告，涉及主要对象是修复f-string在错误显示上的问题。由于markdown渲染不起作用，所以在PR checklist部分使用了原始的HTML代码。

https://github.com/vllm-project/vllm/issues/9294
这是一个bug报告，主要涉及到对模型输出层进行剪枝后导致模型输出完全随机的问题。

https://github.com/vllm-project/vllm/issues/9293
这是一个关于在collect_env.py工具中添加环境变量收集的问题，用户遇到的问题多与环境变量相关，通过在collect_env.py工具中整合此信息，可以更容易地调试用户问题。

https://github.com/vllm-project/vllm/issues/9292
该issue类型为新特性需求，主要涉及的对象是支持Mamba2模型。这个需求是由于要添加对Mamba2的支持，需要进行集成测试、支持分块填充、修复张量并行性等工作。

https://github.com/vllm-project/vllm/issues/9291
这是一个关于提出需求的issue，主要涉及PrefillDecode与Speculative Decoder的整合，由于需要支持chunk prefill和spec decoding。

https://github.com/vllm-project/vllm/issues/9290
这是一个bug报告，主要涉及vLLM模型的内存利用问题，可能由于`gpumemoryutilization`参数没有完全生效，导致在特定设置下出现内存溢出的bug。

https://github.com/vllm-project/vllm/issues/9289
这个issue类型是需求提出，主要对象是实现vLLM V1功能。这个需求由于项目开发需要，要求实现vLLM V1功能。

https://github.com/vllm-project/vllm/issues/9288
这个issue属于bug报告，主要涉及到FP8 Quantization example使用过时的Ammo库而非Nvidia TensorRT Model Optimizer，导致该示例无法运行。

https://github.com/vllm-project/vllm/issues/9287
这是一个文档更新类型的issue，涉及的主要对象是软件的功能模块。导致此issue的原因是过时的注释可能导致误解。

https://github.com/vllm-project/vllm/issues/9286
这是一个bug报告，主要涉及MiniCPM的LoRA功能存在问题，可能是由于启用LoRA后导致的问题。

https://github.com/vllm-project/vllm/issues/9285
这是一个bug报告，主要涉及对象是VLLM项目中的`max_decode_seq_len`参数。原因是参数名拼写错误导致长于`max_seq_len_to_capture`的序列仍被误认为是cuda图捕获，进而导致程序崩溃。

https://github.com/vllm-project/vllm/issues/9284
这是一个bug报告，涉及到vllm的api中关于图像参数的问题，导致了400 Bad Request错误的症状。

https://github.com/vllm-project/vllm/issues/9283
这是一个bug报告，主要涉及vllm模型在同时进行多次调用时性能可能下降的问题。造成这个bug的原因可能与模型异步调用的方式有关。

https://github.com/vllm-project/vllm/issues/9282
这个issue是关于bug报告，主要涉及的对象是MiniCPM3-4B模型。由于Lora支持的问题，导致模型输入无响应并出现错误。

https://github.com/vllm-project/vllm/issues/9281
这是一个功能需求报告，涉及到添加HPU手动种子设置到`seed_everything`方法中。

https://github.com/vllm-project/vllm/issues/9280
这是一个Bug报告类型的issue，涉及的主要对象是VLLM对LoRa配置`modules_to_save`的支持问题，可能是由于未支持该参数导致用户在使用时遇到了错误。

https://github.com/vllm-project/vllm/issues/9279
这是一个Bugfix类型的issue，主要涉及的对象是vLLM中AMD测试并行作业中的Shard ID参数设置问题，因为在`runamdtest.sh`脚本中赋值`shardid`参数的替换只在`shardid==`后有空格时才进行，导致多个作业中的shardid都变为零，这个问题是由于`commands`变量在每次循环迭代时被覆盖导致。

https://github.com/vllm-project/vllm/issues/9278
这是一个bug报告 issue，涉及的主要对象是benchmark_serving.py 文件。由于没有提供 `ignore_eos` 参数，在测试输入时出现了问题。

https://github.com/vllm-project/vllm/issues/9277
这是一个Bug报告，涉及的主要对象是multiprocessing engine。这个问题由于priority未正确传递到实际的engine，导致了优先调度无法正常工作。

https://github.com/vllm-project/vllm/issues/9276
这个issue是一个bug报告，主要涉及的对象是在使用`skip_tokenizer_init`和`num_scheduler_steps`时出现了错误。这个bug是由于未初始化tokenizer导致的。

https://github.com/vllm-project/vllm/issues/9275
这个issue是一个用户提出需求的类型，主要对象是vllm中LoRA模型的权重加载功能。由于LoRA权重保存格式不支持正则表达式形式，导致无法加载`target_modules`。

https://github.com/vllm-project/vllm/issues/9274
这是一个用户提出需求的issue，主要涉及VLLM推理引擎的人工增加推理时间的问题。用户想通过手动方式延长Qwen2.5在服务器上的推理时间，并受到Noam Brown关于推理时间缩放的启发。

https://github.com/vllm-project/vllm/issues/9273
这个issue类型是用户提出需求，主要对象是VLLM 0.6.2版本的vllm-flash-attn模块，可能是因为用户想确认是否需要单独安装flash-attn。

https://github.com/vllm-project/vllm/issues/9272
这是一个bug报告，涉及到VLLM项目中的优先级调度无法正常工作的问题。原因是客户端传递的优先级没有被正确地传递到VLLM引擎中，导致优先级无法生效。

https://github.com/vllm-project/vllm/issues/9271
这是一个用户提出需求的issue，主要涉及的对象是benchark_throughput脚本，用户希望暴露max_num_seqs参数给该脚本。

https://github.com/vllm-project/vllm/issues/9270
该issue类型是用户寻求关于vllm推理代码的帮助，涉及主要对象是特定模型推理集成问题。用户希望了解如何在vllm中运行特定模型的推理，由于缺乏集成指引而提出疑问。

https://github.com/vllm-project/vllm/issues/9269
这是一个bug报告，涉及的主要对象是安装VLLM时出现的错误。该问题是由于使用不支持的特性导致的bug。

https://github.com/vllm-project/vllm/issues/9268
该issue属于用户提出需求类型，主要涉及实现设备无关性以支持多样化硬件的问题，原因是当前针对不同硬件设备需要实现独立的Worker/Executor/Model Runner框架，导致代码冗余和维护困难。

https://github.com/vllm-project/vllm/issues/9267
这个issue类型是代码优化，主要涉及mypy类型检查，由于代码扩展导致部分错误的问题。

https://github.com/vllm-project/vllm/issues/9266
这是一个关于bug报告的issue，主要涉及到PyTorch的 CPU draft model 使用原生的 forward 函数。这个问题可能是由于CPU draft模型在执行forward过程中出现了异常或错误导致的。

https://github.com/vllm-project/vllm/issues/9265
这是一个用户提出需求的issue，主要涉及改进logging for embedding models，由于当前的logging metrics不够native导致缺少KV cache和generation tokens的问题。

https://github.com/vllm-project/vllm/issues/9264
这是一个Bug报告，主要涉及到新增的beam search实现忽略了停止条件，导致在聊天模式中无法停止。

https://github.com/vllm-project/vllm/issues/9263
这是一个Bug报告，主要涉及的对象是AsyncLLMEngine，由于请求过长导致引擎在返回时卡住，需要发送更多请求才能解除卡住状态。

https://github.com/vllm-project/vllm/issues/9262
该issue属于用户提出需求类型，主要涉及的对象是持续集成 (CI) 构建流程。这个问题是因为测试在开发期间意外运行了几次，需要在代码中添加注释以明确只用于测试个别模型，不应合并到主分支测试。

https://github.com/vllm-project/vllm/issues/9261
这是一个用户提交需求的问题单，主要涉及到隐藏"best_of"功能，用户提出希望代表提交的需求。

https://github.com/vllm-project/vllm/issues/9260
这是一个Bug报告，涉及vLLM服务的Streaming response功能在使用过程中出现异常，导致仅返回一个token然后产生异常日志。

https://github.com/vllm-project/vllm/issues/9259
该issue属于需求提出类型，涉及主要对象为Mergify自动标记PRs的配置。由于目前需要更多路径以实现更全面的标记，故用户提出了此需求。

https://github.com/vllm-project/vllm/issues/9258
这是一个关于编译器通用修饰器的建议类型的issue，主要涉及的对象是torch.compile模块。由于缺乏通用修饰器功能，可能导致用户在编译器的使用中遇到一些限制或不便。

https://github.com/vllm-project/vllm/issues/9257
这个issue属于bug报告，涉及主要对象为vLLM中的Neuron模块。由于Neuron当前使用的`torch`版本与`triton`存在依赖性冲突，导致与vLLM最新版本不兼容的错误，用户需解决该依赖性问题。

https://github.com/vllm-project/vllm/issues/9256
这个issue属于用户提出需求，并主要涉及改进快速入门文档。原因可能是现有文档在系统要求、安装步骤、术语解释等方面不够清晰，导致新用户在快速生成项目时遇到困难。

https://github.com/vllm-project/vllm/issues/9255
这是一个bug报告，主要涉及vllm服务器无法在离线情况下提供模型的问题。可能是由于缺少适当机制支持在本地应用程序中使用已经下载的模型而导致。

https://github.com/vllm-project/vllm/issues/9254
这是一个bug报告类型的issue，主要涉及的对象是docker build过程中CUDA archs < 7.0被错误检测的问题。

https://github.com/vllm-project/vllm/issues/9253
这是一个bug报告，主要涉及vllm中新的beam search实现忽略了停止条件导致的问题。

https://github.com/vllm-project/vllm/issues/9252
这是一个[CI/Build]类型的issue，涉及Dockerfile.cpu文件中的`PIP_EXTRA_INDEX_URL`配置。用户提出需要通过构建参数来配置`PIP_EXTRA_INDEX_URL`，因为目前只支持默认值"https://download.pytorch.org/whl/cpu"，而某些用户需要从镜像仓库或其他存储库获取pytorch whl。

https://github.com/vllm-project/vllm/issues/9250
这是一个bug报告，涉及主要对象是RoPE处理方式，问题是由于vLLM中RoPE代码未按规范配置所致。

https://github.com/vllm-project/vllm/issues/9249
这是一个Bug报告，问题涉及到Llama 3.2 Vision的RuntimeWarning导致coroutine未被await，可能由于未正确调用异步函数all_mm_data而产生。

https://github.com/vllm-project/vllm/issues/9248
这是一个bug报告，主要涉及到在添加Qwen2-Audio模型支持时遇到的问题。这个问题是由于模型架构'Qwen2AudioForConditionalGeneration'目前不受支持而导致的。

https://github.com/vllm-project/vllm/issues/9247
这是一个用户提出需求的类型。该问题单涉及的主要对象是FlashAttentionBackend和paged_attention_kernel。原因是用户不清楚FlashAttentionBackend如何与paged attention相关联，导致提出了关于两者之间关系的问题。

https://github.com/vllm-project/vllm/issues/9246
这是一个bug报告。该问题涉及vllm的安装过程，并因为某些原因导致在4090版本上编译错误。

https://github.com/vllm-project/vllm/issues/9245
这是一个特性增强的issue，主要涉及Kernel中添加fused moe kernel配置，提升了吞吐量和延迟性能。

https://github.com/vllm-project/vllm/issues/9244
这是一个Bug报告，涉及到vllm项目的安装问题，由于uv pip无法正确处理pip安装最新wheels的命令，导致无法成功安装。

https://github.com/vllm-project/vllm/issues/9243
这个issue是关于Bug报告，主要涉及VLLM的启动失败以及使用FLASHINFER时报错的问题。

https://github.com/vllm-project/vllm/issues/9242
这个issue是一个feature请求，涉及的主要对象是为vllm添加GLM-4v支持。由于GLM4V部署报告KeyError: 'transformer.vision.transformer.layers.45.mlp.fc2.weight'，需要修复此问题。

https://github.com/vllm-project/vllm/issues/9241
这是一个关于bug报告的issue，主要涉及版本从0.5.3到0.6.2间在async_llm_engine.py文件中打印推理指标时输出不显示的问题。

https://github.com/vllm-project/vllm/issues/9240
This issue is a question about the inference performance of the GPTQ model. The user is inquiring about the reasons behind the difference in inference efficiency between the nonquantized model and models using GPTQ and GPTQ Marlin kernels.

https://github.com/vllm-project/vllm/issues/9239
这个issue是一个bug报告，主要涉及到在P40设备上通过容器安装vllm时产生的构建错误。导致这个问题的原因可能是环境配置或软件安装过程中的问题。

https://github.com/vllm-project/vllm/issues/9238
这是一个关于性能问题的bug报告，主要涉及到vllm处理大图像预处理时速度过慢，可能是由于最新代码构建引起的性能退化。

https://github.com/vllm-project/vllm/issues/9237
这是一个关于在vLLM中支持输入图像嵌入(minicpmv)的模型问题，由于不同多模态模型之间多图像输入格式的一致性很难实现，作者采用了一种基于字典的方法来附加额外参数。

https://github.com/vllm-project/vllm/issues/9236
这是一个bug报告，涉及到Qwen2-VL-72B在Docker部署API服务时出现了AssertionError，可能是由于IPC路径相关的问题导致。

https://github.com/vllm-project/vllm/issues/9235
这是一个涉及修复采样问题的工作，属于Bugfix类型的issue，主要涉及vLLM的sonnet采样操作。

https://github.com/vllm-project/vllm/issues/9234
这是一个bug报告，涉及vllm项目中的quantization在动态形状下的兼容性问题，由于在C++端注册自定义操作时会导致动态形状直接特化为整数而失败，建议改为在Python端注册自定义操作。

https://github.com/vllm-project/vllm/issues/9233
这个issue类型是改进型，主要对象是模型支持信息的收集过程。由于之前的改进未统一在一个进程中收集模型支持信息，导致需要进行后续跟进来优化这一过程。

https://github.com/vllm-project/vllm/issues/9232
这是一个Bug报告，涉及的主要对象是vLLM模型运行环境。由于GPU内存未正确清理导致初始化vLLM实例时出现错误。

https://github.com/vllm-project/vllm/issues/9231
这是一个关于调试自定义LLMs中分页注意力问题的bug报告，主要涉及到VLLM模型和HF模型间生成结果不符的问题，可能是由于分页注意力中kv_cache数值不匹配导致的。

https://github.com/vllm-project/vllm/issues/9230
这是一个bug报告，主要涉及VLLM中压测出现的AsyncLLMEngine has failed导致服务器进程终止的问题。

https://github.com/vllm-project/vllm/issues/9229
这是一个测试编译前进问题的bug报告，主要涉及编译功能。由于编译过程出现问题，用户提出了此issue。

https://github.com/vllm-project/vllm/issues/9228
这是一个用户提出的需求，主要对象是希望在AARCH64架构上启用vLLM，以支持ARM处理器，旨在拓展vLLM在不同应用场景中的可用性。

https://github.com/vllm-project/vllm/issues/9227
这个issue是关于一个 bug 报告，涉及的主要对象是 llama。这个bug的症状是 Llama 与 Lora 无法启动。

https://github.com/vllm-project/vllm/issues/9226
这是一个bug报告，涉及vllm项目在dockerfile中无法安装的问题，可能是由于特定的提交引起的。

https://github.com/vllm-project/vllm/issues/9225
这是一个Bug报告类型的issue，主要涉及vllm v0.6.2在多GPU上崩溃的问题，原因可能是端口被占用导致绑定地址错误。

https://github.com/vllm-project/vllm/issues/9224
这是一个需求提议类型的issue，主要涉及构建自定义操作（custom ops）同时支持CPU和CUDA，用户提出了需要根据张量设备进行分发的建议。

https://github.com/vllm-project/vllm/issues/9218
这是一个bug报告，涉及的主要对象是Machete unittests，由于`machete_supported_schedules`在CUDA下被注册但没有tensor参数，导致导致`NotImplementedError`，需要将其dispatch key移回`CatchAll`。

https://github.com/vllm-project/vllm/issues/9217
该issue属于功能改进类型，主要涉及的对象是VLM的vision encoder。原因是为了维护一致性，在传递`QuantizationConfig`和关联的`prefix`参数给vision towers时，添加了注释忽略了尚未通过现有方法进行量化的vision tower。

https://github.com/vllm-project/vllm/issues/9216
这是一个bug报告类型的issue，涉及的主要对象是vllm 6.0中的gptq4bits功能。由于并发达到10时出现错误，可能是导致了该bug的根本原因。

https://github.com/vllm-project/vllm/issues/9215
这是一个Bug报告，涉及到使用H20进行多机器推断时通过ray集群导致推断崩溃的问题。

https://github.com/vllm-project/vllm/issues/9213
这是一个bug报告，涉及的主要对象是DeepSeekCoderV2LiteInstruct模型。由于DeepSeekCoderV2LiteInstruct模型在TP=2与fp8 marlin下过小，导致weight loading multiple gpu "large"测试失败。

https://github.com/vllm-project/vllm/issues/9212
这个issue类型是bug报告，主要涉及的对象是Machete中的TMA组件。这个问题由于传递给TMA组件的尺度参数形状设置错误导致了无效结果的bug。

https://github.com/vllm-project/vllm/issues/9211
该issue属于用户请教问题类型，涉及主要对象为如何使用Raw query在Prometheus中计算vllm部署模型服务的QPS指标，用户想了解如何通过Raw query来计算在线模型服务的请求并发量。

https://github.com/vllm-project/vllm/issues/9210
这个issue类型是代码贡献（pull request）的请求，主要对象是核心组件（core components），由于代码的重要性需要仔细审查，因此提出了给关键部分指定codeowners的建议。

https://github.com/vllm-project/vllm/issues/9209
这个issue是一个bug报告，涉及的主要对象是工具（tool）的调用和结束原因（finish reason）。导致问题的原因是在使用带有streaming特性的tool_choice时，finish_reason显示为"stop"而不是"tool_calls"。

https://github.com/vllm-project/vllm/issues/9208
这个issue属于bug报告类型，主要涉及模型支持检查的改进，由于之前的检测方式容易出错，现在需要改为使用相同的技术来通信结果。

https://github.com/vllm-project/vllm/issues/9207
这个issue是bug报告，主要涉及Llama with Lora模型无法启动的问题，可能由于环境配置或代码错误导致。

https://github.com/vllm-project/vllm/issues/9206
这是一个关于功能需求的issue，主要涉及vLLM中简单的数据并行性，由于用户希望在单台机器上部署多个vLLM实例，导致需要实现数据并行处理。

https://github.com/vllm-project/vllm/issues/9205
这是一个关于安装问题的bug报告，主要涉及到VLLM的安装，可能是由于包哈希不匹配导致的错误。

https://github.com/vllm-project/vllm/issues/9204
该issue是关于文档改进的，主要涉及调试文档的提升。由于现有文档可读性较差，可能影响用户理解，因此需要对文档进行优化。

https://github.com/vllm-project/vllm/issues/9203
这是一个关于提出需求的RFC（Request for Comments）类型的issue，主要涉及到Shared Mixture of Experts。由于GPU内存需求的原因，提议添加一个新的Mixtraltype模型，并实现专家共享，以便在不同客户端的多个模型实例之间共享专家，以减少内存需求。

https://github.com/vllm-project/vllm/issues/9202
这个issue是一个Bugfix类型的问题，涉及主要对象为TPUModelRunner。由于未正确设置`is_first_step_output`变量为可选字段，导致TPU的多步骤执行功能出现问题。

https://github.com/vllm-project/vllm/issues/9201
这是一个bug报告，涉及的主要对象是代码中的参数传递问题，导致了`priority`参数被错误地传递给了`prompt_adapter_request`函数，最终影响了代码的正确性。

https://github.com/vllm-project/vllm/issues/9200
这是一个bug报告类型的issue，主要涉及的对象是vllm软件和与其兼容的模型。由于开发分支中的bug修复尚未发布新版本，可能导致用户在生产环境中使用vllm版本0.6.2时遇到一些问题。

https://github.com/vllm-project/vllm/issues/9199
该issue是一个用户提出的需求，主要涉及添加一个helm chart示例，用于在Kubernetes集群上部署vllm。用户希望通过此示例实现一个自主部署的配置，包括在启动、可读性和健康检查方面的k8s探针，以确保模型完全加载并在健康检查返回200后将pod标记为运行状态。

https://github.com/vllm-project/vllm/issues/9198
这是一个Bug报告类型的issue，主要涉及vllm中初始化多个LLM模型时出现内存错误的情况。导致这个问题的原因是GPU上内存不足而导致无法缓存块。

https://github.com/vllm-project/vllm/issues/9197
这是一个bug报告类型的issue，主要涉及到了GitHub Actions中的一个脚本工具actions/github-script。导致这个bug的原因可能是版本升级引发的功能变化或者依赖更新导致的问题。

https://github.com/vllm-project/vllm/issues/9196
这是一个关于升级`actions/checkout`工具版本的issue，不是bug报告。

https://github.com/vllm-project/vllm/issues/9195
这个issue类型是功能更新，该问题单涉及的主要对象是vllm的actions/setup-python工具。由于更新版本时涉及到node版本的更新和依赖项的更新，可能在使用中会遇到一些改动所带来的问题。

https://github.com/vllm-project/vllm/issues/9194
该issue类型为任务需求，涉及到mypy类型检查相关的优化。原因是需要解决`vllm/entrypoints`中的mypy警告，并在`format.sh`和CI中开启`mypy`检查。

https://github.com/vllm-project/vllm/issues/9193
这是一个Bug报告，涉及vllm/vllm-openai:v0.6.2在Kubernetes上调用`v1/completions`和`v1/chat/completions`时出现"500 Internal Server Error"的问题。

https://github.com/vllm-project/vllm/issues/9192
这是一个需求提案类型的issue，主要涉及的对象是vLLM项目的PR自动标记。这个提案是由于项目活跃度高，PR数量庞大，希望自动标记PR以便根据个人兴趣和PR状态快速识别。

https://github.com/vllm-project/vllm/issues/9191
这是一个bug报告，涉及的主要对象是 VLLM 模型。由于缺少reshape_and_cache属性导致AttributeError异常。

https://github.com/vllm-project/vllm/issues/9190
这是一个关于性能问题的bug报告，主要涉及VLLM中phi 3.5 vision模型运行大量prompt时CPU RAM消耗过高并最终导致进程被终止的问题。

https://github.com/vllm-project/vllm/issues/9189
这个issue类型为功能改进，涉及主要对象为Mamba和Jamba kernels以及相关模型，用户提出了改进持续批处理的需求。

https://github.com/vllm-project/vllm/issues/9188
这是一个关于bug的报告，主要涉及到MistralTokenizer和CachedChatGLM4Tokenizer对象，由于调用了不存在的属性`vocab`导致了bug。

https://github.com/vllm-project/vllm/issues/9187
这是一个Bug报告，涉及的主要对象是vllm中的CachedChatGLM4Tokenizer对象。由于缺少属性'vocab'而导致AttributeError异常。

https://github.com/vllm-project/vllm/issues/9186
这是一个关于特性需求的issue，主要涉及到VLLM中视觉编码器的输出状态配置。这个需求是由于在使用siglip或clip作为多模态视觉编码器时，可能会出现不同情况需要处理输出状态的需求。

https://github.com/vllm-project/vllm/issues/9185
这是一个bug报告类型的issue，主要涉及到vllm的benchmarking。该问题是由于需要处理大的seq长度而导致的bug。

https://github.com/vllm-project/vllm/issues/9184
这是一个bug报告类型的issue，涉及的主要对象是qwen2.5中的StreamingChatLanguageModel，问题是由于调用StreamingChatLanguageModel时出现错误日志报告。

https://github.com/vllm-project/vllm/issues/9183
这是一个Bug报告类型的Issue，主要涉及Qwen2.5-Math-7B-Instruct模型在MATH数据集中输出乱码的问题。这可能是由于vLLM引擎在某些情况下产生奇怪输出，与使用`transformers`库相比，造成了输出的差异。

https://github.com/vllm-project/vllm/issues/9182
这是一个Bug报告类型的Issue，涉及到VLLM安装过程中出现的版本检测错误问题。

https://github.com/vllm-project/vllm/issues/9181
这个issue类型是需求增强，涉及支持prefill only模型的调度器的改进。原因是为了更高效地支持只填充（仅编码器）模型。

https://github.com/vllm-project/vllm/issues/9180
这是一个bug报告类型的issue，涉及的主要对象是vllm库。这个问题可能是由于安装nightly版pytorch时遇到的错误导致的。

https://github.com/vllm-project/vllm/issues/9179
这个issue是关于bug修复的，涉及到Compressed Tensors和loading lora到quantized model，问题出现的原因是加载lora时不支持基本层QKVParallelLinear。

https://github.com/vllm-project/vllm/issues/9178
这是一个bug报告，涉及的主要对象是TPU backend，由于记忆分析逻辑错误导致了问题。

https://github.com/vllm-project/vllm/issues/9177
这是一个Bug报告，涉及到Quantization不支持dummy weight格式。这个问题可能导致test_cpu_offload_gptq和test_cpu_offload_compressed_tensors无法正常工作。

https://github.com/vllm-project/vllm/issues/9176
这个issue类型是bug报告，涉及的主要对象是模型的前向传播过程。原因可能是在使用pipeline parallelism时，在小批量大小情况下导致极低的吞吐量。

https://github.com/vllm-project/vllm/issues/9175
这是一个bug报告类型的issue，主要涉及vllm的Qwen2.5-7B-Instruct模型在CPU上运行时出错。导致错误的原因可能是CPU模型输入的问题。

https://github.com/vllm-project/vllm/issues/9174
这个issue是bug报告，涉及到FP8模型中的kv_scale参数。由于缺少对kv_scale参数的处理，导致需要添加remap来处理kv_scale。

https://github.com/vllm-project/vllm/issues/9173
这是一个用户提出需求的Issue，涉及主要对象为将KServe部署指南的链接更新。

https://github.com/vllm-project/vllm/issues/9172
这是一个bug报告，涉及的主要对象是vllm中的端口绑定。由于代码中存在冗余的部分导致端口绑定失败，造成"Address already in use"的错误。

https://github.com/vllm-project/vllm/issues/9171
这个issue类型为用户提出需求，主要涉及到VLLM软件包，用户提出了为setup.py添加分类器的需求，以增加软件包在PyPI上的可发现性。

https://github.com/vllm-project/vllm/issues/9170
这个issue是一个文档（Doc）修复类型的问题，主要涉及到VLM的提示占位符样本错误。原因是由于markdown解析错误，所以在这里使用了原始HTML。

https://github.com/vllm-project/vllm/issues/9169
这是一个用户提出需求的issue，主要涉及的对象是支持自定义`max_mm_tokens`。由于当前环境信息的收集可能由于某些原因导致了该需求未被满足。

https://github.com/vllm-project/vllm/issues/9168
这是一个bug报告，主要涉及vLLM的OpenAI-api server在`/docs`端点加载失败的问题。造成这一Bug的原因可能是服务器在特定版本中无法正常工作，导致GET请求无法正确响应。

https://github.com/vllm-project/vllm/issues/9167
这个issue是关于bug报告，涉及到OpenAI服务器中vLLM的`best_of`和`n`参数验证问题。问题源于内部验证只检查了`best_of`参数而未明确检查`n`参数，导致错误提示不清晰。

https://github.com/vllm-project/vllm/issues/9166
这个issue类型是工作进行中的原型重建，涉及主要对象为系统的架构。由于正在进行系统架构调整，用户可能在问题的处理过程中遇到一些功能或界面上的变化或不稳定性。

https://github.com/vllm-project/vllm/issues/9165
该issue为改进建议类型，主要涉及持续集成和测试程序，目的是通过使用虚拟加载数据来跳过模型下载和加载时间。

https://github.com/vllm-project/vllm/issues/9164
这是一个用户提出的功能需求，涉及优化支持更大批次大小的序列。由于advance_step.cu中对序列数量的限制导致无法支持更大批次，希望改进以提高性能。

https://github.com/vllm-project/vllm/issues/9163
这是一个关于性能问题的bug报告，该问题涉及vLLM加载模型时内存占用过高，导致Out of Memory错误。

https://github.com/vllm-project/vllm/issues/9162
这是一个关于增加额外目录的mypy检查的issue，涉及到代码质量优化。

https://github.com/vllm-project/vllm/issues/9161
这是一个更新类型的问题，主要涉及到Github仓库中的测试环境依赖的管理。这个问题的根本原因是在CI测试过程中未指定`lmeval`的确切版本，导致需要将其移到全局测试依赖项并锁定版本为`lmeval==0.4.4`。

https://github.com/vllm-project/vllm/issues/9160
这个issue属于bug报告类型，涉及的主要对象是权重加载逻辑。由于`lm_head`在权重中不存在导致了EAGLE模型加载时出现问题，解决方案优化了权重加载逻辑以提高内存效率，并修复了EAGLE权重加载问题。

https://github.com/vllm-project/vllm/issues/9159
这个issue是一个Bugfix类型的issue，涉及到`setup.py`的错误信息问题。由于当前版本的`setup.py`在未检测到neuron版本时显示错误的信息，导致了输出错误。

https://github.com/vllm-project/vllm/issues/9158
这是一个功能需求的issue，主要讨论了vllm中新增的numschedulersteps参数的设置和使用方式，用户希望了解该参数的具体含义和设置指南，以及针对70B模型在2x A100上的特定情况下如何选择合适的数值范围。

https://github.com/vllm-project/vllm/issues/9157
这是一个关于bug报告的issue，涉及主要对象是vllm的多GPU推断功能。由于多GPU推断时内存使用过高，且无法实现不均匀的负载分配，用户提出了关于内存使用和负载分配的问题。

https://github.com/vllm-project/vllm/issues/9156
这是一个bug报告，主要涉及vLLM API Server在模型初始化过程中出现Segmentation Fault的问题，原因是与NCCL初始化相关，导致分布式执行时出现“unhandled system error”，最终导致进程崩溃。

https://github.com/vllm-project/vllm/issues/9155
这是一个文档更新类型的issue，主要涉及的对象是vllm项目下的vllm.rst文档。由于缺少视频示例，用户提出需要更新文档以包含有关视频的示例。

https://github.com/vllm-project/vllm/issues/9154
这是一个用户提出需求的issue，主要涉及的对象是添加`JambaToolParser`以支持对ai21labs/AI21Jamba1.5Mini和ai21labs/AI21Jamba1.5Large的工具调用。

https://github.com/vllm-project/vllm/issues/9153
这是一个Bug报告issue，主要涉及的对象是vllm的bounding box预测功能。导致该问题的原因可能是vllm与OAI客户端结合时无法正确输出bounding box坐标。

https://github.com/vllm-project/vllm/issues/9152
这个issue是一个Bug报告，主要涉及的对象是在Docker中无法通过pip安装vllm，可能由于Dockerfile配置或构建错误导致。

https://github.com/vllm-project/vllm/issues/9151
这是一个关于在vLLM前端添加早期验证的问题，主要涉及到在加载模型之前进行一些快速检查，而不是等到模型加载完成后才进行验证。

https://github.com/vllm-project/vllm/issues/9150
该issue属于用户提出需求类型，主要对象是vllm项目中的PR（Pull Request）8797。由于未得到审阅导致用户感到困扰，希望得到他人关注和审阅。

https://github.com/vllm-project/vllm/issues/9149
这个issue类型是需求提出，涉及主要对象是BlockSpaceManagerV1，由于该功能正在被BlockSpaceManagerV2取代，需要引入一个环境变量来明确设置以允许使用BlockSpaceManagerV1。

https://github.com/vllm-project/vllm/issues/9148
这是一个用户提出需求的issue，问题涉及到支持更多模型的bitsandbytes量化，在测试中出现了一些头尺寸不被整除导致的不支持情况。

https://github.com/vllm-project/vllm/issues/9147
这是一个需求类型的 issue，主要涉及改进用户体验。原因是用户希望能够点击链接。

https://github.com/vllm-project/vllm/issues/9146
这是一个用户提出需求的issue，主要涉及如何在vllm中保存日志到本地路径和控制日志速率的问题。由于缺乏相关参数，用户无法找到解决办法。

https://github.com/vllm-project/vllm/issues/9145
This issue is a bug report related to the Intel GPU, specifically fixing xpu decode input where seq_lens use None instead of an empty list. The bug occurs due to incorrect input handling, leading to an issue with decoding input on Intel GPUs.

https://github.com/vllm-project/vllm/issues/9144
这个issue是一个bug报告，主要涉及Neuron后端的分页注意力支持，可能导致markdown渲染失败。

https://github.com/vllm-project/vllm/issues/9143
这是一个Bug报告，主要涉及的对象是VLLM模型在长上下文输入情况下使用`--enable-lora`参数导致生成速度显著下降的问题。

https://github.com/vllm-project/vllm/issues/9142
这是一个Bug报告，涉及对象为vllm中的free_seq()函数。由于在某些情况下，free_seq()函数被多次不必要地调用，导致了问题的产生。

https://github.com/vllm-project/vllm/issues/9141
这个issue是一个Bug报告，主要涉及的对象是vLLM中当传递具有Wx1维度的图像时发生崩溃。该问题是由于transformers库对于这种情况抛出异常导致的。

https://github.com/vllm-project/vllm/issues/9140
这个issue是bug报告，主要涉及utils模块的代码更新导致冲突，需要进行独立审核。

https://github.com/vllm-project/vllm/issues/9139
这个issue是关于代码修复的类型，主要涉及到vllm项目中的注释和变量名称。原因可能是之前的注释和变量名称不准确或不清晰。

https://github.com/vllm-project/vllm/issues/9138
这是一个[Feature]类型的issue，主要涉及torch.compile中对rms_norm和quant操作进行融合的功能添加，旨在提高性能。

https://github.com/vllm-project/vllm/issues/9137
这是一个用户提出需求的issue，主要涉及项目的README文件。由于缺少具体内容，用户想在README文件中添加对Slack的说明。

https://github.com/vllm-project/vllm/issues/9136
这是一个Bug报告类型的Issue，涉及主要对象为github仓库vllm下的attention.cu文件。导致问题的原因是计算的索引超出了数组的限制。

https://github.com/vllm-project/vllm/issues/9135
这个issue类型为文档改进，并涉及到在README中包含性能基准测试。原因是缺少性能基准信息可能导致用户无法准确了解该项目的性能表现。

https://github.com/vllm-project/vllm/issues/9134
这是一个用户提出需求的issue，主要涉及在Vllm上运行多GPU和多节点的本地hugging face模型推理的问题，可能是由于集成模型和运行配置方面的问题导致。

https://github.com/vllm-project/vllm/issues/9133
这个issue是一个Bug报告，主要涉及的对象是vllm的docker container，由于找不到lcuda导致容器崩溃，并无法使用--enable-lora功能。

https://github.com/vllm-project/vllm/issues/9132
该issue属于文档改进类型，主要涉及贡献者和安装文档，旨在改善文档组织和易读性，指向安装文档关键部分以便新开发者找到信息。

https://github.com/vllm-project/vllm/issues/9131
这是一个需求报告类型的issue，主要涉及到在推断时添加`mm_processor_kwargs`的支持。原因可能是为了在推断过程中能够提供定制的处理参数。

https://github.com/vllm-project/vllm/issues/9130
这个issue类型是用户提出需求，主要涉及docker image的更新，由于安全和包版本等原因，用户希望将vLLM docker image升级到更现代的操作系统版本。

https://github.com/vllm-project/vllm/issues/9129
这个issue类型是维护和改进任务，主要涉及CMake配置的清理和重构，目的是提高可读性和稳定性。

https://github.com/vllm-project/vllm/issues/9128
这是一个bug报告，涉及的主要对象是模型输入。由于未知原因导致出现了`assert len(indices) == len(inputs)`的错误。

https://github.com/vllm-project/vllm/issues/9127
这是一个bug报告，主要涉及的对象是vLLM Benchmarking中的Llama 3.1 405B模型。由于输入长度超过8192时出现错误，用户寻求解决设置配置以处理更大输入长度的帮助。

https://github.com/vllm-project/vllm/issues/9126
这是一个用户提出需求的issue，主要对象是获取推理指标并希望将其包含在API响应或日志中。由于当前环境未提供所需的推理度量指标，用户希望了解如何获取这些指标。

https://github.com/vllm-project/vllm/issues/9125
这个issue是关于提出需求的，主要对象是添加一个新的阿拉伯语模型，可能会出现处理token的方式不同的困难。

https://github.com/vllm-project/vllm/issues/9124
该issue是关于需求的，主要涉及的对象是支持prefill only models的attention实现。由于需要简化attention实现并支持单独启用双向模式，导致了提出这个问题。

https://github.com/vllm-project/vllm/issues/9123
这个issue是关于bug报告，主要涉及的对象是`BlockSpaceManagerV1.get_common_computed_block_ids`函数。由于未正确处理空的`ids_list`导致msgspec序列化错误。

https://github.com/vllm-project/vllm/issues/9122
这是一个bug报告，涉及主要对象是vllm中的`BlockSpaceManagerV1.get_common_computed_block_ids`函数。导致这个问题的原因是函数没有处理空的`ids_list`，导致`commonprefix`返回了一个空字符串导致msgspec序列化失败。

https://github.com/vllm-project/vllm/issues/9121
这个issue类型是需求提出，主要对象是OpenVINO，用户需要明确说明使用2.4.0版本的torch，并使用最新的`optimum`和`optimumintel`来支持最新的transformer版本。

https://github.com/vllm-project/vllm/issues/9120
这是一个Bug报告，涉及到加载quantized model时出现的错误。由于不支持特定的base layer导致无法加载模型。

https://github.com/vllm-project/vllm/issues/9119
这是一个Bug报告，主要涉及VLLM仓库的安装问题，由于最新提交的版本信息错误导致无法构建。

https://github.com/vllm-project/vllm/issues/9118
这是一个Bug报告，问题涉及在K8s中以非root用户运行VLLM Open AI时遇到的500错误。可能是由于配置缺失导致的。

https://github.com/vllm-project/vllm/issues/9117
这是一个需求提议，主要涉及到在MQLLMEngine上添加对beam search API支持的功能。

https://github.com/vllm-project/vllm/issues/9115
这是一个bug报告类型的issue，涉及的主要对象是model input for decode。由于CPU backend的问题已经被解决，建议对其他backend进行相应变更以避免需要进一步修复。

https://github.com/vllm-project/vllm/issues/9114
这是一个关于如何在CPU版本下运行Llama 3.2的使用问题，用户寻求关于如何集成Llama模型以进行推断的帮助。

https://github.com/vllm-project/vllm/issues/9113
这是一个Bug报告，涉及对象是TPOT。由于v0.6.2版本中当tp=1时，对于批量大小约为10时TPOT变得非常缓慢，而在v0.5.5版本中没有出现这个问题。

https://github.com/vllm-project/vllm/issues/9112
这个issue是关于功能需求（Feature Request），主要涉及对象是vLLM是否支持ONNX模型。

https://github.com/vllm-project/vllm/issues/9111
这是一个bug报告类型的issue，主要涉及的对象是在AMD环境下使用MultiStep Feature时出现了缺少参数的错误。

https://github.com/vllm-project/vllm/issues/9110
这是一个功能需求的issue，涉及主要对象为VLLM引擎和ModelConfig，用户提出在本地运行VLLM服务器时缺少InferenceClient类的问题。

https://github.com/vllm-project/vllm/issues/9109
这个issue类型是需求支持，主要对象是要求支持Jetson AGX Orin。由于Jetson AGX Orin的硬件和软件特征与现有支持列表不匹配，用户请求添加对该设备的支持。

https://github.com/vllm-project/vllm/issues/9108
这个issue属于功能改进类型，主要涉及vLLM模型接口的明确化和支持OOT嵌入模型。原因是之前缺乏明确的接口规范，可能导致开发和重构过程中的混淆，现在通过新增`interface_base.py`来明确定义所需的方法，同时将嵌入模型自动检测放入测试中。

https://github.com/vllm-project/vllm/issues/9106
这是一个bug报告类型的issue，主要涉及vllm的chat接口问题，用户在调用chat方式时出现错误。原因可能是接口实现存在问题。

https://github.com/vllm-project/vllm/issues/9105
这是一个功能需求的issue，主要对象是vllm core中的beam search功能。由于beam search功能需要在vllm core之外重新实现，因此需要将其从vllm core中移除。

https://github.com/vllm-project/vllm/issues/9104
这是一个bug报告，主要涉及的对象是VLM CLI参数的特殊错误处理。由于多个次要版本已经过去，因此此问题要移除在v0.5.1中已经删除的VLM CLI参数的特殊错误处理。

https://github.com/vllm-project/vllm/issues/9103
这是一个bug报告，涉及主要对象为BlockManagerV2，在特定情况下出现了encoderInput为None导致的问题。

https://github.com/vllm-project/vllm/issues/9102
这个issue类型是功能需求提出，主要对象是torch.compile中的blocksparse attention，由于custom op只接受Python常量和张量，需要调整实现以支持此限制。

https://github.com/vllm-project/vllm/issues/9101
这是一个bug报告，主要涉及vLLM中的一个问题，由于try-catch条件不正确导致在非CUDA平台上无法正确导入Flash Attention Backend。

https://github.com/vllm-project/vllm/issues/9100
这是一个bug报告类型的issue，主要涉及vllm下的draft model在AMD平台上的运行问题，由于try-catch块中出现ImportError而不是ModuleNotFoundError导致了该问题的发生。

https://github.com/vllm-project/vllm/issues/9099
这是一个Bug报告，涉及的主要对象是Llama-3.2-11B-Vision-Instruct模型。该问题可能是由于BlockManager V2导致的测试失败。

https://github.com/vllm-project/vllm/issues/9098
这个issue类型是在提出需求，主要涉及的对象是优化模型的forward computation逻辑。由于连续数据批处理和异构模型的复杂性，导致了attention层的复杂度难以处理，因此提出通过前向上下文来隐藏连续批处理的复杂性。

https://github.com/vllm-project/vllm/issues/9097
这是一个功能需求提出的issue，主要对象是VLLM项目中的Flash Infer模块，问题涉及到与torch.compile singlegraph capture的兼容性。

https://github.com/vllm-project/vllm/issues/9096
这是一个bug报告，用户在尝试使用vllm来部署finetuned Llama 3.1 8B模型时遇到了异常。

https://github.com/vllm-project/vllm/issues/9095
这个issue是一个PR提交指南，涉及的主要对象是vLLM项目。原因可能是为了规范和提高代码质量，同时优化审查流程。

https://github.com/vllm-project/vllm/issues/9094
这是一个关于Bug报告的issue，主要涉及VLLM模型在Kubernetes环境中出现问题。由于Kubernetes对GPU资源进行了隔离，导致CUDA流捕获不受支持，从而影响了模型的预期执行。

https://github.com/vllm-project/vllm/issues/9093
这是一个bug报告，涉及到vLLM的`lora init id`参数应大于0的限制问题。由于`lora init id`参数可以被设置为0，导致在推断过程中未正确使用LoRA，尽管意图是要利用LoRA。

https://github.com/vllm-project/vllm/issues/9092
这个issue属于Bug报告类型，涉及安装vllm时无法使用openvino后端的问题，可能由于安装环境或者配置的问题导致。

https://github.com/vllm-project/vllm/issues/9091
这是一个bug报告，主要涉及的对象是Qwen2-VL模型支持。由于某种原因导致无法在docker中启动Qwen2-VL模型，用户遇到下载过程中的异常。

https://github.com/vllm-project/vllm/issues/9090
这是一个bug报告，涉及的主要对象是模型运行器。导致这个问题的原因是PP支持并未在嵌入模型运行器中实现，因此导致嵌入模型无法正常运行。

https://github.com/vllm-project/vllm/issues/9089
这是一个bug报告类型的issue，涉及主要对象是在CPU后端上添加交叉注意力和编码器-解码器模型支持。由于缺乏交叉注意力和编码器-解码器模型支持，导致用户无法在CPU上运行llama 3.2版本。

https://github.com/vllm-project/vllm/issues/9088
这是一个文档更新的issue，涉及更新README.md文件以包含有关Ray峰会幻灯片的信息。可能是由于markdown渲染问题，所以需要使用原始的HTML来呈现信息。

https://github.com/vllm-project/vllm/issues/9087
这个issue是关于功能需求的，主要涉及到前端API支持beam search。原因是为了让API服务器在不使用前端多进程的情况下运行更高层次的beam search。

https://github.com/vllm-project/vllm/issues/9086
这是一个关于bug修复的issue，涉及的主要对象是在处理旧版本的pytorch时使用`impl_abstract`。这个问题的原因是在特定情况下使用`register_fake`会导致markdown渲染异常，因此需要作出相应调整。

https://github.com/vllm-project/vllm/issues/9085
这个issue属于bug报告，主要涉及Continuous Integration（CI）lint错误。由于lint错误导致一个提取请求仍然被自动合并。

https://github.com/vllm-project/vllm/issues/9084
这个issue属于bug报告类型，主要对象是vLLM中的block manager v2。导致这个bug的原因是v2 block manager无法支持mllama multimodal模型的textonly请求，因此需要使用v1 block manager作为备用方案。

https://github.com/vllm-project/vllm/issues/9083
这是一个 bug 报告 issue，主要涉及 vLLM 中关于注册自定义配置的问题。由于 vLLM 现在能够处理自定义配置，因此不再需要将它们注册到 huggingface，而移除配置注册将避免改变 huggingface 的行为。

https://github.com/vllm-project/vllm/issues/9082
这是一个Bug报告类型的issue，主要涉及vLLM在使用特定的json schema生成数据时出现crash的问题。

https://github.com/vllm-project/vllm/issues/9081
这是一个bug报告，涉及的主要对象是改善prefix cache benchmark脚本的可重复性。这个bug由于缺乏随机种子导致多次运行prefix cache benchmark时结果不一致。

https://github.com/vllm-project/vllm/issues/9080
这是一个 bug 报告，涉及主要对象为 prefix cache benchmark。由于缺乏随机种子导致无法复现多次运行的问题。

https://github.com/vllm-project/vllm/issues/9079
这个issue是关于功能需求的，主要涉及Prefill-Decode separation 在vllm中的实现以及新增的InfiniteStore功能。导致该需求的原因是为了优化数据传输效率和管理KV缓存。

https://github.com/vllm-project/vllm/issues/9078
这个issue属于PR提交类型，主要涉及vAttention的内存布局问题导致的浪费。

https://github.com/vllm-project/vllm/issues/9077
这是一个Bug报告类型的issue，主要涉及前缀缓存示例的改进。由于过少的提示样本无法展示前缀缓存的作用，导致之前的示例在不同运行之间存在较大的方差。

https://github.com/vllm-project/vllm/issues/9076
这是一个Bug报告，涉及的主要对象是vllm中的transformer的Autoconfig。这个问题的症状是vllm覆盖了mllama的Autoconfig，需要移除这一行代码。

https://github.com/vllm-project/vllm/issues/9075
这是一个请求移除特定内容的issue，类型为用户提出需求，主要对象涉及到AMD Ray Summit Banner。

https://github.com/vllm-project/vllm/issues/9074
这是一个bug报告，涉及vllm的文档不清晰导致用户无法正确使用工具。

https://github.com/vllm-project/vllm/issues/9073
这是一个bug报告，该问题涉及vllm安装，由于特定环境要求导致构建失败。

https://github.com/vllm-project/vllm/issues/9072
这是一个用户提出需求的issue，主要涉及需要理解在Q4路线图中对torch.compile的支持，可能由于缺乏对该功能具体支持需求的理解而提出帮助请求。

https://github.com/vllm-project/vllm/issues/9071
这个issue属于bug报告，涉及的主要对象是Qwen2VL中的图像嵌入层。由于图像嵌入维度未与像素值匹配，会在批处理推断时引发维度不匹配错误。

https://github.com/vllm-project/vllm/issues/9070
这是一个bug报告类型的issue，涉及到vllm框架在使用8 x MI300x GPUs时出现的两个问题：低成功率和对于张量并行大小的限制。

https://github.com/vllm-project/vllm/issues/9069
这是一个Bug报告类型的issue，涉及的主要对象是Pixtral Model在vLLM中的加载问题。该问题是由于vLLM不支持包含视觉tower的模型，导致无法处理包含文本和图像处理组件的Pixtral Model，从而出现了问题。

https://github.com/vllm-project/vllm/issues/9068
这个issue类型是bug报告，涉及的主要对象是Pixtral模型与vllm v0.6.2的docker版本，在使用mistral tokenizer时缺少OpenCV依赖导致了ImportError。

https://github.com/vllm-project/vllm/issues/9067
这是一个用户提出需求的issue，主要涉及到如何在多模态语言模型中截断输入时避免图像特征与相应的图像标记不对齐的问题。

https://github.com/vllm-project/vllm/issues/9065
这是一个用户提出需求的类型，主要对象是vllm的中文文档翻译。由于缺乏回应，用户提出如何贡献中文文档翻译的建议。

https://github.com/vllm-project/vllm/issues/9064
这是一个代码优化类型的issue，主要涉及到 `vllm.model_executor.models` 的命名空间问题，由于需要避免污染该命名空间，因此将模型注册表等相关代码移动到了单独的 `registry.py` 文件中。

https://github.com/vllm-project/vllm/issues/9063
这个issue类型是bug报告，涉及的主要对象是Lora模型加载的问题。由于加载路径不同导致需要多次尝试加载方能成功，可能是由于加载过程中的路径处理问题所致。

https://github.com/vllm-project/vllm/issues/9062
这个issue是一个bug报告，主要涉及到flash attention arches的设置问题，由于之前的PR改动导致flash attention arches没有正确设置，需要恢复到之前的状态。

https://github.com/vllm-project/vllm/issues/9061
这是一个功能需求类型的issue，主要涉及torch.compile和自定义allreduce的集成问题，由于pytorch目前不支持在动态图中使用自定义操作，导致了无法隐藏运行时大小检查和内存复制或者计算等方面的问题。

https://github.com/vllm-project/vllm/issues/9060
这是一个Bug报告，涉及到模型运行环境及FlashAttention构建问题，导致了sm版本不匹配的错误提示。

https://github.com/vllm-project/vllm/issues/9059
这是一个bug报告issue，主要涉及到MistralTokenizer模块的兼容性问题，导致在使用OpenAI API工具时出现了测试失败的症状。

https://github.com/vllm-project/vllm/issues/9058
这是一个bug报告，主要涉及torch.compile集成功能。由于vLLM某些特性尚未经过测试，需要通过设置不同的编译级别来测试功能，检测torch.compile的问题或模型完整性。

https://github.com/vllm-project/vllm/issues/9057
这个issue是一个[Misc]类型的需求报告，主要涉及LoRA模块以及请求排序逻辑的修改，导致需要支持chunked prefill，并保证prefill序列始终在解码序列之前。

https://github.com/vllm-project/vllm/issues/9056
这个issue是一个[Feature]类型，主要涉及的对象是支持`BERTModel`的Encoder-only嵌入模型。原因是由于需要添加对BERT模型的支持以及改进EncoderOnly模型和XFORMERS后端的注意力机制。

https://github.com/vllm-project/vllm/issues/9055
这是一个关于修复失败的spec decode测试的bug报告，涉及的主要对象是vLLM中的CI/Build流程。原因可能是测试中未正确设置block manager版本导致出现问题。

https://github.com/vllm-project/vllm/issues/9054
这是一个bug报告，主要涉及到VLLM项目中的Spec Decode不支持BlockManagerV1导致测试失败的问题。

https://github.com/vllm-project/vllm/issues/9053
这是一个关于Bug报告的issue，涉及主要对象是Mistral服务器中的autotool功能。由于在使用max_tokens时，使用streaming和非streaming方式调用tool会导致不一致的行为和响应，用户希望行为和响应在这种情况下能更一致。

https://github.com/vllm-project/vllm/issues/9052
这是一个bug报告的issue，涉及的主要对象是LLama-3.1-405B Inference with vLLM TPU。造成的原因是Triton未安装，导致GPU相关功能不可用。

https://github.com/vllm-project/vllm/issues/9051
这是一个bug报告，主要涉及vllm在使用GGUF模型时遇到了infer device type错误，可能是由于缺少libcuda.so.1文件导致的。

https://github.com/vllm-project/vllm/issues/9050
这是一个bug报告，涉及的主要对象是使用`torchserve`部署vLLM时遇到的错误。原因是输入不合法导致`TypeError`。

https://github.com/vllm-project/vllm/issues/9049
这是一个Bug报告，主要涉及的对象是vLLM项目中的一个Issue。由于代码中存在一个问题，导致在使用自动工具选择时，发送流请求会导致`IndexError`异常和未发送 `[DONE]` 消息。

https://github.com/vllm-project/vllm/issues/9048
这是一个Bug报告，涉及VLLM项目下的一个Issue，由于发送流请求时出现IndexError导致响应流不包含`data: [DONE]`消息。

https://github.com/vllm-project/vllm/issues/9047
这个issue是关于功能需求的，主要涉及到VLLM项目中的多步输出流。

https://github.com/vllm-project/vllm/issues/9046
这是一个bug报告，主要涉及vllm项目中的custom_all_reduce功能出现'invalid argument'错误。问题可能源于环境配置或代码实现方面的问题。

https://github.com/vllm-project/vllm/issues/9045
这是一个bug报告，主要涉及的对象是InternViT中的parallel attention实现以及QK normalization。这个问题是由于现有的parallel attention实现与QK normalization不兼容，导致了NVLM-D模型在测试中出现问题。

https://github.com/vllm-project/vllm/issues/9044
这个issue是一个Bugfix类型的问题，主要涉及CPU model输入导致的`IndexError: list index out of range`错误。

https://github.com/vllm-project/vllm/issues/9043
这是一个用户提出需求类型的issue，主要涉及到vLLM v0.6.2缺少CUDA 11.8二进制文件的问题。由于缺少相应的CUDA 11.8 wheels，用户希望开发者发布与最新版本对应的CUDA 11.8二进制文件。

https://github.com/vllm-project/vllm/issues/9042
这个issue是关于bug修复的，主要涉及OPT模型权重加载的问题，由于某些版本的OPT模型没有嵌入层和lm_head权重相连，导致权重加载时出现问题。

https://github.com/vllm-project/vllm/issues/9041
这是一个用户提出需求的issue，主要对象是希望支持的新模型"Nvidia/NVLM-D-72B"。原因可能是该模型目前尚未得到支持，用户希望得到支持和解决问题。

https://github.com/vllm-project/vllm/issues/9040
这个issue类型为需求提出，主要涉及的对象是支持新模型NVLM 1.0，由于该模型具有新架构，需要时间来在vLLM中实现支持。

https://github.com/vllm-project/vllm/issues/9039
这个issue是关于修改Power架构下的vLLM项目，使得oneDNN库成为可选依赖项，目前该库在Power架构下没有被使用。

https://github.com/vllm-project/vllm/issues/9038
这个issue是一个Bug报告，主要涉及的对象是num_computed_tokens变量。由于在使用MultiStep调度时，主函数中未正确遵循num_computed_tokens的语义，导致了更新错误的问题。

https://github.com/vllm-project/vllm/issues/9037
这是一个Bug报告，涉及到在CPU上运行Llama 3.21B时出现错误。原因可能是安装vllm时出现的问题。

https://github.com/vllm-project/vllm/issues/9036
这是一个Bug报告。该问题涉及Pixtral模型在vLLM中出现了不支持的视觉配置导致的问题。

https://github.com/vllm-project/vllm/issues/9035
这个issue类型是用户提出需求，涉及的主要对象是增加对llama32在rocm vllm中的支持，原因可能是用户需要在rocm vllm中使用llama32相关的命令。

https://github.com/vllm-project/vllm/issues/9034
这个issue是一个bug报告，主要涉及的对象是vLLM中的Sequence.get_output_token_ids_to_return方法。这个bug是由于当没有生成新token时，未返回空列表导致的。

https://github.com/vllm-project/vllm/issues/9033
这是一个Bug报告类型的Issue，涉及到VLLM_USE_MODELSCOPE在使用时出现连接超时异常，导致无法成功调用模型所需的配置文件。

https://github.com/vllm-project/vllm/issues/9032
这是一个性能回归bug报告，涉及到Transformers 4.45.1对`outlines` guided decoding造成的明显减慢现象。

https://github.com/vllm-project/vllm/issues/9030
这个issue是一个bug报告，主要涉及到mllama在非默认词汇大小下的加载错误。该问题可能由于tokenizer初始化中的偏移错误导致特殊标记的添加问题而出现。

https://github.com/vllm-project/vllm/issues/9029
这是一个用户提出需求的类型。该问题单涉及的主要对象是在 vllm 项目中的注意力机制。由于缺少前向上下文，导致在该项目中无法正确实现某个功能。

https://github.com/vllm-project/vllm/issues/9028
这是一个bug报告，涉及的主要对象是使用chunked prefill时连续使用统计数据不正确，可能由于token计数错误导致。

https://github.com/vllm-project/vllm/issues/9027
这是一个功能请求，该问题单涉及的主要对象是 "ibmgranite/granite3.08binstruct"。

https://github.com/vllm-project/vllm/issues/9026
这个issue是一个bug报告，涉及到vLLM中xformer prefill for encoder-decoder存在的问题。原因是K和V应该不会被prompt长度切断，因为它们是根据视觉编码器输出计算的。

https://github.com/vllm-project/vllm/issues/9025
这是一个文档更新类型的issue，主要涉及的对象是Granite模型文档。

https://github.com/vllm-project/vllm/issues/9024
这是一个bug报告，主要涉及VLLM在使用cpu时出现的"IndexError: list index out of range"错误。原因可能是代码中的访问了超出范围的列表索引。

https://github.com/vllm-project/vllm/issues/9023
这是一个Bug报告，涉及到前端日志记录重复错误堆栈，由于引擎在处理当前批次时出现错误，导致每个请求都记录了相同的堆栈信息。

https://github.com/vllm-project/vllm/issues/9022
这是一个性能问题，涉及Llama 3.1 405B和70B的吞吐量差异，用户询问为什么这两者在基准测试中速度相差5倍。

https://github.com/vllm-project/vllm/issues/9021
这个issue是关于缺少cu118 wheels for 0.6.2 release的bug报告，主要涉及vllm项目发布页面缺少针对cu118的wheels，在0.6.2版本发布页面中未能找到cu118 wheels，可能是由于发布时遗漏导致用户提出了缺少相关组件的问题。

https://github.com/vllm-project/vllm/issues/9020
这是一个Bug报告，主要涉及Mistral工具调用解析器的ID约束问题，由于Mistral工具调用解析器生成的ID与Mistral工具调用和模板约束不符合导致的问题。

https://github.com/vllm-project/vllm/issues/9019
这个issue类型是bug报告，主要涉及Mistral工具调用解析器生成的ToolCall IDs不符合Mistral工具调用和模板约束，导致无法在后续函数调用工作流中使用。

https://github.com/vllm-project/vllm/issues/9017
这个issue是一个BugFix类型的报告，涉及的主要对象是防止导出重复的OpenTelemetry spans。由于合并CC([Core] Asynchronous Output Processor)后，非Beam搜索请求导致导出重复的OpenTelemetry spans，解决该问题的原因是将`finished_before`列表传递给`do_tracing()`，类似于在`do_log_stats()`中的使用。

https://github.com/vllm-project/vllm/issues/9016
这是一个关于在vLLM中使用Molmo模型整合的issue，主要是关于修复Markdown渲染问题。

https://github.com/vllm-project/vllm/issues/9015
这是一个bug报告，涉及vLLM中的Pipeline Parallelism支持问题，导致用户在加载LLaMA 3.2 90B视觉模型时遇到NotImplementedError。

https://github.com/vllm-project/vllm/issues/9014
这是一个bug报告，主要涉及Triton中使用`tl.load`时需要明确指定`other`值，否则可能会导致潜在的NaN值传播问题。

https://github.com/vllm-project/vllm/issues/9013
这是一个Bugfix类型的issue，主要涉及的对象是benchmark_serving脚本中的sample_hf_requests函数。问题出现的原因是sample_hf_requests函数在使用dataset.shuffle()时导致了每次运行时输入提示的不一致性，希望通过添加random_seed参数和修改shuffle(seed=RANDOM_SEED)来确保benchmark的可重复性。

https://github.com/vllm-project/vllm/issues/9012
这是一个Bug报告，主要对象是VLLM中处理1x1图像时出现的通道数错误问题，由于输入始终为PIL图像，数据格式为通道最后，但VLLM无法推断图像格式，导致了1x1图像上的错误。

https://github.com/vllm-project/vllm/issues/9011
这是一个Bug报告，涉及到vllm在A100 GPU上启动时遇到CUDA OOM错误的问题。

https://github.com/vllm-project/vllm/issues/9010
这个issue类型属于文档反馈，主要涉及的对象是FP8静态量化，用户提出了关于为什么FP8静态量化被标记为不推荐使用的疑问。

https://github.com/vllm-project/vllm/issues/9009
这是一个需求类型的issue，主要涉及ShareGPT数据集的输入和输出序列长度默认值问题。由于benchmark_serving.py未为ShareGPT数据集提供inputlen和outputlen值，导致用户想了解该数据集的默认数值及如何在vllm中使用该数据集。

https://github.com/vllm-project/vllm/issues/9008
该issue属于[Model]类型，主要涉及的对象是添加对mixtral的一组LoRA模块支持。由于缺少`alllinear`选项，导致在mixtral中生成的`target modules`不完整，用户提出需要添加缺失的模块（`w1`，`w2`，`w3`和`gate`）。

https://github.com/vllm-project/vllm/issues/9007
这是一个bug报告，该问题涉及到vLLM的模型，并因模板错误导致模型在无需调用工具时产生了工具调用响应。

https://github.com/vllm-project/vllm/issues/9006
这个issue是一个需求报告，主要涉及vLLM Roadmap Q4 2024的规划内容。由于缺乏对`transformers`后端支持等功能，用户提出了一些功能增强和硬件支持方面的需求。

https://github.com/vllm-project/vllm/issues/9005
这是一个bug报告，主要对象是Neuron的硬件，并因为旧代码不支持`user`安装而导致问题。

https://github.com/vllm-project/vllm/issues/9004
这是一个模型相关的issue，涉及支持Gemma2 embedding model，并包含了相关的PR描述。

https://github.com/vllm-project/vllm/issues/9003
这是一个功能需求提议，主要涉及 Torch 的编译过程，并需要添加日志记录峰值内存使用情况。

https://github.com/vllm-project/vllm/issues/9002
这是一个Bug报告类型的Issue，主要涉及Llama 3.2 vision模型在streaming和non-streaming模式下处理带有图像输入时的差异，可能由于图像编码器引起的Prompt tokens数量不一致导致问题。

https://github.com/vllm-project/vllm/issues/9001
这个issue是一个文档修复类的问题，主要涉及到vLLM的api_client.py示例，导致了markdown渲染不起作用。

https://github.com/vllm-project/vllm/issues/9000
这个issue是关于功能改进的，主要对象是VLLM模型的接口和自动检测功能。因为更新了模型注册表，现在不需要将几乎每个模型硬编码到支持PP列表中，这个问题主要解决了反复添加模型的繁琐工作。

https://github.com/vllm-project/vllm/issues/8999
这个issue是关于添加了一个新参数到embeddings创建过程中，用于控制截断操作。

https://github.com/vllm-project/vllm/issues/8998
这是一个bug报告，涉及 vLLM 应用程序中的测试无法顺序运行的问题。可能原因是无法在多个测试中使用不同值的 `tensor_parallel_size`。

https://github.com/vllm-project/vllm/issues/8997
这个issue是一个Bug报告，主要对象是benchmark_serving.py脚本。导致问题的原因是在Windows下读取和写入json数据集时未指定UTF-8编码，导致与Windows的兼容性问题。

https://github.com/vllm-project/vllm/issues/8996
这个issue属于用户提出需求类型，主要涉及vllm的RISC-V支持，可能由于用户当前环境下无法安装vllm而提出该问题。

https://github.com/vllm-project/vllm/issues/8995
这是一个bug报告，涉及的主要对象是Vllm server (docker)，由于某些配置参数不正确导致无法加载指定的模型"Mistral-8x22B-Instruct-v0.1"。

https://github.com/vllm-project/vllm/issues/8994
这是一个Bug报告，涉及到vllm下的无法加载特定模型的tokenizers问题。可能是由于tokenizers问题导致无法加载llama213B和alma13B模型，而加载7B版本的模型却没有问题。

https://github.com/vllm-project/vllm/issues/8993
这是一个用户提出的需求类型的issue，主要涉及到从Jamba中分离出Mamba以提供更广泛的支持，并为将来将Mamba缓存机制整合到vLLM引擎中奠定基础。

https://github.com/vllm-project/vllm/issues/8991
这是一个bug报告，主要对象涉及到MiniCPM-V模型中处理带有图片但没有占位符的情况。这个问题是由于在提供图片但没有占位符时，`token_ids`未定义而导致的。

https://github.com/vllm-project/vllm/issues/8990
这是一个Bug报告，主要涉及MiniCPMV模型在用户错误省略图像占位符时引发UnboundLocalError错误的问题。

https://github.com/vllm-project/vllm/issues/8989
这个issue属于bug报告类型，主要涉及vLLM中openai.serving_chat尝试在output.text为空时调用_create_chat_logprobs导致IndexError的问题。

https://github.com/vllm-project/vllm/issues/8988
这是一个bug报告，主要涉及openai.serving_chat中的_create_chat_logprobs调用问题导致的异常行为。

https://github.com/vllm-project/vllm/issues/8987
这个issue是一个文档更新类型的问题，涉及的主要对象是vllm中的支持模型列表。由于缺少部分模型信息，导致用户提出了需要添加DeepSeek, DeepSeekV2和Granite模型的需求。

https://github.com/vllm-project/vllm/issues/8986
该issue是一个bugfix，其中涉及到Fuyu tensor的并行推断问题。由于某个多模态模型，分布式推断无法正常运行，导致该issue的产生。

https://github.com/vllm-project/vllm/issues/8985
这是一个Bug报告，涉及vllm容器在特定情况下出现崩溃的问题。原因是在设置numschedulersteps为8且请求中包含"response_format": { "type": "json_object" }时，会导致错误并使vllm崩溃。

https://github.com/vllm-project/vllm/issues/8984
这是一个bug报告，主要涉及到使用vllm进行多卡加速推理时出现的问题。原因可能是CUDA错误导致无法正确搭载模型或指定设备号时仍然出现问题。

https://github.com/vllm-project/vllm/issues/8983
这是一个bug报告，主要涉及的对象是vllm中的分布式推理功能。由于数值张量形状不匹配导致出现了运行时错误。

https://github.com/vllm-project/vllm/issues/8982
这是一个bug报告，主要涉及到torch.compile的问题，由于之前的修改导致了tensor别名的问题。

https://github.com/vllm-project/vllm/issues/8981
这是一个需求类issue，主要对象是Vllm中的规范解码功能。由于单个GPU的内存和计算限制，需要在多个GPU上对草稿模型进行分片，以便处理更大的模型。

https://github.com/vllm-project/vllm/issues/8980
这是一个功能需求，主要涉及VLLM在多个GPU上划分draft模型的问题，用户提出需要支持draft模型在多个GPU上分片，以解决单个GPU内存和计算限制导致模型大小受限的问题。

https://github.com/vllm-project/vllm/issues/8979
这是一个bug报告，涉及到Phi3V和Ultravox的EmbeddingInputs支持。由于代码中存在额外的if语句和图片特征尺寸维度的错误使用，以及Ultravox在代码中没有实际支持`audio_embeds`，导致了这些bug。

https://github.com/vllm-project/vllm/issues/8978
这是关于bug报告的issue，主要涉及使用vllm进行模型服务时出现hangs现象，原因可能是相关环境配置或代码实现问题。

https://github.com/vllm-project/vllm/issues/8977
这个issue是关于bug报告，主要涉及的对象是VLLM的图片映射器。问题出现的原因是默认的图片映射器可能会在某些特殊情况下发生错误，导致用户难以区分是VLLM的问题还是底层HF模型图像处理器的问题。

https://github.com/vllm-project/vllm/issues/8976
这个issue类型为bug报告，主要涉及的对象是生成的kernel文件配置顺序，由于使用了`set`去重导致`Schedules`创建顺序随机，从而导致在CI上出现sccache缓存未命中的问题。

https://github.com/vllm-project/vllm/issues/8975
这是一个bug报告类型的issue，主要涉及SpecDecode模块的测试问题，导致需要修复以使用flash attention backend来进行spec decode CI tests。

https://github.com/vllm-project/vllm/issues/8974
这是一个bug报告，涉及到vllm 0.6.2版本中出现的“Bus error (core dumped)”问题。原因可能是版本升级导致的程序崩溃。

https://github.com/vllm-project/vllm/issues/8973
这个issue是一个功能增强提议，涉及到Zero point support的添加和AWQ Fused MoE的支持，可能由于需要在MarlinMoE kernel上实现zero point support以及对AWQ的支持而产生。

https://github.com/vllm-project/vllm/issues/8972
这是一个bug报告，涉及到添加对Ovis模型的支持问题。由于当前不支持Ovis模型，导致用户在使用AIDCAI/Ovis1.6Gemma29B模型时出现数值错误。

https://github.com/vllm-project/vllm/issues/8971
这是一个bug报告，该问题涉及的主要对象是MoE配置文件加载。这个问题由于MoE配置文件命名依赖于TP大小、GPU名称和dtype，导致在加载MoE配置文件时无法静默失败，需要显式打印出搜索到的`config_file_path`。

https://github.com/vllm-project/vllm/issues/8970
这个issue是一份Bug报告，主要涉及到VLLM在使用llama3.23binstruct模型时出现了模型精度显著下降的问题。

https://github.com/vllm-project/vllm/issues/8969
这是一个用户提出需求的issue，主要涉及DummyModelLoader，提出了添加process_weights_after_loading功能的需求。

https://github.com/vllm-project/vllm/issues/8968
这是一个关于性能下降的bug报告，涉及VLLM模型在使用上出现问题。由于VLLM在推理时精度明显下降，用户反馈与Hugging Face Transformers相比表现不佳。

https://github.com/vllm-project/vllm/issues/8967
该issue是一个功能请求，主要涉及的对象是Reward Modelling。由于当前端点无法满足奖励模型的需求，导致需要通过一种不太理想的方式来与OpenAI兼容服务器一起使用。

https://github.com/vllm-project/vllm/issues/8966
这个issue是关于文档中的建议和意见，主要涉及的对象是示例代码中的数据分批处理方法。用户提出了由于需要连续的批处理而使用ray.remote()替代ds.map_batches()的建议。

https://github.com/vllm-project/vllm/issues/8965
该issue属于[Core] [Frontend] 类型，主要涉及到vLLM中的优先调度问题。这个问题导致了缺失的优先处理，用户提出了关于OpenAIAPI中应用新的优先调度的需求。

https://github.com/vllm-project/vllm/issues/8964
这是一个需求提出的issue，主要涉及支持BERT模型的代码组织和prefill only models相关的功能开发。原因是为了支持prefill only models的特定需求，需要对模块和功能进行相应的调整和添加。

https://github.com/vllm-project/vllm/issues/8963
这是一个bug报告，涉及的主要对象是vllm server。由于一个assertion失败，导致vllm server启动时出现错误，引起了无法启动的问题。

https://github.com/vllm-project/vllm/issues/8961
这是一个用户提出的需求类型的issue，主要涉及的对象是硬件相关的代码。导致这个需求的原因是为了使设备在不同硬件上都能通用。

https://github.com/vllm-project/vllm/issues/8960
这个issue是关于bugfix，主要涉及的对象是config.yaml文件的参数顺序问题，由于参数顺序错误导致问题。

https://github.com/vllm-project/vllm/issues/8959
这是一个关于前端Neuron模块中解析override-neuron-config中字面量的问题，主要涉及到添加bool或number参数导致初始化失败的bug。

https://github.com/vllm-project/vllm/issues/8957
这是一个用户提出需求的类型，主要涉及max_position_embeddings参数的调整，原因是为了LoRA的兼容性。

https://github.com/vllm-project/vllm/issues/8956
这是一个[Core]类型的issue，主要涉及调度策略通过EngineArgs设置，之前只能在LLMEngine创建后修改调度器才能激活新的优先级策略。

https://github.com/vllm-project/vllm/issues/8955
这是一个bug报告，主要涉及vllm在ROCm上构建失败的问题，可能是由于无法成功下载所需的cudanvcc文件导致的。

https://github.com/vllm-project/vllm/issues/8954
这个issue类型是bug报告，主要涉及minicpmv在1x1图片上崩溃的问题，可能是由于1x1图片导致的bug或者用户遇到了相关问题寻求帮助。

https://github.com/vllm-project/vllm/issues/8953
这是一个Bugfix类型的issue，涉及的主要对象是pynvml库中的`nvmlDeviceGetName`函数。由于pynvml库在最新版本中将该函数的返回类型从字符串更改为二进制字符串，导致了相关错误。

https://github.com/vllm-project/vllm/issues/8952
这是一个Bug报告，涉及的主要对象是Llama-3.2-11B-Vision-Instruct server。由于在调用guided generation时传入的参数导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/8951
这是一个关于在github上的vLLM项目下的CI/Build方面的issue，主要涉及的对象是包含`cv2` via `mistral_common[opencv]`。造成这个issue的原因是要通过添加`opencvpython`来支持Pixtral项目中的python包和docker镜像，但markdown渲染存在问题，因此使用了原始的html代码。

https://github.com/vllm-project/vllm/issues/8950
这个issue是一个Bug报告，涉及的主要对象是`num_computed_tokens`变量。这个Bug是由于PR CC([Core] MultiStep + Single Step Prefills via Chunked Prefill code path)引入的，导致了`num_computed_tokens`在计算sampled tokens时出现了错误的更新。

https://github.com/vllm-project/vllm/issues/8949
这个issue类型为功能建议，主要涉及 torch.compile 对象支持哪些模型和功能，以及需要的性能测试和优化。

https://github.com/vllm-project/vllm/issues/8948
这个issue类型是bug报告，主要涉及AsyncLLMEngine CUDA runtime error 'device-side assert triggered'的问题，由于未知原因导致了这个错误。

https://github.com/vllm-project/vllm/issues/8947
这是一个Bug报告，涉及主要对象为vllm serve命令。由于参数位置错误导致无法成功运行服务器。

https://github.com/vllm-project/vllm/issues/8946
这是一个关于修复bug的issue，该问题主要涉及模型InternVL2中的max_dynamic_patch暴露问题。造成这个bug的原因是markdown渲染不起作用，需要使用原始的HTML。

https://github.com/vllm-project/vllm/issues/8945
这是一个bug报告，主要涉及VLLM在应用LoRA适配器时出现崩溃的情况。原因可能是技术限制或操作缺失。

https://github.com/vllm-project/vllm/issues/8944
这是一个bug报告，主要涉及了 BlockSpaceManagerV1 类中的一个方法的拼写错误导致的问题。

https://github.com/vllm-project/vllm/issues/8943
这是一个软件bug报告，主要涉及的对象是 LoRA 模型的支持。由于作者已经在本地测试，但仍需确认是否存在问题。

https://github.com/vllm-project/vllm/issues/8942
该issue属于非Bug报告类型，主要涉及前端（Frontend）的改进，针对支持HF新的`continue_final_message`参数。原因可能是为了增强应用的功能性或性能。

https://github.com/vllm-project/vllm/issues/8941
这是一个bug报告，涉及主要对象为Qwen2.5模型的bitsandbytes支持，导致模型输出不符合预期。

https://github.com/vllm-project/vllm/issues/8940
这个issue是关于提出新模型支持的类型，主要涉及到对Molmo模型的支持问题。该问题可能由于当前版本的vllm模型还不支持Molmo模型而导致用户提出需求。

https://github.com/vllm-project/vllm/issues/8939
这是一个关于用户提出需求的问题，主要涉及到vLLM能否实现使用不同批次进行前缀缓存，可能是由于用户想要在不同批次下使用vLLM进行前缀缓存而提出的。

https://github.com/vllm-project/vllm/issues/8938
这是一个bug报告。该问题涉及到vLLM的OpenAI benchmarking脚本无法使用--served-model-name参数进行模型验证的问题，导致benchmarking脚本试图验证Hugging Face模型，但实际上模型名称不一定是有效的HF模型。

https://github.com/vllm-project/vllm/issues/8937
这个issue是一个bug报告，涉及的主要对象是vllm库中的tensor_parallel模块。由于使用v0.6.1.post1或0.6.2版本时出现错误，这可能是由于版本兼容性问题导致的。

https://github.com/vllm-project/vllm/issues/8936
这是一个Bug报告。该问题涉及v0.6.2版本的vllm，主要对象是Qwen2-VL模型。由于升级到v0.6.2版本后，OCR步骤的准确性显著下降，只能输出图像中大约1/10的文本。

https://github.com/vllm-project/vllm/issues/8935
这是一个优化建议，主要涉及到提高hash_of_block函数的计算速度，由于当前时间复杂度为O(n^2)，希望通过优化到O(n)来提高效率。

https://github.com/vllm-project/vllm/issues/8933
这是一个Bug报告，该问题涉及的主要对象是Vllm0.6.2，可能由于某些原因导致了资源泄漏的UserWarning。

https://github.com/vllm-project/vllm/issues/8932
这是一个关于硬件后端退役策略的需求报告，主要涉及到vLLM对不同硬件后端支持的问题。由于vLLM团队要求硬件供应商跟进PyTorch更新，未能支持最新PyTorch版本的硬件后端将被删除。

https://github.com/vllm-project/vllm/issues/8931
这个issue是一个需求提出类型的问题，主要涉及到安装文档的组织和提交记录的docker。原因可能是用户希望更清晰地了解安装过程和每个提交的docker信息。

https://github.com/vllm-project/vllm/issues/8930
这是一个关于构建/持续集成的问题，主要涉及到使用CMake的FetchContent管理依赖项时出现的缓存问题。原因是在使用`pip install e .`时，`CMAKE_BINARY_DIR`的路径变化导致了缓存未命中和过多的编译。

https://github.com/vllm-project/vllm/issues/8929
这是一个Bug报告，涉及的主要对象是VLLM项目中的某个功能。由于软件在Colab平台上出现`ValueError: XFormers does not support attention logits soft capping.`错误，用户希望得到问题的解决方案。

https://github.com/vllm-project/vllm/issues/8928
该issue属于代码修改类型，主要涉及前端界面中的beam search模拟器温度可修改性，由于需要在代码中让温度成为可修改参数。

https://github.com/vllm-project/vllm/issues/8927
这是一个CI/Build类型的issue，涉及到CPU W8A8测试失败导致需要暂时禁用该测试的问题。

https://github.com/vllm-project/vllm/issues/8926
这是一个用户提出需求的issue，主要涉及获取logits而不是logprobs用于教师模型离线蒸馏，可能是出于加快速度的考虑。

https://github.com/vllm-project/vllm/issues/8925
这个issue属于bug报告类型，涉及主要对象为`test_phimoe.py`中的`get_gpu_memory`函数。这个bug是由于`get_gpu_memory`在导入时初始化CUDA，导致OOM问题而提出的。

https://github.com/vllm-project/vllm/issues/8924
这个issue是一个bug报告，主要涉及的对象是模型multimodal配置初始化过程中出现的错误。由于缺少配置文件中的架构信息，导致了初始化过程失败并且错误信息不具有说明性，需要进行错误处理和改进。

https://github.com/vllm-project/vllm/issues/8923
这是一个bug报告，涉及主要对象是multimodal model config初始化时的处理逻辑。由于未找到任何架构，导致出现未处理的错误和不详细的错误信息。

https://github.com/vllm-project/vllm/issues/8921
这是一个关于修复已解决的问题的Issue，主要对象是vLLM中的`BaichuanTokenizer`类。这个问题之所以出现，是因为原来的修复补丁不再需要，因为它们在之前的提交中已经对HF的实现进行了修复。

https://github.com/vllm-project/vllm/issues/8919
这个issue是关于bug修复的，涉及到wheel名称的修正。

https://github.com/vllm-project/vllm/issues/8918
这是一个性能问题的bug报告，涉及vllm引擎版本从v0.5.4到v0.6.2的性能退化，导致TTFT（Time To First Token）性能下降。

https://github.com/vllm-project/vllm/issues/8917
这是一个bug报告，涉及修复发布版本名称的问题，原因是版本字符串中包含特殊字符`+`导致无法作为URL使用。

https://github.com/vllm-project/vllm/issues/8915
这是一个添加对Granite 20b工具使用流支持的issue，主要涉及代码功能改进和测试覆盖的更新，由于需要区分不同的Granite模型版本，进行了相应的重命名。

https://github.com/vllm-project/vllm/issues/8914
这是一个需求更改的issue，主要涉及benchmark_throughput.py脚本的修改，用户添加了`maxnumseqs`参数作为可选参数。原因是需要将输出标记和总标记分开。

https://github.com/vllm-project/vllm/issues/8913
该issue属于用户提出需求，主要涉及QuantizationConfig和QuantizeMethodBase的重构，旨在简化内核集成，由于硬件和内核多样性增加导致紧密耦合的问题。

https://github.com/vllm-project/vllm/issues/8912
这是一个bug报告，涉及到Tool调用LLama 3.1/3.2时出现KeyError: '<tool_call>'错误。用户尝试通过langgraph查询Llama3.2模型时，导致vLLM出现内部服务器错误。

https://github.com/vllm-project/vllm/issues/8911
这是一个用户提出需要的需求问题，涉及到需要跳过P2P检查的功能。用户希望能够通过设置环境变量来直接信任驱动程序，从而减少检查的时间。

https://github.com/vllm-project/vllm/issues/8910
这是一个功能需求类型的issue，主要涉及的对象是FlashAttention backend，由于之前限制了特定的大小，而现在原生的FA内核支持任意大小的头尺寸，导致需要支持所有头尺寸最多到256的问题。

https://github.com/vllm-project/vllm/issues/8909
这个issue类型是功能需求提议，主要涉及对象是对于checkpoint定义中直接使用压缩张量。原因可能是希望重复使用压缩和量化配置实现。

https://github.com/vllm-project/vllm/issues/8908
这是一个bug报告类型的issue，主要涉及vllm的使用问题。由于在0.6.2版本中无法再使用比单节点GPU数量更大的tensor_parallel_size，导致代码无法正常运行。

https://github.com/vllm-project/vllm/issues/8907
这个issue是用户需求类型，主要涉及使用vllm中的guided_regex功能在离线模式下的问题，需求是在本地环境中使用guided_regex而非通过服务器。

https://github.com/vllm-project/vllm/issues/8906
这是一个bug报告，涉及到深度学习模型中的参数形状检查问题。导致这个bug的原因是参数位置混淆导致形状错误。

https://github.com/vllm-project/vllm/issues/8905
This issue type is a feature request regarding adding multi-video support for LLaVA-Onevision models, aiming to enhance the functionality of the models.

https://github.com/vllm-project/vllm/issues/8904
这是一个bug报告，主要涉及Tokenization不匹配问题，由于HuggingFace的AutoTokenizer与vLLM的tokenization存在差异，导致了token生成不一致。

https://github.com/vllm-project/vllm/issues/8903
这是一个关于使用vLLM Docker运行metallama/Llama3.211BVisionInstruct时发生OOM的bug报告，主要涉及GPU服务器规格和运行inference的问题。

https://github.com/vllm-project/vllm/issues/8902
该issue类型为用户提出需求，主要涉及的对象是在VLLM中生成FSM的过程。由于生成复杂JSON模式的FSM过程耗时长，用户希望添加一个特性来保存生成的模式，并在重新部署时重新加载，以节省时间。

https://github.com/vllm-project/vllm/issues/8901
这是一个性能优化类型的 Issue，主要涉及到LLMEngine中的SamplingParams对象。通过移除对SamplingParams的防御性拷贝，在某些场景下可以提高性能，问题提出者想要了解是否理解准确。

https://github.com/vllm-project/vllm/issues/8900
这个issue是关于构建流程的优化，主要涉及到Dockerfile、pyproject.toml和构建脚本，并因为使用过程中出现了"dirty"repo问题而提出优化需求。

https://github.com/vllm-project/vllm/issues/8898
该issue类型为关于性能的提议，主要涉及模型并行性。用户提出了关于如何在vllm中处理模型的权重分布的问题，特别是在某些部分应用了Tensor Parallel（TP）而其他部分没有的情况下。

https://github.com/vllm-project/vllm/issues/8897
这是一份关于Github上vllm项目中关于硬件支持的一个issue，主要内容是为intel GPU添加异步输出处理。

https://github.com/vllm-project/vllm/issues/8896
这是一个关于支持Qwen2.5-Math-RM-72B作为嵌入模型的问题。

https://github.com/vllm-project/vllm/issues/8895
这是一个 Bug 报告，主要涉及 Qwen2-VL-72B 的 API 服务部署问题，可能是由于参数错误导致 AssertionError。

https://github.com/vllm-project/vllm/issues/8894
这是一个bug报告类型的issue，涉及到代码中的环境配置错误，导致了bug的产生。

https://github.com/vllm-project/vllm/issues/8893
这是一个关于Bug的报告，主要涉及的对象是VLLM模型。导致这个Bug的原因可能是无法在forked subprocess中重新初始化CUDA，即使使用了'spawn'启动方法。

https://github.com/vllm-project/vllm/issues/8892
这个issue是一个Bugfix类型的问题，主要涉及vLLM中的Fuyu批量推断功能，修复了在`max_num_seqs>1`时的问题。原因是markdown渲染不起作用，所以使用了原始的html标签。

https://github.com/vllm-project/vllm/issues/8891
这个issue是一个bug报告，主要涉及到vLLM中benchmark_serving.py在测试prefill性能时无法支持`--hf-output-len 1`的问题。

https://github.com/vllm-project/vllm/issues/8890
这是一个bug报告，主要涉及速度提升（speedup）结果的变异，原因可能是运行多次时导致的。

https://github.com/vllm-project/vllm/issues/8889
这是一个Bug报告类型的Issue，涉及vllm的安装问题，用户遇到了安装失败的情况。

https://github.com/vllm-project/vllm/issues/8888
这个issue属于特性增加类型，主要对象是vLLM下的CUDA图捕获功能。由于图捕获通常耗时较长，该提议是为了添加一个标志以允许延迟捕获CUDA图，以改善捕获过程的效率。

https://github.com/vllm-project/vllm/issues/8887
这是一个Bugfix类型的issue，涉及多步调度中的PyObject缓存问题导致单步和多步调度之间的对象混淆。

https://github.com/vllm-project/vllm/issues/8885
这是一个bug报告，涉及到运行推断失败的问题，原因可能是环境配置或程序bug。

https://github.com/vllm-project/vllm/issues/8884
这是一个bug报告，该问题涉及到在XPU上进行多节点TP+PP执行，由于Ray初始化无法连接现有的ray集群导致的问题。

https://github.com/vllm-project/vllm/issues/8882
这是一个性能优化类型的issue，主要涉及到在前端中利用`recv_multipart`的非阻塞和异常特性以减少每个请求中的`poll`操作。原因是在优化前每个`poll`操作最少耗时10微秒。

https://github.com/vllm-project/vllm/issues/8881
这个issue类型为bug报告，涉及主要对象是代码中的 self._async_stopped 变量，由于产生了断言错误导致。

https://github.com/vllm-project/vllm/issues/8880
这个issue是一个Bugfix类型的问题，涉及到OpenGVLab/InternVL2Llama376B的view size与input tensor的size和stride不兼容，导致在调用`view`时出现bug。

https://github.com/vllm-project/vllm/issues/8879
这是关于使用Llama-3.2-11B-Vision-Instruct时发生内存溢出的 bug 报告，涉及的主要对象是在特定环境下运行该版本的Llama。可能是由于当前环境下PyTorch版本不匹配或配置不正确导致的内存溢出问题。

https://github.com/vllm-project/vllm/issues/8878
这个issue类型是bug报告，涉及的主要对象是vllm的安装过程。原因是缺少对CUDA版本要求的说明导致用户无法从源代码编译flash attention模块。

https://github.com/vllm-project/vllm/issues/8877
这个issue是关于bug报告，涉及主要对象为--quantization=awq选项。由于在启动代码中添加quantization=awq或quantization=gptq会导致系统在有太多并发连接时重新启动。

https://github.com/vllm-project/vllm/issues/8876
这是一个关于重命名`PromptInputs`和`inputs`并保持向后兼容性的问题，属于代码维护类型，涉及主要对象为代码变量命名。

https://github.com/vllm-project/vllm/issues/8875
这是一个bug报告，主要对象是torch.compile中的empty tensor的使用。由于使用None而不是empty tensor，导致了关于profiling的问题。

https://github.com/vllm-project/vllm/issues/8874
这是一个bug报告，主要涉及的对象是更新模型测试和示例。这个issue中提到的bug主要是由于一些功能的实现逻辑问题或者期望输出与实际输出不符合等问题导致的。

https://github.com/vllm-project/vllm/issues/8873
这是一个bug报告类型的issue，主要涉及vLLM中的`aqlm`在使用`--cpu-offload-gb`参数时出现失败的情况。

https://github.com/vllm-project/vllm/issues/8872
这个issue类型是bug报告，主要涉及软件中的测试用例无法通过，可能是由于软件升级导致的兼容性问题。

https://github.com/vllm-project/vllm/issues/8871
这个issue是一个[增强功能]类型的报告，涉及到vllm中pallas.py文件的更新以支持trillium，用户提出此需求是因为trillium不支持"lite"加速类型但也不使用megacore，需要对代码进行调整以支持这一新的硬件加速器。

https://github.com/vllm-project/vllm/issues/8870
这个issue是关于bug修复的，涉及主要对象是encoder-decoder models，由于提供`seed`参数会导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/8869
该issue类型为需求提出，主要涉及对聊天模板添加模型上下文信息的功能。由于某些模型对给定提示非常敏感，用户希望能够检测聊天模板中使用的模型，并相应调整输入。

https://github.com/vllm-project/vllm/issues/8868
这个issue是关于bug报告，主要涉及Core模块中关于避免空闲时产生的性能指标日志噪音问题，导致系统在空闲时仍然不断记录日志消息。

https://github.com/vllm-project/vllm/issues/8867
这是一个bug报告，主要涉及 `print_warning_once` 函数在当前主程上打印无用的行信息的问题。这个bug的原因在于...

https://github.com/vllm-project/vllm/issues/8866
这是一个关于性能问题的bug报告，涉及到vLLM和Gradio的性能比较，用户提出vLLM运行速度较慢的问题。

https://github.com/vllm-project/vllm/issues/8865
该issue类型为用户对系统性能进行测试的类型，主要涉及sccache缓存功能与CC以及MQA的比较，在尝试通过vllm的公共sccache bucket来测试缓存命中是否正常。

https://github.com/vllm-project/vllm/issues/8864
这是一个Bug报告，用户希望在使用vLLM时能添加一个flag来禁止输出中包含提示本身的文本，而不是只是文本的情况。

https://github.com/vllm-project/vllm/issues/8863
这是一个关于GPU设备类型推断问题的bug报告，用户在使用vllm时遇到了无法推断GPU设备类型的错误。

https://github.com/vllm-project/vllm/issues/8862
这是一个bug报告，涉及安装vllm在CPU AMD Ryzen 7 PRO 8700GE上时遇到的问题。由于无法在该环境下正常安装，用户提出了无法通过官方文档提供的方法完成安装的问题。

https://github.com/vllm-project/vllm/issues/8861
这是一个bug报告，主要涉及vLLM下的OpenAI兼容API在非流式API使用中未正确生成使用情况信息的问题。

https://github.com/vllm-project/vllm/issues/8860
这是一个GitHub上的issue，类型为工作正在进行中（Work in Progress），主要涉及A100 FP8 Quantization Method和Kernel for PhiMOE。这个issue由于Markdown（标记语言）渲染出现问题，所以使用了原始的HTML来进行标记。

https://github.com/vllm-project/vllm/issues/8859
这是一个关于代码重构的issue，主要涉及到GGUF参数的打包和传递。由于之前的GGUF实现效率较低，需要进行重构以提高性能。

https://github.com/vllm-project/vllm/issues/8858
这个issue是一个bug报告，主要涉及的对象是vllm库中的模型输入。由于在调用scheduler.py文件时出现了断言错误，导致了报错信息为"len(seqs) == 1"。

https://github.com/vllm-project/vllm/issues/8857
这是一个功能请求的issue，涉及主要对象是"qwen2vl"模块。由于无法仅仅依赖图像嵌入来生成新的prompt_token_ids，导致了难以实现此功能的困难。

https://github.com/vllm-project/vllm/issues/8856
这是一个关于添加新模型或改进现有模型的问题，主要涉及支持将图像嵌入作为qwen2vl输入。这个问题可能由于markdown格式错误导致描述内容未正确显示。

https://github.com/vllm-project/vllm/issues/8855
这是一个Bug报告，涉及Llama 3.2 3B Instruct模型的KeyError错误。问题可能由于模型推断过程中出现的'KeyError: 'type''错误导致。

https://github.com/vllm-project/vllm/issues/8854
这是一个关于需求的问题，主要对象是需要支持多模态模型系统消息。这个问题的原因是用户希望在文本查询和/或图像中使用多模态模型，但当前系统消息不兼容此操作。

https://github.com/vllm-project/vllm/issues/8853
这个issue是关于bug报告，涉及到vllm的OpenAI服务器在之前稳定的环境下出现outofmem错误，可能是由于新引擎导致的问题。

https://github.com/vllm-project/vllm/issues/8852
这个issue是关于bug报告，主要涉及vllm在从源代码安装时遇到的问题，由于安装时的输出不符合预期，用户尝试多种方式都未成功，但使用`pip install vllm`时却没有问题。

https://github.com/vllm-project/vllm/issues/8851
这是一个bug报告，涉及的主要对象是在使用Poetry安装时出现错误。由于缺少NumPy模块导致初始化失败，触发了错误的提示信息，最终导致安装失败。

https://github.com/vllm-project/vllm/issues/8850
这个issue类型是Core类型的任务提交，主要涉及到vLLM的AsyncLLMEngine。由于markdown渲染不起作用，所以使用了原始的html代码。

https://github.com/vllm-project/vllm/issues/8849
这是一个bug报告，涉及到VLLM在部署EAGLE-Qwen2-7B-Instruct模型时无法支持EAGLE Spec Decode的问题，导致出现加载权重参数时的错误。

https://github.com/vllm-project/vllm/issues/8847
这是一个关于性能的bug报告，主要涉及vLLM服务器的低吞吐量问题，可能是由于未知原因导致。

https://github.com/vllm-project/vllm/issues/8846
这是一个Bug报告，涉及到VLLM模型的预测结果与OpenCompass不一致。原因可能是模型配置在两个平台上不同导致的。

https://github.com/vllm-project/vllm/issues/8845
这是一个优化和性能改进的issue，主要涉及编译和构建过程，目的是通过在文件级别设置所需的计算能力来减小构建时间和减小wheel文件大小。

https://github.com/vllm-project/vllm/issues/8844
这是一个bug报告，主要涉及vllm的preemption功能在设置sample_n参数为3、5或10时出现Assertion Error的问题。

https://github.com/vllm-project/vllm/issues/8843
这是一个用户提出需求的issue，主要涉及的对象是VLLM模型在TPUs上编译模型图的支持。该问题想知道是否有办法获取或下载已编译好的LLM模型图，以避免在TPU VMs上耗费时间进行图编译。

https://github.com/vllm-project/vllm/issues/8842
这是一个CI/Build相关的bug报告，主要涉及到VLLM项目中的发布流程。由于缺少setuptools-scm导致了构建失败。

https://github.com/vllm-project/vllm/issues/8841
这是一个Bug报告类型的issue，主要涉及vllm在安装过程中出现的错误。由于未定义的符号导致了ImportError，用户在提出该问题时希望获得解决该错误的帮助。

https://github.com/vllm-project/vllm/issues/8840
这是一个Bug报告，主要涉及异常组ExceptionGroup中的未处理错误，原因可能是在TaskGroup中发生了一个子异常。

https://github.com/vllm-project/vllm/issues/8838
这是一个涉及添加加密端点的问题，属于PR提交相关。原因可能是markdown渲染不正确导致的描述问题。

https://github.com/vllm-project/vllm/issues/8837
这个issue是一个[杂项]类型的PR（Pull Request），主要涉及更新配置加载以及移除不必要的配置，由于配置加载问题导致了相关的问题。

https://github.com/vllm-project/vllm/issues/8836
这是一个bug报告，涉及到MQLLMEngine在10000ms内未响应所导致的TimeoutError。

https://github.com/vllm-project/vllm/issues/8835
这是一个用户提出需求的 issue，主要涉及到希望控制抽样器执行顺序的功能请求。

https://github.com/vllm-project/vllm/issues/8834
该issue是一个CI/Build相关的问题，主要涉及缺少ci依赖项，导致版本v0.6.2缺少whl包。

https://github.com/vllm-project/vllm/issues/8832
这是一个bug报告，涉及vllm项目版本v0.6.2缺少whl包的问题。

https://github.com/vllm-project/vllm/issues/8831
这是一个关于前端日志记录最大支持并发性的问题，需要评估可能的最大批量大小。

https://github.com/vllm-project/vllm/issues/8830
这是一个Bug报告，主要问题是由于无效的转义序列'\ '导致了CI测试警告。

https://github.com/vllm-project/vllm/issues/8829
这是一个bug报告，涉及的主要对象是代码库中的测试文件。由于transformers 4.45的升级导致了测试出现问题，需要修复以解决测试无法通过的情况。

https://github.com/vllm-project/vllm/issues/8828
这是一个修复由于`transformers` v4.45.0版本发布导致的测试失败并跳过部分测试的问题。

https://github.com/vllm-project/vllm/issues/8826
该issue类型为需求提出，主要涉及到Llama3.2 Vision Model的功能支持。由于性能尚待优化，用户正在寻求关于如何更好支持该模型的帮助。

https://github.com/vllm-project/vllm/issues/8825
这是一个bug报告，主要涉及的对象是在构建Docker镜像时未包含README文档，导致vLLM PyPi中没有描述。

https://github.com/vllm-project/vllm/issues/8824
这是一个bug报告类型的issue，涉及主要对象是BlockManager V2逻辑，由于未考虑未见token导致无法正确决定能否交换序列组，可能导致NoFreeBlocksAvailable异常出现。

https://github.com/vllm-project/vllm/issues/8823
这是一个功能改进（Feature Improvement）类型的issue，主要涉及Python多进程方法的选择。由于历史遗留代码可能存在问题，需要改善现有选择机制以避免潜在的错误。

https://github.com/vllm-project/vllm/issues/8822
这是一个bug报告，主要涉及MllamaForCausalLM模型的量化支持。

https://github.com/vllm-project/vllm/issues/8821
这是一个用户提出需求的类型的issue，主要涉及vllm与dynamo的兼容性问题。由于需要在pytorch的编译过程中对整数进行特殊处理，用户寻求帮助以解决每次运行模型都会触发重新编译的问题。

https://github.com/vllm-project/vllm/issues/8820
这是一个 bug 报告，主要涉及 mllama 项目的日志记录问题，导致了冗余的警告信息。

https://github.com/vllm-project/vllm/issues/8819
这是一个Bug报告，涉及vllm软件版本升级后导致性能下降的问题，用户询问如何解决相关性能问题。

https://github.com/vllm-project/vllm/issues/8818
这是一个用户提出需求的类型，主要涉及的对象是该项目的安装流程。这个需求提出了让那些只想更改Python文件的用户更容易构建项目的建议。

https://github.com/vllm-project/vllm/issues/8817
这是一个文档更新的Issue，涉及更新Transformers 4.45版本的文档内容。由于Markdown渲染出现问题，需要使用原始的HTML代码。

https://github.com/vllm-project/vllm/issues/8816
这是一个关于VLLM内存使用的问题，类型为用户需求/问题，用户关注VLLM如何分配内存导致GPU消耗过多的情况。

https://github.com/vllm-project/vllm/issues/8815
这是一个bug报告，涉及的主要对象是advance_step.cu文件，由于可能存在警告，需要进行修复。

https://github.com/vllm-project/vllm/issues/8814
该issue类型为Bug报告，涉及到升级gcc版本。原因是使用gcc 9.4时出现了一个bug导致大量日志输出，升级到gcc 10能够解决该问题。

https://github.com/vllm-project/vllm/issues/8813
该issue属于用户提出需求类型，主要涉及如何在vllm中使用BitsAndBytesConfig以及其他quantization参数的配置问题。用户想要运行特定模型的推理，但不清楚如何与vllm集成。

https://github.com/vllm-project/vllm/issues/8812
这是一个用户提出需求的issue，主要涉及升级版本至Llama 3.2的支持。原因是Llama 3.2是一种不同于以前的多模型Llama模型的架构。

https://github.com/vllm-project/vllm/issues/8811
这个issue属于添加新功能的类型，主要涉及的对象是Llama 3.2模型。原因是受阻于HuggingFace Transformers库的发布，导致需要等待相关PR合并才能进行新版本发布。

https://github.com/vllm-project/vllm/issues/8810
这是一个需要撤销之前提交的issue，主要涉及到软件功能的兼容性问题。因为需要发布新模型，导致合并冲突，需要在合并新模型代码后重新应用更改。

https://github.com/vllm-project/vllm/issues/8809
这是一个关于在Neuron设备上增加CPU分析器的issue，由于MQLLM引入后，导致原本的分析器出现了问题，目前只添加了CPU分析器。

https://github.com/vllm-project/vllm/issues/8808
这是一个用户提出需求的issue，主要涉及的对象是希望支持新模型allenai/Molmo-7B-0-0924 VisionLM。其原因是该模型包含视觉信息，而之前的模型不包含视觉信息。

https://github.com/vllm-project/vllm/issues/8807
这个issue类型是Bug报告，主要涉及的对象是模型生成质量。原因可能是升级到新版本后模型生成质量变差。

https://github.com/vllm-project/vllm/issues/8806
这是一个用户需求问题，主要涉及如何在vllm中训练已冻结的模型，由于vllm不支持使用已加载的模型进行训练，用户希望了解是否可以反过来使用vllm已加载的模型来传递给训练器。

https://github.com/vllm-project/vllm/issues/8805
这是一个Bug报告，涉及vllm在不同版本下返回转义Unicode字符串的问题。原因可能是版本不同导致结果差异。

https://github.com/vllm-project/vllm/issues/8804
这个issue类型是一个改进提案，主要涉及到vLLM的Core模块。由于前述功能组合的支持尚未实现，需要进一步完善。

https://github.com/vllm-project/vllm/issues/8803
这是一个bug报告，涉及到vLLM在使用`multiprocessing`和设置`tensor_parallel_size > 1`时出现`leaked shared_memory`警告，可能由于多进程共享内存导致。

https://github.com/vllm-project/vllm/issues/8802
这个issue类型为用户提出需求，主要涉及的对象是Pixtral模型。由于用户需要在Pixtral模型中添加LoRA适配器，以支持其在HF版本上进行finetuning，并请求在未来版本中添加该功能。

https://github.com/vllm-project/vllm/issues/8801
这是一个用户提出需求的类型，主要涉及Marlin MoE内核中动态组块的问题，由于需要性能调查，提议将组块数量作为参数传递给内核，以减小文件大小和编译时间。

https://github.com/vllm-project/vllm/issues/8800
这个issue类型是一个功能改进请求，主要涉及的对象是Marlin MoE内核，由于当前无法直接使用BFloat16输入类型，需要在GPTQ代码路径中执行类型转换。

https://github.com/vllm-project/vllm/issues/8799
这是一个bug报告，主要涉及的对象是在加载具有bitsandbytes 8位量化的模型时出现的问题。这个问题可能是由于PyTorch版本2.4.0与CUDA版本12.1之间的兼容性问题导致的。

https://github.com/vllm-project/vllm/issues/8798
这是一个测试Pull Request，不属于bug报告或需求提出，主要对象是pipeline generator。

https://github.com/vllm-project/vllm/issues/8797
这个issue是一个Bugfix类型的问题，涉及的主要对象是convert_fp8函数，由于函数中存在bug导致测试出现问题。

https://github.com/vllm-project/vllm/issues/8796
这是一个bug报告，主要涉及的对象是vllm中的convert_fp8函数。这个问题是由于错误使用vllm::Fp8KVCacheDataType::kAuto数据类型导致的，正确的类型应该是vllm::Fp8KVCacheDataType::kFp8E5M2。

https://github.com/vllm-project/vllm/issues/8795
这是一个需求提出类型的issue，主要涉及到CI（持续集成）的配置，请求添加CODEOWNERS以及将prefill拆分成单独的测试。

https://github.com/vllm-project/vllm/issues/8794
这是一个用户提出需求的 issue，主要涉及 vLLM 项目中使用本地存储的模型路径时遇到的错误。用户在尝试加载本地存储的 finetuned model 时遇到 wrong_path 错误，尽管路径是正确的。

https://github.com/vllm-project/vllm/issues/8793
这个issue类型是bug报告，主要对象是Qwen2.5模型的长文本处理，由于文档中未明确指出是否支持这个特性，可能导致用户遇到问题。

https://github.com/vllm-project/vllm/issues/8792
这是一个bug报告，涉及的主要对象是在openai server image中缺少特定的依赖包(bitsandbytes和timm==0.9.10)，导致出现ModuleNotFoundError和无法运行MiniCPMV的问题。

https://github.com/vllm-project/vllm/issues/8791
这是一个Bug报告，涉及VLLM项目中端口绑定失败的问题，由于某次提交后出现的bug导致pp>1时绑定失败。

https://github.com/vllm-project/vllm/issues/8790
这是一个bug报告，涉及主要对象是神经网络模型中的全连接层。导致该问题的原因是在代码中，默认情况下未从配置中加载全连接层的偏置项。

https://github.com/vllm-project/vllm/issues/8789
这个issue是一个bug报告，涉及的主要对象是Eagle模型。由于未能从配置文件中加载偏置导致的bug。

https://github.com/vllm-project/vllm/issues/8788
这是一个bug报告，涉及安装vLLM在ROCm上遇到的问题。由于缺少CUDA或HIP安装，导致配置不完整并出现错误。

https://github.com/vllm-project/vllm/issues/8787
这是一个功能需求的issue，主要涉及VLLM项目中添加RWKV v5 (Eagle)支持的任务。

https://github.com/vllm-project/vllm/issues/8786
这是一个用户需求类型的issue，主要涉及到FlashInfer backend的改进。原因是为了集成新的FlashInfer内核，并在FlashInfer后端中实现更多vLLM功能。

https://github.com/vllm-project/vllm/issues/8785
这个issue是一个bug报告，主要涉及到ROCm下的AWQ Marlin支持问题，由于未完全检查平台支持导致了AWQ Marlin与AWQ量化参数不兼容的bug。

https://github.com/vllm-project/vllm/issues/8784
这是一个Bug报告，涉及Disabling Marlin时设置--quantization gptq无效的问题。原因可能是设置quantization gptq只对主模型起作用，而未对草稿模型进行设置。

https://github.com/vllm-project/vllm/issues/8783
这是一个bug报告，涉及vllm模型在进行speculative decoding时出现了输出不一致的问题。

https://github.com/vllm-project/vllm/issues/8782
这个issue是一个功能请求，主要涉及到添加一个名为"Goodput"的用户定义指标，用户提出了从GenAI服务用户角度衡量服务的需求。

https://github.com/vllm-project/vllm/issues/8781
这是一个标题为"[Bugfix] No num_gpus for ROCm and XPU when connecting to a ray cluster"的bug报告，涉及到连接到ray cluster时无法获取ROCm和XPU的gpu数量的问题。

https://github.com/vllm-project/vllm/issues/8780
这是一个bug报告，主要涉及的对象是test_schedule_swapped_simple测试。由于使用BlockManagerV2时，在test_scheduler.py中的测试无法通过，导致了需要修复test_schedule_swapped_simple测试的问题。

https://github.com/vllm-project/vllm/issues/8779
这个issue是关于提出一个新的vLLM引擎架构方案，属于需求提出类型，主要涉及的对象是vLLM引擎的设计。由于当前引擎存在简化性和性能性的困境，导致需要一个新的设计来兼顾两者，同时也希望解决技术债务问题。

https://github.com/vllm-project/vllm/issues/8778
这是一个bug报告，涉及vllm 0.6.1.post2版本的LLMEngine不能被序列化导致的错误。

https://github.com/vllm-project/vllm/issues/8777
这是一个Bugfix类型的issue，主要涉及CI/Build方面的问题，在ROCm 6.2升级后出现了两种不同类型的CI失败，分别导致了OOM错误和测试参数识别错误。

https://github.com/vllm-project/vllm/issues/8776
该issue是一个bug报告，主要涉及的对象是Pascal内核。导致该bug的原因是引入了不支持低于sm_70架构的acquire/release指令，导致在旧架构上编译时出现构建错误。

https://github.com/vllm-project/vllm/issues/8775
Issue type: Bug report
Main object involved: VLLM model
Reason for empty outputs: Possible issue in the predict method of the ChipexpertGenerateServer class.

https://github.com/vllm-project/vllm/issues/8774
这是一个Bug报告，主要对象是vllm库中的ChipexpertGenerateServer类。由于代码中的一个字符串格式错误，导致输出为空。

https://github.com/vllm-project/vllm/issues/8773
这是一个bug报告issue，主要涉及到vllm的输出为空的问题。导致该问题的原因可能是模型响应不符合预期或者输入数据存在问题。

https://github.com/vllm-project/vllm/issues/8772
这是一个特性需求，涉及到将静态项目元数据从`setup.py`迁移到`pyproject.toml`，旨在提高静态元数据管理的效率。

https://github.com/vllm-project/vllm/issues/8771
这是一个bug报告，主要涉及VLLM的构建过程中出现的问题，由于没有正确设置版本信息以及引入了setuptools-scm导致构建失败。

https://github.com/vllm-project/vllm/issues/8770
这个issue是关于[Hardware][CPU]上启用mrope和支持Qwen2-VL的问题。

https://github.com/vllm-project/vllm/issues/8769
这是一个关于性能比较的问题，主要涉及到benchmarking script中生成tokens的总数与指定的输出长度是否一致。由于生成的tokens总数与预期的输出长度不匹配，用户想了解其性能表现并与benchmark_throughput.py进行比较。

https://github.com/vllm-project/vllm/issues/8768
这个issue类型为升级请求，主要涉及的对象是最新版本的软件库`bitsandbytes`，由于旧版本的4bit模型不支持CUDA图，导致用户提出升级请求。

https://github.com/vllm-project/vllm/issues/8767
这是一个Bug修复类的Issue，主要涉及的对象是vLLM在Ray 2.9版本兼容性的问题。由于Ray 2.9版本没有暴露`available_resources_per_node`函数，导致部署vLLM 0.5.5及以上版本时出现错误。

https://github.com/vllm-project/vllm/issues/8765
这个issue是一个bug报告，主要涉及的对象是scheduler。原因是由于一个小的拼写错误导致了问题。

https://github.com/vllm-project/vllm/issues/8764
这是一个bug报告，涉及主要对象是FastAPI和Ray之间的兼容性。该问题是由于FastAPI的升级导致不能与Ray 2.9版本兼容，因此需要允许使用较低版本的FastAPI来维护Ray 2.9的兼容性。

https://github.com/vllm-project/vllm/issues/8763
这是一个bug报告，主要涉及的对象是vllm项目的soft drop beam search功能。由于某种原因，导致在进行beam search请求时会出现"bad request"的问题，需要进行修复。

https://github.com/vllm-project/vllm/issues/8762
这个issue类型是用户提出需求，用户需要获取vllm中的logits信息。这是因为用户想要在运行benchmark_throughput.py时执行softmax优化，但发现vllm的输出中并没有logits信息。

https://github.com/vllm-project/vllm/issues/8761
这是关于功能增强或新功能的 issue ，主要对象是 MQLLMEngine。

https://github.com/vllm-project/vllm/issues/8760
该issue类型是一个功能修改，主要涉及对象是`PromptInputs`、`PromptType`、`inputs`和`prompt`，发起修改的原因是为了重命名这些对象，同时保留了对之前命名的向后兼容性。

https://github.com/vllm-project/vllm/issues/8759
该issue属于用户提出需求并请教问题的类型，主要涉及到vllm的dequantization功能。用户不理解为什么在去量化过程中需要减去1024或64然后再加回去。

https://github.com/vllm-project/vllm/issues/8758
这是一个[CI/Build]类型的issue，涉及将示例文件夹添加到Docker镜像中，以便在提供模型时可以利用模板。

https://github.com/vllm-project/vllm/issues/8757
这是一个bug报告，涉及的主要对象是vllm中的函数cpu_offload_gb，在使用该函数时会导致程序运行错误。

https://github.com/vllm-project/vllm/issues/8756
这是一个Bug报告，关于vllm_cpu中的tensor parallel processes不工作。原因可能是与_CPU环境不兼容所致。

https://github.com/vllm-project/vllm/issues/8755
这是一个bug报告，主要涉及vllm 0.5.1环境中请求报错的问题。可能是由于请求报错导致的bug。

https://github.com/vllm-project/vllm/issues/8753
这是一个bug报告，该问题涉及的主要对象是OLMoForCausalLM模型。由于环境中的PyTorch版本不支持该模型导致的问题。

https://github.com/vllm-project/vllm/issues/8752
这个issue类型是bug报告，主要涉及的对象是test_chunked_prefill_scheduler.py中的测试用例。由于BlockManagerV2在计算所需块数量时不使用Sequence中设置的block_size，导致了这些测试用例失败。

https://github.com/vllm-project/vllm/issues/8751
这是一个feature请求，涉及vLLM中的自定义浮点运行时量化。

https://github.com/vllm-project/vllm/issues/8750
这是一个与代码重命名相关的issue，而非bug报告，主要涉及到`PromptInputs`到`PromptType`以及`inputs`到`prompt`的重命名问题。

https://github.com/vllm-project/vllm/issues/8749
这个issue属于性能分析类型，主要涉及性能仪表板中的数据波动分析，具体包括查找导致性能变化的提交以及进行相关性能问题的跟踪和解决。

https://github.com/vllm-project/vllm/issues/8748
这个issue属于bug报告类型，涉及到了torch dynamo的编译问题，导致了编译错误。

https://github.com/vllm-project/vllm/issues/8747
这是一个bug报告，涉及OLMoE在TP>1时产生错误输出的问题。原因可能是输出被严重降级，导致了严重的评估准确性损失。

https://github.com/vllm-project/vllm/issues/8746
该issue是关于硬件上添加对Neuron的on-device采样支持，属于硬件相关类型的需求。

https://github.com/vllm-project/vllm/issues/8745
这个issue是关于bug报告，涉及的主要对象是vllm模型加载问题，由于缺少版本0.6.1中的关键更新，导致用户在尝试初始化模型时出现错误。

https://github.com/vllm-project/vllm/issues/8744
这是一个关于bug修复的issue，主要涉及到无法序列化特定模型输入，导致服务器在缓存用尽时崩溃的问题。

https://github.com/vllm-project/vllm/issues/8743
这是一个bug报告，主要涉及VLLM bitsandbytes模型比AWQ模型慢的问题，可能是由于bitsandbytes量化处理导致性能下降。

https://github.com/vllm-project/vllm/issues/8742
这是一个特性需求的issue，主要涉及对vllm中的`mm_processor_kwargs`进行推理覆盖支持的功能。

https://github.com/vllm-project/vllm/issues/8741
这是一个Bug修复的Issue，主要涉及对象是Marlin MoE act在is_k_full为False时的顺序问题，导致了在代码中运行内核时规模张量的递增出现错误。

https://github.com/vllm-project/vllm/issues/8740
这是一个需求类型的issue，主要涉及添加 LlamaForSequenceClassification 模型到 vLLM。由于使用`last_hidden_state`在`compute_logits`中导致了形状不匹配的问题。

https://github.com/vllm-project/vllm/issues/8739
这是一个增加模型功能的issue，主要涉及的对象是vLLM框架下的LlamaForSequenceClassification模型。由于返回`last_hidden_state`引起了形状不匹配问题，需要解决这个bug并补充文档、添加测试。

https://github.com/vllm-project/vllm/issues/8738
这是一个bug报告，主要涉及VLLM模型在负载增加时由于KV缓存达到100%而导致服务器崩溃的问题。

https://github.com/vllm-project/vllm/issues/8737
这是关于使用 FusedMoE 模型的一个需求指引类型的 issue，主要涉及到 deepseek 模型使用问题。

https://github.com/vllm-project/vllm/issues/8736
这是一个Bug报告，主要涉及的对象是vllm下的InternVl2-8B-AWQ模型，在运行中出现了进程启动错误。

https://github.com/vllm-project/vllm/issues/8735
这是一个bug报告，主要涉及的对象是程序在生成GPU P2P访问缓存时出现了卡住的问题。

https://github.com/vllm-project/vllm/issues/8734
这是一个需求提出类型的issue，主要涉及到依赖管理工具dependabot。由于尚未配置所有依赖，导致目前仅与GitHub Actions有关，用户希望启用dependabot管理已知漏洞。

https://github.com/vllm-project/vllm/issues/8733
这个issue是一个Bugfix类型的报告，主要涉及cpu_model_runner缺少输入intermediate_tensors的问题，导致internlm2模型无法工作。

https://github.com/vllm-project/vllm/issues/8732
这是一个bug报告类型的issue，主要涉及VLLM模型的使用过程中出现的TypeError错误。由于RPCServer进程在回应准备探测之前终止导致无法对模型进行输入转储的操作。

https://github.com/vllm-project/vllm/issues/8731
这是一个bug报告，涉及的主要对象是在使用vllm时遇到OutOfMemoryError的问题，可能是由于GPU内存分配不正确导致。

https://github.com/vllm-project/vllm/issues/8730
这是一个用户提出需求的issue，主要涉及的对象是为VLLM添加支持加载本地文件（图片和视频）的多模态功能。由于当前环境中PyTorch版本为2.4.0，CUDA版本为12.0，但在加载本地文件时遇到了问题。

https://github.com/vllm-project/vllm/issues/8729
这个issue是关于[Hardware][CPU] Refactor CPU model runner的类型为改进代码结构，主要涉及到CPU模型运行程序，由于需要使用其他模块来重构`cpu_model_runner.py`，所以提交了该问题。

https://github.com/vllm-project/vllm/issues/8728
这个issue是一个bug报告，主要涉及的对象是test_scheduler.py中的测试用例。由于BlockManager V2的变化导致测试失败，需要更新测试代码以适配新的BlockManager。

https://github.com/vllm-project/vllm/issues/8727
这个issue属于[Misc]类型，涉及添加conftest插件用于应用forking装饰器，可能是由于缺乏适当的清理问题导致。

https://github.com/vllm-project/vllm/issues/8726
这是一个用户提交的需求类型的issue，主要涉及到重新实现vllm核心上的beam search。由于缺少具体信息，无法确定提交者或具体需求。

https://github.com/vllm-project/vllm/issues/8725
这是一个用户提出需求的issue，主要涉及VLLM支持本地LORA路径而非Huggingface的LORA路径，可能出现的原因是相关文档信息不清晰导致用户困惑。

https://github.com/vllm-project/vllm/issues/8724
这是一个缺少PR描述的issue，属于Core类型，主要涉及支持valkey数据库的问题。由于缺少PR描述导致这个issue的内容未能完整表达。

https://github.com/vllm-project/vllm/issues/8723
这个issue是一个bug报告，涉及主要对象是vLLM的CPU模式构建，由于依赖于已被删除的默认依赖，导致构建失败。

https://github.com/vllm-project/vllm/issues/8722
这是一个Bug报告，主要涉及的对象是vLLM的CPU后端构建问题，由于缺少cmake依赖导致CPU模式构建失败。

https://github.com/vllm-project/vllm/issues/8721
这是一个Bug报告，涉及的主要对象是CUDA内存错误。原因是由于CUDA内存不足导致了`torch.OutOfMemoryError`错误。

https://github.com/vllm-project/vllm/issues/8720
这是一个用户需求问题，主要涉及的对象是如何加载使用8位二进制量化的模型。用户想知道如何改变代码以将模型加载为8位而不是4位。

https://github.com/vllm-project/vllm/issues/8719
这是一个bug报告，涉及主要对象是VLLM模型在NVIDIA A100上的运行性能不稳定。原因可能是由于不同大小文本的批量操作导致性能不一致，同时推测可能是由于Prompt缓存、GPU利用率、消息处理速度等因素导致了推理性能波动。

https://github.com/vllm-project/vllm/issues/8718
这个issue是关于Bug报告，主要涉及到vllm项目中的BlockManager V1和BlockManager V2的单元测试失败问题，可能是由于BlockManager V2的更新导致现有测试用例无法通过。

https://github.com/vllm-project/vllm/issues/8717
这是一个关于添加OOT multimodal模型注册测试的issue，涉及到vLLM代码库的Core部分，可能是由于新功能的开发和测试未完全完成所致。

https://github.com/vllm-project/vllm/issues/8716
这个issue是关于提出需求的类型，主要涉及VLLM（Very Large Language Model）的量化和性能优化。它提出了对ALPINDALE模型实现量化的改进方案，并寻求关于设计方面的反馈。

https://github.com/vllm-project/vllm/issues/8715
这是一个bug报告，主要涉及mistral-common模块的升级，由于需要解决https://github.com/vllmproject/vllm/issues/8650中的问题。

https://github.com/vllm-project/vllm/issues/8714
这是一个bug报告，涉及到vLLM在ROCm 6.0.3下编译错误的问题，原因是__hip_bfloat16在该版本不支持+操作符。

https://github.com/vllm-project/vllm/issues/8713
这是一个用户需求类型的issue，需求是针对构建设置的调整。

https://github.com/vllm-project/vllm/issues/8712
这个issue类型是用户提出需求，主要对象是优化vllm中的Prefix Caching triton kernels编译过程，避免在首次运行时导致e2e性能下降。

https://github.com/vllm-project/vllm/issues/8711
这是一个Bug报告，涉及的主要对象是vllm 0.6.0版本中启动qwenvl7b时出现的异常。问题可能由于调用多次后导致模型无返回结果。

https://github.com/vllm-project/vllm/issues/8710
这是一个 bug 报告，涉及到 vllm 在 CPU 模式下安装失败的问题。原因可能是安装过程中遇到了错误无法继续。

https://github.com/vllm-project/vllm/issues/8709
这是一个Bug报告，涉及的主要对象是tool_parser。这个问题是由于在使用不支持tool调用的模型时，tool_parser错误处理导致流式对话完成失败并引发与tool调用无关的错误。

https://github.com/vllm-project/vllm/issues/8708
该issue类型为bug报告，主要涉及的对象是VLLVM中关于支持f16算术和比较指令的功能，出现此问题的原因是当前环境需要支持.target sm_53或更高版本。

https://github.com/vllm-project/vllm/issues/8707
这是一个bug报告，涉及主要对象为VLM模型（PaliGemma、Fuyu和Persimmon），由于最新transformers版本中删除了config.vocab_size，导致相关代码出现错误，需要改为使用config.text_config.vocab_size。

https://github.com/vllm-project/vllm/issues/8706
这是一个关于功能使用的问题，涉及到max_tokens和max_model_len参数的差异，用户想知道它们之间是否有区别。

https://github.com/vllm-project/vllm/issues/8705
这个issue是一个[Patch]类型的问题，涉及的主要对象是Multi-image example。原因是为了让读者更清楚返回的内容，因为当前返回的tuple中有些项对于大多数类型都是None。

https://github.com/vllm-project/vllm/issues/8704
该issue是关于代码改动的，主要涉及到deprecating block manager v1，将block manager v2设为默认值，简化代码路径。

https://github.com/vllm-project/vllm/issues/8703
这个issue属于需求提出类型，主要对象是项目中的`CudaMemoryProfiler`，由于命名不够准确可能导致混淆，用户提出应将其重命名为`DeviceMemoryProfiler`。

https://github.com/vllm-project/vllm/issues/8702
这个issue类型是bug报告，涉及的主要对象是CUTLASS的revision检测功能。由于当前使用方式不兼容，导致出现了错误信息或打印错误的hash的问题。

https://github.com/vllm-project/vllm/issues/8701
这个issue类型为代码优化类型，主要涉及到的对象是bonus token逻辑。由于之前的bonus token逻辑已经过时，PR的目的是清理并移除这部分逻辑。

https://github.com/vllm-project/vllm/issues/8700
这个issue类型是用户提出需求，主要涉及添加对序列分类模型的支持，由于需要建立高质量的合成数据流水线、验证模型推理和多代理系统等需求，所以用户提出了新增对Skywork/SkyworkRewardLlama3.18B等序列分类模型的支持。

https://github.com/vllm-project/vllm/issues/8699
该issue类型是bug报告，涉及的主要对象是vllm中的flash attention模块。由于什么样的原因导致了什么样症状的bug或者用户提出了关于什么的问题或者寻求什么样的帮助，无法从提供的信息中得知。

https://github.com/vllm-project/vllm/issues/8698
这是一个Bug报告，主要涉及的对象是vllm下的AMD MI250，由于GPU利用率高但吞吐量低，导致了性能问题。

https://github.com/vllm-project/vllm/issues/8697
这是一个bug报告，涉及的主要对象是vLLM中加载Qwen 2.5 GGUF q3模型时出现了AssertionError。由于加载的权重形状与预期的词汇大小不匹配，在vocab_parallel_embedding.py文件中触发了这个错误。

https://github.com/vllm-project/vllm/issues/8696
该issue为一个Model类型的问题，涉及的主要对象是为qwen2vl添加pp支持，可能由于需要在最新的qwen2vl上引入pp支持而引起。

https://github.com/vllm-project/vllm/issues/8694
这是一个功能需求的issue，主要涉及vLLM的Memory Tiering功能。由于需求增加了Context Caching with TTL支持、Blockwise swapping for DRAM and Disk、Layered transfer between DRAM and HBM等功能，需要进行相关功能的开发和实现。

https://github.com/vllm-project/vllm/issues/8693
这是一个bug报告，主要涉及Pixtral-12B模型在CPU上不被支持，导致无法运行的问题。

https://github.com/vllm-project/vllm/issues/8692
这是一个关于支持在Neuron设备上进行多节点推理的GitHub issues，主要涉及Neuron Transformers Neuronx库的使用。

https://github.com/vllm-project/vllm/issues/8691
这是一个bug报告，主要涉及到使用`--tensor-parallel-size 4`在Qwen2.5-72B-Instruct模型上出现错误，可能是由于参数设置错误或代码逻辑问题导致的。

https://github.com/vllm-project/vllm/issues/8690
这是一个bug报告，涉及到vllm中的Custom Allreduce功能的性能优化问题，主要是因为作者在移除内存栅栏并启用__ldcv后发现性能提升，希望得到在不同硬件上的测试和优化指导。

https://github.com/vllm-project/vllm/issues/8689
这是一个文档修复类型的issue，主要涉及到AMD安装指南中的错字问题。由于文档中存在错误，导致Markdown渲染出现问题，需要使用原始HTML进行更正。

https://github.com/vllm-project/vllm/issues/8688
这个issue类型为功能需求变更，主要涉及的对象是输入数据类型的重命名。由于代码中存在一些命名不规范或者过时的输入数据类型，需要进行相应的更名以提高代码的可读性和维护性。

https://github.com/vllm-project/vllm/issues/8687
这是一个功能优化的issue，主要涉及VLM的更新和优化，包括使用新的构造函数和处理BLIP dummy sequence data。

https://github.com/vllm-project/vllm/issues/8686
这是一个bug报告，该问题涉及的主要对象是vllm0.6.1.post2版本的A800模型。由于某种原因导致在运行benchmark_throughput.py时出现了RuntimeError。

https://github.com/vllm-project/vllm/issues/8685
这个issue属于功能需求反馈类型，主要涉及了vLLM模型对HF版本的Pixtral模型提供支持的问题。由于HF的Pixtral模型使用了不同的格式，导致需要调整vLLM的支持来满足新模型，以便实现模型的量化功能。

https://github.com/vllm-project/vllm/issues/8684
这是一个需求反馈类型的issue，主要对象是beam search功能，用户提出需要添加输出以手动检查正确性。

https://github.com/vllm-project/vllm/issues/8683
这个issue是一个功能需求，主要对象是改进分布式后端选择。出现这个问题的原因是在初始化CUDA上下文时，使用基于`fork`的多进程方法无法正常工作。

https://github.com/vllm-project/vllm/issues/8682
这个issue类型是文档反馈，主要对象是Qwen2-VL-72B。由于文档不清晰，用户提出了潜在的改进建议并寻求相关支持。

https://github.com/vllm-project/vllm/issues/8681
这个issue是一个关于bug报告的问题单，涉及到QLoRA推断返回交替结果的问题，可能是由于adapter的使用导致。

https://github.com/vllm-project/vllm/issues/8680
这是一个bug报告，主要涉及VLLM serve Gemma 2 9B在处理超过4096个tokens时的问题，可能由环境配置或代码逻辑导致。

https://github.com/vllm-project/vllm/issues/8679
这是一个用户提出需求的类型，涉及主要对象为系统路径指定。原因可能是安装过程强制要求git克隆CUTLASS，而用户已经在系统中编译了CUTLASS，希望能够在安装时指定CUTLASS路径。

https://github.com/vllm-project/vllm/issues/8678
这是一个用户提出需求的issue，主要涉及将BlockSpaceManagerV2设置为默认的BlockManager。由于要逐步废弃BlockManager V1，因此用户提出希望在此过程中将BlockSpaceManagerV2设为默认选项。

https://github.com/vllm-project/vllm/issues/8677
这是一个bug报告，该问题涉及的主要对象是Vllm项目中的Neuron + Vllm inference功能。由于Neuron目前仅支持pytorch 2.1，而最近的更改使用了仅支持torch>2.4的新Python自定义Op高级API，导致在安装步骤和运行离线推理脚本时出现错误。

https://github.com/vllm-project/vllm/issues/8676
这个issue是一个bug报告，涉及的主要对象是更新neuron sdk到2.20版本。导致这个issue的原因是neuron sdk更新到2.20版本后，可能出现了某些问题或者需要特定的更新操作。

https://github.com/vllm-project/vllm/issues/8675
这是一个功能需求类型的issue，主要涉及的对象是`SequenceData`和`Sequence`类。由于代码重复和缓存处理不当，导致需要添加新的构造函数以及更改属性的缓存机制。

https://github.com/vllm-project/vllm/issues/8674
这是一个关于硬件升级的Issue，涉及到AMD GPU和ROCm 6.2的支持。

https://github.com/vllm-project/vllm/issues/8673
这是一个类型为功能需求变更的issue，主要涉及对象是代码中的`PromptInputs`和`inputs`，由于与`LLMInputs`的混淆导致需要对命名进行调整。

https://github.com/vllm-project/vllm/issues/8672
该issue类型是一个为提出需求，主要涉及的对象是OpenAI API server的FastAPI middleware层，用户寻求了关于在FastAPI middleware层中传播使用计算的支持，以便跟踪请求活动。

https://github.com/vllm-project/vllm/issues/8671
这是一个文档更新类型的issue，主要涉及到neuron的文档内容。由于文档不够清晰或者有小的错漏，导致需要进行一些澄清性的修改。

https://github.com/vllm-project/vllm/issues/8670
这个issue属于[CI/Build]类别，涉及的主要对象是删除ROCm构建中的一个测试。这个问题可能是因为测试文件不再需要或者有其他原因造成构建失败。

https://github.com/vllm-project/vllm/issues/8669
这是一个bug报告，主要涉及vLLM下llama 3.1 70B GGUF Q4模型在A100 80G GPU上推理速度慢的问题，可能由配置问题导致。

https://github.com/vllm-project/vllm/issues/8668
这是一个Bug报告类型的Issue, 主要对象是vllm中运行serve命令时出现的JSONDecodeError，可能是由于数据输入问题导致的。

https://github.com/vllm-project/vllm/issues/8667
该issue类型为功能需求，主要涉及到vllm不支持外部树形多模态模型的问题，由于vllm版本>=0.6不支持此功能。

https://github.com/vllm-project/vllm/issues/8666
这是一个bug报告，涉及的主要对象是vllm项目下的Dockerfile文件。这个issue的产生是由于Dockerfile中存在硬编码依赖cuda 12.1的问题。

https://github.com/vllm-project/vllm/issues/8665
该issue属于Bug报告，主要涉及vllm在GPU环境下安装问题。由于无法安装vllm在GPU上，导致用户提交了该问题。

https://github.com/vllm-project/vllm/issues/8664
这是一个用户提出需求的issue，主要涉及vLLM如何使用generation_config.json作为默认生成配置的问题。原因是当前配置与generation_config.json不一致，用户想了解是否可以让vLLM默认使用generation_config.json。

https://github.com/vllm-project/vllm/issues/8663
这是一个[Model]类型的issue，主要涉及的对象是GLM-4v模型的支持。这个issue是因为需要添加对GLM-4v模型的支持以及满足vllm==0.6.1.post2+cu123版本的要求。

https://github.com/vllm-project/vllm/issues/8662
这是一个Bug报告，涉及到引擎在更新采样参数时出现了意外行为。

https://github.com/vllm-project/vllm/issues/8661
这个issue属于优化类型，主要对象是Marlin MoE kernels的编译过程。由于将Marlin MoE kernels拆分为多个文件，以实现更快的并行编译，从而显著减少了编译时间。

https://github.com/vllm-project/vllm/issues/8660
这个issue是一个bug报告，主要涉及的对象是vLLM 0.6.0 CPU后端和Gemma2模型。由于vLLM 0.6.0 (cpu)在加载Gemma2模型时出现错误，导致用户无法正常使用该模型。

https://github.com/vllm-project/vllm/issues/8659
这是一个bug报告，主要涉及的对象是MarlinMoE kernels。由于代码中的act_order相关代码当前未使用，因此需要将其清理掉，同时确保is_k_full在这些内核中始终为true，这样的需求可能是为了优化代码或确保正确性。

https://github.com/vllm-project/vllm/issues/8658
这个issue类型是bug报告，主要涉及的对象是vllm中的phi3v模型。由于没有正确处理`num_crops`参数，导致一些测试用例无法正常通过，需要在处理`num_crops`参数的地方进行修复。

https://github.com/vllm-project/vllm/issues/8657
这是一个用户提出需求的issue，主要涉及多模态处理器kwargs参数的支持。问题是为了能够在初始化时传递processors_kwargs来覆盖数值，特别是在多模态模型中提供有效的processor_kwargs，并提供简洁的实现方式。

https://github.com/vllm-project/vllm/issues/8656
这是一个bug报告，涉及的主要对象是重构组合权重加载逻辑。这个问题可能由于加载权重时出现意外权重导致的bug。

https://github.com/vllm-project/vllm/issues/8655
这是一个bug报告，涉及对象为is_xpu函数。由于缺少vllm._C模块导致出现了ModuleNotFoundError的错误提示。

https://github.com/vllm-project/vllm/issues/8654
这是一个Bug报告类型的issue，涉及到vllm库的运行时错误。由于某种原因导致了运行时错误，具体原因可能是在`determine_num_available_blocks`函数中的代码逻辑问题导致了该Bug。

https://github.com/vllm-project/vllm/issues/8653
这个issue是一个Misc类型的PR，涉及非CUDA hf benchmark_througput的添加，用于与openvino后端进行比较。

https://github.com/vllm-project/vllm/issues/8652
这是一个Bug报告类型的Issue，主要涉及的对象是针对XPU的安装问题，可能由于CC无法安装、Dockerfile构建失败或源文件构建错误而导致安装失败。

https://github.com/vllm-project/vllm/issues/8651
这个issue是性能优化建议，主要对象是在使用cutlass的情况下默认采用per_token量化为fp8，以提高性能。

https://github.com/vllm-project/vllm/issues/8650
这是一个bug报告类型的issue，主要涉及对象是vllm项目中的一个模块，由于模块中的某个属性引发了 AttributeError: module 'cv2.dnn' has no attribute 'DictValue'错误。

https://github.com/vllm-project/vllm/issues/8649
这是一个功能需求提案，主要涉及对象是`collect_env.py`，用户想要显示AMD GPU的拓扑结构。

https://github.com/vllm-project/vllm/issues/8648
这个issue是关于[Frontend]批量推断功能的改进。

https://github.com/vllm-project/vllm/issues/8647
这是一个需求提出的issue，主要涉及前端的改进，即为`llm.chat()` API添加批量推断功能。因为目前API仅支持单个对话进行推断。

https://github.com/vllm-project/vllm/issues/8646
这是一个性能优化相关的issue，主要涉及到代码中的tl.atomic_add导致MI300性能严重下降的问题。

https://github.com/vllm-project/vllm/issues/8645
这是一个关于优化性能的issue，涉及vLLM中CUDA图的使用问题。原因是多步骤和分块预取在`main`上的使用不够充分，导致性能不佳。

https://github.com/vllm-project/vllm/issues/8644
这个issue是关于修复Quark quantized fp8 llama3.1模型在调用时出现HIPBLAS_STATUS_NOT_SUPPORTED错误的bug报告。

https://github.com/vllm-project/vllm/issues/8643
这是一个bug报告，涉及到marlin_moe_ops.cu文件中部分代码的删除问题。由于之前的修改遗漏了一些内容，导致仍存在需要删除的无用代码。

https://github.com/vllm-project/vllm/issues/8642
该issue类型为功能需求，主要对象是项目的安全性。由于项目缺乏安全指引，用户建议创建一个名为"SECURITY.md"的文件以提高项目的安全性。

https://github.com/vllm-project/vllm/issues/8641
这是一个Bug报告，涉及使用FlashInfer处理FP8模型时出现的错误。

https://github.com/vllm-project/vllm/issues/8640
这是一个Bug修复类型的Issue，涉及到MistralTokenizer的非UTF Unicode标记的处理。原因是在Tekken处理中，逐个解码标记是危险的，导致无效标记不能正确解码。

https://github.com/vllm-project/vllm/issues/8639
该issue是性能问题，主要涉及到了accept rate of typical acceptance sampling。可能由于性能回归导致了实验结果不符合预期。

https://github.com/vllm-project/vllm/issues/8638
这是一个Bug报告，涉及的主要对象是加载嵌入模型intfloat/e5-mistral-7b-instruct时出现绑定错误。由于CUDA版本与cuDNN版本不兼容所导致的。

https://github.com/vllm-project/vllm/issues/8637
这是一个Bug修复类型的issue，该问题涉及vLLM中的多步调度和beam search功能，由于启用`best_of>1`时导致失败，因此需要禁用多步调度来解决这个问题。

https://github.com/vllm-project/vllm/issues/8636
这是一个用户提出需求的issue，主要涉及vLLM与Ray结合使用的问题，用户想要利用Ray进行离线批量推断。

https://github.com/vllm-project/vllm/issues/8635
这是一个issue类型为Bug报告，涉及到vllm项目中有关"Fish merge"的问题。由于Markdown渲染无法正常工作，导致需要在原始HTML中填写PR描述，可能是由于渲染问题导致无法正确呈现PR的检查清单和描述。

https://github.com/vllm-project/vllm/issues/8633
这是一个用户提出需求的issue，主要对象是针对OpenAI o1模型的Chain-of-thought（CoT）推理工作流的功能和改进提案。

https://github.com/vllm-project/vllm/issues/8632
这个issue属于bug报告类型，主要涉及vLLM中加载量化模型与非量化模型之间的区别，用户遇到问题是加载量化模型时没有执行自定义的logging代码。

https://github.com/vllm-project/vllm/issues/8631
这是一个提出需求的issue，主要对象是希望在本地模型上使用OpenAI Python SDK进行在线推理，可能由于现有实现不兼容导致相关错误。

https://github.com/vllm-project/vllm/issues/8630
这是一个Bug报告类型的issue，涉及到OpenGVLab/InternVL2-Llama3-76B模型的view size与input tensor的size和stride不兼容的问题。

https://github.com/vllm-project/vllm/issues/8629
这是一个Bug报告类型的Issue，涉及的主要对象是模型的内存泄漏问题，由于内存泄漏导致了程序出现了异常行为。

https://github.com/vllm-project/vllm/issues/8628
这是一个Bug报告，涉及到VLLM中的Speculative decoding在CPU执行中参数被忽略导致只返回1个结果的问题。

https://github.com/vllm-project/vllm/issues/8627
这是一个Bug报告，主要涉及MistralTokenizer的Detokenization问题，由于某些语言的编码错误导致症状。

https://github.com/vllm-project/vllm/issues/8626
这是一个bug报告，主要涉及对象是vllm在pascal tesla P100上的不兼容性问题，可能是由于GPU计算能力较低导致无法正常运行。

https://github.com/vllm-project/vllm/issues/8625
这是一个bug报告，涉及vllm中关于"completion_tokens"计数不正确的问题，导致在使用streaming时每次请求的completion_tokens在最后一个token之前都始终为1。

https://github.com/vllm-project/vllm/issues/8624
这个issue属于bug报告类型，涉及对象为vllm中的qwen2-vl模块。由于缺少'gelu_quick'属性导致AttributeError错误。

https://github.com/vllm-project/vllm/issues/8622
这是一个功能需求类型的issue，主要涉及如何获得给定输入和输出的logps。用户提出了关于如何获取给定输入和输出logps的问题。

https://github.com/vllm-project/vllm/issues/8620
这是一个bug报告类型的issue，主要涉及vllm的部署功能，由于某种原因导致了测试样本结果的加速度不理想，导致了draft acceptance rate为0.000。

https://github.com/vllm-project/vllm/issues/8619
这是一个关于代码优化的issue，主要涉及的对象是VLLM的logits resort操作，由于原代码实现复杂，用户提出优化简化其过程。

https://github.com/vllm-project/vllm/issues/8618
这个issue类型是文档修复和改进，主要涉及GGUF量化文档的增加。用户提出了关于GGUF模型使用docker、如何使用openai兼容api运行GGUF模型以及运行GGUF模型需要模板的问题。

https://github.com/vllm-project/vllm/issues/8617
这是一个用户提出需求的issue，主要涉及到vllm的在线推理服务。导致这个问题的原因是用户想要获取当前队列中的请求数量或正在处理中的请求数量。

https://github.com/vllm-project/vllm/issues/8616
这个issue是一个Bugfix类型的报告，主要涉及的对象是sonnet benchmark在benchmark_serving.py中的bug，导致在multimodal更新后出现问题，需要添加None到输入请求并处理数值解包不匹配的情况。

https://github.com/vllm-project/vllm/issues/8615
这个issue类型为Misc，涉及主要对象是benchmark_throughput.py文件中的engine_use_ray选项。这个issue的提出可能是由于engine_use_ray选项被误用或者不再需要，需要从代码中移除。

https://github.com/vllm-project/vllm/issues/8614
这是一个bug报告，主要涉及VLM中InternVL在num_scheduler_steps大于1时出错的问题。

https://github.com/vllm-project/vllm/issues/8613
这是一个Bug报告，涉及vllm在部署模型时出现的矩阵形状不匹配导致的RuntimeError。

https://github.com/vllm-project/vllm/issues/8612
这是一个bug报告，涉及的主要对象是Prometheus /metrics端点，导致metrics端点无法返回任何内容。

https://github.com/vllm-project/vllm/issues/8610
这个issue属于用户需求类型，主要涉及vllm的性能测试，用户想了解官方网站是否提供性能测试信息。可能由于用户对vllm的性能表现感兴趣，希望了解官方是否有相关数据可供参考。

https://github.com/vllm-project/vllm/issues/8609
这个issue是一个bug报告，涉及的主要对象是pytorch。由于新版本的pytorch将`CUDA_CUDA_LIB`变量重命名为`CUDA_CUDA_LIBRARY`，导致了编译时的问题。

https://github.com/vllm-project/vllm/issues/8593
这是一个Bug报告，涉及主要对象是vllm项目中的mistral tokenizer，由于禁用了引导解码会导致mistral模型的所有请求出现内部服务器错误。

https://github.com/vllm-project/vllm/issues/8588
这个issue类型是[Model]，主要关于支持使用FP8 pertensor weights and activations压缩张量格式的MoE模型。

https://github.com/vllm-project/vllm/issues/8586
这是一个关于如何进行vLLM引擎后端独立调试和性能评估的询问类型issue，主要涉及vLLM引擎的调试和性能测量。 

https://github.com/vllm-project/vllm/issues/8584
这是一个用户提出需求的issue，主要涉及前端部分的代码。

https://github.com/vllm-project/vllm/issues/8583
这是一个关于bugfix的issue，涉及到MQLLMEngine的健康检查问题。由于健康检查请求排在主输入套接字上许多输入请求的队列之后，导致客户端在等待引擎处理所有输入并响应时超时。

https://github.com/vllm-project/vllm/issues/8582
这是一个关于需求的问题，主要涉及VLLM在多个TPU主机上的运行问题。由于VLLM未能正确检测和利用所有4个主机，导致用户无法在更大模型上运行。

https://github.com/vllm-project/vllm/issues/8581
这个issue是一个关于提出需求的类型，主要涉及到改进sampling功能。由于现有的samplers在小模型中存在重复问题，需要一种能够完全避免重复的新的sampler，以提高结果的一致性。

https://github.com/vllm-project/vllm/issues/8580
这是一个Bug报告类型的issue，涉及主要对象是Gemma2模型在8k上下文长度情况下出现错误响应。该问题可能是由于启用了flashinfer后，在发送长提示时出现了重复相同的第一句话的bug。

https://github.com/vllm-project/vllm/issues/8579
这是一个Bug报告类别的issue，涉及主要对象是在使用FP32精度和4个H100 GPUs上出现Triton断言错误，可能由于环境设置或代码逻辑问题导致此现象。

https://github.com/vllm-project/vllm/issues/8578
这是一个Bug报告，主要涉及lm-format-enforcer guided decoding功能导致MQLLMEngine崩溃的问题。由于引擎偶尔会出现步骤耗时过长，导致服务器无法响应健康检查请求。

https://github.com/vllm-project/vllm/issues/8577
这个issue是一个Kernel类型的提议，主要涉及支持fp8 kv缓存用于rocm自定义分页关注，由于此前Custom paged attention kernel for rocm已经存在，现在此提议添加了支持fp8 kv缓存的功能。

https://github.com/vllm-project/vllm/issues/8576
这个issue是关于功能改进的提议， 主要涉及到多进程方法的默认设置。由于`fork`方法已知存在问题，因此提议将默认设置更改为`spawn`。

https://github.com/vllm-project/vllm/issues/8575
这是一个bug报告，涉及到使用IPv6时需要显式指定IPv6套接字选项的问题。

https://github.com/vllm-project/vllm/issues/8574
这个issue是一个Bugfix类型的问题，主要涉及到vLLM的engine client，由于缺少`dead_error`属性导致出现了新的stack trace。

https://github.com/vllm-project/vllm/issues/8573
这个issue是一个优化类型的问题，主要涉及Marlin MoE中关于删除`thread_m_blocks`模板参数的优化。

https://github.com/vllm-project/vllm/issues/8572
这个issue是一个bug报告，涉及到改进MQLLMEngine在启动失败时的错误处理方式，目的是通过引发异常来导致主进程的非零退出码。

https://github.com/vllm-project/vllm/issues/8571
该issue是一个Bug报告，涉及的主要对象是Phi3.5 mini和MoE LoRA推断。由于LoRA推断bug，作者创建了一个新的模型类来解决问题。

https://github.com/vllm-project/vllm/issues/8570
这是一个bug报告，主要涉及使用docker容器时选择GGUF模型quant版本出错的问题，导致CUDA内存不足错误。

https://github.com/vllm-project/vllm/issues/8569
该issue属于用户提出问题类型，主要涉及SamplingType.RANDOM_SEED和SamplingType.RANDOM的差异，以及关于SamplingParams中'seed'参数的使用疑问。

https://github.com/vllm-project/vllm/issues/8568
这是一个bug报告，涉及的主要对象是"tool_choice"的设置问题，由于设置问题导致了验证失败。

https://github.com/vllm-project/vllm/issues/8567
这是一个关于bug报告的issue，主要涉及vllm的OpenAI API在本地进行离线批量推理时出现错误的问题，可能是由于端点使用问题或初始化问题导致了404错误。

https://github.com/vllm-project/vllm/issues/8566
这是一个功能需求类型的issue，主要涉及对象是vllm库，用户提出希望实现离线FP8量化功能。

https://github.com/vllm-project/vllm/issues/8565
这是一个Bug报告，涉及vllm在Intel GPU ARC 770上无法被导入的问题，可能是由于构建docker时出现了问题。

https://github.com/vllm-project/vllm/issues/8564
这是一个bug报告，涉及的主要对象是在基准测试动态提供的LoRA适配器时遇到的问题。由于环境设置或者程序逻辑错误，导致无法成功加载LoRA适配器并执行模型评估的问题。

https://github.com/vllm-project/vllm/issues/8563
这是一个bug报告，涉及vllm的安装问题，用户在安装vllm时遇到了安装失败的问题。

https://github.com/vllm-project/vllm/issues/8562
这是一个bug报告，涉及的主要对象是typical acceptance sampler。这个问题可能是由于token ids 恢复不正确导致的。

https://github.com/vllm-project/vllm/issues/8561
这个issue是关于功能建议类型的，主要涉及到为vllm创建ProfileConfig用于个性化配置性能分析。由于用户在使用过程中关注内存使用情况，提出了希望提供灵活性的配置选项的建议。

https://github.com/vllm-project/vllm/issues/8560
这是一个Bug报告类型的Issue，涉及到vllm中Profiling Runtime报错，用户试图将模型修改为本地模型Llama27bhf导致的问题。

https://github.com/vllm-project/vllm/issues/8559
这是一个bug报告，主要涉及VLLM中LoRA Ranks的动态加载行为，因为在使用rank 16时出现了操作明显变慢的情况。

https://github.com/vllm-project/vllm/issues/8558
这是一个bug报告，涉及的主要对象是Custom All Reduce OP，发生hang的原因是使用了不安全的自定义allreduce同步操作。

https://github.com/vllm-project/vllm/issues/8557
这个issue是关于GitHub上的vLLM项目中一个未指定类型的问题，涉及到支持自定义操作检查的功能添加。原因是由于一些后端仍在使用较旧版本的PyTorch，导致一些Markdown无法正确呈现。

https://github.com/vllm-project/vllm/issues/8556
这是一个Bug报告，主要涉及对象是vllm项目中的Config初始化函数。由于参数错误导致在运行api_server时出现TypeError错误。

https://github.com/vllm-project/vllm/issues/8555
这是一个bug报告，涉及到vllm中使用Mistral模型时文件名硬编码导致调整模型困难的问题。

https://github.com/vllm-project/vllm/issues/8554
这个issue是一个需求提报，主要涉及的对象是FastAPI。由于公司不允许在生产环境中使用某些端点，提出了增加一个新参数选项来禁用FastAPI文档的功能。

https://github.com/vllm-project/vllm/issues/8553
这是一个bug报告，主要涉及到vllm下的一个模型在在线推理中出现问题，可能是由于模型在在线推理时出现了异常。

https://github.com/vllm-project/vllm/issues/8552
这是一个用户提出需求的issue，主要涉及vllm的请求处理机制。由于用户想控制在一个批次中运行请求的数量，需要调整引擎参数来实现。

https://github.com/vllm-project/vllm/issues/8551
这个issue是一个CI/Build类型的问题，主要涉及vLLM的Entrypoints测试。由于ROCm环境下部分测试失败，需要重新启用测试并排除失败的测试案例。

https://github.com/vllm-project/vllm/issues/8550
这个issue类型是文档改进，主要涉及安装文档的改进。原因是增加了从另一个issue中获得的经验教训。

https://github.com/vllm-project/vllm/issues/8549
这是一个关于提交PR的类型issue，主要涉及对vLLM项目中的Agent Splitting功能进行启用。造成该问题的原因可能是markdown渲染问题导致描述无法展示。

https://github.com/vllm-project/vllm/issues/8548
这个issue是关于bugfix的，主要的对象是AsyncLLMEngine。恶意请求发送 `SamplingParam(n=1.0)` 可能导致引擎出现错误状态，因为 n 应该是整数而不是浮点数。

https://github.com/vllm-project/vllm/issues/8547
这个issue属于Feature请求类型，主要涉及的对象是Quantisation Support with CPU Backend功能。由于当前项目不支持在CPU后端上运行量化模型，用户询问是否有计划在未来支持，并寻求相关帮助或解决方案。

https://github.com/vllm-project/vllm/issues/8546
这是一个关于提交PR时缺少描述的Issue，主要涉及缺乏对PR类型的正确标注。原因可能是作者未填写对应的描述内容。

https://github.com/vllm-project/vllm/issues/8545
该issue是一个Bugfix类型的问题，主要涉及Encoder-Decoder模型中由于beam search导致的cross attention计算错误。

https://github.com/vllm-project/vllm/issues/8544
这个issue是一个Bugfix类型的报告，涉及到`GraniteForCausalLM`模型，由于logits processor可能返回None，导致在granite代码中进行的后续scaling失败，因此需要修复。

https://github.com/vllm-project/vllm/issues/8543
这个issue是一个关于安全优化的bug报告，涉及的主要对象是在`tensorparallelsize`参数大于`1`时通过ZMQ进行广播消息时的安全性问题。

https://github.com/vllm-project/vllm/issues/8542
这是一个bug报告。该问题涉及到使用vllm中的deepseekCoderv2Instruct模型输出错误的问题，出现了奇怪的中文字符。造成这种现象的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/8541
这个issue属于文档更新类型，主要涉及到调试文档中关于`gpu_memory_utilization`和CUDA OOM问题的解释补充，可能是由于当前文档不够详细或清晰而导致用户在理解和解决相关问题时遇到困难。

https://github.com/vllm-project/vllm/issues/8540
这是一个bug报告，涉及到Dockerfile.cpu在使用podman构建时出现的问题。由于未设置`WORKDIR`，导致相对路径的`RUN mount`命令无法正常执行。

https://github.com/vllm-project/vllm/issues/8539
这是一个bug报告，涉及问题是运行vllm时出现CUDA错误导致的崩溃。

https://github.com/vllm-project/vllm/issues/8538
这个issue类型是bug报告，该问题单涉及的主要对象是在AMD MI300X上使用FP8量化导致vLLM崩溃。这个bug是由于在指定quantization为fp8时，vLLM在加载到GPU内存后崩溃，而不指定quantization时一切正常。

https://github.com/vllm-project/vllm/issues/8537
这是一个bug报告类型的issue，涉及的主要对象是OpenAI API服务器启动时的错误。原因是在`disablefrontendmultiprocessing`启用时，未绑定API服务器端口导致http服务器始终出错。

https://github.com/vllm-project/vllm/issues/8536
这是一个Bug报告，主要涉及的对象是在2 or 4-gpu A100设置中加载vllm.LLM模型时可能会导致默认文本编码从utf8更改为ascii，需要手动指定编码。

https://github.com/vllm-project/vllm/issues/8535
这个issue类型是用户提出需求，主要对象是关于vllm的安装过程。由于用户在安装vllm时遇到了问题，需要寻求帮助或解决方案。

https://github.com/vllm-project/vllm/issues/8534
这是一个关于性能优化的issue，主要涉及到CUDA初始化相关的函数调用。由于多次运行测试时出现CUDA重新初始化错误，因此需要减少部分CUDA相关函数的调用来避免这种问题。

https://github.com/vllm-project/vllm/issues/8533
这是一个功能更新的issue，涉及到Varlen prefill和Prefill chunking支持的添加，主要涉及到mamba kernels和Jamba model。

https://github.com/vllm-project/vllm/issues/8532
这是一个关于安装问题的bug报告，主要涉及vllm项目从源代码构建时出现的错误。问题可能是由于环境设置或安装过程中的一些步骤错误导致的。

https://github.com/vllm-project/vllm/issues/8531
这是一个bug报告类型的issue，主要涉及vllm的benchmark_serving.py在运行过程中生成不同数量的tokens，可能是由于代码中的随机性导致了这种结果。

https://github.com/vllm-project/vllm/issues/8530
该issue属于对性能的提案，主要涉及了draft model的选择，由于vLLM需要draft model和target model的词汇量相同，因此用户寻求适当的drafter选择建议。

https://github.com/vllm-project/vllm/issues/8529
这个issue属于bug报告类型，涉及到ppc64le平台的Dockerfile和CI相关问题，由于CI代理磁盘空间问题，以及Dockerfile.ppc64le构建/测试出现的bug。

https://github.com/vllm-project/vllm/issues/8528
这是一个关于对比v1和v2版本的块管理器在不同工作负载下的性能的issue。Block manager v2的性能较慢，特别是在全前缀匹配时更慢，但是在块管理器v2中的哈希操作更快。

https://github.com/vllm-project/vllm/issues/8527
这个issue类型是bug报告，涉及的主要对象是模型运行过程中的kvcache tensors，产生这个bug的原因是在遇到致命错误时，dump出的kvcache tensors过大导致写入到磁盘所需时间过长，进而造成引擎循环被阻塞，健康检查超时，服务器被杀死。

https://github.com/vllm-project/vllm/issues/8526
这个issue属于功能需求提出类型，主要涉及了torch.compile中的allreduce操作。这个问题提出了改进现有代码的方式，以便更好地支持未来类似操作。

https://github.com/vllm-project/vllm/issues/8525
这个issue是关于改进Nullable kv Arg Parsing的bug报告，主要涉及前端CLI引擎参数解析中的错误处理问题。由于argparse会捕获`ValueError`而导致错误消息未能正确传达给用户。

https://github.com/vllm-project/vllm/issues/8524
这个issue属于代码重构类型，主要涉及到移除基于 Triton 的采样器；这个问题由于未使用的代码路径未来不会被计划使用，因此需要去除并回退。

https://github.com/vllm-project/vllm/issues/8523
这是一个关于提出需求的issue，主要涉及vllm的APC introspection接口，用户希望通过添加函数使vllm实例能够处理给定的块，以提高吞吐量。

https://github.com/vllm-project/vllm/issues/8522
这是一个功能需求类型的issue，主要涉及的对象是外部缓存服务。由于缺乏对外部缓存服务的度量衡，用户提出了需要将度量指标添加到外部缓存服务的需求。

https://github.com/vllm-project/vllm/issues/8521
这个issue是一个Bugfix类型的问题，主要涉及到vLLM中的Mistral tokenizer。导致这个问题的原因是在Pixtral + guided_json组合使用时会导致Internal Server Error。

https://github.com/vllm-project/vllm/issues/8520
这个issue属于CI/Build类型，主要涉及的对象是将kernels/test_gguf.py从ROCm中排除，可能由于CI/Build过程中的特定需求或者配置造成了这个问题。

https://github.com/vllm-project/vllm/issues/8519
这是一个Bug报告，主要涉及了在导入vllm后randomization无法正常工作的问题。可能的原因是加载了llm后导致随机性失效，导致每次运行脚本时产生相同的结果。

https://github.com/vllm-project/vllm/issues/8518
这是一个贡献代码的问题单，主要涉及的对象是DbrxExperts类，由于需要使用FusedMoe作为基类并自定义权重加载器，以暴露FusedMoe类的现有功能。

https://github.com/vllm-project/vllm/issues/8517
这个issue类型是关于改进发布流程的请求，主要涉及到版本控制和发布过程。

https://github.com/vllm-project/vllm/issues/8516
这是一个bug报告，主要对象是测试框架，由于等待镜像时测试失败，添加了超时设置以调试挂起的问题。

https://github.com/vllm-project/vllm/issues/8515
这是一个bug报告，涉及的主要对象是mistral模型。导致该bug的原因是Mistral Large Instruct 2407工具调用泄漏。

https://github.com/vllm-project/vllm/issues/8514
这是一个bug报告类型的issue，涉及主要对象为文档更新，由于文档需要更新以解决特定问题。

https://github.com/vllm-project/vllm/issues/8513
该issue类型为用户提出需求，主要涉及的对象是针对大规模批量推理的优化参数。原因是用户希望找到最佳的参数设置，以提高405B FP8输出的生成效率。

https://github.com/vllm-project/vllm/issues/8512
这个issue类型为用户提出需求，主要涉及的对象是向vLLM文档添加互斥特性的兼容性矩阵，用户希望为用户提供快速查阅以便规划实施或学习。

https://github.com/vllm-project/vllm/issues/8510
这是一个Bug报告，涉及到triton在运行时导入了setuptools但没有明确指定setuptools是一个运行时要求，导致当安装的setuptools版本较旧并且期望存在`distutils`包时出现问题，而该包在Python 3.12中已移除。

https://github.com/vllm-project/vllm/issues/8509
这个issue类型是修复bug，主要涉及的对象是ray库的版本限制，因为ray 2.36引入了不兼容修改，需要限制版本以避免CI失败。

https://github.com/vllm-project/vllm/issues/8508
这个issue属于bug报告类型，主要涉及vLLM中StarCoder模型的低速服务问题，用户提出了为什么StarCoder模型速度很慢的疑问。

https://github.com/vllm-project/vllm/issues/8507
This is a feature request regarding the support for DynamicCache mechanism in vllm, in order to improve efficiency in multiturn interactions through reusing intermediate computations from previous iterations.

https://github.com/vllm-project/vllm/issues/8506
这是一个bug报告类型的issue，主要涉及的对象是GGUF kernel的构建过程。由于未在代码中添加`__CUDA_ARCH__ >= 610`导致了对`sm_60`的构建失败。

https://github.com/vllm-project/vllm/issues/8505
这是一个Bug报告类型的issue，涉及的主要对象是Dockerfile.cpu。导致此问题的原因是Dockerfile.cpu 中使用了`mount`而不使用`COPY`来处理`requirementsx.txt`文件，导致容器镜像构建失败。

https://github.com/vllm-project/vllm/issues/8504
这是一个关于bug报告的issue，主要涉及的对象是Vllm库的Vram使用情况。由于参数设置和模型加载可能导致Vram使用量异常增加的问题。

https://github.com/vllm-project/vllm/issues/8503
这个issue属于Bug报告，主要涉及修复Grok FP16运行的问题。造成这个问题的原因可能是Markdown渲染错误导致无法正确显示内容。

https://github.com/vllm-project/vllm/issues/8502
这是一个bug报告类型的issue，主要涉及的对象是vllm的安装过程。问题是由于Dockerfile.cpu中未提供绝对路径而导致的错误。

https://github.com/vllm-project/vllm/issues/8501
这是一个关于代码质量和代码审查流程的issue，主要涉及的对象是vLLM前端（Frontend）的OpenAI服务器。这个issue问题是由于未能通过`revision`参数运行推断导致的错误。

https://github.com/vllm-project/vllm/issues/8500
这个issue属于bug报告，主要涉及vllm-cpu docker容器运行过程中出现的AttributeError错误。原因可能是代码中缺少了相应的属性或方法导致的错误。

https://github.com/vllm-project/vllm/issues/8499
这是一个用户提出需求的issue，主要涉及vLLM中的TPU支持问题，由于logprobs在TPU上不受支持，导致无法运行`lmevalharness`对某些评估进行处理。

https://github.com/vllm-project/vllm/issues/8498
这个issue是一个[Core]类型的bug报告，涉及到对vLLM代码质量标准的要求。由于markdown渲染问题，导致文本无法正确显示。

https://github.com/vllm-project/vllm/issues/8497
这是一个关于提出新增功能需求的类型的issue，主要涉及到LoRa服务实现，由于用户对Microsoft提出的有关在生产中提供LoRa服务的论文感兴趣。

https://github.com/vllm-project/vllm/issues/8496
这是一个Bug报告，主要涉及"LlavaNext"的 feature size 计算错误导致的问题。

https://github.com/vllm-project/vllm/issues/8495
这是一个缺少PR描述的问题，主要涉及到对benchmark_serving.py的更新，支持远程HF数据集中的样本和图像输入。

https://github.com/vllm-project/vllm/issues/8494
这是一个Bug报告，主要涉及VLLM项目中的DeepSeek-V2模型出现CUDA图形错误问题。原因可能是与模型输入相关的错误导致了此问题。

https://github.com/vllm-project/vllm/issues/8493
这是一个bug报告类型的issue，主要涉及vllm的安装过程。由于缺少Triton和无法导入vllm._C模块，可能导致wsl崩溃退出。

https://github.com/vllm-project/vllm/issues/8492
这是一个BugFix类型的issue，主要涉及的对象是vllm项目中的shutdown/cleanup功能。由于存在循环引用导致垃圾回收器未及时清理，engine析构函数未被调用，从而导致无法可靠地进行清理操作。

https://github.com/vllm-project/vllm/issues/8491
这是一个bug报告，涉及到vLLM中的端口绑定问题，由于未提前绑定端口导致出现"[Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use"错误。

https://github.com/vllm-project/vllm/issues/8490
这是一个bug报告，涉及的主要对象是Pixtral模型中的`position_meshgrid`函数。这个问题是由于Python 3.8不支持标准集合中的通用类型，导致了类型提示错误。

https://github.com/vllm-project/vllm/issues/8489
这是一个用户提出的需求类型的issue，主要涉及对象是TPU（Tensor Processing Unit）。

https://github.com/vllm-project/vllm/issues/8488
这是一个需求提出类的issue，主要涉及到torch.compile中添加一个flag来禁用自定义操作。由于当前torch.compile会消耗更多内存，用户希望测试一个新的flag来控制其行为。

https://github.com/vllm-project/vllm/issues/8487
这是一个关于使用vllm在Arrow Flight RPC中进行推理的问题，用户正在探求如何集成vLLM以利用连续批处理，并反映当前不利用连续批处理所导致的问题。

https://github.com/vllm-project/vllm/issues/8486
这个issue类型是需求提出，主要对象是VLM模型的添加和支持。由于LLaVAOneVision模型在Huggingface中支持更多configs，但Huggingface在发布版本中使用默认值，导致出现了配置相关的问题。

https://github.com/vllm-project/vllm/issues/8485
该issue属于安装问题，主要对象是vLLM在Jetson上的安装，用户提出了相关安装环境和安装方式，可能由于安装过程中遇到了问题或不清楚如何进行安装。

https://github.com/vllm-project/vllm/issues/8484
这是一个bug报告，涉及的主要对象是模型Qwen2VLForConditionalGeneration。由于PyTorch版本为2.4.0，但模型不支持LoRA，且LoRA已被启用，导致出现了该bug。

https://github.com/vllm-project/vllm/issues/8483
这个issue类型是用户提出需求，主要对象是vLLM，用户想要了解vLLM是否支持多模态LLM的嵌入API，可能出现的原因是用户需要在vLLM中获取minicpmv 2.6的嵌入。

https://github.com/vllm-project/vllm/issues/8482
这是一个Bug报告类型的issue，涉及主要对象是vllm 0.6.1版本。由于PyTorch版本与CUDA版本不匹配，导致在Triton后端启动时出现崩溃。

https://github.com/vllm-project/vllm/issues/8481
这是一个用户提出需求类型的issue，主要涉及`llm.chat()` API在批量推理方面的功能不足。可能由于目前API仅支持一对话进行推理，导致用户无法充分利用vLLM进行高效的离线处理。

https://github.com/vllm-project/vllm/issues/8480
这是一个bug报告，涉及torch.compile功能的问题，由于功能化传递的失误导致GPU块的丢失和内存消耗的异常情况。

https://github.com/vllm-project/vllm/issues/8479
这是一个关于Kernel模块中Fullgraph和opcheck测试的issue，涉及主要对象为添加新的opcheck测试和模型，并发现了一些问题需要修复。

https://github.com/vllm-project/vllm/issues/8478
这个issue是关于添加"awq fused moe method"的功能请求，不涉及具体bug报告。

https://github.com/vllm-project/vllm/issues/8477
这是一个Bug报告类型的issue，主要涉及到vllm模型在CPU环境中使用multistep scheduling时导致的bug，用户提出需要代码中增加异常或警告以通知用户该功能不受支持。

https://github.com/vllm-project/vllm/issues/8476
该issue类型是bug报告，涉及主要对象为MLPSpeculator，由于ParallelLMHead被替换为ReplicatedLinear，导致无法正确使用现有逻辑进行量化。

https://github.com/vllm-project/vllm/issues/8475
这是一个Bug报告，该问题涉及vLLM在CPU环境下无法支持prompt adapter的问题。原因可能是CPU环境不支持该功能导致用户无法正常使用。

https://github.com/vllm-project/vllm/issues/8474
这是一个bug报告，涉及的主要对象是ROCmFlashAttentionMetadata。由于开发者没有AMD GPUs进行本地测试，所以提出了修改建议。

https://github.com/vllm-project/vllm/issues/8473
该issue类型为版本更新请求，涉及的主要对象是软件版本。由于需要修正或更新特定版本中存在的问题或添加新功能，因此请求将软件升级至v0.6.1.post2。

https://github.com/vllm-project/vllm/issues/8472
这是一个关于bug报告的issue，涉及的主要对象是在使用multistep功能时出现的AMD GPU崩溃问题，可能是由于ROCmFlashAttentionMetadata类未实现advance_step方法导致。

https://github.com/vllm-project/vllm/issues/8471
这是一个用户提出需求的issue，主要对象是获取首个token的延迟数据。由于benchmark_latency.py只提供了处理单个请求批次的延迟数据，用户希望能够获得首个token的延迟数据。

https://github.com/vllm-project/vllm/issues/8469
这是一个CI/Build类型的issue，涉及的主要对象是更新Ruff版本。由于Ruff版本被更新到0.6.5，可能可以运行`ruff check`，所以提交了这个issue。

https://github.com/vllm-project/vllm/issues/8468
这是一个Bug报告，涉及的主要对象是vllm项目中的final output truncation问题。

https://github.com/vllm-project/vllm/issues/8467
这是一个关于文档修复的Issue，涉及到补充oneDNN在cpu backend安装文档中的缺失内容。

https://github.com/vllm-project/vllm/issues/8466
这是一个性能改进的提议，该问题主要涉及的对象是vllm库中的_fwd_kernel()函数。由于移动v变量的初始化，导致性能出现一致的提升。

https://github.com/vllm-project/vllm/issues/8465
这个issue属于用户提出需求类型，主要对象是vllm，用户想在离线服务环境中收集性能指标，可能是由于用户需要评估vllm在离线服务场景下的性能表现。

https://github.com/vllm-project/vllm/issues/8464
这是一个[CI/Build]类型的issue，涉及到vLLM项目中Python 3.8版本的支持问题。原因是Python 3.8即将到期，开发者希望移除针对3.8版本的特定代码分支。

https://github.com/vllm-project/vllm/issues/8463
这是一个用户提出需求的issue，主要涉及要在int4量化下运行phi-3.5视觉指导模型，但目前不清楚如何支持quantization。

https://github.com/vllm-project/vllm/issues/8462
这个issue类型是一个开发需求，主要涉及vLLM的核心逻辑修改，旨在支持仅编码模型（如xlm-roberta、bge-m3...），用户提出了这个改进来增强vLLM的功能性。

https://github.com/vllm-project/vllm/issues/8461
这是一个Bug报告，涉及的主要对象是模型架构'Qwen2AudioForConditionalGeneration'，导致该问题的原因是该模型架构目前不受支持。

https://github.com/vllm-project/vllm/issues/8460
这是一个bug报告，涉及到vLLM在x86 CPU上从v0.6.1版本开始无法正常工作的问题。这可能是由于安装问题或者软件版本兼容性导致的。

https://github.com/vllm-project/vllm/issues/8459
该issue类型为用户提出需求，主要涉及AutoModelForSequenceClassification的正确使用，用户寻求关于如何与vllm集成以运行推理的帮助。

https://github.com/vllm-project/vllm/issues/8458
这是一个bug报告，该问题涉及如何在没有互联网连接的情况下运行VLLM服务。问题出现的原因是无法在没有互联网连接的情况下运行。

https://github.com/vllm-project/vllm/issues/8457
这是一个Performance类型的issue，主要涉及到自定义allreduce中增加弱内存屏障。根据描述，问题是由于使用内存屏障导致了1~3us的延迟。

https://github.com/vllm-project/vllm/issues/8456
这个issue是关于bug报告，涉及主要对象为vLLM下的OpenAPI，由于FastAPI 0.113.0升级导致Python 3.8用户出现TypeError，提出需要使用较旧版本FastAPI来解决这个问题。

https://github.com/vllm-project/vllm/issues/8455
这是关于性能问题的issue，主要涉及的对象是vllm和sglang在内存消耗方面的差异。根据用户描述，问题可能是由于MHA和MLA之间的差异导致的内存消耗问题。

https://github.com/vllm-project/vllm/issues/8454
这个issue是关于用户提出需求的，主要涉及如何在vllm中部署lora模型和如何实现lora模块的可插拔性。用户想要通过在传递标志的方式控制是否使用lora模块。

https://github.com/vllm-project/vllm/issues/8453
这是一个用户提出需求的issue，主要涉及vllm对于支持encode only models的讨论。原因是vllm在支持更多模型和功能时出现了复杂性增加，需要不同的模块和处理器，导致某些新特性牺牲以实现兼容性，最终导致结果不佳。

https://github.com/vllm-project/vllm/issues/8452
这个issue属于需求提出类型，主要涉及的对象是VLLM中的支持编码模型。由于新模型需要增加对编码模型的支持，需要调整注意力机制实现以及支持新模型的特性。

https://github.com/vllm-project/vllm/issues/8451
这是一个关于在GitHub上的vLLM项目中新增部署使用 Kubernetes 指南的文档问题，不是一个bug报告。

https://github.com/vllm-project/vllm/issues/8450
这是一个Bug报告，涉及vllm中的/chat/completions接口，由于无法评估类型注释导致TypeError。

https://github.com/vllm-project/vllm/issues/8449
这是一个bug报告，主要涉及的对象是vllm项目的quant test。原因是初始化cuda导致了tp tests失败。

https://github.com/vllm-project/vllm/issues/8448
这是一个关于构建Docker镜像过程中遇到问题的Bug报告，主要涉及Dockerfile的构建过程。由于使用Dockerfile构建镜像时遇到了Killed的问题，即使有32GB的RAM，用户反馈无法成功构建镜像。

https://github.com/vllm-project/vllm/issues/8447
这是一个Bug报告，涉及VLLM项目中的节点服务挂起的问题。原因可能是与NCCL库的使用相关。

https://github.com/vllm-project/vllm/issues/8446
这是一个文档相关的问题，主要涉及到软件安装工具的选择，由于某些原因导致用户被建议使用pip而非conda，这可能是由于某些功能在使用pip时更加稳定或兼容。

https://github.com/vllm-project/vllm/issues/8445
该issue属于用户提出需求类型，主要对象是针对torch.compile插件，用户希望添加自定义编译后端。

https://github.com/vllm-project/vllm/issues/8444
这是一个bug报告，主要涉及GPU机器在重启后无法再次加载模型的问题，可能是由于资源释放不完全导致的。

https://github.com/vllm-project/vllm/issues/8443
这是一个bug报告，涉及修复了无法下载模型的问题，原因是在使用`VLLM_USE_MODELSCOPE=true`时，`ModelScope`会patch `file_exists`和`hf_hub_download`方法来下载模型。

https://github.com/vllm-project/vllm/issues/8442
这个issue是一个Bug报告，主要涉及的对象是Qwen2-VL GPTQ-Int8模型。由于加载额外的偏差导致Qwen2VL GPTQ无法正常工作。

https://github.com/vllm-project/vllm/issues/8441
这是一个用户提出需求的issue，主要涉及的对象是vLLM的在线推理批处理功能。用户希望了解如何在指定的批大小情况下并行处理vLLM的请求，因为当前无法确定vLLM内部处理提示的批大小。

https://github.com/vllm-project/vllm/issues/8440
这个issue类型是版本更新请求，涉及主要对象是项目中的软件版本。

https://github.com/vllm-project/vllm/issues/8439
这个issue属于bug报告类型，主要涉及VLLM中猜测解码的速度比正常解码慢的问题。

https://github.com/vllm-project/vllm/issues/8438
这个issue类型是需求提出，主要涉及对象是vLLM在Kubernetes (K8s)环境中的部署文档缺失。用户提出由于当前文档只提供Docker的部署指导，而不包括K8s的部署信息，导致K8s用户无法顺利部署vLLM。

https://github.com/vllm-project/vllm/issues/8437
这是一个CI/Build类型的issue，主要涉及的对象是在vLLM上启用InternVL2 PP测试仅在单节点上进行。原因是要保证代码质量并提高审查流程的效率。

https://github.com/vllm-project/vllm/issues/8436
这是一个 bug 报告，主要涉及到 pixtral tests 使用 pickle 文件可能存在的安全风险，该问题是由于 pickle 文件存在导致的。

https://github.com/vllm-project/vllm/issues/8435
这是一个bug报告，该问题涉及FastAPI 0.113.0版本对vLLM OpenAPI的影响，因导致了破坏性bug。

https://github.com/vllm-project/vllm/issues/8434
这是一个功能提议，提出了关于在 bitsandbytes 量化中应用张量并行性的问题。

https://github.com/vllm-project/vllm/issues/8433
这是一个CI/Build类型的issue，主要涉及xformers的引入导致ROCm环境下的测试出现问题。

https://github.com/vllm-project/vllm/issues/8432
这是一个bug报告，涉及主要对象为PyTorch环境，由于CUDA内存错误导致数值无法广播到正确的形状，触发数值错误。

https://github.com/vllm-project/vllm/issues/8431
这个issue是一个文档更新类型的问题，主要涉及的对象是Pixtral示例更新。由于chunked prefill被默认关闭导致旧的命令不再适用，需要更新示例以适应此变化。

https://github.com/vllm-project/vllm/issues/8430
这是一个bug报告，涉及到模型中的group_topk功能，问题是由于返回类型更新不正确导致融合马林内核产生无意义的输出。

https://github.com/vllm-project/vllm/issues/8429
这是一个bug报告类型的issue，涉及主要对象为Pixtral + guided_json功能。由于引发Internal Server Error的原因，用户在使用这一功能时始终失败。

https://github.com/vllm-project/vllm/issues/8428
这是一个CI/Build类型的issue，主要涉及到的对象是InternVL2。这个issue的产生是由于多节点测试功能在主分支上出现故障，导致阻塞了很多PR。

https://github.com/vllm-project/vllm/issues/8427
这是一个修复Bug的issue，主要涉及的对象是multistep cuda graph block table。这个issue的原因是可能之前的代码在flashinfer后端中存在兼容性问题，导致了某些bug或不能正常运行的情况。

https://github.com/vllm-project/vllm/issues/8426
这是一个包含bug修复内容的issue，涉及到VLLM组件的一系列问题和修复。

https://github.com/vllm-project/vllm/issues/8425
这个issue是关于bug修复。主要涉及的对象是vLLM中的多模态模型。这个bug是由于chunked prefill和prefix caching与多模态模型不兼容，导致出现错误和功能无法正常工作。

https://github.com/vllm-project/vllm/issues/8424
这是一个关于代码质量要求的issue，属于[Kernel]分类。这个问题主要涉及对自定义操作定义和CPU实现进行代码重构，原因是当前的markdown渲染不起作用，需要使用原始的html标记。

https://github.com/vllm-project/vllm/issues/8423
这个issue是关于bug修复的，主要涉及到Hermes2ProToolParser类的类变量重复赋值问题。原因可能是由于代码逻辑错误导致的。

https://github.com/vllm-project/vllm/issues/8422
这是一个bug报告，主要涉及NCCL在实例化LLM类时被卡住，无法使用CTRL + C强制停止，导致不知如何处理。

https://github.com/vllm-project/vllm/issues/8421
该issue是一个bug报告，主要涉及LlavaNexT模型在多GPU环境下出现的token和占位符不匹配的问题。

https://github.com/vllm-project/vllm/issues/8420
这是一个Bug报告，涉及主要对象为PyNcclCommunicator。由于属性错误导致实例化LLM类时出现错误，错误提示为`AttributeError: 'PyNcclCommunicator' object has no attribute 'device'`。

https://github.com/vllm-project/vllm/issues/8419
这是一个关于内存问题的bug报告，主要涉及vLLM在decode阶段生成过多token可能导致内存耗尽的情况。

https://github.com/vllm-project/vllm/issues/8418
这是一个用户询问的问题类型的issue，主要涉及到vllm中关于max-num-seqs, max-num-batched-tokens 和 max-model-len的关系。在该issue中，用户想讨论如何计算每个批处理中处理的最大标记数以及max-num-batched-tokens与max-num-seqs*max-model-len之间的关系。

https://github.com/vllm-project/vllm/issues/8417
这是一个bug报告，涉及到日志统计功能的修复。问题产生的原因是需要跳过已被抢占的序列的日志统计，因为它们的状态已经被重置为预填充状态。

https://github.com/vllm-project/vllm/issues/8416
这个issue是一个Bug报告，主要涉及的对象是vllm中的 Worker 类。这个问题由于使用了错误的关键字参数或删除了必要参数导致了不同的错误提示，用户希望得到关于如何在使用draft model时解决该问题的帮助。

https://github.com/vllm-project/vllm/issues/8415
这是一个bug报告，主要涉及Pixtral在使用chunked prefilling时出现的问题，导致了图像处理错误和功能异常。

https://github.com/vllm-project/vllm/issues/8414
这是一个bug报告，主要涉及的对象是CMake。由于使用CUDA_SUPPORTED_ARCHS更改时，CMake将错误地显示Python版本而不是CUDA架构版本，导致这种不正确的输出。

https://github.com/vllm-project/vllm/issues/8413
这是一个Bug报告，主要涉及到vllm项目中启动后显存占用越来越大导致程序崩溃的问题。

https://github.com/vllm-project/vllm/issues/8411
这是一个bug报告，涉及的主要对象是Pixtral inference在LLMEngine/AsyncEngine中工作不正确。这个问题是由于需要手动填充输入token ids才能使代码正确运行。

https://github.com/vllm-project/vllm/issues/8410
这是一个bug报告，涉及Valuable Debug Info Obtained的Custom All Reduce OP，问题是NCCL hang导致超时。

https://github.com/vllm-project/vllm/issues/8409
这个issue属于bug报告类型，涉及到使用vllm时在运行fp8模型时出现数值错误。

https://github.com/vllm-project/vllm/issues/8408
这是一个Bug报告，主要涉及VLLM（Visual Language Learning Model）的准确性问题。由于当前环境导致VLLM-Qwen2-VL-7B-Instruct模型的准确度较低。

https://github.com/vllm-project/vllm/issues/8407
这是一个用户提出需求的类型，主要涉及的对象是BLIP/BLIP-2模型。由于需要支持加载复合模型以及提取中间状态，导致了需要对BLIP vision encoder进行重构和对BLIP2进行更新的问题。

https://github.com/vllm-project/vllm/issues/8406
这个issue是关于Bug报告，主要涉及到Qwen2-VL GPTQ模型无法正常工作的问题。原因可能是最新版本的vllm0.6.1安装后启动服务时出现了问题。

https://github.com/vllm-project/vllm/issues/8405
这是一个特性需求的issue，主要涉及前端工具调用internlm/internlm2_57bchat模型的支持。原因是为了增加支持工具调用解析器的管理，以及为那些不支持并行工具调用的模型添加并行测试跳过配置。

https://github.com/vllm-project/vllm/issues/8404
这个issue属于技术问题讨论，主要涉及自定义Allreduce中的内存顺序和volatile关键字的使用。由于缺乏内存屏障，可能导致某些步骤的顺序混乱。

https://github.com/vllm-project/vllm/issues/8403
这是一个bug报告，主要涉及到torch模块在多GPU环境下初始化错误的问题，导致了一个GPU实例错误的使用情况。

https://github.com/vllm-project/vllm/issues/8402
这是一个Bug报告，涉及的主要对象是KubeRay的CUDA设备检测问题，由于CUDA_VISIBLE_DEVICES为空导致GPU支持被禁用。

https://github.com/vllm-project/vllm/issues/8401
该issue类型为用户请求帮助，主要涉及如何使用openai兼容的API运行GGUF模型。用户提出了关于如何集成该模型到vllm中以及如何运行特定模型推断的问题。

https://github.com/vllm-project/vllm/issues/8400
这个issue属于bug报告类型，主要涉及Pixtral在模型输入方面的问题，可能由于数据不匹配导致出现了dummy tokens数量不对的错误。

https://github.com/vllm-project/vllm/issues/8399
这个issue是一个Bug报告，涉及到vLLM中最大位置嵌入的配置问题导致生成错误的情况。

https://github.com/vllm-project/vllm/issues/8398
这是一个技术需求提出的issue，主要涉及到torch.compile中递归编译加载模型的解决方案。原因是为了解决直接应用torch.compile到模型时的一些限制。

https://github.com/vllm-project/vllm/issues/8397
这是一个Bug报告，涉及到vllm库中使用tensor_parallel时出现错误的问题，原因是在版本`v0.6.1`中出现bug，回滚到`v0.6.0`即可解决。

https://github.com/vllm-project/vllm/issues/8396
该issue类型为功能增强，主要涉及的对象是MRotaryEmbedding模型中使用_ROPE_DICT提高内存利用率的功能。由于现有模型在不同层之间不能共享RoPE，导致额外的GPU内存占用和初始化时间增加的问题。

https://github.com/vllm-project/vllm/issues/8395
这是一个Bug报告。主要涉及对象是Qwen72B service(TP=4)。GPU和CPU利用率高达100%，伴有多个请求处于CLOSE_WAIT状态，推测可能是由于系统资源过载导致服务无法正常响应。

https://github.com/vllm-project/vllm/issues/8394
该issue类型为用户提出需求，并主要涉及支持语音模型qwenaudio。由于缺乏计划支持此类声音模型，用户提出相应问题。

https://github.com/vllm-project/vllm/issues/8393
这是一个Bug报告，涉及的主要对象是vllm版本0.5.5的AsyncLLMEngine，因为使用`asyncio.gather`时无法返回响应而导致hangs。

https://github.com/vllm-project/vllm/issues/8392
这是一个Bug报告类型的Issue，主要涉及VLLM在升级版本时出现的CUDA初始化错误问题。

https://github.com/vllm-project/vllm/issues/8391
该issue属于[Kernel]类型，涉及的主要对象是factor registrations。由于需要factor out custom op definitions from implementations，并允许CPU实现与CUDA实现并存，可能导致markdown无法渲染工作。

https://github.com/vllm-project/vllm/issues/8390
这是一个bug报告，涉及的主要对象是vllm下的AMD weight loading tests，由于CUDA在forked subprocess中重新初始化时出现错误，导致了无法使用CUDA与multiprocessing结合的问题。

https://github.com/vllm-project/vllm/issues/8389
这个issue类型是需求提出；主要涉及的对象是使用TPU的分布式后端；用户提出了使用Ray作为默认分布式后端的需求。

https://github.com/vllm-project/vllm/issues/8388
这是一个Bug报告类型的issue，涉及的主要对象是加载qwen2-vl模型出现的错误。这个问题是由于HuggingFace将rope_type键设置为"default"而不是"mrope"引起的。

https://github.com/vllm-project/vllm/issues/8387
这是一个bug报告，涉及vllm下的Rocm gemm tuning无法正常工作的问题。导致该问题的原因可能是代码配置或参数设置不正确。

https://github.com/vllm-project/vllm/issues/8386
该issue是一个[Model]类型的问题，主要涉及添加一个名为Solar Model的新模型。这个问题的原因是希望上传了一个名为Solar 22B model的模型，但在markdown渲染时遇到问题，因此使用了原始的html。

https://github.com/vllm-project/vllm/issues/8385
这个issue是一个用户提出需求的类型，主要涉及的对象是增加一个多模态功能用于评估离线延迟、吞吐量和在线服务对Pixtral多模态模型的表现评估。

https://github.com/vllm-project/vllm/issues/8384
这是一个bug报告，主要涉及到关于inductor的问题，由于在inductor看到视图被突变时会复制张量，因此隐藏分片操作在自定义操作下解决了这个问题。

https://github.com/vllm-project/vllm/issues/8383
这是一个建议改善性能的问题，涉及JSONLogitsProcessor，在每个请求中重复实例化对象并重新编译正则表达式，可能导致性能下降。

https://github.com/vllm-project/vllm/issues/8382
这个issue属于Bug报告类型，主要涉及程序中的某个命令无法正确执行，可能是由于参数设置错误导致的。

https://github.com/vllm-project/vllm/issues/8381
这是一个bug报告，涉及对象是VLLM项目下的支持commandr模型的功能，由于硬件不支持torch编译，导致了bug。

https://github.com/vllm-project/vllm/issues/8380
这个issue类型是bug报告，涉及的主要对象是Quantization methods on ROCm。由于scaled_mm需要的参数scale_a和scale_b在2.5rc1版本中成为非可选参数，所以需要修复这个问题。

https://github.com/vllm-project/vllm/issues/8379
这是一个版本更新类型的Issue，涉及的主要对象是软件版本。

https://github.com/vllm-project/vllm/issues/8378
这是一个功能需求的issue，主要涉及MultiStep和Chunked Prefill的支持。

https://github.com/vllm-project/vllm/issues/8377
这个issue是一个PR描述的未填写bug。

https://github.com/vllm-project/vllm/issues/8376
这是一个bug报告，涉及到vllm项目中的一个文件的bug修复。原因是在offline模式下出现了失败的情况。

https://github.com/vllm-project/vllm/issues/8375
这是一个bug报告的issue，主要涉及InternVL2在使用多个图像进行推理时出现的RuntimeError问题。

https://github.com/vllm-project/vllm/issues/8374
这是一个bug报告，涉及的主要对象是在AWS Sagemaker上运行的vLLM。由于在ml.p5.48xlarge节点上部署vLLM 0.6.0时出现CUDA OOM错误，导致OOM错误的原因需要进一步调查。

https://github.com/vllm-project/vllm/issues/8373
这个issue是关于CI/Build的问题，主要涉及排除test_moe.py文件进行AMD Kernels测试以便调查。造成这个问题的原因可能是因为在提交PR之前未按照相应规范填写信息表格。

https://github.com/vllm-project/vllm/issues/8372
这是一个Bug报告，主要涉及LLM（Large Language Model）下载时出现OOM（Out Of Memory）错误，用户尝试在强大的环境下下载模型仍然失败。

https://github.com/vllm-project/vllm/issues/8371
这是一个bug报告，主要涉及的对象是无法加载gemma 2模型，导致GPU内存不足的问题。

https://github.com/vllm-project/vllm/issues/8370
这是一个关于性能问题的bug报告，主要涉及到模型解码过程中评分时间过长的问题。

https://github.com/vllm-project/vllm/issues/8369
这是一个bug报告，涉及vllm项目中的"Crash after few multi image calls"问题，由于设置了特定参数后，进行多图像推断时会引发服务器崩溃。

https://github.com/vllm-project/vllm/issues/8368
这是一个bug报告类型的issue，主要涉及vllm在使用LoRA模型后对非LoRA请求执行速度变慢的情况。

https://github.com/vllm-project/vllm/issues/8367
这是一个Bug报告类型的issue，主要涉及的对象是vLLM 0.6.0和PyTorch 2.4.0版本。由于在加载量化模型时出现CUDA错误，用户请求解决此问题。

https://github.com/vllm-project/vllm/issues/8366
这是一个bug报告，涉及到工具解析是否启用的逻辑问题，导致在不同条件下出现不一致现象。

https://github.com/vllm-project/vllm/issues/8365
这个issue是关于硬件的，类型为[Hardware][intel GPU] bump up ipex version to 2.3，主要涉及的对象是更新ipexxpu版本到2.3，并修复了一些问题，包括支持Arc图形梯度、支持GQA模型像llama38B、支持一些新内核像gelu_quick、更新oneAPI版本到2024.2.1。

https://github.com/vllm-project/vllm/issues/8364
这是一个Bug报告，主要涉及的对象是MistralTokenizer对象，由于缺少了get_vocab属性导致了这个Bug。

https://github.com/vllm-project/vllm/issues/8362
这是一个bug报告，用户提出了关于vLLM v0.6.0版本缺少CUDA 12 wheel文件的问题。

https://github.com/vllm-project/vllm/issues/8361
这是一个bug报告，涉及的主要对象是internvl2模型，可能是由于传入的prompt文本格式不正确导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/8360
这是一个功能需求的issue，主要涉及到生成函数中条件性包含提示的功能实现。该需求的动机是为了提高在流式场景下的效率。

https://github.com/vllm-project/vllm/issues/8359
这是一个功能提议的issue，主要涉及的对象是在`vllm/entrypoints/api_server.py`中的`generate`函数。原因是当前实现中，无论是否启用了流式传输，`generate`函数始终在其响应中包含提示，这在流式传输模式下会导致效率低下。

https://github.com/vllm-project/vllm/issues/8358
这是一个bug报告，主要涉及MistralTokenizer对象缺少'get_vocab'属性导致的错误。

https://github.com/vllm-project/vllm/issues/8357
这个issue是一个Bugfix类型的issue，主要涉及的对象是GGUF内核。由于缺少`IQ1_M`量化实现，导致产生了段错误的bug。

https://github.com/vllm-project/vllm/issues/8356
这个issue类型为用户提出需求，主要对象是vllm服务器的关闭操作。作者想知道如何使用命令关闭vllm服务器，表现为缺乏关于关闭vllm服务器的指令的使用说明。

https://github.com/vllm-project/vllm/issues/8355
这个issue是一个bug报告，涉及的主要对象是在使用cpu backend时出现的编码器/解码器模型缺失代码检查问题。

https://github.com/vllm-project/vllm/issues/8354
这是一个bug报告，主要涉及的对象是torch profiler中的GPUExecutor，由于API不匹配导致了bug。

https://github.com/vllm-project/vllm/issues/8353
这个issue是关于安全性的需求提出，要求在CI工作流中添加Bandit安全检查。

https://github.com/vllm-project/vllm/issues/8352
这是一个Bug报告，主要涉及的对象是在Kaggle TPU VM v3-8环境下使用vllm库时出现的Kernel死机问题。这个问题可能是由于使用了`libopenblasdev`而非`libopenblasbase`导致的。

https://github.com/vllm-project/vllm/issues/8351
这是一个Bug报告，主要涉及的对象是vllm v0.6.0 profiler中的GPUExecutorAsync对象。这个问题可能是由于ROCm环境下出现了属性'_run_workers'缺失导致的错误。

https://github.com/vllm-project/vllm/issues/8350
这是一个Bug报告，涉及的主要对象是VLLM下的guided generation功能。这个问题产生的原因是由于guided generation在模型没有足够的可选标记时无法完成所请求的结构生成，导致生成的json结构不完整。

https://github.com/vllm-project/vllm/issues/8349
这个issue类型是性能问题，主要涉及模型在V0.6.0版本性能表现不佳，导致成功响应量仅占请求量的一半。

https://github.com/vllm-project/vllm/issues/8348
这是一个功能增强的issue，主要涉及VLM核心模块，修改支持多模态模型的前缀缓存。

https://github.com/vllm-project/vllm/issues/8347
这个issue是一个bug报告，主要涉及的对象是VLLM项目的前端部分。导致这个bug的原因是run_batch API在处理请求失败时没有创建适当的error responses，包括由于无效的URL端点或不支持的情况而导致的请求失败。

https://github.com/vllm-project/vllm/issues/8346
该issue是一个需求报告，涉及到VLM核心模块，主要是为了增加精确的多模态占位符追踪功能。导致此需求的原因是当前多模态提示占位符仅与多模态嵌入索引相关联，需要引入一种精确跟踪多模态占位符范围的机制。

https://github.com/vllm-project/vllm/issues/8345
这个issue为[Kernel]类型，涉及主要对象为phi-3-small-8k/128k model triton kernel，为对blocksparse attention prefill Triton kernel进行修改以添加prefix-caching support的功能增强。

https://github.com/vllm-project/vllm/issues/8344
这个issue属于bug报告，主要涉及到恢复缺失的CI文件，可能是由于文件丢失或被删除导致的。

https://github.com/vllm-project/vllm/issues/8343
这个issue类型是功能需求，主要涉及的对象是为Llama 3.1和3.2工具添加支持。原因是Llama模型与其他模型在工具调用方面有一些显著差异，因此需要对现有单元测试进行调整以满足Llama的需求。

https://github.com/vllm-project/vllm/issues/8342
这是一个关于性能优化的issue，涉及到chunked prefill和prefix caching的设置。

https://github.com/vllm-project/vllm/issues/8340
这个issue是一个bug报告，主要涉及了multistep lookahead block allocation和cuda graph max capture之间的兼容性问题，导致可能会分配比max capture使用的更多的blocks。

https://github.com/vllm-project/vllm/issues/8339
这个issue类型为需求提出，涉及主要对象为模型工具调用解析支持。这个问题是由于需要增加对`ibm-granite/granite-20b-functioncalling`的工具调用解析支持，同时添加一个基于granite fc论文的示例聊天模板而被提出。

https://github.com/vllm-project/vllm/issues/8338
这个issue是关于为Gemma2添加bitsandbytes支持的问题。

https://github.com/vllm-project/vllm/issues/8337
该issue类型为功能需求反馈，主要对象是CUDA Time Layerwise Profiler。由于用户需要通过分析CUDA（GPU核心）每个模块/层所花费的时间来了解性能表现，因此提出了开发类似Layerwise profiler来实现这一功能的需求。

https://github.com/vllm-project/vllm/issues/8336
这是一个用户提出需求的issue，主要对象是改进vLLM在ROCm设备上的安装体验。由于ROCm Docker镜像体积庞大和从源代码构建vLLM耗时较长，导致安装时间较长，用户提出需要官方预构建二进制文件以加速安装过程。

https://github.com/vllm-project/vllm/issues/8335
这是一个特性需求的issue，主要涉及到输出流支持的添加，针对于 multistep + async 的实现。 由于需求对性能有一定要求，缓存输出对象以降低Python对象分配，同时通过最近的delta outputs改进来维持好的性能。

https://github.com/vllm-project/vllm/issues/8334
该issue是一个代码质量问题，涉及主要对象是vLLM项目中的代码逻辑（Core），原因是缺少支持pinned caching with prefix caching的功能。

https://github.com/vllm-project/vllm/issues/8333
这是一个关于需求提出的issue，主要涉及到vLLM中提供固定缓存时效功能的问题。最后，用户提出了需要改进现有缓存管理逻辑来支持不同的缓存策略的需求。

https://github.com/vllm-project/vllm/issues/8332
这是一个用户需求报告，主要对象是关于如何停止VLLM生成过程的问题。用户想要通过`curl`触发停止当前生成的需求。

https://github.com/vllm-project/vllm/issues/8331
这是一个bug报告，用户遇到了使用vllm进行离线推断时的权限验证错误问题。

https://github.com/vllm-project/vllm/issues/8330
这个issue是一个bug报告，主要涉及对象是vLLM v0.6.0 (CPU)服务器。这个问题由于设置VLLM_CPU_OMP_THREADS_BIND导致vLLM v0.6.0 (CPU)服务器启动失败，可能是由于配置错误或程序逻辑问题导致的。

https://github.com/vllm-project/vllm/issues/8329
这是一个针对代码修改建议的issue，主要涉及Qwen2-MOE GPTQ模型的加载优化。

https://github.com/vllm-project/vllm/issues/8328
这是一个测试PR，属于其它类型，主要对象是针对代码质量的测试。

https://github.com/vllm-project/vllm/issues/8327
这个issue是一个需求提出，涉及对象是CI/Build中的HIP builds。缺乏对`ccache`和`sscache`的使用导致了HIP builds的性能不佳。

https://github.com/vllm-project/vllm/issues/8326
这是一个Bug报告类型的Issue，涉及主要对象是vLLM v0.6.0的Benchmark Profiling功能。由于运行Benchmark时遇到了TimeoutError导致OpenAI服务器未能在5000ms内做出响应。

https://github.com/vllm-project/vllm/issues/8325
这是一个Bug报告类型的issue，主要涉及的对象是vllm项目的Speculative Decodengram算法。由于设置了use_message_queue_broadcaster=True而触发了一个错误，将其设置为False可以解决问题。

https://github.com/vllm-project/vllm/issues/8324
这是一个bug报告，涉及主要对象为Buildkite pipeline generator。该问题由于部分测试命令中的转义字符问题，导致与特定PR的兼容性出现了问题。

https://github.com/vllm-project/vllm/issues/8323
这是用户提出需求的类型，主要涉及vLLM模块的使用问题，用户想了解是否可以直接将input_embeds传递给generate函数。

https://github.com/vllm-project/vllm/issues/8322
这个issue类型是其它类型，主要对象是设备选项，由于设备选项在不同部分重复，故用户提出优化移动设备选项的问题。

https://github.com/vllm-project/vllm/issues/8321
这是一个关于代码中出现错误提示的bug报告，主要对象涉及到vllm库和transformers库。由于代码中存在错误或配置问题导致程序出现了段错误并转储核心。

https://github.com/vllm-project/vllm/issues/8320
这是一个bug报告类型的issue，主要涉及修复验证令牌中的正确奖励令牌。原因是令牌匹配不正确导致性能下降，请求审查修复。

https://github.com/vllm-project/vllm/issues/8319
该问题类型为需求提出，主要涉及添加NVIDIA Meetup幻灯片、公布AMD Meetup、添加联系信息等内容。由于缺少特定的幻灯片、会议信息和联系方式，导致用户需要补充这些内容以完善会议相关信息。

https://github.com/vllm-project/vllm/issues/8318
这是一个bug报告，涉及vLLM在TPU上使用较大context size时崩溃的问题。原因是在较高的context size下，vLLM会崩溃。

https://github.com/vllm-project/vllm/issues/8317
这是一个需求类型的issue，涉及到Speculative Decoding测试的重构。

https://github.com/vllm-project/vllm/issues/8316
这是一个bug报告，主要涉及图片预处理在VLLM(Qwen2-vl)推断过程中执行两次的问题，导致性能下降。

https://github.com/vllm-project/vllm/issues/8315
这是一个关于加载已finetuned模型时出现异常的bug报告，用户提出了如何正确使用vllm进行推断的问题。

https://github.com/vllm-project/vllm/issues/8314
该issue类型为代码优化，涉及主要对象为mistral tokenizer的类型注释问题。这个问题由于mistral tokenizer的输入输出与HF的tokenizer不同，导致需要将它们拆分为不同的函数以避免引入过多的联合类型。

https://github.com/vllm-project/vllm/issues/8313
这是一个性能问题的bug报告，主要对象是vllm下的guided generation，在离线模式下运行时性能出现异常下降，导致生成速度严重受影响。

https://github.com/vllm-project/vllm/issues/8312
这是一个需求提出的issue，主要涉及的对象是需要支持Qwen2工具调用模板。该需求由于要求支持Qwen2工具调用模板，可能由于缺乏相应功能或特性而导致。

https://github.com/vllm-project/vllm/issues/8311
这是一个非常明显的问题报告，涉及vllm下的awq_triton kernels的基准测试。由于一些功能性问题，用户可能遇到了性能测量方面的困难。

https://github.com/vllm-project/vllm/issues/8310
这个issue是一个Kernel类型的贡献，涉及到为rocm添加自定义分页注意力内核的支持。

https://github.com/vllm-project/vllm/issues/8309
这是一个bug报告，该问题单涉及的主要对象是修复ppc64le构建工作，由于构建错误导致出现问题。

https://github.com/vllm-project/vllm/issues/8308
这是一个Bugfix类型的issue，主要涉及LRU cache在Outlines' guide getters上的启用问题，导致vLLM的structured generation从版本0.5.2到0.5.3变慢。

https://github.com/vllm-project/vllm/issues/8307
这是一个Bug报告类型的Issue，主要涉及的对象是vLLM中LogitsProcessor的`_get_guide`函数。由于LRUCache注释在0.5.3版本中被Outlines实现替换，导致生成结构化文本的速度下降，可能是由于Outlines的diskcachebased实现效率不高引起的。

https://github.com/vllm-project/vllm/issues/8306
这是一个建议性质的问题，主要涉及到重构和分离vLLM核心上的波束搜索功能。由于波束搜索是一个搜索算法，而不是采样算法，与其他采样算法有冲突，因此导致了代码维护和性能优化方面的挑战。

https://github.com/vllm-project/vllm/issues/8305
该issue属于bug报告类型，主要涉及的对象是模型运行程序。造成该问题的原因是模型运行程序崩溃导致了非法内存访问等错误，用户希望能够更好地重现这个问题，因此提出了需要在程序崩溃时将模型运行程序输入内容进行转储的需求。

https://github.com/vllm-project/vllm/issues/8303
这是一个Bug报告，涉及的主要对象是3090显卡。由于更新到v0.6.0版本后，用户的两个Nvlinked 3090显卡无法进行P2P和与张量并行工作，尽管Nvlink已启用和正常工作。

https://github.com/vllm-project/vllm/issues/8302
该issue是一个功能需求，主要涉及的对象是vllm容器，用户提出了关于在多主机推理中使用ray集群启动逻辑的请求。

https://github.com/vllm-project/vllm/issues/8301
这是一个Bug报告，主要涉及的对象是vllm的Mistral Large 2407模型。由于使用MistralTokenizer替代Autotokenizer导致了解析器错误地将工具当作模型回复。

https://github.com/vllm-project/vllm/issues/8299
这个issue是一个Bugfix类型的报告，涉及的主要对象是InternVL2 vision embeddings process。这个问题是由于在使用Pipeline Parallel过程中，出现了形状不匹配的RuntimeError导致的。

https://github.com/vllm-project/vllm/issues/8298
这是一个Bug报告，涉及到使用Encoder/Decoder模型时在CPU后端缺少代码检查，导致用户在尝试运行这些模型时出现错误。

https://github.com/vllm-project/vllm/issues/8297
这个issue是一个模型支持的请求，主要关注的对象是MiniCPM34B模型，由于markdown渲染问题导致需要使用原始HTML标签。

https://github.com/vllm-project/vllm/issues/8296
这是一个bug报告，主要涉及vllm下的Qwen2VL模型服务在使用--enable-prefix-caching参数时出现错误，导致与多模态大模型不兼容。

https://github.com/vllm-project/vllm/issues/8295
该issue属于需求提出类型，主要对象是将vLLM中的单一处理过程拆分为异步标记化、模型推理和去标记化步骤，以提高GPU利用率。

https://github.com/vllm-project/vllm/issues/8294
这是一个功能增强类型的issue，涉及的主要对象是指标系统。由于需要在日志中实时显示指标，因此新增了一个名为num_cumulative_preemption的指标。

https://github.com/vllm-project/vllm/issues/8293
这个issue是一个bug报告，主要涉及的对象是vLLM模型。这个bug是由于weight loading过程中变量命名错误导致PhiMoE模块加载权重时出现异常字符串。

https://github.com/vllm-project/vllm/issues/8292
这是一个bug报告类型的issue，主要涉及的对象是vllm下的cohere和jamba模型适配器的正确使用问题。由于一个Pull request导致了cohere模型的适配器支持缺失，需要修复继承适配器接口的问题。

https://github.com/vllm-project/vllm/issues/8244
这是一个性能问题报告，主要涉及vLLM在特定硬件配置下的性能表现，可能由于配置不匹配或软件问题导致性能表现低于预期。

https://github.com/vllm-project/vllm/issues/8243
这是一个bug报告，主要涉及到vllm安装过程中出现的NotImplementedError get_device_capability错误。这个问题可能是由于vllm与当前环境中的PyTorch版本不兼容所导致的。

https://github.com/vllm-project/vllm/issues/8242
这是一个bug报告，主要涉及的对象是vllm中启用--enable-prefix-caching时GPU内存利用率低于预期。这个问题的原因可能是导致GPU内存利用率低下的设置或代码问题。

https://github.com/vllm-project/vllm/issues/8241
这是一个用户提出需求的issue，主要涉及的对象是vllm的serving profiler工具。

https://github.com/vllm-project/vllm/issues/8240
这是一个功能需求的issue，主要涉及的对象是Block Manager v2的内容哈希生成逻辑。由于Block Manager v2在prefix caching模式下不支持LoRA和prompt adapter，导致用户提出了需要支持这两个功能的需求。

https://github.com/vllm-project/vllm/issues/8239
这是一个用户提出需求的问题，涉及到针对vLLM进行批量离线推理时如何确定批处理大小的问题。用户提到不清楚当前GPU利用率、适宜的批处理大小以及如何实现自动设置动态批处理大小的困惑。

https://github.com/vllm-project/vllm/issues/8238
这个issue是针对bug报告，主要涉及的对象是vLLM中对多输入支持的扩展。导致该bug的原因是原始的LLaVA模型并未经过训练以支持多图像输入。

https://github.com/vllm-project/vllm/issues/8237
这是一个bug报告，涉及vllm在pipeline parallel设置下使用response_format参数导致卡住的问题。

https://github.com/vllm-project/vllm/issues/8236
这是一个功能需求类型的issue，主要对象是vllm中的LLaVA离线推断功能，用户提出了希望添加支持多图像输入的功能。

https://github.com/vllm-project/vllm/issues/8235
这是一个Bug报告，主要对象是vllm下的TextTokensPrompt类，由于无法导入TextTokensPrompt类导致了错误。

https://github.com/vllm-project/vllm/issues/8234
这个issue属于bug报告类型，主要涉及的对象是metrics计算，由于--num-scheduler-steps大于1导致了metrics错误。

https://github.com/vllm-project/vllm/issues/8233
这是一个Bug报告，涉及到metrics计算错误导致生成吞吐量指标输出不正确。

https://github.com/vllm-project/vllm/issues/8232
这是一个关于新模型支持的需求提出的issue，主要涉及支持MiniCPM3ForCausalLM MiniCPM3-4B模型，由于未得到回应，用户可能在寻求增加新模型支持的帮助。

https://github.com/vllm-project/vllm/issues/8231
这是一个讨论型issue，涉及benchmark_throughput.py脚本中的吞吐量计算，用户对计算过程和定义生成吞吐量的方式提出了疑问。

https://github.com/vllm-project/vllm/issues/8230
这个issue是一个Bug报告，主要涉及的对象是vLLM 0.5.5版本。由于启用了prefix caching导致CUDA错误，具体表现为CUDA error: illegal memory access。

https://github.com/vllm-project/vllm/issues/8229
这是一个bug报告，涉及到kvcache中的hash collision问题。询问如果发生kvcache block_hash冲突是否会导致推断不正确。原因可能是kvcache实现中存在的哈希碰撞问题。

https://github.com/vllm-project/vllm/issues/8228
该issue属于性能问题，涉及主要对象是vLLM中的Base Model和LoRA Models，用户关注的问题是在部署多个LoRA模型时，Base Model的推理次数。

https://github.com/vllm-project/vllm/issues/8227
这是一个Bug报告，主要涉及的对象是vllm中的compressed-tensors选项。由于未支持Qwen2-57B-A14B-Instruct这一具体的量化类型，导致了这个问题的出现。

https://github.com/vllm-project/vllm/issues/8226
这个issue是关于bug报告，涉及主要对象为vllm下的sm75环境。由于使用了`--num-scheduler-steps 8`参数导致TaskGroup中出现未处理的错误。

https://github.com/vllm-project/vllm/issues/8225
这是一个用户提出需求和问题的类型的issue，涉及对象为vllm模型和Quantization功能。由于在使用FP8和INT8数据类型时出现了警告和OOM错误，用户想了解是否影响以及是否可以在不同的环境中运行。

https://github.com/vllm-project/vllm/issues/8224
这个issue是关于Pull Request提交前的检查清单，并非bug报告或用户需求。主要涉及到vLLM项目的代码质量和审查流程。

https://github.com/vllm-project/vllm/issues/8223
这是一个Bug报告类型的Issue，主要涉及TTFT（Time to First Token）性能问题。由于同时启用了chunked prefix和prefix cache，尽管prefix cache命中率高达94.5%，但TTFT性能仍然表现不佳。

https://github.com/vllm-project/vllm/issues/8222
这是一个用户提出需求的类型，主要对象是CUDA 12，并询问是否还有基于CUDA 12的".whl"安装包。原因可能是用户希望找到适用于CUDA 12的安装包以进行相应的操作。

https://github.com/vllm-project/vllm/issues/8221
该issue类型为安装问题反馈，主要涉及到在aarch64架构上安装vllm时的Dockerfile问题，用户寻求针对该架构的最新Dockerfile。

https://github.com/vllm-project/vllm/issues/8220
这个issue类型是需求问题，主要涉及的对象是`SqueezeLLM`模块的移除。

https://github.com/vllm-project/vllm/issues/8219
这是一个bug报告，涉及到vLLM在负载下的不稳定问题，可能是由GPU缓存使用达到100%后触发的。

https://github.com/vllm-project/vllm/issues/8218
这是一个功能需求报告，主要涉及"Supporting Guided Decoding via AsyncLLMEngine"。由于无法通过generate方法传递GuidedDecodingRequest对象，用户提出希望能够支持通过AsyncLLMEngine.generate()接口实现Guided Decoding的功能。

https://github.com/vllm-project/vllm/issues/8217
这是一个用户提出需求的issue，主要涉及将GPTQ的Marlin MoE支持添加到项目中，由于当前Marlin MoE内核仅支持int4，因此需要更新或添加大型MoE模型的可选测试。

https://github.com/vllm-project/vllm/issues/8216
这是一个BugFix类型的issue，涉及的主要对象是Granite模型配置。由于最新的transformers发布中还未包含granite模型，导致该模型的配置加载出现问题。

https://github.com/vllm-project/vllm/issues/8215
这是一个用户提出需求的issue，主要涉及如何在MiniCPM-V-2_6模型或任何视觉语言模型中执行多图像推论的问题。由于用户已经展示了对vllm的使用方式，但可能遇到了某些问题。

https://github.com/vllm-project/vllm/issues/8214
这个issue类型是[其他类型]，主要涉及的对象是在提交PR时需要遵循特定的格式和标准。由于提交者未填写PR描述，导致无法提交。

https://github.com/vllm-project/vllm/issues/8213
这是一个bug报告类型的issue，该问题单涉及的主要对象是VLLM日志记录间隔环境变量。由于缺少控制日志频率的环境变量，导致日志记录存在问题。

https://github.com/vllm-project/vllm/issues/8212
这个issue类型是bug报告，主要对象是FastAPI 0.113.0版本和vLLM OpenAPI，由于FastAPI 0.113.0版本的Pydantic支持重构导致了Pydantic与OpenAIAPI调用的失败。

https://github.com/vllm-project/vllm/issues/8211
这是一个软件升级的issue，主要涉及到升级一个项目下的某个组件版本。

https://github.com/vllm-project/vllm/issues/8210
这是一个用户提出需求的类型的issue，主要涉及调整终端显示日志间隔时间的问题。可能由于用户希望更改日志显示频率或者根据特定需求调整终端日志显示间隔，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/8209
这是一个bug报告，主要涉及的对象是程序的关机功能。这个issue可能是由于程序无法正常关闭导致的。

https://github.com/vllm-project/vllm/issues/8208
这是一个bug报告，涉及问题Executor在关闭时会出现问题，原因是解释器正在关闭，无法导入模块，之前导入的模块为None。

https://github.com/vllm-project/vllm/issues/8207
这是一个Bug报告，主要涉及到VLLM项目在环境设置上出现了Crashing的问题。由于在环境设置过程中出现了ImportError和undefined symbol错误导致的问题。

https://github.com/vllm-project/vllm/issues/8206
这个issue类型是功能需求，涉及的主要对象是在vLLM中添加Granite MoE。

https://github.com/vllm-project/vllm/issues/8205
这是一个用户提出需求的issue，主要对象是vllm中的benchmark_serving.py文件。用户希望了解是否可以在Multimodal LLM（llava）中使用benchmark_serving.py，并支持图像输入。

https://github.com/vllm-project/vllm/issues/8204
这是一个bug报告，主要涉及到API服务器的端口绑定问题，可能是由于引擎在准备时使用了一些端口导致的。

https://github.com/vllm-project/vllm/issues/8203
这是一个CI/Build类型的issue，涉及到multiproc worker tests的超时问题。由于在ROCm上使用spawn而非fork会增加开销，导致五秒时间可能不足以创建和销毁八个worker进程，因此需要将超时时间增加到60秒。

https://github.com/vllm-project/vllm/issues/8201
这个issue是一个[Model]类型的问题，主要涉及的对象是InternVL2模型，目的是支持多图像输入。原因是当前模型不支持多图像输入，需要更新以解决该问题。

https://github.com/vllm-project/vllm/issues/8200
这是一个用户提出需求的issue，主要涉及需要对缓存的内核进行重新设计以支持不同的布局。由于许多快速注意力内核只支持HND布局，该问题是针对调整布局以支持不同内核需求的。

https://github.com/vllm-project/vllm/issues/8199
这个issue是一个Bug报告，涉及到vLLM的Core模块，由于配置了非空的提示logprobs导致vLLM崩溃。

https://github.com/vllm-project/vllm/issues/8198
这是一个Bug报告，主要涉及vLLM在启用*prompt* logprobs支持时出现崩溃的问题。

https://github.com/vllm-project/vllm/issues/8197
这是一个功能需求类型的issue，主要涉及到解决使用大型检查点训练的QLoRA适配器无法使用TP>1进行模型分片的问题。

https://github.com/vllm-project/vllm/issues/8196
该issue属于用户提出需求类型，主要涉及使用vllm调用qwen2模型的问题。用户不清楚如何编写聊天模板以运行特定模型的推理。

https://github.com/vllm-project/vllm/issues/8195
这个issue是用户提出需求类型的问题，主要涉及vllm模型支持DiT模型的输出为Tensor (N, C, H, W)布局。用户提出了希望支持DiT模型的多模态输入和输出需求。

https://github.com/vllm-project/vllm/issues/8194
这是一个Bug报告，涉及的主要对象是vLLM引擎。由于构建docker镜像时出现的错误，导致发送请求时出现Bug。

https://github.com/vllm-project/vllm/issues/8193
这是一个特性需求的issue，主要涉及到vLLM的benchmarking脚本`benchmark_serving.py`，由于当前脚本不支持配置`logprobs`参数，导致无法实现在benchmark过程中对不同`logprobs`设置对vLLM性能影响的评估。

https://github.com/vllm-project/vllm/issues/8192
这个issue类型是功能需求，主要涉及的对象是OpenVINO vLLM backend的GPU支持。由于目前的OpenVINO vLLM backend不支持GPU设备，用户提出了希望添加GPU支持的需求。

https://github.com/vllm-project/vllm/issues/8191
这是一个[Frontend]类型的issue，主要涉及`benchmark_serving.py`文件的改进，用户提出了希望添加`--logprobs`参数的需求。

https://github.com/vllm-project/vllm/issues/8190
这个issue是关于用户提出如何释放cuda内存的问题，属于用户寻求帮助类型；用户想要运行特定模型的推断，并询问如何将其与vllm集成。原因可能是缺乏文档或指导，导致用户不清楚如何操作。

https://github.com/vllm-project/vllm/issues/8189
这是一个bug报告，主要涉及的对象是vllm中的FlashAttention模块。这个问题的原因是FlashAttention模块仅支持Ampere GPU或更新的版本，导致出现RuntimeError错误。

https://github.com/vllm-project/vllm/issues/8188
这是一个bug报告类型的issue，主要涉及到vllm的最小VRAM需求问题。在使用Llama 3.1 70B和405B模型时，由于最大序列长度大于KV缓存中可以存储的最大token数，导致了数值错误。

https://github.com/vllm-project/vllm/issues/8187
这是一个Bug报告，涉及的主要对象是在使用FastAPI和多GPU环境下实现vLLM Library。问题可能是由于NCCL的一些异常情况导致的Force Shutdown。

https://github.com/vllm-project/vllm/issues/8186
这个issue是一个Bug报告，主要涉及Phi-3.5-MoE-Instruct在vLLM上产生了奇怪的字符串。原因可能是模型实现中的问题。

https://github.com/vllm-project/vllm/issues/8185
这是一个关于Bug报告类型的issue，主要涉及的对象是vllm中GPU blocks的分配问题。问题的原因是由于max_seq_length的变化导致了KV cache中可存储token数量的不足。

https://github.com/vllm-project/vllm/issues/8184
这是一个Bug报告，主要涉及GPU内存使用量异常增加导致OOM（Out of Memory）错误。

https://github.com/vllm-project/vllm/issues/8183
这个Issue属于用户提出需求，并主要涉及VLLM OpenAI Docker镜像。用户想要了解如何使用VLLM进行SQL编码器的推理，并寻求启动OpenAI服务器的Python脚本方法。

https://github.com/vllm-project/vllm/issues/8182
这是一个bug报告，主要涉及到VLLM项目中的multi-step decoding功能。由于添加了`numschedulersteps 8`参数后出现了`AssertionError: Logits Processors are not supported in multi-step decoding`错误。

https://github.com/vllm-project/vllm/issues/8181
这个issue类型是文档更新，主要涉及的对象是VLM的文档。

https://github.com/vllm-project/vllm/issues/8180
这是一个bug报告，主要涉及对象是使用PyTorch和CUDA的环境。由于CUDA error: CUBLAS_STATUS_ALLOC_FAILED导致了RuntimeError，需要解决CUDA内存分配失败的问题。

https://github.com/vllm-project/vllm/issues/8179
这个issue类型是bug报告，涉及的主要对象是vllm的async_llm_engine.py模块。由于vllm版本为0.6.0，在使用特定命令时会出现"Aborted request"的错误提示，有时成功有时失败。

https://github.com/vllm-project/vllm/issues/8178
这是一个Bug报告，涉及到VLLM中部分监控指标不正确的问题。这可能是由于v0.6.0版本更新导致的。

https://github.com/vllm-project/vllm/issues/8177
这是一个bug报告，主要涉及到vLLM 0.5.5在4xH100 SXM环境下遇到的CUDA错误导致的问题。

https://github.com/vllm-project/vllm/issues/8176
这个issue类型是bug报告，涉及的主要对象是vLLM的性能表现。由于没有得到响应，导致了性能改进提案和性能回归报告未能解决。

https://github.com/vllm-project/vllm/issues/8175
这个issue是一个关于增加benchmark_throughput.py中block_size选项的功能需求，不是一个bug报告。

https://github.com/vllm-project/vllm/issues/8174
这是一个bug报告，涉及到vllm在安装过程中出现了无法复制文件的错误，可能是由于缺少文件或文件损坏导致的。

https://github.com/vllm-project/vllm/issues/8173
这是一个bug报告，涉及主要对象为FlashInfer API，由于未显式设置`q_data_type`导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/8172
这个issue是关于bug报告，主要对象是vllm项目中的ray workers。这个bug是由于未将环境变量传递给workers导致的，特定情况下会导致vllm崩溃。

https://github.com/vllm-project/vllm/issues/8170
这个issue属于用户需求类型，主要涉及vllm是否支持allenai/OLMoE-1B-7B-0924模型，由于未得到响应，用户来询问如何支持他们想要的模型。

https://github.com/vllm-project/vllm/issues/8169
这是一个bug报告，主要涉及到vLLM项目的OpenAI版本升级引起的问题。原因是在1.35版本中遇到错误，在升级到1.43版本后问题解决，可能需要扩大支持范围以应对未知的API添加问题。

https://github.com/vllm-project/vllm/issues/8168
这个issue是关于新的Model加载格式的改进。

https://github.com/vllm-project/vllm/issues/8167
这是一个环境信息收集相关的issue，涉及主要对象是KV cache memory utilization tracking。原因可能是收集环境相关信息时出现了某些数据缺失，导致无法准确追踪KV缓存内存利用情况。

https://github.com/vllm-project/vllm/issues/8166
该issue属于版本更新类型，涉及主要对象为项目版本控制工具。

https://github.com/vllm-project/vllm/issues/8165
这是一个bug报告，涉及的主要对象是GPTQMarlinLinearMethod。由于在构建不受支持的平台上的GPTQMarlin配置时失败，导致了`GPTQMarlin.get_min_capability()`无法查询支持情况，这个issue报告了这一问题。

https://github.com/vllm-project/vllm/issues/8164
这是一个bug报告，涉及的主要对象是benchmark serving script。原因是当前脚本中的输入和输出token吞吐量以端到端延迟计算，而不是prefill或decode延迟，可能导致数据误导。

https://github.com/vllm-project/vllm/issues/8163
这是一个bug报告，涉及Gemme LoRA测试中测试输入的更改。这可能是由于测试用例失败或表现异常而导致。

https://github.com/vllm-project/vllm/issues/8162
这是一个关于从vLLM中移除对PEFT依赖的issue，主要涉及的对象是代码库中的模型。由于markdown渲染的问题，造成了这个问题的提交。

https://github.com/vllm-project/vllm/issues/8161
这是一个关于创建CODE_OF_CONDUCT.md文档的文档问题，旨在向vllm项目添加最新版本的行为准则。

https://github.com/vllm-project/vllm/issues/8160
这是一个bug报告，主要涉及LoRA测试，并希望将其标记为软失败。

https://github.com/vllm-project/vllm/issues/8159
这个issue属于用户提出需求类型，主要涉及vLLM模型中部分上下文支持的功能，由于目标模型的上下文大于草稿模型，用户希望支持部分上下文以提供更灵活的模型训练。

https://github.com/vllm-project/vllm/issues/8158
这是一个Bug报告，主要涉及的对象是vllm异步引擎。由于PyTorch版本与CUDA版本不匹配，导致无法使用adag。

https://github.com/vllm-project/vllm/issues/8157
这是一个bug修复的issue，主要涉及到代码中的性能问题，由于`AsyncLLMEngine` 导致了服务运行缓慢，所以提出了替换为 `MPLLMEngine` 的改进。

https://github.com/vllm-project/vllm/issues/8156
这是一个关于需求的issue，涉及主要对象是vllm中的LLMEngine和AsyncLLMEngine，用户提出了关于如何使用response_format和guided output的问题。

https://github.com/vllm-project/vllm/issues/8155
这个issue是一个bug报告，涉及主要对象是CLIP encoder。由于CLIP encoder在假设下层神经层的数量小于隐藏层数量时可能出错，可能导致缺失 `post_layernorm`的问题。

https://github.com/vllm-project/vllm/issues/8154
这个issue类型是功能需求，主要涉及Cherry pick of delayed sampling功能，用户提出需要在vllm server传递额外的标志来使用该功能。

https://github.com/vllm-project/vllm/issues/8153
该issue类型为用户提出需求，主要涉及对象为vllm下的VisionEncoderDecoderModel，用户寻求关于是否有该模型的问题。

https://github.com/vllm-project/vllm/issues/8152
这是一个用户提出需求的issue，主要对象是为vLLM添加对`GPTNeoXForSequenceClassification`架构的支持。这个问题是由于目前vLLM不支持该架构，导致训练和评估RLHF方法变得缓慢。

https://github.com/vllm-project/vllm/issues/8151
这是一个用户提出需求的issue，主要涉及vllm模型与Internvl2 8b视频推理的集成问题。由于缺乏集成指导，用户不知道如何使用vllm进行视频推理。

https://github.com/vllm-project/vllm/issues/8150
这是一个特性需求的issue，主要涉及到对性能进行优化。

https://github.com/vllm-project/vllm/issues/8149
这个issue是关于bug报告，主要涉及Marlin对weight_bits = uint4b8的支持问题，导致加载下载的模型时出现量化异常。

https://github.com/vllm-project/vllm/issues/8148
该issue类型是用户提出需求，主要对象是实现混合专家模型的专家并行功能，希望在推理过程中减少通信成本。

https://github.com/vllm-project/vllm/issues/8147
这个issue是性能问题报告，主要涉及到vLLM的CPU性能影响，用户提出了关于CPU性能优化的需求和问题。

https://github.com/vllm-project/vllm/issues/8146
这是一个需求报告的issue，主要关注是支持MultiModal inputs using Llama3.1的功能。由于当前版本的vLLM不支持Llama3.1的MultiModal输入，用户希望了解是否可以通过启用llama3.1作为VLM或者其他方式来支持。

https://github.com/vllm-project/vllm/issues/8145
这是一个bug报告，主要涉及VLLM中启用特定解码功能时与CUDA图表相互影响，导致数值传输错误。

https://github.com/vllm-project/vllm/issues/8144
这个issue属于版本发布讨论类型，主要涉及的对象是vllm项目。原因可能是为了讨论即将发布的版本v0.6.0以及相关的性能问题和合并请求。

https://github.com/vllm-project/vllm/issues/8143
这是一个bug报告，涉及的主要对象是VLLM模型。由于环境中PyTorch版本不匹配，导致在使用VLLM 0.5.5版本时出现错误。

https://github.com/vllm-project/vllm/issues/8142
这是一个bug报告，涉及的主要对象是deepseek-v2和deepseek-v2-lite，由于使用相同的启动命令时，deepseek-v2-lite成功而deepseek-v2遇到错误。

https://github.com/vllm-project/vllm/issues/8141
这是一个Bug报告。该问题涉及部署AWQ模型在H20上时遇到的错误，可能由于GPU内存释放导致。

https://github.com/vllm-project/vllm/issues/8140
这是一个功能增强的issue，主要涉及添加一个新的指标(metric)来跟踪每次迭代的token数量，以帮助了解import GEMM的形状。

https://github.com/vllm-project/vllm/issues/8139
这个issue类型是用户提出需求，主要对象是要支持一个新的模型Qwen2-VL。原因是该模型目前还未被vllm支持，用户在询问此模型是否可以被加入支持。

https://github.com/vllm-project/vllm/issues/8138
这是一个Bug报告类型的issue，主要涉及到vllm服务在使用过程中出现了Tensor not iterable错误。错误可能由参数传递或代码逻辑不正确导致。

https://github.com/vllm-project/vllm/issues/8137
这是一个bug报告，涉及的主要对象是TPU。由于未使用XLA rank导致了持久缓存路径问题。

https://github.com/vllm-project/vllm/issues/8136
这是一个bug报告，主要涉及在使用较小模型时出现CUDA内存错误的问题。这个问题可能由于多进程运行时导致CUDA内存不足而出现。

https://github.com/vllm-project/vllm/issues/8135
这是一个功能更新（Feature Update）类型的issue，主要涉及到GPTQ激活排序功能的更新。由于需要支持`actorder`参数和`weight_g_idx`参数的加载以及相应的测试，导致了这个issue的提出。

https://github.com/vllm-project/vllm/issues/8134
这是一个用户提出需求的类型，该问题单主要涉及CI相关功能。由于at-mentions引起的spam，需要修改PR remainder以减少垃圾信息。

https://github.com/vllm-project/vllm/issues/8133
这是一个功能需求的issue，主要涉及到在cuda镜像中使用Python 3.12版本。由于Python 3.11在vLLM图像中表现更快，而Python 3.12带来了更多性能修复，因此建议更新Python版本以提高性能。

https://github.com/vllm-project/vllm/issues/8132
这是一个关于在vllm中添加级联推理（cascade inference）功能的PR。

https://github.com/vllm-project/vllm/issues/8131
这是一个关于修复bug并整合测试用例的issue，主要涉及flashinfer fp8 kv-cache的测试。

https://github.com/vllm-project/vllm/issues/8130
该issue为关于CI/Build改进的问题，主要涉及Enabling kernels tests for AMD，并忽略了一些失败的测试。这可能是因为AMD内核测试存在一些问题导致部分测试无法通过。

https://github.com/vllm-project/vllm/issues/8129
这个issue类型为bug报告，涉及的主要对象是GHA workflow。由于使用了`'`符号导致问题，需要移除并修复。

https://github.com/vllm-project/vllm/issues/8128
这是一个bug报告，该问题单涉及的主要对象是TPU，由于next_token_ids的形状问题导致了bug。

https://github.com/vllm-project/vllm/issues/8127
该issue类型是用户提出需求，主要涉及vLLM模型的quantization支持，用户希望实现int8量化支持，以减少内存使用、提高性能和降低运营成本。

https://github.com/vllm-project/vllm/issues/8126
该issue类型为需求更改，涉及的主要对象为`engine_use_ray`。由于mo无法分配足够的时间来处理这个问题，需要对之前的提交进行重新处理。

https://github.com/vllm-project/vllm/issues/8125
这是一个关于GitHub上的vllm项目中有关MPLLMEngine异步流处理的issue，涉及主要对象为MPLLMEngine。

https://github.com/vllm-project/vllm/issues/8124
这是一个用户提出需求的issue，主要涉及的对象是CI系统。这个问题是由于需要限制只有PR的审阅者和提交者可以触发CI，而现有的工作流无法实现该需求。

https://github.com/vllm-project/vllm/issues/8123
该issue是一个Bug报告，涉及的主要对象是向`requirementscommon.txt`中添加`PEFT`，可能由于markdown渲染问题导致无法工作。

https://github.com/vllm-project/vllm/issues/8122
这是一个提交PR（Pull Request）描述信息不完整的issue，主要涉及更新awq fused版本，可能是由于提交者未在PR描述中填入必要信息所导致。

https://github.com/vllm-project/vllm/issues/8121
该issue类型为用户提出需求或请教问题，涉及主要对象为`max_num_batched_tokens`。由于用户困惑为什么较小的`max_num_batched_tokens`值会获得更好的ITL，并且询问关于模型并行运行速度，序列总长度超过`max_num_batched_tokens`会受限制等问题，寻求关于如何设置该参数的帮助。

https://github.com/vllm-project/vllm/issues/8120
这个issue类型为代码优化，涉及主要对象为缓存预取和前缀缓存的功能。由于缺少正确的合著者，导致了合作者列表的错误，需要添加正确的合著者信息。

https://github.com/vllm-project/vllm/issues/8119
这是一个bug报告，问题涉及的主要对象是权重加载，由于权重加载的问题导致了 unfused pathway 的加载出现 bug。

https://github.com/vllm-project/vllm/issues/8118
这个issue是一个bug报告，主要涉及的对象是vllm在mac上pip安装时出现的错误。由于缺少"+empty"标签，导致在默认pip安装vllm时失败，需要手动添加参数才能成功安装。

https://github.com/vllm-project/vllm/issues/8117
这是一个bug报告issue，主要涉及LLM.chat()方法的文档错误导致用户误解消息输入的格式，建议修复文档描述或者修改方法以支持多对话生成。

https://github.com/vllm-project/vllm/issues/8116
这是一个Bug报告类型的issue，主要涉及的对象是加载使用GPTQ量化的GPTBigCode模型时出现的加载失败问题。由于使用了marlin核导致加载失败，并且尚未合并GPTQ的等效更改，用户提出寻求正确的修复方法的帮助。

https://github.com/vllm-project/vllm/issues/8115
这是一个Bug报告类型的issue，主要涉及vLLM的metrics endpoint显示的TTFT计数与成功请求总数不匹配，可能由于某种原因导致了此Bug。

https://github.com/vllm-project/vllm/issues/8114
这是一则bug报告，主要涉及Jamba 1.5 Mini在运行beam searches时只返回少量字符然后提前停止的问题。原因可能是相关代码逻辑或参数设置方面的错误。

https://github.com/vllm-project/vllm/issues/8112
这是一个Bugfix类型的Issue，主要涉及到detokenizer.py中的代码逻辑问题，导致长度在某些情况下呈指数级增长的Bug。

https://github.com/vllm-project/vllm/issues/8111
这是一个用户提出需求的issue，主要涉及VLLM在分类或确定任务中如何输出每个可能标记的logprob，用户希望获得关于不同类别或真假判断的概率输出。可能由于代码逻辑或参数设置的问题导致了logprob输出异常。

https://github.com/vllm-project/vllm/issues/8110
这是一个Bug报告类型的issue，涉及的主要对象是在CPU环境下使用VLLM框架部署MiniCPM-2B模型时出现的错误。由于未指定Framework，导致出现了错误提示并需要设置`trust_remote_code=True`来解决。

https://github.com/vllm-project/vllm/issues/8109
这个issue是一个Bugfix类型的报告，涉及的主要对象是detokenizer.py文件。导致这个bug的原因是在代码中将`next_iter_tokens`的引用直接赋给`prev_tokens`，使它们指向同一个对象，当`prev_tokens`被`next_iter_tokens`扩展时，两者的长度都会加倍，从而导致长度呈指数增长。

https://github.com/vllm-project/vllm/issues/8108
这是一个bug报告，涉及Flat PA中数值块的计算错误，因为添加了空表块引起了问题。

https://github.com/vllm-project/vllm/issues/8107
这是一个bug报告，该问题涉及的主要对象是Docker image for 0.5.4缺少package timm==0.9.10导致服务器出现错误。

https://github.com/vllm-project/vllm/issues/8106
这是一个bug报告，主要涉及的对象是vllm下的siglip模块。由于siglip作为多模态视觉编码器时不需要后层归一化，而原始实现中错误地包含了后层归一化，导致输出异常。

https://github.com/vllm-project/vllm/issues/8105
这是一个用户提出需求的issue，主要涉及vLLM是否可以同时处理多轮对话和多用户身份的情况。用户询问如何实现多轮和多用户实例同时处理的方式，以及如何让模型记住每位用户说过的话。由于模型遗忘了用户说过的内容，用户希望能实现这样的功能。

https://github.com/vllm-project/vllm/issues/8104
这是一个关于性能问题的issue，用户提出了在为大量提示提供服务时速度过慢的问题，希望改善性能。

https://github.com/vllm-project/vllm/issues/8103
这个issue类型是chore类型的， 主要对象是更新`checkwheelsize.py`脚本，由于需要更灵活地设置wheel的最大大小，所以需要从环境变量中读取MAX_SIZE_MB值。

https://github.com/vllm-project/vllm/issues/8102
这是一个bug报告，涉及的主要对象是新模型['UltravoxModel']。由于该模型架构目前不受支持，导致出现数值错误（ValueError）的情况。

https://github.com/vllm-project/vllm/issues/8101
这是一个bug报告，涉及的主要对象是vllm环境。由于未指定原因导致多次运行后出现数值错误和Zip文件错误，用户提出了是否是一个严重的bug。

https://github.com/vllm-project/vllm/issues/8100
这是一个bug报告，主要涉及vllm库中的guided choice功能，由于运行api server时出现了AsyncEngineDeadError的情况。

https://github.com/vllm-project/vllm/issues/8099
该issue类型为bug报告，涉及vllm的安装问题，由于使用Image构建源代码时出现错误。

https://github.com/vllm-project/vllm/issues/8098
这是一个bug报告，主要对象是在offline chat中支持multimodal inputs时出现的问题，可能是由于`LLM.chat()`文档错误导致的多次生成chat的调用问题。

https://github.com/vllm-project/vllm/issues/8097
这个issue是一个bug报告，主要涉及的对象是InternVL226B模型在多卡并行情况下出现的问题，由于tensor_parallel_size设置不当导致25无法被4整除。

https://github.com/vllm-project/vllm/issues/8096
这是一个Bug报告，涉及的主要对象是vllm下的minicpm-v2.6模型，由于GGUF量化而导致`AssertionError`错误。

https://github.com/vllm-project/vllm/issues/8095
这个issue是关于Bug报告，涉及vllm的CPU安装编译错误，用户询问当前环境问题导致的bug。

https://github.com/vllm-project/vllm/issues/8094
这个issue类型是bug报告，涉及VLLM中使用float16时的错误，可能是由于原因导致的dtype float16 Failure to use enable-chunked-prefill。

https://github.com/vllm-project/vllm/issues/8092
这是一个关于性能优化和bug修复的issue，主要涉及Server的重构，由于AsyncLLMEngine导致了显著的性能下降。

https://github.com/vllm-project/vllm/issues/8091
这是一个bug报告，主要涉及vLLM和FlashInfer的集成问题，导致无法在T4上运行Gemma22b。

https://github.com/vllm-project/vllm/issues/8089
这个issue是有关性能问题的，主要涉及到vLLM引擎在TP=8情况下性能无法达到预期，并且具有更差的延迟指标。原因可能是由于vLLM在该设置下的扩展性不佳。

https://github.com/vllm-project/vllm/issues/8088
这是一个关于bug报告的issue，涉及到vLLM中的ChatCompletionRequest类缺少参数`parallel_tool_calls`导致无法正确调用OpenAI兼容服务器。

https://github.com/vllm-project/vllm/issues/8087
这是一个Bug报告类型的issue，主要涉及VLLM在tensor-parallel-size大于1时出现Stuck的问题。

https://github.com/vllm-project/vllm/issues/8086
这个issue是关于性能提升建议，主要涉及TTFT随着批量tokens数目线性增加的问题，作者希望了解为什么TTFT与prompt长度呈这样明显的增长关系，以及寻求解决方案。

https://github.com/vllm-project/vllm/issues/8085
这是一个bug报告，涉及到vllm的服务停止问题，由于无法正确停止服务导致重新加载模型时出现了问题。

https://github.com/vllm-project/vllm/issues/8083
这个issue属于bug报告类型，主要涉及的对象是在构建CPU docker镜像时发生的系统崩溃。原因可能是与PyTorch版本2.4.0+cu121相关的构建过程中出现了无法解决的问题。

https://github.com/vllm-project/vllm/issues/8082
这是一个bug报告类型的issue，主要涉及到VLLM中的LoRA模型训练过程中的lm_head和embed_tokens相关问题，由于未完全训练lm_head的支持导致了适配器质量显著下降。

https://github.com/vllm-project/vllm/issues/8081
这是一个bug报告类型的issue，主要涉及VLLM与Lumina_mgpt集成时加载权重时遇到的问题。原因可能是环境配置或集成方式不正确导致的。

https://github.com/vllm-project/vllm/issues/8080
这是一个bug报告，主要涉及vllm项目下的CUDA错误导致的RuntimeError。

https://github.com/vllm-project/vllm/issues/8078
这个issue类型为用户提出需求，主要涉及的对象是新模型"quantized Qwen2 MoE models"。该问题暂未得到回应，用户可能寻求支持新模型的集成或解决相关问题。

https://github.com/vllm-project/vllm/issues/8076
这是一个代码清理类的issue，涉及主要对象为RoPE的`forward_native`方法。

https://github.com/vllm-project/vllm/issues/8075
该issue属于用户提出的需求类型，主要涉及改进vLLM中关于快速引导解码的功能。由于预定义输出的确定性特性未完全被充分利用，用户提出了关于并行解码以提高解码效率的建议。

https://github.com/vllm-project/vllm/issues/8074
这是一个功能需求的issue，该问题涉及主要对象是vLLM在Kubernetes上支持多节点服务。由于缺乏集成到vLLM代码库中的解决方案，用户提出了希望官方团队开发此功能的需求。

https://github.com/vllm-project/vllm/issues/8073
这是一个bug报告类型的issue，主要涉及的对象是使用vllm库时遇到了OutOfMemoryError错误，可能是由于使用 speculative decoding (SD) 时导致内存不足而出现问题。

https://github.com/vllm-project/vllm/issues/8072
该issue属于用户提出需求类型，主要涉及的对象是在vllm中添加smoothquant支持。由于用户认为smoothquant功能很棒，并已经在其他项目中添加了该功能，因此希望在vllm中也能集成smoothquant。

https://github.com/vllm-project/vllm/issues/8071
这是一个Bug报告，涉及VLLM中的SpeculativeDecoding输出了无意义的单词，可能是由于温度设置导致的问题。

https://github.com/vllm-project/vllm/issues/8070
这个issue是一个bug报告，主要涉及到模型架构不支持的问题，导致数值错误。

https://github.com/vllm-project/vllm/issues/8069
这是一个用户提出需求的issue，主要涉及到VLLM是否支持使用mpirun启动多个卡，并绑定不同的CPU到每个卡上。可能是由于性能测试需要，用户想要通过mpirun将不同的CPU绑定到不同的卡上。

https://github.com/vllm-project/vllm/issues/8068
这是一个Bug报告，涉及主要对象是vllm下的CUDA Graph，由于取最小值而导致数值不匹配的问题。

https://github.com/vllm-project/vllm/issues/8067
这个issue是一个特性需求，主要对象是Huggingface generation interface，用户提出需求希望实现beam search with nonzero temperature来增加生成的多样性。

https://github.com/vllm-project/vllm/issues/8066
这是一个关于Bug报告的issue，主要涉及到VLLM下的TPU InternVL2 Model Error Graph，由于不支持内置的_XLAC.PyCapsule._xla_get_replication_devices_count而导致了错误Graph的问题。

https://github.com/vllm-project/vllm/issues/8065
这是一个用户提出需求的issue，主要涉及如何在TPU环境下使用InternVL2模型。用户因不知道如何将InternVL2与vllm集成而提出了这个问题。

https://github.com/vllm-project/vllm/issues/8063
这是一个关于修改vLLM中音频相关包的可选安装的问题，涉及主要对象是相关依赖项因许可证问题导致的BUG。

https://github.com/vllm-project/vllm/issues/8062
这个issue是关于"Neuron"模块下添加或覆盖神经元配置和量化配置的需求。

https://github.com/vllm-project/vllm/issues/8061
这是一个 Bugfix issue，涉及到 VLLM 中 ViT 模型在 CPU 后端上运行时的问题。由于前一个修复对每个 ViT 模型的编码器注意力都强制使用了 xformers，导致了 VLM 在 CPU 后端的支持出现问题，需要添加一个针对 CPU 后端的 SDPA 回退来解决这个问题。

https://github.com/vllm-project/vllm/issues/8060
这是一个用户提出需求报告的Github上的issue，目的是为`run_batch.py`添加一个tqdm进度指示器。

https://github.com/vllm-project/vllm/issues/8059
这是一个BugFix类型的issue，主要涉及多步骤调度器在请求取消时导致崩溃的问题。

https://github.com/vllm-project/vllm/issues/8058
这是一个Bug报告类型的issue，涉及vLLM在尝试使用多个GPU时出现hang的问题，可能是由于nccl步骤导致的。

https://github.com/vllm-project/vllm/issues/8057
这是一个bug报告，涉及vllm版本0.4.3中使用guided decoding时出现的错误。

https://github.com/vllm-project/vllm/issues/8056
这个issue是一个Bugfix类型的问题，涉及主要对象是无法接受没有.gguf扩展名的gguf文件。这个问题是因为即使使用了"quantization gguf loadformat gguf"标志位，gguf文件没有.gguf扩展名也无法运行。

https://github.com/vllm-project/vllm/issues/8055
这个issue是一个bug报告，涉及到InternVL2B模型的张量并行推断问题，导致输出是无意义的。

https://github.com/vllm-project/vllm/issues/8054
这是一个技术更新的issue，主要对象是vLLM项目中的Ascend NPU后端。由于该PR是vLLM昇腾的POC版本且不再维护，用户可能遇到无法获取最新支持和功能的问题。

https://github.com/vllm-project/vllm/issues/8053
这个issue属于bug报告，主要涉及vllm项目中使用多个多模态输入时出现的Bad Request问题，可能是由于功能尚在开发中或者存在使用问题。

https://github.com/vllm-project/vllm/issues/8052
这是一个bug报告，涉及的主要对象是Phi-3.5-MoE模型。由于模型之间的兼容性问题，导致CI测试失败。

https://github.com/vllm-project/vllm/issues/8051
这是一个Bug报告类型的Issue，主要涉及的对象是vllm项目中的测试模块。原因是测试中使用的某些方法可能导致测试结果不稳定，可能是硬件差异或内核漂移等因素引起的。

https://github.com/vllm-project/vllm/issues/8050
这个issue是一个优化性质的需求提出，主要涉及Async + Multi-step的性能优化，由于之前的异步操作仅在解码步骤中执行，现在扩展到了解码的所有步骤和前一个提示执行，最终导致性能改进了约28%。

https://github.com/vllm-project/vllm/issues/8049
这是一个与代码贡献相关的issue，主要涉及的对象是vLLM的前端。由于多模态占位符填充逻辑的简化，以及markdown渲染问题，导致该issue的内容。

https://github.com/vllm-project/vllm/issues/8048
这是一个空白的issue，类型未知，无具体内容说明问题对象和原因。

https://github.com/vllm-project/vllm/issues/8047
这是一个Bug报告issue，涉及到vLLM的Core模块，通过专门解码时返回prompt_logprobs可能导致崩溃。

https://github.com/vllm-project/vllm/issues/8046
这个issue是一个工作进行中的内核开发问题，涉及的主要对象是Machete W4A8，原因导致了一些bug或需求的提出。

https://github.com/vllm-project/vllm/issues/8045
这个issue是关于bug报告的，主要涉及vLLM生成JSON时的不一致性以及在特定条件下的生成速度问题。可能是由于同时启用了speculative decoding（ngram）、设置了温度大于0.0时导致的。

https://github.com/vllm-project/vllm/issues/8044
这是一个优化工程的Issue，主要涉及到Dockerfile的构建过程。

https://github.com/vllm-project/vllm/issues/8043
这是一个CI/Build类型的issue，主要涉及更新Cutlass至3.5.1标记。原因是使用`v3.5.1`标签能让我们再次使用`GIT_SHALLOW=TRUE`，加快了获取Cutlass的速度。

https://github.com/vllm-project/vllm/issues/8042
这是一个关于安装问题的bug报告，涉及主要对象为vLLM在ROCM上无sudo权限安装的问题。由于缺少libamdhip64.so文件，导致安装失败。

https://github.com/vllm-project/vllm/issues/8041
这是一个清理（cleanup）类型的issue，主要涉及到从引擎中移除`use-ray`功能，以及考虑对`workeruseray`功能进行相似处理。

https://github.com/vllm-project/vllm/issues/8040
这是一个性能问题报告，主要涉及到vLLM中的Sampler组件，用户反馈当num_head从1增加到8时，latency显著增加，导致性能严重下降。

https://github.com/vllm-project/vllm/issues/8039
这个issue是关于内核更改接口以适应连续批处理的需求。

https://github.com/vllm-project/vllm/issues/8038
这是一个用户提出需求的 issue，目标是添加一个类似于 https://docs.ray.io/en/latest/serve/tutorials/vllmexample.html 的 RayServe 示例。

https://github.com/vllm-project/vllm/issues/8037
这是一个Bugfix类型的issue，主要涉及vllm0.5.5版本的ModelScope模型bug修复。

https://github.com/vllm-project/vllm/issues/8036
该问题类型为用户提出需求，主要对象是将T5模型贡献给vLLM项目。由于该用户希望将T5模型贡献给项目，并表示自己是第一次贡献，希望从团队中学习，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/8035
这是一个bug报告，涉及TPU类型的API，由于bug导致了需修复。

https://github.com/vllm-project/vllm/issues/8034
这是一个Bug报告，涉及的主要对象是Exaone模型。这个问题由于CC([MODEL] add Exaone model support)与CC([Core] Logprobs support in Multistep)不兼容，导致了CI测试的失败。

https://github.com/vllm-project/vllm/issues/8033
这个issue属于用户提出问题类型，主要涉及VLLM的API Server以及使用`AsyncLLMEngine`来提供单个模型实例的服务。用户想了解在这种情况下是否推荐直接使用`AsyncLLMEngine`。

https://github.com/vllm-project/vllm/issues/8032
这是一个功能改进的issue，主要涉及的对象是 Fused Marlin MoE 模块。

https://github.com/vllm-project/vllm/issues/8031
这个issue是技术需求类型的，主要涉及vllm模型在GPU上支持的最大并发请求数的问题，用户疑惑如何确定单个V100 GPU支持的最大并发请求数量以及处理过程中可能出现的问题。

https://github.com/vllm-project/vllm/issues/8030
这是一个关于许可证问题的issue，主要涉及vLLM项目的依赖关系。由于Librosa依赖的soxr库采用LGPL许可证，导致一些大公司无法安装vLLM，用户请求重新考虑Librosa的使用。

https://github.com/vllm-project/vllm/issues/8029
这是一个bug报告类型的issue，涉及的主要对象是Qwen Multimodal Support (Qwen-VL / Qwen-VL-Chat)模型。原因是当前Qwen模型在VLLM中跳过加载视觉转换器权重，导致第三方无法支持QwenVLChat。

https://github.com/vllm-project/vllm/issues/8028
这是一个bug报告，涉及主要对象是multimodal models。原因是`max_num_batched_tokens`过小导致mismatched placeholder count错误。

https://github.com/vllm-project/vllm/issues/8027
这是一个bug报告，涉及了使用v0.5.4版本的vllm后，获得不一致的单词回应。原因可能是在调用API时出现了异常响应。

https://github.com/vllm-project/vllm/issues/8026
这个issue是关于修复ppc64le平台上的Dockerfile以及为buildkite添加一个脚本的问题，属于代码改进类型的贡献。

https://github.com/vllm-project/vllm/issues/8025
这是一个bug报告，涉及的主要对象是VLLM环境下出现了CUDA error导致的RuntimeError。

https://github.com/vllm-project/vllm/issues/8024
这是一个bug报告，涉及的主要对象是vLLM在V100上运行时遇到错误。由于不支持在非Ampere架构上进行mma > mma布局转换，导致出现了core dumped的错误。

https://github.com/vllm-project/vllm/issues/8023
这是一个bug报告，主要涉及VLLM在环境Flashinfer支持SM75时仍然出现错误的问题。由于Flashinfer环境更新导致VLLM出现错误，用户提出了该问题。

https://github.com/vllm-project/vllm/issues/8022
这个issue类型是用户提出需求，该问题单涉及的主要对象是希望支持BAAI/bgererankerv2m3模型。由于没有收到任何回应，用户希望vllm项目能够支持这个模型。

https://github.com/vllm-project/vllm/issues/8021
这是一个用户提出需求的issue，主要对象是在多节点多GPU上使用torchrun启动分布式推理服务器，用户遇到无法正常工作的问题，可能是由于配置或操作步骤不正确导致。

https://github.com/vllm-project/vllm/issues/8020
这是一个bug报告，主要涉及的对象是在运行vllm时输出结果出现了只有尾随换行符的情况，可能由于程序版本升级或GPU内存不足等原因导致。

https://github.com/vllm-project/vllm/issues/8019
这是一个bug报告类型的issue，主要涉及批量任务执行时报错，可能是由于环境版本0.5.4导致的问题。

https://github.com/vllm-project/vllm/issues/8018
这是一个功能需求类型的issue，主要涉及到vllm核心内核的支持外部交换器，由于GPU内存不足，需要将kv缓存交换到CPU内存。

https://github.com/vllm-project/vllm/issues/8017
这个issue是关于Bug报告，主要涉及InternVL2-2B模型在使用tensor parallel inference时输出乱码的问题，原因是由于`internlm2` backbone中的`split_qkv`函数引入的问题。

https://github.com/vllm-project/vllm/issues/8016
这是一个bug报告，主要涉及v0.5.5版本的vLLM，由于随机性导致程序崩溃并需要完全重新启动。

https://github.com/vllm-project/vllm/issues/8015
这是一个misc类型的issue，主要涉及在vLLM中添加ngram重复检查功能，且当检测到重复时停止生成。

https://github.com/vllm-project/vllm/issues/8014
这是一个功能需求报告，主要涉及的对象是打印请求指标到标准输出，以避免需要使用 opentelemetry 进行性能分析。

https://github.com/vllm-project/vllm/issues/8013
这是一个bug报告，涉及Flashinfer backend的FP8 Datatype参数替换导致字符串结果略有不同的问题。

https://github.com/vllm-project/vllm/issues/8012
这是一个用户提出需求的 issue，主要涉及的对象是 vLLM 中的 causal_conv1d_update 接口。由于连续批处理导致当前批处理中的状态来自不同位置，需要改变接口以允许传递索引列表进行更新。

https://github.com/vllm-project/vllm/issues/8011
这个issue是用户提出需求，请求关于TPU进行异步输出处理的帮助。

https://github.com/vllm-project/vllm/issues/8010
这是关于文档更新的问题，主要涉及NVIDIA AMMO 工具更名为 "TensorRT model optimizer"，导致相关文档过时。

https://github.com/vllm-project/vllm/issues/8009
这是一个bug报告，主要涉及Flashinfer后端的Metadata积累过程中的数据类型参数问题，用户提出由于无法一致预测`data_type`为`uint8`或`fp8`而遇到的问题。

https://github.com/vllm-project/vllm/issues/8008
这个issue是一个bug报告，涉及的主要对象是vLLM中的Neuron。该问题可能是由于并发请求达到或超过最大序列数（max_num_seqs）时，导致Neuron性能急剧下降。

https://github.com/vllm-project/vllm/issues/8007
这是一个bug报告类型的issue，主要涉及的对象是vLLM和Neuron。由于并发请求达到或超过max_num_seqs导致性能急剧下降，出现请求延迟严重增加的症状。

https://github.com/vllm-project/vllm/issues/8006
这是一个性能问题的报告，主要涉及到vLLM模型的INT4量化未能带来预期的推理速度提升。

https://github.com/vllm-project/vllm/issues/8005
这是一个性能问题，并涉及到vLLM中的prefix cache使用。原因可能是无法观察到推理速度上的提高，即使是相同的提示重复使用，这可能是由于对于相同的提示，推理的速度没有提升。

https://github.com/vllm-project/vllm/issues/8004
这个issue类型是bug报告，主要对象是vLLM的安装过程。由于rocm 6.2下的source安装仍需libamdhip64.so.6，导致出现该bug。

https://github.com/vllm-project/vllm/issues/8003
这个issue类型是特性需求，涉及到Proof of Work价值的提案。此需求主要涉及到运行LLM的矿工和验证者，由于LLM的非确定性特性和其结果的难以比较导致矿工有动机使用轻量模型，需要提出解决这一问题的方案。

https://github.com/vllm-project/vllm/issues/8002
这是一个关于RFC（Request for Comments）的问题，提出了关于构建`vllm-flash-attn`的源码的建议，并询问了不同构建方法的意见。

https://github.com/vllm-project/vllm/issues/8001
这是一个bug报告，涉及的主要对象是markdown渲染问题。这个issue的症状是由于markdown渲染不起作用，所以在这里使用原始HTML。

https://github.com/vllm-project/vllm/issues/8000
这是一个功能改进类的issue，主要涉及的对象是vllm中的linear layers。由于未正确处理没有偏置的线性层情况，在调用`extra_repr`时可能会出现问题。

https://github.com/vllm-project/vllm/issues/7999
这是一个bug报告issue，涉及的主要对象是通过CUDA图扩展的批处理大小设置不当导致程序崩溃。

https://github.com/vllm-project/vllm/issues/7998
这是一个bug报告，涉及的主要对象是VLM中的multimodal模型；该问题由于现有的处理方式导致placeholder tokens在超出上下文长度时被截断，进而造成在模型内分配multimodal features给placeholder tokens时出现混乱错误。

https://github.com/vllm-project/vllm/issues/7997
这是一个用户询问如何在离线推理过程中获取GPU虚拟块利用率日志的问题，问题类型是用户提出需求。

https://github.com/vllm-project/vllm/issues/7996
这个issue类型是bug报告，涉及的主要对象是InternVL2-26B模型的推断错误，由于尝试将大量的tokens分配给占位符导致了错误。

https://github.com/vllm-project/vllm/issues/7995
这是一个关于性能优化的issue，主要对象是VLLM的内核，经原因为降低内存消耗而修改默认值。

https://github.com/vllm-project/vllm/issues/7994
这是一个功能需求的issue，主要涉及vllm支持外部交换器的功能。由于GPU内存不足时，需要将kv缓存交换到CPU内存，因此添加了外部交换器接口以及相应的实现，来扩展token的存储空间。

https://github.com/vllm-project/vllm/issues/7993
这个issue是一个bug报告，涉及的主要对象是vllm程序的.gguf文件运行问题，由于没有 .gguf 扩展名会导致无法正确运行文件。

https://github.com/vllm-project/vllm/issues/7992
这是一个Bug报告，涉及的主要对象是在A100 GPU上运行Jamba-1.5-mini时发生的崩溃问题。原因可能是与experts_int8量化相关的问题。

https://github.com/vllm-project/vllm/issues/7991
这是一个关于Bug报告的Issue，主要涉及VLLM 0.4.3构建过程中出现的定义错误，可能是由于依赖引入错误导致的。

https://github.com/vllm-project/vllm/issues/7990
这是一个功能需求问题，主要涉及使用vllm部署多个lora时切换问题。由于用户希望同时使用两个lora设备而不进行切换导致症状。

https://github.com/vllm-project/vllm/issues/7989
这个issue是关于Bug报告，涉及主要对象为VLLM中的Pallas backend，由于Pallas后端错误地查询了加速器类型，导致了无法在GKE Autopilot上正确部署VLLM的问题。

https://github.com/vllm-project/vllm/issues/7988
这个issue是用户询问关于VLLM安装的问题，主要涉及VLLM不同版本的区别和针对特定CUDA版本的安装需求。

https://github.com/vllm-project/vllm/issues/7987
这是一个Bug报告，该问题主要涉及VLLM引擎中的AsyncEngineDeadError错误，可能由于某些参数设置无效导致任务意外结束。

https://github.com/vllm-project/vllm/issues/7986
这是一个bug报告类型的issue，涉及到vllm0.5.5版本中的代码问题，导致容器快速退出。

https://github.com/vllm-project/vllm/issues/7985
这是一个bug报告，涉及主要对象是核心和内核，此问题由于之前的错误PR导致`kvcachedtype=auto`存在错误，目前结果还需要进一步验证。

https://github.com/vllm-project/vllm/issues/7984
这是一个关于用户提出需求的issue，主要涉及到HuatuGPTVision 7B模型的架构 LlavaQwen2ForCausalLM 在vLLM中不被支持的问题。

https://github.com/vllm-project/vllm/issues/7983
这是一个用户寻求帮助的问题，涉及主要对象为vLLM模型的部署和集成。用户想要运行ColPali的推理，但不清楚如何将其与vLLM集成，因为ColPali使用了PaliGemma和一些适配器。

https://github.com/vllm-project/vllm/issues/7981
这是一个用户提出需求的问题，主要涉及如何使用vllm生成logits而不是文本。用户提出该问题的原因可能是无法找到logits生成的文件路径。

https://github.com/vllm-project/vllm/issues/7979
这是一个bug报告，主要涉及的对象是vllm的模型运行。导致GPU内存使用超过20G的原因可能是模型参数设置不当。

https://github.com/vllm-project/vllm/issues/7978
这是一个bug报告类型的issue，涉及vllm模型在运行时需要提供chat template，由于transformers v4.44不再默认提供chat template导致出现400错误码。

https://github.com/vllm-project/vllm/issues/7977
这是一个bug报告，主要涉及到LoRA的使能问题，用户在不同条件下得到了不同的答案。

https://github.com/vllm-project/vllm/issues/7976
此issue属于需求更改类型，涉及主要对象为`GPTQ`项目，由于新需求，需要更新代码以使用`vLLMParameters`。

https://github.com/vllm-project/vllm/issues/7975
这是一个功能改进类型的issue，主要涉及到torch.compile中的代码。由于在https://github.com/vllmproject/vllm/pull/7898中跳过了dynamo runtime的开销，不再需要在分析后重置代码了。

https://github.com/vllm-project/vllm/issues/7974
这个issue是一个Bugfix类型的问题，涉及到VLLM中`NestedTensor`处理的异常，导致在堆叠多模态张量以表示每个提示中的多个图像时出现异常。

https://github.com/vllm-project/vllm/issues/7973
这是一个用户提交需求的issue，主要涉及更新TPU Int8以使用新的vLLM参数。原因可能是为了提高模型的性能或准确度。

https://github.com/vllm-project/vllm/issues/7972
这是一个更新类型的issue，涉及的主要对象是`FBGEMMFp8`。由于需要更新使用`vLLMParameters`，才能满足最新的要求。

https://github.com/vllm-project/vllm/issues/7971
这是一个代码清理（Cleanup）类型的issue，主要涉及的对象是`SqueezeLLM`。由于需要清理和分离线性方法（linear methods）以及kv_cache和moe在quantization文件夹中的关系，因此需要移除`SqueezeLLM`。

https://github.com/vllm-project/vllm/issues/7970
这个issue是关于bug的，主要涉及punica ops在torch注册中的可选性问题。由于pytorch使用较旧版本时需要punica ops作为torch ops，导致了这个bug的产生。

https://github.com/vllm-project/vllm/issues/7969
这是一个关于进行Roberta embedding模型测试的Issue，主要涉及到cuda图形在编码器模型中不受支持，导致需要禁用以运行测试。

https://github.com/vllm-project/vllm/issues/7968
这个issue是关于Bug报告，主要涉及Multistep with n>1的失败。原因可能是由于请求的处理发生了错误。

https://github.com/vllm-project/vllm/issues/7967
这是一个关于需求升级的问题，主要涉及PyTorch XLA nightly，可能是由于当前版本存在不足或者需要新增功能导致的。

https://github.com/vllm-project/vllm/issues/7966
这个issue类型是文档修复，涉及主要对象是链接。由于链接指向错误的URL，因此导致了404页面不存在的问题。

https://github.com/vllm-project/vllm/issues/7965
这是一个bug报告，主要涉及beam search在生成时无法终止，可能是由于生成过程中的一个错误导致。

https://github.com/vllm-project/vllm/issues/7964
这个issue为功能改进提议，涉及benchmark_throughput.py脚本中添加`--async-engine`选项，主要因为使用AsyncLLMEngine接口替代LLM，并根据`disablefrontendmultiprocessing`的指定来确定是否使用独立的前端。

https://github.com/vllm-project/vllm/issues/7963
这是一个添加Ultravox模型支持多个音频块的问题。

https://github.com/vllm-project/vllm/issues/7962
这是一个关于文档请求的issue，主要涉及Speculative Decoding在vLLM中的lossless guarantees，用户提出需要关于此功能的文档信息。

https://github.com/vllm-project/vllm/issues/7961
这是一个bug报告类型的issue，主要涉及到Meta-Llama-3-8B-Instruct-FP8.yaml lm-eval测试。原因可能是该测试存在缺陷或错误，需要移除。

https://github.com/vllm-project/vllm/issues/7960
这是一个bug报告类型的issue，主要涉及vllm中加载MoE模型时发生的segfault错误，可能由于fused_moe triton kernel的问题导致。

https://github.com/vllm-project/vllm/issues/7959
该issue属于性能优化类问题，主要涉及前端与API服务器之间的客户端取消连接检查频率限制，由于当前逻辑导致每次生成令牌时都会进行检查，导致成本过高，可以通过限制每秒最多一次的方式来优化。

https://github.com/vllm-project/vllm/issues/7958
这是一个关于如何在vllm中指定特定GPU进行张量并行和管道并行的使用问题。用户想要在单个节点内使用管道并行和张量并行来提高性能，但对如何指定GPU进行这种设置有疑问。

https://github.com/vllm-project/vllm/issues/7957
这是一个关于性能优化的issue，主要涉及前端代码。原因是通过一些优化措施，预计可以获得接近100%的吞吐量提升。

https://github.com/vllm-project/vllm/issues/7956
这是一个关于性能问题的报告，主要涉及到AsyncLLMEngine和LLM class之间的性能降低，导致在特定配置下性能差异较大的现象。

https://github.com/vllm-project/vllm/issues/7954
这是一个Bugfix类型的issue，主要涉及GGUF模型在使用tensor parallelism时出现了错误的声音嵌入分片，导致计算逻辑不正确。

https://github.com/vllm-project/vllm/issues/7953
这是一个Bug报告类型的issue，主要涉及到加载safetensors模型权重时出现文件找不到的问题。原因可能是方法中的某个部分导致了文件无法被找到。

https://github.com/vllm-project/vllm/issues/7952
这是一个功能需求的issue，主要涉及到"Context Caching"的功能。由于长提示导致TTFT速度较慢，用户提出了保存kv缓存以提高速度的需求。

https://github.com/vllm-project/vllm/issues/7951
这是一个Bug报告，涉及的主要对象是Mi300x x8，由于缺少模块'vllm._C'导致无法启动openai/api_server.py，错误版本号可能是造成问题的原因。

https://github.com/vllm-project/vllm/issues/7950
这是一个特性请求，涉及主要对象是Gemmma 2模型在VLLM TPU上运行时出现的错误。原因是pallas backend不支持logit softcapping，用户希望在VLLM中实现此功能。

https://github.com/vllm-project/vllm/issues/7949
这个issue类型是用户提问，涉及的主要对象是vLLM版本升级。该用户关注同一版本代码下载后差异以及日志输出和代码行号的不同，可能导致症状为代码差异。

https://github.com/vllm-project/vllm/issues/7948
这是一个Bugfix类型的issue，主要涉及VLM（Variational Latent Modular）模块，由于多模态张量堆叠与多图像支持之间的不兼容性，导致了VLM的CI失败。

https://github.com/vllm-project/vllm/issues/7947
这个issue属于需求提出类型，主要涉及的对象是vllm中的speculative decoding功能。由于工作负载的不同，用户希望增加多个提议者的支持，以实现不同的调度策略。

https://github.com/vllm-project/vllm/issues/7946
这个issue是关于bug报告，主要涉及VLLM版本0.4.3下部署多个LORA时出错，可能是由于CUDA错误导致设备上无法执行内核图像。

https://github.com/vllm-project/vllm/issues/7945
这是一个bug报告，主要对象是CI（Continuous Integration），由于Pull Request中的错误导致了测试失败的问题。

https://github.com/vllm-project/vllm/issues/7944
该issue属于Bug报告类型，涉及主要对象为vllm中的模型推理。导致此问题的原因可能是模型在进行推理时未能正确重置，导致在切换到不同模型时仍然得到与第一个模型相关的结果。

https://github.com/vllm-project/vllm/issues/7943
这个issue属于功能需求类型，涉及LLM的前端，是关于添加Torch profiler支持的功能需求。

https://github.com/vllm-project/vllm/issues/7942
这是一个用户提出需求的issue，主要涉及的对象是vLLM模型和EXAONE 3.0模型。由于EXAONE 3.0模型是作为开放模型发布，因此用户希望将其集成到vLLM中。

https://github.com/vllm-project/vllm/issues/7941
这是一个bug报告，涉及的主要对象是代码中的budget对象。导致该问题的原因是在特定情况下，budget对象的seqs num没有被正确更新。

https://github.com/vllm-project/vllm/issues/7940
这是一个Bug报告，主要涉及的对象是vllm库。由于某些GPU相关函数不可用，导致出现了"RuntimeError: operator torchvision::nms does not exist"的错误。

https://github.com/vllm-project/vllm/issues/7939
这是一个关于bug报告的issue，主要涉及vllm和torchrun的兼容性问题，用户遇到了使用vllm时程序hang的情况。

https://github.com/vllm-project/vllm/issues/7938
这个issue是一个PR提交问题，主要涉及到使用ray[adag]替代cuda的问题，原因是markdown渲染不起作用。

https://github.com/vllm-project/vllm/issues/7937
这是一个bug报告，主要涉及的对象是VLLM项目的文档，由于当前的量化示例无法正常运行，用户提出并修复了这个问题。

https://github.com/vllm-project/vllm/issues/7936
这是一个bug报告，主要涉及vllm的api_server在版本高于0.5.3时经常崩溃的问题。导致这个问题的原因可能是软件中的某些bug或者兼容性问题。

https://github.com/vllm-project/vllm/issues/7935
这是一个性能问题的报告，主要涉及vllm中使用openAI客户端/服务器时性能下降的情况。原因可能是openAI客户端导致GPU利用率下降和吞吐量变慢。

https://github.com/vllm-project/vllm/issues/7934
这个issue是一个Bug报告，主要涉及vllm在压力测试下出现的异常延迟问题，原因可能是线程利用率达到100%，但Python堆栈为空。

https://github.com/vllm-project/vllm/issues/7933
这个issue是用户提出需求，希望支持使用SLURM启动集群而不仅仅是Ray。原因是大多数集群只支持SLURM而不是Ray。

https://github.com/vllm-project/vllm/issues/7932
这是一个bug报告，涉及的主要对象是TPU系统。这个issue之所以出现，是因为worker index与node边界不对齐。

https://github.com/vllm-project/vllm/issues/7931
这是一个关于在模型中支持通过ID加载权重的issue，主要涉及到vLLM下的UltravoxModel，原因是当前的模型在加载Whisper encoder权重时无法加载其他权重源，用户提出需要通过model ID加载额外的权重。

https://github.com/vllm-project/vllm/issues/7930
这个issue属于用户提出需求类型，主要涉及的对象是vLLM模型在支持google/madlad400-3b-mt翻译模型上的问题。导致问题的原因是vLLM当前不支持所需的T5ForConditionalGeneration架构和T5Tokenizer标记器。

https://github.com/vllm-project/vllm/issues/7929
这是一个bug报告，涉及的主要对象是mypy类型检查，由于合并导致的CI出现了新的mypy错误。

https://github.com/vllm-project/vllm/issues/7928
这个issue是一个[feature request]类型，主要涉及的对象是在vllm添加了flashinfer后端。由于缺少描述导致提交不完整，需要填写相关信息。

https://github.com/vllm-project/vllm/issues/7927
这是一个Bug报告，主要涉及Tensorizer Test中的测试回归问题。由于Markdown渲染不起作用，导致需要在此处使用原始HTML代码。

https://github.com/vllm-project/vllm/issues/7926
这是一个bug报告，涉及到ROCM硬件相关的环境变量问题。这个问题可能是由于默认环境变量设置造成的。

https://github.com/vllm-project/vllm/issues/7925
这是一个关于代码质量优化的issue，涉及主要对象为shell脚本。由于shell脚本中存在一些潜在的问题，导致需要通过添加shellcheck来进行shell脚本代码检查和优化。

https://github.com/vllm-project/vllm/issues/7924
这个issue是一个bug报告，涉及到CUDA环境下的问题，由于空的CUDA_VISIBLE_DEVICES参数导致了错误。

https://github.com/vllm-project/vllm/issues/7923
这是一个[Frontend]类型的issue，主要涉及VLM的前端，解决了在OpenAI前端中处理多个多模态项目的支持。

https://github.com/vllm-project/vllm/issues/7922
这是一个关于bug报告的issue，主要涉及添加OLMoE功能时遇到的问题，原因可能是未能成功追踪问题导致缺乏详细信息。

https://github.com/vllm-project/vllm/issues/7921
这个issue类型为功能改进，主要对象是VLLM的async postprocessor和multi-step功能。原因是为了解决运行多步执行时被阻塞的问题，提高性能。

https://github.com/vllm-project/vllm/issues/7920
这是一个Bug报告，主要涉及OpenAI服务器在高负载下出现"ZMQError Too many open files"错误，可能是由于ZeroMQ前端引起的问题。

https://github.com/vllm-project/vllm/issues/7919
该issue是一个bug报告，涉及到vLLM中的`request.max_tokens`参数设置问题，导致出现了一个断言错误。

https://github.com/vllm-project/vllm/issues/7918
这是一个Bug报告，涉及到vllm:num_requests_waiting数据在/metrics端点未发布，导致数据缺失的问题。

https://github.com/vllm-project/vllm/issues/7917
这是一个bug报告，报告了TGI版本过旧的问题，可能导致程序无法正常运行。

https://github.com/vllm-project/vllm/issues/7916
这是一个bug报告，主要涉及phi3vision模型服务部署中的`image_idx`错误导致无法将提示传递给输入处理器。

https://github.com/vllm-project/vllm/issues/7915
这是一个用户提出需求的issue， 主要对象是VLLM模型，用户尝试强制修改embedding_mode为True时出现了错误。

https://github.com/vllm-project/vllm/issues/7914
这是一个Bug报告，涉及主要对象为在装有A100 GPU的机器上使用Dockerfile 0.5.5版本时出现的错误。由于编译时出现了超时错误，可能是由于编译配置或环境不兼容等原因引发了该问题。

https://github.com/vllm-project/vllm/issues/7913
这个issue属于Bug报告类型，主要涉及VLLM的服务器在处理100次左右的请求后随机卡住而不报错的问题，用户寻求关于服务器运行异常的帮助。

https://github.com/vllm-project/vllm/issues/7912
这是一个关于功能支持问题的issue，主要涉及vllm的工具调用功能。用户提出根据文档指示无法执行特定函数调用，希望确认这一是否持续，同时提到本地llm响应存在问题。

https://github.com/vllm-project/vllm/issues/7911
这是一个用户提出需求的issue，主要涉及的对象是异步后处理器的虚拟引擎支持。

https://github.com/vllm-project/vllm/issues/7910
这是一个关于启用vLLM的内存分层功能的问题，主要涉及了不同BlockManager版本之间的兼容性和功能支持问题。

https://github.com/vllm-project/vllm/issues/7909
这是一个用户提出需求的issue，主要涉及的对象是在加载Lora时出现的错误。由于minicpmvd2_6不支持LoRA，导致用户无法在vllm端点中加载相应的finetuning脚本。

https://github.com/vllm-project/vllm/issues/7908
这是一个需求类型的issue，主要涉及的对象是关于使用vllm中的Async Mistral 7B进行离线推断时如何传递JSON content-type的问题。由于缺乏Async引擎的包装类，导致不能使用`AsyncLLMEngine`类传递模式schema。

https://github.com/vllm-project/vllm/issues/7907
这是一个bug报告，涉及vllm无法使用speculative decoding，可能由于版本升级导致的KeyError: 40错误。

https://github.com/vllm-project/vllm/issues/7906
这是一个特性需求，涉及添加控制向量支持到 vllm 的问题。

https://github.com/vllm-project/vllm/issues/7905
This is a feature request issue for adding support for the Qwen2VL model in the VLLM repository. The issue involves changes related to incorporating rotary embedding with multimodal sections and certain modifications in model execution for Qwen2VL model support.

https://github.com/vllm-project/vllm/issues/7904
这个issue是一个bug报告，涉及的主要对象是vllm库下的ray + vllm async engine。由于某种原因导致async_llm_engine的后台循环停止运行，引发了错误消息和意外的引擎关闭行为。

https://github.com/vllm-project/vllm/issues/7903
这个issue是关于bug报告，涉及到vllm安装过程中无法导入模块的问题，可能由于缺少相关库文件导致。

https://github.com/vllm-project/vllm/issues/7902
这是一个功能需求的issue，主要涉及的对象是vLLM的多模态输入处理。由于需要支持每个提示的多个{图像，音频}输入，因此需要对输入进行堆叠，以便为每个提示内的多个图像创建维度。

https://github.com/vllm-project/vllm/issues/7901
这是一个集成新模型请求的issue，涉及的主要对象是在vllm中集成EXAONE 3.0模型。由于EXAONE 3.0是一个开放模型，所以需要进行集成，以解决相关任务的繁琐性。

https://github.com/vllm-project/vllm/issues/7900
这是一个Bug报告，主要涉及对象是vllm 0.5.5版本以及相关组件，由于启用了prefix caching导致出现CUDA error导致程序崩溃。

https://github.com/vllm-project/vllm/issues/7899
这是一个Bugfix类型的issue，涉及vLLM项目中排名计算的统一性问题，导致了特定情况下测试失败。

https://github.com/vllm-project/vllm/issues/7898
这是一个优化性能的issue，主要涉及到减少TPU上的运行时开销。该问题表明需要避免Dynamo guard评估开销，以提高解码步骤的执行效率。

https://github.com/vllm-project/vllm/issues/7897
这个issue是一个CI/Build类型的问题，涉及到vLLM项目中多图像输入模型测试中重复代码清理。原因可能是为了优化代码质量和提高审查效率。

https://github.com/vllm-project/vllm/issues/7896
这是一个bug报告，涉及的主要对象是部署vllm中 llama 3.1 405b BF16 模型的一个replica无法稳定运行，并出现了NCCL error。

https://github.com/vllm-project/vllm/issues/7895
这是一个关于功能需求的issue，主要涉及vLLM是否支持前缀解码器及如何实现非因果自注意力的问题。

https://github.com/vllm-project/vllm/issues/7894
这是一个关于扩展 CUDA 图大小的 issue，主要涉及 H200 模型和如何通过添加批大小 512、768 到 CUDA 图捕获列表来实现对 kv 缓存的有效利用。原因是为了支持模型大小的效率使用。

https://github.com/vllm-project/vllm/issues/7893
该issue属于Bug报告类型，主要涉及VLLM在启动服务后进行推断时出现提示，并且无法正常进行推断。原因可能是环境配置或软件版本兼容性问题。

https://github.com/vllm-project/vllm/issues/7892
这是一个用户提出需求的issue，主要对象是前端（Frontend）的LLMEngine，用户提出增加一个`return_hidden_states`选项以返回模型的隐藏状态。

https://github.com/vllm-project/vllm/issues/7891
这个issue类型是用户提出问题，并寻求关于如何测试VLLM服务器返回时间的帮助，主要对象是VLLM下的minicpm-v-2.6服务。

https://github.com/vllm-project/vllm/issues/7890
这是一个Bug报告，主要涉及CUDA_VISIBLE_DEVICES的检测问题，导致程序无法正确检测设备。

https://github.com/vllm-project/vllm/issues/7889
这是一个关于文档更新的Issue，涉及到VLLM包中的tensorizer部分，用户提出需要在文档中添加安装`tensorizer`扩展的说明。

https://github.com/vllm-project/vllm/issues/7888
这个issue类型是添加新的cutlass配置，主要涉及的对象是针对llama70B的性能优化。这个issue由于需要为llama70B添加最佳性能的一组cutlass内核配置而提出。

https://github.com/vllm-project/vllm/issues/7887
这是一个针对bug的报告，涉及到代码中的attention group_size参数存在的问题，导致flashinfer backend在cudagraph禁用时崩溃。

https://github.com/vllm-project/vllm/issues/7886
这是一个bug报告，涉及的主要对象是ScalarType的编译和FakeScalarType的注册问题。这个bug的原因是在Pytorch版本为2.3时`ScalarType`无法编译，无法注册`FakeScalarType`，可能因为缺少相关的检查和支持。

https://github.com/vllm-project/vllm/issues/7885
这个issue是一个需求提议，主要涉及Neuron中添加对上下文长度和token生成桶(bucket)的支持。原因是为了通过环境变量控制上下文长度和token生成桶，以优化Neuron设备上的延迟性能。

https://github.com/vllm-project/vllm/issues/7884
这是一个用户试验性更改的类型，涉及主要对象为代码。由于暂时性更改尚未完全实现或测试，可能导致一些bug或问题的出现。

https://github.com/vllm-project/vllm/issues/7883
这是一个bug报告，主要涉及了vllm项目中的prefix-caching aware scheduling功能。由于在代码执行流程中未考虑prefix caching导致了性能上的低效问题。

https://github.com/vllm-project/vllm/issues/7882
这是一个Bug报告，主要涉及VLLM模型在A100 GPU上加载时出现加载失败的问题，原因可能是与CUDA版本或相关驱动问题有关。

https://github.com/vllm-project/vllm/issues/7881
这个issue是一个bug报告，主要对象是vllm项目中的output processor，由于单输出条件的错误导致了问题。

https://github.com/vllm-project/vllm/issues/7880
这是一个Bug报告，涉及的主要对象是GGUF quants of LLAMA 3.1 8B模型和tensor_parallel_size参数。该问题导致在特定条件下无法生成特殊标记符号。

https://github.com/vllm-project/vllm/issues/7879
这个issue类型是功能改进，主要对象是将自定义执行器移动到插件系统中，以减少主代码库的复杂性。

https://github.com/vllm-project/vllm/issues/7878
这是一个bug报告，该问题涉及Requests超过75k输入标记导致错误。

https://github.com/vllm-project/vllm/issues/7877
这是一个bug报告，涉及的主要对象是Kubernetes Pod重启，可能由于Request Cancelation w/ Scheduler Steps Set导致。

https://github.com/vllm-project/vllm/issues/7876
这是一个增强功能型的issue，涉及到github actions workflows的linting，主要对象是github actions workflows的配置。原因是为了避免未来意外引入错误，已经通过本地运行linting和CI自动运行linter的方式实现linting。

https://github.com/vllm-project/vllm/issues/7875
该issue类型为性能优化，涉及主要对象是vllm的speculative decoding过程。由于cuda memcpy的异常出现导致内存拷贝开销较大，最终影响了性能表现。

https://github.com/vllm-project/vllm/issues/7874
这是一个bug报告，主要涉及vLLM 0.5.4版本中enable_chunked_prefill导致吞吐量略低于0.5.3~0.5.0版本的问题。原因是优先考虑预填充导致系统抢占情况。

https://github.com/vllm-project/vllm/issues/7873
这是一个Bug报告类型的issue，主要涉及vllm在运行时出现了错误。原因是由于某个问题导致了LLM模型在初始化过程中发生了错误。

https://github.com/vllm-project/vllm/issues/7872
这是一个关于提交PR描述内容不完整的issue，主要涉及的对象是CI/Build。由于PR描述内容缺失，可能导致无法明确PR的类型和变更内容，需要补充完整的描述信息。

https://github.com/vllm-project/vllm/issues/7871
这是一个Bug报告，涉及vllm的分布式推理功能。由于环境变量配置不正确，导致无法正常进行分布式推理，产生了相关错误。

https://github.com/vllm-project/vllm/issues/7870
这个issue是关于Bug报告，主要涉及VLLM初始化时出现的加载密钥错误，导致程序无法正确运行。

https://github.com/vllm-project/vllm/issues/7869
这是一个bug报告，涉及主要对象是使用awq模型和不同的tensor-parallel-size时出现不同结果的问题。原因可能是与tensor-parallel-size参数设置有关。

https://github.com/vllm-project/vllm/issues/7868
这是一个Bug报告类型的Issue，主要涉及的对象是在Databricks Runtime 14.3 ML环境下导入`vllm`时出现了`NameError: name '_C' is not defined`错误。这可能是由于依赖项如`torch`在Databricks环境中处理或安装方式导致的。

https://github.com/vllm-project/vllm/issues/7867
这是一个bug报告，涉及的主要对象是使用vllm docker部署模型时出现的错误。由于使用了错误的finetune方法，导致生成的checkpoint_300文件夹错误，需要寻求解决方案。

https://github.com/vllm-project/vllm/issues/7866
这是一个bug报告，该问题涉及的主要对象是vllm代码中的enable_prefix_caching函数。由于PyTorch版本不匹配导致 CUDA 版本错误，可能导致 enable_prefix_caching 错误。

https://github.com/vllm-project/vllm/issues/7865
这是一个Bug报告，涉及的主要对象是无法收到设备H20上发出请求的响应。由于请求失败导致了这个Bug的出现。

https://github.com/vllm-project/vllm/issues/7864
这是一个bug报告，主要涉及到vLLM在响应用户输入时产生了异常和与Ollama不同的响应。导致这种问题的原因可能是vLLM模型或者其设置与Cerebrium之间的兼容性问题。

https://github.com/vllm-project/vllm/issues/7863
这是一个用户提出需求的issue，主要涉及对象是希望将Chexagent Multimodel整合到vLLM中。这个问题的原因是Chexagent模型的架构未被vLLM支持，用户希望得到帮助解决这一集成问题。

https://github.com/vllm-project/vllm/issues/7861
这是一个bug报告类型的issue，主要涉及LVLM中设置num_crops的问题，并且用户在初始化processor时无法找到设置num_crops的方法，可能由于缺乏相关文档或功能实现导致无法解决该问题。

https://github.com/vllm-project/vllm/issues/7860
这是一个bug报告，主要涉及的对象是在运行InternVL226B和InternVL240B模型时遇到的错误。可能是由于tokenizer配置问题导致的错误。

https://github.com/vllm-project/vllm/issues/7859
这是一个bug报告，主要涉及vllm中无法使用具有较短上下文的草稿模型。由于7B模型上下文只有32k，而目标模型需要128K上下文，导致出现无法处理分布式草稿工作者未生成令牌的情况。

https://github.com/vllm-project/vllm/issues/7858
这是一个关于GPU利用率波动的问题类型，主要涉及LLaMA 3 70B在高并发情况下GPU利用率仅为50%，用户希望了解为什么会出现这种情况。

https://github.com/vllm-project/vllm/issues/7857
该问题类型为关于GPU利用率波动的文档问题，主要涉及LLaMA 3 70B的部署。由于高并发环境下，GPU利用率平均仅为50%，可能是由于设置参数或其他未知原因导致。

https://github.com/vllm-project/vllm/issues/7856
这是一个Bug报告，涉及的主要对象是vllm中的minicpmv2_6模块。由于PyTorch只占用16GB GPU内存，但vllm发生OOM错误，可能由于内存管理或其他代码问题导致。

https://github.com/vllm-project/vllm/issues/7855
这个issue是一个Bugfix类型的报告，涉及的主要对象是vLLM模型中的base64嵌入。由于数据类型没有被强制指定为float32导致了OpenAI Python客户端返回错误大小的嵌入，进而引发了这个问题报告。

https://github.com/vllm-project/vllm/issues/7854
这是一个bug报告类型的issue，主要涉及的对象是benchark_throughput工具无法在vllm后端上工作，可能是由于用户可能的错误引起的。

https://github.com/vllm-project/vllm/issues/7853
这是一个bug报告，主要涉及的对象是custom allreduce p2p cache file generation。导致这个问题的原因是处理测试过程输出时可能会包含警告消息，需要添加标记来标识数据的起始和结束。

https://github.com/vllm-project/vllm/issues/7852
这个issue类型是Bug报告，主要涉及的对象是pynvml。由于用户仅安装nvidiamlpy而未卸载pynvml，导致出现警告提示，需要更新相关文档告知用户卸载pynvml。

https://github.com/vllm-project/vllm/issues/7851
这是一则关于优化特征解码逻辑的需求报告，主要涉及批处理的张量操作和逻辑处理，旨在提高处理各种批处理场景时的效率。

https://github.com/vllm-project/vllm/issues/7850
这个issue是一个Bug报告，涉及到vLLM的量化测试。导致该问题的原因可能是markdown渲染的问题。

https://github.com/vllm-project/vllm/issues/7849
这是一个bug报告，用户在使用Chatglm2时出现了KeyError异常。

https://github.com/vllm-project/vllm/issues/7848
这个issue是一个bug报告，主要涉及到vllm中的image processor文件下载错误。问题可能是由于多GPU环境下的下载未完成所导致的。

https://github.com/vllm-project/vllm/issues/7847
这是一个关于如何在初始化中传递`system_prompt`的问题，属于用户提出需求的类型。用户想要了解如何在启动命令中添加system_prompt，并询问是否可以通过文件传递该参数。

https://github.com/vllm-project/vllm/issues/7846
这是一个Bug报告，涉及VLLM 0.5.5在初始化分布式环境时出现的错误。原因可能是与加载模型时出现的错误密切相关。

https://github.com/vllm-project/vllm/issues/7845
这是一个Bug报告类型的Issue，主要涉及VLLM中的Prefix Caching功能。这个问题描述了启用了Prefix Caching后，第二次调用VLLM时输出与第一次运行时不同的情况，推测这并非预期行为。

https://github.com/vllm-project/vllm/issues/7844
这是一个bug报告，涉及Llama 3.18binstruct在V100上的运行问题，用户在模型加载后发起任何聊天完成请求会导致崩溃。

https://github.com/vllm-project/vllm/issues/7843
该issue属于bug报告类型，主要涉及vllm下的gguf模型在使用beamsearch参数时出现错误，并寻求潜在的修复方法。

https://github.com/vllm-project/vllm/issues/7842
这个issue属于需求提出类型，主要涉及到对模型推理过程中发生无限循环的问题，提出了需要添加`no_repeat_n_gram`参数来避免此类情况的需求。

https://github.com/vllm-project/vllm/issues/7841
这是一个关于bug报告的issue，主要涉及的对象是vllm库的使用。由于添加了`stream_options={"include_usage": True}`行导致了错误，同时用户也需要指导如何正确调用/返回/产生completion_tokens以在Gradio中显示。

https://github.com/vllm-project/vllm/issues/7840
该issue类型为bug报告，主要涉及vLLM中的Phi-3v模块，由于重复调用了_calc_hd_transform_size()导致输入特定尺寸的图像会导致程序崩溃。

https://github.com/vllm-project/vllm/issues/7839
这是一个功能改进的issue，主要涉及到更新RemoteOpenAIServer以提高效率，减少下载的模型文件数量。

https://github.com/vllm-project/vllm/issues/7838
这个issue类型是bug报告，涉及的主要对象是RemoteOpenAIServer，由于某bug引起了问题。

https://github.com/vllm-project/vllm/issues/7837
这是一个关于vllm的讨论类issue，涉及到多节点多GPU的tensor parallel和pipeline parallel推理问题。用户在询问在多节点情况下，不同卡之间是如何通信的。

https://github.com/vllm-project/vllm/issues/7836
这是一个bug报告，该问题单涉及的主要对象是CI/Build。由于在`RemoteOpenAIServer`中下载了所有HF文件，导致了性能和效率问题。

https://github.com/vllm-project/vllm/issues/7835
这是一个属于Bugfix类型的issue，主要涉及的对象是InternVL2测试中的`test_internvl.py`文件。该问题由于在InternVL模型仓库中解决了`dynamic_module`和`trust_remote_code`之间的冲突问题，因此不再需要使用`snapshot_download`来下载和加载模型。

https://github.com/vllm-project/vllm/issues/7834
这是一个bug报告，涉及到CI测试，主要解决在服务器启动时排除模型下载时间导致的错误。

https://github.com/vllm-project/vllm/issues/7833
这是一个bug报告，涉及对象是vllm下的AI21-Jamba-1.5-Mini模型，由于CUDA遇到非法内存访问导致RuntimeError。

https://github.com/vllm-project/vllm/issues/7832
这个issue是关于支持 AI21-Jamba-1.5-Large（以及mini）的问题，主要涉及的对象是对新模型支持的疑问。

https://github.com/vllm-project/vllm/issues/7831
这是一个用户提出需求的issue，主要涉及到使用slurm cluster而不使用guided decoding的用户。

https://github.com/vllm-project/vllm/issues/7830
这是一个Bug报告，主要涉及vLLM与ray后端和启用nsight导致无法获取性能指标的连接问题。

https://github.com/vllm-project/vllm/issues/7829
这是一个Bugfix类型的issue，该问题涉及到tool_calls处理，因为_fsm_states对象在每次新model_execute时被刷新，导致了返回的tool_calls格式不正确。

https://github.com/vllm-project/vllm/issues/7828
这是一个Bug报告，涉及Nemotron 340B模型无法正确生成EOS令牌的问题。

https://github.com/vllm-project/vllm/issues/7827
这是一个bug报告类型的issue，涉及主要对象是工具的CPU解析错误，可能是由于环境设置不正确导致的。

https://github.com/vllm-project/vllm/issues/7826
这是一个类型为[Model]的issue，提出了支持多个音频块/音频URL的功能增强。

https://github.com/vllm-project/vllm/issues/7825
这个issue属于代码优化类型，涉及更新压缩张量的生命周期以消除在`create_weights`中使用`prefix`，由于选择方案时需要消除`prefix`，导致需要更新压缩张量以匹配其他量化方法的生命周期。

https://github.com/vllm-project/vllm/issues/7824
这是一个Bugfix类型的issue，主要涉及修复XPU Dockerfile构建的问题。造成该问题的原因是Ubuntu 20.04不再受Triton XPU Backend支持，需要使用22.04版本以支持新的Python版本。

https://github.com/vllm-project/vllm/issues/7823
这是一个版本更新的issue，主要涉及的对象是软件的版本号。

https://github.com/vllm-project/vllm/issues/7822
这是一个bug报告，涉及的主要对象是BlockManagerV2，由于没有标记prefix缓存块为computed导致低吞吐量。

https://github.com/vllm-project/vllm/issues/7821
这是一个关于计算Llama-2模型系列低噪声困惑度估计的例子的问题，主要涉及到对KV缓存系统进行实际困惑度测量。由于KV缓存的表示精度不同，导致了不同的困惑度表现。

https://github.com/vllm-project/vllm/issues/7820
这是一个优化建议类型的issue，主要涉及模型测试的重新组织。由于超时错误，根据给出的指导重新分裂模型测试，同时将分布式基本正确性和模型测试移动到相应的文件中，以避免测试逻辑的碎片化。

https://github.com/vllm-project/vllm/issues/7819
这个issue是关于添加新的Exaone模型支持的，属于[Model]类型的问题。

https://github.com/vllm-project/vllm/issues/7817
这是一个bug报告，涉及的主要对象是vLLM，由于在调试时引入了新的日志记录器，并且打印了attention，导致CUDA错误发生。

https://github.com/vllm-project/vllm/issues/7815
这是一个Bug报告，涉及的主要对象是Llama3.170B在两个节点上运行的问题。由于升级到master版本后出现了"Error: No available node types can fulfill resource request"的错误。

https://github.com/vllm-project/vllm/issues/7814
这个issue类型是对Core部分的修改请求，主要涉及到Multi Step Scheduling中的Chunked Prefill支持，用户提出了添加Force Single Step和Ignore Prefill策略的需求。

https://github.com/vllm-project/vllm/issues/7813
这是一个bug报告，涉及对象是vLLM ROCm Docker镜像构建，原因可能是OpenAI Triton仓库移动到不同的GitHub仓库。

https://github.com/vllm-project/vllm/issues/7812
这是一个用户提出需求的类型，主要涉及vllm中的seed参数使用方式。由于用户注意到在vllm中存在两个不同的seed参数，分别在vllm.LLM和vllm.SamplingParams中，因此想了解这两者之间的区别。

https://github.com/vllm-project/vllm/issues/7811
这是一个bug报告，主要涉及vLLM在回复内容中以`assistant`开头的问题。原因可能是tokenizer未将其添加到提示中，而模型自己添加了它。

https://github.com/vllm-project/vllm/issues/7810
这是一个特定类型的PR，用于增加Intel GPU管道并行支持。

https://github.com/vllm-project/vllm/issues/7809
这是一个用户提出需求的issue，主要对象是希望LLM先回答FAQs，可能是因为用户希望首先通过LLM解决问题或获取答案。

https://github.com/vllm-project/vllm/issues/7808
这是一个关于如何在vllm中生成独立样本的提问，属于用户请教问题类型。用户主要关注如何确保生成的样本是独立的，由于不确定最佳操作方式，提出了关于如何设置参数以生成独立样本的问题。

https://github.com/vllm-project/vllm/issues/7807
这是一个Bugfix类型的issue，主要涉及到vllm中的cpu/xpu模型运行器，由于参数'is_prompt'被移除，导致benchmark_throughput函数出现TypeError错误。

https://github.com/vllm-project/vllm/issues/7806
这是一个bug报告，涉及的主要对象是添加Torch profiler支持CPU-only设备。由于之前的版本在CPU-only设备上会出现错误，这个问题解决后，profiler可以正常运行了。

https://github.com/vllm-project/vllm/issues/7805
这是一个更新请求，涉及到更新`qqq`以使用`vLLMParameters`。这个更新请求可能是为了优化算法或者适应新的参数配置方式。

https://github.com/vllm-project/vllm/issues/7804
这个issue是关于Bug报告，涉及主要对象是使用`tiiuae/falcon40b`模型进行offline_inference.py时出现的错误。由于`config.json`文件中的`tiiuae/falcon40b`缺少`num_ln_in_parallel_attn`字段，导致了该bug的产生。

https://github.com/vllm-project/vllm/issues/7803
这是一则需求提出的issue，主要涉及的对象是更新`marlin`以使用`vLLMParameters`，添加了运行Marlin通道分组量化模型的权重加载TP测试。

https://github.com/vllm-project/vllm/issues/7802
这是一个bug报告，涉及的主要对象是vllm模块。由于缺少"openai.types"模块导致了ModuleNotFoundError异常，用户在此寻求帮助解决此问题。

https://github.com/vllm-project/vllm/issues/7801
这是一个bug报告，涉及的主要对象是mistral-large模型，由于NCCL相关错误导致无法运行。

https://github.com/vllm-project/vllm/issues/7800
这个issue是关于bug报告的，主要涉及了在运行mistral-large时出现与NCCL相关的错误。这可能是由于环境设置或命令运行错误导致的。

https://github.com/vllm-project/vllm/issues/7799
这是一个用户提出需求的issue，主要涉及请求支持deepseek-gptq版本。由于缺乏对deepseek-gptq版本的支持，用户提出需要该版本的支持。

https://github.com/vllm-project/vllm/issues/7798
这是一个功能需求提议，涉及主要对象为优化内核性能，通过启用FlashInfer后端以提高FP8 KV Cache的性能。

https://github.com/vllm-project/vllm/issues/7797
这是一个Bug报告，涉及的主要对象是vLLM phi-3-vision服务器。由于在发送第二个请求时出现错误，导致了"Attempted to assign 1 x 2509 = 2509 image tokens to 0 placeholders"的错误信息。

https://github.com/vllm-project/vllm/issues/7796
这是一个bug报告，该问题单涉及的主要对象是torch.compile的性能优化。由于只在第一次运行时进行性能分析来确定kvcache的空间，之后不再运行性能分析，导致编译只会为无用代码添加保护和代码缓存，进而增加dynamp的开销。

https://github.com/vllm-project/vllm/issues/7795
这是一个用户提出需求的issue，主要涉及vllm下的vision-language model，用户想要在模型forward完成后hook一些特征，但不清楚如何获取这些特征。

https://github.com/vllm-project/vllm/issues/7794
这个issue类型是bug报告，主要对象是triton import warning 出现的错误拼写问题导致用户提出了修正。

https://github.com/vllm-project/vllm/issues/7793
这是一个Bug报告，涉及的主要对象是FP8 Marlin库。这个Issue是由于CUDA内存不足导致的OutOfMemoryError。

https://github.com/vllm-project/vllm/issues/7792
这是一个用户提出需求的issue，主要涉及VLLM不支持Snowflake Arctic Embed家族的嵌入模型。原因是目前只有Mistral嵌入模型得到支持。

https://github.com/vllm-project/vllm/issues/7791
这是一个bug报告，主要对象是vLLM中的distributed executor，由于默认使用multiprocessing导致crashes，用户希望解决这个问题。

https://github.com/vllm-project/vllm/issues/7790
这个issue类型是功能需求，该问题单涉及的主要对象是神经魔术预量化W8A8/16模型的TPU后端。该需求可能是为了实现在TPU后端上启用神经魔术预量化W8A8/16模型的检查点而提交。

https://github.com/vllm-project/vllm/issues/7789
这是一个改进提案，主要对象是LLM引擎，旨在为LLM引擎添加多步骤支持。

https://github.com/vllm-project/vllm/issues/7788
这个issue是关于文档修复的，涉及的主要对象是CC([Model] Add UltravoxModel和UltravoxConfig，由于文档格式错误和复制粘贴问题导致了错误的渲染效果。

https://github.com/vllm-project/vllm/issues/7787
这是一个Bug报告，涉及的主要对象是vLLM下的一个issue。由于请求发送到服务器时导致了服务器崩溃，出现了断言错误："Does not support prefix-enabled attention"。

https://github.com/vllm-project/vllm/issues/7786
这是一个bug报告的issue，主要涉及前端的错误抑制。由于取消了大量请求中有其他请求正在进行时，后端响应终止请求会耗费很长时间，希望通过超时抑制来消除这种情况。

https://github.com/vllm-project/vllm/issues/7785
这是一个Bug报告类型的Issue，主要涉及安装vllm时出现的构建错误。该问题的症状是在构建vllm时遇到了错误，错误提示中提到了缺少numpy模块和CMake配置不完整等原因。

https://github.com/vllm-project/vllm/issues/7784
这是一个bug报告，涉及到vllm工具中关于`vllm serve --load-format`参数选项的问题。由于原来的选项列表没有及时更新导致用户无法加载sharded_state模型，需要直接引用LoadFormat枚举来更新选项。

https://github.com/vllm-project/vllm/issues/7783
这是一个功能需求的issue，涉及的主要对象是为Phi-3-vision模型添加支持多图片输入的功能。

https://github.com/vllm-project/vllm/issues/7782
这是一个bug报告，主要涉及的对象是vllm的docker容器。由于版本0.5.1的docker容器中的 /metrics 端点显示的信息较少，缺少一些关于vllm服务器的数据，例如 'gpu_cache_usage_perc' metric，导致了这个问题的出现。

https://github.com/vllm-project/vllm/issues/7781
这是关于无法加载视觉模型`microsoft/Phi-3.5-vision-instruct`的Bug报告。

https://github.com/vllm-project/vllm/issues/7780
这是一个bug报告，主要涉及文件存在性检查问题，导致加载模型时出现数值错误(ValueError)。

https://github.com/vllm-project/vllm/issues/7779
这是一个bug报告，主要涉及vllm在线模式下的logprob变化的问题。原因是在线模式和离线模式在输出相同的情况下logprob有所不同，希望在多次运行中得到相同的输出。

https://github.com/vllm-project/vllm/issues/7778
这是一个关于功能需求的issue，主要涉及LLM在解码阶段使用json格式输出时高吞吐量的问题，可能是由于guided_json参数引起，导致了生成吞吐量低的症状。

https://github.com/vllm-project/vllm/issues/7777
这是一个关于使用fp8 cutlass scaled_mm核心导致输出错误的Bug报告，涉及主要对象为vllm项目。因为使用了新的fp8 cutlass scaled_mm核心，导致输出异常，token ids也不符合预期。

https://github.com/vllm-project/vllm/issues/7776
这是一个讨论问题类型的issue，主要涉及vLLM中的依赖关系，通过对依赖关系的优化来改善分布式测试的相关性。

https://github.com/vllm-project/vllm/issues/7775
这是一个bug报告类型的issue，主要涉及vllm在8 * A800上推理时出现通信异常的问题，可能由于NCCL通信问题导致。

https://github.com/vllm-project/vllm/issues/7774
这是一个bug报告类型的issue，涉及到VLLM库中的guide decoding功能，在调用openai api时会导致参数错误，可能是由于guide decoding逻辑导致的。

https://github.com/vllm-project/vllm/issues/7772
这是一个Bug报告，主要涉及PyTorch的连接被对端关闭的问题。由于连接被对端关闭，导致程序出现异常。

https://github.com/vllm-project/vllm/issues/7771
这个issue类型是用户提出需求，主要涉及的对象是`JetMoE`模型支持。用户提出了关于增加`JetMoE`模型支持的需求。

https://github.com/vllm-project/vllm/issues/7770
这是一个bug报告类型的issue，主要涉及的对象是vllm中的llama3生成器。由于设置temp=1、top_k=1和random seed后，仍无法强制生成固定响应，导致用户提出如何强制生成固定响应的问题。

https://github.com/vllm-project/vllm/issues/7769
这个issue类型是用户提出需求，主要对象是在vLLM文档中增加更新日志和常见问题解答。这个需求是由于用户希望了解vLLM不同版本之间的变化以及获取常见问题的解答。

https://github.com/vllm-project/vllm/issues/7768
这是一个Bug报告，主要涉及的对象是GPU内存利用率不足的情况。用户指出在使用特定模型时，GPU内存利用率无法达到期望值，即使设置了相应的参数也无效。

https://github.com/vllm-project/vllm/issues/7767
这是一个Bug报告，主要涉及vllm中的本地批量推理模式在mistral-7B上导致OOM错误。该问题可能是由于资源占用过高导致内存溢出错误。

https://github.com/vllm-project/vllm/issues/7766
这是一个特性更新的issue，主要涉及了MoE权重加载和Marlin Fused MoE Kernel的添加。由于扩展了权重加载支持和添加了新的Kernel，可能是为了改进模型性能或功能。

https://github.com/vllm-project/vllm/issues/7765
这是一个BugFix类型的issue，涉及主要对象为Kernel。因AMD构建存在问题，需要回滚融合的moe kernel PR。

https://github.com/vllm-project/vllm/issues/7764
这是一个关于BUG的问题，在AMD构建中引起故障。

https://github.com/vllm-project/vllm/issues/7763
这是一个bug报告，主要涉及的对象是TPU后端。由于在TP > 1情况下初始化TPU运行时导致了死锁，需要修复。

https://github.com/vllm-project/vllm/issues/7762
这是一个特性更新类的issue，涉及的主要对象是`gptq_marlin_24`，由于需要更新quantization来使用`vLLMParameters`。

https://github.com/vllm-project/vllm/issues/7761
这个issue类型是建议增加功能，主要涉及将Llama405B模型包含在夜间基准测试中。原因是成本，由于需要1624个H100s，可能导致Akash.Network考虑提供基础设施。

https://github.com/vllm-project/vllm/issues/7760
这个issue类型是一个PR提交前的Checklist，主要是关于提交代码时需要满足的条件。用户提交这个issue是为了寻求PR提交前的指导和帮助。这个issue主要涉及vLLM项目的PR提交标准和流程。

https://github.com/vllm-project/vllm/issues/7759
这是一个功能需求的issue，主要涉及添加更多的百分位数和延迟信息，以便更好地评估吞吐量性能。

https://github.com/vllm-project/vllm/issues/7758
该issue类型为功能开发/问题解决，涉及到使用NATS消费任务并将结果返回的功能开发。

https://github.com/vllm-project/vllm/issues/7757
这是一个bug报告，涉及主要对象是vLLM项目中的machete构建功能。由于machete源代码应该不适用于CUDA版本低于12.0，所以这导致了一个安装错误，用户提出了需要修复这个问题的帮助。

https://github.com/vllm-project/vllm/issues/7756
这个issue是一个bug报告，主要涉及的对象是关于VLLM_HOST_IP设置的错误，可能是由于配置问题导致同一VLLM主机IP设置错误。

https://github.com/vllm-project/vllm/issues/7755
这是一个功能需求的issue，主要涉及到为torch添加自定义操作以进行all_reduce。导致问题的原因是当前实现中混合了inplace和outofplace内核，导致无法在torch编译中正常运行。

https://github.com/vllm-project/vllm/issues/7754
该issue属于bug报告类型，涉及支持新的视觉语言模型（https://huggingface.co/OpenGVLab/InternVL2-26B），由于模型类型错误导致出现KeyError异常，最终导致服务器未能在指定时间内回复。

https://github.com/vllm-project/vllm/issues/7753
这是一个性能优化类的issue，主要涉及chunked prefill和prefix caching的同时启用。原因是为了简化处理部分块时的逻辑。

https://github.com/vllm-project/vllm/issues/7752
该issue为用户提出需求，主要涉及VLLM与`Formatron`的集成，并由于Python解释器开销过大而希望通过集成Rust后端的`Formatron`来改善性能。

https://github.com/vllm-project/vllm/issues/7751
这是一个关于性能问题的issue，主要涉及Phi-3-vision在高吞吐量配置下的并发调用效率问题，可能由于配置或使用问题导致批处理并行性无法正常工作，进而影响到总体推理时间。

https://github.com/vllm-project/vllm/issues/7750
该issue是一个文档修复类型的问题，主要涉及到`add/remove_logger`功能，在该section中Markdown渲染无法正常显示，因此需要使用原始的HTML标记来解决。

https://github.com/vllm-project/vllm/issues/7749
该issue是一个bug报告，涉及主要对象是前端代码。由于在执行`do_log_stats`时打开了新的`socket`，而在此同时调用了`RPCClient.cleanup()`，导致在已关闭的上下文中调用`context.socket`引发了`ZMQError`异常。

https://github.com/vllm-project/vllm/issues/7748
这是一个性能优化的提案，但用户在比较llama38binstruct与mlpspeculator版本的性能时未看到任何提升，询问原因并询问关于GPU内存释放的问题。

https://github.com/vllm-project/vllm/issues/7746
这是一个Bug报告，主要对象是vllm项目下的api_server。由于发送空提示导致服务器崩溃，需要修复该bug。

https://github.com/vllm-project/vllm/issues/7745
这个issue是bug报告类型，涉及的主要对象是vllm项目中的llama 3.1模型。由于使用的是旧的模型checkpoint，导致了运行时错误，需要重新下载整个模型来解决问题。

https://github.com/vllm-project/vllm/issues/7744
这个issue属于安装问题，涉及的主要对象是vLLM的源码构建。出现这个问题的原因是无法成功构建vLLM源码，可能由于最近的更改导致了此错误。

https://github.com/vllm-project/vllm/issues/7743
这是一个关于功能改进的issue，主要涉及到了异步后处理和多步执行的结合。

https://github.com/vllm-project/vllm/issues/7742
这是一个Bug报告。该问题涉及到vLLM库中的MLP Speculator在请求Prompt Logprobs时导致服务器崩溃的问题。

https://github.com/vllm-project/vllm/issues/7741
这是一个用户提出需求的 issue，主要涉及使用vllm时需要等待每个预测响应，可能是由于需求使用先前的预测结果来补充后续预测而导致。

https://github.com/vllm-project/vllm/issues/7740
这个issue是一个特性请求，主要涉及vllm不支持phi3.5模型的多图像功能，导致用户提出需要改进的需求。

https://github.com/vllm-project/vllm/issues/7739
这是一个feature请求issue，主要涉及的对象是将Mistral Tokenization集成到模型中，以提高鲁棒性和聊天编码，在使用官方Mistral模型时出现的编码/解码问题和聊天编码问题可能是由于缺乏测试和兼容性所致。

https://github.com/vllm-project/vllm/issues/7738
这个issue属于用户询问问题类型，主要涉及vllm在quantization设置上的疑惑。原因是用户想确认使用的quantization类型，并咨询是否有支持8bit quantization。

https://github.com/vllm-project/vllm/issues/7737
这个issue属于用户提出需求类型，主要对象是vllm的cli（命令行界面），用户提出为什么vllm cli没有提供config参数的问题。

https://github.com/vllm-project/vllm/issues/7736
这是一个用户提出需求的类型的issue，主要涉及vLLM模型中如何在推理过程中访问注意力矩阵或者KVCache，用户想了解是否有类似transformers包中output_attentions=True参数的选项。

https://github.com/vllm-project/vllm/issues/7735
这个issue是一个bug报告，涉及的主要对象是CPU backend。由于缺少`mm_limits`初始化，导致CPU VLM支持出现故障。

https://github.com/vllm-project/vllm/issues/7734
这是一个bug报告，涉及到vLLM项目中chat方法中一个参数名字的拼写错误。

https://github.com/vllm-project/vllm/issues/7733
这是一个空白的WIP（work in progress）类型的issue，具体问题或需求尚未填写。

https://github.com/vllm-project/vllm/issues/7732
这是一个bug报告，主要涉及的对象是vLLM server以及stabilityai/stablelm-3b-4e1t模型，在使用dtype为FP16或BF16在CPU上进行推理时导致容器被终止，而当使用dtype为FP32时则正常工作。

https://github.com/vllm-project/vllm/issues/7731
这是一个bug报告，主要涉及的对象是vllm模型。由于不支持特定的模型架构（'PhiMoEForCausalLM'），导致数值错误（ValueError）的问题。

https://github.com/vllm-project/vllm/issues/7730
这是一个bug报告，涉及的主要对象是在clean build环境下安装依赖并且无法将PYTHONPATH传递给CMake和后续的Python调用。

https://github.com/vllm-project/vllm/issues/7729
这是一个Model类型的issue，主要涉及为vLLM添加对MSFT Phi-3.5-MoE模型的支持。出现该问题的原因是在进行模型推断时出现了错误。

https://github.com/vllm-project/vllm/issues/7728
这是一个bug报告，主要涉及 vllm 的使用环境和可能出现的硬件故障。由于硬件计算使用过程中可能存在CUDA核心编译问题，导致服务器硬件关机或电源供电异常。

https://github.com/vllm-project/vllm/issues/7727
这是一个bug报告，主要涉及的对象是在vllm环境中运行MiniCPMV2_6int4模型时出现了加载模型权重错误的问题。

https://github.com/vllm-project/vllm/issues/7726
这个issue是一个Bug报告，主要涉及vLLM 0.5.4和Llama 3.1，由于频繁的非确定性导致不同输出。

https://github.com/vllm-project/vllm/issues/7725
这是一个用户提出需求的issue，主要涉及的对象是BitNet模型支持。由于开源BitNet实现提供了独特的tokenizer和模型架构，需要引入一个新的BitNet模型来解决问题。

https://github.com/vllm-project/vllm/issues/7724
这是一个用户提出需求的类型issue，主要涉及使用FP8或其他量化算法对Minicpmv2_6进行推断，询问如何在vllm中实现，并寻求哪种量化算法具有最佳的吞吐量提升。

https://github.com/vllm-project/vllm/issues/7723
这是一个bug报告，涉及的主要对象是在构建过程中未在`pyproject.toml`中同步`jinja2`，导致在干净环境中出现构建错误。

https://github.com/vllm-project/vllm/issues/7722
这是一个Bug报告，涉及的主要对象是vllm的CPU推理功能。该问题可能是由于CPU不支持AVX512，所以在推理过程中发生了错误。

https://github.com/vllm-project/vllm/issues/7721
这是一个bug报告，涉及的主要对象是vllm库。由于GPU内存不足导致出现了CUDA内存溢出错误。

https://github.com/vllm-project/vllm/issues/7720
这是一个测试请求类型的issue，主要对象是代码。由于没有具体内容提及，无法确定具体原因导致了什么样症状的问题。

https://github.com/vllm-project/vllm/issues/7719
这个issue属于文档类型，涉及主要对象是多模态语言模型。导致这个issue的原因是markdown渲染无法正常工作，需要使用原始的HTML。

https://github.com/vllm-project/vllm/issues/7718
这是一个Bug报告类型的issue，主要涉及对象是vllm环境，用户报告了无法加载microsoft/Phi-3.5-vision-instruct的错误。

https://github.com/vllm-project/vllm/issues/7717
这是一个关于bug报告的issue，主要涉及VLLM的自动AWQ量化示例无法运行，可能由于数据集版本不匹配导致无法找到所需的文件。

https://github.com/vllm-project/vllm/issues/7716
这个issue属于bug报告，主要涉及前端的改进启动失败用户体验，由于RPCServer在初始化过程中失败导致抛出RuntimeError和TimeoutError，使得用户面对的错误信息不易理解。

https://github.com/vllm-project/vllm/issues/7715
这是一个bug报告类型的issue，涉及到vLLM的GPU转CPU冗余传输问题。由于markdown渲染问题，导致PR标题类型无法正常展示，需要通过HTML进行处理。

https://github.com/vllm-project/vllm/issues/7714
这是一个bug报告，主要涉及neuralmagic的quants和fp8 cache在ampere环境下无法正常工作的问题，可能是由于fp8 cache设置为ampere不支持的dtype所导致。

https://github.com/vllm-project/vllm/issues/7713
这是一个Bug报告，涉及主要对象是vLLM中的MixtralForCausalLM架构和mistralai/Mixtral-8x7B-Instruct-v0.1模型。由于某种原因导致在加载模型时出现错误。

https://github.com/vllm-project/vllm/issues/7712
这个issue是一个[Hardware][Intel GPU]的bug报告，涉及到xpu_model_runner和xpu tensor parallel的问题。由于代码重构和修复的不完整导致了markdown渲染问题。

https://github.com/vllm-project/vllm/issues/7711
这个issue属于bug报告，涉及到端口使用。由于绑定已被使用的端口，但无法找到该进程，导致了CI失败。

https://github.com/vllm-project/vllm/issues/7710
这个issue是一个Bugfix类型的问题，涉及的主要对象是Phi-3.5-vision-instruct模型。这个问题是由于num_crops配置未正确加载导致的错误。

https://github.com/vllm-project/vllm/issues/7709
这个issue类型是bug报告，主要涉及cpu offload测试，由于多次cudagraph捕获和从cpu到gpu的模型权重传输导致测试失败。

https://github.com/vllm-project/vllm/issues/7708
这是一个关于修复ShardedStateLoader在vllm fp8量化中的问题的Bug报告。

https://github.com/vllm-project/vllm/issues/7707
这是一个关于修复`worker_class_fn` API在子类化执行器时的一个问题的Bug报告。

https://github.com/vllm-project/vllm/issues/7706
这是一个功能改进类的issue，主要涉及规范解码草案模型的默认最大长度设置问题。

https://github.com/vllm-project/vllm/issues/7705
这个issue类型是需求提出，主要涉及的对象是Dockerfile的优化和参数传递，用户提出了希望清理整理Dockerfile、优化依赖安装和传递Python版本等需求。

https://github.com/vllm-project/vllm/issues/7704
这是一个 bug 报告类型的 issue，主要涉及的对象是 vllm 中的 mixtral 8x7b 模型，由于无法运行该模型导致了报错。

https://github.com/vllm-project/vllm/issues/7703
这个issue类型是Bug报告，主要涉及的对象是在使用多步操作时没有使用异步引擎导致的错误。

https://github.com/vllm-project/vllm/issues/7702
这是一个用户提出需求的issue，主要涉及vllm和tr的应用对齐问题，用户询问在使用过程中是否需要对prompt进行特殊处理，可能是由于文档和实际应用中的格式要求不一致引起的。

https://github.com/vllm-project/vllm/issues/7701
这个issue类型是关于功能增强的需求，主要涉及对象是Machete kernels和GPTQMarlin、CompressedTensorsWNA16，用户提出了添加Machete kernels作为后端内核的请求以及性能优化的相关需求。

https://github.com/vllm-project/vllm/issues/7700
这是一个用户需求类型的问题，主要涉及对象是vllm推断请求的终止操作。该问题由于无法停止推断请求而导致了症状。

https://github.com/vllm-project/vllm/issues/7699
这是一个Bug报告，涉及vLLM在启动多节点集群时崩溃的问题，由于NCCL错误导致初始化失败，缺乏详细错误信息。

https://github.com/vllm-project/vllm/issues/7698
这是一个bug报告，主要涉及AsyncLLMEngine中的async generator和异常处理逻辑的问题，由于逻辑消费请求的队列只抛出异常实例而不是异常类，导致了CancelledError类实例没有被正确抛出，同时引发了生成器在引擎中未消耗所有响应输出就意外退出的问题。

https://github.com/vllm-project/vllm/issues/7697
这是一个RFC（Request For Comments）类型的issue，主要涉及vLLM的内存分层功能。由于当前的vLLM很少使用第二存储层（DRAM），导致了对于包含共享prompt的请求进行重新计算的低效问题。

https://github.com/vllm-project/vllm/issues/7696
这个issue是有关于Kernel的改进，主要涉及支持在并行中支持预填和解码阶段的注意力核心，以及使用CUDA多流来启用分块预填时支持并行操作。原因是为了在启用分块预填标志时更好地利用GPU资源。

https://github.com/vllm-project/vllm/issues/7695
这是一个关于添加`jinja2`作为显式构建要求的需求，涉及到Machete核心的构建过程中产生的问题。

https://github.com/vllm-project/vllm/issues/7694
这是一个用户提出需求的issue，主要涉及vllm在spot instances上进行批量推断时如何定期备份处理结果，提出了如何避免由于instance被驱逐而导致进度丢失的问题。

https://github.com/vllm-project/vllm/issues/7693
这是一个Bug报告，涉及主要对象是VLLM服务器设置中的tools和tool_choice参数。原因可能是处理这些参数时的一致性问题，导致了在有/无这些参数的情况下产生不一致的输出表现。

https://github.com/vllm-project/vllm/issues/7692
这是一个建议（RFC）类型的issue，主要涉及将Ascend NPU作为一个新的后端添加到VLLM中。由于许多用户希望在Ascend NPU上使用VLLM，因此提出了这个建议。

https://github.com/vllm-project/vllm/issues/7691
这个issue是一个Bug报告，主要涉及的对象是vLLM中的新模型PhiMoE。这个bug的症状是当输入的长度达到一定值N时（对于Phi模型，N等于原始位置嵌入的长度4096），使用LongRoPE的模型会在第N个token后产生垃圾数据。

https://github.com/vllm-project/vllm/issues/7690
该issue类型为功能需求提议，涉及主要对象为vllm模型的加载和预填充。由于LLM推理请求每秒不固定，需要按需启动vllm引擎，因此需要重叠模型加载和预填充，尤其是对于加载时间较长的大型模型。

https://github.com/vllm-project/vllm/issues/7689
这个issue是一个bug报告，主要涉及的对象是Qwen2 GGUF 模型。在运行推断过程中出现错误，作者寻求解决方案。

https://github.com/vllm-project/vllm/issues/7687
该issue是关于更新文档的，主要对象是OpenVINO。问题类型是文档修改。原因是更新了文档内容且修复了markdown渲染不起作用的问题。

https://github.com/vllm-project/vllm/issues/7686
这是一个软件开发过程中的issue，类型为代码重构，主要涉及到`xpu_worker`和 `xpu_executor`。原因可能是为了提高代码质量和审查效率进行重构。

https://github.com/vllm-project/vllm/issues/7685
这是一个bug报告，涉及的主要对象是Intel GPU（XPU），由于Intel GPU仍在使用pytorch 2.1，不支持punica kernel中使用的`torch.library.custom_op`，因此出现了这个问题。

https://github.com/vllm-project/vllm/issues/7684
这是一个用户提出需求的issue，主要涉及的对象是vllm下的Vision Language Models。由于当前vllm仅支持语言模型而不支持视觉模型，导致出现了Pipeline parallelism的NotImplementedError错误，用户希望增加对视觉语言模型的支持以实现更高效的工作负载扩展和模型性能维护。

https://github.com/vllm-project/vllm/issues/7683
这是一个关于测试的需求提出类型的issue，主要涉及vllm项目的后端配置和CI服务相关，用户提出了如何添加OpenVINO后端测试的问题。

https://github.com/vllm-project/vllm/issues/7681
这是一个Bug报告，主要涉及VLLM中当`echo=True`时，会在最后一条消息后附加聊天模板(`assistant`)的问题。由于使用`echo=True`参数时期望的输出并非如此。

https://github.com/vllm-project/vllm/issues/7680
这是一个关于性能对比的issue，涉及的主要对象是vllm模型。由于量化后性能没有提升，导致最大响应时间仍然较高，用户提出了性能不如预期的问题。

https://github.com/vllm-project/vllm/issues/7679
这是一个bug报告，涉及主要对象是vLLM Docker容器。该问题由于缺少OpenTelemetry包而导致配置otlp_traces_endpoint时出错。

https://github.com/vllm-project/vllm/issues/7678
这是一个bug报告，主要涉及的对象是vllm库。该问题可能是由于缺少对FATRelu的支持而导致无法部署`openbmb/ProSparseMiniCPM1Bsft`，从而引发错误。

https://github.com/vllm-project/vllm/issues/7677
这是一个用户请教问题类型的issue，主要涉及到如何在vllm中使用guided_decoding_backend功能。用户询问如何实现在vllm中运行特定模型的推理，表明对集成vllm的方式缺乏了解。

https://github.com/vllm-project/vllm/issues/7675
该issue类型为bug报告，涉及pynvml用户；由于存在问题在https://github.com/vllmproject/vllm/pull/7649中引发警告。

https://github.com/vllm-project/vllm/issues/7674
这是一个用户提出需求的类型，该问题单涉及的主要对象是添加nvidia相关的库到collect env。可能由于当前的环境中缺少nvidia相关的库，用户需要将其添加进来。

https://github.com/vllm-project/vllm/issues/7673
这个issue属于[Core]类型，主要涉及到对executor类的重构，目的是使继承GPUExecutor更容易。这个问题的原因是为了让XPUExecutor能够共享GPUExecutor的大部分代码，从而简化继承过程。

https://github.com/vllm-project/vllm/issues/7672
这个issue类型是Bugfix，涉及的主要对象是vllm下的glm4v模型。由于之前的量化方法导致的不准确性，特别影响了glm4v模型的性能表现，需要修复这一问题。

https://github.com/vllm-project/vllm/issues/7671
这是一个bug报告，涉及到Streaming API的Abort功能无法正常工作的问题。原因可能是在启用Streaming时，Abort请求无法终止操作，导致无法成功中止请求。

https://github.com/vllm-project/vllm/issues/7670
这个issue属于bug报告类型，主要涉及的对象是XPU的自定义操作（custom op）。由于forward_xpu未被实现，导致需要回退到本地实现。

https://github.com/vllm-project/vllm/issues/7669
这是一个bug报告 issue，主要涉及Ray v2.23、llavanext以及在进行500批量推理时出现的图像标记和占位符不匹配的问题。

https://github.com/vllm-project/vllm/issues/7668
这是一个Bug报告，主要涉及vllm环境下使用llama-3.1-70b-instruct进行推断时，输入超过8k个标记将导致无限输出的问题。这个问题的症状是输出文本不断重复，并无法停止直到达到最大标记限制。

https://github.com/vllm-project/vllm/issues/7667
这是一个关于持续集成的功能需求。

https://github.com/vllm-project/vllm/issues/7666
这是一个提出需求的issue，主要涉及vLLM中前端和核心部分的logits processor构建，要求通过相关请求参数在`LLMEngine`中构建标准的logits processors，以提高性能并简化用户体验。

https://github.com/vllm-project/vllm/issues/7665
这是一个Bug修复类型的Issue，主要涉及到程序中的参数类型设置问题，导致在特定情况下无法正确设置参数数值。

https://github.com/vllm-project/vllm/issues/7664
这是一个Bug报告，主要涉及的对象是vLLM logger，原因是由于某些条件下存在的bug导致了现有的loggers被禁用。

https://github.com/vllm-project/vllm/issues/7663
这个issue是一个特性添加类型，主要涉及到添加了一个名为`AttentionState`的抽象概念。由于需要持续重复使用attention后端特定对象，因此需要引入这个抽象以消除模型运行器中对于flashinfer的特殊处理。

https://github.com/vllm-project/vllm/issues/7662
这个issue是一个用户提出需求的类型，主要涉及的对象是vllm的GGUF quantization with tensor parallelism功能。由于当前版本不支持GGUF quantization与tensor parallelism的结合，导致用户遇到数值错误的问题并请求增加该功能。

https://github.com/vllm-project/vllm/issues/7661
这个issue属于功能新增请求，主要涉及的对象是ScalarType。由于需要支持无符号零的NAN表示，以及对`float8_e4m3fnuz`的支持，可能是由于当前版本无法满足特定需求而导致的。

https://github.com/vllm-project/vllm/issues/7660
这是一个 bug 报告，主要涉及的对象是 TPU，由于冗余的输入张量克隆导致的问题。

https://github.com/vllm-project/vllm/issues/7659
这个issue是一个bug报告类型，涉及的主要对象是文档构建过程。由于添加了`msgspec`导致文档构建错误，需要寻找一个永久性的解决方案。

https://github.com/vllm-project/vllm/issues/7658
这是一个关于Kernel的issue，它的类型是[Kernel] Add opcheck tests for punica kernels，主要涉及添加torch.library.opcheck tests for punica custom ops。

https://github.com/vllm-project/vllm/issues/7657
这是一个关于虚拟办公时间的通知发布类型的issue，主要对象是参与vLLM项目的开发者和用户，用户可能寻求关于项目内容和贡献方式的帮助。

https://github.com/vllm-project/vllm/issues/7656
这是一个用户提出的需求。该问题单涉及的主要对象是vLLM。由于OpenAI现在支持JSONSchema，用户希望vLLM也能支持`response_format.type == 'json_schema'`，以便在两种格式之间轻松切换。

https://github.com/vllm-project/vllm/issues/7655
这是一个bug报告，主要涉及到在WSL中使用vLLM时出现CUDA内存不足错误的问题，可能是由于显卡容量不足导致。

https://github.com/vllm-project/vllm/issues/7654
这是一个类型为[Frontend]的issue，涉及将OpenAI协议中的json_schema功能与vLLM整合，由于markdown渲染不起作用，需要使用raw html。

https://github.com/vllm-project/vllm/issues/7653
这个issue属于Bug报告类型，主要涉及的对象是使用0.5.4版本的vllm进行大规模请求时出现错误。由于大量请求导致EndOfStream异常，触发了一系列异常栈输出，造成API请求出错。

https://github.com/vllm-project/vllm/issues/7652
这是一个bug报告类型的issue，主要涉及的对象是vllm项目中的Multistep组件。由于某些pythonization步骤被跳过，导致了logprobs计算和使用的问题。

https://github.com/vllm-project/vllm/issues/7651
这个issue类型是需求提出，涉及主要对象是将mamba_ssm和causal_conv1d kernels迁移到vLLM，由于多次尝试将改进推送到相关仓库失败，导致需要迁移代码以加快内核的开发/优化进度。

https://github.com/vllm-project/vllm/issues/7650
这是一个Bug报告，主要涉及vLLM在Gemma2模型推理时产生了不正确的响应。由于安装的attention后端不当，导致模型生成的响应与提示内容不相关。

https://github.com/vllm-project/vllm/issues/7649
这是一个bug报告，涉及主要对象是vllm下的fused_moe模型。原因可能是编码错误导致打印出的设备名称是bytes而不是字符串。

https://github.com/vllm-project/vllm/issues/7648
这是一个功能需求的issue，主要涉及的对象是`LLM`类，由于需要支持流式处理，所以对`.generate()`方法进行了修改。

https://github.com/vllm-project/vllm/issues/7647
这是一个[杂项]类型的issue，主要涉及重构一个用于CPU后端的Dockerfile。原因可能是为了简化安装流程和优化依赖管理。

https://github.com/vllm-project/vllm/issues/7646
这是一个用户提出需求的issue，主要对象是LLAVA，用户提出需要支持chunked_prefill以提高RLHF中生成步骤的吞吐量。

https://github.com/vllm-project/vllm/issues/7645
这是一个bug报告，涉及的主要对象是vLLM模型。导致这个问题的原因可能是模型在使用不同数量的GPU时产生不一致的输出。

https://github.com/vllm-project/vllm/issues/7644
这是一个bug报告类型的issue，涉及到XPU构建时不需要构建core_ext模块的问题。该问题可能由于XPU不需要构建core_ext模块导致的。

https://github.com/vllm-project/vllm/issues/7643
这是一个PR提交说明，涉及vLLM中SPMD模式支持和性能优化。

https://github.com/vllm-project/vllm/issues/7642
这个issue类型是技术改进，主要对象是Gemmar模型的加速。

https://github.com/vllm-project/vllm/issues/7641
这是一个功能性需求的issue，涉及对象是在run_batch API中发布Prometheus指标，问题是由于run_batch不在api_server中创建Prometheus客户端，导致无法发布Prometheus指标。

https://github.com/vllm-project/vllm/issues/7640
这个issue是bug报告类型，主要涉及的对象是run_batch logger。这个问题的产生是因为run_batch logger与vllm.entrypoints.logger发生冲突，导致run_batch logger无法正常工作。

https://github.com/vllm-project/vllm/issues/7638
这是一个更改请求类型的issue，涉及主要对象是Gemini RoPE模块。由于Gemini RoPE实际上与普通的RotaryEmbedding相同，导致出现无效重复代码的情况。

https://github.com/vllm-project/vllm/issues/7637
这是一个非bug报告类型的issue，主要涉及Llama3 RoPE初始化的重构。

https://github.com/vllm-project/vllm/issues/7636
该issue类型为性能优化，主要涉及RotaryEmbedding中的`forward_native2`方法的优化和重构。原因是为了提高在TPU上的运行效率，减少冗余的数据拷贝操作，从而降低延迟。

https://github.com/vllm-project/vllm/issues/7635
这是一个寻求帮助的issue，主要涉及TTFT profiling with respect to prompt length。用户在测试中发现当prompt长度小于400时，TTFT似乎保持在一个水平值约100ms，但对此结果感到困惑，希望得到解释。

https://github.com/vllm-project/vllm/issues/7634
这个issue属于bug报告类型，涉及的主要对象是使用TPU的功能。由于mark_dynamic只用于虚拟运行，可能导致某些功能异常或错误。

https://github.com/vllm-project/vllm/issues/7633
这个issue是关于用户提出的需求，主要对象是vllm服务器/引擎。由于服务器崩溃/失败时，无法轻松判断服务器状态，用户提出希望在失败时能够退出程序以便重新启动。

https://github.com/vllm-project/vllm/issues/7632
这是一个bug报告，提到了关于vllm docker 0.5.4 的bug，由于设置 prompt='' 导致整个 worker 崩溃。

https://github.com/vllm-project/vllm/issues/7631
这是一个关于增加在解码阶段为编码-解码模型添加CUDA图支持的GitHub issue，主要涉及的对象是encoder-decoder模型。原因是目前对于encoder-decoder模型缺少CUDA图支持，导致需要在解码阶段进行相应的更改。

https://github.com/vllm-project/vllm/issues/7630
这是一个bug报告，主要对象是TPU backend，由于当前TPU backend为`WorkerInput`创建空的TPU tensors导致了不必要的开销。

https://github.com/vllm-project/vllm/issues/7629
这是一个bug报告，主要涉及对象是CI测试环境中的API服务器。导致该问题的原因是服务器在启动时超时，导致所有相关构建都失败。

https://github.com/vllm-project/vllm/issues/7628
这是一个bug报告，涉及的主要对象是OpenGVLab/InternVL-Chat-V1-5，用户反馈该版本无法正确停止。原因可能是默认停止token或聊天模板的问题。

https://github.com/vllm-project/vllm/issues/7627
这是一个bug报告，主要涉及vLLM中关于lossless guarantees of speculative decoding的文档请求。问题表现为在温度为0时，使用和不使用speculative decoding生成的结果不同。

https://github.com/vllm-project/vllm/issues/7626
这是一个Bug报告，涉及的主要对象是BitsAndBytesModelLoader类。由于该类不支持BitsAndBytes量化，导致出现AttributeError异常，用户请求修复此问题。

https://github.com/vllm-project/vllm/issues/7625
这是一个Bug报告，涉及到VLLM下的一个错误，由于RuntimeError导致出现AsyncEngineDeadError错误。

https://github.com/vllm-project/vllm/issues/7624
这是一个bug报告，涉及到vllm中的量化模型加载器。由于bitsandbytes量化尚未完全优化，导致部分模型可能无法正常工作。

https://github.com/vllm-project/vllm/issues/7623
这是一个bug报告，主要涉及对象是VLLM中的offline chat inference功能。由于LLM对象缺少chat()属性，导致用户无法进行离线聊天推断，可能是由于最新包没有更新该功能导致的。

https://github.com/vllm-project/vllm/issues/7622
这是一个bug报告，涉及 vllm server 部署base和lora模型后请求lora模型失败的问题。造成这个问题的原因可能是加载lora模型失败导致的。

https://github.com/vllm-project/vllm/issues/7621
这是一个bug报告类型的issue，主要涉及ci引擎和日志记录测试的修复。原因是开发者在ci系统中启用了调试级别的日志记录，以便在ci出现问题时能够获得更多信息。

https://github.com/vllm-project/vllm/issues/7620
这个issue是一个bug报告，涉及主要对象为libcudart的更新。由于现有libcudart的问题，导致需要进行更新以修复相应的bug。

https://github.com/vllm-project/vllm/issues/7619
这是一个关于性能回归的bug报告，主要涉及Block manager v2的低吞吐量问题，可能是由于warmup批次耗时过长导致。

https://github.com/vllm-project/vllm/issues/7618
这个issue是一个Bugfix类型的问题，主要涉及AsyncEngineRPCServer中引擎引用未清除导致资源未正确释放的情况。

https://github.com/vllm-project/vllm/issues/7617
这是一个Bug报告，涉及到vLLM中的"Fix custom_ar support check"。由于之前删除了`is_custom_op_supported`，但仍在此处使用，导致了该bug的出现。

https://github.com/vllm-project/vllm/issues/7616
这个issue类型是关于CI/Build的改进，涉及到性能基准文件的重组。由于需要将所有文档、workflow yaml文件和脚本放置在特定的文件夹内，可能是为了更好地组织和管理这些文件。

https://github.com/vllm-project/vllm/issues/7615
这个issue类型是Model相关的，涉及的主要对象是新增的Ultravox speech model。由于Markdown渲染问题，提交PR前的checkbox和部分内容显示有问题。

https://github.com/vllm-project/vllm/issues/7614
这是一个bug报告，涉及的主要对象是vllm，由于CUDA error导致运行autofp8时出现非法内存访问错误。

https://github.com/vllm-project/vllm/issues/7613
这是一个bug报告类型的issue，主要涉及到vLLM在TPUs上使用GKE和RayServe时的一些问题。由于未正确安装ray dashboard和serve库，计算节点数量不准确，以及环境变量设置错误，导致了在TPUs上使用时的相关问题。

https://github.com/vllm-project/vllm/issues/7612
这是一个bug报告，涉及的主要对象是openai依赖包。由于旧版本的openai包缺少types模块，导致在导入时出现错误。

https://github.com/vllm-project/vllm/issues/7611
这个issue是关于bug报告，涉及的主要对象是模型配置文件，由于nvidia更改了检查点 tokenizer，在lmeval测试中导致失败，需要通过切换到一个更稳定的模型检查点来修复。

https://github.com/vllm-project/vllm/issues/7610
This issue is a [Build/CI] improvement request for enabling passing AMD tests in vLLM. It aims to fix an issue related to markdown rendering in the PR description, ensuring the correct classification and checklist completion.

https://github.com/vllm-project/vllm/issues/7609
这是一个用户提出需求的issue，主要涉及vllm的安装和容器镜像大小的问题，用户希望获得CPU版本的镜像并减小GPU镜像的大小。

https://github.com/vllm-project/vllm/issues/7608
这是一个bug报告，涉及的主要对象是openai.types模块，由于没有将其声明为依赖导致了ModuleNotFoundError的问题。

https://github.com/vllm-project/vllm/issues/7607
这个issue是一个功能提议，主要涉及的对象是将Prefix caching kernel 应用于 Pallas 的TPU后端，由于需要启用基于CUDA和Triton的内核实现。

https://github.com/vllm-project/vllm/issues/7606
这个issue属于功能改进类型，主要涉及prefix cache hit rate 和evictor v2 的优化。由于更新last access time后不必`.move_to_end`会导致性能提升、但v1和v2性能差异仍需进一步调查。

https://github.com/vllm-project/vllm/issues/7605
这个issue是关于调整AMD测试结构的build/CI问题。

https://github.com/vllm-project/vllm/issues/7604
这是一个关于性能回归的bug报告，主要涉及VLLM的chunked_prefill参数在使用过程中导致性能下降的问题。

https://github.com/vllm-project/vllm/issues/7603
这个issue是关于模型上的支持问题，涉及到Pipeline parallel support for JAIS。导致该问题的原因可能是JAIS具有一些不支持张量并行性的模型。

https://github.com/vllm-project/vllm/issues/7602
这是一个bug报告，问题涉及的主要对象是Minitron lm eval配置，可能由于测试失败导致该问题。

https://github.com/vllm-project/vllm/issues/7601
这个issue属于用户提出需求类型，主要涉及Kernel的tuned triton配置。原因可能是用户需要为ExpertsInt8添加经过优化的triton配置文件。

https://github.com/vllm-project/vllm/issues/7600
这是一个关于bug报告，涉及到aDAG，由于asyncio的cancel API无法正常取消aDAG任务，导致测试变得不稳定。

https://github.com/vllm-project/vllm/issues/7599
这是一个bug报告，涉及主要对象为Metallama3在H100 GPU上的内存使用异常，可能是由于bfloat16数据类型导致的问题。

https://github.com/vllm-project/vllm/issues/7598
这个issue类型是关于PR提交流程的bug报告，涉及主要对象是vLLM的贡献者。由于markdown渲染不起作用，PR描述部分无法正确显示，导致无法使用PR Checklist。

https://github.com/vllm-project/vllm/issues/7597
这是一个关于引擎和执行器清理问题的Bug报告，由于清理不干净可能导致下游CI失败。

https://github.com/vllm-project/vllm/issues/7596
这个issue是关于在Kernel中修复用于支持dynamo的aqlm和ggml内核的类型问题，其中主要涉及到数据类型和tensor处理的更改。

https://github.com/vllm-project/vllm/issues/7595
这个issue类型为文档更新，主要涉及更新支持量化的硬件表格，由于LaTex不支持某些Unicode字符，导致文档变更可能会破坏PDF构建。

https://github.com/vllm-project/vllm/issues/7594
这是一个Kernel类型的issue，涉及添加对ScalarType的完整dynamo支持。由于缺乏flatten/unflatten方法和注册假类型，导致需要修复该问题。

https://github.com/vllm-project/vllm/issues/7593
这是一个bug报告，涉及的主要对象是Gemma 2 9b软件。由于无法使用rope scaling，导致无法正确运行Gemma 2 9b，出现bug。

https://github.com/vllm-project/vllm/issues/7592
这是一个bug报告，主要涉及的对象是vllm 0.5.4版本，问题是通过enable_chunked_prefill =True设置后，该版本的吞吐量略低于0.5.3~0.5.1版本，导致性能出现异常。

https://github.com/vllm-project/vllm/issues/7591
这是一个关于在vLLM项目中注册punica函数为torch操作的issue，属于代码质量和功能改进类型，主要涉及CUDA核心和计算核心的变化，可能是为了支持dynamo和避免图形中断所提交的贡献。

https://github.com/vllm-project/vllm/issues/7590
这是一个bug报告，主要涉及Phi-3-small-128k-instruct模型在4个T4 GPU上遇到内存错误的问题。造成这种症状的原因可能是模型尝试分配远超实际大小的内存。

https://github.com/vllm-project/vllm/issues/7589
这是一个空白的测试PR，类型为其他类型，涉及的主要对象为代码提交流程。

https://github.com/vllm-project/vllm/issues/7588
这是一个Bug报告，主要涉及vllm中的GPU P2P检查脚本无法找到cuda运行时的问题，导致出现`AssertionError: libcudart.so is not loaded in the current process`错误。

https://github.com/vllm-project/vllm/issues/7587
这是一个Bug报告，主要涉及VLLM环境下推理过程中出现的错误。该问题可能由于代码运行时的某些异常情况或版本兼容性问题导致。

https://github.com/vllm-project/vllm/issues/7586
这是一个用户需求提出的类型的issue，主要涉及到对vllm项目中的benchmarking脚本扩展的需求。由于当前的benchmark脚本无法测试针对speculative decode性能，用户提出了增加一个benchmark_spec_decode.py脚本的建议。

https://github.com/vllm-project/vllm/issues/7585
这是一个bug报告，该问题单涉及的主要对象是Punica SGMV kernel，由于缺少对输入 token 数量的断言导致了错误的内核计算结果。

https://github.com/vllm-project/vllm/issues/7584
这个issue是一个Bug修复类型的问题，主要涉及Ray后端在处理pg拓扑错误时提供更好的错误提示。

https://github.com/vllm-project/vllm/issues/7583
这是一个PR描述issue，主要涉及vLLM项目的bug或功能修复。原因可能是提交PR之前未填写相应信息所致。

https://github.com/vllm-project/vllm/issues/7582
这个issue属于bug报告类型，主要涉及的对象为vllm项目中的设备命名。这个问题可能由于设备名称不一致导致了bug，需要使用nvml来解决。

https://github.com/vllm-project/vllm/issues/7581
这是一个Bug报告，涉及VLLM Docker镜像在运行时消耗大量GPU内存的问题。

https://github.com/vllm-project/vllm/issues/7580
这是一个bug报告，涉及主要对象为vllm下的一个issue。由于使用了"internlm2_57bchat4bit"模式，导致运行时出现了错误。

https://github.com/vllm-project/vllm/issues/7579
这是一个Bug报告，主要涉及的对象是模型的性能表现，在使用batch size为1时，发现fp8的性能比fp16差，可能是由于某种原因导致的。

https://github.com/vllm-project/vllm/issues/7578
这是一个用户提出需求的issue，主要涉及的对象是如何在vllm中开启并发推理。原因是当前环境下的PyTorch版本不支持并发推理，导致用户无法进行此操作。

https://github.com/vllm-project/vllm/issues/7577
这个issue是关于bug报告，主要涉及swap_blocks函数在cache_kernels.cu文件中的错误处理问题，导致复制缓存数据时可能出现数值错误的情况。

https://github.com/vllm-project/vllm/issues/7576
这是一个Bug报告类型的Issue，涉及的主要对象是vLLM模型在运行过程中卡住了。由于什么样的原因导致了这个bug目前并不清楚，需要进一步分析和调试来解决。

https://github.com/vllm-project/vllm/issues/7575
这是一个bug报告，主要对象是安装vllm在Jetson AGX Orin上遇到的问题。由于缺少numpy模块导致构建wheel失败。

https://github.com/vllm-project/vllm/issues/7574
这是一个优化类的issue，主要对象是将quantization cpu offloading tests从fastcheck移出，解决了Fastcheck运行时间过长的问题。

https://github.com/vllm-project/vllm/issues/7573
该issue类型为bug报告，涉及的主要对象是vLLM中的aDAG测试。这个问题是因为NCCL可能导致CI失败和死锁，所以暂时关闭NCCL以排除相关问题。

https://github.com/vllm-project/vllm/issues/7572
这个issue属于bug报告，主要对象是测试。由于测试次序不当，导致测试失败。

https://github.com/vllm-project/vllm/issues/7571
这个issue是一个bug报告，涉及到vLLM项目中的一个功能更新。导致这个问题的原因是markdown渲染不起作用，需要使用原始的html格式。

https://github.com/vllm-project/vllm/issues/7570
这是一个用户提出需求的issue，主要涉及于使用uvloop与zmq解耦前端。原因是uvloop比原生Python asyncio事件循环实现更快，需要在启动基于zmq的RPC服务器的引擎时显式启用它。

https://github.com/vllm-project/vllm/issues/7569
该issue属于功能需求提出，主要涉及API对推测解码和前缀缓存的控制。原因是用户希望在运行时能够控制推测解码，而当前只能在启动时进行设置。

https://github.com/vllm-project/vllm/issues/7568
这个issue是一份未填写PR描述的bug报告，主要涉及创建和使用自定义NCCL组进行aDAG操作。原因可能是markdown渲染在此处无效，所以使用了原始HTML。

https://github.com/vllm-project/vllm/issues/7567
这是一个bug报告，用户在使用vllm的过程中遇到了推断速度慢的问题，想知道是否需要更好的GPU或者是否还有其他原因导致这个问题。

https://github.com/vllm-project/vllm/issues/7566
这是一个bug报告类型的issue，主要涉及的对象是模型配置（ModelConfig），由于硬编码的float16数据类型导致在主线上出现有关转换为float16的提示信息，但其实并不影响加载模型，只是检查模型是否用于嵌入。

https://github.com/vllm-project/vllm/issues/7565
这是一个bug报告，该问题涉及到openai python client在测试中的retry机制和使用context manager的问题，由于共享client fixture导致测试失败。

https://github.com/vllm-project/vllm/issues/7564
这是一个bug报告，主要涉及的对象是PyTorch的`tensor.data_ptr`的使用方式。原因是缺乏严格的constness导致一些问题，需要使用`mutable_data_ptr`和`const_data_ptr`进行替换。

https://github.com/vllm-project/vllm/issues/7563
这是一个bug报告issue，涉及的主要对象是markdown rendering功能无法正常工作。导致这个问题的原因是markdown渲染有问题，所以在文本中使用了原始的html代码。

https://github.com/vllm-project/vllm/issues/7562
这是一个bug报告，涉及的主要对象是Neuron类。导致这个bug的原因是NeuronWorker类缺少execute_worker方法，导致TypeError。

https://github.com/vllm-project/vllm/issues/7561
这是一个Bug报告，涉及对象是Llama3.1中torch.bfloat16到torch.float16类型转换的问题。由于传入dtype为bfloat16，但日志显示在进行不必要的转换，可能是由于代码中的某些逻辑导致的。

https://github.com/vllm-project/vllm/issues/7560
这是一个用户提出需求的Issue，主要涉及 Automatic Prefix Caching 和 Truncating 的问题。由于上下文长度限制导致的输入截断，使得 Automatic Prefix Caching 失效，提出了可能实现 Context Shifting 来解决此问题的需求。

https://github.com/vllm-project/vllm/issues/7559
这是一个需求提出类型的issue，主要涉及支持LlavaNextVideo模型的相关功能和API。该issue的产生可能是由于缺乏支持特定视频输入类型和其他功能导致的。

https://github.com/vllm-project/vllm/issues/7558
这是一个提出需求的类型，主要涉及 vLLM 对视频输入的支持。该需求由于目前模型对图像和视频输入的处理方式不同而提出，希望提供新的 API 和推理支持来处理视频输入。

https://github.com/vllm-project/vllm/issues/7557
这是一个bug报告，主要涉及到在vllm代码中引入的多进程功能导致 Guided decoding 功能无法正常使用的问题。

https://github.com/vllm-project/vllm/issues/7556
这是一个bug报告，涉及的主要对象是在使用`pdb`调试时出现了报错"`module '__main__' has no attribute '__spec__`"。原因可能是程序在调试过程中无法识别模块的特殊属性导致报错。

https://github.com/vllm-project/vllm/issues/7555
这是一个bug报告，主要涉及到在CPU/MacOS环境下运行预构建的Docker容器时出现的崩溃问题。原因可能是Docker镜像未针对非CUDA环境构建。

https://github.com/vllm-project/vllm/issues/7554
这是一个bug报告，主要涉及VLLM版本0.5.3.post1中使用distributed_executor_backend=mp导致进程无法通过CTRL+C终止，可能的原因是在特定情况下无法正确处理CTRL+C信号。

https://github.com/vllm-project/vllm/issues/7553
这个issue类型是bug报告，主要涉及的对象是vllm模型在NVIDIA GeForce RTX 4090 GPU上的内存利用情况。由于CUDA图形占用过多内存导致oom错误，用户提出了有关内存利用异常和发生oom错误的问题。

https://github.com/vllm-project/vllm/issues/7551
这个issue类型是用户提出需求，主要涉及对象是vllm中的constrained decoding参数设置，由于用户在SamplingParams中没有找到相关参数导致无法应用constrained decoding进行离线推断。

https://github.com/vllm-project/vllm/issues/7550
这是一个Bug报告，涉及的主要对象是在线fp8量化以及Jais模型。由于cutlass_scaled_mm()断言错误，导致了assert错误的bug。

https://github.com/vllm-project/vllm/issues/7549
这是一个新功能提议的issue，涉及到对 speculative decode worker 进行扩展，以实现新的 decode 策略。

https://github.com/vllm-project/vllm/issues/7548
这是一个Bug报告，主要涉及LLM模型的运行问题，可能是由于`NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)`信息导致的错误。

https://github.com/vllm-project/vllm/issues/7547
该issue属于用户提出需求，主要涉及API KEY authentication功能，用户希望能够为模型生成API KEY并追踪用户的使用情况。

https://github.com/vllm-project/vllm/issues/7546
这个issue是关于“Feature”的类型，主要涉及的对象是VLLM库中的Multi-modal Support功能。用户在询问关于VLLM对于MiniCPMV2.6模型是否支持多图像和视频输入，由于VLLM目前不支持处理多个图像或视频作为输入文件，并询问了关于是否有计划扩展VLLM功能来支持这些输入的问题。

https://github.com/vllm-project/vllm/issues/7545
这是一个bug报告，主要涉及vllm中的loadformat参数选择错误导致vllm serve命令失败。

https://github.com/vllm-project/vllm/issues/7544
这个issue是一个bug报告，主要涉及的对象是XLA cache path环境变量。原因是 XLA cache path环境变量不正确导致了TPU的bug。

https://github.com/vllm-project/vllm/issues/7543
这是一个用户需求问题，主要涉及到使用vllm模型时在分布式设置下无法完全访问模型权重的问题。这可能是由于tp=2参数限制导致的。

https://github.com/vllm-project/vllm/issues/7542
这是一个用户提出需求的类型，该问题单涉及的主要对象是vllm的异常处理功能。由于vllm启动的server在遇到错误时会直接挂断，用户提出了希望支持异常处理的需求。

https://github.com/vllm-project/vllm/issues/7541
这是一个关于bug报告的issue，主要涉及到vllm的CUDA设备设置问题。由于指定的CUDA设备不起作用，导致程序始终在GPU 0 上运行而非指定的GPU 6。

https://github.com/vllm-project/vllm/issues/7540
这个issue类型是性能问题，涉及的主要对象是VLLM和TGI模型的性能比较。原因可能是VLLM的GPU利用率较低和QPS较低，需要优化性能。

https://github.com/vllm-project/vllm/issues/7539
这是一个Bug报告，主要涉及到vllm中的CUDA错误导致内存访问异常。

https://github.com/vllm-project/vllm/issues/7538
这是一个bug报告类型的issue，主要涉及到无法在多个GPU上执行的问题。由于使用`CUDA_VISIBLE_DEVICES`指定多个GPU，但在部署模型时出现了CUDA内存不足的错误，导致无法进行多卡执行。

https://github.com/vllm-project/vllm/issues/7537
这是一个属于Bug报告类型的issue，主要涉及vllm流式输出时出现卡顿现象，导致输入长度过长时会卡6-9秒的问题。

https://github.com/vllm-project/vllm/issues/7536
该issue类型为功能需求，涉及主要对象是自定义操作注册和torch操作。由于需要注册自定义操作和将其从torch操作中使用，用户可能遇到相关操作在使用过程中的问题或需要支持和指导。

https://github.com/vllm-project/vllm/issues/7535
这是一个空白内容的 issue，类型为测试 PR，主要对象为代码的提交和测试过程。

https://github.com/vllm-project/vllm/issues/7534
这个issue类型是bug报告，主要涉及的对象是参数加载过程中的标量权重，由于标量权重在某些情况下没有形状，可能导致加载错误并产生相关的问题。

https://github.com/vllm-project/vllm/issues/7533
这是一个bug报告，涉及的主要对象是TPU（Tensor Processing Unit），由于不同rank拥有略有不同的XLA图导致XLA缓存共享无法实现重用，因此需要设置per-rank的XLA缓存目录来解决问题。

https://github.com/vllm-project/vllm/issues/7532
这是一个Bug报告类型的Issue，涉及的主要对象是无法成功运行Mistral-Nemo-Instruct-2407模型。由于缺少必要的C编译器环境，导致用户无法成功运行该模型。

https://github.com/vllm-project/vllm/issues/7531
这个issue是一个测试PR，不是bug报告或者用户需求或提问，主要对象是加载模型，由于需要测试加载模型的功能是否正常，因此创建了该PR。

https://github.com/vllm-project/vllm/issues/7530
这是关于代码重构和逻辑优化的issue，主要涉及到`MultiModalConfig`的初始化和性能优化。由于之前对所有模型都初始化`MultiModalConfig`，导致开发者容易混淆，所以需要进行重构并将`MultiModalConfig`作为`ModelConfig`的属性进行处理。

https://github.com/vllm-project/vllm/issues/7529
这是关于修复vLLM中lm-eval小型模型测试失败的bug报告，主要涉及评估任务得分准确性和输出改进。

https://github.com/vllm-project/vllm/issues/7528
这是一个功能需求的issue，涉及到vLLM中的Multi-step scheduling功能。原因是Multi-step scheduling与Chunked Prefill功能相互作用复杂，需要制定适当的调度策略以解决问题。

https://github.com/vllm-project/vllm/issues/7527
这个issue属于功能增强类型，主要涉及的对象是VLLM中的MoE权重加载和新增Marlin MoE内核。产生这个问题的原因是需要扩展权重加载以支持分组和每通道权重量化，同时更新MoE模型以使用更新的权重加载。

https://github.com/vllm-project/vllm/issues/7526
这是一个关于bug报告的issue，主要涉及vLLM KV缓存初始化错误，导致无法容纳模型的最大序列长度，尽管有足够的VRAM可用。

https://github.com/vllm-project/vllm/issues/7525
这是一个需求报告，主要涉及到构建系统的定制化和动态依赖关系。这个问题是由于当前的构建设置在不同的Dockerfile中使用不同的方法和依赖关系管理方式，导致过时的构建方法仍在使用，希望通过更新构建方式和统一依赖管理来解决这个问题。

https://github.com/vllm-project/vllm/issues/7524
该issue类型为用户提出需求，主要涉及的对象是针对vllm测试套件中的GPU内存利用问题。由于默认情况下vllm会占用整个GPU的内存，导致无法在pytestxdist中并行运行多个测试，可能会出现OOM错误；用户提出需要一个基于GB的替代方案来解决GPU内存利用问题。

https://github.com/vllm-project/vllm/issues/7523
这是一个bug报告，主要涉及下载过程中下载了不必要的文件，导致耗时较长。

https://github.com/vllm-project/vllm/issues/7521
这个issue类型是bug报告，涉及的主要对象是`compressedtensors`模块。由于`compressedtensors`的`accelerate`依赖与nvcr.io/nvidia/pytorch:24.05py3 docker容器不兼容，导致出现`ImportError`相关的问题。

https://github.com/vllm-project/vllm/issues/7520
这是一个类型为[Core]的issue，主要涉及的对象是支持GGUF quantization的tensor parallelism。该issue的存在是由于之前实现的GGUF quantization缺乏tensor parallelism支持所导致的。

https://github.com/vllm-project/vllm/issues/7519
这是一个关于提出新功能的issue，主要涉及的对象是vLLM的Context Parallelism。

https://github.com/vllm-project/vllm/issues/7518
这个issue类型是关于性能优化建议，主要涉及到prefix caching的性能提升的讨论。原因可能是对prefix caching在模型执行中的作用机理理解不清导致的。

https://github.com/vllm-project/vllm/issues/7517
这是一个Bug报告，涉及主要对象是autoawq marlin方法。由于vllm版本0.5.4存在错误，导致autoawq marlin方法必须为非零点，否则会出错。

https://github.com/vllm-project/vllm/issues/7516
这是一个Bug报告，涉及到vllm项目下的ImportError问题，由于某个Pull Request引入了错误而导致。

https://github.com/vllm-project/vllm/issues/7515
这个issue是硬件相关的改动，涉及的主要对象是CPU后端。原因是为了支持使用IPEX进行各种权重量化方法，特别是AWQ方法。

https://github.com/vllm-project/vllm/issues/7514
这是一个bug报告，主要涉及到由于端口已被占用导致vllm启动错误的问题。

https://github.com/vllm-project/vllm/issues/7513
这个issue是关于bug报告，涉及主要对象是AMD硬件，由于ROCM quantization在版本0.5.4中检查失败，导致GPTQ和AWQ出现问题。

https://github.com/vllm-project/vllm/issues/7512
该issue是一个Bug报告，涉及到VLLM中版本0.5.4中的ROCm量化检查失败问题。由于`model_is_embedding`缺少量化参数，可能导致使用GPTQ或AWQ量化方法在ROCm GPU上执行模型推理时出现量化检查失败。

https://github.com/vllm-project/vllm/issues/7511
这是一个bug报告，涉及的主要对象是Docker.xpu构建过程。由于环境设置问题导致构建时出现错误。

https://github.com/vllm-project/vllm/issues/7510
这是一个关于支持在notebooks中使用tqdm的issue，用户建议使用`tqdm.auto`替代`tqdm`以在jupyter notebooks中更美观。

https://github.com/vllm-project/vllm/issues/7509
这个issue属于BUG报告类型，主要涉及的对象是vllm下的flashinfer backend。这个问题由于attention group_size设置为6时，在flashinfer backend下禁用了cuda graph导致vllm崩溃。

https://github.com/vllm-project/vllm/issues/7508
这是一个bug报告，主要涉及到hidden states的处理问题，可能导致speculative decoding时出现错误。

https://github.com/vllm-project/vllm/issues/7507
这是一个关于bug报告的issue，主要对象是模型测试。由于在生成重复的chatml token，导致了CI测试失败。

https://github.com/vllm-project/vllm/issues/7506
该issue类型为用户提出需求，并主要涉及vllm中的嵌入模型。这个问题的出现可能是由于用户在集成大型嵌入模型时遇到了困难，需要指导如何在vllm中进行推理操作。

https://github.com/vllm-project/vllm/issues/7505
这是一个bug报告，主要涉及draft模型中的HiddenStates在禁用特定解码时未正确处理导致形状错误的问题。

https://github.com/vllm-project/vllm/issues/7504
这个issue类型是Bug报告，主要涉及前端（Frontend）部分，该问题是由于embedding请求导致OpenAI服务器停顿而提出的。

https://github.com/vllm-project/vllm/issues/7503
这是一个bug报告，主要涉及的对象是vllm模型。这个问题是由于vllm模型在torch 2.3.1环境下是否支持Llama-3.1-405B-Instruct multimodal而导致的。

https://github.com/vllm-project/vllm/issues/7502
这是一个Bug报告，主要涉及OpenAI服务器在为聊天模型处理嵌入请求时发生的错误，导致服务器崩溃并需要重启的情况。

https://github.com/vllm-project/vllm/issues/7501
此issue属于需求提出，主要对象是测试脚本。由于未覆盖到cudagraph，在测试中出现了问题或者需要进一步测试。

https://github.com/vllm-project/vllm/issues/7500
这是一个关于修复性能基准测试中崩溃问题的bug报告，主要涉及到Llama 70B和mixtral基准测试，可能是由于markdown渲染问题导致的。

https://github.com/vllm-project/vllm/issues/7499
这是一个bug报告，主要涉及GPU环境下pytest失败和cutlass与torch._scaled_mm之间存在大差距。

https://github.com/vllm-project/vllm/issues/7498
这是一个bug报告，涉及的主要对象是vllm的安装过程。由于cuda版本和设备不匹配，导致无法构建docker镜像中的mamba-ssm模块，出现了构建失败的症状。

https://github.com/vllm-project/vllm/issues/7497
这是一个bug报告类型的issue，涉及对象为Dockerfile的构建过程。这个问题导致构建失败的原因是在构建过程中出现了错误提示，涉及到了CMake和Torch库的配置问题。

https://github.com/vllm-project/vllm/issues/7496
这个issue是一个缺少描述的PR（Pull Request），涉及添加flash attention的因果参数并需要填写对应的PR描述。

https://github.com/vllm-project/vllm/issues/7495
这是一个用户提出需求的issue，主要涉及的对象是vllm库，用户想知道是否有选项可以减少GPU内存使用量，因为当前使用默认设置时，GPU内存占用较大可能会导致OOM错误。

https://github.com/vllm-project/vllm/issues/7494
这是一个Bug报告类型的issue，主要涉及的对象是DeepSeek-Coder-V2-Instruct-AWQ模块。由于self.quant_method未被正确设置，导致了assertion错误的bug。

https://github.com/vllm-project/vllm/issues/7493
这是一个bug报告，涉及的主要对象是GitHub仓库中的文档更新。这个问题的原因可能是由于文档未正确生成导致部分内容缺失。

https://github.com/vllm-project/vllm/issues/7492
这是一个 bug 报告，主要涉及 Mistral 7B instruct v0.1 的生成文本 token 时出现问题。

https://github.com/vllm-project/vllm/issues/7491
这是一个Bug报告，主要涉及的对象是Mistral 7b instruct v0.1模型。由于在将 add_eos_token 设置为True 时无法生成输出，可能是参数配置不正确导致的问题。

https://github.com/vllm-project/vllm/issues/7490
这是一个用户提出需求的类型的issue，主要涉及vllm的小型模型加载问题。由于PagedAttention的存在，导致小模型无法被正常加载，用户希望能够在测试时使用小型模型。

https://github.com/vllm-project/vllm/issues/7489
这是一个bug报告，主要对象是VLLM项目中的CPU测试功能。这个问题很可能是由于插件问题导致CPU测试无法正常运行。

https://github.com/vllm-project/vllm/issues/7487
这是一个bug报告，涉及的主要对象是文档页面的缺失，由于文档页面缺失导致用户无法找到所需信息。

https://github.com/vllm-project/vllm/issues/7486
这是一个bug报告，涉及的主要对象是CI中的ray导入。由于重复生成的token导致CI构建失败。

https://github.com/vllm-project/vllm/issues/7485
这是一个bug报告，主要涉及到ray gpu executor中的worker index对齐问题，由于未对应机器边界导致本地rank错误。

https://github.com/vllm-project/vllm/issues/7484
这是一个替代问题单的issue，主要涉及前端部分，需要从API服务器进程中生成引擎进程。可能由于目前的方法不够灵活或效率不高导致用户需要寻求这种改进。

https://github.com/vllm-project/vllm/issues/7483
这是一个用户提出需求类型的issue，主要涉及的对象是要发布NVIDIA Meetup公告。由于issue内容为空，用户可能正在寻求发布NVIDIA Meetup的方式或者寻求相关帮助。

https://github.com/vllm-project/vllm/issues/7481
这个issue是一个bug报告，涉及Gemna logits softcaps bug和openai server metrics bug，用户提出了需要发布v0.5.5版本来解决这些问题并包含更多性能优化的需求。

https://github.com/vllm-project/vllm/issues/7480
该issue是关于提出需求的，主要涉及添加一个类似于FastChat的路由服务器来为多个模型提供中心端点。由于需要实现一种可用于设置多个模型并连接到统一路由器服务器的方案，以解决FAQ中的一个问题。

https://github.com/vllm-project/vllm/issues/7479
这是一个bug报告，涉及的主要对象是aqlm测试在H100系统上失败。造成这种状况的原因可能是测试逻辑或环境设置问题。

https://github.com/vllm-project/vllm/issues/7478
这个issue是关于功能需求的，主要涉及VLLM是否支持Falcon Mamba模型，并询问何时将支持。这可能是由于用户尝试使用Falcon Mamba模型时遇到问题，希望VLLM能够添加对其的支持。

https://github.com/vllm-project/vllm/issues/7477
这是一个bug报告，涉及ROCm 6.2头文件兼容性中出现的函数调用歧义导致的构建问题。

https://github.com/vllm-project/vllm/issues/7476
这是一个关于在OpenAI服务器模式下使用同步LLM引擎的前端问题。

https://github.com/vllm-project/vllm/issues/7475
这是一个关于代码质量和测试的issue，主要涉及到`QwenModel`的模型测试，用户提出了缺乏针对`QwenModel`的文本测试的问题，并希望能添加类似于其他测试用例的文本测试。

https://github.com/vllm-project/vllm/issues/7474
这是一个Bug报告，涉及的主要对象是Model serving。由于参数设置错误或者代码逻辑问题导致模型加载出错，出现了错误提示和异常信息。

https://github.com/vllm-project/vllm/issues/7473
这个issue类型是bug报告，主要对象是LM Eval Tests。由于当前容忍度为0.02，但实际测试出现错误范围为0.025至0.03，因此需要将容忍度更新至0.05。

https://github.com/vllm-project/vllm/issues/7472
这是一个Bug报告，主要涉及到vllm在使用至少2个具有不同CUDA计算能力的GPU时的测试问题。由于未按照PCI总线ID顺序排列GPU导致测试错误地指出了CUDA计算能力，用户提出了相应的解决办法并控制其验证相关问题。

https://github.com/vllm-project/vllm/issues/7471
这是一个bug报告类型的issue，主要涉及的对象是vLLM库在AMD GPUs上的FP8 quantization支持问题。发生这个问题的原因是ROCm当前不支持FP8 quantization导致用户无法在AMD GPUs上使用vLLM进行推断服务。

https://github.com/vllm-project/vllm/issues/7470
这是一个bug报告，主要涉及BlockManagerV2中的滑动窗口分配问题，用户担心实际分配时没有考虑滑动窗口的情况。

https://github.com/vllm-project/vllm/issues/7469
这是一个性能问题的issue，主要涉及ROCm 6.2支持和FP8支持以及关于运行速度较慢的问题。

https://github.com/vllm-project/vllm/issues/7467
这是一个bug报告类型的issue，涉及的主要对象是GitHub上的vllm项目下的文档支持硬件页面。由于合并了更新支持硬件文档的PR，导致文档构建失败，需要回滚该PR并修复bug。

https://github.com/vllm-project/vllm/issues/7466
这是一个bug报告，涉及问题主要是在使用vllm在k8s中部署具有多个GPU的情况下出现NCCL错误。由于无法在k8s容器中启用NCCL P2P，即使GPU在同一节点上，用户寻求解决此问题或有关如何启用NCCL的帮助。

https://github.com/vllm-project/vllm/issues/7465
这是一个用户提出需求的类型，该问题涉及的主要对象是vllm的部署方式。由于当前无法使用.env文件部署vllm，用户提出希望vllm能够支持使用.env文件来配置。

https://github.com/vllm-project/vllm/issues/7464
这是一个bug报告，主要涉及 Gemma-2-2b-it 模型加载在 vLLM==0.5.1 版本在 Tesla T4 GPU 上卡住的问题。

https://github.com/vllm-project/vllm/issues/7463
这个issue是关于bug报告，涉及到vllm在使用`chat.completions` API时出现"Method Not Allowed"错误。

https://github.com/vllm-project/vllm/issues/7461
这是一个用户提出需求的类型，主要涉及vllm实现early-stopping功能。这个问题的产生可能是用户希望在计算/内存资源不足时能主动中止请求，而不需要用户手动退出请求。

https://github.com/vllm-project/vllm/issues/7460
这是一个关于需求和最佳实践的问题，主要涉及版本发布说明和模型部署方法。用户希望了解如何发布新版本的说明以及如何指导用户部署具有更高性能的模型。

https://github.com/vllm-project/vllm/issues/7459
这是一个bug报告，主要涉及AutoModelForSequenceClassification模型在使用AutoFP8量化后性能显著下降的问题，并尝试比较bfloat16和fp8的性能。

https://github.com/vllm-project/vllm/issues/7458
这是一个bug报告，主要涉及对象为vLLM在TPU上运行时出现的不必要警告，并提出了抑制该警告的问题。

https://github.com/vllm-project/vllm/issues/7457
这是一个功能需求类型的issue，主要涉及的对象是TPU支持多主机推断。这个问题的原因是全局排名和世界大小需要转换为本地排名和本地世界大小。

https://github.com/vllm-project/vllm/issues/7456
这是一个bug报告，涉及主要对象为LLAMA 3.1 405B模型的加载问题。由于使用pipeline parallelism时模型加载失败导致的问题。

https://github.com/vllm-project/vllm/issues/7455
这是一个bug报告，主要涉及Github的secret在从fork中不能传递到拉取请求中的问题，导致需要禁用该功能直到找到其他解决方案。

https://github.com/vllm-project/vllm/issues/7454
该issue类型为bug报告，涉及的主要对象是VllmWorkerProcess pid=3253，由于环境vllm更新到最新版本后，出现了“No available block found in 60 second”这一bug。

https://github.com/vllm-project/vllm/issues/7453
这是一个功能需求的issue，主要涉及的对象是OpenAI服务器的chat completions API。这个需求是为了添加对prompt_logprobs选项的支持，并通过详细说明了实现细节。

https://github.com/vllm-project/vllm/issues/7452
这个issue是一个bug报告，涉及到vllm的core模块中的multi-step args and sequence.py，由于markdown渲染不工作，需要使用原始的html代码表达。

https://github.com/vllm-project/vllm/issues/7451
这是一个功能增强类型的issue，主要涉及的对象是vllm worker、openai client和benchmark_serving.py，用户希望添加Torch profiler支持来帮助性能调优。

https://github.com/vllm-project/vllm/issues/7450
这个issue属于功能需求提议，涉及vllm项目集成flash-infer FP8 KV Cache Chunked-Prefill功能。原因可能是为了提升模型性能或者改进模型训练过程。

https://github.com/vllm-project/vllm/issues/7449
这是一个bug报告，涉及到PR描述未填写的问题。这可能是由于疏忽或者忘记填写PR描述所致。

https://github.com/vllm-project/vllm/issues/7448
这是一个特性需求的issue，涉及的主要对象是VLLM中的跨注意力并行QKV计算，由于跨注意力层需要从前一解码器层次的隐藏状态计算Q，并从编码器输出隐藏状态计算KV，目前的解决方法效率低下，需要构建一个新的QKV并行线性层。

https://github.com/vllm-project/vllm/issues/7447
该issue属于用户提出需求类型，主要对象是encoder-decoder模型，由于目前在decode阶段不支持cuda graph，用户希望增加这一支持以提高decode阶段速度。

https://github.com/vllm-project/vllm/issues/7446
这是一个功能需求issue，主要涉及在vLLM中添加支持音频语言模型的基础设施。

https://github.com/vllm-project/vllm/issues/7445
这个issue属于bug报告类型，主要涉及quantized bitsandbytes模型的支持，并包括了修复eagermode的问题。

https://github.com/vllm-project/vllm/issues/7444
该issue类型为文档补充需求，主要对象是LLM Compressor的FP8和INT8 W8A8量化。

https://github.com/vllm-project/vllm/issues/7443
这个issue是关于Bug的，涉及到前端ZMQ客户端的改进。导致这个问题的原因是需要改进ZMQ客户端的鲁棒性，确保不会出现等待服务器响应但永远不会到来的死锁情况。

https://github.com/vllm-project/vllm/issues/7442
该issue属于用户提出需求类型，主要涉及Gemina 2模型的sliding window attention支持。用户询问是否有计划支持奇数层的sliding window attention，以及在这些层不使用sliding windows会对性能造成什么样的影响。

https://github.com/vllm-project/vllm/issues/7441
这是一个Bug报告，涉及的主要对象是LLama 3.1的部署过程。由于LoRA权重不支持导致了异常错误。

https://github.com/vllm-project/vllm/issues/7440
该issue是一个bug报告，主要涉及vLLM项目中关于追踪模型前向时间到span traces的问题。由于之前的PR中留下了PP>1不支持模型前向时间的问题，导致了这个bug。

https://github.com/vllm-project/vllm/issues/7439
该issue类型属于功能需求提议，主要涉及到CI流程中的Models Test和Vision Language Models Test的拆分。由于Models Test执行时间长导致CI过程变慢，需要进行拆分以提升效率。

https://github.com/vllm-project/vllm/issues/7438
该issue属于用户提出需求类型，涉及主要对象为vLLM的模型架构插件，由于vLLM需要支持外部模型架构插件，使得用户可以通过创建独立包并安装在与vLLM相同的环境中来实现对模型架构的定制。

https://github.com/vllm-project/vllm/issues/7437
这个issue类型属于代码优化类，涉及主要对象为项目fp8，由于需要更新以使用`vLLMParameters`，这可能由于代码结构调整或性能优化需求所致。

https://github.com/vllm-project/vllm/issues/7436
这是一个新功能需求类型的issue，主要涉及的对象是vLLM模型。

https://github.com/vllm-project/vllm/issues/7435
这是一个bug报告，主要对象是改进logits processors logging message。原因可能是之前的错误信息缺少了具体的`token_id`，并且测试文件中存在不必要的注释行。

https://github.com/vllm-project/vllm/issues/7434
这个issue属于bug报告，涉及到vLLM项目中Markdown渲染无效的问题，可能是由于Markdown解析出现问题导致。

https://github.com/vllm-project/vllm/issues/7433
该issue类型为需求提出，主要涉及的对象是CI（持续集成）。

https://github.com/vllm-project/vllm/issues/7432
这是一个bug报告，主要涉及的对象是GPTQ quantization行为。原因可能是环境中的PyTorch版本与CUDA版本不兼容导致的。

https://github.com/vllm-project/vllm/issues/7430
这是一个Bug报告类型的Issue，主要涉及的对象是vllm软件在MacOS下无法正常工作。由于缺少torch==2.4.0的兼容性版本导致了安装过程中出现错误。

https://github.com/vllm-project/vllm/issues/7429
这是一个bug报告，涉及的主要对象是关于使用InfiniBand连接2个主机通过vllm docker执行 llama3.1 405B FP8 的问题。这个问题出现的原因可能是无法正确配置InfiniBand网络导致vllm执行失败。

https://github.com/vllm-project/vllm/issues/7428
这是一个关于简化Jamba状态管理的GitHub Issue，涉及到Jamba状态管理的优化和与Mamba缓存整合的问题。

https://github.com/vllm-project/vllm/issues/7427
这是一个功能需求类型的 issue，主要涉及持续集成 (CI) 测试流程的优化，目的是避免对相同提交运行相同的测试。

https://github.com/vllm-project/vllm/issues/7426
这是一个关于添加插件系统实现的功能需求，该问题涉及的主要对象为插件系统。

https://github.com/vllm-project/vllm/issues/7425
这是一个bug报告，涉及的主要对象是flash attention，问题是关于修复gemma2无法与flash attention一起工作的bug。

https://github.com/vllm-project/vllm/issues/7424
这个issue类型是bug报告，主要对象涉及到`--engine-use-ray`参数设置。由于用户设置了`engineuseray`参数，导致出现了警告信息，需要对其进行废弃和移除处理。

https://github.com/vllm-project/vllm/issues/7423
这个issue是关于bug的报告，涉及到vllm的entrypoints。更改依赖项导致了测试运行问题。

https://github.com/vllm-project/vllm/issues/7422
这是一个更新issue，主要涉及的对象是`awq`和`awq_marlin` quant方法，需要更新为使用`vLLMParameters`进行权重加载。

https://github.com/vllm-project/vllm/issues/7421
这是一个Bug报告，涉及的主要对象是VLLM库中的samplers/test_logprobs.py文件，由于某些情况下会触发Triton断言/崩溃，导致程序报错或崩溃。

https://github.com/vllm-project/vllm/issues/7420
这个issue类型是功能需求，主要涉及支持新模型LLaVA-OneVision的问题，原因是希望将新模型与现有模型进行混合以实现更多功能。

https://github.com/vllm-project/vllm/issues/7419
这是一个Bug报告类型的issue，主要涉及的对象是GEMMA2模型，由于使用FlashAttention后与FlashInfer产生不一致的结果。

https://github.com/vllm-project/vllm/issues/7418
这是一个用户提出需求的类型，询问如何在OpenVINO中使用Intel GPU。

https://github.com/vllm-project/vllm/issues/7417
这个issue是一个bug报告，主要涉及到mypy中的typing improvements，问题在于需要缩小`basiccorrectnesstest`中错误的原因。

https://github.com/vllm-project/vllm/issues/7416
这是一个功能增强的issue，主要涉及`GB`常量的合并和允许浮点数`GB`参数。

https://github.com/vllm-project/vllm/issues/7415
该issue是一个用户提出需求的类型，主要涉及到`fused_moe` triton kernel的优化，请求支持W8A16 with Int8，并在Ampere/Ada lovelace/Hopper上运行。

https://github.com/vllm-project/vllm/issues/7414
这个issue属于bug报告类型，涉及的主要对象是BlockSpaceManagerV1，由于相同的新提示同时输入时可能会导致缓存重复而可能引发并发问题。

https://github.com/vllm-project/vllm/issues/7413
这是一个bug报告，主要涉及的对象是VLLM（Very Large Language Model）。由于只支持通过AsyncLLMEngine使用Pipeline并行性，否则性能会严重受损，用户想知道是否有办法在设置的同时使用LLMEngine。

https://github.com/vllm-project/vllm/issues/7412
这是一个CI/Build类型的issue，主要涉及性能基准的更新，包括升级trt-llm至r24.07，并添加SGLang。可能是由于需要保持代码质量和提高审核效率而需求提交前提醒PR满足一定标准。

https://github.com/vllm-project/vllm/issues/7411
这是一个 bug 报告，主要涉及 VLLM 项目中的 API 服务器和引擎进程之间的分离问题，导致由于 Hugging Face 使用 AutoConfig 加载配置时导致的 CUDA 初始化问题。

https://github.com/vllm-project/vllm/issues/7410
这是一个Bugfix类型的issue，主要涉及Chameleon模型在使用tensor parallelism时无法正常运行的问题，导致权重加载错误和输出logits的处理问题。

https://github.com/vllm-project/vllm/issues/7409
这个issue是一个[Model]类型的问题，主要涉及的对象是InternViT vision encoder。由于AWQ 26B模型在使用`tp=1`时VRAM耗尽，因此决定添加用于测试大型InternVL2模型的测试。

https://github.com/vllm-project/vllm/issues/7408
这是一个用户提出需求的issue，主要涉及到GPU利用率和推理执行的优化。由于GPU kernel启动仅需要很短的时间，提出了在等待GPU完成推理时进行其他操作以提高性能的建议。

https://github.com/vllm-project/vllm/issues/7407
这个issue是关于CI/Build的改进，主要涉及到vLLM的资源管理。原因是之前使用的存储桶不属于vLLM组管理，导致需要迁移图像资源到新的由vLLM组管理的存储桶。

https://github.com/vllm-project/vllm/issues/7406
这是一个更新请求，涉及的主要对象是`awq`和`awq_marlin`的量化方法，用户寻求的帮助是更新这两种方法以使用`vLLMParameters`进行权重加载。

https://github.com/vllm-project/vllm/issues/7405
这是一个功能需求类型的issue，主要涉及的对象是collect env。用户提出了添加commit id的需求，以便更好地识别用户所使用的代码版本。

https://github.com/vllm-project/vllm/issues/7404
这是一个 bug 报告类型的 issue，主要涉及使用 gemma2 时出现的 KV Cache 警告问题，用户认为警告是困惑的。

https://github.com/vllm-project/vllm/issues/7403
这个issue类型是需求提出，主要涉及vLLM的文档内容，用户希望添加构建vLLM时使用VLLM_TARGET_DEVICE=empty的指南。

https://github.com/vllm-project/vllm/issues/7402
这是一个关于技术优化的issue，主要涉及VLLM项目中的detokenization处理。这个问题针对的是优化处理效率，通过将detokenization移出关键模型路径，转移到前端处理逻辑。

https://github.com/vllm-project/vllm/issues/7401
这是一个Bug报告类型的issue，主要涉及Dockerfile构建错误。由于Dockerfile中的语法问题导致了构建失败。

https://github.com/vllm-project/vllm/issues/7400
这是一个bug报告，涉及的主要对象是代码中的量化/awq/gemm_kernels.cu文件，导致缺少部分结果的bug是由于N=64时，计算错误而引起的。

https://github.com/vllm-project/vllm/issues/7399
这个issue属于Bug报告类型，主要涉及到vllm项目中的gemm_kernels.cu文件，用户发现了该文件中的一个bug。该bug可能导致程序运行出现异常行为。

https://github.com/vllm-project/vllm/issues/7398
这是一个Bug报告，涉及的主要对象是在安装为其他包时出现了PackageNotFoundError检查导致整个过程崩溃的问题。

https://github.com/vllm-project/vllm/issues/7397
这个issue类型是性能优化，主要涉及的对象是crossattention QKV计算的效率问题。由于crossattention需要从两个输入中计算QKV，当前方法存在效率问题，导致计算过程中存在冗余的GEMM操作。

https://github.com/vllm-project/vllm/issues/7396
这是一个优化测试过程的issue，主要涉及LoRA测试的时间消耗过大。

https://github.com/vllm-project/vllm/issues/7395
这是一个关于增加交叉注意力并行QKV计算效率的功能需求，涉及主要对象为VLLM库中的QKVParallelLinear模块。由于跨注意力层需要在解码阶段只计算Q以及在填充阶段同时计算Q和KV，目前的不足之处是需要多次应用QKVParallelLinear层来实现，因此提出了引入交叉注意力QKV并行线性层以优化计算效率。

https://github.com/vllm-project/vllm/issues/7394
这是一个bug报告，涉及的主要对象是vLLM的前端。由于`zeromq`前端在高负载情况下出现问题，导致服务器在活动请求过多时崩溃。

https://github.com/vllm-project/vllm/issues/7393
这个issue是关于bug报告，主要涉及vllm项目下的openai_vision_api_client.py文件，问题可能由于部署和调用模型时出现的错误导致。

https://github.com/vllm-project/vllm/issues/7392
这个issue是一个Bug报告，涉及到vLLM中的图片批量推断功能，导致由于不同长宽比的图片导致运行时错误。

https://github.com/vllm-project/vllm/issues/7391
这是一个bug报告，主要涉及的对象是vLLM调度器(_schedule_default())中的_token_budget。这个问题是由于可能调度超出分配的令牌预算而导致断言失败。

https://github.com/vllm-project/vllm/issues/7390
这是一个Bug报告，主要涉及到VLLM中的Guided Regex功能。问题是由于错误的正则表达式输入导致返回了错误的错误码和描述。

https://github.com/vllm-project/vllm/issues/7389
这是一个Bug报告。主要涉及的对象是在调用部署的嵌入式模型时出现了"TypeError: 'async for' requires an object with __aiter__ method, got coroutine"错误。

https://github.com/vllm-project/vllm/issues/7388
这个issue属于bug报告类，涉及的主要对象是vLLM中的`facebook/chameleon30b`模型，问题表现为加载权重时出现断言错误。

https://github.com/vllm-project/vllm/issues/7387
这个issue是一个Bug报告，涉及到vllm代码库中的一个关于Markdown渲染的问题。由于Markdown渲染不起作用，导致在PR Checklist部分无法正确显示格式。

https://github.com/vllm-project/vllm/issues/7386
这个issue是关于在vLLM中添加Triton实现以支持AWQ模型的改进，不是bug报告。

https://github.com/vllm-project/vllm/issues/7384
这是一个用户提出需求的类型，该问题单涉及的主要对象是RequestMetrics，在vllm中添加了与preempt相关的新指标。

https://github.com/vllm-project/vllm/issues/7383
这是一个bug报告，主要涉及VLLM中NCCL相关操作的问题，用户提出了关于`pynccl`包裹`allgather`和`broadcast`操作以及CUDA图捕获中使用NCCL操作的疑问。

https://github.com/vllm-project/vllm/issues/7382
这是一个bug报告，该问题涉及vLLM下的Completions API在使用上出现异常行为，导致与Chat API输出不一致且表现较差。

https://github.com/vllm-project/vllm/issues/7381
这个issue是提出一个新功能需求，主要涉及的对象是`LLMEngine`和`AsyncLLMEngine` API。由于当前API在每个步骤返回所有序列的累积输出和相关数据，而实际上对于`LLM.generate`或OpenAI服务器API，只需要最终输出或增量数据，因此提出增加一个`output_kind`参数来控制返回内容类型。

https://github.com/vllm-project/vllm/issues/7380
这是一个bug报告，主要涉及到chunked prefill + block manager v2中的边缘情况问题。这个问题的原因是当chunked prefill中一个序列的最后一个chunk被预加载，并且prompt_len是block size的倍数时会出现该问题。

https://github.com/vllm-project/vllm/issues/7379
这个issue类型属于性能问题报告，主要对象是vllm在CPU实例下的推断速度低。由于生成吞吐量低，最大值约为每秒4个标记，用户寻求帮助调试性能问题发现原因。

https://github.com/vllm-project/vllm/issues/7378
这个issue是一个Bug报告，主要涉及对象是FP8 triton kernel for opt-125m。该Bug是由于非法内存访问导致的。

https://github.com/vllm-project/vllm/issues/7377
This issue is a bug report regarding inefficiency in the python implementation of `compute_slot_mapping` due to looping over large lists.

https://github.com/vllm-project/vllm/issues/7376
这是一个bug报告，涉及主要对象为`PerTensorScaleParameter`和`BasevLLMParameter`，问题出现在权重加载上导致症状为错误的加载。

https://github.com/vllm-project/vllm/issues/7375
这是一个针对使用vllm库中Mixtral8x7BInstructv0.1AWQ模型的bug报告，用户在使用时总是得到空的生成文本，而其他模型却能正常工作。

https://github.com/vllm-project/vllm/issues/7374
这是一个bug报告，涉及的主要对象是GPTQ模型在使用Tensor Parallel时出现了输出错误。这个问题的原因是当`tensorparallelsize`设置大于1时，带有`desc_act=True`参数的GPTQ模型无法生成有效的文本。

https://github.com/vllm-project/vllm/issues/7373
这个issue类型是bug报告，主要涉及phi3-vision的错误，由于图像处理问题导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/7372
这个issue是一个bug报告，主要涉及修复ITL recording在serving benchmark中的问题，由于`async_request_openai_completions`中TTFT被错误地记录为ITL，在渲染markdown时出现的bug。

https://github.com/vllm-project/vllm/issues/7371
这是一个功能需求的问题单，主要对象是block manager v2，由于chunked prefill无法与block manager v2一起使用，导致无法正常工作。

https://github.com/vllm-project/vllm/issues/7370
这个issue是关于性能优化和功能改进的讨论，涉及到VLLM系统中KVCACHE的数据传输效率问题。由于非连续的GPU内存分布导致循环传输效率低下，用户提出使用CUDA操作并行化处理以提高性能。

https://github.com/vllm-project/vllm/issues/7369
这是一个关于CI/Build中Enabling LoRA tests on ROCm的issue，主要涉及到ROCm环境下LoRA测试的启用。这个issue可能是由于LoRA测试在ROCm环境下无法正常运行而引起的。

https://github.com/vllm-project/vllm/issues/7368
这个issue是关于安装问题，主要涉及vllm在受限环境下安装时出现错误，用户询问如何防止安装过程尝试下载cutlass导致的问题。

https://github.com/vllm-project/vllm/issues/7367
这是一个用户提出需求的issue，主要涉及的对象是使用vllm进行推断时出现的错误。该问题由于未能正确使用AsyncLLMEngine代替LLMEngine导致。

https://github.com/vllm-project/vllm/issues/7366
这是一个用户需求类型的RFC（Request for Comments）issue，主要涉及vLLM对encoder/decoder模型的支持。这个issue提出了希望社区能够贡献PR以提高vLLM的encoder/decoder功能与decoder-only功能的成熟度，并分析了现有支持及未来需要新增支持的模型和特性。

https://github.com/vllm-project/vllm/issues/7365
这是一个需求提出的issue，主要涉及的对象是vLLM的RPC服务器，由于要在不同的机器上部署API服务器和vLLM引擎服务器，因此需要保持RPC服务器的TCP协议连接。

https://github.com/vllm-project/vllm/issues/7364
这个issue类型为代码优化，主要涉及的对象是项目中的性能问题，由于先前代码提交未完全解决审查意见和一些小的改进，导致需要进一步完善及优化。

https://github.com/vllm-project/vllm/issues/7363
这个issue类型为功能需求，涉及主要对象是新添加的解码策略参数，用户提出需要使用这个新参数来改进动态并行策略。

https://github.com/vllm-project/vllm/issues/7362
这个issue属于用户提出需求类型，主要对象是实现并行评分器功能。由于需要获取下一个token的特征并在单个前向请求中运行目标模型并返回下一个token的得分，因此用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/7361
这是一个用户提出需求类型的issue，主要涉及的对象是模型执行器和模型运行器。由于需要实现一个并行解码算法的未来特性请求，需要修改当前的实现以便在需要时返回一系列下一个token而不仅仅是最后一个token。

https://github.com/vllm-project/vllm/issues/7360
这是一个bug报告，主要涉及的对象是ModelInputForGPUBuilder，由于重新初始化程序的问题导致entrypoints测试出现了问题。

https://github.com/vllm-project/vllm/issues/7359
这是一个用户提出的需求类型的 issue，主要涉及实现从提示中随机抽取的令牌创建草案，原因是为了根据先前的草案信息和已接受的令牌数量，将新的令牌附加到动态草案中。

https://github.com/vllm-project/vllm/issues/7358
这是一个用户提出需求的issue，主要涉及的对象是保存推测解码状态。由于需要调试和训练自定义的推测模型，用户希望能够接收到推测解码算法的内部状态信息。

https://github.com/vllm-project/vllm/issues/7357
这是一个Bug报告，涉及的主要对象是`bartowski/gemma227bitGGUF`模型。导致这个问题的原因可能是`vllm`构建版本不支持此模型的体系结构。

https://github.com/vllm-project/vllm/issues/7356
这是一个BUG报告，涉及VLLM工具的后台循环错误导致程序出错的问题。

https://github.com/vllm-project/vllm/issues/7355
这是一个bug报告类型的issue，涉及主要对象是vllm中的LLM Engine，用户提出由于某种原因导致了在使用LLM Engine进行推断时出现错误结果的问题。

https://github.com/vllm-project/vllm/issues/7354
该issue类型为文档改进，涉及主要对象是InternVL示例。由于markdown渲染问题，导致需要在PR描述中添加`stop_token_ids`。

https://github.com/vllm-project/vllm/issues/7353
这个issue属于用户需求提出类型，主要涉及vllm.LLM引擎，由于vllm engines无法使用连续的批处理，用户提出了希望为vllm.LLM引擎启用连续批处理或允许在线更新模型参数的需求。

https://github.com/vllm-project/vllm/issues/7352
这是一个关于bug报告的issue，主要涉及LLAMA3.1 70B GPTQ和LLAMA3 70B GPTQ的CUDA内存不足问题。

https://github.com/vllm-project/vllm/issues/7351
该issue类型为需求提案，主要涉及实现新的 speculative decode dynamic parallel strategy。原因是团队需要实现该功能来提高模型的性能和效率。

https://github.com/vllm-project/vllm/issues/7350
该issue类型为用户提出需求，主要对象是vllm的命令行接口。由于用户想要通过配置文件来启动vllm，但当前的CLI并不提供这样的功能，用户希望通过传入配置文件路径的方式进行配置。

https://github.com/vllm-project/vllm/issues/7349
这是一个Bug报告，涉及的主要对象是VLLM中internvl2-8b提问无限循环。由于环境中的Python版本与CUDA不匹配，导致无法正确收集相关信息。

https://github.com/vllm-project/vllm/issues/7348
这是一个bug报告，主要涉及vllm执行serve命令时出现的无限循环回答问题。

https://github.com/vllm-project/vllm/issues/7347
这是一个bug报告，主要涉及的对象是vllm工具中的服务器模块。这个bug的原因是允许同时将`model`作为参数和选项传递，导致`model`选项被忽略，需要修复这一行为。

https://github.com/vllm-project/vllm/issues/7346
这个issue类型为功能需求，涉及对象为RequestMetrics，用户要求在vllm中添加预抢占指标，以监视资源不足时的预抢占情况。

https://github.com/vllm-project/vllm/issues/7345
这是一个Bug报告，涉及到VLLM的服务器启动问题，由于在指定本地路径加载模型时会出现错误，但在使用`python3 -m vllm.entrypoints.openai.api_server`命令加载模型却没有问题。

https://github.com/vllm-project/vllm/issues/7344
这是一个bug报告，主要涉及vllm下的LLama3 LoRA模型加载失败的问题，导致的原因是加载LoRA模块时期望的目标模块与实际接收到的模块不匹配。

https://github.com/vllm-project/vllm/issues/7343
这个issue是一个功能需求提出，主要对象是为vllm添加对于draft model的量化配置支持。 由于当前无法指定量化方法，导致在设置INT4 gptqbased草稿模型时可能会出现使用`gptq_marlin`配置量化并引发错误的问题。

https://github.com/vllm-project/vllm/issues/7342
这是一个Bug报告，主要对象是`ops.scaled_fp8_quant`函数，由于输入形状为()导致返回的形状错误。

https://github.com/vllm-project/vllm/issues/7341
这是一个关于优化模型前向传播中GPU内存重复利用的Issue，主要对象是vLLM模型。原因是在模型前向传播期间，存在大量的临时GPU内存分配给激活张量，导致资源浪费，缩减了可用缓存空间，并降低了能够同时运行的批次和上下文长度数量。

https://github.com/vllm-project/vllm/issues/7340
这个issue是关于性能优化的需求类型，主要涉及的对象是TPU编译过程。由于编译时间较长，用户提出使用mark_dynamic来减少编译时间的问题。

https://github.com/vllm-project/vllm/issues/7339
此issue为功能需求，主要涉及到vllm模型在小型模型推理效率方面存在瓶颈，用户希望了解为什么vllm在这方面不如SGLang和TensorRT，并询问是否会改进。

https://github.com/vllm-project/vllm/issues/7338
这个issue是一个bug报告，主要对象是vLLM在处理长提示时出现OOM问题，导致需要启用FusedSDPA和chunked prefill功能。

https://github.com/vllm-project/vllm/issues/7337
这个issue是一个Bug报告，涉及的主要对象是MiniCPMV2.5服务器。由于response_format的使用导致extra_body中的stop_token_ids失效，使得推断过程无法按预期停止，需要找出原因并解决这个问题。

https://github.com/vllm-project/vllm/issues/7336
这是一个bug报告，涉及对象为`AsyncLLMEngine`，由于异常后不必要地排队了最终消息并中止了未启动的请求。

https://github.com/vllm-project/vllm/issues/7335
这个issue是一个功能需求类型，用户提出需求，希望支持声音模型像cosyVoice一样。

https://github.com/vllm-project/vllm/issues/7334
这是一个特性更新的issue，主要涉及Fused MoE权重加载的更新。由于AWQ模型具有转置权重，需要对权重加载逻辑进行复杂处理，而旧版的expert_params_mapping需要针对fp16和fp8进行重构，这导致了调用fused_moe时出现问题。

https://github.com/vllm-project/vllm/issues/7333
这是一个用户在询问需求方面的问题，主要涉及vllm是否支持动态量化以及相关功能的现状和未来计划。

https://github.com/vllm-project/vllm/issues/7332
这是一个bug报告，主要涉及的对象是编译FSM索引时出现的高内存使用和子进程OOM问题，用户询问如何跳过编译FSM索引。

https://github.com/vllm-project/vllm/issues/7331
这是一个Bug报告，主要涉及的对象是TPU Dockerfile，在构建时失败。

https://github.com/vllm-project/vllm/issues/7330
这个issue类型是用户提出需求的问题，主要对象是vllm模型的流式输出。产生这个问题的原因可能是用户对流式输出的设置不理解，希望直接输出模型生成的内容。

https://github.com/vllm-project/vllm/issues/7329
这个issue类型是在进行功能增强或优化的讨论，并且涉及到输入预处理的问题。

https://github.com/vllm-project/vllm/issues/7327
这是一个Bug报告，涉及的主要对象是fine tuned llama 3.1模型。由于环境中使用了不兼容的PyTorch版本（2.4.0），导致生成过程无法结束。

https://github.com/vllm-project/vllm/issues/7326
这个issue类型是CI/Build类型， 主要对象是在vLLM中添加e2e correctness test。原因是为了通过openai服务器添加N个并发客户端来运行lm eval，并使用本地补全进行测试。

https://github.com/vllm-project/vllm/issues/7325
该issue类型为用户提出需求，主要涉及对象为在8-GPU A800（80G）服务器部署qwen2-7b模型作为API时，如何配置参数以支持更高并发性能。该问题的根本原因在于如何设置参数以实现更好的并发性能利用8个GPU。

https://github.com/vllm-project/vllm/issues/7324
这个issue类型是代码改进建议，主要涉及的对象是使用`torch.testing.assert_close`来改进测试。由于目前当`allclose`测试失败时，无法准确了解错误的性质，因此建议使用更易于调试和诊断的方法。

https://github.com/vllm-project/vllm/issues/7323
这个issue属于技术优化，主要涉及的对象是gptq_marlin kernel，问题由于需要使用 ScalarType 来代替 num_bits 来进行 dispatch，以支持C++17标准下将标量类型作为模板参数传递。

https://github.com/vllm-project/vllm/issues/7322
这个issue属于用户提出需求类型，主要涉及对象是VLLM在8xA100上运行DeepSeekCoderV2InstructFP8的支持问题。原因是VLLM使用的Triton不支持FP8 Marlin mixedprecision，导致用户在寻求解决方案。

https://github.com/vllm-project/vllm/issues/7320
这是一个功能需求问题，主要涉及bitsandbytes库的支持，用户提出需要添加对特定模型的支持。

https://github.com/vllm-project/vllm/issues/7319
这个issue是一个bug报告，涉及的主要对象是flashinfer后端代码，由于`is_profile_run`的bug导致flashinfer v0.1.3运行失败。

https://github.com/vllm-project/vllm/issues/7318
这是一个用户提出需求的issue，主要对象是OpenAI API和VLLM，由于添加了Enable_prefix_caching参数，导致用户不知道如何计算当前处理的token数量。

https://github.com/vllm-project/vllm/issues/7317
这是一个功能改进（Feature Improvement）类型的issue，主要涉及到GPTQ Marlin核心代码的编译时间过长。原因是`gptq_marlin.cu`文件过于庞大和使用了大量模板，导致开发者编译所需时间过长。

https://github.com/vllm-project/vllm/issues/7316
这是一个文档更新类的issue，涉及到更新项目的readme文件。

https://github.com/vllm-project/vllm/issues/7315
这是一个用户提出需求的issue，主要涉及支持attention backend with FlexAttention。由于PyTorch版本需要升级到2.5.0，目前无法使用该功能。

https://github.com/vllm-project/vllm/issues/7314
这个issue类型是请求赞助，涉及主要对象是Skywork AI。

https://github.com/vllm-project/vllm/issues/7313
这是一个bug报告，涉及到vllm下的TPU Dockerfile构建失败的问题。由于在主分支（commit `5fb4a3f6785e3612bf1741f6e43a4184a37649c1`）和标签`v0.5.4`上执行`docker build f Dockerfile.tpu t vllmtpu .`失败。

https://github.com/vllm-project/vllm/issues/7312
这是一个bug报告类型的issue，涉及的主要对象是当前环境及程序输出。由于问题描述不清晰，可能导致了无法准确识别bug或者需要的帮助类型。

https://github.com/vllm-project/vllm/issues/7311
这个issue是一个需求提出类型，主要对象是为了在prompt adapter中支持安装 "peft"。由于缺乏此功能，用户提出了需要支持 "peft" 安装的需求。

https://github.com/vllm-project/vllm/issues/7310
该issue类型为用户需求，涉及主要对象为issue模板的展现形式，由于collect_env.py输出较大，需要将其放置在一个可以折叠的detail块中以提高可读性。

https://github.com/vllm-project/vllm/issues/7309
这是一个Bug报告，主要涉及的对象是Front-end server。由于有太多的挂起请求导致前端服务器过载，触发异常并导致新请求失败。

https://github.com/vllm-project/vllm/issues/7308
这个issue属于bug报告类型，主要涉及的对象是BitAndBytes组件。由于某种原因导致这个组件出现错误，需要临时修复以解决问题。

https://github.com/vllm-project/vllm/issues/7307
这个issue是一个功能需求类型的问题，主要涉及的对象是在PyTorch中的测试模块。由于使用`torch.allclose`不够好诊断测试失败，用户建议使用`torch.testing.assert_close`作为建议实践。

https://github.com/vllm-project/vllm/issues/7306
这个issue属于bug报告，主要涉及CI在应用`ready`标签时不会运行`fastcheck`，导致CI仅运行了/pr/测试的问题。

https://github.com/vllm-project/vllm/issues/7305
该issue是一个Bugfix类型的问题，主要涉及的对象是Kernel。增加`atol`的值来修复`test_flash_attn.py`中的测试失败。这是由于测试失败导致了CC相关问题。

https://github.com/vllm-project/vllm/issues/7304
这是一个用户提出需求的类型，主要涉及保存分片状态的问题。可能由于脚本执行时出现卡住的情况，用户想要使用VLLM保存分片的需求。

https://github.com/vllm-project/vllm/issues/7303
这是一个Bug报告，涉及对象是vllm模型在Tesla T4上加载后无响应，可能由于VRAM使用异常或模型无法正常加载导致。

https://github.com/vllm-project/vllm/issues/7302
这个issue是一个Bugfix类型的issue，主要对象是`torch.dtype`，由于存储格式不规范导致了问题。

https://github.com/vllm-project/vllm/issues/7301
这是一个bug报告，涉及vLLM的Speculative Decoding脚本，并询问如何计算或访问接受率。

https://github.com/vllm-project/vllm/issues/7300
这是一个Bug报告，涉及的主要对象是使用ChenMnZ/MistralLargeInstruct2407EfficientQATw2g64GPTQ无法输出内容的问题。由于未能正常输出内容，很可能由于加载的模型在处理消息时出现了错误，导致没有生成相应的输出。

https://github.com/vllm-project/vllm/issues/7299
这是一个bug报告，主要涉及Meta-Llama-3.1-70B-Instruct模型的使用信息没有被正确添加，导致用户无法获得正确的使用信息。原因可能是在最新的vllm image中，对使用信息的处理存在问题。

https://github.com/vllm-project/vllm/issues/7298
该issue属于改进建议类型，主要对象是Dockerfile.cpu的构建优化，通过使用新的安装方式和缓存绑定挂载等措施来提高构建效率。

https://github.com/vllm-project/vllm/issues/7297
这是一个bug报告，主要涉及vllm在升级到v0.5.4版本后出现的 hang 问题，用户遇到了vllm无法响应的情况。

https://github.com/vllm-project/vllm/issues/7296
这是一个bug报告，主要涉及AsyncLLMEngine 和 LLMEngine 的性能问题，导致不同结果。

https://github.com/vllm-project/vllm/issues/7295
这是一个用户报告的bug，主要涉及到在部署qwen2时出现问题，可能由于vllm版本0.5.3.post1与CUDA版本不兼容导致。

https://github.com/vllm-project/vllm/issues/7294
这是一个bug报告，涉及的主要对象是vLLM API server。由于vLLM在使用指定模型时出现数值错误异常，可能是因为当前环境的配置与模型要求不符。

https://github.com/vllm-project/vllm/issues/7293
这是一个用户提出需求的issue，主要涉及的对象是vllm项目的构建系统。原因是NVIDIA Jetson Orin设备使用CUDA计算能力8.7，需要将其添加到白名单中。

https://github.com/vllm-project/vllm/issues/7292
这个issue是一个bug报告，涉及到LoRA与PP相关的问题，由于某种原因导致了需要修复该问题。

https://github.com/vllm-project/vllm/issues/7291
这是一个Bug报告，主要涉及的对象是GGUF模型加载示例。由于可能存在NotImplementedError错误，用户寻求帮助解决问题。

https://github.com/vllm-project/vllm/issues/7290
这是一个bug报告，涉及vllm升级到v0.5.4后出现"500 Internal Server Error"的问题。

https://github.com/vllm-project/vllm/issues/7289
该issue类型为用户提出需求，涉及到Beam Search及其缺乏控制多样性的参数。

https://github.com/vllm-project/vllm/issues/7288
这个issue是关于[Frontend]的需求提出，希望移除lora的max_num_batched_tokens限制。

https://github.com/vllm-project/vllm/issues/7287
这个issue属于用户提出需求类型，主要对象是"HammingDiversityLogitsProcessor"，用户提出了关于增加新功能的需求。

https://github.com/vllm-project/vllm/issues/7286
这是一个bug报告类型的issue，主要涉及用户使用docker命令添加多个lora时出现的错误问题。由于模型`xyzlora`不存在，导致报错信息为"The model `xyzlora` does not exist."。

https://github.com/vllm-project/vllm/issues/7285
这个issue属于文档修复类型，主要涉及到scheduler.py文件中的拼写错误修正。由于markdown解析问题，需要使用原始HTML来填写描述部分。

https://github.com/vllm-project/vllm/issues/7284
这是一个bug报告，主要涉及FlashInfer backend，可能由于代码错误导致生成错误的输出。

https://github.com/vllm-project/vllm/issues/7283
这个issue类型为Bug报告，主要涉及的对象是vllm server。由于传入空提示导致服务器立即崩溃并无法恢复。

https://github.com/vllm-project/vllm/issues/7282
这个issue类型是需求提出，主要涉及的对象是`merge_async_iterators`工具函数。问题是由于新增的`is_cancelled`参数导致不兼容性，因此需要将其变为可选参数以保持向后兼容性。

https://github.com/vllm-project/vllm/issues/7281
这是一个更新请求，主要涉及的对象是`gptq_marlin`参数。这个更新是为了使用`vLLMParameters`简化线性层权重加载，同时添加`PackedColumnParameter`以支持不带行并行性的紧凑参数。

https://github.com/vllm-project/vllm/issues/7280
这是一个空内容的issue，类型是用户提出需求。该问题单涉及的主要对象是TPU测试。

https://github.com/vllm-project/vllm/issues/7279
这是一个关于bug报告的issue，主要涉及的对象是Prometheus metrics，由于`zeromq` frontend导致了metrics在`v0.5.4`版本中出现问题。

https://github.com/vllm-project/vllm/issues/7278
这是一个功能需求问题，主要涉及构建时是否需要生成每次提交的wheel包，由于每次需要大约810分钟来构建CUDA 12.1 wheel，因此提出不需要每次提交时都生成wheel的建议。

https://github.com/vllm-project/vllm/issues/7277
这是一个功能需求的issue，主要涉及到VLLM项目代码重用compressedtensors中的压缩和量化配置。造成这个需求的原因可能是为了提高代码复用性和减少重复开发工作。

https://github.com/vllm-project/vllm/issues/7276
该issue属于文档更新类型，主要涉及更新支持量化硬件目标的表格。可能是为了维护最新的硬件支持信息而提出。

https://github.com/vllm-project/vllm/issues/7275
这个issue类型属于bug报告，涉及的主要对象是软件代码中的一个函数名。原因是函数名拼写错误导致bug，需要更正以解决问题。

https://github.com/vllm-project/vllm/issues/7274
这是一个bug报告，针对vllm项目中一个函数名称拼写错误的问题。

https://github.com/vllm-project/vllm/issues/7273
这是一个功能需求问题，主要涉及将MiniCPMVQwen2重命名为MiniCPMV2.6，涉及到对逻辑的轻微修改和添加新的支持模型。

https://github.com/vllm-project/vllm/issues/7272
这是一个Bug报告，主要涉及到OpenGVLab/InternVL2-26B下的AsyncEngineDeadError，可能由于使用不同的vision transformer模型导致任务意外完成，引发数值错误。

https://github.com/vllm-project/vllm/issues/7271
这个issue类型是Kernel相关的，涉及主要对象是 vLLM 中的 asymmetric quantization。由于没有使用正确的markdown渲染，导致无法显示正确格式的内容。

https://github.com/vllm-project/vllm/issues/7270
该issue类型是针对代码质量的PR需求，主要涉及对CUDA kernels或其他compute kernels的变更，由于markdown渲染无法正常工作，需要使用原始的html进行标记。这是一个代码贡献相关的问题。

https://github.com/vllm-project/vllm/issues/7269
这是一个bug报告，主要涉及 Llama3.1 GGUF 模型加载失败的问题；原因是由于模型权重中 `rope_freqs.weight` 的处理方式导致了加载失败。

https://github.com/vllm-project/vllm/issues/7268
这是一个bug报告类型的issue，主要涉及的对象是 Llama3.1 GGUF 模型的权重加载失败。这个问题是由于新的 `llama.cpp` 转换脚本更新后部分模型包含了额外的 `rope_freqs.weight` 张量，导致权重加载错误。

https://github.com/vllm-project/vllm/issues/7267
这是用户提出需求的issue类型，主要涉及的对象是对MiniCPM-V2_6模型的支持。由于目前vllm还未支持该模型，用户想知道当前支持模型最接近的是哪一个，并询问支持该模型的难度。

https://github.com/vllm-project/vllm/issues/7266
这个issue类型是CI/Build问题，主要涉及的对象是OpenTelemetry版本的管理。由于版本不匹配可能导致的bug或者用户提出需要关于OTel可用性错误信息清晰度的改进。

https://github.com/vllm-project/vllm/issues/7265
这是一个用户提出需求的issue，主要涉及对象为VLLM中的Chroma和Quadrant。由于无法直接在VLLM的openai包装器中使用自定义嵌入函数，导致出现内部函数参数错误或内存错误的问题。

https://github.com/vllm-project/vllm/issues/7264
这个issue是一个Bug报告，主要涉及对象是GPTQ在T4s上的失败。问题是由于`min_capability`重构错误导致`override_quantization_method`错误地认为始终在Ampere系统上。

https://github.com/vllm-project/vllm/issues/7263
该issue类型为用户提出需求，该问题单涉及的主要对象是某个功能的增加。由于用户需要实现某项特定功能，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/7262
类型：Bug报告
主要对象：VLLM插件
原因：stream_options.include_usage被设置为True后，每个数据块都会带有使用统计信息，与0.5.3版本或当前OpenAI API的行为不一致。

https://github.com/vllm-project/vllm/issues/7261
这是一个Bug报告，主要涉及GPU内存泄漏导致OOM（Out of Memory）的问题。

https://github.com/vllm-project/vllm/issues/7260
这是一个Bug报告，主要涉及vllm服务中的模型名称显示错误的问题，可能是由于模型名称未正确匹配导致。

https://github.com/vllm-project/vllm/issues/7259
该issue类型为用户提出需求，主要对象是支持更长的max_num_batched_tokens for lora。由于许多模型需要长上下文能力，导致需要支持更长的max_num_batched_tokens。

https://github.com/vllm-project/vllm/issues/7258
这是一个PR（Pull Request）类型的issue，主要涉及支持encoder/decoder模型的服务，清理了之前的代码逻辑以解决mypy引发的警告并更新了async引擎，最终导致的bug修复和新功能添加。

https://github.com/vllm-project/vllm/issues/7257
这是一个关于在Intel平台上支持压缩张量 W8A8 的CPU后端的问题。

https://github.com/vllm-project/vllm/issues/7256
这是一个Bug报告。该问题涉及的主要对象是VLLM（Very Large Language Model）。由于估计不准确导致GPU内存溢出错误。

https://github.com/vllm-project/vllm/issues/7255
这是一个性能优化类型的issue，主要涉及vLLM的speculative decoding功能。由于当前的评分机制效率低下，导致在speculative decoding中没有或很少性能改进。

https://github.com/vllm-project/vllm/issues/7254
这是一个Bug报告，涉及的主要对象是无法使用FlashAttention-2后端，并且由于缺少vllm_flash_attn包导致该问题。

https://github.com/vllm-project/vllm/issues/7253
这是一个Bug报告，主要涉及的对象是Lora服务。由于在启用pipeline parallelism时未考虑到PPMissingLayer不是真正的lm_ head，导致了报错。

https://github.com/vllm-project/vllm/issues/7252
这是一个需求提议的issue，主要涉及vLLM在 speculative decoding 过程中因为不同的vocab_size而出现的错误。

https://github.com/vllm-project/vllm/issues/7251
这是一个关于软件依赖更新的问题，涉及到OpenVINO和optimumintel版本更新，由于安装过程中的markdown渲染问题导致的bug。

https://github.com/vllm-project/vllm/issues/7250
这个issue是一个Bug报告，主要涉及的对象是vllm在线推理功能。由于启用了前缀缓存导致的内部服务器错误。

https://github.com/vllm-project/vllm/issues/7249
这个issue类型是功能需求，涉及的主要对象是在cuda平台上的分布式系统；该需求是为了避免重复代码，根据用户的请求添加专门的方法。

https://github.com/vllm-project/vllm/issues/7248
这是一个功能需求的issue，主要对象是`vllm.entrypoints`，由于需要更改类型注解和启用`followimports=silent`，部分解决了Enable mypy type checking引起的问题。

https://github.com/vllm-project/vllm/issues/7247
该issue类型为功能请求，主要涉及目标是为vLLM引入对RBLN NPU的初始支持。原因是要开发RBLN后端，使其能够与Atom和Rebel等NPU兼容，并支持大型语言模型。

https://github.com/vllm-project/vllm/issues/7246
这个issue属于bug报告类型，主要涉及的对象是ngc24.05版本，可能是由于某种原因导致了"RuntimeError: Cannot re-initialize CUDA in forked subprocess"错误的症状。

https://github.com/vllm-project/vllm/issues/7245
这个issue是关于文档的修复和改进请求。原因是markdown渲染在该部分不起作用，需要使用raw html来进行渲染。

https://github.com/vllm-project/vllm/issues/7244
这是一个性能问题的issue，主要涉及Speculative Decoding Performance，由于使用了Flashinfer导致了性能变化。

https://github.com/vllm-project/vllm/issues/7243
这是一个需求类型的issue，提出了添加在线推测解码示例的需求。

https://github.com/vllm-project/vllm/issues/7242
这是一个bug报告，主要涉及到vllm项目中的ITL计算问题，可能是由于缺少了条件判断语句导致的TTFT被错误地添加到ITL列表中。

https://github.com/vllm-project/vllm/issues/7241
这是一个需求类型的issue，主要涉及到vllm中最大并发请求数量限制的问题，用户希望提高当前默认设置的100。

https://github.com/vllm-project/vllm/issues/7240
这是一个bug报告类型的issue，主要涉及vllm新版本(v0.5.4)无法加载gptq模型的问题。

https://github.com/vllm-project/vllm/issues/7239
这是一个Bug报告类型的issue，主要涉及Mixtral Model的初始化问题，导致在同一个shell会话中无法实例化新的模型，可能是由于CUDA状态在尝试实例化第一个LLM实例时初始化的原因。

https://github.com/vllm-project/vllm/issues/7238
这个issue属于bug报告类型，主要涉及前端的错误处理和修复异步引擎测试失败的问题。

https://github.com/vllm-project/vllm/issues/7237
这个issue是一个CI/Build类型的问题，主要涉及ROCm平台下tensorizer测试的启用。原因可能是在Markdown渲染问题导致PR描述未能正确显示。

https://github.com/vllm-project/vllm/issues/7236
这个issue是一个用户提出需求的类型，主要对象是请求vllm添加对LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct模型的支持。由于这个模型是一个新的架构，用户请求vllm团队添加对该模型的支持。

https://github.com/vllm-project/vllm/issues/7235
这个issue类型属于用户提出需求。该问题单涉及的主要对象是注册自定义操作flash attention。由于当前工作正在进行中（"[wip]"），用户可能正在尝试注册自定义操作并遇到一些困难或需要指导。

https://github.com/vllm-project/vllm/issues/7234
这个issue类型是bug报告，主要涉及的对象是markdown渲染的问题。导致这个bug的原因是markdown渲染失败。

https://github.com/vllm-project/vllm/issues/7233
这是一个关于代码优化的issue，涉及到Kernel模块。由于替换了自定义的blockReduce函数，导致性能有微小的变化。

https://github.com/vllm-project/vllm/issues/7232
这是一个Bug修复类型的issue，主要涉及vLLM项目中的speculative decoding。当前存在一个问题，当使用speculative decoding生成token时请求logprobs会导致服务器崩溃，原因是sampler输出中的logprobs中存在token id为None的条目。

https://github.com/vllm-project/vllm/issues/7231
这是一个Bug报告类型的Issue，主要涉及到vllm 0.2.1.post1版本中出现的"command not found: vllm"问题，用户寻求解决这个bug的方法。

https://github.com/vllm-project/vllm/issues/7230
这是一个关于增加 vLLM 模型的多图像输入支持的 issue，主要涉及模型的改进和新功能添加。

https://github.com/vllm-project/vllm/issues/7229
该issue类型为技术改进需求，主要涉及的对象是`vllm/core`模块；用户希望通过启用mypy类型检查部分解决关于启用mypy类型检查的问题。

https://github.com/vllm-project/vllm/issues/7228
这个issue是关于Bug报告，主要涉及对象是llama 3.1 70b和MistralLarge。由于Mistral Large在测试中出现质量下降和失败的问题，可能导致其不受支持。

https://github.com/vllm-project/vllm/issues/7227
这是一个bug报告，涉及的主要对象是SamplingParams Constructor中的top_k参数，由于top_k参数未能自动验证为整数类型，导致导致TypeError异常。

https://github.com/vllm-project/vllm/issues/7226
这是一个bug报告，主要涉及的对象是代码中的top_k参数。用户提出了因为top_k参数不是整数类型会导致TypeError错误的问题。

https://github.com/vllm-project/vllm/issues/7225
这是一个bug报告类型的issue，主要涉及对象是GPTQ和GPTQ Marlin模型。由于`qzeros`被错误地设置为`"meta"`设备参数，在使用cpu offloading时导致了模型创建失败和权重混乱的症状，需要移除该设定来解决这一问题。

https://github.com/vllm-project/vllm/issues/7224
这是一个bug报告，涉及到关机aDAG workers时可能出现的异步llm引擎退出问题。

https://github.com/vllm-project/vllm/issues/7223
这是一个Bug报告，涉及的主要对象是vLLM的Open AI-compatible API 和 LangChain/LangGraph tools的集成问题。用户由于vLLM在调用函数时出现错误，导致无法正常使用该API。

https://github.com/vllm-project/vllm/issues/7222
这个issue类型是bug报告，涉及将`zmq`前端从TCP改为IPC所引起的问题，由于使用TCP需要设置端口且出现了一些问题，因此需要将其改为IPC。

https://github.com/vllm-project/vllm/issues/7221
这个issue是一个功能需求，主要涉及支持FLUTE量化，由于FLUTE支持各种新的量化技术和模型，希望为vLLM带来新的研究和改进。

https://github.com/vllm-project/vllm/issues/7220
这是一个需求提出的issue，主要涉及将API与OAI的结构化输出对齐，因为OpenAI API引入了支持结构化输出的功能。

https://github.com/vllm-project/vllm/issues/7219
这是一个bug报告，涉及的主要对象是针对FP8 Marlin动态量化的处理逻辑。由于未将pertensor scales扩展到channelwise，导致在fp16到fp8动态量化情况下出现问题。

https://github.com/vllm-project/vllm/issues/7218
这是一个Bug修复类型的Issue，涉及MLPSpeculator模型在使用speculative decoding时出现的问题。由于`logits_processor`中填充截断操作的维度错误，导致vocab维度未被截断，进而在处理过程中出现越界类型错误。

https://github.com/vllm-project/vllm/issues/7217
这是一个BugFix类型的issue，主要涉及的对象是vllm的前端多进程。由于后端在初始化过程中意外终止并未回复`IS_SERVER_READY`消息，可能导致前端多进程hang的问题。

https://github.com/vllm-project/vllm/issues/7216
这是一个Bug报告，单涉及的主要对象是Dynamic FP8 Marlin quantization，由于在不同硬件平台上出现了不一致的行为。

https://github.com/vllm-project/vllm/issues/7215
该issue是关于更新TensorRT-LLM版本以进行性能比较的文档问题，属于需求类型，主要涉及的对象是TensorRT-LLM版本号。

https://github.com/vllm-project/vllm/issues/7214
这是一个用户提出需求的类型issue，主要对象为vllm API server和Flask app。由于API server以JSON对象序列形式返回响应，用户想在Flask应用程序中以流的方式显示最终文本，并希望获取关于如何正确切割文本以及实现实时更新屏幕响应的建议。

https://github.com/vllm-project/vllm/issues/7213
这是一个Bug报告，主要关注于VLLM中的前端多进程处理导致初始化错误时服务器卡住的问题。

https://github.com/vllm-project/vllm/issues/7212
这是一个bug报告，涉及的主要对象是vLLM中的IBM granite 7b model。由于CUDA错误导致运行推理时触发了`RuntimeError: CUDA error: deviceside assert triggered`错误。

https://github.com/vllm-project/vllm/issues/7211
这是一个Bug报告，主要涉及的对象是vllm中使用了MIG实例的GPU并行性配置问题。原因是在启动ray和vllm之前需要将CUDA_VISIBLE_DEVICES设置为字符串，导致数值转换错误。

https://github.com/vllm-project/vllm/issues/7210
这是一个需求类型的issue，主要涉及的对象是为 Rocm 添加 fp8 Linear Layer，由于 Rocm 使用了 torch 2.5 版本，需要进行相应的数据类型和权重调整。

https://github.com/vllm-project/vllm/issues/7209
这是一个关于性能优化的issue，主要涉及到Evictor V1和V2中使用prefix caching模式时的性能问题，导致了时间到第一个标记（TTFT）增加和系统变慢。

https://github.com/vllm-project/vllm/issues/7208
这是一个bug报告，主要涉及到FP8 Triton kernel的问题，由于缺乏性能调优而导致性能稍低。

https://github.com/vllm-project/vllm/issues/7207
这是一个bug报告，涉及到vllm在rocm docker中无法识别GPU的问题。这可能是由于Docker容器内的进程无法看到GPU的原因导致的。

https://github.com/vllm-project/vllm/issues/7206
这是一个特性增强（feature enhancement）类型的issue，主要涉及TensorCache的缓存功能引入，旨在解决在调度器/准备输入之间缓存张量的需求。

https://github.com/vllm-project/vllm/issues/7205
这是一个BugFix类型的issue，涉及的主要对象是在设置`VLLM_PORT`时可能会导致ZMQ出现问题，导致分布式多处理和前端多处理端口冲突，从而造成程序崩溃。

https://github.com/vllm-project/vllm/issues/7204
这是一个bug报告，主要涉及到vllm 0.5.4版本下的GPTQ Marlin模型在cpu offloading情况下运行失败的问题。由于之前的修复似乎未生效，导致用户仍然无法成功运行模型。

https://github.com/vllm-project/vllm/issues/7203
这是一个Bug报告。该问题涉及到VLLM库中的SamplingParams构造函数。由于参数数值范围不当导致错误信息不清晰，用户提出应添加验证以确保参数值在可接受范围内，并提供更具体的错误信息。

https://github.com/vllm-project/vllm/issues/7202
这是一个关于bug报告的issue，主要涉及vllm使用中的pipeline-parallel-size设置问题。由于设置了pipeline-parallel-size为8，但只有1或2个GPU在同时运行，导致出现了GPU利用率不高的现象。

https://github.com/vllm-project/vllm/issues/7201
这是一个关于Bug报告的issue，涉及主要对象为NCCL库。由于使用tensor_parallel时出现"RuntimeError: NCCL error: invalid usage"错误，可能是由于NCCL库的不正确使用导致的。

https://github.com/vllm-project/vllm/issues/7200
这个issue为Bug报告类型，主要涉及的对象为加载fp16模型时发生OOM错误。由于将fp16模型加载为fp8量化模型导致OOM错误。

https://github.com/vllm-project/vllm/issues/7199
这个issue类型是功能需求，涉及的主要对象是为vLLM项目中的LoRA功能增加MiniCPMV2.5的支持。

https://github.com/vllm-project/vllm/issues/7198
这个issue类型是bug报告，涉及的主要对象是在使用OpenVINO backend时出现的模块缺失错误 ModuleNotFoundError: No module named 'gguf'.

https://github.com/vllm-project/vllm/issues/7197
这是一个Bug报告，用户在调用vllm下的FlexibleArgumentParser时出现了导入错误。

https://github.com/vllm-project/vllm/issues/7196
这个issue类型是bug报告，主要涉及的对象是vllm/entrypoints/openai/rpc/server.py文件中的端口使用问题，由于端口冲突导致了"ZMQError: Address already in use"错误。

https://github.com/vllm-project/vllm/issues/7195
该issue类型是用户提出需求，主要涉及调整`self.model_config.max_model_len`size来解决GPU内存不足导致的错误。

https://github.com/vllm-project/vllm/issues/7194
这是一个bug报告，该问题涉及vllm中的pipeline-parallelism功能，由于pipelineparallelism导致了tool calling响应不完整的bug。

https://github.com/vllm-project/vllm/issues/7193
这是一个bug报告，主要涉及对象是evictor-v2在使用AutoPrefixCache时性能表现较差，问题出在evictor_v2::LRUEvictor中使用的self.free_table和update机制导致性能下降。

https://github.com/vllm-project/vllm/issues/7192
这个issue是用户提出需求，请求支持Qwen-VL模型。

https://github.com/vllm-project/vllm/issues/7191
这是一个Bug报告，涉及的主要对象是vllm库中的flash_attn_varlen_func()函数。这个问题可能是由于函数调用时传递了一个未定义的关键字参数'softcap'而导致的。

https://github.com/vllm-project/vllm/issues/7190
这是一个Bug报告，主要涉及的对象是使用Interval2生成的内容。这个问题可能是由于环境中使用的PyTorch版本与其他组件之间不兼容所致。

https://github.com/vllm-project/vllm/issues/7189
这是一个bug报告，涉及LM Format Enforcer工具的版本更新问题，导致在新的OpenAI兼容服务器中引发异常。

https://github.com/vllm-project/vllm/issues/7188
这是一个Bug报告，主要涉及v0.5.4版本的docker release缺失/metrics端点，并导致无法获得与模型相关的统计信息。

https://github.com/vllm-project/vllm/issues/7187
这是一个Model类型的issue，主要涉及在InternVL2模型中添加AWQ量化支持。

https://github.com/vllm-project/vllm/issues/7186
这是一个需求提交的issue，主要涉及的对象是VLM中各种ViTs模型。由于更新了ViT版本，导致需要更新对应模型的加载权重部分，以确保正确加载模型。

https://github.com/vllm-project/vllm/issues/7185
这是一个关于bug报告的issue，涉及的主要对象是chunked prefill与fp8 kv-cache。这个问题由于chunked prefill与fp8 kv-cache不兼容而导致bug无法触发。

https://github.com/vllm-project/vllm/issues/7184
这是一个bug报告，涉及fp8 quantization在LoRA适配器模型上运行时出现错误。由于环境因素导致的错误。

https://github.com/vllm-project/vllm/issues/7183
这是一个bug报告，主要涉及的对象是SpecDecode中的sampler测试，由于GPU数量大于1时导致测试失败。

https://github.com/vllm-project/vllm/issues/7182
这是一个关于更新 Dockerfile 的 issue，主要涉及到 CPU 版本的 protobuf 安装问题。用户提出此 issue 是因为 LlamaTokenizer 的新行为需要安装 protobuf，并对 .buildkite/runcputest.sh 中的冗余安装进行了优化。

https://github.com/vllm-project/vllm/issues/7179
这是一个bug报告，主要对象是vllm(v0.5.3)中的openai接口，由于流式返回取消了usage信息导致的问题。

https://github.com/vllm-project/vllm/issues/7178
该issue是一个BugFix类型的报告，涉及LM Eval Tests中DeepSeek所需的`trust_remote_code`标志的启用问题。这个问题由于没有在配置文件中添加可选标志导致。

https://github.com/vllm-project/vllm/issues/7177
这是一个功能需求类型的issue，该问题涉及VLLM模型库中是否支持添加mistral-large模型。原因可能是该模型与最接近的模型之间存在一些技术障碍导致的。

https://github.com/vllm-project/vllm/issues/7176
这是一个bug报告，涉及vLLM中FlashInfer backend生成错误输出的问题。由于切换了attn backend导致flash infer输出的结果不正确。

https://github.com/vllm-project/vllm/issues/7175
这是一个bug报告，主要涉及NeuronWorker这个对象，导致了TypeError: Can't instantiate abstract class NeuronWorker with abstract method execute_worker的问题。

https://github.com/vllm-project/vllm/issues/7174
这是一个用户提出需求的issue，主要涉及优化Hopper架构的线性核心Kernel。由于Hopper Nvidia发布了新的`wgmma`指令，Marlin (v1)无法达到最高FLOPs，因此引入了这个新的Kernel。

https://github.com/vllm-project/vllm/issues/7173
这是一个Bug报告，涉及Vllm在使用OpenAI模块时出现asyncio.exceptions.CancelledError异常的问题。

https://github.com/vllm-project/vllm/issues/7172
这是一个改进性质的issue，主要针对代码优化。原因是为了提高数据传输效率，采用了非阻塞数据传输功能。

https://github.com/vllm-project/vllm/issues/7171
这个issue是关于软件bug修复的问题，涉及到CUTLASS FetchContent的冲突。导致这个问题的原因是标签不受FetchContent与`GIT_SHALLOW`支持的限制。

https://github.com/vllm-project/vllm/issues/7170
这个issue是一个bug报告，涉及的主要对象是Suri vllm的markdown渲染问题，导致PR描述无法正确显示。

https://github.com/vllm-project/vllm/issues/7169
这是一个bug报告，主要涉及VLLM在使用不同的LORAS进行推断时出现的问题。由于模型只处理第一个LORA，导致第二个LORA的推断也来自于第一个LORA，可能是由于数据处理或模型逻辑的错误所致。

https://github.com/vllm-project/vllm/issues/7168
这是一个Bug报告，涉及的主要对象是添加Vision Language Models的Pipeline Parallelism支持中出现的错误。

https://github.com/vllm-project/vllm/issues/7167
这是一个bug报告，主要涉及到GroupCoordinator.recv()方法中的拼写错误。导致这个问题的原因是拼写错误导致markdown渲染失败。

https://github.com/vllm-project/vllm/issues/7166
这个issue类型为bug报告，涉及的主要对象是vLLM latest version在Inf2上运行失败，原因可能是版本兼容性问题导致无法运行。

https://github.com/vllm-project/vllm/issues/7165
这是一个需求修改类型的issue，涉及到更新ROCM基础镜像和添加OpenAI服务器入口点。

https://github.com/vllm-project/vllm/issues/7164
这个issue是一个Bugfix类型的问题，涉及的主要对象是InternVL2模型。由于输入处理程序的问题，导致图像token数量与图像嵌入大小不匹配，最终出现了错误。

https://github.com/vllm-project/vllm/issues/7163
这个issue是关于bug修复，涉及的主要对象是关于使用IP4 localhost form for zmq bind导致的问题。

https://github.com/vllm-project/vllm/issues/7162
这是一个性能优化类的issue，主要涉及到减少Python对象分配，包括针对特定对象类型的内存优化，以及调度器针对不同情况的优化。

https://github.com/vllm-project/vllm/issues/7161
这是一个Bug报告，涉及的主要对象是vllm。用户反映在使用Llama3.1进行推理时，容器崩溃，可能由于版本兼容性或其他问题导致这一bug。

https://github.com/vllm-project/vllm/issues/7160
这是一个bug报告，主要涉及到VLLM模型中图像token数量与图像嵌入大小不匹配的问题，可能是由于参数配置不正确导致的。

https://github.com/vllm-project/vllm/issues/7159
这个issue是关于[Core]类型的问题，主要对象是vLLM项目中的Ray Driver和Worker，由于并发轮询结果导致了NCCL死锁bug的情况。

https://github.com/vllm-project/vllm/issues/7157
这个issue是一个Bugfix类型的问题，主要涉及到前端工具功能的启用。原因是由于消息解析时未正确处理除`role`和`content`之外的字段，导致一些消息类型的信息丢失。

https://github.com/vllm-project/vllm/issues/7156
这是一个bug报告，主要涉及对象是VLLM代码。由于self._num_computed_tokens数量超出self.get_len()的范围，导致了assert错误。

https://github.com/vllm-project/vllm/issues/7155
该issue类型为性能优化提案，主要涉及最大模型长度参数对LLM类GPU利用的限制问题，提议修改GPU内存分配方式以改善资源利用效率。

https://github.com/vllm-project/vllm/issues/7154
这是一个类型为Bug的Issue，主要涉及vllm在langchain上无法识别llama3.1。原因可能是环境配置或代码逻辑问题。

https://github.com/vllm-project/vllm/issues/7153
这是一个bug报告，主要涉及LLaVA模型以及SigLIP编码器和替代解码器的支持。由于计算特征大小时出现的两个隐藏错误导致了占位符令牌数量的错误，需要修复这些问题。

https://github.com/vllm-project/vllm/issues/7152
这是一个Bug报告类型的Issue，主要涉及vllm与Gemma-2-9b-it在推理过程中产生了意外结果的问题，可能由于模型参数或推理机制不正确导致。

https://github.com/vllm-project/vllm/issues/7151
这是一个bug报告类型的issue，涉及的主要对象是vllm软件的更新版本0.5.3.post1。由于CUDA在forked subprocess中无法重新初始化而导致出现了异常信息。

https://github.com/vllm-project/vllm/issues/7150
这是一个Bug报告，涉及到vllm在推断过程中输出无限制问题。可能由于某种原因导致此Bug。

https://github.com/vllm-project/vllm/issues/7149
This is a bug report regarding the deployment function in the VLLM repository on GitHub, related to a divisible relationship requirement that is not met in the model structure.

https://github.com/vllm-project/vllm/issues/7148
这是一个关于使用vllm时生成响应时间过长的bug报告，主要涉及的对象是Internvl2模型。由于生成速度为0，请求永远无法完成。

https://github.com/vllm-project/vllm/issues/7147
这是一个关于性能优化的Issue，主要涉及Chunked-Prefill功能在高QPS下对ITL/e2e延迟的性能提升问题。由于VLLM 0.5.3版本和A100 GPU测试结果与预期相差较大，用户希望了解更多测试信息以及相关设置指导。

https://github.com/vllm-project/vllm/issues/7146
这是一个功能需求提交，涉及LoRA的支持扩展至64以上的Ranks功能。

https://github.com/vllm-project/vllm/issues/7145
这是一个bug报告，主要涉及到程序在运行两三次后报错，询问如何解决这个问题。

https://github.com/vllm-project/vllm/issues/7144
该issue类型为用户提出需求，主要涉及VLLM是否支持将siglip模型作为视觉模型在多模态模型中使用。原因可能是用户需要在特定模型上运行推断，但不清楚如何将其集成到VLLM中。

https://github.com/vllm-project/vllm/issues/7143
该issue为用户提出需求类型，主要涉及支持新模型"Mantis"。由于需要支持新的Siglip模型，用户提出了对该模型的支持需求。

https://github.com/vllm-project/vllm/issues/7142
这是一个BugFix类型的issue，涉及的主要对象是CUDA illegal memory access错误。发现该错误很可能是由于启用前缀缓存时传递给flash_attn内核的prefill_meta.block_table状态不一致引起的。

https://github.com/vllm-project/vllm/issues/7141
这是一个Bug报告，涉及到无法加载Gemma2模型的问题。由于传递给LLM类的参数中包含了不必要的参数，导致出现了RuntimeError错误。

https://github.com/vllm-project/vllm/issues/7140
这个issue属于bug报告类型，主要涉及到LLAMA（Versioned Long Long-Time Memory Attention）模型中的运行时错误。原因可能是在InferenceMode之外对推理张量进行原地更新，提出帮助寻求如何规避这一问题。

https://github.com/vllm-project/vllm/issues/7139
该issue类型为功能需求，涉及对象为版本更新以及与llama3.1 405b fp8 checkpoint的兼容性确认。由于需要确认特定版本的checkpoint是否与vLLM兼容，故用户目前等待确认。

https://github.com/vllm-project/vllm/issues/7138
这是一个bug报告，主要涉及的对象是vLLM项目中的ZMQError异常。原因可能是缺少设备导致该异常出现。

https://github.com/vllm-project/vllm/issues/7137
这是一个[Core]类型的issue，涉及的主要对象是改进代码中的采样核函数。由于目前使用的排序核较慢，用户提出使用更快的flashinfer采样核函数来减少GPU采样器的时间。

https://github.com/vllm-project/vllm/issues/7136
这是一个Bug报告，涉及的主要对象是在单GPU上部署离线推理时设置tensor_parallel_size=1导致模型加载失败。

https://github.com/vllm-project/vllm/issues/7135
这个issue属于bug报告类型，主要涉及vllm、xformers和vllm-nccl-cu12项目的安装过程。由于setuptools版本更新导致代码中使用了即将被弃用的选项，进而导致了构建wheel时出现错误。

https://github.com/vllm-project/vllm/issues/7134
这是一个bug报告，主要涉及到vllm安装过程中遇到的问题。由于版本不匹配导致无法安装特定版本的软件包。

https://github.com/vllm-project/vllm/issues/7133
这是一个关于如何构建类似AI-Ask功能的AI知识库的文档问题，提出者想了解如何使用vLLM实现，可能是由于缺乏相关指导文档导致提出帮助请求。

https://github.com/vllm-project/vllm/issues/7132
该issue类型为功能需求，涉及主要对象为 vllm 中的 run_batch API。由于目前的 run_batch API 只支持对话完成，导致用户需求添加对嵌入的支持及支持输入文件中空行的功能。

https://github.com/vllm-project/vllm/issues/7131
这个issue是一个RFC（Request for Comments），主要对象是vLLM插件系统的提案，由于用户需求增加了对vLLM的定制化需求，导致提出了这个插件系统的讨论。

https://github.com/vllm-project/vllm/issues/7130
这是一个用户提出需求类型的问题，主要涉及到需要添加一个插件。由于缺乏详细内容，无法确定具体原因导致的需求。

https://github.com/vllm-project/vllm/issues/7129
这是一个bug报告，涉及的主要对象是LoRA模块加载时的CUDA设备匹配问题。这个问题由于未明确指定设备导致加载LoRA和嵌入张量时出现CUDA设备不匹配的错误，并通过在加载张量时显式指定设备来解决。

https://github.com/vllm-project/vllm/issues/7128
这是一个文档修改类型的issue，涉及到vLLM库中文档标题的更新。原因是为了确保标题名称的清晰度。

https://github.com/vllm-project/vllm/issues/7127
这个issue是关于bug报告，主要涉及到在不同环境下pytorch寻找libcudart.so不同导致的问题。

https://github.com/vllm-project/vllm/issues/7126
这是一个功能需求的issue，主要涉及VLM核心模块在处理多模态输入时的计算问题。

https://github.com/vllm-project/vllm/issues/7125
这是一个功能需求类型的issue，主要涉及如何运行gemm2-27b模型的int4量化版本。

https://github.com/vllm-project/vllm/issues/7124
该issue是用户提出了需求，主要涉及模型架构插件，由于之前的合并请求被拒绝，导致用户需要通过自己的分支来支持特定的场景。

https://github.com/vllm-project/vllm/issues/7123
这是一个用户提出需求的issue，主要涉及替换调度程序。原因是一些使用案例需要优先考虑其他指标而不是最大吞吐量，例如在不同用户请求之间维持公平性。

https://github.com/vllm-project/vllm/issues/7122
这是一个功能增强的issue，主要涉及Minicpmv模型在离线推理中添加多图支持。

https://github.com/vllm-project/vllm/issues/7121
这是一个Bug报告，主要涉及的对象是vllm项目的api_server.py文件。导致问题的原因是未使用args.trust_remote_code参数，导致出现了功能异常。

https://github.com/vllm-project/vllm/issues/7120
这是一个性能问题报告，主要涉及了Llama2-7b-hf模型和Llama2-7b-chat-hf模型的推理速度差异。可能是由于模型结构或优化方式不同导致性能差异。

https://github.com/vllm-project/vllm/issues/7119
这是一个需求类型的issue，主要涉及collect env的功能。

https://github.com/vllm-project/vllm/issues/7118
这是一个Bug报告，主要涉及ZMQ错误导致的设备无法找到，可能是由于地址错误或配置问题导致的。

https://github.com/vllm-project/vllm/issues/7117
这是一个用户提出需求的issue，主要涉及VLLM模型的代码输出处理。由于存在非并行抽样和非束搜索的情况，用户希望简化输出处理流程。

https://github.com/vllm-project/vllm/issues/7116
这是一个性能改进的提议，涉及到vLLM中的SequenceGroup和Sequence的概念转换效率问题，由于在函数操作时使用的是SequenceGroup级别，而实际逻辑更适合使用Sequence级别操作，导致代码效率低下且难以维护。

https://github.com/vllm-project/vllm/issues/7115
这是一个Bug报告，涉及PaliGemma检测任务失败，可能由于GemmA处理器中的额外loc标记导致输出问题。

https://github.com/vllm-project/vllm/issues/7114
这个issue是关于技术改进，主要对象是CI/测试系统的优化，通过新进程方式来执行测试。

https://github.com/vllm-project/vllm/issues/7112
这是一个Bug报告，主要涉及vllm-flash-attn包无法与FlashAttention-2后端一起使用，原因可能是环境配置问题。

https://github.com/vllm-project/vllm/issues/7111
这是一个BugFix类型的issue，主要涉及到`AsyncLLMEngine`的异步请求取消机制存在问题，导致在客户端断开连接时出现一系列错误及不一致的行为。

https://github.com/vllm-project/vllm/issues/7110
这个issue主要是一个功能需求提出，主要涉及vLLM下的一些基本自定义操作的优化。

https://github.com/vllm-project/vllm/issues/7109
这个issue属于性能优化类型，主要涉及SPMD架构和序列化性能优化，用户提出了优化性能、集成功能和运行基准测试等建议。

https://github.com/vllm-project/vllm/issues/7108
该issue类型为性能问题报告，主要涉及vLLm的性能表现与sglang的比较，由于性能不佳需要进行优化，可能存在性能回归问题。

https://github.com/vllm-project/vllm/issues/7107
这是一个bug报告，涉及的主要对象是vLLM API服务器。由于GPU数量不足导致数目要求超过了可用GPU总数，产生了数值错误(ValueError)。

https://github.com/vllm-project/vllm/issues/7106
这是一个bug报告，主要涉及的对象是vllm镜像的构建环境。该问题由于使用的基础镜像存在多个漏洞导致，用户请求修复。

https://github.com/vllm-project/vllm/issues/7105
这是一个针对bug的报告，主要涉及的对象是`speculative_draft_tensor_parallel_size`参数，由于在使用MLPSpeculator时，默认值未设置导致出现问题。

https://github.com/vllm-project/vllm/issues/7104
这个issue是关于关闭H100性能基准测试的问题，主要是由于当前H100代理无法工作导致。

https://github.com/vllm-project/vllm/issues/7103
这是一个Bug报告，涉及VLLM服务器的多Lora请求出现了错误。

https://github.com/vllm-project/vllm/issues/7102
该issue为代码优化建议，主要涉及硬件平台上的is_tpu使用问题，用户建议统一使用current_platform.is_tpu()。

https://github.com/vllm-project/vllm/issues/7101
这是一个关于测试优化的issue，主要涉及CI和前端，通过使用parametrized fixture进行测试。

https://github.com/vllm-project/vllm/issues/7100
这是一个文档更新类型的issue，主要涉及MLPSpeculator相关文档内容的更新。由于需要添加MLPSpeculator的相关文档和链接，可能是为了使项目更易于理解和使用。

https://github.com/vllm-project/vllm/issues/7099
这是一个bug报告类型的issue，涉及的主要对象是Ray DAG测试。问题是由于Pull Request中的更改可能与adag不兼容，导致执行一个ray adag测试后出现错误，需要进行进一步调查。

https://github.com/vllm-project/vllm/issues/7098
这个issue是关于bug报告，主要涉及的对象是CI系统中的等待时间过长问题，导致所有测试失败。

https://github.com/vllm-project/vllm/issues/7097
这是一个用户提出的需求性issue，主要涉及了ci以及distributed方面的测试命令合并。由于项目需要优化测试流程，用户提出了合并分布式测试命令的需求。

https://github.com/vllm-project/vllm/issues/7095
该issue类型为重复应用代码，涉及的主要对象为前端代码中运行uvicorn的代码。Bug报告。

https://github.com/vllm-project/vllm/issues/7094
这是一个Bug报告issue，主要涉及的对象是llava HFValidationError OSError。用户无法加载llava模型，尝试了不同版本仍然无效，提问是否最近有所更改。

https://github.com/vllm-project/vllm/issues/7093
这是一个bug报告，涉及的主要对象是vllm 0.5.2，由于python程序运行vllm时退出会导致WSL崩溃。

https://github.com/vllm-project/vllm/issues/7092
这个issue类型为功能需求，主要对象是代码库中的编译功能。原因是为了让测试变得更加方便。

https://github.com/vllm-project/vllm/issues/7091
这个issue是关于[Revive to use loopback address for driver IP]的bug报告，涉及到vLLM项目中的驱动IP。由于单个GPU节点，导致了未能正确使用环回地址作为驱动IP。

https://github.com/vllm-project/vllm/issues/7090
这个issue类型属于Bugfix，涉及主要对象是CUDA图生成数量错误，导致了speculative decoding时产生了错误数量的CUDA图。

https://github.com/vllm-project/vllm/issues/7089
这是一个关于性能优化的issue，涉及VLLM模型的forward时间统计，由于未能在没有同步时正常工作，可能导致了一些潜在的bug或性能问题。

https://github.com/vllm-project/vllm/issues/7088
这是一个关于bug修复的issue，主要涉及替换自定义的`blockReduce`函数为`cub::BlockReduce`，原因是原先的函数存在bug。

https://github.com/vllm-project/vllm/issues/7087
这是一个bug报告，主要涉及的对象是在安装vllm时出现的错误。由于编译指令不支持对应的架构，导致在特定GPU设备上出现"CUDA error: no kernel image is available for execution on the device"的错误。

https://github.com/vllm-project/vllm/issues/7086
这是一个功能需求的issue，涉及到GPTQModel的动态量化功能的支持，主要是为了实现对特定层和模块的量化配置进行覆盖控制。

https://github.com/vllm-project/vllm/issues/7085
这是一个关于更新CUTLASS到3.5.1版本的问题，涉及的主要对象是代码库中的CUDA核心。原因是CUTLASS 3.5.1 中包含了修复上游`RowBroadcast`的问题，需要更新至最新的main分支以解决相关问题。

https://github.com/vllm-project/vllm/issues/7084
这是一个bug报告，涉及的主要对象是Shieldgemma模型。由于配置文件中指定了transformers版本为4.42.4，可能导致无法成功运行Shieldgemma模型。

https://github.com/vllm-project/vllm/issues/7083
这是一个Bug报告，主要涉及到VLLM中的InternVL2 Inference功能。由于GPU执行器无法找到可执行该计算的引擎，导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/7082
这是一个bug报告，主要涉及vLLM测试中出现的内存泄漏问题导致的flaky错误。

https://github.com/vllm-project/vllm/issues/7081
此issue是一个bug报告，涉及到ReplicatedLinear类不完全支持LoRA，导致未来的VL模型可能无法充分使用LoRA功能。

https://github.com/vllm-project/vllm/issues/7080
这是一个bug报告，主要涉及前端功能。由于用户指定的 `MAX_SEQUENCE_LENGTH` 值超过了模型的最大序列长度导致错误，现在已更改为警告以避免产生意外行为或 CUDA 错误。

https://github.com/vllm-project/vllm/issues/7079
这是一个技术性问题，涉及到GitHub上vLLM项目中Integrate fused Mixtral MoE with Marlin kernels的开发内容。

https://github.com/vllm-project/vllm/issues/7078
这个issue是一个[Frontend]类型的需求报告，主要涉及的对象是OpenAI API server。因为缺乏readiness和liveness endpoints导致Kubernetes无法确定vLLM是否准备好接收请求或者是否仍处于运行状态。

https://github.com/vllm-project/vllm/issues/7077
这是一个Bug报告，主要涉及到vllm的运行环境和API服务器的异常行为。可能由于配置参数设置不当或代码逻辑错误导致了API服务器的异常行为。

https://github.com/vllm-project/vllm/issues/7076
这是一个关于增加安全层的建议，属于用户提出需求类型的issue，主要涉及的对象是系统安全性。

https://github.com/vllm-project/vllm/issues/7075
这是一个Bug报告，涉及的主要对象是DeepSeek-V2-Lite quantized模型，由于模型形状错误导致产生了错误。

https://github.com/vllm-project/vllm/issues/7074
这是一个bug报告，涉及到vLLM在使用过程中出现的错误。由于网络接口配置错误导致Gloo连接失败和设备初始化问题。

https://github.com/vllm-project/vllm/issues/7073
这个issue属于Bug报告类型，主要涉及服务器在处理多个请求时出现OutOfMemoryError。由于服务器在处理多个请求时出现内存溢出错误，导致运行失败。

https://github.com/vllm-project/vllm/issues/7072
这个issue类型是bug报告，主要涉及的对象是PyTorch环境的配置，由于PyTorch版本与CUDA版本不匹配导致了bug。

https://github.com/vllm-project/vllm/issues/7071
这是一个需求类型的issue，主要涉及的对象是vLLM前端（例如OpenAI API server），用户提出了添加一个abort_request端点的需求。

https://github.com/vllm-project/vllm/issues/7070
这是一个Bug报告，涉及vLLM库中加载gemma29b微调模型时出现的错误。由于`paged_kv_indptr`的大小与`batch_size + 1`的大小不匹配而导致`RuntimeError`错误。

https://github.com/vllm-project/vllm/issues/7069
这个issue是一个不再需要的功能移除请求，涉及的主要对象是一个名为"error_on_invalid_device_count_status"的功能。

https://github.com/vllm-project/vllm/issues/7068
这是一个关于使用OpenAI API调用时出错的bug报告，主要涉及对象是Phi-3-vision-128k-instruct模型。由于在使用不同格式的prompts时都出现相同的错误，可能是由于参数传递或者模型接受格式不正确导致的问题。

https://github.com/vllm-project/vllm/issues/7067
这个issue是一个[Model]类型的问题， 主要涉及的对象是`InternVLChatModel`。由于`load_weights`方法与`Qwen2`、`Llama`和`InternLM2`的权重加载逻辑耦合严重，导致难以维护。

https://github.com/vllm-project/vllm/issues/7066
这个issue是一个关于优化文件大小的杂项类型的PR。

https://github.com/vllm-project/vllm/issues/7065
这个issue是一个bug报告，主要涉及的对象是`tracing.py`文件。出现这个bug的原因是导入opentelemetry的问题，在markdown渲染时无法显示正确的内容。

https://github.com/vllm-project/vllm/issues/7064
这是一个关于构建/CI Bug的报告类型的issue，涉及到的主要对象是PyTorch的环境配置。由于环境配置中的PyTorch版本与CUDA版本不兼容，导致了构建问题。

https://github.com/vllm-project/vllm/issues/7063
这是一个用户提出需求的issue，主要涉及VLLM中的BitAndBytes量化模型无法直接在多GPU上使用，可能由于VLLM当前不支持BNB量化模型在多GPU上运行导致。

https://github.com/vllm-project/vllm/issues/7062
这个issue是关于对服务基准测试中p90和p95进行更多百分位数的需求，不是关于bug报告。

https://github.com/vllm-project/vllm/issues/7061
这是一个Bug报告，该问题涉及到使用VLLM中的Guided Decoding时出现的类型错误，由于试图序列化未支持的对象类型导致无法正常运行。

https://github.com/vllm-project/vllm/issues/7060
这是一个关于支持新模型的用户提出需求问题，涉及的主要对象是模型`gemma-2-2b-it`，用户希望能够获得更快的推断速度。

https://github.com/vllm-project/vllm/issues/7059
这是一个Bug报告，主要对象是Qwen2模型在将tensor_parallel_size设置为1时没有相应，导致代码在特定部分无法继续执行。

https://github.com/vllm-project/vllm/issues/7058
这是一个bug报告，主要涉及的对象是在vllm下使用Qwen2模型时，当tensor_parallel_size设置为1时无法获取响应。这个问题可能由于模型参数配置不正确导致。

https://github.com/vllm-project/vllm/issues/7057
这是一个关于使用问题的bug报告，涉及主要对象为vllm，可能由于配置参数设置不当导致了失败的情况。

https://github.com/vllm-project/vllm/issues/7056
这个issue属于性能优化类型，主要涉及的对象是mypy的检查过程。这个问题是因为mypy在本地运行时速度过慢，提出了一种优化方案来加快检查速度。

https://github.com/vllm-project/vllm/issues/7055
这是一个改进需求（Feature Enhancement），主要涉及到前端（Frontend）的聊天消息解析（chat message parsing）。

https://github.com/vllm-project/vllm/issues/7054
这个issue是一个Bug报告，主要涉及的对象是CI的分布式测试。由于可能的代码实现问题，导致了CI中pp测试的失败。

https://github.com/vllm-project/vllm/issues/7053
这是一个关于测试问题的bug报告，涉及到了测试资源清理方面的困难。这个问题是由于在分布式推理中清理资源遇到困难而导致的。

https://github.com/vllm-project/vllm/issues/7052
这是一个需求提出类型的issue，主要涉及vllm项目下的core模块。由于需要在core内部获取kv_cache而不经过worker，因此实现了get_kv_from_block方法。

https://github.com/vllm-project/vllm/issues/7051
这是一个性能优化类型的issue，主要涉及到了`seq_group.get_seqs()`方法的优化。原因是在方法中引入了`seqs: List[Sequence]`来提供性能提升。

https://github.com/vllm-project/vllm/issues/7050
此issue属于Bug报告类型，主要涉及到在设置SamplingParams参数`n`值大于200时，vllm会停止（没有GPU活动），导致无法进行生成操作。

https://github.com/vllm-project/vllm/issues/7049
该issue是关于新功能的实现，主要涉及修改VLLM中的异步输出处理器。由于GPU总是在CPU之前运行，通过在每个解码步骤中完全重叠的cuda图前向传递，使得吞吐量提高了11%。

https://github.com/vllm-project/vllm/issues/7048
这个issue是一个bug报告，涉及到在sagemaker中运行vllm时出现了"Input prompt (9762 tokens) is too long and exceeds limit of 8192"的错误。

https://github.com/vllm-project/vllm/issues/7047
这是一个bug报告类型的issue，主要涉及VLLM模型在接收到第一个输入消息"When are you?"后出现了IndexError导致程序崩溃的问题。

https://github.com/vllm-project/vllm/issues/7046
这是一个bug报告，涉及v1/chat/completion" API endpoint，由于并发请求导致了"Error executing method start_worker_execution_loop"错误。

https://github.com/vllm-project/vllm/issues/7045
这是一个关于RFC（Request For Comments）的issue，涉及`--engine-use-ray`选项的deprecation和removal。由于Ray选项未被广泛使用且性能方面存在问题，因此提议移除该选项。

https://github.com/vllm-project/vllm/issues/7044
这个issue类型是Bug报告，涉及的主要对象是更新run-amd-test.sh脚本。由于未填写PR描述，在提交之前未按照PR检查列表进行检查。

https://github.com/vllm-project/vllm/issues/7043
这是一个CI/Build类型的issue，涉及到Docker Hub配额问题导致构建失败。

https://github.com/vllm-project/vllm/issues/7041
这个issue是一个bug报告，涉及的主要对象是multiprocessing机制。原因是由于不正确地使用守护线程和资源回收导致了进程挂起的问题。

https://github.com/vllm-project/vllm/issues/7040
这是一个用户提出需求的issue，主要涉及的对象是vllm和amazon/FalconLite2模型。用户想要在vllm中使用amazon/FalconLite2模型进行性能基准测试，但是发现该模型不受支持。

https://github.com/vllm-project/vllm/issues/7039
这是一个Bug报告类型的Issue，描述了在使用`pipeline_parallel_size=2`时，当prompt使用的标记超过可用标记数量一半时会出现错误。原因是检查输入prompt长度时，将可用块的数量减半导致错误。

https://github.com/vllm-project/vllm/issues/7038
这是一个用户提出需求的issue，主要涉及的对象是使用Falconlite2进行vllm benchmarking。用户提出了想要同时使用falconlite2和vllm进行基准测试，但由于vllm尚不支持falconlite2，无法实现该需求。

https://github.com/vllm-project/vllm/issues/7037
这个issue类型是优化建议，涉及对象是CI/Build流程。由于已经有了`lmevalharness`对compressedtensors检查点的覆盖率，因此没有必要保留这个测试和依赖关系。

https://github.com/vllm-project/vllm/issues/7036
这是一个bug报告，主要涉及VLLM项目中的测试失败问题，由于无法对对象进行pickle处理，需要一种绕过此问题的临时解决方案。

https://github.com/vllm-project/vllm/issues/7035
这个issue为CI/Build bug报告类型，主要涉及Python 3.12支持的问题，由于缺少对应依赖库的更新，导致无法通过CI构建。

https://github.com/vllm-project/vllm/issues/7034
这是一个Bugfix类型的issue，主要涉及CUTLASS header-only line的问题。由于CUTLASS_ENABLE_HEADERS_ONLY变量没有被CUTLASS构建系统正确识别，导致构建了大量无需的CUTLASS目标文件，使得compile_commands.json文件变得巨大。

https://github.com/vllm-project/vllm/issues/7033
这个issue属于用户提问类型，主要对象是VLLM模型的特殊 tokens 输出问题，可能是由于更新VLLM到最新版本后导致的。

https://github.com/vllm-project/vllm/issues/7032
这个issue属于bug报告类型，主要涉及的对象是在AMD GPU上使用vLLM 3.1 405 B版本的FP16模型加载失败。这个问题可能由于网络请求出现错误和本地vLLM worker进程异常结束导致。

https://github.com/vllm-project/vllm/issues/7031
这是一个bug报告，涉及的主要对象是vLLM中的模型Metallama3.1 405 B FP8以及AMD GPU，导致的问题是在ROCm环境下无法加载该模型，提示fbgemm_fp8量化在ROCm中不受支持。

https://github.com/vllm-project/vllm/issues/7030
这是一个用户需求问题，主要涉及使用vllm进行Mistral 7B的离线推理时如何传递JSON内容类型。用户希望通过`generate`方法使用JSON模式，但仅使用提示似乎无法产生所需的JSON输出。

https://github.com/vllm-project/vllm/issues/7029
这是一个Bug报告，涉及到vllm和transformers的模型输出不一致。可能是由于版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/7028
这是一个关于GPU内存使用异常的bug报告，用户遇到了降低gpu_memory_utilization数值后无法加载模型的问题。

https://github.com/vllm-project/vllm/issues/7027
这是一个请求清理代码的Issue，涉及主要对象为Punica C信息。由于最近合并了一个相关PR并进行了重构，因此需要清理剩余的Punica C信息。

https://github.com/vllm-project/vllm/issues/7026
这个issue是一个PR（Pull Request）类型描述问题，主要对象是vLLM项目下的代码贡献。由于缺少Markdown渲染而导致无法正确显示PR Checklist和相关内容。

https://github.com/vllm-project/vllm/issues/7025
这是一个bug报告，主要涉及安装vllm时出现的OSError导致"No space left on device"错误。

https://github.com/vllm-project/vllm/issues/7024
这是一个Bug报告，涉及的主要对象是vllm llama3/3.1-8b的响应问题。由于设置了max_tokens参数为任意大小时，响应内容被截断，导致无法获取完整信息。

https://github.com/vllm-project/vllm/issues/7023
这是一个[Core] 类型的issue，主要涉及到 sampler 中未使用的代码导致性能问题。

https://github.com/vllm-project/vllm/issues/7022
这个issue类型是功能增强，主要涉及的对象是FlashAttention后端。这个问题由于attention logits softcapping的支持不完整导致Gem2模型运行时出现多个问题，用户提出了此需求以解决这些问题。

https://github.com/vllm-project/vllm/issues/7021
这是一个feature类型的issue，针对的主要对象是Frontend，关于往服务器中添加安全机制的问题。新增的安全机制使用FastAPI的依赖注入来替换之前的中间件验证方式，主要改进包括：在OpenAPI模式中正确记录了授权头信息；API密钥现在可以在带有API密钥的服务器上使用Swagger `/docs`；未来对新路由添加身份验证保护将更加容易。

https://github.com/vllm-project/vllm/issues/7020
这是一个需求调整（Feature Request）类型的issue，涉及主要对象是MiniCPMV，由于未来功能（LoRA 和 BNB）的支持需求，导致需要将MiniCPMV的不同版本分离以方便支持。

https://github.com/vllm-project/vllm/issues/7019
这是一个Bug报告，涉及的主要对象是LLMEngine。由于缺少一个必要的参数 'prompt_adapter_request'，导致出现了 TypeError: LLMEngine._add_processed_request() missing 1 required positional argument 错误。

https://github.com/vllm-project/vllm/issues/7018
这是一个Bugfix类型的issue，主要涉及vLLM中的block table，并且由于未正确传播block table导致了flash attention生成不正确的结果或者出现无效内存访问。

https://github.com/vllm-project/vllm/issues/7017
这是一个用户提出需求的issue，涉及的主要对象是Qwen-VL-Chat模型，由于该模型在vllm API中不受支持导致无法执行相关代码。

https://github.com/vllm-project/vllm/issues/7016
这是一个用户提出需求的issue，主要涉及到在vLLM中实现无分类器指导的功能。用户之前在issue中提到了由于论文作者提出的一个问题，现在已经根据另一个用户的建议实现了一个版本，并希望能够得到一些建议。

https://github.com/vllm-project/vllm/issues/7015
这是一个bug报告，涉及到vllm llama3/3.1-8b，用户提到调用llama3时响应总是被截断，寻求解决方法。

https://github.com/vllm-project/vllm/issues/7014
这个issue是性能优化类型，主要涉及的对象是代码中的`prepare_input`函数，是由于性能考虑而提出修改建议。

https://github.com/vllm-project/vllm/issues/7013
这是一个bug报告类型的issue，涉及主要对象为在使用Ray进行分布式推断时引起的CUDA错误。由于运行环境中缺少CUDA，在导入torch后调用torch.cuda.is_available()导致了这一bug。

https://github.com/vllm-project/vllm/issues/7012
这是一个技术修改类的issue，主要涉及vllm项目中前端代码中运行uvicorn的代码重构回滚操作。

https://github.com/vllm-project/vllm/issues/7011
这是一个bug报告，涉及的主要对象是在ROCm上运行多个GPTQ模型时出现了令牌生成失败的问题。这个问题可能是由于模型加载过程中持续生成令牌直至达到令牌限制，导致即使传递参数如温度也无效。

https://github.com/vllm-project/vllm/issues/7010
这是一个关于在Github上vLLM项目的一个issue，主要类型是技术改进（Kernel），涉及添加了Fused Layernorm + Asymmetric int8 Quant。原因可能是为了改善代码质量和审阅流程效率。

https://github.com/vllm-project/vllm/issues/7009
这是一个Bug报告，主要涉及vllm下的llamaindexllmsollama项目中在和Ray集成时出现的错误，错误的原因是在使用TinyLlama模型时遇到了`AttributeError: 'dict' object has no attribute 'kwargs'`异常。

https://github.com/vllm-project/vllm/issues/7008
这是一个bug报告类型的issue，涉及主要对象是Kernel，因为代码中的输入问题导致在flashinfer prefill wrapper中出现错误。

https://github.com/vllm-project/vllm/issues/7007
这是一个bug报告issue，主要涉及到`get_beam_search_score`函数中当`seq_len`参数为`None`时返回错误评分的问题。这个问题是因为`seq_len`默认设置为输出和输入（或提示）标记的长度，而应该默认设置为只有输出标记的长度导致的。

https://github.com/vllm-project/vllm/issues/7006
这是一个用户提出需求的issue，主要涉及的对象是在优化的量化列表中添加压缩张量，其原因是目前列表中只包含了下划线版本而未包含短横线版本导致。

https://github.com/vllm-project/vllm/issues/7005
这是一个功能需求类型的issue，主要涉及TPU后端加载时W8A16量化的添加。这个问题是由于需要为TPU后端添加新的加载时int8权重量化功能，以优化模型的性能。

https://github.com/vllm-project/vllm/issues/7004
这是一个Bug报告，涉及的主要对象是Mistral Nemo Instruct在使用`guided_json`时几乎从不返回JSON输出。这可能是由于Mistral Nemo不支持使用vLLM生成JSON输出所导致的。

https://github.com/vllm-project/vllm/issues/7003
这是一个Bug报告，涉及的主要对象是VLLM。由于启用了`enable_prefix_caching=True`，导致VLLM在使用异步引擎时崩溃，但当将其设为`enable_prefix_caching=False`时问题消失。

https://github.com/vllm-project/vllm/issues/7002
这个issue是bug报告，涉及主要对象是Gem，由于加载FP8 quantization时遇到"unloaded_params"异常，导致Gemma加载失败，提出降低该异常级别至警告以解决问题。

https://github.com/vllm-project/vllm/issues/7001
这个issue类型是CI/Build改进，该问题单涉及的主要对象是vLLM代码库的构建和持续集成（Continuous Integration），由于缺乏对除零和缺少返回语句警告的处理，导致了编译警告的出现。

https://github.com/vllm-project/vllm/issues/7000
这是一个功能需求类型的issue，主要涉及到vLLM项目中的Multi Step Scheduling。原因是为了增加对多步调度的支持。

https://github.com/vllm-project/vllm/issues/6999
这是一个关于构建设置的问题，涉及到在使用旧版本CMake构建时的依赖错误，导致构建失败。

https://github.com/vllm-project/vllm/issues/6998
这是一个校对文档内容的issue，属于用户提出需求类型，主要涉及的对象是文档内容。这个问题由于语法、标点、句子风格和格式问题导致了需要修正和校对的情况。

https://github.com/vllm-project/vllm/issues/6997
这是一个文档更新issue，主要涉及的对象是更新"run-amd-test.sh"脚本，原因是为了解决docker pull限制问题。

https://github.com/vllm-project/vllm/issues/6996
这是一个关于Kernel的issue，涉及T4显卡(Int8 Cutlass Kernels for SM75)的性能优化以及PR提交准则，表明开发者希望针对某些Gem的形状进行优化配置，由于针对不同M维度大小的Gem形状，性能可能出现不同程度的变化，并要求提交的PR满足特定标准。

https://github.com/vllm-project/vllm/issues/6995
这是一个bug报告，主要涉及的对象是MiniCPM-V文件，由于各种类型检查错误导致了bug。

https://github.com/vllm-project/vllm/issues/6994
这个issue属于需求提出类型，主要涉及到vLLM编译器的警告标准的更严格设置。由于过多警告会掩盖真正问题并影响视觉效果，开发人员在构建源代码时遇到了大量可能是除零错误的警告导致了这一需求。

https://github.com/vllm-project/vllm/issues/6993
这是一个用户提出需求的issue，主要对象是控制在进行请求时是否使用预测解码，用户提出这个需求的原因是需要在某些输入情况下禁用预测解码。

https://github.com/vllm-project/vllm/issues/6992
这个issue是一个bug报告，涉及到InternViT attention qk_layernorm中的RMSNorm前向传播出现问题，原因是使用了融合的`RMSNorm`导致了严重的数值发散，可能导致了图像幻觉。

https://github.com/vllm-project/vllm/issues/6991
这是一个bug报告，主要涉及到TypeError: 'NoneType' object is not callable错误。由于环境问题导致了该错误。

https://github.com/vllm-project/vllm/issues/6990
这个issue是一个用户需求报告，主要对象是vllm引擎，用户提出需要添加对Python 3.12的支持。

https://github.com/vllm-project/vllm/issues/6989
这是一个bug报告，主要涉及对象是vllm库中的internVL2模型推理不支持的问题。由于当前使用的模型架构不在支持列表中，导致出现数值错误的bug。

https://github.com/vllm-project/vllm/issues/6988
这是一个Bug报告，涉及的主要对象是vllm项目下的一个CUDA编译错误。这个问题可能是由于CUDA版本不匹配导致的编译错误。

https://github.com/vllm-project/vllm/issues/6987
这是一个Bug报告，涉及的主要对象是vllm服务。由于无法导入名称'default_dump_dir'，导致vllm server运行时报错。

https://github.com/vllm-project/vllm/issues/6986
这是一个关于模型支持的问题，用户在使用DeepSeekCoderV2LiteInstruct和DeepSeekV2LiteChat模型时遇到了不支持的错误。

https://github.com/vllm-project/vllm/issues/6985
这是一个Bug报告，涉及的主要对象是AutoAWQ marlin methods和vLLM。由于环境中使用的PyTorch版本与CUDA版本不匹配，导致出现冲突。

https://github.com/vllm-project/vllm/issues/6983
这是一个bug报告，涉及的主要对象是VLLM中的LoRA权重设置。由于在使用LoRA进行模型训练时，出现了不支持的LoRA权重设置，导致了报错和异常现象。

https://github.com/vllm-project/vllm/issues/6982
这是一个bug报告，主要涉及的对象是 feature size calculation，由于依赖于CUDA浮点误差导致在某些特定图像分辨率下出现问题，使得模型在其他设备上运行时出现图像占位符令牌数量不正确的bug。

https://github.com/vllm-project/vllm/issues/6981
这是一个bug报告，涉及的主要对象是在TPU上使用`load_format="dummy"`。导致此bug的原因是TPU不支持`torch.Generator`，因此在TPUs上`load_format="dummy"`功能无法正常工作。

https://github.com/vllm-project/vllm/issues/6980
这个issue类型是bug报告，涉及主要对象为TPU设备。由于缺少readonly=True设置，导致非根设备可能出现错误的bug，需要进行修复。

https://github.com/vllm-project/vllm/issues/6979
这是一个文档问题，涉及的主要对象是相关硬件支持情况，由于对量化方法支持理解存在疑惑，导致提出了关于量化操作在特定GPU上支持情况的问题。

https://github.com/vllm-project/vllm/issues/6978
这是一个功能需求的issue，主要涉及的对象是为VLLM项目添加Fused MoE W8A8(Int8)支持，由于当前Int8 MoE比使用bf16与Fused MoE组合的速度更慢，需要进行更多优化以提升性能。

https://github.com/vllm-project/vllm/issues/6977
这是一个Bug报告，涉及到vllm中InternVL2模型出现图像幻觉错误的问题。原因是使用了错误的ViT模型导致。

https://github.com/vllm-project/vllm/issues/6976
这是一个Bug报告，主要涉及vllm中的CUDA错误导致的内存访问问题。

https://github.com/vllm-project/vllm/issues/6975
这是一个用户提出需求的issue，主要涉及如何在vllm中主动终止请求和停止推理过程，可能出现由于推理时间过长或生成的token重复导致无法停止推理的情况。

https://github.com/vllm-project/vllm/issues/6974
这是一个用户提出需求的类型。该问题涉及的主要对象是在VLLM下添加对QWen 1模型的支持。

https://github.com/vllm-project/vllm/issues/6973
这个issue是关于文档集成的问题，属于[Misc]类型，主要涉及 llama_index serving 集成。

https://github.com/vllm-project/vllm/issues/6972
这个issue是关于提交文档修改的，不是一个bug报告，涉及到LLM服务文档的更新。原因是markdown渲染存在问题，需要使用raw HTML替代。

https://github.com/vllm-project/vllm/issues/6971
这是一个关于性能优化的issue，针对vLLM中多步推理过程中的一个潜在问题。

https://github.com/vllm-project/vllm/issues/6970
这是一个用户提出需求的issue，主要涉及在服务器上添加安全方案，由于当前身份验证方法没有在自动生成的openapi.json文档中记录，导致Swagger页面无法提供登录或授权头的功能。

https://github.com/vllm-project/vllm/issues/6969
这是一个Bug类型的Issue，涉及对象是代码的执行环境，由于代码无法正常工作导致出现错误。

https://github.com/vllm-project/vllm/issues/6968
这个issue是一个bug报告，涉及的主要对象是CI/Build。由于CC开发过程中的更改导致了mypy错误增加，造成了CI失败。

https://github.com/vllm-project/vllm/issues/6967
这是一个Bug报告，主要涉及在线模式下的speculative decoding无法工作，用户寻求关于在线speculative decoding示例的帮助。

https://github.com/vllm-project/vllm/issues/6965
这是一个bug报告，主要涉及的对象是夜间基准测试套件。由于无法在某些情况下正常工作，需要调整为使用`kill 9 [gpu proc pid]`替代`pkill python`。

https://github.com/vllm-project/vllm/issues/6964
这个issue类型为性能优化，主要涉及LLAMA3 70b + speculative decoding，在使用`turboderp/Qwama0.5BInstruct`模型时QPS为2。

https://github.com/vllm-project/vllm/issues/6963
这是一个用户提出需求的issue，主要对象是在vllm下的Speculative decoding模块。由于缺乏时间记录，用户希望添加周期性记录，以便测量提议每个令牌所花费的平均时间、评分所花的时间以及验证所花费的时间。

https://github.com/vllm-project/vllm/issues/6962
这个issue是一个功能需求，主要涉及到punica kernel和IBM granite 20b模型的适配。

https://github.com/vllm-project/vllm/issues/6961
这是一个关于暂时禁用内核和LoRA测试的构建问题，而不是Bug报告。这个问题涉及到CI成本节约措施，需要开发者在本地运行测试来确认代码。

https://github.com/vllm-project/vllm/issues/6960
这是一个Bugfix类型的issue，涉及主要对象是支持cpu offloading的quantization方法。由于参数未能正确保留权重张量上的属性，导致BNB在此问题上仍不受支持。

https://github.com/vllm-project/vllm/issues/6959
这个issue是一个关于CI/Build（持续集成/构建）的问题，主要涉及到ROCm（Radeon开源显卡驱动平台）。可能是由于未启用tensorizer测试，导致需要修复的问题。

https://github.com/vllm-project/vllm/issues/6958
这个issue类型是技术提案，涉及异步输出处理的POC，作者试图使用asyncio来实现。

https://github.com/vllm-project/vllm/issues/6957
这是一个bug报告，涉及vllm 0.5.3post1版本无法启动带有gemma227bitFP8D模型的api_server，可能是由于模型量化或代码问题导致的。

https://github.com/vllm-project/vllm/issues/6956
这是一个bug报告，涉及的主要对象是vllm中的placement group创建方式。原因是在某些情况下，未正确管理placement group可能导致ValueError异常。

https://github.com/vllm-project/vllm/issues/6955
这是一个性能优化建议的issue，问题主要涉及VLLM在使用TensorParallel时处理token性能问题，原因可能是处理性能在tensor-parallel上缩放不佳。

https://github.com/vllm-project/vllm/issues/6954
这是一个bug报告类型的issue，涉及的主要对象是vLLM的代码中的一个模块。导致这个bug的原因是在一个重构过程中未正确设置`SamplingParams.max_tokens`，导致引擎在未定义`max_tokens`时出现崩溃现象。

https://github.com/vllm-project/vllm/issues/6953
这是一个Bug报告，主要涉及vllm在使用response_format类型为json_object时发生崩溃，其他response_format类型和无response_format字段正常工作。

https://github.com/vllm-project/vllm/issues/6952
这是一个Bug报告类型的Issue，主要涉及vLLM的CPU Offloading功能。用户在调整`cpu_offload_gb`参数时出现了`RuntimeError: b_q_weight is not on GPU`错误，尝试了不同的参数设置均未成功。

https://github.com/vllm-project/vllm/issues/6951
这是一个更新请求类型的issue，涉及主要对象是项目中的PyTorch依赖。

https://github.com/vllm-project/vllm/issues/6950
这是一个关于性能优化的issue，主要涉及Ada Lovelace架构的性能问题。由于Cutlass内核在某些情况下表现略逊于scaled_mm内核，但在一些benchmark测试中表现更好，但在输出令牌吞吐量方面略逊于scaled_mm，需要进一步调查和比较。

https://github.com/vllm-project/vllm/issues/6949
这个issue是一个文档修复类型的issue，主要涉及到修复文档中的一个很小的拼写错误。原因是由于markdown渲染不起作用，所以在PR检查列表部分使用了原始的HTML。

https://github.com/vllm-project/vllm/issues/6948
这是一个bug报告，涉及到OpenVINO和相关的依赖更新，可能导致在Markdown渲染方面出现问题。

https://github.com/vllm-project/vllm/issues/6947
这是用户提出的需求类型issue，主要涉及对象是Llama模型。由于找不到相关参数来启用embeddings API，用户无法实现这一功能，因此请求帮助启用embeddings API。

https://github.com/vllm-project/vllm/issues/6946
这个issue类型是bug报告，主要对象是MiniCPM-Llama3-V-2_5。由于tensor_parallel_size大于1时出现错误，用户无法正常运行程序。

https://github.com/vllm-project/vllm/issues/6945
这是一个性能优化提议类型的issue，主要涉及vllm的性能问题，用户提出希望通过增加吞吐量来改进性能。

https://github.com/vllm-project/vllm/issues/6944
该issue属于用户提出需求类型，主要涉及的对象是该项目是否支持在torch 2.4下开箱即用，原因可能是用户希望该项目能够兼容最新版本的torch。

https://github.com/vllm-project/vllm/issues/6943
这是一个性能问题报告，主要涉及VLLM模型在输入长度较长时性能严重下降的问题，可能是由于FlashAttention核心运算时间较长导致。

https://github.com/vllm-project/vllm/issues/6942
这是一个bug报告类型的issue，主要涉及的对象是SiglipVisionModel的实现和vLLM的Attention layer。在实现中遇到了CUDA Error无法正常工作，因此只能使用基本的注意力机制。

https://github.com/vllm-project/vllm/issues/6941
这个issue是用户提出需求，希望支持在vLLM中使用SiglipVisionModel，类似于支持CLIPVisionModel。

https://github.com/vllm-project/vllm/issues/6940
这是一个用户需求类型的issue，主要涉及如何实现在使用LLMs时实现实时流式响应，由于当前代码的generate方法会等待整个响应生成完毕才展示，用户希望能够实现流式展示并增强用户交互性。

https://github.com/vllm-project/vllm/issues/6939
这个issue是一个bug报告，涉及的主要对象是MiniCPMV库。由于未指定版本号在配置中导致AttributeError错误，需要进行代码修改以解决此问题。

https://github.com/vllm-project/vllm/issues/6938
这是一个使用问题，主要涉及Multi Node llama 405B inference，在尝试在databricks上运行一个multi node server时出错。

https://github.com/vllm-project/vllm/issues/6937
这个issue为安装相关问题，主要对象为用户的环境和安装方式。用户想知道在CUDA版本为12.0和Python为3.10的环境下应选择哪个发布版本。

https://github.com/vllm-project/vllm/issues/6936
这个issue是一个功能补充类型的问题，涉及的主要对象是LLM类。由于需要增加应用聊天模板的功能，因此提出了添加apply_chat_template方法和更新generate方法的需求。

https://github.com/vllm-project/vllm/issues/6935
这是一个特性请求，涉及到将 Jamba 中的 MoE 层替换为标准 vLLM FusedMoE 层。

https://github.com/vllm-project/vllm/issues/6934
这是一个Bug报告类型的issue，主要涉及vllm加载"MiniCPMV2"时出错的问题，可能是由于缺少"preprocessor_config.json"文件导致的。

https://github.com/vllm-project/vllm/issues/6933
这是一个bug报告，涉及的主要对象是TPU后端的贪婪解码实现，问题是关于解码过程出现错误。

https://github.com/vllm-project/vllm/issues/6932
这是一个功能需求，用户提出需要支持INT4类型数据对MiniCPM-Llama3-V-2_5进行操作。

https://github.com/vllm-project/vllm/issues/6931
这是一个bug报告，主要涉及的对象是vLLM的CPU目标，导致的症状是构建失败，原因是没有安装正确的cpuonly torch。

https://github.com/vllm-project/vllm/issues/6930
这是一个Bugfix类型的issue，主要涉及到PaliGemma MMP中的linear层初始化问题，导致Paligemma不支持张量并行性。

https://github.com/vllm-project/vllm/issues/6929
该问题类型为性能改进提议，主要涉及到与sglang项目性能进行比较。提出原因是希望改进性能，但未收到任何响应。

https://github.com/vllm-project/vllm/issues/6928
该issue类型为用户提出需求，主要涉及的对象是支持rerank模型。由于Rerank模型对于RAG工作流程至关重要，用户询问是否有计划支持这个功能，以及实现这一功能的主要步骤。

https://github.com/vllm-project/vllm/issues/6927
这是一个Bugfix类型的issue，涉及到vLLM中的logit processor，由于索引超出了词汇表的大小，导致出现BUG。

https://github.com/vllm-project/vllm/issues/6926
这个issue是一个Bug报告，主要涉及的对象是vLLM项目中的benchmark_serving.py文件。由于FlashInfer Backend导致的Speculative Decoding错误引起了TransferEncodingError问题。

https://github.com/vllm-project/vllm/issues/6925
这是一个bug报告类型的issue，主要涉及处理字典条目为张量列表的情况。这个问题是由于markdown渲染不起作用而导致使用原始html时引起的。

https://github.com/vllm-project/vllm/issues/6924
这是一个关于在vllm项目中添加对Qwen2的Pipeline parallel支持的需求。

https://github.com/vllm-project/vllm/issues/6923
这是一个Bug报告，主要涉及对象是BlockSpaceManagerV2。由于启用了BlockSpaceManagerV2中的前缀缓存，导致系统变慢，尤其是时间到第一个标记增加，用户正在寻求解决这一问题。

https://github.com/vllm-project/vllm/issues/6922
该issue是一个提交PR时未填写描述和检查列表的问题报告，主要涉及vLLM项目的代码质量和审查流程效率受影响。

https://github.com/vllm-project/vllm/issues/6921
这个issue类型为功能增强需求，涉及Github Actions工作流程以及标签管理。

https://github.com/vllm-project/vllm/issues/6920
这个issue类型是功能增强提案，主要涉及的对象是vLLM的pipeline parallelism分区策略。由于当前PP kvcache容量受限于所有PP阶段中最小的一个，通常是第一个或最后一个阶段，导致整体吞吐量受限，因此需要引入新的分区算法。

https://github.com/vllm-project/vllm/issues/6919
这是一个关于安装问题的bug报告，主要涉及vllm的wheels构建过程。由于缺少GPU环境，导致在构建Docker镜像时发生了`Unknown runtime environment`错误。

https://github.com/vllm-project/vllm/issues/6918
这是一个bug报告，主要涉及vllm在设备环境信息收集过程中出现了段错误（Segmentation fault）的问题。

https://github.com/vllm-project/vllm/issues/6917
这是一个关于代码修改和功能添加的issue，主要涉及到在vLLM中支持`torch.compile`时添加元函数以防止torch.compile图形断裂的问题。

https://github.com/vllm-project/vllm/issues/6916
这是一个bug报告，主要涉及的对象是无法从`vllm` repo的Dockerfile构建图像，可能是由于步骤中提供的命令无法成功构建而导致的。

https://github.com/vllm-project/vllm/issues/6915
这是一个性能优化提议，涉及到移动推测解码的准备输入到GPU，以提高性能。

https://github.com/vllm-project/vllm/issues/6914
该issue属于代码质量改进类型，涉及到的主要对象是在vLLM中的一些内核警告。这个问题的出现是因为开发者在多个内核中遇到了一些次要警告，需要进行修复。

https://github.com/vllm-project/vllm/issues/6913
该issue类型为需求提出，主要涉及LLMEngine中输出处理导致GPU性能瓶颈的问题。

https://github.com/vllm-project/vllm/issues/6912
这是一个功能需求的issue，主要涉及到减少LoRA延迟的技术实现。

https://github.com/vllm-project/vllm/issues/6911
这是一个需求提议，涉及的主要对象是将pipeline parallelism与speculative decoding结合起来，以降低延迟。

https://github.com/vllm-project/vllm/issues/6910
这是一个Bug报告，主要涉及的对象是模型google/paligemma3bmix448，由于使用tensor parallelism导致在热身阶段出现异常。

https://github.com/vllm-project/vllm/issues/6909
这是一个bug报告，主要涉及vLLM对于在Rank Stabilized LoRA训练的LoRA适配器输出存在较大偏差的问题，提出需要调整代码来适配使用Rank Stabilized LoRA适配器的需求。

https://github.com/vllm-project/vllm/issues/6908
这个issue是一个bug修复类型的问题，涉及的主要对象是vLLM的CUDA内核文件中未使用的变量。由于声明了但未使用的变量导致了警告提示，需要将其清除以解决问题。

https://github.com/vllm-project/vllm/issues/6907
这是一个bug报告，主要涉及到cuda OOM errors在多次请求中持续存在的问题，可能由于内存利用过高导致了这个bug。

https://github.com/vllm-project/vllm/issues/6906
这是一个用户提出需求的issue，主要涉及的对象是VLLM模型的功能。由于用户希望提出一种新的特性，让模型支持在给定上下文中生成多个模型内容，以加快内容生成速度。

https://github.com/vllm-project/vllm/issues/6905
这是一个Bug报告，主要问题是vLLM在生成JSON字符串时未正确关闭文本值的问题。

https://github.com/vllm-project/vllm/issues/6904
这个issue是一个bug报告，主要涉及Kernel下的marlin divide-by-zero warnings问题，可能是由于缺少`constexpr`和静态断言导致的。

https://github.com/vllm-project/vllm/issues/6903
这个issue是关于优化序列化过程的，涉及到将数据发送给workers时减少payload大小，并且需要修改现有数据类以支持msgspec兼容的数据结构。原因是通过发送增量以减少要发送给workers的负载大小，以及使用msgspec.msgpack可以提高端到端吞吐量。

https://github.com/vllm-project/vllm/issues/6902
这是一个bug报告类型的issue，涉及主要对象是Mistral8x7BInstruct程序运行时的8-way TP和enablelora功能。原因是在执行`determine_num_available_blocks`时导致程序崩溃，可能是由于GPU内存损坏导致的illegal memory access错误。

https://github.com/vllm-project/vllm/issues/6901
这个issue是关于bug报告，主要涉及CUDA kernels中的函数警告修复，导致Tensor.data()过时的警告。

https://github.com/vllm-project/vllm/issues/6900
这是一个增强功能的issue，涉及主要对象是VLLM中的处理器。这个PR的目的是为了避免重复的维护工作，删除了VLLM中重复的处理器。

https://github.com/vllm-project/vllm/issues/6899
这是一个关于功能需求的issue，主要涉及到FlashInfer后端的Sliding Window支持。由于vLLM的FlashAttention封装不支持Sliding Window，用户询问了对于Sliding Window和分页KV缓存之间的冲突，以及这是否意味着无法在FlashInfer中使用。

https://github.com/vllm-project/vllm/issues/6898
这是一个bug报告，涉及的主要对象是vllm在openvino上的运行。由于openvino安装或启动代码的问题导致出现bug，用户请求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/6897
这是一个bug报告类型的issue，涉及到vllm的rope_scaling配置问题，用户在部署deepseek coder 6.7b时遇到了"Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}"的错误提示。

https://github.com/vllm-project/vllm/issues/6896
这是一个bug报告，涉及了在使用vllm时在Ray Actor环境下出现"No CUDA GPUs are available"错误的问题，原因可能是Ray Actor包装器导致某些进程无法访问CUDA GPU。

https://github.com/vllm-project/vllm/issues/6895
这是一个bug报告，涉及主要对象是VLLM环境中的GPU P2P访问缓存生成过程。该问题的原因是无法读取提交哈希导致的异常行为。

https://github.com/vllm-project/vllm/issues/6894
这是一个bug报告，涉及的主要对象是vllm下的distributed inference功能。由于视觉语言模型（VL model）推理时出现错误，并导致程序崩溃或运行缓慢，用户请求帮助和建议。

https://github.com/vllm-project/vllm/issues/6893
这是一个bug报告，涉及到vllm项目中的CUDA wrapper代码，由于硬编码的路径错误导致在特定环境下无法正确定位cudart库，进而导致程序无法继续执行。

https://github.com/vllm-project/vllm/issues/6892
这是一个CI/Build类型的issue，主要涉及限制CPU CI测试时间以避免测试队列阻塞。导致这个问题的原因可能是测试队列被长时间占用而导致其他测试无法运行。

https://github.com/vllm-project/vllm/issues/6891
该issue是一个技术贡献的PR描述问题，涉及到对TPU tensor parallelism的添加操作，但需要填写PR描述信息。

https://github.com/vllm-project/vllm/issues/6890
这是一个Bug报告，主要涉及Vllm的API服务器无法接收到支持的参数`truncate_prompt_tokens`，可能由于文档与实际部署不一致导致。

https://github.com/vllm-project/vllm/issues/6889
这是一个bug报告，主要涉及到在设置tp=2或tp=4时出现NCCL ERROR。这可能是由于压缩张量导致的问题。

https://github.com/vllm-project/vllm/issues/6888
这是一个性能优化建议的issue，主要涉及到提高ray dag和spmd worker的性能问题。原因可能是目前spmd worker速度较慢，提议使其与ray dag速度相当。

https://github.com/vllm-project/vllm/issues/6887
这是一个Bug报告，涉及的主要对象是vllm的一个issue，由于某些原因导致了关于"dag teardown error AttributeError: 'Worker' object has no attribute 'core_worker'"的问题。

https://github.com/vllm-project/vllm/issues/6885
这是一个bug报告，主要涉及的对象是vllm下的Speculative Decoding + FlashInfer + benchmark_serving.py功能。可能是由于TransferEncodingError导致的问题。

https://github.com/vllm-project/vllm/issues/6884
这个issue是一个bug报告，主要涉及的对象是运行`gradientai/Llama38BInstructGradient1048k`时出现的错误。这个bug的症状是出现了错误消息"[Bug]: First input (bf16) and second input (uint8) must have the same dtype!"。

https://github.com/vllm-project/vllm/issues/6883
这个issue属于一个功能需求的讨论，涉及到通过使用多进程以及`zeromq`来提高OpenAI服务器的性能。

https://github.com/vllm-project/vllm/issues/6882
该issue是一份要求将代码合并到主分支的请求，涉及vLLM项目的代码贡献规范问题。

https://github.com/vllm-project/vllm/issues/6881
这个issue是一个bug报告，主要涉及的对象是vLLM中的tensorizer模块。原因是在测试过程中，出现了内存分析bug。

https://github.com/vllm-project/vllm/issues/6880
这是一个特性开发提议，并非bug报告，主要涉及的对象是vLLM项目中的openai server原型。由于需要在高QPS场景下提供速度优势，提议采用基于zeromq和protobufs的协议改进openai server功能。

https://github.com/vllm-project/vllm/issues/6879
这是一个性能改进的提案，主要涉及在vLLM中使用Python数组以替换Python列表来进行零拷贝张量创建。这个提案主要是为了减少在将数据从CPU内存复制到GPU内存时发生的拷贝操作。

https://github.com/vllm-project/vllm/issues/6878
这是一个关于在 offline LLM 上支持引导解码的 issue，用户提出了如何在 Mistral 7B 使用离线推理传递 JSON content type 的问题。

https://github.com/vllm-project/vllm/issues/6877
这是一个用户提出需求的issue，主要涉及的对象是支持Python 3.12版本。由于ray目前不支持Python 3.12，用户提出希望该库未来能够支持该版本的需求。

https://github.com/vllm-project/vllm/issues/6876
这是一个Bug报告，主要涉及的对象是vLLM加载本地7B模型时速度过慢，可能是由于加载模型时的进度条显示需要51分钟，加载一个shard约需9分钟，原因可能与模型权重格式、加载方式以及环境有关。

https://github.com/vllm-project/vllm/issues/6875
这是一个Bug报告，主要涉及DeepSeek-v2-Lite中与FP8相关的错误。该Bug可能由于环境设置不当而导致。

https://github.com/vllm-project/vllm/issues/6874
这个issue是一个bug报告，涉及的主要对象是pyzmq based openai server prototypes，原因可能是markdown解析问题导致显示异常。

https://github.com/vllm-project/vllm/issues/6873
这是一个bug报告，主要涉及vllm与llamaindex框架在CPU上只利用了一个线程，导致推理速度缓慢的问题。

https://github.com/vllm-project/vllm/issues/6872
该issue是关于缺少PR描述的问题，主要对象是vLLM的代码。由于markdown渲染不起效，需要使用原始的HTML。

https://github.com/vllm-project/vllm/issues/6871
这是一个文档相关的Issue，主要涉及到vLLM代码库中`cutlass_fp8_supported`函数的正确传参问题，导致对是否可以使用fp8 activation的判断不准确。

https://github.com/vllm-project/vllm/issues/6870
这是一个bug报告，主要涉及LLaMa 3.1 405B on 8*H100的TMA descriptor初始化失败导致的错误。

https://github.com/vllm-project/vllm/issues/6869
这是一个用户提出需求的issue，主要涉及的对象是`generate`方法。用户希望`LLM.generate`方法支持传递`prompt_embeds`作为参数，以便在只微调嵌入层时能够使用，这样能够支持同一模型后端支持多个自定义微调嵌入层。

https://github.com/vllm-project/vllm/issues/6868
该issue属于技术讨论类型，主要涉及vllm模型中关于块大小对性能的影响的讨论。作者提出了关于块大小对吞吐量和延迟的影响的假设，并寻求对实验结果与预期不符的建议。

https://github.com/vllm-project/vllm/issues/6867
这个issue是关于优化调度程序的改进，不是bug报告。

https://github.com/vllm-project/vllm/issues/6866
这是一个Bug报告，该问题涉及到Vllm的一个索引越界错误。导致此Bug的原因是用户提供的标记ID超出了词汇表的大小限制。

https://github.com/vllm-project/vllm/issues/6865
这是一个性能优化类型的issue，主要涉及到`free_finished_seq_groups`函数。原因是在处理大规模离线批量推理时，该函数在每个模型步骤之后调用，导致性能下降。

https://github.com/vllm-project/vllm/issues/6864
这个issue属于bug报告类型，主要对象是vllm项目。导致问题的原因是缺少`libcuda.so`，导致无法加载`cuTensorMapEncodeTiled` symbol，进而导致了llvm python进程的失败。

https://github.com/vllm-project/vllm/issues/6863
该issue是一个测试报告，涉及主要对象为"Nemotron"。由于测试结果并未明确表明问题或提出需求，因此没有直接反映出任何bug或问题。

https://github.com/vllm-project/vllm/issues/6862
这是一个bug报告，涉及主要对象是vllm中的mistral模型，由于某种原因导致调用mistral模型时出现错误，而在其他模型系列中并未出现同样的错误。

https://github.com/vllm-project/vllm/issues/6861
这是一个Bug报告类型的issue，主要涉及BNB模型无法加载的问题。由于vllm在评估BNB模型时在`adapter_name_or_path`参数上出现错误，导致用户无法成功加载BNB模型。

https://github.com/vllm-project/vllm/issues/6860
This issue is a documentation update for CI/Build changes in the vllm project.

https://github.com/vllm-project/vllm/issues/6859
这个issue是一个Bugfix类别的问题，涉及到vLLM中的一个示例代码中的一个额外的参数解析行，导致markdown的渲染出现问题。

https://github.com/vllm-project/vllm/issues/6858
该issue类型是文档整理与更新，涉及主要对象为VLM（Vision Language Models），由于之前每个VLM需要单独创建示例文件，现在经过最近的重构后不再需要，因此此PR将这些示例文件整合到一个文件中以清理文档和示例文件夹。

https://github.com/vllm-project/vllm/issues/6857
这是一个用户提出需求的issue，主要涉及在vLLM文档站点上添加RunLLM聊天小部件。由于缺少该小部件，用户可能需要更方便地与RunLLM进行交流和咨询。

https://github.com/vllm-project/vllm/issues/6856
这是一个性能优化相关的问题，主要对象是PyTorch XLA版本。这个问题是因为旧版本XLA编译时间较长，通过升级版本并使用新的动态形状支持来减少编译时间。

https://github.com/vllm-project/vllm/issues/6855
这个issue是一个PR描述不完整的bug报告，主要涉及到vLLM项目中的文档修复和改进，原因可能是提交PR时未按要求填写描述。

https://github.com/vllm-project/vllm/issues/6854
这是一个功能请求(issue)。主要对象是vLLM中的多步调度(Multi-Step Scheduling)。该问题涉及到GPU空闲等待CPU操作，导致GPU泡泡(513ms的GPU泡泡)。

https://github.com/vllm-project/vllm/issues/6853
这是一个bug报告，主要涉及的对象是关于vllm文档中采样页面不显示的问题。这可能是源文件中存在的拼写错误导致的。

https://github.com/vllm-project/vllm/issues/6852
这个issue属于bug报告类型，主要涉及Sm90ColOrScalarBroadcast，由于batch size不是gemm tile size的倍数导致了illegal memory access。

https://github.com/vllm-project/vllm/issues/6851
这是一个缺少PR描述的问题，主要涉及vllm中markdown渲染无法正常工作，可能是由于文本格式问题导致。

https://github.com/vllm-project/vllm/issues/6850
这个issue是一个[Kernel]类型的PR，涉及到添加awq_dequantize_triton的实现。由于CUDA实现无法使用时，这个实现作为一个备用方案。

https://github.com/vllm-project/vllm/issues/6849
这是一个bug报告，主要涉及修复 get_num_blocks_touched 逻辑计算错误导致的问题。

https://github.com/vllm-project/vllm/issues/6848
这是一个关于在vLLM中为Ada Lovelace添加优化int8内核的Kernel类别issue，主要涉及对Gemm形状进行优化配置。由于无法使用markdown渲染，内容展示时采用了原始的html格式。

https://github.com/vllm-project/vllm/issues/6847
这是一个文档更新类型的issue，主要涉及到更新README.md文件中的参数名称。原因是由于之前的PR修改导致markdown渲染不起作用，需要使用原始HTML语法。

https://github.com/vllm-project/vllm/issues/6846
这是一个临时强制执行“eager_mode”以修复已知问题的问题，属于bug报告类型，主要涉及bitsandbytes quantization。原因是在修复问题之前需要暂时开启eager_mode。

https://github.com/vllm-project/vllm/issues/6845
这个issue类型是bug报告，涉及的主要对象是ROCm PyTorch版本。原因是嵌入层不支持长输入导致BUG。

https://github.com/vllm-project/vllm/issues/6844
这是一个关于文档更新的issue，主要涉及的对象是neuron构建相关的文档。由于markdown渲染不起作用，所以在这里使用了原始html。

https://github.com/vllm-project/vllm/issues/6843
这是一个用户需求类型的issue，主要涉及的对象为VLLM模型文档，用户提出添加Nemotron支持的需求。

https://github.com/vllm-project/vllm/issues/6842
这个issue是一个bug报告，涉及的主要对象是vllm项目中的 Kernel 模块。这个问题是由于缺少正确的padding导致的非法内存访问问题。

https://github.com/vllm-project/vllm/issues/6841
这是一个bug报告类型的issue，涉及到conditional testing功能的初始建立，主要问题是关于测试文件本身被更改后是否应该重新运行相关测试，以及依赖项的准确性和精确性。

https://github.com/vllm-project/vllm/issues/6840
这是一个需求提出类型的issue，主要涉及Dockerfile中的CUDA版本恢复到12.1，由于12.4版本要求主机保持最新导致的问题。

https://github.com/vllm-project/vllm/issues/6839
这是一个用户提出需求的issue，主要涉及grpc openai server prototypes。由于标题中含有"[ DO NOT MERGE ]"，说明该issue可能是一些原型性工作或者未经验证的代码，需要保持不合并到主分支。

https://github.com/vllm-project/vllm/issues/6838
这是一个bug报告，主要涉及Kernel（CUDA kernels或其他compute kernels）对象。导致此问题的原因是在segmented_max_reduction中，变量`i`是一个索引，可能会超过int32_t的最大大小。

https://github.com/vllm-project/vllm/issues/6837
这是一份关于支持pipeline parallelism with Ray accelerated DAG的issue，主要涉及到benchmarking result中涉及到的不同backend间的性能比较。

https://github.com/vllm-project/vllm/issues/6836
这是一个Bug报告，主要涉及的对象是VLLM项目中的`multi_modal_kwargs`。由于当前`multi_modal_kwargs`在分布式广播之前被放置在GPU上，导致只允许平坦的张量字典进行广播，无法处理嵌套张量。

https://github.com/vllm-project/vllm/issues/6835
这是一个Bug修复的Issue，主要涉及到Tensorizer测试失败的问题。由于Markdown渲染不起作用，导致必须在此处使用原始HTML。

https://github.com/vllm-project/vllm/issues/6834
这个issue类型是bug报告，涉及的主要对象是文档`conf.py`。由于缺少了mock导入，导致了网页上的Sampling页面不再显示。

https://github.com/vllm-project/vllm/issues/6833
这是一个Bug报告，涉及的主要对象是vllm库中的enable_prefix_caching功能。由于enable_prefix_caching导致了持续的非法内存访问错误。

https://github.com/vllm-project/vllm/issues/6832
这是一个bug报告，涉及主要对象是VLLM（Very Large Language Model）。由于Neuron基础镜像中的requirements字符串格式错误，并且镜像未公开，导致构建失败。

https://github.com/vllm-project/vllm/issues/6831
该issue是一个bugfix类型的issue，主要涉及args.stream参数无法正常工作。这个bug可能是由于代码逻辑错误或者参数设置问题导致的。

https://github.com/vllm-project/vllm/issues/6830
该issue属于功能请求类型，涉及到EAGLE draft model的支持。

https://github.com/vllm-project/vllm/issues/6829
这是一个关于VLLM在使用llama 3 8b模型时出现OOM的bug报告，主要涉及的对象是在A10 GPU上运行VLLM时内存占用过高，导致无法成功加载模型。

https://github.com/vllm-project/vllm/issues/6828
这是一个非bug类型的issue，主要涉及到前端代码的重构和代码结构统一。

https://github.com/vllm-project/vllm/issues/6826
这是一个Bug报告，涉及到vLLM在使用Pipeline Parallelism时推理一个请求速度非常慢的问题。

https://github.com/vllm-project/vllm/issues/6825
该issue类型为用户提出需求，主要涉及在多节点上部署vLLM并使用qwen2模型以加快推理速度的问题，由于大的TP可能导致注意力头的划分错误并出现错误，而且qwen2上未实现PP功能，用户希望能够实现在每个节点上独立并行运行多个模型并在一个端口上合并的效果。

https://github.com/vllm-project/vllm/issues/6824
该issue类型为用户提出需求，并涉及到如何在具有不同内存的两个GPU上部署模型，用户询问如何最大化利用总共的80GB内存。

https://github.com/vllm-project/vllm/issues/6823
这是一个bug报告，涉及到在使用Nsight system记录多GPU运行程序时出现的错误，可能是由于多GPU环境中的某些bug导致。

https://github.com/vllm-project/vllm/issues/6822
这是一个特性需求的issue，主要涉及到TeleFLM模型的支持。用户提出希望添加对TeleFLM模型的支持，并说明已经进行了相关测试并得到合理的模型响应。

https://github.com/vllm-project/vllm/issues/6821
这个issue是关于PR提交流程的问题，主要涉及到PR标题分类的问题。由于markdown渲染不起作用，导致需要使用原始HTML代码来解决。

https://github.com/vllm-project/vllm/issues/6820
这是一个[CI/Build]类型的issue，涉及升级Dockerfile至Ubuntu 22.04，由于当前使用的是Ubuntu 20.04，其中的curl版本较老导致缺少`failwithbody`标志和存在高安全漏洞。

https://github.com/vllm-project/vllm/issues/6819
这个issue是一个bug报告，涉及的主要对象是代码中的内存访问和高并发情况下的计算错误，导致了错误的结果和非法内存访问。

https://github.com/vllm-project/vllm/issues/6818
这是一个Bug报告，主要涉及的对象是在8xA100 AWS集群上运行推理时遇到警告并卡住的情况。由于警告信息"[WARNING shm_broadcast.py:404] No available block found in 60 second."导致推理过程无法进行。

https://github.com/vllm-project/vllm/issues/6817
这是一个需求提出的issue，主要涉及 MoE 模块的更新，用户希望能在单个 GPU 上加载 Mixtral。

https://github.com/vllm-project/vllm/issues/6816
这个issue类型是bug报告，涉及到调试（debugging）文档，主要对象是解决出现卡顿（hangs）的已知问题，用户可能遇到了程序卡顿的情况，寻求帮助或解决方案。

https://github.com/vllm-project/vllm/issues/6815
这是一个Bug报告，涉及的主要对象是Llama3.1-70B-FP8，用户提出了3090是否不再支持FP8的问题，可能是由于3090的计算能力下降导致的。

https://github.com/vllm-project/vllm/issues/6814
这是一个Bug报告，主要涉及的对象是VLLM下的MiniCPMVConfig对象。由于MiniCPMVConfig对象缺少'version'属性，导致出现了AttributeError。

https://github.com/vllm-project/vllm/issues/6813
这是一个用户提出的需求类型的issue，主要涉及支持TPU设备上的集体通信。这个issue可能由于XLA设备中缺少集体通信支持而产生。

https://github.com/vllm-project/vllm/issues/6812
这个issue类型是功能需求，主要涉及的对象是支持在初始化Ray集群中添加对TPU的支持，以及将在PR中添加对HPU的支持。

https://github.com/vllm-project/vllm/issues/6811
这是一个改进优化类的issue，主要涉及Dockerfile.rocm文件的调整。由于vLLM项目未使用torchaudio和Numpy的热修复，因此进行了相应的简化调整。

https://github.com/vllm-project/vllm/issues/6810
这是一个bug报告类型的issue，问题所涉及的主要对象为"tensorizer test"。原因是标记tensorizer测试为软失败并将其与分组测试分开是为了快速检查。

https://github.com/vllm-project/vllm/issues/6809
这是一个bug报告，主要涉及了vllm项目中的tensorizer功能。由于最近tensorizer功能出现不稳定情况，导致测试失败，可能是由于PR引入的问题。

https://github.com/vllm-project/vllm/issues/6808
这个issue是一个功能需求报告，涉及的主要对象是扩展代码以支持SSD offloading，并实现从block中获取KV缓存和将KV缓存添加到block中的功能。由于工作尚未完成，导致目前功能不可用。

https://github.com/vllm-project/vllm/issues/6807
这是一个Bug报告，涉及到vLLM中Embedding层不支持长输入所导致的问题。

https://github.com/vllm-project/vllm/issues/6806
这个issue是关于修复由于引入https://github.com/vllmproject/vllm/pull/6708 导致的 flaky 测试。

https://github.com/vllm-project/vllm/issues/6805
这是一个用户提出需求的issue，主要涉及的对象是在vllm框架中集成MiniGPT4_video模型。原因是用户表示在阅读教程后仍然无法成功集成该模型到vllm中，希望得到帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/6804
这个issue类型是技术文档改进，关注于改进多节点服务文档，解决了与多节点服务相关的问题。

https://github.com/vllm-project/vllm/issues/6803
这是一个关于如何在多节点环境中设置环境变量以及环境变量继承如何工作的讨论问题，涉及到多节点推断设置。环境变量设置不正确可能导致集群配置困难，特别是关于网络配置使机器之间通信的问题。

https://github.com/vllm-project/vllm/issues/6802
这个issue是一个bug报告，涉及的主要对象是在使用`multiproc_gpu_executor.py`时遇到的CPU线程并行化问题。由于`OMP_NUM_THREADS`未在`import torch`之前设置，导致无法正确配置torch操作的CPU线程并行化，导致存在bug。

https://github.com/vllm-project/vllm/issues/6801
这个issue类型是关于性能优化的讨论，主要涉及vllm项目的性能问题和优化计划。

https://github.com/vllm-project/vllm/issues/6800
这是一个Bug报告，涉及到vLLM和LoRA Adapter在不同包版本下得分不一致的问题。导致这个问题的原因可能是不同包版本之间的兼容性或功能实现差异。

https://github.com/vllm-project/vllm/issues/6799
该问题类型为需求发布幻灯片，主要涉及文档发布。

https://github.com/vllm-project/vllm/issues/6798
这是一个Bug修复的Issue，涉及主要对象为VLLM库中加载wNa16时出现空的通道尺度，导致传递空指针给marlin。原因是在行并行时，整数除法导致0大小张量，而创建通道组时输入大小大于分区输入大小。

https://github.com/vllm-project/vllm/issues/6797
这是一个关于改进OpenAI Server架构的请求，涉及到将API服务器和AsyncLLMEngine分离为两个进程通信。由于目前它们共享相同的asyncio事件循环，导致CPU组件和API服务器争夺相同资源。

https://github.com/vllm-project/vllm/issues/6796
这是一个bug报告类型的issue，主要涉及的对象是vllm项目中的lm_head和embeddings的关联处理。由于`tie_word_embeddings`为true时，导致加载`lm_head`时出现问题，类似之前在gemma项目中解决的情况。

https://github.com/vllm-project/vllm/issues/6795
这是一个技术改进的issue，主要涉及到Marlin内核的精度增加问题。

https://github.com/vllm-project/vllm/issues/6794
这是一个bug报告，主要涉及的对象是重新现场SG汇编（SGLANG）基准结果并可视化。由于markdown无法正确渲染，所以使用了原始HTML。

https://github.com/vllm-project/vllm/issues/6793
这是一个Bug修复的Issue，涉及主要对象是vLLM中的`ReplicatedLinear`层。这个问题是由于`ReplicatedLinear`层的权重加载函数没有正确设置，导致了在加载mixtral_quant模型时出现了model weight_loader为None的问题。

https://github.com/vllm-project/vllm/issues/6792
这个issue是一个bug报告，涉及的主要对象是vLLM的核心逻辑。由于self.forward_dag在init_workers_ray之后定义，如果init_workers_ray出现异常，self.forward_dag将不存在，从而导致错误消息的异常。

https://github.com/vllm-project/vllm/issues/6791
这是一个bug报告，用户在使用vllm文档中的示例时遇到了tensorizer错误。

https://github.com/vllm-project/vllm/issues/6790
这个issue是一个Bug报告，涉及的主要对象是Engine迭代超时，可能由环境问题导致异常抛出。

https://github.com/vllm-project/vllm/issues/6789
这是一个用户提出需求的issue，主要涉及对象是vllm模型。用户想了解是否可以在classification model中使用vllm，由于不清楚vllm是否支持该模型，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/6788
这个Issue是一个Bugfix类型的报告，主要涉及到解决可能的数据竞争问题，导致CSAN报错。

https://github.com/vllm-project/vllm/issues/6787
这是一个Bug报告，主要涉及MiniCPM-V OpenAI Compatible Server 的图片占位符问题。由于缺少图片占位符，可能导致界面显示异常或不完整。

https://github.com/vllm-project/vllm/issues/6786
这是一个bug报告，主要涉及问题是vllm在没有安装triton的情况下无法正常工作。导致这个bug的原因是在库中的各个地方都会导入triton，而缺少了triton作为依赖会导致代码无法正常运行。

https://github.com/vllm-project/vllm/issues/6785
该issue类型为用户提出需求，主要对象是ngramspecdecode阶段的功能。用户提出了关于在speculative decoding中评估多个ngram猜测的建议。

https://github.com/vllm-project/vllm/issues/6784
这是一个针对bug的报告，主要涉及的对象是Jamba model，由于Jamba model不支持chunked prefill，导致了需要添加断言以确保其不会与chunked prefill或prefix caching一起使用。

https://github.com/vllm-project/vllm/issues/6783
这是一个Bug报告，主要涉及的对象是`lmevaluationharness`与`vllm`。导致问题的原因可能是在运行特定命令后发生Segmentation fault错误。

https://github.com/vllm-project/vllm/issues/6782
这个issue是一个bug报告，主要涉及到vLLM中的生成令牌数与spesulative decoding相关的问题。这个bug可能由于生成令牌数在使用speculative decoding时计算错误而引起。

https://github.com/vllm-project/vllm/issues/6781
该issue是关于性能问题的反馈，主要涉及到Qwen2-72B-GPTQ-Int4 模型在特定环境下 TTFT 速度较慢。可能的原因是参数调整后未导致明显改善。

https://github.com/vllm-project/vllm/issues/6780
这个issue类型是bug报告，主要对象是N-gram spec_decode in flash_attention，由于是赋值操作时形状不匹配导致了无法广播输入数组的错误。

https://github.com/vllm-project/vllm/issues/6779
这个issue是关于[Core]部分的改进提议，主要涉及到SequenceData和Sampler，通过使用`array.array`来加速填充操作，提升了性能。

https://github.com/vllm-project/vllm/issues/6778
这是一个功能需求提出的issue，主要涉及到支持`Mistral-Large-Instruct-2407`函数调用。源于对`MistralLargeInstruct2407`性能优越的需求，以及可能类似于前一个issue的功能需求。

https://github.com/vllm-project/vllm/issues/6777
这是一个bug报告，主要涉及到 Medusa SD 在性能上表现不如基准系统，可能由于模型格式不兼容导致性能下降。

https://github.com/vllm-project/vllm/issues/6776
这是一个Bug报告，主要涉及的对象是vllm下的qwen2-72b-instruct模型。产生了CUDA error: an illegal memory access was encountered的错误，发生在batch_size=128时，原因尚未查明。

https://github.com/vllm-project/vllm/issues/6775
这是一个Bug报告类型的Issue，主要涉及Llama 3.1在分布式推理过程中遇到的问题，导致了服务启动时出现了与`gloo`相关的崩溃情况。

https://github.com/vllm-project/vllm/issues/6774
这是一个Bug报告，主要涉及VLLM--max-model-len配置的鲁棒性，由于环境中PyTorch版本不匹配，导致配置参数无法正确识别。

https://github.com/vllm-project/vllm/issues/6773
这是一个用户提出需求的issue，主要涉及使用Quantized模型在分布式环境下运行推理时遇到的问题。

https://github.com/vllm-project/vllm/issues/6772
这是一个用户提出需求的类型的issue，主要涉及chat API assistant 的预填内容功能。用户期望能够预填assistant的回应，但系统似乎不支持此功能。

https://github.com/vllm-project/vllm/issues/6771
该issue为文档更新类型，涉及vLLM项目的PR任务和代码质量要求。

https://github.com/vllm-project/vllm/issues/6770
这是一个bug报告类型的issue，主要涉及vLLM的multiprocessing GPU executor，在0.5.3版本中引入的signal calls导致部署时出现错误，用户希望在此问题修复后发布0.5.3.post2版本以解决问题。

https://github.com/vllm-project/vllm/issues/6769
这是一个bug报告，涉及的主要对象是使用Dockerfile.openvino构建docker镜像时出现的错误。原因可能是切换至Ubuntu 20.04导致的构建问题。

https://github.com/vllm-project/vllm/issues/6768
这是一个用户提出需求的issue，主要涉及的对象是如何使用medusa speculative sampling 推断模型。由于显示的硬件信息中GPU型号为NVIDIA H100，但PyTorch版本为2.3.1+cu121，一些限制了2.3版本下的medusa功能，用户可能遇到了一些推断模型时的问题。

https://github.com/vllm-project/vllm/issues/6767
这是一个Bug报告，该问题涉及的主要对象是vLLM项目下的Llama 405b fp8运行过程中可能存在的数据竞争问题。

https://github.com/vllm-project/vllm/issues/6766
这是一个bug报告，主要涉及到VLLM中分布式执行器后端中`pt_main_thread`进程在主进程关闭后无法正常终止的问题。造成这一问题的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/6765
这是一个Bug类型的Issue，涉及主要对象为GH200，由于FP8 Quantization与`--cpu-offload-gb`不兼容，导致CPU offload时出现错误。 

https://github.com/vllm-project/vllm/issues/6764
这是一个Bug报告。问题主要涉及vllm模型在进行推断时输出被提前停止或截断。可能是由于系统环境配置或代码调用方式等原因导致的。

https://github.com/vllm-project/vllm/issues/6763
该issue是关于[Kernel]Add Fused Layernorm + Dynamic-Per-Token Quant Kernels，通过增加融合的LAYERNORM +动态-按标记量子核心。

https://github.com/vllm-project/vllm/issues/6762
这是一个bug报告，主要涉及的对象是ROCm安装指南。问题是由于Docker镜像缺少适当的HIP设置而导致的。

https://github.com/vllm-project/vllm/issues/6761
这是一个bug报告类型的issue，主要涉及的对象是`kv_cache_dtype=fp8`在FP8模型检查点中没有保存默认的k/v_scale为float导致的问题。

https://github.com/vllm-project/vllm/issues/6760
这是一个bug报告，涉及的主要对象是vLLM 70B模型，用户提出了该模型在性能方面出现了异常的现象。

https://github.com/vllm-project/vllm/issues/6759
这是一个bug报告，主要涉及到ZeroMQ（zmq）的使用问题，由于错误的使用导致了hang的情况。

https://github.com/vllm-project/vllm/issues/6758
这是一个bug报告，涉及的主要对象是在使用tie_word_embeddings时加载lm_head weights出现错误，造成了无法加载权重文件的问题。

https://github.com/vllm-project/vllm/issues/6757
这是一个bug报告，涉及CUDA graph的解码token错误，原因可能是优化bug导致的问题。

https://github.com/vllm-project/vllm/issues/6756
这是一个bug报告，涉及主要对象为meta-llama/Llama-Guard-3-8B-INT8，由于环境中最新的Docker镜像和RTX 4090，导致无法运行该组件。

https://github.com/vllm-project/vllm/issues/6755
该issue是一个Bugfix类型，涉及到openai_embedding_client.py文件，修复了openai客户端默认使用base64编码而非指定的编码格式的问题。

https://github.com/vllm-project/vllm/issues/6754
这个issue是文档修复类型的，主要涉及AMD ROCm硬件用户需要参考MI300x tuning guide获得性能和调整建议。因为markdown渲染不起作用，所以使用了原始HTML。

https://github.com/vllm-project/vllm/issues/6753
这是一个用户提出需求的issue，主要涉及的对象是前端（Frontend），他们提出添加一个新的`allowed_token_ids`解码请求参数来限制解码令牌。

https://github.com/vllm-project/vllm/issues/6752
这是一个bug报告，涉及的主要对象是项目中的disabled test_config。



https://github.com/vllm-project/vllm/issues/6751
这是一个bug报告，涉及到vLLM在多节点服务器上使用`trustremotecode`时导致的程序崩溃问题。

https://github.com/vllm-project/vllm/issues/6750
这是一个bug报告，主要涉及的对象是使用vllm部署llama3 405BinstructFP8的用户环境。由于配置存在错误，导致部署时报错，用户需要解决配置问题。

https://github.com/vllm-project/vllm/issues/6749
这是一个在GitHub上的vLLM项目中的issue，类型为Draft PR，主要涉及添加rwkv6功能。这个Draft PR并不满足合并条件，仍处于开发早期阶段。

https://github.com/vllm-project/vllm/issues/6748
这是一个bug报告，主要涉及代码中缺少f-string导致的错误返回信息问题。

https://github.com/vllm-project/vllm/issues/6747
这个issue属于bug报告，主要涉及的对象是调用不支持图片的模型时返回的错误信息，并由于返回的错误信息没有被插值处理而呈现出不符合预期的字符串。

https://github.com/vllm-project/vllm/issues/6746
这是一个bug报告类型的issue，主要涉及的对象是vllm中的meta-llama/Meta-Llama-3.1-405B-Instruct-FP8模型。产生问题症状的原因是无法正确在单机8xH100设备上运行该模型。

https://github.com/vllm-project/vllm/issues/6745
这是一个Bug报告，问题涉及的主要对象是quantization标志，原因是在使用quantization=gptq_marlin或quantization=awq_marlin时没有使用正确的marlin版本的kernels导致问题。

https://github.com/vllm-project/vllm/issues/6744
这是一个Bug报告，涉及的主要对象是vllm中的openai_embedding_client模块。由于vllm版本为0.5.3，在使用特定的嵌入模型时返回的嵌入长度为8192而非期望的4096，可能是由于模型配置或调用参数设置不正确导致的问题。

https://github.com/vllm-project/vllm/issues/6743
这是一个bug报告，涉及到 vllm 项目中的 speculative decode seeded test。该问题是由于未正确测试 spec 解码情况下的 perrequest 种子导致的。

https://github.com/vllm-project/vllm/issues/6741
这是一个安装问题类的bug报告，主要涉及的对象是vllm库。该问题由于无法导入特定的模块而导致 ImportError 错误。

https://github.com/vllm-project/vllm/issues/6740
这是一个用户提出需求的issue，主要涉及的对象是vllm项目中的前端部分。由于需要将`run_server`拆分为`build_server`和`run_server`两部分，以实现更灵活的服务器配置。

https://github.com/vllm-project/vllm/issues/6739
这是一个优化型的问题，涉及到了Mamba缓存单一缓冲器的分配优化。

https://github.com/vllm-project/vllm/issues/6738
这是一个Bug报告类型的issue，主要涉及的对象是vllm中的FP8模型和KV-Cache-Scales，问题可能由于nvidiaammo工具和vllm之间的不兼容导致，用户在加载一起使用FP8模型和KV-Cache-Scales时遇到问题。

https://github.com/vllm-project/vllm/issues/6737
这个issue是关于GPU性能利用率低的报告问题，涉及主要对象为vllm模型的推理过程。造成GPU计算性能利用率低的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/6736
类型是bug报告，主要对象是vllm库，由于传入了不支持的参数'ignore_patterns'导致 TypeError 错误。

https://github.com/vllm-project/vllm/issues/6735
这是一个Bug报告，主要涉及的对象是与批量推断相关的响应不一致，可能由于批大小设置不同导致了推断结果不一致。

https://github.com/vllm-project/vllm/issues/6734
这是一个bug报告类型的issue，主要涉及的对象是vllm-0.5.3.post1版本下部署Qwen2-72b-instruct-awq模型时出现的CUDA错误。导致这个bug的原因可能是高并发时出现了非法内存访问导致的异常。

https://github.com/vllm-project/vllm/issues/6733
这个issue是关于bug报告，主要涉及到了CC（Seed issue with Pipeline Parallel），由于baseline_llm_generator和test_llm_generator设置了相同的全局种子，导致测试未能正确验证每个请求的种子，需要允许baseline和test使用不同的种子。

https://github.com/vllm-project/vllm/issues/6732
这是一个bug报告，主要涉及到VLLM 0.5.3.post1版本在运行过程中出现了RuntimeError: NCCL error的问题。

https://github.com/vllm-project/vllm/issues/6731
这是一个用户提出需求的类型的issue，主要涉及对象是Llama-3.1，用户寻求为Llama-3.1 添加支持的帮助。

https://github.com/vllm-project/vllm/issues/6730
这是一个bug报告，涉及的主要对象是与下载链接兼容性有关的问题。这个问题是因为当设置VLLM_USE_MODELSCOPE为True时，使用snapshot_download()时出现了TypeError错误，原因是该函数不支持'ignore_patterns'参数。

https://github.com/vllm-project/vllm/issues/6729
这是一个用户提出的调整OpenAI API服务器的issue，主要涉及的对象是OpenAI API服务器。由于缺乏具体描述，无法确定具体问题所在。

https://github.com/vllm-project/vllm/issues/6728
该issue类型为用户提出需求，主要对象是vLLM在Ascend NPU上的支持。由于Ascend NPU具有强大的计算能力，用户希望vLLM可以在Ascend NPU上运行，以服务更多用户。

https://github.com/vllm-project/vllm/issues/6727
这是关于bug报告的issue，主要涉及VLLM模型的配置问题，导致了无法找到 'adapter_name_or_path' 的错误提示。

https://github.com/vllm-project/vllm/issues/6726
这是一个bug报告，涉及的主要对象是vLLM下的enablechunkedprefill功能。由于启用了该功能，在发送具有长提示的并发请求时，调度器会跳过一些请求，导致请求被拒绝。

https://github.com/vllm-project/vllm/issues/6725
这是一个Bug报告，主要涉及vllm在Ubuntu 20.04上的8-way tensor parallelism w/ Punica功能问题。由于GLIBC版本限制可能导致了Tensor Parallelism和Punica支持不稳定的bug。

https://github.com/vllm-project/vllm/issues/6724
这是一个Bug报告，涉及到vLLM下的Chameleon模型token padding的问题。由于直接传入提示token ids会导致Chameleon在填充过程中出错。

https://github.com/vllm-project/vllm/issues/6723
这是一个bug报告类型的issue，涉及的主要对象是代码中的某个函数/方法。由于布尔表达式判断错误导致了程序崩溃，触发了断言失败的异常。

https://github.com/vllm-project/vllm/issues/6722
这是一个bug报告，涉及的主要对象是vllm的engine。由于某些原因导致了 AsyncEngineDeadError 和 RuntimeError 的问题。

https://github.com/vllm-project/vllm/issues/6721
这个issue是用户提出需求类型，主要对象是GLM4 function call的支持。由于缺乏对GLM4 function call的支持，用户希望了解是否可以添加此功能。

https://github.com/vllm-project/vllm/issues/6720
这个issue属于bug报告类型，涉及到代码中的attention模块的前缀启用导致的输入参数顺序错误问题，可能是由于参数顺序错误导致了功能实现的错误。

https://github.com/vllm-project/vllm/issues/6719
这是一个Bug报告类型的issue，主要涉及的对象是vllm的温度参数设置。由于temperature=0时未能实现贪婪抽样，可能是由于代码逻辑或实现方式的问题导致的。

https://github.com/vllm-project/vllm/issues/6718
这是一个关于当前环境信息的issue，涉及的主要对象是vllm。由于未能明确表达具体问题或需求，无法判断是bug报告还是用户提出需求，以及未明确提出问题或寻求帮助。

https://github.com/vllm-project/vllm/issues/6717
这是一个Bug报告，涉及到在docker环境下出现错误的问题。由于修改了代码中的参数，导致在docker中调用`init_device`函数时出现了错误。

https://github.com/vllm-project/vllm/issues/6716
这是一个Bug报告类型的issue。该问题涉及的主要对象是VLLM模型的内存管理。由于内存对象无法清理，导致了Shard Memory对象的问题。

https://github.com/vllm-project/vllm/issues/6715
该issue属于功能需求问题，主要涉及logits processor的tensor缓存，并提出了缓存机制的更新。

https://github.com/vllm-project/vllm/issues/6714
这是一个功能需求类型的issue，主要涉及到MLP Speculator CI测试的优化，需要将模型从当前的3B换成更小的llama160maccelerator模型，以提高速度和节省内存。

https://github.com/vllm-project/vllm/issues/6713
这是一个Bug报告，涉及到llavav1.6模型的运行问题，由于找不到适合执行此计算的引擎而导致RuntimeError。

https://github.com/vllm-project/vllm/issues/6712
该issue是关于对model runner/input builder developer APIs进行调整的改进请求。

https://github.com/vllm-project/vllm/issues/6711
该issue是关于构建/持续集成（CI）的改进，主要涉及修复从Docker Hub仓库中拉取容器的访问限制问题。可能原因是之前存在访问限制导致无法成功拉取容器。

https://github.com/vllm-project/vllm/issues/6710
这是一个bug报告，主要涉及到构建过程中使用sccache的问题，可能由于条件判断错误导致构建不正确。

https://github.com/vllm-project/vllm/issues/6709
这是一个关于功能需求的issue，主要涉及的对象是Prompt adapters，由于缺乏对 *.pt 类型的支持，导致用户提出需要支持该类型的prompt adapters。

https://github.com/vllm-project/vllm/issues/6708
这是一个关于bug修复的issue，主要涉及到vLLM下的cudagraph capture功能。由于之前的cudagraph capture逻辑不支持每个VE的图表捕获，导致在flashinfer封装中出现了大小不匹配的问题。

https://github.com/vllm-project/vllm/issues/6707
这是一个Bug报告，涉及到vllm的Engine在max_tokens未定义时崩溃的问题，导致整个实例崩溃并且未来的调用也失败。

https://github.com/vllm-project/vllm/issues/6706
这是一个优化建议类型的issue，主要涉及到持续集成（CI）和夜间基准测试（nightly benchmark）的流程。

https://github.com/vllm-project/vllm/issues/6705
这是一个Bug报告，主要涉及VLM下的Streaming功能，由于某种原因导致了Streaming chat completions在最后一个chunk中未返回CompletionUsage。

https://github.com/vllm-project/vllm/issues/6704
这个issue类型是需求修正，主要涉及的对象是构建过程中的 wheel 大小限制，因为可能会导致过大的wheel文件无法上传到pypi。

https://github.com/vllm-project/vllm/issues/6703
这是一个Bug报告，主要涉及LLama 405B，由于on GPU advance step optimization导致运行多个并发请求时崩溃。

https://github.com/vllm-project/vllm/issues/6702
这是一个CI测试相关的issue，用于添加非均匀AutoFP8量化的Smoke测试。

https://github.com/vllm-project/vllm/issues/6701
这是一个bug报告，涉及对象为multi-GPU inference (tensor_parallel_size=2)，由于Intel GPUs导致multi-GPU inference失败。

https://github.com/vllm-project/vllm/issues/6700
这是一个Bug报告类型的Issue，主要涉及的对象是vLLM 0.5.3和LLAMA 3.1 405B FP8模型。由于某种原因，vLLM 0.5.3在加载LLAMA 3.1 405B FP8模型时会出现卡住的情况。

https://github.com/vllm-project/vllm/issues/6699
这是一个Bug报告类型的issue，主要涉及OpenAI API Completions和Chat API的不一致性。原因是环境信息收集时出现了问题。

https://github.com/vllm-project/vllm/issues/6698
这是一个bug报告类型的issue，主要涉及到vllm项目中与种子相关的实现。导致bug的原因是per-request种子的实现假设在pipeline parallel情况下，抽样发生在与驱动程序工作进程相同的进程中，但实际情况并非总是如此。

https://github.com/vllm-project/vllm/issues/6697
这是一个Bug报告，主要涉及vllm在Triton服务器上无法正常工作的问题。可能是由于引擎后台任务失败导致的错误。

https://github.com/vllm-project/vllm/issues/6696
这是一个版本升级的issue，主要涉及到软件版本号的更新。

https://github.com/vllm-project/vllm/issues/6695
这是一个Bug修复类的Issue，主要涉及通信优化，通过替换发送操作为部分发送加全局收集，提高了性能。

https://github.com/vllm-project/vllm/issues/6694
这是一个bug报告，主要涉及到chunked prefill功能的log error修复。这个bug是由于log error导致的。

https://github.com/vllm-project/vllm/issues/6693
这是一个 bug 报告，主要涉及 RoPE 错误在 Llama 3.1 中的修复。这可能是由于 RoPE 错误导致程序功能异常或崩溃。

https://github.com/vllm-project/vllm/issues/6692
这是一个用户提出需求的issue，主要对象是vllm中的Meta Llama 3.1版本。用户提出了关于修改配置文件以加载特定数值的需求。

https://github.com/vllm-project/vllm/issues/6691
这个issue属于文档修改类型，涉及到distributed部分的文档内容，主要原因是需要修正文件中的参数顺序以保持一致性。

https://github.com/vllm-project/vllm/issues/6690
这是一个bug报告类型的issue，主要涉及的对象是更新`transformers`版本以解决Llama 3.1中的问题，以及引入新的测试引擎。这是由于markdown渲染无法正常工作，所以需要使用原始的html代码。

https://github.com/vllm-project/vllm/issues/6689
这是一个bug报告类型的issue，主要涉及到Meta Llama 3.1模型。由于升级到 transformers 4.43.1 之后出现了一些rope scaling相关的错误，需要查看并调整相关设置。

https://github.com/vllm-project/vllm/issues/6688
这个issue类型为文档更新，主要涉及 llama3.1 版本的支持。

https://github.com/vllm-project/vllm/issues/6687
该issue类型为用户提出需求，涉及主要对象为vTensor - Flexible Virtual Tensor Management for Efficient LLM Serving。由于该功能看起来很棒，用户可能在探讨如何利用vTensor来提高LLM服务的效率。

https://github.com/vllm-project/vllm/issues/6686
这是一个Bug修复类型的issue，主要涉及到vLLM项目中有关日志记录的逻辑。导致问题的原因是日志记录顺序不当，导致时间计算错误，进而使得time_to_first_token_seconds不准确。

https://github.com/vllm-project/vllm/issues/6685
这是一个bug报告，主要涉及到ChatCompletionRequest类中SamplingParams的logprobs变量问题。由于变量logprobs应该是一个整数，但代码中设置为布尔类型导致了该问题。

https://github.com/vllm-project/vllm/issues/6684
这是一个性能优化讨论的issue，涉及优化_prepare_model_input_tensors函数，作者提出了关于加速循环并实现cuda kernel的问题，希望了解优化计划和时间表。

https://github.com/vllm-project/vllm/issues/6683
这是一个bug报告，涉及到vllm在A40-48G环境下的源码编译失败问题，其根本原因可能是与cuda-12.1相关的安装或配置问题。

https://github.com/vllm-project/vllm/issues/6682
这个issue是一个bug报告，主要涉及的对象是在加载两个模型时出现CUDA内存溢出错误。

https://github.com/vllm-project/vllm/issues/6681
这是一个关于优化性能和类型检查的issue，涉及的主要对象是一些Python文件和目录。原因是为了实施更严格的类型检查，更新相关设置以解决问题。

https://github.com/vllm-project/vllm/issues/6680
这是一个用户提出需求的issue，主要涉及到需要在无法使用docker的环境下从源代码构建vLLM，由于无法使用docker，用户需要提供详细的构建指令。

https://github.com/vllm-project/vllm/issues/6679
这个issue是关于代码质量和发布流程的问题，涉及到vLLM项目中的Chameleon模型支持。原因是需要`transformers`来支持该模型的发布，导致了该PR将其移出支持列表。

https://github.com/vllm-project/vllm/issues/6678
这个issue类型是用户提出需求，请教问题等，主要涉及将容器镜像发布到额外的镜像注册表（qhcr或quay.io），由于需要增加冗余备份，用户提出了这个建议。

https://github.com/vllm-project/vllm/issues/6677
这是一个关于在 vLLM 中调整 Ada Lovelace 的 FP8 内核性能的 kernel 类型的 issue。

https://github.com/vllm-project/vllm/issues/6676
这是一个用户提出需求类型的issue，主要涉及的对象是Prefix Caching功能，用户询问是否支持将其卸载到CPU以及是否有计划支持此功能。

https://github.com/vllm-project/vllm/issues/6675
这是一个关于版本合并的issue，主要涉及代码库的维护和更新。

https://github.com/vllm-project/vllm/issues/6674
这是一个版本更新（feature request）类型的issue，主要涉及的对象是软件版本号。原因可能是需要发布新功能或修复现有功能中的问题。

https://github.com/vllm-project/vllm/issues/6673
这是一个用户提出需求的问题，主要对象是模型加载器。由于缺少支持忽略模式的功能，导致用户体验不佳。

https://github.com/vllm-project/vllm/issues/6672
这是一个bug报告，主要涉及的对象是加载pt模型检查点，由于只有第一个rank会加载tqdm，导致之前的输出不可见。

https://github.com/vllm-project/vllm/issues/6670
这是一个功能改进类的issue，主要对象是用户界面，导致这个问题的原因是缺少加载模型的日志信息。

https://github.com/vllm-project/vllm/issues/6669
这是一个Bug报告类型的Issue，主要涉及的对象是VLLM代码库中的`_get_stats`实现。由于当前实现假设每个序列在每次迭代都会获得一个新的token，但实际上在speculative decoding中这种假设不成立，导致`num_generation_tokens`统计结果不准确。

https://github.com/vllm-project/vllm/issues/6668
这是一个Bug报告，涉及Vllm模型中的DeepSeek-V2-Chat-0628，由于ChildProcessError导致worker进程崩溃。

https://github.com/vllm-project/vllm/issues/6667
这个issue是关于将fp8支持添加到`reshape_and_cache_flash`中的特性改进，不是bug报告。

https://github.com/vllm-project/vllm/issues/6666
这是一个功能增强类的issue，主要涉及长上下文模型的chunked prefill操作。由于长上下文模型可能出现OOM错误，因此需要默认启用chunked prefill，避免初始内存分配过大导致的问题。

https://github.com/vllm-project/vllm/issues/6665
这是一个bug报告类型的issue，主要涉及FBGEMM Fp8 quantization中的null `modules_to_not_convert`导致的错误。

https://github.com/vllm-project/vllm/issues/6664
该issue类型为需求提出，涉及主要对象是SPMD worker，由于描述内容待填写，用户可能在尝试使用SPMD worker来减少控制平面通信时遇到了困难，需要相关方面提供帮助。

https://github.com/vllm-project/vllm/issues/6663
这个issue是关于bug报告，涉及主要对象是distributed_ray_executor，由于早期异常阻止了正确初始化它，导致了访问compiled_dag时出现属性错误。

https://github.com/vllm-project/vllm/issues/6662
这个issue属于用户提出需求类型，主要涉及的对象是vLLM模型。增加默认chunk大小到2048可能是为了提高模型性能或者解决特定应用场景下的问题。

https://github.com/vllm-project/vllm/issues/6661
这个issue类型是功能需求提议，主要涉及的对象是在Docker容器中添加vim编辑器。由于需要在Docker容器内部进行代码调试和修改，用户提出了需要在Docker中添加vim编辑器的需求。

https://github.com/vllm-project/vllm/issues/6660
该issue类型为用户提出需求，主要涉及对象为如何在vllm中禁用日志记录。可能由于用户需要在使用vllm时禁用LLM类的日志记录，所以提出了这个问题。

https://github.com/vllm-project/vllm/issues/6659
这是一个需求类型的issue，主要涉及的对象是VLLM模型中的beam search功能。由于代码中存在过时警告，需要移除以优化代码。

https://github.com/vllm-project/vllm/issues/6658
这个issue类型是性能优化，主要涉及对Llama3推断优化的相关工作。原因为提供一系列的优化措施，以使推断栈在更少的设备上运行大规模模型或在使用更多GPU时获得最高吞吐量。

https://github.com/vllm-project/vllm/issues/6657
该issue属于用户提出需求类型，主要涉及fp8.py中添加忽略层的功能。由于在llama.py中进行逻辑处理会导致难以维护，因此提出了对Fp8Config.get_quant_method(layer)方法进行扩展，以实现忽略特定层的功能需求。

https://github.com/vllm-project/vllm/issues/6656
这是一个用户提出需求的issue，主要涉及的对象是在构建CUDA 11.8 wheel时使用不同的sccache存储桶。

https://github.com/vllm-project/vllm/issues/6655
这是一个bug报告，主要涉及的对象是DeepSeek-Coder-V2-Lite-Instruct，在CPU上运行时尝试使用了CUDA，导致问题发生。

https://github.com/vllm-project/vllm/issues/6654
该issue类型为文档更新，涉及主要对象为vLLM文档。原因是文档中的警告需要更新以反映最新工作和版本发布计划。

https://github.com/vllm-project/vllm/issues/6653
这个issue类型是文档更新，主要涉及的对象是AutoAWQ文档。导致这个问题的原因是AutoAWQ内容已过时，需要更新以反映vLLM的新版本。

https://github.com/vllm-project/vllm/issues/6652
这是一个用户提出需求的issue，主要涉及的对象是ChatCompletion模块。

https://github.com/vllm-project/vllm/issues/6651
这个issue是关于Bug报告，主要涉及vLLM中动态非对称量化内核的问题。由于竞态条件的存在，导致某些token在单元测试中非确定性地失败，即使在调试时打印语句显示该token总是通过，进一步增加了可能的原因。

https://github.com/vllm-project/vllm/issues/6650
这个issue类型是bug报告，主要涉及的对象是VLLM + RAY系统。由于在多个节点上运行时出现错误，猜测可能是由于节点之间的通信或数据同步问题导致。

https://github.com/vllm-project/vllm/issues/6649
这是一个Bugfix类型的issue，涉及Kernel，主要问题是由于循环计数器为int32_t，在模型具有100万的最大序列长度时导致访问非法内存地址。

https://github.com/vllm-project/vllm/issues/6648
这是一个关于修改批次大小（batch size）的bug报告，涉及到在AMD GPU上使用vllm的过程中批次大小动态变化的问题。

https://github.com/vllm-project/vllm/issues/6647
这是一个 bug 报告，主要涉及 vLLM 库的主分支大小超过 200MB，导致 Docker 构建过程失败。

https://github.com/vllm-project/vllm/issues/6646
这是一个bug报告，主要涉及的对象是SamplingParams()方法中的n参数设置。由于n参数大于256时，生成输出的过程会无限期挂起，导致GPU利用率为0，这可能是由于代码中存在死循环或内存溢出等问题。

https://github.com/vllm-project/vllm/issues/6645
这个issue是一个bug报告，涉及到SpecDecode统计日志在默认设置下很难被记录，导致了统计数据无法及时输出的问题。

https://github.com/vllm-project/vllm/issues/6644
这是一个bug报告，涉及到PaliGemma serving的问题，由于某种原因导致运行docker容器和执行Python代码后出现错误输出。

https://github.com/vllm-project/vllm/issues/6643
这是一个bug报告，主要涉及的对象是`AsyncMetricsCollector`。由于`AsyncMetricsCollector`在收集speculative decoding metrics时需要与log output的时间间隔完全匹配，导致实际上很少记录speculative decoding stats。

https://github.com/vllm-project/vllm/issues/6642
这是一个bug报告，涉及的主要对象是vllm的metrics模块。用户遇到了一个错误导致计数器只能被非负数递增的问题。

https://github.com/vllm-project/vllm/issues/6641
这个issue类型为用户请教问题，主要涉及的对象是vllm工具中的max_num_seqs和max_model_len参数。用户询问这两个参数的作用和操作阶段，以及由于何种原因导致设置参数后仍可以处理超过预期长度的输入。

https://github.com/vllm-project/vllm/issues/6640
这是一个Bug报告，涉及vLLM在AWS Inferentia上失败的问题。由于未指定环境，尝试使用不同版本的vLLM均遇到了相同的问题。

https://github.com/vllm-project/vllm/issues/6639
这是一个bug报告，主要涉及安装vllm-0.5.2在cuda-11.8上遇到的问题，由于`vllmflashattn`不支持cuda11.8导致安装失败。

https://github.com/vllm-project/vllm/issues/6638
这是一个bug报告，主要涉及vllm库下的Qwen1.5版本中使用engine.generate()方法时streaming生成的token存在错误。这可能是由于文本字段在streaming结果中出错，但是token_ids字段正常导致的。

https://github.com/vllm-project/vllm/issues/6637
这个issue是一个[Frontend]类型的需求提出，主要涉及的对象是 vLLM 的前端。由于缺少 `add_special_tokens` 参数，用户无法在 OpenAI API Completions Endpoint 中指定该参数。

https://github.com/vllm-project/vllm/issues/6636
这是一个Bug报告，主要涉及vllm在执行`vllm serve`时出现卡住的问题。造成这个问题的原因是什么导致了vllm无法正常运行。

https://github.com/vllm-project/vllm/issues/6635
这是一个bug报告，针对vllm在AWS neuron上无法工作的问题。

https://github.com/vllm-project/vllm/issues/6634
这是一个用户提出需求的issue，主要涉及Chat Templates based on glm4的创建，可能是因为缺乏关于如何基于glm4制作Chat模板的相关信息。

https://github.com/vllm-project/vllm/issues/6633
这是一个Model相关的issue，涉及主要对象是Chameleon模型，用户希望添加支持图像和文字输入的功能，由于当前实现仅允许每个请求只有一个图像，导致无法正常渲染markdown并需使用原始HTML。

https://github.com/vllm-project/vllm/issues/6632
这个issue是关于bug报告，主要涉及的对象是vllm库以及ray连接，由于尝试使用ray连接时遇到ConnectionError导致超时，可能是由于网络连接问题或程序配置错误导致的。

https://github.com/vllm-project/vllm/issues/6631
这是一个bug报告，主要涉及vllm在函数调用模式下无法正常工作的问题。原因是vllm没有正确使用通过OpenAI接口传递的工具信息参数。

https://github.com/vllm-project/vllm/issues/6630
这是一个Bug报告，涉及到exceptiongroup.ExceptionGroup中出现未处理错误的问题。导致这个问题的原因是ASGI应用程序中出现了异常。

https://github.com/vllm-project/vllm/issues/6629
这是一个Bug报告，涉及的主要对象是OpenAI server。 由于环境信息中显示版本不匹配或配置不正确，导致OpenAI服务器意外关闭。

https://github.com/vllm-project/vllm/issues/6628
This issue is a documentation related issue in vLLM, specifically regarding the PR checklist section. It appears that the markdown rendering is not working properly, leading to the need for using raw HTML in this part of the document.

https://github.com/vllm-project/vllm/issues/6627
这个issue属于CI/Build类型，涉及到对Alex的AWQ Marlin进行集成测试。由于markdown渲染问题，内容使用了原始的html标记。

https://github.com/vllm-project/vllm/issues/6626
这是一个bug报告，涉及的主要对象是前端。该问题由于在OpenAIcompatible服务器的当前实现中，无法正确表示诸如表情符号等Unicode代码点的部分导致了无法确定实际token ID的情况。

https://github.com/vllm-project/vllm/issues/6624
这个issue是关于bugfix的，主要涉及到vLLM模型中`vocab_size`字段访问的问题。由于`vocab_size`被移动到`text_config`中而导致vLLM在加载模型时出现属性访问错误。

https://github.com/vllm-project/vllm/issues/6623
这是关于性能的讨论，而不是bug报告。主要讨论了在vLLM上运行不同模型时，Llava 7B使用较小的批量大小和 GPU 块数。

https://github.com/vllm-project/vllm/issues/6622
这是一个安装问题报告，主要涉及对象是安装vllm时遇到torch版本不匹配的问题，可能是由于torch版本不符合要求导致的。

https://github.com/vllm-project/vllm/issues/6621
这个issue类型是Model，涉及的主要对象是Phi3HDImageEmbedding，由于需要重构和解耦这个模块。

https://github.com/vllm-project/vllm/issues/6620
这是一个用户提出需求的issue，涉及的主要对象是VLLM，用户问题是关于支持奖励模型API的功能是否可用。

https://github.com/vllm-project/vllm/issues/6619
这个issue类型是代码质量问题，该问题单涉及的主要对象是代码中滥用的`noqa`。

https://github.com/vllm-project/vllm/issues/6618
该issue是一个功能增强请求，主要涉及的对象是torch.inference_mode。由于一些硬件后端（如TPU）不支持torch.inference_mode，导致需要引入一个包装类来为不支持的后端回退到torch.no_grad。

https://github.com/vllm-project/vllm/issues/6617
这是一个性能改进的建议，主要涉及到FlashInfer和FlashAttention后端的比较。由于FlashInfer并没有像博客中描述的那样带来较大的性能提升，用户提出了对性能回归的报告，并讨论了可能的原因。

https://github.com/vllm-project/vllm/issues/6616
这是一个bug报告，主要涉及对象是DeepSeek-V2 - MI300x 模型，由于最新版本的rocmlvllm不支持DeepSeek-V2Chat0628模型导致vllm崩溃无法加载。

https://github.com/vllm-project/vllm/issues/6615
这是一个用户提出需求的issue，主要对象是vllm中的4D attention mask功能。由于Huggingface提供了这一接口，用户想了解vllm是否有计划集成这一功能。

https://github.com/vllm-project/vllm/issues/6614
这是一个bug报告，涉及的主要对象是程序在共享内存中未找到可用的block，可能是由于程序在特定环境中无法正常运行所引起的。

https://github.com/vllm-project/vllm/issues/6613
这是一个关于VLM添加支持图像嵌入作为输入的问题报告，属于特性需求类型，涉及主要对象是VLM。造成这个问题的原因是需要支持直接将图像嵌入传递给语言模型。

https://github.com/vllm-project/vllm/issues/6612
This is a feature request for adding AWQ support to the Marlin kernel, involving modifications and additions to support AWQ quantization. The issue was raised to enhance the performance of AWQ models on specific GPU configurations and to plan future testing and improvements.

https://github.com/vllm-project/vllm/issues/6611
这是一个关于支持Nemotron模型的issue，该issue类型是功能请求，涉及的主要对象是Nemotron-3、Nemotron-4和Minitron模型。

https://github.com/vllm-project/vllm/issues/6610
这是一个性能问题（Performance issue），主要对象是多节点并行 pipeline，用户询问为什么在多个节点间提升带宽后没有提升性能，寻求可能的瓶颈和进一步测试方法。

https://github.com/vllm-project/vllm/issues/6609
这是一个关于Bug修复的issue，主要涉及Marlin FP8中的channelwise支持问题。由于channelwise支持在Marlin FP8中出现问题，导致了某些张量的破坏。

https://github.com/vllm-project/vllm/issues/6608
这是一个未完善的issue，类型为功能开发，涉及Marlin固件。这个issue可能由于新功能开发的进度不完整而未完善。

https://github.com/vllm-project/vllm/issues/6607
这是一个bug报告，主要涉及的对象是使用Ray作为分布式后端加载任何Phi3 mini/small模型时出现错误。造成这个问题的原因可能是与项目无关的bug。

https://github.com/vllm-project/vllm/issues/6606
这个issue是一个[ Kernel ]类型的问题，主要涉及的对象是`fp8-marlin`与`fbgemm-fp8`模型，由于需要将`fp8marlin`打开以在Ampere上运行W8A16，并且由于Markdown渲染不起作用，需要使用原始HTML格式进行编写。

https://github.com/vllm-project/vllm/issues/6605
该issue属于bug报告类型，主要涉及了使用LoRA Adapter与vllm中的NeMO出现的Vocabulary size参数不兼容的问题。

https://github.com/vllm-project/vllm/issues/6604
该issue属于用户提出需求类型，主要对象是支持多模态对话交互的VLM模型。用户提出了需要在嵌入式设备上部署图像编码器和投影仪，并在请求中发送编码向量以减少数据传输量和降低延迟的需求。

https://github.com/vllm-project/vllm/issues/6603
这是一个Bug报告，主要涉及的对象是VLLM Docker环境。由于环境中存在未知的Version信息，可能导致加载mistral和gemma模型时出现错误。

https://github.com/vllm-project/vllm/issues/6602
这是一个功能需求类型的Issue，主要涉及到前端的chat utils模块移动，用户希望通过此操作更容易地为离线LLM添加多模态支持。

https://github.com/vllm-project/vllm/issues/6601
这是一个用户需求提出的issue，主要涉及到如何在vllm中使用类似transformers中device_map方法来指定每个层的部署方式。问题的根本原因是用户所需的模型需要大于可用GPU的总内存，且无法满足并行部署GPU数量必须为32的倍数的要求。

https://github.com/vllm-project/vllm/issues/6600
这是一个非bug类型的用户提出需求的issue，涉及管理HTTP连接，旨在重用同步/异步客户端会话以及封装常见请求使用的代码。

https://github.com/vllm-project/vllm/issues/6599
这是一个需求提出的issue，主要涉及的对象是实现多节点支持。由于目前不支持InfiniBand，用户提出需要添加相应功能以支持该特性。

https://github.com/vllm-project/vllm/issues/6598
这是一个文档更新的问题，用户提出了关于VLLM中PP文档的需求。

https://github.com/vllm-project/vllm/issues/6597
这个issue属于bug报告类型，涉及主要对象是分布式共享内存功能。由于多节点情况下出现bug，可能导致共享内存操作不正确。

https://github.com/vllm-project/vllm/issues/6596
该issue是一个提出需求的类型，主要涉及到模型输入构建和注意力元数据构建的优化。

https://github.com/vllm-project/vllm/issues/6595
该issue类型为用户提出需求，主要涉及对象为vllm下的Gemba29b语言模型。由于Gemba29b目前受到4K滑动窗口的限制，用户提出希望支持8K token context，以加速开发完全本地代理系统的进展。

https://github.com/vllm-project/vllm/issues/6594
该issue类型是bug报告，主要涉及的对象是前端服务。由于AsyncLLMEngine发生错误会导致服务响应错误并最终需等待容器重启，故用户提出需要在引擎出错时立即终止服务器运行的需求。

https://github.com/vllm-project/vllm/issues/6593
这个issue是一个[Kernel]类型的问题，需要更新FP8 dynamic per token quantization kernels来添加`scale_ub`输入。

https://github.com/vllm-project/vllm/issues/6592
这是一个需求提出的issue，主要涉及vLLM项目中对于FP8量化中"ignored_layers"支持的缺失。原因是vLLM目前没有遵循"ignored_layers"字段，而是对所有层中的所有模块应用统一的量化。

https://github.com/vllm-project/vllm/issues/6591
这是一个Bug报告，主要涉及Intel GPU测试在CI构建中失败的问题，可能是由于测试未通过导致CI构建失败。

https://github.com/vllm-project/vllm/issues/6590
这是一个针对vLLM项目的[Kernel]类型的issue，主要涉及到启用动态分配`fp8`的问题，导致的原因可能是希望改进模型在处理压缩张量时的性能。

https://github.com/vllm-project/vllm/issues/6589
这个issue属于Bug报告类型，主要涉及VLLM中输出采样时的逻辑选择。该问题源于当前的采样heuristic导致生成的解决方案偏向短序列，建议通过平均累积log_probs来改进选择逻辑。

https://github.com/vllm-project/vllm/issues/6587
这个issue是一个Bugfix类型的报告，涉及主要对象是vLLM server在使用Pipeline Parallel时可能发生的Crash问题。由于请求在处理过程中被中止，可能导致出现KeyErrors而导致Crash问题。

https://github.com/vllm-project/vllm/issues/6586
这是一个bug报告，主要涉及vLLM下的Pipeline Parallel的运行时环境，描述了在请求被中止时可能导致服务器崩溃的问题。

https://github.com/vllm-project/vllm/issues/6584
这是一个用户提出需求的issue，主要涉及LLM2Vec（Fine-Tuned Embeddings）的支持，由于LlamaBiModel目前不受支持，用户希望能够解决这个问题。

https://github.com/vllm-project/vllm/issues/6582
这个issue是一个bug报告，涉及的主要对象是vllm，并由于CUDA_VISIBLE_DEVICES设置为特定的MIG或GPU uuid导致 vllm 不支持多实例 GPU。

https://github.com/vllm-project/vllm/issues/6581
这是一个bug报告，主要涉及前端部分，由于重复初始化记录器导致了问题。

https://github.com/vllm-project/vllm/issues/6580
这个issue属于文档更新类型，主要涉及wheel位置相关内容。由于切换到版本不可知的wheels，所以需要更新文档中nightly wheel的链接。

https://github.com/vllm-project/vllm/issues/6579
这是一个bug报告，主要涉及w8a8_utils.py文件中的input_scale参数。这个问题是由于input_scale参数的文档不清晰导致的。

https://github.com/vllm-project/vllm/issues/6578
这是一个bug报告，主要涉及对象是AsyncMetricsCollector。由于原始代码未能按照指定的时间间隔进行度量数据的同步，导致了_last_metrics_collect_time未能重置，从而出现了bug。

https://github.com/vllm-project/vllm/issues/6577
这是一个Bug报告，涉及的主要对象是`AsyncMetricsCollector`类。这个问题存在的原因是`_last_metrics_collect_time`变量没有在每次收集时重置，导致性能开销增加。

https://github.com/vllm-project/vllm/issues/6576
这个issue类型是用户提出需求，关于fp8 quantization在ROCm中的支持问题。

https://github.com/vllm-project/vllm/issues/6575
这是一个bug报告，主要涉及到vllm框架中的CUDA错误问题，用户提出了打印输入到llama模型时出现CUDA错误的情况，希望获得解决方法。

https://github.com/vllm-project/vllm/issues/6574
这个issue类型是bug报告，涉及主要对象是vllm中使用internlm/internlm2_57bchat模型。由于运行vllm容器在Kubernetes中时加载internlm/internlm2_57bchat模型时出现了"ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error."错误。

https://github.com/vllm-project/vllm/issues/6573
该issue类型为用户提出需求，涉及主要对象为VLLM项目中的Lora Adapter功能。由于新版mistral模型生成的safetensors与VLLM期望的权重键名不同，以及包含VLLM难以处理的新键名，用户希望能实现一个更新，使得从mistral-finetune生成的Lora safetensors文件可以直接加载为Lora适配器，而无需手动映射权重键名或处理陌生的键名。

https://github.com/vllm-project/vllm/issues/6572
这个issue是一个bug报告，涉及的主要对象是vllm中的async_llm_engine。由于版本兼容性问题导致程序崩溃并触发了异常退出，需要进一步排查原因。

https://github.com/vllm-project/vllm/issues/6571
这是一个用户提出需求的类型 issue，主要涉及的对象是 Llava-Next-Video 模型。由于需要实现视频处理和文本嵌入的合并，用户正在寻求对该模型的支持。

https://github.com/vllm-project/vllm/issues/6570
这个issue类型是用户提出需求，请求升级vLLM的numpy依赖版本。

https://github.com/vllm-project/vllm/issues/6569
这是一个bug报告，主要涉及加载模型时无法及时显示进度，以及可能缺少部分权重文件导致模型加载后结果错误的问题。

https://github.com/vllm-project/vllm/issues/6568
这是一个bug报告，针对prefix-caching benchmark tool的增强功能。

https://github.com/vllm-project/vllm/issues/6567
这是一个bug报告，主要涉及vllm在Pascal GPU上使用gptq模型出现错误的问题。问题出现的原因可能是在处理长提示时出现错误。

https://github.com/vllm-project/vllm/issues/6566
这是一个提出新功能的issue，涉及到在api server中支持加载和卸载LoRA适配器的功能。

https://github.com/vllm-project/vllm/issues/6565
这个issue类型是bug报告，涉及的主要对象是MI50 GPU。由于MI50 GPU不支持某些指令，导致编译时出现错误信息。

https://github.com/vllm-project/vllm/issues/6564
这是一个Bug报告，涉及的主要对象是Distributed Inference and Serving。由于缺少vllm.commit_id模块，导致无法读取commit hash，进而出现RuntimeWarning。

https://github.com/vllm-project/vllm/issues/6563
该issue是一个功能需求类型的问题，主要涉及vllm项目对于支持新模型"Mistral-Nemo"的困难。

https://github.com/vllm-project/vllm/issues/6562
这是一个Bug报告，主要涉及vllm库无法导入的问题，由于GLIBC_2.32版本缺失导致。

https://github.com/vllm-project/vllm/issues/6561
这是一个bug报告类型的issue，主要涉及vllm中无法充分利用全部VRAM来增加上下文长度的问题。这可能是由于参数设置不当或程序逻辑错误导致的。

https://github.com/vllm-project/vllm/issues/6560
该issue类型为性能问题反馈，主要涉及的对象是在H100上运行大批量的Vicuna模型时的GPU利用率低的问题，可能导致性能下降。

https://github.com/vllm-project/vllm/issues/6559
这是一个关于支持 `fbgemm` checkpoints 的问题，类型为 PR（Pull Request）提交，涉及的主要对象是代码库中的 `fbgemm` 模块。

https://github.com/vllm-project/vllm/issues/6558
这是一个bug报告，主要涉及fp8模型加载错误的问题，可能是由于模型量化的过程中导致的。

https://github.com/vllm-project/vllm/issues/6557
这个issue是用户提出修改vLLM核心逻辑的请求，允许自定义Executor和TokenizerGroup类，为了进行易于实验和定制化部署。

https://github.com/vllm-project/vllm/issues/6556
这个issue类型是提出需求，主要涉及的对象是关于SPMD式控制平面的工作控制。由于原来的架构存在一些缺陷，如与NCCL广播操作干扰驱动功能、难以支持推导解码等，因此提出了改进控制平面架构的需求。

https://github.com/vllm-project/vllm/issues/6555
This is a bug report regarding markdown rendering issue in a PR checklist on the vLLM Github issue titled "test".

https://github.com/vllm-project/vllm/issues/6554
这是关于bug报告的issue，主要涉及到PR comment bot。此问题由于`s`被误认为是字符串结束而导致GHA workflow失败。

https://github.com/vllm-project/vllm/issues/6553
这个issue是用户提出需求类型，涉及的主要对象是支持rope扩展方法。

https://github.com/vllm-project/vllm/issues/6552
这个issue类型是[Kernel]，涉及主要对象是实现fallback for FP8 channelwise using torch._scaled_mm。原因是在一些情况下，CUTLASS kernels不支持，导致需要应用rescale操作来支持channelwise quantization。

https://github.com/vllm-project/vllm/issues/6551
这个issue是关于bug报告，主要涉及vllm不支持多实例GPU的问题，可能导致设置MIG slice或整个GPU时出现错误。

https://github.com/vllm-project/vllm/issues/6550
该issue类型为用户提出需求，用户想要了解如何一键安装和调用vLLM中所有的测试用例，但在文档中未找到相关说明，因此寻求其他人的建议。

https://github.com/vllm-project/vllm/issues/6549
这个issue类型为测试需求，涉及主要对象是vLLM的cpu offloading功能。由于目前的测试基础设施只针对没有cpu offloading的vLLM进行测试，可能是为了验证该功能的正确性。

https://github.com/vllm-project/vllm/issues/6548
这个issue类型是代码修复，主要涉及的对象是模型Mistral-Nemo，由于新增了`head_dim` override，导致需要修改配置文件并提供FP8 quantized checkpoint。

https://github.com/vllm-project/vllm/issues/6501
这个issue是一个bug报告，涉及到Gemmma 27B 在GCP A100上崩溃的问题。由于flashinfer.py中的PagedAttention forwards没有更新，导致了Gemmma 27B崩溃的问题。

https://github.com/vllm-project/vllm/issues/6499
这是一个Bug报告。主要涉及的对象是在使用gemm2 27b时调用openai.api返回空内容的问题。原因可能是在使用API请求时出现问题，而不使用API请求时正常。

https://github.com/vllm-project/vllm/issues/6498
这是一个用户提出需求的类型的issue，主要涉及将嵌入模型（如bge、e5等）部署到vllm中，并寻求相关资源指引。

https://github.com/vllm-project/vllm/issues/6497
这个issue属于bug报告类型，涉及主要对象为vllm安装过程。造成此问题的原因是未能满足numba软件包的需求。

https://github.com/vllm-project/vllm/issues/6496
这是一个用户提出的需求类型的issue，主要涉及的对象是vllm中的模型。由于GPU内存限制，用户提出了对于CPU的卸载实现，以扩展GPU可用内存的需求。

https://github.com/vllm-project/vllm/issues/6495
这是一个Bug报告，涉及到程序中TP/PP驱动程序排序错误导致工作人员返回错误答案的问题。

https://github.com/vllm-project/vllm/issues/6494
这是一个Bug报告类型的Issue，主要涉及VLLM版本0.5.0在A100机器上的CUDA 12.1环境下性能指标下降的问题，用户想知道为什么版本2的性能指标反而出现了下降，以及应该如何调整参数。

https://github.com/vllm-project/vllm/issues/6493
这个issue是关于一个Pull Request（PR）描述缺失的问题，主要涉及到vLLM项目的代码质量和审查流程效率。这可能是由于提交者未填写必要的描述而导致的。

https://github.com/vllm-project/vllm/issues/6492
这是一个缺少PR描述的issue，涉及到Markdown渲染错误，无法在该部分正确显示。

https://github.com/vllm-project/vllm/issues/6491
这是一个bug报告，主要涉及的对象是dummy weights。由于未对dummy weight进行种子设置，导致无法对输出的一致性进行测试。

https://github.com/vllm-project/vllm/issues/6490
这个issue类型是CI/Build更新，涉及的主要对象是flashinfer库，由于需要使用最新版本的flashinfer并包含重要更新和修复，所以进行了从0.0.8版本到0.0.9版本的更新。

https://github.com/vllm-project/vllm/issues/6489
这个issue属于更新通知类型，主要涉及到vllm中的测试脚本，更新flashinfer版本至v0.0.9引起。

https://github.com/vllm-project/vllm/issues/6488
这个issue是一个bug报告，涉及主要对象是测试（tests），由于PP=4 tests出现flaky现象以及关于zmq连接问题的报告，可能导致了一些测试卡住的情况。

https://github.com/vllm-project/vllm/issues/6487
这个issue是一个[Kernel]类型的变更，涉及到支持Fp8通道权重。由于设备没有支持cutlass，导致无法运行这些模型，需要寻求解决方案。

https://github.com/vllm-project/vllm/issues/6486
这是一个bug报告，主要涉及对象是vllm库。导致这个问题的原因是缺少了`jsonschema.protocols`模块。

https://github.com/vllm-project/vllm/issues/6485
这是一个bug报告，主要涉及到speculative decoding中LogProbs的CPU序列化问题，其原因是LogProbs在序列化时出现了错误。

https://github.com/vllm-project/vllm/issues/6484
这是一个涉及模型支持的issue，主要围绕添加对Mamba模型的支持所做的更改及相应的测试。

https://github.com/vllm-project/vllm/issues/6483
这个issue是一个PR Checklist未填写的问题，涉及对象为vLLM代码贡献。原因可能是提交的PR没有按照规范填写相关信息。

https://github.com/vllm-project/vllm/issues/6482
该issue类型是文档修复，涉及的主要对象是 ROCm Dockerfile。由于过时的 patch 和文档需更新，因此需要进行清理和简化，以解决相关问题。

https://github.com/vllm-project/vllm/issues/6481
这是一个文档类型的issue，提醒用户在调试完成后取消调试环境变量，涉及用户设置和维护调试环境的操作过程。

https://github.com/vllm-project/vllm/issues/6479
这是一个用户提出需求的issue，主要涉及支持新模型"Mamba Codestral"，由于该模型是非transformer架构，但已经有类似的模型"Mamba"得到支持，因此用户不确定如何支持新模型。

https://github.com/vllm-project/vllm/issues/6478
这是一个bug报告，主要涉及Gem29b，在运行时遇到AttributeError错误，即"_OpNamespace" '_C' object has no attribute 'rotary_embedding'，原因可能是环境问题。

https://github.com/vllm-project/vllm/issues/6477
这个issue属于bug报告，主要涉及的对象是在GCP上运行Gemma 27B时出现的崩溃问题，可能是由于使用OpenAI镜像导致。

https://github.com/vllm-project/vllm/issues/6476
这是一个文档更新的Issue，主要涉及README的内容更新以突出激活量化。由于Markdown渲染的问题，使用了原始HTML。

https://github.com/vllm-project/vllm/issues/6473
这是一个bug报告，涉及的主要对象是vllm-openvino。由于设置了`use_cache`为True，但加载的模型仅支持`use_cache=False`，导致数值错误。

https://github.com/vllm-project/vllm/issues/6472
这是一个bug报告，主要涉及安装过程中构建docker镜像时出现的错误。由于as和FROM关键字大小写不匹配以及ENV关键字格式问题导致构建失败。

https://github.com/vllm-project/vllm/issues/6471
该issue是关于Feature请求，主要涉及Pipeline parallelism支持的qwen模型，由于只支持特定的架构，导致该功能无法在其他架构上使用。

https://github.com/vllm-project/vllm/issues/6470
这是一个修正拼写错误的问题，主要对象是代码规范和文档，可能是由于疏忽或粗心导致。

https://github.com/vllm-project/vllm/issues/6469
这是一个bug报告，涉及到vllm中的PeftModelForCausalLM模型不支持JSON序列化，导致无法正确进行推理操作。

https://github.com/vllm-project/vllm/issues/6468
这是一个关于提出需求的issue，主要涉及到vLLM中 Speculative Decoding 算法的性能优化提议。由于缺乏有关成本系数的信息，导致用户无法获取每个模型的成本系数数值。

https://github.com/vllm-project/vllm/issues/6467
这个issue属于文本内容修改类型，涉及的对象是vLLM代码库中的一些拼写错误和类型问题。原因是在撰写新功能（支持预测解码的多提议者功能）过程中，开发者注意到了这些错误和问题，并决定将修改内容整理在一个单独的PR中提交。

https://github.com/vllm-project/vllm/issues/6465
这是一个Bug报告类型的issue，该问题单涉及的主要对象是运行名为Qwen2-54B-A14B-GPTQ-Int4(MOE)时发生的错误。由于未提供具体错误信息，无法准确分析导致错误的原因。

https://github.com/vllm-project/vllm/issues/6464
这个issue类型是bug报告，涉及的主要对象是vllm模型的部署。由于GLIBC版本不匹配导致无法导入vllm模型，出现了无法运行的情况。

https://github.com/vllm-project/vllm/issues/6463
这是一个bug报告，涉及的主要对象是vLLM的前端代码。由于Mount操作未在路由器上正常工作，导致`/metrics`端点丢失，需要修复此问题。

https://github.com/vllm-project/vllm/issues/6462
这是一个Bug报告，用户在使用vllm 0.5.2时无法加载gemma-2-9b-it模型。

https://github.com/vllm-project/vllm/issues/6461
这是一个bug报告，涉及的主要对象是vllm下的版本0.5.2。由于版本切换导致Prometheus metrics在/metrics路径下消失，用户提出了这个问题并怀疑可能是一个回归问题。

https://github.com/vllm-project/vllm/issues/6460
这个issue属于bug报告类型，主要涉及的对象是"boardwalk" image asset。原因是在之前的功能尺寸计算中导致了错误，现在经过修复不再需要测试该图像资源，从而减少了相关测试的运行时间。

https://github.com/vllm-project/vllm/issues/6459
这个issue类型为安装问题，主要涉及的对象是vllm软件。由于缺少pyzmq软件包，导致安装过程中出现错误。

https://github.com/vllm-project/vllm/issues/6458
这是一个活动公告类型的issue，主要对象是开发者社区。

https://github.com/vllm-project/vllm/issues/6457
这个issue属于功能需求类型，主要涉及的对象是TPU硬件支持，由于需要支持MoE模型使用GMM Pallas内核，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/6456
这个issue属于Bugfix类型，主要涉及AsyncLLMEngine的prefix caching功能无法正常工作的问题。原因是之前在重用请求时logprobs被错误计算，导致一些token没有生成logprobs。

https://github.com/vllm-project/vllm/issues/6455
这个issue类型是性能优化请求，主要涉及到Pipeline Parallel scenarios中Llama模型初始化内存占用优化。

https://github.com/vllm-project/vllm/issues/6454
这个issue类型为功能增强，涉及主要对象为性能优化和度量指标。原因是为了添加一种对引擎度量指标进行推测解码的功能，并测试相关度量是否正确记录。

https://github.com/vllm-project/vllm/issues/6453
这是一个bug报告，主要涉及到版本控制，由于0.5.0版本无法在主干上构建文档，所以建议将sphinx-argparse版本固定为0.4.0。

https://github.com/vllm-project/vllm/issues/6452
这是一个Bug报告，主要涉及vLLM在Inferentia和AWS neuron上无法加载Mistral模型，可能是由于内存问题导致Python进程的占用达到23.5GB而导致机器冻结。

https://github.com/vllm-project/vllm/issues/6451
这个issue是关于添加一个头尺寸为120以支持GPU 推理的模型问题，不是bug报告。

https://github.com/vllm-project/vllm/issues/6450
该问题单属于用户提出需求类型，涉及的主要对象是网站vllm，用户寻求将Google Cloud添加到赞助商列表。

https://github.com/vllm-project/vllm/issues/6449
这个issue是一个Bug报告，主要涉及OpenAI API中的Seed功能。这个问题是由于当前API仅在driver进程上存储/推进状态信息而需要将其扩展到进行采样的worker。

https://github.com/vllm-project/vllm/issues/6448
该issue主要是关于PR提交的问题，内容涉及到PR的描述和分类准确性，可能由于没有完整填写PR描述导致此issue。

https://github.com/vllm-project/vllm/issues/6447
这是一个bug报告，主要涉及到flashinfer工具的问题，由于0.0.9版本修复了一个导致segfault问题的bug。

https://github.com/vllm-project/vllm/issues/6446
这个issue是一个bug报告，涉及的主要对象是vllm项目中的分布式系统，由于缺少特定的层条件导致了问题。

https://github.com/vllm-project/vllm/issues/6445
这是一个bug报告，主要涉及Gemini2-27b-it启动时出现的TypeError错误。原因可能是NoneType对象被调用导致的异常。

https://github.com/vllm-project/vllm/issues/6444
这是一个功能增强类的issue，主要涉及的对象是vLLM缓存目录。由于缺乏缓存目录，导致需要使用`s3`或`curl`下载VLM示例和测试中使用的图片，该issue旨在为vLLM添加缓存目录以解决这一问题。

https://github.com/vllm-project/vllm/issues/6443
这是一个Bug报告，主要涉及的对象是vllm中的llama模型。导致此问题的原因是embed_tokens的形状不匹配llama3配置，用户希望能够调整token级别的配置。

https://github.com/vllm-project/vllm/issues/6442
这是一个提速优化issue，主要涉及vLLM中的token处理，由于list comprehension被NumPy操作替代，造成sampler处理大批量数据时速度提升了50%。

https://github.com/vllm-project/vllm/issues/6440
这是一个提出建议的Issue，讨论了如何集成Quant支持以优化Intel CPU和GPU性能。

https://github.com/vllm-project/vllm/issues/6439
这个issue类型是文档更新，主要对象是希望更新关于成功性检查的文档。由于信息不清晰，用户提出需要给出一个清晰的成功性检查消息。

https://github.com/vllm-project/vllm/issues/6438
这是一个Bug报告，主要涉及对象是Microsoft/Phi-3-mini-128k-instruct模型。这个问题的症状是在批处理请求时出现了严重的计算错误，导致部分响应结果不正确。

https://github.com/vllm-project/vllm/issues/6437
这个issue是关于文档添加的，涉及的主要对象是flashinfer后端环境。由于markdown渲染问题，需要使用原始的html标签。

https://github.com/vllm-project/vllm/issues/6436
这个issue是针对vLLM项目中的一个优化提升，不属于bug报告。该问题涉及的主要对象是ClipVisionModel模型。原因是为了节省GPU空间而进行了代码优化。

https://github.com/vllm-project/vllm/issues/6435
这是一个用户提出需求的issue，主要对象涉及到GitHub repository。由于项目获得GitHub赞助商批准并能够链接到OpenCollective，因此需要添加FUNDING.yml文件。

https://github.com/vllm-project/vllm/issues/6434
这个issue类型是关于软件版本发布计划，主要涉及到vllm软件版本迭代及相关功能优化。由于计划移除beam search功能以提升性能，因此需要逐步发布新版本来逐步移除这一功能。

https://github.com/vllm-project/vllm/issues/6433
这是一个版本升级请求的issue，涉及的主要对象是软件版本。

https://github.com/vllm-project/vllm/issues/6432
这是一个bug报告类型的issue，主要涉及的对象是vllm v0.5.1版本的autogen在处理空的"tools_call"列表时出现了错误。

https://github.com/vllm-project/vllm/issues/6431
这个issue主要是关于文档和测试更新，涉及到vLLM CLI的新功能需求，而非bug报告。

https://github.com/vllm-project/vllm/issues/6430
这是一个Bug报告，涉及的主要对象是图像转换功能。由于一处CC错误导致Paligemma对PNG文件的支持存在问题。

https://github.com/vllm-project/vllm/issues/6429
这是一个bug报告，主要涉及的对象是VLLM中设置max_position_embeddings参数的情况，由于将其设定为512K或更高时，会导致非法的内存访问错误。

https://github.com/vllm-project/vllm/issues/6428
这个issue是关于bug修复。问题的主要对象是Benchmark serving script，修复的bug是因为在函数'sample_random_requests'中使用了全局参数'args'导致的。

https://github.com/vllm-project/vllm/issues/6427
这是一个bug报告，涉及的主要对象是vllm中的Paligemma模型。由于PNG图片可能具有4个通道（RGBA），而SigLip默认的`num_channels`参数设置为3，导致无法使用PNG文件进行处理。

https://github.com/vllm-project/vllm/issues/6426
这个issue类型是bug报告，主要涉及的对象是CI流程。由于cherry pick未正确处理Prompt Logprobs detokenization，导致会发布错误版本`v0.4.3.post1`。

https://github.com/vllm-project/vllm/issues/6425
这个issue是一个BugFix类型的问题，主要涉及的对象是Jamba模块。这个bug的症状是当`finished_requests_ids` + `current_running_requests` > `max_mamba_cache_capacity`时，Mamba缓存槽映射没有正确清理。

https://github.com/vllm-project/vllm/issues/6424
这是一个用户提出需求的issue，主要涉及的对象是vllm下的attention layer，用户希望添加返回softmax的功能。

https://github.com/vllm-project/vllm/issues/6423
这是一个关于修复DeepSeekv2中启用所有层量化的issue，涉及到的主要对象是DeepSeekv2的某个层，用户报告了层`self_attn.kv_a_proj_with_mqa`可以使用量化到fp8，原因是这一层可以使用ReplicatedLinear，它使用默认的weight_loaded。

https://github.com/vllm-project/vllm/issues/6422
这个issue类型为功能更新，涉及主要对象为AWQ MoE模型。由于重构后的代码逻辑复杂，导致新增功能的实现需要处理更多的索引逻辑，以及重新映射参数，同时还存在Deepseek功能不受支持的问题。

https://github.com/vllm-project/vllm/issues/6421
这是一个Bug报告，涉及的主要对象是vllm.engine.async_llm_engine。由于多线程访问时出现错误，可能是由于后台循环发生错误导致的。

https://github.com/vllm-project/vllm/issues/6420
这个issue属于bug报告类型，主要涉及CI/CD构建过程中的提交ID问题，可能是由于提交ID未正确识别导致构建失败。

https://github.com/vllm-project/vllm/issues/6419
这个问题是一个Bug报告，涉及的主要对象是vLLM项目中的测试模块。由于一些测试没有参数化`model_name`，导致了一些测试错误，同时也添加了对prompt adapters的测试。

https://github.com/vllm-project/vllm/issues/6418
这是一个用户提出需求的issue，涉及主要对象为分布式推理。由于现有文档不包含关于分布式推理的建议，用户提出需要添加这方面的建议。

https://github.com/vllm-project/vllm/issues/6417
这个issue是一个[ Misc ]类型的PR请求，涉及主要对象是将MoE Refactor应用到DeepSeekv2和Qwen2Moe以支持`fp8`，用户请求支持fused AWQ，原因是markdown渲染无法正常工作，因此使用原始html。

https://github.com/vllm-project/vllm/issues/6416
这是一个用户提出需求的issue，主要对象是`LLM`类，由于无法通过`LLM`类应用聊天模板，用户提出希望添加此功能的请求。

https://github.com/vllm-project/vllm/issues/6415
这是一个关于"[ Kernel ] AWQ Fused MoE"的GitHub上的issue，主要涉及AWQ MoE模型和相关的合并内核问题。

https://github.com/vllm-project/vllm/issues/6414
这个issue是一个bug报告，主要涉及的对象是尝试使用vLLM的OpenAI API服务器部署Llamafied版本的InternLM257BChat1M模型时遇到的超时错误。导致这个问题的可能原因是Llamafied版本的InternLM257BChat1M模型在部署过程中出现了超时错误。

https://github.com/vllm-project/vllm/issues/6413
这是一个bug报告类型的issue，主要涉及CI/Build，原因是测试生成的提交哈希的问题。

https://github.com/vllm-project/vllm/issues/6412
此issue是一个文档相关的问题单，涉及vLLM的夜间基准测试文档，用户提交了添加夜间基准测试文档的需求。

https://github.com/vllm-project/vllm/issues/6411
这是一个bug报告，主要涉及LM Format Enforcer版本更新到v10.3，由于JSON Schema解析能力的问题导致了一些bug。

https://github.com/vllm-project/vllm/issues/6410
这是一个需求类型的issue，主要涉及vllm项目中关于CI和分布式的功能测试，提出了为pipeline并行性增加正确性测试的需求。

https://github.com/vllm-project/vllm/issues/6409
该issue类型为bug报告，涉及的主要对象是samplers/test_logprobs.py文件。由于使用了`half`精度导致在H100GPU上测试失败，需要将精度改为`float`以解决问题。

https://github.com/vllm-project/vllm/issues/6408
这是一个Bug报告，主要涉及的对象是`samplers/test_logprobs.py`，由于精度问题导致了测试失败。

https://github.com/vllm-project/vllm/issues/6407
这是一个bug报告，问题涉及到在设置tensor_parallel_size > 1时不能正常工作；用户遇到这个问题是由于设置了tensor_parallel_size = 2时出现错误。

https://github.com/vllm-project/vllm/issues/6406
这是一个关于优化代码以支持管道并行的问题，主要涉及模型的权重加载和层构造部分，旨在减少修改代码量并简化模型支持管道并行的实现。

https://github.com/vllm-project/vllm/issues/6405
这个issue是一个文档维护类型的问题，主要涉及到vLLM项目中的spec_decode.rst文档。由于额外的句号导致markdown渲染出错，需要将末尾的句号移除。

https://github.com/vllm-project/vllm/issues/6404
这是一个用户提出需求的issue，主要对象是beam search功能。原因是为了在完全删除时能够及时通知。

https://github.com/vllm-project/vllm/issues/6403
这个issue类型是功能需求，涉及的主要对象是Mixtral模型。由于需要添加pipeline支持，用户希望在Mixtral模型上运行pipeline并进行测试。

https://github.com/vllm-project/vllm/issues/6402
这个issue是其他类型，提出了关于软件功能变更的需求，主要涉及到beam search的deprecation警告。这是因为开发团队计划在下一个主要发布版本中废弃beam search功能，故需要在当前版本中添加警告信息。

https://github.com/vllm-project/vllm/issues/6401
该issue属于文档整理类型，主要对象是最新新闻内容。

https://github.com/vllm-project/vllm/issues/6400
这个issue属于bug报告类型，主要涉及的对象是vllm中的DistributedGPUExecutorAsync模块。由于移除了_run_workers_async函数，导致worker在执行新实现时受限于循环，无法异步地进行kv cache传输，用户提出了关于新实现优势以及如何允许worker在生成期间异步传输kv的问题。

https://github.com/vllm-project/vllm/issues/6399
这是一个Bugfix类型的issue，涉及主要对象是多节点间的驱动工作线程分配问题。由于之前合并了一个pull request导致驱动工作线程分配不同步，可能导致死锁情况发生。

https://github.com/vllm-project/vllm/issues/6398
这是一个对CI配置中Github bot文字修正的问题，主要对象是CI系统的运行和Github bot。

https://github.com/vllm-project/vllm/issues/6397
这个issue是一个bug报告，涉及的主要对象是TPUv5elitepod，由于当前代码未正确设置megacore，导致出现了bug。

https://github.com/vllm-project/vllm/issues/6396
这个issue是一个功能需求，主要涉及的对象是修复Quantized类型使用中的歧义问题，由于新增的fp8 Marlin使得参数`num_bits`不能准确区分权重是8位整数还是4位整数，导致需要复制和粘贴Marlin代码并创建额外的Python入口点来解决这种歧义。

https://github.com/vllm-project/vllm/issues/6395
这是一个 Bug 报告，主要涉及 Gemma2 在特定条件下引发的 ValueError 错误。原因可能是由于输入长度过短导致。

https://github.com/vllm-project/vllm/issues/6394
这个issue是关于CI/Build的，涉及的主要对象是Python wheel构建，由于项目转向`TORCH_LIBRARY`而导致的构建问题。

https://github.com/vllm-project/vllm/issues/6393
这是一个bug报告，主要涉及XPU backend在运行时需要执行setvars.sh设置LD_LIBRARY_PATH以便找到IPEX依赖的MKL库。Bug的症状是使用XPU backend时会出现错误消息。

https://github.com/vllm-project/vllm/issues/6392
这是一个修复文档中的拼写错误的issue，属于文档问题类型，主要涉及的对象是代码中的Word Pipeline。原因可能是拼写错误引起的文档不准确。

https://github.com/vllm-project/vllm/issues/6391
这是一个bug报告，该问题涉及到发布管道在目录权限上存在问题。

https://github.com/vllm-project/vllm/issues/6390
这是一个bug报告，主要涉及release pipeline的-e flag，由于flag被错误放置导致了问题。

https://github.com/vllm-project/vllm/issues/6389
这是一个bug报告，涉及到发布流程中插值的修复问题，可能是由于插值算法实现不正确或者参数配置错误导致的。

https://github.com/vllm-project/vllm/issues/6388
这是一个bug报告类型的issue，涉及到release-pipeline.yaml文件的格式问题。由于YAML格式错误，导致了构建过程出现了一些问题。

https://github.com/vllm-project/vllm/issues/6387
这是一个Bug报告。该问题涉及VLLM中加载Gemma 2 27b-it模型时出错。由于引擎初始化错误导致的问题。

https://github.com/vllm-project/vllm/issues/6386
这个issue是关于Github上vllm项目中的一个需求提案，主要涉及在安装库时将生成的git提交哈希嵌入到库中，目的是为了避免意外提交提交ID并保持`setup.py`不会影响开发者的git状态。

https://github.com/vllm-project/vllm/issues/6385
这是一个bug报告，主要涉及的对象是Vllm中的某些LLM模型和使用3个GPU时出现的运行时AssertionError。这个bug可能由于代码中对3个GPU不可分割的数值计算错误而导致。

https://github.com/vllm-project/vllm/issues/6384
这是一个关于核心功能的bug报告，涉及到 vLLM 项目中 CUDA 内核的性能问题。由于没有针对 SM89 进行调优，导致性能表现非常糟糕。

https://github.com/vllm-project/vllm/issues/6383
这个issue属于测试问题，涉及GitHub Actions（GHA）工作流。其内容为空可能是测试相关配置或工作流的问题。

https://github.com/vllm-project/vllm/issues/6382
这是一个bug报告，主要涉及FP8 MoE kernel代码中的非法内存访问问题，由于chunking机制在处理大输入时导致频繁发生的错误。

https://github.com/vllm-project/vllm/issues/6381
这个issue类型是功能增强要求，涉及主要对象为GHA workflows。导致这个需求的原因可能是为了提高CI/CD流程的效率和自动化程度。

https://github.com/vllm-project/vllm/issues/6380
这是一个需求提出类型的issue，主要涉及将构建一组默认的nightly wheels，通过此操作可以帮助测试。

https://github.com/vllm-project/vllm/issues/6379
这个issue是一个CI/Build类型的问题，主要涉及到GPU状态检查的修复。由于rocminfo在“undetermined”初始状态下启动，导致了一个bug。

https://github.com/vllm-project/vllm/issues/6378
这是一个提出新特性需求的issue，主要涉及的对象是在vLLM中使用torch.compile进行图优化系统。

https://github.com/vllm-project/vllm/issues/6377
这个issue类型是一个功能需求，主要涉及vLLM项目中的优化器实现，由于需要实现新的优化器，用户提交了对应的PR。

https://github.com/vllm-project/vllm/issues/6376
这是一个Bug报告类型的Issue，主要涉及vLLM和llavahf/llavav1.6mistral7bhf服务。由于同时发送多个请求导致llava model容器出现错误“Please increase the max_chunk_bytes parameter”，无法接受新请求，需要重启才能解决。

https://github.com/vllm-project/vllm/issues/6375
这是一个bug报告，主要涉及到 gfx908 GPU，由于更新到版本0.5.1导致了无法工作的问题。

https://github.com/vllm-project/vllm/issues/6374
这个issue是关于优化的需求，主要涉及限制预加载批处理大小以避免重新编译。

https://github.com/vllm-project/vllm/issues/6373
这是一个Bug报告，涉及到代码中hard-coded值的问题，导致了在特定情况下测试用例失败。

https://github.com/vllm-project/vllm/issues/6371
这是一个关于bug的报告，该问题涉及对VLLM模型在OpenAI API上使用`stream_options.include_usage`时数据不完整的问题。

https://github.com/vllm-project/vllm/issues/6370
这是一个bug报告，主要涉及vLLM 0.5.1在tensor parallel 2环境下出现的Hang问题。原因可能是环境配置或代码逻辑上的问题。

https://github.com/vllm-project/vllm/issues/6369
这是一份bug报告，涉及的主要对象是在skip speculation功能激活且在某一步骤中所有序列都没有生成草稿令牌的情况下，导致出现hang error错误。

https://github.com/vllm-project/vllm/issues/6368
这个issue是一个功能请求，其涉及的主要对象是添加Ascend NPU的支持。由于目前项目只支持GPU等硬件加速器，缺乏对NPU的支持，用户希望通过该功能请求实现更快速、更高效的计算。

https://github.com/vllm-project/vllm/issues/6367
这是一个bug报告，涉及PaliGemma的数据类型不匹配导致RuntimeError。

https://github.com/vllm-project/vllm/issues/6366
这是一个Bug报告，涉及到Paligemma加载问题，由于输入类型和权重类型不一致导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/6365
这是一个需求提出类型的issue，主要涉及测试用例的组织和默认运行设置。

https://github.com/vllm-project/vllm/issues/6364
这个issue是一个Bug报告，涉及到transformers的tokenizer问题。由于之前版本的bug导致在LLava 1.6中运行推理时出现错误，需要升级transformers版本来解决。

https://github.com/vllm-project/vllm/issues/6363
这个issue属于bug报告，主要对象是vllm项目中的flashinfer功能。由于缺少对flashinfer后端的copy / swap blocks支持，导致flashinfer与chunked prefill结合输出垃圾数据和可能存在问题。

https://github.com/vllm-project/vllm/issues/6362
这是一个关于功能讨论的issue，主要涉及到VLLM中的多模态数据处理，用户询问关于连续批处理中如何匹配多模态数据和序列的问题。原因是可能存在无法确定多模态数据属于哪个输入的问题。

https://github.com/vllm-project/vllm/issues/6361
这是一个Bug报告，主要涉及vllm引擎中的AsyncEngineDeadError错误。由于存在32个并发的大规模推断，导致出现了Background loop errored already错误。

https://github.com/vllm-project/vllm/issues/6359
这个issue是关于代码清理的，不属于bug报告。主要涉及对象是Marlin代码。用户提交此issue可能是为了进一步净化Marlin代码，提高代码质量。

https://github.com/vllm-project/vllm/issues/6358
这是一个功能需求类型的issue，主要涉及支持压缩张量模型的激活序列量化，由于需要支持激活序列的组内量化，需要添加相应的参数和层参数。

https://github.com/vllm-project/vllm/issues/6357
这是一个bug报告，主要涉及到GPTBigCode模型中的lm_head。由于在lora模式下的权重设置问题导致lm_head实现被替换，导致了一个关于张量大小不匹配的错误。

https://github.com/vllm-project/vllm/issues/6356
这个issue是关于对`compressed-tensors`集成中支持具有偏差的模型的需求。

https://github.com/vllm-project/vllm/issues/6355
这是一个Bug报告类型的Issue，涉及主要对象是vllm在安装过程中出现的导入错误。原因可能是vllm无法正确升级到下一个版本。

https://github.com/vllm-project/vllm/issues/6354
该issue是一个Bugfix，涉及到修复`RayMetrics`由于错误的API使用导致无法正常工作的问题。

https://github.com/vllm-project/vllm/issues/6353
该issue是一个Misc类型的PR，涉及到删除代码中未使用的`separate_bias_add`。

https://github.com/vllm-project/vllm/issues/6352
这个issue是一个bug修复类型的issue，主要涉及到vLLM在使用CUDA_VISIBLE_DEVICES获取设备数量时的代码优化问题。原因是之前代码没有统一使用环境变量"CUDA_VISIBLE_DEVICES"，导致了代码复杂性和使用不统一。

https://github.com/vllm-project/vllm/issues/6351
这是一个与软件功能相关的需求提出issue，主要涉及的对象是flashinfer功能。由于需要flashinfer新版本才能通过CI测试，旧版本会导致CI失败。

https://github.com/vllm-project/vllm/issues/6350
这是一个关于持续集成/构建的issue，主要涉及AMD CI的转变至Docker Hub并优化构建流程。

https://github.com/vllm-project/vllm/issues/6349
这是一个Bugfix类型的issue，涉及主要对象是OpenVINO。由于kv_cache_dtype类型无法序列化为json，导致出现usage stats logging exception warning的问题。

https://github.com/vllm-project/vllm/issues/6348
这是一个用户提出需求的 issue，主要涉及 FlashAttention 3 的支持，由于该功能有望带来 1.5 倍的性能改进。

https://github.com/vllm-project/vllm/issues/6347
这是一个文档更新类的issue， 主要对象是vllm项目中的pipeline parallel功能的说明。

https://github.com/vllm-project/vllm/issues/6346
这是一个bug报告，涉及到vllm在分布式和misc方面与如何找到libcudart.so一致性的问题。由于pytorch在libcudart.so的查找方式上的改变，导致vllm出现了问题。

https://github.com/vllm-project/vllm/issues/6345
这个issue是一个bug报告，主要涉及的对象是OpenAI的batch file format。原因是pydantic验证错误导致的问题。

https://github.com/vllm-project/vllm/issues/6344
这是一个bug报告类型的issue，主要涉及到代码中的重复分支问题，由于重复的分支导致了静态分析工具检测到问题，需要合并这些分支以优化代码。

https://github.com/vllm-project/vllm/issues/6343
这个issue类型是改进建议，主要涉及到优化vllm项目中的CSR矩阵计算方法。由于用户困惑需要安装scipy，提出了优化建议。

https://github.com/vllm-project/vllm/issues/6342
这是一个Bug报告，主要涉及的对象是OpenAI batch文件格式的pydantic验证错误。这个问题可能是由于输入文件格式不符合要求而导致的。

https://github.com/vllm-project/vllm/issues/6341
这个issue是一个[杂项]类型的问题，主要涉及在Guided Processor测试中添加fixture的请求。原因可能是缺少fixture导致一些测试无法成功运行。

https://github.com/vllm-project/vllm/issues/6340
此issue属于Bug报告类型，主要涉及的对象是vllm中的OpenVINO功能。由于在执行benchmark_throughput.py时出现了异常，可能是由于环境配置或代码逻辑错误导致的。

https://github.com/vllm-project/vllm/issues/6339
这个issue是关于bug报告，主要涉及的对象是VLLM下的LLaVA next feature size计算。由于Markdown渲染不起作用，故采用原始HTML，可能导致了视觉效果方面的问题。

https://github.com/vllm-project/vllm/issues/6338
这是一个功能改进型的issue，涉及主要对象为vllm项目下的draft_model_runner模块。由于在advance_step中处理prepare_inputs的操作尚未在GPU上执行，导致了性能表现不佳，用户提出将其转移到GPU上以提升性能。

https://github.com/vllm-project/vllm/issues/6337
这是一个bug报告，涉及的主要对象是算法库vllm中的指标统计功能。由于某种原因导致时间值的直方图条目始终都是相同的，造成统计数据不正确。

https://github.com/vllm-project/vllm/issues/6336
这是一个性能优化的建议类型的issue，主要涉及NVIDIA Nsight Compute在Linux上的使用问题，提出了关于性能降级的报告和未得到回应的问题。

https://github.com/vllm-project/vllm/issues/6335
这是一个bug报告，涉及到Vllm程序中无法正确找到libcudart.so文件，导致初始化CustomAllreduce时出现超时问题。

https://github.com/vllm-project/vllm/issues/6334
这是一个bug报告，涉及到无法在最新版本中运行phi-3-small模型。导致这个bug的原因可能是在0.5.1版本中尝试运行phi-3-small模型时出现了错误，而在先前版本中并没有这个问题。

https://github.com/vllm-project/vllm/issues/6333
这是一个Bug报告，主要涉及VLLM中的LoRA请求。原因是在推理时出现了错误。

https://github.com/vllm-project/vllm/issues/6332
这是一个bug报告，涉及的主要对象是`tests/samplers/test_logprobs.py`测试模块。用户发现在当前环境下，测试在 `float32` 精度下失败，尽管在 `float16` 精度下却通过了。用户希望了解是否这种情况是预期的，以及希望得到关于测试在不同精度下失败的解决帮助。

https://github.com/vllm-project/vllm/issues/6331
这是一个bug报告，涉及主要对象为Gemma 2 GPTQ模型。由于环境配置中PyTorch版本与CUDA版本不匹配，导致完成API输出，但通过批量推断输出不完整。

https://github.com/vllm-project/vllm/issues/6330
这是一个环节内容为空的issue，类型为未完成的工作（wip），涉及的主要对象是项目的代码库。

https://github.com/vllm-project/vllm/issues/6329
这是一个Bug报告，涉及的主要对象是Gloo库在两台计算机之间无法进行通信。这个问题可能是由环境信息中提到的PyTorch版本、CUDA版本或者操作系统等因素导致。

https://github.com/vllm-project/vllm/issues/6328
这是一个Bug报告类型的issue，主要涉及VLLM的稳定性问题，用户反映在使用特定参数时，输出结果的稳定性较差，可能是由于模型版本问题或参数设置不当导致。

https://github.com/vllm-project/vllm/issues/6327
这个issue属于需求提出类型，主要对象是VLLM模型中的RowParallelLinear模块，作者提出了一个关于优化算法的建议。

https://github.com/vllm-project/vllm/issues/6326
这是一个bug报告，涉及的主要对象是 GPTBigCode 模型的 LoRA 功能。由于 tensor 大小不匹配，导致服务器启动失败。

https://github.com/vllm-project/vllm/issues/6325
这是一个bug报告，涉及的主要对象是VLLM代码库。该问题由于asynchronous generator已在运行时导致RuntimeError: aclose()错误。

https://github.com/vllm-project/vllm/issues/6324
这是一个bug报告，主要涉及VLLM中最大上下文长度被超出，导致了BadRequestError错误。

https://github.com/vllm-project/vllm/issues/6323
这是一个功能需求的issue，主要涉及到在vllm模型中实现混合注意力（hybrid attention）的问题。发起这个issue的原因是由于目前vllm模型只使用全局注意力而忽略了局部注意力，用户希望在vllm/flashattn中通过设置窗口大小来实现局部注意力，以加速预填充阶段并优化内存消耗。

https://github.com/vllm-project/vllm/issues/6322
这是一个Bug报告，主要涉及VLLM 0.5.1和LLaVA 1.6之间的异常。用户在使用服务时遇到异常并且需要重启服务，即使之前正常运行的图片也会出现这个问题。

https://github.com/vllm-project/vllm/issues/6321
这是一个用户提出需求的类型的 issue，主要涉及到对模型 "InternVL2" 的支持。由于InternVL2是一个功能强大的开源多模态大型语言模型，用户希望将其集成到vLLM框架中以获得更大的受益。

https://github.com/vllm-project/vllm/issues/6320
这个issue是关于代码优化和文档规范的改进，主要对象是项目的代码库。原因是发现部分代码可以进行优化以提高可读性。

https://github.com/vllm-project/vllm/issues/6319
这是一个bug报告，涉及vllm在k8s pod中启动模型时需要大约1小时才能成功启动的问题。

https://github.com/vllm-project/vllm/issues/6318
这个issue是一个bug报告，涉及的主要对象是vLLM中的snapshot下载函数。由于`snapshot_download`函数在处理不同来源的参数时会导致错误，因此用户提出修复此bug的问题。

https://github.com/vllm-project/vllm/issues/6317
这是一个用户提出需求的issue，主要涉及vLLM模型的条件性CPU权重卸载功能。由于GPU资源有限，导致大型模型的性能可能出现高延迟的问题。

https://github.com/vllm-project/vllm/issues/6315
这是一个关于核心功能改进的issue，主要涉及支持Lora适配器谱系和基础模型元数据管理。由于Lora命令支持json格式且用户可以显式指定基础模型，需要利用ModelCard中的`root`和`parent`字段展示lora适配器谱系，并引入`BaseModelPath`来使用基础模型信息代替`served_model_names`。

https://github.com/vllm-project/vllm/issues/6314
这是一个bug报告issue，主要涉及vLLM在启用LoRA时对于`bigcode/gpt_bigcodesantacoder`模型出现的tensor size mismatch错误导致服务器无法启动的问题。

https://github.com/vllm-project/vllm/issues/6313
这个issue属于bug报告类型，主要涉及的对象是NeuronExecutor。由于NeuronWorker类无法实例化且引入softtuned prompts功能后导致CI测试通过出错。

https://github.com/vllm-project/vllm/issues/6312
这是一个用户提出需求的issue，该问题涉及VLLM对多个GPU的控制问题，用户想要指定VLLM使用其中一个GPU而不是默认占用所有GPU。

https://github.com/vllm-project/vllm/issues/6311
这个issue属于用户提出需求类型，主要涉及对象是在使用vllm加载模型后运行自定义的post_init函数。用户之所以提出这个问题是因为想要在transformers模型加载后执行特定的自定义操作。

https://github.com/vllm-project/vllm/issues/6309
这是一个bug报告，涉及主要对象为vllm项目中的编译错误，由于使用torch2.1编译时出现了错误。

https://github.com/vllm-project/vllm/issues/6308
这是一个Bug报告，涉及Llama370B在运行过程中出现"Gloo Connection reset by peer"的错误。

https://github.com/vllm-project/vllm/issues/6307
这个issue属于用户提出需求类型，主要对象是vLLM，用户提出了关于是否支持CrossLayer Attention的问题。

https://github.com/vllm-project/vllm/issues/6306
这是一个Bug报告，用户在尝试使用ngram speculative decoding时遇到了错误。

https://github.com/vllm-project/vllm/issues/6305
这是一个bug报告，涉及的主要对象是mistralai/Mixtral8x22Bv0.1模型。用户反映在运行推理时，由于tokenizer的问题导致模型输出随机。

https://github.com/vllm-project/vllm/issues/6304
这是一个bug报告，涉及到triton >= 3.0.0的问题。原因是由于triton升级引起的编译错误。

https://github.com/vllm-project/vllm/issues/6303
这是一个关于bug的修复请求，主要涉及MLPSpeculator功能在`tie_weights=False`模型下出现问题。这个bug的原因可能是由于MLPSpeculator未正确使用ParallelLMHead导致的。

https://github.com/vllm-project/vllm/issues/6302
这是一个bug报告，涉及到MLPSpeculator model中`tie_weights=False`导致的crash问题，原因是与CC([CORE] Quantized lmhead Framework)相关的更改引起的。

https://github.com/vllm-project/vllm/issues/6301
这个issue是关于bug报告，主要涉及对象是VLLM模型的最大上下文长度问题，导致错误的原因是请求的tokens数量超过了模型设定的最大上下文长度限制。

https://github.com/vllm-project/vllm/issues/6300
这是一个特性需求的issue，主要涉及到支持 speculative decoding 中的多个提议者，其中提到了 ngram proposer 和 DraftModelBased Proposers 的性能问题。

https://github.com/vllm-project/vllm/issues/6299
这是一个Bug报告类型的Issue，主要涉及Vllm在初始化CustomAllreduce时出现超时错误。这可能是由于Vllm版本升级后在同一GPU卡上出现问题所致。

https://github.com/vllm-project/vllm/issues/6296
这是一个Bug报告类型的issue，主要涉及对象是OpenVINOExecutor。由于无法实例化具有抽象方法的TPUExecutor类导致的TypeError错误。

https://github.com/vllm-project/vllm/issues/6295
这是一个bug报告，主要涉及多GPU模型运行受限于4096线程限制导致的错误。

https://github.com/vllm-project/vllm/issues/6294
这是一个关于bug报告的issue，主要涉及到vllm开启speculative decoding时导致并发请求失败，出现了未处理的错误和GPU利用率为零的问题。

https://github.com/vllm-project/vllm/issues/6293
该issue为需求类型，针对CI中测试TPU的提案。 

https://github.com/vllm-project/vllm/issues/6292
这是一个Bug报告，主要涉及VLLM在使用4张4090部署推理时出现性能问题的情况。原因可能是配置参数设置不当导致响应速度缓慢。

https://github.com/vllm-project/vllm/issues/6291
这个issue是关于bug报告，涉及的主要对象是chatglm2模块；由于vllm升级到0.5.0版本后，用户发现chatglm2在API服务器启动时出现问题。

https://github.com/vllm-project/vllm/issues/6290
这是一个用户需求类型的问题，涉及到vllm在k8s集群中产生大量日志文件的问题，用户希望了解如何禁用这些日志文件的生成。

https://github.com/vllm-project/vllm/issues/6289
这是一个特性需求的提出，主要对象是 `UnquantizedFusedMoEMethod`，用户提出需要添加 `CustomOp` 接口以便支持多种硬件后端，因为目前直接导入 Triton fused MoE kernel 限制了其他硬件后端支持 MoE 模型。

https://github.com/vllm-project/vllm/issues/6288
这个issue是一个bug报告，主要涉及的对象是block_manager_v1.py文件中的compute_full_blocks_in_seq函数。由于某种原因导致在此行代码中需要设置为-1，但未指明具体原因。

https://github.com/vllm-project/vllm/issues/6287
这个issue属于bug报告，主要涉及的对象是MoE layer。该bug导致MoE layer无法支持2D输入形状，用户因此提出了对此进行修复的需求。

https://github.com/vllm-project/vllm/issues/6286
这个issue是关于文档修复的问题，主要涉及注释错误的剪切问题，原因是来自另一个项目的注释不适用于当前项目。

https://github.com/vllm-project/vllm/issues/6285
这是一个Bug报告，主要涉及vLLM的speculative decoding功能无法匹配到tokens，可能是由于代码实现问题导致。

https://github.com/vllm-project/vllm/issues/6284
这是一个用于测试的issue，主要涉及文本解码验证令牌，类型为测试需求。

https://github.com/vllm-project/vllm/issues/6283
这个issue类型是bug报告，涉及的主要对象是vllm中的Phi3 Rotary Embedding实现。由于相同输入在不同batch中会产生不同的嵌入结果，用户提出是否这是预期行为。

https://github.com/vllm-project/vllm/issues/6282
这是一个bug报告，主要涉及的对象是针对'gte-Qwen2'嵌入模型的支持。由于当前版本存在嵌入一致性问题，导致无法通过相关测试。

https://github.com/vllm-project/vllm/issues/6281
这个issue类型是用户提出需求，涉及的主要对象是如何修改WeMM使其兼容vllm。由于WeMM中使用了自定义的Plora模块替代了基本的线性层，需要调整代码以支持该模型。

https://github.com/vllm-project/vllm/issues/6280
这是一个用户提出需求的issue，主要涉及的对象是CI（持续集成）。

https://github.com/vllm-project/vllm/issues/6279
这是一个bug报告，涉及的主要对象是TPUExecutor。这个问题是由于前一个PR导致的，没有添加prompt adapter方法造成了TPU backend的错误。

https://github.com/vllm-project/vllm/issues/6278
这是一个bug报告，问题涉及到使用docker加载模型时出现错误。用户希望解决加载模型失败的问题。

https://github.com/vllm-project/vllm/issues/6277
这是一个功能需求类型的issue，主要对象是为TPU backend添加一个CI测试。

https://github.com/vllm-project/vllm/issues/6276
这个issue是一个Bug报告，涉及的主要对象是VLLM的L40 4card inference。由于某种原因导致了概率性出现错误信息，用户寻求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/6275
这是一个功能增强的请求（RFC），主要涉及vLLM中LoRA管理的改进，以使其更适合生产环境。原因是LoRA集成在生产环境中面临几个挑战，需要解决以确保平稳高效的部署和管理。

https://github.com/vllm-project/vllm/issues/6274
这是一个需求提出类型的issue，主要涉及到/v1/models接口的响应内容。由于无法在响应中展示Lora与基础模型之间的血缘关系，用户提出应在root字段表示模型路径，parent字段表示Lora适配器的基础模型，以解决这一问题。

https://github.com/vllm-project/vllm/issues/6273
这是一个功能需求的issue，主要涉及对象是LLMEngine中的SamplingController接口。原因是为了改进guided decoding的表现和API。

https://github.com/vllm-project/vllm/issues/6272
这个issue类型是需求，主要对象是模型部署推理配置优化，用户提出了关于优化模型部署推理配置以满足延迟和吞吐量两种主要需求的问题。

https://github.com/vllm-project/vllm/issues/6271
这是一个Bug报告，主要涉及在生成过程中出现异常，原因是传入的种子被误认为是字符串。

https://github.com/vllm-project/vllm/issues/6270
这是一个关于使用CUTLASS kernels进行FP8层带偏置的Kernel变更issue。使用CUTLASS kernels来处理FP8的偏置会导致Qwen27b速度提升，该问题主要涉及代码优化和性能提升。

https://github.com/vllm-project/vllm/issues/6269
这是一个Bug报告类型的Issue，主要涉及的对象是NeuronWorker类。由于导入vllm._C模块失败，导致NeuronWorker类实例化时出现TypeError错误。

https://github.com/vllm-project/vllm/issues/6268
这个issue是一个CI/Build类型的问题，涉及到启用mypy类型检查的问题。

https://github.com/vllm-project/vllm/issues/6267
这是一个bug报告，涉及主要对象是GPU L1缓存的压力问题，可能由于GPU L1缓存压力过大导致的bug。

https://github.com/vllm-project/vllm/issues/6266
这个issue属于bug报告，涉及的主要对象是scheduler.finished_requests_ids；由于scheduler.finished_requests_ids在没有输出时被重置，导致未将完成的请求id发送回工作者。

https://github.com/vllm-project/vllm/issues/6265
这是一个功能需求的报告，主要涉及CogVlm2模型的支持问题。此问题的原因是模型使用了自定义的int4量化方法，需要进行支持。

https://github.com/vllm-project/vllm/issues/6264
这是一个bug报告，涉及的主要对象是Qwen2 Moe FP8模型在L40上不受支持。由于加载fp8 qwen2 moe模型后，出现了配置文件config.json无法识别的问题。

https://github.com/vllm-project/vllm/issues/6263
这是一个bug报告，涉及使用vllm时出现的模型加载错误以及相关依赖安装问题。用户希望使用`microsoft/Phi3small8kinstruct`模型进行Lora finetune，但遇到了无法加载模型配置的错误，以及请求模型不存在的问题。

https://github.com/vllm-project/vllm/issues/6262
这个issue是一个BugFix类型的报告，涉及的主要对象是vLLM项目中的outlines包版本错误导致的模块未找到错误。

https://github.com/vllm-project/vllm/issues/6261
这是一个bug报告，涉及到OpenAI Compatible Server for OpenVINO version of VLLM及其依赖的outlines模块版本问题，导致出现ModuleNotFoundError: No module named 'outlines.caching'错误。

https://github.com/vllm-project/vllm/issues/6260
这个issue是一个feature需求，主要涉及的对象是对新模型XLMRobertaForSequenceClassification的集成和使用。出现这个需求是因为想要实现vllm对‘任何’模型的集成，并替换模型层以提升性能。

https://github.com/vllm-project/vllm/issues/6259
这是一个bug报告，该问题涉及vllm从源代码构建后无法生成可执行文件的问题。原因可能是构建过程中出现了错误或遗漏步骤。

https://github.com/vllm-project/vllm/issues/6258
这是一个Bug报告，涉及的主要对象是vllm库下的tensor parallel功能。由于版本升级和特定内核的改动，导致在使用4个卡进行tensor parallel计算时产生了错误的答案。

https://github.com/vllm-project/vllm/issues/6257
这是一个用户提出需求的issue，主要涉及如何在langchain_community中使用VLLM对图像进行推断。用户在使用microsoft/Phi3vision128kinstruct模型时遇到集成问题，需要帮助。

https://github.com/vllm-project/vllm/issues/6256
这是一个关于bug报告类型的issue，涉及的主要对象是TPU Dockerfile中的`outlines`安装问题。

https://github.com/vllm-project/vllm/issues/6255
这个issue是关于BugFix的，主要涉及到vllm中engine的超时问题，由于请求步骤残留导致引擎超时错误。

https://github.com/vllm-project/vllm/issues/6254
这个issue是一个Bug报告，涉及到vllm的异步引擎出现超时错误的问题。导致该问题的原因是请求结束和引擎生成完成判断之间的差异，可能导致引擎超时错误或无法处理下一个请求。

https://github.com/vllm-project/vllm/issues/6253
这个issue是关于用户提出需求的，主要对象是vllm模型。由于启用前缀缓存需要禁用滑动窗口，导致在设置max model len时受到限制，需要增加max model len值超过默认的滑动窗口值，用户询问是否可能实现这一要求。

https://github.com/vllm-project/vllm/issues/6252
这是一个bug报告，主要涉及到在vLLM上使用google/gemma-2-27b-it时出现segfault的问题。可能是由于某种特定环境下的原因导致了程序崩溃。

https://github.com/vllm-project/vllm/issues/6251
这是一个用户提出需求的issue，主要涉及的对象是为vllm添加固定输入长度和输出长度的基准测试。这个需求可能是为了更全面地评估vllm性能，或者用于验证模型的准确性和稳定性。

https://github.com/vllm-project/vllm/issues/6250
这是一个Bug报告类型的issue，主要涉及到加载LoRA适配器针对Llama3的问题。由于PyTorch版本为2.3.0+cu121，可能导致该适配器无法正常加载，从而引发了该Bug。

https://github.com/vllm-project/vllm/issues/6249
这是一个bug报告，涉及主要对象为使用了fp8量化的deepseekv2模型。该问题由于torch不支持fp8 cat操作而导致RuntimeError异常的发生。

https://github.com/vllm-project/vllm/issues/6248
这是一个用户提出需求类型的issue，主要涉及配置vLLM模型在GCP上使用所有可用CUDA设备的功能。原因是在GCP部署时没有设置环境变量的选项，导致需要为每种可能的设置构建一个容器或注册一个模型。

https://github.com/vllm-project/vllm/issues/6246
这是一个提出需求的issue，主要涉及VLLM中关于fp8数据类型的支持问题，用户提出希望在attention计算中支持fp8类型，而当前存在需要先将fp8转换为fp16/bf16的问题。

https://github.com/vllm-project/vllm/issues/6245
这是一个bug报告，主要涉及TPU模型输入参数，由于未添加None导致了多模态参数错误。

https://github.com/vllm-project/vllm/issues/6244
这是一个关于参数设置不一致的问题报告，涉及对象为vllm中的gptq_marlin模块，由于不同部分要求不同的最小线程限制，导致参数设置存在矛盾。

https://github.com/vllm-project/vllm/issues/6243
这是一个安装问题的bug报告，主要涉及vllm在安装OpenVINO时出现的依赖冲突错误。

https://github.com/vllm-project/vllm/issues/6242
这是一个关于Gemba2-9b在A10G 24gb GPU上无法正常工作的bug报告。

https://github.com/vllm-project/vllm/issues/6241
该issue类型为性能优化建议，主要涉及的对象是vLLM中的`LLMEngine`，问题是由于`LLMEngine`与tensor并行的rank 0进程共处一个进程中，导致了难以使用不同GPU创建多个vLLM实例，需要测量对象序列化的开销。

https://github.com/vllm-project/vllm/issues/6240
这是一个Bug报告，涉及的主要对象是vllm0.5.1在L20上使用FP8量化推理速度慢的问题。这个问题可能是由于v0.5.1版本中的FP8量化在L20上的性能比FP16更慢引起的。

https://github.com/vllm-project/vllm/issues/6239
这是一个bug报告类型的issue，涉及的主要对象是vllm库和在使用qwen1.5 110b int4模型进行推理时遇到的低GPU利用率问题。问题可能由于配置参数设置不正确导致真实批量大小无法提高而引起GPU使用率低的情况。

https://github.com/vllm-project/vllm/issues/6238
这个issue是关于一个bug修复的问题，涉及的主要对象是vLLM代码中linear.py文件中的needs_scalar_to_array逻辑检查。最终导致这个bug的原因是markdown格式不能正确渲染，所以使用了raw html。

https://github.com/vllm-project/vllm/issues/6237
这是一个bug报告，主要涉及安装FlashInfer时出现的错误。由于安装了错误版本的FlashInfer导致出现`'NoneType' object is not callable`的TypeError错误。

https://github.com/vllm-project/vllm/issues/6235
这是一个关于bug的报告，主要涉及的对象是"ray worker rank assignment"，该问题导致了tensor parallel发生在不同机器上，造成性能问题。

https://github.com/vllm-project/vllm/issues/6234
这是一个关于增加LoRa adapter灵活性的issue，用户希望支持从HuggingFace动态加载LoRa适配器。

https://github.com/vllm-project/vllm/issues/6233
这个issue是一个需求，主要涉及的对象是支持从HuggingFace加载lora适配器。由于用户在启动引擎时必须指定本地的lora路径，导致操作繁琐，用户希望能够在运行时加载远程的lora模型。

https://github.com/vllm-project/vllm/issues/6231
这是一个bug报告，涉及的主要对象是Lora adapter model。相对路径中的`~`符号无法正常工作，应该修复这个问题。

https://github.com/vllm-project/vllm/issues/6230
这是一个文档修复类的issue，主要涉及到vllm项目中的lora adapter路径在服务器启动脚本中存在问题，导致了运行时错误。

https://github.com/vllm-project/vllm/issues/6229
该issue是关于文档错误的反馈，主要涉及到Lora模型加载路径问题，可能导致无法成功加载模型。

https://github.com/vllm-project/vllm/issues/6228
这个issue是一个bug报告，主要涉及的对象是vllm的openai api server。由于cuda内存使用量未达到预期值，导致模型无法成功加载，用户寻求帮助解决该问题。

https://github.com/vllm-project/vllm/issues/6227
这是一个BugFix类型的issue，主要涉及到在OpenAI APIs中使用LoRA tokenizer时的问题，可能导致适配器使用自定义添加的token时出现行为不正确的情况。

https://github.com/vllm-project/vllm/issues/6226
这个issue类型是RFC（Request For Comments），主要对象是vLLM的beam search支持。由于强烈的社区反对，导致该提议引起了争议，并最终决定继续支持beam search。

https://github.com/vllm-project/vllm/issues/6225
这是一个Bug报告，主要涉及的对象是vllm代码库中的"benchmark_throughput.py"文件，由于代码中调用的函数参数不一致导致在CPU环境下出现TypeError异常。

https://github.com/vllm-project/vllm/issues/6224
这是一个Bug报告，涉及到LLM（Large Language Model）的推断过程中出现的数值错误，导致无法为图像占位符正确分配令牌。

https://github.com/vllm-project/vllm/issues/6223
这是一个修复Bug的Issue，修复了vLLM在某些情况下detokenization of prompt logprobs会出现错误的问题。

https://github.com/vllm-project/vllm/issues/6222
这是一个文档更新类型的issue，涉及主要对象为Pipeline Parallel。由于缺乏相关文档，用户提出需要更新Pipeline Parallel的文档。

https://github.com/vllm-project/vllm/issues/6221
这个issue是一个Bug报告，主要涉及的对象是vllm项目下的flashinfer功能。原因是在特定环境下，在Docker build时flashinfer缺失。

https://github.com/vllm-project/vllm/issues/6220
这个issue为bug报告类型，主要涉及 Gemma2 支持的上下文限制和 vllm 实际支持的上下文限制不同导致 bug。

https://github.com/vllm-project/vllm/issues/6219
这是一个bug报告，主要涉及VllmWorkerProcess在TP大于1时不正确退出的问题，可能会导致nsys在使用时卡住。

https://github.com/vllm-project/vllm/issues/6218
这是关于功能需求的问题，主要涉及FlashInfer和Gemma 2在AMD GPU上的兼容性。用户想知道如何在AMD GPU上运行Gemma 2模型，并提到遇到了环境变量设置问题以及导入错误。

https://github.com/vllm-project/vllm/issues/6217
这是一个关于bug的报告，主要涉及Phi3 mini GPTQ 4Bit/8Bit模型在环境中出现问题的情况。原因可能是缺少 "model.safetensors.index.json" 文件导致的。

https://github.com/vllm-project/vllm/issues/6216
这是一个bug报告，涉及主要对象为硬件和CUDA，由于未正确使用CUDA_VISIBLE_DEVICES的设备ID，导致无法获取设备信息的问题。

https://github.com/vllm-project/vllm/issues/6215
这是一个Bug报告类型的issue，主要涉及vllm在lightllm环境中无法正常工作的问题。由于需要在本地执行配置文件才能加载指定的模型，导致出现数值错误并提示设置`trust_remote_code=True`选项以解决问题。

https://github.com/vllm-project/vllm/issues/6214
这个issue是一个bug报告，主要涉及到Mamba cache的Cuda Graph padding问题，导致在使用未捕获的批处理大小时，会出现不兼容的张量形状导致的RuntimeError。

https://github.com/vllm-project/vllm/issues/6213
这是一个bug报告，主要涉及的对象是使用vllm作为推理引擎时GPU计算能力识别不正确。这个问题可能是由于GPU计算能力识别代码的bug或者配置参数设置错误引起的。

https://github.com/vllm-project/vllm/issues/6212
这个issue是关于文档更新的类型为文档修复和改进，主要涉及vLLM与CPU后端的部署方面的内容。由于CPU后端与GPU后端不同，需要更新相关考虑事项的文档。

https://github.com/vllm-project/vllm/issues/6211
这是一个关于使用问题的issue，主要涉及VLLM的context长度无法达到预期的bug。可能由于设置参数不当导致context长度无法增加到期望的标准，用户请求帮助解决context长度只有9415 tokens的问题。

https://github.com/vllm-project/vllm/issues/6210
这个issue类型是优化建议，涉及的主要对象是Kernel中的fused_moe配置，由于最后一个chunk大小较小，建议重新加载kernel配置以提高性能。

https://github.com/vllm-project/vllm/issues/6209
这是一个用户提出需求的类型，主要涉及的对象是vllm是否支持多模态的API调用以及是否支持minicpm-v2.5，用户询问如何与vllm集成以运行特定模型的推理。

https://github.com/vllm-project/vllm/issues/6208
这个issue是一个bug报告，主要涉及的对象是VLLM下的Qwen2 72B模型。由于并发测试时服务不可访问，导致了AsyncEngineDeadError错误。

https://github.com/vllm-project/vllm/issues/6207
这是一个bug报告，主要涉及到在使用16个L4 GPU的环境中发生错误。可能是由于环境设置或模型不支持此配置所导致的问题。

https://github.com/vllm-project/vllm/issues/6206
这是一个关于性能优化的issue，主要涉及GPU KV缓存的利用率问题。由于GPU KV缓存使用率较低，用户想要了解如何通过Radix前缀缓存来提高KV缓存利用率。

https://github.com/vllm-project/vllm/issues/6205
这个issue属于文档需求类型，主要涉及的对象是为开发自定义多模态插件提供指南。

https://github.com/vllm-project/vllm/issues/6204
这是一个Bug报告，涉及对象是DeepSeek-v2代码，由于CUDA设备内核图像无效导致RuntimeError。

https://github.com/vllm-project/vllm/issues/6203
这是一个Bugfix类型的issue，主要涉及到vLLM引擎中的outlines FSM编译进度缓存问题。导致此问题的原因是在引擎重新启动后，相同的JSON模式请求会导致FSM重新编译而outlines缓存无法正常工作。

https://github.com/vllm-project/vllm/issues/6202
这是一个关于改进建议的RFC，主要涉及OpenAI Entrypoint中的`stop_reason`字段被弃用，建议使用`finish_reason`替代。原因是`stop_reason`并非OpenAI规范中的一部分，可能会导致对现有用户的不兼容性。

https://github.com/vllm-project/vllm/issues/6201
这是一个bug报告，涉及到vLLM 0.5.1 tensor parallel 2启动时卡在某个步骤上，表现为进程无法继续。

https://github.com/vllm-project/vllm/issues/6200
这是一个bug报告，涉及vllm库的安装过程。由于pip安装时依赖项缺失或版本不兼容，导致安装过程中出现错误。

https://github.com/vllm-project/vllm/issues/6199
这是一个bug报告，涉及的主要对象是加载Mistral-7B-v0.3模型时的safetensors格式支持问题，由于下载的部分safetensors文件无法正常加载导致额外需要下载consolidated.safetensors文件。

https://github.com/vllm-project/vllm/issues/6198
这是一个Bug报告，涉及的主要对象是google/gemma-2-9b。原因是运行该软件时出现了TypeError: 'NoneType' object is not callable错误。

https://github.com/vllm-project/vllm/issues/6197
该Issue类型为用户提出需求，主要涉及VLLM模型中如何使两次不同输入调用的结果保持一致，可能由于用户希望在不同输入情况下确保结果的一致性，可能需要使用种子值等方法进行控制。

https://github.com/vllm-project/vllm/issues/6196
这是一个bug报告，涉及的主要对象是CompletionStreamResponse。由于在序列化为JSON时，`object`字段被错误地移除导致症状为缺失该字段的bug。

https://github.com/vllm-project/vllm/issues/6195
这是一个bug报告，主要涉及用户对简单API服务器和OpenAI API服务器之间的终端点差异不清楚，导致一些用户错误请求终端点而收到404响应的问题。

https://github.com/vllm-project/vllm/issues/6194
这是一个bug报告，该问题涉及的主要对象是Llama370B模型加载后无法使用端点发出请求。可能由于加载模型后未正确初始化端点导致无法正常使用所致。

https://github.com/vllm-project/vllm/issues/6193
这是一个bug报告，涉及的主要对象是vllm库中的guided decoding功能。由于使用Phi-3-small进行引导解码时，请求失败并导致程序崩溃，可能是由于模型无法正确处理请求而导致的问题。

https://github.com/vllm-project/vllm/issues/6192
这是一个bug报告，涉及的主要对象是vllm.LLM库中加载gemma227b模型时遇到的错误。由于将 attention backend 变量设置为 FLASHINFER 后遇到错误，导致用户无法成功加载模型，请求解决此问题。

https://github.com/vllm-project/vllm/issues/6191
这是一个用户需求提出类型的issue，主要涉及到在离线推理中使用类似OpenAI API的功能。用户提出希望能够通过代码中的提示来运行不同的llm推理，而不是使用API调用的方式。

https://github.com/vllm-project/vllm/issues/6190
这是一个Bug报告issue，涉及的主要对象是使用vLLM库中的Gemma2模块。由于使用最新版本的vLLM和FlashInfer时，在特定的代码环境下出现了AsyncEngineDeadError: Task finished unexpectedly with Gemma2 9B错误。

https://github.com/vllm-project/vllm/issues/6189
这是一个用户提出需求的issue，主要涉及vLLM模型在GPU上设备放置，由于当前不支持精确的模型设备放置导致用户提出了此问题。

https://github.com/vllm-project/vllm/issues/6187
这是一个需求提出的issue，主要涉及vLLM中lazy import功能的添加。由于没有使用VLM功能，但在推理时出现了错误，提出希望实现VLM相关模块的延迟导入，以减少对VLM组件的强依赖。

https://github.com/vllm-project/vllm/issues/6186
这是一个关于加载问题的bug报告，主要涉及到BNB Gemma2 9b。由于PyTorch版本与CUDA版本不兼容导致加载问题。

https://github.com/vllm-project/vllm/issues/6185
这是一个关于性能优化的issue，主要涉及到vLLM的Speculative Decoding功能。原因可能是当前的draftmodel speculative decoding效率不高。

https://github.com/vllm-project/vllm/issues/6184
这是一个bug报告，涉及到vllm在WSL2环境下的使用问题，由于kernel版本不符合要求导致模型无法加载。

https://github.com/vllm-project/vllm/issues/6183
这是一个功能需求的issue，涉及到在处理大型对象广播时的改进措施。

https://github.com/vllm-project/vllm/issues/6182
该issue类型为新功能需求；主要涉及的对象是项目的测试流程；用户提出了添加新的测试内容的需求。

https://github.com/vllm-project/vllm/issues/6180
这是一个Bug报告的Issue，主要涉及Vllm中的多进程文件未找到的错误。原因是在加载模型时出现了文件未找到错误，可能是由于多个VllmWorkerProcess尝试访问另一个VllmWorkerProcess的临时triton缓存文件，但文件不存在。

https://github.com/vllm-project/vllm/issues/6179
这是一个bug报告，涉及到在使用vllm进行fp8推断时遇到的问题。导致这个问题的原因可能是在处理KV缓存时使用了不支持FP8的backend，导致推断时出现错误。

https://github.com/vllm-project/vllm/issues/6178
这个issue类型是用户提出需求，该问题单涉及的主要对象是支持AVX2指令集的CPU。由于AVX512指令集要求，导致了部分CPU无法进行推断，用户要求扩展CPU支持到AVX2以解决此问题。

https://github.com/vllm-project/vllm/issues/6177
这是一个Bug报告，涉及Gemina-2-27B和4个A10s，由于输出为空字符串的问题导致。

https://github.com/vllm-project/vllm/issues/6176
这是一个bug报告，主要涉及LLaVA 1.6版本在0.5.1版本下出现异常，并且在处理较大图像请求后表现出故障模式。

https://github.com/vllm-project/vllm/issues/6175
这个issue属于用户提出需求类型，涉及的主要对象是vLLM gemma2，用户希望增加对ROPE scaling的支持。

https://github.com/vllm-project/vllm/issues/6174
这是一个bug报告，涉及的主要对象是OpenAI Embedding Client和vLLM模型，由于未明确指定encoding_format导致base64编码错误。

https://github.com/vllm-project/vllm/issues/6173
这是一个Bug报告，用户在运行Gemma模型时遇到了无法支持FlashAttention的问题。

https://github.com/vllm-project/vllm/issues/6172
这是一个bug报告，涉及Gemma 2 9B的加载错误，TypeError: 'NoneType' object is not callable，可能是由于新版本0.5.1导致的。

https://github.com/vllm-project/vllm/issues/6171
这是一个bug报告，涉及的主要对象是vllm库。由于升级到0.5.1版本后，每次进行推断时都会出现新的bug。

https://github.com/vllm-project/vllm/issues/6170
这个issue类型是一个[RFC]（请求讨论）的类型，主要涉及vLLM项目中关于实现通过KV缓存传输进行解聚预加载的功能。用户提出了一些实现上的目标和步骤，以及需要的功能支持。

https://github.com/vllm-project/vllm/issues/6169
这是一个bug报告，该问题涉及的主要对象是使用VLLM Gemma 2 9B模型的用户。这个问题可能是由于Docker运行命令配置不正确或环境设置问题导致的，用户无法成功加载模型。

https://github.com/vllm-project/vllm/issues/6168
这个issue属于文档改进类型，主要涉及到移动指南内容以及改进输入处理和多模态文档。

https://github.com/vllm-project/vllm/issues/6167
这个issue是文档改进类型的需求，主要涉及vLLM支持的模型按类型重新组织，可能由于当前文档的排版格式问题而导致markdown渲染不正常。

https://github.com/vllm-project/vllm/issues/6166
这是一个Bug报告，涉及对象为运行gemma2 7b时出现CUDA error导致程序报错的问题。

https://github.com/vllm-project/vllm/issues/6165
这是一个关于功能请求的issue，主要涉及到返回隐藏状态的问题，由于新代码已经支持`return_hidden_states`，但在LLM引擎中尚未支持，用户想知道开发进展和时间表。

https://github.com/vllm-project/vllm/issues/6164
该issue属于功能优化类型，主要涉及对`_prepare_model_input_tensors`方法的重构，并引入了`ModelRunnerInputBuilder`来进行逻辑隔离和模块化。由于存在逻辑耦合和复杂性，需要通过重构和优化来提升代码的可维护性和性能。

https://github.com/vllm-project/vllm/issues/6163
这是一个bug报告，主要涉及到环境变量设置问题。原因可能是release pipeline步骤中的环境变量设置无法正常工作，需要将其添加到sscache编译作业中。

https://github.com/vllm-project/vllm/issues/6162
这是一个bug报告，涉及的主要对象是vllm的release wheel build环境变量。这个问题可能由于没有正确设置环境变量导致构建失败。

https://github.com/vllm-project/vllm/issues/6161
这是一个功能需求，主要对象是wheel构建过程中的debug信息，用户要求将构建过程中的debug信息剥离以减小最终wheel文件的大小。

https://github.com/vllm-project/vllm/issues/6160
这个issue是一个bug报告，涉及到vllm项目中的Batch expansion在与lora结合时出现了问题，导致了Spec decode功能无法正常工作。

https://github.com/vllm-project/vllm/issues/6158
这是一个bug报告，涉及的主要对象是vllm项目的文档构建过程。由于模板已经被移除，导致在构建PDF时出现文件找不到的情况。

https://github.com/vllm-project/vllm/issues/6157
该issue类型为版本更新请求，主要对象是软件版本。这个问题是由于需要更新软件版本而导致的。

https://github.com/vllm-project/vllm/issues/6156
这是一个Bug报告，主要涉及deepseek-coder-v2-lite-instruct在使用vllm时在4个GPU上其中一个GPU的利用率为0%，可能由于多GPU并行计算设置问题导致。

https://github.com/vllm-project/vllm/issues/6155
这是一个用户提出需求的issue，涉及到在Vllm中如何使用多实例，并询问其功能是否独立支持，原因在于用户想要利用类似TensorRTLLM后端的多模型支持。

https://github.com/vllm-project/vllm/issues/6154
这是一个用户提出需求的issue，主要涉及vLLM在aws batch上无法查看作业进度，希望能够暴露tqdm进度条以实现进度记录。

https://github.com/vllm-project/vllm/issues/6153
这是一个bug报告，主要涉及GPU内存分配问题，由于CUDA内存不足导致了"torch.cuda.OutOfMemoryError"异常。

https://github.com/vllm-project/vllm/issues/6152
这是一个Bug报告，涉及的主要对象是vllm中的CUDA初始化问题，由于在设置tensor_parallel_size大于1时导致CUDA重新初始化进程出错，最终报出RuntimeError。

https://github.com/vllm-project/vllm/issues/6151
该issue类型为功能需求，主要涉及vLLM在新backend上的集成。原因导致用户提出新设备集成需求。

https://github.com/vllm-project/vllm/issues/6150
这个issue是bug报告，主要涉及到性能问题，用户发现开启了chunked-prefill-enabled后性能低于默认设置，寻求解决该问题。

https://github.com/vllm-project/vllm/issues/6149
这个issue属于编码质量问题，涉及VLM的验证逻辑和文档更新，旨在避免新开发人员对最新API更改感到困惑。

https://github.com/vllm-project/vllm/issues/6148
这是一个功能需求问题，主要涉及的对象是 ChatGLMForCausalLM 模型。由于 LoRA 功能未被支持但已启用，导致了这个问题的提出。

https://github.com/vllm-project/vllm/issues/6147
这是一个bug报告issue，涉及的主要对象是在使用多个GPU时出现CUDA错误。可能由于GPU型号不同所导致。

https://github.com/vllm-project/vllm/issues/6146
这是一个文档改进的issue，涉及改进最大图像特征大小计算和虚拟数据生成的一致性。可能由于markdown渲染问题，采用了HTML格式叙述。

https://github.com/vllm-project/vllm/issues/6145
这是一个bug报告，涉及的主要对象是使用vllm时在推断中使用了tensor_parallel出现错误。这个问题可能由于某种概率导致Worker进程意外终止而导致。

https://github.com/vllm-project/vllm/issues/6144
这个issue是关于Bugfix，涉及到在vLLM中启用chunked prefill和使用flash-attn后端在prefix缓存中存在问题。这个问题可能由于markdown渲染问题导致描述不清晰，需要使用原始html来填写信息。

https://github.com/vllm-project/vllm/issues/6143
这是一个增加Intel Gaudi（HPU）推断后端的需求类型的issue，主要涉及到对应硬件和软件环境的支持，由于缺少对Beam search等功能的支持导致了部分功能无法实现。

https://github.com/vllm-project/vllm/issues/6142
这是一个关于功能需求的issue，主要涉及到deepseek-v2的AWQ支持问题，用户询问是否已经支持了。导致问题的原因可能是尚未支持该功能或者存在其他配置问题。

https://github.com/vllm-project/vllm/issues/6141
这是一个关于bug报告的issue，主要涉及使用vllm的LoRA adapters时出现的Internal Server Error问题。

https://github.com/vllm-project/vllm/issues/6140
这是一个bug报告，涉及的主要对象是解决MoE MP问题的自定义Triton缓存管理器。原因是缺少这个自定义缓存管理器导致编译错误问题。

https://github.com/vllm-project/vllm/issues/6139
该issue是一个功能需求报告，主要涉及到在Qwen2模型中实现Dual Chunk Attention方法。由于需要扩展模型上下文长度，并且需要处理1M tokens输入，因此用户提出了对Qwen2模型集成Dual Chunk Attention方法的需求。

https://github.com/vllm-project/vllm/issues/6138
这是一个Bug报告，主要涉及VLLM的解码功能在处理`best_of>1`时出现失败的问题，导致用户可能无法得到正确的解码结果。

https://github.com/vllm-project/vllm/issues/6137
这是一个Bug报告，主要涉及到VLLM的Spec. decode功能，导致对于n>1或best_of>1的请求失败。

https://github.com/vllm-project/vllm/issues/6136
这个issue是一个Bug报告，主要涉及的对象是在Grafana中使用模板数据源以允许自动导入时出现的问题，由于无法通过ConfigMaps在Kubernetes上导入仪表板JSON文件时设置数据源名称而导致问题产生。

https://github.com/vllm-project/vllm/issues/6135
这是一个bug报告，主要涉及的对象是Phi-3长上下文与fp8 kv缓存的相互作用。由于Phi3的旋转嵌入/缩放实现与fp8 kv缓存不兼容，导致在超过4096个token时获得的输出为无意义字符串。

https://github.com/vllm-project/vllm/issues/6134
这是一个Bug报告类型的Issue，主要涉及到无法找到CUDA库的根目录，导致安装vLLM时出现错误。

https://github.com/vllm-project/vllm/issues/6133
该issue类型为用户提出需求，主要涉及使用vLLM跨多个GPU进行推断及管理多个LoRA，可能是因为用户需要在该环境下进行模型推断并进行LoRA管理。

https://github.com/vllm-project/vllm/issues/6132
这是一个需求类型的issue，主要涉及到分布式单节点PP（Parallel Processing）多进程支持。由于缺乏PP多进程支持，用户想要在分布式环境中实现并行处理。

https://github.com/vllm-project/vllm/issues/6131
这是一个用户提出需求的issue，主要涉及的对象是vllm的server模式和GPU利用率。问题是用户想要充分利用GPU资源，但目前GPU利用率始终低于50%。

https://github.com/vllm-project/vllm/issues/6130
这是一个用户需求类型的issue，主要涉及了VLLM的核心功能中的多进程Pipeline Parallel支持，用户希望能够扩展支持多节点操作，而当前只适用于单节点操作。

https://github.com/vllm-project/vllm/issues/6129
这是一个Bug报告，主要涉及vllm推理服务中的日志记录问题，用户提出无法通过命令行禁止生成日志文件导致占用大量磁盘空间的问题。

https://github.com/vllm-project/vllm/issues/6128
这是一个用户提出需求的 issue，主要涉及 vllm 模型是否支持 embedding 输入，用户希望运行特定模型的推理，但不清楚如何将其集成到 vllm 中。

https://github.com/vllm-project/vllm/issues/6127
该issue是关于性能优化而非bug报告，主要涉及分布式权重加载过程的优化，由于之前的实现导致每个进程都需要读取全部模型权重，在模型较大时会导致加载速度慢。

https://github.com/vllm-project/vllm/issues/6126
这是一个bug报告，涉及到vllm工具中的CUDA运行时错误问题，用户遇到了"RuntimeError: No suitable kernel"的错误提示。

https://github.com/vllm-project/vllm/issues/6125
这是一个关于硬件相关的issue，主要涉及vLLM的CPU后端。由于需要改进异步LLM引擎性能并支持tensor parallel，在CPU后端启用了多进程和张量并行处理。

https://github.com/vllm-project/vllm/issues/6124
这是一个Bug报告，主要涉及的对象是vllm下的Phi3-vision模型。由于不同docker容器版本导致GPU blocks数量显著下降，出现了错误和问题。

https://github.com/vllm-project/vllm/issues/6123
这是一个用户提出需求的issue，主要涉及vLLM在旧nvidia gpu上支持LoRA适配器的问题，由于vLLM依赖的upstream项目punicaai/punica不支持旧gpu，导致部分用户无法在Kaggle等环境中运行。

https://github.com/vllm-project/vllm/issues/6121
这是一个功能需求的issue，主要涉及到VLM（Vision and Language Multimodal）模块中计算模型最大多模态标记数量的功能添加。

https://github.com/vllm-project/vllm/issues/6120
这个issue属于bug报告类型，涉及主要对象是支持Python3.9和Python3.8的PP功能，原因是解决了部分情况下的问题。

https://github.com/vllm-project/vllm/issues/6119
这是一个需求提升类的issue，涉及的主要对象是VLLM项目的安装文档。用户建议提升建议的Python版本至3.10以与CI环境对齐，并为用户提供更好的用户体验。

https://github.com/vllm-project/vllm/issues/6118
这个issue是一个bug报告，涉及到vllm的安装问题，由于缺少符号__nvJitLinkAddData_12_1导致ImportError。

https://github.com/vllm-project/vllm/issues/6117
这是一个用户提出的需求类型的issue，主要对象是VLLM的core组件和分布式系统。该需求是希望在管道并行大小大于1时，仍然能够在张量并行组中使用自定义的allreduce。

https://github.com/vllm-project/vllm/issues/6116
这个issue是一个Bug报告，涉及的主要对象是在运行Mixtral 8x7b FP8量化时遇到了非法内存访问错误。


https://github.com/vllm-project/vllm/issues/6115
这是一个 bug 报告，主要涉及到核心和分布式功能，由于支持层大小在管道并行推断中不可被 pp 大小整除导致的问题。

https://github.com/vllm-project/vllm/issues/6114
这是一个特性需求的issue，主要涉及pipeline parallel inference，由于不支持层大小不能被pp大小整除，导致需求提出。

https://github.com/vllm-project/vllm/issues/6113
这是一个代码质量改进的issue，涉及到vLLM下的`CompressedTensorsW8A8`。这个issue由于发现不需要在`act_scale`部分做分支，做了软件工程上的简化。

https://github.com/vllm-project/vllm/issues/6112
这个issue属于[Hardware][NV] Add support for ModelOpt static scaling checkpoints. 主要对象是为vLLM添加支持ModelOpt静态缩放检查点的功能。原因是需要通过HF加载ModelOpt检查点并且需要hf_quant_config.json文件来加载正确的量化格式。

https://github.com/vllm-project/vllm/issues/6111
这个issue是一个功能需求，主要涉及到在vllm项目中实现模拟的fp8推理。

https://github.com/vllm-project/vllm/issues/6110
这是一个关于支持Fp8的功能改进类 issue，主要涉及到vLLM项目中的`compressedtensors` Fp8模型的支持。由于新增了相关功能和重构了一些工具，可能是为了改进模型压缩和性能问题。

https://github.com/vllm-project/vllm/issues/6109
这个issue是关于bug修复的，主要涉及的对象是使用`MultiprocessingGPUExecutor`时的OMP_NUM_THREADS设置。这个bug修复源于使用多进程GPU执行器时CPU争用导致的性能回归。

https://github.com/vllm-project/vllm/issues/6108
这个issue是关于添加额外模型的需求，而不是一个bug报告。

https://github.com/vllm-project/vllm/issues/6107
这是一个bug报告，主要对象是vLLM到HF格式的文本转换，由于转换不完全正确导致VLM测试中出现误报警告。

https://github.com/vllm-project/vllm/issues/6106
这是一个bug报告，涉及的主要对象是ray cluster。由于环境中CUDA版本不匹配，导致出现Segmentation fault的bug。

https://github.com/vllm-project/vllm/issues/6105
这是一个bug报告，主要涉及的对象是vLLM在增加并发请求时GPU利用率下降的问题。

https://github.com/vllm-project/vllm/issues/6104
该issue属于功能增强请求类型，主要涉及修改vllm的基础镜像。

https://github.com/vllm-project/vllm/issues/6103
这是一个Bug报告，涉及的主要对象是使用fused_moe_kernel triton kernel时出现的编译错误。这个问题的原因是可能是triton的一个bug，导致在使用TP>1时会引发错误。

https://github.com/vllm-project/vllm/issues/6102
这个issue属于新功能添加类型，主要涉及的对象是Nvidia GPU。由于现有的vLLM方法预先分配了大量的缓存张量块，导致了复杂的手动块表管理、性能开销以及刚性的GPU内存利用，因此用户提出了增加vmm（虚拟内存管理）kv缓存的功能需求。

https://github.com/vllm-project/vllm/issues/6101
这是一个关于文档问题的类型(issue)，主要涉及对象为LLM模型中的`prompts`输入。由于文档不清晰，用户想知道`prompts`输入是否会自动添加`chat_template`，以及如何选择使用的`chat_template`，因此产生了这个问题。

https://github.com/vllm-project/vllm/issues/6100
该issue是一个用户提出需求的类型，涉及的主要对象是为BloomForCausalLM添加LoRA支持。由于当前vLLM不支持LoRA适配器，用户提出希望添加LoRA适配器支持。

https://github.com/vllm-project/vllm/issues/6099
这是一个bug报告，该问题涉及到vllm库中的enable_prefix_caching功能。由于开启enable_prefix_caching导致了triron的崩溃。

https://github.com/vllm-project/vllm/issues/6098
这是一个bug报告类型的issue，主要涉及的对象是在使用cuda环境下运行qwen2-instruct-7b时出现了cuda错误。原因可能是参数设置不当导致了这些错误。

https://github.com/vllm-project/vllm/issues/6097
这是一个用户提出需求的类型的issue，主要涉及的对象是GLM4v VLM模型。用户想了解是否有计划支持这个模型，但未得到回复。

https://github.com/vllm-project/vllm/issues/6095
这个issue是关于创建CI jobs for Power(ppc64le)，属于用户提出需求的类型，主要涉及的对象是在IBM Power porting team的开发人员。由于不清楚要为Power添加哪些jobs，因此请求团队帮助解决此问题。

https://github.com/vllm-project/vllm/issues/6094
这个issue是关于文档修复的，主要涉及到构建文档环境的问题，可能是由于缺少适当的环境设置导致文档构建出错。

https://github.com/vllm-project/vllm/issues/6093
这是一个Bug报告，涉及的主要对象是Jamba中的`compute_logits`函数。由于原始实现中通过`ParallelLMHead`的权重传递给`LogitsProcessor`而不是对象本身，导致出现了问题。

https://github.com/vllm-project/vllm/issues/6092
这是一个bug报告，主要涉及到在无效状态下出现的错误。由于缺乏清晰的信息，导致不熟悉此问题的人无法理解。

https://github.com/vllm-project/vllm/issues/6091
这个issue类型为文档改进，主要涉及vLLM Frontend的图片插入问题，由于markdown渲染不起作用，需要使用原始HTML标记。

https://github.com/vllm-project/vllm/issues/6090
这是一个Bug报告，主要涉及VLLM项目中的Ray错误问题，由于执行顺序导致了Ray错误的现象。

https://github.com/vllm-project/vllm/issues/6089
这是一个Bug报告，涉及到vLLM的vision language配置被移除而导致的错误。

https://github.com/vllm-project/vllm/issues/6088
这个issue类型是Misc（其他类型），主要涉及vLLM中拓展Fp8 MoE支持到Qwen的功能。原因可能是要提高MoE模型的权重加载方式的通用性，以及为Qwen添加fp8支持和相应的测试覆盖。

https://github.com/vllm-project/vllm/issues/6087
这是一个bug报告，涉及到vLLM中CI的问题。问题是由于某些环境中的“/”未被去除，导致无法找到路径“//tokenize”。

https://github.com/vllm-project/vllm/issues/6086
这是一个Bug报告，涉及的主要对象是VLLM中的Flashinfer功能。由于CUDA Graph，导致Flashinfer卡住，用户提出了这个问题。

https://github.com/vllm-project/vllm/issues/6085
这是一个用户提出需求的issue，主要涉及如何在openai api server中部署多个模型并为每个模型指定不同的GPU，用户想知道如何加载多个模型，并且允许用户通过指定不同的模型调用ChatCompletionRequest.model，同时为每个模型分配不同的GPU。

https://github.com/vllm-project/vllm/issues/6084
这是一个用户提出需求的类型，主要涉及到加速和部署使用vLLM的Llava系列和Phi3-Vision的最佳实践。问题是关于讨论vllm，用户在此寻求加速和部署LLM和多模态LLM的最佳实践。

https://github.com/vllm-project/vllm/issues/6083
这是一个用户请求帮助的issue，主要对象是vllm的使用方式。用户询问如何在vllm中集成特定模型以进行推理。

https://github.com/vllm-project/vllm/issues/6082
这个issue是关于[ Misc ] Refactor Marlin Python Utilities的一个需求，主要涉及更新compressedtensors和gptq_marlin以使用process_weights_after_loading来重新打包，以及将marlin实用程序提取出来供gptq_marlin和compressedtensors共享。

https://github.com/vllm-project/vllm/issues/6081
这是一个用户提出需求的类型，主要涉及Attention机制中`kv_scale`的拆分为`k_scale`和`v_scale`。由于目前的实现没有分开处理`key`和`value`的缩放，导致了在FP8模型中存在的精度损失。

https://github.com/vllm-project/vllm/issues/6080
该issue属于功能需求类型，主要涉及的对象是平台抽象化。原因是为了消除代码中的`ifelse`模式，将平台识别集中管理，从而实现平台抽象化的目标。

https://github.com/vllm-project/vllm/issues/6079
这是一个bug报告，涉及的主要对象是程序中对Ray模块的导入警告。由于未正确处理Ray模块的导入情况，导致日志中出现多次不必要的警告，甚至影响了相关功能的正常运行。

https://github.com/vllm-project/vllm/issues/6078
这个issue属于用户提出需求，主要涉及的对象是在VLLM中添加对可交换基数注意力的支持。原因是用户希望通过调整基数注意力功能来提高特定设置下的计算效率。

https://github.com/vllm-project/vllm/issues/6077
该issue属于功能请求类型，主要涉及的对象是vLLM的调度机制。由于当前vLLM仅支持基于请求到达时间的先来先服务调度，用户提出增加优先级调度机制以实现请求的优先排序，以实现细粒度调度和公平性维护的需求。

https://github.com/vllm-project/vllm/issues/6076
该issue是关于bug报告，主要涉及的对象是vLLM的conftest.py文件。由于之前的类型提示破坏了Python 3.8部分，导致markdown渲染不起作用，因此在此处使用原始的HTML。

https://github.com/vllm-project/vllm/issues/6075
这个issue是文档修复类型的，涉及到vLLM项目中的PR描述缺失问题。这个问题可能是由于提交PR时未填写完整的描述导致的。

https://github.com/vllm-project/vllm/issues/6074
这个issue是一则bug报告，主要涉及LoRA功能，用户测试了最大等级128导致出现了什么问题。

https://github.com/vllm-project/vllm/issues/6073
这个issue类型是用户提出需求，涉及的主要对象是vLLM在Kubernetes上的运行。由于vLLM的 `/health` endpoint 在服务启动后才返回 HTTP 200，导致难以正确配置`liveness check`，需要添加 `/ready` endpoint 来提前返回状态。

https://github.com/vllm-project/vllm/issues/6072
这是一个bug报告，涉及主要对象为使用tensor parallel加载LoRA时速度很慢的问题，可能是由于多个进程加载LoRA时只有第一个进程速度较快，而其它进程速度很慢所导致的。

https://github.com/vllm-project/vllm/issues/6071
这是一个bug报告，主要涉及的对象是docker hub image for vllm和gemm227Bit。由于模型类型不被识别，导致了无法加载模型的错误。

https://github.com/vllm-project/vllm/issues/6069
这是一个bug报告，主要涉及benchmark_serving.py中使用--model参数时出现的TypeError问题，由于get_model()函数中的问题导致了这个bug。

https://github.com/vllm-project/vllm/issues/6068
这是一个用户请求使用说明的类型，主要涉及gemm2-27b的初始化问题，用户感到困惑并寻求如何使用4位量化的帮助。

https://github.com/vllm-project/vllm/issues/6067
这个issue是一个bug报告，涉及的主要对象是vllm模型和Lora支持。由于Qwen2 MoE模型不支持LoRA，但启用了LoRA导致服务器运行失败，用户提出了关于模型支持Lora的问题。

https://github.com/vllm-project/vllm/issues/6066
这是一个用户提出需求的issue，主要对象是是否支持Ascend 910B，由于对该硬件的支持尚未确定，用户想了解未来是否会支持。

https://github.com/vllm-project/vllm/issues/6065
这是一个关于提交PR描述草稿中markdown渲染问题的bug报告，主要涉及vLLM项目的PR描述填写问题。原因是markdown渲染不起作用导致的。

https://github.com/vllm-project/vllm/issues/6064
这是一个Bug报告，主要涉及的对象是benchmark_serving.py脚本。这个问题由于某些环境变量设置不正确，导致benchmark_serving.py无法正确计算Median TTFT。

https://github.com/vllm-project/vllm/issues/6063
这是一个bug报告issue，涉及到在Jetson设备上禁用NCCL支持。由于NCCL支持未禁用，在运行代码时出现了特殊标记的错误。

https://github.com/vllm-project/vllm/issues/6062
这是一个bug报告，主要涉及vllm下的一个issue，用户在尝试初始化一个LLM实例时一直遇到相同的错误，起因可能是最近Phi3模型的一些变化导致。

https://github.com/vllm-project/vllm/issues/6061
这个issue属于bug报告类型，主要涉及的对象是文档依赖项。这个问题由于移除了某些文档依赖项而导致某些文档部分变空白。

https://github.com/vllm-project/vllm/issues/6060
这是一个Bug报告，涉及主要对象是 vllm 生成服务。由于更换到新的LLM模型后，在调用服务时出现乱码token的问题，导致生成结果含有乱码字符。

https://github.com/vllm-project/vllm/issues/6059
这是一个bug报告，涉及的主要对象是AMD CI环境，由于缺少'nvmlDeviceGetHandleByIndex'属性导致大多数AMD CI运行失败。

https://github.com/vllm-project/vllm/issues/6058
这是一个bug报告，针对vllm环境下加载minicpm模型时出现KeyError: 'lm_head.weight'的问题。

https://github.com/vllm-project/vllm/issues/6057
这是一个用户提出需求的issue，主要涉及如何在请求OpenAI Completions API时使用beam search，可能出现问题是由于缺乏正确的使用方法或参数传递导致的。

https://github.com/vllm-project/vllm/issues/6056
这个issue是关于bug报告，主要涉及对象是pytorch中的torch.cuda.device_count函数。由于pytorch的device_count函数在第一次调用时会缓存设备数量，导致在一些情况下可能出现与CUDA环境不一致的问题。

https://github.com/vllm-project/vllm/issues/6055
该issue属于bug报告类型，涉及设备数量的修复问题。

https://github.com/vllm-project/vllm/issues/6054
这是一个bug报告，涉及到使用--pipeline-parallel-size选项时出现问题。由于使用了pipeline-parallel-size选项，导致出现了该问题。

https://github.com/vllm-project/vllm/issues/6053
这是一个用户提出需求的issue，主要涉及对象是vllm模型的快速注意力机制。导致此问题的原因可能是部署模型时耗时过长。

https://github.com/vllm-project/vllm/issues/6052
这是一个用户提出需求的issue，主要涉及VLLM中attention backend的内核统一问题。这个问题由于目前使用不同内核导致了attention backend逻辑复杂化和性能子优。

https://github.com/vllm-project/vllm/issues/6051
这个issue类型是功能需求提出，涉及到vllm项目中Kernel和Model的调整，主要是为了满足Gemma2模型使用flashinfer需要添加logits_soft_cap功能。由于Gemma2模型需要logits_soft_cap支持，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/6050
这是一个特性需求的issue，涉及MLPSpeculator的Tensor Parallel支持，主要是为了实现对draft worker和target worker不同Tensor Parallel大小的支持。

https://github.com/vllm-project/vllm/issues/6048
这个issue是一个文档格式修复的问题，不是一个bug报告，主要涉及的对象是vLLM的文档。原因是markdown无法正确呈现内容，所以使用了原始的HTML格式。

https://github.com/vllm-project/vllm/issues/6047
这个issue属于性能测试提案，主要涉及新增H100套件，从语境上看并不是涉及bug报告或用户问题。

https://github.com/vllm-project/vllm/issues/6046
这是一个关于Github上vLLM中的一个issue，主要是关于前端代码中的API URL断言放松问题。

https://github.com/vllm-project/vllm/issues/6045
这是一个关于在VLLM中创建带有引导解码功能的示例文件的问题，提出了关于正则表达式和引导输入方面的困惑，可能由于markdown渲染问题，导致展示不完整。

https://github.com/vllm-project/vllm/issues/6044
这是一个Bug报告，主要涉及到vLLM中flashinfer模块的问题，由于没有显式的`end_forward`调用，导致中间缓冲区可能残留垃圾数据，影响结果。

https://github.com/vllm-project/vllm/issues/6043
这是一个关于在ROCm triton flash attention和naive flash attention中添加alibi slopes支持的issue。

https://github.com/vllm-project/vllm/issues/6042
这是一个Bug报告，主要涉及"Watchdog caught collective operation timeout"错误。由于无法重现且难以定位，用户请求提供足够的堆栈跟踪以便调试。

https://github.com/vllm-project/vllm/issues/6041
这个issue类型是文档相关的更改，涉及主要对象为"simple api server"，由于已有相关issue提及移除过时的api server，该问题可能是为了进一步减少"simple api server"的可见性。

https://github.com/vllm-project/vllm/issues/6040
这是一个关于贡献PR描述缺失的issue，主要对象是用于提升vLLM代码质量和审查效率的PR描述内容。由于缺少PR描述，可能导致不符合规范或无法准确理解贡献的目的。

https://github.com/vllm-project/vllm/issues/6039
这是一个Bug报告类型的issue，主要涉及的对象是vllm项目中的RayTokenizerGroupPool模块。由于在旧版本的Ray中缺少`ActorDiedError`，所以需要使用`RayActorError`来代替。

https://github.com/vllm-project/vllm/issues/6038
这是一个Bug报告，主要对象是Speculative decoding的实现。问题源于Speculative decoding目前不遵守每个请求的seed，导致输出在每次请求时都不同。

https://github.com/vllm-project/vllm/issues/6037
这个issue是关于文档中的问题，主要涉及到API的使用。由于文档示例中使用了`api_server`，导致部分用户错误地使用了这个已经被弃用的API名称。

https://github.com/vllm-project/vllm/issues/6036
这个issue是一个功能增强请求，主要涉及的对象是支持Microsoft Runtime Kernel Library来增强低精度计算能力。原因是为了优化混合精度DNN模型部署，特别是在大型语言模型中的量化计算。

https://github.com/vllm-project/vllm/issues/6035
这是一个正在进行中的功能添加issue，主要涉及向模型添加辅助头并在serving时获取分数。

https://github.com/vllm-project/vllm/issues/6034
这是一个bug报告，主要涉及对象是Speculative decoding。原因是Speculative decoding未遵守每个请求的种子值，导致bug。

https://github.com/vllm-project/vllm/issues/6033
这是一个bug报告，该问题涉及vllm库的使用。由于当前主代码在Python3.8上无法正常工作，用户无法成功导入LLM模块，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/6032
这是一个功能改进的Issue，涉及VLLM的SPMD工作执行和Ray加速DAG，通过引入SPMD执行模式来改善性能。

https://github.com/vllm-project/vllm/issues/6031
这个issue类型是CI/Build，涉及的主要对象是在vLLM中重新启用大型模型的评估。这个问题由于重新启用大型模型时hfcache设置完成，markdown渲染不起作用，因此在此处使用原始HTML。

https://github.com/vllm-project/vllm/issues/6030
这是一个bug报告，涉及csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu中重复的分支代码，导致功能异常。

https://github.com/vllm-project/vllm/issues/6029
这是一个bug报告，主要涉及的对象是fused_moe kernel，由于长输入导致crash，需要通过执行kernel切片并聚合输出来解决问题。

https://github.com/vllm-project/vllm/issues/6028
这是一个用户提出需求的类型，主要涉及对支持量化kv缓存（压缩张量）的改进。由于用户需要加载量化模型并使用额外的kv缓存压缩，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/6027
这是一个bug报告，主要涉及的对象是vllm的1B模型。由于将模型推理初始化时间（TPOT）增加，用户想知道造成这种情况的原因。

https://github.com/vllm-project/vllm/issues/6026
这是一个bug报告，主要涉及Gemme2-9b模型不支持的问题。由于该模型不在支持的架构列表中，导致运行推理时产生数值错误。

https://github.com/vllm-project/vllm/issues/6025
这是一个Bug报告，主要涉及的对象是vllm/vllmopenai Docker Image。由于CUDA张量未释放，导致生产者进程在异常后终止并冻结。

https://github.com/vllm-project/vllm/issues/6024
这是一个bug报告，主要涉及模型测试中的缓存问题。

https://github.com/vllm-project/vllm/issues/6023
这个issue属于bug报告类型，主要涉及的对象是使用APC后出现的并发请求处理错误。原因可能是由于APC启用导致返回其他请求的响应。

https://github.com/vllm-project/vllm/issues/6022
这是用户提出需求的类型，主要对象是使用Offline Batched Inference来运行多个对话，由于用户想要实现这个功能但不清楚如何操作。

https://github.com/vllm-project/vllm/issues/6021
这是一个Bug报告，涉及到vllm在离线和在线调用时结果不一致的问题。原因可能是参数配置不正确导致的。

https://github.com/vllm-project/vllm/issues/6020
这是一个未提供具体内容的GitHub issue，类型为需求。主要对象为GitHub bot。原因可能是用户在测试GitHub bot功能时创建了此问题。

https://github.com/vllm-project/vllm/issues/6018
这个issue是一个文档更新类型的问题，主要涉及到更新benchmark backend for scalellm。由于markdown渲染不起作用，所以使用了原始的html代码。

https://github.com/vllm-project/vllm/issues/6017
这个issue类型是需求提出，主要涉及对象是新增模型"facebook/seamless-m4t-v2-large"。用户提出了关于支持未被vllm项目现有模型所支持的SeamlessM4Tv2Model的需求，并指出了相关架构以及需要解决的问题。

https://github.com/vllm-project/vllm/issues/6016
这个issue属于bug报告类型，主要涉及HF cache，可能是由于缓存问题导致的bug症状或问题提出。

https://github.com/vllm-project/vllm/issues/6015
这是一个bug报告，涉及的主要对象是vllm下的embeddings功能。由于某种原因导致了引擎后台任务失败的错误。

https://github.com/vllm-project/vllm/issues/6014
这是一个bug报告类型的issue，主要涉及core的逻辑代码，由于修复prompt限制的逻辑错误导致了问题。

https://github.com/vllm-project/vllm/issues/6013
这是一个bug报告，涉及的主要对象是在xpu和cpu上修复了tp问题。由于xpu路径在`xpu_model_runner`中广播参数，已经移到了`worker`类中，导致需要一些字段（如`seq_lens_tensor`、`max_decode_seq_len`）在制作attention metadata时是必需的。

https://github.com/vllm-project/vllm/issues/6012
这个issue是关于用户需求的问题，涉及的主要对象是vllm模型加载。由于缺少`config.json`文件，用户无法加载本地路径上的模型，询问是否支持加载huggingface本地pytorch pt模型或onnx模型以及如何在离线Python代码和OpenAI Completions API中加载这些模型。

https://github.com/vllm-project/vllm/issues/6011
这是一个bug报告，涉及的主要对象是vllm库中的deepseek coder v2 lite模型。由于unset了`VLLM_TRACE_FUNCTION`参数后再次运行导致出现"Segmentation fault"错误。

https://github.com/vllm-project/vllm/issues/6010
这是一个关于添加分布式测试的Issue，不是Bug报告。

https://github.com/vllm-project/vllm/issues/6009
这是一个bug报告类型的issue，主要涉及到Concurrent timeout问题，原因可能是在测试时出现了50个并发请求，导致了4次连接超时的失败。

https://github.com/vllm-project/vllm/issues/6008
这是一个硬件特定的问题，主要涉及Intel CPU，旨在添加Intel OpenMP调优到Docker文件中。

https://github.com/vllm-project/vllm/issues/6007
这个issue类型是bug报告，主要涉及对象是CUDA初始化过程，由于缺少NVML查询导致了意外的CUDA初始化。

https://github.com/vllm-project/vllm/issues/6006
该issue是一个Feature请求，主要涉及的对象是在ROCm上启用了Scaled FP8 GEMM，因为新AMD硬件（MI30x及更高版本）的加速计算。

https://github.com/vllm-project/vllm/issues/6005
这是一个CI/Build类型的issue，主要对象是vLLM的large LM-Eval测试。由于每次重新下载Llama70B和Mixtral8x7B，导致测试耗时长达40分钟，因此需要通过挂载持久性卷来重新启用这些测试。

https://github.com/vllm-project/vllm/issues/6004
这个issue是Bug报告，主要涉及的对象是vLLM中的`distributed_executor_backend=mp`功能。由于torch初始化顺序导致，某些GPTQ quantization在tp>1时无法与`distributed_executor_backend=mp`共存，会导致`RuntimeError`错误。

https://github.com/vllm-project/vllm/issues/6003
这是一个文档更新类的issue，涉及主要对象是vLLM的CPU支持描述，可能是由于描述过时导致用户发现与实际情况不符，希望修正。

https://github.com/vllm-project/vllm/issues/5994
这是一个Bug报告，涉及的主要对象是使用MI100的docker环境。这个问题是由于使用的环境导致出现了HIP错误：invalid argument，导致了运行时出错并显示了相应的错误信息。

https://github.com/vllm-project/vllm/issues/5993
这是一个Bug报告，涉及的主要对象是VLLM加载模型时出现了显存溢出错误，可能是由于gpu_memory_utilization设置不当导致的。

https://github.com/vllm-project/vllm/issues/5992
这是一个用户提出需求的issue，主要涉及的对象是vllm。由于vllm不支持特定的格式，导致用户无法使用该工具进行推断。

https://github.com/vllm-project/vllm/issues/5991
这是一个关于bug报告类型的issue，主要涉及到分布式计算环境中的CUDA初始化问题。原因可能是在vLLM分离驱动worker和tp 0 worker之前，需要一个不进行CUDA初始化的进程使用默认的多进程方法。

https://github.com/vllm-project/vllm/issues/5990
这是一个bug报告，涉及到在初始化hf config class时出现的pickleable错误，导致phi-3v测试失败。

https://github.com/vllm-project/vllm/issues/5989
这是一个CI/Build类型的issue，主要涉及vLLM下的Phi3Vision模型，由于在主分支上MP测试失败，但在原始PR上却通过，导致临时将该模型从TP测试中移除以解除其他PR的CI阻塞。

https://github.com/vllm-project/vllm/issues/5988
这是一个“功能增强”类型的issue，主要涉及构建CI/Build流程中代码重用且目的是简化新端到端测试的添加。

https://github.com/vllm-project/vllm/issues/5987
这是一个Bug报告，问题涉及到worker model loop在没有更多序列处理时未能正确停止，导致workers可能错误地保持在无限广播循环中。

https://github.com/vllm-project/vllm/issues/5986
这个issue是关于前端的bug报告，涉及了Bad words sampling参数。由于markdown渲染不起作用，所以在这里使用了raw html，并且需要修复相关问题。

https://github.com/vllm-project/vllm/issues/5985
这是一个用户需求反馈问题，主要涉及支持BartForSequenceClassification模型的请求。由于该模型目前不被支持，导致用户无法成功运行该模型。

https://github.com/vllm-project/vllm/issues/5984
这是一个用户提出需求的issue，主要涉及的对象是RotaryEmbedding。由于未提供具体内容，无法分析导致的症状或问题具体是关于什么的。

https://github.com/vllm-project/vllm/issues/5983
这个issue属于bug报告类型，主要涉及vllm下的FlashAttentionMetadata类的初始化参数缺失导致的TypeError错误。

https://github.com/vllm-project/vllm/issues/5982
这是一个bug报告，主要涉及vllm中的 AttributeError: 'NoneType' object has no attribute 'prefill_metadata' bug，用户在运行LLaVANeXT过程中遇到了此问题。

https://github.com/vllm-project/vllm/issues/5981
这是一个Bug报告类型的Issue，主要涉及的对象是Phi-3-Vision模型的使用。由于Phi-3-Vision模型的最大序列长度大于键值缓存中可存储的最大令牌数量，导致出现了新的bug。

https://github.com/vllm-project/vllm/issues/5980
这是一个用户提出需求的issue，主要涉及SPMD worker执行模式的引入，目的是减少系统性能开销并提升任务执行效率。

https://github.com/vllm-project/vllm/issues/5979
这是一个用户提出需求的类型，主要涉及的对象是将最新的html设为默认选项。

https://github.com/vllm-project/vllm/issues/5978
这是一个bug报告类型的issue，主要涉及TPU sampler输出的问题，导致在CC([Spec Decode] Introduce DraftModelRunner)中引入了一个bug。

https://github.com/vllm-project/vllm/issues/5977
这个issue是一个bug报告，涉及的主要对象是TPU backend的`_PAD_SLOT_ID`常量。由于该常量从0更改为1，导致了在插入注意力KV到KV缓存时，丢弃填充标记出现了问题。

https://github.com/vllm-project/vllm/issues/5976
这是一个bug报告，主要涉及vLLM模型在特定版本范围内出现的一个新bug，导致最大序列长度大于KV缓存中存储的最大令牌数量。

https://github.com/vllm-project/vllm/issues/5975
这个issue是关于功能需求的，主要涉及vLLM中支持Ampere GPU上FP8的扩展，问题提出了关于如何改进性能和内存利用的讨论。

https://github.com/vllm-project/vllm/issues/5974
这是一个性能优化的issue，主要涉及的对象是`SequenceStatus.is_finished`方法。由于每次调用该方法时都会创建一个列表，导致性能较慢，因此通过切换到`IntEnum`来进行简单的比较可以加快速度。

https://github.com/vllm-project/vllm/issues/5973
这个issue类型为用户提出需求，主要涉及的对象是模块"yum install"。由于未提供具体内容，无法分析导致的症状或问题。

https://github.com/vllm-project/vllm/issues/5972
这是一个PR草案，涉及到对`_prepare_model_input_tensors`进行重构，目的是为了之后的重构和优化工作。

https://github.com/vllm-project/vllm/issues/5971
这个issue类型是功能改进。主要对象是`get_min_capability`方法。由于旧方法不够灵活，需要更新为类方法以支持所有压缩张量方案。

https://github.com/vllm-project/vllm/issues/5970
这是一个重构重构MoE以将Fp8与Mixtral隔离的问题，主要涉及到代码重构和支持Fp8检查点。原因是之前MoE层和重复的权重加载逻辑存在耦合，并且对于其他MoE模型不支持Fp8逻辑。

https://github.com/vllm-project/vllm/issues/5969
这是一个Bug报告，主要涉及的对象是vllm软件。这个问题发生的原因是无法将一个只含有单个元素的整数张量转换为索引，导致程序崩溃。

https://github.com/vllm-project/vllm/issues/5968
这是一个优化类型的issue，主要涉及数据结构在分配器中的优化，通过更改数据结构可以提高性能。

https://github.com/vllm-project/vllm/issues/5967
这个issue是关于未完成的上游合并（WIP upstream merge），涉及到vLLM项目中的PR提交过程中缺少PR描述的问题。

https://github.com/vllm-project/vllm/issues/5966
这个issue是关于代码重构的问题，涉及到entrypoints tests的重新组织，由于为了更好地保持Git历史，作者将该过程拆分为多个PR。

https://github.com/vllm-project/vllm/issues/5965
这个issue属于功能需求，主要涉及的对象是MLPSpeculator模型的新特性。这个需求是由于需要减小模型的大小并在vLLM中支持这两个新功能。

https://github.com/vllm-project/vllm/issues/5964
这是一个bug报告，主要涉及vLLM下的Whisper支持，可能由于缓存设置不当导致输出错误。

https://github.com/vllm-project/vllm/issues/5963
这个issue属于bug报告类型，主要涉及对象是服务器引擎和logit_bias参数，由于发送无效logit_bias参数会使服务器进入一种糟糕的状态，需要在将其添加到LLMEngine之前验证logit_bias参数的格式以避免在LLMEngine中抛出错误。

https://github.com/vllm-project/vllm/issues/5962
这是一个bug报告，涉及到文件标记为可执行权限的问题。由于一些文件被错误标记为可执行权限，导致了一些不必要的执行权限设置错误。

https://github.com/vllm-project/vllm/issues/5961
这是一个bug报告，主要涉及的对象是vLLM，该问题可能由启用了"enable-chunked-prefill"选项导致vLLM在运行时崩溃。

https://github.com/vllm-project/vllm/issues/5960
这是一个关于修复文件权限错误的bug报告issue。

https://github.com/vllm-project/vllm/issues/5959
这是一个用户提出需求的issue，主要涉及的对象是vLLM的API Server。由于没有提供`/info` endpoint，用户提出需要获取模型配置信息的需求。

https://github.com/vllm-project/vllm/issues/5958
这是一个关于添加优先调度功能的GitHub issue，涉及vLLM的核心功能。

https://github.com/vllm-project/vllm/issues/5957
这是一个bug报告，涉及到vllm 0.5.0.post版本下的 transformers，用户反映qwen1.5-32b-chat没有响应。

https://github.com/vllm-project/vllm/issues/5956
这个issue是一个Bugfix类型的问题，涉及的主要对象是修复了关于Intel CPU的`multi_modal_kwargs`未传递给`CPUModelInput`的问题。导致这个bug的原因是在HTML渲染中未能正确传递参数。

https://github.com/vllm-project/vllm/issues/5955
这是一个需求类型的issue，主要涉及支持在CI上运行多节点测试的脚本。

https://github.com/vllm-project/vllm/issues/5954
该issue是一个bug报告，涉及的主要对象是vLLM模型。由于vLLM没有正确考虑`config.json`中额外的EOS token，导致模型输出重复，类似于之前发布的Llama 3，用户提出了这个bug并提出了相关的修复方案。

https://github.com/vllm-project/vllm/issues/5953
这个issue类型是用户提出需求，主要对象是希望支持从Google获取的Gemma2模型。由于vllm已经支持最接近的模型gemma，用户在询问如何支持他们想要的模型，但没有得到回应。

https://github.com/vllm-project/vllm/issues/5952
这是一个bug报告类型的issue，涉及的主要对象是 Chunked prefill 和 non-chunked output，在长提示下表现不一致。原因可能是环境信息中Python版本显示不完整。

https://github.com/vllm-project/vllm/issues/5951
这是一个bug报告，涉及到vllm项目中的Phi-3-medium-*模型在使用4个GPU时无法被正确支持的问题。

https://github.com/vllm-project/vllm/issues/5950
这是一个用户提出需求的issue，主要涉及的对象是vllm项目中的LLM类和其相关方法。用户提出了希望vllm支持使用LLM的最后隐藏状态嵌入向量的功能。

https://github.com/vllm-project/vllm/issues/5949
这是一个Bug报告，涉及主要对象为AMD测试。由于`transformers >= 4.42.0`安装导致与AMD容器不兼容，由此引发了部分`torchvision`功能无法在容器的版本下正常运行的问题。

https://github.com/vllm-project/vllm/issues/5948
这是一个用户提出需求的issue，主要涉及的对象是vllm，用户想要在多节点多CPU环境下运行推理。

https://github.com/vllm-project/vllm/issues/5947
该issue类型为bug报告，主要涉及的对象是benchmark testing。由于输入数据随机采样且输出固定，可能存在的问题是之前的测试数据不规范导致了结果不稳定，因此需要进行修复。

https://github.com/vllm-project/vllm/issues/5946
这个issue类型为用户提出需求，主要对象是关于添加FAQ文档到'serving'模块。由于用户提到了在短期内解决常见问题和在长期内建立一个常见问题的知识库，表明用户希望解决项目中一些常见问题并提供更好的支持和文档。

https://github.com/vllm-project/vllm/issues/5945
这是一个用户提出需求的issue，主要涉及对象是如何将vllm保存日志到文件。原因是用户希望能够将日志保存到文件而不是仅在屏幕上显示。

https://github.com/vllm-project/vllm/issues/5944
这个issue是关于bug报告，主要涉及到vllm项目中的`work_use_ray`功能无法正常运行。由于代码变更，现在需要使用`engine_config.parallel_config.distributed_executor_backend`来替代`ray`，需要及时更新文档。

https://github.com/vllm-project/vllm/issues/5943
这个issue是关于硬件优化的需求，涉及对象为Intel CPU，用户希望使用ipex varlen attention来计算prompts以获得更好的性能。根据描述，这个需求是为了提高计算效率和代码质量。

https://github.com/vllm-project/vllm/issues/5942
这是一个优化类型的Issue，主要对象是vllm项目中的代码。由于使用了一个numpy ndarray对象池来保存序列数据，导致了提升了请求速率和tokens速率。

https://github.com/vllm-project/vllm/issues/5941
这个issue是关于Kernel部分增加per-tensor和per-token AZP epilogues的需求，属于其他类型的贡献请求。

https://github.com/vllm-project/vllm/issues/5940
这是一个开发者提交的[ Misc ]类型的Issue，主要涉及代码重构，旨在简化权重加载逻辑。由于需要减少在`linear.py`函数中的复杂性，将`w8a8`加载转换为使用`process_weights_after_load`方法，以便将N个逻辑PerTensor尺度转换为按通道的尺度。

https://github.com/vllm-project/vllm/issues/5939
这是一个bug报告类型的issue，涉及主要对象是MoE kernel，由于batch size过大，导致可能出现非法内存访问的问题。

https://github.com/vllm-project/vllm/issues/5938
这是一个Bug报告，主要涉及的对象是MoE triton kernel在大工作量下发生非法内存访问，可能是因为工作量过大导致。

https://github.com/vllm-project/vllm/issues/5937
这个issue是关于vLLM项目的Virtual Office Hours通知，属于活动公告类型，主要涉及到vLLM社区的成员和对vLLM项目感兴趣的人。

https://github.com/vllm-project/vllm/issues/5936
这个issue是一个bug报告，主要涉及Attention模型中的`kv_scale`参数创建问题，可能导致FP8 KV cache quantization未启用时产生错误。

https://github.com/vllm-project/vllm/issues/5935
该issue类型是需求提出，主要对象是前端部分，由于当前版本存在一些障碍使得base64 embedding无法启用。

https://github.com/vllm-project/vllm/issues/5934
该issue类型为用户提出需求，主要涉及的对象是添加支持新模型"microsoft/Florence2base"。由于尚未收到任何响应，用户希望知道支持他们想要的模型存在何种困难。

https://github.com/vllm-project/vllm/issues/5933
这是一个关于提交PR描述不完整的问题，主要涉及代码质量和审查流程效率。可能是因为未按照规范填写PR描述而导致此问题。

https://github.com/vllm-project/vllm/issues/5932
该issue类型为性能优化（Enhancement）请求，主要对象是针对mi300X的fused_moe Triton内核，由于调整后的配置文件带来了约10%的性能提升。

https://github.com/vllm-project/vllm/issues/5931
这是一个Bugfix类型的issue，涉及的主要对象是cutlass 3.x epilogues。由于计算类型设置错误导致使用了16位的输出数据类型而不是fp32，导致了测试无法通过的问题。

https://github.com/vllm-project/vllm/issues/5930
这是一个[Kernel]类型的issue，主要涉及Punica维度的添加以适用于IBM Granite 3b和8b系列模型。

https://github.com/vllm-project/vllm/issues/5929
这是一个关于修复markdown渲染问题的PR。

https://github.com/vllm-project/vllm/issues/5928
这是一个"[Misc]"类型的issue，主要涉及需要从`RowParallelLinear`中移除`fp8_shard_indexer`，以简化权重加载。

https://github.com/vllm-project/vllm/issues/5927
这个issue是一个bug报告，主要对象是分布式代码中的tensor dict keys。原因是%不应该出现在tensor dict keys中，造成了markdown渲染错误。

https://github.com/vllm-project/vllm/issues/5926
这个issue是一个Bugfix类型的问题，涉及的主要对象是在openai completions benchmark中缺失了最后一个itl。原因是当存在"usage"字段时，会忽略最后一个token的itl记录，因为API返回了包含最后一个token的"usage"字段。

https://github.com/vllm-project/vllm/issues/5925
这是一个关于扩展vLLM日志记录API的Issue，请求支持自定义Prometheus客户端和其他任意日志记录器。

https://github.com/vllm-project/vllm/issues/5924
这是一个Bug报告，涉及到vLLM中模型"talk to itself"并忽略`<|im_end|>`的问题。由于该问题导致模型执行时会出现对自身对话的情况，并不遵守`<|im_end|>`的设定。

https://github.com/vllm-project/vllm/issues/5923
这是一个功能需求问题，涉及前端代码，要求在tokenize端点中支持聊天完成输入。

https://github.com/vllm-project/vllm/issues/5922
这是一个CI/Build类型的issue，涉及vLLM中的Sampler测试。由于最新的`transformers`版本引入了一个破坏性改变，导致CI测试出现错误。

https://github.com/vllm-project/vllm/issues/5921
这是一个关于bug修复的issue，涉及的主要对象是在加载具有融合QKV/MLP的模型时出现的问题。问题的原因是在权重加载时对于融合QKV/MLP模型的处理逻辑不正确导致了加载错误。

https://github.com/vllm-project/vllm/issues/5920
这是一个需求报告，主要涉及的对象是Blip2模型支持，由于需要实现BLIP2模型以及BLIP2支持问题而产生。

https://github.com/vllm-project/vllm/issues/5919
这个issue是一个bug报告，涉及的主要对象是Detokenizer.decode_prompt_logprobs_inplace函数。由于`prev_tokens`与`next_iter_tokens`是同一个引用，可能导致`prev_tokens`指数级增长，需要修复这个问题。

https://github.com/vllm-project/vllm/issues/5917
这是一个bug报告issue，涉及主要对象是vLLM中的kernel集成。原因是集成的prototype在性能方面出现了问题。

https://github.com/vllm-project/vllm/issues/5916
这是一个提供给Kubernetes用户的文档说明，不是bug报告。问题涉及的主要对象是Kubernetes用户。原因可能是希望分享从特定GitHub链接中学到的经验教训。

https://github.com/vllm-project/vllm/issues/5915
这是一个Bug报告，涉及的主要对象是vLLM中的FP8模型加载问题。由于FP8配置假设每个分片具有单独的尺度，但合并的层具有单一尺度，导致了模型加载时未能正确处理合并线性模块的尺度。

https://github.com/vllm-project/vllm/issues/5914
这是一个用户提出需求的 issue，主要涉及提高模型贪心解码效率的方法讨论，由于模型在 confidence 低时不必要的推测导致速度下降。

https://github.com/vllm-project/vllm/issues/5913
这是一个bug报告，涉及主要对象为Gemma 1，问题可能是由于未经优化的`GemmaRMSNorm`导致Gemma 1的速度下降。

https://github.com/vllm-project/vllm/issues/5912
这个issue类型是用户提出需求，主要涉及的对象是为VLLM模型增加对新的Gemma 2模型的支持，由于当前尚未支持这些新模型，用户希望能够为其添加支持。

https://github.com/vllm-project/vllm/issues/5911
这个issue类型是改进提案，主要涉及前端代码，处理用户设定的`max_model_len`超出模型参数的错误条件，改为警告，以适应特定需求。原因是先前用户超出最大长度可能触发错误，现在改为警告，提高灵活性。

https://github.com/vllm-project/vllm/issues/5910
这是一个关于新增模型支持的issue，涉及主要对象是支持头部大小为120的模型。导致这个问题出现的原因是代码中有关120不是16的倍数导致的症状。

https://github.com/vllm-project/vllm/issues/5909
这是一个bug报告，主要涉及Lora模块中使用safetensor keys进行模块验证逻辑的问题，导致模型训练时出现未预料的情况。

https://github.com/vllm-project/vllm/issues/5908
这是一个关于新增模型的issue，主要涉及Gemma 2模型的一些改动以及临时处理措施，因为Gemme 2的注意力logit softcapping和滑动窗口注意力问题而需要对代码进行修改。

https://github.com/vllm-project/vllm/issues/5907
这是一个Bug报告类型的issue，涉及的主要对象是vLLM项目中的CUDA内存溢出问题。由于在warmup阶段未考虑到logprobs内存使用，当请求大量logprobs时会导致内存溢出。

https://github.com/vllm-project/vllm/issues/5906
这是一个bug报告，涉及的主要对象是vllm的docker容器。原因可能是在发送请求到已部署模型的/v1/embeddings端点时出现了"Internal Server Error"响应。

https://github.com/vllm-project/vllm/issues/5905
这是一个Bug报告，主要涉及的对象是VLM中的`multi_modal_kwargs`，问题是由于预提交测试未包含最新的分布式后端代码，导致最终测试捕获了这个bug。

https://github.com/vllm-project/vllm/issues/5904
这是一个关于代码重构的Issue，主要涉及到测试文件的重新组织。

https://github.com/vllm-project/vllm/issues/5903
这是一个Bug修复的Issue，涉及到vLLM引擎在收到无效请求后失败的问题。问题是由于传递无效的`logit_bias`参数，导致服务器进入错误状态。

https://github.com/vllm-project/vllm/issues/5902
这是一个bug报告issue，涉及到markdown渲染问题导致无法显示内容。

https://github.com/vllm-project/vllm/issues/5901
这个issue是关于一个Bug报告，主要涉及的对象是`AsyncEngineDeadError`，报告的症状是出现了多种不同情况下的`AsyncEngineDeadError`错误。

https://github.com/vllm-project/vllm/issues/5899
这是一个bug报告，涉及主要对象是如何在同一端口上为多个模型提供服务。这个问题是由于尝试在相同端口上部署两个模型时引起的，导致了地址已被占用的错误。

https://github.com/vllm-project/vllm/issues/5898
这是一个bug报告，主要涉及VLLM在批处理大小大于1时产生不一致响应的问题。这可能是由于批处理调度程序处理多个请求时的方式导致的。

https://github.com/vllm-project/vllm/issues/5897
这是一个关于在vLLM embeddings中添加支持base64编码的issue，用户需求的是增加对base64格式的编码支持，以便根据指定的`encoding_format`返回embeddings。

https://github.com/vllm-project/vllm/issues/5896
这是一个Bug修复类型的Issue，主要涉及Model中的flags以及Phi-3-Vision的相关内容。由于未设置默认值导致vLLM未将Phi3Vision视为视觉模型，需要更新接口以解决此问题。

https://github.com/vllm-project/vllm/issues/5895
该问题是一个bug报告，涉及的主要对象是运行`benchmark_serving.py`时返回异常结果的情况。由于服务器无法响应任何有效的令牌，导致TTFT结果为0，从而使得最终结果异常。

https://github.com/vllm-project/vllm/issues/5894
这是一个bug报告，主要对象是MLPSpeculator模块，导致了错误消息不清晰。

https://github.com/vllm-project/vllm/issues/5893
这个issue是关于bug报告，主要涉及的对象是MLPSpeculator和`num_speculative_tokens`参数。由于设置的`num_speculative_tokens`值超过支持的最大值，导致用户收到了令人困惑的错误信息。

https://github.com/vllm-project/vllm/issues/5892
这是一个关于持续集成构建的类型为bug报告，主要涉及multi_modal_kwargs参数的广播错误导致的问题。

https://github.com/vllm-project/vllm/issues/5891
这是一个用户提出需求的issue，主要涉及的对象是添加对Lora适配器的分布式推理支持。由于Lora适配器在加载70B模型时遇到问题，导致生成吞吐量和提示吞吐量为0。

https://github.com/vllm-project/vllm/issues/5890
这是一个Bug报告，涉及的主要对象是vllm（llama-3-8b-instruct），查询带有logprobs和echo导致vllm崩溃。

https://github.com/vllm-project/vllm/issues/5889
这是一个bug报告issue，主要涉及到vllm的安装和编译过程中发生的错误。这个问题导致了在运行时导入_C动态链接库时出现未定义符号错误。

https://github.com/vllm-project/vllm/issues/5888
这个issue是关于Bugfix的问题，主要涉及Phi3-Vision中图片尺寸解析的错误，导致在处理2D张量时出现问题。

https://github.com/vllm-project/vllm/issues/5887
这是一个bug报告，主要涉及Phi-3-Vision示例中的上下文长度问题，导致vllm在某些情况下会出现内存溢出错误。

https://github.com/vllm-project/vllm/issues/5886
这是一个需求提出类型的issue，主要涉及vLLM中的SmartSpec方法是否会被实现到主代码库中。

https://github.com/vllm-project/vllm/issues/5885
这是一个bug报告，涉及主要对象是phi3 Vision。由于同时处理多个请求可能导致后端崩溃，导致应用拒绝进一步请求。

https://github.com/vllm-project/vllm/issues/5884
这个issue是一个feature的提案，主要涉及的对象是vllm引擎的控制最大队列时间功能。由于队列长度超出设定值，导致需要对请求进行错误处理。

https://github.com/vllm-project/vllm/issues/5883
这是一个bug报告，关于vllm在RTX 3070上内存超限的问题。

https://github.com/vllm-project/vllm/issues/5882
这个issue是一个性能优化类型的issue，主要对象是逻辑块。由于不需要逻辑块，导致了性能提升的bug。

https://github.com/vllm-project/vllm/issues/5881
这是一个Bug报告，涉及到ASGI应用程序异常，可能由于代码中的错误导致了异常。

https://github.com/vllm-project/vllm/issues/5880
这个issue是一个Bugfix类型的issue，涉及的主要对象是处理多GPU启用时在运行`examples/llava_example.py`时出现的bug。该bug的症状是当前广播逻辑仅支持第一级张量，但对于`multi_modal_kwargs`，张量可以嵌套在其下。

https://github.com/vllm-project/vllm/issues/5879
这是关于在vLLM中关于LLaVANexT的一个文档添加的issue，用户反馈了关于如何使用LLaVANexT的困惑。

https://github.com/vllm-project/vllm/issues/5878
这是一个 bug 报告，主要涉及的对象是硬件(TPU)。问题出现的原因是当前的 swapping 操作在无意中将整个 CPU KV 缓存复制到 GPU，导致操作非常缓慢。

https://github.com/vllm-project/vllm/issues/5877
这是一个优化类型的issue，主要涉及到使用numpy ndarray池来存储序列数据。原因是为了提高性能并确保在用户请求中只使用Python列表。

https://github.com/vllm-project/vllm/issues/5876
这是一个bug报告，主要涉及的对象是`MLPSpeculator`中的`num_speculative_tokens`处理，问题导致加载过多层级的speculator模型，但不影响实际生成的speculative tokens数量。

https://github.com/vllm-project/vllm/issues/5875
这个issue是一个修复bug的问题单，主要涉及MLPSpeculator的cuda graph。这个bug可能由于未正确应用来自Medusa PR 4978的调整导致。

https://github.com/vllm-project/vllm/issues/5874
这是一个功能优化的issue，主要涉及重构线性层权重加载的逻辑和引入新的参数类型。

https://github.com/vllm-project/vllm/issues/5873
这个issue类型是文档更新类，涉及主要对象是环境变量的使用，由于之前环境变量的使用方式可能引发了冲突，需要更新文档避免此类问题。

https://github.com/vllm-project/vllm/issues/5872
这是一个Bug报告，主要涉及的对象是vLLM代码库，由于detokenization逻辑中的问题导致了vLLM在使用`prompt_token_ids`和`prompt_logprobs`时偶尔会卡住。

https://github.com/vllm-project/vllm/issues/5871
这是一个用户提出需求的类型。该问题单涉及的主要对象是在TPU上实现张量并行。由于需求是在TPU上实现分布式推理支持，从而为该需求提供基于Ray的TPU执行器。

https://github.com/vllm-project/vllm/issues/5870
这个issue是一个bug报告类型的issue，主要涉及移除图像特征功能，可能是因为在CI中进行测试时才需要，但该功能出现问题导致markdown渲染无法正常工作。

https://github.com/vllm-project/vllm/issues/5869
这是一个Bugfix类型的issue，主要涉及的对象是TPU后端，由于CPU缓存分配问题导致的bug。

https://github.com/vllm-project/vllm/issues/5868
这是一个Bug报告issue，主要涉及的对象是LLaVa Next Value服务。由于配置问题在Docker环境中运行LLaVa服务时出现"Incorrect type of image sizes"错误。

https://github.com/vllm-project/vllm/issues/5867
这是一个关于bug报告的issue，主要涉及的对象是v0.5.0.post1版本的vllm以及IP绑定错误。用户遇到IP绑定错误可能是由于版本升级导致的。

https://github.com/vllm-project/vllm/issues/5866
这是一个Bug报告，涉及VLLM模型推断中出现的问题，导致end_sync操作有时会死锁。

https://github.com/vllm-project/vllm/issues/5864
这是一个Bug报告，主要对象是vllm中的模型Phi3VForCausalLM。由于版本不支持导致无法运行该模型，出现数值错误。

https://github.com/vllm-project/vllm/issues/5863
这是一个bug报告，涉及问题主要对象为vllm项目中的llava_example.py文件。由于在启用多个GPU并将`type`参数设置为`image_features`时，运行该文件会报错。

https://github.com/vllm-project/vllm/issues/5862
这是一个用户提出需求的 issue，主要涉及 vllm 在线推理时无法找到 'stream' 开关所导致的问题。

https://github.com/vllm-project/vllm/issues/5861
这是一个Bug报告，主要涉及对象是Ray版本问题，导致加载模型时出现错误。

https://github.com/vllm-project/vllm/issues/5860
这是一个bug报告，涉及到TPU backend 的KV cache size 计算错误，由于计算错误导致了问题。

https://github.com/vllm-project/vllm/issues/5859
这是一个用户提出需求的类型的issue，主要涉及支持在vllm中记录模型的输入和输出至文件的功能。

https://github.com/vllm-project/vllm/issues/5858
这是一个用户提出需求的issue，主要涉及的对象是Optimumquanto quantized models，用户想知道是否可能通过vllm来加速这些模型。

https://github.com/vllm-project/vllm/issues/5857
这是一个关于需求提出的问题单，主要涉及VLLM模型的benchmark测试，因为目前只支持Sonnet和ShareGPT数据集，但输入和输出长度不固定，提出需求实现固定输入和输出长度的benchmark。

https://github.com/vllm-project/vllm/issues/5856
这是一个特性改进的issue，涉及vllm项目中的speculative decoding功能，主要对象是支持draft model在不同的tensor-parallel大小上运行。最后的问题可能是关于性能改进或功能扩展的需求。

https://github.com/vllm-project/vllm/issues/5855
这是一个关于提升TPU后端并行采样和交换功能的功能需求，涉及的主要对象是TPU后端。

https://github.com/vllm-project/vllm/issues/5854
这个issue类型是bug报告，主要涉及到vllm中CUDAGraph captured generation stuck的问题，可能由于custom_all_reduce使用cross_device_reduce_1stage而不是cross_device_reduce_2stage导致生成处于随机卡住状态。

https://github.com/vllm-project/vllm/issues/5853
这是一个Bug报告，主要涉及到服务器在使用chunked-prefill时出现错误。原因可能是当前环境下的某些配置不兼容导致的问题。

https://github.com/vllm-project/vllm/issues/5852
这个issue属于[Model]类型，涉及的主要对象是VLM配置中像素值和图像特征的支持。由于要移除`image_input_type`，用户接口被更新为`multi_modal_data = {"image": image}`。

https://github.com/vllm-project/vllm/issues/5851
该issue是一个bug报告，涉及的主要对象是vLLM中的pixel_values和image_features功能。原因是移除了对这两个功能的支持，导致相关功能无法正常使用。

https://github.com/vllm-project/vllm/issues/5850
这个issue是关于bug报告，主要涉及硬件TPU，由于使用了不支持的抽样参数，导致引发错误。

https://github.com/vllm-project/vllm/issues/5849
这是一个bug报告，主要涉及vLLM在处理`min_tokens`时未正确处理多个eos tokens导致的问题。

https://github.com/vllm-project/vllm/issues/5848
这是一个bug报告，主要对象是"vllm"中的"distributed/test_shm_broadcast.py"文件，由于分布式通信操作测试失败导致该问题。

https://github.com/vllm-project/vllm/issues/5847
这是一个Bug报告，主要是针对加载具有巨大上下文长度的小型模型导致OutOfMemoryError的问题。

https://github.com/vllm-project/vllm/issues/5846
这是一个Bugfix类型的issue，主要涉及到VLLM项目中的decode_prompt_logprobs_inplace()函数，由于未跳过序列中第一个标记，导致造成token_id与all_token_ids[token_position]检查无法正常工作，进而导致detokenize_incrementally生成不正确的结果。

https://github.com/vllm-project/vllm/issues/5845
该issue为CI/Build类型，涉及vLLM的PR提交流程。由于未填写PR描述和标题分类，导致无法顺利提交PR。

https://github.com/vllm-project/vllm/issues/5844
该issue是一个bug报告，涉及的主要对象是影响图像上传的async_get_and_parse_image函数。这个bug的原因是未在调用该函数时使用await关键字导致图像未能正确上传。

https://github.com/vllm-project/vllm/issues/5843
这是一个bug报告，涉及的主要对象是Phi 3 Vision的API，由于连接保持开启时连续发起两次请求且使用不同图片会导致一个错误，可能是由于某行代码缺少await关键字所致。

https://github.com/vllm-project/vllm/issues/5842
这个issue属于bug报告类型，涉及的主要对象是VLLM安装过程。由于模型配置中的量化方法与`quantization`参数中指定的量化方法不匹配，导致出现数值错误。

https://github.com/vllm-project/vllm/issues/5841
这个issue是一个Bugfix类型的issue，主要涉及NeuronExecutor，在运行examples/offline_inference_neuron.py脚本时会遇到一个断言错误。这个问题是因为scheduler输出，如blocks_to_swap_in等是空列表，但是断言检查空字典。

https://github.com/vllm-project/vllm/issues/5840
这是一个Bug报告，主要涉及neuron的离线推断示例脚本，由于调度器输出为空列表导致NeuronExecutor中的断言错误。

https://github.com/vllm-project/vllm/issues/5839
这个issue属于CI/Build类型，主要涉及对压缩张量的端到端测试。可能由于markdown渲染问题，导致无法正确显示section内容。

https://github.com/vllm-project/vllm/issues/5838
这个issue是关于CI/Build的问题，涉及LM Eval Harness Based CI Testing，主要是为了添加lmeval harness based testing功能。

https://github.com/vllm-project/vllm/issues/5837
这是一个用户提出需求的issue，主要涉及自动合并测试PR的功能。

https://github.com/vllm-project/vllm/issues/5836
这是一个功能请求，涉及修改HF（HuggingFace）模型的`config.json`文件。由于需要修改HF模型的配置文件以及与OpenAI兼容的服务器测试，所以提出了相关问题。

https://github.com/vllm-project/vllm/issues/5835
这个issue是一个功能需求（Feature request），主要涉及的对象是对vLLM实现是否支持分布式推理（distributed speculative inference）的问题。

https://github.com/vllm-project/vllm/issues/5834
该issue属于需求提出类型，涉及到对gloo和nccl通信测试的添加。原因是需要测试gloo通信，并相应地设置`GLOO_SOCKET_IFNAME`/`NCCL_SOCKET_IFNAME`。

https://github.com/vllm-project/vllm/issues/5833
这是一个bug报告，主要涉及VLLM下的OPT-x模型输出不一致的问题，可能是由于当前环境中PyTorch版本与CUDA版本不匹配所导致。

https://github.com/vllm-project/vllm/issues/5832
这是一个文档修复（[Doc]）类型的issue，涉及使用OpenAI Server与VLM的示例，以及添加URL验证功能。这个问题的主要原因是为了改进vLLM代码质量和提高审查效率。

https://github.com/vllm-project/vllm/issues/5831
这个issue类型是功能改进，主要对象是TPU backend。

https://github.com/vllm-project/vllm/issues/5830
这是一个用户提出需求的issue，主要涉及vLLM中多LoRA问题实现的具体细节。该问题探讨了adapter切换的机制，包括基于队列的权重因素和GPU内存中adapter数控制等，用户希望了解如何管理adapter以优化模型服务器的性能。

https://github.com/vllm-project/vllm/issues/5829
这是一个Bug报告，主要涉及的对象是VocabParallelEmbedding层，问题是该层目前不支持2D输入形状。

https://github.com/vllm-project/vllm/issues/5828
这是一个Bug报告类型的Issue，主要涉及到vllm在0.5.0.post1版本出现的NCCL错误，导致vllm无法加载。原因可能是版本升级导致的系统错误。

https://github.com/vllm-project/vllm/issues/5827
这个issue属于Bug报告类型，主要涉及的对象是Alibaba-NLP/gte-Qwen2-7B-instruct模型的部署，用户反馈当调用模型的/v1/embeddings端点时收到"Internal Server Error"响应，请求解决该问题以使模型作为嵌入模型运行。

https://github.com/vllm-project/vllm/issues/5826
这是一个bug报告类型的issue，主要涉及的对象是vLLM模型的使用者。由于目前vLLM镜像不支持某些模型架构（如'NVEmbedModel'），导致用户在尝试在Kubernetes集群中运行特定模型时出现错误。

https://github.com/vllm-project/vllm/issues/5825
这是一个特性要求类的issue，主要涉及vLLM代码库，作者提出将"Classifier-Free Guidance"实现到vLLM中。

https://github.com/vllm-project/vllm/issues/5824
该issue属于代码优化类型，主要涉及到cpu_worker文件中的无用代码。原因可能是需要简化代码结构或者提高代码执行效率。

https://github.com/vllm-project/vllm/issues/5823
这是一个Bug报告issue，涉及使用temperature参数时请求永远不返回的问题，可能是因为temperature值大于2时应该返回400错误，但实际上请求没有返回。

https://github.com/vllm-project/vllm/issues/5822
这是一个Bug报告，涉及vLLM的一个issue，由于传入的参数`logit_bias`不正确导致AsyncEngineDeadError错误。

https://github.com/vllm-project/vllm/issues/5821
该issue属于功能增强类型，主要涉及到测试图像的使用方式。这个改动是为了提高使用测试图像的便利性，通过将相关方法整理到新的类中（ImageAsset）。

https://github.com/vllm-project/vllm/issues/5820
这是一个功能需求，主要涉及对象是针对vllm项目中的Phi-3视觉模型，用户希望能够在该模型中支持多个图像输入，而不仅仅限于单个图像。

https://github.com/vllm-project/vllm/issues/5819
这是一个Bug报告，涉及主要对象是VLLM系统。由于KeyError导致了推理过程中的异常。

https://github.com/vllm-project/vllm/issues/5818
该issue属于文档更新类型，主要对象是VLMs的用户。这个问题是由于计划进行几项对VLMs的重大更改，需要提前通知用户。

https://github.com/vllm-project/vllm/issues/5817
这是一个需求提出类型的issue，主要涉及的对象是为vllm项目添加对DeepSeek VL的支持。由于缺乏对DeepSeek VL的支持，用户希望在该模型中进行测试并修复相关问题。

https://github.com/vllm-project/vllm/issues/5816
这个issue类型是提出需求，涉及的主要对象是CI workflow for PRs，由于当前CI成本过高，需要采取措施降低CI成本，因此提出了更改CI工作流程的建议。

https://github.com/vllm-project/vllm/issues/5815
这是一个bug报告类型的issue，主要涉及vllm attention backend设置为flashinfer时出现的问题。通过描述中的环境设置和操作步骤可以推断，用户遇到了与flashinfer后端相关的bug。

https://github.com/vllm-project/vllm/issues/5814
这是一个Bug报告类型的Issue，主要涉及的对象是test_skip_speculation测试功能。导致这个Bug的原因是跳过功能未考虑多个草稿工作器情况。

https://github.com/vllm-project/vllm/issues/5813
这是一个功能增强类型的issue，主要涉及vLLM中的bitsandbytes量化模块，并提供了对张量并行性的支持。

https://github.com/vllm-project/vllm/issues/5810
这个issue类型是需求提出，该问题单涉及的主要对象是将分布式推理中的模型注册支持外部扩展，原因是当前系统在分布式推理环境中缺乏对模型的外部注册支持。

https://github.com/vllm-project/vllm/issues/5809
这是一个功能需求类型的issue，主要涉及到MLPSpeculator Tensor Parallel支持的问题。

https://github.com/vllm-project/vllm/issues/5808
这是一个用户提出需求的issue，主要对象是支持MiniCPM-Llama3-V-2_5模型。由于目前vllm 0.5.0.post1版本不支持该模型，用户希望添加对其的支持。

https://github.com/vllm-project/vllm/issues/5807
这是一个关于向 vLLM 添加控制向量支持的问题，其中提到当前存在bug，主要涉及控制向量请求传递问题导致不同输出、影响推理速度以及结构需要改进的情况。

https://github.com/vllm-project/vllm/issues/5806
这是一个发布跟踪的issue，类型为需求追踪。该问题单涉及的主要对象是软件版本的发布进度跟踪。这个issue的目的是追踪v0.5.1版本的预计发布日期和相关的Pull Request（PR），提供了与发布相关的链接和注释。

https://github.com/vllm-project/vllm/issues/5805
这个issue属于需求提出类型，主要对象是vLLM项目的开发和规划。用户提出了vLLM在Q3 2024路线图中要实现的主要特性和目标计划，并邀请社区参与讨论和贡献。

https://github.com/vllm-project/vllm/issues/5804
这是一个关于bug报告的issue，主要涉及的对象是vllm库。由于缺少名为'libcudart.so.12'的共享对象文件导致了无法找到'rms_norm'属性，出现了AttributeError。

https://github.com/vllm-project/vllm/issues/5803
这是一个用户提出需求的Issue，主要涉及到vLLM logger在默认情况下禁用其他已存在的loggers，可能导致其他代码无法输出日志的问题。

https://github.com/vllm-project/vllm/issues/5801
这个issue是一个bug报告，主要涉及的对象是shm broadcast功能。由于当前块未准备好进行写入/读取时无法找到另一个块，导致了这个bug。

https://github.com/vllm-project/vllm/issues/5800
这是一个功能需求类型的issue，主要涉及集成测试和模型质量测试的需求。由于vLLM新增了更多适用于CI的节点，需要添加模型质量测试以确保内核和调度器的更改不会降低模型的准确性表现，并且保证不会破坏lmevalharness的集成。

https://github.com/vllm-project/vllm/issues/5799
这个issue类型是功能提议，涉及的主要对象是DraftModelRunner，由于需要在模型执行过程中实现多个功能并行，导致需要引入DraftModelRunner来优化性能。

https://github.com/vllm-project/vllm/issues/5798
这个issue是bug报告类型，涉及的主要对象是FlexibleArgumentParser。该问题由于缺乏关于替换实际参数的Bug修复，导致了需要添加单元测试。

https://github.com/vllm-project/vllm/issues/5797
这是一个Bug报告，涉及到vllm中OpenAI API的模拟与shell_gpt OpenAI API调用不兼容的问题。

https://github.com/vllm-project/vllm/issues/5796
这个issue属于用户提出需求类型，主要涉及vllm的推理请求缓存设置。用户询问是否需要启用缓存以及缓存保存位置，原因可能是希望了解如何利用缓存来处理类似的推理请求。

https://github.com/vllm-project/vllm/issues/5795
这个issue是一个Bugfix类型的问题，涉及的主要对象是FlexibleArgumentParser的参数处理。由于代码中对下划线的替换导致了参数解析错误的bug。

https://github.com/vllm-project/vllm/issues/5794
这是一个特性需求类的issue，主要涉及到对`compressed-tensors`支持`w8a16`模型进行扩展。由于需要支持`w8a16`模型并允许使用相同的内核，所以需要将支持的方案从`CompressedTensorsW4A16`更改为`CompressedTensorsWNA16`。

https://github.com/vllm-project/vllm/issues/5793
这是一个Bug报告，涉及的主要对象是在A10和A100/H100 GPU上使用GPTQ模型和marlin内核时遇到的质量响应差异。由于不同的GPU环境，导致在A10上的GPTQ模型质量表现异常，同时用户尝试了不同设置和模拟环境，但问题仍然存在。

https://github.com/vllm-project/vllm/issues/5792
这个issue属于性能问题，主要涉及到在具有张量并行性的1个设备上AllReduce等待时间过长。原因是张量字典的异步广播导致CUDA内核在不同时间启动，进而影响性能。

https://github.com/vllm-project/vllm/issues/5791
这是一个关于在MLPSpeculator上添加E2E测试的Issue，主要涉及测试和性能优化。

https://github.com/vllm-project/vllm/issues/5790
这是一个bug报告，主要涉及vllm在openai api server中使用tensor parallelism时出现错误。原因是设置的tensor_parallel_size大于了本地GPU的数量。

https://github.com/vllm-project/vllm/issues/5789
这是一个用户提出需求的issue，主要涉及文档页面的编辑功能，用户希望增加一个"建议编辑"按钮便于直接修改文档。

https://github.com/vllm-project/vllm/issues/5788
这个issue属于文档问题，需要添加"Phi-3-medium"到支持的模型列表中。

https://github.com/vllm-project/vllm/issues/5787
这是一个用户提出需求的issue，主要涉及到vllm的benchmark_latency.py脚本，用户在运行该脚本后发现生成的JSON文件过大，希望减小文件大小或获得分析工具推荐。

https://github.com/vllm-project/vllm/issues/5786
这个issue类型是bug报告，主要对象是使用qwen272binstructint4 gptq 部署在1张a100 上面时批量请求300个请求时报错了。由于什么原因导致了这种bug，需要进行进一步的排查和修复。

https://github.com/vllm-project/vllm/issues/5785
这是一个Bug报告，涉及的主要对象是vLLM引擎。由于一些潜在原因导致使用8xH100时初始化失败，可能是由于高内存使用导致进程被OOM杀手或者其他意外错误。

https://github.com/vllm-project/vllm/issues/5784
这个issue类型为bug报告，主要涉及的对象是vLLM模型。由于内存耗尽导致vLLM崩溃，无法自动恢复健康状态，并需要在prometheus中展示相关信息。

https://github.com/vllm-project/vllm/issues/5783
这是一个建议类型的issue， 主要涉及到文档问题。由于缺少下载模型的警告，用户可能会遇到下载模型的问题或困惑。

https://github.com/vllm-project/vllm/issues/5782
这是一个bug报告，主要涉及的对象是使用multi-node offline distributed inference时VLLM在推理的最后几个样本上卡住，可能是由于某种原因导致的bug。

https://github.com/vllm-project/vllm/issues/5781
这是一个需求类型的issue，主要涉及的对象是vllm项目中的cuda kernels，由于使用32位整数作为索引可能导致溢出问题，建议使用64位整数以提高安全性。

https://github.com/vllm-project/vllm/issues/5780
这个issue属于用户提出需求类型，主要涉及问题是启动后缺少特定方法，可能是由于配置不正确导致的。

https://github.com/vllm-project/vllm/issues/5779
这是一个Bug报告，涉及的主要对象是使用vllm和ray进行分布式推理时遇到的问题。由于使用Gloo进行全网格连接时出现了报错，可能由于代码实现或配置问题导致了这个Bug。

https://github.com/vllm-project/vllm/issues/5778
这是一个bug报告，涉及的主要对象是vllm模型服务。由于多线程测试完vllm部署的模型服务报错，导致出现异常报错信息。

https://github.com/vllm-project/vllm/issues/5777
这是一个bug报告，涉及对象为vllm中的deploy internLM2功能。由于响应内容过长导致内存耗尽，出现了无法停止的bug。

https://github.com/vllm-project/vllm/issues/5776
这是一个关于提交模型的新内容的issue，用户需要填写PR描述以及符合相应的提交要求。

https://github.com/vllm-project/vllm/issues/5775
这个issue类型是提出需求，主要涉及vLLM架构的分布式推断，由于当前架构不够灵活，导致添加推测解码和协作进程时出现困难。

https://github.com/vllm-project/vllm/issues/5774
这是一个Bugfix类型的issue，主要涉及的对象是vllm引擎中处理大批量请求时出现的错误。这个问题的根源是在处理20k到100k个异步请求时会出错，需要通过重构代码使用Semaphore来限制每次最多处理10k个并发请求，并增加了一个tqdm进度条来显示处理进度。

https://github.com/vllm-project/vllm/issues/5773
这个issue属于需求提出类型，主要对象是支持Qwen2系列模型，由于当前版本不支持Qwen2系列模型的大小，导致无法运行该系列模型。

https://github.com/vllm-project/vllm/issues/5772
这个issue是一个Bugfix类型的问题，涉及到vLLM的Phi3Vision和torchvision依赖。由于缺少图像调整功能，导致传递动态形状图像输入时出现错误。

https://github.com/vllm-project/vllm/issues/5771
这是一个Bug修复类型的issue，主要涉及的对象是vLLM中的URL验证和安全性功能。这个问题由于缺乏URL验证导致了SSRF漏洞，需添加URL验证以确保只有受信任的域名可以用于获取图片，从而提高安全性。

https://github.com/vllm-project/vllm/issues/5770
这个issue是一个模型相关的问题，涉及的主要对象是Chameleon MixedModal EarlyFusion Foundation Models。问题的原因是需要为Chameleon模型添加文本到文本推理的支持，以便生成文本数据。

https://github.com/vllm-project/vllm/issues/5769
这个issue属于bug报告类型，主要涉及到`scales_shard_indexer`这个组件。该bug可能由于缺乏`shard_id`的范围检查导致程序运行异常。

https://github.com/vllm-project/vllm/issues/5768
这是一个用户提出需求的issue，主要对象是vLLM的shm broadcast功能。由于固定chunk size导致共享内存空间浪费，需要支持变长对象以提高灵活性。

https://github.com/vllm-project/vllm/issues/5767
这是一个Bug报告，主要涉及到vllm中的图像处理模块phi3vision和torchvision的依赖问题，导致在调用API时出现了不同图片尺寸支持的错误。

https://github.com/vllm-project/vllm/issues/5766
这是一个关于功能需求的issue，主要涉及的对象是CI（持续集成），提出的问题是关于启用分布式测试的功能。

https://github.com/vllm-project/vllm/issues/5765
这个issue是关于性能优化的需求，主要涉及的对象是在KV缓存模型中实现了特定优化功能的MultiStepWorker。由于未清除序列 ID 列表中的序列 ID，导致在序列结束时无法将其移除，需要找到一种方法来在序列结束时通知并移除这些序列 ID。

https://github.com/vllm-project/vllm/issues/5764
这个issue是关于bug报告，涉及到`request.tool_choice`无法完全耗尽的问题，可能导致一些消息未定义。

https://github.com/vllm-project/vllm/issues/5763
该issue属于向开发团队提出新模型支持的请求，涉及的主要对象是DeepSeekCoderV2模型。用户提出该请求的原因是他们在将模型端口到vLLM时缺乏经验。

https://github.com/vllm-project/vllm/issues/5762
这是一个用户提出需求的issue，主要涉及的对象为设置vllm中--max-logprobs参数。由于用户在使用不同的LLM时需要手动设置max-logprobs值为词汇表的大小，希望能有简便的方式将max-logprobs默认设置为词汇表的大小。

https://github.com/vllm-project/vllm/issues/5761
这个issue类型为文档修改，主要对象是TPU安装过程中的依赖错误。这个问题由于缺少安装提示导致用户常见的依赖错误。

https://github.com/vllm-project/vllm/issues/5760
这是一个Bug报告，涉及主要对象为TPU执行者。由于某种原因导致了TPU后端出现错误，当前PR提供了对此错误的简单修复。

https://github.com/vllm-project/vllm/issues/5759
这是一个bug报告，主要涉及vllm运行推理时出现断言错误的问题，可能是由于NCCL通信库初始化失败导致的。

https://github.com/vllm-project/vllm/issues/5758
这个issue是一个需求问题，主要涉及安装vllm时内存不足导致OOM错误。

https://github.com/vllm-project/vllm/issues/5757
这是一个类型为迁移/协调的issue，主要涉及AWS CI模板的移除。

https://github.com/vllm-project/vllm/issues/5756
这是一个bug报告，涉及的主要对象是vllm/entrypoints/openai/run_batch.py文件。这个问题是因为之前的一个bug修复（#Bugfix AsyncLLMEngine hangs with asyncio.run）导致的不必要的错误信息输出。

https://github.com/vllm-project/vllm/issues/5755
这个issue类型是用户提出需求，主要涉及到在跨节点广播时添加消息队列，驱动进程移出tp 0 worker以提高效率，并需要添加多节点测试。

https://github.com/vllm-project/vllm/issues/5754
这是一个bug报告，涉及的主要对象是vllm的core模块下的分布式部分。由于未详细解释、解决了竞态条件和删除了读取端的初始化检查，导致了这个issue。

https://github.com/vllm-project/vllm/issues/5753
这个issue是用户提出需求，请求支持对bnb预量化模型的读取。该问题涉及的主要对象是vllm。

https://github.com/vllm-project/vllm/issues/5752
这是一个需求提出类型的issue，主要涉及了支持稀疏KV缓存框架的问题。原因是当前大型模型推断中，KV缓存占用了重要的GPU内存，减小KV缓存的大小是一项重要的改进方向。

https://github.com/vllm-project/vllm/issues/5751
这个issue类型是需求提出，主要对象是支持稀疏KV缓存框架。由于当前大型模型推断中KV缓存占据了大部分GPU内存，因此减小KV缓存的大小是一个重要的改进方向。

https://github.com/vllm-project/vllm/issues/5750
这个issue是一个功能需求，主要涉及的对象是对比sparseml和vllm模型在压缩张量上的准确性，用户希望通过对比验证sparseml生成的token ids是否位于vllm生成的top_n logprobs之中。

https://github.com/vllm-project/vllm/issues/5749
这个issue是一个Bug报告，涉及Detokenizer.detokenize_incrementally函数在计算长度时导致显著延迟的问题。

https://github.com/vllm-project/vllm/issues/5748
这是一个bug报告，涉及的主要对象是`RayTokenizerGroupPool`，由于tokenizer actors死亡导致的bug。

https://github.com/vllm-project/vllm/issues/5747
这个issue是关于Bug报告，主要涉及vLLM。用户遇到了一个类型错误，可能是由于代码中出现了'int'对象缺少'expansion'属性而导致的。

https://github.com/vllm-project/vllm/issues/5746
这个issue是关于bug报告，涉及到VLLM项目中的custom allreduce test在A100上失败的问题，导致是因为一个来自pull request的合并竞争。

https://github.com/vllm-project/vllm/issues/5745
该issue属于用户提出需求类型，主要涉及文档中对支持硬件的描述，用户希望添加一个表格以详细说明各内核支持的硬件情况。

https://github.com/vllm-project/vllm/issues/5744
这个issue是一个Bug修复类型的issue，主要涉及的对象是GPU在处理Cutlass Gemm Op时无法满足Shared Memory要求导致的运行时错误，并提出了在这种情况下回退到宽松Gem Op的解决机制。

https://github.com/vllm-project/vllm/issues/5743
这个issue属于Bug修复类型，涉及主要对象为ROCm的vLLM。由于环境变量数值改变后无法立即获取新值，导致torch.cuda.device_count()方法需要等待rocm6.2/pytorch2.4正式发布才能获取新数值。

https://github.com/vllm-project/vllm/issues/5742
这个issue是一个功能需求的提出，主要涉及到前端的持续使用统计功能，由于需要实现`Continuous streaming of UsageInfo`，导致了需要添加这一功能。

https://github.com/vllm-project/vllm/issues/5741
这个issue是关于功能需求的，主要对象是vLLM项目不支持非x86架构的CPU推理，由于需求支持PowerPC架构，提出了需要在项目中考虑非x86平台的问题。

https://github.com/vllm-project/vllm/issues/5740
这是一个bug报告，主要涉及到KeyError异常，由于可能是由于缺少/错误的key导致的。

https://github.com/vllm-project/vllm/issues/5739
这个issue是关于修复lora请求的bug。

https://github.com/vllm-project/vllm/issues/5738
这是一个Bug报告，涉及的主要对象是在AWS Inferentia实例上使用VLLM进行模型推理时出现的问题。由于环境配置或其他原因导致模型推理中出现错误，用户寻求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/5737
这是一个用户请教问题的issue，主要涉及的对象是vllm项目中的encoder-only模型，用户想了解vllm是否支持类似BAAI/bgem3的embedding模型，并如何使用。由于用户对vllm中的RAG模型的支持情况以及如何使用某些特定模型存在疑问，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/5736
这个issue是关于软件Bug报告的，涉及的主要对象是vllm下的一个Python脚本。由于缺少Ray模块导致了无法进行多节点推理，用户提出了安装Ray模块的问题。

https://github.com/vllm-project/vllm/issues/5735
这个issue属于PR提交类型的问题，涉及的主要对象是在Github上维护的vLLM项目。原因可能是由于markdown渲染不起作用导致html标签被使用。

https://github.com/vllm-project/vllm/issues/5734
这是一个用户提出需求的类型，涉及主要对象为Langchain，由于Langchain无法访问OpenAIEmbeddings导致用户请求支持或寻求帮助。

https://github.com/vllm-project/vllm/issues/5733
该issue属于功能需求类型，主要涉及支持LoRA中的偏置设置，并且由于需要支持调整偏置来使adapters与vLLM兼容。

https://github.com/vllm-project/vllm/issues/5732
这是一个Bug报告，涉及的主要对象是PyTorch运行环境。由于环境中同时出现asyncio.exceptions.CancelledError和asyncio.exceptions.TimeoutError，可能导致程序出现异常并中断运行。

https://github.com/vllm-project/vllm/issues/5731
这是一个bug报告，主要涉及的对象是在部署模型时遇到CUDA错误导致的运行时错误。

https://github.com/vllm-project/vllm/issues/5730
这个issue是一个bug报告，主要涉及的对象是vllm的api_server.py文件。由于命令中的参数错误导致了无法识别的参数，进而导致了无法执行功能调用的BUG。

https://github.com/vllm-project/vllm/issues/5729
这是一个Bug报告，涉及的主要对象是VLLM项目的执行环境。此问题导致了`asyncio.exceptions.CancelledError`和`asyncio.exceptions.TimeoutError`异常的出现。

https://github.com/vllm-project/vllm/issues/5728
该issue属于技术讨论类型，涉及主要对象为vllm库中的attention_kernels.cu文件中的命名定义。由于对NUM_ELEMS_PER_THREAD的理解不清晰，导致用户提出了关于线程组加载元素数量以及shared memory分配等方面的问题。

https://github.com/vllm-project/vllm/issues/5727
这是一个Bug报告类型的Issue，涉及的主要对象是在两台V100服务器上运行Distributed Inference and Serving Vllm时出现错误。导致该问题的原因是PyTorch版本与CUDA版本不匹配。

https://github.com/vllm-project/vllm/issues/5726
这是一个bug报告，主要涉及到使用vllm中的qwen2-1.5b-gptq-in4模型在单个A10080G GPU上进行多进程部署时出现OOM错误的问题。

https://github.com/vllm-project/vllm/issues/5725
这个issue是关于bug修复，涉及到Hugging Face在生成文本时需要传递eos_token_id的问题，由于没有传递eos_token_id导致继续生成文本而导致测试失败。

https://github.com/vllm-project/vllm/issues/5724
这个issue是关于bug修复的，主要涉及的对象是vllm项目中的不同模型，由于之前模型没有统一支持 `tie_word_embeddings` 功能，导致需要在启用该功能时显示清晰的错误信息。

https://github.com/vllm-project/vllm/issues/5723
这个issue是一个提出需求的请求，主要涉及到vLLM在在线RL训练过程中的权重更新问题，用户希望实现在运行时同步权重，以便加速训练过程。

https://github.com/vllm-project/vllm/issues/5722
该issue属于用户提出需求类型，主要涉及对象为新模型Nemotron-4-340B，用户询问是否计划支持该模型以及是否已经在进行相关工作。

https://github.com/vllm-project/vllm/issues/5721
该issue类型为需求提出，主要涉及的对象是支持Chameleon模型的集成。这个问题的原因是用户希望vllm项目能够支持Chameleon模型，但由于Chameleon模型与现有的vllm框架存在一些区别，因此需要进行调整和改进。

https://github.com/vllm-project/vllm/issues/5720
这是一个Bug报告，主要对象是Offsets计算错误导致了问题。

https://github.com/vllm-project/vllm/issues/5719
这是一个用户提出需求类型的issue，主要对象是分布式功能。

https://github.com/vllm-project/vllm/issues/5718
这个issue是一个功能需求，主要涉及的对象是vllm项目的前端。这个需求由于用户希望支持在参数命名中同时使用下划线和短横线，因此提出了添加FlexibleArgumentParser来处理这一要求。

https://github.com/vllm-project/vllm/issues/5717
这个issue是一个[Kernel][CPU] Add Quick `gelu` to CPU类型的问题，涉及的主要对象是vLLM的CPU XPU Ops。由于markdown渲染不起作用，所以使用了原始HTML来呈现问题描述和PR检查列表。

https://github.com/vllm-project/vllm/issues/5716
这是一个用户提出需求的RFC（Request For Comments），主要涉及LLM类在资源清理方面的改进。原因是当前项目中存在资源未释放的问题，用户需求是能够在上下文管理器中自动释放资源。

https://github.com/vllm-project/vllm/issues/5715
这个issue是一个bug报告，涉及到CUDA版本检查的问题，由于CUDA版本不匹配可能导致错误。

https://github.com/vllm-project/vllm/issues/5714
这是一个Bug修复的Issue，主要涉及vLLM在加载模型时内存不足导致的问题，用户提出希望在OOM时提供更有帮助的提示信息。

https://github.com/vllm-project/vllm/issues/5713
这是一个Bug报告类型的Issue，主要涉及Mixtral-8x7B-Instruct-v0.1模块，由于CUDA设备内核图像无效导致了"Triton Error [CUDA]: device kernel image is invalid"的错误。

https://github.com/vllm-project/vllm/issues/5712
这个issue类型是用户提出问题，主要涉及async_llm_engine.py中出现的错误信息，用户请求帮助理解该错误信息的含义。

https://github.com/vllm-project/vllm/issues/5711
这是一个bug报告，主要涉及的对象是在A100 GPU上运行Llama 3 70B，导致尝试分配160MiB内存失败。

https://github.com/vllm-project/vllm/issues/5710
这是一个bug报告，主要涉及对象是vLLM中的QwenVL和QwenVLChat模型。由于vLLM在处理仅包含文本输入的情况下出现KeyError错误，需要支持QwenVL模型以便能够运行仅包含文本输入的情况。

https://github.com/vllm-project/vllm/issues/5709
这是一个功能需求的issue，涉及到对HuggingFace的`ChatCompletionRequest`类添加模板相关参数，由于需要控制`apply_chat_template`的行为和模板，因此引发了这个问题。

https://github.com/vllm-project/vllm/issues/5708
这是一个功能需求类型的问题，主要涉及的对象是vLLM的`UsageInfo`，用户提出持续流式传输`UsageInfo`的需求，以解决在性能评估过程中遇到的“账务”问题。

https://github.com/vllm-project/vllm/issues/5707
这是一个关于bug的报告，主要涉及vllm启动的openai api在进行对话时出现自问自答情况的问题。

https://github.com/vllm-project/vllm/issues/5706
这是一个[改进建议]类型的issue，主要涉及"top1_proposal output tensor initialization"。由于使用`torch.tensor().expand()`操作进行初始化导致效率低下，用户建议采用`torch.full()`和`torch.zero()`操作来改善初始化效率和代码可读性。

https://github.com/vllm-project/vllm/issues/5705
这是一个用户提出需求的issue，主要对象是支持torch==2.3.1版本。由于当前代码不支持该版本，导致用户需要请求相关支持。

https://github.com/vllm-project/vllm/issues/5704
这是一个用户提出需求的问题，主要涉及到在NVIDIA多型号的GPU上实现vllm在多卡上的并行运行的功能。这个问题的原因是目前vllm本地部署模型加载时只使用了一块RTX3060显卡，无法成功运行9B的模型，用户希望能够在多个卡上实现并行运行。

https://github.com/vllm-project/vllm/issues/5703
这个issue是一个Doc类型的bug报告，主要涉及文档构建过程中出现的错误。由于https://github.com/vllmproject/vllm/pull/5614引入了重复的显示目标名称错误，导致文档构建时出现问题。

https://github.com/vllm-project/vllm/issues/5702
这是一个关于性能改进的问题。主要讨论对象是在vllm中使用flash attention算法的选择及其性能考量。该问题探讨选用flash_attn_varlen_func()而非其他实现版本的理由，以及是否带来更优的速度表现。

https://github.com/vllm-project/vllm/issues/5701
这是一个bug报告，主要涉及文档内容的错误。由于RST文件不允许有两个targets，用户提出需要重新编辑文档并更改目标名称。

https://github.com/vllm-project/vllm/issues/5700
这是一个bug报告，主要涉及到vllm库的参数输入错误导致的异常情况。

https://github.com/vllm-project/vllm/issues/5699
这是一个bug报告，主要涉及vllm环境下的输出文本与top_logprobs不匹配的问题，可能是由于请求参数设置不当导致的。

https://github.com/vllm-project/vllm/issues/5698
这个issue是一个优化建议，主要涉及sampler的优化，由于top_p=1且top_k>0时，排序不是必要的且不如topk高效。

https://github.com/vllm-project/vllm/issues/5697
这是一个Bug报告，涉及主要对象是VLLM（Very Large Language Model）。由于Engine迭代超时，导致出现此TimeoutError。

https://github.com/vllm-project/vllm/issues/5696
这是一个Bug报告，主要涉及的对象是在运行Triton_flash_attention.py时出现的错误警告。由于无法初始化NVML导致的警告，可能是由于环境配置或依赖的问题引起的。

https://github.com/vllm-project/vllm/issues/5695
这是一个Bug报告类型的issue，主要涉及到vllm项目中blocksparse attention的缺陷，具体是由于缺少Scipy库导致的错误。

https://github.com/vllm-project/vllm/issues/5694
这个issue类型为bug报告，主要涉及的对象是在vllm项目中使用A100时出现的GPU数量限制问题。

https://github.com/vllm-project/vllm/issues/5693
这是一个Bug报告，主要涉及的对象是vllm项目中的vision chat completion功能。由于某种原因导致输出结果中的提示信息出现混乱，从而导致了该Bug报告。

https://github.com/vllm-project/vllm/issues/5692
这是一个bug报告，主要涉及的对象是vllm库中的推理错误。原因是在使用大型模型'/data/models/Qwen/Qwen257BA14B'时出现了报错。

https://github.com/vllm-project/vllm/issues/5691
这是一个需求类型的issue，主要涉及使用MQA kernel进行目标模型验证，但由于添加cuda图支持和其他困难导致了一些实现上的困难。

https://github.com/vllm-project/vllm/issues/5690
这是一个关于安装问题的bug报告。用户提到了vllm在Mac上安装失败，询问ray和torch的版本与vlllm兼容性问题。这可能是由于xformers不支持PEP 517构建导致的安装问题。

https://github.com/vllm-project/vllm/issues/5689
这个issue是属于用户提出需求类型，主要对象是在ci和distributed方面加入关于自定义allreduce的测试。

https://github.com/vllm-project/vllm/issues/5688
这是一个bug报告，涉及到vllm项目中前端的一个命令行参数问题。由于参数使用不当导致CLI参数无法正确解析，需要修复该问题。

https://github.com/vllm-project/vllm/issues/5687
这个issue是一个Bug报告，涉及对象是vllm项目。导致这个问题的原因可能是"illegal memory access"，导致服务器在运行中突然失败。

https://github.com/vllm-project/vllm/issues/5686
这是关于如何在使用 vllm 时传递上下文以便模型能够仅使用该上下文进行机器翻译的问题，类型为用户提出需求。

https://github.com/vllm-project/vllm/issues/5685
这是一个关于改进GPU分布式执行器的issue，主要涉及到对xpu设备的重构和改进。

https://github.com/vllm-project/vllm/issues/5684
这是一个Bugfix类型的issue，涉及到Phi3vision的支持初始化问题，由于`sampling_param`传递错误导致产生混乱/不完整的答案。

https://github.com/vllm-project/vllm/issues/5683
这是一个用户提出需求的类型，主要对象是要添加一个名为"facebook/multi-token-prediction"的新模型，原因是为了加速模型推理过程。

https://github.com/vllm-project/vllm/issues/5682
这是一个关于vLLM在CPU上运行推荐设置的问题，用户询问在具有96个核心的双插槽服务器上如何分配核心以实现高性能。

https://github.com/vllm-project/vllm/issues/5681
这个issue类型是其他类型，主要对象是conftest。由于只有"Just a few minor changes"这句话描述不够详细，无法确定具体是什么样的修改或问题，可能是用户提出了一些小改进或建议。

https://github.com/vllm-project/vllm/issues/5680
这是一个bug报告类型的issue，涉及的主要对象是vllm项目中的依赖项`tqdm`，因为`llm.py`中使用了它但未被正确包含在依赖项中，导致问题单作者提出需要将其添加到依赖项中。

https://github.com/vllm-project/vllm/issues/5679
这个issue是关于bug报告，主要涉及对象是在使用nvidiancclcu11时开启cuda_graph导致RunTimeError:NCCL error。

https://github.com/vllm-project/vllm/issues/5678
这是一个bug报告，涉及的主要对象是`flash_attn_cuda.varlen_fwd`函数在启用前缀缓存时可能会输出错误结果，可能由于某些请求导致问题发生。

https://github.com/vllm-project/vllm/issues/5677
这是一个bug报告，涉及vLLM在使用`quantization=gptq_marlin`参数时出现错误。

https://github.com/vllm-project/vllm/issues/5676
这个issue是关于代码质量的改进，提出了对"launch_tgi_server.sh"脚本进行参数化。

https://github.com/vllm-project/vllm/issues/5675
这是一个Bug报告类型的Issue，主要涉及vllm中模型量化使用多GPU设置时出现的错误。这可能是由于输入大小与量化权重形状不对齐导致的。

https://github.com/vllm-project/vllm/issues/5674
这是一个关于安装问题的bug报告，主要涉及vLLM项目中pip install -e安装方式失败的情况。原因可能是环境配置或代码修改导致安装失败。

https://github.com/vllm-project/vllm/issues/5673
这是一个功能需求类型的issue，主要涉及设置文件目录的创建，但目前尚未实现。

https://github.com/vllm-project/vllm/issues/5671
这是一个关于安装问题的bug报告，主要涉及vllm构建过程中无法找到Python导致的错误。

https://github.com/vllm-project/vllm/issues/5669
这是一个bug报告，涉及的对象是关于使用`fork`默认设置以处理多进程的问题。原因是在创建cuda上下文后，使用`fork`不安全，导致需要在用户代码中使用`if __name__ == "__main__"`。

https://github.com/vllm-project/vllm/issues/5668
这个issue是一个BugFix类型的报告，主要涉及到modelscope版本1.15.0的问题，由于ignore_file_pattern未正确修复，导致了相关功能异常。

https://github.com/vllm-project/vllm/issues/5667
这是一个bug报告，涉及vllm库中的一个命令行参数命名方式不符合惯例引发的问题。

https://github.com/vllm-project/vllm/issues/5666
这是一个Bug报告类型的Issue，涉及VLLM中启用前缀缓存但推断速度未提升的问题。

https://github.com/vllm-project/vllm/issues/5665
This is a bug report for the lack of a fully sharded layer for `QKVParallelLinearWithLora`, causing issues with certain LLMs such as `Baichuan7B`.

https://github.com/vllm-project/vllm/issues/5664
该issue类型为需求报告，主要涉及对象为vllm类LLM的CPU推理支持。由于目前的class LLM不支持在CPU上运行推理，用户在尝试在CPU上运行时遇到了问题，希望了解是否有替代方案或解决方法。

https://github.com/vllm-project/vllm/issues/5663
这是一个Bug报告，主要涉及到VLLM下的Qwen2-72B-Instruct-gptq-int4模型，由于在并发测试过程中，无论是并发限制还是10个并发限制，输出结果都会重复。

https://github.com/vllm-project/vllm/issues/5662
这是一个bug报告，主要涉及vllm在使用openai completion接口时出现500 Internal Server Error的问题。

https://github.com/vllm-project/vllm/issues/5660
这个issue属于bug报告类型，主要涉及vllm在使用两张a100启动时报错，可能是由于上下文长度过少导致的问题。

https://github.com/vllm-project/vllm/issues/5659
这是一个bug报告，涉及到并发请求干扰GREEDY响应的问题。

https://github.com/vllm-project/vllm/issues/5658
这是一个bug报告，主要涉及VLLM模型在环境16bf上运行速度过慢的问题，用户想要达到至少50 tokens/second的推理速度。

https://github.com/vllm-project/vllm/issues/5657
这是一个bug报告，涉及到Ray distributed backend不支持通过ModelRegistry APIs导入out-of-tree models。这导致了在单节点环境下无法使用out-of-tree模型。

https://github.com/vllm-project/vllm/issues/5656
该issue属于功能需求添加类型，主要涉及Model中添加FP8 KV cache for Qwen2，用户提出了扩展FP8 KV缓存加载功能的需求。

https://github.com/vllm-project/vllm/issues/5655
这是一个Bug报告，涉及的主要对象是ModelRegistry类，由于get_supported_archs方法没有包含OOT模型，导致使用LLM引擎时无法加载OOT模型。

https://github.com/vllm-project/vllm/issues/5654
这是一个Bug报告，涉及到AsyncLLMEngine的使用问题。由于Python的Bug导致AsyncLLMEngine在使用asyncio.run时出现挂起现象。

https://github.com/vllm-project/vllm/issues/5653
这是一个bug报告类型的issue，主要涉及的对象是vLLM不同版本下IfEval指标的性能差异。导致不同版本之间性能差异的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/5652
该issue类型为功能需求，涉及主要对象为支持CPU inference with VSX PowerPC ISA。原因是需要提供基本的CPU支持供PowerPC（Power9和POWER10）使用。

https://github.com/vllm-project/vllm/issues/5651
这个issue属于Bug报告类型，主要对象是vLLM，由于multiprocessing KeyError导致了vLLM崩溃。

https://github.com/vllm-project/vllm/issues/5650
这个issue是功能增强类型，涉及主要对象是`CompressedTensorsW8A8StaticTensor` scheme。由于之前的scheme只支持per-tensor weight quantization，用户提出扩展为支持channelwise quantization的需求。

https://github.com/vllm-project/vllm/issues/5649
这是一个特性需求，主要涉及到OpenAI-Compatible Tools API + Streaming for Hermes & Mistral models。由于OpenAI-style tool calling的支持尚未添加，用户提出了对应的功能需求。

https://github.com/vllm-project/vllm/issues/5648
这个issue类型是用户提出需求，涉及主要对象是在AWS CI模板中为A100 GPU添加一个队列，导致用户希望能够使用A100 GPU来运行在`testpipeline.yaml`中定义为`gpu: a100`的步骤。

https://github.com/vllm-project/vllm/issues/5646
此issue类型为技术改进，主要涉及移除nvidia runtime docker基础镜像，由于不希望依赖该镜像中的任何内容，试图修复与nvidia runtime镜像相关的问题。

https://github.com/vllm-project/vllm/issues/5645
这是一个bug报告，主要涉及OpenBLAS在高负载长时间运行时出现错误的问题。原因可能是缺少设置`OPENBLAS_NUM_THREADS`导致的。

https://github.com/vllm-project/vllm/issues/5643
这个issue是关于Bugfix的，主要涉及的对象是w8a8 benchmarks for int8 case。这个问题是由于int8 benchmark在CC中被忽略，导致了渲染markdown时出现问题。

https://github.com/vllm-project/vllm/issues/5642
这个issue是一个Bugfix类型的问题，涉及到CUDA版本检查和警告抑制。问题产生的原因是在CUDA 12.5上编译时出现大量警告，通过修复CUDA_VERSION检查来解决这些问题。

https://github.com/vllm-project/vllm/issues/5641
这个issue类型是bug报告，涉及的主要对象是vllm在MI300x上使用ROCm6.1 docker时产生garbage，因为当前的cmake没有使用正确的HIP target或gpu架构。

https://github.com/vllm-project/vllm/issues/5640
这是一个bug报告，主要涉及的对象是vllm在NVIDIA Jetson AGX Orin上的安装问题。由于torch.version.cuda返回为`none`导致出现`Unknown runtime environment`错误，用户正在寻求有关此安装问题的帮助。

https://github.com/vllm-project/vllm/issues/5639
这是一个bug报告，主要涉及了关于sampling和repetition penalties导致的不一致行为问题。问题产生的原因是在处理请求时未提前计算do_penalties导致了数据结构错误，影响了抽样行为的正常运作。

https://github.com/vllm-project/vllm/issues/5638
这是一个bug报告，涉及到在使用docker环境下请求max-num-seqs with speculation时出现错误。由于请求次数乘以speculationtokens超过了maxnumseqs，导致引擎抛出错误的问题。

https://github.com/vllm-project/vllm/issues/5637
这是一个bug报告，涉及的主要对象是vllm库。由于新版本中处理进程方式的变化，导致在设置`tensor_parallel_size`大于1时程序提示`RuntimeError`错误。

https://github.com/vllm-project/vllm/issues/5636
这是一个环境信息汇报类的Issue，主要对象是模型运行速度较慢，可能由于环境配置或参数设置不当导致。

https://github.com/vllm-project/vllm/issues/5635
这是一个bug报告，涉及的主要对象是vllm中无法在cuda版本==12.1的情况下成功运行docker镜像0.5.0。由于新版本vllm（0.4.3之后）无法正常工作，可能是因为指定的cuda版本要求与实际环境中的cuda版本不匹配引起的。

https://github.com/vllm-project/vllm/issues/5634
这个issue类型是用户提出需求，主要涉及的对象是vLLM的RESTful API server。由于设定参数"--max-num-seqs"值为默认值256或大于6时无法生效，可能是由于参数设定方式或API server限制导致。

https://github.com/vllm-project/vllm/issues/5633
该issue是一个Bugfix类型的报告，主要涉及vLLM的核心功能，由于之前的代码存在bug，在处理chunked prefill scheduler时可能导致内存交换使用过多，在大量请求n>=2的情况下。

https://github.com/vllm-project/vllm/issues/5632
这是一个用于测试CI的PR，不是一个bug报告或用户需求。

https://github.com/vllm-project/vllm/issues/5631
该问题类型为安装问题，主要涉及对象是vllm在Windows操作系统上的安装。原因可能是用户在尝试安装时遇到了困难或错误。

https://github.com/vllm-project/vllm/issues/5629
这是一个功能需求类型的issue，主要涉及benchmark_latency.py脚本，由于长上下文可能导致模型初始化失败，用户提出添加max-model-len参数限制GPU内存使用。

https://github.com/vllm-project/vllm/issues/5628
这是一个Bugfix类型的issue，涉及到vLLM中Phi-3 Long RoPE scaling的实现。由于vLLM与Huggingface transformers在Phi3微调上输出的logits存在差异，主要原因是vLLM中位置嵌入实现的偏离。

https://github.com/vllm-project/vllm/issues/5626
这是一个关于性能问题的用户需求类型的Issue，涉及主要对象是GPU KV cache的利用率。用户怀疑GPU KV缓存使用率过低导致生成吞吐速度慢，希望了解如何增加GPU KV缓存使用率以提高生成吞吐速度。

https://github.com/vllm-project/vllm/issues/5625
这是一个修复bug的issue，涉及到移除无用的import。由于导入了transformers logging但未使用，因此需要修复这个问题。

https://github.com/vllm-project/vllm/issues/5624
这是一个功能改进类的issue，主要涉及的对象是CI模板。由于现在默认模板应该使用`testtemplateaws.j2`，因此需要移除原先的模板`testtemplate.j2`。

https://github.com/vllm-project/vllm/issues/5623
这个issue类型是CI/Build相关问题，涉及到更新Pytest Marker for VLMs。由于VLMs数量增加，需要更新pytest marker来反映测试套件的目的。

https://github.com/vllm-project/vllm/issues/5622
这是一个 bug 报告 issue，主要涉及到 VLLM 模型无法推断在 MiniCPM 训练后或 AWQ 上的问题。由于 MiniCPM 训练后的推理会出错，并且 AWQ 模型目前无法在 VLLM 上运行，因此需要解决这个问题。

https://github.com/vllm-project/vllm/issues/5621
这是一个bug报告，主要对象是vLLM中的部分模型，由于部分模型在使用`tie_word_embeddings`设置时无法正确加载权重导致生成无意义的输出。

https://github.com/vllm-project/vllm/issues/5620
这是一个拼写错误修复的issue，主要涉及到代码中的拼写错误。原因是代码中存在拼写错误导致了问题。

https://github.com/vllm-project/vllm/issues/5619
这是一个bug报告，主要涉及的对象是TCP通信的IP获取问题，由于无法正确获得IP地址导致了通信问题。

https://github.com/vllm-project/vllm/issues/5618
这是一个bug报告类型的issue，主要涉及格式化脚本 `format.sh`，由于拼写错误导致被标记为问题。

https://github.com/vllm-project/vllm/issues/5616
这个issue是关于CI(持续集成)的问题，涉及到添加Intel GPU(XPU)到持续集成管道中。 原因是XPU持续集成未能被触发，导致项目中的Intel GPU(XPU)推断后端无法被正确测试。

https://github.com/vllm-project/vllm/issues/5615
这个issue是关于性能基准测试中存在相同名称的不同指标的问题，要求遵循特定的PR标题前缀规范。由于命名不一致导致性能指标解析困难。

https://github.com/vllm-project/vllm/issues/5614
这个issue类型是文档更新，涉及的主要对象是Dockerfile相关的文档内容。由于当前文档不易于新用户查找相关信息，需要进行小的导航改进，清理部分内容冗余和嵌套的项目符号。

https://github.com/vllm-project/vllm/issues/5613
这是一个bug报告，涉及的主要对象是vllm中的custom_all_reduce.cuh文件，由于传递了无效参数导致了错误。

https://github.com/vllm-project/vllm/issues/5612
这个issue为bug报告，涉及的主要对象是p2p测试功能。由于两个进程对p2p能力存在分歧导致的bug，用户提出不希望测试失败影响代码。

https://github.com/vllm-project/vllm/issues/5611
这是一个Bug报告，涉及的主要对象是模型支持的嵌入类型。由于模型配置信息与期望的类型不匹配，导致在托管特定模型时发生内部服务器错误。

https://github.com/vllm-project/vllm/issues/5610
这是一个用户提出需求的类型，主要涉及到Release pipeline的设置和构建发布wheels的优化，由于Dockerfile的调整，引发了需要重新设置Release pipeline的问题。

https://github.com/vllm-project/vllm/issues/5609
该issue属于功能需求类型，主要涉及的对象是VLLM模型的生成过程。由于用户希望能够在生成过程中控制初始token，以减少模型输出中的随机性，因此提出了这个功能需求。

https://github.com/vllm-project/vllm/issues/5608
该issue类型为功能新增，主要涉及对象为scheduler。导致这个问题的原因是需要在scheduler方法add_seq_group中添加用户信息作为输入参数，并在OpenAI Completions API中使用时支持用户信息传入。

https://github.com/vllm-project/vllm/issues/5607
这是一个Bug报告，主要涉及GREEDY responses在发送并发请求时变得不一致的问题。这可能是由于并发采样请求影响到GREEDY responses导致的。

https://github.com/vllm-project/vllm/issues/5606
这是一个bug报告，涉及的主要对象是NVRTC。由于NVRTC有时会将系统语言环境重置为ANSI_X3.41968，导致了这个问题。

https://github.com/vllm-project/vllm/issues/5605
这是一个用户提出需求的issue，主要涉及的是如何在调度程序中获取用户信息。用户希望能够基于用户创建调度策略，但目前调度程序无法获取用户标识符，希望能够找到其他方法来获取这些信息。

https://github.com/vllm-project/vllm/issues/5604
这个issue是一个关于bug修复的问题，主要涉及到分布式训练中16个GPU的本地排列，导致了问题的发生。

https://github.com/vllm-project/vllm/issues/5603
这是一个功能需求类型的issue，主要涉及LoRA适配器在LRU缓存中的固定支持。

https://github.com/vllm-project/vllm/issues/5602
这个issue类型是优化性质的，主要涉及block_manager_v2和block_manager_v1的性能优化对比。原因是为了使block_manager_v2成为默认选项，并优化了在批处理中的Block对象分配/释放问题。

https://github.com/vllm-project/vllm/issues/5601
这是一个Bug报告，涉及的主要对象是在启动llama7b时遇到错误。由于缺少适当的内核，导致出现"RuntimeError: No suitable kernel. h_in=16 h_out=4096 dtype=Float out_dtype=Half"的错误提示。

https://github.com/vllm-project/vllm/issues/5600
这是一个需求提出的issue，主要对象是支持Qwen2 embedding，问题由于当前最佳嵌入模型AlibabaNLP/gteQwen27Binstruct在使用嵌入端点时返回错误，用户希望能支持除E5mistral外的其他嵌入模型。

https://github.com/vllm-project/vllm/issues/5599
这个issue是一个bug报告，主要对象是vLLM的parallel config。原因是缺少了一个配置项导致了markdown渲染无法正常工作。

https://github.com/vllm-project/vllm/issues/5598
这是一个用户提出需求的issue，主要涉及VLLM项目中的配置参数设置问题。造成该问题的原因是默认参数限制导致无法满足用户对并行VLLM作业提交和集成的需求。

https://github.com/vllm-project/vllm/issues/5596
这是一个Bug报告，涉及的主要对象是GPTQ-Marlin kernel。由于特定参数组合（`group_size=32`, `desc_act=True`, `tp=4`）导致了GPTQ Marlin kernel发生非法内存访问并出现错误行为。

https://github.com/vllm-project/vllm/issues/5595
这个issue属于模型改进类型，涉及到Phi3 rope scaling type的命名问题。原因在于之前的命名方式混乱，需要进行更正。

https://github.com/vllm-project/vllm/issues/5594
这是一个bug报告，主要涉及的对象是vllm在NVIDIA上部署时受到了numpy2.0升级的影响，导致环境无法运行。

https://github.com/vllm-project/vllm/issues/5593
这是一个Bug报告，涉及的主要对象是`cpu_worker.py`文件，由于代码中`CPUWorker`对象缺少`cpu_cache`属性导致了错误。

https://github.com/vllm-project/vllm/issues/5592
这是一个bug报告，涉及到文档中的格式问题，导致了图表的轻微错位可能会让用户产生困惑。

https://github.com/vllm-project/vllm/issues/5591
这是一个类型为[Model]的issue，涉及的主要对象是将`CLIPVisionModel`从`transformers`移植到`LlavaForConditionalGeneration`和`LlavaNextForConditionalGeneration`。

https://github.com/vllm-project/vllm/issues/5590
这个issue是关于Bug报告，主要涉及的对象是vllm/executor/ray_gpu_executor.py中的代码。由于sorted(gpu_ids)操作导致GPU ID的顺序不正确，在使用NVIDIA HGX A100平台进行模型推断时会出现NCCL错误。

https://github.com/vllm-project/vllm/issues/5589
这是一个bug报告，该问题涉及的主要对象是"InternVLChatConfig"对象。由于PyTorch版本不兼容或者相关类缺少属性导致了" 'InternVLChatConfig' object has no attribute 'num_attention_heads'"这个bug。

https://github.com/vllm-project/vllm/issues/5588
这是一个关于使用AutoTokenizer进行基准测试服务的问题，涉及到vLLM项目中的一个GitHub issue。

https://github.com/vllm-project/vllm/issues/5587
这是一个Bug报告，主要涉及到vllm的安装问题。由于NumPy 2.0版本发布导致的破坏性更改，用户在运行vllm时可能会遇到`ModuleNotFoundError: No module named 'numpy.lib.function_base'`错误。

https://github.com/vllm-project/vllm/issues/5586
这个issue是一个Feature请求，主要涉及vLLM的benchmark script和组件的解耦。用户提出需求将`get_tokenizer`函数提取到`backend_request_func`中，避免依赖vLLM组件的问题。

https://github.com/vllm-project/vllm/issues/5585
该issue类型为用户提出需求，并询问如何在使用vllm时指定GPU。该问题涉及主要对象为vllm框架及其API服务器。用户提出问题的原因是不知道如何在vllm中集成特定模型以在GPU上运行推理。

https://github.com/vllm-project/vllm/issues/5584
这个issue类型是优化建议，主要涉及的对象是通过重复使用LogicalTokenBlock.token_ids来提高性能。

https://github.com/vllm-project/vllm/issues/5583
这是一个硬件相关的问题，涉及的主要对象是CPU后端，因为缺乏markdown渲染功能，所以使用了原始的HTML显示内容。

https://github.com/vllm-project/vllm/issues/5582
这是一个bug报告类型的issue，主要涉及限制numpy版本。原因是numpy 2.0的发布引入了许多破坏性更改，导致了与该项目构建相关的问题。

https://github.com/vllm-project/vllm/issues/5581
这是一个关于文档更新的需求，主要涉及多节点调试和崩溃调试方面的帮助提示。

https://github.com/vllm-project/vllm/issues/5580
这是一个用户提出需求的issue，主要对象是VLLM前端。由于目前加载权重时无法显示模型的峰值内存使用，用户希望能添加这一功能，以便更好地监控模型的内存使用情况。

https://github.com/vllm-project/vllm/issues/5579
这是一个Bug报告类型的Issue，主要涉及Mixtral8x7B在4 x L40环境下出现高延迟的问题，用户怀疑可能与缺少NVLink有关。

https://github.com/vllm-project/vllm/issues/5578
这是一个Bug报告，涉及的主要对象是VLLM项目中的Chunked Prefill Scheduler。这个问题的原因是发送多个n>=2请求时会填满CPU KV缓存，特别是当启用了分块预填充时，引起了对虚拟内存的过度使用。

https://github.com/vllm-project/vllm/issues/5577
这是一个bug报告类型的issue，涉及的主要对象是CUDA capability check，由于条件判断反转导致了一些量化测试被禁用。

https://github.com/vllm-project/vllm/issues/5576
这是一个bug报告，涉及的主要对象是vllm安装过程中出现的torch版本冲突。这个问题可能是由于torch版本冲突导致的安装失败。

https://github.com/vllm-project/vllm/issues/5575
这个issue是一个PR提交问题，涉及主要对象为cuda vs non-cuda decode测试，由于markdown渲染不起作用，可能导致PR描述内容不能正确显示。

https://github.com/vllm-project/vllm/issues/5574
这是一个用户提出需求的issue，主要涉及支持非AVX512的vLLM构建和测试。原因是为了使vLLM能够用AVX2构建并进行简单测试。

https://github.com/vllm-project/vllm/issues/5572
这是一个用户提出的关于如何在VLLM中使用Langchain进行RAG目的的issue，主要涉及到VLLM模型初始化和调用过程中出现的错误类型。

https://github.com/vllm-project/vllm/issues/5571
这是一个[CI/Build]类型的issue，主要涉及性能基准测试结果的可读性问题，通过调整结果内容和展示方式来提升可理解性。

https://github.com/vllm-project/vllm/issues/5570
这是一个bug报告，该问题涉及的主要对象是VLLM工具中的逻辑自动选择分布式执行器后端。这个问题是由于在检查是否在Ray placement group中运行时调用`ray.util.get_current_placement_group()`会有副作用，导致即使默认使用多进程分布式后端时也会启动Ray集群。

https://github.com/vllm-project/vllm/issues/5569
这是一个Bug报告，涉及主要对象为bitsandbytes quantization feature，由于最新的bitsandbytes量化功能，导致官方Llama38BInstruct产生垃圾。

https://github.com/vllm-project/vllm/issues/5568
这是一个改进性的issue，主要涉及的对象是OpenAI Completions API。由于之前仅测试了批处理文本数据，现在需要验证是否逻辑处理批处理文本和批处理token IDs。

https://github.com/vllm-project/vllm/issues/5567
这是一个性能问题的issue，主要涉及VLLM(batch的大小)的性能表现，用户提出在增加batch时没有增加生成吞吐量的问题。

https://github.com/vllm-project/vllm/issues/5566
这是一个bug报告，主要涉及使用Marlin内核的GPTQ模型进行推理时出现错误。原因可能是命令行参数错误或API调用错误导致的。

https://github.com/vllm-project/vllm/issues/5565
该issue属于用户提出需求类型，主要涉及到vLLM版本控制和开发版本管理的问题。由于当前的版本控制方式导致开发版本与发布版本等同，在热修复和补丁发布等方面存在困难，用户希望提出新的版本控制方案进行讨论和改进。

https://github.com/vllm-project/vllm/issues/5564
该issue是一个Bug报告，涉及主要对象为LoRA Adapter loading speed。升级到vllm 0.5.0后，出现了LoRA适配器加载速度显著下降的问题。

https://github.com/vllm-project/vllm/issues/5563
这是一个bug报告，涉及的主要对象是vllm的speculative decoding server，在运行过程中出现了数值转换错误导致程序崩溃。

https://github.com/vllm-project/vllm/issues/5562
这个issue是关于修复w8a8基准测试并添加Llama-3-8B的内容，属于Bug修复类型。这个问题涉及到GPU上直接缩放的Cutlass内核以及Llama38B的形状。这个问题发生的原因是之前的基准测试出现了问题，需要修复以及新增Llama-3-8B的形状。

https://github.com/vllm-project/vllm/issues/5561
这个issue类型是性能优化建议，涉及主要对象是vLLM的autoregressive proposal方法，由于sampler在执行时浪费了时间和计算资源导致性能不佳。

https://github.com/vllm-project/vllm/issues/5560
这个issue是关于添加bias epilogue支持到`cutlass_scaled_mm`的Kernel功能修改。

https://github.com/vllm-project/vllm/issues/5559
这是一个[Kernel]类型的issue，涉及到为IBM granite模型的13b系列添加punica维度的问题。

https://github.com/vllm-project/vllm/issues/5558
这是一个bug报告，主要涉及Pallas后端，发生的bug是由于在CC([Core][Distributed]中引入的错误导致代码重复，需要修复。

https://github.com/vllm-project/vllm/issues/5557
这是一个用户提出需求的类型的issue，主要涉及了如何实现通过KV缓存传输实现分理预填（disaggregated prefilling）。这个问题涉及了如何在不同vLLM实例之间转移KV缓存的需求。

https://github.com/vllm-project/vllm/issues/5556
这是一个bug报告，主要涉及vllm库在使用tensorparallel 4时出现的错误，导致出现PyO3 Runtime的PanicException异常。

https://github.com/vllm-project/vllm/issues/5555
这是一个需求提出类型的issue，主要涉及的对象是amd，用户提出需要将ccache添加到项目中以减少每次运行重新编译导致长达40分钟的构建时间。

https://github.com/vllm-project/vllm/issues/5554
这个issue属于bug报告类型，主要涉及OpenAI batch response格式的错误。导致这个问题的原因是响应主体应该被嵌套到另一个对象中。

https://github.com/vllm-project/vllm/issues/5553
这是一个文档修改类型的issue，主要涉及的对象是将Cerebrium作为一个集成选项添加到项目中。原因是markdown渲染不起作用，因此需要在此处使用原始的html代码。

https://github.com/vllm-project/vllm/issues/5552
这是一个关于重构以整合控制平面通信的[RFC]，主要涉及到Worker和ModelRunner类，在多GPU控制平面通信方面有改进的需求。

https://github.com/vllm-project/vllm/issues/5551
该issue为测试请求，不是bug报告。主要涉及的对象为CI的功能。由于需要对CI进行测试，用户提交了这个PR来请求CI的功能验证，但请求不要合并此PR。

https://github.com/vllm-project/vllm/issues/5550
这个issue属于Bug报告类型，主要涉及prompt_logprobs参数设置导致OOM错误，可能是由于设置该参数引起了内存溢出问题。

https://github.com/vllm-project/vllm/issues/5549
这个issue类型是一个优化建议，主要涉及持续集成（CI）构建过程中的资源利用问题。

https://github.com/vllm-project/vllm/issues/5548
这是一个用户提出需求的类型，该问题单涉及的主要对象是文档。由于缺少具体内容，用户请求将ZhenFund添加为赞助商。

https://github.com/vllm-project/vllm/issues/5547
该issue是关于bug报告的，主要涉及到CUDA环境配置问题，导致出现"RuntimeError: CUDA error: no kernel image is available for execution on the device"的错误。

https://github.com/vllm-project/vllm/issues/5546
这是一个bug报告，涉及到测试中的flaky test，用户修复了与CUDA设备计数相关的问题。

https://github.com/vllm-project/vllm/issues/5545
这是一个bug报告，涉及vllm下的llava-v1.6-mistral-7b-hf项目，用户遇到了jinja模板处理错误导致的问题。

https://github.com/vllm-project/vllm/issues/5544
这个issue是一个关于bug的报告，涉及的主要对象是`tests/distributed/test_utils.py`文件。由于不确定assert行是否需要修改，导致了Distribute Tests过程中的错误。

https://github.com/vllm-project/vllm/issues/5543
这是一个bug报告，涉及到vLLM中的`enableprefixcaching`存在缓存问题导致完成不一致的情况。

https://github.com/vllm-project/vllm/issues/5542
这个issue类型为功能需求，主要涉及的对象是`CompressedTensorsW8A8DynamicToken` scheme，用户请求对其添加通道级量化支持以支持模型。

https://github.com/vllm-project/vllm/issues/5541
这是用户提出的一个关于“Feature”的需求，主要涉及到vllm模型在处理asymmetric tensor parallel时出现的问题以及可能的改进。

https://github.com/vllm-project/vllm/issues/5540
这是一个功能建议类型的issue，主要涉及的对象是在Mixtral模型中添加LoRA支持。用户提出了增加对GPTQ和AWQ量化Mixtral模型支持的需求。

https://github.com/vllm-project/vllm/issues/5539
这是一个bug报告，涉及到Vllm 0.3.0版本的异常输出问题。由于使用了错误的vllm容器，导致出现了奇怪的输出。

https://github.com/vllm-project/vllm/issues/5538
这是一个关于禁止在使用lora时使用分块预填充的问题，属于杂项类型。用户反馈无法支持此功能。

https://github.com/vllm-project/vllm/issues/5537
这是一个bug报告，涉及VLLM的CUDA illegal memory access错误。由于vllm版本升级和启用前缀缓存，导致这个错误的出现。

https://github.com/vllm-project/vllm/issues/5536
这是一个测试CI的PR，不是bug报告，主要涉及持续集成系统相关内容。

https://github.com/vllm-project/vllm/issues/5535
这是一个Bug报告类型的Issue，主要涉及的对象是Mixtral 8x7B Instruct的FP8量化模型。由于FP8量化模型在推理时性能非常慢，导致生成吞吐量远低于预期。

https://github.com/vllm-project/vllm/issues/5534
这是一个性能提升的建议，不涉及bug报告，主要涉及使用vllm.attention.ops.triton_flash_attention替换flash_attn package。用户提出此建议是因为他的GPU过旧，无法安装flash_attn package。

https://github.com/vllm-project/vllm/issues/5533
这是一个用户需求相关的issue，主要涉及vllm中`enablechunkedprefill`参数的使用问题。用户想了解在使用`enablechunkedprefill`时与`maxnumseqs`、`maxnumbatchedtokens`和`maxmodellen`参数之间的限制，并询问是否可以同时使用`enablechunkedprefill`和`enableprefixcaching`。

https://github.com/vllm-project/vllm/issues/5532
这是一个bug报告，涉及到vLLM中的from_lora_tensors()函数在使用mp作为--distributed-executor-backend时执行速度非常缓慢的问题。原因可能是使用mp导致从LORA张量加载数据速度变慢，严重影响了多个LORA推理的效率。

https://github.com/vllm-project/vllm/issues/5531
这是一个Bug报告，涉及的主要对象是在vLLM v0.4.3及更高版本中，在张量并行情况下调用list_loras()会导致系统挂起。bug的原因可能是在v0.4.3中进行的调度优化改进。

https://github.com/vllm-project/vllm/issues/5530
这个issue属于CI 测试类型，主要涉及到代码集成。原因可能是为了验证代码变更的正确性，避免引入错误。

https://github.com/vllm-project/vllm/issues/5529
这个issue是一个bug报告，涉及的主要对象是CPU测试。由于未对LLaVANeXT进行排除，导致CPU测试脚本存在问题。

https://github.com/vllm-project/vllm/issues/5528
这是一个bug报告，主要涉及到p2p cache生成速度慢的问题，由于原因是生成过程中CPU进程生成速度较慢，导致用户反馈生成时间过长。

https://github.com/vllm-project/vllm/issues/5527
这是一个Bug报告，涉及的主要对象是在使用MOE模型、2卡推理时出现了AssertionError("Invalid device id")报错。这可能是由于设备id无效导致的问题。

https://github.com/vllm-project/vllm/issues/5526
这个issue是关于重组测试文件的工作，涉及到的主要对象是测试文件的组织结构。

https://github.com/vllm-project/vllm/issues/5525
这是一个bug报告，主要涉及对象是AsyncLLMEngine类的重复处理问题，由于不必要的重复调用导致了功能重复执行的问题。

https://github.com/vllm-project/vllm/issues/5524
这个issue是一个Bugfix类型的问题，主要涉及修复参数名称不正确导致的bug。

https://github.com/vllm-project/vllm/issues/5523
这是一个bug报告，主要涉及到vLLM版本0.5.0在加载模型速度较慢的问题。原因可能是与GPU内存使用和执行进程有关。

https://github.com/vllm-project/vllm/issues/5522
这是一个版本升级的issue，主要涉及的对象是软件的版本号。

https://github.com/vllm-project/vllm/issues/5521
这是一个bug报告，涉及到使用multiproc_gpu_executor时出现的关于shutdown错误的问题。

https://github.com/vllm-project/vllm/issues/5520
这个issue不是bug报告，是关于需求提出的。主要涉及vLLM的使用数据增强功能。产生该需求是因为需要收集vLLM不同功能的使用统计信息来了解效果和性能情况。

https://github.com/vllm-project/vllm/issues/5519
这是一个bug报告类型的issue，主要涉及的对象是vllm，由于测试时限制了可见设备数量而导致bug或者需要帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/5518
这个issue属于Bug报告类型，主要涉及添加基本正确性的GPU测试到4 GPU管道中。这个问题提出了在节点上运行GPU少于实际GPU数量时可能存在的问题。

https://github.com/vllm-project/vllm/issues/5517
这是一个bug报告类型的issue，主要涉及的对象是Cudagraph在A10G/L4 GPUs上的内存消耗过高。原因是由于使用较旧的cuda driver版本导致内存消耗较高，建议升级到cuda 12.4或更高版本来减少内存使用。

https://github.com/vllm-project/vllm/issues/5516
这个issue类型是bug报告，主要涉及的对象是CUDA kernels或其他compute kernels。导致这个bug的原因是开发者在检查广播标量和广播向量时出现了错误。

https://github.com/vllm-project/vllm/issues/5515
这是一个Misc类型的issue，主要是在vLLM中添加了用于记录cudagraph内存使用情况的日志。可能有助于调试内存问题。Bugfix。添加了用于记录cudagraph内存使用情况的日志，帮助调试内存问题。

https://github.com/vllm-project/vllm/issues/5514
这是一个[Kernel]类型的issue，主要涉及Cutlass int8 kernel的更新配置，报告了添加5个不同Gemmm形状区域的int8 Cutlass Kernel配置以及根据Gemmm问题形状将其分配到正确配置的问题。

https://github.com/vllm-project/vllm/issues/5513
这个issue类型是bug报告，主要涉及的对象是加载FP8 weights过程中出现的错误，由于环境问题导致无法成功加载模型。

https://github.com/vllm-project/vllm/issues/5512
这个issue属于bug报告，主要涉及到分布式系统的功能 `is_in_the_same_node`，由于存在一个错误导致用户无法正确判断两个节点是否在同一个节点上。

https://github.com/vllm-project/vllm/issues/5511
这个issue是一个bug报告，主要涉及的对象是vllm项目中的格式脚本（format.sh），因为在一个pull request（https://github.com/vllmproject/vllm/pull/5474）之后出现了问题需要修复。

https://github.com/vllm-project/vllm/issues/5510
这是一个Bug报告，涉及的主要对象是Docker Image版本0.5.0和0.4.3，由于与nvidiasmi的问题导致无法正常工作。

https://github.com/vllm-project/vllm/issues/5509
这是一个用户提出需求的类型，主要对象是关于文档管理的。由于缺少第四次会议的幻灯片，用户请求添加。

https://github.com/vllm-project/vllm/issues/5508
这个issue是关于禁用test_fp8.py的CI/Build类型问题，主要涉及到测试用例的稳定性问题。

https://github.com/vllm-project/vllm/issues/5507
这个issue是关于bug fix，主要涉及修正了`calib_size`单词拼写错误的问题。

https://github.com/vllm-project/vllm/issues/5506
这个issue是关于bug报告，主要涉及vllm项目中的CUTLASS FP8 kernels，问题出现在执行20b granite模型时发生了非法内存访问，导致需要回退到使用scaled_mm。

https://github.com/vllm-project/vllm/issues/5505
这是一个bug报告，主要涉及的对象是CUDA kernels，由于执行fp8 CUTLASS kernels时发生非法内存访问，导致执行20b granite模型时出错。

https://github.com/vllm-project/vllm/issues/5504
这是一个bug报告issue，主要涉及的对象是vLLM中的CUDA内核，由于执行fp8 CUTLASS内核时出现非法内存访问，导致需要回退到scaled_mm操作。

https://github.com/vllm-project/vllm/issues/5503
这是一个bug报告，主要涉及v0.5.0 docker image中使用bitsandbytes quantization时出现的ModuleNotFoundError错误。

https://github.com/vllm-project/vllm/issues/5502
这是一个用户提出需求的问题，涉及使用vLLM运行RAG系统，用户想通过vLLM运行llm模型并获取答案，但无法定义上下文和问题的提示模板。

https://github.com/vllm-project/vllm/issues/5501
这是一个Bug报告，主要涉及的对象是通过`pip install vllm`导入vllm模块时出现的错误。由于vllm不支持torch2.3.0版本，导致出现了无法导入vllm._C模块的错误。

https://github.com/vllm-project/vllm/issues/5499
这是一个Bug报告，主要涉及VLLM的部署环境配置，由于缺少必要的参数导致出现了RuntimeError。

https://github.com/vllm-project/vllm/issues/5498
这是一个bug报告类型的issue，主要涉及的对象是qwen272binstruct的lora适配器，由于找不到合适的内核，导致出现RuntimeError错误。

https://github.com/vllm-project/vllm/issues/5497
这个issue是关于Kernel的调优，针对Qwen257BA14B配置进行调优，通过增加吞吐量性能。原因是需要优化该配置的性能。

https://github.com/vllm-project/vllm/issues/5496
这个issue属于bug报告，涉及的主要对象是Qwen/Qwen2-72B-Instruct 128k server，由于PyTorch版本2.3.0+cu121与其他环境信息不兼容，导致服务器无法正常运行。

https://github.com/vllm-project/vllm/issues/5495
这是一个bug报告，主要涉及ray在tp>=2时无法正常工作的问题。

https://github.com/vllm-project/vllm/issues/5493
这是一个关于如何获取FP8缩放因子用于KV缓存的问题，属于用户寻求帮助类型的issue，主要涉及VLLM的FP8量化功能。这个问题出现的原因是文档未清楚说明如何获取KV缓存的缩放因子。

https://github.com/vllm-project/vllm/issues/5492
这是一个未填写PR描述的硬件相关issue，主要涉及添加FP8 kv缓存支持到Intel CPU。原因是未按照指定格式填写PR描述，导致渲染问题。

https://github.com/vllm-project/vllm/issues/5491
这个issue属于需求提案类型，主要涉及vLLM中加载和卸载多个LLM的API功能的添加。原因是为了提高资源利用率、扩展性和成本效益。

https://github.com/vllm-project/vllm/issues/5490
这是一个bug报告，该问题涉及vllm的接口调用问题，用户在调用接口时，不传递system参数会导致输出异常。

https://github.com/vllm-project/vllm/issues/5489
这个issue是一个用户提出需求的类型，主要对象是让vllm支持tensorrt编译之后的engine。原因可能是用户希望通过tensorrt加速但并发性没有vllm做得好。

https://github.com/vllm-project/vllm/issues/5488
这是一个用户需求类型的issue，主要涉及 VLLM 项目中的 latency benchmarking 功能，由于固定随机种子会导致输出结果随机性受到影响，用户提出启用随机种子选项的建议以增加配置灵活性。

https://github.com/vllm-project/vllm/issues/5487
这是一个Bug报告，主要涉及对象是torch._jit_internal模块。导致该问题的原因是循环导入导致无法正确引入名称 'boolean_dispatched'。

https://github.com/vllm-project/vllm/issues/5486
这是一个Bug报告，主要涉及的对象是从源代码构建一个项目时出现了意外的符号。这个问题可能是由于构建过程中的一些错误导致的。

https://github.com/vllm-project/vllm/issues/5485
这个issue是一个Bug报告，主要涉及的对象是VLLM项目中的Embedding模型。由于在4x V100机器上部署OpenAI兼容服务器时遇到问题，导致在设置tensorparallelsize为1时能够正常运行，但当发送嵌入请求时遇到错误信息。造成这种状况可能是由于代码中的某些问题，提出修复方法是简单地通过设置prompt_lens与_prepare_pooling，但用户对此感到困惑。

https://github.com/vllm-project/vllm/issues/5484
这是一个Bug报告，主要涉及NCCL出现连接超时问题，导致程序偶发性挂起。

https://github.com/vllm-project/vllm/issues/5483
这是一个功能需求类型的issue，主要涉及的对象是vLLM的前端代码。由于现有的实现方式无法支持使用非预编译的新中间尺寸的linear模块，用户提出了在注意力模块中仅支持使用linear模块的需求。

https://github.com/vllm-project/vllm/issues/5482
这是一个用户提出需求的issue，主要涉及的对象是 vllm 项目。

https://github.com/vllm-project/vllm/issues/5481
这是一个bug报告，涉及的主要对象是VLMs在CPU上的相关测试。由于CPU运行时存在特定问题，导致了一些与VLMs相关的bug，需要更新CPU测试以帮助捕捉这些问题。

https://github.com/vllm-project/vllm/issues/5480
这是一个用户提出需求的issue，主要涉及vllm库中的模型推理过程中使用bitsandbytes功能的集成问题。用户询问如何将bitsandbytes功能与vllm库集成以运行一个混合模型qlora的推理过程。

https://github.com/vllm-project/vllm/issues/5479
这是一个bug报告，主要涉及的对象是在8xL40S上加载Mixtral-8x22B-Instruct-v0.1-FP8时导致SIGSEGV错误，用户希望得到解决此问题的帮助。

https://github.com/vllm-project/vllm/issues/5478
这个issue是关于bug报告，涉及的主要对象是vllmflashattn模块，由于仍存在引发虚假out shape error的情况，因此需要撤销之前的更改，暂时等待适当修复。

https://github.com/vllm-project/vllm/issues/5477
这是一个bug报告，涉及vllm v0.4.3+的Ray worker中创建第二个NCCL Group的问题，导致NCCL errors during broadcast，作者寻求解决方案。

https://github.com/vllm-project/vllm/issues/5476
这是一个bug报告issue，主要涉及的对象是vllm-flash-attn==2.5.9.post1库。由于一个步骤漏掉导致了shape错误，需要修复。

https://github.com/vllm-project/vllm/issues/5475
这是一个bug报告，主要涉及多个Lora支持下使用前缀缓存时出现性能问题。原因可能是前缀缓存不能正确处理使用不同系统提示的不同Lora头导致的缓存失效。

https://github.com/vllm-project/vllm/issues/5474
这个issue类型是一个建议，提出了将dev requirements分为lint和test两部分的建议。原因是为了让安装lint requirements更容易。

https://github.com/vllm-project/vllm/issues/5473
这个issue类型是bug报告，涉及的主要对象是CUDA设备计数获取函数。该问题由于在设置`CUDA_VISIBLE_DEVICES`前调用`torch.cuda.device_count()`导致的环境变量未能正确更新所引发。

https://github.com/vllm-project/vllm/issues/5472
这个issue属于Misc类型，主要涉及到移除关于FP8 quantization的性能警告。由于FP8 quantization现在实现了良好的加速，因此移除了其性能警告。

https://github.com/vllm-project/vllm/issues/5471
这个issue是文档更新类型，涉及的主要对象是Tensorizer。原因是需要更新vLLM中关于Tensorizer的文档介绍和链接，以及引导用户查看示例脚本和使用指南。

https://github.com/vllm-project/vllm/issues/5470
该issue是一个Bugfix类型的issue，主要涉及的对象是scheduler.py文件中的一个拼写错误"requeset"应该改为"request"。这个bug是因为拼写错误导致markdown渲染不起作用而导致的。

https://github.com/vllm-project/vllm/issues/5469
这是一个用户提出需求的issue，涉及主要对象为上传wheel文件到Buildkite和S3 bucket。

https://github.com/vllm-project/vllm/issues/5468
这是一个Bug报告，涉及的主要对象是Python multiprocessing resource_tracker，导致了Benign error在多进程后端下报告该错误。

https://github.com/vllm-project/vllm/issues/5467
这个issue类型是用户提出需求，该问题涉及的主要对象是vLLM的OpenAI兼容服务器。由于当前系统不支持用户定义的额外请求参数被记录在日志中，用户提出希望能够将自定义的额外参数传递给服务器以便记录在日志中。

https://github.com/vllm-project/vllm/issues/5466
这是一个功能改进的issue，主要涉及到代码库中的量化测试配置及CUDA计算能力检查，通过优化代码以避免重复的检查操作。

https://github.com/vllm-project/vllm/issues/5465
这是一个bug报告，该问题涉及vllm在使用OpenAI兼容服务器时出现的运行时错误。

https://github.com/vllm-project/vllm/issues/5464
这是一个需求提出的issue，主要涉及CI/CD环境配置。由于需要新增AMD、Neuron和Intel测试，并将默认软失败关闭，可能是为了扩展CI测试范围和提高测试准确性。

https://github.com/vllm-project/vllm/issues/5463
这个issue是一个回退操作（Revert）类型的，主要涉及的对象是CI/Build，原因可能是添加了导致问题的配置选项。

https://github.com/vllm-project/vllm/issues/5462
这是一个bug报告，主要涉及的对象是 AttributeError，由于缺少提示导致报错。

https://github.com/vllm-project/vllm/issues/5461
这是一个Bug报告，涉及的主要对象是Torch2.3代码。由于缺少了'rms_norm'属性，导致出现了AttributeError错误。

https://github.com/vllm-project/vllm/issues/5460
这是一个bug报告，主要涉及的对象是gpt_bigcode模型，由于未正确考虑`c_attn`层的`input_scale`和`weight_scale`在Q/K/V分区之间的共享关系，导致无法加载FP8检查点，出现错误。

https://github.com/vllm-project/vllm/issues/5459
这是一个用户提出需求的issue，主要涉及到PagedAttention模块的head维度限制问题，用户想了解如何实现head维度也可以是8的倍数。

https://github.com/vllm-project/vllm/issues/5458
这是一个bug报告，涉及的主要对象是在`--tensor-parallel-size > 1`情况下出现错误。造成这个bug的原因可能是与PyTorch版本2.3.0和CUDA 11.5之间的不兼容。

https://github.com/vllm-project/vllm/issues/5457
这是一个bug报告类型的issue，主要涉及到vLLM在M2 Mac上安装时遇到的依赖问题，可能是由于Torch版本不兼容导致无法安装的情况。

https://github.com/vllm-project/vllm/issues/5456
这是一个bug报告，涉及到vLLM从源代码重新构建时旧的可执行文件未能清除/覆盖导致的问题。

https://github.com/vllm-project/vllm/issues/5455
这是一个bug报告，主要涉及的对象是测试。由于模型问题导致了测试无法通过。

https://github.com/vllm-project/vllm/issues/5454
这是一个Bug报告，主要涉及到vllm/entrypoints/api_server.py中的代码。由于'_C'对象缺少'rns_norm'属性，导致AttributeError错误。

https://github.com/vllm-project/vllm/issues/5453
这个issue是文档修复类型，涉及到vLLM项目中指定模型修订版本的参数更新和文档澄清。由于参数命名不够清晰和文档不一致导致用户难以理解或使用。

https://github.com/vllm-project/vllm/issues/5452
这是一个用户提出需求的issue，主要对象是支持CPU推断的AVX2 ISA。由于目前只支持AVX512 CPUs，用户提出需要支持AVX2 CPUs以扩大vLLM的应用范围和吸引更多开发者的参与。

https://github.com/vllm-project/vllm/issues/5451
这个issue是关于bug修复类型的，主要涉及的对象是CPU runner。原因是multi_modal_input格式错误导致了bug。

https://github.com/vllm-project/vllm/issues/5450
这是一个Bug报告，涉及的主要对象是vllm/vllmopenai:v0.5.0在k8s上部署时出现了内部断言错误，导致无法正常工作。

https://github.com/vllm-project/vllm/issues/5449
这个issue类型为用户提出需求，主要涉及的对象是vllm，在询问如何同时使用embedding model和LLM时提出了问题。

https://github.com/vllm-project/vllm/issues/5448
这是一个 bug 报告 issue，涉及主要对象为 vllm 库，由于某种原因导致出现 AttributeError: '_OpNamespace' '_C_cache_ops' object has no attribute 'reshape_and_cache_flash' 错误信息。

https://github.com/vllm-project/vllm/issues/5447
这是一个关于在Github上的vLLM项目中提出的添加Bert Embedding Model的issue。

https://github.com/vllm-project/vllm/issues/5446
这个issue类型为硬件相关的功能改进，主要涉及生成自定义激活操作，由于需要避免重新跟踪导致的重新编译，可能出现了markdown渲染问题。

https://github.com/vllm-project/vllm/issues/5445
这是一个bug报告，主要涉及的对象是使用multilora_inference.py调用qwen2-1.5b时出现的RuntimeError。问题是由于缺乏适当的内核导致的，需要寻找潜在的替代解决方案或修复。

https://github.com/vllm-project/vllm/issues/5444
这是一个bug修复类型的issue，涉及主要对象是MultiModalData。由于`TYPE_CHECKING for MultiModalData`存在问题，导致需要进行小修复。

https://github.com/vllm-project/vllm/issues/5443
这是一个Bug报告，主要涉及vllm/vllmopenai:v0.4.3在k8s上部署14b模型时出现AsyncEngineDeadError异常的问题。

https://github.com/vllm-project/vllm/issues/5442
这个issue是一个Bug报告，主要涉及对象是vLLM中避免在world size为1时进行warmup的问题。由于没有测试环境来验证vGPU+vLLM，提出可以避免在world size = 1时进行warmup，以解决此问题。

https://github.com/vllm-project/vllm/issues/5441
这是一个bug报告，主要涉及的对象是Qwen2 LoRA模型的punica维度支持问题。造成该问题的原因是punica尚未支持所有qwen2模型的中间大小。

https://github.com/vllm-project/vllm/issues/5440
这个issue是关于Bug报告，主要涉及的对象是程序运行环境。由于输入类型错误导致了 "TypeError: a bytes-like object is required, not 'str'" 这样的异常。

https://github.com/vllm-project/vllm/issues/5439
这是一个bug报告，涉及到resource_tracker中的注册错误，出现了UserWarning。由于添加调试代码后引发了此问题。

https://github.com/vllm-project/vllm/issues/5438
这是一个文档更新类型的issue，涉及更新调试文档的可读性，旨在改进调试提示的清晰度。由于当前的调试文档可读性较差，导致用户在调试过程中可能遇到困难，因此提出了这个问题以寻求改进。

https://github.com/vllm-project/vllm/issues/5437
这是一个需求类型的issue，主要涉及到LLaVA文档的更新。

https://github.com/vllm-project/vllm/issues/5436
这个issue是一个bug报告，涉及到vllm0.5.0版本中Outlines Integration的编译进度显示错误，用户询问为什么vllm仍然编译FSM而不是Guide。

https://github.com/vllm-project/vllm/issues/5435
这个issue是一个功能请求，主要涉及到在支持`24`稀疏度的`w4a16`模型中添加`CompressedTensors24`支持，由于需要支持稀疏量化并通过compressedtensors保存模型，导致需要调整相应的内核。

https://github.com/vllm-project/vllm/issues/5434
这是一个需求提出类型的issue，主要涉及的对象是vLLM模型库。用户在资源受限的环境中提出是否可以实现PagedAttention功能，由于模型参数无法容纳在CPU内存中，用户希望探讨如何将其拓展至更适合的环境。

https://github.com/vllm-project/vllm/issues/5433
该issue属于用户提出需求类型，主要涉及vllm中的sampling parameters，由于当前sampling parameters不支持guided grammar参数，导致用户无法在LLM Engine中使用guided grammar。

https://github.com/vllm-project/vllm/issues/5432
这个issue是一个[ Misc ]类型的PR，主要涉及到在`compressedtensors`中修复一些建议性问题。

https://github.com/vllm-project/vllm/issues/5431
这是一个用户提出需求的issue，问题涉及到vLLM中对于RecurrentGemmaForCausalLM模型的支持问题，因为当前不支持该模型架构导致取值错误。

https://github.com/vllm-project/vllm/issues/5430
该issue类型为用户调试提示，涉及的主要对象是长时间等待模型下载或加载，在此情况下用户可能会认为系统卡死。

https://github.com/vllm-project/vllm/issues/5428
这个issue是一个Bugfix类型的报告，涉及到vllm项目中的arg_utils.py文件中关于lora_dtype数值类型的修复。该问题可能是由于前一个pull request中的变更引发了问题，需要通过本次修复来解决。

https://github.com/vllm-project/vllm/issues/5427
这是一个文档错误修复的issue，涉及主要对象为代码示例的错误。原因是代码示例中存在错误，需要修正文档内容。

https://github.com/vllm-project/vllm/issues/5426
这个issue类型是用户提出需求，讨论vGPU在CI测试中的可行性和成本效益，以及测试软件支持范围的需求。

https://github.com/vllm-project/vllm/issues/5425
此issue属于需求提出类型，主要对象为vLLM的前端界面。由于缺乏"Prefill Speed"（输入速率）信息导致用户反馈性能评估不够全面。

https://github.com/vllm-project/vllm/issues/5424
这是一个bug报告，涉及的主要对象是VLLM库中的CUDA内存分配过程。导致CUDA内存不足的bug是由于在设置`prompt_logprobs=1`时，使用了较大的batch_size，从而触发了torch.cuda.OutOfMemoryError错误。

https://github.com/vllm-project/vllm/issues/5423
这是一个用户提出需求的类型，主要涉及到guided decoding & logit processor API的改进。由于API不完整且存在多个问题，导致了需要支持batch/async logit processing以及改善logit processing performance的问题。

https://github.com/vllm-project/vllm/issues/5422
这个issue是关于进行更新到ROCm 6.1、改进Dockerfile、修复测试的问题，主要涉及到硬件、AMD以及构建文档。由于升级后一些配置不正确或代码逻辑缺陷，导致了一些bug和持续集成构建相关的问题。

https://github.com/vllm-project/vllm/issues/5420
这是一个Bug报告，该问题涉及 Automatic Prefix caching not working 的问题，主要是由于 PyTorch 版本为 2.1.2+cu121 并且 CUDA 可用，导致自动前缀缓存无法正常工作。

https://github.com/vllm-project/vllm/issues/5419
这是一个需求提出的issue，主要涉及到持续集成（CI）流程中对使用sccache来构建Docker镜像的需求。

https://github.com/vllm-project/vllm/issues/5417
这是一个bug报告，涉及的主要对象是vllm中的GLM-4V模型部署，问题可能由于某些原因导致了KeyError错误。

https://github.com/vllm-project/vllm/issues/5416
这是一个bug报告，主要涉及vLLM在不同context lengths下消耗视频内存不同的问题。

https://github.com/vllm-project/vllm/issues/5415
这是一个关于如何在Hugging Face下载模型时指定特定分支的使用问题，主要涉及到下载模型时如何选择特定分支。用户提出问题的原因是无法通过文档中提供的方式成功指定要使用的分支。

https://github.com/vllm-project/vllm/issues/5414
这个issue是一个功能需求，涉及主要对象是针对vllm下的draft model和target model，用户提出了支持不同尺寸的张量并行度的需求。

https://github.com/vllm-project/vllm/issues/5413
这是一个用户提出需求/请教问题的issue，主要涉及MoE层在Jamba block中的疑惑。用户疑惑MoE层的具体定义及相关的数学或图表，希望能够指导或分享Jamba所采用的准确论文，并寻求潜在的替代/修复方法。

https://github.com/vllm-project/vllm/issues/5412
这是一个bug报告类型的issue，该问题涉及的主要对象是vLLM Speculative Decoding中使用FlashInfer作为后端时出现的bug。这个bug的原因是在执行`ModelRunner`类中`model_runner.py`文件的`_prepare_model_input`函数期间，`paged_kv_indices`和`paged_kv_indptr`的错误输入元数据导致了不正确的输出结果产生。

https://github.com/vllm-project/vllm/issues/5411
这是一个bug报告，涉及vLLM中evict v2 出现长上下文长度时的问题。这个bug导致了在为16K prompt分配新块时evict()的成本较高，解决方法是更新update()中的move_to_end函数来保持块的最近最少使用顺序。

https://github.com/vllm-project/vllm/issues/5410
这是一则关于文档修复的issue，主要涉及vLLM的CI（持续集成）功能。由于缩进错误导致markdown渲染无法工作，因此在文档部分使用了原始的HTML内容。

https://github.com/vllm-project/vllm/issues/5409
这个issue类型是用户提出需求，涉及的主要对象是添加调试提示。由于缺乏调试信息，用户提出了关于调试hang/crash的帮助请求。

https://github.com/vllm-project/vllm/issues/5408
这是一个需求类型的issue，涉及主要对象是 Worker 和 ModelRunner 类。由于目前的代码结构使得控制平面通信与模型执行代码混杂，导致难以替换控制平面机制，同时也难以提高多GPU设置的性能。

https://github.com/vllm-project/vllm/issues/5407
这是一个bug报告，主要涉及的对象是使用 Qwen272BInstructionGPTQInt4 模型的 Openai 服务器请求问题。由于该模型在 Vllm 上接收请求的方式与其他模型不同，导致了请求被分散接收的问题。

https://github.com/vllm-project/vllm/issues/5406
这是一个用户提出需求的issue， 主要对象涉及vllm模型的hidden states，由于提取hidden states的功能不支持，用户希望增加类似于transformers output_hidden_states的功能。

https://github.com/vllm-project/vllm/issues/5405
这是一个bug报告，涉及主要对象为vllm服务。导致这个问题的原因是nccl导致vllm服务启动时间过长。

https://github.com/vllm-project/vllm/issues/5404
这是一个bug报告，主要涉及生成文本时设置不同参数导致结果不同的问题。

https://github.com/vllm-project/vllm/issues/5403
这是一个文档错误修复的issue，主要对象是vLLM中的markdown渲染功能。原因可能是由于缺少逗号导致的markdown渲染无法正常工作。

https://github.com/vllm-project/vllm/issues/5402
这个issue类型是Bug报告，主要涉及的对象是TorchSDPA。由于TorchSDPA缺乏设备断言，可能导致在NVIDIA GPU上使用出错。

https://github.com/vllm-project/vllm/issues/5401
该issue为bug报告，主要涉及的对象是vllm的Kernel，在CUDA 12.5版本下编译时出现了大量关于mma.sp警告消息。原因是之前的修复导致在较早版本的CUDA中构建失败，这个PR添加了对CUDA版本的检查，在CUDA 12.5及以后的版本中选择ordered_metadata版本的指令。

https://github.com/vllm-project/vllm/issues/5400
这是一个用户提出需求的issue，主要涉及到如何使用最新版本的预测解码。原因可能是用户需要了解如何使用预测解码功能或者文件中缺少相关文档。

https://github.com/vllm-project/vllm/issues/5399
这是一个性能优化的issue，主要涉及到使用共享内存广播Python对象的协议。

https://github.com/vllm-project/vllm/issues/5398
这是一个Bug报告，问题是关于vllm项目中的lora_dtype数值类型在arg_utils.py中修复的。这个bug可能导致参数值类型错误。

https://github.com/vllm-project/vllm/issues/5397
这个issue是一个bug报告，涉及的主要对象是VLLM项目中的arg_utils.py文件。这个问题是由于参数`lora_dtype`缺少值类型，导致引发了bug。

https://github.com/vllm-project/vllm/issues/5396
这是一个关于优化Kernel的issue，主要涉及对FP8 quantize kernel的改进，通过优化来提高速度。

https://github.com/vllm-project/vllm/issues/5395
这是一个bug报告，该问题涉及的主要对象是Llama3输出受限于大约10个标记。原因可能是输出被截断或限制导致无法完整显示。

https://github.com/vllm-project/vllm/issues/5394
这个issue属于测试类，主要对象是AMD CI基础设施，由于是用于测试基线AMD CI用途，因此不能被合并。

https://github.com/vllm-project/vllm/issues/5393
这是一个对先前 soft_fail 设置进行修改的 issue，主要涉及到 AMD 测试，由于构建稳定和排队时间可控，因此决定撤销先前的 soft_fail 设置。

https://github.com/vllm-project/vllm/issues/5392
这个issue类型是bug报告，涉及的主要对象是Buildkite agent路径。由于在检查路径存在性时没有正确获取`buildkiteagent`的路径，导致了构建失败。

https://github.com/vllm-project/vllm/issues/5391
这个issue是[Kernel]类型，主要涉及的对象是cutlass kernels，原因是需要将epilogues与kernel分离以支持添加新的epilogues。

https://github.com/vllm-project/vllm/issues/5390
这个issue是一个特性请求，主要涉及在`cutlass_scaled_mm_dq`内核中添加融合偏置加操作。原因是为了优化W8A8线性层与偏置的性能，并且目前仅支持fp8路径，而不支持int8路径。

https://github.com/vllm-project/vllm/issues/5389
这个issue类型是文档更改，涉及到VLLM的环境变量设置；用户提出了使用`VLLM_TARGET_DEVICE`替代`VLLM_BUILD_WITH_NEURON`的问题。

https://github.com/vllm-project/vllm/issues/5388
这个issue属于文档更新类型，主要涉及FP8 W8A8量化特性的文档补充。

https://github.com/vllm-project/vllm/issues/5387
该issue属于功能增强类别，主要涉及ModelOpt FP8检查点的支持，由于需要将ModelOpt键的名称转换为vLLM在FP8量化模式下识别的键名称。

https://github.com/vllm-project/vllm/issues/5386
这是一个关于文档问题的issue，主要对象是赞助商列表的排序。原因可能是希望赞助商列表按照字母顺序进行排序，提升文档的可读性。

https://github.com/vllm-project/vllm/issues/5385
这是一个需求提出的Issue，主要涉及添加对`w4a16`模型的支持。

https://github.com/vllm-project/vllm/issues/5384
这个issue类型是版本更新，主要对象是项目的软件版本号。

https://github.com/vllm-project/vllm/issues/5383
这个issue是关于文档问题的。它提到了当前VLM支持存在的限制，并需要在VLM页面上添加相关文档说明。

https://github.com/vllm-project/vllm/issues/5382
这是一个用户请教问题的类型的issue，涉及主要对象是vllm中的PagedAttention和Cuda Graphs，用户提出了关于如何在PagedAttention中使用Cuda Graphs的问题。

https://github.com/vllm-project/vllm/issues/5381
这是一个关于升级codespell版本的CI/Build类型的issue，涉及的主要对象是vLLM项目。由于升级后的版本允许有内联注释忽略，而使用markdown渲染时出现问题，需要使用原始的HTML代码解决。

https://github.com/vllm-project/vllm/issues/5380
这个issue是关于bug修复的，主要涉及LLaVA-NeXT模块。原因可能是在CC模块中初步支持LLaVA-NeXT后出现的问题需要进行跟进修复。

https://github.com/vllm-project/vllm/issues/5379
这是一个特性提议的issue，主要涉及OpenVINO中的vLLM后端的性能评估和比较。

https://github.com/vllm-project/vllm/issues/5378
这是一个新增功能需求的 issue，主要涉及到为 `compressed_tensors` 模型添加对 `w4a16` 支持。这个需求是由于需要支持 quantized `w4a16` 模型，并通过 compressedtensors 进行保存。

https://github.com/vllm-project/vllm/issues/5377
这个issue类型为功能需求，涉及主要对象是OpenVINO vLLM backend。由于OpenVINO提供更好的性能以及支持现代vLLM特性，用户提出需要在vLLM中集成OpenVINO backend以提高效率和功能性。

https://github.com/vllm-project/vllm/issues/5376
这是一个bug报告，涉及的主要对象是vllm 0.4.3，由于CUDA error导致了内存访问错误的问题。

https://github.com/vllm-project/vllm/issues/5375
这是一个 bug 报告，主要涉及的对象是 vllm 模块。由于缺少特定模块的安装导致出现 ModuleNotFoundError 错误。

https://github.com/vllm-project/vllm/issues/5374
这个issue类型是代码贡献，主要涉及vLLM API服务器中关于图像输入类型的检查。这个问题由于对`VisionLanguageConfig`的使用导致需要在代码中添加检查，为了优化开发者体验而提出改进。

https://github.com/vllm-project/vllm/issues/5373
该issue是一个[Frontend][Misc]类型的问题，主要涉及在VLM API服务器中强制像素值作为输入类型的更改。这个问题可能由于未在启动vLLM API服务器时进行图像输入类型检查而导致。

https://github.com/vllm-project/vllm/issues/5372
这是一个修复拼写错误的问题，主要对象是文本内容。由于拼写错误导致了一处拼写错误，需要进行修复。

https://github.com/vllm-project/vllm/issues/5371
这是一个Bug报告，涉及的主要对象是vllm 0.4.3的RTX 4090驱动程序。由于CUDA错误引起的内存访问异常导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/5370
这是一个Bug报告，主要涉及vLLM在8 GPU设置下只能使用--tensor-parallel-size=2而无法使用4或8，导致GPU在加载模型过程中出现故障并需要重启计算机的问题。

https://github.com/vllm-project/vllm/issues/5369
这是一个用户提出的需求类型的issue，主要涉及到同一节点检测功能的添加。这个需求是因为CI只能在同一节点运行，为了在多节点上测试正确性而提出。

https://github.com/vllm-project/vllm/issues/5368
这是一个提出需求的issue，主要涉及到代码改进。原因是为了使代码更清晰，并将其分离成一个独立的PR。

https://github.com/vllm-project/vllm/issues/5367
这是一个功能需求提交的issue，涉及的主要对象是支持在tensor parallel模式下处理attention heads的分配问题。由于用户需要将一个70B的模型运行在3个GPU上，但当前vLLM不支持这种情况，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/5366
这是一个Bug报告，主要涉及VLLM Docker镜像启动后未监听8000端口导致API连接问题。

https://github.com/vllm-project/vllm/issues/5365
这是一个bug报告，涉及的主要对象是加载nvidia/Llama3-ChatQA-1.5-8B模型。由于某种原因导致加载此模型时出现错误日志。

https://github.com/vllm-project/vllm/issues/5364
这个issue是一个bug报告，涉及到vLLM的核心逻辑，描述了由于prefix缓存问题导致需要重新计算kvcache的bug。

https://github.com/vllm-project/vllm/issues/5363
这个issue类型为bug报告，涉及的主要对象是Falcon框架。由于 `trust_remote_code=True`，导致Falcon出现错误，无法正常运行。

https://github.com/vllm-project/vllm/issues/5362
这是一个特性请求 issue，主要涉及添加每晚基准测试，对比多个模型的性能。

https://github.com/vllm-project/vllm/issues/5361
这是一个bug报告，主要涉及的对象是tests/test_sharded_state_loader.py中的一个flaky test，原因是torch.empty张量可能包含NaN值导致的问题。

https://github.com/vllm-project/vllm/issues/5360
这是一个关于Bug报告的issue，主要涉及VLLM在Openshift多GPU环境下无法正常启动的问题。导致该问题的可能原因是VLLM在识别给定GPU时出现冻结，包括尝试将Nvidia操作员粘贴到POD中后仍无法启动。

https://github.com/vllm-project/vllm/issues/5359
这是一个类型为升级同步（Upstream sync）的Issue，涉及主要对象为vllm项目。由于合并提交时漏掉了一个特定的提交，导致了需要再次进行上游同步。

https://github.com/vllm-project/vllm/issues/5358
这是一个bug报告类型的issue，涉及主要对象是vllm中的GLM-4v模型支持。这个问题由于GLM-4v部署时报告了KeyError: 'transformer.vision.transformer.layers.45.mlp.fc2.weight'导致。

https://github.com/vllm-project/vllm/issues/5357
这是一个bug报告，主要涉及的对象是vllm_runner，问题出现的原因是使用del关键字可能导致测试的不稳定性。

https://github.com/vllm-project/vllm/issues/5356
这是一个关于对Triton核心进行修改以支持多个lora服务的初始提交的RFC类型的issue，涉及的主要对象为Triton kernels和punica kernels。由于原先的SLoRA分页权重格式导致性能下降和出现错误的问题，需要修改为连续权重格式解决问题。

https://github.com/vllm-project/vllm/issues/5355
这是一个bug报告，涉及的主要对象是vLLM中的VRAM使用情况。由于没有考虑计算log probabilities时的VRAM使用，可能导致消耗大量VRAM，最终引发了一系列OOM（Out of Memory）问题。

https://github.com/vllm-project/vllm/issues/5354
这是一个功能需求类型的issue，主要涉及到"broadcast_tensor_dict"的合并，原因是两处调用需要合并为一处调用。

https://github.com/vllm-project/vllm/issues/5353
这是一个Breaking类型的issue，主要涉及到vLLM中的FP8模型检查点格式更改，由于目前的act_scale使用模糊，因此提出了将其更改为input_scale的建议。

https://github.com/vllm-project/vllm/issues/5352
这是一个Bug报告，主要涉及FP8 CUTLASS支持检查的问题。导致问题的原因是在torch 2.4.0.dev20240603+cu121版本中，`cutlass_fp8_supported`输出为False。

https://github.com/vllm-project/vllm/issues/5351
这是一个bug报告类型的issue，涉及主要对象为 TorchSDPAMetadata。由于环境信息中PyTorch版本与CUDA版本不匹配，导致TorchSDPAMetadata过时，需要更新。

https://github.com/vllm-project/vllm/issues/5350
这个issue类型是更新需求，涉及的主要对象是`compressed-tensors` quantization方法，由于新的配置文件结构，需要更新以符合新的配置文件结构。

https://github.com/vllm-project/vllm/issues/5349
这是一个bug报告，主要涉及的对象是"recompute"，由于计算后的token未能正确处理重新计算导致问题。

https://github.com/vllm-project/vllm/issues/5348
这是一个功能需求的issue，主要涉及Speculative Decoding Worker的集成TypicalAcceptanceSampler，并通过配置参数来提高接受率。

https://github.com/vllm-project/vllm/issues/5347
这是一个bug报告，主要涉及的对象是测试用例（hf_runner），由于使用了不恰当的del关键字导致测试不够鲁棒。

https://github.com/vllm-project/vllm/issues/5346
该issue是一个需求提出，主要涉及MoE（Mixture of Experts）相关功能的重构。由于现有功能被嵌入到特定模型定义中，导致每个模型文件都需要重新实现大部分功能。

https://github.com/vllm-project/vllm/issues/5345
这是一个代码优化类型的issue，主要涉及到VLLM项目中的CPU后端。由于`cuda_utils.h`被错误地包含在CPU后端中，而未被使用，导致了需要进行移除的问题。

https://github.com/vllm-project/vllm/issues/5344
这是一个bug报告，涉及的主要对象是vllm下的server。由于OOM抑制了assert的提示，触发了一些特定的提示长度导致的错误。

https://github.com/vllm-project/vllm/issues/5343
这是一个Bug报告，问题涉及到vllm项目。由于代码问题导致运行时出现了`AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight'`的错误。

https://github.com/vllm-project/vllm/issues/5342
这是一个bug报告，涉及的主要对象是vllm的Speculative decoding功能。导致该bug的原因是在使用speculative模式时，当prompt_length+output_length超过2048时会出现`AttributeError: 'NoneType' object has no attribute 'numel'`的错误。

https://github.com/vllm-project/vllm/issues/5340
这是一个bug报告，该问题涉及到vllm下的DbrxFusedNormAttention缺失cache_config参数导致的错误。

https://github.com/vllm-project/vllm/issues/5339
这个issue属于性能问题，主要涉及的对象是VLLM模型在使用KV缓存块时的性能表现，由于首次命中缓存的情况下执行速度较慢，而后续命中缓存的请求速度则较快。

https://github.com/vllm-project/vllm/issues/5338
这个issue类型是用户需求报告，主要涉及对象是如何通过CLI启动时传递开关来减少vllm在终端输出的信息量。由于终端输出过多Info信息，用户希望找到方法将其减少。

https://github.com/vllm-project/vllm/issues/5337
这是一个Bug报告，涉及Python的垃圾回收系统导致的flaky tests。

https://github.com/vllm-project/vllm/issues/5336
这是一个Bug报告，主要涉及到VLLM（Very Large Language Model）和FastChat library的使用，由于某种原因导致在使用finetuned CodeLLama7bhf模型进行推理时，输出结果为空字符串。

https://github.com/vllm-project/vllm/issues/5335
这是一个用户提出需求的类型，并且主要涉及的对象是benchmark_throughput.py和benchmark_latency.py脚本。这个问题由于缺少选择分布式执行器的参数，导致用户需要手动修改代码以选择执行器，希望提供更方便的方式选择分布式执行器。

https://github.com/vllm-project/vllm/issues/5334
这个issue类型为bug报告，主要涉及 llama 2 设置 echo=True for openai-api server 出现了Unexpected prompt token logprob behaviors的问题。

https://github.com/vllm-project/vllm/issues/5333
这是一个用户提出需求的issue，主要涉及的对象是vLLM的`get_open_port`函数。由于限制了一次只能返回一个端口导致需要使用多个端口时出现问题。

https://github.com/vllm-project/vllm/issues/5332
这是关于移除guided_decoding中sort_keys=True选项的bug报告，涉及到代码中的一个问题。

https://github.com/vllm-project/vllm/issues/5331
这是一个优化建议类型的issue，主要涉及文档构建过程中使用不必要的重型CPU实例，提出使用较小的CPU实例以节约成本和空间。

https://github.com/vllm-project/vllm/issues/5330
这是一个bug报告类型的issue，涉及主要对象为CI AWS pipeline中的buildkite agent。由于buildkite agent没有位于Docker容器内，导致benchmark结果无法上传和显示在buildkite构建中。

https://github.com/vllm-project/vllm/issues/5329
这是一个bug报告，涉及的主要对象是在vLLM中共享带有内部状态的logits processors导致的输出受影响的问题。

https://github.com/vllm-project/vllm/issues/5328
这个issue属于bug报告类型，主要涉及vLLM在环境中不支持虚拟GPU的问题，导致初始化nccl时出现错误。

https://github.com/vllm-project/vllm/issues/5327
这个issue是一个[升级依赖项至PyTorch 2.3.1]类型的问题，涉及到PyTorch和triton版本的冲突，导致了性能问题。

https://github.com/vllm-project/vllm/issues/5326
This is an issue related to PR submission checklist for the vLLM project on Github, indicating a bug report for markdown rendering not working properly.

https://github.com/vllm-project/vllm/issues/5325
这是一个用户提出需求的issue，主要涉及到vllm对于mistral v0.3的函数调用功能，用户想要运行mistral instruct 0.3的推理。

https://github.com/vllm-project/vllm/issues/5324
该issue类型为文档改进，涉及主要对象为vLLM文档。由于缺少一个关于自动前缀缓存（APC）的部分，用户提出了补充该内容的需求。

https://github.com/vllm-project/vllm/issues/5323
这个issue是一个bug报告，涉及到vLLM项目中的unit tests在AMD ROCm平台上存在问题，导致需要修复或跳过部分测试。

https://github.com/vllm-project/vllm/issues/5322
这个issue是关于缺少PR描述。

https://github.com/vllm-project/vllm/issues/5321
这是一个bug报告，涉及vLLM版本更新后在使用中出现的输出显示问题。

https://github.com/vllm-project/vllm/issues/5320
这是一个bug报告，主要涉及的对象是在高端机器上广播metadata时出现性能问题，可能是由于在这些机器上 `gloo` 性能不佳，而 'nvlink' 更好的原因导致。

https://github.com/vllm-project/vllm/issues/5319
这个issue是关于特性新增（Feature）的，主要涉及的对象是CompletionRequest类，原因是要在CompletionRequest类中继续实现stream_option参数。

https://github.com/vllm-project/vllm/issues/5318
这是一个用户提出需求的issue，主要涉及支持新模型 mistralai/Codestral22Bv0.1，由于无法通过 OpenAI server API 加载该模型所致。

https://github.com/vllm-project/vllm/issues/5317
这是一个bug报告，关于在CPU上编译VLLM时遇到的问题。

https://github.com/vllm-project/vllm/issues/5316
这个issue是一个性能问题报告，主要涉及vllm版本0.4.3下的gptq和awq量化未能改善性能，可能由于版本更新引起性能退化。

https://github.com/vllm-project/vllm/issues/5315
这是一个用户提出需求的issue，主要涉及Docker镜像的版本问题，用户希望能够通过创建一个标记为"latest"的不稳定版本来解决。

https://github.com/vllm-project/vllm/issues/5314
该issue属于需求提出类型，主要涉及kv cache在vllm上的实现及并发推理的共享，提出了对性能优化的建议。

https://github.com/vllm-project/vllm/issues/5313
这是一个bug报告，主要涉及到vllm项目中的"Speculative decoding"功能，发生了生成内容不一致的问题。造成这一问题的原因可能是由当前环境信息或者模型配置不匹配所导致的。

https://github.com/vllm-project/vllm/issues/5312
这是一个bug报告，涉及的主要对象是vLLM的logprobs值限制问题。原因是当前的实现不遵守服务器参数maxlogprobs的限制，导致返回的logprobs被限制，需要修改以确保只检查正整数值。 

https://github.com/vllm-project/vllm/issues/5311
这是一个bug报告，涉及的主要对象是vllm项目中的CUDA kernel执行问题。原因是CUDA error导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/5309
这是一个用户提出需求的issue，主要涉及的对象是vllm中的chat template功能，由于需求是如何在VLLM中实现选择聊天模板的功能，需要讨论如何实现。

https://github.com/vllm-project/vllm/issues/5308
这个issue属于用户提出需求类型，主要对象是vllm项目的kv cache模块。提问者想讨论是否可以通过添加新的GPU资源来增加kv缓存块的总数，而不涉及张量并行或其他并行方法。

https://github.com/vllm-project/vllm/issues/5307
这是一个与代码改进相关的issue，涉及的主要对象是LLaVA模型测试代码。

https://github.com/vllm-project/vllm/issues/5306
这是一个用户提出需求的issue，主要涉及的对象是对话生成模型GLM-4-9B-Chat，用户因暂无响应而询问为何无法支持这一模型。

https://github.com/vllm-project/vllm/issues/5305
这个issue是关于bug报告，涉及到GPU内存使用量不符合设置，导致实际使用量与期望值不符合的问题。

https://github.com/vllm-project/vllm/issues/5304
这个issue属于bug报告，主要涉及的对象是vllm项目的安装过程。由于某种原因导致安装vllm时卡在了"Installing build dependencies ..."阶段。

https://github.com/vllm-project/vllm/issues/5303
这个issue是关于Bugfix的，主要涉及的对象是在处理以":"开头的内容时的客户端行为。由于markdown渲染不起作用，所以使用了原始html，可能导致问题。

https://github.com/vllm-project/vllm/issues/5302
这是一个bug报告，主要涉及的对象是vllm/vllmopenai:v0.4.3 docker环境下的模型推理过程。由于将maxnumseqs设为1时出现了500 Internal Server Error，推测是由于该设置导致的bug。 

https://github.com/vllm-project/vllm/issues/5301
这是一个关于安装的bug报告，主要涉及vllm项目。问题来源于使用torch=2.1.1和cuda=12.1.1在RunPod实例上构建时出错。

https://github.com/vllm-project/vllm/issues/5300
这是一个技术需求类型的issue，涉及的主要对象是前端vLLM。由于需要优化批处理和吞吐量，希望能够通过一次性传递多个LoRA适配器到`generate()`来实现。

https://github.com/vllm-project/vllm/issues/5299
这是一个Bug报告，主要涉及的对象是vLLM库。由于升级到vLLM版本0.4.3后，无法请求超过5个logprobs，导致无法运行脚本。

https://github.com/vllm-project/vllm/issues/5298
这是一个Bug报告，涉及对象为VLLM和Hugging Face模型加载时细调后推理结果不一致的问题。原因可能是与transformers+lora的相关性有关。

https://github.com/vllm-project/vllm/issues/5297
这是一个关于性能问题的bug报告，主要涉及的对象是vLLM的MoE kernel和HuggingFace，在BF16和FP16下观察到MoE kernel比HuggingFace慢的情况。可能是由于不同的数据类型和实现导致了这种现象。

https://github.com/vllm-project/vllm/issues/5296
这是一个bug报告，主要涉及对象是调度器（SchedulerOutputs），原因是`infeasible_seq_groups`未被添加到`ignored_seq_groups`导致功能不完整。

https://github.com/vllm-project/vllm/issues/5295
这个issue类型为文档更新请求，主要对象是 Ray Summit CFP。由于缺少具体内容，用户提出了需要添加 Ray Summit CFP 的请求。

https://github.com/vllm-project/vllm/issues/5294
该issue类型为Kernel调整，涉及的主要对象是Mixtral 8x22b配置和FP8 on H100，由于markdown渲染问题，需要使用原始HTML。

https://github.com/vllm-project/vllm/issues/5293
该issue属于功能需求类型，主要涉及将协调员添加到tp和pp中以减少代码重复。该问题主要解决了分布式编程中的重复代码问题。

https://github.com/vllm-project/vllm/issues/5292
这个issue是一个特性请求，主要涉及的对象是硬件TPU集成。由于性能问题，暂时禁用了（Fast）topp抽样。

https://github.com/vllm-project/vllm/issues/5291
这个issue类型为优化建议，主要涉及主对象是模型的logits_scale参数。由于其值为1.0时的微小优化，提出了这个建议。

https://github.com/vllm-project/vllm/issues/5290
这是一个Bug报告，涉及到AsyncLLMEngine在关闭时不必要地记录异常，导致OpenAI Batch文件格式带来`asyncio.exceptions.CancelledError`的问题。

https://github.com/vllm-project/vllm/issues/5289
这是一个Bug报告，涉及的主要对象是减少复制提示/输出标记时的性能开销。触发bug的原因是复制大张量会增加不可忽视的开销。

https://github.com/vllm-project/vllm/issues/5288
这个issue类型是bug报告，涉及的主要对象是flash_attn backend，导致症状bug是由于方式在attention kernel调用中编码多个查询标记导致了flash_attn协议的破坏。

https://github.com/vllm-project/vllm/issues/5287
这是一个关于文档更新的issue，涉及的主要对象是新增赞助商Sequoia。

https://github.com/vllm-project/vllm/issues/5286
这是一个bug报告，涉及到flash_attn后端的问题，导致CUDA非法内存访问的现象。

https://github.com/vllm-project/vllm/issues/5285
这个issue是关于bug报告，主要涉及对象是EngineArgs，因为传入Optional参数时易混淆导致问题，需要使用具名参数进行显式赋值。

https://github.com/vllm-project/vllm/issues/5284
这是一个bug报告，涉及的主要对象是日志信息格式化，导致bug由于尝试将列表格式化为整数而导致的大堆栈跟踪。

https://github.com/vllm-project/vllm/issues/5283
这是一个bug报告，涉及的主要对象是vllm docker images。这个问题据推测是由于CUDA函数调用导致的错误，使得v0.4.3版本无法正常工作。

https://github.com/vllm-project/vllm/issues/5282
这是一个bug报告，涉及到自定义操作导入错误，导致出现了`NameError: name 'vllm_cache_ops' is not defined`的错误消息缺失问题。

https://github.com/vllm-project/vllm/issues/5281
这是一个Bug报告，涉及对象为VLLM中的模型加载过程。由于内存不足导致无法加载7B AWQ量化模型，出现了Running out of memory的问题。

https://github.com/vllm-project/vllm/issues/5280
这是一个Bug报告，主要涉及vllm在版本0.4.2和版本0.4.3之间预测结果的变化。由于v0.4.3的变化导致了实际标记和logprobs的预测结果发生了改变。

https://github.com/vllm-project/vllm/issues/5279
这个issue是一个特性请求，主要涉及动态图像大小支持，由于要充分利用LLaVANeXT的多分辨率功能，故需要插入动态数量的图像token。

https://github.com/vllm-project/vllm/issues/5278
这是一个功能需求类型的issue，涉及前端组件中OpenAI API服务器的修改，主要讨论了在生成对话提示时是否添加BOS特殊标记的问题。

https://github.com/vllm-project/vllm/issues/5277
这个issue是关于pull request描述填写不完整的问题，属于文档修正类型，主要涉及vllm的PR描述填写规范，可能是由于开发人员忘记按照要求填写PR描述而导致。

https://github.com/vllm-project/vllm/issues/5276
该issue属于功能需求类型，涉及主要对象为VLM（Variable-length language model）。由于需要动态支持图片尺寸，用户提出了对VLM模型的输入处理进行更新的需求。

https://github.com/vllm-project/vllm/issues/5275
该issue是一个[Kernel]类型的更新，涉及到Cutlass int8 kernel configs for SM80，主要是添加6个int8 Cutlass Kernel配置来适应不同的Gemm形状。

https://github.com/vllm-project/vllm/issues/5274
这是一个bug报告，涉及主要对象是GPU内存利用率。这个问题可能由于不合适的gpu_memory_utilization值导致了不同的内存错误。

https://github.com/vllm-project/vllm/issues/5273
这是一个bug报告，主要涉及chatglm3 with lora adapter，由于出现了AttributeError导致的bug。

https://github.com/vllm-project/vllm/issues/5272
这是一个Bug报告，涉及的主要对象是VLLM接口的调用。由于代码中出现了类型对象无法被下标访问的错误，导致了报错异常。

https://github.com/vllm-project/vllm/issues/5271
这是一个文档问题报告，涉及到 "get_attn_backend" 方法的文档字符串需修复。

https://github.com/vllm-project/vllm/issues/5270
这是一个bug报告，涉及的主要对象是vllm项目下的Sequence类。由于初始化Sequence类时未设置self.prompt_token_ids，却在后续使用self.prompt_token_ids导致的bug。

https://github.com/vllm-project/vllm/issues/5269
这是一个用户提出需求的issue，主要涉及的对象是vllm模型加载过程。由于32GBCPU内存不足以加载qwen1.5 32b bf16模型，用户提出如何在较少的CPU内存下运行vllm的问题。

https://github.com/vllm-project/vllm/issues/5268
这是一个关于bug报告类型的issue，主要涉及vllm推理THUDM/chatglm36b128k环境无法停止的问题。该问题可能是由于程序bug或环境配置错误导致。

https://github.com/vllm-project/vllm/issues/5267
这是一个Bug报告，涉及的主要对象是系统环境配置和GPU相关信息。由于PyTorch版本和CUDA版本不匹配，导致生成吞吐量为0.0 tokens/s的问题。

https://github.com/vllm-project/vllm/issues/5266
这是一个用户向VLLM项目提交的寻求帮助的问题，主要关于如何在文本生成模型中获取输出嵌入的问题。该问题的根源是当前环境下的方法LLM.encode()仅适用于嵌入模型，用户想对文本生成模型进行操作却不知如何在VLLM中集成。

https://github.com/vllm-project/vllm/issues/5265
这是一个bug报告，涉及到PP groups的删除操作。这个问题可能是由于PP groups没有被正确地清除导致的bug。

https://github.com/vllm-project/vllm/issues/5264
这是一个Bug报告类型的Issue，主要涉及VLLM下的API服务。由于某种原因导致调用`prompt_logprobs`时会每次都产生400服务器错误。

https://github.com/vllm-project/vllm/issues/5263
这是一个[特性改进]的issue，主要涉及benchmark_serving.py脚本的改进，通过添加ITL结果和调整TPOT结果来改善性能分析和统计数据的可用性。

https://github.com/vllm-project/vllm/issues/5262
这个issue是一个bug报告，主要涉及的对象是vLLM中的Kernel，由于代码中未允许8位输出，导致问题出现。

https://github.com/vllm-project/vllm/issues/5261
这是一个 bug 报告，主要涉及的对象是环境收集功能。这个问题是由于两行代码未能换行导致的 bug 报告。

https://github.com/vllm-project/vllm/issues/5260
这是一个用户提出需求的issue，主要对象是CI（持续集成），用户希望添加夜间基准测试。

https://github.com/vllm-project/vllm/issues/5259
这是一个需求类型的issue，涉及主要对象是transformers版本信息的收集。原因是缺乏transformers版本信息导致无法确定与具体模型支持相关的问题。

https://github.com/vllm-project/vllm/issues/5258
这是一个bug报告，主要涉及到markdown渲染无法正常工作。由于markdown渲染问题，导致了无法正确显示内容。

https://github.com/vllm-project/vllm/issues/5257
这个issue是关于缓存图片的问题报告。

https://github.com/vllm-project/vllm/issues/5256
这是一个针对软件测试相关的issue，主要涉及到持续集成中的AMD测试。由于AMD测试一直存在失败且设备不稳定，导致阻塞了工程师的工作进度，因此需要将这些测试标记为软失败，以避免阻碍PR的合并。

https://github.com/vllm-project/vllm/issues/5255
该issue类型为功能需求，主要对象是自定义层（custom layers）。导致该需求的原因是目前自定义层存在两个问题：1.对于TPU和Gaudi等设备不支持._custom_ops的直接导入；2.假设自定义操作在所有设备上以相同方式实现。

https://github.com/vllm-project/vllm/issues/5254
这个issue是一个Bugfix类型的报告，涉及主要对象是`MultiprocessingGPUExecutor`和`MultiprocessingGPUExecutorAsync`。由于`check_health()`方法在`world_size == 1`时需要`worker_monitor`属性，但这两个对象在这种情况下没有该属性，导致了报错。

https://github.com/vllm-project/vllm/issues/5253
这是一个功能改进请求的issue，主要涉及到代码中的CUDA compute capability检查优化。原因是为了避免重复的代码和更好地管理测试配置。

https://github.com/vllm-project/vllm/issues/5252
这个issue是关于bug的报告，主要涉及到`SpecDecodeWorker`和`Top1Proposer`对`proposer_worker`方法的假设没有被类型捕捉到，导致了一些bug。

https://github.com/vllm-project/vllm/issues/5251
这是一个功能增强提议，主要涉及`HfRunner`模型加载简化，用户不再需要手动注册新模型来加载HuggingFace模型，而是通过传递`is_embedding_model`或`is_vision_model`来指示如何加载模型。

https://github.com/vllm-project/vllm/issues/5250
这是一个关于bug报告的issue，涉及主要对象是prometheus metric vllm:request_success_total。该问题可能是由于某种错误导致统计值被错误地增加了两次而引起的。

https://github.com/vllm-project/vllm/issues/5249
这个issue属于bug报告类型，主要涉及的对象是vllm的CPU Dockerfile。由于使用`pip install setuptools>=49.4.0`安装setuptools时导致的文件名错误，在构建CPU docker实例时出现了问题。

https://github.com/vllm-project/vllm/issues/5248
这是一个bug报告，该问题涉及的主要对象是KVCache的分配在CUDA_VISIBLE_DEVICES中只在第一个设备中进行，即使使用`tensorparallelsize`大于1。由于这种问题导致了KVCache仅在第一个设备中分配的情况，用户可能希望支持在所有设备上完全分配KVCache。

https://github.com/vllm-project/vllm/issues/5246
这是一个关于如何使用GLIBCXX_USE_CXX11_ABI=1编译的问题，类型为用户提出需求，涉及主要对象为编译环境和PyTorch。由于编译环境中的GLIBCXX_USE_CXX11_ABI设置不正确，导致了编译时出现问题。

https://github.com/vllm-project/vllm/issues/5245
这个issue类型为用户提出需求，涉及vLLM中支持Classifier-Free Guidance Logits processor的功能。该问题源于需要在vLLM中执行Model Forward Process与unconditional_ids相结合以计算logits。

https://github.com/vllm-project/vllm/issues/5244
这是一个bug报告，主要涉及的对象是在vllm环境下运行Phi-3-small-128k-instruct时出现的环境信息收集问题。导致这个问题的原因可能是与PyTorch版本和CUDA相关联的配置不匹配。

https://github.com/vllm-project/vllm/issues/5243
这个issue是关于bug报告，主要涉及的对象是"StopChecker"类。由于StopChecker假设每一步只生成一个token，而实际可能一步生成多个token，导致last_token_id捕获不准确，用户提交该issue寻求解决这一问题。

https://github.com/vllm-project/vllm/issues/5242
这是一个bug报告，涉及的主要对象是MoE tuning脚本。原因是在重构过程中错误地省略了批量大小1536和3072，导致此问题。

https://github.com/vllm-project/vllm/issues/5241
这个issue是一个关于CI/Build优化的类型为[CI/Build]的PR，主要涉及vLLM的CPU CI执行时间。原因是通过将CPU核心绑定到本地内存节点，从而减少执行时间。

https://github.com/vllm-project/vllm/issues/5240
这是一个bug报告，主要涉及LLM的Tokenizer设置问题，导致每次调用非缓存的Tokenizer，需要通过应用适配器来解决这个问题。

https://github.com/vllm-project/vllm/issues/5239
该issue为性能问题报告，涉及的主要对象是VLLM模型的Speculative Decoding性能下降，用户提出了关于性能实验结果的疑问。

https://github.com/vllm-project/vllm/issues/5238
这是一个关于调整Mixtral MoE配置的Kernel问题，由于配置错误导致了ITL结果不符合预期。

https://github.com/vllm-project/vllm/issues/5237
该issue类型为功能需求。主要涉及对象为前端开发，用户提出在VLLM中增加对OpenAI Vision API的支持。由于需要在VLLM中增加GPT4V兼容推理的支持，用户提交了此功能需求。

https://github.com/vllm-project/vllm/issues/5236
这是一个Bug报告，主要涉及LLM.generate()方法在某些填充方式下出现问题。填充输入会导致生成结果出现错误，用户希望了解导致此问题的原因。

https://github.com/vllm-project/vllm/issues/5235
这是一个bug报告，涉及的主要对象是prefix caching示例。这个bug是由于旧代码中缺少warmup导致prefix缓存实际上不会发生。

https://github.com/vllm-project/vllm/issues/5234
这个issue类型是功能需求提出，主要涉及的对象是为vLLM添加一个高效的接口来评估固定提示-完成对的概率。由于当前接口效率低下或不完全支持该使用案例，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/5233
这个issue属于bug报告类型，涉及的主要对象是下载模型（models）时出现的错误。

https://github.com/vllm-project/vllm/issues/5232
这是一个用户提出需求的issue，主要涉及的对象是vllm-flash-attn，用户请求支持cu118版本，由于当前版本似乎不支持cu118，导致用户提出该问题。

https://github.com/vllm-project/vllm/issues/5231
这个issue是关于修正Mixtral FP8检查点加载的问题，主要涉及模型权重的加载和缩放因子更新，在之前的实现中没有正确加载w1和w3的权重缩放因子，导致性能表现下降。

https://github.com/vllm-project/vllm/issues/5230
这个issue属于需求提出类型，主要涉及的对象是VLLM项目中的分布式执行器。

https://github.com/vllm-project/vllm/issues/5229
这个issue属于bug报告类型，主要涉及的对象是使用`torch.compile()`和`MultiprocessingGPUExecutor`的用户。由于使用默认的异步编译进程池与`MultiprocessingGPUExecutor`不兼容，导致`torch.compile()`无法正常工作，用户需要将`TORCHINDUCTOR_COMPILE_THREADS`设置为1来禁用异步编译进程池。

https://github.com/vllm-project/vllm/issues/5228
这是一个用户提出需求的issue，主要涉及在vLLM中实现自定义注意力掩码的功能。由于用户想要从解码器 Causal LLM 到双向上下文 LLM 的转换，需要使用自定义的注意力掩码来实现该功能。

https://github.com/vllm-project/vllm/issues/5227
这是一个用户提出需求的类型，该问题单涉及的主要对象是如何通过`LLM`对象启动推理服务。用户想要在项目中通过编程方式启动和停止vllm OpenAI兼容的推理服务器，以替代使用命令行操作。

https://github.com/vllm-project/vllm/issues/5226
这是一个关于bug修复的issue，主要涉及对象是vllm项目中的提示日志概率（prompt_logprobs）功能。由于设置`SamplingParams.detokenize`为True时导致`prompt_logprobs`被跳过，可能会影响用户只需要提示日志概率而不需要解标记文本的需求。

https://github.com/vllm-project/vllm/issues/5224
这是一个版本发布跟踪issue，主要涉及到 vllm 项目 v0.5.0 版本的更新计划和特性开发情况。

https://github.com/vllm-project/vllm/issues/5223
该issue属于[杂项]类型，主要涉及增加对规范解码的支持到吞吐性能基准测试脚本的问题。最终导致这个问题的原因是markdown呈现不正常，所以使用了原始的html标记。

https://github.com/vllm-project/vllm/issues/5222
这是一个bug报告，主要涉及对象是运行vllm进行推理时出现的CUDA错误。原因可能是CUDA错误导致的不可纠正的ECC错误。

https://github.com/vllm-project/vllm/issues/5221
该issue为文档更新类别，涉及vLLM的分布式推断和Serving功能，用户提出了关于更新文档以反映使用MultiprocessingGPUExecutor作为Ray的替代方案的需求。

https://github.com/vllm-project/vllm/issues/5220
这是一个bug报告，涉及的主要对象是vllm中的Mixtral-8x22。由于客户端发送多个并发请求时，取消范围导致请求被取消，可能导致异常情况。

https://github.com/vllm-project/vllm/issues/5219
这是一个bug报告，涉及的主要对象是Mistral 7B程序在NVidia Tesla P100上出现CUDA错误导致崩溃。

https://github.com/vllm-project/vllm/issues/5218
这是一个关于提出需求的issue，主要涉及的对象是vllm提供的W4A8 quantization功能。由于新集成的QQQ quantization方法可能会减少一些推理性能，用户可能需要帮助来理解如何在其模型中使用该功能。

https://github.com/vllm-project/vllm/issues/5217
这个issue是一个Bugfix类型的报告，主要涉及的对象是vLLM中的`prompt_logprobs`参数。原因是因为`prompt_logprobs`值为0时引发了`AssertionError`错误。

https://github.com/vllm-project/vllm/issues/5215
这个issue是一个功能增强请求，涉及的主要对象是CI/Build，由于开发者在之前的工作中忘记为`test_input.py`和`multimodal`目录添加新的CI步骤`inputstests`。

https://github.com/vllm-project/vllm/issues/5214
这个issue属于新功能需求，并涉及到处理模型输入时注册的相关对象，由于更改了`MultiModalRegistry`接口导致了症状性的问题。

https://github.com/vllm-project/vllm/issues/5213
这是一个Bug报告，主要涉及到vllm==0.4.3版本中的一个问题。由于prompt_logprobs=0导致断言错误，进而无法与echo=True一起使用，最终导致OpenAI兼容API服务器出现500内部服务器错误。

https://github.com/vllm-project/vllm/issues/5212
这个issue类型为安装问题，主要对象是punica构建。用户可能遇到了由于环境或安装方式导致的punica构建失败的问题。

https://github.com/vllm-project/vllm/issues/5211
这个issue类型是用户请教问题，主要涉及的对象是如何在vllm模型中运行推理并释放GPU内存。用户可能遇到问题是由于缺乏集成指南或指导，无法成功实现运行推理操作。

https://github.com/vllm-project/vllm/issues/5210
这是一个bug报告，涉及VLLM_ATTENTION_BACKEND环境变量设置问题，由于某些边缘情况未处理导致测试环境变量设置错误，影响其他测试。

https://github.com/vllm-project/vllm/issues/5209
这是一个用户提出需求的issue，主要涉及对模型功能的更新要求，包括增加对Mirostat、Dynamic Temperature和Quadratic Sampling的支持。

https://github.com/vllm-project/vllm/issues/5208
这是一个Bug报告，主要涉及VLLM_ATTENTION_BACKEND设置在GHA环境下覆盖自动选择后端，导致其他内核单元测试失败的问题。

https://github.com/vllm-project/vllm/issues/5207
这个issue是一个BugFix类型的报告，主要涉及的对象是LLM中的tokenizer setter。这个问题是由于tokenizer长度获取方法的变化导致了detokenize的延迟问题。

https://github.com/vllm-project/vllm/issues/5206
这个issue类型是bug报告，主要涉及到vllm库中的detokenize_incrementally函数，更新版本导致生成时间显著增加，原因是函数中的一些操作耗费了更多时间。

https://github.com/vllm-project/vllm/issues/5205
这个issue是关于功能需求的，主要涉及到HuggingFace的配置文件问题，用户提出希望能在vLLM CLI中提供覆盖配置文件的选项以解决加载模型时可能出现的问题。

https://github.com/vllm-project/vllm/issues/5204
这是一个Bug报告，涉及的主要对象是vllm 0.3.0和0.4.3版本。升级到0.4.3版本后，返回的token更为分散，导致无法满足条件匹配["Final", "Answer", ":"]，从而影响了LangChain AsyncFinalIteratorCallbackHandler机制。

https://github.com/vllm-project/vllm/issues/5203
这是一个特性需求的issue，主要涉及的对象是vllm的Speculative Decoding功能。由于draft模型和target模型的vocab大小不一致，导致推断流程无法运行，提出是否可以实现不一致的vocab大小支持。

https://github.com/vllm-project/vllm/issues/5202
这是一个用户提出需求的issue，主要涉及VLLM模型不支持量化版本，用户希望添加支持以提高成本效益。

https://github.com/vllm-project/vllm/issues/5201
该issue类型属于功能需求，主要涉及到vLLM的新特性"Speculative edits"。该需求由于现有框架对前缀的不可变性做出了假设，导致用户提出了对于新的speculative方法的需求。

https://github.com/vllm-project/vllm/issues/5200
这是一个bug报告，涉及主要对象是更新setup.py以便为CPU构建wheel。由于之前的bug，尝试构建时会触发GPU的扩展构建，导致失败。

https://github.com/vllm-project/vllm/issues/5199
这是一个Bug报告，主要涉及的对象是在T4 GPU上应用LoRA时遇到的错误。由于在T4 GPU上启用LoRA时出现错误，但在L4或A100 GPU上没有问题，可能是由于硬件兼容性或代码实现上的问题导致的。

https://github.com/vllm-project/vllm/issues/5198
这是一个bug报告，涉及主要对象是在T4 GPU上应用LoRA时遇到的错误。由于T4 GPU上enable_lora=True引发错误，而在L4或A100 GPU上没有此问题。

https://github.com/vllm-project/vllm/issues/5197
该issue类型为功能需求提议，主要涉及RoPE theta的定制化，原因是改变RoPE theta有助于提高规模模型的性能。

https://github.com/vllm-project/vllm/issues/5196
这是一个Bug报告，涉及的主要对象是代码的推送操作。原因可能是推送操作出现错误导致无法成功提交代码。 

https://github.com/vllm-project/vllm/issues/5195
这是一个用户提出需求的issue，主要涉及的对象是在k8s HPA中使用gpu_cache_usage_perc作为自定义指标。由于gpu_cache_usage_perc是一个小数值（0到1之间），而不是计数器，导致用户无法找到实现HPA策略的方法。

https://github.com/vllm-project/vllm/issues/5194
这是一个bug报告，涉及主要对象是代码中的`parse_fine_tuned_lora_name`函数。由于原来的代码在解析失败时可能没有提供清晰的错误信息，导致用户困惑并引发AssertionError，导致了这个issue的产生。

https://github.com/vllm-project/vllm/issues/5193
这个issue属于用户提出需求类型，主要涉及到在部署llama370b模型时遇到的内存限制以及希望了解启用和禁用lora时CUDA图加速的内存消耗差异。

https://github.com/vllm-project/vllm/issues/5191
这个issue是关于支持加载GGUF模型的功能需求。

https://github.com/vllm-project/vllm/issues/5190
这是一个Bug报告，主要对象是加载squeezellm模型的过程中出现了运行时错误。

https://github.com/vllm-project/vllm/issues/5189
这是一个用户提出需求的类型issue，主要对象是添加PaliGemma Google的CuttingEdge Open Vision Language Model。

https://github.com/vllm-project/vllm/issues/5188
这个issue是一个Bug报告，涉及的主要对象是vllm下的Core模块。由于hashing logic for non-full blocks存在问题，导致了部分块的哈希计算错误。

https://github.com/vllm-project/vllm/issues/5187
这是一个Bugfix类型的issue，涉及到vLLM的前端api_server.py文件，由于使用`prompt_token_ids`而不是字符串prompt在`/generate`端点上进行文本生成导致了错误。

https://github.com/vllm-project/vllm/issues/5186
这是一个Bug报告，主要涉及的对象是vLLM的api_server.py文件。这个问题由于在使用`/generate`API中使用`prompt_token_ids`字段时引发了两个bug，一个导致错误，另一个是缺乏对`token_ids`字段的实现，使得无法返回token_ids。

https://github.com/vllm-project/vllm/issues/5185
这个issue是一个bug报告，主要涉及的对象是vllm库中的MoE kernels。由于缺少'moe_kernels'模块导致了ModuleNotFoundError的bug。

https://github.com/vllm-project/vllm/issues/5184
这是一个Bug报告，主要涉及对象是vLLM项目中的`LLM.encode()`方法。这个问题产生的原因是一些用户尝试对生成模型调用`LLM.encode()`方法，但该方法仅适用于嵌入模型。

https://github.com/vllm-project/vllm/issues/5183
这是一个关于在vLLM中将fp8层切换到使用CUTLASS内核的问题，类型为Kernel。

https://github.com/vllm-project/vllm/issues/5182
这是一个Bug报告，涉及Offline Inference with the OpenAI Batch file format，由于某些原因导致产生了`asyncio.exceptions.CancelledError`错误。

https://github.com/vllm-project/vllm/issues/5181
这是一个`bug报告`类型的issue，主要涉及`Offline Inference Embedding Example`代码。由于`vllm0.4.3`版本中执行`model.encode(prompts)`这一行代码会导致错误，用户报告了这个问题。

https://github.com/vllm-project/vllm/issues/5180
这是一个Bugfix类型的issue，主要涉及修复了关于前缀缓存示例的问题。

https://github.com/vllm-project/vllm/issues/5179
这是一个用户提出需求的类型的issue，该问题单涉及的主要对象是支持BERT模型。

https://github.com/vllm-project/vllm/issues/5178
这是一个bug报告，涉及对象是 LoRA 模型支持的添加和 CohereForCausalLM 模型使用时的 ValueError。

https://github.com/vllm-project/vllm/issues/5177
这是一个Bug报告类型的issue，主要涉及的对象是vllm中的示例代码`offline_inference_with_prefix.py`。由于代码实现问题，导致了inference速度比较不准确和说明信息错误的bug。

https://github.com/vllm-project/vllm/issues/5176
这是一个关于VLLM的功能需求类型的issue，主要涉及到prefix caching功能在VLLM服务器中的表现问题，包括跨多个请求时缓存无法持续、询问是否可以实现LFU替代LRU方式、以及想要在服务器启动时配置KV缓存的前缀列表并防止其在服务器停止前被转储的问题。

https://github.com/vllm-project/vllm/issues/5175
这个issue是一个文档修复请求，主要涉及vLLM项目的PR提交检查列表在Markdown渲染上的显示问题。

https://github.com/vllm-project/vllm/issues/5174
这个issue是一个Bug报告，主要涉及到vLLM代码中的@abstractproperty被deprecated所导致的问题。

https://github.com/vllm-project/vllm/issues/5173
这是一个bug报告，主要涉及vllm引擎中的cuda out of memory导致的异步引擎死亡错误，需要添加异常处理以重新启动引擎。

https://github.com/vllm-project/vllm/issues/5172
这个issue是一个[Model]类型的问题，涉及到vLLM的fp8 gemm计算添加。这个问题是为了提升性能和优化代码而添加的一个功能更新。

https://github.com/vllm-project/vllm/issues/5171
这个issue属于文档更新类型，涉及主要对象为"vllm"项目的GPTBigCodeForCausalLM LoRA支持，问题是由于更新导致文档遗漏了相关信息。

https://github.com/vllm-project/vllm/issues/5170
这是一个Bug报告类型的issue，主要涉及的对象是vLLM模型。由于多个GPU Rank时，模型启动出现挂起，可能是由于环境配置或代码逻辑导致的。

https://github.com/vllm-project/vllm/issues/5169
这是一个Bug报告，主要涉及的对象是vllm项目中的lora功能。这个问题由于内存访问错误导致了数据溢出的Bug。

https://github.com/vllm-project/vllm/issues/5168
这是一个关于构建过程中针对旧版本CUDA的警告的issue，主要涉及CUTLASS 3.x kernels的编译问题。造成这个问题的原因可能是为了防止在不兼容的CUDA版本下编译导致的错误。

https://github.com/vllm-project/vllm/issues/5167
这是一个用户提出需求的issue，主要涉及vLLM项目的性能优化方面。由于OctoAI展示了如何通过一些优化技术提高速度，用户想要知道如何达到相同的性能水平。

https://github.com/vllm-project/vllm/issues/5166
这是一个bug报告类型的issue，涉及的主要对象是编译问题。由于当前的cutlass kernel在CUDA 11下编译会出现问题，因此特意将其放入一个只在CUDA 12下编译的代码块中，以避免编译问题。

https://github.com/vllm-project/vllm/issues/5164
这是一个Bug报告类型的issue，主要涉及 vLLM 中使用LoRA adapters 时出现 KeyError: 1 的问题。原因是在批量推断过程中，尝试删除一个已经移除的LoRA adapter 导致崩溃。

https://github.com/vllm-project/vllm/issues/5162
这是一个Bug报告，涉及的主要对象是VLLM库中的AsyncLLMEngine。问题出现的原因是尝试在库中启用KV缓存时遇到异常信息。

https://github.com/vllm-project/vllm/issues/5161
这是一个Bug报告类型的issue，主要涉及环境中使用多个GPU时出现问题。由于在WSL2环境中使用2个GPU时无法正常工作，可能是由于PyTorch版本、CUDA版本或其他配置问题导致。

https://github.com/vllm-project/vllm/issues/5160
此issue为关于"Bug"的报告，主要涉及到AsyncLLMEngine模块中的Token处理效率和KeyValue缓存利用的问题。由于KeyValue缓存未被使用，可能导致Token处理效率低下，用户提出需要进一步调查此差异根本原因。

https://github.com/vllm-project/vllm/issues/5159
这是一个[Kernel]类型的issue，主要涉及int8 static quantize kernel和torch::Tensor的使用问题，由于传递float而非tensor作为scale参数导致了CUDA图中CPU与GPU的同步问题。

https://github.com/vllm-project/vllm/issues/5158
这个issue类型是改进建议，主要涉及的对象是vLLM中的GPU内存利用参数。由于GPU内存利用参数应用方式不同导致vLLM未充分利用GPU资源的结果。

https://github.com/vllm-project/vllm/issues/5157
这是一个关于优化二进制文件大小的bug报告，主要涉及GPU架构保护相关的代码优化。由于每个CUTLASS内核都被编译为每个`CUDA_SUPPORTED_ARCHS`中定义的架构，导致二进制文件大小过大。

https://github.com/vllm-project/vllm/issues/5156
这是一个用户提出需求的issue，主要涉及的对象是VLLM模型的函数调用支持。由于VLLM在推理速度上表现出色，但无法找到如何使用函数调用，用户提出了关于VLLM支持函数调用的需求。

https://github.com/vllm-project/vllm/issues/5155
该issue属于用户提出需求类型，主要涉及的对象是vLLM中的Linear adapter支持。由于当前vLLM仅支持特定层的LoRA适配器运行推断，用户希望扩展至线性层以实现推断训练支持一致性。

https://github.com/vllm-project/vllm/issues/5154
这是一个bug报告类型的issue，主要涉及OpenAI的部署模型和FastAPI的离线推断方式，存在部署耗时问题。

https://github.com/vllm-project/vllm/issues/5152
这是一个bug报告的issue，主要涉及vLLM项目中的CUDA illegal memory access bug，导致运行特定测试时出现错误。

https://github.com/vllm-project/vllm/issues/5151
这是一个bug报告，问题涉及的主要对象是代码中的文件路径。由于拼写错误导致的路径错误，可能导致程序无法正确读取或保存文件，需要修复路径拼写错误。

https://github.com/vllm-project/vllm/issues/5150
这是一个软件安装bug报告，主要对象是vllm库的最新版本编译失败，用户遇到了无法成功安装vllm的问题。

https://github.com/vllm-project/vllm/issues/5149
这是一个bug报告类型的issue，涉及的主要对象是Kernel模块。这个问题可能是由于未正确使用::ordered_metadata修饰符导致的bug。

https://github.com/vllm-project/vllm/issues/5148
这是一个bug报告类型的issue，主要涉及vllm中使用temperature参数进行推理时生成结果不一致的问题。

https://github.com/vllm-project/vllm/issues/5147
这是一个bug报告类型的issue，主要涉及的对象是vllm项目中的模型推理过程。由于GPU内存不足，导致推理过程中出现了CUDA out of memory的错误。

https://github.com/vllm-project/vllm/issues/5146
该issue属于优化和重构类型，主要涉及的对象是int8量化内核。原因是根据作者的描述，通过重构和优化内核，旨在提高性能并解决指令依赖链导致的问题。

https://github.com/vllm-project/vllm/issues/5145
这是一个bug报告类型的issue，涉及的主要对象是gptq_marlin测试，由于某个bug导致需要增加一个测试来确保问题得到修复。

https://github.com/vllm-project/vllm/issues/5144
该issue类型为Kernel更新问题，涉及到Cutlass fp8配置。由于Markdown格式无法正常呈现，所以使用了原始HTML。由此导致在PR标题和分类中囊括了不适用的分类。

https://github.com/vllm-project/vllm/issues/5143
这是一个用户提出需求的类型，主要涉及到数据并行化的问题。由于数据量大，用户希望在多节点上对模型进行离线推理，并使用数据并行化和张量并行化。

https://github.com/vllm-project/vllm/issues/5142
这个issue是一个Bugfix类型的报告，涉及的主要对象是MPT models。导致这个bug的原因是MPTConfig中的attn_config是一个字典而不是一个对象，导致在使用GQA时返回了num_heads而不是正确的num_kv_heads。

https://github.com/vllm-project/vllm/issues/5141
这个issue是一个构建问题报告，涉及的主要对象是cu11，导致这个问题的原因是sm_90a被禁用。

https://github.com/vllm-project/vllm/issues/5140
这个issue是关于CI/Build测试buildkite monorepo插件的。原因是markdown渲染不起作用，因此在此处使用原始html。

https://github.com/vllm-project/vllm/issues/5139
这是一个bug报告issue，主要涉及到前端（Frontend）。由于logit_bias_logits_processor接收了一个没有意义的参数token_ids，可能导致了未知的问题。

https://github.com/vllm-project/vllm/issues/5138
这个issue是一个Bug报告，涉及的主要对象是vllm中flash attn backend。该问题由于vllmflashattn == 2.5.8.post3版本中的不必要复制导致的。

https://github.com/vllm-project/vllm/issues/5137
这个issue类型是bug报告，主要对象是Kernel中的CUTLASS kernels。由于之前的实现方式导致在处理每个张量和每个token的标度时需要处理不同类型的数据，这导致了性能问题和一些同步和GPU/CPU之间数据交换的困扰。

https://github.com/vllm-project/vllm/issues/5136
这是一个Bug报告，主要涉及的对象是Kernel中的Marlin_24，由于PTX 8.5引入了新的ordered_metadata修饰符，导致需要在mma.sp指令中添加该修饰符。

https://github.com/vllm-project/vllm/issues/5135
这是一个关于在 `ChatCompletionRequest` 类中添加对 `stream_options` 支持的特性请求。

https://github.com/vllm-project/vllm/issues/5134
这是关于用户需求的问题，涉及使用vllm与tensorrt-LLM的教程。

https://github.com/vllm-project/vllm/issues/5132
这是一个bug报告，涉及主要对象是nsys在跟踪cuda kernel时无法捕获除rank 0以外的其他进程发起的调用。

https://github.com/vllm-project/vllm/issues/5131
该issue属于特性需求类型，主要涉及实现在Speculative Decoding verifier中使用典型接受采样作为一种替代采样技术。原因可能是为了通过Medusa的典型接受来提高接受率。

https://github.com/vllm-project/vllm/issues/5130
这个issue类型是需求类型，主要对象是持续集成（CI）/构建（Build）系统。这个问题是由于文件大小限制导致提交构建出错。

https://github.com/vllm-project/vllm/issues/5129
该issue属于bug报告类型，涉及的主要对象是`seq_lens_tensor`在model_runner.py中被重复定义。这可能是由于编码错误或重构失误导致的。

https://github.com/vllm-project/vllm/issues/5128
该issue是一个用户提出需求的类型，主要涉及的对象是VLLM和PreTrainedModel对象。用户希望能够使VLLM支持加载PreTrainedModel对象，以加速其在MOE-LoRA架构中的应用。

https://github.com/vllm-project/vllm/issues/5127
这是一个用户提出需求的issue，主要对象是 Triton GPTQ 实现，由于实现的差异性导致性能问题。

https://github.com/vllm-project/vllm/issues/5126
这是一个用户提出需求的issue，主要涉及VLLM在抽取式问答方面的应用。由于用户不清楚如何在每个提示中仅从上下文中选择令牌，才提出了这个问题。

https://github.com/vllm-project/vllm/issues/5125
这个issue类型是文档更新，主要对象是API reference和LLM entrypoints的文档。

https://github.com/vllm-project/vllm/issues/5124
这是用户提出需求的类型，主要对象是关于支持LLaVANeXT-Video项目的新模型。由于目前Hugging Face尚未支持该模型，用户希望知道是否会有支持该项目的计划。

https://github.com/vllm-project/vllm/issues/5123
这是一个bug报告，主要涉及的对象是generation process，问题可能是由于closing procedure导致的长时间卡顿。

https://github.com/vllm-project/vllm/issues/5122
这是一个bug报告类型的issue，涉及主要对象是添加gptq_marlin测试。这个问题可能是由于之前bug报告中遗漏了相关测试，导致未能完全覆盖bug情况，需要新增测试用例以验证和覆盖bug报告中的问题。

https://github.com/vllm-project/vllm/issues/5121
这是一个bug报告类型的issue，涉及的主要对象是gptq_marlin模型。这个issue的出现可能是由于模型在特定情况下产生了错误结果，需要针对这个bug进行测试来确认和修复问题。

https://github.com/vllm-project/vllm/issues/5120
这个issue是关于Bug修复的问题，主要涉及到SparseML Activation Quantization。原因是之前记录了许多关于输出维度+复制键的警告，但这是有意的。

https://github.com/vllm-project/vllm/issues/5119
这是一个Bug的报告，主要涉及 vLLM 的自动检测 sparseml 模型的问题。由于之前需要手动指定 quantization sparseml 部分，因此用户可能遇到了无法自动检测 sparseml 模型的困扰。

https://github.com/vllm-project/vllm/issues/5118
这是一个需求提出的issue，主要涉及要简化代码和修复类型注释，在合并后将启用`tests/`目录下的类型检查。

https://github.com/vllm-project/vllm/issues/5117
这是一个关于API使用问题的issue，用户在尝试通过OpenAI库请求模型时遇到了无法使用多个抽样参数的困惑，由于模型接受整数而不是列表作为抽样参数，导致了此问题的发生。

https://github.com/vllm-project/vllm/issues/5116
这是一个添加新功能的issue，主要涉及添加对`w4a16`模型的支持，包括更新`CompressedTensorsConfig`以及使用kerne等操作。

https://github.com/vllm-project/vllm/issues/5115
这是一个[Kernel]类型的issue，主要涉及为在paged attention kernel中添加对块大小96的支持。这个问题出现的原因是当前的kernel只支持8、16和32的块大小，导致无法支持使用96块大小的模型。

https://github.com/vllm-project/vllm/issues/5114
这是一个关于代码重构和功能扩展的 issue，主要涉及 CUTLASS kernels 中的 epilogue 定义，修复了与 CUDA Graphs 相关的一些问题，并且扩展了功能。

https://github.com/vllm-project/vllm/issues/5113
这是一个Bug报告，涉及到使用LLM entrypoint和LoRA适配器时偶尔出现崩溃的问题。原因可能是代码重试时有时会顺利通过。

https://github.com/vllm-project/vllm/issues/5112
这是一个[CI/Build]类型的issue，涉及到为amd服务器清理Docker功能。导致这个问题的原因是磁盘空间超过70％时需要清理Docker功能。

https://github.com/vllm-project/vllm/issues/5111
这是一个Bug报告，用户在运行vLLM示例代码时遇到了错误。

https://github.com/vllm-project/vllm/issues/5110
这是一个用户提出需求的issue，涉及到CI测试模板在AWS平台上的更新。

https://github.com/vllm-project/vllm/issues/5109
这是一个bug报告，涉及到vllm对ibm-granite 3b和8b codeinstruct模型产生空响应的问题，而20b和34b模型则产生了预期的响应。

https://github.com/vllm-project/vllm/issues/5108
这个issue是一个bug报告，涉及到gptq_marlin模型中的"g_idx_sort_indices"参数，并由于该参数被错误地视为需要从模型加载而引发了问题。

https://github.com/vllm-project/vllm/issues/5107
这是一个bug报告类型的issue，主要对象是vllm加载模型时出现的内存溢出问题。

https://github.com/vllm-project/vllm/issues/5105
这是一个Bug报告，涉及的主要对象是在部署Hermes2Promistral7b模型时使用多个Lora适配器。该问题的原因可能是在应用大量适配器负载时，出现了无法找到存储适配器位置的错误。

https://github.com/vllm-project/vllm/issues/5104
这是一个Bug报告，问题涉及主要对象是vllm库中的LLM类，由于未能清理内存使用量导致第二次调用`f()`时出现GPU内存不足错误。

https://github.com/vllm-project/vllm/issues/5103
这是一个文档更新类型的issue，涉及主要对象为 vllmnccl 模块。由于移除了 vllmnccl 模块，需要更新一些旧注释和命令，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/5102
这是一个bug报告，主要涉及VLLM在WSL2上运行时出现无响应的问题。原因可能是与CUDA 12.1和WSL环境相关的设置或兼容性问题。

https://github.com/vllm-project/vllm/issues/5101
这是一个需求提出的issue，主要涉及到对ModelRunner进行更改以支持具有不同输入签名的模型。由于需要支持模型的不同输入签名，所以需要对ModelRunner进行相应的修改和优化。

https://github.com/vllm-project/vllm/issues/5100
这个issue是关于提升OpenAI服务器测试设置的优化建议，不是bug报告。此问题涉及主要对象是对vLLM测试环境的设置逻辑和测试文件的重新组织。

https://github.com/vllm-project/vllm/issues/5099
这是一个功能改进类型的issue，主要对象是LLM引擎的Sequence模块。由于引入CC导致需要传递`prompt`和`multi_modal_data`参数，无论是否实际使用，因此需要简化构建测试中的dummy Sequence，解决相关症状。

https://github.com/vllm-project/vllm/issues/5098
这是一个功能改进类型的 issue，主要涉及到 vLLM 库中的工具函数和版本获取。

https://github.com/vllm-project/vllm/issues/5097
这是一个bug报告，主要涉及到`codespell`工具在使用`git diff`时无法正确跳过指定文件的问题。原因可能是与`codespell`本身的bug有关。

https://github.com/vllm-project/vllm/issues/5096
这个issue属于Bug报告，主要涉及的对象是`merge_async_iterators`函数，由于`python=3.9`下无法运行相关测试导致的问题。

https://github.com/vllm-project/vllm/issues/5095
这是一个bug报告类型的issue，涉及主要对象为vllm模型。由于Granite模型需要transformers升级，导致在vLLM中无法正确运行该模型。

https://github.com/vllm-project/vllm/issues/5094
这是一个bug报告，涉及在vLLM + Ray环境下无法运行分布式推断的问题。由于swap空间可能过大，导致出现错误提示和连接问题。

https://github.com/vllm-project/vllm/issues/5093
这是一个Bug报告。主要对象涉及DynamicNTKScalingRotaryEmbedding的实现。该问题出现的原因是动态NTK重新计算了所有长度的基本参数，导致与transformers的实现不一致。

https://github.com/vllm-project/vllm/issues/5092
这个issue是一个Bugfix类型的问题，主要涉及到`Sequence`在测试中传递的参数错误导致的问题。

https://github.com/vllm-project/vllm/issues/5091
这个issue类型是优化建议，主要涉及对象是vllm-nccl，并由于NCCL 2.19默认开启虚拟内存导致内存消耗过大，提示需要添加新环境变量解决问题。

https://github.com/vllm-project/vllm/issues/5090
这个issue是用户提出的一个需求，主要对象是添加vLLM CLI用于服务和查询OpenAI兼容服务器。由于PR描述部分需要选择适当的前缀以指示更改类型，但markdown渲染不起作用，因此需要使用原始html。

https://github.com/vllm-project/vllm/issues/5089
这是一个用户提出需求的类型，该问题单涉及的主要对象是在文档中添加Dropbox作为赞助商。

https://github.com/vllm-project/vllm/issues/5088
这是一个Bug报告，涉及到Gemma模型在使用quantization gptq_marlin时出现的RuntimeError问题，问题可能由于部分权重未从检查点中初始化导致。

https://github.com/vllm-project/vllm/issues/5087
这是一个用户提出需求的类型，该问题单涉及的主要对象是"skywork moe"。由于缺乏具体内容，无法确定具体问题是关于bug还是其他，并且无法分析导致的原因和具体问题。

https://github.com/vllm-project/vllm/issues/5086
这是一个关于安装问题的报告，主要涉及到 vllm 的安装。由于环境中使用的 CUDA 版本为 12.1，可能导致导入 LLM 时出现错误。

https://github.com/vllm-project/vllm/issues/5085
这是一个用户提出需求的issue，主要涉及在vLLM中采纳Colossal Inference 特性，希望实现性能提升，由于batched prefilling的性能问题导致速度不如预期。

https://github.com/vllm-project/vllm/issues/5084
这个issue类型属于Bug报告，主要涉及的对象是vLLM（Very Large Language Model）。问题是由于执行方法start_worker_execution_loop时出错，可能导致分布式执行中的死锁。

https://github.com/vllm-project/vllm/issues/5083
这是一个特性请求（Feature Request）issue，主要涉及OpenAI Triton后端的硬件兼容性问题，用户探讨在不同平台上运行Triton代码的可能性。

https://github.com/vllm-project/vllm/issues/5082
这个issue属于bug报告，涉及的主要对象是VLLM中的API服务。由于请求generate时指定的参数不正确，导致返回错误Not Found并出现KeyError异常。

https://github.com/vllm-project/vllm/issues/5081
这是一个关于GitHub上vLLM项目的issue，类型为添加新模型，主要涉及支持MAPNEO模型。这个issue的具体内容是支持MAPNEO模型，但由于markdown渲染不起作用，所以使用了原始HTML来展示。

https://github.com/vllm-project/vllm/issues/5080
这个issue类型为用户提出需求，涉及的主要对象是VLLM的量化选项。由于VLLM有多种不同的量化选项，用户在询问这些选项的区别，特别是对于Marlin选项的使用。

https://github.com/vllm-project/vllm/issues/5079
这是一个bug报告，主要涉及对象为vllm的模型使用中出现了关于gpu_memory_utilization参数设置不合适导致数值溢出的问题。

https://github.com/vllm-project/vllm/issues/5078
这是一个用户提出需求的issue，主要涉及vllm的离线批量推理以及LLama3-8b-Instruct模型使用Lora Adapter的情况。

https://github.com/vllm-project/vllm/issues/5077
这个issue是一个Bugfix类型的报告，主要涉及vLLM中的EOS token相关问题。导致这个bug的原因是在设置OpenAI API兼容服务器时，当前实现暴露了EOS token给用户，尽管API用户并不需要知道分词器的EOS token是什么，因此需要在不显式指定的情况下移除EOS token以解决这个问题。

https://github.com/vllm-project/vllm/issues/5076
这是一个bug报告，主要涉及vllm中的command-r-plus-gptq模型，由于GPU内存利用率很高但视频内存使用率很低，导致推理速度非常慢。

https://github.com/vllm-project/vllm/issues/5075
这个issue是关于Github上vllm项目中的一个bug修复。

https://github.com/vllm-project/vllm/issues/5074
这是一个用户提出需求的issue，主要对象是CUDA Graph，由于当前内存占用较高，用户建议增加输出缓冲区以减少内存占用。

https://github.com/vllm-project/vllm/issues/5073
该issue类型为功能需求提案，主要对象是为vllm添加性能基准测试的CI，由于需要对vllm的性能进行基准测试以及与其他替代方案进行比较，由此提出该需求。

https://github.com/vllm-project/vllm/issues/5072
该issue为性能相关的问题，主要涉及VLLM性能的提升建议，针对吞吐量、版本选择、并行计算等问题进行讨论。

https://github.com/vllm-project/vllm/issues/5071
这是一个bug报告，主要涉及问题出现在使用`pip install -e .`时的依赖和安装异常，其中一个是由于`urllib3`版本不匹配导致的依赖问题，另一个是安装过程中的异常问题。

https://github.com/vllm-project/vllm/issues/5069
这个issue是一个Model相关的问题，主要涉及支持falcon-11B模型的添加。该问题的出现可能是由于需要对falcon-11B模型进行支持。

https://github.com/vllm-project/vllm/issues/5068
这是一个Bug报告，涉及对象为测试案例 'microsoft/Phi-3-small-8k-instruct'，因为特殊标记可能导致崩溃。

https://github.com/vllm-project/vllm/issues/5067
这是一个bug报告，主要涉及了vLLM中计算log_probs时内存使用量未被考虑在profile运行中，导致CUDA内存溢出错误。

https://github.com/vllm-project/vllm/issues/5066
这个issue是一个Feature提案，主要涉及到vllm项目中的模型性能和状态管理的提升，需要将transformers库中的past_key_values集成到vllm项目的kvcache机制中，以提高模型的灵活性和性能。

https://github.com/vllm-project/vllm/issues/5065
这个issue类型是功能需求，主要涉及的对象是实现异构推理解码的功能。这个需求由于需要对CPU操作进行编译以及实现自定义操作的tensordriven分发，以实现异构推理解码。

https://github.com/vllm-project/vllm/issues/5064
这是一个针对在vllm中添加Internlm2 LoRA支持的模型问题。

https://github.com/vllm-project/vllm/issues/5063
这是一个用户提出需求的issue，主要涉及的对象是vllm中的guided decoding功能和正则表达式的使用。用户希望实现对指定key的初始字符串的强制限制，询问是否支持以及如何实现。

https://github.com/vllm-project/vllm/issues/5062
这是一个bug报告，涉及的主要对象是加载模型权重。由于当前环境中使用的PyTorch版本为2.1.2+cu121，可能导致加载模型权重时出现无限加载的问题。

https://github.com/vllm-project/vllm/issues/5061
这是一个bug报告，涉及的主要对象是vllm中的mistralai/Mistral-7B-Instruct-v0.3模型。由于设置了`add_prefix_space`参数导致了tokenizer需要从较慢的tokenizer进行转换，最终导致了错误消息中提到的问题。

https://github.com/vllm-project/vllm/issues/5060
这是一个Bug报告类型的issue，涉及的主要对象是vllm引擎。由于某种原因导致了"Background loop has errored already."这个错误的症状。

https://github.com/vllm-project/vllm/issues/5058
这是一个bug报告，主要涉及的对象是不同语言模型在Pascal环境下的性能表现不同，可能由于硬件或软件配置不同而导致AQLM正常工作，AWQ生成乱码，GPTQ运行缓慢的问题。

https://github.com/vllm-project/vllm/issues/5057
这是一个bug报告类型的issue，主要涉及"Cannot build cpu docker image"这个具体问题。由于当前环境的Docker不适用，导致用户无法按照CPU安装说明构建docker镜像。

https://github.com/vllm-project/vllm/issues/5055
该issue为需求提出类型，主要涉及改进GPUExecutorAsync中的execute_model_async功能，旨在减少调度成本以提高小模型的性能。

https://github.com/vllm-project/vllm/issues/5054
这是一个功能需求类型的issue，主要涉及到前端开发。由于OpenAI只有一个分词器可供所有模型使用，对于其他用户来说很难确定要使用哪个分词器，因此这个问题提出了添加分词/去分词端点的需求，以解决在终端应用程序中对令牌计数或其他令牌使用的问题。

https://github.com/vllm-project/vllm/issues/5053
这是一个关于性能改进的issue，不是bug报告。主要涉及的是优化外轮廓功能。

https://github.com/vllm-project/vllm/issues/5052
这是一个bug报告，涉及运行Vllm在ray集群中时日志加载卡在“loading”的问题。由于未能解决加载过程中的阻塞现象，用户提出了关于nvidia-smi、CUDA版本、Ray版本和尝试的其他解决方案的疑问。

https://github.com/vllm-project/vllm/issues/5051
这个issue属于功能需求类型，主要涉及的对象是添加一个名为`num_requests_preempted`的新指标。这个需求是为了解决现有指标无法提供关于GPU利用率的完整信息，以及提供高级调度器避免将新请求添加到低效GPU上的问题。

https://github.com/vllm-project/vllm/issues/5049
这是一个需求类型的issue，涉及到为offline LLM类应用chat模板，新加入了generate_chat()方法。

https://github.com/vllm-project/vllm/issues/5048
这个issue类型为安装问题报告，主要涉及vllm的安装环境和安装方式。由于用户安装vllm时遇到了问题或感到困惑，需要寻求帮助或解决方案。

https://github.com/vllm-project/vllm/issues/5047
这是一个技术更新类型的issue，涉及到使用`TORCH_LIBRARY`替代`PYBIND11_MODULE`来定义自定义PyTorch运算符。由于需要支持`torch.compile`在vllm模型上的使用，以及通过使用Python的稳定API来构建和链接，因此进行了相关修改和更新。

https://github.com/vllm-project/vllm/issues/5046
这个issue是关于版本更新，属于其他类型，主要涉及的对象是代码库的版本更新。

https://github.com/vllm-project/vllm/issues/5045
该issue类型是用户提出需求，涉及主要对象为日志记录系统。由于用户认为调试信息只能通过配置文件来开启不便（他从未编写过配置文件），因此提出添加一个环境变量来更改用户可见的默认日志记录级别。

https://github.com/vllm-project/vllm/issues/5044
这是一个bug报告类型的issue，主要涉及serving benchmark的用户体验问题，由于用户错误配置benchmark参数导致benchmark执行失败。

https://github.com/vllm-project/vllm/issues/5042
这是一个关于vLLM中批处理机制的讨论问题，用户想要了解在prefill和decode阶段中批处理是如何工作的，可能是在探讨算法实现方面的疑问。

https://github.com/vllm-project/vllm/issues/5041
这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM服务器在Kubernetes中实现更好的自动扩展和负载均衡，由于现有的metrics缺少一些关键指标，导致无法有效支持自动扩展vLLM服务器或在多个vLLM服务器之间更有效地分配负载。

https://github.com/vllm-project/vllm/issues/5040
这个issue类型为用户提出需求，涉及的主要对象是CI（持续集成）流程。由于该issue仅包含“draft”一词，推测用户可能想要草拟或起草一些内容，可能需要相关支持或指导。

https://github.com/vllm-project/vllm/issues/5039
这个issue是一个[Model]类型的PR，主要涉及的对象是vLLM中的Mixtral模型，它启用了FP8中的QKV并优化了内核调优脚本。

https://github.com/vllm-project/vllm/issues/5038
这是一个关于代码修改的 issue，类型为 Core 类型，涉及的主要对象是 LoRA embedding sharding，由于多个 LoRA 与分片状态加载器一起使用时出现问题，需要进行修复。

https://github.com/vllm-project/vllm/issues/5037
这是一个功能增强的issue，主要涉及的对象是vllm库中的动态逐标记激活量化功能。这个issue的提出是为了支持w8a8模型的动态逐标记激活量化，通过向CompressedTensorsScheme中添加CompressedTensorsW8A8DynamicToken方案来实现。

https://github.com/vllm-project/vllm/issues/5036
这是一个功能需求的issue，涉及到vLLM社区中的punica kernel以及与LoRA相关的问题，由于punica kernel无法支持特定维度导致问题。

https://github.com/vllm-project/vllm/issues/5035
这是一个bug报告，主要涉及VLLM在运行中报错，可能由于缺少CUDA驱动或者内存不足导致。

https://github.com/vllm-project/vllm/issues/5034
这是一个用户提出的需求问题，主要涉及代码构建过程，其原因是为了加快构建速度。

https://github.com/vllm-project/vllm/issues/5033
这是一个bug报告，主要对象是vLLM模型，问题是模型在回答提示时出现广告内容。

https://github.com/vllm-project/vllm/issues/5032
这是一个关于修复OpenAI `tools`和`tool_choice`参数支持以及OpenAI ChatCompletion LogProbs格式不正确的问题报告。

https://github.com/vllm-project/vllm/issues/5031
这是一个bug报告，涉及到vLLM项目中关于logprobs处理和采样的问题。由于logprobs处理不当导致了与OpenAI规范不兼容的错误。

https://github.com/vllm-project/vllm/issues/5030
这是一个bug报告，主要涉及的对象是"Command-R"功能，由于某些原因导致该功能输出不符合预期，显示了错误的内容。

https://github.com/vllm-project/vllm/issues/5029
这是一个Bug报告，主要涉及到vLLM项目中聊天日志（chat log）的logprobs格式错误引发的问题。

https://github.com/vllm-project/vllm/issues/5028
这是一个bug报告，该问题涉及VLLM在处理token时出现的差异。原因可能是在不同环境下对应token的处理方式不一致所致。

https://github.com/vllm-project/vllm/issues/5026
这个issue是一个bug报告，主要涉及的对象是前端代码。由于`finish_reason`的严格性导致了bug出现。

https://github.com/vllm-project/vllm/issues/5025
这个issue类型是潜在的功能增强请求，主要涉及SGMV Triton Kernels对多个Lora进行计算，旨在提高速度和处理能力。

https://github.com/vllm-project/vllm/issues/5024
这是一个bug报告，涉及LLAMA 3和VLLMOpenAI的集成问题，导致结果产生了错误的输出。

https://github.com/vllm-project/vllm/issues/5023
这是一个bug报告，涉及的主要对象是使用 Docker image vllm/vllmopenai:latest 运行 Mistral 7b inst v0.3 时出现了错误。

https://github.com/vllm-project/vllm/issues/5022
这是一个Bug报告，主要涉及的对象是变量'lora_b_k'。这个bug由于变量'lora_b_k'在赋值前被引用导致了UnboundLocalError。

https://github.com/vllm-project/vllm/issues/5021
这是一个与代码质量提升相关的需求，该问题主要涉及Core代码中的Helpers for PP功能，可能是为了提升代码的可维护性或性能方面的需求。

https://github.com/vllm-project/vllm/issues/5019
这是一个bug报告issue，涉及Marlin moe integration，由于markdown渲染不起作用，用户在提PR时遇到了问题。

https://github.com/vllm-project/vllm/issues/5018
这个issue类型是需求提出，主要涉及对象是LoRA支持的模型。由于没有定义LoRA支持模型的统一接口，需要添加一个基类来实现统一接口。

https://github.com/vllm-project/vllm/issues/5017
这是一个用户提出需求的issue，主要涉及到mypy类型检查在tests/目录下的应用问题。原因可能是为了提高代码质量和可靠性。

https://github.com/vllm-project/vllm/issues/5016
这是一个功能需求类型的 issue，涉及到 vLLM 中的 speculative decoding 和 chunked prefill 的结合，旨在平衡延迟和吞吐量优化。

https://github.com/vllm-project/vllm/issues/5015
这是一个功能需求类型的issue，主要涉及增加vLLM的接受率，因为当前的拒绝采样机制过于严格，可能会导致拒绝掉一些合理的潜在预测token。

https://github.com/vllm-project/vllm/issues/5014
这是一个bug报告，主要涉及vllm中使用openai api时推理结果与原始方式或批处理方式产生巨大差异的问题。

https://github.com/vllm-project/vllm/issues/5013
这是一个bug报告，涉及主要对象是在AMD Radeon PRO W7900 (gfx1100)上使用chunkedprefill功能时出现的hang。这个问题可能是由于在该GPU上使用python naive attention引起的。

https://github.com/vllm-project/vllm/issues/5012
这个issue类型为用户提出需求，主要对象是文档。用户希望在vllm的文档中加入有关如何使用`ccache`的指导。

https://github.com/vllm-project/vllm/issues/5011
这是一个bug报告，涉及的主要对象是在尝试在本地环境下运行vLLM时遇到的JSON文件错误，导致无法使用TheBloke/Mistral7BInstructv0.1GGUF模型。

https://github.com/vllm-project/vllm/issues/5010
这是一个需求提出类型的 issue，主要对象是添加新模型 tiiuae/falcon-11B 到 vllm 模型支持列表。由于新模型的架构细节和功能上的差异，用户希望支持这个新模型以提供更多多语言处理的潜力。

https://github.com/vllm-project/vllm/issues/5009
这是一个bug报告，涉及主要对象为Dockerfile.cpu文件，由于当前目录设置错误导致无法正确导入Python包，进而出现NameError: name 'vllm_ops' is not defined的问题。

https://github.com/vllm-project/vllm/issues/5008
这是一个Bug报告，主要涉及OpenAI服务中聊天完成日志概率格式不正确导致客户端无法正确解析答案。

https://github.com/vllm-project/vllm/issues/5007
这是一个关于性能问题的问题，主要涉及Vllm在L40s GPU上的性能表现，提出了性能改进的建议，但没有收到任何响应。原因可能是性能回归，导致性能下降。

https://github.com/vllm-project/vllm/issues/5006
这是一个bug修复类型的issue，主要涉及Outlines功能的性能问题，由于使用Outlines导致CPU使用率高，批处理吞吐量下降。

https://github.com/vllm-project/vllm/issues/5005
该issue为bug报告，涉及到Mistral v0.3 Weight Loading的问题，由于markdown渲染无法正常工作，因此需要使用原始的html。

https://github.com/vllm-project/vllm/issues/5004
这是一个改进代码组织的issue，主要涉及代码中的工具和测试代码。

https://github.com/vllm-project/vllm/issues/5003
该issue属于功能需求，主要涉及的对象是vLLM中的Tensor Parallelism功能。由于attention heads数量无法被3整除而导致无法运行模型，用户提出需要支持不整除的张量分布以解决这一问题。

https://github.com/vllm-project/vllm/issues/5002
该issue属于用户提出需求类型，主要对象是请求支持InternLM2ForCausalLM lora loading，可能由于缺乏该功能导致用户无法加载相关lora文件。

https://github.com/vllm-project/vllm/issues/5001
这是一个Bug报告，主要涉及到在H20 GPU上出现错误。这个问题可能由于PyTorch版本与CUDA版本不兼容导致。

https://github.com/vllm-project/vllm/issues/5000
这是一个bug报告类型的issue，主要涉及动态规格解码功能的改进和性能优化。由于即使达到禁用规格解码的条件也仍然执行了规格解码逻辑，导致禁用规格解码比不使用规格解码更慢，问题来源于控制流的广播。

https://github.com/vllm-project/vllm/issues/4999
这是一个bug报告，涉及的主要对象是vllm项目中的ray_gpu_executor.py文件。由于worker的IP等于driver的IP时，worker会被分配给driver_dummy_worker，导致一个worker未被调用在模型并行处理中。

https://github.com/vllm-project/vllm/issues/4998
这个issue是关于性能改进的提议，主要涉及到使用具有不同vRAM的多个GPU来分割模型的问题。由于不同GPU具有不同的vRAM，加载大型模型时可能导致OOM错误。

https://github.com/vllm-project/vllm/issues/4997
这个issue是关于bug修复的，主要涉及到vLLM中的ShardedStateLoader，由于没有正确保存使用`enable_lora=True`的模型导致问题。

https://github.com/vllm-project/vllm/issues/4996
这是一个bug报告，主要涉及VLLM在GPU P2P功能上出现问题导致无响应的情况。

https://github.com/vllm-project/vllm/issues/4995
该issue是一个特性需求，主要涉及的对象是lora与chunked prefill的集成。由于lora index逻辑中未考虑到不需要采样的情况，导致了lora无法与chunked prefill正常工作。

https://github.com/vllm-project/vllm/issues/4994
这个issue是一个bug报告，涉及主要对象是chunekd prefill功能。出现问题的原因可能是markdown渲染失败导致hang现象。

https://github.com/vllm-project/vllm/issues/4993
这是一个bug报告，该问题涉及的主要对象是vllm库。由于vllm目前还不支持microsoft/Phi-3-small-128k-instruct这个模型架构，导致出现数值错误(ValueError)。

https://github.com/vllm-project/vllm/issues/4992
这是一个类型为已完成的issue，主要涉及P2P访问检查的改进。

https://github.com/vllm-project/vllm/issues/4991
这个issue是一个小问题报告，主要涉及的对象是 llama.py 中的一个小错误，可能是由于疏忽导致了标记错误。

https://github.com/vllm-project/vllm/issues/4990
这是一个bug报告，主要涉及支持分片张量化模型的问题，由于内部线程池在设备只是'cuda'时似乎丢失了设备索引的跟踪，导致CUDA设备索引未能正确传递给TensorDeserializer。

https://github.com/vllm-project/vllm/issues/4989
这是一个bug报告类型的issue，主要涉及的对象是加载mistral-7B-instruct-v03模型时出现KeyError错误。造成这个bug的原因可能是模型加载过程中涉及到的某些参数或权重缺失或命名错误。

https://github.com/vllm-project/vllm/issues/4988
该issue类型为功能需求，主要对象是支持多个PP组，以帮助支持CUDAGraph。

https://github.com/vllm-project/vllm/issues/4987
这个issue类型是bug报告，主要对象是benchmarking serving模块，由于返回的index值为-1超出了范围，导致bug出现。

https://github.com/vllm-project/vllm/issues/4986
This issue is a [Model] type, focusing on adding a new model or improving an existing model. It involves implementing the Phi3vision model and testing the model weights loading. The low completion status is due to ongoing work on implementing and testing various components related to the Phi3vision model.

https://github.com/vllm-project/vllm/issues/4985
这是一个特性添加的issue，涉及的主要对象是vllm中的logits processor。由于需要在logits processor函数中包含prompt token IDs，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/4983
这是一个性能优化的issue，主要涉及Marlin 24的预填充功能，由于旧版本的代码导致内存和计算之间的重叠不足，降低了计算性能。

https://github.com/vllm-project/vllm/issues/4982
这是一个用户提出需求的issue，主要涉及的对象是新增模型DeepSeek VL。用户想要支持这个模型，询问了当前支持模型Llava的难度。

https://github.com/vllm-project/vllm/issues/4981
这是一个用户需求问题，主要涉及vLLM在特定GPU上启动的方法。由于无法成功启动第二个vLLM实例，用户询问如何在空闲GPU上启动。

https://github.com/vllm-project/vllm/issues/4980
这是一个bug报告，涉及vllm的安装问题。该问题可能是由于安装过程中出现的错误导致无法成功安装。

https://github.com/vllm-project/vllm/issues/4979
这是一个bug报告，涉及的主要对象是vLLM中的unsloth模型。由于unsloth模型在推理时输出结果异常，用户提出了询问unsloth模型是否受到vLLM v0.4.2支持的问题。

https://github.com/vllm-project/vllm/issues/4978
这个issue类型是功能需求提出，涉及的主要对象是vLLM的Medusa实现。用户提出了关于vLLM是否支持Medusa头部以及对支持新模型的需求。

https://github.com/vllm-project/vllm/issues/4977
这是一个bug报告类型的issue，主要涉及的对象是代码中的注释。这个问题的原因是注释没有在合并之前被移除，导致了markdown渲染不起作用的问题。

https://github.com/vllm-project/vllm/issues/4976
这是一个功能需求的issue，主要对象是关于logit processor和prompt token ids的使用。这个需求是为了增强logit processor的功能，使其能够根据不同的参数来处理prompt tokens ids和model-generated tokens。

https://github.com/vllm-project/vllm/issues/4975
这是一个Bug报告，主要涉及vllm库中的pagedattention运行错误。由于修改了pagedattention函数并添加了新的参数，导致了运行错误。

https://github.com/vllm-project/vllm/issues/4974
这个issue是关于一个bug报告，涉及主要对象是VLLM服务器。这个bug导致服务器在启动过程中卡住，最后的日志显示在'Using XFormers backend'这一行，可能是由于某些代码导致启动过程被卡住。

https://github.com/vllm-project/vllm/issues/4973
这个issue属于文档修复类型，涉及到的主要对象是GitHub上的vLLM项目。这个问题由于markdown渲染问题导致PR描述无法正常显示，需要使用HTML格式修复。

https://github.com/vllm-project/vllm/issues/4972
这是一个Bug报告，涉及的主要对象是vllm-0.4.2版本。由于环境配置错误导致运行时出现错误。

https://github.com/vllm-project/vllm/issues/4971
这是一个[Hardware][Intel]Optimize CPU backend and add more performance tips类型的issue，主要涉及优化CPU后端性能和添加性能提示。

https://github.com/vllm-project/vllm/issues/4970
这是一个bug报告，主要涉及的对象是Punica operator，由于找不到适合的kernel导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/4969
这是一个bug报告，主要涉及对象是vllm的LLMEngine，由于尝试在8-GPU机器上启动8个tp=1 LLMEngine导致GPU内存溢出。

https://github.com/vllm-project/vllm/issues/4968
这个issue是用户提出需求，主要对象是关于输出打印信息的时间统计优化。由于时间统计信息不清晰，用户无法计算第一个token时间和下一个token时间，需要修改时间单位以提高可读性。

https://github.com/vllm-project/vllm/issues/4967
这是一个功能需求提案，主要对象是关于支持`stream_options`选项的功能。

https://github.com/vllm-project/vllm/issues/4966
这是一个Bug报告，用户在使用vllm和gemma7b时遇到了速度缓慢的问题。

https://github.com/vllm-project/vllm/issues/4965
这是一个关于GitHub上vLLM项目中的issue，类型为技术改进，涉及使用pytorch sdpa数学后端替换ROCm中现有的简单注意力机制。

https://github.com/vllm-project/vllm/issues/4964
这是一个用户需求类型的issue，主要涉及到vllm的推理速度较慢。用户询问如何优化gemm7B在vllm中的推理速度，并寻求帮助。

https://github.com/vllm-project/vllm/issues/4963
这是一个关于输出格式问题的报告，主要涉及到logprobs的输出类型。由于logprobs生成的是List[Dict[int, float]]而非期望的[List[Dict[int, Logprob]]，用户希望知道如何获取token的字符串值而不是token id。

https://github.com/vllm-project/vllm/issues/4962
这是一个bug报告，主要涉及vllm中CPU worker的问题，由于attention backend仅通过版本字符串来确定是否返回与CPU兼容的attention backend，导致在安装完整版本（非cpu版本）时无法实例化CPU模型。

https://github.com/vllm-project/vllm/issues/4961
该issue属于bug报告类型，涉及到vllm的安装，由于setuptools 70.0.0版本的发布导致了安装失败的问题。

https://github.com/vllm-project/vllm/issues/4960
这是一个关于改进代码逻辑的Issue，主要涉及改进用户偏好设置和修复FlashInfer的问题。原因是当前逻辑在NVIDIA GPU上存在一些问题，导致用户无法正确设置偏好选项。

https://github.com/vllm-project/vllm/issues/4959
这是一个bug报告，涉及到FastAPI相关的问题。由于GPU内存不足导致了CUDA out of memory错误。

https://github.com/vllm-project/vllm/issues/4958
这个issue类型为用户提出需求，主要涉及的对象是microsoft/Phi-3-vision-128k-instruct模型。由于vllm在视觉支持方面落后，用户询问关于microsoft/Phi-3-vision-128k-instruct模型支持的情况。

https://github.com/vllm-project/vllm/issues/4957
这是一个功能需求相关的issue，主要对象是vLLM序列化模型和Tensorizer，用户提出了需要支持加载分片vLLM序列化模型至多个GPU的需求，由于目前的限制导致无法实现此功能。

https://github.com/vllm-project/vllm/issues/4956
这是一个新模型添加的issue，涉及的主要对象是fastspeech2_conformer模型，由于缺少一个新的attention mechanism导致需要添加。

https://github.com/vllm-project/vllm/issues/4955
这是一个用户提出需求的issue，主要涉及分布式推理后端的自动选择。由于当前针对单GPU和多节点推理的选择不够灵活，用户希望增加一个自动选择后端的功能以提升性能。

https://github.com/vllm-project/vllm/issues/4954
这个issue是关于修复CUTLASS kernels在CUDA graphs中的问题，属于Kernel类别。原因是需要将CUDA stream传递给CUTLASS GEMMs以避免未来出现CUDA graphs相关的问题。

https://github.com/vllm-project/vllm/issues/4953
这是一个bug报告，涉及对象是对于模型“microsoft/Phi3medium128kinstruct”的支持问题。根据报告，尝试在4*A6000服务器上使用该模型时遇到了错误。

https://github.com/vllm-project/vllm/issues/4952
这是一个关于代码修改的issue，主要涉及的对象是`create_lora_manager()`函数。导致该issue的原因是markdown渲染问题，需要使用原始的HTML代码来展示内容。

https://github.com/vllm-project/vllm/issues/4951
这是一个[Kernel]类型的issue，涉及的主要对象是为MI300X添加fused_moe Triton配置。这个issue主要是为了在提交PR前添加必要的配置。

https://github.com/vllm-project/vllm/issues/4950
这是一个关于安装过程中镜像大小过大的bug报告，用户提出需要减小容器镜像大小至5GB以下的问题。

https://github.com/vllm-project/vllm/issues/4949
这个issue类型是用户提出需求，主要对象是软件在构建速度上的优化。由于频繁合并的pull requests导致本地构建耗时长，用户提出希望能够获取预发布版本或夜间构建版本以加快构建过程。

https://github.com/vllm-project/vllm/issues/4948
这个issue是一个报告bug类型的问题，涉及到vLLM中markdown渲染无法正常工作的情况。原因可能是由于未正确填写PR描述，导致无法解决特定问题。

https://github.com/vllm-project/vllm/issues/4947
这个issue属于功能需求提升，主要涉及MLPSpeculator的引入和实现。

https://github.com/vllm-project/vllm/issues/4946
这是一个Bug报告类型的issue，主要涉及了VLLM（Very Large Language Model）模型加载过程中出现的错误。由于将模型配置文件中的torch_dtype更改为float16导致加载模型时出现错误，可能导致分布式执行中的死锁。

https://github.com/vllm-project/vllm/issues/4945
这是一个Bug报告，主要涉及到CI/Build中遇到的问题，由于未将`build/`目录添加到codespell的忽略列表中，当运行`./format.sh all`时会在codespell检查中失败并报告多个问题。

https://github.com/vllm-project/vllm/issues/4944
这个issue属于Bug报告类型，主要涉及Kernel，因为Phi2与LoRA导致attention head_size为80，Flash Attention不支持，导致loratest失败。

https://github.com/vllm-project/vllm/issues/4943
这是一个Bug报告类型的Issue，主要涉及的对象是MiniCPM-Llama3-V-2_5，由于不支持模型架构'MiniCPMV'，导致出现数值错误(ValueError)。

https://github.com/vllm-project/vllm/issues/4942
该issue属于提出需求的类型，主要涉及的对象是针对vLLM的ModelRunner子类，通过支持跨注意力和编码器序列来进一步支持编码器/解码器模型，由于需要对vLLM核心进行更改以适应交叉注意力机制来支持编码器/解码器模型的推理过程。

https://github.com/vllm-project/vllm/issues/4941
这是一个非bug类型的用户提出需求的issue，涉及到构建和持续集成过程中的设置问题。

https://github.com/vllm-project/vllm/issues/4940
这是一个Bug报告，主要涉及VLLM在Docker环境下运行时出现的CUDA错误问题。

https://github.com/vllm-project/vllm/issues/4939
这是一个[Feature]类型的issue，主要涉及支持HF Hub远程加载用于LoRA适配器的功能。由于当前代码不支持远程加载LoRA适配器，因此用户提出了需求并希望增加这一功能。

https://github.com/vllm-project/vllm/issues/4938
这是一个bug报告，主要涉及使用vllm时当tensor_parallel_size > 1时出现的问题。由于调用ray.init()多次导致程序卡住。

https://github.com/vllm-project/vllm/issues/4937
这个issue是针对添加idefics2模型的需求，属于[Feature]类型。

https://github.com/vllm-project/vllm/issues/4936
这是一个bug报告，主要涉及text_generation_router的infer函数在运行时出现了"no permits available"的错误提示。这可能是由于设置numprompts和requestrate参数不当导致的并发请求限制问题。

https://github.com/vllm-project/vllm/issues/4935
这个issue类型是Bug报告，主要涉及的对象是flag名称`max_seq_len_to_capture`，由于flag名称错误导致了bug。

https://github.com/vllm-project/vllm/issues/4934
这是一个Bug报告，主要涉及vllm中的`speculative decoding`功能，由于某些原因导致`shape mismatch`错误。

https://github.com/vllm-project/vllm/issues/4933
这个issue类型是bug报告，主要涉及的对象是性能和规范解码，导致症状可能包括不支持草稿模型在不同的张量并行大小比目标模型上。

https://github.com/vllm-project/vllm/issues/4932
该issue类型为文档需求，涉及前端vLLM中添加提示标记id到logit处理器的问题。出现这个问题是因为markdown渲染不起作用，所以使用了原始的html标记。

https://github.com/vllm-project/vllm/issues/4931
这是一个性能优化类的issue，主要涉及到的对象是FP8模块，由于删除了一个额外调用的核函数，导致了吞吐量提升了约2%。

https://github.com/vllm-project/vllm/issues/4930
这个issue属于用户提出需求，主要涉及的对象是Qwen2模型，由于缺少rope_scaling支持导致无法正确输出结果。

https://github.com/vllm-project/vllm/issues/4928
这是一个用户提出需求的issue，涉及主要对象是vllm的logit processor。用户提出希望能够在vllm的logit processor中获得prompt和生成token的token ID信息。

https://github.com/vllm-project/vllm/issues/4927
这是一个关于实现在moe层中启用融合topk_softmax kernel的GitHub issue。

https://github.com/vllm-project/vllm/issues/4926
这个issue是关于性能测试的[RFC]，主要涉及vLLM在不同工作负载上的性能跟踪。

https://github.com/vllm-project/vllm/issues/4925
这个issue类型为增加功能请求，主要对象是给vLLM项目添加感谢赞助商的部分。

https://github.com/vllm-project/vllm/issues/4924
这个issue是关于更新"setup.py"文件，属于文档修复类型，涉及到PR描述填写问题。由于markdown渲染失败，导致需要使用原始html代码。

https://github.com/vllm-project/vllm/issues/4923
这是一个bug报告，主要涉及的对象是vllm下的embedding model在使用tensor parallel时无法正常工作。这个问题可能是因为embedding模型并不适用于tensor parallel，导致了无法运行的情况。

https://github.com/vllm-project/vllm/issues/4922
这是一个构建/持续集成相关的问题，涉及的主要对象是 ROCm（Radeon Open Compute Platform），用户提出了将 Dockerfile.rocm 中的 ROCm 版本切换至 v. 6.1 的需求。

https://github.com/vllm-project/vllm/issues/4921
这是一个功能增强类型的issue，主要涉及MoE（Mixture of Experts）的调优和基准测试脚本改进。

https://github.com/vllm-project/vllm/issues/4920
这个issue属于文档需求类型，主要涉及benchmarking脚本和TGI docker的环境变量设置问题，用户提出需要添加文档说明和环境变量HF_TOKEN的设置。

https://github.com/vllm-project/vllm/issues/4919
该issue为一个通知类型，主要涉及vLLM Virtual Open Office Hours的安排。用户提出了关于参加Virtual Office Hours活动的需求和时间安排的问题。

https://github.com/vllm-project/vllm/issues/4918
这是一个功能需求类问题，主要涉及到Marlin警告提示问题。由于Marlin在较小的GPU上表现良好，因此需要移除关于m_block尺寸减小的警告。

https://github.com/vllm-project/vllm/issues/4917
这是一个bug报告类型的issue，主要涉及到vLLM多轮对话中自动前缀缓存功能在启用后未观察到性能改进的问题。

https://github.com/vllm-project/vllm/issues/4916
这是一个Bugfix类型的issue，主要涉及修复关于fp8格式的载入问题。原因是`torch.uniform_`目前不支持FP8，导致无法加载dummy weight。

https://github.com/vllm-project/vllm/issues/4915
这是一个Bug报告，涉及的主要对象是Phi3 LoRA模块加载失败。原因是加载的LoRA模块与预期的模块不匹配，导致数值错误。

https://github.com/vllm-project/vllm/issues/4914
这是一个Bug报告，涉及的主要对象是vLLM。这个问题是由于在某些带有安全保护的分布式系统中，当前逻辑无法正确找到可用的端口，导致NCCL Timeout错误。

https://github.com/vllm-project/vllm/issues/4913
这个issue是关于安装问题，涉及主要对象是vllm。由于CUDA编译器识别错误导致建立可编辑的过程失败。

https://github.com/vllm-project/vllm/issues/4912
这是一个bug报告类型的issue，涉及的主要对象是vllm库中的pydantic和transformers模块。这个问题由于版本冲突导致了部分模块无法成功导入，出现了RuntimeError。

https://github.com/vllm-project/vllm/issues/4911
这是一个bug报告，涉及的主要对象是关于`max_context_len_to_capture`和`max_seq_len_to_capture`的混淆。由于名称混淆，导致了这样的bug报告。

https://github.com/vllm-project/vllm/issues/4910
这是一个关于代码重构的issue，涉及LLaVA模型，旨在解决持续集成（CI）失败时定位问题的难题。

https://github.com/vllm-project/vllm/issues/4909
该issue属于用户提出需求类型，主要对象是vllm和其支持的模型。由于用户想要讨论是否vllm能够支持推断出一个支持800k长内容的70B模型，表明用户希望了解vllm对长内容推断的支持情况。

https://github.com/vllm-project/vllm/issues/4908
这是一个 bug 报告，主要涉及 vllm 库中的离线推理嵌入（offline inference embedding）功能。这个问题可能由于环境配置不正确导致。

https://github.com/vllm-project/vllm/issues/4907
这是一个bug报告，主要涉及`flashattn`的添加和删除，由于index计算时的整数溢出导致了错误。

https://github.com/vllm-project/vllm/issues/4906
这是一个Bug报告，主要涉及的对象是vllm下的FlashAttention-2 backend。由于flash_attn package未找到，导致无法使用FlashAttention2 backend，需要用户安装该package以获得更好的性能。

https://github.com/vllm-project/vllm/issues/4905
该issue是一个[CI/Build]类型的问题，涉及Marlin kernel构建条件性。用户提出了只有在gpu compute capacity >= sm_80时才支持GPTQMarlin，因此可以忽略该kernel构建。

https://github.com/vllm-project/vllm/issues/4904
这是一个bug报告，主要涉及`llm_engine_example.py`文件中更多请求导致程序卡住的问题，可能是由于大量请求时导致处理速度过慢而出现的。

https://github.com/vllm-project/vllm/issues/4903
这是一个bug报告，涉及到vLLM项目中关于custom all reduce nvlink检查的问题。这个问题由于`pynvml.nvmlDeviceGetHandleByIndex`在处理节点最大设备索引时可能会抛出错误，导致部署在多节点时出现错误。

https://github.com/vllm-project/vllm/issues/4902
该问题类型是需求提出，主要对象是支持Falcon-11B模型，用户提出需要增加Falcon-11B模型的支持，包括在fp16下的工作推断。

https://github.com/vllm-project/vllm/issues/4901
这是一个非bug报告类型的issue，主要涉及对marlin_24 GPU kernel进行单元测试。

https://github.com/vllm-project/vllm/issues/4900
这是一个用户需求，针对VLLM性能测试中需要分别对Prefill和Decode两个阶段进行独立性能测量的问题。

https://github.com/vllm-project/vllm/issues/4899
这是一个用户提出需求的issue，主要涉及到vllm在离线推断中通过轮廓强制生成json，并寻求相关文档和示例的帮助。

https://github.com/vllm-project/vllm/issues/4898
这是一个用户提出需求的issue，主要涉及测试用例的更新和添加新模型，可能因前期测试内容不足或模型升级而引起。

https://github.com/vllm-project/vllm/issues/4897
这是一个Bug报告类型的Issue，涉及的主要对象是vLLM的调度器（Scheduler）。造成这个问题的原因是一个小错误导致调度器在判断能够同时支持多少个LoRA设备时出现了偏差。

https://github.com/vllm-project/vllm/issues/4896
这是一个用户提出需求的类型。该问题单涉及的主要对象为vllm serving engine。原因是用户需要在vllm serving engine中添加类似OpenAI风格的批处理API。

https://github.com/vllm-project/vllm/issues/4895
这是一个包含bug修复和新功能需求的issue，主要涉及到v0.4.3版本的发布追踪。由于一些阻碍因素和美国节假日，预计发布日期推迟到5月30日。

https://github.com/vllm-project/vllm/issues/4894
这是一个技术改进的PR（Pull Request），旨在优化并简化并行工作任务调度的逻辑。

https://github.com/vllm-project/vllm/issues/4893
这是一个需求类型的issue，主要涉及FP8 kv-cache scaling factors的加载，由于目前无法在相应的模型上测试，需要更新指定模型以包含kvcache scaling factors。

https://github.com/vllm-project/vllm/issues/4892
这是一个用户提出需求的类型，主要对象是vllm项目的发布计划，用户想了解下一个版本计划发布的日期。

https://github.com/vllm-project/vllm/issues/4891
这是一个Bug报告，用户在环境中实例化`CohereForAI/c4ai-command-r-v01`模型时遇到了内存分配错误。

https://github.com/vllm-project/vllm/issues/4890
这是一个Bug修复类型的issue，涉及到vLLM对tiktoken版本的严格要求导致无法与最新版tiktoken一起使用，造成无法在相同环境中使用vLLM和支持OpenAI's GPT4o模型的最新tiktoken。

https://github.com/vllm-project/vllm/issues/4889
这是一个关于改进`ShardedStateLoader`功能的PR，属于[Core]类型，主要涉及到从HF下载checkpoint的功能修改。该改进是为了解决在本地文件系统中没有检查点时无法下载的问题。

https://github.com/vllm-project/vllm/issues/4888
该issue是关于功能需求的，主要涉及到vLLM中支持encoder/decoder模型的工作。这个问题的根本原因是要修改xFormers后端以支持交叉注意力操作。

https://github.com/vllm-project/vllm/issues/4886
这个issue是一个新模型添加请求，主要涉及到Phi-2 LoRA支持。原因是该功能尚未添加，用户想要为vLLM新增Phi2 LoRA支持。

https://github.com/vllm-project/vllm/issues/4885
这个issue属于bug报告类型，主要涉及的对象是vllm模型的最大长度验证函数，由于多个最大长度参数时选取的逻辑不正确导致bug。

https://github.com/vllm-project/vllm/issues/4884
该issue属于Bug报告类型，主要涉及的对象是VLLM对`tiktoken`版本要求过于严格，导致无法与最新版本的`tiktoken`同时使用，用户希望能放宽对`tiktoken`版本的要求。

https://github.com/vllm-project/vllm/issues/4883
这是一个bug报告，涉及的主要对象是vllm中的模型运行环境设置。问题可能是由于CUDA_VISIBLE_DEVICES参数设置不正确导致的assertion error。

https://github.com/vllm-project/vllm/issues/4882
这个issue属于用户反馈需求类型问题，主要涉及vllm中的gpu使用设置，由于可能存在设置问题导致无法正确配置gpu个数。

https://github.com/vllm-project/vllm/issues/4881
这是一个用户提出需求的issue，主要涉及vLLM的pip包安装方法更新问题。用户询问关于CPU后端的pip安装方式是否会更新，并表达了想要通过`pip install vllm=0.4.2+cpu`安装的意愿。

https://github.com/vllm-project/vllm/issues/4880
这是一个关于bug报告的issue，主要涉及的对象是在使用vllm进行tensor并行时所遇到的GPU内存使用问题，原因可能是tensor并行导致每个GPU的内存使用超出预期。

https://github.com/vllm-project/vllm/issues/4879
这是一个Bug报告，该问题涉及Vllm在处理请求时出错。原因是加载Lora checkpoint时出现错误导致所有处理请求失败。

https://github.com/vllm-project/vllm/issues/4875
这个issue是关于构建/CI的，涉及扩展AMD测试，不是bug报告。

https://github.com/vllm-project/vllm/issues/4874
这个issue是针对性能优化的PR，主要对象是vLLM模型的测试运行，由于模型测试运行时间过长而被中断，需要通过共享HuggingFace缓存来减少运行时间。

https://github.com/vllm-project/vllm/issues/4873
这个issue是一个需求提出类型的问题，涉及主要对象是为vLLM添加控制面板支持，由于Fastchat目前不支持最新的vLLM特性，导致了这个问题的提出。

https://github.com/vllm-project/vllm/issues/4872
这是一个bug报告，用户在使用vllm项目中的speculative decoding feature时，遇到了shape error。

https://github.com/vllm-project/vllm/issues/4871
这个issue属于文档更新类型，主要涉及到更新Ray Data分布式离线推断示例。这个issue是由于需要使用新的`ray_remote_args_fn` API来实现张量并行性，以进行批量推断操作，同时也涉及到Markdown渲染问题的原因所导致的。

https://github.com/vllm-project/vllm/issues/4870
这是一个关于优化vllm设置模式并使安装模式成为默认的issue，旨在提高最终用户友好性。

https://github.com/vllm-project/vllm/issues/4869
这是一个Bug报告，主要对象是CC([Misc] Enhance attention selector)，问题是由于`kv_cache_dtype`参数未正确传递给注意力后端的`__init__`方法而导致的bug。

https://github.com/vllm-project/vllm/issues/4868
这个issue类型是功能需求，该问题涉及的主要对象是CI/Build。由于缺乏健康检查功能，用户提出了添加健康检查以解决相关问题。

https://github.com/vllm-project/vllm/issues/4867
这个issue类型是功能需求报告，主要涉及的对象是vLLM的Dockerfile，用户提出了为了增加可靠性而添加健康检查的建议。

https://github.com/vllm-project/vllm/issues/4866
这是一个代码优化类的Issue，主要涉及到代码中的过期注释和`pynccl_backend`变量的使用问题。

https://github.com/vllm-project/vllm/issues/4865
这是一个用户提出需求的issue，主要涉及的对象是在环境中使用KubeRay进行多节点分布式推理的实现。该问题是关于如何在当前环境下实现多节点之间的RDMA。

https://github.com/vllm-project/vllm/issues/4864
这个issue是关于文档修正的，主要对象是文件命名。导致这个问题的原因是文件名中存在一个拼写错误。

https://github.com/vllm-project/vllm/issues/4863
这是一个关于性能优化和代码逻辑的问题，主要涉及vllm的flash-attn后端。原因是用户想了解`forward_decode`是否等待`forward_prefix`执行完毕后再运行，以及如何利用chunked-prefill实现更好的性能。

https://github.com/vllm-project/vllm/issues/4862
这个issue类型是bug报告，涉及的主要对象是CORS preflight requests。这个bug是由于浏览器不发送认证信息导致授权令牌无法通过预检请求发送到服务器而引发的。

https://github.com/vllm-project/vllm/issues/4861
这是一个用户提出需求的issue，主要涉及的对象是添加一个控制面板来管理多个vllm实例。由于vllm openai兼容服务器的快速发展，用户希望将fastchat中的控制面板功能移植到vllm中，以享受fastchat带来的最新特性。

https://github.com/vllm-project/vllm/issues/4860
这是一个关于代码优化的问题，用户询问为什么在打开前缀缓存后，在解码阶段优化PA内核时间成本，导致PA的时间消耗减少。

https://github.com/vllm-project/vllm/issues/4859
这个issue是用户（用户提出需求）提出希望添加`local_files_only`参数以在本地路径加载模型的功能。导致这个问题的原因是当前的代码需要连接到huggingface.co，用户希望能够避免这种远程连接。

https://github.com/vllm-project/vllm/issues/4858
这是一个bug报告，主要涉及对象是使用VLLM框架时遇到的CUDA错误，原因是在使用`device 'cpu'`时仍然引起了CUDA错误。

https://github.com/vllm-project/vllm/issues/4857
这是一个Bug报告类型的issue，主要涉及OpenAI Embedding Client的代码丢失问题，可能是由于版本更新或是文件缺失所导致。

https://github.com/vllm-project/vllm/issues/4856
这个issue类型是bug报告，涉及的主要对象是v0.4.1版本的VLLM_USE_MODELSCOPE无法正常工作，导致仍然从huggingface下载模型。

https://github.com/vllm-project/vllm/issues/4854
这是一个用户提出需求类型的issue，主要涉及的对象是将自行训练的模型适配到vllm上。原因是用户想要将他们训练的模型适配到vllm这一框架上。

https://github.com/vllm-project/vllm/issues/4853
该issue类型是用户提出需求，其中涉及的主要对象是如何确定在可接受时间段内支持多少并发请求，可能由于硬件环境信息的收集和系统配置导致用户希望了解如何利用演示API服务器来评估并发请求的支持能力。

https://github.com/vllm-project/vllm/issues/4852
这是一个bug报告，涉及主要对象为vLLM模型版本0.4.0.post1和0.4.2，用户在比较两个版本时发现最新版本速度变慢，希望了解为何出现这种现象。

https://github.com/vllm-project/vllm/issues/4851
这个issue是关于代码质量和审查效率的问题，主要涉及前端代码的修改，目的是跟踪`openai.run_batch`入口的使用情况。

https://github.com/vllm-project/vllm/issues/4850
这个issue是用户提出需求，主要对象是Kernel中的Qwen1.532B模型。问题由于Qwen1.532B模型需要支持LoRA，因此需要在Punica中增加额外的尺寸。

https://github.com/vllm-project/vllm/issues/4849
这是一个bug报告类型的issue，主要涉及的对象是vllm中的DeepSeekMath 7b模型，由于token长度超过模型限制而导致`AssertionError`，用户寻求解决方法。

https://github.com/vllm-project/vllm/issues/4848
该issue是一个功能需求，主要涉及benchmark_latency和benchmark_throughput的JSON输出支持。由于原功能缺乏JSON输出功能，为了更方便解析和聚合benchmark数据，用户提出了添加JSON输出支持的请求。

https://github.com/vllm-project/vllm/issues/4847
这是一个用户提出需求的issue，主要涉及benchmark_throughput和benchmark_latency两个指标，需要增加写入JSON文件的功能。原因可能是为了方便对结果进行聚合分析，避免解析日志数据。

https://github.com/vllm-project/vllm/issues/4846
这个issue是一个Bugfix类型的问题，主要涉及的对象是 vLLM 的 Prefix Caching Guards 更新版本，由于 markdown 渲染不起作用，所以在描述中使用了原始的 HTML 代码。

https://github.com/vllm-project/vllm/issues/4845
这个issue是一个bug报告，主要涉及vllm的triton autotune配置缺失，导致autotune选择了一个较差的配置。

https://github.com/vllm-project/vllm/issues/4844
获取错误，请重新获取

https://github.com/vllm-project/vllm/issues/4843
这个issue是关于如何将一个已经实例化的Hugging Face模型加载到vLLM进行推断的问题。

https://github.com/vllm-project/vllm/issues/4842
该issue是一个文档修复类型的问题，主要涉及的对象是vLLM的README文件。该issue的出现可能是由于markdown渲染不起作用，需要使用原始的HTML代码来突出显示第四次聚会。

https://github.com/vllm-project/vllm/issues/4841
这是一个需求提出类型的issue，主要涉及到添加一个新的kernel来融合fused-moe gemm中的去量化过程，由于该PR在DeepSpeed项目中的另一个PR的依赖关系，需要更新。

https://github.com/vllm-project/vllm/issues/4840
这个issue类型是bug报告，涉及的主要对象是vLLM中的Speculative decoding功能。由于torch.distributed未初始化时的广播绕过和使用2GPU运行器来运行集成测试，导致了一些测试失败的问题。

https://github.com/vllm-project/vllm/issues/4839
这是一个bug报告，主要涉及使用VLLM库时遇到的Neuron后端不支持缓存操作的问题。

https://github.com/vllm-project/vllm/issues/4838
这是一个需求特性提议，主要对象是Neuron docker image的构建和发布。由于当前的docker images不支持Neuron (Inferentia)，导致用户提出需要测试的、管理的Neuron docker image，并希望增加关于在容器中运行vLLm Neuron的文档的需求。

https://github.com/vllm-project/vllm/issues/4837
这是一个针对Github上vLLM项目的issue，主要涉及核心功能的改进，旨在支持encoder/decoder模型，并解决cross-attention KV缓存和内存管理问题。

https://github.com/vllm-project/vllm/issues/4836
这个issue是一个Bug报告，主要涉及的对象是vllm Docker image和neuron。由于缺少Ray模块导致无法运行，出现了启动失败的症状。

https://github.com/vllm-project/vllm/issues/4835
这个问题是关于bug报告，涉及主要对象是vllm中的模型加载过程。原因是由于不同数据类型共享相同的rope模块，导致加载不同数据类型的模型时出现错误。

https://github.com/vllm-project/vllm/issues/4834
这个issue类型为构建/持续集成问题，主要涉及到AMD入口点测试的启用。该问题的原因是要添加对OpenAI测试的支持，并更新了ray到最新版本，并设置了特定的环境变量。

https://github.com/vllm-project/vllm/issues/4833
这个issue类型为用户提出需求，主要对象是新增模型Google's Paligemma family of models。由于尚未得到回复，用户正在询问vllm团队是否有困难支持他们想要的模型。

https://github.com/vllm-project/vllm/issues/4832
这是一个用户提出需求的类型，主要涉及VLLM模型在混合模式CPU/GPU上运行的问题，因为用户希望能够在GPU内存有限的情况下运行LLM模型。

https://github.com/vllm-project/vllm/issues/4831
这是一个Bug报告。该问题涉及的主要对象是vllm模型在进行推理时结果出现错误。原因可能是vllm与Hugging Face 在使用相同的模型检查点时导致推理结果不正确。

https://github.com/vllm-project/vllm/issues/4830
该issue类型为硬件特定的变更需求，主要涉及到对Intel平台的LoRA适配器在CPU后端的支持。由于设备不满足`compute capacity >= 8.0`的要求，导致需要添加新的函数实现以支持`punica`内核的启动。

https://github.com/vllm-project/vllm/issues/4829
这是用户提出的需求类型的 issue，主要对象是支持在 Kubernetes 上使用 LWS 服务 vLLM，在多节点分布式推理中的部署。由于 LWS 社区有相关的示例，因此用户正在寻求关于如何使用 LWS 部署 vLLM 的帮助。

https://github.com/vllm-project/vllm/issues/4828
这是一则Bugfix类型的Issue，涉及到vLLM的模型加载器。问题是由于避免循环引入`LlavaForConditionalGeneration`造成的bug。

https://github.com/vllm-project/vllm/issues/4827
这是一个bug报告类的issue，主要涉及到VLLM中使用FP8 E5M2 KV Cache时GPU能力小于8.9导致的问题。

https://github.com/vllm-project/vllm/issues/4826
这是一个用户提出的问题，涉及如何将图像传递给vllm API 端点，可能是由于缺少在 JSON 主体中传递图像的字段而导致的。

https://github.com/vllm-project/vllm/issues/4825
这是一个关于如何在部署LLama3-8b时使用"tensor-parallel-size"参数的使用问题，用户提出了关于AsyncLLMEngine的疑问，主要关注在并行计算方面的问题。

https://github.com/vllm-project/vllm/issues/4824
该issue类型为功能需求提出，主要涉及对象是 qwen2（例如 Qwen2Attention），请求支持 rope_scaling 功能，以便使用 yarn/ntk 特性。

https://github.com/vllm-project/vllm/issues/4823
这是一个用户在询问关于性能问题的issue，主要涉及vLLM中的键值缓存（kv cache）和注意力计算的效率问题。用户关注在decode阶段的计算效率，其疑惑是键值缓存中的token是否非连续存储会导致内存拷贝耗时增加或计算效率降低。

https://github.com/vllm-project/vllm/issues/4822
这是一个Bug报告类型的issue，涉及主要对象是VLLM中的Llava模型。由于输出通常被截断并未完全显示，可能是由于长度截断导致的。

https://github.com/vllm-project/vllm/issues/4821
这是一个Bug报告，用户遇到了在NVIDIA RTX 4060 TI 16GB上运行Llama 3时出现内存溢出的问题。

https://github.com/vllm-project/vllm/issues/4820
这是一个关于bug报告的issue，主要涉及Lora 3 & 4测试出现非法内存访问失败的问题，可能是由于某次提交引起的。

https://github.com/vllm-project/vllm/issues/4819
这个issue是一个Bug报告，主要涉及的对象是Lora测试。由于最新的commit导致Lora测试失败，因此暂时将xformer优先级提高来进行测试。

https://github.com/vllm-project/vllm/issues/4818
这个issue是关于优化的需求，主要涉及到了graph mode函数的合并。由于在图形捕获过程中需要手动打开图形模式，在捕获外不需要，因此建议将这两个函数合并为一个。

https://github.com/vllm-project/vllm/issues/4817
这是一个用户提出需求的issue，主要涉及的对象是项目的README文件。由于添加第四次聚会的公告缺失，用户请求将其添加到README中。

https://github.com/vllm-project/vllm/issues/4816
这个issue是一个Bugfix类型的问题，涉及的主要对象是ParallelConfig类。导致这个问题的原因是`distributed_executor_backend`在`ParallelConfig`中没有根据命令行标志正确设置。

https://github.com/vllm-project/vllm/issues/4815
这个issue属于功能需求，主要涉及的对象是marlin unit tests和benchmarking code，用户提出要将这些代码从magic_wand中移植到marlin中。

https://github.com/vllm-project/vllm/issues/4814
这是一条bug报告，主要涉及如何在将token化的输入传递给模型之前删除EOS token，在句子末尾的EOS token导致模型生成与原输入类似但不一致的结果。

https://github.com/vllm-project/vllm/issues/4812
这是一个Bug报告类型的Issue，主要涉及于VLLM中的ArgumentHelper类缺少属性'enable_prefix_caching'，导致了CUDA driver版本不足的错误。

https://github.com/vllm-project/vllm/issues/4811
这是一个用户提出需求的issue，涉及主要对象是将已有模型转换为另一种格式，原因是当前模型无法被VLLM框架支持。

https://github.com/vllm-project/vllm/issues/4810
这是一个bug报告类型的issue，主要涉及到vllm的quantization优化问题，可能导致速度比非量化模型更慢。

https://github.com/vllm-project/vllm/issues/4809
这是一个bug报告，涉及到vllm下的一个issue中关于`VLMBase`类的添加和解决`ModelRegistry.load_model_cls()`循环引用错误的问题。

https://github.com/vllm-project/vllm/issues/4808
这是一个特性需求的issue，主要涉及vLLM中的speculative decoding功能。

https://github.com/vllm-project/vllm/issues/4807
这个issue是一个Bug报告，涉及的主要对象是vllm的ModelRegistry.load_model_cls()功能。由于Mistral (LlaMA) executor的循环导入关系导致了LlamaModel的导入错误。

https://github.com/vllm-project/vllm/issues/4806
这是一个性能优化的问题，主要涉及到Qwen 7b chat model在高并发环境下CPU利用率达到100%，而GPU利用率不高的情况。可能由于Python协程实现的调度和计算逻辑限制，导致CPU成为计算瓶颈，影响GPU的性能表现。

https://github.com/vllm-project/vllm/issues/4805
这是一个bug报告，涉及到在vllm环境中定义torch.nn.modules可能影响输出结果的问题，原因可能是模块定义的位置导致了不同的结果。

https://github.com/vllm-project/vllm/issues/4804
这是一个Bugfix类型的issue，主要涉及文档的CI failure问题，由于reST中存在重复的目标名称导致CI失败，通过将ref目标更改为匿名的方式进行修复。

https://github.com/vllm-project/vllm/issues/4803
这是一个性能优化的问题，涉及到如何正确测试tensorrt-llm serving，由于TTFT性能严重不佳，提出升级性能的建议。

https://github.com/vllm-project/vllm/issues/4802
这是一个关于性能问题的提案，主要涉及Deepseek-v2模型的性能优化，原因是实现了最新的Deepseekv2模型后性能较慢。

https://github.com/vllm-project/vllm/issues/4800
这是一个需求类型的issue，主要涉及的对象是文档`PoolingParams`。由于新的类`PoolingParams`被引入，需要添加与之相关的页面以完善文档信息。

https://github.com/vllm-project/vllm/issues/4799
这是一个需求提出的issue，主要涉及了Microsoft Phi3Small8K和Phi3Small128K模型以及blocksparse flash attention功能。

https://github.com/vllm-project/vllm/issues/4798
该issue属于文档更新类型，涉及添加以前的meetups到文档中。原因是文档渲染不工作，因此在此处使用原始HTML。

https://github.com/vllm-project/vllm/issues/4797
这是一个需求提出类型的issue，主要涉及扩展测试集包括Regression、Basic Correctness、Distributed、Engine 和 Llava Tests，目的是启用这些测试。

https://github.com/vllm-project/vllm/issues/4796
这个issue是关于文档修改，不是bug报告，主要涉及README文件中支持模型列表的缩短。

https://github.com/vllm-project/vllm/issues/4795
这是一个Bug报告，主要涉及对象是vLLM项目的`logprobs`参数与OpenAI规范不兼容导致的问题。

https://github.com/vllm-project/vllm/issues/4794
这个issue是关于前端的改动，支持OpenAI批处理文件格式。

https://github.com/vllm-project/vllm/issues/4793
这是一个bug报告，涉及主要对象是FP8量化过程。导致这个bug的原因是在加载FP16检查点后，权重数据类型已经被设置为FP8。

https://github.com/vllm-project/vllm/issues/4791
这个issue是一个增强性质的问题，涉及的主要对象是vLLM的构建过程。由于使用`setup.py bdist_wheel`构建vLLM wheel，以及未添加`nobuildisolation`标志导致构建依赖项在实际构建包之前被安装，从而导致构建过程较慢。

https://github.com/vllm-project/vllm/issues/4790
该issue类型为功能需求报告，主要涉及到在vllm中添加对GPTQ Marlin 2:4稀疏结构支持的功能更新。原因是为了能够运行2:4稀疏模型，并且目前已支持的配置包括group_size为128或1以及4位或8位。

https://github.com/vllm-project/vllm/issues/4789
这是一个Bug报告，涉及的主要对象是vllm的Async engine。由于0.4.*版本的更新，导致代码在执行时会hang，用户疑惑是代码存在问题还是vllm出现了bug。

https://github.com/vllm-project/vllm/issues/4788
这是一个需求类型的issue，主要涉及的对象是为gptq marlin kernel添加bfloat16支持。由于一些模型使用fp16推理时会发生溢出，因此需要为量化内核添加bfloat16支持。

https://github.com/vllm-project/vllm/issues/4787
该issue是关于对vLLM的支持长上下文LoRA的请求，需要增加批量化的rotary embedding kernel以提高多个长上下文长度LoRA的服务效率。

https://github.com/vllm-project/vllm/issues/4786
这是一个Bug报告，该问题涉及的主要对象是RAM OOM Error Loading 480GB MoE Model。由于RAM OOM错误导致加载480GB MoE模型失败，可能由于模型过大或者内存不足导致。

https://github.com/vllm-project/vllm/issues/4785
这是一个bug报告类型的issue，主要涉及到多GPU运行时出现的问题。由于PyTorch版本与CUDA运行时版本不匹配可能导致的bug。

https://github.com/vllm-project/vllm/issues/4784
这是一个bug报告，涉及的主要对象是部署Phi-3-mini-128k模型。由于PyTorch版本不兼容或配置错误导致了断言错误的部署问题。

https://github.com/vllm-project/vllm/issues/4783
这是用户提出需求的类型的issue，主要涉及如何在使用VLLM时修改批处理大小。用户在使用VLLM时想要运行特定模型的推理，但不清楚如何集成。

https://github.com/vllm-project/vllm/issues/4782
这个issue类型是bug报告，主要涉及到使用tensorizer_uri与LLM时的文档不正确，导致对应的代码片段无法正常工作。

https://github.com/vllm-project/vllm/issues/4781
这个issue属于功能需求，主要涉及bfloat16支持，由于某些模型在使用fp16推理时会溢出，需要为quantization kernel添加bfloat16支持。

https://github.com/vllm-project/vllm/issues/4780
这个issue是一个提交PR来启用pytest test_custom_all_reduce.py的问题，属于Misc类型。用户遇到的问题是在markdown渲染时无法正常显示，因此使用了原始的html格式。

https://github.com/vllm-project/vllm/issues/4779
这个issue属于Bug报告类型，主要涉及的对象是 vllm 的安装过程。由于某种原因导致用户在使用 `pip install vllm` 安装 vllm 时卡在了两个小时，请求解决该安装阻塞问题。

https://github.com/vllm-project/vllm/issues/4778
这是一个[Misc]类型的issue，主要涉及需要将'dtype'作为一个参数分离出来。由于markdown渲染问题，所以需要使用原始的html语法。

https://github.com/vllm-project/vllm/issues/4777
这是一个用户提出需求的issue，主要对象是支持OpenAI Batch Chat Completions文件格式。

https://github.com/vllm-project/vllm/issues/4776
该issue是 feature 请求类型，主要涉及的对象是 vLLM 的支持 bitsandbytes 量化和 QLoRA。原因是为了支持 QLoRA 在 vLLM 中的应用，实现对基础模型进行 bitsandbytes 4位量化，并配合低秩但高精度的 LowRank 权重矩阵生成输出。

https://github.com/vllm-project/vllm/issues/4775
这是一个功能需求，涉及到SequenceController在SamplingParams中的引入，用于在模型前向计算过程中处理logit token masks等功能。

https://github.com/vllm-project/vllm/issues/4774
这个issue是关于bug报告，主要涉及到qwen Moe模型的Hugging Face修改同步问题，由于在加载权重时出现异常，导致某些层的专家被截断，需要对代码进行进一步同步和修复。

https://github.com/vllm-project/vllm/issues/4773
这个issue属于用户提出需求类型，主要涉及的对象是vLLM在Mac环境下的构建和安装。由于macOS环境下暂不支持运行vLLM，用户希望能够在MacOS上导入vLLM用于开发，需要构建一个平台无关的wheel以提升开发体验。

https://github.com/vllm-project/vllm/issues/4772
这是一个Bug报告，主要涉及VLLM中的Unexpected Special Tokens in prompt_logprobs Output问题，可能由于环境配置或输入数据问题导致。

https://github.com/vllm-project/vllm/issues/4771
这是一个用户提出功能需求的issue，主要对象是为vllm提供CPU支持的Docker镜像，由于现有的Docker镜像只支持GPU而排除了部分用户，因此用户提出希望添加CPU支持的镜像来简化用户使用流程。

https://github.com/vllm-project/vllm/issues/4770
这是一个功能需求的issue，主要涉及vllm项目中的CI测试在NVLink启用的机器上进行测试。

https://github.com/vllm-project/vllm/issues/4769
这是一个功能需求提议，主要涉及Logits processor plugins，旨在提供支持自定义Logits处理器，以消除直接更改vLLM源代码的需要。

https://github.com/vllm-project/vllm/issues/4768
该issue是关于在vLLM的Kernel中添加对滑动窗口的支持，主要涉及到paged_attention_v1/v2 kernels，由于新的block manager需要在一个block的中间开始attention计算，否则会多计算一些token。

https://github.com/vllm-project/vllm/issues/4767
这个issue是关于提出需求，主要对象是对vllmnccl库的访问权限。由于路径限制，用户希望将libnccl.so文件迁移到conda环境的库目录中以便更便于在生产环境中使用。

https://github.com/vllm-project/vllm/issues/4766
这个issue是一个功能需求报告，主要涉及的对象是paged_attention_v1模型，用户希望该模型能够支持参数'attn_bias'。

https://github.com/vllm-project/vllm/issues/4765
这是一个 Bugfix 类型的 issue，主要涉及到 OpenAI 服务器中调用 init_logger 造成日志无法显示的问题。

https://github.com/vllm-project/vllm/issues/4764
这是一个bug报告，涉及主要对象是修复了块缓存中的前缀缓存问题，导致不正确删除块并在分配器中产生问题。

https://github.com/vllm-project/vllm/issues/4763
这个issue属于用户提出需求类型，主要涉及QServe库支持W4A8KV4 Quantization(QoQ)，由于QServe系统引入了一系列创新，包括W4A8KV4 Quantization技术，以提高LLM serving的效率和吞吐量。

https://github.com/vllm-project/vllm/issues/4762
这个issue是一个Bugfix类型的问题，主要涉及到修复了blockv2中前缀缓存的问题，之前未正确删除block导致了immutable allocator仍然可以获取已经释放的缓存块，这是由于在evictor中pop块时未正确删除缓存列表的追踪记录引起的。

https://github.com/vllm-project/vllm/issues/4761
这是一个Bug报告，涉及到VLLM中的blockv2前缀缓存问题，由于未正确删除block导致immutable allocator仍然能够获取已释放的“缓存”块，可能导致问题。

https://github.com/vllm-project/vllm/issues/4760
这是一个性能问题报告，主要涉及对象是RTX 3090显卡，用户反馈生成速度较低，并询问出现这种情况的原因。

https://github.com/vllm-project/vllm/issues/4759
这是一个bug报告，主要涉及到entrypoints测试运行过程中的内存泄漏问题。

https://github.com/vllm-project/vllm/issues/4758
这个issue是关于bug报告，涉及Chat Completions API中自定义角色功能的失效问题，原因是在版本升级中导致了这一功能的失败。

https://github.com/vllm-project/vllm/issues/4757
这是一个性能优化的issue，主要涉及的对象是实现快速广播功能的张量字典，并且因为减少广播次数从而提升了传输速度。

https://github.com/vllm-project/vllm/issues/4756
这是一个Bug报告，涉及主要对象是在使用mistral-7b + lora模型时，开启tensor parallelism=8会导致CUDA错误。

https://github.com/vllm-project/vllm/issues/4755
这是一个用户提出需求的issue，主要涉及vLLM在支持定制角色方面的问题，原因是升级到v0.4.2后发现定制角色不再被支持。

https://github.com/vllm-project/vllm/issues/4754
该issue属于功能需求类型，主要涉及到自定义allreduce功能的重构。原因是之前该功能仅绑定在world group上，通过此PR实现正确绑定到tp group，要求除去函数内的import并移除默认组None选项。

https://github.com/vllm-project/vllm/issues/4753
这是关于支持fp8 KV cache的问题，属于贡献代码的类型。

https://github.com/vllm-project/vllm/issues/4752
这个issue是一个提交pull request之前需要满足的检查清单，主要涉及到向vLLM的客户端API服务器添加TensorizerArgs。这个issue是用来帮助维护代码质量和提高审查过程效率的。

https://github.com/vllm-project/vllm/issues/4751
这个issue类型是功能增强，主要涉及Attention Selector的改进；原因是为了提供更多信息并移动kv_cache_dtype字段，为后续的解码过程使用flashattn做准备。

https://github.com/vllm-project/vllm/issues/4750
这是一个 bug 报告，主要对象是 custom allreduce 函数。这个问题的原因是函数名称错误导致症状，而用户提出了修改函数名称的需求。

https://github.com/vllm-project/vllm/issues/4749
这是一个关于在github上vllm项目中的Kernel部分添加w8a8 CUTLASS kernels的issue。

https://github.com/vllm-project/vllm/issues/4748
这是一个bug报告类型的issue，主要涉及到vLLM项目中初始化SeqGroup的问题，由于markdown渲染不起作用，导致需要使用原始HTML代码进行修复。

https://github.com/vllm-project/vllm/issues/4747
这个issue属于用户提出需求类型，主要涉及的对象是vLLM的使用方法。用户询问如何在特定任务上评估vLLM模型的logprobs，以及如何进行特定模型的推理。

https://github.com/vllm-project/vllm/issues/4746
这是一个用户寻求帮助的问题，主要涉及vllm框架与OpenAI服务器批量请求接口的问题，原因是v1/chat/completions端点不支持批处理，用户想知道如何实现请求的批处理。

https://github.com/vllm-project/vllm/issues/4744
这是一个bug报告类型的issue，主要涉及vllm在4-GPU实例上使用AutoAWQ时无法利用GPU的问题，用户寻求帮助解决GPU利用率为0%的现象。

https://github.com/vllm-project/vllm/issues/4743
这是一个关于需求改进的issue，主要涉及的对象是vLLM中关于模型量化的功能。这个问题是由于默认的量化设置限制用户无法轻松自定义量化设置，需要修改配置文件来实现。

https://github.com/vllm-project/vllm/issues/4742
这是一个Bug报告，涉及的主要对象是在使用LoRA模型时出现数值错误（ValueError），可能是由于PyTorch版本2.3.0与其他环境参数不兼容所导致的问题。

https://github.com/vllm-project/vllm/issues/4741
这是一个Bug报告，主要涉及的对象是squeezeLLM 4bit量化模型过程中遇到的问题。由于某些原因导致了无法使用sparse的情况下，调用LLM模块时出现了错误。

https://github.com/vllm-project/vllm/issues/4740
这个issue是关于bug报告，涉及主要对象是vllm版本0.4.1和0.4.2，由于tensor_parallel_size为4时，导致logits在不同版本之间表现不同。

https://github.com/vllm-project/vllm/issues/4739
这是一个功能需求提出的issue，主要涉及到vllm模型对Blip2模型的支持问题。由于Blip2模型具有不同的架构，目前vllm还不支持该模型。

https://github.com/vllm-project/vllm/issues/4738
这个issue类型是功能改进，主要涉及的对象是项目的构建和版本控制，用户想要通过使用setuptools-scm从git直接获取版本信息，以便正确构建Dockerfiles。

https://github.com/vllm-project/vllm/issues/4362
这是一个bug报告，涉及的主要对象是VLLM_USE_MODELSCOPE环境变量。由于代码中判断条件修改后导致环境变量无法正常工作。

https://github.com/vllm-project/vllm/issues/4361
这个issue属于bug报告，主要涉及的对象是vllm的FlashAttention backend，原因是flash_attn包未找到，导致无法使用该backend。

https://github.com/vllm-project/vllm/issues/4360
这是一个关于安装问题的bug报告，涉及的主要对象是vllm的安装过程，由于环境缺乏internet访问导致安装过程中获取github文件失败。

https://github.com/vllm-project/vllm/issues/4359
这是一个用户提出需求的issue，涉及主要对象为GPTQ/AWQ模型的量化优化问题，可能由于未完全优化导致速度比非量化模型慢。

https://github.com/vllm-project/vllm/issues/4358
这个issue是一个Bug报告，主要涉及的对象是vllm中的RayGPU backend，由于RayGPU backend目前不支持Speculative decoding导致了该错误。

https://github.com/vllm-project/vllm/issues/4357
这是一个功能需求的issue，主要涉及到添加用于基于多进程worker的工具的功能。这个需求可能是为了增加多进程支持以提高性能。

https://github.com/vllm-project/vllm/issues/4356
这是一个关于在GitHub上的vLLM项目中前端APIs添加动态LoRA模型加载/卸载的issue。

https://github.com/vllm-project/vllm/issues/4355
这是一个Bug修复类型的Issue，涉及到Pydantic models for OpenAI API。由于原API允许额外字段导致的无效输入不会引发错误，此问题提出更新Pydantic模型以在调用API时出现额外字段时引发错误。

https://github.com/vllm-project/vllm/issues/4354
这个issue是一个需求提出，涉及对象是vllm的多GPU支持，由于未知原因需要强制启用eager模式。

https://github.com/vllm-project/vllm/issues/4353
这是一个特性集成的issue，涉及的主要对象是将flashinfer集成到解码阶段。由于需要改进如使用flashinfer的rope embedding和支持alibi_slope等功能所致。

https://github.com/vllm-project/vllm/issues/4352
该issue为改进建议类，主要涉及到代码逻辑的重构。

https://github.com/vllm-project/vllm/issues/4351
这个issue是一个[Core]类型的问题，涉及到对AQLM的量化操作进行重构。

https://github.com/vllm-project/vllm/issues/4349
这是一个功能需求提出的issue，主要涉及的对象是`ExecutorBase`。这个需求的提出是为了解决`ExecutorBase`无法被垃圾回收的问题，以便引入`MultiprocessingGPUExecutor`。

https://github.com/vllm-project/vllm/issues/4348
这是一个功能需求添加的issue，主要涉及对象是`DistributedGPUExecutor` abstract class，原因是为了引入`sibling MultiprocessingGPUExecutor`做准备。

https://github.com/vllm-project/vllm/issues/4347
这是一个bug报告，主要涉及将`ray_utils.py`文件从`engine`移动到`executor`包导致了类型检查错误的问题。

https://github.com/vllm-project/vllm/issues/4346
这个issue类型是用户提出需求，主要涉及对象是vllm的`max_model_len`和`max_position_embeddings`参数。由于用户误解了参数限制的作用，导致提出了关于预填序列长度限制问题和如何集成特定模型进行推理的疑问。

https://github.com/vllm-project/vllm/issues/4345
这是一个功能需求的issue，主要涉及的对象是在单节点多GPU环境下使用多进程执行器。这个issue由于需要引入`MultiProcGPUExecutor`作为tensor并行的替代方案而产生。

https://github.com/vllm-project/vllm/issues/4344
这是一个bug报告，该问题涉及到MistralAI/Mixtral-8x22B-Instruct-v0.1模块的加载问题，出现了在第一、第二次尝试加载时失败，直到第三次尝试加载时才成功的情况，可能是由于某种异常的竞争导致的。

https://github.com/vllm-project/vllm/issues/4343
这是一个优化性能的issue，涉及主要对象是MoE kernel / Mixtral，用户提出了关于优化FP8支持的问题。

https://github.com/vllm-project/vllm/issues/4342
这是一个用户提出需求的 issue，主要涉及于将 vLLM 中的 `linear_method` 通用化为 `quant_method`，以实现各模块自定义量化逻辑。

https://github.com/vllm-project/vllm/issues/4341
这是一个bug报告类型的issue，涉及到vllm Docker container在WSL2上出现NCCL错误的问题，可能是由于版本迁移或环境配置问题引发的。

https://github.com/vllm-project/vllm/issues/4340
这是一个用户提出需求的类型，主要涉及到Docker用户和NCCL库访问权限的问题，由于`/root/.config/vllm/nccl/cu12/libnccl.so.2.18.1`文件对其他用户不可访问，用户建议在文档中说明如何更改权限。

https://github.com/vllm-project/vllm/issues/4339
这个issue属于bug报告类型，主要涉及vLLM在输入无效数据时不返回错误的问题，可能由于数据输入错误导致无法准确处理，用户在提出改进的请求。

https://github.com/vllm-project/vllm/issues/4338
这是一个bug报告，主要对象是vllm 0.4.1版本。由于vllm尝试从github下载软件包，导致无法与pip 517兼容。

https://github.com/vllm-project/vllm/issues/4337
这是一个关于在Mypy中对lora文件夹进行类型标注的问题，不是关于bug报告。

https://github.com/vllm-project/vllm/issues/4336
这个issue是一个Bugfix类型的issue，主要涉及vLLM引擎的日志统计功能出现问题，导致使用`engineuseray`时日志输出为0。

https://github.com/vllm-project/vllm/issues/4335
这个issue类型为bug报告，涉及的主要对象是`ActorHandle`对象，由于设置`engineuseray`时导致`'ActorHandle'`对象没有属性`decoding_config`，因此出现了bug。

https://github.com/vllm-project/vllm/issues/4334
这是一个关于需求提议的issue，主要涉及vLLM中动态设置RoPE缩放参数的问题，用户在自己的代码中无法动态设置这些参数。

https://github.com/vllm-project/vllm/issues/4333
这是一个bug报告，主要涉及的对象是vllm==0.4.0版本的模型权重无法找到问题。由于升级到最新版本后出现了无法找到模型权重的错误，用户寻求帮助解决该问题。

https://github.com/vllm-project/vllm/issues/4332
这是一个需求提议的issue，主要涉及于支持在 vllm 中加载序列化的 fp8 模型以及使用静态和动态权重和激活尺度。

https://github.com/vllm-project/vllm/issues/4331
这是一个bug报告，主要涉及的对象是vllm支持的模型。由于代码中存在"AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight'"错误，导致了启动错误。

https://github.com/vllm-project/vllm/issues/4330
这是一个升级软件版本的issue，主要涉及软件功能的修复和改进。原因可能是当前版本存在一些问题或需要新增功能。

https://github.com/vllm-project/vllm/issues/4329
这是一个功能需求问题，涉及的主要对象是logger功能。原因可能是用户希望在logger中增加额外的信息。

https://github.com/vllm-project/vllm/issues/4328
这是一个API变更的issue，主要涉及LLM的prompt参数的整合，通过整合参数减少用户传入不同长度的prompt、prompt_token_ids和multi_modal_data的机会，以及方便使用HuggingFace processor定义输入处理接口。

https://github.com/vllm-project/vllm/issues/4327
这是一个Bug报告，主要涉及的对象是VLLM项目下的Quantization方法fp8在当前GPU不受支持导致数值错误的问题。

https://github.com/vllm-project/vllm/issues/4326
这个issue类型是bug报告，主要对象是vLLM的logging模块。由于非延迟字符串操作被启用，导致性能下降，可能影响vLLM的性能，用户提出需要禁用这些操作。

https://github.com/vllm-project/vllm/issues/4325
该issue是一个功能需求，涉及实现tree attention用于加速并行解码，主要对象是解码过程中的注意力机制。

https://github.com/vllm-project/vllm/issues/4324
这个issue是关于bug修复的，涉及的主要对象是OLMo模型。导致bug的原因是由于在transformers 4.40.0中hf_olmo和某些key之间的冲突导致OLMo模型部署异常。

https://github.com/vllm-project/vllm/issues/4323
这是一个bug报告，主要涉及Phi-3 (microsoft/Phi-3-mini-128k-instruct) 中的一个bug，由于当前环境为 docker 0.4.0.post1，导致 phi-3 (microsoft/Phi-3-mini-128k-instruct) 在 rope_scaling 出现 assert "factor" 失败的问题。

https://github.com/vllm-project/vllm/issues/4322
这是一个bug报告，涉及到Flash Attention插件，由于环境配置问题导致Flash Attention功能无法正常工作。

https://github.com/vllm-project/vllm/issues/4321
这是一个bug报告，涉及的主要对象是OpenAI serving。由于async初始化方法被移除，导致当访问`chat_template`时会出现AttributeError的bug。

https://github.com/vllm-project/vllm/issues/4320
这是一个关于代码质量的issue，主要涉及到CUDA图支持以及相关测试。导致此issue可能是关于之前代码的缺陷或未通过的测试。

https://github.com/vllm-project/vllm/issues/4319
这是一个提出需求的issue，主要对象是CI（持续集成），导致这个问题的原因是为了及早发现调整CC([Misc] Reduce supported Punica dtypes)的问题。

https://github.com/vllm-project/vllm/issues/4318
这是一个用户提出需求的issue，主要涉及到代码优化，避免重复造轮子。

https://github.com/vllm-project/vllm/issues/4317
这是一个bug报告，主要涉及vllm项目中的服务器启动过程中出现的属性错误问题。

https://github.com/vllm-project/vllm/issues/4316
这是一个关于特性实现疑问的issue，主要涉及vLLM中的continuous batching特性的具体实现方式。用户提出了关于该特性具体实现方式的问题，可能由于文档缺乏详细说明而导致用户困惑。

https://github.com/vllm-project/vllm/issues/4315
这是一个Bug报告，主要涉及的对象是vLLM项目中的dbrx模块。这个问题可能是由于dbrx模型本身发生变化，导致vLLM需要新的transformers，使得在某些系统上无法正常运行dbrx。

https://github.com/vllm-project/vllm/issues/4314
这是一个用户提出需求的类型的 issue，该问题主要涉及 vllm 的 offline Engine 和 batched streaming inference 功能。由于当前的 AsyncEngine 不支持这个功能，用户希望能够实现对模型性能的离线批处理流式评估。

https://github.com/vllm-project/vllm/issues/4313
这是一个bug报告，涉及到vllm的编译和安装问题。由于代码中出现了TypeError导致的问题。

https://github.com/vllm-project/vllm/issues/4312
这是一个bug报告，涉及的主要对象是vllm下的一个issue，由于某个版本的代码在运行时出现了错误。

https://github.com/vllm-project/vllm/issues/4310
这是一个Bug报告，涉及主要对象是OLMo模型部署。原因是在Transformers 4.40.x中，由于hf_olmo和transformers模块的key冲突导致的OLMo模型部署失败。

https://github.com/vllm-project/vllm/issues/4309
这是一个关于代码重构和功能改进的issue，主要涉及vLLM中采样器和支持提示日志概率的改动，希望解决chunked prefill情况下的问题。

https://github.com/vllm-project/vllm/issues/4307
这是一个bug报告，涉及的主要对象是修复了在AQLM合并中破坏的fp8接口。这个问题由于fp8接口在AQLM合并中导致了错误，需要进行修复。

https://github.com/vllm-project/vllm/issues/4306
这是一个bug报告，涉及的主要对象是要支持Phi-3模型。由于Phi3的config.json与之前模型不同，导致无法支持LongRope功能，产生了无法运行Phi3模型的问题。

https://github.com/vllm-project/vllm/issues/4305
这是一个功能需求提出的issue，涉及主要对象是vLLM中的Outlines模块，用户要求能够定义whitespace pattern。

https://github.com/vllm-project/vllm/issues/4304
这是一个PR（Pull Request），涉及的主要对象是Punica库。这个问题是为了减少二进制文件大小而减少Punica支持的dtype组合。

https://github.com/vllm-project/vllm/issues/4303
这是一个用户提出需求的issue，主要涉及的对象是实现并支持批量并行解码。由于需要在vllm上提高推理效率，在高请求率情况下，使用草案模型/基于树的验证可能会带来额外的开销，影响服务延迟。

https://github.com/vllm-project/vllm/issues/4302
这是一个建议性质的issue，主要涉及使用库切换的建议。

https://github.com/vllm-project/vllm/issues/4301
这是一个bug报告类型的issue，主要涉及到vllm工具在使用fine-tuned Mistral7Bv0.1模型时出现`ValueError: Cannot find the config file for awq`错误，可能是由配置文件缺失或路径设置错误导致的。

https://github.com/vllm-project/vllm/issues/4300
这是一个文档问题，涉及在 benchmarks/benchmark_throughput.py 中使用公共API的问题，由于 markdown 渲染失败，所以使用原始的 html 代码。

https://github.com/vllm-project/vllm/issues/4299
这是一个准备构建每次提交的wheels和容器的问题单，类型为功能需求。

https://github.com/vllm-project/vllm/issues/4298
这是一个关于在vLLM模型中添加Phi-3支持的issue，主要涉及的对象是模型。由于添加新的支持层和优化了层名称映射，导致需要对代码进行改动和测试。

https://github.com/vllm-project/vllm/issues/4297
这是一个bug报告类型的issue，涉及的主要对象是VLLM中的LLama 3 8B Instruct模型。由于`tokenizer.json`文件中指定的停止令牌与LLama 3 Instruct模型所需的停止令牌不匹配，导致了这个问题。

https://github.com/vllm-project/vllm/issues/4296
这是一个Bug报告，涉及的主要对象是vllm下的OpenAIServingChat模块。由于tokenizer对象在完全初始化之前被访问，导致AttributeError错误。

https://github.com/vllm-project/vllm/issues/4295
这个issue类型是验证功能实现（speculative decoding）的正确性，主要对象是CUDA graphs的支持情况。

https://github.com/vllm-project/vllm/issues/4294
这是一个Bug报告类型的Issue，主要涉及到程序中调用了未定义的函数'ncclGetVersion'导致错误。

https://github.com/vllm-project/vllm/issues/4293
这是一个Bug报告，主要涉及vllm 0.4.1版本在部署llama3后出现的引擎迭代超时错误。

https://github.com/vllm-project/vllm/issues/4292
这是一个bug报告类型的issue，涉及到前端代码。导致这个bug的原因是当提供给`chattemplate`的值看起来像一个文件路径，但文件打不开时，会将文件名作为文本聊天模板读取，导致模型无法正确生成输出。

https://github.com/vllm-project/vllm/issues/4290
这是一个功能需求提议，目标对象是通过启用支持Pascal GPU来实现更好的性能。

https://github.com/vllm-project/vllm/issues/4289
这个issue是关于一个未完成的Infrastructure for encoder/decoder支持的工作内容，不是bug报告。

https://github.com/vllm-project/vllm/issues/4288
这个issue是用户提出需求类型的，主要涉及的对象是在使用vllm-0.2.1.post1版本时遇到了无法安装最新版本vllm的问题，希望了解如何在旧版本中添加llama3或修改代码以解决。

https://github.com/vllm-project/vllm/issues/4287
这是一个Bug报告，主要关于vLLM在CPU上抛出asyncio.exceptions.CancelledError的问题。产生这个问题的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/4286
这个issue是一个Bugfix类型的问题，涉及的主要对象是RayWorkerWrapper。导致这个问题的原因是RayWorkerWrapper缺少了`init_cached_hf_modules`方法，使得在`trust_remote_code`中无法使用。

https://github.com/vllm-project/vllm/issues/4285
这个issue属于bug报告类型，主要涉及vllm的模块错误导致运行时出现ModuleNotFoundError。

https://github.com/vllm-project/vllm/issues/4283
这是一个关于文档更新的issue，主要涉及SkyPilot部署文档中的缩进问题和自动扩展功能，可能由于Markdown渲染问题导致指示错误。

https://github.com/vllm-project/vllm/issues/4282
这是一个Bug报告，涉及到了vllm代码库中P-Tuning_V2 fine-tuning模型的支持问题，并且由于加载权重时出现KeyError导致了错误。

https://github.com/vllm-project/vllm/issues/4281
这个issue是一个功能增强请求，主要对象是CI（持续集成）。原因是为了通过增加ccache来提高wheel构建作业的响应速度。

https://github.com/vllm-project/vllm/issues/4280
这是一个关于性能优化的issue，主要涉及到代码中的`get_max_num_running_seqs`函数，通过对其进行优化来提高性能。

https://github.com/vllm-project/vllm/issues/4279
这是一个非bug报告类型的issue，主要涉及到数据集采样代码的更新，用户在此寻求修改函数以确保两个benchmark结果的可比性。

https://github.com/vllm-project/vllm/issues/4278
这是一个bug报告，主要涉及日志记录功能，由于进程挂起后返回到一个函数时无法准确定位进程位置，导致用户需要手动分析调用链，通过在日志文件中添加`from`和`to`信息来改善调试问题。

https://github.com/vllm-project/vllm/issues/4277
这是一个Bug报告，涉及的主要对象是使用vllm项目在 llm3 70b 模型上使用 LLVM 时出现了性能问题，导致模型在实例化时超时。

https://github.com/vllm-project/vllm/issues/4276
这是一个[Doc]类型的issue，涉及更新SkyPilot文档以添加Llama3和scalingup说明。因为markdown渲染不起作用，所以使用原始html。

https://github.com/vllm-project/vllm/issues/4275
这是一个bug报告，涉及vllm_ops未定义的问题，可能由于环境配置或代码实现问题导致。

https://github.com/vllm-project/vllm/issues/4274
这是一个bug报告，涉及的主要对象是Open AI server，由于CPU only engine部署时出现了kernel错误导致请求无法成功响应。

https://github.com/vllm-project/vllm/issues/4273
该issue是一个用户提出需求的类型，涉及到Rework logger以便提供pythonic自定义日志配置的能力。原因是为了让用户能够以更符合Python规范的方式通过logging.config.dictConfig注入自定义日志配置，以扩展vLLM的功能。

https://github.com/vllm-project/vllm/issues/4272
这个issue是文档更新类型，主要涉及到修复`autodoc`指令的问题。由于使用了不正确的指令和选项以及未更新的导入路径，导致了类的文档未能正确显示和函数缺失。

https://github.com/vllm-project/vllm/issues/4271
这是一个bug报告类型的issue，涉及到vllm项目中关于使用绝对路径来解决nccl库完整性检查问题，由于`ldd`需要库的全路径，在rocm上导致了问题。

https://github.com/vllm-project/vllm/issues/4270
这个issue是关于性能修复的bug报告，主要涉及到vLLM的调度器（Scheduler），由于logger.debug中fstring的求值导致调度迭代的额外开销增加，从而影响了调度器的整体性能。

https://github.com/vllm-project/vllm/issues/4269
这个issue类型是功能需求，主要涉及的对象是项目中的 standalone_api_server。

https://github.com/vllm-project/vllm/issues/4268
这是一个bug报告，主要涉及vllm在AMD集群上出现断言失败的问题，导致了在加载模型后出现了由llvm引发的低级错误。

https://github.com/vllm-project/vllm/issues/4267
这是一个用户提出需求的issue，主要对象是AMD CI pipeline，旨在扩展在AMD硬件上运行的测试集。

https://github.com/vllm-project/vllm/issues/4266
该issue属于用户提出需求类型，主要涉及vllm中支持speculative model的功能。用户对vllm中关于speculation的支持提出了疑问，并希望了解如何使用此功能。

https://github.com/vllm-project/vllm/issues/4265
这是一个关于使用特定GPU的需求问题，用户在vllm中想要选择特定的GPU运行模型而遇到了困难。

https://github.com/vllm-project/vllm/issues/4264
这个issue属于用户提出需求，主要涉及的对象是AMD CI pipeline，用户希望在AMD硬件上运行更多的测试。

https://github.com/vllm-project/vllm/issues/4263
这是一个Bug报告，涉及VLLM中使用CUDA图时可能导致进程挂起无法退出的问题。这可能是由于CUDA图在VLLM中的使用出现了问题。

https://github.com/vllm-project/vllm/issues/4262
这是一个Bug报告类型的issue，涉及主要对象是使用最新docker镜像加载CohereForAIc4ai-command-r-plus模型时出现崩溃，可能由于参数设置或环境配置不当导致。

https://github.com/vllm-project/vllm/issues/4261
这个issue是一个Bug修复类型的，主要涉及Scheduler性能退化的问题，由于在调度器迭代PR后，日志记录功能导致了性能下降。

https://github.com/vllm-project/vllm/issues/4260
该issue属于用户提出需求类型，需要在serving models api_server.py中添加"eos_token_id"参数终结符。由于新的LLama3模型使用不同的终结符，需要在API中明确定义这一终结符以解决角色解析不准确的问题。

https://github.com/vllm-project/vllm/issues/4259
这个issue是一个bug报告，涉及的主要对象是nccl check，由于缺少fallback策略导致用户无法加载默认的libnccl.so文件，需要解决这一问题。

https://github.com/vllm-project/vllm/issues/4258
这是一个bug报告，问题涉及到使用lora时在Python 3.8上出现的TypeError。这个bug导致了TypeError错误的出现。

https://github.com/vllm-project/vllm/issues/4257
这是一个Bug报告类型的Issue，主要涉及的对象是ncclGetVersion函数。这个问题是因为在NGC官方镜像中找到了nccl库但ldd命令检查失败，导致了未定义ncclGetVersion的bug。

https://github.com/vllm-project/vllm/issues/4256
这是一个bug报告，涉及主要对象为CPU model runner。由于在之前的变更中引入了类型注解错误导致这个bug。

https://github.com/vllm-project/vllm/issues/4255
这个issue类型是需求提出，主要对象是关于"chunked prefill"的支持问题。由于缺乏文档支持，用户询问是否现在已支持这一功能。

https://github.com/vllm-project/vllm/issues/4254
这是一个用户提出需求的issue，主要涉及vllm的HTTP请求处理，用户想要在请求中包含SamplingParams参数。这可能导致当前用户无法通过aiohttp发送请求获取所需的信息。

https://github.com/vllm-project/vllm/issues/4253
这是一个关于性能问题的问题单，用户在运行0.5B模型时发现GPU内存占用异常高，希望寻求更加内存高效的运行方式。

https://github.com/vllm-project/vllm/issues/4252
这是一个Bug报告，主要涉及Benchmark Serving.py计算出来的ttft和latency异常，可能是由于环境信息中PyTorch版本号为2.2.1+cu121导致的。

https://github.com/vllm-project/vllm/issues/4251
这个issue是一个Bug报告，涉及到模型Qwen72B在Docker环境下无法正确结束对话，导致出现重复打印信息的问题。

https://github.com/vllm-project/vllm/issues/4250
这是一个用户提出需求的issue，主要涉及vllm无法支持diverse beam search功能，用户希望vllm能够实现这一功能。

https://github.com/vllm-project/vllm/issues/4249
这个issue类型是bug报告，主要对象是async engine。由于问题出现在使用speculative decoding时，可能导致async执行出现问题。

https://github.com/vllm-project/vllm/issues/4248
这个issue类型是功能优化，主要涉及到nccl初始化过程中关于unique id的处理。

https://github.com/vllm-project/vllm/issues/4247
这是一个Bug报告，涉及的主要对象是VLLM（Very Large Language Model），该问题是由于代码中的KeyError错误导致的。

https://github.com/vllm-project/vllm/issues/4246
这是一个用户提出需求的类型，主要涉及的对象是FlashAttention后端。由于FlashAttention v2不支持Turing GPU，用户请求支持FlashAttention v1.0.9以减少vram的使用。

https://github.com/vllm-project/vllm/issues/4245
该issue类型为需求或者工作管理，主要涉及到Upstream sync操作。原因可能是需要将某个项目同步至上游仓库，但具体内容并未提供。

https://github.com/vllm-project/vllm/issues/4244
该issue是一个关于在MoE核心/Mixtral中支持FP8的问题。

https://github.com/vllm-project/vllm/issues/4243
这是一个关于性能问题的问题，主要涉及使用SFT模型在vllm上推断速度较慢的情况，原因可能是未正确配置vllm导致。

https://github.com/vllm-project/vllm/issues/4242
这是一个bug报告，主要涉及在同一GPU上加载多个模型的问题。由于尝试在GPU上启动第二个模型时出现错误，导致产生了报错。

https://github.com/vllm-project/vllm/issues/4241
这是一个bug报告类型的issue，主要涉及到vLLM在多GPU设置下使用Ray时出现的内存泄漏问题，用户提到当前环境下无法通过ray.shutdown()完全清除所有GPU的内存，可能是由于ray.remote装饰的类不支持max_calls参数导致的。

https://github.com/vllm-project/vllm/issues/4240
这个issue类型是bug报告，主要涉及的对象是VLLM中的流式请求。由于用户需要在流式请求时停止生成令牌，因此寻求如何中止流式请求的解决方案。

https://github.com/vllm-project/vllm/issues/4239
这个issue类型是用户提出需求，主要对象是如何在使用 llama-3 时应用特殊模板，由于直接使用提示语句导致异常响应，用户想要了解是否需要特殊模板以及如何支持所需的模型。

https://github.com/vllm-project/vllm/issues/4238
这是一个Bug报告，主要涉及的对象是VLLM的GPU利用率以及性能问题，用户提出了GPU利用率低和性能低于预期的问题。

https://github.com/vllm-project/vllm/issues/4237
这个issue类型是用户提出需求，主要涉及的对象是添加ngram prompt查找解码功能。由于直接从提示获取草稿，因此不需要另一个模型或修改后的模型来获取提案，这是最便利的享受推测加速的方式。

https://github.com/vllm-project/vllm/issues/4236
这个issue是关于bug报告，涉及的主要对象是模型注册更新。这个问题由于`_MODEL_REGISTRY`在`model_loader.py`中被移除，需要更新注册模型的文档。

https://github.com/vllm-project/vllm/issues/4235
这个issue类型是文档更新需求，涉及更新添加新模型页面的内容；这个问题由于代码变更导致文档需要更新注册模型部分的内容。

https://github.com/vllm-project/vllm/issues/4234
这个issue是关于功能需求的，主要涉及VLLM中的LORA适配器加载问题，用户想知道是否能在不为每个新训练的LORA重新启动基础模型的情况下根据需要加载LORA适配器。

https://github.com/vllm-project/vllm/issues/4233
这是一个bug报告类型的issue，主要涉及的对象是vllm的Core模块和Distributed模块。由于`pynvml`工作在物理设备ID上，而不是在`CUDA_VISIBLE_DEVICES`中的相对索引上，导致了`_is_full_nvlink`函数无法正确查询nvlink信息，需要使用真实的物理设备ID来解决。

https://github.com/vllm-project/vllm/issues/4232
这是一个Bug报告，主要涉及的对象是VLLM模型。该问题由于Attention头数目不可被张量并行大小整除而导致。

https://github.com/vllm-project/vllm/issues/4231
这个issue类型是用户提出需求，主要对象是pytest。由于缺少importlib选项，pytest可能会继续使用本地路径进行模块导入，而不是使用已安装的路径。

https://github.com/vllm-project/vllm/issues/4230
这个issue是关于需求提出的，主要对象是beam search功能，用户希望添加温度参数以增强抽样过程的可选性。

https://github.com/vllm-project/vllm/issues/4229
这是一个bug报告，涉及的主要对象是vLLM环境下的代码运行。这个问题的原因是`vllm_ops`未被定义导致了`NameError`。

https://github.com/vllm-project/vllm/issues/4228
这是一个提议添加新模型到项目的issue，主要对象是moondream vision语言模型。原因是该模型需要被添加到项目中以提供支持。

https://github.com/vllm-project/vllm/issues/4226
该issue属于用户提出需求类型，主要对象是vllm下的集成模型控制器面板支持。由于vllm不断增加LLM支持特性，导致用户提出需要一个类似fastchat控制器功能的解决方案来与模型工作 looselycoupled，并动态加入和注册到控制器的服务器后端。

https://github.com/vllm-project/vllm/issues/4225
这个issue属于功能需求提出，主要涉及的对象是文档生成，由于当前文档中未包含嵌套/多文件示例，并提出将示例脚本改写为笔记本形式以实现更好的展示。

https://github.com/vllm-project/vllm/issues/4224
这是一个Bug报告，主要涉及到vLLM在多用户环境下的NCCL定位机制问题。问题的症状是vLLMmanaged NCCL .so文件只安装给安装vLLM的用户，导致其他用户无法访问，可能是由于NCCL检测机制只考虑单用户环境而导致。

https://github.com/vllm-project/vllm/issues/4223
该issue属于bug报告类型，涉及主要对象是`EngineArgs`，由于默认值在flag类型参数上显示可能导致用户误解需要为其传递数值，因此用户提出了停止显示flag类型参数默认值的改进需求。

https://github.com/vllm-project/vllm/issues/4222
这个issue类型属于功能改进，涉及更新lmformatenforcer版本和在文档中添加解码库链接。这个改进主要是为了提高性能和用户理解参数含义。

https://github.com/vllm-project/vllm/issues/4221
这是一个用户提出需求的issue，主要对象是VLLM模型的部署情况。由于一些较大的模型（如grok）无法在单台机器上部署，用户提出了支持高效多节点部署的需求。

https://github.com/vllm-project/vllm/issues/4220
这是一个bug报告，主要涉及的对象是vllm/outputs.py中的RequestOutput.from_seq_group方法，由于设置了错误的索引值导致在OpenAIServingChat.chat_completion_stream_generator中引发了IndexError。

https://github.com/vllm-project/vllm/issues/4219
这是一个bug报告的issue，主要涉及文档缺失和不同步的`EngineArgs`，导致开发者文档遗漏和有关引擎参数的问题。

https://github.com/vllm-project/vllm/issues/4218
这是关于bug修复的issue，主要涉及Marlin kernel在H100上的崩溃问题，原因是使用了inline PTX assembly引入了异步复制功能。

https://github.com/vllm-project/vllm/issues/4217
这是一个bug报告，涉及主要对象为loader.py文件中的download_weights_from_hf函数。导致此问题的原因是revision参数没有传递给download_weights_from_hf函数，导致加载特定版本的模型时出现不匹配问题。

https://github.com/vllm-project/vllm/issues/4215
这是一个用户提出需求的issue，主要是针对vllm中beam search模式不足的问题。由于目前实现的beam search版本过于简化，导致生成文本输出的创造性受到束缚，用户希望能够在beam search中设置更多选项来提升生成输出的创造性。

https://github.com/vllm-project/vllm/issues/4214
这是一个Bug报告，涉及的主要对象是获取tokenizer时未能传递`tokenizer_revision`，导致docker镜像无法加载自定义版本的tokenizer。

https://github.com/vllm-project/vllm/issues/4213
该issue类型为功能需求，主要涉及设置重置节点GPU功能，用于解决"HIP outofmemory"情况，保证测试之间独立性，避免前一作业影响当前作业性能。

https://github.com/vllm-project/vllm/issues/4212
这是一个性能优化建议，主要涉及到vllm项目中的bonus tokens的重新启用。

https://github.com/vllm-project/vllm/issues/4211
这是一个需求类型的Issue，主要涉及的对象是与nccl相关的包信息。由于项目转移到vllm管理的nccl，需要收集`nccl`相关的包信息。

https://github.com/vllm-project/vllm/issues/4210
这是一个bug报告，涉及vllm性能退化问题，原因可能是CC([3/N] Refactor scheduler for chunked prefill scheduling)导致了吞吐量降低。

https://github.com/vllm-project/vllm/issues/4209
这是一个bug报告，涉及OpenAI API Server中的令牌速率显示问题。

https://github.com/vllm-project/vllm/issues/4208
这是一个类型为性能优化的github issue，主要涉及到前端的vLLM模块和`tensorizer`的更新，用户提出了自动检测vLLM-tensorized模型和更新`tensorizer`版本的需求。

https://github.com/vllm-project/vllm/issues/4207
此issue是一个bug报告，主要涉及到无法找到ptxas，可能是由于CUDA环境配置问题导致的。

https://github.com/vllm-project/vllm/issues/4206
这是一个用户需求报道，主要涉及 vllm 库在 Pypi 上版本发布的时间问题。由于用户在 VPN 受限的环境下无法通过克隆源码安装，只能通过 Pypi 安装，但目前版本不可用，用户询问新版本 v0.4.1 何时会发布到 Pypi。

https://github.com/vllm-project/vllm/issues/4205
这个issue属于用户报告bug类型，主要涉及的对象是LLAVA server的请求。由于PyTorch版本与CUDA不匹配，导致用户无法成功连接LLAVA server。

https://github.com/vllm-project/vllm/issues/4204
这是一个文档问题报告，涉及VLLM引擎参数文档缺失的问题。原因是缺少了Lora引擎的参数信息，导致相关内容未被正确展示在文档页面中。

https://github.com/vllm-project/vllm/issues/4203
这个issue是用户提出需求，寻求关于如何在AsyncLLMEngine中使用LoRARequest的帮助。

https://github.com/vllm-project/vllm/issues/4202
这是一个Bug报告，主要对象涉及到GPU的quantization method。导致该问题的原因是当前GPU的最低兼容性要求高于当前环境所支持的版本，导致无法使用awq量化方法。

https://github.com/vllm-project/vllm/issues/4201
这是一个 bug 报告，涉及安装 vllm 时出现构建失败的问题，主要对象为安装 vllm 的环境。这个问题可能是由于 Python 版本与 CUDA 版本不兼容引起的。

https://github.com/vllm-project/vllm/issues/4200
这是一个用户提出需求的issue，主要涉及支持GPT-4V的图像输入功能。由于需要对OpenAI Chat Completions API进行相应调整以支持该功能，导致该issue的提出。

https://github.com/vllm-project/vllm/issues/4199
这是一个功能增强的issue，主要涉及LLaVANeXT模型的初始支持，由于输入图像大小受固定配置限制，可能导致无法充分利用额外分辨率优势。

https://github.com/vllm-project/vllm/issues/4198
这个issue描述了一个bug，主要涉及的对象是vllm下的SqueezeLLM模型，可能由于某些原因导致了出现类似于之前报告bug的情况但仍然失败的错误。

https://github.com/vllm-project/vllm/issues/4197
这是一个关于支持图像处理器的技术问题，涉及主要对象为vLLM中的多模态数据处理，由于数据类型转换错误导致的bug已被修复。

https://github.com/vllm-project/vllm/issues/4196
这是一个性能优化的issue，主要对象是eager mode的运行效率，由于某个函数导致主机时间延迟，影响了运行性能。

https://github.com/vllm-project/vllm/issues/4195
这是一个Bug报告，主要涉及到在使用docker构建的环境中出现的问题。 由于VLLM升级了torch的版本到2.2，可能导致了在第二个环境中无法运行`python m torch.utils.collect_env`的错误。

https://github.com/vllm-project/vllm/issues/4194
这个issue类型为需求提出，主要涉及vLLM项目中的多模态支持。根据内容，用户提出了对vLLM V1引擎重构中多模态模型性能优化的需求。

https://github.com/vllm-project/vllm/issues/4193
这是一个Bug报告，主要涉及到vLLM工具在使用时由于共享 outlines 缓存数据库导致的磁盘I/O错误问题。

https://github.com/vllm-project/vllm/issues/4192
该issue类型为功能需求，主要对象为更新docker以支持llama3模型。此问题可能是由于llama3模型与vllm模型最接近，但vllm模型目前尚未提供支持导致的。

https://github.com/vllm-project/vllm/issues/4191
这个issue是一个文档修复问题，涉及到Github上vLLM项目中的文档内容格式。可能由于markdown渲染问题，导致文档中的html标签未能正确显示。

https://github.com/vllm-project/vllm/issues/4190
这个issue是一个功能需求，涉及的主要对象是EngineArgs和SchedulerConfig对象。原因是需要添加一个控制最大排队时间的参数，以避免超出最大排队长度导致错误或者返回错误503的情况。

https://github.com/vllm-project/vllm/issues/4189
这个issue是一个Bugfix类型的问题，主要涉及到JSON whitespace相关的bug，原因是未正确重置CFGFSM导致了allwhitespace问题。

https://github.com/vllm-project/vllm/issues/4188
这是一个bug报告，主要涉及的对象是LLama 3基础生成模块，问题出现在LLama 3基础生成模块没有根据传入的停止词停止生成。

https://github.com/vllm-project/vllm/issues/4187
这是一个bug报告，主要涉及到vllm的CUDA错误问题。原因可能是缺少CUDA内核图像导致的运行时错误。

https://github.com/vllm-project/vllm/issues/4186
这是一个Bug报告，涉及的主要对象是Mixtral 8x7b模型结合lora时出现的错误。用户尝试包含lm_head层在lora中，在代码vllm/lora/utils.py中，但是出现了不支持或忽略该层的问题。

https://github.com/vllm-project/vllm/issues/4185
这个issue是关于用户提出需求，请求添加vLLM openai server中的状态监控API。

https://github.com/vllm-project/vllm/issues/4184
这个issue属于性能讨论类，探讨了在部署vllm服务时使用两种不同的方法所涉及的性能问题。由于两种部署方法的不同，用户疑惑哪种方法更优，并请求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/4183
这是一个优化代码的issue，主要涉及WorkerWrapper的改动。原因是为了修复类型错误、减少冗余和提高代码执行效率。

https://github.com/vllm-project/vllm/issues/4182
该issue为bug报告，主要涉及模型中的`eos_token_id`字段在`generation_config.json`中的支持问题。

https://github.com/vllm-project/vllm/issues/4181
这是一个bug报告的issue，涉及软件版本v0.4.1 Release Tracker，主要问题是在CustomAllreduce pcie nvlink topology detection和LoRA loading check上存在bug，导致出现了性能回归和OpenAI API Server报告0 tokens/s的问题。

https://github.com/vllm-project/vllm/issues/4180
这是一个bug报告类型的issue，主要涉及到使用LLama 3 8B模型时生成结果无法停止的问题，可能是由于eos token配置不正确导致。

https://github.com/vllm-project/vllm/issues/4179
这个issue是关于文档添加多阶段dockerfile可视化的需求。

https://github.com/vllm-project/vllm/issues/4178
这个issue是一个[杂项]类型的问题，主要涉及到“llama3-instrcut”模型的对话模板添加，可能是由于需要更新新的对话模板导致该问题。

https://github.com/vllm-project/vllm/issues/4177
这是一个版本更新的issue，涉及的主要对象是软件的版本号。

https://github.com/vllm-project/vllm/issues/4176
这是一个关于软件版本更新的类型为维护性需求的issue，主要对象是transformers库和llama 3 titkoken tokenizer，由于需要支持转换后的tokenizer导致需要对tokenizers库进行小的更改。

https://github.com/vllm-project/vllm/issues/4175
这是一个文档问题报告，主要涉及Meta Llama 3和Llama 2的命名错误。原因可能是相关文档未正确说明Meta Llama 3的支持，并且 Llama 2 的命名存在问题。

https://github.com/vllm-project/vllm/issues/4174
这是一个bug报告类型的issue，主要涉及的对象是markdown渲染功能。由于markdown渲染不起作用，导致需要使用原始HTML。

https://github.com/vllm-project/vllm/issues/4173
这个issue是一个Bugfix类型的报告，涉及到vllm中的fp8 kv_cache检查错误，导致在获取CUDA版本失败时发生错误。

https://github.com/vllm-project/vllm/issues/4172
这是一个Bug报告，涉及的主要对象是AMD测试。由于OOM（Out of Memory）导致AMD测试时OOM异常，造成端口8000无法访问，从而导致CI运行出现卡顿或失败。

https://github.com/vllm-project/vllm/issues/4171
这是一个bug报告类型的issue，主要涉及的对象是vllm库中的bloom-3b模型，发生了服务器崩溃并出现了AssertionError的情况。

https://github.com/vllm-project/vllm/issues/4169
这是一个Bug报告类型的Issue，主要涉及的对象是VLLM项目的代码。由于缺少名为'transformers_modules'的模块导致了ModuleNotFoundError报错。

https://github.com/vllm-project/vllm/issues/4168
这是一个Bug报告，主要涉及vllm下的Yarn-Mistral-7B-128k模型加载问题，可能导致代码错误。

https://github.com/vllm-project/vllm/issues/4167
这个issue类型是功能需求，主要对象涉及到vllm工具中的命令行功能。由于用户想要更加方便地使用命令行功能，提出了新增`vllm serve`来包装`vllm.entrypoints.openai.api_server`的需求。

https://github.com/vllm-project/vllm/issues/4166
这是一个bug报告，主要涉及到HuggingFace实现与Vllm实现在测试中的耦合问题。由于`HfRunner`在模型测试中加载tokenizer时使用了get_tokenizer方法，导致可能与`VllmRunner`的实现产生重叠。

https://github.com/vllm-project/vllm/issues/4165
这是一个Bug报告，涉及的主要对象是async executor。由于缺少num_lookahead_slots参数，导致出现了特定的stacktrace问题。

https://github.com/vllm-project/vllm/issues/4164
这是一个Bug报告，涉及的主要对象是vllm在不同版本中的Docker镜像。由于在`v0.4.0.post1`标签中缺少特定的代码，导致在构建和使用该标签对应的Docker镜像时会导致无法执行的问题。

https://github.com/vllm-project/vllm/issues/4163
这是一个关于安装问题的报告，主要涉及的对象是vllm。这个问题可能是由于安装环境或方法不正确而导致的无法成功导入vllm的错误。

https://github.com/vllm-project/vllm/issues/4162
这个issue类型是bug报告，主要涉及对象是vLLM的_schedule_default函数在添加了chunked_prefill特性后的行为变化。导致这个bug的原因是添加了chunked_prefill特性后，vLLM的调度函数对作业的优先级处理出现了不一致，不符合原始功能预期。

https://github.com/vllm-project/vllm/issues/4161
这个issue是关于对Mypy的修复，不是bug报告，主要涉及的对象是nested directories的typing。这个问题的提出是为了解决nested directories中的typing问题。

https://github.com/vllm-project/vllm/issues/4160
该issue属于功能需求类型，涉及VLLM项目下的InternLM2模型的LoRA加载支持问题，由于修改后进行Lora加载后推理结果不正确，可能是由于修改的代码部分未正确处理数据导致。

https://github.com/vllm-project/vllm/issues/4159
这个issue类型是bug报告，主要涉及的对象是CustomAllreduce的NVlink拓扑检测，由于当前代码未正确评估NVlink的对等连接性，导致在某些>2 PCIE GPU配置下CustomAllReduce错误启用，可能导致模型加载时挂起。

https://github.com/vllm-project/vllm/issues/4158
这是一个关于性能问题的bug报告，主要涉及到服务器的吞吐量下降和波动，可能由于某种原因导致了性能回退。

https://github.com/vllm-project/vllm/issues/4157
这是一个Bug报告，主要涉及的对象是该项目的模型架构['MiniCPMForCausalLM']。原因可能是模型架构当前不受支持，导致数值错误。

https://github.com/vllm-project/vllm/issues/4156
这个issue属于用户提出需求类型，主要涉及到对于vllm中KV cache的直接访问需求，原因是用户想要进行一个涉及到在节点之间复制KV cache内容的实验。

https://github.com/vllm-project/vllm/issues/4155
这是一个bug报告类型的issue，主要涉及由于网络问题导致下载不完整的nccl库，可能导致core dump，提出需要在初始化过程中添加完整性检查以及相关测试。

https://github.com/vllm-project/vllm/issues/4154
这是一个bug报告类型的issue，主要涉及的对象是用户当前运行环境。由于PyTorch版本与CUDA版本不匹配，导致无法处理请求。

https://github.com/vllm-project/vllm/issues/4153
这是一个需求类型的issue，主要涉及到vllm和outlines之间的版本依赖关系。由于版本更新导致的错误未能正常更新vllm以使用outlines的最新版本功能，用户在寻求解决此问题的方案。

https://github.com/vllm-project/vllm/issues/4152
这个issue类型是用户提出需求，主要对象是AMD ROCm 6.1支持，用户询问何时可以期待vLLM支持ROCm 6.1。

https://github.com/vllm-project/vllm/issues/4151
这是一个bug报告issue，涉及vllm库在推理过程中出现了RuntimeError导致程序报错。

https://github.com/vllm-project/vllm/issues/4150
这个issue类型是bug报告，主要涉及的对象是async engine。由于代码重构导致调用`stat_logger.log()`的位置发生变化，造成async引擎停止向Prometheus记录日志统计信息。

https://github.com/vllm-project/vllm/issues/4149
这是一个bug报告，涉及的主要对象是在使用guided_json和其他约束解码字段时出现了log probability无法序列化的问题。

https://github.com/vllm-project/vllm/issues/4148
此issue属于功能开发类型，涉及到vllm引擎的端到端测试以验证基本正确性，旨在比较vllm OpenAI服务器输出的tokens与由`AutoModelForCausalLM.from_pretrained()`创建的HuggingFace模型生成的tokens，存在的问题是不同输出中`logprobs`的排列方式不同导致需要寻找解决方案。

https://github.com/vllm-project/vllm/issues/4147
这个issue属于用户提出需求类型，主要涉及使用多GPU运行34B模型，用户想要在A100 40G上进行特定模型的推理，但不清楚如何与vllm集成。

https://github.com/vllm-project/vllm/issues/4146
这是一个功能需求的issue，主要涉及对象是PrefixCachingBlockAllocator。因为该issue描述了为PrefixCachingBlockAllocator添加自动前缀缓存支持的功能，主要讨论了实现此功能的LRU缓存策略，而非在现有代码中存在的bug。

https://github.com/vllm-project/vllm/issues/4145
这个issue是关于bug报告，主要涉及benchmark_serving.py的性能问题，由于每个请求的延迟逐渐增加，用户想了解如何获取每个请求的延迟。

https://github.com/vllm-project/vllm/issues/4144
这是一个寻求使用帮助的issue，主要涉及vllm模型在模型并行运行时的集成问题。由于缺乏对vllm集成的了解，用户无法实现特定模型的推理操作。

https://github.com/vllm-project/vllm/issues/4143
这是一个Bug报告，涉及的主要对象是vllm库。由于缺少定义的符号 _ZN3c106detail14torchCheckFail，导致了ImportError，无法正确导入和使用vllm库中的功能。

https://github.com/vllm-project/vllm/issues/4142
这是一个功能需求类型的issue，主要涉及的对象是在启用块管理器V2时启用前缀缓存。原因是为了通过APC功能在块管理器V2上维护内容哈希和物理块之间的映射，并改善数据块的分配和回收处理。

https://github.com/vllm-project/vllm/issues/4141
这个issue是关于用户需求的，主要对象是PhiForCausalLM模型，用户要求为其添加LoRA支持，但目前该模型并不支持LoRA。

https://github.com/vllm-project/vllm/issues/4140
这是一个Bug报告，主要涉及的对象是vllm在Win10的WSL2环境下无法启动的问题。可能由于升级到vllm0.4.0版本后，导致在多卡部署模型时出现错误。

https://github.com/vllm-project/vllm/issues/4139
这是一个Bugfix类型的issue，主要涉及到chunked prefill导致prompt logprob计算错误的问题。

https://github.com/vllm-project/vllm/issues/4138
这是一个bug报告，涉及主要对象为LoRa loading check。由于`peft`指定了`target_modules`，需要对LoRa loading check进行优化。

https://github.com/vllm-project/vllm/issues/4137
这是一个bug报告，主要涉及VLLM的TRTLLM后端，由于输出解析错误导致CC基准测试失败。

https://github.com/vllm-project/vllm/issues/4136
这是一个Bug报告类型的Issue，涉及到使用benchmark trtllm时遇到错误。由于代码中重复使用`json.load`导致数据重复读取，以及在流模式中只收集最后一块数据，导致输出长度错误。

https://github.com/vllm-project/vllm/issues/4135
这是一个bug报告，涉及的主要对象是async llm engine。这个问题可能是由于某种原因导致了async llm engine在使用mixtral-8x7b时失败，用户需要找出bug的根本原因。

https://github.com/vllm-project/vllm/issues/4134
这是一个用户提出需求的issue，主要涉及的对象是为Swallow-MS-7B LoRA添加punica dimension。由于该模型扩大了词汇量，为了支持LoRA，需要在punica中添加相应的大小。

https://github.com/vllm-project/vllm/issues/4133
这是一个bug报告，涉及的主要对象是vllmnccl版本更新后导致docker无法更新缓存层。

https://github.com/vllm-project/vllm/issues/4132
这是一个bug报告，主要涉及VLLM模型的print输出缺少细节，需要通过extra_repr添加更多细节。

https://github.com/vllm-project/vllm/issues/4131
这是一个bug报告，主要涉及VLLM在ROCm环境中出现的设备顺序错误导致的问题。

https://github.com/vllm-project/vllm/issues/4130
这是一个需求类型的issue，主要涉及在离线接口中增加对指导解码的支持。由于markdown渲染不起作用，所以使用了原始的HTML。

https://github.com/vllm-project/vllm/issues/4129
这是一个关于代码修复和改进的 issue，主要涉及 AMD 硬件上的 xformer 清理和改进，解决了 CI 失败的问题。

https://github.com/vllm-project/vllm/issues/4128
这是一个Bugfix类的issue，主要涉及Kernel，由于Triton只支持2的幂次方块大小，导致现有的前缀预填充内核只支持幂次方为2的头维度。

https://github.com/vllm-project/vllm/issues/4127
这是一个bug报告，涉及到chunked prefill的head size必须为2的幂次方，导致不支持非2的幂次方的情况。

https://github.com/vllm-project/vllm/issues/4126
这是一个由合并竞争引起的CI错误问题，涉及的主要对象是代码库中的两个PR。

https://github.com/vllm-project/vllm/issues/4125
这个issue是一个用户提出需求的类型，主要涉及对象为w2f功能。由于未提供具体内容，无法分析导致的症状或问题。

https://github.com/vllm-project/vllm/issues/4124
这个issue是一个用户提出的需求，主要涉及的对象是关于支持HuggingFaceM4/idefics2-8b作为视觉模型的功能添加。原因是该模型具有更好的基准表现，但至今尚未有相应的PR提交，用户在寻求对该功能的支持。

https://github.com/vllm-project/vllm/issues/4123
这是一个关于CI（持续集成）的问题，目标对象是移动CPU和AMD测试的顺序。这个问题是由于贡献者PR导致的AMD主机挂起，需要解决等待AMD/CPU测试完成后才能开始其他测试的约束问题。

https://github.com/vllm-project/vllm/issues/4122
这是一个Bug报告类型的Issue，主要涉及的对象是VLLM项目中的rocm_flash_attn.py文件。该问题是由于拼写错误导致代码看起来不清晰，需要修正错误的拼写。

https://github.com/vllm-project/vllm/issues/4121
这是一个关于持续集成的问题，涉及主要对象是AMD测试，在主要原因导致的bug是flaky测试。

https://github.com/vllm-project/vllm/issues/4120
这是一个bug报告，关于在nani3x上运行时，VLLM_USE_TRITON_FLASH_ATTN标志被忽略，而是使用了naive实现的FA，需要修正这种行为以便测试在vLLM中使用Triton FA。

https://github.com/vllm-project/vllm/issues/4119
这是一个Bug报告。该问题涉及的主要对象是VLLM中的Chat templates。原因可能是Chat模板的应用不正确，导致模型只会谈论Jinja模板内容。

https://github.com/vllm-project/vllm/issues/4118
这是一个功能需求的issue，涉及vLLM中支持FP8计算的初始支持，由于需要在加载模型权重后计算权重的per-tensor scaling factor并相应地量化权重。

https://github.com/vllm-project/vllm/issues/4117
这是一个bug报告，涉及到vllm 0.4.0.post1版本在多线程模式下出现数通过的问题。

https://github.com/vllm-project/vllm/issues/4116
此issue是一个[Model]类型的问题，涉及更新MPT模型以匹配最新的更改，并添加低精度层归一化。原因可能是要改进模型性能或功能。

https://github.com/vllm-project/vllm/issues/4115
这是一个用户提出需求的issue，主要涉及到向vLLM添加Jamba支持。用户希望将Jamba模型整合到vLLM中，并实现请求ID的传递和缓存管理，以及在请求完成时清理缓存。

https://github.com/vllm-project/vllm/issues/4114
这个issue是Bug报告类型，主要涉及的对象是vllm在两个节点上运行时所遇到的CUDA driver版本不一致的问题，导致无法正常运行。

https://github.com/vllm-project/vllm/issues/4113
这个issue是一个[CI/Build]类型的PR，涉及主要对象是针对长时间CPU测试启用Intel队列。由于修复了pos_encoding kernel的错误并启用了部分`models`下的测试，在测试相关于仅CPU的cuda时跳过了相关测试。

https://github.com/vllm-project/vllm/issues/4112
这是一个Bug报告，涉及VLLM在并行处理请求时输出不稳定的问题。

https://github.com/vllm-project/vllm/issues/4111
这个issue是一个bug报告，涉及的主要对象是deepseek-coder-33b-instruct和deepseek-coder-6.7b-instruct模块。该问题可能由于代码实现错误或数据类型不匹配导致了其中两个模块返回值为空的bug。

https://github.com/vllm-project/vllm/issues/4110
这个issue类型为功能增强，涉及主要对象为vllm代码库中的Ray CPU Executor功能。这个变更是为了增强vllm对分布式执行的支持，使其能够结合CPU和Ray进行执行。

https://github.com/vllm-project/vllm/issues/4109
这个issue是关于更新Outlines Integration功能从`FSM`到`Guide`的。

https://github.com/vllm-project/vllm/issues/4108
这个issue是bug报告，主要涉及系统中的NCCL watchdog线程异常终止，导致CUDA错误引发非法内存访问。

https://github.com/vllm-project/vllm/issues/4107
这是一个 bug 报告，主要涉及到方法 `get_tokenizer` 参数名的问题，由于使用了错误的参数名 `tokenizer_revision` 导致参数被忽略没有效果。

https://github.com/vllm-project/vllm/issues/4106
这是一个Bug报告，主要涉及到vLLM的tensor parallel功能。导致这个bug的原因是引入了一个module初始化顺序依赖问题，使得workers被分配了一个无效的设备索引，导致异常。

https://github.com/vllm-project/vllm/issues/4105
这是关于bug修复的issue，涉及修复engineuse-ray在主分支上的问题，原因是访问配置时通过`args`而不是`kwargs`导致出错。

https://github.com/vllm-project/vllm/issues/4104
该issue属于用户提出需求类型，涉及到vllm的 `enable_prefix_caching` 标志是否对 prompts 和生成的 kv 缓存进行重用，由于可能存在未明确描述的缓存重用问题，用户寻求关于 RadixAttention 的具体细节和潜在修复方案的帮助。

https://github.com/vllm-project/vllm/issues/4103
这是一个bug报告，涉及主要对象是VLLM的模型服务。由于缺少nvmlInit属性导致了报错。

https://github.com/vllm-project/vllm/issues/4102
这是一个Bug修复类型的issue，主要涉及的对象是vLLM的naive attention实现，由于在不同的attention技术实现过程中引入了几个拼写错误，导致vLLM无法在navi3x上运行，需要修复这些错误并重新运行benchmark测试。

https://github.com/vllm-project/vllm/issues/4100
这是一个Bug报告，涉及的主要对象是开启OpenAI服务器时使用 --engine-use-ray 选项导致崩溃。

https://github.com/vllm-project/vllm/issues/4099
这是一个bug报告类型的issue，主要涉及的对象是对Python 3.8版本的支持。由于Python 3.8版本未经适当测试，可能会导致出现问题，因此提出了需要在Python 3.8上进行简单的sanity check测试。

https://github.com/vllm-project/vllm/issues/4098
这是一个Bugfix类型的issue，主要涉及到vLLM的quantization方法的更新未及时导致部分脚本中的参数选择不准确。

https://github.com/vllm-project/vllm/issues/4097
这个issue类型是代码重构，主要涉及的对象是模型加载代码。由于重构代码提供了新的接口和配置对象，使得加载权重的方式更加灵活和可扩展。

https://github.com/vllm-project/vllm/issues/4096
这是一个[Frontend]类型的issue，主要涉及到添加一个本地Kobold Lite聊天界面的入口点。该问题主要是由于markdown渲染问题导致的。

https://github.com/vllm-project/vllm/issues/4095
这个issue属于Bug报告，主要涉及针对nsight profiling功能的bug修复。由于之前的代码更改引入了问题，导致了与nsight profiling相关的工作线程性能分析功能无法正常工作。

https://github.com/vllm-project/vllm/issues/4094
这是一个用户提出需求的issue，主要涉及如何在非HuggingFace模型中使用vLLM，用户希望能够在不转换整个代码库为HuggingFace的情况下插入vLLM。

https://github.com/vllm-project/vllm/issues/4093
这是一个Bug报告issue，主要涉及到vllm中的 guided_json 生成错误的问题。由于 llama2-13b 模型输出的结果异常，导致无法正确生成预期的JSON输出。

https://github.com/vllm-project/vllm/issues/4092
这个issue是一个bug报告，涉及的主要对象是vLLM项目中的类型注解。由于Python 3.8不支持Sequence类型的泛型别名，导致了在使用collections.abc.Sequence时出现问题。

https://github.com/vllm-project/vllm/issues/4091
这是一个bug报告，主要涉及vLLM在运行并发请求时输出结果变得不确定的问题。原因可能是并发请求导致输出结果的不确定性。

https://github.com/vllm-project/vllm/issues/4090
这个issue是文档改进类型的问题，主要涉及的对象是关于tensorizer使用的解释，可能是由于当前文档表达不清晰导致用户无法准确了解如何使用。

https://github.com/vllm-project/vllm/issues/4089
这个issue是一个Bugfix类型的报告，涉及的主要对象是Python 3.8中出现的"'xxx' object is not subscriptable"错误，可能是由于类型错误导致的。

https://github.com/vllm-project/vllm/issues/4088
这是一个关于修复日志问题的issue，主要涉及到CPUExecutor模块。由于最近一个CC（实现RFC“增强BaseExecutor接口以支持硬件无关的预测解码”）引入了一个日志问题。

https://github.com/vllm-project/vllm/issues/4087
这是一个新增模型支持的issue，主要涉及的对象是MiniCPM-V。由于该模型是在0.5.3.post1版本后添加的，因此只能在下一个版本中包含，用户需要从源代码安装vLLM来使用。

https://github.com/vllm-project/vllm/issues/4086
这是一个用户提出需求的类型issue，主要涉及到vllm是否支持FacebookAI/roberta-large模型的推理。缺乏响应可能是由于缺乏对该模型支持的明确说明或者文档。

https://github.com/vllm-project/vllm/issues/4085
这是一个安装问题的类型，涉及主要对象是安装最新版本的vLLM，由于PyTorch版本与CUDA版本不兼容导致无法成功安装。

https://github.com/vllm-project/vllm/issues/4084
这是一个用户需求类型的issue，主要涉及的对象是vllm模型。由于用户在使用模型钩子时只能捕获第一个token的信息而不是所有token的信息，所以提出了如何更好地在库中使用模型钩子的问题。

https://github.com/vllm-project/vllm/issues/4083
这个issue是一个Bug报告，涉及主要对象为vllm_C模块，由于之前的修复操作未成功，导致用户仍然遇到相同的问题。

https://github.com/vllm-project/vllm/issues/4082
该issue是用户提出需求，希望为ChatGLM3和llama2模板添加chat模板来支持多轮对话，以解决在多轮对话中请求需要在流模式下添加eos令牌的问题。

https://github.com/vllm-project/vllm/issues/4081
这是一个bug报告，涉及的主要对象是在运行pytest时出现的TypeError错误。造成这种错误的可能原因是与mypy更改相关。

https://github.com/vllm-project/vllm/issues/4080
这是一个用户提出需求的issue，涉及主要对象是vllm工具中的tools/tool_choice功能。用户提出这个问题是因为想要在使用open api端点进行推断时调用功能/tools功能，但目前无法得到关于如何实现的详细信息。

https://github.com/vllm-project/vllm/issues/4079
这个issue是一个bug报告，主要涉及的对象是vllm项目的分布式推理功能。由于程序hang或crash时很难调试，添加了一个选项来记录每个函数调用，以便更容易确定bug所在的功能和调用堆栈。

https://github.com/vllm-project/vllm/issues/4078
这是一个用户提出需求的类型的issue，主要涉及的对象是360Zhinao模型系列。由于需求新增支持360Zhinao模型，原因可能是为了将其集成到相应的系统或平台中。

https://github.com/vllm-project/vllm/issues/4077
这是一个Bug报告，涉及的主要对象是加载nccl库出现bus error，可能是由于nccl库版本不兼容或配置错误导致的问题。

https://github.com/vllm-project/vllm/issues/4076
这个issue是一个bug报告，涉及主要对象是vllm中的tensor_parallel_size参数设置为2时，导致输出结果异常。产生这个bug的原因是由于代码实现逻辑或参数设置可能存在问题。

https://github.com/vllm-project/vllm/issues/4075
这个issue是一个Bugfix类型，涉及的主要对象是filelock版本需求，由于未正确设置版本要求，导致markdown渲染无法工作。

https://github.com/vllm-project/vllm/issues/4074
这是一个Bug报告类型的issue，主要涉及到vLLM环境中出现的"RuntimeError: Unknown layout"错误。由于某种原因导致了布局未知的运行时错误。

https://github.com/vllm-project/vllm/issues/4073
这个issue类型是功能需求提议，主要涉及的对象是对Mixtral 8x22Bv0.1版本的支持。由于未明确记录支持该版本所导致的用户提出对应的功能需求。

https://github.com/vllm-project/vllm/issues/4072
这个issue是关于bug修复的，主要涉及的对象是该项目中的`setup.py`文件。这个bug的原因是`extras_require`应该是一个字典，而不是一个列表，导致在安装过程中无法正确设置额外的依赖。

https://github.com/vllm-project/vllm/issues/4071
这个issue是关于代码优化和bug修复的，主要涉及的对象是executor类，由于一些逻辑设置错误导致了bug症状和功能缺失。

https://github.com/vllm-project/vllm/issues/4070
这是一个Bug报告，主要涉及vLLM在发送response_format为json的请求两次后进入损坏状态并产生错误响应的问题。造成这种症状的原因可能是与vLLM本身有关。

https://github.com/vllm-project/vllm/issues/4069
这是一个Bug报告，涉及无法加载模型的问题，由于CUDA和多GPU导致IndexError。

https://github.com/vllm-project/vllm/issues/4068
这是一个关于功能需求的issue，主要涉及到LoRA适配器的更新和优化。

https://github.com/vllm-project/vllm/issues/4067
这个issue类型为环境配置错误，主要对象是尝试加载mistralai/Mixtral-8x7B-Instruct-v0.1模型。由于PyTorch版本与CUDA版本不匹配，导致无法成功加载模型。

https://github.com/vllm-project/vllm/issues/4066
这是一个关于软件兼容性问题的issue，涉及主要对象为vllm，用户询问是否vllm支持CUDA 11.3版本和PyTorch 1.12。

https://github.com/vllm-project/vllm/issues/4065
这个issue属于bug报告，主要对象是vllm中的模型加载问题。由于不正确的模型保存和加载流程导致了无法正确加载自训练模型的问题。

https://github.com/vllm-project/vllm/issues/4064
这是一个用户提出需求的类型，主要对象是对支持Mixtral-8x22B-v0.1的请求，由于缺乏该支持而导致用户提出了关于支持时间的问题。

https://github.com/vllm-project/vllm/issues/4063
这是一个特性需求，主要涉及到Punica和ChineseMixtral两个对象。这个需求是由于ChinesMixtral的词表大小为57000，需要将额外的57088大小的内容集成到Punica中。

https://github.com/vllm-project/vllm/issues/4062
这是一个Bug报告类型的Issue，涉及主要对象为vLLM项目。问题由于从cuda Driver版本535.54.03迁移到cuda Driver版本535.161.08后，在最新的vLLM v0.4.0.post1版本出现了错误。

https://github.com/vllm-project/vllm/issues/4061
这个issue是关于升级triton到2.2.0版本的Misc类型问题，涉及到vLLM项目中的requirements文件。由于Pytorch已经包含了triton依赖，因此在requirements文件中可以安全地移除triton。

https://github.com/vllm-project/vllm/issues/4060
这是一个用户提出需求类型的issue，主要涉及的对象是添加 Radix_tree.py 作为数据结构来识别通用前缀。

https://github.com/vllm-project/vllm/issues/4059
该issue属于用户提出需求类型，主要对象是vllm在T4平台上无法支持Automatic Prefix Caching功能。由于T4平台的计算能力不符合要求，导致该功能无法被实现。

https://github.com/vllm-project/vllm/issues/4058
这是一个bug报告，主要问题是用户在运行vllm时得到了一些奇怪的输出。这可能是因为设置不正确导致的。

https://github.com/vllm-project/vllm/issues/4057
这是一个特性请求，涉及功能的问题，主要对象是sliding window attention，导致chunked prefill和prefix caching不能与sliding window attention配合使用。

https://github.com/vllm-project/vllm/issues/4056
该issue类型是功能需求，主要涉及对象是vLLM模型中的cuda graph功能。由于当前设置导致了当prefill和decodes一起批处理时cuda graph未启用，用户希望通过启用cuda graph来尝试提高性能。

https://github.com/vllm-project/vllm/issues/4055
这个issue是一个bug报告，涉及的主要对象是vLLM中的Prompt logprob APIs。这个bug是由于chunked prefill导致函数假设1个序列组元数据计算所有prompts，而实际上不是这样导致的。

https://github.com/vllm-project/vllm/issues/4054
这是一个bug报告，主要涉及的对象是vllm项目中的代码中的数据类型错误转换问题，由于在代码中将数据类型转换为torch.float16可能不适合所有情况，特别是当模型设计为使用bfloat16或其他数值精度时，造成了该bug。

https://github.com/vllm-project/vllm/issues/4053
这是一个需求提出的issue，主要对象是Baichuan-13B模型。由于Baichuan-13B的hidden_size不满足支持LoRA的需求，需要在Punica中加入额外的大小以支持该功能。

https://github.com/vllm-project/vllm/issues/4052
这是一个用户提需求，关于如何在vLLM上使用OpenAI客户端进行批量请求后者进行基准测试的问题。

https://github.com/vllm-project/vllm/issues/4051
这是一个Bug报告，主要涉及的对象是NeuronWorker，由于缺少了必需的位置参数`cache_config`导致出现了bug。

https://github.com/vllm-project/vllm/issues/4050
这是一个bug报告，涉及主要对象是计算机程序中的时间函数。导致该问题的原因是`time.time()`和`time.monotonic()`不能直接相减计算时间间隔，导致了性能指标计算错误。

https://github.com/vllm-project/vllm/issues/4049
这是一个Bug报告，主要涉及的对象是代码中的时间计算函数。此问题为计算时间时使用了不兼容的时间函数导致无法正确计算持续时间。

https://github.com/vllm-project/vllm/issues/4048
这个issue是关于用户提出需求，询问本地测试部分代码修改的策略，主要涉及到VLLM代码库的局部修改的测试方法，用户想知道是否有避免每次都需要从源代码重新构建的方法来更快更有效地测试修改。

https://github.com/vllm-project/vllm/issues/4047
这个issue是关于bug报告，主要涉及构建vLLM软件时可能出现的问题。该问题可能由于误导性错误信息导致构建失败，需要添加构建源代码的故障排除建议。

https://github.com/vllm-project/vllm/issues/4046
这是一个Bug报告，主要涉及的对象是Streaming inference response结果不能正常序列化为json，导致接收端无法正常处理数据格式，需要删除最后输出以使客户可以更方便地处理流推理结果。

https://github.com/vllm-project/vllm/issues/4044
这是一个功能需求类型的issue，主要涉及到 `max_num_batched_tokens` 参数的计算问题，用户在设定该参数时缺乏指导，可能导致性能不佳。

https://github.com/vllm-project/vllm/issues/4043
该issue是一个Bug报告，主要涉及的对象是Mypy的typing部分。在该issue中提到的lazy initialization导致了部分字段仅在lazy initialization完成后才能访问，而这可能会导致一些问题。

https://github.com/vllm-project/vllm/issues/4042
这是一个bug报告，涉及到在初始化过程中出现hang的问题。

https://github.com/vllm-project/vllm/issues/4041
这是一个bug报告，主要涉及到VLLM项目中的一个POST请求出现了500 Internal Server Error，可能是由于程序逻辑错误或服务器配置问题导致的。

https://github.com/vllm-project/vllm/issues/4040
这是一个bug报告类型的issue，主要涉及到的对象是custom allreduce功能。该问题产生的原因是在 https://github.com/vllmproject/vllm/pull/2760 后，默认情况下未正确启用custom allreduce功能。

https://github.com/vllm-project/vllm/issues/4039
这是一个bug报告，主要涉及的对象是type hint在Python 3.8版本下的修复，由于修复没有涵盖所有的type annotations，导致出现了错误的typing。

https://github.com/vllm-project/vllm/issues/4038
这是一个用户提出需求的issue，主要涉及的对象是vLLM项目中的logger.py文件。这个issue提出了重构logger以支持用户以Python风格提供自定义日志配置的需求。

https://github.com/vllm-project/vllm/issues/4037
这是一个bug报告，主要涉及到CI/Test（持续集成/测试）相关的问题。

https://github.com/vllm-project/vllm/issues/4036
这是一个bug报告，主要涉及到Python 3.8中的类型提示问题，导致在某些情况下出现错误。

https://github.com/vllm-project/vllm/issues/4035
这是一个Bug报告，主要对象是vllm代码库中的Python 3.8环境下出现的类型错误。

https://github.com/vllm-project/vllm/issues/4034
这是一个用户提出需求的issue，主要涉及到如何在使用现有的Ray集群时初始化vLLM引擎，问题出现的原因是可能存在在vLLM调用`ray.init`之后再次尝试初始化Ray集群导致的错误提示。

https://github.com/vllm-project/vllm/issues/4033
这是一个关于需求的issue，涉及到对于VLLM模型添加bitsandbytes 4bit quantization的支持。原因可能是现有的实现对于特定用途有较大优势，但尚未被添加。

https://github.com/vllm-project/vllm/issues/4032
这是一个Bug报告，涉及的主要对象是LoRA模块中的bug，由于之前的pull request导致CI失败，原因可能是相关代码重构和合并顺序的问题。

https://github.com/vllm-project/vllm/issues/4031
这个issue是一个功能需求，主要涉及的对象是支持Int8 dtype用于存储权重，目前使用FP16浪费50%的VRAM。原因是当使用Int8模型时，权重以FP16存储，导致消耗大量VRAM。

https://github.com/vllm-project/vllm/issues/4030
这是一个性能问题的issue，主要涉及vLLM和TGI模型的表现差异，可能由于设置不当导致vLLM表现较差。

https://github.com/vllm-project/vllm/issues/4029
这个issue类型是用户提出需求，主要对象是在使用vllm时希望能够将两个大模型分布在四个GPU上，并且希望这两个模型能够在同一个进程中运行。由于vRAM不足导致无法正确配置两个LLM实例在不同GPU上运行。

https://github.com/vllm-project/vllm/issues/4028
该issue是关于代码重构的问题，主要涉及到前端处理提示信息的重构。原因是为了优化代码结构和日志记录方式，以及明确处理特定端点类型下的请求ID前缀。

https://github.com/vllm-project/vllm/issues/4027
这个issue是一个bug报告，涉及到vLLM的API服务器启动过程中出现卡住的问题。可能是由于启动参数配置错误或者程序逻辑问题所导致。

https://github.com/vllm-project/vllm/issues/4026
该issue是功能改进类型，涉及的主要对象是代码文件中的`merge_async_iterators`函数。由于`merge_async_iterators`函数的功能不仅限于OpenAI server，为了改善代码组织，提议将其移动到常用工具中。

https://github.com/vllm-project/vllm/issues/4025
这个issue属于用户提出需求类型，主要涉及VLLM模型中支持4位KV Cache，需求源于希望减少在处理长文本时GPU内存使用量的问题。

https://github.com/vllm-project/vllm/issues/4024
这是一个用户提出需求的issue，主要涉及对象是vllm下的一个特定功能WorkerWrapper，问题是因为特定函数`set_cuda_visible_devices`过于具体且使用范围狭窄，同时初始化过程中的部分信息在初始化后丢失，导致难以获取这些信息。

https://github.com/vllm-project/vllm/issues/4023
这是一个bug报告，涉及的主要对象是在测试多个关注后端时出现了问题。原因是之前的方法不起作用，因为在选择器.py文件的第24行使用了lru_cache导致问题。

https://github.com/vllm-project/vllm/issues/4022
该issue是一个bug报告，涉及到在scheduler.py文件中修正一个小错字导致的问题。

https://github.com/vllm-project/vllm/issues/4021
这是一个bug报告，主要涉及到CUDA内存分配问题，由于每个worker在每个GPU上都会分配内存，导致了过多的CUDA上下文占用。

https://github.com/vllm-project/vllm/issues/4020
这是一个功能需求报告，主要涉及模型和内核，由于缺少16和32 kernel sizes，导致部署SanjiWatsuki/TinyMixtral32x248M和BEEspokedata/smol_llama220M等小型模型时与vllm不兼容。

https://github.com/vllm-project/vllm/issues/4019
这是一个Bug报告，主要涉及的对象是在使用多个MI250x GPU进行tensor-parallel推理时，进程被意外终止。造成这种症状的原因可能是硬件环境或代码实现上的问题。

https://github.com/vllm-project/vllm/issues/4018
这是一个Bug报告，主要涉及ROCm Triton FlashAttention在性能上出现问题。由于Triton在编译或自动调优内核时导致性能较低，同时出现了重复的tokenizer警告。

https://github.com/vllm-project/vllm/issues/4017
该issue类型为用户提出需求，请教问题，主要涉及对象为vLLM的安装程序。由于用户使用AMD GPU，希望vLLM能提供预编译版本以便在该硬件上使用。

https://github.com/vllm-project/vllm/issues/4016
这个issue是一个Bugfix类型的报告，主要涉及vLLM的服务器接口。导致这个bug的原因是当用户在发送`ChatCompletionRequest`时没有设置`max_tokens`，错误信息会显示为负数值，导致错误判断。

https://github.com/vllm-project/vllm/issues/4015
这个issue是一个特性需求，主要涉及的对象是vLLM项目的CUDA核心代码，由于需要支持更大的词汇表，以便于在更多模型上进行lm head的修改。

https://github.com/vllm-project/vllm/issues/4014
这是一个用户提出需求的issue，主要对象是vllm下的"Distributed"模块。由于当前的`init_distributed_environment`需要`world_size`和`rank`参数，用户希望将其变为可选参数，并设置默认的`distributed_init_method`为`env://`，以便与`torchrun`更方便地使用。

https://github.com/vllm-project/vllm/issues/4013
这是一个Bug报告，该问题涉及vLLM与CohereForAI/c4ai-command-r-plus在DGX H100上运行时出现KeyError的问题。感谢你的问题，如果还有其他问题，欢迎继续提出。

https://github.com/vllm-project/vllm/issues/4012
这是一个功能需求的 issue，主要涉及添加 LoRA 支持到 quantized 模型，并引入了对张量并行的支持。

https://github.com/vllm-project/vllm/issues/4011
这是一个Bug报告，主要涉及VLLM安装问题。由于PyTorch版本不兼容，导致安装失败。

https://github.com/vllm-project/vllm/issues/4010
这是一个Bug报告，涉及的主要对象是github上的vllm项目。由于缺少`image_input_type`和其他视觉相关配置，导致无法正确加载vision model llava 1.5 7b，触发了AssertionError。

https://github.com/vllm-project/vllm/issues/4009
这是一个关于代码重构的issue，主要涉及到BlockAllocator类，由于多个子类都实现了相同的方法，因此提出将该方法抽象到基类中。

https://github.com/vllm-project/vllm/issues/4008
这是一个Bug报告，涉及vllm 0.4.0.post1中不支持模型架构‘LlavaForCausalLM’，导致数值错误。

https://github.com/vllm-project/vllm/issues/4007
这个issue是一个Kernel类型的问题报告，涉及解决在a10 GPU上无法找到适当的kernel的问题，由于缺少h in=16 h out=640 dtype=Float out_dtype=Half的操作符所致。

https://github.com/vllm-project/vllm/issues/4006
这是一个关于添加mypy类型注释的issue，主要涉及代码质量和规范性，由于缺少类型注释导致markdown渲染失败。

https://github.com/vllm-project/vllm/issues/4005
这是一个用户提出需求的issue，主要涉及对象是vllm模型，问题是用户需要调整GPU计算能力以支持40k长度的提示信息计算，由于softmax和其他计算超出了单个GPU的容量，用户寻求如何调整解决这一问题。

https://github.com/vllm-project/vllm/issues/4004
这是一个bug报告，涉及主要对象为pydantic和transformers版本冲突，导致vllm openai server和transformers无法同时正常运行。

https://github.com/vllm-project/vllm/issues/4003
这是一个关于性能问题的议题，主要对象是VLLM项目。由于没有得到响应，导致了性能回归问题的报告没有得到处理。

https://github.com/vllm-project/vllm/issues/4002
这是一个功能增强类型的issue，主要涉及的对象是针对Mixtral 8x22在A10080G和H100设备上的fused MoE配置，旨在提高高批量大小下的性能。

https://github.com/vllm-project/vllm/issues/4001
这是一个Bug报告类型的Issue，主要涉及到vllm项目中的benchmark_serving模块，由于请求函数输入模型名称未正确匹配，导致测试结果不正确。

https://github.com/vllm-project/vllm/issues/4000
这是一个用户提出需求的issue，主要涉及vLLM服务器无法处理异常终止请求导致的排队问题，用户希望找到一种方式清除队列并重置系统。

https://github.com/vllm-project/vllm/issues/3999
该issue类型为用户提出需求，主要涉及对象是添加chatglm6b模型支持，由于chatglm6b是一个非常受欢迎的模型，用户希望在项目中恢复其支持。

https://github.com/vllm-project/vllm/issues/3998
这是一个bug报告，涉及的主要对象是Qwen1.5-14B-Chat使用vllm==0.3.3版本在Tesla V100-PCIE-32GB显卡上部署。这个问题可能是由于部署环境导致结果全部是感叹号的情况。

https://github.com/vllm-project/vllm/issues/3997
这是用户提出的技术问题，涉及到vllm的flash_attn与xformers的使用以及如何集成特定模型进行推理。这个问题可能由于集成困难而导致。

https://github.com/vllm-project/vllm/issues/3996
这个issue属于bug报告，主要涉及的对象是在使用`AsyncLLMEngine`和`asyncio.run`时出现的无法正确完成或关闭的问题，可能是由于`AsyncLLMEngine`中调用了`asyncio.get_event_loop`而与顶层调用`asyncio.run`产生冲突导致。

https://github.com/vllm-project/vllm/issues/3995
这是一个用户提出需求的issue，主要涉及到对VLLM添加对JetMoE模型的支持。

https://github.com/vllm-project/vllm/issues/3994
这是一个bug报告，主要涉及的对象是vLLM模型在大词汇表大小上与Lora适配器的兼容性问题。导致此问题可能是因为vLLM在特定环境下无法支持大词汇表大小和Lora适配器的结合。

https://github.com/vllm-project/vllm/issues/3993
这个issue类型是关于代码改进的需求，主要涉及到改进vLLM的前端以支持CPU后端。原因可能是目前前端没有对CPU后端的支持，需要对代码进行相应修改。

https://github.com/vllm-project/vllm/issues/3992
这个issue类型是硬件相关的需求，主要对象是vLLM的CPU后端。导致这个需求的原因是要启用CPU后端的异步引擎代码路径。

https://github.com/vllm-project/vllm/issues/3991
这是关于模型加载失败的bug报告，涉及的主要对象是无法在两块NVIDIA GeForce RTX 4090上加载模型。原因可能是由于PyTorch版本与CUDA版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/3990
该issue类型是用户提出问题，主要涉及的对象是fp8和fp8_e5m2。由于当前PyTorch环境的CUDA版本和硬件配置导致了用户对于何时使用fp8和何时使用fp8_e5m2的疑问。

https://github.com/vllm-project/vllm/issues/3989
这是关于文档中安装问题的bug报告，主要涉及到vllm项目的安装流程。这个问题由于在安装过程中执行了不必要的命令导致。

https://github.com/vllm-project/vllm/issues/3988
这是一个关于升级PyTorch和Xformers版本的请求，涉及到Xformers依赖的PyTorch版本问题。这个issue的主要原因是Xformers需要使用至少PyTorch 2.1.2，但用户需要使用至少PyTorch 2.2.1，因此提出请求升级Xformers至v0.0.25.post1，以支持PyTorch 2.2.2。

https://github.com/vllm-project/vllm/issues/3987
这是一个功能请求的issue，主要涉及Xformers升级到`v0.0.25.post1`和PyTorch 2.2.2，但由于环境问题导致本地测试无法运行。

https://github.com/vllm-project/vllm/issues/3986
这是一个bug报告类型的issue，主要涉及到vllm-nccl安装过程中使用`python setup.py develop`命令无法正常运行的问题，导致了无法完成预期的库文件安装。

https://github.com/vllm-project/vllm/issues/3985
该issue类型是性能优化建议，主要涉及layernorm加速优化。由于每次`self`发生变化时重新编译，导致性能表现不佳，提出提取计算到独立函数以减少编译次数。

https://github.com/vllm-project/vllm/issues/3984
这是一个bug报告issue，涉及triton编译问题，由于triton不支持具有动态条件的`if`表达式（三元运算符），因此出现了编译错误。

https://github.com/vllm-project/vllm/issues/3983
这是一个用户提出需求的类型的issue，主要涉及的对象是Mixtral8x22Bv0.1模型。用户提出了关于是否支持该模型运行的问题，由于编译时间过长导致了用户的疑问。

https://github.com/vllm-project/vllm/issues/3982
这是一个Bug报告类型的GitHub Issue，主要对象是关于VLLM项目中的`hf_config`。由于`hf_config`中的`architectures`字段可能为`None`，导致在使用`in`语句时出现`TypeError: argument of type 'NoneType' is not iterable`错误。

https://github.com/vllm-project/vllm/issues/3981
这是一个关于性能问题的bug报告，涉及到FP8 KV Cache在FP16模型上的性能下降，可能是由于缺少FP8硬件所致。

https://github.com/vllm-project/vllm/issues/3980
这是一个Bug报告，涉及的主要对象是vllm项目中的GPTQ模型。这个问题可能是由于模型加载后输出重复单个标记，或者在OpenAI API中输出仅为标点符号而导致的。

https://github.com/vllm-project/vllm/issues/3979
这是一个bug报告类型的issue，该问题单涉及的主要对象是Vllm推断速度在LoRA模型上比NonLoRA模型慢。导致这个问题的可能原因是LoRA模型训练导致推断速度降低。

https://github.com/vllm-project/vllm/issues/3978
这个issue类型为分解type类型，主要涉及VLM后端、LLaVA-NeXT模型、OpenAI API服务器、前端GPT-4V聊天完成API支持等对象，由于应对范围蔓延，导致分解该PR，并且需验证分支与主分支的不漏变更。

https://github.com/vllm-project/vllm/issues/3977
这是一个关于GitHub上vLLM项目中的一个issue，类型为优化提案，涉及主要对象为linear layers的实现。由于`linear_weights`属性在字典和layer中都保存了相同的张量，当引用被替换时会导致不正确的matmul结果。

https://github.com/vllm-project/vllm/issues/3976
这是一个Bug报告，主要涉及的对象是LLM引擎在神经元上初始化时出现了缺少参数的错误，导致了无法正常启动的问题。

https://github.com/vllm-project/vllm/issues/3975
这是一个需求报告，该问题单涉及的主要对象是Int8 Activation Quantization。由于需要加速Prefill的计算速度，工程团队提出了一种增量式的量化方法，为了快速支持量化模型，他们打算首先替换线性和注意力模块，并在运算前动态量化激活函数。

https://github.com/vllm-project/vllm/issues/3974
这是一个bug报告类型的issue，主要涉及LLM在多个GPU上无法加载的问题。可能由于某种原因导致在多GPU情况下无法完成模型加载，但在单GPU情况下可以顺利加载。

https://github.com/vllm-project/vllm/issues/3972
这个issue是关于Model类型的，涉及主要对象为Gemma，问题类型为为AMD提供ROCm支持后的一些微小调整。

https://github.com/vllm-project/vllm/issues/3971
该issue是关于提出需求的，主要涉及前端工具和RAG（Retrieval-Augmented Generation）。由于用户对工具调用参数、不必要的工具调用返回、文档参数的用途、以及未来功能的建议进行了讨论。

https://github.com/vllm-project/vllm/issues/3970
这个issue是一个bug报告，涉及的主要对象是vllm中的get_num_new_tokens函数。该问题产生的原因可能是由于代码中的判断逻辑错误，导致在某些情况下无法正确返回应该计算的新token数量。

https://github.com/vllm-project/vllm/issues/3969
该issue类型为功能需求，主要涉及合并对计算能力 6.x 的支持。由于代码排除了某些GPU，尽管它们可以工作，用户提出了合并支持的代码以解决这个问题。

https://github.com/vllm-project/vllm/issues/3968
这是一个关于在CPU后端添加视觉语言模型支持的问题，属于Misc类型。

https://github.com/vllm-project/vllm/issues/3967
这是一个模型相关的issue，主要涉及滑动窗口支持的添加。它需要进一步测试并添加对分页注意力功能的支持。这个问题可能是由markdown渲染问题导致的。

https://github.com/vllm-project/vllm/issues/3966
这个issue属于性能优化建议类型，主要涉及了项目中的up_proj部分。由于使用两个shrink和expand kernels计算adapter导致性能降低，提议合并它们以提高性能。

https://github.com/vllm-project/vllm/issues/3965
这个issue属于bug报告类型，涉及的主要对象是vllm库中的LML模型。由于NVIDIA A100 GPU实际上支持fp8_e5m2量化，导致用户发现代码能够成功运行，但应该不应该成功运行。

https://github.com/vllm-project/vllm/issues/3964
这是一个bug报告，该问题主要涉及的对象是benchmark_serving.py，由于环境中PyTorch版本与CUDA版本不匹配，导致benchmark_serving.py运行时生成了404 Not Found错误。

https://github.com/vllm-project/vllm/issues/3963
这个issue是一个文档修复类型的问题，涉及主要对象是模型选择和使用。原因是先前的模型需要特殊访问权限，导致引用示例启动时出现异常。

https://github.com/vllm-project/vllm/issues/3962
这是一个bug报告，涉及vllm项目的ROCm构建失败的问题。

https://github.com/vllm-project/vllm/issues/3961
这个issue是[测试]类型的，主要涉及的对象是xformer和flash attention测试。原因是当前只在主分支中测试了flash attention实现，需要确保测试xformer attention能够端到端工作。

https://github.com/vllm-project/vllm/issues/3960
这个Issue是关于功能需求的，主要涉及实现Tree Attention功能。由于计划替换已有的kernel，提出了疑问如何实现新功能。

https://github.com/vllm-project/vllm/issues/3959
这是一个bug报告，主要涉及的对象是docker build中的缓存机制。由于docker build中的缓存共享在多个构建之间存在竞争条件，导致有时会使用缓存的vllmnccl轮子，有时会使用源码分发，造成不确定的行为。

https://github.com/vllm-project/vllm/issues/3958
该issue为用户提出需求，并询问是否可以启用前缀缓存以提高在使用多个LoRa时的性能。

https://github.com/vllm-project/vllm/issues/3957
这是一个bug报告，主要涉及的对象是vllm项目。由于定义了一个名为HOST_IP的环境变量导致初始化过程hang。

https://github.com/vllm-project/vllm/issues/3956
这是一个Bug报告类型的Issue，涉及主要对象为vllm和HF transfomers。由于输入prompt中包含中文单词可能导致vllm与HF transfomers之间在mixtral 8x7b输出一致性方面出现不一致的结果。

https://github.com/vllm-project/vllm/issues/3955
这个issue是关于bug修复的，主要涉及到vllm中的utils.py文件中merge_dict函数的TypeError问题。原因是在代码中出现了'type' object is not subscriptable的错误。

https://github.com/vllm-project/vllm/issues/3954
该issue是关于优化MoE实现性能的进一步改进。

https://github.com/vllm-project/vllm/issues/3953
这是一个Bug报告，主要涉及到无法连续调用Open AI API服务器时指定`guided_grammar`参数。原因可能是在指定参数时出现的错误。

https://github.com/vllm-project/vllm/issues/3952
这是一个Bug报告，主要对象是vLLM的StableLM 12b模型，由于head size设置错误导致了错误的症状。

https://github.com/vllm-project/vllm/issues/3951
这是一个bug报告类型的issue，主要涉及vLLM项目中的speculative decoding功能的端到端正确性测试，由于生成的输出在使用和不使用speculative decoding时温度为0时应相等，但发现了几处错误导致测试无法通过。

https://github.com/vllm-project/vllm/issues/3950
该issue是关于项目结构重构的讨论，涉及到将部分模块移动到不同的目录下，主要涉及到vllm的模型执行器和设备通讯相关模块。原因是为了更好地组织代码结构和功能模块。

https://github.com/vllm-project/vllm/issues/3949
这个issue属于功能需求提出类型，主要涉及到"LoRA gptbigcode"模型的实现。

https://github.com/vllm-project/vllm/issues/3948
这个issue是文档相关的，涉及到了项目支持策略。

https://github.com/vllm-project/vllm/issues/3947
这是一个关于软件质量的问题，涉及的主要对象是软件中的一个测试用例。问题是由于测试用例的不稳定性导致了CI流程中的失败。

https://github.com/vllm-project/vllm/issues/3946
这是一个bug报告，涉及的主要对象是step_top_logprobs变量。由于step_top_logprobs为None，调用`step_top_logprobs[token_id]`时会导致异常。

https://github.com/vllm-project/vllm/issues/3945
这个issue属于Bug报告类型，涉及对象为OpenAi兼容服务器中`guided_json`参数。产生这个问题是由于PR CC(Add guided decoding for OpenAI API server)添加了guided decoding，但行为不一致/意外。

https://github.com/vllm-project/vllm/issues/3944
这是一个Bug报告，涉及VLLM在启用`enableprefixcaching`时出现crash的问题。

https://github.com/vllm-project/vllm/issues/3943
这是一个需求提出类型的issue，主要对象涉及"magic wand"版本，由于缺乏具体规范导致需要制定测试计划来运行远程推送。

https://github.com/vllm-project/vllm/issues/3942
这是一个用户提出需求的issue，主要涉及vllm模型在使用过程中存在的问题，用户希望实现token级别的重用以提高效率。

https://github.com/vllm-project/vllm/issues/3941
这个issue属于bug报告类型，主要涉及vllm安装过程中出现的CUDA问题。由于缺少CUDA运行时环境，导致出现了相关错误提示。

https://github.com/vllm-project/vllm/issues/3940
这是一个用户提出需求的issue，主要涉及vllm支持序列并行的功能。原因可能是用户希望进行序列并行操作，但目前vllm尚不支持该功能。

https://github.com/vllm-project/vllm/issues/3938
这是一个bug报告，涉及vllm metrics在启用`--engine-use-ray`时消失的问题。原因是无法在uvicorn.run和AsyncLLMEngine进程之间共享prometheus_client的REGISTRY变量，导致vLLM的指标无法显示。

https://github.com/vllm-project/vllm/issues/3937
这是一个关于Bug修复的Issue，主要涉及到Prometheus metrics输出中model_name标签存在问题。由于部署本地模型时暴露了模型路径信息，并且metrics中的model_name与ModelCard中提供的模型名称不一致，导致了这个问题的产生。

https://github.com/vllm-project/vllm/issues/3936
该issue属于测试类型，涉及的主要对象是对使用AMD agent的Buildkite功能进行测试。

https://github.com/vllm-project/vllm/issues/3935
这个issue是关于bug报告，主要涉及vllm在OpenAI Compatible Server中出现的leading space within content的问题，可能由于软件版本或代码实现导致了这一bug。

https://github.com/vllm-project/vllm/issues/3934
这是一个用户提出需求的issue，主要涉及对象是LoRa的支持。由于当前代码不支持高于64的LoRa ranks，导致用户无法使用r=128和r=256，从而出现数值错误的bug。

https://github.com/vllm-project/vllm/issues/3933
这是一个关于添加CUDA架构版本依赖内容的issue，提到了v100图形卡不受支持的问题。

https://github.com/vllm-project/vllm/issues/3932
这是一个Bug报告，主要涉及到vLLM在编译依赖版本号问题的修复。由于编译版本提示问题导致CUDA_SUPPORTED_ARCHS支持变量列表不清晰，用户对Punica在7.0下运行的疑问。

https://github.com/vllm-project/vllm/issues/3931
这是一个关于需求的问题，涉及对象是如何使用VLLM加载模型。由于用户只有1个RTX4090（24G），希望VLLM能够将某些层迁移到CPU，某些层保留在GPU上。

https://github.com/vllm-project/vllm/issues/3930
这是一个用户询问关于获取稳定版本的docker镜像的问题，主要涉及vllm的安装和使用。用户可能遇到启动报错的问题，希望找到可用的稳定版本docker镜像解决该问题。

https://github.com/vllm-project/vllm/issues/3929
这个issue类型为用户提出需求，涉及主要对象为VLLM。用户在询问VLLM的Page Attention V2是否包含了Flash Decoding的实现。

https://github.com/vllm-project/vllm/issues/3928
这是一个Bug报告，涉及到CPU推断中出现错误，由于Torch未启用CUDA，导致无法加载Mixtral 8x7b模型进行推断。

https://github.com/vllm-project/vllm/issues/3927
这个issue类型为用户提出需求，主要涉及对象为vllm的paged attention功能。由于用户不清楚如何在特定模型上打开paged attention功能，因此提出了如何使用该功能的问题。

https://github.com/vllm-project/vllm/issues/3926
这是一个bug报告，主要涉及 PyTorch 在特定环境下出现了`RuntimeError: ptxas` failed with error code 1`。原因可能是与 CUDA 版本或相关依赖库不兼容。

https://github.com/vllm-project/vllm/issues/3925
这是一个Bugfix类型的issue，涉及的主要对象是GPTNeoX模型在vLLM中加载参数时出现KeyError，原因是`rotary_emb.cos_cached`和`rotary_emb.sin_cached`存在导致KeyError的问题。

https://github.com/vllm-project/vllm/issues/3924
这是一个关于增加Fuyu-8B支持的模型问题。

https://github.com/vllm-project/vllm/issues/3923
该issue属于用户提出需求类型，主要对象是vllm下的openai entrypoint，由于现有的entrypoints只能在GPU上工作，用户希望添加对CPU的支持。

https://github.com/vllm-project/vllm/issues/3922
这个issue属于[Feature]类型，主要涉及Marlin kernel的扩展，支持AutoGPTQ模型，提供更多功能，方便用户在Marlin kernels中运行现有的AutoGPTQ模型。

https://github.com/vllm-project/vllm/issues/3921
这是一个用户提出需求的issue，主要涉及的对象是vllm项目。用户询问关于“ROCm + Triton Backend”是否已经存在于代码中，表明用户想了解该功能在哪里可以找到。

https://github.com/vllm-project/vllm/issues/3920
这是一个bug报告，主要涉及加载Command R时出现错误，导致无法运行模型。

https://github.com/vllm-project/vllm/issues/3919
该issue是一个Bug修复类型的问题，主要涉及到CommandR模块中的RoPE max_position_embeddings参数。由于使用了错误的max_model_len导致长上下文使用有问题，因此用户提出了这个修复。

https://github.com/vllm-project/vllm/issues/3918
这是一个Bug报告，涉及到`Prefix Caching`的性能不稳定问题，导致首个token的处理时间在不同请求下波动较大。

https://github.com/vllm-project/vllm/issues/3917
这个issue属于功能需求提议类型，主要涉及到HPU利用时间的测量功能。由于用户需要对工作量片段进行时间测量，并添加相应的代码调用。

https://github.com/vllm-project/vllm/issues/3916
这是一个Bug报告，主要涉及vllm的构建过程中出现的错误，用户提到遇到了"bus error (core dumped)"的问题。

https://github.com/vllm-project/vllm/issues/3915
这个issue是一个需求反馈，主要涉及benchmark脚本的cpu选项添加。原因是在进行性能测试时需要指定cpu选项。

https://github.com/vllm-project/vllm/issues/3914
这是一个bug报告，该问题涉及的主要对象是vLLM。用户提出了vLLM在同时使用`prompt_logprobs`和`enable_prefix_caching`时会导致程序崩溃的问题。

https://github.com/vllm-project/vllm/issues/3913
这是一个关于重构ops和cache_ops层的issue，主要涉及vLLM代码的优化和扩展，由于现有的ops和cache_ops层结构不够抽象，导致需要添加第三方高性能ops/内核实现时存在困难。

https://github.com/vllm-project/vllm/issues/3912
这是一个bug报告，该问题涉及VLLM库中FlashAttention包未找到导致的错误信息显示问题。

https://github.com/vllm-project/vllm/issues/3911
这是一个bug报告，问题涉及vllm0.4.0 post1版本中双卡加载qwen72b出现的'ncclGetVersion'未定义错误。导致这个问题的原因可能是ncclGetVersion函数未正确定义或加载，导致无法使用。

https://github.com/vllm-project/vllm/issues/3910
这是一个关于文档的问题，主要对象是关于prefix caches release的时机，原因可能是用户需要了解在多个请求之间具有长时间间隔时如何释放缓存。

https://github.com/vllm-project/vllm/issues/3909
这是一个Bug报告类型的issue，涉及主要对象为在环境中尝试服务Mixtral8x7B时出现的subprocess.CalledProcessError错误信息。这个问题可能是由于编译命令中的参数设置或CUDA张量共享问题导致的。

https://github.com/vllm-project/vllm/issues/3908
这是一个用户求助问题，主要涉及用户如何在两个GPU上运行特定模型，由于不了解如何在vllm中进行集成，导致无法进行推理操作。

https://github.com/vllm-project/vllm/issues/3907
这是一个bug报告，涉及到vLLM中的消息协议结构的更新。产生这个问题可能是由于消息字段存在潜在问题，需要对象化结构来解决。

https://github.com/vllm-project/vllm/issues/3906
这是一个Bug报告，涉及的主要对象是OpenAI服务器。由于输入不是有效的字符串，导致了API请求失败，返回状态码400的错误。

https://github.com/vllm-project/vllm/issues/3905
这是一个[Kernel]类型的issue，主要涉及PyTorch Labs Fused MoE Kernel Integration，由于集成后出现了一些矩阵形状不匹配导致结果有轻微差异的bug。

https://github.com/vllm-project/vllm/issues/3904
这是一个需求类型的issue，主要涉及的对象是distributed_init和worker。由于distributed_init当前与worker耦合，导致其只适用于gpu backend，需要refactor使其变得更通用。

https://github.com/vllm-project/vllm/issues/3903
这是一个Bugfix类型的issue，主要涉及的对象是Prefix Caching Guards。由于`context_attention_fwd`存在限制，导致不支持prefix caching的情况，因此添加了相应的检查。

https://github.com/vllm-project/vllm/issues/3902
这是一个用户提出需求的issue，主要涉及vLLM在多节点分布式推断上的支持。该问题由于部分团队可能缺乏Ray知识而提出希望在Kubernetes上提供简单的协同功能。

https://github.com/vllm-project/vllm/issues/3901
这个issue是一个Bugfix类型的问题，涉及主要对象是vLLM的prefix caching warmup步骤。导致这个问题的原因是因为执行context_fwd_attention时生成的代码会导致第一个请求延迟约3秒。

https://github.com/vllm-project/vllm/issues/3900
这是一个Bug报告，主要涉及对象是vllm中的'MergedColumnParallelLinear'模块。导致这个Bug的原因可能是该模块缺少'weight'属性。

https://github.com/vllm-project/vllm/issues/3899
这是一个Bugfix类型的issue，主要涉及的对象是logits processor和prompt logprobs。由于prompt logprobs未被处理导致bug，用户重新提交了相关修改。

https://github.com/vllm-project/vllm/issues/3898
这是一个Bug报告，涉及主要对象是使用python code生成google/gemma2b模型pompts时出现的KeyError bug。

https://github.com/vllm-project/vllm/issues/3897
这是一个关于如何确定vllm引擎是否已满的问题。用户想知道如何检查引擎是否接受更多请求以避免排队，并询问是否可以获取引擎利用率的问题。

https://github.com/vllm-project/vllm/issues/3896
这是一个bug报告，主要涉及Marlin内核构建时出现多个警告，表达了右操作数的”%"为零。这可能是由于代码中存在除零操作导致的错误。

https://github.com/vllm-project/vllm/issues/3895
这个issue是一个Bug报告，涉及的主要对象是软件的高内存使用问题，其原因是由于高内存使用导致程序被终止。

https://github.com/vllm-project/vllm/issues/3894
该issue类型为功能性开发，涉及对象为LLMEngine和speculative decoding组件的集成。由于尚未实现完整的e2e正确性实现，导致输出为垃圾。

https://github.com/vllm-project/vllm/issues/3893
这个issue是一个关于在vLLM项目中添加新的模型（minicpm和其变体）的类型为[Model]的请求。

https://github.com/vllm-project/vllm/issues/3892
这是一个bug报告，主要涉及vllm下的一个issue，由于在生成总长度超过8192个标记时，CommandR输出不正确，而理论上模型通过绳子扩展支持128k长度，但出现此问题。

https://github.com/vllm-project/vllm/issues/3891
这是一个Bug报告。该问题涉及的主要对象是在WSL 2中无法加载lora adapters。导致此问题的原因可能是环境设置或依赖项配置的错误。

https://github.com/vllm-project/vllm/issues/3890
这是一个优化需求，主要对象是代码中的一些低效部分，由于代码效率不高导致性能延迟较高。

https://github.com/vllm-project/vllm/issues/3889
这个issue类型为性能优化需求，主要涉及的对象是对Latency benchmark的稳定性要求。该问题由于之前使用平均延迟计算不稳定，提出了使用更多迭代次数、采用中值而非平均值来增加稳定性的解决方案。

https://github.com/vllm-project/vllm/issues/3887
这是一个Bug报告类型的Issue，涉及的主要对象是VLLM runtime。这个问题可能是由于if语句缺少对self.cache_config.enable_prefix_caching的条件检查而导致的。

https://github.com/vllm-project/vllm/issues/3886
这是一个bug报告类型的issue，主要涉及对象是使用GPTQ进行quantize的JAIS模型。由于特定层的输入特征不是32的倍数，导致quantization时出现数值对齐错误，进而引发了数值错误异常。

https://github.com/vllm-project/vllm/issues/3885
这个issue类型是需求用户提出需求，主要对象是关于如何最大化Typical GPUs（如A100，V100）的吞吐量表现。由于设置的参数（max_num_batched_tokens=10240, max_num_seqs=1024）不符合预期，导致GPU利用率仅为20%。

https://github.com/vllm-project/vllm/issues/3884
该问题是关于核心功能的一个issue，涉及到完全可工作的分块预填充终端到终端功能。

https://github.com/vllm-project/vllm/issues/3883
这是一个Bugfix类型的issue，涉及修复Llava在`tensor_parallel_size > 1`时的推断问题。原因是之前在`RayGPUExecutor`中没有将`vision_language_config`传递给`worker`。

https://github.com/vllm-project/vllm/issues/3882
这是一个Bug报告。该问题主要涉及vllm中运行`examples/llava_example.py`时在`tensor_parallel_size > 1`下出错。

https://github.com/vllm-project/vllm/issues/3880
这个issue是关于bug报告的，主要对象是vllm下的一个prefix caching功能。由于fp8 quantized KV Cache的问题导致triton kernel出现错误，无法使用prefix caching功能。

https://github.com/vllm-project/vllm/issues/3879
这是一个Bug报告，主要涉及的对象是使用mistralai/Mistral7BInstructV0.2时出现的错误TypeError: 'TokenizerGroup' object is not callable。

https://github.com/vllm-project/vllm/issues/3878
这是一个关于Bug报告的issue，涉及的主要对象是vllm 0.4.0.post1和AMD MI250x。由于使用特定的Docker命令加载模型后，vllm服务在加载模型后崩溃。

https://github.com/vllm-project/vllm/issues/3876
该issue是一个Bugfix类型的issue，主要涉及到使用`min_tokens`和`prompt_logprobs`参数时可能出现的bug。原因是在这两种参数一起使用时，`logits`的形状会受到影响，导致`start_idx`指针无法正确跟踪序列。

https://github.com/vllm-project/vllm/issues/3874
这是一个Bug报告，主要对象是vllm库下的offline_inference.py脚本以及GPU内存清理程序，由于enforce_eager参数设置不同导致在非eager模式下GPU内存清理不完全，引发GPU内存未完全释放问题。

https://github.com/vllm-project/vllm/issues/3873
这个issue是一个用户提出需求的类型，主要涉及到缺乏llava 1.5在OpenAI兼容服务器上使用的文档和示例。用户询问是否有参考示例来使用llava通过OpenAI SDK，并提出了潜在的问题，但未收到回应。

https://github.com/vllm-project/vllm/issues/3872
这是一个Bug报告，涉及到vLLM模型在生成对话回复时出现问题，可能是由于停止生成响应导致。

https://github.com/vllm-project/vllm/issues/3871
这个issue类型是用户需求。该问题单主要涉及的对象是"Core"。由于未明确说明如何添加三行代码以启用"out-of-tree model register"，用户提出了需要相关帮助的需求。

https://github.com/vllm-project/vllm/issues/3870
这个issue类型是用户提出需求，主要涉及的对象是实现基于当前聊天和聊天历史的AI对话功能，请求使用vllm和llama-2-13B模型帮助实现。

https://github.com/vllm-project/vllm/issues/3869
这是一个bug报告，主要涉及OLMo模型中的Tensor Parallelism，由于未实现`ff_proj`的Tensor Parallelism导致输出错误。

https://github.com/vllm-project/vllm/issues/3868
这是一个功能需求类型的issue，涉及主要对象为LM Format Enforcer Guided Decoding Support。由于用户simonmo提出需要添加LMFE解码支持到OpenAI服务器，故提出了此问题。

https://github.com/vllm-project/vllm/issues/3867
这个issue是一个bug报告，涉及的主要对象是处理`_apply_logits_processors`函数中的`prompt_logprobs`时产生的AssertionError。原因是在处理带有`logits_processors`和`prompt_logprobs`的请求时，出现了`AssertionError`，因为代码断言所有logits张量的行都已被处理。

https://github.com/vllm-project/vllm/issues/3865
该issue类型是bug报告，涉及的主要对象是requirements.txt文件。由于未在多个版本要求之间添加逗号，导致无法成功执行 `pip install e .`。

https://github.com/vllm-project/vllm/issues/3864
这是一个修复lint错误的问题，涉及的主要对象是代码文件quantize.py。

https://github.com/vllm-project/vllm/issues/3863
这是一个功能需求，用户提出了希望能够通过pytest标记来避免全局测试清理，以加快CPUonly测试的速度，即使在同一目录/文件中使用GPU的其他测试。

https://github.com/vllm-project/vllm/issues/3862
这是一个Bug报告，主要涉及vllm在ROCm环境下构建时出现了版本兼容性问题导致的错误。

https://github.com/vllm-project/vllm/issues/3861
这个issue类型为需求提出，主要涉及的对象是vLLM的项目路线图。由于用户希望在Q2 2024看到vLLM的新特性和发展方向，提出了对性能优化、硬件覆盖、模型支持等方面的需求和期望。

https://github.com/vllm-project/vllm/issues/3860
这个issue类型是bug报告，主要涉及的对象是pynccl库。该问题由于使用了不稳定的args参数，导致可能出现`1`的错误，解决方法是使用self.rank等属性代替args。

https://github.com/vllm-project/vllm/issues/3859
该issue为bug报告，主要涉及对象是`vllm_nccl`包的安装与pip缓存相关的问题，由于pip在缓存中存储wheel会导致其他CI作业直接使用缓存中的wheel，而需要手动删除缓存。

https://github.com/vllm-project/vllm/issues/3858
这个issue是关于bug报告，涉及到PyNCCL errors在结束时出现。原因是与某个PR相关的更改导致的。

https://github.com/vllm-project/vllm/issues/3857
这是一个关于持续集成中构建wheels而非源码安装的问题，属于技术优化类型，主要涉及到项目的构建流程。这可能是为了提高构建性能和可重复性而提出的建议。

https://github.com/vllm-project/vllm/issues/3856
这个issue属于bug报告类型，主要涉及的对象是vllm项目中的CPU executor。由于此处的代码使用了一个在编写时不存在的属性，导致CPU executor分支缺乏测试，因此用户建议改进executor后端的测试策略以提高测试覆盖率。

https://github.com/vllm-project/vllm/issues/3855
这是一个关于特性提议的issue，主要对象是vLLM模型，提议采用PyTorch Labs Triton kernels以提升性能。

https://github.com/vllm-project/vllm/issues/3854
该issue为Bug报告，涉及到vLLM代码中的CPU Torch更新版本导致的问题。

https://github.com/vllm-project/vllm/issues/3853
这个issue类型是功能改进，涉及 Chunked Prefill 的调度优化。由于之前一个 PR 导致了一个回归问题，需要更新调度算法来支持 chunked prefill。

https://github.com/vllm-project/vllm/issues/3852
这个issue是关于建议性质的，主要对象是项目中的一个文件`vllm/entrypoints/api_server.py`。原因是用户在项目中的很多问题中使用了`m vllm.entrypoints.api_server`，显示用户可能没有注意到该文件顶部的说明，建议删除该文件以避免将来的混淆。

https://github.com/vllm-project/vllm/issues/3851
这个issue属于bug报告类型，主要涉及到vllm下的ChatCompletionRequest类以及max_tokens参数。由于设置max_tokens后发送的大型提示接近了max_model_len长度，导致端点返回一个空输出，并且status_code为200。

https://github.com/vllm-project/vllm/issues/3850
这个issue是一个有关"[Frontend] openAI entrypoint dynamic adapter load"的需求提出，并涉及为vLLM添加动态adapter加载功能。

https://github.com/vllm-project/vllm/issues/3849
这个issue是关于bug报告，主要涉及CommandR GPTQ模型的加载问题，原因是模型加载类似于Gemma的方式，导致了无法正确加载的bug。

https://github.com/vllm-project/vllm/issues/3848
这是一个bug报告，涉及主要对象为vllm中运行Mixtral8x7b时出现的属性错误。问题可能是由于vllm版本与openai版本不兼容导致的。

https://github.com/vllm-project/vllm/issues/3847
这是一个bug报告，涉及到使用vllm库中的一个示例时出现问题，原因可能是在多GPU环境下未正确利用所有GPU资源导致CUDA内存溢出。

https://github.com/vllm-project/vllm/issues/3846
这是一个bug报告，涉及到vllm的prefix caching feature在official docker image中导致第二个请求响应时间变长的问题。

https://github.com/vllm-project/vllm/issues/3845
这个issue属于文档更新类型，涉及到VLLM引擎的参数文档更新。

https://github.com/vllm-project/vllm/issues/3844
这是一个bug报告，涉及的主要对象是 neuron 软件包。问题可能是由于缺少依赖项 `ray` 和 `outlines` 导致无法启动服务器。

https://github.com/vllm-project/vllm/issues/3843
这个issue是关于PR描述中缺少内容的问题，导致无法正确填写PR信息。

https://github.com/vllm-project/vllm/issues/3842
这是一个关于环境信息收集的bug报告，涉及的主要对象是PyTorch环境。由于环境信息中包含了不匹配的PyTorch版本和CUDA版本，导致可能出现与运行环境不匹配的问题。

https://github.com/vllm-project/vllm/issues/3841
这是一个关于代码改进的issue，主要对象是项目中的公共依赖项。由于需要更好地管理不同后端的依赖关系，因此将公共依赖项转移到`requirementscommon.txt`中。

https://github.com/vllm-project/vllm/issues/3840
这个issue是一个bug报告，涉及的主要对象是CPU backend，由于缺少`kv_scale`参数导致了CPU CI测试失败。

https://github.com/vllm-project/vllm/issues/3839
这是一个bug报告，涉及vllm中的async_llm_engine在使用多个GPU时出现错误导致服务器关闭的问题。

https://github.com/vllm-project/vllm/issues/3838
该issue类型为用户提出需求，主要涉及VLLM中MoE（例如Mixtral）在运行时动态选择专家数量的功能。由于用户想要同时保证性能和速度，希望能够通过单独触发Mixtral来使用单个7B专家，并在效率上与单个7B相媲美。

https://github.com/vllm-project/vllm/issues/3837
这个issue是关于实现RFC以增强BaseExecutor接口以支持硬件无关的推测解码，主要涉及的对象是vLLM的BaseWorker接口。由于需要实现拒绝采样器和在提议方法和验证模型之间建立适用于硬件后端的通信，导致代码需要对执行程序的逻辑进行调整以支持此功能。

https://github.com/vllm-project/vllm/issues/3836
这是一个bug报告，主要对象是`benchmark_serving.py`文件，由于参数错误导致了bug。

https://github.com/vllm-project/vllm/issues/3835
这是一个用户提出需求的类型的issue，主要涉及要发布第三次见面会的幻灯片。

https://github.com/vllm-project/vllm/issues/3834
这是一个PR类型的issue，主要涉及实现CPU/GPU在BlockManagerV2中的交换功能。源自于对vLLM项目的改进意愿。

https://github.com/vllm-project/vllm/issues/3833
这是一个用户提出需求的issue，主要涉及到VLLM工程中的分布式初始化方法，用户希望添加一个简化集群上Slurm多节点设置的选项。

https://github.com/vllm-project/vllm/issues/3832
这是一个Bug报告，主要涉及Mixtral CUDA和使用`bfloat16`时的错误。

https://github.com/vllm-project/vllm/issues/3831
这个issue是关于提交PR时未填写描述的问题报告。原因可能是疏忽或忘记填写必要的信息。

https://github.com/vllm-project/vllm/issues/3830
该issue类型为bug报告，主要涉及了request logging的改进问题。原因是在vLLM的请求日志中存在两个小的可观察性问题，导致SRE可能会认为日志中记录的提示是完整的输入，从而造成困惑。

https://github.com/vllm-project/vllm/issues/3829
这个issue是关于向vLLM添加支持Cohere CommandR+模型的更改。

https://github.com/vllm-project/vllm/issues/3828
这是一个Bug报告，主要涉及VLLM软件包。由于无法找到有效的设备ID，导致出现错误提示信息。

https://github.com/vllm-project/vllm/issues/3827
这是一个关于安装问题的bug报告，主要涉及vllm的安装。由于环境问题，导致用户无法通过pip安装vllm。

https://github.com/vllm-project/vllm/issues/3826
这是一个Bug报告，涉及V100 lora运行 punica.py出现CUDA错误的问题，可能由于设备上没有可执行的内核图像导致。

https://github.com/vllm-project/vllm/issues/3825
该issue属于用户提出需求类型，主要对象是vllm工具在不同硬件环境下的性能分析。用户在CPU系统上使用perf top时遇到了问题，希望找到更高效的工具监控性能，并寻求关于如何在使用torch作为后端框架时启用分析器的帮助。

https://github.com/vllm-project/vllm/issues/3824
这是一个关于硬件相关的Issue，提出了要分离CPUModelRunner和ModelRunner以提高维护效果。

https://github.com/vllm-project/vllm/issues/3821
这个issue是一个bug报告，主要涉及到vllm库的升级后出现的每个工作进程在每个GPU上占用内存超过1的问题。

https://github.com/vllm-project/vllm/issues/3820
这个issue是关于[Core] Dynamic model class的需求提出，主要对象是允许动态指定模型实现的方式，导致这个需求的原因是为了可以通过导入路径来动态指定模型的实现。

https://github.com/vllm-project/vllm/issues/3819
这个issue是文档更新类型的，未指明有具体的相关问题，用raw html标签来解决markdown渲染问题。

https://github.com/vllm-project/vllm/issues/3818
这是一个Bug报告，涉及Ray部署在多个节点上时，在工作节点上的'torch.cuda.is_available()'为False，导致无法分配GPU。

https://github.com/vllm-project/vllm/issues/3817
这是一个关于向vLLM项目贡献代码的用户提出需求的Issue，主要涉及启用`hf_transfer`后端的默认问题。由于环境变量的方式启用`hf_transfer`会错过一些用户，并导致即使在此Python环境中只有可用，也被强制全局启用`hf_transfer`。

https://github.com/vllm-project/vllm/issues/3816
这个issue类型是文档修改，涉及到vLLM的代码质量和代码审查流程。

https://github.com/vllm-project/vllm/issues/3815
这是一个bug报告，主要涉及到VLLM项目中的LoRA模型加载失败的问题。这可能是由于环境配置或代码错误导致的。

https://github.com/vllm-project/vllm/issues/3814
这是一个特性增强的issue，主要涉及的对象是vLLM的Intel GPU推理后端。由于需要使用特定版本的oneAPI、仅支持Intel Data Center GPU、仅支持half类型等限制，导致该issue的存在。

https://github.com/vllm-project/vllm/issues/3813
这是一个Bug报告，涉及的主要对象是vllm库中的miqu70b模型加载问题。由于在加载miqu70b模型后发生了AssertionError，可能是由于程序中的某些代码错误或者依赖关系不正确导致的。

https://github.com/vllm-project/vllm/issues/3812
这是一个关于安装问题的类型为用户提出需求的issue，主要涉及对象为Tesla V100和cuda11.4。由于没有权限安装更高版本的cuda驱动程序，导致无法成功安装vllm，用户需要帮助解决安装问题。

https://github.com/vllm-project/vllm/issues/3811
这个issue是一个安装问题，主要涉及对象是Tesla V100 GPU，由于无权限安装更高版本的CUDA驱动程序，导致无法成功安装vllm。

https://github.com/vllm-project/vllm/issues/3810
这是一个文档问题报告issue，主要对象是VLLM的文档，缺少了关于异步引擎参数的说明。

https://github.com/vllm-project/vllm/issues/3809
这是一个关于RFC（Request For Comments）的issue，主要涉及到修改ExecutorBase接口以实现硬件无关的推测解码功能，由于核心推测解码逻辑是算法性的vLLM功能，RFC提议添加BaseWorker接口以实现所有工作线程与推测解码兼容。

https://github.com/vllm-project/vllm/issues/3808
这是一个bug报告，主要涉及到vllm/vllm-openai:v0.4.0环境中无法找到libcuda.so文件的问题，导致出现AssertionError。

https://github.com/vllm-project/vllm/issues/3807
这是一个Bug报告，涉及Qwen2MoE Config配置问题，由于某些原因导致了启动时出现问题。

https://github.com/vllm-project/vllm/issues/3806
这个issue是文档更新类型的，涉及主要对象是vLLM的README.md文件。由于markdown渲染出现问题，需要使用原始的html标签，用户可能提出需要更新文档的帮助。

https://github.com/vllm-project/vllm/issues/3805
这是一个关于升级到PyTorch 2.2.1时管理NCCL问题的issue，涉及的主要对象是NCCL库。

https://github.com/vllm-project/vllm/issues/3804
这个issue属于bug报告类型，涉及的主要对象是vllm版本获取机制，在非CPU Docker镜像中无法使用`importlib.metadata.version()`函数获取版本信息，导致出现问题。

https://github.com/vllm-project/vllm/issues/3803
这个issue类型为bug报告，涉及主要对象为vllm项目中的sm 7.0/7.5 binary版本问题，由于0.4.0.post1版本升级修复了该二进制文件在预编译包中的问题。

https://github.com/vllm-project/vllm/issues/3802
这是一个需求提出类的 issue，主要对象是尝试管理 nccl 版本，可能由于系统无法有效管理 nccl 版本导致的问题。

https://github.com/vllm-project/vllm/issues/3801
这个issue类型是bug报告，主要涉及目标是CI/Build相关内容，由于当前的wheel构建未包含sm 7.0 / 7.5二进制文件，导致该问题的症状。

https://github.com/vllm-project/vllm/issues/3800
这是一个bug报告，主要涉及到VLLM库中的SafeTensorsInfo模块，由于尝试加载一个与LoRA Adapters合并的Hugging Face模型时出现了一个未知的关键字参数'sharded'的情况。

https://github.com/vllm-project/vllm/issues/3799
这是一个bug报告，涉及到缺少 "__init__.py" 文件、超时调整和名称冲突问题。导致bug产生的原因可能是这些问题导致程序无法正确运行。

https://github.com/vllm-project/vllm/issues/3798
这是一个关于bug报告的issue，主要涉及的对象是在`vllm/core/block/`和`vllm/spec_decode/`目录下添加`__init__.py`文件。由于缺少`__init__.py`文件，导致从这些模块中导入时出现问题。

https://github.com/vllm-project/vllm/issues/3797
这个issue是关于bug报告，主要涉及的对象是VLLM推理Yi:34b。由于某种原因导致推理过程一直重复直到达到最大token，用户寻求协助解决这个问题。

https://github.com/vllm-project/vllm/issues/3796
这是一个bug报告，涉及的主要对象是vllm的一个entrypoint无法正常运行，并且导致出现错误。

https://github.com/vllm-project/vllm/issues/3795
这个issue属于Bug报告类型，主要涉及FastAPI Swagger中/docs页面无法在离线状态下正确显示，可能是由于无法下载CDN中的.js文件导致的问题。

https://github.com/vllm-project/vllm/issues/3794
这是一个功能需求提议，该问题涉及的主要对象是`vllm`的`outlines`依赖，用户希望`vllm`可以将其结构生成依赖设置为可选，因为用户需要使用`outlines`包的新版本中的修复。

https://github.com/vllm-project/vllm/issues/3793
这是一个Bug报告，涉及到RuntimeError和内核适配问题，报告者的当前环境发生了错误。

https://github.com/vllm-project/vllm/issues/3792
这是一个用户询问问题类型的issue，主要涉及对象是vllm和tensorRT。用户提出了如何将vllm与tensorRT集成以运行特定模型推断的问题。

https://github.com/vllm-project/vllm/issues/3791
这个issue是关于bug报告，主要涉及vLLMEngine的文档页面无法正常访问，需要在requirementsdocs.txt中添加transformers和pycpuinfo依赖才能解决问题。

https://github.com/vllm-project/vllm/issues/3790
这是一个性能问题的issue，主要涉及部署模型时使用Ray导致速度缓慢的症状。

https://github.com/vllm-project/vllm/issues/3789
这是一个特性需求类型的issue，主要涉及在vLLM中添加OpenTelemetry分布式追踪功能。由于缺乏分布式追踪功能，用户无法将追踪数据导出到可视化工具，可能导致缺乏系统的观察性和故障排查能力。

https://github.com/vllm-project/vllm/issues/3788
该issue是一个bug报告，主要涉及vLLM前端的一个问题，即允许在根路径下使用默认中间件时可能存在的问题。

https://github.com/vllm-project/vllm/issues/3787
这是一个bug报告的issue，主要涉及到CPU backend CI的问题，由于CC([Hardware][Intel] Add CPU inference backend)段引入了CI错误，导致了markdown渲染失败。

https://github.com/vllm-project/vllm/issues/3786
这是一个功能请求issue，主要涉及的对象是vllm 0.4.0版本。由于vllm从0.3.3版本升级到0.4.0版本后，在基于`nvcr.io/nvidia/pytorch:23.10py3`的Python 3.10，cuda 12.2.2，torch 2.1.0a0+32f93b1的docker镜像上出现错误，而0.3.3版本没有这个问题。

https://github.com/vllm-project/vllm/issues/3785
这个issue是一个bug报告，涉及的主要对象是在使用Mixtral 8x7B时在P100设备上出现的CUDA错误。由于在V100设备上没有此问题，可能由于不同设备的硬件或驱动导致此错误。

https://github.com/vllm-project/vllm/issues/3784
这是一个Bug报告，主要涉及的对象是vllm模块。由于缺少vllm._C模块导致了ModuleNotFoundError错误，可能是环境中缺少必要的依赖或安装不完整所致。

https://github.com/vllm-project/vllm/issues/3783
这是一个关于改善CI/Build速度的建议，主要涉及到block manager CPU-only unit tests。原因是全局清理torch缓存对于CPUonly测试是不必要的，并且导致了CPUonly测试的严重减慢。

https://github.com/vllm-project/vllm/issues/3782
这是一个Bug报告，涉及主要对象为CUDA 11.8，由于一个PR导致CUDA 11.8不支持layernorm优化。

https://github.com/vllm-project/vllm/issues/3781
该issue属于bug报告类型，主要涉及Mistral-7B-v0.1服务器命令，原因是在使用时出现了`NotImplementedError: Sliding window is not allowed with prefix caching enabled!`错误，用户希望找到一种方法可以关闭滑动窗口而保持前缀缓存功能。

https://github.com/vllm-project/vllm/issues/3780
这个issue是关于RFC（Request for Comments），主要涉及到vllm如何测试和支持第三方模型。这个问题主要由于vllm与transformers之间的一致性检查存在困难，导致开发者认为应该将第三方模型的支持设为"best effort"，而不是严格与transformers的实现一致。

https://github.com/vllm-project/vllm/issues/3779
这是一个Bug报告类型的Issue，涉及对象是vllm-0.4.0版本的软件。由于vllm-0.4.0版本中的一个代码行导致无法获取vllm的版本号属性，从而引发了该Bug。

https://github.com/vllm-project/vllm/issues/3778
这是一个bug报告，涉及的主要对象是名为Qwen-14B-Chat-Int4的项目。由于某种错误导致了引导json文件的问题。

https://github.com/vllm-project/vllm/issues/3777
这个issue是一个bug报告，主要涉及到避免加载不正确的LoRA配置文件。由于未在提交前检查完整性，可能会导致加载错误的LoRA配置，造成错误。

https://github.com/vllm-project/vllm/issues/3776
该issue是关于代码维护的建议，主要涉及到分离CPUModelRunner和ModelRunner，提高维护性。

https://github.com/vllm-project/vllm/issues/3775
这是一个bug报告，涉及主要对象为OLMo模型。导致该问题的可能原因是在`tensor_parallel_size>1`情况下运行时输出结果不正确，推测可能是SwiGLU中的张量并行处理有问题。

https://github.com/vllm-project/vllm/issues/3773
这个issue类型是bug报告，涉及的主要对象是vLLM的openai服务。由于新版本的openai在传递参数时出现错误，导致无法成功调用客户端的chat.completions.create方法。

https://github.com/vllm-project/vllm/issues/3772
这是一个Bug报告，涉及的主要对象是在使用JAIS模型时遇到的`n_inner`参数错误导致的GPU并行化问题。

https://github.com/vllm-project/vllm/issues/3771
这是一个关于bug报告的issue，涉及的主要对象是docker启动vllm时配置了host_IP，但仍然出现连接超时的错误。

https://github.com/vllm-project/vllm/issues/3770
这个issue是一个bug报告，涉及的主要对象是在使用torch.cuda.set_device时出现崩溃，原因是新的使用状态收集器导入了vllm.model_executor.model_loader，导致transformers开始使用尚未通过_init_workers_ray配置的cuda环境进行上下文初始化，从而导致使用tp > 1情况下崩溃。

https://github.com/vllm-project/vllm/issues/3769
这个issue属于修复问题（Bug Fix），涉及主要对象为requirements.txt文件。由于requirements.txt文件中psutil重复导致问题，并添加了tiktoken作为ROCm依赖，需要进行修复。

https://github.com/vllm-project/vllm/issues/3768
这是一个bug报告，主要涉及TTFT计算和Chat Completions API，由于之前在delta中没有检查实际`content`是否为`Null`，可能会误将`role` chunk与空`content`误认为是真实标记响应。

https://github.com/vllm-project/vllm/issues/3767
这个issue类型是bug报告，主要涉及的对象是vLLM模型配置。原因是`attention_bias`未能按预期从Llama模型配置中加载，导致功能异常。

https://github.com/vllm-project/vllm/issues/3766
这是一个bug报告，涉及的主要对象是vllm在使用多张A100卡部署时出现失败。由于环境问题和可能的兼容性原因导致无法成功部署，用户希望找到导致多卡部署问题的原因并解决此问题。

https://github.com/vllm-project/vllm/issues/3765
这是一个bug报告，主要涉及vllm在使用中无法返回内容的问题。可能是由于未正确集成具体模型导致。

https://github.com/vllm-project/vllm/issues/3764
这个issue是关于bug报告，涉及的主要对象是2080ti GPU，用户遇到了vllm=0.4.0在启动时出现的错误。

https://github.com/vllm-project/vllm/issues/3763
该issue属于性能优化类型，主要涉及到并行工作者在每个步骤中调度的开销。

https://github.com/vllm-project/vllm/issues/3762
这个issue属于设计讨论类型，涉及主要对象为vllm项目中的launcher/task/coordinator/communicator设计和实现。

https://github.com/vllm-project/vllm/issues/3760
这是一个Bug报告，涉及对象是vllm工具的min_tokens参数，用户反映了无法找到`SamplingParams` API中的min_tokens，并寻求从源码安装解决该问题。由于该参数缺失，导致用户无法进行文本摘要任务。

https://github.com/vllm-project/vllm/issues/3759
这个issue类型是bug报告，涉及的主要对象是TTFT（Time to First Token）。导致两种不同环境下TTFT不同的原因可能是配置设置或不同版本的VLLM。

https://github.com/vllm-project/vllm/issues/3758
这是一个文档问题（Doc issue），主要涉及到`--engine-use-ray`参数未在文档中记录的bug。

https://github.com/vllm-project/vllm/issues/3756
这是一个用户提出需求的issue，主要涉及vllm模型是否支持特定的quantization类型，用户希望能够在弱设备上运行该模型。

https://github.com/vllm-project/vllm/issues/3755
这个issue是用户提出需求，希望有更真实对话来进行基准测试。

https://github.com/vllm-project/vllm/issues/3754
这是一个bug报告，涉及的主要对象是`RayTokenizerGroupPool`。由于在PR之前，`RayTokenizerGroupPool`中的`tokenizer_config`未传递给`self._local_tokenizer_group`，导致了初始化失败，对于需要配置的tokenizer（例如`trust_remote_code=True`）无法正常工作。

https://github.com/vllm-project/vllm/issues/3753
这个issue是关于CI/Build的，涉及Marlin测试出现失败的问题，由于Marlin与exllama内核之间的bitwise正确性不一致，导致测试失败。

https://github.com/vllm-project/vllm/issues/3752
这是一个关于vllm中`prompt_logprobs`参数使用的问题，类型是用户提出需求，并问及关于该参数预期输出的问题。

https://github.com/vllm-project/vllm/issues/3751
这个issue是一个Bugfix类型的问题，主要涉及支持AutoAWQ Models Serialized in Marlin Format，由于未添加对Marlin格式的AutoAWQ Models的支持，导致了AWQ + Marlin Error。

https://github.com/vllm-project/vllm/issues/3750
这是一个需求提出类型的issue，主要涉及vllm中如何将LLM对象固定到特定的CUDA设备上。原因是用户希望在同一Python脚本中使用多个vllm实例，每个实例在不同的CUDA设备上运行。

https://github.com/vllm-project/vllm/issues/3749
这是一个功能需求的issue，主要涉及对象是VLLM项目中的detokenization功能。由于用户需求或者实际场景需要，在生成文本时可选择是否进行token还原。

https://github.com/vllm-project/vllm/issues/3748
这是一个功能需求，该功能需求主要涉及代码中的初始化过程。由于需要在初始化时禁用分词器，用户提出需要支持在不进行分词的情况下生成文本，因此提出了该需求。

https://github.com/vllm-project/vllm/issues/3747
这是一个bug报告，主要涉及的对象是vllm后 fastapi的 server，由于模型权重格式问题导致推理过程卡住。

https://github.com/vllm-project/vllm/issues/3746
这是一个文档更新的Issue，涉及到构建源码和torch/cuda版本依赖的说明。该Issue的主要原因是用户安装时遇到困难，尤其在使用Cmake构建系统时出现了更多问题。

https://github.com/vllm-project/vllm/issues/3745
这是一个文档更新的Issue，涉及更新 Transformers 到 v4.39.2 版本的内容。原因是为了修复向后兼容性和功能性问题。

https://github.com/vllm-project/vllm/issues/3744
这是一个 Bug 报告，涉及主要对象为在 MGX 平台（NVidia GH200）上使用 Ray 的多节点服务。由于 ARM 架构上不可用 Cupy NCCL 导致的问题，导致出现了多节点工作人员初始化时的 hang 问题。

https://github.com/vllm-project/vllm/issues/3743
这是一个Bug报告类型的Issue，涉及主要对象是VLLM在一个机器上启动两个模型时出现的CUDA错误，导致第二个模型启动失败。

https://github.com/vllm-project/vllm/issues/3742
这是一个用户提出需求的issue， 主要对象是Pytorch 2.2 版本。用户希望Pytorch 2.2 版本能够实现更快的flash attention功能，目前的Pytorch 2.1版本速度较慢。

https://github.com/vllm-project/vllm/issues/3741
这是一个关于性能优化的bug报告，主要涉及对象是tokenizer。由于计算tokenizer的长度过于昂贵，因此导致性能问题。

https://github.com/vllm-project/vllm/issues/3740
这个issue属于bug报告，主要涉及的对象是vLLM项目中的tokenizer。原因是由于一个提交导致了性能下降，造成了latency和throughput的明显变化。

https://github.com/vllm-project/vllm/issues/3739
这是一个bug报告，主要涉及CMake构建生成的可执行文件包含不必要的PTX的问题，由于缺少`real`标志导致体积翻倍。

https://github.com/vllm-project/vllm/issues/3738
这是一个关于bug的报告，涉及主要对象是日志系统。此问题是由于某些文件未使用中央配置的日志记录器而引起的。

https://github.com/vllm-project/vllm/issues/3737
这是一个技术性问题，涉及到使用了已经废弃的`pkg_resources`模块而导致需要更新到非废弃的`importlib`模块的更改。

https://github.com/vllm-project/vllm/issues/3736
这是一个用户提出需求的 issue，主要涉及到如何从`AsyncLLMEngine`中提取`stat_logger`的信息，但存在访问困难，可能是由于异步运行导致的。

https://github.com/vllm-project/vllm/issues/3735
这是用户提出的需求类型的问题，涉及主要对象为`vllm`，由于用户需要另一种获取库版本的途径来符合他们的docker安装, 导致提出了这个问题。

https://github.com/vllm-project/vllm/issues/3734
这是一个需求提报的issue，主要涉及的对象是e5-mistral-7b-instruct模型和Embedding API。由于需要在embedding模式下关闭KV缓存，并实现嵌入向量生成功能，所以用户提出了关于功能集成和后端计算的问题。

https://github.com/vllm-project/vllm/issues/3732
这是一个bug报告，主要涉及加载不了`mixtral`模型，可能是由于模型结构或环境问题导致的。

https://github.com/vllm-project/vllm/issues/3731
这个issue类型为Bug报告，主要对象是vLLM的前端，由于logprobs=0的行为与OpenAI规范不一致导致了错误。

https://github.com/vllm-project/vllm/issues/3730
这是一个关于CI / Build的issue，主要涉及FP8 KV缓存系统的困惑度计算测试，旨在测试不同量化和缩放方法下的困惑度表现。

https://github.com/vllm-project/vllm/issues/3729
这是一个需求提出类型的issue，主要涉及模型加载过程中的性能优化，提出了使用分布式加载方法来加速模型加载的需求。

https://github.com/vllm-project/vllm/issues/3728
这是一个bug报告，主要涉及的对象是Cohere模型，由于 Transformers 库过时导致无法加载模型。

https://github.com/vllm-project/vllm/issues/3727
这是一个bug报告，主要涉及对象是`CohereForAI/c4aicommandrv01`模型的最大上下文窗口长度设置问题，可能由于参数`max_position_embeddings`和`model_max_length`不一致导致用户无法正确设置模型长度。

https://github.com/vllm-project/vllm/issues/3726
这是一个bug报告，主要涉及的对象是vllm的安装过程。这个问题可能由于ninja工具未知目标导致安装失败。

https://github.com/vllm-project/vllm/issues/3725
这是一个关于提议（Proposal）的issue，主要涉及对象是为vllm添加对Intel GPU的初步支持。原因可能是想要实现更好的性能和扩展性。

https://github.com/vllm-project/vllm/issues/3724
这是一个用户报告的bug类型的issue，涉及主要对象为CuPy和Multiprocess模块。原因可能是在安装vllm0.3.3和相关包时出现了“AssertionError: daemonic processes are not allowed to have children”的错误。

https://github.com/vllm-project/vllm/issues/3723
这是一个Bug报告，用户遇到了一个关于在使用Vllm过程中无法运行dbrx的问题。原因是当前环境下不支持DbrxForCausalLM模型架构，导致数值错误异常。

https://github.com/vllm-project/vllm/issues/3722
这是一个bug报告，涉及的主要对象是vllm在多节点多GPU环境下的CUDA错误。原因可能是设备序号无效导致的错误。

https://github.com/vllm-project/vllm/issues/3721
这是一个bug报告，主要涉及到vllm在WSL windows 10 CPU机器上安装失败的问题。这个bug可能是由于当前环境下PyTorch、CUDA等相关信息不能正确获取导致的。

https://github.com/vllm-project/vllm/issues/3720
这是一个bug报告类型的issue，涉及到vllm项目中的e2e模型测试的问题。由于重新开启了一些模型测试后发现多个e2e测试在当前环境下出现了错误，导致部分测试无法通过。

https://github.com/vllm-project/vllm/issues/3719
这是一个bug报告，涉及的主要对象是在尝试在离线环境中托管vllm时遇到了错误。

https://github.com/vllm-project/vllm/issues/3718
这是一个用户提出需求的issue，主要涉及的对象是为offline LLM课程提供聊天式指导/交流方法。该需求是为了利用和激活指导调整功能。

https://github.com/vllm-project/vllm/issues/3717
这是一个功能需求类型的issue，主要涉及到为没有提供默认聊天模板的模型分发一组默认聊天模板。造成此问题可能是原始模型的`tokenizer_config.json`中未提供与模型调优相关的聊天/指令模板。

https://github.com/vllm-project/vllm/issues/3716
这是一个类型为修正类型注解错误的issue，主要涉及的对象是代码中的类型注解。由于对应代码中类型注解存在小错误导致编译或运行时可能出现问题。

https://github.com/vllm-project/vllm/issues/3715
这是一个功能需求类的issue，涉及更新vLLM中Outlines集成的方式从FSM到Guide，用户希望新增加速/快进功能以提高模型性能，并可能辅助其他框架集成。

https://github.com/vllm-project/vllm/issues/3714
这是一个功能需求的issue，主要涉及的对象是将AICI集成到现有的guided decoding stack中。

https://github.com/vllm-project/vllm/issues/3713
这是一个需求提出的issue，主要涉及的对象是与lm-format-enforcer集成，并由于需要在运行模型服务时与多个不同schema结构接口的客户进行交互导致此需求。

https://github.com/vllm-project/vllm/issues/3712
这个issue属于升级版本，主要对象是软件的版本控制，问题是需要将软件版本升级到v0.4.0。

https://github.com/vllm-project/vllm/issues/3711
这是一个bug报告，主要涉及的对象是API的变更导致的兼容性问题。原因是API的修改导致了使用者的代码无法兼容。

https://github.com/vllm-project/vllm/issues/3710
这是一个Bug报告类型的Issue，主要涉及到无法在vllm docker容器中以2 4090运行的问题。这可能是由于环境配置问题引起的。

https://github.com/vllm-project/vllm/issues/3709
这是一个用户提出需求的issue，主要涉及的对象是模型Qwen2ForCausalLM。由于该模型不支持LoRA，但LoRA被启用，因此用户无法运行特定模型进行推理。

https://github.com/vllm-project/vllm/issues/3708
这个issue类型是版本回退，涉及主要对象是vllm项目的版本管理。原因是在实际发布之前回滚了版本更新。

https://github.com/vllm-project/vllm/issues/3707
这是一个bug报告，并涉及到vLLM 0.3.3 中的logprobs行为不符合OpenAI规范导致的问题。

https://github.com/vllm-project/vllm/issues/3706
This issue is a feature enhancement request, involving the introduction of SpeculativeConfig for speculative decoding in the LLMEngine. The user is seeking to streamline the engine configurations and potentially extend flags for future enhancements.

https://github.com/vllm-project/vllm/issues/3705
这是一个更新版本号的issue，不涉及具体的bug报告或需求提出。

https://github.com/vllm-project/vllm/issues/3704
这是一个开发需求类型的issue，主要涉及到在docker构建镜像中添加ccache，旨在通过CMake的改动减少CI构建时间。

https://github.com/vllm-project/vllm/issues/3703
这是一个bug报告，主要涉及的对象是代码中的参数设置。由于未显式设置`enable_prefix_caching=True`，导致前缀缓存未生效，可能出现性能问题。

https://github.com/vllm-project/vllm/issues/3702
这是一个bug报告，主要涉及了dist模块中通信器的销毁操作，由于可能出现已销毁的情况导致了需要添加一个检查来更好地清理。

https://github.com/vllm-project/vllm/issues/3701
这是一个bug报告，主要涉及的对象是block manager。原因可能是日志消息不清晰导致用户体验差。

https://github.com/vllm-project/vllm/issues/3700
这是一个用户提出需求的类型的issue，主要涉及的对象是MoE Triton kernel configs。这个问题的原因是AWS请求为A100 40GB GPUs添加MoE Triton kernel configs，以支持Mixtral和DBRX。

https://github.com/vllm-project/vllm/issues/3699
这个issue是一个Bug修复类型的问题，涉及的主要对象是ROCm软件中的rccl路径和注意力选择逻辑。导致这些bug的原因是之前的提交引入了几个错误，例如librccl.so文件名问题和之前的重构在注意力后端中引入的问题。

https://github.com/vllm-project/vllm/issues/3698
该issue属于bug报告类型，主要涉及的对象是项目中的构建过程。由于没有显式列出nvtools作为依赖项，导致构建时缺少nvtools而抛出错误。

https://github.com/vllm-project/vllm/issues/3697
这是一个Bug报告，主要对象是VLLM。由于在先前的预测后跟着VLLM的预测会导致VLLM hang住，用户在此报告了这个问题。

https://github.com/vllm-project/vllm/issues/3696
这是一个bug报告，主要涉及的对象是VLLM在预测过程中因内存耗尽导致不可预测的问题。

https://github.com/vllm-project/vllm/issues/3695
该issue是一个bug报告，涉及的主要对象是neuron_executor.py文件。由于缺少vision_language_config选项，导致markdown渲染无法正常工作。

https://github.com/vllm-project/vllm/issues/3694
这是一个bug报告，涉及的主要对象是vllm下的NeuronExecutor类，导致问题的原因是NeuronExecutor.__init__()方法在调用时传入的参数数量与定义的数量不匹配。

https://github.com/vllm-project/vllm/issues/3693
这是一个bug报告，主要涉及了benchmark格式在Buildkite中的报告问题，可能由于代码中的错误导致UI显示异常。

https://github.com/vllm-project/vllm/issues/3692
这是一个bug报告，主要涉及的对象是Kernel DBRX Triton MoE，可能由于代码执行问题导致了benchmark_latency.py运行结果异常。

https://github.com/vllm-project/vllm/issues/3691
这个issue是一个[Feature]类型的问题，主要涉及到为vLLM添加一个新的多GPU执行器。由于ROCm下使用ray时出现了TP>1的延迟问题，因此需要新增torchrun并行执行的多GPU执行器来解决这个问题。

https://github.com/vllm-project/vllm/issues/3690
这是一个关于用户提出需求的 issue，主要涉及支持新模型Jamba。这可能是因为Jamba是一个新型号，具有新颖的混合架构MoE Mamba模型，看起来非常有前景。

https://github.com/vllm-project/vllm/issues/3689
这是一个需求变更的issue，涉及的主要对象是GPTQ marlin quants加载兼容性，原因是由于Autogptq更新导致了需要支持新的`checkpoint_format`属性来存储`marlin`和未来的格式，以确保与最新Autogptq版本的兼容性。

https://github.com/vllm-project/vllm/issues/3688
这是一个Bug报告，涉及的主要对象是`async_llm_engine`，用户描述该引擎无法正常工作。

https://github.com/vllm-project/vllm/issues/3687
这是一个bug报告，主要涉及的对象是triton库中的Automatic Prefix Caching功能。这个bug是由于`triton==2.1.0`不支持Nvidia Turing架构，升级到`triton==2.2.0`可以解决这个问题。

https://github.com/vllm-project/vllm/issues/3686
这是一个关于GitHub上vllm的issue，属于PR Checklist类型，主要涉及支持多节点推理功能。由于Core模块移除了cupy依赖，现在可以顺利支持多节点推理。

https://github.com/vllm-project/vllm/issues/3685
这个issue是一个BugFix类型的问题，主要涉及tokenizer出现了out of vocab size导致的问题。

https://github.com/vllm-project/vllm/issues/3684
这是一个bug报告类型的issue，主要涉及对象为vllm下的asynchronous high concurrency测试。由于使用ThreadPoolExecutor创建线程池导致了Segmentation fault错误，用户在求解如何解决这一问题。

https://github.com/vllm-project/vllm/issues/3683
这是一个bug报告类型的issue，涉及VLLM引擎框架在推断时生成的第一个字符始终为空格，可能由于当前环境版本v0.3.3存在bug导致此问题。

https://github.com/vllm-project/vllm/issues/3682
这是一个bug报告类型的issue，主要涉及到vllm库的安装环境问题，可能是由于版本不兼容导致的undefined symbol错误。

https://github.com/vllm-project/vllm/issues/3681
这个issue是用户请教问题类型的，主要涉及VLLM模型的paged attention部分的测试代码缺失，用户寻求了帮助以理解该模型的实现和功能。

https://github.com/vllm-project/vllm/issues/3680
这个issue类型是用户提出需求，主要对象是vllm代码库。由于未启用类型检查，用户希望讨论如何启用mypy类型检查。

https://github.com/vllm-project/vllm/issues/3679
这是一个提出需求的类型，主要涉及的对象是Triton MoE kernel configs。由于A100 GPUs上缺少适用于DBRX的最佳配置，导致用户提交此需求。

https://github.com/vllm-project/vllm/issues/3678
这是一个提议新增测试的issue，涉及测试layer正确性和滑动窗口功能的验证。

https://github.com/vllm-project/vllm/issues/3677
这是一个关于GitHub项目中新增MistralModel和Embedding API的issue，涉及内容为bugs修复和新功能添加。

https://github.com/vllm-project/vllm/issues/3676
这是一个Bug报告issue，涉及主要对象是CohereForAI/c4ai-command-r-v01模型。由于max_model_len设定值大于模型配置文件中的值，导致数值设置失败并引发错误。

https://github.com/vllm-project/vllm/issues/3675
这个issue是一则关于bug报告，主要涉及的对象是项目vllm的构建系统。由于默认的nvcc线程数量和任务数设置过高导致系统负载过重，可能导致系统崩溃。

https://github.com/vllm-project/vllm/issues/3674
这是一个需求报告的issue，主要涉及使用Skypilot在A100机器上运行模型测试，因为无法在GCP中获取足够的资源。

https://github.com/vllm-project/vllm/issues/3673
这个issue类型是安装问题，主要对象是vllm的安装过程。由于缺少完整的cuda toolkit安装，导致了LIBNVTOOLSEXT未设置的错误。

https://github.com/vllm-project/vllm/issues/3672
这个issue是一个BugFix，主要涉及的对象是VLLM模型的处理停止字符串和停止token id的功能。原因是停止字符串在与token边界对齐时出现问题，多个token组成的停止字符串未正确排除输出，以及在特定条件下截断输出的bug。

https://github.com/vllm-project/vllm/issues/3671
这个issue类型是bug报告，涉及的主要对象是`commandr`。由于 `kv_cache` 类型错误和部分代码冗余引起了bug。

https://github.com/vllm-project/vllm/issues/3670
这是一个关于代码优化的issue，主要涉及detokenization逻辑的简化。

https://github.com/vllm-project/vllm/issues/3669
这是一个用户提出需求的issue，主要对象是关于VLLM模型。用户希望在文档中添加支持Command-R的相关信息。

https://github.com/vllm-project/vllm/issues/3668
这是一个关于安装问题的bug报告，涉及的主要对象是vllm。这个问题可能是由于使用Python 3.12时在安装vllm过程中出现了关于PyTorch版本的setuptools子进程错误导致的。

https://github.com/vllm-project/vllm/issues/3667
这个issue类型是需求提出，主要涉及Prefix caching在BlockManagerV2中的支持缺失，造成需要实现eviction policies、tracking computed bit、E2E correctness test等功能。

https://github.com/vllm-project/vllm/issues/3666
该issue类型是功能需求，涉及的主要对象是BlockManagerV2，用户提出了关于实现CPU/GPU交换的功能需求。

https://github.com/vllm-project/vllm/issues/3665
这是一个特性需求的issue，主要涉及到V2版本的block manager中实现滑动窗口支持，由于目前尚未提供滑动窗口功能，需要添加 `SlidingWindowBlockTable` 来支持这一特性。

https://github.com/vllm-project/vllm/issues/3664
这是一个功能增强类的issue，主要涉及的对象是在Rocm上为向量查询添加自定义内核。根据描述，用户提出了需求并提交了相关的代码修改，旨在通过添加自定义内核来提高Linear Layer中向量输入的性能。

https://github.com/vllm-project/vllm/issues/3663
这是一个bug报告，涉及到无法访问/v1端点的问题。原因可能是服务器无法找到v1端点，导致返回404错误。

https://github.com/vllm-project/vllm/issues/3662
这是一个关于代码性能优化的Kernel issue，主要涉及Layernorm性能问题。原因可能是当前实现的性能不足，需要进行优化。

https://github.com/vllm-project/vllm/issues/3661
这是一个bug报告，主要涉及的对象是nccl库命名规范。原因是PyTorch安装的nccl库命名不符合系统惯例，导致pure PyTorch用户使用不便。

https://github.com/vllm-project/vllm/issues/3660
这个issue是添加支持DBRX模型，不是bug报告。

https://github.com/vllm-project/vllm/issues/3659
这个issue属于需求开发类型，主要对象是对`DBRX`的支持。

https://github.com/vllm-project/vllm/issues/3658
这是一个用户提出需求的issue，主要涉及支持Databricks发布的DBRX模型，可能需要定制化实现。

https://github.com/vllm-project/vllm/issues/3657
这是一个Bug报告，主要涉及ChatCompletion的logprobs功能无法正常工作。导致这个问题的可能原因是在使用API时没有正确获取到logprobs。

https://github.com/vllm-project/vllm/issues/3656
这是一个bug报告，主要涉及的对象是vllm库的使用。由于域名解析失败导致出现了"socket.gaierror: [Errno -3] Temporary failure in name resolution"的错误。

https://github.com/vllm-project/vllm/issues/3655
这是一个关于如何在vLLM中使用`Tensor`输入的问题，主要涉及如何与使用自定义分词器的模型集成，用户想知道当前版本的vLLM是否支持这种方式并寻求帮助。

https://github.com/vllm-project/vllm/issues/3654
这是一则关于功能需求的issue，主要涉及支持CPU的执行器以及相关特性的支持。

https://github.com/vllm-project/vllm/issues/3653
这个issue类型是bug报告，主要涉及的对象是Gemma模型实现。由于vLLM的RMS norm计算使用了BF16而不是正确的FP32，导致了这个bug。

https://github.com/vllm-project/vllm/issues/3652
这个issue类型是代码修正，主要涉及的对象是KVCache类型，由于CC([Core] Refactor Attention Take 2)的变更导致需要进行一些微小的修复。

https://github.com/vllm-project/vllm/issues/3651
这是一个 bug 报告，主要对象是 vllm 的 api_server.py 文件。由于在运行命令时使用了未识别的参数 --api-key，导致出现错误 unrecognized arguments: --api-key。

https://github.com/vllm-project/vllm/issues/3650
这是一个用户提出需求的Issue，主要对象是希望在VLLM中指定每个请求生成的令牌数。由于用户需要在每个请求中分别指定生成的令牌数，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/3649
这个issue类型是documentation fixes and improvements，主要涉及的对象是为vLLM添加lora支持。由于文档标题和内容没有填写，导致需要完善文档描述。

https://github.com/vllm-project/vllm/issues/3648
这是一个特性添加的issue，主要涉及的对象是VLLM的内核，由于需要优化编译时间和改进缓存布局，引入了flashattention库中的一些特性。

https://github.com/vllm-project/vllm/issues/3647
该issue是关于功能需求的，主要涉及VLLM的LLM引擎，由于LLM引擎在初始化时必须调用_init_tokenizer方法，导致无法在不提供tokenizer的情况下初始化引擎，用户希望能支持在不需要tokenizer的情况下初始化LLM引擎。

https://github.com/vllm-project/vllm/issues/3646
这是一个Bug报告，涉及的主要对象是在WSL（Debian, Windows 11）上无法在运行offline_inference.py例代码时输出结果。这可能是由于在使用mistralai/Mistral7BInstructv0.2时，以及需要使用dtype="half"和无法使用bfloat16导致的。

https://github.com/vllm-project/vllm/issues/3645
这是一个bug报告，涉及的主要对象是ipv4地址解析问题，由于在修复ipv6地址解析bug的过程中导致了ipv4地址解析问题的出错。

https://github.com/vllm-project/vllm/issues/3644
这是一个Bug报告，用户在尝试在H100 GPUs上以tp_size=2运行MPT7B时遇到了vllm初始化步骤卡住并显示socket超时错误的问题。

https://github.com/vllm-project/vllm/issues/3643
这个issue是一个[功能改进]类型的问题，主要涉及到针对ROCm的默认FA使用Triton Kernel的相关调整。这个改动是为了添加一个新的Triton FA后端并移除一些AMD版本中不支持的参数，以及允许在多个后端中选择而不仅仅是两个后端之间的选择。

https://github.com/vllm-project/vllm/issues/3642
这个issue是一个功能需求，主要涉及到在集成gptq_marlin时支持act_order和所有group_sizes。由于新增功能需要验证多GPU模式是否正常工作，所以用户可能需要帮助验证多GPU模式。

https://github.com/vllm-project/vllm/issues/3641
这是一个bug报告，涉及的主要对象是`get_distributed_init_method`函数，由于IPv6环境下返回的URL格式导致无法正确解析端口号。

https://github.com/vllm-project/vllm/issues/3640
这是一个bug报告，主要涉及的对象是VLLM模块，原因是VLLM版本0.3.3无法启动，而版本0.3.2可以正常工作。

https://github.com/vllm-project/vllm/issues/3639
这是一个bug报告，主要涉及的对象是vllm库。由于未初始化tensor model parallel group导致报错。

https://github.com/vllm-project/vllm/issues/3638
这是一个需求类型的issue，主要涉及CI系统中添加测试用例来运行示例脚本。这个问题由于缺乏在CI中测试示例代码而导致的。

https://github.com/vllm-project/vllm/issues/3637
这是一个bug报告issue，涉及主要对象是vllm。原因可能是由于safetensor包出现了HeaderTooLarge错误，用户寻求关于该错误的帮助。

https://github.com/vllm-project/vllm/issues/3636
这是一个Bug报告，主要涉及vLLM的Kernel，由于执行punica kernel在非零GPU上导致了`RuntimeError: CUDA error: an illegal memory access was encountered.`错误。

https://github.com/vllm-project/vllm/issues/3635
这是一个关于用户提出需求的issue，主要涉及到改进vllm在生成文本时跳过detokenization操作的功能。由于当前环境下CPU利用率较高，GPU利用率较低，用户希望优化算法以提高性能。

https://github.com/vllm-project/vllm/issues/3634
这个issue是关于新增CPU推断后端的需求，涉及到vLLM项目的硬件和Intel处理器。由于需要扩展vLLM的支持到CPU推断，并且添加了新的功能但还未完全实现，因此提出了是否符合标准的RFC以及相关的检查清单。

https://github.com/vllm-project/vllm/issues/3633
这是一个bug报告，涉及vllm在本地运行时出现了"socket.gaierror"错误，由于尝试连接到"dns.google"导致的。

https://github.com/vllm-project/vllm/issues/3632
这是一个Bug报告，主要涉及 VLLM 模型在生成输出文本时logprobs与实际文本不对应的问题。原因可能是logprobs计算错误或模型解码逻辑问题。

https://github.com/vllm-project/vllm/issues/3631
这个issue是关于bug报告，主要涉及vllm项目中的模型测试。由于使用了forked导致一些测试失败，这个问题导致了一些GPU清理和初始化方面的异常行为。

https://github.com/vllm-project/vllm/issues/3630
这个issue是一个bug报告，涉及的主要对象是vllm库。由于未定义的符号错误，导致了vllm库的导入失败。

https://github.com/vllm-project/vllm/issues/3629
这是一个bug报告，涉及到当加载gemma 7B后遇到了一个错误，TypeError: flash_attn_varlen_func() got an unexpected keyword argument 'alibi_slopes'。可能是由于参数传递错误导致的问题。

https://github.com/vllm-project/vllm/issues/3628
这是一个关于Benchmark的issue，主要涉及DeepSpeed后端的更改以支持持久部署和张量并行处理，原因是原始代码使用了非持久的pipeline部署，不支持张量并行处理，导致在tp > 1时会生成错误。

https://github.com/vllm-project/vllm/issues/3627
这是一个bug报告，主要涉及的对象是vllm项目中的模块。由于环境配置问题导致出现了无法获取属性的错误。

https://github.com/vllm-project/vllm/issues/3626
这是一个PR（Pull Request）类型的issue，涉及的主要对象是添加非阻塞采样器与Triton内核。由于markdown渲染问题，原文中提到使用了原始html标签。

https://github.com/vllm-project/vllm/issues/3625
这是一个修改代码的issue，主要对象是vllm项目的核心功能。原因是需要移除cupy依赖，可能是为了降低项目对外部库的依赖性。

https://github.com/vllm-project/vllm/issues/3624
这个issue属于用户提出需求类型，主要对象是如何安全地将经过训练的LLM模型交付给客户以进行演示。由于模型敏感且可能被滥用，需要加密确保数据安全。

https://github.com/vllm-project/vllm/issues/3623
这个issue是一个bug报告，涉及到vLLM中的Sampler模块，主要是关于修改了CPUGPU通信的_get_ranks函数，但在section部分markdown渲染无法正常工作，因此使用了原始html代码。

https://github.com/vllm-project/vllm/issues/3622
这是一个bug报告，涉及的主要对象是isort合并顺序的问题，导致主分支破坏了lint。

https://github.com/vllm-project/vllm/issues/3621
该issue类型为Misc，涉及的主要对象是Latency/throughput benchmarks。由于之前的latency和throughput benchmarks不支持下载目录选项，导致用户无法指定模型下载目录，因此提出了此问题。

https://github.com/vllm-project/vllm/issues/3620
这是一个关于新增功能提案的issue，主要对象是为支持Cloud TPUs而对 vLLM 进行的工作。

https://github.com/vllm-project/vllm/issues/3619
这是一个关于需求提议的issue，主要涉及到vllm项目中持续集成环境的优化。

https://github.com/vllm-project/vllm/issues/3618
这是一个关于持续集成中测试范例的问题，涉及的主要对象是vllm项目中的示例脚本，原因是当前目录中的示例脚本没有在持续集成中进行测试。

https://github.com/vllm-project/vllm/issues/3617
这个issue是一个bug报告，主要涉及VLLM库在使用`enforce_eager=False`时出现`ValueError: bytes must be in range(0, 256)`的问题。

https://github.com/vllm-project/vllm/issues/3616
这个issue类型是用户提出需求，主要涉及增强系统的可观测性，用户希望能够获得更多的操作指标以监控系统状态。由于用户需要对生产监控获得更多运行指标，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/3615
这是一个向Github上的vLLM项目提交的关于Misc类的issue，主要对象是Latency/Throughput benchmarks。由于之前的Latency和Throughput benchmarks不支持`downloaddir`选项，用户无法指定模型下载目录，因此这个issue提出了给benchmark脚本添加该选项的需求。

https://github.com/vllm-project/vllm/issues/3614
这是一个功能需求提出的issue，主要涉及的对象是针对EETQ量化方法的实现。由于无法在线量化，用户请求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/3613
这是一个关于优化代码性能的issue，主要涉及benchmark_throughput.py中sample_requests函数执行时间长的问题，可能导致吞吐量差异。

https://github.com/vllm-project/vllm/issues/3612
这是一个bug报告，主要涉及FastAPI和uvicorn与多GPU使用存在集成问题，导致了TypeError错误。

https://github.com/vllm-project/vllm/issues/3611
这是一个Bug报告，涉及的主要对象是Mixtral-8x7B-Instruct-v0.1，导致这个Bug的原因是数值解包不匹配导致的数值错误。

https://github.com/vllm-project/vllm/issues/3610
这是一个用户提出需求的issue，主要对象是为vllm添加对xverse models的支持。用户请求通过pip安装支持xverse models的最新版本。

https://github.com/vllm-project/vllm/issues/3609
这是一个关于提交PR的类型不正确的issue，涉及到vLLM项目的代码质量和整体代码审查流程。原因可能是提交者未按照正确格式填写PR标题。

https://github.com/vllm-project/vllm/issues/3608
这个issue是关于bugfix的，涉及对象是自动前缀参数以及日志信息，这个bug是由于未将参数传递给`cache_config`而引起的。

https://github.com/vllm-project/vllm/issues/3607
这是一个Bug报告类型的Issue，主要涉及的对象是vllm中使用的模型，由于使用vllm openai API输出的对话存在多个换行符，可能是由于代码设置不当或API返回格式不正确导致的问题。

https://github.com/vllm-project/vllm/issues/3606
该issue类型是关于提出需求，主要对象是关于LLM Serving的高吞吐GPU-Efficient方案，问题是由GPU效率低下在顺序生成令牌时导致成本高昂的瓶颈，以及KVCache占用过多内存无法同时容纳更多序列所导致的挑战。

https://github.com/vllm-project/vllm/issues/3604
这是一个BugFix类型的issue，涉及的主要对象是`rotary_embedding.py`文件中的一个方法命名问题。导致了这个bug是因为原方法`get_device()`返回的是device id而不是`torch.device`。

https://github.com/vllm-project/vllm/issues/3603
这是一个bug报告，主要涉及的对象是代码中的字典parent_child_dict和sample对象。导致这个错误的原因可能是sample.parent_seq_id中的键值4对应的值在parent_child_dict字典中不存在。

https://github.com/vllm-project/vllm/issues/3602
这是一个关于代码错误的bug报告，主要涉及批次大小对齐的功能。由于批次大小对齐逻辑错误导致测试不稳定。

https://github.com/vllm-project/vllm/issues/3601
这个issue类型是bug报告，涉及的主要对象是无法成功构建CUDA镜像。导致该bug的原因可能是卡在构建过程中，导致长时间无法完成。

https://github.com/vllm-project/vllm/issues/3600
这是一个bug报告类型的issue，涉及主要对象是CI/Build配置。由于未尊重`MAX_JOBS`环境变量导致默认配置在要求编译cuda内核的项目中引起崩溃。

https://github.com/vllm-project/vllm/issues/3599
这是一个Bug修复类型的Issue，涉及到软件中使用`SoftLockFile`替代`LockFile`引发的向后兼容性问题。

https://github.com/vllm-project/vllm/issues/3598
这是一个Bug报告，主要涉及VLLM项目的Empty生成输出问题，可能由于7bmistral模型的某些原因导致。

https://github.com/vllm-project/vllm/issues/3597
这是一个bug报告，涉及主要对象是MoE模型。由于之前的代码更改导致MoE模型被破坏，需要修复。

https://github.com/vllm-project/vllm/issues/3596
这是一个bug报告，涉及到vllm中cache_ops.reshape_and_cache API无法正确复制数据的问题。这可能是由于代码实现上的错误导致的。

https://github.com/vllm-project/vllm/issues/3595
这个issue类型是bug报告，涉及的主要对象是代码中的一个函数调用。由于一个拼写错误导致函数调用出现问题，需要修复这个bug。

https://github.com/vllm-project/vllm/issues/3594
这是一个用户提出需求的issue，主要是关于使用vllm从特定层提取隐藏状态的问题。由于提取隐藏状态的方法速度较慢，用户希望找到更快的方法，而且想获得所有层的隐藏状态嵌入。

https://github.com/vllm-project/vllm/issues/3593
这个issue类型为bug报告, 主要涉及的对象是在API服务使用phi-2b时出现了模块未找到的错误。由于当前环境中缺少名为'transformers_modules'的模块，导致了该错误的症状。

https://github.com/vllm-project/vllm/issues/3592
这是一个更新请求，主要涉及的对象是`transformers`版本。由于`4.39.1`中修复了对LLaVA模型的一些破坏性更改，导致 https://github.com/vllmproject/vllm/pull/3042 无法通过。

https://github.com/vllm-project/vllm/issues/3591
这是一个bug报告，主要涉及BLOOM版权声明的修复。这个问题可能是由于BLOOM版权声明错误导致的。

https://github.com/vllm-project/vllm/issues/3590
这是一个bug报告，主要涉及的对象是vLLM中的Falcon模型，由于未正确使用tied embeddings导致某些Falcon模型出现了bug。

https://github.com/vllm-project/vllm/issues/3589
这个issue是关于使用问题，主要涉及在WSL ubuntu2204中运行vllm_worker程序导致系统退出，用户希望运行指定模型的推理操作，但不知道如何与vllm集成。

https://github.com/vllm-project/vllm/issues/3588
这是一个bug报告。用户在部署AquilaChat2-34B模型时遇到了针对文本质量和长度的问题。由于环境设置不当，导致模型生成的文本质量和长度不符合预期。

https://github.com/vllm-project/vllm/issues/3587
这是一个需求提出的RFC（请求提案变更）类型的issue，主要涉及分布式推断环境的接口和抽象。问题的根本原因是当前代码库中的分布式推断存在问题导致头绪混乱，并且出现了死锁和挂起的情况，尤其在升级到pytorch 2.2.0版本时问题更为突出。

https://github.com/vllm-project/vllm/issues/3586
这是一个bug报告，主要涉及对象是vllm中的`paged_attention_v2`函数。由于`num_head_kv=8`和`num_head=32`导致了部分输出不正确的症状。

https://github.com/vllm-project/vllm/issues/3585
这是一个bug报告，涉及到vllm中的`paged_attention_v2`函数，问题出现在参数设置上导致部分结果错误。

https://github.com/vllm-project/vllm/issues/3584
这是一个关于bug报告的issue，主要涉及工具使用中的错误提示信息问题。由于ActionInput中多了‘Observ’字符导致了tool调用出错。

https://github.com/vllm-project/vllm/issues/3583
这是一个用户提出需求的类型的issue，主要涉及支持RWKV模型，由于准备2T的检查点，提议使用线性注意力来实现更长的上下文和更快的推理。

https://github.com/vllm-project/vllm/issues/3582
这是一个Bug报告，主要涉及vllm下的模型启动错误，问题可能由于导入模块时的引用错误导致。

https://github.com/vllm-project/vllm/issues/3581
这是一个性能问题报告，主要涉及到vllm在AWQ上的性能问题，由于性能表现不佳而提出改进建议。

https://github.com/vllm-project/vllm/issues/3580
这个issue是一个用户提出的关于使用vLLM推理 AWQ基本模型的问题，用户想知道如何在不融合的情况下直接使用lora权重进行推理。

https://github.com/vllm-project/vllm/issues/3579
这个issue属于Bug报告类型，涉及主要对象为Falco models（40B和180B版本），由于Hugging Face Transformers库未包含`lm_head`权重导致vllm无法正确处理tiedembedding Falcon模型，从而导致无法正确推断，特别是对于像OpenBuddy/falcon40bv16.14k这样基于Falcon的微调模型。

https://github.com/vllm-project/vllm/issues/3578
这个issue属于bug报告，主要涉及的对象是`filelock.LockFile`和`filelock.SoftLockFile`。由于使用`LockFile`导致`.lock`文件不会被删除，可能导致其他用户引用未使用的`.lock`文件并引发错误。

https://github.com/vllm-project/vllm/issues/3577
这个issue是一个Bugfix类型的问题，涉及到停止词设置为 eos 时出现的错误，导致生成的句子被截断，降低了生成质量。

https://github.com/vllm-project/vllm/issues/3576
这是一个用户提出需求的问题，用户想了解在运行OPT模型推断时，停止生成新输出token的条件在哪里查找。

https://github.com/vllm-project/vllm/issues/3575
这是一个关于需求提议的issue，主要对象是扩展LLM上下文窗口至2048k tokens。原因是希望增强预训练语言模型的能力。

https://github.com/vllm-project/vllm/issues/3574
这是一个bug报告，涉及到vllm中的token输出。由于在streaming过程中没有正确处理潜在的stop sequences的prefix，导致了错误的token输出。

https://github.com/vllm-project/vllm/issues/3573
该问题类型为用户提出需求，主要涉及对象是新增功能，用户询问是否希望实现长上下文窗口功能。

https://github.com/vllm-project/vllm/issues/3572
这是一个Bug报告，涉及的主要对象是vllm项目下的llm_engine.py文件中的stop sequence检查功能。由于只使用`endswith`方法检查stop sequence，导致即使停止序列出现在输出文本中但不在结尾位置时仍然继续输出，用户建议改为检查文本中是否包含停止序列。

https://github.com/vllm-project/vllm/issues/3571
这个issue是关于CI/CD的改进，主要涉及vLLM项目中的neuron backend支持。原因可能是为了提高代码质量和审查效率。

https://github.com/vllm-project/vllm/issues/3570
这是一个关于在vLLM前端支持多个采样参数的需求提出的issue。

https://github.com/vllm-project/vllm/issues/3569
这是一个关于在vLLM中添加starcoder2 awq支持的问题。

https://github.com/vllm-project/vllm/issues/3568
这是一个Bug报告类型的issue，主要涉及对象是raylet程序。由于/tmp/ray/session_存储空间超过95%，可能导致对象创建失败，需要进行spilling操作。

https://github.com/vllm-project/vllm/issues/3567
这个issue类型为性能问题报告，主要涉及vllm的guided_json和nonguided_json性能对比，用户提出通过benchmark测试发现在100% GPU缓存利用率下的延迟和吞吐率表现不一致的问题。

https://github.com/vllm-project/vllm/issues/3565
这是一个bug报告类型的issue，关于从qwen2.py中移除临时参数的问题。8676478672867899

https://github.com/vllm-project/vllm/issues/3564
这个issue类型是bug报告，主要涉及的对象是vllm fastapi在GPU启用的kube集群中无法正常运行。由于环境中的GPU信息和驱动版本等配置可能不兼容或有缺失，导致无法启动成功。

https://github.com/vllm-project/vllm/issues/3563
该issue属于用户提出需求类型，主要涉及的对象是vLLM模型权重的CPU分担。由于AI模型规模快速扩张，需要更大内存容量的GPU，用户缺乏强大GPU资源，难以运行vLLM在多个模型上。因此用户希望通过cpu_offload_weight功能在vLLM中实现将模型权重分担到CPU，以解决这一挑战。

https://github.com/vllm-project/vllm/issues/3562
这是一个用户提出需求的issue，主要对象是在vllm上支持Phi-2与LoRA模型的情况。由于未收到任何响应，用户询问是否有困难支持所需的模型。

https://github.com/vllm-project/vllm/issues/3561
这是一个用户提出问题的issue，主要涉及VLLM下如何知道或配置并发请求（令牌数量）的问题，提问者关心不同参数的含义及配置对模型并发请求处理能力的影响。

https://github.com/vllm-project/vllm/issues/3560
这个issue类型是用户提出需求，主要涉及的对象是vllm，用户提出需要测试PyTorch Nightly和其他依赖库以获取早期信息。

https://github.com/vllm-project/vllm/issues/3559
这是一个bug报告，涉及对象是vllm模型。由于处理完所有提示后程序卡住，用户想知道处理和生成之间的关系导致的问题。

https://github.com/vllm-project/vllm/issues/3558
这个issue是一个[ROCm] [Hardware][AMD] Remove xformer patches and ray issue fix类型的bug报告，涉及的主要对象是ROCm路径，由于最新版本的破坏了ROCm的使用，需要固定版本2.9.3并做一些补丁修复光线问题。

https://github.com/vllm-project/vllm/issues/3557
这个issue是一个bug报告，涉及主要对象是使用`pytest --forked`时未能处理CUDA不是fork-safe导致的测试失败问题。

https://github.com/vllm-project/vllm/issues/3556
这是一个bug报告类型的issue，主要涉及vllm的性能分析操作，用户提出关于代码中特定函数和时间间隔的疑问。

https://github.com/vllm-project/vllm/issues/3555
这是一个用户提出需求的类型的issue，主要涉及VLLM加速框架是否兼容TensorRTLLM的问题。原因可能是用户希望了解两者是否可以一起使用。

https://github.com/vllm-project/vllm/issues/3554
这个issue是一个bug报告，主要涉及修复Qwen2的问题，由于markdown渲染不起作用，所以在此处使用原始的html。

https://github.com/vllm-project/vllm/issues/3553
这是一个Bug报告类型的Issue，主要涉及的对象是在Gemma加载之后进行量化或者LoRA操作时出现的问题。这个问题是因为在VLLM实现中，embed_token和lm_head被绑定在一起，导致在加载权重时可能会出现lm_head不存在的情况，从而触发错误。

https://github.com/vllm-project/vllm/issues/3552
这是一个bug报告，主要涉及到在启动服务器时未指定正确的后端导致错误出现。

https://github.com/vllm-project/vllm/issues/3551
这个issue属于升级需求，主要对象是更新了transformers版本到v4.39.0以及删除StarCoder2Config，原因是为了更好地升级transformers版本并移除不再需要的配置文件。

https://github.com/vllm-project/vllm/issues/3550
这是一个关于重构调度程序的issue，旨在为将来的分块预填充调度提供便利，主要涉及的对象是调度程序。由于需要针对不同的策略进行调度，导致需要重构调度程序以支持分块预填充。

https://github.com/vllm-project/vllm/issues/3549
这是一个用户提出需求的issue，主要关注的对象是关于动态内存压缩和LLM用于加速推理的特性。

https://github.com/vllm-project/vllm/issues/3548
这是一个CI/Build类型的issue，主要涉及的对象是更新transformers库以支持像Starcoder2这样的模型。由于需要更新库以支持新模型，所以需要进行相应的更改。

https://github.com/vllm-project/vllm/issues/3547
这个issue类型为文档改进， 主要涉及的对象是项目的示例脚本，用户提出了建议将示例脚本同步至文档仓库中使其更易发现和可搜索。

https://github.com/vllm-project/vllm/issues/3546
这是一个Bug报告，主要涉及的对象是模型架构['CohereForCausalLM']，由于该架构目前不受支持导致数值错误。

https://github.com/vllm-project/vllm/issues/3545
这是一个用户提出需求的issue， 主要对象是项目vllm。 由于项目需要每夜生成镜像和wheels，用户希望能够简单地构建并上传至Google容器仓库和存储桶。

https://github.com/vllm-project/vllm/issues/3544
这是一个bug报告，主要涉及VLLM下的inference输出不同的问题。造成这种症状的原因可能与批量大小不同导致模型输出结果不同有关。

https://github.com/vllm-project/vllm/issues/3543
这是一个bug报告，主要涉及模型Qwen2ForCausalLM在该环境下不支持LoRA，但文档中却标记支持LoRA，可能是代码或文档存在不一致导致的问题。

https://github.com/vllm-project/vllm/issues/3541
这是一个Bug报告类型的Issue，在构建simple_knn时出现了错误。

https://github.com/vllm-project/vllm/issues/3540
这个issue是关于Bug报告，主要涉及vLLM中使用punica LoRA内核无法导入的问题。由于修改源代码后重建安装，可能导致出现上述错误提示，用户寻求如何正确修改代码以使其正常工作。

https://github.com/vllm-project/vllm/issues/3539
这个issue是一个bug报告，涉及的主要对象是代码中的1D query查询功能。产生这个问题的原因是在将输入从2D改为1D查询时，代码可能与其他PR中重构的代码产生了冲突。

https://github.com/vllm-project/vllm/issues/3538
这是一个关于代码修改的issue，涉及主要对象为chunked prefill data update。由于需要统一机制到computed_num_blocks方法，导致出现了此issue。

https://github.com/vllm-project/vllm/issues/3537
这是一个BugFix类型的issue，主要涉及Neuron的构建脚本。由于markdown渲染不起作用，故在此使用原始html。

https://github.com/vllm-project/vllm/issues/3536
这是一个用户提出需求的issue，主要对象是增加对于Guided Decoding的支持。原因是用户希望在offline接口中与OpenAI推理服务器同样支持Guided Decoding功能。

https://github.com/vllm-project/vllm/issues/3534
这个issue是一个Bugfix类型的issue，涉及的主要对象是ROCm支持。由于之前的更改导致ROCm出现了问题，这个issue修复了剩下的问题，使得ROCm能够成功构建vLLM。

https://github.com/vllm-project/vllm/issues/3533
这是一个功能需求（Feature Request）issue，主要涉及的对象是vLLM模型加载使用CoreWeave的`tensorizer`功能。由于模型张量存储在容器映像中导致映像变得庞大，用户提出了使用`tensorizer`加载模型以减小容器映像大小和加载模型时间的需求。

https://github.com/vllm-project/vllm/issues/3532
这是一个关于“Feature”的issue，主要涉及Heavy-Hitter Oracle (H2O)在生成推理中的效率问题。原因是为了提高内存效率而引入H2O，希望通过动态平衡保留最近令牌和H2令牌来提高性能。

https://github.com/vllm-project/vllm/issues/3531
这个issue是关于一个新功能提议，涉及的主要对象是提高大型语言模型推理效率的技术。

https://github.com/vllm-project/vllm/issues/3530
这是一个功能增强的需求，主要对象是用于基准测试的请求发送程序。

https://github.com/vllm-project/vllm/issues/3529
该issue是一个bug报告，主要涉及的对象是vllm库。由于环境中vllm版本与其他库的不兼容性，导致出现了"AttributeError: 'Namespace' object has no attribute 'kv_cache_dtype'"的错误提示。

https://github.com/vllm-project/vllm/issues/3528
这是一个bug报告类型的issue，主要涉及的对象是vllm库。出现这个问题的原因可能是在安装vllm时出现了导入错误，导致出现了undefined symbol错误。

https://github.com/vllm-project/vllm/issues/3527
这是一个bug报告，主要对象是缺少nvcc命令在PATH环境变量中的路径，导致在构建VLLM时失败。

https://github.com/vllm-project/vllm/issues/3526
这是一个Bug报告，涉及主要对象为vllm软件包的pip安装过程。由于缺少NumPy模块导致的错误，无法成功获取构建wheel所需的依赖，进而引发了报错现象。

https://github.com/vllm-project/vllm/issues/3525
这是一个bug报告类型的issue，涉及对象为VLLM模型的多GPU推断。导致该问题的原因可能是torch.distributed模块的运行时错误。

https://github.com/vllm-project/vllm/issues/3524
该issue类型是需求提出，主要涉及到在vLLM中实现全张量并行化的LoRA层，由于当前的multiLoRA层只能使用张量并行化运行其中一个LoRA，在更高的序列长度、rank以及张量并行化尺寸下具有更好的性能，但在较小规模下性能类似。

https://github.com/vllm-project/vllm/issues/3523
这是一个Bug报告，主要涉及的对象是 `aisingapore/sea-lion-7b-instruct` 模型，由于 `config.embedding_fraction == 1.0` 的断言失败导致了问题。

https://github.com/vllm-project/vllm/issues/3522
该issue类型为用户提出需求，主要涉及对象是vLLM和KubeRay。由于vLLM在启动时急切检查可用的GPU资源并失败快速，导致无法利用KubeRay的自动缩放功能。

https://github.com/vllm-project/vllm/issues/3521
这是一个Bug报告类型的issue，主要涉及的对象是vllm代码中的IP暴露问题。这个问题可能是由于代码中的某些设置导致内部IP地址暴露，用户希望解决这个问题并将主机地址127.0.0.1显示为外部IP，而不是系统IP 192.168.100.17。

https://github.com/vllm-project/vllm/issues/3520
这是一个Bug报告，主要涉及的对象是vllm模型。由于长时间运行后，vllm速度变慢并且可能丢失一些数据，可能是由于长时间运行导致的性能问题。

https://github.com/vllm-project/vllm/issues/3519
这个issue属于用户提出需求类型，主要涉及到支持新模型CogVLM。由于需要修改vLLM引擎以支持视觉模块，导致了用户提出的问题。

https://github.com/vllm-project/vllm/issues/3518
这是一个用户提出需求的issue，用户希望能够在vllm中指定生成的token数量。

https://github.com/vllm-project/vllm/issues/3517
这是一个bug报告，主要涉及到vllm在使用ray的情况下缺少tokenizer所导致的错误。

https://github.com/vllm-project/vllm/issues/3516
这是一个功能需求类型的issue，主要涉及Token Rank在Logprobs对象中的添加以及与IBM的相关功能对齐。

https://github.com/vllm-project/vllm/issues/3515
这是一个需求类型的issue，主要涉及到在VLLM模型中添加attention sinks功能。用户提出了支持Attention Sink以解决模型关注到序列中前几个token导致生成无关输出的问题。

https://github.com/vllm-project/vllm/issues/3514
这是一个Bug报告类型的Issue，主要涉及对象是vllm。由于用户不知道如何将loratuned metallama/Llama27bchathf集成到vllm中，导致无法正确生成预期结果。

https://github.com/vllm-project/vllm/issues/3513
这是关于使用问题的bug报告，主要涉及VLLM在切换Ray根临时目录时出现PermissionDenied错误，原因可能是Docker文件中切换用户时对`/tmp`文件夹权限不足。

https://github.com/vllm-project/vllm/issues/3512
这是一个bug报告，主要涉及的对象是OpenAI服务器的前端代码，由于没有使用正确的tokenizers以及并行async tokenization，导致了功能失效。

https://github.com/vllm-project/vllm/issues/3511
这是一个功能需求类型的issue，主要涉及对象是`LRUCache`和`BaseTokenizerGroup`。由于缺乏泛型类型和非必需关键字参数反映在`BaseTokenizerGroup`抽象方法中，导致需要添加对泛型类型的支持来反映这些非必需参数。

https://github.com/vllm-project/vllm/issues/3510
这是一个bug报告，主要涉及的对象是vLLM与Hugging Face ZeroGPU Spaces之间的兼容性问题，由于vLLM在模型加载时直接分配工作线程到设备，破坏了ZeroGPU的GPU分配约定，导致了此问题。

https://github.com/vllm-project/vllm/issues/3509
这是一个关于安装问题的bug报告，涉及vllm在Neuron安装过程中出现错误。由于脚本在虚拟环境中失败，导致安装失败。

https://github.com/vllm-project/vllm/issues/3508
这是一个关于使用lru_cache进行一些环境检测工具的需求问题，针对Python 3.8兼容性问题。

https://github.com/vllm-project/vllm/issues/3507
这是一个关于bug报告的issue，主要涉及对象是vllm项目中“[Core] Cache some utils”功能，由于Python 3.8的兼容性问题导致了该功能出现bug。

https://github.com/vllm-project/vllm/issues/3506
这是一个文档修正的issue，主要涉及amd-installation.rst文件中拼写错误的修正。缺失的字母导致markdown渲染无法正常工作。

https://github.com/vllm-project/vllm/issues/3505
这是一个文档修复的issue，涉及的主要对象是 neuron-installation.rst。因为缺少 git clone 指令，导致在安装指南中的格式不匹配。

https://github.com/vllm-project/vllm/issues/3504
这个issue是一个bug报告，主要涉及到vllm模型在长长度下生成出现问题。出现这个问题的原因可能是在长输入下，vllm与transformers生成的结果不一致。

https://github.com/vllm-project/vllm/issues/3503
这个issue类型是环境配置报告，主要涉及VLLM的数据类型设置。由于环境中PyTorch版本为2.2.1+cu121，用户希望通过YAML文件设置VLLM的数据类型。

https://github.com/vllm-project/vllm/issues/3502
这是一个提出拉取请求的issue，涉及集成FP8 attention kernel。由于markdown渲染不起作用，需要使用原始html进行描述。

https://github.com/vllm-project/vllm/issues/3501
这是一个用户提出需求的 issue，主要对象是为了实现动态下载 LoRA 适配器，目前需要先将适配器下载到服务器上。

https://github.com/vllm-project/vllm/issues/3500
这是一个Bug修复的Issue，主要涉及的对象是vLLM模型的tokenizer和vocabulary size不相等导致的问题。

https://github.com/vllm-project/vllm/issues/3499
这是一个用户提出需求的issue，主要涉及了通过LoRA对vllm进行扩展的功能。用户想要实现在初始化时附加LoRA，而不是在每个请求中都进行附加的操作。

https://github.com/vllm-project/vllm/issues/3498
这是一个bug报告，主要涉及 vllm 下的 beam search 设置，在输出中出现异常。原因可能是与 beam search 相关的代码逻辑或参数设置问题导致输出异常。

https://github.com/vllm-project/vllm/issues/3497
这是一个bug报告，主要涉及到代码中的值错误导致了无法完成填充操作，进而导致数值错误。

https://github.com/vllm-project/vllm/issues/3496
这个issue是一个bug报告类型的问题，涉及的主要对象是vLLM中的Dynamic Multi LoRA Load和Delete功能。原因可能是由于markdown渲染问题，导致描述内容未正确显示。

https://github.com/vllm-project/vllm/issues/3495
这个issue类型是一个[CI/Build]类型的issue，主要涉及到代码格式化工具isort的引入。由于缺乏明确同意引入此功能的确认，导致了该issue的提出。

https://github.com/vllm-project/vllm/issues/3494
这是一个bug报告，主要涉及VLLM在使用过程中导致内存占用过高的问题。

https://github.com/vllm-project/vllm/issues/3493
这是一个bug报告，主要关于在使用vllm.generate函数时，无法正确解码自定义的额外token所导致的问题。

https://github.com/vllm-project/vllm/issues/3492
这个issue类型是Bug修复，主要涉及的对象是代码库中的block manager子系统。由于缺少一些功能特性，如换入/换出实现、滑动窗口支持等，导致BlockManagerV2无法成为默认选项。

https://github.com/vllm-project/vllm/issues/3491
这是一个技术文档方面的问题，用户询问关于PIPELINE_PARALLEL_SIZE的使用方法和对推理性能以及GPU内存使用的影响。

https://github.com/vllm-project/vllm/issues/3490
这是一个用户提出需求的issue，主要涉及于vLLM模型的功能增强，请求计算并记录serving FLOPs，旨在帮助调试性能并检查GPU利用率。

https://github.com/vllm-project/vllm/issues/3489
这是一个用户提出需求的类型的issue，主要涉及对象是vllm的GPU内存使用控制，用户想了解为什么增加maxnumseqs会减少内存使用。

https://github.com/vllm-project/vllm/issues/3488
这是一个bug报告，涉及到vLLM中DynamicNTKScalingRotaryEmbedding的实现与Transformer实现不同所导致的性能下降问题。

https://github.com/vllm-project/vllm/issues/3487
这是一个Bug报告，主要涉及的对象是CUDA的初始化。这个问题是由于在CC(Support inference with transformersneuronx)中不慎引入了在设置`CUDA_VISIBLE_DEVICES`之前初始化CUDA的情况而导致的。

https://github.com/vllm-project/vllm/issues/3486
这是一个Bug报告，涉及的主要对象是在启动openai_api_server时出现错误。原因是调用模型`Qwen1.514BChatGPTQInt8`时报错提示该模型不存在。

https://github.com/vllm-project/vllm/issues/3485
这个issue是关于bug报告，主要涉及vllm中的模型`Qwen1.514BChatGPTQInt8`不存在而导致api请求返回错误。

https://github.com/vllm-project/vllm/issues/3484
这是一个关于性能问题的报告，主要对象是vllm中的tensor parallelism功能。由于某种原因，导致使用该功能时并没有产生预期的效果，具体表现为性能没有提升。

https://github.com/vllm-project/vllm/issues/3483
这是一个bug报告类型的issue，涉及到vLLM中的markdown渲染问题。由于markdown渲染不起作用，所以在内容中使用了原始的html。

https://github.com/vllm-project/vllm/issues/3482
这是一个bug报告，主要涉及使用VLLM在L4机器上运行时出现的"Expected all tensors to be on the same device"异常问题。

https://github.com/vllm-project/vllm/issues/3481
这是一个bug报告，涉及对象为vllm下的compute_num_jobs函数。这个bug是由于没有在尝试确定nvcc线程数量之前检查_is_cuda()而导致的，因此在没有CUDA/nvcc安装的机器上没有定义CUDA_HOME时会出现错误。

https://github.com/vllm-project/vllm/issues/3480
这个issue是关于活动通知的，不是bug报告或需求。

https://github.com/vllm-project/vllm/issues/3479
这个issue类型是文档更新，涉及更新vLLM第三次Meetup的README。由于markdown渲染不起作用，导致需要使用HTML来更新内容。

https://github.com/vllm-project/vllm/issues/3478
该issue类型为文档修复，对于vLLM的PR模板进行了修复，主要对象是修复PR模板中填写问题导致的Github通知看起来不清晰和不直观。

https://github.com/vllm-project/vllm/issues/3477
这是一个bug报告，主要涉及的对象是hipFuncSetAttribute API，由于缺少头文件，导致编译错误。

https://github.com/vllm-project/vllm/issues/3476
这是一个功能需求问题，涉及到在vllm中添加使用`tensorizer`进行模型加载的功能。

https://github.com/vllm-project/vllm/issues/3475
这个issue是一个文档问题，涉及GitHub上vLLM项目中关于PR Checklist的markdown渲染错误。

https://github.com/vllm-project/vllm/issues/3474
这是一个关于代码质量标准的Issue，主要涉及了代码质量标准的未满足。

https://github.com/vllm-project/vllm/issues/3473
该issue是一个[CI/Build]类型的问题，主要涉及到CI（Continuous Integration）部分的一个坏的导入修复。由于markdown渲染不起作用，所以在此处使用了原始的HTML。

https://github.com/vllm-project/vllm/issues/3472
这个issue是用户提出需求类型的，主要涉及的对象是对vLLM支持xai-org/grok-1模型的请求。由于模型代码基于JAX且需要int8量化，用户希望有人将其移植到PyTorch以便与vLLM集成并进行MOE架构的额外优化。

https://github.com/vllm-project/vllm/issues/3471
该issue属于软件开发类型，主要涉及硬件Neuron的重构，移除了不必要的neuron模型定义，对Neuron支持进行了完全隔离，旨在简化开发和测试。

https://github.com/vllm-project/vllm/issues/3470
这是一个bug报告，涉及主要对象为moe_align_block_size函数。由于新的变更导致moe_align_block_size_kernel函数不兼容AMD GPU，产生了bug。

https://github.com/vllm-project/vllm/issues/3469
这个issue是关于代码质量的问题，主要对象是vLLM代码库中markdown渲染问题，可能是由于markdown渲染不起作用所致。

https://github.com/vllm-project/vllm/issues/3468
这是一个用户提出需求的issue，主要对象是关于Microsoft提出的DeepSpeed-FP6优化方法，询问vLLM团队是否研究过该方法，可能是因为想了解该方法是否对模型速度和精度有所优化。

https://github.com/vllm-project/vllm/issues/3467
这个issue是一个bug报告，主要涉及的对象是vLLM OpenAI server，由于无法支持复杂消息内容导致此问题。

https://github.com/vllm-project/vllm/issues/3466
这个issue属于功能需求类型，主要对象是针对vllm项目中的单节点多GPU部署。由于对ray依赖的需求不符合实时同步推理的特定要求，并且希望在非ray集群环境下使用“轻量级”选项来提高生产安全合规性，因此提出了对ray可选性的改进。

https://github.com/vllm-project/vllm/issues/3465
这个issue是关于移除"--forked"选项的工作进行中（Work in Progress）。

https://github.com/vllm-project/vllm/issues/3464
这是一个关于如何增加vLLM端点请求处理数量的问题。主要涉及vLLM服务在处理组请求时性能问题，表现为请求处理数量限制。

https://github.com/vllm-project/vllm/issues/3463
这是一个bug报告，主要涉及的对象是VLLM（Very Large Language Model）。由于缺少 megablocks 和 STK 库的安装，导致出现了数个错误提示及数个异常，最终引发了该数个异常和错误的bug。

https://github.com/vllm-project/vllm/issues/3462
这个issue是一个功能改进的提议，主要涉及Attention模块的重构。原因是为了隐藏特定于后端的Attention实现细节，以便更方便地引入新的后端。

https://github.com/vllm-project/vllm/issues/3461
该issue属于代码改进类型，涉及到vLLM项目中的cache_stream和cache_events对象。这个问题主要由于vLLM引入CUDA graphs后使得原本的finegrained overlapping功能被禁用，导致这两个对象成为了无用的对象。

https://github.com/vllm-project/vllm/issues/3460
这是一个bug报告，涉及到LoRA模型无法支持Deepseek-coder-7b-instruct模型，原因是vocab size的限制。

https://github.com/vllm-project/vllm/issues/3459
这个issue是关于Bug报告，涉及到vLLM中的markdown渲染问题，导致无法正确显示内容。

https://github.com/vllm-project/vllm/issues/3458
这是一个用户提出的需求类型的issue，主要对象是Popular chinese LLMs baichuan/baichuan2/qwen/chatlgm， 用户希望提供对这些LLMs的支持。

https://github.com/vllm-project/vllm/issues/3457
这是一个Bug报告类型的Issue，主要涉及的对象是vllm库中的tensor_parallel_size参数设置为2时，worker_use_ray自动设置为true的情况。导致这个问题的原因可能是Ray初始化时出现了停止的情况。

https://github.com/vllm-project/vllm/issues/3456
这是一个文档相关的issue，涉及到markdown渲染无法显示，因此需要使用原始的html代码。

https://github.com/vllm-project/vllm/issues/3455
这是一个Bug报告，主要涉及的对象是分布式推理中的Ray工作器引发异常导致的死锁问题。由于异常在初始化过程中被抛出，导致主进程和工作进程无法互相等待对方的信号而导致死锁。

https://github.com/vllm-project/vllm/issues/3454
这个issue是一个bug报告，主要涉及到beam search logits processor，由于相关bug导致了问题已经在另一个issue中被描述。

https://github.com/vllm-project/vllm/issues/3453
这是一个关于使用`dataclass`的问题，主要涉及markdown渲染问题。原因是markdown渲染出现问题，所以在这里使用原始的html。

https://github.com/vllm-project/vllm/issues/3452
这是一个特性需求类型的issue，主要涉及的对象是 `InputMetadata` 类。这个需求是为了让模型运行器中的代码变得更清晰。

https://github.com/vllm-project/vllm/issues/3451
这是一个功能需求类型的issue，主要涉及添加对控制向量的支持，相关链接可参考提供的github页面。

https://github.com/vllm-project/vllm/issues/3450
这是一个bug报告，主要涉及API stream返回两个停止的问题。问题可能由于代码逻辑错误导致API调用返回多余的结果。

https://github.com/vllm-project/vllm/issues/3449
这是一个功能改进的issue，主要涉及的对象是异步tokenization的线程池支持，由于需要使用线程池代替ray来处理tokenization，且考虑HF tokenizers的线程安全性问题，每个线程都会使用单独的tokenizer实例。

https://github.com/vllm-project/vllm/issues/3448
这是一个bug报告，涉及的主要对象是Guided Generation Logits Processor，由于代码中的_logits_processors处理不当导致了cache共享错误的bug。

https://github.com/vllm-project/vllm/issues/3447
这是一个bug报告，涉及主要对象为vllm模型的Qwen7BChat，报告了TypeError: endswith first arg must be str or a tuple of str, not bytes这一错误，可能是由于stop_string "\bObserv"导致的。

https://github.com/vllm-project/vllm/issues/3446
这是一个功能需求提交，主要涉及前端支持新的lora模块添加到OpenAI Entrypoints的问题。由于先前版本的OpenAI entrypoints不支持将lora适配器添加到实时服务器，因此用户需要更新版本以解决该问题。

https://github.com/vllm-project/vllm/issues/3445
该issue是一个CI/Build类型的问题，主要涉及到vLLM中关于Shard tests for LoRA and Kernels的速度问题。由于markdown解析不起作用，导致必须使用原始html，希望优化代码质量并提高审查效率。

https://github.com/vllm-project/vllm/issues/3444
这个issue类型为文档问题，涉及到代码风格问题和markdown渲染的不兼容。

https://github.com/vllm-project/vllm/issues/3443
这是一个bug报告，主要涉及OpenAI API实现不一致和未测试边缘情况，导致一些小问题和易破坏的边缘情况。

https://github.com/vllm-project/vllm/issues/3442
这个issue类型是Bug报告，涉及的主要对象是vLLM的代码库。由于升级到pytorch 2.2时移除cupy依赖和避免nccl 2.19 bug，导致了markdown渲染无法正常工作，需要使用原始的HTML代码来解决。

https://github.com/vllm-project/vllm/issues/3441
这是一个bug报告，涉及主要对象是OpenAI代码中的echo/logprob功能，可能是由于流式处理`logprob`和`echo`组合而导致问题。

https://github.com/vllm-project/vllm/issues/3440
这是一个bug报告，涉及到从OAI服务器中移除了一个多余的打印消息导致的问题。

https://github.com/vllm-project/vllm/issues/3439
这是一个特性请求，主要涉及AQLM量化方法的介绍和性能分析。

https://github.com/vllm-project/vllm/issues/3438
这个issue类型是bug报告，主要涉及的对象是vLLM在非Linux平台上的安装问题。由于vLLM仅支持Linux系统，用户在Windows或MacOS平台上运行安装过程导致出现了"nvcc_cuda_version is not defined"错误，用户提出需要打印更清晰的错误消息以告知他们vLLM仅支持Linux。

https://github.com/vllm-project/vllm/issues/3437
这是一个与测试相关的issue，主要对象是测试配置文件 test_config.py。原因是在进行复审时忘记将测试添加到持续集成（CI），导致需要将 test_config.py 添加到持续集成中。

https://github.com/vllm-project/vllm/issues/3436
这是一个修复模板问题的Issue，涉及到Issue模板和标签的问题。问题的症状是由于模板中指定的标签不存在，导致无法正确应用标签到新创建的问题上。

https://github.com/vllm-project/vllm/issues/3435
这是一个关于bug报告的issue，主要涉及到使用Docker部署vllm模型时遇到的问题。由于模型的最大序列长度超过了KV缓存所能存储的最大标记数，导致出现数值错误。

https://github.com/vllm-project/vllm/issues/3433
这是一个用户提出需求类型的issue，主要涉及的对象是为vllm添加对Cohere的Command-R模型的支持。

https://github.com/vllm-project/vllm/issues/3432
该issue是一个Bug报告，主要涉及Mixtral8x7BInstruct with GPTQ 4bit quantization在temperature为0时输出非确定性的问题。

https://github.com/vllm-project/vllm/issues/3431
这是一个关于性能优化的issue，主要涉及到evictor的改进，由于使用`OrderedDict`替代`Dict`，提升了缓存处理速度。

https://github.com/vllm-project/vllm/issues/3430
这是一个bug报告，涉及的主要对象是mTLS支持相关的参数。由于之前的PR未添加这些参数，导致在`api_server.py`中缺少必要的参数，同时默认数值也未正确对齐，导致与uvicorn不一致。

https://github.com/vllm-project/vllm/issues/3429
该问题类型为功能需求，主要涉及更新Dockerfile以实现ModelScope支持，用户提出了是否需要修改文档或其他文件的疑问。

https://github.com/vllm-project/vllm/issues/3428
这是一个用户提出需求的issue，主要涉及vllm和Modelscope，用户希望添加Docker支持以运行vllm。

https://github.com/vllm-project/vllm/issues/3426
这是一个Bug报告，涉及到torch和triton之间版本不兼容的问题，导致用户在特定环境下出现错误信息。

https://github.com/vllm-project/vllm/issues/3425
这是一个关于安装问题的bug报告，涉及的主要对象是vllm在Windows 11上的安装。由于安装过程中遇到了问题，用户提出了安装失败的问题。

https://github.com/vllm-project/vllm/issues/3424
这是一个用户提出需求的issue，主要对象是vllm项目。用户想了解如何在pytorch/xla上运行vllm。

https://github.com/vllm-project/vllm/issues/3423
这个issue是一个用户提出需求的类型， 主要对象是vllm，用户询问是否有计划支持pytorch 2.2.0。

https://github.com/vllm-project/vllm/issues/3421
这个issue属于文档错误修复类型，主要涉及修改v_vec.png和value.png的内容以及解释"value"部分，用户提出帮助用户更好理解文档的需求。

https://github.com/vllm-project/vllm/issues/3420
这个issue类型是需求提出，涉及的主要对象是Falcon软件不同版本的聊天格式。

https://github.com/vllm-project/vllm/issues/3419
该问题为用户提出需求，主要涉及的对象是设置主机IP环境变量。由于主机有多个IP地址且用户需要指定使用哪一个IP地址，需要在分布式集群中让用户指定使用的IP地址。

https://github.com/vllm-project/vllm/issues/3418
这是一个需求提出类型的issue，主要涉及的对象是ChatGLM/ChatGLM2。

https://github.com/vllm-project/vllm/issues/3417
这个issue类型是用户提出需求，主要涉及的对象是vllm（版本0.3.3）是否支持 wizardcoder 和 deepseek 模型的 lora 推理，原因是项目需要使用这些模型的 loraadapters 进行推理。

https://github.com/vllm-project/vllm/issues/3416
这是一个Bug报告，涉及的主要对象是docker启动vllm容器时出现报错。由于GPU数量配置错误导致启动时报错"The number of required GPUs exceeds the total number of available GPUs in the cluster"。

https://github.com/vllm-project/vllm/issues/3415
这是一个bug报告，主要涉及对象是GitHub上vllm中的一个issue，因为在neuron上没有CUDA环境导致model_runner中的一次提交破坏了neuron的行为。

https://github.com/vllm-project/vllm/issues/3414
这是一个bug报告，涉及到Marlin配置表达问题，由于这个小问题导致了一些日志解析出错。

https://github.com/vllm-project/vllm/issues/3413
这是一个关于贡献者指南和PR模板的问题，涉及到改进项目的PR流程。

https://github.com/vllm-project/vllm/issues/3412
这个issue属于错误操作类型，涉及到git操作。原因是不正确的git操作导致了该问题。

https://github.com/vllm-project/vllm/issues/3411
这个issue属于功能改进类型，涉及主要对象为vLLM中的Gemma实现，原因是要改进实现方式以使用近似的GELU函数。

https://github.com/vllm-project/vllm/issues/3410
这个issue属于用户提出需求类型，主要涉及的对象是uvicorn，用户提出了在mTLS支持方面参数不足的问题。

https://github.com/vllm-project/vllm/issues/3409
该issue类型为功能需求提议，主要对象是LLM模型的entrypoint。由于现有设计导致用户在等待所有请求完成之后才能获得输出结果，用户提出了需要增加选项来实现每个引擎步骤输出结果的流式传输。

https://github.com/vllm-project/vllm/issues/3408
这是一个bug报告，涉及到修复 `dist.broadcast` 在没有group参数时导致的程序停滞问题。

https://github.com/vllm-project/vllm/issues/3407
这是一个bug报告，涉及的主要对象是34B AWQ模型在使用两个A2 GPU时出现异常，可能由于CUDA内存参数或eagersomething参数导致请求卡住造成CPU和GPU占用率异常高。

https://github.com/vllm-project/vllm/issues/3406
这是一个bug报告，涉及主要对象是在一台机器上同时运行多个Mixtral实例。问题出现的原因是两个Ray实例之间的冲突导致了hang和错误提示。

https://github.com/vllm-project/vllm/issues/3405
这个issue类型为bug报告，主要涉及的对象是EvictionPolicy.LRU模式下的快速删除块操作。这个问题是由于回收PhysicalTokenBlocks时，其metainfo未被及时处理，导致在LRU模式下需要快速删除块时，通过整个free_table循环查找而非直接从SortedList中删除。

https://github.com/vllm-project/vllm/issues/3404
这个issue属于Bug报告类型，主要涉及到LoRA fine-tuned LLM模型加载时出现的AssertionError问题。问题可能源自于模型导出过程中包含不符合vLLM代码预期的张量，导致加载模型时崩溃。

https://github.com/vllm-project/vllm/issues/3403
这是一个用户提出需求的类型为Feature请求的issue，主要涉及的对象是VLLM。由于用户需要支持Cohere Command-R模型在VLLM中的使用，请求对该模型提供支持。

https://github.com/vllm-project/vllm/issues/3402
这是一个需求优化的issue，主要对象是vLLM中的prefill with prefix cache功能。问题出现的原因是当前实现中批处理大小较小，导致效率低下和重复的incremental_detokenize操作。

https://github.com/vllm-project/vllm/issues/3401
这是一个关于bug报告的issue，主要涉及的对象是vllm项目中的通信操作。由于在代码中未正确处理group参数，导致调用时会hang所有workers。

https://github.com/vllm-project/vllm/issues/3400
这是一个bug报告，该问题涉及的主要对象是VLLM模型benchmark serving脚本。由于被除数为零，导致了ZeroDivisionError: float division by zero错误。

https://github.com/vllm-project/vllm/issues/3399
这是一个bug报告，涉及到VLLM中AWQ和GPTQ模型在并发性能方面慢于ORI模型的问题，可能由于配置或代码中的某些问题导致了这种现象。

https://github.com/vllm-project/vllm/issues/3398
这个issue是提供了一个与主题相关的学术论文链接，而非具体的bug报告或需求。

https://github.com/vllm-project/vllm/issues/3397
这是一个bug报告，涉及的主要对象是vllm的benchmark_serving.py脚本。由于输出长度为零导致的浮点数除零错误引发了此问题。

https://github.com/vllm-project/vllm/issues/3396
这是一个bug报告，该问题涉及安装 `flash_attn` 到 Docker 镜像中。由于最近的修改使得 `flash_attn` 成为一个显式依赖项导致构建失败且增大了 Wheel 大小，因此需要在 Docker 构建中独立安装 `flash_attn`，以在容器化环境中使用 `FlashAttentionBackend` 而不影响其他使用方式。

https://github.com/vllm-project/vllm/issues/3395
这是一个用户提出需求类型的issue，主要涉及到VLLM的批处理功能，用户想要指定最大批处理大小以在连续批处理中找到最佳的最大批处理大小。

https://github.com/vllm-project/vllm/issues/3394
这是一个用户提出需求的类型的 issue，主要涉及的对象是将 FastV 推断加速方法与 vLLM 合并，需要实现在某个层之后删除一些 token 以达到推断加速和降低 GPU 使用的效果。

https://github.com/vllm-project/vllm/issues/3393
这是一个用户提出需求的issue，主要涉及解决当多个vllm实例部署在kubernetes中时，grafana显示多个vllm实例指标时混乱的问题。请求添加一个model_name作为变量，允许用户选择显示哪个vllm实例的指标。

https://github.com/vllm-project/vllm/issues/3392
这是一个bug报告，主要涉及的对象是vllm库和AutoAWQ库，用户在使用AWQ和Marlin进行量化时出现了无法加载权重的错误。

https://github.com/vllm-project/vllm/issues/3391
这是一个关于需求的问题，涉及AsyncLLMEngine.generate方法返回内容格式的调整。用户希望方法调用只返回新生成的文本而不包含每次增量文本。

https://github.com/vllm-project/vllm/issues/3390
这是一个bug报告，主要涉及的对象是在使用MIG分区的H100 GPU上运行vllm时出现(core dumped)的问题，可能是由于MIG启用时脚本崩溃并且减少提示数量也导致崩溃，其中始终在最后一个提示处崩溃。

https://github.com/vllm-project/vllm/issues/3389
这是一个功能需求类型的issue，主要涉及的对象是kernel benchmark script。由于需要调整moe kernel在A100/H100上的性能参数，需要修改benchmark脚本以便直接使用结果。

https://github.com/vllm-project/vllm/issues/3388
这是一个bug报告，涉及Lint工具，因为主分支(master)上Lint功能无法正常工作。这可能是由于代码中的错误或配置问题导致的。

https://github.com/vllm-project/vllm/issues/3387
这是一个bug报告，涉及到vllm在使用过程中出现的导入错误问题，具体原因可能是由于缺少libcudart.so.11.0共享对象文件所致。

https://github.com/vllm-project/vllm/issues/3386
这是一个文档修复类的 issue，主要涉及安装构建所需的依赖包，由于缺少必要的依赖包导致运行 `setup.py` 出错。

https://github.com/vllm-project/vllm/issues/3385
这是一个bug报告类型的issue，涉及的主要对象是vLLM实现，问题在于实现中的窗口大小2048应当对2049个标记进行屏蔽，但实际只对2048个标记进行了遮罩。

https://github.com/vllm-project/vllm/issues/3384
这是一个性能问题的bug报告，主要涉及的对象是VLLM的采样过程。由于CPU和GPU数据同步导致了性能瓶颈，用户反馈推断速度慢，希望寻求解决方案。

https://github.com/vllm-project/vllm/issues/3383
这是一个bug报告类型的issue，主要涉及的对象是名为"test_openai_server.py::test_guided_regex_completion"的测试用例。由于无内容，可能是用户正在报告某个测试用例或功能存在问题，需要修复或调试。

https://github.com/vllm-project/vllm/issues/3382
这是一个用户提出需求的issue，主要涉及到对ChatGLM3和BaiChuan7B等模型进行LoRA支持的实现。导致这个需求的原因是这些模型中的特定层已经被合并，因此需要实现新的线性层来满足LoRA集成的需求。

https://github.com/vllm-project/vllm/issues/3381
这是一个bug报告，主要涉及`RequestMetrics`对象，由于在生成时间戳时混用了`time.monotonic()`和`time.time()`导致了不一致的度量指标。

https://github.com/vllm-project/vllm/issues/3380
这是一个bug报告，主要涉及设置Google DNS for IPv6的问题。由于s.connect(("dns.google", 80))可能导致代码崩溃，并且出现了IPv4的情况已被修复。

https://github.com/vllm-project/vllm/issues/3379
该issue属于用户需求，用户希望VLLM能够选择使用哪个GPU进行推断计算。

https://github.com/vllm-project/vllm/issues/3378
这是一个bug报告，涉及的主要对象是GPU分配器（gpu_allocator）。由于在一些情况下，当启用了prefix caching时，gpu_allocator在计算num_free_gpu_blocks是否足够时没有检查cached_blocks中是否包含所有seq需要的块，导致了这个问题。

https://github.com/vllm-project/vllm/issues/3377
这是一个bug报告，主要涉及vllm中的prefix caching与sliding window attention相互不兼容的问题。由于get_sliding_window函数在调用时没有考虑use_sliding_window的取值，导致在使用prefix caching进行离线批量推理时出现AssertionError错误。

https://github.com/vllm-project/vllm/issues/3376
这个issue属于bug报告类型，主要涉及到了共享内存大小不足导致编译错误的问题。

https://github.com/vllm-project/vllm/issues/3375
这个issue是关于bug报告，涉及的主要对象是处理从文件加载的LoRa模块。原因是在加载时未分配设备导致了问题。

https://github.com/vllm-project/vllm/issues/3374
这是一个bug报告类型的issue，主要涉及的对象是lora设备加载失败。原因可能是通信模块故障或配置错误导致。

https://github.com/vllm-project/vllm/issues/3373
这是一个bug报告，主要涉及对象为Qwen 1.5中的prefix caching与sliding window attention的兼容性问题，导致AssertionError异常。

https://github.com/vllm-project/vllm/issues/3372
该issue属于安全问题报告类型，涉及对象为vllm项目的维护者，提出者希望能够负责安全漏洞披露并建议添加安全政策。导致此问题的原因是研究人员发现了潜在的安全问题，需要及时进行修复。

https://github.com/vllm-project/vllm/issues/3371
这个issue类型是Bug报告，主要涉及的对象是ray_compiled_dag，由于异步引擎在ray上运行时出现了问题，需要进行修复。

https://github.com/vllm-project/vllm/issues/3370
这个issue类型是测试相关的，主要针对异步LLM引擎的分布式测试。

https://github.com/vllm-project/vllm/issues/3369
这个issue是一个bug报告，涉及到vllm的停用词处理，问题出现的原因是停用词不在句末导致程序出错。

https://github.com/vllm-project/vllm/issues/3368
这是一个bug报告，涉及的主要对象是生成IPv4地址时的正则表达式。由于正则表达式的设定问题导致持续集成测试失败。

https://github.com/vllm-project/vllm/issues/3367
这是一个bug报告，涉及VLLM中使用TensorParallel时启动第二个API服务器时出现问题。可能是由于Ray集群注册worker时出现错误导致。

https://github.com/vllm-project/vllm/issues/3366
这是一个 bug 报告，主要涉及 stop words 处理的问题，导致了在 stop words 不在结尾时停止功能出现错误。

https://github.com/vllm-project/vllm/issues/3365
这个issue是一个简单的拼写错误修复问题，涉及的主要对象是文本内容。原因可能是作者不小心输入错误，导致出现了拼写错误。

https://github.com/vllm-project/vllm/issues/3364
这是一个bug报告，涉及到kv cache fp8不支持自动前缀缓存的问题，由于Triton不支持fp8，导致无法使用"auto prefix cache"功能。

https://github.com/vllm-project/vllm/issues/3363
这是一个技术性问题，涉及 ModelExecutor 的新量化内核，用户可能在尝试实现新的量化内核时遇到了问题，需要寻求帮助。

https://github.com/vllm-project/vllm/issues/3362
这是一个需求提出类型的issue，主要涉及的对象是 GPTBigCode 项目中的 Lora Adapter 模块。

https://github.com/vllm-project/vllm/issues/3361
这是一个用户提出需求的issue，主要涉及到在文本生成过程中如何排除不良语言的问题。

https://github.com/vllm-project/vllm/issues/3360
这是一个需求类的issue，主要涉及到issue模板的添加。由于开发人员负担过重，需要先对问题进行分类。

https://github.com/vllm-project/vllm/issues/3359
这是一个用户提出需求的issue，主要涉及到`FlashAttentionBackend`在大小限制方面的问题。用户想要了解为什么只支持特定大小的头部，是否可以扩展支持所有尺寸的头部。

https://github.com/vllm-project/vllm/issues/3358
这是一个bug报告，主要涉及vllm引擎与Codellama2 13B模型在使用过程中需要更多内存的问题，可能是由于custom allreduce kernels 暂时被禁用导致的。

https://github.com/vllm-project/vllm/issues/3357
这是一个关于性能问题的bug报告，涉及的主要对象是vllm项目下的block allocator。由于实现自动前缀缓存后，即使在禁用前缀缓存的情况下，block allocator的性能下降，这导致了性能问题。

https://github.com/vllm-project/vllm/issues/3356
该issue类型为用户提出需求，主要涉及的对象是DeepSeek VL。这个问题是用户希望DeepSeek VL能够支持所有服务功能所致，由于当前DeepSeek VL不支持所有服务功能，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/3355
这个issue是一个bug报告，主要涉及vllm下的prefix caching功能与sliding window attention的不兼容性，导致出现了AssertionError错误。

https://github.com/vllm-project/vllm/issues/3354
这是一个Bug报告，涉及Neuron项目下的问题，由于缺少reset_peak_memory_stats参数导致了RuntimeError错误。

https://github.com/vllm-project/vllm/issues/3353
这是一个bug报告类型的issue，涉及到Neuron库中的错误属性问题。由于Neuron库中调用的mpmath模块版本不兼容导致出现了AttributeError。

https://github.com/vllm-project/vllm/issues/3352
这是一个bug报告，主要涉及文档中的图表出现错误。原因可能是数值参数设定不一致导致了症状Bug。

https://github.com/vllm-project/vllm/issues/3350
这是一个bug报告，主要对象是在使用MultiLoRA并在GPU上设置了tensor parallelism = 1时加载CodeLlama-34B时出现的缺失punica kernel导致的异常。

https://github.com/vllm-project/vllm/issues/3349
这是一个bug报告类型的issue，主要涉及了测试上下文注意力的正确性。由于测试发现了注意力正确性方面的问题，导致用户提出了该bug报告。

https://github.com/vllm-project/vllm/issues/3348
这是一个 bug 报告，涉及主要对象是使用 AWQ 量化进行推理时 GPU 内存异常消耗的问题。

https://github.com/vllm-project/vllm/issues/3347
这个issue是一个bug报告，主要涉及的对象是代码中的block_manager.py，由于共享块引发了双重释放错误，导致出现"double free"错误。

https://github.com/vllm-project/vllm/issues/3346
这是一个用户提出需求的issue，主要涉及的对象是要为vLLM添加对Qwen2MoeModel的支持。

https://github.com/vllm-project/vllm/issues/3345
这是一个关于如何减少VLLM中GPU内存使用的issue，属于用户提出需求并请教问题的类型，主要涉及对象是VLLM模型的GPU内存利用情况。这个问题由于需要10G的GPU内存而非用户期望的7G，可能是因为某些参数设置不正确导致内存占用较高，用户希望找到可以减少GPU内存使用的参数。

https://github.com/vllm-project/vllm/issues/3344
这是一个bug报告，涉及的主要对象是Qwen2模型的checkpoints。由于Qwen2在加载具有`tie_word_embeddings=True`的checkpoints时存在问题，导致出现了一些错误，这个issue的目的就是为了解决这个问题。

https://github.com/vllm-project/vllm/issues/3343
这是一个bug报告，涉及主要对象是`RayGPUExecutorAsync`参数`use_ray_compiled_dag`，该问题可能由于缺少`use_ray_compiled_dag`参数而导致了功能无法正常工作，需要测试是否在设置`use_ray_compiled_dag=True`时能够解决问题。

https://github.com/vllm-project/vllm/issues/3342
这个issue为“用户提出需求”，主要对象是vllm的MLFQ实现。

https://github.com/vllm-project/vllm/issues/3341
这个issue是一个bug报告，主要对象是vllm下的一个组件。由于输入了一个超出范围的浮点数作为top_k参数，导致服务器崩溃并需要手动重启。

https://github.com/vllm-project/vllm/issues/3340
这是一个bug报告，涉及的主要对象是vllm项目中的Baichuan chat template。由于模板文件生成了额外的换行符，导致生成的文本格式与官方实现不一致。

https://github.com/vllm-project/vllm/issues/3339
这是一个用户提出需求的类型的issue，主要对象是GPTQ量化内核的集成问题，用户希望添加4位NormalFloat（NF4）的支持。

https://github.com/vllm-project/vllm/issues/3338
这是一个bug报告，涉及的主要对象是vllm引擎，由于任务意外完成导致AsyncEngineDeadError错误。

https://github.com/vllm-project/vllm/issues/3337
该issue类型为功能需求，涉及对象为vLLM的GeGLU内核，通过添加近似GELU内核为将来更改模型定义做准备，可能由于Hugging Face尚未修复其模型而导致需求添加近似GELU内核。

https://github.com/vllm-project/vllm/issues/3336
这是一个用户提出需求的类型的issue，主要涉及BentoML文档在"Serving"章节的部分更新。

https://github.com/vllm-project/vllm/issues/3335
这是一个用户提出需求的类型的issue，主要涉及的对象是要求支持CohereForCausalLM模型。用户提出需求的原因是希望在项目中添加对CohereForCausalLM模型的支持，因为该模型支持多种语言，包括一些其他模型不支持的语言，如越南语和波斯语，用户认为该模型会被非英语AI社区广泛使用。

https://github.com/vllm-project/vllm/issues/3334
这是一个bug报告，涉及的主要对象是vllm3.2和deepseek coder33b，由于某种原因导致了TCPStore不可用的错误提示。

https://github.com/vllm-project/vllm/issues/3333
这是一个关于软件功能使用问题的issue，主要涉及到使用vllm时关于随机种子设置的疑惑。用户想了解在LLM引擎和SamplingParams中设置随机种子的最佳实践，以及这两者之间的优先级关系。

https://github.com/vllm-project/vllm/issues/3332
该issue属于bug报告类型，主要涉及vllm 0.3.3版本在CUDA 11.8上的兼容性问题，由于PyTorch和vllm都依赖不同版本的CUDA，导致在编译或安装时出现版本不匹配的错误。

https://github.com/vllm-project/vllm/issues/3331
这是一个bug报告，主要涉及对象是使用marlin内核和marlin模型时发生的quantization匹配错误。

https://github.com/vllm-project/vllm/issues/3330
该issue是用户提出需求，主要涉及的对象是CohereForAI/c4aicommandrv01。由于用户希望获得对CohereForAI/c4aicommandrv01的支持，以及该模型在多种语言生成和高性能RAG功能上的表现。

https://github.com/vllm-project/vllm/issues/3329
这个issue是bug报告类型，主要涉及的对象是vllm中的vicuna模型，由于无法找到模型权重导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/3328
这是一个功能需求的issue，主要涉及的对象是实现了一个JSON grammar解析引擎。由于启动时间过长，该功能默认禁用，用户需要使用`enablejsonmode`命令行参数来启用。

https://github.com/vllm-project/vllm/issues/3327
该issue是关于用户需求的，主要涉及的对象是vLLM模型。由于用户想要将模型加载到CPU内存而不是GPU内存，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/3326
该issue属于用户提出需求类型，主要涉及在vLLM中是否能够同时服务多个模型的问题。发起问题的原因是想要选择不同的GPU加载不同模型，但可能受限于GPU内存保留给单个模型的kv缓存。

https://github.com/vllm-project/vllm/issues/3325
这是关于使用`vllm/vllm-openai:v0.3.3`镜像时无法启动StarCoder 2的Bug报告，主要涉及到`transformers`不支持当前发布版本中的SC2，导致出现错误。

https://github.com/vllm-project/vllm/issues/3324
这是一个bug报告，该问题涉及的主要对象是`ops.silu_and_mul`和`ops.gelu_and_mul` kernel，由于这些kernel只支持连续的张量，非连续的张量会导致输出错误结果和单元测试失败。

https://github.com/vllm-project/vllm/issues/3323
这个issue属于bug报告类型，主要涉及了加载finetuned Gemma 2B模型时出现KeyError: lm_head.weight的问题。导致此问题的原因可能是Unsloth输出的权重文件缺少lm_head.weight的信息。

https://github.com/vllm-project/vllm/issues/3322
这个issue属于用户提出的问题，主要涉及chat templates的查找问题，可能是由于存储位置不清晰导致未找到对应的jinja file。

https://github.com/vllm-project/vllm/issues/3321
这是一个bug报告，针对的主要对象是在ROCm上的blockReduceSum计算。由于之前的修复不完全，导致在ROCm上进行layernorm单元测试时出现失败的情况。

https://github.com/vllm-project/vllm/issues/3320
这是一个bug报告，主要涉及对象是vllm的get_tokenizer函数。导致问题的原因是由于在使用AutoTokenizer编码字符串时没有与EOS token相关的选项，若启用tokenizer中的add_eos_token选项可能导致模型生成不相关的序列。

https://github.com/vllm-project/vllm/issues/3319
这是一个bug报告，主要涉及修复marlin模型下的量化参数错误导致的问题。

https://github.com/vllm-project/vllm/issues/3318
这个issue是用户提出需求，请求添加aya-101模型到vllm。

https://github.com/vllm-project/vllm/issues/3317
这是一个用户提出需求的issue，主要涉及的对象是提交第二个拉取请求。可能是由于项目需要进一步修改或添加内容，用户需要再次提交请求。

https://github.com/vllm-project/vllm/issues/3316
这个issue是关于性能问题的。主要对象是vllm中的lora功能，用户在测试中发现动态调用lora的速度比合并lora权重后的速度要慢，询问是否正常。

https://github.com/vllm-project/vllm/issues/3315
这是用户提出需求的类型，主要涉及Qwen1.5模型以及参考llama的Lora，可能是由于Qwen1.5模型目前尚未实现Lora功能，希望参考llama的实现来完成此功能。

https://github.com/vllm-project/vllm/issues/3314
这是一个关于用户提出需求的issue，主要涉及的对象是vllm团队。用户询问了关于pipeline parallelism未在新路线图中的原因，可能是出于实现挑战或者当前计划中没有合适安排。

https://github.com/vllm-project/vllm/issues/3313
这是一个特性需求，用户希望在`LLM` API中支持多个不同的采样参数。

https://github.com/vllm-project/vllm/issues/3312
该issue类型为bug报告，涉及到在cuda 11.8环境下无法成功运行gemma-7b模型。由于环境配置问题，导致用户无法顺利运行相关模型。

https://github.com/vllm-project/vllm/issues/3311
这是一个Bug报告类型的Issue，涉及chat-ui和vllm一起使用时，Llama-2-70b-chat-hf输出异常的问题。造成这一问题的原因可能是对话输出格式在第八条记录后出现异常。

https://github.com/vllm-project/vllm/issues/3310
这是一个Bug报告，主要涉及VLLM中的LoRA模型加载错误导致AsyncEngineDeadError，用户寻求解决方案避免客户端HTTP会话无限挂起。

https://github.com/vllm-project/vllm/issues/3309
这是一个bug报告，涉及的主要对象是vllm项目的编译过程。由于未能正确设置 `NVIDIA_SUPPORTED_ARCHS` 参数，导致编译失败。

https://github.com/vllm-project/vllm/issues/3308
这是一个功能需求的issue，主要涉及到添加和删除finetuned weights的API支持。这个问题产生的原因是在生产系统中需要一个API动态地添加或移除finetuned weights，并且推理调用者不需要在每次调用中指定LoRA位置。

https://github.com/vllm-project/vllm/issues/3307
这是一个用户提出需求的issue，主要对象是关于在nm-vllm中使用稀疏推断和仅权重为int8量化是否可以同时用于进一步提高推断速度的问题。用户询问是否有相关计划。

https://github.com/vllm-project/vllm/issues/3306
这是一个bug报告，主要涉及flash_attn模块和vllm wheel之间的导入问题，由于flash_attn模块已被添加到vllm wheel中，但导入还是失败导致该问题出现。

https://github.com/vllm-project/vllm/issues/3305
这个issue属于功能需求类，主要对象是linting工具，由于当前linter不检查行宽度，导致需要重新启用这个检查的功能。

https://github.com/vllm-project/vllm/issues/3304
这是一个bug报告，主要涉及vllm运行时未检测到支持的设备，并由于未在本地下载模型而导致的错误。

https://github.com/vllm-project/vllm/issues/3303
这个issue类型是bug报告，涉及的主要对象是OAI Chat completions response中包含了chatml tokens for phi-2的问题。由于模型响应中包含了错误的额外token，并且完赛原因是"length"，可能是由于配置chat模板不当导致的。

https://github.com/vllm-project/vllm/issues/3302
这是一个bug报告，主要对象是GPU内存分配。最可能由于参数设置或者算法实现问题导致了GPU内存消耗异常增加的情况。

https://github.com/vllm-project/vllm/issues/3301
这是一个bug报告，涉及到获取tokenizer时出现了错误。由于CC(v0.3.3 vllm.entrypoints.openai.api_server)的问题导致该错误。

https://github.com/vllm-project/vllm/issues/3300
这是一个bug报告，涉及的主要对象是vllm库。由于CUDA 12.1的版本问题，在运行`pip install e .`时导致了编译内核出现错误。

https://github.com/vllm-project/vllm/issues/3299
这是一个文档更新类的Issue，主要涉及的对象是关于LoRA支持信息的模型。由于LoRA文档中存在小错误，导致用户需要更新相关信息。

https://github.com/vllm-project/vllm/issues/3298
这是一个关于bug的issue，主要涉及到best_of方法的行为。由于判断条件错误导致了逻辑错误。

https://github.com/vllm-project/vllm/issues/3297
这是一个关于性能问题的bug报告，用户在使用8个GPU时比使用4个GPU时表现更差，需要对此进行调查。

https://github.com/vllm-project/vllm/issues/3296
这是一个bug报告，主要涉及vllm项目中的api_server模块，问题出现在GPU配置和调用vllm API时导致报错。

https://github.com/vllm-project/vllm/issues/3295
这是一个bug报告，涉及到vllm引擎中的异步引擎错误。由于某些情况下任务意外结束导致的异常错误，请求了在GitHub上开启一个issue。

https://github.com/vllm-project/vllm/issues/3294
这个issue是用户提出需求类型的问题，主要涉及到在使用vllm和ray serve进行多副本的张量并行时可能遇到的问题或需要解决的方案。

https://github.com/vllm-project/vllm/issues/3293
这是一个bug报告类型的issue，主要涉及vLLM在使用ray cluster时无法将rayhead节点固定到placementgroup中，导致vLLM无法成功启动。

https://github.com/vllm-project/vllm/issues/3292
这是一个bug报告，主要涉及lm-evaluation-harness的使用问题，由于API breakage导致评估套件运行速度变慢，影响了模型性能的评估。

https://github.com/vllm-project/vllm/issues/3291
这是一个bug报告，涉及v0.3.3版本的API服务器无法使用Neuron SDK启动的问题。原因是在初始化LLM引擎时出现了错误。

https://github.com/vllm-project/vllm/issues/3290
此issue属于功能改进，请汇报了关于在ROCm上启用基于 FP8（e4m3fn）KV 缓存的建议，主要涉及的对象是在AMD GPU上使用 FP8 缓存。原因是为了在推理过程中减少量化损失，并展示了如何通过AMD Quantizer和nVIDIA AMMO实现FP8量化及KV缓存。

https://github.com/vllm-project/vllm/issues/3289
这是一个功能改进的issue，主要涉及新的AWQ内核的实现。

https://github.com/vllm-project/vllm/issues/3288
该issue属于文档更新类型，主要涉及的对象是OpenAI兼容服务器，原因是为了提供关于启动服务器、调用OpenAI客户端、兼容性、参数覆盖、额外采样参数支持和命令行参数等信息。

https://github.com/vllm-project/vllm/issues/3287
这是一个功能需求的issue，主要涉及的对象是AQLM CUDA支持。由于新增的AQLM压缩推断在运行时格式为1x16和2x8时性能较好，而其他格式在性能上可能受到影响，用户寻求支持这两种格式的CUDA kernels以提高性能。

https://github.com/vllm-project/vllm/issues/3286
这是一个bug报告，涉及的主要对象是 vllm 下的一个 issue。这个问题可能是由于 main 分支上前缀测试错误导致的。

https://github.com/vllm-project/vllm/issues/3285
这是一个bug报告，主要涉及对象是VLLM模型训练在4块RTX 4090 GPU上时出现OOM错误，可能由于模型长度较长、启用了eager模式以及多GPU利用率过高导致内存不足。

https://github.com/vllm-project/vllm/issues/3284
这个issue是关于bug报告，涉及到v0.3.3版本在部署到Inferentia2/NeuronSDK时出现的模块找不到的问题，由于安装包时未将指定目录下的文件复制到期望的路径，导致部署时出现错误。

https://github.com/vllm-project/vllm/issues/3283
这是一个用户提出需求的issue，主要涉及对象是使用json模板与mixtral 7x8b模型。用户想要强制生成的json遵循初始模式而非按字母顺序排列，因为按字母顺序排列对生成质量产生了重大影响。

https://github.com/vllm-project/vllm/issues/3282
这是一个关于bug报告的issue，涉及到vllm tip/master中llama模型推断产生的回归问题，由于合并了PR https://github.com/vllmproject/vllm/pull/3005 导致。

https://github.com/vllm-project/vllm/issues/3281
这是一个关于如何卸载模型的问题，类型为用户提出需求。用户想要在加载多个模型进行比较时能够卸载模型以释放资源，但目前无法找到正确的方法进行模型的卸载。

https://github.com/vllm-project/vllm/issues/3280
这是一个bug报告，涉及主要对象是OpenAI API server，在添加outlines后出现了numba仅支持CUDA而不支持ROCM的错误导致的问题。

https://github.com/vllm-project/vllm/issues/3279
该issue为用户提出需求类型的问题，主要涉及的对象是vLLM推理服务器的调度器。由于调度器过早安排提示，导致了非常小的提示批次，进而导致解码阶段饥饿，用户通过添加`schedulerusedelay`特性来动态增加调度延迟以改善ITL性能。

https://github.com/vllm-project/vllm/issues/3278
该issue类型为用户提出需求，主要涉及对象是vLLM是否支持在cuda图模式下与cupy进行多节点连接，用户想知道vLLM是否有计划支持此功能。

https://github.com/vllm-project/vllm/issues/3277
该issue是关于特性需求的，主要涉及对Serving Benchmark的改进，包括增加前缀缓存基准测试，并添加保存每个请求的单个令牌延迟到结果json以进行进一步调试的功能。

https://github.com/vllm-project/vllm/issues/3276
这是一个bug报告，主要涉及安装最新代码出现的错误。原因可能是源代码中存在问题导致安装失败。

https://github.com/vllm-project/vllm/issues/3275
这是一个bug报告，涉及的主要对象是文档（Docs）。因为未模拟导入（Unmocked Imports）导致页面无法正确渲染。

https://github.com/vllm-project/vllm/issues/3274
这个issue类型为需求，主要对象是合并他人 fork 的代码，问题由于需要整合另一个用户fork的代码。

https://github.com/vllm-project/vllm/issues/3273
这是一个功能新增的issue，主要涉及FlashInfer、Flash Attention后端和缓存引擎，提供了新增功能和改进的实现详情。

https://github.com/vllm-project/vllm/issues/3272
这是一个bug报告，涉及到测试中的潜在状态泄漏，导致CUDA非法内存访问错误。

https://github.com/vllm-project/vllm/issues/3271
这是一个bug报告类型的issue，主要涉及到在Kubernetes中使用Double T4运行Zephyr 7b模型时出现的高CPU使用率问题。由于CPU使用率在接收多个请求时急剧上升到1400-1500%，甚至在空闲期间也不会降低，用户请求解决此问题。

https://github.com/vllm-project/vllm/issues/3270
这个issue类型是bug报告，主要涉及的对象是moe模型实现中的缓冲区容量。原因是缓冲区对于专家来说存在溢出问题，导致可能出现问题。

https://github.com/vllm-project/vllm/issues/3269
这个issue是一个bug报告，主要涉及的对象是vLLM的FlashAttention backend，由于FlashAttention backend导致了构建错误和包大小显著增加，因此用户提出了使`flash_attn`可选的需求。

https://github.com/vllm-project/vllm/issues/3268
这是一个bug报告，涉及的主要对象是lru cache。这个issue的存在可能是由于lru cache不起作用引起的。

https://github.com/vllm-project/vllm/issues/3267
这是一个bug报告，主要涉及安装过程中的错误。由于缺少numpy模块，导致了安装过程出现错误。

https://github.com/vllm-project/vllm/issues/3266
这是一个bug报告，主要涉及的对象是CC(Separate attention backends)，由于某些条件下Python环境无法找到`pip`而导致的错误。

https://github.com/vllm-project/vllm/issues/3265
这个issue是关于bug报告，涉及的主要对象是安装vLLM的过程。由于最近的代码变更导致安装过程中出现了错误，可能是由于创建了不同的子环境并尝试使用其中的程序所导致的问题。

https://github.com/vllm-project/vllm/issues/3264
这是一个bug报告，涉及到Automatic Prefix Caching与multi-LoRA功能冲突的问题，用户需要解决KVcache在不同LoRA适配器间的共享问题。

https://github.com/vllm-project/vllm/issues/3263
这是一个bug报告，主要涉及自动前缀缓存冲突和多LoRA支持的问题，由于LoRA ID未包含在用于前缀块的哈希中，导致冲突问题。

https://github.com/vllm-project/vllm/issues/3262
这是一个bug报告，主要涉及的对象是blockReduceSum函数，由于默认将warp大小设置为32，不考虑不同架构的情况，导致了错误的warp计数。

https://github.com/vllm-project/vllm/issues/3261
这是一个需求提问类型的issue，主要涉及对象是vLLM模型的8位量化支持问题，用户寻求关于在大上下文窗口下使用vLLM时需使用8位量化的帮助。

https://github.com/vllm-project/vllm/issues/3260
该issue属于用户提出需求类型，主要涉及将引擎健康检查连接到OpenAI服务器，以便在Kubernetes健康探针中使用。这个问题的提出是为了使异步引擎更加健壮。

https://github.com/vllm-project/vllm/issues/3259
这是一个bug报告，主要对象是Tensor对象。由于Tensor对象具有多个值，导致了布尔值的歧义性。

https://github.com/vllm-project/vllm/issues/3258
这是一个bug报告，涉及的主要对象是 Linear Layer 中与 Torch 兼容性的问题。由于torch.empty()在1.9.0及以后版本有一个函数签名改动，导致在模型加载时调用该函数时出现错误，需要在layers/linear.py文件中进行修正。

https://github.com/vllm-project/vllm/issues/3257
这是一个关于性能优化的issue，主要涉及到GPU利用率和吞吐量的问题。原因可能是当前配置下无法最大化GPU利用率，需要优化配置以提高吞吐量。

https://github.com/vllm-project/vllm/issues/3256
这个issue是Bug报告，涉及的主要对象是vllm项目中与CUDA版本相关的依赖包。该问题是由于CUDA 11.8构建时依赖于cupycuda12x而非cupycuda11x，导致出现安装问题的症状。

https://github.com/vllm-project/vllm/issues/3255
这是一个bug报告，涉及的主要对象是Docker image。由于合并CC（Separate attention backends）后，导致使用Docker image时出现了错误。

https://github.com/vllm-project/vllm/issues/3254
这是一个bug报告类型的issue，主要涉及将flash_attn包含在docker image中的问题。问题产生的原因是flash_attn在Docker image中缺失，解决方法是确保将thirdparty_files的内容复制到image中。

https://github.com/vllm-project/vllm/issues/3253
这是一个bug报告，主要涉及的对象是vllm库中设置tensor_parallel_size参数时出现的RunTimeError。问题可能是由于未找到可用的GPU导致的。

https://github.com/vllm-project/vllm/issues/3252
这是一个Bug报告，涉及的主要对象是benchmark_serving.py脚本。该问题是因为`model`是必选参数，而`tokenizer`不是必选参数，导致执行命令时缺少必选参数而出现错误。

https://github.com/vllm-project/vllm/issues/3251
这是一个bug报告，主要涉及的对象是模型在计算logits时出现了缓存不正确的问题，导致生成时返回的logits不完整。

https://github.com/vllm-project/vllm/issues/3250
这个issue属于功能需求，主要涉及了"Lookahead scheduling"的概念的引入。这个issue由于需要提前为每个序列分配KV插槽，以便用于推测解码，带来了新功能和修改的需求。

https://github.com/vllm-project/vllm/issues/3249
这是一个关于bug报告的issue，主要对象是vllm下的EngineArgs类，由于传入了一个未预期的关键字参数'ray_workers_use_nsight'导致了TypeError错误。

https://github.com/vllm-project/vllm/issues/3248
这是一个关于vLLM在FastAPI环境中处理并发请求的bug报告，主要涉及的对象是OrionStarAI/Orion14BChatInt4 quantization model。导致该问题的原因是在生成答案之前接收到另一个请求时出现错误。

https://github.com/vllm-project/vllm/issues/3247
这是一个bug报告，涉及到使用nsys profile时出现的错误。用户提到想要使用nsight system来profile vllm，但即使Ray默认没有使用，nsys profile仍然失败。

https://github.com/vllm-project/vllm/issues/3246
这是一个bug报告，涉及到代码中的参数错误，错误地填充了前缀测试用例，导致了问题。

https://github.com/vllm-project/vllm/issues/3245
这个issue属于用户提出需求的类型，主要涉及模型与LoRA适配器的支持关系。原因是该模型不支持LoRA，导致数值错误，并询问哪些模型受支持以及关于模型适配的计划。

https://github.com/vllm-project/vllm/issues/3244
这是一个bug报告，主要涉及VLLM中的解码过程，由于传递多个标记导致非法内存访问错误。

https://github.com/vllm-project/vllm/issues/3243
这是一个用户提出需求的issue，主要目的是增强lora测试中的层次和排名变化。

https://github.com/vllm-project/vllm/issues/3242
这是一个需求提出的issue，主要涉及对象是Vim-Like-LaTeX-Markup (vllm)插件。由于tmux窗口大小可能会改变，建议添加`dynamic_ncols=True`选项以提高使用体验。

https://github.com/vllm-project/vllm/issues/3241
这是一个bug报告，主要涉及文件锁在全局`/tmp/`目录下的访问权限问题，导致多用户系统出现冲突。

https://github.com/vllm-project/vllm/issues/3240
该issue类型为性能优化，针对的主要对象是针对A100 TP2的fused moe kernel配置。

https://github.com/vllm-project/vllm/issues/3239
这是一个bug报告，涉及的主要对象是VLLM模型的自动前缀缓存bug。这个bug由于当整个提示已被计算时导致创建非计算提示标记的零尺寸张量，引发了torch.arrange中的异常。

https://github.com/vllm-project/vllm/issues/3237
这是一个用户提出需求的issue，涉及的主要对象是OpenAI Tools的函数调用功能。这个问题是由于需要更新使用新的guided generation实现，用户希望在查询时可以根据需要选择使用模板系统或者引导生成参数。

https://github.com/vllm-project/vllm/issues/3236
这是一个需求修改类型的issue，主要涉及代码实现和查询格式的调整，因为当前的查询格式不适用于chunked prefill，导致在启用chunked prefill后无法有效批处理填充和解码请求。

https://github.com/vllm-project/vllm/issues/3235
这是一个需要重构查询形状以支持特定功能的Issue，主要涉及到查询形状和填充配置的调整。这个问题是因为当前查询格式不适合chunked prefill的情况而引起的。

https://github.com/vllm-project/vllm/issues/3234
这是一个bug报告，主要涉及FP8 KV cache与prefix caching结合时的问题，由于FP8 KV缓存的数据类型与Prefix cache Triton内核不兼容，导致了该问题。

https://github.com/vllm-project/vllm/issues/3233
这是一个功能需求的issue，主要涉及的对象是模型运行器（model_runner），产生的问题是为了使Sampler更简单和清晰而将`logits`计算和收集迁移到`model_runner`中。

https://github.com/vllm-project/vllm/issues/3232
这是一个bug报告，涉及的主要对象是在使用vllm:0.3.2时部署 mistralai/Mixtral8x7BInstructv0.1 模型时出现权限被拒绝的错误。

https://github.com/vllm-project/vllm/issues/3231
该issue属于代码优化类型，主要涉及的对象是`tensor_model_parallel_all_gather`函数。这个问题是由于CC(Use NCCL instead of ray for controlplane communication)造成的，导致`tensor_model_parallel_all_gather`函数目前似乎没有被使用，而被`tensor_model_parallel_gather`函数替代。

https://github.com/vllm-project/vllm/issues/3230
这是一个bug报告，主要涉及vllm的benchmark model，可能由于请求发送后未等待响应导致服务访问错误。

https://github.com/vllm-project/vllm/issues/3228
这是一个bug报告，涉及的主要对象是在运行pytest测试时出现的`_ZNSt15__exception_ptr13exception_ptr9_M_addrefEv`未定义符号错误，问题可能是由于运行环境中的gcc版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/3227
这是一个bug报告，主要对象是vllm的代码库。由于前缀bug导致了一些功能无法正常工作。

https://github.com/vllm-project/vllm/issues/3226
这个issue是关于bug报告；主要涉及的对象是vllm下的SqueezeLLM quantization model；由于无法成功加载SqueezeLLM quantization model导致的bug。

https://github.com/vllm-project/vllm/issues/3225
这个issue是用户提出需求的类型，主要对象是对QLORA/QA-QLORA权重的支持，原因是目前仅支持原始LORA作为未融合的适配器，用户希望能够在不与基础模型融合的情况下添加对QLORA/QA-QLORA支持。

https://github.com/vllm-project/vllm/issues/3224
这个issue属于bug报告类型，涉及到core tests的修复，由于特定的commit导致了测试失败。

https://github.com/vllm-project/vllm/issues/3223
这是一个寻求帮助的问题，主要涉及Qwen1.5模型如何使用本地路径。据描述，出现问题的代码无法正常工作。

https://github.com/vllm-project/vllm/issues/3222
这是一个bug报告，涉及主要对象为vllm 0.3.2。由于NCCL_ERROR_INTERNAL_ERROR导致的内部错误，导致了该错误。

https://github.com/vllm-project/vllm/issues/3221
这个issue类型为功能更新，主要涉及将FlashInfer集成到vLLM中，但目前尚未实际使用。

https://github.com/vllm-project/vllm/issues/3220
这是一个bug报告，涉及到了time.time()和time.monotonic()函数的错误使用。由于混用了这两个函数并尝试进行计算，导致了功能异常中的bug。

https://github.com/vllm-project/vllm/issues/3219
这是一个性能问题报告，涉及的主要对象是lora模块加载，可能由于不同的启动方式导致了性能差异。

https://github.com/vllm-project/vllm/issues/3216
这个issue是关于需求的，用户询问了何时将Qwen Lora模型整合并加载。

https://github.com/vllm-project/vllm/issues/3215
该问题属于需求询问类型，主要涉及的对象是Qwen Lora模型集成。由于用户想了解何时集成Qwen Lora模型，可能是因为希望在模型中使用该功能或者了解相关集成进度。

https://github.com/vllm-project/vllm/issues/3214
这是一个bug报告，涉及的主要对象是`vllm`和`transformers`，由于更新操作导致'SamplingParams' object没有update属性，出现了错误。

https://github.com/vllm-project/vllm/issues/3213
这个issue类型是bug报告，涉及的主要对象是vLLM的get_ip()方法。原因是缺少fallback导致无法部署新版本到Sagemaker。

https://github.com/vllm-project/vllm/issues/3212
这是一个bug报告，该问题涉及vLLM在部署到Sagemaker时出现的错误。由于`get_ip()`函数在0.3.0版本中的改动导致了无法连接到Sagemaker的问题。

https://github.com/vllm-project/vllm/issues/3211
这个issue是用户提出的需求类型，需求涉及支持在OpenAI和Context Free Grammar中使用任意的json_object，可能由于现有功能限制导致无法实现该需求。

https://github.com/vllm-project/vllm/issues/3210
这是一个用户提出需求的issue，主要涉及对象是npcache load format，由于未能提供比safetensors更快的加载速度，反而更慢，用户在询问是否有其特定使用场景。

https://github.com/vllm-project/vllm/issues/3209
这是一个bug报告类型的issue，涉及的主要对象是vLLM模型服务端。由于接收到多个并发请求时，服务器会返回空响应的情况，可能是由于请求速率过高导致部分响应为空。

https://github.com/vllm-project/vllm/issues/3208
这是一个关于性能优化和开发需求的issue，主要涉及Mixtral前向传播中的计算优化问题，由于时间大部分花费在全连接层/融合moe内核，需要使用fp8进行计算以提升性能。

https://github.com/vllm-project/vllm/issues/3207
这是一个Bug报告，涉及的主要对象是VLLM的LLMEngine对象。导致这个bug的原因是闭包中不正确地引用了self，导致整个LLMEngine对象被封装并序列化。

https://github.com/vllm-project/vllm/issues/3206
该issue为一个功能增强类型的内容，主要涉及异步tokenization和线程池的使用。在该issue中，用户提出使用线程池而非ray来进行tokenization，主要是由于HF tokenizers在官方声明中不是线程安全的，尽管实际上除非涉及填充/截断这种操作，否则它们在实践中可能是线程安全的。

https://github.com/vllm-project/vllm/issues/3205
这是一个需求提出类型的issue，主要涉及日志记录相关的度量指标，原因可能是为了满足交付物1的要求。

https://github.com/vllm-project/vllm/issues/3204
这个issue类型为功能开发中的工作进行中（Work in Progress），主要涉及到chunked prefill部分的实现。由于目前chunked prefill功能尚未实现，导致用户无法使用相关功能。

https://github.com/vllm-project/vllm/issues/3203
这个issue是用户提出需求。用户希望能支持ExLlamaV2作为一个快速而优秀的库来运行LLM。

https://github.com/vllm-project/vllm/issues/3202
这是一个用户提出需求的问题，主要对象是 OpenAI 服务器，用户提出了关于添加授权机制以确保端点安全的需求。

https://github.com/vllm-project/vllm/issues/3201
这是一个关于代码报错的bug报告，主要涉及VLLM是否支持QWEN LoRa模型的问题。由于使用了特定的路径和参数调用LLM时产生了报错，导致程序无法正常运行。

https://github.com/vllm-project/vllm/issues/3200
这是一个bug报告类型的issue，主要涉及的对象是vllm项目。由于缺少一些微小的修改，导致AutoGPTQ无法支持vllm，进而影响了对Gemma的支持。

https://github.com/vllm-project/vllm/issues/3199
这是一个Bug报告，主要涉及的对象是VLLM中的Model QWenLMHeadModel。由于启用了LoRA功能，但该模型并不支持LoRA，导致了数值错误（ValueError）。

https://github.com/vllm-project/vllm/issues/3198
这是一个bug报告类型的issue，主要涉及了使用vllm项目中的RayWorkerVllm初始化LLM引擎时，在特定条件下会出现CUDA设备未找到的错误。由于在初始化过程中调用了torch.cuda相关函数导致CUDA_VISIBLE_DEVICES设置无效，进而导致torch.cuda.is_available()返回False。

https://github.com/vllm-project/vllm/issues/3197
这是一个 bug 报告，涉及对象是 punica LoRA kernels，由于所需的计算容量需要大于等于 8.0，导致在 SM 7.5 的 V100 GPU 上无法运行。

https://github.com/vllm-project/vllm/issues/3196
这是一个bug报告，涉及的主要对象是在使用llamahf65B模型进行分布式推断时遇到Ray的OOM异常。

https://github.com/vllm-project/vllm/issues/3195
这是一个bug报告，涉及到使用vllm的openchat模型时产生完全错误答案的问题。可能由于模型加载或推理过程中出现了问题导致。

https://github.com/vllm-project/vllm/issues/3194
这个issue类型为bug报告，主要涉及对象是VLLM中的自动前缀缓存基准测试，修复了TPOT/ITL计算和处理模型返回空响应的问题。

https://github.com/vllm-project/vllm/issues/3193
这是一个bug报告，主要涉及vLLM下的automatic prefix caching功能。由于启用了此功能会偶尔导致崩溃，涉及的vLLM版本为openbuddydeepseek67bv18.14kgptq，GPU为4 x RTX3090。

https://github.com/vllm-project/vllm/issues/3192
这是一个用户提出需求的issue，主要涉及对象是vLLM模型。用户想要获取在使用vLLM时的注意力分数。

https://github.com/vllm-project/vllm/issues/3191
这是一个功能需求提议，主要涉及的对象是`LLMEngine`中的分布式工作管理器部分。由于需要在不使用ray的情况下支持单盒分布式执行，并且将代码分离为不同的硬件后端，因此需要添加分布式模型执行器抽象。

https://github.com/vllm-project/vllm/issues/3190
这是一个关于bug报告的issue，涉及主要对象是vLLM中的distributed inference功能。由于设置了tensor parallel size大于1，导致在ray上无法分布式推断时出现了GPU分配问题。

https://github.com/vllm-project/vllm/issues/3189
这个issue类型为用户提出问题，主要涉及的对象是关于VLLM服务器生成token数的度量，用户询问如何准确计算服务器的token生成速率问题。

https://github.com/vllm-project/vllm/issues/3188
这个issue是一个bug报告，涉及对象为TextGenerationStreamResponse类。由于client.text_generation方法的参数错误导致了TypeError异常。

https://github.com/vllm-project/vllm/issues/3187
这个issue是新增功能请求，涉及的主要对象是支持嵌入模型的集成。

https://github.com/vllm-project/vllm/issues/3186
这是一个包含关于新增Triton采样核心和相应问题的技术性讨论，涉及到代码性能优化以及问题解决方案的讨论。

https://github.com/vllm-project/vllm/issues/3185
这个issue类型是性能优化建议，主要涉及到模型加载时的性能问题，由于默认设备为'cpu'导致了加载速度严重变慢。

https://github.com/vllm-project/vllm/issues/3184
这是一个bug报告，涉及到Cupy在Docker环境中导入错误的问题，由于使用了tensorparallel >1导致vllm在cupy导入时出现问题。

https://github.com/vllm-project/vllm/issues/3183
这是一个新功能需求，主要涉及Jais模型的支持，而不是bug报告。

https://github.com/vllm-project/vllm/issues/3182
这是一个bug报告，主要涉及到vllm项目中模型加载过程中的性能问题。原因是默认设备设置为'cpu'导致加载速度严重降低，从而需要改进实现以提高加载速度。

https://github.com/vllm-project/vllm/issues/3181
这是一个需求提出类型的issue，主要涉及的对象是requirements-dev.txt文件。由于缺少aiohttp==3.9.3包，在benchmarking脚本中无法正常运行，需要添加该依赖包。

https://github.com/vllm-project/vllm/issues/3180
这是一个安装问题报告issue，主要涉及到vllm的编译安装失败的情况。可能由于环境配置不兼容导致编译安装失败。

https://github.com/vllm-project/vllm/issues/3179
这是一个关于bug的issue，主要涉及的对象是vLLM项目下的ChatCompletion功能，由于返回的logprobs格式不符合OpenAI API的规范导致了这个问题。

https://github.com/vllm-project/vllm/issues/3177
这是一个用户提出新增功能需求的issue，主要涉及的对象是Qwen2设备。

https://github.com/vllm-project/vllm/issues/3176
这是一个bug报告，主要涉及的对象是vllm的代码。由于域名dns.google无法正确解析为IP地址，在特定网络环境下可能会导致socket.gaierror异常。

https://github.com/vllm-project/vllm/issues/3175
这是一个bug报告，涉及的主要对象是benchmark_throughput.py脚本，用户提出了设置正确的gpumemoryutilization值解决了错误问题，疑问是关于benchmark_throughput.py参数的正确使用。

https://github.com/vllm-project/vllm/issues/3174
这是一个bug报告，主要涉及到s-lora的服务器API中缺少loramodules参数，导致客户端发送请求时需要添加lora本地路径，从而导致不现实的情况。

https://github.com/vllm-project/vllm/issues/3172
这是一个需求提出类型的issue，主要涉及的对象是vllm库中GPU的指定，由于版本更新导致以前可指定GPU的功能在0.2.6之后的版本中不再受支持，用户提出了需要指定GPU编号的功能以提高资源利用和吞吐量。

https://github.com/vllm-project/vllm/issues/3171
这是一个bug报告类型的issue，主要涉及到vllm项目中的benchmark_prefix_caching.py文件。由于`prefix_pos`不是`LLM.generate`的参数，所以需要将其从benchmark_prefix_caching.py中删除。

https://github.com/vllm-project/vllm/issues/3170
这是一个bug报告，主要涉及的对象是vllm==0.3.3库。由于某种原因导致该版本需要访问Google，可能导致用户难以理解或者产生疑惑。

https://github.com/vllm-project/vllm/issues/3169
这个issue类型是bug报告，涉及主要对象是jsonschema版本。由于jsonschema版本低于4.3.0，导致引入包时出现错误。

https://github.com/vllm-project/vllm/issues/3168
这是一个bug报告类型的issue，涉及主要对象为VLLM引擎中的max queue time控制功能。由于设置max_wait_q_len为3时，测试中的请求被错误地保留在等待队列中，导致测试陷入无限循环状态。

https://github.com/vllm-project/vllm/issues/3167
这是一个功能需求的issue，主要涉及的对象是vLLM。由于缺少Quadratic和Cubic Sampling方法导致用户提出了添加这些功能的需求。

https://github.com/vllm-project/vllm/issues/3166
这是一个功能需求的issue，主要对象涉及到Sequence类，提出将`eos_token_id`存储在Sequence中以方便频繁访问。

https://github.com/vllm-project/vllm/issues/3165
这个issue类型为用户提出需求，涉及的主要对象是Model support中的starcoder2，用户提出需要支持bigcode/starcoder215b。

https://github.com/vllm-project/vllm/issues/3164
这是一个关于工具功能改进的issue，主要涉及到对vLLM仓库中过期或无关的问题进行自动清理。原因可能是问题过多导致组织混乱，需要更好的管理和维护。

https://github.com/vllm-project/vllm/issues/3162
这是一个功能需求类型的issue，主要涉及的对象是用于高张量并行性模型分析的Ray框架。

https://github.com/vllm-project/vllm/issues/3161
这个issue是一个功能需求，主要涉及到vLLM版本信息的记录和访问。原因是为了方便运维和版本一致性验证。

https://github.com/vllm-project/vllm/issues/3160
这个issue是一个bug报告，涉及的主要对象是vllm server，问题是由于配置不当导致无法通过100 VU 运行load test。

https://github.com/vllm-project/vllm/issues/3159
该issue属于用户提出需求的类型，主要对象是关于使用API端点进行微调的功能。用户提出这个需求是因为希望更易于进行微调操作。

https://github.com/vllm-project/vllm/issues/3158
这个issue是一个bug报告，涉及到样式在自动前缀缓存中的问题。

https://github.com/vllm-project/vllm/issues/3157
这个issue类型是bug报告，主要涉及的对象是CI。由于问题可能导致了模型测试失败，用户提出需要修复模型测试的帮助。

https://github.com/vllm-project/vllm/issues/3156
这是一个bug报告，涉及的主要对象是FP8 KV cache，由于Triton kernel目前不支持KV8，导致了无法使用prefix caching的问题。

https://github.com/vllm-project/vllm/issues/3155
这是一个发布跟踪（Release Tracker）类型的issue，主要对象是针对Vllm版本 v0.4.0 的进展情况。

https://github.com/vllm-project/vllm/issues/3154
这是一个bug报告类型的issue，主要涉及到生成文本的速度问题。导致生成文本速度慢的原因需要进一步分析。

https://github.com/vllm-project/vllm/issues/3153
这是一个功能需求提议，主要对象是支持Mistral模型推断。

https://github.com/vllm-project/vllm/issues/3152
这是一个bug报告，涉及的主要对象是vllm模型Mixtral-8x7B的部署问题。这个问题可能是由于vllm版本0.3.2与cuda=12.1和pytorch=2.1.2不兼容导致的，具体表现为NCCL_ERROR_INVALID_USAGE错误。

https://github.com/vllm-project/vllm/issues/3151
这是一个bug报告，主要涉及LoRA adapter模型中存在剩余张量导致的Assertion Error。

https://github.com/vllm-project/vllm/issues/3150
这是一个bug报告，涉及到stop_token_ids包含special_tokens导致输出文本被截断的问题。

https://github.com/vllm-project/vllm/issues/3149
这是一个未填写内容的issue，类型为需求提出。主要对象扩展到与该issue相关的软件应用。

https://github.com/vllm-project/vllm/issues/3148
这是一个用户提出需求的issue，主要涉及的对象是OpenAI server。这个issue是关于支持response_format为json_object的功能需求，目的是让vLLM能够处理任意的JSON格式数据。

https://github.com/vllm-project/vllm/issues/3147
这是一个bug报告，主要对象涉及是vLLM与Langchain的集成，导致了ImportError。

https://github.com/vllm-project/vllm/issues/3146
这是一个bug报告类型的issue，主要涉及到VLLM库中在准备prefill输入时未正确更新context_len和block tables。这可能导致预期的输入metadata不完整或不准确。

https://github.com/vllm-project/vllm/issues/3145
这个issue是一个bug报告，涉及的主要对象是vllm应用程序。由于某种原因，开启了echo选项会导致内部服务器错误。

https://github.com/vllm-project/vllm/issues/3144
这个issue属于用户提出需求类型，主要对象是OpenAI completion API，用户提出了添加选项以截断提示标记的功能请求。

https://github.com/vllm-project/vllm/issues/3143
这是一个bug报告，该问题涉及到vllm下的streaming response功能，由于使用CC(exclude_unset=True)参数会导致tokens字段在streaming logprobs中被移除，需要恢复不正确排除的字段来解决这个问题。

https://github.com/vllm-project/vllm/issues/3142
这是一个关于bug报告的issue，主要涉及vllm项目中的`exclude_unset=True`参数导致部分字段在序列化时被移除的问题。

https://github.com/vllm-project/vllm/issues/3141
这个issue是一个需求提出类型，主要涉及vllm中v100对int4的支持情况，用户希望确认这一功能是否真正可行。

https://github.com/vllm-project/vllm/issues/3140
这个issue是一个功能增强请求，涉及的主要对象是为AMD GPU添加对Punica内核的支持，主要原因是为了在AMD GPU上启用multiLoRA，并解决不同于Nvidia GPU的warp size差异引起的问题。

https://github.com/vllm-project/vllm/issues/3139
这是一个Bug报告，主要涉及模块导入错误，导致了"ModuleNotFoundError: No module named 'outlines.fsm'"的错误。

https://github.com/vllm-project/vllm/issues/3138
这是一个用户询问问题的类型，主要涉及的对象是关于使用fused_moe kernel实现专家并行性的问题。用户想了解当前kernel实现是否能够支持实现专家并行性。

https://github.com/vllm-project/vllm/issues/3137
这是一个用户提出需求的类型，该问题单主要涉及到vllm中如何禁用cuda_graph。由于用户可能希望在使用过程中禁用cuda_graph以解决特定问题或者优化性能，因此提出了如何在vllm中禁用cuda_graph的问题。

https://github.com/vllm-project/vllm/issues/3136
这是一个bug报告，主要涉及vllm和fastchat模型生成了"assistant"特殊标记的问题。由于特殊标记干扰了生成结果，用户希望找到解决方案。

https://github.com/vllm-project/vllm/issues/3135
这是一个功能需求的issue，主要涉及的对象是triton="2.0.0"版本的prefix_prefill.py，用户提出是否计划实现该版本的prefix_prefill.py。

https://github.com/vllm-project/vllm/issues/3134
这是一个bug报告，该问题涉及到代码中相对导入路径的修复，可能由于相对导入路径不正确导致错误的模块引入。

https://github.com/vllm-project/vllm/issues/3133
这是一个bug报告，涉及ROCm构建问题，用户遇到了使用VLLM时出现的模块未找到错误。

https://github.com/vllm-project/vllm/issues/3132
这是一个bug报告，主要涉及的对象是一个任务。由于任务在意料之外地结束，导致了这个问题的出现。

https://github.com/vllm-project/vllm/issues/3131
这是一个bug报告，主要涉及对象是vllm中的detokenize_incrementally.py文件，由于输出文本被截断，导致出现了错误的现象。

https://github.com/vllm-project/vllm/issues/3130
这是一个特性请求（Feature Request）的issue，主要涉及Chunked Prefill功能。由于内存绑定的解码请求与计算绑定的预填充请求之间存在重叠，可以极大地提高系统效率。

https://github.com/vllm-project/vllm/issues/3129
这是一个更新请求，涉及到版本升级调整，关闭了一个版本发布跟踪任务。

https://github.com/vllm-project/vllm/issues/3128
这是一个用户提出需求的issue，主要涉及到使用vLLM支持Hugging Face下载的Mixtral-8x7B-Instruct-v0.1模型的4-bit量化版本，用户希望得到关于如何在vLLM中使用该4-bit版本的帮助。

https://github.com/vllm-project/vllm/issues/3127
这是一个bug报告，主要涉及的对象是benchmarking script，由于未限制最大并发性导致无法精确控制负载水平。

https://github.com/vllm-project/vllm/issues/3126
这个issue是用户提出需求，希望将vllm发布到condaforge，并询问是否有计划这样做。

https://github.com/vllm-project/vllm/issues/3125
这是一个功能需求类型的issue，主要涉及到在离线环境中使用预缓存文件加载模型的功能。问题提出需要增加`localfilesonly`标志支持加载本地缓存文件。

https://github.com/vllm-project/vllm/issues/3124
这个issue属于一个新功能需求，主要涉及实现`min_tokens`抽样参数，解决了相关PR无法继续的问题。

https://github.com/vllm-project/vllm/issues/3123
这个issue属于需求提出类型，主要涉及cupy在ROCm backend中的启用问题，由于需要在cudagraph模式下成功运行吞吐量基准测试脚本，因此需要启用cupy。

https://github.com/vllm-project/vllm/issues/3122
这是一个bug报告，涉及的主要对象是在使用vllm库时出现RuntimeError。这个bug可能是由于在特定环境下运行模型时引发的异常而导致。

https://github.com/vllm-project/vllm/issues/3121
这是一个功能需求的issue，主要涉及vLLM中添加Sarathi-Serve支持，以提高吞吐量和降低延迟。

https://github.com/vllm-project/vllm/issues/3120
这是一个功能需求类型的issue，主要对象是VLLM模型。由于缺乏直接测量模型权重内存使用的功能，用户提出了添加这一功能的建议。

https://github.com/vllm-project/vllm/issues/3119
这个issue是用户提出需求，并询问使用vllm进行离线推理时是否应该使用tokenizer模板，由于文档不明确，用户需要澄清在离线推理时是否需要应用tokenizer模板。

https://github.com/vllm-project/vllm/issues/3118
这是一个bug报告，涉及Qwen1.5的使用。导致问题的原因可能是tokenizer和vocab_size之间的不一致导致了TypeError。

https://github.com/vllm-project/vllm/issues/3117
这个issue是关于Bug报告，涉及的主要对象是为VLLM添加Encoder-decoder模型支持和T5模型支持的功能。 由于T5large和更大模型的输出在某些时候出现NaN值，可能是由于对应的CUDA内核存在问题，导致此问题。

https://github.com/vllm-project/vllm/issues/3116
这个issue类型是bug报告，主要涉及对象是LLM模块的内存分配与释放，由于LLM创建失败时没有正确释放内存所导致的bug。

https://github.com/vllm-project/vllm/issues/3115
这是一个bug报告类型的issue，描述了ConnectionResetError错误，主要涉及网络连接问题导致的错误。

https://github.com/vllm-project/vllm/issues/3113
该issue类型为Hackathon项目，主要对象是LoRa。由于用户需要在Hackathon项目中进行LoRa的操作，因此提出了这个issue。

https://github.com/vllm-project/vllm/issues/3112
这是一个关于bug报告的issue，主要涉及NeuronLS在Windows Subsystem for Linux（WSL）环境下构建时遇到的权限错误问题。

https://github.com/vllm-project/vllm/issues/3111
这是一个bug报告，该问题涉及到SWA模型未支持前缀缓存功能。可能由于该功能未被实现或存在错误导致该bug。

https://github.com/vllm-project/vllm/issues/3110
这个issue类型是需求提出，主要涉及的对象是vllm这个项目。源自用户对ORCA批量优化功能的需求。

https://github.com/vllm-project/vllm/issues/3109
这是一个用户提出需求的类型，涉及日志级别设置功能，用户希望能够通过命令行参数选择日志级别而不是固定的'info'，问题可能源于用户希望在启动API服务器时能够更灵活地控制日志输出级别。

https://github.com/vllm-project/vllm/issues/3108
这是一个测试相关的任务，不是bug报告，主要涉及Block Manager和Scheduler。

https://github.com/vllm-project/vllm/issues/3107
这是一个bug报告，主要涉及安装vllm时出现的依赖问题，导致无法安装成功。

https://github.com/vllm-project/vllm/issues/3106
这是一个提出需求的issue，涉及的主要对象是"Chunked Prefill"功能。由于还在进行中（Work in Progress），用户可能正在尝试实现或者讨论关于这个功能的改进，具体表现为为了实现这个功能而需要完成的工作或线索等。

https://github.com/vllm-project/vllm/issues/3105
这是一个功能需求的issue，主要涉及到kv缓存模块的重构，使得开发者可以更容易地实验和自定义kv缓存变化。

https://github.com/vllm-project/vllm/issues/3104
这是一个bug报告，涉及的主要对象是vllm项目中的kv类型检查，由于原因未将kv类型检查放置位置导致错误报告nvcc未在AMD平台上找到。

https://github.com/vllm-project/vllm/issues/3103
这是一个关于实现vLLM Worker的issue，类型是功能需求。主要涉及的对象是vLLM Worker，由于目前工作尚未与LLMEngine完全配合，需要在后续的PR中进行。

https://github.com/vllm-project/vllm/issues/3102
这是一个需求类型的issue，主要涉及的对象是GPU运行相关的代码文件。由于之前硬编码了关于cuda的API调用，导致无法支持更多设备，因此提出提供接口以隐藏cuda API调用来更友好地支持新设备。

https://github.com/vllm-project/vllm/issues/3101
这是一个bug报告类型的issue，主要涉及到模型架构不支持的问题，导致数值错误。

https://github.com/vllm-project/vllm/issues/3100
这是一个用户提出需求的类型，该问题单涉及的主要对象是缓存配置信息的展示。由于用户希望能通过Prometheus指标获取缓存配置信息并在Grafana中展示，所以提出了这个需求。

https://github.com/vllm-project/vllm/issues/3099
这是一个bug报告，主要涉及对象是vllm中的LogitsProcessors。由于在v0.3.2版本中`logits_processors`的使用变得非常缓慢，因此需要解决相关问题。

https://github.com/vllm-project/vllm/issues/3098
这是一个bug报告，涉及主要对象为VLLM库。由于文件'ldconfig'不存在或不可访问，导致无法使用prefix_pos参数。

https://github.com/vllm-project/vllm/issues/3097
这个issue是一个需求提出类型的问题，涉及主要对象是软件中的新功能和性能优化，用户请求添加了新的功能支持和性能优化，并要求在特定日期发布。

https://github.com/vllm-project/vllm/issues/3096
这是一个关于性能问题的bug报告，针对vLLM在高并发情况下达到第一个token速度慢的情况。

https://github.com/vllm-project/vllm/issues/3095
这是一个功能需求类型的issue，主要涉及添加批处理的RoPE内核，由于目前需要为每个LoRA请求调用旋转嵌入内核，导致无法有效地为多个具有不同上下文长度的LoRA提供服务。

https://github.com/vllm-project/vllm/issues/3094
这是一个bug报告，主要涉及的对象是cpu kv cache。由于请求从`Running`状态变为`Pending`，且观察到cpu kv cache一直为0%，可能导致的原因是请求未能正确被处理或缓存未被有效利用。

https://github.com/vllm-project/vllm/issues/3093
该issue属于特性请求类型，主要涉及到许可证更新。由于原始仓库的LICENSE标头被继承到了分支中，需要帮助将metadata从Apache更新为Neural Magic Community License。

https://github.com/vllm-project/vllm/issues/3091
这是一个用户提出需求的issue，主要涉及对VLLM添加AQLM支持。原因可能是用户希望在项目中使用AQLM，并希望VLLM能够支持该功能。

https://github.com/vllm-project/vllm/issues/3090
这是一个用户提出需求的issue，主要涉及对象是VLLM库的模型加载功能。这个issue由于用户希望扩展VLLM库支持从S3位置加载模型而非本地路径而产生。

https://github.com/vllm-project/vllm/issues/3089
这是一个用户提出需求的issue，主要涉及支持starcoder2架构，用户希望检查代码是否存在bug。

https://github.com/vllm-project/vllm/issues/3088
这个issue是关于使用`accelerate.split_between_processes`和`vllm`结合的问题，需要解决在分布式推理时指定特定GPU的需求。主要涉及对象是分布式推理和GPU选择。由于`accelerate`初始化了世界大小为4的分布式模式，导致在小模型下不需要pp或tp时可能会出现错误。

https://github.com/vllm-project/vllm/issues/3087
这是一个关于bug报告的issue，主要涉及的对象是vLLM中的`logits_processors`，由于升级到v0.3.2导致生成速度变慢、内存占用增加并出现OOM crashes。

https://github.com/vllm-project/vllm/issues/3075
这个issue是一个用户提出需求的类型，主要涉及的对象是支持新架构StarCoder2，发起者询问是否有专家支持该新架构，因为他们想要为vLLM实现这个功能。

https://github.com/vllm-project/vllm/issues/3074
这是一个用户提出需求的类型，此问题涉及添加dstack上的另一个服务选项。

https://github.com/vllm-project/vllm/issues/3073
这个issue属于bug报告，主要涉及的对象是vLLM针对Google Gemma模型在AMD MI 300X GPUs上的服务失败。这个问题可能是由于兼容性或配置问题导致的。

https://github.com/vllm-project/vllm/issues/3071
这是一个bug报告，主要对象是vllm软件，问题是由于collective operation超时导致的错误。

https://github.com/vllm-project/vllm/issues/3070
这是一个bug报告，主要涉及到vllm项目中使用的prometheus_client库版本限制问题，导致无法正确导入模块而出现错误。

https://github.com/vllm-project/vllm/issues/3069
这是一个bug报告，主要涉及NCCL在使用两张卡时出现错误的问题。错误提示是NCCL无法在图中捕获，可能是由于CUDA运行时版本或CUDA驱动版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/3068
这是一个bug报告，主要涉及在HPC集群上运行OpenAI兼容服务器时出现的问题，由于Apptainer环境中缺少vllm的安装导致无法正常运行。

https://github.com/vllm-project/vllm/issues/3067
这是一个bug报告，涉及的主要对象是vllm无法在ROCm上成功从源代码构建，可能是由于软件版本不兼容或安装步骤有问题导致的。

https://github.com/vllm-project/vllm/issues/3066
这是一个关于bug报告的issue，涉及到vLLM的内存错误处理方式问题，由于OOM错误没有正确处理导致前一次请求未能中止，影响了下一次的调用。

https://github.com/vllm-project/vllm/issues/3065
这是一个功能改进的issue，主要涉及的对象是vllm下的LLMEngine，通过将logprob detokenization逻辑迁移到LLMEngine来实现目的。

https://github.com/vllm-project/vllm/issues/3064
这是一个文档修正的issue，涉及主要对象是openai兼容服务器的文档。

https://github.com/vllm-project/vllm/issues/3063
这是一个Bug报告，主要对象是vllm模型。由于设置特定范围的温度导致vllm生成只含有空白字符的输出，这可能是vllm的问题而不是特定模型的问题。

https://github.com/vllm-project/vllm/issues/3062
这是一个关于减少Docker镜像中漏洞的建议类型的问题单，主要对象是关于vLLM项目的Docker镜像，问题源于依赖完整的Ubuntu基础镜像导致镜像中存在大量漏洞，用户希望找到/创建一个不依赖Ubuntu且可支持CUDA的镜像或通过其他方式减少漏洞。

https://github.com/vllm-project/vllm/issues/3061
这是一个bug报告，主要涉及到通过源代码构建VLLM并在运行推断时出现模块导入错误的问题，可能是由于构建VLLM时的缺失或版本不兼容导致。

https://github.com/vllm-project/vllm/issues/3060
这个issue类型是用户提出需求，主要涉及添加llmperf benchmark到现有的benchmarks中，原因是llmperf benchmark在blogs中越来越受欢迎，提出者认为应该将其加入。

https://github.com/vllm-project/vllm/issues/3059
这是一个用户提出需求的问题，主要涉及的对象是使用vllm部署的openai服务器。用户想知道是否可以在服务推理结束之前发送终止信号，以节省GPU资源。

https://github.com/vllm-project/vllm/issues/3058
这个issue属于用户提出需求类型，主要涉及请求优先级机制的缺失。这个问题可能是由于没有找到优先级队列来处理请求导致用户无法获得对一些请求更快的响应。

https://github.com/vllm-project/vllm/issues/3057
这是一个bug报告，涉及主要对象是vllm项目下的mutilgpu测试，由于参数设置错误导致了TypeError错误。

https://github.com/vllm-project/vllm/issues/3055
这是一个关于效率问题的bug报告，单涉及主要对象是VLLM的batching机制。这个问题可能是由于某些原因导致在batch中第一个sequence生成response耗时较长。

https://github.com/vllm-project/vllm/issues/3054
该issue是一个用户提出需求类型的问题，主要涉及的对象是qwen2。由于缺乏对Lora的支持而导致用户提交该需求。

https://github.com/vllm-project/vllm/issues/3053
这是一个bug报告，涉及到 vllm 库的代码。由于某些原因导致了Segmentation fault错误，需要进一步排查代码中的问题。

https://github.com/vllm-project/vllm/issues/3052
这个issue类型是询问需求或者功能支持，涉及主要对象是VLLM framework是否可以支持华为的910B芯片，用户可能由于希望使用华为的910B芯片而询问VLLM framework是否能够支持。

https://github.com/vllm-project/vllm/issues/3051
这是一个bug报告，主要涉及的对象是vllm容器。由于参数配置错误导致初始化LLM引擎时的错误，可能导致无法正确运行qwen1.5-14b-chat。

https://github.com/vllm-project/vllm/issues/3050
这是一个用户提出需求的issue，主要涉及对象为Gemma，由于目前不支持LoRA，用户希望添加LoRA支持。

https://github.com/vllm-project/vllm/issues/3049
这是一个关于bug报告的issue，主要涉及VLLM推理程序在升级版本后使用Gemini7Bit时出现CUDA内存溢出错误。

https://github.com/vllm-project/vllm/issues/3048
这是一个bug报告，主要涉及到OpenAI的API服务器在每10秒打印大量不必要的日志，用户想要知道如何关闭这些日志输出。

https://github.com/vllm-project/vllm/issues/3047
这是一个关于合并Gemma模型到Llama模型架构的问题，涉及主要对象是模型架构。由于Gemma与Llama在多个方面存在差异，提出了是否正确决定合并两个模型的疑问。

https://github.com/vllm-project/vllm/issues/3046
这是一个bug报告，主要涉及到代码中的函数名称问题。由于函数名不一致，导致了潜在的代码逻辑错误或混淆问题。

https://github.com/vllm-project/vllm/issues/3045
这是一个bug报告，主要涉及VLLM项目中的类型注释问题。可能由于类型注释错误导致了一些功能异常或错误。

https://github.com/vllm-project/vllm/issues/3044
这是一个bug报告，主要涉及对象为Gemmas的LoRA适配器，用户遇到了无法加载LoRA适配器并出现错误的问题。

https://github.com/vllm-project/vllm/issues/3043
这是一个建议性issue，主要涉及的对象是`gather_cached_kv` kernel，由于该kernel目前未被使用，提议移除。

https://github.com/vllm-project/vllm/issues/3042
这个issue类型是用户提出需求，主要对象是vLLM的视觉语言模型支持。由于需求增加了视觉语言支持，需要进行API更改以支持视觉输入。

https://github.com/vllm-project/vllm/issues/3041
这是一个bug报告，涉及主要对象为使用mistralai/Mixtral-8x7B-v0.1进行模型服务的过程。由于可能使用了不适当的chat template导致404错误和无意义的响应。

https://github.com/vllm-project/vllm/issues/3040
这是一个bug报告，涉及对象是AMD consumer GPU，由于FlashAttention仅支持AMD MI200 GPU或更新型号，因此导致用户在RX 7900 XTX上运行vllm时出现错误。

https://github.com/vllm-project/vllm/issues/3039
这个issue属于技术改进类型，主要涉及到移除未使用的配置文件，并使用`transformers.PretrainedConfig`代替。可能由于需要优化代码结构或者减少不必要的文件而提出。

https://github.com/vllm-project/vllm/issues/3038
这是一个bug报告，主要对象是稳定性AI的Transformers实现，用户提出了一个关于稳定性AI上传的稳定LM的问题。

https://github.com/vllm-project/vllm/issues/3037
这个issue是关于bug报告，主要涉及vLLM在多节点推理时使用CuPy出现问题，可能导致vLLM在Ray集群上初始化时挂起或模型加载变慢。

https://github.com/vllm-project/vllm/issues/3036
这是一个Bug报告，主要涉及对象是Gem7bit模型的tokenizer下载问题。由于无法加载'tokenizer for 'google/gemma7bit''，可能是由于文件名冲突或路径不正确导致的。

https://github.com/vllm-project/vllm/issues/3035
这是一个用户提出需求的issue，主要涉及vllm在k8s pod中加载模型时无法指定最大缓存内存使用，导致内存占用问题。

https://github.com/vllm-project/vllm/issues/3034
这是一个bug报告，主要涉及的对象是在部署qwen1.57BChat过程中调用API时遇到10个字符缺失的问题。由于在调用api时include_stop_str_in_output默认为False，在llm_engine.py中stop token被截断，导致出现字符缺失的问题。

https://github.com/vllm-project/vllm/issues/3033
这是一个bug报告，该问题涉及VLLM中的部署过程。由于ndarray对象缺少'_torch_dtype'属性，导致出现AttributeError错误。

https://github.com/vllm-project/vllm/issues/3032
这是一个bug报告，主要涉及的对象是vllm中的`sampler.py`文件。由于prompt logprobs导致的张量传播问题，用户无法获得预期的输出，并遇到了 size mismatch 错误。

https://github.com/vllm-project/vllm/issues/3031
这是一个用户提出需求的类型。该问题单主要涉及的对象是将hf_transfer添加到requirements.txt中。用户提出需求是为了能够通过设置'HF_HUB_ENABLE_HF_TRANSFER=True'来大幅提高从HF下载权重文件的速度。

https://github.com/vllm-project/vllm/issues/3030
这是一个bug报告，主要涉及torch.profiler.profile()在vllm中出现大量空白时间的问题，导致在运行步骤之前存在大量空白。根据用户的比较，vllm的性能比tensorrtllm慢，推测这些空白时间导致了性能下降。

https://github.com/vllm-project/vllm/issues/3029
该issue类型为需求提出，主要涉及的对象是在vLLM中引入了一种名为speculative decoding的新特性。由于开发者希望实现该特性并对现有的尝试进行比较，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/3028
这是一个bug报告，涉及vllm中使用`--kv-cache-dtype fp8_e5m2`导致错误的问题。

https://github.com/vllm-project/vllm/issues/3027
该issue类型为功能需求，主要涉及对象是OpenAI API的completions/chat completions，用户提出了关于实现logit bias的需求。

https://github.com/vllm-project/vllm/issues/3026
这是一个bug报告，主要涉及的对象是 v0.3.2 下在使用 stop ID 和 stop words 时，有时候响应内容会在最后一个 token 处被截断。

https://github.com/vllm-project/vllm/issues/3025
这个issue是关于要实现新的内核以提高解码速度，属于性能优化类型，主要涉及到解码过程中的内核实现。

https://github.com/vllm-project/vllm/issues/3024
这是一个bug报告，主要涉及AsyncLLMEngine无法在生成完成时停止迭代，导致期望的输出与实际输出不符。

https://github.com/vllm-project/vllm/issues/3023
这是关于bug报告，主要涉及到vLLM中的logits processor，由于`prompt_logprobs`设置为非None值时，导致logits计算错误引发了AssertionError。

https://github.com/vllm-project/vllm/issues/3022
这个issue属于未填写内容的类型，主要涉及到vllm项目。未提供具体问题或需求。

https://github.com/vllm-project/vllm/issues/3021
这是一个用户请求帮助的issue， 主要涉及multi round decoding问题。由于需要在连续的多个步骤中保留之前的信息，用户询问如何在代码中实现使用所有之前的信息。

https://github.com/vllm-project/vllm/issues/3020
这是一个用户提出需求的问题，主要涉及vllm下的alltoall通信集合的实现方式，由于用户可能需要在vllm中实现alltoall通信集合，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/3019
这是一个关于CUDA GPU不可用的bug报告，涉及主要对象是vllm在Kubernetes上运行时遇到的问题。这可能是由于设备配置或环境设置不正确导致的问题。

https://github.com/vllm-project/vllm/issues/3018
这是一个bug报告，主要涉及的对象是OpenAI-compatible server。由于一个小错误导致了在`add_generation_prompt`为`False`时触发`AttributeError`，并提到了一个类型定义上的疑惑。

https://github.com/vllm-project/vllm/issues/3017
这个issue是一个改进建议，主要涉及到输出响应类的优化。

https://github.com/vllm-project/vllm/issues/3016
这个issue是关于bug报告，涉及的主要对象是Yi34BChat模型。由于在使用vllm==0.3.2版本进行推断时，输出的文本总是被截断，这是由于未正确检查停止标记后截断序列引起的。

https://github.com/vllm-project/vllm/issues/3015
这是一个需求类型的issue，涉及的主要对象为Engine。原因是为生产用例添加健康检查以检测引擎故障，并使异步引擎更加健壮。

https://github.com/vllm-project/vllm/issues/3014
这是一个功能需求类型的issue，主要涉及vLLM中的torch.compile()支持，作者希望通过此功能对模型进行编译以提高性能。

https://github.com/vllm-project/vllm/issues/3013
这个issue是关于模型生成中的大长度方差问题，属于性能优化类型的问题，主要涉及的对象是VLLM模型的生成结果。原因可能是VLLM在生成过程中展示了比较高的平均长度差异，用户希望找出导致这种情况的原因。

https://github.com/vllm-project/vllm/issues/3012
这个issue是一个用户需求报告，主要涉及到GPU使用的指定问题。用户在VLLM代码中无法明确指定不同模型使用的GPU，导致无法实现不同模型在不同GPU上的运行。

https://github.com/vllm-project/vllm/issues/3011
这个issue是一个用户提出需求的类型，主要对象是GPTBigCodeForCausalLM，用户希望添加LoRA支持，因为他们已经为每种语言微调了适配器。

https://github.com/vllm-project/vllm/issues/3010
这是一个关于性能优化的issue，主要涉及到flashattention在v2.5.0版本中引入的新特性及与xformers相比的性能表现差异。原因是flashattention的kernel在某些情况下表现优于vllm的paged attention kernels。

https://github.com/vllm-project/vllm/issues/3009
该issue属于功能改进类型，主要涉及到对激活层进行性能优化，使用者提出了关于激活函数的比较和优化建议。

https://github.com/vllm-project/vllm/issues/3008
这是一个用户提出的需求类型的issue，主要涉及的对象是Docker openai模板。用户提出了添加pip安装hf_transfer到Docker模板的建议，以提高权重下载速度的需求。

https://github.com/vllm-project/vllm/issues/3007
这个issue类型为功能需求，主要涉及的对象是VLLM项目中的prefix prefill kernels模块。由于用户需要支持GQA数据集，因此提出了使prefix prefill kernels模块支持GQA的需求。

https://github.com/vllm-project/vllm/issues/3006
这是一个功能需求的issue，主要涉及的对象是添加代码用于令牌长度测试和延迟测试。

https://github.com/vllm-project/vllm/issues/3005
这个issue类型为功能增强提案，主要涉及attention层的重构和性能改进，通过分离处理Ampere或更新的NVIDIA GPU与其他GPU的代码路径，简化了前者的代码，同时提升ALiBi模型的性能。

https://github.com/vllm-project/vllm/issues/3004
这是一个bug报告类型的issue，涉及对象是使用openai API server和GPU进行推理时遇到问题。由于日志显示GPU利用率满，但未收到任何结果，可能是由于硬盘问题导致推理卡住。

https://github.com/vllm-project/vllm/issues/3003
这是一个关于代码逻辑错误的bug报告，主要涉及的对象是内存高效注意力前向函数memory_efficient_attention_forward。由于输入张量格式与函数要求的格式不一致，导致拼接批处理时产生结果不一致的问题。

https://github.com/vllm-project/vllm/issues/3002
这是一个bug报告，涉及主要对象是在Nvidia Jetson设备上安装vllm时出错。由于缺少NumPy模块和CUDA runtime，导致安装过程失败。

https://github.com/vllm-project/vllm/issues/3001
这个issue类型为用户提出需求，主要涉及对象是如何在离线情况下使用lora进行批量推理，用户可能因为需要在无网络环境下对数据进行批量推理而提出了这个问题。

https://github.com/vllm-project/vllm/issues/3000
这是一个用户提出的问题，涉及主要对象是Lora Adapter在处理具有Vocab大小为40960的模型时出现错误，可能是由于该方法不适用于扩展词汇或拥有超过32000个单词大小的模型。

https://github.com/vllm-project/vllm/issues/2999
该issue类型为用户提出需求，主要涉及对象是OpenAI api的文档。这导致了用户感觉文档中关于如何批量处理数据的信息不足。

https://github.com/vllm-project/vllm/issues/2998
这是一个用户提出需求的类型的issue，主要涉及的对象是TRT-LLM backend。原因可能是用户希望在TRTLLM backend中增加benchmark支持。

https://github.com/vllm-project/vllm/issues/2997
这是一个bug报告，主要涉及到C编译器的找不到问题，导致生成API调用时出现错误。

https://github.com/vllm-project/vllm/issues/2996
这是一个Bug报告，涉及到LLMEngine文档在readthedocs上无法正确渲染的问题。可能是由于readthedocs版本与本地版本不一致导致的显示错误。

https://github.com/vllm-project/vllm/issues/2995
这个issue是一个bug报告，涉及的主要对象是OpenAI Completions API。由于最近对入口点进行的修改，导致之前支持的两种用例无法正常操作，主要症状是无法在生成时回显原始内容或在不输出logprobs的情况下回显原始内容。

https://github.com/vllm-project/vllm/issues/2994
这个issue是一个待处理中的bug报告，主要涉及Sparse Integration Test，由于Marlin测试存在问题，导致该测试无法进行。

https://github.com/vllm-project/vllm/issues/2993
这是一个bug报告，涉及主要对象为vLLM benchmarking script。问题由于不支持openAI兼容的API服务器导致无法使用"/generate"端点进行验证。

https://github.com/vllm-project/vllm/issues/2992
这是一个bug报告，主要涉及的对象是openai chat completion API的benchmarking脚本。由于以前的版本无法使用流式API追加文本，并且不支持`/v1/chat/completions` API，所以会出现问题。

https://github.com/vllm-project/vllm/issues/2990
这是一个版本更新请求的issue，涉及到主要对象的更新至0.3.2版本。

https://github.com/vllm-project/vllm/issues/2988
这是一个bug报告类型的issue，主要涉及的对象是vllm项目中的AMD Instinct MI300 X GPUs，由于设置了超过1的张量并行度导致vLLM在处理concurrent requests时出现错误。

https://github.com/vllm-project/vllm/issues/2987
这个issue是一个功能需求，涉及到修改metrics名称中的model_name字段，以方便grafana管理员管理。原因是model_name包含目录字符，不便于管理。

https://github.com/vllm-project/vllm/issues/2986
这是一个bug报告，主要涉及到VLLM中的Cupy模块。由于Cupy在VLLM0.3.1版本中出现问题，导致了某些具体错误或功能无法正常运行。

https://github.com/vllm-project/vllm/issues/2985
这是一个bug报告，涉及到VLLM模型在中文语言下无法输出内容的问题。可能由于模型参数不正确或者数据预处理出现问题导致。

https://github.com/vllm-project/vllm/issues/2984
这是一个关于bug报告的issue，主要涉及Mistral + YaRN模型，由于当前断言不成立而引发问题。

https://github.com/vllm-project/vllm/issues/2983
这是一个bug报告类型的issue，主要涉及到YaRN模型。此问题发生的原因是函数_yarn_linear_ramp_mask()缺少了一个参数'device'，导致YaRN模型无法正常工作。

https://github.com/vllm-project/vllm/issues/2982
这是一个bug报告，主要涉及到项目中的测试环境以及依赖项中的`flash_attn`，导致偶发性测试失败。

https://github.com/vllm-project/vllm/issues/2981
这个issue是一个bug报告，主要对象是vllm中的 qwen-14b 微调后的模型。由于某种原因，当使用vllm进行推理时，结果始终为空，用户寻求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/2980
这是一个关于性能问题的bug报告，主要涉及到使用AWQ量化时性能低于未量化模型的情况。

https://github.com/vllm-project/vllm/issues/2979
这是一个优化型issue，主要涉及替代优化了fused MoE Kernel，由于更积极的参数搜索，实现了胜过TensorRT kernels的表现，用户提出了性能优化的需求。

https://github.com/vllm-project/vllm/issues/2978
这个issue属于文档新增类型，主要涉及vLLM paged attention kernel的实现文档，由于新手可能对该实现复杂度较高，作者提供了详细解释与示例以帮助理解。

https://github.com/vllm-project/vllm/issues/2977
这是一个Bug报告，主要涉及到VLLM中的LoRA模型不支持当前的操作，导致数值错误。

https://github.com/vllm-project/vllm/issues/2976
这是一个功能需求，主要涉及到在文本生成过程中识别停止条件的问题。由于当前无法确定文本生成停止的具体原因，用户提出添加`stop_reason`字段以指示匹配的停止标识符。

https://github.com/vllm-project/vllm/issues/2975
该issue是一个优化性质的问题，涉及主要对象是Gemini中的GeGLU层。

https://github.com/vllm-project/vllm/issues/2974
这是一个优化建议的issue，主要涉及Gemam的RMSNorm优化。

https://github.com/vllm-project/vllm/issues/2973
这个issue是关于bug报告，主要涉及VLLM项目的安装错误，由于变量“num_layers”在cache_kernels.cu中无法作为常量使用，导致编译时出现错误。

https://github.com/vllm-project/vllm/issues/2972
这是一个Bug报告，主要涉及的对象是GitHub上的vLLM项目中的Dockerfile.rocm文件。由于Dockerfile.rocm中未更新相关支持信息，导致在MI100系统上构建时无法兼容新的gfx908架构，从而造成FlashAttention构建失败。

https://github.com/vllm-project/vllm/issues/2971
这个issue属于bug报告类型，主要涉及的对象是服务器推理速度。导致此问题的原因可能是最近升级到0.3.2版本后，服务器推理速度明显下降。

https://github.com/vllm-project/vllm/issues/2970
这个issue属于bug报告类型，主要涉及的对象是vllm项目中依赖torch版本的问题，由于torch<2.2导致GCP/A3无法使用vllm。

https://github.com/vllm-project/vllm/issues/2969
这个issue类型为用户提出需求，关注点是vLLM是否利用了transformers中关于静态缓存和torch编译的新速度优化。

https://github.com/vllm-project/vllm/issues/2968
这是一个需求类型的issue，主要涉及版本更新和新增模型支持的功能性改进。

https://github.com/vllm-project/vllm/issues/2967
这是一个升级 issue， 主要对象是 ROCm 中的 transformers 模块。

https://github.com/vllm-project/vllm/issues/2966
这个issue类型为需求添加，主要涉及的对象是Gemini模型文档。该需求是为了将Gemma模型添加到文档中，因此是为了完善文档内容而提出的。

https://github.com/vllm-project/vllm/issues/2965
该issue为升级transformers到v4.38.0版本的操作请求，涉及的主要对象是transformers库。

https://github.com/vllm-project/vllm/issues/2964
这是一个特性添加的问题，涉及Gemmma模型的实现。

https://github.com/vllm-project/vllm/issues/2963
这是一个Bug报告类型的Issue，主要涉及到vllm库中的ReduceOpConversion功能。这个问题发生的原因可能是与输出维度和数据布局相关的问题。

https://github.com/vllm-project/vllm/issues/2962
这是一个关于需求的issue，主要涉及追踪生产环境的分支，用户可能由于需要跟踪生产环境的变化而提出这个问题。

https://github.com/vllm-project/vllm/issues/2961
这是一个关于bug报告的issue，主要涉及vLLM在容器内部没有网络访问时出现的连接错误。该问题可能由于缺乏网络访问而造成的`OSError`影响了`getsockname`返回的IP。

https://github.com/vllm-project/vllm/issues/2960
该issue属于用户提出需求的类型，主要涉及的对象是VLLM项目中的模型支持扩展，由于Gem模型架构的不同而需要对其进行适配。

https://github.com/vllm-project/vllm/issues/2959
这是一个Bug报告类型的Issue，主要涉及到VLLM和Mixtral 8x7b模型加载速度变慢的问题。可能是由于VLLM版本升级到0.3.1导致模型加载过程变得异常缓慢。

https://github.com/vllm-project/vllm/issues/2958
这是一个bug报告，主要涉及的对象是Ruff linter warning。由于使用`lstrip()`可能引入潜在的bug，用户提出应该使用`removeprefix()`修复此问题。

https://github.com/vllm-project/vllm/issues/2957
这是一个 bug 报告，主要涉及的对象是 vllm 的 qwen7B/qwen14B 大模型加速器。这个问题可能是由于 vllm 0.3.0 版本加速器与单轮问答测试模型和流式输出测试模型的准确性问题导致的。

https://github.com/vllm-project/vllm/issues/2956
这个issue是一个bug报告，主要涉及vllm加速的qwen14B模型推理结果准确性下降的问题，可能是由于vllm加速导致的推理结果与原始qwen推理结果不一致而造成。

https://github.com/vllm-project/vllm/issues/2955
这个issue类型是bug报告，主要涉及的对象是vllm的qwen模型。由于推理结果与原始结果不一致，导致了明显的准确率下降。

https://github.com/vllm-project/vllm/issues/2954
这是一个bug报告，涉及到vllm在Qwen流式推断中导致的准确率显著下降的问题，可能由于流式细胞术结果与原来的Qwen结果不一致而导致。

https://github.com/vllm-project/vllm/issues/2953
这是一个bug报告，主要涉及VLLM的推理结果准确率问题，原因是基于VLLM qwen7B的结果与原始答案存在显著差异。

https://github.com/vllm-project/vllm/issues/2952
这是一个bug报告，主要涉及的对象是Vllm加速对QWen系列模型的影响。导致该问题的原因是在使用Vllm加速后，QWen系列模型的答案准确性显著下降。

https://github.com/vllm-project/vllm/issues/2951
这是一个关于实验结果准确性下降的bug报告，主要涉及的对象是实验中的qwen14B。由于实验结果与原始结果存在显著差异，需要解决这个问题。

https://github.com/vllm-project/vllm/issues/2950
这是一个bug报告，主要涉及到使用vllm qwen14B进行推断时与原始qwen结果不一致，并导致显著降低准确性。

https://github.com/vllm-project/vllm/issues/2949
这是一个关于bug报告的issue，涉及主要对象为vllm qwen7B，产生症状的原因可能是模型不一致导致结果准确性显著下降。

https://github.com/vllm-project/vllm/issues/2948
这是关于AWQ Quantization Memory Usage的bug报告，涉及到vllm的推理引擎。由于AWQ量化版本和未量化版本占用GPU内存相似，用户在询问为何会出现这一情况。

https://github.com/vllm-project/vllm/issues/2947
这是一个bug报告，主要涉及的对象是AsyncLLMEngine。这个问题是由于输出中的token_ids和text内容不一致导致的。

https://github.com/vllm-project/vllm/issues/2946
这是一个bug报告，主要涉及unittest测试过程中初始化LLM实例时使用小的max_num_seqs参数导致测试失败的问题。

https://github.com/vllm-project/vllm/issues/2945
这个issue属于bug报告，主要对象是Yarn-Mistral模型。原因是由于缺少了一个参数'device'，导致了函数_yarn_linear_ramp_mask()报错。

https://github.com/vllm-project/vllm/issues/2943
这是一篇讨论，指向了一个相关的研究论文，而不是一个bug报告或需求。

https://github.com/vllm-project/vllm/issues/2942
这是一个bug报告，该问题涉及的主要对象是vLLM的多GPU支持失败与AMD MI210。由于xFormers无法加载C++/CUDA扩展，可能是由于版本不匹配导致的。

https://github.com/vllm-project/vllm/issues/2940
这是一个bug报告，涉及主要对象是vllm中的benchmarking script。由于benchmarking script中的断言不匹配，导致在使用openai chat apis时失败。

https://github.com/vllm-project/vllm/issues/2939
这是一个bug报告，涉及的主要对象是`early_stopping`参数在completion API中无法正常工作。这个问题可能是由于缺少对`early_stopping`的实现导致的。

https://github.com/vllm-project/vllm/issues/2938
这是一个关于bug的报告，主要涉及到`v0.3.1`版本中的early stopping设置问题，由于协议定义中的遗漏导致early stopping未能正确切换到`True`状态。

https://github.com/vllm-project/vllm/issues/2937
这是一个bug报告，主要涉及到因为温度过低导致生成文本为空的问题。

https://github.com/vllm-project/vllm/issues/2936
这是一个关于功能需求的issue，主要涉及VLLM中的mistral模型，用户提出了关于添加embedding支持的建议。

https://github.com/vllm-project/vllm/issues/2935
该issue为用户提出需求，问题涉及到如何在离线推理时获取指标（Metrics）信息，由于目前无法通过公开API直接获取这些指标信息，用户尝试使用内部方法来获取，但并没有成功。

https://github.com/vllm-project/vllm/issues/2934
这是一个bug报告类型的issue，主要涉及到`benchmark_serving.py`文件中的一个注释更新。根据issue标题和内容描述，用户可能发现了注释中的一处错误或者需要更正的地方，希望进行修改。

https://github.com/vllm-project/vllm/issues/2933
该issue是一个Bug报告，主要涉及的对象是代码中的max_prompt_len变量，由于某些原因导致在部分情况下为0，导致了运行时异常。

https://github.com/vllm-project/vllm/issues/2932
这是一个bug报告，涉及的主要对象是vllm API。这个问题是由于vllm在没有输入时，终端每10秒刷新一次，导致用户询问是否这是特殊设置或者一个bug。

https://github.com/vllm-project/vllm/issues/2931
这个issue是bug报告，涉及的主要对象是`get_ip`函数。通过纯IPv6环境导致的bug，可能是因为该函数在纯IPv6环境下无法正确获取IP地址而导致的错误。

https://github.com/vllm-project/vllm/issues/2930
这是一个bug报告，主要涉及到vllm项目中的beam search测试。导致bug的原因是CC组中一个关于`vllm:prompt_tokens_total`指标计算的变更，这导致beam search测试失败。

https://github.com/vllm-project/vllm/issues/2929
这是一个性能问题的issue，涉及的主要对象是vllm库中的awq_dequant模块。由于直接调用cuBLAS导致速度比使用torch.matmul慢，需要分析原因。

https://github.com/vllm-project/vllm/issues/2928
这是一个bug报告，主要涉及的对象是vllm中的awq gemm模块。由于移除了cuda stream，导致了最多2.5倍的速度提升。

https://github.com/vllm-project/vllm/issues/2927
这个issue是一个优化需求，涉及的主要对象是vllm的部署效果与原始数据之间的一致性。由于vllm部署的结果与原始数据不一致，用户希望尽快优化qwen14B的集成效果。

https://github.com/vllm-project/vllm/issues/2926
这个issue是用户提出的需求，主要涉及到VLLM模型中的stop words处理方式。由于当前只支持单个stop words，用户希望支持多个stop words，并提出了具体的更新请求。

https://github.com/vllm-project/vllm/issues/2925
这是一个关于功能需求的issue，主要涉及到VLLM模型在处理多次输入时内存残留的问题，用户希望了解如何清除模型的缓存或历史记录。

https://github.com/vllm-project/vllm/issues/2924
这个issue类型是用户提出需求，主要涉及的对象是如何指定本地模型，由于用户不清楚如何使用命令下载本地模型而询问如何指定本地模型。

https://github.com/vllm-project/vllm/issues/2923
这个issue是用户提出的需求，主要涉及VLLM引擎的初始化方法，通过添加`tokenizer_init_kwargs`参数来改进。

https://github.com/vllm-project/vllm/issues/2922
这是一个标题为"."的issue，类型为bug报告，主要涉及的对象是该Github仓库中的某个功能或模块。由于该标题为空，导致无法准确描述问题或需求，可能是用户未按要求填写相关信息或系统存在bug。

https://github.com/vllm-project/vllm/issues/2921
这是一个bug报告，主要涉及使用vllm v0.2.4和qwen14bchat模型时遇到的未知ID相关错误。

https://github.com/vllm-project/vllm/issues/2920
这个issue是关于功能请求的，主要对象是VLLM，用户提出了集成QUICK核心进行AWQ量化的需求。

https://github.com/vllm-project/vllm/issues/2919
这是一个bug报告，主要对象是vllm模型中的chatglm2 response错误，由于特殊标记存在于返回的响应中，导致在某些情况下特殊标记出现在句子开头。

https://github.com/vllm-project/vllm/issues/2918
这是一个bug报告，主要对象是在OpenAI的Chat Completions中添加LogProbs选项。该问题由于之前API服务器不支持`logprobs`对`ChatCompletionRequest`的请求而引起。

https://github.com/vllm-project/vllm/issues/2917
这是一个用户提出需求的issue，主要涉及支持在vLLM上使用Smaug-72B-v0.1模型。

https://github.com/vllm-project/vllm/issues/2916
这是一个用户提出需求的issue，主要涉及如何在VLLM中动态调用/添加新的Lora模块到live server。用户想知道是否可以在不重启服务器的情况下实时调用最近添加的Lora模块。

https://github.com/vllm-project/vllm/issues/2915
这是一个关于软件bug的报告，主要涉及的对象是API服务器。由于缺少`loramodules`模块，导致无法成功运行API服务器。

https://github.com/vllm-project/vllm/issues/2914
这个issue是请求将标签0.3.1添加到docker镜像vllm/vllmopenai的需求类型。

https://github.com/vllm-project/vllm/issues/2913
这是一个优化型问题，涉及的主要对象是MoE（Mixture of Experts）内核。原因是由于TensorRT MoE内核在小批量大小范围内表现良好，而融合 MoE 内核在大批量大小范围内表现更好，因此希望能够创建一个统一的内核以覆盖所有情况。

https://github.com/vllm-project/vllm/issues/2912
这是一个bug报告，主要涉及vllm在使用djl-deepspeed时出现hang的问题。

https://github.com/vllm-project/vllm/issues/2911
这是一个用户提出的功能请求，主要涉及LangChain集成LoRA支持。用户希望在GPU内存不足的情况下利用vLLM，并提到了QLoRA的集成。

https://github.com/vllm-project/vllm/issues/2910
这是一个用户提出需求的issue，主要涉及vLLM模型生成结果的确定性选项。用户想要知道是否有一个选项或标志可以确保在相同的提示下，不同运行的结果是相同的，而不是随机的。

https://github.com/vllm-project/vllm/issues/2909
这是一个bug报告，主要涉及的对象是vllm的api_server.py文件，由于命令行参数传递错误导致出现unrecognized arguments错误。

https://github.com/vllm-project/vllm/issues/2908
这是一个功能需求类型的Issue，主要涉及测试新增功能的需求。由于缺乏基础正确性测试，需要添加一个简单的测试以确保主干通过基本正确性测试。

https://github.com/vllm-project/vllm/issues/2907
这是一个用户提出需求的issue，主要涉及的对象是增强`from huggingface_hub import snapshot_download`的功能。

https://github.com/vllm-project/vllm/issues/2906
这是一个bug报告类型的issue，主要涉及到vLLM在GCP上使用tensor-parallel-size 2加载失败。原因可能是由于vLLM在此设置下存在的问题导致。

https://github.com/vllm-project/vllm/issues/2905
这个issue类型是代码优化类型，涉及vllm下的worker初始化逻辑，用户提出需要让分布式初始化逻辑看起来更清晰。

https://github.com/vllm-project/vllm/issues/2904
这个issue类型是bug报告，涉及的主要对象是vllm的代码中的函数`_yarn_linear_ramp_mask`。由于在调用函数`_yarn_linear_ramp_mask`时缺少了参数`device`，导致了缺少必要参数的错误。

https://github.com/vllm-project/vllm/issues/2903
这是一个用户提出需求的类型，主要涉及的对象是VLLM项目。由于用户需要支持千问的图片对话模型，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/2902
这是一个关于需求的问题，用户想了解vl模型是否支持lora后的模型以及是否可以支持部署。

https://github.com/vllm-project/vllm/issues/2901
这是一个用户提出需求的问题，主要涉及vLLM对请求进行排队时的控制机制，用户想要了解如何控制当队列过长时vLLM是否会拒绝请求。

https://github.com/vllm-project/vllm/issues/2900
这是一个安装问题，主要涉及的对象是软件vllm，原因可能是与AMD Radeon 7900XTX GPU相关的安装失败。

https://github.com/vllm-project/vllm/issues/2899
这是一个bug报告，涉及到vllm tokenizer在生成token ids时出现了重复的 `<s>` token，导致不符合预期的tokenized prompt ids。

https://github.com/vllm-project/vllm/issues/2898
这是一个用户提出需求的issue，主要涉及到将ray作为单节点部署的可选项，提出了对于实时同步推理需求的轻量级选项需求。

https://github.com/vllm-project/vllm/issues/2897
这是一个关于功能需求的问题，主要涉及VLLM模型中迭代prompt tokens的过程。用户不清楚模型是如何从输入序列生成tokens，可能是因为现有函数主要处理已生成的输出导致。

https://github.com/vllm-project/vllm/issues/2896
这是一个用户提出需求的issue，主要涉及到对于稀疏层添加偏置支持的改进请求。

https://github.com/vllm-project/vllm/issues/2895
这个issue类型是功能需求，主要涉及添加docker-compose.yml文件和对应的.env文件。由于vLLM缺少Composefile，用户提供了一个可用的基础版本以供使用和进一步开发。

https://github.com/vllm-project/vllm/issues/2894
这个issue属于用户提出需求类型，主要涉及模型在多个名称下提供服务，用户希望能够方便地更新模型名称而不影响用户配置。

https://github.com/vllm-project/vllm/issues/2893
这是一个用户提出需求的issue，主要涉及Vllm Lora是否支持SGMV，用户希望社区是否有计划支持SGMV以提升灵活性。

https://github.com/vllm-project/vllm/issues/2892
该issue属于用户提出需求类型，主要涉及Hugging Face Hub中的模型代码下载功能，用户希望新增`coderevision`参数以指定特定版本的模型代码，在之前默认下载最新版本可能导致不一致的情况。

https://github.com/vllm-project/vllm/issues/2891
这是一个bug报告，涉及主要对象为模型的输出结果。由于将tensor_parallel_size从1更改为4导致不同的输出结果，用户想知道为什么在这种情况下会出现不同的情况。

https://github.com/vllm-project/vllm/issues/2890
该issue属于用户提出需求类型，主要涉及的对象是对vLLM集成XLoRA的需求。用户提出了需要在vLLM中实现XLoRA所需的2 KV缓存的功能，由于XLoRA采用双向前向传播方法，因此需要增加此功能。

https://github.com/vllm-project/vllm/issues/2889
这是一个Bug报告，涉及的主要对象是LLM模块，由于LLM入口没有启用LoRa功能而导致问题。

https://github.com/vllm-project/vllm/issues/2888
该issue是一个功能需求的报告，主要涉及AI Controller Interface (AICI)的集成，提出了一些功能实现的支持，并就部分功能的实现方式提出了疑问。

https://github.com/vllm-project/vllm/issues/2887
这是一个关于软件版本更新的issue，主要对象是v0.3.1版本，由于需要发行新版本或修复问题导致的更新。

https://github.com/vllm-project/vllm/issues/2886
这是一个CI（持续集成）相关问题，涉及到vllm在AMD平台下构建的支持。该问题主要由于代码无法在AMD平台上编译而导致相关功能的问题。

https://github.com/vllm-project/vllm/issues/2885
这是一个bug报告，涉及的主要对象是修复 ROCm 下的 Dockerfile 以使 flash-attention 能够成功构建。导致此问题的原因可能是之前的 Dockerfile 配置有错误。

https://github.com/vllm-project/vllm/issues/2884
这是一个bug报告，涉及主要对象为vLLM的版本更新。由于更新到最新版本`vLLM-0.2.7`后出现了问题，可能是由于代码变更导致的错误。

https://github.com/vllm-project/vllm/issues/2883
这是一个bug报告，主要涉及到vllm项目中的DeciLM模型。由于构造函数的更改未在子类DeciLMForCausalLM中反映引起了DeciLM模型的故障，但由于持续集成中测试无法正常运行，导致该问题未被及时发现。

https://github.com/vllm-project/vllm/issues/2882
这是一个bug报告，主要涉及的对象是`LLM` class，由于importing punica kernels raises an exception未被释放导致`LLM` class在执行`gc.collect()`后未被GCed。

https://github.com/vllm-project/vllm/issues/2881
该issue属于bug报告类型，主要涉及对象是`LLMEngine.add_request()`方法中的`SamplingParams`对象，由于传递给该方法的`SamplingParams`对象在方法返回后被修改会影响异步采样过程，导致可能出现问题。

https://github.com/vllm-project/vllm/issues/2880
这是一个bug报告，涉及到Mistral和Mixtral之间LoRA代码的对齐，由于代码不一致导致相关问题和修复。

https://github.com/vllm-project/vllm/issues/2879
这是一个用户提出需求的issue，主要涉及到vLLM中异步标记化的问题。 由于同步标记化导致长提示的标记化阻塞事件循环，造成在高QPS场景下标记生成和请求处理变慢。

https://github.com/vllm-project/vllm/issues/2878
这个issue属于bug报告类型，主要涉及到vLLM中的_init_workers_ray函数以及在ray actor中加载多个vllm模型时所导致的问题。原因是由于手动使用`set_cuda_visible_devices`设置GPU id导致影响了在ray actor中加载多个vllm模型的情况。

https://github.com/vllm-project/vllm/issues/2877
这是一个用户提出的需求，关于让vLLM日志格式化变更可选的问题，主要涉及日志记录功能。这个问题的原因是为了更好地与现有系统集成，允许用户选择是否要应用vLLM的日志格式化更改。

https://github.com/vllm-project/vllm/issues/2876
这是一个功能需求的issue，主要涉及的对象是RequestOutput类。由于缺乏额外的指标，用户希望为RequestOutput添加一些用于第三方日志/指标记录的额外指标。

https://github.com/vllm-project/vllm/issues/2875
这是一个Bug报告，涉及的主要对象是MixtralModel对象。这个问题是由于Mixtral无法启动，因为出现了AttributeError错误，可能是由于缺少org_vocab_size属性导致的。

https://github.com/vllm-project/vllm/issues/2874
这是一个性能优化的issue，主要涉及AWQ gemm kernel的更新和速度提升，用户可能在寻求提升模型性能和验证长序列长度的解决方案。

https://github.com/vllm-project/vllm/issues/2873
这是一个bug报告，主要涉及GQA模型，由于当前的实现与前缀缓存不兼容，导致返回错误结果。

https://github.com/vllm-project/vllm/issues/2872
这是一个用户提出需求的issue，主要涉及的对象是Marlin downstream PR。由于用户希望提交Marlin代码的变更，导致了这个issue的产生。

https://github.com/vllm-project/vllm/issues/2871
这是一个用户提出的需求问题，主要涉及VLLM是否支持HalfQuadratic Quantization (HQQ)。由于部分模型已使用HQQ，因此用户希望VLLM也能支持该功能。

https://github.com/vllm-project/vllm/issues/2870
这是一个bug报告，主要涉及的对象是vllm的使用者。由于用户发送给模型的消息数量大于1时，模型会在每2到3次查询中填充所有剩余的完成标记与新行符“\n”，可能是由于某些参数配置或版本更新导致的问题。

https://github.com/vllm-project/vllm/issues/2869
这是一个bug报告，主要涉及到`vllm:prompt_tokens_total`计数指标的计算问题，因为代码在处理批次中具有不同令牌长度的多个提示时出现错误。

https://github.com/vllm-project/vllm/issues/2868
这是一个重构的issue，涉及从MistralForCausalLM迁移到LlamaForCausalLM。

https://github.com/vllm-project/vllm/issues/2867
该issue是一个代码重构类型的问题，涉及到模型命名的变更。

https://github.com/vllm-project/vllm/issues/2866
这是一个用户的需求报告，主要涉及如何基于huggingface模型类RagTokenForGeneration添加一个模型执行列表中的模型。用户想了解如何在这种情况下添加新模型。

https://github.com/vllm-project/vllm/issues/2865
这是一个bug报告，主要涉及到vLLM Release 0.3.0在AMD Instinct MI300X上安装失败的问题，原因是setup.py不支持MI300系列且Dockerfile.rocm文件中build arguments设置有误。

https://github.com/vllm-project/vllm/issues/2864
这是一个bug报告，主要涉及到vllm中的模型qwen1.5-72B-awq with tp=2，由于使用两个A100 80G，一直无法停止答案。

https://github.com/vllm-project/vllm/issues/2863
这个issue属于bug报告类型，主要涉及GPU内存利用问题。由于多个进程共享GPU，导致当前内存分析逻辑的假设不准确，需要在引擎初始化时考虑其他进程的GPU内存使用情况。

https://github.com/vllm-project/vllm/issues/2862
这是一个需求类型的issue，涉及Sparse fused gemm integration功能。

https://github.com/vllm-project/vllm/issues/2861
这是一个Bug报告，涉及主要对象是vllm项目中的internlm模型。这个问题由于缺少`num_key_value_heads`参数导致了一个错误。

https://github.com/vllm-project/vllm/issues/2860
这是一个功能改进类的issue，涉及的主要对象是将InternLMForCausalLM迁移到LlamaForCausalLM。

https://github.com/vllm-project/vllm/issues/2859
这是一个bug报告，主要涉及的对象是`LLM`类。由于内存释放问题, 导致出现了GC bug。

https://github.com/vllm-project/vllm/issues/2858
这是一个关于文档更改的issue，主要对象是benchmark api server，可能由于文档中仍包含有关该服务器的引用导致混淆。

https://github.com/vllm-project/vllm/issues/2857
这是一个bug报告，涉及的主要对象是Ray workers无法在未连接互联网时启动。这个问题可能是因为需要访问Google Public DNS才能成功启动。

https://github.com/vllm-project/vllm/issues/2856
这是一个bug报告，涉及LLAMA 70B AWQ在AWS G5.48xl机器上OOM的问题，可能由于内存占用过高导致。

https://github.com/vllm-project/vllm/issues/2855
这个issue属于bug报告，主要涉及CuPy在AMD后端上的不兼容性问题，导致需要使用PyTorch NCCL而不是CuPy NCCL来解决。

https://github.com/vllm-project/vllm/issues/2854
这是一个功能改进的issue，涉及的主要对象是替换Yi模型定义为使用`LlamaForCausalLM`，原因是为了避免代码重复和确保Yi模型继承所有为llama做的修复，如LoRA支持。

https://github.com/vllm-project/vllm/issues/2853
该issue类型为功能需求，主要涉及GPU内存达到容量时数据未能自动转移至CPU内存，用户询问了CPU KV cache的触发条件及如何调用CPU KV cache进行数据交换。

https://github.com/vllm-project/vllm/issues/2852
这是一个功能提议的issue，主要涉及到vllm项目中的"Usage Stats Collection"。

https://github.com/vllm-project/vllm/issues/2851
这个issue类型为bug报告，涉及主要对象为vllmproject/vllm的CC(Refactor llama family models)。由于测试失败，需要撤销之前的重构操作。

https://github.com/vllm-project/vllm/issues/2850
这是一个关于缺失 Prometheus 指标的 bug 报告，涉及 vLLM 0.3.0 版本的指标缺失问题。可能是由于更新或配置问题导致了部分指标缺失。

https://github.com/vllm-project/vllm/issues/2849
这个issue类型是用户请教问题，主要涉及如何在使用docker部署的LLM中消费LLM API。询问者希望了解如何在Langchain VLLMOPENAI中使用LLM API。

https://github.com/vllm-project/vllm/issues/2848
这是一个用户提出问题的issue。主要讨论了输入tokens中的prompt_logprobs和输出tokens中的logprobs之间的计算区别。由于计算逻辑的差异，用户可能遇到了与prompt_logprobs和logprobs相关的问题或需要进一步了解这两者之间的区别。

https://github.com/vllm-project/vllm/issues/2847
这个issue是关于bug报告的，主要涉及 LoRA rank 参数超出最大值导致的数值错误。

https://github.com/vllm-project/vllm/issues/2846
这是一个用户提出需求的类型的issue，主要涉及到实现Telemetry Support功能。由于需要收集信息并发送至服务器，可能是为了跟踪使用数据或优化性能。

https://github.com/vllm-project/vllm/issues/2845
这是一个关于bug报告的issue，主要涉及Docker Python版本不一致导致的异常错误。

https://github.com/vllm-project/vllm/issues/2844
这是一个关于改进性能的问题，涉及主要对象是CI系统，由于需要确保模型输出与Huggingface版本完全匹配，因此对测试进行了快照基础的实现。

https://github.com/vllm-project/vllm/issues/2843
这是一个功能需求类型的issue，涉及主要对象是模型参数的打包和解包。

https://github.com/vllm-project/vllm/issues/2842
这个issue类型是功能增强，主要涉及文档构建功能，由于需要确保文档能够在CI中成功构建，因此这个功能被新增。

https://github.com/vllm-project/vllm/issues/2840
这是一个bug报告，主要涉及的对象是Distributed batch inference example代码。由于llm实例不可序列化，导致代码运行时出现错误。

https://github.com/vllm-project/vllm/issues/2839
这是一个空内容的Bug报告，主要涉及Generate功能。由于未提供具体内容，用户无法成功生成所需信息，因此需要修复。

https://github.com/vllm-project/vllm/issues/2838
这是一个升级请求，涉及主要对象是PyTorch库，由于升级导致可能会破坏AMD GPU的支持，需要更新docker文件和xformers patch以修复这一问题。

https://github.com/vllm-project/vllm/issues/2837
这是一个关于bug报告的issue，主要涉及的对象是模型miqu-1-70b-sf-gptq。由于该模型在推理过程中出现了内存溢出问题，用户在询问这个模型是否与其他70b模型有所不同。

https://github.com/vllm-project/vllm/issues/2836
这是一个用户提出需求的issue，主要涉及如何使用模型和tokenizer创建vllm.LLM()对象。问题出现的原因是vllm当前不支持bitsandbytes量化方法。

https://github.com/vllm-project/vllm/issues/2835
该issue属于用户提出需求类型，主要涉及支持Google的TPU硬件。原因可能是用户对VLLM软件在2024Q1的路线图中添加对TPU硬件的支持感兴趣并想了解该问题的最新进展。

https://github.com/vllm-project/vllm/issues/2834
这个issue是用户提出需求的类型，涉及主要对象是文档和LoRA适配器。用户指出LoRA适配器目前不在文档中，因此他们撰写了关于LoRA的文档部分。

https://github.com/vllm-project/vllm/issues/2833
这是一个bug报告，主要涉及Mistral AWQ在使用Mixtral GPTQ with TP=2时未生成任何输出，可能是由于GPU配置2*A6000导致的。

https://github.com/vllm-project/vllm/issues/2832
这是一个功能增强类的issue，主要涉及添加对OLMo模型的支持。

https://github.com/vllm-project/vllm/issues/2831
这是用户提出的需求。该问题单涉及的主要对象是为Mixtral添加LoRA支持。原因是当前的软件版本没有LoRA配置，导致用户无法在Mixtral上使用LoRA功能。

https://github.com/vllm-project/vllm/issues/2830
这是一个关于更换构建系统的PR，主要对象为项目内的构建系统，重写主要是为了通过CMake构建C++/CUDA文件而无需使用python或setup.py，旨在简化使用并保持逻辑一致性。

https://github.com/vllm-project/vllm/issues/2829
这是一个性能问题的讨论，而非bug报告。主要涉及的对象是使用Lora和未使用Lora进行离线批处理推理时的性能差异。

https://github.com/vllm-project/vllm/issues/2828
这是一个bug报告，涉及对象是vllm项目中的支持loras on quantized models的问题，可能由于编辑了配置文件而导致产生了某些症状的bug。

https://github.com/vllm-project/vllm/issues/2827
这是一个bug报告，涉及的主要对象是vllm在离线集群上的运行问题。这个问题是由于vllm在预加载模型的情况下仍然试图查找IP地址，而导致了网络不可达的错误。

https://github.com/vllm-project/vllm/issues/2826
这是一个bug报告，涉及vLLM在Ray Cluster上启动过程中卡在Initializing阶段的问题，原因可能是vLLM或Ray在此过程中出现了故障。

https://github.com/vllm-project/vllm/issues/2825
这是一个 bug 报告，涉及的主要对象是 GBNF 格式和 DeepSeeker 模型。这个问题出现的原因是无法正确加载 GBNF 格式或使用 DeepSeeker 模型时出现的错误。

https://github.com/vllm-project/vllm/issues/2824
这是一个关于VLLM是否能在CUDA 11.7上运行的问题，属于用户询问类型的issue，主要涉及VLLM在CUDA 11.7环境下的可用性。

https://github.com/vllm-project/vllm/issues/2822
这是用户提出的功能需求，主要涉及向 vLLM CLI 中添加 `vllm serve modelname` 命令，以改进接口提供更好的访问 OpenAI API 服务。

https://github.com/vllm-project/vllm/issues/2821
这是一个用户提出需求的issue，主要涉及LoRA在量化模型中的支持问题。由于无法与4/8位、awq等格式兼容，用户希望能够在此情况下使用LoRA。

https://github.com/vllm-project/vllm/issues/2820
这个issue是关于代码拼写检查的改进，属于代码优化类别，主要对象是vllm项目。

https://github.com/vllm-project/vllm/issues/2819
这是一个提出需求的issue，主要对象是为OpenAI API server添加引导解码（guided decoding）功能。

https://github.com/vllm-project/vllm/issues/2818
这个issue是一个功能需求，涉及对象是添加对gunicorn多进程处理的支持，用户寻求帮助解决相关问题。

https://github.com/vllm-project/vllm/issues/2817
这是一个bug报告，主要涉及vLLM在MI300X GPUs上使用tensor parallel size为8时生成不完整或无意义的响应的问题。

https://github.com/vllm-project/vllm/issues/2816
这是一个用户提出需求并寻求帮助的issue，主要涉及如何在VLLM Multi-Lora中处理lm_head和embed_tokens模块，可能由于adapter_weights中的lora设置问题而导致。

https://github.com/vllm-project/vllm/issues/2815
这个issue属于功能需求类型，主要涉及的对象是vLLM的API服务。原因是用户提出了对指导解码（guided decoding）功能的支持，以解决由于支持约束解码而引起的问题。

https://github.com/vllm-project/vllm/issues/2814
这是一个 bug 报告，涉及的主要对象是 vLLM 对 Qwen 1.5 的支持。由于当前存在错误，导致引用实现时出现错误。

https://github.com/vllm-project/vllm/issues/2813
这个issue属于bug报告类型，主要涉及openai completions api的echo参数在特定条件下引发的500 Internal Server Error。

https://github.com/vllm-project/vllm/issues/2812
这是一个bug报告，主要涉及对象是使用Python3.9在纯IPv6环境下遇到无法获取可用端口的问题。导致这个问题的原因是使用了错误的IP地址绑定导致返回了不可达错误。

https://github.com/vllm-project/vllm/issues/2811
这是一个bug报告，涉及要解决CUDA图形内存泄漏问题。

https://github.com/vllm-project/vllm/issues/2810
这是一个bug报告，涉及的主要对象是vllm.LLMEngine，导致此问题的原因是在初始化LLMEngine时使用多个lora出现CUDA错误。

https://github.com/vllm-project/vllm/issues/2809
这是一个需求添加的Issue，主要涉及vLLM项目中的Splitwise实现。由于需要实现Splitwise中的并行化处理逻辑，引入了MSCCL++通信库以提高KVcache传输速度。

https://github.com/vllm-project/vllm/issues/2808
这个issue类型属于bug报告，涉及的主要对象是自定义的全部减少内核。这个问题由于稳定性问题导致自定义全部减少内核被暂时禁用。

https://github.com/vllm-project/vllm/issues/2807
这是一个bug报告，主要涉及的对象是vllm实例中GPU内存资源的正确分享。导致此问题的原因是无法正确地共享GPU资源，导致第二个进程失败。

https://github.com/vllm-project/vllm/issues/2806
这是一个缺少内容的issue，类型为用户提出需求。

https://github.com/vllm-project/vllm/issues/2805
该issue是用户询问如何在使用多Lora推理时调整生成长度的问题，属于用户请教问题类型。

https://github.com/vllm-project/vllm/issues/2804
这个issue类型是更新请求，涉及主要对象是升级torch版本。原因是希望更新到`torch==2.2.1`，但由于还没有发布`torch==2.2`容器，导致无法更新ROCM。

https://github.com/vllm-project/vllm/issues/2802
这是一个bug报告类型的issue，主要涉及vllm项目中的`counter_generation_tokens`计量衡，并由于未包含prompt阶段生成的tokens而导致统计不准确。

https://github.com/vllm-project/vllm/issues/2801
这是一个用户需求问题，主要涉及AutoQuant quantization model for INT8/INT4 inference，由于INT4 runs faster than AWQ and FP16，用户可能希望了解更多关于AutoQuant的性能和特性。

https://github.com/vllm-project/vllm/issues/2800
这是一个关于bug的报告，涉及了版本0.3.0中prompt_logprobs和logits_processors同时设置时出现AssertionError的问题。可能是由于参数设置或代码逻辑问题导致的bug。

https://github.com/vllm-project/vllm/issues/2799
这个issue类型为bug报告，涉及的主要对象是代码中的变量或功能。原因可能是未正确设置或处理变量导致出现空值的情况。

https://github.com/vllm-project/vllm/issues/2798
这是一个bug报告，涉及的主要对象是在使用Nvidia-H20和特定版本的PyTorch时出现的CUBLAS错误。导致这个问题的原因可能是与CUDA加速相关的配置或实现问题。

https://github.com/vllm-project/vllm/issues/2797
这是一个bug报告，主要涉及的对象是vllm库。导致这个问题的原因可能是torch和xFormers版本与vllm库不匹配。

https://github.com/vllm-project/vllm/issues/2796
这是一个性能优化相关的issue，提出了关于如何进行增量构建的文档需求，旨在减少编译时间，提高开发迭代速度。

https://github.com/vllm-project/vllm/issues/2795
这是一个bug报告，涉及的主要对象是在多台机器上进行分布式推断。由于设备ID无效导致的错误，用户无法成功运行代码。

https://github.com/vllm-project/vllm/issues/2794
这是一个bug报告，主要涉及的对象是使用多个AMD GPUs的系统。由于Ray在v2.9版本中官方不支持AMD GPUs，导致在使用多个AMD GPUs时出现死锁和仅有一个GPU被暴露给每个worker的问题。

https://github.com/vllm-project/vllm/issues/2793
这是一个bug报告，主要涉及到vllm==0.2.7在特定条件下导致CPU内存泄漏，造成了Out of Memory错误的问题。

https://github.com/vllm-project/vllm/issues/2792
这是一个bug报告，涉及的主要对象是ROCm/flashattention，由于缺少对`gfx908`架构的支持，导致在MI100上提供LLM服务时输出乱码。

https://github.com/vllm-project/vllm/issues/2791
这是一个用户提出需求的问题，主要涉及 VLLM 下的改进Speculative decoding。用户提出了关于添加Eagle、Medusa、Look Ahead decoding等新的改进以提高VLLM性能的建议。

https://github.com/vllm-project/vllm/issues/2790
这个issue类型为bug报告，涉及的主要对象是ROCm构建，由于先前提交引起的问题，导致ROCm构建失败并出现编译错误。

https://github.com/vllm-project/vllm/issues/2789
这是一个用户报告bug的issue，该问题涉及vllm下的模型无法按预期继续会话并添加额外指令和响应。造成此问题的原因可能是无法正确使用模型的属性。

https://github.com/vllm-project/vllm/issues/2788
这是一个Bug报告，主要涉及VLLM软件在使用4个A6000处理器运行CodeLlama-70B时产生错误输出的问题，通过降级至`v0.2.7`可以解决。

https://github.com/vllm-project/vllm/issues/2787
这是一个关于功能特性确认的问题，主要涉及到需要确认multilora的工作情况。用户提出了slora功能是否正常工作以及如何跟踪lora推断的疑问。

https://github.com/vllm-project/vllm/issues/2786
这是一个关于模型压缩及GPU资源利用的问题，类型为用户提出需求/寻求帮助。主要涉及的对象是使用vllm进行模型压缩的用户。导致GPU资源占用异常的原因可能是配置参数设置不正确或者环境搭建存在问题。

https://github.com/vllm-project/vllm/issues/2785
这是一个Bug报告，涉及到VLLM中Qwen1.5-7B-Chat模型初始化时出现的参数设置错误所致的问题。

https://github.com/vllm-project/vllm/issues/2783
这是一个用户报告bug的issue，涉及主要对象为vllm加载官方llama模型时出现的错误。这个问题可能由于缺少config.json文件而产生。

https://github.com/vllm-project/vllm/issues/2782
这是一个用户提出需求的issue，主要涉及的对象是支持将logprobs设置为模型词汇大小。由于模型tokenizer和vocab_size的使用方式不一致，导致无法轻松设置logprobs到模型词汇大小。

https://github.com/vllm-project/vllm/issues/2781
这是一个bug报告，该问题涉及的主要对象是docker container `vllm/vllmopenai:v0.3.0`，由于`kvcachedtype=fp8_e5m2`导致CC错误。

https://github.com/vllm-project/vllm/issues/2780
这是一个关于bug报告的issue，涉及的主要对象是vLLM v0.3.0中的openai.api_server功能，并由于FileNotFoundError导致失败。

https://github.com/vllm-project/vllm/issues/2779
这是一个用户提出需求的issue，主要涉及到vLLM的Splitwise实现。原因可能是用户想将Splitwise集成到vLLM中以实现特定功能。

https://github.com/vllm-project/vllm/issues/2778
这是一个bug报告，该问题涉及的主要对象是docker container `vllm/vllm-openai:v0.3.0`，由于使用`--kv-cache-dtype=fp8_e5m2`参数导致出现错误。

https://github.com/vllm-project/vllm/issues/2777
这是一个用户提出需求的issue，主要涉及vllm项目中支持neuron后端的设置文档，提供了关于为推理准备trn1/inf2实例的指导，需要进行与PR https://github.com/vllmproject/vllm/pull/2569中的更改相关的步骤。

https://github.com/vllm-project/vllm/issues/2776
该issue为性能优化建议，主要涉及到layernorm kernels的调优。通过使用共享内存来提高性能，特别针对隐藏大小小于4090的情况，可以获得更好的性能表现。

https://github.com/vllm-project/vllm/issues/2775
这个issue是一个用户提出需求的问题，主要涉及将multiLoRA功能集成到OpenAI服务器中。由于尚未对客户端权限进行划分到特定模型的工作，用户可能无法正确区分多个LoRA模块返回的数值。

https://github.com/vllm-project/vllm/issues/2774
这是一个用户提出需求的issue，主要涉及到设置本地日志级别的功能。由于默认始终使用`logging.DEBUG`，用户想要通过环境变量`LOG_LEVEL`手动设置日志级别。

https://github.com/vllm-project/vllm/issues/2773
这是一个关于bug报告的issue，主要对象是vLLM的docker容器版本`vllm/vllm-openai:v0.3.0`。由于最新版本的容器出现了加载模型后的错误，用户正在寻求解决这个问题。

https://github.com/vllm-project/vllm/issues/2772
这是一个性能优化的issue，主要涉及FlashInfer的实现和GQA PagedAttention的改进，由于提高了处理速度，导致了性能指标的提升。

https://github.com/vllm-project/vllm/issues/2771
这是一个bug报告，主要涉及vllm在可编辑模式下安装时遇到的问题，原因是缺少对CUDA版本12.1的版本边界定义。

https://github.com/vllm-project/vllm/issues/2770
这是一个bug报告，涉及的主要对象是NCCL，在使用vllm v0.2.7时出现hang现象，升级到v0.3.0后使用自定义的all reduce替代才解决了问题。

https://github.com/vllm-project/vllm/issues/2769
这是一个需求类型的Issue，涉及主要对象是为MoE（Mixture of Experts）添加融合的Top-K softmax kernel。由于需要更多MoE相关的kernel和优化性能，开发者提出了两个待办事项。

https://github.com/vllm-project/vllm/issues/2768
这个issue类型是用户提出需求，主要涉及的对象是vllm对AMD Radeon™ 7900系列GPU（gfx1100）的支持。由于flashattention不完全支持gfx1100，需要使用vllm参考实现，需要通过特定的docker build命令来构建镜像。

https://github.com/vllm-project/vllm/issues/2767
这是一个用户需求类型的issue，主要涉及vLLM导入FlashInfer中的PagedAttention kernels以支持GQA。原因是FlashInfer在批处理GQA解码注意力方面比vLLM的PagedAttention快3倍。

https://github.com/vllm-project/vllm/issues/2766
这个issue类型是用户提出需求，涉及的主要对象是关于context (conversation history)在multi-turn conversation中的维护。由于用户希望了解在vLLM中多轮对话时支持的最大context长度以及如何处理此问题，表明用户关注在多轮对话中如何有效地维护和利用对话历史。

https://github.com/vllm-project/vllm/issues/2765
这个issue是一个bug报告类型，涉及主要对象是`benchmark_latency`。由于使用了固定的输入token ids和`profile_dir`bug，导致了`benchmark_latency`功能无法正常工作。

https://github.com/vllm-project/vllm/issues/2764
这是一个用户提出需求的类型，主要对象是添加更多的Prometheus metrics。

https://github.com/vllm-project/vllm/issues/2763
这个issue类型是用户提出需求，该问题单涉及主要对象是支持新的OLMo模型。由于当前系统尚未支持新的OLMo模型，用户在此提出希望系统能够支持这些新模型的需求。

https://github.com/vllm-project/vllm/issues/2762
这个issue是关于功能需求的，主要对象是BlockAllocator类，由于需要实现自动前缀缓存功能，导致用户提出了关于实现自动前缀缓存的需求。

https://github.com/vllm-project/vllm/issues/2761
该issue属于用户提出需求类型，主要涉及的对象是在GPTQ & AWQ Fused MOE中支持增加对quantized fusedMoE的支持以及添加GPTQ group gemm kernels。由于当前代码需要优化以支持这些新特性，因此用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/2760
这是一个bug报告，涉及到vllm中的custom allreduce内核问题，主要原因是不安全的同步导致一些奇怪的现象出现。

https://github.com/vllm-project/vllm/issues/2759
该issue为特性实现类型，主要涉及到实现fused_moe kernel与cutlass grouped gemm之间的结合。

https://github.com/vllm-project/vllm/issues/2758
这个issue是关于bug报告的，主要涉及Llama Guard在HuggingFace's Transformers和vLLM中的输出不一致，可能是由于代码实现上的差异导致的。

https://github.com/vllm-project/vllm/issues/2757
这是一个用户提出需求的issue，主要对象是vllm，用户提出了关于是否vllm支持多模态LLM的疑问。

https://github.com/vllm-project/vllm/issues/2756
这是一个bug报告，主要涉及到json packet boundary determination的问题，导致无法收到消息。

https://github.com/vllm-project/vllm/issues/2755
这是一个bug报告，涉及的主要对象是vllm库中加载internlm2-chat-20b模型时出现数值错误。由于输入数据的形状与要求不匹配，导致数值数值错误的异常。

https://github.com/vllm-project/vllm/issues/2754
这是一个bug报告，涉及的主要对象是VLLM Server的采样参数设置。由于采样参数中top_k仍然为1，用户可能怀疑是否发送了正确的请求。

https://github.com/vllm-project/vllm/issues/2753
这个issue属于功能需求提出类型，主要涉及了vLLM在Intel CPU和GPU设备上的集成。由于需要逐步将代码从CUDA设备转移到非CUDA设备，用户希望对vLLM进行改动以支持Intel GPU/CPU设备的运行。

https://github.com/vllm-project/vllm/issues/2752
这是一个bug报告，涉及的主要对象是vLLM模型。由于增加并发请求数后vLLM无法响应请求，可能是与配置或参数设置有关。

https://github.com/vllm-project/vllm/issues/2751
这是一个用户提出的需求问题，主要涉及的对象是openai server，用户希望支持early_stopping sampling参数。

https://github.com/vllm-project/vllm/issues/2750
这个issue是关于bug修复，主要涉及的对象是test_cache.py文件，由于CI测试失败导致的问题。

https://github.com/vllm-project/vllm/issues/2749
这是一个用户报告类型的issue，主要涉及的对象是Mistral model architecture。由于该模型架构没有被支持，导致了这个问题的出现。

https://github.com/vllm-project/vllm/issues/2748
这是一个bug报告，涉及的主要对象是使用VLLM运行GPTQ模型时的内存使用量问题。导致这个问题的原因可能是内存泄漏或者参数设置不正确。

https://github.com/vllm-project/vllm/issues/2747
这是一个关于Bug报告的issue，主要涉及对象是vllm和lmevaluationharness。由于缺少特定的符号导致了ImportError。

https://github.com/vllm-project/vllm/issues/2746
这是一个bug报告，涉及的主要对象是VLLM项目中的triton_kernel模块。这个问题的原因是无法导入指定的模块，可能由于代码中对'triton_kernel'模块中的'context_attention_fwd'功能的调用出现了问题。

https://github.com/vllm-project/vllm/issues/2745
这是一个关于bug报告的issue，主要涉及vllm在AlmaLinux release 9.1（Lime Lynx）上的支持情况。由于工作推断器在启动后一直处于挂起状态，用户反馈了这个问题。

https://github.com/vllm-project/vllm/issues/2744
这是一个性能改进的issue，主要涉及支持MQA/GQA在解码阶段的优化，并且主要针对小批量大小的情况。

https://github.com/vllm-project/vllm/issues/2743
这是一个需求类型的issue，主要涉及VLLM项目中解码阶段使用flash attention的需求。

https://github.com/vllm-project/vllm/issues/2742
这是一个bug报告，涉及vllm模型使用tp=2进行推理时结果随机的问题，使用disablecustomallreduce能解决。可能是由于自定义allreduce机制导致的。

https://github.com/vllm-project/vllm/issues/2741
这个issue属于功能需求类型，涉及主要对象为LLMEngine内部的原始分词器。这个问题是由于LLMEngine使用TokenizerGroup，导致在LLM中的get_tokenizer和set_tokenizer方法需要调整，以操作LLMEngine中的原始分词器而提出的。

https://github.com/vllm-project/vllm/issues/2740
这是一个Bug报告，主要涉及到VLLM下的生成文本操作。由于某些原因导致了Prefix错误，导致程序出现异常。

https://github.com/vllm-project/vllm/issues/2739
这是一个bug报告，主要涉及的对象是vllm中的offline inference模块。由于prefix长度为198个token且prompt长度不超过1k tokens，但在使用prefix时触发了断言错误，推断的原因可能是prefix与数据不匹配导致的错误。

https://github.com/vllm-project/vllm/issues/2738
这是关于软件版本更新的问题，主要涉及的对象是pytorch库。这个问题由于v2.1.2版本中存在的内存溢出bug导致，用户在询问是否会更新到解决了这一bug的新版本v2.2.0。

https://github.com/vllm-project/vllm/issues/2737
这是一个用户提出需求的issue，主要涉及vllm scheduler中的prompt数量限制。由于prompt限制，用户无法提供足够的提示来进行对话。

https://github.com/vllm-project/vllm/issues/2736
这是一个文档更新类型的issue，主要涉及到VLLM的langchain服务说明文档可能需要更新。可能是由于最新版本的变化或者错误导致用户需要更新相关说明文档。

https://github.com/vllm-project/vllm/issues/2735
这是一个Bug报告类型的Issue，主要涉及到无法构建vllm项目的wheels，可能是由于构建依赖关系或构建后端不支持可编辑构建而导致的问题。

https://github.com/vllm-project/vllm/issues/2734
这是一个bug报告，该问题单涉及了 llama2-70b-chat，由于某些原因导致Watchdog在处理collective operation时出现超时，导致程序在处理一些请求后会卡住。

https://github.com/vllm-project/vllm/issues/2733
这个issue是一则bug报告，主要涉及vLLM的默认参数设置与Hugging Face在性能表现上的差异，导致在使用openai api_endpoint.py脚本时出现显著较差的结果。

https://github.com/vllm-project/vllm/issues/2732
这是一个用户询问问题的类型，主要涉及的对象是在cpp文件中如何使用vllm。

https://github.com/vllm-project/vllm/issues/2731
这是一个bug报告，问题涉及的主要对象是vLLM服务器。由于未知原因导致服务器在运行一段时间后会随机卡住，无法生成tokens。

https://github.com/vllm-project/vllm/issues/2730
这个issue属于功能需求类型，主要涉及将`aioprometheus`的指标迁移到`prometheus_client`，由于`prometheus_client`更受关注且更活跃维护，因此需要进行更迁换。

https://github.com/vllm-project/vllm/issues/2729
这是一个bug报告类型的issue，主要涉及的对象是vllm中的`context_attention_fwd`函数。由于`context_attention_fwd`函数调用的过程中触发了关于CC的错误，可能与Prefix Caching Support相关的实验功能有关。

https://github.com/vllm-project/vllm/issues/2728
这是一个bug报告，涉及的主要对象是vllm 0.3版本的Mixtral GPTQ模型。由于调用`llm.generate`函数时，程序会在`llm_engine.step()`函数或`_run_workers`调用中卡住，导致无法生成任何输出。

https://github.com/vllm-project/vllm/issues/2727
这个issue是一个bug报告，涉及的主要对象是chat serving功能。原因是缺少了对tokenizer可用性的等待导致问题，需要添加相关实现来解决。

https://github.com/vllm-project/vllm/issues/2726
这是一个需求报告，涉及到vllm中的Choronz patch，用户希望更新xformers到版本0.0.24。

https://github.com/vllm-project/vllm/issues/2725
这是一个关于bug报告的issue，主要涉及到ROCM平台下包含<hip/hip_bf16.h>导致的编译错误问题，原因是函数在没有_static inline_定义的情况下导致了多重定义错误。

https://github.com/vllm-project/vllm/issues/2724
这是一个关于用户提出需求的类型，主要涉及的对象是支持vision/audio模型的计划。由于缺乏支持，用户询问是否有计划支持vision/audio模型像QwenVLChat和QwenAudioChat这样的模型。

https://github.com/vllm-project/vllm/issues/2723
这是一个需求提出类型的issue，主要涉及gemm kernels的重构操作。由于代码重复导致功能冗余，需要合并两个函数并添加参数以提高效率。

https://github.com/vllm-project/vllm/issues/2722
该issue类型为用户提出需求，请求支持。主要对象是 Torch2.2 。由于用户需要在该版本中获得支持，因此提出了此问题。

https://github.com/vllm-project/vllm/issues/2721
这是一个bug报告，主要涉及到在使用internlm2-chat-20b模型时出现了不支持指定模型架构的问题。可能是由于代码中对特定模型架构的支持存在问题，导致用户无法成功调用指定模型。

https://github.com/vllm-project/vllm/issues/2720
这是一个bug报告，主要涉及安装vllm0.3.0时出现的CUDA相关编译错误。导致这个问题的原因可能是缺少了-cuda库。

https://github.com/vllm-project/vllm/issues/2718
这是一个关于更新README的issue，类型为用户提出需求。主要对象是meetup slides。

https://github.com/vllm-project/vllm/issues/2717
这个issue类型是bug报告，涉及的主要对象是"accident"。由于开发环境配置错误导致的bug。

https://github.com/vllm-project/vllm/issues/2716
这是一个bug报告，涉及的主要对象是python 3.8的语法。这个问题是由于代码中的语法不兼容python 3.8导致的。

https://github.com/vllm-project/vllm/issues/2715
这是一个用户提出需求的类型，主要对象是在 vllm 中运行 openai 服务器时如何在 Detach 模式下运行。

https://github.com/vllm-project/vllm/issues/2714
这是一个bug报告，主要涉及的对象是Mixtral 8x7b模型升级到版本0.3.0后输出了错误的数据。这个问题可能是由于引入了融合内核导致的。

https://github.com/vllm-project/vllm/issues/2713
这个issue是关于bug报告，主要涉及对象是LLM.llm_engine.tokenizer类。由于LLM.llm_engine.tokenizer类的行为不符合预期，导致了调用时发生AttributeError错误。

https://github.com/vllm-project/vllm/issues/2712
这是一个bug报告类型的issue，该问题涉及主要对象为编译工具punica。由于编译punica时出现报错，用户需要寻求帮助解决此问题。

https://github.com/vllm-project/vllm/issues/2711
这是一个关于无法导入特定名称的模块的bug报告，涉及主要对象为vllm模型的triton_kernel模块，可能是由于引入模块时出现了错误或缺失导致的。

https://github.com/vllm-project/vllm/issues/2710
这是一个用户提出需求的issue，主要涉及的对象是vllm，可能是由于功能缺失或者技术限制导致无法加载lora模块，用户希望得到关于支持lora加载的帮助。

https://github.com/vllm-project/vllm/issues/2709
这是一个关于性能问题的bug报告，主要涉及了logit processor在批量计算时的并行模式。用户遇到了由于logit processor被串行调用导致的性能问题，尝试使用ProcessPoolExecutor进行并行处理但效果不佳，希望能得到作者的帮助优化以实现并行计算。 

https://github.com/vllm-project/vllm/issues/2708
该issue属于Bug报告，涉及到docker build过程中遇到的错误。由于Dockerfile中使用了未知的`mount`标志，导致了解析错误并无法成功构建Docker镜像。

https://github.com/vllm-project/vllm/issues/2707
这个issue是一个关于bug报告的类型，主要对象是在2080ti下无法使用vllm，可能是由于硬件兼容性问题导致的。

https://github.com/vllm-project/vllm/issues/2706
这个issue是关于bug报告，涉及的主要对象是vLLM v0.3.0版本的logprobs参数和请求日志。由于Pydantic2版本在处理logprobs时出现了问题，可能导致logprobs过期，而且由于openai服务器重构，接口日志中的prompt参数始终为None。

https://github.com/vllm-project/vllm/issues/2705
这个issue类型是bug报告，主要涉及AsyncLLMEngine的使用问题，可能是由于Tornado方法与协程的结合导致的错误。

https://github.com/vllm-project/vllm/issues/2704
这是关于代码语法问题的bug报告，涉及的主要对象是"vllm项目中的serving_completion.py"文件。由于在代码中使用的python typing语法不兼容python3.8，导致了此问题。

https://github.com/vllm-project/vllm/issues/2703
这是一个Bug报告，主要涉及VLLM下的`logprob` & `echo` combo功能，问题是设置`echo`为`true`时服务器无响应，由于提交CC(CI: make sure benchmark script exit on error)导致。

https://github.com/vllm-project/vllm/issues/2702
这是一个bug报告，主要涉及到vLLM在只使用prompt_token_ids作为输入提示时仍尝试返回解码的提示，实际上这是不必要的。这个问题可能是由于代码逻辑中的错误导致的。

https://github.com/vllm-project/vllm/issues/2701
这是一个用户提出需求的issue，主要涉及如何获取句子概率而不需要生成句子，存在速度较慢的问题。

https://github.com/vllm-project/vllm/issues/2699
这是一个关于技术问题的bug报告，主要涉及对象是使用`Qwen-14B-Chat-Int4`模型时遇到的数值对齐错误导致的ValueError。

https://github.com/vllm-project/vllm/issues/2698
这是一个用户提出的需求，涉及主要对象是代码库中的SamplingParams类。原因是当前代码仅支持单词作为停止词，用户提出支持多词停止词的功能。

https://github.com/vllm-project/vllm/issues/2697
这是一个bug报告，涉及到使用quantization时无法按照传入的模型修订版下载配置文件的问题。

https://github.com/vllm-project/vllm/issues/2696
这是一个用户提出需求的issue，主要涉及vLLM的batch inference在Ray分布式环境下的运行示例。

https://github.com/vllm-project/vllm/issues/2695
这是一个Bug报告，涉及的主要对象是在安装vllm时出现的错误。由于使用Python 3.12导致安装失败，降级到Python 3.10后问题得以解决。

https://github.com/vllm-project/vllm/issues/2694
这是一个bug报告，该问题涉及Streaming在VLLM版本0.2.7中无法正常工作。原因可能是调用streaming参数时出现错误。

https://github.com/vllm-project/vllm/issues/2692
这是一个bug报告，主要涉及到在使用多个GPU运行模型时服务器崩溃的问题。可能由于多GPU配置下的运行设置问题导致服务器崩溃。

https://github.com/vllm-project/vllm/issues/2690
这是一个bug报告，主要涉及的对象是DeciLM模型。由于0.3.0版本中初始化DeciLM时传入参数个数不一致，导致了TypeError和无法加载DeciLM的问题。

https://github.com/vllm-project/vllm/issues/2689
这是一个bug报告，涉及的主要对象是参数描述。由于参数描述错误导致出现了bug症状或者用户无法正确理解参数信息。

https://github.com/vllm-project/vllm/issues/2688
这是一个Bug报告，该问题涉及的主要对象是LoRa网络中的前缀缓存功能。这个问题是由于LoRa索引映射没有考虑到前缀长度而引发的断言失败。

https://github.com/vllm-project/vllm/issues/2687
这是一个bug报告，涉及解决chatglm3和Baichuan2的聊天模式支持问题，原因是当前代码无法支持它们而导致其自问自答。

https://github.com/vllm-project/vllm/issues/2686
这个issue是一个bug报告，主要涉及到codellama 70b在使用docker image时停止生成文本的问题，可能是由于设置的"stop"字段导致了该问题的发生。

https://github.com/vllm-project/vllm/issues/2685
这是一个bug报告，涉及Ray Workers在纯IPv6环境下获取IP地址失败导致的问题。

https://github.com/vllm-project/vllm/issues/2684
这是一个bug报告，涉及的主要对象是测试文件 test_cache.py。由于两个pull request同时合并导致一个race condition，从而引发了测试失败。

https://github.com/vllm-project/vllm/issues/2683
这是一个bug报告类型的issue，主要涉及到OpenAIServingChat无法在正在运行的事件循环中实例化，其原因是在异步调用的情况下，tokenizer尚未加载完成就被访问。 

https://github.com/vllm-project/vllm/issues/2682
这个issue是一个bug报告，涉及的主要对象是NCCL watchdog thread。由于某种原因导致NCCL watchdog线程异常终止，引发了该bug。

https://github.com/vllm-project/vllm/issues/2681
这是一个需求提出的issue，主要涉及vLLM在Q1 2024的路线图，用户提出了相关特性的讨论和贡献。

https://github.com/vllm-project/vllm/issues/2680
这是一个bug报告，主要涉及的对象是支持CodeLlama-70b-Instruct-hf。问题是由于缓存操作模块缺失符号导致无法运行，引发了异常情况。

https://github.com/vllm-project/vllm/issues/2679
这是一个bug报告，主要涉及Llama 7b模型中，在执行模型过程中出现TP=1和TP=2之间的logits不匹配的问题。可能由于代码中的执行模型函数的输出存在问题，导致了这个bug。

https://github.com/vllm-project/vllm/issues/2678
这个issue属于用户提出需求类型，主要对象是vllm所实现的kernel。由于目前vllm仅支持在cuda设备上运行kernel，而用户希望添加设备调度以支持其他类型设备，如cpu或xpu。

https://github.com/vllm-project/vllm/issues/2677
这个issue是一个需求类型的问题，主要涉及的对象是在vLLM中添加Mixtral MoE层的单元测试。这个问题的提出是为了增加对Mixtral MoE层的单元测试覆盖。

https://github.com/vllm-project/vllm/issues/2676
这是一个bug报告，主要涉及的对象是vllm库，由于缺少'model.beacon_embed_tokens.weight'导致了KeyError错误。

https://github.com/vllm-project/vllm/issues/2675
这是一个关于权限错误的bug报告，涉及主要对象是多用户在同一台机器上使用vLLM时出现.tmp目录被锁定的问题。

https://github.com/vllm-project/vllm/issues/2674
这个issue类型是bug报告，涉及到VLLM模型在TP=1时出现的误报警告问题。原因可能是代码中存在不必要的警告输出逻辑，导致用户在设定TP=1时接收到虚假警告信息。

https://github.com/vllm-project/vllm/issues/2673
该issue类型为bug报告，涉及主要对象为Mixtral的quantization support，由于CC(Fused MOE for Mixtral)的优化破坏了quantization support，导致需要通过hacky方式暂时修复。

https://github.com/vllm-project/vllm/issues/2672
这是一个技术文档bug报告，涉及对象为代码中的变量名称，由于一个小拼写错误导致了烦人的问题。

https://github.com/vllm-project/vllm/issues/2671
这是一个bug报告，主要涉及到Neuron-ls库的安装问题。这个issue的症状是尝试使用"pip install e ."时出现了CC错误。

https://github.com/vllm-project/vllm/issues/2670
这是一个bug报告，涉及到修复传递hf_config参数的问题。用户提出的问题是无法通过openai服务器传递hf_config参数。

https://github.com/vllm-project/vllm/issues/2669
这是一个bug报告，涉及到Ray worker内存耗尽导致任务失败的问题。

https://github.com/vllm-project/vllm/issues/2668
这是一个bug报告，主要涉及的对象是Mixtral AWQ，问题是vLLM在使用Mixtral AWQ时返回结果被截短，原因可能是参数设置不当导致症状表现为回复被截短。

https://github.com/vllm-project/vllm/issues/2667
这是一个bug报告类型的issue，主要涉及到需要修复默认值为1.0的长度惩罚参数。

https://github.com/vllm-project/vllm/issues/2666
这是一个bug报告，涉及到vllm项目下internlm2模型的代码复制和修改。由于移除了`einops`并修复了加载`internlm2chat20b`模型时的bug，用户提供了测试脚本和结果，并描述了相关问题。

https://github.com/vllm-project/vllm/issues/2665
这是一个关于设置LLM和辅助模型在不同CUDA设备上的问题，属于需求类问题，涉及到利用CUDA_VISIBLE_DEVICES实现不同设备分配。

https://github.com/vllm-project/vllm/issues/2664
这是一个bug报告，该问题涉及到在使用`--engine-use-ray`时出现的错误信息，是由于直接调用了Actor方法导致请求错误。

https://github.com/vllm-project/vllm/issues/2663
这是一个BUG报告，主要涉及的对象是MoE模型，由于新的融合MoE内核不支持量化，导致了该问题。

https://github.com/vllm-project/vllm/issues/2662
这是一个用户提出需求的issue，主要涉及支持Web Dashboard的功能需求。用户希望能够同时部署Web Dashboard作为vLLM推理引擎，以便更轻松地观察、诊断和管理推理系统。

https://github.com/vllm-project/vllm/issues/2661
这是一个bug报告，涉及主要对象是vllm项目中的pip安装问题。由于CUDA版本为12.2，导致使用pip安装时出现错误。

https://github.com/vllm-project/vllm/issues/2660
这是一个用户提出需求的issue，主要涉及vLLM的代码合并问题。由于用户需要将qwen-72b-chat-int4的支持代码合并到主分支，进而使用QwenLM大型语言模型的模型推理。

https://github.com/vllm-project/vllm/issues/2659
这是一个bug报告，主要涉及的对象是vllm项目中的entrypoints/api_server.py文件，原因是用户想知道如何解决一个输出带有提示的问题。

https://github.com/vllm-project/vllm/issues/2658
这是一个bug报告，涉及到reject sampler中num_accepted_tokens的计算错误，导致accepted.sum()返回不正确的结果。

https://github.com/vllm-project/vllm/issues/2657
这是一个关于功能询问的issue，涉及VLLM对LLM输出是否产生影响，症结在于加速框架是否会影响最终结果的准确度。

https://github.com/vllm-project/vllm/issues/2656
该issue类型为版本更新，涉及更新新功能和文档。

https://github.com/vllm-project/vllm/issues/2655
这个issue属于bug报告类型，涉及到vllm项目中的模型服务运行时。由于max model len参数大于max seq len参数导致错误，导致现有模型无法正常提供服务。

https://github.com/vllm-project/vllm/issues/2654
这个issue类型是需求提出，主要涉及编译构建系统，用户寻求帮助寻找更适合的构建工具优化当前系统的编译过程。

https://github.com/vllm-project/vllm/issues/2653
这是一个bug报告，主要涉及到在使用Beam Search时出现IndexError。这可能是由于参数传递引起的错误。

https://github.com/vllm-project/vllm/issues/2652
这是一个Bug报告，涉及对象为在8GPU上加载模型时出现的数学计算错误，导致了 ValueError 错误的产生。

https://github.com/vllm-project/vllm/issues/2651
这个issue是一个bug报告，涉及的主要对象是vllm和chatglm2模型。由于某种原因导致了异常在回调函数中引发错误，用户寻求如何避免这种异常。

https://github.com/vllm-project/vllm/issues/2650
这是一个用户提出需求的issue，主要涉及对vllm的Prometheus指标添加相关提案。由于已有的计数器类型指标不够具有意义，用户希望增加Histogram类型的指标。

https://github.com/vllm-project/vllm/issues/2649
这个issue是关于用户提出需求的，主要涉及到如何从Mistral / Mixtral获取`logprobs`和softmax概率的问题，用户想要获得模型输出的原始分数和概率。

https://github.com/vllm-project/vllm/issues/2648
这个Issue类型为Bug报告，涉及的主要对象是编译器（CC）。该Bug由于ROCM6.0环境下编译源代码出现错误而导致。

https://github.com/vllm-project/vllm/issues/2647
这个issue是用户提出需求，主要对象是项目的基准测试，希望能够支持对填充和解码阶段的数据进行单独测试。这个需求可能由于项目需要更精确地评估不同阶段的性能表现而提出。

https://github.com/vllm-project/vllm/issues/2646
这是一个关于BUG报告的issue，主要涉及到编译源代码错误，可能是由于某个提交导致出现了多次定义的错误。

https://github.com/vllm-project/vllm/issues/2645
这是一个关于功能需求的Issue，主要涉及的对象是PR（Pull Request），用户提出了是否接受将ExLlamaV2 kernels与AWQ集成的请求。由于ExLlamaV2 kernels的性能优化，用户希望在解码过程中获得更高的运行速度。

https://github.com/vllm-project/vllm/issues/2644
这个issue是一个bug报告，主要涉及到开发中出现的错误提示消息，由于最新开发版本中的一个关键字参数错误导致了该bug提示。

https://github.com/vllm-project/vllm/issues/2643
这个issue类型为用户提出需求，主要涉及对象是在集成vLLM中的API服务器以便lm-evaluation-harness更轻松。这个问题主要是由于需要在不同网络环境下使用vLLM的tokenizer功能，但目前该功能尚未支持，因此用户提出了添加`/get_tokenizer`端点到`api_server`的建议。

https://github.com/vllm-project/vllm/issues/2642
这个issue属于bug报告，主要涉及到在vllm中存在重复的IPC open操作。原因可能是代码逻辑错误或者数据管理问题导致了这种症状。

https://github.com/vllm-project/vllm/issues/2641
这是一个bug报告，涉及到custom_all_reduce.cuh文件，用户遇到了`resource already mapped`错误，可能是由于Custom all reduce kernels引起的。

https://github.com/vllm-project/vllm/issues/2640
这是关于bug报告的issue，主要涉及VLLM项目中的Worker构造函数，出现了关键字参数'cache_config'的错误。这个问题可能由于新的master分支中的变化引起，导致在运行特定命令时出现了报错。

https://github.com/vllm-project/vllm/issues/2639
这个issue类型是用户提出需求，涉及到更新runbenchmark.sh文件，主要是为了添加--kv-cache-dtype选项支持fp8_e5m2数据类型。

https://github.com/vllm-project/vllm/issues/2638
这是一个关于需求的问题，主要涉及的对象是vLLM模型是否支持卷积层。用户可能想了解vLLM模型是否支持卷积层以及如何在模型中使用该功能。

https://github.com/vllm-project/vllm/issues/2637
该issue属于需求提出类型，主要涉及优化llama家族模型的重构。由于存在重复的代码逻辑，导致需要对llama家族模型进行优化以减少冗余代码。

https://github.com/vllm-project/vllm/issues/2636
这个issue是bug报告，涉及的主要对象是vllm的开发。由于tensorparallel大于1时，vllm的开发工作出现问题。

https://github.com/vllm-project/vllm/issues/2635
这个issue属于用户提出需求类型，主要涉及添加对Pascal GPU的计算能力6.x的支持。原因是由于vLLM的限制，用户无法使用旧架构的GPU进行计算。

https://github.com/vllm-project/vllm/issues/2634
这是一个bug报告，主要对象是vllm软件。这个问题可能是由于安装过程中出现的错误导致vllm软件无法在Python中找到。

https://github.com/vllm-project/vllm/issues/2633
这是一个用户提出需求的issue，主要涉及Dockerfile中添加`buildarg punica_kernels=0`选项，希望在构建docker镜像时避免构建Punica内核。

https://github.com/vllm-project/vllm/issues/2632
该问题类型是性能优化，主要对象是Punica编译过程。由于将Punica内核拆分为单独文件，编译时间从30分钟缩短到4分钟，且默认开启Punica编译，可能是为了提高编译效率。

https://github.com/vllm-project/vllm/issues/2631
这是一个bug报告，主要涉及vllm中AWQ使用长上下文导致GPU内存占用过多，最终导致GPU内存不足的问题。

https://github.com/vllm-project/vllm/issues/2630
这是一个bug报告，该问题单涉及的主要对象是vLLM项目中的Ray依赖关系。原因是vLLM不再导入Ray AIR，因此去除了对pandas和pyarrow的要求。

https://github.com/vllm-project/vllm/issues/2629
这是一个关于无法访问文档链接的bug报告类型的issue，主要涉及执行Offline Batched Inference示例时遇到404 Client Error的问题。

https://github.com/vllm-project/vllm/issues/2628
这是一个bug报告，主要涉及的对象是执行Offline Batched Inference的示例代码。该问题出现的原因是HTTPError导致的400 Client Error，用户寻求解决该错误并成功执行示例。

https://github.com/vllm-project/vllm/issues/2627
这是一个关于bug的报告，主要涉及ROCm环境下的vllm编译相关的问题，导致需要确保vllm与所有支持的架构一起编译，以满足多种架构的需求。

https://github.com/vllm-project/vllm/issues/2626
这是一个bug报告，涉及的主要对象是vllm项目的代码。产生这个bug的原因是代码中有一个键错误导致程序无法找到特定变量，用户寻求帮助解决这个错误。

https://github.com/vllm-project/vllm/issues/2625
这是一个bug报告，主要对象是StableLMEpochForCausalLM类的Class名字错误导致了问题。

https://github.com/vllm-project/vllm/issues/2624
这是一个Bug报告，主要涉及到VLLM项目中的Memory Leak问题，导致新版本中遇到CUDA out of memory错误。

https://github.com/vllm-project/vllm/issues/2623
这是关于软件安装问题的Bug报告，主要涉及的对象是CUDA版本兼容性，可能由于软件版本不匹配导致报错信息显示CUDA版本过低。

https://github.com/vllm-project/vllm/issues/2622
这是一个bug报告，主要涉及模型设置`head_dim`时导致kv_cache大小错误，进而导致PagedAttention CUDA kernel错误写入GPU内存。

https://github.com/vllm-project/vllm/issues/2621
这是一个bug报告，涉及到Mixtral AWQ工具无法正常工作，可能是由于取消作用域导致的CancelledError错误。

https://github.com/vllm-project/vllm/issues/2620
这个issue是一个bug报告，主要涉及vllm 0.2.7中使用tensor-parallel > 1推理模式时出错，可能由于无法正确设置tensor-parallel=2导致Mixtral模型无法提供推理服务。

https://github.com/vllm-project/vllm/issues/2619
这是一个bug报告，主要涉及vllm开发中tensor-parallel大于1时出现的问题。由于pydantic版本过高，导致运行时出现错误，用户希望寻求解决方案或降级pydantic版本。

https://github.com/vllm-project/vllm/issues/2618
This is a bug report related to a small refactor in the async_llm_engine codebase, possibly due to a minor issue causing an eyesore.

https://github.com/vllm-project/vllm/issues/2617
这是一个bug报告类型的issue，涉及主要对象是vllm模型与Triton的集成。由于模型生成多个响应，而使用generate()方法无法处理该情况，导致出现错误信息。

https://github.com/vllm-project/vllm/issues/2616
这是一个用户提出的需求，主要涉及"swap_blocks"的单元测试。由于可能测试覆盖不完整，用户请求批准执行剩余的工作。

https://github.com/vllm-project/vllm/issues/2615
这是一个用户提出需求的类型，主要涉及VLLM构建过程中torch版本设置的问题。由于当前的构建流程会自动下载并安装指定版本的torch，用户希望了解如何更改所需的torch版本并探究为何VLLM在源代码构建中总是下载torch等包而非使用已安装的版本。

https://github.com/vllm-project/vllm/issues/2614
这个issue类型是提出需求，主要对象是vLLM中的KV cache management。由于需要实现自动前缀缓存，提出了管理KV块的新方案，以及与KV块Trie相比的设计优势。

https://github.com/vllm-project/vllm/issues/2613
这是一个用户提出需求的问题，主要涉及如何获取生成的文本中第一个token对应的logit值。由于用户希望获取特定token的logit值而不是采样token，所以寻求方法实现这一需求。

https://github.com/vllm-project/vllm/issues/2612
这是一个bug报告，主要涉及vllm中的prefix功能，可能是由于最新版本中的某些更改导致，导致了运行时出现了assert错误。

https://github.com/vllm-project/vllm/issues/2611
这是一个bug报告，主要涉及输出中包含EOS tokens的问题，原因是需要扩展标志来支持停止令牌ID。

https://github.com/vllm-project/vllm/issues/2610
该issue属于功能需求类型，主要涉及到为OpenAI入口点添加Lora接口。

https://github.com/vllm-project/vllm/issues/2609
这是一个Bug报告类型的Issue，主要涉及的对象是CI测试环境。由于缺少libcuda.so文件导致CI测试失败。

https://github.com/vllm-project/vllm/issues/2607
这是一个功能需求报告，主要涉及了"Speculative Decoding"功能。由于该功能还在实验阶段，并不支持一些特定功能，因此用户可能会遇到一些限制性的问题。

https://github.com/vllm-project/vllm/issues/2606
这是一个关于bug的报告，主要涉及的对象是vllm中的`get_beam_search_score`方法实现，问题是因为计算beam分数时错误地包含了提示长度，导致长度惩罚未能正确应用。

https://github.com/vllm-project/vllm/issues/2605
这是一个需求类型的issue，主要涉及构建速度较慢的punica kernels，并建议默认情况下不构建它们。

https://github.com/vllm-project/vllm/issues/2604
这是一个关于构建时间过长的问题，主要涉及应用LoRA后构建时间显著增加，建议只在生成最终软件时构建punica，而不是默认情况下构建。

https://github.com/vllm-project/vllm/issues/2603
该issue类型是需求报告，主要涉及的对象是多LoRA功能的文档内容。

https://github.com/vllm-project/vllm/issues/2602
这是一个用户提出需求的类型，主要涉及的对象是对多LoRA支持扩展到更多体系结构。原因是目前仅支持Llama和Mistral体系结构，用户希望扩展到Yi、Qwen、Phi和Mixtral等架构。

https://github.com/vllm-project/vllm/issues/2601
该issue的类型是讨论提议，主要涉及多个LoRA与量化模型结合的问题。造成这个讨论的原因是多LoRA可能无法与量化模型直接兼容，需要先对基础模型的输出进行去量化处理。

https://github.com/vllm-project/vllm/issues/2600
这个issue是一个功能需求提议，主要涉及如何将多个LoRA功能与OpenAI服务器集成。由于目前无法通过vLLM OpenAI服务器查询LoRAs，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/2599
这是关于修改vllm项目中的vocab_size参数的建议。

https://github.com/vllm-project/vllm/issues/2598
这是一个用户需求，要求在建模代码中添加来自Stable LM 2的config.qkv_bias。这个需求主要涉及建模代码的修改。

https://github.com/vllm-project/vllm/issues/2597
这是一个用户提出需求的 issue，主要涉及的对象是 vLLM 的 OpenAI api 服务器，用户希望减少主句的长度以便更容易扩展服务器功能。

https://github.com/vllm-project/vllm/issues/2596
这是一个bug报告，该问题涉及主要对象为vllm库中的offline inference功能。这个问题的原因是generate()函数的参数'prefix_pos'在当前版本中并不支持，导致用户无法成功运行例子代码。

https://github.com/vllm-project/vllm/issues/2595
这是一个bug报告，该问题涉及的主要对象是LLM引擎。由于无法处理请求，可能是由于引擎出现了性能问题或者其他异常导致的。

https://github.com/vllm-project/vllm/issues/2594
这是一个bug报告，主要涉及的对象是vllm中的paged attention功能。此问题是由于paged attention不支持特定block_size（例如64/128/156）导致的，用户希望未来是否会支持这些block_size或者是否需要手动实例化核函数来解决。

https://github.com/vllm-project/vllm/issues/2592
这个issue类型为用户提出需求，该问题主要涉及AWS Sagemaker Docker，用户提出需求提供一个Docker用于在AWS Sagemaker中部署vLLM，以增加vLLM的使用率。

https://github.com/vllm-project/vllm/issues/2591
这是一个用户提出需求的类型，主要对象是"Jais"。由于内容为空，用户可能在提交issue时遗漏了关键信息或者出现了数据丢失，导致无法准确定义具体问题。

https://github.com/vllm-project/vllm/issues/2590
这是一个Bug报告类型的issue，主要涉及对象是VLLM模型加载功能。原因可能是加载实现导致了无法加载指定模型，从而出现了错误。

https://github.com/vllm-project/vllm/issues/2589
这是一个fix类型的issue，主要涉及修正命名和许可证问题。由于上一个PR存在一些问题，导致了命名和许可证的错误。

https://github.com/vllm-project/vllm/issues/2588
这是一个关于功能需求的issue，主要涉及vllm的`max_concurrent_workers`参数被禁用了的问题，用户想了解为什么这个参数被禁用了。

https://github.com/vllm-project/vllm/issues/2587
这是一个用户提出需求的issue，主要涉及的对象是vllm模型的内存管理。由于vllm模型的内存是预分配的，用户想知道如何测量在特定序列长度下的最大GPU内存消耗。

https://github.com/vllm-project/vllm/issues/2586
这是一个用户提出需求的类型的issue，主要涉及内容是如何部署vLLM模型与KServe，可能由于缺乏相关教程或指导而提出。

https://github.com/vllm-project/vllm/issues/2585
该issue为用户提出需求类型，涉及主要对象为VLLM项目的支持。

https://github.com/vllm-project/vllm/issues/2584
这个issue是关于文档错误的bug报告，涉及的主要对象是模型支持表，由于最新文档缺少Supported Models Table，导致无法正确构建该文档的问题。

https://github.com/vllm-project/vllm/issues/2583
这是一个bug报告，该问题单涉及的主要对象是创建OptionalCUDAGuard时使用了错误的设备，导致了可能出现的bug。

https://github.com/vllm-project/vllm/issues/2582
该issue类型是bug报告，涉及的主要对象是`vllm` Dockerfile中的目标。这个问题可能是由于意外的更改导致用户无法按照之前的步骤构建`vllm`目标。

https://github.com/vllm-project/vllm/issues/2581
这是一个功能需求，主要涉及到ROCm项目的编译目标设置。原因是用户希望能够在没有GPU的机器上为不同目标编译，以解决无法在没有GPU的机器上构建docker镜像的问题。

https://github.com/vllm-project/vllm/issues/2580
这是一个bug报告，涉及到在ROCm上发生RuntimeError的问题，用户运行在`rocm/pytorch:rocm5.7_ubuntu22.04_py3.10_pytorch_2.0.1`容器中遇到了此问题。

https://github.com/vllm-project/vllm/issues/2579
这是一个bug报告类型的issue，涉及主要对象是与vllm相关的benchmark功能。由于POST /generate请求返回404 Not Found，用户请求帮助解决此问题。

https://github.com/vllm-project/vllm/issues/2578
这是一个用户提出需求的 issue，主要涉及的对象是vllm加速的chatglm3模型，在输出时不带上prompt只输出结论这一功能。

https://github.com/vllm-project/vllm/issues/2577
这是一个bug报告类型的issue，涉及到LangChain默认ReAct pipeline中在streaming和非streaming模式下停止标记行为不一致的问题。

https://github.com/vllm-project/vllm/issues/2576
这是一个用户需求类型的issue，主要涉及的对象是internlm2-20b模型。

https://github.com/vllm-project/vllm/issues/2575
这是一个标题为"1"的空内容issue，属于用户提出需求的类型，主要涉及的对象是vllm项目。

https://github.com/vllm-project/vllm/issues/2573
这是一个关于需要支持生产级服务器用于推理的问题，讨论了在使用vLLM堆栈中遇到的性能瓶颈和扩展性问题。

https://github.com/vllm-project/vllm/issues/2572
这是一个bug报告，该问题涉及到chatglm3使用时的并行设置，由于并行值大于1时产生错误结果，推测可能是并行计算导致的bug。

https://github.com/vllm-project/vllm/issues/2571
这是一个bug报告，涉及主要对象是vLLM代码构建过程，由于添加了多LoRA支持导致构建速度显著变慢，建议考虑默认关闭 punica。

https://github.com/vllm-project/vllm/issues/2570
这是一个bug报告，涉及的主要对象是API中的max_tokens参数。由于最近的更改，导致传递max_tokens=None时出现崩溃。

https://github.com/vllm-project/vllm/issues/2569
这是一个功能需求类型的issue，主要涉及支持使用transformers-neuronx在Inferentia上进行模型推理。

https://github.com/vllm-project/vllm/issues/2568
该问题类型是Bug报告，主要对象是VLLM中使用Mistral7B模型时在批处理多个句子和单独处理句子时产生不一致的文本生成结果。

https://github.com/vllm-project/vllm/issues/2567
这是一个需求类型的issue，主要涉及lint工具格式化python文件的问题，可能由于部分文件没有使用yapf格式化而导致。

https://github.com/vllm-project/vllm/issues/2566
该issue类型为性能优化，主要涉及到vLLM中的模型推理速度问题，原因是使用cublas代替`torch.matmul`可能提高推理效率。

https://github.com/vllm-project/vllm/issues/2565
这个issue属于bug报告，主要涉及到vLLM模型在计算最大序列长度时未考虑到绳子缩放，导致无法使用完整上下文，最终默认值为4096。

https://github.com/vllm-project/vllm/issues/2564
这是一个讨论issue，涉及到vLLM中的默认`length_penalty`设置问题。由于`length_penalty`对序列长度的惩罚设置可能会影响生成结果，用户在询问是否应将默认值设为0。

https://github.com/vllm-project/vllm/issues/2563
这是一个功能需求的issue，主要涉及到需要将CC（支持从输入嵌入生成）与主分支的最新更改合并的问题。

https://github.com/vllm-project/vllm/issues/2562
这是一个用户提出需求的issue，主要涉及的对象是对OpenAI API参数的更新。

https://github.com/vllm-project/vllm/issues/2561
这是一个bug报告，涉及Mystral AWQ quantised模型在A10 GPU上加载时出现OOM错误的问题。

https://github.com/vllm-project/vllm/issues/2560
这是关于新功能请求的issue，主要涉及RadixAttention技术实现的KV缓存重用，以及与持续批处理和分页注意力等现有技术的兼容性。

https://github.com/vllm-project/vllm/issues/2559
这是一个关于vLLM是否支持自我推断解码的问题，涉及的主要对象是vLLM模型。用户提出了关于vLLM未来支持自我推断解码的疑问，并询问是否有相关的规划。

https://github.com/vllm-project/vllm/issues/2558
这是一个bug报告，主要涉及到在加载使用h2o llm studio微调训练的模型时出现的问题。由于未提供详细的Traceback信息，用户正在寻求关于加载模型失败的帮助。

https://github.com/vllm-project/vllm/issues/2557
该问题属于bug报告，涉及主要对象为AsyncLLMengine在设置不同temperature时产生不一致输出，可能是由于AsyncLLMengine的设置不正确导致不同于LLMengine输出的症状。

https://github.com/vllm-project/vllm/issues/2556
这是一个关于性能问题的bug报告，主要涉及GPU利用率下降的情况。这可能是由于GPU内存碎片过多导致的。

https://github.com/vllm-project/vllm/issues/2555
这是一个bug报告，涉及的主要对象是Beam search running OOM on A100。由于beam search在初始化引擎时发生OOM，可能是由于模型大小及序列长度过大导致内存不足。

https://github.com/vllm-project/vllm/issues/2554
这是一个bug报告类型的issue，涉及到vLLM模型在GPU内存利用方面存在问题，导致CUDA out of memory错误。

https://github.com/vllm-project/vllm/issues/2553
这个issue是关于代码注释的需求，主要涉及调用context_attention_fwd函数的频率问题，由于缺乏文档说明导致用户对代码行为感到困惑。

https://github.com/vllm-project/vllm/issues/2552
这个issue是关于bug报告，主要涉及的对象是`benchmark_serving.py`文件。导致这个问题的原因是进度条问题，以及需要支持HTTPS端点的需求。

https://github.com/vllm-project/vllm/issues/2551
这个issue是关于优化性能，而不是bug报告，主要涉及到AWQ的上下文处理。

https://github.com/vllm-project/vllm/issues/2549
这是一个用户提出需求的issue，主要涉及vLLM库的KV Cache和获取LogProbs功能。由于用户需要在大量序列上运行推断，并获取它们的log probabilities，他们问及库是否能自动优化KV Cache以重复利用共同前缀的计算。

https://github.com/vllm-project/vllm/issues/2548
这是一个用户提出需求的issue，主要涉及的对象是针对vLLM的Grammar from Llamacpppython，用户希望能够使用该Grammar与vLLM结合，以便获取所需的响应。

https://github.com/vllm-project/vllm/issues/2547
该issue是一个用户需求的提出，用户想要在通过openai服务器动态运行模型时通过传递args来覆盖HF配置类中的现有参数，但目前不支持这一功能。

https://github.com/vllm-project/vllm/issues/2546
该issue为用户提出需求，请求寻找更高性能的服务器来运行DistilWhisper模型。

https://github.com/vllm-project/vllm/issues/2545
这是一个bug报告，主要涉及的对象是vllm项目中的一个函数。由于一个小错误导致了无法将端口转换为整数值的问题。

https://github.com/vllm-project/vllm/issues/2544
这是一个bug报告，主要涉及对象是使用Python 3.10.13、torch 2.1.2和vllm 0.2.7的用户，由于CUDA版本导致了运行时错误导致出现了特定的异常情况。

https://github.com/vllm-project/vllm/issues/2543
这是一个bug报告类型的issue，主要涉及 Mixtral 8x7B 模型在使用过程中出现输出不确定性和输出为空的问题。原因可能是与模型量化以及温度参数设置相关。

https://github.com/vllm-project/vllm/issues/2542
这是一个涉及性能改进的 issue，类型不是 bug 报告。

https://github.com/vllm-project/vllm/issues/2541
这个issue类型是用户请教问题，主要对象是如何使用vllm模型。

https://github.com/vllm-project/vllm/issues/2540
这是一个bug报告，主要涉及torch.distributed模块的初始化过程中出现的数值转换错误，导致端口无法被解析为整数值，进而引发数值转换异常。

https://github.com/vllm-project/vllm/issues/2539
这是一个用户提出需求的issue，主要涉及支持Orion模型的请求。在本次问题中，用户请求支持Orion模型：https://huggingface.co/OrionStarAI/Orion14BBase。

https://github.com/vllm-project/vllm/issues/2538
这是一个bug报告，涉及到的主要对象是代码输出中包含了结束符eos token。这个问题可能是由于代码输出中不应该包含eos token，导致用户提出疑问并寻求解决的帮助。

https://github.com/vllm-project/vllm/issues/2537
这是一个用户提出需求的issue，主要涉及支持Orion模型。由于用户希望支持Orion模型，因此提出了该需求。

https://github.com/vllm-project/vllm/issues/2536
这是一个用户提出需求的类型的issue，主要涉及添加Orion模型到项目中。由于用户需要在项目中增加Orion模型，所以提出了这个需求。

https://github.com/vllm-project/vllm/issues/2535
这是一个用户提出需求的类型，主要对象是为 vllm 添加一个 Orion 模型。这个需求可能是为了增加模型选择，并提高模型的多样性。

https://github.com/vllm-project/vllm/issues/2534
这是一个用户提出需求的issue，主要涉及的对象是希望支持DeepSeekMoE项目。由于该项目目前不支持DeepSeek MoE，用户希望看到该支持的功能。

https://github.com/vllm-project/vllm/issues/2532
这是一个需求修改的 issue，主要涉及改进基准测试服务。可能是因为服务性能不佳或者需要调整功能实现。

https://github.com/vllm-project/vllm/issues/2531
这个issue类型是bug报告，主要涉及的对象是从pydantic v1迁移到v2，并由于版本更新导致了错误代码传播问题。

https://github.com/vllm-project/vllm/issues/2530
这是一个bug报告，主要涉及的对象是vllm下的`tensor_model_parallel_gather()`函数。由于一个小bug导致了在pipeline并行度大于1的情况下`tensor_model_parallel_gather()`无法正常工作，需要修复。

https://github.com/vllm-project/vllm/issues/2529
这个issue类型为功能需求提议，涉及的主要对象是OpenAI completion protocol。由于缺乏对`prompt`输入参数多样性的支持，用户无法按照需求提供不同形式的输入数据导致此需求被提出。

https://github.com/vllm-project/vllm/issues/2528
这是一个bug报告，主要涉及修复`tensor_model_parallel_gather()`中的一个小错误，导致在管道并行度大于1时无法正常工作。

https://github.com/vllm-project/vllm/issues/2527
这是一个bug报告，涉及的主要对象是支持internlm2模型的功能。由于使用TP2输出混乱的文本，可能是导致这个问题的原因。

https://github.com/vllm-project/vllm/issues/2526
这是一个关于提出需求的RFC（Request For Comments）类型的issue，主要涉及的对象是vLLM上的推理速度，由于现有的技术环境中，大型语言模型（LLMs）在生成人工智能（GenAI）工作负载和模型中越来越受到关注和普及，作者提出了在Intel平台上通过Intel® Extension for PyTorch*来加速vLLM推理的需求。

https://github.com/vllm-project/vllm/issues/2525
该issue类型为用户提出需求，主要涉及的对象是InternLM2PreTrainedModel。这个问题是用户希望vllm和tp能支持InternLM2PreTrainedModel，因为它需要支持200k的输入和输出来构建一个模型服务。

https://github.com/vllm-project/vllm/issues/2524
这是一个用户询问问题的类型，主要涉及VLLM模型中的预填充和解码阶段，用户想了解如何分开进行测试。

https://github.com/vllm-project/vllm/issues/2523
这是一个bug报告，涉及的主要对象是`scheduler.running`中的deque数据结构。这个问题是由于CC在更改调度器序列组列表为deque时，未将`running` deque转换回列表造成的。

https://github.com/vllm-project/vllm/issues/2522
这是一个用户提出需求的issue，主要涉及VLLM中的通信操作的改进，需要添加一个新的参数`group`来指定通信的目标组。 

https://github.com/vllm-project/vllm/issues/2521
这是一个bug报告，问题涉及无法加载模型，可能由于某些原因导致的加载失败。

https://github.com/vllm-project/vllm/issues/2520
这个issue类型为bug报告，涉及到vllm在Windows WSL Ubuntu下无法响应openapi请求，导致出现"POST /v1/chat/completions HTTP/1.1" 404 Not Found错误。

https://github.com/vllm-project/vllm/issues/2519
这是一个用户提出需求的问题，主要对象是Prefix Cache support的使用。用户想要了解如何衡量生成带有和不带有前缀响应之间的性能提升，希望有人提供一个示例来演示性能提升。

https://github.com/vllm-project/vllm/issues/2518
这是一个bug报告类型的issue，涉及主要对象是vLLM的rope scaling功能。由于无法成功实现rope scaling功能，导致使用长提示进行推理时仍然出现警告提示。

https://github.com/vllm-project/vllm/issues/2517
这是一个bug报告，主要涉及的对象是Prefix Caching错误，由于需要在Turing GPUs上使用较小的块大小来解决与baichuan模型的错误相关的问题。

https://github.com/vllm-project/vllm/issues/2516
这个issue是关于代码重构，涉及到OpenAI完成API和新添加的前缀缓存功能。原因是为了改进现有代码的结构和性能。

https://github.com/vllm-project/vllm/issues/2515
这是一个用户提出需求的issue，主要涉及服务器的api server，用户期望支持prefix caching功能以处理特定的系统提示字符串。

https://github.com/vllm-project/vllm/issues/2514
这个issue是关于功能需求的，涉及支持每个请求的种子，主要对象是请求的唯一性标识`request_id`。由于`request_id`唯一性可能存在问题，可能需要分配一个保证唯一的内部id，以解决如何在生成之前设置种子的问题。

https://github.com/vllm-project/vllm/issues/2513
这是一个关于bug报告的issue，主要涉及vLLM中的prefix caching error with baichuan模型问题。由于更改`examples/offline_inference_with_prefix.py`中的llm行导致错误，用户怀疑可能与`Alibi`相关。

https://github.com/vllm-project/vllm/issues/2512
这是一个bug报告，涉及主要对象是修复本地safetensors模型加载的问题。

https://github.com/vllm-project/vllm/issues/2511
这是一个功能改进的issue，主要涉及到"Prefix caching and deallocation mechanism"。由于block.ref_count不适合表示多少个序列共享一个前缀，因此引入了新的seq_ref_counter来处理此问题。

https://github.com/vllm-project/vllm/issues/2510
这是一个优化建议类型的issue，主要涉及到优化采样算法中的累加运算。该问题由于性能考量，提出了通过使用上三角矩阵相乘加速累加运算的方法。

https://github.com/vllm-project/vllm/issues/2509
这是一个关于性能调优的issue，主要涉及到vllm框架中的`max_num_batched_tokens`参数。通过调整该参数，用户尝试最大化内存利用率和吞吐量，在保证模型稳定性的前提下寻求最佳调优方案。

https://github.com/vllm-project/vllm/issues/2508
这是一个提出需求的类型，该问题涉及的主要对象是vllm。由于用户对于vllm是否能应用其解码方法感到困惑。

https://github.com/vllm-project/vllm/issues/2507
这是一个bug报告，涉及到代码编译时遇到错误导致发布版本0.2.8包时出现问题。

https://github.com/vllm-project/vllm/issues/2506
这是一个bug报告，主要涉及的对象是Vllm项目中的模型架构。这个问题是由于Model architectures中包含了不被支持的架构导致的数值错误，用户提出了一个关于支持InternLM2的问题。

https://github.com/vllm-project/vllm/issues/2505
这是一个需求提出的issue，主要涉及的对象是benchmark serving。由于下载了完整的sharegpt数据集，但仅发送了20个提示，因此建议考虑使用更小的数据集或生成虚拟输入。

https://github.com/vllm-project/vllm/issues/2504
这个issue类型是用户提出了疑问，主要涉及对象是vLLM模型和HuggingFace模型，用户想了解这两者结构是否完全相同。

https://github.com/vllm-project/vllm/issues/2503
这是一个用户提出需求的issue，涉及主要对象是代码中硬编码的`device="cuda"`，导致不易支持其他设备的问题。

https://github.com/vllm-project/vllm/issues/2502
这是一个bug报告，涉及的主要对象是vllm工具。由于地址家族不受协议支持，导致无法正常执行程序。

https://github.com/vllm-project/vllm/issues/2501
该issue类型为代码优化，主要涉及控制消息的广播逻辑简化。这是因为之前的广播逻辑复杂且混乱，需要进行统一和简化。

https://github.com/vllm-project/vllm/issues/2500
这是一个bug报告，涉及的主要对象是在prepare_prompt函数中调用的max()函数。由于传入max()函数的参数是空列表导致崩溃，可能是因为未正确处理空序列而引发的问题。

https://github.com/vllm-project/vllm/issues/2499
这是一个需求类型的issue，主要对象是completion api代码。开发者对代码的可读性和维护性提出了改进建议，以便更好地支持批量完成功能。

https://github.com/vllm-project/vllm/issues/2498
这是一个bug报告，涉及的主要对象是 ROCm 平台中的一些单元测试失败，需要调整测试代码来修复失败的测试。

https://github.com/vllm-project/vllm/issues/2497
这个issue类型是功能需求，涉及主要对象是集成Marlin内核用于Int4 GPTQ推理，用户提出了关于GPTQ模型运行、性能优化和支持特定功能的需求。

https://github.com/vllm-project/vllm/issues/2496
这个issue属于bug报告类型，主要涉及vllm项目在使用单个GPU时初始化进程组的问题，由于这一问题导致了不必要的性能损耗。

https://github.com/vllm-project/vllm/issues/2495
该issue属于用户提出需求类型，主要涉及的对象是向vLLM贡献新模型Qwen2。

https://github.com/vllm-project/vllm/issues/2494
这是一个bug报告，主要涉及问题是在offline_inference_with_prefix.py中的缓存示例代码存在问题，由于LLM.generate调用批处理所有提示将导致缓存问题。

https://github.com/vllm-project/vllm/issues/2493
这是一个bug报告，涉及的主要对象是在使用AMD Radeon 780M GPU尝试构建vLLM时遇到了CUDA_HOME未找到的错误，导致构建不成功。

https://github.com/vllm-project/vllm/issues/2492
这是一个用户请教问题类型的issue，主要涉及对象是如何设置`max_num_batched_tokens`和`max_num_seqs values`以达到最大推理性能，问题出现在设置不同的`max_num_batched_tokens`和`max_num_seqs`时导致输出token不一致，原因可能是参数设置不当。

https://github.com/vllm-project/vllm/issues/2491
这是一个bug报告，主要涉及的对象是vllm框架在GPU利用率方面存在显著波动，导致性能吞吐量受限。

https://github.com/vllm-project/vllm/issues/2490
这是一个功能需求类型的issue，主要涉及的对象是vllm代码库，用户提出需要添加对最新transformer更新中引入的prompt_lookup_num_tokens的支持，以提高推断速度。

https://github.com/vllm-project/vllm/issues/2434
这是一个功能需求的issue，涉及的主要对象是VLLM库中的Sampler类。由于用户希望在文本生成过程中动态限制token，根据每个生成步骤确定允许的token集合，因此提出了实现`allowed_tokens_fn`特性的需求。

https://github.com/vllm-project/vllm/issues/2433
该issue类型是需求提议，主要涉及vLLM的online serving benchmark脚本的重构和功能增强。原因是为了使脚本更易于使用和贡献，同时增加更多特性和支持。

https://github.com/vllm-project/vllm/issues/2432
这是一个用户提出需求的issue，涉及主要对象是日志记录功能，由于需要在项目中添加JSON格式的日志记录支持，并希望使用loguru来输出JSON格式的日志。

https://github.com/vllm-project/vllm/issues/2431
这个issue类型为bug报告，主要涉及的对象是错误信息显示，由于错误信息不准确或者不清晰导致用户提出修复错误信息的需求。

https://github.com/vllm-project/vllm/issues/2430
这是一个关于系统性能问题的用户询问，主要对象是VLLM模型在推理过程中使用显存过多导致OOM错误。

https://github.com/vllm-project/vllm/issues/2429
Issue type is a bug report, related to the Deepseek MoE model, causing garbled text generation.

https://github.com/vllm-project/vllm/issues/2428
这是一个bug报告，主要涉及的对象是变量名和权重名不匹配导致的问题。

https://github.com/vllm-project/vllm/issues/2427
这是一个 bug 报告类型的 issue，涉及的主要对象是客户端 socket 连接问题。这个问题可能是由于网络地址连接失败导致的。

https://github.com/vllm-project/vllm/issues/2426
这是一个用户提出需求的类型，主要涉及VLLM项目集成Tree Speculate，并询问是否可能进行集成。由于用户认为Tree Speculate能在加速推理方面提供痛点缓解，因此提出了此功能请求。

https://github.com/vllm-project/vllm/issues/2425
这个issue类型是文档问题报告，涉及的主要对象是快速入门指南中与模型范围相关的格式。导致这个问题的原因可能是格式错误或排版问题。

https://github.com/vllm-project/vllm/issues/2424
这个issue类型是功能需求，主要涉及到实现多步骤工作，由于需要多次调用底层模型来进行推断，但需要未来的PR来支持在调度迭代中安排多于一个令牌。

https://github.com/vllm-project/vllm/issues/2423
这是一个关于如何启用vllm默认连续批处理的问题，涉及主要对象是vllm连续批处理功能。由于新手在测试时不确定连续批处理是否正常工作，因此导致不确定测试结果。

https://github.com/vllm-project/vllm/issues/2422
这是一个bug报告，主要涉及的对象是vllm库。由于某些未知原因导致了Phi-2错误的问题。

https://github.com/vllm-project/vllm/issues/2421
这个issue属于用户提出需求类型，主要对象是获取原始的logprobs而不是经过softmax处理的logprobs。由于用户希望获取原始的logprobs数据而不是经过softmax处理后的数据，可能是为了进行个性化的后续处理或分析。

https://github.com/vllm-project/vllm/issues/2420
该issue类型为用户寻求关于如何在vLLM中运行starchat模型时所需的chat template jinja文件，涉及主要对象为starchat模型。由于用户不清楚如何获取该文件，所以提出了这个问题。

https://github.com/vllm-project/vllm/issues/2419
这是一个bug报告，主要涉及到vllm下的Qwen-14B-Chat-AWQ与不同tensor_parallel_size参数配置的兼容性问题，导致ray worker在tensor_parallel_size=2时出现错误。

https://github.com/vllm-project/vllm/issues/2418
这个issue类型为bug报告，主要涉及VLLM中模型最大序列长度导致的数值错误，原因是模型的最大序列长度大于KV缓存中可存储的最大标记数量，需要调整gpu_memory_utilization或max_model_len参数。

https://github.com/vllm-project/vllm/issues/2417
这是一个bug报告，主要涉及vLLM模型在tensor_parallel_size >= 2时生成模型出现故障的问题。

https://github.com/vllm-project/vllm/issues/2416
这个issue类型是bug报告，主要涉及的对象是vLLM的gpu资源分配问题，由于ray的placement_group与vLLM的placement_group冲突导致了问题。

https://github.com/vllm-project/vllm/issues/2415
这是一个bug报告，涉及主要对象是vllm和transformers模型的预测结果差异。导致这个问题的原因是vllm和transformers默认的top_k值不同所导致的症状。

https://github.com/vllm-project/vllm/issues/2414
这是一个Bug报告，主要涉及的对象为使用vllm库中的openai.api_server时运行curl命令返回Connection refused错误。由于何种原因导致此错误目前尚不明确。

https://github.com/vllm-project/vllm/issues/2413
这是一个bug报告类型的issue，涉及的主要对象是vllm 0.2.7版本和Mixtral8x7BInstructv0.1GPTQ模型。由于使用0.2.7版本导致模型运行时内存耗尽，而在0.2.6版本中工作正常。

https://github.com/vllm-project/vllm/issues/2412
这是一个关于代码实现细节的问题，涉及到VLLM模型中key_block_shape和value_block_shape的形状不一致的情况。

https://github.com/vllm-project/vllm/issues/2411
这是一个关于构建vllm源码时遇到的问题的用户需求类型的issue，涉及主要对象为在没有GPU的机器上构建vllm并在拥有GPU的机器上运行，原因是因为缺少GPU导致的构建错误。

https://github.com/vllm-project/vllm/issues/2410
这是一个bug报告issue，主要涉及vllm这个模型在生成回答时出现了错误的情况，可能由于模型训练不足或参数设置问题导致生成的回答不令用户满意。

https://github.com/vllm-project/vllm/issues/2409
这个issue是关于修复或跳过ROCm平台上一些单元测试的问题，其中涉及到`test_activation.py`、`test_attention.py`和`test_cache.py`三个测试文件的调整，以及跳过`test_pos_encoding.py`文件中的测试。导致这个问题的原因是部分单元测试在ROCm平台上失败了。

https://github.com/vllm-project/vllm/issues/2408
这是一个bug报告，涉及到ipv4 ipv6双栈地址问题，由于`socket.getaddrinfo`仅返回IPv6地址，导致了bug。

https://github.com/vllm-project/vllm/issues/2407
这是一个bug报告，涉及的主要对象是vllm工具中的utils.py文件。由于无法在启动时获取本地ipv4 IP，导致在`get_ip()`函数中出现`socket.gaierror: [Errno 2] Name or service not known`的错误。

https://github.com/vllm-project/vllm/issues/2406
这是一个bug报告，涉及主要对象为vLLM在使用Ray时出现的错误。这个问题可能是由于在vLLM的代码中对`driver_dummy_worker`变量的处理不当导致的。

https://github.com/vllm-project/vllm/issues/2405
该issue属于用户提出需求类型，主要涉及的对象是MoE architectures中的expert parallel strategy。由于当前实现未能充分利用稀疏激活特性而使用了与密集模型相同的计算量，用户提出了在MoE中实现expert parallel策略以优化计算效率的需求。

https://github.com/vllm-project/vllm/issues/2404
这是一个bug报告，主要涉及的对象是vllm中的awqquantized 4bit Yi34B模型。由于4bit模型在长提示情况下返回第一个单词的速度较慢，但之后输出速度却快于16bit模型，用户想了解其中的原因。

https://github.com/vllm-project/vllm/issues/2403
这个issue类型是用户提出需求，主要对象是Openai接口的Chatglm模型。由于用户希望Openai接口能够自动适配或者指定模型类型而不需要手动编写jinja模板chattemplate，这导致了用户提出了关于增加主流大模型chat template功能的需求。

https://github.com/vllm-project/vllm/issues/2402
这是一个bug报告，主要涉及VLLM模型中的Yi-34B-200K和max_position_embedding参数，可能是由于max_position_embedding设置不当导致输出结果异常。

https://github.com/vllm-project/vllm/issues/2401
这是一个性能优化相关的issue，主要涉及到在prefill阶段支持MQA/GQA的问题。原因是xformers目前不支持MQA/GQA，所以需要对`key`和`value`进行扩展，才能正确计算结果。

https://github.com/vllm-project/vllm/issues/2400
这是一个用户提出需求的Issue，主要涉及请求参数控制。

https://github.com/vllm-project/vllm/issues/2399
这是一个用户提出需求的 issue，主要对象涉及服务器端的指标收集，用户提出希望通过 `/metrics` 端点支持“平均首个令牌延迟”的功能。

https://github.com/vllm-project/vllm/issues/2398
这个issue是bug报告，涉及到sampler，由于vocab_size不一致导致decode错误。

https://github.com/vllm-project/vllm/issues/2397
这是一个用户提出的需求类型的issue，主要对象是日志输出格式。用户需求将日志输出配置为JSON格式，以便更好地解析日志。

https://github.com/vllm-project/vllm/issues/2396
这是一个关于功能需求的issue，主要涉及LLMEngine模块下的batching和streaming操作，用户提出了如何在处理多个请求，同时实现流式处理的问题。

https://github.com/vllm-project/vllm/issues/2395
该issue是关于在AWS EKS k8's pod上使用2个A10 GPU时出现内存问题的bug报告。

https://github.com/vllm-project/vllm/issues/2394
这是一个功能请求，涉及Mixtral的混合任务卸载技术。

https://github.com/vllm-project/vllm/issues/2393
这个issue属于改进需求类型，主要对象是vllm-openai docker镜像，因为当前镜像使用的Cuda版本为12.1，导致在不同GPU驱动程序下出现版本不一致的问题，希望更新镜像以支持从11.8版本及更高的动态cuda版本。

https://github.com/vllm-project/vllm/issues/2391
这是一个bug报告，涉及到VLLM项目中的内存分配问题，导致无法加载模型文件并出现内存分配错误。

https://github.com/vllm-project/vllm/issues/2390
这是一个bug报告，主要涉及Baichuan模型在部署时使用OpenAI API生成诗歌时出现的问题。原因是Baichuan模型未提供chat_template导致模型自问自答，需要将其转换为Jinja格式以使tokenizer正常工作。

https://github.com/vllm-project/vllm/issues/2389
这是一个bug报告类型的issue，涉及到Baichuan模型的处理方式导致了模型回复异常，用户提出了需要将Baichuan自定义函数转换为Jinja格式的需求。

https://github.com/vllm-project/vllm/issues/2388
这是一个用户需要提交内容的issue，涉及的主要对象是prefix。原因可能是用户想提交特定内容给VLLM，并寻求相关的帮助和指导。

https://github.com/vllm-project/vllm/issues/2387
该issue类型为用户提出需求，并涉及到设置vllm中的CUDA_VISIBLE_DEVICES环境变量；由于用户想要在另一块GPU上运行vllm，因此需要指定CUDA_VISIBLE_DEVICES环境变量来实现这一需求。

https://github.com/vllm-project/vllm/issues/2386
这是一个bug报告，主要涉及vllm关于加载合并的Mistral 8x7b模型时失败的问题。

https://github.com/vllm-project/vllm/issues/2385
该issue属于重命名需求，主要涉及到“phi_1_5”变量命名。这个需求是由于vLLM同时支持phi和phi2模型，因此需要统一变量命名。

https://github.com/vllm-project/vllm/issues/2384
该issue类型为代码优化，主要对象是attention模块中的未使用代码。

https://github.com/vllm-project/vllm/issues/2383
这是一个bug报告类型的issue，主要涉及的对象是vllmopenai docker image 0.2.7，由于配置错误导致出现错误，需更新Phi建模。

https://github.com/vllm-project/vllm/issues/2382
这是一个关于CUDA OOM的问题报告，涉及主要对象是Mixtral 8x7B AWQ Tensor Parallelism 2。由于GPU内存利用率设置和缓存块计算的不准确导致运行出现了CUDA OOM错误。

https://github.com/vllm-project/vllm/issues/2381
这是一个bug报告，主要涉及的对象是Qwen-14B-Chat-Int4 GPTQ model using vLLM。导致这个问题的原因是GPTQ model在vLLM下没有加速。

https://github.com/vllm-project/vllm/issues/2380
这是一个关于技术理解的问题，主要涉及到vllm中的LLM类，用户疑惑LLM类是否天然异步，并询问如何异步调用LLM模型的问题。造成症状的原因可能是用户对LLM类的工作方式有误解。

https://github.com/vllm-project/vllm/issues/2379
这是一个bug报告，该问题涉及到权重加载逻辑。由于`tp_size`大于`num_kv_heads`导致了错误的响应。

https://github.com/vllm-project/vllm/issues/2378
这是一个功能需求的Issue，主要涉及将vLLM在Intel GPU设备上通过SYCL执行的原型开发。

https://github.com/vllm-project/vllm/issues/2377
这是一个bug报告，主要涉及程序在高吞吐量设置下使用急切模式时的性能问题，其根本原因是在每次迭代中过度填充块表导致性能瓶颈。

https://github.com/vllm-project/vllm/issues/2376
这是一个bug报告，主要涉及在尝试用法语查询时，响应内容被截断。可能是由于查询参数设置不正确导致了截断响应的问题。

https://github.com/vllm-project/vllm/issues/2375
这是一个用户提出需求的类型issue，主要涉及到VLLM模型中的文本生成算法，用户提出了关于改进文本生成质量的需求。

https://github.com/vllm-project/vllm/issues/2374
这个issue类型是建议改进，主要针对V100 GPU用户提示错误信息改进。原因是建议增加对于初学者的友好指导，以提供更周到的建议。

https://github.com/vllm-project/vllm/issues/2373
这是一个关于性能优化的问题，主要涉及到VLLM和TRTLLM两个框架对AWQ量化推断的比较，用户想知道VLLM中AWQ GEMM内核性能差的原因以及是否可以将TRTLLM中的内核计算移植到VLLM中以提升性能。

https://github.com/vllm-project/vllm/issues/2372
这是一个需求新增类型的issue，主要涉及的对象是为vllm增加新模型StableLM3B，原因可能是支持稳定性增强和功能扩展。

https://github.com/vllm-project/vllm/issues/2371
这是一个bug报告，涉及的主要对象是CC改变调度程序以使用deque而不是列表，在函数`abort_seq_group`中引入了一个bug。这个bug是由于尝试在迭代过程中对deque进行改变而导致的异常。

https://github.com/vllm-project/vllm/issues/2370
这是一个用户提出需求的issue， 主要涉及的对象是vllm， 用户询问如何在vllm中使用Splitwise插件，由于关键字搜索不到指引，发帖寻求帮助。

https://github.com/vllm-project/vllm/issues/2369
这是一个bug报告，涉及到更新文档内容。由于存在拼写错误，需要进行小的澄清更改。

https://github.com/vllm-project/vllm/issues/2368
这个issue属于优化建议类型，主要涉及了在注意力机制内核中在FP32中计算logits*V来提高数值精度。原因是在之前的做法中，将logits从FP32转换为BF16/FP16，然后计算dot(logits, V)，而现在的做法是将V从BF16/FP16转换为FP32，然后计算dot(logits, V)。

https://github.com/vllm-project/vllm/issues/2367
这是一个Bug报告，主要涉及对象是vllm模型的性能表现。由于v2.0.7版本速度变慢导致每秒token数量下降，用户报告此问题。

https://github.com/vllm-project/vllm/issues/2366
这是一个bug报告，主要涉及vllm源码的构建问题，用户提出希望有一个预构建的开发容器，其中vllm以可编辑模式安装，不需要编译所有CUDA kernels，以提升开发体验。

https://github.com/vllm-project/vllm/issues/2364
这是一个用户提出需求的issue，主要涉及vllm项目中的Phi2模型，用户希望通过API计算prompt在整个数据集上的perplexity。

https://github.com/vllm-project/vllm/issues/2363
这是一个bug报告，主要涉及的对象是torch.cuda内存管理。由于内存不足而导致CUDA out of memory错误。

https://github.com/vllm-project/vllm/issues/2362
这是一个关于代码实现细节的问题，主要涉及PagedAttention模块中的forward实现，已经向xops.memory_efficient_attention_forward传递query时使用'query.unsqueeze(0)'而不是'query.unflatten(0, (batch_size, seq_len)'，导致疑惑问题。

https://github.com/vllm-project/vllm/issues/2360
这是一个功能需求的issue，主要涉及到OpenAI API的重构和函数调用的实现。原因是为了使代码更清晰易修改，并将完成部分视为传统功能。

https://github.com/vllm-project/vllm/issues/2359
这是一个bug报告类型的issue，主要涉及MixtralForCausalLM模型在运行中产生了NaN数值，可能是由于代码实现中的bug导致。

https://github.com/vllm-project/vllm/issues/2358
这是一个关于bug报告的issue，涉及的主要对象是vllm的使用过程。由于在使用tensor parallel时，未能释放gpu资源导致第二个作业无法创建cluster。

https://github.com/vllm-project/vllm/issues/2357
这是一个建议提案类型的issue，主要涉及调度策略以提高吞吐量，因为通过长度排序序列会减少填充标记的百分比，而引入了一个新的调度策略来实现这一点。

https://github.com/vllm-project/vllm/issues/2356
这是一个反馈问题类型的issue，主要涉及的对象是项目中的某个功能或设计。

https://github.com/vllm-project/vllm/issues/2355
这是一个关于持续集成环境设置的Issue，涉及到GPU CI环境的基本设置，主要对象是持续集成测试环境。原因是由于开发人员想要运行测试以及基准测试，当前的问题是关于调试模型输出和内存调优方面的问题，开发人员希望得到帮助解决这些问题。

https://github.com/vllm-project/vllm/issues/2354
这是一则关于功能需求的问题，涉及vllm的paged attention以及prompt cache的应用。用户询问是否可以通过缓存特定前缀的kvcache状态来优化模型性能。

https://github.com/vllm-project/vllm/issues/2353
这是一个用户提出需求的 issue，主要涉及 VLLM 使用快照下载私有模型的问题。

https://github.com/vllm-project/vllm/issues/2352
该issue属于bug报告类型，主要涉及vLLM下的GPTQ模型部署消耗高内存的问题，用户关注的是部署使用vLLM时消耗的GPU内存较高，希望找到原因并解决。

https://github.com/vllm-project/vllm/issues/2351
这是一个bug报告，主要涉及vllm在运行时服务器死机的问题，可能与内存泄漏和CUDA Graph相关的问题有关。

https://github.com/vllm-project/vllm/issues/2350
这是一个bug报告，主要涉及的对象是vllm库。由于运行时出现了CUDA内部断言失败的错误，可能是由于程序中的CUDA设备类型判断出现问题导致的。

https://github.com/vllm-project/vllm/issues/2349
这是一个用户提出需求的issue，该问题涉及支持SelfExtend式上下文扩展的功能。由于论文中描述了一种方法来扩展任何基于rope的模型的上下文窗口，而不需要在推断时进行微调，因此用户提出了如何在vllm中添加对此的支持。

https://github.com/vllm-project/vllm/issues/2348
这是一个bug报告，主要涉及的对象是 metallama/Llama-2-70b-chat-hf 模型。这个问题可能由于内存不足导致的OOM（Out Of Memory）错误。

https://github.com/vllm-project/vllm/issues/2347
这是一个bug报告，涉及到日志记录问题和请求统计的问题，导致最后的值持续报告给prometheus以及总是报告一个运行中的请求。

https://github.com/vllm-project/vllm/issues/2345
这是关于v0.2.7版本中存在内存泄漏的bug报告，主要涉及到RayWorkerVllm进程，可能是由于程序中某些内存未正确释放导致了不断增加的内存占用。

https://github.com/vllm-project/vllm/issues/2344
这是一个Bug报告，涉及主要对象为VLLM项目中的openai_server模块。由于模块路径无法找到导致了ModuleNotFoundError，进而导致了无法正常运行相关功能的问题。

https://github.com/vllm-project/vllm/issues/2343
这是一个bug报告，涉及的主要对象是logger的格式字符串。这个问题是由于缺少括号导致的bug，用户提出了关于格式字符串错误的问题。

https://github.com/vllm-project/vllm/issues/2342
这个issue类型为bug报告，涉及主要对象为GPU内存利用率设置参数，由于设置为0.9时实际利用率超过0.9导致问题。

https://github.com/vllm-project/vllm/issues/2341
这是一个关于用户需求的issue，主要涉及到FastAPI在运行时设置root_path参数的问题。其原因是在路径代理后，Swagger无法正确展示页面导致404错误。

https://github.com/vllm-project/vllm/issues/2340
这是一个bug报告类型的issue，主要涉及到类型提示修复矿工的问题。原因是类型提示错误导致矿工问题。

https://github.com/vllm-project/vllm/issues/2339
这是一个关于模型压缩结果偏差的bug报告，涉及到awq压缩llama 2 70bchat模型的准确性问题。由于awq压缩后的模型准确率大幅下降，用户寻求对此问题的解释或解决方案。

https://github.com/vllm-project/vllm/issues/2338
这是一个bug报告，主要涉及的对象是尝试加载模型"THUDM/chatglm6bint4"时发生了OOM异常，可能由于模型过大导致内存溢出。

https://github.com/vllm-project/vllm/issues/2337
这是一个版本升级的请求，主要涉及软件的更新和发布跟踪。

https://github.com/vllm-project/vllm/issues/2336
这是一个特性新增的issue，主要对象是vLLM中的优化拒绝采样器。由于实现了改进的拒绝采样和批处理在GPU上的操作，为了提高效率和准确性。

https://github.com/vllm-project/vllm/issues/2335
这个issue属于bug报告，主要涉及的对象是test_cache。这个问题的原因是在控制平面通信中使用NCCL替代ray，但最终却保留了kernel，导致需要回滚`test_cache`的相关更改。

https://github.com/vllm-project/vllm/issues/2334
这是一个用户提出需求的issue，主要涉及vllm是否支持从huggingface进行私有模型服务。

https://github.com/vllm-project/vllm/issues/2333
这是一个bug报告，主要涉及LLaMA V1模型加载服务器时出现加载问题的情况。这个问题可能是由于预先下载的模型数据导致服务器无法正常加载，需要手动删除checkpoint后重新下载才可以成功部署。

https://github.com/vllm-project/vllm/issues/2332
这是一个功能需求的issue，主要涉及到软件版本发布的跟踪和更新细节。

https://github.com/vllm-project/vllm/issues/2331
这是一个用户提出需求的issue，主要涉及到block_size参数。有人问及为什么block_size受到了8, 16, 32的限制，是否可以使用更大的block_size处理更长的序列。

https://github.com/vllm-project/vllm/issues/2330
这个issue属于功能需求提出，主要涉及支持2/3/8位GPTQ量化模型，提出了关于比特量化模型性能和适用性的讨论。

https://github.com/vllm-project/vllm/issues/2329
这是一个bug报告，涉及到在加载本地模型时出现“Authentication token does not exist, failed to access model”错误，原因可能是缺少身份验证令牌导致无法访问模型。

https://github.com/vllm-project/vllm/issues/2328
这是一个bug报告，涉及的主要对象是CUDA error when loading mixtral model。这个问题可能是因为加载mixtral model时出现CUDA错误，导致了报错。

https://github.com/vllm-project/vllm/issues/2327
这是一个关于性能优化和潜在改进的问题，涉及主要对象为vllm项目。用户提出了关于如何使vllm运行更快以及是否可以应用类似于GPTFast等优化方法的问题。

https://github.com/vllm-project/vllm/issues/2326
这个issue属于用户提出需求类型，主要涉及的对象是加载awq量化模型到CPU内存而不是GPU内存。这个问题可能由于用户希望在CPU上运行模型而不是GPU上而产生。

https://github.com/vllm-project/vllm/issues/2325
这是一个bug报告，主要涉及的对象是nvmlDeviceGetHandleByPciBusId()函数。该问题的原因可能是函数当前版本中未实现，导致用户无法正常使用该功能。

https://github.com/vllm-project/vllm/issues/2324
这是一个bug报告，主要涉及到CogVLM和VLLM之间的不同结果。问题可能源于网络权重的不同或代码实现的差异。

https://github.com/vllm-project/vllm/issues/2323
这是一个bug报告，主要涉及的对象是vLLM中使用CUDA Graph和torch.distributed.all_reduce时出现的内存泄漏问题。这个问题导致CPU内存持续增长，每个actor的堆内存每分钟泄漏约50MB，总泄漏量约为400MB每分钟。

https://github.com/vllm-project/vllm/issues/2322
这是一个更新依赖关系的问题，涉及主要对象是pydantic库版本。由于OpenAI更新了对pydantic的版本要求，导致需要将pydantic版本从1.10.13提升到>= 1.9.0, < 3。

https://github.com/vllm-project/vllm/issues/2321
这是一个bug报告，主要涉及到未使用的常量引起的问题。

https://github.com/vllm-project/vllm/issues/2320
这个issue类型是bug报告，主要涉及到vllm和awq quantification yi-34b-chat model，由于CUDA error导致了RuntimeError的问题。

https://github.com/vllm-project/vllm/issues/2319
这是一个关于安装autogptq出错的bug报告，涉及的主要对象是在RHEL8 ppc64le环境中使用podman安装autogptq时遇到的问题。由于文件缺失导致无法成功生成QiGen kernels，最终导致安装过程失败。

https://github.com/vllm-project/vllm/issues/2318
这是一个bug报告，主要涉及的对象是GPTQ和SqueezeLLM模型。由于GPTQ模型不支持CUDA图，导致了此问题的症状。

https://github.com/vllm-project/vllm/issues/2317
这是一个bug报告，涉及GPU不同VRAM尺寸导致OOM错误。

https://github.com/vllm-project/vllm/issues/2316
这是一个关于增加请求级别指标的需求提出的issue，涉及到优化和重构Prometheus及添加性能监控指标的功能。

https://github.com/vllm-project/vllm/issues/2315
这是一个bug报告，主要涉及的对象是Gradio的示例代码。导致这个bug的原因是传递给`Blocks.queue()`的`concurrency_count`参数已经被弃用，需要移除才能解决问题。

https://github.com/vllm-project/vllm/issues/2314
这个issue是一个用户提出需求，询问如何在OpenShift或Kubernetes上部署vLLM的问题。

https://github.com/vllm-project/vllm/issues/2313
这是一个关于升级Pydantic版本的需求问题，涉及主要对象是vLLM项目。该问题由于OpenAI要求使用Pydantic版本大于等于1.9.0且小于3，导致需要将vLLM中的Pydantic版本也进行对应升级。

https://github.com/vllm-project/vllm/issues/2312
这是一个bug报告，主要涉及的对象是使用Mixtral模型及AWQ命令的用户。导致该问题的原因可能是内存不足导致的错误或者功能无法正常运行。

https://github.com/vllm-project/vllm/issues/2311
这是一个bug报告，主要涉及的对象是在使用docker镜像vllm/vllmopenai:latest无法服务4位训练模型。导致这个问题可能是由于运行参数或模型配置不正确所引起的。

https://github.com/vllm-project/vllm/issues/2310
这是一个bug报告，问题涉及到在运行OPT-2.7B模型和SharedGPT数据集的吞吐量基准测试中生成了一个无效标记，导致运行时错误。

https://github.com/vllm-project/vllm/issues/2309
该issue类型为bug报告，主要涉及安装在Windows系统上的问题。原因是在安装过程中出现了名为'nvcc_cuda_version'的变量未定义的错误。

https://github.com/vllm-project/vllm/issues/2308
这是一个关于性能和并发性的问题，用户询问在使用vllm时遇到了性能不佳的情况，主要涉及到对比不同系统下的速度以及考虑使用api web server与离线批处理推理是否会自动提高速度等问题。出现这种情况可能是因为用户在使用vllm时没有进行并发处理或发送批量请求导致的。

https://github.com/vllm-project/vllm/issues/2307
这是一个用户提出需求的issue，主要涉及在为OpenAI网页服务器添加gradio chatbot功能时遇到的问题。

https://github.com/vllm-project/vllm/issues/2306
这是一个用户提出需求的issue，主要涉及的对象是为vllm下的openai webserver添加chatbot功能。

https://github.com/vllm-project/vllm/issues/2305
这个issue类型为修复拼写错误和删除未使用代码，主要涉及的对象是项目中的拼写错误和未使用代码。原因是在项目中存在拼写错误和未使用代码，用户希望修复这些问题。

https://github.com/vllm-project/vllm/issues/2304
这是一个bug报告，主要涉及的对象是在A100 GPU上运行多个实例的Mistral 7B，导致vRAM占用过高的问题。

https://github.com/vllm-project/vllm/issues/2303
该issue属于用户提出需求/请教问题类型，主要涉及Python和Rust两种语言针对HTTP服务器并发性能的比较。由于Python的http server对于用户自定义调度逻辑比较困难，用户想知道是否使用Rust语言的http server能提供更好的并发性能。

https://github.com/vllm-project/vllm/issues/2302
这个issue属于用户提出需求类型，主要涉及的对象是Prometheus指标。用户提出添加更多指标，以便监控服务器的性能和生成速度。

https://github.com/vllm-project/vllm/issues/2301
这是一个需求提出类型的issue，主要涉及的对象是关于在vLLM中添加对不具有注意力偏置的GPT-NeoX模型的支持。由于Hugging Face Transformers的更新导致存在版本差异问题，需要修改代码以正确初始化这些模型。

https://github.com/vllm-project/vllm/issues/2300
这是一个性能优化的issue，主要涉及GEMM kernel的优化，通过实现自定义的GEMM kernel提高小批量大小下的性能。

https://github.com/vllm-project/vllm/issues/2299
这个issue类型是bug报告，主要涉及到vllm的使用问题。导致这个问题的原因可能是与vllm模型结合使用时出现的异常行为。

https://github.com/vllm-project/vllm/issues/2298
这是一个关于使用多个GPU对模型进行量化的问题，涉及对象是vllm下的llama 2 70b模型，由于只使用了一个GPU导致OOM错误，用户想要知道是否可以使用多个GPU进行量化。

https://github.com/vllm-project/vllm/issues/2297
这是一个关于API服务器异常终止请求的bug报告。这个问题涉及到vilm/vinallama2.7bchat模型，可能由使用`bfloat16`数据类型训练导致。

https://github.com/vllm-project/vllm/issues/2296
这是一个用户提出的需求问题，主要涉及到VLLM中的prefix prompt cache功能。用户希望添加接口能够生成和存储特定prefix内容的kv cache，并在请求阶段根据传入参数使用相应的cache。

https://github.com/vllm-project/vllm/issues/2295
这是一个bug报告，涉及主要对象是尝试使用Chatglm3-6b与api-server时得到了意外结果。可能是由于命令执行不正确导致生成了意外结果。

https://github.com/vllm-project/vllm/issues/2294
这是一个bug报告，主要涉及到vllm下的Yi-34B-Chat-4bits-GPTQ模型在运行过程中会持续输出空白 token 直到达到最大长度的问题。

https://github.com/vllm-project/vllm/issues/2293
这个issue类型是性能优化，主要涉及的对象是在Github上的vllm项目中实现的tensor parallel MOE。由于对模型权重进行适当地分片并在不同rank间运行MLP操作，该issue旨在提高模型训练时的效率和速度。

https://github.com/vllm-project/vllm/issues/2292
这是一个用户提出需求的issue，主要对象是Vllm，由于缺少对chat completion api的logprobs支持，用户提出了增加该功能的需求。

https://github.com/vllm-project/vllm/issues/2291
这是一个空白内容的Issue，无法确定是Bug报告还是其他类型，也无法确定主要对象是什么。

https://github.com/vllm-project/vllm/issues/2290
这是一个功能改进的issue，涉及到Scheduler的数据结构改动，目的是提高调度器的性能。

https://github.com/vllm-project/vllm/issues/2289
这是一个bug报告，该问题涉及vLLM + Ray的分布式推理过程中出现的问题。由于设置单个GPU时可以正常工作，但可能存在使用多GPU时的问题，导致GPU使用异常。

https://github.com/vllm-project/vllm/issues/2288
该issue类型为文档更新需求，涉及主要对象为vLLM和Haystack集成。

https://github.com/vllm-project/vllm/issues/2287
这是一个用户提出的问题，涉及的主要对象是如何正确设置使用`vllm.entrypoints.openai.api_server`时的min_p采样，可能是由于缺乏相关文档或信息导致的。

https://github.com/vllm-project/vllm/issues/2286
这个issue类型为用户提出需求，主要涉及对象是如何测试前缀共享性能，在论文中复现前缀共享部分的实验，用户寻求关于如何进行实验以及缺乏关于前缀设置代码的帮助。

https://github.com/vllm-project/vllm/issues/2285
这是一个关于Bug的报告，主要涉及的对象是通信测试功能。导致这个Bug的原因是以前的通信测试导入功能无法正常工作。

https://github.com/vllm-project/vllm/issues/2284
这是一个bug报告，涉及的主要对象是ChatGLM3-6B模型。由于vllm版本为0.2.6，可能导致了属性错误 'ChatGLMConfig' object has no attribute 'num_hidden_layers' 的问题。

https://github.com/vllm-project/vllm/issues/2283
这是一个bug报告，涉及的主要对象是在运行`pip install vllm`时出现了`OSError`错误。这个问题的原因是缺少NumPy模块导致torch模块中的transformer.py文件无法正常初始化。

https://github.com/vllm-project/vllm/issues/2282
这个issue类型是用户提出需求，询问如何检索支持的模型，主要对象是vllm库。用户提出这个问题可能是因为想了解如何查询库中支持的模型列表。

https://github.com/vllm-project/vllm/issues/2281
这是一个bug报告，主要涉及的对象是vllm库中的AsyncLLMEngine，由于任务意外结束导致了AsyncEngineDeadError错误。

https://github.com/vllm-project/vllm/issues/2280
这是一个用户咨询问题，主要涉及设置LLAMA结构中的张量并行大小和管道并行大小。用户对于如何在4块A6000上部署hf 70b模型，针对不同的tp和pp组合哪个超参数更好提出了疑问。

https://github.com/vllm-project/vllm/issues/2279
这是一个用户提出需求的issue，主要涉及支持FP8-E5M2 KV Cache，问题源于需要优化内存使用以提高吞吐量。

https://github.com/vllm-project/vllm/issues/2278
这是一个用户提出需求的issue，主要涉及到VLLM项目中的prompt evaluation和token generation操作。由于当前调度策略在接受新请求时需要暂停解码操作，用户提出了是否可以并行执行prompt evaluation和token generation的问题。

https://github.com/vllm-project/vllm/issues/2277
这是一个用户提出的需求问题，主要涉及prompt evaluation和token generation的并行执行，由于当前调度策略需要在接受新请求时暂停解码。

https://github.com/vllm-project/vllm/issues/2276
这是一个关于用户提出需求的issue，主要涉及的对象是`ChatCompletionRequest`。根据标题可以推测用户希望在openai api server中添加对`logprobs`的支持。

https://github.com/vllm-project/vllm/issues/2275
这是一个关于bug报告的issue，主要涉及到VLLM下的awqquantized qwen-72b-chat模型，在输入较长文本时返回空字符串。导致这个问题的原因可能是输入文本长度超出了模型配置中设置的最大tokens数量。

https://github.com/vllm-project/vllm/issues/2274
这个issue类型为功能增强请求，涉及主要对象为ROCm 6.0和MI300。因为当前系统缺乏对ROCm 6.0和MI300的支持，所以用户提交了这个请求以增加对它们的支持。

https://github.com/vllm-project/vllm/issues/2273
这是一个bug报告，主要涉及测试用例文件路径错误导致无法运行的问题。

https://github.com/vllm-project/vllm/issues/2272
这是一个bug报告，涉及的主要对象是mixtral。由于与mixtral的不兼容性导致pytorch cuda错误，可能是由用户错误导致。

https://github.com/vllm-project/vllm/issues/2271
这是一个bug报告，该问题涉及到Mixtral 7bx8 GPTQ模型在推断过程中出现了错误。由于某种原因导致了GPU内存释放时的异常，最终导致了Traceback错误。

https://github.com/vllm-project/vllm/issues/2270
这是一个bug报告，涉及的主要对象是API server测试代码。产生这个问题的原因可能是之前的代码会导致测试代码陷入死循环。

https://github.com/vllm-project/vllm/issues/2269
这是一个bug报告，主要涉及llama 70b加载失败。由于系统错误导致worker意外退出，并可能由于高内存使用、ray stop force或意外崩溃等原因。

https://github.com/vllm-project/vllm/issues/2268
这是一个bug报告，主要涉及vllm的awq gemm int4核心较慢，导致性能低下。

https://github.com/vllm-project/vllm/issues/2267
这是一个bug报告，主要涉及到vllm项目中的completion_stream_generator功能。这个bug可能是由于CC(openai v1/completions api)返回的结果中包含了两个停止标记，导致了问题。

https://github.com/vllm-project/vllm/issues/2266
这是一个bug报告，主要对象是openai v1/completions api的completion_stream_generator。这个问题可能是由于设置stream为true时返回两个停止符所导致的。

https://github.com/vllm-project/vllm/issues/2265
这是一个用户提出需求的issue，涉及到对vllm的缓存淘汰功能的支持，用户希望在持久会话中能够管理缓存。

https://github.com/vllm-project/vllm/issues/2264
这是一个bug报告类型的issue，主要涉及的对象是VLLM模型。这个问题是关于中国羊驼模型和中国驼羊模型效果相似的原因。

https://github.com/vllm-project/vllm/issues/2263
这个issue是关于bug报告类型，涉及到OpenAI API中特殊标记符的问题，由于tokenizer的行为导致生成文本时出现了重复的bos_token。

https://github.com/vllm-project/vllm/issues/2262
这是一个用户提出需求的issue，主要涉及vllm是否能支持Fuyu-8B这个multimodel llm模型，由于当前vllm只支持纯文本，用户想知道如何处理包含图片的multimodel模型。

https://github.com/vllm-project/vllm/issues/2260
这个issue类型是用户提出需求，请求添加一个"About"标题到README.md文件中，以清晰区分"Latest News"部分和基本信息部分。

https://github.com/vllm-project/vllm/issues/2258
这个issue类型是bug报告，涉及的主要对象是vLLM中的async llm engine，此bug由于返回被忽略的句子两次引起。

https://github.com/vllm-project/vllm/issues/2257
这个issue类型是用户提出问题，涉及vLLM在线服务场景中的连续批处理技术是否包含批处理大小概念，用户想了解连续批处理技术中是否包含批处理大小的概念。

https://github.com/vllm-project/vllm/issues/2255
该问题是一个bug报告，主要涉及的对象是使用vllm加载chatglm2时输出变乱码的问题。由于输入的问题为"请你列举20条关于零点不好的建议"，导致输出结果异常。

https://github.com/vllm-project/vllm/issues/2254
这个issue类型是bug报告，涉及的主要对象是修改助手消息，由于代码中存在错误或者数据格式不符导致的问题。

https://github.com/vllm-project/vllm/issues/2253
这个issue属于用户提出问题的类型，主要对象是vllm的本地模型加载，可能由于加载时间长导致用户感到困扰或者产生疑虑。

https://github.com/vllm-project/vllm/issues/2252
这个issue是用户提出需求，询问是否vLLM支持Selective batching，希望实现同一批次中不同输入序列生成时输出长度不同的功能。

https://github.com/vllm-project/vllm/issues/2251
这是一个bug报告，涉及的主要对象是尝试在最新的vllm docker image中运行以AWQ格式量化的Mixtral 8x7b模型时出现了加载失败的错误。

https://github.com/vllm-project/vllm/issues/2250
这个issue类型是bug报告，涉及对象是vllm的neural-chat-7b-v3-1模型。由于请求返回了多个对话轮次而非单个应答，可能是由于模型解码过程中出现了错误导致。

https://github.com/vllm-project/vllm/issues/2249
这是一个bug报告，该问题单涉及的主要对象是修复issue #2248，由于原因尚未提供，导致出现问题需要进一步解决。

https://github.com/vllm-project/vllm/issues/2248
这是一个bug报告，主要涉及的对象是vLLM 0.2.5版本以及相关配置参数，该问题可能是由于内存配置或者GPU利用率设置不当导致的。

https://github.com/vllm-project/vllm/issues/2247
这是一个关于如何将本地模型文件挂载到vllm Docker镜像中的问题，涉及的主要对象是Docker容器和vllm模型文件。这个问题可能是由于挂载路径不正确或者Docker命令有误导致的，导致vllm无法加载模型文件。

https://github.com/vllm-project/vllm/issues/2246
这是一个更新安装说明的 issue，涉及到 vLLM 安装时 CUDA 11.8 xFormers 兼容性错误的问题。

https://github.com/vllm-project/vllm/issues/2245
这是一个bug报告，涉及对象为vLLM安装相关的文档。用户报告的问题是xFormers在cu118上不兼容，导致安装过程中出现错误。

https://github.com/vllm-project/vllm/issues/2244
这是一个技术问题的issue，涉及的主要对象是vllm在macos / ARM上的支持。原因是用户在尝试构建vLLM执行时遇到了问题，并且提到了一些测试结果和新的ARM指令。

https://github.com/vllm-project/vllm/issues/2243
这个issue是一个功能需求，主要涉及的对象是AutoAWQ模块的支持Mixtral，并需要实现新的`modules_to_not_convert`参数以防止quantize模型中的`gate`，原因是要在加载这个4bit模型时避免将其量化为线性层导致内存消耗较大。

https://github.com/vllm-project/vllm/issues/2242
这是一个关于用户提出需求的类型，主要涉及的对象是构建windows wheels with cuda 12.1，由于cuda 12.1，用户希望项目能提供对应的预编译二进制包。

https://github.com/vllm-project/vllm/issues/2241
这是一个用户请教问题类型的issue，主要涉及的对象是`generate`函数和`sampling_params`参数。用户想了解在使用特定参数生成N个样本时，这些生成的N个样本的质量是否彼此独立。

https://github.com/vllm-project/vllm/issues/2240
这是一个bug报告类型的issue，涉及主要对象是使用VLLM加载Llama27BChatHf模型时出现了奇怪的答案。这可能是由于模型配置、库版本或内存占用等原因导致的。

https://github.com/vllm-project/vllm/issues/2239
这是一个bug报告，涉及vllm.engine.async_llm_engine中的Task finished unexpectedly错误，可能是由于_assert output == other_output_断言失败引起。

https://github.com/vllm-project/vllm/issues/2238
这是一个用户提出需求的issue，主要涉及如何对回答中重复部分进行惩罚。由于重复部分导致了一些回答不准确的问题。

https://github.com/vllm-project/vllm/issues/2237
这个issue是一个bug报告，涉及到构建Docker镜像时可能出现的问题。由于之前在相同目录中运行过`pip install e .`，导致本地存在的.so文件在构建阶段被错误地复制到Docker容器中覆盖了构建阶段的结果。

https://github.com/vllm-project/vllm/issues/2236
这是一个bug报告类型的issue，主要涉及的对象是vllm在docker image中出现ModuleNotFoundError错误。由于使用vllm的docker image时出现了ModuleNotFoundError错误。

https://github.com/vllm-project/vllm/issues/2235
这是一个bug报告，主要涉及pipelined-parallel-size参数值错误导致的错误。

https://github.com/vllm-project/vllm/issues/2234
这是一个Bug报告，涉及VLLM 0.2.6中deploy wizardcoder-34b-gptq时遇到"500 Internal Server Error"的问题，由于引擎步骤中出现了AssertionError导致。

https://github.com/vllm-project/vllm/issues/2233
这是一个关于性能问题的问题，主要涉及的对象是加载一个70B的开源模型，由于加载67B的DeepSeek模型需要超过120分钟，用户想了解是否属于正常情况。

https://github.com/vllm-project/vllm/issues/2232
这个issue类型是bug报告，该问题涉及到多个用户无法同时加载相同模型，导致需要手动删除锁定文件来解决此问题。

https://github.com/vllm-project/vllm/issues/2231
这是一个bug报告，主要涉及到vllm中的context_len参数，可能由于滑动窗口处理不当导致最后一块不包含所有有效填充槽。

https://github.com/vllm-project/vllm/issues/2230
这是一个bug报告，问题涉及到使用SELinux启动api_serverr.py时遇到的问题。这可能是由于SELinux限制多进程或工作者进程的生成导致的服务器无法正常启动。

https://github.com/vllm-project/vllm/issues/2229
这是一个bug报告，主要涉及vllm==0.2.6库在使用cuda11.8加载engine时出现崩溃问题，设置'enforce_eager = False'和tp=4时触发。

https://github.com/vllm-project/vllm/issues/2228
该issue类型为用户提出需求，涉及到在/vllm/entrypoints/api_server.py中如何添加历史记录。用户想要知道如何将历史记录添加到请求中的提示中，并询问是否应该使用模板，以及每个模型是否有不同的模板。

https://github.com/vllm-project/vllm/issues/2227
这是一个Bug报告，主要涉及Mistral-7B-Instruct-v0.2这个模型，由于尝试获取模型的重试次数过多导致了错误。

https://github.com/vllm-project/vllm/issues/2226
这是一个关于用户提出需求的issue，主要涉及 Pipeline Parallelism 的支持问题。

https://github.com/vllm-project/vllm/issues/2225
这是一个关于需求的问题，涉及主要对象是torch requirement，由于可能出现的原因是用户希望降低torch的要求。

https://github.com/vllm-project/vllm/issues/2224
这是一个bug报告，主要涉及AsyncLLMEngine在使用greedy decoding时会出现不同预测结果的问题。原因可能是并行请求导致的不确定性。

https://github.com/vllm-project/vllm/issues/2223
这是一个用户提出需求的issue，主要涉及的对象是在GPTQ quantization过程中添加新的quantization kernels。由于需要支持不同量化位数的使用情况，因此需要增加额外的选项和相应的代码实现。

https://github.com/vllm-project/vllm/issues/2222
这是一个bug报告，该问题涉及到修复一个损坏的链接。由于链接损坏，导致用户无法访问相关页面。

https://github.com/vllm-project/vllm/issues/2221
这是一个特性改进的issue，主要涉及vLLM项目中的控制节点通信方式的优化，旨在减少ray通信的序列化开销。

https://github.com/vllm-project/vllm/issues/2220
这是一个bug报告，主要涉及到获取logits时出现的形状错误问题，原因可能是模型输出形状不符合预期。

https://github.com/vllm-project/vllm/issues/2219
这是一个关于软件支持的问题，主要涉及的对象是CUDA 11.8版本。用户可能提出了关于CUDA 11.8版本是否不再受支持的疑问，或者寻求关于该版本的相关帮助。

https://github.com/vllm-project/vllm/issues/2218
这个issue类型是bug报告，主要对象是Mixtral-8x7B-Instruct-v0.1模型，生成了无用的输出。原因可能是某些关键参数配置错误导致模型无法正常输出。

https://github.com/vllm-project/vllm/issues/2217
这是一个Bug报告，涉及的主要对象是使用HIPGraph和TP 8时出现的错误。由于未提供具体的命令和错误日志信息，无法判断具体的问题。

https://github.com/vllm-project/vllm/issues/2216
这是一个Bug报告，主要涉及到安装pip时出现" No space left on device"的错误，导致无法安装包。

https://github.com/vllm-project/vllm/issues/2215
这是一个bug报告，主要涉及Mixtral8x7BInstructv0.1生成JSON时在某些情况下无法按给定格式生成，导致生成超大输出导致服务器严重减慢。

https://github.com/vllm-project/vllm/issues/2214
这是一个bug报告，主要涉及模型参数"logprob"在选择填充答案时得分最高的token并不会被选中的问题。

https://github.com/vllm-project/vllm/issues/2213
这是一个bug报告，主要涉及ShareGPT dataset模型在最新版本中出现一个意外的关键字参数错误，导致不能成功进行基准测试。

https://github.com/vllm-project/vllm/issues/2212
该issue类型为用户提出需求，主要涉及对象是vllm，用户提出希望将PowerInfer整合到vllm中，以利用CPU和GPU结合进行更快的推理。

https://github.com/vllm-project/vllm/issues/2211
这是一个 bug 报告，主要涉及到 Mixtral 模型的权重加载问题。由于手动下载权重后无法正常加载，用户希望能够解决在不重新下载所有模型的情况下加载权重的问题。

https://github.com/vllm-project/vllm/issues/2210
这是一个功能需求的issue，主要涉及的对象是OpenAI API的重构和函数调用处理。由于需要实现函数调用处理，对OpenAI API进行了重构，同时增加了类似于OpenAI API中的"tools"功能。

https://github.com/vllm-project/vllm/issues/2209
这是一个功能改进类型的issue，主要涉及到vllm项目中的sampler功能模块。

https://github.com/vllm-project/vllm/issues/2208
这是一个Bug报告，涉及到VLLM的Mixtral模型。由于`DummyModule`未使用量化线性方法，导致Mixtral模型不支持TP > 1的量化，从而导致加载权重错误。

https://github.com/vllm-project/vllm/issues/2207
这是一个性能优化的issue，涉及主要对象是_prepare_sample函数，由于减少_prepare_sample函数的延迟，不影响模型执行。

https://github.com/vllm-project/vllm/issues/2206
这是一个bug报告，主要涉及Ray默认收集使用数据导致用户要求禁用该功能。

https://github.com/vllm-project/vllm/issues/2205
这个issue是用户提出的需求，请求增加对json格式和正则表达式格式的支持，主要涉及到的对象是项目中的功能CC，用户提出此需求可能是为了优化数据的处理和展示方式。

https://github.com/vllm-project/vllm/issues/2204
这是一个用户提出需求的 issue， 主要对象是 vllm 项目的 openai 入口点，请求增加一个 json 格式选项以便更好地格式化输出。

https://github.com/vllm-project/vllm/issues/2203
这是一个bug报告，涉及的主要对象是 Mixtral，问题可能是由于在tensorparallel 2中运行Mixtral时出现的CC错误。

https://github.com/vllm-project/vllm/issues/2202
这是一个bug报告，主要涉及VLLM下的Mixtral-8x7B-Instruct-v0.1-GPTQ模型的权重加载错误问题，可能是由于HF量化模型的密钥不匹配导致的。

https://github.com/vllm-project/vllm/issues/2201
该issue类型为协作需求, 主要涉及对象为Ray Integration以及vllm的experimental accelerated DAG API, 由于当前nightly implementation存在一些限制，例如actor出现异常或worker死机导致hang，DAG初始化后无法运行其他actor任务，用户提出了关于这些限制以及改进方向的讨论。

https://github.com/vllm-project/vllm/issues/2200
这个issue是关于需求或问题的提问，主要涉及的对象是内部和外部碎片的计算，以及Hugging Face Transformers库中是否存在碎片问题。由于用户想了解如何测量内部和外部碎片，以及是否存在标准Hugging Face Transformers中的碎片问题。

https://github.com/vllm-project/vllm/issues/2199
这是一个bug报告，主要涉及到在使用AWQ模型进行推断时出现CUDA OOM错误，用户寻求关于VLLM框架中AWQ模型推理问题的帮助。

https://github.com/vllm-project/vllm/issues/2198
这是一个bug报告，涉及的主要对象是使用Mixtral TheBloke/Mixtral8x7BInstructv0.1GPTQ在VLLM上加载长prompt时导致block_manager容量溢出的问题。

https://github.com/vllm-project/vllm/issues/2197
这个issue类型为bug报告，主要涉及的对象是vllm引擎中的AsyncEngineDeadError，由于任务异常结束导致报错提示用户在Github上提交issue寻求帮助。

https://github.com/vllm-project/vllm/issues/2196
这是一个bug报告，主要涉及vLLM模型输出与HF模型输出不一致的问题，可能是由于底层实现架构不一致导致的。

https://github.com/vllm-project/vllm/issues/2195
这是一个关于修复GPU使用的bug报告，主要涉及到计算机设备的GPU使用情况。可能由于程序未正确识别或利用GPU资源导致无法正常运行，需要修复以解决GPU相关问题。

https://github.com/vllm-project/vllm/issues/2194
这是一个用户询问问题类型的issue，主要涉及到了该项目中各部分（model, kvcache等）的内存消耗情况。提问者想了解如何获取每个部分的内存占用。

https://github.com/vllm-project/vllm/issues/2193
这是一个用户提出需求的类型，主要对象是vLLM模型的GPU内存使用。由于vLLM只支持单一模型运行并占用固定大小的GPU内存，导致难以在同一GPU上共享多个模型。

https://github.com/vllm-project/vllm/issues/2192
这个issue类型是关于自定义all reduce kernels的讨论，主要涉及到优化性能和内存的问题。由于作者对fast allreduce进行了修改，与原始版本有显著差异，因此性能文档中的数字无效。

https://github.com/vllm-project/vllm/issues/2191
这是一个用户提出需求的issue，主要涉及为vLLM添加对批量完成（离线）的支持。原因是为了提高吞吐量，支持一次传递一批提示文本。

https://github.com/vllm-project/vllm/issues/2190
这是一个bug报告，主要涉及到PyTorch中CudaCachingAllocator的非阻塞行为问题。由于不同流之间缺乏顺序保证导致内存重用不安全，需要适当地同步sampling tensors来修复此问题。

https://github.com/vllm-project/vllm/issues/2189
这是一个bug报告，主要涉及vllm和HF推理结果不一致的问题，用户寻求调整来使得结果更接近HF。

https://github.com/vllm-project/vllm/issues/2188
这个issue属于对项目的增强需求，主要涉及到vLLM中的speculative decoding功能的实现和优化。

https://github.com/vllm-project/vllm/issues/2187
这是一个关于错误的bug报告，问题涉及到拉取manifest时出现的“max retries exceeded: unexpected EOF”错误。可能是由于网络或服务器问题导致的连接中断而产生的。

https://github.com/vllm-project/vllm/issues/2186
这是一个bug报告，涉及调度器错误地假设每个等待序列都处于等待状态，导致了预emption机制无法正确恢复序列组中未终止的序列。

https://github.com/vllm-project/vllm/issues/2185
这是一个关于如何在CPU模式下使用docker运行vLLM的问题，用户在寻求相关帮助。

https://github.com/vllm-project/vllm/issues/2184
这个issue是一个bug报告，主要涉及的对象是`TheBloke/deepseekcoder33BinstructAWQ`库。由于出现了`AssertionError: assert output == other_output`错误，用户在特定输入下无法顺利运行该库。

https://github.com/vllm-project/vllm/issues/2183
该issue类型是文档更新，主要涉及更新`gpumemoryutilization`参数的帮助文本以与vllm文档保持一致。

https://github.com/vllm-project/vllm/issues/2182
这是一个关于提醒CUDA图内存使用可能引起OOM的问题，主要涉及的对象是CUDA图使用者。原因是使用CUDA图可能导致内存耗尽，用户可能需要解决这个问题。

https://github.com/vllm-project/vllm/issues/2181
这是一个bug报告，主要涉及的对象是Mixtral在tensor-parallel 2上运行时发生错误。由于某种原因导致该命令无法成功执行。

https://github.com/vllm-project/vllm/issues/2180
这是一个修复bug的issue，主要涉及ROCm平台上GPTQ的支持。

https://github.com/vllm-project/vllm/issues/2179
这是一个bug报告，主要涉及到vllm在同一台机器上被多个用户使用时出现的tmp目录被其他用户锁定的问题，导致PermissionError异常。

https://github.com/vllm-project/vllm/issues/2178
这是一个bug报告类型的issue，主要涉及对象是尝试运行最新vllm docker容器的用户。由于CUDA版本不匹配导致出现了缺少共享对象文件的错误。

https://github.com/vllm-project/vllm/issues/2177
这是一个bug报告，主要涉及Sampler在处理较大模型时出现RuntimeError的问题，可能是由于PR引入了错误的修改导致了stream冲突问题。

https://github.com/vllm-project/vllm/issues/2176
这是一个bug报告，主要涉及的对象是vllm下的'vicuna-7b-v1.5'模型的量化操作。由于某些原因导致CUDA错误，可能是代码调用参数或环境配置发生了变化。

https://github.com/vllm-project/vllm/issues/2175
这是一个bug报告；主要涉及的对象是使用CUDA 12.1和Dockerfile构建vllm的过程；由于缺乏定义CUDA版本12.1下的x86_64-linux-gnu-g++版本边界，导致无法成功构建vllm。

https://github.com/vllm-project/vllm/issues/2174
这是一个bug报告。该问题涉及的主要对象是加载深度学习模型时出现的错误。由于可能与Hugging Face键相关，导致了无法加载指定模型的bug。

https://github.com/vllm-project/vllm/issues/2173
这个issue是关于bug报告，涉及的主要对象是AsyncLLMEngine和Mixtral-8x7B-Instruct-v0.1。导致该问题的原因可能是AsyncLLMEngine在处理Mixtral-8x7B-Instruct-v0.1输出时出现了错误，并导致了相关异常。

https://github.com/vllm-project/vllm/issues/2172
这是一个功能需求类型的issue，主要对象是`benchmark_serving.py`文件。由于用户希望能够在OpenAI API server中支持`benchmark_serving.py`文件，需要添加`endpoint`和`model`参数以便用户评估他们的OpenAI兼容vLLM服务器。

https://github.com/vllm-project/vllm/issues/2171
这是一个bug报告，主要涉及的对象是Mixtral-8x7B-Instruct-v0.1模型。由于在加载模型权重时出现了SafetensorError，可能是由于权重文件头部反序列化时出现了InvalidHeaderDeserialization错误，导致了该问题的出现。

https://github.com/vllm-project/vllm/issues/2170
这是一个bug报告，主要涉及vllm的使用问题，由于使用了未知的量化方法"gptq"导致数值错误。

https://github.com/vllm-project/vllm/issues/2169
这是一个特性请求的issue，主要涉及Faster and memory efficient top-p top-k kernel。这个问题的提出是为了向vllm添加一个更快速和更节省内存的cuda kernel，以支持float16/bfloat16并限制topk<=1024。

https://github.com/vllm-project/vllm/issues/2168
这是一个用户提交询问问题类型的issue，涉及主要对象是Llama2架构的自定义优化。用户询问有关Ray、Torchrun和Deepspeed分布式性能的比较，以及如何为Llama2架构进行定制优化。

https://github.com/vllm-project/vllm/issues/2167
这是一个bug报告，涉及的主要对象是在不同类型的GPU上使用相同提示时产生不同输出，可能是由于硬件差异导致的。

https://github.com/vllm-project/vllm/issues/2166
这是一个bug报告，主要涉及到项目中的一个命名错误。由于该错误导致无法显示图片（!image），用户希望修正该拼写错误以解决问题。

https://github.com/vllm-project/vllm/issues/2165
这个issue属于bug报告，主要涉及的对象是使用vllm中的gptq模块时，在CUDA 12.2和V100 GPU上出现了CUDA error导致的RuntimeError。

https://github.com/vllm-project/vllm/issues/2164
这是一个bug报告类型的issue，主要涉及RoPE kernel对长序列的处理不完善，导致在处理很长的序列时出现问题。

https://github.com/vllm-project/vllm/issues/2163
这个issue是关于bug修复，主要涉及到vLLM在单个序列长度大于系统KV缓存大小时出现挂起的问题。

https://github.com/vllm-project/vllm/issues/2162
这个issue属于文档错误的修复，主要涉及到GPU内存利用率选项的描述修正，因为之前的描述没有清晰指明输入的范围和默认值可能导致用户的困惑。

https://github.com/vllm-project/vllm/issues/2161
这是一个关于禁用CUDA图表功能的问题，类型为用户提出需求。该问题涉及主要对象为SqueezeLLM。由于某些原因，用户想要禁用CUDA图表功能。

https://github.com/vllm-project/vllm/issues/2160
这个issue属于bug报告，主要涉及的对象是代码中的一个拼写错误。原因是在代码中存在一个拼写错误，导致了错误或者混淆的行为。

https://github.com/vllm-project/vllm/issues/2159
这是一个需求类型的issue，主要涉及的对象是vLLM的模型Phi 2。这个问题是由于需要将Phi 2添加到支持的模型列表中，因为作者发现vLLM的Phi 2输出与HF（Hugging Face）的匹配。

https://github.com/vllm-project/vllm/issues/2158
这是一个bug报告，涉及的主要对象是xformers库。由于不同版本的xformers对torch版本有不同要求，导致可能引发torch版本冲突的问题。

https://github.com/vllm-project/vllm/issues/2157
这是一个发布新版本的issue，涉及的主要对象是项目的软件版本。

https://github.com/vllm-project/vllm/issues/2156
这是一个bug报告，涉及主要对象为运行vllm的AMD MI210，由于导入模块时出现"_ZNSt15__exception_ptr13exception_ptr9_M_addrefEv"未定义的符号，导致了报错信息中提到的undefined symbol错误。

https://github.com/vllm-project/vllm/issues/2155
这是一个bug报告，主要涉及对象是vLLM项目中的PyTorch和xformers版本的管理。这个问题是由于未固定PyTorch和xformers版本所致，导致了vLLM + CUDA 11.8包构建失败并可能出现不稳定的情况。

https://github.com/vllm-project/vllm/issues/2154
这个issue是一个bug报告，主要涉及的对象是GPTQ模型，由于GPTQ模型不支持CUDA图，因此用户提出了需要临时禁止使用CUDA图的问题。

https://github.com/vllm-project/vllm/issues/2153
这是一个功能需求的issue，涉及到LLava模型的支持添加。由于LLava模型的一些基本实现问题，用户向开发团队提出了关于支持LLava模型加载、多模态推理和API问题的需求。

https://github.com/vllm-project/vllm/issues/2152
这个issue类型是功能改进建议，主要涉及的对象是减少对CuPy的依赖性。原因是考虑到`TORCH_NCCL_AVOID_RECORD_STREAMS=1`可以启用CUDA图来使用`torch.distributed.all_reduce`，从而避免对CuPy的依赖。

https://github.com/vllm-project/vllm/issues/2151
这是一个bug报告，主要涉及torch.distributed.all_reduce内存使用问题，由于未释放内存导致了内存占用高的bug。

https://github.com/vllm-project/vllm/issues/2150
这是一个关于bug报告的issue，主要涉及到torch.distributed.all_reduce未释放内存的问题。这可能是由于在使用并行处理时，激活内存未被重复使用导致的。

https://github.com/vllm-project/vllm/issues/2149
这是一个bug报告，主要涉及的对象是GPTQ，由于目前仅支持float16精度，导致不支持bfloat16。

https://github.com/vllm-project/vllm/issues/2148
该issue类型为用户提出需求，在文档中添加关于CUDA图支持的详细信息。

https://github.com/vllm-project/vllm/issues/2147
这是一个bug报告，主要涉及到GPTQ模型不支持CUDA图，导致在运行时出现错误。

https://github.com/vllm-project/vllm/issues/2146
这是一个建议删除 Llama 分词器警告的 issue，涉及主要对象为 Llama V1 分词器。其原因是警告有可能误导用户使用不需要的 HF 内部分词器。

https://github.com/vllm-project/vllm/issues/2145
这是一个需求提出类型的issue，主要涉及的对象是`quantization`参数。

https://github.com/vllm-project/vllm/issues/2144
这是一个bug报告，主要涉及的对象是vllm库。由于缺少名为'vllm.entrypoints'的模块，导致出现了"No module named 'vllm.entrypoints'"的错误提示。

https://github.com/vllm-project/vllm/issues/2143
这是一个用户提出的需求类型的 issue，主要涉及优化内存使用的建议，提出了使用LRU缓存来保存CUDA图数据的方法。

https://github.com/vllm-project/vllm/issues/2142
这是一个功能需求类型的issue，主要涉及logit processors在处理文本生成时缺少额外上下文信息，用户提出希望能够传递额外的参数以提供更详细的信息。

https://github.com/vllm-project/vllm/issues/2141
这是一个用户提出需求的 issue，主要涉及对vLLM添加 LogitProcessors 的功能。用户提出该需求的原因是为了支持实现数字水印算法，并希望将更复杂的 LogitProcessor 集成到 vLLM 项目中。

https://github.com/vllm-project/vllm/issues/2140
这是一个bug报告，主要涉及的对象是PyTorch CUDA内存管理。由于CUDA内存分配超出容量导致的"torch.cuda.OutOfMemoryError"错误。

https://github.com/vllm-project/vllm/issues/2139
这是关于如何在AWS SageMaker上提供VLLM api端点的问题，用户无法在本地或者通过Postman在外部访问端点。

https://github.com/vllm-project/vllm/issues/2138
这是一个 bug 报告，涉及到 ROCm 支持在 GPTQ 下的构建失败。原因可能是由于尚未经过测试的 ROCm 支持导致构建失败。

https://github.com/vllm-project/vllm/issues/2137
这是一个bug报告，主要涉及ROCM Docker构建失败，可能是由于缺少某一行代码导致的。

https://github.com/vllm-project/vllm/issues/2136
这是一个bug报告，主要涉及的对象是vLLM软件中的KV缓存和VRAM使用问题，由于VRAM profiling或者之前的问题，导致vLLM无法正确分配剩余的VRAM用于KV缓存，导致了无法使用所有剩余VRAM的bug。

https://github.com/vllm-project/vllm/issues/2135
这是一个用户提出需求的issue，主要涉及该项目的文档，用户希望添加量化支持的说明。

https://github.com/vllm-project/vllm/issues/2134
这个issue类型是用户需求，涉及主要对象是服务Mixtral 8x7B with vllm OpenAI Server。由于文档不清晰，用户不清楚如何设置`tensor_parallel_size = 2`以在两个A100 GPU上分割Mixtral，希望获得帮助。

https://github.com/vllm-project/vllm/issues/2133
这是一个bug报告，涉及的主要对象是模型权重加载逻辑。由于无法运行特定模型（ehartford/dolphin2.5mixtral8x7b），导致了此bug。

https://github.com/vllm-project/vllm/issues/2132
这个issue类型是bug报告，主要涉及到模型ehartford/dolphin-2.5-mixtral-8x7b在VLLM环境中无法找到权重文件的问题，可能由于该模型与VLLM互动时的文件格式不匹配导致。

https://github.com/vllm-project/vllm/issues/2131
这是一个关于bug报告的issue，主要涉及的对象是vllm中的torch.compile功能。原因是使用torch.compile无法获得性能提升，甚至有性能下降现象。

https://github.com/vllm-project/vllm/issues/2130
这个issue类型是用户请教问题，主要涉及对象是chattemplate的使用。用户提出问题的原因是对如何使用chattemplate进行chatbot构建不够清楚。

https://github.com/vllm-project/vllm/issues/2129
这是一个关于bug报告的issue，主要涉及对象是模型vllm中的输出变化问题。由于设置前缀后输出发生变化，以及在请求无法全部一次性添加到调度程序时批处理大小过大会影响输出，可能是由于模型的内部逻辑或参数设置导致的。

https://github.com/vllm-project/vllm/issues/2128
这是一个用户提出需求的类型，该问题单涉及的主要对象是Llama ILQL训练模型。

https://github.com/vllm-project/vllm/issues/2127
这是一个bug报告，涉及到在没有GPU的机器上构建docker镜像时遇到的问题。原因可能是vllm在没有GPU的机器上安装时出现了错误。

https://github.com/vllm-project/vllm/issues/2126
这是一个bug报告，涉及到GPU设备的使用问题。导致这个bug的可能原因是代码中同时使用了不同的GPU设备。

https://github.com/vllm-project/vllm/issues/2125
这是一个用户提出需求的issue，主要涉及对象是`AsyncLLMEngine`，用户希望找到同时支持流式推断和批处理推断的解决方案。

https://github.com/vllm-project/vllm/issues/2124
这是一个bug报告issue，涉及的主要对象是Llama-2-70b-chat-hf模型。导致症状是Llama-2-70b-chat-hf模型得到比Llama-2-70B-Chat-GPTQ模型更差的结果。

https://github.com/vllm-project/vllm/issues/2123
这是一个关于bug的报告，主要涉及SamplingParams中使用custom special token导致的问题。原因是当skip_special_tokens设置为true时，特殊的停用词会被排除在new_text之外，导致_check_stop无法处理该词。

https://github.com/vllm-project/vllm/issues/2122
这是一个bug报告类型的issue，主要涉及的对象是vLLM下的模型chatglm3-6b。由于tokenizer未按照预期运行，导致用户在使用该模型时无法获得期望的结果。

https://github.com/vllm-project/vllm/issues/2121
这是一个用户提出需求的类型的issue，主要涉及到关于vLLM模型中集成packing inference的想法。由于用户希望通过在连续批处理过程中实现packing来充分利用GPU，提出了以上问题。

https://github.com/vllm-project/vllm/issues/2120
这是一个需求提议类型的issue，主要对象是VLLM模型的单步生成多个token功能。

https://github.com/vllm-project/vllm/issues/2119
这是一个关于bug报告的issue，主要涉及的对象是在H100机器上使用多个GPU启动推理服务器时出现错误。由于在单个H100 GPU上使用时没有问题，可能由于代码中的不合适的inplace更新导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/2118
该issue类型为用户提出需求，主要涉及对象是如何配置使用的GPU；由于缺乏对GPU选择进行配置的参数，导致无法更改使用的GPU编号，即使尝试设置CUDA_VISIBLE_DEVICES仍未生效。

https://github.com/vllm-project/vllm/issues/2117
该issue类型是用户提出需求，用户想要支持使用tiktoken作为tokenizer选项。

https://github.com/vllm-project/vllm/issues/2116
这是一个bug报告，涉及到测试文件中调用destroy_model_parallel的修复。这个问题出现的原因可能是测试文件中未正确调用相关函数导致。

https://github.com/vllm-project/vllm/issues/2115
这是一个bug报告，涉及的主要对象是pytest tests/models。由于在H100计算机上运行命令时出现了错误，用户提出了问题。

https://github.com/vllm-project/vllm/issues/2114
这是一个bug报告，涉及的主要对象是vllm项目中的tests/samplers/*.py代码文件。由于模型并行组件未被正确销毁，导致在运行`pytest tests/samplers/`时出现`AssertionError: tensor model parallel group is already initialized`的问题。

https://github.com/vllm-project/vllm/issues/2113
这个issue属于用户提出问题类型，主要涉及的对象是Dockerfile中的`nvcc`安装位置，用户询问了vllm/vllmopenai镜像中如何构建nvcc的疑问。

https://github.com/vllm-project/vllm/issues/2111
这个issue类型是性能问题，主要涉及的对象是vllm模型的推断速度，用户关注在使用不同安装方式（pre-built library vs pip3 install -e）是否会影响推断速度。

https://github.com/vllm-project/vllm/issues/2110
这个issue是关于功能增强的，主要对象是api_server，由于该用户做了一些更新导致功能存在一些问题，他希望能将这些更新合并到vLLM中。

https://github.com/vllm-project/vllm/issues/2109
这个issue属于用户提出需求，主要涉及API服务器，用户希望能够通过传递SSL证书和密钥文件路径来启用HTTPS。

https://github.com/vllm-project/vllm/issues/2108
这是一个bug报告，主要涉及两个不同GPU上加载的模型在推理时导致的"Expected all tensors to be on the same device"错误。由于两个模型加载在不同设备上，导致出现这个错误。

https://github.com/vllm-project/vllm/issues/2107
该issue为性能优化问题，主要涉及VLLM中H100的性能优化需求。可能由于当前设置未能使H100在速度上有显著提升，用户希望寻求优化性能的方式。

https://github.com/vllm-project/vllm/issues/2106
这是一个bug报告，主要涉及vllm项目中的`paged_attention_v1()`函数，由于函数参数不兼容而导致了错误输出和服务端报错。

https://github.com/vllm-project/vllm/issues/2105
这是一个bug报告，该问题单涉及的主要对象是增加语法功能。原因是实现增量的 LALR / 正则表达式解析器以确定合法的下一个令牌集合。

https://github.com/vllm-project/vllm/issues/2104
这是一个bug报告，涉及的主要对象是VLLM的代码库。由于传入的tokens参数为None导致了报错，可能是参数传递的错误或者缺少必要的输入信息。

https://github.com/vllm-project/vllm/issues/2103
这个issue类型是bug报告，主要涉及多次采样时结果长度显著增加的问题，可能由于参数设置问题或采样结果间的不独立性导致。

https://github.com/vllm-project/vllm/issues/2102
这个issue属于技术问题讨论，涉及的主要对象是等待队列。由于`engine.add_request()`和`scheduler._schedule()`可能同时执行，导致了线程安全性问题，用户在询问如何确保线程安全。

https://github.com/vllm-project/vllm/issues/2101
这是一个bug报告，涉及的主要对象是Dockerfile.rocm文件。可能由于Dockerfile.rocm文件中存在某些错误或问题，需要修复以确保正常运行。

https://github.com/vllm-project/vllm/issues/2100
该issue类型为功能增强（feature enhancement），主要涉及对象是AsyncLLMEngine函数的generate功能，原因是需要将AsyncIterator类型添加到函数中，同时解决了yapf对toml未安装时的警告问题。

https://github.com/vllm-project/vllm/issues/2099
这是一个bug报告，该问题涉及的主要对象是vllm服务。由于async_llm_engine出现了意外结束，导致了OpenAI调用时报错。

https://github.com/vllm-project/vllm/issues/2098
这个issue为用户提出需求类型，主要涉及到vllm模型的推理性能。用户提出了关于Mixtral-8x7B推理速度较慢的问题，并希望vllm团队能够关注并改进相关工作。

https://github.com/vllm-project/vllm/issues/2097
这个issue类型是用户提问，主要涉及的对象是破壁机的用途和区别。用户提出了关于破壁机是否可以用来制作果汁的问题。

https://github.com/vllm-project/vllm/issues/2096
This issue is a feature request regarding support for specifying the number of active experts in the Mixtral model.

https://github.com/vllm-project/vllm/issues/2095
该issue类型为软件版本升级请求，涉及到代码库vllm，并由于需要合并CC(Mixtral expert parallelism)分支而产生。

https://github.com/vllm-project/vllm/issues/2094
该issue类型为用户提出需求，主要对象是多模型部署。询问如何在两个GPU上分别加载完整模型而非使用张量并行。

https://github.com/vllm-project/vllm/issues/2093
这是一个关于服务启动报404错误的bug报告，主要涉及的对象是VLLM模型的API服务器。导致这个问题的原因可能是模型路径设置不正确或参数配置错误导致服务无法正常调用。

https://github.com/vllm-project/vllm/issues/2092
这是一个关于需求的问题，涉及的主要对象是在vllm中如何指定特定的GPU，可能由于用户需要精确控制训练过程中GPU的分配。

https://github.com/vllm-project/vllm/issues/2091
这是用户寻求关于如何在 Docker 上运行模型并使用 Gradio 构建聊天界面的帮助。

https://github.com/vllm-project/vllm/issues/2090
该issue是关于特性改进（Feature Enhancement），涉及到Mixtral模型的并行计算方案优化。这次改进主要是为了提高Mixtral模型的效率和性能。

https://github.com/vllm-project/vllm/issues/2089
这个issue类型是bug报告，主要涉及vllm项目中支持Mixtral on ROCm的问题，由于代码问题导致了报错。

https://github.com/vllm-project/vllm/issues/2088
这是一个bug报告，主要涉及对象是VLLM模型，由于输入位置超过4096导致输出错误。

https://github.com/vllm-project/vllm/issues/2087
这是一个文档相关的需求问题，用户希望在 ROCm 支持的模型上添加说明。

https://github.com/vllm-project/vllm/issues/2086
这个issue类型是文档问题，主要涉及的对象是CUDA 11.8的安装指导，可能是由于文档内容陈旧或不准确引起用户无法正确安装CUDA 11.8。

https://github.com/vllm-project/vllm/issues/2084
这是一个bug报告，涉及主要对象是在AWS SageMaker notebooks上无法运行tensor_parallel_size大于1的模型。由于缺少适当的NCCL配置，导致初始化时出现错误并在运行LLM引擎时引发了DistBackendError错误。

https://github.com/vllm-project/vllm/issues/2083
该issue属于用户提出需求类型，主要涉及对象是vllm模型。由于在示例中没有找到关于vllm支持离线推理中流式聊天的说明，用户想要知道vllm是否支持离线推理中的流式聊天。

https://github.com/vllm-project/vllm/issues/2082
这个issue属于用户提出需求类型，主要涉及的对象是vLLM支持的AMD GPU模型。用户提出这个问题是想了解目前vLLM支持哪些型号的AMD GPU。

https://github.com/vllm-project/vllm/issues/2081
这个issue属于用户提出需求类型，主要对象是vLLM框架的维护者。用户想咨询关于在vLLM框架中集成Mac Metal API支持的可能性，因为在Mac设备中使用Metal API能够带来GPU加速计算的性能优势。

https://github.com/vllm-project/vllm/issues/2080
这是一个关于系统提示不适用于Mistral-7B-Instruct-v0.1的bug报告，用户提到在使用系统提示时出现了错误，但在不使用系统提示时正常运行。原因可能是系统提示与特定模型不兼容。

https://github.com/vllm-project/vllm/issues/2079
这个issue类型是更新需求，涉及的主要对象是xformers版本依赖关系。由于ROCm版本与CUDA使用的xformers版本不匹配，导致需要更新xformers版本以解决兼容性问题。

https://github.com/vllm-project/vllm/issues/2078
这是一个bug报告，主要涉及的对象是模型在执行"llama 70B AWQ"时出现NAN。可能由于某些原因导致模型输出中出现了NAN。

https://github.com/vllm-project/vllm/issues/2077
该issue是用户提出需求，询问是否可以在vllm中使用类似llama-2-70b-chat-gptq这样的自动压缩模型。

https://github.com/vllm-project/vllm/issues/2076
这是一个bug报告，issue主要涉及Vllm Docker无法运行Mixtral-8x7B-Instruct-v0.1模型，可能由于模型架构不受支持导致数值错误。

https://github.com/vllm-project/vllm/issues/2075
这是一个bug报告，涉及的主要对象是vllm库的chat功能。原因可能是由于模型版本0.2.3的问题导致无法成功进行chat推理。

https://github.com/vllm-project/vllm/issues/2074
这是一个bug报告类型的issue，主要涉及Mixtral AWQ的推理过程出现错误。导致该错误的原因可能是引擎在运行过程中遇到了一些问题。

https://github.com/vllm-project/vllm/issues/2073
这是一个性能问题报告，涉及主要对象为vLLM模型在A100和H100上的性能表现。问题可能由于vLLM在与TGI相似的环境下表现较差而导致。

https://github.com/vllm-project/vllm/issues/2072
这是一个用户提出需求的issue，主要涉及的对象是安装vllm时需要使用CUDA 11.8而非12.1版本。

https://github.com/vllm-project/vllm/issues/2071
这是一个用户需求类型的issue，主要涉及的对象是添加策略。由于用户需要添加特定的策略，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/2070
这是一个未提供具体内容的issue，无法确定类型是bug报告还是其它类型，该问题单涉及的主要对象未知。

https://github.com/vllm-project/vllm/issues/2069
这是一个bug报告。该问题涉及的主要对象是Mixtral的tokens-per-second速度。由于某种原因导致了TPS速度比预期慢，导致用户观察到~1011 TPS的速度。

https://github.com/vllm-project/vllm/issues/2068
这是一个功能需求的issue，主要涉及的对象是实现Triton-based AWQ kernel。问题可能由于性能模型不正确和一些功能缺失导致。

https://github.com/vllm-project/vllm/issues/2067
这是一个 Bug 报告，主要涉及的对象是 A100，由于未进行分布式推理导致 Runtime AsyncEngineDeadError 错误。

https://github.com/vllm-project/vllm/issues/2066
这个issue是一个用户需求，主要涉及的对象是允许用户选择不同的文件夹来存储Ray日志和临时文件，由于Ray日志在Sagemaker上填满了/tmp文件夹，需要提供选项让用户选择存储位置。

https://github.com/vllm-project/vllm/issues/2065
这个issue是用户提出需求类型，主要涉及的对象是为vLLM添加与Neuron工具链兼容的构建选项。

https://github.com/vllm-project/vllm/issues/2064
这是一个关于bug的报告，主要涉及到Mistral/Mixtral在处理超过4096个token的prompt时生成无意义内容的问题。导致此问题的原因可能是引擎在处理大量tokens时出现了错误。

https://github.com/vllm-project/vllm/issues/2063
这是一个bug报告，涉及的主要对象是Falcon 7B模型在vLLM中的运行问题，由于probability tensor包含inf、nan或<0的元素导致RuntimeError。

https://github.com/vllm-project/vllm/issues/2062
该issue类型为发布通知，主要涉及到新增发布的DeciLM7b和DeciLM7binstruct模型。由于这两个模型具有较高的性能和效率，开发者发布了该issue来通知用户有关这些新模型的相关信息。

https://github.com/vllm-project/vllm/issues/2061
这是一个Bug报告，涉及的主要对象是vllm代码中的LLM引擎。由于代码中调用LLMEngine函数时传入的参数不正确，导致初始化过程中出现错误。

https://github.com/vllm-project/vllm/issues/2060
这是一个关于技术性问题的报告，主要涉及vLLM库的API服务器在特定条件下可能导致ClientOSError错误，用户希望找到避免这种情况的解决方案。

https://github.com/vllm-project/vllm/issues/2059
这是一个关于bug的报告，主要涉及mixtral-8x7B-Instruct-v0.1模型在处理长篇提示时输出垃圾内容或无意义重复的问题。

https://github.com/vllm-project/vllm/issues/2058
这个issue类型为用户提出需求，主要涉及的对象是VLM模型和GPT4V API。

https://github.com/vllm-project/vllm/issues/2057
这个issue是关于性能问题的bug报告，主要涉及的对象是vllm在WSL2中使用NVIDIA驱动的影响。原因可能是升级NVIDIA驱动导致推理性能下降。

https://github.com/vllm-project/vllm/issues/2056
这是一个bug报告，涉及到使用Ray进行分布式推理时无法从ModelScope拉取模型的问题，导致工作节点无法下载模型导致的异常。

https://github.com/vllm-project/vllm/issues/2055
这是一个用户提出需求的类型，针对实现添加RR策略。

https://github.com/vllm-project/vllm/issues/2054
这是一个用户提出需求的issue，主要涉及的对象是添加政策。可能是由于现有政策不完善或者未列明清楚导致用户提出需求希望添加新的政策。

https://github.com/vllm-project/vllm/issues/2053
这是一个关于bug报告的issue，涉及到vllm项目中运行falcon40b模型时出现错误的问题。由于概率张量中包含了`inf`、`nan`或小于0的元素，导致了RuntimeError错误。

https://github.com/vllm-project/vllm/issues/2052
这是一个bug报告，主要涉及的对象是v0.2.4 Docker Image，由于缺少额外配置，导致出现`KeyError: 'mixtral'`错误。

https://github.com/vllm-project/vllm/issues/2051
此issue属于bug报告类型，主要涉及chatglm3-6b模型模板不匹配问题。由于chat-template模板适用于chatglm2而不适用于chatglm3，导致用户反馈模板不匹配问题。

https://github.com/vllm-project/vllm/issues/2050
这是一个Bug报告，主要涉及VLLM加载Qwen14b模型时，Rayworker进程会出现卡死现象。可能是由LLM引擎导致的问题，用户提出了希望解决使用异步LLM引擎的方式。

https://github.com/vllm-project/vllm/issues/2049
这个issue是关于移除依赖项einops的请求，主要涉及Phi模型未使用该软件包。

https://github.com/vllm-project/vllm/issues/2048
这是一个性能问题的issue，主要涉及的对象是Mixtral 7bx8模型在八个T4 GPU上的生成速度。由于生成速度低于预期且不符合其他模型的表现，用户需要找出导致此问题的原因。

https://github.com/vllm-project/vllm/issues/2047
这是一个需求类型的issue，主要涉及的对象是v0.2.4版本的VLLM软件。由于缺少cuda11.8 wheel发布，用户提出了需要该版本的需求。

https://github.com/vllm-project/vllm/issues/2046
这个issue类型是用户提出需求，需要升级transformers版本至4.36.0来支持Mixtral功能。

https://github.com/vllm-project/vllm/issues/2045
这是一个类型为需求报告的issue，主要涉及对象是PyTorch版本。由于xformers (v0.0.23)要求PyTorch v2.1.1，因此需要将PyTorch版本升级为v2.1.1。

https://github.com/vllm-project/vllm/issues/2044
该issue属于功能需求类型，主要涉及到模型加载的延迟实现。

https://github.com/vllm-project/vllm/issues/2043
这是一个bug报告。该问题涉及的主要对象是vllm下的设置prompt_logprobs=1时返回的结果。问题由于某些位置的词并非具有最高概率导致部分位置有两个key，其他位置只有一个key，导致结果有些混乱。

https://github.com/vllm-project/vllm/issues/2042
这是一个用户提出需求的 issue，主要涉及到更新 Dockerfile 以构建 Megablocks。由于需要为多种架构构建 Megablocks，因此需要更新 Dockerfile 并构建 wheels 以便在服务器上安装。

https://github.com/vllm-project/vllm/issues/2041
这是一个用户提出需求的issue，主要涉及改进支持的权重格式，由于目前仅支持 Mistral 的 pt 格式权重，在未来可能会有 HF 修改 pt 权重文件的可能性。

https://github.com/vllm-project/vllm/issues/2040
这是一个bug报告，该问题单涉及的主要对象是MegaBlocks软件。由于MegaBlocks清理了其依赖项，不再需要Python 3.10，所以用户提出了需要移除Python 3.10依赖的问题。

https://github.com/vllm-project/vllm/issues/2039
这是一个用户提出需求的类型的issue，主要涉及的对象是关于提高LLM推理吞吐量的一篇论文。

https://github.com/vllm-project/vllm/issues/2038
这个issue是一个bug报告，主要涉及的对象是megablocks。由于导入错误信息问题，用户在使用megablocks时遇到了bug。

https://github.com/vllm-project/vllm/issues/2037
这个issue是用户提出需求，主要涉及的对象是在使用Retriever与Question Answer Chain时想要在vllm中实现Retrieval QA支持。

https://github.com/vllm-project/vllm/issues/2036
这是一个bug报告，主要涉及的对象是Mixtral库，由于用户未安装`megablocks`导致了错误信息产生不准确的问题。

https://github.com/vllm-project/vllm/issues/2035
这是一个bug报告类型的issue，主要涉及latency benchmark script的修复。

https://github.com/vllm-project/vllm/issues/2034
这个issue类型是更新请求，涉及的主要对象是vllm项目。由于可能需要提升版本以获取新功能或修复bug，触发了此更新请求。

https://github.com/vllm-project/vllm/issues/2033
这是一个更新说明文档的issue，涉及的主要对象是megablocks requirement for mixtral。这个issue是由于需要在mixtral项目的README.md中添加关于megablocks的要求。

https://github.com/vllm-project/vllm/issues/2032
这个issue类型是需求提出，主要对象是重构Mixtral以重用MegaBlocks中代码，由于MoE实现中存在从MegaBlocks直接复制代码的情况，导致代码重复且易出错。

https://github.com/vllm-project/vllm/issues/2031
这是一个bug报告，主要涉及vLLM在TP > 1时内存占用超过90%的问题。 导致这个问题的原因可能是当前内存分析不准确。

https://github.com/vllm-project/vllm/issues/2030
这个issue是关于代码回滚操作，属于管理代码变更的类型，主要涉及项目中的需求变更。原因可能是之前提交的更改引起了问题，需要撤销以修复bug或解决不确定的问题。

https://github.com/vllm-project/vllm/issues/2029
这个issue类型为需求更新，涉及的主要对象是mixtral项目的requirements.txt文件。由于缺少对megablocks的依赖声明，导致用户提出需要添加该依赖项的需求。

https://github.com/vllm-project/vllm/issues/2028
这是一个bug报告，涉及到vllm仓库下的Mixtral加载格式问题。原因可能是Mixtral加载格式设置不正确导致的功能问题。

https://github.com/vllm-project/vllm/issues/2027
这是一个bug报告类型的issue，涉及的主要对象是vLLM Docker container。原因可能是Dockerfile不支持Mixtral导致该容器无法正常工作。

https://github.com/vllm-project/vllm/issues/2026
这个issue是关于bug报告，涉及主要对象是vLLM Docker container，由于运行容器时遇到了错误导致无法正常工作。

https://github.com/vllm-project/vllm/issues/2025
这个issue类型为bug报告，问题涉及的主要对象是项目中的一个文件。由于拼写错误导致的症状显示有关共享功能的描述错误。

https://github.com/vllm-project/vllm/issues/2024
这个issue属于bug报告，涉及的主要对象是模型加载过程中出现的KeyError异常，可能是由于模型参数加载错误或与模型结构不匹配导致的。

https://github.com/vllm-project/vllm/issues/2023
这是一个关于项目是否遵循官方代码的询问，属于用户提出需求类型。主要涉及对象是 Mixtral 实现。由于用户想确认 Mixtral 实现是否遵循官方发布的 Mixtral 代码，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/2022
这是一个bug报告，涉及的主要对象是Mixtral-8x7B-v0.1模型。由于模型中某些参数的命名错误导致了TypeError，需要修复这个问题。

https://github.com/vllm-project/vllm/issues/2021
这是一个bug报告，涉及主要对象为ARM aarch-64 server在Ubuntu 22.04.3系统下的构建失败。由于缺少numpy模块导致的错误信息提示未能成功获取构建所需的依赖。

https://github.com/vllm-project/vllm/issues/2020
这是一个bug报告，涉及到vLLM中Mixtral模型的问题，由于使用CUDA 12.1安装vLLM后出现了KeyError导致模型无法实例化。

https://github.com/vllm-project/vllm/issues/2019
这是一个bug报告，涉及到在线和离线模型服务速度不一致的问题，用户提出了关于FastAPI在线服务速度比离线服务慢2秒的疑问。

https://github.com/vllm-project/vllm/issues/2018
这是一个bug报告类型的issue，涉及主要对象为vLLM项目中的Mixtral模型，由于模型名称从`mistral`更改为`mixtral`，与`transformers==4.36.0.dev0`兼容性可能会受影响，可能会导致加载权重两次的问题。

https://github.com/vllm-project/vllm/issues/2017
这个issue类型是bug报告，涉及到PaddedGatherOp.forward()方法的参数个数不匹配，导致出现TypeError错误。

https://github.com/vllm-project/vllm/issues/2016
这是一个bug报告，涉及对象为vllm中的一个属性'MistralConfig'，导致问题用户无法使用模型。

https://github.com/vllm-project/vllm/issues/2015
这是一个非bug报告类型的issue，主要涉及的对象是 Mixtral 模型。这个问题单由于 Mixtral 未被添加到支持的模型列表文档中，以及需要对 `mixtral.py` 进行代码清理而产生。

https://github.com/vllm-project/vllm/issues/2014
这是一个bug报告，涉及主要对象是kernels/test_attention.py测试项。由于不同GPU（A100和V100）运行测试时出现错误，用户寻求关于V100上的测试错误的帮助。

https://github.com/vllm-project/vllm/issues/2013
这是一个关于bug的报告，主要涉及的对象是使用v0.2.3 docker时无法识别4090 GPU，可能由于配置问题导致。

https://github.com/vllm-project/vllm/issues/2012
这个issue是关于bug报告，主要涉及的对象是/v1/chat/completions端点的tokenization。由于vLLM的tokenization和HF tokenizers的apply_chat_template方法都会添加特殊token，会导致最终的tokenization包含多余的特殊token。

https://github.com/vllm-project/vllm/issues/2011
用户提出需求，要求添加对`mistralai/Mixtral8x7Bv0.1`和`mistralai/Mixtral8x7BInstructv0.1`模型的支持。

https://github.com/vllm-project/vllm/issues/2010
这是一个用户询问类型的issue，主要涉及的对象是vllm-0.2.1版本和Baichuan2，用户因为某些原因需要使用vllm-0.2.1版本，但不确定该版本是否支持Baichuan2。

https://github.com/vllm-project/vllm/issues/2009
这是一个关于性能优化的问题，主要涉及AWQ Triton kernel中的量化零点解压缩以及相关性能提升，提出了一些疑问和困惑，包括当前Triton kernel如何运行、为什么比CUDA kernel慢等问题。

https://github.com/vllm-project/vllm/issues/2008
这是一个bug报告，主要涉及的对象是vllm模型下的抽样机制。该问题可能是由于多线程并发导致答案不稳定，需要进一步分析导致这种bug的原因。

https://github.com/vllm-project/vllm/issues/2007
这个issue属于bug报告，主要涉及的对象是vllm库中的代码。导致这个问题的原因是设置`prompt_logprobs`参数时会触发`AssertionError: tensor model parallel group is already initialized`异常。

https://github.com/vllm-project/vllm/issues/2006
这个issue属于询问类型，主要涉及KV cache的分配问题，用户想了解源码中KV cache的分配位置以及gpu_cache的作用。

https://github.com/vllm-project/vllm/issues/2005
这是一个Bug报告，涉及的主要对象是VLLM模型的初始化和销毁操作。问题出现的原因是由于未正确调用`destroy_model_parallel`函数导致的初始化问题。

https://github.com/vllm-project/vllm/issues/2004
该issue类型为代码注释，主要对象是关于跳绳缓存的逻辑。

https://github.com/vllm-project/vllm/issues/2003
这是一个代码风格修复的Issue，主要涉及的对象是代码库中名为baichuan的部分，可能是由于代码在风格上不符合规范导致的问题。

https://github.com/vllm-project/vllm/issues/2001
这个issue是用户提出需求，要求添加mamba_chat (2.8b)模型，并询问vllm团队是否能够帮助添加。

https://github.com/vllm-project/vllm/issues/2000
这是一个bug报告，涉及主要对象是Qwen/Qwen7BChat，由于AsyncEngineDeadError导致任务意外完成，需要寻求帮助解决这个错误。

https://github.com/vllm-project/vllm/issues/1999
这是一个Bug报告，主要涉及VLLM项目的软件安装问题。这个问题出现的原因是安装命令下载的wheel文件不支持当前平台导致的错误。

https://github.com/vllm-project/vllm/issues/1998
该issue类型为用户提出需求，主要涉及QuIP# 2-bit Quantization Support。由于用户希望获得该功能的支持，提出了相关需求。

https://github.com/vllm-project/vllm/issues/1997
这是一个功能需求提出的issue，主要涉及的对象是attention kernel。由于head_mapping参数从全局内存加载，需要将其替换为num_kv_heads来避免这种加载。

https://github.com/vllm-project/vllm/issues/1996
这是一个bug报告，涉及OpenAI服务器在模型返回空结果时报错问题，原因是未定义的completion_tokens变量被引用。

https://github.com/vllm-project/vllm/issues/1995
这是一个用户提出需求的类型的 issue，主要涉及如何在8个 A100 GPU 上部署 vllm 模型，并且实现高并发支持。

https://github.com/vllm-project/vllm/issues/1994
这是一个功能需求提出的issue，主要涉及的对象是代码中的`head_mapping`变量。这个问题提出了使用`num_queries_per_kv`替代`head_mapping`，以避免全局内存访问。

https://github.com/vllm-project/vllm/issues/1992
这是一个bug报告，主要对象是vllm下的completion API。由于`echo`代码路径中未完全考虑`logprobs`的存在条件，导致了在OpenAI服务器中v0.2.3版的Streaming功能失效。

https://github.com/vllm-project/vllm/issues/1991
这是关于发布新模型Mixtral的问题报告，主要涉及到新模型的介绍和相关链接。

https://github.com/vllm-project/vllm/issues/1990
这是一个bug报告，主要对象是vllm模型，由于使用gpt2xl和llama27bawq时出现了不完整的回答。

https://github.com/vllm-project/vllm/issues/1989
这是一个bug报告，主要涉及的对象是构建Docker镜像；由于没有提供具体的Outputs信息，导致无法判断构建过程中出现了什么问题。

https://github.com/vllm-project/vllm/issues/1988
这个issue属于用户提出需求类型，主要涉及的对象是vLLM OpenAI server模式下的 `model` 键。用户认为当重新训练并部署模型时，希望请求中的 `model` 被忽略，以便后端能够接受当前活动的任何模型。

https://github.com/vllm-project/vllm/issues/1987
这是一个bug报告类型的issue，主要涉及到Baichuan2-7B-Chat模型的问题。由于使用"ALIBI"替代"ROPE"加载Baichuan27BChat模型导致无法输出eos token。

https://github.com/vllm-project/vllm/issues/1986
这是一个bug报告issue，主要对象是vllm inference server。由于缺乏互联网连接，导致在运行vllm inference server时出现ConnectionError错误，用户希望在没有互联网连接的私有服务器上同时使用h2ogpt和vllm进行推理。

https://github.com/vllm-project/vllm/issues/1985
这个issue是有关性能的问题，主要对象是调度器引入更多填充tokens导致性能下降。

https://github.com/vllm-project/vllm/issues/1984
这是一个bug报告，涉及的主要对象是加载NousResearch/Yarn-Llama-2-70b-32k模型时出现的断言错误。导致这个问题的原因是在`assert max_position == original_max_position * scaling_factor` 中使用了`max_position_embeddings`而不是`max_sequence_length`。

https://github.com/vllm-project/vllm/issues/1983
这个issue类型是用户提出需求，涉及的主要对象是prefix cache功能。由于实现了rudimentary版本的前缀缓存功能在特定场景下表现出显著的性能提升，因此用户提出是否需要此功能，并描述了性能测试结果和需求。

https://github.com/vllm-project/vllm/issues/1982
该issue类型为用户提出需求，用户想知道VLLM是否支持流式输出以及如何解决没有stream_chat接口的问题。

https://github.com/vllm-project/vllm/issues/1981
这是关于代码生成模型（CodeGen）设置过程中出现的bug报告，主要涉及对象是vllm代码生成模型。由于在初次测试时出现了"KeyError: 'transformer.h.0.attn.causal_mask'"错误，可能是由于配置或代码实现上的问题导致了这一异常情况。

https://github.com/vllm-project/vllm/issues/1980
这是一个 bug 报告，主要涉及的对象是 vllm 软件。用户怀疑由于 CUDA 版本不匹配导致版本 >= 0.2.2 产生错误输出。

https://github.com/vllm-project/vllm/issues/1979
这是一个bug报告，针对VLLM模型的输出异常问题，用户使用了StarCoder训练的模型，但VLLM输出与HF输出不同，可能由于模型训练或输入数据问题导致。

https://github.com/vllm-project/vllm/issues/1978
这是一个bug报告，涉及的主要对象是vLLM模型和LLaMA模型参数加载过程。问题是由于LLaMA的`rotary_emb.cos_cached`和`rotary_emb.sin_cached`的缓存值在加载vLLM模型参数时导致KeyError，虽然这些缓存值不是vLLM所需或直接管理的，但却引发了兼容性问题。

https://github.com/vllm-project/vllm/issues/1977
这是一个bug报告，主要涉及的对象是vLLM模型，该问题由于未处理Cached Rotary Embeddings导致在加载LLaMA参数时发生KeyError。

https://github.com/vllm-project/vllm/issues/1976
这是一个bug报告，主要涉及到添加一个标志来在输出文本中包含停止字符串。这个问题的原因是缺少这个功能导致用户无法将停止字符串包含在输出文本中。

https://github.com/vllm-project/vllm/issues/1975
这是一个bug报告，主要涉及运行测试时缺少httpx模块导致的错误。

https://github.com/vllm-project/vllm/issues/1974
这是一个bug报告类型的issue，涉及主要对象是pytest测试。由于在CUDA服务器上运行时打印出错误，并且启动`api_server.py`并使用CURL命令返回合理结果时出现问题。

https://github.com/vllm-project/vllm/issues/1973
这是一个特性需求，主要对象是vLLM项目中的文本生成引擎。由于当前vLLM在生成文本时会去除终止字符串，但用户希望保留终止字符串以便了解生成停止的原因。

https://github.com/vllm-project/vllm/issues/1972
这是一个bug报告，涉及到程序中的一个拼写错误导致编译失败。

https://github.com/vllm-project/vllm/issues/1971
这是一个bug报告，主要涉及的对象是VLLM的代码库中的sampler.py文件。由于忽略了output_token_ids，导致logprobs大于0且使用重算策略交换序列时会出现shape mismatch的IndexError错误。

https://github.com/vllm-project/vllm/issues/1970
这是一个关于bug报告的issue，主要涉及的对象是VLLM项目中的lookahead decoding功能。在尝试实现greedy lookahead decoding过程中，用户遇到了关于`Sequence.append_token_id`行为的疑问，尝试向子序列追加新的tokens后出现了不符合预期的输出。

https://github.com/vllm-project/vllm/issues/1969
这个issue是关于建议性提案，提出了使用pre-commit替代format.sh的建议，主要涉及项目的开发者和贡献者。原因可能是为了简化代码格式化工具的调用，提高项目代码风格的一致性。

https://github.com/vllm-project/vllm/issues/1968
这个issue类型是bug报告，涉及的主要对象是VLLM模型的推断过程。由于使用了tensor parallelism导致在InferenceMode外部对推断张量进行了原地更新，导致了RuntimeError。

https://github.com/vllm-project/vllm/issues/1967
这是一个bug报告，主要涉及OpenAI服务版本升级至v0.2.3后，在Mistral 7B finetune中出现了`output.logprobs`为`None`的错误。 原因可能是与先前的某些Pull Requests有关。

https://github.com/vllm-project/vllm/issues/1966
这是一个bug报告，涉及的主要对象是Latency benchmark script。由于脚本出现错误，导致无法执行，用户需要解决这个问题。

https://github.com/vllm-project/vllm/issues/1965
这是一个bug报告，涉及的主要对象是VLLM代码库。由于输入提示过长导致 UnboundLocalError 异常。

https://github.com/vllm-project/vllm/issues/1964
这是一个Bug报告，涉及的主要对象是 quickstart.rst 示例。由于一个小错误导致混淆，示例中的后缀名应为 `.jinja` 而非 `.json`。

https://github.com/vllm-project/vllm/issues/1963
这个issue属于用户提出的问题，主要涉及的对象是vllm的beam search功能。由于结果几乎相同，用户询问是否支持多样性beam search。

https://github.com/vllm-project/vllm/issues/1962
这是一个bug报告，主要涉及的对象是针对baichuan模板在使用chatml时出错的问题。这个问题可能是由于默认的baichuan部署响应结果的语义不合理，需要在cli中指定baichuanstyle模板才能得到正确的结果。

https://github.com/vllm-project/vllm/issues/1961
这是一个用户提出问题的issue，主要涉及VLLM后端在benchmark_throughput.py运行时请求的批处理大小是多少的问题。

https://github.com/vllm-project/vllm/issues/1960
这是一个用户询问问题的类型的issue，主要涉及Baichuan213B量化模型的支持问题。由于用户对vllm是否支持Baichuan213B量化模型感到困惑，导致需要解决这个问题。

https://github.com/vllm-project/vllm/issues/1959
这个issue是一个bug报告，主要涉及的对象是VLLM库中的CUDA内核，由于缺少设备保护导致非法内存访问错误。

https://github.com/vllm-project/vllm/issues/1958
这是一个关于使用vllm时遇到生成文本重复的bug报告，问题涉及到vllm库的使用。可能的原因是在greedy SamplingType下设置了不正确的参数引起了文本重复。

https://github.com/vllm-project/vllm/issues/1957
这是一个Bug报告，涉及主要对象为RayWorkerVllm。由于使用了`--tensor-parallel-size 4`导致部分GPU卡在100%利用率并卡住，最终出现NCCL操作超时导致程序崩溃的症状。

https://github.com/vllm-project/vllm/issues/1956
这是一个bug报告，该问题涉及的主要对象是InternLM模型。由于InternLM模型不遵循`config.json`中的`rope_scaling`参数，导致长文本输入时出现错误。

https://github.com/vllm-project/vllm/issues/1955
这是一个关于功能需求的issue，主要涉及的对象是v100显卡，用户提出了关于该显卡是否被支持的问题。

https://github.com/vllm-project/vllm/issues/1954
这个issue类型是性能问题报告，主要涉及到多GPU并发处理时的延迟比较长的情况。原因可能是调度花费的时间比较多，导致了8GPUs系统处理速度慢于2GPUs系统。

https://github.com/vllm-project/vllm/issues/1953
这个issue类型是bug报告，涉及的主要对象是VLLM运行时内存错误。该问题可能是由于内存错误导致出现错误提示信息并无法继续查询操作。

https://github.com/vllm-project/vllm/issues/1952
这是一个bug报告，主要涉及LLama270B(AWQ quantized)模型在输入超过4096 tokens时出现错误"RuntimeError: probability tensor contains either `inf`, `nan` or element < 0"，可能是由于模型处理超长输入时出现了概率张量中包含无穷大、NaN或负值的情况导致的。

https://github.com/vllm-project/vllm/issues/1951
这是一个用户提出需求的类型issue，主要涉及对象是希望了解有关该项目性能方面的详细信息。具体原因是用户希望了解这个项目对延迟和吞吐量的影响。

https://github.com/vllm-project/vllm/issues/1950
这是一个bug报告，涉及的主要对象是Docker。由于缺少cuda arch list作为构建选项，导致无法加载`vllmopenai/v0.2.3`镜像，可能引起了CC错误并可能导致未来的回归。

https://github.com/vllm-project/vllm/issues/1949
这是一个bug报告类型的issue，涉及的主要对象是torch.cuda内存分配。由于内存分配问题导致CUDA内存不足，可能是由于模型加载或分配内存方式错误导致。

https://github.com/vllm-project/vllm/issues/1948
这是一个用户需求问题，主要涉及的对象是vLLM代码库。该问题的原因是vLLM目前在代码中大量使用`cuda`，导致只能兼容NVIDAI/AMD/Intel GPU，无法支持其他加速器，因此需要使用更通用的抽象（如DeepSpeed的xpu）来支持更多种硬件。

https://github.com/vllm-project/vllm/issues/1947
这个issue是一个bug报告，涉及的主要对象是vllm项目中的一个文档文件。这个问题是由于拼写错误导致的，用户指出了“adpated”应当更改为“adapted”。

https://github.com/vllm-project/vllm/issues/1946
这是一个关于bug报告的issue，主要涉及vllm中的streaming功能。由于未能按标记一次一个token地进行流式处理，导致用户在Jupyter Notebook中无法正确实现流式操作。

https://github.com/vllm-project/vllm/issues/1945
这是一个功能需求提报，主要对象是 SamplingParams 类。这个问题由于生成的 tokens 过短而导致，用户提出需要添加 min_tokens 参数来避免这种情况。

https://github.com/vllm-project/vllm/issues/1943
这是一个用户提出需求的issue，主要涉及Qwen模型的chat_stream替代generate功能，原因是之前的generation参数存在问题。

https://github.com/vllm-project/vllm/issues/1942
这是一个bug报告类型的issue，主要涉及的对象是vllm中的LLM引擎。由于在多GPU加速器运行环境中，当温度大于等于1.6且top_p大于等于0.9时，出现了断言错误，导致了TP问题无法恢复。

https://github.com/vllm-project/vllm/issues/1941
这是一个bug报告，主要涉及 AWQ quantization 算法不支持 bfloat16 激活数据类型的问题，用户希望该算法能够支持 bfloat16 作为激活数据类型。

https://github.com/vllm-project/vllm/issues/1940
这是一个用户提出需求的issue，主要涉及如何在多个gpu上启用数据并行ism，用户希望实现数据并行ism来提高推理性能，但尝试使用多线程进行并行请求导致运行时问题。

https://github.com/vllm-project/vllm/issues/1939
这是一个 bug 报告，涉及到 VLLM 引擎的日志记录问题。原因是在某个提交中移除了实际的提示日志记录功能，但引擎仍然会记录提示信息。

https://github.com/vllm-project/vllm/issues/1938
这是一个功能增强类的issue，主要涉及对象是"MPTAttention"模块，原因是需要修改"mpt.py"来支持GQA。

https://github.com/vllm-project/vllm/issues/1937
这是一个bug报告，主要涉及的对象是在使用多个GPU时执行`benchmarks/benchmark_latency.py`时出现错误。这个问题可能是由于配置或代码问题导致的。

https://github.com/vllm-project/vllm/issues/1936
这是一个bug报告，主要涉及vllm-openai版本升级后出现的无法加载图像的问题。

https://github.com/vllm-project/vllm/issues/1935
这是一个bug报告，问题涉及到在WSL2中使用vllm进行基准测试时遇到的模型最大序列长度限制导致的索引错误。

https://github.com/vllm-project/vllm/issues/1934
这是一个bug报告，涉及vllm项目中的缓存驱逐机制，可能由于逻辑问题导致在重新安排请求时立即取消了某些请求。

https://github.com/vllm-project/vllm/issues/1933
这是一个bug报告类型的issue，主要涉及的对象是使用vllm推理gptj模型时产生的结果缺少部分信息。这可能是由于模型设置或参数调整不当导致的。

https://github.com/vllm-project/vllm/issues/1932
这是一个用户需求问题，涉及模型加载器的兼容性问题，导致不能支持ChatGLMForConditionalGeneration。

https://github.com/vllm-project/vllm/issues/1931
该issue是一个bug报告，主要涉及的对象是vllm加载Int8-quantified (smoothquant) Codellama-13B模型时出现错误。由于引擎初始化过程中出现问题，导致加载模型和进行推断时出错。

https://github.com/vllm-project/vllm/issues/1930
这是一个bug报告，涉及的主要对象是CUDA 12.1版本的vLLM的0.2.3版本。由于双重释放导致了错误。

https://github.com/vllm-project/vllm/issues/1929
这是关于软件bug的报告，主要涉及 VLLM 软件在v2.2或更高版本下无法生成内容的问题。

https://github.com/vllm-project/vllm/issues/1928
这是一个关于代码设计讨论的issue，主要讨论了是否需要将head_mapping作为参数传递给paged_attention kernel，可能由于实现模型需要表达query和key-value之间的关系而引起了讨论。

https://github.com/vllm-project/vllm/issues/1927
这个issue是关于bug报告，涉及的主要对象是vLLM模型的serving server。由于GPU KeyValue cache使用率达到10%，导致服务器出现hang的问题。

https://github.com/vllm-project/vllm/issues/1926
该issue类型为优化建议，主要涉及的对象是模型执行时的CUDA图，提出了使用CUDA图优化CPU开销的建议。

https://github.com/vllm-project/vllm/issues/1925
这是一个关于bug报告类型的issue，主要涉及的对象是vllm中的KV缓存清除功能。由于缓存未清除可能导致形状错误，用户提出了关于为什么vllm不需要清除缓存以及是否存在bug的疑问。

https://github.com/vllm-project/vllm/issues/1924
这是一个bug报告，主要涉及到Red Hat Enterprise Linux release 9.3 主机中在Docker中无法找到CUDA驱动或设备，可能是由于未能正确检测到CUDA驱动和nvidiasmi命令而导致。

https://github.com/vllm-project/vllm/issues/1923
这是一个bug报告，涉及到构建版本环境变量的支持问题。

https://github.com/vllm-project/vllm/issues/1922
这是一个Bug报告，涉及软件版本不一致问题，导致Pip无法正确识别软件版本。

https://github.com/vllm-project/vllm/issues/1921
这个issue类型是一个需求提出，主要涉及对象是vllm软件在macOS上是否支持M2芯片，用户想知道软件能否在此平台上正常运行。

https://github.com/vllm-project/vllm/issues/1920
这是一个bug报告，主要涉及到Chatglm2-6b-32k通过vLLM加速后对话异常终止的问题。这可能是因为vLLM对Chatglm2的适配引起的异常，导致对话在第三轮输出异常终止。

https://github.com/vllm-project/vllm/issues/1919
这是一个bug报告，涉及主要对象为在使用VLLM过程中出现的quantization方法错误，由于未知的quantization方法'gptq'导致产生了ValueError异常。

https://github.com/vllm-project/vllm/issues/1918
这个issue属于bug报告类型，主要涉及vllm下的chatglm3-6b模型自动输出特殊token，导致了程序表现异常。

https://github.com/vllm-project/vllm/issues/1917
这是一个bug报告类型的issue，主要涉及VLLM中的对话进行不正常终止，可能是由于聊天模板出现问题所导致。

https://github.com/vllm-project/vllm/issues/1916
这是一个用户提出需求的issue，主要对象是支持 Yi-34B-Chat-8bits（gptq），用户想知道何时可以支持该语言模型。

https://github.com/vllm-project/vllm/issues/1915
该issue类型为用户提出需求，请教问题，主要涉及对象是Tensor Core和CUDA Core。用户询问了关于在A100上默认使用Tensor Core进行推理，以及针对int4量化模型推理是否需要反量化计算的问题。

https://github.com/vllm-project/vllm/issues/1914
这个issue是关于bug报告，主要涉及到vllm中的chatml token。这个问题可能是由于服务启动时重复生成chatml token导致的。

https://github.com/vllm-project/vllm/issues/1913
这是一个bug报告，主要涉及对象是使用Sheared Llama 1.3B ShareGPT时出现的AssertionError错误。原因是调用`LLM`时参数设置不正确导致。

https://github.com/vllm-project/vllm/issues/1912
这是一个bug报告，该问题涉及的主要对象是API服务器的启动命令。由于缺少了'quant_method'参数，导致了KeyError，从而引发了错误。

https://github.com/vllm-project/vllm/issues/1911
这是一个bug报告，主要涉及VLLM库中的CUDA错误导致无法初始化CUBLAS标准的问题。

https://github.com/vllm-project/vllm/issues/1910
这是一个关于如何在没有互联网连接的情况下运行 VLLM 服务的问题，主要涉及到 VLLM 模型在离线模式下无法正常工作的情况。

https://github.com/vllm-project/vllm/issues/1909
这是一个用户提出需求的issue，主要对象是vllm模型，用户想要在使用`.generate`生成文本后能够反向传播梯度到加载的模型。

https://github.com/vllm-project/vllm/issues/1908
该issue类型为用户提出需求，关注主要对象为vllm.LLM模型。用户提出了如何终止vllm.LLM并释放GPU内存的问题。

https://github.com/vllm-project/vllm/issues/1907
该issue为关于项目是否支持cuda version 11.6的询问，类型为技术支持。

https://github.com/vllm-project/vllm/issues/1906
这个issue类型是用户请求帮助，主要对象是无法解决特定问题。由于无法解决问题而导致用户寻求帮助。

https://github.com/vllm-project/vllm/issues/1905
这是一个bug报告，主要涉及Mistral AWQ模型在CUDA 12.2下出现的"INTERNAL ASSERT FAILED"错误。

https://github.com/vllm-project/vllm/issues/1903
这是一个待解决的Bug报告，涉及到VLLM的并行计算问题。由于TP参数大于1时，num_gpus设定不正确，导致需要等待CC修复此问题。

https://github.com/vllm-project/vllm/issues/1902
这是一个bug报告，涉及Beam Search结果问题，用户提出了关于生成截断序列的疑问。

https://github.com/vllm-project/vllm/issues/1901
这是一个bug报告，主要涉及的对象是vLLM库。由于GPU的compute capability不支持bfloat16，导致出现数值错误，用户在Colab笔记本中无法使用vLLM。

https://github.com/vllm-project/vllm/issues/1900
这个issue属于bug报告类型，涉及主要对象是`test_worker.py`文件。原因是`test_worker.py`文件存在问题导致测试失败，需要修复并重命名为`test_model_runner.py`。

https://github.com/vllm-project/vllm/issues/1899
这是一个功能更新类型的issue，涉及主要对象为Yi模型和配置文件。这个问题由于Yi作者改变了他们的代码/配置文件以兼容LLaMA，导致用户需要慢慢移除该模型和配置代码。

https://github.com/vllm-project/vllm/issues/1898
这是一个用户提出需求的issue，主要涉及的对象是vllm库中的custom layers，用户希望添加PyTorch原生实现的自定义层，并列出了该变动带来的多方面益处。

https://github.com/vllm-project/vllm/issues/1897
这个issue是关于用户提出需求的，主要涉及如何在OpenRLHF中加载本地LLM并重新调整LLM引擎的权重。用户提出这个问题可能是因为需要在训练期间重新调整LLM模型的权重，并释放vLLM GPU内存以为训练留更多内存。

https://github.com/vllm-project/vllm/issues/1896
这是一个bug报告，主要涉及的对象是sampler tests。该问题是由于重构worker和InputMetadata后，导致sampler tests出现了问题。

https://github.com/vllm-project/vllm/issues/1895
这是一个用户提出问题的issue，主要涉及的对象是PagedAttention模块。用户询问为什么在PagedAttention中使用`memory_efficient_attention_forward`，并询问能否替换为普通的attention。

https://github.com/vllm-project/vllm/issues/1894
这是一个bug报告，主要涉及服务器无法在多个GPU上启动OpenAI server的问题，可能出现的原因是服务器一直处于初始化状态，具体问题需要查看nvidiasmi日志进行进一步分析。

https://github.com/vllm-project/vllm/issues/1893
这是一个用户提出需求类型的issue，主要涉及到添加一个选项来在docker构建过程中使用nvcc_threads来防止OOM，这个需求的提出是因为在构建过程中出现了内存耗尽的问题。

https://github.com/vllm-project/vllm/issues/1892
这是一个用户提出需求的类型，主要涉及VLLM模型中的presence_penalty或frequency_penalty带来的问题。用户发现在生成长文本时，使用这两个超参数会导致标点符号在文本中逐渐消失，推测是因为这两个参数使标点符号被过度惩罚。

https://github.com/vllm-project/vllm/issues/1891
这是一个关于系统运行时可能出现的问题的类型，涉及到kv缓存容量不足时可能会发生的情况，用户关注当缓存容量不足时会发生什么，是否会导致OOM错误或者部分缓存会被迁移到CPU。

https://github.com/vllm-project/vllm/issues/1890
这是一个需求提交类型的issue，主要涉及为VLLM项目添加以Prometheus格式的生产指标。

https://github.com/vllm-project/vllm/issues/1889
这是一个优化性质的issue，主要涉及sampler的优化，由于以前的操作导致GPU无法连续运行，现在通过重排操作来减少采样开销以提高性能。

https://github.com/vllm-project/vllm/issues/1888
这个issue是一个性能问题类型，主要涉及到Tensor Parallelism (Mistral 7B + FP16)。由于Tensor Parallelism没有很好地扩展，导致性能不如预期，用户提出了关于性能缩放和使用Ray/KubeRay的问题。

https://github.com/vllm-project/vllm/issues/1887
该issue类型属于用户提出需求，要求添加repetition_penalty支持到OpenAI API。

https://github.com/vllm-project/vllm/issues/1886
这是一个bug报告，主要涉及SamplingParams文件中文档注释中的一个拼写错误。原因是在文档注释中的“special”一词存在拼写错误导致问题。

https://github.com/vllm-project/vllm/issues/1885
这是一个关于实现差异的issue，主要涉及vLLM中的`top_p`和`top_k`采样方法。由于在实现上的不同导致了概率分布生成可能不一致，排序顺序、筛选方式等细节也存在差异。

https://github.com/vllm-project/vllm/issues/1884
这是一个用户提出需求的问题，主要涉及如何注册用户修改的模型和分词器，原因是当前使用受限于特定目录中的模型和分词器，希望能够在不将其纳入vllm或transformers的情况下进行注册。

https://github.com/vllm-project/vllm/issues/1883
这个issue类型是bug报告，涉及的主要对象是AWQ文档更新，由于性能问题导致了用户遇到了相关症状的问题。

https://github.com/vllm-project/vllm/issues/1882
这个issue类型是一个功能需求，主要涉及 AWQ Quantization 的内核更新和优化。原因是当前AWQ内核未经优化，用户希望更新内核并测试新内核性能。

https://github.com/vllm-project/vllm/issues/1881
这个issue类型是用户提出需求，主要涉及对象是Qwen-14B-Chat-Int4(gptq)，用户寻求帮助使用vllm解决问题。

https://github.com/vllm-project/vllm/issues/1880
这是一个性能优化类型的issue，主要涉及到vLLM中的MQA/GQA模块的优化问题。由于vLLM当前没有使用优化的内核，导致无法发挥MQA/GQA的优势，可能导致性能下降。

https://github.com/vllm-project/vllm/issues/1879
这是一个bug报告，主要涉及的对象是OpenAI Chat API with vLLM。由于缺少完整的请求消息内容导致在客户端尝试与服务器进行对话时出现问题。

https://github.com/vllm-project/vllm/issues/1878
这是一个关于BUG报告的issue，主要涉及ChatGLM2-6B-32K模型使用vllm激素和外挂知识库后生成内容重复的问题。原因可能是在长文本交互过程中出现了内容重复生成的情况。

https://github.com/vllm-project/vllm/issues/1877
这是一个bug报告，涉及的主要对象是vllm的4bit模型。由于CUDA内存耗尽导致的错误。

https://github.com/vllm-project/vllm/issues/1876
这个issue属于代码改进类型，主要涉及Baichuan 2的权重归一化处理。原因是缺少权重归一化导致Baichuan 2支持出现问题。

https://github.com/vllm-project/vllm/issues/1875
这是一个bug报告，涉及的主要对象是vllm与OpenAI API的兼容性，由于SSL证书验证错误导致了无法使用OpenAI API。

https://github.com/vllm-project/vllm/issues/1874
这个issue是一个bug报告，涉及的主要对象是vLLM Baichuan tokenizer。原因是由于`transformers >= 4.34`导致Baichuan tokenizer加载出现错误。

https://github.com/vllm-project/vllm/issues/1873
这是一个bug报告，涉及到vllm在多GPU上无法通过tensor-parallel提高吞吐量的问题。可能的原因是tensor parallelism配置不当导致性能下降。

https://github.com/vllm-project/vllm/issues/1872
这是一个bug报告，涉及到vllm项目中的代码文件sampling_params.py的第72行，描述了Special这个词在此处不正确。可能是由于拼写错误或者逻辑问题导致了该症状的bug。

https://github.com/vllm-project/vllm/issues/1871
该issue为用户提出需求类型，主要对象是保存pytorch profiler输出用于延迟基准测试，因为用户希望能够保存跟踪文件以便在网站如https://ui.perfetto.dev/或tensorboard中查看。

https://github.com/vllm-project/vllm/issues/1870
这是一个特性请求，主要关注在项目中添加延迟指标。

https://github.com/vllm-project/vllm/issues/1869
这个issue属于功能需求提议，主要涉及到OpenAI兼容服务的参数更新。

https://github.com/vllm-project/vllm/issues/1868
这是一个用户提出需求的issue，主要涉及VLLM模型的自定义服务器参数，原因是用户认为添加`min_p`和`repetition_penalty`这些非官方参数对扩展功能有用。

https://github.com/vllm-project/vllm/issues/1867
这是一个bug报告，涉及主要对象为RoPE类的缓存键错误，由于CC(Avoid multiple instantiations of the RoPE class)引入的错误使得在模型具有`rope_scaling`配置时，缓存中的`key`会在初始化时抛出错误。

https://github.com/vllm-project/vllm/issues/1866
这是一个用户提议的issue，涉及的主要对象是为了支持在Inferentia上进行LLM推断。可能由于硬件加速器的差异，需要对transformersneuronx和vLLM进行调整和集成。

https://github.com/vllm-project/vllm/issues/1865
这是一个BUG报告类型的issue，主要涉及vllm在使用torchrun启动时会hang的问题。这可能是由于vllm在使用`torchrun`时出现问题，导致在初始化模型并打印日志后程序hang住。

https://github.com/vllm-project/vllm/issues/1864
这个issue类型为用户提出需求，涉及的主要对象是API server。原因可能是用户想了解API server是否支持动态批处理发送请求。

https://github.com/vllm-project/vllm/issues/1863
这是一个用户提出需求的issue，主要对象是要求支持INT8量化的llama模型。用户希望能够直接使用vllm推理已量化为INT8的模型，但目前使用smoothquant和torchint存在兼容性问题。

https://github.com/vllm-project/vllm/issues/1862
这是关于集成Vllm与Langchain和Streaming的问题，用户遇到的问题是无法将数据以流的方式处理，导致无法像预期的那样工作。

https://github.com/vllm-project/vllm/issues/1861
This issue is a bug report related to the chat model "Yi-34B-Chat" in vllm 0.2.2 where the model does not stop until "finish_reason" "length" is reached, and tokens between text are not displayed, only "\n" is seen.

https://github.com/vllm-project/vllm/issues/1860
这是一个bug报告，主要涉及的对象是Qwen-style dynamic-NTK ROPE kernel实现。由于动态NTK ROPE内核在处理不同长度的提示时导致生成的文本质量下降，甚至出现了无意义的输出，用户希望解决这一问题。

https://github.com/vllm-project/vllm/issues/1859
这个issue属于bug报告，主要涉及的对象是使用vllm运行Yi34BChat4bit。由于参数设定tensorparallelsize为4时无法运行，只有设定为2才能正常运行，且输出内容会重复直到max_tokens停止。

https://github.com/vllm-project/vllm/issues/1858
这是一个bug报告，涉及的主要对象是通过设置max-model-len参数进行吞吐量基准测试。导致问题的原因是默认值可能导致torch.cuda.OutOfMemoryError错误。

https://github.com/vllm-project/vllm/issues/1857
这是一个用户提出需求的issue，主要涉及如何获取LLM隐藏状态，由于用户需要使用最终输入token的隐藏状态来表示整个句子，但在源代码中并没有找到直接满足该需求的解决方案。

https://github.com/vllm-project/vllm/issues/1856
这是一个需求相关的issue，主要涉及的对象是软件版本[v0.2.3] Release Tracker。原因可能是为了添加新功能、修复问题或进行改进。

https://github.com/vllm-project/vllm/issues/1855
这个issue是关于bug报告，主要涉及benchmark_latency.py文件，由于`max_num_seqs`参数导致当batchsize=1且n=2时程序无响应。

https://github.com/vllm-project/vllm/issues/1854
这是一个用户提出需求的issue，主要对象涉及的是程序的性能优化。导致该需求的原因是为了保存性能分析结果方便后续分析。

https://github.com/vllm-project/vllm/issues/1853
这是一个关于性能优化的问题，涉及到GPTQ W4A16和AWQ W4A16的实现比较，用户想了解它们谁更快，是否可以共享相同的CUDA函数，由于两者的数学计算类似，希望得到回复解答。

https://github.com/vllm-project/vllm/issues/1852
这个issue是关于bug报告，涉及到Fix num_gpus参数。由于 TP > 1 导致了 num_gpus 参数没有正确修复的bug。

https://github.com/vllm-project/vllm/issues/1851
这是一个bug报告，主要涉及的对象是vllm项目中的 num_gpus 参数。这个问题可能是由于某种情况下 num_gpus 参数无法正确设定，导致程序出现问题。

https://github.com/vllm-project/vllm/issues/1850
这个issue类型是需求提出，主要对象是优化调度中的列表操作，导致需求是为了通过使用Numpy来减少列表操作。

https://github.com/vllm-project/vllm/issues/1849
这是一个用户提出需求的issue，主要对象是添加获取内存统计数据的API接口。原因是当前没有可用的接口来监控内存使用情况，因此用户请求添加这样的接口来进行监控。

https://github.com/vllm-project/vllm/issues/1848
这是一个bug报告，主要涉及vllm库中使用`prompt_token_ids`时出现的`ValueError`，可能是由于长prompt序列长度导致的形状不匹配错误。

https://github.com/vllm-project/vllm/issues/1847
这个issue是一个bug报告，主要涉及的对象是vllm模型。这个问题可能是由于在500~1000次迭代后，使用`logprobs > 0`时导致`IndexError`异常。

https://github.com/vllm-project/vllm/issues/1846
这是一个bug报告，涉及到vllm框架中模型加载的问题。由于使用vllm加载vicuna7bv1.5模型时卡住，而在fastchat框架中工作正常，导致在加载过程中出现卡死现象。

https://github.com/vllm-project/vllm/issues/1845
这个issue属于bug报告类型，涉及使用docker中的共享内存，在docker容器中出现了与`tensorparallelsize`有关的崩溃问题。

https://github.com/vllm-project/vllm/issues/1844
这是一个用户提出需求的类型，主要涉及的对象是vllm cli，用户希望创建一个更方便的入口点用于运行openai兼容服务器，并解决`cuda_utils`部分导入的常见问题。

https://github.com/vllm-project/vllm/issues/1843
该issue类型为代码重构，涉及主要对象为worker和InputMetadata，原因是为了优化代码结构和提高性能。

https://github.com/vllm-project/vllm/issues/1842
这是一个关于需求讨论的issue，主要涉及Cache Events是否必要的讨论。原因在于操作都在同一流中进行，可能导致一些同步问题。

https://github.com/vllm-project/vllm/issues/1841
这个issue是关于bug报告，主要涉及的对象是 RoPE 在 ChatGLM-32K 中的实现，由于错误实现导致了问题。

https://github.com/vllm-project/vllm/issues/1840
这个issue是关于重构注意力机制的，主要涉及到`PageAttention`模块。该修改涉及简化`forward`方法，移除了`PagedAttentionWithRoPE`和`PagedAttentionWithALiBi`，以及对不同模型的测试。

https://github.com/vllm-project/vllm/issues/1839
该issue属于功能需求的类型，主要涉及的对象是Latency benchmark。这个需求是希望在性能测试中添加profile选项以便使用PyTorch分析GPU活动。

https://github.com/vllm-project/vllm/issues/1838
这是一个bug报告，涉及到VLLM项目中使用两个4090 GPU 运行性能不佳的问题。可能是由于程序或配置问题导致。

https://github.com/vllm-project/vllm/issues/1837
这是一个用户提出需求的类型的issue，该问题单涉及的主要对象是vLLM的文档。

https://github.com/vllm-project/vllm/issues/1836
这个issue是关于合并EmbeddedLLM/vllm-rocm到vLLM主分支的需求，主要涉及添加ROCm支持。原因是之前的PR中有太多更改，所以需要新的pull request来继续这项工作。

https://github.com/vllm-project/vllm/issues/1835
这是一个bug报告，涉及主要对象是LLM模型。由于无法实现模型的流式输出，可能是由于代码逻辑错误或者参数设置问题导致的。

https://github.com/vllm-project/vllm/issues/1834
这是一个用户提出需求的类型，主要涉及的对象是VLLM。由于用户希望VLLM能够支持DeepSeek模型，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/1833
这个issue类型为bug报告，涉及的主要对象是VLLM的多GPU利用。由于同时启动两个GPU导致内存不足错误。

https://github.com/vllm-project/vllm/issues/1832
这是一个bug报告，主要涉及的对象是docker build错误。由于代码问题导致的docker构建错误。

https://github.com/vllm-project/vllm/issues/1831
这个issue是一个bug报告，涉及的主要对象是vllm项目的Docker构建过程。原因是由于缺少setuptools>=49.4.0导致在构建Docker时出现数值错误。

https://github.com/vllm-project/vllm/issues/1830
这是一个bug报告，涉及到vllm中的一个模型推断错误，导致出现"an illegal memory access was encountered"错误。

https://github.com/vllm-project/vllm/issues/1829
这是一个用户提出需求的issue，主要涉及到的对象是模型训练参数。这个问题提出了关于early stopping 参数设置以及beam search的使用关系的疑问。

https://github.com/vllm-project/vllm/issues/1828
这个issue是关于bug报告，主要对象是RoPE类，问题是避免多次实例化RoPE类可能会导致的问题。

https://github.com/vllm-project/vllm/issues/1827
这是一个用户的需求问题，主要涉及到如何指定使用的GPU编号。由于默认使用的GPU编号是cuda:0，用户想知道如何指定其他GPU编号来使用。

https://github.com/vllm-project/vllm/issues/1826
这是一个bug报告。该问题涉及的主要对象是vllm在Google Colab中无法运行。可能是由于安装过程中的某些步骤问题导致无法成功运行。

https://github.com/vllm-project/vllm/issues/1825
这是一个关于模型推断结果不一致的bug报告，涉及的主要对象是vllm的chatglm3模型和huggingface源。可能是由于模型版本不一致或参数设置问题导致推断结果异常。

https://github.com/vllm-project/vllm/issues/1824
这是一个bug报告，该问题涉及到网络连接问题。由于网络不可访问导致了无法使用本地模型的情况。

https://github.com/vllm-project/vllm/issues/1823
这是一个bug报告，主要涉及对象是使用qwen的awq模型时出现的ValueError，可能是由于torch.bfloat16类型不被支持导致。

https://github.com/vllm-project/vllm/issues/1822
这是一个BUG报告，该问题单涉及的主要对象是项目的主分支（main branch）。原因可能是代码格式错误导致的bug。

https://github.com/vllm-project/vllm/issues/1821
该issue类型为需求提出，涉及主要对象为Vllm与Ray Serve的集成。由于Vllm向Ray请求了所有资源，导致无法与其他模型同时使用，用户提出需要更好地与Ray Serve整合以避免此限制。

https://github.com/vllm-project/vllm/issues/1820
这是一个用户提出需求的类型的issue，主要涉及到vLLM模型在使用`best_of`采样时对累积对数概率进行标准化的选项。

https://github.com/vllm-project/vllm/issues/1819
该issue是一个bug报告，主要涉及到OPT weight loading的问题，由于未添加`model.`前缀导致在测试model "facebook/opt6.7b"时出现了KeyError。

https://github.com/vllm-project/vllm/issues/1818
这是一个代码质量问题，需要修正注释信息。

https://github.com/vllm-project/vllm/issues/1817
这是一个关于代码改进的issue，主要涉及的对象是ops.h头文件。由于没有添加头文件保护宏导致了重定义错误。

https://github.com/vllm-project/vllm/issues/1816
这是一个bug报告，该问题涉及到VLLM中的QWen模块，在使用awq和设置tensor_parallel_size为2时，出现了输入尺寸错误。原因是在代码执行过程中出现了参数设置错误导致的bug。

https://github.com/vllm-project/vllm/issues/1815
这是一个bug报告类型的issue，主要涉及vllm在AWQ模型推断过程中出现"Exception: No such file or directory"错误，可能由于缺少相关文件或目录所导致。

https://github.com/vllm-project/vllm/issues/1814
这是一个bug报告，涉及到模块导入问题，用户遇到了模块未找到的错误。由于某种原因导致了"vllm._C"模块未能正确导入，导致出现ModuleNotFoundError。

https://github.com/vllm-project/vllm/issues/1813
这是一个关于安装vLLM时出现问题的bug报告，涉及的主要对象是安装vLLM的用户。由于安装过程中可能未正确配置Python版本和重新安装PyTorch导致的API服务器运行失败。

https://github.com/vllm-project/vllm/issues/1812
这是一个bug报告，主要涉及的对象是pip安装时出现的依赖问题导致的安装失败。原因是缺少了必要的numpy模块和CUDA驱动不匹配。

https://github.com/vllm-project/vllm/issues/1811
该issue类型为用户提出需求，询问Vllm是否兼容推理自定义微调模型，涉及主要对象是Vllm模型。

https://github.com/vllm-project/vllm/issues/1810
这是一个bug报告，主要涉及的对象是模型参数匹配问题，由于模型结构和参数不匹配导致出现KeyError异常。

https://github.com/vllm-project/vllm/issues/1809
这是一个bug报告类型的issue，主要涉及的对象是vllm-master（0.2.2）和llama，由于多卡并行推理时会出现乱码和重复的现象。

https://github.com/vllm-project/vllm/issues/1808
这是一个bug报告类型的issue，涉及对象是添加"core42/jais-13b-chat"模型支持时遇到的问题，由于`(relative_pe): AlibiPositionEmbeddingLayer()`，导致无法加载checkpoint，作者请求帮助解决这个问题。

https://github.com/vllm-project/vllm/issues/1807
这是一个用户提出需求的issue，该问题涉及的主要对象是请求在CUDA 11.8、Torch 2.0.1和Python 3.9环境下发布一个适用的vllm.whl文件。这个问题由于当前vllm版本与CUDA 11.8不兼容，用户希望得到一个兼容的解决方案。

https://github.com/vllm-project/vllm/issues/1806
这是一个bug报告，主要涉及VLLM加速ChatGLM2-6B-32K进行长文本生成时出现无意义重复生成问题导致崩溃的情况。

https://github.com/vllm-project/vllm/issues/1805
这是一个关于性能问题的bug报告，主要涉及Llama 7B的latency benchmark，由于output长度为1和128时延迟相似，可能需要查看benchmark_latency.py脚本是否存在问题。

https://github.com/vllm-project/vllm/issues/1804
这是一个功能增强的issue，主要涉及到为VLLM添加对多个LoRA适配器的支持。原因是为了实现在单个批处理中运行多个LoRA适配器，以类似于SLoRA和punica项目的方式。

https://github.com/vllm-project/vllm/issues/1803
这是一个bug报告，涉及的主要对象是类命名。原因是命名错误导致代码中出现了不一致的类名，需要修复。

https://github.com/vllm-project/vllm/issues/1802
这个issue是一个用户提出需求的特性请求，主要涉及的对象是VLLM模型。由于当前的输入导向任务（比如总结和代码修改）的性能需求，用户希望实现Prompt lookup decoding (PLD)来提高模型的吞吐量。

https://github.com/vllm-project/vllm/issues/1801
这是一个bug报告，涉及到Nvidia驱动版本545.29.02导致vLLM在调用`tensorparallelsize 2`时出现问题，无法运行大于单个GPU RAM的模型。

https://github.com/vllm-project/vllm/issues/1800
这个issue是用户提出的需求。主要对象是将fastchat Conversation中的特殊tokens作为默认的停用词加入到OpenAI API服务器中。

https://github.com/vllm-project/vllm/issues/1799
这是一个bug报告，涉及vLLM在初始化过程中下载来自HuggingFace服务器的小型GPT-2模型导致无法在无互联网访问的生产环境中正常运行的问题。

https://github.com/vllm-project/vllm/issues/1798
这是一个用户提出问题的issue，主要涉及VLLM模型的执行过程，用户怀疑存在懒加载机制导致执行时间短暂的问题。

https://github.com/vllm-project/vllm/issues/1797
这是一个正在进行中的实现 speculative decoding 的任务，类型为功能开发，主要针对 vllm 中的实现。

https://github.com/vllm-project/vllm/issues/1796
这是一个功能需求问题，主要涉及对象是模型初始化在GPU上以减少CPU内存占用。这个问题可能是由于模型在CPU上初始化导致内存占用过高而提出的。

https://github.com/vllm-project/vllm/issues/1795
这是一个优化性能的issue，主要涉及的对象是模型执行过程。问题产生的原因是为了减少CPU开销，通过使用`torch.compile`与CUDA图形来实现优化。

https://github.com/vllm-project/vllm/issues/1794
这是一个bug报告，涉及的主要对象是VLLM框架不能支持XverseForCausalLM模型。导致此问题的原因是VLLM版本0.2.1目前不支持该模型架构。

https://github.com/vllm-project/vllm/issues/1793
这是一个寻求帮助的问题，主要涉及到如何使用`nvcc`将`vllm`中的`csrc`部分编译成目标文件。问题出现的可能原因是编译CUDA内核时遇到了错误。

https://github.com/vllm-project/vllm/issues/1792
这是一个关于CUDA多GPU运行时OOM（out of memory）bug报告，主要涉及到在使用多GPU时出现内存错误，但在单GPU上正常运行的模型。

https://github.com/vllm-project/vllm/issues/1791
这个issue属于技术改进需求，主要涉及对象为`BlockTable`和`BlockAllocator`，由于`BlockTable`的定义位置不当而导致`BlockAllocator`无法使用它。

https://github.com/vllm-project/vllm/issues/1790
这是一个用户提出需求的issue，主要涉及Internlm动态网络，并询问如何支持使上下文达到16k的问题。可能由于当前Internlm不支持这一功能，用户希望了解如何实现。

https://github.com/vllm-project/vllm/issues/1789
这是一个bug报告类型的issue，涉及对象为openai_completion_client.py 文件。该问题由于openai模块缺少了Completion属性所导致，使得用户无法进行推理流处理。

https://github.com/vllm-project/vllm/issues/1788
这个issue类型是bug报告，该问题涉及的主要对象是"log"，原因可能是日志记录功能出现异常或者无法正常记录日志。

https://github.com/vllm-project/vllm/issues/1787
这是一个需求更改类型的issue，主要涉及到命名规范问题。由于实际功能与命名不符，用户提出应将部分类名更改，以更准确地反映其功能。

https://github.com/vllm-project/vllm/issues/1786
这是一个用户询问问题的类型的issue，主要涉及到C++ API中的`c10::optional`类型在Rust bindings中的转换问题。这个问题是由于`c10::optional`是一个类而不是枚举类型，导致用户在生成Rust bindings时遇到困难，请求如何将其转换为Rust中的对应类型。

https://github.com/vllm-project/vllm/issues/1785
这是一个升级问题，主要涉及的对象是 examples-docs。由于 OpenAI API 版本升级，需要将示例升级为 V1 版本，以确保与 `vllm` 和文档完全兼容。

https://github.com/vllm-project/vllm/issues/1784
这是一个bug报告类型的issue，主要涉及到VLLM项目中的模型加载和初始化过程。由于未完全优化的awq量化和原始tokenizer初始化时间过长，导致了模型加载时出现了错误和警告信息。

https://github.com/vllm-project/vllm/issues/1783
这是一个 bug 报告，问题涉及到 GitHub 上的 vllm 库中的重复生成以及出现 ValueError: max() arg is an empty sequence 错误。用户使用 vllm 加速 ChatGLM26B32K 时遇到了重复生成相同单词并最终出现上述错误的问题。

https://github.com/vllm-project/vllm/issues/1782
这是一个bug报告，涉及到OOM during Docker build，用户在构建Docker镜像时遇到系统OOM导致的问题。

https://github.com/vllm-project/vllm/issues/1781
这个issue是一个Bug报告，主要涉及的对象是在NVIDIA官方镜像中安装vllm。导致此问题的原因是安装vllm时会导致预构建的PyTorch被卸载和重新安装，导致调用vllm时出现错误。

https://github.com/vllm-project/vllm/issues/1780
这是一个bug报告，涉及到vllm项目中的一个方法签名的类型提示问题，导致阅读困难。

https://github.com/vllm-project/vllm/issues/1779
这是一个bug报告，该问题涉及到vllm下的代码。此问题的原因是在openai api中忽略了disablelogrequests标志导致无法禁用日志记录。

https://github.com/vllm-project/vllm/issues/1778
这是一个用户提出需求的类型。主要对象是模型加载到多个GPU。由于原因是需要将模型的一部分加载到一个GPU，另一部分加载到另一个GPU，导致用户寻求帮助。

https://github.com/vllm-project/vllm/issues/1777
这是一个bug报告，主要对象是pip install -e .失败，可能是因为编译过程中出现了错误导致的。

https://github.com/vllm-project/vllm/issues/1776
这是一个bug报告，涉及的主要对象是模型部署。这个问题由于GPU内存容量限制导致需求的blocks数量超过实际可用blocks数量而引起，造成请求始终在等待队列中无法调度。

https://github.com/vllm-project/vllm/issues/1775
这是一个bug报告类型的issue，主要涉及chatglm模式无法被AutoAWQ量化的问题，可能是由于chatglm模式目前尚未得到支持所致。

https://github.com/vllm-project/vllm/issues/1774
这是一个bug报告，主要涉及到生成问题的模型Qwen14b-int4，问题出现在没有添加maxtokens参数时无法生成完整的问题。

https://github.com/vllm-project/vllm/issues/1773
这是一个用户提出需求的类型，主要涉及的对象是batch中长度不一的序列。由于序列长度的不均匀性可能影响GPU计算效率，用户想知道序列长度的均匀性对GPU计算的影响，希望获得相关建议。

https://github.com/vllm-project/vllm/issues/1772
这是一个Bug报告类型的Issue，主要涉及VLLM中"Build from source"的错误。这个问题可能由于编译源代码时出现的错误导致无法成功构建。

https://github.com/vllm-project/vllm/issues/1771
这是一个bug报告，主要涉及的对象是vllm在RTX4090上的部署。这个问题由于Fatal python error:Bus error导致了程序运行时的错误。

https://github.com/vllm-project/vllm/issues/1770
这是一个Bug报告类型的issue，主要涉及的对象是vLLM中的GPU消耗异常高，可能由于配置问题导致GPU消耗远高于预期。

https://github.com/vllm-project/vllm/issues/1769
该issue是一个关于功能需求的问题，主要涉及VLLM中的量化方法问题。用户提出了关于未知量化方法"gptq"的问题，并询问该如何使用"gptq"模型。由于最新代码中未对"gptq"进行支持，导致用户遇到无法使用该模型的问题。

https://github.com/vllm-project/vllm/issues/1768
这是一个bug报告，用户在设置'servedmodelname'选项时引发了错误，导致API调用时出现数值错误。

https://github.com/vllm-project/vllm/issues/1767
这是一个用户提出需求的issue，主要涉及的对象是支持长上下文长度模型。由于GPU VRAM大小限制，用户无法部署需要更长上下文长度的模型。

https://github.com/vllm-project/vllm/issues/1765
这个issue是关于bug报告，主要涉及vllm服务在多请求过程中出现了内存不足导致的任务被杀死的问题。

https://github.com/vllm-project/vllm/issues/1764
这是一个关于修改模型文档字符串的问题，类型为代码质量改进，主要涉及模型文档字符串的修正。

https://github.com/vllm-project/vllm/issues/1763
这是一个bug报告，该问题单涉及的主要对象是Ray process。由于无法启动Ray process被杀死，导致出现了致命异常。

https://github.com/vllm-project/vllm/issues/1762
这是一个bug报告，主要涉及的对象是API在处理并发请求时出现卡顿现象。由于可能与其他相关问题存在关联，并且终端日志仅记录了部分请求信息，导致出现API卡顿并延迟1分钟才能继续处理请求。

https://github.com/vllm-project/vllm/issues/1761
此issue属于bug报告，主要涉及模型批量推理时输出与单次推理不一致，可能由于批处理推理逻辑的实现方式引起。

https://github.com/vllm-project/vllm/issues/1760
这是一个bug报告，主要涉及VLLM模型以及Hugging Face Tokenizer，问题是生成的回复中的标记数与指定的max_tokens值不匹配，可能是由于模型在生成回复时额外添加了标记导致。

https://github.com/vllm-project/vllm/issues/1759
这是一个bug报告，涉及的主要对象是vllm模型。这个问题可能是由于在单GPU上执行推理时触发了"AssertionError: tensor model parallel group is already initialized"错误。

https://github.com/vllm-project/vllm/issues/1758
这是一个bug报告类型的issue，主要涉及的对象是vllm项目中的cuda_utils模块。这个问题的根源可能是编译过程中出现了未定义符号的错误，导致了无法找到特定的符号"_ZNSt15__exception_ptr13exception_ptr10_M_releaseEv"。

https://github.com/vllm-project/vllm/issues/1757
这是一个用户询问类型的issue，主要涉及到如何检查输入的模型架构是否被VLLM支持，并询问最佳做法。原因可能是用户想知道如何在模型选择方面做出最佳决策。

https://github.com/vllm-project/vllm/issues/1756
这个issue是一个功能增强请求，主要涉及到vLLM对于chat API的支持，由于作者替换了分支导致问题重现。

https://github.com/vllm-project/vllm/issues/1755
这个issue是关于bug报告，涉及的主要对象是vllm下载模型的行为。原因可能是vllm无法正确识别已下载的模型文件，导致在指定--download-dir参数后仍然尝试下载模型文件。

https://github.com/vllm-project/vllm/issues/1754
这是一个用户提出需求的issue，主要涉及支持Lookahead Decoding的建议。用户想知道在vLLM中是否可以实现Lookahead Decoding，并希望得到相关支持。

https://github.com/vllm-project/vllm/issues/1753
这是一个用户提出需求的issue，主要涉及添加worker registry service来托管多个vllm模型，并通过单个api网关进行访问。由于现有的vllm集成不能完全满足其需求，用户希望扩展功能，例如支持beam search等。

https://github.com/vllm-project/vllm/issues/1752
这是一个bug报告类型的issue，主要涉及的对象是vllm模块下的tensor-parallel-size参数和device_map功能。这个问题可能由于GPU资源分配不当导致OOM或推理速度问题，用户寻求解决方案以提高推理速度。

https://github.com/vllm-project/vllm/issues/1751
这是一个关于用户提出需求的issue，主要对象是关于加速视觉语言模型像LLaVa这样的多模态LLM，用户询问如何在给定示例中使用。

https://github.com/vllm-project/vllm/issues/1750
这个issue属于用户提出需求类型，主要涉及对象为支持 PP for LLaMA 以及支持 num_layers % pp_size != 0。原因可能是当前模型不支持PP以及优化工程和调度程序以实现完全流水线。

https://github.com/vllm-project/vllm/issues/1749
这个issue属于功能需求类型，主要涉及的对象是vLLM项目的主分支合并EmbeddedLLM/vllm-rocm分支，用户提出了需要动态选择CUDA或ROCm的功能，但尚需通过所有单元测试。问题的原因可能是为了增强PyTorch在CUDA或ROCm环境下的灵活性与适应性。

https://github.com/vllm-project/vllm/issues/1748
这个issue类型是需求提出，主要对象是vllm模型的greedy sampling功能。用户提出需要将greedy sampling中的top_p设置为0并将top_k设置为-1。

https://github.com/vllm-project/vllm/issues/1747
这是一个bug报告，主要涉及的对象是服务端和客户端，由于用户数增加到60时部分请求出现错误响应，但在服务器中没有看到任何错误日志，用户想要找到如何调试此问题。

https://github.com/vllm-project/vllm/issues/1746
这是一个bug报告，主要涉及Qwen14B模型在vllm中推理长文本提示时出现错误响应的问题，由于输入长提示时生成结果错误且重复。

https://github.com/vllm-project/vllm/issues/1745
这个issue是一个bug报告，主要涉及SamplingParams类的缺陷，原因是在``__repr__``方法中缺少了stop_token_ids。

https://github.com/vllm-project/vllm/issues/1744
这是一个用户提出的需求类型的 issue，主要涉及的对象是 vllm 模型。由于 vllm 模型目前不支持设置随机种子，导致用户无法实现每次请求的推理都可复现。

https://github.com/vllm-project/vllm/issues/1743
这是一个bug报告。该问题涉及到使用api_openai在chatglm3服务时产生错误的“assistant” tokens。这个问题可能是由于API调用逻辑或模型问题导致的。

https://github.com/vllm-project/vllm/issues/1742
这个issue是关于需求的，涉及到是否可以将lookahead decoding集成到vllm中。

https://github.com/vllm-project/vllm/issues/1741
这是一个需求提出类的issue，主要涉及到文档内容的补充需求。原因是引擎参数文档缺失，导致用户难以理解参数含义，需要浏览源代码来获取信息。

https://github.com/vllm-project/vllm/issues/1739
这是一个Bug报告，涉及的主要对象是distribute serve在Santacode 1.1B模型上的性能问题，原因可能是在L4 GPU上进行分布式服务时性能表现不佳。

https://github.com/vllm-project/vllm/issues/1738
这是一个Bug报告，涉及到vLLM的Python模块加载错误问题，可能是由于缺少"libcudart.so.12"文件导致的。

https://github.com/vllm-project/vllm/issues/1737
这是一个bug报告，主要涉及对象是ScaledActivation的输入参数input_is_parallel。由于错误的处理导致了提出者希望修复AWQ中TP支持问题。

https://github.com/vllm-project/vllm/issues/1736
这是一个bug报告，涉及的主要对象是vllm的推理引擎和guidance项目。由于集成后推理速度下降，可能与vllm的机制有关。

https://github.com/vllm-project/vllm/issues/1735
这是一个bug报告，主要涉及到vllm中的chatglm3-6b模型。由于设置tensor_parallel_size为4时会出现错误响应，而将其设置为1或2时则正常。

https://github.com/vllm-project/vllm/issues/1734
这个issue类型是bug报告，主要涉及的对象是vlllm系统。由于AWQ协议处理速度慢，导致pending request增多时系统停止生成输出。

https://github.com/vllm-project/vllm/issues/1733
这是一个Bug报告，涉及的主要对象是AsyncLLMEngine对象，导致了TypeError异常。

https://github.com/vllm-project/vllm/issues/1732
这是一个 BugFix 类型的 issue， 主要对象是loading safetensors的功能。由于 CC(Migrate formatter from yapf to ruff) 引入的bug导致加载safetensors时出现错误，需要修复。

https://github.com/vllm-project/vllm/issues/1731
这个issue是一个BugFix类型的问题，主要涉及了对AWQ功能的支持。由于TP在某些模型上无法正常工作，导致了需要修复的bug。

https://github.com/vllm-project/vllm/issues/1730
这个issue类型是文档更新，主要涉及的对象是README.md文件。由于文档链接需更新，导致出现修复需求。

https://github.com/vllm-project/vllm/issues/1729
这是一个关于需求的问题，主要涉及到从Google Cloud Storage存储桶而不是Hugging Face存储库下载模型的方式。这个问题可能是由用户希望通过指定GCP存储桶位置来下载模型而引起的。

https://github.com/vllm-project/vllm/issues/1728
这个issue类型为用户请教问题，主要涉及的对象是参数logits_processors。由于用户想知道如何使用logits_processors来修改当前的logit。

https://github.com/vllm-project/vllm/issues/1727
这是一个bug报告，主要涉及调度器中的问题，由于GPU block数量超过限制导致序列组待在等待队列中永远不会被调度，并且还存在一个与'seq_lens'相关的错误。

https://github.com/vllm-project/vllm/issues/1726
这是一个bug报告，问题涉及LLM模型加载时出现的NCCL错误。由于NCCL错误导致了无法加载指定模型，用户寻求解决方案或帮助。

https://github.com/vllm-project/vllm/issues/1725
该issue类型为bug报告，主要涉及的对象为使用chatglm2-6b-32k模型进行推理过程时出现错误。由于32k长文本，在8个GPU上推理过程失败，但Hugging Face版本的模型在2个GPU上表现良好，导致了这一问题。

https://github.com/vllm-project/vllm/issues/1724
这个issue属于bug报告类型，涉及的主要对象是VLLM模型中的停用词功能。由于停用词未在生成文本的末尾生效，导致生成的文本中可能不会正确停止。

https://github.com/vllm-project/vllm/issues/1723
这是一个bug报告，主要涉及`vllm`加载`ChatGLM2-6B-32K`模型时出现的NCCL相关问题。导致此问题的原因是NCCL相关错误在初始化LLM引擎时发生。

https://github.com/vllm-project/vllm/issues/1722
这个issue是关于bug报告的，主要涉及的对象是文档中代码块的格式。由于格式问题，导致在页面展示时出现了错误的渲染。

https://github.com/vllm-project/vllm/issues/1721
这是一个bug报告，涉及vllm 0.2.2+cu121和vllm 0.2.2+cu118生成的答案不一致的问题。原因可能是软件版本更新导致的不一致。

https://github.com/vllm-project/vllm/issues/1719
这是一个bug报告，涉及到模块导入的循环依赖问题导致的 ImportError。

https://github.com/vllm-project/vllm/issues/1718
这个issue是一个bug报告，主要涉及的对象是与软件/库导入相关的错误。原因可能是缺少必要的库文件或者版本不匹配导致出现ImportError。

https://github.com/vllm-project/vllm/issues/1717
这是一个用户需求报告，主要涉及的对象是解决vllm+cu118无法安装的问题，导致的主要症状是无法找到 libcudart.so.12 共享对象文件。

https://github.com/vllm-project/vllm/issues/1716
这是一个bug报告，主要涉及的对象是vllm库，由于最新更新（v.0.2.2）导致了ImportError的错误。

https://github.com/vllm-project/vllm/issues/1715
这个issue类型为bug报告，主要涉及的对象是quantized model performance提示信息。这个bug的原因是在非量化模型上打印了量化模型性能的警告消息。

https://github.com/vllm-project/vllm/issues/1714
这个issue类型是功能改进（Feature Enhancement），主要涉及的对象是为所有模型添加AWQ支持，通过添加`ScaledActivation`。由于在quantized GPTBigCodeForCausalLM中出现了KeyError导致CC(load_weights)错误，需要修复这个问题。

https://github.com/vllm-project/vllm/issues/1713
这是一个bug报告，主要涉及ChatGLM的配置对象（ChatGLMConfig），由于配置对象缺少属性'num_hidden_layers'导致了AttributeError。

https://github.com/vllm-project/vllm/issues/1712
这是一个bug报告，主要涉及对象是`StreamingResponse`。这个问题导致了生成文本后才开始实时流式传输文本，可能会导致响应延迟。

https://github.com/vllm-project/vllm/issues/1711
这是一个bug报告类型的issue，主要涉及到vllm下的BloomForCausalLM模型加载问题。由于无法支持BloomForCausalLM模型的量化，导致加载失败。

https://github.com/vllm-project/vllm/issues/1710
该issue类型是bug报告，主要涉及的对象是vllm中的docker container。由于使用`--tensor-parallel-size`参数在docker container中运行13B模型时导致崩溃，而在发布的vllm lib中运行正常。

https://github.com/vllm-project/vllm/issues/1709
这是一个用户提出需求的issue，涉及发布官方docker镜像，由于缺乏相关文档，用户希望推送docker镜像并给出操作指南。

https://github.com/vllm-project/vllm/issues/1707
这是一个bug报告，涉及的主要对象是vLLM API，由于API无法处理多个提示的批量请求，导致用户无法获得速度提升的效益。

https://github.com/vllm-project/vllm/issues/1706
这是一个安全问题报告，主要涉及Ray项目的安全问题，原因是有人通过HN讨论发现了相关漏洞并提交了链接。

https://github.com/vllm-project/vllm/issues/1705
这是一个bug报告，主要涉及的对象是无法在多GPU上使用gpt2-xl。这个问题可能由于多GPU环境下的性能或配置问题导致模型无法正常工作。

https://github.com/vllm-project/vllm/issues/1704
这个issue类型属于bug报告，涉及的主要对象是代码中的注解。原因是缺少了一个关键的单词"not"，可能导致误解或错误使用内部方法。

https://github.com/vllm-project/vllm/issues/1703
这是一个关于bug报告的issue，主要涉及vllm下的opt awq模型，问题是由于代码重构引起的两个bug。

https://github.com/vllm-project/vllm/issues/1702
该issue类型为需求提出，主要对象涉及vllm的分布式推断功能。这个问题是关于是否支持使用多台机器进行推断，用户想要了解是否vllm支持将一个实例在两台拥有单个GPU的机器上运行。

https://github.com/vllm-project/vllm/issues/1701
这个issue是关于bug报告，涉及主要对象是LLM实例的初始化，由于重复初始化模型并行导致了错误提示。

https://github.com/vllm-project/vllm/issues/1700
这个issue是一个bug报告，涉及的主要对象是VLLM模型运行过程中出现的CUDA错误。这个问题可能是由于内存访问异常导致的。

https://github.com/vllm-project/vllm/issues/1699
这是一个用户提出需求的issue，主要涉及的对象是Intel GPU，原因是希望开始在Intel GPU上上游SYCL内核支持。

https://github.com/vllm-project/vllm/issues/1698
这是一个Bug报告，主要涉及对象是在单个A10 GPU服务器上部署baichuan2-13b模型。由于VLLM目前不支持量化方法来部署此类模型，导致无法使用autoawq方法部署baichuan模型，触发了TypeError错误。

https://github.com/vllm-project/vllm/issues/1697
这是一个bug报告，主要涉及vllm安装后版本不一致导致代码不匹配的问题。

https://github.com/vllm-project/vllm/issues/1696
这是一个bug报告，涉及主要对象为safetensors模型。由于最近重构的结果，所有参数都在hf_model_weights_iterator中被转换为tensor，因此无法直接使用get_tensor方法来检索torch.Tensor，因此用户可能遇到了无法从PySafeSlice中检索标量的问题。

https://github.com/vllm-project/vllm/issues/1695
这是一个功能需求提议，主要涉及的对象是VLLM项目中的HF模型配置。由于需要支持HF官方的量化方法（如AWQ和GPTQ），该提议表示需要读取HF模型配置中的量化配置。

https://github.com/vllm-project/vllm/issues/1694
这是一个bug报告，主要涉及vllm在回答Qwen-7B-Chat时出现答案重复多次的问题，可能由于vllm模型在特定条件下产生重复输出的现象。

https://github.com/vllm-project/vllm/issues/1693
这是一个用户请求删除issue的类型。该问题单涉及的主要对象是OOM llama 2。由于用户误操作或其他原因，导致用户在此issue中请求删除。

https://github.com/vllm-project/vllm/issues/1692
这是一个文档更新类的issue，主要涉及的对象是添加模型文档。原因是对代码重构后的文档需要进行更新。

https://github.com/vllm-project/vllm/issues/1691
这个issue类型为bug报告，涉及的主要对象是vllm的comm test功能。由于没有具体内容，导致无法进行通信测试，需要修复该问题。

https://github.com/vllm-project/vllm/issues/1690
这个issue是关于bug报告的，涉及的主要对象是支持的模型列表。由于模型列表未更新，导致用户无法正确识别支持的模型，因此提出了更新支持模型列表的问题。

https://github.com/vllm-project/vllm/issues/1689
这个issue类型是版本升级请求，主要涉及对象是项目的版本号。

https://github.com/vllm-project/vllm/issues/1688
这是一个关于迁移代码格式化工具的issue，涉及的主要对象是Python项目中的代码格式规范，由于选择了使用更快速的工具 `ruff` 替换 `yapf`，导致这个issue产生。

https://github.com/vllm-project/vllm/issues/1687
这是一个bug报告，主要涉及对象是vllm中的safetensors模型，由于空张量导致加载错误。

https://github.com/vllm-project/vllm/issues/1686
这个issue类型是用户提出需求，该问题单涉及的主要对象是支持Nemotron，由于缺乏Nemotron的支持，用户希望能够在VLLM中添加对Nemotron的支持。

https://github.com/vllm-project/vllm/issues/1685
这是一个用户提出需求的issue，主要涉及安装程序在Windows系统下无法在没有WSL的情况下运行的问题，原因是公司策略对GPU机器不能启用WSL，导致了CUDA runtime未找到的错误。

https://github.com/vllm-project/vllm/issues/1684
这是一个bug报告，涉及的主要对象是vllm源代码。由于使用2 GPUs会导致 AssertionError 错误，而在使用vllm2.1.0post1时却没有问题，因此用户请求解决方案。 

https://github.com/vllm-project/vllm/issues/1683
这是一个bug报告，主要涉及的对象是vllm在推理时对长文本的处理。由于对长prompt进行推理时出现了与HF模型不一致且不连贯的回答，可能是由于vllm目前不支持长文本推理或不支持修改后的qwen14B模型。

https://github.com/vllm-project/vllm/issues/1682
这是一个bug报告，主要涉及的对象是vLLM项目中的quantized GPTBigCodeForCausalLM模型加载权重时出现KeyError异常。这个问题可能是由于AWQ支持不完整导致的。

https://github.com/vllm-project/vllm/issues/1681
这是一个bug报告，涉及主要对象是docker和vllm，由于docker启动server时出现了错误。

https://github.com/vllm-project/vllm/issues/1680
这是一个用户提出需求的issue，主要涉及的对象是vllm支持torch==2.1.0版本cu11.8，导致问题的原因可能是在指定的URL中找不到对应的vllm包。

https://github.com/vllm-project/vllm/issues/1679
这个issue是关于开发一个新功能的提案，涉及 VLLM 模型中的 cached multi-query attention。

https://github.com/vllm-project/vllm/issues/1678
这是一个关于功能需求的问题，主要涉及vllm是否支持 quantized INT4 和 INT8 模型的问题。可能由于用户需要在项目中使用此类模型，或者对未来是否会有相应支持有所关注。

https://github.com/vllm-project/vllm/issues/1677
这是一个性能问题的bug报告，主要涉及API服务器和AsyncLLM引擎的性能缺陷。原因是在asyncio循环中存在慢速实现以支持流式处理，以及阻塞调用引起的问题。

https://github.com/vllm-project/vllm/issues/1676
这是一个用户提出需求的问题单，主要涉及对象是vllm模型。原因是用户希望在同一进程中运行多个vllm模型，以获取不同模型的对话输出。

https://github.com/vllm-project/vllm/issues/1675
这是一个用户提出需求的issue，主要涉及的对象是模型的token生成时间，由于用户想要打印每个生成token的时间戳，未完成的需求可能导致打印时间信息的bug。

https://github.com/vllm-project/vllm/issues/1674
该issue为用户提出需求类型，主要涉及Vllm模型如何与Streamlit UI连接的问题。由于用户希望通过Streamlit UI展示Vllm模型的输出，因此寻求代码示例来实现这一功能。

https://github.com/vllm-project/vllm/issues/1673
这是一个Bug报告，主要涉及vllm中的llm.generate功能，由于prompt token ids过长导致生成过程卡住。

https://github.com/vllm-project/vllm/issues/1672
这是一个bug报告，涉及的主要对象是vLLM在处理大量查询时出现CUBLAS_STATUS_EXECUTION_FAILED错误。这个问题是由于模型输出长响应时导致CUDA错误而引起的。

https://github.com/vllm-project/vllm/issues/1671
这个issue是一个性能问题报告，主要涉及到vllm模型在加载baichuan7B时消耗更多内存的情况。可能由于某些原因导致vllm使用更多内存，用户希望找出原因并进行解决。

https://github.com/vllm-project/vllm/issues/1670
这个issue类型是bug报告，涉及的主要对象是vLLM与Huggingface ChatGLM2模型的输出不一致问题。由于不同实现在处理稀有问句和长序列时存在差异，导致输出结果略有不同。

https://github.com/vllm-project/vllm/issues/1669
该issue类型为功能需求提案，在实验性功能中新增了前缀缓存支持，主要涉及VLLM模型的前缀缓存功能。

https://github.com/vllm-project/vllm/issues/1668
这是一个bug报告，涉及的主要对象是移除`MptConfig`。原因是在Transformer库中有一个不同版本的`MptConfig`，所以需要将其更改为`MPTConfig`。

https://github.com/vllm-project/vllm/issues/1667
这个issue属于优化性质，主要涉及LLaMA项目的性能提升，减少CPU负担。

https://github.com/vllm-project/vllm/issues/1666
这是一个bug报告类型的issue，主要涉及到vllm项目中的engine模块，问题是由于重复添加被忽略的序列组，导致批量生成长提示时输出数量不正确。

https://github.com/vllm-project/vllm/issues/1665
这是一个功能优化的issue，主要涉及迁移代码检查工具从`pylint`到`ruff`，开发者提到`ruff`实现了更多实用的规则，并且具有更高的检查速度和自动修复功能。

https://github.com/vllm-project/vllm/issues/1664
这是一个bug报告，主要涉及的对象是新实现的Microsoft Phi 1.5模型，问题可能出现在RoPE（Rotary Positional Encoding）部分的实现中。

https://github.com/vllm-project/vllm/issues/1663
这是一个用户提出需求的issue，主要涉及的对象是openai requests，用户希望OpenAI可以在流式请求中包含输入、输出和总标记数的信息，因为当前的OpenAI后端在流式请求中并不包含这些信息，而这是一个用户非常要求的功能。

https://github.com/vllm-project/vllm/issues/1662
这是一个用户提出的需求。该问题单涉及的主要对象是VLLM项目的功能性。由于缺乏对Prometheus指标的支持，用户需要增加此功能以便更好地进行监控，特别是在运行多个实例时。

https://github.com/vllm-project/vllm/issues/1661
这是一个用户提出需求的issue，主要涉及的对象是vllm库的支持版本。由于vllm目前不支持Torch 2.1版本，用户希望能够添加对该版本的支持。

https://github.com/vllm-project/vllm/issues/1660
这是一个bug报告，涉及的主要对象是VLLM API server。由于在加载模型时导致PC变慢并且加载时间过长，可能的原因是与3090 GPU和WSL环境的兼容性问题。

https://github.com/vllm-project/vllm/issues/1659
这是一个需求类型的issue，主要涉及Cached multi-query attention实现的修改。原因是为了使用当前的单个查询注意力内核轻微修改以实现缓存的多查询注意力。

https://github.com/vllm-project/vllm/issues/1658
这是一个关于bug报告的issue，主要涉及的对象是benchmark_latency.py脚本。由于未正确设置max_num_seqs参数，当batchsize=1且n=2、4或8时会导致程序卡住无法运行。

https://github.com/vllm-project/vllm/issues/1657
这是一个Bug报告，涉及的主要对象是添加BlueLM模型。该问题是由于之前的1651号问题引起的。

https://github.com/vllm-project/vllm/issues/1655
这是一个bug报告，该问题涉及到vllm库中的批量生成功能。由于prompt过长，导致生成的输出数量不符预期。

https://github.com/vllm-project/vllm/issues/1654
这是一个关于优化性能的问题，涉及到如何使用 vllm 快速地提取嵌入，可能由于每次调用 generate 函数导致速度过慢。

https://github.com/vllm-project/vllm/issues/1652
这是一个关于模型量化框架设计的问题，主要涉及到在支持模型和量化算法数量增加时的设计不便之处。

https://github.com/vllm-project/vllm/issues/1651
这是一个bug报告，涉及的主要对象是vllm中的BlueLMForCausalLM模型。由于当前版本的vllm不支持BlueLM-7B-Chat-32K模型的架构，导致了报错信息。

https://github.com/vllm-project/vllm/issues/1650
该issue属于用户提出问题类型，主要涉及对象是LLM项目中的设置CUDA设备问题。由于未指定CUDA设备，导致代码默认在设备0上运行，用户需要知道如何更改设备。

https://github.com/vllm-project/vllm/issues/1649
这是一个用户提出的需求问题，主要涉及的对象是将 DeepSpeedMII 后端添加到 benchmark 脚本中。

https://github.com/vllm-project/vllm/issues/1648
这是一个功能需求提出的issue，主要对象是增加使用情况报告功能。

https://github.com/vllm-project/vllm/issues/1647
这是一个bug报告，该问题涉及到vLLM Triton Server Example的配置错误导致AssertionError，用户寻求关于vLLM Triton后端的配置问题的帮助。

https://github.com/vllm-project/vllm/issues/1646
这是一个bug报告，主要涉及vllm中的beam search生成过程，由于deepcopy()的性能问题，导致CPU利用率过高的现象。

https://github.com/vllm-project/vllm/issues/1645
这是一个撤销Docker构建的问题，类型为操作错误; 主要涉及对象为项目的构建过程。

https://github.com/vllm-project/vllm/issues/1644
这个issue属于Bug报告类型，主要涉及的对象是OpenAI的官方Client和VLLM OpenAI兼容的API。由于SSL证书验证错误导致了连接失败，在OpenAI版本大于1.0.0时出现该问题。

https://github.com/vllm-project/vllm/issues/1643
这是一个bug报告类型的issue，主要涉及到使用vllm生成对话总结时出现只能生成一个句号的问题。可能的原因是模型在处理特定输入数据时存在某种限制或错误，导致生成结果不符合预期。

https://github.com/vllm-project/vllm/issues/1642
这是一个用户提出需求的issue，主要涉及的对象是VLLM模型的 Sampler功能。由于用户尝试使用 min_p 参数进行推理时获得更好的结果，因此提出了支持 Min P Sampler 的需求。

https://github.com/vllm-project/vllm/issues/1641
这是一个功能增强类型的issue，涉及的主要对象是配置文件解析器。这个issue产生的原因是需要支持解析torch.dtype，以便更好地处理相关数据类型。

https://github.com/vllm-project/vllm/issues/1640
这是一个Bug报告，主要涉及对象是get_num_kv_heads函数。导致这个Bug的原因可能是在该函数中存在错误逻辑导致程序运行异常。

https://github.com/vllm-project/vllm/issues/1639
这个issue类型是bug报告，主要涉及的对象是处理大型激活张量时可能产生的溢出问题，问题的原因可能是激活张量过大导致的。

https://github.com/vllm-project/vllm/issues/1638
这个issue是关于用户提出需求的，涉及主要对象是vLLM和jais模型。造成这个问题是用户想了解vLLM是否支持jais模型。

https://github.com/vllm-project/vllm/issues/1637
这个issue是关于bug报告，主要对象是seed的设置，用户提出了如何在生成之前设置seed的问题。

https://github.com/vllm-project/vllm/issues/1636
这是一个关于功能使用问题的issue。主要涉及对象是vLLMOpenAI，用户提出无法在Langchain网站上实现批处理和异步批处理操作。

https://github.com/vllm-project/vllm/issues/1635
这个issue是用户提出的一个功能需求，涉及主要对象为prompt caching。由于长fewshot examples的重复编码导致效率低下，用户建议实现缓存重叠提示以便重复使用。

https://github.com/vllm-project/vllm/issues/1633
这是一个需求类型的issue，主要涉及对象是RoPE selection logic的移动操作。原因可能是为了优化代码结构或提高代码的可维护性。

https://github.com/vllm-project/vllm/issues/1632
这是一个用户提出需求的issue，主要对象是vLLM团队。用户希望默认支持添加01ai/Yi34B200K模型，因为当前需要手动添加，希望团队能够在默认配置中包含该模型。

https://github.com/vllm-project/vllm/issues/1631
这是一个bug报告，主要涉及的对象是EleutherAI/gptj6b模型，由于参数形状不匹配导致了 gptj AssertionError。

https://github.com/vllm-project/vllm/issues/1630
这个issue类型是用户提出问题，主要涉及的对象是数据存储方式。用户询问为什么数据要以16字节的块进行存储，推测与cuda内核有关。

https://github.com/vllm-project/vllm/issues/1629
这是一个bug报告，主要涉及的对象是AsyncLLMEngine模块中的_run_workers_async函数。这个问题由于在该函数中执行的cpu密集型操作阻塞了事件循环，导致请求无法得到最佳处理，出现了响应显示不平滑的现象。

https://github.com/vllm-project/vllm/issues/1628
这是一个bug报告，主要涉及`_AsyncLLMEngine._run_workers_async`函数，由于使用`getattr(worker, method)`导致了CPU密集型代码和事件循环阻塞，影响了快速响应请求，导致了tokens显示效果不理想。

https://github.com/vllm-project/vllm/issues/1627
这是一个关于代码缺失的问题，主要涉及到系统提示相关的部分。用户提出了由于代码缺失导致无法处理系统提示的问题。

https://github.com/vllm-project/vllm/issues/1626
这个issue类型是问题询问，主要对象是关于如何使用Python 3.9构建该项目。这可能是因为用户想要更新到Python 3.9版本，但不清楚如何在该项目中进行构建操作。

https://github.com/vllm-project/vllm/issues/1625
这是一个Bug报告，涉及的主要对象是VLLM（Very Large Language Model）。导致此Bug的原因可能是模型权重加载过程中发生了KeyError。

https://github.com/vllm-project/vllm/issues/1624
该issue类型为性能优化，主要针对构建过程进行优化，减少构建时间。

https://github.com/vllm-project/vllm/issues/1623
这是一个bug报告，主要涉及Top p或temperature为0.0001时出现RuntimeError，可能由于概率张量包含inf、nan或小于0的元素导致。

https://github.com/vllm-project/vllm/issues/1622
这个issue类型是功能需求报告，主要涉及的对象是vllm项目中的quantization、weight loading 和 tensor parallelism 代码，用户提出了希望重构quantized linear logic，并扩展量化支持到所有模型的功能。

https://github.com/vllm-project/vllm/issues/1621
这是一个关于性能优化的问题，主要涉及对象为sampler CPU overhead，用户寻求减少CPU开销的帮助。

https://github.com/vllm-project/vllm/issues/1620
这是一个bug报告，涉及到vllm在与openai-python v1版本集成时，无法使用参数"repetition_penalty"导致的TypeError错误。

https://github.com/vllm-project/vllm/issues/1619
这是一个Bug报告，主要涉及到VLLM中的"logits_processors"参数，在操作过程中似乎丢失了。可能是由于代码实现问题导致的。

https://github.com/vllm-project/vllm/issues/1618
这是一个bug报告，主要涉及Paged attention v2在T4 GPU上比v1慢的问题。由于v2在实际模型情景下表现比v1慢，作者希望测试T4 GPU来验证这一情况。

https://github.com/vllm-project/vllm/issues/1617
这是一个功能需求提出的issue，主要涉及的对象是config parser和ChatGLM2模块，用户希望在`_get_and_verify_max_len`方法中添加seq_length参数。

https://github.com/vllm-project/vllm/issues/1616
这是一个关于CUDA内存不足的bug报告，主要涉及到在使用meta-llama/Llama-2-13b-hf模型时出现的内存分配问题，可能是由于模型过大而导致。

https://github.com/vllm-project/vllm/issues/1615
这是一个用户提出的需求问题单，主要涉及Llama模型定义与torch.compile的兼容性，导致用户寻求2倍速度提升。

https://github.com/vllm-project/vllm/issues/1614
这是一个bug报告，涉及的主要对象是vllm的 llma-2-13b-chat-hf 模型以及A100 GPU。出现的问题是在A100 GPU上使用vllm 0.2.1.post1版本时出现CUDA内存不足的错误。

https://github.com/vllm-project/vllm/issues/1613
这是一个功能需求，主要对象是API服务器，由于CC（输入被附加到输出）而导致需要更精细调控输出的问题。

https://github.com/vllm-project/vllm/issues/1612
这是一个bug报告，主要对象是vllm项目下的generate调用。问题的根本原因可能是输入被错误地追加到了输出中。

https://github.com/vllm-project/vllm/issues/1611
这是一个性能问题报告，涉及 OPT13b 模型的模型并行化在两个 A100 GPU 上性能比单个 A100 GPU 效率更低，即使GPU缓存可用块数翻倍。

https://github.com/vllm-project/vllm/issues/1610
这个issue是关于用户提出需求。主要对象是将S-Lora整合到vLLM中。根据用户的描述，用户希望能够在不需要将模型合并的情况下进行推理，这对于为Mixture of Expert模型提供服务或需要基于相同基础模型进行多个不同的finetuned lora适配器的服务非常有用。

https://github.com/vllm-project/vllm/issues/1609
这是一个bug报告，涉及对象为Dockerfile中的CUDA版本。由于CUDA版本不匹配导致Docker构建失败。

https://github.com/vllm-project/vllm/issues/1608
这是一个bug报告，主要涉及的对象是VLLM模型。这个问题是因为传入的参数中缺少了logits_processors导致的。

https://github.com/vllm-project/vllm/issues/1607
这是一个bug报告，主要涉及GPU KV cache的使用情况达到100%导致模型无法生成输出的问题。

https://github.com/vllm-project/vllm/issues/1606
该issue类型为用户提出需求，主要对象是请求在项目中支持Ascend系列显卡。由于最近的地缘政治情况导致中国开发者无法访问 NVIDIA 显卡，因此希望项目能够扩展对Ascend系列显卡的支持。

https://github.com/vllm-project/vllm/issues/1604
这是一个bug报告类型的issue，主要对象是vllm模型在生成文本时当输入tokens超过3000个时出现低质量和重复性结果的问题。由于输入tokens数量过多，可能导致模型生成的文本内容混乱且重复。

https://github.com/vllm-project/vllm/issues/1603
这是一个bug报告，涉及主要对象为YiForCausalLM模型，由于不支持该模型架构导致了错误提示。

https://github.com/vllm-project/vllm/issues/1602
这是一个bug报告，主要涉及到vllm==0.1.3 ray==2.8.0中的LLMEngine.step() missing 1 required positional argument: 'request_arrival_time'，可能是由于参数缺失或不正确导致的问题。

https://github.com/vllm-project/vllm/issues/1601
这是一个bug报告，主要涉及的对象是ChatGLM模型。由于Transformers版本问题导致ChatGLM启动失败。

https://github.com/vllm-project/vllm/issues/1600
这个issue是关于用户提出需求的，主要涉及将Yi模型添加到量化支持中，源于Yi模型被添加到AutoAWQ和Yi的相关Pull Requests。

https://github.com/vllm-project/vllm/issues/1599
这是一个bug报告，主要对象是torch.repeat_interleave函数。由于cpu同步操作的存在，导致了性能问题。

https://github.com/vllm-project/vllm/issues/1598
这是用户提出需求的类型的issue，主要涉及的对象是预加载过程中的分页注意力操作。用户提出了希望支持针对预加载的分页注意力，以便在每批中并行计算多个标记。

https://github.com/vllm-project/vllm/issues/1597
该issue类型为Bug报告，主要涉及的对象为Docker build环境中CUDA版本不匹配，导致构建失败。

https://github.com/vllm-project/vllm/issues/1596
这是一个需求提出类型的issue，主要涉及构建与发布vLLM的CUDA11.8版本wheels，以及与CUDA 12.1版本wheels文件名的命名规则。

https://github.com/vllm-project/vllm/issues/1595
该issue类型是关于功能需求的，主要涉及如何在每次生成之前设置特定的种子。由于用户想要在每次生成文本前设置特定种子，而不必每次重新加载模型，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/1594
这是一个关于CUDA GPU不可用错误的bug报告，用户在运行vLLM时遇到了GPU内存分配问题，请求帮助。

https://github.com/vllm-project/vllm/issues/1593
这是一个bug报告类型的issue，涉及主要对象为模型分布训练（Distributed Inference）。造成该问题的原因是NCCL版本问题导致GPU重复检测错误，用户寻求解决此问题的方法。

https://github.com/vllm-project/vllm/issues/1592
这是一个bug报告，主要涉及Mistral 7B模型和AWQ相关的问题，用户遇到的问题是bfloat16和模型量化方面的不匹配。

https://github.com/vllm-project/vllm/issues/1591
这是一个bug报告。该问题涉及到vllm在处理空字符串时导致服务器崩溃并无法响应后续请求。

https://github.com/vllm-project/vllm/issues/1590
这是一个bug报告，涉及/v1/assistants API路径，由于服务器端没有找到该路径，导致返回404 Not Found状态码。

https://github.com/vllm-project/vllm/issues/1589
这是一个用户提出需求的issue，主要涉及的对象是vLLM项目。由于CUDA版本兼容性的问题，用户建议支持旧版本的torch和CUDA，以便更多人可以安装vLLM。

https://github.com/vllm-project/vllm/issues/1588
这是一个功能需求提出的issue，主要涉及的对象是从 www.modelscope.cn 下载模型，由于原始版本不兼容，导致用户需要支持该功能。

https://github.com/vllm-project/vllm/issues/1586
这是一个请求帮助的issue，主要涉及vllm中key cache和value cache的形状问题，提问者想了解其中x的作用、为什么key和value形状不同以及形状中的34652代表什么。

https://github.com/vllm-project/vllm/issues/1585
这个issue属于bug报告，主要涉及的对象是对应模型dollyv212b。由于未能正确使用自定义的指令跟随管道，导致模型响应不佳。

https://github.com/vllm-project/vllm/issues/1584
这是一个bug报告，涉及的主要对象是VLLM Core中的block_manager.py文件，在Beam Search过程中出现了Double free错误，可能原因是对已释放的物理Token块再次释放所导致的错误。

https://github.com/vllm-project/vllm/issues/1583
这是一个用户提出需求的issue，主要涉及vllm框架中的qwenchat7b模型，用户想要改造生成输出为指令形式的数据。原因是用户希望获得chat方法来输出具有指令形式的数据。

https://github.com/vllm-project/vllm/issues/1582
该issue类型是用户提出需求，该问题单涉及的主要对象是vllm模型。由于用户想要使用vllm的chat方法而不仅仅是generate方法，因此提出了该问题。

https://github.com/vllm-project/vllm/issues/1581
这个issue是一个bug报告，涉及的主要对象是"chatglm.py"文件中的代码。这个问题是由于self.total_num_kv_heads % tp_size不等于0导致的。

https://github.com/vllm-project/vllm/issues/1580
该issue类型为功能需求，涉及主要对象为新增GPTQ的支持。由于希望最小化代码变更并避免与正在进行的重构工作可能发生的冲突，导致了此需求。

https://github.com/vllm-project/vllm/issues/1579
这是一个bug报告，主要涉及VLLM中模型命名的问题导致无法加载模型。

https://github.com/vllm-project/vllm/issues/1578
这是一个bug报告类型的issue，涉及的主要对象是Tesla V100S GPU。由于输出被截断，用户请求帮助检查。

https://github.com/vllm-project/vllm/issues/1577
这是一个修复bug的issue，涉及主要对象是重复惩罚（repetition penalty）的逻辑。原因是修复后的惩罚逻辑与Hugging Face库对其进行了对齐，解决了之前CC(Support repetition_penalty)的问题。

https://github.com/vllm-project/vllm/issues/1576
这是一个用户提出需求的issue，主要涉及的对象是vLLM模型的批处理大小设置。用户想要测试不同批处理大小下vLLM的端到端延迟。

https://github.com/vllm-project/vllm/issues/1574
这是一个用户提出需求的类型的issue，主要涉及对VLLM支持稀疏性的问题。用户希望了解是否可以实现半结构稀疏性以降低推断延迟。

https://github.com/vllm-project/vllm/issues/1573
这个issue是用户报告的一个bug，涉及的主要对象是vllm加载AWQ量化模型时出现内存溢出问题。这个问题可能是由于模型AWQ处理过程或者VLLM的使用方式导致的。

https://github.com/vllm-project/vllm/issues/1572
这个issue类型为API兼容性问题，涉及主要对象为OpenAI API和https://github.com/Yidadaa/ChatGPTNextWeb。由于API路径不匹配，导致无法加载模型用于聊天目的。

https://github.com/vllm-project/vllm/issues/1571
这是一个建议性的改进 issue，主要涉及的对象是LLAMA模型中的GPU内存缓存，提出在`LlamaDecoderLayer.forward()`方法末尾添加`torch.cuda.empty_cache()`以减轻模型前向传播过程中的GPU缓存增加问题。

https://github.com/vllm-project/vllm/issues/1570
这是一个bug报告，涉及的主要对象是多个提示时生成的结果，由于没有设置适当的采样参数，导致生成的结果发生变化。

https://github.com/vllm-project/vllm/issues/1569
该issue类型为用户提出需求，主要涉及vLLM是否支持Dynamic SplitFuse方法，原因是研究人员认为这是DeepSpeedFastGen优于vLLM的主要原因之一。

https://github.com/vllm-project/vllm/issues/1568
这是一个用户提出需求的类型的issue，主要对象是添加新功能的实现。用户提出了关于引入flash decoding++的请求。

https://github.com/vllm-project/vllm/issues/1567
这个issue是一个feature请求，主要涉及支持Yi34B模型。由于其模型架构不完全一致，用户在询问vLLM是否能够支持该模型。

https://github.com/vllm-project/vllm/issues/1566
这是一个bug报告，主要涉及在ray cluster上使用tensor parallelism时遇到问题。用户尝试在ray集群上使用tensor parallelism时，设置tensor_parallel_size=2时无法加载模型，只能设置为1。

https://github.com/vllm-project/vllm/issues/1565
该issue为用户提出需求，希望在vLLM中集成SwitchTransformer / NLLB-MoE，询问是否已经有相关工作并询问主要挑战是什么。

https://github.com/vllm-project/vllm/issues/1564
这是一个用户提出需求的issue，主要涉及的对象是在VLLM下支持加载MPT模型的功能缺失。用户询问关于在AWQ格式中加载MPT模型的支持时间以及是否有临时解决方法。

https://github.com/vllm-project/vllm/issues/1563
这是一个bug报告，涉及的主要对象是在Google Colab上尝试使用EleutherAI/gpt-j-6b模型进行推理时出现的错误。这个问题可能是由于tensor model parallel group已经初始化而导致的。

https://github.com/vllm-project/vllm/issues/1562
这是一个需求提出的issue，主要涉及到vLLM项目维护者和DeepSpeed框架，用户提出了关于实现Dynamic SplitFuse的需求。

https://github.com/vllm-project/vllm/issues/1561
这是一个bug报告，针对Baichuan2-7b-chat模型无法输出</s> eos标记的问题。

https://github.com/vllm-project/vllm/issues/1560
这是一个用户提出需求的issue，主要涉及对象是增加对Baichuan2模型和多个LoRA适配器的支持。这个问题由于用户需要同时使用Baichuan213B和多个LoRA适配器，需要在批量推理中支持这些功能，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/1559
这个issue类型为bug报告，主要涉及对象是Yarn-Mistral-7b-128k模型。由于内存不足，导致CUDA内存溢出错误，用户希望了解如何设置max_seq_len以修改模型加载前的标记长度。

https://github.com/vllm-project/vllm/issues/1558
这个issue属于bug报告，主要涉及的对象是支持chatglm3模型。由于模型在特定输入下生成的结果不合理，导致了输出内容与预期不符的问题。

https://github.com/vllm-project/vllm/issues/1557
这是一个用户提出需求的issue，主要涉及的对象是 vllm 模型。用户询问是否有可能支持 LongLora 模型中使用的 short shift attention。

https://github.com/vllm-project/vllm/issues/1556
这是一个bug报告，涉及Mistralbased模型的查询问题，由于某些原因导致了数值错误"Double free!"的异常。

https://github.com/vllm-project/vllm/issues/1555
该issue类型属于用户需求提出，主要涉及对象是vllm_engine的请求处理机制。由于缺乏关于llm_engine中请求处理过程的详细说明和文档，用户提出了关于如何并行处理更多请求的问题。

https://github.com/vllm-project/vllm/issues/1554
这个issue属于用户需求类型，主要涉及vLLM是否支持Yi34B模型，并询问其支持情况及链接。

https://github.com/vllm-project/vllm/issues/1553
这个issue类型为文档补充，主要涉及的对象是函数`prompt_logprobs`。由于缺少描述，用户无法清楚了解该函数的作用和用法。

https://github.com/vllm-project/vllm/issues/1552
该issue属于用户提出需求类型，主要对象是为vLLM项目添加对ChatGLM3模型的支持。这个问题产生的原因是团队目前受到其他更高优先级工作的压倒，导致存在五个无法合并的过时PR，并需要社区志愿者的帮助将ChatGLM3模型推动完成。

https://github.com/vllm-project/vllm/issues/1551
该issue是一个发布追踪记录，涉及主要内容是新功能的添加和改进，不是bug报告。

https://github.com/vllm-project/vllm/issues/1550
这个issue类型是性能优化报告，主要涉及vLLM框架在2023年11月2日的性能瓶颈问题。原因可能是某些函数调用时消耗过多时间，需要进一步优化。

https://github.com/vllm-project/vllm/issues/1549
这是一个关于使用不同GPU运行finetuned模型时输出结果不一致的bug报告，主要涉及的对象是vllm模型和两种不同的GPU：A100(40GB)和A10G(24GB)。造成这种症状的原因可能是CUDA版本差异和使用不同GPU所导致的模型输出不稳定。

https://github.com/vllm-project/vllm/issues/1548
这是一个bug报告，主要涉及的对象是vllm模块。由于CUDA版本不匹配，导致安装vllm失败。

https://github.com/vllm-project/vllm/issues/1547
这是一个bug报告，涉及到使用quantization squeezellm在vllm中运行时缺少quant_config.json文件导致的错误。

https://github.com/vllm-project/vllm/issues/1546
这是一个bug报告，涉及的主要对象是Worker，由于sequences有不同的prompt lens导致了bug。

https://github.com/vllm-project/vllm/issues/1545
这是一个Bug报告，主要涉及的对象是AWQ模型的sampling过程。导致这一问题的原因可能是在sampling过程中存在大量的异步操作，导致AWQ模型在生成第一个token时比FP16模型慢。

https://github.com/vllm-project/vllm/issues/1544
这是一个bug报告，涉及到AWQ量化模型在推理过程中权重仍然映射为float16和int32的问题。产生这个问题可能是因为在代码中无法设置dtype为int4，并且get_supported_act_dtypes()只支持torch.half。

https://github.com/vllm-project/vllm/issues/1543
这是一个bug报告issue，主要涉及的对象是vLLM服务器。由于max_tokens设置过大（比如设为10000），导致服务器抛出AsyncEngineDeadError错误，且无法恢复处理未来请求。

https://github.com/vllm-project/vllm/issues/1542
这是一个用户提出需求的特性请求，主要对象是控制HuggingFace模型中config.json文件的内容。由于用户希望在加载模型时能够自定义config.json文件中的数值，以实现对默认参数的修改。

https://github.com/vllm-project/vllm/issues/1541
这个issue类型是bug报告，涉及的主要对象是OpenAI API。由于API只服务一个模型，当请求中的模型字符串不匹配时，导致出现错误，用户体验较差。

https://github.com/vllm-project/vllm/issues/1540
这是一个用户提出需求的类型，主要涉及的对象是部署在Kubernetes上的应用程序。这个需求是由于在部署过程中，Ingress需要进行健康检查以确保Pod正常运行。

https://github.com/vllm-project/vllm/issues/1539
这是一个bug报告，主要涉及Dockerfile在Windows平台下构建时出现的错误，可能由于版本兼容性或执行命令错误导致进程被逐渐终止。

https://github.com/vllm-project/vllm/issues/1538
这是一个bug报告类型的issue，主要涉及vllm中的AquiliaForCausalLM模型。由于AquiliaForCausalLM模型不支持量化操作，导致出现了数值错误（ValueError）。

https://github.com/vllm-project/vllm/issues/1537
这个issue属于用户提出问题的类型，主要涉及VLLM中Llama engine的实现方式，用户询问为何无法找到Llama engine的具体实现代码并询问其生成多个选择的机制。

https://github.com/vllm-project/vllm/issues/1536
这是一个bug报告，主要涉及torch.repeat_interleave函数的CPU同步问题，导致性能下降。

https://github.com/vllm-project/vllm/issues/1535
这是一个用户提出需求的issue，主要涉及 VLLM 下的 prompt_logprobs 功能，用户希望避免推理以节省运行时间。因此，该问题可能是关于优化运行时性能的建议或需求。

https://github.com/vllm-project/vllm/issues/1534
这是一个bug报告，主要涉及调度器在处理长提示时出现卡顿的问题。由于调度器停止将 seq_group 从等待列表中添加，导致出现长提示时会出现卡顿现象。

https://github.com/vllm-project/vllm/issues/1533
这个issue是关于用户提出需求的，主要涉及支持baichuan2模型和多lora适配器在单个批次中的使用。原因可能是为了提高效率和支持更多的硬件配置。

https://github.com/vllm-project/vllm/issues/1532
这是一个Bug报告，主要涉及到vllm模型在设置prompt_logprobs = 1时，处理超长输入时可能导致的OOM问题。

https://github.com/vllm-project/vllm/issues/1531
这是一个bug报告，主要涉及的对象是在设置engine_use_ray=True、worker_use_ray=False并且TP>1时出现的bug。Bug的原因可能是在特定的设置组合下引发了错误行为。

https://github.com/vllm-project/vllm/issues/1530
这是一个用户提出需求的issue，主要涉及系统提示模板的更改。这个问题由于用户希望能够调整默认系统提示以更好地适应任务要求。

https://github.com/vllm-project/vllm/issues/1529
这是一个变更请求类型的issue，主要涉及的对象是项目中的`MPTConfig`配置。由于MPT已经是HF Transformers中的一个常规模型，因此需要移除自定义的`MPTConfig`配置。

https://github.com/vllm-project/vllm/issues/1528
这个issue类型是bug报告，主要对象是代码格式检查器（linter），由于lint failures太多，容易忽略新出现的问题，因此用户提出只检查已修改的文件以减少出现lint failures的数量。

https://github.com/vllm-project/vllm/issues/1527
这是一个关于编译问题的bug报告，涉及到vllm在使用CUDA 12.1时无法成功构建的问题，原因是由于pytorch依赖的pybind问题。

https://github.com/vllm-project/vllm/issues/1526
这个issue类型是bug报告，涉及的主要对象是模型加载器（model_loader），由于官方MPT模型类名的大小写不一致导致vllm在特定情况下抛出`ValueError`。

https://github.com/vllm-project/vllm/issues/1525
这是一个bug报告，主要涉及vLLM下的输出响应被截断的问题。由于使用vLLM框架在ML服务器上运行推断时，输出结果被截断，而不使用vLLM框架的情况下，使用普通huggingface代码运行时返回更大的响应。

https://github.com/vllm-project/vllm/issues/1524
这是一个修复重复记录日志信息的bug报告，主要对象是项目vllm。由于代码重复记录日志消息导致了症状，需要修复以避免冗余日志输出。

https://github.com/vllm-project/vllm/issues/1523
此issue属于用户提出需求类型，主要涉及对象是在VLLM模型中支持LoRA finetuned models。用户提出这个问题是因为需要在同一时间内使用多个LoRA适配器，并尝试自行实现这些功能。

https://github.com/vllm-project/vllm/issues/1522
这是关于bug报告的issue，主要涉及vllm库中的cuda_utils模块，由于循环导入导致了ImportError错误。

https://github.com/vllm-project/vllm/issues/1521
这个issue类型是用户请求指导，主要涉及的对象是vllm工具，用户想要了解如何使用vllm计算标记的数量。

https://github.com/vllm-project/vllm/issues/1520
这是一个bug报告，涉及的主要对象是OpenAICompatible Server在Docker环境中出现错误。这个问题可能是由于isinstance()函数的参数错误导致了TypeError异常。

https://github.com/vllm-project/vllm/issues/1519
这个issue是一个bug报告，主要涉及OpenAI-Compatible Server无法从相同终端访问的问题。造成这个问题的原因可能是API调用的配置或终端环境设置不正确。

https://github.com/vllm-project/vllm/issues/1518
这是一个bug报告类型的issue，主要涉及到Codellama和CUDA kernel错误。由于可能的配置或代码问题，导致用户在推理过程中遇到CUDA kernel错误。

https://github.com/vllm-project/vllm/issues/1517
这个issue是关于用户提出需求的问题，主要涉及对象是vllm。原因是用户想要指定vllm在特定的GPU上运行。

https://github.com/vllm-project/vllm/issues/1516
这是一个用户提出需求的issue，涉及vllm能否同时处理两个提示，用户想了解是否可以同时处理多个问题。

https://github.com/vllm-project/vllm/issues/1515
这是一个用户提出需求的类型issue，主要涉及API服务器是否会自动批量处理用户请求，导致使用者无法设置批量大小而感到困惑。

https://github.com/vllm-project/vllm/issues/1514
这是一个bug报告，涉及的主要对象是VLLM的attention & cache ops。这个问题由于整数溢出导致了数组索引溢出的情况。

https://github.com/vllm-project/vllm/issues/1513
这个issue是在询问一个特定函数的使用情况，属于用户提出问题类型，主要涉及到函数gather_cached_kv()的使用情况。用户不清楚该函数的具体用途和在何处被调用，因此提出了这个问题。

https://github.com/vllm-project/vllm/issues/1512
这是一个bug报告，主要涉及VLLM模型的GPU内存消耗问题。问题可能由于多GPU环境下内存分配不正确导致每个GPU消耗相同的内存。

https://github.com/vllm-project/vllm/issues/1511
这是一个类型为功能需求提议的issue，主要涉及的对象是代码质量检测工具和代码格式化工具的更替。用户提出替换pylint和yapf为ruff的原因是ruff速度快且被广泛采用，同时指出yapf存在格式化限制和不确定性的问题。

https://github.com/vllm-project/vllm/issues/1510
这是一个功能需求提议，针对VLLM中的长上下文问题。

https://github.com/vllm-project/vllm/issues/1509
这是一个用户需求类型的issue，主要针对vLLM类型的Python包，问题是关于添加 py.typed 文件以便消费者可以进行类型检查。

https://github.com/vllm-project/vllm/issues/1508
这是一个特性需求报告，涉及vLLM模型中的W8A8推理支持。其原因是为了实现更好的模型推理性能。

https://github.com/vllm-project/vllm/issues/1507
这是一个功能增强的issue，主要涉及支持int8 KVCache Quant在Vllm中的实现。原因是为了提高吞吐量而引入了此功能。

https://github.com/vllm-project/vllm/issues/1506
这是一个提出问题的issue，主要涉及到vLLM模型在处理大于24832上下文长度时的问题。原因是vLLM在检查时使用了float32位宽，导致限制了模型上下文长度的设定。

https://github.com/vllm-project/vllm/issues/1505
该问题属于用户提出需求类型，主要涉及VLLM与ChatGLM3的兼容性。这可能是因为用户希望在VLLM中添加对ChatGLM3的兼容性支持。

https://github.com/vllm-project/vllm/issues/1504
这是一个bug报告，主要涉及OpenAI API server的echo功能的实现，由于修改了部分逻辑导致了一些行为变更和边缘情况的处理。

https://github.com/vllm-project/vllm/issues/1503
这个issue是一个bug报告，涉及的主要对象是vLLM的OpenAI API server。由于ChatCompletion请求默认不遵守FastChat会话模板设置的`stop_token_ids`和`stop_str`，导致在使用vLLM作为OpenAI兼容服务器时可能引发模型持续生成无关内容的问题。

https://github.com/vllm-project/vllm/issues/1502
这是一个bug报告类型的issue，主要涉及的对象是vLLM的OpenAI API服务器以及FastChat对话模板。由于默认ChatCompletion请求无法正确处理FastChat对话模板中设置的`stop_token_ids`和`stop_str`参数，导致在使用vLLM作为OpenAI API服务器时可能会出现生成的内容不相关的问题。

https://github.com/vllm-project/vllm/issues/1501
这个issue是bug报告，主要涉及InternLM模块中的bias参数；问题来源于未正确使用config.bias参数导致的bug。

https://github.com/vllm-project/vllm/issues/1500
这是一个bug报告，涉及的主要对象是api_server。由于请求中存在缺失的prompt导致内部服务器错误，需要将sampling_params转换为pydantic模型来自动验证请求输入。

https://github.com/vllm-project/vllm/issues/1499
这是一个bug报告，主要涉及的对象是vllm服务器。问题可能是由于安装后服务器在运行后自动关闭并重新启动，而没有显示任何错误消息。

https://github.com/vllm-project/vllm/issues/1498
这是一个用户询问安装问题的issue，用户希望在特定环境下安装vllm但遇到了依赖更新torch和cuda的问题。

https://github.com/vllm-project/vllm/issues/1497
这是一个bug报告，主要涉及对象为处理文本生成时可能出现的特殊符号跳过问题。该问题由于`skip_special_token=True`设置导致第一个特殊符号被跳过，造成最终结果出现重复内容的情况。

https://github.com/vllm-project/vllm/issues/1496
这个issue是关于bug报告，主要涉及对象是vLLM中的ColumnParallelLinear模块。该问题的症状是模型初始化阶段出现未知行为，可能是由于去除了初始化bias为零的步骤而导致。

https://github.com/vllm-project/vllm/issues/1495
这是一个关于优化的issue，主要涉及到如何实现磁盘缓存功能，用户正在寻求针对大量相似prompt prefixes的优化方案。

https://github.com/vllm-project/vllm/issues/1494
这是一个bug报告，涉及的主要对象是日志记录功能。这个问题的原因是当前的info日志实际上并未被记录，导致在OpenAI的api_server中args日志内容为空，该问题提出修复此问题的pull request。

https://github.com/vllm-project/vllm/issues/1493
这是一个用户提出需求的issue，主要是要求在vLLM中添加聊天模板支持。由于现有的FastChat无法处理模板，因此希望引入一种更标准化的模板处理方式，并增强对OpenAI Chat API的兼容性。

https://github.com/vllm-project/vllm/issues/1492
这是一个bug报告类型的issue，主要涉及vLLM服务器在处理大量请求时出现无响应的情况，导致请求挂起并最终导致无法继续处理新请求。

https://github.com/vllm-project/vllm/issues/1491
该问题类型是用户提出了需求，主要涉及的对象是GGUF支持，用户询问是否有计划或时间表支持GGUF。这个问题可能是由于用户希望了解软件是否计划支持GGUF，并想了解相关计划或时间表。

https://github.com/vllm-project/vllm/issues/1490
这个issue类型是bug报告，主要涉及的对象是ColumnParallelLinear模块，由于未正确初始化bias导致出现了一系列异常现象和bug。

https://github.com/vllm-project/vllm/issues/1488
这是一个bug报告，主要涉及的对象是VLLM的AWQ模型。该问题的原因可能是在使用VLLM中的v100 GPU运行AWQ模型时出现错误，并寻求解决方法。

https://github.com/vllm-project/vllm/issues/1487
这个issue类型是bug报告，涉及的主要对象是vllm和flask框架。由于版本兼容性问题导致vllm在与flask框架一起使用时出现一些异常现象，如第二次请求直接调用abort_request，无法进行更多的推理。

https://github.com/vllm-project/vllm/issues/1486
这是一个bug报告，涉及的主要对象是vLLM中的数组索引溢出问题，导致内存溢出和测试不通过。

https://github.com/vllm-project/vllm/issues/1485
这个issue属于bug报告，主要涉及的对象是api_server的engine.generate()函数。由于参数"prompt"缺失，导致当输入"prompt_token_ids"时出现错误。

https://github.com/vllm-project/vllm/issues/1484
这是一个用户提出需求的issue，主要涉及的对象是是否可以根据某些条件将请求路由到不同的模型工作者。

https://github.com/vllm-project/vllm/issues/1482
这是一个关于用户需求的问题，主要涉及到在NVIDIA Launchpad上运行vllm，并询问是否支持cuda版本12.x。

https://github.com/vllm-project/vllm/issues/1481
这是一个关于功能支持的问题，主要涉及vLLM的CPU推理以及量化模型支持的问题。用户想了解vLLM在CPU上是否支持LLM推理以及量化模型，并询问vLLM和GGML之间的区别。

https://github.com/vllm-project/vllm/issues/1480
这个issue类型为bug报告，主要涉及的对象是vllm模型的CUDA内存管理，导致CUDA OOM问题。

https://github.com/vllm-project/vllm/issues/1479
这是一个bug报告，涉及到在NVIDIA RTX 3060显卡上加载TheBloke/CodeLlama-7B-AWQ模型时出现内存不足的问题。

https://github.com/vllm-project/vllm/issues/1478
这是一个需求提出类型的issue，主要对象是OpenAI API中的model_check功能。由于检查模型名称的必要性存疑，用户建议删除或提供关闭选项。

https://github.com/vllm-project/vllm/issues/1477
这是一个用户提出需求的issue，主要对象是支持chatglm2。由于缺乏对chatglm2的支持，用户提出了这个问题。

https://github.com/vllm-project/vllm/issues/1476
这个issue是一个bug报告，主要涉及对象是model "Qwen7BChat"在vllm版本0.2.1下出现的AssertionError，可能是由于AsyncEngineDeadError导致的任务意外终止。

https://github.com/vllm-project/vllm/issues/1475
这是一个关于需求探讨的issue，主要讨论了VLLM团队是否在考虑使用TorchDynamo和XLA进行优化，以及可能带来的冲突。用户主要关注VLLM是否能获得类似的性能提升。

https://github.com/vllm-project/vllm/issues/1474
这是一个Bug报告，主要涉及的对象是使用EleutherAI/gpt-j-6b模型创建推理服务器时出现错误。产生这个Bug的原因可能是某些配置或参数设置不正确。

https://github.com/vllm-project/vllm/issues/1473
这个issue是用户提出需求类型的问题，主要涉及的对象是部署模型到多个GPU，并且由于一些指导不清导致用户陷入困境。

https://github.com/vllm-project/vllm/issues/1472
这是一个关于GPU内存利用bug的报告，主要涉及vLLM使用awq模型时出现的GPU内存预分配异常导致的问题，用户寻求关于GPU内存利用异常的帮助。

https://github.com/vllm-project/vllm/issues/1471
这是一个用户提出需求的issue，主要涉及到VLLM生成请求中请求ID连续导致的问题，用户希望能够获取非连续的请求ID来区分请求状态。

https://github.com/vllm-project/vllm/issues/1470
这个issue类型是用户提出需求，主要涉及llm库中的模型引擎，用户询问如何触发处理待处理请求、获取已处理提示的输出、获取等待请求或中止所有请求的机制，并报告了处理过程中出现的错误。

https://github.com/vllm-project/vllm/issues/1469
这是一个用户提出的需求类型的issue，主要涉及的对象是vLLM的SamplingParams。由于缺乏API，用户提出了添加logits processor以允许自定义代码在模型生成logits后对其进行修改，以实现特定格式输出等需求。

https://github.com/vllm-project/vllm/issues/1468
这是一个用户提出需求的类型，该问题涉及的主要对象是支持同时处理提示和生成的功能。

https://github.com/vllm-project/vllm/issues/1467
这是一个需求提出的issue，该问题涉及到添加模型注册以支持私有模型。原因是用户目前需要在本地维护vLLM的内部版本，以便与专有模型一起运行。

https://github.com/vllm-project/vllm/issues/1466
这个issue类型是用户提出需求，主要涉及对象是vllm，用户询问如何在使用vllm时添加最大窗口大小。这可能是因为用户想要控制vllm窗口的大小，但目前无法实现。

https://github.com/vllm-project/vllm/issues/1465
这是一个bug报告，主要涉及vllm版本0.2.1post和ray版本2.7.1中当`tensor_parallel_size`设置为2时出现的连接问题。

https://github.com/vllm-project/vllm/issues/1464
这是一个bug报告，涉及的主要对象是Llama-7b模型。由于使用批处理大小为3时，`"metallama/Llama27bhf"` 返回与原始HF模型不一致的输出，但批处理大小为1时输出一致。

https://github.com/vllm-project/vllm/issues/1463
这是一个bug报告，涉及的主要对象是FastAPI server与mistral AWQ模型的相互启动。由于某些AWQ mistral变体引发的错误导致服务器无法正常启动。

https://github.com/vllm-project/vllm/issues/1462
这是一个bug报告，涉及的主要对象是cumulative log_probs。由于v0.2.1版本移除了cumulative_logprob功能，在使用awq quant时总是返回0.0，可能导致该问题报告。

https://github.com/vllm-project/vllm/issues/1461
这是一个功能需求类型的issue，主要涉及的对象是支持内存共享的查询提示。由于目前仅共享了前缀的内存，而针对每个查询都进行了计算，用户提出需要共享内存以提高吞吐量。

https://github.com/vllm-project/vllm/issues/1460
这是一个用户在Github上提出的需求问题，主要涉及到模板格式的更改。由于用户在使用mistral模型时，发现默认模板格式不适用，希望了解如何修改格式。

https://github.com/vllm-project/vllm/issues/1459
这是一个bug报告，该问题涉及的主要对象是对Baichuan2的支持问题，原因可能是相关文件修改引发的推理错误。

https://github.com/vllm-project/vllm/issues/1458
这是一个bug报告，主要涉及对象是vllm下的一个llama2模型。造成此问题的原因可能是与最新版本的tokenizers不兼容。

https://github.com/vllm-project/vllm/issues/1457
这个issue类型属于需求提出，主要对象是Aquila模型中缺少rope_scaling参数，导致某个特定模型无法正常运行。

https://github.com/vllm-project/vllm/issues/1456
这是一个需求更改类型的问题，主要涉及VLLM中的Tensor Parallel功能，用户提出需要取消此功能，以便按顺序处理请求。

https://github.com/vllm-project/vllm/issues/1455
这是一个bug报告，涉及的主要对象是vllm的inference过程。由于attention_ops.paged_attention_v1限制了头部尺寸，导致了RuntimeError: Unsupported head size 32的错误。

https://github.com/vllm-project/vllm/issues/1453
这个issue是关于bug报告，涉及到CUDA版本不匹配导致的问题。

https://github.com/vllm-project/vllm/issues/1452
这是一个用户提出需求和请教问题的issue，主要涉及到vllm支持高吞吐量的参数设置和性能优化，用户想获取关于处理大量用户负载、优化设置以及处理更多并发请求的指导和建议。

https://github.com/vllm-project/vllm/issues/1451
这个issue类型为性能比较报告，主要涉及的对象是TGI和vLLM，用户对性能差异提出了疑问并建议更新TGI版本。

https://github.com/vllm-project/vllm/issues/1450
这是一个bug报告，涉及vllm 0.2.0版本的使用问题。由于此版本在连续发送请求时出现阻塞现象，而在使用vllm 0.1.4版本时未出现这个问题。

https://github.com/vllm-project/vllm/issues/1449
这是一个用户提出需求的issue，涉及对象是使用vLLM加载模型时内存不足的问题，用户询问增加tensor_parallel_size参数是否可以解决该问题，并咨询是否有其他方法可以处理。

https://github.com/vllm-project/vllm/issues/1448
这是一个bug报告，主要对象是使用llama270B模型时出现的异常。由于设置了'tensor_parallel_size=8'导致出现“RuntimeError: probability tensor contains either `inf`, `nan` or element < 0”错误。

https://github.com/vllm-project/vllm/issues/1447
这是一个空白issue，无法确定类型及主要对象。

https://github.com/vllm-project/vllm/issues/1446
这是一个bug报告，主要涉及的对象是vllm的api接口。由于没有正确设置模型量化参数并尝试使用"int4"，导致无法成功量化模型。

https://github.com/vllm-project/vllm/issues/1445
这是一个bug报告，主要涉及的对象是Qwen-14B-chat tokenizer。由于未能加载自定义的QWenTokenizer导致了无法加载tokenizer的错误提示。

https://github.com/vllm-project/vllm/issues/1444
这个issue类型是用户提出需求，主要涉及对象是添加Locally Typical Sampling功能，用户希望在vLLM中加入typical_p参数，并表达了希望通过这个功能实现更好的文本生成效果的需求。

https://github.com/vllm-project/vllm/issues/1443
这是一个用户提出需求的issue，主要涉及的对象是支持pipeline parallel，原因是因为tensor parallel需要偶数个GPU，所以需要支持任意数量的GPU。

https://github.com/vllm-project/vllm/issues/1442
该issue属于用户提出需求类型，主要对象是项目中的pre-commit hooks。由于用户认为将format.sh加入pre-commit hooks会比直接运行format.sh更好，因此提出了这个需求。

https://github.com/vllm-project/vllm/issues/1441
这是一个bug报告，涉及的主要对象是vllm在Mac环境下的安装问题。由于缺少CUDA_HOME路径，导致无法构建vllm包，出现了RuntimeError错误。

https://github.com/vllm-project/vllm/issues/1440
这是一个性能优化相关的issue，主要涉及到CUDA graph在Llama 2模型前向传播过程中的应用。原因是CUDA graph在低批次大小下可以略微提高前向传播速度，但在较大批次大小下则没有明显的收益。

https://github.com/vllm-project/vllm/issues/1439
这是一个用户提出需求的issue，主要涉及的对象是将vLLM与PyCharm IDE集成，用户希望寻找一个Jetbrain Plugin兼容vLLM并且类似Copilot的功能。

https://github.com/vllm-project/vllm/issues/1438
这是一个用户提出需求的类型，主要涉及到vLLM项目的Docker镜像的构建和自动构建问题。原因可能是用户希望在类似runpod这样的服务上更轻松地进行推断。

https://github.com/vllm-project/vllm/issues/1437
这是一个bug报告，主要涉及vllm server在使用多个GPU时出现错误的问题。由于设置`tensorparallesize=4`导致出现错误。

https://github.com/vllm-project/vllm/issues/1436
这个issue属于bug报告，涉及的主要对象是Aquila2模型，由于缺少线性rope_scaling导致了bug症状。

https://github.com/vllm-project/vllm/issues/1435
这个issue类型是关于性能问题的讨论，主要涉及tensor_parallel_size参数对吞吐量的影响，用户提出无论设置为1还是8，性能并没有显著变化，可能存在潜在的瓶颈导致这种情况。

https://github.com/vllm-project/vllm/issues/1434
该issue类型为功能需求提议，主要涉及添加基于tgi的vllm端点。

https://github.com/vllm-project/vllm/issues/1433
这是一个关于bug报告的issue，主要涉及LLama-2模型的配置文件缺失问题。造成这个问题的原因可能是配置文件丢失或者路径设置不正确。

https://github.com/vllm-project/vllm/issues/1432
这是一个关于在离线集群上使用Vllm时出现请求连接错误的bug报告，主要涉及Vllm无法在离线环境下正常运行的问题。

https://github.com/vllm-project/vllm/issues/1431
这是一个关于系统兼容性的问题，用户询问为什么低于7.0的计算能力的GPU不被支持，以及问是否有可能让其GPU与Tesla P4卡兼容。

https://github.com/vllm-project/vllm/issues/1430
这是一个bug报告，主要涉及vllm下的fine-tuned Mistral模型运行出现500错误响应的问题，可能由于finetuned版本引起的异常。

https://github.com/vllm-project/vllm/issues/1429
这是一个bug报告，涉及的主要对象是FastAPI与typingextensions和typinginspect版本之间的冲突，导致vLLM在Docker环境下无法正常运行。

https://github.com/vllm-project/vllm/issues/1428
这个issue是性能问题，涉及的主要对象是Mistral7B和Llama27B模型，在finetuning和量化后出现了Mistral速度比Llama2慢的现象。

https://github.com/vllm-project/vllm/issues/1427
这是一个bug报告，涉及的主要对象是类型提示。原因可能是类型提示存在错误或不一致，导致功能无法正常工作。

https://github.com/vllm-project/vllm/issues/1426
这是一个用户需求提出的issue，主要涉及的对象是为vLLM添加对StableLM-3B的支持。

https://github.com/vllm-project/vllm/issues/1425
这是一个开发者取消并计划重做的问题，涉及实现HF chat模板在tokenizer中的功能。

https://github.com/vllm-project/vllm/issues/1424
这是一个功能需求类型的问题，主要涉及对象为VLLM模型。这个需求是关于支持重复惩罚(repetition_penalty)，用户希望系统支持此功能。

https://github.com/vllm-project/vllm/issues/1423
这是一个需求讨论类的issue，主要涉及vLLM调度器无法对整个序列组进行调度，而只能调度其中的一部分序列。导致该问题的原因是GPU内存不足以容纳整个序列组。

https://github.com/vllm-project/vllm/issues/1421
这是一个bug报告，涉及的主要对象是在用两个RTX4090 GPU 运行Llama-2 13B时出现的尺寸不匹配错误，导致了参数拷贝时的RuntimeError。

https://github.com/vllm-project/vllm/issues/1420
这个issue是关于bug报告，涉及的主要对象是在加载模型"TheBloke/WizardLM-Uncensored-Falcon-40B-GPTQ"时出现了'KeyError: 'transformer.h.0.mlp.dense_4h_to_h.bias'的错误。

https://github.com/vllm-project/vllm/issues/1419
这是一个bug报告，涉及的主要对象是LLM（Low-Level Model）加载时出现的配置文件缺失问题。导致这个问题的原因是调用AWQ（Advanced Weight Quantization）时未找到相应的配置文件。

https://github.com/vllm-project/vllm/issues/1418
这是一个优化建议类型的issue，涉及的主要对象是vllm中的decoder模块。因为在prompt阶段，建议使用最后一个token对应的隐藏状态作为查询计算注意力，从而避免不必要的计算和函数调用。

https://github.com/vllm-project/vllm/issues/1417
这是一个缺少内容的issue，类型为需求提出，涉及对象为"num_gpu"。

https://github.com/vllm-project/vllm/issues/1416
这个issue类型是功能改进，主要涉及的对象是优化CUDA kernel执行top-k和top-p操作，导致原功能较慢并且内存占用较大。

https://github.com/vllm-project/vllm/issues/1415
该issue类型为功能需求，主要涉及添加Docker文件到项目中以及优化构建速度。

https://github.com/vllm-project/vllm/issues/1414
这是一个用户提出需求的issue，主要涉及vllm不支持跨不同查询缓存前缀，用户询问是否能够修改vllm实现以支持此功能，问题根源在于vllm未能利用不同查询间的共享前缀进行缓存。

https://github.com/vllm-project/vllm/issues/1413
这是一个改进建议类型的issue，主要涉及LLMEngine demo脚本的重构，旨在提升代码的清晰度和模块化。

https://github.com/vllm-project/vllm/issues/1412
这是一个关于bug报告的issue，主要涉及到Falcon模型在使用>8个GPU集群时出现除以0的错误，可能是由于整数除法导致num_kv_heads除以16得到0而引发的问题。

https://github.com/vllm-project/vllm/issues/1411
该issue是一个bug报告，涉及的主要对象是vllm项目下的InternLM 20B。由于ray.get()函数调用出现了错误，导致了报错的堆栈信息。

https://github.com/vllm-project/vllm/issues/1410
这是一个用户提出需求的issue， 主要关注对象是 Falcon180BAWQ 的支持问题，用户想知道是否有计划在不久的将来支持此功能。

https://github.com/vllm-project/vllm/issues/1408
这是一个bug报告类型的issue，主要涉及对象为代码中的无用语句。由于`self.num_generation_tokens = context_lens.shape[0]`，所以被移除的语句变得多余。

https://github.com/vllm-project/vllm/issues/1407
这个issue属于bug报告类型，主要涉及Baichuan2的不同部署方式导致获取答案错误的问题。

https://github.com/vllm-project/vllm/issues/1406
这个issue类型是用户提出需求，主要对象是vllm模型。由于该模型目前不支持repetition_penalty参数，用户希望在未来的版本中能够添加该参数以提高生成文本的多样性。

https://github.com/vllm-project/vllm/issues/1404
这是一个bug报告，主要涉及的对象是vllm模型。这个问题的症状是模型在回答之前会先补全输入的prompt。

https://github.com/vllm-project/vllm/issues/1403
这是一个BUG报告，主要涉及的对象为vLLM中的Baichuan模型。由于新版本transformers==4.34.0与当前vllm Baichuan模型不兼容，导致出现错误信息。

https://github.com/vllm-project/vllm/issues/1402
这个issue是一个bug报告，涉及到代码在2GPU A100上出现"incomplete"错误，但Assertion Error后没有更多的信息导致用户无法了解具体问题。

https://github.com/vllm-project/vllm/issues/1401
这是一个bug报告，涉及到在部署vLLM跨两台机器时出现的错误。由于process group初始化超时导致异常的bug。

https://github.com/vllm-project/vllm/issues/1400
这是一个bug报告，主要涉及VLLM在Tesla T4上不支持Bfloat16数据类型的问题，导致数值错误或内存不足的异常。

https://github.com/vllm-project/vllm/issues/1399
这是一个bug报告，该问题涉及到vocabulary size不能被worker数量整除，导致无法成功运行推理。

https://github.com/vllm-project/vllm/issues/1398
这个issue是关于性能问题的，涉及到VLLM模型的推理速度和批量大小，用户询问其他人对于llama-70B-chat-hf VLLM在batch size为2时推理速度的经验。

https://github.com/vllm-project/vllm/issues/1397
这是一个关于技术支持的问题，主要对象是vllm项目的开发者。用户询问vllm是否支持macOS M1或M2芯片，由于代码只包含了cuda，可能导致无法在这些芯片上运行。

https://github.com/vllm-project/vllm/issues/1396
这是一个关于软件兼容性的建议类型的issue，主要涉及的对象是要升级的pydantic库版本。原因是由于Ray的更新，现在兼容pydantic v2，因此用户提出升级pydantic版本的需求。

https://github.com/vllm-project/vllm/issues/1395
这是一个bug报告类型的issue，主要涉及到在tensor parallel模式下加载大型模型时，出现RAM OOM问题。这个问题的原因是多GPU环境下内存占用过高。

https://github.com/vllm-project/vllm/issues/1394
这是一个关于安装问题的bug报告，主要涉及的对象是vllm软件。此问题可能是由于torch版本设置不正确导致了缺失torch.version.cuda而无法安装vllm。

https://github.com/vllm-project/vllm/issues/1393
这是一个bug报告，涉及对象是使用多个GPU进行推断时出现的错误。这个问题可能是由于内存不足导致的。

https://github.com/vllm-project/vllm/issues/1392
这是一个用户提出需求的issue，主要涉及的对象是VLLM项目的重复惩罚功能。由于主要开发者未合并相关代码，并有关于认证支持的疑问，用户决定自行将代码合并到自己的分支。

https://github.com/vllm-project/vllm/issues/1391
这是一个关于兼容性问题的询问，涉及到vLLM模型与CUDA版本兼容性的讨论。

https://github.com/vllm-project/vllm/issues/1390
这是一个用户提出需求的类型，主要对象是vllm模型的参数设置。用户可能由于缺乏对vllm模型参数的了解而提出如何设置参数以最小化模型性能下降的问题。

https://github.com/vllm-project/vllm/issues/1389
这是一个bug报告，主要针对TP > 1时出现的错误。

https://github.com/vllm-project/vllm/issues/1388
这是一个bug报告，涉及的主要对象是更新配置文件以支持使用动态NTK的模型。由于缺少对应配置，该问题导致模型的上下文长度较短，需要通过修改配置文件来实现对应支持。

https://github.com/vllm-project/vllm/issues/1387
这是一个关于技术需求的issue，主要涉及到如何实现对多个用户进行批量推理服务的问题。用户提出这个问题是因为想了解如何实现类似于chatGPT这样的流式批量推理服务。

https://github.com/vllm-project/vllm/issues/1386
这是一个Bug报告，涉及LLM.generate方法在版本0.2.1中出现AssertionError的问题。

https://github.com/vllm-project/vllm/issues/1385
这是一个bug报告，涉及主要对象为Starcoder输出结果，在升级到0.2.0版本后出现输出为噪音的问题，可能是由于升级版本导致。

https://github.com/vllm-project/vllm/issues/1384
这是一个用户提出需求的issue，主要涉及VLLM模型为何不使用Apex进行推断加速，可能是用户想了解为什么VLLM没有集成Apex来提升推断速度。

https://github.com/vllm-project/vllm/issues/1383
这个issue类型是bug报告，主要涉及的对象是拼写错误。由于拼写错误导致了bug的症状，需要进行修正。

https://github.com/vllm-project/vllm/issues/1382
这是一个 bug 报告，涉及主要对象是 vllm 工具。由于使用 vllm 进行模型推理过程中出现了 _pickle.UnpicklingError: invalid load key, 'v.' 错误。

https://github.com/vllm-project/vllm/issues/1381
这是一个需求更改类型的issue，主要涉及调度器和输入张量形状的更改。导致这个更改的原因是为了能够使用更广泛的库和硬件，并为未来的优化提供基础。

https://github.com/vllm-project/vllm/issues/1380
这是一个bug报告，该问题涉及的主要对象是vllm的AsyncEngineDeadError。原因是在运行vllm时出现了"Task finished unexpectedly"错误。

https://github.com/vllm-project/vllm/issues/1379
这是一个bug报告，主要涉及的对象是sampler test。由于某些原因导致了sampler test功能存在问题需要修复。

https://github.com/vllm-project/vllm/issues/1378
这是一个bug报告，涉及的主要对象是PyTorch index URL，在构建软件包时出现错误。

https://github.com/vllm-project/vllm/issues/1377
这是一个需求类型的issue，主要涉及的对象是在构建项目wheel时使用的PyTorch版本。由于已将PyTorch版本固定为2.0.1，因此需要在工作流程中进行相应修复。

https://github.com/vllm-project/vllm/issues/1376
这是一个功能需求类型的issue，主要涉及的对象是vLLM项目中的数据收集和项目路线规划。原因是缺乏对硬件类型、性能计数器、云服务提供商和用户架构使用情况的可见性和信号，影响了项目优先级的确定。

https://github.com/vllm-project/vllm/issues/1375
这是一个关于优化利用率的问题，不是bug报告，主要涉及的对象是VLLM模型以及利用率优化。用户询问了如何针对较小的模型提高利用率的问题。

https://github.com/vllm-project/vllm/issues/1374
该issue类型为用户的问题/需求，主要对象是Nvidia Tesla P40 GPU。用户询问关于在7B模型上使用Nvidia Tesla P40 GPU生成tokens的速度。

https://github.com/vllm-project/vllm/issues/1373
这是一个bug报告，主要涉及了vLLM中的特殊标记之间总是存在空格的问题，问题由于缺少huggingface的`spaces_between_special_tokens`实现导致。

https://github.com/vllm-project/vllm/issues/1372
这是一个bug报告，主要涉及Scheduler源代码，用户提出了关于调度程序在特定条件下是否会传递预获取的seq_group的疑问。

https://github.com/vllm-project/vllm/issues/1371
这是一个用户提出需求的issue，主要涉及VLLM是否支持多轮对话中的KV-cache功能，用户询问了关于多轮对话中是否共用缓存的问题。

https://github.com/vllm-project/vllm/issues/1370
这是一个关于bug报告的issue，主要涉及的对象是模型推理中的单一查询缓存kv_attention机制。由于长序列推理时kv序列长度超过了最大支持的长度限制，导致CUDA错误和相关的运行时错误。

https://github.com/vllm-project/vllm/issues/1369
这是一个bug报告，涉及的主要对象是vllm模块。由于CUDA版本和Pytorch版本与vllm不匹配，导致了ImportError错误。

https://github.com/vllm-project/vllm/issues/1368
该问题属于bug报告类型，涉及主要内容是关于在多个副本中创建负载均衡器的问题。因为一些全局变量在实例初始化时被设置后不能更改，导致需要重置这些变量，然后遇到了设置新副本设备到设备ID 1的问题，同时尝试使用多线程导致了信号错误和地址错误。

https://github.com/vllm-project/vllm/issues/1367
这是一个bug报告，涉及的主要对象是vllm框架，由于缺少部分包导致出现了无法正常导入相关模块的错误。

https://github.com/vllm-project/vllm/issues/1366
这个issue类型为功能增强（Feature Enhancement），涉及的主要对象是将Mistral 7B模型添加到`test_models`中。

https://github.com/vllm-project/vllm/issues/1365
这是一个用户提出的需求类型的issue，主要涉及的对象是在VLLM项目中添加对HuggingFace Chat Templates的支持。

https://github.com/vllm-project/vllm/issues/1364
这个issue类型是bug报告，主要涉及到vllm.engine.async_llm_engine，由于任务意外完成导致引擎异常，需要反馈并提供详细信息。

https://github.com/vllm-project/vllm/issues/1363
这是一个关于如何在Kubernetes集群中跨多个节点部署vllm模型的问题，用户想要利用不同GPU类型的多节点之间使用Ray来实现更大的LLM模型。

https://github.com/vllm-project/vllm/issues/1362
这是一个询问类别的问题，主要围绕vllm是否使用Flash-Decoding算法展开讨论。

https://github.com/vllm-project/vllm/issues/1361
这是一个需求类型的issue，主要涉及对象是conversation template的来源问题。由于fastchat只是一种替代方式，导致用户希望使用更准确的huggingface tokenizer template。

https://github.com/vllm-project/vllm/issues/1360
这是一个bug报告类型的issue，主要对象是vllm的最新代码发布问题。由于缺乏良好的GPU支持，导致Mistral模型无法在VLLM中使用，从而出现了无法支持量化的错误。

https://github.com/vllm-project/vllm/issues/1359
这是一个bug报告，涉及主要对象为vllm项目。导致这个issue的原因是CUDA driver版本不符合CUDA runtime版本所导致的错误。

https://github.com/vllm-project/vllm/issues/1358
这是一个用户提出需求的issue，主要涉及项目的readme.md文件。由于readme.md文件中缺少"hi"，用户请求在文件中添加该内容。

https://github.com/vllm-project/vllm/issues/1357
这是一个用户提出需求的issue，主要涉及vLLM支持添加Token Filtering API功能。

https://github.com/vllm-project/vllm/issues/1356
这是一个bug报告，主要关注的对象是AWQ kernel的CUDA stream问题，由于AWQ kernels在默认stream而非当前CUDA stream中启动，导致需要修改确保在当前CUDA stream中启动。

https://github.com/vllm-project/vllm/issues/1355
这个issue类型为发布新版本的需求，涉及主要对象为版本控制。原因是需要升级版本到v0.2.1，并发布相应的Release Tracker。

https://github.com/vllm-project/vllm/issues/1354
这是一个bug报告，涉及的主要对象是vLLM模型。由于内存不足导致OOM错误，用户需要解决如何在vLLM中避免这一问题。

https://github.com/vllm-project/vllm/issues/1353
这个issue是一个关于bug报告的问题，主要涉及对象是vllm、xformers和Conda环境。这个问题是由于xformers最新版本在Windows系统上无法工作，导致vllm安装过程中尝试重新构建xformers而失败。

https://github.com/vllm-project/vllm/issues/1352
这是一个bug报告类型的issue，主要涉及的对象是OpenAI的API请求处理过程。由于API返回的响应对象不符合预期，导致了APIError异常的出现。

https://github.com/vllm-project/vllm/issues/1351
这是一个bug报告，涉及到使用OpenAI api时出现的"400 Bad Request"错误。可能是由于服务端或客户端的某些问题导致的请求错误。

https://github.com/vllm-project/vllm/issues/1350
这是一个用户提出需求的issue，主要对象是在vLLM中添加dockerfile。用户关注的问题是docker镜像的大小过大，是否需要进一步减小镜像大小。

https://github.com/vllm-project/vllm/issues/1349
这是一个用户提出问题类型的issue，主要涉及到如何编写Aquila api pload。用户提出了关于如何编写api prompt的问题，由于之前给出的示例是错误的，导致无法正确执行命令。

https://github.com/vllm-project/vllm/issues/1348
这是一个用户提出需求的issue，主要涉及的对象是实现PagedAttention V2 kernel。通过引入sequence-level parallelism，V2 kernel在小批量大小时能够获得巨大的加速，这说明用户希望通过实现V2版本的kernel来优化算法的性能。

https://github.com/vllm-project/vllm/issues/1347
这是一个bug报告，主要涉及vLLM模型中特殊标记之间始终存在空格的问题，原因是在VLLM中的API不完整导致无法禁止特殊标记之间的空格。

https://github.com/vllm-project/vllm/issues/1346
这是一个需求跟踪的issue，针对的主要对象是软件项目的发布跟踪。

https://github.com/vllm-project/vllm/issues/1345
这是一个用户提出需求的issue，主要涉及对象是V100，用户询问是否有支持V100的量化计划，并提出了一些需要修改的地方。

https://github.com/vllm-project/vllm/issues/1344
这是一个bug报告，涉及的主要对象是vllm0.2.0软件。由于某些原因导致了运行时出现"activation_ops undefined symbol"错误。

https://github.com/vllm-project/vllm/issues/1343
该问题单属于用户提出需求类型，主要对象是vllm在k8s上进行健康检查。由于当前健康检查方式相对简单且请求资源消耗较大，用户提出添加简单计算以提供更有效的检查。

https://github.com/vllm-project/vllm/issues/1342
这是一个用户询问问题类型的issue，主要涉及对象是如何在VLLM中调用像transformers这样的模型功能。用户提出此问题可能是由于在使用VLLM时遇到了无法调用相应函数的困惑。

https://github.com/vllm-project/vllm/issues/1341
这个issue是一个用户提出需求，询问AWQ是否支持Qwen baichuan model。原因可能是用户想要了解该模型的支持情况或寻求相关帮助。

https://github.com/vllm-project/vllm/issues/1340
这是一个bug报告，涉及vllm baichuan13b-chat模型在api服务器上输出<reserved_107>的问题。由于api只输出response，可能导致用户无法得到正确的模型响应。

https://github.com/vllm-project/vllm/issues/1339
这是一个bug报告类型的issue，主要涉及的对象是AquilaChat2-*模型。由于模型加载时显示错误，原因是架构名称不匹配，用户提出了需要添加架构名称并解决该问题的需求。

https://github.com/vllm-project/vllm/issues/1338
这是一个性能优化类的issue，主要涉及detokenization功能的优化。这个问题的目的是通过两个主要更改提高detokenization速度，由于快速tokenizer不需要使用慢速的`_convert_tokens_to_string_with_added_encoders`循环，并使用缓存属性来提高速度。

https://github.com/vllm-project/vllm/issues/1337
这是一个性能优化类型的issue，主要涉及到GPU与CPU同步时机的调整。原因是为了解决在之前PR中引入的轻微性能回退问题。

https://github.com/vllm-project/vllm/issues/1336
这是一个需求类型的issue，主要涉及 Lightweight (C++ / Native) Runtime，由于Roadmap中未提及轻量级运行时的跟踪问题，因此用户创建了这个问题以便让感兴趣的人能够订阅更新。

https://github.com/vllm-project/vllm/issues/1334
这个issue是关于bug报告，涉及到vllm项目的输出结果与torch项目的不一致问题。产生这个问题的可能原因是vllm cuda操作符与torch操作符之间的数值差异较大，可能导致模型输出结果逐渐偏离预期。

https://github.com/vllm-project/vllm/issues/1333
这是一个关于随机采样结果产生重复的问题，主要涉及到vllm项目中的ray.Workers，问题类型为用户提出疑问。

https://github.com/vllm-project/vllm/issues/1332
这是关于OOM（Out of Memory）错误的bug报告，涉及到VLLM的benchmark运行。由于内存不足导致OOM错误，用户在运行特定的benchmark时遇到问题。

https://github.com/vllm-project/vllm/issues/1331
这是一个用户需求提出的类型的issue，主要涉及的对象是更新README.md文件以包含有关Aquila2模型的信息。由于发布了Aquila2模型并将其开源在HuggingFace上，用户希望更新README.md文件，以便包含有关Aquila2模型的信息。

https://github.com/vllm-project/vllm/issues/1330
这是一个bug报告，主要涉及的对象是vllm模型升级至0.2.0版本后加载模型时出现错误。导致这个bug的原因是由于ray任务发生了TypeError异常。

https://github.com/vllm-project/vllm/issues/1329
这是一个关于调度器问题的疑问，涉及对象为服务在流模式下的请求处理。由于请求在调度器中被打断导致重新计算，可能引发错误。

https://github.com/vllm-project/vllm/issues/1328
这是一个功能需求的issue，涉及主要对象是vLLM模型。通过添加`prompt_logprobs`支持返回提示标记的对数概率，以支持OpenAI服务器中的`echo`功能，同时优化`logprobs`逻辑以进行批处理查询，可能会导致合并冲突并需要性能测试。

https://github.com/vllm-project/vllm/issues/1327
这个issue属于bug报告类型，涉及的主要对象是70B llama-2。原因可能是连续批处理中的bug导致响应异常，用户可能在寻找解决方案或希望限制并发请求数量。

https://github.com/vllm-project/vllm/issues/1326
这个issue是一则关于"特性添加"的请求，主要涉及的对象是为VLLM添加对SqueezeLLM量化方法的支持。由于用户希望在项目中运行4位稠密非均匀量化方案，因此请求添加相应的内核和量化配置文件。

https://github.com/vllm-project/vllm/issues/1325
这是一个bug报告类型的issue，主要涉及模型检查点文件的加载问题，由于 KeyError: 'base_lrs' 导致了该bug。

https://github.com/vllm-project/vllm/issues/1324
这是一个bug报告，报告了vllm安装过程中出现的transitive dependency导致pip命令 hang 的问题。

https://github.com/vllm-project/vllm/issues/1323
这是一个用户提出需求的issue，主要涉及VLLM对Qwen长聊天的支持问题，可能是由于VLLM目前还不支持Qwen的ntk_alpha方法，导致用户无法在VLLM中对Qwen的长聊天进行支持。

https://github.com/vllm-project/vllm/issues/1321
这是一个bug报告类型的issue，主要涉及Mistral-7B-Instruct-v0.1运行中的内存泄漏问题，导致数值错误和Double free错误。

https://github.com/vllm-project/vllm/issues/1320
这是一个bug报告，涉及的主要对象是在tensor_parallel_size大于1时出现的错误。出现这个问题是因为在设置NCCL_IGNORE_DISABLED_P2P=1之前出现了错误。

https://github.com/vllm-project/vllm/issues/1319
这是一个bug报告，涉及对象是openbuddy-llama2，由于路径错误导致生成的文本结果不符合预期。

https://github.com/vllm-project/vllm/issues/1318
这个issue类型是性能优化，涉及的主要对象是尝试用RPyC替代Ray实现，由于数据传输效率低导致了tokens生成速度较慢的问题。

https://github.com/vllm-project/vllm/issues/1317
这是一个用户提出需求的issue，主要涉及的对象是AsyncLLMEngine，用户想确认在1x1流量的情况下，AsyncLLMEngine是否会自动形成更大的批次来最大化GPU内存利用率。

https://github.com/vllm-project/vllm/issues/1316
这是一个关于需求和技术细节方面的问题，涉及主要对象是关于continuous batching的实现，由于连续批处理可能会导致prompt阶段和生成阶段混合的情况，用户询问如何处理这种情况。

https://github.com/vllm-project/vllm/issues/1315
这个issue类型为用户提出需求，涉及的主要对象是vllm下的`generate()`方法。用户提出疑问是关于`generate()`方法最大能接受多少个prompts，可能由于对vllm的使用需求而需要获取更多关于方法的详细信息。

https://github.com/vllm-project/vllm/issues/1314
这是一个bug报告，涉及的主要对象是ROCm端口的测试功能。测试中某些用例失败的原因尚未解决。

https://github.com/vllm-project/vllm/issues/1313
这是一个关于软件迁移至ROCm平台的issue，类型为功能性的需求。主要涉及的对象是vLLM的核心功能，并由于无法在特定设置下使用pip安装而引发。

https://github.com/vllm-project/vllm/issues/1312
这是一个关于输出被截断的bug报告，涉及VLLM框架下Baichuan2_13B模型的部署问题，用户在调用接口时遇到响应被截断的情况。

https://github.com/vllm-project/vllm/issues/1311
这是一个bug报告，涉及的主要对象是`SequenceOutputs`类的`__repr__`方法。原因是该方法存在错误，导致无法正确表示`SequenceOutputs`类的实例。

https://github.com/vllm-project/vllm/issues/1310
这是一个建议性质的issue，涉及主要对象为`entrypoints.openai.api_server`实现。由于需要使用`monitoring`功能并考虑到`prometheusfastapiinstrumentator`的依赖关系，用户提出了是否将其加入代码库的建议。

https://github.com/vllm-project/vllm/issues/1309
这个issue是一个bug报告，主要涉及到在vllm中排序logits的时机调整问题。由于logits在应用抽样参数之前被排序，导致抽样参数应用顺序错误，进而影响了最终结果。

https://github.com/vllm-project/vllm/issues/1308
这是一个bug报告，主要涉及v0.2.0版本切换时出现的'base_lrs' KeyError错误。可能是由于版本更新导致的代码兼容性问题。

https://github.com/vllm-project/vllm/issues/1307
这个issue类型是用户提出需求，该问题单涉及的主要对象是添加qwen-int4支持。由于原先未提供qwen-int4支持，用户提出了这个需求。

https://github.com/vllm-project/vllm/issues/1306
这个issue类型是bug报告，涉及到项目"vllm"的不完整问题。这可能是由于未完整填写相关内容或操作中遇到了bug导致。

https://github.com/vllm-project/vllm/issues/1305
这是一个bug报告，主要对象是qwen模型。由于qwen模型的输出不符合预期，用户反馈出现了问题。

https://github.com/vllm-project/vllm/issues/1304
这是一个用户提出需求的issue，主要涉及支持Attention Sink，由于需要实现Efficient Streaming Language Models，用户请求支持此功能。

https://github.com/vllm-project/vllm/issues/1303
这是一个文档修复的 issue，主要对象是 mistral.py 文件的注释，问题可能是关于注释不清晰或错误导致的。

https://github.com/vllm-project/vllm/issues/1302
该issue类型为用户提出需求，主要对象是Python API。由于用户无法通过Python API获得可以迭代的token流，因此请求提供示例代码以展示如何直接在Python中实现这一功能。

https://github.com/vllm-project/vllm/issues/1301
这是一个关于依赖问题的bug报告，主要涉及到vllm在使用CUDA 12和Torch 2.1.0时无法成功运行。造成这个问题的原因是PyTorch接口在23.06和23.07版本之间发生了变化。

https://github.com/vllm-project/vllm/issues/1300
这是一个bug报告，涉及到HuggingFace TGI的benchmark_serving.py运行时出现关于max_tokens的错误。可能是由于参数设置不正确导致的问题。

https://github.com/vllm-project/vllm/issues/1299
这是一个bug报告类型的issue，主要涉及对象为程序的输出结果。可能是由于程序逻辑错误或算法问题导致了出现不符合预期的答案。

https://github.com/vllm-project/vllm/issues/1298
这是一个bug报告，涉及的主要对象是设置gpu_memory_utilization。由于将gpu_memory_utilization的值设定为1.0导致OOM（Out of Memory）错误，用户希望知道gpu_memory_utilization的最大可设置值是多少。

https://github.com/vllm-project/vllm/issues/1297
这是一个技术研究和想法的问题，主要涉及到在低VRAM GPU上运行大型模型的批处理模式。

https://github.com/vllm-project/vllm/issues/1296
该issue是用户提出需求类型，主要涉及VLLM缺乏支持Embedding功能将导致无法构建知识库的问题。

https://github.com/vllm-project/vllm/issues/1295
这是一个bug报告，主要涉及的对象是AWQ内核中的gate_up_proj，由于长度超过一定值导致了溢出问题。

https://github.com/vllm-project/vllm/issues/1294
这是一个功能需求的issue，主要涉及的对象是OpenAI端点的支持情况。这个问题是由于vLLM API缺少对conversationtemplate参数的支持，导致用户提出了在启动API时指定conversationtemplate路径的需求。

https://github.com/vllm-project/vllm/issues/1293
这是一个关于需求类型的问题，主要涉及Docker镜像。用户想知道是否有任何Dockerfile或者官方Docker镜像。

https://github.com/vllm-project/vllm/issues/1292
这是一个内容更新的类型，主要涉及更新README.md文件，用户提出了添加meetup slides到最新动态部分以及删除与TGI相关的过时性能比较的建议。

https://github.com/vllm-project/vllm/issues/1291
这是一个Bug报告，主要涉及的对象是VLLM项目中的attention_ops.cpython-39-x86_64-linux-gnu.so文件，由于符号未定义导致ImportError，用户询问这个错误的原因。

https://github.com/vllm-project/vllm/issues/1290
这是一个关于软件构建环境问题的bug报告，涉及的主要对象是torch版本和vllm项目。由于torch 2.1.0编译时与CUDA 12.1不兼容，导致安装vLLM时出现错误。

https://github.com/vllm-project/vllm/issues/1289
这是一个bug报告，涉及主要对象是vllm项目中的finished_reason属性。由于误解导致了ignored序列的长度问题。

https://github.com/vllm-project/vllm/issues/1288
这是一个bug报告，主要涉及的对象是API server的ipv4 / ipv6支持，导致在使用`localhost`作为`host`时出现问题。

https://github.com/vllm-project/vllm/issues/1287
这是一个功能请求的issue，主要涉及模型实例在GPU内存中的利用，用户提出希望vLLM支持在单个GPU上同时运行多个模型实例的需求。Bug报告

https://github.com/vllm-project/vllm/issues/1286
这是一个用户提出需求的issue，主要涉及对象是LLaVA项目。由于用户想了解如何支持LLaVA，导致提出了这个问题。

https://github.com/vllm-project/vllm/issues/1285
这个issue类型是bug报告，主要涉及vllm在多线程grpc服务器下可能无法正确处理样本导致的KeyError错误。

https://github.com/vllm-project/vllm/issues/1284
这是一个bug报告，主要涉及对象为使用Tesla P100GPU运行Bfloat16时的计算能力限制问题。原因是Bfloat16仅支持计算能力大于等于8.0的GPU，导致在Tesla P100GPU（计算能力为6.0）上出现RuntimeError。

https://github.com/vllm-project/vllm/issues/1283
这是一个bug报告，主要涉及vLLM安装问题，由于可能与PyTorch版本和CUDA版本不兼容导致安装和运行失败。

https://github.com/vllm-project/vllm/issues/1282
该issue属于bug报告类型，主要涉及的对象是vLLM使用中出现的Quantization Method AWQ不支持当前GPU的情况；导致该问题的原因是GPU的Compute Capability不符合AWQ要求。

https://github.com/vllm-project/vllm/issues/1281
这是一个bug报告，涉及的主要对象是vllm中的api_server.py文件。由于版本不同导致出现AttributeError，需要解决配置文件中的属性错误。

https://github.com/vllm-project/vllm/issues/1280
这是一个bug报告，涉及的主要对象是处理不受支持的架构列表。导致该问题的原因可能是为什么要抛出错误而不是简单地过滤出不受支持的架构。

https://github.com/vllm-project/vllm/issues/1279
这是一个功能需求提议的issue，主要涉及的对象是vllm项目的OpenAI API功能。由于无法使所有cuda版本一致导致本地构建失败，用户尝试暴露`logit_bias`功能，并希望其他用户尝试解决问题。

https://github.com/vllm-project/vllm/issues/1278
这个issue类型是功能需求，主要涉及对象是更新模型加载器(model_loader.py)，原因是为了添加MistralForCausalLM awq量化解决方案。

https://github.com/vllm-project/vllm/issues/1277
这是一个用户提出问题的issue，主要涉及vLLM中attention设计的问题，询问如何确保当`num_prompt_tokens > 0`时，`num_generation_tokens == 0`。原因可能是在代码中未找到相应实现。

https://github.com/vllm-project/vllm/issues/1276
这是一个关于性能问题的bug报告，主要涉及VLLM模型加载时耗时长的问题。原因可能是AWQ格式加载较慢，导致执行API服务器耗时较长。

https://github.com/vllm-project/vllm/issues/1275
该issue类型是用户提出需求，涉及主要对象为使用正则表达式进行后缀匹配。由于处理长的stop words列表时可能会过于昂贵，用户提出了一种改进建议。

https://github.com/vllm-project/vllm/issues/1274
这个issue类型为bug报告，涉及的主要对象为代码中的"sequence status"。由于拼写错误导致了bug，需要修正问题。

https://github.com/vllm-project/vllm/issues/1273
这是一个bug报告，涉及主要对象为vLLM中的`stream`参数。由于`stream`参数接受了除`true`和`false`之外的其他字符串值，导致了与OpenAI和Azure OpenAI不一致的行为。

https://github.com/vllm-project/vllm/issues/1271
这是一个关于加载本地保存的fine-tuned LLama2模型时出现错误的bug报告。

https://github.com/vllm-project/vllm/issues/1270
这是一个用户提出需求的issue，主要涉及vLLM项目的多提示批量支持问题，用户想要了解是否计划支持这一功能。

https://github.com/vllm-project/vllm/issues/1269
这是一个关于性能问题的bug报告，涉及主要对象为vllm 0.2.0版本。用户发现在最新版本中未能看到预期的性能提升，可能由于不兼容的GPU版本导致无法使用量化功能，且与0.1.8版本相比性能表现相同。

https://github.com/vllm-project/vllm/issues/1268
这是一个bug报告，涉及的主要对象是vllm 0.2.0版本。由于版本更新导致无法检测NVIDIA驱动程序版本，导致CloudWatch不断输出错误日志。

https://github.com/vllm-project/vllm/issues/1267
这是一个bug报告，涉及的主要对象是vLLM模型在GPU上运行时出现了RuntimeError。该问题由于尝试在RTX 6000 Ada GPU上运行vLLM时，出现了shared memory不足的错误。

https://github.com/vllm-project/vllm/issues/1266
这是一个用户提出需求的类型的issue， 主要对象是希望vLLM添加对PolyLM模型的支持。用户提出这个需求是因为PolyLM是一个明确支持多语言的语言模型，并在基准测试中表现出色， 对于想要使用非英语或中文语言模型的用户来说，PolyLM的支持将会非常有帮助。

https://github.com/vllm-project/vllm/issues/1265
这是一个用户需求的issue，主要涉及生成文本的功能，提出了关于实现从embedding输入生成文本的需求。

https://github.com/vllm-project/vllm/issues/1264
这是一个用户提出需求类型的issue，涉及主要对象为实现YaRN模型支持，由于之前的基础工作，现在需要实现对YaRN模型的支持。

https://github.com/vllm-project/vllm/issues/1263
这个issue是一个功能增强需求，涉及到OpenAI API server中模板选择的问题，请求支持在请求中指定模板名称。

https://github.com/vllm-project/vllm/issues/1262
这是一个bug报告，主要涉及内存泄漏问题，导致内存利用率在一天内从50%上升至75%。

https://github.com/vllm-project/vllm/issues/1261
这是一个用户提出需求的类型，主要涉及ChatGLM 2的实现。原因可能是要改进vLLM的功能并为模型推断提供更好的支持。

https://github.com/vllm-project/vllm/issues/1259
这是一个bug报告，主要涉及到关于bfloat16检查操作的问题。原因是因为该检查需要CUDA初始化，导致在使用Ray workers时无法设置正确的环境变量，从而引发后续异常。

https://github.com/vllm-project/vllm/issues/1258
这是一个bug报告，主要涉及的对象是使用了`awq`量化的模型在使用4 tensor并行时出现错误。导致该问题的原因是`intermediate_size`的计算导致了无法满足要求。

https://github.com/vllm-project/vllm/issues/1257
这是一个用户提出需求的issue，主要涉及的对象是vllm中的频率惩罚功能。用户提出的问题是频率惩罚对某些特定token或字符串不起作用，希望添加一个功能来处理这种情况。

https://github.com/vllm-project/vllm/issues/1256
这是一个用户提出需求的类型，用户在询问是否有计划支持Intel ARC。

https://github.com/vllm-project/vllm/issues/1255
这是一个bug报告，涉及对象是尝试运行vllm中的Qwen14Bchat时出现错误。错误原因可能是命令执行导致的问题，请查看详细信息以找到解决方案。

https://github.com/vllm-project/vllm/issues/1254
这是一个升级和删除特定配置项的问题，涉及到MistralConfig 的移除，不属于 bug 报告。

https://github.com/vllm-project/vllm/issues/1253
这是一则用户提出需求的issue，主要涉及了StreamingLLM在VLLM下的支持问题，用户提出了关于是否可以实现该功能的疑问。

https://github.com/vllm-project/vllm/issues/1252
这是一个关于提出解决Turing GPU上AWQ问题的问题。它涉及到主要对象是使用Tesla T4的用户。

https://github.com/vllm-project/vllm/issues/1251
这个issue是关于代码优化和功能改进的，涉及到调度器（Scheduler）和策略（policy）的模块化重构。

https://github.com/vllm-project/vllm/issues/1250
这是一个功能需求的issue，主要涉及的对象是benchmarking脚本，用户提出了添加Triton作为后端选项的需求。

https://github.com/vllm-project/vllm/issues/1249
这是一个建议性质的issue，涉及主要对象是计算时间持续的方式，由于wall clock时间会倒退，建议使用单调时间来计算。

https://github.com/vllm-project/vllm/issues/1247
这个issue为用户提出需求，要求为`mistral7binstructv0.1`模型提供量化支持，由于目前无法对该模型进行量化，导致无法进行推理操作。

https://github.com/vllm-project/vllm/issues/1246
这是一个bug报告，主要涉及Baichuan 13B模型的性能问题，用户提出了由于性能差异导致的bug。

https://github.com/vllm-project/vllm/issues/1245
这个issue是一个功能需求反馈，主要涉及到在vLLM的api_server.py中缺少`readinessProbe`和`livenessProbe`的功能，导致在Kubernetes上部署LLM模型时无法指示apiserver的健康状态。

https://github.com/vllm-project/vllm/issues/1244
该issue类型为用户提出需求，涉及到更新`api_server`以包含`livez`和`readyz`端点，用于Kubernetes部署，因为这对健康检查非常有帮助。

https://github.com/vllm-project/vllm/issues/1243
这个issue属于功能增强类型，主要涉及了对于受限解码的改进。原因是为了让用户能够生成特定的标记。

https://github.com/vllm-project/vllm/issues/1242
这是一个bug报告，主要涉及AWQ模型在处理中等长度上下文时出现巨大延迟增加的问题，可能是由于AWQ模型处理方式导致的。

https://github.com/vllm-project/vllm/issues/1241
这是一个bug报告，主要涉及的对象是CUDA kernel function `single_query_cached_kv_attention_kernel`。由于存在一个vulnerable memory modification to gpu shared memory，导致出现了一个潜在的内存修改漏洞。

https://github.com/vllm-project/vllm/issues/1240
这是一个功能需求类的issue，主要涉及日志记录功能，要求默认情况下禁用详细请求日志记录，默认情况下不再记录详细请求信息并提供了新的开关控制功能。

https://github.com/vllm-project/vllm/issues/1239
这个issue是关于bug报告，主要涉及的对象是`TORCH_CUDA_ARCH_LIST`。这个问题是由于`TORCH_CUDA_ARCH_LIST`包含不受支持的CUDA架构导致的错误消息问题。

https://github.com/vllm-project/vllm/issues/1238
这是一个bug报告类型的issue，涉及主要对象为类型注解。这个问题可能由于类型注解错误导致代码逻辑错误或者类型检查失败。

https://github.com/vllm-project/vllm/issues/1237
这是一个关于需求的issue，主要涉及的对象是关于模型的并行推断方法。用户想知道是否存在更干净的方法来实现数据并行推理，而不是通过修改CUDA_VISIBLE_DEVICES的方式。

https://github.com/vllm-project/vllm/issues/1236
这是一个bug报告，主要涉及的对象是vllm项目中的AWQ功能在尝试添加Mistral时出现了错误。这个问题可能是由于torch.bfloat16到torch.float16类型转换引发的非法内存访问导致。

https://github.com/vllm-project/vllm/issues/1235
这个issue类型是用户提出需求，主要涉及的对象是添加关于如何在vLLM中使用AutoAWQ模型的文档。由于用户希望了解如何结合使用AutoAWQ模型和vLLM，所以提出了更新文档的请求。

https://github.com/vllm-project/vllm/issues/1234
这是一个关于无法加载7B AWQ量化模型导致内存不足的bug问题，用户主要关注的是在RTX 3060上加载大模型时遇到的内存问题。

https://github.com/vllm-project/vllm/issues/1232
这个issue类型为用户提出需求，主要涉及的对象是`api_server`入口点。由于vLLM会返回带有结果的提示，用户提出引入`return_prompt`请求选项来解决这个问题。

https://github.com/vllm-project/vllm/issues/1231
这是一个bug报告，涉及的主要对象是vLLM在OpenAI API中不支持中文和表情符号，可能由于无法获得实际标记导致无法处理此类文本数据。

https://github.com/vllm-project/vllm/issues/1230
这个issue是一个用户提出的需求。用户希望创建一个示例，演示如何使用Ray Data和vLLM进行分布式离线推断。

https://github.com/vllm-project/vllm/issues/1229
这是一个用户提出需求的issue，主要涉及的对象是该库的功能扩展，用户希望该库能够支持Grammar和GBNF文件。

https://github.com/vllm-project/vllm/issues/1228
这个issue属于功能增强类型，主要涉及benchmarking脚本，添加了`dtype`参数以增加与某些特定量化模型的兼容性，比如`TheBloke/CodeLlama13BInstructAWQ`。

https://github.com/vllm-project/vllm/issues/1227
这个issue类型是功能需求，涉及的主要对象是FastAPI服务器。这个问题是由于部署LLMs在AWS ALB上使用路径前缀时，ALB不支持路径重写，导致请求中包含了不正确的路径，从而破坏了API的正常使用，用户希望能够设置自定义的服务器基本路径来解决这个问题。

https://github.com/vllm-project/vllm/issues/1226
这是一个bug报告，问题涉及的主要对象是`vllm`下的模型名称`tiiuae/falcon-rw-7b`。由于在文档中多加了一个tick符号导致模型名称不匹配，需要移除额外的tick符号修复该问题。

https://github.com/vllm-project/vllm/issues/1225
这是一个Bug报告，涉及主要对象是vLLM的安装过程。由于CUDA架构不匹配导致数值错误。

https://github.com/vllm-project/vllm/issues/1224
这个issue类型是提出需求，涉及的主要对象是部分模型，由于部分模型缺少一些属性，导致需要为其提供默认值以避免错误和提醒用户。

https://github.com/vllm-project/vllm/issues/1223
这是一个bug报告，主要涉及测试prompt attention时出现OOM问题，所需测试较长的序列长度导致参考实现无法处理。

https://github.com/vllm-project/vllm/issues/1222
这是一个类型为bug报告的issue，主要涉及的对象是修复Mistral模型的部分。错的断言可能导致了bug的产生。

https://github.com/vllm-project/vllm/issues/1221
这是一个用户提出需求的类型的issue，主要对象是使Mistral成为支持的模型，可能由于现有模型列表不包含Mistral，用户希望将其添加到支持的模型列表中。

https://github.com/vllm-project/vllm/issues/1220
这是一个bug报告类型的issue，涉及的主要对象是Mistral model。由于MistralConfig的bug和滑动窗口的问题导致了需要修复并提交此issue。

https://github.com/vllm-project/vllm/issues/1219
这是一个用户提出需求的issue，主要对象是vllm模型。用户想要添加对对比搜索的支持，因为研究表明这可以明显改善模型质量，并且目前在transformers HF库中已经支持，希望vllm也可以具备这一功能。

https://github.com/vllm-project/vllm/issues/1218
这是一个用户提出需求的类型的issue，主要涉及的对象是vLLM，用户希望该工具能够全面支持Python 3.12，并包括在CI中进行测试以及将wheels上传至PyPI。

https://github.com/vllm-project/vllm/issues/1217
这个issue是一个需求提出类型，主要对象是GitHub Actions更新的依赖配置。原因导致用户寻求帮助是为了主动保持GitHub Actions工作流程的更新和安全性，并减少维护工作的负担。

https://github.com/vllm-project/vllm/issues/1216
该issue属于用户提出需求类型，主要涉及到VLLM库是否支持来自META官方实现，或者只支持Hugging Face风格。此问题可能由用户对库的支持范围和兼容性产生疑问。

https://github.com/vllm-project/vllm/issues/1215
这是一个用户需求提出的issue，主要对象是希望为OpenAI API服务器指定聊天提示模板。

https://github.com/vllm-project/vllm/issues/1214
这是一个bug报告，问题主要涉及到Illegal Memory Access导致的Internal Server Error。

https://github.com/vllm-project/vllm/issues/1213
这个issue类型是bug报告，主要涉及对象是Vicuna 7B GPTQ 16k模型，可能由于模型切换或训练问题导致了随机输出的bug。

https://github.com/vllm-project/vllm/issues/1212
这是一个版本升级的issue，涉及主要对象是软件的版本控制和发布。

https://github.com/vllm-project/vllm/issues/1211
这是一个用户需求类型的issue，主要涉及到带有per-request seed支持的需求。

https://github.com/vllm-project/vllm/issues/1210
这是一个用户提出需求的类型，主要涉及的对象是“qwen model”；用户可能需要添加一个名为"rope_scaling"的功能或特性到模型中。

https://github.com/vllm-project/vllm/issues/1209
这是一个用户提出需求的issue，主要涉及支持在超过8个GPU上进行sharding llama270b，由于现有代码不支持这一功能，用户寻求帮助解决这一问题。

https://github.com/vllm-project/vllm/issues/1208
这是一个关于bug报告的issue，主要涉及的对象是vllm在使用3个gpu时出现"cutoff error"。由于使用3个gpu可能导致算法错误或不支持，从而引发该bug。

https://github.com/vllm-project/vllm/issues/1207
这个issue类型是bug报告，涉及主要对象为vllm加载baichuan2-13b-chat量化模型时出错。原因是在使用官方提供的4位量化后仍然报错。

https://github.com/vllm-project/vllm/issues/1206
这是一个bug报告，涉及的主要对象是GPU KV cache。由于GPU KV cache使用率达到100%，导致GPU卡住并无法继续提供服务。

https://github.com/vllm-project/vllm/issues/1205
这是一个提议性的issue，主要涉及到在代码中移除对Ray AIR的依赖。

https://github.com/vllm-project/vllm/issues/1204
这是一个用户提出需求的类型，用户想知道是否可以使用MLCAI库来编译LLM模型。

https://github.com/vllm-project/vllm/issues/1202
这是一个bug报告类型的issue，主要涉及Greedy decoding生成不一致的输出。原因可能是设置不同导致结果不同。

https://github.com/vllm-project/vllm/issues/1201
这是一个关于CUDA内存不足错误的bug报告，主要涉及到PyTorch框架。这个问题是由GPU内存分配不当引起的。

https://github.com/vllm-project/vllm/issues/1200
这是一个bug报告，主要涉及的对象是处理多个请求时出现Key Error，用户遇到了由于系统配置和硬件环境导致的Key Error问题。

https://github.com/vllm-project/vllm/issues/1199
这是一个用户提出需求的类型的issue，涉及的主要对象是vLLM模型。用户询问vLLM是否需要额外更改才能支持Mistral 7B并指出了使用滑动窗口注意力可能需要在vLLM端进行小的修改。

https://github.com/vllm-project/vllm/issues/1198
这个issue属于功能改进类，主要对象涉及`max_num_batched_tokens`参数；由于默认值2560不适用于所有模型，导致需要根据模型的最大长度来动态设置`max_num_batched_tokens`参数。

https://github.com/vllm-project/vllm/issues/1197
这是一个bug报告，主要涉及的对象是vllm版本0.1.7，由于参数错误导致了 TypeError: issubclass() arg 1 must be a class 错误。

https://github.com/vllm-project/vllm/issues/1196
这是一个用户提出需求的issue，主要对象是Mistral-7B-v0.1支持。

https://github.com/vllm-project/vllm/issues/1195
这是一个问题类型为缺失内容（Empty Issue）的Issue，涉及的主要对象是关于设备。原因可能是用户忘记填写具体描述或信息，导致无法准确理解问题或需求。

https://github.com/vllm-project/vllm/issues/1194
这是一个用户提出需求的issue，主要对象是pip package vllm，用户请求发布新版本0.1.8以支持量化（AWQ），由于当前版本0.1.7不具备该功能。

https://github.com/vllm-project/vllm/issues/1193
这是一个关于 vllm 推断生成结果过短的 bug 报告，主要涉及的对象是推断模块，可能原因是在某些情况下会导致生成的结果字数不足。

https://github.com/vllm-project/vllm/issues/1192
这是一个bug报告，主要涉及到vllm模型在运行过程中出现错误。由于参数配置问题，导致出现包含大量感叹号的错误信息。

https://github.com/vllm-project/vllm/issues/1191
这是一个关于如何获取vLLM响应的JSON格式的问题，属于用户提出需求并咨询问题类型，主要涉及vLLM响应的格式化和相关库支持的问题。

https://github.com/vllm-project/vllm/issues/1190
这是一个关于需求的问题，主要涉及对象是vllm的大型模型，用户提出如何将fp16转换为int8。这可能是由于用户在模型部署或性能优化过程中遇到了类型转换的问题而提出的。

https://github.com/vllm-project/vllm/issues/1189
这是一个用户提出需求的issue，涉及的主要对象是VLLMs的配置参数`max_num_batched_tokens`，用户希望将其默认值设置为4096或8192来更好地支持当前4096/8192个prompt的模型。

https://github.com/vllm-project/vllm/issues/1188
这是一个bug报告，涉及到VLLM的wizardcode请求的任务意外完成。原因可能是由于GPU环境或者命令执行出现了异常导致了任务提前结束。

https://github.com/vllm-project/vllm/issues/1187
该issue类型为用户提出需求，主要对象是BartForSequenceClassification模块，用户希望实现类似"facebook/bartlargemnli"的速度需求。

https://github.com/vllm-project/vllm/issues/1186
这是一个用户提出需求的issue，主要涉及的对象是VLLM模型。原因是目前模型无法生成特殊意义的特殊标记，用户希望通过添加可选参数解决这个问题。

https://github.com/vllm-project/vllm/issues/1185
这是一个bug报告，主要涉及vLLM批处理推断时输出为空的问题。这可能是由于某些输入情况导致的，导致vLLM未能生成任何预测。

https://github.com/vllm-project/vllm/issues/1184
这是一个关于修改拼写错误的bug报告，涉及对象为vllm项目下的stage_devices参数，由于拼写错误导致在代码中应该使用的是placement_group参数。

https://github.com/vllm-project/vllm/issues/1183
这是一个bug报告类型的issue，主要涉及到HuggingFace/Starchat模型在生成输出时产生了垃圾内容。导致该问题的原因可能是stop条件没有正确设置。

https://github.com/vllm-project/vllm/issues/1182
这个issue类型是bug报告，主要涉及的对象是vLLM的Parallel Requests在AsyncLLMEngine下产生了不同的结果。原因可能是在发送多个并行请求时，可能导致同一查询的答案不一致。

https://github.com/vllm-project/vllm/issues/1181
这是一个修改请求（PR），涉及到`tensor parallel/quantization/weight loading refactor`，主要涉及`vLLM`的并行线性逻辑简化。

https://github.com/vllm-project/vllm/issues/1180
这个issue属于bug报告类型，主要涉及的对象是`vllm.entry_points.api_server.py`文件中的stream操作。原因是当前实现中没有优雅处理用户请求取消，导致需要增加代码来处理`asyncio.CancelledError`并在发生时执行必要的中止操作。

https://github.com/vllm-project/vllm/issues/1179
这个issue属于bug报告类型，主要涉及到vllm中的Sampler模块的正向传播方法，用户怀疑在应用采样参数时出现了错误导致的问题。

https://github.com/vllm-project/vllm/issues/1178
这是一个bug报告类型的issue，主要涉及的对象是"Serving Qwen/Qwen-14B-Chat-Int4"模型。由于某种原因导致了无法正确运行该模型，导致出现了错误。

https://github.com/vllm-project/vllm/issues/1177
这是一个Bug报告，主要涉及对象是vllm的源码安装，由于缺少某个符号导致启动报错。

https://github.com/vllm-project/vllm/issues/1176
这是一个文档改进类型的issue，涉及主要对象是RoPE初始化，可能是因为原始代码缺乏详细注释导致用户希望增加更多解释性注释。

https://github.com/vllm-project/vllm/issues/1175
该issue属于用户提出需求类型，主要涉及的对象是如何结合vllm与其他库/技术以进一步加速推理，可能由于推理的速度较慢导致用户希望探讨是否有其他库/技术可用于在vllm基础上实现次秒级性能。

https://github.com/vllm-project/vllm/issues/1174
这是一个用户提出需求的issue，主要关注在如何在超过单个GPU容量的模型上进行推断。造成这个问题的原因是在4090 GPU上选择13b模型时，模型没有被跨越两个GPU进行切分。

https://github.com/vllm-project/vllm/issues/1173
这是一个功能需求类型的issue，主要涉及的对象是qwen-14b模型。由于缺乏对qwen7b和qwen14b的支持，用户提出了需要同时支持这两个模型的需求。 

https://github.com/vllm-project/vllm/issues/1172
这是一个Bug报告，主要涉及到无法加载Qwen-14B-chat的问题。 由于未提供详细的信息，无法准确分析导致出现该问题的原因。

https://github.com/vllm-project/vllm/issues/1171
这个issue类型为讨论（Discussion），主要涉及的对象是vLLM解码加速方案。由于采样是vLLM的瓶颈，用户提出了采用Medusa Heads加速解码，并询问关于采用该方法可能面临的挑战。

https://github.com/vllm-project/vllm/issues/1170
这是一个用户提出需求的issue，主要涉及到对于prompt注意力机制的建议。

https://github.com/vllm-project/vllm/issues/1169
这个issue是一个bug报告类型，主要涉及的对象是使用vllm.entrypoints.openai.api_server时出现了错误的提示信息。该问题可能由于参数设置或代码逻辑错误导致了错误的完整提示内容生成。

https://github.com/vllm-project/vllm/issues/1168
这是一个用户提出需求的issue，主要涉及LLM Engine的更详细文档的需求。

https://github.com/vllm-project/vllm/issues/1167
该issue类型为用户提出需求，主要涉及对象为VLLM模型Phi 1.5。由于Phi 1.5是Microsoft推出的新模型，用户希望支持该模型并指出了一些特殊功能和需求，如MixFormerSequentialConfig和4位支持。

https://github.com/vllm-project/vllm/issues/1166
这个issue属于功能需求类型，主要涉及的对象是`uvicorn`模块。这个问题是为了添加标准的额外依赖项，以便在安装和使用`uvicorn`时推荐使用一些推荐的额外依赖项，其中包括`uvloop`，它是asyncio的高性能替代品。 Windows系统上不支持`uvloop`，`uvicorn`会在Windows系统上回退到使用asyncio。

https://github.com/vllm-project/vllm/issues/1165
这是一个用户提出需求的类型，主要涉及的对象是`VLLM_VERBOSITY`环境变量。

https://github.com/vllm-project/vllm/issues/1164
这个issue是bug报告类型，涉及的主要对象是Falcon40B模型的配置。原因是vLLM未能正确读取KV头的数量，导致返回了无意义的输出。

https://github.com/vllm-project/vllm/issues/1163
这是一个用户提出需求的issue，主要涉及对象是tokenizer revision，在原因可能是tokenizer版本不受支持导致用户无法指定版本的问题。

https://github.com/vllm-project/vllm/issues/1162
这是一个用户提出需求的issue，主要涉及的对象是vLLM的文档更新。由于Langchain对vLLM的支持需要更新文档并添加使用说明，因此用户提交了这个issue。

https://github.com/vllm-project/vllm/issues/1161
这是一个需求提出类型的issue， 主要涉及YaRN模型在vLLM下的实现和测试，用户提出了将YaRN实现在vLLM中并进行验证的需求。

https://github.com/vllm-project/vllm/issues/1160
这个issue属于bug报告类型，主要涉及raylet节点内存压力导致工作节点被OOM杀死，用户寻求如何解决内存问题及使用大模型进行推理时的内存需求。

https://github.com/vllm-project/vllm/issues/1159
这是一个用户提出需求的 issue，主要对象是将vllm-client仓库转移到vllm-project组织。由于需要将Python客户端库更改为vllm的组织，用户请求转移该仓库或者允许其执行转移操作。

https://github.com/vllm-project/vllm/issues/1158
这是一个bug报告，涉及的主要对象是API请求验证，由于缺少对`None` prompt和无效`SamplingParam`属性的检测，导致了请求验证的问题。

https://github.com/vllm-project/vllm/issues/1157
这是一个bug报告，涉及的主要对象是使用Tesla T4 GPU的用户。这个问题是因为GPU的计算能力不符导致数值类型的错误。

https://github.com/vllm-project/vllm/issues/1156
该issue是一个bug报告，主要涉及的对象是代码中的sampler logic，由于只有一个seq for the prompt seq group，导致代码逻辑中的if条件判断变得多余。

https://github.com/vllm-project/vllm/issues/1155
这是一个关于需求的问题，主要涉及对象是VLLM模型加载，用户询问是否支持加载量化模型以便在其GPU上运行推断。

https://github.com/vllm-project/vllm/issues/1154
这是一个需求提出的issue，主要涉及的对象是在attention kernel中分配更多共享内存，导致的bug或问题是支持更长的上下文长度。

https://github.com/vllm-project/vllm/issues/1153
这个issue类型是bug报告，涉及的主要对象是AWS EC2 T4 g4dn.xlarge实例。由于GPU的compute capability不满足要求，导致出现数值错误。

https://github.com/vllm-project/vllm/issues/1152
这是一个用户提出需求的问题，关注对象是是否支持 Whisper，可能是因为用户想要了解如何在项目中使用 Whisper 或者优化支持 Whisper 的对话模型。

https://github.com/vllm-project/vllm/issues/1151
这是一个Bug报告，涉及的主要对象是在离线情况下以特定模型名形式（如'organization/model-name'）调用代码时出现的Huggingface连接错误问题。

https://github.com/vllm-project/vllm/issues/1150
这是一个bug报告，涉及主要对象为使用类似 'organization/model-name' 格式的模型名称时，由于判断错误导致无法离线使用而报错的问题。

https://github.com/vllm-project/vllm/issues/1149
该issue为一个活动通知，主要对象是vLLM项目的用户和贡献者，用户可以在此了解项目进展并参与交流讨论。

https://github.com/vllm-project/vllm/issues/1148
这是一个用户发布需求的issue，主要对象是vLLM社区，由于需要组织第一次vLLM见面会而发布这个问题。

https://github.com/vllm-project/vllm/issues/1147
这是一个bug报告，主要涉及Ray安装无法被检测到的问题。可能由于某些原因导致导致即使Ray已经被安装和运行，系统仍然报告Ray未安装的错误。

https://github.com/vllm-project/vllm/issues/1145
这是一个文档错误类型的issue，主要涉及到openai_client.py文件链接失效的问题。由于链接指向的文件名已更新为openai_completion_client.py，导致用户无法访问所需的文件内容。

https://github.com/vllm-project/vllm/issues/1144
这是一个bug报告，主要涉及的对象是vLLM项目。这个问题是因为GPU不支持`bfloat16`，导致需要修改成使用`float16`，所以用户建议在不支持时降级为`float16`并发出警告。

https://github.com/vllm-project/vllm/issues/1143
这是一个关于硬件兼容性的问题，用户在询问VLLM是否兼容H800的GPU。

https://github.com/vllm-project/vllm/issues/1142
这是一个Bug报告，主要涉及对象是“ChatCompletionRequest”类。由于代码中调用了一个不存在的属性“stop_token_ids”，导致了 AttributeError。

https://github.com/vllm-project/vllm/issues/1141
这是一个bug报告，涉及IP bind错误导致的启动服务问题。

https://github.com/vllm-project/vllm/issues/1140
这是一个关于bug报告的issue，主要涉及到VLLM的bloom生成模块，出现了生成无意义输出的错误。

https://github.com/vllm-project/vllm/issues/1139
这个issue类型属于用户提出需求，主要涉及修改benchmark_throughput.py以使用多轮提示来进行吞吐量基准测试。导致用户提出此需求的原因是希望使用多轮提示更贴近实际情况。

https://github.com/vllm-project/vllm/issues/1138
这是一个关于增加对 GPT2LMHeadModel 的量化支持的需求提出的issue，不是bug报告。

https://github.com/vllm-project/vllm/issues/1137
这是一个bug报告，主要涉及测试性能问题。这个issue可能是由于baichuan2-13B测试性能较慢导致的。

https://github.com/vllm-project/vllm/issues/1136
这是一个bug报告，涉及到VLLM项目中生成器无法停止的问题，原因是当使用非默认的eos_token_id时出现了该bug。

https://github.com/vllm-project/vllm/issues/1135
这是一个用户提出需求的类型，主要对象是将vLLM部署在容器中，以节省时间。

https://github.com/vllm-project/vllm/issues/1134
这是一个BUG报告，主要涉及VLLM中的NaN问题，由于提交e67b4f2导致了输出错误。

https://github.com/vllm-project/vllm/issues/1133
这是一个关于项目运行方式的疑惑的问题，涉及到vllm implementation。由于对项目如何运行不清楚导致bug或使用困难。

https://github.com/vllm-project/vllm/issues/1132
这是一个用户提出需求的issue，主要对象是vLLM engine。原因是用户希望vLLM engine添加一个可选的回调函数，用于提供特定的运行统计信息，以便将这些信息导出到Prometheus指标中，以便在生产环境中可以根据这些信息将请求路由到不同机器上运行的vLLM engine实例。

https://github.com/vllm-project/vllm/issues/1131
这是一个用户提出需求的issue，主要涉及"vLLM"在将本地训练的模型添加到Hugging Face时的需求。由于预先条件要求上传到Hugging Face，用户提出是否可以直接服务于本地目录中保存的预训练Transformer模型。

https://github.com/vllm-project/vllm/issues/1130
这个issue是一个bug报告，主要涉及的对象是使用vllm进行gpt2推理的功能。这个bug是由于输入prompt为空时触发的错误导致的，进而导致后续请求无法得到结果，除非重新启动服务。

https://github.com/vllm-project/vllm/issues/1129
这是一个bug报告，用户在尝试运行peft模型时遇到了错误，因为在模型主文件夹中缺少config.json文件。

https://github.com/vllm-project/vllm/issues/1128
这是一个bug报告，涉及的主要对象是vllm。由于输出与Hugging Face不一致，用户提出了问题并寻求帮助。

https://github.com/vllm-project/vllm/issues/1127
这是一个Bug报告类型的Issue，主要涉及的对象是vllm.engine.async_llm_engine。这个问题产生的原因可能是某个任务意外结束导致了异常错误。

https://github.com/vllm-project/vllm/issues/1126
这是一个bug报告，主要对象是使用baichuan2作为llm的用户。由于不同的采样方法导致生成答案质量下降。

https://github.com/vllm-project/vllm/issues/1125
这是关于bug报告的issue，主要涉及的对象是使用vLLM进行分布式推断时的docker容器。由于docker容器的共享内存分配不足，当使用TP时，可能导致vLLM出现卡顿或性能不佳的情况。

https://github.com/vllm-project/vllm/issues/1124
这是一个bug报告，主要对象是vllm的model config。由于无法在本地`config.json`文件中修改QWen7b模型的配置，导致始终使用默认的模型配置。

https://github.com/vllm-project/vllm/issues/1123
这是一个bug报告类型的issue，涉及主要对象为PyTorch中的`DtypeTensor`，由于PyTorch正在弃用`DtypeTensor`，用户在使用最新版本时遇到了相关错误提示。

https://github.com/vllm-project/vllm/issues/1122
这是一个功能需求提交的Issue，主要涉及对象是支持在更多于8个GPU上进行分片的llama270b，该需求的原因是目前Llama2仅有8个KV头，无法在超过8个GPU上运行Llama270b。

https://github.com/vllm-project/vllm/issues/1121
这是一个关于软件bug报告的issue，主要涉及vLLM的输入序列问题，导致出现了`AssertionError: Prompt input should have only one seq.`错误。

https://github.com/vllm-project/vllm/issues/1120
这是一个bug报告，涉及到在使用`download_dir`参数时，VLLM仍然会下载和使用`~/.cache/`路径下的文件。这可能是由于代码中的逻辑错误导致的。

https://github.com/vllm-project/vllm/issues/1119
这个issue是用户询问有关实验中使用的批处理参数的问题，属于用户提问类型。该问题涉及了VLLM（Very Large Language Model）和TGI。用户的疑问可能源于对实验参数选择的好奇，希望了解在比较实验中为什么选择了特定的批处理参数。

https://github.com/vllm-project/vllm/issues/1118
这是一个bug报告，用户遇到了在运行 benchmarks/benchmark_throughput.py 时出现卡住的问题，原因还需要进一步分析。

https://github.com/vllm-project/vllm/issues/1117
这是一个用户提出需求的issue，主要涉及实验结果的展示是否考虑到了延迟和吞吐量在不同并发情况下的问题。

https://github.com/vllm-project/vllm/issues/1116
这是一个bug报告，该问题涉及LLama-70b在8A100机器上无法正确识别多个GPU。出现这个问题的原因可能是未能检测到GPU。

https://github.com/vllm-project/vllm/issues/1115
这是一个用户提出需求的issue，主要涉及的对象是engine的`dtype`参数。用户提出需要将`float16`和`float32`添加到dtype选择中。

https://github.com/vllm-project/vllm/issues/1114
这个issue是关于功能需求的，主要涉及的对象是vllm下的AWQ模型。由于AWQ模型目前只支持float16而不支持bfloat16，用户提出希望增加对bfloat16的支持或者添加一个--dtype float16选项的建议。

https://github.com/vllm-project/vllm/issues/1113
这是一个关于bug报告的issue，主要涉及对象是vllm不支持GPU H100 serving设备，导致用户在使用H100设备时出现CUDA错误。

https://github.com/vllm-project/vllm/issues/1112
该issue属于功能增强类型，主要涉及到int8 KVCacheQuant和W8A8 inference在vLLM中的实现，原因是为了提高吞吐量和减少首个token延迟。

https://github.com/vllm-project/vllm/issues/1111
这是一个bug报告，涉及的主要对象是vllm在使用两块L4 GPU且tensor_parallel_size设置为2时出现问题。由于这种特定配置条件下程序无法正常工作，用户寻求帮助解决问题。

https://github.com/vllm-project/vllm/issues/1110
这个issue类型是bug报告，主要涉及的对象是Qwen-7B-Chat，由于用户在使用长文本时（3k以上）无法得到回答，导致了此问题的提出。

https://github.com/vllm-project/vllm/issues/1109
这是一个bug报告，主要涉及chatglm2在多GPU上推理存在问题。由于GQA限制，超过2个GPU的推理结果不正确。

https://github.com/vllm-project/vllm/issues/1108
这是一个bug报告类型的issue，主要涉及到vLLM的context长度的支持问题。用户提出vLLM当前不支持较长context长度的问题，导致无法达到预期的效果。

https://github.com/vllm-project/vllm/issues/1107
这是一个用户提出需求的类型，涉及的主要对象是API server。

https://github.com/vllm-project/vllm/issues/1106
该issue是关于新增功能（Feature），主要涉及到简单API令牌认证的添加。这个改动可能是为了方便用户安装fschat和accelerate，以及对vllm的openai服务器和vanilla服务器添加了FastAPI本地访问令牌认证。

https://github.com/vllm-project/vllm/issues/1105
这是一个bug报告，涉及到API-Server在长时间运行后变得非常缓慢的问题。原因可能是与虚拟机、Conda环境和A100 GPU相关的配置或资源管理问题。

https://github.com/vllm-project/vllm/issues/1104
这是一个bug报告，主要涉及 vllm 在特定prompt上出现故障。由于无法正常运行，用户寻求关于此问题的帮助。

https://github.com/vllm-project/vllm/issues/1103
这个issue类型为用户提出需求，涉及的主要对象是Orca implementations。由于用户对于Orca实现的基线感到好奇，希望能够查看实现以更好地理解Orca存在的问题。

https://github.com/vllm-project/vllm/issues/1102
这个issue类型为bug报告，主要涉及的对象是API代码，由于多余的日志输出和重复的方法导致了不必要的代码和复杂性。

https://github.com/vllm-project/vllm/issues/1101
这是一个bug报告，涉及的主要对象是vllm下的beam_search模块。由于beam_search时父序列不是子序列的前缀，导致出现潜在错误。

https://github.com/vllm-project/vllm/issues/1100
这是一个bug报告类型的issue，主要对象涉及到gptq Qwen-7B-Chat-Int4 load_weights函数，导致错误信息中涉及了KeyError。

https://github.com/vllm-project/vllm/issues/1099
这是一个性能优化的issue，主要涉及到`detokenize_incrementally`函数，用户提出将`convert_tokens_to_string`只转换新计算的token以提升性能。

https://github.com/vllm-project/vllm/issues/1098
这是一个关于文档的问题，涉及到聊天完成消息和`--served-model-name`参数的文档说明问题。问题由于不清楚`--served-model-name`参数与`fastchat`注册的对话模板必须匹配才能解决使用`messages`输入格式时的聊天完成消息导致。

https://github.com/vllm-project/vllm/issues/1097
这是一个功能改进建议的issue，主要涉及vLLM在处理特殊token时生成无法停止的问题。原因可能是默认情况下跳过了特殊token，且提出者认为应该支持`stop_token_ids`参数来解决问题。

https://github.com/vllm-project/vllm/issues/1096
这是一个bug报告，涉及的主要对象是vllm项目下的rope_theta和max_position_embeddings参数。

https://github.com/vllm-project/vllm/issues/1094
这个issue是一个感谢信息，不属于bug报告或需求提出，主要涉及的对象是软件用户。

https://github.com/vllm-project/vllm/issues/1093
该issue为用户提出需求，请求增加新的接口功能。该问题主要涉及到llm.generate。用户希望能够在llm.generate中结合其他API使用。

https://github.com/vllm-project/vllm/issues/1092
这是一个bug报告，主要涉及的对象是代码中关于Baichuan2Chat7B模型的应用。由于应用了错误的模型，导致生成结果混乱。

https://github.com/vllm-project/vllm/issues/1091
这是一个用户提出需求的issue，主要涉及docker镜像和vllm项目。用户希望有一个可用的Dockerfile和相关requirements文件，以节省时间。

https://github.com/vllm-project/vllm/issues/1090
这是一个用户提出需求的issue，涉及的主要对象是LLM类。由于频繁使用和要求，用户希望将`gpu_memory_utilization`和`swap_space`参数明确添加到LLM类中。

https://github.com/vllm-project/vllm/issues/1089
该issue是一个发布追踪类型的内容，主要涉及到VLLM的性能优化和支持新增模型等方面，用户提出了几项需求和待完成的任务。

https://github.com/vllm-project/vllm/issues/1088
这是一个关于改进建议和沟通方式的类型，涉及到vLLM的用户和开发者社区，由于目前的沟通方式效率不高而导致创建了vLLM Discord服务器。

https://github.com/vllm-project/vllm/issues/1087
这是一个Bug报告，涉及的主要对象是OpenAI API。由于在vllm连接到OpenAI API并输入超过2560个字符的文本时，会导致客户端挂起。

https://github.com/vllm-project/vllm/issues/1086
这个issue类型为社区建议，主要对象是vLLM Discord server。由于社区用户希望添加 Discord 服务器以增进社区交流。

https://github.com/vllm-project/vllm/issues/1085
这是一个bug报告，主要涉及Baichuan2-7B模型在使用`model.generate`函数时产生无效输出的问题，与Baichuan2-13B模型没有相同的问题。可能是由于模型参数或数据不匹配导致的。

https://github.com/vllm-project/vllm/issues/1084
这是一个bug报告，主要涉及WSL（Windows Subsystem for Linux）上的性能问题以及VLLM在该环境下的运行表现。由于使用'pin_memory=False'导致性能下降，用户寻求优化设置或建议解决此问题。

https://github.com/vllm-project/vllm/issues/1083
这是一个用户提出需求的issue，主要涉及的对象是为vllm添加Prometheus导出器。这个需求源于用户希望通过Prometheus客户端为API添加`/metrics`路由以帮助vllm成为更完整的产品。

https://github.com/vllm-project/vllm/issues/1082
这是一个bug报告，用户遇到了vllm在运行baichuan时出现的错误“Waiting sequence group should have only one prompt sequence”。原因可能是设置了不正确的batch大小。

https://github.com/vllm-project/vllm/issues/1081
这是一个bug报告，主要涉及llm_engine和async_engine的step方法不一致，可能由于异步引擎取消繁忙循环而导致了问题。

https://github.com/vllm-project/vllm/issues/1080
这个issue属于功能需求提议，主要涉及LLM模块的量化支持问题。

https://github.com/vllm-project/vllm/issues/1079
这是一个bug报告，涉及的主要对象是vllm模型部署。由于高并发情况下触发了recompute模式，导致部分请求的延迟过长。

https://github.com/vllm-project/vllm/issues/1078
该issue类型是用户提出需求，主要涉及API client package的开发和移动问题。由于需要一个简单且只有最小依赖的API客户端包，以便于标准化客户端并为后续兼容性升级做准备。

https://github.com/vllm-project/vllm/issues/1076
这个issue是一个功能需求报告，主要涉及vLLM中`diversity_penalty`参数的使用问题，用户希望能够实现输出的多样性。

https://github.com/vllm-project/vllm/issues/1075
这个issue类型是用户提出需求，询问是否vLLM api server有类似TGI提供的健康和指标端点。

https://github.com/vllm-project/vllm/issues/1074
该issue属于bug报告，涉及主要对象是vLLM的Docker构建过程。由于AWQ的最低架构要求导致无法构建vLLM，并希望通过启用`TORCH_CUDA_ARCH_LIST`来选择目标GPU解决这一问题。

https://github.com/vllm-project/vllm/issues/1073
这是一个bug报告，涉及对象是safetensors对于quantized models的支持。由于safetensors需要在进行转置操作之前转换为常规张量，导致了AWQ模型在vLLM中无法加载的错误。

https://github.com/vllm-project/vllm/issues/1072
这个issue是关于功能需求的，主要涉及实现非自回归语言模型M2M在vLLM中的可能性，发起者询问如何在vLLM中实现M2M，因为目前支持的模型都是因果语言模型，而M2M是非自回归且具有编码解码器结构。

https://github.com/vllm-project/vllm/issues/1071
这是一个bug报告，涉及的主要对象是vLLM无法加载以Safetensors格式保存的AWQ模型。这个问题由于vLLM不能正确加载AutoAWQ保存的Safetensors格式的模型，导致用户无法发布大量测试模型。

https://github.com/vllm-project/vllm/issues/1070
这是一个关于无法构建vLLM Docker容器的bug报告，主要涉及Docker构建过程中的GPU架构设置问题，导致构建失败。

https://github.com/vllm-project/vllm/issues/1069
这是一个bug报告，涉及主要对象是HuggingFace Transformers和vllm，可能由于使用不同库执行相同任务导致结果不一致。

https://github.com/vllm-project/vllm/issues/1068
这是一个bug报告，涉及到修复了get_max_num_running_seqs函数的问题，由于之前的问题导致了等待和交换序列组数量的错误计算。

https://github.com/vllm-project/vllm/issues/1067
这是一个bug报告，主要涉及的对象是代码库中的参数初始化问题，因未正确初始化参数导致模型无法正常运行。

https://github.com/vllm-project/vllm/issues/1066
这是一个关于如何部署API服务器为HTTPS的问题，属于用户寻求帮助类型，涉及对象为API服务器和HTTPS配置。由于应用程序以HTTPS打开，无法通过HTTP请求API，用户寻求解决方案而不想修改api_server.py文件。

https://github.com/vllm-project/vllm/issues/1065
这是一个bug报告，主要涉及vLLM模型在生成token时出现问题，可能由于`prompt token ids: None`表示的问题导致无法生成tokens。

https://github.com/vllm-project/vllm/issues/1064
这是一个需求类型的issue，主要涉及的对象是AWQ。由于AWQ不支持Turing GPUs，需要添加最低功能要求。

https://github.com/vllm-project/vllm/issues/1063
这是一个bug报告，主要涉及的对象是AWQ在Turing GPUs上不支持。由于AWQ不支持Turing架构，导致了无法构建kernel的错误信息。

https://github.com/vllm-project/vllm/issues/1062
这是一个关于GPU消耗问题的bug报告，主要涉及环境中的虚拟机、Conda环境和A100 GPU，并询问由于什么原因导致GPU内存使用量过高的问题。

https://github.com/vllm-project/vllm/issues/1061
这是一个空白的issue。

https://github.com/vllm-project/vllm/issues/1060
这是一个bug报告，主要涉及的对象是vllm项目中的CUDA版本不匹配问题。

https://github.com/vllm-project/vllm/issues/1059
这是一个bug报告，涉及的主要对象是`AsyncLLMEngine`。由于使用忙碌循环导致CPU利用率过高，需要采取措施减少CPU利用率并确保后台任务受到保护。

https://github.com/vllm-project/vllm/issues/1058
这是一个关于bug报告的issue，主要涉及vllm模型在重新初始化ray时出现hang的问题。由于ray状态没有正常重置，导致了脚本第二次运行时卡住的情况。

